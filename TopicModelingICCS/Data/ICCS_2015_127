Procedia Computer Science
Volume 51, 2015, Pages 944–953
ICCS 2015 International Conference On Computational Science

Leveraging workﬂows and clouds for a multi-frontal solver
for ﬁnite element meshes
Bartosz Balis1,2 , Kamil Figiela1,2 , Maciej Malawski1 , and Konrad Jopek1
1

AGH University of Science and Technology, Department of Computer Science, Krakow, Poland
2
AGH University of Science and Technology, ACC CYFRONET, Krakow, Poland

Abstract
Scientiﬁc workﬂows in clouds have been successfully used for automation of large-scale computations, but so far they were applied to the loosely-coupled problems, where most workﬂow
tasks can be processed independently in parallel and do not require high volume of communication. The multi-frontal solver algorithm for ﬁnite element meshes can be represented as
a workﬂow, but the ﬁne granularity of resulting tasks and the large communication to computation ratio makes it hard to execute it eﬃciently in loosely-coupled environments such as
the Infrastructure-as-a-Service clouds. In this paper, we hypothesize that there exists a class
of meshes that can be eﬀectively decomposed into a workﬂow and mapped onto a cloud infrastructure. To show that, we have developed a workﬂow-based multi-frontal solver using the
HyperFlow workﬂow engine, which comprises workﬂow generation from the elimination tree,
analysis of the workﬂow structure, task aggregation based on estimated computation costs,
and distributed execution using a dedicated worker service that can be deployed in clouds or
clusters. The results of our experiments using the workﬂows of over 10,000 tasks indicate that
after task aggregation the resulting workﬂows of over 100 tasks can be eﬃciently executed, and
the overheads are not prohibitive. These results lead us to conclusions that our approach is
feasible and gives prospects for providing a generic workﬂow-based solution using clouds for
problems typically considered as requiring HPC infrastructure.
Keywords: Scientiﬁc workﬂows, multifrontal solver, ﬁnite element method, distributed computing infrastructures

1

Introduction

Scientiﬁc workﬂows are successfully used for automation of computational problems in a variety
of scientiﬁc disciplines [15]. Using workﬂows lets the user focus on describing a computational
problem as an abstract graph of tasks, while the workﬂow management system transparently
takes care of its execution that includes, amongst others, provisioning of the required computing
resources, task scheduling, deployment of application components, and data staging. A workﬂow
944

Selection and peer-review under responsibility of the Scientiﬁc Programme Committee of ICCS 2015
c The Authors. Published by Elsevier B.V.

doi:10.1016/j.procs.2015.05.230

Leveraging workﬂows and clouds for a multi-frontal solver . . .

Balis, et. al

deﬁned once enables transparent utilization of diverse Distributed Computing Infrastructures
(DCIs), such as clusters, grids or clouds.
Solvers for ﬁnite element meshes are traditionally targeted for typical HPC machines designed for maximum performance but not necessarily the best price/performance ratio which is
the primary design goal of cloud infrastructures. Recently, cloud infrastructures are increasingly
used by large industry enterprises relying on HPC, oﬀering reduction of cost and turnaround
time in comparison to maintaining an on-premises HPC infrastructure [10]. The elasticity of
resource allocation oﬀered by the cloud also makes it an excellent solution for multi-phase
computational problems where individual stages of the computation have diﬀerent resource
demands and need to be scheduled independently.
The multi-frontal solver algorithm introduced by Duﬀ and Reid [4, 5] is the state of the art
solver for mesh-based computations. The solver algorithm can be decomposed into a graph of
tasks. This has been already done for one or two-dimensional ﬁnite diﬀerence method [11], as
well as for two-dimensional [14] and three-dimensional [13] adaptive ﬁnite element method. The
decomposition of multi-frontal solver into graph of tasks follows the structure of the elimination
tree. For adaptive mesh based computations, namely for grids with point and edge singularities,
it is possible to construct elimination trees with lightweight computational tasks close to the
root of the elimination tree [12, 6, 1]. This concerns computational meshes in both two and
three dimensions. These observations motivated us to analyze the solver workﬂow for cloud
computations.
In this paper, we apply the workﬂow approach to parallelizing computational problems
arising from the ﬁnite element model and utilizing the direct multi-frontal solver method. We
argue that cloud computing can be an eﬀective and eﬃcient model for such problems. The ﬁrst
issue to consider is the possibility of a high communication/computation ratio that, in a cloud
infrastructure based on commodity hardware, may lead to signiﬁcant communication overheads.
Moreover, the ﬁne-grained nature of tasks in the workﬂow may result in prohibitive overheads of
the workﬂow execution system, leading to poor parallel eﬃciency and lack of speedup. However,
our hypothesis is that a broad class of meshes can be eﬀectively decomposed and mapped onto a
distributed infrastructure. Having a three-dimensional domain with high-order polynomials for
shape functions can further balance the computation and communication time. Furthermore,
the multi-frontal method is known to be memory-intensive [3, p. 259]. As Clouds typically oﬀer
more memory per node [10] in comparison to typical HPC machines, they can be used to tackle
larger problems that would have been possible otherwise. Finally, unlike HPC machines, cloud
services are accessible to everyone at any time. Our target environment is the infrastructureas-a-service (IaaS) cloud, which allows for dynamic on-demand creation of computing nodes in
the form of virtual machines (called instances), but our approach can also be used on more
traditional commodity clusters that do not have fast interconnects.
To evaluate our hypothesis, we implemented a solution based on the solver described in
[8] and our workﬂow runtime environment HyperFlow [2]. We propose a loosely coupled solution which comprises two architectural layers: the solver front-end and the workﬂow system
backend. The solver front-end generates the elimination tree which is automatically converted
by the workﬂow backend to a graph of tasks (workﬂow), which are mapped to available computing resources and executed. As a result, problem decomposition is well decoupled from its
mapping to the execution environment, facilitating transparent access to diverse computing infrastructures. Furthermore, the user does not need to learn the runtime environment and is not
concerned with low-level programming related to decomposition of and communication between
tasks. The workﬂow system hides this complexity. Finally, the direct multi-frontal method for
sparse linear systems is well-deﬁned in terms of CPU, data and memory complexity. As a re945

Leveraging workﬂows and clouds for a multi-frontal solver . . .

Balis, et. al

sult, it is possible to perform the appropriate task clustering and calculate accurate execution
plans for workﬂow schedulers [9], decreasing the overheads and performance loss resulting from
inaccurate predictions.
The results of our preliminary experiments reported in this paper indicate that the initial
workﬂow graph generated from the elimination tree is indeed characterized by such a ﬁne
granularity of tasks that the overheads of the distributed workﬂow execution are prohibitive.
We show, however, that by applying even simple task aggregation, the task sizes can be increased
so that the overheads are mitigated. These results are promising and conﬁrm the feasibility
of our approach, giving the prospects of good performance when larger and more complex
problems are considered.
The paper is organized as follows: in Section 2 we present how the computation of multifrontal solver algorithm can be represented as a workﬂow. Next, in Section 3 we introduce
the architecture of our prototype implementation of the workﬂow-based multifrontal solver.
Section 4 covers the experiments and their results. Finally, in Section 5 we present the ﬁnal
conclusions and directions for further research.

2

Workﬂow representation of multi-frontal solver computation

The core idea of our approach is to represent a multi-frontal solver computation as a graph of
tasks that can be executed using a workﬂow management system. This is achieved in two major
steps. (1) Workﬂow generation: ﬁrst, a directed acyclic graph (DAG) of ﬁne-grained tasks is
generated from the elimination tree. (2) Task aggregation: next, the structure of the graph
is analyzed in order to estimate the computation and communication costs and perform the
aggregation of tasks in order to reduce the communication to computation ratio. The following
subsections describe this approach in detail.

2.1

Workﬂow generation

The multi-frontal solver algorithm is widely used to solve Finite Element Method and Finite
Diﬀerence Method problems since they generate regular sparse matrices. Multi-frontal solver
requires a binary elimination tree that represents the order in which frontal matrices are merged
and eliminated. The tree is usually generated by planar graph analysis of the linear system of
equations, but in the case of the Finite Element Method the elimination tree can also be derived
from the mesh. The heuristic for generating suboptimal elimination tree was introduced in [12].
The leaves of an elimination tree represent individual elements of the mesh while the remaining nodes represent subsequent steps of the elimination of a multifrontal solver. Each node
of the tree has a frontal matrix and a corresponding right-hand side of the equation that needs
to be persisted during the process of solving. The matrices for leaf nodes are initially populated by element matrices and then altered during solver execution. The entire solving process
requires traversing the elimination tree twice: the ﬁrst bottom-up traversal is the elimination
phase while the second top-down traversal is the backward substitution phase. In the elimination
phase, matrices are merged and partially eliminated starting from leaves and then traversing
up the tree. The partial eliminations are equivalent to computing the Schur complements of
the matrix. Finally, the solver computes the solution at the root node. For regular grids, the
top root problem is expensive, and its computational cost is equal to O(N 1.5 ) in the 2D case
and O(N 2 ) in the 3D case. However, in the case of adaptive grids, the root problem solution is
946

Leveraging workﬂows and clouds for a multi-frontal solver . . .

(a) Mesh

Balis, et. al

(b) Tree

Heavy
Light

Backward substitution

Elimination

Figure 1: Example mesh with three vertex singularities and its possible corresponding elimination tree.

Figure 2: Workﬂow DAG for tree presented on Fig. 1. The upper elimination part comprises
tasks of high computing cost (heavy), while the lower backward substitution part has low
computing cost (light).

no longer a bottleneck. During the backward substitution phase the tree is traversed top-down
in order to propagate the solution from the upper nodes down the tree. Finally, the leaves are
updated with the solution of the corresponding elements of the mesh.
This process can be represented as a workﬂow: a DAG of tasks created by mirroring the
elimination tree with respect to the root node. Fig. 1 shows an example mesh with three vertex
singularities and its possible elimination tree which is transformed into a workﬂow presented in
Fig. 2.

2.2

Workﬂow structure and its properties

The workﬂow has several interesting properties. First, the DAG is symmetrical with respect to
the root node of the elimination tree, but the elimination phase that involves matrix factorization has a much higher computational cost than the backward substitution phase. Second, the
costs of computation and communication can be precisely calculated analytically by analyzing
the mesh and the elimination tree [12]. Finally, well-separated groups (sub-trees) of tasks, which
correspond with the structure of the original mesh, can be identiﬁed in the workﬂow structure.
Due to all these properties, aggregation, planning and scheduling of the workﬂow can be done
eﬀectively and accurately, so that tasks are allocated in the best possible way that maximizes
data locality and minimizes communication.
Workﬂows with similar properties can also result from meshes with edge singularities
(Fig. 3), and for combined vertex and edge singularities (Fig. 4).
947

Leveraging workﬂows and clouds for a multi-frontal solver . . .

Balis, et. al

(b) Tree

(a) Mesh

(c) Workﬂow

Figure 3: Example mesh with multiple edge singularities and possible elimination tree with
workﬂow.
Moreover, the memory usage for workﬂow tasks can also be accurately estimated, so that
graph partitioning and task scheduling algorithms can take it into account and produce a resource allocation plan which meets the requirements of the workﬂow but avoids unnecessary
over-provisioning of resources.

2.3

Task aggregation

The main idea behind the multi-frontal solver is to replace one substantial computation with
a large number of small ones that can be done in parallel. Consequently, the frontal matrices
are usually very small, resulting in very ﬁne-grained tasks in the workﬂow. To reduce the
communication to computation ratio and to minimize the impact of processing in distributed
infrastructure, agglomeration of tasks is required. The analysis of the elimination tree provides
us with the estimates of task runtimes, which can be the basis for task agglomeration, using
either simple aggregation heuristics or more advanced graph partitioning algorithms available
e. g. in Metis [7].
Currently we use the following simple task aggregation
heuristic: we estimate the number of FLOPs for processing of
each subtree of the elimination tree, and if the number of FLOPs
for a given subtree is below a speciﬁed threshold, then the subtree
is clustered into a single task in the aggregated workﬂow. Such
an aggregated task processes the entire subtree sequentially. The
threshold may be chosen by the user or by selecting e.g. the nth
percentile of estimated FLOPs of all the subtrees. For example,
if we set the threshold to 95 percentile, we aggregate the 95% of
smaller subtrees into tasks, and we leave the tasks corresponding to top 5% of the largest subtrees (those are tasks near the
root of the tree). Thus, the resulting workﬂow will have enough
parallelism but the overheads due to too ﬁne grained tasks will Figure 4: Example mesh
be mitigated. This simple policy does the aggregation only on with multiple edge and verthe leaf nodes but not near the root of the tree.1
tex singularities.
1 The

948

small number of tasks related to nodes of the elimination tree that can be aggregated close to the root

Leveraging workﬂows and clouds for a multi-frontal solver . . .

Balis, et. al

Hyperflow infrastructure

Mesh
generator

Elimination tree
generator

Application,
infrastructure
and billing
model

Execution
planner and
optimizer

Workflow
generator

Execution plan

Mesh

Elimination
tree

Workflow
graph

Task
scheduler

Workflow
engine

Provisioner

RabbitMQ
Task status

Task queue

Monitoring

Element
Element
Element
matrices
matrices
matrices
Global
storage

Solver
Local
service Executor storage
Executor11
Worker nodes / VMs
M

Figure 5: Workﬂow-based solver execution environment. Solver components are marked red,
Hyperﬂow components are blue and generic ones are green.

3

Prototype of the workﬂow-based multifrontal solver

To evaluate the feasibility of our approach, we have developed a prototype of the workﬂowbased solver environment. The architecture of the environment is composed of multiple looselycoupled components, as presented in Fig. 5.
Mesh generator. The mesh generator is an external component provided by the user. It
generates the mesh of rectangular elements and their corresponding matrices. The mesh may
be either 2 or 3-dimensional.
Elimination tree generator. This component is responsible for creating elimination tree
for a given mesh. In our prototype we used an existing implementation [12] which currently is
limited to 2D meshes.
Workﬂow generator. It performs analysis of the elimination tree and generates the workﬂow DAG as described in Section 2. At this stage we know the structure of the elimination tree
and derive size of the matrices for each node. This allows us to calculate the volume of data
transfers as well as the amount of computation for each workﬂow node, subsequently used by
the planner and scheduler. This component also performs task aggregation.
Solver service. The solver service is the component which performs the actual computation
of workﬂow tasks for both elimination and backward substitution phases. It executes jobs
received from the HyperFlow engine via the RabbitMQ message broker. The solver service
is hosted on the Worker nodes deployed as Virtual Machines in the Cloud. In a typical
conﬁguration, one instance of the solver service will be created for each core of a VM. As
solver services are permanently working processes, they can also act as local in-memory storage
services for intermediate data used by subsequent jobs.
Local and global storage service. The matrices of the tree nodes as well as the computation state need to be persisted between the elimination and the backward substitution
phases. We propose a two-level storage architecture: the local storage for matrices that are
of the elimination tree, can be improved by assigning several tasks to one frontal matrix, and implementing
concurrent Schur complement computations.

949

Leveraging workﬂows and clouds for a multi-frontal solver . . .

Balis, et. al

only accessed from a single worker node and the global storage for matrices that need to be
accessed by multiple worker nodes. This approach enables the scheduler to map tasks according
to data dependencies in order to maximize data locality and reduce data transfer and access
costs. To this end, the elimination and backward substitution jobs for a given tree node need
to be scheduled to same worker solver service.
Hyperﬂow infrastructure. The Hyperﬂow workﬂow management environment orchestrates and executes the tasks of the solver. The HyperFlow infrastructure is composed of multiple components: (1) Workﬂow engine which drives the workﬂow execution. It keeps track
of workﬂow tasks ready for execution using the message queue to submit jobs for such tasks
and awaiting their completion. (2) Execution planner and optimizer whose objective is to
create an execution plan for a given workﬂow DAG, infrastructure and user-deﬁned constraints,
such as deadline or cost. The generated execution plan is the input for the task scheduler and
the provisioner. (3) Task scheduler which receives jobs (representing tasks ready for execution) from the workﬂow engine and assigns them to computational resources according to the
execution plan and dynamic scheduling algorithms. (4) Provisioner, responsible for adapting
the computing infrastructure according to the execution plan and the measurements made by
the scheduler. The provisioner creates additional VM instances or shuts down existing ones,
according to the current demand for computing power.
The planner, scheduler and provisioner components are optional: they only are required
when optimization goals, such as time or budget constraints, are provided.

4

Experiments

The objective of the experiments was to test the functionality of the prototype and evaluate
the feasibility of our approach. The central question is whether it is possible to generate the
DAG of tasks in such a way that the communication cost and other overheads of the system do
not outweigh the beneﬁts of parallel execution.
The experiments were conducted in a simpliﬁed deployment of our environment, without
using the execution planner, scheduler and provisioner components. The scheduling of tasks is
done implicitly by multiple solver services fetching jobs from the message queues. This results in
eﬃcient, opportunistic load-balancing of tasks to all CPU cores available on the worker nodes.
Moreover, only a global ﬁle-based storage was used.
We performed the experiments using a number of elimination trees for various twodimensional vertex, edge and vertex+edge singularities problems with diﬀerent mesh reﬁnement
levels and polynomial degrees. The test data sets were obtained from [12]. The results indicate
that the whole process gives correct results.
To evaluate the performance of the workﬂow-based solver and the associated overheads,
we selected a medium-scale workﬂow with 12278 tasks that represented a 2-dimensional edge
singularity problem with 10-fold mesh reﬁnement and 5th-degree polynomials. The tests were
performed on a single 16-core c3.4xlarge instance on Amazon EC2 (eu-west-1 region). We
used the Hyperﬂow workﬂow engine to drive the workﬂow. Our prototype implementation uses
the ﬁle system as global storage for persisting node data. We used Linux tmpfs (a RAM based
ﬁlesystem) as a fast storage backend and RabbitMQ as the message queue.
First, we performed a series of experiments without task aggregation in order to estimate
the overhead of the workﬂow engine. The test conﬁgurations and the workﬂow execution times
are given in Table 1. In conﬁgurations (1) and (2) the entire computation was performed
sequentially without using the workﬂow engine. In scenario (2) the data of each task was
read and written to the ﬁlesystem, thus adding the IO overheads. Conﬁgurations (3) and (4)
950

Leveraging workﬂows and clouds for a multi-frontal solver . . .

#
1
2
3
4
5
6

description of conﬁguration
sequential, no Hyperﬂow, data in memory
sequential, no Hyperﬂow
sequential, with Hyperﬂow
parallel, with Hyperﬂow, 2-16 workers
no-op, with Hyperﬂow, no RabbitMQ
no-op, with Hyperﬂow, with RabbitMQ

total time (s)
7.9
11.5
53.0
47.0
29.0
47.0

Balis, et. al

time per task (ms)
0.6
0.9
4.3
3.8
2.4
3.8

Table 1: Total computation times divided by the number of tasks for diﬀerent conﬁgurations –
without task aggregation.

included the HyperFlow engine, RabbitMQ server and Solver services – a single instance in
scenario (3) and 2 to 16 instances in scenario (4).
As expected, for ﬁne-grained tasks increasing the number of workers does not reduce the
total computation time due to the domination of the communication and workﬂow orchestration overheads. For example, it takes 13 seconds for Hyperﬂow to initialize and parse a large
workﬂow ﬁle. To better measure the overheads of the workﬂow system, we executed the workﬂow in conﬁguration (5), where the communication through RabbitMQ was disabled and the
computations were replaced by empty operations (no-op). Similarly, in conﬁguration (6) the
empty operations were used, but the communication was enabled. The results conﬁrm that
overheads related to workﬂow orchestration dominate the execution time. The rightmost column in Table 1 shows the execution time divided by the number of tasks. The observed system
overhead per task (3.8 ms) is over 6 times greater than the actual computation time (0.6 ms)
which gives us a hint regarding the desired task granularity.
Next, in order to increase task granularity, we have enabled aggregation at the level of the
99th percentile (see section 2.3) which resulted in a workﬂow with 166 tasks. For such a case,
the average run time of tasks in the elimination phase increased from 1.6 ms to 43 ms, as
shown in histograms of task distributions in Fig. 6. Task run times were measured directly by
the solver service, so they do not include the overheads shown in the rightmost column of the
conﬁguration (3) in Table 1. The distribution of task execution times shows that in addition
to the majority of heavier tasks, there is also a large number of light tasks represented by the
peak in the leftmost area of both histograms. On the left histogram this peak represents the
tasks from the leaves of the elimination tree, while on the right histogram the peak represents
the non-aggregated tasks near the root of the tree (resulting from our aggregation heuristic
described in Section 2.3). The reduced size of the workﬂow resulted in a reduction of the
workﬂow orchestration overheads and the total computation time was 12.5 s, comparable to
the sequential algorithm. Moreover, we also observed speedup when increasing the number of
workers as shown in Fig. 7.

5

Conclusions and future work

Using a workﬂow management system as an execution layer of a multi-frontal direct solver can
lead to signiﬁcant beneﬁts by enabling on-demand access to non-expensive cloud computing
infrastructures. Moreover, the users will also beneﬁt in terms of usability because the workﬂow
management system hides the complexity related to interfacing the runtime environment. In
fact, in the future we are aiming towards a dedicated scientiﬁc gateway wherein end users could
focus on deﬁning the problem in a solver-as-a-service environment, while the system would
951

Leveraging workﬂows and clouds for a multi-frontal solver . . .

3000

Balis, et. al

50
40
30

Count

Count

2000

1000

0.000

0.005

0.010

20
10

0.015

0.00

0.05

Execution time (seconds)

0.10

0.15

Execution time (seconds)

13
12
11
10
9
8
7
6
5
4
3
2
1
0

4

Speedup

Execution time (seconds)

Figure 6: Task execution time histogram for eliminate tasks without (left) and with aggregation
of tasks (right).

3

2

1

0
1

2

3

4

5

6

7

8

9

10 11 12 13 14 15 16

Number of workers

(a) Execution time

1

2

3

4

5

6

7

8

9

10 11 12 13 14 15 16

Number of workers

(b) Speedup

Figure 7: Execution time (left) and speedup (right) of a workﬂow after aggregation that reduced
the number of tasks to 166.

transparently generate the appropriate mesh and workﬂow as well as allocate the necessary
computing resources, possibly from various computing infrastructures.
Finally, the results of our preliminary experiments also prove that multi-frontal solver computations in a cloud can be done eﬃciently. Although the overheads of the workﬂow execution
engine are prohibitive for ﬁne-grained tasks, we can observe that even simple aggregation leads
to task sizes for which the overheads are not dominating. We should note that these experiments were performed using the problems of relatively small size, for which the use of a cloud
infrastructure and distributed processing cannot reach its full potential. We can conclude,
however, that for more complex meshes arising from e.g. 3D problems, where the size of the
tasks and their computing costs can be orders of magnitude larger, the system overheads we
measured will be negligible. We foresee that such workﬂow-based computations may be then
deployed to various computing infrastructures, including IaaS clouds and clusters.
Our prototype is the ﬁrst step towards leveraging cloud infrastructure in scientiﬁc computations that typically require HPC environments. In the future work, we plan more experiments
on larger meshes, particular three dimensional ones for which we expect to achieve even better
computation to communication ratios due to larger frontal matrices. We also plan to improve
task aggregation by employing a two-step heuristics leveraging application-speciﬁc task clustering and application-independent graph partitioning. We are also working on an application
performance model which may be used for optimization of deployment and workﬂow scheduling.
952

Leveraging workﬂows and clouds for a multi-frontal solver . . .

Balis, et. al

Acknowledgments
This work is partially supported by the PLGrid Core project (POIG.02.03.00-00-096/10); and
by AGH University of Science and Technology, Faculty of Computer Science, Electronics and
Telecommunications, statutory project no. 11.11.230.124. K.F. work was supported by AGH
Dean’s Grant.

References
[1] H. AbouEisha, M. Moshkov, V. Calo, M. Paszynski, D. Goik, and K. Jopek. Dynamic programming
algorithm for generation of optimal elimination trees for multi-frontal direct solver over h-reﬁned
grids. Procedia Computer Science, 29:947–959, 2014.
[2] Bartosz Balis. Increasing Scientiﬁc Workﬂow Programming Productivity with HyperFlow. In
Proceedings of the 9th Workshop on Workﬂows in Support of Large-Scale Science, WORKS ’14,
pages 59–69, Piscataway, NJ, USA, 2014. IEEE Press.
[3] Claudio Canuto, M Youssuﬀ Hussaini, Alﬁo Quarteroni, and Thomas A Zang. Spectral Methods:
Fundamentals in Single Domains. Springer, 2006.
[4] Iain S Duﬀ and John K Reid. The multifrontal solution of indeﬁnite sparse symmetric linear.
ACMMathSoft, 9(3):302–325, 1983.
[5] Iain S Duﬀ and John K Reid. The multifrontal solution of unsymmetric sets of linear equations.
SIAMSciStat, 5(3):633–641, 1984.
[6] D. Goik, K. Jopek, M. Paszynski, A. Lenharth, D. Nguyen, and K. Pingali. Graph grammar
based multi-thread multi-frontal direct solver with galois scheduler. Procedia Computer Science,
29:960–969, 2014.
[7] George Karypis and Vipin Kumar. A fast and high quality multilevel scheme for partitioning
irregular graphs. SIAM J. Sci. Comput., 20(1):359–392, December 1998.
[8] Carlos Torres-Verd´ın L. Demkowicz M. Paszy´
nski, D. Pardo and V. Calo. A parallel direct solver
for the self-adaptive hp ﬁnite element method. Journal of Parallel and Distributed Computing,
70(3):270–281, 2010.
[9] Maciej Malawski, Kamil Figiela, Marian Bubak, Ewa Deelman, and Jarek Nabrzyski. Scheduling
multi-level deadline-constrained scientiﬁc workﬂows on clouds based on cost optimization. Scientiﬁc Programming (in press), 2015.
[10] Aniruddha Marathe, Rachel Harris, David K. Lowenthal, Bronis R. de Supinski, Barry Rountree,
Martin Schulz, and Xin Yuan. A comparative study of high-performance computing on the cloud.
In Proceedings of the 22Nd International Symposium on High-performance Parallel and Distributed
Computing, HPDC ’13, pages 239–250, New York, NY, USA, 2013. ACM.
[11] Pawel Obrok, Pawel Pierzchala, Arkadiusz Szymczak, and Maciej Paszynski. Graph grammarbased multi-thread multi-frontal parallel solver with trace theory-based scheduler. Procedia Computer Science, 1(1):1993–2001, 2010.
[12] A. Paszynska, M. Paszynski, K. Jopek, M. Wozniak, D. Goik, P. Gurgul, H. AbouEisha,
M. Moshkov, V. Calo, A. Lenharth, D. Nguyen, and K. Pingali. Quasi-optimal elimination trees
for 2d grids with singularities. Scientiﬁc Programming, in press., 2014.
[13] A. Paszynska, M. Paszynski, and D. Pardo. Parallel multi-frontal solver for p adaptive ﬁnite
element modeling of multi-physics computational problems. Journal of Computational Science,
1(1):48–54, 2010.
[14] Maciej Paszy´
nski and Robert Schaefer. Graph grammar-driven parallel partial diﬀerential equation
solver. Concurrency and Computation: Practise and Experience, 22(9):1063–1097, 2010.
[15] I. Taylor, E. Deelman, D. Gannon, and M. Shields (Eds.). Workﬂows for e-Science. Springer, New
York, Secaucus, NJ, USA, 2007.

953

