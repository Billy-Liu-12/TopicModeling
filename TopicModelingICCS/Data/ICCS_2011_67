Available online at www.sciencedirect.com

Procedia Computer Science 4 (2011) 2146‚Äì2155

International Conference on Computational Science, ICCS 2011, iWAPT2011

Towards a Multi-Level Cache
Performance Model for 3D Stencil Computation
Ra¬¥ul de la Cruza,1 , Mauricio Araya-Polob
a Barcelona

Supercomputing Center, Barcelona (Spain)
USA, The Woodlands, TX (USA)

b Repsol

Abstract
It is crucial to optimize stencil computations since they are the core (and most computational demanding segment)
of many ScientiÔ¨Åc Computing applications, therefore reducing overall execution time. This is not a simple task,
actually it is lengthy and tedious. It is lengthy because the large number of stencil optimizations combinations to test,
which might consume days of computing time, and the process is tedious due to the slightly different versions of code
to implement. Alternatively, models that predict performance can be built without any actual stencil execution, thus
reducing the cumbersome optimization task. Previous works have proposed cache misses and execution time models
for speciÔ¨Åc stencil optimizations. Furthermore, most of them have been designed for 2D datasets or stencil sizes that
only suit low order numerical schemes. We propose a Ô¨Çexible and accurate model for a wide range of stencil sizes up
to high order schemes, that captures the behavior of 3D stencil computations using platform parameters. The model
has been tested in a group of representative hardware architectures, using realistic dataset sizes. Our model predicts
successfully stencil execution times and cache misses. However, predictions accuracy depends on the platform, for
instance on x86 architectures prediction errors ranges between 1-20%. Therefore, the model is reliable and can help
to speed up the stencil computation optimization process. To that end, other stencil optimization techniques can be
added to this model, thus essentially providing a framework which covers most of the state-of-the-art.
Keywords: stencil computation, HPC, code optimization, homogeneous multi-core, performance model

1. Introduction
Finite Difference (FD) is a widely used method to solve Partial Differential Equations (PDE). Astrophysics [1]
and oceanography [2] are examples, among many, of scientiÔ¨Åc Ô¨Åelds where large computer simulations are frequently
deployed. These kind of simulations may consume weeks of supercomputer time. SpeciÔ¨Åcally, if they are PDE+FD
based, most of this execution time is spent on stencil computations. For instance, Reverse-Time Migration is a seismic
imaging technique in geophysics, where up to 80% of the execution time is devoted to stencil computations [3].
The main objective of this work is to improve stencil-based applications performance. Usually, high performance
is achieved by algorithmic changes that lead to suboptimal implementations, where many executions of those different implementations are required. Notice here that code modiÔ¨Åcations must not taint the numerical soundness of
Email addresses: delacruz@bsc.es (Ra¬¥ul de la Cruz), araya.mauricio@repsol.com (Mauricio Araya-Polo)
work was partially supported by project TIN2007-60625 of Spanish Government‚Äôs Science and Innovation Ministry.

1 This

1877‚Äì0509 ¬© 2011 Published by Elsevier Ltd. Open access under CC BY-NC-ND license.
Selection and/or peer-review under responsibility of Prof. Mitsuhisa Sato and Prof. Satoshi Matsuoka
doi:10.1016/j.procs.2011.04.235

Ra¬¥ul de la Cruza et al. / Procedia Computer Science 4 (2011) 2146‚Äì2155

2147

the algorithm. Thus, adding another issue to be kept into account. In this work, we deÔ¨Åne a test case as a set of
characteristics that include platform and stencil parameters, such as: stencil size, data size, algorithm implementation,
architecture, etc. Since, every one of those parameters may change among executions, the number of total test cases
is large. Therefore it is not feasible to Ô¨Ånd a possible suboptimal stencil scheme by manual means; the automatization
of this task is a must.


	
	



	





















1: for k = to Y ‚àí do
2:
for j = to X ‚àí do
3:
for i = to Z ‚àí do
4:
Xi,t j,k = C0 ‚àó Xi,t‚àí1
j,k

t‚àí1
t‚àí1
t‚àí1
t‚àí1
+CZ1 ‚àó (Xi‚àí1,
j,k + Xi+1, j,k ) + . . . + C Z ‚àó (Xi‚àí , j,k + Xi+ , j,k )
t‚àí1
t‚àí1
t‚àí1
t‚àí1
+C X1 ‚àó (Xi, j‚àí1,k + Xi, j+1,k ) + . . . + C X ‚àó (Xi, j‚àí ,k + Xi, j+ ,k )
t‚àí1
t‚àí1
t‚àí1
+CY1 ‚àó (Xi,t‚àí1
j,k‚àí1 + Xi, j,k+1 ) + . . . + C Y ‚àó (Xi, j,k‚àí + Xi, j,k+ )












Figure 1: Test cases tree, this one with a basic conÔ¨Åguration based on three main factors:
platform, stencil size and dataset size.

5:
end for
6:
end for
7: end for

Algorithm 1: The classical stencil algorithm pseudo-code. Z, X,
Y are the dimensions of the data set including ghost points. denotes the neighbors used for the central point contribution. CZ1...Z ,
C X1...X , CY1...Y are the spatial discretization coefÔ¨Åcients for each
direction and C0 for the self-contribution. Notice that in this work
the coefÔ¨Åcients are considered constant.

We identify three ways to accomplish this: by brute force, i.e. trying every possible test case (as auto-tuning), by
modeling the algorithm behavior and a hybrid approach based on the two previous ideas. The auto-tuning approach
has been widely covered in the literature [4]. The latter two are the guiding ideas followed in this work. Unfortunately,
modeling has a shortcoming, which is the limited coverage of the experimental space regarding Ô¨Åne grain test cases.
Therefore, the hybrid approach may be the ideal solution. In this scenario, the branches shown in Figure 1 are covered
by modeling, and the leaves by an auto-tuning kind of mechanism.
In this work, the model-based way of covering the experiments space is explored. We consider that it has three
advantages with respect to auto-tuning. First, there is no need for real executions of the code, which can reduce hours
or days of testing time. Second, the development of a framework to control the experiments execution is not needed.
And third, which is the most important, the model‚Äôs Ô¨Çexibility allows the extension without a substantial effort in
implementation and experimental executions. Furthermore, auto-tuning framework extensions, in particular if these
modiÔ¨Åcations highly affects the test cases tree, produce combinatorial explosion of code to be modiÔ¨Åed.
Nevertheless, the modeling approach has a main drawback: it strongly relies on the ability to capture the algorithm
behavior in an accurate fashion, which is not simple to achieve. In fact, this is the key point of this work. Thus the
main effort has focused on modeling in an efÔ¨Åcient way how stencil computations perform on different platforms.
Furthermore, in contrast to previous work [5], our model takes into account a complete memory hierarchy, from RAM
to register level, considering any intermediate cache level.
In order to achieve our goal, we have modeled the behavior of the stencil computation. After that, the model has
been implemented in a program that receives the following parameters: stencil size, architecture speciÔ¨Åcations and
dataset size. The reÔ¨Ånement and deployment of the model follow a methodology described in this work as well.
In the following subsections we provide the contextual information for this work. First, a detailed explanation of
the stencil computation itself and its challenges, and secondly, an overview of the literature about them.
1.1. Stencil Computation Overview
In this subsection, we review the stencil computation characteristics, and then its main challenges are identiÔ¨Åed.
Basically, the stencil central point accumulates the contribution of neighbor points in every axis of the cartesian system
(see Algorithm 1). This operation is repeated for every point in the computational domain, thus solving the spatial
differential operator. The domain contains interior points (inside the solution domain) and ghost points (outside the
solution domain). We identify the two main stencil computation challenges [6] as:

2148

Ra¬¥ul de la Cruza et al. / Procedia Computer Science 4 (2011) 2146‚Äì2155

‚Ä¢ The non-contiguous memory access pattern. In order to compute the central point of the stencil, a set of neighbors have to be accessed. Some of these neighbor points are spread out in the memory hierarchy, thus paying
many cycles in latencies. Furthermore, depending on the stencil order (related to the number of neighbors that
contribute to the spatial differential operator) this problem becomes even more burdensome.
‚Ä¢ The low computation/access and re-utilization ratios. After gathering a set of data points, just one central point
is computed, and only some of those accessed data points will be useful for the next central point computation.
1.2. Related Work
Several works have been published about performance and modeling of stencil computations on modern processors. For instance, Datta and Kamil [7, 5] have proposed cost models to capture the performance of a set of stencil
optimizations. Such models are based basically on the Stanza Triad micro-benchmark (STriad) derived from the
STREAM Triad benchmark [8]. Using this model, a 7-point naive stencil computation is modeled by taking into
account three types of memory access costs: Ô¨Årst, intermediate and stream access. The access cost of the Ô¨Årst nonstreamed cache line is represented by C f irst , and the streamed cache lines by C stream . Prefetched accesses are only
activated after some number of misses (denoted as k), where Cintermediate is used instead of that cost. Considering a FD
domain of L elements (where L = N 3 , being N the axis domain size) and W elements per cache line, (L/W) cache
lines must be fetched to compute the stencil. Therefore, their cost model for low-order stencils can be summarized as:
C stencil = C f irst + k ‚àó Cintermediate + ( (L/W) ‚àí k ‚àí 1) ‚àó C stream

(1)

In addition, they developed a cache blocking model for Rivera tiling [9], where an N 3 problem is traversed using
I √ó J √ó N blocks, being I and J the most unit-stride dimensions of the cut. This model was built on the top of the above
mentioned model. They set the lower and upper bounds for I √ó J √ó N blocking of a 7-point stencil computation. The
lower bound assumes high reuse (2C stencil , one plane to read and one plane to write), while the upper bound assumes
no cache reuse at all (4C stencil , three planes to read and one plane to write). Given an N 3 grid problem, the number
of non-streamed (T f irst ), partially streamed (T intermediate ) and streamed (T streamed ) cache lines are computed as: T f irst =
2
N3
N3
J, or NI J if I = J = N, T intermediate = T f irst ‚àó (k ‚àí 1) and T stream = T total ‚àí T intermediate ‚àí T f irst ,
I if I N, or I J if I = N
N3
where T total = (I/W)
. Consequently, the cost of sweeping a 3D domain in a blocked fashion is approximated as:
I
C stencil = C f irst T f irst + Cintermediate T intermediate + C stream T stream

(2)

Lately, Datta et al. [5] have extended their model to time-skewing stencil computations. They distinguished mainly
Ô¨Åve cases of cache misses: three preferred cases with compulsory misses and other two cases with capacity misses.
Kamil, Datta et al. set the basis for modeling stencil computations, e.g. streamed and non-streamed data for Rivera
blocking. However, there are at least two issues which were not considered in their works. First, they are taking into
account only low-order 3D stencil computations (7-point or 27-point in a compact fashion). Nevertheless, in many
scientiÔ¨Åc computing applications high-order stencils are required, from 25-point upward [10] to 85-points and more.
We believe that a parametrical model supporting different stencil order sizes is advisable for future research. Second,
the memory model is approached as an uniÔ¨Åed homogeneous hierarchy, leaving aside multi-level cache peculiarities
of modern architectures. Further, the number of available registers in the ALU or the different cost between load
and store operations are not considered in their model. We believe that those aspects must be taken into account in
order to lay down a Ô¨Çexible, accurate and reliable model suitable for any kind of stencil computation and architecture.
However, as far as we know, no stencil computation model with such characteristics can be found in the literature,
which motived us to develop the current work.
The remainder of this paper is organized as follows: Section 2 introduces the stencil computation model, its
internals, features and considerations. In Section 3, we evaluate the performance and the model validity over different
current architectures. Finally, Section 4 presents our conclusions.
2. Modeling Stencil Computation
In this section, we present the basis for a Ô¨Çexible and accurate model suitable for a wide range of stencil computations and architectures. As mentioned before, the multi-level cache hierarchy has to be considered to predict properly

Ra¬¥ul de la Cruza et al. / Procedia Computer Science 4 (2011) 2146‚Äì2155

2149

the behavior of stencil computations in current architectures.
It is a widely accepted fact that the stencil computation is memory bound [5, 6]. Therefore, only memory transactions between the CPU and the memory subsystem are contemplated, and computation bottlenecks are supposed to be
negligible. In other words, the execution of the pipeline is dominated by the data transfer latency. Also, interferences
between data and instruction at cache level are not taken into account. On the other hand, due to possible register
spilling, our model estimates the number of loads and stores in the Ô¨Årst cache level (L1).
This section proceeds as follows: Ô¨Årstly, we review the base model. Secondly, the number of read and write misses
in stencil computations are approximated. And Ô¨Ånally, some peculiarities, like prefetching, are added to the model.
Software prefetching is considered as an extension to the model, as many other user driven possible optimizations,
and therefore it is not covered in this work, but it is part of our future work plan.
2.1. Base Model
Given a grid of N 3 words (element precision, either single or double) to traverse, we can set the fundamental
parameters involved in stencil computations. Consider the grid dimensions as I = J = K = N for a naive implementation,
where I is the unit-stride dimension and J and K are the least stride dimensions excluding ghost points. A 3D stencil
computation of order requires 2 ‚àó + 1 Z-X input planes in order to compute one Z-X output plane [6]. The size of
these planes (in words) differs depending on the operation to perform (read or write). Thus, we proceed to calculate
the memory requirements to compute one k iteration of the sweep (outermost loop of Algorithm 1). Let P be the
number of planes and S the plane size in words, the following terms for read and write operations are obtained,
Pread = 2 ‚àó + 1

Pwrite = 1

S read = (I + 2 ‚àó ) ‚àó (J + 2 ‚àó )

S write = I ‚àó J

(3)

The stencil computation involves back and forth dataset transfers, which may produce conÔ¨Çicts depending on
cache policies because the memory resources are shared. For instance, during a write operation, two types of write
policies can be enforced for a cache miss: write-through and write-back. Considering the memory hierarchy of
our testbed architectures (described in Section 3.1.3), the former does not produce a cache line allocation (no write
allocate). On the contrary, the latter produces cache line allocation (write allocate), thus sharing memory resources
among input data and leading to cache pollution. Therefore, considering the above described policies, the words kept
by the memory subsystem to compute a Z-X plane (one k iteration) are:
Pwriteback
= Pread + Pwrite
total
writeback
S total

= Pread ‚àó S read + Pwrite ‚àó S write

writethrough

Ptotal

= Pread

writethrough
S total

= Pread ‚àó S read

(4)

Given an architecture with a memory hierarchy of n cache levels, it can be assumed that the total time (T total )
required to compute a stencil is based on the following formula,
T total = T L1 + ¬∑ ¬∑ ¬∑ + T Li + ¬∑ ¬∑ ¬∑ + T Ln + T Memory

(5)

where T Li and T Memory are the time to access data in Li cache level and main memory respectively. In order to
estimate T Li and T Memory , the number of cache line hits (HitsLi ) and cache line misses (MissesLi ) must be calculated.
Depending on the level, the way to compute hits and misses differ due to hardware policies. Regarding this aspect,
three memory hierarchy groups have been established to calculate hits and misses: Ô¨Årst (L1), intermediate (from L2
to Ln) and last (Memory). When the CPU issues a word load instruction, the data is brought from the closer cache
level (L1). If the data is not present, a miss is Ô¨Çagged and passed to the next level of the hierarchy, which will Ô¨Ånally
fetch an entire cache line containing the data. The remaining levels also follow this mechanism.
Recall that a stencil computation, as shown in Algorithm 1, requires several values in the internal loop to compute
a single point: grid points (X t‚àí1 ), weights (CZ,X,Y,0 ), and indices (i, j, k) to access grid points. In the CPU register bank,
grid points and weights are kept in Floating-Point Registers (FPR), while indices use General-Purpose Registers
(GPR). Leaving aside compiler optimizations, grid points must be fetched in every sweep of the loop, whereas
weights and indices might be partially reused. However, note that depending on: the order of the stencil ( ), the
dimension of the problem (dim) and the available registers (FPR f ree and GPR f ree ), data reuse becomes burdensome,
leading to register spilling and a higher CPU-L1 trafÔ¨Åc.
After deep analysis of the assembly stencil code on our testbed architectures, a couple of assertions can be pointed
out. First, the required ‚àó dim + 1 weights are reused along the sweep if enough FPR f ree registers are available.

2150

Ra¬¥ul de la Cruza et al. / Procedia Computer Science 4 (2011) 2146‚Äì2155

Second, the compiler keeps one index register for each grid point, whereas those points along Z axis share the same
index ((2 ‚àó ‚àó (dim ‚àí 1)) + 1). Likewise, index registers are also reused whether enough GPR f ree resources exist.
As shown, the L1 cost is difÔ¨Åcult to predict accurately, and some extra information about how registers are used is
required. In order to obtain such information in the Ô¨Årst level, we proceed to estimate the register pressure (Reggrid ,
Regweight and Regindex ) to compute each point of the domain (I √ó J √ó K) as,
Reggrid = (2 ‚àó ‚àó dim) + 1

word
T L1
= word/Bwread
L1

Regweight = max(( ‚àó dim + 1) ‚àí FPR f ree , 0)

word
Hitsword
L1 = ((Reggrid + Regweight + Regindex ) ‚àó I ‚àó J ‚àó K) ‚àí MissesL1

Regindex = max(((2 ‚àó ‚àó (dim ‚àí 1)) + 1) ‚àí GPR f ree , 0)

(6)

word
T L1 = Hitsword
L1 ‚àó T L1

where hits (Hitsword
L1 ) are estimated by the difference between loads issued by CPU ((Reggrid + Regweight + Regindex ) ‚àó
read
I ‚àó J ‚àó K) and misses triggered to the Ô¨Årst hierarchy (Missesword
L1 ). Let word be the data granularity and BwL1 the L1
word
read bandwidth, the L1 cost (T L1 ) is calculated as the cost of transferring a word from L1 to CPU (T L1 ) times the
hits in L1 (Hitsword
L1 ). Following a similar scheme, the cost for any intermediate cache level can be approximated as,
cacheline
T Li
= cacheline/Bwread
Li

Hitscacheline
= Missescacheline
‚àí Missescacheline
Li
Li
Li‚àí1
cacheline
T Li = Hitscacheline
‚àó T Li
Li

(7)

where the number of hits (Hitscacheline
) depends on misses issued in the previous cache level (Li‚àí1) and misses Ô¨Çagged
Li
in the current one (Li). Then, the time cost in level i can be estimated straightforwardly as the time required to transfer
cacheline
a cache line (data granularity) from level i (T Li
) times the number of transfers performed (Hitscacheline
). Finally,
Li
the accessing data cost for the last level (main memory) can be computed with the following equations,
cacheline
T Memory
= cacheline/Bwread
Memory

cacheline
Hitscacheline
Memory = MissesLn
cacheline
T Memory = Hitscacheline
Memory ‚àó T Memory

(8)

Given that data is allocated in the last hierarchy level, any miss issued by previous level (Ln) will generate necessarily a hit in main memory (misses are not possible). In other words, the access cost (T Memory ) is directly based on
cacheline
misses issued by the previous level (Missescacheline
) times the bandwidth (T Memory
) to access the data in such level.
Ln
2.2. Read Misses
Once the base model has been set up for the memory hierarchy, the different cases to be predicted are explored.
We will treat read misses because write misses have a negligible inÔ¨Çuence on the total miss estimation. Our model
covers four cases of miss estimation (see Figure 2), where these cases depend on three distinct types of cache misses
(compulsory, capacity and conÔ¨Çict misses) [11]. Ordered from lower to highest penalty, the four cases are:
writepolicy
1. Best possible scenario (lower bound) is obtained when all the required Z-X planes (S total
) for one k iteration
Ô¨Åt entirely in Li cache. Thus, one plane (nplanesLi ) per k iteration is fetched. In this case, only compulsory
misses (cold-start misses) are Ô¨Çagged. Cold misses arise due to loads of new data which is not in the hierarchy.
Let II, JJ and KK be the extended dimensions (including ghost points) of I, J and K , and let W be the words
per cache line (W = cacheline/word), the misses of Li are,

Missescacheline
= II/W ‚àó JJ ‚àó KK ‚àó nplanesLi where nplanesLi = 1
Li

(9)

2. The second scenario occurs when all the required planes do not Ô¨Åt in Li, leading to capacity misses. However,
we assume that k-central plane, with a higher temporal reuse than the rest (1 vs 2 ‚àó + 1), is probably not
replaced from cache due to conÔ¨Çict misses. In other words, k-central plane, when it Ô¨Åts in cache, might be
partially reused in the following iterations, giving nplanesLi = (Pread ‚àí 1) for Equation 9.
writepolicy
) do not Ô¨Åt in Li, as on the previous one.
3. On the third scenario, we also assume that all the planes (S total
Nevertheless in this case, the k-central plane overwhelms a signiÔ¨Åcant part of the Li cache (say half of the
capacity), therefore reducing the possibility of temporal reuse. Therefore, nplanesLi = Pread planes must be
read for Equation 9 when traversing the Y axis of the stencil loop.
4. Finally, in the worst scenario (upper bound) neither the planes nor the columns of the k-central plane Ô¨Åt in the
cache. In this case, due to capacity and conÔ¨Çict misses, we consider that all the planes must be read at every k
iteration (Pread ), but the columns of the k-central plane must be read at each j iteration (Pread ‚àí 1) as well. This
scenario gives nplanesLi = (2 ‚àó Pread ‚àí 1) for Equation 9.

2151

Ra¬¥ul de la Cruza et al. / Procedia Computer Science 4 (2011) 2146‚Äì2155









	





















	









	














	

	

	















































	










Figure 2: Cases considered for read misses during k iteration. Each plane color represents the misses generated when
accessing the plane. A light color means none or few misses and a dark color implies a high ratio of misses.
2.3. Prefetching
Hardware prefetching is a common feature in modern architectures. It enables a reduction of cache misses, thus
increasing the hit ratio and minimizing latency. Hardware prefetching, also known as streaming in the literature,
consists of prefetching data from main memory into cache before it is explicitly requested. As processors have
become faster, and memory latencies have risen, the signiÔ¨Åcance of prefetching has increased. Besides, most modern
architectures support hardware prefetching of streams in different levels of the hierarchy simultaneously. Hence, it
is necessary to include prefetching so that predict stencil behaviour in current architectures. Dick [12] proposes a
simple prefetching model. Unfortunately, this model computes cost in cycles per memory access, and therefore it is
not compatible with our model, which is based on hit and miss metrics.
The prefetching modeling is complex, in particular to Ô¨Ågure out if a prefetched stream Ô¨Çags a miss. We devise a
simple approach, misses of the model are divided into two groups: prefetched and non-prefetched. First, we calculate
) in the cache level i,
the X-Y planes (nplanesSLitream ) that can be prefetched and not prefetched (nplanesNon‚àístream
Li
Non-S tream
nplanesLi
= max(nplanesLi ‚àí pre fLi , 0)
cacheline
T Li
= cacheline/Bwread
Li

Missescacheline
Li

= II/W ‚àó JJ ‚àó KK

- stream
nplanesSLitream = nplanesLi ‚àí nplanesNon
Li
T -S treamcacheline
= cacheline/Bw-S treamread
Li
Li

Non- stream
‚àó nplanesLi

Misses-S treamcacheline
Li

= II/W ‚àó JJ ‚àó KK

Hitscacheline
= Missescacheline
‚àí Missescacheline
Li
Li
Li‚àí1
Hits-S treamcacheline
= Misses-S treamcacheline
‚àí Misses-S treamcacheline
Li
Li
Li‚àí1

(10)

‚àó nplanesSLitream

cacheline
cacheline
T Li = Hitscacheline
‚àó T Li
+ Hits-S treamcacheline
‚àó T Li
Li
Li

(11)

where pre fLi refers to the number of stream channels supported by the current hierarchy level i. Second, prefetched
and non-prefetched cache line misses within planes are computed. Finally, hits are obtained for the next level (i +
cacheline
for non-prefetched hits and T -S treamcacheline
for prefetched hits. The way to
1) using their transfer cost, T Li
Li
compute the speciÔ¨Åc bandwidths for streamed and non-streamed planes is shown in Subsection 3.1.3.
3. Experiments and Model Validation
In this section the experimental results are presented to validate the model. To that end the predicted execution
times (or cache misses) are compared with real execution results (or HW/SW counted cache misses), and their relative
error are estimated. In order to achieve that, we have followed a methodology that is introduced in this section.
3.1. Methodology
Basically, our experimental methodology consists in the following Ô¨Åve stages:
Stage 1: Some representatives test cases are selected within the experimental space.
Stage 2: Test cases selected in Stage 1 are executed and the results are stored for further use.
Stage 3: Information regarding platform conÔ¨Åguration is gathered, for instance cacheline size, prefetching options or
streamed and non-streamed bandwidth. Some of them, for instance bandwidth, is obtained executing benchmarking programs (see Section 3.1.3). The rest of the information is obtained reviewing the manufacturer‚Äôs
datasheets. Afterwards, the consolidated information is added to the model through conÔ¨Åguration Ô¨Åles.
Stage 4: Predicted performance is obtained by using the model for test cases selected in Stage 1.
Stage 5: Finally, real execution (Stage 2) and model generated (Stage 4) performance data are compared, obtaining
relative errors.

2152

Ra¬¥ul de la Cruza et al. / Procedia Computer Science 4 (2011) 2146‚Äì2155

The methodology has a feedback mechanism, where model parameters can be adjusted (Stage 3) and experiments
repeated if the relative error (computed in Stage 5) is out of acceptable correctness range. In general terms, once
the model parameters are compiled from platform characteristics, the model should return accurate results. However,
it may occur that some characteristics are not clearly speciÔ¨Åed for the platform (e.g. streaming channels through
memory hierarchy), then a process of reÔ¨Ånement must be performed. This platform parameters reÔ¨Ånement process
follows Stages 3-4-5 as many times as it is needed.
In the following subsections we describe the main features of each methodology stage.
3.1.1. Stage 1: Test-cases Parameters
The test cases space is a combination of: stencil size, platform and dataset dimension. The stencil sizes of the
experiments are: 13 (academic), 25 (widely used), 43 and 85 (high order) points. We will carry out experiments for
three platforms. For the sake of simplicity we have collapsed the 3D dataset dimension to one parameter, since the
datasets are cubic. The dataset size ranges from 128 to 512 points cubic, with a 16 points stride. In total, we have
288 test case results to analyze, but before proceeding to that, the following two subsections describe the testbed
conÔ¨Åguration and the experimental setup used.
3.1.2. Stage 2: Real Execution Performance
Two sets of real data have been collected for the validation step: the real execution times and the software/hardware
counters (cache misses), when they are available by the platform. The results are presented combined with model
results in Subsection 3.1.4. In order to gather the results, in particular those related to cache misses, the following
tools (or frameworks) have been deployed:
PapiEx (PAPI): PapiEx is a performance analysis tool designed to measure transparently and passively the hardware
performance counters of an application using PAPI framework.
hpmcount/libhpm (HPCT/HPM): hpmcount and libhpm provide hardware metric and resource utilization statistics
after application execution. They are developed by IBM to support Power-based systems.
Valgrind: Valgrind is an instrumentation framework for building dynamic analysis tools. It contains a set of tools to
perform debugging and proÔ¨Åling.
The Ô¨Årst two items of the above list provide hardware counter information. The information gathered is used as the
reference data in validation. The last item is a cache simulator, it roughly captures the platform behavior by software
counters, notice that prefetching is not considered by that tool. Although that tool do not capture streaming features,
it is useful in terms of analysis to proÔ¨Åle memory instructions issued by CPU. Therefore, the Valgrind cache simulator
allows to partially validate the non-streaming version of our model.
3.1.3. Stage 3: Model Parameters and Architectural Characterization
In this section, Ô¨Årst, the platforms used to carry out the experiments are described. Finally, the tools employed to
obtain the platform proÔ¨Åle information are introduced. We have used the following platforms available in the PRACE 2
project (detailed platform characteristics are presented in Table 1):
AMD Opteron: Louhi is a Cray XT4/XT5 supercomputer with 1012 XT4 and 672/180 XT5 compute nodes, hosted
at the IT Center for Science in Finland.
Intel Nehalem: Inti supercomputer, hosted at the Atomic Energy Commission, sports 256 Nehalem processors.
IBM Blue Gene/P: Jugene supercomputer, hosted at the Julich Research Centre, is based on the Blue Gene/P processor with 294912 PowerPC 450 cores.
In order to obtain memory bandwidth measures on the testbed architectures, we have used STREAM2 3 , which
aims to extend the functionality of the STREAM [8] benchmark in two aspects. First, it measures sustained bandwidth, providing accurate information across the memory hierarchy. Second, STREAM2 exposes more clearly the
2 Partnership

for Advanced Computing in Europe - http://www.prace-project.eu/

3 http://www.cs.virginia.edu/stream/stream2

2153

Ra¬¥ul de la Cruza et al. / Procedia Computer Science 4 (2011) 2146‚Äì2155
System name
Architecture
Chips √ó Cores
Clock
SP GFlops‚Ä†
DP GFlops‚Ä†
L1 Cache (D+I)
L2 Cache
L3 Cache
Main memory
Bandwidth
Watts x hour
Compiler

Inti
Intel Xeon Core i7
X5570 (Nehalem-EP)

Jugene
IBM BlueGene/P
PowerPC450

Louhi
AMD Opteron
2360/8360 (Barcelona)

2√ó4
2.93 GHz
93.76 (SSE2)
46.88
32 kB + 32 kB
256 kB per core
8 MB (shared)
24 GB
32 GB/s
95
Intel Compiler
(v11.1)

1√ó4
850 MHz
13.6
13.6
32 kB + 32 kB
1920 B per core
2 x 4 MB (shared)
2 GB
13.6 GB/s
39
IBM XL Compiler
(v9.0/v11.01)

1√ó4
2.3 GHz
73.6 (SSE2)
36.8
64 kB + 64 kB
512 kB per core
2048 kB (shared)
8 GB
42.7 GB/s
75
Portland/GCC Compiler
(v10.2/v4.1.2)

Table 1: Architectural conÔ¨Åguration of the platforms used in our tests. ‚Ä† Only one core is considered.
performance differences between reads and writes. It is based on the same ideas as STREAM, but a different set
of vector kernels are used: FILL (8 bytes/write), COPY (16 bytes/read&write), DAXPY (24 bytes/read&write) and
DOT (16 bytes/read). Recall that our model requires isolated read and write bandwidths for all cache hierarchy levels.
Thus, we have deployed DOT and FILL kernels. Furthermore, to obtain the necessary non-streamed bandwidths for
our model, we have extended STREAM2 to provide such information. DOT and FILL kernels have been modiÔ¨Åed
in such a way that the stride parameter (o f f set) is carefully set for consecutive reads and writes. Also, most modern
architectures support next-stride cacheline prefetching. Thus, we have used a combination of two techniques in DOT
and FILL kernels to avoid triggering the prefetching engine. The stride parameter has been redeÔ¨Åned as,
o f f set = stride + MOD(I, epsilon)

(12)

where stride is a constant large enough to avoid next cacheline prefetching mechanism. Besides, the second term
MOD(I, epsilon), which is a cyclic variable that depends on the I iteration of the loop and epsilon constant, prevents
stride cacheline prefetching engine to be triggered.
Table 2 shows detailed streamed and non-streamed bandwidths measures for the testbed platforms. The different
cache level bandwidths can be deduced from analyzing STREAM2 output (see Figure 3) and focusing in the areas
where bandwidth drops substantially (due to cache level overÔ¨Çow) when vector size increases (X axis). Every step in
Figure 3 represents the sustained bandwidth for each memory level in the hierarchy.

Register
availability‚Ä†
Block size
Prefetching
channels
Cache
capacity

Measured
Bandwidth‚Ä°
(GB/s)

GPR f ree
FPR f ree
cacheline
pre fL1
pre fL2
pre fL3
sizeL1
sizeL2
sizeL3 ‚àó
Bwread
L1
Bwread
L2
Bwread
L3
Bwread
Memory
write
BwL1
write
Bw Memory

Intel
Nehalem

IBM
BlueGene/P

AMD
Opteron

10
12
64 bytes
2
2
0
32 kB
256 kB
8 MB
49.4 (23.4)
29.4 (12.5)
21.1 (8.1)
8.2 (3.5)
49.4 (24.6)
7.9 (3.9)

26
26
32/128 bytes
0
7
0
32 kB
1920 bytes
8 MB
6.2 (3.2)
2.1 (1.3)
2.0 (0.6)
2.0 (0.6)
3.3 (3.3)
3.3 (1.9)

10
12
64 bytes
2
0
0
64 kB
512 kB
2 MB
29.1 (14.6)
13.9 (4.1)
7.6 (2.2)
3.3 (1.3)
29.9 (8.6)
4.9 (0.8)

Intel Nehalem (Inti)
Read Stream
Write Stream
Read Non-stream
Write Non-stream

50

40

30
GB/s

Parameters

20

10

0

Table 2: Parameters used in our model to predict stencil
executions on each platform. ‚Ä† The available registers for
each architecture have been estimated doing assembly register analysis of a naive case. ‚Ä° Bandwidth measured using
our STREAM2 version, which is able to capture streaming
and non-streaming bandwidths (shown in parenthesis). ‚àó L3
cache is shared among the cores.

1000

10000
100000
Size in words

1e+06

Figure 3: STREAM2 results of stream and non-stream
bandwidths for read and write operations for the hierarchy levels in Inti platform (Intel Xeon). Several compiler
Ô¨Çags were tested to obtain the maximum bandwidth performance.

2154

Ra¬¥ul de la Cruza et al. / Procedia Computer Science 4 (2011) 2146‚Äì2155

Going through the results, we can observe that almost all the platforms show an impressive beneÔ¨Åt when using
prefetching engines. As the manufacturer‚Äôs datasheet indicates [13], the IBM architecture implement write-through
policy (no write allocation) in their L1 data caches. This matches the STREAM2 results, as it should be expected the
performance remains constant between L1-L3 caches.
Finally, STREAM2 gives a better performance for write than read operations in Intel Nehalem architecture. This
reveals a problem with the binary code generated by the compiler for STREAM2. As Molka [14] reports, the write
and read operations should be similar, therefore same read and write bandwidths were used in these cases.

Cache missess [s]

3.1.4. Stage 4 & 5: Model Performance and Validation
In order to visualize and analyze the amount of data at hand, Ô¨Årstly we review the results for an interesting case,
secondly we summarize the platforms results in Table 3. In Figure 4.1, we can observe a clear pattern, the predicted
curve of cache misses separates from real and simulated curves, the latter two following a similar path. Even so, the
relative error respect to the predicted results stays between 10 to 20% (see Figure 4.2).
Figure 4.3 depicts a different scenario, the real execution time and the model curve follow a closer path, where
relative error remains under 12%. The reason for this behavior is that although the model computes execution time
based on cache misses and latencies, it captures correctly the effect of the streaming mechanism giving a different
penalty cost to prefeteched and non-prefeteched cache misses (see Subsection 2.3). Therefore, the execution time
prediction is more precise than the cache misses prediction. Also, the stencil size for those experiments is 85-point,
which implies that the time prediction is only slightly perturbed by the prefetching mechanism.
6e+08
5e+08
4e+08
3e+08
2e+08
1e+08
0
100

predicted L2 cache misses
HW counted L2 cache misses
Valgrind simulated L2 cache misses

150

200

250

300

350

400

450

500

550

500

550

Error [%]

Dataset sizes [points]
50
45
40
35
30
25
20
15
10
5
0
100

relative error (predicted vs HW counted) L2 cache misses
relative error (predicted vs simulated) L2 cache misses

150

200

250

300

350

400

450

50
45
40
35
30
25
20
15
10
5
0
100

15

predicted execution time
measured execution time
relative error

12
9
6
3

150

200

250

300

350

400

450

500

Error [%]

Time [s]

Dataset sizes [points]

0
550

Dataset sizes [points]

Figure 4: SW/HW counters and model results (the top two graphs shows misses, the bottom graph represents executions time) for Lohui (AMD) system, with 85-point stencil. In terms of L2 cache misses, the relative error (the middle
graph) stays under 15% for the most relevant part of the graph. This is important since realistic 3D datasets in industry
and large scientiÔ¨Åc simulations are usually bigger than the datasets covered in this work.
Further works will include an extended robustness analysis. In particular, we will break down the factors contribution of the modeled performance for the carried out experiments.

2155

Ra¬¥ul de la Cruza et al. / Procedia Computer Science 4 (2011) 2146‚Äì2155
Platforms
Stencil sizes (points)
Max
Min
Average
Standard deviation

13
37.2
3.5
13.0
16.4

AMD Opteron
25
43
37.0
1.8
11.6
18.7

18.2
1.8
8.5
3.7

85

13

13.4
0.5
5.8
2.7

17.2
2.4
11.1
6.8

IBM BlueGene/P
25
43
85
24.5
0.6
17.1
7.5

27.9
8.7
21.1
10.8

16.8
1.0
10.8
10.1

13
46.2
0.4
9.9
0.5

Intel Nehalem
25
43
28.9
0.6
11.9
7.1

38.2
0.6
22.1
4.9

85
20.0
2.4
12.5
0.5

Table 3: Relative errors (%) between predicted and measured execution times. The model is highly accurate for AMD
architecture, where the average relative error is 9.7% for all stencil sizes. In terms of relative error standard deviation,
the model is most effective for Intel architecture with an average of 3.5%. Overall, considering both accuracy and
stability of the results, the model results for AMD outperform the ones for Intel followed by IBM closely.
4. Conclusions and Future Work
In general terms, the proposed model captures the performance behavior of the stencil computation. The accuracy
of the prediction ranges regarding the platform. In x86 architectures case, on which we have spent most of our research
time, a high level of prediction accuracy is obtained. Furthermore, the average relative error in execution time is 13%
for all stencil sizes. These results show that we are in the right path to achieve a dependable multi-level cache model,
which at the same time remains simple, Ô¨Çexible and extensible.
Our future work can be summarized in the following three lines. First, improve the model for +90% accuracy, enhancing mainly the prefetching equations. Second, add support for multi-core architectures to the model.
Third, extend the model to capture stencil optimizations performance, particularly spatial/temporal blocking, software prefetching and semi-stencil algorithm. The latter line of research is the one that motivates us the most, because
those techniques contribute to achieving highly optimized stencil-based applications.
The Ô¨Ånal goal is not only to have an accurate and Ô¨Çexible model, but also to have the chance of freely changing
platform parameters. The latter will be helpful to forecast performance for current platform modiÔ¨Åcations as well
as for future platforms. Finally, the model is integrable with auto-tuning techniques, thus providing a rich synergy
towards efÔ¨Åcient stencil codes implementation.
References
[1] A. Brandenburg, Computational aspects of astrophysical MHD and turbulence, Vol. 9, CRC, 2003.
[2] C. D. Groot-Hedlin, A Ô¨Ånite difference solution to the helmholtz equation in a radially symmetric waveguide: Application to near-source
scattering in ocean acoustics, Journal of Computational Acoustics 16 (2008) 447‚Äì464.
[3] M. Araya-Polo, F. Rubio, M. Hanzich, R. de la Cruz, J. M. Cela, D. P. Scarpazza, 3D seismic imaging through reverse-time migration on
homogeneous and heterogeneous multi-core processors, ScientiÔ¨Åc Programming: Special Issue on the Cell Processor 16.
[4] K. Datta, M. Murphy, V. Volkov, S. Williams, J. Carter, L. Oliker, D. Patterson, J. Shalf, K. Yelick, Stencil computation optimization and
auto-tuning on state-of-the-art multicore architectures, in: SC ‚Äô08: Proceedings of the 2008 ACM/IEEE conference on Supercomputing, IEEE
Press, Piscataway, NJ, USA, 2008, pp. 1‚Äì12.
[5] K. Datta, S. Kamil, S. Williams, L. Oliker, J. Shalf, K. Yelick, Optimization and performance modeling of stencil computations on modern
microprocessors, SIAM Rev. 51 (1) (2009) 129‚Äì159.
[6] R. de la Cruz, M. Araya-Polo, J. M. Cela, Introducing the semi-stencil algorithm, 8th International Conference on Parallel Processing and
Applied Mathematics.
[7] S. Kamil, P. Husbands, L. Oliker, J. Shalf, K. Yelick, Impact of modern memory subsystems on cache optimizations for stencil computations,
in: MSP ‚Äô05: Proceedings of the 2005 workshop on Memory system performance, ACM Press, New York, NY, USA, 2005, pp. 36‚Äì43.
[8] J. D. McCalpin, Stream: Sustainable memory bandwidth in high performance computers, Tech. rep., University of Virginia, Charlottesville,
Virginia, a continually updated technical report. http://www.cs.virginia.edu/stream/ (1991-2007).
[9] G. Rivera, C. W. Tseng, Tiling optimizations for 3D scientiÔ¨Åc computations, in: Proc. ACM/IEEE SC 2000, 2000, p. 32.
[10] L. Peng, R. Seymour, K. ichi Nomura, R. K. Kalia, A. Nakano, P. Vashishta, A. Loddoch, M. Netzband, W. R. Volz, C. C. Wong, High-order
stencil computations on multicore clusters, in: IPDPS ‚Äô09: Proceedings of the 2009 IEEE International Symposium on Parallel&Distributed
Processing, IEEE Computer Society, Washington, DC, USA, 2009, pp. 1‚Äì11.
[11] O. Temam, C. Fricker, W. Jalby, Cache interference phenomena, in: SIGMETRICS ‚Äô94: Proceedings of the 1994 ACM SIGMETRICS
conference on Measurement and modeling of computer systems, ACM, New York, NY, USA, 1994, pp. 261‚Äì271.
[12] K. Dick, A mathematical model of hardware prefetching (2007).
[13] I. J. of Research, Development, Overview of the ibm blue gene/p project, IBM J. Res. Dev. 52 (1/2) (2008) 199‚Äì220.
[14] D. Molka, D. Hackenberg, R. Schone, M. S. Muller, Memory performance and cache coherency effects on an intel nehalem multiprocessor
system, in: PACT ‚Äô09, IEEE Computer Society, Washington, DC, USA, 2009, pp. 261‚Äì270.

