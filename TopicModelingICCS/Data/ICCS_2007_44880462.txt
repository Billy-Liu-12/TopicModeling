An Experiment with Fuzzy Sets in Data Mining
David L. Olson1, Helen Moshkovich2, and Alexander Mechitov
1

University of Nebraska, Department of Management, Lincoln, NE USA 68588-0491
dolson3@unl.edu
2
Montevallo University, Comer Hall, Montevallo, AL USA 35115
MoshHM@montevallo.edu, Mechitov@montevallo.edu

Abstract. Fuzzy modeling provides a very useful tool to deal with human
vagueness in describing scales of value. This study examines the relative error
in decision tree models applied to a real set of credit card data used in the
literature, comparing crisp models with fuzzy decision trees as applied by See5,
and as obtained by categorization of data. The impact of ordinal data is also
tested. Modifying continuous data was expected to degrade model accuracy, but
was expected to be more robust with respect to human understanding. The
degree of accuracy lost by See5 fuzzification was minimal (in fact more
accurate in terms of total error), although bad error was worse. Categorization
of data yielded greater inaccuracy. However, both treatments are still useful if
they better reflect human understanding. An additional conclusion is that when
categorizing data, care should be taken in setting categorical limits.
Keywords: Decision tree rules, fuzzy data, ordinal data.

1 Introduction
Classification tasks in business applications may be viewed as tasks with classes
reflecting the levels of the same property. Evaluating creditworthiness of clients is
rather often measured on an ordinal level as, e.g., {excellent}, {good}, {acceptable},
or {poor} (Ben David et al., 1989.) Applicants for a job are divided into accepted and
rejected, but sometimes there may be also a pool of applicants left for further analysis
as they may be accepted in some circumstances [2], [11]. Different cars may be
divided into groups {very good}, {good}, {acceptable}, {unacceptable}. This type of
tasks is called “ordinal classification” [5]. The peculiarity of the ordinal classification
is that data items with {better} qualities (characteristics) logically are to be presented
in {better} classes: the better the article in its characteristics the closer it is to the class
{accepted}. It was shown in[6] that taking into account possible ordinal dependence
between attribute values and final classes may lead to a smaller number of rules with
the same accuracy and enable the system to extend obtained rules to instances not
presented in the training data set.
There are many data mining tools available, to cluster data, to help analysts find
patterns, to find association rules. The majority of data mining approaches to
classification tasks, work with numerical and categorical information. Not many data
mining techniques take into account ordinal data features.
Y. Shi et al. (Eds.): ICCS 2007, Part II, LNCS 4488, pp. 462–469, 2007.
© Springer-Verlag Berlin Heidelberg 2007

An Experiment with Fuzzy Sets in Data Mining

463

Real-world application is full of vagueness and uncertainty. Several theories on
managing uncertainty and imprecision have been advanced, to include fuzzy set
theory [13], probability theory [8], rough set theory [7] and set pair theory [14], [15].
Fuzzy set theory is used more than the others because of its simplicity and similarity
to human reasoning. Although there is a wide variety of different approaches within
this field, many view advantages of fuzzy approach in data mining as an …”interface
between a numerical scale and a symbolic scale which is usually composed of
linguistic terms” [4].
Fuzzy association rules described in linguistic terms help increase the flexibility
for supporting users in making decisions. Fuzzy set theory is being used more and
more frequently in intelligent systems. A fuzzy set A in universe U is defined
as A = {(x, μ A ( x)) | x ∈U , μ A ( x) ∈[0,1]} , where
indicating the degree of membership of

μ A (x)

is a membership function

x to A . The greater the value of μ A ( x) , the

more x belongs to A . Fuzzy sets can also be thought of as an extension of the
traditional crisp sets and categorical/ordinal scales, in which each element is either in
the set or not in the set (a membership function of either 1 or 0.)
Fuzzy set theory in its many manifestations (interval-valued fuzzy sets, vague sets,
grey-related analysis, rough set theory, etc.) is highly appropriate for dealing with the
masses of data available. This paper will review some of the general developments of
fuzzy sets in data mining, with the intent of seeing some of the applications in which
they have played a role in advancing the use of data mining in many fields. It will
then review the use of fuzzy sets in two data mining software products, and
demonstrate the use of data mining in an ordinal classification task. The results will
be analyzed through comparison with the ordinal classification model. Possible
adjustments of the model to take into account fuzzy thresholds in ordinal scales will
be discussed.

2 Fuzzy Set Experiments in See5
See5, a decision tree software, allows users to select options to soften thresholds
through selecting a fuzzy option. This option would insert a buffer at boundaries
(which is how PolyAnalyst works as well). The buffer is determined by the software
based on analysis of sensitivity of classification to small changes in the threshold. The
treatment from there is crisp, as opposed to fuzzy. Thus, in decision trees, fuzzy
implementations seem to be crisp models with adjusted set boundaries.
See5 software was used on a real set of credit card data[10]. This dataset had 6,000
observations over 64 variables plus an outcome variable indicating bankruptcy or not
(variables defined in [10]). Of the 64 independent variables, 9 were binary and 3
categorical. The problem can be considered to be an ordinal classification task as the
two final classes are named as “GOOD” and “BAD” with respect to financial success.
This means that majority of the numerical and categorical attributes (including binary
ones) may be easily characterized by more preferable values with respect to “GOOD”
financial success.
The dataset was balanced to a degree, so that it contained 960 bankrupt outcomes
(“BAD”) and 5040 not bankrupt (“GOOD.”) Winnowing was used in See5, which

464

D.L. Olson, H. Moshkovich, and A. Mechitov

reduced the number of variables used in models to about 20. Using 50 percent of the
data for training, See5 selected 3000 observations at random as the training set, which
was then tested on the remaining 3000 observations in the test set. Minimum support
on See5 was varied over the settings of 10, 20, and 30 cases. Pruning confidence
factors were also varied, from 10% (greater pruning), 20%, 30%, and 40% (less
pruning). Data was locked within nominal data runs, so that each treatment of pruning
and minimum case settings was applied to the same data within each repetition. Five
repetitions were conducted (thus there were 12 combinations repeated five times, or
60 runs). Each run was replicated for original crisp data, original data using fuzzy
settings, ordinal crisp data, ordinal data using fuzzy settings, and categorical data
(See5 would have no difference between crisp and fuzzy settings). Rules obtained
were identical across crisp and fuzzy models, except fuzzy models had adjusted rule
limits. For instance, in the first run, the following rules were obtained:
CRISP MODEL: RULE 1:
RULE 2:
RULE 3:

RULE 4:

RULE 5 ELSE

IF RevtoPayNov ≤ 11.441,
then GOOD
IF RevtoPayNov > 11.441 AND
IF CoverBal3 = 1
then GOOD
IF RevtoPayNov > 11.441 AND
IF CoverBal3 = 0 AND
IF OpentoBuyDec > 5.35129
then GOOD
IF RevtoPayNov > 11.441 AND
IF CoverBal3 = 0 AND
IF OpentoBuyDec ≤ 5.35129 AND
IF NumPurchDec ≤ 2.30259
then BAD
GOOD

The fuzzy model for this data set:
IF RevtoPayNov ≤ 11.50565
then GOOD
IF RevtoPayNov > 11.50565 AND
IF CoverBal3 = 1
then GOOD
RULE 3:
IF RevtoPayNov > 11.50565 AND
IF CoverBal3 = 0 AND
IF OpentoBuyDec > 5.351905
then GOOD
RULE 4:
IF RevtoPayNov > 11.50565 AND
IF CoverBal3 = 0 AND
IF OpentoBuyDec ≤ 5.351905 AND
IF NumPurchDec ≤ 2.64916
then BAD
RULE 5 ELSE
GOOD

FUZZY MODEL: RULE 1:
RULE 2:

Binary and categorical data are not affected by the fuzzy option in See5. They are
considered already “fuzzified” with several possible values and corresponding
membership function of 1 and 0.
Models run with initial data (numeric and categorical scales) in original crisp form,
and using See5’s fuzzification were obtained. There were 15 runs averaged for each
pruning level, and 20 runs averaged for each minimum case level. The overall line is

An Experiment with Fuzzy Sets in Data Mining

465

based on all 60 runs. The average number of crisp rules was 9.4 (fuzzy rules were the
same, with different limits). Crisp total error averaged 488.7, while fuzzy total error
averaged 487.2.
The number of rules responded to changes in pruning rates and minimum case
settings as expected (the tie between 20 percent and 30 percent pruning rates can be
attributed to data sampling chance). There were no clear patterns in error rates by
treatment. Fuzzy models were noticeably different from crisp models in that they had
higher error rates for bad cases, with corresponding improvement in error in the good
cases. The overall error was tested by t-test, and the only significant differences found
were that the fuzzy models had significantly greater bad error than the crisp models,
and significantly less cheap error. The fuzzy models had slightly less overall average
error, but given the context of credit cards, bad error is much more important. For
data fit, here the models were not significantly different. For application context, the
crisp models would clearly be preferred. The generalizable conclusion is not that crisp
models are better, only that in this case the fuzzy models were worse, and in general
one cannot count on the same results across crisp and fuzzified models.
In this case introducing fuzzy thresholds in the rules did not lead to any significant
results. The usage of small fuzzy intervals instead of crispy thresholds did not
significantly improve the accuracy of the model and did not provide better
interpretation of the introduced “interval rules.” On the other hand, crisp data was not
significantly better than the fuzzy data.
The same tests were conducted with presenting relevant binary and categorical
variables in an ordinal form. See5 allows stating that the categorical scale is
“[ordered]” with the presented order of attribute values corresponding to the order of
final classes. The order is not derived from the data but is introduced by the user as a
pre-processing step in rules/tree formation.
See5 would not allow locking across data sets, and required different setup for
ordinal specification, so we could not control for data set sampling across the tests.
Some categorical and/or binary variables such as “Months late” were clearly ordinal
and were marked as ordinal for this experiment. Categorical variables with no clear
ordinal qualities such as “State,” were left nominal. Crisp rules averaged 7.0, with
total error of 487. Fuzzy total error was 482.
The number of rules clearly dropped. Expected response of number of rules to
pruning and minimum case settings behaved as expected, with the one anomaly at 20
percent pruning, again explainable by the small sample size. Total error rates within
ordinal model were similar to the nominal case in the first set of runs, with fuzzy
model total error rates showing up as slightly significant (0.086 error probability) in
the ordinal models.
Comparing nominal and ordinal models, the number of rules was significantly
lower for ordinal models (0.010 error probability.) There were no significances in
errors across the two sets except for total error (ordinal models had slightly
significantly lower total errors, with 0.087 error probability.) This supports our
previous finding that using ordinal scales where appropriate lead to a set of more
interesting rules without loss in accuracy [6].
The data was categorized into 3 categories for each continuous variable. This in
itself is another form of fuzzification. Twenty five variables were selected based upon
the typical winnowing results of the original data. The same tests were conducted,

466

D.L. Olson, H. Moshkovich, and A. Mechitov

although fuzzification was not used since See5 would have no difference in applying
its fuzzification routine (we did run as a check, but results were always identical).
Average number of rules was just over 6, with average total error 495.7. Results were
much worse than the results obtained in prior runs with continuous and ordinal data.
That is clearly because in the third set of runs, data was categorized manually, while
in the prior two runs See5 software set the categorical limits. The second set of runs
involved data converted to fuzzy form by the See5 software. These are two different
ways to obtain fuzzy categories. Clearly the software can select cutoff limits that will
outperform ad hoc manual cutoffs.

3 Fuzzy Sets and Ordinal Classification Task
Previous experiments showed very modest improvements in the rule set derived from
introducing of fuzzy intervals instead of crisp thresholds for continuous scales using
SEE5. Interpretation of the modified rules was not “more friendly” or “more logical.”
Using stable data intervals was in general slightly more robust than using crisp
thresholds. Considering ordinal properties of some categorical/binary attributes led to
a better rule set although this did not change the fuzzy intervals for the continuous
scales. This supports our previous findings [6].
One of the more useful aspects of fuzzy logic may be the orientation on the
partition of continuous scales into a pre-set number of “linguistic summaries”[12]. In
[1] this approach is used to form fuzzy rules in a classification task. The main idea of
the method is to use a set of pre-defined linguistic terms for attributes with continuous
scales (e.g., “Young”, “Middle”, “Old” for an attribute “Age” measured
continuously). In this approach. the traditional triangular fuzzy number is calculated
for each instance of age in the training data set, e.g. age 23 is presented in ‘Young”
with a 0.85 membership function and in “Middle” with a 0.15 membership function (0
in “Old”). Thus the rewritten data set is used to mine interesting IF-THEN rules using
linguistic terms.
One of the advantages of the proposed approach stressed by the authors is the
ability of the mining method to produce rules useful for the user. In [3], the method
was used to mine a database for direct marketing campaign of a charitable
organization. In this case the domain expert defined appropriate uniform linguistic
terms for quantitative attributes. For example, an attribute reflecting the average
amount of donation (AVGEVER) was fuzzified into “Very low” (0 to $300), “Low”
($100 to $500), “Medium” ($300 to $700), “High” ($500 to $900) and “Very High”
(over $700). The analogous scale for frequency of donations (FREQEVER) was
presented as follows: “Very low” (0 to 3), “Low” (1 to 5), “Medium” (3 to 7), “High”
(5 to 9) and “Very High” (over 7). Triangular fuzzy numbers were derived from these
settings for rule mining. The attribute to be predicted was called “Response to the
direct mailing” and included two possible values “Yes” and “No.” The database
included 93 attributes, 44 having continuous scales.
Although the application of the method produced a huge number of rules (31,865)
with relatively low classification accuracy (about 64%), the authors argued that for a
task of such complexity the selection of several useful rules by the user of the results
was enough to prove the usefulness of the process. The presented rules found useful
by the user were presented as follows:

An Experiment with Fuzzy Sets in Data Mining

467

Rule 1: IF a donor was enrolled in any donor activity in the past (ENROLL=YES),
THEN he/she will have RESPONSE=YES
Rule 2: IF a donor was enrolled in any donor activity in the past (ENROLL=YES)
AND did not attend it (ATTENDED=NO), THEN he/she will have RESPONSE=YES
Rule 3: IF FREQEVER = MEDIUM, THEN RESPONSE=YES
Rule 4: IF FREQEVER = HIGH, THEN RESPONSE=YES
Rule 5: IF FREQEVER = VERY HIGH, THEN RESPONSE=YES
We infer two conclusions based on these results. First, if obvious ordinal
dependences between final classes of RESPONSE (YES/NO) and such attributes as
ENROLL, ATTENDED, and FREQEVER were taken into account the five rules
could be collapsed into two without any loss of accuracy and with higher levels for
measures of support and confidence: rule 1 and a modified rule 3 in the following
format “IF FREQEVER is at least MEDIUM, THEN RESPONSE=YES.” Second,
although presented rules are “user friendly” and easily understandable, they are not as
easily applicable. Overlapping scales for FREQEVER makes it difficult for the user to
apply the rules directly. It is necessary to carry out one more step - agree on the
number where “medium” frequency starts (if we use initial database) or a level of a
membership function to use in selecting “medium” frequency if we use the
“rewritten” dataset. The assigned interval of 3 to 5 evidently includes “High”
frequency (which does not bother us) but also includes “Low” frequency” which we
possibly would not like to include into our mailing list. As a result a convenient
approach for expressing continuous scales with overlapping intervals at the
preprocessing stage may be not so convenient in applying simple rules.
This presentation of the ordinal classification task allows use of this knowledge to
make some additional conclusions about the quality of the training set of objects.
Ordinal classification allows introduction of the notion of the consistency of the
training set as well as completeness of the training set. In the case of the ordinal
classification task quality of consistency in a classification (the same quality objects
should belong to the same class) can be essentially extended: all objects with higher
quality among attributes should belong to a class at least as good as objects with
lower quality. This condition can be easily expressed as follows: if Ai(x) ≥ Ai(y) for
each i=1, 2, …, p, then C(x) ≥ C(y).
We can also try to evaluate representativeness of the training set by forming all
possible objects in U (we can do that as we have a finite number of attributes with a
small finite number of values in their scales) and check on the proportion of them
presented in the training set. It is evident that the smaller this proportion the less
discriminating power we’ll have for the new cases. We can also express the resulting
rules in a more summarized form by lower and upper border instances for each
class [6].
Advantages of using ordinal scales in an ordinal classification task do not lessen
advantages of appropriate fuzzy set techniques. Fuzzy approaches allow softening
strict limitations of ordinal scales in some cases and provides a richer environment for
data mining techniques. On the other hand, ordinal dependences represent essential
domain knowledge which should be incorporated as much as possible into the mining
process. In some cases the overlapping areas of attribute scales may be resolved by
introducing additional linguistic ordinal levels. For example, we can introduce an

468

D.L. Olson, H. Moshkovich, and A. Mechitov

ordinal scale for age with the following levels: “Young” (less than 30), “Between
young and medium” (30 to 40), “Medium” (40 to 50), “Between medium and old” (50
to 60) and “Old” (over 60). Though it will increase the dimensionality of the problem,
it would provide crisp intervals for the resulting rules.
Ordinal scales and ordinal dependences are easily understood by humans and are
attractive in rules and explanations. These qualities should be especially beneficial in
fuzzy approaches to classification problems with ordered classes and “linguistic
summaries” in the discretization process. The importance of ordinal scales for data
mining is evidenced by appearance of this option in many established mining
techniques. See5 includes the variant of ordinal scales in the problem description [9].

4 Conclusions
Fuzzy representation is a very suitable means for humans to express themselves.
Many important business applications of data mining are appropriately dealt with by
fuzzy representation of uncertainty. We have reviewed a number of ways in which
fuzzy sets and related theories have been implemented in data mining. The ways in
which these theories are applied to various data mining problems will continue to
grow.
Ordinal data is stronger than nominal data. There is extra knowledge in knowing if
a greater value is preferable to a lesser value (or vice versa). This extra information
can be implemented in decision tree models, and our results provide preliminary
support to the idea that they might strengthen the predictive power of data mining
models.
Our contention is that fuzzy representations better represent what humans mean.
Our brief experiment was focused on how much accuracy was lost by using fuzzy
representation in one application – classification rules applied to credit applications.
While we expected less accuracy, we found that the fuzzy models (as applied by See5
adjusting rule limits) usually actually were more accurate. Models applied to
categorical data as a means of fuzzification turned out less accurate in our small
sample. While this obviously cannot be generalized, we think that there is a logical
explanation. While fuzzification will not be expected to yield better fit to training
data, the models obtained by using fuzzification will likely be more robust, which is
reflected in potentially equal if not better fit on test data. The results of these
preliminary experiments indicate that implementing various forms of fuzzy analysis
will not necessarily lead to reduction in classification accuracy.

References
1. Au, W-H, Keith C. C. Chan: Classification with Degree of Membership: A Fuzzy
Approach. ICDM (2001): 35-42
2. David, B. A. (1992): Automated generation of symbolic multiattribute ordinal knowledgebased DSSs: Methodology and applications. Decision Sciences, 23(6), 157-1372
3. Chan, Keith C. C., Wai-Ho Au, Berry Choi: Mining Fuzzy Rules in A Donor Database for
Direct Marketing by a Charitable Organization. IEEE ICCI (2002): 239-246

An Experiment with Fuzzy Sets in Data Mining

469

4. Dubois, D., E. Hullermeier, H. Prade: A Systematic Approach to the Assessment of Fuzzy
Association Rules. Data Mining and Knowledge Discovery, (2006), July, 1-26
5. Larichev, O.I., Moshkovich, H.M. (1994): An approach to ordinal classification problems.
International Trans. on Operations Research, 82, 503-521
6. Moshkovich H.M., Mechitov A.I., Olson, D.: Rule Induction in Data Mining: Effect of
Ordinal Scales. Expert Systems with Applications Journal, (2002), 22, 303-311
7. Pawlak, Z.: Rough set, International Journal of Computer and Information Sciences.
(1982), 341-356
8. Pearl, J.: Probabilistic reasoning in intelligent systems, Networks of Plausible inference,
Morgan Kaufmann, San Mateo,CA (1988)
9. See5 - http://www.rulequest.com
10. Shi, Y., Peng, Y., Kou, G., Chen, Z.: Classifying credit card accounts for business
intelligence and decision making: A multiple-criteria quadratic programming approach,
International Journal of Information Technology & Decision Making 4:4 December
(2005), 581-599
11. Slowinski, R. (1995): Rough set approach to decision analysis. AI Expert,19-25
12. Yager, R.R.: On Linguistic Summaries of Data, in G. Piatetsky-Shapiro and W.J. Frawley
9Eds.) Knowledge Discovery in Databases, Mento Park, CA: AAAI/MIT Press, (1991),
347-363
13. Zadeh, L.A.: Fuzzy sets, Information and Control 8 (1965), 338-356
14. Zhao, K.-G.: Set pair analysis – a new concept and new systematic approach. Proceedings
of national system theory and regional analysis conference, Baotou (1989) (In Chinese)
15. Zhao, K.-G.: Set pair analysis and its preliminary application, Zhejiang Science and
Technology Press (2000) (In Chinese)

