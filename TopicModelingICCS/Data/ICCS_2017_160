Available online at www.sciencedirect.com

ScienceDirect

This space is reserved for the Procedia header, do not use it
This space
isComputer
reservedScience
for the
header, do not use it
Procedia
108CProcedia
(2017) 345–354
This space is reserved for the Procedia header, do not use it

International Conference on Computational Science, ICCS 2017, 12-14 June 2017,
Zurich, Switzerland

Learning Entity and Relation Embeddings for Knowledge
Learning Entity and Relation Embeddings for Knowledge
Resolution
Learning Entity and Relation
Embeddings for Knowledge
Resolution
Hailun Lin, Yong Liu∗,∗ Weiping
Wang, Yinliang Yue, and Zheng Lin
Resolution
Hailun Lin, Yong Liu, Weiping Wang, Yinliang Yue, and Zheng Lin
Institute of Information Engineering, Chinese Academy of Sciences, Beijing, China

∗
Hailun
Yong LiuEngineering,
, Weiping
Wang,Academy
Yinliang
Yue, and
Zheng
Lin
InstituteLin,
of Information
Chinese
of Sciences,
Beijing,
China
linhailun@iie.ac.cn

linhailun@iie.ac.cn
Institute of Information Engineering, Chinese Academy of Sciences, Beijing, China
linhailun@iie.ac.cn

Abstract
Abstract
Knowledge resolution is the task of clustering knowledge mentions, e.g., entity and relation
Knowledge
resolution is the task of clustering knowledge mentions, e.g., entity and relation
mentions
Abstractinto several disjoint groups with each group representing a unique entity or relation.
mentions
into several
disjoint
groups
with each group representing
a unique from
entity or relation.
Such
resolution
is a central
step
in constructing
Knowledge resolution
is the
task
of clusteringhigh-quality
knowledge knowledge
mentions, graph
e.g., entity unstructured
and relation
Such
resolution
is a central
step
in constructing
high-quality
knowledge
graph
fromand
unstructured
text.
Previous
research
has
tackled
this
problem
by
making
use
of
various
textual
mentions into several disjoint groups with each group representing a unique entity orstructural
relation.
text.
Previous
has tackled this
by making
ofmay
various
textual
structural
features
from aresearch
semantic
or aproblem
knowledge
graph. use
This
lead
to from
poorand
performance
Such resolution
is
a centraldictionary
step in constructing
high-quality
knowledge
graph
unstructured
features
from a semanticwith
dictionary
ornot
a knowledge
graph.
ThisInmay
lead toitpoor
performance
on
knowledge
poor or
well-known
contexts.
addition,
is also
by
text.
Previousmentions
research has tackled
this
problem
by making
use of
various
textual
and limited
structural
on
knowledge
mentions
with dictionary
poor or notorwell-known
contexts.
In
addition,
itpropose
is also limited
by
the
coverage
of
the
semantic
knowledge
graph.
In
this
work,
we
ETransR,
features from a semantic dictionary or a knowledge graph. This may lead to poor performance
the
coverage
of the
semantic dictionary
or knowledge
graph.
In this
work, we propose
ETransR,
aonmethod
which
automatically
learns
entity
and
relation
feature
representations
in
continuous
knowledge mentions with poor or not well-known contexts. In addition, it is also limited by
avector
method
which
automatically
learns
entity and
relation feature
representations
in continuous
spaces,
order
to measure
the semantic
relatedness
knowledge
mentions
for
knowledge
the coverage
ofinthe
semantic
dictionary
or knowledge
graph.of In
this work,
we propose
ETransR,
vector
spaces,
in order to measure
thetwo
semantic
relatedness
of knowledge
mentions
for knowledge
resolution.
Experimental
results
on
benchmark
datasets
show
that
our
proposed
method
a method which automatically learns entity and relation feature representations in continuous
resolution.
Experimental
results on
two benchmark
datasets
show that
our proposed
method
delivers
signiﬁcant
improvements
compared
with
the
state-of-the-art
baselines
on
the
task of
vector spaces, in order to measure the semantic relatedness of knowledge mentions for knowledge
delivers
signiﬁcant
improvements
compared
with
the
state-of-the-art
baselines
on
the
task of
knowledge
resolution.
resolution. Experimental results on two benchmark datasets show that our proposed method
knowledge resolution.
delivers
signiﬁcant
improvements
compared
with
the state-of-the-art
baselines
on the task
of
Keywords:
knowledge
graph,by
knowledge
resolution,
knowledge
representation,
entity embedding,
rela©
2017 The
Authors.
Published
Elsevier
B.V.
Keywords:
knowledge
graph,ofknowledge
knowledge
representation,
embedding,
relaPeer-review
under
responsibility
the scientificresolution,
committee of
the International
Conferenceentity
on Computational
Science
knowledge
resolution.
tion
embedding
tion embedding
Keywords: knowledge graph, knowledge resolution, knowledge representation, entity embedding, relation embedding

1 Introduction
1 Introduction
Access to an organized knowledge graph is critical for many real-world tasks, such as query sug1
Introduction
Access
an question
organizedanswering.
knowledgeMost
graphreal-world
is critical for
many real-world
tasks, such
as query suggestion to
and
information
is unstructured,
interconnected,
gestion
andoften
question
answering.
real-world
informationconstructing
is unstructured,
interconnected,
noisy,
expressed
in the Most
form
This
ansuch
organized
knowlAccessand
to an
organized
knowledge
graphofistext.
critical
forinspires
many real-world tasks,
as query
sugnoisy,
and often
expressed
in
the form
of text.
This
inspires
constructing
anknowledge
organized graphs
knowledge
graph
from
the
large
volume
of
noisy
text
data.
A
large
number
of
gestion and question answering. Most real-world information is unstructured, interconnected,
edge
graphconstructed,
from the large
volume
of noisy Knowledge
text data. Valut
A large
knowledge
graphs
have
suchin
asthe
Freebase
[8],number
YAGO of
[9],
Probase [18].
An
noisy,been
and often expressed
form of[1],
text. This inspires
constructing
an organized
knowlhave
been constructed,
such
as Freebaseknowledge
[1], Knowledge
Valut
[8], YAGOresolution.
[9], ProbaseGiven
[18]. the
An
important
component
in
constructing
graphs
is
knowledge
edge graph from the large volume of noisy text data. A large number of knowledge graphs
important
component
in constructing
knowledge
graphsmentions)
is knowledge
resolution. Given
the
knowledge
mentions
(e.g.,
entity
mentions
and
relation
in
unstructured
text
data,
have been constructed, such as Freebase [1], Knowledge Valut [8], YAGO [9], Probase [18]. An
knowledge
mentions (e.g., entityismentions
and
relationmentions
mentions)
indisjoint
unstructured
text
the
goal of knowledge
to cluster
knowledge
into
groupsGiven
withdata,
each
important
componentresolution
in constructing
knowledge
graphs is knowledge
resolution.
the
the
goal
of knowledge
resolution
is to cluster knowledge mentions into disjoint groups with each
group
representing
a
unique
knowledge.
knowledge mentions (e.g., entity mentions and relation mentions) in unstructured text data,
group representing a unique knowledge.
the goal of knowledge resolution is to cluster knowledge mentions into disjoint groups with each
∗ Corresponding author: Yong Liu (liuyong@iie.ac.cn)
group
representing a unique knowledge.
∗
Corresponding author: Yong Liu (liuyong@iie.ac.cn)

∗ Corresponding

author: Yong Liu (liuyong@iie.ac.cn)

1877-0509 © 2017 The Authors. Published by Elsevier B.V.
Peer-review under responsibility of the scientific committee of the International Conference on Computational Science
10.1016/j.procs.2017.05.045

1
1
1

346	

Learning Entity and Relation Embeddings
Resolution
Lin, Liu, Wang, et al.
Hailun Lin etfor
al. Knowledge
/ Procedia Computer
Science 108C (2017) 345–354

The knowledge resolution task is challenging due to the fact that many knowledge mentions
are ambiguous: the same mention can refer to various diﬀerent real world entities or relations when they appear in diﬀerent contexts, and many knowledge has various mention forms.
Knowledge resolution plays a critical role in high-quality knowledge graph construction. When
knowledge extracted from text data is ready to be inserted into the knowledge graph, it is necessary to know which real world knowledge this piece of information should be associated with.
If a wrong decision is made here, the knowledge graph will not only lose some information, but
also introduce errors.
Traditional approach to knowledge resolution usually makes use of various textual and
structural features from a semantic dictionary or a knowledge graph (e.g., [5], [16]), which may
lead to poor performance on knowledge mentions with poor not well-known contexts. Moreover,
its performance is also limited by the coverage of the semantic dictionary or knowledge graph.
Recently, a promising approach for the task is embedding knowledge into a continuous vector
space to learn feature representations, in order to measure the semantic relatedness of knowledge
mentions for knowledge resolution. Following this approach, many methods have been explored,
which will be introduced in detail in Section 2. Among these methods, some notable works,
including TransE [3], TransH [17], TransR [11] are eﬀective and eﬃcient. TransE represents
entities as points and relations as translations from head entities to tail entities in a vector
space. TransH models a relation as a hyperplane together with a translation operation on it.
TransR models entity and relation embeddings in separate vector spaces, which are bridged
by relation-speciﬁc matrices. These methods mainly focus on modeling single knowledge in
continuous space, which ignore modeling the semantic relatedness between knowledges.
In order to learn better knowledge representations to model the complicated semantic correlations between knowledge triples, we propose a new model, named ETransR. ETransR is
the extension form of TransR, which models entities and relations in distinct vector spaces,
i.e., entity space and multiple relation spaces, and performs translation in the corresponding
relation space. The basic idea of ETransR is illustrated in Figure 1. In ETransR, for each
knowledge tripe (h, r, t), it ﬁrst embeds entities in entity space, relations in relation space. And
then it projects entity embeddings into relation space by relation-speciﬁc matrix Mr , which can
obtain hr +r ≈ tr . The model can make the head/tail entities that actually hold the relation
(denoted as circles) close with each other, and get far away from those that do not hold the
relation (denoted as triangles).

Figure 1: The illustration of ETransR
Given two knowledge triples (h1 , r1 , t1 ) and (h2 , r2 , t2 ), if they are two mention forms of a
unique knowledge, then they will satisfy tr1 - hr1 ≈ tr2 - hr2 . For head entities, if h1 is the same
as h2 , it will have hr2 + r1 ≈ tr1 , hr1 + r2 ≈ tr2 . And for tail entities, if t1 is the same as t2 , it
2

	

Learning Entity and Relation Embeddings
Resolution
Lin, Liu, Wang, et al.
Hailun Lin etfor
al. Knowledge
/ Procedia Computer
Science 108C (2017) 345–354

will have hr1 + r1 ≈ tr2 , hr2 + r2 ≈ tr1 . In addition, considering that the representations of a
group of knowledge mentions of a unique knowledge usually exhibit same semantic patterns, we
extend ETransR by clustering same knowledge mentions into groups and learning knowledge
embeddings for each group, named as CETransR (cluster-based ETransR), which follows the
idea of piecewise linear regression [14].
Our main contribution is the development of an innovative knowledge representation model
for measuring knowledge semantic correlations to help boost the knowledge resolution performance, which automatically learns entity and relation feature representations in continuous
vector spaces. Experimental results on two benchmark datasets validate the eﬀectiveness and
eﬃciency of our method compared with the state-of-the-art methods.
The rest of this paper is organized as follows. In Section 2, we give a review of related
work. Section 3 introduces our entity and relation embedding models for learning knowledge
representations. Section 4 is devoted to the experimental results. Finally, the paper is concluded
in Section 5.

2

Related Work

Knowledge embedding has received a lot of attentions in recent years, existing knowledge embedding methods aim to represent entities and relations of knowledge graphs as vectors in a
continuous vector space, where they usually deﬁned a loss function to evaluate the representations. A variety of approaches have been explored for knowledge embedding. They can be
divided into three major categories.
The ﬁrst category is the translation based approach. Some notable works, including TransE
[3], TransH [17], TransR [11] and TransA [10] are eﬀective and eﬃcient. TransE assumes
h+r ≈ t when (h, r, t) is a golden triple, which indicates that t should be the nearest neighbor of
h+r. To deal with relations with diﬀerent mapping properties, TransH is established to project
entities into a relation-speciﬁc hyperplane and the relation becomes translating operation on
hyperplane. For a relation r, TransH models the relation as a vector r on a hyperplane with
wr as the normal vector. For a triple (h, r, t), the entity embeddings h and t are ﬁrst projected
to the hyperplane of wr , denoted as h⊥ and t⊥ . By projecting entity embeddings into relation
hyperplanes, it allows entities playing diﬀerent roles in diﬀerent relations. Both TransE and
TransH assume embeddings of entities and relations being in the same space Rk . However, an
entity may have multiple aspects, and various relations focus on diﬀerent aspects of entities.
Hence, it is intuitive that some entities are similar and thus close to each other in the entity
space, but are comparably diﬀerent in some speciﬁc aspects and thus far away from each
other in corresponding relation spaces. To address this issue, TransR is proposed to model
entities and relations in distinct spaces, i.e., entity space and multiple relation spaces, which
are bridged by relation-speciﬁc matrices. In TransR, for each triple (h, r, t), entities embeddings
are set as h, t ∈ Rk and relation embedding is set as r ∈ Rd . For each relation r, TransR
sets a projection matrix Mr ∈ Rk×d , which may projects entities from entity space to relation
space. These methods deﬁne a global margin-based loss function to learn knowledge embedding
representations, and share the same set of candidate settings for diﬀerent entities and relations,
which ignore the individual locality of knowledge representations and seems not convincing in
theory. To address this issue, TransA is proposed to adaptively ﬁnds the optimal loss function
by adaptively determining its margin over diﬀerent knowledge triples.
The second category is the tensor decomposition based approach, which is a well-developed
mathematical tool for data analysis. Some notable works, including TUCKER [6], RESCAL
[13] and TRESCAL [7] belong to this category. TUCKER decomposition, also known as high3

347

348	

Learning Entity and Relation Embeddings
Resolution
Lin, Liu, Wang, et al.
Hailun Lin etfor
al. Knowledge
/ Procedia Computer
Science 108C (2017) 345–354

order singular value decomposition (SVD), factorizes a tensor into a core tensor multiplied by a
matrix along each dimension. RESCAL is a tensor factorization based method. Compared with
other tensor factorizations, the main advantage of RESCAL is that it can exploit a collective
learning eﬀect when applied to relational data. RESCAL has shown very good results in
various canonical relational learning tasks such as link predication. TRESCAL is an extension
of RESCAL, which tries to encode rules into RESCAL. However, it focuses solely on a single
rule, i.e., the arguments of a relation should be entities of certain types.
The third category is the energy based approach, which assigns low energies to plausible
triples of a knowledge graph and employs neural network for learning. For example, Unstructured model [2] is proposed as a naive version of TransE by assigning all r = 0, leading to score
function fr (h, t) =∥ h − t ∥22 . This model cannot consider diﬀerences of relations. Structured
embedding (SE) model [4] designs two relation-speciﬁc matrices for head and tail entities, i.e.,
Mr,1 and Mr,2 , and deﬁnes the score functions as an L1 distance between two projected vectors,
i.e., fr (h, t) =∥ Mr,1 h − Mr,2 t ∥1 . Since the model has two separate matrices for optimization,
it cannot capture precise relations between entities and relations. Neural tensor network (NTN)
model [15] represents entities in a d-dimensional vectors created separately by averaging pretrained word vectors, and then learns a d×d×m tensor describing the interactions between these
latent components in each of m relations. Meanwhile, the corresponding high complexity of
NTN may prevent it from eﬃciently applying on large-scale knowledge graphs. However, these
methods mainly focus on modeling single knowledge in continuous vector spaces, which ignore
the semantic relatedness modeling between knowledges. We intend to propose an comprehensive translation method to automatically learning entity and relation feature representations in
continuous vector spaces, in order to measure the semantic relatedness of knowledge mentions
for knowledge resolution.

3

Embedding by Translating on Two Vector Spaces

In order to model the semantic correlations between knowledges, we propose ETransR model, which models entities and relations in distinct vector spaces, and performs translation in
the corresponding relation space, in order to measure the semantic relatedness of knowledge
mentions for knowledge resolution.

3.1

Embedding Model

In ETransR model, for each knowledge triple (h, r, t), h, t ∈ Rk represents entity embeddings
of h and t, respectively; r ∈ Rm represents relation embeddings of r. k denotes the entity
embeddings dimension, m denotes the relation embeddings dimension. It is noted that k and
m are not necessarily identical, i.e., k ̸= m. For each relation r, we set a projection matrix
Mr ∈ Rk×m , to project entity embeddings in entity space to relation space. Speciﬁcally, for a
triple (h, r, t), the embeddings h and t are ﬁrst projected to the relation space with projection
matrix Mr . The entities embedding projections are denoted as hr and tr , where hr = hMr ,
tr = tMr . We expect hr and tr can be connected by the relation embedding r on the relation
space with low error if (h, r, t) is a golden triple, and high error if (h, r, t) is a incorrect triple.
Thus we deﬁne a score function ||hr + r − tr ||d to measure the plausibility that the knowledge
triple is incorrect, where d denotes L1 or L2 -norm. Given two knowledge triples (h1 , r1 , t1 ) and
(h2 , r2 , t2 ), we deﬁne a score function to measure the dissimilarity between the two triples. The
score function is correspondingly deﬁned as
fr1 ,r2 (h1 , t1 ; h2 , t2 ) = ||(tr1 − hr1 ) − (tr2 − hr2 ) + (r1 − r2 )||d
4

(1)

	

Learning Entity and Relation Embeddings
Resolution
Lin, Liu, Wang, et al.
Hailun Lin etfor
al. Knowledge
/ Procedia Computer
Science 108C (2017) 345–354

The score is expected to be lower for two similar triples and higher for two dissimilar triples. In
this paper, we restrict ||h||2 ≤ 1, ||t||2 ≤ 1, ||r||2 ≤ 1, ||hMr ||2 ≤ 1, ||tMr ||2 ≤ 1. The model
parameters consist of all elements of the entities’s embeddings vectors, relations’ embeddings
vectors and relations’ projection matrices.
ETransR learns a unique vector for each relation, which may be under-representative to ﬁt
all knowledge triple mentions representing a unique knowledge triple, because these knowledge
triple mentions exhibit same semantic patterns. In order to better model these knowledge
triples, we extend ETransR by clustering same knowledge triple mentions into groups and
learning knowledge embeddings for each group, named as CETransR (cluster-based ETransR),
which follows the idea of piecewise linear regression [14].
In CETransR, it ﬁrst segments input knowledge triple mentions into several groups, where
each group represents a unique knowledge triple. Formally, for a speciﬁc knowledge triple
(h, r, t), all knowledge triple mentions in the dataset are clustered into multiple groups, and
knowledge triple mentions in each group are exhibit same semantic patterns. All entity pairs
(h, t) are represented with their vector oﬀset (t − h) for clustering. Afterwards, we learn a separate relation vector rc for each cluster and projection matrix Mr for each relation, respectively.
The entities embeddings projection vectors are deﬁned as hr,c = hMr , tr,c = tMr . Given
two similar knowledge triples (h1 , r1 , t1 ) and (h2 , r2 , t2 ), the score function is correspondingly
deﬁned as
fr1 ,r2 (h1 , t1 ; h2 , t2 ) = ||(tr1 ,c − hr1 ,c ) − (tr2 ,c − hr2 ,c )||d + α(||rc − r1 ||d − ||rc − r2 ||d )

(2)

where (||rc − r1 ||d − ||rc − r2 ||d ) aims to ensure cluster-speciﬁc relation vector rc not far away
from the original relation vector r1 and r2 ; α controls the eﬀect of this constraint, 0 ≤ α ≤ 1.
In this paper, we restrict ||h||2 ≤ 1, ||t||2 ≤ 1, ||r||2 ≤ 1, ||rc ||2 ≤ 1 , ||hr,c ||2 ≤ 1, ||tr,c ||2 ≤ 1.
The model parameters consist of all elements of the entities’s embeddings vectors, relations’
embeddings vectors and relations’ projection matrices.

3.2

Training

To learning knowledge embeddings with our proposed models, we deﬁne a margin-based ranking
criterion as objective for training:
∑
∑
L=
[γ+
((h1 ,r1 ,t1 );(h2 ,r2 ,t2 ))∈S ((h′ ,r1 ,t′ );(h′ ,r2 ,t′ ))∈S ′
1

1

2

′

′

2

′

(3)

(h1 ,r1 ,t1 ;h2 ,r2 ,t2 )

′

fr1 ,r2 (h1 , t1 ; h2 , t2 ) − fr1 ,r2 (h1 , t1 ; h2 , t2 )]+
∆

′

where [x]+ = max(0, x); γ is a margin hyperparameter, γ > 0; S is the correct triples set; S
is the incorrect triples set, constructed according to Equation 4, is composed of correct triples
with either the head or tail entity replaced by a random entity, but not both at the same time.
′

′

′

′

′

S(h1 ,r1 ,t1 ;h2 ,r2 ,t2 ) = {(h1 , r1 , t1 ); (h2 , r2 , t2 )|h1 , h2 ∈ E} ∪ {(h1 , r1 , t1 ); (h2 , r2 , t2 )|t1 , t2 ∈ E} (4)
More speciﬁcally, during constructing incorrect triples, we follow the method described in [17],
which sets diﬀerent probabilities for replacing the head or tail entity for corrupting the golden
triple. We denote this sampling method as “bern”. In addition, we also construct incorrect
triples by randomly corrupting the golden triple, which sets equal probability for replacing the
head or tail entity. We denote this sampling method as “unif”. In experiments, we will validate
5

349

350	

Learning Entity and Relation Embeddings
Resolution
Lin, Liu, Wang, et al.
Hailun Lin etfor
al. Knowledge
/ Procedia Computer
Science 108C (2017) 345–354

the impact that how the performance of our approach changes with the “unif” and “bern”
incorrect triple sampling methods.
The loss function (Equation 3) favors lower values for similar triples than for dissimilar
triples, and is thus a natural implementation of the intended criterion. It is worthwhile to note
that for a given entity, its embedding vector is the same when the entity appears as the head
or as the tail of a triple. The optimization is carried out by stochastic gradient descent (SGD)
in minibatch mode to minimize the above loss function. The set of golden triples are randomly
traversed multiple times. After a minibatch, the gradient is computed and the model parameters are updated. The detailed optimization procedure is described in Algorithm 1. It ﬁrstly
exploits the random procedure proposed in [3] to initialize all entities embeddings and relations
embeddings, and uses identity matrix to initialize all relation-speciﬁc projection matrices. All
the entities embedding vectors, relation embedding vectors and projection matrices are ﬁrst
normalized during each main iteration. Then, it samples a small set of triples from the training
dataset, and adds these triples into the training triples of the minibatch. It is noted that, for
each such triple, a single incorrect triple is sampled. In the algorithm, the parameters are then
updated by taking a gradient step with constant learning rate. The algorithm is stopped based
on its performance on a validation dataset.
Algorithm 1 Model learning algorithm
′

Input: Correct triples set S, incorrect triples set S , entities set E, relations set R, entity
embedding dimension k, relation embedding dimension m, margin γ, data bach size B;
Output: The well trained embedding model;
1: for all e ∈ E, r ∈ R do
2:
e← uniform(− √6k , √6k );
3:
r← uniform(− √6m , √6m );
4:
Mr ← Ik×m ;
5: end for
6: loop
7:
norm(e), norm(r), norm(eMr );
8:
Sbatch ← Sample(S, B);
9:
Tbatch ← ∅;
10:
for (h1 , r1 , t1 ), (h2 , r2 , t2 ) ∈ Sbatch do
′
′
′
11:
(h1 , r1 , t1 ) ← Sample(S(h1 ,r1 ,t1 ) ); // sample a incorrect triple according to (h1 , r1 , t1 )
12:
13:
14:
15:

′

′

′

(h2 , r2 , t2 ) ← Sample(S(h2 ,r2 ,t2 ) ); // sample a incorrect triple according to (h2 , r2 , t2 )
′

′

16:
17:

4

′

′

′

Tbatch ← Tbatch ∪ {(h1 , r1 , t1 ), (h2 , r2 , t2 ), (h1 , r1 , t1 ), (h2 , r2 , t2 )};
end for
∑
Update knowledge embeddings w.r.t. (((h1 ,r1 ,t1 );(h2 ,r2 ,t2 ));((h′ ,r1 ,t′ );(h′ ,r2 ,t′ )))∈Tbatch ∇[γ+
′

′

′

1

1

2

2

fr1 ,r2 (h1 , t1 ; h2 , t2 ) − fr1 ,r2 (h1 , t1 ; h2 , t2 )]+ ;
end loop
return The trained model;

Experiments

In this section, we will evaluate the eﬀectiveness of our proposed method for the knowledge
resolution task. We will: (1) compare the knowledge resolution accuracy of our method, with
6

	

Learning Entity and Relation Embeddings
Resolution
Lin, Liu, Wang, et al.
Hailun Lin etfor
al. Knowledge
/ Procedia Computer
Science 108C (2017) 345–354

four state-of-the-art baseline methods; (2) show how the performance of our method changes
with respect to the increase of the number of the entity and relation embedding dimensions.

4.1

Experimental Settings

All experiments were conducted on a server running 64-bit Linux OS, with 16 core 2GHz
AMD Opteron(tm) 6128 Processors and 32GB RAM. In the experiments, we used two widely
used datasets: one from WordNet [12], and the other from Freebase [1]. For the dataset from
WordNet, we employ WN11 used in [15]. For the dataset from Freebase, we employ FB15K
used in [2]. WN11 contains 11 relation types and 38,696 related entities. FB15K contains 1,345
relation types and 14,951 related entities. They were randomly split into three groups, i.e.,
train data, valid data and test data. The details of these datasets are shown in Table 1.
Dataset
Entities
Relations
Train
Valid
Test

WN11
38,696
11
112,581
2,609
10,544

FB15K
14,951
1,345
483,142
50,000
59,071

Table 1: Statistics of the datasets
As knowledge resolution aims to cluster knowledge triples into several disjoint groups, this
means it is to judge whether a given triple is correctly classiﬁed or not. This can be seen as a
classiﬁcation task. Evaluation of classiﬁcation needs negative labels. The dataset WN11 already
contains negative triples, which are obtained by corrupting by correct ones. For FB15K, we
construct the incorrect triples following the same way as [15]. The decision rule for classiﬁcation
is as follows. For a triple (h, r, t), if the dissimilarity score obtained by fr is less than a relationspeciﬁc threshold δr , then the triple is classiﬁed to be positive, and negative otherwise. δr is
determined by maximizing classiﬁcation accuracy on the validation set.

4.2

Experimental Results and Analysis

Triple classification accuracy. For experiments of ETransR and CETransR, the learning
rate λ for stochastic gradient descent (SGD) is select among {0.001, 0.005, 0.01, 0.05, 0.1}, the
margin γ among {1, 2, 4, 8, 10}, the dimensions of entity embedding k, relation embedding m
among {10, 20, 40, 50, 60, 100, 200}, the batch size B among {20, 30, 70, 120, 420, 480, 960,
1440}, the α for CETransR among {0.001, 0.005, 0.01, 0.1}. The dissimilarity measure d is set
either to the L1 -norm or L2 -norm. All parameters are determined according to classiﬁcation
accuracy on the validation set. Regarding the strategy of constructing negative labels, we use
“unif” to denote the traditional way of replacing head entity or tail entity with equal probability,
and use “bern” to denote reducing false negative labels by replacing head entity or tail entity
with diﬀerent probabilities. Under the “unif” setting, the optimal conﬁgurations are: λ = 0.01,
γ = 1, k = 40, m = 40, B = 70, α = 0.1 and taking L1 -norm as dissimilarity measure on WN18;
λ = 0.005, γ = 2, k = 60, m = 60, B = 180, α = 0.005 and taking L1 -norm as dissimilarity
measure on FB15K. Under the “bern” setting, the optimal conﬁgurations are: λ = 0.01, γ = 2,
k = 50, m = 50, B = 420, α = 0.01 and taking L1 as dissimilarity measure on WN18; λ = 0.001,
γ = 2, k = 100, m = 100, B = 480, α = 0.01 and taking L1 as dissimilarity measure on FB15K.
For both datasets, we traverse all the training triples for 1000 rounds.
7

351

352	

Learning Entity and Relation Embeddings
Resolution
Lin, Liu, Wang, et al.
Hailun Lin etfor
al. Knowledge
/ Procedia Computer
Science 108C (2017) 345–354

Experimental results on WN11 and FB15K are shown in Table 2. From Table 2 we can see
that: (1) ETransR and CETransR outperform baseline methods consistently. It indicates that
ETransR and CETransR ﬁnd a better model to express the semantic features of entities and
relation types. (2) All the methods using “bern” sampling strategy perform better than using
“unif” strategy, especially on FB15K which have much more relation types. (3) CETransR
performs better than ETransR. It indicates that a ﬁne-grained model may handle complicated
internal semantic correlations under each relation type better than coarse-grained model.
Approach
TransE(unif)
TransE(bern)
TransH(unif)
TransH(bern)
TransR(unif)
TransR(bern)
CTransR(unif)
CTransR(bern)
ETransR(unif )
ETransR(bern)
CETransR(unif )
CETransR(bern)

Accuracy
WN11 FB15K
0.759
0.796
0.759
0.802
0.777
0.790
0.788
0.802
0.855
0.817
0.859
0.839
0.856
0.841
0.857
0.845
0.871
0.864
0.873
0.869
0.882
0.873
0.885
0.876

Table 2: Evaluation results of triple classiﬁcation
Some relation types classiﬁcation results in FB15K tripes with CETransR are shown in
Table 3. From Table 3 we can ﬁnd obvious patterns that: Category#1 is about team-play
position relation type, Category#2 is about award-winner relation type, Category#3 is about
ﬁle-writer relation type. It is obvious that by clustering we can learn more and ﬁne-grained
knowledge embeddings, which can further help improve the performance of triple classiﬁcation.
No.
1

2
3

Relation types
/soccer/football player/position s
/soccer/football player/current team./sports/sports team roster/position
/sports/pro athlete/teams./sports/sports team roster/position
/award/award winning work/awards won./award/award honor/award winner
/award/award ceremony/awards presented./award/award honor/award winner
/award/award category/winners./award/award honor/award winner
/ﬁlm/ﬁlm/written by
/ﬁlm/ﬁlm/writer
Table 3: Evaluation results of triple classiﬁcation

Impact of entity and relation embedding dimensions. Next we conducted experiments
over the FB15K dataset to illustrate how the performance of our CETransR method changes,
with respect to the increase of the number of entities and relation embedding dimensions, using
“bern” sampling strategy. Here the parameter settings are the same as those described in the
above experiments. The results are graphically depicted in Figure 2.
From Figure 2 we can see that: (1) The triple classiﬁcation accuracy increases as the number
8

Learning Entity and Relation Embeddings
Resolution
Lin, Liu, Wang, et al.
Hailun Lin etfor
al. Knowledge
/ Procedia Computer
Science 108C (2017) 345–354

of entity and relation embedding dimensions increases. It indicates that ﬁne-grained entity and
relation embeddings are better for modeling knowledge complicated semantic relationships. (2)
When we ﬁxed the relation embedding dimension m, the triple classiﬁcation accuracy increases
as the number of entity embedding dimension k increases. But the increasing speed slows down,
vice versa. In addition, the accuracy decreases when the value of |k − m| increases. It indicates
that it should ensure the granularity of entity embeddings and relation embeddings, otherwise
it will cause distortion of entity and relation semantic representations.






































































	























Figure 2: The impact of the entity and relation embedding dimensions on the triple classiﬁcation
of CETransR over the FB15K dataset

5

Conclusion

In this paper, we studied the problem of knowledge resolution. We proposed a knowledge representation method to automatically learning the feature representations of entities and relations
in embedding spaces. This paper proposed ETransR and CETransR models, which embed
entities and relations in continuous vector spaces. These models aim to model complicated
semantic correlations between knowledge triple mentions, in order to improve the performance
of synonym and multi-sense knowledge mentions resolution. Experimental results show that
this method outperforms the state-of-the-art methods on the task of knowledge resolution.
Acknowledgments
This work is supported by the National Key Research and Development Program of China
(No. 2016YFB1000902), the National Natural Science Foundation of China (No. 61602467,
61502478, 61402464, 61402473).

References
[1] Kurt Bollacker, Colin Evans, Praveen Paritosh, Tim Sturge, and Jamie Taylor. Freebase: a
collaboratively created graph database for structuring human knowledge. In Proceedings of the

9

353

354	

Learning Entity and Relation Embeddings
Knowledge
Resolution
Lin, Liu, Wang, et al.
Hailun Linfor
et al.
/ Procedia Computer
Science 108C (2017) 345–354

ACM SIGMOD international conference on Management of data, pages 1247–1250. ACM, 2008.
[2] Antoine Bordes, Xavier Glorot, Jason Weston, and Yoshua Bengio. A semantic matching energy
function for learning with multi-relational data. Machine Learning, 94(2):233–259, 2014.
[3] Antoine Bordes, Nicolas Usunier, Alberto Garcia-Duran, Jason Weston, and Oksana Yakhnenko.
Translating embeddings for modeling multi-relational data. In Advances in Neural Information
Processing Systems, pages 2787–2795, 2013.
[4] Antoine Bordes, Jason Weston, Ronan Collobert, and Yoshua Bengio. Learning structured embeddings of knowledge bases. In Proceedings of the Conference on Artificial Intelligence, number
EPFL-CONF-192344, 2011.
[5] David Guy Brizan and Abdullah Uz Tansel. A. survey of entity resolution and record linkage
methodologies. Communications of the IIMA, 6(3):5, 2015.
[6] Kai-Wei Chang, Wen-tau Yih, and Christopher Meek. Multi-relational latent semantic analysis.
In In Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing,
pages 1602–1612, 2013.
[7] Kai-Wei Chang, Wen-tau Yih, Bishan Yang, and Christopher Meek. Typed tensor decomposition
of knowledge bases for relation extraction. In Proceedings of the 2014 Conference on Empirical
Methods in Natural Language Processing, pages 1568–1579, 2014.
[8] Xin Dong, Evgeniy Gabrilovich, Geremy Heitz, Wilko Horn, Ni Lao, Kevin Murphy, Thomas
Strohmann, Shaohua Sun, and Wei Zhang. Knowledge vault: A web-scale approach to probabilistic
knowledge fusion. In Proceedings of the ACM SIGKDD international conference on Knowledge
discovery and data mining, pages 601–610. ACM, 2014.
[9] Johannes Hoﬀart, Fabian M Suchanek, Klaus Berberich, and Gerhard Weikum. Yago2: A spatially
and temporally enhanced knowledge base from wikipedia. In Proceedings of the Twenty-Third
international joint conference on Artificial Intelligence, pages 3161–3165. AAAI Press, 2013.
[10] Yantao Jia, Yuanzhuo Wang, Hailun Lin, Xiaolong Jin, and Xueqi Cheng. Locally adaptive
translation for knowledge graph embedding. In Proceedings of the Thirtieth AAAI Conference on
Artificial Intelligence, pages 992–998, 2016.
[11] Yankai Lin, Zhiyuan Liu, Maosong Sun, Yang Liu, and Xuan Zhu. Learning entity and relation
embeddings for knowledge graph completion. In Proceedings of the Twenty-Ninth AAAI Conference
on Artificial Intelligence, pages 2181–2187, 2015.
[12] George A Miller. Wordnet: a lexical database for english. Communications of the ACM, 38(11):39–
41, 1995.
[13] Maximilian Nickel, Volker Tresp, and Hans-Peter Kriegel. Factorizing yago: scalable machine
learning for linked data. In Proceedings of the 21st international conference on World Wide Web,
pages 271–280. ACM, 2012.
[14] HP Ritzema. Drainage principles and applications. Number 16. ILRI, 2006.
[15] Richard Socher, Danqi Chen, Christopher D Manning, and Andrew Ng. Reasoning with neural
tensor networks for knowledge base completion. In Advances in Neural Information Processing
Systems, pages 926–934, 2013.
[16] Norases Vesdapunt, Kedar Bellare, and Nilesh Dalvi. Crowdsourcing algorithms for entity resolution. Proceedings of the VLDB Endowment, 7(12):1071–1082, 2014.
[17] Zhen Wang, Jianwen Zhang, Jianlin Feng, and Zheng Chen. Knowledge graph embedding by
translating on hyperplanes. In Proceedings of the Twenty-Eighth AAAI Conference on Artificial
Intelligence, pages 1112–1119, 2014.
[18] Wentao Wu, Hongsong Li, Haixun Wang, and Kenny Q. Zhu. Probase: A probabilistic taxonomy for text understanding. In Proceedings of the ACM SIGMOD International Conference on
Management of Data, pages 481–492, 2012.

10

