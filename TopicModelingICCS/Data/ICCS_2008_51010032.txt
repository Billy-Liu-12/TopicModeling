Why Petascale Visualization and Analysis
Will Change the Rules
Hank Childs
Lawrence Livermore National Laboratory,
Box 808, L-557 Livermore, CA 94551-0808
Phone: 925-422-4035, Fax.: 925-423-6961
childs3@llnl.gov

Abstract. In the last decade, supercomputers and the scientiﬁc simulations performed on them have dramatically increased in size. Currently,
simulations can use hundreds of TeraFLOPs (trillions of ﬂoating point
operations per second) and generate many, many terabytes of data. In
the near future, we will see PetaFLOP computing and petabytes of data.
In addition, a critical step in the simulation process is “postprocessing”:
applying visualization and analysis techniques to better understand the
simulation. As a result, the issues of visualizing and analyzing massive
data sets have never been more important. This puts the spotlight on
two key issues. One, are we prepared for the unprecedented scale of data
that we will need to postprocess? And, two, assuming that we can handle data of this size, can we intelligently analyze these simulations? I will
argue that, for both of these questions, we must “change the rules” and
make dramatic departures from our current modus operandi.

M. Bubak et al. (Eds.): ICCS 2008, Part I, LNCS 5101, p. 32, 2008.
© Springer-Verlag Berlin Heidelberg 2008

