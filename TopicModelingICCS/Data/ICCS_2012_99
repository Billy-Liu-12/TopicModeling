Available online at www.sciencedirect.com

Procedia Computer Science 9 (2012) 1979 – 1987

Enhanced Encoding Techniques for the Open Trace Format 2
Michael Wagner, Andreas Kn¨upfer, Wolfgang E. Nagel

Abstract
Highly eﬃcient encoding of event trace data is a quality feature of any event trace format. It not only enables
measurements of long running applications but also reduces bias caused by intermediate memory buﬀer ﬂushes.
In this paper we present encoding techniques that will remarkably increase memory eﬃciency without introducing
overhead for the compression. We applied these techniques to the Open Trace Format 2, a state-of-the-art Open Source
event trace data format and library used by the performance analysis tools Vampir, Scalasca, and Tau. In addition, we
show that these encoding techniques are a basic step in achieving a complete in-memory event trace workﬂow.
Keywords: Encoding Techniques, Compression, Event Tracing, Trace Format, OTF2

1. Introduction
Performance analysis is the basis for eﬀective performance optimization, especially for large parallel applications.
Therefore, performance analysis tools for parallel applications form a vital part of the HPC software landscape. Of
that, event trace recording and post-mortem analysis is one of the two main branches; proﬁling is the other. The
basic idea of event tracing is to log runtime events (e.g. entering or leaving a function) together with a precise time
stamp and further event speciﬁc information. This allows a detailed post-mortem analysis of the parallel behavior
and in particular the timing, which is the most prominent criterion for computing performance [1, 2, 3]. Besides pure
contemplation of recorded events, the same method can be used as basis for detailed event replay or performance
prediction [4, 5].
The nature of event tracing is to collect and store very detailed information. Even though single event records that
describe single events are rather small, this frequently results in huge generated data volumes, because the number
of events is quite large. In all cases, a low overhead during the trace generation process is desirable – either in terms
of run-time alteration or in terms of memory consumption. For the purpose of performance analysis it is most vital
because it will reduce the disturbance of the behavior under observation. One of the components involved is the event
trace data format. It controls the in-memory buﬀer during the recording period as well as the trace ﬁle format when
data is written to permanent storage at the end of a recording period. It is responsible for overhead in terms of timing
and memory consumption.
Up to now, trace compression is done only to minimize storage space for traces on the ﬁle system. External
compression libraries are very strong with regard to memory eﬃciency as well as time eﬃciency. Nevertheless, time
for data compression is too high in relation to time granularity of event tracing. Therefore, trace compression, so far,
is only applied after the measurement has ﬁnished. Thus, the main challenge in reducing memory consumption during
Email address: michael.wagner@zih.tu-dresden.de (Michael Wagner)

1877-0509 © 2012 Published by Elsevier Ltd. Open access under CC BY-NC-ND license.
doi:10.1016/j.procs.2012.04.216

1980

Michael Wagner et al. / Procedia Computer Science 9 (2012) 1979 – 1987

run-time is to increase memory eﬃciency without decreasing time eﬃciency. This is impossible with any compression
library. The contribution of this paper is a set of encoding techniques that meet this challenge: remarkably increase
memory eﬃciency without introducing overhead for the compression. These techniques are based on and applied
experimentally to the Open Trace Format 2, a state-of-the-art Open Source event trace library used by the performance
analysis tools Vampir, Scalasca, and Tau. Therefore, a wide group of users will beneﬁt from these improvements. In
addition, these techniques can be generalized and applied to other trace formats with a similar basic design.
In the following section we give an overview over existing event trace formats and their design goals. After that,
in section 3, we describe the diﬀerent encoding techniques we applied. The results of these techniques are evaluated
in section 4. At the end, we explain our future research goals based on this results in section 5 and, ﬁnally, summarize
the paper.
2. Related Work
Event trace data formats used to be seen as an integral part of the diﬀerent performance tools only. They have
been considered as a mere transport layer moving information from the run-time measurement to the post-mortem
analysis. Also, all tools used to implement their own formats with diﬀerent APIs and slightly diﬀerent terminologies,
even though the fundamental workings and the pieces of information covered were almost the same.
This was true or still is true for the following existing formats. The CLOG and SLOG family of formats were
an integral part of the Jumpshot families of viewers [6, 7, 8]. The format contains almost ready-made graphical
representations that can be loaded selectively to visualize a selected part of the data set on screen. To the best of our
knowledge, there is no active development behind SLOG and Jumpshot anymore. The Epilog Trace format is the
native format that comes integrated with the Scalasca tool-set. It strictly uses one output ﬁle per process/thread and is
tailored towards the replay-based automatic search for well-understood typical performance problems by Scalasca [9].
The Paraver Trace Format is the native format of the Paraver trace visualizer and the Dimemas simulator. Paraver’s
format is more generic than all other formats: instead of predeﬁned data-records for all typical events it uses generic
data tuples that can be interpreted in diﬀerent ways. There exist more trace formats in the HPC performance analysis
areas. Also, there is some degree of interoperability via format converters, even though converting huge data sets is
increasingly problematic. A general overview about trace formats can be found in [10].
The ﬁrst trace format developed with interoperability in mind was the Open Trace Format (OTF) [11], created
by Technische Universit¨at Dresden (Germany), University of Oregon (USA) and the Lawrence Livermore National
Lab (USA). It consists of a deﬁned API together with a read/write library and a number of support tools. As the ﬁrst
format it was distributed as a separate software package and was adopted by the Vampir tool-set, the TAU tool-set,
and the Microsoft HPC Server built-in MPI trace collection layer, among others.
The successor version OTF2 was jointly developed by Technische Universit¨at Dresden (Germany), the J¨ulich
Supercomputing Centre (Germany), the German Research School for Simulation Sciences Aachen (Germany), Technische Universit¨at M¨unchen (Germany), University of Oregon (USA), RWTH Aachen University (Germany), and
GNS GmbH Braunschweig (Germany). It is part of Score-P, a software infrastructure for instrumentation and runtime recording, and will become the common native event trace format for Vampir, Scalasca, and TAU [12, 13]. There
have been many successive improvements in the way from early simple event trace formats to the tool-speciﬁc formats
to OTF and OTF2. For example, OTF introduced selective access to the trace data in the time and space dimensions.
OTF2 adapted the built-in buﬀering layer so it can be re-used as the in-memory record collection buﬀer in the run-time
measurement system [12].
Many event trace formats employ general-purpose data compression. Besides that, there are also special-purpose
semantic compression techniques for event trace data. An early approach was presented in [14]: conﬂating trace
data streams from separate processes or threads, if they were similar enough. Another approach is the CCG data
structure [15, 10] which captures similar reoccurring event sequences in a trace, within or across diﬀerent processes
and threads. It keeps detailed timing information and allows controlling bounds of timing deviation due to the semantic
compression. It is not intended as a general ﬁle format but as a analysis-time memory data structure. The Scalatrace
semantic trace compression [4] uses extended regular expressions to represent repeated event patterns within or across
diﬀerent processes/threads. It was designed as a ﬁle data format, yet for the purpose of event replay. Therefore, it
keeps only coarse timing histograms, which are insuﬃcient for post-mortem performance analysis. These semantic

Michael Wagner et al. / Procedia Computer Science 9 (2012) 1979 – 1987

1981

based compression techniques are all very complex and costly and do not provide an alternative to the techniques in
this paper. To the best of our knowledge, there are no event trace formats that use eﬃcient on-line trace compression,
so far.
3. Encoding Techniques
In this section we will present the diﬀerent encoding improvements that noticeably increase memory eﬃciency of
the trace format. The ﬁrst two optimizations are already implemented in the release version of OTF2: the leading zero
elimination and the separation of timing information and event data. Both were already presented in [12]. Hence, they
are the basis for all further encoding techniques and will be described shortly in the following as well. As far as we
know, all other presented techniques form a novel approach in on-line trace compression. They were neither presented
nor implemented in any format, yet. All measurements in Section 4 present the advantages of the new techniques in
comparison to OTF2, which already contains the ﬁrst two techniques.
But ﬁrst, we want to explain the basic memory representation of event records, e.g. in OTF. An event record
consists of three main parts: ﬁrst a record token that deﬁnes the type of an event (e.g. entering or leaving a code
region, sending or receiving a message, etc.); second, a time stamp telling when the event occurred; and third, event
speciﬁc attributes, e.g. a region ID for an region enter record (see Figure 1).








      	 
         


Figure 1: Basic memory representation of event records: ﬁrst a one-byte record token that deﬁnes the type of an event (e.g. entering or leaving a
code region, sending or receiving a message, etc.); followed by a time stamp of eight bytes telling when the event occurred; and third, event speciﬁc
attributes (e.g. a region ID for an region enter record) with four bytes each.

3.1. Splitting of Timing Information and Event Data
Most trace formats store events and their according timing information in a single event record (see Figure 1). This
is consequential to the general event tracing approach because an event is only meaningful with information about the
time it occurred. However, the advantage of storing timing and event data separately (see Figure 2) is the elimination
of redundant timing information for consecutive events with same time stamp, which occurs quite often. For example,
calling MPI Send() results in the following event pattern: an enter event for the MPI Send region, an MPI send event
with the message-speciﬁc information, and an leave event for the MPI Send region. The timing information of the
MPI send event is matched either to the enter or the leave event. This means, for this example, there are only two time
stamps stored instead of three, i.e. a reduction of 8 bytes. The trade oﬀ is an additional byte for a token for the new
time stamp record.




      	
  

 






   
     

Figure 2: Splitting of timing information and event data results in two separate records: ﬁrst a record storing only the time stamp that is valid until
the next time stamp record; and second, a record storing only the event attributes.

3.2. Leading Zero Elimination
The memory reserved for single attributes of an event is usually determined by the largest value this element has
to represent. Therefore, a region ID is typically stored in a 32-bit integer while a performance counter is stored in a
64-bit integer. However, most values are much smaller than the hypothetical maximum but result in the same memory

1982

Michael Wagner et al. / Procedia Computer Science 9 (2012) 1979 – 1987

allocation. By omitting bytes that are zero, memory allocation for most values is reduced. To still be able to read this
not ﬁx-length representation of the value the number of remaining data bytes has to be stored in front of the value.
Figure 3 shows this for a simple event record with a four-byte and an eight-byte attribute.
	
 



            
       


Figure 3: Leading zero elimination: all bytes that are zero are omitted. The number of the remaining data bytes its stored in front of them.

3.3. Diﬀerence Storing
Basically, there are two diﬀerent types of values that need to be stored: monotonic increasing values like time
stamps and arbitrary values like region IDs. Furthermore, some of the monotonic increasing values begin with a very
high oﬀset. Storing only the diﬀerence to the previous value leads to much smaller values to store. In combination
with the described leading zero elimination this results in less memory allocation for the stored value. At the moment
this is only applied to time stamps. To still allow random access to events, the buﬀer is divided in several small
blocks. Each block begins with a complete time stamp. This way, access to events is not completely random but it is
not necessary to read the whole trace from the beginning.
3.4. Bit-Level Encoding
The previous optimizations can be optimized even more by encoding very small numbers directly into the token
byte. With the leading zero elimination (see Section 3.2) a token is always followed by a very small number: the
number of remaining data bytes. This number can be easily encoded in the token byte by simply adding the value to
the token (see Figure 4). This eliminates the additional byte storing the number of data bytes. Of course, we had to
consider the addition in the distribution of token IDs to still enable an unique identiﬁcation of events. Because the
token byte is limited to 256 values, only the most frequently occurring events (see Figure 5) use this encoding: time
stamps and enter/leave records. To make decoding easier there are token for each record length: time stamp with one
data byte, time stamp with two data bytes, ..., time stamp with eight data bytes. With these two optimizations (leading
zero elimination and bit-level encoding) the size of the sample record is reduced from thirteen to seven bytes.
 	


	


       
      

	
Figure 4: Merging of token and length byte: the number of remaining data bytes of the ﬁrst attribute is encoded in the token byte by simply adding
the value to the token.

3.5. Event Distribution and Encoding Implications
So far, the presented encoding techniques are a beneﬁt for most event records with a reduction of memory allocation. But to reduce the overall memory allocation even more, it was necessary to identify those event records
that consume a lot of memory in total and, therefore, are preferential for further optimization. Figure 5 shows the
results of our survey: the distribution of the diﬀerent event record types is far from uniform. Enter and leave events

Michael Wagner et al. / Procedia Computer Science 9 (2012) 1979 – 1987

1983

are the dominating events in typical application traces. In addition, most traces contain only a moderate number of
program regions. This means the region ID parameter in enter and leave event records is moderately small. Therefore,
these two events bear a high potential for further memory reduction. As a result of this statistical survey enter and
leave event records are treated diﬀerently from other event records. As mentioned before, enter records are already
optimized by bit-level encoding. If the value of an enter’s region ID is small enough1 the value is directly encoded in
the event token (see Figure 6). Otherwise only the length of the following data bytes, as described before, is encoded
in the event token. This means that for many application traces the size of most enter records is reduced to only one
single byte. For leave records the region ID is not stored anymore but can be delivered by keeping track of the function
call stack. Thus, leave records always require only a single byte. That means, that the size of most enter and all leave
records is reduced from ﬁve bytes to a single byte.

Memory Distribution by Event Category (in %)

Enter/Leave
MPI P2P

MPI Collective
Others

100
90
80
70
60
50
40
30
20
10

lu
7.
13
rro
co
so
0.
13
f
_t
ra
te
9.
12

rf2
w
7.
12
ps
m
m
la
6.
12
n
yo
ch
ta
2.
12

p2
po
1.
12

s4
fd
5.
11
3d
ie
sl
le
7.
10

ilc
m
4.
10

Figure 5: Memory allocation by event type for SPEC MPI 2007 applications: Enter and leave events are the dominating events.

	
		
 

	

Figure 6: Merging of token and value: if the value of an enter’s region ID is small enough the value is directly encoded in the event token

1 The 256 values in the token byte are distributed among three domains: token for events, reserved token for upcoming events, and the remaining
token for directly encoding enter events including region ID.

1984

Michael Wagner et al. / Procedia Computer Science 9 (2012) 1979 – 1987

3.6. Timer Resolution Reduction
Typically, tracing tools check for available timer sources and use the one with the highest resolution e.g. a CPU
cycle counter. However, events occur in a much lower frequency. In addition, the remaining error after post-mortem
synchronization of non-global timer sources is usually in the range of microseconds [16]. Therefore, it is possible
to reduce the stored timer resolution of high frequent timers without perceivably degrading the timing information.
The reduced timer resolution then is still a few orders higher than the event frequency. This ensures that derived
values (e.g. total execution time of a function) can still be computed correctly. An optimal reduction factor2 can be
determined after the measurement with less eﬀort but it is very diﬃcult to do it in before. This is still work in progress.
Thus, we use only a very slight reduction for high resolution timers from our experience. No reduction is done for
medium and small resolution timers to ensure the program behavior is not modiﬁed.
4. Evaluation
Most signiﬁcant in evaluating an event tracing library besides scalability and usability, are memory consumption
and the introduced runtime overhead. The presented encoding enhancements will only aﬀect the internal memory
representation. Thus, they will not lead to an impact on OTF2’s usability or scalability. However, they will inﬂuence
the consumed memory and runtime overhead. In the following we will show that the optimizations will drastically
reduce the memory allocation without increasing the runtime overhead. Therefore, we compared OTF2 with and
without the encoding techniques as well as its predecessor OTF.
However, comparing diﬀerent event trace formats is not a trivial task. In most cases, event tracing libraries are
very closely integrated in the according event tracing tools which generate their own individual overhead. Also, they
generate diﬀerent values for events, e.g. time stamps. On the other hand, generating events with random or ﬁxed
values is not an option because compression ratio strongly relates to the type of input data and, therefore, would lead
to misleading results. Thus, we developed a simulator that reads in an existing trace ﬁle and writes the records to the
diﬀerent event trace formats. With this, all three event traces contain exactly the same real-life data.
As reference event traces we used a subset of the SPEC MPI 2007 benchmarks3 that represent a variety of applications from diﬀerent research ﬁelds, diﬀerent communication behaviors and diﬀerent length, i.e. trace sizes.
4.1. Runtime Memory Consumption
The primary goal of the encoding enhancements is to drastically increase memory eﬃciency, which leads to less
interrupts for storing collected data to disk and, therefore, more accurate results. Next to that, the overall storage size
of an experiment drops to a much smaller amount. Figure 7 shows the eﬀects of the presented encoding techniques.
To be able to classify the capability of these enhancements we compared them to the memory eﬃciency achieved
with zlib4 - a well established general purpose compression library. The ﬁgure shows the results for OTF, OTF2,
OTF2 with zlib compression, OTF2 only with the lossless enhancements (ee), and OTF2 additionally with the lossy
timer resolution reduction (rtr). The results show that the encoding enhancements reduce the memory consumption
by a factor of 3.6 to 5.8 and, therefore, realize memory eﬃciency equal to or better than a compression library. An
additional nice eﬀect is that the compression ratio with zlib of trace data with enhanced encoding is 5.4 to 9.0. This
means, it is still notably higher than without enhanced encoding which compresses with a ratio of 3.5 to 4.7.
4.2. Runtime Overhead
Next to the memory eﬃciency, time eﬃciency is equally important for a successful measurement of an application.
Introducing to much overhead will drastically reduce the accuracy of a measurement or might even record a diﬀerent
behavior than the real application. This means, an event trace format as part of an event tracing tool has to introduce
as less overhead as possible. Therefore, improvements in memory eﬃciency must be judged by the overhead they
introduce.
2 An optimal reduction factor is the highest possible reduction before the program behavior is modiﬁed in any way i.e. events that occur at
diﬀerent original time stamps occur at the same reduced time stamp.
3 http://www.spec.org/mpi2007
4 http://zlib.net/

Michael Wagner et al. / Procedia Computer Science 9 (2012) 1979 – 1987

Memory consumption normalized to OTF (in %)

OTF
OTF2

OTF2 + zlib
OTF2 + ee

1985

OTF2 + ee + rtr

100
90
80
70
60
50
40
30
20
10

lu
7.
13
rro
co
so
0.
13
f
_t
ra
te
9.
12

rf2
w
7.
12
ps
m
m
la
6.
12
n
yo
ch
ta
2.
12

p2
po
1.
12

s4
fd
5.
11
3d
ie
sl
le
7.
10

ilc
m
4.
10

Figure 7: Memory consumption of OTF without zlib compression, OTF2 without zlib compression, OTF2 with zlib compression, OTF2 only with
the lossless enhancements (ee), and OTF2 additionally with the lossy reduced timer resolution (rtr). All values are normalized to OTF’s memory
consumption.

Table 1 shows on the left side the time consumption of OTF, OTF2, OTF2 with zlib compression, and OTF2 with
enhanced encoding. It shows two important results: First, the enhanced encoding does not introduce new overhead.
The overhead is in the same range as OTF2 without the enhanced encoding. Second, using a compression library
to increase memory eﬃciency will drastically decrease time eﬃciency. Even more, overhead introduced by a compression library is usually not equally distributed over all events. In most cases, a certain amount of data is collected
and then compressed in one step to achieve a high compression ratio. In our measurements, compressing every 1 MB
creates an overhead of approx. 24 ms in average. Therefore, using a compression library for higher memory eﬃciency
is unfavorable.
Table 1: Time and memory consumptions of OTF, OTF2, OTF2 with zlib, and OTF2 with enhanced encoding (including timer resolution reduction)
for selected SPEC MPI 2007 test cases. All values are normalized to OTF’s time or memory consumption, respectively.

Trace Format
OTF
OTF2
OTF2 with zlib
OTF2 with enhanced encoding

Minimum
100.0 %
52.8 %
231.4 %
50.9 %

Time
Average
100.0 %
54.1 %
249.9 %
51.5 %

Maximum
100.0 %
55.2 %
285.5 %
52.4 %

Memory Consumption
Minimum Average Maximum
100.0 % 100.0 %
100.0 %
75.9 %
80.6 %
85.1 %
17.7 %
19.5 %
23.1 %
13.8 %
17.8 %
23.8 %

With this, the goal to drastically increase memory eﬃciency without increasing runtime overhead was clearly achieved.

1986

Michael Wagner et al. / Procedia Computer Science 9 (2012) 1979 – 1987

5. Future Work
One of the most urgent challenges in event tracing is still the massive amount of data collected during a tracing run.
And, this amount of data is steadily growing in three dimensions: ﬁrst, by targeting even more detailed information;
second, for even longer application runs; and third, on even bigger number of cores. Especially the ladder is already
pushing against the limits of today’s – and sadly, most likely also tomorrow’s – parallel ﬁle systems. Without any
special handling, current tracing tools are capable to handle about ten or twenty thousand of parallel processes. There
are some ﬁrst steps to overcome today’s limits and push them to hundreds and thousands processes [17, 18]. But what
about millions or tens of millions of cores?
Next to that, current data volumes are already way to big to be analyzed on single or a few processes. So mostly,
event trace analysis is already realized in a highly parallel distributed fashion [2]. Some tools like Scalasca [3] use
even the same number of cores for measurement and analysis. However, the typical workﬂow still is, to record an
application’s behavior, store it to disk, and then read it in again to analyze it. Obviously, it seems a good idea to
just keep the data in main memory for the complete workﬂow. This will not only provide advantages in ﬁle system
interaction but will reduce total time to insight. In addition, it will enable new ways of event tracing. Our ﬁnal goal is
to enable interactive event tracing. Only some potential features could be to start an application measurement with a
set of tracing parameters, e.g. number of counters, level of detail, etc., then stop at a deﬁned point, look into the ﬁrst
part of the measurement, get ﬁrst insights, and then decide how to go on: delete unnecessary parts from the recording
buﬀer, set a new level of detail, choose diﬀerent counters, start over, abort, etc. This should only give a ﬁrst impression
on the possibilities this new approach might oﬀer.
But, there is one catch: keeping the data in main memory the whole time implies that always all data ﬁts into a
single memory buﬀer. Unfortunately, this is not the case today. Quite contrary, they produce hundreds of megabytes
to tens of gigabytes of data per process. In addition, a tracing buﬀer should only use a small part of total main memory
because typically the measured application needs most of it. Therefore, data is not only stored to ﬁle at the end of
each measurement run, but every time a memory buﬀer is exhausted, which can be quite often.
That is why, it is absolutely essential to ﬁt all data into a single memory buﬀer. In our opinion there are three
key parts in achieving this goal: intelligent high-level ﬁlters and phase-based selection form the ﬁrst part. So, only
valuable events are stored. Of course, this is not as trivial as it sounds. Currently, we study the eﬀects of runtime loop
phase selection by classifying loop phases and storing only a representative of each loop class. There is a wide ﬁeld of
diﬀerent ﬁlters and selections to research, which can achieve very good results, when combined together. The second
is a very eﬃcient storage of events. This topic is addressed in this paper but we think there is still more potential. In
contrast to this two parts that reduce the amount of data by a factor, the last part has to reduce whatever amount of
data is left to a ﬁxed size – the size of the tracing buﬀer. This last step is a very important one because once this step is
realized, it guarantees that any reasonable measurement will ﬁt into a single tracing buﬀer – and this, of course, with
very little overhead. This step, called real-time event reduction, is our current subject of research.
Therefore, the presented encoding enhancements provide a great advancement for today’s event tracing tools, yet,
they are only one step on the path to a complete in-memory event tracing workﬂow, which is the basis for an interactive
event tracing approach.
6. Conclusion
In this paper, we presented novel techniques for an enhanced encoding in the Open Trace Format 2. By applying
these techniques we reduced the memory allocation for event trace data during measurement runtime by a factor up
to 5.8 without increasing the overhead of the tracing library. With this, we achieved the memory eﬃciency of wellestablished compression libraries without their negative impact on the recorded application behavior. Furthermore,
we presented our future goal in providing a complete in-memory event tracing workﬂow that enables the user to get
results faster and less prone to bias by intermediate memory buﬀer ﬂushes. In addition, it is the basis to evolve from
post-mortem event trace analysis to interactive event trace analysis.

Michael Wagner et al. / Procedia Computer Science 9 (2012) 1979 – 1987

1987

[1] H. Brunst, D. Hackenberg, G. Juckeland, H. Rohling, Comprehensive Performance Tracking with Vampir 7, in: M. S. M¨uller, M. M. Resch,
A. Schulz, W. E. Nagel (Eds.), Tools for High Performance Computing 2009, 2010, pp. 17–29.
[2] M. S. M¨uller, A. Kn¨upfer, M. Jurenz, M. Lieber, H. Brunst, H. Mix, W. E. Nagel, Developing Scalable Applications with Vampir, VampirServer and VampirTrace, in: C. Bischof, M. B¨ucker, P. Gibbon, G. R. Joubert, T. Lippert, B. Mohr, F. J. Peters (Eds.), Parallel Computing:
Architectures, Algorithms and Applications, Vol. 15 of Advances in Parallel Computing, IOS Press, 2008, pp. 637–644.
URL http://www.booksonline.iospress.nl/Content/View.aspx?piid=8455
´
[3] M. Geimer, F. Wolf, B. J. N. Wylie, E. Abrah´
am, D. Becker, B. Mohr, The Scalasca Performance Toolset Architecture, Concurr. Comput. :
Pract. Exper. 22 (2010) 702–719. doi:10.1002/cpe.v22:6.
[4] M. Noeth, P. Ratn, F. Mueller, M. Schulz, B. R. de Supinski, ScalaTrace: Scalable compression and replay of communication traces for
high-performance computing, J. Parallel Distrib. Comput. 69 (2009) 696–710. doi:10.1016/j.jpdc.2008.09.001.
URL http://dl.acm.org/citation.cfm?id=1556512.1556729
[5] S. Girona, J. Labarta, R. M. Badia, Validation of Dimemas Communication Model for MPI Collective Operations, in: Proceedings of the 7th
European PVM/MPI Users’ Group Meeting on Recent Advances in Parallel Virtual Machine and Message Passing Interface, Springer-Verlag,
London, UK, 2000, pp. 39–46.
URL http://dl.acm.org/citation.cfm?id=648137.746640
[6] C. E. Wu, A. Bolmarcich, M. Snir, D. Wootton, F. Parpia, A. Chan, E. Lusk, W. Gropp, From Trace Generation to Visualization: A Performance Framework for Distributed Parallel Systems, in: Proc. of SC2000: High Performance Networking and Computing, Dallas, TX, USA,
2000.
[7] A. Chan, W. Gropp, E. Lusk, Scalable Log Files for Parallel Program Trace Data (draft), Tech. rep., Argonne National Laboratory (2000).
[8] A. Chan, W. Gropp, E. Lusk, An Eﬃcient Format for Nearly Constant-Time Access to Arbitrary Time Intervals in Large Trace Files, Scientiﬁc
Programming 16 (2-3) (2008) 155–165.
[9] F. Wolf, B. Mohr, EPILOG Binary Trace-Data Format, Tech. Rep. FZJ-ZAM-IB-2004-06, Forschungszentrum J¨ulich (May 2004).
[10] A. Kn¨upfer, Advanced Memory Data Structures for Scalable Event Trace Analysis, S¨udwestdeutscher Verlag f¨ur Hochschulschriften, ISBN
3838109430, Sept. 2009, Dissertation an der Technischen Universit¨at Dresden, Fakult¨at f¨ur Informatik, Dez. 2008.
[11] A. Kn¨upfer, R. Brendel, H. Brunst, H. Mix, W. E. Nagel, Introducing the Open Trace Format (OTF), in: Computational Science ICCS 2006:
6th International Conference, LNCS 3992, Springer, Reading, UK, 2006.
[12] D. Eschweiler, M. Wagner, M. Geimer, A. Kn¨upfer, W. E. Nagel, F. Wolf, Open Trace Format 2 – The Next Generation of Scalable Trace
Formats and Support Libraries, in: Proc. of the International Conference on Parallel Computing (ParCo), Ghent, Belgium, 2011, accepted for
publication.
[13] A. Kn¨upfer, C. R¨ossel, D. an Mey, S. Biersdorf, K. Diethelm, D. Eschweiler, M. Gerndt, D. Lorenz, A. D. Malony, W. E. Nagel, Y. Oleynik,
P. Saviankou, D. Schmidl, S. Shende, R. Tsch¨uter, M. Wagner, B. Wesarg, F. Wolf, Score-P – A Joint Performance Measurement Run-Time
Infrastructure for Periscope, Scalasca, TAU, and Vampir, in: Proc. of the Intl. Parallel Tools Workshop, Dresden, Germany, 2011, (accepted
for publication).
[14] O. Y. Nickolayev, P. C. Roth, D. A. Reed, Real-Time Statistical Clustering for Event Trace Reduction, The International Journal of Supercomputer Applications and High Performance Computing 11 (2) (1997) 144–159.
[15] A. Kn¨upfer, W. E. Nagel, Compressible Memory Data Structures for Event-Based Trace Analysis, Future Generation Computer Systems
22 (3) (2006) 359–368. doi:10.1016/j.future.2004.11.021.
URL http://www.sciencedirect.com/science/article/B6V06-4F53N8K-4/2/0590754c4f23d4f68d0308566b945168
[16] J. Doleschal, A. Kn¨upfer, M. S. M¨uller, W. E. Nagel, Internal Timer Synchronization for Parallel Event Tracing, in: A. Lastovetsky,
T. Kechadi, J. Dongarra (Eds.), Proceedings of the 15th European PVM/MPI Users’ Group Meeting on Recent Advances in Parallel Virtual Machine and Message Passing Interface, Springer-Verlag, Berlin, Heidelberg, 2008, pp. 202–209. doi:10.1007/978-3-540-87475-1 29.
[17] N. Ali, P. Carns, K. Iskra, D. Kimpe, S. Lang, R. Latham, R. Ross, L. Ward, P. Sadayappan, Scalable I/O Forwarding Framework for HighPerformance Computing Systems, in: Proceedings of the 11th IEEE International Conference on Cluster Computing (CLUSTER), 2009.
[18] W. Frings, F. Wolf, V. Petkov, Scalable Massively Parallel I/O to Task-Local Files, in: Proceedings of the Conference on
High Performance Computing Networking, Storage and Analysis, SC ’09, ACM, New York, NY, USA, 2009, pp. 17:1–17:11.
doi:http://doi.acm.org/10.1145/1654059.1654077.
URL http://doi.acm.org/10.1145/1654059.1654077

