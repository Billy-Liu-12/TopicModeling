Single Data Copying for MPI Communication
Optimization on Shared Memory System
Qiankun Miao1 , Guangzhong Sun1, , Jiulong Shan2 , and Guoliang Chen1
1

Anhui Province-MOST Key Co-Lab of High Performance Computing
and Its Applications, Department of Computer Science,
University of Science and Technology of China, Hefei, 230027, P.R. China
2
Microprocessor Technology Lab, Intel China Research Center, Beijing, China
miao@mail.ustc.edu.cn, gzsun@ustc.edu.cn, jiulong.shan@intel.com,
glchen@ustc.edu.cn

Abstract. Shared memory system is an important platform for high
performance computing. In traditional parallel programming, message
passing interface (MPI) is widely used. But current implementation of
MPI doesn’t take full advantage of shared memory for communication.
A double data copying method is used to copy data to and from system
buﬀer for message passing. In this paper, we propose a novel method to
design and implement the communication protocol for MPI on shared
memory system. The double data copying method is replaced by a single data copying method, thus, message is transferred without the system buﬀer. We compare the new communication protocol with that in
MPICH an implementation of MPI. Our performance measurements indicate that the new communication protocol outperforms MPICH with
lower latency. For Point-to-Point communication, the new protocol performs up to about 15 times faster than MPICH, and it performs up to
about 300 times faster than MPICH for collective communication.

1

Introduction

Message Passing Interface (MPI) [1] is a standard interface for high performance
parallel computing. MPI can be used on both distributed memory multiprocessor
and shared memory multiprocessor. It has been successfully used to develop
many parallel applications on distributed memory system.
Shared memory system is an important architecture for high performance
computing. With the advent of multi-core computing [4], shared memory system will draw more and more attention. There are several programming models
for shared memory system, such as OpenMP [2] and Pthread [3]. However, we
still need to program on shared memory system with MPI. First, it is required
to reuse the existing MPI code on distributed memory system for quickly developing applications on shared memory system. Second, applications developed
using MPI are compatible across a wide range of architectures and need only a
few modiﬁcations for performance tune-up. Third, the future computation will
Corresponding author.
Y. Shi et al. (Eds.): ICCS 2007, Part I, LNCS 4487, pp. 700–707, 2007.
c Springer-Verlag Berlin Heidelberg 2007

Single Data Copying for MPI Communication Optimization

701

shift to multi-core computation. Consequently, exploiting the parallelism among
these cores becomes especially important. We must ﬁgure out how to program
on them to get high performance. MPI can easily do such works due to its great
success on parallel computing today.
It is diﬃcult to develop an eﬃcient MPI application on shared memory system, since the MPI programming model does few considerations of the underlying architecture. In the default communication protocol of MPI, receiver copies
data from system buﬀer after sender copied the data to the system buﬀer. Usually a locking mechanism is used for synchronization on shared memory system,
which has a high overhead. Therefore, MPI programs suﬀer severe performance
degradation on shared memory system. In this paper, we propose techniques to
eliminate these performance bottlenecks. We make use of the shared memory
for direct communication bypass buﬀer copying instead of double data copying.
Meanwhile, A simple busy-waiting polling method is used to reduce expense of
locking for synchronization. To evaluate these techniques, some experiments are
carried out on basic MPI primitives. We make comparisons of the performance
between the new implementation and the original implementation in MPICH
[5]. Results indicate that the new implementation is able to achieve much higher
performance than MPICH do.
There are other works studying on the optimized implementation of the communication in MPI programs on shared memory system. A lock-free scheme is
studied on NEC SX-4 machine [6]. They use a layered communication protocol
for a portable consideration. A shared memory communication device in MPICH
is conducted for Sun’s UltraEnterprise 6000 servers [7]. They use a static memory management similar as we describe in Section 2. TMPI [8] use a threadlevel approach for communication optimization. TMPI can achieve signiﬁcant
performance advantages in a multiprogrammed environment. We consider the
condition only a process per processor. This condition is usually happened in
real computation. Our implementation is not restricted to a certain machine.
We could achieve lower latency than the native MPI implementation.
The rest of the paper is organized as follows. Next section introduces the
default communication implementation of MPI and discusses its drawbacks on
shared memory system. Section 3 describes the design and implementation of the
new approach used in this paper, which makes eﬃcient communication on shared
memory system. Section 4 describes our evaluation methodology and presents the
evaluation results. Section 5 concludes the paper and presents the future work.

2

Motivation

In traditional implementation of MPI, a general shared memory device is needed,
which can be used on many diﬀerently conﬁgured systems. There is a limitation
that only a small memory space shared by all processes can be allocated on some
systems. As a result, the communication between two processes is through a shared
memory system buﬀer [9]. The system buﬀer can be accessed by all the processes
in a parallel computer. We indicate this data transmission procedure in Fig. 1.

702

Q. Miao et al.
Shared System Buffer

Mem ory Spac e of A

Mem ory Spac e of B

Fig. 1. Communication in original MPI, the message packet is transmitted by copying
them into and out of the shared system buﬀer

A typical implementation of communication between two processes could be
as follows. Suppose process A intends to send a message to process B. We use A
to denote process A and B to denote process B following. There is a free shared
memory segment pre-allocated and a synchronization ﬂag can be accessed by A
and B. The synchronization ﬂag is used to indicate whether the shared memory
segment is ready for A to write to or for B to read from. At the beginning,
A checks the synchronization ﬂag. If the shared memory segment is ready for
writing, A copies data to be transmitted from its user buﬀer into the shared
memory segment. Then A sets the synchronization ﬂag after ﬁnishing the data
copying. This indicates that the data in the shared memory segment is ready
to be copied to B’s user buﬀer. A can either wait for an acknowledgement after
copying or it can continue execution. If the shared memory segment is not ready
for A, it chooses a back-oﬀ strategy, and tries again later. Meanwhile, B can
check for a forthcoming data packet by probing the synchronization ﬂag. When
it ﬁnds the ﬂag is set, B copies the data in shared memory segment to its own
user buﬀer. After the data transmitted completely, B clears the synchronization
ﬂag. At this time new communication can start. MPICH implements the above
mechanism as an abstract device interface ch shmem [7].
The communication strategy above is ineﬃcient for a few reasons.
1. A double data copying protocol is required for each data exchange. But this
is unnecessary and will increase the burden of memory/cache system. It will
result in high latency of the communication.
2. A shared system buﬀer is required where the data is copied to and from.
This is an extreme waste of capacity of memory. On shared memory system,
local memory and cache capacity as well as aggregate bandwidth will limit
the scalability of MPI implementation.

Single Data Copying for MPI Communication Optimization

703

3. The cost of back-oﬀ strategy for synchronization is extremely large. The
synchronization cost can adversely aﬀect communication performance for
short message.
4. Furthermore, for those collective communications (e.g. MPI Bcast), the extra
copy and complex synchronization will aggravate these problems.
In a word, the critical problem for the communication on shared memory
system is how to reduce the memory/cache capacity and the waiting time for
synchronization. In the next section, we will propose techniques to solve these
problems.

3

Design and Implementation

To solve the problems existing in MPI for shared memory system, we design
new communication protocols. We use primitives (IPC/shm) to create process
level shared memory segment for message to be transmitted. Communication
among processes uses a single data copying protocol. Considering usually only
one process per processor we choose a simple busy-waiting polling strategy for
synchronization.
3.1

Optimized Communication Protocol

In Section 2, we have described the default implementation of communication in
MPI. In order to transmit data among processes, a double data copying protocol
is required. One is used to copy the data into shared memory, and the other is
used to copy the data from the shared memory. However, the double data copying
can be reduced to only one data copying when the sender process need retain an
independent copy of the data. Even no data copying is required when the sender
process needn’t retain a copy of the data or all the processes share the same
data structure and do operation on disjoint parts of the data structure. We can
allocate a shared memory segment for the data to be transmitted. Thus, every
process can access this shared segment. Data in the shared memory segment
can be copied to the receiver process’s user buﬀer either in the shared memory
or in its private space. If all the processes deal with disjoint parts of the same
data structure, we need no data transmission because after one process updated
the data all other processes can see the update immediately and they can use
IPC/shm to directly read the data. This technique is able to obtain lower latency
and less memory consumption, because it needs only one or no data copying and
no extra system buﬀer. Fig. 2 illustrates this technique.
According to the above discussion, we can model the required time for sending
a message packet with size of n bytes by the following formulas. We use Toriginal
to indicate the communication time of the original version and Toptimized to
indicate that of our optimized implementation.
Original MPI device using two data copying:
Toriginal = 2Tdatacpy + Tsyn

(1)

704

Q. Miao et al.

IPC

Mem ory Spac e of A(shared)

Mem ory Spac e of B

Fig. 2. Optimized communication with only a single buﬀer copying by employing IPC

Optimized implementation using single data copying:
Toptimiezed = Tdatacpy + Talloc + Tf ree + Tbwsyn

(2)

Optimized implementation using no data copying:
Toptimiezed = Talloc + Tf ree + Tbwsyn

(3)

Where Tdatacpy represents the time for one data copying, Tsyn represents the time
for communication synchronization in original version, Talloc and Tf ree represent
the time for shared memory allocating and deleting respectively, Tbwsyn represents the time for communication synchronization using busy-waiting strategy
in our implementation.
In general case, the cost of the shared memory segment allocation and free
is very little compared with the cost of one data copying. So, Toriginal is larger
than Toptimized . This indicates that we could achieve higher performance using
the optimized communication protocol.
3.2

Busy-Waiting Polling Strategy

A synchronization mechanism is used to prevent the processes from interfering
with each other during the communication procedure. It deﬁnes the time when
sender process makes data ready and the time when receiver process completes
the data copying. Usually the mechanism is provided by locks and semaphores
on shared memory system. Unfortunately, typical implementations of lock tend
to cause a system call, which is often very time-consuming. So, the cost of synchronization by using a lock is too expensive to provide high performance. When
the lock is not free, a back-oﬀ strategy is used. This may delay the starting time
of communication. Though the lock is already free, it may still need to wait for
the process to be active again.
With the assumption that there is usually only one application process per
processor and processor resources are not needed for other tasks, we use a simple
busy-waiting polling approach to replace exponential back-oﬀ strategy. An application process repeatedly tests the synchronization ﬂag as frequently as possible

Single Data Copying for MPI Communication Optimization

705

to determine when it may access the shared memory. Once the process has found
the synchronization ﬂag switched, it would immediately detect the change. Consequently, the data transmission can start without delay. This polling strategy
would reduce the time for synchronization on shared memory system when there
is only one process per processor.

4

Performance Evaluation

We conduct experiments to study the performance of our implementation. Latency time is collected for multiple repetitions of each operation over various
message sizes between 0 byte and 1 megabytes. We use latency time here to
denote the total transmission time of a message among processes. For 0 byte
message, latency time is only the overhead of communication start-up and end.
All the results given are averaged over multiple runs.
4.1

Platform Conﬁguration

The target system is a 16-way shared memory multiprocessor system running
Suse Linux 9.0 platform. It has 16 x86 processors running at 3.0 GHz, 4 levels
of cache with each 32MB L4 cache shared amongst 4 CPUs. As for the interconnection, the system uses two 4x4 crossbars. We use MPICH-1.2.5.2 (aﬃliated
with device ch shmem and gcc 3.3.3) library form Argonne National Laboratory
to generate the executables. Each process is bound to a processor to prevent
operating system from migrating processes among processors.
4.2

Performance Evaluation Results

Point-to-Point communication transmits data between a pair of processes. We
choose the standard MPI Send/Recv for our test. Collective communication is
often used in real applications. Usually it involves a large number of processes.
This means that there is a large amount of data to be transmitted among the involved processes. So, its performance heavily depends on the implementation. We
investigate MPI Bcast, MPI Gather, MPI Scatter, MPI Alltoall, MPI Reduce to
evaluate the collective communication with new communication approach. We
only present the results of MPI Bcast due to the limited paper size. In MPI Bcast
a root process broadcasts a message to all the other processors.
We present the experimental results in Fig. 3. We compare two implementations of Send/Recv in the left of Fig. 3, one is the default version in MPICH
which is labeled Original Send/Recv, the other is the optimized version which is
labeled New Send/Recv. In the right of Fig. 3, we compare two implementations
of Bcast which are labeled Original Bcast and New Bcast respectively.
From Fig. 3, we can see that our new implementation outperforms the default
implementation in MPICH. Our implementation has lower latency on various
size message for both point-to-point communication and collective communication. Our implementation is almost six times faster than MPICH for short

706

Q. Miao et al.

Fig. 3. Left: Latency time of original MPICH Send/Receive (upper line) and optimized
implementation (lower line). Right: Latency time of original MPICH Bcast (upper line)
and optimized implementation (lower line).

message and up to ﬁfteen times faster for long message. Our implementation
gains great success for collective communication. The optimized implementation
is about ﬁve times faster than MPICH for short message. It is two orders of
magnitude faster than MPICH when the message size is larger than 1kBytes.
For message size from 1 bytes to 4kBytes, the transmission time of broadcast is
almost no increase in the new implementation. That’s because all other processes
can directly copy data from the sender process’s user buﬀer simultaneously, and
the transmission time is determined by the cost of memcpy() which is almost
same for various short data sizes. As the message size is large, memory bandwidth contention will limit the communication performance, the transmission
time increases with message size. When the message is so large that the system
buﬀer pre-allocated for communication of the original MPI implementation can’t
hold the total message at one time, it is required to split the message to several
small pieces and transmits one piece at a time. We can see a step-up increase
of the transmission time in MPICH from Fig. 3. However, all receivers can still
directly copy from the sender’s buﬀer in the optimized implementation, since
the optimized implementation isn’t limited on message size.

5

Conclusion

In this paper, we present an optimized implementation of the communication
signiﬁcantly improves the performance of MPI communication on shared memory system. The optimized methods include a single data copying protocol and
a busy-waiting polling strategy for synchronization. Some experiments are conducted to evaluate the performance of a few basic MPI primitives with the
optimized communication. The experimental results indicate that the primitives

Single Data Copying for MPI Communication Optimization

707

with optimized communication achieved lower latency than the native version
in MPICH.
For future work, we intend to use the methods to develop more real-world
applications on shared memory system and make a source to source translator
to translate the MPI programs written for distributed system into an eﬃcient
shared memory version. We will also compare the overall performance of an
application written using optimized message passing with that of an application
written using shared memory programming model such as OpenMP.

Acknowledgements
This work is supported by the National Natural Science Foundation of China
No.60533020. This work is partially ﬁnished at Intel China Research Center.
We would like to thank the anonymous referees for their useful suggestions to
improve the presentation of this paper.

References
1. The MPI Forum. The MPI Message-Passing Interface Standard. http://www.mpiforum.org/. (1995).
2. OpenMP Standards Board. OpenMP: A Proposed Industry Standard API
for Shared Memory Programming. http://www.openmp.org/openmp/mpdocuments/paper/paper.html. (1997).
3. Pthread interface. ANSI/IEEE Standard 1003.1. (1996)
4. M. Greeger. Multicore CPUs for the Masses. ACM Queue, 3(7) (2005):63–64
5. W. Gropp, E. Lusk: Skjellum A, Doss N. MPICH: A high-performance, portable
implementation for the MPI message-passing interface. Parallel Computing 1996;
22, (1996) pp. 789–928.
6. W. Gropp, E. Lusk: A high-performance MPI implementation on a shared-memory
vector supercomputer. Parallel Computing , 22, (1997) pp. 1513–1526.
7. B. V. Protopopov, A. Skjellum: Shared-memory communication approaches for an
MPI message-passing library. Concurrency: Practice and Experience, 12(9), (2000)
pp. 799–820.
8. H. Tang, K. Shen, and T. Yang: Compile/Run-time Support for Threaded MPI Execution on Multiprogrammed Shared Memory Machines. In Proceedings of ACM
SIGPLAN Symposium on Principles and Practice of Parallel Programming, (1999)
pp. 107–118.
9. D. Buntinas, M. Guillaume, W. Gropp: Data Transfers between Processes in an SMP
System: Performance Study and Application to MPI. International Conference on
Parallel Processing (ICPP), Columbus, Ohio, USA, (2006), pp. 487–496.

