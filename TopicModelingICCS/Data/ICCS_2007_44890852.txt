Application of Classification Methods to Individual
Disability Income Insurance Fraud Detection
Yi Peng1, Gang Kou1, 3,*, Alan Sabatka2, Jeff Matza2, Zhengxin Chen1,
Deepak Khazanchi1, and Yong Shi1
1

Peter Kiewit Institute of Information Science, Technology & Engineering, University of
Nebraska, Omaha, NE 68182, USA
2
Mutual of Omaha, Omaha, NE, USA
3
Thomson Co., R&D, 610 Opperman Drive, Eagan, MN 55123, USA
Tel.: ++1 402 4030269
kougang@yahoo.com

Abstract. As the number of electronic insurance claims increases each year, it
is difficult to detect insurance fraud in a timely manner by manual methods
alone. The objective of this study is to use classification modeling techniques to
identify suspicious policies to assist manual inspections. The predictive models
can label high-risk policies and help investigators to focus on suspicious records
and accelerate the claim-handling process.
The study uses health insurance data with some known suspicious and
normal policies. These known policies are used to train the predictive models.
Missing values and irrelevant variables are removed before building predictive
models. Three predictive models: Naïve Bayes (NB), decision tree, and
Multiple Criteria Linear Programming (MCLP), are trained using the claim
data. Experimental study shows that NB outperformed decision tree and MCLP
in terms of classification accuracy.
Keywords: Classification, Insurance Fraud Detection, Naïve Bayes (NB),
decision tree, and Multiple Criteria Linear Programming (MCLP).

1 Introduction
Health care frauds cost private health insurance companies and public health
insurance programs at least $51 billion in calendar year 2003 (The National Health
Care Anti-Fraud Association, 2005). Traditional heuristic-rule based fraud detection
techniques can not identify complex fraud schemes. Such a situation demands more
sophisticated analytical methods and techniques that are capable of detecting fraud
activities from large databases.
The objective of this study is to design, develop and test classification models to
detect abnormal behaviors in disability income insurance data. The predictive models
*

The corresponding author.

Y. Shi et al. (Eds.): ICCS 2007, Part III, LNCS 4489, pp. 852 – 858, 2007.
© Springer-Verlag Berlin Heidelberg 2007

Application of Classification Methods

853

can label high-risk policies and help investigators to focus on suspicious records and
accelerate the claim-handling process. To achieve the goal, this project studies the
attributes; preprocesses dataset; and develops classification models using a three-step
approach. The first step is model construction. A predictive model is constructed
based on training dataset, which includes predefined class labels for each record. The
second step is model validation. This step uses validation dataset to tune the model
weights during estimation and assess the classification accuracy of the model. The
third step is model usage. The classification model developed and validated in the
first and second step is used to classify future unknown data into predefined classes.
For comparison purposes, three classifiers: Naïve Bayes (NB), decision tree, and
Multiple Criteria Linear Programming (MCLP), are constructed for the dataset. The
resultant classification model(s) should provide acceptable classification accuracies
and allow domain experts to use the classification model in future detection of
abnormal behaviors.
This paper is organized as follows: The next section describes data and
preprocessing steps, including feature selection and data transformation. The third
section gives an overview of the three classification techniques. The fourth section
presents the classification results and compares the three classifiers. The last section
concludes the paper with future research direction.

2 Data Understanding and Data Preparation
The insurance data are from Mutual of Omaha insurance company (Mutual of Omaha,
2006). There are five datasets that provide information about policy types, claims,
producers, and clients. The attributes include numeric, categorical, and date types.
These datasets were combined into one dataset using a common unique key. The final
dataset has over 18,000 records with 103 variables. A target attribute indicates the
class of each record in the final dataset. These records belong to either Normal or
Abnormal class.
Data preparation removes irrelevant variables and missing values; conducts
correlation analysis to understand the relationship between attributes and the target
attribute; selects attributes for classification modeling; and transforms attributes to
appropriate forms for the three classifiers. If all instances of a variable are missing,
this variable has no use in the classification and was removed.
Correlation analysis describes the relation between two or more variables. The goal
of correlation analysis is to find out which variables are strongly associated with the
target variable. Since many variables are categorical, Cramer's V, a popular chisquare-based measure of nominal association, was used. Cramer’s V ranges from 0 to
1. The closer Cramer’s V is to 0, the smaller association between the two variables.
The closer V is to 1, the stronger association between the two variables. Attributes
with V> 0.1 were selected for classification. As a result, the number of attributes was
reduced from 103 to 20.
The software tools used in this study are base SAS, SAS Enterprise Miner (EM),
and a C++ program for MCLP. Naïve Bayes (NB), decision tree, and MCLP require

854

Y. Peng et al.

different data types. The tree node of SAS EM handles the data transformation for
decision tree method. NB requires attributes to be categorical while MCLP requires
attributes to be numeric. Data transformation for NB and MCLP were carried out by
base SAS and C++.

3 Classification Techniques
Based on the available data, three classification methods -- decision tree, Naïve
Bayes, and Multiple Criteria Linear Programming (MCLP), were proposed. They
classify data from different perspectives.
Decision tree is a machine learning technique that applies a divide-and-conquer
method to recursively build a tree to classify data records. It is one of the most
popular classification tools due to its high classification accuracy and easily
interpretable results. From our previous experience, decision tree is efficient and
superior to many other classifiers for different data sets. SAS EM provides tree nodes
that compute decision tree solution.
Naïve Bayes (NB) classifier is a simple probabilistic classifier that computes the
probabilities of instances belonging to each predefined class and assigns instances to
the class that has the highest probability. NB is proposed for this project for two
reasons. First, many variables selected by correlation analysis are categorical and NB
is designed to classify categorical data. Second, despite its naïve design, NB works
well in many real-world situations and outperforms some complex classifiers. In
addition, NB takes approaches that are different from decision tree and MCLP to
build classifier. NB function was implemented in base SAS.
MCLP is a mathematical-based classification approach that classifies records into
distinct groups based on two or more objective functions. Previous experiences (Peng
et al 2004; Kou et al 2005) proved that MCLP achieves comparable or superior
classification results than other popular classification methods for various datasets. A
self-developed C++ program (Kou et al 2003) was applied in this study to construct
and solve the MCLP model.

4 Empirical Study
4.1 Decision Tree
The Tree node in SAS EM provides three criteria for tree splitting: chi-square test,
entropy reduction, and gini reduction. Each criterion uses different techniques to split
tree and establish a different decision tree model. Since each technique has its own
bias, it may potentially provide a stronger solution if the results from three models are
integrated. The method of integrating results from multiple models is called ensemble
in machine learning. The following tables summarize the output of four decision tree
models: chi-square test, entropy reduction, gini reduction, and ensemble. For each
model, the confusion matrix is reported.

Application of Classification Methods

855

Table 1. Decision Tree Model using chi-square test

Actual
1
2
Total

Confusion Matrix
Predicted
1
2
18510
12
316
37
18826
49

Total
18522
353
18875

18510 (2nd row, 2nd column) is the correct predictions that a record is class 1 –
Normal.
12 (2nd row, 3rd column) is the incorrect predictions that a record is class 2 Abnormal.
316 (3rd row, 2nd column) is the incorrect predictions that a record is class 1 – Normal.
37 (3rd row, 3rd column) is the correct predictions that a record is class 2 - Abnormal.
Table 2. Decision Tree Model using entropy reduction

Actual
1
2
Total

Confusion Matrix
Predicted
1
2
18458
64
102
251
18560
315

Total
18522
353
18875

Table 3. Decision Tree Model using gini reduction

Actual
1
2
Total

Confusion Matrix
Predicted
1
2
Total
18496
26
18522
213
140
353
18709
166
18875

Table 4. Ensemble: chi-square, entropy reduction, and gini reduction

Actual
1
2
Total

Confusion Matrix
Predicted
1
2
Total
18519
3
18522
227
126
353
18746
129
18875

The ensemble node creates a new model by averaging the posterior probabilities
from chi-square, entropy, and gini models.

856

Y. Peng et al.

All decision tree results, except the one using entropy reduction, have less than
50% classification accuracy for class 2 records. Since the prediction accuracy of class
2 (Abnormal class) is important in fraud detection, decision tree is not a satisfactory
solution for this dataset.
Table 5. Naïve Bayes

Confusion Matrix
Training: 40%
Actual
1
2
Total

Predicted
1
2
7272 134
1
143
7273 277

Total
7406
144
7550

Test: 30%
Predicted
Actual
1
2
1
4907 656
2
42
58
Total 4949 714
All data
Actual
1
2
Total

Predicted
1
2
17949 573
2
351
17951 924

Total
5563
100
5663

Total
18522
353
18875

NB produces much higher class 2 classification accuracy than decision tree. For
test data, NB achieves 99% classification accuracy for class 2 records, while the
highest result of decision tree (using entropy reduction) is only 71%.
4.2 Multiple Criteria Linear Programming (MCLP)
MCLP requires all variables to be numeric, which is contrary to NB. The following
tables illustrate the training and test results of MCLP. The overall classification
accuracies for training and test are 59% and 57.95%, respectively.
The results of NB, Decision Tree, and MCLP indicate that probability-based
methods, such as NB, outperform decision tree and MCLP for this dataset. The reason
may be that many variables that are strongly associated with the target variable are
categorical. When these variables were transformed into numerical types, part of the
information is lost.

Application of Classification Methods

857

Table 6.

Training

Actual
1
2
Total

Predicted
1
2
61
43
104

39
57
96

Total
100
100
200

Corr
Rate
61%
57%
59%

Test

Actual
1
2
Total

Predicted
1
2
143
110
7743 10679
7886 10789

Total
253
18422
18675

Corr Rate
56.52%
57.97%
57.95%

5 Directions for Future Projects
Based on the results of this project, we propose several future directions. The first
direction is to further validate the classification classifier with new data. The second
direction is to update the existing labeled data and classifiers. A fundamental job of a
classifier is to identify patterns in unknown data that are similar to those in known
data. The quality of known or training data determines the classification results.
Labeled data need to be updated periodically because data patterns change over time.
Accordingly, classifiers need to be trained periodically to catch the changes in the
data. The third direction is to try different classification methods that can handle
categorical data and compare the results with NB. The last suggestion is to try oneclass classification algorithm. Normal classification needs two or more classes, while
one-class classification only needs information of the target class. One-class
classification is useful when labeled target class records are available. By
concentrating on target class data, one-class classification may generate better
results.

Acknowledgment
This research has been supported by “Data Mining Project - Individual Disability
Income Product”, Mutual of Omaha.

858

Y. Peng et al.

References
1. The National Health Care Anti-Fraud Association, available at: http://www.nhcaa.org/
(2005)
2. Kou, G., Liu, X., Peng, Y., Shi, Y., Wise, M., Xu, W.: Multiple Criteria Linear
Programming to Data Mining: Models, Algorithm Designs and Software Developments
Optimization Methods and Software 18 (4):, Part 2 AUG (2003) 453-473
3. Kou, G., Peng, Y., Shi, Y., Wise, M., Xu, W.: Discovering Credit Cardholders’ Behavior
by Multiple Criteria Linear Programming Annals of Operations Research 135 (1):, JAN
(2005) 261-274
4. Mutual of Omaha Website: http://www.mutualofomaha.com/. (2006)
5. Peng, Y., Kou, G., Chen, Z. and Shi, Y.: Cross-validation and ensemble analyses on
multiple-criteria linear programming classification for credit cardholder behavior in Bubak
M et al, eds., ICCS 2004, LNCS 3039:, Krakow, POLAND, JUN 06-09, (2004) 931-939

