Automation of Data Traﬃc Control on DSM
Architectures
Michael Frumkin, Haoqiang Jin, and Jerry Yan�
Numerical Aerospace Simulation Systems Division
NASA Ames Research Center

Abstract. The distributed shared memory (DSM) architecture simpliﬁes development of parallel programs by relieving a user from the tedious task of distributing data across processors. Furthermore, it allows
incremental parallelization using, for example, OpenMP or Java threads.
While it is easy to demonstrate good performance on a few processors,
achieving good scalability still requires a good understanding of data
ﬂow in the application. In this paper we discuss ADAPT, an Automatic
Data Alignment and Placement Tool, that detects data congestions in
FORTRAN array oriented codes and suggests code transformations to
resolve them. We then show how ADAPT suggested transformations, including data blocking, data placement, data transposition and page size
control improve performance of the NAS Parallel Benchmarks.

1

Signiﬁcance of Data Traﬃc Control

Few programming constructs allow explicit speciﬁcation of data location in computer memory1 . That leaves data placement decisions to the compiler and the
operating system. The time for accessing data varies across diﬀerent computer
architectures, making code tuning machine dependent. Some memory problems
such as cache trashing and false sharing are not diﬃcult to identify and overcome with array dimension padding and variable privatization. Other data trafﬁc problems such as data locality, excessive TLB misses and thread interference
are diﬃcult to diagnose and cure. Even the best implementations of Computational Fluid Dynamics (CFD) codes can achieve only about 20% of machine
peak performance[14], spending 80% of the time waiting for data.
Two main factors contribute to the low eﬃciency of these codes. First, the
distribution of Floating Point Instructions (FPI) does not present an optimal
mix to keep all the functional units busy. There is no obvious cure for this.
Second, many operations are stalled waiting for data, see Example 1 in Section
2. Solving this problem is the subject of the paper.
Approach to data traﬃc optimization. The ﬁrst step in addressing this challenge is to identify code constructs causing data congestion, and the type of
�
1

M/S T27A-2, NASA Ames Research Center, Moﬀett Field, CA 94035-1000; e-mail:
{frumkin,hjin,yan}@nas.nasa.gov
By explicit constructs, we mean statements such as the “register” qualiﬁer in C or
data distribution directives in High Performance Fortran.

V.N. Alexandrov et al. (Eds.): ICCS 2001, LNCS 2074, pp. 771–780, 2001.
c Springer-Verlag Berlin Heidelberg 2001
�

772

M. Frumkin, H. Jin, and J. Yan

congestion. We use four congestion metrics in this paper: Primary Data Cache
(PDC) misses, Secondary Cache (SC) misses, Translation Lookaside Buﬀer
(TLB) misses, and Cache Invalidations (CI). The second step involves choosing a proper remedy to resolve congestions and applying it to the code. This
remedy can be either code transformation, or changing program environment
such as page size or page placement/migration mechanisms.
Methods for controlling data traﬃc. These problems have been studied elsewhere [4,10,11] and techniques have been developed to improve data traﬃc.
These techniques include data grouping for cache optimization, optimization
of the page size for reduction of TLB misses, placement of pages to resolve
contention in memory channel and data transposition for reduction of cache invalidations and TLB misses. Even with these techniques in hand, it is not easy
for the user to identify the appropriate transformations to be applied to various
part of the code. For example, it has been known that computations of the rhsz
and zsolve operators in BT and SP of the NAS Parallel Benchmarks (OpenMP
version) exhibit performance which is 3-4 times worse than of corresponding operators in the x-direction [7]. Neither, the reason for this, nor a mechanism for
ﬁxing the problem has been known in spite of NPB being de facto standards for
reporting improvements in compilers and tools [12].
Many code transformations to improve data traﬃc are already implemented
in compilers. These include loop interchange, loop fusion, software pipelining,
prefetching, and modiﬁcation of variable declarations (such as padding and privatization). Nevertheless, compilers cannot perform expensive analysis involving
full interprocedural data dependency analysis. Many types of analysis are not
possible at compile time at all.
Another approach to identifying data traﬃc problems relies on hardware
counters that gather event statistics during program execution. Tools for collecting and analyzing the events, such as perfex and Performeter, have been
built on top of these counters [6,13]. These tools allow a user to instrument code
and identify code constructs with performance anomalies. However, these tools
are diagnostic in nature and leave analysis of the problems and identiﬁcation of
the remedies to the user.
In this paper we present ADAPT, a tool that combines both static code analysis and run-time testing. ADAPT analyzes data-to-data and data-to-computations aﬃnity in the source code to report potential data traﬃc problems. The tool
also inserts diagnostic statements in the code for those expressions that inﬂuence
data traﬃc but cannot be evaluated at compile time. These statements are evaluated at run time and issue warnings about poorly performing code constructs.
We demonstrate ADAPT’s ability to identify problematic data traﬃc constructs
and to suggest cures for these problems on three simulated CFD applications
BT, SP and LU of the NAS Parallel Benchmarks (NPB). ADAPT was able to
resolve data traﬃc problems with the rhsz and zsolve operators and helped to
improve performance of the codes by 27% on average.

Automation of Data Traﬃc Control on DSM Architectures

2

773

Automated Detection of Data Traﬃc Problems

To be able to control data traﬃc in an application, the user needs information
on data movement across the memory hierarchy. While details of such movement
can be complicated and machine dependent, they can be formulated in terms of
cache parameters, array oﬀsets, and data access strides, and can be characterized
by simple metrics such as cache misses and invalidations. In a few cases, such as
accessing shared data, data traﬃc depends on the cache coherency protocol and
is sensitive to variations in execution order across diﬀerent threads.
A tool that detects and corrects data traﬃc congestions greatly reduces the
burden on a user to tune his code manually. Such a tool can reduce data traﬃc
by increasing data reuse, avoid memory contention by optimizing initial data
placement, and reduce the number of TLB misses and thread interference by
optimizing page size. A typical scenario in which such a tool is applied is shown
in the following example.
Example 1. The ﬁrst two loop nests in lhsz of SP from NPB (serial version)
are shown in Figure 1, left pane. The attempt to optimize the ﬁrst nested loop
actually slows down the computations since it increases the number of accessed
memory pages and poorly utilizes the primary cache, see lhsz curve in Figure 2.
Merging the ﬁrst and the second nested loops and recalculating the expressions,
see right pane in Figure 1, decreases the number of pages accessed, improves
cache utilization, and reduces the total execution time, in spite of increasing the
total number of FPI, see lhsz t curve in Figure 2.
We have implemented such a tool by adding features to ADAPT (Automatic
Data Alignment and Placement Tool) [2]. Originally, ADAPT was designed for
the automatic insertion of HPF directives into FORTRAN code2 . The tool is able
to identify data-to-data and to data-to-computations aﬃnity and to express such
aﬃnities through HPF ALIGN and DISTRIBUTE directives. ADAPT’s ability
to extract these aﬃnities is the key to enabling it with automatic data traﬃc
diagnostic capabilities.
Data-to-data aﬃnity. Two data items have an aﬃnity if both are required
by the same instruction (directly or indirectly) during program execution. For
a pair of arrays used in the same loop nest statement, the aﬃnity relation is
a correspondence between array elements referenced by the same loop index.
Grouping of aﬃne data items and packing the groups into a continuous stream
improves program performance by hiding the memory latency. In general, aﬃnity
is a many-to-many relation leaving many degrees of freedom to group aﬃne data
items. In [3], it is shown that the eﬃciency of grouping aﬃne array elements
depends on the geometry of the array interference lattice, deﬁned as a set of
solutions to the Cache Miss Equation [4].
The aﬃnity relation can be deduced for each array pair in each nest statement. A control dependence results in aﬃnity relations between the arrays involved in the control statement and all arrays in the basic blocks immediately
2

ADAPT is built on top of CAPTools [9]. It uses a CAPTools generated data base
and CAPTools code analysis and utilities.

774

M. Frumkin, H. Jin, and J. Yan

lhsz
do j=1,ny
do i=1,nx
do k=1,nz
cv(k)=ws(i,j,k)
rhon(k)=SFunction(rho(i,j,k))
end do
do k=2,nz-1
lhs(i,j,k,1)=0.0d0
lhs(i,j,k,2)=-dttz2*cv(k-1)
-dttz1*rhon(k-1)
lhs(i,j,k,3)= 1.0d0
+c2dttz1*rhon(k)
lhs(i,j,k,4)= dttz2*cv(k+1)
-dttz1*rhon(k+1)
lhs(i,j,k,5)=0.0d0
end do
end do
end do

lhsz t
do k=2,nz-1
do j=1,ny
do i=1,nx
lhs(i,j,k,1)=0.0d0
lhs(i,j,k,2)=-dttz2*ws(i,j,k-1)
-dttz1*SFunction(rho(i,j,k-1))
lhs(i,j,k,3)=1.0d0
+c2dttz1*SFunction(rho(i,j,k))
lhs(i,j,k,4)=dttz2*ws(i,j,k+1)
-dttz1*SFunction(rho(i,j,k+1)
lhs(i,j,k,5)=0.0d0
end do
end do
end do

Fig. 1. Data Traﬃc Optimization. The original code (left), taken from lhsz.f of
NPB2.3-serial, saves ﬂoating point instructions used in SFunction. However, such loop
ordering creates a large number of TLB and PDC misses. By rearranging computations
(right pane) these problems are resolved, improving execution time in spite of increase
in the number of FPI. The proﬁles of both codes are shown in Figure 2.

dominated by the statement. The most common case observed in our CFD applications is one-to-many aﬃnity relations between arrays resulting from diﬀerence
operators on structured discretization grids. These relations can be described by
a stencil (i.e. by a set of vectors with constant elements) and we call them stencil
relations. In order to deduce the aﬃnity for arrays used in diﬀerent statements
of the same nest, ADAPT uses the chain rule, see [2]. The union of aﬃnity relations over all directed paths leading from an array u to an array q forms the
nest aﬃnity relation between q and u. The relation lists all elements of u used
for computation of each element of q and is a one-to-many mapping.
Data-to-computations aﬃnity. We represent a program by a bipartite graph
called the program aﬃnity graph. Let C be the set of program statements, and
let D be the program data, i.e. the set of (virtual) memory locations referenced
in the program. We say a memory location d has an aﬃnity with a statement c if
the datum at address d is either an operand or a result of c. The program aﬃnity
graph has C and D as the vertices of the parts with appropriately directed arcs
connecting statements with data aﬃne to them. Many program properties can
be expressed in terms of the aﬃnity graph. For example, a statement c2 depends
on a statement c1 if there is a directed path from c1 to c2. Otherwise, c1 and c2
are independent and can be executed in any order.
The analysis of the aﬃnity graph can be simpliﬁed by indexing the statements inside the nests and the memory locations used by the arrays. In this

Automation of Data Traﬃc Control on DSM Architectures

775

Number of Events (normalized)

lhsx
lhsz
lhsz_t

Cycles

Time

FPI

TLB

PDC

SC

CI

Fig. 2. The eﬀect of TLB (Table Lookaside Buﬀer) and PDC (Primary Data Cache)
usage optimization on the performance of lhsz nest. The performance of a similar operator in x-direction (lhsx) is given as a reference. The horizontal axis shows types of
events measured with the use of hardware counters. The vertical axis shows a normalized number of measured events. As above, FPI stands for Floating Point Instructions,
SC for Secondary Cache misses, and CI for secondary Cache Invalidations.

case, the arcs connecting data and statements can be expressed as (I; idx(I))
where I is a loop nest index tuple, and idx(I) is a memory address of an array
element referenced in iteration I. In our application domain (CFD applications
on structured grids), the index function is usually a linear function of I with
symbolic coeﬃcients known at compile time. There are only a few nests in our
applications where this is not a case. These include the core of the FFT algorithm, where idx(i, j, k) = i + 2 · j · k; nests interpolating data between distinct
grids, where idx function is read from a ﬁle; nests accessing specially enumerated
grid points where the idx function is stored in a precomputed array. The tool
identiﬁes nests with nonlinear access functions, without any further analysis.
Some properties of the data traﬃc can be deduced using only symbolic analysis of the idx function (see the thread noninterference condition below). Others
require knowledge of the actual numerical values of the coeﬃcients. If the properties of the data traﬃc can be expressed by a symbolic expression but cannot
be veriﬁed without knowing the numerical values of the coeﬃcients, ADAPT
inserts the expression in the code and the user receives a warning at run time.
We call this a run time test.
Checking cache unfriendly access patterns. In general, cache friendly computations involve good temporal and spatial locality [5] and cannot be expressed
in simple closed-form terms [4]. However, some necessary conditions for cache
friendly computations can be formulated and checked. The ﬁrst condition is simple: array elements accessed in the iterations of the innermost loop cover contiguous memory locations. Otherwise, nonunit stride in memory access would
cause underutilization of data loaded into the cache.

776

M. Frumkin, H. Jin, and J. Yan

Detection of self interference. If an array self aﬃnity relation is a stencil
relation, ADAPT represents the addresses of the corresponding array elements
as a polylinear function of array sizes and the index coeﬃcients. For each possible
pair of the stencil vectors, ADAPT generates a set of constraints for the array
dimensions of the form nx · ny �= m/h · S, where nx, ny are the array sizes, S is
the primary cache size, m is any positive integer, and h is an empirically chosen
small integer (h = 1, 2, cf.[3]). If neither nx nor ny is known at compile time, a
satisﬁability test for these constraints is inserted into the program as a run time
test.
Detection of cross interference. Cross interference between two arrays happens when aﬃne elements are mapped to the same cache location. Detecting cross interference is similar to identifying self interference, except that it
involves the inter-array oﬀset and the dimensions of both arrays. The cross
interference constraints are represented by a set of polylinear inequalities:
idxa (i + e1 , j + e2 , k + e3 ) + oﬀab − idxb (i + d1 , j + d2 , k + d3 ) �= m/h · S,
where ei , di , i = 1, 2, 3 are components of the stencil vectors. Evaluation of
these inequalities requires knowledge of oﬀab and can be done, for example, if
both arrays are in the same common block or are segments of the same (bigger)
array.
Detection of high TLB misses. TLB misses (as in Example 1) usually result
from large memory strides due to iterations of an inner loop of a nest. For
each array in the nest our TLB miss test checks a suﬃcient conditions for TLB
misses in the inner loop: the number of iterations of the innermost nest exceeds
TLB SIZE, and the distance between the ﬁrst and last address accessed in the
inner loop exceeds PAGE SIZE*TLB SIZE. If both conditions are met, ADAPT
issues a warning about high TLB misses in the inner loop of the nest. If both
conditions can not be proved to be false, the tool inserts a run time test.
Checking thread noninterference. This condition can be formulated as absence
of overlap of the address spaces accessed by diﬀerent threads. If the noninterference condition is satisﬁed, then the memory accessed by a thread can be placed
in the memory of the processor running the thread, improving data locality. This
condition is checked only for “read/write” arrays since thread interference would
cause cache lines invalidations. In the case of “read” arrays, this condition is not
checked since read arrays are copied into secondary cache and threads do not
aﬀect each other, provided that initial placement of the array has been done
correctly.
Interference of threads depends on the data sharing protocol implemented in
the DSM computer. For example, if data coherence is supported at the level of
secondary cache lines, as it is on the Origin 2000, then read/write interference
can happen if two threads are accessing two diﬀerent words on the same cache
line. For a software performance tool, it is possible to be aware of the data
sharing protocol and adjust CI estimates accordingly. We have implemented
a more general approach: for each nest and each thread, ADAPT evaluates the
interference as a ratio of the number of memory locations adjacent to the memory
locations accessed by other threads to the total number of memory locations

Automation of Data Traﬃc Control on DSM Architectures

777

accessed by the thread. For example, in Figure 1 the interference ratio for lhs is
P/(nx · ny) in lhsz and P/(nx · ny · nz) in lhsz t, which correlates well with the
CI numbers in Figure 2. This interference indicator is similar to the surface-tovolume ratio used to estimate cache utilization [3], and to the communicationsto-computations ratio used to characterize MPI programs.
Detection of the data sources and the initial data placement. Page placement
on a DSM computer is commonly controlled by a simple policy such as “First
Touch” or “Round Robin”. More sophisticated page placements can be implemented with special tools, such as dplace, see [13]. An inappropriate initial data
placement – for example, concentrating data on a single processor – can cause
memory contention during execution and may hamper application scalability.
Therefore, we enabled ADAPT with a capability to detect data initialization
constructs in the code and to insert an equitable data placement directive before each construct.

3

Experiments

We conducted experiments on 16 processors of a 512 processor single image
SGI Origin 2000 (MIPS R12000 CPU). We submitted jobs through the Portable
Batch System (PBS), which dedicates requested resources to the job and minimizes interference with other jobs running on the machine. We used 16 processors, since this is the minimal number of processors where the slowdown due to
memory traﬃc eﬀects was well pronounced for the 643 -point grids of NPB class
A. The eﬀects are similar up to 32 processors, after which the load imbalance
becomes dominant.
The primary memory hierarchy on the Origin 2000 involves registers, primary
data and instruction caches, secondary uniﬁed data and instruction cache, and
the main memory. The access time to data located at diﬀerent levels of memory
is shown in Table 1, cf. [13]. The primary cache is directly mapped 2-way set
Table 1. Access Time (in machine cycles) to Data in Origin 2000 Memory.
Data Location Access Time
registers
0
L1 cache
2-3
L1 cache
8-10
L2 cache
75-250
L2 cache
∼2000

Condition
L1
L1
L2
L2

hit
miss, L2 hit
miss, TLB hit
miss, TLB miss

associative, having 512 lines of 32 bytes each in each set. The secondary cache
is shared by data and instructions and it is also directly mapped 2-way set
associative containing 32K 128 byte lines. The main memory is split in modules
of 768 MB per node (2 processors per node), totaling 196 GB of memory on a
512 processor machine. The TLB has 64 entries containing base addresses of 64
pairs of pages.

778

M. Frumkin, H. Jin, and J. Yan

A cache coherency protocol guarantees that data accessed by diﬀerent processors do not go stale. This protocol invalidates a line in secondary cache every time
a processor requests exclusive ownership (usually for writing) of data mapped
to the line. In this case, all copies cached in all other processors are invalidated,
and each processor working with this line has to request a fresh copy of the line
to resume computations. An implication of this protocol is that it will cause
signiﬁcant slowdown when two processors attempt to write data located within
the same 128 byte segment of main memory.
As the test codes, we chose the OpenMP version of the PBN3.0-b2, a release
of the NAS Parallel Benchmarks which includes optimized serial, OpenMP, HPF
and Java versions. The suite is designed for demonstrating capabilities of compilers and tools applied to CFD codes [7].
We measured execution time at various levels of optimization. Since we used
the -O3 ﬂag in the compilation, special eﬀort was required in many cases to
prevent the compiler from undoing our optimizations. We speciﬁed compiler ﬂags
for suppressing prefetching: --LNO:prefetch=0 to obtain accurate numbers for
the hardware counters, and ﬂag OPTFLAGS=-OPT:reorg common=OFF to enforce
our own padding of arrays declared in the common blocks.

4

Experimental Results and Discussion

We applied the tool to the SP, BT and LU codes of the NPB [7] (optimized
OpenMP version PBN-O). For each code ADAPT was able to generate the
following data traﬃc optimization diagnostics.
• Nests for initial data placements. ADAPT detected all nests where data
were initialized. In all cases, the arrays were initialized from array of smaller
dimensions (or constants). The initial data placement was appropriately implemented in the original code and we essentially did not make any changes.
• Nests with nonunit strides and advice on loop interchange. Such nests were
detected only in calculation of the so-called right hand side array by the subroutines rhs, exact rhs, and erhs.
• Nests with big strides and advise on loop interchange and data transpositions. Nests with big strides were detected in the subroutines rhs, exact rhs,
erhs and in zsolve. Advice on changing jik into kji loops was issued for the
ﬁrst three subroutines. In zsolve (BT and SP only), a dependency in the k
index prevented the parallelization of the k loop, and no loop interchange was
carried out.
• Nests with self or cross interference, and advice on padding. No arrays
with self interference were detected indicating that the existing paddings of the
ﬁrst and second dimensions were suﬃcient. The cross interference condition was
presented in the form that array oﬀsets cannot be equal to a multiple of the
cache size plus a stencil vector oﬀset.
Following ADAPT’s advice, we implemented a number of changes in the
original OpenMP code by hand. Almost all changes occurred in rhs, zsolve,
buts, and blts. We classify these changes into 3 categories, as shown in Table

Automation of Data Traﬃc Control on DSM Architectures

779

2: removing auxiliary arrays and applying nest fusion in rhs, loop interchange
in rhs, removing auxiliary arrays in solvers: zsolve in BT and SP and buts,
blts in LU. Both exact rhs and erhs were outside the main iteration loop, so
we did not make any changes to these subroutines. Incremental improvement
in performance via data traﬃc optimization for each benchmark is tabulated in
Table 2. The improvement was about 200% for rhs and 20% for zsolve. Average
overall speedup was about 27% on 16 processors. Some optimizations that gave
Table 2. Improving Benchmark Performance via Data Traﬃc Optimization. Time
(in seconds) was measured on 16 processors of a 400MHz Origin 2000 machine, SGI
OpenMP compiler.
Application
BT.A
SP.A
LU.A

Original Data reuse
Loop
Removing solver Total
Code and nest fuse interchange
aux. arrays
speedup
54.00
51.44
44.22
42.12
22%
63.40
55.89
38.70
37.92
40%
59.35
52.19
48.95
48.78
18%

performance improvements in simple test codes did not result in expected improvements in the benchmarks. Optimizing page size by providing each processor
with one page from each array did not aﬀect performance of the jobs running
under PBS. It improved performance of jobs running outside PBS by 10%. We
could not ﬁnd a remedy for the thread interference warning (high CI number) in
zsolve, because the nest has a dependency in the z-direction, preventing loop
interchange.

5

Conclusions and Related Work

In this paper we presented ADAPT, a tool to identify data traﬃc problems in
array-oriented Fortran codes. The tool advises the user on excessive cache and
TLB misses, and possible thread interference, and suggests code transformations
for improving data traﬃc. Some of the transformations are counterintuitive, since
they reduce data traﬃc and overall computational time by increasing the number
ﬂoating point instructions.
We demonstrated data traﬃc improvements using three simulated CFD applications BT, SP and LU from the NAS Parallel Benchmark suite. For some
subroutines performing operations in the z-direction, code transformations improved performance by a factor of 3 and improved scalability of these subroutines. Overall data traﬃc optimizations improved the benchmark performance
on 16 processors by 27% on average.
Research is being carried out in three main directions to enhance ADAPT’s
ability to control data traﬃc: reducing communications in MPI programs [9],
optimizing data distributions in HPF programs [2], and improving spatial and

780

M. Frumkin, H. Jin, and J. Yan

temporal data locality for optimizing cache performance [4]. With the proliferation of the DSM architecture, data traﬃc control on DSM is becoming increasingly important. Some problems of data distributions and page migrations
on DSM are the subject of recent papers [10,11]. We have plans to implement
some global data traﬃc control features in ADAPT and to test it on a wider
class of CFD applications and on variety of diﬀerent implementations of DSM.
Finally, we plan to incorporate ADAPT with CAPO, a parallelization tool that
automatically inserts OpenMP directives into FORTRAN codes [8].
Acknowledgements. This work was supported by the NASA High Performance Computing and Communications (HPCC) Program, RTOP #725-10-31.
The authors appreciate Rob F. Van Der Wijngaart’s eﬀort in reviewing this
paper.

References
1. D. Bailey, J. Barton, T. Lasinski, and H. Simon (Eds.). The NAS Parallel Benchmarks. NAS Technical Report RNR-91-002, www.nas.nasa.gov.
2. M. Frumkin, J. Yan. Automatic Data Distribution for CFD Applications on Structured Grids. The 3rd Annual HPF User Group Meeting, Redondo Beach, CA, August 1-2, 1999, 5 pp. Full version: NAS Technical report NAS-99-012, December
99. 27 pp.
3. M. Frumkin, R.F. Van der Wijngaart. Eﬃcient Cache Use for Stencil Operations
on Structured Discretization Grids. Submitted to JACM, see also NAS Technical
Report NAS-00-15, www.nas.nasa.gov.
4. S. Gosh, M. Martonosi, S. Malik. Cache Miss Equations: An Analytical Representation of Cache Misses. ACM ICS 1997, pp. 317–324.
5. J.L. Hennessy, D.A. Patterson. Computer Organization and Design. Morgan Kaufmann Publishers, San Mateo, CA, 1994.
6. Innovative Computing Lab., UTK. PAPI: Hardware Performance Counter Application Programming Interface. http://icl.cs.utk.edu/papi.
7. H. Jin, M. Frumkin, J. Yan. The OpenMP Implementation of NAS Parallel
Benchmarks and Its Performance, NAS Technical Report, NAS-99-011, 1999, 26
pp, www.nas.nasa.gov.
8. H. Jin, M. Frumkin, J. Yan. Automatic Generation of OpenMP Directives and
Its Application to Computational Fluid Dynamics Codes, Springer LNCS, v. 1940,
p. 440–456.
9. S.P. Johnson, C.S. Ierotheou, M. Cross. Automatic Parallel Code Generation on
Distributed Memory Systems. Parallel Computing, V. 22 (1996), pp. 227-258.
10. D. S. Nikolopoulos, T. S. Papatheodorou, C. D. Polychronopoulos, J. Labarta, E.
Ayguade. Is Data Distribution Necessary in OpenMP? Proceedings of Supercomputing 2000. Dallas, TX, Nov. 4-10, 2000. 14 pp.
11. D. S. Nikolopoulos, T. S. Papatheodorou, C. D. Polychronopoulos, J. Labarta,
E. Ayguade. Leveraging Transparent Data Distribution in OpenMP via User-Level
Dynamic Page Migration. Springer LNCS, v. 1940, p. 415–427.
12. Proceedings of Supercomputing 2000. Dallas, TX, Nov. 4-10, 2000.
13. SGI Inc. Technical Document. Origin2000 and Onyx2 Performance Tuning and
Optimization Guide. http://techpubs.sgi.com.
14. J. Taft. Performance of the Overﬂow-MLP CFD Code on the NASA Ames 512
CPU Origin System. NAS Technical Report, NAS-00-05. www.nas.nasa.gov.

