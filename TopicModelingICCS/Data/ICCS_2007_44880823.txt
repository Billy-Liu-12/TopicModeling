Analysis of Linux Scheduling with VAMPIR
Michael Kluge and Wolfgang E. Nagel
Technische Universit¨
at Dresden, Dresden, Germany
{Michael.Kluge,Wolfgang.Nagel}@tu-dresden.de

Abstract. Analyzing the scheduling behavior of an operating system
becomes more and more interesting because multichip mainboards and
Multi-Core CPUs are available for a wide variety of computer systems.
Those system can range from a few CPU cores to thousands of cores. Up
to now there is no tool available to visualize the scheduling behavior of a
system running Linux. The Linux Kernel has an unique implementation
of threads, each thread is treated as a process. In order to be able to
analyze scheduling events within the kernel we have developed a method
to dump all information needed to analyze process switches between
CPUs into ﬁles. These data will then be analyzed using the VAMPIR
tool. Traditional VAMPIR displays will be reused to visualize scheduling
events. This approach allows to follow processes as they switch between
CPUs as well as gathering statistical data, for example the the number
of process switches.

1

Introduction

The VAMPIR [7] tool is widely used to analyze the behavior of parallel (MPI,
OpenMP and pthreads) as well as sequential programs. This paper will demonstrate how the capabilities of VAMPIR can be used to analyze scheduling events
within the Linux kernel. These events are gathered by a Linux kernel module
that has been developed by the authors. This development has been motivated
by an scheduling problem of an OpenMP program that will be used within this
paper to demonstrate the application of the software.
Linux itself is an operating system with growing market share in the HPC
environment. Linux has its own way of implementing threads. A thread is not
more than a process that shares some data with other processes. Within the
Linux kernel there is no distinction between a thread and a process. Each thread
also has its own process descriptor. So within this paper the terms ’thread’ and
’process’ do not diﬀer much. Although we will talk about OpenMP threads,
those threads are also handled by the Linux Kernel as normal processes when
we are talking about scheduling.
The ﬁrst section gives an short overview about the state of the art in monitoring the Linux kernel. The next section is dedicated to our Linux kernel module
and the output to OTF [1]. Within the third section will show how various VAMPIR displays that have been designed to analyze time lines of parallel programs
or messages in MPI programs can be reused for an visual analysis of Linux
scheduling events. This paper is closed by a short summary and an outlook.
Y. Shi et al. (Eds.): ICCS 2007, Part II, LNCS 4488, pp. 823–830, 2007.
c Springer-Verlag Berlin Heidelberg 2007

824

2

M. Kluge and W.E. Nagel

Analyzing Scheduling Events in the Linux Kernel

Analyzing scheduling events is an interesting piece within the whole ﬁeld of performance analysis due to eﬀects that can be traced back to a speciﬁc process
placement or cache thrashing. Within this paper we are referring to a multiprogramming environment. This means that multiple programs do run in parallel
on a given set of CPUs. The processes associated to these programs are not
pinned to a speciﬁc CPU. Therefore the scheduler is free to place the processes
as needed onto available CPUs.
We have identiﬁed two main approaches to analyze the scheduling behavior
of an speciﬁc system. The ﬁrst idea is to instrument the kernel scheduler itself
to monitor its actions. This would have the advantage of having insight into
scheduler decisions. An other idea is an indirect approach. If the CPU number
that process is running on over time is traced as well as information about the
process state (running or suspended), the priority, the nice value, interactivity
etc. one can show strength and weaknesses within the scheduler also. All the
information needed for the second approach are available within the process
descriptor in the Linux kernel. The information needed for the ﬁrst approach is
only locally available only within the scheduler implementation and not globally
in the kernel. Opposite to that, the list of current tasks and their properties are
available everywhere within the kernel.
There is no exiting tool we have found that is able to gather those information described above. The Linux Trace Toolkit [5] collects information about
processes but does not have any information about the CPU number a process
is running on. There are tools that are able to instrument a kernel (like KernInst
[11] or KTau [8]) that require a unique program to be written and put into the
kernel. Monitoring the /proc ﬁle system [9] would be an solution but cannot
provide the ﬁne granularity needed. For AIX the AIX kernel trace facilities can
be used to gather data about various events in the kernel [2].
For really ﬁne grained monitoring of the process-to-CPU mapping and the
process state we decided to try a diﬀerent way.

3

Tracing Scheduling Events

Our approach utilizes the second idea from the section above. Because the information about all tasks on the system are available at each point in the Linux
kernel, the main idea is to write a kernel module that dumps the information
needed at speciﬁable time intervals. Some kind of GUI or automatic analysis
tool could later be used to analyze this data. The advantage of a kernel module
is the ability to load and unload the module as needed as well as the short time
for recompilation after a change in the source code because the kernel itself is
not being touched [4].
The design of the kernel module is as follows. A kernel thread is created and
inspects all given threads at an adjustable time interval. The minimum time
between two inspections is the so called ’kernel frequency’ which can be chosen

Analysis of Linux Scheduling with VAMPIR

825

at the kernel setup with 100, 250 or 1000 ticks per second. The kernel module is
given a particular process id (PID) to watch. It will inspect this PID and all its
children and will dump all needed information (actually CPU and process state)
to the relayfs [12] interface. This way the user is able to select all processes
or any part of the current process tree. On problem here are processes that
get reparented. If a process ﬁnishes that still has child processes, those child
processes will get the init process (1) as parent. If not all processes but a speciﬁc
subset is traced, this child processes will vanish from the trace at this point in
time.
The kernel module itself can be started, conﬁgured and stopped via an interface that has been made available through the sysfs ﬁle system. So the kernel
module can stay within the kernel without generating overhead when nothing
needs to be measured.
To dump the data gathered to the disk, a relatively new part of the Linux
kernel, relayfs, is used. relayfs is an virtual ﬁle systems that has been designed
for an eﬃcient transfer of large amounts of data from the kernel to user space.
It uses one thread per CPU to collect data through a kernel wide interface and
to transfer the data. The data is collected inside relayfs within sub buﬀers. Only
full sub buﬀers are transfered to the user space. On the user side, one thread
per CPU is running to collect the full sub buﬀers and to write the data to a ﬁle
(one ﬁle per CPU). This thread is sleeping until it gets a signal from the kernel
that a full sub buﬀers is available. This approach is scalable and disturbs the
measured tasks as less as possible.
In summary the kernel module currently supports the following features:
– enable/disable tracing from the user space on demand
– tracing of user selectable processes or tracing the whole system
– changing parameter settings from the user space (via sysfs)

4

Using VAMPIR to Analyze Scheduling Events

Now we have a collection of events that describes which process has been on
which CPU in which state in the system at diﬀerent timestamps. The amount
of data can become very large and needs a tool to be analyzed. Some kind of
visual and/or automatic analysis is needed here. There are basically two diﬀerent
things that we want to analyze from those trace ﬁles:
1. number of active processes on each CPU
2. following the diﬀerent processes (and their current states) on the CPUs over
the time
As threads and processes are basically treated the same way by the Linux
kernel, the hierarchical structure between all processes/threads is also known at
this point. For the ﬁrst application it is possible to count the tasks in the state
’runnable’ on each CPU. To actually be able to view this data the following
approach have been identiﬁed:

826

M. Kluge and W.E. Nagel

– each CPU is mapped to what VAMPIR recognizes as a process
– task switches can be show as an one byte message between the associated
CPUs (processes)
– forks and joins can also be shown as messages
– the number of forks, joins and task switches per kernel tick are put into
counters
By using diﬀerent message tags for the forks, joins and task switch diﬀerent
colors can be used within VAMPIR to make the display even more clear. The
ﬁlter facilities of VAMPIR can be used to analyze CPU switches, forks or joins
independently. Due to the zooming feature of VAMPIR (which updates each
open display to the actual portion of the time line that is selected) it is possible
to analyze the scheduling behavior over time.
On the beginning all CPUs (processes for VAMPIR) do enter a function called
’0’. When the ﬁrst process is scheduled onto a CPU is will leave this function and
enter a function called ’1’. By following this idea we can have a very informative display about the number of runnable processes on the diﬀerent CPUs. By
looking at VAMPIR’s time line and the counter time line in parallel we already
get a good feeling on what was happening on the system.
For following the processes over diﬀerent CPUs this scheme needs to be extended not only to have one VAMPIR process line for a CPU but to have multiple
process lines per CPU where the real processes will be placed on. Those lines
will be called a stream on that CPU from now. In this scenario, processes that
enter a speciﬁc CPU will be placed on a free stream. So for each process one
or two virtual function were deﬁned for VAMPIR. One is always needed and
denotes that on one stream a speciﬁc process ID is present. This can further
be extended to have distinct virtual VAMPIR functions for the two states of a
process (running/not running). In the second case we can generate a leave event
for one virtual function and an enter event to the other virtual function on the
same stream when a process switches its state.
The idea of modeling task switches as messages allows to use VAMPIR’s
Message Statistics window to analyze how many processes switched from one
CPU to another and how often this took place for each CPU (from-to) pair.

5

OTF Converter

To be able to analyze the collected data with VAMPIR a tool is needed to convert
the data dumped by the kernel module to a trace ﬁle. We have chosen to utilize
the OTF library due to its easy handling. Within relayfs the data obtained by
the kernel thread are dumped to the ﬁle that is associated with the CPU where
the kernel thread is running on at this point in time. The converter has been
written to serialize all events within these ﬁles after the program run and to
follow the tasks when they jump between the CPUs. It generates OTF output
with all necessary information like process names and state, CPU utilization
together with various counters.

Analysis of Linux Scheduling with VAMPIR

827

The example we will look at within the next section creates about 1GB of
trace data together from all CPUs. This example runs for about 6 minutes on
8 CPUs. The conversation to the OTF ﬁle format takes about one minute and
results in OTF ﬁles between 100 and 120 MB.

6

Example

Our example is derived from a problem observed on our Intel Montecito test
system. It has 4 Dual Core Itanium 2 CPUs running at 1.5 GHz (MT disabled).
The multiprogramming capabilities of a similar system (SGI Altix 3700) have
been investigated with the PARbench Tool [6], [3], [10]. One result here has been
that an OpenMP parallelized program that is doing independent computation in
all threads all the time (without accessing the memory) is behaving unexpected
hin an overload situation. We put eight sequential tasks and eight parallel tasks
(which open eight OpenMP threads each) on eight CPUs. So we have 72 active
threads that all need CPU time and do hardly any memory access. The algorithm
used on each tasks is the repeated (100000 times) calculation of 10000 Fibonacci
numbers. The sequential version takes about 2 seconds to run. The OpenMP
parallel program exists in two ﬂavors. The ﬁrst ﬂavor has one big parallel section,
100000 ∗ 10000 numbers are calculated in one block. The second implementation
opens and closes the OpenMP parallel section 100000 times to calculate 10000
Fibonacci numbers. One parallel task with 8 parallel threads also needs 2 seconds
for both ﬂavors. In the overload situation, all 72 threads did run concurrently
on the system. If we used the ﬁrst OpenMP implementation all 72 tasks/threads
Table 1. Wall time in seconds of the sequential and parallel program version in diﬀerent
execution environments
program

big OpenMP block small OpenMP block
busy waiting yield CPU
sequential
19 − 21
2−3
8 − 16
parallel
19 − 21
45 − 50
21 − 23

exited after about 20 seconds (+/- 1 second). If we use the second ﬂavor, the
eight sequential tasks exit after 2 to 3 seconds and the parallel tasks exit after
45 to 50 seconds.
The explanation for the diﬀerent behavior we found after the investigation
with our tool. It is the fact, that for the ﬁrst ﬂavor the tasks do not synchronize.
On an overloaded system the tasks get out of sync easily. The default behavior of
the OpenMP implementation for a synchronization point is a busy wait for 200ms
and a call to sleep() afterwards. That way the OpenMP threads for the ﬁrst ﬂavor do synchronize just once and they use their full timeslice to do calculation. In
the second ﬂavor the parallel task spend part of their time slice with busy waiting. By putting the busy waiting time to 0 by using export KMP_BLOCKTIME=0

828

M. Kluge and W.E. Nagel

Fig. 1. Screenshot of the full time line, note the three diﬀerent phases

this can be improved. The sequential tasks exit after 8 to 16 seconds and the
parallel tasks need between 21 and 23 seconds. The numbers are compiled in
table 1.
The VAMPIR screenshot for the scheduling time line for all three runs is given
in Figure 1. All switches for a task from one CPU to another is marked by a
(blue) line. From the beginning of the time line to about 1:50 min the run for the
one big OpenMP block has taken place. Afterwards the OpenMP busy waiting
example is executed. As the last example from about 5:30 minutes to the end
of the time line the run with disabled busy waiting is shown. Figure 2 shows all
switches from/to all CPUs. By zooming in and looking into the diﬀerent parts of
the time line, the following facts could be collected for the three diﬀerent runs:
1. After spawning all the processes the system is balanced after a relatively
short period of time. The load on the individual CPUs is well balanced.
Almost no rescheduling occurs during this period of time.
2. For the second run the balancing of the system takes much longer. During
the whole second run every few seconds there are some scheduling events
where tasks switch between CPUs. The reason for this is that some tasks
get suspended (after the busy wait time has elapsed) and the system needs
to be re-balanced afterwards.
3. The third case again is very diﬀerent. Tasks get suspended very often and
awakened thus the CPU utilization jitters a lot (due to the short OpenMP
regions and no busy waiting). For that reason the system never gets well
balanced but due to the fact that there are no CPU cycles spent busy waiting
this scenario has a shorter wall time than the second one.

Analysis of Linux Scheduling with VAMPIR

829

Fig. 2. Screenshot of all process switches

7

Conclusion

The work presented has two main results. First of all we designed a convenient
measurement environment to collect scheduling events from the Linux kernel (a
kernel module + relayfs). And we reused VAMPIR’s capabilities for a diﬀerent
purpose. Traditional displays from VAMPIR have been reinterpreted for our purposes and do provide very useful information to analyze the scheduling behavior
of a Linux system. A test case has been investigated and the underlying problem
has been identiﬁed.
For the future there are various opportunities to follow. One very interesting
idea is to correlate this information with a traditional program trace to be able
to follow eﬀect like cache thrashing or other things that are only analyzable by
looking at the whole system and not only looking onto a single program trace
obtained in user space.
This work has also shown that short OpenMP sections in an overload situation
on Linux is counterproductive. With busy waiting disabled this can be improved.
This way the OpenMP threads do sleep while waiting on a barrier. For this there
is a possibility that the Linux kernel classiﬁes this threads as ’interactive’ and
starts to shorten their timeslice.
The authors wants to thank her colleges Andreas Kn¨
upfer, Holger Brunst,
Guido Juckeland and Matthias Jurenz for useful discussions and a lot of ideas.

830

M. Kluge and W.E. Nagel

References
1. Andreas Kn¨
upfer, Ronny Brendel, Holger Brunst, Hartmut Mix, and Wolfgang
E. Nagel. Introducing the Open Trace Format (OTF). In Vassil N. Alexandrov,
Geert Dick van Albada, Peter M.A. Sloot, Jack Dongarra, Eds., Computational
Science – ICCS 2006: 6th International Conference, Reading, UK, May 28-31,
2006. Proceedings, volume II of Lecture Notes in Computer Science. Springer Berlin
/ Heidelberg.
2. IBM.
http://publib16.boulder.ibm.com/doc link/en US/a doc lib/aixprggd
/genprogc/trace facility.htm.
3. M.A. Linn. Eine Programmierumgebung zur Messung der wechselseitigen Einﬂusse von Hintergrundlast und parallelem Programm. Technical Report J¨
ul-2416,
Forschungszentrum J¨
ulich, 1990.
4. Robert Love. Linux Kernel Development (german translation). Number ISBN
3-8273-2247-2. ADDISON-WESLEY, 1 edition, 2005.
5. Mathieu Desnoyers and Michel R. Dagenais. Low Disturbancea Embedded System Tracing with Linux Trace Toolkit Next Generation. http://ltt.polymtl.ca,
November 2006.
6. W.E. Nagel. Performance evaluation of multitasking in a multiprogramming environment. Technical Report KF-ZAM-IB-9004, Forschungszentrum J¨
ulich, 1990.
7. Wolfgang E. Nagel, Alfred Arnold, Michael Weber, Hans-Christian Hoppe, and
Karl Solchenbach. VAMPIR: Visualization and Analysis of MPI Resources. In
Supercomputer 63, Volume XII, Number 1, pages 69–80, 1996.
8. A. Nataraj, A. Malony, A. Morris, and S. Shende. Early Experiences with KTAU
on the IBM BG/L. In Proceedings og EUROPAR 2006 Conference, LNCS 4128,
pages 99–110. Springer, 2006.
9. redhat Documentation. http://www.redhat.com/docs/manuals/linux/RHL-7.3Manual/ref-guide/ch-proc.html, November 2006.
10. Rick Janda. SGI Altix: Auswertung des Laufzeitverhaltens mit neuen PARBenchKomponenten. Diplomarbeit, Technische Universit¨
at Dresden, June 2006.
11. Ariel Tamches and Barton P. Miller. Using dynamic kernel instrumentation for
kernel and application tuning. In International Journal of High-Performance and
Applications 13, 3, 1999.
12. Tom Zanussi et.al. relayfs home page. http://relayfs.sourceforge.net, November
2006.

