Interval Arithmetic and Computational Science:
Performance Considerations
Alistair P. Rendell, Bill Clarke, and Josh Milthorpe
Department of Computer Science, Australian National University
Canberra ACT0200, Australia
alistair.rendell@anu.edu.au

Abstract. Interval analysis is an alternative to conventional floating-point
computations that offers guaranteed error bounds. Despite this advantage,
interval methods have not gained widespread use in large scale computational
science applications. This paper addresses this issue from a performance
perspective, comparing the performance of floating point and interval
operations for some small computational kernels. Particularly attention is given
to the Sun Fortran interval implementation, although the strategies introduced
here to enhance performance are applicable to other interval implementations.
Fundamental differences in the operation counts and memory references
requirements of interval and floating point codes are discussed.

1 Introduction
The majority of science is concerned with physical phenomena, such as velocity,
temperature, or pressure that are by their very nature continuous. Meanwhile
computations of these quantities are performed using the discrete environment of the
digital computer. To bridge this divide it is normal to approximate values to a
specified precision using a finite set of machine-representable numbers. Inherent in
this process is the concept of a rounding error, the effect of which can be hard to
predict a priori.
Currently most scientific codes use IEEE 754 double precision arithmetic [1] and
give relatively little or no attention to the effects of rounding errors. While this may
have been okay in the past, on today’s machines that are capable of multi teraflop
(>1012 operations) per second and with double precision arithmetic providing just 1516 significant figures, it is easy to see the potential for rounding errors to compound
and become as large as the computed quantities themselves.
Interval arithmetic is a fundamentally different approach to floating point
computations that that was first proposed by R.E. Moore in 1965 [2], but is yet to
achieve widespread use in computational science applications. The idea is to represent
a floating point number by two floating point numbers corresponding to a lower and
upper bound (referred to as the infima and suprema respectively). In concept it is
identical to expressing the uncertainty in a quantity as 1.234±0.001, except that the
interval representation would be written as [1.233, 1.235] since this is easier to
manipulate. The basic rules for interval addition and multiplication are as follows:
V.N. Alexandrov et al. (Eds.): ICCS 2006, Part I, LNCS 3991, pp. 218 – 225, 2006.
© Springer-Verlag Berlin Heidelberg 2006

Interval Arithmetic and Computational Science: Performance Considerations

[ x, x ] + [ y, y ] = [ x + y, x + y ]

219

(1)

[ x, x ] × [ y, y ] = [min( x × y, x × y , x × y, x × y ),
max( x × y, x × y , x × y, x × y )]

(2)

Division is slightly more complicated, particularly if the interval appearing in the
denominator spans 0, and for details the reader is referred to [3]. Suffice it to say that
anything that can be computed using floating point arithmetic can also be computed
using intervals, although there are some fundamental differences between interval and
standard floating point arithmetic. For instance interval arithmetic is not distributive
[3], so the order in which interval operations are performed can lead to different
interval results; the trick is to find the order of operations that give rise to the
narrowest or sharpest possible interval result.
Interval computations can be used to bound errors from all sources, including input
uncertainty, truncation and rounding errors. Given this apparent advantage it is
perhaps somewhat surprising that intervals are not already widely use in
computational science. The reason for this is arguably twofold; first difficulties
associated with designing interval algorithms that produce narrow or sharp interval
results, and second the performance penalty associated with use of intervals. In this
paper we do not address the former, but instead focus on the performance penalty
associated with performing interval operations on a computer.
There is a range of software products that are designed to support interval
computations on popular architectures (see reference 4). Most of these (e.g. FILIB
[5]) take the form of C++ template libraries and as such have obvious limitations for
widespread use in computational science. A much more attractive alternative is
provided by the Sun Studio Fortran compiler [6], as this has been extended to
provide support for an interval data type. Technically this means that using this
compiler it is possible to make an interval version of an existing computational
science application code by simply changing all floating point data types to intervals.
While we are investigating such things [7], in this paper the goal is to determine the
likely performance penalty a typical user might see if he/she where to make such a
switch.

2 Simple Interval Operations
Our first objective was to compare asymptotic performance for pipelined floating
point and interval operations. This was done using the following simple Fortran loop:
do i = 1, size(input)
result = result op input(i)
enddo
where result and input were either double precision floats or double precision
intervals, and op was either addition or multiplication. This loop was placed in a
separate procedure that was called many times, with care taken when initializing
vector input to ensure that result did not over or underflow.

220

A.P. Rendell, B. Clarke, and J. Milthorpe

The benchmark code was compiled using version 8.1 of the Sun Fortran compiler
(part of Sun Studio 10) with options:
–fast –xarch=v9b [-xia]
where fast is a macro corresponding to a number of different optimization flags,
v9b requests a 64-bit binary, and xia indicates that the compiler should use interval
support. Timing results for this simple benchmark run on a 900MHz UltraSPARC III
Cu system are given in Table 1. As intervals require twice the storage of the
equivalent floating point values results obtained using a given floating point vector
length are compared with interval results obtained using half that vector length.
Results for three different vector sizes are considered. For the small case the data
resides in the 64KB level 1 cache, for the medium case the data resides in the 8MB
level 2 cache, while for the large case data will be drawn from main memory.
Table 1. Performance (nsec) for summation and product benchmarks compiled using Sun
Fortran 95 8.1 and run on a 900MHz UltraSPARC III Cu under Solaris 5.10

Name
Small
Medium
Large

⎯⎯ Floating Point ⎯⎯
Input Size Sum Product
2000
1.7
1.7
512000
1.7
1.7
4096000
7.2
7.0

⎯⎯⎯ Interval ⎯⎯⎯
Input Size Sum Product
1000
35.7
102
256000
57.4
122
2048000
58.6
124

The timing results for the floating point benchmarks are identical for both the small
and medium vector size, reflecting the fact that the compiler has used prefetch
instructions (evident in the assembly) to mask the cost of retrieving data from level 2
cache. For the large data set the performance drops markedly due to level 2 cache
misses (confirmed using hardware performance counters). For the interval benchmark
while there appears to be some cache effects, the most notably observation is the huge
performance penalty associated with using intervals compared to floating point
variables.
This prompts the obvious question, what should be the relative cost of performing
an interval operation over the equivalent floating point operation? From equation 1 it
would appear that an interval addition should cost at least twice that of a floating
point addition – since two data items must be loaded into registers and two floating
point additions performed (one on the infimas and one on the supremas). Some extra
cost might also be expected since when adding the infimas the result must be rounded
down, while when adding the supremas the result must be rounded up. For the interval
product the expected performance is a little more difficult to predict; Equation 2
suggests the need to form 8 possible products and then a number of comparisons ino
order to obtain the minimum and maximum of these. While we will return to this
issue in more detail it does appear that a slowdown of 21 for the small interval
addition benchmark and 60 for the small interval product benchmark over their
equivalent floating point versions is excessive.

Interval Arithmetic and Computational Science: Performance Considerations

221

Indeed it is already clear from these simple results that for interval arithmetic to
be widely used for large scale computational science applications the performance
penalty associated with use of intervals needs to be substantially reduced. From
above and via close inspection of the generated assembly there appears to be four
main reasons why the Sun interval code is slower than its equivalent floating point
code:
1. The inherent cost of interval arithmetic over equivalent floating point (at least
2 for addition and 8 for multiplication as discussed above);
2. The fact that the Sun compiler translates each interval operation into a
function call;
3. A lack of loop unrolling due in part to the use of procedures to implement the
basic interval operations; and
4. Need for additional instructions to change the rounding mode every time an
interval operation is performed.
To quantify these effects Table 2 gives timing results for the floating point benchmark
modified to run in an analogous fashion to the interval code. That is the compiler
settings are first adjusted to prevent loop unrolling, then the benchmark is re-written
so that each floating point addition or multiplication is performed in a separate
function, and then additional calls are inserted into this separate function to switch
rounding mode every time it is called. These results coupled with the larger inherent
cost of interval operations explain the bulk of the performance differences between
the floating point and interval benchmarks (i.e. the differences seen in Table 1).
Table 2. Performance (nsec) of modified sum and product floating-point benchmarks compiled
using Sun Fortran 95 8.1 and run on a 900MHz UltraSPARC III Cu under Solaris 5.10. See text
for details of modifications.

Size
Small
Medium
Large

Initial
Sum Prod
1.7
1.7
1.7
1.7
7.2
7.0

No Unrolling
Sum Prod
4.5
5.6
4.5
5.6
7.0
7.8

+Function Call
Sum
Prod
24.5
24.5
29.4
30.7
54.5
57.1

+Rounding
Sum Prod
28.9 26.9
33.8 32.4
56.4 54.4

While the above analysis is based on observations from Sun’s Fortran
implementation of intervals it should be stressed that issues 2-4 are likely to occur for
interval implementations based on C++ template libraries. Specifically these will
invariably replace each interval operation by a function call which performs the basic
operations under directed rounding, but in so doing the presence of the special
function seriously limits the possibility of loop unrolling and other advanced compiler
optimizations.
With the above performance data in mind, an obvious strategy when using intervals
for large scale applications would be to rewrite selective kernels removing the use of
separate function calls, minimizing the need for rounding mode to be switched, and
generally enhancing the ability of the compiler to optimize the code. For example a
modified version of the addition kernel would look like:

222

A.P. Rendell, B. Clarke, and J. Milthorpe

call round_up
sum%inf= -0.0d0
sum%sup= 0.0d0
do i = 1, size(input)
sum%inf = sum%inf – input(i)%inf
sum%sup = sum%sup + input(i)%sup
enddo
sum%inf = -sum%inf
call round_nearest
In this code the rounding mode is changed once before the loop and once at the end,
and to enable only one rounding mode to be used one of the end points is negated
before and after the loop (this trick is also used by the Sun compiler).
There is, however, one caveat that should be noted when attempting to hand
optimizing any interval code. Specifically Sun implements an extended interval
arithmetic [8] that guarantees containment for operations on infinite intervals such as
[-∞,0]. Dealing with infinite intervals adds a level of complexity that for the current
purpose we will ignore. (Arguing that for the vast majority of computational science
applications computations on infinity are not meaningful and in anycase if required
this special condition can be handled by an “if test” that in most cases will be ignored
given the underlying branch prediction hardware.)
Generating hand optimized code for the product benchmark is a little bit more
complex as there are at least two alternative interval product formulations [7]. The
first is a direct translation of the min/max operations given in Equation 2, while the
second recognizes that depending on the signs of the two infima and two suprema it is
usually only necessary to perform two multiplications in order to obtain the result. An
exception arises when both operands span zero, in which case four multiplications are
required. This scheme requires multiple if tests so will be referred to as the branching
approach. For the purpose of generating hand optimized code for the product
benchmark both options have been programmed.
At this point as well as considering the performance of the hand optimized sum and
product kernels it is pertinent also to consider the performance of the Sun Fortran 95
intrinsic sum and product functions, since these perform identical functions to the
original kernel. Thus in Table 3 we compare timings obtained from the basic interval
code, with those obtained using the intrinsic functions and hand optimized versions of
these routines. These show that the performance obtained using the intrinsic sum is
significantly better than that obtained by the initial benchmark code. Indeed for the
smallest vector size the intrinsic function outperforms the hand optimized code by
40%, but for the larger cases the hand optimized code is superior. The reason for this
is that the hand optimized code has been compiled to include prefetch instructions,
while the intrinsic function does not (evident from the assembly). Thus for the small
benchmark where data is drawn from level 1 cache the intrinsic function shows
superior performance, while for the larger benchmarks the processor stalls waiting for
data to arrive in registers. For the hand optimized summation code the performance is
now within a factor of four from the equivalent floating point code, and while this is
larger than we might have expected, it is significantly better than our initial results.
For the product use of the intrinsic function is roughly twice as fast as the original
code if the data is in cache. The hand optimized min/max approach appears to give the

Interval Arithmetic and Computational Science: Performance Considerations

223

best performance, although it is still approximately 16 times slower than the
equivalent floating point code. In comparison to min/max the branching approach is
slightly slower, but this is probably not surprising given that exactly what branches
are taken depends on the data values and if these are random (which they are by
design in the benchmark used here) branch prediction is ineffective. It is therefore
somewhat interesting to see (via disassembly of the relevant routine) that Sun appear
to have implemented their interval product using a branching approach.
Table 3. Performance (nsec) comparison of initial summation and product benchmark with
versions that uses Fortran intrinsic functions and versions that are hand optimised. Codes
compiled using Sun Fortran 95 8.1 and run on a 900MHz (1.1nsec) UltraSPARC III Cu under
Solaris 5.10.

Name
Small
Medium
Large

Summation Benchmark
Initial Intrinsic Hand
69.1
4.71
6.7
90.7
16.3
6.7
92.3
73.0
36.6

⎯⎯⎯ Product Benchmark ⎯⎯⎯
Initial Intrinsic Min/Max Branch
119
66.6
26.8
37.0
139
78.0
27.2
38.5
141
136.0
56.6
48.5

Noting the relative difference between the two alternative product formulations it is
of interest to consider the relative performance of these two schemes on other
platforms, and in particular on an out-of-order processor with speculative execution.
Thus the original Sun Fortran benchmark was re-written in C and run on an Intel
Pentium 4 based system. As this processor has only a 512KB level 2 cache results are
given in Table 4 for just the small and large benchmarks sizes. These show that the
branching product formulation is now significantly faster than the min/max
formulation. Comparing the Pentium 4 and UltraSPARC III Cu we find that the small
summation benchmark runs faster on the Pentium 4 by a factor that is roughly equal to
the clock speed ratio (2.6 faster compared to a clock speed ratio of 2.9). For the large
summation benchmark this ratio is higher reflecting the greater memory bandwidth of
the Pentium compared to the UltraSPARC. For the product the branching code is
approximately 3.5 times faster than the min/max code on the UltraSPARC.
Table 4. Performance (nsec) of hand optimized C benchmark compiled using gcc 3.4.4 and run
on a 2.6GHz (0.385nsec) Intel Pentium 4 system under the Linux 2.6.8-2-686-smp core

Name
Small
Large

Sum
Hand
2.6
6.6

⎯⎯ Product ⎯⎯
Min/Max Branch
35.2
7.6
36.3
8.7

3 Compound Interval Operations
The previous section compared the performance of basic floating point and interval
addition and multiplication operations. In this section we consider the dot-product and
AXPY (Alpha times X plus Y) operations that form part of the level 1 Basic Linear

224

A.P. Rendell, B. Clarke, and J. Milthorpe

Algebra Subprogram (BLAS) library. As well as being slightly more complex, these
operations are of interest since they form the building blocks for higher level BLAS
operations, such as matrix multiplication. While the dot product and AXPY
operations were explicitly coded in Fortran, dot product is also a Fortran90 intrinsic
function and an interval version of AXPY is available as part of the Sun Performance
Library. Thus in Table 5 we present timing results for both these operations obtained
using basic Fortran code, Sun intrinsic functions or the Sun performance library, and
hand optimized versions of these routines produced using a similar strategy to that
outlined above for the simple benchmarks. These results again show that significant
benefit can be gained from hand optimizing these routines.
Table 5. Performance (nsec) of various floating pint and interval level 1 BLAS benchmarks
compiled using Sun Fortran 95 8.1 and run on a 900MHz (1.1nsec) UltraSPARC III Cu under
Solaris 5.10. See text for details.

Name
Small
Medium
Large

Floating Point
Dot AXPY
3.4
6.3
3.3
6.6
14.6
18.6

Sun Interval
Dot AXPY
81.4
250
108
306
216
314

Hand Interval
Dot AXPY
18.1
34.1
18.4
85.0
94.7
152

4 Conclusions and Discussions
Most software support for interval computation is in the form of libraries with all
interval operations being compiled to library calls; this appears to be the approach that
Sun have taken in providing Fortran support for intervals. The results obtained here
show that such implementations are unlikely to result in performance levels that are
acceptable for them to be used in large scale scientific computations. Better
performance can be obtained by effectively inlining the interval operations,
minimizing changes in rounding mode, and allowing the compiler to see and optimize
as much of the time consuming interval code as possible. Such an approach does
come with potential drawbacks, notably the containment problems alluded to above.
For the bulk of computational science applications, however, we believe that this is
not a major issue compared with the need to obtain much better performance from
current interval implementations. (At least from the perspective of generating a few
large scale proof of concept computation science applications that show a positive
benefit from the use of intervals.)
While the performance of current interval implementations is likely to be too slow
for them to find widespread use in computational science, it is of interest to consider
architectural changes that might alter this conclusion. To this end in Table 6 we
compare the floating point and memory operation mix required for floating point and
interval based dot and AXPY operations (assuming that an interval multiplication
requires 8 floating point multiplications). This shows that while on both the
UltraSPARC III Cu and Pentium 4 processors the floating point versions are
load/store limited, the interval versions tend to be floating point limited. Other nontrivial interval operations seems to exhibit similar behavior leading to us to the

Interval Arithmetic and Computational Science: Performance Considerations

225

general conclusion that interval arithmetic code requires a higher ratio of floatingpoint or flow-control operations to memory operations than does the corresponding
floating point code. Given that the performance of most modern computational
science applications tend to be load store limited yet on chip real estate is becoming
increasingly cheap (as evidenced by the development of larger on chip caches plus
multithreaded and multi-core chips) raises the question as to whether with relatively
little effort new chips could be designed to perform interval operations much faster.
Table 6. Comparison of operation mix for floating point and interval based Dot and AXPY
level 1 BLAS operations (of length n) with hardware limits

Dot
AXPY
Limit

Floating
Interval
Floating
Interval
UltraSPARC III Cu
Pentium 4

Floating-Point Ops
Add
Multiply
n
n
2n
8n
n
n
2n
8n
1
1
1
1

Memory Ops
Load Store
2n
1
4n
2
2n
n
4n
2n
1
1

Ratio
FP:Mem
1:1
5:2
2:3
5:3
2:1
2:1

Acknowledgements. This work was supported in part by the Australian Research
Council through Discovery Grant DP0558228.

References
1. IEEE Standard 754-1985 for Binary Floating-point Arithmetic, IEEE, (1985). Reprinted in
SIGPLAN 22, 9-25.
2. R.E. Moore, “The Automatic Analysis and Control of Error in Digital Computation Based
on the Use of Interval Numbers”, in “Proc. of an Adv. Seminar Conducted by the
Mathematics Research Center, US Army, University of Wisconsin, Madison”, Ed L.B.
Rall, John Wiley, (1965), pp 61-130. (Available at http://interval.louisiana.edu/
Moores_early_papers/Moore_in_Rall_V1.pdf)
3. G.W. Walster, “Interval Arithmetic: The New Floating-Point Arithmetic Paradigm”, Oct
1997, Sun Microsystems white paper, available at http://www.mscs.mu.edu/~globsol/
Papers/f90-support-clean.ps
4. Interval web page http://www.cs.utep.edu/interval-comp/main.html
5. M. Lerch, G. Tischler, and J.W. von Gudenberg, filib++ - interval library specification and
reference manual. Techn. Rep. 279, Lehrstuhl für Informatik II, Universität Würzburg, 2001.
6. Fortran 95 interval arithmetic programming reference, Sun Studio 10. Tech. Rep. 8190503-10, Sun Microsystems, Inc., Jan 2005.
7. J. Milthorpe, “Using Interval Analysis to Bound Numerical Errors in Scientific
Computing”, honours thesis, Department of Computer Science, Australian National
University, October 2005.
8. G.W. Walster, Closed Interval Systems, Sun Microsystems whitepaper, 2002, available at
http://www.sun.com/software/sundev/whitepapers/closed.pdf.

