Procedia Computer Science
Volume 80, 2016, Pages 2226–2230
ICCS 2016. The International Conference on Computational
Science

A Suite of Java Message-Passing Benchmarks to Support
the Validation of Testing Models, Criteria and Tools
George G. M. Dourado1 , Paulo S. L. Souza1 , Rafael R. Prado1 ,
Raphael N. Batista1 , Simone R. S. Souza1 , Julio C. Estrella1 ,
Sarita M. Bruschi1 , and Joao Lourenco2
1
University of São Paulo, ICMC, São Carlos, São Paulo, Brazil
{georgemd, pssouza, rafaelrp, rbatista, srocio, jcezar, sarita}@icmc.usp.br
2
Universidade Nova de Lisboa, NOVA-LINCS, Caparica, Portugal
joao.lourenco@fct.unl.pt

Abstract
This paper proposes a novel suite of benchmarks for the evaluation of the structural testing of
concurrent programs with message-passing paradigm. This suite is composed of thirteen bugfree programs and ﬁve faulty programs. The benchmarks are developed in Java and are available
as free-software on the Internet. They were validated with experimental studies and also have
been used in diﬀerent research and for educational aims. The obtained results showed that the
benchmarks can generate qualiﬁed workload for the testing of message-passing programs. The
main contribution of this study is the development of a more robust and fair suite of benchmarks
capable of improving the evaluation of the testing activity applied to concurrent programs.
Keywords: Benchmark, Concurrent Programs, Message-Passing, Test, Tools, Model, Criteria, Java

1

Introduction

The development of concurrent programs is challenging. Unlike sequential programs, concurrent
programs are composed of processes and/or threads running independently and potentially
interacting with each other, which results in a non-deterministic behavior. These and other
features make the development of concurrent applications more complex and therefore more
error-prone, making essential the testing process in this types of software.
However, the testing of concurrent programs still requires further research and study. The
TestPar project (http://testpar.icmc.usp.br) [6], contributes to reduce the gap in the testing
of concurrent programs by developing new techniques, algorithms, models, criteria and tools.
Despite the progress already achieved with TestPar, the evaluation of those artifacts is still a
challenge. Some of the problems faced by researchers are related to both fair-comparison of
diﬀerent proposals and oﬀer of a set of programs simple enough to be validated manually, if necessary, but complex enough to exercise nontrivial aspects of communication and synchronization
found in real world concurrent programs.
2226

Selection and peer-review under responsibility of the Scientiﬁc Programme Committee of ICCS 2016
c The Authors. Published by Elsevier B.V.

doi:10.1016/j.procs.2016.05.387

A Suite of Java Message-Passing Benchmarks...

George, Paulo, Rafael ... and Joao

In the context of testing of concurrent programs, benchmarks are used for comparisons of
distinct approaches in a fair and uniform way, to determine if models can represent the programs
to be tested, if criteria can reveal bugs and guide the test data selection process and if testing
tools can properly deal with a wide range of source codes. Some of the main benchmarks of
concurrent programs found in the literature are IBM [3], Rungta [5], Valgrind [7], Inspect [8].
Although the above benchmarks contribute to the testing of concurrent programs, they
consider exclusively the shared memory paradigm. The majority does not have diﬀerent test
cases to cover distinct code segments, does not use new communication and synchronization
primitives and does not enable execution with a ﬂexible number of processes and/or threads.
To reduce such gaps, this paper presents a suite of message-passing bug-free and faulty
benchmarks developed by the TestPar group, which considers the message-passing paradigm
and makes extensive use of blocking, non-blocking and collective communication primitives,
including a complete and standardized documentation.

2

Our Benchmarks

The proposal benchmarks are composed of 13 bug-free Java message-passing programs (Table
1), which are divided into two categories: micro-benchmarks (01 to 04) and benchmarks (05
to 13). Micro-benchmarks are ideal for the validation of models, criteria and tools in early
stages of development. They have a smaller number of lines of code (LOC), a lower cyclomatic
complexity, require no input tests and use only one type of primitive (blocking, non-blocking
or collective) per process. Their reduced size enables the manual validation of a proposal.
Benchmarks, on the other hand, are larger in terms of lines of code and cyclomatic complexity,
can have two or more test cases and also provide ﬂexible the amount of processes. They
can have a combination of types of primitives in the same process and produce representative
computation, such as calculation of the Greatest Common Divisor (GCD) or simulations like
token-ring topologies. They aim to validate tools in advanced stages of development.
N◦
01
02
03
04

Micro-benchmarks
One_to_All
Blocking_MP_PP
Non_Blocking_MP
Non_Blocking_BSend

N◦
05
06
07
08
09

Benchmarks
Parallel_GCD
GCD_Two_Slaves
GCD_LCM
TR_Iterations
TR_Same_Primitives

N◦
10
11
12
13

Benchmarks
TR_Diﬀerent_Primitives
Sieve_of_Eratosthenes
Omega_Network
Roller_Coaster_MP

Table 1: Message-Passing Benchmarks.

This suite of benchmarks also has ﬁve faulty programs that were developed based on bug-free
benchmarks (Table 2). A single bug was inserted in each faulty benchmark to avoid interference
among bugs. Bugs 1, 3 and 4 (Table 3) are based on the error taxonomy proposed in [2]. Bugs
2 and 5 (Table 3) are based on known errors in sequential programs, but that also interfere
in concurrency aspects. We classify the inserted bugs into observability and locking error [4]
(Table 2). An observability error occurs when the program behavior can hide the error, i.e.
the program outputs can be showed as correct in some cases. A locking error occurs when a
program blocks forever waiting for a resource (a deadlock).
The bug inserted in benchmark 02_Blocking_MP_PP_Fault was the use of buﬀers with
smaller sizes than the messages sent by the client processes. This causes the server process to
receive incomplete information. The bug inserted in benchmark 03_Non_Blocking_MP_Fault
was the exclusion of commands related to communication aspects among processes. The exclusion of these commands may result in the printing of an empty string in the Client process,
if the message has not arrived in time in the non-blocking receive primitive. In benchmark
05_Parallel_GCD_Fault_1, the target process in a blocking send primitive was modiﬁed to
send the message to a process that already ﬁnished its execution. Despite of displaying the
2227

A Suite of Java Message-Passing Benchmarks...

Faulty Benchmarks
02_Blocking_MP_PP_Fault
03_Non_Blocking_MP_Fault
05_Parallel_GCD_Fault_1
05_Parallel_GCD_Fault_2
06_GCD_Two_Slaves_Fault

Bug
1
2
3
4
5

Type of error
observability
observability
locking
locking
observability

N◦
1
2
3
4
5

Table 2: Faulty Benchmarks.

George, Paulo, Rafael ... and Joao
Description of the Bugs, based on [2].
“Diﬀerent lengths of messages in matching send/receive operations"
“Command removed"
“Send/receive inconsistency caused by incorrect sender and/or
receiver speciﬁed for a message"
“Send/receive inconsistency caused by an error in the program
execution ﬂow"
“Assignment incorrectly using variables"

Table 3: Types of Inserted Bugs.

correct output, the process to which the message should be sent will be locked in a blocking
receive, waiting a message that will never be sent. The bug inserted in benchmark 05_Parallel_GCD_Fault_2 is related to a loop preceding the blocking send primitive. A subtraction
operation was replaced by an addition, which resulted in an inﬁnite loop that prevents the
execution of a send primitive. This bug can be revealed or not, depending of the selected test
case. The bug inserted in benchmark 06_GCD_Two_Slaves_Fault is an incorrect assignment
to a variable. The error may or may not be revealed, depending on the selected test case and
the order in which messages arrive in the receive primitive.
For more details, please visit the website http://testpar.icmc.usp.br/benchmarks, which
makes available the access to the source code and a complete documentation showing the
overview, the benchmark objective, expected inputs/outputs, run mode, table with the program features, pseudo-code and Parallel Control Flow Graph (PCFG) for every benchmarks.
A key point about this benchmarks is they are not toy-benchmarks, since they can establish more complex and larger interaction standards (communication and synchronization),
depending on how many processes and threads are created. For example, consider the TR_Iter
(TR_Iterations) benchmark (Table 4), which were executed in two ways: the ﬁrst one consists
of a token-ring topology composed of 4 processes and the second one composed of 101 processes.
Benchmark/Features
TR_Iter
(4 proc/101 proc)
TR_Same
(4 proc/101 proc)
TR_Dif
(4 proc/101 proc)
Sieve
(4 proc/101 proc)
Omega_Network
(9 proc/113 proc)
Roller_Coaster_MP
(5 proc/102 proc)

LOC

Nr. of
Sends

Nr. of
Receives

Nr. of Nodes
and Edges

Nr. of Op.
over Data

Info Model
Total

Nr. of Sync
Edges

297 / 6214

4 / 101

4 / 101

176 / 4444

612 / 15259

796 / 19905

12 / 10100

357 / 7244

4 / 101

4 / 101

207 / 4960

683 / 16688

898 / 21850

12 / 10100

389 / 8343

17 / 405

7 / 201

362 / 9092

827 / 21294

1213 / 30992

51 / 40500

510 / 11762

5 / 102

7 / 201

475 / 12794

1049 / 27821

1536 / 40918

15 / 10200

1051 / 15043

33 / 545

13 / 193

730 / 12194

2674 / 45606

3450 / 58538

372 / 104160

562 / 9680

15 / 403

11 / 205

238 / 4603

1562 / 34154

1826 / 39365

132 / 81806

Table 4: Information of how the increase of processes inﬂuence the amount of elements generated.

The analysis of the data presented in Table 4 indicates that the amount of LOC, and the
number of send and receive primitives (Nr. of Sends and Nr. of Receives columns) increase
proportionally with the amount of created processes. Looking at the Nr. of Nodes and Edges
column (which corresponds to the sum of the nodes and edges that compose each benchmark) it
is possible to notice a signiﬁcant increase, from 176 to 4444. The Nr. of Op. over Data column
(which corresponds to the number of operations that involves dataﬂow, like deﬁnition and use
of variables) also shows the increase in the complexity of each benchmark when instantiating
more processes. The Info Model Total column summarize the information obtained in the four
previous columns into a single value. Finally, the Nr. of Sync Edges column shows the increase
in the amount of synchronization edges generated by the ValiPar tool, from the test model and
criterion All-Sync-Edges which, in this particular case, increased from 12 to 10100. Based on
the information summarized on Table 4, is possible to conclude that the developed benchmarks
are not toy-benchmarks once they show a signiﬁcant increase in diﬀerent perspectives, especially
those related in some way, to communication and synchronization among processes and threads.
2228

A Suite of Java Message-Passing Benchmarks...

3

George, Paulo, Rafael ... and Joao

Validation of the Benchmarks

The objective of this validation is to verify the ability of the benchmarks to generate a controlled
and qualiﬁed demand on testing models, criteria and tools. Here, controlled and qualiﬁed
demand are deﬁned as the use of known programs with well-deﬁned features. The validation
considered all bug-free and faulty benchmarks, applied to the Java ValiPar [6] testing tool.
All experiments were conducted on a cluster with 13 nodes interconnected by a Gigabit
Ethernet. Each node is an Intel(R) Core(TM) i7-4790 3.60GHz processor with 32 GB of RAM
managed by Ubuntu 14.04.1 LTS. OpenJDK Java Virtual Machine 1.7 was used. Each physical
node has 4 virtual hosts, each with 4 virtual cores and 8 GB of RAM. The KVM was used as
a Virtual Machine Monitor (VMM). Faulty benchmarks were used to verify the eﬀectiveness of
testing criteria in revealing defects. After executing all interleavings (variants) for diﬀerent test
cases and evaluating selected testing criteria, all inserted bugs were revealed by Java ValiPar.
Table 5 provides data about the non-deterministic and deterministic executions, trace ﬁle
and generation of variants from bug-free benchmarks using Java ValiPar tool. The deterministic
execution aims at replaying a synchronization sequence from a concurrent program. In the
non-deterministic execution the program can perform diﬀerent pairs of synchronizations. The
trace ﬁles register information about the synchronizations performed by the program, and the
generation of variants are used for the automatic generation of new synchronization sequences.
Non-Determ. column (Table 5) shows that all benchmarks were successfully executed in
a non-deterministic mode. This shows that our benchmarks exercise diﬀerent primitives in
the Java ValiPar Tool and respond with expected outputs. The Determ. column shows the
Java ValiPar tool successfully managed the deterministic execution of 12 out of 13 benchmarks.
09_TR_Same_Primitives benchmark was the exception, and the analysis of the trace ﬁles revealed the program found a limitation in the deterministic execution algorithm, known as cyclic
dependency [1]. In few words, a cyclic dependency is a deadlock produced by the deterministic
execution algorithm and occurs because such algorithm always assumes messages are received
in an FIFO order; for speciﬁc cases the FIFO order is not true.
Trace ﬁles were correctly generated for all benchmarks, based on the deterministic executions
that were carried out successfully. Furthermore, non-deterministic and deterministic executions
generate equal or equivalent trace ﬁles (only with a subtle diﬀerence in the timestamps of
the communications). The generation of variants (Variant column) was successful for most
benchmarks, except where there were collective primitives not supported by the algorithm.
Table 6 provides information about the number of synchronization edges generated by the
Java ValiPar tool (Total column), executable (Exec.) and non-executable (Non-Exec.) synchronization edges, and average of executed variants / average of generated variants (Variants)
- based on an average of 10 executions. Some benchmarks have the result 0/0 for variants
because they execute the portion of the variant generation algorithm responsible for discarding
non-executable required elements, since the variant would cause deadlock, according to traces.
Table 7 shows the covered required elements / generated required elements for all-uses (AU),
all-intra-message-uses (AAMU) and all-inter-message-uses (AEMU) criteria. These results
show the incremental feature of the benchmarks. The number of required elements increases
as the complexity of the benchmark increases in relation to cyclomatic complexity, number of
synchronizations primitives, number of LOC and number of processes.
The (AAMU) column in Table 7, shows none of the elements is covered, being always 0/X
(where X is a positive integer). This result is expected since this criterion corresponds to an
intra-process communication, i.e. communication of a send/receive pair in the same process,
something possible to occur, although not expected. The communication pattern implemented
in this suite does not enable intra-process communication. When the benchmark does not use
senders and receivers primitives in the same process, the pattern is 0/0, otherwise, the X will
be a positive integer value.
2229

A Suite of Java Message-Passing Benchmarks...
N ◦ Non-Determ. Determ. Trace Variant
01
yes
yes
yes
02
yes
yes
yes
yes
yes
yes
yes
yes
03
04
yes
yes
yes
yes
05
yes
yes
yes
yes
06
yes
yes
yes
yes
07
yes
yes
yes
yes
08
yes
yes
yes
yes
09
yes
no
no
yes
10
yes
yes
yes
11
yes
yes
yes
12
yes
yes
yes
yes
13
yes
yes
yes
yes

Table 5: Execution Informations.

4

George, Paulo, Rafael ... and Joao

N ◦ Total Exec. Non-Exec.
Variants
01
4
4
0
02
16
12
4
5,2/5,2
6
4
2
0,0/0,0
03
04
6
4
2
6,0/12,0
05
24
7
17
18,0/54,0
06
12
6
6
18,0/58,0
07
39
13
26
52,8/146,6
08
12
8
4
0,0/0,0
09
12
8
4
12,3/70,7
10
51
19
32
11
15
9
6
12
372
28
344
0,0/0,0
13
132
19
113
309,6/4089,5

Table 6: Data of Synchronization Edges.

N◦
AU
AAMU AEMU
01
22/42
0/0
4/8
02
25/107
0/0
46/522
03
31/103
0/15
20/30
04
16/47
0/7
10/14
05 124/257
0/59
17/123
06 114/283
0/30
6/60
07 278/628
0/71
25/159
08 168/339
0/8
8/24
09 176/451
0/21
10/63
10 227/546
0/236
34/726
11 301/1381
0/6
9/54
12 452/2199 0/985 88/4724
13 167/1185 0/1008 32/3066

Table 7: Data of testing criteria (covered / generated required elements).

Conclusions and Future Work

This paper presented a suite of thirteen bug-free message-passing benchmarks and ﬁve versions
with documented bugs. All of them were developed in Java language and support the validation
and evaluation of testing models, criteria and tools. Experimental studies on ValiPar structural
testing tool were conducted to validate the benchmarks and indicating that they can generate
controlled and qualiﬁed demands on models, criteria and tools used for the testing of concurrent
programs. The experiments also revealed strengths and limitations of the artifacts.
Our benchmarks were also used to evaluate other research work. For instance, the benchmarks were used in the evaluation of new techniques for the extraction of structural information
from Java concurrent programs. They were also used for evaluation of new algorithms for deterministic execution of concurrent programs. In other research work, the benchmarks were
used to support the investigation about the beneﬁts of the use of automatic generation of test
data techniques, deﬁned to sequential programs, applied to concurrent programs. Also, the
benchmarks have been used to support the teaching of concurrent programming and testing.
Therefore, it was possible to conﬁrm the applicability of our benchmark in other research scenarios. The future works include: the development of more faulty benchmarks; development of
instances of these benchmarks for C/MPI; and the investigation on how to use properly these
benchmarks as open educational resources.

References
[1] R. N. Batista. Otimizando o teste estrutural de programas concorrentes: uma abordagem determinística e paralela. Master’s thesis, ICMC/USP, São Carlos, SP, 2015. In: Portuguese.
[2] J. DeSouza, B. Kuhn, B. R. de Supinski, V. Samofalov, S. Zheltov, and S. Bratanov. Automated,
scalable debugging of mpi programs with intel message checker. SE-HPCS, pages 78–82, New York,
NY, USA, 2005. ACM.
[3] Y. Eytani and S. Ur. Compiling a benchmark of documented multi-threaded bugs. In Parallel and
Distributed Processing Symposium, 2004. Proceedings. 18th International. IEEE, 2004.
[4] H. Krawczyk, B. Wiszniewski, and P. Mork. Classiﬁcation of software defects in parallel programs.
Relatório Técnico, Faculty of Eletronics, Technical University of Gdansk, Poland, 1994.
[5] N. Rungta and E. G. Mercer. Clash of the titans: Tools and techniques for hunting bugs in
concurrent programs. In 7th Workshop on Parallel and Distributed Systems: Testing, Analysis, and
Debugging, pages 9:1–9:10. ACM, 2009.
[6] P. S. L. Souza, S. R. S. Souza, M. G. Rocha, R. R. Prado, and R. N. Batista. Data ﬂow testing in
concurrent programs with message passing and shared memory paradigms. In ICCS 2013.
[7] Valgrind-Developers. Valgrind-3.6.1, 12 2013.
[8] Y. Yang. Inspect: A framework for dynamic veriﬁcation of multithreaded C programs, 12 2013.

2230

