Available online at www.sciencedirect.com

Procedia Computer Science 18 (2013) 1730 – 1736

International Conference on Computational Science, ICCS 2013

The Support Vector Regression with Adaptive Norms
Chunhua Zhanga , Dewei Lia , Junyan Tanb
a Information
b College

School, Renmin University of China, Beijing 100872, China
of Science, China Agricultural University, Beijing 100083, China

Abstract
This study proposes a new method for regression – l p -norm support vector regression (l p SVR). Some classical SVRs minimize
the hinge loss function subject to the l2 -norm or l1 -norm penalty. These methods are non-adaptive since their penalty forms are
ﬁxed and pre-determined for any types of data. Our new model is an adaptive learning procedure with lp-norm (0 < p < 1),
where the best p is automatically chosen by data. By adjusting the parameter p, l p SVR can not only select relevant features
but also improve the regression accuracy. An iterative algorithm is suggested to solve the l p SVR eﬃciently. Simulations and
real data applications support the eﬀectiveness of the proposed procedure.
Keywords: Regression; Support vector machine; Norm; Feature selection;

1. Introduction
Support vector machines (SVMs), being computationally powerful tools for pattern classiﬁcation and regression, have been successfully applied to a variety of real-world problems([1]- [6]). Regards to the support vector
regression (SVR), some classical SVRs minimize the hinge loss function subject to the l2 -norm or l1 -norm penalty
([7]). We call them l2 SVR or l1 SVR correspondingly. These methods are non-adaptive since their penalty forms
are ﬁxed and pre-determined for any types of data.
Recently, l p -norm (p ∈ (0, 1)) attracts great attention in the optimization framework, the idea that using l p norm can ﬁnd sparse solutions is considered in [8]-[11]. Correspondingly, [12]-[17] propose l p -norm (0 < p < 1)
support vector machine for classiﬁcation (l p SVC), which replace the l2 -norm penalty by the l p -norm (p ∈ (0, 1))
penalty in the objective function in the primal problem in the standard linear l2 SVC. Compared with SVC with a
ﬁxed norm, l p SVC is desired for feature selection since it can automatically select relevant features by adjusting
the parameter p. However, l p SVC is used only to solve classiﬁcation problems. This motivates us to consider a
new model with l p -norm for regression problems.
This paper proposes a new method for regression – l p -norm support vector regression (l p SVR), which replaces
l2 -norm by l p -norm (0 < p < 1) in the classical l2 SVR. Our new model is an adaptive learning procedure with
l p -norm (0 < p < 1), where the best p is automatically chosen by data. By adjusting the parameter p, l p SVR
can not only select relevant features but also improve the regression accuracy. In order to solve the non-convex
problem in our model, an eﬃcient algorithm is constructed using the successive linear approximation algorithm
(SLA)([18]).
E-mail address: zhangchunhua@ruc.edu.cn Tel.:+86-010-8250-0694.

1877-0509 © 2013 The Authors. Published by Elsevier B.V. Open access under CC BY-NC-ND license.
Selection and peer review under responsibility of the organizers of the 2013 International Conference on Computational Science
doi:10.1016/j.procs.2013.05.341

1731

Chunhua Zhang et al. / Procedia Computer Science 18 (2013) 1730 – 1736

Now we describe our notation. All vectors are column vectors unless transposed to a row vector by a super
script T . For a vector x in Rn , [x]i (i = 1, 2, · · · , n) denotes the i-th component of x. |x| denotes a vector in Rn of
1
absolute value of the components of x. x p denotes that (|[x]1 | p + · · · + |[x]n | p ) p . Strictly speaking, x p is not a
general norm when 0 < p < 1, but we still follow this term l p -norm, because the forms are same except that the
values of p are diﬀerent. x 0 is the number of nonzero components of x. For two vectors x ∈ Rn and y ∈ Rn , (x · y)
denotes the inner product of x and y.
This paper is organized as follows. In section 2, the l p SVR is introduced. In section 3, the SLA is proposed to
solve l p SVR. In section 4, the lower bounds for the absolute value of nonzero entries in any local optimal solution
is established. In section 5, numerical experiments are given to demonstrate the eﬀectiveness of our method. We
conclude this paper in section 6.
2. l p Support Vector Regression
Suppose that the training set T is given by
T = {(x1 , y1 ), · · · , (xl , yl )} ∈ (Rn × R)l ,

(1)

where x j ∈ Rn , y j ∈ R ( j = 1, · · · , l), the linear regression problem is to ﬁnd a decision function f (x) = (w · x) + b
to derive the value of y for any x by the function y = f (x).
In the classical l2 SVR, the decision function is decided by the solution to the following optimization problem:
min

w, b, ξ,η

s.t.

1
w
2

l

(ηi + ξi ) ,

(2)

((w · xi ) + b) − yi ≤ ε + ηi , i = 1, · · · , l ,
yi − ((w · xi ) + b) ≤ ε + ξi , i = 1, · · · , l ,
ξi , ηi ≥ 0 , i = 1, · · · , l.

(3)
(4)
(5)

2
2

+C
i=1

Replacing the ﬁrst term 12 w 22 in the objective function of the above problem by w
proposes the following problem:

p
p

(0 < p < 1), l p SVR

l

min

w, b, ξ,η

s.t.

w

p
p

+C

(ηi + ξi ) ,

(6)

i=1

((w · xi ) + b) − yi ≤ ε + ηi , , i = 1, · · · , l ,
yi − ((w · xi ) + b) ≤ ε + ξi , i = 1, · · · , l ,
ξi , ηi ≥ 0 , i = 1, · · · , l,

(7)
(8)
(9)

where C (C > 0), p (0 < p < 1) and ε (ε > 0) are parameters. The algorithm of l p SVR is described as follows:
Algorithm 1
1. Give the training set T = {(x1 , y1 ), · · · , (xl , yl )} ∈ (Rn × R)l , where xi ∈ Rn ,yi ∈ R, i = 1, · · · , l;
2. Select proper parameters C, p, ε, where C > 0, 0 < p < 1 and ε > 0;
3. Solve problem (6)-(9) and get the solution (w∗ , b∗ );
4. Select the feature set: {i|[w∗ ]i 0, (i = 1, · · · , n)};
5. Construct the decision function y = (w˜ ∗ · x) + b∗ , where the components of w˜ ∗ are nonzero components of
w∗ and the components of x˜ are also corresponding to nonzero components of w∗ .
3. The SLA for problem (6)-(9)
Consider the problem (6)-(9), the objective function is not diﬀerentiable, because of the absolute value in the
ﬁrst item. In order to make this problem smooth, we introduce the variable ν = ([ν]1 , · · · , [ν]n )T , and get the

1732

Chunhua Zhang et al. / Procedia Computer Science 18 (2013) 1730 – 1736

following equivalent problem:
l

n

[ν]ip + C

(ηi + ξi ) ,

(10)

((w · xi ) + b) − yi ≤ ε + ηi , , i = 1, · · · , l ,
yi − ((w · xi ) + b) ≤ ε + ξi , i = 1, · · · , l ,
ξi , ηi ≥ 0 , i = 1, · · · , l,
−ν ≤ w ≤ v.

(11)
(12)
(13)
(14)

min

w,ν, b, ξ, η

i=1

s.t.

i=1

When p ∈ (0, 1), we note that the problem (10)-(14) is diﬀerentiable, but not convex. In fact, it is the minimization
of a concave objective function over a polyhedral set. Even though it is diﬃcult to ﬁnd a global solution to
this problem, a fast successive linear approximation (SLA) algorithm ([18]) terminates ﬁnitely at a stationary
point which satisﬁes the necessary optimality condition for problem (10)-(14). For convenience we state the SLA
algorithm below.
Algorithm 2 (SLA for problem (10)-(14))
1. Select the proper parameters C > 0, 0 < p < 1, ε > 0 and a precision δ(0 < δ << 1), start with a random
ν0 = ([ν]01 , [ν]02 , · · · , [ν]0n )T and let k = 1;
2. Solve the following problem
l

n

[νk−1 ]ip−1 [ν]i + C

min

w, ν, b, ξ, η

(ηi + ξi ) ,

i=1

s.t.

where (νk−1 ) p−1

((w · xi ) + b) − yi ≤ ε + ηi , , i = 1, · · · , l ,
yi − ((w · xi ) + b) ≤ ε + ξi , i = 1, · · · , l ,
ξi , ηi ≥ 0 , i = 1, · · · , l,
−ν ≤ w ≤ v.
= ([νk−1 ]1p−1 , · · · , [νk−1 ]np−1 )T , and get its solution (wk , bk , ηk , ξk , νk );

n

(16)
(17)
(18)
(19)

l

[νk−1 ]ip−1 ([νk ]i − [νk−1 ]ip−1 ) + C

3. If

(15)

i=1

i=1

(ηki − ηk−1
+ ξik − ξik−1 ) < δ(0 < δ
i

1), then stop and get the

i=1

solution w∗ = wk , b∗ = bk ; Otherwise, let k = k + 1 and go back to step 2.
4. The lower bounds for nonzero components in solutions
In Algorithm 1, it is easy to see that our l p SVR can accomplish feature selection and regression simultaneously. Feature selection needs to ﬁnd the nonzero components of the solution to the problem (6)-(9). However,
usually the above Algorithm 2 can only provide an approximate local solution where nonzero components in the
solution can not be identiﬁed theoretically. Using a similar strategy in [8], we get the following theorem 1, which
can be used to identify nonzero components in any local optimal solutions to the problem (6)-(9), even though the
Algorithm 2 can only ﬁnd the approximate local optimal solution.
Theorem 1 For any local optimal solution (w∗ , b∗ , ξ∗ ) to the problem (6)-(9), we have
l

|[w∗ ] j | ≥ (p/(C

1

|xi | j )) 1−p , j = 1, 2, · · · , n.
i=1

Proof. Suppose w∗ 0 = k, (1 < k ≤ n), without loss of generality, let w∗ = ([w]∗1 , · · · , [w]∗k , 0, · · · , 0)T where
[w]∗i
0, i = 1, · · · , k. Let w˜ ∗ = ([w]∗1 , · · · , [w]∗k )T , x˜i = ([xi ]1 , · · · , [xi ]k )T ∈ Rk , i = 1, · · · , l, we consider a new
optimization problem:
l

min

˜ ξ,
˜ η˜
w,
˜ b,

w˜

p
p

+C

(˜ηi + ξ˜i ) ,
i=1

(20)

1733

Chunhua Zhang et al. / Procedia Computer Science 18 (2013) 1730 – 1736

˜ − yi ≤ ε + η˜ i , , i = 1, · · · , l ,
((w˜ · x˜i ) + b)
yi − ((w˜ · x˜i ) + b) ≤ ε + ξ˜i , i = 1, · · · , l ,
ξ˜i , η˜ i ≥ 0 , i = 1, · · · , l,

s.t.

(21)
(22)
(23)

where w˜ ∈ Rk , b˜ ∈ R, η˜ ∈ Rl , ξ˜ ∈ Rl . Obviously, (w˜ ∗ , b∗ , η∗ , ξ∗ ) is a local minimizer of problem (20)-(23).
According to the KKT condition, there exist Lagrange multipliers α∗i , β∗i (i = 1, · · · , l) satisfy:
l

p(|w˜ ∗ | p−1 · sign(w˜ ∗ )) −

(β∗i − α∗i ) x˜i = 0,

(24)

i=1

0 ≤ α∗i ≤ C, 0 ≤ β∗i ≤ C.

(25)

According to (24), we have
l

p(|w˜ ∗ | p−1 · sign(w˜ ∗ )) =

(βi − αi ) x˜i .
i=1

Furthermore, by (25), we have
p |w˜ ∗ | p−1 =
So, w∗j ≥ (p/(C

l

l
i=1

l

(βi − αi ) x˜i ≤

i=1

|βi − αi | | x˜i | ≤ C

l
i=1

|xi |, 0 < p < 1

1

i=1

|xi | j )) 1−p , for j = 1, · · · , n.

According to Theorem 1, we can identify the nonzero components of the local optimal solution to (6)-(9).
Based on the Algorithm 2 and Theorem 1, the new algorithm is established as follows:
Algorithm 3 (l p -SVR):
1. Give the training set T = {(x1 , y1 ), · · · , (xl , yl )} ∈ (Rn × R)l , where xi ∈ Rn ,yi ∈ R, i = 1, · · · , l;
2. Select proper parameters C, p, , where C > 0, 0 < p < 1;
3. Solve problem (6)-(9) by Algorithm 2 and get the solution (w∗ , b∗ );
4. Compute L j = (p/(C

l

1

i=1

|xi | j )) 1−p , j = 1, · · · , n; select the feature index set: F = {i|[w∗ ]i | ≥ Li , i = 1, · · · , n};

5. Construct the decision function f (x) = sgn((w˜ ∗ · x˜) + b∗ ), where w˜ ∗ are composed by the components in the
F of w∗ and the components of x˜ are also corresponding to components in the feature set F of w∗ .
In the following section, our experiments are conducted according to the algorithm 3.
5. Numerical experiments
In this section, some experiments on simulation datasets and real datasets are conducted respectively, by
comparing l p SVR with l2 SVR, l1 SVR. Note that, the performance of each method depend on the parameters (C,
p and ε in l p SVR; C and ε in l2 SVR and l1 SVR). Therefore, these parameters should be adjusted properly. In
our experiments, the best value of these parameters are chosen by ﬁve-fold cross validation. C is obtained through
searching in the range 2−7 − 24 , p is chosen from 0.1 − 0.9 and ε is chosen from 0.01 to 0.1.
In order to evaluate the performance of algorithms, some evaluation criteria ([19], [20]) commonly used should
be introduced in the following:
m

1
MS E =
m

m

( f (xi ) − yi ) , MAPE =
2

i=1

|yi − f (xi )|
|yi |

m

i=1

m
i=1

× 100%, N MS E =

(yi − f (xi ))2
m

i=1
m

R2 =

m

( f (xi ) − y¯ )2

i=1
m

i=1

(yi − y¯ )2

m
i=1

,r =

m

(m

f (xi
i=1

)2

f (xi )yi −
−(

m
i=1

f (xi
i=1

m

f (xi )

m

(yi − y¯ )2

))2 )(m

yi
i=1

m
i=1

y2i

−(

,

m

yi
i=1

)2 )

,

1734

Chunhua Zhang et al. / Procedia Computer Science 18 (2013) 1730 – 1736

where m is the number of testing samples, f (xi ) denotes the predict value of xi and y¯ is the average value of
y1 , · · · , yl .
5.1. Simulation datasets
The simulation datasets are generated as follows. The inputs xi ∈ Rn are stochastic vectors independently
generated in [0, 1], and the numbers of samples and features are described in Table 1. The outputs are determined
Table 1. Description of simulation datasets
Datasets
1
2
3
4

No. of samples
400
500
500
100

No. of features
50
60
50
120

by some simple functions. For example, in dataset 1, the output yi is given by
yi = 2[xi ]1 + 3[xi ]2 + 4[xi ]3 + 0.1 × rand(1);
In dataset 2,
yi = 8[xi ]1 − 7[xi ]5 + 6[xi ]9 − 5[xi ]13 + 4[xi ]20 − 3[xi ]31 + 2[xi ]45 − [xi ]49 + rand(1);
In dataset 3,
yi = 100[xi ]3 + 20[xi ]17 + 3[xi ]21 + 0.4[xi ]36 + 0.05[xi ]44 ;
In dataset 4,
yi = 2[xi ]1 + 3[xi ]2 + 4[xi ]3 + rand(1).
The results on four datasets are illustrated in Table 2. We show the eﬀectiveness of l p SVR from two aspects:
feature selection and regression accuracy. On the one hand, from the data in 3th column, it is easy to see that
l p SVR selects the minimal features. On the other hand, the data in 4th-8th column show that l p SVR derives
the smallest MSE, MAPE, NMSE, and the largest R2 , r among these methods in most datasets. This indicates
that the statistical information in these datasets is well presented by our l p SVR with fairly small feature sets and
regression errors.
Table 2. Results on simulation datasets
Datasets
Regressor
No. of selected
features
1
l p SVR
3
50
l2 SVR
l1 SVR
3
2
l p SVR
8
60
l2 SVR
l1 SVR
16
3
l p SVR
5
50
l2 SVR
l1 SVR
16
4
l p SVR
3
120
l2 SVR
l1 SVR
3

MSE

MAPE

NMSE

R2

r

0.0008
0.0017
0.0008
0.0811
0.0871
63.0302
0.0763
0.1038
0.3486
0.1222
1.2196
0.1420

0.0064
0.0084
0.0062
0.1561
0.1623
5.6449
0.0346
0.0393
0.0802
0.0780
0.2420
0.0734

0.0003
0.0006
0.0003
0.0044
0.0048
3.4472
0.0221
0.0301
0.1011
0.0377
0.3758
0.0438

1.0080
1.0059
0.9963
0.9848
0.9999
3.6134
0.9596
0.9622
1.0428
0.9023
0.6735
0.7510

0.9999
0.9997
0.9999
0.9978
0.9976
0.7724
0.9889
0.9849
0.9879
0.9843
0.7999
0.9875

5.2. Real datasets
For further evaluation of our method, we choose four real datasets: ”bodyfat”, ”cpusmall”, ”housing” and
”insurance”, which are commonly used in testing machine learning algorithms. More detailed description can be
found in Table 3.
Table 4 lists the results of three methods on four real datasets. It can be seen that our l p SVR can accomplish
the desired feature selection and achieve the good regression accuracy. The reason maybe that it can balance these
two aspects better than the other two methods by adjusting the parameter p.

1735

Chunhua Zhang et al. / Procedia Computer Science 18 (2013) 1730 – 1736
Table 3. Description of real datasets
Datasets
bodyfat
cpusmall
housing
insurance

Table 4. Results on real datasets
Datasets
Regressor
bodyfat

cpusmall

housing

insurance

l p SVR
l2 SVR
l1 SVR
l p SVR
l2 SVR
l1 SVR
l p SVR
l2 SVR
l1 SVR
l p SVR
l2 SVR
l1 SVR

No. of samples
252
500
452
500

No. of selected
features
1
14
1
8
12
2
3
13
3
9
83
2

No. of features
14
12
13
85

MSE

MAPE

NMSE

R2

r

0.0000
0.0000
0.0305
0.0010
0.0009
0.0992
0.0097
0.0097
0.1372
2.3464
2.8524
3.3524

0.0017
0.0016
0.0981
0.0124
0.0122
0.1625
0.0446
0.0416
0.2673
0.1123
0.1121
0.1556

0.0004
0.0004
1.1352
0.1508
0.1445
15.4089
0.2607
0.2587
3.6738
0.0141
0.0172
0.0198

1.0017
1.0041
0.0550
0.7845
0.7973
14.7173
0.7223
0.7710
3.7220
0.9411
0.9916
0.9070

0.9998
0.9998
0.1710
0.9354
0.9378
0.2143
0.8607
0.8641
0.7769
0.9933
0.9917
0.9927

6. Conclusions
For regression problems, a new model l p SVR is proposed in this paper. The main contribution is that the
desired feature selection and good regression performance are implemented simultaneously by introducing the
adaptive norms – l p -norm, where the parameter p can be chosen ﬂexibly in (0, 1) by data. Computational comparisons between our l p SVR and other popular methods including l2 SVR and l1 SVR indicate the eﬀectiveness
of our method. We believe that its good performance mainly comes from the fact that the parameter p is adjusted
properly.
Acknowledgements
This work has been partially supported by grants from National Natural Science Foundation of China (No.11271361,
No.11201480) and Chinese Universities Scientiﬁc Fund (No.2011JS039).
References
[1] V.Vapnik, Statistical Learning Theory, Wiley, New York (1998).
[2] Y. Tian,Y. Shi, X. Liu, Recent advances on support vector machines research, Technological and Economic Development of Economy,
2012, 18(1): 5-33.
[3] Z. Qi, Y. Tian, Y. Shi, Robust twin support vector machine for pattern classiﬁcation, Pattern Recognition, 2013, 46(1): 305-316.
[4] Z. Qi, Y. Tian, Y. Shi, Laplacian twin support vector machine for semi-supervised classiﬁcation, Neural Networks, 2012, 35:46-53.
[5] Z. Qi, Y. Tian, Y. Shi, Twin support vector machine with Universum data, Neural Networks, 2012, 36C:112-119.
[6] N.Deng, Y.Tian, C.Zhang, Support Vector Machines – optimization based theory, algorithms and extensions, CRC Press (2012).
[7] P.Bradley, O.Managsarian, Feature selection via concave minimization and support vector machines, The Fifth International Conference
on Machine Learning (1998), 82-90.
[8] X.Chen, F.Xu, Y.Ye, Lower bound theory of nonzero entries in solutions of l2 -l p minimization (2009)http://www.standardford.edu/ yyye/.
[9] A. Bruckstein, D. Donoho, M. Elad, From sparse sulutions of systems of equations to sparse modeling of signals and images. SIAM
Reviewer 51 (2009) 34-81.
[10] J. Fan, R. Li, Varible selection via nonconcave penalized likelihood and its oracle properties, J Amer Statis Assoc 96 (2001) 1348-1360.
[11] Z. Xu, H. Zhang, Y. Wang, X. Chang, L 1 regularizer, Science in China Series F-InfSci 52 (2009) 1-9.
2
[12] W. Chen, Y.Tian, L p -norm proximal support vector machine and its applications, Procedia Computer Science, ICCS 1(1) (2010) 24172423.
[13] Y.Tian, J.Yu, W.Chen, l p -norm support vector machine with CCCP, In Proc. the 7th FSKD (2010) 1560-1564.
[14] J.Tan, C.Zhang, N.Deng, Cancer related gene identiﬁcation via p-norm support vector machine, The 4th International Conference on
Computational Systems Biology (2010) 101-108.

1736

Chunhua Zhang et al. / Procedia Computer Science 18 (2013) 1730 – 1736

[15] C.Zhang, J.Tan, etc. Feature Selection in multi-instance learning, The International Symposium on Operations Research and its Applications (2010) 462-469.
[16] J.Tan, Z.Zhang, L.Zhen, C.Zhang, N.Deng, Adaptive feature selection via a new version of support vector machine. Neural Comput &
Applic, (2012) doi:10.1007/s00521-012-1018-y.
[17] C.Zhang, Y.Shao, J.Tan, N.Deng, Mixed-norm linear support vector machine, Neural Comput & Applic, (2012) doi:10.1007/s00521012-1166-0.
[18] P.Bradley, O.Mangasarian, W.Street, Feature selection via mathematical programming, INFORMS Journal on Computing (1998),
doi:10.1287/ijoc.10.2.209.
[19] S.Weisberg, Applied linear regression seconded, Wiley, New York (1985).
[20] R.Staudte, S.Sheather, Robust estimationand testing: Wiley series in probability and mathematical statistics, Wiley, New York (1990).

