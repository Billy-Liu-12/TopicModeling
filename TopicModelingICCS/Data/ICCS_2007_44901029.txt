A Novel Multi-objective Evolutionary
Algorithm
Bojin Zheng1 and Ting Hu2
1

2

College of Computer Science,South-Central University For Nationalities,
Wuhan,430074,China
Department of Computer Science, Memorial University of Newfoundland, St.
John’s, NL, A1B 3X5, Canada
zhengbojin@gmail.com, tingh@cs.mun.ca
Abstract. Evolutionary Algorithms are recognized to be eﬃcient to deal
with Multi-objective Optimization Problems(MOPs) which are diﬃcult to
be solved with traditional methods. Here a new Multi-objective Optimization Evolutionary Algorithm named DGPS which is compound with Geometrical Pareto Selection Method (GPS), Weighted Sum Method (WSM)
and Dynamical Evolutionary Algorithm (DEA) is proposed. Some famous
benchmark functions are carried out to test this algorithm’s performance
and the numerical experiments show that this algorithm runs much faster
than SPEA2, NSGAII, HPMOEA and can obtain ﬁner approximate
Pareto fronts which include thousands of well-distributed points.
Keywords: Multi-Objective Optimization, Evolutionary Algorithm, Geometrical Pareto Selection, Weighted Sum Method.

1

Introduction

Multi-objective optimization problems(MOPs)are very common in economics,
and engineering etc., but they are a class of very diﬃcult problems. In 1984,
David Schaﬀer proposed Vector Evaluated Genetic Algorithm (VEGA) [1,2] to
deal with MOPs. Since then many Multi-objective Optimization Evolutionary
Algorithms (MOEAs) are proposed, most of them base on Pareto techniques.
Since the works of E. Zitzler et al.[3,4,5] in 1999, the importance of elitism
strategy in multi-objective search was recognized and supported experimentally.
In general, , the current popular MOEAs employ explicit or implicit “archive”
[6,7] which is designed to store the non-dominated solutions to implement the
elitism. So the schema of current MOEAs could be depicted as follow[8]:
MOEA = Archive + Generator
The equation means that if we want to design an eﬃcient MOEA, we should
pay attention to two aspects: ﬁrstly, design a more eﬃcient algorithm to deal
with archive, i.e., elitist space, secondly, design a more eﬃcient algorithm in the
production of new individuals.
As to the generator, hundreds of Evolutionary Algorithms would be meaningful to us. Among them, Dynamical Evolutionary Algorithm (DEA) [9] would be
worth of researching. Zou [10] proposed High Performance MOEA (HPMOEA)
Y. Shi et al. (Eds.): ICCS 2007, Part IV, LNCS 4490, pp. 1029–1036, 2007.
c Springer-Verlag Berlin Heidelberg 2007

1030

B. Zheng and T. Hu

[11] which combines DEA with Ranking method and Niching method to deal
with MOPs and reported good results.
As to the archiving algorithm, except the schema of “Ranking-alike operator
+ Niching-alike operator”, there exists another schema, which is called “the
sampling schema”. Geometrical Pareto Selection (GPS) would be such a typical
algorithm[12,13].
In this paper, we propose a novel MOEA which combines Geometrical Pareto
Selection Method (GPS), Weighted Sum Method (WSM) and Dynamical Evolutionary Algorithm (DEA) together. The experimental results show that this
algorithm runs much faster than SPEA2, NSGAII[14], HPMOEA and can obtain ﬁner approximate Pareto fronts which include thousands of well-distributed
points.

2

Background

In this section, we present three major components of DGPS to help understand
our work. For the detail information please refer to [10],[13].
2.1

Geometrical Pareto Selection

For convenience of discussion, we only discuss how to deal with the two-objective
optimization problems.
The method can be depicted as follows:
1. Use some techniques to estimate Pareto front,F1 ⊂ (L1 , U1 ),F2 ⊂ (L2 , U2 ).
Here F is the objective functions.
2. Choose a constant integer κ, here κ is an angle constant which splits the
Pareto front equally. Create an array: Array. And the array,i.e.,archive, is
used to store the current best solutions (but not non-dominated solutions).
3. Choose a point A(x, y) which is far away from Pareto front.
4. When a new individual N(x1 ,y1 ) will be inserted into archive, use such rules:
– Compute its slope α with point A(x,y).
α = xy11 −y
−x
– Compute its distance dis with point A(x,y).
dis = (y1 − y)2 + (x1 − x)2
– Compute the location pos in archive.
2 −y
, here γ = L
pos = α−γ
κ
L1 −x
– Modify the archive. If dis > Array[pos].dis, then Array[pos] := N (x1 , y1 ).

For the above method, it’s easy to generalize this method from 2-objective
problems to N-objective problems. Namely, we can get the angles respectively
by using the projective method. We should get N-1 angles totally and then ﬁnd
the very point to compare with the current point.

A Novel Multi-objective Evolutionary Algorithm

2.2

1031

Dynamical Evolutionary Algorithm

DEA ﬁrstly is designed as a single objective optimizer. In DEA the individuals
is considered as particles and the population as a dynamical system or particle
system. So every particle is assigned a momentum and an activity, these two
quantities are incorporated to control selections and to drive particles moving
and searching all the time and everywhere. In DEA, the iterative step t in DEA
like the generation in the traditional GA is called time t.
The momentum P of particle xi at time t is deﬁned as
P (t, xi ) = f (t, xi ) − f (t − 1, xi )
Here f is the value of objective function.
The activity of a(t, xi ) particle xi is deﬁned as the count that xi is selected
as the parent individual at time t.
We can generally take a weight co-eﬃcient λ ∈ (0, 1) to indicate that which
one of the two terms is more signiﬁcant than the other in selection. Namely
t

|p(k, xi )| + (1 − λ)a(k, xi )

slct(t, xi ) = λ
k=0

Based on slct(t, xi ), not ﬁtness, the individuals would be selected to evolve.
2.3

Weighted Sum Method

When searching in the solution space of MOPs, the individuals have to evolving toward multiple directions in common MOEAs. Actually, Evolutionary Algorithms are more eﬃcient while evolving toward only one direction. So the
Weighted Sum Method is employed to seek the extreme points, that is, the
whole population is divided into several subpopulations, every subpopulation
would evolving by a weight setting. Considering that trade-oﬀ solutions also are
very important, an additional subpopulation is suggested to optimize the mean
of all objectives.
While the objective functions are multi-modal, the method should cooperate
with other operators to make the subpopulations evolve toward the right extreme
points.

3

Introduction to New Algorithm

In this algorithm, GPS is an archiving algorithm, DEA is used as single-objective
optimizer, WSM would guide the evolution of population. As to trade-oﬀ solutions, they will be generated by the optimizer during the optimization.
The algorithm can be depicted Figure 1:

1032

B. Zheng and T. Hu

Program DGPS;
1 Initialize Populations and archive
2 While not satisfy the stopping criteria do
3 For i=1 to SubPopNumber
4
Use DEA to optimize the corresponding objective
5
Try to insert every oﬀspring into archive by GPS method
6
If successful in insertion or the oﬀspring is better than its parent
7
then replace its parent
8 End For
9 End While
10 Using Cutoﬀ strategy to clear all dominated solutions
11 Output all non-dominated solutions
Fig. 1. Pseudocode of DGPS

Note:
1) The deﬁnition of better
The better function is used to compare two individuals, we note the prototype
as bool Better(Indi1, Indi2).When solving the single objective problems, DEA
uses a simple strategy to compare two individuals. The individual whose ﬁtness
is smaller is the better(for minimization). But when solving the multi-objective
problems, we modify the strategy as follows:
If the Indi1’s corresponding objective value is smaller
then return true
Else if the Indi1’s corresponding objective value equals to Indi2’s
then if the average of all objectives of Indi1 is smaller
then return true
else return false
2) Cutoﬀ Strategy
In GPS, some dominated solutions are not eliminated until the iteration stops.
So after the iteration, these dominated solutions should be removed by this
strategy.

4

Experimental Results and Analysis

We test this algorithm with some famous test-bed functions, the results are
very satisfactory. Because HPMOEA employs DEA, but with diﬀerent archiving
algorithm and search strategy, we compare our results with it.
For a fair comparison, we set the same generation number and size of population (the same evaluation times) to both the algorithms.
Problem 1: KUR
min f1 (X) =

n−1
i=1
n
i=1

−10 exp(−0.2 x2i + x2i+1 )

|xi |0.8 + 5 sin x3i
min f2 (X) =
xi ∈ [−5 , 5] , i = 1, · · · , 3

(1)

A Novel Multi-objective Evolutionary Algorithm

1033

2

0

−2

−4

−6

−8

−10

−12
−20

−19

−18

−17

−16

−15

−14

Fig. 2. APF of KUR by DGPS

Fig. 3. APF of KUR by HPMOEA

The Pareto front of KUR is non-convex and disconnected. Because of the discrete nature of the Pareto-optimal regions, optimization algorithms may have difﬁculties in ﬁnding Pareto-optimal solutions in all regions. But both the proposed
MOEA and HPMOEA obtain very good results, as shown in Figure 2 and 3.
Problem 2: VNT
M in
M in
M in
s.t. :

f1 (X) = 0.5(x21 + x22 ) + sin(x21 + x22 )
2
2
2 +4)
+ (x1 −x272 +1) + 15
f2 (X) = (3x1 −2x
8
f3 (X) = (x2 +x1 2 +1) − 1.1 exp(−x21 − x22 )
1
2
− 3 ≤ x1 , x2 ≤ 3

(2)

This is a diﬃcult problem with high objective dimensions. But this algorithm
generated a very ﬁne pictures with obvious features, see Figure 4. From Figure 5,
HPMOEA can not obtain the whole Pareto front, and its Pareto front does not
include enough points.

1034

B. Zheng and T. Hu

0.3

0.2

0.1

0

−0.1
17
16.5

10
8

16

6
4

15.5
15

2
0

Fig. 4. POF of VNT by DGPS

Fig. 5. POF of VNT by HPMOEA

From the pictures, we can see that this algorithm can obtain very ﬁne curves in
a single run. Obviously, this will contribute great convenience to decision-maker
for the ﬁnal decision and decease greatly the decision risk of lack of sampling
points. It is remarkable that the sampling points distribute very equally.
In this algorithm, multi-subpopulation strategy make the population evolve
towards multiple directions, so it is easier to obtain the extreme points. Though
this algorithm get more sampling points, the consumed time still is less than
some other MOEAs. We compare this algorithm with HPMOEA and other two
famous MOEAs - SPEA2 and NSGA-II which base on PISA[15].
All the algorithms are carried out on a machine with one Intel PIV 2.4G CPU.
In the experiments, every algorithm runs 30 times, the sizes of populations are
set to 100, the numbers of generation of DGPS and HPMOEA are set to 2000
and the numberd of SPEA2 and NSGA-II are set to 200. The results are listed
in Table 1.

A Novel Multi-objective Evolutionary Algorithm

1035

Table 1. Comparison of Consumed Time
Problem

ZDT3

SRN
KUR

Algorithm
DGPS
HPMOEA
SPEA2
NSGAII
DGPS
HPMOEA
DGPS
HPMOEA

Avg. Num.
points
1420.0
100
100
100
2026.1
100
1381.4
100

of Average time
0.837s
21.840s
73.006s
72.646s
0.282s
29.815s
1.440s
19.761s

From Table 1 we can see that this algorithm runs much faster than HPMOEA
and can get more sampling points, so we can conclude that this algorithm is
better than HPMOEA. Literature[10] points out that HPMOEA have a better
convergent rate than SPEA2, NSGAII, so we can conclude that this algorithm
can run faster than SPEA2,NSGAII and get ﬁner approximate Pareto Front.

5

Conclusions and Future Work

In this paper, we proposed a new MOEA. The experimental results show that this
algorithm can obtain ﬁner approximate Pareto front with less time. In contrast
with the some previous MOEAs, which may spend hours to perform one run
for only hundreds of non-dominated solutions, this algorithm can obtain tens of
thousands of solutions in a single run under a bearable time limitation.
In this paper, The theoretical analyses such as convergence property, convergence ratio are not introduced here, we will leave it as the future work.
Acknowledgement. The authors gratefully acknowledge the ﬁnancial support
of the National Natural Science Foundation of China under Grant No.60473014
and No.60603008.

References
1. Schaﬀer, J.: Some Experiments in Machine Learning Using Vector Evaluated Genetic Algorithms. PhD thesis, Vanderbilt University (1984)
2. Schaﬀer, J.: Multiple objective optimization with vector evaluated genetic algorithms. In: Proceedings of the First International Conference on Genetic Algorithms. (1985) 93–100
3. Zitzler, E., Thiele, L.: Multiobjective evolutionary algorithms: a comparative case
study and the strength pareto approach. Evolutionary Computation, IEEE Transactions on 3(4) (1999) 257–271
4. Zitzler, E., Thiele, L., Laumanns, M., Fonseca, C.M., da Fonseca, V.G.: Performance assessment of multiobjective optimizers: an analysis and review. Evolutionary Computation, IEEE Transactions on 7(2) (2003) 117–132

1036

B. Zheng and T. Hu

5. Laumanns, M., Zitzler, E., Thiele, L.: On the eﬀects of archiving, elitism, and
density based selection in evolutionary multi-objective optimization. Evolutionary
Multi-Criterion Optimization, Proceedings 1993 (2001) 181–196
6. Knowles, J., Corne, D.: Properties of an adaptive archiving algorithm for storing
nondominated vectors. Evolutionary Computation, IEEE Transactions on 7(2)
(2003) 100–116
7. Knowles, J.D., Corne, D.W., Fleischer, M.: Bounded archiving using the lebesgue
measure. In: Evolutionary Computation, 2003. CEC ’03. The 2003 Congress on.
Volume 4. (2003) 2490–2497
8. Corne, D., Knowles, J.: Some multiobjective optimizers are better than others.
In: Evolutionary Computation, 2003. CEC ’03. The 2003 Congress on. Volume 4.
(2003) 2506–2512
9. Li, Y., Zou, X., Kang, L., Michalewicz, Z.: A new dynamical evolutionary algorithm
based on statistical mechanics. Journal of Computer Science and Technology 18(3)
(2003) 361–368
10. Zou, X.: Research On Theory of Dynamical Evolutionary Algorithm and their
Applications. PhD thesis, Wuhan University (2003)
11. Zou, X.F., Kang, L.S.: Fast annealing genetic algorithm for multi-objective optimization problems. International Journal of Computer Mathematics 82(8) (2005)
931–940
12. Zheng, B., Li, Y., Peng, S.: GPS: A geometric comparison-based pareto selection
method. In Kang, L., Cai, Z., Yan, X., eds.: Progress in Intelligence Computation
and Applications, International Symposium on Intelligent Computation and its
Application, ISICA 2005. Volume 1., Wuhan,China (2005) 558– 562
13. Zheng, B.: Researches on Evolutionary Optimization. PhD thesis, Wuhan University (2006)
14. Deb, K., Pratap, A., Agarwal, S., Meyarivan, T.: A fast and elitist multiobjective
genetic algorithm: Nsga-ii. Evolutionary Computation, IEEE Transactions on 6(2)
(2002) 182–197
15. Bleuler, S., Laumanns, M., Thiele, L., Zitzler, E.: Pisa - a platform and programming language independent interface for search algorithms. In Fonseca, C.,
ed.: Evolutionary Multi-Criterion Optimization(EMO 2003), LNCS 2632. (2003)
494–508

