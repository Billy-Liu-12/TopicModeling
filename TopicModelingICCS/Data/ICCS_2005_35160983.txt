Improving Performance of Distributed Haskell in
Mosix Clusters
Lori Collins, Murray Gross, and P.A. Whitlock
Department of Computer and Information Sciences, Brooklyn College,
2900 Bedford Avenue, Brooklyn, NY 11210-2889
magross@its.brooklyn.cuny.edu

Abstract. We present experimental results demonstrating the performance improvement obtained in our distributed Haskell implementation
when we replaced scheduling according to a micromanagement paradigm
with contention-driven scheduling with automatic load balancing. This
performance enhancement has important implications in the area of automatic run-time optimization.

1

Introduction

We modiﬁed version 5.04 of Glasgow distributed Haskell (GdH)[1, 2] to take advantage of the special properties of a cluster of computers managed by the Mosix
distributed operating system[3], which provides optimized task distribution with
automatic load balancing. Standard GdH uses a PVM-based[4] micromanaging
scheduler to lock a single process on each real processor. Experience with conventional parallel programs convinced us that performance would be maximized
by executing multiple computations on each processor. Below we present background and test results for modiﬁcations we made to standard GdH to maximize
performance by using Mosix distribution and load balancing instead of the standard GdH scheduling.

2

Parallel/Distributed Haskell and the Mosix
Environment

While functional languages provide a number of advantages over procedural languages [5], four characteristics are particularly important in the area of parallel
and distributed execution: (1) functions are ﬁrst class values, i.e., passable from
one function to another as ordinary data; (2) a value bound to a variable cannot
change (referential transparency); (3) in Haskell, function evaluation is lazy and
(4) side eﬀects are forbidden except in the rare cases such as I/O where they
cannot be avoided. The ﬁrst-class status of functions, referential transparency
and the prohibition against side eﬀects provide potentially radical reductions
in coordination delay and overhead. Lazy execution means that computation of
values is delayed until they are actually required. This guarantees that once a
V.S. Sunderam et al. (Eds.): ICCS 2005, LNCS 3516, pp. 983–986, 2005.
c Springer-Verlag Berlin Heidelberg 2005

984

L. Collins, M. Gross, and P.A. Whitlock

value is computed, it may be freely used by any process in the computation
without fear of change. Overall computing time is reduced since processes block
on values only while they are currently being computed, a necessary delay. A
value not yet available will be computed by whichever process ﬁrst requires it.
The eﬀect of these speciﬁc language characteristics causes parallel Haskell
programs to serialize on critical paths rather than on the programmer’s conceptual scaﬀolding. Since it is clearly impossible to improve performance beyond
what is obtained on critical paths, optimal performance is obtained without
programmer intervention.
Our distributed calculations are run on a cluster of 15 computers currently
running Debian Linux with OpenMosix patches[3]. Mosix makes a cluster of separate computers run like a symmetric multiprocessor (SMP) by dynamic process
migration[3]. This is ideal in heterogeneous conﬁgurations, since it automatically
enables load balancing on processors by forcing process migrations to achieve
load balancing across available CPU’s. Requests for system services are trapped
by Mosix, executed on the home machine when they cannot be performed on the
worker machines, and the results are transmitted back to the worker machine.
A high-eﬃciency ﬁle system[6] permits processors to access ﬁles on other processors with minimal overhead. Data can be distributed across the cluster and
there are only a few clearly deﬁned conditions that prevent dynamic migration
of tasks from one machine to another.

3

Modification to the GdH Compiler

In the original GdH design, PVM[4] was used for all remote process creation
and communication. Rather than building a virtual machine of virtual processors provided by PVM processes, our modiﬁed version of the run-time system
builds a virtual machine of virtual processors created by forking local processes,
which may be migrated to remote processors by Mosix. The send() and recv()
calls to PVM were replaced by local system calls. We use UDP-based interprocess communication on the loop back interface. After error testing of our new
GdH-BC system we proceeded to test whether the hypothesized eﬃciencies had
actually been achieved[7].

4

Performance Results

The initial tests compared the performance of runs on a single processor of the
ﬀactsm seq program, complied with serial GHC[8], with the program ﬀactsm par,
compiled by our modiﬁed GdH-BC and run on fourteen or ﬁfteen real processors. The tests consisted of twenty-six sets of arguments with execution times
varying from seconds to hours. Each experiment was repeated three times. The
serial and parallel calculations gave the same answers in all cases. The speedup,
S, between the serial and parallel calculation is given by:
S=

Execution time on a single processor, best sequential algorithm
,
Execution time using p processors

(1)

Improving Performance of Distributed Haskell in Mosix Clusters

985

10

9

Parallel speedup

8

7

6

5

4

3

2
0

20

40

60

80

100

120

140

160

180

Serial execution time in minutes

Fig. 1. Observed speedup, as defined by Eq. (1), when the test program was run in
parallel using GdH-BC as compared to the serial execution times. The line is a linear
fit to the data
250

245

Sum of three best times in minutes

240

235

230

225

220

215

210

205
200
15

20

25

30

35

40

45

50

55

60

Number of processes

Fig. 2. Total timings from parallel runs of the test program with a fixed number of
processes. The line is a fit to the data

and is shown in Figure 1. As expected we saw a decrease in execution time
with our parallel calculations in comparison to our serial execution times. Mosix
gathers execution time statistics before distributing a calculation, so quickly
completed calculations are not distributed as extensively as longer calculations.
Our claim that best performance would be achieved by executing multiple
processes on each processor is conﬁrmed by Figure 2. In the ﬁgure, we have
displayed the total time of the three best runs versus the number of processes
requested per run. Once there are more processes than the ﬁfteen real processors the execution time decreases until an equilibrium point is reached and the

986

L. Collins, M. Gross, and P.A. Whitlock

running time begins to increase with the number of processes. We think it safe
to say that the increase in running time when the number of processes increases
beyond a ”balance point” is the result of increases in system overhead associated
with process coordination and load balancing.

5

Conclusions

We conclude that the original design of the GdH run-time system, which precluded more than one PVM session per processor, imposes an unnecessary performance barrier. We ﬁnd that at least under some circumstances, the contention driven supervisory model in our GdH-BC outperforms the micromanaged
scheduling provided by the original PVM-based version of the system. While
further testing is required, we believe that our version of the Haskell system provides signiﬁcant performance advantages over competitors in implementation of
parallel/distributed computation.

Acknowledgments
The cluster of computers was made possible by ONR Grant N00014-96-1-1-1057.
The initial development of the GdH-BC compiler was done by Dino Klein and
Qing Shou. Testing of the compiler was begun by Qing Shou and Kerim Simsek.
One of us, L.C., is supported by NSF Grant number HRD0217542.

References
1. Trinder, P.W., Loidl, H-W., and Pointon R.F.: Parallel and distributed haskells.
Journal of Functional Programming 12 (2002) 469–510.
2. Pointon, R.F., Trinder, P.W., and Loidl H-W.: The Design and Implementation
of GdH. In: Markus Mohnen, Pieter W. M. Koopman (Eds.) Implementation of
Functional Languages, 12th International Workshop, Lecture Notes in Computer
Science 2011 (2001) 53–70
3. Barak, A., La’adan, O., and Shiloh, A.: Scalable cluster computing with MOSIX for
LINUX. Proc. 5th Annual Linux Expo, Atlanta, GA (1999) 95–100
4. Sunderam, V.: PVM: A Framework for Parallel Distributed Computing. Concurrency: Practice & Experience 2 No. 4 (1990) 315–339
5. Hughes, J.: Why functional programming matters. Computer Journal 32 (1990)
1–23
6. Amar, L., Barak, A., and Shiloh A.: The MOSIX Parallel I/O System for Scalable
I/O Performance. Proc. 14-th IASTED Int. Conference on Parallel and Distributed
Computing and Systems, Cambridge, MA (2002) 495–500
˜
7. http://146.245.249.159/kerim/timing.pdf
8. Peyton Jones, S.L, Hall, C., Hammond, K., and Partain, W.: The Glasgow Haskell
compiler: a technical overview. UK Joint Framework for Information Technology
(JFIT) Technical Conference, Keele, (1993) 249–257

