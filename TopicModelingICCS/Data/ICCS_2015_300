Procedia Computer Science
Volume 51, 2015, Pages 160–169
ICCS 2015 International Conference On Computational Science

Point Distribution Tensor Computation on Heterogeneous
Systems
Ivan Grasso14 , Marcel Ritter234 , Biagio Cosenza1 , Werner Benger345 ,
G¨
unter Hofstetter2 , and Thomas Fahringer1
1

Institute for Computer Science
Institute for Basic Sciences in Engineering Science
3
Institute for Astro- and Particle Physics
University of Innsbruck, Austria
4
AHM Software - Airborne Hydromapping
Center for Computation & Technology at Louisiana State University
2

5

Abstract
Big data in observational and computational sciences impose increasing challenges on data
analysis. In particular, data from light detection and ranging (LIDAR) measurements are
questioning conventional methods of CPU-based algorithms due to their sheer size and complexity as needed for decent accuracy. These data describing terrains are natively given as big
point clouds consisting of millions of independent coordinate locations from which meaningful
geometrical information content needs to be extracted. The method of computing the point
distribution tensor is a very promising approach, yielding good results to classify domains in a
point cloud according to local neighborhood information. However, an existing KD-Tree parallel approach, provided by the VISH visualization framework, may very well take several days to
deliver meaningful results on a real-world dataset. Here we present an optimized version based
on uniform grids implemented in OpenCL that is able to deliver results of equal accuracy up
to 24 times faster on the same hardware. The OpenCL version is also able to beneﬁt from a
heterogeneous environment and we analyzed and compared the performance on various CPU,
GPU and accelerator hardware platforms. Finally, aware of the heterogeneous computing trend,
we propose two low-complexity dynamic heuristics for the scheduling of independent dataset
fragments in multi-device heterogenous systems.
Keywords:

1

Introduction

Point datasets are present in many scientiﬁc domains. Smoothed particle hydrodynamics methods in astrophysics [4], echo sounding in engineering, 3D surface reconstruction [16] and urban
reconstruction [20] in graphics are typical examples where data generated by numerical simulations or observations are processed as point primitives. Today’s range-sensing devices are
160

Selection and peer-review under responsibility of the Scientiﬁc Programme Committee of ICCS 2015
c The Authors. Published by Elsevier B.V.

doi:10.1016/j.procs.2015.05.217

Point Distribution Tensor Computation on Heterogeneous Systems

I. Grasso

capable of producing highly detailed point datasets containing hundreds of millions of sample points. Light detection and ranging (LIDAR) technology, in particular, allows collecting
millions of data points e.g. from airborne scanners in order to produce high-resolution digital
elevation maps. However, depending on the application, large point datasets may require a
prohibitively high computational power for processing.
This paper is motivated by the LIDAR surveying application of shallow waters [7] where,
starting from a point dataset, a second order tensor ﬁeld is computed and used as a basis for
several other algorithms such as point classiﬁcation and geometry reconstruction [22]. A ﬁrst
CPU-based parallel implementation of such a point distribution tensor was implemented in the
VISH visualization shell [5]. VISH is a productive framework that provides functionalities for
both eﬃcient data processing and visualization of big data. However, as surveyed datasets grow
from several thousand of points to many millions of points, the tensor computation becomes a
bottleneck for data processing.
In this paper we focus on exploiting the computational power of emerging heterogeneous
computing systems in order to improve the tensor computation of massive datasets of millions
of points. Our study makes the following contributions: First, we implemented a new tensor
computation code in OpenCL using a uniform grid space partitioning approach, and evaluated
its performance against the current KD-Tree implementation available in VISH. Second, we
investigated the performance of the implemented code on 8 diﬀerent devices, comprising four
GPUs, three CPUs and one accelerator, from desktop and server domains. Finally, we proposed
two low-complexity dynamic heuristics for the scheduling of independent dataset fragments
and compared them with three static scheduling heuristics in two multi-device heterogeneous
systems.

2

OpenCL Programming Model

OpenCL [14] is an open industry standard for programming heterogeneous systems composed
of devices with diﬀerent capabilities such as CPUs, GPUs and other accelerators. The platform
model consists of a host connected to one or more compute devices. Each device logically consists of one or more compute units (CUs) which are further divided into processing elements
(PEs). Within a program, the computation is expressed through the use of special functions,
called kernels, that are, for portability reasons, compiled at runtime by an OpenCL driver.
A kernel represents a data-parallel task and describes the computation performed by a single
thread, which is called work-item in OpenCL. During the program execution, based on an index space (N-Dimensional Range), a certain number of work-items are generated and executed
in parallel. The index space can also be subdivided into workgroups, each of them consisting of many work-items. The exchange of data between the host and the compute devices is
implemented through memory buﬀers, which are passed as arguments to the kernel before its
execution. In the past few years, OpenCL has emerged as the de facto standard for heterogeneous computing, with the support of many vendors such as Adapteva, Altera, AMD, ARM,
Intel, Imagination Technologies, NVIDIA, Qualcomm, Vivante and Xilinx.

3

Tensor Computation

For a set of N points {Pi : i = 1, ..., N } the point distribution tensor S at the point Pi is deﬁned
as:
N
1
S(Pi ) =
ω(|tik |)(tik ⊗ tτik ),
(1)
N
k=1

161

Point Distribution Tensor Computation on Heterogeneous Systems

I. Grasso

Figure 1: Input point distribution (left) and output tensor (right) of the river Rhein dataset. For the
input points, the height (z-axis) drives a colormap (left) while the output tensor is used for coloring
(planarity) and surface shading in VISH (right).

whereby ω(x) = θ(r − x) is a threshold function dependent on a radius r [22], tik = Pi − Pk ,
τ
is the transpose and ⊗ denotes the tensor product. A graphical result of the computation is
depicted in Figure 1.
The naive approach for the tensor computation is therefore to test every point with all the
others, leading to a quadratic algorithmic complexity. In real models composed of millions of
points this approach is not applicable due to the inherent performance problem. To mitigate
this problem spatial partitioning methods have been investigated [2, 25, 26].
KD-Tree Implementation. The tensor ﬁeld computation algorithm, currently implemented
in VISH, makes use of a KD-Tree data structure to ﬁnd the neighbors of a certain point. After
the tree building phase, in which the points of the dataset are inserted into the KD-Tree, the
computation of the tensor distribution is executed for each point with a series of range queries
dependent on a given search radius (threshold function). The computational loop over the points
was parallelized using OpenMP with dynamic scheduling and packets of 10000 loop iterations.
The KD-Tree code was integrated in a computational VISH module, and implemented via C++
templates and STL containers.
Uniform Grid OpenCL Implementation. A uniform grid space partitioning approach
involves a spatial partitioning of the model system into equally-sized boxes (cells) containing
diﬀerent numbers of points. It is important to ensure that the grid box size is not smaller than
the radius size, as this would force the algorithm to check many surrounding grid boxes. On the
other hand, if the grid box is larger than the radius, each box would contain numerous points
and the process of locating neighbors would once again be checking many points outside the
radius area. In our case, the uniform grid approach is eﬀective because the radius is an input
parameter of the program and therefore we are able to tune the grid box size accordingly.
We implemented the tensor computation application in OpenCL using a uniform grid space
partitioning approach. We used a grid with a cell size of two times the radius, which implies
that each point can only interact with points in the neighboring cells (27 in a 3D space). The
complete program, described in Algorithm 1, is composed of three phases: initialization, computation and ﬁnalization. During the initialization phase, the OpenCL devices are initialized,
the OpenCL kernels are compiled and the metadata of the dataset is loaded. The metadata
contains information about the number of fragments present in the dataset, the number of
points for each fragment, plus other additional information useful for the graphical visualization. The dataset consists of independent fragments of spatially ordered points to facilitate the
162

Point Distribution Tensor Computation on Heterogeneous Systems

I. Grasso

Algorithm 1 The OpenCL tensor computation algorithm
1:
2:
3:
4:
5:
6:
7:
8:
9:
10:
11:
12:
13:
14:
15:

devices initialization()
metadata ← load dataset metadata()
for all f ragments in dataset do
pts ar ← load points data(f ragment)
write points to device(pts ar)
create unif orm grid(pts ar, radius){
hash ar ← compute hash values(pts ar)
index ar ← sort points indices(hash ar)
begin end ar ← compute interval(hash ar)
}
compute tensor(pts ar, index ar, begin end ar)
tsr ar ← read tensor f rom device()
write tensor to disk(tsr ar)
end for
devices f inalization()

Initialization Phase
Computation Phase

Finalization Phase

data manipulation and visualization. Each fragment contains a small percentage of replicated
data necessary for the computation of the tensor algorithm at points close to the border of
the fragment. Once the initialization phase is completed the system is ready to schedule the
fragments on the available devices and the computation phase will start.
For each fragment the point’s coordinates will be loaded in main memory and transferred to
the device memory where the computation will take place. On the device, the uniform grid will
be created and used during the tensor computation in the search for the neighboring points.
Once the computation is done, the computed tensor data is transferred back to the host’s main
memory and ﬁnally saved to the disk. The ﬁnalization phase releases all the devices and the
used memory.
The steps necessary for the creation of the uniform grid are described in Algorithm 1 (lines
6-10). The algorithm consists of multiple OpenCL kernels. The ﬁrst kernel (line 7) calculates
a hash value for each point based on its cell ID and stores them in an array in device main
memory (hash ar ). The array is then sorted based on the cell IDs while updating at the same
time the order of the point IDs. Sorting is performed using a bitonic algorithm. The result of
this computation is an array of point IDs sorted by cell (index ar ). The last kernel (line 9) is
then executed to ﬁnd the begin and the end position of any given cell. The kernel generates
an OpenCL work-item for each point and compares the cell ID of the current point with the
cell ID of the previous one in the hash ar array. If the two indices are diﬀerent, the current
work-item ID is used as start index of the current cell and the begin end ar array is updated
using a scattered write operation. During the execution of the compute tensor kernel (line 11),
using the begin end ar and index ar arrays, we calculate the neighbor cells for each point in
the fragment and for each point present in the cells we compute the diﬀerence to the current
point in each dimension (x, y, z). If the length of the diﬀerence vector is less than the radius,
the tensor array and the points counter are updated. Finally, in the last step, each element of
the tensor array is divided by the points counter.

4

Scheduling Independent Fragments

As previously mentioned in Section 3, the tensor computation is applied on single fragments that
compose the complete dataset. The fragments are completely independent of each other and
can be computed in parallel using the available devices present in the system. During program
163

Point Distribution Tensor Computation on Heterogeneous Systems

I. Grasso

execution a scheduler is responsible for the allocation of the fragments among the heterogeneous
devices. The scheduling problem has been extensively investigated and numerous methods have
been reported in the literature [6, 18, 17]. In our program we implement two low-complexity
scheduling heuristics: SimpleH and SimpleHS. SimpleH analyzes the dataset metadata and
sorts the list of fragments based on the number of points contained in each of them. The
algorithm then proceeds by dynamically assigning the fragment with the smallest number of
points to the slowest device and the fragment with the biggest number of points to the fastest
device. Following this pattern, the scheduler continues to dynamically assign fragments until
all of them are processed. SimpleHS follows a similar pattern. A fragment is assigned to the
slowest device, if the predicted execution time of the fragment on that device is lower than the
predicted execution time of all the remaining fragments on the fastest device. The execution
time for each fragment is predicted with a quadratic regression model using the number of points
of the fragment. During the program execution, information regarding number of points per
fragment and execution times are stored. These information will then be used to build a more
accurate model whenever the slowest device is ready to compute a new fragment. Although
for simplicity the heuristic algorithms are described taking into consideration only two devices,
they can be applied to heterogeneous systems composed of a single slow device (CPU) and
multiple equally fast devices (e.g. GPUs). In Section 6 we evaluate and compare SimpleH and
SimpleHS with three heuristics which are widely used to address the problem of scheduling
independent tasks in heterogeneous computing systems: Min-Min [13, 6], Max-Min [13, 6], and
Suﬀerage [19]. Because these are static heuristics, it is assumed that an accurate estimation
of the expected execution time for each fragment on each device is known prior to execution
and contained within an ETC (expected time to compute) matrix. The Min-Min heuristic
proceeds by assigning a previously unassigned fragment to a device in every iteration. The
assignment is decided based on a two-step procedure. In the ﬁrst step, the algorithm computes
the minimum completion time (MCT) of each unassigned fragment over the devices in order
to ﬁnd the best device which can complete the processing of that fragment at earliest time.
This decision is made taking into account the current loads of the devices and the execution
time of the fragment on each device. In the second step, the algorithm selects the fragment
with the minimum MCT among all unassigned fragments and assigns the fragment to its best
device found in the ﬁrst step. The Max-Min heuristic diﬀers from the Min-Min in the fragment
selection policy adopted in the second step of the fragment-to-device assignment procedure.
Unlike Min-Min, which selects the fragment with the minimum MCT, Max-Min selects the
fragment with the maximum MCT and then assigns it to the best device found in the ﬁrst
step. Suﬀerage is also similar to Min-Min but adopts a diﬀerent fragment selection policy. In
the ﬁrst step of the process, the algorithm computes the second MCT value in addition to the
MCT value for each fragment. In the second step, the suﬀerage value, which is deﬁned as the
diﬀerence between the MCT and the second MCT values of a fragment, is taken into account.
Suﬀerage selects the fragment with the largest suﬀerage and assigns it to the best device found
in the ﬁrst step.

5

Experimental Environment

In order to evaluate the performance of the KD-Tree and OpenCL implementations presented
in Section 3, we use a dataset of 58 million points, generated using a combination of LIDAR
and echo sounding data captured at the river Rhein in Rheinfelden [7]. The dataset is stored in
the HDF5 [23] format, based on the scientiﬁc data format F5 [21, 3], to be easily manipulated
with the VISH infrastructure. The dataset is composed of 65 fragments that contain between
one thousand and 3.5 million points each.
164

Point Distribution Tensor Computation on Heterogeneous Systems
Device
OpenCL vendor
OpenCL version
Operating System
Host Connection
Type
Class
Compute Units
Max Workgroup
Clock (MHz)
Images
Cache
Cache Line
Cache Size (KB)
Global Mem (MB)
Constant (KB)
Local Type
Local (KB)

S9000
AMD
SDK v2.9
CentOS6.5
PCIe 3.0
GPU
server
28
256
900
Yes
R/W
64
16
3072
64
Scratch
32

K20m
NVIDIA
CUDA 6.5
CentOS6.5
PCIe 3.0
GPU
server
13
1024
705
Yes
R/W
128
208
4799
64
Scratch
48

Phi7120
Intel
SDK 2014
CentOS6.5
PCIe 2.0
ACL
server
240
8192
1333
No
R/W
64
256
11634
128
Global
32

2x E5-2690v2
Intel
SDK 2014
CentOS6.5
CPU
server
40
8192
3000
Yes
R/W
64
256
129006
128
Global
32

2x Opt.6168
AMD
SDK v2.9
CentOS6.5
CPU
server
24
1024
1900
Yes
R/W
64
64
64421
64
Global
32

I. Grasso
Radeon5870
AMD
SDK v2.9
CentOS5.9
PCIe 2.0
GPU
consumer
20
256
850
Yes
None
1024
64
Scratch
32

GTX480
NVIDIA
CUDA 6.5
CentOS5.9
PCIe 2.0
GPU
consumer
15
1024
1401
Yes
R/W
128
240
1536
64
Scratch
48

i7-2600K
Intel
SDK 2014
Mint16
CPU
consumer
8
8192
3400
Yes
R/W
64
256
7965
128
Global
32

Table 1: Benchmarked OpenCL devices

To represent the broad spectrum of OpenCL-capable hardware we selected eight devices,
comprising four GPUs, three CPUs, and one accelerator. Their device characteristics as reported by OpenCL are summarized in Table 1. To exploit the computational capabilities of
heterogeneous machines, we evaluated diﬀerent scheduling heuristics. The experiments were
performed on two diﬀerent heterogeneous target architectures composed of three OpenCL devices: two GPUs and one CPU. The ﬁrst platform, mc1, consists of an Intel i7-2600K CPU and
two NVIDIA GTX 480, while the second, mc2, holds two Intel Xeon E5-2690 v2 CPUs (reported
as a single OpenCL device) and two AMD Fire Pro S9000 GPUs. For the static scheduling
heuristics we utilized, as estimation time for each fragment (ETC matrix), the actual time that
the fragment will take to be computed on the diﬀerent devices. Diﬀerently, for the computation
of the coeﬃcients in the SimpleHS heuristic, we used the multi-parameter ﬁtting present in the
GNU Scientiﬁc Library.
All the benchmarked programs were compiled with GCC version 4.8.1 with the -O3 optimization ﬂag. In each diﬀerent device, the OpenCL kernels were compiled by the respective vendor
compilers at runtime during the program initialization. All the experiments were conducted
on the previously described dataset. The measurements were collected for the computational
phase of the program, excluding the initialization and ﬁnalization phases. We repeated each
experiment 10 times and we computed the mean value and the standard deviation of the measured performance. In all the presented experiments, the standard deviation is negligible, thus
we do not report it.

6

Performance Analysis

KD-Tree and OpenCL Implementations. To compare the performance of the KD-Tree
version and our OpenCL implementation, we executed the tensor computation on the input
dataset on the same multi-core CPU (Intel i7-2600K). Both implementations are parallel: the
KD-Tree version uses OpenMP to parallelize the loop over all points, while the OpenCL approach is inherently parallel. The building phase of the tree in the KD-Tree implementation
is sequential, however, it represents a very small part of the overall run time. The OpenCL
version of the program experiences a signiﬁcant speedup (24×) over the currently implemented
VISH KD-Tree version, reducing the execution time from 1 hour to 150 seconds. The performance improvement comes from diﬀerent reasons. First, grid data structures are more suited
for range queries (all the particles around a point in a given radius) while KD-Tree structures
are more suited for k-nearest neighbors queries (ﬁrst N-points close to a given point). Second,
vectorization is rather hard in KD-Tree codes where many data-dependent branches are present.
In contrast, the uniform grid OpenCL code can be more easily autovectorized by compilers.
165

Point Distribution Tensor Computation on Heterogeneous Systems

Intel i7-2600K

Intel i7-2600K

AMD Opteron 6168

AMD Opteron 6168

Intel Xeon E5-2690 v2

Intel Xeon E5-2690 v2

Intel Xeon Phi 7120

Intel Xeon Phi 7120

AMD Radeon 5870

AMD Radeon 5870

Nvidia GTX 480

Nvidia GTX 480

Nvidia K20m

Nvidia K20m

AMD Fire Pro S9000

AMD Fire Pro S9000
0

10

Write

20

30

40

Build

50

60

70

Compute

80

90

100

Read

0

I. Grasso

2

5

7

10

12

14

17

19

22

24

OpenCL version

(a) Normalized execution time spent in the diﬀerent (b) Speedup of the diﬀerent devices over the Intel i7parts of the OpenCL tensor computation algorithm
2600K

Figure 2: OpenCL Performance Analysis

Third, we applied a few code optimizations that improve the performance of the OpenCL code.
However, the optimizations only partially aﬀect the speedup over the KD-Tree version, which
remains signiﬁcant even in their absence (12.9×).

Heterogeneous Devices. Since OpenCL supports heterogeneous devices, we analyzed the
performance of our OpenCL code on a set of heterogeneous architectures described in Table 1.
Figure 2a depicts the percentage of execution time spent in the diﬀerent phases of the
OpenCL tensor computation described in Algorithm 1. The blue color represents the transfer of
the fragment points to the device (line 5), the green color represents the time spent building the
uniform grid structure (line 6-10), the yellow color indicates the time spent in the computation
(line 11), while the red color identiﬁes the transfer of the tensors to the host device (line 12).
In all the tested hardware the movement of data does not represent an important part of the
execution time. Write and Read functions are always under 5% of the total time. The only
exception is the AMD Fire Pro S9000 where the data transfers represent 9.0% and 11.6% of
the execution time, respectively. This is mainly due to the small amount of time spent in the
tensor computation thanks to the strong computational capabilities of the device.
In Figure 2b we present the performance comparison of the heterogeneous architectures.
The speedup of the CPUs respects the characteristics of the hardware. The AMD Opteron,
with a higher number of compute units but a lower clock rate, experiences a 1.6× speedup over
the Intel i7 while the Xeon, with 40 compute units and a similar clock rate, reaches a 4.5×
speedup. All the GPUs show signiﬁcant improvements in performance compared to the Intel
i7. The desktop GPUs AMD Radeon 5870 and NVIDIA GTX 480 reach a speedup of 4.7× and
12.0×, respectively. The server GPUs NVIDIA K20m and AMD Fire Pro S9000, designed for
the HPC market, show a speedup of 14.5× and 23.8×, respectively. It is worth underlining that
although the NVIDIA K20 oﬀers higher theoretical peak performance, in our test the AMD Fire
Pro S9000 is around 1.5 times faster. The only accelerator present in our test is the Intel Xeon
Phi. Although its peak performance is comparable with the tested server GPUs, it reaches only
a speedup of 4.4× compared to the Intel i7. The diﬀerence in performance between the GPUs
and the Xeon Phi is diﬃcult to investigate as it derives from the diﬀerences in the architecture
and from the diﬀerent maturity of the OpenCL toolchains.
In conclusion the results show that the problem is well-suited for massively parallel GPU
architectures, reducing the processing time of the complete input dataset to 6.3 seconds in case
of the AMD Fire Pro S9000.
166

Point Distribution Tensor Computation on Heterogeneous Systems

Server Platform

Consumer Platform

Device

I. Grasso

Scheduling Heuristics
Suﬀerage

Min-Min

Max-Min

SimpleH

SimpleHS

mc1-CPU1

5976.34 [15]

0.00 [0]

5980.55 [7]

6124.42 [21]

4807.08 [19]

mc1-GPU1

5971.16 [25]

5993.26 [32]

5988.02 [29]

5962.76 [23]

6014.41 [24]
6049.29 [22]

mc1-GPU2

5974.84 [25]

6502.75 [33]

5988.23 [29]

5984.76 [21]

Ex. time (ms)

5976.34

6502.75

5988.23

6124.42

6049.29

Norm. to Suﬀ.

100.00%

91.90%

99.80%

97,58%

98.79%

Suﬀerage

Min-Min

Max-Min

SimpleH

SimpleHS

mc2-CPU1

2708.42 [21]

1341.58 [7]

2711.67 [12]

2758.30 [28]

2758.30 [28]

mc2-GPU1

2706.41 [22]

2986.75 [29]

2710.42 [26]

2797.53 [20]

2797.53 [20]

mc2-GPU2

2709.77 [22]

2766.57 [29]

2712.34 [27]

2838.69 [17]

2838.69 [17]

Ex. time (ms)

2709.77

2986.75

2712.34

2838.69

2838.69

Norm. to Suﬀ.

100.00%

90.73%

99.91%

95.45%

95.45%

Table 2: Performance of the diﬀerent scheduling heuristics in two heterogeneous systems

Fragments Scheduling. As previously described in Section 4, we conducted a set of experiments with scheduling heuristics in two heterogeneous machines. The objective of our scheduler
is to ﬁnd a fragment-to-device assignment that minimizes the total execution time (makespan).
Table 2 shows, for each device in the two systems, the time spent to execute the number of
assigned fragments (in square brackets) for the particular scheduling policy. The table also
presents for each heuristic the makespan and the normalized result to the Suﬀerage heuristic.
In both systems Suﬀerage reaches an almost perfect load balancing between the three available
devices, fully utilizing the entire machines.
In both systems the static scheduling heuristics obtain similar results, with Max-Min that
reaches almost the same performance of Suﬀerage, while Min-Min shows 91.90% and 90.73%
of the performance, respectively. These results are justiﬁed by the structure of the dataset.
Usually, datasets collected with LIDAR technology contain few fragments with a big number of
points and many small fragments with fewer points. Due to the fragment selection policy, Sufferage and Max-Min perform the assignment of the large fragments in early iterations resulting
in a better load balancing between the devices. Diﬀerently, Min-Min favors the assignment of
fragments with lower cost in early iterations, not reaching the same performance in terms of
makespan. It is noteworthy that the three static scheduling heuristics use a perfectly correct
estimated execution time for the fragments (ETC matrix) that will not be available at scheduling time. The resulting performance of the heuristics is therefore only useful as a comparison
parameter for our low-complexity heuristics SimpleH and SimpleHS, which are only based on
information available at scheduling time. SimpleH, based on the assumption that the GPUs
devices are always faster than the CPU, dynamically assigns the fragments with more points
to the GPUs and the one with fewer points to the CPU. This simple mechanism facilitates the
devices load balancing by avoiding assigning large fragments to slow devices. Although SimpleH
is capable of reaching good performance, it also shows its weakness with our input dataset. The
heuristic does not take into account the number of remaining fragments to assign and, when few
are left, continues to distribute them to the CPU. This behavior can lead to load imbalance if
the GPUs have to wait for the CPU that received one of the last fragments. This issue is solved
with the SimpleHS heuristic, previously described in Section 4. SimpleHS tries to predict the
approximate execution time of a new fragment based on the execution time of the previous
ones. Although at the beginning the prediction error is high, it rapidly decreases during the
167

Point Distribution Tensor Computation on Heterogeneous Systems

I. Grasso

scheduling of fragments. It is noteworthy that the overhead introduced by the prediction model
is negligible and does not impact the performance of the scheduler. In our tests, SimpleHS
is able to correctly predict when to stop the assignment of fragments to the CPU, obtaining
a better load balancing between the devices. As depicted in Table 2, SimpleHS, scheduling
less fragments to the CPU, always achieves better or equal performance compared to SimpleH,
reaching 98.79% and 95.45% of the Suﬀerage performance in the two systems.
These results validate the success of the proposed heuristics which, using only information
available at scheduling time, show comparable performance to more sophisticated methods
which require an accurate estimation of the expected execution times.

7

Related Work

The study of the interaction of millions of points, present in modern datasets, requires scalable systems capable of supporting the large computational demands. In order to actually
improve the scalability of such systems many spatial partitioning methods were proposed and
investigated [2, 25, 26]. Some of these approaches are suitable for simulations which frequently
have high density in one or several spatial locations and some perform best with uniformly
distributed points. In recent years, among such methods, uniform grid data structures have
received great attention from the research community. Erra et al.[8], leveraging the GPU processing power, implemented an eﬃcient framework which permits to simulate the collective
motion of high-density individual groups. Aaby et al. [1] presented the parallelization of agentbased model simulations (ABMS) with millions of agents on multiple GPUs and multi-core
processors. Vigueras et al. [24] proposed diﬀerent parallelization strategies for the collision
check procedure that takes place in agent-based simulations. Green [11] described how to implement a simple particle system in CUDA using a uniform grid data structure. Husselmann
et al. [12] presented single- and multiple-GPU solutions for grid-boxing in multi-spatial-agent
simulations. While the uniform grid approach of these works is similar to ours, they are restricted by the language of choice to some speciﬁc hardware. Diﬀerently, using OpenCL, our
work is not limited to a single platform and can be executed in multiple heterogeneous devices.
This advantage allows us to compare diﬀerent platforms and fully exploit the computational
performance of heterogeneous systems as shown in other recent work [15].

8

Conclusions

This paper proposes an OpenCL implementation of the second order tensor ﬁeld computation
of massive point datasets. Compared with an existing KD-Tree parallel approach which uses
OpenMP, our approach is 24× faster on an Intel i7-2600K. Since OpenCL supports heterogeneous devices, we investigated the performance of our implementation on a set of heterogeneous
architectures, showing a remarkable reduction of the execution time. Furthermore, aware of the
heterogeneous computing trend, we investigated diﬀerent scheduling policies on two heterogeneous machines. The obtained results validate the success of the proposed SimpleHS heuristic,
which shows comparable performance to more complex static heuristics, only using information available at scheduling time. In the future, we plan to extend our work to distributed
environment using the libWater library [10, 9].

9

Acknowledgment

This research has been supported by the FWF Austrian Science Fund as part of project I 1523
“Energy-Aware Autotuning for Scientiﬁc Applications” and by the FWF Doctoral School CIM
Computational Interdisciplinary Modelling under contract W01227.
168

Point Distribution Tensor Computation on Heterogeneous Systems

I. Grasso

References
[1] Brandon G. Aaby, Kalyan S. Perumalla, and Sudip K. Seal. Eﬃcient simulation of agent-based
models on multi-gpu and multi-core clusters. In SIMUTools, pages 29:1–29:10, 2010.
[2] J. Barnes and P. Hut. A hierarchical O(N log N) force-calculation algorithm. Nature, 324, 1986.
[3] W. Benger. Visualization of General Relativistic Tensor Fields via a Fiber Bundle Data Model.
PhD thesis, FU Berlin, 2004.
[4] W. Benger, M. Haider, J. Stoeckl, B. Cosenza, M. Ritter, D. Steinhauser, and H. Hoeller. Visualization Methods for Numerical Astrophysics. 2012.
[5] W. Benger, G. Ritter, and R. Heinzl. The concepts of vish. In 4th High-End Visualization
Workshop, pages 26–39, 2007.
[6] T. D. Braun, H. J. Siegel, N. Beck, L. B¨
ol¨
oni, M. Maheswaran, A. I. Reuther, J. P. Robertson,
M. D. Theys, B. Yao, D. A. Hensgen, and R. F. Freund. A comparison of eleven static heuristics for
mapping a class of independent tasks onto heterogeneous distributed computing systems. JPDC,
61(6):810–837, 2001.
[7] W. Dobler, R. Baran, F. Steinbacher, M. Ritter, M. Niederwieser, W. Benger, and M. Auﬂeger.
Die Zukunft der Gew¨
asservermessung: Die Verkn¨
upfung moderner und klassischer Ans¨
atze: Airborne Hydromapping und F¨
acherecholotvermessung entlang der Rheins bei Rheinfelden. WasserWirtschaft, 9:18–25, 2013.
[8] U. Erra, B. Frola, V. Scarano, and I. Couzin. An eﬃcient gpu implementation for large scale
individual-based simulation of collective behavior. In HIBI, pages 51–58, 2009.
[9] I. Grasso, S. Pellegrini, B. Cosenza, and T. Fahringer. libwater: Heterogeneous distributed computing made easy. ICS, 2013.
[10] I. Grasso, S. Pellegrini, B. Cosenza, and T. Fahringer. A uniform approach for programming
distributed heterogeneous computing systems. JPDC, 74(12):3228–3239, 2014.
[11] S. Green. Particle simulation using cuda. NVIDIA Whitepaper, 2010.
[12] A. V. Husselmann and K. A. Hawick. Spatial data structures, sorting and gpu parallelism for
situated-agent simulation and visualisation. In MSV, pages 14–20, 2012.
[13] O. H. Ibarra and C. E. Kim. Heuristic algorithms for scheduling independent tasks on nonidentical
processors. J. ACM, 24(2):280–289, 1977.
[14] Khronos OpenCL Working Group. The OpenCL 2.0 speciﬁcation, 2013.
[15] K. Koﬂer, I. Grasso, B. Cosenza, and T. Fahringer. An automatic input-sensitive approach for
heterogeneous task partitioning. In ICS, 2013.
[16] M. Levoy, K. Pulli, B. Curless, S. Rusinkiewicz, D. Koller, L. Pereira, M. Ginzton, S. Anderson,
J. Davis, J. Ginsberg, J. Shade, and D. Fulk. The digital michelangelo project: 3d scanning of
large statues. SIGGRAPH, pages 131–144, 2000.
[17] C. Liu and S. Baskiyar. A general distributed scalable grid scheduler for independent tasks. JPDC,
69(3):307–314, 2009.
u, and Z. Shi. A revisit of fast greedy heuristics for mapping a class of independent
[18] P. Luo, K. L¨
tasks onto heterogeneous computing systems. JPDC, 67(6):695–714, 2007.
[19] M. Maheswaran, S. Ali, H. J. Siegel, D. A. Hensgen, and R. F. Freund. Dynamic mapping of a
class of independent tasks onto heterogeneous computing systems. JPDC, 59(2):107–131, 1999.
[20] P. Musialski, P. Wonka, D. G. Aliaga, M. Wimmer, L. van Gool, and W. Purgathofer. A survey
of urban reconstruction. In EUROGRAPHICS 2012 State of the Art Reports, pages 1–28, 2012.
[21] M. Ritter. Introduction to HDF5 and F5. Technical Report CCT-TR-2009-13, Center for Computation and Technology, Louisiana State University, 2009.
[22] M. Ritter and W. Benger. Reconstructing Power Cables From LIDAR Data Using Eigenvector
Streamlines of the Point Distribution Tensor Field. In Journal of WSCG, pages 223–230, 2012.
[23] The HDF Group. HDF5 - Homepage. http://www.hdfgroup.org/HDF5, 2014.
[24] G. Vigueras, J. M. Ordu˜
na, M. Lozano, J. M. Cecilia, and J. M. Garc´ıa. Accelerating collision
detection for large-scale crowd simulation on multi-core and many-core architectures. Int. J. High
Perform. Comput. Appl., 2014.
[25] M. S. Warren and J. K. Salmon. Astrophysical n-body simulations using hierarchical tree data
structures. In ACM/IEEE Conference on Supercomputing, pages 570–576, 1992.
[26] M. S. Warren and J. K. Salmon. A parallel hashed oct-tree n-body algorithm. In ACM/IEEE
Conference on Supercomputing, pages 12–21, 1993.

169

