Available online at www.sciencedirect.com

ScienceDirect
Procedia Computer Science 108C (2017) 2413–2417

International Conference on Computational Science, ICCS 2017, 12-14 June 2017,
Zurich, Switzerland

Effective Learning with 2-Dimensional Active
Effective
Learning
2-Dimensional
Selection
on with
Feature
and InstanceActive
Effective
Learning
with
2-Dimensional
Active
Selection
on
Feature
and
Instance
Selection
on Feature
and1 Instance
1
1
2†

Xiao-Yu Zhang , Shupeng Wang , Lei Zhang , Chao Li , Yang Chen2†,
*
1
Xiao-Yu Zhang11, Shupeng
Wang111,*,and
LeiBinbin
Zhang11Li
, Chao
Li2†
, Yang Chen2†
,
Yong Wang
2†
Xiao-Yu
Zhang
,
Shupeng
Wang
,
Lei
Zhang
,
Chao
Li
, YangChina
Chen2†,
1
1 Chinese Academy of
1 Sciences, Beijing,
Institute of Information
Engineering,
Yong Wang1, and Binbin Li1
1
Yong
Wangzhanglei1,
,Chinese
and Binbin
Lioflibinbin}@iie.ac.cn
{zhangxiaoyu,
wangshupeng,
wangyong,
Institute
of Information
Engineering,
Academy
Sciences, Beijing, China
*

1
Institute
of Information
Engineering,
Chinese wangyong,
Academy
Sciences, Beijing, China
National
Computer
Network
Emergency
Response
Technicaloflibinbin}@iie.ac.cn
Team/Coordination
Center of
{zhangxiaoyu,
wangshupeng,
zhanglei1,
2
China,
Beijing,
China
{zhangxiaoyu,
wangshupeng,
zhanglei1,
wangyong,
libinbin}@iie.ac.cn
National Computer Network Emergency Response Technical Team/Coordination Center of
2
chenyang}@cert.org.cn
National Computer Network {lichao,
Emergency
Response
Technical Team/Coordination Center of
China,
Beijing,
China
China,
Beijing, China
{lichao,
chenyang}@cert.org.cn
{lichao, chenyang}@cert.org.cn
2

Abstract
Active learning is an effective way to minimize the number of queries on ground-truth labels from an
Abstract
oracle
human-computer
interaction.
Traditional
learning
focusononground-truth
the selectionlabels
of unlabeled
Abstract
Active via
learning
is an effective
way to minimize
the active
number
of queries
from an
instances
that
are
most
informative
for
parameter
optimization
under
the
existing
hypothesis.
Active
learning
is
an
effective
way
to
minimize
the
number
of
queries
on
ground-truth
labels
from an
oracle via human-computer interaction. Traditional active learning focus on the selection of However,
unlabeled
the
variation
of
optimal
feature
subset
and
the
emergence
of
new
class
labels
are
neglected.
In
this
paper,
oracle
via
human-computer
interaction.
Traditional
active
learning
focus
on
the
selection
of
unlabeled
instances that are most informative for parameter optimization under the existing hypothesis. However,
instances
that
are
most
informative
for
parameter
optimization
under
the
existing
hypothesis.
However,
we
propose
a
novel
active
learning
method
with
2-dimentional
selection
on
both
feature
and
instance.
the variation of optimal feature subset and the emergence of new class labels are neglected. In this paper,
the
optimal
feature
subset
and thewith
emergence
of new class
labels
neglected.
Inthe
this
paper,
For
feature-dimensional
selection,
discriminative
feature
selection
is implemented
to
extractand
smallest
we variation
propose
aofnovel
active
learning
method
2-dimentional
selection
onare
both
feature
instance.
we propose
a novel
active
learning
method
with feature
2-dimentional
on both
feature
instance.
possible
subset
of features
that can
most accurately
reveal
theselection
underlying
classification
labels.
For
For
feature-dimensional
selection,
discriminative
selection
is
implemented
to
extractand
the
smallest
For feature-dimensional
selection,
discriminative
featurereveal
to extract
smallest
instance-dimensional
selection,
indeterminate
model
isselection
adopted
toimplemented
balance between
modelthe
update
and
possible
subset of features
thatancan
most accurately
the is
underlying
classification
labels.
For
model
upgrade.
the active
selection
in both
dimensions
into a unified
framework
possible
subset Finally,
of features
thatancan
most accurately
reveal
theareunderlying
classification
For
instance-dimensional
selection,
indeterminate
model
is adopted
tointegrated
balance between
model labels.
update and
for
robust
modelFinally,
learning.
instance-dimensional
selection,
an indeterminate
model
is adopted
balance between
model framework
update and
model
upgrade.
the active
selection in both
dimensions
aretointegrated
into a unified
model
upgrade.
Finally,
the active selection in both dimensions are integrated into a unified framework
for2017
robust
model
learning.
©
Theactive
Authors.
Published
by selection,
Elsevier B.V.
Keywords:
learning,
feature
instance selection, matrix optimization, decision theory
for
robust
model
learning.
Peer-review under responsibility of the scientific committee of the International Conference on Computational Science
Keywords: active learning, feature selection, instance selection, matrix optimization, decision theory
Keywords: active learning, feature selection, instance selection, matrix optimization, decision theory

1 Introduction
1 InIntroduction
machine learning applications, human intuition serves as precious and essential instruction for
1 Introduction

model
training [1][2][3].
However, manually
labeling is
labor-intensive
Finding
In machine
learning applications,
human intuition
serves
as preciousand
andtime-consuming.
essential instruction
for
an
effective
way[1][2][3].
to utilizeapplications,
human effort
effectively
is a challenging
problem.
Traditionally,
the instances
In machine
learning
human
intuition
serves
as precious
and
essential instruction
for
model
training
However,
manually
labeling
is
labor-intensive
and
time-consuming.
Finding
model
training
However,
manually
labeling
is labor-intensive
andTraditionally,
time-consuming.
Finding
to
be
labeled
as[1][2][3].
training
are collected
in an
unselective
way. The
learning
algorithm has
choice
an
effective
way
to utilizedata
human
effort
effectively
is a challenging
problem.
theno
instances
an
effective
way
to
utilize
human
effort
effectively
is
a
challenging
problem.
Traditionally,
the
instances
but
to
passively
accept
and
learn
from
the
training
data.
In
contrast,
active
learning,
aiming
at
achieving
to be labeled as training data are collected in an unselective way. The learning algorithm has no choice
to
as accept
trainingand
data
are from
collected
in an unselective
way. The
learning
algorithm
no choice
butbetolabeled
passively
learn
the training
data. In contrast,
active
learning,
aiminghas
at achieving
but *to passively accept and learn from the training data. In contrast, active learning, aiming at achieving
S. Wang is the joint first author with X.-Y. Zhang.
C. Li and Y. Chen are the corresponding authors.
S. Wang is the joint first author with X.-Y. Zhang.
†*
S.
jointare
first
with X.-Y.
Zhang.
C. Wang
Li andisY.the
Chen
theauthor
corresponding
authors.
†
C. Li and Y. Chen are the corresponding authors.
†
*

1877-0509 © 2017 The Authors. Published by Elsevier B.V.
Peer-review under responsibility of the scientific committee of the International Conference on Computational Science
10.1016/j.procs.2017.05.163

2414	

Xiao-Yu Zhang et al. / Procedia Computer Science 108C (2017) 2413–2417

the highest learning performance with the least possible labeling effort, is an ideal solution with the
ability to implement selective sampling on the unlabeled dataset [4][5][6]. The main idea is to actively
select the most informative instances with the greatest potential to improve the model. The strategy of
instance selection plays a critical role in active learning algorithms, which is formulated by the
evaluation of informativeness of unlabeled instances [7]. Another issue with effective learning is that
high dimensional raw data are mathematically and computationally inconvenient to handle due to the
curse of dimensionality. In order to build robust learning models, feature selection is a typical and
critical process, which selects a subset of relevant and informative features meanwhile removes the
irrelevant and redundant ones from the input high-dimensional feature space [8]. Feature selection
improves both effectiveness and efficiency of the learning model in that it can enhance the generalization
capability and speed up the learning process [9].
As we can see, in-depth mining in both instance and feature dimension is necessary for effective
model learning. Not only the informative unlabeled instances should be selected for labeling, but also
the features need to be rearranged according to the labeled dataset with newly labeled instance constantly
incorporated. In this paper, we propose a novel active learning method with 2-dimensional selection.
On one hand, feature-dimensional selection is employed with joint ℓ2,1 -norm minimization based on the
labeled dataset. On the other hand, instance-dimensional selection is implemented via comprehensive
evaluation of instances’ informativeness in both model update and model upgrade.
In the text that follows, we let 𝒙𝒙 ∈ 𝒳𝒳 ⊂ ℝ𝑑𝑑 denote the input feature of an instance, and 𝑦𝑦 ∈ 𝒴𝒴 =
{1, … , 𝑐𝑐} denote the label indicating the class that an instance falls into, where 𝑐𝑐 is the number of classes.
𝑈𝑈 and 𝐿𝐿 stand for the unlabeled and labeled dataset, respectively. Probabilistic model is used for
classification based on the posterior distribution 𝑃𝑃(𝑦𝑦|𝒙𝒙; 𝜃𝜃𝐿𝐿𝑐𝑐 ) of label 𝑦𝑦 conditioned on the input 𝒙𝒙, where
𝜃𝜃𝐿𝐿𝑐𝑐 is 𝑐𝑐-class classification model parameter optimized for the corresponding labeled dataset 𝐿𝐿.

2 Active Learning with 2-Dimensional Selection
2.1 Feature-Dimensional Selection
Feature-dimension selection tries to select a subset of relevant and informative features with optimal
discriminative ability with respect to the classification performance, and meanwhile remove the
irrelevant and redundant ones from the input high-dimensional feature space to ensure efficient learning
in the following stage.
Given the labeled instances {𝒙𝒙1 , … , 𝒙𝒙𝑛𝑛 } ∈ 𝐿𝐿, we stack them in the form of instance matrix 𝐗𝐗 =
[𝒙𝒙1 , … , 𝒙𝒙𝑛𝑛 ] ∈ ℝ𝑑𝑑×𝑛𝑛 , where 𝑑𝑑 and 𝑛𝑛 are the numbers of features and instances respectively. As for the
corresponding labels {𝑦𝑦1 , … , 𝑦𝑦𝑛𝑛 }, we construct the label matrix 𝐘𝐘 = [𝒚𝒚1 , … , 𝒚𝒚𝑛𝑛 ]𝑇𝑇 ∈ ℝ𝑛𝑛×𝑐𝑐 , where 𝑐𝑐 is the
number of classes, and 𝒚𝒚𝑖𝑖 ∈ ℝ𝑐𝑐 is the label vector with the 𝑦𝑦𝑖𝑖 -th element evaluated as 1 and the rest 0.
Based on 𝐗𝐗 and 𝐘𝐘, discriminative feature selection aims at extracting the smallest possible subset of
features that can accurately reveal the underlying classification labels. Denoting the projection matrix
as 𝐀𝐀 ∈ ℝ𝑑𝑑×𝑐𝑐 , the objective is as follows.
𝑛𝑛

min ∑‖𝐀𝐀𝑇𝑇 𝒙𝒙𝑖𝑖 − 𝒚𝒚𝑖𝑖 ‖2 + 𝛼𝛼‖𝐀𝐀‖2,1

(1)

min ℒD (𝐀𝐀) = ‖𝐗𝐗 𝑇𝑇 𝐀𝐀 − 𝐘𝐘‖2,1 + 𝛼𝛼‖𝐀𝐀‖2,1

(2)

𝐀𝐀

𝑖𝑖=1

The first term in (1) is the loss of projection, and the second term is the ℓ2,1 -norm regularization with
parameter 𝛼𝛼 to enforce several rows of 𝐀𝐀 to be all zero. The matrix format of (1) is as follows.
𝐀𝐀

The augmented cost function 𝒥𝒥D (𝐀𝐀, 𝐩𝐩, 𝐪𝐪) can be introduced for the minimization of ℒD (𝐀𝐀) in (2).

	

Xiao-Yu Zhang et al. / Procedia Computer Science 108C (2017) 2413–2417

𝒥𝒥D (𝐀𝐀, 𝐩𝐩, 𝐪𝐪) = Tr[(𝐗𝐗 𝑇𝑇 𝐀𝐀 − 𝐘𝐘)𝑇𝑇 𝐏𝐏(𝐗𝐗 𝑇𝑇 𝐀𝐀 − 𝐘𝐘)] + 𝛼𝛼Tr(𝐀𝐀𝑇𝑇 𝐐𝐐𝐐𝐐)

2415

(3)

where 𝐩𝐩 and 𝐪𝐪 are auxiliary vectors, while 𝐏𝐏 and 𝐐𝐐 are diagonal matrices defined as 𝐏𝐏 = diag(𝐩𝐩) and
𝐐𝐐 = diag(𝐪𝐪) respectively. The operator diag(∙) places a vector on the main diagonal of a square matrix.
The 𝑖𝑖-th diagonal element of 𝐏𝐏 and 𝐐𝐐 are as follows.
{

𝑝𝑝𝑖𝑖𝑖𝑖 =
𝑞𝑞𝑖𝑖𝑖𝑖 =

1
1
=
𝑖𝑖
𝑇𝑇
− 𝐘𝐘) ‖2 2‖𝐀𝐀 𝐱𝐱 𝑖𝑖 − 𝐲𝐲𝑖𝑖 ‖2

2‖(𝐗𝐗 𝑇𝑇 𝐀𝐀

1
2‖𝐚𝐚𝑖𝑖 ‖2

With the vectors 𝐩𝐩 and 𝐪𝐪 given, the optimal projection matrix 𝐀𝐀 can be calculated by
𝐀𝐀∗ = arg min 𝒥𝒥D (𝐀𝐀, 𝐩𝐩, 𝐪𝐪) = (𝐗𝐗𝐗𝐗𝐗𝐗 𝑇𝑇 + 𝛼𝛼𝐐𝐐)−1 𝐗𝐗𝐗𝐗𝐗𝐗
𝐀𝐀

(4)

(5)

The global optimal solution can be achieved iteratively in an alternate minimization way until
convergence. After obtaining the optimal 𝐀𝐀, discriminative features can be selected accordingly. We
first sort the rows of 𝐀𝐀 by the sums along the row dimension of abs(𝐀𝐀) . Feature selection can
subsequently be performed by retaining the 𝑓𝑓 features corresponding to the top 𝑓𝑓 rows of sorted 𝐀𝐀.

2.2 Instance-Dimensional Selection

Given the existing learning model, instance-dimensional selection tries to select a subset of unlabeled
instances to query the human oracle for their labels. The selected instances should be as informative as
possible to minimize labeling effort and meanwhile maximize the improvement in learning performance.
Traditional learning models are usually determinate in that the number of classes 𝑐𝑐 is a preset
constant based on empirical analysis or prior knowledge. Active learning methods in this scenario are
collectively referred to as Determinate Active Learning (DAL), which is not applicable to many
complicated real-world classification tasks where the number of class is not determinate. On one hand,
the parameters need to be refined under the constraint of the existing model; on the other hand, the
model itself should be reformulated when instances within new classes are labeled. For clarity, we name
the former process model update and the latter model upgrade. Correspondingly, active learning in this
scenario is named Indeterminate Active Learning (IAL). In formula, the informativeness of an instance
for triggering model upgrade is as follows.
ℐupgrade (𝒙𝒙; 𝜙𝜙𝐿𝐿 , 𝜃𝜃𝐿𝐿𝑐𝑐 ) = 𝑃𝑃(𝑦𝑦 = 𝑐𝑐 + 1|𝒙𝒙; 𝜙𝜙𝐿𝐿 )ℱ(𝒙𝒙, 𝑐𝑐 + 1; 𝜃𝜃𝐿𝐿𝑐𝑐+1 )

(6)

where 𝜙𝜙𝐿𝐿 is the parameter of a specific model that assesses the probability of instance 𝒙𝒙 falling into
none of the existing classes, i.e. 𝑃𝑃(𝑦𝑦 = 𝑐𝑐 + 1|𝒙𝒙; 𝜙𝜙𝐿𝐿 ), and
𝑐𝑐
ℱ(𝒙𝒙, 𝑦𝑦; 𝜃𝜃𝐿𝐿𝑐𝑐 ) = ∑ 𝐻𝐻(𝑦𝑦𝑢𝑢 |𝒙𝒙𝑢𝑢 ; 𝜃𝜃𝐿𝐿+(𝒙𝒙,𝑦𝑦)
)
𝒙𝒙𝑢𝑢 ∈𝑈𝑈−𝒙𝒙

(7)

represents the sum of entropy along all the rest unlabeled instances 𝒙𝒙𝑢𝑢 ∈ 𝑈𝑈 − 𝒙𝒙. Similarly, an instance’s
value with respect to model update can be formularized as follows.
ℐupdate (𝒙𝒙; 𝜙𝜙𝐿𝐿 , 𝜃𝜃𝐿𝐿𝑐𝑐 ) &

𝑐𝑐

= 𝑃𝑃(1 ≤ 𝑦𝑦 ≤ 𝑐𝑐|𝒙𝒙; 𝜙𝜙𝐿𝐿 ) ∑ 𝑃𝑃(𝑦𝑦|𝒙𝒙; 𝜃𝜃𝐿𝐿𝑐𝑐 )ℱ(𝒙𝒙, 𝑦𝑦; 𝜃𝜃𝐿𝐿𝑐𝑐 )

(8)

𝒙𝒙∗IAL = arg min 𝒮𝒮(𝒙𝒙; 𝜙𝜙𝐿𝐿 , 𝜃𝜃𝐿𝐿𝑐𝑐 )

(9)

𝑦𝑦=1

IAL incorporates the potential for both model update and upgrade in a unified criterion as follows.
𝒙𝒙∈𝑈𝑈

Xiao-Yu Zhang et al. / Procedia Computer Science 108C (2017) 2413–2417

2416	

where
𝒮𝒮(𝒙𝒙; 𝜙𝜙𝐿𝐿 , 𝜃𝜃𝐿𝐿𝑐𝑐 ) = log (ℐupdate (𝒙𝒙; 𝜙𝜙𝐿𝐿 , 𝜃𝜃𝐿𝐿𝑐𝑐 ) − min ℐupdate (𝒙𝒙; 𝜙𝜙𝐿𝐿 , 𝜃𝜃𝐿𝐿𝑐𝑐 ) + 𝜎𝜎)
+𝜆𝜆log (− (ℐupgrade (𝒙𝒙; 𝜙𝜙𝐿𝐿 , 𝜃𝜃𝐿𝐿𝑐𝑐 )

𝒙𝒙∈𝑈𝑈

(10)

− max ℐupgrade (𝒙𝒙; 𝜙𝜙𝐿𝐿 , 𝜃𝜃𝐿𝐿𝑐𝑐 )) + 𝜎𝜎)
𝒙𝒙∈𝑈𝑈

where 𝜆𝜆 is a tunable parameter which controls the weight on model upgrade, and 𝜎𝜎 is a very small
number for the prevention of negative infinity.

2.3 Learning Framework

Active learning with 2-dimensional selection integrates both feature and instance selection into a
unified framework. Initially, given the labeled dataset 𝐿𝐿, feature-dimensional selection is implemented
to obtain the most discriminative feature subset w.r.t. 𝐿𝐿. Based on the selected features, a learning model
ℋ is trained with optimal performance on 𝐿𝐿. Using ℋ as instructive guidance, instance-dimensional
selection is used to select the most informative instances from the unlabeled dataset 𝑈𝑈. After that, the
selected instances are labeled by the oracle, and added into the labeled dataset 𝐿𝐿. With the variation of
𝐿𝐿 , a new round of feature-dimensional selection is carried out followed by instance-dimensional
selection. This procedure iterates until a certain number of instances are labeled. Finally, the ultimate
learning model ℋ is returned.

3 Experiments

Instance
# initially labeled
# selected per round
# total
Feature
# selected
# total

500
250
5000
60
151

Table 1: Detailed setup on image dataset.

Classification Accuracy

In order to validate the performance of the proposed method, we carry out classification tasks on
image dataset. Image classification at the semantic level is an important topic in multimedia analysis
and understanding. The dataset is a subset selected from the Corel image CDs. In the dataset, there are
50 categories with different semantic meanings, such as car, ship, human, etc. Each category contains
100 images, thus there are altogether 5000 images. Color and texture features are employed to represent
images. The color features consist of 125-dimensional color histogram and 6-dimensional color moment
in RGB. The texture features are extracted using 3-level discrete wavelet transformation, and the mean
and variance averaging on each of 10 sub-bands form a 20-dimensional vector. In this experiment, we
compare AL-2DS with traditional active learning (AL). As for instance selection, both IAL and DAL
are examined. Therefore, four methods are compared, i.e. IAL-2DS, DAL-2DS, IAL, and DAL. Detailed
setup about number of instances and dimension of features is listed in Table 1. For classification,
regularized softmax regression is used to model the multiple classes simultaneously. 6 rounds of active
learning are performed and the classification performance is recorded in Figure 1.
65%
60%
55%

DAL

50%

IAL

45%
40%

DAL-2DS

35%

IAL-2DS

30%

500

750

1000
1250
1500
1750
Number of Training Data

2000

Figure 1: Comparison of classification performance.

Analysis of the results are as follows.

IAL-2DS outperforms IAL, and DAL-2DS outperforms DAL. Via 2-dimensional mining, not
only the informative instances are actively selected for model re-training, but also the

	

Xiao-Yu Zhang et al. / Procedia Computer Science 108C (2017) 2413–2417





discriminative features are actively adjusted with the expansion of training dataset. As a result,
active learning with 2-dimensional selection surpasses the traditional 1-dimensional selection
on instances.
IAL-2DS outperforms DAL-2DS, and IAL outperforms DAL. Given an ill-posed initial
training dataset where the classes are not completely revealed, both the potential to optimize
the existing model and the ability to introduce new class into the model are essential. In this
scenario, indeterminate active learning is effective in discovery of informative instances with
both known and unknown class information.
IAL-2DS outperforms all the other methods. The highest classification performance indicates
that IAL-2DS is the most effective way of active learning, which combines the advantages of
dynamic feature optimization and indeterminate way of instance selection.

4 Conclusion
In this paper, we proposed an effective learning method with 2-dimensional active selection on
feature and instance. In the feature dimension, informative features are dynamically selected with the
expansion of training dataset by optimizing the discriminative ability. In the instance dimension,
informative instances are actively selected with integration of potential benefit in both model update
and model upgrade. Compared with traditional active learning, the advantages of the proposed method
are two-fold. Firstly, feature selection is coupled with instance selection during the learning process,
make the leaner more adaptive to the ever-changing information. Secondly, the balance between model
update and model upgrade brings about a more comprehensive measurement for the informativeness of
instances. Encouraging results are received from experiments on image dataset.

5 Acknowledgment
This work was supported by National Natural Science Foundation of China (Grant 61501457).

References
[1]
[2]
[3]
[4]
[5]
[6]
[7]
[8]
[9]

C. M. Bishop, Pattern Recognition and Machine Learning. New York: Springer, 2006.
X. Zhang, C. Xu, J. Cheng, H. Lu, and S. Ma, “Effective annotation and search for video blogs with integration
of context and content analysis,” IEEE Transactions on Multimedia, 11(2), pp. 272–285, 2009.
X. Y. Zhang, S. Wang, X. Zhu, X. Yun, G. Wu, and Y. Wang, “Update vs. upgrade: modeling with
indeterminate multi-class active learning,” Neurocomputing, 162, pp. 163–170, 2015.
X. Y. Zhang, S. Wang, and X. Yun, “Bidirectional active learning: a two-way exploration into unlabeled and
labeled data set,” IEEE Transactions on Neural Networks and Learning Systems, 26(12), pp. 3034–3044,
2015.
P. Campigotto, A. Passerini, and R. Battiti, “Active learning of Pareto fronts,” IEEE Transactions on Neural
Networks and Learning Systems, vol. 25, no. 3, pp. 506–519, 2014.
I. Zliobaite, A. Bifet, B. Pfahringer, and G. Holmes, “Active learning with drifting streaming data,” IEEE
Transactions on Neural Networks and Learning Systems, vol. 25, no. 1, pp. 27–39, 2014.
X. Zhang, “Interactive Patent classification based on multi-classifier fusion and active learning,”
Neurocomputing, 127, pp. 200–205, 2014.
Liu, H. and Motoda, H., 2012. Feature Selection for Knowledge Discovery and Data Mining. Springer Science
& Business Media.
Saeys, Y., Inza, I. and Larrañaga, P., 2007. A review of feature selection techniques in bioinformatics.
Bioinformatics, 23(19), 2507-2517.

2417

