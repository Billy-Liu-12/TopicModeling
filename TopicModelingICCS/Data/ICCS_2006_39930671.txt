High Performance Virtual Backup and Archive System∗
Dan Feng, Lingfang Zeng, Fang Wang, and Peng Xia
Key Laboratory of Data Storage System, Ministry of Education
School of Computer, Huazhong University of Science and Technology, Wuhan, China
dfeng@hust.edu.cn, zenglingfang@tom.com

Abstract. Built on a sequential write/read device, a tape library is seldom
considered as a viable place for fast backup/restore data. With the help of the
virtualization technology, in this paper we propose a virtual backup and archive
system, called VBAS. The purpose of VBAS is to maintain a consistent view of
mass storage so that the user can effectively manage it. And VBAS allows users
to create files and directories as well as delete, open, close, read, write and/or
extend the files on the device(s). VBAS maintains security on the files and
provides the management for fragmentation. Moreover, VBAS can support
large-scale file systems. Users have two ways to access VBAS: using general
backup application, and through the APIs provided by VBAS. Based on VTL,
RAID-DP, and iSCSI, VBAS not only has the disk-file-system-like functions,
but also retains the characteristics of tape library storage, thus achieving a good
tradeoff between cost and performance. The prototype system performance is
presented and improvements are analyzed to achieve higher write/read
performance.

1 Introduction
Tape is by far the most popular media for the near-line or offline (e.g. archiving data)
storage. As the amount of stored data in many data-intensive applications, such as high
energy physics research, weather prediction, spatial data and seismic data analysis,
increases dramatically, tape libraries can play an important role in maintaining and
backing up critical data. The disadvantage of tape drives is that they are
sequential-access devices. This makes them much too slow for general-purpose storage
operations.
The primary driving force behind users’ adoption of archiving has traditionally been
to reduce total cost of ownership (TCO). Though cost is often thought to be the most
important factor, it ranks lower than data retrieval performance and scalability in the
selection criteria for an archiving solution. Users want to achieve file retrieval in
minutes or even seconds.
∗

This paper is supported at Huazhong University of Science and Technology by the National
Basic Research Program of China (973 Program) under Grant No. 2004CB318201, National
Science Foundation of China No.60273074, No.60303032, Huo Yingdong Education
Foundation No.91068.

V.N. Alexandrov et al. (Eds.): ICCS 2006, Part III, LNCS 3993, pp. 671 – 678, 2006.
© Springer-Verlag Berlin Heidelberg 2006

672

D. Feng et al.

Nowadays, many file systems based on disk and tape storages have been presented.
AMASS [1] for UNIX, designed for direct-attached and network-attached storage
environments, is a cost-effective solution for enterprises that have more data than their
disk capacity can support. AMASS transforms libraries into truly online direct-access
mass storage. AMASS presents an automated optical, tape, or DVD library as one
device and one mount point (showed in Figure 1.), through a standard UNIX file system
interface consisting of directories and files. Users or applications can save or access
files residing on libraries in the same way they would with magnetic disk. AMASS
provides transparent access to data on disk and tape library storage.

Fig. 1. AMASS provides transparent access to data on disk and library storage

However, disks in AMASS are used based on the general-purpose file system, and
thus they have to confront the same set of problems inherited from general-purpose file
systems.
SEPATON's Disk Dynamic File System allows large I/O streams to execute
efficiently and has the built-in infrastructure to dynamically balance performance
across all available disks in their VTL appliance. The Disk Dynamic File System has
the important side effects of not only sustaining maximum throughputs, but also
dynamically load balancing I/O streams without any requirement for performance
“tuning” [2]. But, The Disk Dynamic File System is still a disk-related file system and
it is not feasible for the management of tape library storage system.
Our virtual backup and archive system (VBAS) effectively integrates the virtual tape
library (VTL) [3], the VTS technology [10], the RAID-DP technology [4], and the
iSCSI [5] technology to provide transparent tape file access for users while retaining
other functions of tape libraries. Our study of VBAS shows the main advantages of
VBAS over the conventional tape libraries and conventional (disk-based) file systems
as follows: (1) High backup and restore performance with disk-file-system-like
functions. (2) Low cost compared with simple disk-based system. (3) Some finer
functions integrated both RAID and the tape library.
The main contributions of this paper are: (1) Overcomes general file systems more
prone to being infested with viruses and having an inherent problem of fragmentation.
(2) Provides a virtual backup and archive system based on RAID. (3) Implements the
prototype system via virtual tape library technology and presents and discusses the
experiment results.
The rest of the paper is organized as follows. The design and implementation of
VBAS is presented in the section that follows. Test results and performance analysis

High Performance Virtual Backup and Archive System

673

are discussed in Section 3. Finally, we give the conclusions and outline some remaining
problems in Section 4.

2 Design and Implementation of VBAS
VBAS enables data management without disrupting end-user or application accesses.
Done correctly, the VBAS can reduce storage management overhead, simplify
end-user accesses, and enable additional storage management functionalities. The
VBAS solutions transform a mass storage system from a “two-size-fits-all”
(high-performance disk and low-cost tape) world into a sophisticated mix of discrete
storage elements with a wide variety of performance, cost and capacity attributes. The
alphabet soup of technologies (VTL, iSCSI, SATA, etc.) presents an excellent
opportunity for the storage systems applications to locate the right data on the right
device at the right time.
2.1 Hardware Architecture of a VBAS-Based Testbed
Figure 2 shows the hardware architecture of our mass storage system testbed based on
VBAS. The target device comprises a RAID and a tape library connected by a SCSI
channel, also called a hybrid device. The console and application servers (Web server,
E-mail server etc.) and the target device are interconnected by a TCP/IP network. The
console, web server, e-mail server and backup server form an initiator and they access
data in the hybrid device through the iSCSI protocol.

Fig. 2. Hardware architecture of the mass storage system testbed based on VBAS

2.2 The VBAS-Based Software Deployment
iSCSI builds on the two most widely used protocols from the storage and the
networking worlds. From the storage side, iSCSI uses the SCSI command set, the core
storage commands used throughout all storage configurations. On the networking side,
iSCSI uses IP and Ethernet that form are the basis for most enterprise networks and
used in metropolitan and wide area networks.

674

D. Feng et al.

With the help of iSCSI, shown in Figure 2, almost all of backup applications, such as
tar [6], taper [7] and bacula [8], can access the target (hybrid device) via VBAS. It is
possible that, for a backup application, the daemon servers may be deployed in the
backup server while its proxies may be installed in the web server or E-mail server.
Figure 3 shows the functional and logical relationship among the function modules
in the initiator and the target. The target device refers to an entity that presents itself as
a SCSI direct access disk and tape sequential access while running within the Linux
kernel space. In the user space implementation, the entity providing the SCSI
functionality and the entity responsible for transmitting SCSI over a given protocol
form one logical piece of code. However, in the kernel space implementation, the entity
responsible for handling SCSI commands has an existence independent of the low-level
front-end target driver that is responsible for the transmission of SCSI in a
device-specific manner. Thus, in terms of visualization, it may be better to think of the
kernel space modules as consisting of two distinct entities - the generic SCSI target
mid-level and the low-level front-end protocol-specific target driver. The VBAS
module is implemented in the initiator. It can work with the traditional disk-related file
system and this solution is transparent for user space application. Also, users can
custom their applications based on the APIs provided by VBAS.

Fig. 3. The functional and logical relationship among the function modules of the prototype
system

In the target, our function modules are implemented in SCST (The SCSI target
mid-level subsystem for Linux) [9], which is a generic SCSI target middle level for
Linux. It is designed to provide unified, consistent interface between SCSI target
drivers and Linux kernel and simplify target drivers development as much as possible.
Although data distribution policy is different comparing with [10] [11], in substance,
their implementation technologies are analogical. And they all have to record all the
logical objects [12] on the RAID. In Figure 3, the SCSI command analysis module
receives SCSI sequential commands from the backup application, and determines
whether the commands should be executed on the RAID or on the tape library. Then it
delivers them to the proper module (or media). The SCSI command transform module

High Performance Virtual Backup and Archive System

675

is responsible for transforming SCSI sequential commands into SCSI block commands.
The LBA (logical block address) mapping module maintains the block mapping
information, which associates the logical unit of an object with its logical block address
in the RAID. The data transfer module performs data transfer between RAID and tape
library according to some information lifecycle management policy.
In addition, VBAS provides a client application that is implemented using the API of
VBAS. The client application may be deployed in the console (Figure 2.) and perform
remote file management operations. Users also can implement their own remote file
management application by the API of VBAS.
2.3 The VBAS Data Structures
Traditionally, a file system represents the logical structures and software routines used
to control access to the storage on a hard disk system. However, VBAS, a tape-based
file system that hides the details about tapes and, provides an API for applications, is
designed and implemented to provide the “access by name” functions, including tape
file creation, tape file read, tape update, tape file deletion, tape file copy, tape file
renaming and tape defragmentation.
1. VBAS data structures and functions
The file allocation table (FAT) of every library slot and every tape are defined. The
FILE_NODE constructs the file name information and the FILE_RECORD records
some information of file in true tapes. The virtual tape list and logical object list are
defined. For the type element, its value may be one of the defined constants –
LOGICAL_BLOCK, FILE_MARK, SET_MARK, BEGIN_NODE or END_NODE.
Some main functions (in target software) dispose the transform from SCSI stream
command to SCSI block command, such as INQUIRY (0x12), REWIND (0x01),
READ (0x8), WRITE (0xA), MODE_SENSE (0x1A), WRITE_FILEMARKS (0x10),
SPACE (0x11) etc.. Specially, for the RAID, the command type of write and read is 10,
so the VBAS has two transform functions (transform_write_6to_10 and
transform_read_6to_10).
2. The system/configure files in VBAS
At the same time, VBAS provides some system files and configure files which
facilitate the configuration about VBAS. The system files record some file metadata
information in VBAS, and the configure files provides the configure information of
VBAS. For instance, the TapeLibrarySlotInfo.txt records the tape library slot
information (defined by SLOT_FAT) both in the VTL and the true tape library. The
TapeVBASFAT.txt stores the information of tape file allocation table (defined by
TAPE_FAT) in VBAS. The FileName.rec gives file name information (defined by
FILE_RECORD), and the configure information for the VBAS is set or got by the
administrator.
3. File operation algorithms in VBAS
This subsection shows those file operation algorithms, such as create, dir, read,
update, copy, erase, rename and defragment etc. For each file operation algorithm, it
begins at the initialization file system function - InitFileSystem(), and ends at the close
file system function – CloseFileSystem(). Because those algorithms aim at the write or
read files in a tape, they are all applicable to both the true tape library and the VTL.
Also, as mentioned above, in the initiator, users can also use the APIs provided by the

676

D. Feng et al.

VBAS to implement tape file management. Key file operation algorithms for VBAS
are as follows (update, copy, erase, rename and defragment are omitted):
Create {
1: Open the file TapeLibrarySlotInfo.txt in VBAS, create read/write buffer, initiate
the process of read/write and open the device file (e.g. st0 or nst0) of VTL.
2: Judge if the online VTL tape runs out of space or not. If the tape runs out of space,
exchange a new VTL tape.
3: Deal with the name confliction according to the SLOT_FAT and open the file of
tape file allocation: TapeVBASFAT.txt, and form the current TAPE_FAT.
4: Write data to the buffer.
5: The write process waits for that the buffer becomes full, and writes data to the
VTL tape.
6: Write the remainder data in the buffer and close the FileName.rec,
TapeVBASFAT.txt and TapeLibrarySlotInfo.txt.
7: Free the read/write buffer. And stop read/write process and close the device file of
VTL.
}
Dir {
1: Open the file TapeLibrarySlotInfo.txt in VBAS, create read/write buffer, initiate
the process of read/write and open the device file of VTL.
2: Read the every item in the TAPE_FAT of VBAS and list them one by one.
3: Close the VBAS and close the device file of VTL.
}
Read {
1: Open the file TapeLibrarySlotInfo.txt in VBAS, create read/write buffer, initiate
the process of read/write and open the device file of VTL.
2: Judge if the required file is in the online VTL tape or not. If not, exchange a new
VTL tape, and open the files: TapeVBASFAT.txt and FileName.rec.
3: The read process reads the file to the buffer according to the first address of the
TAPE_FAT and the FILE_RECORD.
4: The application reads data from the file buffer by the read function of VBAS.
5: Close the FileName.rec, TapeVBASFAT.txt and TapeLibrarySlotInfo.txt.
6: Free the read/write buffer and stop read/write process and close the device file of
VTL, close VBAS.
}

3 Test Results and Performance Analysis
To test the write/read performance of VBAS, Windows 2000 and Redhat Linux were
used in front-end servers. Under Windows 2000, we adopted Auto Backup. Under the
Redhat Linux system (Linux kernel 2.4.20-8), we adopted tar and Taper [7] as the
backup software. These three kinds of backup software performed well with VBAS.
The results indicate that VBAS has compatibility with multiple operating systems and
backup software.
We tested the performance of VBAS by using taper under Redhat Linux (also Linux
kernel 2.4.20-8). The tape drive we adopted was HP MSL5030, and the tape media was

High Performance Virtual Backup and Archive System

677

hp ultrium 200GB data cartridge (C7971A). We adopted the SEGATE ST3404LC
SCSI disk to simulate a VTL tape.
Our main concern is the backup time (write performance) and the restore time (read
performance) for different primary backup devices. Also, the write/read performance
both in different Ethernet and in the local node was tested in our experiment. Table 1
and 2 show the results. There are four group data, and each group recorded those test
results about the write time and the read time. Moreover, we adopted different number
of file, and the total size of file(s) was also ranked from 50Mbyte to 1Gbyte.
Table 1. Throughput in different Ethernet

Number
of file

Total
size
of file
(Mbyte)

1
1
1
381
3386
22216

50.0
200.0
1000.0
85.0
455.1
1000.0

The VTL tape in VBAS
1000M Ethernet
100M Ethernet
Write
Read
Write
Read
Throughput Throughput Throughput Throughput
(Mbyte/Min) (Mbyte/Min) (Mbyte/Min) (Mbyte/Min)
1509.7
1000.0
603.8
750.0
1182.0
705.9
1006.4
571.4
1115.1
714.3
1090.0
566.0
850.6
1020
850.6
850.6
684.7
941.6
681.2
910.2
503.9
759.5
503.9
659.3

Table 2. The throughput comparison of the local node of VTL and the physical tape

Number
of file
1
1
1
381
3386
22216

Total
size
of file
(Mbyte)
50.0
200.0
1000
85.0
455.1
1000.0

The VTL in Local Node
Write
Read
Throughput Throughput
(Mbyte/Min) (Mbyte/Min)
1509.7
1509.7
1313.3
750.0
1257.4
750.0
1020.7
1275.0
989.1
1335.3
672.1
1153.8

The Physical Tape
Write
Read
Throughput Throughput
(Mbyte/Min) (Mbyte/Min)
377.4
187.5
407.5
203.4
399.3
196.7
510.3
212.5
460.4
224.4
427.7
269.1

The results showed that the average write time of the VTL tape in 1000M Ethernet is
99.14% of that in 100M Ethernet and the average read time in 1000M Ethernet is
95.68% of that in 100M Ethernet, which indicated that the different network
environment has few influence for VBAS. However, the test results in local node
indicate that network has large influence for VBAS. For instance, the average write
time of the VTL tape in local host is 54.62% of that in 100M Ethernet and the average
read time in local host is 61.64% of that in 100M Ethernet.
At the same time, the size of single file much affects the write performance of
VBAS. For example, Table 1 and Table 2 show the write throughput are descending
with the ascending number of file. Moreover, the average backup speed of the VTL

678

D. Feng et al.

tape in local node is 2.65 times of that of the physical tape and the average restore speed
of the VTL tape in local node is 3.77 times of that of the physical tape, which indicate
that the VTL tape in VBAS can enhance the backup speed greatly. So we can get a
conclusion that our VBAS is fit for backup and restore applications, specially, for large
size file.

4 Conclusion and the Future Work
It is important to note that simply replacing tape with low-cost disk will not provide the
technological advantages. A disk-based backup solution can provide for smaller
backup windows, and also provide somewhat faster recovery of distinct data files. But
again, a disk-based solution does not provide off-site protection. VBAS significantly
improves backup and restore by enabling all the data to remain online for faster,
consistent restores. VBAS opens doors for new strategic applications by removing the
cost and complexity of large quantities of traditional disk storage. The other traditional
file system problems regarding performance and security are also no longer a concern.
This is due to the fact that VBAS is typically proprietary and designed to act similar to
a tape system with performance of disks. So, it is the primary benefits from archiving
for end users to reduce in primary disk space and to improve performance (as expressed
in faster response times).

References
1. Website, May, 2005, http://www.adic.com/
2. Paul Feresten. Comparing Host-Based D2D to VTLs for Backup and Restore - Part 2.
Website, 2005. http://www.wwpi.com/index.php?option=com_content&task=view&id=
132&Itemid=67
3. T. E. Anderson, M. D. Dahlin, J. M. Neefe, D. A. Patterson, D. S. Roselli, and R. Y. Wang.
Serverless network file systems. ACM Transactions on Computer Systems, 14(1):41–79,
February 1996.
4. Chris Lueth, Network Appliance, Inc. October, 2004. NetApp Data Protection: Double
Parity RAID for Enhanced Data Protection with RAID-DP. March 5, 2005, available from:
http://www.netapp.com/tech_library/3298.html
5. J. Satran et al. Internet Small Computer Systems Interface (iSCSI). Available from:
http://www.ietf.org/rfc/rfc3720.txt,April 2004.
6. Website, January 10, 2005, http://savannah.gnu.org/projects/tar/
7. Website, January 10, 2005, http://www.e-survey.net.au/taper/
8. Website, January 10, 2005, http://www.linux.org/apps/AppId_8816.html
9. Ashish A. Palekar etc. Design and Implementation of a Linux SCSI Target for Storage Area
Networks. Proceedings of the 5th Annual Linux Showcase & Conference, 2001.
10. Mu Fei, SHU Ji-wu, Li Bigang, ZHENG Wei-min. A Virtual Tape System Based on Storage
Area Networks. H. Jin, Y Pan, N. Xiao and J. Sun (Eds.), GCC'2004 Workshop on Storage
Grid and Technologies, LNCS 3252, pp.278-285, 2004.
11. Jussi Myllymaki,Miron Livny. Disk-tape joins: synchronizing disk and tape access. ACM
SIGMETRICS Performance Evaluation Review, pp.279 – 290, 1995.
12. ANSI, SCSI Stream Commands-2 (SSC-2), revision 09, 9 July 2003, http://www.t10.org

