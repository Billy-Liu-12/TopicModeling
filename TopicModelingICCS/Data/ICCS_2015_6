Procedia Computer Science
Volume 51, 2015, Pages 2474–2482
ICCS 2015 International Conference On Computational Science

Parallel solution of DDDAS variational inference problems
Vishwas Rao1 and Adrian Sandu1
Virginia Tech., Blacksburg, Virginia, United States
visrao@vt.edu, asandu7@vt.edu

Abstract
Inference problems in dynamically data-driven application systems use physical measurements
along with a physical model to estimate the parameters or state of a physical system. Developing parallel algorithms to solve inference problems can improve the process of estimating and
predicting the physical state of a system. Solution to inference problems using the variational
approach require multiple evaluations of the associated cost function and gradient, where the
gradient is deﬁned as the increase/decrease inﬂection point of the variable between two points.
In this paper we present a scalable algorithm based on augmented Lagrangian approach to
solve the variational inference problem. The augmented Lagrangian framework facilitates parallel cost function and gradient computations. We show that the methodology is highly scalable
with increasing problem size by applying it for the Lorenz-96 model.
Keywords: DDDAS, time-parallel, variational inference

1

Introduction

Dynamically data-driven application systems (DDDAS [2, 4]) is a paradigm whereby simulations
and measurements become a symbiotic feedback control system. An important application of
DDDAS is the solution of inference problems where information from physical measurements
is combined with a mathematical model to obtain estimates of the state or parameters of a
physical system.
In this work, we develop a parallel framework to solve the inference problems. Our methodology is based on the variational framework and can have numerous applications in DDDAS.
Examples include data assimilation for numerical weather prediction, the dynamic steering
of the measurement process to improve overall forecasts [13], and dynamic location of faulty
sensors.
Typically, there are two popular approaches to solve inference problems: variational and
ensemble-based methods. The variational approach is based on optimal control theory, whereas
the ensemble-based approaches is rooted in statistical estimation theory. In this paper, we focus on the variational approach. The variational approach requires the development of adjoint
and tangent linear models. This method is popular in most national and international numerical weather forecast centers to provide the initial state for their forecast models. Variational
2474

Selection and peer-review under responsibility of the Scientiﬁc Programme Committee of ICCS 2015
c The Authors. Published by Elsevier B.V.

doi:10.1016/j.procs.2015.05.356

Parallel solution of DDDAS variational inference problems

V Rao and A Sandu

approaches require applying an optimization algorithms, such as L-BFGS (Limited memory
Broyden Fletcher Goldfard Shanno) [9] and conjugate gradient methods. The gradient is evaluated by solving the adjoint model. The entire process of evaluating the cost function, gradient,
and subsequently using these computations in the optimization process is highly sequential.
The current generation of computers are becoming more and more parallel and the processor
speed is not increasing rapidly. As our understanding of the physics improves, the computer
model representing physics of the dynamical system becomes more and more complex. Hence
it is necessary to develop parallel and scalable algorithms to solve inference problems.
Tr´emolet and Le Dimet have shown how variational data assimilation (4D-Var) can be used
to couple models and to parallelize the computations in the spatial direction [14]. In [11], the
author considers diﬀerent data distribution strategies to perform scalable 4D-Var. However,
the scalability is restricted to spatial parallelism. A scalable approach for variational data
assimilation is presented in [6]. The authors achieve parallelism by dividing the 4D-Var into
multiple 3D-Var sub-problems. In [1], the authors show how to apply augmented Lagrangian
approach to constrained inference problems in the context of graphical models.
In this paper, we present a scalable parallel algorithm for solving the 4D-Var problem. The
algorithm evaluates the important components of 4D-Var namely, cost function and gradient
computations in parallel. The most important contribution of this work is that it achieves
parallelism in temporal direction and can be conveniently extended to achieve parallelism in
spatio-temporal directions. This approach is similar to our previous work on adjoint-based timeparallel integration [12]. The paper is organized as follows: Section 2 gives a brief description
about data assimilation and 4D-Var. In Section 3 we show how the 4D-Var can be reformulated
using augmented Lagrangian technique so that the gradient and cost function evaluations can
be parallelized. Section 4 gives a detailed description of the parallel algorithm. Section 5 shows
the numerical experiments with the chaotic Lorenz-96 model and Section 6 gives concluding
remarks and future directions.

2

Data assimilation and 4D-Var

Data assimilation (DA) is the fusion of information from imperfect model predictions and noisy
data, to obtain a consistent description of the state of a physical system [5, 7]. To obtain an
estimate of the true state of a system xtrue , three diﬀerent sources of information are combined:
the prior information, the model, and the observations. The best estimate that optimally fuses
all these sources of information is called the analysis xa . The following subsections brieﬂy
describe the sources of information.
Prior information. The prior information encapsulates our current knowledge of the system. The prior information is captured by the background estimate of the state xb and the
corresponding background error covariance matrix B.
The model. The model captures our knowledge about the physical laws that govern the
evolution of the system. The model evolves an initial state x0 ∈ Rn at the initial time t0 to
future states xi ∈ Rn at future times ti . A general model equation is represented as follows:
xi = M0,i (x0 ) .

(1)
2475

Parallel solution of DDDAS variational inference problems

V Rao and A Sandu

The observations. Observations represent the snapshots of reality available at discrete time
instances. Speciﬁcally, measurements yi ∈ Rm of the physical state are taken at times ti , i =
1, · · · , N
, i = 1, · · · , N.
yi = Ht xtrue (ti ) − εmeas
i
The observation operator Ht maps the physical state space onto the observation space. The
.
measurement errors are denoted by εmeas
i
The model state is related to observations by the following relation:
yi
εobs
i

=
=

H (xi ) − εobs
i ,
εrepres
i

+

εmeas
i

i = 1, · · · , N,

(2)

.

The observation operator H maps the model state space onto the observation space. The observation error term εobs
).
accounts for both measurement and representativeness errors (εrepres
i
i
The measurement errors can be attributed to faulty equipments. The representativeness errors
are due to the inaccuracies of the numerical approximation inherent to the model.

2.1

Four dimensional variational data assimilation (4D-Var)

In 4D-Var, one ﬁnds the control variable which minimizes the mismatch between the model
forecasts and the observations. In strong-constraint 4D-Var, the control parameters are the
initial conditions x0 ; they uniquely determine the state of the system at all future times via
the model equation (1). The background state is the prior knowledge of the initial conditions
xb0 . Given the background value of the initial state xb0 , the covariance of the initial background
errors B0 , the observations yi at ti and the corresponding observation error covariances Ri ,
i = 1, · · · , N , the 4D-Var problem provides the estimate xa0 of the true initial conditions by
minimizing the following cost function:
J (x0 ) =

1
x0 − xb0
2

T

x0 − xb0 +
B−1
0

1
2

N
i=1

T

(H (xi ) − yi ) R−1
i (H (xi ) − yi ) ,

(3)

subject to the constraint posed by the model equation (1). The 4D-Var can be formulated as
xa0 =arg min

J (x, x0 )

x0

(4)

subject to

(1) .

The optimal solution for the inverse problem (4) can be found by using the adjoint sensitivity
approach described extensively in [8] and [16]. The solution procedure using the adjoint sensitivity procedure cannot be parallelized as it is. In order to parallelize the solution procedure,
we reformulate the problem using the augmented Lagrangian framework.

3

4D-Var using the augmented Lagrangian framework

The 4D-Var cost function is given by
J =
2476

1
x0 − xb0
2

T

x0 − xb0 +
B−1
0

1
2

N
k=1

T

(H (xk ) − yk ) R−1
k (H (xk ) − yk ) .

(5)

Parallel solution of DDDAS variational inference problems

V Rao and A Sandu

The cost function (5) is subject to the following model constraints:
xk+1 = Mk,k+1 (xk ) ,

k = 0, 1, · · · , N − 1 .

(6)

In equation (6) Mk,k+1 represents the model, given the physical state, xk at time tk evaluates the
state xk+1 at tk+1 . The augmented Lagrangian associated with (5) with (6) as the constraints
is given by [15]:
L

=

1
x0 − xb0
2
N−1

−

T

B−1
0

x0 −

xb0

1
+
2

N
k=1

T

(H (xk ) − yk ) R−1
k (H (xk ) − yk )

λTk+1 (xk+1 − Mk,k+1 (xk ))

k=0

+

μ
2

N−1
k=0

T

(xk+1 − Mk,k+1 (xk )) P−1
k (xk+1 − Mk,k+1 (xk )) ,

(7)

where, λ are Lagrange parameters, R is the observation error covariance matrix, and P is a
scaling matrix. The gradient of the augmented Lagrangian can be computed in parallel over
the diﬀerent sub-intervals.

4

The parallel algorithm

The solution procedure for 4D-Var requires multiple cost function and gradient evaluations. In
this Section we describe a detailed procedure to evaluate the cost function and the gradients
in parallel. Equation (7) can be used to carry out the cost function computations in parallel.
Each cost function computation requires one model evaluation per processor. To complete the
gradient evaluation, each processor needs to compute one forward and adjoint model. It should
be noted that to begin the optimization, we need an initial guess for x and λ. The initial guess
for xk ’s can be obtained by performing a forward integration using the background value for
initial conditions. At optimality, we know that
x0 − xb0 + λ0 = 0
B−1
0

(8)

holds. Hence we can use (8) to obtain an initial guess for λ.
The optimization proceeds in two stages: inner and outer iterations. Inner iterations solve
the optimization for a particular value of μ and λ. Subsequently, the values are μ and λ are
updated after every inner iteration. The update after iteration k is carried out as follows:
μk+1
λk+1
i

=

ρμk

=

λki

(9a)
k

− μ (xi − M (xi−1 ))

, i = 1, . . . , N .

(9b)

Diﬀerent strategies to update μ and λ can be used [3]. To achieve temporal parallelism, the assimilation window is divided into multiple sub-intervals. The state variable for the optimization
process is the augmented variable containing the state vector at the beginning of each of the
sub-interval. The initial state vector of the augmented Lagrangian in (7) is an augmentation
of states at the beginning of every sub-interval obtained by propagating the background state
using model equations. Figures 1 and 2 illustrate the convergence process. We start initially
with a low value of μ, ensuring that the constraints are imposed loosely. Hence large discontinuities and large errors are seen in the green curve. Subsequently, μ is increased and as a result
2477

Parallel solution of DDDAS variational inference problems

9

14

8

12

X(16)

X(8)

7
6
5

10
8
6

4
3

V Rao and A Sandu

0

0.2

0.4

0.6

0.8

Time

4

0

0.2

0.4

0.6

0.8

Time

10

FinalSolution
X(24)

8

Background
Iteration−1

6

Iteration−3
Iteration−5

4

Observations
2

0

0.2

0.4

0.6

0.8

Time

Figure 1: Convergence of the parallel algorithm towards the ﬁnal solution. Three diﬀerent
variables are shown as an illustration.

the constraints are satisﬁed accurately. Black curve does not have any discontinuities, has very
low errors compared to previous iterations and resembles the strong-constraint 4D-Var solution
closely.

5

Numerical experiments

In this study we test the parallel implementation of the 4D-Var algorithm using the 40-variable
Lorenz-96 model [10]:
dxi
= xi−1 (xi+1 − xi−2 ) − xi + F ,
(10)
dt
where x = (x1 , x2 , . . . , x40 )T ∈ R40 is the state vector, and the forcing F = 8. We use synthetic
observations to perform the data assimilation. The synthetic observations are generated by
perturbing the reference trajectory with normal noise at uncertainty level of 5%. The background uncertainty is set to 8%. For convenience, no observations are taken at the beginning
of an assimilation window. We compare the parallel and serial versions of the algorithm were
run on a multi-core machine. The parallel algorithm was implemented in parallel-matlab.

5.1

Numerical results

We compare the performance of the proposed parallel implementation with that of the standard
4D-Var. We show both root mean square error (RMSE) and timing comparisons between the
two implementations. The RMSE relative to the reference solution is used to compare the
2478

0.2

0.6

0

0.4

−0.2

0.2

X(16)

X(8)

Parallel solution of DDDAS variational inference problems

−0.4
−0.6
−0.8

V Rao and A Sandu

0
−0.2

0

0.2

0.4

0.6

−0.4

0.8

0

0.2

Time

0.4

0.6

0.8

Time

0.5

Errors after Iteration 1

X(24)

0
−0.5

Errors after Iteration 3
−1

Errors after Iteration 5

−1.5
−2

0

0.2

0.4

0.6

0.8

Time

Figure 2: Error in the solution at diﬀerent stages. The serial solution is taken as a reference to
compute the errors. Three diﬀerent variables are shown as an illustration.

1

10

Forecast
Parallel 4D−Var
Serial 4D−Var
0

RMSE

10

−1

10

−2

10

0

0.1

0.2

0.3
Time

0.4

0.5

0.6

(a) RMSE comparison for 6 Observations

Figure 3: RMSE comparison between serial and 4D-Var for Lorenz model.
2479

Parallel solution of DDDAS variational inference problems

V Rao and A Sandu

Time taken for performing 4D−Var

1200

Parallel 4D−Var
Serial 4D−Var

1000

800

600

400

200

0
0.2

0.3

0.4

0.5

Assimilation Window Size

Figure 4: Timing Comparisons between serial and parallel 4D-Var (with 12 threads) for Lorenz
model .
analysis obtained by both 4D-Var and parallel 4D-Var. The RMSE is given by

RMSE =

1
nvar

nvar
i=1

(xi − xtrue
)
i

2

,

(11)

where xtrue is the reference solution, and x is the analysis (both propagated forward in time over
the window). RMSEs for both implementations are calculated over the assimilation window by
propagating the states, x, and xtrue using the full model and applying (11). Figure 3(a) show
the RMSEs of the parallel and the standard 4D-Var. We see that the parallel 4D-Var matches
the serial version well. Figures 5(a) and 5(b) show the scalability of cost function and gradient
computations. Figure 4 shows the timing comparisons of serial and parallel 4D-Var. It can
be seen that the parallel algorithm is scalable in a weak sense, i.e., as the total computational
work increases, the time taken to complete it is approximately constant.

6

Conclusions and future work

In this paper we have presented a scalable framework to solve variational DDDAS inference
problems in parallel. Cost function computations and gradient evaluations, which are the
main components of the algorithm, can be performed completely in parallel. We illustrate the
proposed approach using a data assimilation test for the Lorenz-96 model. We see that the
algorithm is feasible and gives real speed-up. The Lagrange parameter λ and μ can be updated
to improve the convergence of the optimization. We intend to apply this algorithm to large scale
problems such as, shallow water models and WRF (Weather Research and Forecast) models.
A natural extension to this methodology is to include space-time parallelism. We intend to use
certain ideas from [6] to further improve the scalability of the algorithm.
2480

Parallel solution of DDDAS variational inference problems

V Rao and A Sandu

1
Serial
Parallel

Computational time

0.8

0.6

0.4

0.2

0
2

4

6
No. of observations

8

10

(a) Comparison for one cost function evaluation

2.5
Serial
Parallel

Computational time

2

1.5

1

0.5

0
2

4

6
No. of observations

8

10

(b) Comparison for one gradient evaluation

Figure 5: Timing comparisons for cost function and gradient computations between serial and
parallel algorithms for Lorenz model.

2481

Parallel solution of DDDAS variational inference problems

V Rao and A Sandu

Acknowledgements
This work was supported by AFOSR DDDAS program through the awards FA9550-12-1-0293DEF and AFOSR 12-2640-06, managed by Dr. Frederica Darema.

References
[1] P. Aguiar, E. P. Xing, M. Figueiredo, N. A. Smith, and A. Martins. An augmented lagrangian
approach to constrained map inference. In L. Getoor and T. Scheﬀer, editors, Proceedings of the
28th International Conference on Machine Learning (ICML-11), pages 169–176, New York, NY,
USA, 2011. ACM.
[2] A. Aved, F. Darema, and E. Blasch. Dynamic data driven application systems. www.1dddas.org,
2014.
[3] E. G. Birgin and J. M. Mart´ınez. Improving ultimate convergence of an augmented Lagrangian
method. Optimization Methods and Software, 23(2):177–195, 2008.
[4] E. Blasch, G. Seetharaman, and K. Reinhardt. Dynamic data driven applications system concept
for information fusion. Procedia Computer Science, 18(0):1999 – 2007, 2013. 2013 International
Conference on Computational Science.
[5] R. Daley. Atmospheric data analysis, volume 2. Cambridge University Press, 1993.
[6] L. D’Amore, R. Arcucci, L. Carracciuolo, and A. Murli. A scalable approach for variational data
assimilation. Journal of Scientiﬁc Computing, 61(2):239–257, 2014.
[7] E. Kalnay. Atmospheric modeling, data assimilation, and predictability. Cambridge University
Press, 2003.
[8] S. Li and L. Petzold. Adjoint sensitivity analysis for time-dependent partial diﬀerential equations
with adaptive mesh reﬁnement. Journal of Computational Physics, Elsevier, 198:310–325, jul 2004.
[9] D. C. Liu and J. Nocedal. On the limited memory BFGS method for large scale optimization.
Mathematical programming, 45(1-3):503–528, 1989.
[10] E. N. Lorenz. Predictabilty: A problem partly solved. In Proceedings of Seminar on Predictability,
pages 40–58, 1996.
[11] J. Rantakokko. Strategies for parallel variational data assimilation. Parallel Computing,
23(13):2017 – 2039, 1997.
[12] V. Rao and A. Sandu. An adjoint-based scalable algorithm for time-parallel integration. Journal of
Computational Science, 5(2):76 – 84, 2014. Empowering Science through Computing + BioInspired
Computing.
[13] A. Sandu, A. Cioaca, and V. Rao. Dynamic sensor network conﬁguration in infosymbiotic systems using model singular vectors. Procedia Computer Science, 18(0):1909 – 1918, 2013. 2013
International Conference on Computational Science.
[14] Y. Tr´emolet and F.-X. Le Dimet. Parallel algorithms for variational data assimilation and coupling
models. Parallel Computing, 22(5):657 – 674, 1996.
[15] S. J. Wright and J. Nocedal. Numerical optimization, volume 2. Springer New York, 1999.
[16] S. Li Y. Cao and L. Petzold. Adjoint sensitivity analysis for diﬀerential-algebraic equations:
algorithms and software. Journal of Computational and Applied Mathematics, Elsevier, 149:171–
191, 2002.

2482

