An Adaptive Fuzzy kNN Text Classifier
Wenqian Shang1, Houkuan Huang1, Haibin Zhu2, Yongmin Lin1,
Youli Qu1, and Hongbin Dong1
1

School of Computer and Information Technology, Beijing Jiaotong University, 100044,
China
shangwenqian@hotmail.com
2
Senior Member, IEEE, Dept. of Computer Science, Nipissing University, North Bay,
ON P1B 8L7, Canada
haibinz@npissingu.ca

Abstract. In recent years, kNN algorithm is paid attention by many researchers
and is proved one of the best text categorization algorithms. Text categorization
is according to training set which is assigned class label to decide a new document which is not assigned class label belongs to some kind of document. Until
now, kNN algorithm has still some issues to need to study further. Such as: improvement of decision rule; selection of k value; selection of dimensions (i.e.
feature set selection); problems of multiclass text categorization; the algorithm’s executive efficiency (time and space) etc. In this paper, we mainly focus
on improvement of decision rule and dimension selection. We design an adaptive fuzzy kNN text classifier. Here the adaptive indicate the adaptive of dimension selection. The experiment results show that our algorithm is effective and
feasible.

1 Introduction
With the development of the web, large numbers of documents are available on the
Internet. Automatic text categorization becomes more and more important for dealing
with massive data. It becomes a key technology to deal with and organize large numbers of documents. More and more methods based on statistical theory and machine
learning has been applied to text categorization in recent years. For example, knearest neighbor (kNN)[1]-[4], Naive Bayes[5][6], Decision Tree[7][8], Support
Vector Machines (SVM)[9], Linear least squares fit[10], neural network[11][12],
SWAP-1, Rocchio and so on. Among these algorithms, kNN algorithm is studied by
many researchers and is proved one of the best text categorization algorithms.
In recent years, many researchers study the improvement of kNN when the class distribution is uneven. They mainly focus on the improvement of decision function to
resolve the problem of uneven class distribution, such as [13]-[17]. Our algorithm is
based on the improvement of decision function too, but our improvement is very different from theirs. We adopt the theory of fuzzy sets, through analyzing the relationship
among distance, similarity and membership function, according to the similarity, design
a new weighted factor, at the same time we study the effect of dimension selection to
V.N. Alexandrov et al. (Eds.): ICCS 2006, Part III, LNCS 3993, pp. 216 – 223, 2006.
© Springer-Verlag Berlin Heidelberg 2006

An Adaptive Fuzzy kNN Text Classifier

217

categorization performance. We design a formula of dimension selection. The experiment shows that our improvement is feasible.

2 The Classical kNN Algorithm Based on SWF Rule
At present, there are two main decision rules in kNN algorithm, that is, the discrete
value rule DVF (Discrete-Valued Function) and the weighted similarity rule SWF
(Similarity-Weighted Function). The most widely used is the SWF rule. This paper
mainly focuses on this rule. The kNN algorithm based on SWF rule can be described
as follows:
The system searches k documents (called neighbors) that have the maximal similarity (cosine similarity) in training sets. According to what classes these neighbors
are affiliated with, it grades the test document’s candidate classes. The similarity
between the neighbor document and the test document is taken as this class weight of
neighbor documents. The decision function can be defined as follows:
k

μ j ( X ) = ∑ μ j ( X i ) sim( X , X i )

(1)

i =1

Where
not (

μ j ( X i ) ∈ {0,1}shows whether X i belongs to ω j ( μ j ( X i ) = 1 is True) or
μ j ( X i ) = 0 is False); where ω j is the sort of document class;

sim( X , X i ) denotes the similarity between training document and test document.
Then the decision rule is: If μ j ( X ) = max μ i ( X ) , then X ∈ ω j .
i

3 The Improved kNN Decision Rule
In classical kNN algorithm, there is an obvious problem: when the density of training
data is uneven it may decrease the precision of classification if we only consider the
sequence of first k nearest neighbors but do not consider the differences of distances.
To solve this problem, we adopt the theory of fuzzy sets, constructing a new membership function based on document similarities as follows:
k

μ j (X ) =

∑μ
i =1

Where j=1, 2, …, c,

j

1
(1 − sim( X , X i )) 2 /(b−1)
k
1
∑
2 /( b −1)
i =1 (1 − sim ( X , X i ))

( X i ) sim( X , X i )

(2)

μ j ( X i ) sim( X , X i ) is the membership of known sample X to

class j. If sample X belongs to class j then the value is 1, otherwise 0. From this formula, we can see that in reality the membership is using the different distance of
every neighbor to the candidate classifying sample to weigh its effect. Parameter b is

218

W. Shang et al.

used to adjust the degree of a distance weight. From paper [6] we can know that the
best value field of b is between 1.5 and 2.5, always using 2, in this paper we take the
value 2. Then fuzzy k-nearest neighbors’ decision rule is: If μ j ( X ) = max μ i ( X ) ,
i

then X

∈ω j .

Why we amend formula (1) to formula (2), the reasons mainly are:
1) Similarity has relation to distance
A distance function or a similarity function is a general measurement of pattern recognition. In topology, a distance can be defined as [18]: suppose the space is Ω , x
+

and y are arbitrary two points in this space. Mapping d ( x, y ) : Ω × Ω → R is
called the two points’ distance, if the mapping satisfies three conditions as follows:
(1) d ( x, x ) = 0, ∀x ∈ Ω ;
(2) d ( x, y ) = d ( y , x), ∀x, y ∈ Ω ;
(3) d ( x, y ) ≤ d ( x, z ) + d ( z , y ), ∀x, y , z ∈ Ω .
If the d(x, y)’s value field is [0, 1], i.e., d ( x, y ) : Ω × Ω → [0,1] , then d(x, y) is

called unitary distance between x and y and is expressed by

d 0 ( x, y ) .

d 0 ( x, y ) to express similarity sim(x, y). Sim(x, y)
can be defined as follows [19]: f : d 0 ( x, y ) → sim( x, y ) ∈ [0,1] , where f must
We can use a unitary distance

satisfy the following conditions:
If d 0 ( x1 , y1 ) > d 0 ( x2 , y 2 ) , then sim( x1 , y1 ) ≤ sim( x 2 , y 2 ) ;
If d 0 ( x1 , y1 ) < d 0 ( x 2 , y 2 ) , then sim( x1 , y1 ) ≥ sim( x 2 , y 2 ) ;
If d 0 ( x1 , y1 ) = d 0 ( x2 , y 2 ) , then sim( x1 , y1 ) = sim( x2 , y 2 ) .
Through the above definition, we can educe the function relationship between a
distance and a similarity from the definition of distance, which is given independently; whereas we can educe another transfer relationship from the definition of distance, which is given by similarity. This is the internal relationship between a distance
and a similarity. The relationship between a unitary distance d 0 ( x, y ) and a similarity sim(x, y) of arbitrary two points in feature space
plementary function sim( x, y ) = 1 − d 0 ( x, y ) [19].

Ω can be described using com-

2) The similarity has relation to membership
The value field of a similarity and a membership is [0, 1]. This is not an occasion but
a result of consanguineous affiliation between them. Through the concept of existing
fields, we can relate a similarity with a membership. The existent field can be described as follows: around a point x in feature space Ω forms a field, for an arbitrary
point y in the space, the value of this field can be measured by the similarity sim(x, y)
that y towards x. Such a field is called an existing field. This field forms a fuzzy set

An Adaptive Fuzzy kNN Text Classifier

219

around the center point x, can be described as A. Any point in the field belongs to A.
Its membership can be measured by the similarity between this point and the center
point x. The nearer the distance is to x, the higher the possibility it belongs to A. Contrariwise it is not. The value of membership in the center point x is maximal, i.e., the
constant value 1. Hence, from the meaning of existing field, around a point x in feature space Ω , there exists a potential fuzzy set A, the similarity which any point y for
A in this set can be measured by the similarity between y and x, that is,
μ A ( y ) = sim( x, y ) [19].

4 The Research of Dimension Selection
In text categorization, dimension is defined as the number of the feature words in VSM
(Vector Space Model). For text documents, the dimension is always thousands upon
thousands. Such high dimension is not permitted by a classifier. This is so called curse
of dimensionality. Among this feature words, only a lot is useful for text categorization, many of them are noise words which hurt the performance of text categorization.
At present, there are many methods to reduce the dimension space. In text categorization, people always use feature selection method to reduce the dimension, such as
Information Gain, Cross Entropy, Mutual Information, CHI, Weight Evidence of
Text, Odds Ratio and so on. These methods can reduce the dimension space greatly.
But there still over thousands feature words after feature selection. So how many
dimensions to be selected are proper; how many dimensions to be selected are to
make the categorization performance best.
After we study other authors’ experiments in kNN and its variants and the research
of our own experiment, we find that when the classes and number of documents in
training set are certain we can approximately make sure the dimensions of selection.
It follows the formula as follows:

⎢ log(num(max(class )))⎦ ⎥
dim ensions = ⎢ ⎣
⎥ × 1000
⎣ ⎡ln(num(min(class )))⎤ ⎦

(3)

Where num(max(class)) is the document numbers of the maximum class in training
set, num(min(class)) is the document numbers of the minimum class in training set. If

⎢ ⎣log(num(max(class )))⎦ ⎥
⎢
⎥ is less than 1, we consider it as 1.
⎣ ⎡ln(num(min(class )))⎤ ⎦
5 Experiment
5.1 The Datasets
In this paper, we use two datasets to validate our algorithm. One dataset comes from
the International Database Center, Dept. of Computing and Information Technology,

220

W. Shang et al.

Fudan University, China. The other dataset is Reuters-21578. We select 15 classes
among 90 classes. The distribution of the class is uneven.
5.2 Experiment Result and Analysis
In experiment of Fudan’s dataset, the feature dimension is 2000; the step of k is 5. In
experiment of Reuters-21578, the feature dimension is 1000; the step of k is 5 too.
The experiment result can be described as Table 1 and Table 2:
Table 1. The categorization performance in Chinese dataset when k is different

Value k

kNN

fkNN

Macro-F1

Micro-F1

Macro-F1

Micro-F1

10

81.972

79.907

83.802

82.346

15
20
25
30
35
40
45
50

81.571
81.077
80.717
80.307
80.061
78.458
77.885
77.657

79.326
78.397
77.468
76.887
76.423
74.100
74.100
72.822

83.441
82.564
82.547
81.876
81.627
81.016
80.612
80.232

81.882
80.720
80.372
79.443
78.978
78.165
77.700
77.120

Table 2. The categorization performance in Reuters-21578 dataset when k is different

Value k

kNN

fkNN

10

Macro-F1
67.000

Micro-F1
86.664

Macro-F1
68.529

Micro-F1
86.483

15
20
25
30
35
40
45
50

66.855
66.715
66.942
66.540
65.738
65.033
64.446
60.739

86.628
86.737
85.519
86.337
86.374
86.374
86.337
86.265

69.472
69.179
69.560
69.499
68.300
68.050
68.364
67.827

86.991
86.919
87.028
86.882
86.810
86.919
87.100
86.919

From Table 1 and Table 2, we can find that the improved kNN algorithm (fkNN)
show better categorization performance than kNN no matter what the dataset is Chinese data or Reuters-21578. It has about 3% improvement in average performance.
This proves that our improvement in decision rule is effective and feasible. This
method solves the uneven problem of class distribution better.

An Adaptive Fuzzy kNN Text Classifier

221

Table 3. The categorization performance in Chinese dataset when dimension is different. Note
that the highest accuracy is highlighted with bold font

dimension
1000
2000
3000
4000
5000
6000
7000
8000

kNN
Macro-F1
80.207
77.735
78.219
76.150
76.396
74.671
69.258
76.150

fkNN
Micro-F1
76.655
73.287
73.287
70.151
70.267
67.596
60.163
70.151

Macro-F1
82.133
80.612
81.931
80.155
79.306
77.440
74.595
70.527

Micro-F1
79.791
77.700
79.210
76.423
74.913
72.358
67.712
62.137

Table 4. The categorization performance in Reuters-21578 dataset when dimension is different.
Note that the highest accuracy is highlighted with bold font

dimension

kNN

1000

Macro-F1
64.446

2000

fkNN

64.473

Micro-F1
86.337
84.375

Macro-F1
68.364
67.773

Micro-F1
87.100
84.847

3000

63.525

84.012

67.675

84.375

4000
5000
6000
7000
8000
9000
10000

63.977
63.177
63.250
63.412
64.030
63.209
62.829

83.830
83.539
83.503
83.612
83.576
83.321
83.031

65.696
65.755
65.728
65.623
65.853
65.751
65.896

84.230
84.230
84.048
84.121
83.975
83.939
83.830

Note that in Table 3 and Table 4, k takes 45. From Table 5, we can see that when
dimension is 1000, the categorization performance of kNN and fkNN reaches the
best. This result is very accordant with our dimension design formula:

⎢ ⎣log(619)⎦ ⎥
⎢
⎥ × 1000 =1×1000=1000. From Table 6, we can find that when dimen⎣ ⎡ln(59) ⎤ ⎦
sion is 1000, kNN and fkNN’s categorization performance is the best. This result is
consistent with our dimension formula design too. We use our dimension formula
design in other author’s kNN and its variant experiment [13][14][21]. It has the same
good result of fitting our dimension formula. Even though it can not get the best point,
it must be the better point. The experiment result shows that this dimension formula
design is effective and feasible.

222

W. Shang et al.

6 Conclusion
In this paper, we mainly discuss the improvement of decision rule and design a new
algorithm of fkNN (fuzz kNN) to improve categorization performance when the class
distribution is uneven. Based on this, we study the selection of dimensions and design
a dimension selection formula. The experiment proves that our method is effective.
In the future, we need to study further on how to select the k; the impact of value k
to dimension selection; how to improve the decision rule further, what their effects to
be on each other and so on.

Acknowledgment
This research is partly supported by Beijing Jiaotong University Science Foundation
under the grant 2004RC008.

References
1. Cover, T. M., Hart P. E.: Nearest neighbor pattern classification. IEEE Transaction on Information Theory. Vol. IT-13 (1967) 21-27, 1967
2. Yang, Y.: An Evaluation of Statistical Approaches to Text Categorization. Information
Retrieval. Vol. 1 (1997) 76-88
3. Yang, Y., Lin X.: A Re-examination of Text Categorization Methods. In: Proc. of the 22nd
Annual International ACM SIGIR Conference on Research and Development in the Information Retrieval. ACM Press, New York (1999) 42-49
4. Masand, B., Lino, G., Waltz, D.: Classifying news stories using memory based reasoning.
In: 15th Ann Int ACM SIGIR Conference on Research and Development in Information
Retrieval. Copenhagen (1992) 59-64
5. Lewis, D. D.: Naïve (Bayes) at forty: the independence assumption in information retrieval. In: Proc. of the 10th European Conference on Machine Learning. Chemnitz (1998)
4-15
6. Mccallum, A., Nigam K.: A comparison of event models for naïve bayes text classification. In: AAAI-98 Workshop on Learning for Text Categorization. Madison, Wisconsin
(1998) 41-48
7. Lewis, D. D., Ringuette M.: Comparison of two learning algorithms for text categorization. In: Proc. of the Third Annual Symposium on Document Analysis and Information
Retrieval. Las Vegas (1994) 81-93
8. Apte, C., Damerau, F., Weiss, S.: Text mining with decision rules and decision trees. In:
Proc. of the Conference on Automated Learning and Discovery, Workshop 6: Learning
from Text and the Web. CMU (1998) 487-499
9. Joachims, T.: Text categorization with support vector machines: learning with many relevant features. In: Proc. of the 10th European Conference on Machine Learning. Chemnitz
(1998) 137-142
10. Yang, Y., Chute, C. G.: An example-based mapping method for text categorization and retrieval. ACM Transaction on Information System. Vol. 12 (1994) 252-277
11. Ng, H. T., Goh, W. B., Low, K. L.: Feature selection, perceptron learning, and a usability
case study for text categorization. In: 20th Ann Int ACM SIGIR Conference on Research
and Development in Information Retrieval. (1997) 67-73

An Adaptive Fuzzy kNN Text Classifier

223

12. Wiener, E., Pedersen, J. O., Weigend, A. S.: A neural network approach to topic spotting.
In: Proc. of the 4th Annual Symposium on Document Analysis and Information Retrieval.
(1995) 317-332
13. Tan, S.: Neighbor-weighted K-nearest neighbor for unbalanced text corpus. Expert Systems with Application. Vol. 28 (2005) 667-671
14. Han, E., Karypis, G., Kumar, V.: Text Categorization Using Weight Adjusted k-Nearest
Neighbor Classification. In: Proc. of 5th Pacific-Asia Conference on Advances in Knowledge Discovery and Data Mining. (2001) 53-66
15. Shankar, S., Karpis, G.: A Feature Weight Adjustment Algorithm for Document Categorization. In: Proc. of the International Workshop on Multimedia Data Mining. (2000)
16. Li, B., Lu, Q., Yu, S.: An Adaptive k-Nearest Neighbor Text Categorization Strategy.
ACM Transactions on Asian Language Information Processing. Vol. 3 (2004) 215-226
17. Lim, H.: An Improved KNN Learning Based Korean Text Classifier with Heuristic Information. In: Proc. of the 9th International Conference on Neural Information Processing.
(2202) 731-735
18. Dubois, D., Prade, H.: Fuzzy sets and systems (Theory and application). Oxford, Uk, Academic Press. (1980)
19. Zhao, S.: The method of fuzzy mathematics in pattern recognition. School of the WestNorth Electronic Engineering Press, Xi’an. (1987)
20. Bian, J., Zhang, X.: Pattern recognition. Tsinghua University Press, Beijing (2000)
21. Cardoso-Cachopo, A., Oliveira, A. L.: An empirical comparison of text categorization
methods. In Proceedings of the 10th International Symposium on String Processing and Information Retrieval (SPIRE'03), number 2857 in Lecture Notes in Computer Science,
Springer Verlag. (2003) 183-196

