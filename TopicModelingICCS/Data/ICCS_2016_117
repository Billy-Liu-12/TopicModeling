Procedia Computer Science
Volume 80, 2016, Pages 2181–2189
ICCS 2016. The International Conference on Computational
Science

Algorithmic Approach for Learning a Comprehensive
View of Online Users
Kourosh Modarresi
Adobe Inc., San Jose, U.S.
kouroshm@alumni.stanford.edu

Abstract
Online users may use many different channels, devices and venues for any online user experience. To
make all services such as web design, ads, web content, shopping, personalized for every user; we need
to be able to recognize them regardless of device, channels and venues they are using. This, in turn,
requires building up a comprehensive view of the user which includes all of their behavioral
characteristics - that are spread all over these different venues. This would not be possible without
having all behavioral related data of the user which requires the capacity of connecting the user all over
the devices, and channels, so to have all of their behavior under a single view. This work is a major
attempt in doing this using only behavioral data of users while protecting the user’s privacy.
Keywords: Matrix Completion, Regularization, Matrix Reconstruction, Singular Value Decomposition, User
Behavior Data

1 Introduction
Creating an ideal digital users’ experience has to be based on understanding the users and so creating
an echo environment that is favorable to them. This way, the users would have a desirable online
experience. One could expect a great digital experience may lead to a maximum return on the emarketing different metrics such as increase in usage, purchase and so on. Often, the basis of the study
of the users’ preferences is on tracking cookies, login parameters - such as email address, userid and so
on. Tracking users through cookies is not very reliable method in gaining a comprehensive view of the
users. One major issue about tracking by cookies is that users could delete them besides the fact that
cookies cannot recognize the same users as they may use different devices or digital venues and
channels. Deploying login credentials has only limited effects since users may not user any credentials
in their online journeys. Also, tracking IP address has the major limitation of being restricted to only
Selection and peer-review under responsibility of the Scientiﬁc Programme Committee of ICCS 2016
c The Authors. Published by Elsevier B.V.

doi:10.1016/j.procs.2016.05.378

2181

Algorithmic Approach for Learning a Comprehensive View of Online Users

Kourosh Modarresi

one device as the IP address changes from one device to another. Those are some of the reasons
supporting the use of behavior data for user identification for this work. Users behavior will be recorded
regardless of whether they use any login credentials, or what device they are using and is also
independent of whether any cookies are used to track them. One of the major challenges in user
recognition based on behavioral data is that user behavior is distributed among all devices (such as
phone, desktop and so on), digital venues (email, browser, …) and channels (search engine, commercial
websites, .). Users may not be fully understood if we look at only a fraction of their interactional data
with the digital space and thus to get a comprehensive view of a user, we need to collect all behavioral
data the users have left on all these dispersed locations. To do this, i.e., to gather all these characteristics
of the users - from all these channels, devices and venues – we need to find a way to recognize the user
cross all these devices, channels and venues. This would suggest that this process is an iterative problem
and as we could recognize the same users – no matter where they are engaging the digital sphere –
enabling us to bring together more behavioral data from all points, we get a better understanding of the
user through all this collected data and thus we could identify the same user across all the channels more
effectively.

2 The Model in Recognizing the Users
This work uses only behavioral data and assumes no access to private information of users, such as
name, last name, email address, SSN, DLN, login information. Also, no information from cookies is
used. The model is based on the following steps;
(A) Centering the Data Matrix
(B) Transformation of the Data Matrix X to a Higher Dimensional Space
(C)
(D)
(E)
(F)
(G)
(H)

Computing SVD (Singular Value Decomposition)
Completing the Matrix
Computing the Best Rank-k Matrix for the Matrix X
Updating the Original Matrix
Projecting the New Row onto the k-dimensional Space (SVD Updating)
Computing the New Distance and Detecting a Possible Match

2.1 The Format and Representation of User’s Behavioral Data
The user data entails all possible information of the interactions users have in the digital space.
Matrix representation of data is used in this work and so, the data is represented in the matrix form,
called matrix X. Rows of the matrix are users (i) and its columns (j) are different features that
represent users’ online interactions. The columns could include interaction variables such as “browser
type”, “operating system type”, “url”, “the time of interaction” and “length of interaction”. Each
matrix entry - ‫ݔ‬௜௝ – displays the specific feature of user i on interaction behavior j. Thus each entry
2182

Algorithmic Approach for Learning a Comprehensive View of Online Users

Kourosh Modarresi

(‫ݔ‬௜௝ ) is the relation between a specific interactive variable j and a specific user i. The data matrix, X
has m rows and n columns (m × n).
In general, the entries of the matrix X may be explicit such as the browser, rating and total purchases,
or they may be implicit data such as like/not like of specific content derived from some explicit data.
Explicit data is not always available and often not enough of that could be found [11,12,15]. Historic
(logged) data or live data (streaming) data and quite often a combination of both are used to design
and test the user recognition model.

2.2 Centering the Data Matrix
The data matrix contains a variety of data types that have different range and scales. The matrix also
includes non-numerical data that may be transformed to numeric data using dummy variables. All types
of data need to be centered to prevent domination of some variables (columns) as a results of their larger
range. The process of centering [ 85] is done by reducing the mean of each column from all the column
entries,
ଵ

ܺ = ܺ( ‫ܫ‬௡ െ 11௧ )
௡

Where I am the identity matrix and 1 is a column vector with all entries to be one.

2.3 Transformation of the Data Matrix X to a Higher Dimensional Space
The data matrix X has n variables (columns). In general, for a unique definition and identification of
each user, this number of dimensions may not be sufficient. Thus, to make the users separable and
distinct from each other, the data matrix is transformed to a higher dimensional space. The number of
dimensions of the new space will be computed so that every user is defined and identified uniquely.
Modern data possesses many characteristics such as high correlation. To make the transformation of the
data matrix X, we use new coordinates of n+1 to n+p, where p is the required number of added features
so we could identify all users uniquely.
௡ା௣

‫ݔ‬௪ =

෍ ܽ௟ ‫ݔ‬௟ + ‫א‬௟
௟ୀ௡

Where ‫א‬௟ is random white noise and w = n+1:n+p .

2183

Algorithmic Approach for Learning a Comprehensive View of Online Users

Kourosh Modarresi

2.4 Computing SVD
For this work the singular value decomposition (SVD) of the matrix X is defined the usual way [33],
as;

ܺ = ܷ‫ ܸܦ‬௧
Where: U, the left singular vectors, is m×n orthogonal matrix,
ܷܷ ௧ = ܷ ௧ ܷ = ‫ܫ‬
V, the right singular vectors, is n×n orthogonal matrix
ܸܸ ௧ = ܸ ௧ ܸ = ‫ܫ‬
and D = diag (݀ଵ , ݀ଶ , … , ݀௡ ) with the singular vectors;
݀ଵ ൒ ݀ଶ ൒ ‫ ڮ‬. ൒ ݀௡ ൒ 0
Another view of the computation of SVD is by using minimum reconstruction error,
min ฮܺ െ ܷ௤ ‫ܦ‬௤ ܸ௤ ฮ
Which could be rewritten as,
argminԡܺ െ ܺ‫ݑݒ‬௧ ԡଶଶ
(௨,௩,ௗ)

Equivalently,
min ෍[‫ݔ‬௜௝ െ ‫ݑ‬௜ ݀௜ ‫ݒ‬௝ ் ]ଶ

௎, ஽,௏

2.5 Completing the Matrix
Modern data is often very sparse [ 58] and the data under study in this work is even more sparse.
That is due to the fact that for every user entry (row) of the data matrix X, even when coming from the
same channel or device, there are many missing (unknown) entries. This is due to the fact the number
of variables is large and it is unlikely that user’s interaction involves all different variables. Since the
columns of data matrix X is a combination of all variables from all possible devices and entries and
although these channels and devices have many similar and overlapping features (columns), still each
of these venues have many unique variables that are not shared by many other venues. As a result of
these two factors, i.e., the missing values for each channel and the uniqueness of many variables of these
channels, each row contains many unknown (missing) entries.
At this step, the unknown entries of the data matrix are computed using an iterative SVD algorithm
[ 75,76]. In short, at the first step, the missing entries in each column of X are replaced with the median
of the column. Then, SVD is computed for the matrix X. The third step involves the computation of the
best k-rank matrix (k<< min (m, n)). The forth step requires the reconstruction of X using the low rank
(k-rank) singular vectors and singular values matrix. Then, the newly computed values of the missing
entries are compared with the values from the previous iteration. All of the above steps are repeated
until convergence. The summary of the algorithm in this part is;
2184

Algorithmic Approach for Learning a Comprehensive View of Online Users

Kourosh Modarresi

For a centered X:
Step (1) compute the
݉݅݊
ถ ฮܺ െ ܷ௤ ‫ܦ‬௤ ܸ௤ ฮ
௎೜ ,௏೜ ,஽೜

Where q is computed using a threshold of variation of the original data in X.
Step (2) computes the new X – of q-rank
ܺ௤ = ܷ௤ ‫ܦ‬௤ ܸ௤
Using newly computed ܺ‫ݍ‬, we have new values for the missing entries.
Step (3) Iterate steps (1) and (2) till convergence,
‫݅( ݍܺۅ‬íܺ‫ߜ ۅ)݅( ݍܺۅۅ݅( ݍ‬
For small ߜ.

2.6 Computing the Best Rank-k Matrix for the Matrix X
At this step, using the results in the previous sections, we need to compute the best low dimensional
space that we should project matrix X onto. Matching of any new user will take place in this new space.
This new space has k- dimensions (section 2.5). The computation of k is based on the preservation of a
minimum threshold of the original information (variation) of the data. The determination of a specific
threshold depends on many considerations including the specific applications, the required accuracy and
the computational cost.

2.7 Updating the Original Matrix
For any incoming user for whom the identity is unknown, update the original matrix by adding the
new row and do all steps of 2.2-6.

2.8 Projecting the New Row onto the k-dimensional Space (SVD
Updating)
Project the new row in SVD space to get the new coordinates of the new data point.
For any new entry‫ݔ‬௠ାଵ :
(1) Compute the missing entry (section 2.5) to get,‫ݔ‬௡௘௪ .
(2) Compute its projection in the new (low dimensional) space;
2185

Algorithmic Approach for Learning a Comprehensive View of Online Users

Kourosh Modarresi

‫ݑ‬௡௘௪ = ‫ݔ‬௡௘௪ * V

2.9 Computing the New Distance and Detecting a Possible Match
For the new row in U, ‫ݑ‬௡௘௪ , compute the distance between the new row (vector) with all of the
previous rows in the matrix U (left singular vector of X). In other words, compute the closest user in the
space to the new one, in the sense of Euclidean (2nd) norm.
‫݊݅ܯ‬
ถ ‫ݑۅ‬௡௘௪ í‫ݑ‬௜ ‫ݑۅۅ‬௜ ‫ۅ‬İ
௜

For a small İ If the equation condition is not satisfied i.e., if there is no user satisfying the inequality
then there is no match with the known users and the ‫ݔ‬௡௘௪ is flagged as a new row (new user) in the data
matrix X.

2.10 SAMPLE OF RESULTS
Different data set is used to test this model. For all different examples of data matrices, k-fold cross
validation is used to validate the model. The model was applied on X=200,000 by 234 data matrix.
The obtained accuracy was 92%.

References
[1]

[2]
[3]
[4]
[5]
[6]
[7]

[8]
[9]
[10]
[11]

G. Adomavicius and A. Tuzhilin, “Towards the next generation of recommender systems: a survey
of the state-of-the-art and possible extensions,” IEEE Trans. on Data and Knowledge Engineering
17:6, pp. 734–749, 2005.
L. Backstrom, J. Leskovec, “Supervised Random Walks: Predicting and Recommending Links in
J. Baumeister, “Stable Solution of Inverse Problems”, Vieweg, Braunschweig, Germany, 1987.
S. Becker, J. Bobin, and E. J. Candès. NESTA,” a fast and accurate first-order method for sparse
recovery,” SIAM J. on Imaging Sciences 4(1), 1-39, 2009.
A. Bjorck, “Numerical Methods for Least Squares Problems” ,SIAM, Philadelphia,1996.
S. Boyd and L. Vandenberghe, “Convex Optimization”, Cambridge University Press, 2004.
J.S. Breese, D. Heckerman, and C. Kadie, “Emperical analysis of predictive algorithms for
collaborative filtering,” Proceedings of Fourteenth Conference on Uncertainty in Artificial
Intelligence. Morgan Kaufmann, 1998.
P. A. Businger, G. H. Golub, “Singular value decomposition of a complex Matrix”, Algorithm 358,
Comm. Acm, No. 12, pp. 564-565, 1969.
J. Cadima and I. T. Jolliffe, “ Loadings and correlations in the interpretation of principal
components”, Journal of Applied Statistics, 22:203–214, 1995.
J-F Cai, E. J. Candès and Z. Shen, “A singular value thresholding algorithm for matrix completion,”
SIAM J. on Optimization 20(4), 1956-1982, 2008.
E. J. Candès and Y. Plan, “Matrix completion with noise,” Proceedings of the IEEE 98(6), 925936, 2009.

2186

Algorithmic Approach for Learning a Comprehensive View of Online Users
[12]

[13]
[14]
[15]
[16]
[17]

[18]
[19]
[20]

[21]
[22]
[23]
[24]
[25]
[26]
[27]
[28]
[29]
[30]
[31]
[32]
[33]
[34]
[35]
[36]
[37]

Kourosh Modarresi

E. J. Candès and Y. Plan, “Tight oracle bounds for low-rank matrix recovery from a minimal
number of random measurements,” IEEE Transactions on Information Theory 57(4), 2342-2359,
2009.
E. J. Cand`es and T. Tao, “Decoding by linear programming”, IEEE Transactions on Information
Theory, 51(12):4203–4215, 2005.
E. J. Candès and B. Recht, “Exact matrix completion via convex optimization,” Found. of Comput.
Math., 9 717-772, 2008.
E. J. Candès, “Compressive sampling,” Proceedings of the International Congress of
Mathematicians, Madrid, Spain, 2006.
E. J. Candès and T. Tao, “Near-optimal signal recovery from random projections: universal
encoding strategies,” IEEE Trans. Inform. Theory, 52 5406-5425, 2004.
Claypool, M., Gokhale, A., Miranda, T., Murnikov, P., Netes, D., and Sartin M., "Combining
content-based and collaborative filters in an online newspaper," Proceedings of the ACM SIGIR’99
Workshop on Recommender Systems, 1999.
R. Courant and D. Hilbert, “Methods of Mathematical Physics”, Vol. II, Interscience, New York,
1953.
A. d’Aspremont, L. El Ghaoui, M.I. Jordan, and G. R. G. Lanckriet, “A direct formulation for sparse
PCA using semidefinite programming”, SIAM Review, 49(3):434–448, 2007.
A. R. Davies and M. F. Hassan, “Optimality in the regularization of ill-posed inverse problems”, in
P. C. Sabatier (Ed.), Inverse Problems: An interdisciplinary study, Academic Press, London, UK,
1987.
B. DeMoor, G. H. Golub, “The restricted singular value decomposition: properties and
applications”, SIAM J. Matrix Anal. Appl., 12, No. 3, pp. 401-425, 1991.
D. L. Donoho and J. Tanner, “ Sparse nonnegative solutions of underdetermined linear equations
by linear programming”, Proc. of the National Academy of Sciences, 102(27):9446–9451, 2005.
Efron, B., Hastie, T., Johnstone, I., and Tibshirani, R., “Least Angle Regression,” The Annals of
Statistics, 32, 407–499, 2004.
Lars Elden, “Algorithms for the Regularization of Ill-Conditioned Least Squares Problems”, BIT
17, pp. 134-145, 1977.
Lars Elden, “A Note on the Computation of the Generalized Cross-Validation Function for IllConditioned Least Squares Problems”, BIT 24, pp. 467-472, 1984.
Heinz. W. Engl, M. Hanke, and A. Neubauer, “Regularization methods for the stable solution of
inverse problems” , Surv. Math. Ind., No. 3, pp. 71-143, 1993.
H. W. Engl, M. Hanke, and A. Neubauer, “Regularization of Inverse Problems”, Kluwer,
Dordrecht, 1996.
H. W. Engl, K. Kunisch, and A. Neubauer, “Convergence rates for Tikhonov regulari- sation of
non-linear ill-posed problems” , Inverse Problems, (5), pp. 523-540, 1998.
H. W. Engl , C. W. Groetsch (Eds), “Inverse and Ill-Posed Problems”, Academic Press, London,
1987.
M. Fazel, H. Hindi, and S. Boyd. “A rank minimization heuristic with application to minimum order
system approximation”, Proceedings American Control Conference, 6:4734–4739, 2001.
W. Gander, “On the linear least squares problem with a quadratic Constraint”, Technical report
STAN-CS-78-697, Stanford University, 1978.
G. H. Golub, C. F. Van Loan, “Matrix Computations”, 4th Ed., Computer Assisted Mechanics and
Engineering Sciences, Johns Hopkins University Press, US, 2013.
Gene H. Golub, Charles F. Van Loan, “An Analysis of the Total Least Squares Problem”, Siam J.
Numer. Anal., No. 17, pp. 883-893, 1980.
Gene H. Golub, W. Kahan, “Calculating the Singular Values and Pseudo-Inverse of a Matrix”,
SIAM J. Numer. Anal. Ser. B 2, pp. 205-224, 1965.
Gene H. Golub, Michael Heath, Grace Wahba, “Generalized Cross-Validation as a Method for
Choosing a Good Ridge Parameter”, Technometrics 21, pp. 215-223, 1979.
Hastie, T., Tibshirani, R., and Friedman, J. ,” The Elements of Statistical Learning; Data mining,
Inference and Prediction”, New York: Springer Verlag, 2001.
Hastie, T.J and Tibshirani, R. "Handwritten Digit Recognition via Deformable Prototypes", AT&T
Bell Laboratories Technical Report, 1994.

2187

Algorithmic Approach for Learning a Comprehensive View of Online Users
[38]

[39]

[40]
[41]
[42]
[43]

[44]
[45]
[46]
[47]
[48]
[49]
[50]
[51]
[52]
[53]
[54]
[55]
[56]

[57]
[58]
[59]

[60]
[61]
[62]
[63]
[64]

Kourosh Modarresi

Hastie, T., Tibshirani, R., Eisen, M., Brown, P., Ross, D., Scherf, U., Weinstein, J., Alizadeh, A.,
Staudt, L., and Botstein, D., “ ‘Gene Shaving’ as a Method for Identifying Distinct Sets of Genes
With Similar Expression Patterns,” Genome Biology, 1, 1–21, 2000.
David Heckerman, David Maxwell Chickering, Christopher Meek, Robert Rounthwaite, and Carl
Kadie, “Dependency networks for inference, collaborative filtering, and data visualization,” Journal
of Machine Learning Research, 1:49–75, 2000.
T. Hein, “Some analysis of Tikhonov regularization for the inverse problem of option pricing in the
price-dependent case,” ,SIAM Review, (21)No. 1, pp. 100-111, 1979.
T. Hein and B. Hofmann, “On the nature of ill-posedness of an inverse problem in option pricing,”
,Inverse Problems,(19), pp. 1319-11338, 2003.
B. Hofmann, “Regularization for Applied Inverse and Ill-Posed problems ,” Teubner, Stuttgart,
Germany, 1986.
B. Hofmann, “Regularization of nonlinear problems and the degree of illposedness,” in G. Anger,
R. Goreno, H. Jochmann, H. Moritz, and W.
Webers (Eds.), inverse Problems: principles and Applications in Geophysics,Technology, and
Medicine, Akademic Verlag, Berlin, 1993.
T. A. Hua and R. F. Gunst, “Generalized ridge regression: A note on negative ridge parameters,”
Comm. Statist. Theory Methods, 12, pp. 37-45, 1983.
V. K. Ivankov, “On linear problems which are not well-posed ,” Dokl. Akad. Nauk SSSR, 145, pp.
270-272, 1962.
Jeffers, J., “Two Case Studies in the Application of Principal Component,” Applied Statistics, 16,
225–236, 1967.
Jolliffe, I. , Principal Component Analysis, New York: Springer Verlag, 1986.
I. T. Jolliffe, “Rotation of principal components: choice of normalization Constraints,” Journal of
Applied Statistics, 22:29–35, 1995.
I. T. Jolliffe, N.T. Trendafilov, and M. Uddin, “A modified principal component technique based
on the LASSO,” Journal of Computational and Graphical Statistics, 12:531–547, 2003.
Misha E. Kilmer and Dianne P. OLeary, “Choosing regularization parameters in iterative methods
for ill-posed problems,” SIAM J. MATRIX ANAL. APPL., Vol. 22, No. 4, pp. 1204-1221. 2001.
Andreas kirsch, “An Introduction to the Mathematical theory of Inverse problems ,” Springer
Verlag, New York, 1996.
Mardia, K., Kent, J., and Bibby, J., “Multivariate Analysis,” New York: Academic Press, 1979.
G. Linden, B. Smith, and J. York, “Amazon.com recommendations: item-to-item collaborative
filtering,” Internet Computing 7:1, pp. 76–80, 2003.
Rahul Mazumder, Trevor Hastie and Rob Tibshirani, “Spectral Regularization Algorithms for
Learning Large Incomplete Matrices,” JMLR 2010 11 2287-2322, 2010.
McCabe, G., “Principal Variables,” Technometrics, 26, 137–144, 1984.
Kourosh Modarresi and Gene H Golub, “An Adaptive Solution of Linear Inverse Problems”,
Proceedings of Inverse Problems Design and Optimization Symposium (IPDO2007), April 16-18,
Miami Beach, Florida, pp. 333-340, 2007.
Kourosh Modarresi, “A Local Regularization Method Using Multiple Regularization Levels”,
Stanford, CA, April 2007.
Kourosh Modarresi, “Computation of Recommender System Using Localized Regularization”,
Procedia Computer Science, Volume 51, 2015, Pages 2407–2416.
Kourosh Modarresi and Gene H Golub, “An Efficient Algorithm for the Determination of Multiple
Regularization Parameters,” Proceedings of Inverse Problems Design and Optimization
Symposium (IPDO), April 16-18, 2007, Miami Beach, Florida, pp. 395-402, 2007.
D. W. Marquardt, “Generalized inverses, ridge regression, biased linear estimation,” and nonlinear
estimation, Technometrics, 12, pp. 591-612, 1970.
K. Miller, “Least Squares Methods for Ill-Posed Problems with a prescribed bond,” SIAM J. Math.
Anal., No. 1, pp. 52-74, 1970.
B. Moghaddam, Y. Weiss, and S. Avidan, “Spectral bounds for sparse PCA: exact and greedy
algorithms,” Advances in Neural Information Processing Systems, 18, 2006.
V. A. Morozov, “On the solution of functional equations by the method of regularization”,Sov.
Math. Dokl., 7, pp. 414-417, 1966.
V. A. Morozov, “Methods for Solving Incorrectly Posed Problems, “ Springer-Verlag, New York,
1984.

2188

Algorithmic Approach for Learning a Comprehensive View of Online Users
[65]
[66]
[67]

[68]
[69]
[70]
[71]
[72]

[73]
[74]
[75]
[76]
[77]

[78]
[79]
[80]
[81]
[82]
[83]

[84]
[85]

Kourosh Modarresi

A. Narayanan, V. Shmatikov, “Robust de-anonymization of large sparse datasets,” IEEE
Symposium on Security and Privacy, 2008, 111-125.
B. K. Natarajan, “Sparse approximate solutions to linear systems,” SIAM J. Comput., 24(2):227–
234, 1995.
R. Otazo, E. J. Candès and D. Sodickson, “Low-rank and sparse matrix decomposition for
accelerated dynamic MRI with separation of background and dynamic components,” To appear in
Magnetic Resonance in Medicine, 2013.
R. L. Parker , “Understanding inverse theory,” Ann. Rev. Earth Planet. Sci., No. 5, pp. 35-64, 1977.
T. Raus, “The principle of the residual in the solution of ill-posed problems with nonselfadjoint
operator,” Uchen. Zap. Tartu Gos. Univ., 75, pp. 12-20, 1985.
T. Reginska, “A Regularization Parameter in Discrete Ill-Posed Problems,” SIAM J. Sci. Comput.,
No. 17, pp. 740-749, 1996.
F. Ricci, L. Rokach, B. Shapira. P.B. Kantor (Eds.), “Recommender Systems Handbook,” Springer,
New York, NY, USA, 2011.
E. Sadikov, M. Medina, J. Leskovec, H. Garcia-Molina.,“Correcting for Missing Data in
Information Cascades,” ACM International Conference on Web Search and Data Mining (WSDM),
2011.
A. Tarantola and B. Valette , “Generalized nonlinear inverse problems solved using the least
squares criterion,” Reviews of Geophysics and Space Physics, No. 20, pp. 219-232 , 1993.
A. Tarantola, “Inverse Problem Theory, Elsevir, Amsterdam ,” 1987.
Tibshirani, R., “Regression Shrinkage and Selection via the Lasso,” Journal of the Royal Statistical
Society, Series B, 58, 267–288, 1996.
R. Tibshirani, “Regression shrinkage and selection via the LASSO,” Journal of the Royal statistical
society, series B, 58(1):267–288, 1996.
A. N. Tikhonov, “Solution of Incorectly Formulated Problems and the
Regularization Method,” Soviet Math. Dokl., 4(1963), pp. 1035-1038; English translation of Dokl.
Akad. Nauk. SSSR, 151(1963), pp. 501-504, 1963.
A. N. Tikhonov, “Regularization of incorrectly posed problems,” Dokl. Akad. Nauk. SSSSR, 153,
(1963), pp. 49-52= Soviet Math. Dokl., 4, 1963.
A. N. Tikhonov, V. Y. Arsenin, “Solutions of Ill-Posed Problems,” Winston, Washington,
D.C.(1977).
A. N. Tikhonov, A. V. Goncharsky(Eds), “Ill-Posed Problems in the Natural Sciences,”,MIR,
Moscow, 1987.
A. N. Tikhonov, A. V. Goncharsky, V. V. Stepanov, A. G. Yagola, “Numerical Methods for the
Solution of Ill-Posed Problems,” Kluwer, Dordrecht, the Netherlands, 1995.
R. Witten and E. J. Candès, “ Randomized algorithms for low-rank matrix factorizations: sharp
performance bounds,” To appear in Algorithmica, 2013.
Z. Zhang, H. Zha, and H. Simon, “Low rank approximations with sparse
factors I: basic algorithms and error analysis,” SIAM journal on matrix
analysis and its applications, 23(3):706–727, 2002.
Z Zhou, J. Wright, X. Li, E. J. Candès and Y. Ma, “Stable Principal Component Pursuit,”
Proceedings of International Symposium on Information Theory, June 2010.
H. Zou, T. Hastie, and R. Tibshirani, “Sparse Principal Component Analysis,” Journal of
Computational & Graphical Statistics, 15(2):265–286, 2006.

2189

