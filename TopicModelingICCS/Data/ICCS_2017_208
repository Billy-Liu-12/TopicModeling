Available online at www.sciencedirect.com

ScienceDirect
Procedia Computer Science 108C (2017) 1803–1810
This space
is reserved for the Procedia header, do not use it

This space is reserved for the Procedia header, do not use it
This space is reserved for the Procedia header, do not use it

International Conference on Computational Science, ICCS 2017, 12-14 June 2017,
Zurich, Switzerland

Parallel Monte Carlo on Intel MIC Architecture
Parallel Monte Carlo on Intel MIC Architecture
Emanouil
Atanassov,
TodorCarlo
Gurov, on
Sofiya
Ivanovska,
Aneta Karaivanova
Parallel
Monte
Intel
MIC and
Architecture
Emanouil
Atanassov, Todor Gurov, Sofiya Ivanovska, and Aneta Karaivanova
Institute for Information and Communication Technologies, Bulgarian Academy of Sciences
Emanouil Atanassov,
Gurov,
Sofiya
Ivanovska,
Aneta Karaivanova
Acad. Todor
G. Bonchev
str., Block
25A,
Sofia 1113, and
Bulgaria
Institute for Information and Communication Technologies, Bulgarian Academy of Sciences
(emanouil,gurov,sofia,anet)@parallel.bas.bg
Acad.
G.
Bonchev
str., BlockTechnologies,
25A, Sofia 1113,
Bulgaria
Institute for Information
and
Communication
Bulgarian
Academy of Sciences
(emanouil,gurov,sofia,anet)@parallel.bas.bg
Acad. G. Bonchev str., Block 25A, Sofia 1113, Bulgaria
(emanouil,gurov,sofia,anet)@parallel.bas.bg

Abstract
Trade-off
Abstractbetween the cost-efficiency of powerful computational accelerators and the increasing
energy needed
to perform numerical tasks can be tackled by implementation of algorithms on
Trade-off
Abstractbetween the cost-efficiency of powerful computational accelerators and the increasing
the Intel
Multiple
Integrated
Cores tasks
(MIC)can
architecture.
The
best performance
of the algoenergy
needed
to perform
numerical
becomputational
tackled by
implementation
of the
algorithms
on
Trade-off between
the cost-efficiency
of powerful
accelerators and
increasing
rithms
requires
the Integrated
use of appropriate
optimization
and parallelization
approachesof
throughout
the
Intel
Multiple
Cores
(MIC)
architecture.
The
best
performance
the
algoenergy needed to perform numerical tasks can be tackled by implementation of algorithms on
all process
of their
design.
Monte Carlo
methods and
Carlo
methodsthroughout
depend on
rithms
requires
the Integrated
use of appropriate
optimization
andQuasi-Monte
parallelization
approaches
the Intel
Multiple
Cores (MIC)
architecture.
The best performance
of the algoa huge
number
of computational
cores.
Inmethods
this paper
weQuasi-Monte
present the advances
in our depend
studies on
all
process
of
their
design.
Monte
Carlo
and
Carlo
methods
on
rithms requires the use of appropriate optimization and parallelization approaches throughout
the
performance
ofcomputational
algorithms forcores.
solving
multidimensional
integrals
on Intel MIC
architecture
a
huge
number
of
In
this
paper
we
present
the
advances
in
our
studies
on
all process of their design. Monte Carlo methods and Quasi-Monte Carlo methods depend on
and performance
their comparison
with the performance
of Monte Carlo methods.
TheIntel
fastMIC
implementations
the
algorithms
forcores.
solving
integrals
on
architecture
a huge number ofofcomputational
In multidimensional
this paper we present
the advances
in our
studies on
are due
tocomparison
the high parallelism
in the operations
with
the methods.
many coordinates
of the sequences
and
their
with
the
performance
of
Monte
Carlo
The
fast
implementations
the performance of algorithms for solving multidimensional integrals on Intel MIC architecture
achieved
with
Intel
MIC architecture.
These implementations
easy to be
and
are
thethe
high
parallelism
in the operations
with
the methods.
many are
coordinates
ofintegrated
the sequences
and due
theirtocomparison
with the performance
of Monte
Carlo
The fast implementations
demonstrate
high
performance
in terms ofThese
timingimplementations
and computational
speeds.
achieved
with
the
Intel
MIC
architecture.
are
easy
to
be
integrated
and
are due to the high parallelism in the operations with the many coordinates of the sequences
demonstrate
high
performance
in
terms
of
timing
and
computational
speeds.
Keywords:
Monte
Intel B.V.
MIC architecture,
performanceare
analysis
achieved
theCarlo
Intel methods,
MICbyarchitecture.
These implementations
easy to be integrated and
© 2017 Thewith
Authors.
Published
Elsevier
Peer-review
under
responsibility
of
the
scientific
committee
of
the
International
Conference
on Computational Science
demonstrate
high
performance
in
terms
of
timing
and
computational
speeds.
Keywords: Monte Carlo methods, Intel MIC architecture, performance analysis
Keywords: Monte Carlo methods, Intel MIC architecture, performance analysis

1 Introduction
1
Introduction
The Intel’s Many Integrated Core (MIC) architecture is used in the Intel Xeon Phi line of
1
Introduction
processors,
which are used as co-processor cards in the first generation, but can be used as

The Intel’s Many Integrated Core (MIC) architecture is used in the Intel Xeon Phi line of
fully
functional main
processors
in the subsequent
In our current
high-performance
processors,
used asCore
co-processor
cards ineditions.
theisfirst
but
can Phi
be used
The
Intel’s which
Many are
Integrated
(MIC) architecture
usedgeneration,
in the Intel
Xeon
line as
of
computing
system
they
are
used
as
co-processors
in
servers
equipped
with
standard
Intel Xeon
fully functional
main
in the subsequent
our current
high-performance
processors,
which
are processors
used as co-processor
cards ineditions.
the firstIn
generation,
but
can be used as
CPUs.
Even
withthey
this are
limited
theinIntel
Xeon
Phis compete
with GPU
computing
system
used functionality
as
servers
equipped
with standard
Intel cards
Xeon
fully
functional
main processors
in co-processors
the subsequent
editions.
In our current
high-performance
for
the
role
of
accelerators
in
heterogeneous
systems
where
they
can
significantly
improve
the
CPUs. Even
withthey
this are
limited
theinIntel
Xeon
Phis compete
with GPU
computing
system
used functionality
as co-processors
servers
equipped
with standard
Intel cards
Xeon
computational
efficiency
and
power
consumption
with
respect
to
those
resulting
from
use
of
for the role
of with
accelerators
in heterogeneous
where
they
cancompete
significantly
the
CPUs.
Even
this limited
functionalitysystems
the Intel
Xeon
Phis
with improve
GPU cards
standard
CPUs.
Details
on
how
to
program
for
the
Intel
Xeon
Phi
coprocessors
can
be
found
in
computational
efficiency and
consumption
with
respect
those
resulting improve
from usethe
of
for
the role of accelerators
in power
heterogeneous
systems
where
theytocan
significantly
[1].
SomeCPUs.
of theirDetails
main characteristics
are: 1)
units that allow
processing
standard
how
to program
forequipped
the with
Intel with
Xeonvector
Phi
can from
be
found
computational
efficiency on
and
power
consumption
respect
to coprocessors
those resulting
use in
of
of
integer
or
floating
point numbers
at once; 2) with
running many
cores
low frequencies
[1].several
SomeCPUs.
of
theirDetails
main
characteristics
are: 1)
units
thatatallow
processing
standard
on how to program
forequipped
the Intel Xeonvector
Phi coprocessors
can be
found in
and
3) availability
of floating
hyperthreading.
In practice
if the
instructions
on such
of several
integer
point numbers
once;
2) vector
running
many
cores
lowaccelerators
frequencies
[1].
Some of
their or
main
characteristics
are: 1)atequipped
with
vector
units
thatatallow
processing
are
not
used,
then
they
perform
as
slow
regular
CPUs.
The
Intel
compilers
available
in the
andseveral
3) availability
of floating
hyperthreading.
In practice
if the
instructions
onatsuch
of
integer or
point numbers
at once;
2) vector
running
many cores
lowaccelerators
frequencies
Parallel
Studio
XE
2016
package
provide
direct
access
to
the
vector
instructions
via
compiler
are not
used, thenofthey
perform as slow
regular ifCPUs.
The instructions
Intel compilers
available
in the
and
3) availability
hyperthreading.
In practice
the vector
on such
accelerators
intrinsics,
thus facilitating
the useprovide
of vector
instructions
by
program
developers.
Another
Parallel
Studio
XE they
2016 perform
package
direct
access
thethe
vector
instructions
via compiler
are
not used,
then
as slow regular
CPUs.to The
Intel compilers
available
in the
intrinsics,
thus facilitating
the useprovide
of vector
instructions
program
developers.
Another
Parallel
Studio
XE 2016 package
direct
access toby
thethe
vector
instructions
via compiler
1
intrinsics, thus facilitating the use of vector instructions by the program developers. Another
1877-0509 © 2017 The Authors. Published by Elsevier B.V.
1
Peer-review under responsibility of the scientific committee of the International Conference on Computational Science
10.1016/j.procs.2017.05.149
1

1804	

Parallel Monte Carlo on Intel
MIC Architecture
E. Atanassov, et al.
Emanouil
Atanassov et al. / Procedia Computer Science 108C (2017) 1803–1810

possibility is the direct coding of assembly instructions inside C codes, which is more complex
but still feasible for Xeon Phi accelerators. The different ways of using Intel MIC accelerators
and their inherent composite structure lead to multiple parallelization approaches that can be
applied to such systems. When one tries to implement then in practice, various steps have to
be chosen and the particular target system guides these choices.
For our tests we used the Avitohol High Performance System, built with Xeon Phi 7120P
accelerators, hosted at our institute. Avitohol consists of 150 servers SL250S equipped with
both dual Xeon CPU E5-2650 v2 at 2.60GHz and dual Xeon Phi 7201P accelerator cards. The
total accessible RAM on the system by the regular CPUs and the accelerator cards are 9600 GB
and 4800 GB, respectively. The operating system on the servers is Red Hat Enterprise Linux,
while Intel’s own special version of Linux OS (part of the MPSS package) is installed on the
accelerators. Currently the exact versions on the servers and for the MPSS are 6.7 and 3.6-1,
respectively. This system achieved 332th place in the Top 500 list when it entered operation,
with a theoretical peak performance of about 413 TELOP/s, of which 90% is contributed by
the accelerators. One can conclude that the optimal use of accelerators is the only way to fully
leverage the power of such kinds of systems. However, many software packages do not have
optimised versions for accelerators.
Monte Carlo simulations are an important type of computations routinely run on HPC systems like Avitohol. They are especially important for modelling of real-world phenomena, when
deterministic methods are not yet developed. Typically Monte Carlo methods are amenable to
efficient parallelisation. The computer implementations of Monte Carlo usually employ pseudorandom generators when sampling random variables and there is large amount of research done
on quality and performance of pseudorandom generators. When using them on HPC systems it
is important to be able to use parallel independent streams of pseudorandom numbers, otherwise results from simulations on different processors may coincide or be correlated, effectively
removing the advantage of parallelism or introducing bias in the results. When developing a
computer implementation of a Monte Carlo algorithm one usually relies on using an established
library containing pseudorandom number generators like the specialised SPRNG package ([2])
or the general Intel Math Kernel Library (MKL).
In this work we used the Intel’s implementation of the set of 6024 Mersenne Twister pseudorandom number generators MT2203 from Intel MKL, since parameters of the generators are
meant to provide mutual independence of the corresponding sequences. We consider this to be
one reasonable choice when implementing Monte Carlo algorithms. One alternative way of implementing stochastic simulations is to use scrambled quasi-random sequences. These sequences
in some cases may be used as drop-in replacement for pseudo-random numbers in Monte Carlo
algorithms, with the aim to achieve faster convergence. Because they are fully or partially
deterministic, they provide better theoretical error bounds for suitable sets of functions. In
many algorithms they also behave well
use, achieving convergence rates that are
 in practical

substantially better than the usual O N −1/2 rate of Monte Carlo methods. One established
⋆
measure for the quality of quasi-random sequences is the star-discrepancy DN
, which is related
to the integration error via the classic Koksma-Hlawka inequality (see e.g.,[6]). By using this
measure one can single-out the class of low-discrepancy sequences, which are those that can
be proved to have a rate of convergence of their discrepancy to zero of O (N −s logs N ). This
rate is presumed to be the optimal one ([11]). Other measures of irregularity are also used,
in order to cover other function classes or use cases, for example when the functions to be
integrated are periodic. The practical behaviour of low-discrepancy sequences in an algorithm
is highly dependent on the nature of the function to be integrated. In practice the observed
rates of convergence are frequently better than what one would expect from the theoretical
2

	

Parallel Monte Carlo on Intel
MIC Architecture
E. Atanassov, et al.
Emanouil
Atanassov et al. / Procedia Computer Science 108C (2017) 1803–1810

estimates, especially in high dimensions. In fact, for many sequences using reasonable number
of points and high dimension would yield a theoretical estimate that is worse than the trivial
⋆
one DN
≤ 1. Thus it is expected that for high dimensions using low-discrepancy sequences may
be worse than pseudorandom numbers, although for some problems like Mathematical Finance
it was shown that this is not the case. In addition to such considerations, one must take into
account that the use of quasi-Monte Carlo methods involving low-discrepancy sequences has
some practical issues.
Although the Monte Carlo methods are easy to parallelize by utilizing suitable library
implementation that yields parallel independent streams of pseudo-random numbers, the use
of low-discrepancy sequences in parallel programs requires that all numbers from a certain
interval of the sequence are used in the computation. Thus quasi-Monte Carlo methods are
not as resilient to failures or delays in the computations as are pure Monte Carlo and require
more sophisticated synchronization. The error estimation for quasi-Monte Carlo methods is also
not automatic and requires the use of some sort of “scrambling” (see, e.g., [4, 12, 13]). Since
the low-discrepancy sequences are partially deterministic, their definitions frequently involve
complicated construction and sometimes involve large sets of parameters. The selection of good
values for these parameters is problem dependent, but not easy to accomplish in a theoretically
clean way.
In this paper we consider two representative families of low-discrepancy sequences. The
Sobol’ sequences are a highly popular family of sequences whose quality is dependent on a set
of so-called “direction numbers”. We also consider the direction numbers provided by Frances
Y. Kuo and Stephen Joe ([8]). How exactly we implement the generation of the Sobol’ sequence
is discussed in the next section. Another popular family of low-discrepancy sequences is the
Halton sequence. Many techniques are applied in order to break the well-known problem of
correlation between successive dimensions which can be observed in practice. One theoretically
grounded solution is proposed in [5]. Although this definition can be considered as a special
case of a “scrambled” version of the original Halton sequence, the condition that is imposed on
the coefficients that are used in the scrambling provides much better error estimates of their
discrepancy(see Theorem 2.3, [5]). Through the use of the random shifting terms one can have
the benefits of the usual “scrambling”, i.e., possibility to obtain automatic error estimates. We
have tabulated suitable “admissible numbers” for sufficiently high dimensions (up to 16000)
and there isn’t any practical difficulty to extend them to even higher dimension if necessary.
The condition on the admissible numbers is not highly restrictive and thus allows some freedom
which can be used to achieve some additional properties.
In our previous works we developed algorithms for generation of the sequences of Sobol’ and
Halton that are tailored to specific hardware platforms. In this work we consider their practical
application on the Intel MIC architecture when solving integrals and compare them with an
established pseudorandom number generator, taking into account the requirements for optimal
use of the Xeon Phi resources in parallel MPI applications. We show numerical and timing
results and compare their performance, obtaining as a result recommendations for the practical
use of quasi-Monte Carlo applications employing these sequences and conclusions about their
suitability.
The paper is organized as follows: In the section 2 we present the general approaches for
developing of parallel Monte Carlo and quasi-Monte Carlo applications on Intel MIC architecture. The numerical results are presented in section 3. In the conclusion, we summarise the
main results and describe guidelines for future work.
3

1805

1806	

Parallel Monte Carlo on Intel
MIC Architecture
E. Atanassov, et al.
Emanouil
Atanassov et al. / Procedia Computer Science 108C (2017) 1803–1810

2
2.1

On the development of parallel Monte Carlo and quasiMonte Carlo applications on Intel MIC
General considerations when developing MPI applications targeting Intel MIC architecture

In our previous work [3] we developed generating codes for the Sobol and Halton sequences that
were specifically optimised for the Intel Xeon Phi accelerators. Our main goal was to achieve
maximum performance when running sequentially, which was possible only after carefully controlling the use of vector instructions in the code. When approaching parallel applications the
main problem becomes how to utilize efficiently the hardware hyperthreading that is available
in the Xeon Phi cards. The two main approaches for parallelising quasi-Monte Carlo methods
are “blocking” and “leap-frogging”. Although it is possible to develop an optimised version of
the “leap-frogging” approach for the Halton sequence, it is going to be significantly slower than
the sequential version. For the Sobol’ sequence our code allows for the use of “leap-frogging”,
but only when the number of processing elements is a power of two. Since the number of cores
in Xeon Phi is 60 or 61 for the first generation cards, using powers of two is not suitable in
this case also. Therefore we always use the blocking approach, where the difference between
the codes running at different processing elements is in the starting point. The main problem
when developing MPI quasi-Monte Carlo applications with our generators was revealed to be in
the amount of memory available per process. When the number of processing elements is high,
for example 244 when using 4x hyperthreading, the amount of memory available per process
becomes 67 MB, which is not that high. That is why we had to employ some strategies for
decreasing the memory footprint of our generation codes in order to make them suitable for
high-dimensional problems.
In general there are certain techniques that help produce optimized executables. First of
all, we noticed significant improvement from use of profiling (-prof-gen followed by -prof-use).
Stripping the executable from debugging information using x86 64-k1om-linux-strip was also
beneficial. Lastly, many options that control the behavour of the MPI library are available and
there are established techniques for tuning the execution (for example, via the mpitune utility).
Noticeable improvements were obtained from using all these techniques.
Since most Monte Carlo algorithms are embarrassingly parallel, they benefit greatly from
parallel implementations, and consequently Monte Carlo has become a focal point in GPU
computing. GPU speed-up examples reported in the literature often involve Monte Carlo
algorithms, and there are software tools commercially available that help migrate Monte Carlo
models to GPU. A recent survey of Monte Carlo and randomized quasi-Monte Carlo methods
for integration based on existing (quasi) Monte Carlo sequences in GPU libraries can be found
in Xu, Okten [14]. Some new results regarding quasi-Monte Carlo multilevel methods for
iintegration can be found in [7, 9, 10] but they use only lattice rules.

2.2

Parallel version of the algorithm for generation of Sobol’ sequences on MIC

For the Sobol sequence the generation is achieved by first generating a suitable table of vectors
of “twisted” direction numbers, which are obtained from the regular direction numbers by
cumulative “xor” operation. We point that with the blocking approach to parallelisation these
numbers are the same for every processing element. This means that the same memory can
be initialized and then used across threads that run on the same Xeon Phi card. When using
4

	

Parallel Monte Carlo on Intel
MIC Architecture
E. Atanassov, et al.
Emanouil
Atanassov et al. / Procedia Computer Science 108C (2017) 1803–1810

OpenMP for parallelisation this is trivially achieved by sharing the corresponding array. With
MPI applications it became a bit more complicated. For each Xeon Phi card, participating in
the computation, a region of shared memory is requested via System V IPC calls like shmget.
Only one process from each card needs to initialize the segment, while all others may use it after
passing through an MPI Barrier synchronization call. When we know the maximum number of
points that we need to generate we can limit the number of “twisted” direction numbers that
we need to generate, thus saving some of the memory for other uses. New versions of MPI
provide more portable ways of sharing memory across processes and probably in the future we
shall make use of them instead.

2.3

Parallel version of the algorithm for generation of Halton sequences on MIC

For the Halton sequence the same problem with multiplication of the amount of memory used
for preprocessing arises, but in this case such a straightforward solution does not exist, because
many of the quantities that are to be pre-computed are different depending on the starting
point. What we did instead was to once again use the knowledge of the total number of points
to be generated in order to limit the number of quantities to pre-compute and to switch to use
of short integers instead of regular 32 bit integers where possible. This optimisation has the
added benefit that it reduces the required memory bandwidth during execution and increases
the probability of cache hits. Although the benefit of these changes was limited, it did allow
the application to fit in memory in our use cases.
Note that because we use the modified Halton sequences, certain optimisations that can be
done for the original sequences are not possible. Our particular implementation is optimised for
high dimensions that are multiples of 64. That is why even when less dimensions are necessary
still at least 64 coordinates are generated. That is why in the timing results to be shown next
one can observe that the results for dimensions 32 and 64 are similar. It is possible to switch
to multiples of 8, at the expense of some loss in performance for higher dimensions.

3

Timing and numerical results

In our previous works we have established that the hand-tuned vectorised version of the generators performs better than the auto-vectorised or sequential version. In this work we present
results with the parallel version of the generators when using the Intel MPI library. The practical problems we consider are integration problems. The functions to be integrated are defined
as follows:

d 
d


3
3
F1 =
, F2 =
xi +
|4xi − 2|,
4
i=1
i=1
F3 =

d

π

i=1

2

sin πxi

and F4 = (1 + 1/d)d

d


(xi )1/d

i=1

over the d− dimensional unite cube I d = [0, 1]d , where d = 32, 64.
First of all, for the same number of terms 230 (approximately 1 billion) we evaluate the
performance using varying number of threads in order to gauge the effects of hyperthreading.
In the next table we can observe that using 183 or 244 threads is justified and yields better
performance than simply using the 61 physical cores of the 7120P Xeon Phi card. The results
5

1807

1808	

Parallel Monte Carlo on Intel
MIC Architecture
E. Atanassov, et al.
Emanouil
Atanassov et al. / Procedia Computer Science 108C (2017) 1803–1810

for the Halton sequences are shown, but similar conclusion is reached for the Sobol sequence the performance of 183 and 244 threads is superiour to 61 and 122.
Table 1: Effects of hyperthreading for Halton sequence, Xeon Phi cards
Cards
Threads\Dimension
61
122
183
244

1
32
45.82
33.78
28.46
27.91

2
64
62.82
40.22
37.99
36.15

32
25.89
16.32
14.87
14.49

64
33.83
20.85
20.01
19.24

However, when switching to the use of multiple cards and nodes we arrive at different results.
The reason for that is that even such seemingly high number of points is still relatively low to
demonstrate the benefit of using hyperthreading. That is why we show scalability results with
61 cores for the test function F1 in the next table.
Table 2: Scalability results using 61 cores, within different number of Xeon Phi cards
Generator

dimension
d=32

Sobol
d=64
d=32
Halton
d=64
d=32
MT2203
d=64

Cards
Time (s)
Speedup
Time (s)
Speedup
Time (s)
Speedup
Time (s)
Speedup
Time (s)
Speedup
Time (s)
Speedup

1
5.60
7.25
15.32
22.25
26.99
26.94

2
2.90
1.9
3.71
1.9
7.62
2.0
10.93
20.9
13.64
1.9
14.03
1.9

4
1.55
3.6
1.93
3.7
3.97
3.8
5.72
3.9
7.16
3.8
7.23
3.7

8
0.83
6.7
1.07
6.8
2.12
7.2
2.96
7.5
3.57
7.6
3.71
7.2

16
0.52
10.7
0.62
11.6
1.24
12.3
1.61
13.8
1.97
13.7
2.00
13.4

32
0.39
14.3
0.44
16.4
0.73
20.9
1.02
21.8
1.16
23.2
1.14
23.5

Obviously, the times observed are relatively small, but appropriate for such kind of problems.
If we increased the number of points 10 times, we would observe that using 244 and 183 threads
per card would start to dominate. We also conclude that in all cases using the sequences of
Sobol’ and Halton is competitive with respect to use of pseudo-random number generators and
may actually prove to be faster depending on the particular generator used. Although the
scalability for this number of cards does not look ideal, it is acceptable and shall improve if
larger number of points are used.
In the next figures we show the integration errors with with varying number of terms of the
sequences used. The approximate errors are obtained by varying the initial seeds 10 times. The
chosen dimensions are relatively high for these types of problems - 32 and 64. Although it is
generally believed that such dimensions are not suitable for low-discrepancy sequences and are
more advantageous for the regular (“crude”) Monte Carlo method, we observe better results
with the low-discrepancy sequences of Sobol and (modified) Halton, which improve with the
increase in the number of terms and reach differences of orders of magnitude for some test cases.
6

Parallel Monte Carlo on Intel
MIC Architecture
E. Atanassov, et al.
Emanouil
Atanassov et al. / Procedia Computer Science 108C (2017) 1803–1810

We can observe how after certain number of terms the low-discrepancy sequences show better
results and which one of them is superiour depends on the test function. For the integrals I3
and I4 in 64 dimensions we observe obviously higher orders of magnitude for the convergence
rates of the low-discrepancy sequences.
I3, dimension d=64

I2, dimension d=64
0
Sobol sequences
Halton sequences
pseudorandom numbers

log2 of error

log2 of error

Sobol sequences
Halton sequences
pseudorandom numbers

-10

-5

-15

-20
-10

20

15

log2 of N

-25
15

30

25

20

Figure 1: Integration errors for F2 .

log2 of N

30

25

Figure 2: Integration errors for F3 .

I4, dimension d=32

I4, dimension d=64

0
Sobol sequences
Halton sequences
pseudorandom numbers

-1

Sobol sequences
Halton sequences
pseudorandom numbers

-10

log2 of error

-2
log2 of error

	

-3

-15

-4

-5

-6
15

-20

20

log2 of N

25

Figure 3: Integration errors for F4 .

4

30

15

20

log2 of N

25

30

Figure 4: Integration errors for F4 .

Conclusions and directions for future work

In this work we present variants of our generation routines for the Halton and Sobol quasirandom sequences that are suitable for development of parallel quasi-Monte Carlo applications
using MPI, targeting accelerators based on Intel MIC architecture. We compared them with
a classic pseudorandom generator from an established library (Intel MKL) and we concluded
that they are highly competitive in terms of speed and offer improved error convergence for
the particular problem considered. The generation of the terms of the Sobol’ sequences can be
7

1809

1810	

Parallel Monte Carlo on Intel
MIC Architecture
E. Atanassov, et al.
Emanouil
Atanassov et al. / Procedia Computer Science 108C (2017) 1803–1810

accomplished 3-4 times faster than obtaining pseudorandom numbers via the MT2203 generator,
with acceptable scalability. The generators of the Halton sequences are still faster than the
MT2203 from MKL, under our testing conditions. Depending on the practical problem at hand
this can translate into substantial savings in computational time. The expected advantage of
the low-discrepancy sequences can be observed when the number of function evaluations are
sufficiently high and the integral’s dimension is also relatively high.
Our intention is to offer these generation codes under open source license. Since many realworld processes or simulation problems from the social sciences require simulations with high
constructive dimensionality we plan to make applications in these areas.

5

Acknowledgment

This work was supported by the National Science Fund of Bulgaria under Grant #DFNI-I02/8
and by the European Commission under H2020 project VI-SEEM (Contract Number 675121).

References
[1] Intel
Xeon
Phi
Coprocessor
Instruction
Set
Architecture
Reference
Manual.
https://software.intel.com/sites/default/files/forum/278102/327364001en.pdf.
[2] SPRNG-Scalable Parallel Random Number Generators Library. hhttp://www.sprng.org.
[3] E. Atanassov, T. Gurov, A. Karaivanova, S. Ivanovska, M. Durchova, and D. Dimitrov. On the
parallelization approaches for Intel MIC architecture, volume 1773. 2016.
[4] Emanouil Atanassov, Aneta Karaivanova, and Sofiya Ivanovska. Tuning the Generation of Sobol
Sequence with Owen Scrambling. In Proceedings of the 7th International Conference on Large-Scale
Scientific Computing, LSSC’09, pages 459–466. Springer-Verlag, 2010.
[5] Emanouil I. Atanassov. On the Discrepancy of the Halton Sequences, volume New Series Vol. 18,
Fasc. 1–2, pages 15–32. 2004.
[6] Russel E. Caflisch. Monte carlo and quasi-monte carlo methods. Acta Numerica, 7:1–49, 1998.
[7] Robert N. Gantner and Christoph Schwab. Computational Higher Order Quasi-Monte Carlo
Integration, pages 271–288. Springer International Publishing, Cham, 2016.
[8] Stephen Joe and Frances Y. Kuo. Constructing sobol sequences with better two-dimensional
projections. SIAM J. Scientific Computing, 30(5):2635–2654, 2008.
[9] Quoc T. Le Gia Josef Dick and Christoph Schwab. Higher order quasi monte carlo integration for
holomorphic, parametric operator equations. SIAM/ASA Journal on Uncertainty Quantification,
4(1):48–79, 2016.
[10] Scheichl R. Schwab Ch. Sloan I.H. Ullmann E. Kuo, F.Y. High-performance financial simulation
using randomized quasi-monte carlo methods. to appear in Math. of Comp.
[11] Harald Niederreiter. Random Number Generation and quasi-Monte Carlo Methods. Society for
Industrial and Applied Mathematics, Philadelphia, PA, USA, 1992.
[12] Art B. Owen. Randomly Permuted (t,m,s)-Nets and (t, s)-Sequences, pages 299–317. Springer
New York, 1995.
[13] Art B. Owen. Scrambling sobol’ and niederreiter-xing points. J. Complex., 14(4):466–489, 1998.
[14] L. Xu and G. Okten. High-performance financial simulation using randomized quasi-monte carlo
methods. Quantitative Finance, 15(8):1425–1436, 2015.

8

