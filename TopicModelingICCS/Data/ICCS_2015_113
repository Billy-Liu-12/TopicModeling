Procedia Computer Science
Volume 51, 2015, Pages 2518–2532
ICCS 2015 International Conference On Computational Science

Multi-INT Query Language for DDDAS Designs
Alex J. Aved and Erik P. Blasch
Air Force Research Laboratory, Information Directorate
Rome, NY, USA
alexander.aved@us.af.mil, erik.blasch.1@us.af.mil

Abstract
Context understanding is established from coordination of content, analysis, and interaction between
users and machines. In this manuscript, a live-video computing (LVC) approach is presented for access
to data, comprehension of context, and analysis for context assessment. Context assessment includes
multimedia fusion of query-based text, images, and exploited tracks, which are utilized for information
retrieval. We explore developments in database systems which enable context to be extracted by userbased queries. Using a common image processing data set, we demonstrate activity analysis with
context, privacy, and semantic-awareness in a Dynamic Data-Driven Applications System (DDDAS).
Keywords: dynamic data applications systems, real-time, query language, scene context, live-video computing

1 Introduction
Current developments in cognitive computing, cloud architectures, and distributed access afford
access to a large volume of multimedia information (Liu, B., et al., 2014). Multimedia data
exploitation combined with contextual analysis (Blasch, Steinberg, et al., 2013) can provide increased
responses to queries, higher resolved accuracies, and great situation awareness. To enhance computer
vision and multimedia developments, we focused on database designs.
Early database systems were designed for storage, retrieval and querying of alphanumeric data
(Date, 1977). A typical database management system (DBMS) implementation supports business
applications by persisting application state, resolving queries, and facilitating transactions to mitigate
concurrency errors. A Multi-Media DataBase (MMDBS) utilizes a traditional DBMS to manage
metadata and indices, but also encompasses additional technologies and services not typically present
in DBMSs which include: video on demand, document management and imaging, spatial data,
specialized query languages, face recognition and relevance feedback, to name a few. Because
multimedia content, and video in particular, can be quite large and its communication bandwidth
intensive, MMDBS are often paired with specialized communication frameworks, such as the HeRO
protocol discussed in (Tantaoui, Hua, & Do, 2004), to provide content delivery to a multitude of
concurrent users without overwhelming the physical communication medium.

2518

Selection and peer-review under responsibility of the Scientiﬁc Programme Committee of ICCS 2015
c The Authors. Published by Elsevier B.V.

doi:10.1016/j.procs.2015.05.360

Multi-INT Query Language for DDDAS Designs

Alex Aved and Erik Blasch

Multimedia constructs include content (data), entities (features), and scenes (context). Context
enhanced information fusion examples include imagery (Liu et al., 2012), user queries (Blasch,
Steinberg, et al., 2013), text and tracking (E. Blasch, Bosse, & Lambert, 2012), and content-based
image retrieval (CBIR) (Ezekiel et al., 2013). The multiple applications of fusion require resource
management (E. Blasch et al., 2008) to facilitate the ability of the user-defined queries to be
determined from the information management system.
Dynamic Data-Driven Applications Systems (DDDAS) combines measurements, modeling, and
software as a systems approach for real world engineering purposes (Darema, 2005). Recent DDDAS
examples include information fusion (Blasch, Seetharaman, et al., 2013), vehicle tracking (Fujimoto,
et al., 2014), privacy (Fan, Xiong, et al., 2014), and signal processing (Bhattacharyya, et al, 2013;
Sudusinghe, et al. 2014). We use these examples in motivating a multimedia example.
The rest of the paper is as follows. Section 2 discusses context indexing and Section 3 database
design. Section 4 details the Live Video Computing (LVC) DataBase Management System
(LVDBMS). Inspired by DDDAS constructs for coordination between software services and
measurement systems, a video-base activity analysis is presented. Section 6 provides conclusions.

2 Multimedia Data Representation for Context Indexing
2.1 Multimedia Indexing
Multimedia constructs include content (data), entities (features), and scenes (context) that are utilized
through indexing and retrieval. Emerging DDDAS constructs include multimedia information (Blasch,
Aved, 2015) such as layered sensing (Mendoza-Schrock, et al., 2009), image fusion (Wu, et al., 2011),
and video tracking (Wu, et al., 2012). Collections of multimedia information can grow to very large
sizes, consuming many gigabytes of storage space. In order to utilize multimedia content it must be
retrieved; whether the retrieval is to find a movie based upon its title, or one is looking for images,
clips of audio or video segments showing a particular subject or class of objects. As an example,
consider a table of records in a traditional relational database. Each record in the table can be
considered as a point in a multidimensional space (Samet, 1990, 2006). Consider a record for an
object-track relation with the following fields: {object_id, track_id, situation_id, start_date,
end_date}. In this case, records in this table correspond with points in a 5-dimensional space, where
three of the dimensions refer to, say, integers (object_id, track_id and situation_id) and the other two
dimensions are of type date-time (i.e. start_date, end_date). The DBMS manages the collection of
these records and stores them in a file on some persistent media. In order to facilitate efficient retrieval
of records in the database, indexes can be created.
The index itself is simply another table (or, correspondingly, a file created and maintained by the
DBMS). For example, an index over the field object_id could contain only object_id’s and the location
of associated records in the corresponding employee-department file. By utilizing the index file in
order to resolve queries, less data would need to be loaded and processed, since the index file contains
primarily object_id data (and not other data fields such as situation_id). To further enable efficient
retrieval, an ordering can be imposed upon the records, either in the primary data file or in the index.
However, to accommodate future record operations to the primary data table (e.g. delete, insert,
update) it is often more efficient to impose the ordering only on the data in the index files. For
numeric fields, the ordering can be based upon numeric value. For character fields, the order can be
based upon corresponding ASCII or UNICODE numeric values, or based upon lexicographic order.
For other types of data, such as color, the ordering could be based upon the corresponding
hexadecimal value (e.g. red is “ff0000”) or the color’s wavelength.
Samet (Samet, 2006) identifies five key questions that should be considered when deciding how to
represent a dataset:

2519

Multi-INT Query Language for DDDAS Designs

Alex Aved and Erik Blasch

(1) What is the type of the data; continuous, discrete?
(2) When will operations be performed; e.g. a log file might only have data appended to its end.
(3) How should the ordering of the data be applied; should the data in the primary file be ordered,
or only the index files? Which attributes should be included in the ordering?
(4) Which data be added or removed? Will additional attributes be added in the future? and,
(5) How much is the quantity data - will fit into the primary memory of the computer hosting the
database, or will disk-resident data access algorithms need to be utilized?
There are many different ways data can be represented, and considering questions such as these can
guide the process of designing an implementation.
When considering multimedia for browsing and searching, an index is required to make achievable
the responsiveness, timeliness and corresponding computation requirements associated with browsing
achievable. Some fundamental questions pertaining to multimedia data are what, which and how. At
what granularity should the item be represented in the index; as a whole or by frame or a clip of
frames? Which refers to which items should be indexed; such as - should all pixels shown in each
frame of video be represented somewhere in an index, or should only moving objects be stored?
Should the time index of when an object appears or disappears be recorded? How to index an item
pertains to selecting and extracting features to be indexed. Data indexing, and more specifically
multimedia data indexing is a multifaceted and difficult problem, and as such, there is a significant
quantity of research and correspondingly, solutions and indexing algorithms and data structures. Some
works that addresses the issues of multimedia indexing holistically are (Bolle, Yeo, & Yeung, 1998;
Brunelli, Mich, & Modena, 1999; Snoek & Worring, 2005; Y. Wang, Liu, & Huang, 2000).
To illustrate multimedia indexing, consider the information that can be extracted from a video: the
visual component (the visual content represented by pixels in the frames), the auditory information
(i.e., audio tracks), and text (text that can be extracted; and metadata pertaining to the video itself such
as genre, actors, etc.). A multitude of semantic properties of the video can be extracted from the
metadata pertaining to its content: the type of video (e.g., education, training, entertainment), the time
period the video covers; major actors who appear, and so forth (Boggs, 1996; R. Jain & Hampapur,
1994). To index content that is depicted visually in the video, pattern recognition approaches can be
employed; for example, template matching (e.g., Bayes classifier, decision trees, Hidden Markov
Models, face and people detection (Belhumeur, Hespanha, & Kriegman, 1996)). The reader is referred
to (A. K. Jain, Duin, & Mao, 2000) for a comprehensive review of pattern recognition techniques. To
index videos, they can be decomposed into a series of semantic shots, and each shot can be
individually indexed (Ide, Yamamoto, & Tanaka, 1999; Nagasaka & Tanaka, 1992). To index audio
data, a number of different techniques can be employed, for example sounds can be analyzed to detect
musical instruments or talking (Foote, 1997; Wold, Blum, Keislar, & Wheaten, 1996).

2.2 Multimedia Retrieval
To index multimedia content, first data is decomposed and segmented and features which
correspond to points in a multidimensional space are extracted. The next step is to efficiently store and
retrieve those points and correspondingly, the associated multimedia content. Some of the questions
raised in the previous section relate to how data will be represented for storage and retrieval.
Storage of data on a disk implies that it is organized; logically the data is organized into buckets
and physically the buckets are oriented in pages. Pages (and correspondingly, the buckets containing
data points) are stored in files. The simplest way to store a set of points in a file is as an unordered
sequential list. The downside is that in order to does an equality search on the file for a particular
attribute value, the entire file must be processed. With this organization as a starting point, there are
numerous structures and algorithms that facilitate indexed storage and retrieval, one example is the
Grid File (Nievergelt, Hinterberger, & Sevcik, 1984).

2520

Multi-INT Query Language for DDDAS Designs

Alex Aved and Erik Blasch

Another straight-forward technique to organize data in a file is to utilize a hash function. The
concept behind a hash function is to utilize a mathematical function to distribute items (i.e., key-value
pairs) into buckets which are stored on persistent media in a file (or files, depending upon the
implementation). Given a key, the hash function can suggest which bucket to store the value into. In
the case that the bucket is at capacity, there are various algorithms that determine how to manage the
overflow (collision resolution, load factor, etc.) (Aho, Hopcroft, & Ullman, 1983; Cormen, Leiserson,
Rivest, & Stein, 2001; Pieprzyk & Sadeghiyan, 2001).
When choosing an index structure it is important to consider the type of data that will be stored; for
example, strings or numbers, point data, lines (or line equations), rectangles, regions, surfaces,
volumes, etc., and the types of queries that will be performed; point queries, range queries, window
queries, etc. For point data, one can utilize index structures like the Binary Search Trees (Bentley,
1975), B-Tree (Scheuermann & Ouksel, 1982) or B+-Tree).
The tree structures discussed thus far are referred to as space-partitioning structures; they are
hierarchical data structures that decompose the space into disjoint partitions. A downside is that if they
become unbalanced then their implementation suffers in terms of input/output (I/O). The SpacePartitioning Generalized Search Tree (SP-GiST) index supports input-output messaging, even in the
case where the tree structure is unbalanced (Aref & Ilyas, 2001).
When working with high-dimensional data, one method of data management is to reduce the
dimensionality and utilize one of the hierarchical data structures discussed previously, such as an RTree (Guttman, 1984). Alternatively, indices that are not based upon the dimensions of the objects, but
on the distances between them (the interobject distances), e.g. SparseMap (Hristescu & Farach-Colton,
1999), FastMap and MetricMap (J. T. L. Wang et al., 1999).

3 Database Systems in support of Context Analysis
Traditional DBMSs orient data in tables, such that each table contains records (or tuples in the
relational vernacular). Each record in a table has a common attribute structure, illustrated in the right
side of Figure 1. LVC is a stream-oriented paradigm that operates over streaming data as input. A
comparison of concepts between traditional database computing and LVC is presented in Table 1 and
graphically illustrated in Figure 1. Live Video Query Language (LVQL) is the query language of the
LVC implementation to specify events in terms of spatio-temporal observations and correlations of
objects in video streams.

Figure 1. LVC stream data model contrasted with relational record and disk data model.

2521

Multi-INT Query Language for DDDAS Designs

Alex Aved and Erik Blasch

Concept
LVC
DBMS
Storage
Camera
Hard Drive
Relation
Video stream
Record
Data unit
Video frame
Tuple
Data granularity
Object
Attribute
Query language
LVQL
SQL
Table 1. Comparison of LVC and traditional DBMS concepts.

LVC is the theoretical framework upon which the LVDBMS prototype system is based (Figure 2).
Traditional video stream processing applications are designed specifically to solve a particular
problem, termed siloed, Siloed systems are designed to work with a specific set of cameras or camera
hardware. The result of siloed application development is applications that are not capable of
operating with each other in a reciprocal fashion to share information and provide additional value and
value-add opportunities. If context data developed from one application needs to be combined for
auditing, reporting or other purposes; additional software (middleware) must be purchased and
interfaced with these applications.

Figure 2. Logical relationship among high-level components of the LVDBMS prototype and major
components of the framework encapsulated in each tier.

However the downside is that this middleware must be installed and configured on a case-by-case
bases and “adapters” for each application must be configured or developed to provide applicationspecific interfaces to the middleware. The middleware must then perform an extract, transform and
load (ETL) process to transform data received from the application-specific adapters into a common
data format that is amenable to further processing. The result is additional middleware software that
must be purchased and maintained and also staff resources to install, configure, maintain and upgrade,
as appropriate. Figure 3 illustrates the common repository approach with middleware, which
centralizes some processing and business logic but does not address redundancy and resource sharing
and contention issues with the physical sensors.

2522

Multi-INT Query Language for DDDAS Designs

Alex Aved and Erik Blasch

Figure 3. Applications built as information silos utilizing middleware and application-specific data adapters.

Libraries such as OpenCV (Bradski, 2000) and Integrated Performance Primitives (IPP) (Taylor,
2007) are commonly used by programmers when developing repository applications, to provide basic
data-handing functionality. The OpenCV library provides a comprehensive assortment of image
processing and data management routines and data structures. For example, the IPP library provides
functions and associated data structures that are specifically tuned to take advantage of features
provided by modern multicore processors such as parallel data processing instructions. However these
common libraries provide low-level functionality that programmers use as conveniences; and do not
generally provide out-of-the-box high-level application functionality. (For example, OpenCV routines
could be used to read in camera frames, and other routines would need to be called in the proper order
with the proper parameters and settings in order to interpret imagery depicted in the frames.)
The LVC approach leverages (1) a common video processing software infrastructure to provide a
common programmable interface to clients, (2) a shareable pool of camera resources to share context
data with the goal is to create an ecosystem for collaboration and information sharing, and (3)
searchable database to allow users to draw new insights that are not possible with siloed information
frameworks. The LVC approach facilitates rapid application development by allowing application
architects and software developers to focus their resources on the business problem at hand, rather
than implementation issues pertaining to computer vision and stream processing implementations.

4 LVDBMS for Context Analysis
The components of the LVDBMS are logically grouped into four tiers, illustrated in Figure 4. Each
tier defines one or more web service interfaces to facilitate communication between the layers. The
four layers include:
x The camera layer encompasses cameras and their corresponding adapters. Camera adapters are
conceptually similar to device drivers in computer systems, allowing for disparate camera device
hardware to connect with a standard LVDBMS interface.
x The spatial processing layer processes the metadata and video streams from the camera adapters
and passes results to the stream processing layer. A host in this layer communicates with
multiple camera adapters, but a camera adapter communicates with only a single spatial
processing layer host.
x The stream processing layer receives subquery evaluation streams from spatial processing layer
hosts and computes final query results for delivery to clients. As this interfaces with end users
and applications (i.e. the client layer), it contains logic for managing authentication, connections
and session state with LVDBMS clients.

2523

Multi-INT Query Language for DDDAS Designs

Alex Aved and Erik Blasch

x The client layer encompasses LVDBMS end users and client applications. Clients authenticate and
interact with the LVDBMS by browsing the catalog of cameras, submitting queries and receiving
query results.

Figure 4. LVC approach showing common core software stack and shared sensor resources, all aligned for
multi-use and multi-workload processing.

The LVDBMS illustration depicted in Figure 5 illustrates how a query flows down through the
LVDBMS architecture, and then how data and query results flow back up through the layers and back
to the client.

Figure 5. LVDBMS query and subsequent decomposition.

A query is posed by an end user or client application to the LVDBMS. The initial query is submitted
to the stream processing layer host to which the client is connected. The stream processing layer host
maintains metadata pertaining to available spatial processing layer hosts (also referred to as camera
servers, as they interface with cameras via their adapters and perform processing) and their associated
cameras. With the metadata information, the stream processing layer host translates a query into one
or more subqueries. Each subquery corresponds to a particular camera server host, where it will be
sent for evaluation. Camera adapters process imagery from camera sensors and translate it into a
stream of images and corresponding metadata, which is sent to its respective camera server. Metadata

2524

Multi-INT Query Language for DDDAS Designs

Alex Aved and Erik Blasch

associated with each video frame from the camera adapter includes information pertaining to the frame
itself (i.e. timestamp, sequence number, etc.) and to objects observed within the frame and segmented
out by the camera adapter (i.e. object identifier, a bounding box identifying the location of the object
within the frame). Subqueries evaluate LVQL expressions over video streams (specifically, over the
intersection of video streams specified by the query and video streams managed by a particular camera
server to which the subquery was sent) and stream subquery evaluation results back to the respective
stream processing layer host. The stream processing layer host receives one or more intermediate
results for each evaluation time step and computes a final query result (for the particular point in time),
which is then delivered back to the end user or client application. The query information is based on
the data model.

4.1 LVC-DMBS Data Model
LVC, and correspondingly the LVDBMS, is concerned with computation over video streams. As
such, the event and data models revolve around objects that are observable by imaging sensors and
depicted in temporally oriented frames in the video streams that emanate from these sensors.
Therefore, it follows that an event (i.e., a simple event) is defined to be occurrence of an action that
may be observed by one (or more) cameras and represented in frame data in corresponding data
streams. We note that in this work, the terms video stream and camera stream are used
interchangeably, as are enabling hardware device terms such as camera and imaging sensor.
From the perspective of an LVDBMS client, events may be specified in LVQL by using a
combination of spatial and temporal components, or operators. Thus, a user can leverage the LVQL to
specify a complex event in terms of simple events that are related temporally. For example, a simple
event could be a person (or more generally some object) appearing in a scene or moving in front of a
desk (where the term scene refers to some portion of the real world that is observed by a camera and
rendered into a sequence of frames in a video stream). A complex event, or activity, relates simple
events with temporal operators (Blasch et al., 2013). For example, a complex event could be defined
as a person first appearing in a scene and then, within some threshold of time, moving in front of a
desk. (Since the LVQL presented in this work is 2-dimensional, there is no distinction between
touching and in-front-of, as that type of scene information is not captured by the cameras.)
A spatiotemporal query is formulated in LVQL. This query specification defines which video
streams will be monitored for the occurrence of an event. That is, if the query specifies that a
particular video stream will be monitored for the appearance of an object, if an object subsequently
appears in a different video stream, there will be no impact upon the query result. An object is a
fundamental component of an event specification. As indicated in Figure 6, there are two basic types
of objects that are recognized: dynamic objects are detected automatically by the image processing
software, and static objects are indicated by users of the system. The third class of objects are crosscamera dynamic objects. These are dynamic objects that were first recognized in one video stream and
subsequently recognized in a second stream. The inclusion of the cross-camera object class simplifies
the expression of queries that define events correlating objects that appear in multiple video streams.
Note that in each respective stream these objects also qualify as dynamic objects, Table 2 illustrates
the interrelationships among the various objects graphically.

Figure 6. Illustration of interrelationships among LVDBMS objects, static objects are declared by the user.

2525

Multi-INT Query Language for DDDAS Designs

Alex Aved and Erik Blasch

Another view of the data flow in the LVDBMS is presented as an example in Figure 7. Starting
from the left, a camera observes a scene of a sidewalk and building. Within the scene objects are
observed, including a door and a pedestrian walking assigned identifiers and tracked within their
respective video streams. (Note that the door is manually identified by an operator with a static
object.)
Object class
Static

Dynamic
Cross-camera
dynamic

Description
Objects of this class are defined by the user and do not move within the scene.
For example, a static object may be defined over a window or door for
subsequent use in a query.
Salient objects that are detected automatically within a video stream. A model of
the scene background is maintained and as an object passes through the scene, its
appearance is distinguished from the background. If its size is beyond a threshold
it is segmented, assigned a unique identifier and tracked.
Objects detected in one video stream and subsequently matched to an object in a
second video stream are classified as cross-camera dynamic objects.
Table 2: Comparison classes of LVC objects.

Figure 7. Depiction of the data flow in the LVDBMS, from sensor to user via query.

Continuing with the example Figure 7, the scene is segmented and objects are tracked and sent to
the server in the spatial processing layer. Metadata received from the sensor adapter is used as input
for spatial operators which send results to the stream processor residing in the stream processing layer.
The final query result is streamed to the user from the stream processing layer host.
Additionally, the LVDBMS provides permission context. When a user requests to monitor the
imagery from a video stream, the images come from the camera server. This allows the user to observe
the same images in sequence with query evaluation results and eliminates a potential capacity
bottleneck if multiple users view images from the same sensor simultaneously. By serving content
from an LVDBMS host (rather than the sensor directly), authentication and authorization information
pertaining to the user’s session is evaluated.

4.2 LVDBMS Query Language
LVQL is the query language of the LVDBMS that enables contextual acquisition of information.
Analysts and programmers leverage the query language in order to develop applications that interact
with video streams and (in the future) other modalities of sensors. As such, the programmers and
application designers need only know the details of the query language, and do not need to spend time

2526

Multi-INT Query Language for DDDAS Designs

Alex Aved and Erik Blasch

developing stream processing algorithms or low-level details of the LVDBMS. LVQL permits for the
specification of an event and a corresponding action to be defined over a video stream (or a set of
video streams). LVQL a declarative language, meaning that the user defines a logical event
specification and not the particular flow of control or algorithms that will be executed to determine the
query result. An LVQL expression specifies a spatio-temporal event, and an action that is to be
triggered when the event is recognized. The basic form of a query (specifically, an ActionEvent) is as
follows:
ACTION UserSpecifiedAction
ON EVENT EventSpecification
which signifies that an action UserSpecifiedAction corresponds with EventSpecification and will be
executed the first time a query evaluation result of true is returned. EventSpecification is an event
specification that is generated by a context free grammar which consists of a set of rules, or
productions, which can be utilized to express (describe) an event. A simplified set of LVQL
productions is presented in Figure 8, where items shown in light blue represent tokens recognized by
the language.
Lvql := ActionEvent | VDL
ActionEvent := [action UserSpecifiedAction] on EventSpecification
EventSpecification := NotSpTmplEvent ( BooleanOperator NotSpTmplEvent )
NotSpTmplEvent := [not] SpatialTemporalEvent
SpatialTemporalEvent := CompositSpatialEvent | CompositTemporalEvent
CompositSpatialEvent := appear | north | northwest | inside | meet | ...
CompositTemporalEvent := before | meets
BooleanOperator := and | or | not BooleanOperator
VDL := VCmdType view ViewIdentifier over VStreamIdent [ set VPrivFilter ]
VCmdType := create | update | delete
VTargetStmt := target eq ( querytargets | nonquerytargets |
previouslymasked | none )
VTmpScpStmt := temporalscope eq ( querynonactive | queryactive |
permanent | none )
VObjScpStmt := objectscope eq ( static | dynamic | crosscameradynamic
| none )
VStreamIdent := ( Cameraidentifier | ViewIdentifier )
Cameraidentifier := camIdent
ViewIdentifier := viewIdent

Figure 8. An illustration of the LVQL grammar (omitting parameters such as thresholds).

Figure 8 shows the LVQL syntax which consists of either an Action Event or a View Definition
Language (VDL) production. In the case of an Action Event, which specifies a query, the event
definition must contain a spatial operator (e.g. Appear, North, Meet, etc.) The VDL is used to define
privacy filters and views over video streams, and is discussed in (Aved, 2013).
Events in LVQL are expressed in terms of spatial and temporal operators and Boolean logic. The
simplest event that can be expressed is the appearance of an object in a video stream by using the
Appear() operator. The Appear() operator accepts two arguments (i.e., operands) where the first
operand specifies the video stream, the object class (and possibly filter criteria) that the operator will
be applied to, and the second is a threshold. (All spatial operators accept a threshold argument.) The
threshold for the Appear() operator specifies the minimum size of an object that will satisfy the
appearance condition, in terms of the area of the minimum bounding rectangle (MBR) that contains the
object. For example, Appear(s1.*, 200) will return true each time it is evaluated if a dynamic object
with an MBR of area greater than or equal to 200 is observed in the current video frame. In the case of
a spatial operator such as West(), three arguments are accepted; the first two correspond to objects in
the video stream, and the third is again a threshold. West() returns true if the object specified by the

2527

Multi-INT Query Language for DDDAS Designs

Alex Aved and Erik Blasch

first operand is to the left of the object specified by the second operand, in a stream (e.g., see Figure
8). The third argument, the threshold, specifies the amount of separation between these objects (i.e. the
distance between the bottom of the upper object’s MBR and the top of the lower object’s MBR). For
example a value of 10 pixels means the upper object must be at least 10 pixels above the lower object.
Note that this threshold can be negative, allowing MBRs to overlap. The ability to ascertain properties
of the target resolves context such as geometry, orientation, intensity, etc. for context assessment.

5 Results
The LVQL query language provides an interface between operators (real-time operation), analysts
(forensic mode) and applications (both) for the quantification and analysis of content in video streams.
The LVDBMS, along with the LVQL, shows examples of linkages between multimedia content,
analyses and streaming video analysis, and methods to provide contextual awareness.
LVQL queries expressed in the LVDBMS are continuously evaluated and results streamed back to
the corresponding user or application. The interval at which they are evaluated is referred to as the
resolution of the query; (e.g., query resolution of 1 second). The video dataset leveraged for the work
was generated by the DARPA Video and Image Retrieval and Analysis Tool (VIRAT) program, which
captured a number of activities using both aerial and ground-based video coverage (Figure 9).

Figure 9. Representative videos were selected for evaluation from the VIRAT dataset.

Complex and simple video clips were used to detect objects and extract activity analysis.
Representative frames are shown in Figure 10 for a simple scene showing an automobile and a group
of people. The LVQL extracts metrics of activity and complexity from video as shown in Figure 10.

Figure 10. Video from the VIRAT collection depicting a objects (people and vehicles), people and activities
(walking, loading, unloading).

2528

Multi-INT Query Language for DDDAS Designs

Alex Aved and Erik Blasch

Figure 11 provides two metrics defined in terms of LVQL queries which help characterize the
activity: labeled as “count” and “activity.” Count is defined by the query action 'appear' on
appear(c0.*,250), which identifies when salient objects are observable and greater than or equal a
certain size, in this case 250 pixels in area. The Activity metric is defined by the query action 'appear
ct' on count(appear(c0.*,250)), which wraps the appear operator in an aggregate function count which
cumulatively counts the number of distinct objects based upon its track. The derivative of the count
metric provides the number of objects coming into or exiting the scene. Filtering over the activity
count derivative indicates when activities are occurring that can be used to alert operators, cue other
cameras, or determine scene-rich content.
Table 3 provides a tabular illustration of the activity indication and count data of Figure 11,
showing data rows corresponding to time indices when the difference between consecutive object
count values is non-zero (that is, when objects are detected to be entering or exiting the scene). The
LVQL query provides a concise summary of object entrance and exit activity in the video clip. Other
LVQL spatial and temporal operators can produce extensions or refinements of this information to
provide additional context such as activity in certain regions of the scene, and so forth.
Time index (S)

Activity
indication

Count of
objects

Difference

11
1
1
75
1
2
1
117
1
3
1
131
1
4
1
153
1
5
1
157
1
6
1
169
1
7
1
180
1
8
1
187
1
9
1
188
1
10
1
230
1
11
1
242
1
12
1
255
1
13
1
322
1
14
1
335
1
15
1
379
1
16
1
415
1
17
1
485
1
18
1
496
1
19
1
498
1
20
1
538
1
22
2
539
1
23
1
546
1
24
1
577
1
25
1
582
1
26
1
Table 3. Tabular illustration of the information conveyed in Figure 11.

2529

Multi-INT Query Language for DDDAS Designs

Alex Aved and Erik Blasch

Figure 11. Graph illustrating activity taking place within the video depicted in Figure 10 (“Activity
indication”) and the cumulative number of objects identified (“Count”), for a portion of the video.

6 Conclusions
The LVDBMS, a stream-oriented database platform architected for real-time stream processing,
enables content analysis via user-based queries, establishing Level 5 fusion. As enhanced by DDDAS,
the focus on software coordination with content control (e.g., exploitation) from the query design
supports multimedia indexing. Fundamental questions that must be answered when selecting an
indexing technique are what, which and how. Multimedia index retrieval generally entails a hash
function to construct a concise representation of the multimedia object, and a corresponding index
structure to organize the corresponding features in a multidimensional feature space.
The LVC concept transfers multimedia data from the source to the receiver as a stream-oriented
series of temporally aligned frames of still imagery. Mathematical operations can be applied to the
images to extract information; that is, to convert from pixel-spatial representations to numeric
representations that are amenable to fusing with other data sources to provide context for analysis. The
LVC query language, LVQL affords activity model selection, sensor control, and context
representation within the video to assist operators and analysts with scene and content understanding.
The LVDBMS facilitates timely responses to user queries by leveraging various hash and index
structures that represent objects extracted from multimedia video streams. Queries expressed in LVQL
syntax permit users and applications to specify events and thus enable multimedia fusion of querybased text. A tracker (from an OpenCV implementation) generates tracks for salient objects, which are
leveraged to increase the accuracy of maintaining persistent object detections within streams.
An example of a text-based query enabling content analysis and coordination demonstrates the
concept usefulness. A straight-forward LVQL operator checks for the appearance of salient objects in
a video selected from the VIRAT dataset. An aggregate operator is applied to the Appear operation to
indicate the number of objects, and thus the complexity, of the scene as a function of time.
Future efforts include using the information exploitation system for big-data problems including
physics-based and human-based video-to-text information fusion (E Blasch, et al., 2014) and
integration of other tracking methods (Mei, et al., 2013). Large scale multimedia applications will
require dynamic-driven application systems approaches to that bring together the context, privacy, and
semantic-aware analysis.

2530

Multi-INT Query Language for DDDAS Designs

Alex Aved and Erik Blasch

Acknowledgements
This work is partly supported by the Air Force Office of Scientific Research (AFOSR) under the
Dynamic Data Driven Application Systems program and the Air Force Research Lab.

References
Aho, A. V., Hopcroft, J. E., & Ullman, J. (1983). Data structures and algorithms. Addison-Wesley
Longman Publishing Co., Inc.
Aref, W. G., & Ilyas, I. F. (2001). Sp-gist: An extensible database index for supporting space
partitioning trees. Journal of Intelligent Information Systems, 17(2), 215–240.
Aved, A. J. (2013). Scene Understanding for Real Time Processing of Queries Over Big Data
Streaming Video. Orlando, Florida, USA. Retrieved from:
http://etd.fcla.edu/CF/CFE0004648/20130220_Alexander_Aved_EECS_PhD.pdf
Belhumeur, P., Hespanha, J., & Kriegman, D. (1996). Eigenfaces vs. Fisherfaces: Recognition using
class specific linear projection. European Conference on Computer Vision, 43–58.
Bentley, J. L. (1975). Multidimensional binary search trees used for associative searching.
Communications of the ACM, 18(9), 509–517.
Bhattacharyya, S. S., Deprettere, E., Leupers, R., Takala, J. (editors). (2013). Handbook of Signal
Processing Systems. Springer, second edition.
Blasch, E., Bosse, E., & Lambert, D. A. (2012). High-Level Information Fusion Management and
Systems Design. Artech House.
Blasch, E., Nagy, J., Aved, A., Jones, E. K., Pottenger, W. M., Basharat, A., Chen, G. (2014). Context
aided video-to-text information fusion. International Conference on Information Fusion.
Blasch, E. P., & Aved, A. J. (2015). Dynamic Data-Driven Application System (DDDAS) for Video
Surveillance User Support. Presented at the International Conference on Computational Science,
Reykjavík, Iceland.
Blasch, E., Salerno, J., Kadar, I., Hintz, K., Biermann, J., & Das, S. (2008). Resource management
coordination with level 2/3 fusion issues and challenges [Panel Report]. IEEE Aerospace and
Electronic Systems Magazine, 23(3), 32–46.
Blasch, E., Seetharaman, G. Reinhardt, K. (2013). Dynamic Data Driven Applications System concept
for Information Fusion. Int’l Conf. on Computational Science, pp. 1999-2007.
Blasch, E., Steinberg, A., Das, S., Llinas, J., Chong, C., Kessler, O., White, F. (2013). Revisiting the
JDL model for information Exploitation. International Conference on Information Fusion.
Boggs, J. M. (1996). The art of watching films. ERIC.
Bolle, R. M., Yeo, B. L., & Yeung, M. (1998). Video query: Research directions. IBM Journal of
Research and Development, 42(2), 233–252.
Bradski, G. (2000). The OpenCV Library. Dr. Dobb’s Journal of Software Tools.
Brunelli, R., Mich, O., & Modena, C. M. (1999). A Survey on the Automatic Indexing of Video Data.
Journal of Visual Communication and Image Representation, 10(2), 78–112.
Cormen, T. H., Leiserson, C. E., Rivest, R. L., et al. (2001). Introduction to algorithms. MIT press.
Darema, F. (2005). Grid computing and beyond: The context of dynamic data driven applications
systems. Proceedings of the IEEE, 93(2):692–697.
Date, C. J. (1977). An Introduction to Database Systems (2nd ed.). Addison-Wesley Publishing.
Ezekiel, S., Alford, M. G., Ferris, D., Jones, E., et al. (2013). Multi-scale decomposition tool for
Content Based Image Retrieval. IEEE Applied Imagery Pattern Recognition Workshop.
Fan, L., Xiong, L., (2014). An adaptive approach to real-time aggregate monitoring with differential
privacy. IEEE Trans. Knowledge Data Eng., 26(9): 2094-2106.

2531

Multi-INT Query Language for DDDAS Designs

Alex Aved and Erik Blasch

Foote, J. (1997). Content-based retrieval of music and audio, Proc. SPIE, Vol. 3229.
Fujimoto, R., Guin, A., Hunter, M., et al. (2014). A dynamic data driven application system for
vehicle tracking, International Conference on Computational Science, pp. 1203-1215.
Guttman, A. (1984). R-trees: a dynamic index structure for spatial searching (Vol. 14). ACM.
Hristescu, G., & Farach-Colton, M. (1999). Cluster-preserving embedding of proteins. Technical
Report 99-50, Computer Science Department, Rutgers University.
Ide, I., Yamamoto, K., & Tanaka, H. (1999). Automatic video indexing based on shot classification.
Advanced Multimedia Content Processing, 87–102.
Jain, A. K., Duin, R. P. W., & Mao, J. (2000). Statistical pattern recognition: A review. IEEE
Transactions on Pattern Analysis and Machine Intelligence, 22(1), 4–37.
Jain, R., & Hampapur, A. (1994). Metadata in video databases. ACM Sigmod Record, 23(4), 27–33.
Liu, B., Chen, Y., et al. (2014). Information Fusion in a Cloud Computing Era: A Systems-Level
Perspective, IEEE Aerospace and Electronic Systems Mag., Vol. 29, No. 10, pp. 16–24, Oct. .
Liu, Z., Blasch, E., Xue, Z., Zhao, J., Laganiere, R., & Wu, W. (2012). Objective assessment of
multiresolution image fusion algorithms for context enhancement in night vision: a comparative
study. IEEE Tr. on Pattern Analysis and Machine Intelligence, , 34(1), 94–109.
Mendoza-Schrock, O., Patrick, J. A., et al. (2009). Video Image Registration Evaluation for a Layered
Sensing Environment. Proc. IEEE Nat. Aerospace Electronics Conf (NAECON).
Mei, X., Ling, H, et al. (2013). Efficient Minimum Error Bounded Particle Resampling L1 Tracker
with Occlusion Detection, IEEE Trans. on Image Processing, Vol. 22, Issue 7, 2661 – 2675.
Nagasaka, A., Tanaka, Y. (1992). Automatic video indexing and full-video search for object
appearances.
Nievergelt, J., Hinterberger, H., & Sevcik, K. C. (1984). The grid file: An adaptable, symmetric
multikey file structure. ACM Transactions on Database Systems (TODS), 9(1), 38–71.
Pieprzyk, J., & Sadeghiyan, B. (2001). Design of hashing algorithms. Springer-Verlag New York, Inc.
Samet, H. (1990). The design and analysis of spatial data structures (Vol. 85). Addison-Wesley
Reading MA.
Samet, H. (2006). Foundations of multidimensional and metric data structures. Morgan Kaufmann.
Scheuermann, P., & Ouksel, M. (1982). Multidimensional B-trees for associative searching in
database systems. Information Systems, 7(2), 123–137.
Snoek, C. G. M., & Worring, M. (2005). Multimodal video indexing: A review of the state-of-the-art.
Multimedia Tools and Applications, 25(1), 5–35.
Sudusinghe, K., Cho, I., van der Schaar, M., Bhattacharyya, S. S., (2014). Model based design
environment for data-driven embedded signal processing systems. Int’l Conf. on Computational
Science, pp. 1193–1202.
Tantaoui, M. A., Hua, K. A., & Do, T. T. (2004). BroadCatch: a periodic broadcast technique for
heterogeneous video-on-demand. Broadcasting, IEEE Transactions on, 50(3), 289–301.
Taylor, S. (2007). Optimizing Applications for Multi-Core Processors, Using the Intel Integrated
Performance Primitives. Intel Press.
Wang, J. T. L., Wang, X., Lin, K. I., Shasha, D., Shapiro, B. A., & Zhang, K. (1999). Evaluating a
class of distance-mapping algorithms for data mining and clustering. ACM SIGKDD
international conference on Knowledge discovery and data mining, pp. 307–311.
Wang, Y., Liu, Z., & Huang, J. C. (2000). Multimedia content analysis-using both audio and visual
clues. Signal Processing Magazine, IEEE, 17(6), 12–36.
Wold, E., Blum, T., Keislar, D., Wheaten, J. (1996). Content-based classification, search, and retrieval
of audio. MultiMedia, IEEE, 3(3), 27–36.
Wu, Y., Blasch, E, Chen, G., Bai, L., Ling, H. (2011). Multiple Source Data Fusion via Sparse
Representation for Robust Visual Tracking. Int’l Conf. on Information Fusion.
Wu, Y., Wang, J., Cheng, J., Lu, H., et al., (2012). Real-Time Probabilistic Covariance Tracking with
Efficient Model Update. IEEE Trans. on Image Processing, 21(5):2824-2837.

2532

