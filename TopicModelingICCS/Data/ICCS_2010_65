Procedia Computer
Science
Procedia Computer
001(2010)
1–9
Procedia
ComputerScience
Science
(2012)
279–287

www.elsevier.com/locate/procedia

International Conference on Computational Science, ICCS 2010

On twisted factorizations of block tridiagonal matrices
Wilfried N. Gansterera , Gerhard K¨onigb
a University

of Vienna, Research Lab Computational Technologies and Applications
of Vienna, Department of Computational Biological Chemistry

b University

Abstract
Non-symmetric and symmetric twisted block factorizations of block tridiagonal matrices are discussed. In contrast
to non-blocked factorizations of this type, localized pivoting strategies can be integrated which improves numerical
stability without causing any extra ﬁll-in. Moreover, the application of such factorizations for approximating an
eigenvector of a block tridiagonal matrix, given an approximation of the corresponding eigenvalue, is outlined. A
heuristic strategy for determining a suitable starting vector for the underlying inverse iteration process is proposed.
c 2012 Published by Elsevier Ltd. Open access under CC BY-NC-ND license.
⃝

Keywords: twisted factorizations, twisted block factorizations, block tridiagonal eigenvalue problem, eigenvector
computation

1. Introduction
In this paper, we discuss strategies for eﬃciently computing twisted block factorizations of a block tridiagonal
matrix W(p) with p square diagonal blocks. Such factorizations form the basis for approximating eigenvectors of
W(p) if approximations of the corresponding eigenvalues are available.
In the most general setting considered here, W(p) does not have to be symmetric:
⎞
⎛
⎟⎟⎟
⎜⎜⎜ B1 C1
⎟⎟⎟
⎜⎜⎜A B
C2
2
⎟⎟⎟
⎜⎜⎜ 2
⎜
..
..
..
⎟⎟⎟⎟ .
W(p) := ⎜⎜⎜⎜
(1)
.
.
.
⎟⎟⎟
⎜⎜⎜
A p−1 B p−1 C p−1 ⎟⎟⎟⎟⎠
⎜⎜⎜⎝
Ap
Bp

The dimensions bi of the p quadratic diagonal blocks Bi (i = 1, . . . , p) are called block sizes in the following and
determine shape and size of the p − 1 subdiagonal blocks Ai (i = 2, . . . , p), and of the p − 1 superdiagonal blocks Ci
(i = 1, . . . , p − 1). In many situations, symmetric W(p) is of particular interest. In this case Bi = Bi for i = 1, . . . , p,
and Ci = Ai+1 for i = 1, . . . , p − 1.
In this paper, we discuss non-symmetric as well as symmetry-preserving factorizations of W(p) and compare them
to related work in the literature. The general structure of these factorizations is W(p) = PLDU (in the non-symmetric
case) or W(p) = PLDL P (in the symmetric case), where L and U are block tridiagonal with identity matrices along
the diagonal, D is block diagonal, and P is also block diagonal with permutation matrices along the diagonal.
Email addresses: wilfried.gansterer@univie.ac.at (Wilfried N. Gansterer), gerhard@mdy.univie.ac.at (Gerhard K¨onig)

c 2012 Published by Elsevier Ltd. Open access under CC BY-NC-ND license.
1877-0509 ⃝
doi:10.1016/j.procs.2010.04.031

280

W.N.
G. G.
K¨oK¨
nig
/ Procedia
1 (2012)
279–287
W.N. Gansterer,
Gansterer and
onig
/ ProcediaComputer
Computer Science
Science 00
(2010) 1–9

2

1.1. Motivation
Since block tridiagonal structure can be considered a generalization of band structure, matrices as deﬁned in (1)
arise in many situations. In particular, they can be an intermediate result of a reduction step of a general dense matrix,
for example, in a block tridiagonalization process [1] or in a bandwidth reduction process [2, 3]. In particular, the
context of block tridiagonalization as a preprocessing step for computing spectral information of a dense symmetric
matrix provides motivation for investigating suitable factorizations of W(p), as outlined in the following.
The block tridiagonal divide-and-conquer (BD&C) method [4, 5] allows for computing eigenvalues and eigenvectors of symmetric W(p) without reducing it to tridiagonal form. However, it turns out that in some constellations the
accumulation of the eigenvector information in this divide-and-conquer process can become the main performance
limiting factor—in particular, in cases where reduced accuracy approximations (with respect to the “full” accuracy
determined by the problem instance and its condition as well as by the given ﬂoating-point arithmetic) are not suﬃcient. Thus, a central question arising in this context is the following: Given approximate eigenvalues, is it possible to
ﬁnd eﬃcient alternatives to the eigenvector accumulation process in the BD&C method for eﬃciently computing the
corresponding eigenvectors of a symmetric block tridiagonal matrix ?
The idea pursued in this paper is based on representing W(p) as a product of two block tridiagonal matrices
with special structure (for every diagonal block, either the superdiagonal block or the subdiagonal block is nonzero).
Equivalently, W(p) can be represented as a product of three matrices (a block diagonal and two block tridiagonals
with the structure mentioned before, but identity matrices along the diagonal). Based on such a representation, the
idea is to design a fast and eﬃcient inverse iteration process for computing a desired eigenvector.
The candidate representations of W(p) are twisted block factorizations, a blocked generalization of twisted factorizations of tridiagonal matrices, which have been considered earlier (see, for example, [6]). Among all possible
twisted block factorizations of shifted W(p) one is selected as the basis for inverse iteration with a properly chosen
starting vector. This idea is motivated by central components of the MRRR algorithm for computing eigenvectors of
a symmetric tridiagonal matrix summarized in [7].
In this paper, we discuss algorithms for computing twisted block factorizations of W(p). We distinguish nonsymmetric and symmetry-preserving factorizations and compare them to approaches appearing in the literature. When
the ultimate objective is to compute an eigenvector of W(p) based on these factorizations and on a given approximation
of the corresponding eigenvalue, central algorithmic questions are (i) how to pick one of the twisted block factorizations and—related to that—(ii) how to choose the starting vector for the inverse iteration process. We also discuss
two strategies based on the factorization methods discussed, one of them based on [6], the other based on a heuristic
which seems not to have been investigated before.
1.2. Related Work
Most existing studies concerning twisted factorizations focussed on tridiagonal matrices and were motivated by
the objective to eﬃciently calculate their eigenvectors [7, 8, 9, 10, 6, 11, 12].
In 1990, Demmel and Kahan showed that the Cholesky factorization of a tridiagonal matrix into two bidiagonals
can be used to compute all eigenvalues of a symmetric deﬁnite tridiagonal matrix to high accuracy [13]. Later, it
was also shown that bidiagonal LDL representations of tridiagonal matrices often determine eigenvalues with high
relative accuracy [8].
Such results formed the basis for the development of very eﬃcient methods for the calculation of eigenvectors
of tridiagonal matrices. Based on Fernando’s solution to Wilkinson’s problem [9] Parlett and Dhillon [6] suggested
to use twisted factorizations of tridiagonal matrices to determine a good starting vector for inverse iteration. This is
justiﬁed by the fact that the position of the largest component in the eigenvector to be computed is associated with
the minimal diagonal element of the twisted factorizations. The proper choice of a starting vector based on twisted
factorizations leads to stability and rapid convergence of the inverse iteration process.
Less work has been done so far on banded or block tridiagonal matrices. Meurant [14] reviewed the connections
between the inverse of a block tridiagonal matrix and its twisted factorization, thereby deducing formulas for the
blocks of the inverse of a block tridiagonal matrix. Parlett and Dhillon [6] dicussed a blocked extension of the
tridiagonal case and they also suggested a strategy for determining a good initial approximation to an eigenvector of
W(p).

281
3

W.N.W.N.
Gansterer,
G. and
K¨onig
/ Procedia
Computer
Science
1 (2012)
279–287
Gansterer
G. K¨
onig / Procedia
Computer
Science
00 (2010)
1–9

Very recently, V¨omel and Slemons published a theoretical treatment of twisted factorizations of banded matrices [15]. They gave a proof of the existence of two twisted factorizations of a given banded matrix by using a double
factorization of the twisted block. They also discuss the connections to the inverse of the matrix and consider the use
of their twisted factorizations for inverse iteration on band matrices.
However, V¨omel and Slemons focus on non-blocked twisted factorizations of a banded matrix. When pivoting
is introduced for enhancing numerical stability, their approach does in general not preserve the structure of block
tridiagonal or banded matrices due to ﬁll-in. In order to address both aspects—numerical stability and preservation
of block tridiagonal structure—our focus is on twisted block factorizations. By integrating localized pivoting within
blocks in the factorization process we can improve numerical stability without causing ﬁll-in. The approach discussed
in this paper is more directly related to the twisted block factorizations indicated in [6]. However, this paper does
not explicitly discuss the pivoting issue. Moreover, we propose a computationally less expensive alternative for
determining a starting vector for inverse iteration.
1.3. Synopsis
In Section 2, forward and backward block LU factorizations of a block tridiagonal matrix with local pivoting
are reviewed. In Section 3 non-symmetric as well as symmetry-preserving twisted block factorizations of W(p) are
discussed. The computation of an eigenvector to a given eigenvalue approximation using inverse iteration based on
these factorizations is the topic of Section 4, where we also distinguish several strategies for choosing a starting vector.
Conclusions are given in Section 5.
2. Block LU Factorizations of a Block Tridiagonal Matrix
As outlined before, the basic idea discussed in this paper is the factorization of block tridiagonal W(p) into two
block tridiagonals or, alternatively, into two block tridiagonals and a block diagonal. In this section, we summarize
block LU factorizations of W(p) with integrated local pivoting. This forms the basis for the twisted block factorizations of W(p) discussed in Section 3. We assume that all factorizations outlined in these two sections exist.
2.1. Scalar LU Factorization
Scalar LU factorization decomposes any given matrix M into a unit lower triangular matrix L and an upper triangular matrix U [16]. In the standard forward process, the subdiagonal elements in column k of the matrix M are
eliminated by multiplication with an elimination matrix whose subdiagonal elements of the respective column k are
given as Mik /Mkk (i = k + 1, . . . , n). In exact arithmetic, this process breaks down if at some point Mkk = 0. In
ﬂoating-point arithmetic, stability problems may arise if Mkk is very small. In order to cope with this problem, pivoting strategies have been developed. Partial pivoting, for example, identiﬁes the largest element in absolute value of the
current column k and interchanges the corresponding rows of the submatrix yet to be processed in order to make Mkk
as large as possible. Formally, these row interchanges can be represented as the application of a permutation matrix P
from the left:
M = PLU.
In the following, we generalize this scalar forward LU factorization process to a block-based LU factorization process
with local pivoting for block tridiagonal W(p).
2.2. Forward Block LU Factorization
When generalized to a block tridiagonal matrix W(p) deﬁned as in (1), the resulting factors L and U will be lower
and upper block bidiagonal, respectively. We illustrate the process for p = 4. Based on the ansatz
⎛
⎜⎜⎜P1
⎜⎜⎜
W(4) = ⎜⎜⎜⎜
⎜⎜⎝

P2

P3

P4

⎞⎛
⎟⎟⎟ ⎜⎜⎜ L1
⎟⎟⎟⎟ ⎜⎜⎜⎜ M2
⎟⎟⎟ ⎜⎜⎜
⎟⎠⎟ ⎜⎝⎜

L2
M3

L3
M4

L4

⎞⎛
⎟⎟⎟ ⎜⎜⎜U1
⎟⎟⎟⎟ ⎜⎜⎜⎜
⎟⎟⎟ ⎜⎜⎜
⎟⎠⎟ ⎜⎝⎜

N1
U2

N2
U3

⎞
⎟⎟⎟
⎟⎟⎟
⎟⎟
N3 ⎟⎟⎟⎠⎟
U4

(2)

282

W.N.
G. G.
K¨oK¨
nig
/ Procedia
1 (2012)
279–287
W.N. Gansterer,
Gansterer and
onig
/ ProcediaComputer
Computer Science
Science 00
(2010) 1–9

⎛
⎜⎜⎜ P1 L1 U1
⎜⎜⎜P M U
= ⎜⎜⎜⎜ 2 2 1
⎜⎜⎝

P1 L1 N1
P2 L2 U2 + P2 M2 N1
P3 M3 U2

P2 L2 N2
P3 L3 U3 + P3 M3 N2
P4 M4 U3

4

P3 L3 N3
P4 L4 U4 + P4 M4 N3

⎞
⎟⎟⎟
⎟⎟⎟
⎟⎟⎟ ,
⎟⎟⎟⎠

the deﬁning equations for the block LU factorization process with local pivoting can be derived block by block.
Starting from the top left corner (in “forward” direction), the ﬁrst step is to factorize B1 = P1 L1 U1 using partial
pivoting. Then, from the equations
P1 L1 N1
P2 M2 U1

= C1
= A2

(3)

the matrices N1 and M2 := P2 M2 can be computed as solutions of two triangular systems. B1 has to be non singular,
so that for arbitrary C1 and A2 these linear systems have a unique solution. Note that the permutation matrix from the
next partial pivoting step appears already implicitly in M2 without being known explicitly.
Rewriting the next equation B2 = P2 L2 U2 + M2 N1 into
(4)

B2 − M2 N1 = P2 L2 U2

reveals that the next step is to factorize the Schur complement B2 − M2 N1 with partial pivoting in order to compute P2 ,
L2 , and U2 . Note that only at this point P2 is computed explicitly (as mentioned before, so far it was only contained
implicitly in the solution of Eqn. (3)).
Now we can proceed with solving linear systems for N2 and M3 := P3 M3 , factorizing B3 − M3 N2 , solving for N3
and M4 := P4 M4 , and ﬁnally factorizing B4 − M4 N3 (again assuming that all linear systems have a unique solution).
As a result, the entire block LU factorization (2)in forward direction of W(4) has been constructed.
2.3. Backward Block LU Factorization
The block tridiagonal LU factorization can also be performed backwards, starting from the factorization of the
lower right block B p . In this case, the resulting L and U will be upper and lower block bidiagonal, respectively.
Again, we illustrate the process for p = 4. Based on the ansatz
⎛
⎜⎜⎜P1
⎜⎜⎜
W(4) = ⎜⎜⎜⎜
⎜⎜⎝

P2

⎛
⎜⎜⎜P1 L1 U1 + P1 M1 N2
⎜⎜⎜
P2 L2 N2
= ⎜⎜⎜⎜
⎜⎜⎝

P3

P4

⎞⎛
⎟⎟⎟ ⎜⎜⎜L1
⎟⎟⎟ ⎜⎜⎜
⎟⎟⎟ ⎜⎜⎜
⎟⎟⎟ ⎜⎜⎜
⎠⎝

M1
L2

⎞⎛
⎟⎟⎟ ⎜⎜⎜U1
⎟⎟⎟ ⎜⎜⎜ N
⎟⎟ ⎜⎜ 2
M3 ⎟⎟⎟⎟⎠ ⎜⎜⎜⎜⎝
L4

M2
L3

P1 M1 U2
P2 L2 U2 + P2 M2 N3
P3 L3 N3

U2
N3

U3
N4

P2 M2 U3
P3 L3 U3 + P3 M3 N4
P4 L4 N4

we start with factorizing B4 = P4 L4 U4 using local partial pivoting. From the equations
P 3 M3 U 4
P4 L4 N4

U4

⎞
⎟⎟⎟
⎟⎟⎟
⎟⎟⎟
⎟⎟⎟
⎠

(5)

⎞
⎟⎟⎟
⎟⎟⎟
⎟⎟
P3 M3 U4 ⎟⎟⎟⎟⎠
P4 L4 U4

= C3
=

A4

the matrices N4 and M3 := P3 M3 can be computed as solutions of two linear systems, assuming (as before) that B4 is
non singular. Then, B3 can be rewritten as B3 = P3 L3 U3 + M3 N4 , which leads to
B3 − M3 N4 = P3 L3 U3 .
Thus, factorizing the Schur complement B3 − M3 N4 with local partial pivoting yields P3 , L3 , and U3 . Proceeding
analogously to the forward case (but in backward direction) determines the remaining unknown submatrices in the
factorization (5).

283
5

W.N.W.N.
Gansterer,
G. and
K¨onig
/ Procedia
Computer
Science
1 (2012)
279–287
Gansterer
G. K¨
onig / Procedia
Computer
Science
00 (2010)
1–9

3. Twisted Block Factorizations
A twisted block factorization of W(p) is the result of combining some forward factorization steps as reviewed
in Section 2.2 with some backward factorization steps as reviewed in Section 2.3. We use the abbreviation TF(k)
(k = 1, 2, . . . , p) for a twisted block factorization which combines k − 1 forward steps and p − k backward steps. Note
that for k = p we get the forward factorization from Section 2.2, and for k = 1 we get the backward factorization
from Section 2.3. We denote the diagonal block at position k, where forward and backward elimination steps meet, as
“twisted block”.
In Section 3.1, we discuss non-symmetric twisted block factorizations with local pivoting, which yield nonsymmetric representations of W(p). Symmetry-preserving variants, which may sometimes be relevant for symmetric
W(p), are the topic of Section 3.2.
3.1. Unsymmetric Twisted Block Factorizations
In the following, we illustrate TF(3) of W(4), where two elimination steps are done in forward direction, and one in
backward direction. In order to distinguish the steps done in forward and backward direction, the blocks constructed
in the forward direction are marked by the superscript “+ ”, while the blocks constructed in the backward direction are
marked by the superscript “− ”. Note that forward and backward elimination processes are completely independent of
each other until the computation of the blocks in the row where forward and backward factorization meet.
Based on the ansatz
⎞⎛ +
⎞⎛ +
⎛ +
⎞
⎟⎟⎟ ⎜⎜⎜ L1
⎟⎟⎟ ⎜⎜⎜U1 N1+
⎜⎜⎜P1
⎟⎟⎟
⎟
⎜
⎟
⎜
+
+
+
+
+
⎟
⎟
⎜
⎜⎜⎜⎜
⎟⎟⎟⎟
⎜
P2
M2 L 2
U2 N2
⎟
⎜
⎟
⎜
⎟
⎜
⎟
⎜
⎟⎟⎟ ⎜⎜⎜
⎟⎜
⎟⎟⎟
W(4) = ⎜⎜⎜
(6)
P3
M3+ L3 M3− ⎟⎟⎟⎟⎠ ⎜⎜⎜⎜⎝
U3
⎟⎟⎠ ⎜⎜⎝
⎜⎜⎝
⎟⎟⎠
−
−
−
−
P4
L4
N4 U4
⎛ + + +
⎜⎜⎜ P1 L1 U1
⎜⎜⎜P+ M + U +
= ⎜⎜⎜⎜ 2 2 1
⎜⎜⎝

P+1 L1+ N1+
P+2 L2+ U2+ + P+2 M2+ N1+
P3 M3+ U2+

P3 L3 U3 +

P2 L2+ N2+
P3 M3+ N2+ +
P−4 L4− N4−

P3 M3− N4−

⎞
⎟⎟⎟
⎟⎟⎟
⎟,
− −⎟
P3 M3 U4 ⎟⎟⎟⎟⎠
P−4 L4− U4−

we again derive the deﬁning equations block by block.
In the forward direction, the ﬁrst step is to factorize B1 = P+1 L1+ U1+ . Then, N1+ and M2+ := P+2 M2+ can be computed
as solutions of two linear systems. As in (4), the Schur complement of B2 is factorized for computing P+2 , L2+ , and U2+ .
Using this information, N2+ and M3+ := P3 M3+ are computed as solutions of two linear systems.
At this point, the forward part of TF(3) is completed, and the next steps are conducted in backward direction.
After factorizing B4 = P−4 L4− U4− , N4− and M3− := P3 M3− are computed as solutions of two linear systems.
Finally, we can work on the third block row where both factorizations meet. The diagonal block B3 in this row
can be expressed as B3 = P3 L3 U3 + M3+ N2+ + M3− N4− . Thus, from factorizing
B3 − M3+ N2+ − M3− N4− = P3 L3 U3
we obtain P3 , L3 , and U3 and thus have determined all blocks in factorization (6).
In some situations (for example, when computing eigenvectors of W(p) as discussed in Section 4) it is convenient
to reformulate the factorization (6) as
W(4) = PLDU
with block diagonal D and block tridiagonals L and U, which have identity matrices along the diagonal. In our
example, for TF(3) of W(4)
⎞
⎛
I
⎟⎟⎟
⎜⎜⎜
⎟⎟⎟
⎜⎜⎜ + + −1
⎟⎟⎟
⎜⎜⎜ M2 L1
I
L = ⎜⎜⎜
⎟⎟ ,
−1
−1 ⎟
+
+
−
−
⎜⎜⎜
I M4 L4 ⎟⎟⎟⎟
M3 L2
⎜⎝
⎠
I

284

W.N.
G. G.
K¨oK¨
nig
/ Procedia
1 (2012)
279–287
W.N. Gansterer,
Gansterer and
onig
/ ProcediaComputer
Computer Science
Science 00
(2010) 1–9

⎛ + +
⎜⎜⎜L1 U1
⎜⎜⎜
D = ⎜⎜⎜⎜
⎜⎜⎝

and

⎛
⎜⎜⎜I
⎜⎜⎜
⎜⎜⎜
U = ⎜⎜⎜⎜
⎜⎜⎜
⎜⎜⎝

U1+

L2+ U2+

−1

I

L3 U3

N1+
U2+
U4−

−1

I
−1

6

⎞
⎟⎟⎟
⎟⎟⎟
⎟⎟⎟ ,
⎟⎟⎟⎠
−

L4− U4

N2+
N3−

I

⎞
⎟⎟⎟
⎟⎟⎟
⎟⎟⎟
⎟⎟⎟ .
⎟⎟⎟
⎟⎟⎟
⎠

3.2. Symmetric Twisted Block Factorizations
So far, the discussion was dominated by non-symmetric LU-based factorizations. Nevertheless, for symmetric
W(p) we can also construct a symmetric factorization W(p) = PLDL P a with block diagonal D. This construction
proceeds analogously to the method summarized in Section 3.1. The main diﬀerence is that a symmetric indeﬁnite
factorization, for example, a Bunch-Kaufman factorization [17], has to be used for factorizing diagonal blocks.
It suﬃces to illustrate the ﬁrst two steps (in order to simplify the notation, we omit the superscripts for the direction
of the factorization process): In the forward direction, we ﬁrst factorize B1 = P1 L1 D1 L1 P1 , where P1 is a permutation
and D1 is a direct sum of 1 × 1 and 2 × 2 pivot blocks. Then, M1 := P2 M1 can be computed as the solution of the linear
system M1 D1 L1 P1 = A2 , followed by the symmetric factorization of the updated diagonal block B2 − P2 M1 D1 M1 P2 .
The process is continued until the twisted block, and performed analogously in the backward direction starting with
the symmetric factorization of B p .
4. Computing an Eigenvector of W(p)
In this section we outline an approach for approximating an eigenvector v of W(p) based on the twisted block
factorizations summarized in Section 3, assuming that an approximation λˆ of the corresponding eigenvalue λ is available. Analogously to the ideas leading to [7] the approach is based on one step (or a few steps) of inverse iteration
ˆ It uses a properly chosen twisted block factorization of the block tridiagonal matrix
on the shifted matrix W(p) − λI.
ˆ for determining a suitable starting vector.
W(p) − λI
4.1. Review Inverse Iteration
Given the eigenvalue approximation λˆ ≈ λ (λˆ is called “shift” in the following), inverse iteration computes an
approximation vˆ for the eigenvector v as follows:
1. initialize vˆ (0) , i := 0
2. repeat
ˆ y(i+1) = vˆ (i)
3.
solve W(p) − λI
4.
vˆ (i+1) := y(i+1) / y(i+1)
5.
i := i + 1
6. until convergence

2

In general, a random starting vector vˆ (0) is considered appropriate [18]. However, for the special structure of W(p)
considered in this paper it is possible to determine a better starting vector from twisted factorizations of W(p), as
already indicated in [6].

W.N.W.N.
Gansterer,
G. and
K¨onig
/ Procedia
Computer
Science
1 (2012)
279–287
Gansterer
G. K¨
onig / Procedia
Computer
Science
00 (2010)
1–9

285
7

4.2. Inverse Iteration Based on Twisted Block Factorizations
ˆ = PLDL P as discussed in Section 3.2, the solver step 3 of the
Given a twisted block factorization W(p) − λI
inverse iteration process involves solving three linear systems, one block diagonal system and two combined forward
and back (block) substitutions.
Note that except for the extreme cases TF(p) and TF(1), the factor L has nonzero blocks above and below the main
block diagonal. Thus, in the general case, solving a linear system with one of these factors resulting from a twisted
block factorization TF(k) involves forward substitution down to the block number k − 1, and backward substitution up
to the block number p − k until the two directions meet at block number k.
So far, we have not speciﬁed which one of the p possible twisted block factorizations to use in the inverse iteration process. The choice of one of these factorizations determines the starting vector vˆ (0) . The idea is that with a
properly chosen starting vector very few iterations (ideally only one) of the inverse iteration process should suﬃce for
computing a good approximation of the eigenvector v.
4.3. Choice of Starting Vector
In the following, we mention two strategies for determining the starting vector vˆ (0) for the inverse iteration process
ˆ We restrict ourselves to starting vectors vˆ (0) with entry one in position j and zeros in all other positions.
on W(p) − λI.
When solving a block bidiagonal system with such a vector vˆ (0) as the right hand side, all entries of the solution vector
below position j will be zero. Thus, this position j is in fact the “starting position” of the back- or forward substitution
process and we can identify this starting position with the starting vector vˆ (0) (since j completely determines vˆ (0) ).
The ﬁrst (SVD-based) strategy has been mentioned in [6]. It determines a good initial approximation to the
eigenvector sought based on the block with the smallest singular value over all diagonal blocks in the matrices D over
all possible twisted block factorizations PLDU.
Due to the potentially higher multiplicity of eigenvalues in block tridiagonal matrices (compared to irreducible
tridiagonal matrices), such block-oriented strategies seem important for another reason: Identifying a starting block
allows for determining bi diﬀerent scalar starting positions and thus potentially for approximating bi diﬀerent eigenvectors for an eigenvalue with multiplicity higher than one. Nevertheless, [15] already hints at a possible drawback
of this strategy for determining a starting vector: in some situations, the requirement of computing singular value
decompositions may be a limiting factor, for example, for large block sizes bi (relative to n).
Thus, we propose an alternative heuristic scalar strategy: Determine the position m of the diagonal element with
minimum absolute value over all diagonal elements from the blocks Ui in the matrices D over all possible twisted block
PLDU factorizations. Then, choose the starting vector vˆ 0 for the inverse iteration process with entry one at position
m, and use the factorization which contains this minimum diagonal element |Dmm | for solving the linear system. This
heuristic has advantages in computational cost over the one derived from [6], but it remains to be investigated whether
any analytical results can be derived and whether it achieves competitive numerical results in practical computations.
4.4. Numerical Example
The applicability of the approach based on twisted block factorizations is demonstrated with a numerical example.
Using a preliminary draft implementation of the twisted block factorizations discussed in Section 3, we applied the
SVD-based strategy from Section 4.3 for determining starting vectors and computed all eigenvectors of a random
symmetric matrix W(p) with n = 1000 and bi = 5 (i = 1, . . . , p). The eigenvalues used as shifts were computed using
the routine LAPACK/dsyevd.
The numerical results are very encouraging. The average residual over all computed eigenvectors was 1.3277 ·
10−14 , and only one residual was larger than 10−12 (see Fig. 1). The average scalar product between corresponding
eigenvectors computed by our method and computed by LAPACK/dsyevd was 1.2234 · 10−16 .
Moreover, preliminary experiences indicate that the method based on twisted block factorizations can achieve
improvements in runtime performance compared to competing tridiagonalization-based approaches using LAPACK
routines.

286

W.N.
G. G.
K¨oK¨
nig
/ Procedia
1 (2012)
279–287
W.N. Gansterer,
Gansterer and
onig
/ ProcediaComputer
Computer Science
Science 00
(2010) 1–9

8

Distribution of 1000 Residuals
160
140
120
count

100
80
60
40
20
0
−15

−14.5

−14

−13.5

−13

log10 (residual)

−12.5

−12

−11.5

Figure 1: Residuals for all eigenvectors of a random symmetric matrix W(p) with n = 1000 and bi = 5 (i = 1, . . . , p) computed via twisted block
factorizations and the SVD-based strategy for determining the starting vector (bin size 0.08 at the log-scale).

5. Conclusions
We have summarized and discussed strategies for computing non-symmetric as well as symmetric twisted factorizations of block tridiagonal matrices. In contrast to some earlier approaches, we focussed on twisted block factorizations which integrate localized pivoting while preserving the original structure. Moreover, we discussed the
application of such factorizations in the context of approximating eigenvectors of block tridiagonal matrices based
on inverse iteration. For this context, we proposed a heuristic for deriving a suitable starting vector for the inverse
iteration process which may be an interesting alternative to an earlier approach in the literature.
In the future, we will on the one hand work on the analysis of the proposed heuristic in order to analyze the quality
of the resulting starting vector. On the other hand, we will work on questions related to eﬃcient implementations of
the concepts developed in this paper, on their experimental evaluation and on the comparison with competing concepts
in terms of the resulting quality of the eigenvector approximation as well as in terms of computational eﬃciency.
References
[1] Y. Bai, W. N. Gansterer, R. C. Ward, Block tridiagonalization of “eﬀectively” sparse symmetric matrices, ACM Trans. Math. Softw. 30 (2004)
326–352.
[2] C. H. Bischof, B. Lang, X. Sun, Parallel tridiagonalization through two-step band reduction, in: Proceedings of the 1994 Scalable HighPerformance Computing Conference, Washington D.C., 1994, pp. 23–27.
[3] C. H. Bischof, B. Lang, X. Sun, A framework for symmetric band reduction, ACM Trans. Math. Software 26 (2000) 581–601.
[4] W. N. Gansterer, R. C. Ward, R. P. Muller, An extension of the divide-and-conquer method for a class of symmetric block-tridiagonal
eigenproblems, ACM Trans. Math. Softw. 28 (2002) 45–58.
[5] W. N. Gansterer, R. C. Ward, R. P. Muller, W. A. Goddard, III, Computing approximate eigenpairs of symmetric block tridiagonal matrices,
SIAM J. Sci. Comput. 25 (2003) 65–85.
[6] B. N. Parlett, I. S. Dhillon, Fernando’s solution to Wilkinson’s problem: An application of double factorization, Linear Algebra Appl. 267
(1997) 247–279.
[7] I. S. Dhillon, B. N. Parlett, C. V¨omel, The design and implementation of the MRRR algorithm, ACM Trans. Math. Softw. 32 (4) (2006)
533–560.
[8] I. S. Dhillon, B. N. Parlett, Multiple representations to compute orthogonal eigenvectors of symmetric tridiagonal matrices, Linear Algebra
Appl. 387 (2004) 1–28.
[9] K. V. Fernando, On computing an eigenvector of a tridiagonal matrix .1. Basic results, SIAM J. Matrix Anal. Appl. 18 (4) (1997) 1013–1034.
[10] B. N. Parlett, For tridiagonals T replace T with LDLt, J. Comput. Appl. Math. 123 (1-2) (2000) 117–130.
[11] B. N. Parlett, I. S. Dhillon, Relatively robust representations of symmetric tridiagonals, Linear Algebra Appl. 309 (2000) 121–151.
[12] B. Parlett, O. Marques, An implementation of the dqds algorithm (positive case), Linear Algebra Appl. 309 (1-3) (2000) 217–259.
[13] J. W. Demmel, W. Kahan, Accurate singular values of bidiagonal matrices, SIAM J. Sci. Stat. Comput. 11 (1990) 873–912.
[14] G. Meurant, A review on the inverse of symmetric tridiagonal and block tridiagonal matrices, SIAM J. Matrix Anal. Appl. 13 (3) (1992)
707–728.
[15] C. V¨omel, J. Slemons, Twisted factorization of a banded matrix, BIT 49 (2) (2009) 433–447.
[16] G. H. Golub, C. F. Van Loan, Matrix Computations, 3rd Edition, Johns Hopkins University Press, Baltimore, MD, 1996.

W.N.W.N.
Gansterer,
G. and
K¨onig
/ Procedia
Computer
Science
1 (2012)
279–287
Gansterer
G. K¨
onig / Procedia
Computer
Science
00 (2010)
1–9

287
9

[17] J. R. Bunch, L. C. Kaufman, Some stable methods for calculating inertia and solving symmetric linear equations, Math. Comp. 31 (1977)
163–179.
[18] I. C. F. Ipsen, Computing an eigenvector with inverse iteration, SIAM Rev. 39 (1997) 254–291.

