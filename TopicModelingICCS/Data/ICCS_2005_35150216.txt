A Multiresolutional Approach for Facial Motion
Retargetting Using Subdivision Wavelets
Kyungha Min and Moon-Ryul Jung
Dept. of Media Technology,
Graduate School of Media Communications, Sogang Univ., Seoul, Korea
{minkh, moon}@sogang.ac.kr

Abstract. We propose a new facial motion retargetting method using
wavelet-based multiresolutional analysis of triangular meshes. We deﬁne
the displacement of the source model as the diﬀerence between the neutral and the current expressions of the source model, and decompose it to
the displacement of the base mesh and the “displacements” of the wavelet
coeﬃcients. We compute the wavelet coeﬃcients of a mesh by using the
wavelet basis functions derived from a mesh subdivision scheme. To get
the motion of the target model, we add the displacement of the source
model to the target model level by level starting from the base mesh.
The wavelet-based representation of mesh facilitates the comparison between two expressions of the same model and the comparison between
the expressions of the source and target models.

1

Introduction

Digital facial animation is one of the most popular research areas in computer
graphics. The development of entertainment markets such as computer games,
animations and movies accelerates the progress of digital facial animation techniques to build an eﬃcient framework of realistic facial animation. In human
body animation, motion retargetting, which creates new motions of the target
model from the motion data of the source model, has introduced an innovative
tool for creating realistic character animation [3]. The major diﬃculty of applying motion retargetting to facial models is that faces do not have the obvious
control parameters to represent and control facial motions such as joint angles
for body motion [7].
The problem of motion retargetting can be formulated as the problem of obtaining the current expression of the target model by adding the scaled version
of the displacement of the source model to the neutral expression of the target
model(See Fig. 1). To handle the problem of facial motion retargetting, we propose a multiresolution approach. To materialize this approach, we present two
key ideas.
This research has been supported by the fund (R01-2002-000-00311-01) from KOSEF
(Korea Science and Engineering Foundation).
V.S. Sunderam et al. (Eds.): ICCS 2005, LNCS 3515, pp. 216–223, 2005.
c Springer-Verlag Berlin Heidelberg 2005

A Multiresolutional Approach for Facial Motion Retargetting

217

Fig. 1. The process of motion retargetting: Add the scaled version of the displacement
of the source model to the neutral expression of the target model to get the current
expression of the target model

The ﬁrst key idea is that we retarget the displacement of the base mesh
of the source model by using the displacement parameters with respect
to the local coordinate system deﬁned by the normal vector and the
principal directions on the base mesh. Since the principal directions are
good at representing the local geometry and orthogonal to each other [1], we use
the normal vector and the two principal directions to deﬁne the local coordinate
systems. It contrasts to the approach of Noh and Nuemann [6], where the source
model is deformed to the target model to deﬁne the local coordinate systems for
the target model, which correspond to those on the source model.
The second key idea is to represent the facial mesh as a multi-resolution
mesh and use the wavelet coeﬃcients as the “control parameters” of
the facial motion. To represent the source facial model faithfully, we decompose the model into the base model and wavelet coeﬃcients that store the details
of the model using a multiresolution scheme called ”subdivision wavelets” [4].
We decompose the displacement of the source model into the displacement of the
base model and those of the wavelet coeﬃcients. We then retarget the multi-level
displacements to the target model.

2

Previous Work

Retargetting motions to new bodies were ﬁrst developed by Gleicher[3] to make
captured motions reusable. This idea was also applied to facial motion, and

218

K. Min and M.-R. Jung

there are some work[5, 6, 8, 10] in facial motion retargetting. Noh and Neumann
[6] compute the displacement of each vertex on the source mesh at each frame
relative to the neutral frame, and add the scaled version of the displacements to
the neutral frame of the target mesh. If there are frames whose displacements
from the neutral frame are large, the retargetted version of the displacements
tend to intersect each other. Noh and Neumann’s method has to apply ﬁltering
to the retargetted displacements to remove the intersections.
Pyun and Shin [8] proposed an example-based retargetting scheme. In this
approach, the system uses a set of correspondences between example poses of
the source and target facial models provided by the animator. This method
produces more accurate results than that of Noh and Neumann, because of
suﬃcient mapping data between source and target expressions. However, the
more detailed the facial model is, the larger number of example expressions are
needed.
Na and Jung[5] represent each frame of the source mesh by the base mesh and
a sequence of normal oﬀsets from the base mesh. The displacement of the source
base mesh is retargetted by using an example-based approach of [8]. The ﬁne
motion of the target model is obtained by adding the normal oﬀset displacements
of the source mesh to the normal oﬀsets of the target mesh at the neutral frame,
level by level.
Wang et al. [10] learn a decomposable generative model from a set of facial
motions of diﬀerent people. A given motion of the source face is ﬁrst decomposed
into the content (e.g. smile) and the style parameters (a particular style of smile).
Then the content motion of the source is transformed to the target motion
by applying the scale factor between the source and target faces and the style
parameters to the decomposable generative model.

3
3.1

Multiresolution Representation of Meshes
Construction of the Facial Models

The facial models used in this paper are triangular meshes with subdivision connectivity created by an experienced designer. Given a mesh without subdivision
connectivity, we can remesh it so that it has subdivision connectivity by using
the method proposed by Eck et al. [2].
3.2

Mesh Representation Using Subdivision Wavelets

For our experimentation we apply multi-resolution analysis to a given mesh three
times to obtain the wavelet coeﬃcients for three levels of the mesh. More levels
of wavelet coeﬃcients can be used to better approximate given meshes.
To represent a function using wavelet coeﬃcients, we need to have appropriate
basis functions, that is, scaling functions and wavelet functions. As Lounsbery
and his colleagues [4] have shown that appropriate scaling and wavelet functions
can be constructed for a mesh created by means of a given subdivision scheme,
we construct appropriate scaling and wavelet functions to represent the mesh.

A Multiresolutional Approach for Facial Motion Retargetting

219

The wavelet functions constructed this way are called ”subdivision wavelets”.
In this paper, we use the Butterﬂy subdivision scheme, an interpolation subdivision scheme, since it facilitates the computation of the analysis and synthesis
ﬁlters. The analysis ﬁlters Aj , B j and synthesis ﬁlters P j , Qj are determined
by the subdivision scheme employed and the scaling functions derived from the
subdivision scheme.
As shown in Figure 3.2, analysis ﬁlter Aj ﬁlters a given mesh to the next
lower resolution. Analysis ﬁlter B j computes vj , the wavelet coeﬃcients, which
is the diﬀerence between the consecutive resolutions.
The ﬁlters Aj , B j are represented as matrices, as follows:
Vj = Aj Vj+1

vj = B j Vj+1 .

Given Vj and vj , Vj+1 is reconstructed by applying the synthesis ﬁlters P j
and Qj as follows:
Vj+1 = P j Vj + Qj vj
In our subdivision scheme, P j and Qj are computed as follows:
(P j

Aj
Bj

Qj ) =

−1

=

Ij
Nj

−αj
I − Nj αj
j

(1)

Here Nj represents the portion of the subdivision matrix which weight the
“new vertices” introduced by each application of subdivision. Ij is the portion of the subdivision matrix which weight the “old vertices”, which is the
identity matrix, because the Butterﬂy subdivision is an interpolation scheme.
The matrix αj are used to deﬁne the wavelet functions in terms of the scaling functions, and are computed by solving a linear system involving the inner products of the scaling functions. See Lounsbery and et al. [4] for
details.
Given the base mesh V0 , a series of wavelet coeﬃcients vj , and the synthesis ﬁlters P j , Qj , the original mesh Vn is reconstructed by the following formula:
n−1

Vn = (

P i )V0 +

i=0

n−1

n−1

(

P k )Qj vj

(2)

j=0 k=j+1

Let the neutral expression and the current expression of the target model be
T0 and Tj . They have the following representations:
T0 = ( T00 , t00 , t10 , t20 )

Tj = ( T0j , t0j , t1j , t2j )

The motion retargetting is achieved by adding the displacement between S0
and Sj to T0 in order to obtain Tj .

220

K. Min and M.-R. Jung

Fig. 2. Multiresolutional analysis of a facial model: The original model V3 is ﬁltered
down to the base mesh V0 , and the wavelet coeﬃcients, vj , are computed in the ﬁltering
process

3.3

The Motion Retargetting Procedure

The multiresolutional motion retargetting is achieved in the following steps.
Step 1. Compute the displacement Mj of the source model between the neutral
expression and the current expression, as follows:
Mj = ( M0j , m0j , m1j , m2j ), where
M0j ≡ S0j − S0j , and mij ≡ sij − si0 , for 0 ≤ i ≤ 2.

(3)

The displacement of the base mesh M0j is deﬁned as the diﬀerence of the
current expression S0j from the neutral expression S00 , as follows:
M0j ( ≡ S0j − S00 ) = {di | di = pij − pi0 , where pij ∈ S0j and pi0 ∈ S00 },
where pi0 and pij are the corresponding vertices between the base mesh of the
neutral expression and the base mesh of the current expression.
As shown in Figure 3, the displacement vector di is represented as follows:
di = αi Ni + βi Ji + γi Ki

(4)

Here Ni is the normal vector, and Ji and Ki are the principal directions at
the location pi0 . The principal directions of a triangular mesh are computed by
the algorithm in [9].
As shown in Eqn 3, the displacement of upper levels of the mesh, mij , is deﬁned
to be the diﬀerence of the wavelet coeﬃcients between the neutral expression
and the target expression. The wavelet coeﬃcients can be compared and thus
subtracted because the wavelet basis functions on the neutral expression and the
target expression correspond to each other.

A Multiresolutional Approach for Facial Motion Retargetting

221

Fig. 3. Retargetting the displacement di on the source base mesh to the displacement
ˆ i on the target base mesh
d

Step 2. Transform the displacement of the source model, ( M0j , m0j , m1j , m2j )
ˆ 0, m
to the displacement of the target model, ( M
ˆ 0j , m
ˆ 1j , m
ˆ 2j ).
j
0
ˆ
ˆ
Each motion vector di in Mj is computed as follows:
ˆ i = sαi N
ˆ i + sβi J
ˆi + sγi K
ˆi
d

(5)

ˆ i, J
ˆi , K
ˆ i ) is the target local coordinate
as shown in the right of Figure 3. Here (N
system deﬁned at the corresponding target vertex. Here s is the scale factor,
which is deﬁned to the ratio between the surface area of the source base mesh
and that of the target mesh mesh. We use m
ˆ ij = mij . In other words, we use the displacements of the wavelet coeﬃcients on the source model as the displacements
of the wavelet coeﬃcients on the target model, for the following reasons. The
wavelet functions for each level of the target model have the same forms as those
of the source model. Though the supports of the wavelet functions are not the
same between the source model and the target model, these supports correspond
to each other between the source and target models, starting from the base mesh.
Step 3. Obtain the current expression of the target model Tj = ( T0j , t0j , t1j , t2j )
by adding the displacement of the target model computed in Step 2 to the
neutral expression T0 = ( T00 , t00 , t10 , t20 ). That is, compute:
Tj = ( T0j , t0j , t1j , t2j ), where
ˆ 0j , and tij = ti0 + m
ˆ ij , for 0 ≤ i ≤ 2.
T0j = T00 + M
Step 4. Finally, reconstruct the mesh corresponding to Tj by using Eqn 2:
mesh(Tj ) = P 2 P 1 P 0 Tj 0 + P 2 P 1 Q0 tj 0 + P 2 Q1 tj 1 + Q2 tj 2 .

4

Implementation and Results

We implemented the proposed algorithm in a personal computer with 2.0 GHz
Pentium-4 CPU and 512 MByte Main Memory. Each of the facial models has

222

K. Min and M.-R. Jung

Fig. 4. Retargetting of the facial motions

A Multiresolutional Approach for Facial Motion Retargetting

223

2964 vertices and 5760 triangles. We tested the proposed algorithm for one source
model and two target models. The source model is created from one female
volunteer, and two target models are created from another female volunteer and
one male volunteer. The motions we tested in this paper are smiling (See Fig. 4
(a)) and crying (See Fig. 4 (b)).

5

Conclusion

In this paper, we presented a new facial motion retargetting scheme based on
the multiresolutional analysis of polygonal meshes. The displacement between
the neutral and the current expressions are extracted from the base mesh of the
source model and as well as from the wavelet coeﬃcients. The algorithm creates
the motion of the target model by adding the multiresolution displacements of
the source model to the target base mesh and the wavelet coeﬃcients of the target
model at the neutral frame, level by level. In this process, the wavelet coeﬃcients
play the role of control parameters to represent and control facial motion. Hence
we solved one of the inherent diﬃculties in facial motion retargetting, which is
the lack of obvious control parameters to control facial motion.

References
1. do Carmo, M. P., Diﬀerential Geometry of Curves and Surfaces, Pearson Education, 1976.
2. Eck, M., DeRose, T., Duchamp, T., Hoppe, H., Lounsbery, M., and Stuetzle, W.,
“Multiresolution Analysis of Arbitrary Meshes”, Proceedings of SIGGRAPH 1998
Annual Conference Series, pp. 173–182, 1995.
3. Gleicher, M., “Retargetting motion to new characters,” Proceedings of SIGGRAPH 1998 Annual Conference Series, pp. 33-42, 1998.
4. Lounsbery, M., DeRose, T. D., and Warren, J., “Multiresolution analysis for surfaces of arbitrary topological type,” ACM Transactions on Graphics, Vol. 16, No.
1, pp. 34-73, 1997.
5. Na, K. and Jung, M., “Hierarchical Retargetting of Fine Facial Motions,” Proceedings of Eurographics 2004, pp. 687-695, 2004.
6. Noh, J., Neumann, U., “Expression cloning,” Proceedings of SIGGRAPH 2001
Annual Conference Series, pp. 277-288, 2001.
7. Parke, F. I., Waters, W., Computer Facial Animation, AK Peters, 1996.
8. Pyun, H. and Shin, S. Y., “An example-based approach for facial expression
cloning,” In ACM SIGGRAPH/Eurographics Symposium on Computer Animation
(2003), PP. 167-176.
9. Taubin, G., “Estimating the Tensor of Curvature of a Surface from a Polyhedral
Approximation,” Proceedings of IEEE ICCV 1995, pp. 852 - 857, 1995.
10. Wang, Y., Huang, X., Lee, C., Zhang, S., and Li. Z., “High Resolution Acquisition, Learning and Transfer of Dynamic 3D Facial Expressions,” Proceedings of
Eurographics 2004, pp. 677-686, 2004.

