Available online at www.sciencedirect.com

Procedia Computer Science 9 (2012) 1920 – 1929

Using Performance Measurements to Improve MapReduce Algorithms
Todd D. Plantenga, Yung Ryn Choe, Ann Yoshimura
Sandia National Laboratories, Livermore, CA 94551-9159

Abstract
The Hadoop MapReduce software environment is used for parallel processing of distributively stored data. Data
mining algorithms of increasing sophistication are being implemented in MapReduce, bringing new challenges for
performance measurement and tuning. We focus on analyzing a job after completion, utilizing information collected
from Hadoop logs and machine metrics. Our analysis, inspired by [1] [2], goes beyond conventional Hadoop JobTracker analysis by integrating more data and providing web browser visualization tools. This paper describes examples where measurements helped diagnose subtle issues and improve algorithm performance. Examples demonstrate
the value of correlating detailed information that is not usually examined in standard Hadoop performance displays.
Keywords: software performance, Hadoop, MapReduce, cluster monitoring

1. Contributions
MapReduce [3] is a software environment for parallel processing of distributively stored data. The Apache Hadoop
[4] software environment provides a popular implementation for distributed data storage and MapReduce computing.
Many researchers are developing Hadoop-based MapReduce algorithms for sophisticated data analysis [5] [6] [7] [8].
Although MapReduce simpliﬁes the job of writing parallel algorithms and Hadoop provides a platform-neutral Java
implementation, the construction of an algorithm that performs well at scale is still a nontrivial task.
The Hadoop JobTracker web view provides a useful summary of map and reduce task performance. However,
Hadoop logs on master and slave nodes contain additional useful information (even at default logging levels), and
machine metrics can be gathered with third party tools (in our case, Ganglia [9]). We synthesize these data sources to
help analyze algorithm performance after job completion. Our work is inspired by the type of analysis reports used
by Paul Burkhardt [1] [2]. The new contributions of our work include: integrating information from multiple data
sources, mining Hadoop DataNode logs for information, creating interactive visualizations that run in a standard web
browser, and implementation of a web-based client/server system for analysis and reporting. This paper describes our
tools and provides examples of their use in analyzing algorithms.
Our motivation is to improve performance of an algorithm that will be used repeatedly. We assume test runs are
made to collect data that drive algorithm improvements. To this end we analyze jobs after completion and do not
strive for a real time view of performance.
Email address: tplante@sandia.gov (Todd D. Plantenga)

1877-0509 © 2012 Published by Elsevier Ltd. Open access under CC BY-NC-ND license.
doi:10.1016/j.procs.2012.04.210

Todd D. Plantenga et al. / Procedia Computer Science 9 (2012) 1920 – 1929

1921

2. Related Work
The standard Hadoop JobTracker, described in Section 3, is a good starting point for job analysis. Some of our
tools present the same JobTracker job information, but in a form that is easier to derive conclusions from. We collect
additional statistics about data transfers and machine performance that are not available with the Hadoop JobTracker.
Hadoop allows the user to turn on Java job proﬁling [10], which collects information useful for analyzing individual map and reduce tasks. We have not included Java proﬁling, in part to avoid any impact on job performance.
A number of commercial tools provide cluster monitoring capability (IBM Tivoli, HP Enterprise Software), some
specialized for Hadoop MapReduce (Cloudera, Karmasphere, MapR). These systems are primarily interested in monitoring the health of a cluster and the real time status of jobs in progress, which diﬀers from our goal. We focus on
the performance of a single Hadoop job, after it completes. Many of the machine performance metrics collected by
cluster monitoring systems are of interest in the analysis of this job. We obtain such metrics from Ganglia [9], but in
principle they could come from other monitoring systems.
The Apache Chukwa project [11] is a cluster monitoring system that uses HDFS and MapReduce for storage and
processing. Chukwa aims to be a general purpose cluster monitor, and though it can certainly monitor a Hadoop
cluster, again it does not focus on the deep analysis of individual jobs.
As mentioned in the introduction, our work was originally motivated by proﬁle charts from the study by Burkhardt
[1] [2]. He parsed the Hadoop job history log ﬁle and used Gnuplot scripts to create insightful graphics. We create
similar proﬁle charts for task duration (both individual tasks and stacked tasks versus time), plus an additional table
view containing task times and counter values. We use similar machine metrics from Ganglia, but display them in
our plots of task execution so the data can be visually correlated with Hadoop jobs. Our tool also mines a diﬀerent
source of information, Hadoop DataNode logs, to present a view of data transfers between HDFS and MapReduce
jobs. Finally, our tools are coded in Java and JavaScript, and run in a separate web server that presents displays to a
standard web browser.
3. Performance Measurements
A single Hadoop job consists of parallel map tasks, a shuﬄe and sort phase, and parallel reduce tasks that run after
shuﬄing completes. Users write code for map and reduce tasks, while the Hadoop MapReduce framework manages
shuﬄing, sorting, and coordination of parallel tasks. Hadoop jobs are usually designed to read through large amounts
of input data that is stored across a compute cluster in the Hadoop Distributed File System (HDFS). Hadoop scales
by allocating a number of tasks based on the input data size and available computing resources. To learn more about
Hadoop jobs and infrastructure, we suggest [4] and [12].
Performance of an algorithm coded as a Hadoop job is inﬂuenced by many factors. Here we assume the job is
analyzed in isolation on a compute cluster, though in practice it may share resources with other scheduled jobs. We
also assume the user is not tuning core cluster parameters such as the number of nodes or disk scheduling policy. The
user is able to modify map and reduce source code, and set a number of job-speciﬁc Hadoop parameters. Our goal is
to provide a deep level of performance information to guide the user in modifying these areas of code.
The basic Hadoop JobTracker summary provides a starting point for analysis. Figure 1 shows an example for a
job with 96 map tasks and 64 reduces. Links in the upper table drill to speciﬁc tasks, where it is possible to inspect
start and ﬁnish times, counter values, and task log ﬁles one by one. The lower table in the ﬁgure shows counter values
accumulated over the entire job. For example, the FileSystemCounters show that the mappers collectively read 2.470
Gbytes of input from HDFS, and the reducers emitted 256.8 Gbytes of results.
Figure 2 results if the user clicks on the link to Analyse This Job at the top of Figure 1. Data in this table shows
the average execution time of a task, and lists the ten slowest tasks. Reduce tasks are shown in the ﬁgure, and similar
data is available for map and shuﬄe tasks. Hadoop ﬁnds the information in Figures 1 and 2 from a job history ﬁle,
which is maintained by the master JobTracker process.
Our tool shows all the information from the Hadoop JobTracker summary, while adding greater detail and interactive capabilities. Figure 3 shows a table view from our tool for the same job data in Figures 1 and 2. The ﬁgure
shows all reduce tasks (map tasks are on a separate tab), sorted from slowest to fastest (the second column). Each task
includes the ID, host compute node, start and stop times (hidden in Figure 3), and all counter values (the ﬁrst ﬁve are
shown). Counters are integer values incremented by map or reduce tasks during execution, with a ﬁnal total reported

1922

Todd D. Plantenga et al. / Procedia Computer Science 9 (2012) 1920 – 1929

Figure 1: Standard view of one job provided by the Hadoop JobTracker.

Figure 2: Hadoop JobTracker analyze feature for the reduce tasks in a job.

Todd D. Plantenga et al. / Procedia Computer Science 9 (2012) 1920 – 1929

1923

to the master JobTracker. Hadoop supplies a few system counters (columns 4 and 7 are examples in Figure 3), and
allows users to deﬁne their own counters (columns 5 and 6 are examples). Counters often provide insight into performance time; in Figure 3 it is clear that task duration is correlated with the count of Reduce output records. The
columns in this table can be rearranged by dragging with the mouse, and rows can be sorted on any column value by
simply clicking on the column header.

Figure 3: New task table view of one job, showing reduce tasks in order of task duration. Columns can be rearranged, and rows sorted by any
column header. Columns on the right show the values of job counters, which often correlate with job duration.

All the data in Figure 3 is found in the job history ﬁle managed by the Hadoop JobTracker. The standard Hadoop
JobTracker display provides access to the same information, but only after drilling down into a particular task. Our
tool pulls all counter values together into one table for an easier comparison, and enables some simple analysis by
allowing the user to interactively sort and rearrange data.
Figure 4 shows how we integrate Ganglia cluster performance statistics with job execution. The MapReduce job
in this case consists of 3153 map tasks, followed by 64 reduces. The graph plots the number of active tasks over time,
showing the tasks in stacked form, as in [2] and [13]. For example, the shape for reduce tasks shows 64 tasks starting
at about the same time, but with a 5 minute diﬀerence in ﬁnish times. Map tasks exhibit a sawtooth pattern because the
compute cluster supported a maximum of 256 parallel tasks. In the beginning, all 256 map tasks start and ﬁnish nearly
together, and the 5-10 second pause between tasks is evident. Over time some maps run faster than others, and groups
of 256 tasks gradually lose any incidental synchronization. Figure 4 goes beyond the usual stack plot by including
instantaneous CPU and network use for the cluster. This data was gathered and stored by Ganglia, and retrieved
programmatically by our tool for time windows matching the Hadoop job of interest. Our current implementation
plots relative CPU and network use, scaled to ﬁt the height of the diagram.
Figure 5 is an example of our data transfer display page. We mine the log ﬁles of DataNode processes on each

1924

Todd D. Plantenga et al. / Procedia Computer Science 9 (2012) 1920 – 1929

Figure 4: Stacked chart view of tasks, integrated with machine statistics (CPU and network use) from Ganglia.

Figure 5: Data transfer information extracted from DataNode logs. See the text for an explanation.

Todd D. Plantenga et al. / Procedia Computer Science 9 (2012) 1920 – 1929

1925

compute node to gather statistics on HDFS block transfers associated with a MapReduce job. The left side control
buttons in Figure 5 select transfer types to display, in this case all transfers associated with map tasks. The matrix in
the middle plots a box for every HDFS transfer to a map task. The row and column indices of a box are the source
and destination compute nodes (respectively) of a transfer, and the area of the box is proportional to the amount of
data transferred. Clicking on a box generates details for that node transfer pair. Figure 5 highlights the transfer of 56
Mbytes from node nb27 to node nb49 (the compute cluster has 65 nodes named nb01 through nb65). The right side
table shows the transfer was a single HDFS block, and provides detailed timing information.
The main value of the data transfer display is to indicate patterns of data movement. For the map phase of a job,
Hadoop strives to schedule map tasks to minimize the need for data movement. Ideally, the data transfer matrix of
maps will have boxes only on the main diagonal (upper left to lower right), as these indicate local transfers within a
compute node. Each compute node of a cluster usually has multiple processors and disks, but only a single DataNode
process. Hence, a local transfer on a compute node reads data from one of the disks attached to the node; non-local
transfers read from a remote disk and move data over the network. In Figure 5 about ten transfers are oﬀ the main
diagonal. Section 5 examines a scenario where job parameters aﬀect the number of local transfers.
4. Case Study: Reducer Delays
This section explains how the task time-line view helped decrease reduce execution time. The job of interest
consisted of 98 map tasks followed by 64 reduces, run on a cluster with 65 compute nodes. Figure 6 shows a time-line
of task executions (sometimes called “swim-lanes” [14]). It is similar to the stacked chart view in Figure 4, but here
the tasks are shown individually. Each horizontal line is the execution interval of a map, shuﬄe, or reduce task, with
tasks ordered from earliest start time (bottom) to latest start time (top). It is clear that reduce tasks dominate the
execution time of the job.

Figure 6: Task durations for one job, using default job parameters. Total reduce time is the elapsed time between start of the ﬁrst shuﬄe/reduce
task and completion of all tasks.

The standard Hadoop JobTracker views indicate this much, but Figure 6 reveals a curious inverse relation between
reduce task duration and shuﬄe task duration; furthermore, shuﬄe times are slowest for tasks that started earlier. The
ﬁrst 44 shuﬄe tasks run about twice as long as the last 21 tasks (with one anomaly), but the ensuing reduce tasks for
slow shuﬄes are much shorter in duration. The vertical line labeled “Last Mapper Finish Time” falls precisely at the

1926

Todd D. Plantenga et al. / Procedia Computer Science 9 (2012) 1920 – 1929

Figure 7: The same job as Figure 6, but the start of shuﬄe/reduce tasks is delayed until 90% of map tasks are complete. More reduce tasks run
slowly, and total reduce time increases by 20%.

Figure 8: The same job as Figure 6, but the start of shuﬄe/reduce tasks begins after just 1% of map tasks are complete. All reduce tasks run faster,
and total reduce time decreases by 22%. Note the scale for the time axis in this ﬁgure is stretched out relative to Figures 6 and 7, making visual
comparisons diﬃcult.

Todd D. Plantenga et al. / Procedia Computer Science 9 (2012) 1920 – 1929

1927

start time of the ﬁrst short shuﬄe / long reduce task. This data point came from the master JobTracker log ﬁle, while
all the task times came from the job history ﬁle. The line appears well beyond the last map ﬁnish time, indicating a
delay in the master JobTracker before it considers mapping complete. The line correlates perfectly with the change in
behavior of shuﬄe tasks.
From Figure 6, we hypothesize that shuﬄe and reduce task durations in this job are strongly aﬀected by whether
they start before or after completion of the last map task. We can control the start time by modifying the job parameter
reduce.slowstart.completedmaps. The default value is 0.05, meaning that shuﬄe/reduce tasks are started when
5% of map tasks complete. In Figures 7 and 8 we change this to 90% and 1%, respectively. The relationships noted
in Figure 6 continue to hold in Figures 7 and 8.
Delaying the start of shuﬄe/reduce tasks until 90% of maps complete causes the total reduce time to increase from
116 to 139 seconds (Figure 7). Starting shuﬄe/reduces sooner (Figure 8) cuts the total time to 91 seconds. Note that
the time axis in Figure 8 is stretched out relative to Figures 6 and 7 (an unfortunate artifact).
In this case study the task time-line view of our tool made it easy to recognize the inverse relationship of task
durations, and clearly validates the improvement. However, it does not explain why reduce tasks run faster when their
shuﬄe phase is started before maps complete. In our experience, most MapReduce jobs do not have this relationship.
Manual examination of task log ﬁles suggests that the ﬁrst shuﬄe tasks launch a number of fetch threads to get map
output, and then fall into a wait state. By contrast, the shuﬄe tasks that start after the vertical line launch their fetch
threads and immediately start receiving map outputs. The wait time seems to explain why early start shuﬄes have
longer durations, but it is not clear why reduce tasks then run faster.
5. Case Study: HDFS Replication
This section uses the data transfer view to examine trade-oﬀs in HDFS storage strategy. We investigate one performance characteristic of an algorithm that searches for subgraph isomorphisms from data deﬁning a social network
graph [7]. The algorithm runs as a series of MapReduce jobs, where each map phase (except the initial job) reads
inputs from the output of the previous reduce phase. The outputs of a reduce phase are HDFS ﬁles. We can adjust
the replication count of HDFS ﬁles when they are created by setting the parameter dfs.replication. When the
count is higher, HDFS creates additional replicas of output ﬁle blocks on separate compute nodes. This should make
it easier for subsequent MapReduce jobs to start a map task where data exists locally; however, it takes longer for
the reduce output phase to complete because all replicas must ﬁnish writing before the next MapReduce job can start
reading data.
We ran the algorithm twice on the same input data, with diﬀerent settings for the replication count in each run.
One run used the default HDFS replication count of three, and the other used a count of one (no replicas). Figure 9
compares the data transfer views of the two runs for one particular map phase. The MapReduce job executed 96 map
tasks on a cluster of 65 compute nodes. Recall that boxes on the main diagonal indicate local map task data transfers;
i.e., situations where the map task executes on the same node where its input data block is located. Clearly, using
three replicas makes it easier to get local execution, and the map phase execution time on the left is 15% faster than
on the right. However, in this example the reduce phase time required to write additional replicas far outweighed the
savings in the map phase.
6. Software Architecture
We developed code for our tool with the goal of making it easy to deploy in a standard Hadoop environment. Our
design limits dependencies to simplify porting to diﬀerent versions of Hadoop. We did not architect a general purpose,
highly scalable framework such as Chukwa. The tools have performed well on a cluster of 65 compute nodes, but our
current method of collecting DataNode log ﬁles is not scalable to larger clusters.
We start a standalone server using the Java-based web server Jetty [15] (the same server libraries bundled with
Apache Hadoop). Our server connects via TCP/IP with the master Hadoop JobTracker so it can make queries through
the Hadoop JobClient API. We implemented a second server for code separation, and to ensure our data processing
does not burden Hadoop servers. Users contact our web server with a standard web browser (screen shots in this paper
are from Google Chrome version 16.0). Browser code is all in JavaScript, leveraging the open source libraries Ext

1928

Todd D. Plantenga et al. / Procedia Computer Science 9 (2012) 1920 – 1929

Figure 9: Data transfers for map tasks in a job. The left side shows map tasks from an HDFS input ﬁle that was replicated three times per block.
The right side shows maps for the same job, but taking input from an HDFS input ﬁle that was replicated once per block. Transfers on the main
diagonal indicate local block access; oﬀ diagonal entries imply data transfers over the network.

JS [16] and D3 [17]. Browser controls make use of the Ext JS grid table control, and the SVG-based charts and data
transfer matrix are created with D3. The web server transmits data in JSON format.
Our web server implements three services for browser clients:
1. getAllJobs The Hadoop JobTracker (or JobHistoryServer in Hadoop 1.0.0) is queried for all job identiﬁers
using methods from JobClient. The location of each job’s history ﬁle is stored for the getJobInfo service. This
information is rendered in table format to the user (not shown in this paper), allowing them to choose a job for
analysis.
2. getJobInfo The history ﬁle of a particular job is fetched directly, without going through Hadoop servers.
The Hadoop JobHistoryParser class is used to extract information from the ﬁle. Data is used to generate a task
table view (Figure 3), task timing chart (Figure 6), and stacked task timing chart (Figure 4). The stacked chart
incorporates machine metrics, if available, from Ganglia (examples in this paper used version 2.1.7). We make a
Ganglia service call to get metrics in JSON format over the time range of a MapReduce job. Note that Ganglia
reduces the time resolution of older data to save storage; hence, we conﬁgure Ganglia to save metrics at ﬁne
resolution (15 second bins) for several hours so that high resolution metrics will be available when a user analyzes
a job.
3. getJobTransfers Hadoop DataNode ﬁles are parsed from each compute node to ﬁnd HDFS block transfers
associated with a particular job. Our server queries the JobClient for cluster status to determine log ﬁle locations,
and then requests speciﬁc ﬁles as an HTTP transfer from servers on each node. The DataNode servers do not
separate data transfers by job, so the entire log ﬁle (usually one day’s worth of output) must be scanned to ﬁnd
records of interest. We wrote a simple parser for these log ﬁles. Our server collects all the log ﬁles, extracts
records, and generates the data transfer view (Figure 5).
The web server deployment runs with Hadoop version 0.21. Code is dependent on the Hadoop classes JobClient
and JobHistoryParser, and on the particular format of DataNode log ﬁles. It also runs on Hadoop 0.20 and 1.0 deployments, accommodating changes in the job history formats. Note that the web server must be run with permission
to read log ﬁles created by the master JobTracker and local DataNodes.
Our deployment was tested on a cluster of 65 compute nodes (Linux operating system, Intel i7 quad core processors, 12 GB memory, 4 disks per node, Gigabit Ethernet network). Data analysis is a little slow for jobs with

Todd D. Plantenga et al. / Procedia Computer Science 9 (2012) 1920 – 1929

1929

thousands of map tasks, but even intensive performance analysis tasks have little eﬀect on any executing Hadoop
jobs. Our software architecture will probably not scale to signiﬁcantly larger clusters because of the way DataNode
information is gathered by getJobTransfers. We hope to address this by distributing the work of DataNode log
parsing to processes running on the data nodes themselves, instead of centrally at our server. This will parallelize the
work of reading and parsing, and reduce the amount of web traﬃc.
7. Summary
The Apache Hadoop environment for MapReduce is becoming increasingly popular as a platform for sophisticated
analysis of large data sets. We developed a set of tools that integrate data from log ﬁles and machine metrics to examine
job performance. The goal is to provide insight for improving the performance of complex MapReduce algorithms.
Our tools run as a separate server process, with a user friendly interface available through any current web browser.
We provide table and chart views of map and reduce tasks, integration with Ganglia machine metrics, and a view
of data transfers between HDFS and MapReduce jobs. We plan to expand on our work as the tools are used within
Sandia National Laboratories.
8. Acknowledgments
This work was funded by the Laboratory Directed Research & Development (LDRD) program at Sandia National
Laboratories. Sandia National Laboratories is a multiprogram laboratory operated by Sandia Corporation, a wholly
owned subsidiary of Lockheed Martin Corporation, for the United States Department of Energy’s National Nuclear
Security Administration under contract DE-AC04-94AL85000.
[1] P. Burkhardt, A proﬁle of Apache Hadoop MapReduce computing eﬃciency, part I (2010).
URL
http://www.cloudera.com/blog/2010/12/a-profile-of-hadoop-mapreduce-computing-efficiency-sra-paul-burkhardt/
[2] P. Burkhardt, A proﬁle of Apache Hadoop MapReduce computing eﬃciency, part II (2010).
URL http://www.cloudera.com/blog/2010/12/a-profile-of-hadoop-mapreduce-computing-efficiency-continued/
[3] J. Dean, S. Ghemawat, MapReduce: Simpliﬁed data processing on large clusters, in: OSDI’04: Sixth Symposium on Operating Systems
Design and Implementation, USENIX Association, 2004.
[4] Apache Hadoop project (2011).
URL http://hadoop.apache.org/
[5] Apache Mahout (2011).
URL http://mahout.apache.org/
[6] U. Kang, C. E. Tsourakakis, C. Faloutsos, PEGASUS: A peta-scale graph mining system - implementation and observations, in: ICDM,
2009.
[7] T. D. Plantenga, Inexact subgraph isomorphism in MapReduce, submitted for publication.
[8] U. Kang, C. E. Tsourakakis, A. P. Appel, C. Faloutsos, J. Leskovec, Radius plots for mining tera-byte scale graphs: Algorithms, patterns, and
observations, in: SIAM Conference on Data Mining, SIAM, 2010.
[9] M. L. Massie, B. N. Chun, D. E. Culler, The ganglia distributed monitoring system: Design, implementation, and experience, Parallel
Computing 30 (2004) 817–840.
[10] Java virtual machine proﬁler interface (2004).
URL http://docs.oracle.com/javase/1.5.0/docs/guide/jvmpi/jvmpi.html
[11] J. Boulon, A. Konwinski, R. Qi, A. Rabkin, E. Yang, M. Yang, Chukwa: A large-scale monitoring system, in: LISA’10: Proceedings of the
24th International Conference on Large Installation System Administration, USENIX Association, 2010.
[12] T. White, Hadoop, The Deﬁnitive Guide, Second Edition, O’Reilly / Yahoo! Press, 2010.
[13] O. O’Malley, A. C. Murthy, Winning a 60 second dash with a yellow elephant, in: Proceedings of Sort Benchmark, Yahoo, 2009, pp. 1–9.
[14] S. Kavulya, J. Tan, R. Gandhi, P. Narasimhan, An analysis of traces from a production MapReduce cluster, in: 10th IEEE/ACM International
Conference on Cluster, Cloud and Grid Computing, 2010.
[15] Jetty (2011).
URL http://jetty.codehaus.org/jetty/
[16] Ext JS (2011).
URL http://www.sencha.com/products/extjs/
[17] D3 (2011).
URL http://mbostock.github.com/d3/

