The AGEB Algorithm for Solving the Heat
Equation in Three Space Dimensions and Its
Parallelization Using PVM
Mohd Salleh Sahimi1 , Norma Alias2 , and Elankovan Sundararajan2
1

Department of Engineering Sciences and Mathematics,
Universiti Tenaga Nasional, 43009 Kajang, Malaysia
Sallehs@uniten.edu.my,
2
Department of Industrial Computing,
Universiti Kebangsaan Malaysia, 43600 UKM, Malaysia
norm ally@hotmail.com
elan@ftsm.ukm.my

Abstract. In this paper, a new algorithm in the class of the AGE
method based on the Brian variant (AGEB) of the ADI is developed
to solve the heat equation in 3 space dimensions. The method is iterative, convergent, stable and second order accurate with respect to
space and time. It is inherently explicit and is therefore well suited for
parallel implementation on the PVM where data decomposition is run
asynchronously and concurrently at every time level. Its performance is
assessed in terms of speed-up, eﬃciency and eﬀectiveness.

1

Introduction

The ADI method deals with two-dimensional parabolic (and elliptic) problems.
Since the method has no analogue for the one-dimensional case, in [1] the alternating group explicit method which oﬀers its users many advantages was
developed. It is shown to be extremely powerful and ﬂexible. It employs the
fractional splitting strategy of Yanenko [2] which is applied alternately at each
intermediate time step on tridiagonal systems of diﬀerence schemes. Its implementation was then extended to two space dimensions [3]. In this paper, we
present the formulation of AGEB for the solution of the heat equation in three
space dimensions and then describe its parallel implementation on the PVM on
a model problem.

2

Formulation of the AGEB Method

Consider the following heat equation,
∂U
∂2U
∂2U
∂2U
+
+
+ h(x, y, z, t),
=
∂t
∂x2
∂y 2
∂z 2

(x, y, z, t) ∈ R × (0, T ] ,

V.N. Alexandrov et al. (Eds.): ICCS 2001, LNCS 2073, pp. 918–927, 2001.
c Springer-Verlag Berlin Heidelberg 2001
�

(1)

The AGEB Algorithm for Solving the Heat Equation

919

subject to the initial condition
U (x, y, z, 0) = F (x, y, z),

(x, y, z, t) ∈ R × {0} ,

and the boundary conditions
U (x, y, z, t) = G(x, y, z, t),

(2)

(x, y, z, t) ∈ ∂R × (0, T ] ,

(3)

where R is the cube 0 < x, y, z < 1 and ∂R its boundary. A generalised approximation to (1) at the point (xi , yj , zk , tN +1/2 ) is given by (with 0 ≤ θ ≤ 1),
[N +1]

[N ]

ui,j,k − ui,j,k
Δt

=

�
1 � 2
2
2 [N +1]
2
2
2 [N ]
θ(δ
+
δ
+
δ
)u
+
(1
−
θ)(δ
+
δ
+
δ
)u
x
y
z
x
y
z
i,j,k
i,j,k
(Δx)2
[N +1/2]

+ hi,j,k

,

i, j, k = 1, 2, . . . , m ,

(4)

leading to the seven-point formula,
[N +1]

[N +1]

[N +1]

[N +1]

[N +1]

−λθui−1,j,k + (1 + 6λθ)ui,j,k − λθui+1,j,k − λθui,j−1,k − λθui,j,k−1
[N +1]

[N +1]

[N ]

[N ]

− λθui,j+1,k − λθui,j,k+1 = λ(1 − θ)ui−1,j,k + (1 − 6λθ)(1 − θ)ui,j,k
[N ]

[N ]

[N ]

[N ]

+ λ(1 − θ)ui+1,j,k + λ(1 − θ)ui,j−1,k + λ(1 − θ)ui,j,k−1 + λ(1 − θ)ui,j+1,k
[N ]

[N +1/2]

+ λ(1 − θ)ui,j,k+1 + Δthi,j,k

(5)

.

By considering our approximations as sweeps parallel to the xy-plane of the
cube R, (5) can be written in matrix form as,
[N +1]

Au[xy]

=f .

(6)

By splitting A into the sum of its constituent symmetric and positive deﬁnite
matrices G1 , G2 , G3 , G4 , G5 and G6 we have
A = G 1 + G 2 + G 3 + G4 + G 5 + G 6 ,

(7)

with these matrices taking block banded structures as shown in Fig. 1.
Using the well-known fact of the parabolic-elliptic correspondence and employing the fractional splitting of Brian [4], the AGEB scheme takes the form,
(n+1/7)

(rI + G1 )u[xy]

(n)

= (rI − (G1 + G2 + G3 + G4 + G5 + G6 ))u[xy] + f
(n)

(n+2/7)

(rI + G2 )u[xy]

= ((rI + G1 ) − A)u[xy] + f
(n+1/7)

+ G2 u[xy]

(n+2/7)

+ G3 u[xy]

(n+3/7)

+ G4 u[xy]

(n+4/7)

+ G5 u[xy]

(n+5/7)

+ G6 u[xy]

= ru[xy]

(n+3/7)

= ru[xy]

(n+4/7)

= ru[xy]

(n+5/7)

= ru[xy]

(n+6/7)

= ru[xy]

(rI + G3 )u[xy]
(rI + G4 )u[xy]
(rI + G5 )u[xy]
(rI + G6 )u[xy]

(n)
(n)
(n)
(n)
(n)

u(n+1) = u(n) + 2(u(n+6/7) − u(n) ) .

(8)

920

M.S. Sahimi, N. Alias, and E. Sundararajan

(a)

(c)

(b)

(d)

Fig. 1. (a) A, (b) G1 + G2 , (c) G3 + G4 , and (d) G5 + G6 . Note that diag(G1 + G2 ) =
diag(A)/3, diag(G3 + G4 ) = diag(A)/3, diag(G5 + G6 ) = diag(A)/3. All are of order
(m3 × m3 ).

The AGEB Algorithm for Solving the Heat Equation

921

The approximations at the ﬁrst and the second intermediate levels are computed directly by inverting (rI+G1 ) and (rI+G2 ). The computational formulae
for the third and fourth intermediate levels are derived by taking our approximations as sweeps parallel to the yz-plane. Here, the u values are evaluated at
points lying on planes which are parallel to the yz-plane and on each of these
planes, the points are reordered row-wise (parallel to the y-axis). Finally, by
considering our approximations as sweeps parallel to the xz-plane followed by a
reordering of the points column-wise (parallel to the z-axis) enable us to determine the AGEB equations at the ﬁfth and sixth intermediate levels. Note that
all solutions at each iterate are generated rather than stored. Hence the actual
inverse is not used by the algorithm. The AGEB sweeps involve tridiagonal systems which in turn entails at each stage the solution of (2 × 2) block systems.
The iterative procedure is continued until convergence is reached.

3

Parallel Implementation of the AGEB Algorithm

One must ensure that an eﬀective parallel implementation of the algorithm leads
to a substantial increase in the computational count per data exchange, major
reduction in synchronisation frequency and subsequent decrease in communication sessions.
A typical parallel implementation involves the assignment of a block of grids
to each task to a surface so that each task only communicates with its limited
nearest neighbours. Only the top, bottom, left and right surfaces of the block
need to be exchanged between neighbouring tasks. As an example, Fig. 2 illustrates the pattern of communication with 4 tasks (p = 4) on a linear system of
order m = 32.
It is also important to maintain load balancing in the distribution of m
grids to tasks P1 , P2 , . . . , Pp . The data decomposition of the AGEB algorithm is
run asynchronously and simultaneously at every time level, where each task is
allocated m/p grids. It proceeds for every task at each time level until the local
error and the approximate solution are computed at the last time level. These
tasks then send the local errors to the master, which in turn processes the global
error.
On the PVM, parallel implementation of AGEB is based on one master and
many tasks. The master program is responsible in constructing the m grid sizes,
computing the initial values, portioning the grid into blocks of surface, assigning
these blocks to the p task modules, distributing the task to diﬀerent processors
and receiving local errors from the tasks. Each block that is assigned to a task
module is composed of m/p blocks.
A task process starts computations after it receives a work assignment. A
task module q(q < P ) performs the AGEB iterations on the grid points of the
assigned block which is composed of surfaces with indices between
SU Rq (start) =

m(q − 1)
p

and SU Rq (end) =

where SU R refers to the surface. The task q will transmit

mq
−1 ,
p

922

M.S. Sahimi, N. Alias, and E. Sundararajan

G1

G2

G5

G3

G6

G7

G32

G31

G8
G30

P1
G10
G28

G11

G27

G14
G24

G12
G26

G15
G23

P3
G17

G29

P2

G9

G13

G4

G25

G16
G22

G21

P4
G18

G19

G20

Fig. 2. Communication of data exchange between 4 tasks and 32 grids

i. to its upper neighbour (task q − 1), SU Rq (start) and receive from it
SU Rq (start) − 1 = SU Rq−1 (end)
ii. to its lower neighbours (task q + 1), SU Rq (end) and receive from it
SU Rq (end) + 1 = SU Rq+1 (start) .
Since multiple copies of the same task code run simultaneously, the tasks will
exchange data with their neighbours at diﬀerent times. At this point, the barrier
function is called by the PVM library routine for synchronisation. The tasks will
repeat the above procedure, until the local convergence criterion is met. The
deﬁnition of the residual computed in the task q is as follows,
�
�
��
� [N +1]
[N ] �
r[i][j][k] = max �ui,j,k − ui,j,k �, (i, j, k) ∈ q .
The tasks will return all its local errors to the master module. After receiving
the locally converged blocks from the tasks, the master module checks whether
the global convergence is satisﬁed,
r[i][j][k] ≤ �, ∀i, j, k ∈ [0, m] ,
�
�
��
� [N +1]
[N ] �
where r[i][j][k] = max �ui,j,k − ui,j,k �, (i, j, k) ∈ q .

The AGEB Algorithm for Solving the Heat Equation

923

This procedure is repeated and the system terminates if a global convergence
is reached. Otherwise the master repartitions the blocks and reassigns them to
the p tasks.

4

Numerical Results and Discussion

The following problem is solved using the AGEB algorithm,
∂U
∂2U
∂2U
∂2U
+
+
+ h(x, y, z, t),
=
∂t
∂x2
∂y 2
∂z 2

0 ≤ x, y, z, t ≤ 1,

t≥0 ,

with
h(x, y, z, t) = (3π 2 − 1)e−t sin πx sin πy sin πz ,
subject to the initial condition,
U (x, y, z, 0) = sin πx sin πy sin πz ,
and boundary conditions
U (0, y, z, t) = U (1, y, z, t) = U (x, 0, z, t) = U (x, y, 0, t) = U (x, y, 1, t) = 0 ,
Our PVM platform consists of a cluster of 9 SUN Sparc Classic II workstations each running at a speed of 70 MHz and conﬁgured with 32 Mbytes
of system memory. The workstations are connected by a 100/10 base T/3com
Hub, Super stack II via a Baseline Dual Speed Hub 12 port network. The PVM
software only resides in the master processor.
As measures of performance of our algorithm, the following deﬁnitions are
used:
Speed-up ratio
Eﬃciency

Sp = T1 /Tp
Ep = Sp /p

(9)
(10)

Eﬀectiveness

Fp = Sp /Cp

(11)

where Cp = pTp , T1 the execution time on a serial machine and Tp the computing
time on a parallel machine with p processors.
Figure 3 shows the speedup factor plotted against the number of processors
p. It indicates that high speedups are obtained only for large values of m. An
impressive gain in the speedups can be expected for even larger problems. Possible reasons for this are: the relatively high communication time for passing
data between the master and slaves compared with the computation time of the
AGEB method; wasteful idle time at a barrier synchronisation point before proceeding with the next iteration; the contribution from parallel processing which
is less than the number of processors and the contribution from the distributed
memory hierarchy, which reduces the time consuming access to virtual memory
for large linear equations. However, we are constrained by the relatively small

924

M.S. Sahimi, N. Alias, and E. Sundararajan

Fig. 3. Speedup ratio vs Number of processors

Fig. 4. Eﬃciency vs Number of processors

The AGEB Algorithm for Solving the Heat Equation

925

memory of the system. The speedup starts to degrade when more than 5 processors are used. It must also be noted that the timing for message passing is
relatively slow for inter-processor communication using Ethernet Card 10-based
network.
As expected, Fig. 4 depicts that eﬃciency decreases with increasing p. It
deteriorates when more than 3 processors are used. This deterioration is a result
of poor load balancing attained when only the small block is spread across more
than 3 processors. Hence the high overhead cost due to synchronisation. From
(9)–(11),
Fp = Sp /(pTp ) = Ep /Tp = Ep Sp /T1
which clearly shows that Fp is a measure both of speedup and eﬃciency. Therefore, a parallel algorithm is said to be eﬀective if it maximises Fp and hence
Fp T1 (= Sp Ep ). From Fig. 5, we see that Fp T1 has a maximum when p = 3 for
m = 39 which indicates that p = 3 is the optimal choice of number of processors.
Similarly, we can infer from m = 45 and 55 that the optimal choice of number of
processors is given respectively by p = 3 and p = 4 allowing for inconsistencies
due to load balancing.

Effectiveness x 1.0E-6 per second

0.8
0.7
0.6

m=39

0.5

m=45
m=55

0.4

m=65

0.3
0.2
0.1
0
1

2

3

4

5

6

7

8

9

No. of processors

Fig. 5. Eﬀectiveness vs Number of processors

As a general conclusion, the inherently explicit, stable and highly accurate
algorithm is found to be well suited for parallel implementation on the PVM
where data decomposition is run asynchronously and concurrently at every time
level.

926

5

M.S. Sahimi, N. Alias, and E. Sundararajan

Conclusions

The stable and highly accurate AGEB algorithm is found to be well suited for
parallel implementation on the PVM where data decomposition is run asynchronously and concurrently at every time level. The AGEB sweeps involve
tridiagonal systems which require the solution of (2 × 2) block systems. Existing parallel strategies could not be fully exploited to solve such systems. The
AGEB algorithm, however, is inherently explicit and the domain decomposition
strategy is eﬃciently utilised.
The PVM is favoured to MPI as the computing platform because of the
ﬂexibility of the former to communicate across architectural boundaries. This is
especially relevant since this research project was initially undertaken with the
view of utilising readily available heterogeneous cluster of computing resources
in our laboratories. Furthermore, we have already noted that higher speedups
could be expected for larger problems. Coupled with this is the advantage that
the PVM has on its fault tolerant features. Hence, for large real ﬁeld problems
which we hope to solve in the immediate future these features become more
important as the cluster gets larger.

Acknowledgments
The authors wish to express their gratitude and indebtedness to the Universiti
Kebangsaan Malaysia, Universiti Tenaga Nasional and the Malaysian government for providing the moral and ﬁnancial support under the IRPA grant for
the successful completion of this project.

References
1. Evans, D.J., Sahimi, M.S.: The Alternating Group Explicit Iterative Method
(AGE) to Solve Parabolic and Hyperbolic Partial Diﬀerential Equations. In: Tien,
C.L., Chawla, T.C. (eds.): Annual Review of Numerical Fluid Mechanics and
Heat Transfer, Vol. 2. Hemisphere Publication Corporation, New York Washington
Philadelphia London (1989)
2. Yanenko, N.N.: The Method of Fractional Steps. Springer-Verlag, Berlin Heidelberg
New York (1971)
3. Evans, D.J., Sahimi, M.S.: The Alternating Group Explicit(AGE) Iterative Method
for Solving Parabolic Equations I: 2-Dimensional Problems. Intern. J. Computer
Math. 24 (1988) 311-341
4. Peaceman, D.W.: Fundamentals of Numerical Reservoir Simulation. Elsevier Scientiﬁc Publishing Company, Amsterdam Oxford New York (1977)
5. Geist, A., Beguelin, A., Dongarra, J., Jiang, W., Manchek, R., Sunderam, V.,
PVM: Parallel Virtual Machine & User’s Guide and Tutorial for Networked Parallel
Computing. MIT Press, Cambridge, Mass (1994)
6. Hwang, K. and Xu, Z., Scalable Parallel Computing: Technology, Architecture,
Programming. McGraw Hill, (1998)
7. Lewis,T.G. and EL-Rewini,H., Distributed and Parallel Computing, Manning Publication, USA, (1998)

The AGEB Algorithm for Solving the Heat Equation

927

8. Quinn, .M.J., Parallel Computing Theory and Practice, McGraw Hill, (1994)
9. Wilkinson,.B. and Allen, M., Parallel Programming Techniques and Applications
Using Networked Workstations and Parallel Computers, Prentice Hall,Upper Saddle River, New Jersey 07458 (1999)

