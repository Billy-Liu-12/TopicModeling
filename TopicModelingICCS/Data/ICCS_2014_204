Procedia Computer Science
Volume 29, 2014, Pages 2336‚Äì2350
ICCS 2014. 14th International Conference on Computational Science

Data Centric Framework for Large-scale
High-performance Parallel Computation
Kenji Ono1,2 , Yasuhiro Kawashima3 , and Tomohiro Kawanabe4
1

RIKEN, Advanced Institute for Computational Science, Kobe, Japan
keno@riken.jp
2
Kobe University, Kobe, Japan
3
Fujitsu Systems East Co.Ltd., Nagano, Japan
kawashima.yasuh@jp.fujitsu.com
4
University of Tokyo, Institute of Industrial Science, Tokyo, Japan
tkawanab@iis.u-tokyo.ac.jp

Abstract
Supercomputer architectures are being upgraded using diÔ¨Äerent level of parallelism to improve
computing performance. This makes it diÔ¨Écult for scientists to develop high performance code
in a short time. From the viewpoint of productivity and software life cycle, a concise yet
eÔ¨Äective infrastructure is required to achieve parallel processing. In this paper, we propose a
usable building block framework to build parallel applications on large-scale Cartesian data
structures. The proposed framework is designed such that each process in a simulation cycle
can easily access the generated data Ô¨Åles with usable functions. This framework enables us
to describe parallel applications with fewer lines of source code, and hence, it contributes
to the productivity of the software. Further, this framework was considered for improving
performance, and it was conÔ¨Årmed that the developed Ô¨Çow simulator based on this framework
demonstrated considerably excellent weak scaling performance on the K computer.
Keywords: Domain Decomposition, Simulation Life Cycle, File Management, Computational Fluid
Dynamics, Middleware

1

Introduction

Scientists and engineers strongly demand to obtain scientiÔ¨Åc knowledge and/or to derive useful
information from large-scale datasets that are yielded through high-performance computation
(HPC). Therefore, data analysis and visualization methods that help acquire information from
computed results are considerably important together with simulations. In other words, post
processes become increasingly signiÔ¨Åcant, and therefore, HPC is required for not only simulation
but also the entire simulation process.
2336

Selection and peer-review under responsibility of the ScientiÔ¨Åc Programme Committee of ICCS 2014
c The Authors. Published by Elsevier B.V.
doi:10.1016/j.procs.2014.05.218

Data Centric Framework

Ono, Kawashima and Kawanabe

Data analysis and visualization can be classiÔ¨Åed into three categories [MWYT07]: 1) Coprocessing, which is usually operated on some speciÔ¨Åc environment that combines a supercomputer and a PC cluster connected high-speed network. Co-processing can provide information immediately after executing the simulation. 2) In-situ processing, which can avoid
time-consuming Ô¨Åle I/O operations, and it is the most promising approach in the peta and
exascale era. This in-situ research is currently investigated vigorously to address issues. 3)
File-based oÔ¨Ñine analysis/visualization, which is called post-processing. Although post processing is a traditional approach, many users request a method for repeatable analytic and
statistic processing because this approach enables them to analyze large datasets in detail even
if it requires a long processing time. From the viewpoint of HPC, however, large-scale parallel
computation yields multiple and large Ô¨Åles at the same time. This makes it diÔ¨Écult to operate
an entire simulation process smoothly.
High-Ô¨Ådelity parallel Computational Fluid Dynamics (CFD) simulation is an HPC application that generates large datasets. Figure 1 presents a conventional simulation cycle of a
practical CFD. In this simulation cycle, intensive Ô¨Åle I/O is required between the simulation
and database processes owing to the large and multiple Ô¨Åles that are generated. It is essential
to address this bottleneck in the simulation cycle to facilitate eÔ¨Écient execution of simulation,
and therefore, we need to design a simulation system to consider an entire simulation cycle.
Therefore, in this paper, we focus on this issue of managing the multiple distributed Ô¨Åles between the simulation and the data analysis/visualization processes so that each process in a
simulation cycle can mutually access data Ô¨Åles.
This paper presents a common framework to describe both simulators and data processing.
For instance, key technologies such as the management of subdomains in the domain decomposition method and eÔ¨Écient Ô¨Åle management for Cartesian data structures are explained in
detail in section 3.

	














	












Figure 1: Conventional simulation cycle for Ô¨Çuid simulation and a concept of the proposed
framework. The centered Ô¨Åle management library assists data transfer between Ô¨Åle I/O intensive
processes, i.e., simulation, visualization, and database.

2337

Data Centric Framework

2

Ono, Kawashima and Kawanabe

Related Work

Language, library, and framework are middleware that contribute to productivity. Many papers
related to the Ô¨Åeld of parallel simulations have been published. Computer languages enable us
to code high-performance parallel programs for complex architectures, which help improve
the productivity of software development. Recently, modern architectures employ cache-based
multicore CPU, NUMA, and massively parallel processing. Increasing the combination of such
devices, increases the complexity of programming, which leads to diÔ¨Éculties in developing high
performance simulators. To help writing a parallel code, one of the most successful example is
that of the hybrid parallel programming model used to combine MPI and OpenMP.
Domain SpeciÔ¨Åc Language (DSL) is an alternative approach. Physis [MNSM11] is a framework for stencil computations that is designed for a variety of parallel computing platforms
including GPGPU. Physis is language extension and consists of a translator and runtime layer
for each supported platform. ROSE compiler [SQ03] is used to generate platform-speciÔ¨Åc source
code from the Physis code, which is then compiled by a platform-native compiler to generate
optimized executables.
A general framework to describe partial diÔ¨Äerential equations (PDE) is proposed using
Object-Orientd Technology (OOT) such as OpenFoam [ope14]. OpenFoam was originally developed as a general C++ toolbox or PDE, and it was applied to the development of customized
numerical solvers to solve continuum mechanics problems, e.g., CFD. For more speciÔ¨Åc purposes, especially in the CFD Ô¨Åeld, Overture [Hen02], SAMRAI [HK02], and Cactus [GAL+ 03]
are widely used. Further, we used the OOT framework SPHERE to construct parallel simulators
coupling with diÔ¨Äerent physics, and conÔ¨Årmed its scalability on several platforms [OTY09]. Although there are many parallel frameworks as described above, it is diÔ¨Écult to Ô¨Ånd a framework
that achieves scalable performance. Our framework aims to construct a scalable application
and to demonstrate its performance in addition to productivity.
As for the Ô¨Åle I/O issue, we can Ô¨Ånd many state-of-the-art libraries, such as HDF5 [HDF14],
netCDF [net14], and ADIOS [ADI14]. They provide eÔ¨Écient parallel Ô¨Åle I/O APIs, database,
multi-platform support, and many other features. Even if we employ these libraries, however,
we still need to design speciÔ¨Åc databases for applications to organize distributed Ô¨Åles using the
APIs provided. Thus, we focused on designing meta-data to organize numerous Ô¨Åles in this
paper.

3

Framework Design

To develop high-performance codes in a short time, it is considerably important to develop a
framework, design, and policy. We aim to construct a domain speciÔ¨Åc application framework
based on standard languages and fundamental libraries considering versatility, portability, continuity, and maintainability. The scope of the proposed application framework is multicore
architecture and their cluster, stencil computation on Cartesian grid, parallel calculation by
domain decomposition method, and C++ class library. The proposed framework consists of
several class libraries that are described in the subsequent subsections.

3.1

Parameter Handling

A simulation code requires input parameters to control solvers or to provide physical parameters initially. To describe parameter, various formats such as plain text, XML, and YAML have
been proposed. In this study, we developed a light-weight simple parameter parsing library
2338

Data Centric Framework

Ono, Kawashima and Kawanabe

TextParser [AIC14] instead of the versatile yet heavy XML library. The TextParser library enables us to describe various parameters through hierarchical nested structure, range description,
and simple dependency notations such as a ternary operator in C. Besides, this class library
can be operated on multiple platforms.
Once an instance of this class library reads a parameter, a parameter tree is constructed.
Then, the user can traverse this tree to obtain parameters without accessing the disk. The
following list is an example of parameter notations.
List 1: Parameters for computational domain.
DomainInfo {
UnitOfLength = "NonDimensional"
GlobalOrigin = (-0.5, -0.5, -0.5)
GlobalRegion = (1.0, 1.0, 1.0)
GlobalVoxel = (256, 256, 256)
/* GlobalPitch = (1.0e-02, 1.0e-02, 1.0e-02) */
GlobalDivision = (4, 3, 2) // number of divisions for the entire computational domain
}

Note that curly brackets enable us to deÔ¨Åne the hierarchy of parameters. In this example, the
parameter GlobalDivision is written in an expression such as a directory path on a Linux
system, i.e., /DomainInfo/GlobalDivision. The three numbers in parenthesis is a vector
notation. In List 1, comments are stated. This commenting style of C and C++ provides good
readability as well.
Further, this library works together with the MPI library so that only the master node reads
the input parameter Ô¨Åle, and then, it distributes parameters to other nodes. This mechanism
can suppress disk access contention when all nodes read a parameter Ô¨Åle together, or can reduce
the amount of load required in transmitting a parameter Ô¨Åle at the staging process.

3.2

Core Library for Parallel Processing on Cartesian Grid

Cartesian Partitioning Manager Library (CPMlib) provides a basic framework for describing
parallel processing code with the domain decomposition method on the Cartesian grid [AIC14].
The main functions of this library are methods for dividing subdomains while retaining load balance, simple wrapper functions of communication for various data type, templates and macros
to access arrays, index exchange between local and global regions, and interfaces for C/C++,
Fortran, and other languages.
3.2.1

Partition Management

One of the main tasks in domain decomposition is partition management. CPMlib provides two
methods for domain decomposition, manual and automatic division. If the CPMlib identiÔ¨Åes
the /DomainInfo/GlobalDivision keyword in an input parameter, the entire computational
domain is divided by speciÔ¨Åed numbers for each axis. Otherwise, the CPMlib will attempt to
identify the optimal number of divisions based on the total number of MPI processes, which is
speciÔ¨Åed by an argument when the ‚Äúmpirun‚Äù command is invoked at command line.
The optimal number of division is obtained as follows:
1. Enumerate alternatives whose product of the number of division is the same as the number
of processes.
2. Calculate the number of cells inside and at the surface area for each subdomain.
2339

Data Centric Framework

Ono, Kawashima and Kawanabe

3. Sort candidates in the order of better load balance.
4. If the load balance is same, choose a better candidate that has ratio of surface-to-volume
required to minimize the amount of communication. This is because the guide cell (halo)
is related to the amount of communication in the calculation.
List 2 shows the deÔ¨Ånition of a partition method for an entire computational domain. The
arguments are taken from the input parameters described in the List 1.
List 2: Domain partioning method.
cpm_ParaManager::VoxelInit( int div[3], int vox[3], double origin[3], double region[3]
, size_t maxVC=1);
// @param [in] div Division size of a computational domain for each axis direction
// @param [in] vox Number of grid cells for each direction
// @param [in] origin Origin of coordinates for the entire computational domain
// @param [in] region Size of the entire computational domain for each direction
// @param [in] maxVC Maximum number of halo regions for the communication buffer

Once the number of division is determined, the rank number of the MPI process is assigned
to each subdomain, and the rank table that stores the neighbors rank is generated. The rank
number is commonly allocated by lexical order. However, optimal allocation based on rank
mapping [IGO12] often achieves a considerably better communication performance. In this
rank table, a negative value is assigned for the face that does not have a neighbor subdomain,
i.e., the outer boundary.
3.2.2

Array Indexing

After the entire computational domain is divided, the relationship of the index between the
local and global regions is determined as shown in Figure 2. The CPMlib provides application
programming interfaces (APIs) to obtain the head and tail index in each subdomain, which are
calculated based on rank information and local dimension size inside the library.
Inside the simulation code, users can access values of arrays by following the macro in List
3. This _F_IDX_S3D() macro provides the one-dimensional index of position from a threedimensional notation (_I, _J, _K) with a dimension size (_NI, _NJ, _NK) and a guide cell
size _VC in each subdomain.
List 3: Macro for indexing.
#define _F_IDX_S3D(_I,_J,_K,_NI,_NJ,_NK,_VC) \
( (size_t)(_K+_VC-1) * (size_t)(_NI+2*_VC) * (size_t)(_NJ+2*_VC) \
+ (size_t)(_J+_VC-1) * (size_t)(_NI+2*_VC) + (size_t)(_I+_VC-1) )

3.2.3

Communication API

Although users may write communication procedures using the APIs provided by the MPI
library, the CPMlib renders simple wrapper APIs. For instance, the native APIs of the MPI
library can be substituted through concise expressions of the CPMlib as described in List 4.
2340

Data Centric Framework

Ono, Kawashima and Kawanabe


























	







































	

Figure 2: Relation of the index between the local and global regions. CPMlib provides APIs to
manage this information required to describe parallel programs based on the domain decomposition method.

List 4: Comparison between native MPI and wrapper method for allreduce communication.
Wrapper method allows fewer arguments.
float* buf, array;
MPI_Allreduce(buf, array, n, MPI_FLOAT, MPI_SUM, MPI_COMM_WORLD) // API of MPI library
cpm->Allreduce(buf, array, n, MPI_SUM) // API of CPMlib

Parallel codes use not only collective communication but also point-to-point communication
as shown in Figure 3. The following method in List 5 deÔ¨Ånes neighbor communication inside all
subdomains. Class methods are implemented using the template class such that the methods
correspond to various data types. Of course, we also provide simple wrapper APIs for pointto-point communications that includes neighbor and periodic communications.
List 5: Wrapper method for neighbor communication.
template<class T>
cpm_ErrorCode
cpm_ParaManager::BndCommS3D( T *array, int imax, int jmax, int kmax,
int vc, int vc_comm, int procGrpNo=0 );

3.3

Performance Monitor

To evaluate performance, users need performance measuring tools. In fact, many performance
evaluation tools that range from simple to integrated functions, e.g., gettimeofday() in standard C library, prof utility, PAPI [pap14], TAU [tau14], and other commercial tools are available.
Some tools require low-level access to hardware and some functions are primitive. Thus, we
proposed an easy-to-use performance monitor PMlib [AIC14].
2341

Data Centric Framework

(a) Neighbor communication.

Ono, Kawashima and Kawanabe

(b) Periodic communication.

Figure 3: Point-to-point communication. The blue area indicates the inside of the subdomain
and the white area represents the guide cell region. Cells shaded by green and red are transferred
to each other.

PMlib oÔ¨Äers simple APIs to acquire GÔ¨Çops values and to display useful statistic information. List 6 demonstrates simple statistics of our Ô¨Çuid simulator described in section 4. Label
represents the measurement sections given by the user. Statistics are reported in mimicry by
output format of prof command utility. Detail mode exhibits more information against every
process.
In this version of PMlib, GÔ¨Çops values are calculated using the declared Ô¨Çops count and
obtained timing for speciÔ¨Åed sections. This approach is simple and portable because other
libraries are not mandatory. In our future work, we plan to introduce the PAPI interface to
obtain a more accurate Ô¨Çop count by accessing hardware counters.
List 6: Example of performance report by PMlib.
Report of Timing Statistics PMlib version 1.2
Parallel Mode
: Hybrid (24576 processes x 8 threads)
Statistics per MPI process [Node Average]
Label
| call |
accumulated time [s]
|
flop | messages[Bytes]
|
|
avr
avr[%]
sdv
avr/call |
avr
speed
--------------+------+--------------------------------------+------------------------Proj Velocity : 400 6.868e+00 42.13 1.26e-02 1.717e-02 9.060e+10
12.28 Gflops
Pseudo Vel
:
20 2.871e+00 17.61 1.06e-01 1.435e-01 3.347e+10
10.86 Gflops
SOR2 SMA
: 800 2.195e+00 13.46 6.83e-02 2.743e-03 3.020e+10
12.81 Gflops
A.R. Poi Res : 400 1.047e+00
6.43 1.31e-01 2.619e-03 7.864e+07
71.57 MB/s
Sync Pressure : 800 7.787e-01
4.78 1.17e-01 9.734e-04 7.463e+12
8.72 TB/s
Sync Variation:
20 4.760e-01
2.92 5.14e-02 2.380e-02 9.437e+07
189.07 MB/s
Poi Nrm DivMax: 400 3.683e-01
2.26 2.37e-03 9.209e-04 4.194e+09
10.60 Gflops
Sync Velocity :
20 2.146e-01
1.32 6.06e-03 1.073e-02 4.478e+12
18.97 TB/s
--------------+------+--------------------------------------+------------------------Total
|
1.630e+01
1.643e+11
9.38 Gflops
Performance|
230625.10 Gflops

3.4

Distributed File Management

File management plays an important role in this framework because distributed, numerous,
and large Ô¨Åles are generated. Once a simulator generates a large-scale dataset, the Ô¨Åles can not
2342

Data Centric Framework

Ono, Kawashima and Kawanabe

be moved or copied for the sake of the limitation of disk space and Ô¨Åle I/O cost in most cases.
Hence, all tasks related to data processing must be invoked where the data exists.
Cartesian Input/Output library (CIOlib) is a Ô¨Åle handling library for structured data and
provides the following functions [AIC14]: management of distributed Ô¨Åles, various Ô¨Åle loading
patterns, staging assistance, and the combination of distributed Ô¨Åles and format exchange.

3.4.1

Management using Meta-information

The CIO library adopts a Ô¨Åle-per-process policy. This approach can be operated on most
computer systems and can achieve reasonable disk I/O performance. However, because this
approach yields multiple Ô¨Åles, meta-information plays a role in the management of all Ô¨Åles, and
users can operate Ô¨Åles through the APIs provided. Of course, because MPI-IO API allows us
to bundle many data Ô¨Åles into a Ô¨Åle, Ô¨Åle management is essential.
Meta-information required to manage distributed Ô¨Åles is classiÔ¨Åed into two types: information related to the parallel process and information related to computed results. This
information is described in proc.dÔ¨Å and index.dÔ¨Å Ô¨Åles respectively, and is usually generated
automatically after simulation. The following list shows proc.dÔ¨Å that includes domain, MPI
rank, and process information. This information is suÔ¨Écient to reconstruct local subdomains
and the global domain. Conversely, the index Ô¨Åle includes the properties of the result Ô¨Åle and
some annotations, e.g., min/max value inside each subdomain, for each time slice.
List 7: Example of proc.dÔ¨Å Ô¨Åle provided by CIOlib.
Domain {
GlobalOrigin = (-0.5, -0.5, -0.5)
GlobalRegion = (1.0, 1.0, 1.0)
GlobalVoxel = (64, 64, 64)
GlobalDivision = (3, 1, 1)
}
MPI {
NumberOfRank = 3
NumberOfGroup = 1
}
Process {
Rank[@] {
ID
= 0
VoxelSize = (22, 64, 64)
HeadIndex = (1, 1, 1)

3.4.2

|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
| }

TailIndex = (22, 64, 64)
}
Rank[@] {
ID
= 1
VoxelSize = (21, 64, 64)
HeadIndex = (23, 1, 1)
TailIndex = (43, 64, 64)
}
Rank[@] {
ID
= 2
VoxelSize = (21, 64, 64)
HeadIndex = (44, 1, 1)
TailIndex = (64, 64, 64)
}

File Loading Patterns

The number of execution processes may vary in some cases, for instance, between simulation
and data processing/visualization, or the restart process in simulation. In many cases, Ô¨Åle-based
data processing after simulation employs much fewer processes than the simulation itself. We
consider and provide various Ô¨Åle loading patterns for the user‚Äôs convenience.
The information included in the proc.dÔ¨Å can automatically process the following patterns.
2343

Data Centric Framework

Ono, Kawashima and Kawanabe

Normal Loading The number of processes and the situation of partitioning are the same
between the previous and the current session1 . Most cases choose this pattern as shown in
Figure 4(a).
ReÔ¨Ånement Loading In this reÔ¨Ånement case as shown in Figure 4(b), the number of processes is the same; however, the resolution is diÔ¨Äerent. Each process in the current session
reads the same Ô¨Åle that the process has written in the previous session. However, because the
resolution is doubled, interpolation should be performed. Users can overload the interpolation
method.
MxN Loading M and N denote the number of processes in the previous and current sessions,
respectively. Even if M and N are equal but the partitioning is diÔ¨Äerent, the loading pattern
becomes MxN loading. Figure 4(c) shows this pattern for the same resolution, and Figure 4(d)
demonstrates the case of diÔ¨Äerent resolutions and diÔ¨Äerent partitioning. This loading pattern
is considerably useful when the user changes the number of processes in a successive simulation
process.
3.4.3

Staging Assistance

In certain computational environments, the Ô¨Åles required for computation need to be transferred
from the global Ô¨Åle system to the local Ô¨Åle system, which is a dedicated disk partition for
computations in a batch job, before the calculation begins. This Ô¨Åle transfer process is called
a stage-in. In the MxN Ô¨Åle loading pattern, each current process may need diÔ¨Äerent Ô¨Åles that
the process wrote in the previous session. In this case, the Ô¨Åle that the process needs must be
determined prior to submitting a batch job. This procedure is also performed using proc.dÔ¨Å
information. Figure 5 depicts an example. The previous session has M = 9 processes and the
current session has N = 4. For instance, the current process of rank 0 (N{0}) demands M{0,
1, 3, 4}. This relationship can be obtained from the information described in proc.dÔ¨Å.

4

Applications

Thus far, we developed a Ô¨Çow simulator and a data processing program utilizing the proposed
framework described in section 3 to demonstrate our concept.

4.1

Building Applications

Since the most tedious tasks are already implemented into the framework described in section
3, it is comparatively simpler to build a new parallel application for Cartesian data structures.
List 8 shows a basic example of writing a parallel application. To create other applications, it is
suÔ¨Écient to write a small amount of code (in fact, it is approximately 30 lines) before you want
to describe your parallel algorithm. This is obviously a signiÔ¨Åcant advantage for productivity.
List 8: Building parallel application with CPMlib.
1
2

#include "cpm_ParaManager.h"
#include "cpm_TextParserDomain.h"

3
4

int main( int argc, char **argv )
1 Session

2344

represents the calculation or process in this context.

Data Centric Framework
		

Ono, Kawashima and Kawanabe
	

	




(a) Normal loading pattern with same resolution, same partitioning. This example shows three
MPI processes.

		

	

	




(b) Loading pattern with diÔ¨Äerent resolution, same partitioning. Four MPI processes read
previous Ô¨Åles with interpolation.
		

	

	




(c) Loading pattern with same resolution, diÔ¨Äerent partitioning. The current four processes
read data from the previous three Ô¨Åles.

		

	

	




(d) Loading pattern with diÔ¨Äerent resolution, diÔ¨Äerent partitioning. The current three processes read data from the previous four Ô¨Åles with interpolation.

Figure 4: CIOlib supports various Ô¨Åle loading patterns.

2345

Data Centric Framework

Ono, Kawashima and Kawanabe
	





	

























	
	

















Figure 5: File distribution for the staging process.

5

{
// Instance of parallel manager
cpm_ParaManager *paraMngr = cpm_ParaManager::get_instance(argc, argv);

6
7
8

// get domain parameters
args = ....

9
10
11

// domain decomposition
paraMngr->VoxelInit( args... );

12
13
14

// receive domain informations
int myRank = paraMngr->GetMyRankID();
int numProc = paraMngr->GetNumRank();

15
16
17
18

// get neighbor rank
int nID[6];
const int* neighbour = paraMngr->GetNeighborRankID();
for (int i=0; i<6; i++)
{
nID[i] = neighbour[i];
}

19
20
21
22
23
24
25
26

// get head index for each domain
int head[3];
const int* hdx = paraMngr->GetVoxelHeadIndex();
head[0] = hdx[0];
head[1] = hdx[1];
head[2] = hdx[2];

27
28
29
30
31
32
33

// write your own parallel algorithm
printf("Rank = %d : Head(%d %d %d)\n", myRank, head[0], head[1], head[2]);
...

34
35
36
37

}

2346

Data Centric Framework

4.2

Ono, Kawashima and Kawanabe

Application Examples

Flow Simulator The developed simulator FFV-C [AIC14] solves a three-dimensional unsteady thermal Ô¨Çow on Cartesian grid system. The aim of this simulator is to provide useful
information from computed results to assist the design of practical industrial products, where
engineers demand both large-scale calculation and data processing.
The computation was performed on the K computer [MKS+ 12] in RIKEN. The K computer
has over 80,000 8-core compute nodes. Its theoretical peak performance is over 1016 Ô¨Çoatingpoint operations per second (10 PFLOPS). Figure 6(a) presents a snapshot of the computational
results. This simulation performed with 9,216 processors on the K computer for a computational
domains consists of 6, 144 √ó 3, 072 √ó 1, 536 cells. This Ô¨Åne grid suÔ¨Éciently resolves turbulent
vortex structures after 24 hours computation. In fact, we can observe hairpin-type vortex
structures on the front windshield, which are remarkable structures at fully turbulent Ô¨Çow.
Figure 6(b) represents a weak scaling performance up to 32,768 processes √ó 8 cores for a
simple cavity Ô¨Çow case. The computation load assigned to each subdomain is 256 √ó 256 √ó 256
cells in this performance evaluation. The measured serial performance is 15.16 GÔ¨Çops and it is
11.8% of the theoretical peak performance for a node. This hybrid parallel performance exhibits
excellent scalability of the developed Ô¨Çow simulator that is built on the proposed framework.
Table 1 reveals the details of timing measured by PMlib built in FFV-C application. The
values of the standard deviation for Ô¨Çop count tells us that the load balance is perfect in this
case. Further, it is identiÔ¨Åed that section A R Poisson Src is the main factor that inhibits
performance owing to the MPI allreaduce operation.
Table 1: Measured timing of cavity Ô¨Çow case with 16,384 processors by the PMlib function.
Sdv represents the standard deviation.
Measured section
Poisson SOR2 SMA
Poisson Src Norm
Pseudo Velocity
A R Poisson Src
Sync Pressure
Projection Velocity
Divergence of Pvec
Sync Velocity

Call
2000
1040
20
1040
2000
20
20
20

Avr[%]
34.19
28.72
13.70
8.55
6.77
2.56
1.40
1.05

Flop
4.5e+11
6.5e+11
2.6e+11
2.7e+08
4.9e+13
3.5e+10
1.2e+10
1.1e+13

Sdv
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0

Flops
10.72
18.58
15.81
26.41
5.77
11.23
7.19
8.91

GÔ¨Çops
GÔ¨Çops
GÔ¨Çops
MB/s
TB/s
GÔ¨Çops
GÔ¨Çops
TB/s

Figure 7 shows a comparison result of FFV-C‚Äôs single core performance on diÔ¨Äerent machines. Using the proposed framework, the simulator can be run on several computing platforms. Besides, the performance statistics are obtained automatically and can be analyzed,
which helps performance improvement.
Data Analysis Program Next, the developed framework was applied to construct an application to sample data and extracting an iso-surface as shown in Figure 8. Both functions
are classiÔ¨Åed into the Ô¨Ålter program for computed results. The data sampling module reads
computed data Ô¨Åles and extracts values at speciÔ¨Åed points. Then, the sampled data is used in
the application to draw a graph. On the other hand, the iso-surface generation is implemented
using the Marching-cubes algorithm to extract the speciÔ¨Åed iso-surface value as polygon data.
2347

Data Centric Framework

Ono, Kawashima and Kawanabe

(a) Flow calculation with 29G elements. The magnitude of vorticity is
visualized by the iso-surface.




 	



 
 
 
 
 


 

 

 

 

 



(b) Weak scaling performance for a cavity Ô¨Çow.

Figure 6: Example of a large-scale Ô¨Çuid simulation for an automobile conÔ¨Åguration and the
performance of FFV-C simulator.

This geometry data is visualized by a rendering application. Figure 6(a) presents an example
of an extracted iso-surface. These Ô¨Ålter programs can be described eÔ¨Éciently using less source
code by employing the same building block (see List 8 as prototyping).

5

Concluding Remarks

In this study, we successfully developed a useful framework to describe large-scale parallel
applications on Cartesian data structures. This framework consists of helpful functions such as
the management of domain decomposition, Ô¨Åle management/access, parameter handling and
2348

Data Centric Framework

Ono, Kawashima and Kawanabe



!&%& $
!&!($
" #'&$
'&%'











	



	




	



")

Figure 7: Comparison of single core performance for diÔ¨Äerent architectures, e.g., Intel Westmere
2.66 GHz, Sandy-Bridge 2.6 GHz, the K computer 2.0 GHz, and FX-10 1.85 GHz. The model
size ranges from 163 to 2563 . Owing to the special compiler, the K computer achieves higher
GÔ¨Çops than FX-10.



'!





!





	#%



















!

!!%#$!!



	




 '
 
 '
 '"%
 "
 
"
 
 

 







Figure 8: Diagram of simulator and post processing programs based on the proposed framework.
These applications share data access methods and software components.

2349

Data Centric Framework

Ono, Kawashima and Kawanabe

performance tools, and it is provided by the C++ class libraries. It was demonstrated that
this framework enables us to describe parallel applications with less amount of source codes. A
developed CFD application indicated excellent weak scaling performance on the K computer
and portability on several platforms. Therefore, it was conÔ¨Årmed that the proposed framework
contributes to the productivity of applications with higher scalability. Further, the current
approach appears to be valid for the maintenance and long life-cycle of simulation code.
Acknowledgments We thank the RIKEN Advanced Institute for Computational Science for
allowing us to use the K computer for obtaining our results.

References
[ADI14]
[AIC14]
[GAL+ 03]

[online], 2014. http://www.olcf.ornl.gov/center-projects/adios/.
[online], 2014. https://github.com/avr-aics-riken/.
T. Goodale, G. Allen, G. Lanfermann, J. Masso, T. Radke, E. Seidel, and J. Shalf. The
cactus framework and toolkit: Design and applications. In 5th International Conference
Vector and Parallel Processing - VECPAR ‚Äô2002. Springer, 2003.
[HDF14]
[online], 2014. http://www.hdfgroup.org.
[Hen02]
W.D. Henshaw. Overture: An object-oriented framework for overlapping grid applications.
In AIAA conference on Applied Aerodynamics, 2002.
[HK02]
R.D. Hornung and S.R. Kohn. Managing application complexity in the SAMRAI objectoriented framework. In Concurrency and Computation, volume 14 of Practice and Experience (Special Issue), pages 347‚Äì368, 2002.
[IGO12]
Satoshi Ito, Kazuya Goto, and Kenji Ono. Automatically optimized core mapping to subdomains of domain decomposition method on multicore parallel environments. Computers
& Fluids, 2012.
[MKS+ 12] Hiroyuki Miyazaki, Yoshihiro Kusano, Naoki Shinjou, Fumiyoshi Shoji, Mitsuo Yokokawa,
and Tadashi Watanabe. Overview of the k computer system. FUJITSU Sci. Tech. J.,
48(3):255‚Äì265, 2012.
[MNSM11] Naoya Maruyama, Tatsuo Nomura, Kento Sato, and Satoshi Matsuoka. Physis: An implicitly parallel programming model for stencil computations on large-scale gpu-accelerated
supercomputers. In Proceedings of the 2011 ACM/IEEE Conference on Supercomputing
(SC‚Äô11), pages 1‚Äì12, Nov. 2011.
[MWYT07] Kwan-Liu Ma, Chaoli Wang, Hongfeng Yu, and Anna Tikhonova. In-situ processing and
visualization for ultrascale simulations. Journal of Physics:Conference Series, 78:012043,
2007.
[net14]
[online], 2014. http://www.unidata.ucar.edu/software/netcdf/.
[ope14]
[online], 2014. http://www.openfoam.com/.
[OTY09]
Kenji Ono, Tsuyoshi Tamaki, and Hiroyuki Yoshikawa. Development of a framework for
parallel simulators with various physics and its performance. Lecture Notes in Computational Science and Engineering, 67:9‚Äì18, 2009.
[pap14]
[online], 2014. http://icl.cs.utk.edu/papi/index.html.
[SQ03]
M. Schordan and D. Quinlan. A source-to-source architecture for user-deÔ¨Åned optimizaÀú urmAl‚Äônyi
Àú
Àú uszAÀù
tions. In L. BAÀù
and P. Schojer, editors, Lecture Notes in Computer Science,
volume 2789, pages 214‚Äì223, Berlin / Heidelberg, 2003. Springer.
[tau14]
[online], 2014. http://www.cs.uoregon.edu/research/tau/home.php.

2350

