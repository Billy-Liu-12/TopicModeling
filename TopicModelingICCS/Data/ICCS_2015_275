Procedia Computer Science
Volume 51, 2015, Pages 1742–1751
ICCS 2015 International Conference On Computational Science

An individual-centric probabilistic extension for OWL:
Modelling the Uncertainness
Salvatore F. Pileggi
INRIA & UPMC-LIP6, Paris, France
flavio.pileggi@lip6.fr

Abstract
The theoretical beneﬁts of semantics as well as their potential impact on IT are well known
concepts, extensively discussed in literature. As more and more systems are currently using
or referring semantic technologies, the challenging third version of the web (Semantic Web or
Web 3.0) is progressively taking shape. On the other hand, apart from the relatively limited
capabilities in terms of expressiveness characterizing current concrete semantic technologies,
theoretical models and research prototypes are actually overlooking a signiﬁcant number of
practical issues including, among others, consolidated mechanisms to manage and maintain
vocabularies, shared notations systems and support to high scale systems (Big Data). Focusing
on the OWL model as the current reference technology to specify web semantics, in this paper
we will discuss the problem of approaching the knowledge engineering exclusively according to a
deterministic model and excluding a priori any kind of probabilistic semantic. Those limitations
determine that most knowledge ecosystems including, at some level, probabilistic information
are not well suited inside OWL environments. Therefore, despite the big potential of OWL, a
consistent number of applications are still using more classic data models or unnatural hybrid
environments. But OWL, even with its intrinsic limitations, reﬂects a model ﬂexible enough to
support extensions and integrations. In this work we propose a simple statistical extension for
the model that can signiﬁcantly spread the expressiveness and the purpose of OWL.
Keywords: Semantic Computing, Knowledge Modelling, Ontology Engineering, OWL

1

Introduction

During the past years semantic technologies have progressively emerged as a response to the
challenges of a new generation of systems reﬂecting a novel perspective of the information,
extremely demanding in terms of interoperability [1]. The theoretical beneﬁts of semantics,
as well as their potential impact on IT, are well known concepts, extensively discussed in
literature. As more and more systems are currently using or referring semantic technologies, the
challenging third version of the web (Semantic Web or Web 3.0 [2]) is progressively taking shape.
A semantic approach is currently in use into a wide range of application domains to model and
1742

Selection and peer-review under responsibility of the Scientiﬁc Programme Committee of ICCS 2015
c The Authors. Published by Elsevier B.V.

doi:10.1016/j.procs.2015.05.378

Modelling the Uncertainness

Salvatore F. Pileggi

process complex data ecosystems. Common examples include (but evidently are not limited to)
the next generation of services (semantic services [3]) and a set of techniques and solutions that
apply semantic reasoning as well as speciﬁc semantic support to enhance the interoperability
among systems on a large scale. On the other hand, apart from the relatively limited capabilities
in terms of expressiveness characterizing current concrete semantic technologies, theoretical
models and research prototypes are actually overlooking a signiﬁcant number of practical issues
including, among others, consolidated mechanisms to manage and maintain vocabularies, shared
notations systems and support to high scale systems (Big Data [4]). Therefore, while the role of
semantics inside modern IT is actually a completely consolidated concept, semantic technologies
are expected to evolve in the next future to cover current lacks and to provide a more consistent
support to practical applications. While the technology has progressed to embrace signiﬁcant
semantics, now semantic technologies have to evolve consequently and have to exhaustively
cope larger sets of requirements from multiple domains. Focusing on the OWL model [5] as the
current reference technology to specify web semantics, in this paper we will discuss the problem
of approaching the knowledge engineering exclusively according to a deterministic model and
excluding a priori any kind of probabilistic semantic. Indeed, in the era of Big Data entropy
plays a key role in many contexts and, consequently, the uncertainness is more and more part
of most modern models and processes. Modelling uncertainness is a big challenge. In the
context of this paper, uncertainness is approached from a semantic perspective, assuming the
Ontology schema as a deterministic asset inside the data model and the Ontology population
as a probabilistic component of the information. Even though without exhaustively covering
the modelling of uncertainness at an overall level, this vision is very consistent at a practical
level but, unfortunately, not supported by current models and technologies. Those limitations
determine that most knowledge ecosystems including, at some level, probabilistic information
are not well suited inside OWL environments. Therefore, despite the big potential of OWL, a
consistent number of applications are still using more classic data models or unnatural hybrid
environments. Because of its ubiquity and its intrinsic relation to real world actions, processes
and phenomena, modelling uncertainness and its contextual processing are extremely important.
Addressing uncertainness according to an ontologic approach guarantees a consistent semantic
model and increased reasoning capabilities. OWL, even considering its intrinsic limitations,
reﬂects a model ﬂexible enough to support extensions and integrations. In this work a simple
statistical extension for the model that can signiﬁcantly spread the expressiveness and the
purpose of OWL is proposed.
This paper is structured according to a classical schema: the introductory part is completed
by the next section that provides an overview of the related work focusing on the main diﬀerences
between this work and similar ones; then the core part of the paper is proposed; ﬁnally, as
common, the paper ends with a conclusions section. The core part is composed of three diﬀerent
sub-sections that respectively deal with the representation of probabilistic semantics on OWL
according to an individual-centric perspective, with its computation and with the description
of a complex study case.

2

Related Work

Despite most researchers explicitly or implicitly address the need for probabilistic semantics,
there is not, currently, a signiﬁcantly extended and comprehensive documentation in literature,
probably due to a fundamental lack of consensus about an uniﬁed view of probabilistic ontologies [6]. While theoretical studies on uncertainness appear too abstracted to cope requirements
of applied semantics, most practical works (e.g.[7]) seem to provide ad-hoc solutions for spe1743

Modelling the Uncertainness

Salvatore F. Pileggi

ciﬁc problems more than a comprehensive generic solution. Indeed, in a system dealing with
probabilistic ecosystems, due to the intrinsic need for probabilistic schemas, OWL structures
are usual to have a minor scope and cannot provide, generally, trustful inferring and reasoning.
However it is actually possible to clearly identify several approaches to probabilistic semantics.
[8], [6] and [9] proposes a Bayesian extension to OWL, explicitly aimed at the development of
probabilistic ontologies maintaining, according to the authors, the compliance with the original
model. Similar approaches integrates OWL with probabilistic statements. The natural probabilistic extension of rdf ([10]) is probably the most natural one but several alternatives based on
other logics have been proposed during the last years. For instance, in [11] probabilistic Datalog
is used to model uncertainness, as well as in [12] log-linear description logics are adopted. These
models propose close approaches that cannot evidently cover the needs of a dynamic scenario
as the Semantic Web [13], even considering their potential evolution [14]. The work proposed
signiﬁcantly diﬀers from the cited ones because of the following interrelated points:
Practical approach. The objective of this work is not to fully cover the theoretical model of
uncertainness and, certainly, not to provide a new semantic technology able to represent
and process probabilistic ontologies in a wide contest such as the Semantic Web. On the
contrary, this work has a practical focus and wants to explain how the current OWL model
can support a probabilistic understanding of ontologies by using just minor extensions that
are not altering the OWL philosophy.
Individual-centric perspective for uncertainness. The uncertainness is indeed modelled
from an individual-centric perspective according to the OWL model [5]. In practice, the
concept schema inside the ontology is assumed to be fully deterministic but the population
it isn’t. This approach assures a convergence between the semantic interoperability at the
basis of semantic technologies and the requirements in terms of probabilistic modelling.
This is coherent with the previous point and, as extensively discussed in the following
sections of the paper, this solution cover a signiﬁcant set of practical case in the context
of popular domains (e.g. Data Mining and Machine Learning) where the ontology schema
is well known (or needs to be well known) while its population is the result of complex
processes that usually introduce entropy and elements of uncertainness.
Simplicity and Applicability. From a naive point of view, OWL is a consolidated data
model that can be currently considered as a de facto standard for knowledge representation. However, the potential of OWL-like models are for the most unexplored. In the
philosophy of this work, designers are expected to model their knowledge environments
exactly as they are doing by using classic OWL. The uncertainness is modelled, where
needed, in a natural way without introducing any logic external to OWL.

3

Probabilistic Semantics to model Uncertainness

An OWL Ontology is composed of four diﬀerent correlated elements: the concept (or class
or thing), the property (ObjectProperty and DataProperty) and the individual. Focusing on an
individual-centric perspective (ﬁgure 1), an individual element of the ontology can be associated
to one or more concepts or, equivalently, the individual element is an instantiation of one or
more given classes (concepts). Furthermore an individual can be related to other individuals
through ObjectProperties, as well as it can be related to other data types external to the
ontology through DataProperties. The deterministic speciﬁcation of the individuals inside the
ontology makes the whole model a deterministic environment. On the contrary, a probabilistic
1744

Modelling the Uncertainness

Salvatore F. Pileggi

Figure 1: Individual-centric perspective of an OWL Ontology and probabilistic extensions.
speciﬁcation of individuals means, as explained in the next sub-section, a probabilistic model
able to represent the uncertainness in practical cases.

3.1

Modelling Probabilistic Semantics

An individual inside an OWL Ontology (eq. 1) is an element associated with a given Domain
represented by the Ontology itself. The association of an individual with a Domain is a fully
deterministic relation in the classic OWL and in this model extension too.
Individual = [Domain, N ame, T ype, Assertions, ]

(1)

An individual, as any other element of the Ontology, is univocally identiﬁed inside the
domain by its Name. The identiﬁer is intrinsically deterministic. The characterization of an
individual (”what is it?”) is provided through the speciﬁcation of its Type (Class Assertion)
that is an association with one or more ontology concepts (also called things). This relation is
similar to the instantiation (individual in this case) of a class (concept in this case) in objectoriented models. In OWL, the ontology schema is deterministic as well as the association of
individuals with concepts meanings that the existence and the nature of individuals are fully
deterministic assumptions. The proposed model maintains a deterministic schema of concepts
but introduces a probabilistic association of individuals with concepts (eq. 2). This means
that concepts and their hierarchies are always valid inside a given domain as well as speciﬁed
individuals. But the association of an individual with a concept is valid with a given probability
(eq. 2).
T ypei = T :

i is − a[p=px ] x
x is − a T hing

⇔

is − a(i, x)[p=px ]
is − a(x, T hing)

(2)

In the same way, a set of P Assertions (eq. 3) deﬁnes the relations of an individual i with
other individuals as well as with information external to the Ontology, respectively through
assertions on ObjectProperties (OP Assertions) and on DataProperties (DP Assertions).
P
AssertionsP
i = [OP Assertionsi

∪

DP AssertionsP
i ]

(3)
1745

Modelling the Uncertainness

Salvatore F. Pileggi

In the classic OWL model, assertions are understood as deterministic relations and, therefore, they are always valid like deterministic facts. Addressing probabilistic assertions is equivalent to establish a relation through a given property with a well-deﬁned probability (eq. 4 and
eq. 5).
⎧
i has[p=px ] x with − value A
⎪
⎪
⎨
x is − a ObjectP roperty
=
OP AssertionsP
i
A is − a[p=pA ] y
⎪
⎪
⎩
y is − a T hing
⎧
(4)
has(i, x, A)[p=px ]
⎪
⎪
⎨
is − a(x, ObjectP roperty)
⇔
is − a(A, y)[p=pA ]
⎪
⎪
⎩
is − a(y, T hing)
⎧
⎨ i has[p=px ] x with − value A
x is − a DataP roperty
=
DP AssertionsP
i
⎩
A is − a[p=pA ] some data
⎧
(5)
has(i, x, A)[p=px ]
⎨
is − a(x, ObjectP roperty)
⇔
⎩
is − a(A, some data)[p=pA ]
Probabilistic features are not extended to negative property assertions that provide a unique
opportunity to make statements where we know something that is not true. This kind of information is particularly important in OWL where the default stance is that anything is possible
until you say otherwise. The encoding of those theoretical individual-centric probabilistic extensions in an OWL implementation is pretty immediate and can consist of XML properties in
the corresponding XML tags to associate given probabilities. A possible syntax is showed in the
code example 1 where an individual i is deﬁned as part of an Ontology and is associated with a
concept a. Property assertions relate a given data (value) through the dataproperty Dp1 and an
ontology concept k through the objectproperty Op1. As showed in the example, an added XML
property provides a further semantic expressing the probabilities for the considered relations.
Code example 1: Use of extended OWL codiﬁcation.
<o w l : N a m e d I n d i v i d u a l r d f : a b o u t=”&Ontology ; i ”>
<r d f : t y p e r d f : r e s o u r c e=”&Ontology ; a ” statOWL:probability=”pa” />
<Ontology:Dp1 statOWL:probability=”pDp1”>v a l u e</ Ontology:Dp1>
<Ontology:Op1 r d f : r e s o u r c e=”&Ontology ; k” statOWL:probability=”pOp1” />
</ o w l : N a m e d I n d i v i d u a l>

3.2

Inferring Probabilistic Semantics

The extensions provided are semantically relevant even though, as showed, they just require
minor syntactic notations. Those extended semantics are, without any doubt, signiﬁcant at a
level of knowledge representation as they allow to dynamically deﬁne probabilistic types and
relations in the context of a formal data model. Furthermore, a fundamental and eﬀective
compliance with the original model and its philosophy is maintained. On the other hand, the
1746

Modelling the Uncertainness

Salvatore F. Pileggi

most appreciated OWL feature is surely the capability to infer information from facts by deﬁning
logic rules. Inferred concepts make Ontology a rich data model where complex relations can
be expressed in a declarative way, embedded in the schema and computed in an interoperable
context. The OWL inference model doesn’t need signiﬁcant extensions to work on probabilistic
data. The key concept is that an inference rule, that involves probabilistic data, returns a
probabilistic outcome. The probability associated with the inferred concept is a function of the
probabilities related to the input elements. In the context of this work, the elements of the
ontology are assumed to be statistically independent, so the probability of an inferred concept
can be calculated as the product of the probabilities associated with the input elements. The
k
probability Pinf
of a concept inferred by the rule k involving n ontology elements is given by
the product of the probabilities associated with the involved elements (eq. 6). A simple example
of OWL inference rule is deﬁned by the code example 2: a concept c is inferred by the rule k
assuming that the objectproperty Op1 is associated with some element of type b.
k
Pinf

n

n

ei
i=1

P (ei )

=

(6)

i=1

Code example 2: Inferred concept involving an Individual Class and an Object Property.
<o w l : C l a s s r d f : a b o u t=”&Ontology ; c ”>
<o w l : e q u i v a l e n t C l a s s>
<o w l : R e s t r i c t i o n>
<o w l : o n P r o p e r t y r d f : r e s o u r c e=”&Ontology ; Op1” />
<owl:someValuesFrom r d f : r e s o u r c e=”&Ontology ; b” />
</ o w l : R e s t r i c t i o n>
</ o w l : e q u i v a l e n t C l a s s>
</ o w l : C l a s s>
The probability Pck associated with the output of the rule k can be obtained particularizing
eq. 6 to an individual-centric probabilistic model. In the example, there are two diﬀerent
element of uncertainness: the probability P(Op1) with whom Op1 is associated to its value and
the probability P(b) of the value to be an element of type b. Therefore, Pck is deﬁned by eq. 7.
Pck (Op1 ∩ b) = P (Op1)P (b)

3.3

(7)

A case study

In this sub-section, a use case that points out the need for probabilistic semantics (and the
beneﬁts introduced by them) is shortly described. That is a complex case study which integrates common information retrieval techniques with abductive semantic reasoning on geographic space [15]. The data source is a general purpose social network (e.g.Facebook, Twitter,
Google+) that implies a potential Big Data context with the consequent strong requirements
for knowledge representation [16, 17, 18] and mining as well as for information geo-processing
[19]. An outline of the process is showed in ﬁgure 2. It is composed of two sequential phases:
• Information Retrieval. The purpose of this phase is to collect contents from a social network matching a given pattern and to geo-localize them according to a semantic approach
[17, 18]. It is based on the simultaneous processing of natural language and meta-data.
1747

Modelling the Uncertainness

Salvatore F. Pileggi

Figure 2: Schematic view of the case study proposed.
• Information Processing. The input of this second sub-process is the information retrieved
by the previous phase. That information is processed in its social and geographic context
[15], modelled by an extensive set of semantic proﬁles reﬂecting spatial and social relations.
The reliability of the whole process depends on the accuracy of data retrieved during the
ﬁrst phase of the process. The key factor in this sense is the social network as an input.
The most popular social platforms are evidently involved in complex business models and they
are clearly evolving according to a commercial philosophy, along with a fundamental charm
to attract millions of users. Even following a business-driven evolution, the last generation
network is oﬀering exciting possibilities from a scientiﬁc perspective because the explicit and
implicit knowledge they can provide. Apart from the intrinsic and generic diﬃculty to manage
extreme-scale dynamic information, retrieving information on social networks usually proposes
further challenges in terms of reliability with respect to the retrieval on static contents (e.g.
documents). First of all the (continuously changing) content itself. A multimedia content
is not easy to ”understand” if it is not considered in the proper context. Even for humans.
Much more for a machine. Even though limiting the analysis to text contents, the common
ambiguities characterizing natural languages have a stronger impact because of the shortness
of social content that signiﬁcantly limits the performance of retrieval techniques [20]. Metadata commonly associated with content is a powerful integration for basic (but key) purposes
(e.g.geo-localization and timing) as well as for more sophisticated scopes (normally aimed at
establishing relations among contents and/or users). Meta-data can be diﬀerent from platform
to platform. Some meta-data is automatically managed by platforms, others by users. Designing
”perfect” meta-data is hard. For example the famous hash-tag characterizing Twitter (and more
recently appeared also on Facebook) is intrinsically understood as an user-centric mechanism:
its completely syntactic character is very appreciated by users and suitable for a certain kind
of quantitative analysis on large scale but it looks not too adequate for detailed analysis.
Timing and geo-localization are intrinsically critical for social contents: there is a clear diﬀerence
between the time and the place a content belong and the time and place of its publication. Many
platforms simply doesn’t model that key diﬀerence. Others provide speciﬁc ﬁelds that are not
always properly used by network members. It’s also common that platforms try approximations
on geo-localization for commercial purposes that can have a bad impact on semantic spaces
[17, 18]. The process represented in ﬁgure 2, like any other potentially addressing Big Data in a
context-less ambiguous environment such as social networks, can be aﬀected by a fundamental
lack of reliability. But, if an individual-centric probabilistic model is adopted, then also the
opportunity to model and process the uncertainties, the ambiguities and the entropy as a part
of the knowledge in a semantic context is oﬀered. A deterministic process is not managing
any probabilistic information so, in most cases, there is no information about the reliability of
processes as a function of the information they are processing. In some case, if the reliability
1748

Modelling the Uncertainness

Salvatore F. Pileggi

of the information retrieval process is known both with a correspondent error rate, the whole
process is assumed to have the same reliability of the retrieval phase. This case study, just
analysing overall statistics from low-scale experiments on real data and without the need to
go throughout details, shows that common approximations are far away from the reality and
that probabilistic semantics are a key factor to a reliable semantic computation in presence of
uncertainness. Deﬁning the correctness of a retrieval process on social networks is not obvious.
In this work we have adopted an ad-hoc approach that assure a proper understanding of what
is correct, what it is not and what is ambiguous. The information retrieval phase takes in input
social objects and has to determine:
• Content. It is a ﬁlter. A given pattern (such as representing a topic of interest) determines
the contents of interest. In this experiment, the pattern is deﬁned by a set of generic
keywords in a given domain. For example, the keyword party in the domain lifestyle is
synonymous with ”people having a party”. Detecting ”Tea Party” in an evident politic
context is considered an error, even though the syntactic matching. Errors can be limited
increasing the number of keywords but, in that case, the capability of detection is usually
decreasing especially on short contents. Experiment at a relatively low scale on known
dataset have shown a good reliability in the range 91-96% with an average error of 8%
(see table 1). Cases in which there is a syntactic matching but the semantic matching
cannot be determined are excluded by this statistic as well as contents that doesn’t have
a syntactic matching are excluded a priori regardless from their meaning.
• Localization. In order to provide a contextual geo-processing, contents have to be associated with a location. The process under study adopts a semantic model for the geographic
space [17, 18] that assumes the geographic space partitioned into correlated containers.
Therefore, regardless from the level of detail, an error happens when a content is associated to a wrong container. The localization works on meta-data and, eventually, on the
text content. Also the performance of localization (see table 1) are pretty good (average
error around the 8%) but the error variance is relatively high. Anyway, most experiment
are in a range of reliability between 83% and 94%. Non-localizable contents were not
considered.
• Timing. Determining the correctness of the timing is pretty hard. Considering a relatively
accurate time range (1 day) and only explicit information, performance looks good (see
table 1) but those numbers are just a reference as an accurate validation is objectively
hard.
As previously mentioned, the experiment about information retrieval were done on real set
of data on relatively low scale. It’s hard to predict how those analytics could evolve as the scale
signiﬁcantly grows up. It is reasonable to assume a proportional increasing of the information
entropy and results varying in a wide range as a function of the patterns and of the concrete
network clusters that are analysed (assuming the processing of a whole network is unrealistic).
According to the proposed approach, ontology are populated according to an individual-centric
probabilistic model and, therefore, every query has its own probability associated with the
result. Anyway, the semantic reasoner works on speciﬁc inferred concepts so it is possible to
establish at least a kind of ”query class” and assume the reasoning outcomes proportional to
those classes. In order to provide a probabilistic overview of the process reliability, two diﬀerent
classes of reasoning are considered:
• Quantitative analysis. It assumes a contextual semantic geo-processing of the information
based on a quantitative approach. In practice the ﬁltering on contents is not relevant (all
1749

Modelling the Uncertainness

Salvatore F. Pileggi

Table 1: Probabilistic overview of the case study.
Information Retrieval
Reliability(range) Reliability(av.) Error(range)
Localization
83-94%
92%
6-17%
Timing
87-95%
91%
5-13%
Content
91-96%
92%
4-9%
Information Processing
Quantitative Analysis
72-89%
84%
11-28%
Qualitative Analysis
66-86%
77%
14-34%

Error(av.)
8%
9%
8%
16%
23%

contents are considered) or very weak (entire domain(s) are considered). The performance
of this kind of analysis are based on Localization and Timing (as previously deﬁned). So
the performances of any quantitative query are proportional to the performance showed
in table 1.
• Qualitative analysis. It extends the quantitative analysis with a strong and critical dependence on the content. Consequently, inferred concepts will depend on the content (as
previously deﬁned). Performance of qualitative query can be considered proportional to
the results shown in table 1.
This case study points out the importance, in terms of analysis reliability and capability, of
probabilistic semantic in a context of complex reasoning in presence of uncertainness, ambiguity
and entropy.

4

Conclusions

A probabilistic OWL extension was proposed in the paper. The individual-centric perspective
assures a double-side consistency as, from one hand, it allows eﬀectiveness in the solution
of practical problems involving uncertainness and entropy by applying semantic technologies
and, from another hand, it is fully compliant with the philosophy of the original model. This
approach was experimentally used in diﬀerent contexts. The case study described in the paper
can be considered a signiﬁcant test for the amount and, above all, the complexity of the target
information. The use of a probabilistic model enabled a more consistent and reliable analysis, at
a quantitative and a qualitative level, as well as further and improved analysis capabilities. An
individual-centric probabilistic extension for OWL oﬀers the opportunity to model and process
the uncertainties and the entropies as a part of the knowledge in a semantic context. The
practical approach, as well as the key requirement to maintain the model as simple as possible,
has aﬀected more than one theoretical issue but, in general, leads to a smart and intuitive
approach at the time to reﬂect the uncertainness on the Ontology population covering a wide
range of real use cases.

Acknowledgments
Thank the Prof. Robert Amor (University of Auckland, NZ) for making possible this research
and for his valuable help and support.
1750

Modelling the Uncertainness

Salvatore F. Pileggi

References
[1] Stefan Decker, Sergey Melnik, Frank Van Harmelen, Dieter Fensel, Michel Klein, Jeen Broekstra,
Michael Erdmann, and Ian Horrocks. The semantic web: The roles of xml and rdf. Internet
Computing, IEEE, 4(5):63–73, 2000.
[2] Tim Berners-Lee, James Hendler, Ora Lassila, et al. The semantic web. Scientiﬁc american,
284(5):28–37, 2001.
[3] Sheila A McIlraith, Tran Cao Son, and Honglei Zeng. Semantic web services. IEEE intelligent
systems, 16(2):46–53, 2001.
[4] Steve Lohr. The age of big data. New York Times, 11, 2012.
[5] Deborah L McGuinness, Frank Van Harmelen, et al. Owl web ontology language overview. W3C
recommendation, 10(10):2004, 2004.
[6] Paulo CG Costa and Kathryn B Laskey. Pr-owl: A framework for probabilistic ontologies. Frontiers
in Artiﬁcial Intelligence and Applications, 150:237, 2006.
[7] Ruofei Zhang, Zhongfei Zhang, Mingjing Li, Wei-Ying Ma, and Hong-Jiang Zhang. A probabilistic
semantic model for image annotation and multimodal image retrieval. In Computer Vision, 2005.
ICCV 2005. Tenth IEEE International Conference on, volume 1, pages 846–851. IEEE, 2005.
[8] Prasenjit Mitra, Natasha F Noy, and Anuj Rattan Jaiswal. Omen: A probabilistic ontology
mapping tool. In The Semantic Web–ISWC 2005, pages 537–547. Springer, 2005.
[9] Zhongli Ding and Yun Peng. A probabilistic extension to ontology language owl. In System
Sciences, 2004. Proceedings of the 37th Annual Hawaii international conference on, pages 10–pp.
IEEE, 2004.
[10] Octavian Udrea, VS Subrahmanian, and Zoran Majkic. Probabilistic rdf. In Information Reuse
and Integration, 2006 IEEE International Conference on, pages 172–177. IEEE, 2006.
[11] Henrik Nottelmann and Norbert Fuhr. Adding probabilities and rules to owl lite subsets based
on probabilistic datalog. International Journal of Uncertainty, Fuzziness and Knowledge-Based
Systems, 14(01):17–41, 2006.
[12] Jan Noessner and Mathias Niepert. Elog: a probabilistic reasoner for owl el. In Web Reasoning
and Rule Systems, pages 281–286. Springer, 2011.
[13] Pavel Klinov and Bijan Parsia. Pronto: Probabilistic ontological modeling in the semantic web.
In International Semantic Web Conference (Posters & Demos), 2008.
[14] Rommel N Carvalho, Kathryn B Laskey, and Paulo CG Costa. Pr-owl 2.0–bridging the gap to
owl semantics. In Uncertainty Reasoning for the Semantic Web II, pages 1–18. Springer, 2013.
[15] Salvatore F. Pileggi and Robert Amor. Modelling metropolitan activity through abductive reasoning on geographic space. In 2014 IEEE International Conference on Computer and Information
Technology (CIT). IEEE, 2014.
[16] Salvatore F. Pileggi, Carlos Fernandez-Llatas, and Vicente Traver. When the social meets the
semantic: Social semantic web or web 2.5. Future Internet, 4(3):852–864, 2012.
[17] Salvatore F. Pileggi and Robert Amor. Mansion-gs: semantics as the n-th dimension for geographic
space. In International Conference on Information Resource Management (Conf-IRM 2014), 2014.
[18] Salvatore F. Pileggi and Robert Amor. Semantic geographic space: From big data to ecosystems
of data. In Big Data in Complex Systems351-374. Springer, 2015.
[19] Salvatore F. Pileggi and Robert Amor. Addressing semantic geographic information systems.
Future Internet, 5(4):585–590, 2013.
[20] Gonzalo Navarro. Spaces, trees, and colors: The algorithmic landscape of document retrieval on
sequences. ACM Computing Surveys (CSUR), 46(4):52, 2014.

1751

