Unusual Event Recognition for Mobile Alarm System
Sooyeong Kwak, Guntae Bae, Kilcheon Kim, and Hyeran Byun
Dept. of Computer Science, Yonsei University, Seoul, Korea, 120-749
{ksy2177,gtbae,kimkch}@cs.yonsei.ac.kr, hrbyun@yonsei.ac.kr

Abstract. This paper proposes an unusual event recognition algorithm, which is
a part of a mobile alarm system. Our systems focus on unusual event. When the
system detects the unusual event, the photos of emergency situation are passed
to the user’s portable devices such as mobile phone or PDA along with event
description to help the user’s final decision. The system combines the foreground segmentation, object tracking and unusual event recognition to detect
the Drop off, Abandon and Steal bag event. The event recognition module constructs the Bayesian network of each event and uses inference algorithm to detect the unusual event. The proposed system tested in PETS2006 and CAVIAR
dataset. The proposed algorithm showed good results on the real world environment and also worked at real time speed.
Keywords: Mobile alarm system, Background subtraction, Object tracking,
Event recognition.

1 Introduction
Visual surveillance is a major research area in computer vision. The recent rapid increase in the number of surveillance cameras has led to a strong demand for automatic
methods of processing their outputs. Due to this fact, the necessity of automatic techniques which process and analyze human behaviors and activities is more evident
each day.
There have been a number of famous visual surveillances systems. The IBM Smart
Surveillance System[1] is a middleware offering for use in surveillance systems and
provides video based behavioral analysis capabilities. W4[2] system employs a combination of shape analysis and tracking, and constructs model of people’s appearances
in order to detect and track groups of people as well as monitor their behaviors even
in the presence of occlusion and in outdoor environment. The VSAM system[3] can
monitor activities over a large area using multiple cameras that are connected into a
network. It can detect and track multiple persons and vehicles within cluttered scenes
and monitor their activities over long periods of time.
This paper proposes an unusual event recognition algorithm, which is a part of a mobile alarm system. Unlike most previous surveillance system, our systems focus on
unusual event. When the system detects the unusual event, the photos of emergency
situation are passed to the user’s portable devices such as mobile phone or PDA along
with event description to help the user’s final decision. Fig. 1 shows that overall system
flow chart of the proposed event recognition for mobile alarm system.
Y. Shi et al. (Eds.): ICCS 2007, Part IV, LNCS 4490, pp. 417–424, 2007.
© Springer-Verlag Berlin Heidelberg 2007

418

S. Kwak et al.

Our proposed system can be divided into three parts: foreground segmentation
module detects the location of person and bag using background subtraction method;
object tracking module tracks the detected objects which deal with occlusion of multiple objects; and event recognition module which integrates the tracking results in
order to recognize three unusual events.
This rest of this paper is organized as follows. Video is segmented into background
and foreground regions by a background subtraction algorithm described in section 2.
Section 3 presents tracking algorithm. Section 4 describes the event recognition
method. Some experimental results are given in Section 5 and then summary and
conclusions are presented in Section 6.

Fig. 1. Overall system flow chart of the proposed event recognition for mobile alarm system

2 Foreground Segmentation
In order to extract the foreground region, we used the background subtraction method.
Background subtraction has been widely used to detect and track moving object obtained from a static camera. We proposed modified Horprasert’s algorithm which call
the sequential Horprasert’s algorithm.
Horprasert et al.[4] proposed the statistical background model that separates the
brightness from the chromaticity component in the batch mode. The batch mode requires long training time and the training images are to be stored in memory while the
sequential model does not. Furthermore, the sequential algorithm is of great advantage
in practice, especially when cameras can move around and stop to detect foreground
objects. We modify the Horprasert’s algorithm to work in the sequential mode.
In Horprasert's background model a pixel p is modeled by 4-tuple < μ p ,σ p , a p , b p >
where is μ p the expected color value, σ p is the standard deviation of RGB color
value, a p is the variation of the brightness distortion, and b p is the variation of the
chromaticity distortion of pixel p. We calculate sequences of < μ p t , σ p t , a p t , b p t >
at every tth frame when the scene is stable with the assumption that the sequence
approximates < μ p ,σ p , a p , b p > as t becomes bigger. It is shown in equation (1).

Unusual Event Recognition for Mobile Alarm System

< μ p , σ p , a p , b p > → < μ p t ,σ p t , a p t , b p t >

419

(1)

The sequences are calculated by the following. We define the expectation and
standard deviation of the color vector of pixel p as μ p t = ( μ r t ( p ), μ g t ( p ), μ b t ( p )) and
σ p t = (σ r t ( p ), σ g t ( p ), σ b t ( p )) , respectively, up to the tth frame. For sequential back-

ground training process we calculate μ p t and σ p t using equation (2) and (3).
t − 1 t −1
1
μ i ( p) + Cit ( p)
t
t
2
t − 1 t −1
1 t
t
2
σ i ( p) =
σ i ( p) + Ci ( p) − μ it ( p)
t
t

μ it ( p) =

(

(2)

)

(3)

Where i=r,g,b and C p t = (C r t ( p ), C g t ( p ), Cb t ( p )) is the observed color of pixel p at the
tth frame. The brightness and chromaticity distortions can be obtained using the temporal mean μ p t and standard deviation σ p t . The variation of the brightness distortion
a p t and the chromaticity distortion b p t are calculated by equation (4).

(a tp ) 2 =

t − 1 t −1 2 1 t
t − 1 t −1 2 1 t 2
(a p ) + (a p − 1) 2 , (b tp ) 2 =
(b p ) + (γ p )
t
t
t
t

(4)

The details of the algorithms can be found in [5]. Fig 2 shows results of proposed
background subtraction.

(a)

(b)

(c)

(d)

Fig. 2. The results of Background subtraction (a) Original image in CAVIAR dataset (b) Results of Background subtraction in CAVIAR dataset (c) Original image in PETS2006 dataset
(d) Results of Background subtraction in PETS2006 dataset

3 Object Tracking
For detected object tracking, we take the results of foreground segmentation which is
described by a bounding box (It can be know the location and size of the tracking
object). The tracking module applies Senior’s method[6] to handle the occlusion and
merge/split between multiple object using appearance model. This model is used to
localize object during partial occlusions, detect complete occlusions and resolve depth
ordering of objects during occlusions.

420

S. Kwak et al.

At first, the bounding box distance between each of the foreground region and all
the currently active tracks is computed to track correspondence. If the distance is
lower than the threshold value, we consider that each foreground region associated to
tracks. And then, it analyzes four possible results such as existing object, new object,
merge detected and split detected. Each case has the predefined rules which are well
described in Senior paper[6]. Fig. 3 shows that the processes of tracking when multiple objects are occluded in PETS2006 Dataset.

1053

1057

1061

1070

Fig. 3. The processes of tracking when multiple objects are occluded in PETS2006 Dataset,
with frame numbers

4 Event Recognition
We define three events such as Drop off, Abandon, and Steal bag. In order to recognize these events, we construct Bayesian network in each event which shown in Fig.
4. Our event structure refers to [7]. Drop off bag and Steal bag events need three
pieces of evidence and Abandon bag event needs two pieces of evidence. All evidences of each event are described in Table 1. Useful information of tracking object
such as speed, direction and the distance of between two object can computed from
the bounding box and it used as evidence. If all evidences are observed, we can know
the probability of each event using Bayesian inference[8]. Given the evidence, the
posterior probability is computed using the equation (5).
n

P ( Event |

e1k , e k2 ,..., e kn )

=

∏ P (e

| Event ) P ( Event )

i =1

n

∏ P (e
i =1

where

i
k

n

i
k

| Event ) P ( Event ) +

∏ P (e

i
k

| ¬Event ) P (¬Event )

(5)

i =1

{Dropp off, Abandon, Steal}∈ Event, {D,A,S}∈ k,

i = 1,2,...,n

To compute equation (5), several prior and conditional probabilities are needed.
The prior and the conditional probability can be determined by expert. These are
shown in equation (6). When the posterior probability of each event is over the 0.5,
the alarm is triggered.
P( Event ) = 0.5, P (¬Event ) = 0.5 , P(eki | Event ) = 0.9, P(eki | ¬Event ) = 0.5

(6)

Unusual Event Recognition for Mobile Alarm System

(a)

(b)

421

(c)

Fig. 4. The structure of Bayesian networks (a) Bayesian model of Drop-off bag event (b)
Bayesian model of Abandon bag event (c) Bayesian model of Steal bag event
Table 1. The definitions of evidence

Event

Drop off bag

Abandon bag

Steal bag

Evidence

e1D
eD2
eD3

: The bag did not appear 0.1 second ago.
: The bag shows up now.
: The distance between the person and the bag now is

less than 30 pixels
e1A : The distance between the person and the bag 0.1 second ago is less than 40 pixels
e 2A : The distance now is larger than 60 pixels
e1S : The person approaches the bag and the distance between the person and the bag is less than 60 pixels.
eS2 : The person stops near the bag.
eS3 : The person takes away the bag

5 Experimental Results
The proposed system was implemented in C/C++ and was run on Pentium IV-3.0
GHz PC with 1G RAM. We used the PETS2006 Dataset S1 (Take 1-C)[9] and CAVIAR Test Dataset (Person leaving bag by wall)[10] in order to detect the three unusual
event, such as the Drop off, Abandon, and Steal bag, in a real world environment. The
description of test videos shows below. The average running time of the proposed
system was about 13fps.
PETS2006 Dataset S1 (Take 1-C): The scenarios are filmed from multiple cameras
and involve multiple actors. The scenario contains a person with a rucksack who
loiters before leaving the item of bag unattended. The dataset consists of 1479 frames
recording activities in Victoria metro station. The video sequence includes a total of
24 moving objects, people and bag appearing at close, medium and fat distances from
the camera. The image size is 360x288 pixels.

422

S. Kwak et al.

CAVIAR Test Dataset (Person leaving bag by wall): These include a person leaving a package in a public place. The dataset consists of 837 frames recording activities
in the entrance lobby of the INRIA Labs at Grenoble, France. The resolution is halfresolution PAL standard(384x288 pixels, 25 frames per second).
5.1 Performance of Object Tracking

Performance of the object tracking was evaluated with respect to the ground truth in
each frame of test sequences. The evaluation method refers to [11]. Each frame tested
to see if the number of objects as well as their sizes and locations match the corresponding ground truth data for that particular frame. To evaluate the tracking algorithm, we compute True Negative(TN), True Positive(TP), False Negative(FN), and
False Positive(FP) for every frame in the sequence. These definitions describe below.
Also, we compute the Accuracy using the equation (7).
- TN: Number of frames where both ground truth and system results agree on the
absence of any object.
- TP: Number of frames where both ground truth and system results agree on the
presence of one or more objects, and the bounding box of at least one or more objects
coincides among ground truth and tracker results.
- FN: Number of frames where ground truth contains at least one object, while system
either does not contain any object or none of the system’s objects fall within the
bounding box of any ground truth object.
- FP: Number of frames where system results contain at least one object, while ground
truth either does not contain any object or none of the ground truth either does not
contain any object or none of the ground truth’s objects fall within the bounding box
of any system object.
Table 2 shows the performance of the tracking algorithm. Because of the foreground segmentation part does not detect the small moving objects in PETS2006
dataset, the Accuracy is not good.
Accuracy =

TP + TN
TF

(7)

where TF is total number of frames.
Table 2. Performance of the tracking algorithms

TP
TN
FP
FN
Accuracy

PETS2006 Dataset S1
(Take 1-C)
1258
46
45
130
0.881

CAVIAR Test Dataset
(Person leaving bag by wall)
599
195
15
31
0.945

Unusual Event Recognition for Mobile Alarm System

423

5.2 Performance of Event Recognition

In order to evaluate the event recognition, we compare the frame number when an
even is triggered with our results and the ground truth. The performance of the event
recognition shows in Table 3. The key frame of the three events such as Drop off,
Abandon and Steal bag shown in Fig. 5. We can see in Table 3, the alarm event is
detected within an error of 25 frames except the abandon bag event in PETS2006
Dataset S1. In that sequence, the merge and split algorithm in tracking module has not
separate quickly the human and the bag.
Table 3. Performance of event recognition

PETS2006 Dataset S1
(Take 1-C)
Ground truth
Our result

CAVIAR Test Dataset
(Person leaving bag by wall)
Ground truth
Our result

Drop off bag

1922

1935

949

974

Abandon bag
Steal bag

2086
none

2100
none

988
1348

1003
1359

Fig. 5. It shows the key frame of the three events. First row shows the Drop off bag event in
PETS2006 test dataset and the second row shows the Drop off bag and Abandon bag event in
CAVIAR test dataset. The Steal bag event in CAVIAR test dataset is shown at last row.

6 Summary and Conclusions
This paper proposed unusual event recognition for mobile alarm system. Our system has
three main modules such as moving object detection, tracking and event recognition. In

424

S. Kwak et al.

order to detect the foreground object, we used the background subtraction method. After
candidate foreground region are detected, appearance model is used for moving object
tracking. In this paper, we also used Bayesian inference algorithm in order to recognize
the unusual event. The proposed algorithm showed good results on the real world environment and also worked at real time speed. The proposed framework can be easily
employed or integrated into a variety of vision surveillance systems.
Acknowledgments. This research was supported by the Ministry of Information and
Communication, Korea under the Information Technology Research Center support
program supervised by the Institute of Information Technology Assessment, IITA2005-(C1090-0501-0019).

References
1. Chiao-Fe Shu, Hampapur A., Lu M., Brown L., Connell J., Senior A., and Yingli Tian :
IBM smart surveillance system (S3): a open and extensible framework for event based surveillance, IEEE Conference on Advanced Video and Signal Based Surveillance (2005)
318-323
2. I. Haritaoglu, D. Harwood, and L. S. Davis : W : Real-time surveillance of people and their
activities, IEEE Transaction on Pattern Analysis and Machine Intelligence, (2000) Vol. 22,
809–830
3. R. T. Collins, A. J. Lipton, T. Kanade, H. Fujiyoshi, D. Duggins, Y. Tsin, D. Tolliver,
N. Enomoto, O. Hasegawa, P. Burt, and L.Wixson : A system for video surveillance and
monitoring, Carnegie Mellon University, Pittsburgh, PA, Technical Report, CMU-RI-TR00-12, (2000)
4. T. Horprasert, D. Harwood, L.S. Davis: A statistical approach for real-time robust background subtraction and shadow detection. Proceeding of IEEE Frame RateWorkshop
(1999) 1-19.
5. Jung-Ho Ahn and Hyeran Byun: Human silhouette extraction method using region based
background subtraction, International Conference on Mirage 2007 (To be appear)
6. Andrew Senior: Tracking people with probabilistic appearance models, Proceedings 5th
IEEE International Workshop on PETS, (2002)
7. Fengjun Lv, Xuefeng Song, Bo Wu, Vivek Kumar Singh, and Ramakant Nevatia.: LeftLuggage Detection using Bayesian Inference, Proceedings 9th IEEE International Workshop on PETS, (2006) 83-90
8. D'Ambrosio : Inference in Bayesian networks, AI Magazine (1999) 21-35
9. http://homepages.inf.ed.ac.uk/rbf/CAVIARDATA1/
10. http://www.cvg.rdg.ac.uk/PETS2006/index.html
11. Faisal Bashir and Fatih Porikli : Performance Evaluation of Object Detection and Tracking
Systems, Proceedings 9th IEEE International Workshop on PETS, (2006) 7-13

