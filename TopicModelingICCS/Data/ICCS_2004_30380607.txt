Behavior Based Detection of Unfavorable
Resources
Krzysztof Cetnarowicz1 and Gabriel Rojek2

2

1
Institute of Computer Science,
AGH University of Science and Technology,
Al. Mickiewicza, 30 30-059 Krak´
ow, Poland
cetnar@agh.edu.pl
Department of Computer Science in Industry,
AGH University of Science and Technology,
Al. Mickiewicza 30, 30-059 Krak´
ow, Poland
rojek@agh.edu.pl

Abstract. This article considers a problem of security in a computer
systems in context of some mechanisms which act in societies and which
mechanisms are seen as being useful to assure security in a computer
system. A new approach to the security problem is discussed, which
refers to identifying computer resource (e.g. agent, program) on the basis of this resource behaviour (actions which this resource undertakes).
Mechanisms are presented, which may enable to recognize and dismiss
resources undesirable or harmful in the computer system on the basis
of behavior observation. Proposed mechanisms were tested in simulations of the computer system under DoS attack. The results of tests are
presented and discussed.

1

Introduction

Present computer systems have become more and more complex and related to
each other. Large nets, as well as almost all devices connected to the Internet, are
open systems. It is impossible to have full knowledge about topology or current
state of such systems even for the owner, or an administrator of a computer
system [5]. Agent technology makes possible the full ﬂow of resources among open
computer systems. Autonomic agents can yet freely migrate in the net without
the knowledge of the owner or an administrator. Agents can also execute their
tasks without anybody’s knowledge. These tasks could be useful or destructive
for the system on which an agent operate. An agent which migrate in an open
system could be equally desirable or undesirable in a computer system. This
ambiguousness causes problems with the use of artiﬁcial immune systems which
makes possible to distinguish two classes of resources: self or nonself [3,4,6,7,
8]. In the case of migrating agent in open system, the origin of supply (self /
nonself) does not play an essential part in the process of assuring security [2].
M. Bubak et al. (Eds.): ICCS 2004, LNCS 3038, pp. 607–614, 2004.
c Springer-Verlag Berlin Heidelberg 2004

608

2

K. Cetnarowicz and G. Rojek

Social Approach to Security Problem

In the introduction mentioned facts and trends lead to the formulation of a new
foundation of solution of computer’s security problem. Attacks have generally
decentralized nature and harmful resources (which cause these attacks) should be
seen as in some way autonomous agents. In order to oppose against such threats,
the computer system should be seen as a multiagent system, which consists of:
– ”good” (desirable) resources (e.g. agents, programs),
– ”bad” (undesirable) resources,
as it was proposed in [2].
Taking into consideration the above–mentioned problems with security systems, which are centralized and operate on fragments of code (look for known
signatures), a way is proposed, how the division ”good” / ”bad” should be made.
It could be stated, that:
– decentralized security system could be realized in the form of functions,
mechanisms or algorithms that are build in as big amount of components of
protected system as possible;
– division ”good” / ”bad” could be based on observation and evaluation of
behavior of components of the protected system.
Authors formulating above mentioned paradigms where inspired by some of
ethically–social mechanisms that act in human societies. That mechanisms prevent from misuses in societies in the way to enable secure functioning of an
individual in the environment of others individuals. Some of individuals in society could be prejudicial for others individuals, what is main analogy to the
society of agents, programs and other resources in a computer system.
An individual in a society seems trustworthy if behavior of this individual
could be observed by other individuals in a society and this behavior is evaluated
by majority as good and secure [9,10]. The decision about trustworthy of an
individual are in society made in the decentralized way — all individuals in a
society make own decisions which form one decision of the society. The decisions
undertaken by individuals in the societies are made on the ground of observation
and evaluation of behavior (and / or intentions) of the individual which decisions
concerns.
2.1

Decentralization of Security System

Decentralization paradigm could be realized in multiagent algorithms by means
of equipping all agents (agents that exist in the protected system) with some
additional goals, tasks and mechanisms. Those goals, tasks and mechanism are
named a division proﬁle and should be designed to assure security for agents and
the computer system, which those agents assemble. So the agents will execute
tasks they have been created for and simultaneously will execute tasks connected
with security of all agents in the system and / or the computer system. The
name ”division proﬁle” is inspired by M–agent architecture which could be used
to describe an agent (M–agent architecture was introduced among others in [1]).

Behavior Based Detection of Unfavorable Resources

2.2

609

Observation and Evaluation of Behavior

Undertaking actions by an acting agent should be seen as objects. Those objects
create a sequence which could be registered by agents observing that acting
agent. Registered objects–actions could be processed in order to qualify whether
it is a ”good” or ”bad” acting agent. It should be mentioned, that the quoted
notions of ”good” and ”bad” do not have absolute meaning. ”Good” resource
is a desirable resource for a deﬁnite computer system in which evaluation takes
place. ”Bad” resource is an undesirable resource for a given system, although it
can happen that it would be greatly desirable in a diﬀerent computer system.

3

Division Proﬁle

Division proﬁle is a class of agent activity whose goal is to observe others agents
in society and possible other elements of the environment. Those observations
should be made in order to distinguish individuals whose behavior is unfavorable or incorrect (”bad”) for the observer. Such distinguished ”bad” individuals
should be adequately treated (e.g. convicted, avoided, liquidated) which should
also be formed by a division proﬁle. In the case of a multiagent system, it is
possible to equip every agent in the system with division proﬁle mechanisms,
so the security is assured by all agents existed in the system. Division proﬁle is
deﬁned as:
ad = (Md , Qd , Sd )
where Md is a set of division states md of agent a, Qd is a conﬁguration of goals
qd of agent’s a division proﬁle, Sd is a conﬁguration of strategies sd of agent’s a
division proﬁle.
3.1

Division State

Division state md of agent a is represented as a vector:
j
md = (m1d , m2d , ..., mj−1
d , md )

where j is the number of neighboring agents; neighboring agents are agents which
are visible for agent a (including itself, if all agents in the system are visible for
agent a, j is equal to the number of all existing agents in the system); mkd is the
factor subordinated to neighboring agent number k; this factor can be a number
from any range and it indicates whether the agent number k is ”good” or ”bad”
(low number indicates ”good”, high number indicates ”bad”).
3.2

How to Fix the Division State

To ﬁx division state (or to distinguish between ”bad” and ”good” individuals)
some mechanisms of the immune system can be used. Fixing of division state is

610

K. Cetnarowicz and G. Rojek

inspired by immunological mechanisms — generation of T cells in the immune
system. This article does not present an artiﬁcial immune system, but some
immunological mechanisms are used in a part of presented solutions. Immunological mechanisms should operate on actions made by observed agents. This
approach is opposite to the one proposed in e.g. [3,12] in which immunological
mechanisms operate on the resource’s structure.
Immunological intruders detection in the computer environment has to be
done on the basis of certain characteristic structures. These structures in the
case of behavior observation can be chains of actions performed by an observed
agent. These chains are of the settled length l, so one chain contains l objects,
which present undertaken actions by observed agent (one object represents one
action). We should deﬁne the way how agent a will recognize (notice, but not
estimate) actions undertaken by neighbors. It is possible to store all actions
undertaken by agents in the environment of agent computer system. The action
stored should be accompanied by the notion by whom a particular action has
been undertaken. This method presumes the mediation of the environment and /
or resources of the environment in the process of recognizing undertaken actions.
Detectors. The method of ﬁxing the division state refers to the mechanism of
immune system. Once detector’s set is generated, this detector’s set is used to
ﬁnd ”bad” among presented sequences of action–objects.
In order to generate a set of detectors R, own collection W should be speciﬁed. This collection includes correct, ”good” sequences of action–objects. This
collection W should consist of action–object sequences of length l, which is undertaken by the agent–observer. This is correct, because of the assumption that
actions which the agent undertakes are evaluated as ”good” by him. Presuming
there are stored h last actions undertaken by every agent, own collection W will
contain h − l + 1 elements.
Algorithm of Detectors Generation. The algorithm of detectors generation
refers to the negative selection — the method in which T–lymphocytes are generated. From set R0 of generated sequences of length l those reacting with any
sequence from collection W are rejected. Set R0 contains every possible sequence
(but it is also possible to use a set of random generated sequences). Sequence
reaction means that elements of those sequences are the same. Sequences from
set R0 which will pass such a negative selection create a set of detectors R.
Behavior Estimation of Neighboring Agents. First stage is a neighbor observation during which actions (and order of those actions) executed by neighboring agents are remembered. Those remembered actions create sequence N of presumed length h. After the next stage of detectors generation, generated detectors
are used to ﬁnd ”bad”, unfavorable agents. Every subsequence n of length l of
sequence N is compared with every detector r from set R, as it is shown in Fig. 1.
If sequence n and r match, it means ﬁnding ”bad”, unfavorable actions. Sequence

Behavior Based Detection of Unfavorable Resources

611

Neigbour 1
(N1 )

✁❆
Detector
Set
(R)

Neigbour 2
(N2 )

r
r
r
Neigbour k
(Nk )

r
r
r
Neigbour j
(Nj )

✗
✲

❄❄
Match

no

✔
✕

yes

❄
Qualiﬁcation of
Neighbour k as
”bad” or ”worse”

Fig. 1. Process of behavior estimation of neighboring agents, process presented for
agent number k

matching means that the elements of the sequences compared are the same. The
number of matches for every observed agent should be counted. On this basis
j
behavior estimation is made — division state md = (m1d , m2d , ..., mj−1
d , md ) of
j−1
j
agent-observer is modiﬁed to the md = (m1d , m2d , ..., md , md ), where j is the
number of agents in the environment, mkd is assigned to the number of counted
matches for agent number k.
3.3

Conﬁguration of Goals of Agent’s Division Proﬁle

The way neighboring agents are treated is described by Qd — conﬁguration of
goals qd of agent’s division proﬁle. Conﬁguration of goals of an agent is constant
(however it is possible to design such a system, which in is possible the goal’s
adaptation). In the system described the conﬁguration of goals consists only
from one goal — liquidation neighboring agent (or agents) number k, if mkd =
j
max(m1d , m2d , ..., mj−1
d , md ).
3.4

Conﬁguration of Strategies of Agent’s Division Proﬁle

Actions, which should be undertaken by agent a in order to treat agent number
k in the way described by the conﬁguration of goal, are speciﬁed by Sd — the
conﬁguration of strategies sd of agent’s division proﬁle. The conﬁguration of

612

K. Cetnarowicz and G. Rojek

strategies of the agent is constant and in the system described the conﬁguration
of strategies consists only of one goal: if the goal is to liquidate agent number k,
a demand of deleting agent number k is send to the environment (coeﬃcient od
equal to the mkd is attributed to this demand).
This conﬁguration of strategies presumes an intervention of system’s environment in the liquidation of the agent. In the systems described the environment
calculates the sum of coeﬃcients for every agent separately attributed to demands and liquidates all agents which have the maximum sum of coeﬃcients
and this sum is larger than constant OU . Periodically, after a constant time
period, the calculated sums of coeﬃcients are set to 0. Constant coeﬃcient OU
is introduced in order to get tolerance for behavior that is evaluated as ”bad”
in a short time, or is evaluated as ”bad” by a small amount of agents.

4

Experiment

In the computer system there are some operation which must be executed in
couples, for example: open and close a ﬁle, connection request and disconnection
request. There are a lot of attack techniques which consist in doing only one
from a couples (or trios...) of obligatory operations (for example so–called SYN
ﬂood attack [11]). There is simulated a system with two types of agents:
– good agents which perform some operations in couples (e.g. open, open,
open, close, open, close, close, close);
– bad agents which perform only one from a couples of some operations (e.g.
open, open, open, open, open, open, open, open).
In the simulation there is no possibility of distinguishing the type of an agent
on the basis of the agent’s structure. So the only possibility is to observe the
agent’s behavior and process the actions observed (actions–objects) to distinguish whether the agent is good or bad.
4.1

Simulation — Unsecured Computer System

First a case was simulated, in which only good agents exist in the computer
system — initially there are 80 good agents in the system, which do not have
any security mechanisms. Next a case was simulated, in which good and bad
unsecured agents exist in the computer system — initially there are 64 good
agents and 16 bad agents, all agents do not have any security mechanisms. The
system in those two cases was simulated to 300 time periods and there were 10
simulations performed. Diagram in Fig. 2 shows the average numbers of agents
in separate time periods.
If there are not any bad agents in the simulated system, all good agents
can exist without any disturbance. The existence of bad agents in the system
causes problems with executing tasks by good agents, which die after some time
periods. Bad agents still remain in the system, which is blocked by those bad
agents.

Behavior Based Detection of Unfavorable Resources

613

Fig. 2. Number of agents in separate time periods

4.2

Simulation — Secured Computer System

A case was simulated, in which good and bad secured agents exist in the computer system — initially there are 64 good agents and 16 bad agents. All agents
in the system were equipped with the division proﬁle security mechanisms. The
system was simulated to 300 time periods and there were 10 simulations performed. Diagram in Fig. 2 shows the average numbers of agents in separate time
periods.
In the environment there are last 18 actions stored undertaken by every agent.
After 18 actions have been undertaken by every agent, detectors are constructed
of length l = 5. Agents use their division proﬁle mechanisms to calculate which
neighboring agent they want to eliminate. Agent demand to eliminate these
neighbors which have the maximum of detector’s matchings. Agents present their
demands to the environment with the number of matchings. The environment
counts matchings in the demands presented and eliminates agents as it was
proposed in the description of division proﬁle mechanisms. The constant OU is
set up to 480.
After detectors were constructed, bad agents were distinguished due to the
division proﬁle mechanisms. In the same time period, recognized agents were
deleted, what makes it possible for agents belonging to good type to function
freely.

5

Conclusion

This paper presents a discussion of security problem in the computer system.
Security solutions which create new security paradigms were proposed:

614

K. Cetnarowicz and G. Rojek

– equip all system resources (e. g. agents, programs) with security mechanisms,
– security mechanisms should be based on activity observation rather than
looking for some fragments of code (signatures),
– design the environment of computer system in such a way so as to support
security mechanisms with which system’s resources are equipped.
In this paper security mechanisms with immunological approach were presented
which fulﬁll the said security paradigms. All these security mechanisms were
called a division proﬁle. The conception presented was simulated and the obtained results conﬁrm the eﬀectiveness of this solution. The simulation enables
to anticipate how the described mechanisms will function in the real world of
computer systems.
Security mechanisms designed on the grounds of the conception presented
have such advantages as detection of previously unseen danger activities, detection based on activity observation and decentralized detection.

References
˙
1. Cetnarowicz K., Nawarecki E., Zabi´
nska M.: M–agent Architecture and its Application to the Agent Oriented Technology. Proc. of the DAIMAS’97, St. Petersburg
(1997)
2. Cetnarowicz K., Rojek G., Werszowiec-Plazowski J., Suwara M.: Utilization of
Ethical and Social Mechanisms in Maintenance of Computer Resources’ Security.
Proc. of the Agent Day 2002, Belfort (2002)
3. Forrest S., Perelson A. S., Allen L., Cherukuri R.: Self-nonself Discrimination in
a Computer. In Proc. of the 1994 IEEE Symposium on Research in Security and
Privacy, IEEE Computer Society Press, Los Alamitos (1994) 202–212
4. Forrest S., Perelson A. S., Allen L., Cherukuri R.: A Change–detection Algoritm
Inspired by the Immune System. IEEE Transactions on software Engineering, IEEE
Computer Society Press, Los Alamitos (1995)
´
5. Gibbs W. W.: Jak przetrwa´c w niebezpiecznym ´swiecie? Swiat
nauki, Wydawnictwo Pr´
oszy´
nska i s-ka (2002)
6. Hofmeyr S. A., Forrest S.: Architecture for an Artiﬁcial Immune System. Evolutionary Computation, vol. 7, No. 1 (2002) 45–68
7. Kim J., Bentley P.: The Human Immune System and Network Intrusion Detection.
7th European Congress on Intelligent Techniques and Soft Computing (EUFIT ’99)
Aachen September 13-19 (1999)
8. Kim J., Bentley P.: Negative Selection within an Artiﬁcial Immune System for
Network Intrusion Detection. The 14th Annial Fall symposium of the Korean Information Processing Society, Seoul October 13-14 (2000)
9. Ossowska M.: Normy moralne. Wydawnictwo Naukowe PWN, Warszawa (2000)
10. Ricken F.: Etyka Og´
olna. Wydawnictwo ANTYK — Marek Derewiecki, Kety
(2001)
11. Schetina E., Green K., Carlson J.: Bezpiecze´
nstwo w sieci. Wydawnictwo HELION,
Gliwice (2002)
12. Wierzcho´
n S. T.: Sztuczne systemy immunologiczne: teoria i zastosowania. Akademicka Oﬁcyna Wydawnicza Exit, Warszawa (2001)

