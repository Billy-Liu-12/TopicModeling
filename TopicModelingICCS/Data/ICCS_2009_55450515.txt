Lag-Dependent Regularization for MLPs Applied
to Financial Time Series Forecasting Tasks
Andrew Skabar
Department of Computer Science and Computer Engineering
La Trobe University, Victoria 3086 Australia
a.skabar@latrobe.edu.au

Abstract. The application of multilayer perceptrons to forecasting the future
value of some time series based on past (or lagged) values of the time series usually requires very careful selection of the number of lags to be used as inputs, and
this must usually be determined empirically. This paper proposes a regularization
technique by which the influence that a lag has in determining the forecast value
decreases exponentially with the lag, and is consistent with the intuitive notion
that recent values should have more influence than less recent values in predicting
future values. This means that in principle an infinite number of dimensions could
be used. Empirical results show that the regularization technique yields superior
performance on out-of-sample data compared with approaches that use a fixed
number of inputs without lag-dependent regularization.
Keywords: Multilayer perceptrons, financial time series forecasting, weight
regularization.

1 Introduction
Over the last two or so decades there has been much interest in applying multilayer
perceptron (MLP) models to financial time series forecasting tasks. The appeal of MLPs
is their property of being universal function approximators; that is, they are able to approximate any target function to arbitrary degree of accuracy [1]. However, the ability
of a model to achieve good performance on in-sample data does not guarantee that it
will perform well in forecasting out-of-sample data, and this is particularly a concern in
the case of models such as MLPs, which, due to their complexity, together with the high
level of noise present in financial time series, can very easily overfit the training data. It
is not surprising, then, that there has been considerable debate about whether non-linear
models such as MLPs are able to provide any better performance financial time series
forecasting than linear or random walk models [2–4].
The main factor determining the complexity of an MLP model (and hence its propensity to overfit training data) is the number of weights, and this depends on two
factors: the number of inputs to the model, and the number of hidden layer units.
Since each input fans out to each hidden unit, the total number of weights will be a
function of the product D·h, where D is the input dimensionality and h is the number
of hidden units; hence, choosing an appropriate value for these two parameters plays
G. Allen et al. (Eds.): ICCS 2009, Part II, LNCS 5545, pp. 515–523, 2009.
© Springer-Verlag Berlin Heidelberg 2009

516

A. Skabar

an important role in determining the overall complexity of the model. Fortunately,
other means also exist for mitigating against the effect of model complexity. For
example, it is common to include a regularization term into the error function, the
purpose of which is to impose a penalty against large magnitude weights, thereby
controlling the effective complexity of the model [5].
The main problem with which we are concerned in this paper is how to select an
appropriate number of inputs. As is common practice on financial time series forecasting tasks, we assume that the input to the MLP is a vector of delayed returns (rt-1,
rt-2, …, rt-D)T, where rt-n is the return n days prior to day t (and which we will refer to
as the nth 'lag'). The problem, then, is how to select an appropriate value for D. If D is
too small, then we may miss out on detecting important patterns in the time series; if
D is too large then we run the risk of modeling noise in the training data, thereby
leading to overfitting. A common practice in determining the optimum input dimensionality is to experiment with a range of values, and to then use some model
selection criteria such as the Akaike Information Criterion (AIC) [6] or the Schwarz
criterion (BIC) [7] to make the final selection [8]. Typically, the number of lags selected lies in the range two to five; however, there can be dramatic differences in
performance observed by increasing the dimensionality by just one.
Motivated by the intuitive notion that recent values should have more influence
than less recent values in predicting future values, in this paper we propose a weight
regularization technique by which the influence that a lag has in determining the forecast value decreases exponentially with the lag. This means that in principle an infinite number of dimensions can be used. Empirical results show that the regularization
technique yields superior performance on out-of-sample data compared with approaches that use a fixed number of inputs without lag-dependent regularization.
The remainder of this paper is structured as follows. In Section 2, we describe previous work, in which we used a generative model to predict the probability of an
upward/downward direction of change in the value of a return series. Specifically, we
show how the covariance matrix used to estimate densities can be parameterized in
such a way that the influence of past returns decreases exponentially with time, thus
allowing an effectively infinite input dimensionality. In Section 3 we show how this
idea of exponentially decreasing influence of past returns can be implemented in
discriminative models such as MLPs. Section 4 provides empirical results of applying
the technique to several datasets, and Section 5 concludes the paper.

2 Related Work
In [9] we showed how the intuitive notion that recent values of the time series should
to be more influential than less recent values could be implemented within a generative model. Assuming a set of examples X = {x1, x2, …, xN} where xn = (rn-1, rn-2, … ,
rn-D)T is a vector of delayed returns on day n, the objective was to predict the posterior
probability of a positive return on day n. Since each x belongs to one of two classes,
which we will denote as C+ for positive returns and C– for negative returns, the problem is thus a binary classification problem. Using a Parzen density estimation [10,11],
the probability density function for examples belonging to C+ can be estimated as

Lag-Dependent Regularization for MLPs Applied to Financial Time Series

517

X

p(x | C+ , Σ) =

+
1
1
1
exp(− (x − x+n )T ∑−1(x − x+n )) ,
2
X+ ( 2π ) D/2 ∑1/2 n=1

∑

(1)

where ∑ is the covariance matrix and X+ is the set of examples belonging to C+. The density p(x | C− , Σ) can be calculated similarly. These densities can then be combined using

Bayes’ Theorem to estimate the posterior probabilities P(C+ |x, Σ) and P(C− |x, Σ) ,
which represent the probability respectively of a positive and negative return.
Since we expect that the importance of lags to decrease with the length of the lag,
this suggests that the kernel used to estimate a density should not be symmetrical (i.e.,
spherical), but of a form such that its width along dimensions corresponding to recent
returns is smaller than its width along dimensions corresponding to less recent returns.
Assuming that significance decreases exponentially with the length of the lag, we
proposed the following expression for the variance in the direction corresponding to
the nth lag:

vt −n = a ek ( n −1)

(2)

where vt − n ( n ∈{1, 2, ..., D} ) is the variance of the kernel in the direction parallel to the
axis corresponding to lag rt − n , k is an exponential scaling factor, and a is the variance
parallel to the first lag. These values are thus the diagonal components of the covariance matrix, with all off-diagonal components being equal to zero.
In general, the higher the variance of the time series, the larger will be the value of
a. The value of k will depend on how rapidly the influence of lags in predicting future
values diminishes with the length of the lag. For example, a series in which only the
most recent value is significant in predicting future values will have a large k value
(i.e., rapid decay in significance, or, equivalently, rapid increase of variance in dimension parallel to that lag). Conversely, time series in which previous lags are significant
in predicting future values will have a smaller k value.

3 MLPs for Time Series Forecasting
An MLP is a function of the following form:

⎛ P
⎞
wkj g ⎜
w ji xin ⎟
⎜
⎟
j =0
⎝ i =0
⎠
Q

f (x n ) = h(u ) where u =

∑

∑

(3)

where P is the number of inputs, Q is the number of units in a hidden layer, xin is the
input at unit i from example n, wji is a numerical weight connecting input unit i with
hidden unit j, and wkj is the weight connecting hidden unit j with output unit k. The
function g(x) is either a sigmoid (i.e., g(x) = (1 + exp(−x))−1 ) or some other continuous, differentiable, nonlinear function. For regression problems h(u) is the identity
function (i.e., h(u) = u), and for classification problems h(u) is a sigmoid.

518

A. Skabar

Thus, an MLP with some given architecture and weight vector w, provides a mapping from an input vector x to a predicted output y given by y = f (x, w). For time
series forecasting tasks performed in this paper, the vector x = (x1, x2, …, xD) is assumed to be a vector of lagged returns in which x1 = rt-1, x2 = rt-2, etc. Given some
data, D, consisting of n independent items (x1, y1), …, (xN, yN), the objective is to find
a suitable w.
3.1 Maximum Likelihood Training
The conventional approach to finding the weight vector w is to use a gradient descent
method to find a weight vector that minimizes the error between the network output
value, f(x, w), and the target value, y. In the following we assume that the objective is
to predict the direction of change in the next value of the return series; more specifically, the probability that the direction of change is positive. Therefore, under the
assumption that the target values are binary (y = 1 for a positive change; y = 0 for a
negative change), then the likelihood of observing the training data D, given some
weight vector w is given by

(

p ( D | w ) = ∏ f ( x n , w ) y + (1 − f ( x n , w )) (1− y
n

n

n

)

)

(

= exp ∑ y n ln f ( x n , w ) + (1 − y n ) ln(1 − f ( x n , w ))
n

)

(4)

We wish also to specify a prior distribution for the weights, and we assume that this
distribution is Gaussian with mean 0 and inverse variance α. Thus,
1/2

⎛α ⎞
p(w ) = ⎜
⎟
⎝ 2π ⎠

⎛ α m
⎞
exp ⎜ − ∑ wi 2 ⎟
⎜ 2
⎟
i=1
⎝
⎠

(5)

We wish to find the weight vector which maximises the product of the likelihood and
the prior. This can be shown to be equivalent to minimizing the following error term:
N

E = −∑ y n ln f (x n , w ) + (1 − y n ) ln(1 − f (x n , w )) +
n =1

α

m

∑ wi 2
2

(6)

i=1

The first term in this expression is commonly referred to as the ‘cross-entropy error’;
the second term is a ‘regularization’ term. Effectively, the regularization term punishes weight vectors in which the magnitude of weights is large, thereby helping reduce the possibility of overfitting to the training data. Note that aside from the
weights, there is only one free parameter in the above equation: α, which specifies the
inverse variance for the weights prior.
3.2 Lag-Dependent Regularization
The regularization term in Equation 6 treats all weight equally. However, by separating
weights into groupings, it is possible to implement a lag-dependent regularization,
which achieves a similar result to the variance parameterization described in the case of
generative models. Consider the use of non-spherical kernels of the form described in

Lag-Dependent Regularization for MLPs Applied to Financial Time Series

519

Section 2 above. From a discriminative classifier perspective, this is effectively making
the assumption that noise in the training examples varies in such a way that input dimensions corresponding to recent returns are less noisy than inputs corresponding to
less recent returns. We can build this assumption into an MLP by using separate regularization coefficients for different families of input-to-output layer weights. Specifically, weights fanning our from inputs corresponding to recent returns should have
smaller weight-regularization coefficients than weights fanning out from inputs corresponding to less recent returns.
We first consider weights in the input-to-hidden layer weights. We use the
notation wi p hq to represent the weight connecting input unit ip with hidden unit hq (i0
represents the bias). Separating these weights into D+1 groups, where D is the input
dimensionality and Q is the number of units in the hidden layer, we have

α⎛

Q

∑

⎜ wi h 2 +
2 ⎜ j=1 0 j
⎝

Q

∑

wi1h j 2 +

Q

∑

j=1

wi2 h j 2 + ... +

j=1

⎞

Q

∑ wi h 2 ⎟⎟
D j

j=1

(7)

⎠

We now apply exponential scaling factor of e k ( n −1) to weights fanning from the input
nodes 1 to D giving

α⎛

Q

∑

⎜ wi h 2 + e0 k
2 ⎜ j=1 0 j
⎝

Q

∑

wi1h j 2 + e k

j=1

Q

∑

wi2 h j 2 + ... + e( D −1) k

j=1

⎞

Q

∑ wi h 2 ⎟⎟
D j

j=1

(8)

⎠

which can be more succinctly expressed as

α ⎛⎜

Q

D

⎛

⎞⎞

Q

∑ wi h 2 + ∑ ⎜⎜ ek (n−1) ∑ wi h 2 ⎟⎟ ⎟⎟
2⎜
⎝

0 j

j=1

n =1 ⎝

n j

j=1

(9)

⎠⎠

The hidden-to-output layer weights can be treated as a single group. Incorporating
these, the full regularization expression becomes

α ⎛⎜

Q

∑

2 ⎜ j=1
⎝

wi0 h j 2 +

⎞
⎛ k ( n −1) Q
⎞ Q
2
2⎟
⎜
⎟
e
w
+
w
∑⎜
∑ in hj ⎟ ∑ h j o ⎟
n =1 ⎝
j=1
⎠ j=0
⎠
D

(10)

Note that as with the generative approach described in Section 2, this modification
still introduces two additional parameters: a parameter controlling the rate at which
the value of the regularization coefficient varies with input lag k, and a parameter
controlling the overall level of regularization, α.

4 Experiments
We have applied the approach we have described to the daily close price of three
financial time series: the Australian All Ordinaries (AORD) index, the Dow Jones
Industrial Average (DJIA) index, and the Australian–U.S. Foreign Exchange (AUSE)
rate. In each case we used a 20-year out-of-sample forecast period from 1 January

520

A. Skabar

1987 to 31 December 2006. Forecasts were performed in 25-day forecast windows, in
which the model was constructed using the data points immediately preceding the
forecast period. The number of data points used for model construction was 500
(approx. 2 years). This model was then used to predict the directional change probability for each data point in the forecast window. We note that the 25-day forecasting
window period was chosen entirely for computational reasons, and there is no reason
why a separate model cannot be constructed for a single prediction. The number of
lags used in the input vector (i.e., the input dimensionality) was five. The number of
hidden layer units was 50. As with the work described in [9], here, too, we are concerned with predicting the direction of change. That is, rather than forecasting the
values of the return, we forecast the (probability of) the sign of the return.

4.1 Testing Directional Forecast Accuracy
An obvious measure of direction-of-change forecast accuracy is the fraction of days
in some test period for which the sign is predicted correctly, and we refer to this as the
sign ratio (SR). We would like to know whether the value of SR differs significantly
from what we would expect if the signs of the actual and forecast predictions were
independent. If P is the fraction of days in the out-of-sample test period for which the
actual movement is up, and l Pˆ is the fraction of days for which the predicted movement is up, the expected fraction of days corresponding to a correct upward prediction
is P × Pˆ , and the expected fraction of days corresponding to a correct downward
prediction is (1− P ) × (1− Pˆ ). Thus, the expected fraction of correct predictions is
( P × Pˆ ) + ((1− P ) × (1− Pˆ )). Since this is just the success ratio that we would expect if the signs of the actual and forecast predictions are independent, this is known
as the sign independence ratio (SRI) [12]:

SRI = ( P × Pˆ ) + ((1− P ) × (1− Pˆ )),

(7)

Pesaran & Timmermann (1992) show how SR and SRI can be combined to produce a
directional accuracy test in which values for the test are normally distributed under
the assumption that predicted and forecast values are independently distributed. The
variance in SRI and SR can be calculated as follows:
var( SRI ) =

1
4
[(2 Pˆ − 1) 2 P (1 − P ) + (2 P − 1) 2 Pˆ (1 − Pˆ ) + PPˆ (1 − P )(1 − Pˆ )]
n
m

(8)

1
SRI (1 − SRI ) .
n

(9)

var( SR ) =

The value of the Pesaran-Timmermann test, which we refer to as the PT-score, is
given by:
PT-score =

SR − SRI
var( SR ) − var( SRI )

∼ N (0,1) ,

(10)

Although the PT-score provides a measure of how statistically significant a set of
forecasts is, it does not distinguish between models that differ in some important

Lag-Dependent Regularization for MLPs Applied to Financial Time Series

521

respects. For example, it is often useful to analyze results of classification tasks using
a confusion matrix. For binary classification tasks, this will be a 2 by 2 matrix
Act +

Act −

⎡ TP
⎢
Pred − ⎣ FN

FP ⎤
TN ⎥⎦

Pred +

where the rows represent predicted positives and predicted negatives, and the
columns represent actual positives and actual negatives. The problem is that many
different confusion matrices can result in the same PT-score. Thus, although the
Pesaran-Timmermann test provides a measure of the statistical significance of a set of
directional forecasts, it does not take into account the different nature of the models in
respect to their ability to predicted positives/negatives.
One of the advantages of predicting the posterior probability of a directional
change is that we can then use these probabilities to select the threshold for our classification. For example, we may only choose to make a directional forecast of up if the
posterior probability exceeds a value of, say, 60%. For this reason, it is important to
have a measure of how our classifier performs not at just a 50% decision threshold,
but across the whole range of thresholds. Receiver Operating Characteristic (ROC)
curves provide such a measure. Plotting the true positive rate against the false positive
rate for decision thresholds from 0 to 1 produces an ROC curve. The area under the
curve provides a convenient single-value summary of the classifier’s performance.

4.2 Results
Table 1 and 2 show the respectively the PT-score and area under ROC curve for the
Australian All Ordinaries Index corresponding to a range of values for α and k from
Equation 10. The highest value in each table is shown in bold, and in both tables corresponds to α = 1.50 and k = 0.60. Table 3 show the critical values for PT-score. Thus
the observed PT-score of 5.135 is significant at the 10-6 level.
Table 1. PT-scores for Australian All Ordinaries (AORD) index

Α

k
0.08

0.10

0.20

0.40

0.60

0.80

1.00

1.50

2.00

3.00

0.000

1.472

1.472

1.472

1.472

1.472

1.472

1.472

1.472

1.472

1.472

0.001

1.404

1.404

1.355

1.472

1.671

1.915

2.126

2.058

2.626

1.404

0.010

2.099

2.118

2.141

2.212

2.813

2.896

2.856

3.174

1.570

2.099

0.100

2.103

2.081

1.964

1.979

2.224

2.889

3.028

2.228

2.618

2.103

0.500

2.832

2.332

3.039

1.916

3.523

2.607

2.194

2.776

3.284

2.832

1.00

3.224

4.156

2.220

4.297

3.242

4.042

3.775

3.711

2.194

3.224

1.50

4.003

3.438

2.649

3.550

5.135

4.087

4.110

2.814

2.469

4.003

2.00

2.968

3.467

1.800

2.840

3.786

3.584

2.299

0.421

0.612

2.968

10.0

1.349

1.890

2.719

2.468

1.436

0.387

0.928

-1.266

1.496

1.349

522

A. Skabar
Table 2. Area under ROC curve for Australian All Ordinaries (AORD) index

α

k
0.08

0.10

0.20

0.40

0.60

0.80

1.00

1.50

2.00

3.00

0.000

0.512

0.512

0.512

0.512

0.512

0.512

0.512

0.512

0.512

0.512

0.001

0.511

0.511

0.510

0.512

0.512

0.513

0.517

0.521

0.523

0.511

0.010

0.515

0.516

0.516

0.516

0.519

0.520

0.522

0.523

0.519

0.515

0.100

0.516

0.514

0.515

0.518

0.520

0.526

0.524

0.521

0.523

0.516

0.500

0.521

0.523

0.529

0.527

0.528

0.531

0.526

0.525

0.530

0.521

1.00

0.533

0.534

0.530

0.537

0.528

0.537

0.536

0.532

0.520

0.533

1.50

0.533

0.531

0.529

0.540

0.545

0.541

0.539

0.529

0.523

0.533

2.00

0.529

0.533

0.526

0.532

0.540

0.540

0.529

0.502

0.510

0.529

10.0

0.519

0.523

0.524

0.522

0.513

0.511

0.508

0.492

0.501

0.519

Table 3. Critical values for PT-score
Sig. level

0.05

0.01

10-3

10-4

10-5

10-6

10-7

10-8

PT-score

1.645

2.327

3.090

3.719

4.265

4.753

5.199

5.612

By observing the column corresponding to k = 0.6, it can be seen from both tables
that as α is increased the PT-score and Area steadily rise to a maximum value, and
then begin to decline. This can be explained by the fact that a small value of α provides only a small penalty against large weights, thus leading to overfitting on the
training data. Conversely, a large α imposes a high penalty on large weights, leading
to underfitting on the training data. In both cases (i.e., overfitting and underfitting),
we have suboptimal performance on holdout data.
Now consider the rows of each table. It can be seen that the maximum values in
each row correspond to k values in the vicinity of 0.4 to 0.8. While the pattern observed when k is increased from 0.08 to 3.00 is not one of steadily rising to a maximum, and then declining (i.e., the maximum is not as clearly pronounced as is the
case for varying α), the value of k does clearly affect the performance on holdout data.
In fact, it is interesting to compare these results with those obtained by using a fixed
number of inputs without any differentiation on the prior distribution of weight families. Using 1, 2, 3, 4 and 5 inputs yields respectively the following values for the area
under ROC curve: 0.534, 0.523, 0.525, 0.534 and 0.529. These values are well below
the value of 0.545 achieved using differentiated priors. It is interesting to note that the
best values for the MLP approach (PT-score = 5.135, Area = 0.545) are very close to
those for the density estimation-based approach described in Section 2 (PT-score =
5.704, Area = 0.543).
We performed the same experiments on the Australian–U.S. Foreign Exchange rate
and the Dow Jones Industrial Average index. For the exchange rate, the best values
obtained for PT-score and Area under ROC curve were 2.72 and 0.517, which are not
as significant as the values of 3.22 and 0.521 obtained using the density estimationbased approach. Results for the DJIA were not statistically significant, either for the
neural networks approach or the density estimation-based approach.

Lag-Dependent Regularization for MLPs Applied to Financial Time Series

523

5 Conclusions
A lag-dependent regularization technique has been proposed for use with MLPs applied to financial time series forecasting tasks. The technique is motivated by the
intuitive notion that recent values of the series should have more influence than less
recent values in predicting future values. The technique has been tested on three financial datasets, with directional forecast accuracy found to be significant on two of
these datasets. A disadvantage of the MLP approach, as compared with a density
estimation-based approach that we have previously proposed, is that in addition to the
value of α and k, MLPs also depend on factors such as starting weights. We are currently applying the regularization technique described here within a Bayesian MLP
learning framework. The integrative nature of the Bayesian framework is expected to
reduce some of the variation between results obtained from different starting weights.

References
1. Cybenko, G.: Approximation by superpositions of a sigmoidal function. Mathematics of
Control, Signals and Systems 2, 304–314 (1989)
2. Adya, M., Collopy, F.: How effective are neural networks at forecasting and prediction? a
review and evaluation. Journal of Forecasting 17, 481–495 (1998)
3. Chatfield, C.: Positive or negative? International Journal of Forecasting 11, 501–502
(1995)
4. Tkacz, G.: Neural network forecasting of Canadian gdp growth. International Journal of
Forecasting 17, 57–69 (2001)
5. Bishop, C.M.: Neural Networks for Pattern Recognition. Oxford University Press, Oxford
(1995)
6. Akaike, H.: A new look at statistical model evaluation. IEEE Transactions on Automatic
Control AC-19, 716–723 (1974)
7. Schwarz, G.: Estimating the dimension of a model. Annals of Statistics 6, 461–464 (1978)
8. Franses, P.H., van Dijk, D.: Non-Linear Time Series Models in Empirical Finance. Cambridge University Press, Cambridge (2000)
9. Skabar, A.: A kernel-based technique for direction-of-change financial time series forecasting. In: Bubak, M., van Albada, G.D., Dongarra, J., Sloot, P.M.A. (eds.) ICCS 2008,
Part II. LNCS, vol. 5102, pp. 441–449. Springer, Heidelberg (2008)
10. Parzen, E.: On the estimation of a probability density function and mode. Annals of
Mathematical Statistics 33, 1065–1076 (1962)
11. Duda, R.O., Hart, P.E.: Pattern Classification and Scene Analysis. John Wiley and Sons,
New York (1974)
12. Pesaran, M.H., Timmermann, A.: A simple non-parametric test of predictive performance.
Journal of Business & Economic Statistics 10, 461–465 (1992)

