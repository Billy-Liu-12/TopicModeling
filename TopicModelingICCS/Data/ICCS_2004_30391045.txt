Balanced Binary Search Trees Based Approach for
Sparse Matrix Representation
Igor Balk1, Igor Pavlovsky2, Andrey Ushakov3, and Irina Landman3
1

2

R3 Logic Inc, 80 Sherman Street, Camridge MA 02140, USA
Terra Discount Club LLC, 110 Strathmore Rd 302, Brighton MA 02135 USA
3
Corning Ltd, 4 Birzhevaya Linia, 199034, St. Petersburg, Russia

Abstract. In this paper novel method of memory allocation for sparse matrixes
is presented. Sparse matrixes are widely used in computational electrodynamics. This paper show that use of balanced binary search trees for memory
allocation of sparse matrixes guarantees O(ln(n)) access and insertion time.
Comparison with traditional hash map memory allocation method was also
made and presented in the paper.

1 Introduction
Many aspects of modern life require sophisticated computations to be performed. The
majority of these computations and simulations are based on operations with sparse
matrixes. A sparse matrix is a matrix with number of zero elements much larger than
number of non-zero elements. These matrices are used in many computational
disciplines such as image processing, computational electro and hydrodynamics and
many others. Utilizing this characteristics, we can significantly reduce memory
space required to store these matrixes and use special algorithms with this type of
matrixes much faster than with dense matrixes. Good examples of such computations
utilizing sparse matrices are image processing and electromagnetic simulation.
Performance of mathematical libraries used for these simulations becomes critically
important as complexity level growth. Direct algorithms will give O(n3), where n is
the size of the system to be solved, increase in computational time and memory
required, and it is challenging problem even for modern supercomputers. To solve
this problem we developed fast numerical linear algebra library for sparse matrixes,
which is based on data structures with O(ln(n)) access time, where n is dimension of
the matrix.
Several data structures with O(ln(n)) access time are known in graph theory. In
our work we are utilizing different kinds of binary trees in order to accelerate
performance of the library. The most natural assumption is to use red-black trees and
AVL trees. These trees are well known in computer science theory and widely used
for solution of network routing problem. We decided to use these trees to create a data
structure in which sparse matrix will be stored.

M. Bubak et al. (Eds.): ICCS 2004, LNCS 3039, pp. 1045–1048, 2004.
© Springer-Verlag Berlin Heidelberg 2004

1046

I. Balk et al.

2 Theory
There are many publications available describing the theory of binary trees. These
trees are popular since they allow the search of the element to be performed in c*ln(n)
operations, where c is some constant and n is the number of elements in the tree. The
major and still unanswered question is how to reduce the constant c. There are several
techniques, which will give improvements in c value depending on the problem. Two
most well known techniques are AVL trees and red-black trees. Use of this trees gives
performance of a search and insertion operation in a tree close to optimal. [1, 2, 3]
A binary search tree (BST) is an AVL tree if the difference in height between the
subtrees of each of its nodes is between -1 and +1. Said another way, a BST is an
AVL tree if it is an empty tree or if its subtrees are AVL trees and the difference in
height between its left and right subtree is between -1 and +1. In order to demonstrate
performance we will mention some theorem [2]: an AVL tree with n nodes has height
between log2 (n + 1) and 1.44 * log2 (n + 2) - 0.328. An AVL tree with height h has
between pow (2, (h + .328) / 1.44) - 2 and pow (2, h) - 1 nodes.
For comparison, an optimally balanced BST with n nodes has height ceil (log2 (n
+ 1)). An optimally balanced BST with height h has between pow (2, h - 1) and pow
(2, h) - 1 nodes.The average speed of a search in a binary tree depends on the tree's
height, so the results above are quite encouraging: an AVL tree will never be more
than about 50% taller than the corresponding optimally balanced tree. Thus, we have
a guarantee of good performance even in the worst case, and optimal performance in
the best case [2]. And for the red-black theorem, similar to the one we introduced for
AVL trees [1, 3], can be formulated: a red-black tree with n nodes has height at least
log2 (n + 1) but no more than 2 * log2 (n + 1). A red-black tree with height h has at
least pow (2, h / 2) - 1 nodes but no more than pow (2, h) - 1.
And for comparison, an optimally balanced BST with n nodes has height ceil
(log2 (n + 1)). An optimally balanced BST with height h has between pow (2, h - 1)
and pow (2, h) - 1 nodes. Thus we can see that if we will use AVL and red-black trees
to store data in sparse matrixes we will get performance close to optimal.

3 Implementation
Based of on the theory described above we have developed memory allocation
algorithm for sparse matrixes. We used AVL trees to represent rows and colons of
space matrixes. As it was shown above similar results could be obtained using redblack trees. Algorithm was implemented in ANSII C for maximum performance and
portability. Important to mention, that described approach allows easy parallelization
and further speed up on multiprocessor architecture.

4 Computational Results
We performed some preliminary tests comparing our new library with the data hash
map memory allocation model, which is widely used in sparse matrix manipulation

Balanced Binary Search Trees Based Approach for Sparse Matrix Representation

1047

for example in such popular package as Matlab. Both memory models were
implemented in ANSII C with same compiler optimization. We performed two tests
to demonstrate performance of new memory model. The results of the tests are shown
on Figures 1and 2.
Time vs fill factor

Time vs matrix size (fill factor 0.1)

1000000

1000000

100000

100000

10000

10000
hash map(ms)
new algorithm(ms)

1000
100

100

10

10

1
100

hash map(ms)
new algorithm (ms)

1000

1
0.001

1000

Fig. 1. Filling time vs. matrix size for fixed
filling factor

0.01

0.1

1

Fig. 2. Filling time vs. filling factor for fixed
matrix size

On these figures dependence of the time of random filling, for the matrix of given
size, from zero up to given filling factor is demonstrated. The Figure 1 demonstrates
this dependence for the fixed filling factor = 0.1 and matrix size varying from the
n=102 to the n=103. On vertical axis filling time is shown and on horizontal - size of
the matrix. As it’s clear from the graph even with relatively small matrixes new
algorithm of memory allocation gives order of magnitude saving in time. Figure 2
shows filling time (vertical axis) for the sparse matrix with fixed size and various
filling factors (horizontal axis). The size of the matrix n is set to be 103 and filling
factor is varying from 0.002 to 0.1.
Table 1. Speed of algorithm depending of filling factor for fixed matrix size (n=1000)
Fill factor
0.003
0.004
0.005
0.006
0.007
0.008
0.009
0.01
0.02
0.03
0.04
0.05
0.06
0.07
0.08
0.09
0.1

Hash map (ms)
35
50
80
110
150
200
240
291
1122
3646
10344
20900
34840
51143
71342
93955
117979

New algorithm (ms)

10
20
40
50
60
80
90
110
130
150

1048

I. Balk et al.
Table 2. Speed of algorithm depending of matrix size for fixed filling factor (f=0.1)
Matrix size

Hash map (ms)
200
300
400
500
600
700
800
900
1000

New algorithm (ms)
40
200
630
1623
6329
18307
39156
71403
117549

5
10
20
40
60
80
120
150

5 Conclusion
We believe that the described library will provide engineers and researchers with a
fast, reliable, and cost effective tool to satisfy the growing need for computational
tools. A simple, intuitive interface, fast data access algorithms, and low cost will
make this product a “must have” tool for people using numerical linear algebra in
their work.

References
1. Cormen, C. H., C. E. Leiserson, and R. L. Rivest, Introduction to Algorithms. McGrawHill, 1990.
2. Knuth, D. E., The Art of Computer Programming, Volume 3: Sorting and Searching, 2nd
ed. Addison-Wesley,
3. Sedgewick, R., Algorithms in C, Parts 1-4, 3rd ed. Addison-Wesley, 1998
4. Trefethen, Lloyd N.(Lloyd Nicholas)., David Bau, III, Numerical linear algebra SIAM,
Philadelphia, 1997
5. Gene H. Golub, Charles F. Van Loan, Matrix Computations, 2 nd ed.The John Hopkins
University Press, Baltimore, 1998
6. Igor Balk, Andrey Ushakov, Irina Landman “FNLAL: AVL-Tree based fast numerical
linear algebra”, Corning Technical Report, Corning, NY, 2001

