Parallel Optimization Methods Based on
Direct Search
Rafael A. Trujillo Ras´
ua1,2 , Antonio M. Vidal1 ,
and V´ıctor M. Garc´ıa1
1

Departamento de Sistemas Inform´
aticos y Computaci´
on
Universidad Polit´ecnica de Valencia
Camino de Vera s/n, 46022 Valencia, Espa˜
na
{rtrujillo, avidal, vmgarcia}@dsic.upv.es
2
Departamento de T´ecnicas de Programaci´
on,
Universidad de las Ciencias Inform´
aticas,
Carretera a San Antonio de los Ba˜
nos Km 2 s/n,
La Habana, Cuba
trujillo@uci.cu

Abstract. This paper is focused in the parallelization of Direct Search
Optimization methods, which are part of the family of derivative-free
methods. These methods are known to be quite slow, but are easily parallelizable, and have the advantage of achieving global convergence in
some problems where standard Newton-like methods (based on derivatives) fail. These methods have been tested with the Inverse Additive
Singular Value Problem, which is a diﬃcult highly nonlinear problem.
The results obtained have been compared with those obtained with derivative methods; the eﬃciency of the parallel versions has been studied.

1

Introduction

The general unrestricted optimization problem can be stated as follows: Given a
continuous function f : n −→ , ﬁnd the minimum of this function. The most
popular optimization methods use derivative or gradient information to build
descent directions. However, there are many situations where the derivatives
may not exist or cannot be computed, or are too expensive to compute. In such
situations the derivative-free methods can be the only resource. These methods
were studied ﬁrst by Hooke and Jeeves [5] in 1961. In 1965 another important
method of this kind was discovered, the Nelder-Mead simplex method [11].
All these methods are known to be fairly robust and locally convergent, but
they are also quite slow, compared with derivative methods. For some time, this
caused a lack of interest in these methods. However, with the advent of parallel
computing these methods were again popular, since they are easily parallelizable
[13, 2, 6].
This paper address the parallelization of a subset of these methods, termed in
[9] as “Direct Search methods”, where these methods are deeply described and
studied. For the work described in this paper, diﬀerent versions of these Direct
V.N. Alexandrov et al. (Eds.): ICCS 2006, Part I, LNCS 3991, pp. 324–331, 2006.
c Springer-Verlag Berlin Heidelberg 2006

Parallel Optimization Methods Based on Direct Search

325

Search methods have been implemented and parallelized; in this paper the best
versions are presented, and, as expected, the parallel versions obtain substantial
time reductions over the sequential versions. Still more remarkable is the overall
robustness of the direct search methods. These methods have been tested with
a very diﬃcult problem, the Inverse Additive Singular Value Problem (IASVP),
which is a problem where Newton-like methods usually fail to converge, even
using globalization techniques as Armijo’s rule [7] . As will be shown, the Direct
Search methods converge where Newton’s method fails, although they are still
substantially slower than Newton’s method.
The next section is devoted to the description of the Direct Search methods
and the sequential versions implemented. Next, the parallel versions are presented, followed by the numerical results of the sequential and parallel methods.
Finally the conclusions shall be given.

2

Direct Search Methods

We will present two diﬀerent direct search methods; both belong to the class of
Generating Set Search (GSS), as deﬁned by Kolda et. al. in [9]. These methods
start from an initial point x0 , an initial step length Δ0 and a set of directions
spanning n : D0 = {di }i=1,...p , di ∈ n , so that every vector in n can be
written as a nonnegative linear combination of the directions in Dk .
The driving idea of the GSS methods is to ﬁnd a decrease direction among
those of Dk . To do that, in each iteration k the objective function f is evaluated
along the directions in Dk . At that stage, the actual point shall be xk and the
step length is Δk ; therefore, f (xk + Δk di ) is computed, for i = 1, ..., p, until a
direction di is found such that f (xk + Δk di ) < f (xk ). If no such direction is
found, the step length is decreased and the function is evaluated again along the
directions.
When an acceptable pair (Δk , di ) is found, the new point is updated:
(xk+1 = xk + Δk di ), a new step length Δk+1 is computed and the set of directions is possibly modiﬁed or updated. This procedure shall be repeated until
convergence (that is, when the step length is small enough).
This general algorithmic framework can be implemented in many ways.Among
the versions that we have implemented, we have chosen the two versions described below, since they give the best results.
2.1

Method GSS1

In our ﬁrst version, the direction set chosen as
Dk = {±ei } ∪ {(1, 1, · · · , 1) , (−1, −1, · · · , −1)} ,

(1)

that is, the set of the coordinate axes and the vectors (1, 1, · · · , 1) and
(−1, −1, · · · , −1) which very often accelerate the search when the initial point is
far from the optimum.

326

R.A.T. Ras´
ua, A.M. Vidal, and V.M. Garc´ıa

Another characteristic is the strategy proposed in [6] of doubling the step
length when the same descent direction is chosen in two consecutive iterations.
A distinct characteristic of our method (not proposed before, as far as we
know) is the appropriate rotations of the set of directions. The evaluations are
carried out in the order in which the directions are located in Dk . Therefore, if the
descent directions are located in the ﬁrst positions of the vector, the algorithm
should ﬁnd the optimum faster. To accomplish that, we propose that if in the
iteration k the ﬁrst descent direction is di , it means that d1 , ..., di−1 are not
descent directions and, most likely, will not be descent directions in the next
iterations. Therefore, in our algorithm, these directions are displaced to the end
of the Dk set; that is, the new set would be: (di , di+1 , ..., dn , d1 , ..., di−1 ).
2.2

Method GSS2

This method follows a similar strategy to the described by Hooke and Jeeves
in [5]. Here the initial direction set is D0 = {±ei }, and, unlike in GSS1, all
the directions in Dk are explored. Then, choosing all the descent directions:
(k) (k)
(k)
di,1 , di,2 , ...di,j , a new descent direction is built:
(k)

(k)

(k)

d = di,1 + di,2 + ... + di,j

(2)

which probably will cause a greater descent. If this is not true, the algorithm
(k) (k)
(k)
will choose among di,1 , di,2 , ...di,j the direction with larger descent.
Clearly, this algorithm should need less iterations for convergence than GSS1,
but the cost of each iteration is larger, since the function will be evaluated along
all the directions.

3

Parallelization of the Direct Search Methods

The simplicity of the direct search methods and the independence of the function
evaluations along each direction makes the parallelization of these methods a
relatively easy task. The only serious problem to solve is the load imbalance
that can suﬀer the algorithm GSS1. We will describe ﬁrst the simpler parallel
version of GSS2, and then two diﬀerent options for GSS1.
3.1

Parallel Method PGSS2

The 2n function evaluations needed for each iteration of GSS2 can be distributed among p processors, so that each one carries out approximately 2n
p evaluations.The parallel algorithm PGSS2 would start by distributing the initial point
x0 , the initial step length Δ0 and a set of approximately 2n
p directions. In each
iteration each processor would evaluate the function along each direction, and
would return the descent directions found and the corresponding function values
to the “Root” processor. This processor would form the new direction (as in
(2)), would evaluate the function along this direction and obtain the new point
xk+1 and the new step length Δk+1 , which would be broadcasted to the other
processors.

Parallel Optimization Methods Based on Direct Search

327

In this algorithm PGSS2 there is hardly any load imbalance, and few communications, so that the speed-up is expected to be close to the optimum.
3.2

Parallel Method PGSS1

The parallel version of the algorithm GSS1 follows the same data distribution of
GSS2. However, the underlying strategy must be diﬀerent. Let us recall that in
the GSS1 algorithm the function is evaluated sequentially along each direction
until a descent direction is found.
As in the parallel version of GSS2, the directions are distributed among the
processors. If a processor ﬁnds among its directions a descent one, it should
warn the rest of processors to stop searching. The strategy chosen for this
parallel algorithm is to select a number of evaluations m that any processor should perform before reporting to the other processors. If after m evaluations there has been no success, they must perform m evaluations more,
and repeat the process until a descent direction is found or they run out of
directions.
It might happen that when the processors broadcast their results, several
descent directions are found. If this happens, the direction of larger descent is
chosen. This can cause that the parallel algorithm PGSS1 can make less iterations than the sequential version GSS1.
3.3

Parallel Method Master-Slave MSPGSS1

In the section 3.2 the algorithm described will perform a number of innecesary
evaluations, depending on the value of m, and maybe some innecesary communications (if m is too small). As an attempt to improve it, we implemented an
asynchronous parallel version, using a Master-Slave scheme.
The Master processor controls all the search, while the slaves perform the
evaluations of the function along the directions. The directions are distributed
among the slaves; after each evaluation, the slave sends the value with a nonblocking message to the master processor. The non-blocking message allows that
the slave goes on with its work. When the master detects a descent, sends to
all the slaves a Success message; then, computes the new point and step length,
and broadcasts it to all the slaves.
A drawback of this algorithm is that there is one processor less to perform
evaluations, although this allows a better control and overlaps computing time
with communication time.

4
4.1

Experimental Results
Sequential Experiments

All these algorithms have been implemented and applied to a diﬃcult problem,
the Inverse Additive Singular Value Problem (IASVP), which can be deﬁned as:

328

R.A.T. Ras´
ua, A.M. Vidal, and V.M. Garc´ıa

Given a set of matrices A0 , A1 , ..., An ∈ m×n (m ≥ n) and a set of real
numbers S ∗ = {S1∗ , S2∗ , ..., Sn∗ }, where S1∗ > S2∗ > ... > Sn∗ , ﬁnd a vector c =
[c1 , c2 , ..., cn ]t ∈ n , such that S ∗ are the singular values of
A(c) = A0 + c1 A1 + ... + cn An .

(3)

This problem is usually formulated as a nonlinear system of equations, and
is solved with diﬀerent formulations of Newton’s method; several formulations
are analyzed in [3]. However, it can be formulated as well as an optimization
problem, minimizing the distance between the desired singular values S ∗ and
the singular values of A(c).
The sequential algorithms were implemented in C, using Blas [4] and LAPACK
[1], and were executed in a 2GHz Pentium Xeon with 1 GByte of RAM and
with operating system Red Hat Linux 8.0. For the experiments carried out with
sequential algorithms, random square matrices were generated with sizes n =
5, 10, 15, 20, 25, 30, 40, 50 ; the solution vector c∗ was chosen randomly as well,
(0)
and the initial guesses ci were taken perturbing the solution with diﬀerent
(0)
values δ: ci = c∗ + δ, i = 1, · · · , n .
To obtain a fair appreciation of the performance of the Direct Search methods applied to the IASVP, the results of these methods were compared to Newton’s method. Since there was convergence problems, Armijo’s rule used to improve convergence in Newton’s method. However, it became clear that Newton’s
method is very sensitive to the distance from the initial point to the solution.
When the perturbation δ used was small, δ = 0.1 or smaller, Newton’s method
converged always. However, when δ = 1.1 it only converged in the case with
n = 5, and when δ = 10.1 it did not converge in any test. Meanwhile, the Direct
Search methods converged in all the cases, although in a quite large number of
iterations.
Table 1. Number of iterations to convergence, δ = 0.1
n
GSSI
GSSII
Newton

5
82
51
3

10 15 20 25 30 40 50
630 2180 967 827 6721 7914 7178
669 611 372 322 2237 1368 7292
3
7
5 7
7
5
6

In the table 2 are shown the execution times of GSS1, GSS2 and Newton’s
method for the case δ = 0.1, when Newton’s method converges. GSS1 is faster
than GSS2, but Newton’s method is much faster than Direct Search methods,
when it converges.
Table 2. Execution Times (seconds), δ = 0.1
n
GSSI
GSSII
N ewton

5
0,03
0,02
0,00

10
1,26
0,50
0,00

15
3,37
2,96
0,01

20
2,80
4,12
0,01

25
4,10
6,77
0,02

30
40
50
45,00 139,76 237,48
86,21 170,34 2054,88
0,04 0,08
0,25

Parallel Optimization Methods Based on Direct Search

329

These results show that the application of the sequential Direct Search methods to high complexity problems is not practical, since the execution times are
too high. However, they show a very interesting property; at least for this problem, they are far more robust than Newton’s method when the initial guess is
far away from the solution.
4.2

Parallel Experiments

The tests were carried out in a cluster with 20 2GHz Intel Xeon biprocessors, each
one with 1 Gbyte of RAM, disposed in a 4x5 mesh with 2D torus topology and
interconnected through a SCI network. The parallel methods were implemented
using MPI [12]. The sizes of the problems were conditioned by the high cost of
the sequential solution. Due to this, the sizes of the problems were not too large:
n = 72, 96, 120, 144. However, it must be noted that Newton’s algorithm did not
converge in these cases, even taking a initial guess very close to the real solution.
The tables 3,4,5 summarize the execution times of the parallel algorithms.
Table 3. Execution Times (seconds) PGSS1
size\processors 1
72
763
96
3929
120
10430
144
36755

2
307
1982
4961
17935

4
136
621
1891
5626

6
80
362
943
3187

8
59
336
669
2209

10
64
266
605
1599

16
32
187
364
1207

Table 4. Execution Times (seconds) PGSS2
size\processors 1
72
278
96
2710
120
5020
144
14497

2
141
1400
2581
7349

4
72
710
1322
3749

6
50
482
897
2537

8
38
375
703
1948

10
33
313
560
1565

16
23
210
377
1028

Table 5. Execution Times (seconds) MSPGSS1
size\processors 1
72
763
96
3929
120
10430
144
36755

2
762
3940
10614
36786

4
214
1228
3494
12258

6
133
767
2108
6244

8
95
467
1459
4375

10
108
482
1134
3630

16
58
263
958
2286

It must be observed that the sequential algorithm is the same for PGSS1 and
MSPGSS1, and therefore the times are identical. These times are quite similar
as well to the times of the MSPGSS1 with two processors, since there is only a
slave processor performing the evaluations.

330

R.A.T. Ras´
ua, A.M. Vidal, and V.M. Garc´ıa

The Master-slave algorithm performs worse than the other two. The fact of
having one less processor performing evaluations does not seem to be the reason,
since with 10 or 16 processors the inﬂuence should be less important.
The comparison of the algorithms PGSS1 and PGSS2 shows interesting
trends; for these new sizes the sequential algorithm GSS1 deteriorates quite fast
when the problem size grows, unlike in the smaller test cases (See Table 2). With
only one processor, the algorithm GSS2 is twice faster for large problems.
However, when the number of processors increases the situation changes; the
PGSS2 algorithm gives good speedups, as expected, but the PGSS1 algorithm
obtains great advantages when the number of processors grows, so that for large
number of processors it obtains execution times close to those of PGSS2. It
can be seen that the algorithm PGSS1 obtains speed-ups over the theoretical
maximum, which is the number of processors. This happens because the parallel
algorithm does not follow the same search strategy than the sequential one; this
causes that the number of iterations and the number of function evaluations is
diﬀerent for both algorithms.
The algorithm PGSS2 gives the best results overall.

5

Conclusions

Three parallel Direct search methods have been implemented and described.
Some new strategies have been applied:
1. In the GSS1 method, the new strategy of rotating the serch directions complements the strategy described in [6] of doubling the time step. This increases the probabilities of a descent direction being selected in two consecutive iterations, decreasing the average execution time.
2. In the GSS2 method, the addition of all the descent directions generates
easily a new descent direction.
3. A Master-slave asynchronous algorithm was designed and implemented. It
obtains quite good speed-ups, but the overall time execution is, with the
present version, worse than the other two versions.
Finally, we would like to remark the great robustness shown by these methods
in the IASVP problem, which is a really testing problem. For this problem, the
convergence radius shown by these methods is far larger than the shown by
Newton method, which, when converges, is much faster.
This shows that the Direct Search methods could be very useful for solution
of diﬃcult problems; if not for the full procedure, they could be used to obtain a
good initial approximation for Newton’s method, composing therefore a hybrid
method. As shown, the long execution times of the Direct Search methods can
be alleviated using parallel versions of these methods.

Acknowledgement
This work has been supported by Spanish MCYT and FEDER under Grant
TIC2003-08238-C02-02 .

Parallel Optimization Methods Based on Direct Search

331

References
1. Anderson E., Bai Z., Bishof C., Demmel J., Dongarra J.: LAPACK User Guide;
Second edition. SIAM (1995)
2. Dennis J.E. and Torczon V.: Direct search methods on parallel machines; SIAM J.
Optim., 1, (1991) 448-474
3. Flores F., Garc´ıa V.M., and Vidal A.M.: Numerical Experiments on the solution
of the Inverse Additive Singular Value Problem; Computational Science - 5th International Conference, Atlanta, GA, USA (2005)
4. Hammarling S., Dongarra J., Du Croz J., Hanson R.J.: An extended set of fortran
basic linear algebra subroutines; ACM Trans. Mat. Software (1988)
5. Hooke R. and Jeeves T.A.: Direct Search solution of numerical and statistical
problems; Journal of the Association for Computing Machinery, (1961) 212-229
6. Hough P.D., Kolda T.G. and Torczon V.: Asyncronous parallel pattern for nonlinear optimization. SIAM J. Sci. Comput., 23, (2001) 134-156
7. C. T. Kelley: Iterative methods for linear and nonlinear equations; SIAM (1995).
8. Kolda T.G. and Torczon V.: On the Convergence of Asyncronous Parallel Pattern
Search; Tech. Rep. SAND2001-8696, Sandia National Laboratories, Livermore, CA,
(2002); SIAM J. Optim., submitted.
9. Kolda T. G., Lewis R. M. and Torczon V.: Optimization by Direct Search: New
perspective on Some Clasical and Modern Methods; SIAM Review, Vol. 45, No. 3
(2003) 385-442
10. Kumar V., Gramar A., Gupta A. and Kerypis G.: Introduction to Parallel Computing: Desing and analysis of Algorithms; Redwood City, CA (1994)
11. Nelder J.A. and Mead R.: A simplex method for function minimization; The
Computer Journal, 7 (1965) 308-313
12. Snir M., Otto S., Huss-Lederman S., Walker D. and Dongarra J.: MPI: The Complete Reference; MIT Press, (1996)
13. Torczon V.: Multi-Directional Search: A Direct Search Algorithm for Parallel
Machines; Technical Report 90-7, Department of Computational and Applied
Mathematics, Rice University, Houston, TX (1990), Author’s 1989 Ph.D. dissertation.

