Implementation of Kolmogorov Learning
Algorithm for Feedforward Neural Networks
Roman Neruda, Arnošt Štědrý, and Jitka Drkošová�
Institute of Computer Science, Academy of Sciences of the Czech Republic,
P.O. Box 5, 18207 Prague, Czech Republic
roman@cs.cas.cz

Abstract. We present a learning algorithm for feedforward neural networks that is based on Kolmogorov theorem concerning composition of ndimensional continuous function from one-dimensional continuous functions. A thorough analysis of the algorithm time complexity is presented
together with serial and parallel implementation examples.

1

Introduction

In 1957 Kolmogorov [5] has proven a theorem stating that any continuous function of n variables can be exactly represented by superpositions and sums of
continuous functions of only one variable. The ﬁrst, who came with the idea
to make use of this result in the neural networks area was Hecht-Nielsen [2].
Kůrková [3] has shown that it is possible to modify the original construction for
the case of approximation of functions. Thus, one can use a perceptron network
with two hidden layers containing a larger number of units with standard sigmoids to approximate any continuous function with arbitrary precision. In the
meantime several stronger universal approximation results has appeared, such
as [6] stating that perceptrons with one hidden layer and surprisingly general
activation functions are universal approximators.
In the following we review the relevant results and show a learning algorithm
based on Sprecher improved version of the proof of Kolmogorov’s theorem. We
focus on implementation details of the algorithm, in particular we proposed an
optimal sequential version and studied its complexity. These results have also
lead us to consider ways of suitable parallelization. So far, we have realized one
parallel version of the algorithm running on a cluster of workstations.

2

Preliminaries

By R we denote the set of real numbers, N means the set of positive integers,
I = [0, 1] and thus I n is the n-dimensional unit cube. By C(I n ) we mean a set
of all continuous functions deﬁned over I n .
�

This research has been partially supported by GAASCR under grants B1030006 and
A2030801, and by GACR under grant 201/99/0092.

V.N. Alexandrov et al. (Eds.): ICCS 2001, LNCS 2074, pp. 986–995, 2001.
c Springer-Verlag Berlin Heidelberg 2001
�

Implementation of Kolmogorov Learning Algorithm

Deﬁnition 1. The sequence {λk } is integrally
� independent if
any ﬁnite selection of integers tp for which p |tp | �= 0

�

p tp λp

987

� = 0for

Deﬁnition 2. By sigmoidal function we mean any function σ : R → I with the
following limits: limt→−∞ σ(t) = 0 limt→∞ σ(t) = 1
Deﬁnition 3. The set of staircase-like functions
�k of a type σ is deﬁned as: the
set of all functions f (x) of the form f (x) = i=1 ai σ(bi x + ci ), and denoted as
S(σ).
Deﬁnition 4. A function ωf : (0, ∞) → R is called a modulus of continuity of a
function f : I n → R if ωf (δ) = sup{|f (x1 , . . . , xn ) − f (y1 , . . . , yn )|; (x1 , . . . , xn ),
(y1 , . . . , yn ) ∈ I n with |xp − yp | < δ for every p = 1, . . . , n}.
In the following we always consider (neural) network to be a device computing
certain function dependent on its parameters. Without the loss of generality we
limit ourselves only to networks with n inputs with values taken from I and one
real output. Thus we consider functions f : I n → R.
Moreover, we talk about two types of network architectures. One is the usual
multilayer perceptron with two hidden layers. Perceptron units in each layer
compute the usual aﬃne combination of its inputs and weights (and bias) and
then apply a sigmoidal activation function. An example can be the most common
perceptron with logistic sigmoid. The (single) output unit computes just the
linear combination.
The second type of network is a more general feedforward network where the
units in diﬀerent layers can compute diﬀerent activation functions that can be
more complicated than the logistic sigmoid. The description of a concrete form
of such a network is subject to the section 4.

3

Kolmogorov Theorem

The original Kolmogorov result shows that every continuous function deﬁned
on n-dimensional unit cube can be represented by superpositions and sums of
one-dimensional continuous functions.
Theorem 1 (Kolmogorov). For each integer n ≥ 2 there are n(2n + 1) continuous monotonically increasing functions ψpq with the following property: For
every real-valued continuous function f : I n → R there are continuous functions
φq such that
� n
�
2n
�
�
φq
ψpq (xp ) .
(1)
f (x1 , . . . , xn ) =
q=0

p=1

Further improvements by Sprecher provide a form that is more suitable for
computational algorithm. Namely, the set of functions ψpq is replaced by shifts
of a ﬁxed function ψ which is moreover independent on a dimension. The overall
quite complicated structure is further simpliﬁed by suitable parameterizations
and making use of constants such as λp , β, etc.

988

R. Neruda, A. Štědrý, and J. Drkošová

Theorem 2 (Sprecher). Let {λk } be a sequence of positive integrally independent numbers. There exists a continuous monotonically increasing function
ψ : R+ → R+ having the following property: For every real-valued continuous
function f : I n → R with n ≥ 2 there are continuous functions φ and a constant
β such that:
n
�
λp ψ(xp + qβ)
(2)
ξ(xq ) =
p=1

f (x) =

2n
�
q=0

φq ◦ ξ(xq )

(3)

Another result important for computational realization is due to Kůrková
who has shown that both inner and outer functions ψ and φ can be approximated
by staircase-like functions (cf. Deﬁnition 3) with arbitrary precision. Therefore,
standard perceptron networks with sigmoidal activation functions can, in principle, be used in this approach. The second theorem of hers provides the estimate
of units needed for approximation w.r.t. the given precision and the modulus of
continuity of the approximated function.
Theorem 3 (Kůrková). Let n ∈ N with n ≥ 2, σ : R → I be a sigmoidal
function, f ∈ C(I n ), and ε be a positive real number. Then there exists k ∈ N
and functions φi , ψpi ∈ S(σ) such that:
� n
�
k
�
�
(4)
|f (x1 , . . . , xn ) −
φi
ψpi xp | ≤ ε
i=1

p=1

for every (x1 , . . . , xn ) ∈ I n .
Theorem 4 (Kůrková). Let n ∈ N with n ≥ 2, σ : R → I be a sigmoidal
function, f ∈ C(I n ), and ε be a positive real number. Then for every m ∈ N such
that m ≥ 2n + 1 and n/(m − n) + v < ε/||f || and ωf (1/m) < v(m − n)/(2m − 3n)
for some positive real v, f can be approximated with an accuracy ε by a perceptron
type network with two hidden layers, containing nm(m + 1) units in the ﬁrst
hidden layer and m2 (m+1)n units in the second one, with an activation function
σ.

4

Algorithm Proposal

Sprecher sketched an algorithm based on Theorem 2 that also takes into account
Theorem 4 by Kůrková. Here we present our modiﬁed and improved version that
addresses crucial computational issues.
The core of the algorithm consists of four steps:
For each iteration r:
1. Construct the mesh Qn of rational points dk dissecting the unit cube (cf. (5)).

Implementation of Kolmogorov Learning Algorithm

989

2. Construct the functions ξ in the points dqk (see (6) and (11)).
3. Create sigmoidal steps θ described in (10).
4. Compile the outer functions φjq (see (9)) based on θ and previous approximation error er .
5. Construct the r-th approximation fr of original function f according to (14),
and the r-th approximation error er according to (13).
4.1

The Support Set Q

Take integers m ≥ 2n and γ ≥�m + 2 where n is the input dimension. Consider�a
�k
set of rational numbers Q = dk = s=1 is γ −s ; is ∈ {0, 1, . . . , γ − 1}, k ∈ N .
Elements of Q are used as coordinates of n-dimensional mesh
Qn = {dk = (dk1 , . . . , dkn ); dkj ∈ Q, j = 1, . . . , n} .

(5)

Note that the number k determines the precision of the dissection.
For q = 0, 1, . . . , m we construct numbers dqk ∈ Qn whose coordinates are
determined by the expression
dqkp = dkp + q

k
�

γ −s

(6)

s=1

Obviously dqkp ∈ Q for p = 1, 2, . . . , n. We will make use of dqk in the deﬁnition
of functions ξ.
4.2

The Inner Function ψ

The function ψ : Q → I is then deﬁned with the help of several additional
deﬁnitions. For the convenience, we follow [11] in our notation.
ψ(dk ) =

k
�

i˜s 2−ms γ −ρ(s−ms ) ,

(7)

s=1

nz − 1
,
ρ(z) =
n−1
�
�
s−1
�
ms = �is � 1 +
[il ] · . . . · [is−1 ] ,
l=1

i˜s = is − (γ − 2)�is �.

Let [i1 ] = �i1 � = 1 and for s ≥ 2 let [is ] and �is � be deﬁned as:
�
0 for is = 0, 1, . . . , γ − 3
[is ] =
1 for is = γ − 2, γ − 1
�
0 for is = 0, 1, . . . , γ − 2
�is � =
1 for is = γ − 1

Figure 1 illustrates the values of ψ for k = 1, 2, 3, 4; n = 2; γ = 6.

(8)

990

R. Neruda, A. Štědrý, and J. Drkošová

1

1.2

k=1
k=2
k=3
k=4

0.9

1
0.8

0.8

0.6
0.7

0.4

0.6

0.2

0.5

0

0.4
0.3

1
0.9
0.8
0.7
0.6
0.5
0.4
0.3
0.2
0.1

0.2
0.1
0

0

0.1

0.2

0.3

0.4

0.5

0.6

0.7

0.8

0.9

1

0 0

0.8 0.9
0.6 0.7
0.4 0.5
0.3
0.1 0.2

1

Fig. 1. a) Values of ψ(dk ) for various k. b) Values of ξ(dk ) for k = 2.

4.3

The Outer Functions φ

The functions
�r φq in equation (3) are constructed iteratively as functions φq (yq ) =
limr→∞ j=1 φjq (yq ). Each function φjq (yq ) is determined by ej−1 at points from
the set Qn . The construction is described in the following.
For q = 0, 1, . . . , m and j = 1, . . . , r we compute:
1 �
ej−1 (dk ) θ(dqk ; ξ(xq )),
(9)
φjq ◦ ξ(xq ) =
m+1 q
dk

where dk ∈ Q.
The real-valued function θ(dk ; ξ(xq )) deﬁned for a ﬁxed point dqk ∈ Qn . The
deﬁnition is based on a given sigmoidal function σ:
θ(dqk ; yq ) = σ(γ β(k+1) (yq − ξ(dqk )) + 1)
− σ(γ

β(k+1)

(yq −

ξ(dqk )

(10)

− (γ − 2)bk )

where yq ∈ R, and bk is a real number deﬁned as follows:
bk =

∞
�

γ −ρ(s)

n
�

λp .

p=1

s=k+1

The functions ξ(dqk ) are expressed by equation
ξ(dqk ) =

n
�

λp ψ(dqkp )

(11)

p=1

where ψ(dqkp ) are from (7), and coeﬃcients λp are deﬁned as follows.
Let λ1 = 1 and for p > 1 let
λp =

∞
�
s=1

γ −(p−1)ρ(s)

(12)

Implementation of Kolmogorov Learning Algorithm

991

Figure 1 shows values of ξ(dk ) for k = 2.
4.4

Iteration Step

Let f is a known continuous function, e0 ≡ f . The r-th approximation error
function er to f is computed iteratively for r = 1, 2, . . .
er (x) = er−1 (x) −

m
�
q=0

φrq ◦ ξ(xq ),

(13)

where x ∈ I n , xq = (x1 + qβ, . . . , xn + qβ), and β = γ(γ − 1)−1 .
The r-th approximation fr to f is then given by:
fr (x) =

r �
m
�
j=1 q=0

φjq ◦ ξ(xq ).

(14)

It was shown in [11] that fr → f for r → ∞.

0.8
0.6
0.4
0.2
0
-0.2
-0.4
-0.6
-0.8

1
0.8
0.6
0.4
0.2
0
-0.2
-0.4
-0.6
-0.8
-1

0

0.1

0.2

0.3

0.4

0.5

0.6

0.7

0.8

0.9 0.1

0.2

0.3

0.4

0.5

0.6

0.7

0.8

0.9
0

0.1 0.2
0.3 0.4
0.5 0.6
0.7 0.8
0.9

1 0

1
0.9
0.8
0.7
0.6
0.5
0.4
0.3
0.2
0.1

Fig. 2. Approximation of sin(6.28x) · sin(6.28y) for k = 1 and k = 2.

5

Time Complexity

In the following time complexity analysis we consider our practical implementation of the above described algorithm, which introduces few further simpliﬁcations. First, the inﬁnite sums are replaced by sums up to the suﬃciently big
constant K. We also keep the number k(r) ﬁxed to constant k during the iteration loop. Also note that all computations of f are performed on numbers taken
from the dense support set Q.
The time complexity is derived with respect to the following basic operations: For time of the ﬂoating point multiplication and division the symbol t m
is used, while for the time of ﬂoating point addition and subtraction t a is used.

992

R. Neruda, A. Štědrý, and J. Drkošová

In general, we ﬁrst provide ﬁne-grain analysis in the following lemmas, which
is then summarized in the theorem 5 as an estimation of the total amount of
computational time for one iteration step.
The following lemma summarizes the time needed for computing several
terms that are independent of the iteration loop and can be performed in advance.
Lemma 1. The times Tλ , Tb , Tγ , Te0 needed for computations of λp , bk , γ ρ(k+1) ,
e0 are expressed by the following formulae:
1.
2.
3.
4.

Tλ = (2 + K)ta + [p + 2 + 2K + (nK − 1)/(n − 1)]tm ,
Tb = n × Tλ + [2(K − k) + (nK − nk )/(n − 1)]tm + (K − k)ta ,
Tγ = (k + 1 + nk )tm ,
Te0 = γ nk × (2ktm + (k − 1)ta ).

Proof. It is easy to derive these results by examining the corresponding deﬁnitions.
�∞
1. We compute λp = s=1 γ −(p−1)ρ(s) . The value γ 1−p is computed and saved.
If we suppose that ρ(s) is computed using ρ(s − 1) then it costs 2tm + 1ta .
Any entry in the sum consumes (1 + (ρ(s) − ρ(s − 1))tm . The total number
�K
2)tm�
+ s=1 [(2 + ns−1 )tm + ta ].
of operations is 2t
�a∞+ (p +−ρ(s)
n
2. The term bk = s=k+1 γ
p=1 λp . To obtain the entries of the outer
�K
sum consumes s=k+1 [(2 + ns−1 )tm + ta ] and the time for λp .
3. Obvious.
4. The initial error function e0 is set to the function f , while only values f (dk )
are needed. It means that the values of dk must be expressed in the form
suitable as an input of f (decadic form). Since they are originally stored via
their ordinal values, we need (2ktm + (k − 1)ta ) computations for any dk .
In order to compute time TS of one (serial) iteration of the algorithm, we
need to express times for partial steps that are thoroughly described in section 4.
The most inner terms in the computation are numbers dqk that are copmuted by
�k
means of their coordinates dqkp = dkp + q s=1 γ −s .
Lemma 2. Time Tdqk for computing dqk is Tdqk = (k + n)ta + (k + 1)tm .
Proof. To compute the sum costs kta +ktm . Then, tm +nta operations are needed
to complete the computation.
Quite complicated algorithm for computing Ψ is described in 4.2. Its complexity is estimated as follows.
Lemma 3. For any coordinate dkp the function Ψ requires TΨ = (1/6k 3 + 3k 2 +
10k + 1)ta + [(nk − 1)/(n − 1) + (k 2 + k)/2 + 1]tm .

Implementation of Kolmogorov Learning Algorithm

993

Proof. The deﬁnition of ms (cf. 8) plays the key role in computation of Ψ (dqkp ).
The upper bound for the expression of this value is 1/2(s2 + 3s + 4)ta + sta .
To simplify the estimation we consider the worst case s − ms = s as an input
into ρ(s − ms ). This computation then requires 2ta + stm . To estimate the time
consumed to express the function Ψ (dkp ) means to add the entries 1/2(s2 + 5s +
14)ta + (s + (ns − 1)/(n − 1))tm together.
The time Tξ needed for function ξ(dqk ) is straightforwardly computed by
means of the three above mentioned quantities.
�n
Lemma 4. Function ξ(dqk ) = p=1 λp Ψ (dqkp ) consumes for any dqk the amount
Tξ = n × (Tλ + TΨ + Tdqk )
Next we analyze the time Tθ necessary to compute θ.
Lemma 5. To evaluate the function θ costs Tθ = [4K − k + 6 + nk + (2nK −
nk )/(n − 1) + n]tm + [2K − k + 7]ta + Tσ .
Proof. θ(dqk , yq ) = σ(γ ρ(k+1) (yq − ξ(dqk )) + 1) − σ(γ ρ(k+1) (yq − ξ(dqk )) − (γ − 2)bk ))
and according to our assumptions yq = ξ(dqk ). The time Tθ is then the sum
Tγ + Tb + Tσ + 5ta + 3tm . Using lemma 1 we directly obtain the result.
Lemma 6. Time needed to compute the value of the outer function Φ for one
particular dqk is the following. TΦ = [n2 + nk + 3n + 7 − k + (2n + 4)K + (nK (n +
2) − nk − n)/(n − 1)]tm + [(2 + n)K + 2n − k + 7]ta + TΨ + Tdqk + Tσ
Proof. TΦ = Tξ + Tθ . Separate the operations that are computed just once and
do not depend on the iteration loop and use lemmas 4 and 5, which gives the
estimation.
Lemma 7. The time for evaluation of the error function er in one particular
dk is Te = (m + 1)ta .
Proof. If we assume that the values of er−1 (dk ) are saved and Φrq have been
already computed, then the computation of er (dk ) costs (m + 1)ta .
Lemma 8. To generate the r − th approximation to the function f in one particular value consumes Tf = mta .
Proof. The values of fr are recurrently computed by means previous iteration
fr−1 and already computed Φrq , thus only m additions is needed.
Our analysis of the serial case is summarized in the following theorem.
Theorem 5. The computational requirements in one iteration of the sequential
algorithm is estimated as follows.
�
�
TS = O m n γ nk (nk + k 3 ) .
(15)

994

R. Neruda, A. Štědrý, and J. Drkošová

Proof. Express TS according to the algorithm description:
TS = γ nk [mTΦ + Te + Tf ] .
The partial terms used have been speciﬁed by the preceeding lemmas. We do
not take into account terms that can be pre-computed (cf. Lemma 1). Also, a
particular sigmoidal function — step sigmoid — is considered, which can easily
be computed by means of two comparisons. We have sacriﬁced the tightness of
the bound to tractability of the result, and consider ta = tm (cf. Discussion).
�
�
TS ≈ m n γ nk k 3 ta + (nk + k 2 )tm ,
which proves (15).

6

Discussion

The overall time of the serial algorithm consists of initial computations of the
constant terms which take Tλ + Tb + Tγ + Te0 + Tλ and r × TS . The assumption
that the additive and multiplicative operations take approximately the same
time is not a strong simpliﬁcation if one considers current microporcessors such
as Pentiums. Note also that the complexity analysis considers quite optimal
sequential realization of the algorithm described in section 4. In particular, some
formulae have been reformulated for speedup. Also, the mesh Q is represented in
a way which allows eﬃcient access, and we cache once computed values wherever
possible.
From the point of view of parallel implementation, there is one straightforward approach employing m + 1 processors for computations of Φr0 , . . . , Φrm ,
which are mutually independent. This can reduce the TS by a factor of m on
one hand, but it requires additional communication to complete each iteration
by exchanging the computed values. Although a ﬁner analysis of this aproach
has not been performed yet, our ﬁrst experiments show that the communication requirements are tiny compared to the total computation time, especially
for problems of higher dimension. Our parallel implementation has been done
on a cluster of workstations in the PVM environment. Figure 3 shows a typical behaviour of the program running on six machines for two iterations of the
algorithm.
In the future work we plan to focus on the exact analysis of the parallel
implementation, and on employing other means of parallelization, such as clever
partitioning of the input space.

References
1. F. Girosi and T. Poggio. Representation properties of networks: Kolmogorov’s
theorem is irrelevant. Neural Computation, 1:461–465, 1989.
2. Robert Hecht-Nielsen. Kolmogorov’s mapping neural network existence theorem.
In Procceedings of the International Conference on Neural Networks, pages 11–14,
New York, 1987. IEEE Press.

Implementation of Kolmogorov Learning Algorithm

995

Fig. 3. Illustration of a typical parallel behaviour of the algorithm running on cluster
of Pentium workstations under PVM.
3. Věra Kůrková. Kolmogorov’s theorem is relevant. Neural Computation, 3, 1991.
4. Věra Kůrková. Kolmogorov’s theorem and multilayer neural networks. Neural
Networks, 5:501–506, 1992.
5. A. N. Kolmogorov. On the representation of continuous function of many variables
by superpositions of continuous functions of one variable and addition. Doklady
Akademii Nauk USSR, 114(5):953–956, 1957.
6. M. Leshno, V. Lin, A. Pinkus, and S. Shocken. Multilayer feedforward networks
with a non-polynomial activation function can approximate any function. Neural
Networks, (6):861–867, 1993.
7. G.G Lorentz. Approximation of functions. Halt, Reinhart and Winston, New York,
1966.
8. David A. Sprecher. On the structure of continuous functions of several variables.
Transactions of the American Mathematical Society, 115:340–355, 1965.
9. David A. Sprecher. A universal mapping for Kolmogorov’s superposition theorem.
Neural Networks, 6:1089–1094, 1993.
10. David A. Sprecher. A numerical construction of a universal function for Kolmogorov’s superpositions. Neural Network World, 7(4):711–718, 1996.
11. David A. Sprecher. A numerical implementation of Kolmogorov’s superpositions
II. Neural Networks, 10(3):447–457, 1997.

