Procedia Computer Science
Volume 51, 2015, Pages 865‚Äì874
ICCS 2015 International Conference On Computational Science

Local Tuning in Nested Scheme of Global
Optimization
Victor Gergel, Vladimir Grishagin and Ruslan Israfilov
Lobachevsky State University of Nizhni Novgorod, Nizhni Novgorod, Russian Federation.
gergel@unn.ru, vagris@unn.ru, ruslan@israfilov.com

Abstract
Numerical methods for global optimization of the multidimensional multiextremal functions in the
framework of the approach oriented at dimensionality reduction by means of the nested optimization
scheme are considered. This scheme reduces initial multidimensional problem to a set of univariate
subproblems connected recursively. That enables to apply efficient univariate algorithms for solving
the multidimensional problems. The nested optimization scheme served as the source of many
methods for optimization of Lipschitzian function. However, in all of them there is the problem of
estimating the Lipschitz constant as the parameter of the function optimized and, as a consequence, of
tuning to it the optimization method. In the methods proposed earlier, as a rule, a global estimate
(related to whole search domain) is used whereas local Lipschitz constants in some subdomains can
differ significantly from the global constant. It can slow down the optimization process considerably.
To overcome this drawback in the article the finer estimates of a priori unknown Lipschitz constants
taking into account local properties of the objective function are considered and used in the nested
optimization scheme. The results of numerical experiments presented demonstrate the advantages of
methods with mixed (local and global) estimates of Lipschitz constants in comparison with the use of
the global ones only.
Keywords: Global Lipschitz optimization, nested optimization scheme, adaptive tuning, speed-up of convergence

1 Introduction
The multidimensional global optimization problem over the domain
(1)
( y ¬è R N : ai d yi d bi ,1 d i d N}
*
can be defined as a problem of finding the global minimizer y ¬èD of a real function f(y) such that

D

Selection and peer-review under responsibility of the ScientiÔ¨Åc Programme Committee of ICCS 2015
c The Authors. Published by Elsevier B.V.

doi:10.1016/j.procs.2015.05.216

865

Local Tuning in Nested Scheme of Global Optimization

V. Gergel, V. Grishagin and R. IsraÔ¨Ålov

(2)
f ( y* ) min{ f ( y) : y ¬è D} ,
where values ai , bi ,1‚â§i‚â§N, are constants, y=(y1 , ‚Ä¶ , yN) is vector of function arguments from the Ndimensional Euclidean space RN.
The function f(y) is supposed to satisfy the Lipschitz condition
(3)
f ( yc)  f ( ycc) d L yc  ycc , yc, ycc ¬è D ,
where L>0 is the Lipschitz constant and * denotes the Euclidean norm in RN.
The Lipschitzian optimization problem (1)-(3) has been investigated by many researchers having
used different approaches for designing global search algorithms (see, e.g., numerous references
given in monographs (T√∂rn, A., ≈Ωilinskas, A., 1989), (Horst, R., Tuy, H., 1990), (Zhigljavsky, A.A.,
1991), (Pint√©r, J.D., 1996), (Strongin, R.G., Sergeyev, Ya.D. , 2000) ).
For the problem (2) under condition (3) the dimensionality is the crucial factor because the
problem complexity increases significantly when growing dimensionality and development of
efficient methods for solving the multidimensional multiextremal problems is very complicated. In
this way the idea of dimensionality reduction is widely used when the initial multivariate problem is
reduced to the univariate one or to a family of one-dimensional problems.
The first approach to reduction of dimensionality is based on employing the Peano curves ( (Butz,
A.R., 1968), (Goertzel, B., 1999), (Sergeyev, Ya. D., Strongin, R. G., Lera, D., 2013)) continuously
mapping the unit interval [0, 1] on the x-axis onto the hyperparallelepiped (1). Let y(x) be a Peano
curve then because of continuity of the function f(y) for global minimizer y*
(4)
f ( y* ) f ( y( x* )) min{ f ( y( x)) : x ¬è [0,1]} .
As a consequence, instead of the multidimensional problem (2) one can solve the equivalent
univariate problem (4) where the objective function f(y(x)) depends on single argument x only. In the
framework of this approach many multidimensional global search algorithms have been worked out
(Sergeyev, Y.D., Grishagin V.A., 1994), (Strongin, R.G., Sergeyev, Ya.D. , 2000), (Gergel, V.P.,
Strongin, R.G., 2003), (Hime, A. E., Oliveira Jr., H.A., Petraglia, A., 2011), (Sergeyev, Ya. D.,
Strongin, R. G., Lera, D., 2013)).
The next approach uses a partitioning of the feasible domain (1) into several subdomains (as a rule,
rectangular ones) and applies the decision rules of one-dimensional methods for the choice of a
subdomain being most perspective for continuing the optimization. The algorithms based on this
approach are presented in (Jones, D. R., Perttunen, C. D., Stuckman, B. E., 1993) (Pint√©r, J.D., 1996),
(Sergeyev, Y.D., 2000), (Kvasov, D.E., Sergeyev, Ya.D., 2003), (Kvasov, D.E., Sergeyev, Y.D.,
2012).
The last approach which can be mentioned is connected with the well-known nested scheme of
dimensionality reduction. According to this scheme the initial multidimensional problem (2) is
reduced to a set of univariate subproblems connected recursively. This nested optimization scheme
allows to solve the problem (2) via solving several one-dimensional problems applying the efficient
univariate optimization algorithms. The combination of the nested optimization scheme with different
one-dimensional global search methods has served the source of many multidimensional optimization
methods (Piyavskij, S.A., 1972), (Grishagin, V.A., Strongin, R.G., 1984), (Shi, L., √ìlafsson, S., 2000)
(Sergeyev, Y.D., Grishagin, V.A., 2001), (Dam, E.R., Husslage, B., Hertog, D., 2010), (Barkalov,
K.A., Gergel, V.P., 2014). This approach is promising for applications (see, for instance, (Bastrakov,
S., Meyerov, I., Gergel, V., Gonoskov, A., Gorshkov, A., Efimenko, E., Ivanchenko, M., Kirillin, M.,
Malova, A., Osipov, G., Petrov, V., Surmin, I., Vildemanov, A., 2013) ).
Let us briefly describe the main structure of the nested optimization scheme. It is based on the
known relation (see, e.g., (Carr, C.R., Howe, C.W. , 1964) and (Piyavskij, S.A., 1972))
(5)
min{ f ( y) : y ¬è D} min ‚Ä¶ min f ( y1 ,, y N ) .
y1 ¬è>a1 ,b1 @

Let us introduce vectors

866

y N ¬è>a N ,bN @

Local Tuning in Nested Scheme of Global Optimization

V. Gergel, V. Grishagin and R. IsraÔ¨Ålov

ui ( y1 ,, yi ), 1 d i d N ,
and a family of functions defined recursively as
f N (uN ) f ( y) ,
i

f (ui )

min{ f

i 1

(ui1 ) : yi1 ¬è [ai1 , bi1 ]} .

(6)
(7)
(8)

As a result of (5), in order to solve the problem (2) it is necessary to find the global minimum of
the one-dimensional problem
(9)
min{ f 1 ( y1 ) : y1 ¬è [a1 , b1 ]} .
1

However, each evaluation of f (y1) at a fixed point y1 generates the problem

min{ f 2 ( y1 , y2 ) : y2 ¬è [a2 , b2 ]}

(10)
being univariate again because y1 is fixed and so on up to solving the one-dimensional problem
(11)
min{ f N (uN 1 , yN ) : yN ¬è [aN , bN ]} = min{ f ( y) : y N ¬è [aN , bN ]}
with fixed vector uN-1 generated by subproblems of preceding levels of recursion.
In general case, it is required to solve a set of nested one-dimensional problems
(12)
min{ f i (ui1, yi ) : yi ¬è[ai , bi ]}, 1 d i d N .
As it is shown in (Strongin, R.G., 1978), subject to the Lipschitz condition (3) in the problem (2)
i
the univariate functions f (ui-1,yi) are Lipschitzian as well with the same Lipschitz constant L. So, for
solving problems (12) the efficient methods of univariate Lipschitzian optimization can be applied.

2 Estimates of Lipschitz constants
For optimization of a Lipschitzian function it is necessary to know the value of its Lipschitz
constant which can be considered as a parameter of the function. The methods either require strict
value of the constant or use an adaptive scheme of its evaluation. In any case, an estimate of Lipschitz
constant is included into computational scheme of the method as its parameter. The most of methods
use an estimate of the ‚Äúglobal‚Äù Lipschitz constant L from (3) related to whole search region. However,
the behavior and properties of the function over subdomains of the search domain can differ
significantly. As a result, in subdomains where the local Lipschitz constant is less considerably than
global one the method can generate redundant trials (points of iterations) and worsen the search
efficiency (Sergeyev, Ya.D., 1995), (Gergel, V.P., 1996) (Gergel, V.P., 1997). As a hypothesis, one
can suppose that additional information about local properties of the function to be minimized can
accelerate the optimum search.
So far all known methods combining the nested optimization scheme and one-dimensional
algorithms of Lipschitzian optimization were used global estimations of the Lipschitz constant. In this
article it is suggested as a novelty to apply within the nested optimization scheme the univariate
methods taking into account the local behavior of functions (12).
For solving one-dimensional problems (12) the information-statistical global search algorithm
(Strongin, R.G., 1973), (Strongin, R.G., Sergeyev, Ya.D. , 2000) will be used with two different
techniques for adaptive tuning of the method parameter being an estimate of the Lipschitz constant. In
initial version of the algorithm this parameter evaluates the global Lipschitz constant, but in our
research the estimates combine both local and global ones. The first technique has been proposed in
(Sergeyev, Ya.D., 1995), the second one is a linear convolution of local and global estimates.
Let us reformulate the univariate problems (12) in a standard form
(13)
min{M ( x) : x ¬è [a, b]} .
In general case, solving the problem (13) a numerical method of optimization generates in the
1 2
k
domain [a,b] points x1, x2, ‚Ä¶ , xk, ‚Ä¶ and evaluates at these points function values z , z , ‚Ä¶ , z , ‚Ä¶ ,

867

Local Tuning in Nested Scheme of Global Optimization

V. Gergel, V. Grishagin and R. IsraÔ¨Ålov

where zi=œÜ(xi), i ‚â•1, up to fulfillment of a search termination condition. We shall refer to the sequence
{xi}=x1, x2, ‚Ä¶ , xk, ‚Ä¶ as the sequence of trials and to the sequence {zi}=z1, z2, ‚Ä¶ , zk, ‚Ä¶ as the
sequence of trial results.
Let us describe a general computational scheme of the methods to be investigated and afterwards
specify their peculiarities.
The first two trials are to be carried out at the end points of the interval [a,b], i.e., x1=a, x2=b.
k+1
The choice of the point x , k‚â•2, of any subsequent (k+1) -th trial is determined by the following
rule consisting of 4 steps:
Step 1. Renumber by subscripts the points x1, x2, ‚Ä¶ , xk of previous trials in increasing order, i.e.,
(14)
a x0  x1  xk 1 b
and juxtapose to the renumbered points xi, 0‚â§ i ‚â§k-1, the values zi=œÜ(xi) which have been already
obtained earlier as the results of previous trials.
Step 2. For each interval (xi-1, xi) , 1‚â§ i ‚â§ k-1, compute the value
( z  z )2
(15)
R(i) m( xi  xi1 )  i i1  2( zi  zi1 ), 1 d i  k  1,
m( xi  xi1 )
called the characteristic of the interval.
Step 3. Select the interval (xt-1, xt) such that
(16)
R(t ) max{ R(i) : 1 d i d k  1} ,
i.e., the interval with the maximal characteristic. In the case when there exist several intervals
satisfying (16) the interval with the minimal number t is taken for certainty.
Step 4. Accept
xt  xt 1 zt  zt 1
(17)
x k 1

, 1  t  k  1,
2
2m
as the point of the next trial and compute the value zk+1=œÜ(xk+1).
The algorithm contains the parameter m which presents an estimate of the Lipschitz constant for
the function optimized. For example, if the global Lipschitz constant L is known a priori the parameter
can be chosen in accordance with the inequality m>L. However, in the typical case the researcher
does not know the value L. Then, one can evaluate the Lipschitz constant approximately using a
posteriori information about objective function obtained in the course of optimization, i.e., trial points
and trial results.
In the algorithm (Strongin, R.G., 1973) which are the basis of the scheme (14)-(17) the maximal
absolute value of the first divided differences
z z
(18)
M max i i1
1idk 1 x  x
i
i 1
is used as the adaptive estimate of the global Lipschitz constant. The value M is the underestimation of
the Lipschitz constant, therefore it multiplies by the factor r>1 called the reliability parameter in order
to increase the estimate and the parameter m in (15), (17) is taken as
¬≠rM , M ! 0 .
(19)
m ¬Æ
¬Ø 1, M 0
Let us call, following the author (Strongin, R.G., 1973), the method (14)-(17) with the estimate m
from (18), (19) as Algorithm of Global Search (AGS).
As it has been shown in (Sergeyev, Ya.D., 1995) and (Strongin, R.G., Sergeyev, Ya.D. , 2000) in
the situation when in a neighborhood of the global minimizer the local Lipschitz constant is less
significantly than the global Lipschitz constant AGS places trials in this neighborhood with high
density and slows down the search very much. One of the approaches to overcome this difficulty
consists in stopping the global method and continuing the optimization with a local optimization
method (Horst, R., Tuy, H., 1990), (Horst, R., Pardalos, P.M., 1995). Unfortunately it is difficult to

868

Local Tuning in Nested Scheme of Global Optimization

V. Gergel, V. Grishagin and R. IsraÔ¨Ålov

determine the moment of switching from the global method to the local one: too early switching can
lead to the loss of the global minimum and a late one keeps slowing down.
We consider another approach which permits to take into account the both global and local
properties of the objective function within the unified scheme (14)-(17) of the global optimization
method. This approach is connected with the additional estimates of Lipschitz constants in the
subdomains of the search domain and balancing the global and local constants.
Two schemes of tuning estimates and balancing will be used. The first one has been proposed by
Sergeyev (Sergeyev, Ya.D., 1995) and consists in the following.
Let us return to the ordered set of trials (14) and juxtapose to the interval (xi-1, xi) , 1‚â§ i ‚â§ k-1, the
value
(20)
mi rM i
where

max{ Pic, Picc , P0 } ,

(21)

Pic max{ z j  z j1 /( x j  x j1 ) : j i  1, i, i  1} ,

(22)

Mi

Picc M ( xi  xi1 ) / d max ,
d max

max ( xi  xi1 ) ,

1idk 1

(23)
(24)

M from (18) and r >1.
When i=1 or i=k-1 to calculate P ic only the numbers j=i, i+1 or j=i-1, i are considered
correspondingly.
In this scheme mi is an estimate of local Lipschitz constant over the interval (xi-1, xi). It consists of
three parts. P ic reflects the local behavior of the function, Picc accounts for global one, Œº0 >0 is chosen
small to prevent the case of the function being constant over [a, b].
Now to refine on the method (14)-(17) we replace the overall global estimate m from (18), (19) in
the characteristic (15) of the interval (xi-1, xi) with the value mi from (20) and in the expression (17)
with the value mt . The expression (21) can be considered as the convolution of estimates P ic , Picc and

Œº0 by the use of the operation ‚Äúmaximum‚Äù, and this version of the algorithm will be denoted as AGSMC (Algorithm of Global Search with Maximum Convolution).
Another way to construct a specific estimate for each interval (xi-1, xi) proposed here consists in
applying the linear (additive) convolution of local and global estimates. Namely, in the scheme (20)(24) the value Mi from (21) is calculated as
(25)
M i max{ Pic / r  (r  1)Picc/ r , P0 }
and again in (15) the local estimate mi is used instead of m and the value mt substitutes for m in (17).
This variant of the optimization method will be referred to as Algorithm of Global Search with
Additive Convolution (AGS-AC) because P ic , Picc are combined by means of additive convolution
with coefficients 1/r and (r-1)/r correspondingly.
AGS-MC and AGS-AC use estimates of local Lipschitz constants of objective function and
thereby provide (following the terminology of (Sergeyev, Ya.D., 1995)) local tuning of optimization.
All three methods considered belong to the class of characteristical algorithms (Grishagin, V.A.,
1979), (Grishagin, V.A., Sergeyev, Y.D., Strongin, R.G., 1997) therefore we can use the termination
criterion in the form
xt-1 - xt < Œµ
(26)
where Œµ > 0 is the accuracy of search and t from (16), i.e., the process of optimization stops when the
length of the interval with maximal characteristic becomes less than preassigned accuracy Œµ.

869

Local Tuning in Nested Scheme of Global Optimization

V. Gergel, V. Grishagin and R. IsraÔ¨Ålov

The univariate methods AGS, AGS-MC, AGS-AC being characteristical algorithms satisfy the
general convergence conditions of the characteristical class. Partial proofs of their convergence can be
found in (Strongin, R.G., Sergeyev, Ya.D. , 2000). The substantiation of convergence for the nested
optimization scheme using these methods for solving the internal univariate problems is based on the
theoretical results (Strongin, R.G., 1978). In the next section the numerical experiments demonstrate
comparative efficiency of the methods while optimizing significantly multiextremal multidimensional
functions.

3 Computational experiments
For the first series of experiments let us take the known class of test two-dimensional functions
(Grishagin, V.A., 1978)
1

2
2
¬≠¬∞¬ß 7 7
¬∑ ¬Ω¬∞ 2
¬∑ ¬ß 7 7
f ( y1 , y2 ) ¬Æ¬®¬® ¬¶¬¶ Aij aij ( y1 , y2 )  Bij bij ( y1 , y2 ) ¬∏¬∏  ¬®¬® ¬¶¬¶ Cij aij ( y1 , y2 )  Dij bij ( y1 , y2 ) ¬∏¬∏ ¬æ
¬∞¬Ø¬© i 1 j 1
¬π ¬∞¬ø
¬π ¬©i1 j1

>

where aij ( y1 , y2 )

sin(Siy1 ) sin(Sjy 2 ) , bij ( y1 , y2 )

@

>

@

(27)

cos(Siy1 ) cos(Sjy2 ) and the functions (27) are

minimized in the domain 0 d y1 , y2 d 1 . The parameters Aij , Bij , Cij , Dij are the independent random
numbers, distributed uniformly over the interval [-1, 1].
Figure 1 shows the level curves of a function (27) and distribution of the trials executed in the
course of optimization by the nested optimization scheme for two univariate methods: with local
tuning (AGS-AC) and without it (AGS) (the trials are marked with dark dots).

Figure 1: Additive convolution (25) (the left panel) and global estimate (18) (the right panel)

Moreover, this function was minimized by the method with maximum convolution inside of the
nested scheme (AGS-MC). For all methods the reliability parameter r=2.5, the termination criterion
(26) uses Œµ=0.001. In all cases the required accuracy of the problem solution has been achieved.
However, the method AGS-AC with additive convolution has carried out 563 search trials, AGS-MC
has implemented 1357 trials whereas AGS has computed 2955 objective function values. The most

870

Local Tuning in Nested Scheme of Global Optimization

V. Gergel, V. Grishagin and R. IsraÔ¨Ålov

trials are concentrated in a neighborhood of the global minimum for all the methods but AGS-AC has
provided the least density of trials in the search region and AGS has generated the densest location of
trials (especially in the vicinity of the global minimizer).
To get more convincing results for comparison of the optimization algorithms let us apply the
method of operating characteristics (Grishagin, V.A., 1978), (Strongin, R.G., Sergeyev, Ya.D. , 2000).
Following this method a minimized function is supposed to belong to a class with a given probability
measure and to be chosen randomly in accordance with this probabilistic distribution. After the
random choice of n functions and minimizing them by an algorithm with given set of its parameters it
is possible to get the pair (K, P), where K is the average number of trials executed by the algorithm
and P is the number of problems solved successfully. Repeating the minimization of the functions
chosen with different parameters one can obtain several pairs (K, P). The set of these pairs is called the
operating characteristic of the algorithm.
Taking for comparison two algorithms we can plot their operating characteristics on the plane in
the axes (K, P). If for the same K the operating characteristic of a method is placed above the
characteristic of another one, the first method provides better reliability i.e., the probability of the
proper solution of the problem with the same computational expenditures.
Figure 2 contains the operating characteristics of AGS (circles), AGS-MC (squares), AGS-AC
(triangles) obtained after minimization of 100 random functions (27) with r = 3.2 and different values
of accuracy Œµ from (26). Additionally, for comparison with an algorithm based on the approach
differing from nested optimization scheme the well-known method DIRECT (Jones, D. R., Perttunen,
C. D., Stuckman, B. E., 1993) has been taken. Its operating characteristic is marked with stars. The
axis K is presented in the logarithmic scale.

Figure 2: Operating characteristics of the methods compared

871

Local Tuning in Nested Scheme of Global Optimization

V. Gergel, V. Grishagin and R. IsraÔ¨Ålov

The results demonstrate the effect of the search speed-up when using the tool of local tuning and
the significant advantage of the method with additive convolution among the considered methods
based on nested optimization. As for comparison with DIRECT, this method is more efficient when
the number of trials is not too great (and, as a consequence, the reliability is low), but the methods
using the nested scheme provide the high reliability faster than DIRECT.
Another experiment consists in minimization of 4-dimensional functions of the class GKLS
(Gaviano, M., Kvasov D.E., Lera D., Sergeyev Ya.D., 2003) being widespread for testing the global
search methods. In the experiment 20 functions were minimized. For all the test functions the number
of local minima is equal to 10 and domain of attraction for global minimizer is very small: 0.07% of
the search region. The methods compared used accuracy (26) Œµ =0.01 and parameter r=3. The global
minima have been found with predefined accuracy in all the cases. The average numbers of trials
(computed values of the objective function) are presented in Table 1.

Method

AGS

AGS-MC

AGS-AC

Number of trials

64959

37459

15952

Table 1: Average number of trials for the GKLS functions

In accordance with the experiment, the schemes with local tuning (AGS-MC and AGS-AC) are
more efficient than method using the global estimate of Lipschitz constant only (AGS) and the method
AGS-AC demonstrates the best results.

4 Conclusion
The nested optimization scheme of dimensionality reduction aimed at solving the multiextremal
functions satisfying Lipschitz condition has been considered. For increasing the computational
efficiency of the nested scheme a new approach connected with tuning the global optimization to local
behavior of the objective function within the unified computational procedure has been proposed. Two
manners of taking into account local properties of the objective function in the course of solving the
univariate internal subproblems of the nested scheme are used. These manners apply two different
convolutions of local and global estimates of Lipschitz constant. Owing to balancing the local and
global information the significant acceleration of the search is achieved if the high accuracy of
problem solution is required. Computational experiments on two widely used test classes of
multiextremal multidimensional functions have been carried out for comparison the efficiency of the
methods considered and the well-known global optimization method DIRECT. The experiments
confirm the efficiency of the local tuning scheme with additive convolution.
As a further way of the development of the approach it is very interesting to built-in local tuning
into the index method of constrained optimization and especially to expand it into parallel
computations using, for example, ideas of parallelizing the characteristical algorithms and the nested
optimization scheme.

872

Local Tuning in Nested Scheme of Global Optimization

V. Gergel, V. Grishagin and R. IsraÔ¨Ålov

References
Barkalov, K.A., Gergel, V.P. (2014). Multilevel scheme of dimensionality reduction for parallel global
search algorithms. Proceedings of the 1st International Conference on Engineering and
Applied Sciences Optimization , (pp. 2111‚Äì2124). Kos Island, Greece.
Bastrakov, S., Meyerov, I., Gergel, V., Gonoskov, A., Gorshkov, A., Efimenko, E., Ivanchenko, M.,
Kirillin, M., Malova, A., Osipov, G., Petrov, V., Surmin, I., Vildemanov, A. (2013). High
Performance Computing in Biomedical Applications. Procedia Computer Science, 18, 10-19.
Butz, A.R. (1968). Space-Filling Curves and Mathematical Programming. Inform. Control, 12(4),
314‚Äì330.
Carr, C.R., Howe, C.W. . (1964). Quantitative Decision Procedures in Management and Economic:
Deterministic Theory and Applications. New York: McGraw‚ÄìHill.
Dam, E.R., Husslage, B., Hertog, D. (2010). One-dimensional Nested Maximin Designs. Journal of
Global Optimization, 46(2), 287‚Äì306.
Gaviano, M., Kvasov D.E., Lera D., Sergeyev Ya.D. (2003). Software for generation of classes of test
functions with known local and global minima for global optimization. ACM TOMS, 29(4),
469‚Äì480.
Gergel, V.P. (1996). A method of using derivatives in the minimization of multiextremum functions.
Computational Mathematics and Mathematical Physics, 36(6), 729-742.
Gergel, V.P. (1997). A Global Optimization Algorithm for Multivariate Function with Lipschitzian
First Derivatives. Journal of Global Optimization, 10(3), 257‚Äì281.
Gergel, V.P., Strongin, R.G. (2003). Parallel Computing for Globally Optimal Decision Making.
Parallel Computing Technologies / Lecture Notes in Computer Science, 2763, pp. 76-88.
Goertzel, B. (1999). Global Optimization with Space-Filling Curves. Applied Mathematics Letters,
12(8), 133-135.
Grishagin, V.A. (1978). Operating Characteristics of Some Global Search Algorithms. Problems of
Statistical Optimization, 7, 198‚Äì206 (In Russian).
Grishagin, V.A. (1979). On convergence conditions for a class of global search algorithm. Proc. of the
3-rd All-Union Seminar "Numerical methods in nonlinear programming", (pp. 82-84).
Kharkov (In Russian).
Grishagin, V.A., Sergeyev, Y.D., Strongin, R.G. (1997). Parallel Characteristical Algorithms for
Solving Problems of Global Optimization. Journal of Global Optimization, 10(2), 185‚Äì206.
Grishagin, V.A., Strongin, R.G. (1984). Optimization of multiextremal functions subject to
monotonically unimodal constraints. Engineering Cybernetics, 5, 117-122.
Hime, A. E., Oliveira Jr., H.A., Petraglia, A. (2011). Global Optimization Using Space-Filling Curves
and Measure-Preserving Transformations. Soft Computing in Industrial Applications, 96,
121-130.
Horst, R., Pardalos, P.M. (1995). Handbook of Global Optimization. Dordrecht: Kluwer Academic
Publishers.
Horst, R., Tuy, H. (1990). Global Optimization: Deterministic Approaches. Berlin: Springer-Verlag.
Jones, D. R., Perttunen, C. D., Stuckman, B. E. (1993). Lipschitzian optimization without the
Lipschitz constant. J. Optim. Theory Appl., 79, 157‚Äì181.
Kvasov, D.E., Sergeyev, Y.D. (2012). Lipschitz gradients for global optimization in a one-point-based
partitioning scheme. Journal of Computational and Applied Mathematics, 236(16), 40424054.
Kvasov, D.E., Sergeyev, Ya.D. (2003). Multidimensional Global Optimization Algorithm Based on
Adaptive Diagonal Curves. Comput. Math. Math. Phys., 43(1), 40‚Äì56.
Pint√©r, J.D. (1996). Global Optimization in Action (Continuous and Lipschitz Optimization:
Algorithms, Implementations and Applications). Dordrecht: Kluwer Academic Publishers.

873

Local Tuning in Nested Scheme of Global Optimization

V. Gergel, V. Grishagin and R. IsraÔ¨Ålov

Piyavskij, S.A. (1972). An Algorithm for Finding the Absolute Extremum of a Function. USSR
Comput. Math. Math. Phys., 12(4), 57‚Äì67.
Sergeyev, Y.D. (2000). An efficient strategy for adaptive partition of N-dimensional intervals in the
framework of diagonal algorithms. Journal of Optimization Theory and Applications, 107(1),
145‚Äì168.
Sergeyev, Y.D., Grishagin V.A. (1994). A Parallel Method for Finding the Global Minimum of
Univariate Functions. Journal of Optimization Theory and Applications, 80(3), 513-536.
Sergeyev, Y.D., Grishagin, V.A. (2001). Parallel Asynchronous Global Search and the Nested
Optimization Scheme. J. Comput. Anal. Appl., 3(2), 123‚Äì145.
Sergeyev, Ya. D., Strongin, R. G., Lera, D. (2013). Introduction to Global Optimization Exploiting
Space-Filling Curves. Springer.
Sergeyev, Ya.D. (1995). An information global optimization algorithm with local tuning. SIAM
Journal on Optimization, 5(4), 858-870.
Shi, L., √ìlafsson, S. (2000). Nested Partitions Method for Global Optimization. Operations Research,
48(3), 390-407.
Strongin, R.G. (1973). On the convergence of an algorithm for finding a global extremum.
Engineering Cybernetics, 11, 549-555.
Strongin, R.G. (1978). Numerical Methods in Multiextremal Problems (Information-Statistical
Algorithms). Moscow: Nauka (In Russian).
Strongin, R.G., Sergeyev, Ya.D. . (2000). Global Optimization with Non-convex Constraints:
Sequential and Parallel Algorithms. Dordrecht : Kluwer Academic Publishers.
T√∂rn, A., ≈Ωilinskas, A. (1989). Global Optimization. Lecture Notes in Computer Science 350. Berlin:
Springer-Verlag.
Zhigljavsky, A.A. (1991). Theory of Global Random Search. Dordrecht: Kluwer Academic Publishers.

874

