Graphical Notation for Diagramming Coupled
Systems
J. Walter Larson1,2,3
1

Mathematics and Computer Science Division, Argonne National Laboratory,
Argonne, IL 60439, USA
larson@mcs.anl.gov
2
Computation Institute, University of Chicago, Chicago, IL USA
3
Department of Computer Science, The Australian National University
Canberra ACT 0200 Australia

Abstract. Multiphysics and multiscale–or coupled–systems share one
fundamental requirement: Construction of coupling mechanisms to implement complex data exchanges between a system’s constituent models. I have created a graphical schema for describing coupling workflows
that is based on a theoretical framework for describing coupled systems. The schema combines an expanded set of traditional flowchart
symbols with pictograms representing data states. The data pictograms
include distributed mesh, field, and domain decomposition descriptors
and spatiotemporal integration and accumulation registers. Communications pictograms include: blocking- and non-blocking point-to-point and
M × N parallel data transfer; parallel data transposes; collective broadcast, scatter, gather, reduction and barrier operators. The transformation
pictograms include: intergrid interpolation; spatiotemporal integral operators for accumulation of state and flux data; and weighted merging
of output data from multiple source models for input to a destination
model. I apply the schema to simple problems illustrating real situations
in coupler design and implementation.

1

Introduction

Coupled systems are increasingly prevalent in computational science and engineering. Multiphysics models combine subsystem models to achieve higherﬁdelity simulation of the greater whole and superior solutions in their constituent
processes. Multiscale models capture spatiotemporal-scale interactions by coupling separate models that operate on disparate time and length scales. Multiphysics and multiscale models share a common requirement for data exchange
mechanisms that allow their constituent subsystems to compute their respective
states—the coupling problem (cp) [1]. In many cases, coupled systems contain
constituent subsystems possessing high levels of computational complexity that
warrant parallel processing. Coupling in distributed-memory parallel environments is particularly diﬃcult—the parallel coupling problem (pcp) [1].
Most coupled models are built by multidisciplinary teams from legacy model
codes. Coupling mechanisms—or couplers—confront coupled model designers
G. Allen et al. (Eds.): ICCS 2009, Part I, LNCS 5544, pp. 745–754, 2009.
c Springer-Verlag Berlin Heidelberg 2009

746

J.W. Larson

with complexity in choices of operation order (e.g., computation of ﬂuxes followed by interpolation, or vice versa) and algorithms (e.g., M × N data transfer), creating the potential for uncertainty or even software bugs. This raises
the question: Is there a compact way to express coupling workﬂow complexity
that allows rapid, up-front analysis of coupler design? In this paper I propose
a graphical schema for elucidating coupling workﬂows and employ it to explore
coupler design.
Symbols and symbolic diagrams are widely used in place of words to communicate ideas. Symbol sets for diagrams are available for various disciplines,
including electrical engineering, meteorology, and computer science [2]. Computational workﬂows traditionally are characterized by using ﬂowcharts [3,4] that
specify control ﬂow and processing operations in a system. Flowcharts, however,
depict processing at a high level of granularity—in some cases line by line of
code. If we are to use ﬂowcharts, it must be at a coarse level of code granularity,
but with suﬃcient detail to capture often-repeated operations such as intergrid
interpolation. Data ﬂow diagrams [5] capture the data states in various parts of
a system and how information ﬂows through a system; they do not oﬀer detail
pertaining to the transformations driving the data ﬂows. The Uniﬁed Modeling
Language (uml) [6] provides diagrams for elucidating system structure and behavior. The activity diagram depicts a series of processing steps in a system;
ovals represent processing activities and their progression is represented by connecting arrows. The state diagram depicts a series of states of a system; squashed
boxes represent system states, connected by labeled arrows denoting processing
steps or guard expressions. uml’s graphics are easy to draw, but subtle, and
it’s easy for nonspecialists such as computational scientists to make errors using
uml [7]. None of these existing solutions suﬃces to cover the complete problem
of documenting the transformations and data states of a coupling workﬂow.
I have concluded that an approach that incorporates elements of ﬂowcharts
and data ﬂow diagrams will best suit the problem at hand. My graphical schema
leverages ﬂowcharts but augments some of its well-known symbols with pictograms that represent processing activities relevant to coupling. The schema
is derived from a theoretical framework for the cp and pcp. In Section 2 I
summarize this theoretical framework, deﬁning terms for both the schema and
discussion for the remainder of this paper. In Section 3 I deﬁne the schema’s
pictograms and drawing conventions. In Section 4 I construct schematics illustrating some commonly encountered coupling mechanisms.

2

Coupling in Multiphysics and Multiscale Models

Below I deﬁne terms and provide a theoretical overview of data traﬃcking in the
cp and pcp; further details are available in [1]. A coupled system is constructed
from N interacting models—or constituents—{C1, . . . , CN }. Each model has a
spatial domain Γi plus time; intersections between spatial domains result in
overlap domains Ωij = Γi ∩ Γj . Each model’s domain boundary ∂Γi is the intersection of its overlap domains; that is, ∂Γi ≡ ∩j=i Ωij . Coupling entails data

Graphical Notation for Diagramming Coupled Systems

747

exchange between models. Denote each model’s state variables, inputs, and outputs as (Ui , Vi , Wi ). Each of these entities is a set of variables; for example, U
comprises the wind, temperature, pressure, and humidity ﬁelds for a simple atmosphere model. A model solves its equations of evolution on a spatial domain
Γ ; thus the state on the domain is a vector ﬁeld resulting from the Cartesian
product U ×Γ . A model’s inputs (outputs) is also a vector ﬁeld V ×∂Γ (W ×∂Γ ).
Models Ci and Cj are coupled if and only if at a minimum Ci provides output
to (receives input from) Cj . This requires Ωij = ∅. In some cases the data dependency relationship is immediately obvious; that is, Wi ∩ Vj = ∅ or Wj ∩ Vi = ∅.
In other cases, Vi (Vj ) is computable from Wj (Wi ) by a coupling transformation
Tij : Wj → Vi (Tji : Wi → Vj ). Thus far we have described explicit coupling.
Implicit coupling between models Ci and Cj constitutes an overlap between their
respective state variables; that is, Ui ∩ Uj = ∅. Implicit coupling requires a selfconsistent, simultaneous solution for shared-state variables using a solver Sij
(ordering of indices i and j is irrelevant).
Coupling events play a crucial role in the time evolution of multiscale and
multiphysics systems. The time signature of the data exchanged is either instantaneous or integrated; integrated data exchanges involve the delivery of time integrated (averaged) ﬂux (state) data from a source model to a destination model,
which applies integrated ﬂuxes incrementally during intervals between coupling
events. Some coupled systems employ integrated data delivery to loosen intermodel couplings; see Section 4 of [1] for further discussion. Similarly, multiscale
models may use spatially integrated data delivery to transfer information from
smaller to larger length scales.
Thus far we have discussed bipartite coupling. In principle, a constituent Ci
can receive the same input data from more than one model—for example, Cj
and Ck . In this situation, merging of data is required if there is a second-order
overlap domain Ωijk ≡ Γi ∩ Γj ∩ Γk = ∅ and Tij and Tik produce some of
the same input ﬁelds among Vi . Higher-order merges may occur on higher-order
overlap domains, for example, a k−1-way merge on the kth order overlap domain
Ωn1 ,...,nk ≡ Ωn1 ∩ · · · ∩ Ωnk for n1 = · · · = nk . Multipartite (k-way) implicit
coupling occurs on Ωn1 ,...,nk if Un1 ∩ · · · ∩ Uk = ∅ for n1 = · · · = nk ; in this case
a k-way self-consistent solver Sn1 ,...,nk is required.
Coupled system models are implemented on digital computers by using numerical analysis techniques that discretize space and time. A model’s gridpoints derive from a discretization Δi (Γi ); its boundary gridpoints are Δi (∂Γi ). The state,
inputs, and outputs of a numerical model Ci are its state vector Ui ≡ Ui ×Δi (Γi ),
input vector, Vi ≡ Vi × Δi (Γi ), and output vector Wi ≡ Wi × Δi (Γi ). Thus an
explicit coupling transformation delivering data from Cj to Ci is Tij : Wj →
Vi ; Tij will likely comprise a field variable transformation Fij that embodies
natural-law relationships between Wj and Vi and a mesh transformation Gij that
maps the same variable deﬁned on gridpoints in Δj (Ωij ) to values deﬁned on
the gridpoints in Δi (Ωij ). In principle, these operations do not commute; that
is, Gij ◦ Fij = Fij ◦ Gij . This feature is a source of coupling uncertainty and a
motivator for graphical representation of coupling workﬂows.

748

J.W. Larson

Thus far we have assumed a von Neumann uniprocessor computer architecture. Introduction of concurrency complicates the cp, leading to the pcp. In a
single address space, the complication is parallelization of the data transformations Tij and any solvers required by implicit couplings. Distributed-memory architectures introduce further complications—domain decomposition of coupling
data, parallel data transfer, and concurrency in model execution. Domain decomposition of data across a pool of K processes is accomplished by the partitioning
operator P(·); each model will have its own domain decomposition Pi (·). Thus,
for model Ci resident on a pool of Ki processes, Pi (Γi ) = {γi1 , . . . γiKi }, with γiν
Ki
1
the portion of Γi resident on the νth process; similarly, Pi (Ωij ) = {ωij
, . . . ωij
},
K
K
K
Pi (Ui ) = {u1i , . . . , ui i }, Pi (Vi ) = {vi1 , . . . , vi i }, and Pi (Wi ) = {wi1 , . . . , wi i }.
Parallelization of Tij and any implicit solvers requires parallel data transfer to
deliver data from the source model Cj to the destination model Ci ; this amounts
to adding a data movement operation Hij to the mesh and ﬁeld variable transformations. The data mover Hij is a one-way parallel data (i.e., M × N ) transfer
or a two-way parallel data redistribution (i.e., a transpose). Process composition
is the mapping the models to pools of processes or cohorts. On a uniprocessor
system, serial composition is the only option; models run in turn successively
on the single resource. Multiprocessors allow further mapping strategies: parallel
composition, in which models are mapped to nonoverlapping process pools and
execute simultaneously on their respective cohorts; hybrid composition, which
nests serial and parallel compositions to create complex process maps; and overlapping composition in which cohorts of two models partially intersect.

3

Graphical Schema Specification

The schema must enable users to capture the functionality present in coupling
workﬂows. It must make clear, at a glance, design choices in terms of parallelism,
process composition, and number of executable images. The pictograms must be
easy to draw by hand to encourage use on paper and whiteboards. The symbol
set must be suﬃciently complete to cover a wide variety of coupling functions as
outlined in Section 2. The schema must be extensible to allow users to create new
symbols for coupling functions speciﬁc to their applications. The standard color
scheme should be black and white to allow ease of sketching and to allow userdeﬁned color coding as an additional degree of freedom in diagram construction.
Figure 1 displays the symbols and line conventions in the schema. Rectangles
with rounded corners represent subsystem models. Model elements are drawn
with either solid or dashed lines depending on the model’s layout in the system’s
process composition. Ellipses represent the ﬁeld, mesh, and domain decomposition data exchanged and processed during coupling. The graphical schema inherits a number of conventions from ﬂowcharting: rectangles with sharp corners
represent processing operations; a rhombus indicates a decision point; parallelograms indicate i/o operations; a triangle represents extraction of a subset of
data from a data object; an upside-down triangle represents merging of multiple
data objects into a single object; and a circle (upside-down house) represents

Graphical Notation for Diagramming Coupled Systems

749

a continuation point to a corresponding continuation point elsewhere on the
same diagram (on another diagram). A burst symbol indicates parallel communications such as MPI point-to-point and collective operations. Directed arrows
represent processing paths. Processing paths do not cross but may touch in the
case of iterative processing. When space is tight in a diagram, one can draw
processing paths appearing to cross (e.g., Figure 5), but as with ﬂowcharts, the
crossing is not signiﬁcant. One may draw one of the paths with a “bump” in
it to represent this skewness—a convention adopted from circuit diagrams. For
parallel systems, dashed and dotted lines are employed as needed to identify processing occurring on a subset of the parent cohort; for example, diﬀering types of
lines emerging from the decision rhombus with the criterion “MyID == Root?”
distinguish processing paths for root and nonroot processes.
The semantics of the schema are deﬁned as follows. A line connecting a data
(processing) element to a processing (data) element signiﬁes an input (output)
relationship to (from) the processing element. A line connecting two processing
elements signiﬁes ﬂow of control in the direction dictated by the arrow; input
data to the destination processing element is treated separately. A data element
may serve as input to multiple processing elements and a processing element
may have multiple outputs.
Model boxes can be annotated with pertinent information such the name of
the model, as parallel or uniprocessor, parallelism mechanism, and number of
processing elements. Data symbols can be annotated with descriptive information such as model name, grid name, and list of ﬁelds. Processing boxes can be
annotated with the name of the algorithm embodied by the box; this is also
the convention for adding new processing symbols. Continuation symbols are
by deﬁnition annotated by a label indicating the continuation point; shading or
blackening a continuation symbol indicates that the rest of the system in that
direction of the workﬂow is regarded as a black box.
Multiple model boxes with dashed edges represent a serial composition (Figure 2 (a)). Multiple model boxes with solid edges represent a parallel composition
(Figure 2 (b)). A model box with thickened solid edges indicates a separate executable image; in diagrams for which no model box of this type is present, the
system is a single executable. Nesting of solid and dashed model boxes represents
hybrid compositions; dashed (solid) model boxes represent serial (parallel) composition nested within a parallel (serial) composition (Figures 2 (b) and (c)).

Model

Model

Merge

Data

Communications
Extract

Processing

A

B

Input /
Outuput

Decision

Paths

Connectors
Skew Paths

Fig. 1. Shapes of basic symbols used to diagram coupling workflows

750

J.W. Larson

Fig. 2. Graphical conventions for representing process compositions: (a) serial composition, (b) parallel composition, (c) hybrid composition with parallel compositions
embedded within a serial composition, (d) hybrid composition with serial compositions embedded within a parallel composition, (e) hybrid composition with multiple
embedding levels, (f) overlapping composition

Multiple levels of nesting represent deeper levels of hybrid composition (e.g.,
Figure 2 (e)). Intersecting boxes indicate overlapping composition (Figure 2 (f)).
Coupling relies on the description of ﬁeld and mesh data, and in the PCP
on domain decomposition of these data. Figure 3 (a) displays the pictograms
for state, input, and output vectors, discretized domains and overlap regions,
and domain decomposition objects. In some cases, it is convenient to incorporate ﬁeld data with its resident mesh and domain decomposition in a simple
object called a bundle[8], and a separate symbol is provided for this purpose.
Pictograms for time- and space-integrated data objects are included. An “accumulator” is included for use either in place of the integrated ﬁeld objects or
to represent results of global reductions (e.g., MPI REDUCE()) across a process
pool. Not all of the data pictograms used in this paper are shown in Figure
3 (a). Pictograms for state, coupling. and mesh data partitioned across a process pool are labeled by using notation from Section 2; {u, v, w, Δ(γ), Δ(ω)} in
place of {U, V, W, Δ(Γ ), Δ(Ω)}. Also, for ease of hand-drawing of schematics,
the bold-face notation on pictograms may be replaced with vector symbols; for
→

example, U or U in place of U. Integrated data may also be represented by using
∼

triangular brackets— W , w , and so on.
Figure 3 (b) displays the core set of pictograms used to depict intercomponent
data movers Hij and communications operations commonly used in couplers,
including blocking- and nonblocking point-to-point messaging and M × N transfers, parallel data transposes, broadcasts, scatters, gathers, and synchronization.
Other communications pictograms are created by annotating an empty burst
symbol—for example, a global reduction.

Graphical Notation for Diagramming Coupled Systems

U

V

W

StateFields

InputFields

OutputFields

Accumulator

Time
Integrated

Δ(Γ)

Overlap
Mesh

DomainMesh

751

Decomp

(a)
Bundle

Blocking Msg

dt

dA

dx

Nonblocking Msg

Area
Integrated

1-D Space
Integrated

Gather

Broadcast

Barrier

dV
Volume
Integrated

(b)
Nonblocking M X N

Blocking M X N

Miscellaneous
e.g., REDUCE()

Transpose

Scatter

dV
Volume Integral

dt

dx

Time Integral

dA

Space Integral(1D)

Area Integral

(c)
Interpolation

Renormalization

Weighted Merge

Fig. 3. Detailed pictograms for representing (a) data, (b) communications, and (c)
data transformation

Figure 3 (c) displays the core set of data transformation pictograms, including intergrid interpolation, temporal and spatial integration, renormalization to
enforce conservation of ﬂux integrals, and weighted merging.

4

Examples

A common coupled systems problem is intermodel data transfer. Figures 4
and 5 show the model-coupler communications patterns for versions 2 and 3
of the Community Climate System Model, respectively; see [8] for further details. Figure 4 depicts the communications between a model employing hybrid
MPI/OpenMP parallelism and a coupler that is solely OpenMP parallel but
possesses one MPI process. The models are in parallel composition, each implemented in a separate executable. Distributed output data w is gathered from the
atmosphere’s cohort to its root, yielding W. Nonroot processes wait in a barrier
until the root ﬁnishes communicating with the coupler. A single blocking MPI
message containing W is sent to the coupler, which receives it as input V. The
coupler in turn posts a single blocking message to the atmosphere and continues
processing upon completion. The atmosphere receives this message as V, and
scatters the message to its processes, yielding v. Figure 5 depicts parallelized
M × N data transfer between two separate executables, both possessing multiple MPI processes. The atmosphere sends its distributed output w in parallel to

752

J.W. Larson

the coupler. Once the parallel send has completed, the atmosphere receives its
distributed input v from the coupler. All communications are blocking overall;
that is, an MPI Waitall() is invoked to ensure that the model and coupler evolve
together.

Fig. 4. Coupling workflow between two models featuring serialized communications

Three key types of data transformations used in coupling are shown in Figure 6.
Figure 6 (a) depicts integrated data delivery processing. A model takes distributed
instantaneous output w and integrates it with respect to time, accumulating ﬂuxes
and averaging state data. This process is performed over a coupling cycle period,
and the decision box determines whether the model should continue or pause to
couple to the rest of the system. Figure 6 (b) shows a scheme for enforcing conservation of interfacial ﬂuxes under interpolation. Distributed input data v deﬁned
on Grid 1 are received and integrated across the two- dimensional boundary. The
data are then interpolated to yield input deﬁned on Grid 2; they are integrated on
this grid over the boundary. The values of v, deﬁned on Grids 1 and 2, together
with their respective integrals, are then passed through a renormalization function, which computes the ratio of the integrals obtained on Grids 1 and 2 and uses
their ratio to rescale the values of v residing on Grid 2, thus conserving global ﬂux
integrals across the boundary.

Graphical Notation for Diagramming Coupled Systems

Atmosphere MPI/OpenMP

Compute
Outputs from
State

u

753

w
OutputFields

StateFields

Blocking MxN Recv

v

Blocking MxN Send

InputFields

A

Blocking MxN Send

v

A

Blocking MxN Recv

InputFields

Compute
Outputs from
State

u
Coupler MPI

StateFields

w
OutputFields

Fig. 5. Coupling workflow between two models featuring M × N transfer

(a)

u
StateFields

Compute
Outputs from
State

dt

w
OutputFields

<w>
OutputFields

Time Integral

Time to
Couple?

YES
NO
Coupling
Operations

v

v

Grid 1

Grid 2
Interpolation

dA

<v>

Area Integral

Grid 2

(b)
dA

<v>

Area Integral

Grid 1

v

Source 1
Grid 1

v'
Grid 2
Renormalization

v

Interpolation

Source 1
Grid 3

v

(c)

InputFields
Weighted Merge

v

Source 2
Grid 2

v

Interpolation

Source 2
Grid 3

Fig. 6. Coupling workflows for (a) time-integrated data delivery, (b) flux-conserving
interpolation, and (c) merging of inputs

754

5

J.W. Larson

Conclusions

I have proposed a graphical schema for describing intermodel data coupling
that combines control and data ﬂow. The schema is derived from a theoretical
framework for describing intermodel coupling. The schema is rich enough to
describe a wide variety of intermodel coupling situations, and user-extensible. I
have employed the schema to depict commonly-encountered coupling workﬂows.
Associating the schema with software entities—for example, a generic coupling infrastructure toolkit such as the Model Coupling Toolkit (MCT) [9]—will
enable graphical program speciﬁcation for coupler mechanisms. This will require
mappings between the schema and MCT’s classes and methods, combined with
code generation infrastructure, and is an exciting area for future investigation.
Acknowledgments. This work was supported by the US Department of Energy’s Scientiﬁc Discovery through Advanced Computing program. Argonne National Laboratory is operated for the DOE by UChicago Argonne LLC under
contract DE-AC02-06CH11357. I thank the Department of Theoretical Physics/
RSPhysSE at ANU for hosting me as a visiting fellow.

References
1. Larson, J.W.: Ten organising principles for coupling in multiphysics and multiscale
models. ANZIAM Journal (accepted) (2008)
2. Dreyfuss, H.S.: Symbol Sourcebook: An Authoritative Guide to International
Graphic Symbols. McGraw-Hill, New York (1972)
3. ANSI: Standard flowchart symbols and their use in information processing (X3.5).
Technical report, American National Standards Institute, New York (1970)
4. ISO: Information processing–documentation symbols and conventions for data, program, and system flowcharts, program network charts and system resources charts.
Technical report, International Organization for Standardization, Geneva (1985)
5. Yourdon, E.: Structured analysis wiki (2009),
http://www.yourdon.com/strucanalysis/
6. OMG: Unified Modeling Language web site (2009), http://www.uml.org/
7. Bell, E.: Death by UML fever. ACM Queue 2(1), 72–80 (2004)
8. Craig, A.P., Kaufmann, B., Jacob, R., Bettge, T., Larson, J., Ong, E., Ding, C., He,
H.: cpl6: The new extensible high-performance parallel coupler for the community
climate system model. Int. J. High Perf. Comp. App. 19(3), 309–327 (2005)
9. Larson, J., Jacob, R., Ong, E.: The model coupling toolkit: A new fortran90
toolkit for building multi-physics parallel coupled models. Int. J. High Perf. Comp.
App. 19(3), 277–292 (2005)

