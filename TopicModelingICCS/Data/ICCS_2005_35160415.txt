Improving the Data Placement Algorithm of
Randomization in SAN
Nianmin Yao1 , Jiwu Shu2 , and Weimin Zheng 3
1

Department of computer science and technology, tsinghua university
lucos@126.com
2
shujw@tsinghua.edu.cn
3
zwm-dcs@tsinghua.edu.cn

Abstract. Using the randomization as the data placement algorithm
has many advantages such as simple computation, long term load balancing, and little costs. Especially, some latest works have improved it to
make it scale well while adding or deleting disks in large storage systems
such as SAN (Storage Area Network). But it still has a shortcoming that
it can not ensure load balancing in the short term when there are some
very hot data blocks accessed frequently. This situation can often be met
in Web environments. To solve the problem, based on the algorithm of
randomization, an algorithm to select the hot-spot data blocks and a
data placement scheme based on the algorithm are presented in this paper. The diﬀerence is that it redistributes a few very hot data blocks to
make load balanced in any short time. Using this method, we only need
to maintain a few blocks status information about their access frequency
and more than that it is easy to implement and costs little. A simulation
model is implemented to test the data placement methods of our new
one and the one just using randomization. The real Web log is used to
simulate the load and the results show that the new distributing method
can make disks’ load more balanced and get a performance increased by
at most 100 percent. The new data placement algorithm will be more
eﬃcient in the storage system of a busy Web server.

1

Introduction

With the developing of the Internet, a dramatic growth of enterprise data storage
capacity can be observed in the last couple of years. Many things including a
lot of enterprise data coming onto the internet; data warehouse, e-business and
especially the Web contents contribute the growth of storage. So the performance
and capacity of the storage are becoming the bottleneck of IT’s progress. Now,
the SAN (Storage Area Network) is a popular technology to solve the problem. It
can ensure the reliability, serviceability, scalability and availability of the storage.
Though the SAN technology has so many virtues, there are also a few hard
technical problems which need to be solved. One of these problems is how to
place the data among the disks, e.g. the data placement algorithm.
V.S. Sunderam et al. (Eds.): ICCS 2005, LNCS 3516, pp. 415–422, 2005.
c Springer-Verlag Berlin Heidelberg 2005

416

N. Yao, J. Shu , and W. Zheng

The good data placement algorithm can greatly improve the performance of
the storage. More than that, it can also decrease the costs of reorganizing the
data among the disks in SAN while adding or deleting a disk which is a very
common operation in managing the storage system.

2

Related Work

An algorithm commonly being used to place data blocks is RAID[3]. It has
been an industrial standard being applied in most of commercial products. This
method has many virtues such as fault tolerance, high performance and so on.
But it has a shortcoming that all the data must be reconstructed while adding or
deleting disks. Additionally, the RAID 5 can not be used in large-scale storage
system such as SAN because with the number of disks in the storage system
increasing, its writing performance is becoming lower. So RAID is often used
in one disk array and it should not be a good data placement algorithm in the
whole SAN.
Another simple and eﬀective method to distribute the data blocks among
the SAN is using the randomization which is often used now. In this algorithm,
every data block is put into a randomly selected physical disk block. But in fact,
the physical disk block is not completely randomly selected. A pseudo-random
function is often used because the right place for a data block can be easily found
through a simple computation and need not to maintain a very large map list
describing the relationship between the data blocks and the physical disk blocks.
Randomly selecting the location of the data block is good at keeping the load
balancing among all the disks in SAN in a long run. But as the RAID technology,
while adding or deleting the disks, the whole data must be reorganized. And more
than that, it can’t ensure that load is balanced in any short term, for example,
when there are hot data accessed frequently during a short time.
To decrease the costs of reorganizing the data when adding or deleting disks,
an improved randomization algorithm was presented in [2]. The main work of [2]
focused on the hash function being used in the randomization algorithm. Once
the number of disks is changed, it only needs to change the hash function and
move a little data in order to reconstruct the data. Similar works have been done
in [1, 9].
Being improved, the algorithm of randomization now has so many advantages
such as simple computation, long term load balancing, little costs and high
scalability. But all the methods described above have the same shortcoming that
they all can not ensure load is balanced when there is hot data during some short
time. Because if there are some data blocks which are accessed very frequently,
then the load is more often focused on a few disks where these hot data blocks
are saved. In this situation which will often be met in Web environment, the
load is not balanced and the storage resources can not be fully utilized while
using the algorithms discussed above to distribute the data blocks. Intuitively,
if the information of the access frequency of the data blocks is applied into
the distributing scheme, we can make disks loads more balanced. But all the

Improving the Data Placement Algorithm of Randomization in SAN

417

work above did not use this kind of information, because it will cost a lot of
resources if the information of all the data blocks access frequencies are to be
maintained. Some data placement algorithms which take the access frequency of
blocks into account have been given, for example [5] , but they can only be used
in a small storage system such as a disk array, because they totally ignore the
high costs of maintaining all the information of access frequency of blocks. The
data placement algorithm must be simple and eﬀective enough to be used. We
will give a new method based on the randomization in this paper to distribute
the data blocks which can keep short term load balancing and costs little.

3

The Algorithm to Select Hot Data Blocks

The network traﬃc follows the self-similarity model, e.g. it can be bursty on
many or all time scales[6]. So, there will often be a few hot spot data blocks
which burden most loads in the servers. As discussed, the above distributing
methods obviously can not deal with this situation well and make full use of
the storage resources. Because not until the information of data blocks access
frequency is concerned in distributing data blocks, we can not really keep the
load balancing. But how to decrease the costs of maintaining the information of
all the data blocks access frequency? In fact, for some servers on the Internet, we
only need to maintain the access frequency of a small part of all the data blocks,
because there are only a small part of ﬁles of the servers on the Internet which
can be accessed frequently during a short time. In the study of distributed Web
servers, many works make use of this characteristic of servers [2, 3]. For example,
study of the world cup 98s Web log indicates that only 3MB space covers 99%
requests in some short time. So the critical point to solve the problem is how to
eﬀectively select these hot data blocks.
In this section, we will give an algorithm to select the hot data blocks. Tts
data structures are two lists which have ﬁxed length. One called “HL” which will
hold the hot data blocks and the other called “CL” which contains candidate
hot data blocks. Their length is decided by the storage space and it can be 1% of
the whole space in blocks or lower. HL’s elements are ordered by blocks’ access
frequency and CL’s elements are ordered approximately by their insert sequence.
The elements’ data structure contained in the list has information about address
or access frequency such as frequency and hot levle. When we say D1 > D2, it
means that D1.hot levle > D2.hot level or else D1.hot levle = D2.hot level
and D1.f requency > D2.f requency.
The algorithm is described below. In the algorithm, “HOT LEVEL”, “UPGRADE LEVEL” and “CYCLE” are the predeﬁned parameters. CYCLE is the
interval of the number of time units to check and organize the two lists. “Time”
always denotes the current time. The text behind “//” is comment.
(Step 1) Old_time=time;
make all data blocks’ frequency in HL and CL be 0;
(Step 2) A request of data block D comes;

418

N. Yao, J. Shu , and W. Zheng

(Step 3) if(D in HL)
//D’ in HL and D’.logical_address==D.logical_address
then D’.frequency++;
elseif(D in CL)
then {D’.frequency++; move D’ to CL’s tail;}
else {
if (CL is full) then remove the head of CL;
insert D into CL’s tail;
}
(Step 4) if((time - old_time)<CYCLE) then goto (Step 2);
(Step 5) if((time - old_time)>(1+1/3)*CYCLE) // *1
then goto (Step 1);
(Step 6) for(all d in HL){ //deal with all the elements in HL
if(d.frequency > HOT_LEVEL) then d.hot_level++;
else{
d.hot_level= d.hot_level/2; //*2
if(d.hot_level < 1) then remove d from HL;
}
}
reorder elements in HL;
(Step 7) for(all d in CL){//deal with all the elements in CL
if(d.frequency > HOT_LEVEL) then {
d.hot_level++
if (d.hot_level>UPGRADE_LEVEL and ((HL is not full)
or (HL is full and d>HL’s tail) )then {
if (HL is full) then remove HL’s tail element;
insert d into HL;
}
}
}
(Step 8) goto (step 1);
We can see that when the algorithm is functioning, the elements in HL are the
hottest data blocks whose access count is more than HOT LEVEL in about a
time unit for several continuous CYCLEs. The sentence marked by “*1” ensure
that the computing error of frequency is not too big and when this sentence is
true, we can believe that the system is much idle because there is no request
in 1/3(can be changed according to the status of load) time unit. The sentence
marked by “*2” makes the data block’s hot level decrease more quickly than it
increases. Each time when an element in CL is accessed, it will be moved to the
tail of CL so that hot data blocks are not easy to be squeezed out of the CL
because we always squeeze the head element out of the CL. So when running for
a while, elements in the CL must be relatively hot and can be candidates to be
selected into the HL.
Obviously, this algorithm can make sure that all the hottest data blocks will
be put in the HL and its costs are very low because there are only a little simple

Improving the Data Placement Algorithm of Randomization in SAN

419

computations in it and it needs not to access the disks and maintain all the
blocks access frequencies.

4

A New Data Placement Scheme

Now, we can get the information about the hottest data blocks by the algorithm
given in section 3, then the next question is how to use it. We will present a new
data placement scheme in this section based on the algorithm of randomization
and the algorithm to select the hottest data blocks we have given.
SAN systems usually provide storage virtualization functions. Its architecture
is described in ﬁg. 1. The meta data server maintains all the information about
the data distribution. Before a server reads a data block, it must ﬁrst get the
real location of the data from the meta data server. So we can implement the
function of distributing data blocks in the meta server. By the way, some meta
data can be cached in the server and the actual data accesses need not pass
through the meta data server, so the meta data server usually can not become
the bottleneck of the system.

Web server

Web server

FC switch

FC disk

FC disk

Meta server

FC disk

Fig. 1. Virtual Storage Architecture in SAN

In the meta server, we can implement the data placement function like this:
We always assign MAXQL as the command queue length of the busiest disk,
MINQL as the command queue length of the idlest disk. In the following description, MAX QUEUE LENGTH and DIFF QUEUE LENGTH are pre-deﬁned parameters. Initially, the data placement follows the randomization algorithm, for
example, using some pseudo-random hash function. When the system is running, how our data placement scheme is functioning can be described below.
These steps can be inserted after the algorithm’s (Step 7) and before its (Step
8) described in section 3.

420

N. Yao, J. Shu , and W. Zheng

(Step
(Step
(Step
(Step

1)
2)
3)
4)

(Step 5)
(Step 6)
(Step 7)

(Step 8)

(Step 9)

Get the correct value of MAXQL and MINQL.
if not (MAXQL> MAX_QUEUE_LENGTH) then goto (Step 9);
if not (MAXQL-MINQL)>DIFF_QUEUE_LEN then goto (Step 9);
if (There aren’t any hot spot data blocks in the
most busy disk)
then goto (Step 9);
Copy the most hot block to the most idle disk;
Change the physical address of the moved data block
in meta server;
if (There aren’t any data blocks whose physical
addresses were changed and now are not hot)
then goto (Step 9);
Delete the redundant copies of the ever hot data blocks
and change their physical addresses to their original
addresses in meta server;
end;

In the scheme, we do not delete the original moved data block, and then the
hot data block has two redundant copies, so we can also schedule a few requests
to the hot data blocks for better balancing the load. In the scheme, some disks’
space is spent for the redundant copy of hot data blocks. But it is so small that
it can be ignored. For example, if the scheme is used in the Web server of World
Cup 98, the wasted disk space is no more than 3M bytes.
In this distributing scheme, most of the data blocks’ placement follows the
randomization method. It only needs to process a little hot spot data blocks
which have much eﬀect on the storage performance during a short time. So, its
costs are low enough to be applied in the practical storage system.

5

Experiments

To prove our design’s eﬃciency, we implemented a simulation model depicted
in ﬁg. 1 using the CSIM18[10] which is a famous tool to simulate the discrete
events. The model we are using is similar to the one described in [2] . In the
model, one “ROUND” is deﬁned as the time needed by a disk to process a data
block request. The servers continuously send requests to the disks and record
their response time. The diﬀerence between our model and the one in [2] is that
in our model the disks’ requests queue is long enough to hold all the requests,
e.g. the requests will not be discarded. Especially, we use the real Web access log
to drive the model, so it can truly reﬂect the SAN environment. The 1000,000
requests we used to drive the model are extracted from the world cup 98’s web
log at 1998.5.1. For simpliﬁcation, every data block request represents a ﬁle
request in the log.
In the ﬁrst experiment, we set the number of disks as 5 and change the
arriving rate of the requests. Its test results are drawn in ﬁg. 2. In ﬁg. 2, the
X axis represents the interval of incoming requests in percentile of ROUND.
So with the number decreasing, the load is heavier. We can see that in most

Improving the Data Placement Algorithm of Randomization in SAN

421

of the situations, our distributing method is much better than the one using
randomization except that when system is too idle or too busy. We deem that in
the two extreme situations, the load balancing function of the new distributing
method doesn’t work well. The reason of the phenomena can be gotton in our
data placement method such as (Step 5) in section 3 and (Step 2), (Step 3) in
section 4. Because when the storage system are too idle or busy, it will not need
to balance the load what is just our data placement method does.

6000

2800

Randomization
Hot-spot distribution

Randomization
Hot-spot distribution

2600

5000

Mean response time

Mean response time

2400
4000

3000

2000

2200
2000
1800
1600

1000

1400

0
0.188 0.19 0.192 0.194 0.196 0.198 0.2 0.202 0.204 0.206 0.208
Interval of requests(ROUND)

Fig. 2. Tests results of two data placement methods

1200
5

10

15

20

25

30

35

40

45

T number of disks

Fig. 3. Test results while changing the
number of disks

In the testing, we found that the parameters of the model especially the
HOT LEVEL have very big eﬀect on the performance. For example, when we
set the HOT LEVEL as 8, then when the interval of requests is 20% of a ROUND,
the mean response time can reach at 880 ROUNDs which is much better than
before. So if the parameters of the model can be adjusted dynamically according
to the load of the system, we can get more performance enhancements. This will
be our future work.
In our second experiment, we change the number of disks in the model and
get the results described in ﬁg. 3. In order to let the system be not too idle, we
set the interval of requests as 1/n ROUND, in which n denotes the number of
disks. Obviously, we can see from ﬁg. 3 that our data placement scheme is much
better than the other. It can be noticed that when the number of disks is 45, the
two methods get the same results. We believe that it is because that the system
is too idle in this conﬁguration. The followed test proved our conjecure that
when we increase the load, we get much better results than the randomization
method. For simpliﬁcation, we would not show this results here.

6

Conclusions

In this paper, some data placement algorithms are analyzed and they all can
not keep disks’ load balanced when there are hot data accessed frequently which
have much eﬀect on the performance of the system. The reason is that they do

422

N. Yao, J. Shu , and W. Zheng

not take the access frequency of disks into consideration because they may think
it will cost too much. So we present a new data placement method based on the
randomization algorithm which can evenly distributing the hottest data blocks
among disks and is also very easy to implement and costs little.
We implement a simulation model to test the new data placement algorithm
and it proves that it can better balance the load of disks and get much higher
performance. We will implement it in a true SAN system with the virtual storage
function in the future. We can say that it will be very promising while applying
the data placement algorithm into the storage system of a busy Web server.
Acknowledgements. This research is supported by China Postdoctor foundation(No.023240008), the National Natural Science Foundation of China (No:
60473101) and the National High-Tech Research and Develop Program of China
(No: 2004AA111120).

References
1. A. Brickmann, K. Salzwedel, and C. Scheideler: Compact, adaptive placement
schemes for non-uniform capacities. In Proceedings of the 14th ACM Symposium
on Parallel Algorithms and Architectures (SPAA), Winnipeg, Manitoba, Canada
(Aug, 2002) 53–62
2. Cherkasova L, Karlsson M: Scalable web server cluster design with workload-aware
request distribution strategy with WARD. In: Proceedings of the 3rd International
Workshop on Advanced Issues of E-Commerce and Web-Based Information Systems. Los Alamitos: IEEE Computer Society Press (2001) 212–221
3. D. A. Patterson, G. Gibson and R. H. Katz: A case for Redundant Arrays of Inexpensive Disks(RAID). In Proceedings of the 1988 ACM Conference on Management
of Data(SIGMOD) (June 1988) 109–116
4. Du ZK, Zheng MY, Ju JB: A distributed algorithm for content-aware Web server
clusters. Journal of Software, 2003,14(12) 2068–2073
5. G. Weikum, P. Zabback, and P. Scheuermann: Dynamic File Allocation in Disk
Arrays. In Proc. of ACM SIGMOD, (May 1991) 406–415
6. J. C. Mogul: Network behavior of a busy web server and its clients, Technical
Report WRL 95/5, DEC Western Research Laboratory, Palo Alto, CA (1995)
7. M. E. Crovella and A. Bestavros: Self-similarity in world wide web traﬃc: Evidence
and possible causes, In Proceedings of the 1996 ACM SIGMETRICS International
Conference on Measurement and Modeling of Computer Systems (May 1996)
8. P. Berenbrink, A. Brinkmann, and C. Scheideler: Design of the PRESTO multimedia storage network. In International Workshop on Communication and Data
Management in Large Networks, Paderborn, Germany (October 5 1999)
9. R. J. Honicky, E. L. Miller: A fast algorithm for online placement and reorganization of replicated data. 17th International Parallel and Distributed Processing
Symposium (IPDPS) (2003)
10. Schwetman, H.: Object-oriented simulation modeling with C++/CSIM17. In Proceedings of the 1995 Winter Simulation Conference. ed. C. Alexopoulos, K. Kang,
W. Lilegdon, D. Goldsman. Washington, D.C. (1995) 529–533

