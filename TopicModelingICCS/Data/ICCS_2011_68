Available online at www.sciencedirect.com

Procedia Computer Science 4 (2011) 1977‚Äì1986

International Conference on Computational Science, ICCS 2011

Coarse Grained Parallelized ScientiÔ¨Åc Applications on a Cost
EÔ¨Écient Intel Atom Based Cluster
Robin Geyer1,‚àó, Andy Georgi1,‚àó‚àó, Wolfgang E. Nagel1,2,‚àó‚àó
ZIH, Technische Universit¬®at Dresden

Abstract
Modern science requires the ability to evaluate large numbers of results from experiments. Therefore, automated
interpretation of experimental results often has a signiÔ¨Åcant inÔ¨Çuence on the costs of a scientiÔ¨Åc project. One way to
reduce these costs is to use ultra low cost and energy-eÔ¨Écient hardware for building analysis systems. In this paper two
cluster systems which make use of Intel Atom micro-ITX boards and other low cost peripherals are compared. Three
real world applications are chosen to proof the concept of the systems and compare them. Additionally, synthetic
benchmarks are conducted to put the performance of the applications in perspective.
The results of the measurements are discussed and used to evaluate the eÔ¨Äectiveness of the systems in terms of
performance, power consumption and costs. Finally, based on the conclusions, speciÔ¨Åc application areas are suggested
in which the introduced systems perform well and in which it is better to use clusters with high end server components.
Keywords: intel, atom, cluster, analysis, cost-eÔ¨Écient

1. Introduction
In the past, there was a trend in the development of microprocessors. No longer is the performance for one core
crucial, but the energy eÔ¨Éciency. For a Ô¨Åxed energy budget, the performance of an overall system can be increased by
increasing the number of cores. Or, if the energy and buying costs have to be as minimal as possible, using fewer cores
than the high end processors and implementing power saving features on the cost of performance. The last approach is
particularly interesting for mobile devices and always-on embedded devices. To serve the market, the manufacturers
put a number of processors‚Äîlike the Intel Atom, AMD Geode or VIA Nano‚Äîon the market. These are primarily used
in subnotebooks, netbooks, nettops or with lower clock rate also in mobile Internet devices. As in any other segment
of the processor market, the number of cores per processor is growing. Hence, one is able to build very cheap clusters
with a relatively large number of CPU cores. This paper evaluate, if it is possible, to build cluster systems out of these
‚àó Principal

corresponding author
author
Email addresses: robin.geyer@zih.tu-dresden.de (Robin Geyer), andy.georgi@tu-dresden.de (Andy Georgi),
wolfgang.nagel@tu-dresden.de (Wolfgang E. Nagel)
1 URL http://tu-dresden.de/zih
2 Head of Department
‚àó‚àó Corresponding

1877‚Äì0509 ¬© 2011 Published by Elsevier Ltd. Open access under CC BY-NC-ND license.
Selection and/or peer-review under responsibility of Prof. Mitsuhisa Sato and Prof. Satoshi Matsuoka
doi:10.1016/j.procs.2011.04.216

1978

Robin Geyer et al. / Procedia Computer Science 4 (2011) 1977‚Äì1986

components which Ô¨Åt the requirements of scientiÔ¨Åc analysis software. Our experiences and investigations revealed
that most projects produce large amounts of datasets which have to be interpreted independently. The analysis process
related to the projects is serial in most cases. The parallelism is achieved by analyzing as many datasets as possible
simultaneously, which is implemented by executing the serial jobs on multiple cluster-nodes with diÔ¨Äerent datasets. In
these kinds of workÔ¨Çows, a low CPU performance can be compensated by higher parallelism. Additionally, there is no
need for high speed interconnects, the network is only needed to distribute the data and gather the results. Under these
aspects the Intel Atom platforms has been chosen. These platforms achieve less performance‚Äîcompared to common
cluster components‚Äîbut also lower acquisition and operational costs due to low power consumption and cheap spare
parts. The results of this paper can be used to compare the performance, the costs and the power consumption of an
Intel Atom based cluster with other analysis systems.
2. Hardware
To put the results of this work into perspective, this section describes the main components of the two systems
under testing. Based on the given architecture, Ô¨Årst estimates of the expected performance are given.

	

$"%&
$
 '
$'
$"'
$'
$&
$(

!'

$
















'

"'

	 	





	 	



























)*$
$


!
###






	



"



	







	







	



2.1. Cluster Overview
The layout of both clusters is shown in Figure 1. The head-node is used as entry point and runs essential network
services, a batch system for job submitting and a network Ô¨Åle system which allows the sharing of folders over the entire
system. Since these services generate a non-speciÔ¨Åable load, the measurements were performed on the compute-nodes
only. In every system, Gigabit Ethernet is used as interconnect for management and inter-process communication.
Due to the high latency and low bandwidth of Ethernet, a restriction to coarse-grained parallelized applications is
necessary. This only works if the per job memory footprint is small enough. Regarding this footprint, the applications
have been chosen. The applications will only use the network to distribute data and to store the results.




	






	














Figure 1: Cluster Layout for n nodes and one head-node (left) and the comparison of the Diamondville and Pineview platform (right). In our
conÔ¨Åguration four nodes were used for a test system.

The same amount of main memory (2 GB) and hard disk storage (250 GB) has been installed in the nodes of the
diÔ¨Äerent clusters to make them comparable. The decision to use local disks was taken due to the small amount of
available main memory. Contrary to a RAM disk setup, this allows us to install a full feature operating system with a
small memory footprint on each node. In addition, temporary results can be stored on the local disks without aÔ¨Äecting
the network performance.
2.2. Intel Atom Processor Family
Intel announced the Atom processors, Diamondville and Silverthorne, in 2008 [1]. Both were called Atom, but
they diÔ¨Äer in their application. Diamondville is used in desktops, nettops and netbooks, while Silverthorne is almost

1979

Robin Geyer et al. / Procedia Computer Science 4 (2011) 1977‚Äì1986

Number of Cores
Issues per Cycle
SMT
L1 Data Cache/Core
Instruction Set
Thermal Design Power

Atom 330 & D510
2
2
yes
24 KByte
64 bit
8 W / 13 W

Clock Speed
Integer Mult/Div
L1 Instruction Cache/Core
L2 Cache/Core
Lithography

Atom 330 & D510
1.6 GHz
no
32 KByte
512 KByte
45 nm

Table 1: Key features between Intel‚Äôs Atom 330 and D510. The Thermal Design Power of the 330 is 8 watt and for the D510 13 watt, thats an
increase by 5 watt compared to its predecessor.

exclusively used for Mobile Internet Devices. Also, the successors were developed independently. We selected
Diamondville (Atom 330) and successor Pineview (Atom D510) due to the higher performance.
Table 1 shows the characteristics of both processors at a glance. While the Diamondville has its two cores separated on two dies in a single package, the Pineview is monolithic. Figure 1 shows the architecture of both processors.
Intel uses the same cores introduced in 2008 and integrates the memory controller, the Direct Media Interface link
(DMI) and the GMA 3150 graphics core. Theoretically, this integration should result in a memory latency improvement.
2.3. Intel Atom Cores
The Atom cores are designed to handle two instructions at the same time. The two decoders are equal in their ability and can decode one instruction per cycle respectively. But they can only be used simultaneously if the instructions
are already located in the instruction trace cache.
For integer multiply and division, the SIMD Ô¨Çoating-point units are used. There are two SSE units and the scheduler
can dispatch either a single precision Ô¨Çoating point or an integer SIMD instruction to both ports per cycle. All functional units have a processing width of 64 bits except the two SIMD units which have a 128-bit width.
A special characteristic of the Atom architecture is the strict in-order-execution. This has strong inÔ¨Çuence on the performance. While out-of-order-execution yields higher performance, it also comes along with much higher scheduling
complexity and power consumption. Nevertheless, there are techniques for bypassing idle times, which often occur
in in-order-designs[2]. The most important one is Hyper-Threading, Intels implementation of simultaneous multithreading[2]. A technique which does not strictly follow the in-order-design is the ‚ÄùSafe Instruction Recognition‚Äú
algorithm. The corresponding manuals and data sheets provide additional reading on this subject ([3], [4], [5]).
2.4. Intel Atom Platforms
Along with the processor, the Ô¨Årst Atom platforms were released in 2008. The Diamondville platform is illustrated
in Figure 1. The Ô¨Årst release contained an Atom processor with a TDP of 8 watt and a two-component chipset with
a total power consumption of about 22 watt. Intel tried to eliminate the high power consumption of the chipset.
As a result, the 945GSE (6.6 watt), the ICH7 (3.3 watt) and the Atom 330 (8 watt) led to a platform with a total
power consumption of 17.9 watt. The next step in reducing the power consumption is done by introducing the
Pineview platform. Intel discontinued the three-chip conÔ¨Åguration and integrated the northbridge functionality into
the processor die. As a result, the CPU maximum TDP increases from 8 watt to 13 watt as shown in Table 1, but
6.6 watt can be saved due to the removal of the G45GSE chipset. The Pineview platform comes with architectural
changes to improve performance. The main changes are doubling the maximal supported amount of main memory,
including a higher clock rate, increasing the bandwidth of the DMI link from 8 to 10 Gbit/s and reducing the memory
latency due to the integration of the memory controller (IMC). The expectations in reducing the main memory latency
are diminished by the fact that there is still a Front-Side Bus (FSB) between the CPU core and the memory controller
as illustrated in Figure 1.
2.5. Memory and Peripherals
The new Pineview platform can handle up to 4 GB DDR2-800 main memory. The Diamondville only supports
DDR2-533 memory. To achieve comparable results, only 2 GB of DDR2-667 memory were used in both systems. In
the case of the Diamondville the memory is clocked with 533 MHz while Pineview can use the full 667 MHz.

1980

Robin Geyer et al. / Procedia Computer Science 4 (2011) 1977‚Äì1986

Intels 82801GB I/O Controller Hub was replaced by the NM10 Express chipset which connects to the peripherals.
Gigabit Ethernet is already integrated and accessed through the mentioned chipsets. For this device a latency of 150 Œºs
and a bandwidth of 85 MB/s has been measured on both systems. Because only one additional PCI slot is available,
there is no possibility to enhance the network performance adequately. This was the main reason for the selection of
coarse grained parallelized scientiÔ¨Åc applications.
All other provided hardware peripherals, which are not needed for this research, were disabled.
3. Costs
The costs for both boards is around $70 at the time of purchase. The limitation to 2 GB (or 4 GB for Pineview)
main memory, which has to be purchased separately, sets the price to approximately $100 for a functional unit. If hard
drives in the cluster-nodes are necessary the price is respectively higher. To build an actual cluster out of these parts
another cost factor has do be considered, the housing.
Part
Atom ITX-Board
>200 GB HDD
Cabels, etc.

Price in
% of overall cost
‚âà35
‚âà20
‚âà10

Part
2 GB 677MHz memory
Barebone housing

Price in
% of overall cost
‚âà20
‚âà15

Table 2: Rough estimation of the cost for one cluster-node for the Atom cluster. The shares should even be relatively constant for future systems
with respectively newer (larger HDD, more memory, ...) components. This table is only valid for an ultra low cost system with barebone housing.

The only reasonable way (known to the authors) to house the Atom boards in an 19‚Äù rack format is the Travala
C147 (discontinued) or T1200 case. There are many 19‚Äù ITX cases but the C147/T1200 is the only one which can
hold and power two Atom ITX boards plus hard drives. For us, it was not possible to buy the C147/T1200 case for
less than $230. The advantage is the typical rack management-ability while the disadvantage is the enormous cost
compared to the rest of the hardware. A workaround for this problem might be using oÔ¨Ä-the-shelf barebone cases,
which start at $30. If wanted, it is also possible to lock them (Velcro, clue, ...) on a simple 19‚Äù rack slide-in. With
the right size of the slide-in, it is possible to Ô¨Åt 8 boards with peripherals into one or two HU. Table 2 shows the price
of each component on the overall cost. At the time of the purchase of our test systems, the sum of the costs for an
compute-node was lower than $200. Finally, it should be mentioned that a network switch and a suitable head-node
cause extra costs depending on the size of the cluster and the number of users.
4. Software
Besides the synthetic measurements, three programs out of diÔ¨Äerent scientiÔ¨Åc areas were chosen. These programs
are MpCCD (astronomy), POV-Ray (ray tracing/visualization) and RAxML (biology).
4.1. Real World Applications
MpCCD
The institute for planetary geodesy at the Technische Universit¬®at Dresden conducts research on small solar system
bodies (SSSB)[6]. The goal of this research is to reÔ¨Åne the orbit data of already known SSSBs. Therefore images of
the presumed position of the speciÔ¨Åc object and the surrounding star Ô¨Åeld are acquired. MpCCD is used to correlate
the object positions in the images with given star catalogs. From this step two pieces of information are obtained. At
Ô¨Årst, each pixel is now corresponding to an exact astronomical position. The second information is that each object
not found in the star catalog but in the presumed position, is potential the searched SSSB. If the correct object is
identiÔ¨Åed, it is now possible to determine the actual astronomical position with help of the pixel coordinates in the
image. The software itself is not parallelized but coarse grain parallelism can be achieved by starting multiple image
analysis jobs with diÔ¨Äerent images.

Robin Geyer et al. / Procedia Computer Science 4 (2011) 1977‚Äì1986

1981

POV-Ray
The Persistence of Vision Raytracer (POV-Ray)[7] is a free ray tracer which produces images of a relatively high
quality. It is widely used by 3D artists but also by scientists for visualizing complex structures. POV-Ray provides
command line options which can be used to render only a part of an image or speciÔ¨Åc frames of an movie. In this way
parallel processing can be achieved. The partly rendered images can be rejoined by an appropriate post-processing
software. Both POV-Ray measurements render three dimensional structures with a resolution of 1600x1080 pixels.
One of the tests renders 300 frames of a rotating Cantor-dust[8]. The other renders 100 frames of a rotating Mengersponge[9].
RAxML
RAxML[10] for Randomized Axelerated Maximum Likelihood is a software which computes maximum likelihood inferences on large phylogenetic trees. Given a aligned set of DNA or Amino Acid (AA) sequences, it initially
determines the best rearrangement distance for one iteration for the lazy subtree rearrangement to a starting tree, which
is generated by the rapid bootstrapping method. After that, multiple inferences on the starting tree are done under
the parameter of the minimum rearrangement distance which yields the best likelihood on the starting tree. There
are three versions, which have a diÔ¨Äerent degree of parallelism. A single threaded version which is not parallelized
(raxmlHPC), a Ô¨Åne grained parallelized version using the Pthreads library (raxmlHPC-PTHREAD) and a coarse grained
parallelized version (raxmlHPC-MPI). The last one is parallelized on the level of bootstraps and inferences, so it is
possible to do bootstraps or inferences in parallel on a single data set. Since the Atom processors support the SSE3
command set extensions, the SSE3 functionality of these versions are used in the measurements.
4.2. Synthetic Measurements
For general measurements, BenchIT which is described in [11], was used. BenchIT is a framework that uses
synthetic kernels for evaluating diÔ¨Äerent aspects of system performance. These kernels contain standard tests like
BLAS algorithms, MPI communication, simple scientiÔ¨Åc measurements, like FFT, number theory or conjugate gradient solver. Also hardware tests, like memory bandwidth or latency are included. The results can be plotted and
compared in the provided GUI which accelerates the analysis process.
5. Measurements / Results
The energy measurements have been done with the Hameg HM8115-2 8 kW [12] and ZES-ZIMMER LGM95 [13]
power meters. The readout precision was set to 0.1 per electric quantity and one readout per 0.5 seconds. A test had
shown that both power meters operating with a maximal diÔ¨Äerence of under 1%. The power readings presented in this
paper contain only the accumulated used power of the four compute-nodes for each system. The head-node and the
network switch are not subject to our measurements.
5.1. Kernel Measurements
The benchmarks conducted with BenchIT are supposed to give the reader a short and coarse overview about the
systems. All these tests use the full eight cores of each cluster and Hyper-Threading via OpenMPI (16 processes
in sum). The compiler for the benchmark binaries was gcc. While version 4.4.3 is the standard compiler for the
used Debian GNU/Linux operating system versions 4.5.x introduced a new Ô¨Çag especially for Atom processors.
Hence, versions 4.4.3 and 4.5.1 were compared with the basic Ô¨Çags -O3 -mtune=[core2|atom] -funroll-loops
-ffast-math. However, no diÔ¨Äerences bigger than 2% have been observed for the tested applications. As shown
in Table 3 the Atom D510 cluster outperforms the Atom 330 cluster. Section 2.4 describes the main reasons for
this improvement: the integration of the memory controller, the higher main memory bandwidth and the monolithic
design of the Atom D510. Particularly the application-like CGV (Conjugate Gradient Solver) test, with its moderate
network usage, shows results which are far below the theoretical peak performance of the clusters. In this case,
the main bottleneck is the Gigabit Ethernet interconnect. Due to the high latency of the Ethernet technology this
communication stalls the computation.
Figure 2 shows an interesting advantage of the Atom platforms. The memory bandwidth in the AeApBxC test
only drops by a factor of 2 if the data location shifts from cache to memory. The same test performed on high end

1982

Robin Geyer et al. / Procedia Computer Science 4 (2011) 1977‚Äì1986

Benchmark
CGV
Total Comm. Bandwidth
Network Bandwidth
Network Latency
Double Mat-Vec Multiply
Integer Mat-Vec Multiply

Atom 330
Cluster
1.4 GFLOP
1.7 GB/s
‚âà 87 MB/s
150 Œºs
4.1 GFLOP
6.2 GIOP

Atom D510
Cluster
2.1 GFLOP
2.1 GB/s
‚âà 87 MB/s
150 Œºs
5.2 GFLOP
8.7 GIOP

Performance
Factor
1.5
1.24
1.0
1.0
1.27
1.4

Table 3: Comparison of generic BenchIT measurements. The average performance gain is a factor of 1.23.

server clusters with dedicated high speed interconnects, for instance an AMD Opteron or Intel Core i7 cluster with
InÔ¨ÅniBand, show drops by a factor of 10 or 20. In other words, the Atom platforms may not be as fast as a standard
systems but well balanced. So it is not as performance relevant as usual if an application uses data located in the main
memory.

Figure 2: The AeApBxC benchmark calculates A = A + B ¬∑ C on diÔ¨Äerent vectors sizes and calculates the total memory bandwidth.

5.2. Application Measurements
For real world applications introduced in Section 4, power metering has been conducted in addition to the runtime
measurements. The power consumption of the Atom 330 cluster is around 147 watt when idle and has an upper limit
of about 165 watt if the usage of Hyper-Threading is enabled and used. If only one process is mapped to one core the
power consumption does not exceed 157 watt.
The Atom D510 cluster has a lower energy consumption. When idle the four cluster-nodes consumes around 110 watt
under full load, with Hyper-Threading enabled this rises to around 124 watt. If no Hyper-Threading is used the power
consumption does not exceed 121 watt.
ICC:
GCC:
GCC for
POV-Ray:

-pipe -O3 -xSSE3 ATOM --minstruction=movbe
-O3 -fomit-frame-pointer -funroll-loops -mtune=[core2|atom]
-O3 -funroll-all-loops -fpeel-loops -funswitch-loops -funit-at-a-time
-ffast-math -funroll-loops -finline-functions -mtune=[core2|atom]
Table 4: The compiler Ô¨Çags used for the building of the testing software.

Robin Geyer et al. / Procedia Computer Science 4 (2011) 1977‚Äì1986

1983

As diÔ¨Äerent behavior has been observed under the usage of diÔ¨Äerent compilers, only the best (i.e. fastest running)
binaries for each platform are compared. In the deÔ¨Åned scenario the ICC (11.0) has often proved superior. If the
runtime only diÔ¨Äers less than 1%, while using diÔ¨Äerent compilers, binaries compiled with GCC (4.4.3) were compared.
The reason for this decision is the better accessibility of the GCC since it is open source software. Although the 4.5.x
versions of the GCC have a dedicated -mtune=atom Ô¨Çag, measurements with the 4.5.1 doesn‚Äôt show any signiÔ¨Åcant
improvements, at least not in the discussed test cases. The compiler Ô¨Çags used are shown in Table 4.
The naming scheme of the following test cases is: [jobname] [HT] / compiler / Atom [330 | D510],
whereby jobname deÔ¨Ånes a speciÔ¨Åc workload within a software. The optional HT behind the job name indicates if
Hyper-Threading is forced.
MpCCD
MpCCD was the application with the largest runtime variance, depending on the input. Some jobs took less than
two minutes others up to 20 minutes to complete. The set consists of 19 independent jobs which process two, three or
four images. The results are listed in Table 5.
Compile Version /
Cluster
gcc / Atom 330
gcc HT / Atom 330
gcc / Atom D510
gcc HT / Atom D510

Runtime [s]
3449 (2230)
4069
3500 (2250)
3450

Energy
Consumption [Wh]
150.78
177.36
121.14
116.01

Table 5: Results of the MpCCD measurements. With manual scheduling it is possible to reduce the runtime, with nearly the same power consumption, these results are denoted in braces.

Table 5 shows, that the Atom 330 cluster took factor 1.3 more time for the test set. Hyper-Threading does not
bring many advantages in this case. However, the main problem with this test is the job scheduling due to the big
variance in the job time.
POV-Ray
Both POV-Ray tasks render single frames of a movie. The runtime variance of the jobs (i.e. frames) ranges from
50 to 120 sec for the Cantor and 1000 to 3200 sec for the Menger-movie. These numbers refer to single job per core
processing. For the Cantor test, 300 frames were rendered, while for the Menger-movie, 100 frames were computed.
Although forcing Hyper-Threading results in longer runtime for a single job, it works well in this case. Table 6
shows, forcing Hyper Threading yields 30-38% less total runtime and energy consumption.
Compiler Version /
Cluster
menger / icc / Atom 330
menger HT / icc / Atom 330
cantor / icc / Atom 330
cantor HT / icc / Atom 330
menger / icc / Atom D510
menger HT / icc / Atom D510
cantor / icc / Atom D510
cantor HT / icc / Atom D510

Runtime [s]
23665
15247
3425
2194
27145
17290
3495
2175

Energy
Consumption [Wh]
1024.24
679.52
126.32
83.26
893.11
591.36
114.31
74.07

Table 6: Results of the POV-Ray Measurements

In Figure 3, two eÔ¨Äects observed on both Atom clusters, are shown. Firstly, the improvements of the runtime by
using a more suitable compiler. This is basically a behavior which can be seen on every computer. Secondly‚Äîfor the

1984

Robin Geyer et al. / Procedia Computer Science 4 (2011) 1977‚Äì1986




	

	

	

	

	

	







"#$






	






	





	





	

"#$


	










	



!

















Atom 330 cluster‚Äîthe diÔ¨Äerent power levels with about 147 watt when idle and 156 watt when all cores are used by
a single thread. Forcing Hyper-Threading increases the power consumption to over 160 watt. On the other hand it
also reduces the runtime by 35% compared to the same workload. The measurements of the Atom D510 cluster show
similar results.



Figure 3: Comparison of power consumption depending of compiler and Hyper-Threading for the Menger-test. If Hyper-Threading has been used
the power consumption is about 5 watt or 7% higher, but the ‚Äúwall‚Äù-time of the task is reduced by 35%. In the Hyper-Threading cases, the
advantage of using icc is 83% runtime for Atom 330 and 85% for the D510 compared to runtimes with gcc. The power consumption is factor 1.2
higher for the Atom 330 and 1.17 for the D510. The 1on1 cases show around the same results.

RAxML
Two diÔ¨Äerent RAxML tasks were executed, a amino acid alignment (AA) and a DNA alignment. For both tests, a
Ô¨Åxed series of 50 starting random seeds have been chosen. In this manner 50 jobs were produced for both alignments.
The runtime variance of the jobs is small with e.g. 15 sec for the DNA test compiled with icc on the Atom 330 cluster.
Compile Version /
Cluster
AA single / icc / Atom 330
AA pthreads HT / icc / Atom 330
DNA single / icc / Atom 330
DNA pthreads HT / icc / Atom 330
AA single / icc / Atom D510
AA pthreads HT / icc / Atom D510
DNA single / icc / Atom D510
DNA pthreads HT / icc / Atom D510

Runtime [s]
1573
1231
1515
1046
1720
1192
1412
971

Energy
Cons. [Wh]
70.50
56.58
67.82
48.35
58.20
42.12
47.85
34.01

Table 7: Results of the RAxML Measurements

As shown in Table 7, the two general eÔ¨Äects are also observable. Although Hyper-Threading gains performance,
it is not as much as in the POV-Ray tests. By enabling Hyper-Threading the runtime and energy consumption drops
to a maximum of 31% compared to the single thread run with the same binary.
Concerning the three versions of RAxML, Hyper-Threading can be used in diÔ¨Äerent ways. Firstly, by putting 4
instances of the single threaded version on one node. Secondly, by using one instance of the Pthreads version which
spawns 4 threads. Every combination in between also can be used (e.g. 2 single, 1 Pthreads with 2 threads). Hence,
a test was conducted in which the four instances of the single version are measured against the Pthreads version with

Robin Geyer et al. / Procedia Computer Science 4 (2011) 1977‚Äì1986

1985

four threads. It has shown that, independent of the compiler used to compile RAxML, the single threaded version is
always 4-8% slower than the Pthreads version. Hence, Table 7 only shows the results of the Pthreads version for the
Hyper-Threading runs.
5.3. Comparison with highly energy-eÔ¨Écient Sun Fire X4170
To put the measurements in to perspective, we conducted measurements on a highly energy-eÔ¨Écient Sun Fire
X4170 Server[14]. The X4170 features two Intel Xeon X5560 processors (2√ó4 Cores with Hyper-Threading) and
12 GB DDR3-1333 RAM in a SMP environment. This server clearly outperforms both Atom clusters in runtime
as well as in terms of energy-eÔ¨Éciency. For the test-cases above, compared to the X4170, the D510 cluster shows
the following behavior. Runtime factors: minimum 2.96, maximum 8.20, average 5,87 longer runtime. Energy
consumption factors: minimum 1.81, maximum 4.22, average 2.72 more energy consumption.
These results have to be considered with caution! The X4170 used, costs around $3500, our Atom cluster $500,
thats a factor of seven. If Atom hardware for the same price is used it should be much faster than the X4170. The idle
power of the X4170 is 120 watt, under full load it consumes 360 watt. Compared to the D510 cluster with 124 watt
under full load its a factor of 2.90 more. Also, the X4170 has only one hard drive, without the hard drives in the D510
cluster the average energy-consumption factor is only 2.00. Furthermore the following points has do be considered:
the Xeon X5560 is two generations ahead considering the production process and one processor generation ahead;
very much cheaper spare parts for the Atom cluster; for application which particularly request extremely short batch
system latencies, it is possible to build a very cheap cluster with many nodes with the Atom architecture; in an
cluster, nodes can be disabled selectively for reasons of power consumption. Based on this considerations, it is valid
to conclude that the Atom cluster is cost-eÔ¨Écient. In terms of energy-eÔ¨Éciency, it should be noted that the Atom
platform is still ‚Äúenergy-eÔ¨Écient‚Äù, that is proven many times like in [15] and as mentioned in Section 6.
6. Related Work
Generally, mini-ITX computers which using Intel Atoms or similar CPUs are widely used. The industry oÔ¨Äers
several whitepapers. Especially in the high density server market, processors like the Atom are established. VIA
Technologies uses the Eden powered VT310-DP to build a high density cluster for Grid computing in [16]. SeaMicro
oÔ¨Äers system called SM10000 [17], which is far more sophisticated, it features eight dual core Atoms on one board.
Along with special storage and network cards, 64 such processor boards can be used in one SM10000 system as a
distributed memory system. In the scientiÔ¨Åc area, the Carnegie Mellon University runs a project called FAWN (A Fast
Array of Wimpy Nodes) [15] which is the winner of 2010 10GB Joulesort[18] benchmark[19]. It uses energy eÔ¨Écient
CPUs like the AMD Geode and Intel Atom with solid state drives (SSD) for data intensive computations. Also for the
Joulesort benchmark, a similar concept is used by Beckmann et al. in [20]. In [21] Scogland et al. dealing with the
advantages of an ION based Zotac board by using the GPU as a coprocessor.
7. Conclusion
Because of the interconnect, the Atom cluster design discussed in this paper is not suitable for any kind of Ô¨Åne
grained parallel applications. Nevertheless it has been shown that it is usable for some special scenarios, which are:
‚Ä¢ The applications are not parallelized or only parallelized in working package level in a very coarse way.
‚Ä¢ The mix of applications running on the cluster is not subject to change very often. Because the risk that a new
application is not able to perform well on the cluster is too high.
‚Ä¢ The purchase cost has to be as minimal as possible.
‚Ä¢ For the users of the cluster, the runtime is not critical.
‚Ä¢ The provider wants to oÔ¨Äer a system with general purpose CPUs.
‚Ä¢ A energy eÔ¨Écient system is wanted.

1986

Robin Geyer et al. / Procedia Computer Science 4 (2011) 1977‚Äì1986

Another Ô¨Åeld of use is higher education. Especially the principles of setting up a cluster infrastructure and software
can be taught cost-eÔ¨Éciently with this design. If large numbers of students are supposed to actually build a cluster in
a hands-on-seminar it is of great advantage if the hardware to handle with is as cheap as possible.
The development of more sophisticated Atom based integrated main boards is very interesting. There are many
possible solutions which can improve the performance of such a cluster. The SeaMicro systems implement high
density special main boards. Thus a total of total 16 cores are available on a single board. For the commodity Atom
based hardware there are two promising trends. The Ô¨Årst one is the integration of NVIDIA ION chipsets which are
able to use the GPU as a co-processor. With the ION chipset it is also possible to use PCI-Express ports which oÔ¨Äer
the possibility to use a faster interconnect.
In many areas of scientiÔ¨Åc work, the manual interpretation of the generated results takes much more time than
the computation time on the cluster. Hence, the processing speed of the automated analysis system is not critical.
Considering the low idle power, it can be stated that the design introduced in this paper is a suitable solution for non
time critical work. It oÔ¨Äers the user a general purpose machine which is relatively energy eÔ¨Écient and has very low
acquisition costs. In particular if additional energy saving techniques, such as shutting down unneeded nodes through
the batch system, are applied, the energy balance can be further improved.
Appendix A. Sources
[1] Intel Announces Intel Atom Processor Brand.
URL http://www.intel.com/pressroom/chipshots/archive.htm
[2] David A. Patterson, John L. Hennessy: Computer Organization and Design, Morgan Kaufmann, 2009, Ch. 7.5 & 5.3.
[3] http://www.intel.com/technology/platform-technology/hyper-threading/index.htm.
[4] Intel 64 and IA-32 Architectures Optimization Reference Manual.
[5] Intel Atom Processor 330 Series Datasheet.
[6] R. Langhans, A universal computer program for high precision position determination of minor planets on ccd-frames, in: Journ¬¥ees 2002 syst`emes de r¬¥ef¬¥erence spatio-temporels.
[7] POVRay Manual http://www.povray.org/documentation/.
[8] Wolfram Math World about the Cantor Dust.
URL http://mathworld.wolfram.com/CantorDust.html
[9] Wolfram Math World about the Menger Sponge in 2D.
URL http://mathworld.wolfram.com/MengerSponge.html
[10] A. Stamatakis, Distributed and parallel algorithms and systems for inference of huge phylogenetic trees based on the maximum likelihood
method, Ph.D. thesis, Technische Universit¬®at M¬®unchen (October 2004).
[11] S. B. e. a. Guido Juckeland, Benchit - performance measurement and comparison for scientiÔ¨Åc applications, in: PARCO2003 Proceedings,
2003.
[12] Hameg HM8115-2 8 kW Product Description.
URL http://www.hameg.com/147.0.html?L=0
[13] LMG95 Single Phase Precision Power Analyzer Product Description.
URL http://www.zes.com/english/products/single-phase-precision-power-analyzer-lmg95.html
[14] Sun Fire X4170 Server.
URL http://www.sun.com/servers/x64/x4170/specs.xml
[15] D. G. Andersen, J. Franklin, M. Kaminsky, A. Phanishayee, L. Tan, V. Vasudevan, Fawn: a fast array of wimpy nodes, in: Proceedings of the ACM SIGOPS 22nd symposium on Operating systems principles, SOSP ‚Äô09, ACM, New York, NY, USA, 2009, pp. 1‚Äì14.
doi:http://doi.acm.org/10.1145/1629575.1629577.
URL http://doi.acm.org/10.1145/1629575.1629577
[16] I. VIA Technologies, Via vt310-dp mini-itx mainboard, Whitepaper (October 2005).
URL http://www.via.com.tw/en/downloads/whitepapers/initiatives/spearhead/cluster server.pdf
[17] Seamicro sm 10000 system, Whitepapers on SM10000, overall 10 pages in diÔ¨Äerent Whitepapers (2010).
URL http://seamicro.com/
[18] S. Rivoire, M. A. Shah, P. Ranganathan, C. Kozyrakis, Joulesort: a balanced energy-eÔ¨Éciency benchmark, in: Proceedings of the
2007 ACM SIGMOD international conference on Management of data, SIGMOD ‚Äô07, ACM, New York, NY, USA, 2007, pp. 365‚Äì376.
doi:http://doi.acm.org/10.1145/1247480.1247522.
URL http://doi.acm.org/10.1145/1247480.1247522
[19] Sort benchmark home page, Webpage (2010).
URL http://sortbenchmark.org/
[20] A. Beckmann, U. Meyer, P. Sanders, J. Singler, Energy-eÔ¨Écient sorting using solid state disks, International Conference on Green Computing
0 (2010) 191‚Äì202. doi:http://doi.ieeecomputersociety.org/10.1109/GREENCOMP.2010.5598309.
[21] T. Scogland, H. Lin, W. Feng, A Ô¨Årst look at integrated gpus for green high-performance computing, Computer Science - Research and
Development 25 (2010) 125‚Äì134, 10.1007/s00450-010-0128-y.
URL http://dx.doi.org/10.1007/s00450-010-0128-y

