Evolutionary Rule Generation Classification and Its
Application to Multi-class Data

Susan E. Bedingfield and Kate A. Smith
School of Business Systems Monash University Clayton, Victoria 3168 Australia
{sue.bedingfield, kate.smith}@infotech.monash.edu.au

Abstract. This paper considers an evolutionary algorithm based on an
information system for generating classification rules. Custom genetic operators
and a multi-objective fitness function are designed for this representation. The
approach has previously been illustrated using a binary class data set. In this
paper we explore the possibility of using the algorithm on a multi-class data set.
The accuracy of the rules produced by the evolutionary algorithm approach are
compared to those obtained by a decision tree technique on the same data. The
advantages of using an evolutionary classification technique over the more
traditional decision tree structure are discussed.

1

Introduction

The classification of data into distinct groups is one of the main tasks of data mining.
While many data sets involve a binary target attribute (for example good or bad credit
risk), a significant number of important data sets involve the division of the data into
multiple groups which refer to as multi-class data. There are many techniques for
classifying both binary and multi-class data, one of the most widely used being
decision trees [1]. The main reasons for the popularity of decision trees is that they
are a rule based approach and they classify every data point. However for many real
world data sets clear classification boundaries may not exist and this may be useful
information that becomes lost when adopting a technique that forces such clear cut
classifications. In addition decision trees only explore a subset of all possible rules
due to the greedy nature of their decision making [2]. The generation of branches in a
decision tree is dependent upon previous splitting, each split seeks to maximise the
accuracy of the classification at that level rather than looking ahead to consider the
entire search space. Given this limitation, and the attractiveness of a rule based
approach to classification for ease of interpretability, we consider the benefits of using
an evolutionary approach to rule generation.
In this paper we first introduce an evolutionary classification technique and test its
viability on a database with a multi-class target attribute. We then compare the
evolutionary classification technique with See5, a widely used decision tree software
package used for classification. The evolutionary technique used in this paper has
been reported upon in [3],[4] and [5]. So far the technique has been used a database
P.M.A. Sloot et al. (Eds.): ICCS 2003, LNCS 2660, pp. 868–876, 2003.
© Springer-Verlag Berlin Heidelberg 2003

Evolutionary Rule Generation Classification and Its Application to Multi-class Data

869

with a binary consequence attribute – the well known German credit scoring data.
However the technique can be generalised to any number of classes corresponding to
the number of attribute values belonging to the consequence attribute. In the example
that follows we use the well known Iris database with 3 classes.
We then compare the evolutionary technique implemented in CS-classifier with the
decision tree algorithm implemented by See5, both in terms of the classification rules
generated and the error rates observed by testing the rules on randomly generated test
sets.
CS-classifier is a global evolutionary search technique that makes no assumptions
about the attribute values, the consistency or accuracy of the data. See5, on the other
hand, is a greedy search algorithm which forces classification irrespective of the data
quality.
Our aim is to compare the performance of the two methods and determine if the more
flexible nature of the evolutionary approach leads to more insightful and accurate
rules.

2 The Evolutionary Algorithm
2.1 Rules and Rule Structure

Each population consists of a collection of rule sets and each rule set is simply a
collection of rules, the number of rules in each rule set is specified as one of the
parameters in the evolutionary algorithm. Each individual rule is made up of a set of
atoms. An atom is a structure of the form (α,β,γ) where α represents an one of the
attributes, β is a comparison operator (≤ , ≥ , =) and γ a is number within the range of
values assumed by the attribute α .
A rule has a structure of the form (atom1, atom2, …,atomN) where N ≤ M and M ≥2 is
a parameter defining the upper bound on the number of atoms allowed to make a rule.
A restriction is placed on the number of atoms allowed to ensure that the rules are
easily interpreted and understood by humans. For each rule, atom1, atom2, …,atomN-1
have a condition attribute as the attribute component, and atomN has a decision
attribute as its attribute component.
Note that a particular attribute may appear more than once in an atom, for example
when an upper and lower bound for a specific attribute is required. For example, a
rule might be:
(petal length>50, petal width <=60, species = iris setosa).
A rule system, ℜ, comprises a finite set of rules {rule1, rule2,…, rulek} where 1≤k≤Κ,
Κ being a parameter defining the number of allowable rules in a rule set. The
evolutionary algorithm population consists of P rule systems each containing k
individual rules.

870

S.E. Bedingfield and K.A. Smith

2.2 Genetic Operators

The genetic operators used in this paper were an elitist selection criteria, crossover
and mutation.
Crossover
Crossover between individual rule systems is accomplished randomly based on a
fixed crossover probability Pc.The first C rules from each rule system are selected and
swapped.(The starting point for the swap is also determined as part of the crossover
process). C may vary from 0 to M where M is half the size of the smaller rule system.
The rule systems produced by the crossover are added to the population, thus
temporarily increasing its size.
Mutation
Mutation occurs to the rules within the individual rule bases and takes various forms
random changes to the components of each atom, swapping one or more atoms
between two rules, deleting a rule, adding a new rule. The probabilities of each type
of mutation is parameterised .
Selection
The selection criteria used is elitist - the best P rule sets of the current generation are
selected to form the next generation.
2.3 Fitness Function Components

There are several measures of fitness of the individual rules in each rule base that we
are interested in recording and improving over time. We first consider accuracy
measures – both row and column accuracy – of the resulting confusion matrix. The
particular information system we are experimenting with has a decision attribute with
3 possible values. This leads to 3 row accuracy measures M1,….. M3, and 3 column
accuracy measures M4,….. M6,. M1,….. M3, effectively measure the coverage by the
rule system of the 3 classification groups. Whereas M4,…..,M6 measure the accuracy
of the predictions for each classification group. These 6 expressions should optimally
be equal to 1. Analagous to the three accuracy measures M1,….. M3 , are three
measures of inaccuracy, M7,….. M9, designed to reduce incorrect classifications.
We let O represent the set of data points in the Iris database, A the set of attributes.
Let Vd = { v1 , v2 , v3 } represent the set of attribute values that the target attribute can
take. If v∈Vd, where d is the target attribute, we define Ov to be the subset of O whose
decision attribute value is v.
If we let ℜ represent a rule set and if ρ∈ℜ, then we define Oρ to be the subset of O
whose members satisfy the condition attribute values associated with ρ and Oρ ,d(ρ) to
be the subset of O whose members satisfy the rule ρ (i.e. they satisfy the condition
and target values for ρ).
We can express the measures M1,….. M9, as follows:

Evolutionary Rule Generation Classification and Its Application to Multi-class Data

871




O
U
ρ


(
)
d
ρ
=
ν
i


Card Oν i

Card  Oν i I
M i , i = 1,...,3 are defined as

( )








O
U
ρ
 d ( ρ )=ν  
i




Card  U O ρ 
 d ( ρ )=ν 
i



Card  Oν i I 
M i +3 , i = 1,...,3 are defined as






Card  Oν i I 
Oρ  
 d ( ρ ) ≠ν  

i



Card (Oν i )

U

M i + 6 , i = 1,...,3 are defined as

Combining Fitness Function Components
The total fitness function for the evolutionary algorithm can be represented as the
weighted function of the nine components M1-M9 defined above, and attempts to
generate rules over time with maximal coverage, maximal accuracy, minimal
contradictions.
The Fitness Function
The fitness function used for the work presented in this paper has the following form:

F = λF

1

+ (1 - λ )F 2 where 0 < λ < 1 and

 6
 ∑Mi
1
F =  i =1 9

1+ ∑ M i
i=7




,




F

2

9
 6
1
=  ∑ M i + ∑
i =7 1 + M i
 i =1





This form of the fitness function has been discussed previously in [5]. It has been
observed that the component F1 if used solely as the fitness function tends to induce
rules which are highly accurate at the cost of overall coverage. Whereas if the
component F2 is used as the fitness function, the rule systems generated tend to have a

872

S.E. Bedingfield and K.A. Smith

higher classification coverage at the cost of classification accuracy. A linear
combination of the two components F1 and F2 using the parameter lambda has been
used to introduce greater versatility to the fitness function.
Parameter Values
For the experiments reported upon in this paper, the fixed parameter values listed in
table 1 were used
Table 1. Parameter values
Parameter name
PopSize

Value
10

MaxAtoms
RulesPerSet
ChgComponent
SwapAtom
DelRule

6
10
0.5
0.2
0.5

AddRule
AddAtom
CrossOver

1
0.2
0.5

Explanation
The number of rule sets in each population
The maximum initial number of atoms in each
rule
The initial number of rules in each rule set
Probabilty of changing a component of an atom
Probabilty of swapping atoms between rules
Probabilty of deleting rule from a rule set
The probability of adding a new rule to a rule set
– in this case a new rule is always added. Any
empty rules are deleted.
Probability of adding an atom to a rule
Probabilty of crossing two rulesets

3 Experimental Design and Data Set
3.1

The Data

The data on which the rule system generation is based is the Iris database which
originated with Fisher [6]. This data set was chosen because the target attribute is
multiclass and also the data set has been widely reported upon in the literature. The
data contains 150 examples of irises belonging to three different species, iris setosa,
iris versicolor and iris virginica. There are 5 attributes available for modelling the iris
species. These include species, sepal width and length, petal width and length. There
is an equal distribution of cases amongst the three species. For this experiment a 5
fold cross validation was performed on the data to establish the viability of the
technique on a dataset with a non-binary valued consequence attribute. A 5 fold cross
validation was then performed using See5 with the same test and training sets, and a
comparison was made between the two techniques.
5-Fold Cross-Validation
Five distinct training and test sets containing 80% and 20% of the total data set
respectively were randomly generated from the data set. Each pair of sets in turn was
used to test the classification accuracy of the of the rule system generated by CS-

Evolutionary Rule Generation Classification and Its Application to Multi-class Data

873

Classifier and compared with the classification accuracy of decision tree generated by
See5.
CS-classifier was run for each training set, for each value of λ = 0, .2, .4 .6 .8, 1. For
the following comparisons, the most accurate rule set for each corresponding test set
was then selected.
There is one major difference in the rule sets that are generated by CS-classifier and
See5.
The rules generated by See5 provide exactly one classification for all data points in
the training/test sets. Whereas the rules generated by CS-classifier may provide zero,
one or many classifications for any given data point in the training/test sets. Thus
there are
there are four ways of calculating the error rates for the CS-classifier predictions:
1.
2.

3.

4.

error rate = (number of misclassifications)/(total number of data points
classified)
error rate = (number of misclassifications)/(total number of data points). In
this case, if for a specific data point there is no rule providing a prediction,
this is interpreted as a misclassification.
error rate = (total number of misclassifications using most accurate
rule)/(total number of classifications). In this case, if for a specific data point
there is a rule which provides a correct classification and there are no rules of
greater accuracy providing an incorrect classification, then this is interpreted
as a correct classification.
error rate = (total number of misclassifications)/(total number of data points).
In this case also, if for a specific data point there is a rule which provides a
correct classification and there are no rules of greater accuracy providing an
incorrect classification, then this is interpreted as a correct classification.

4 Results
The error rates for training and test sets using each of the four interpretations are
listed in tables 2 and 3 respectively.

Table 2. Error rates for training sets
Method
See5
1. CS-classifier
2. CS-classifier
3. CS-classifier
4. CS-classifier

Set 1

Set 2

Set 3

Set 4

Set 5

mean

Standard
deviation

2.5
3.57
10.00
2.68
9.17

1.67
1.75
6.67
0
5

2.5
4.00
20
3
19.17

2.5
5.98
8.33
4.27
6.67

2.5
5.08
6.67
3.39
5.00

2.33
4.08
10.33
2.67
9.00

0.37
1.61
5.58
1.61
5.93

874

S.E. Bedingfield and K.A. Smith
Table 3. Error rates for test sets

Method
See5
1. CS-classifier
2. CS-classifier
3. CS-classifier
4. CS-classifier

Set 1

Set 2

Set 3

Set 4

Set 5

mean

Standard
deviation

0
0
0
0
0

13.33
0
10
0
10

6.67
6.67
6.67
3.33
3.33

6.67
3.33
3.33
0
0

13.33
6.9
10
3.45
6.67

8.00
3.38
6.00
1.36
4.00

5.00
3.04
3.89
1.66
3.89

Observing Tables 2 and 3 we see that Training accuracy results for See5 are much
more consistent than those for CS-classifier. Also, error rates for See5 on the training
sets appear lower than on the test sets. Conversely, error rates for CS-classifier on the
test sets appear lower than on the training sets.
An analysis of variance was performed between the error rates for See5 on the test
sets and the error results for CS-classifier using the four separate methods for
assessing the error rate. It was found that for methods 1,2 and 4 there is no statistical
difference in error rates, (p-values being .081, .456 and .152 ) between the two
techniques. However if method 3 is used to assess the error rate for CS-classifier, the
analysis indicates an improvement in the classification accuracy (with p-value .011).
For training set 1, the rule derived from the decision tree generated by See5 is as
follows:
If petal-length <= 190: Iris-setosa (40,0)
Else if petal-length > 190:
if petal-width > 170: Iris-virginica (39,1)
else if petal-width <= 170:
if .petal-length <= 490: Iris-versicolor (35,1)
else if petal-length > 490:
if .petal-width <= 150: Iris-virginica (3,0)
else if petal-width > 150: Iris-versicolor (3,1)

The numbers in brackets indicate the number correctly classified followed by the
number misclassified by the rule at each branch. This rule completely classifies test
set 1 correctly.
In comparison, one of the rule sets generated by CS-classifier using training set 1 is as
follows:
Rule 1: (petal_length<239) and (petal_width<=95) then Iris setosa (40,0)
Rule 2: (petal_width>91) and (petal_width<=172) and (sepal_length<=704) then Irisversicolor (40,4)
Rule 3: (petal_width>182) then Iris-virginica (29,0)
Rule 4: (petal_length>517) then Iris-virginica (28,0)
This rule set also correctly classifies test set 1. The numbers in brackets indicate the
total number classified followed by the number misclassified by the rule. Some brief
observations on each of the rules follows.

Evolutionary Rule Generation Classification and Its Application to Multi-class Data

875

Rule 1 of CS-Classifier
In this case either condition for rule 1 of CS-classifier would be sufficient to classify
Iris setosa correctly with complete accuracy on the training set. The minimal class
boundary for petal length of 190 has been found by See5. however the class boundary
found by CS-classifier is 239 rather than 190. This is because for the purposes of
classification, 239 works just as well so that fitness measure is not improved with a
value of 190 rather than 239 (in fact any value between 190 and 299 would work just
as well).
Rule 3 and Rule 4 of CS-Classifier
Rules 3 and 4 classify Iris virginica completely with complete accuracy on both the
training set and test set. There is some classification overlap here – i.e. both Rule 3
and Rule 4 correctly classify some of the same data points in the training and test sets.
Rule 2 of CS-Classifier
In this case all three conditions in rule 2 are necessary to ensure the same level of
accuracy (90% on the training set and 100% on the test set).

5 Conclusion
We have demonstrated the viability of the evolutionary algorithm on a multi-class
data set. We have also demonstrated that the classification rules generated by CSclassifier compare favourably, in terms of accuracy, with those generated by See5.
See5 uses a greedy search algorithm which forces classification rules on the training
data. This can be regarded as an advantage as all data points in the test set can then be
given a classification. However, the disadvantages of this is that the rules may well
provide less accurate classifications on test set and the rules produced may provide
less insight into the true nature of the data divisions. Conversely, although CSclassifier is encouraged to classify by the components M1,..,M3 in the fitness function,
it is not forced to classify and will not do so if it means reducing the value of the
fitness function. CS-classifier has the ability to recognise the possible uncertain and
contradictory nature of a data set. The result of this is that using the rules generated by
CS-classifier, we see a decrease in the classification errors on the test set compared to
the training set, rather than the increase of classification errors generated on the test
set compared with the training that we see when we use the rule generated by See5.
The ability of CS-classifier to provide accurate classification rules can be enhanced
by weighting the fitness function components M1-M9. These weights can be defined in
a way that reflects the actual misclassification costs associated with each of the
components [5]. The technique also lends itself to hybridisation with fuzzy and rough
set approaches, which will be the subject of future work.

876

S.E. Bedingfield and K.A. Smith

References
1.
2.
3.

4.

5.

6.

Quinlan, J. Ross, “C4.5:Programs for Machine Learning”, The Morgan Kaufmann Series
in Machine Learning, Pat Langley, Series Editor, 1992.
Information Discovery Inc., “Rules are much More than Decision Trees”,
http://www.datamining.com/trees.htm, access date: 10 Dec., 2002.
Bedingfield, S. E. and Smith, K. A., “Evolutionary Rule Generation in an Information
System”, in Dagli, C. H. et al. (eds.), Intelligent Engineering Systems Through Artificial
Neural Networks, ASME Press, NY, volume 9, pp. 485–492, 1999.
Bedingfield, S. and Smith, K. A., “A Comparison of Fitness Functions for Evolutionary
Rule Generation”, in M. Mohammadian (ed.), Advances in Intelligent Systems: Theory
and Applications, IOS Press, 2000.
Bedingfield, S. and Smith, K. A., “Evolutionary rule generation and its application to
credit scoring”, L.Reznik and V.Kreinovich (eds.), Soft Computing in Measurement and
Information Acquisition, Physica-Verlag, Heidelberg, accepted for publication, 2001.
Fisher,R.A. "The use of multiple measurements in taxonomic problems" Annals of
Eugenics, 7, Part II, 179–188 (1936).

