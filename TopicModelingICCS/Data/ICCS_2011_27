Available online at www.sciencedirect.com

Procedia Computer Science 4 (2011) 2277–2286

International Conference on Computational Science, ICCS 2011

Parallel Computing Flow Accumulation in Large Digital Elevation
Models
Hiep-Thuan Do, S´ebastien Limet, Emmanuel Melin
LIFO–Universit´e d’Orl´eans (France)

Abstract
This paper describes a new fast and scalable parallel algorithm to compute global ﬂow accumulation for automatic
drainage network extraction in large digital elevation models (DEM for short). Our method uses the D8 model
to compute the ﬂow directions for all pixels in the DEM (except NODATA and oceans). A parallel spanning tree
algorithm is proposed to compute hierarchical catchment basins to model the ﬂow of water from a sink (local minima)
moving on DEM to its outlet (ocean, NODATA, or border of DEM). And ﬁnally, based on local ﬂow accumulation and
the hierarchical trees between sinks, we determinate entirely the global ﬂow accumulation. From that, the drainage
networks of DEM can be extracted. Our method does not need any preprocessing like stream burning on the initial
DEM and tends to make the most of incomplete DEMs. Our algorithms are entirely parallel. Eﬃciency and scalability
have been tested on diﬀerent large DEMs.
Keywords: Parallelism, SPMD, Large Datasets, Digital Elevation Models, Minimum Spanning Tree

1. Introduction
Digital elevation models (DEM for short) are an important source of information in GIS applications. It has been
widely used for modeling surface hydrology including the automatic delineation of catchment areas [1, 2], erosion
modeling or automatic drainage network extraction [3]. All these computations are linked to the determination of ﬂow
direction [4] and then to the calculation of ﬂow accumulation [5]. Moreover, ﬂow accumulation is specially important
to understand topographic controls on water, carbon, nutrient and sediment ﬂows within and over full watersheds.
Geo-science research deals with increasingly large datasets coming from satellite or air plane LIDAR scans. Finer
DEMs have the advantage to obtain more precise results when delimiting speciﬁc areas or running simulations. On one
hand, data increase, on the other hand, sequential means of computation can not keep pace. Moreover, Geo-scientists
need reasonable computation times compatible with an analysis loop. A solution is to parallelize these computations.
This can be achieved if the algorithms are adapted to allow a good scalability to be able to take beneﬁts of massive
clusters available for scientists.
In this paper we propose a method able to compute in parallel the global ﬂow accumulation for automatic extraction of drainage network from a large DEM. We use a totally parallel algorithm to compute global ﬂow directions [6].
Email addresses: hiep-thuan.do@univ-orleans.fr (Hiep-Thuan Do), sebastien.limet@univ-orleans.fr (S´ebastien Limet),
emmanuel.melin@univ-orleans.fr (Emmanuel Melin)

1877–0509 © 2011 Published by Elsevier Ltd. Open access under CC BY-NC-ND license.
Selection and/or peer-review under responsibility of Prof. Mitsuhisa Sato and Prof. Satoshi Matsuoka
doi:10.1016/j.procs.2011.04.248

2278

Hiep-Thuan Do et al. / Procedia Computer Science 4 (2011) 2277–2286

(a) DEM as a raster

(b) Grayscale of the raster

Figure 1: Geological and image rasters

This algorithm combines techniques used in diﬀerent ﬁelds such as hydro-geology, image processing and graph theory and is greatly scalable. From the resulting global ﬂow directions, we propose another scalable parallel algorithm
to compute ﬂow accumulation. The results of the paper allow us to oﬀer a more complete parallel tool to extract,
modelize and interpret, hydrological information from large DEM.
This paper is organized as follows. In Section 2, we brieﬂy present the related works for ﬂow routing models in
geo-hydrology and the solution we use. The section 3 describes steps of a parallel algorithm those intermediary data
structure is crucial to our ﬂow accumulation algorithm. This section is the opportunity to introduce important concepts
necessary to understand the following. On this basis, Section 4 presents how to perform a parallel computation of ﬂow
accumulation on sinks which are homogeneous parts of the DEM. In Section 5, we present the parallel computation
of the water ﬂow through the global DEM. Experimental results are sketched in Section 6. Finally, we close with a
conclusion.
2. Background and Previous work
Digital Elevation Model can be used to represent topography in GIS applications. Data of DEM is generally
stored in one of the following data structures: (1) regular grid structure, (2) triangular irregular network (TIN for
short) structure and (3) contour-based structure [7]. In this paper, we focus onto the grid-type DEMs i.e. 2D grids
that store elevation data for each coordinate of the terrain as illustrated in Figure 1(a). Such a 2D grid is also called a
raster, and a cell of this grid is called a pixel by analogy of the grayscale images (see Fig. 1(b)).
DEMs can be used to extract the hydrographic network making the assumption that rivers ﬂow more probably
into thalwegs and do not cross crests. Three steps are classically used. The Step 1 determines a raster coding ﬂow
direction for each pixel. This produces a forest of trees which root is a local minimum as in Figure 2(a). The Step 2
determines a raster coding accumulated ﬂows i.e. a raster where the value of a pixel (x, y) represents the number of
cells of the DEM that are drained to (x, y). Finally the Step 3 determines a raster segmentation between river pixels
and other ones, classically with the use of a ﬁxed threshold. In this paper, we focus on Step 2 since a parallel solution
for Step 1 can be found in [6] and since a parallel solution for Step 3 is straightforward.
In the ﬁrst step, ﬂow direction is the key point to perform an hydrological analysis onto DEM. It simulates the
way the outﬂow from a given cell will be distributed to one or more neighbouring downslope cells. In the ﬂow routing
models, the potential ﬂow directions are assigned to each cell. The ﬂow directions are then used for modeling the
direction where water ﬂows to the cells of the terrain. Several ﬂow routing models have been proposed such as the D8
based on slope gradient [2], the Rho8 developed by [3], the FD8 in [8], the D∞ [4]. In all these cases, decisions are
taken from local informations and potential parallelism is not aﬀected by the choice of the method.
Unfortunately real datasets are not perfect, they include many incoherences like no-data, plateaus and sinks. In
fact, raw DEMs like those provided by the United States Geological Survey(USGS for short), consist of a multitude
of small sinks and plateaus. A plateau is a ﬂat area with at least one spill-pixel. Authors generally chose to assign
ﬂow direction such as all pixels belonging to the plateau will ﬂow to one spill-pixel. A sink is an area without
spill-points . In this case the problem is that water accumulates in the local minimum. In this case, it is needed to
determine the global direction of the ﬂow and allow water to climb up the hill and to escape the sink. Since in both
cases (plateaus and sinks), ﬂows need to be redirected, and since uphill ﬂow is counter-intuitive, many authors choose
to modify initial datasets to obtain artifact-less raster called in image-computing lower complete [2, 9, 10, 11]. In

Hiep-Thuan Do et al. / Procedia Computer Science 4 (2011) 2277–2286

2279

hydrology, modiﬁcations of data, like stream burning [12, 13], to better reﬂect known hydrology can be useful but
may compromise the ability to derive catchment parameters and conduct to a second terrain analysis [14].
Once catchment basins of rivers are found, it remains to compute ﬂow accumulation. Sequential approaches use
a recursive approach to compute accumulations onto contributing area of each cell. This method is simple but it
requires large amount of memory and is not well suited for large datasets. In [15], Wallis and al. propose to abandon
recursive functions to a queue-based approach that can work concurrently on several partition of the initial terrain.
This solution requires strong synchronizations between the processors to maintain boundary pixels shared by nodes of
the parallel architecture. Nevertheless it makes possible to use all the memory of a PC cluster and this allows scalable
ﬂow computation for large datasets. TerraFlow oﬀers a way to compute ﬂow accumulation of large terrains onto a
single computer via I/O and CPU eﬃcient algorithms [16, 17]. For such an approach, the only way to reduce the
computation time of a ﬁxed dataset is to buy a more powerful computer since it does not take beneﬁts from scalability
of parallel architectures.
In this paper, we show how to use a hierarchical approach to compute in parallel the global ﬂow accumulation of
the DEM. Our algorithm is totally parallel. Pixels are not directly synchronized via communications. Instead, we only
use light weight data structure in memory and we only communicate high level representation of data between nodes
of the parallel architecture.
3. Water path computation
In this section, we brieﬂy present the parallel watershed extraction of DEM, more details can be found in [6]. This
method consists in constructing a minimal spanning tree (MST) onto a graph that represents the global water ﬂow
direction. The minimal spanning tree represents the easiest way to go from one sink of the DEM to the sea.
First, we introduce some deﬁnitions necessary in the following. A digital square grid G with domain D ⊆ Z 2
containing values of type S where S can be any set (typically S is R for DEM) can be considered as a special kind
of graph, where the vertices are called points or pixels. The height values of each vertex v is denoted hv . G can
be endowed with a graph structure G = (V, E) by taking for V the domain D, and for E the set {((x1 , y1 ), (x2 , y2 ), w)|
(x1 , y1 ) and (x2 , y2 ) are connected and the weight of the edge w is max(h(x1 ,y1 ) h(x2 ,y2 ) )}. We denote ≺ the lexicographical
order on pairs of integer, i.e (i, j) ≺ (k, l) iﬀ ( j < l) or (( j = l) and (i < k)). The order ≺ is used to choose one minimal
edge when several are possible. The connectivity in the grid may be either 4-connectivity, or 8-connectivity. We call
this ﬁrst graph G0 . Notice that G0 is the DEM itself and is treated such as by our implementation since it is the most
compact representation of this graph that may contains several millions of vertices. Other graphs are represented by
classical adjacency lists.
We choose a block-distribution for vertices of G0 . The ﬁgure 2(b) gives an example of distribution. The DEM is
distributed onto 6 processors which domains are delimited by the bold lines. The neighbors pixels are represented,
they do not belong to the ﬁrst processor but they are distributed on it to minimize communications.
The method consists in selecting for each vertex v the lightest edge (v, v , w) if and only if hv < hv . If several
edges may be selected, we choose (v, v , w) such that hv is minimal (water goes down the steeper slope). If we ﬁnd
several equal hv value then v is the ﬁrst in the lexicographical order ≺. Selecting the lightest edge models the ﬂooding
process, i.e. when pouring the basin represented by v it will overﬂow via the lowest border (represented by the weight).
Enforcing that hv < hv models the rain falling, i.e. when the basin overﬂows, the water should fall down. The ﬁrst
graph, represented by the arrows of Figure 2(a), is call initial ﬂow direction graph.
Next, the vertices are labelled to detect connected components of the graph restricted to the selected edges. This
is implemented via a parallel approach, each processor computes locally its connected components, each component
is identiﬁed by its root called sink (i.e. the minimal vertex in the component w.r.t. height values deﬁned above).
This connected component is the catchment basin (CB) of the sink. For the sake of conciseness, when there is non
ambiguity, we name sink the catchment basin. One connected component may be distributed on several nodes. This
problem is resolved via the construction of local graphs, illustrated in Figure 2(c), and updates of data via global
exchanges phases to obtain a coherent global dependency graph distributed onto all processors.
We obtain a minor of the initial graph. Indeed, each connected component can be considered as a vertex of another
graph which is a minor of the initial graph. Finally each processor broadcasts the connectivity of the new vertices of
the minor that have external edges. In this way, each processor is able to know the whole connectivity of vertices it is
in charge of.

2280

Hiep-Thuan Do et al. / Procedia Computer Science 4 (2011) 2277–2286

0

1

2

3

4

5

6

7

8

9

10

0

1

2

3

4

5

6

0

1

2

3

4

5

6

7

8

9

10

0

1

2

3

4

5

6

7

8

9

10

0

0

20

15

20

17

16

15

14

15

16

17

0

0

20

15

20

17

16

15

0

0

0

1

1

9

9

2

2

2

3

3

0

0

0

1

1

2

2

2

2

2

3

3

1

0

20

20

20

17

16

16

15

14

15

16

1

0

20

20

20

17

16

16

1

0

0

1

1

9

10

3

3

3

3

3

1

0

0

1

1

2

2

3

3

3

3

3

2

0

20

18

18

18

17

19

13

15

14

15

2

0

20

18

18

18

17

19

2

0

0

0

11

10

10

3

3

3

3

3

2

0

0

0

0

2

2

3

3

3

3

3

3

0

0

18

16

19

18

20

20

20

20

20

3

0

0

18

16

19

18

20

3

0

0

0

11

12

13

3

3

3

3

3

3

0

0

0

0

4

4

3

3

3

3

3

4

0

0

18

15

16

14

16

19

15

17

14

4

0

0

18

15

16

14

16

4

14

15

16

15

4

4

20

5

5

6

6

4

0

0

0

0

4

4

4

5

5

6

6

5

0

0

13

16

18

10

18

17

15

18

13

5

5

14

14

15

15

4

4

21

5

5

6

6

5

0

0

0

0

4

4

4

5

5

6

6

6

0

0

18

18

18

17

18

17

16

18

16

6

6

14

14

14

15

4

4

22

5

5

6

6

6

0

0

0

0

4

4

4

5

5

6

6

7

0

0

17

17

17

17

17

17

17

17

17

7

7

14

14

14

17

18

19

23

24

25

26

27

7

0

0

0

7

7

7

7

7

8

8

8

7

8

9

10

8

0

0

9

8

9

10

11

12

13

14

15

8

8

28

29

30

7

7

7

31

8

8

8

8

8

0

0

0

7

7

7

7

8

8

8

8

9

0

0

9

8

5

10

11

12

10

14

15

9

9

28

28

29

7

7

7

32

8

8

8

8

9

0

0

0

7

7

7

7

8

8

8

8

10

0

0

9

8

9

10

11

12

13

14

15

10

10

28

28

28

7

7

7

33

8

8

8

8

10

0

0

0

7

7

7

7

8

8

8

8

(a) Flow graph onto the DEM

(b) Local Domain

(c) Local labeling for borders

(d) Result

Figure 2: Parallel process

(a) Graph F1

(b) Graph F2

(c) Graph F3

Figure 3: Diﬀerent graphs computed at each step

.
Figure 2 illustrates the computation of one level of the hierarchy. Figure 2(a) gives the initial graph, the number in
the cells are the elevation data and the arrows represent the ﬂow direction. Figure 2(d) gives the result i.e. a labelling
of the pixel that determine the diﬀerent connected component of the graph restricted to the selected edges.
The process is repeated until obtaining a single component, this produces a hierarchy of minors of the initial
graph. The ﬁnal MST is the union of the edges selected at each iteration. Browsing this MST from the lower point,
it is easy to orient it and obtain a hierarchical tree. Figure 3 illustrates the computation of the whole hierarchy (self
arrows are to be ignored) on the DEM given Figure 1(a). We remark that the sink labelled 0 (which ﬁgures the sea)
grows rapidly until encompassing the whole DEM. The resulting hierarchy does not ﬁt our needs since we cannot
distinguish catchment basins we are looking for. This is why the hierarchy construction is driven to avoid merging of
river catchment basins. More details can be found in [6, 18].
4. Parallel computing of ﬂow accumulation inside each sink
In this part we present our parallel algorithm to compute ﬂow accumulation local to each sink. The result of this
step is a 2D grid FA called the ﬂow accumulation matrix that gives for each pixel p the ﬂow accumulation taking into
account only the internal accumulation of the sink it belongs to. More formally, given a DEM D and its initial ﬂow
direction graph FG, the ﬂow accumulation of a pixel p is the number of pixels p such that there is a path from p to
p in FG if p is neither nodata nor belongs to the sea.
The data distribution is the same as in Section 3. The block aﬀected to node i is called the domain Di of i. We call
extern neighboring the set of all pixels p such that there exists a pixel p in the local domain and the edge (p, p ) is in
the initial ﬂow direction graph. For a processor i, the set of pixels having extern neighbors is denoted Bi and is called
the border of Di . To optimize computations et communications, we distribute to each node its extern neighboring.
This is called the extension area.

Hiep-Thuan Do et al. / Procedia Computer Science 4 (2011) 2277–2286

2281

Algorithm 1: Computing local ﬂow accumulation at
p ∈ Di
1 procedure computeFAL(p ∈ Di )
2 begin
3
Stack = ∅;
4
p → Stack;
5
canCompute = true;
6
7
while ((stack ∅) and (canCompute)) do
8
p ← Stack;
9
if (∀ c ∈ Children(p ), FAi [c] = ∞) then
10
FAi [p] = 0;
11
for (∀ c ∈ Children(p )) do
12
FAi [p ] = FAi [p ] + FAi [c] + 1;
13
end

end
else

14
15
16
17

p → Stack;
for ((∀c ∈ Children(p ) s.t. FAi [c]=∞) and
(canCompute)) do
if (lea f (c)) then FAi [c] = 0;
else
if (c ∈ Di ) then c → Stack;
else canCompute = false;
end
end

18
19
20
21
22
23
24
25
26
27

Algorithm 2: Computing local ﬂow accumulation in
Di
1 procedure computeFAL(Di )
2 begin
3
// Computing FAL at border Bi of Di
4
repeat
5
for ( ∀b ∈ Bi ) do
6
if (FA[b] = ∞) then
7
computeFAL(b) in Algorithm 1;
8
end
9
10
11
12
13
14
15
16
17

end
Global synchronization;
Global Exchange of FA[Bi ] for all processors ;
until (FA[p] ∞, ∀p ∈ Bi ) ;
// computing FAL for all other pixels q in Di
for (∀q ∈ Di s.t. FAi [q] = ∞) do
computeFAL(q) in Algorithm 1;
end

18 end

end
end
if (not canCompute) then Stack = ∅;

28 end

Each processor initializes the ﬂow accumulation value of all the pixels of Di to ∞. Then, they compute the
initial ﬂow direction graph using ﬂow routing model D8 illustrated in [2]. For each processor i, we obtain a local
ﬂow direction graph denoted FGi , using the method described in section 3 (see also [18]). Notice that 0≤i≤n FGi
is exactly the ﬂow direction graph of the whole DEM since all processors have all the information to determine the
direction of the ﬂow from each pixel of its local domain. For a pixel p of D, Children(p) is the set of pixels p such
that p goes to p in FG and lea f (p) is true iﬀ Children(p) = ∅.
As seen in previous section, the ﬂow graph FGi is a forest of trees rooted by the sinks. Each tree being the
catchment basin of a sink. Hence, the modeling of the watercourse in each catchment basin can be based on the FGi .
The parallel implementation for local ﬂow accumulation computation is described in Algorithm 2. Each processor
i computes the local ﬂow accumlation of FA only for the pixels of Di (FAi denotes the corresponding part of FA).
Note that due to data distribution onto processors, the catchment basin of a sink s, denoted CBs , can be shared in
several processors i. In this case, the local data at border Bi of each processor i must be exchanged with neighbouring
processors. These exchange phases imply global barriers between all processors which may be very ineﬃcient since
it may need several exchanges depending on shape or size of the catchment basin and the number of processors.
Therefore, in order to avoid useless waits, we do not mix computation for pixels p ∈ Bi and others one. We ﬁrst
compute local ﬂow accumulation for all pixels p ∈ Bi of subdomains Di to concentrate the communication during this
phase. After that, each processor can perform remaining ﬂow accumulation computations using Algorithm 1 without
any exchange nor synchronization.
For Algorithm 1, the ﬂow accumulation of p ∈ Di is determinated if and only if all its children c ∈ FGi have been
determinated. The ﬂow accumulation of a leaf in ﬂow graph FGi is equal to zero. The computation for all children
c is naturally recursively realized until ﬂow accumulation of its children is determined. We used a stack of pixels,
denoted S tack, to replace recursion with iteration in the implementation. Each processor uses its own S tack for locally
computing ﬂow accumulation for a given p. In order to compute the local ﬂow accumulation at p in Algorithm 1,
we use a S tack. The pixel p is inserted into S tack. While ﬂow accumulation of p is still indeterninated and can be
computed (not dependent of extern values), a pixel p is taken from S tack, if ﬂow accumulation of all children of p

2282

Hiep-Thuan Do et al. / Procedia Computer Science 4 (2011) 2277–2286

is determined, the ﬂow accumulation at p can be computed as the sum of children ﬂow accumulations. Otherwise,
p is inserted again into S tack. All children c (ﬂow accumulation equal value∞ ) of p in FGi belong in one of these
three cases.
Case 1: The pixel c is a leaf in the FGi , the ﬂow accumulation of c is deternimated by zero. Our implementation doesn’t
need to insert all leaves into S tack.
Case 2: The pixel c belongs to Di , c is then inserted into the S tack;
Case 3: The pixel c is not in Di , ﬂow accumulation computing for p cannot be continued. In this case, the S tack is
set as empty to optimize the use of memory. This is the case when the pixel belongs to a catchment basin that
is shared by several processors. The computation for c will be considered again after the exchanging of ﬂow
accumulation between neighbouring.
5. Global ﬂow accumulation
In this section we describe how to obtain a global result covering all the DEM from the local computation of
Section 4. The local ﬂow accumulation was computed in each sink. Note that this result is local to a given sink but is
not necessarily local to a processor. The main diﬃculty consists in determining the global ﬂow of the entire DEM and
therefore to drive ﬂow uphill to reach progressively the sea. A very similar problem exists when we want to determine
global catchment basins from DEM which are not lower-complete. In Section 3 we propose to construct a minimal
spaning tree (called hierarchical tree or HT) linking sinks together and representing the most probable path of water
(see also [6]). This is done taking into account the lower part of ridges between neighbor sinks. The hierarchical tree
HT obtained can be used for computing global ﬂow accumulation. Our idea consists in computing ﬂow accumulation
between sinks following the hierarchical tree, then, to use this information to update accumulations ﬂow of pixels in
the entire DEM.
5.1. Flow in the hierarchical tree
We turn now to describe the ﬂow accumulation in the hierarchical tree HT . In Figure 4(a), we can see that the sink
A is a leaf of HT , the water would ﬂow from A into its parent B in HT . The ﬂow accumulation of the sink A is added
to the ﬂow accumulation of B. The ﬂow accumulation of B is the sum of its local ﬂow accumulation (the number of
pixels belonging to the sink B) and the sum of the ﬂow accumulation of all sinks children of B. The propagation is
then done from sink B to its parent P in HT until we reached it root R. The diﬃculty is to translate the transfert of the
water accumulated in one sink to its parent in the ﬂow accumulation matrix.
The hierarchical tree HT imposes an order for the computation of ﬂow accumulation of sinks. For example,
consider the HT of Figure 4(a). Assume that the two sinks A and C have ﬁnished to compute their ﬂow accumulation
and propagated it to sink B. Then, the sink B can propagate its ﬂows into its parent sink P. In the second connected
component, sinks I, J are marked as ﬁnished, but sink L is still pending. This means that G has not enough information
to continue computation and therefore it is the same for its parent D.
In our implementation this work is performed at the same time as the step described thereafter and that consists in
updating the ﬂow accumulation matrix to translate water transfers in this matrix.
5.2. Update of ﬂow accumulation of pixels
We describe how we use ﬂow accumulation at the sink level to update accumulations ﬂow at pixels level. Let A
and B be pixel sinks (i.e. local minima) and respectively CBA and CBB their catchment basins. The HT tells that the
water would ﬂow from A into its parent B. It remains to determine the most probable way the water would take to
ﬂow from A to B. In the HT we have stored the lowest points of the watershed between A and B (they are denoted pS
and pD respectively (see Figure 4). Note that these two points are on either sides of the ridge between CBA and CBB .
In our algorithm, we used both ﬂooding under the HT and ﬂooding on the DEM based on ﬂow directions determined
in the ﬂow graph FGi . In our modeling, to escape from CBA the water follows the path from pS to A in FG (denoted
π[pS ,A] ) in the reverse direction, then it goes from pS to pD and ﬁnally follows the path from pD to B in FG (denoted
π[pD,B] ). The idea, is that the points of π[pS ,A] carry all the accumulation of A and the pixels of π[pD,B] accumulate

Hiep-Thuan Do et al. / Procedia Computer Science 4 (2011) 2277–2286

2283

H. T. Do, S. Limet and E. Melin / Procedia Computer Science 00 (2011) 1 10

7

< postoA , Delta >
A

C

I

J

K

Processor j

L

Processor i

< postoB , A, Delta >
Processor j

Processor i

Delta
B

M

N

O

G

E

F

pS
P

T

pD

pS

pD

postoA

pS

pS

pD

D

A

R

A

A
B

(a) The forest of hierarchical tree

postoB

Delta

Delta

(b) T oA on π[pS ,A]

A
B

B

(c) T oB LOC on
π[pD,B]

(d) T oA

B

(e) T oB LOC

Figure 4: Propagation FA from sink A to sink B

their own draining pixels plus the accumulation of A. So we have two types for propagations, called propagation T oA
(Fig. 4(b)) and propagation T oB LOC (Fig. 4(c)).
In the propagation T oA as described in Algorithm 3, the quantity Delta is used to replace for ﬂow accumulation
of all pixels belonging to the descendant path between pS and position of sink A, denoted π[pS ,A] . This is illustrated
in Figure 4(b), where Delta, that is the ﬂow accumulation of sink A, was determined during the steps of computing
local ﬂow accumulation and ﬂow accumulation into the hierarchical tree. In our parallel implementation, the domain
D of DEM is partitioned into subdomains Di that are mapped by processors i. Therefore, the descendant path π[pS ,A]
may not entirely belong to the same subdomain Di . In this case, the propagation T oA on the descendant path π[pS ,A]
is broken when it reaches a neighbor pixel postoA Di (see Figure 4(d)). To solve this problem, for each processor i,
we use a data structure ListS endAi to communicate the propagation T oA from neighbor postoA to the subdomain D j
in which the postoA belongs to. At processor i, a pair pairA < postoA , Delta > is immediately inserted to ListS endAi .
The ListS endAi is then exchanged with others processors (see Figure 4(d)). To optimize the data exchange between
neighboring processors, each processor i only send to processor j all pairA < postoA , Delta > ∈ ListS endAi such that
the pixel postoA belongs to the subdomain D j . And after that, the propagation T oA on the descendant path π[pS ,A] can
be continued from postoA with the quantity Delta in D j in next iteration. The propagation T oA can be repeated in
several cycles local computation/global exchange, until the ListS endAi for all processors i are entirely empty.
Algorithm 3: Propapation T oA from postoA with
Delta on the path π[pS ,A) at Processor i
1 procedure ToA(PairA < postoA , Delta >)
2 begin
3
if (postoA Di ) then
4
pairA = < postoA , Delta >;
5
pairA ⇒ ListS endAi ;
6
7
8
9
10
11
12
13

end
else
mFAi [postoA ] = Delta;
parenttoA = Parenti [postoA ];
while (postoA parenttoA ) and (parenttoA ∈ Di ) do
FAi [parenttoA ] = Delta;
postoA = parenttoA ;
parenttoA = Parenti [postoA ];
end
if (parenttoA Di ) then
pairA = < parenttoA , Delta >;
pairA ⇒ ListS endAi ;
end

14
15
16
17
18
19
20 end

end

Algorithm 4: Propapation T oB LOC for postoB ∈
π[pD,B)
1 procedure ToB LOC(pair B < postoB , A, Delta >)
2 begin
3
FAi [postoB ] = FAi [postoB ] + Delta;
4
parenttoB = Parent[postoB ];
5
while ((parenttoB postoB ) and (parenttoB ∈ Di )) do
6
FAi [parenttoB ] = FAi [parenttoB ] + Delta;
7
postoB = parenttoB ;
8
parenttoB = Parenti [postoB ];
9
end
10
if ((parenttoB Di )) then
11
pairB = < parenttoB , A, Delta >;
12
pairB ⇒ ListS endBi ;
13
return true;
14
15
16
17

end
else

18

end

PassedSinks LOC[A] = ﬁnished;
return false;

19 end

We turn now to the propagation T oB LOC from sink A to its parent sink B. This propagation go through crossing
point pD ∈ CBB (see Figure 4(c)), the quantity Delta is used to increase for all pixels postoB belonging to the
descendant path π[pD,B] . It is described in Algorithm 4. Firstly, we begin the T oB LOC at the crossing point pD of
B. The propagation T oB LOC is continuously realized for next pixel of pD in π[pD,B] . The ﬁnal step depends on the

2284

Hiep-Thuan Do et al. / Procedia Computer Science 4 (2011) 2277–2286
H. T. Do, S. Limet and E. Melin / Procedia Computer Science 00 (2011) 1 10

8

state we obtain.
One one hand, the propagation T oB LOC, < pstoB, A, Delta > has reached B. It means that the propagation
between sink A and sink B on the path π[pS ,B] is ﬁnished. The sink A is then marked as ﬁnished for propagation to its
parent B. This condition is used to decide that the propagation from sink B can be continued for propagating to its
root in HT .
One the other hand, the propagation reaches a neighbor pixel postoB ∈ π[pD,B] , which does not belong to the
domain of processor i (postoB Di ). This means that the propagation from sink A into sink B is suspended, it needs a
communication. Like above, we used a data structure ListS endBi for each processor i for solving the incompletion of
the descendant path π[pD,B] in Di for propagation of ﬂow accumulation (see Figure 4(e)). A pairB < postoB , A, Delta >
is inserted into ListS endBi . We send ListS endBi to processor j. And after that, the propagation is continued onto
processors j.
Algorithm 6: Parallel computing ﬂow accumulation
in Di
Algorithm 5: Propagation T oB GLO with Delta
from pos ∈ CBB to sinkroot R of B in HT
1 procedure ToB GLO(pairB < pos, A, Delta >)
2 begin
3
if ( ToB LOC(pairB )) then
4
P = getParent(B, pS new , pDnew );
5
propagable LOC = true;
6
ToRoot = PropagableToRoot(B);
7
while ((P root) and (propagable LOC) and (ToRoot))

do
newDelta = FAi [B];
ToA(pS new , newDelta) in Algorithm 3;
pairB = < pDnew , B, newDelta >;
if (pDnew Di ) then
pairB ⇒ ListS endBi ;
propagable LOC = false;
end
else
if ( ToB LOC(pairB )) then
propagable LOC = false;
end
else
B = P;
ToRoot = PropagableToRoot(B);
P = getParent(B, pS new ,pDnew );
propagable LOC = true;
end
end

8
9
10
11
12
13
14
15
16
17
18
19
20
21
22
23
24
25
26
27
end
28 end

end

1 procedure computeFAG(Di )
2 begin
3
for (∀ sink A ∈ HT i ) | (A is leaf) do
4
B = getParent(A, pS , pD);
5
Delta = FAi [A];
6
ToA(< pS , Delta >) in Algorithm 3 ;
7
pairB = < pD, A, Delta >;
8
if (pD Di ) then
9
pairB ⇒ ListS endBi ;
10
end
11
else ToB GLO(pairB ) in Algorithm 5;
12
13
14
15
16
17
18
19
20
21
22
23
24
25
26
27
28
29
30
31

end
allFinished = false;
while (not allFinished) do
Global synchronization;
ListRecvAi = {pairA < pS , Delta >∈ ListeS endA j |
pS ∈ Di };
ListRecvBi = {pairB < pD , X, Delta >∈ ListeS endB j
pD ∈ Di };
ListS endAi = ∅ ;
ListS endBi = ∅ ;
for (∀ pairA ∈ ListRecvAi ) do
ToA(pairA ) in Algorithm 3;
end
ListRecvAi = ∅;
for (∀ pairB ∈ ListRecvBi ) do
ToB GLO(pairB ) in Algorithm 5;
end
ListRecvBi = ∅;
allFinishedi = (ListS endAi =∅) and (ListS endBi =∅);
Global Synchronization;
allFinished = numProcs
allFinishedk ;
1
end

32 end

The global propagation for ﬂow accumulation T oB GLO from a pixel pos ∈ CBB into sink root of sink B is
described in Algorithm 5. The function PropagableT oRoot(B) is used to verify that the sink B can be propagated
to its parent in the hierarchical tree HT , a sink B is propagable to its parent in HT if only if all its sinks children of
B are marked as ﬁnished for propagation. The parallel algorithm is ﬁnished when both ListS endAi and ListS endBi
of all processors i are empty. And, the result FAi partitioned in each processor i is the global ﬂow accumulation of
catchment of rivers.

2285

Hiep-Thuan Do et al. / Procedia Computer Science 4 (2011) 2277–2286

Relative speedup with number of processors
2200
2000 ♦ ♦ ♦
1800
1600
1400
1200
Times in seconds
1000
800
600
400
200
0
0
5

(a) Result of our method (ParaFlow)

5
4

♦

❜
♦

log2(SP(N3p ))
♦
❜

2
1

♦❜

0 ♦❜
0

1

10

20
15
Number of processors

25

♦

30

35

❜

❜

+

4
3.5

♦
♦

♦
TerraFlow
ParaFlow

Relative speedup with number of processors

Relative speedup with number of processors
♦

♦

(b) Comparison Computation Time (in seconds) for DEM b
between TerraFlow and ParaFlow

6
Theoretical linear speedup
DEM a with size 3,980 x 5,701
DEM b with size 7,216 x 10,106
DEM c with size 10,086 x 14,786

♦

Theoretical linear speedup for DEM d
DEM d with size 36,002 x 54,002

+
+

3
2.5

❜

+

log2(SP(N p ))2

❜

1.5
+

1
0.5

2

3
log2(N p )

(c) Small DEMs

4

5

6

0+
2

2.5

3

3.5

4
log2(N p )

4.5

5

5.5

6

(d) Large DEMs

Figure 5: Relative speedup with number of processors (N p )

6. Experimental results
The parallel algorithm described in this paper has been implemented in C++ using MPI library and tested on eight
nodes linked with a Gigabit Ethernet network. Nodes are bi-pro with AMD Opteron Quad-Core 2376 2.3GHz with
16GB SDRAM DDR2-PC5300 EEC 667 MHz, and operating system is linux. Figure 5(a) illustrates the result of our
method for the DEM c. That is the drainage networks for Loire Bretagne region in France.
First of all, we compared the quality of the results of our algorithm with those obtained with TerraFlow and the
main leader of commercial GIS software ArcGIS. The three methods gives very similar river networks (no more than
3% of pixels diﬀer between them).
Let N p be number of processors used and T (N p ) the running time. The results, given in Fig. 5(c) and in Fig. 5(d),
exclude data loading, and saving. The running time displayed for each DEM is the avarage of ﬁve runs of our program
on the DEM (The diﬀerence between several runs on the same DEM is less than 0.1%). Note that, due to memory
size limitation, it is not possible to run our program for DEM d (size 36,002×54,002) onto one sole node. In this
case only, we choose to give a speedup relative to the execution time of our algorithm onto four processors. Then the
speedup linear curve start at coordinate (2,0) and is parallel to the classical one (Fig. 5(d)). Note that this is another
important beneﬁt of our parallel algorithm, to allow computation onto large DEM whose size does not ﬁt into the
memory of one sole PC. Looking at Figure 5(c), for DEM a and DEM b, we remark that the relative speedup is close
to linear speedup at the beginning of the curves. With larger DEM (for example DEM d) speedup increases linearly
(Figure 5(d)). This illustrates the good scalability of our approach.
We compared in Figure 5(b), the running time of TerraFlow/GRASS(Geographic Resources Analysis Support
System) to our method to analyze hydrology of DEM b (the results, here, include times for loading and save data).
Note that TerraFlow is a sequential code so its time duration is represented by on line in Figure 5(b). Even with one

2286

Hiep-Thuan Do et al. / Procedia Computer Science 4 (2011) 2277–2286

processor our method is faster than TerraFlow on this DEM and with 8 processors our method is about ten times faster.
7. Conclusion
In this paper, we presented an eﬃcient and scalable fully parallel algorithm to compute the global ﬂow accumulation. This method allows rapid automatic drainage network extraction in very large DEM. The method does not
use too complex data structures to alleviate memory need, moreover data are distributed onto the cluster. The method
takes into account sea and border of the DEM which may belong to catchment basin of river outside of the datasets.
The results we obtain, from hydrology point of view is very close to those computed by classical software.
We proposed a SPMD parallel implementation onto a PC cluster. We used it on a large DEM, and obtained good
speedups and computation times for huge datasets. The running time for hudge DEMs on a pretty small cluster is of
the same order than the classical GIS processing times for small datasets, with desktop computers. This shows the
interests of such methods with regard to out-of-core algorithms.
The methodology we use to compute the global ﬂow accumulation may apply on many classical processings on
DEMs. Therefore we are working on giving a more general framework to ease implementation those processings in
parallel. We are also working on improving the scalability of our framework when the user wants to store intermediate
results. Each of these results is as big as the initial DEM and the data are distributed over the nodes of the cluster
which needs to eﬃciently parallelize the I/O.
Acknowledgments
The authors would like to thank the town of Orl´eans, the Conseil General du Loiret, the region Centre who granted
this work. We would like also to thank the Compagny G´eo-Hyd that provided the digital elevation models.
References
[1] L. W. Martz, E. D. Jong, Catch: a fortran program for measuring catchment area from digital elevation models, Comput. Geosci. 14 (5) (1988)
627–640. doi:http://dx.doi.org/10.1016/0098-3004(88)90018-0.
[2] J. F. O’callaghan, D. M. Mark, The extraction of drainage networks from digital elevation data, Computer Vision, Graphics and Image
Processing 28 (1984) 328–344. doi:http://dx.doi.org/10.1016/S0734-189X(84)80011-0.
[3] J. Fairﬁeld, P. Leymarie, Drainage networks from grid digital elevation models, Water Resources Research 27 (5) (1991) 709–717.
[4] D. G. Tarboton, Terrain analysis using digital elevation models (TauDEM), Utah State University, Logan, UT, USA (2002).
[5] T. K. Peuker, D. H. Douglas, Detection of surface-speciﬁc points by local parallel processing of discrete terrain elevation data, Computer
Graphics and Image Processing 4 (4) (1975) 375–387.
[6] H.-T. Do, S. Limet, E. Melin, Parallel computing of cachment basins of rivers in large digital elevation models, The International Conference
on High Performance Computing and Simulation (HPCS-2010 Caen).
[7] M. V. Kreveld, Digital elevation models: overview and selected tin algorithms, in: M. van Kreveld, J. Nievergelt, T. Roos, P. Widmayer
(Eds.), Algorithmic Foundations of GIS, Vol. 1340 of LNCS, Springer-Verlag, 1997.
[8] P. Quinn, K. Beven, P. Chevallier, O. Planchon, The prediction of hillslope ﬂow paths for distributed hydrological modelling using digital
terrain models, Hydrological Processes 5 (1991) 9–79.
[9] J. Garbrecht, L. W. Martz, The assignment of drainage direction over ﬂat surfaces in raster digital elevation models, Journal of Hydrology
193 (1997) 204–213. doi:http://dx.doi.org/10.1016/S0022-1694(96)03138-1.
[10] A. Tribe, Automated recognition of valley lines and drainage networks from grid digital elevation models: A review and a new method,
Journal of Hydrology 139 (1992) 263–293.
[11] J. B. T. M. Roerdink, A. Meijster, The watershed transform: Deﬁnitions, algorithms and parallelization strategies 41 (1-2) (2001) 187–228.
URL http://www.cs.rug.nl/ roe/publications/parwshed.pdf
[12] M. F. Hutchinson, A new procedure for gridding elevation and stream line data with automatic removal of spurious pits, Journal of Hydrology
106 (3-4) (1989) 211–232. doi:http://dx.doi.org/10.1016/0022-1694(89)90073-5.
[13] O. Planchon, F. Darboux, A fast, simple and versatile algorithm to ﬁll the depressions of digital elevation models, CATENA 46 (2–3) (2002)
159–176. doi:10.1016/S0341-8162(01)00164-3.
[14] J. N. Callow, K. P. V. Niel, G. S. Boggs, How does modifying a dem to reﬂect known hydrology aﬀect subsequent terrain analysis?, Journal
of Hydrology 332 (1-2) (2007) 30–39. doi:http://dx.doi.org/10.1016/j.jhydrol.2006.06.020.
[15] C. Wallis, D. Watson, D. Tarboton, R. Wallace, Parallel ﬂow-direction and contributing area calculation for hydrology analysis in digital
elevation models, in: PDPTA, 2009, pp. 467–472.
[16] L. Arge, J. S. Chase, P. Halpin, L. Toma, J. S. Vitter, D. Urban, R. Wickremesinghe, Eﬃcient ﬂow computation on massive grid terrain
datasets, Geoinformatica 7 (4) (2003) 283–313. doi:http://dx.doi.org/10.1023/A:1025526421410.
[17] L. Arge, L. Toma, J. S. Vitter, I/o-eﬃcient algorithms for problems on grid-based terrains, J. Exp. Algorithmics 6 (2001) 1.
[18] H.-T. Do, S. Limet, E. Melin, Parallel computing of catchment basins in large digital elevation model, in: HPCA (China), Vol. 5938 of LNCS
- Lecture Notes in Computer Science, Springer-Verlag, Berlin Heidelberg, 2009, pp. 133–138.

