Procedia Computer Science
Volume 80, 2016, Pages 941–950
ICCS 2016. The International Conference on Computational
Science

On Solving Ill Conditioned Linear Systems
Craig C. Douglas1 , Long Lee2 , and Man-Chung Yeung2
1
2

University of Wyoming, School of Energy Resources, Laramie, WY, USA
craig.c.douglas@gmail.com
University of Wyoming, Department of Mathematics, Laramie, WY, USA
{llee,myeung}@uwyo.edu

Abstract
This paper presents the ﬁrst results to combine two theoretically sound methods (spectral
projection and multigrid methods) together to attack ill conditioned linear systems. Our preliminary results show that the proposed algorithm applied to a Krylov subspace method takes
much fewer iterations for solving an ill conditioned problem downloaded from a popular online
sparse matrix collection.
Keywords: numerical analysis, scientiﬁc computing, multigrid, nonlinear data driven applications

1

Introduction

It is well known that robustness and eﬃciency of iterative methods are aﬀected by the condition
number of a linear system. When a linear system has a large condition number, usually due
to eigenvalues that are close to the origin of the spectrum domain, iterative methods tend to
take many iterations before a convergence criterion is satisﬁed. Sometimes, iterative methods
will fail to converge within a reasonable computer elapsed time, or even do not converge at all,
if the condition number is too large. Unstable linear systems, or systems with large condition
numbers, are called ill conditioned. For an ill conditioned linear system, slight changes in the
coeﬃcient matrix or the right-hand-side cause large changes in the solution. Typically, roundoﬀ
error in the computer arithmetics can cause instability when attempts are made to solve an ill
conditioned system either directly or iteratively on a computer.
It is widely recognized that linear systems resulting from discretizing ill posed integral
equations of the ﬁrst kind are highly ill conditioned. This is because the eigenvalues for the ﬁrst
kind integral equations with continuous or weakly singular kernels have an accumulation point
at zero. Integral equations of the ﬁrst kind are frequently seen in statistics, such as unbiased
estimation, estimating a prior distribution on a parameter given the marginal distribution of
the data and the likelihood, and similar tests for normal theory problems. They also arise from
indirect measurements and nondestructive testing in inverse problems. Other ill conditioned
linear systems can be seen in training of neural networks, seismic analysis, Cauchy problem for
Selection and peer-review under responsibility of the Scientiﬁc Programme Committee of ICCS 2016
c The Authors. Published by Elsevier B.V.

doi:10.1016/j.procs.2016.05.386

941

On Solving Ill Conditioned Linear Systems

Douglas, Lee, and Yeung

parabolic equations, and multiphase ﬂow of chemicals. For pertinent references of ill conditioned
linear systems, see Engl [17] and Groetsch [22].
Solving these ill conditioned linear algebra problems becomes a long standing bottleneck for
advancing the use of iterative methods. The convergence of iterative methods for ill conditioned
problems, however, can be improved by using preconditioning. Development of preconditioning
techniques is therefore a very active research area. A preconditioning strategy that deﬂates
few isolated external eigenvalues was ﬁrst introduced by Nicolaides [27], and investigated by
several others [26, 36, 39, 20]. The deﬂation strategy is an action that removes the inﬂuence of
a subspace of the eigenspace on the iterative process. A common way to deﬂate an eigenspace
is to construct a proper projector P as a preconditioner and solve
P, A ∈ CN ×N .

P Ax = P b,

(1)

The deﬂation projector P , which is orthogonal to the matrix A and the vector b against some
subspace, is deﬁned by
P = I − AZ(Z H AZ)−1 Z H ,

Z ∈ CN ×m ,

(2)

where Z is a matrix of deﬂation subspace, i.e., the space to be projected out of the residual,
N and (2)
and I is the identity matrix of appropriate size [30, 20]. We assume that (1) m
Z has rank m. A deﬂated N ×N system (1) has an eigensystem diﬀerent from that of Ax = b.
Suppose that A is diagonalizable, and set Z = [v1 , · · · , vm ], whose columns are eigenvectors
of A associated with eigenvalues λ1 , · · · , λm . Then the spectrum σ(P A) would contain the
same eigenvalues of A, except λ1 , · · · , λm . Usually, eigenvectors are not easily available. This
motivates us to develop an eﬃcient and robust algorithm for ﬁnding an approximate deﬂation
subspace, other than using the exact eigenvectors to construct the deﬂation projector P .
Suppose that we want to deﬂate a set of eigenvalues of A enclosed in a circle Γ that is
centered at the origin with the radius r. Without loss of generality, let this set of eigenvalues
be {λ1 , · · · , λk }. Let the subspace spanned by the corresponding eigenvectors of {λ1 , · · · , λk }
be Zk = Span{v1 , · · · , vk }. Then the deﬂation subspace matrix Z in (2) obtained by randomly
selecting m vectors from Zk can be written as a contour integral [31]
Z=

1
√
2π −1

Γ

(zI − A)−1 Y dz,

(3)

where Y is a random matrix of size N ×m. If the above contour integral is approximated by a
Gaussian quadrature, we have
q

Z=

ωi (zi I − A)−1 Y,

(4)

i=1

where ωi are the weights, zi are the Gaussian points, and q is the number of Gaussian points
on Γ for the quadrature. It is worth noting that (4) is required to solve q shifted linear systems
(zi I − A)X = Y , i = 1, · · · , q. Using (4) for the deﬂation projector P in (2), the preconditioned
linear system (1) is no longer severely ill conditioned.
We remark that the construction of a deﬂation subspace matrix Z through (4) is motivated
by the works in [33, 34, 28, 37].

2

Methodology

We consider the solution of the linear system
Ax = b
942

(5)

On Solving Ill Conditioned Linear Systems

Douglas, Lee, and Yeung

by a Krylov subspace method, where we assume that A ∈ CN ×N is nonsingular and b ∈ CN .
Let an initial guess x0 ∈ CN be given along with its residual r0 = b − Ax0 . A Krylov subspace
method recursively constructs an approximate solution, xj , such that
xj ∈ x0 + Kj (A, r0 ) ≡ x0 + span{r0 , Ar0 , . . . , Aj−1 r0 },
and its residual rj = b − Axj satisﬁes some desired conditions. It is well known that the
convergence rate of a Krylov subspace method depends on the eigenvalue distribution of the
coeﬃcient matrix A. A variety of error bounds on rj exist in the literature. Let us take GMRES
[32] as an example.

2.1

GMRES

In GMRES, the residual rj is required to satisfy the condition
rj

2

=

min

ξ∈x0 +Kj (A,r0 )

b − Aξ 2 ,

namely, the approximate solution xj obtained at iteration j of GMRES is optimal in terms of
residual norm. In the case where A is diagonalizable, an upper bound on rj 2 is provided by
the following result.
Theorem 1. ([30, Corollary 6.33]) Suppose that A can be decomposed as
A = V ΛV −1

(6)

with Λ being the diagonal matrix of eigenvalues. Let E(c, d, a) denote the ellipse in the complex
plane with center c, focal distance d, and semi-major axis a (see Fig. 1(a)). If all the eigenvalues
of A are located in E(c, d, a) that excludes the origin of the complex plane, then
rj
where κ2 (V ) = V

2

V −1

2

2

≤ κ2 (V )

Cj ( ad )
r0
|Cj ( dc )|

2

(7)

and Cj is the Chebyshev polynomial of degree j.

An explicit expression of Cj ( ad )/Cj ( dc ) can be found on p. 207 of [30], and under some
additional assumptions on E(c, d, a) (say, the ellipse in Fig. 1(a)),
Cj ( ad )
≈
Cj ( dc )

√
a + a2 − d2
√
c + c 2 − d2

j

≡ δj .

(8)

The upper bound in (7) therefore contains two factors: the condition number κ2 (V ) of the
eigenvector matrix V and the scalar δ determined by the distribution of the eigenvalues of A.
If A is nearly normal and has a spectrum σ(A) which is clustered around 1, we would have
κ2 (V ) ≈ 1 and δ < 1. In this case, rj 2 decays exponentially in a rate of power δ j , resulting
in a fast convergence of GMRES. The error bound (7) does not tell the whole story, however,
because the convergence rate can also be enhanced if the eigenvalues of A are clustered [38].
Since the ellipse E(c, d, a) in Theorem 1 is required to include all eigenvalues of A, the
outlying eigenvalues may keep the ellipse large, implying a large δ. To reduce δ, we therefore
wish to remove these outlying eigenvalues from σ(A). Any procedure of doing so is known as
deﬂation. GMRES in combination with deﬂation is called Deﬂated GMRES.
943

On Solving Ill Conditioned Linear Systems

Douglas, Lee, and Yeung

Im(z )
1
0.8
0.6

c+a
c−d

c

0.4
0.2

c+d
Re(z )

0
−0.2

c−a

−0.4
−0.6
−0.8
−1
−500

0

500

1000

1500

2000

2500

3000

3500

4000

(b)

(a)

Figure 1: (a) A schematic ellipse in the complex plane with center c, focal distance d, and
semi-major axis a. (b) Eigenvalue distribution of the test matrix bcsstm27.

2.2

Deﬂated GMRES

Suppose x∗ is the exact solution of (5). Let a so-called deﬂation-subspace matrix Z =
[z1 , . . . , zm ] ∈ CN ×m be given, whose columns are linearly independent. Deﬁne the two projectors [39, 20, 41]
P ≡ I − AZ(Z H AZ)−1 Z H

and

P ≡ I − Z(Z H AZ)−1 Z H A,

(9)

where Z H AZ is assumed to be invertible. It is straightforward to verify that P 2 = P, P 2 = P
and P A = AP .
Using P , we split x∗ into two parts:
x∗ = (I − P )x∗ + P x∗ ≡ x∗1 + x∗2 .
For x∗1 , we have
x∗1 = (I − P )x∗ = Z(Z H AZ)−1 Z H Ax∗ = Z(Z H AZ)−1 Z H b.
For x∗2 , we obtain

x∗2 = A−1 P b,

since Ax∗2 = AP x∗ = P Ax∗ = P b. Now, if x# is a solution of the singular system
P Ax = P b,

(10)

then
AP x# = P b

⇔

P x# = A−1 P b = x∗2 .

Based on the above observation, a Deﬂated GMRES algorithm is given in Algorithm 1 [41].
Assume that the nonsingular A ∈ CN ×N has a decomposition (6) with V = [v1 , . . . , vN ] and
Λ = diag{λ1 , . . . , λN }. If we set Z = [v1 , . . . , vm ] in (9), then the spectrum σ(P A) contains all
the eigenvalues of A except λ1 , . . . , λm , namely, σ(P A) = {0, · · · , 0, λm+1 , · · · , λN }.
944

On Solving Ill Conditioned Linear Systems

Douglas, Lee, and Yeung

Choose Z;
Compute x∗1 = Z(Z H AZ)−1 Z H b;
Solve P Ax = P b by GMRES to obtain a solution x# ;
Compute x∗2 = P x# ;
Determine x∗ = x∗1 + x∗2 .
Algorithm 1: Deﬂated GMRES

Perform a QR factorization on V as follows:
V = QR ≡ [Q1 , Q2 ]

R11
0

R12
R22

,

(11)

where Q1 ∈ CN ×m and R11 ∈ Cm×m . If we set Z = [v1 , . . . , vm ] and apply GMRES to solve
(10), an upper bound on rj is given by the following theorem [41].
Theorem 2. Suppose that A has a decomposition (6), and suppose GMRES is used to solve
(10) with Z = [v1 , . . . , vm ]. If all the eigenvalues λm+1 , . . . , λN of A are located in an ellipse
E(c, d, a) which excludes the origin of the complex plane, then
rj

2

≤ κ2 (R22 )

Cj ( ad )
r0 2 .
|Cj ( dc )|

(12)

With (8), the upper bound (12) of the residual norm rj 2 of Deﬂated GMRES is determined
by the condition number of R22 (rather than V ), and the scalar δ which is determined by the
distribution of the undeﬂated eigenvalues λm+1 , . . . , λN of A.

2.3

Spectral Projector and Construction of Z

Spectral projector is described in detail in §3.1.3-§3.1.4 of [31]. Other reference includes [6, 16,
25]. Let A = V JV −1 be the Jordan canonical decomposition of A where
V = [v1 , v2 , . . . , vN ]

and

J = diag{JN1 (λ1 ), JN2 (λ2 ), . . . , JNd (λd )}.

The eigenvalues λi in J are not necessarily distinct and can be repeated according to their
multiplicities, and the diagonal block JNi (λi ) in J is an Ni × Ni Jordan block associated with
the eigenvalue λi .
Let Γ be a given positively oriented simple closed curve in the complex plane. Without loss
of generality, let the set of eigenvalues of A enclosed by Γ be {λ1 , λ2 , . . . , λk }. In other words,
the eigenvalues λk+1 , . . . , λd lie outside the region enclosed by Γ. Set s ≡ N1 + N2 + . . . + Nk ,
the number of eigenvalues inside Γ with multiplicity taken into account. Then the residue
PΓ =

1
√
2π −1

Γ

(zI − A)−1 dz

k

deﬁnes a projection operator onto the space i=1 Null(A − λi I)li where li is the index of λi ,
namely,
Range(PΓ ) = span{v1 , v2 , . . . , vs }.
In particular, if A has a diagonal decomposition (6), PΓ is a projector onto the sum
of the λi -eigenspace Eλi of A.

k
i=1

Eλi

945

On Solving Ill Conditioned Linear Systems

Douglas, Lee, and Yeung

Pick a random matrix Y ∈ CN ×s and set
1
√
Z = PΓ Y =
2π −1

Γ

(zI − A)−1 Y dz

(13)

in (9). Then we almost surely have σ(P A) = {0, · · · , 0, λk+1 , · · · , λd }. Therefore all the eigenvalues of A inside Γ are removed from the spectrum of P A.

2.4

Numerical Examples

In this subsection, we demonstrate the eﬀect of the deﬂation-subspace matrix Z deﬁned by
(13) applied to the solution of the following two test data downloaded from The University of
Florida Sparse Matrix Collection [7]:
(a) bcsstm27 from a mass matrix buckling problem. bcsstm27 is a 1224 × 1224 real symmetric
and indeﬁnite matrix A with 56, 126 nonzero entries. As the right-hand side in (5), we
set b = A1 where 1 = [1, 1, . . . , 1]T . A spectral plot for bcsstm27 is in Figure 1(b).
(b) mahindas from an economic problem. mahindas is a 1258 × 1258 real unsymmetric matrix
A with 7, 682 nonzero entries. Again, we set b = A1 as the right-hand side in (5). A
spectral plot for mahindas is in Figure 2(a).
All the computations were done in Matlab Version 7.1 on a Windows 7 machine with a
Pentium 4 processor. An ILU preconditioner generated by the Matlab function [L, U, P ] =
luinc(A, 0 ) was used for mahindas, namely, instead of solving (5), we solved
˜ = ˜b
Ax
where A˜ = L−1 P AU −1 and ˜b = L−1 P b, and accordingly the A and b in (10) were replaced with
A˜ and ˜b respectively. Since the U factor obtained from luinc had some zeros along its main
diagonal, we replaced those zeros by 1 so that U was invertible. A spectral plot for A˜ is given
in Figure 2(b). On the other hand, we did not use any preconditioner for bcsstm27.
8

10
8

6

6
4
4
2

2

0

0
−2

−2

−4
−4
−6
−6
−8
−8

−8
−6

−4

−2

0

2

4

6

−10
−6

−5

−4

−3

(a)

−2

−1

0

1

2

3

(b)

Figure 2: (a) Eigenvalue distribution of the test matrix mahindas. (b) Eigenvalue distribution
of the ILU(0)-preconditioned mahindas.
Numerical solutions with deﬂated restarted GMRES of the linear systems resulted from the
discretization of the two dimensional steady-state convection-diﬀusion equation
−[uxx + uyy + Re (p(x, y)ux + q(x, y)uy )] = f (x, y),
946

(x, y) ∈ [0, 1]2

(14)

On Solving Ill Conditioned Linear Systems

Douglas, Lee, and Yeung

with Dirichlet boundary conditions were studied in depth in [8]. In [8], two types of delationsubspace matrix Z are used: eigenvectors obtained from the Matlab function eig, and algebraic
subdomain vectors. The Z of algebraic subdomain vectors works well for the ﬂuid ﬂow problem
(14), but not for other problems. Accurately calculating eigenvalues of large linear systems, on
the other hand, is very time-consuming. Therefore deﬂation with the Z of true eigenvectors is
not practicable. Numerical experiments in [8] show that eigenvalues close to the origin hamper
the convergence of a Krylov subspace method. Hence, deﬂation of these eigenvalues is very
beneﬁcial. Based on this observation, we chose in our experiments the Γ in (13) to be a circle
D(c, r) with the center c near the origin. For the Y in (13), we picked a random Y ∈ RN ×m
with m not less than the exact number s of eigenvalues inside Γ. We remark that an eﬃcient
stochastic estimation method of s has been developed in [21]. Moreover, we computed the
integral in (13) by the Legendre-Gauss quadrature
Z=

r
2

1

eπθ

√

−1

((c+reπθ

−1

√

−1

)I−A)−1 Y dθ ≈

r
2

q

ωk eπθk

√

−1

((c+reπθk

√

−1

)I−A)−1 Y, (15)

k=1

where ωk and θk are the Legendre-Gauss weights and nodes on the
interval [−1, 1] with trun√
cation order q. In (15), there are mq linear systems ((c + reπθk −1 )I − A)x = yj to solve.
We solved each of them by BiCG with the stopping tolerance tol = 10−10 and the maximum
number of iterations maxit = N .
In our experiments, we performed the following three computations:
#1 Solve (5) without any deﬂation.
#2 Compute Z through (15). Perform QR factorization on Z: Z = QR where Q ∈ CN ×m
and R ∈ Cm×m . Then set Z = Q in (9), and solve (10).
#3 Use the Matlab function eig to compute the eigenvectors v1 , v2 , . . . , vs of A whose associated eigenvalues lying inside Γ. Pick an M ∈ Rs×m randomly, and set Z =
[v1 , v2 , . . . , vs ]M . Perform QR factorization on Z: Z = QR where Q ∈ CN ×m and
R ∈ Cm×m . Then set Z = Q in (9), and solve (10).
Full GMRES is too expensive to use in terms of time and storage. Hence, we used BiCG as
the Krylov solver in the solution of (5) and (10). The initial guesses for BiCG were x = 0, and
the stopping criteria were b − Ax 2 / b 2 < 10−7 for (5) and P b − P Ax 2 / P b 2 < 10−7 for
(10) respectively.
Numerical results are summarized in Table 1. In this table, the column titled with “#eig
Γ” is a column of numbers of eigenvalues of A inside Γ. The columns titled with “#iter” are
columns of numbers of iterations by BiCG, and the columns with “Err” are columns of true
relative errors b − Ax 2 / b 2 or P b − P Ax 2 / P b 2 .
Without deﬂation, BiCG essentially did not converge for bcsstm27 and the ILU(0)preconditioned mahindas. With an appropriate eigenvalue-deﬂation, however, the situation
was changed signiﬁcantly. The most expensive part in the proposed method is clearly the computation of the Z in (15). In §3, we describe state of the art parallel multigrid methods that
will be applied to the computation of Z.

3

Future Work

We can formulate either geometric multigrid [1, 2, 5, 10, 18, 19, 24, 40] or algebraic multigrid
[35] using the same notation level to level using the abstract multigrid approach developed in
[9, 12, 14, 3, 11, 12, 14].
947

On Solving Ill Conditioned Linear Systems

Matrix
bcsstm27
mahindas

Circle Γ
(c, r)
(0, 5)
(−1, 1)

#eig Γ
363
31

m
400
50

Computation #1
#iter
Err
1224000 4.0 × 10−6
1258000
1.3

Douglas, Lee, and Yeung
Computation #2
#iter
Err
763
8.9 × 10−8
3937
5.0 × 10−8

Computation #3
#iter
Err
266
9.8 × 10−8
1555
4.7 × 10−8

Table 1: A comparison of solving (5) and (10) by BiCG. For mahindas, a ILU(0) preconditioner
was applied. Γ is a circle with center c and radius r. The q in (15) is q = 27 .
Assuming the cost of the smoother (or rougher) on each level is O(Nj ), j = 1, · · · , k,
Algorithm MGC with p recursions to solve problems on level k − 1 has complexity
⎧
1≤p≤σ
⎨ O(Nk )
O(Nk log Nk ) p = σ
WM GC (Nk ) =
(16)
⎩
log p
p > σ.
O(Nk )
Under the right circumstances, multigrid is of optimal order as a solver.
Consider the example (14) in §2.4. A simple geometric multigrid approximation to (14)
produces a very good solution in 4 V Cycles or 2 W cycles using the deﬂated GMRES as the
rougher. Each V or W Cycle is O(Nk ). Hence, we have an optimal order solver for (14), which
would not be the case if we used BiCG or deﬂated GMRES on a single grid.
High performance computing versions of multigrid based on using hardware acceleration
with memory caches was extensively studied in the early 2000’s [15].
Parallelization of Algorithm MGC is straightforward [13].
• For geometric multigrid, on each level j, data is split using a domain decomposition
paradigm. Parallel smoothers (roughers) are used. The convergence rate degrades from
the standard serial theoretical rate, but not by a lot, and scaling is good given suﬃcient
data.
• For algebraic multigrid, the algorithms can be either straightforward (e.g., Ruge-Studen
[29] or Beck [4]) to quite complicated (e.g., AMGe [23]). Solutions have existed for a
number of years, so it is a matter of choosing an exisiting implementation. In some cases,
using a tool like METIS or ParMETIS is suﬃcient to create a domain decomposition-like
system based on graph connections in Aj , which reduces parallelization back to something
similar to the geometric case.
In many cases, the complexity of this type of parallel multigrid for P processors becomes
WM GC,P (Nk ) = WM GC (Nk ) log P/P.

4

(17)

Conclusions

The novelties of this research include (i) we incorporate the delation projector P with the Z
described in (2) and (4) into Krylov subspace methods to enhance the stability and accelerate
the convergence of the iterative methods for solving ill conditioned linear algebraic systems,
and (ii) we will also implement robust and eﬃcient parallel multigrid methods for solving (4)
and realize a software package for a wide variety of applications.
To our best knowledge, the constructions of most, if not all, deﬂation subspace matrices Z
in the literature are problem dependent. Further, some of them are ad-hoc, e.g., the algebraic
subdomain deﬂation in [20]. The method proposed here is problem independent.
948

On Solving Ill Conditioned Linear Systems

Douglas, Lee, and Yeung

Acknowledgments
This research was supported in part by National Science Foundation grants ACI-1440610, ACI1541392, and DMS-1413273.

References
[1] G. P. Astrakhantsev. An iterative method of solving elliptic net problems. Z. Vycisl. Mat. i. Mat.
Fiz., 11:439–448, 1971.
[2] N. S. Bakhvalov. On the convergence of a relaxation method under natural constraints on an
elliptic operator. Z. Vycisl. Mat. i. Mat. Fiz., 6:861–883, 1966.
[3] R. E. Bank and C. C. Douglas. Sharp estimates for multigrid rates of convergence with general
smoothing and acceleration. SIAM J. Numer. Anal., 22:617–633, 1985.
[4] R. Beck. Graph-based algebraic multigrid for lagrange-type ﬁnite elements on simplicial meshes.
Preprint SC 99-22, Konrad-Zuse-Zentrum fur Informationstechnik, 1999.
[5] A. Brandt. Multi–level adaptive solutions to boundary–value problems. Math. Comp., 31:333–390,
1977.
[6] F. Chatelin. Spectral Approximation of Linear Operators. Academic Press, New York, 1984.
[7] Tim A. Davis and Yifan Hu. The university of ﬂorida sparse matrix collection. ACM Transactions
on Mathematical Software, 38:1–25, 2011. http://www.cise.uﬂ.edu/research/sparse/matrices (last
visited April 4, 2016).
[8] R. M. Dinkla. GMRES(m) with deﬂation applied to nonsymmetric systems arising from ﬂuid
mechanics problems. Masters thesis, Delft University of Technology, Delft, The Netherlands, 2009.
[9] C. C. Douglas. Abstract multi–grid with applications to elliptic boundary–value problems. In
G. Birkhoﬀ and A. Schoenstadt, editors, Elliptic Problem Solvers II, pages 453–466. Academic
Press, New York, 1984.
[10] C. C. Douglas. Multi–grid algorithms with applications to elliptic boundary–value problems. SIAM
J. Numer. Anal., 21:236–254, 1984.
[11] C. C. Douglas. A generalized multigrid theory in the style of standard iterative methods. In
Multigrid Methods IV, Proceedings of the Fourth European Multigrid Conference, Amsterdam,
July 6-9, 1993, volume 116 of ISNM, pages 19–34, Basel, 1994. Birkh¨
auser.
[12] C. C. Douglas. Madpack: A family of abstract multigrid or multilevel solvers. Comput. Appl.
Math., 14:3–20, 1995.
[13] C. C. Douglas. A review of numerous parallel multigrid methods. In G. Astfalk, editor, Applications
on Advanced Architecture Computers, pages 187–202. SIAM, Philadelphia, 1996.
[14] C. C. Douglas, J. Douglas, and D. E. Fyfe. A multigrid uniﬁed theory for non-nested grids and/or
quadrature. E. W. J. Numer. Math., 2:285–294, 1994.
[15] C. C. Douglas, J. Hu, M. Kowarschik, U. R¨
ude, and C. Weiss. Cache optimization for structured
and unstructured grid multigrid. Elect. Trans. Numer. Anal., 10:21–40, 2000.
[16] N. Dunford and J. T. Schwartz. Linear Operators, General Theory (Part I). Wiley-Interscience,
Hoboken, New Jersey, 1988.
[17] H. W. Engl. Regularization methods for the stable solution of inverse problems. Surveys Math.
Indust., 3:71–143, 1993.
[18] R. P. Fedorenko. A relaxation method for solving elliptic diﬀerence equations. Z. Vycisl. Mat.
i. Mat. Fiz., 1:922–927, 1961. Also in U.S.S.R. Comput. Math. and Math. Phys., 1 (1962), pp.
1092–1096.
[19] R. P. Fedorenko. The speed of convergence of one iterative process. Z. Vycisl. Mat. i. Mat. Fiz.,
4:559–563, 1964. Also in U.S.S.R. Comput. Math. and Math. Phys., 4 (1964), pp. 227–235.

949

On Solving Ill Conditioned Linear Systems

Douglas, Lee, and Yeung

[20] J. Frank and C. Vuik. On the construction of deﬂation-based preconditioners. SIAM J. Sci.
Comput., 23(2):442–462, 2001.
[21] Y. Futamura, H. Tadano, and T. Sakurai. Parallel stochastic estimation method of eigenvalue
distribution. JSIAM Letters, 2:27–30, 2011.
[22] C. W. Groetsch. Generalized Inverses of Linear Operators. Dekker, New York, 1997.
[23] G. Haase. A parallel AMG for overlapping and non-overlapping domain decomposition. Elect.
Trans. Numer. Anal., 10:41–55, 2000.
[24] W. Hackbusch. Multigrid Methods and Applications, volume 4 of Computational Mathematics.
Springer–Verlag, Berlin, 1985.
[25] T. Kato. Perturbation Theory for Linear Operators. Springer-Verlag, New York, 1976.
[26] L. Mansﬁeld. Damped Jacobi preconditioning and coarse grid deﬂation for conjugate gradient
iteration on parallel computers. SIAM J. Sci. Stat. Comput., 12(6):1314–1323, 1997.
[27] R. A. Nicolaides. Deﬂation of conjugate gradients with applications to boundary value problems.
SIAM J. Numer. Anal., 24:355–365, 1987.
[28] E. Polizzi. Density-matrix-based algorithm for solving eigenvalue problems. Phys. Rev. B, 79, no.
115112, 2009.
[29] J. W. Ruge and K. St¨
uben. Eﬃcient solution of ﬁnite diﬀerence and ﬁnite element equations
by algebraic multigrid (AMG). In D. J. Paddon and H. Holstein, editors, Multigrid Methods for
Integral and Diﬀerential Equations, The Institute of Mathematics and its Applications Conference
Series, pages 169–212. Clarendon Press, Oxford, 1985.
[30] Y. Saad. Iterative Methods for Sparse Linear Systems. SIAM, Philadelphia, 2nd edition, 2003.
[31] Y. Saad. Numerical Methods for Large Eigenvalue Problems. SIAM, Philadelphia, 2011.
[32] Y. Saad and M.H. Schultz. GMRES: A generalized minimal residual algorithm for solving nonsymmetric linear systems. SIAM J. Sci. Stat. Comput., 7:856–869, 1986.
[33] T. Sakurai and H. Sugiura. A projection method for generalized eigenvalue problems using numerical integration. J. comput. Appl. Math., 159:119–128, 2003.
[34] T. Sakurai and H. Tadano. CIRR: A Rayleigh–Ritz type method with contour integral for generalized eigenvalue problems. Hokkaido Math. J., 36:745–757, 2007.
[35] K. St¨
uben. An introduction to algebraic multigrid. In U. Trottenberg, C. W. Oosterlee, and
A. Sch¨
uller, editors, Multigrid, pages 413–532. Academic Press, London, 2000. Appendix A.
[36] J. M. Tang and C. Vuik. On deﬂation and singular symmetric positive semi-deﬁnite matrices. J.
Comput. Appl. Math., 206(2):603–614, 2006.
[37] P. T. P. Tang and E. Polizzi. Feast as a subspace iteration eigensolver accelerated by approximate
spectral projection. SIAM J. Matrix Anal. Appl., 35:354–390, 2014.
[38] A. van der Sluis and H.A. van der Vorst. The rate of convergence of conjugate gradients. Numer.
Math., 48:543–560, 1986.
[39] C. Vuik, A. Segal, and J. A. Meijerink. An eﬃcient preconditioned CG method for the solution of a
class of layered problems with extreme contrasts in the coeﬃcients. J. Comput. Phys., 152(1):385–
403, 1999.
[40] P. Wesseling. An Introduction to Multigrid Methods. John Wiley & Sons, Chichester, 1992.
[41] M. Yeung, J. Tang, and C. Vuik. On the convergence of GMRES with invariant-subspace deﬂation.
Report 10-14, Delft Univ. of Technology, 2010.

950

