Procedia Computer Science
Volume 29, 2014, Pages 1134–1145
ICCS 2014. 14th International Conference on Computational Science

Self-Timed Periodic Scheduling For a Cyclo-Static
DataFlow Model
Amira Dkhil1 , XuanKhanh Do1 , Paul Dubrulle1 , St´ephane Louise1 , and
Christine Rochange2
1

2

CEA, LIST, PC172, 91191, Gif-sur-Yvette, France.
firstname.lastname@cea.fr
IRIT, Universit´e de Toulouse, 118, route de Narbonne, Toulouse cedex 9, France.
rochange@irit.fr

Abstract
Real-time and time-constrained applications programmed on many-core systems can suﬀer from
unmet timing constraints even with correct-by-construction schedules. Such unexpected results
are usually caused by unaccounted for delays due to resource sharing (e.g. the communication
medium). In this paper we address the three main sources of unpredictable behaviors: First, we
propose to use a deterministic Model of Computation (MoC), more speciﬁcally, the well-formed
CSDF subset of process networks; Second, we propose a run-time management strategy of
shared resources to avoid unpredictable timings; Third, we promote the use of a new scheduling
policy, the so-said Self-Timed Periodic (STP) scheduling, to improve performance and decrease
synchronization costs by taking into account resource sharing or resource constraints. This
is a quantitative improvement above state-of-the-art scheduling policies which assumed ﬁxed
delays of inter-processor communication and did not take correctly into account subtle eﬀects
of synchronization.
Keywords: Many-core systems, Real-Time, Data-Flow, Scheduling, Latency, Guarantees

1

Introduction

Real-time embedded systems require both functionally correct and temporally predictable executions. Given the scale of new massively parallel systems, such as the SThorm chip from
STMicroelectronics (64 cores) or the MPPA chip from Kalray (256 cores) [5], the scheduler
design of tasks and communications becomes more complicated and its impact upon the entire
system performance becomes more signiﬁcant. Consequently, multiprocessor scheduling has
been an active area and therefore many scheduling and resource management solutions was
suggested. The Self-Timed Scheduling (STS) strategy (also known as as-soon-as-possible), is
considered as the most appropriate policy for streaming applications [14, 15, 17]. In Dataﬂow
models, each actor (i.e. process) ﬁring starts as soon as their ﬁring rules are satisﬁed.
1134

Selection and peer-review under responsibility of the Scientiﬁc Programme Committee of ICCS 2014
c The Authors. Published by Elsevier B.V.
doi:10.1016/j.procs.2014.05.102

Self-Timed Periodic Scheduling For CSDF Model

Dkhil, Do, Dubrulle, Louise and Rochange

Such models of computation (MoC) oﬀer a good visibility of datapath, actual parallelism
in a given application and synchronization points, which make such paradigm especially well
ﬁtted. Programming languages like StreamIt [18], or ΣC [8] oﬀer even more: An underlying
MoC which is deterministic, and purely data-driven. For both, the MoC is a subset of Khan
Process Network (KPN [9]), which can provably run in bounded memory. This is well-formed
Static DataFlow (SDF [12]) for StreamIt and a superset of well-formed Cyclostatic DataFlow
(CSDF [3]) for ΣC. The basic principle of KPN is simple: Processes are linked with FIFO
channels for communication. All FIFO channels are read-blocking and write are non-blocking.
In the case of SDF, for each ﬁring of a process, a ﬁxed amount of data tokens are expected on
each input channel, and a ﬁxed amount of data token are produced on each output channel. If
any input channel has an insuﬃcient amount of data tokens, the associated process cannot be
ﬁred. For CSDF, the principle is the same, but the number of tokens produced or consumed on
each channel can vary in a cyclic way from one ﬁring of the process to the next (see Figure 1).
For timing concerns, compared to using worst-case execution times, self-timed scheduling
will always do at least as well. Nonetheless, this result can only be true if synchronization
times are negligible. Synchronization is a special form of communication, in which the data
is an information for the control of the application. It has a dual role: (1) Enforcing the
correct sequencing of actors ﬁring, and (2) Ensuring the mutually exclusive access to certain
shared data. To cope with such a requirement, STS introduces explicit synchronization checks
whenever two processors communicate. In this case, each synchronization can cost up to four
accesses to shared memory [16]. As a consequence, due to the complex and irregular dynamics
of self-timed operations, in addition to the high synchronization overhead, many diﬀerent assumptions were imposed, like uniform task execution times or contention-free communication.
However, neglecting subtle eﬀects of synchronization or considering uniform costs for communication operations like in [2, 15] is dangerous and not realistic with regards to actual systems
and applications. Unless a special hardware for ordered communication transactions [17, 10]
or contention free communications [14] (e.g., Time Division Multiple Access (TDMA), RoundRobin (RR)) that maintains a predeﬁned schedule of accesses to the shared memoryis employed,
analysis and optimization of self-timed systems under real-time constraints remains challenging. Nowadays, periodic scheduling is receiving much more attention for streaming applications
[2, 6, 15]. These algorithms provide many nice properties such as timing guarantees for applications, temporal isolation [4] and low complexity of the schedulability tests. It was shown that
interprocessor communication overhead can be deﬁned as a monotonically increasing function
of the number of conﬂicting memory accesses in a given period of the schedule [6]. Nonetheless,
periodic scheduling achieves optimal performance solely for matched I/O graphs i.e a graph
where the product of actor’s worst-case execution time and repetition is the same for all actors.
As in the real world, execution time of processes can vary largely, it is diﬃcult to prove that a
graph is a matched I/O (or to build one).
In this paper, we show that it is possible to guarantee the matched I/O property for any
Cyclo-Static DataFlow (CSDF) graph by using a new scheduling policy noted Self-Timed Periodic (STP) Schedule. STP is a hybrid execution model based on mixing Self-Timed schedule
and periodic schedule while considering variable IPC times. To illustrate the impact of the STP
model on performance, we present the following motivational example.

1.1

Motivational Example

For any consistent CSDF graph, we can show a periodical ﬁring of actors which will return to a
steady state (usually the initial state). This periodical ﬁring is characterized by ﬁring vectors,
1135

Self-Timed Periodic Scheduling For CSDF Model

Dkhil, Do, Dubrulle, Louise and Rochange

Figure 1: (a) CSDF graph (b) Throughput and latency metrics for the mismatched I/O CSDFG
and can be calculated from the topology of the graph and the amount of tokens consumed and
produced by each ﬁring of actors.
For example, in Figure 1(a), we show a CSDF graph of 4 actors and 4 communication
→
→
→
channels. The CSDF graph is characterized by two repetition vectors q and r . r is the
minimal set of actor ﬁrings returning the dataﬂow graph to its initial state. For the example
→
→
→
depicted in Figure 1(a), r = [1, 2, 2, 4] and q = [3, 2, 4, 8]. q is the minimal set of sub-tasks
ﬁrings returning the dataﬂow graph to its initial state. In fact, each actor in the graph is
executed through a periodically repeated sequence of subtasks. For example, if r1 = 1 then
q1 = 3 because actor a3 contains 3 subtasks (i.e. to get qi , we multiply ri by the length of the
consumption and production rates of ai ). The worst-case computation and communication time
of each actor is shown next to its name between round brackets (e.g. 6 for a1 ). This graph is an
example of a mismatched I/O graph since the product of actor execution times and repetition
is not the same for all actors (e.g. 1 × 6 = 2 × 8). Let Υ and L denote the throughput (i.e. rate)
and latency of graphs G, respectively, derived in Figure 1(b) for the example of Figure 1(a). It
has been shown that optimal throughput and latency of a matched I/O dataﬂow graph can be
achieved under Implicit-Deadline Periodic (IDP) schedule [2]. However, for mismatched I/O
graph, it pays a high price in terms of increased latency and decreased throughput. Instead, if
the actors are to be scheduled as Self-timed Periodic (STP) tasks, then it is possible to have
25% to 40% improvement compared to the IDP schedule. In our contribution, we propose two
granularities of scheduling. This depends on whether we use qi or ri as the basic repetition
vector of CSDF. For the proposed example, including the subtasks of actors results in better
performance for latency. However, for throughput ST Pqi gives better results1 .

1.2

Paper Contributions

We propose two classes of STP schedules based on two diﬀerent granularities. The ﬁrst schedule
is based on the repetition vector qi without including the subtasks of actors. The second schedule
has a ﬁner granularity and includes the subtasks of actors. It is based on the repetition vector
ri . For mismatched I/O graphs, we show that it is possible to signiﬁcantly decrease the latency
and increase the throughput under the STP model for both granularities. We will show that our
approach guarantees the property of matched I/O rates for any CSDF graph and consequently
guarantees optimal performance.
The remainder of this paper is organized as follows. In Section 2, we represent a state of
the art methods relative to the scheduling policies of Multi-Rate DataFlow (MRDF) graphs on
1 Let’s assume that an execution under STS has an additional cost of synchronization equal to 25% the
computation time of actors, the latency will be equal to L = 47 units of time. Thus, for a mismatched graph
example, the STP model is equally good to STS in terms of throughput and latency.

1136

Self-Timed Periodic Scheduling For CSDF Model

Dkhil, Do, Dubrulle, Louise and Rochange

multiprocessor systems. The considered model is described in Section 3. Our main contribution
is presented in Section 4. We ﬁnish with sections 5 and 6 where we present the case studies
and we state the conclusions.

2

Related Work

The most prominent performance metrics of concurrent real-time applications are throughput
and latency. Optimizing or analyzing latency of a stream program, in general, requires to ﬁnd
a good tasks scheduling among the computing units. A schedule is more eﬃcient if it hides
communication latencies whenever possible. In [10], Khandalia et al. explored the problem
of eﬃciently ordering interprocessor communication operations in statically scheduled multiprocessors. Their method is based on ﬁnding a linear ordering of communication actors at
compile-time which would minimize synchronization and arbitration costs, but at the expense
of run-time ﬂexibility. In [17], the author proposes to schedule all communications as well as
all computations to eliminate shared resource contention. Their approach is based on using
a hardware central transaction controller that maintains a predeﬁned schedule of accesses to
the shared memory. Another approach in [7] is based on Scenario-Aware Data-Flow (SADF).
In such model, an application is modeled as a collection of SDF graphs, each representing individual scenarios of behavior, and a Finite State Machine (FSM) that speciﬁes the possible
orders of scenario occurrences. The paper provides techniques to analyze worst-case performance analysis (i.e. highest throughput and minimal latency) of such applications. SADF was
primarily designed as a way of modeling behavior and not as a programming model. For this
reason, the execution model is not explicit about scenario transition decisions and data-ﬂow
relation (i.e. there is no way of knowing where the decision to go from one scenario to another
was taken). In [2], Bamakhrama and Stefanov present a complete framework for computing
the periodic task parameters using an estimation of worst-case execution time. They assume
that each write or read has constant execution time but this is often not true. Our approach
is somewhat similar to [2] in using the periodic task model which allows applying a variety
of proven hard-real-time scheduling algorithms for multiprocessors. However, it is diﬀerent in
the way of using the periodic behavior because actors will no longer be strictly periodic but
self-timed assigned to periodic levels. In addition, we treat the case variable execution time of
actors due to synchronization and contention in shared resources.

3
3.1

Model of Computation and Terminology
Timed Graph

The timed graph is a more accurate representation of the CSDF graph, that associates
to each subtask or instance of an actor a computation time and a communication overhead. We consider the Timed graph G = (A, E, ω, fϕ ). The set of actors is denoted by
A = {a11 , a21 , ..., aJ1 1 , ..., a1n , a2n , ..., aJnn }, where n is the total number of actors in the CSDF graph.
The set of edges is denoted by E = {E1 , E2 , ..., Ev }. Each parallel actor ai is represented by
a Directed Acyclic Graph (DAG) and consists in a set of nodes and directed relations. The
nodes represent the instances of an actor, while the directed relations show the FIFO buﬀers.
Each instance aji is viewed as executing through a periodically-repeating sequence of sub-tasks
of length τi ∈ N . The production and consumption behavior of an actor is constant for a given
sub-task but may vary across the diﬀerent sub-tasks.
1137

Self-Timed Periodic Scheduling For CSDF Model

Dkhil, Do, Dubrulle, Louise and Rochange

Figure 2: (a) Acyclic Timed dataﬂow graph of a pipeline example with timing parameters (b)
Decomposition approach example
A real-time DAG of ai is characterized by (τi , Ji , ωi , fϕ (i), Di ), where Ji is the unfolding
factor of ai , ωi is the worst-case computation time, fϕ (i) is its communication time according
to schedule ϕ and Di is the relative deadline.
For many non-terminating dataﬂow graphs, the execution can be divided into ﬁnite iterations. An iteration is a minimal set of actor ﬁrings returning the dataﬂow graph to its initial
state. qi > 0 [3] represents the number of invocations of an actor ai in one iteration of G and
→
−
q = [q1 , q2 , ..., qn ]T , is the basic repetition vector of G.
In order to exploit inter-iteration parallelism more eﬀectively, J iterations can be scheduled
together, where J represents the unfolding factor of G. If the graph is unfolded J times, each
actor ai is executed Ji = J × qi times. According to the DAG model, the execution ﬂow of
subtasks is constrained by their directed relations between instances and sub-tasks which is
particular to CSDF model.
So, scheduling depends on actors and sub-tasks of actors. Each sub-task in aji =
j
{ai (1), ..., aji (τi )}, ∀ j ∈ [1, ..., Ji ], can be treated as a single unit denoted by its total computation time and its total communication time. However, ignoring this ﬁner granularity oﬀered by
CSDF model can result in a pessimistic analysis, as with the example presented in section 1.1.
In this work, we consider only acyclic CSDF graphs. An acyclic graph G has a number of
levels, denoted by L. Assigning actors to levels is based on passing through the directed-acyclic
graph (DAG) of the MRDF application at compile-time. Diﬀerent graph traversals types exist
like topological, breadth-ﬁrst, etc. Actors will be assigned to a set of levels L = {L1 , L2 , ..., Lα }.
Authors in [2], proposed a method based on assigning the actors in the graph according to
precedence constraints. In order to guarantee bounded resource execution, additional precedence edges can be added to the timed graph. As depicted in Figure 2, the DAG is decomposed
into a set of levels executed sequentially.
Deﬁnition 1. Consistency [11]: A CSDF graph is called consistent if and only if it has a nontrivial repetition vector. For a consistent graph, there is a unique smallest non-trivial repetition
vector which is designated as the repetition vector of the CSDF graph. A repetition vector is
called non-trivial if and only if qi > 0 for all ai ∈ A.
−
Theorem 1. In a CSDF graph, a repetition vector →
q = [q , q , ..., q ]T is given by [3]:
1

→
−
−
q =P ·→
r , with P = Pjk =
1138

τj
0

2

n

, if j = k
, otherwise

(1)

Self-Timed Periodic Scheduling For CSDF Model

Dkhil, Do, Dubrulle, Louise and Rochange

−
And, →
r = [r1 , r2 , ..., rn ]T , where ri ∈ N , is a solution of the balance equation:
−
Γ·→
r = 0,

(2)

The topology matrix Γ speciﬁes the connections between edges in directed multigraph. As
an example of this edge-vector topology matrix, a matrix entry Γui would be 0 if edge eu does
not connect to actor ai , pi if actor ai is the source actor of edge eu , and −ci if actor ai is the
sink actor of eu .

3.2

System Model and Schedulability

The system consists of a set Π = {Π1 , Π2 , ..., Πm } of m homogeneous processors. The processors
execute a levels set L = {L1 , L2 , ..., Lα } of α periodic levels. A periodic level Li ∈ L is deﬁned
as Li = (Si , ωli , fϕ (li ), φi , Di ), where Si is the start time of Li , ωi ∈ N∗ is the worst-case
computation time, fST P (i) ∈ N is the worst-case communication time of Li under STP schedule,
φi ≥ ωi +fST P (i) is the level period and Di is the relative deadline of Li where Di = max Dk .
k=1→βj

βj ∈ N∗ represents the number of actors in each level.
A periodic level Li is invoked at time instant t = S1 + (i − 1)φ and has to ﬁnish execution
before time t = S1 + (i − 1)φ + ωi + fϕ (i). If Di = φ , then Li is said to have implicit-deadline.
If Di < φ , then Li is said to have constrained-deadline. Actors (i.e. tasks) in G are scheduled
as implicit-deadline periodic tasks and assigned to levels. At run-time , they are executed in a
self-timed manner. This is possible because actors of level k + 1 consume the data produced in
level k.
The utilization of a task is Ui = ωφii . For each level Li , the total utilization of is ULi =
β i ωj
j=1 φj .

A scheduling algorithm is said to be optimal iﬀ it can schedule any feasible task
set A on Π such that Ui ≤ m. Several global and hybrid algorithms were proven optimal for
scheduling asynchronous sets of implicit-deadline periodic tasks [4]. We restrict our attention
to consistent and live CSDF graphs. In fact, any acyclic graph is live [11].
A static schedule [17] of a consistent and live CSDF graph is valid if satisﬁes the precedence
constraints speciﬁed by the edges. Authors in [14], introduced a theorem that states the sufﬁcient and necessary conditions for a valid schedule. However, this result was established for
Synchronous Dataﬂow graphs where actors have constant execution times. The test in Equation 3 is a novel contribution of this paper. This allows the timing of ﬁring respects the ﬁring
rules of actors.
Theorem 2. Valid Schedule of CSDFG: A schedule S is valid if and only if for any edge e i ❀ j ,
for any instance k ∈ N and for any sub-task τ ∈ [1, ..., τj ]:
s(j, k, τ ) ≥ s(i, k + k , τ ) + ωi + max[fϕ (ai , ζi )]

(3)

The schedule function s(i, k, τ ) ∈ Z+ represents the time at which the τ th sub-task of the
k instance of actor ai starts execution. k is deﬁned as:
th

k =

τ
x=1
τ
x=1

cxj div (pi + di,j )
cxj div (pi + di,j ) − 1

, if
, else

τ
x
x=1 cj

mod (pi + di,j ) = 0

(4)

di,j are the tokens already present on buﬀer(i, j). Similarly, τ is the smallest integer that can
verify the following equation:
1139

Self-Timed Periodic Scheduling For CSDF Model

τ
x=1

pxi + di,j −
cτj − k pi

Dkhil, Do, Dubrulle, Louise and Rochange

τ −1 x
x=1 cj

≥1

(5)

Proof 1. The ﬁring rule of each actor allows precedence constraints not to be violated, it
follows that:
s(j, k, τ ) = t ⇔ buﬀ(i, j) ≥ cτj

(6)

The number of tokens stored on buﬀer buﬀ(i, j) of edge e i j should be greater than or at
least equal to the number of consumed tokens for actor aj . Since CSDF is monotonic:
buﬀ(i, j)

buﬀ (i, j) =⇒ s(j, k, τ )

s(j, k , τ ), ∀ k ≥ k

(7)

FIFO property of communication buﬀers suggests that two consecutive executions of an
actor will always produce output tokens in the order of their ﬁring. FIFO ordering of tokens
can be maintained, if each actor has a constant execution time, or has a self cycle with one
token:
(8)
s(j, k, τ ) ≥ end(i, k , τ )
Then, If actor ai have a variable execution time Ti
s(j, k, τ ) ≥ s(i, k , τ ) + Ti , ∀ Ti ∈ R∗

(9)

Lemma 1 (FIFO property for DAG). : Any Directed Acyclic Graph (DAG) with actors that
have variable execution times can conserve FIFO property
Let Ti = ωi + max[fϕ (ai , ζi )], k = k + k and τ = τepsilon , k and τepsilon are obtained
according to formula 6.

4

Self-Timed Periodic Model

4.1

Assumptions and Deﬁnitions

A graph G refers to an acyclic consistent CSDF graph. A consistent graph can be executed with
bounded memory buﬀers and no deadlock. We base our analysis on the following assumptions:
A1. External sources in dataﬂow: The model is accomplished with interfaces to the outside
world in order to explicitly model inputs and outputs (I/Os). A source and a sink nodes can
be integrated as closures since they deﬁne limits for a portion of an application. A graph G
has a set of input streams I = {I1 , I2 , ..., IΔ } connected to the input actors of G, and a set of
output streams O = {O1 , O2 , ..., OΛ } processed from the output actors of G. An actor ai ∈ A
) the sets of its input and output edges. These
is deﬁned, inter alia, with Eai = (Eaini and Eaout
i
special nodes are deﬁned as follows:
in
out
in
= ∅ and Esrc
= {I1 , I2 , ..., IΔ }, snk ∈ A, Esnk
=
• 1. Nodes uniqueness: src ∈ A, Esrc
out
{O1 , O2 , ..., OΛ } and Esnk = ∅.

• 2. Samples arrival: the ﬁrst samples of source actor arrive prior or at the same time when
the actors of G start execution. They are characterized by a minimum inter-arrival time
assumed to be controlled by the designer to match the periods of the intervals.
1140

Self-Timed Periodic Scheduling For CSDF Model

Dkhil, Do, Dubrulle, Louise and Rochange

Deﬁnition 2. For a graph G under periodic schedule, the worst-case communication overhead
L
fSTj P of any level Lj ∈ L depends on the maximum number of accesses to memory mβj processed
in the time interval [(j − 1) × φ, j × φ[:
L

fSTj P =↑ f (mβj ), ∀Lj ∈ L

(10)

In [6], we proved that f is a monotonic increasing function of the number of conﬂicting memory
accesses.
A2. For periodic schedules, synchronization cost is constant, because periodic behavior
guarantees that an actor ai ∈ Lj , ∀i ∈ [1, ..., βj ], will consume tokens produced at level (j −
1) [6]. The latter implies that actors of the same level can start ﬁring immediately in the
beginning of a given period because all the necessary tokens have already been produced.
Deﬁnition 3. Matched Input/Output rates property:
According to [2], a graph G is a matched input/output (I/O) rates graph if and only if:
η mod Q = 0, where

η = max(ωi qi )
ai ∈A

(11)

Q = lcm(q1 , q2 , ..., qn ) (lcm denotes the least-common-multiple operator). If formula 11 does
not hold, then G is a mis-matched I/O rates graph. If η mod Q = 0, then there exists at least
a single actor in the graph fully utilizing the processor (i.e. which represents the minimal level
period) on which it runs which allows the graph to achieve a maximum throughput.

4.2

Latency Analysis under STP Schedule

A self-timed schedule does not impose any extra latency on the actors. This leads us to the
following result:
Deﬁnition 4. (ST Pqi ) For a graph G, a period φ, where φ ∈ Z+ , represents the period,
→
measured in time-units, of the levels in G. If we consider q as the basic repetition vector of G,
then φ is given by the solution to:
⎧
νj,pk
⎪
⎨φ ≥ max max
(qi ωi + fφ (i)),
iﬀ νj,pi > 1
j=1→αk=1→βj i=1
(12)
⎪
⎩φ ≥ max max (qk ωk + fφ (k)),
iﬀ νj,pi = 1
j=1→αk=1→βj

∀i ∈ [1, βj ] and ∀j ∈ [1, α].
The levels period φ is deﬁned as the maximum execution time of all levels.
Deﬁnition 4 implies an equal period for all the levels. Similarly, we deﬁne the schedule
→
function for the ﬁner granularity of CSDF characterized by the repetition vector r as follows:
→

Deﬁnition 5. (ST Pri ) If we consider r as the basic repetition vector of G, then φ is given by
the solution to:
⎧
νj,pk
⎪
⎪
⎨φ ≥ max max
(ri ωi + fφ (i)),
iﬀ νj,pi > 1
j=1→α k=1→βj i=1
(13)
⎪
⎪
iﬀ νj,pi = 1
⎩φ ≥ max max (rk ωk + fφ (k)),
j=1→α k=1→βj

1141

Self-Timed Periodic Scheduling For CSDF Model

Dkhil, Do, Dubrulle, Louise and Rochange

∀i ∈ [1, βj ] and ∀j ∈ [1, α].
The earliest start time of actors under the STP model is given by the following lemma.
Lemma 2. For a graph G, the earliest start time of an actor ai ∈ Lj , denoted by si,j , under
a strictly periodic schedule is given by:
s(i, j) =

0
(j − 1)φ

, if j = 1
, if j > 1

(14)

Latency is deﬁned as the maximum time elapsed between the ﬁrst ﬁring of src actor and the
last ﬁring of snk actor. Using Equations 14, 13 and 12, it is possible to compute the latency
under the STP model for any acyclic CSDF graph:
LST Pqi /ri = α × φ

(15)

Self-Timed Periodic (STP) Schedule is the main piece of our method. It’s a new hybrid
schedule based on the well-known periodic and self-timed schedules for streaming applications.
The ﬁrst step in STP is based on passing through the directed-acyclic graph (DAG) of the
MRDF application at compile-time. Diﬀerent graph traversals types exist like topological,
breadth-ﬁrst, etc. Actors will be assigned to a set of levels L = {L1 , L2 , ..., Lα }. The best-case
level structure is composed of m tasks which can be executed in parallel by fully utilizing the m
processors provided by the platform, with a cumulative utilization of the task set that does not
exceed m. The level structure does not only depend on precedence constraints between tasks
deﬁned by formula 3, but also on the degree of parallelism and of the architecture.
The second step consists in assigning priorities to actors of the same level using the selftimed (i.e. As-Soon As-Possible (ASAP) start-time) strategy. At run-time, actors are scheduled
according to the assigned priorities. The ﬁnal step assigns to each level a global period φ
according to following formulas:

5
5.1

Evaluation
ΣC: a new programming language for embedded manycores

We use the ΣC [8, 1], a language designed in order to ensure programmability and eﬃciency on
many cores. Close and familiar with C, it minimizes the speciﬁc syntaxes, while making explicit
the construction of parallelism. As a programming language, ΣC relates to StreamIt [18], another programming language developed by Massachusetts Institute of Technology and specially
engineered for modern streaming systems. Whereas the way to build Networks of Processes
in StreamIt is by using the semantic of the code, which limits the topology of the associated
Network (StreamIt topology is hierarchical, and is mostly limited to series-parallel graphs with
nonetheless the important addition of feed-back loops; special features like teleport-messaging
are required to overcome this limitation, see [18]), the way it is done in ΣC, is through a twostep compilation: The ﬁrst step, is an oﬀ-line compilation to build the network of so-called
agents (individual tasks in the stream model) and a communication interconnect called subgraph. This two-step compilation process has the advantage to permit any kind of topology,
because it proceeds to an oﬀ-line execution of the ﬁrst-stage compilation to build the process
network.
1142

Self-Timed Periodic Scheduling For CSDF Model

5.2

Dkhil, Do, Dubrulle, Louise and Rochange

MPPA Platform

We consider the M P P A − 256 [5] clustered architecure, from Kalray, comprising 256 user
cores (i.e. cores with fully processing power provided to the programmer for computing tasks)
organized as 16 (4 × 4) clusters tied by a Network-on-Chip (NoC) with a torus topology. Each
cluster has 16 user processors connected to a shared memory. There are also 2 DMA engines
(one in Rx, one out T x) for communication with the NoC, and one special processor called
Ressource manager which makes the role of orchestra conductor and provides OS-like services.
Each processing core P Ei and the RM are ﬁtted with two-way associative instruction and data
caches (i.e., each location in main memory can be cached in either of two locations in the
cache). In addition to the 16 clusters, there are 4 I/O clusters that provide access to external
DRAM memory or interfaces, etc. The shared memory of a given compute cluster is a modular
memory system. The memory system consists of M memory modules numbered 1, 2, 3, ... M-1,
M, among which the addresses are distributed cyclically, that is, if i is the address of a memory
location, then j ≡ i(modM ) is the address of the module containing the location. For the
MPPA case, the memory system contains 16 memory modules of 128KB, so 2M B per cluster.
Each module has a memory controller connected to each pair of user processors (i.e. via a bus).
The memory is implemented as a multi-bus approach [5]: it provides the same functionality as
a full crossbar with lower impact on surface occupation and power consumption [13].

5.3

Evaluation Results

We evaluate our proposed framework in section 4 by performing an experiment on a set of
5 real-life streaming applications. The objective of the experiment is to compare the worscase end-to-end latency of streaming applications when scheduled using our self-timed periodic
scheduling to their worst-case achievable latency obtained via strictly periodic scheduling. After that, we discuss the implication of our theoretical results from section 4 and the latency
comparison experiment. The streaming applications used in the experiment are real-life applications coming from diﬀerent domains (e.g. signal processing, audio processing, etc.). Some of
these programs have been developed at CEA LaSTRE and some of them are StreamIt benchmarks. We use the ΣC language to implement the streaming applications on MPPA platform.
Each application is executed with a set of input data to generate an execution trace. From
these results, we derive the number of shared memory requests as well as the execution time
of computation operations for each actor. The generated number of memory accesses is then
used in the analytical memory access model presented in [6] in order to have an upper bound
of communication and synchronization overhead. In fact, we can use the same model presented
in [6] because STP schedule conserves the periodic behavior. Since we have a computation time
and a communication overhead of actors, we apply the STP scheduling strategy in order to
delimit the diﬀerent levels of execution and calculate levels period.

5.4

Discussion

In this experiment, we compare worst-case end-to-end latency resulting from our STP approach
to the IDP model. For latency, we report the graph maximum latency according to Formula 15.
For IDP schedule, we used the minimum period vector given in [2]. For STP schedule, we used
the minimum period vector given by Deﬁnition 12. ΣC tool-set deﬁnes the repetition vector
qi . Consequently, we derive only experimental results for one granularity. We will extend these
results in the future for the ﬁner granularity STP model based on the repetition vector ri . Now,
Figure 3 shows the results of comparing the latency of one iteration in the graph under both
1143

Self-Timed Periodic Scheduling For CSDF Model

Application
Moving Average
DCT
BeamFormer
AudioBeam
Laplacian

Dkhil, Do, Dubrulle, Louise and Rochange

Table 1: Presentation of each application
Description
Calculate the average
Functions that implement Discrete Cosine
Transforms and Inverse DCT
Template application to perform beamforming on a set of inputs
Appication to do real-time beamforming on a
microphone input array
Laplace operator

Source
CEA LaSTRE and MIT
CEA LaSTRE and MIT
CEA LaSTRE and MIT
CEA LaSTRE and MIT
CEA LaSTRE and MIT

Figure 3: Results of the latency comparison
IDP and ST Pqi . We clearly see that our STP model delivers an improvement of 25% to 35%
compared to IDP model. For all the applications, the latency was improved without verifying
if the graph is matched I/O or not.

6

Conclusion and Perspectives

We prove that the actors of a streaming application modeled as CSDF graph, can be scheduled
as self-timed periodic tasks. As a result, we conserve the properties of a periodic scheduling and
in the same time improve its performance. We also show how the diﬀerent granularities oﬀered
by CSDF model can be explored to decrease latency. We present an analytical framework for
computing the periodic task parameters while taking into account inter-processor communication and synchronization overhead. As a future work, we will compare self-timed periodic
schedule and self-timed schedule in the presence of non negligeable IPC overhead. We also want
to improve our scheduling test for STP model with a ﬁner granularity.

References
[1] P. Aubry, P.E. Beaucamps, F. Blanc, B. Bodin, S. Carpov, L. Cudennec, V. David, P. Dore,
P. Dubrulle, B. D. de Dinechin, F. Galea, T. Goubier, M. Harrand, S. Jones, J. D. Lesage, S Louise,
N. M. Chaisemartin, T. H. Nguyen, X. Raynaud, and R. Sirdey. Extended cyclostatic dataﬂow

1144

Self-Timed Periodic Scheduling For CSDF Model

[2]
[3]
[4]
[5]

[6]

[7]

[8]
[9]

[10]
[11]
[12]
[13]

[14]
[15]
[16]
[17]
[18]

Dkhil, Do, Dubrulle, Louise and Rochange

program compilation and execution for an integrated manycore processor. Propedia Computer
Science, International Conference on Computational Science (ICCS), Alchemy Workshop, 2013.
MohamedA. Bamakhrama and TodorP. Stefanov. On the hard-real-time scheduling of embedded
streaming applications. Design Automation for Embedded Systems, 2012.
G. Bilsen, M. Engels, R. Lauwereins, and J.A. Peperstraete. Cyclo-static data ﬂow. In Acoustics,
Speech, and Signal Processing, ICASSP-95, 1995.
Robert I. Davis and Alan Burns. A survey of hard real-time scheduling for multiprocessor systems.
ACM Comput. Surv., 2011.
Benoˆıt Dupont de Dinechin, Pierre Guironnet de Massas, Guillaume Lager, Cl´ement L´eger, Benjamin Orgogozo, J´erˆ
ome Reybert, and Thierry Strudel. A distributed run-time environment for
the kalray mppa-256 integrated manycore processor. Procedia Computer Science, 2013.
Amira Dkhil, St´ephane Louise, and Christine Rochange. Worst-Case Communication Overhead
in a Many-Core based Shared-Memory Model. In Junior Researcher Workshop on Real-Time
Computing, Nice, 2013.
M. Geilen and S. Stuijk. Worst-case performance analysis of synchronous dataﬂow scenarios. In
Hardware/software Codesign and System Synthesis, 2010 IEEE/ACM/IFIP International Conference on, CODES+ISSS, USA, 2010.
Thierry Goubier, Renaud Sirdey, St´ephane Louise, and Vincent David. ΣC: A programming model
and language for embedded manycores. In ICA3PP (1). Springer, 2011.
G. Kahn. The semantics of a simple language for parallel programming. In J. L. Rosenfeld,
editor, Information processing, pages 471–475, Stockholm, Sweden, Aug 1974. North Holland,
Amsterdam.
M. Khandelia, N.K. Bambha, and S.S. Bhattacharyya. Contention-conscious transaction ordering
in multiprocessor dsp systems. Signal Processing, IEEE Transactions on, 2006.
E. A. Lee. Consistency in dataﬂow graphs. IEEE Trans. Parallel Distrib. Syst., 1991.
E.A. Lee and D.G. Messerschmitt. Synchronous data ﬂow. Proceedings of the IEEE, 75(9):1235 –
1245, sept. 1987.
Stephane Louise. A formal evaluation of mean-time access latencies for interleaved on-chip shared
banked memory in manycores. In Embedded Multicore Socs (MCSoC), 2013 IEEE 7th International
Symposium on, pages 19–24, Sept 2013.
Orlando Moreira. Temporal analysis and scheduling of hard real-time radios running on a multiprocessor. PHD Thesis, Technische Universiteit Eindhoven, 2012.
Orlando M Moreira and Marco JG Bekooij. Self-timed scheduling analysis for real-time applications. EURASIP Journal on Advances in Signal Processing, 2007.
P. K. Murthy and S. S. Bhattacharyya. Memory management for synthesis of dsp software. 2006.
Sundararajan Sriram and Shuvra S. Bhattacharyya. Embedded Multiprocessors: Scheduling and
Synchronization. Marcel Dekker, Inc., 2nd edition, 2009.
William Thies, Michal Karczmarek, and Saman P. Amarasinghe. Streamit: A language for streaming applications. In Proceedings of the 11th International Conference on Compiler Construction,
ICCC’02, 2002.

1145

