Procedia Computer
Science
Procedia
Science
00 (2010)
1–10
ProcediaComputer
Computer
Science
1 (2012)
155–164

www.elsevier.com/locate/procedia

International Conference on Computational Science, ICCS 2010

From BSP routines to high-performance ones:
formal veriﬁcation of a transformation case
Jean Fortina , Fr´ed´eric Gavaa,∗
a Laboratory

of Algorithms, Complexity and Logic, University of Paris-East
61 avenue du G´en´eral de Gaulle, 94010 Cr´eteil cedex – France

Abstract
Paderborn’s and Oxford’s BSPLib are C libraries supporting the development of Bulk-Synchronous Parallel (BSP)
algorithms. The BSP model allows an estimation of the execution time, avoids deadlocks and non-determinism.
A natural semantics of the classical BSP communication routines have been given and used to certify a classical
numerical computation using the Coq proof assistant. In this paper, we present a semantics that emphasises the highperformance primitives and is here used to formally verify (using Coq) a simple function of optimisation of the source
code that transforms classical BSP routines to their high-performance versions.

c 2012 Published by Elsevier Ltd. Open access under CC BY-NC-ND license.
⃝

Keywords: BSP, Coq, Semantics, Formal Proof, Safe Optimisation

1. Introduction
1.1. General framework
Compilers of parallel languages are generally assumed to generate semantically equivalent machine’s instructions
from the source program. Despite intensive testing and assurance that the sequential part of your program source is
well generated [1], bugs like deadlocks can occur, when the compiler silently generates an incorrect executable for
a correct source. Writing parallel programs is known to be notoriously diﬃcult, as well as tracking down compilerintroduced bugs. Debugging both is usually a nightmare.
To cope with the ﬁrst diﬃculty, structured approaches to parallel programming using high-level tools (models,
languages, etc.) are classical solutions. They are necessary to simplify both the design of parallel algorithms and their
programming but also to ensure a better safety of the generated applications.
The second diﬃculty needs formal methods (rigorous testing is in general not suﬃcient). The veriﬁcation of the
optimisations of communications introduced by a compiler guarantees that the safety properties proved on the source
code hold for the executable compiled code as well.
For sequential programming, this kind of work has begun to be well studied and would clearly become the norm.
But for parallel (high-performance) computing, this kind of research is stammering. There are two main reasons.
∗ Corresponding

author
Email addresses: jean.fortin@ens-lyon.org (Jean Fortin), frederic.gava@univ-paris-est.fr (Fr´ed´eric Gava)
URL: http://lacl.fr/fortin/ (Jean Fortin), http://lacl.fr/gava/ (Fr´ed´eric Gava)

c 2012 Published by Elsevier Ltd. Open access under CC BY-NC-ND license.
1877-0509 ⃝
doi:10.1016/j.procs.2010.04.018

156

J. Fortin
Fortin,and
F. F.
Gava
/ Procedia
Science00
1 (2012)
155–164
J.
Gava
/ ProcediaComputer
Computer Science
(2010) 1–10

2

Figure 1: Scheme of a safe environment of BSP programming

First, parallel computing is complex. Second, most of researchers on parallel computing are usually not accustomed to
formal methods. Research on tools that optimise communication routines has been done [2] with intuitive explanations
on their reliability. To our knowledge, no such tools have been formally veriﬁed.
1.2. BSP Framework
BSP1 is a parallel model which allows an estimation of the execution time on a wide variety of architectures. The
BSP model can also be well adapted to Grid-computing and is generally presented as a bridging and emerging model
for high-performance computations. The PUB [5] is a C library of BSP communication routines2 .
An interesting advantage of BSP libraries is that they use a very small number of safe primitives compared to
the hundreds of the standard MPI. Furthermore, the model of execution is structured, making the analysis of BSP
programs easier, yet programs are still eﬃcient. BSP libraries are thus good candidates for formal semantics investigations. A natural semantics of classical BSP routines with application to the correctness of a scientiﬁc BSP
computation could be found in [6] and the Coq development is available at http://lacl.univ-paris12.fr//gava/bsp-sem.tar.
Moreover, BSP libraries provide high-performance (HP) versions of the BSP routines for both BSP message
passing (BSMP) and remote memory accesses (DRMA). These routines are proposed to programmers for improved
speedup of their program even if they are unsafe: they are unbuﬀered and do not really follow the safe BSP model
of execution. Replacing classical BSP routines by their high-performance pendant is thus of the responsibility of the
programmer or of a non-formally veriﬁed compiler analyser such as those of [2].
That is disappointing if we look for an environment where programmers can write and execute their parallel
scientiﬁc programs in a safe and eﬃcient way. Figure 1 summarises this wanted environment where the ﬁrst part is
the subject of [6].
1.3. Outline
The aim of this paper is the Coq development of a formal operational semantics (Section 3) for both BSMP and
DRMA programming styles of a kernel imperative language (Section 2) with classical and high-performance BSP
primitives. As an application of this semantics, we have done a formal veriﬁcation of a simple optimisation of the
source code that transforms some classical BSP routines by high-performance ones. That is, the original source code
and the faster generated one are semantically equivalent (Section 4). Related work is discussed in Section 5, followed
by conclusions and future work in Section 6.
1 We
2 We

refer to [3, 4] for a gentle introduction to the BSP model.
refer to the manual, http://wwwcs.uni-paderborn.de/~pub/documentation.html for C type and more details of the routines of the PUB.

157
3

J. Fortin,
F. and
Gava
Procedia
Computer
Science
1 (2012)
155–164
J. Fortin
F. /Gava
/ Procedia
Computer
Science
00 (2010)
1–10

2. BSP Core Language
2.1. Formal deﬁnition
Our core language is the classical IMP with a set Exp of expressions (integers, matrix, etc.) with operations
on them. Set X of variables is a subset of Exp with two special variables: pid and nprocs. The syntax is as
follows:
c ::= skip
Null command
|
x := e
Assignment
Sequence
| c1 ; c2
Conditional
| if e then c1 else c2 endif
| while e do c done
Iteration
| declare y := e begin c end New variable

i,p

with x, y ∈ X and e ∈ Exp. Expressions are evaluated to values v (subset of Exp) and we write: Ei , Ri |= e ⇓ v with p the
number of processors and i the pid. In the Coq formalisation, this abstract syntax is presented as inductive data types.
Ei is the store (memory as a mapping from variables to values) of processor i and Ri is the set of received values. We
i,p
i,p
suppose that Ei , Ri |= pid ⇓ i and Ei , Ri |= nprocs ⇓ p. Evaluation of Exp is not total (e.g. evaluation of 1 + true) but
for simplicity always terminates. Parallel operations are3 :
|
|
|
|
|
|

sync
push(x)
pop(x)
put(e, x, y)
get(e, x, y)
send(x, e)

Barrier of synchronisation
Registers a variable x for global access
Delete x from global access
Distant writing of x to y of processor e
Distant reading from x to y
Sending value of x to processor e

Sending a single message is done using send. In the next super-step, each processor can access the received
messages. Another way of communication is through remote memory access: after every processor has registered
a variable for direct access, all processors can read or write the value on other processors. Registering a variable
or deleting it from global access is done using push and pop. DRMA operations are put and get. All get and put
operations are executed during the synchronisation and all get operations are served before a put overwrites a value.
According to the BSP model all messages are received during the barrier of synchronisation and cannot be read
before. Barrier is done using sync which blocks the node until all other nodes have called sync and all messages sent
to it in the current super-step have been received.
In contrast to the PUB, we use basic values instead of arbitrary buﬀer addresses (char ∗). Exp is extended with
ﬁndmsg(i, e) that ﬁnds the eth message of processor i of the previous super-step and nmsg that returns the arity of Ri .
2.2. High Performance Primitives
All BSP primitives of communications have thus their high performance (HP) version (called hpput, hpget and
hpsend with the same parameters). The copies are done asynchronously and unbuﬀered. They are ﬁnished after the
next super-step and the buﬀer (src and dest) must not be changed before. The time at which the destination is written
is undeﬁned (architecture dependant). It is no surprise to say that high-performance operations improve speedup.
Take for example, the classic N-body problem4 . A simple solution would be to compute locally the interactions
of the point masses and them to perform a total exchange following with a computation of the interactions of the local
point masses with the received ones [7]. The graph in Figure 2 shows the speedups of the C+PUB program of this
method where “+HP” is the same program by simply replacing BSP sending by their HP versions.
3 We brieﬂy recall that execution of a BSP program is divided into super-steps, each separated by a global synchronisation; a super-step consists
of each processor doing some calculations on local data and communicating some data to other processors; the collective barrier of synchronisation
event guarantees that all communications of data have completed before the commencement of the next super-step.
4 This

computes the gravitational energy of N point masses:

N N mm
i j
r −r
i=1 j=1 i j
i j

where m are masses and r coordinates.

158

J. Fortin
Fortin,and
F. F.
Gava
/ Procedia
Science00
1 (2012)
155–164
J.
Gava
/ ProcediaComputer
Computer Science
(2010) 1–10

4

N-body problem with total exchange
12

BSP
BSP+HP
Linear

10

Speed-up

8
6
4
2
0
2

4

6

8

10

12

Number of processors for N=50000

Figure 2: Speed-up of the N-body computation with and without high performance routines

E, C, R, c1

i,p

E , C , R , c1

E, C, R, c1 ; c2

i,p

if c1 sync

E , C , R , c1 ; c2

i,p

E, R |= e ⇓ v
E, C, R, x := e
E[x/v], C, R, skip

E, C, R, skip; c2

i,p

E, C, R, (sync; c1 ); c2

i,p

E, C, R, sync; (c1 ; c2 )

i,p

E, R |= e ⇓ true
E, C, R, if e then c1 else c2 endif
E, C, R, while e do c done

E, C, R, sync

E, C, R, c2

i,p

i,p

E, C, R, sync; skip

i,p

i,p

i,p

E, C, R, c1

E, R |= e ⇓ false
E, C, R, if e then c1 else c2 endif

i,p

E, C, R, c2

E, C, R, if e then (c; while e do c done) else skip endif
i,p

E, R |= e ⇓ v and x E
E, C, R, declare x := e begin c end
E[x/v], C, R, c
i,p

Figure 3: Reduction rules of sequential control ﬂow

3. High Performance semantics
3.1. Generalities
In [6] the dynamic semantics is speciﬁed using a big-step operational semantics. This choice simpliﬁes greatly the
semantics and the proof of programs. We used here a small-step semantics [8] that emphasises the high-performance
(HP) routines by specifying the operation of a program one step at a time. There is thus a set of rules that we continue
to apply to conﬁgurations until reaching a ﬁnal conﬁguration if ever. In our case, we will have two kinds of reductions:

J. Fortin,
F. and
Gava
Procedia
Computer
Science
1 (2012)
155–164
J. Fortin
F. /Gava
/ Procedia
Computer
Science
00 (2010)
1–10

if {x → v} ∈ E with E = E ⊕ {x → v}
E, C, R, push(x)
E , C, R, skip
i,p

159
5

if {x → v} ∈ E with E = E ⊕ {x → v}
E, C, R, pop(x)
E , C, R, skip
i,p

i,p

E, R |= e ⇓ pid and {x → v} ∈ E and {y → v } ∈ E with C = C ∪ {put, pid%p, y, v, ←}
E, C, R, put(e, x, y)
E, C , R, skip
i,p

i,p

E, R |= e ⇓ pid and {x → v} ∈ E and {y → v } ∈ E with C = C ∪ {get, pid%p, x, y, ←}
E, C, R, get(e, x, y)
E, C , R, skip
i,p

i,p

E, R |= e ⇓ pid and {x → v} ∈ E with C = C ∪ {send, pid%p, v, ←}
E, C, R, send(x, e)
E, C , R, skip
i,p

i,p

E, R |= e ⇓ pid and {x → v} ∈ E and {y → v } ∈ E with C = C ∪ {hpput, pid%p, y, x, ←}
E, C, R, hpput(e, x, y)
E, C , R, skip
i,p

i,p

E, R |= e ⇓ pid and {x → v} ∈ E and {y → v } ∈ E with C = C ∪ {hpget, pid%p, x, y, ←}
E, C, R, hpget(e, x, y)
E, C , R, skip
i,p

i,p

E, R |= e ⇓ pid and {x → v} ∈ E with C = C ∪ {hpsend, pid%p, x, ←}
E, C, R, hpsend(x, e)
E, C , R, skip
i,p

Figure 4: Reduction rules of the PUB’s routines

local ones (on each processor) and global ones (for the whole parallel machine). The main property is that HP routines
are non-deterministic and communications can be performed at any time.
We denote E[x/v] the insertion or substitution in E of a new binding from x to v. We denote R the received
values of the previous super-step. The communications environment C contains messages to be sent in the current
super-step (noted with ←) and asynchronous messages received from other processors (noted with →). We also note
x a variable that has been registered for global access (DRMA), x for the contrary and x when that is not important.
Note that HP routines do not put in the environment a value but a variable that is a pointer to the value: values are sent
asynchronously with special rules.
3.2. Local computations
Rules for the local computations are given in Figure 3 and Figure 4. In Figure 3 the rules describe the control
ﬂow, as in the classical semantics of IMP. In Figure 4, we show the semantics for the PUB-speciﬁc instructions. In
the case of send, put, get, . . . the rule just adds a message in the environment before it is actually sent by the global
communication rules.
PUB programs are SPMD so a conﬁguration of the parallel machine is represented by a p-vector of instructions,
stores, communications and received values:
E0 , C0 , R0 , c0 · · · E p−1 , C p−1 , R p−1 , c p−1

A ﬁnal conﬁguration is an empty set of instructions (with their environment) on all processors:
E0 , C0 , R0 , skip · · · E p−1 , C p−1 , R p−1 , skip
The global reductions call the local ones with this rule:

160

J. Fortin
Fortin,and
F. F.
Gava
/ Procedia
Science00
1 (2012)
155–164
J.
Gava
/ ProcediaComputer
Computer Science
(2010) 1–10

Ei , Ci , Ri , ci
· · · Ei , Ci , Ri , ci · · ·

i,p

6

Ei , Ci , Ri , ci
· · · Ei , Ci , Ri , ci · · ·

This represents a reduction by a single processor, which then introduces an interleaving of computations. Note
that in the following rules, each ci could be skip. Asynchronous communications are done with these rules:
· · · Ei , Ci ∪ {hpsend, j, x, ←}, Ri , ci · · · where {x → v} ∈ Ei
· · · Ei , Ci , Ri , ci · · · E j , C j ∪ {hpsend, j, x, →}, R j , c j · · ·
· · · Ei , Ci ∪ {hpput, j, y, x, ←}, Ri , ci · · · where {x → v} ∈ Ei
· · · Ei , Ci , Ri , ci · · · E j [y/v], C j , R j , c j · · ·
· · · Ei , Ci ∪ {hpget, j, x, y, ←}, Ri , ci · · · E j , C j , R j , c j · · · where {y → v} ∈ E j
· · · Ei [x/v], Ci , Ri , ci · · · E j , C j , R j , c j · · ·

That is hpsend sends the value pointed by x to the memory E j of processor j, hpput writes the value at destination
and hpget takes the value at source and the two counters are increased.
3.3. Global (BSP) rules
When all asynchronous communications have been done, synchronous communications and BSP synchronisation
is done with this rule:
E0 , C0 , R0 , sync; c0 · · · E p−1 , C p−1 , R p−1 , sync; c p−1
Comm(E0 , C0 , R0 ), c0 · · · Comm(E p−1 , C p−1 , R p−1 ), c p−1
if ∀i, j, x, y, v {hpsend, j, v} Ci ∧ {hpget, j, x, v} Ci ∧ {hpput, j, y, v}

Ci

That is if each processor is in the sync case, communications are done using the Comm function that exchanges
the messages, which ﬁnishes the current super-step. The Comm function speciﬁes the order of the messages during the
communications. It modiﬁes the environment of each processor i such that Comm(Ci , Ri , Ei ) = (Ci , Ri , Ei ) as follows:
Ci = ∅ and
p−1 n j

Ri =

j=0 n=0

j

{ j, n +

a=0

na , v} if {send, i, v} ∈n C j

for BSMP, that is each processor j has sent n j messages to i and thus we take the nth message from this ordering set.
DRMA accesses are deﬁned as follow:
⎡ p−1
⎤
⎢⎢⎢
⎥⎥⎥
x → v ∈ E j and {get, j, x, y ∈ Ci }
⎢
⎥⎥
Ei = Ei ⎢⎢⎣ [y/v][y /v ] if
y → v ∈ E j and {put, i, y , v } ∈ C j ⎥⎦
j=0

That is, ﬁrst, get accesses with the natural order of processors are done (list of substitutions) and then put accesses
ﬁnish the communications.
∞
∞
We denote ⇒ for a ﬁnite derivation and ⇒ for an inﬁnite one. ⇒ (resp. ⇒) is deﬁned by induction (resp. by
co-induction):
∀i · · · Ei , Ci , Ri , skip · · · ⇒ · · · Ei , Ci , Ri , skip · · ·
∀i · · · Ei , Ci , Ri , ci · · ·

· · · Ei , Ci , Ri , ci · · ·
· · · Ei , Ci , Ri , ci · · · ⇒ · · · Ei , Ci , Ri , skip · · ·
· · · Ei , Ci , Ri , ci · · · ⇒ · · · Ei , Ci , Ri , skip · · ·

∀i · · · Ei , Ci , Ri , ci · · ·

· · · Ei , Ci , Ri , ci · · ·

∞

∞

· · · Ei , Ci , Ri , ci · · · ⇒

· · · Ei , Ci , Ri , ci · · · ⇒

J. Fortin,
F. and
Gava
Procedia
Computer
Science
1 (2012)
155–164
J. Fortin
F. /Gava
/ Procedia
Computer
Science
00 (2010)
1–10

161
7

That is, execution of a program is complete in the ﬁnal conﬁguration case or there exists a reduction step or the
program diverges. Programs that neither evaluate nor diverge according to the rules above are said to “go wrong”.
3.4. Results
As mentioned, the semantics was developed using Coq. We give here some intuitions of this development. The full
development is available at http://lacl.univ-paris12.fr/gava/bsp-hp.tar. In the Coq speciﬁcation, the dynamic semantics
are encoded as inductive predicates. Each deﬁning case of each predicate corresponds exactly to an inference rule in
the conventional, on-paper presentation of our semantics. For example, we have one inference rule for each kind of
expression and statement. We do not list the inference rules for lack of space. p-vectors are represented as functions
from Z (Coq’s integer) to instructions or environments.
Lemma 3.1. ⇒ is deterministic for programs that do not used HP routines.

Lemma 3.2. ⇒ is not deterministic.

Take for example, the simple following program:
declare x := pid begin declare y := 1 begin
push(x); hpput((pid + 1) mod nprocs, x, x); x := x + 1; sync; y := x
end end

It is impossible to know which value (pid, pid + 1 or pid − 1) is aﬀected to y.
∞

Lemma 3.3. ⇒ and ⇒ are mutually exclusive.
4. Transformation of the source code

In the general case, knowing if it is possible to replace a standard operation by a high-performance one is undecidable. In this paper, we will only consider detecting some simple cases but still frequent enough in practice to be useful
(all BSP algorithms that have a constant number of super-steps, e.g. example of Section 2, parallel sorting, FFT, some
graph algorithms, etc.).
4.1. Optimisation Conditions
When using the hpput instruction, correct data delivery is only guaranteed if:
1. no communications alter the source area;
2. no subsequent local computations alter the source area;
3. no other communications alter the destination area
4. no computation on the remote process alters the destination area during the entire super-step.
Conditions to replace safely the routine get by hpget are close to these ones. For the send instruction, we have
only one buﬀer on the source area, so the conditions (1) and (2) are enough.
4.2. Overview of the Translation
In order to simplify the detection of optimise operations, we will only consider the programs in which the instruction sync are located outside conditional instructions (while and if). The program is then composed of an alternation
of sequential blocks and sync instructions:

162

J. Fortin
Fortin,and
F. F.
Gava
/ Procedia
Science00
1 (2012)
155–164
J.
Gava
/ ProcediaComputer
Computer Science
(2010) 1–10

8

The advantage is that during the execution, every processor does the same synchronisation at the same point of the
program. This way, if in the block bi we have an instance of put(xl , yd , pid), the four conditions can be translated
(within the block bi ): (1) for each processor, no communication alter xl ; (2) the processor does not modify xl after
the call to put; (3) for each processor, no other communication alters yd on processor pid; (4) for the processor pid,
there is no local modiﬁcation of yd . The optimisation function must comply with those four conditions. For (1) the
check is easy by searching the block of code. For (2) it is necessary to analyse the control ﬂow in order to know which
instructions are to be executed after the call to the put instruction.
For (3), a problem is raised by the determination of the target pid, which might be computed by a complex method.
Thus, it is practically impossible to statically check this exact condition. However, when we are in one of the following
two cases, the analysis is made possible: no communication alters yd , on any processor and the pid are computed by
simple arithmetic expressions, for which it is possible to obtain the result by a direct analysis. In practice, both cases
cover a signiﬁcant number of programs. For (4), there is the same problem with the pid determination. We treat the
programs in the same cases than previously. For other instructions get and send, the technique is identical.
4.3. Optimisation of a block
From this informal analysis we can deduce the following optimisation on a block of code (to simplify the presentation we just take into account the put instruction). The function B that decomposes a program into a list of blocks
can be written as follows:
B(sync) =
B(i1 ; sync) =
B(sync; i2 ) =
B(i1 ; i2 ) =
B(c)

=

T

[]T
[b1 ; . . . ; bn ]T if B(i1 ) = a [b1 ; . . . ; bn ]b
T
[b1 ; . . . ; bn ]b if B(i2 ) = a [b1 ; . . . ; bn ]b
a
[b1 ; . . . ; bn ; b1 ; . . . ; bm ]b
if B(i1 ) = a [b1 ; . . . ; bn ]b and B(i2 ) = a [b1 ; . . . ; bm ]b and (b = T ) ∨ (a = T )
F
[c]F Otherwise
a

where a [b1 ; . . . ; bn ]b are blocks where a (resp. b) is a boolean (T or F) that indicates if a synchronisation occurs before
(resp. after) the ﬁrst (resp. last) block. We recall that for these analyses we suppose that sync can not occur in the
body of the control structures ”if-then-else“ and ”while“
This inductive function O pos is deﬁned in Figure 5 and transforms a block (as a command) to another one. It is
applied to a whole block bl to be analysed (search for a put instructions to be optimised) as follows: O[] (bl). pos
contains the position in the instruction block tree of the current instruction, encoded by a list of directions from the
root (1 for the ﬁrst sub-tree, 2 for the second sub-tree if it exists). The functions no_modify and no_comm_target
are simple in-depth searches of the instruction, to check that the matching instructions are not called.
pos is useful in the call to no_modify_after, which searches only in the instructions that are executed after the
put instruction. no_modify_after is in the same way deﬁned by an in-depth recursive search.
Lemma 4.1. For the semantics ⇒ and a program c, if B(c) = [b1 ; · · · ; bn ] then
and c holds to an equivalent result.

O[] (b1 ); sync; · · · ; sync; O[] (bn )

The proof of semantic preservation for the translation proceeds by induction over the evaluation derivation and case
analysis on the last evaluation rule used. The proof shows that, assuming suitable consistency conditions over the BSP
routines, the generated high-performance ones evaluate in ways that simulate the evaluation of the corresponding BSP
programs. The function that decomposes a program into blocks and the optimisation function are written recursively,
according to the deﬁnitions given above, with the Fixpoint constructor. To prove their correctness, we proceed by
equivalence between the diﬀerent states of the formalisation.
First, we prove that the decomposition into blocks is correct, that is to say, the execution with the small-step
semantics of the sequence of blocks gives the same results that the execution of the original source code. Then, for
a given block, we prove that if the four conditions listed above are true, the optimised code evaluates to the same
values that the initial code would give. To conclude the proof, we show that the optimisation function given above
only changes the put instructions when the four conditions hold.

J. Fortin,
F. and
Gava
Procedia
Computer
Science
1 (2012)
155–164
J. Fortin
F. /Gava
/ Procedia
Computer
Science
00 (2010)
1–10

O pos (c1 ; c2 )

=

163
9

O1::pos (c1 ); O2::pos (c2 )

(while b do c done) = while b do O1::pos (c) done
O pos (if b then c1 else c2 endif) = if e then O1::pos (c1 ) else O2::pos (c2 ) endif
O pos (c) = c when c ∈ {skip, get, send, hpput, hpsend, hpget}
O

pos

O pos (x := e)

=

x := e

(declare y := e begin c end) = declare y := e begin O pos (c) end
⎧
⎪
no_comm_target(bl, xl) ∧
⎪
⎪
⎪
⎪
⎪
no_modify_after(bl, pos, xl) ∧
⎪
⎪
⎪
hpput(xl, yd, pid) if
⎪
⎪
no_comm_target(bl, yd) ∧
⎨
O pos (put(xl, yd, pid)) = ⎪
⎪
⎪
no_modify(bl,
yd))
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎩ put(xl, yd, pid)
Otherwise

O

pos

where bl is the initial whole block to be analysed

Figure 5: Optimisation of a block

5. Related work
5.1. Proof of BSP Programs
Simplicity of the BSP model allows to prove properties and correctness of BSP programs. Diﬀerent approaches for
proofs of BSP programs have thus been studied such as BSP functional programming using Coq [9] or the derivation
of BSP imperative programs using Hoare’s axiom semantics [10]. A small-step semantics for BSPlib programs is
presented in [11] but without BSMP routines, diverging or high-performance programs.
The main drawback of these approaches is that they use their own languages that are not a subset of real programming languages. Also they neither used any proof assistant (except [9]).
5.2. Formally veriﬁed source-code transformations
There exists a considerable body of earlier work on machine-checked correctness proofs of parts of compilers (see
[1] for surveys). Notably, there exists published work tending to focus on a special part of a compiler, such as the
underlying static analyses [12] or translation of a high-level language to virtual machine code [13]. Several formal
semantics of C-like languages have also been deﬁned [14]. But all these works are for sequential programs. Also,
as noticed in [1], shared-memory concurrency is raising serious diﬃculties both with the veriﬁcation of concurrent
programs and with the reuse, in a concurrent setting, of languages and compilers designed for sequential execution.
A work that is close to ours is that of [15]. Using Isabelle/HOL, they formalise the semantics of C0 (a subset
of the C language, close to Pascal) and a compiler from C0 down to DLX assembly code. They provide both a bigstep semantics and a small-step semantics for C0, the latter enabling reasoning about non-terminating and concurrent
executions.
Our approach has the advantage to be simpler than concurrent programming: we used a structured parallelism
(BSP execution). But that shows that the simpler optimisations of how processors exchanged data generate hard
proofs.
6. Conclusion
Formal methods in general and program proof in particular are increasingly being applied to software. These
applications create a strong need for on-machine formalisation and veriﬁcation of programming language semantics.
In this paper, we have presented a formal operational semantics for BSP programs which also introduces highperformance primitives. We have also given a simple transformation of the source code that generated some calls to
high-performance routines in place of BSP classical ones. An originality of this paper is that the semantics of the

164

Fortin,
F.F.Gava
Science001 (2010)
(2012)1–10
155–164
J.J.Fortin
and
Gava/ Procedia
/ Procedia Computer
Computer Science

10

language as well as the transformation have been written in the speciﬁcation language of the Coq proof assistant. The
proof of observational semantic equivalence between the source and generated code has been machine-checked using
Coq which ensures a better trust in the results. An executable compiler can be obtained by automatic extraction of
executable Caml code from Coq. This work is our ﬁrst experiment to create a certiﬁed software for optimisation:
transforming some buﬀered operations to unbuﬀered ones. Much work is necessary to optimise more programs and
certify this translator.
The main goal of this work is an environment where programmers could prove correctness of their BSP programs
and at the end automatically get high-performance versions in a certiﬁed manner. In ﬁnal, adapting all these works to
MPI would be a great challenge because there are still many more parallel routines with more complicated semantics.
A ﬁrst approach would be to only consider collective routines which are still BSP compliant.
[1] X. Leroy, Formal veriﬁcation of a realistic compiler, Communications of the ACM 52 (7) (2009) 107–115.
[2] A. Danalis, L. Pollock, M. Swany, Automatic MPI application transformation with ASPhALT, in: Workshop on Performance Optimization
for High-Level Languages and Libraries (POHLL 2007), in conjunction with IPDPS, 2007.
[3] R. H. Bisseling, Parallel Scientiﬁc Computation. A structured approach using BSP and MPI, Oxford University Press, 2004.
[4] D. B. Skillicorn, J. M. D. Hill, W. F. McColl, Questions and Answers about BSP, Scientiﬁc Programming 6 (3) (1997) 249–274.
[5] O. Bonorden, B. Juurlink, I. V. Otte, O. Rieping, The Paderborn University BSP (PUB) library, Parallel Computing 29 (2) (2003) 187–207.
[6] F. Gava, J. Fortin, Formal Semantics of a Subset of the Paderborn’s BSPlib, in: PDCAT 2008, 2008, to appear.
[7] K. Hinsen, Parallel scripting with Python, Computing in Science & Engineering 9 (6).
[8] F. Gava, J. Fortin, Two Formal Semantics of a Subset of the Paderborn University BSPlib, in: D. E. Baz, F. Spies, T. Gross (Eds.), PDP 2009,
no. P3544, IEEE Computer Society, 2009, pp. 44–51.
[9] F. Gava, Formal Proofs of Functional BSP Programs, Parallel Processing Letters 13 (3) (2003) 365–376.
[10] Y. Chen, W. Sanders, Top-Down Design of Bulk-Synchronous Parallel Programs, Parallel Processing Letters 13 (3) (2003) 389–400.
[11] J. Tesson, F. Loulergue, Formal Semantics for the DRMA Programming Style Subset of the BSPlib Library, in: J. Weglarz, R. Wyrzykowski,
B. Szymanski (Eds.), Seventh International Conference on Parallel Processing and Applied Mathematics (PPAM 2007), LNCS, Springer,
2007.
[12] S. Lerner, T. Millstein, E. Rice, C. Chambers, Automated soundness proofs for dataﬂow analyses and transformations via local rules, in: 32nd
symposium Principles of Programming Languages, ACM Press, 2005, pp. 364–377.
[13] G. Klein, T. Nipkow, A machine-checked model for a java-like language, virtual machine and compiler, ACM Transactions on Programming
Languages and Systems 4 (28) (2006) 619–695.
[14] M. Norrish, C formalised in HOL, Ph.D. thesis, University of Cambridge (1998).
[15] D. Leinenbach, W. Paul, E. Petrova, Towards the formal veriﬁcation of a C0 compiler, in: Proc. Conf. on Software Engineering and Formal
Methods (SEFM), IEEE Computer Society Press, 2005, pp. 2–11.

