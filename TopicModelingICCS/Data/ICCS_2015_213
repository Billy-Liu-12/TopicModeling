Procedia Computer Science
Volume 51, 2015, Pages 2814–2821
ICCS 2015 International Conference On Computational Science

Enhancing ELM-based facial image classiﬁcation
by exploiting multiple facial views
Alexandros Iosiﬁdis, Anastasios Tefas and Ioannis Pitas
Department of Informatics, Aristotle University of Thessaloniki, Thessaloniki, Greece
{tefas,pitas}@aiia.csd.auth.gr

Abstract
In this paper, we investigate the effectiveness of the Extreme Learning Machine (ELM) network in facial
image classiﬁcation. In order to enhance performance, we exploit knowledge related to the human face
structure. We train a multi-view ELM network by employing automatically created facial regions of
interest to this end. By jointly learning the network parameters and optimized network output combination weights, each facial region appropriately contributes to the ﬁnal classiﬁcation result. Experimental
results on three publicly available databases show that the proposed approach outperforms facial image
classiﬁcation based on a single facial representation and on other facial region combination schemes.
Keywords: Extreme Learning Machine, Facial Image Classiﬁcation, Multi-view Learning

1

Introduction

Extreme Learning Machine [7] is a relatively new algorithm for fast Single-hidden Layer Feedforward
Neural (SLFN) network training that leads to fast network training requiring low human supervision.
Conventional SLFN network training algorithms require the input weights and the hidden layer bias
values to be adjusted using a parameter optimization approach, like gradient descend. However, gradient
descend-based learning techniques are generally slow and may decrease the network’s generalization
ability, since they may lead to local minima. Unlike the popular thinking that the networks parameters
need to be tuned, in ELM the input weights and the hidden layer bias values are randomly assigned. The
network output weights are, subsequently, analytically calculated. Despite the fact that the determination
of the network hidden layer output is a result of randomly assigned weights, it has been shown that
SLFN networks trained by using the ELM algorithm have the properties of global approximators [5].
In addition, it has been recently shown that ELM networks are able to outperform other state-of-the-art
classiﬁers, like Support Vector Machine (SVM). Due to its effectiveness and its fast learning process,
the ELM network has been widely adopted in many classiﬁcation problems, including facial image
classiﬁcation [4, 30, 6, 10, 13, 11, 16, 15].
Despite its success in many classiﬁcation problems, the ELM training process by randomly assigning the network hidden layer weights and the corresponding bias values lacks the ability of learning
optimized hidden network parameters from data. For example, a SLFN network trained on a facial
2814

Selection and peer-review under responsibility of the Scientiﬁc Programme Committee of ICCS 2015
c The Authors. Published by Elsevier B.V.
doi:10.1016/j.procs.2015.05.440

Enhancing ELM-based facial image classiﬁcation

A. Iosiﬁdis, A. Tefas, I. Pitas

image database by using the Backpropagation algorithm would be able to learn network hidden layer
parameters highlighting the facial regions, e.g. the regions corresponding to the eyes or to the mouth,
discriminating the persons in the database. On the other hand, the ELM algorithm can be considered as
a learning process formed by two processing steps. The ﬁrst step corresponds to a nonlinear mapping
process of the input space to a (usually high-dimensional) feature space, noted as ELM space hereafter,
preserving some properties of the training data. In the second step, an optimization scheme is employed
for the determination of a linear projection of the training data (represented in the ELM space) to a
low-dimensional feature space determined by the network target vectors. In this sense, ELM is closely
related to kernel methods, like Kernel Spectral Regression (KSR) [1] and kernel Support Vector Machine (kSVM) [3]. Such methods by performing distance-based discrimination, are not able to highlight
the most discriminative facial regions.
In this paper, we investigate the effectiveness of the ELM network in the facial image classiﬁcation
problem. Despite the fact that recent works in face recognition focus their attention on the unconstrained
face recognition problem, usually referred to as ‘Face Recognition in the Wild’ [18, 19, 17, 2, 28],
we focus our attention on a more restrictive application scenario requiring the classiﬁcation of facial
images depicting a nearly-frontal facial pose [14]. Such facial images can be obtained by face detection
and tracking [21, 9, 29]. This problem needs to be addressed in many applications, e.g. applications
facilitating assisted living of the elderly [27, 8], or relating to portable electronic devises usage. In
such cases, the challenges that a facial image classiﬁcation method needs to be able to face include
illumination changes, different facial expressions and facial details (e.g., open/closed eyes), occluded
facial regions (e.g., sun glasses and scarfs) and small face rotation/tilting (up to 20 degrees). We propose
the exploitation of knowledge relating to the human face based on anthropometric ratios. We propose
the use of multi-view Neural Networks, i.e., of multiple jointly-trained SLFN networks, each operating
on a different facial region (view) in order to overcome the previously described weakness of the ELM
algorithm. By optimally weighting the contribution of each view on the ﬁnal classiﬁcation result, the
proposed classiﬁcation scheme automatically combines discriminative information related to different
facial regions, in order to enhance facial image classiﬁcation performance. Experimental results on
three publicly available databases denote that the combined approach is able to outperform facial image
classiﬁcation performed on pre-deﬁned facial regions, as well as on the entire facial image. In addition,
the adoption of the proposed facial view combination scheme outperforms other combination schemes
that are usually employed in the literature.
The remainder of the paper is structured as follows. In Section 2 we describe the proposed facial
image classiﬁcation method. Section 3 presents experiments conducted in order to evaluate its performance. Finally, conclusions are drawn in Section 4.

2

Proposed Method

As has been previously described, we are interested in facial image classiﬁcation in the cases where
the depicted person has a nearly frontal facial pose. In Figure 1a we illustrate a facial image of the
ORL database, having a frontal facial pose. Anthropometric ratios relating to the human face structure
denote that it can be roughly partitioned in ﬁve equal horizontal zones. As can be seen in Figure 1b,
the facial regions corresponding to the human eyes fall in the second and third zone, while the facial
regions belonging to the mouth fall in the fourth and ﬁfth zone. In addition, human faces are (in general)
symmetric with respect to the vertical axis. As has been shown in [24], by exploiting the symmetry of
the human face, enhanced facial image classiﬁcation performance can be achieved. Thus, we can also
roughly partition the facial images in two regions, as shown in Figure 1b.
In order to exploit the above-mentioned properties of human faces in an ELM-based classiﬁcation
framework, we propose the use of ﬁve facial image representations: the entire facial image, an image
2815

Enhancing ELM-based facial image classiﬁcation

A. Iosiﬁdis, A. Tefas, I. Pitas

depicting the person’s eyes, an image depicting the person’s mouth and two images depicting the two
(left and right half) partitions of the person’s face with respect to the vertical axis, as illustrated in
Figure 1c. Such representations can handle partial facial image occlusion that can create problems in
face recognition [25] and facial expression recognition [20]. Furthermore, these facial facial image
representations can be thought as different views of the facial image.

(a)

(b)

(c)

Figure 1: a) Example of a human face having frontal facial pose, b) the same human face partitioned
based on anthropometric ratios and c) the ﬁve facial image views used for face representation.
After determining the facial views, we perform facial image classiﬁcation based on multi-view Neural Networks. We employ the recently proposed Multi-view Regularized Extreme Learning Machine
(MRELM) algorithm [12] to this end. A brief description of the MRELM algorithm is given in the
following.

2.1

Multi-view Regularized Extreme Learning Machine

Let us denote by U a facial image database, containing N images, depicts NP persons. We partition each
facial image in order to create the above-described facial views. After facial view creation, each view
image is represented as a matrix containing the (grayscale) intensity values and the obtained matrices
are vectorized in order to produce vectors xvi ∈ RDv , i = 1, . . . , N, v = 1, . . . , V , where v runs along
the different facial views and Dv is the dimensionality of the v-th facial view.
We would like to employ xvi and the corresponding labels ci , in order to train V SLFN networks,
each operating on one view. To this end we map the vectors of each view v to one ELM space RHv , by
v
using randomly chosen input weights Win
∈ RDv ×Hv and input layer bias values bv ∈ RHv . Hv is the
dimensionality of the ELM space related to view v. By doing this, each vector xvi is mapped to a vector
φvi ∈ RHv . The network target vectors ti = [ti1 , ..., tiNP ]T , each corresponding to one facial image in
the database, are set to tij = 1 for facial images depicting person j, i.e., when ci = j, and to tij = −1
otherwise.
v
In order to determine both the network output weights Wout
∈ RHv ×NP and appropriate view
V
combination weights γ ∈ R , the following optimization problem is solved:
V

Minimize:

J =
V

Subject to:

N

1
v
Wout
2 v=1

2
F

vT v
γv Wout
φi

− ti = ξi , i = 1, ..., N,

v=1
γ 22 =

1,

+

c
2

ξi

2
2

(1)

i=1

(2)
(3)

where ξ i ∈ RNP is the error vector related to the i-th facial image and c is a regularization parameter
expressing the importance of the training error in the optimization process.
2816

Enhancing ELM-based facial image classiﬁcation

A. Iosiﬁdis, A. Tefas, I. Pitas

By setting the representations of xvi in the corresponding ELM space in a matrix Φv =
the network responses of the entire training set are given by:

[φv1 , . . . , φvN ],

V

O=

vT v
Φ .
γv Wout

(4)

v=1

By substituting (2) in (1) and taking the equivalent dual problem, J can be written as:
JD (γ) =

c T
λ
γ Pγ − crT γ + γ T γ + const,
2
2

(5)

kT k lT
l
and
Φ Φ Wout
where P ∈ RV ×V is a matrix having its elements equal to [P]kl = tr Wout
ϑJ
(
γ
)
D
V
T
vT v
r ∈ R is a vector having its elements equal to rv = tr T Wout Φ . By solving for ϑγ = 0, γ
is given by:
−1
λ
γ = P+ I
r.
(6)
c

By substituting (2) in (1) and taking the equivalent dual problem, J can also be written as:
V

1
c
vT
v
+ tr
tr Wout
Wout
2 v=1
2

v
) =
JD (Wout

V

−

c

V

V

vT v lT
l
γv γl Wout
Φ Φ Wout

v=1 l=1

vT v T
tr γv Wout
Φ T + const.

(7)

v=1

By solving for

v
ϑJD (Wout
)
v
ϑWout

v
= 0, Wout
is given by:

v
Wout
=

2
I + γk Φv Φv T
cγk

−1

Φv (2T − O)T ,

(8)

v
In order to jointly optimize J with respect to both Wout
and γ, an iterative optimization scheme
v
formed by two optimization steps is followed, where Wout,1 are initialized by training each SLFN
network independently. The iterative optimization process is terminated when (JD (t) − JD (t +
1))/JD (t) < , where is a small positive value.
v
After the determination of the set {γv , Wout
}Vv=1 , the network response for a given set of facial
v
D
view vectors xt ∈ R is given by:
V

ol =

vT v
γv Wout
φt ,

(9)

v=1

where φvt is the facial view vector representation in the ELM space corresponding to view v.

3

Experiments

In this Section, we describe experiments conducted in order to evaluate the performance of the proposed
facial image classiﬁcation scheme. We have employed three publicly available databases, namely the
ORL, AR and Extended YALE-B databases. A brief description of the databases is given in the following Subsection. We have used the facial images provided by the databases and resized them to ﬁxed size
images of 40 × 30 pixels for computation speed consideration. Since there is not a commonly adopted
2817

Enhancing ELM-based facial image classiﬁcation

A. Iosiﬁdis, A. Tefas, I. Pitas

training-test partitioning of the databases, we have randomly partitioned the databases in ﬁve sets, by
keeping the 10%, 20%, 30%, 40% and 50% of the facial images depicting each person for training and
the remaining facial images for testing.
In all the experiments we have employed the RBF activation function for the networks’ hidden layer
v 2
xv
i −wk 2
outputs calculation φvik = exp
, where wkv is the weight of the k-th hidden layer neuron
2σv2
for view v. The value of σv was set equal to the mean Euclidean distance between the training vectors
xvi and the network hidden layer weights wkv . The dimensionality of the ELM space has been set to
1000 in all the cases, which is a value that has been shown to provide satisfactory performance in many
classiﬁcation problems.

3.1

Databases

The ORL database contains 10 images of 40 persons, leading to a total number of 400 images ([26]).
The images were captured at different times and with different conditions, in terms of lighting, facial
expressions (smiling/not smiling) and facial details (open/closed eyes, with/without glasses). Facial
images were taken in frontal position with a tolerance for face rotation and tilting up to 20 degrees. A
set of ten images depicting a person of the database is illustrated in Figure 2a.
The AR database contains over 4000 images depicting 70 male and 56 female faces ([23]). In our
experiments we have used the preprocessed (cropped) facial images provided by the database, depicting 100 persons (50 males and 50 females) having a frontal facial pose, performing several expressions
(anger, smiling and screaming), in different illumination conditions (left and/or right light) and with
some occluded facial regions (sun glasses and scarf). Each person was recorded in two sessions, separated by two weeks. A set of ten images depicting a person of the database is illustrated in Figure
2b.
The Extended YALE-B database contains images of 38 persons in 9 poses, under 64 illumination
conditions ([22]) captured under varying lighting conditions. In our experiments we have used the
frontal cropped images provided by the database. A set of ten images depicting a person of the database
is illustrated in Figure 2c.

(a)

(b)

(c)
Figure 2: Facial images depicting persons of the a) ORL, b) Extended YALE-B and c) AR datasets.

3.2

Experimental Results

Tables 1, 2 and 3, illustrate the facial image classiﬁcation rates obtained by applying the proposed
approach in the ORL, AR and YALE-B databases, respectively. In these Tables we also provide the
classiﬁcation rates obtained by using each facial view independently and two commonly used view
2818

Enhancing ELM-based facial image classiﬁcation

A. Iosiﬁdis, A. Tefas, I. Pitas

Table 1: Classiﬁcation rates on the ORL database.

10%
20%
30%
40%
50%

Face
73.06 %
81.56 %
85.71 %
88.33 %
87 %

Eyes
55.28 %
61.56 %
65 %
69.17 %
71 %

Mouth
59.17 %
63.75 %
69.29 %
73.33 %
76 %

FaceL
68.89 %
74.69 %
80.71 %
80 %
81 %

FaceR
62.78 %
74.38 %
78.93 %
85.42 %
83 %

Conc
71.94 %
87.81 %
88.57 %
91.67 %
93.5 %

Mean
71.39 %
84.06 %
87.14 %
90.83 %
91 %

MV
75.28 %
88.75 %
90.36 %
94.17 %
96 %

Mean
47.65 %
52.38 %
65.31 %
69.33 %
88.85 %

MV
50.61 %
52.86 %
65.94 %
70.56 %
89 %

Table 2: Classiﬁcation rates on the AR database.

10%
20%
30%
40%
50%

Face
39.52 %
40.71 %
56.06 %
58.94 %
77.23 %

Eyes
39.91 %
47.95 %
70.69 %
75.33 %
81.08 %

Mouth
28.26 %
36.33 %
40.81 %
45.17 %
49.96 %

FaceL
41.17 %
38.95 %
55.5 %
56.28 %
76 %

FaceR
40.7 %
45.48 %
53.94 %
56.06 %
74.54 %

Conc
49.52 %
50.86 %
62.94 %
65.83 %
87.31 %

combination schemes, i.e. view combination by using the mean network output (noned by ‘Mean’) [10]
and classiﬁcation based on a combined data representation obtained by concatenating the facial view
representations in the ELM space (noted by ‘Conc’). It should be noted here that a facial view combination by concatenating the facial view representations in the input space is not expected to enhance facial
image classiﬁcation performance, when compared to the case of using only the entire facial image (i.e.,
the ﬁrst facial view), since the same information is exploited in both cases.
As can be seen in these Tables, facial image classiﬁcation by employing the entire facial image
usually outperforms facial image classiﬁcation based on the remaining facial regions. However, this is
not always the case. In the AR database the facial region depicting the human eyes seems to be more
discriminative, as the ELM network trained on this view outperforms the one trained on the entire facial
image in all the cases. This fact may be explained by the fact that AR database contains facial images
having occlusions (the persons wear sun glasses and scarfs).
Generally, view combination approaches increase the classiﬁcation performance. Facial image classiﬁcation based on a combined representation obtained by concatenating the view vectors generally

Table 3: Classiﬁcation rates on the YALE-B database.

10%
20%
30%
40%
50%

Face
64.61 %
71.21 %
84.15 %
92.24 %
95.23 %

Eyes
51.77 %
58.05 %
54.91 %
84.56 %
87.91 %

Mouth
63.66 %
66.25 %
66.78 %
92.17 %
93.26 %

FaceL
54.4 %
60.42 %
58.42 %
92.11 %
97.2 %

FaceR
61.66 %
76.78 %
72.81 %
90.37 %
92.19 %

Conc
70.74 %
82.77 %
84.04 %
94.94 %
97.62 %

Mean
73.64 %
87.67 %
82.81 %
94.94 %
97.7 %

MV
74.64 %
89.78 %
88.01 %
95.43 %
97.86 %
2819

Enhancing ELM-based facial image classiﬁcation

A. Iosiﬁdis, A. Tefas, I. Pitas

outperforms single-view classiﬁcation. Classiﬁcation based on the mean network output outperforms
the view concatenation approach in most cases. This seems reasonable, since each network trained on
a different view tries to discriminate different facial representations independently. If we consider the
network output vectors as a facial representation of increased discrimination power, the combination of
discriminant facial image representations enhances classiﬁcation performance.
Finally, the combination of the network outputs based on optimized combination weights further
increases classiﬁcation performance. It can be seen that the view combination scheme exploiting optimized weights outperforms the remaining view combination schemes in most cases. However, there
are cases where this increase in the performance is marginal. This is observed in the AR and Yale-B
databases when a relatively large training set is employed. In the cases where the adopted training set is
small (e.g., in the case where the 10% of the data are used for training), the view combination approach
determining optimized weights seems to be more effective.
Overall, it can be seen that the proposed facial image classiﬁcation scheme outperforms the competing ones in most cases and achieves satisfactory performance in all the cases.

4

Conclusions

In this paper, we have investigated the effectiveness of the ELM network in (near-frontal) facial image classiﬁcation. In order to enhance performance we proposed the use of multi-view ELM networks,
which are trained in different facial representations (views) automatically obtained by exploiting anthropometric ratios. By jointly learning the network parameters and optimized network outputs combination
weights, each facial region appropriately contributes to the ﬁnal classiﬁcation result. Experimental results on three publicly available databases show that the proposed approach outperforms facial image
classiﬁcation based on single facial views and on other view combination schemes.

Acknowledgment
The research leading to these results has received funding from the European Union Seventh Framework
Programme (FP7/2007-2013) under grant agreement number 316564 (IMPART).

References
[1] D. Cai, X. He, and J. Han. Speed up Kernel Discriminant Analysis. International Journal on Very Large Data
Bases, 20(1):21–33, 2011.
[2] Z. Cui, W. Li, D. Xu, S. Shan, and X. Chen. Fusing robust face region descriptors via multiple metric learning
for face recognition in the wild. Computer Vision and Pattern Recognition, 2013.
[3] R. E. Fan, P. H. Chen, and C. J. Lin. Working set selection using the second order information for training
svm. Journal of Machine Learning Researchy, 6:1889–1918, 2005.
[4] T. Helmy and Z. Rasheed. Multi-category bioinformatics dataset classiﬁcation using Extreme Learning Machine. IEEE Evolutionary Computation, 2009.
[5] G. B. Huang, L. Chen, and C. K. Siew. Universal approximation using incremental constructive feedforward
networks with random hidden nodes. IEEE Transactions on Neural Networks, 17(4):879–892, 2006.
[6] G. B. Huang, H. Zhou, X. Ding, and R. Zhang. Extreme Learning Machine for regression and multiclass
classiﬁcation. IEEE Transactions on Systems, Man, and Cybernetics, Part B: Cybernetics, 42(2):513–529,
2012.
[7] G. B. Huang, Q. Y. Zhu, and C. K. Siew. Extreme Learning Machine: a new learning scheme of feedforward
neural networks. IEEE International Joint Conference on Neural Networks, 2004.

2820

Enhancing ELM-based facial image classiﬁcation

A. Iosiﬁdis, A. Tefas, I. Pitas

[8] A. Iosiﬁdis, E. Marami, A. Tefas, I. Pitas, and K. Lyroudia. The MOBISERV-AIIA eating and drinking
multi-view database for vision-based assisted living. Journal of Information Hiding and Multimedia Signal
Processing, 6(2):254–273, 2015.
[9] A. Iosiﬁdis, S. G. Mouroutsos, and A. Gasteratos. A hybrid static/active video surveillance system. International Journal of Optomechatronics, 5(1):80–95, 2011.
[10] A. Iosiﬁdis, A. Tefas, and I. Pitas. Minimum Class Variance Extreme Learning Machine for human action
recognition. IEEE Transactions on Circuits and Systems for Video Technology, 23(11):1968–1979, 2013.
[11] A. Iosiﬁdis, A. Tefas, and I. Pitas. Minimum Variance Extreme Learning Machine for human action recognition. IEEE International Conference on Acoustics, Speech and Signal Processing, 2014.
[12] A. Iosiﬁdis, A. Tefas, and I. Pitas. Multi-view regularized Extreme Learning Machine for human action
recognition. Hellenic Conference on Artiﬁcial Intelligence, 2014.
[13] A. Iosiﬁdis, A. Tefas, and I. Pitas. Regularized Extreme Learning Machine for multi-view semi-supervised
action recognition. Neurocomputing, 145:250–262, 2014.
[14] A. Iosiﬁdis, A. Tefas, and I. Pitas. Class-speciﬁc Reference Discriminant Analysis with application in human
behaviour analysis. IEEE Transactions on Human-Machine Systems, D.O.I. 10.1109/THMS.2014.2379274,
2015.
[15] A. Iosiﬁdis, A. Tefas, and I. Pitas. Graph Embedded Extreme Learning Machine. IEEE Transactions on
Cybernetics, D.O.I. 10.1109/TCYB.2015.2401973, 2015.
[16] A. Iosiﬁdis, A. Tefas, and I. Pitas. On the kernel Extreme Learning Machine classiﬁer. Pattern Recognition
Letters, 54:11–17, 2015.
[17] R. Jafri and H. R. Arabnia. PCA-based methods for face recognition. International Conference on Security
& Management, 2007.
[18] R. Jafri and H.R. Arabnia. A survey of component-based face recognition approaches. International Conference on Artiﬁcial Intelligence, 2007.
[19] R. Jafri and H.R. Arabnia. A survey of face recognition techniques. Journal of Information Processing
Systems, 5(6):41–68, 2009.
[20] I. Kotsia, I. Buciu, and I. Pitas. An analysis of facial expression recognition under partial facial image occlusion. Image and Vision Computing, 26(7):1052–1067, 2008.
[21] K. Kucharski, W. Skarbek, G. Stamou, N. Nikolaidis, and I.Pitas. Morphological and adaboost face detectors
comparison. Workshop on Immersive Communication and Broadcast Systems, 2005.
[22] K. C. Lee, J. Ho, and D. Kriegman. Acquiriing linear subspaces for face recognition under varialbe lighting.
IEEE Transactions on Pattern Analysis and Machine Intelligence, 27(5):684–698, 2005.
[23] A. Martinez and A. Kak. PCA versus LDA. IEEE Transactions on Pattern Analysis and Machine Intelligence,
23(2):228–233, 2001.
[24] K. Papachristou, A. Tefas, and I. Pitas. Subspace learning with enriched databases using symmetry. EuroChina Conference on Intelligent Data Analysis and Applications, 2014.
[25] A. Rama, F. Tarres, L. Goldmann, and T. Sikora. More robust face recognition by considering occlusion
information. IEEE International Conference on Automatic Face & Gesture Recognition, 2008.
[26] F. Samaria and A. Harter. Parameterisation of a stochastic model for human face identiﬁcation. IEEE Workshop on Applications of Computer Vision, 1994.
[27] A. Tefas and I. Pitas. Human centered interfaces for assisted living. International Conference on ManMachine Interactions, 2011.
[28] D. Yi, Z. Lei, and S. Z. Li. Towards pose robust face recognition. Computer Vision and Pattern Recognition,
2013.
[29] O. Zoidi, A. Tefas, and I. Pitas. Visual object tracking based on local steering kernels and color histograms.
IEEE Transactions on Circuits and Systems for Video Technology, 23(5):870–882, 2013.
[30] W. Zong and G. B. Huang. Face recognition based on Extreme Learning Machine. Neurocomputing,
74(16):2541–2551, 2011.

2821

