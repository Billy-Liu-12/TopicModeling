Recent Progress in General Sparse Direct
Solvers
Anshul Gupta
IBM T.J. Watson Research Center
P.O. Box 218, Yorktown Heights, NY 10598
anshul@watson.ibm.com
http://www.cs.umn.edu/˜agupta/wsmp.html

Abstract. During the past few years, algorithmic improvements alone
have shaved almost an order of magnitude oﬀ the time required for the
direct solution of general sparse systems of linear equations. Combined
with a similar increase in the performance to cost ratio due to hardware
advances during this period, current sparse solver technology makes it
possible to solve those problems quickly and easily that might have been
considered impractically large until recently. In this paper, we compare
the performance of some commonly used software packages for solving
general sparse systems. In particular, we demonstrate the consistently
high level of performance achieved by WSMP—the most recent of such
solvers. We compare the various algorithmic components of these solvers
and show that the choices made in WSMP enable it to run two to three
times faster than the best amongst other similar solvers. As a result,
WSMP can factor some of the largest sparse matrices available from real
applications in a few seconds on 4-CPU workstation.

1

Introduction

Developing an eﬃcient parallel, or even serial, direct solver for general sparse
systems of linear equations is a challenging task that has been the subject of
research for the past four decades. Several breakthroughs have been made during
this time. As a result, a number of very competent software packages for solving
such systems are available [2,4,6,8,17,23,26,25].
In this paper, we compare the performance of some commonly used software
packages for solving general sparse systems and show that during the past few
years, algorithmic improvements alone have shaved an order of magnitude oﬀ
the time required to factor general sparse matrices. Combined with a similar
increase in the performance to cost ratio due to hardware advances during this
period, current sparse solver technology makes it possible to solve those problems
quickly and easily that might have been considered impractically large until
recently. We demonstrate the consistently high level of performance achieved by
the Watson Sparse Matrix Package (WSMP) and show that it can factor some of
the largest sparse matrices available from real applications in a few seconds on
4-CPU workstation. The WSMP project’s original aim was to develop a scalable
V.N. Alexandrov et al. (Eds.): ICCS 2001, LNCS 2073, pp. 823–831, 2001.
c Springer-Verlag Berlin Heidelberg 2001
�

824

A. Gupta

parallel general sparse solver for a distributed-memory parallel computer like the
IBM SP. However, after completing the serial version of the solver, we realized
that we couldn’t ﬁnd enough large problems that would justify the use of an
SP with several nodes. Therefore, we tailored the parallel version to use a few
CPU’s in a shared-memory environment. It is one of the objectives of this paper
to make the user community aware of the robustness and speed of the current
sparse direct solver technology and encourage scientists and engineers to develop
bigger models with larger sparse systems so that the full potential of these solvers
can be utilized.

2

Comparison of Serial Performance of Some General
Sparse Solvers

In this section, we compare the performance of some of the most well-known
software packages for solving sparse systems of linear equations on a single CPU
of an IBM RS6000 S80. This is a 600 Mhz processor with a 64 KB level-1 cache
and a peak theoretical speed of of 1.2 Gigaﬂops. 2 GB of memory was available
to each solver. A more detailed comparison of the serial and the parallel versions
of these solvers can be found in [19]. Table 1 lists all the test matrices, their
dimensions, number of nonzeros, and application areas of their origin.
Table 2 lists the serial LU factorization time taken by UMFPACK [8], SuperLU [9], SPOOLES [4], SuperLUdist [22,23], MUMPS [1,2], and WSMP [15,17].
This table also lists the year in which the latest version of each of these packages
became available. The best factorization time for each matrix using any solver
released before year 2000 is shown in italics and the overall best factorization
time is shown in boldface.
The most striking observation in Table 2 is the range of times that diﬀerent
packages available before 1999 would take to factor the same matrix. It is not
uncommon to notice the fastest solver being faster than the slowest one by one
to two orders of magnitude. Additionally, none of them gave a consistent level
of performance. For example, UMFPACK is 13 times faster than SPOOLES on
e40r5000 but 14 times slower on ﬁdap011. MUMPS is clearly the fastest and
the most robust amongst the solvers available before 2000. However, the latest
solver WSMP appears to be about two and half times faster than MUMPS on
this machine on an average. WSMP also has the most consistent performance.
It has the smallest factorization time for all but two matrices and is the only
solver that does not fail on any of the test matrices.

3

Algorithmic Features of the Solvers

In this section, we list the algorithms and strategies that these solvers use for
the symbolic and numerical phases of the computation of the LU factors of a
general sparse matrix. We then brieﬂy discuss the role of these choices on the
performance of the solvers. The description of all these algorithms is beyond

Recent Progress in General Sparse Direct Solvers

825

Table 1. Test matrices with their order (N), number of nonzeros (NNZ), and the
application area of origin.
Matrix
af23560
av41092
bayer01
bbmat
comp2c
e40r0000
e40r5000
ecl32
epb3
ﬁdap011
ﬁdapm11
invextr1
lhr34c
lhr71c
mil053
mixtank
nasasrb
onetone1
onetone2
pre2
raefsky3
raefsky4
rma10
tib
twotone
venkat50
wang3
wang4

N
23560
41092
57735
38744
16783
17281
17281
51993
84617
16614
22294
30412
35152
70304
530238
29957
54870
36057
36057
659033
21200
19779
46835
18510
120750
62424
26064
26068

NNZ
484256
1683902
277774
1771722
578665
553956
553956
380415
463625
1091362
623554
1793881
764014
1528092
3715330
1995041
2677324
341088
227628
5959282
1488768
1316789
2374001
145149
1224224
1717792
177168
177196

Application
Fluid dynamics
F.E.M.
Chemistry
Fluid dynamics
Linear programming
Fluid dynamics
Fluid dynamics
Electrical eng.
Thermodynamics
F.E.M.
F.E.M.
Fluid dynamics
Chemical eng.
Chemical eng.
F.E.M.
Fluid dynamics
F.E.M.
Circuit simulation
Circuit simulation
Circuit simulation
Fluid dynamics
Fluid dynamics
Fluid dynamics
Circuit simulation
Circuit simulation
Fluid dynamics
Circuit simulation
Circuit simulation

the scope of this paper. The reader should be able to ﬁnd them in the citations
provided.
1. UMFPACK [8]
– Fill reducing ordering: Approximate minimum degree [7] on unsymmetric structure, combined with suitable numerical pivot search during LU
factorization.
– Task dependency graph: Directed acyclic graph.
– Numerical factorization: Unsymmetric-pattern multifrontal.
– Pivoting strategy: Threshold pivoting implemented by row-exchanges.
2. SuperLU [9]
– Fill reducing ordering: Multiple minimum degree (MMD) [13] on the
symmetric structure of AAT or A+AT , where A is the original coeﬃcient
matrix.

826

A. Gupta

Table 2. LU Factorization time on a single CPU (in seconds) for UMFPACK, SuperLU,
SPOOLES, SuperLUdist , MUMPS, and WSMP, respectively. The best pre-2000 time
is shown in italics and the overall best time is shown in boldface.
Year →
1994
1997
1998
1999
1999
2000
Matrix
UMFPACK SuperLU SPOOLES SuperLUdist MUMPS WSMP
af23560
45.5
31.9
10.5
14.7
8.93
6.19
av41092
186.
772.
Fail
Fail
30.6
8.47
bayer01
1.76
2.40
Fail
3.23
2.26
1.33
bbmat
682.
214.
97.7
Fail
113.
36.7
comp2c
120.
3403
287.
42.0
29.3
4.08
e40r5000
29.7
43.9
395.
2.08
1.18
1.55
ecl32
Fail
Fail
562.
Fail
145.
41.2
epb3
29.7
24.2
5.00
5.67
5.69
2.16
ﬁdap011
168.
39.9
12.2
16.9
18.7
6.38
ﬁdapm11
944.
88.9
15.1
Fail
25.3
11.2
lhr71c
6.80
12.5
Fail
23.0
11.7
3.05
nasasrb
81.8
102.
25.0
Fail
26.8
10.9
onetone1
12.2
184.
113.
10.7
10.0
7.25
onetone2
1.79
28.3
20.7
3.55
2.81
1.11
pre2
Fail
Fail
Fail
Fail
Fail
362.
raefsky3
39.0
146.
10.0
6.86
8.75
4.54
raefsky4
109.
1983
157.
28.1
27.5
7.78
rma10
15.7
Fail
10.7
5.78
9.62
3.76
tib
0.52
266.
1.75
1.47
0.62
0.31
twotone
30.0
Fail
724.
637.
124.
37.9
venkat50
16.2
Fail
11.6
8.11
19.4
4.40
wang3
106.
3226
62.7
36.9
32.3
13.4
wang4
97.3
318.
16.2
23.7
25.6
12.0

– Task dependency graph: Tree.
– Numerical factorization: Supernodal Crout.
– Pivoting strategy: Threshold pivoting implemented by row-exchanges.
3. SPOOLES [4]
– Fill reducing ordering: Generalized nested dissection/multisection [5] on
the symmetric structure of A + AT .
– Task dependency graph: Tree.
– Numerical factorization: Supernodal Crout.
– Pivoting strategy: Threshold rook pivoting that may perform both row
and column exchanges to control growth in both L and U .
4. SuperLUdist [22,23]
– Fill reducing ordering: Multiple minimum degree [13] on the symmetric
structure of A + AT .
– Task dependency graph: Directed acyclic graph.
– Numerical factorization: Supernodal right-looking.

Recent Progress in General Sparse Direct Solvers

827

– Pivoting strategy: No numerical pivoting during factorization. Rows are
preordered to maximize the magnitude of the product of the diagonal
entries [11].
5. MUMPS [1,2]
– Fill reducing ordering: Approximate minimum degree [7] on the symmetric structure of A + AT .
– Task dependency graph: Tree.
– Numerical factorization: Symmetric-pattern multifrontal.
– Pivoting strategy: Preordering rows to maximize the magnitude of the
product of the diagonal entries [11], followed by unsymmetric row exchanges within supernodes and symmetric row and column exchanges
between supernodes.
6. WSMP [15,17]
– Fill reducing ordering: Nested dissection [18,16] on the symmetric structure of A + AT .
– Task dependency graph: Minimal directed acyclic graph [15].
– Numerical factorization: Unsymmetric-pattern multifrontal.
– Pivoting strategy: Preordering rows to maximize the magnitude of the
product of the diagonal entries [11], followed by unsymmetric partial
pivoting within supernodes and symmetric pivoting between supernodes.
Rook pivoting (which attempts to contain growth in both L and U ) is
an option.
The multifrontal method [12,24] for the solving sparse systems of linear equations oﬀers a signiﬁcant performance advantage over more conventional factorization schemes by permitting eﬃcient utilization of parallelism and memory
hierarchy. Our detailed experiments in [19] show that all three multifrontal
solvers—UMFPACK, MUMPS, and WSMP—run at a much higher Megaﬂop
rate than their non-multifrontal counterparts. The original multifrontal algorithm proposed by Duﬀ and Reid [12] uses the symmetric pattern of A + AT to
generate an elimination tree to guide the numerical factorization, which works
on symmetric frontal matrices. This symmetric-pattern multifrontal algorithm
used in MUMPS can incur a substantial overhead for very unsymmetric matrices due to unnecessary dependencies in the elimination tree and extra zeros in the artiﬁcially symmetric frontal matrices. Davis and Duﬀ [8] and Hadﬁeld [21] introduced an unsymmetric-pattern multifrontal algorithm, which is
used in UMFPACK and overcomes the shortcomings of the symmetric-pattern
multifrontal algorithm. However, UMFPACK did not reveal the full potential
of the unsymmetric-pattern multifrontal algorithm because of the choice of a
ﬁll-reducing ordering (AMD), which has now been shown to be less eﬀective
than nested dissection [3]. Moreover, the merging of the ordering and symbolic
factorization within numerical factorization slowed down the latter and excluded
the possibility of using a better ordering while retaining the factorization code.
Other than WSMP, SPOOLES is the only solver that uses a graph-partioning
based ordering. However, it appears that the ﬁll-in resulting from rook pivoting,

828

A. Gupta

which involves both row and column exchanges in an attempt to limit pivot
growth in both L and U , overshadows a good initial ordering. Simple threshold
partial pivoting yields a suﬃciently accurate factorization for most matrices, including all our test cases. Therefore, rook pivoting is an option in WSMP, but
the default is a simple threshold pivoting.
WSMP achieves superior levels of performance by incorporating the best
ideas of the previous solvers into one package, while avoiding their pitfalls and
by introducing new techniques in both symbolic and numerical phases. The
analysis or preprocessing phase of WSMP uses a reduction to a block-triangular
form [10], a permutation of rows to maximize the magnitude of the product of
the diagonal entries [11,20], a multilevel nested dissection ordering [16] for ﬁll-in
reduction, and an improved version [15] of the classical unsymmetric symbolic
factorization algorithm [14] to determine the supernodal structure of factors
and the minimal directed acyclic task- and data-dependency graphs to guide
potentially multiple steps of numerical factorization with minimum overhead and
maximum parallelism. The unsymmetric-pattern multifrontal LU factorization
algorithm of WSMP [15] uses novel data-structures to eﬃciently handle any
amount of pivoting and diﬀerent pivot sequences without repeating the symbolic
phase for each factorization.

4

Performance on a Shared-Memory Parallel Workstation

Having empirically established in Section 2 that MUMPS and WSMP are the
fastest and the most robust among the currently available general sparse solvers,
in Table 3, we give 1- and 4-CPU factorization times of these two solvers on an
IBM RS6000 workstation with 375 Mhz Power 3 processors. These processors
have a peak theoretical speed of 1.5 Gigaﬂops. They share a 4 MB level-2 cache
and have a 64 KB level-1 cache each. 2 GB of memory was available to each single
CPU run and the 4-CPU runs of WSMP. MUMPS, when run on 4 processors,
had a total of 4 GB of memory available to it.
The relative performance of WSMP improves on the Power 3 as it is able to
extract a higher Megaﬂop rate from this machine. The most noteworthy observation from Table 3 (last column) is that out of the 25 test cases, only 3 require
more than 10 seconds on a mere workstation and all but one of the matrices
can be factored in under 13 seconds. The factorization times reported in Table 3
use the default options of WSMP. Many of the large test matrices, such as ﬁdapm11, mil053, mixtank, nasasrb, raefsky3, raefsky4, rma10, venkat50, wang3,
and wang4 have a symmetric structure and would need even less factorization
time if the user switches oﬀ the optional pre-permutation of rows to maximize the
diagonal magnitudes. This row permutation, which is on by default in WSMP,
destroys the structural symmetry and increases the ﬁll-in and operation count
of factorization. Some of the matrices, such as mil053, venkat50, wang3, wang4,
etc. do not require partial pivoting to yield an accurate factorization. Therefore,
if the user familiar with the characteristics of the matrices, switches oﬀ the piv-

Recent Progress in General Sparse Direct Solvers

829

Table 3. Number of factor nonzeros, operation count, and LU factorization (with
partial pivoting) times of MUMPS and WSMP on one and four 375 Mhz Power 3
processors.

Matrix
af23560
av41092
bayer01
bbmat
comp2c
e40r0000
ecl32
epb3
ﬁdap011
ﬁdapm11
invextr1
lhr34c
mil053
mixtank
nasasrb
onetone1
onetone2
pre2
raefsky3
raefsky4
rma10
twotone
venkat50
wang3
wang4

NNZLU
×106
8.34
14.1
2.82
46.0
7.05
1.72
42.9
6.90
12.5
14.0
30.3
5.58
75.9
38.5
24.2
4.72
2.26
358.
8.44
15.7
8.87
22.1
12.0
13.8
11.6

MUMPS
Ops
1
×109 CPU
2.56 7.75
8.42 19.8
.125 2.23
41.4 76.3
4.22 22.3
.172 1.70
64.6 82.6
1.17 5.10
7.01 14.5
9.67 18.2
35.6 53.6
.641 5.30
31.8 68.8
64.4 80.0
9.45 23.0
2.29 7.06
.510 2.33
Fail Fail
2.90 8.37
10.9 19.9
1.40 8.41
29.3 68.7
2.31 9.75
13.8 20.1
10.5 16.6

4
NNZLU
CPUs ×106
3.82
11.1
10.7
9.28
1.26
1.75
32.2
36.6
13.6
3.53
1.20
2.32
41.0
30.3
2.49
5.64
11.8
10.2
10.2
14.9
29.8
16.3
2.43
3.47
26.0
66.2
38.4
27.0
16.4
21.7
4.21
4.34
1.41
1.62
Fail
97.5
5.58
9.42
13.2
11.8
5.38
10.9
32.1
12.5
5.27
12.9
8.34
11.9
7.67
12.2

WSMP
Ops
1
×109 CPU
3.27 4.17
2.02 4.71
.040 1.34
20.1 23.7
1.09 2.72
.250 0.62
21.0 23.9
.451 1.94
3.20 4.22
5.21 6.77
6.90 10.8
.170 1.16
14.4 24.8
19.5 22.9
5.41 7.22
1.79 3.88
.206 0.90
133. 189.
2.57 3.27
4.11 5.09
1.48 2.59
10.4 18.3
1.75 3.01
5.91 6.92
6.09 7.06

4
CPUs
2.13
2.90
1.33
8.68
1.02
0.37
8.71
1.90
2.01
2.73
6.36
1.13
12.9
9.34
3.86
1.96
0.94
77.0
1.54
2.54
1.02
12.9
1.24
3.31
3.51

oting for these matrices and, in general, tailors the various options [17] to her
application, many of the test problems can be solved even faster.

5

Concluding Remarks

In this paper, we show that recent sparse solvers have improved the state of the
art of the direct solution of general sparse systems by almost an order of magnitude. Coupled with the good scalability of these solvers [3] and the availability
of relatively inexpensive high-performance parallel computers, it is now possible
to solve very large sparse systems in only a small fraction of the time that these
solutions would have required just a few years ago. Judging by the availability of
real test cases, it appears that the applications that require the solution of such

830

A. Gupta

systems have not kept pace with the improvements in the software and hardware available to solve these systems. We hope that the new sparse solvers will
encourage scientists and engineers to develop bigger models with larger sparse
systems so that the full potential of the new generation of parallel sparse general
solvers can be exploited.

References
1. Patrick R. Amestoy, Iain S. Duﬀ, Jacko Koster, and J. Y. L’Execellent. A fully
asynchronous multifrontal solver using distributed dynamic scheduling. Technical
Report RT/APO/99/2, ENSEEIHT-IRIT (Toulouse, France), 1999. To appear in
SIAM Journal on Matrix Analysis and Applications.
2. Patrick R. Amestoy, Iain S. Duﬀ, and J. Y. L’Execellent. Multifrontal parallel distributed symmetric and unsymmetric solvers. Computational Methods in Applied Mechanical Engineering, 184:501–520, 2000. Also available at
http://www.enseeiht.fr/apo/MUMPS/.
3. Patrick R. Amestoy, Iain S. Duﬀ, J. Y. L’Execellent, and Xiaoye S. Li. Analysis, tuning, and comparison of two general sparse solvers for distributed memory
computers. Technical Report RT/APO/00/2, ENSEEIHT-IRIT, Toulouse, France,
2000. Also available as Technical Report 45992 from Lawrence Berkeley National
Laboratory.
4. Cleve Ashcraft and Roger G. Grimes. SPOOLES: An object-oriented sparse matrix
library. In Proceedings of the Ninth SIAM Conference on Parallel Processing for
Scientiﬁc Computing, March 1999.
5. Cleve Ashcraft and Joseph W.-H. Liu. Robust ordering of sparse matrices using
multisection. Technical Report CS 96-01, Department of Computer Science, York
University, Ontario, Canada, 1996.
6. Michel Cosnard and Laura Grigori. Using postordering and static symbolic factorization for parallel sparse LU. In Proceedings of the International Parallel and
Distributed Processing Symposium (IPDPS), 2000.
7. Timothy A. Davis, Patrick Amestoy, and Iain S. Duﬀ. An approximate minimum
degree ordering algorithm. Technical Report TR-94-039, Computer and Information Sciences Department, University of Florida, Gainesville, FL, 1994.
8. Timothy A. Davis and Iain S. Duﬀ. An unsymmetric-pattern multifrontal method
for sparse LU factorization. SIAM Journal on Matrix Analysis and Applications,
18(1):140–158, January 1997.
9. James W. Demmel, Stanley C. Eisenstat, John R. Gilbert, Xiaoye S. Li, and Joseph
W.-H. Liu. A supernodal approach to sparse partial pivoting. SIAM Journal on
Matrix Analysis and Applications, 20(3):720–755, 1999.
10. Iain S. Duﬀ, A. M. Erisman, and John K. Reid. Direct Methods for Sparse Matrices.
Oxford University Press, Oxford, UK, 1990.
11. Iain S. Duﬀ and Jacko Koster. On algorithms for permuting large entries to the
diagonal of a sparse matrix. Technical Report RAL-TR-1999-030, Rutherford Appleton Laboratory, April 19, 1999.
12. Iain S. Duﬀ and John K. Reid. The multifrontal solution of unsymmetric sets of
linear equations. SIAM Journal on Scientiﬁc and Statistical Computing, 5(3):633–
641, 1984.
13. Alan George and Joseph W.-H. Liu. Computer Solution of Large Sparse Positive
Deﬁnite Systems. Prentice-Hall, NJ, 1981.

Recent Progress in General Sparse Direct Solvers

831

14. John R. Gilbert and Joseph W.-H. Liu. Elimination structures for unsymmetric
sparse LU factors. SIAM Journal on Matrix Analysis and Applications, 14(2):334–
352, 1993.
15. Anshul Gupta. A high-performance GEPP-based sparse solver. Technical report,
IBM T. J. Watson Research Center, Yorktown Heights, NY, 2001.
ftp://ftp.cs.umn.edu/users/kumar/anshul/parco-01.ps.
16. Anshul Gupta. Fast and eﬀective algorithms for graph partitioning and sparse
matrix ordering. IBM Journal of Research and Development, 41(1/2):171–183,
January/March, 1997.
17. Anshul Gupta. WSMP: Watson sparse matrix package (Part-II: direct solution of
general sparse systems). Technical Report RC 21888 (98472), IBM T. J. Watson
Research Center, Yorktown Heights, NY, November 20, 2000.
http://www.cs.umn.edu/˜agupta/wsmp.html.
18. Anshul Gupta. Graph partitioning based sparse matrix ordering algorithms for
ﬁnite-element and optimization problems. In Proceedings of the Second SIAM
Conference on Sparse Matrices, October 1996.
19. Anshul Gupta and Yanto Muliadi. An experimental comparison of some direct
sparse solver packages. Technical Report RC 21862 (98393), IBM T. J. Watson
Research Center, Yorktown Heights, NY, October 25, 2000.
ftp://ftp.cs.umn.edu/users/kumar/anshul/solver-compare.ps.
20. Anshul Gupta and Lexing Ying. Algorithms for ﬁnding maximum matchings in
bipartite graphs. Technical Report RC 21576 (97320), IBM T. J. Watson Research
Center, Yorktown Heights, NY, October 19, 1999.
21. Steven Michael Hadﬁeld. On the LU Factorization of Sequences of Identically
Structured Sparse Matrices within a Distributed Memory Environment. PhD thesis,
University of Florida, Gainsville, FL, 1994.
22. Xiaoye S. Li and James W. Demmel. Making sparse Gaussian elimination scalable
by static pivoting. In Supercomputing ’98 Proceedings, 1998.
23. Xiaoye S. Li and James W. Demmel. A scalable sparse direct solver using static
pivoting. In Proceedings of the Ninth SIAM Conference on Parallel Processing for
Scientiﬁc Computing, 1999.
24. Joseph W.-H. Liu. The multifrontal method for sparse matrix solution: Theory
and practice. SIAM Review, 34:82–109, 1992.
25. Olaf Schenk, Wolfgang Fichtner, and Klaus Gartner. Scalable parallel sparse LU
factorization with a dynamical supernode pivoting approach in semiconductor device simulation. Technical Report 2000/10, Integrated Systems Laboratory, Swiss
Federal Institute of Technology, Zurich, November 2000.
26. Kai Shen, Tao Yang, and Xiangmin Jiao. S+: Eﬃcient 2D sparse LU factorization
on parallel machines. SIAM Journal on Matrix Analysis and Applications, To be
published.

