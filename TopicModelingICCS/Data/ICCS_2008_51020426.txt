Bound for the L2 Norm of Random Matrix and
Succinct Matrix Approximation
Rong Liu, Nian Yan, Yong Shi, and Zhengxin Chen
Research Center on Fictitious Economy and Data Science,
Chinese Academy of Sciences, Beijing 100080, China
School of Mathematical Science, Graduate University of
Chinese Academy of Sciences, Beijing 100049, China
College of Information Science and Technology,
University of Nebraska at Omaha, Omaha NE 68182, USA
liu.rong@163.com, yshi@gucas.ac.cn, {nyan,zchen}@mail.unomaha.edu

Abstract. This work furnished a sharper bound of exponential form
for the L2 norm of an arbitrary shaped random matrix. Based on the
newly elaborated bound, a non-uniform sampling method was developed
to succinctly approximate a matrix with a sparse binary one and hereby
to relieve the computation loads in both time and storage. This method
is not only pass-eﬃcient but query-eﬃcient also since the whole process
can be completed in one pass over the input matrix and the sampling
and quantizing are naturally combined in a single step.
Keywords: Sampling, Matrix Approximation, Data Reduction, Random Matrix, L2 Norm.

1

Introduction

The computation for matrix by vector product is the most time and storage
consuming step in many numeric algorithms. Typical complexity of this computation is O(mn) in both time and storage for a matrix size of m × n. In real
world problems such as data mining, machine learning, computer vision, and
information retrieval, computations soon become infeasible as the size of matrices gets large since there are so many operations of this type involved. In data
mining and machine learning, low rank approximations provide compact representations of the data with limited loss of information and hereby relieve the
curse of dimensionality. A well known technique for low rank approximations is
the Singular Value Decomposition (SVD), also called Latent Semantic Indexing (LSI) in information retrieval [7] [4], which minimize the residual among
all approximations with the same rank. Traditional SVD computation requires
O(mn min{m, n}) time and O(mn) space. To speed up the SVD-based low rank
approximations and save storage, sampling methods are often used to sparsifying
the input matrices because the time and space complexity for a sparsiﬁed matrix
of N non-zero entries are reduced to O(N min{m, n}) and O(N ) respectively.
If a quantizing (rounding) approach is combined, the computation loads can be
relieved further.
M. Bubak et al. (Eds.): ICCS 2008, Part II, LNCS 5102, pp. 426–435, 2008.
c Springer-Verlag Berlin Heidelberg 2008

Matrix Approximation

427

Attracted by the computational beneﬁts from sparsifying matrices, numerous algorithms for massive data problems adopt sampling methods and diﬀerent error bounds are presented along with them. Frieze et al. (2004) [12] developed a fast Monte-Carlo algorithm which ﬁnd an approximation D in the
vector space spanned by some sampled rows of the input matrix D such that
P ( D − D F ≤ D F ) ≥ 1 − δ in two passes through D. Since many rows
are discarded in the random sampling process, the output matrix can be interpreted as a spare representation of the input one. In low rank approximations, Deshpande and Vempala (2006) [8] further presented an adaptive sampling
method to approximate the volume sampling with a probability at least 3/4 that
D − Dk F ≤ (1 + ) D − Dk F (D k consists of the ﬁrst k terms in the SVD of
D). This algorithm requires more passes over the input matrix. Passes through
a large matrix can be extremely costive if the matrix can not be fed in Random
Access Memory and must be stored in external storages such as Hard Disk. A
alternative entrywise sampling method which uses only one pass was given by
Achlioptas and McSherry (2001) [1]. Where the reconstruction error has a signiﬁcant better bound in L2 norm, a more eﬀective indicator for the trends in a
matrix than Frobenius norm, as P ( D− D 2 ≥ 5b s(m + n)) ≤ 1/(m+n) with
certain regularity. The theorem [2] which bounds the deviations of eigenvalues
of a random symmetric matrix from their medians is used to prove this bound.
In [1] they also includes detailed comparisons of their application to fast SVD
with the methods in [9] and [11].
There are also some achievements in non-sampling based methods. GLRAM
proposed by Ye (2005) [16] is a such one. Instead of the linear transformation
used in SVD, the GLRAM applies a bilinear mapping on the data because each
point is represented by a matrix other than a vector. The algorithm intends to
n
minimize i=1 Ai − LM i RT 2F with the same L and R for all Ai (the data
points) and hereby relieve computation loads. Their result is appealing but does
not admit a closed form solution in general, hence no error bound presented.
From another point of view, Bar-Yossef (2003) [5] pointed out based on information theory that at least O(mn) queries are necessary to produce a ( , δ)
approximation for a given matrix.
The review paper by Mannila (2002) [14] mentioned that methods of [1]
worked very well in low rank approximation. However, [14] also suggested the
necessity of further research for their boundaries. Besides, the following drawbacks are found in these methods under our own investigation. Firstly, the prior
(uniform sampling) in [1] is ﬂat which incorporates no knowledge about the data.
Secondly, a zero entry can even be round to non-zero. Thirdly, the rounding may
change the sign of an entry. Among them, the last two are extremely harmful to
data mining tasks. The main weakness of the method in [3] is that matrices are
required to be symmetric. In addition, this method still demands large amounts
of computation and storage.
As a continuing study for data reduction, the objective of this paper is to
develop an entrywise non-uniform sampling based data reduction (matrix approximation) method with lower reconstruction error.

428

2

R. Liu et al.

Method and Analysis

The data matrix under consideration in this paper is D ∈ Rm×n . Here, m is the
number of data points, and n the number of attributes or the dimension of the
points. The method sparsifying D is delivered and analyzed in the following.
2.1

Sampling Method and Its Bound

The sampling method we present to approximate a matrix is

d˜ij =

0
w.p. 1 − |dij |/c
c · sgn(dij ) w.p. |dij |/c

(1)

Here, w.p. means “with the probability of”. c = b · s, b = maxi,j |dij |, and s ≥ 1
is a controller for the sparsity. Actually, this method directly omit an entry or
round it to ±c in one query unlike the method in [1] where an extra quantizing
step is needed. Moreover, the matrix obtained can be succinctly represented by
a binary matrix corresponding to the sign of the entries, enabling addition in
place of multiplication, with a ﬁnal scaling of the result by c. One of the most
important capability indicators is the residual or the so called reconstruction
error. There diﬀerent metrics for the residual. Among them, matrix norm is a
widely used one. For the insensitivity of F norm to the matrix structure, L2
norm is a preferred metric in classiﬁcation, LSI, and other data mining tasks [1].
Therefore, the error of the sampling method are estimated in L2 norm as the
theorem follows.
Theorem 1. If D is the matrix obtained from the sampling process in Equation
4 2
2
(1), then P ( D − D 2 ≤ ) ≥ 1 − e−2[(1−t) /c −(2+1/t)(m+n)] , for all > 0 and
some ﬁxed t ∈ (0, 1).
The proof of Theorem 1. are given below along with the motivation of this
method.
2.2

Motivation and Analysis

Computing the L2 norm of an arbitrary matrix is a quite diﬃcult task. The
already known results to this problem are mainly involved in symmetric matrices. For an arbitrary matrix B ∈ Rm×n , the following Lemma can be used to
associate B 2 with the L2 norm of a symmetric matrix
A=

0 BT
B 0

∈ R(m+n)×(m+n)

Lemma 1. Let λ1 ≥ λ2 ≥ · · · ≥ λm+n be eigenvalues of A, then
A 2 = λ1 = −λm+n .

(2)
B

2

=

Matrix Approximation

429

Proof : Let x be the eigenvector belongs to the eigenvalue λ. We decompose
the vector into two part as x = [xT1 , xT2 ]T according to the structure of A. Then
Ax = λx can be written as
0 BT
B 0

x
x1
=λ 1
x2
x2

Let y 1 = x1 , y 2 = −x2 , this becomes
0 BT
B 0

y
y1
= −λ 1
y2
y2

i.e. Ay = −λy

so −λ is also an eigenvalue of A. This means that the eigenvalues of A appear in
a pairwise way. Consequently, the L2 norm of the symmetric matrix A is given
by A 2 = maxi |λi | = λ1 = −λm+n . To prove that the L2 norms of B and A
are identical, let us consider the eigenfunction of AT A
0 = |AT A − λI| =

B T B − λI
0
= |B T B − λI| |BB T − λI|
0
BB T − λI

According to the Sylvester’s theorem [6] B T B and BB T have the same nonezero eigenvalues. The equation above further reveals the truth that AT A, B T B,
and BB T are all share the same non-zero eigenvalues. This lead us to the claim
that B 2 = A 2 .
Now we focus on the L2 norm of a symmetric matrix since the Lemma given
above points out that one can always construct a symmetric matrix to make
it has the same L2 norm as the given matrix of arbitrary shape. Symmetric
matrices possess many appealing properties and some of them are very helpful
in estimating the L2 norm. The Rayleigh quotient deﬁned below is especially
important among them owing to providing the information about the eigenvalues
of a symmetric matrix.
R(x) =

xT Ax
,x = 0
xT x

The next Lemma is an improvement for the Theorem in [6] which associates the
L2 norm of a symmetric matrix with the Rayleigh quotient.
Lemma 2. If A is a symmetric matrix, then A 2 = maxx |R(x)|; furthermore, if the matrix takes the form given in Equation (2), then A 2 = λ1 =
maxx R(x).
Proof :

The best known property of Rayleigh quotient (cited in [6]) is
max R(x) = λ1 ,
x=0

min R(x) = λn
x=0

Here, λ1 ≥ λ2 ≥ · · · ≥ λn are eigenvalues of A. This property gives the ﬁrst part
of the Lemma straightforwardly since A 2 = max{|λ1 |, |λn |} = maxx |R(x)|.

430

R. Liu et al.

According to Lemma 1.,
Equation (2).

A

2

= λ1 = maxx R(x) if A takes the form in
n

The Rayleigh quotient can be deﬁned equivalently as R(x) = i,j Aij xi xj on a
unit vector x 2 = 1.
R(x) is a random variable, if A is a random symmetric matrix. Inequalities are
of great use in bounding quantities that might otherwise be hard to compute.
But directly using Rayleigh quotient combined with a random inequality to
bound the L2 norm of A will encounter great diﬃculty because P ( A 2 ≥
) ≥ P (|xT Ax| > ) (recall that A 2 = max x 2 =1 |xT Ax|). Fortunately, [10]
introduced a method that makes it suﬃcient to consider only vectors in a discrete
space. We strengthen their result ([10], [3]) here as a Lemma that
Lemma 3. (Reduction to Discrete Space) (Reduction to Discrete Space) Let
S = {z : z ∈ √tn Zn , z 2 ≤ 1} for some ﬁxed t ∈ (0, 1). If for every v i ∈ S we
have v Ti Av j ≤ , then uT Au ≤ /(1 − t)2 for every unit vector u. And the size
of S (denoted by |S|) is at most e(2+1/t)n .
Proof : Let y be a vector of length at most (1 − t) and C be the speciﬁc
hypercube from the grid √tn Zn in which y lies. Any two points inside C (include
y) are t close since the side length of C is √tn . As the maximum length of y
is assumed to be (1 − t), all vertices of C are within the distance of 1 from
the origin. Therefore, all vertices of C belong to set S. The convex combination
representability of y in the vertices of C can then be extended to S. Namely,
y = i ai v i (ai ≥ 0, i ai = 1). In this way,
y T Ay =

ai v ti A

ai aj v Ti Av j

aj v j =

i

i

≤

i

j

ai aj =
i

j

The inequality is obtained as we assumed that v Ti Av j ≤
Resultingly,
uT Au =

for every v i ∈ S.

y T Ay
≤
(1 − t)2
(1 − t)2

for any unit vector u.
Map every point z ∈ S in a 1-1 correspondence with a n-dimensional hypercube of side length √tn centered on itself as z → Cz (= {z ± w : w ∞ ≤ 2√t n }).
The length of w satisﬁes the following inequality.
n

w

2
2

n

wi2 ≤

=
i=1

i=1

t2
t2
=
4n
4

Then,
z+w

2

≤ z

2

+ w

2

≤ 1 + t/2

Matrix Approximation

431

The inequality above indicates the length of any vector in Cz is bounded by
1 + t/2. The union of these cubes is thus contained in a n-dimensional ball B of
radius (1 + t/2). According to the volume relationship between the union and
the ball,
|S| ·

n

t
√
n

Vol(Cz ) ≤ Vol(B) =

=
z∈Cz

π n/2
(1 + t/2)n
Γ (n/2 + 1)

Consequently,
|S| ≤

π n/2
Γ (n/2 + 1)

√
(1 + t/2) n
t

n

≤ e(2+1/t)n

That’s all for the proof.
In respect that the probability relationship between P ( A 2 ≤ ), P (xT Ax ≤
| x 2 = 1), and P (v Ti Av j ≥ (1 − t)2 | v i , v j ∈ S) is rather involving, we give
an additional Lemma to describe them.
Lemma 4. If A takes the form in Equation (2) and P (v Ti Av j ≥ (1 − t)2 | v i ,
v j ∈ S) ≤ ξ, then P ( A 2 ≤ ) ≥ 1 − ξe2(2+1/t)(m+n) .
Proof : The logic relationship of the propositions above is
∀v i ∈ S, v Ti Av j ≤ (1 − t)2 ⇒ ∀ x

2

= 1, xT Ax ≤ ⇒ A

As a result,
P( A

2

≤ ) ≥ P (xT Ax ≤

| x

2

= 1)

|S| |S|

≥ P(

v Ti Av j ≤ (1 − t)2 | v i , v j ∈ S)

i=1 j=1

Note that
|S| |S|

v Ti Av j ≤ (1 − t)2 | v i , v j ∈ S)

P(
i=1 j=1

|S| |S|

= 1 − P(

v Ti Av j ≥ (1 − t)2 | v i , v j ∈ S)

i=1 j=1
|S| |S|

≥1−

P (v Ti Av j ≥ (1 − t)2 | v i , v j ∈ S)
i=1 j=1
|S| |S|

≥1−

ξ = 1 − |S|2 ξ ≥ 1 − ξe2(2+1/t)(m+n)
i=1 j=1

2

≤

432

R. Liu et al.

The last inequality results from that Lemma 3. points out the maximum size of
T is e(2+1/t)(m+n) (note that the dimension of A is (m + n)). Then the lemma
follows.
Now we focus on furnishing P (v Ti Av j ≥ | v i , v j ∈ S) with an upper bound via
probability inequalities. Markov’s inequality obtained by tailing the expectation
of a random variable is a widely used one in real world applications. Hoeﬀding’s
inequality is similar in spirit to Markov’s inequality, but it is a sharper inequality
owing to that it makes use of a Taylor expansion of second order. Furthermore,
if there are large number of samples, the bound from Hoeﬃnd’s inequality is
smaller than the bound from Chebyshev’s inequality. Before applying Hoeﬀding’s
inequality to get a bound for the sampling method, we give a reﬁnement for it as:
Lemma 5. Let Y1 , · · · , Yn be independent observation such that E(Yi ) = 0 and
αi ≤ Yi ≤ βi . Then, for any > 0,
n

Yi ≥

P

≤ e−2

2

/

È

n
2
i=1 (βi −αi )

i=1

Proof :
is

The original Hoeﬀding’s inequality given in [15] on the same condition
n

n

Yi ≥

P
i=1
n

≤ e−t

2

et

(βi −αi )2 /8

, ∀t > 0

i=1

È

When t = 4 / i=1 (βi −αi )2 , the right hand side of the inequality above archives
2
n
2
its minmum e−2 / i=1 (βi −αi ) . Thus the claim.
A bound for P (v Ti Av j ≥ | v i , v j ∈ S) can then be obtain by applying the
results obtained above. We state it as one of our main results as the Theorem
follows.
Theorem 2. Let B ∈ Rm×n be a random matrix with independent entries of
zero mean. If αij ≤ bij ≤ βij , then P ( B 2 ≤ ) ≥ 1 − δ for ∀ > 0. Where
4 2
n
m
2
2
δ = e−2[(1−t) / i=1 j=1 (xi wj +ui yj ) (βij −αij ) −(2+1/t)(m+n)] for some ﬁxed t ∈
(0, 1), [xT , y T ]T = v i ∈ S, and [uT , wT ]T = v j ∈ S.

È È

Proof :
We construct a symmetric matrix A by B as Equation (2), then
B 2 = A 2 according to Lemma 1. and Lemma 2.. Lemma 4. reveals further that P ( B 2 ≤ ) = P ( A 2 ≤ ) ≥ 1 − ξe2(2+1/t)(m+n) if P (v Ti Av j ≥
(1 − t)2 | v i , v j ∈ S) ≤ ξ. For the matrix A,
v Ti Av j = xT y T

0 BT
B 0

u
w
n

m

= y T Bu + xT B T w =

(xi wj + ui yj )bij
i=1 j=1

Matrix Approximation

433

Since bij are independent, (xi wj + ui yj )bij are also independent and zero mean.
The requirements for Lemma 5. are thus satisﬁed and the range of random
variable (xi wj +ui yj )bij is from (xi wj +ui yj )αij to (xi wj +ui yj )βij or conversely.
As a result,
P (v Ti Av j ≥ (1 − t)2 | v i , v j ∈ S)
≤ e−2(1−t)

4 2

/

È È
n
i=1

m
2
2
j=1 (xi wj +ui yj ) (βij −αij )

=ξ

Then the theorem follows.
At this time we are prepared to prove Theorem 1.. But before going further, we
present the motivation of the sampling method.
A general approach for sparsifying a dense matrix is
d˜ij =

0 w.p. 1 − pij
γij w.p. pij

In many cases, the method that makes the expectation of the sampled matrix
equal to the input one is a reasonable strategy. We follow this too, that is dij =
E(d˜ij ) = 0 · (1 − pij ) + γij · pij = γij · pij . So our method is
d˜ij =

0
w.p. 1 − pij
dij /pij w.p. pij

Deﬁne B = D − D as the error or diﬀerence matrix of the sampling. Obviously,
E(bij ) = 0 and bij are independent due to that d˜ij are sampled independently.
The requirements for Theorem 2. are hereby satisﬁed. The range for the random
variable ˜bij is from dij to (1−1/pij )dij or the converse. According to Theorem 2.,
n
m
the components that contain the ranges are then become as i=1 j=1 (xi wj +
ui yj )2 (dij /pij )2 . Now the last free parameter need to be settled is the probability
pij which is essential for the success of many sampling based methods. From
the analysis above, we know that with the increase of pij the δ in the bound
of B 2 decreases but the density of the matrix D increases. In applications,
compromise between them should be take into consideration. Note that there
is no priori reason to omit each entry in the input matrix D with the same
probability, we set pij = |dij |/c (c is a constant positive real number) so that all
entries of B are contained in segments of equal length c and the bound for B 2
are simpliﬁed further. In addition, that the larger the absolute value of an entry
is, the more likely it retained is desired in many real world applcations. Intend
to guarantee that pij is a well deﬁned probability that contained in the segment
of [0, 1], the scale factor c is set to be s · b. As a result, sparsifying (omitting)
and quantizing (rounding) are combined naturally without any extra steps. The
sampling method given in Equation (1) comes into being as thus.
After explaining the motivation, Theorem 1. can be proofed straightly.

434

R. Liu et al.

Proof of Theorem 1.: For pij = |dij |/c, ni=1
n
m
c2 i=1 j=1 (xi wj + ui yj )2 . And we have
n

m
j=1

(xi wj +ui yj )2 (βij −αij )2 =

m

(xi wj + ui yj )2
i=1 j=1
n

n

m

i=1 j=1
n m

≤

xi ui
i=1
n

(x2i wj2 + u2i yj2 ) + (
i=1 j=1
n m

≤
=

m

(x2i wj2 + u2i yj2 ) + 2

=

i=1
n

m

n

yj2 )(

+
j=1

i=1
m

wj yj )2
j=1
m

n

x2i

u2i +
i=1

m

wj2
j=1

yj2
j=1

wj2 ) ≤ 1

u2i +
i=1

m

xi ui )2 + (

(x2i wj2 + u2i yj2 ) +
i=1 j=1
n
(
x2i
i=1

wj yj
j=1

j=1

The second inequality is obtained from Cauchy’s inequality, and the last is
from the fact that [xT , y T ]T = v i ∈ S, [uT , w T ]T = v j ∈ S. Accordingly,
4 2
2
δ ≤ e−2[(1−t) /c −(2+1/t)(m+n)] . Applying Theorem 2., P ( B 2 ≤ ) ≥ 1 −
4 2
2
e−2[(1−t) /c −(2+1/t)(m+n)] comes forth.
Our method is hereby much sharper in error bound with larger compression ratio
than that of [1]. The bound from [3] is restricted to symmetric data matrices,
moreover, their δ is two times worse than ours.

3

Discussion

In this paper, we furnish a bound of exponential form for the L2 norm of a
random matrix of arbitrary shape. The method we use diﬀers from the one
that bounds the deviations of eigenvalues of a random symmetric matrix from
their medians and thus gives a bound of polynomial form. Through the Rayleigh
quotient of the matrix, our work associates the L2 norm of an input matrix
with the inner product weighted by a constructed symmetric matrix in a reduced discrete space. As a result, an exponential bound for the input matrix
is obtained by applying Hoeﬃding’s inequality to the weighted inner product.
Moreover, the upper probability bound is further improved by two times owing
to utilizing the speciality of the constructed symmetric matrix. These ﬁndings
shed some new light on the understanding of the L2 norm of random matrices
and bound them more tightly. Motivated by the exponential bound we found,
a non-uniform sampling based sparse binary matrix approximation is presented
to accelerate computations and save storages for massive data matrices. In the
sampling method, omitting and rounding are naturally combined together without the need for extra steps because the sampling probabilities we chosen tailor
the segments containing the retained entries into equal length. Consequently, the

Matrix Approximation

435

implementation requires only one pass over the input matrix and the bound is
simpliﬁed further.
Acknowledgments. This work was partially supported by 973 Project of Chinese Ministry of Science and Technology (Grant No.2004CB720103), National
Natural Science Foundation of China (Grant No.70621001, 70531040, 70501030,
10601064, 70472074), National Natural Science Foundation of Beijing (Grant
No.9073020), and BHP Billiton Cooperation of Australia.

References
1. Achlioptas, D., McSherry, F.: Fast Computation of Low Rank Matrix Approximations. In: STOC 2001: Proceedings of the 32nd annual ACM symposium on Theory
of computing, pp. 611–618 (2001)
2. Alon, N., Krivelevich, M., Vu, V.H.: On the Concentration of Eigenvalues of Random Symmetric Matrices. Tech. Report 60, Microsoft Research (2000)
3. Arora, S., Hazan, E., Kale, S.: A Fast Random Sampling Algorithm for Sparsifying
Matrices. In: D´ıaz, J., Jansen, K., Rolim, J.D.P., Zwick, U. (eds.) APPROX 2006
and RANDOM 2006. LNCS, vol. 4110, pp. 272–279. Springer, Heidelberg (2006)
4. Berry, M., Dumais, S., O’Brie, G.: Using Linear Algebra for Intelligent Information
Retrieval. SIAM Review 37, 573–595 (1995)
5. Bar-Yossef, Z.: Sampling Lower Bounds via Information Theory. In: Proceedings
of the 35th annual ACM symposium on Theory of computing, pp. 335–344 (2003)
6. Chen, Y.P.: Matrix Theories, 2nd edn. NorthWestern Polytechnical University
Press, Xian (1998)
7. Deerwester, S., Dumais, S., Furnas, G., Landauer, T., Harshman, R.: Indexing by
Latent Semantic Anlysis. J. American Society for information science 41, 391–407
(1990)
8. Deshpande, A., Vempala, S.: Adaptive Sampling and Fast Low-Rank Matrix Approximation. In: D´ıaz, J., Jansen, K., Rolim, J.D.P., Zwick, U. (eds.) APPROX 2006 and
RANDOM 2006. LNCS, vol. 4110, pp. 292–303. Springer, Heidelberg (2006)
9. Drineas, P., Frieze, A., Kannan, R., Vempala, S., Vinay, V.: Clustering in large
graphs and matrices. In: Proceedings of the 10th Annual ACM-SIAM Symposium
on Discrete Algorithms, pp. 291–299 (1999)
10. Feige, U., Ofek, E.: Spectral Techniques Applied to Sparse random graphs. Random
Structures and Algorithms 27(2), 251–275 (2005)
11. Frieze, A., Kannan, R., Vempala, S.: Fast Monte-Carlo algorithms for ﬁnding lowrank approximations. In: 39th Annual Symposium on Foundations of Computer
Science, pp. 370–378 (1998)
12. Frieze, A., Kannan, R., Vempala, S.: Fast Monte-Carlo algorithms for ﬁnding lowrank approximations. Journal of the ACM 51(6), 1025–1041 (2004)
13. Golub, G.H., Van Loan, C.F.: Matrix Computations, 3rd edn. Johns Hopkins University Press, Maryland (1996)
14. Mannila, H.: Local and Global Methods in Data Mining: Basic Techniques and
Open Problems. In: Proceedings of the 29th International Colloquium on Automata, Languages and Programming, pp. 57–68 (2002)
15. Wasserman, L.: All of Statistics: A Concise Course in Statistical Inference.
Springer, New York (2004)
16. Ye, J.P.: Generalized Low Rank Matrix Approximations of Matrices. Machine
Learning 61(1-3), 167–191 (2005)

