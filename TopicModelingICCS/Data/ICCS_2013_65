Available online at www.sciencedirect.com

Procedia Computer Science 18 (2013) 2549 – 2552

International Conference on Computational Science, ICCS 2013

Real-Time Sound Source Localization on Graphics Processing
Units
Jose A. Bellocha,1 , Alberto Gonzaleza , Antonio M. Vidalb , Maximo Cobosc
a Instituto

de las Telecomunicaciones y Aplicaciones Multimedia, Universitat Polit´ecnica de Val´encia, 46022 Valencia, Spain
de Sistemas Informaticos y Computacion, Universitat Polit´ecnica de Val´encia, 46022 Valencia, Spain
c Computer Science Department, Universitat de Val´
encia, 46100 Valencia, Spain

b Departamento

Abstract
Sound source localization is an important topic in microphone array signal processing applications, such as camera steering
systems, human-machine interaction or surveillance systems. The Steered Response Power with Phase Transform (SRPPHAT) algorithm is one of the most well-known approaches for sound source localization due to its good performance in noisy
and reverberant environments. The algorithm analyzes the sound power captured by a microphone array on a grid of spatial
points in a given room. While localization accuracy can be improved by using a high resolution spatial grid and a high number
of microphones, performing the localization task in these circumstances requires a high computational demand. Graphics
Processing Units (GPUs) are highly parallel programmable coprocessors that provide massive computation when the needed
operations are properly parallelized. This paper analyzes the performance of a real-time sound source localization system
whose processing is totally carried out on a GPU. The proposed implementation yields maximum parallelism by adapting the
required computations to diﬀerent GPU architectures (Tesla, Fermi and Kepler).
Keywords: GPU; Sound Source Localization; Audio Processing; Microphone Arrays

1. Introduction
Microphone arrays are commonly employed in many signal processing tasks, such as speech enhancement,
acoustic echo cancellation or sound source separation [1]. The localization of broadband sound sources under
high noise and reverberation is another challenging task in multichannel signal processing, being a very active
research topic for its applications in human-computer interfaces, teleconferencing or robot artiﬁcial audition.
The Steered Response Power - Phase Transform (SRP-PHAT) algorithm is a direct approach that has been
shown to be very robust in adverse acoustic environments [2]. The algorithm is usually interpreted as a beamformingbased approach that searches for the candidate position that maximizes the output of a steered delay-and-sum
beamformer. However, despite the good performance of the SRP-PHAT, the computational power required by
the algorithm is highly dependent on the number of microphones and the size of the search space, thus, real-time
localization using a ﬁne spatial grid and/or a high number of microphones is a real issue.
Email addresses: jobelrod@iteam.upv.es (Jose A. Belloch), agonzal@dcom.upv.es (Alberto Gonzalez), avidal@dsic.upv.es
(Antonio M. Vidal), maximo.cobos@uv.es (Maximo Cobos)
1 Corresponding author

1877-0509 © 2013 The Authors. Published by Elsevier B.V. Open access under CC BY-NC-ND license.
Selection and peer review under responsibility of the organizers of the 2013 International Conference on Computational Science
doi:10.1016/j.procs.2013.05.438

2550

Jose A. Belloch et al. / Procedia Computer Science 18 (2013) 2549 – 2552

In this paper, we discuss the GPU implementation of the SRP-PHAT algorithm for real-time sound source
localization considering the simulation of diﬀerent acoustic environments. Relevant parameters aﬀecting the
computational cost of the algorithm (number of microphones and spatial resolution) are analyzed, showing their
inﬂuence on the localization accuracy under diﬀerent acoustic environments. The most valuable feature of the
proposed implementation is its scalability and portability, since it can be adapted to any GPU device, even a
notebook GPU. Thus, the experiments and results are presented for diﬀerent GPU architectures (Tesla, Fermi and
Kepler).
2. Sound Source Localization: SPR-PHAT Algorithm
Consider the output from microphone l, ml (t), in an M microphone system. The SRP at the spatial point
x = [x, y, z] for a time frame n of length T is deﬁned as
(n+1)T

Pn (x) ≡

nT

2

M

wl ml (t − τ(x, l)) dt,

(1)

l=1

where wl is a weight and τ(x, l) is the direct time of travel from location x to microphone l. DiBiase [3] showed
that the SRP can be computed by summing the Generalized Cross Correlations (GCCs) for all possible pairs of
the set of microphones. The GCC for a microphone pair (k, l) is computed as
Rmk ml (τ) =

∞
−∞

Φkl (ω)Mk (ω)Ml∗ (ω)e jωτ dω,

(2)

where τ is the time lag, ∗ denotes complex conjugation, Ml (ω) is the Fourier transform of the microphone signal
ml (t), and Φkl (ω) is a combined weighting function in the frequency domain. The phase transform (PHAT) [4] has
been demonstrated to be a very eﬀective GCC weighting for time delay estimation in reverberant environments:
Φkl (ω) ≡

1
.
|Mk (ω)Ml∗ (ω)|

(3)

Taking into account the symmetries involved in the computation of Eq.(1) and removing some ﬁxed energy
terms [3], the part of Pn (x) that changes with x is isolated as
M

M

Pn (x) =

Rmk ml (τkl (x)) ,

(4)

k=1 l=k+1

where τkl (x) is the inter-microphone time-delay function (IMTDF). This function represents the theoretical direct
path delay for the microphone pair (k, l) resulting from a point source located at x. The IMTDF is mathematically
expressed as [4]
τkl (x) =

x − xk − x − x l
,
c

(5)

where c is the speed of sound, and xk and xl are the microphone locations.
The SRP-PHAT algorithm consists in evaluating the functional Pn (x) on a ﬁne grid G with the aim of ﬁnding
the point-source location x s that provides the maximum value:
x s = arg max Pn (x).
x∈G

(6)

Let L be the DFT length of a frame and N p = M(M − 1)/2 the number of microphone pairs. The order of the
computational cost of SRP-PHAT is given by [5]:
SRP-PHATcost ≈ [6.125N p2 + 3.75N p ]L log2 L + 15LQ(1.5N p − 1) + (45N p2 − 30N p )ν ,

(7)

where ν is the average number of functional evaluations (usually the grid size) required to ﬁnd the maximum of
the SRP space.

Jose A. Belloch et al. / Procedia Computer Science 18 (2013) 2549 – 2552

2551

2.1. GPU Implementation
GPUs are well-known for their potential in highly parallel data processing. A GPU is composed by multiple
Stream Multiprocessor (SM), where each SM consists of eight pipelined cores if CUDA capability is 1.2 or 1.3
(Tesla architecture), or 32 pipelined cores if it is 2.0 (Fermi architecture), or even 192 pipelined cores if it is
3.0 or 3.5 (Kepler architecture) [6]. In the CUDA model, the programmer deﬁnes the kernel function where the
code to be executed on the GPU is written. The GPU-based SRP-PHAT implementation is based on a ﬁne-grain
parallelism where each launched CUDA thread carries out few and easy operations. The steps followed in the
implementation are:
1. Deﬁne a spatial grid G with a given spatial resolution r. The theoretical delays from each point of the grid
G to each microphone pair are pre-computed using Eq.(5). A GPU kernel conﬁgured by as many threads as
points of the grid per number of pairs of microphones is launched. Each thread is devoted to compute the
delay of a grid point.
2. For each analysis frame, the GCC of each microphone pair is computed as expressed in Eq.(2). This task
is carried out by another GPU kernel where each thread computes a value of GCC. The number of the
launched threads depends on the frame size and the number of microphones.
3. For each position of the grid x ∈ G, the contribution of the diﬀerent cross-correlations are accumulated
(using delays pre-computed in 1), as in Eq.(4). To this end, a third GPU kernel is launched with the same
number of threads as in the step 1 to compute the cross-correlations that will be later accumulated by a
fourth GPU kernel composed of as many threads as points of the grid.
4. Finally, the position with the maximum score is selected. This task is based on the parallel sum reduction
algorithm described by Mark Harris [6].
3. Experiments and Results
To analyze both the computing and localization performance of the above GPU implementation, a set of
acoustic simulations using the image-source method [7] have been considered. A shoe-box-shaped room with
dimensions 4×6×3 meters and wall reﬂection factor ρ [8] was simulated with diﬀerent microphone conﬁgurations
(M = {6, 10, 20}). Moreover, diﬀerent reﬂection factors (ρ = {0, 0.5, 0.9}) were used to take into account diﬀerent
reverberation degrees.
The audio card used in the real-time prototype uses an ASIO (Audio Stream Input/Output) driver to communicate with the CPU and provides 2048 samples per microphone every 46.43 ms (sample frequency of 44.1
kHz). This time is called for us tbuﬀ . The time employed for the computation of the M frames is denoted by
tproc . The localization system works in real-time as long as tproc < tbuﬀ . The simulations were carried out in three
diﬀerent NVIDIA GPUs, GTS-360M (Tesla architecture), GTX-590 (Fermi architecture), and GTX-690 (Kepler
architecture). Detailed characteristics of each kind of architectures are shown in [6].
Figure 1 (a,b,c) shows the computational performances of the real-time localization system. It presents the
frame computation time tproc achieved by the three diﬀerent GPU accelerators with diﬀerent algorithm conﬁgurations. Speciﬁcally, diﬀerent combinations of spatial grid resolutions (r = {0.05, 0.01, 0.005}) and number of
microphones (M = {6, 10, 20}) have been used. Note that, according to Eq.(7), these parameters are the ones
that mostly aﬀect the computational cost of the algorithm, see Figure 1 (e). As it is illustrated, it is necessary to
have a GTX-690 if we want to execute a real-time system with M=20 microphones and r=0.05. Performances of
GTX-590 and GTS-360M are limited to use M=10 and M=6, respectively. Also, the GPU device with tesla architecture was not able to manage a system composed of M=20 microphones because of memory storage problem
[6]. Observing Figure 1 (d), it becomes clear that the performance diﬀerence between GPUs of Tesla and Fermi
architectures is more meaningful than the performance diﬀerence between Fermi and Kepler architecture. This is
related to the two memory cache levels L1 and L2 that own these two architectures.
Regarding the analyzed diverse acoustic environments, Figure 1 (f) shows the localization performance achieved
by diﬀerent microphone conﬁgurations. Simulated acoustic signals of length 10s corresponding to a male speaker
were used to study the Absolute Mean Error by considering the parameters described in Section 3. Note that, as
ρ gets higher (more reﬂections) the localization error also gets bigger. However, the use of a higher number of
microphones helps a lot in reducing this error, justifying the use of GPU architectures and massive computation
for acoustic localization tasks in real environments.

Jose A. Belloch et al. / Procedia Computer Science 18 (2013) 2549 – 2552
Frame Computation Time (tproc) for Tesla

300
250

100

tbuff

M=6
M = 10
M = 20

30

35

20

tbuff

M=6
M = 10
M = 20

40

tbuff
40

150

Frame Computation Time (tproc) for Kepler

50

50

200

50

Frame Computation Time (tproc) for Fermi

45

Time (ms)

Time (ms)

60

M=6
M = 10

Time (ms)

2552

30
25
20
15
10

10

5

r (m)

r (m)

(a)

(b)

Comparison of Frame Computation Times for M = 10

Time (ms)

200
150
100

50
0
0.005

10

0.015 0.02

0.025 0.03 0.035 0.04 0.045 0.05

(c)
Localization Accuracy
1.5

12

Tesla
Fermi
Kepler

250

0.01

r (m)

Computational Cost

Numer of Operations

300

0
0.005

0
0.005 0.01 0.015 0.02 0.025 0.03 0.035 0.04 0.045 0.05

0.01 0.015 0.02 0.025 0.03 0.035 0.04 0.045 0.05

M=6
M = 10
M = 20

11

10

10

10

Mean Absolute Error

0
0.005

M=6
M = 10
M = 20
1.0

0.5

9

tbuff

10

0.01 0.015 0.02 0.025 0.03 0.035 0.04 0.045 0.05

0.005 0.01 0.015 0.02 0.025 0.03 0.035 0.04 0.045 0.05

r (m)

r (m)

(d)

(e)

0

0

0.1

0.2

0.3

0.4

0.5

0.6

0.7

0.8

0.9

ρ

(f)

Fig. 1. Frame computation times for each computing case in the diﬀerent GPU architectures. (a) Tesla. (b) Fermi. (c) Kepler. (d) Comparison
of frame computation times for M=10. (e) Computational cost for each case. (f) Localization accuracy for diﬀerent number of microphones
M and diﬀerent wall reﬂection factor ρ, using a resolution of r = 0.005 m.

4. Conclusion
The new emerging GPU architectures have allowed us to face up diﬀerent computational problems in sound
source localization that had not been approached before. In this article, we proposed a GPU implementation of
the well-known SRP-PHAT algorithm and we assessed its performance on a real-time localization system. To this
end, diverse acoustic environments and three diﬀerent GPU devices were considered. The results show that using a
high number of microphones has a direct impact on localization performance in highly reverberant environments.
In fact, a system composed of 20 microphones and a GPU with Kepler architecture was shown to be capable of
locating in real-time a sound source with an accuracy close to 5 cm, even in adverse acoustic environments.
Acknowledgements
This work has been partially funded by Spanish Ministerio de Economia y Competitividad TEC2009-13741
and TEC2012-38142-C04-01, Generalitat Valenciana PROMETEO 2009/2013 and GV/2010/027, and Universitat
Polit`ecnica de Val`encia through Programas PAID-05-11, PAID-06-2011 and PAID-05-2012.
References
[1] M. Brandstein, D. Ward, Microphone arrays, Springer, 2001.
[2] J. H. DiBiase, H. F. Silverman, M. S. Brandstein, Robust localization in reverberant rooms, in: M. S. Brandstein, D. Ward (Eds.),
Microphone Arrays: Signal Processing Techniques and Applications, Springer-Verlag, Berlin, Germany, 2001, Ch. 8, pp. 157–180.
[3] J. H. DiBiase, A high accuracy, low-latency technique for talker localization in reverberant environments using microphone arrays, Ph.D.
thesis, Brown University, Providence, RI (May 2000).
[4] M. Cobos, A. Marti, J. J. Lopez, A modiﬁed SRP-PHAT functional for robust real-time sound source localization with scalable spatial
sampling, IEEE Signal Processing Letters 18 (1) (2011) 71–74.
[5] H. F. Silverman, Y. Yu, J. M. Sachar, W. R. Patterson III, Performance of real-time source-location estimators for a large-aperture microphone array, IEEE Transactions on Speech and Audio Processing 13 (4) (2005) 593–606.
[6] NVIDIA CUDA Developer Zone, online at: http://developer.download.nvidia.com/.
[7] J. B. Allen, D. A. Berkley, Image method for eﬃciently simulating small-room acoustics, J. Acoust. Soc. Am. 65 (4) (1979) 943–950.
[8] H. Kuttruﬀ, Room acoustics, Taylor & Francis, Abingdon, Oxford, UK, 2000, 368 pages.

