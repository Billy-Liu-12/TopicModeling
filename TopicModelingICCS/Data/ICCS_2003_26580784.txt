Neural Networks for Event Detection from Time
Series: A BP Algorithm Approach
Dayong Gao1 , Y. Kinouchi 1, and K. Ito 2
1

2

Faculty of Engineering
Faculty of the Integrated Arts and Sciences
The University of Tokushima, Japan

Abstract. In this paper, a relatively new event detection method using
neural networks is developed for financial time series. Such method can
capture homeostatic dynamics of the system under the influence of exogenous event. The results show that financial time series include both
predictable deterministic and unpredictable random components. Neural
networks can identify the properties of homeostatic dynamics and model
the dynamic relation between endogenous and exogenous variables in financial time series input-output system. We also investigate the impact
of the number of model inputs and the number of hidden layer neurons
on forecasting. . . .

1

Introduction

Neural networks have been applied in time series to improve multivariate prediction ability [1–3]. Neural networks map an input space (present and past values
of the time series) onto an output space (future value). Future values of variables
often depend on their past values, the past values of other correlated variables,
and other uncertain factors. For example, the future price of a stock depends on
political and international events as well as various economic indictors.
Most studies on time series analysis using neural networks address the use of
neural networks as a forecasting tool, but limited research focuses on the application of neural networks as a data mining technique to extract event patterns
from time series. These forecasts rely on historical trends or past experiences
to predict the future. The measurement of past trends may be interrupted by
changes in policy or data availability. When the past cannot be used to predict
the future due to major unexpected shifts in economic, political, or social conditions, the future can be more uncertain due to the occurrence of major events.
Evaluating the impact of these events (exogenous variables) on stock markets are
interesting to economists, because the markets are affected by less measurable
but important forces in economic analysis and financial prediction.
In this paper, a systematic approach for time series using neural networks
is developed to generate an event sequence and to model the dynamic relation
between stock market index (endogenous variables) and macroeconomic environment (exogenous variables). Because time series related to the input-output
P.M.A. Sloot et al. (Eds.): ICCS 2003, LNCS 2658, pp. 784−793, 2003.
 Springer-Verlag Berlin Heidelberg 2003

Neural Networks for Event Detection from Time Series
bias

bias

785

bias

i1

X1
X2

i2

o

X3

W1
W2
W3

Σ

n

y=tanh(Σwixi)
1

Wn

Xn

ik

m1
Input layer

m2

Hidden layer

Inset

Output layer

Fig. 1. Neural Network Configuration and Nonlinear Neuron Model (Inset). The
input(x) − output(y) relation of nonlinear neurons is given by a conventional sigmoid
function such as y = tanh(x). The input(x) is expressed by a form Σ(wi xi ), where wi
is a weight for an input component xi .

system of financial markets may include non-linearity, back propagation (BP)
neural networks are used to clarify basic predictive properties. This model can
acquire automatically the dynamics included in the data by use of an error back
propagation (BP) algorithm [4].

2
2.1

Methodological Consideration
Identification by Neural Networks

Identification by neural networks for time series data x1 , x2 , . . ., xk , . . ., xN
requires predicting or estimating a datum xk based on preceding observed n
data xk−n , . . ., xk−2 , xk−1 , i. e., prediction by a nth order MA model, where
x1 , x2 , . . ., xk , . . ., xN are discrete time sequences. In order to implement the
prediction, a multiplayer neural network model as shown in F ig. 1 is used. The
network has one input layer i1 = xk−n , !D, in = xk−1 and one output layer xk .
Each weight wi is adjusted to coincide with output ok and target xk by a
gradient method. For error function F defined by
ε2k ,

F =

εk = ok − xk

(1)

k∈St

where St is a set of training patterns, a correction δwi of wi to minimize error
function is given by
δwi = −η

∂F
∂wi

(2)

where η is the learning rate parameter, a small positive constant. To calculate
δwi for neurons in hidden layers, an error back propagation (BP) algorithm is
used to train the network [4]. After training, the network is tested by the use of
the rest patterns to verify whether the network acquires source dynamics, i. e.,
generalization. If the network output for each test pattern is almost close to

786

D. Gao, Y. Kinouchi, and K. Ito

Table 1. Data Acquisition and Normalized Results. N is the maximum number of
trading days for TOPIX during 1995–2000; m is the normalized factor. Min, Mean,
Max and MaxDev are the minimum, mean, maximum and maximum deviation of
acquired and normalized data respectively.
Data Acquisition
N
Min
Mean
Max
1481 980.00 1415.89 1754.78
Normalized Results
m
Min
Mean
Max
500 −0.87178 −4.2E-06 0.67778

MaxDev
435.89
MaxDev
0.87178

the respective target, the network is considered to have acquired the underlying
dynamics from the training patterns.
2.2

Data Acquisition

The most popular treatment of input data is to feed neural networks with either
the data at each observation, or the data from several successive observations.
In this paper, we consider daily data of TOPIX (Tokyo Stock Price Index) from
Japanese stock market in the period 1995–2000.
Succeeding data of TOPIX, denoted by tk , are converted to the deviation
from their mean value and scaled by a factor m to normalize their values in the
range of -1 to 1 by equation:
tk − t
,
k=1∼N
(3)
m
where k is the number of trading days, corresponding to 6 years from 1995 to
2000 and N is the maximum number. t is the average of tk .
In Table 1, historical data of TOPIX are divided into training phases and
test phases, with each phase containing half of the collected data. The data of
the earlier period (1995-1997) are used for training; the data of the latter period
(1998-2000) are used for test.
Real world data contain varying degree of noise. To demonstrate our method
and to compare with real data from stock markets, two types of the data except
TOPIX are generated as follows: 1) random data with uniform distribution between -1 to 1, and 2) the data from a sinusoidal wave, i. e., deterministic data,
with unit amplitude at an interval of T/60.1 (T: period). These two types of
generated data represent a sequence of true random numbers where no model
has a chance and simple noise-free data, respectively.
xk =

2.3

Model Identification

Various measurement methods such as approximate entropy, auto-correlation,
average mutual information, and number of false nearest neighbors can be used

Neural Networks for Event Detection from Time Series

787

to evaluate time series data [5]. These methods can quantify data regularity
and randomness if there are large amounts of the data. However, stock market
data are relatively limited and this study uses even more limited data containing
various market environments at fixed periods. If the data periods are too long,
market environments change much; financial patterns and the neural network
models also change much.
The following “R-method” of quantifying data regularity and randomness
can be applied to a relatively small amounts of the data. This R-method can
also be used to measure multi-valued property of the data and determine neural
network configuration. To calculate values of R, let x denote the input vector
(i1 , i2 , !D, iN −n ) and y denote the output o. The data produce input vectors x1 ,
x2 ,!D, xN −n and corresponding outputs y1 , y2 , !D, yN −n . If xi and xj are similar,
then yi and yj are also similar for deterministic data, but yi and yj may be quite
different for random data and multi-valued function. The change of |yj − yi | for
yi corresponds to all xj in the neighborhood of each xi as a measurement of data
randomness. Values of R are defined by
1

R=

|∆i |
i

N −n

f (zji )

(4)

i=1 j∈∆i

where
√
∆i = j||xi − xj |/ nxrms ≤ β, j = i ,
√
= j| min |xi − xj |/ nxrms , j = i ,

if xj exists
otherwise

(5)

Here, |∆i | represents the number of xj in the neighborhood ∆i . The xrms is
a root mean square value for the data
√ xk (Eq.1), and the yrms is a root mean
square value for the data yi . The nxrms is the mean length of the vector xi
taken in the n dimensional space. The zji is given by
√
zji = |yj − yi |)/(yrms / nxrms

(6)

The function f (zji ) is a logistic function as following:
f (zji ) = 0,

√
if |xj − xi |)/ nxrms ≤ β and |yj − yi |)/yrms ≤ β

= 1 − (1 + αzji )exp(−αzji ),

otherwise

(7)

where α and β are positive constants. If the data x1 , x2 ,!D, xN are obtained
by sampling a deterministic single-valued time function, using Eq.7, f (zji ) = 0,
because yi ≈ yj for all xj existing in the neighborhood ∆i of any xi . Values of
R are zero when the data number N is relatively large. If the data come from
a random function, f (zji ) = 1 because yj is independent of yi , therefore R=1
for the relatively large data numbers. R for any data will have a value between
zero and one, indicating the degree of randomness in the data. High values of R

788

D. Gao, Y. Kinouchi, and K. Ito
0.9
0.8
Random

Degree of Randomness

0.7

TOPIX

0.6

Sinusoidal

0.5
0.4
0.3
0.2
0.1
0
2

3

4

5

6

7

8

Input Number n

Fig. 2. Optimum Neural Networks. Values of R for TOPIX start low at n=2, but
increase at n=3, and then start to decrease from n=4. This process characterizes the
deterministic nature of the data.

indicate high randomness and high statistical dependence between the data; low
values of R indicate more recognizable patterns and high statistical independence
of the data.

3
3.1

Empirical Results
Model Development

Network size is determined by the input number n, the number of hidden layers
(1 or 2) and the number of neurons (m1 and m2 ) in the hidden layers. If different
network sizes have similar values for the error function, the smallest network size
is optimum [6]. In this paper, we use R-method (see Sect. 2.3) to characterize
the deterministic and random properties of the data in relation to the input data
so as to determine optimum neural networks.
Samples from TOPIX are used to measure the degree of randomness. Values
of R are determined for the input sizes n = 2, 3, 4, 5, 6, 7, 8. Each sample is
grouped into an overlapping cluster with 3 years of the data, enough to characterize the dynamic properties of the system. 6 month graduations are used to
characterize moving group samples MGS(1), MGS(2), !D, MGS(7) such as
M GSk : xk , xk+1 , . . ., xk+n , k = 1 ∼ 7

(8)

where n corresponds to 3 years of the data.
After calculating values of R for both TOPIX and two types of generated
data (see Sect. 2.2), mean values of R corresponding to moving group samples
for TOPIX are used to evaluate deterministic and random properties of the data.
Values of R for two other types of generated data are calculated as comparison.
Here, the data number for these two types of the data is 750 because training
data number for the stock market indexes is about 750. Values of R for random
data are mean values of five generated data sets. For values of R, we choose &A
= 0.4 and &B = 0.01.

Neural Networks for Event Detection from Time Series

789

Table 2. Forecasting Performance for N N 341 (iteration=3000). &G: learning rate parameter; RMS: root mean square error.
Phase Data Period &G
RMS
Training 1995−1997 0.005 0.001926
Test
1998−2000 0.005 0.036991

F ig. 2 shows that the degree of randomness for TOPIX lays between values
of R for random data and sinusoidal data. Therefore, the data from TOPIX
might have both deterministic and random properties.
Values of R can also be used to determine the minimum input number of
dynamic variables needed to train the networks. At input number n=3, there
is the maximum statistical independence between distance observations of data
series, so input number n=3 maximize the network ability to learn from the
data and should be the optimum number of nodes in the input layer. To verify
the optimum input number, RMS error corresponding to various input number
of nodes is calculated. It is found that neural networks corresponding to input
number n=3 give minimum RMS error.
To finally determine the optimum sizes of neural networks, depending on the
determined input number, the networks with 1 hidden layer and 2 hidden layers
are used for training and test by changing neuron sizes in the hidden layer(s).
It is found that the networks with 1 hidden layer and 4 neurons can make RMS
error converge to lower values. Optimum network size NN341 shows the number
of neurons in each layer for TOPIX. Therefore, the neural network forecasters
are constructed based on determined input pattern and the number of neurons
in the hidden layer(s).

3.2

Estimation of Determinstic Component

To verify whether the optimum neural network acquires underlying dynamics of
the system from the data, both training RMS error and test RMS error should
be estimated.
All the neurons in the multiplayer perceptron should desirably learn at the
same rate. To a good degree of approximation, the misadjustment of back propagation (BP) algorithm is directly proportional to the learning rate parameter
[7]. Therefore, if the learning rate parameter is increased so as to accelerate
the learning process, then the misadjustment is increased. Careful attention has
therefore to be given to the choice of the learning rate parameter in the design
of the network in order to produce a satisfactory overall performance.
Table 2 shows the simulation results for N N 341. During convergence, weight
updates not only include the change dictated by the learning rate, but also
include a portion of the last weight change. The training process will terminate
when the RMS error is less than 0.001 or the learning times are over 3,000
iterations. Since the RMS errors for both training and test have small values,

790

D. Gao, Y. Kinouchi, and K. Ito
0.5
0.45
0.4
RMS Error

0.35

Training

0.3

Test

0.25
0.2
0.15
0.1
0.05

3000

2700

2400

2100

1800

1500

900

1200

600

0

300

0

Iteration

Fig. 3. Convergence Process of N N 341. The solid line shows the training process. The
dotted line describes corresponding test process.
1
0.8

Output

0.6

Target

0.2
0
-0.2
-0.4

50
100
150
200
250
300
350
400
450
500
550
600
650
700
750
800
850
900
950
1000
1050
1100
1150
1200
1250
1300
1350
1400
1450
1500

TOPIX

0.4

-0.6

Day

-0.8
-1

Training Phase

Test Phase

Fig. 4. Comparison of Neural Network Outputs (predicated values) and Target Data
(xk ) after training of 3000 iterations.

smaller than the fluctuation of the data, the network obtains good generalization
and acquires the source dynamics.
F ig. 3 shows that before training, synaptic connections wij have random volumes and the network yields large RMS error. After training of 3000 iterations,
where one iteration corresponds to one round of training using all training data,
the average RMS error becomes as small as 0.019626. RMS error in test phases
is somewhat higher than that in training phases. However, RMS errors in both
phases show a similar trend, so neural networks acquire underlying dynamics of
the system from the data.
It is found from F ig. 4 that the network outputs follow the data change in
both early training and later test phases. Clearly, the network has acquired the
regularity in both phases.

3.3

Estimation of Random Component

To evaluate the predictability of neural networks and to analyze the random
components of the data, training errors (the residuals between the outputs of
neural networks and corresponding target data) are used as input data, and
neural networks with the same optimum sizes are retrained.

Neural Networks for Event Detection from Time Series

791

0.2
Target

0.15

Output

Training Error

0.1
0.05
0
-0.05
-0.1
-0.15

750

700

650

600

550

500

450

400

350

300

250

200

150

100

0

50

-0.2

Day

Fig. 5. Prediction by N N 341 Using Training Error as Input Data. When the Training
errors are used as the input data, the outputs of neural networks are close to zero.
0.9
0.8

Degree of Randomness

0.7
0.6
0.5

Random

0.4

TOPIX
Sinusoidal

0.3
0.2

Ο

0.1

5000

3000

1300

1100

741

650

500

300

200

10

100

0

Data Number

Fig. 6. Degree of Randomness for Random Data, TOPIX, and Sinusoidal Data. Maximum number of TOPIX is 741. Maximum number of random and sinusoidal data
is 5000. Values of R for random data are the mean of ten data series. The x-axis
graduation varies.

It is found from F ig. 5 that the outputs of neural networks are close to
zero. This may mean that training errors are white noise, so we can deduce that
TOPIX data are composed of a regular (deterministic) part and a random part.
To verify this further, both TOPIX and two types of generated data (see
Sect. 2.2) are trained similarly using neural networks. After training, the network outputs for random data are close to zero and the network outputs for
deterministic data close to the target data. The network outputs for TOPIX fall
between outputs for random data and outputs for deterministic data.
It is found from F ig. 6 that values of R gradually approach one (1) for random
data and zero (0) for sinusoidal data as the data number increases. Values of R
for random data reduce with decreasing the data number because randomness
may be dificult to find using relatively small amounts of the data. As in the case
of neural network outputs, values of R for TOPIX are between values of R for
random data and sinusoidal data. This evidence gives a convincing picture that
TOPIX are considered to include both deterministic and random components.
Calculating values of R gives us important insights into regularity and noise,
showing how neural networks can forecast performance in financial markets.

792

D. Gao, Y. Kinouchi, and K. Ito
1
0.9
0.8
0.7

cos θ

0.6
0.5
0.4
0.3
0.2
0.1
0
1

4

7

10 13 16 19 22 25 28 31 34 37 40 43 46 49 52 55 58 61 64 67
Moving Pattern Group (MPG)

Fig. 7. Change of Internal Representation. Input data are half-year TOPIX daily data,
and 1-month shift. M P G1 = the period from Jan. 1995 to Jun. 1995, !D, M P G67 =
the period form Jul. 2000 to Dec. 2000.

3.4

Network Adaptability and Environment Influence

Since the neural networks acquire dynamic properties from the data, stock market mechanism can be understood well by examining how external forces influence the network adaptability. The properties of trained networks are represented by weights wi between neurons, a kind of internal representation of the
networks. The optimum network N N 341 has 21 weights denoted by vector w.
The dynamic properties of the system vary with time, and this adaptive change
may be evaluated by the change of weight vector w. The networks are trained
successively by moving pattern groups M P G1 , M P G2 , . . ., M P G67 such as
M P Gk : Pk , Pk+1 , . . ., Pk+n , k = 1 ∼ 67

(9)

where n corresponds to 6 months of the data. Each group has 6 months of the
data, enough to acquire dynamics of the system. Convergence of training by
using each pattern group produces weight vectors, w1 , w2 , . . ., w67 . Dynamics
of the system is characterized by the matrix of transition probabilities between
network groups. The adaptive change of acquired dynamics may be evaluated by
the direction change of weight vectors from w1 . The direction change is measured
by cos &H, that is
cos θk =

wk · w1
, k = 1 ∼ 67
|wk | · |w1 |

(10)

where &H is an angle between two weight vectors.
Inspection of the change of cos θk in F ig. 7 provides insight into how environment influences the network properties. Plotting the weight vectors as a
function of time reveals that the environment influences during the corresponding periods. Since cos θk is more than about 0.6 as a whole, the properties of
the network is considered to be maintained almost the same. Generally, cos θk
is kept relatively constant for a period of time, but there are gradual changes in
some periods. It is therefore considered that homeostatic dynamics of the system may be adjusted occasionally due to environment changes and then stay in
steady state for some periods. At some points, cos θk changes, indicating that
the networks obtain different weight vectors, such as in M P G6 , corresponding
to June 1995–Nov. 1995; M P G40 , corresponding to Apr. 1998–Sep. 1998; and

Neural Networks for Event Detection from Time Series

793

M P G53 , corresponding to May 1999–Oct. 1999, the networks obtain slightly
different weight vectors.

4

Conclusion

This paper demonstrated the usefulness of neural networks as an event detection
technique for financial time series. Empirical results show that dynamic properties of the regularity components were captured by neural network model. The
results can be attributed to the high ability of the network to capture nonlinear market patterns. Calculation of data randomness shows that randomness
combines with determinism to create statistical order. Prediction using training
errors as inputs of networks and calculating degree of randomness prove that financial time series include both predictable components and unpredictable components. The property that weights in neural networks remain relatively stable
in some periods characterizes the structure of financial markets. Such property
shows that stock market has memory, maybe because investors often make market decisions based on past situations. Evaluating the change of weights in neural
networks shows that homeostatic dynamics change occasionally, perhaps because
of adaptation to environmental factors, which may induce the fluctuations. Neural networks can acquire exogenous regularities and endogenous dynamics.
In our research, we have systematically examined effects of various factors
such as the number of model inputs, the number of hidden layer neurons and
influence of the event. The question that still remains is under what conditions
are they better than tranditional statistical methods. Future experiments could
focus on comparative study of neural network approach with other statistical
time series approach to understand the nature of financial markets well.

References
1. Apostolos-Paul N. Refenes, A. Neil Burgess, and Yves Bentz: Neural Networks in
Financial Engiineering: A Study in Methodology. IEEE Transactions on Neural
Networks, 8-6 (1997) 1222–1267.
2. Weigend, A., Huberman, B. and Rumelhart, D.: Predicting the future: a connectionist approach. International Journal of Neural Systems, 1-3 (1990) 193–209.
3. Jingtao Yao, Chew Lim Tan and Hean-lee Poh.: Neural network for technical analysis: a study on KLCI. International Journal of Theoretical and Applied Finance,
2-2 (1999) 221–241.
4. Rumelhart, D. E., Hinton, G. E., Williams, R. J.: Learning internal representation
by error propagation. In Rumelhart, D. E., McClelland, J. L., and the PDP research
group(Eds), parallel distributed processing, MIT press, Cambridge, MA (1986) 318–
362.
5. Perambur S. Neelakanta: Information-Theoretic Aspects of Neural Networks. CRC
Press LLC, (1999).
6. Matsuba, I., Masui, H., Hebishima, S.: Optimizing multiplayer neural networks using
fractal dimensions of time series data. Proc IEEE Trans Ind Elec, 44 (1997) 707–716.
7. Simon Haykin: Neural Network: A Comprehensive Foundation. Macmillan College
Publishing Company (1994).

