Rescheduling for Locality in Sparse Matrix
Computations
Michelle Mills Strout, Larry Carter, and Jeanne Ferrante
University of California, San Diego

Abstract. In modern computer architecture the use of memory hierarchies causes a program’s data locality to directly aﬀect performance.
Data locality occurs when a piece of data is still in a cache upon reuse.
For dense matrix computations, loop transformations can be used to improve data locality. However, sparse matrix computations have non-aﬃne
loop bounds and indirect memory references which prohibit the use of
compile time loop transformations. This paper describes an algorithm to
tile at runtime called serial sparse tiling. We test a runtime tiled version of sparse Gauss-Seidel on 4 diﬀerent architectures where it exhibits
speedups of up to 2.7. The paper also gives a static model for determining
tile size and outlines how overhead aﬀects the overall speedup.

1

Introduction

In modern computer architecture the use of memory hierarchies causes a program’s data locality to directly aﬀect performance. Data locality occurs when a
piece of data is still in the cache upon reuse. This paper presents a technique
for tiling sparse matrix computations in order to improve the data locality in
scientiﬁc applications such as Finite Element Analysis.
The Finite Element Method (FEM) is a numerical technique used in scientiﬁc applications such as Stress Analysis, Heat Transfer, and Fluid Flow. In FEM
the physical domain being modeled is discretized into an unstructured grid or
mesh (see ﬁgure 3). FEM then generates simultaneous linear equations that describe the relationship between the unknowns at each node in the mesh. Typical
unknowns include temperature, pressure, and xy-displacement. These equations
are represented with a sparse matrix A and vectors u and f such that Au = f .
Conjugate Gradient, Gauss-Seidel and Jacobi are all iterative methods for
solving simultaneous linear equations. They solve for u by iterating over the
sparse matrix A a constant number of times, converging towards a solution. The
iteratively calculated value of a mesh node unknown uj depends on the values
of other unknowns on the same node, the unknowns associated with adjacent
nodes within the mesh, and the non-zeros/coeﬃcients in the sparse matrix which
relate those unknowns. Typically the sparse matrix is so large that none of the
values used by one calculation of uj remain in the cache for future iterations on
uj , thus the computation exhibits poor data locality.
For dense matrix computations, compile time loop transformations such as
tiling or blocking [17] can be used to improve data locality. However, since sparse
V.N. Alexandrov et al. (Eds.): ICCS 2001, LNCS 2073, pp. 137–146, 2001.
c Springer-Verlag Berlin Heidelberg 2001
�

138

M. Mills Strout, L. Carter, and J. Ferrante

matrix computations operate on compressed forms of the matrix in order to avoid
storing zeros, the loop bounds are not aﬃne and the array references include
indirect memory references such as a[c[i]]. Therefore, straightforward application
of tiling is not possible. In this paper, we show how to extend tiling via runtime
reorganization of data and rescheduling of computation to take advantage of the
data locality in such sparse matrix computations.
Speciﬁcally, we reschedule the sparse Gauss-Seidel computation at runtime.
First we tile the iteration space and then generate a new schedule and node
numbering which allows each tile to be executed atomically. Typically the numbering of the nodes in the mesh is arbitrary, therefore, renumbering the nodes
and maintaining the Gauss-Seidel partial order on the new numbering allows us
to still use the convergence theorems for Gauss-Seidel. The goal is to select the
tile size so that the tile only touches a data subset, which ﬁts into cache.

0

u0
a00

1 2

3

0
1
2

0
1

3

3

a01
a10

2

A
u1
a11

a12
a21

rptr

u

a23
u2
a32
a22

(a) Logical data associations

u3
a33

0 2 5

...

c 0 1 0 1 2 1 2 ...
a a00 a01 a10 a11 a12 a21 a22 ...
(b) Actual CSR storage format

Fig. 1. Data associated with Mesh

To illustrate, we look at an example of how one would tile the Gauss-Seidel
computation on a one-dimensional mesh. Figure 1(a) shows how we can visualize what data is associated with each node in the mesh. The unknown values
being iteratively updated are associated with the nodes, 1 and the coeﬃcients
representing how the unknowns relate are associated with the edges and nodes.
However, keep in mind that the matrix is stored in a compressed format like
compressed sparse row (see ﬁgure 1(b)) to avoid storing the zeros.
The pseudo-code for Gauss-Seidel is shown below. The outermost loop iterates over the entire sparse matrix generated by solving functions on the mesh.
We refer to the i iterator as the convergence iterator. The j loop iterates over
the rows in the sparse matrix. 2 The k loop which is implicit in the summations
(i)
(i−1)
iterates over the unknowns which are related to uj , with ajk uk and ajk uk
only being computed when ajk is a non-zero value.
1
2

In this example there is only one unknown per mesh node.
There is one row in the matrix for each unknown at each mesh node.

Rescheduling for Locality in Sparse Matrix Computations

139

for i = 1, 2, ..., T
for j = 1, 2, ..., R
�j−1
�n
(i)
(i)
(i−1)
uj = (1/ajj )(fj − k=1 ajk uk − k=j+1 ajk uk
)
The Gauss-Seidel computation can be visualized with the iteration space
graph shown in ﬁgure 2. Each black iteration point 3 , < i, v >, represents the
(i)
computations for all uj where uj is an unknown associated with mesh node
v and i is the convergence iteration. The initial values associated with a 1D
mesh are shown in white. The arrows represent data dependences 4 that specify
when an initial value or a value generated by various iteration points is used
by other iteration points. We refer to each set of computation for a particular
value of i within the iteration space as a layer. Figure 2 contains three layers of
computation over a mesh.

Tile0

i

Data used by Tile0

(a) Original Computation

Tile1

Data used by Tile1

(b) Divided into 2 Tiles

Fig. 2. Gauss-Seidel Iteration Space Graph

Notice that the sparse matrix values associated with the edges adjacent to
a particular mesh node v are reused in each computation layer. However, the
mesh is typically so large that upon reuse the matrix entries are no longer in
the cache. To improve the computation’s data locality, we reschedule it based
on a tiling like the one shown in ﬁgure 2(b). The resulting schedule executes all
of the iteration points in one tile before continuing on to the next tile; in other
words, each tile is executed atomically. By choosing an appropriate tile size the
data used by each tile will ﬁt into cache for all instances of < i, v > within the
tile and therefore improve the data locality of the computation.
In Section 2 we present the algorithm which tiles and reschedules GaussSeidel at runtime. Then in section 3 we give experimental results which show
that improving the data locality does improve code performance. We also outline
the aﬀect of overhead and how to select tile sizes. Finally, we present some related
work and conclusions.
3
4

We use the term iteration point for points in the iteration space graph and node for
points in the mesh.
Some dependences are omitted for clarity.

140

2

M. Mills Strout, L. Carter, and J. Ferrante

Tiling Sparse Computations

In order to tile the iteration space induced by the convergence iteration over
the mesh, we partition the mesh and then grow tiles backwards through the
iteration space based on the seed partitions. Figure 3 shows the iteration space
for a 2D mesh with each layer drawn separately. Edges show the connectivity of
the underlying mesh. We use the resulting tiling to reschedule the computation
and renumber the nodes in the mesh. Since tiles depend on results calculated by
neighboring tiles, the tiles must be executed in a partial order which respects
those dependences.

(2)

(1)

(3)

Tile0

Tile0

Tile0

(2)

Tile2

(3)

Tile2

(1)

Tile2

(1)

Tile1

(3)

Tile1
(2)

Tile1

(2)

Tile3

(3)

Tile3

Fig. 3. Tile layers for T ile0 , T ile1 , T ile2 , and T ile3 . The tile layers for T ile0 are shaded.

We refer to the runtime tiling of sparse matrix computations as sparse tiling.
This paper describes and implements a serial sparse tiling, in that the resulting
schedule is serial. Douglas et al. [4] describe a parallel sparse tiling for GaussSeidel. They partition the mesh and then grow tiles forward through the iteration
space (in the direction of the convergence iterator) in such a way that the tiles
do not depend on one another and therefore can be executed in parallel. After
executing the tiles resulting from parallel sparse tiling, it is necessary to execute
a ﬁll-in stage which ﬁnishes all the iteration points not included in the tiles.
Future work includes determining when to use a serial sparse tiling or a parallel
sparse tiling based on the target architecture and problem size.
Both sparse tiling strategies follow the same overall process at runtime.
1.
2.
3.
4.

Partition the mesh
Tile the iteration space induced by the partitioned mesh
Reschedule the computation
Execute the new schedule

The next sub-sections describe each part of the process for the serial sparse
tiling strategy which we have developed.

Rescheduling for Locality in Sparse Matrix Computations

2.1

141

Partition

Although graph partitioning is an NP-Hard problem [6], there are many heuristics used to get reasonable graph partitions. We use the Metis [11] software
package to do the partitioning at runtime on the mesh. The partitioning algorithm in Metis has a complexity of O(|E|) where |E| is the number of edges in
the mesh [12].
2.2

Tiling

Recall the iteration space for sparse Gauss-Seidel shown in ﬁgure 2 where each
iteration point represents values being generated for the unknowns on the associated mesh node v at convergence iteration i. A tile within this space is a
set of layers, one per each instance of the convergence iterator i. Each tile layer
computes the values for a subset of mesh nodes. The ﬁnal layer of a tile (see
the last layer in ﬁgure 3) corresponds to the nodes in one partition, p, of the
mesh. The tile layers for earlier convergence iterations are formed by adding or
deleting iteration points from the seed partition to allow atomic execution of the
tile without violating any data dependences.
To describe the sparse serial tiling algorithm for sparse Gauss-Seidel we use
the following terminology. The mesh can be represented by a graph G(V, E)
consisting of a set of nodes V and edges E. An iteration point, < i, v >,
represents the computation necessary at convergence iteration i for the unknowns
associated with node v. A tile, T ilep , is a set of iteration points that can be
executed atomically. Each tile is designated by an integer identiﬁer p, which also
(i)
represents the execution order of the tiles. A tile layer, T ilep , includes all
iteration points within tile p being executed at convergence iteration i.
The tiling algorithm generates a function θ that returns the identiﬁer for the
tile which is responsible for executing the given iteration point, θ(< i, v >) : I x
V → {0, 1, ..., m}, where m is the number of tiles. T ile0 will execute all vertex
iterations with θ(< i, v >) = 0, T ile1 will execute all vertex iterations with
θ(< i, v >) = 1, etc. A tile vector, Θ(j) =< θ(< 1, v >), ..., θ(< T, v >) >,
stores tile identiﬁers for all the tiles which will be executing iteration points for
a speciﬁc node in the mesh.
The algorithm shown below gives all nodes a legal tile vector. It takes as
input the part function, part(v) : V → {1, 2, ...m}, which is the result of the
mesh partitioning. The part function speciﬁes a partition identiﬁer for each mesh
node. Recall that we will be growing one tile for each seed partition. The ﬁrst
step in the algorithm is to initialize all tile vectors so that each iteration point is
being executed by the tile being grown from the associated mesh node’s partition
in the mesh. W orklist(T ) is then initialized with all nodes. The loop then grows
the tiles backward from i = T by adding and removing iteration points as needed
in order to maintain the data dependences. A detailed explanation of this loop
is omitted due to space constraints.

142

M. Mills Strout, L. Carter, and J. Ferrante

Algorithm AssignTileVector(part)
(1) ∀v ∈ V, Θ(v) =< part(v), part(v), ..., part(v) >
(2) W orklist(T ) = V
(3) for i = T downto 2
(4)
for each node v ∈ W orklist(i)
(5)
for each (v, w) ∈ E
(6)
if w � ∈W orklist(i − 1) then
(7)
if θ(< i − 1, w >) > θ(< i, v >) then
(8)
w ∈ W orklist(i − 1)
(9)
∀q st. 1 ≤ q ≤ (i − 1), θ(< q, w >) ←− θ(< i, v >)
An upper bound on the complexity of this algorithm is O(T |E|) or equivalently O( TdZ
2 ) where d is the degrees of freedom, |E| is the number of edges in the
mesh, Z is the number of non-zeros in the sparse matrix, and T is the number
of convergence iterations the Gauss-Seidel algorithm will perform.
2.3

Renumbering and Rescheduling

The mesh nodes are renumbered in lexicographical order of their corresponding
tile vectors. The lexicographical order insures that the resulting schedule will
satisfy the Gauss-Seidel partial order on the new numbering. We schedule all
the computations in T ilep before any in T ilep+1 , and within a tile we schedule
the computations by layer and within a layer.
2.4

Execute Transformed Computation

Finally, we rewrite the sparse Gauss-Seidel computation to execute the new
schedule. The new schedule indicates which iteration points should be executed
for each tile at each convergence iteration.

3

Experimental Results for Gauss-Seidel

To evaluate the possible beneﬁts of our approach, we compare the performance
of the Gauss-Seidel routine in the ﬁnite element package FEtk [9] with a runtime
tiled and rescheduled version of the same algorithm. For input, we use the sparse
matrices generated for a nonlinear elasticity problem on 2D and 3D bar meshes.
We generate diﬀerent problem sizes by using FEtk’s adaptive reﬁnement. The
rescheduled code runs on an Intel Pentium III, an IBM Power3 node on the Blue
Horizon at the San Diego Supercomputer Center, a Sun UltraSparc-IIi, and a
DEC Alpha 21164.
When not taking overhead into account the new schedule exhibits speedups
between 0.76 (a slowdown) and 2.7 on the four machines, see ﬁgure 4. Next we
describe the simple static model used for selecting the partition size - the main
tuning parameter for the new schedule. Finally we outline the eﬀect overhead
will have on the overall speedup.

Rescheduling for Locality in Sparse Matrix Computations

2D bar mesh

Raw Speedup

3

2

1
Pentium III, 512K
UltraSPARC-IIi, 512K
Alpha 21164, 96K
Power3, 8MB

0

3D bar mesh

3

Raw Speedup

143

2

1

0

0

5000

10000

15000

20000

25000

30000

35000

40000

45000

Problem Size (# of nodes in mesh)
Fig. 4. Speedups over FEtk’s Gauss-Seidel for 2D and 3D bar mesh without adding
overhead. The partition size was selected to ﬁt into the L2 cache on each machine
whose sizes are shown in the legend.

3.1

Partition Size Selection

Before tiling and rescheduling at runtime the available parameters are the number of nodes in the mesh, the number of unknowns per vertex, the number of
convergence iterations, and the cache size of the target architecture. Using this
information we want to determine which partition size will generate tiles which
ﬁt into a level of cache and therefore improve performance.
In Gauss-Seidel
for each unknown at each
�
� mesh node we iteratively compute
wj = fj − k>i ajk ∗ uj and uj = (wj − k<j ajk ∗ uj )/ajj . Using K as the
average number of neighbors each node has in the mesh and d as the number of
unknowns per mesh node, the computation will use 3 ∗ d scalars for the vectors
u, w, and f and K ∗ d2 associated non-zeros from the sparse matrix A while
updating the unknowns for each mesh node. If we assume compressed sparse
row (CSR) storage format then the amount of memory needed by N mesh nodes
is M em(N ) = N ∗ K ∗ d2 ∗ sizeof (double) + N ∗ K ∗ d2 ∗ sizeof (int) + 3 ∗ N ∗
d ∗ sizeof (double). In all of the experiments we solve for N such that only half
the memory in the L2 cache of the machine is utilized.

Raw Speedup

Raw Speedup

Raw Speedup

Raw Speedup

144

M. Mills Strout, L. Carter, and J. Ferrante
2D bar mesh with 37463 nodes
3D bar mesh with 40687 nodes
Calculated partition size for 2D mesh
Calculated partition size for 3D mesh

2.75

Pentium III, 512K

2.5
2.25
2
1.5

UltraSparc-IIi, 512K

1.4
1.3
1.2
1.5
1

Alpha 21164, 96K

0.5
0
1.2
1.1
1
0.9
0.8
0.7
0.6

Power3, 8MB

100

1000

10000

1e+05

Partition Size (# of rows in matrix)
Fig. 5. How the partitions size aﬀects speedup for 5 convergence iterations on 4 diﬀerent
machines. The outlined symbols represent partitions sizes calculated by the model.

Figure 4 shows the speedups on diﬀerent mesh sizes when the partition sizes
are selected in this manner. When compared to the speedups over a variety of
partitions sizes the calculated partition sizes do reasonably well, see ﬁgure 5,
except on the Alpha. This is probably due to the small L2 size on the Alpha and
the existence of a 2MB L3 cache.
3.2

Overhead

Figures 4 and 5 show the speedups obtained by the tiled and rescheduled code
over FEtk’s implementation of Gauss-Seidel without taking overhead into account. It is important to look at the speedups without overhead because GaussSeidel can be called multiple times on the same mesh within an algorithm like
Multigrid. Therefore, even though overhead might make rescheduling not beneﬁcial for one execution of the Gauss-Seidel computation, when amortized over
multiple calls to the computation we get an overall speedup.
By looking at the calculated partition sizes resulting in the highest (2.68)
and lowest 5 (1.11) speedups we see that the tiled and rescheduled version of
5

Ignoring the Power3 results because speedup was less than 1.

Rescheduling for Locality in Sparse Matrix Computations

145

Gauss-Seidel would need to be called between 5 to 27 times in order to observe
an overall speedup. However, there were partition sizes not calculated by our
simple static model which resulted in an overall speedup with only 1 call to the
tiled and rescheduled Gauss-Seidel. For example, on a 3D mesh with N = 40, 687
an overall speedup of 1.17 is observed even when the overhead cost is included.
This indicates that the tradeoﬀ between raw speedup and overhead must be
considered when calculating partition sizes.

4

Related Work

Douglas et al. [4] does tiling on the iteration space graph resulting from unstructured grids in the context of the Multigrid algorithm using Gauss-Seidel as
a smoother. They achieve overall speedups up to 2 with 2D meshes containing
3983, 15679, and 62207 nodes on an SGI O2. They are able to reschedule their
tiles in parallel and then ﬁnish the remaining computation with a serial backtracking step. Our technique also tiles the Gauss-Seidel iteration space, but we
execute our tiles serially in order to satisfy dependences between tiles. Also, we
do not require a backtracking step which exhibits poor data locality. These two
tiling algorithms are instances of a general class of temporal locality transformations which we will refer to as sparse tiling.
Mitchell et al [14] describe a compiler optimization which operates on nonaﬃne array references in code. The use of sparse data structures causes indirect
array references which are a type of non-aﬃne array reference. Also, Eun-Jin
Im [10] describes a code generator called SPARSITY which generates cacheblocked sparse matrix-vector multiply. Both of these techniques improve spatial
and temporal locality on the vectors u and f when dealing with the system
Au = f . However, they do not improve the temporal locality on the sparse
matrix, because in their rescheduled code the entire sparse matrix is traversed
each convergence iteration. Other work which looks at runtime data reorganization and rescheduling includes Demmel et al. [2], Han and Tseng[8], Ding and
Kennedy [3], and Mellor-Crummey et al.[13].

5

Conclusion

Runtime tiling is possible with unstructured iteration spaces, and we show it can
improve the data locality and therefore the performance of Gauss-Seidel. Specifically we present an algorithm for generating a serial sparse tiling for GaussSeidel. We also describe a simple static model for selecting partition sizes from
which the tiles are grown. Future work includes improving the model used to calculate partition sizes so that the tradeoﬀ between overhead and raw speedup is
taken into account. Also, the performance model needs to determine when to use
a serial sparse tiling or a parallel sparse tiling based on the target architecture
and problem size.

146

M. Mills Strout, L. Carter, and J. Ferrante

References
1. Jeﬀ Bilmes, Krste Asanović, Chee whye Chin, and Jim Demmel. Optimizing matrix
multiply using PHiPAC: a Portable, High-Performance, ANSI C coding methodology. In Proceedings of International Conference on Supercomputing, Vienna, Austria, July 1997.
2. James W. Demmel, Stanley C. Eisenstat, John R. Gilbert, Xiaoye S. Li, and Joseph
W. H. Liu. A supernodal approach to sparse partial pivoting. SIAM Journal on
Matrix Analysis and Applications, 20(3):720–755, July 1999.
3. Chen Ding and Ken Kennedy. Improving cache performance in dynamic applications through data and computation reorganization at run time. In Proceedings of
the ACM SIGPLAN ’99 Conference on Programming Language Design and Implementation, pages 229–241, Atlanta, Georgia, May 1–4, 1999.
4. Craig C. Douglas, Jonathan Hu, Markus Kowarschik, Ulrich Rüde, and Christian
Weiss. Cache Optimization for Structured and Unstructured Grid Multigrid. Electronic Transaction on Numerical Analysis, pages 21–40, February 2000.
5. Matteo Frigo and Steven G. Johnson. Fftw: An adaptive software architecture for
the ﬀt. In Proceedings of the IEEE International Conference on Acoustics, Speech,
and Signal Processing, page 1381, 1998.
6. Michael R. Garey, David S. Johnson, and L. Stockmeyer. Some simpliﬁed NPcomplete graph problems. Theoretical Computer Science, 1:237–267, 1976.
7. Kang Su Gatlin. Portable High Performance Programming via Architecture Cognizant Divide-and-Conquer Algorithms. Ph.d. thesis, University of California, San
Diego, September 2000.
8. Hwansoo Han and Chau-Wen Tseng. Eﬃcient compiler and run-time support for
parallel irregular reductions. Parallel Computing, 26(13–14):1861–1887, December
2000.
9. Michael Holst. Fetk = the ﬁnite element toolkit. http://www.fetk.org.
10. Eun-Jin Im. Optimizing the Performance of Sparse Matrix-Vector Multiply. Ph.d.
thesis, University of California, Berkeley, May 2000.
11. George Karypis and Vipin Kumar. Metis: A Software Package for Partitioning
Unstructured Graphs, Partitioning Meshes, and computing Fill-Reducing Orderings
of Sparse Matrices Version 4.0, 1998.
12. George Karypis and Vipin Kumar. Multilevel k-way partitioning scheme for irregular graphs. Journal of Parallel and Distributed Computing, 48(1):96–129, 10 January 1998.
13. John Mellor-Crummey, David Whalley, and Ken Kennedy. Improving memory
hierarchy performance for irregular applications. In Proceedings of the 1999 Conference on Supercomputing, ACM SIGARCH, pages 425–433, N.Y., June 20–25
1999. ACM Press.
14. Nicholas Mitchell, Larry Carter, and Jeanne Ferrante. Localizing non-aﬃne array
references. In Proceedings of the 1999 International Conference on Parallel Architectures and Compilation Techniques (PACT ’99), pages 192–202, Newport Beach,
California, October 12–16, 1999. IEEE Computer Society Press.
15. Nick Mitchell. Guiding Program Transformations with Modal Performance Model.
Ph.d. thesis, University of California, San Diego, August 2000.
16. R. Clint Whaley and Jack J. Dongarra. Automatically tuned linear algebra software. In Supercomputer 98, 1998.
17. Michael J. Wolfe. High Performance Compilers for Parallel Computing. AddisonWesley, 1996.

