Nearly One-Sided Tests and the Goldreich-Levin
Predicate
Gustav Hast
Department of Numerical Analysis and Computer Science
Royal Institute of Technology, 100 44 Stockholm, Sweden
ghast@nada.kth.se

Abstract. We study statistical tests with binary output that rarely outputs one, which we call nearly one-sided statistical tests. We provide an
eﬃcient reduction establishing improved security for the Goldreich-Levin
hard-core bit against nearly one-sided tests. The analysis is extended to
prove the security of the Blum-Micali pseudo-random generator combined with the Goldreich-Levin bit.
Furthermore, applications where nearly one-sided tests naturally occur
are discussed. This includes cryptographic constructions that replace
real randomness with pseudo-randomness and where the adversary’s
success easily can be veriﬁed. In particular, this applies to signature
schemes that utilize a pseudo-random generator as a provider of
randomness.
Keywords: Nearly one-sided statistical test; Goldreich-Levin predicate;
Pseudo-random generator; Provable security; List decoding.

1

Introduction

Many algorithms are probabilistic and therefore require a source of randomness
to be implemented correctly. This is true in particular for most cryptographic
algorithms. Obtaining random material is often a hard and time consuming process and therefore it is convenient to use a pseudo-random generator to generate
much random looking material from a short truly random seed. One would of
course like to have a guarantee that by exchanging random material for the
output of a generator, the performance of the algorithms are not changed in a
harmful way.
The pioneering works of Blum and Micali [6] and Yao [19] laid the foundation
of the theory of pseudo-randomness. Blum and Micali showed how to construct a
pseudo-random bit generator (PRBG) whose security is based on the hardness of
solving the discrete logarithm problem. More speciﬁcally, they proved and used
the fact that the most signiﬁcant bit is a hard-core predicate for exponentiation.
A predicate b is a hard-core predicate for a function g if it is not feasible to
eﬃciently determine the boolean value of b(x) given the value of g(x). Goldreich
and Levin [11] showed how to construct such a hard-core predicate from any oneway function. This construction can be applied on the above mentioned PRBG
E. Biham (Ed.): EUROCRYPT 2003, LNCS 2656, pp. 195–210, 2003.
c International Association for Cryptologic Research 2003

196

G. Hast

so that the security can be based on the one-wayness of an arbitrary permutation
f . The proof of security provided by [6] was a polynomial reduction from solving
the discrete logarithm problem (or if we use the result from [11] inverting f ), to
breaching the security of the bit generator.
In this work we analyze the security of the well-known pseudo-random generator obtained by combining the works of Blum and Micali [6] and Goldreich
and Levin [11]. We refer to this generator as BM GL. As noted in previous works
(e.g., [10], [11] and [16]) the exact eﬃciency of a reduction between two diﬀerent
cryptographic primitives is of vital interest when determining the practical security consequences of the reduction. Examples of more recent works that deal
with the issue to bridge the gap between theoretical complexity based cryptography and practical cryptography by improved reductions and analysis are [3]
and [7]. For a more extensive list see [2]. In the case of BM GL, the reduction relates the one-wayness of a permutation to the pseudo-randomness of the output
from BM GL. A distribution is considered to be pseudo-random if there is no
statistical test, from a speciﬁc set of admissible tests, that more than negligibly
can distinguish between elements from that distribution and from the uniform
distribution. (A distribution is in fact not considered to be pseudo-random, but
instead an ensemble of distributions. For simplicity reasons we do not make this
distinction throughout the introduction.) Normally, the set of admissible tests
is speciﬁed by a maximum running time. Improvements to the security reduction of BM GL and its analysis have earlier been made by Rackoﬀ (explained in
[8]), Levin [17] and H˚
astad and N¨
aslund [15]. Apart from BM GL, there have
been numerous other constructions of pseudo-random generators based on the
Blum-Micali paradigm, for example constructions exploiting the hardness of the
factoring problem starting with the work of Blum et al. [4].
Earlier analyses of reductions have characterized the eﬃciency of a statistical
test D, distinguishing the distributions X and Y , by using a measure δ such that
Pr [D(x) = 1] − Pr [D(y) = 1] ≥ δ .

x∈X

y∈Y

(1)

In this paper we consider nearly one-sided statistical tests (this concept has been
investigated earlier by Blum and Goldreich [5]) which are tests that, on truly random input, almost always output zero and rarely output one. The measure in (1)
does not capture whether or not a test is nearly one-sided and therefore we introduce the notion of a parameterized distinguisher, which for a test and a certain
pair of input distributions impose two thresholds, separating the corresponding
output distributions of the test. We say that a test D (δ1 , δ2 )-distinguishes X
and Y if
Pr [D(x) = 1] ≤ δ1 < δ2 ≤ Pr [D(y) = 1] .
x∈X

y∈Y

Thus, if δ1 is small and X is the uniform distribution, the test is considered
to be nearly one-sided. (We do not formally deﬁne “small” but instead use the
parameterized distinguisher to express formal results in this paper.) The use of
this characterization of a distinguisher enables a more careful analysis of the

Nearly One-Sided Tests and the Goldreich-Levin Predicate

197

reduction from inverting a permutation to distinguishing between the output
of BM GL and the uniform distribution. The analysis shows that the success
probability of inverting the permutation is proportional to (δ2 − δ1 )2 /δ2 , to be
compared to previous results obtaining (δ2 − δ1 )2 . If δ1 is small compared to δ2
(as is the case with nearly one-sided tests) the increase in reduction quality is
signiﬁcant.
The heart of the improved analysis is in the analysis of the Goldreich-Levin
hard-core bit. Essentially, the classical proof shows that if the adversary can
predict the hard-core bit with advantage ε, then one can invert the one-way
function with probability proportional to ε2 . This was shown by Adcock and
Cleve [1] to be optimal in the general case. In the case of nearly one-sided tests,
the distinguisher can be transformed to a predictor that most of the time has
almost no advantage against a random guess, but for a small fraction of the
inputs it has a signiﬁcant advantage. In the classical proof this predictor had
to make a guess even though its conﬁdence was low. This caused the produced
high quality predictions to be concealed by the big amounts of predictions of low
quality. In our reduction we therefore allow the predictor also to output ⊥, which
means that the conﬁdence is too low to make a prediction. This enables us to
invert the one-way function with probability proportional to ε2 /p, where p is the
probability that the predictor makes a prediction. (The seemingly contradiction
that the success probability increases when the probability of making a prediction
decreases is explained by the fact that this also implies that the conﬁdence in
the prediction increases.)
The reduction that was used to show that the Goldreich-Levin bit is hard
to predict is essentially a list decoding algorithm of the Hadamard code. Using
this viewpoint, the possibility for the predictor to output ⊥ will correspond to
an erasure in the Hadamard code. After the work of Goldreich and Levin [11]
subsequent works in list decoding includes a generalization of the algorithm to
the non-binary and non-linear case [12]. Sudan et al. [18] showed how to transform predicates, that are hard in the worst case, to become predicates that can
be predicted only negligibly better than by a random guess. The transformation
and proof was based on error-correcting codes and list decoding.
In cryptography the major application can be found in diﬀerent types of
authentication schemes as nearly one-sided tests often occur there naturally. For
example, consider a signature scheme that is secure if given a source of random
bits. Suppose that one instead feeds this signature scheme with bits from a bit
generator (e.g. BM GL) and that it then no longer is secure. The attacker of this
schemes now serves as a nearly one-sided distinguisher between the two diﬀerent
schemes because it has a negligible respectively non-negligible probability to
produce a valid signature when attacking the two diﬀerent schemes. Thus, this
attacker can be combined with the signature scheme to build a nearly one-sided
test that distinguishes between the output distribution of the bit generator and
the uniform distribution. In Sect. 8, a more extensive discussion is made about
possible applications.

198

G. Hast

The outline of this paper is as follows: In Sect. 3 the Goldreich-Levin hardcore bit is explained and we discuss why low rate predictors are more powerful
than ordinary predictors when list decoding the Hadamard code. In the next
section we prove a theorem about list decoding Hadamard codes with both erasures and errors and in Sect. 5 this theorem is used to establish the reduction
from inverting a function to predicting the Goldreich-Levin bit. Section 6 discusses statistical tests and their connection with predictors and in Sect. 7 the
security of the BM GL is shown. The paper is concluded with some applications
and open questions.

2

Notation

In this work we use the following notation:
1.
2.
3.
4.
5.
6.
7.
8.

3

By [m] we mean the set {1, . . . , m} and 2[m] is the set of all subsets of [m].
The xor operation is denoted by ⊕.
The function b(r, x) is the inner product between r and x modulo 2.
The i’th unit vector ei is a bit string containing only zeros, except for the
i’th bit (which is 1). The dimension of ei is implicitly given by its use.
We let J, L , where J and L are sets, denote the size of J ∩ L modulo 2.
If x is a bit string, the length of x is denoted by |x|.
When the logarithmic function log is used without the base having been
speciﬁed, it is implicit base 2.
The uniform distribution of bit strings of length n is denoted by Un .

The Goldreich-Levin Bit and List Decoding of
Hadamard Code

Goldreich and Levin [11] showed how to modify an arbitrary one-way function
to make it have a hard-core predicate: if f is a one-way function, then b(r, x)
(the inner product of r and x modulo 2) is a hard-core predicate for the one-way
function f (r, x) = (r, f (x)). This means that there is no eﬃcient algorithm that
given (r, f (x)) as input (where r and x are drawn from the uniform distribution)
can guess the value of b(r, x) signiﬁcantly better than a random guess. We do
not formally deﬁne hard-core predicate as the results in this paper are instead
expressed in terms of the advantage and rate of a predictor (see Deﬁnition 2).
The above result is shown using a reduction from inverting f to predicting
the value of b(r, x). The eﬃciency of the reduction depends on how well the
bit b(r, x) is guessed, which usually is measured by the advantage ε(n) of the
guessing algorithm P , often called the predictor:
ε(n) =

Pr [P (r, f (x)) = b(r, x)] −

r,x∈Un

1
.
2

The main part of the reduction consists of a list decoding algorithm for the
binary Hadamard code.

Nearly One-Sided Tests and the Goldreich-Levin Predicate

199

Deﬁnition 1. The (binary) Hadamard code of a bit string x of length n is
b(r, x) r∈{0,1}n .
The i’th bit of the Hadamard code of x is thus exactly b(i, x), where i is
interpreted in the natural way as a bit string of the same length as x. The task
for a list decoding algorithm is to produce a list of possible x, having oracle access
to a Hadamard code with a certain fraction of errors. The algorithm should come
with a lower bound on the probability that x appears in the list output and an
upper bound on the number of oracle queries made. Given the value of f (x), the
predictor P corresponds in a natural way to the oracle of the Hadamard code,
and the advantage of P (over a ﬁx x) is closely related to the number of errors
of the oracle.
Now suppose that we have a predictor P that on some input answers with
high conﬁdence and on other inputs just ﬂips a coin. The informative answers
from P (when it does not just ﬂip a coin) would then be somewhat clouded by the
random noise provided by the other answers. Let us therefore give the predictor
more freedom by also letting it output ⊥ in those cases where the conﬁdence in
the prediction is low. Instead of characterizing this type of predictor with only
its advantage in the traditional sense, we also use its rate, which is how often
it outputs a prediction. The advantage is generalized in a natural way for this
diﬀerent type of predictor.
Deﬁnition 2. A predictor P : {0, 1}∗ × {0, 1}∗ → {0, 1, ⊥} is said to have rate
δ(n) and advantage ε(n) in predicting b(x, r) from f (x) and r, where
δ(n) =

Pr [P (f (x), r) = ⊥]

x,r∈Un

and
ε(n) =

Pr [P (f (x), r) = b(x, r)] −

x,r∈Un

1
Pr [P (f (x), r) = ⊥] .
2 x,r∈Un

Going back to the list decoding algorithm for the Hadamard code, this new
type of predictor corresponds to a Hadamard code oracle with both errors and
erasures, where the fraction of erasures is 1 − δ (where δ is the rate of the predictor). The heart of the improved reduction is in the analysis of the list decoding
algorithm with an oracle that has a relatively large part of erasures (or in other
words a predictor with low rate). Let us brieﬂy discuss why a predictor with low
rate is more powerful than one with a higher rate and the same advantage. With
more powerful we here mean that the predictor does not have to be called as
many times in the list decoding algorithm. Later we show how a nearly one-sided
test for the Goldreich-Levin bit easily can be turned into a predictor with low
rate.
Assume that P is a predictor with rate δ and advantage ε. Earlier analyses,
that only made use of the advantage, have shown (see [8], Sect. 2.5.2) that the
number of needed calls to P should be at least proportional to ε−2 for the list
decoding algorithm to succeed with probability one half. The probability that
P makes a correct prediction is 12 δ + ε. Let us now ignore all the calls that
received ⊥-answers from P . The probability that P , on the remaining calls,

200

G. Hast

guesses correctly is then ( 12 δ + ε)δ −1 = 12 + δ −1 ε. In some sense this gives us
a not fully working predictor with advantage δ −1 ε, the problem being that it
does not make predictions for all inputs and that it on the average only makes
one prediction per δ −1 calls. If the ﬁrst problem mentioned can be dealt with
in the list decoding algorithm we can expect that the number of calls to P is
proportional to the inverted advantage squared times the extra factor of δ −1 :
O((δ −1 ε)−2 δ −1 ) = O(δε−2 ). Note that if the advantage of P is at least a constant
fraction of the rate, we have that δ = O(ε) and thus the number of calls needed
would only be O(ε−1 ) compared to O(ε−2 ) before. In the proofs of Theorem 1
and 2 we show that this intuition really has merit.

4

List Decoding of Hadamard Code with Errors and
Erasures

The main part of the proof that the Goldreich-Levin bit is a hard-core predicate
consists of a list decoding algorithm of a binary Hadamard code with errors. To
ensure that the power of the low rate of the predictor does not vanish, we repeat
the analysis of the list decoding algorithm (not the original one [11], but one
due to Rackoﬀ explained in [8]) while letting the Hadamard code also contain
erasures. As far as the author is aware of, no previous work has been done on
list decoding the Hadamard code in the presence of errors and erasures.
Theorem 1. There is an algorithm LD that, on input l and n and with oracle
access to a Hadamard code of x (where |x| = n) with an e-fraction of errors
and an s-fraction of erasures, can output a list of 2l elements in time O(nl2l )
asking n2l oracle queries such that the probability that x is contained in the list
def
is at least one half if l ≥ log2 8n(e+c)
(c−e)2 + 1 , where c = 1 − s − e (the fraction of
correct answers from the oracle).
Proof. Let C be an oracle that represents a Hadamard code of a ﬁxed x with
an e-fraction of errors and s-fraction of erasures. In Fig. 1 the list decoding
algorithm LDC is deﬁned. First we prove its correctness in respect to the claim
made in Theorem 1 that it outputs x with at least probability one half. We then
analyze its time complexity.
Correctness of LDC : We start by proving the following claim about the
value of C (ei ⊕ sJ ) which is a principal component in the calculations made in
step 3 of our list decoder LDC .
Claim 1 Let sJ and C be deﬁned as in the description of LDC . Then for a
nonempty J ⊆ [l] and L0 = {i | i ∈ [l], b(si , x) = 1} the following equalities hold:
Pr[(−1)
Pr[(−1)
Pr[(−1)

J,L0

J,L0

J,L0

C (ei ⊕ sJ ) = 0] = s

C (ei ⊕ sJ ) = (−1)b(e
i

J

i
i

,x)

b(e ,x)

C (e ⊕ s ) = −(−1)

]=c
] = e,

where the probabilities are taken over the choices of si in step 1 of LDC .

Nearly One-Sided Tests and the Goldreich-Levin Predicate

201

Implementation of list-decoder LDC : Let LD have oracle access to C :
{0, 1}n → {0, 1, ⊥}. On input l and n, LDC proceeds as follows:
1. Choose s1 ,. . ., sl independently from Un .
2. Deﬁne a predictor C that uses C so that

 1 if C(r) = 0
C (r) = −1 if C(r) = 1

0 if C(r) = ⊥
J,L
3. Calculate diL =
C (ei ⊕ sJ ) for all L ⊆ [l] and i ∈ [n], where
J (−1)
def
sJ = ⊕i∈J si for all nonempty J ⊆ [l].
1−sgn(diL )
.
4. Output the list {z L }L⊆[l] where the i’th bit of z L is deﬁned as
2

Fig. 1. The List-Decoder LDC

Proof. We observe that
b(sJ , x) = ⊕j∈J b(sj , x) = ⊕j∈J∩L0 b(sj , x) = J, L0 ,
and


i
J
 (−1)b(e ,x)⊕b(s ,x) if C answers correctly
i
J
i
J
C (e ⊕ s ) = −(−1)b(e ,x)⊕b(s ,x) if C answers incorrectly

0
if C answers ⊥ .

As J is non-empty, the value of ei ⊕ sJ will be uniformly distributed and thus
the probability that C answers ’⊥’ is s, incorrectly is e and correctly is c. As
b(sJ , x) = J, L0 the claim follows from

i
 (−1)b(e ,x) if C answers correctly
J
i
(−1)b(s ,x) C (ei ⊕ sJ ) = −(−1)b(e ,x) if C answers incorrectly

0
if C answers ⊥ .

As a consequence of their construction, the values of sJ , for nonempty J ⊆
[l], are pairwise independent and uniformly distributed. Let L0 be deﬁned as
{i | i ∈ [l], b(si , x) = 1} and study for a ﬁxed i the value of diL0 calculated in
step 3 of LDC . It is a sum of expressions on the form that is analyzed in Claim
1. The probability of diﬀerent results (expressed in terms of the i’th bit of x) of
each term in this sum is speciﬁed in the claim. Using the value (sign) of the sum
we can thereby guess the i’th bit of x and by knowing the diﬀerent probabilities
we can calculate an upper bound for the probability that the guess is incorrect.
For our guess to be correct we would like to have more terms that equal
i
i
(−1)b(e ,x) than −(−1)b(e ,x) . As we know the probability for each outcome we
can, by using Chebyschev’s inequality, give an upper bound on the probability
that the guess is incorrect. For every non-empty J ⊆ [l], deﬁne ζcJ to be the

202

G. Hast
i

indicator variable for the event that (−1) J,L0 C (ei ⊕sJ ) = (−1)b(e ,x) and let ζeJ
i
be the indicator variable for the event that (−1) J,L0 C (ei ⊕ sJ ) = −(−1)b(e ,x) .
We would like to be able to state that J ζcJ > J ζeJ with high probability.
Chebyschev’s inequality states that for any positive t
Pr[|Y − µ| ≥ tσ] ≤ t−2 ,
where σ is the standard deviation and µ is the expectation of the variable Y .
This inequality is to be applied on the number of incorrect answers Y =
def l
J
1 is the number
J ζe which has the expected value of µ = N e (where N = 2 −√
N e (using the
of terms in the sum) and the standard deviation
σ
is
less
than
√
N (c−e)
fact of pairwise independency). We set t = 2√e which gives us
ζeJ − N e | >

Pr |
J

4e
N (c − e)
≤
.
2
N (c − e)2

Applying the same inequality
on the number of correct answers Y =
√
√
N (c−e)
µ = N c, σ < N c and t = 2√c gives
ζcJ − N c | >

Pr |
J

J

ζcJ with

4c
N (c − e)
≤
.
2
N (c − e)2

If none of the sums J ζcJ and J ζeJ deviate more than N (c−e)
from their
2
expected values we can conclude that the number of correct answers outnumbers
the number of incorrect answers. Thus, the probability that this is not the case
and we thereby are not able to make a correct prediction is at most N4(e+c)
(c−e)2 .
For the algorithm to succeed (in the supposed fashion) each of the n diﬀerent
bits of x has to be predicted correctly. In other words diL0 has to have the correct
4n(e+c)
sign for each i ∈ [n]. An upper bound for this not occurring is N
(c−e)2 which is
the sum of the upper bounds for each bit prediction failure. If N ≥ 8n(e+c)
(c−e)2 then
this bound is less than one half. As N equals 2l − 1 we conclude that if the input
l satisfy
8n(e + c)
+1
l ≥ log
(c − e)2
then the probability that x appears in the output list is at least one half.
Eﬃciency of LDC : The ﬁrst step of LDC takes time O(nl) and the last step
takes time O(n2l ). The time consuming step of the algorithm is the calculation
of the diﬀerent values of diL . The naive way to do this would be by calculating
each diL -value independently for each L. This would make the algorithm work
in time O(n2 22l ) making O(n2l ) calls to C, as there are n2l diﬀerent diL -values
and each value is a sum of 2l − 1 terms and each term can be calculated in time
O(n).

Nearly One-Sided Tests and the Goldreich-Levin Predicate

203

Using Fourier-analysis of functions g : 2[l] → R, the expression for diL ,
J,L
C (ei ⊕ sJ ), can be identiﬁed as the L’th Fourier coeﬃcient of the
J (−1)
function gi (J) = C (ei ⊕ sJ ). Using the fast Fourier-transform algorithm all 2l
Fourier coeﬃcients can be calculated in time O(l2l ). Therefore, for each i we can
calculate all the values {diL }L⊆[l] in time O(l2l ) using 2l oracle queries. There
are n diﬀerent values of i making the total time O(nl2l ) and the total number
of oracle queries n2l .

5

Goldreich-Levin Hard-Core Bit

In this section we make an eﬃcient reduction from inverting a function f to
predicting the Goldreich-Levin bit of f . The list decoding algorithm from the
previous section is the main component of the algorithm that performs the reduction.
Theorem 2. Let P be a probabilistic algorithm with running time tP : N → N,
and rate δP : N → [0, 1] and advantage εP : N → [0, 12 ] in predicting b(x, r)
from f (x) and r. Deﬁne h(n) to be log2 (δP (n)/εP (n)2 ). Then there exists an
algorithm Inv that runs in expected time (tP (n) + h(n) log2 (n)) · h(n) · O(n2 )
and satisﬁes
Pr [f (Inv(f (x), n)) = f (x)] = Ω

x∈Un

εP (n)2
δP (n)

.

Description of inverter Inv: P is a predictor with rate δP (n) and advantage
εP (n). On input y = f (x) and n Inv proceeds as follows:
1. Select j from {−1, . . . , h − 2}, where h = h(n) = log
2

δP (n)
εP (n)2

, with proba-

and set l = log(nδP (n)/εP (n) ) − j + 2. If no j is chosen, stop
bility 2
and output ⊥.
def
2. Call the list-decoder LDPy with input l and n, where Py (·) = P (y, ·).
3. Apply f on each element x of the output from the list-decoder. If f (x ) = y,
then output x and stop.
4. Output ⊥.
j−h

Fig. 2. The inverter Inv

Theorem 2 states that if there is an algorithm P that predicts the GoldreichLevin bit of f , then there also exists an algorithm Inv (depicted in Fig. 2) that
inverts f . If P , for all possible values of x, would have approximately the same
advantage (and rate) in predicting b(r, x) from f (x) and r, this can be shown by
directly applying the list-decoding algorithm LD with the appropriate value of
l. But as this is not true in general we need to make an averaging argument. This
is done by calling LD with small values on l with high probability (to cope with

204

G. Hast

values of x giving P a high advantage), and calling LD with big values on l with
low probability (to cope with values of x giving P an intermediate advantage).
A proof of the theorem can be found in the journal version of this work [14].
Compared to previous analyses the inverting probability improves with approximately a factor of δP−1 (see Proposition 2.5.4 in [8]). In some applications
we know that the advantage εP (n) will be a constant fraction of the rate δP (n)
and then the improvement will be considerable as the inverting probability will
increase from Ω(εP (n)2 ) to Ω(εP (n)).
2
Knowledge about the value of εδPP(n)
(n) is required if we would like to implement
Inv. Therefore we deﬁne a new algorithm Inv , with oracle access to a predictor
P , that takes an additional input h and behaves exactly as Inv but uses h instead
(n)
of log εδPP(n)
2 in the ﬁrst step of the algorithm. From the proof of Theorem 2 we
(n)
can conclude that as long as h ≥ log εδPP(n)
2 the probability of success will not
decrease and the running time and the number of queries to P will be the same
(n)
as in Theorem 2 except for that we substitute log εδPP(n)
2 with h. We thus have
the following corollary which is useful when proving Theorem 3.

Corollary 1. Let P be an arbitrary algorithm predicting b(x, r) from f (x) and
r. There exists an algorithm Inv with oracle access to P such that on input y,
n and h it runs in expected time h2 log2 (n) · O(n2 ) and makes h · O(n2 ) number
of expected calls and satisﬁes
Pr

x∈Un

P

f (Inv (f (x), n, h)) = f (x) = Ω 2−h

if h ≥ log2 (δP (n)/εP (n)2 ), where P has rate δP : N → [0, 1] and advantage
εP : N → [0, 12 ] in predicting b(x, r) from f (x) and r.

6

Nearly One-Sided Statistical Tests

A statistical test is a probabilistic algorithm that takes an input and outputs a
bit. The purpose of a statistical test is to distinguish between diﬀerent distributions. This is done by having diﬀerent output distributions when the input is
drawn from diﬀerent distributions. The output distribution is characterized by
the probability that the output is equal to 1 respectively 0. If the test rarely
outputs 1 we consider the test to be nearly one-sided with respect to the input
distribution. The classical way to measure how well a statistical test distinguishes
between two distributions (or in fact two ensembles of distributions) is through
the following deﬁnition.
Deﬁnition 3. An algorithm D : {0, 1}∗ → {0, 1} δ(n)-distinguishes the ensembles X = {Xn } and Y = {Yn } if for inﬁnitely many values of n
Pr [D(x) = 1] − Pr [D(y) = 1] ≥ δ(n) .

x∈Xn

y∈Yn

Nearly One-Sided Tests and the Goldreich-Levin Predicate

205

The characterization of a statistical test in terms of this deﬁnition is rather
coarse. In particular whether the test is one-sided or not does not show. This is
remedied by the following deﬁnition which explicitly provides absolute bounds
on the probability of outputting 1.
Deﬁnition 4. An algorithm D : {0, 1}∗ → {0, 1} (δ1 (n), δ2 (n))-distinguishes
the ensembles X = {Xn } and Y = {Yn } if for inﬁnitely many values of n
Pr[D(Xn ) = 1] ≤ δ1 (n) < δ2 (n) ≤ Pr[D(Yn ) = 1] .
In addition, D is said to be a (δ1 (n), δ2 (n))-distinguisher for the ensembles X
and Y if D (δ1 (n), δ2 (n))-distinguishes X and Y .
We now discuss the connection between statistical tests and predictors, and
in particular how a nearly one-sided statistical test for the Goldreich-Levin bit
easily can be turned into a low rate predictor that predicts the Goldreich-Levin
bit.
Assume that a distinguisher D satisﬁes
p1 =
and

p2 =

Pr

r,x∈Un ,σ∈U1

[D(f (x), r, σ) = 1]

Pr [D(f (x), r, b(r, x)) = 1] .

r,x∈Un

It is easy to transform this distinguisher into a predictor P guessing b(r, x) with
advantage p2 − p1 . On input (f (x), r) the predictor P samples a uniform bit
σ, queries for D(f (x), r, σ) and outputs σ iﬀ D(f (x), r, σ) = 1 and otherwise
outputs 1 − σ. However, if the probability p1 is very small, then the truly informative answers from D are when it returns 1. In those cases the probability
that the prediction is correct is
Prr,x∈Un [D(f (x), r, σ) = 1 | σ = b(r, x)] · Prσ∈U1 [σ = b(r, x)]
p2
=
,
Prr,x∈Un ,σ∈U1 [D(f (x), r, σ) = 1]
2p1
giving an advantage of

p2
p2 − p 1
1
− =
.
2p1
2
2p1
This should be compared to the success probability when D outputs 0
Prr,x∈Un [D(f (x), r, σ) = 0 | 1 − σ = b(r, x)] · Prσ∈U1 [1 − σ = b(r, x)]
Prr,x∈Un ,σ∈U1 [D(f (x), r, σ) = 0]
=

1
2

· Prr,x∈Un [D(f (x), r, 1 − b(r, x)) = 0]
1 − Prr,x∈Un ,σ∈U1 [D(f (x), r, σ) = 1]

· (1 − Prr,x∈Un [D(f (x), r, 1 − b(r, x)) = 1])
1 − p1
1 − (2 · Prr,x∈Un ,σ∈U1 [D(f (x), r, σ) = 1] − Prr,x∈Un [D(f (x), r, b(r, x)) = 1])
=
2(1 − p1 )
1 − (2p1 − p2 )
=
,
2(1 − p1 )
=

1
2

giving an advantage of

206

G. Hast

1 − (2p1 − p2 ) − (1 − p1 )
p2 − p1
1 − (2p1 − p2 ) 1
− =
=
.
2(1 − p1 )
2
2(1 − p1 )
2(1 − p1 )
(Note that we have implicitly extended the notion of advantage to also apply
when conditioned on the output of D.)
Assume that the test is nearly one-sided and thereby the value of p1 is close
to zero. In this case the advantage of the prediction when D outputs 1 is a factor
p−1
1 better than if it outputs 0. This is why it is better to somewhat change P
so that it still returns σ iﬀ D(f (x), r, σ) = 1 but otherwise outputs ⊥. This
new predictor has rate p1 and advantage (p2 − p1 )/2. Thus we have transformed
a nearly one-sided statistical test for the Goldreich-Levin bit into a low rate
predictor that predicts the Goldreich-Levin bit.

7

Blum-Micali Pseudo Random Generator

Blum and Micali [6] constructed a pseudo-random bit generator based on a oneway permutation and a hard-core predicate associated with the permutation. As
an example they used exponentiation modulo a prime as a one-way permutation
and the most signiﬁcant bit as its hard-core predicate. By using an arbitrary
one-way permutation and the hard-core predicate of Goldreich-Levin [11] the
following construction, referred to as BM GL, is obtained:
Construction 1 Let f : {0, 1}∗ → {0, 1}∗ be a polynomial-time-computable
length-preserving permutation and l : N → N where n < l(n). Given the input
x0 and r such that |x0 | = |r| = n, deﬁne
Gf,l (x0 , r) = b(r, x1 )b(r, x2 ) . . . b(r, xl(n) ) ,
where xi = f (xi−1 ) for i = 1,. . .,l(n). With Gf,l
n we denote the output distribution
of Gf,l (x0 , r), where r and x0 are chosen uniformly and independently from
{0, 1}n .
The security of the BM GL is described by our next theorem.
Theorem 3. Let D be a (p1 (n), p2 (n))-distinguisher for {Ul(n) } and {Gf,l
n } with
running time tD : N → N. Then there exists an algorithm Inv that runs in
expected time [(tf (n)·l(n)+tD (n))+m(n) log2 (n)]·m(n)·O(n2 ), where tf (n) is the
2
def
2 (n)l(n)
time to calculate the function f on a n-bit input and m(n) = log2 (p2p(n)−p
2,
1 (n))
and satisﬁes
Pr [Inv (f (x)) = x] = Ω 2−m(n)
x∈Un

for inﬁnitely many values of n.
For nearly one-sided tests the reduction is an improvement with approximately a factor of p2 (n)−1 over the previous most eﬃcient reduction. The improvement is obtained by transforming nearly one-sided tests to low rate predictors and applying Theorem 2 that works well for that type of predictors.

Nearly One-Sided Tests and the Goldreich-Levin Predicate

207

Implementation of predictor P i : The predictor has access to a distinguisher
D : {0, 1}l(n) → {0, 1}. On input y and r, P i proceeds as follows:
1. Set xi+1 = y and calculate xj for j ∈ {i + 2, . . . , l(n)} where xj = f (xj−1 ) and
n = |y|.
2. Toss i unbiased coins c1 , c2 , . . . , ci−1 and σ.
3. Create the bit string z = c1 . . . ci−1 σb(xi+1 , r) . . . b(xl(n) , r). If D(z) = 1 then
return σ, otherwise return ⊥.
Fig. 3. The predictor P i
Implementation of inverter Inv : The inverter has access to a distinguisher
D : {0, 1}l(n) → {0, 1}. On input y, Inv proceeds as follows:
1. Choose i ∈ [l(n)] uniformly, where n = |y|.
2. Choose j from {−2, −1, . . . , log l(n) } with probability 2−(3+j) .
(n)l(n)2
3. Call Inv from Corollary 1 with input y, n and h = log 22j (pp2 (n)−p
(n))2
providing P i as the predictor. Return the output from Inv

Pi

2

1

.

Fig. 4. The inverter Inv

Note that the running time of Inv only depends upon p1 (n) and p2 (n)
logarithmically. Assuming tf (n) · l(n) ≤ tD (n) and ignoring logarithmic factors
the time-success ratio is p2 (n)l(n)2 tD (n)(p2 (n) − p1 (n))−2 · O(n2 ). Furthermore,
if we assume that p1 (n) ≤ c · p2 (n) for some constant c < 1 the time-success
ratio is l(n)2 tD (n)p2 (n)−1 · O(n2 ), still ignoring logarithmic factors.
The correctness of the theorem is shown by deﬁning a number of hybrid
distributions Hni , connecting the uniform distribution Ul(n) with the generator
output Gf,l
n . In Fig. 3 a predictor is deﬁned for each such hybrid. The inverter
Inv uniformly selects between these predictors and calls Inv from Corollary
1 providing the selected predictor as an oracle. A proof of the theorem can be
found in the journal version of this work [14].

8

Applications

Using pseudo-random material instead of real random material in probabilistic
algorithms may be convenient for many reasons. A system can have problems
obtaining enough random material or the results perhaps need to be reproducible
without storing all random material used.
An important use of pseudo-random material can be found in many implementations of cryptographic primitives. The security deﬁnitions of these primitives often express either the need of authentication or that of conﬁdentiality.
In the case of authentication there is often a natural nearly one-sided test corresponding to an attacker of the protocol, but in the case of conﬁdentiality this
is not always so.

208

G. Hast

Here we consider the security consequences of using pseudo-random material
from a PRBG instead of true random bits in a cryptographic system. We assume
that the system that uses true randomness is secure and want to establish that
the corresponding system is also secure. The standard method for showing this is
done by assuming that there is a successful attacker, when using pseudo-random
material in the system, and transforming this attacker into a statistical test distinguishing between output from the PRBG and the uniform distribution. This
statistical test will become nearly one-sided if the successful attacker produces
a certain type of breakings which we call veriﬁable breakings. If the PRBG is
BMGL then the proof of security can prosper by our more eﬃcient reduction
established in Theorem 3. With a veriﬁable breaking of a cryptographic system
we mean that a successful breaking can be veriﬁed, possibly with the help of
secrets lying in the system such as secret keys.
8.1

Signature Schemes

As an example of how natural nearly one-sided tests come up in the case of
authentication, we take a closer look at signature schemes. Using the standard
deﬁnition of security in an adaptive chosen message attack environment [13], an
attacker may query an oracle for signatures of any messages of its choice. The
attacker is considered successful if it, on a veriﬁcation key as input, can output a
valid signature on any message. Furthermore, the signature should of course not
be identical to a signature returned by the oracle. If there is an attacker (from a
certain group of attackers) that has more than a negligible probability to break
the signature scheme, then the signature scheme is considered to be insecure
against that group of attackers. Now assume that S is a secure signature scheme
but S G is not, where the only diﬀerence between S G and S is that S G uses
bits from a pseudo-random generator G when S uses true random bits. Then
there is an adversary A that can break the signature scheme S G , but not S,
with a non-negligible probability. This adversary can easily be transformed into
a nearly one-sided statistical test D, distinguishing between the output of G and
the uniform distribution, by ﬁrst simulating S, using its input as the source of
randomness, and then attacking S using A.
8.2

Encryption Schemes

A natural way to deﬁne the security of an encryption system is to say that, given
a ciphertext, it should be infeasible to produce the corresponding plaintext. If we
would adopt this as our security deﬁnition, an attacker A could easily be turned
into a natural nearly one-sided test that would distinguish between encryptions
made by a secure encryption scheme and encryptions made by an encryption
scheme that A attacks successfully. In particular this means that if we exchange
the true random material that is used in a secure encryption scheme against
bits produced by a generator, then a successful attacker on the resulting scheme
can be turned into a nearly one-sided test distinguishing between the uniform
distribution and the generator output.

Nearly One-Sided Tests and the Goldreich-Levin Predicate

209

Unfortunately, this natural way of deﬁning security is not strict enough for
all situations. In general we do not permit an attacker to be able to conclude
anything at all about the plaintext, by looking at the ciphertext. This notion
was captured by the deﬁnition of semantic security in [13]. Using this deﬁnition
there is no natural way to transform an attacker into a nearly one-sided test. For
example, if an adversary could predict, by looking at a ciphertext, the i’th bit of
the plaintext (that was drawn from the uniform distribution) with probability
1
2 + ε, where ε is non-negligible, this would render the system insecure. But this
adversary corresponds with a predictor with rate one, and thus not to a nearly
one-sided test.

8.3

Other Applications

In many algorithms, pseudo-randomness is provided by a pseudo-random function. A well-known construction of pseudo-random functions is the GGM construction [9] which uses a pseudo-random generator as a building block. We note
that a nearly one-sided statistical test, distinguishing between the use of real
random functions and a family of GGM-functions, can be transformed into a
nearly one-sided statistical test distinguishing the uniform distribution and the
underlying generator output. Brieﬂy, this is so because the hybrid arguments
used in the security proof of GGM will uphold the one-sidedness in distinguishing between real random functions and GGM-functions.
For some applications in simulations, nearly one-sided tests will occur naturally. Consider a probabilistic algorithm for a decision problem that errs with
very small probability. Assume that this algorithm uses the output from a PRBG
as its source of randomness and that this causes the algorithm to err with substantially higher probability. Then this algorithm can be used to produce a nearly
one-sided test distinguishing the PRBG output and the uniform distribution.

9

Open Problems

In this work we have studied nearly one-sided test for the Goldreich-Levin hardcore bit. A natural extension would be to consider what impact nearly one-sided
tests have on other hard-core predicates. Additional applications where nearly
one-sided tests occur could also be investigated.

Acknowledgments. I am very grateful to Johan H˚
astad for providing me with
good ideas and continuous support. I am also grateful to the referee of Journal of
Cryptology for valuable advice about the presentation as well as for suggesting
interesting extensions. Finally, I would like to thank Yevgeniy Dodis, Mikael
Goldmann, Oded Goldreich, Jonas Holmerin, ˚
Asa Karsberg, Mats N¨
aslund and
the anonymous referees for much appreciated help and comments.

210

G. Hast

References
1. M. Adcock and R. Cleve: A Quantum Goldreich-Levin Theorem with Cryptographic
Applications. Proceedings, STACS 2002, LNCS 2285, 2002, pp. 323–334, SpringerVerlag.
2. M. Bellare: Practice-oriented provable-security. Proceedings, ISW ’97, LNCS 1396,
1997, pp. 221–231, Springer-Verlag.
3. M. Bellare and P. Rogaway: The exact security of digital signatures: How to sign
with RSA and Rabin. Proceedings, EUROCRYPT ’96, LNCS 1070, 1996, pp. 399–
416, Springer-Verlag.
4. L. Blum, M. Blum and M. Shub: A Simple Unpredictable Pseudo-Random Generator. SIAM Journal on Computing, 15, no. 2, 1986, pp. 364–383.
5. M. Blum and O. Goldreich: Towards a Computational Theory of Statistical Tests.
Proceedings, 33rd IEEE FOCS, 1992, pp. 406–416.
6. M. Blum and S. Micali: How to Generate Cryptographically Strong Sequences of
Pseudo-random Bits. SIAM Journal on Computing, 13, no. 4, 1984, pp. 850–864.
7. R. Fischlin and C. P. Schnorr: Stronger Security Proofs for RSA and Rabin Bits.
Journal of Cryptology, 13, no. 2, 2000, pp. 221–244.
8. O. Goldreich: Foundations of Cryptography: Basic Tools. Cambridge U. Press,
2001.
9. O. Goldreich, S. Goldwasser and S. Micali: How to Construct Random Functions.
JACM, 33. no. 4, 1986, pp. 792–807.
10. O. Goldreich, R. Impagliazzo, L. A. Levin, R. Venkatesan and D. Zuckerman:
Security Preserving Ampliﬁcation of Hardness. Proceedings, 31st IEEE FOCS,
1990, pp. 318–326.
11. O. Goldreich and L. A. Levin: A Hard Core Predicate for any One Way Function.
Proceedings, 21st ACM STOC, 1989, pp. 25–32.
12. O. Goldreich, R. Rubinfeld, and M. Sudan: Learning polynomials with queries:
The highly noisy case. SIAM Journal on Discrete Mathematics, 13, no. 4, 2000,
pp. 535–570.
13. S. Goldwasser, S. Micali and R. Rivest: A Digital Signature Scheme Secure Against
Adaptive Chosen-Message Attacks. SIAM Journal on Computing, 17, no. 2, 1988,
pp. 281–308.
14. G. Hast: Nearly One-Sided Tests and the Goldreich-Levin Predicate. Journal of
Cryptology, to appear.
15. J. H˚
astad and M. N¨
aslund: Practical Construction and Analysis of PseudoRandomness Primitives. Proceedings, ASIACRYPT 2001, LNCS 2248, 2001,
pp. 442–459, Springer-Verlag.
16. A. Herzberg and M. Luby: Public Randomness in Cryptography. Proceedings,
CRYPTO ’92, LNCS 0740, 1992, pp. 421–432, Springer-Verlag.
17. L. A. Levin: Randomness and Non-determinism. Journal of Symbolic Logic, 58,
no. 3, 1993, pp. 1102–1103.
18. M. Sudan, L. Trevisan and S. Vadhan: Pseudorandom generators without the XOR
Lemma. Journal of Computer and System Sciences, 62, no. 2, 2001, pp. 236–266.
19. A. C. Yao: Theory and application of trapdoor functions. Proceedings, 23rd IEEE
FOCS, 1982, pp. 80–91.

