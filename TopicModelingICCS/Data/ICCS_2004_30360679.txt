Eﬃcient Algorithm for Linear Pattern
Separation
Claude Tadonki1 and Jean-Philippe Vial2
1

2

University of Geneva, Centre Universitaire d’Informatique
24, rue G´en´eral Dufour, 1211 Gen`eve 4 - Switzerland
claude.tadonki@cui.unige.ch
HEC/LOGILAB, 40 Bd du Pont d’Arve, CH-1211 Geneva – Switzerland
jean-philippe.vial@hec.unige.ch

Abstract. We propose cutting plane algorithm for solving the linear
pattern separation problem, which is a particular case of the more general topic of data mining. The solution we provided, based on convex programming, can also be applied to any other pattern separation scheme
based on a convex discriminant like linear piecewise or quadratic models. Some experimentations are reported with diﬀerent large databases
together with a comparison with a direct implemention with two commercial specialized codes.

1

Introduction

Linear separation [1,5] is an important concept in data mining [3,6]. It is widely
used and has been applied in many ﬁelds, e.g., cancer diagnosis, human genome
, game strategies, pattern recognition , decision/selection making, and others.
Many other separation rules can be found in the litterature, and our method
can handle those of them that are based on a functional rule expressed by a
convex form. In some cases the size of the data set [2] is so large that solving the
mathematical programming problem becomes a challenge even with the stateof-the-art optimization software. In this paper we propose to resort a so-called
cutting plane method to solve the problem eﬃciently, and we discuss ways to
improve performance on the linear separation problem.

2

Problem Formulation

Given a set of points A = {ai ∈ Rn , i = 1, 2, · · · , N }, and a partition S1 ∪ S2 of
the set of indices S = {1, 2, · · · , N }, we wish to ﬁnd w ∈ Rn and γ ∈ R such
that the hyperplane {x | wT x = γ} separates the two subsets A(S1 ) and A(S2 ),
where
A(S1 ) = {ai ∈ A | i ∈ S1 },
A(S2 ) = {ai ∈ A | i ∈ S2 }.
M. Bubak et al. (Eds.): ICCS 2004, LNCS 3036, pp. 679–682, 2004.
c Springer-Verlag Berlin Heidelberg 2004

(1)
(2)

680

C. Tadonki and J.-P. Vial

For typographical convenience, we will write (w, γ) instead of (wT , γ).
Actually, one looks for a strong separation. Thus, given a separation margin
ν > 0, we hope to achieve the separation properties (3-4) displayed bellow
∀ai ∈ A(S1 ) wT ai ≥ γ + ν,
∀ai ∈ A(S2 ) wT ai ≤ γ − ν.

(3)
(4)

In general, there is no guarantee that the two sets can be strongly separated.
Therefore, for any choice of w and γ, we might observe misclassiﬁcation errors,
which we deﬁne as follows
max(−wT ai + γ + ν, 0)
, i ∈ S1 ,
||(w, γ, ν)||
max(wT ai − γ + ν, 0)
e2i =
, i ∈ S2 .
||(w, γ, ν)||

e1i =

(5)
(6)

Our goal is then to build a separation hyperplane {x | wT x = γ} (i.e., compute w and γ) for which the total sum of misclassiﬁcation errors is minimal. In
other words, we want to ﬁnd a vector w and a scalar γ such that the average
sum of misclassiﬁcations errors is minimized [5].
The separation margin ν helps avoiding the useless trivial solution (w, γ) =
(0, 0). Its value is usually set to 1. In some cases the separation margin may lead
to large values for w and γ. It may be necessary [2] to bound w to avoid this
undesirable feature; so, we add the constraint ||w||2 ≤ k.
Formally, we have to solve the following optimization problem
min

(w,γ)∈Rn ×R

1
|S1 |

max(−wT ai + γ + ν, 0) +
i∈S1

1
|S2 |

max(wT ai − γ + ν, 0)

(7)

i∈S2

subject to: ||w||2 ≤ k.

(8)

In accpm the square normed in the constraint ||w||2 ≤ k 2 is also treated as
black box. If w
¯ is not feasible (||w||
¯ 2 > k 2 ), the constraint
w + 2 ¯,w − w
¯ ≤ k2

(9)

holds for any feasible point.
Finally, let us give two bounds on f . Since f (0, 0) = 2ν, then 2ν is an upper
bound of the optimal value of the objective. A straightforward lower bound is 0,
but this can be only attained if perfect classiﬁcation is achieved.
Let us discuss now the formulation of problem (7)–(8) as a standard mathematical programming problem. Let zi , i ∈ S. be an auxiliary variable. The
original problem becomes
min

(w,γ)∈Rn ×R
z≥0

1
|S1 |

zi +
i∈S1

1
|S2 |

zi
i∈S2

(10)

Eﬃcient Algorithm for Linear Pattern Separation

subject to: zi ≥ (−wT ai + γ + ν), i ∈ S1
zi ≥ (wT ai − γ + ν), i ∈ S2
||w||2 ≤ k.

681

(11)
(12)
(13)

Note that the constraints (11)–(12) are numerous but linear. The problem
is thus a large linear programming problem with one quadratic constraint (13).
Some authors [2] prefer to replace the quadratic constraint by a quadratic penalty
term in the objective. Another possibility consists in replacing the Euclidean
norm in (13) by the ∞ norm [4]. The problem becomes then fully linear.
Table 1. Comparison with direct methods
n
10
20
30
40
50
60
70
80
90
100
10
20
30
40
50
60
70
80
90
100

m mosek(1) cplex(1) mosek(2) cplex(2) accpm
10000
2.54
1.95
6.31
1.81
1.95
10000
2.21
2.47
11.37
2.35
2.63
10000
4.45
4.45
18.70
4.12
3.40
10000
6.34
6.11
23.78
5.99
4.50
10000
9.23
8.18
26.11
8.20
4.95
10000
11.84
11.10
30.78
11.10
6.30
10000
15.13
13.07
40.87
12.82
8.86
10000
19.87
15.00
50.21
14.21 10.16
10000
26.04
19.97
69.20
19.46 15.03
10000
30.22
22.19
62.63
21.29 16.81
100000
143.86
81.08
113.40
78.12
5.11
100000
120.47
109.53
132.25
108.29
8.22
100000
172.21
143.98
179.24
141.31 10.59
100000
253.38
194.34
215.89
190.40 16.31
100000
311.35
223.71
280.87
219.60 16.95
100000
576.77
273.46
303.09
285.38 18.97
100000
742.84
408.01
411.66
396.50 28.88
100000
850.15
427.15
478.61
406.19 31.25
100000
906.57
496.57
590.95
439.29 34.06
100000
1443.25
543.25
680.81
493.78 40.30
(1) simplex

3

(2) interior point

Implementation

In our experiments we have considered various sample data sets to test our
algorithm, all generated radonmly using a normal distribution.
The problem were solved using the public release of accpm. All our experiments were conducted on a 500 Mhz SUN Ultrasparc with 256 MB of RAM.
For problems with a very large data set, we performed an out-of-core computation. With a buﬀer of 200, 000 elements, ﬁve accesses to disk per iteration were
required.

682

C. Tadonki and J.-P. Vial

We compare our method with direct methods based on a standard linear programming (LP) formulation of the problem. Recall that the linear programming
formulation was given in (10)–(12). (Note that we drooped the norm constraint
(13).) The (dual) LP formulation can be solved using standard techniques of linear programming such as simplex or interior point methods. We have compared
accpm with two linear programming codes: mosek and cplex. Both oﬀer the
options between a simplex and a primal-dual log barrier algorithm. Table 1 displays our experimental results.

4

Conclusion

We have proposed a cutting plane algorithm for the linear pattern separation
problem. The main idea was to modeling the problem through the purpose of
minimizing the total misclassiﬁcation gap as a convex function. Experimentations and comparative study show that our method is quiet eﬃcient even when
consider very large databases.
Acknowledgement. The authors thank Olivier P´eton and Cesar Beltran for
their useful comments.

References
1. R.A. Bosh and J.A. Smith, Separating Hyperplanes and the Authorship of the Disputed Federalist Papers, American Mathematical Monthly, Volume 105, No 7, pp.
601-608, 1995.
2. M.C. Ferris and T.S. Munson, Interior Point Methods for Massive Support Vector
Machines, Cours/S`eminaire du 3e cycle romand de recherche op`erationnelle, Zinal,
Switzerland, march 2001.
3. J. Han and M. Kamber, Data Mining: Concept and Techniques, Morgan Kaufmann
Publishers, 2000.
4. O.L. Mangasarian, Linear and Non-linear Separation of Patterns by linear programming, Operations Research, 13, pp. 444-452.
5. O.L. Mangasarian, R. Setino, and W. Wolberg, Pattern Recognition via linear programming: Theory and Applications to Medical Diagnosis, 1990.
6. M.S. Viveros, J.P. Nearhos, M.J. Rothman, Applying Data Mining Techniques to a
Health Insurance Information System, 22nd VLDB Conference, Mumbai (Bombay),
India, 1996, pp. 286-294.

