Procedia Computer Science
Volume 29, 2014, Pages 1480–1490
ICCS 2014. 14th International Conference on Computational Science

A High Level Programming Environment for
Accelerator-based Systems
Luiz DeRose, Heidi Poxon, James Beyer, and Alistair Hart
Cray Inc.
ldr@cray.com, heidi@cray.com, beyerj@cray.com, ahart@cray.com

Abstract
Some of the critical hurdles for the widespread adoption of accelerators in h igh performance
computing are portability and programming difficulty. To be an effective HPC p latform, these systems
need a high level software development environment to facilitate the porting and development of
applications, so they can be portable and run efficiently on either accelerators or CPUs. In this paper
we present a high level parallel programming environ ment for accelerator-based systems, wh ich
consists of tightly coupled co mpilers, tools, and libraries that can interoperate and hide the co mp lexity
of the system. Ease of use is possible with compilers making it feasible for users to write applications
in Fo rtran, C, or C++ with OpenACC directives, tools to help users port, debug, and optimize for both
accelerators and conventional multi-core CPUs, and with auto-tuned scientific libraries.
Keywords: Programming environment, Hybrid system, accelerators, GPUs, OpenACC

1 Introduction
The current trend in the supercomputing industry is to provide hybrid systems with accelerators
attached to multi-core processors. However, portability and programmab ility are critical hurdles for
the widespread adoption of accelerated computing in high performance computing. The dominant
programming frameworks for accelerator based systems today are CUDA and OpenCL. They offer the
power to ext ract performance fro m accelerators, but with ext reme costs in usability, maintenance,
development, and portability. To facilitate the migration of applications to hybrid systems with
accelerators attached to CPUs, users need a hybrid programming framework that is simp le and
portable across machine types. It is important that this programming framework allows users to
maintain a single code base. In addition, the required optimization techniques should not be
significantly d ifferent for “accelerated” nodes from the approaches used on current mult i-core x86
processors. The OpenACC application program interface (OpenACC 2011) was created to address
these issues. OpenACC is an open standard that started as a joint init iative between CAPS, Cray,
NVIDIA, and PGI, and is now supported by more than 15 academic and industrial partners.

1480

Selection and peer-review under responsibility of the Scientiﬁc Programme Committee of ICCS 2014
c The Authors. Published by Elsevier B.V.
doi:10.1016/j.procs.2014.05.134

A High Level Programming Environment for Accelerator-based Systems

L. Derose et al.

While programming interfaces like OpenACC are a crucial ingredient, a co mp lete high -level
software development environ ment is needed to make hybrid systems effect ive HPC plat forms. Th is
facilitates the porting and development of applications to run efficiently on either accelerators or
CPUs. In this paper we present the Cray high level programming environment fo r accelerated
computing, which tightly integrates compilers, tools, and scientific libraries. Co mpilers allow users to
write applications in Fort ran, C, or C++ with OpenACC d irectives; the tools help users port, debug,
and optimize fo r accelerators as well as conventional mult i-core CPUs; and auto-tuned adaptive
scientific lib raries provide optimized and accelerated performance while maintain ing th e standard
APIs. The remainder of this paper is organized as follows: In Section 2 we briefly describe the
OpenACC application program interface. In Section 3 we present the Cray programming environment
for accelerated co mputing, focusing on the programmab ility features in the co mpiler, tools, and
lib raries. Section 4 presents performance results using a large scale application. Finally, we present
our conclusions in Section 5.

2 OpenACC Overview
There is currently a wide variety of programming frameworks that support accelerator-based
systems, with CUDA and OpenCL dominating. Both target the hardware at a low level and offer the
power to extract high performance fro m the accelerator. However, the resulting codes are difficult to
write and extend, and are also hard to port to other HPC p latforms. Th is usually results in the
developers having to maintain two distinct versions of the code, one targeted for the homogeneous
system, and an accelerator specific version. In part icular, in the case of CUDA this means t he code
will only generally execute on an NVIDIA GPU. An OpenCL code is more portable in theory, but its
low-level often requires major program modifications when porting.
With the OpenACC application programming interface (API) the orig inal Fortran, C or C++
application is annotated with d irect ives or prag mas and, optionally, with calls to a runtime API that
direct the comp iler to generate kernels that execute on the attached accelerator(s). Support fro m
mu ltip le co mpiler vendors offers portability of applications between systems, reassuring continued
ongoing support and development for the programming interface and enhanced opportunities for
debugging and performance co mparison. By the end of 2012, co mpilers fro m Cray (Cray Inc. 2013),
PGI (Wolfe 2012), and CAPS (CAPS 2012) were already offering full support for the OpenACC 1.0
standard. By the end of 2013, Cray was already offering co mpilers supporting OpenACC 2.0, while
GNU and PGI were reportedly working on support for this new version of the standard.
OpenACC co mpilers include a runtime co mponent that implicit ly manages the accelerator
memo ry, insulating the user fro m much of the co mplexity found in other accelerator programming
frameworks. However, sometimes it can be useful to know where data is held in the device memo ry.
The OpenACC host data directive is the mechanism that allows subprogram calls within host data
region to pass pointers in device memo ry rather than in host memo ry. Th is is particu larly useful for
the seamless interaction with scientific libraries, as described in Section 3.3. In addition, the host data
directive allo ws interoperability with CUDA. This g ives users the flexibility to construct hand -tuned
CUDA kernels for finer control and improved performance in key areas of their application,
processing data already held on the device. Since this paper focuses on a high level parallel
programming environ ment for accelerator-based systems and not the OpenACC standard specifically,
readers can refer to the OpenACC page (OpenACC 2011) for a complete description of the interface.

1481

A High Level Programming Environment for Accelerator-based Systems

L. Derose et al.

3 Cray Programming Environment for Accelerated Computing
The Cray high level programming environ ment for accelerated computing consists of tightly
coupled compilers, libraries, and tools that interoperate and hide the complexity of the system. The
compiler does the “heavy lift ing” to split off the work destined for the accelerator and performs the
necessary data transfers. In addit ion, it does optimizations to take advantage of the accelerator and the
mu lti-core x86 hardware appropriately. Full debuggers with integrated support for the host and the
accelerator are availab le with DDT (Allinea 2011) and TotalView (Gottbrath 2012). The Cray
Performance Tools (DeRose, et al. 2008) provide profiling information for the whole application,
which can be grouped by events, such as copies, kernels, etc., or mapped back to the source code by
line nu mber. A single performance report can include statistics for both the host and the accelerator,
including hardware performance counters information, gathered with PAPI (PAPI team 2011).
The Cray Scientific Libraries use the Cray auto-tuning framewo rk to select the best kernel for each
task. With this scientific libraries interface, data copy and the accelerator or host execution placement
are automatic.

3.1 Cray Compiler for Accelerated Computing
The Cray co mpiler was extended to support OpenACC with the goal of simp lifying the co mplexity
of hybrid code development. The user identifies regions of code via directives to be translated by the
compiler for execution on the accelerator. The user has two mechanisms available for identifying
“kernels” for the compiler; the parallel and kernels directives. The parallel direct ive is prescriptive
and the compiler is given very little responsibility for discovering wor ksharing: all loops that the
programmer wants workshared must have a loop directive on them. The kernels direct ive is more
descriptive requiring the co mpiler to identify all of worksharing loops as well as all of the resulting
accelerator kernels. The loop construct is allowed in the kernels construct so the programmer can
guide the compiler when necessary.
The Cray co mpiler utilizes both vector and parallel analysis systems to identify loops for the
kernels construct, and vector analysis to ensure that vectorization is chosen if possible for loop nests
within parallel constructs. Once the kernels have been identified the co mpiler splits the code into
separate compilation units, the kernels become accelerator routines and are replaced with launching
code in the host code stream. The co mpiler translates the identified work share loops using MIMD
and SIMD style parallelism, performs the data movements by allocating and freeing device memo ry as
well as moving data to and from the device at the start and en d of accelerated regions. The compiler
will explicitly use the accelerator shared memory for reused data when directed by the programmer.
. . .
343. G
345. G
346. G
347. G
348. G
349. G
350. G
351. G
352. G
. . .
369. G
370. G

2
2
2
2
2
2
2
2
2

G--------< !$acc parallel num_workers(2)
G !$acc loop private(i,j,ij)
G g------< DO j = 2,jmax-1
G g g----< DO i = 2,imax-1
G g g ij = (j-2)*(imax-2)+i-1
G g g sendbuffz1(ij) = wrk2(i,j,2)
G g g sendbuffz2(ij) = wrk2(i,j,kmax-1)
G g g----> ENDDO
G g------> ENDDO

2 G--------> !$acc end parallel
2 !$acc update

Figure 1: Compiler feedback with accelerator related information

1482

A High Level Programming Environment for Accelerator-based Systems

L. Derose et al.

Co mpiler feedback is an ext remely important tool for application porting and tuning. Users need to
know the optimizations that were done, and mo re importantly , the optimizat ions that were inhibited
due to particular code constructions. We extended the Cray compiler to provide extensive accelerator
related feedback. Figure 1shows an example of the compiler listing with accelerator information for a
data region of the Himeno benchmark (Riken 2001), a 3D Po isson equation solver. For each loop in
the code the compiler provides an annotated listing with optimization and parallelization in formation
according to the key shown in Figure 2.
Primary Loop Type
------- ---- ---A - Pattern matched
C
D
E
G
I
M

-

Collapsed
Deleted
Cloned
Accelerated
Inlined
Multithreaded

V - Vectorized

Modifiers
--------a - atomic memory operation
b - blocked
c - conditional and/or computed
f
g
i
m
n
p
r
s
w

–
–
–
-

fused
partitioned
interchanged
partitioned
non-blocking remote transfer
partial
unrolled
shortloop
unwound

Figure 2: Cray compiler listing legend

In addition, as shown in Figure 3, the listing presents positive and negative compiler messages
(e.g., ftn-6263), as well as information on data movement (e.g., ftn-6415), which is crit ical for
performance tuning on hybrid systems. This lets the user understand how the compiler will move data,
how it schedule iterat ions between accelerator threads, whether it will parallelize loop nests, etc. Users
can get additional informat ion about specific co mpiler messages using the explain utility, which is
depicted in Figure 4. The Cray Reveal tool (described belo w) presents an integrated, graph ical v iew of
this compiler information.
ftn-6413 ftn: ACCEL File = himeno_caf_acc.f08, Line = 292
A data region was created at line 292 and ending at line 485.
ftn-6263 ftn: VECTOR File = himeno_caf_acc.f08, Line = 306
A loop starting at line 306 was not vectorized because it
contains a reference to a non -vector intrinsic on line 413.
ftn-6415 ftn: ACCEL File = himeno_caf_acc.f08, Line = 310
Allocate memory and copy variable "wgosa" to accelerator,
copy back at line 338 (acc_copy).
ftn-6405 ftn: ACCEL File = himeno_caf_acc.f08, Line = 343
A region starting at line 343 and ending at line 369 was placed
on the accelerator.
ftn-6430 ftn: ACCEL File = himeno_caf_acc.f08, Line = 346
A loop starting at line 346 was partitioned across the thread
blocks.

Figure 3: Example of accelerator related compiler messages

1483

A High Level Programming Environment for Accelerator-based Systems

L. Derose et al.

> explain ftn-6415
ACCEL: Allocate memory and copy %s to accelerator, copy back at line
%s (acc_copy).
The compiler generated code to allocate memory on the accelerator for
the specified data at the starting line. The Accelerator data is
initialized from the host data. At the ending line, the host data is
updated from the accelerator and the accelerator memory is freed.

Figure 4: Explain utility in the Cray programming environment

3.2 The Cray Performance Tools for Hybrid Systems
To port applications to run efficiently on any system, applicat ion developers need a good set of
performance tools to understand the behavior of the application on the system. There are a variety of
performance tools that support accelerated computing, with the NVIDIA Visual Pro filer (NVIDIA
2008), the TAU Performance System (Malony, et al. 2011), and the Vampir Trace (Dietrich, Ilsche
and Juckeland 2010) being the most pro minent ones. While all of these support OpenACC to some
extent, none are tightly integrated with the compilation environment, which limits their usefulness.
The main design goal of the Cray performance tools for accelerated co mputing is to provide the
user with an integrated performance measurement and analysis toolset that would view the hybrid
system (host and accelerator) as a single entity, presenting the application performance data fro m both
hardware co mponents together. Figure 5 shows the Cray Apprentice2 performance overv iew for a
hybrid code. The overview display is similar to the overview that is available for a run on a
homogenous system, with the addition of accelerator specific informat ion. The Function/Region
Profile h ighlights the top three time consuming functions in the program, which could be on the host
or on the accelerator. The Data Movement section provides the total amount of bytes “copied in” and
“copied out” between the host and accelerator. The Profile pane has an a dditional bar for the
accelerator. The “CPU” bar represents the overall wall clock execution t ime for the program, and also
provides a breakdown of the percentage of time spent in computation, co mmunicat ion, and I/O. The
added “GPU” bar provides a time relat ionship between the host and the accelerator, giving the user a
feel for how much of the overall execution time was associated with the accelerator. A short GPU bar
compared to the CPU bar indicates that the accelerator was often idle, while a tall GPU b ar indicates
that most of the execution time was associated with the accelerator. In addition, the GPU bar provides
a breakdown of the time for the GPU, so the user can have a high -level view of the time spent
executing kernels on the accelerator or mov ing data between the host and accelerator. The user can
dive into any of the overview panes to obtain additional in formation such as host or accelerator t imes,
number of occurrences of events, and hardware counter information.
Cray Apprentice2 can also display a GPU t imeline view that correlates accelerator and host
activities, as shown in Figure 6. For a given t ime slice it shows a h istogram of the accelerator activity
by wait, copy, or kernel time, the host callstack relat ive to the accelerator stream act ivity, and the
host/accelerator execution overlap.
In addition, to assist users with code optimizat ion, the Cray compiler and the Cray performance
tools were extended to provide loop level profiling informat ion, and to correlate source code with
analysis to help identify key candidate areas for optimizat ion. The co mpiler provides static analysis
informat ion and instrumentation hooks at the loop level, which are used by th e CrayPat co mponent of
the performance tools to instrument and collect the information. Th is co mbination of static and
runtime in formation is presented in the Reveal graphical user interface, shown in Figure 7. The right
panes have a source code browser with annotated compiler info rmation and co mpiler messages,

1484

A High Level Programming Environment for Accelerator-based Systems

L. Derose et al.

described in Section 3.1. The left pane displays performance information fro m all loops in the code.
The loops can be sorted by time, so the user can quickly identify loops that are best candidates for
parallelization and correlate with the source code on the right.

3.3 Scientific Libraries Support
Extensive work has been done by the scientific libraries co mmunity to support high performance
computing with accelerators, both in industry and academia. So me notable examples are Mag ma
(Agullo, et al. 2009), the NVIDIA CUDA Libraries (Barrachina, et al. 2008), and CULA (EM
Photonics 2010). The main drawback of these libraries is that in general they require a specific API for
the accelerator; therefore compromising portability.

Figure 5: Cray Apprentice2 overview for hybrid systems

1485

A High Level Programming Environment for Accelerator-based Systems

Figure 6: Cray Apprentice2 time line view

Figure 7: Visualization of integrated static and runtime information using Cray Reveal

1486

L. Derose et al.

A High Level Programming Environment for Accelerator-based Systems

L. Derose et al.

One of the design goals of libsci_acc, the Cray scientific libraries fo r accelerators, was to provide
extreme portability, in particu lar, between ho mogeneous and hybrid systems. It is crit ical for
application developers to be able to maintain a single code base that could be used effectively on
hybrid systems, as well as in homogeneous systems. The approach used in libsci_acc was to provide
two sets of interfaces: the simple interface and the expert interface, which can be used in difference
scenarios with minimized code modificat ions. The simp le interface requires minimu m o r no code
modification and can take advantage of the accelerator for performance imp rovement. The expert
interface enable users to conduct fine grain code optimization to maximize performance. We find,
however, that the expert interface is often not necessary to get the best performance, with the simp le
interface giving similar speed-ups.
The simple interface uses Cray’s adaptation and auto-tuning technology. It contains the full set of
BLAS and the major LAPA CK routines for accelerator computing, with the s ame API as the original
versions of these libraries. It automat ically selects where to run the library (CPU, Accelerator, or
Hybrid), depending on the problem, data location, and problem size. Figure 8 illustrates the adaptation
in the simple interface. The library first verifies if the data is in host memo ry or in device memo ry. If it
is in device memo ry the operation is executed on the accelerator. Otherwise, the lib rary will check the
problem size and will decide, based on auto-tuning analysis, if the operation should be executed as
hybrid (host + accelerator), accelerator only, or host only.
Figure 9 displays libsci_acc’s performance results for matrix mu ltip licat ion (DGEMM ) running on
a Cray XK7, a hybrid system with AMD Interlagos CPUs and NVIDIA Kepler (K20) GPUs. The
times collected on this examp le include the overhead fo r data transfer of the necessary data to or fro m
the CPU. We observe that for most matrix sizes the hybrid DGEMM outperforms the accelerator-only
version and that in several cases the hybrid code adds close to 100 GFlo ps to the total performance
(corresponding to the performance of the CPU only code, shown on the libsci curve). We also observe
that the hybrid approach does not appear to pay off for matrix sizes (M=N=K) s maller than 1792. Th is
is because the computation is not large enough to justify the additional co mmunicat ion overhead in the
hybrid approach. For these sizes the lib rary was later modified to call the accelerator version only. Out
of the reminding 26 data points, there were four cases, with mediu m sized matrices, where the
accelerator version slightly outperformed the hybrid approach. We are analy zing these cases to fine
tune the hybrid approach.

Figure 8: Adaptation in the libsci_acc simple interface

1487

A High Level Programming Environment for Accelerator-based Systems

L. Derose et al.

Figure 9: Performance of the libsci acc DGEMM on the Cray XK7

4 Experimental Results
One of the main design goals of the Cray programming environ ment for accelerated co mputing
was ease of use. Therefore, we do not claim that it will outperfo rm hand-coded CUDA code. We
believe that users would be satisfied with the productivity enhancement even with a s mall performance
gap. If needed, users can hand-tune critical kernels with the CUDA interoperability mentioned in
Section 2. Our target is to generate hybrid applications that can achieve at least 80% of the
performance of a hand-coded CUDA version of the same algorith m. To date, we have seen that is
achievable, as we show next with a co mpute-intensive section of the GAM ESS quantum chemistry
package (Gordon 1995).
GAM ESS is a computational chemistry package suite developed and maintained by the Gordon
Group at Iowa State University. We demonstrate the performance of the Cray programming
environment fo r accelerators by comparing with the best known hand-crafted CUDA code using the
same algorith m, developed by ISU and NVIDIA. Under this project, the set of kernels called
CCSD(T), a method to calculate electronic correlat ion energy in water clusters, was isolated from the
larger GAM ESS application and ported to run on accelerators. This set consists of the IJK-tuples, the
IJJ-tuples, and the IIJ-tuples. For our co mparison we used the IJK-tuples kernel, which contains
iterations of commun ication, fo llo wed by various complex array transformations and matrix-matrix
mu ltip lies. Much of the data can be copied to the accelerator and left resident for all iterations. Other
significant data can be calculated directly on the accelerator thereby saving PCIe bandwidth.
The IJK-tuples kernel has appro ximately 1700 lines of Fortran code, split into multip le subroutines
in a nu mber of separate source files. The kernel is initialized using data from an actual GAM ESS
execution. The hand coded CUDA version required appro ximately 1800 lines of new code. In contrast,
the OpenACC imp lementation was developed using the original Fortran source by adding 75
OpenACC directives and doing some minor loop restructuring. The streaming of data transfers used in
the OpenACC version was also similar to the CUDA version. Most of the data is moved to the
accelerator before the iteration loop whose trip-count depends on the number of ranks. Within the

1488

A High Level Programming Environment for Accelerator-based Systems

L. Derose et al.

loop, three arrays are exchanged with other ranks, moved to the accelerator, and remain device
resident for the remainder of the iteration.
Table 1 presents the time in seconds of the IJK-Tuples kernel written in CUDA and OpenACC,
running on 16 nodes of a Cray XK6 system, a hybrid system with AMD Interlagos 2.1 GHz CPUs and
NVIDIA Fermi (X2090) GPUs. It also shows the performance of the original code running on 16
nodes of a Cray XE6 system, which makes a fair co mparison between the homogenous system and the
hybrid system. We observe that the difference in performance between the CUDA version and the
OpenACC version is only 3%, which is much better than our target of not more than 20% performance
degradation. Both the CUDA and OpenACC ports were done before the availability of the NVIDIA
Kepler GPUs. W ith the release of NVIDIA Kep ler GPU late in 2012, we reco mpiled and rerun both
versions of the code on a Cray XK7, without modifying any of the source codes. The results are also
shown in Table 1; the OpenACC version was 12.5% faster than the CUDA version! Although
surprising, this can be explained by the changes in both the architecture (specifically the memory
subsystem) between Fermi and Kep ler, and the CUDA Toolkit, fro m 4.1 to 5.0. This indicates that a
CUDA code tuned for one micro-architecture, such as Fermi, might need to be re -tuned for a different
micro -architecture, while a high level code may only need recompiling to exp loit the tu ning of the
compiler and the libraries. We also ported the IJJ-tuples and the IIJ-tuples kernels to OpenACC.
However, no corresponding CUDA versions existed at the time, and with the observed performance
results, the GAMESS developers opted to complete the port to accelerators with OpenACC.

Cray XE6
CPU
16 ranks per
node
311 seconds

Cray XK6 (IL + Fermi GPUs)

Cray XK7 (IL + Kepler GPUs)

CUDA

OpenACC

CUDA

OpenACC

1 rank per node

1 rank per node

1 rank per node

1 rank per node

134 seconds

138 seconds

76.6 seconds

68.1 seconds

Table 1 : Time of the original version of the IJK-Tuples kernel running on 16 nodes of a Cray XE6
system and the CUDA and OpenACC versions running on 16 nodes of a Cray XK6 and XK7 systems.

5 Conclusions
In this paper we presented the Cray high level programming environ ment wh ich aims to make
accelerated supercomputers a productive platform for HPC. Cray comp iler support for OpenACC
directives allows users to write hybrid applicat ions in Fort ran, C, and C+ +. The co mp iler optimization
then takes advantage of accelerator and mult i-core x86 hardware appropriately. Performance
measurement and analysis tools provide a single view of the system with statistics for the whole
application. Auto-tuned adaptive libraries allow users to extract maximu m performance fro m the
hybrid system, whilst still using the same standard API. Our experiment results show that, with th is
programming environ ment, users can productively use high -level programming frameworks like
OpenACC and reasonably expect to exceed 80% of the performance of low-level, hand tuned codes
for the same algorithm.

1489

A High Level Programming Environment for Accelerator-based Systems

L. Derose et al.

References
Agullo, Emmanuel, Jim Demmel, Jack Dongarra, Bilel Hadri, Jakub Kurzak, and Julien Langou.
"Numerical linear algebra on emerging." Journal of Physics: Conference Series, 2009:
180(1).
Allinea. November 2011. http://www.allinea.com/products/ddt.
Barrachina, S., M. Castillo, F. D. Igual, R. Mayo, and E. S. Qu intana-Orti. "Evaluation and tuning of
the Level 3 CUBLAS for graphics processors." IPDPS. 2008. 1-8.
CAPS. CAPS Compilers. 2012. http://www.caps-entreprise.com/products/caps -compilers/.
Cray Inc. Cray Compiling Environment Release Overview and Installation Guide. 2013.
http://docs.cray.com/cgi-bin/craydoc.cgi?mode=View;id=S-5212-82.
DeRose, Lu iz, Bill Ho mer, Dean Johnson, Steve Kaufmann, and Heidi Po xon. "Cray Performance
Analysis Tools." In Tools for High Performance Computing, by Michael Resch, Rainer
Keller, Valentin Himmler and Bettina Krammer, 191-199. Berlin: Springer, 2008.
Dietrich, Robert, Tho mas Ilsche, and Guido Juckeland. "Non -intrusive Performance Analysis of
Parallel Hard ware Accelerated Applications on Hybrid Architectures." Proceedings of the
2010 39th International Conference on Parallel Processing Workshops (ICPPW '10). 2010.
135-143.
EM Photonics. CULA Tools: GPU Accelerated Linear Algebra. 2010. http://www.culatools.com/.
Go rdon, Mark. The General Atomic and Molecular Electronic Structure System (GAMESS). 1995.
http://www.msg.ameslab.gov/gamess/.
Gottbrath, Chris. "Debugging and optimizing scalable applications on the cray." Cray Users Group
Meeting. Stuttgart, Germany, 2012.
Malony, Allen D., et al. "Parallel performance measurement o f heterogeneous parallel systems with
gpus." Proceedings of ICPP. IEEE, 2011. 176–185.
NVIDIA. Profiler User’s Guide. 2008. http://docs.nvidia.com/cuda/profiler-users-guide/index.html.
OpenACC. OpenACC Directives for Accelerators. 2011. http://www.openacc.org/.
PAPI team . PAPI CUDA. 2011. http://icl.cs.utk.edu/projects/papi/repository/index.php/CUDA.
Riken. The Himeno Benchmark. 2001. http://accc.riken.jp/2444.htm.
Wolfe, Michael. OpenACC Features in PGI Accelerator Fortran Compilers—Part 1. MArch 2012.
http://www.pgroup.com/lit/articles/insider/v4n1a1a.htm.

1490

