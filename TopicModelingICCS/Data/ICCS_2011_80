Available online at www.sciencedirect.com

Procedia Computer Science 4 (2011) 1506–1515

International Conference on Computational Science, ICCS 2011

Numerical modeling of three dimensional self-gravitating Stokes
flow problem with free surface
Mikito Furuchi
Institute for Research on Earth Evolution, Japan Agency for MarineEarth Science and Technology (JAMSTEC), 3173-25 Showa-machi, Kanazawa, Yokohama, Japan

Abstract
A solution code of the Stokes flow system in three dimensions under a self-gravitating field was developed with a free surface
in order to simulate the long-time scale evolution of the solid earth system on the massively parallel-vector super computer Earth
Simulator 2 (ES2). The largely deformable surface of a planetary body is represented by a sharp boundary of scalar color
function, which is advected by the CIP-CLSR method. The self-gravitating field is calculated by the Poisson equation of the
gravity potential. A low viscous material (sticky air) surrounding the planet mimics its free surface motion. In order to solve the
ill conditioned Stokes problem of the finite difference discretization on a staggered grid, I employ the iterative Stokes flow solver,
robust to large viscosity jumps, using a strong Schur complement preconditioner and mixed precision arithmetic utilizing the
double-double method. In this paper, I present an overview of the numerical algorithms and the current parallel programming
strategy for the efficient performance on ES2. In addition, preliminary results of the core formation simulation are demonstrated
in three dimensions as an example of the target Stokes flow system of this simulation code.
Keywords; Stokes flow, free surface, multigrid, Earth simulator, flat mpi ,Krylov

1. Introduction
The simulation of the Stokes flow system is an interesting challenge in the field of computational geodynamics
because it is relevant to the numerical study of the mantle dynamics that plays an essential role in the Earth’s longtime scale thermal evolution. There have been many studies and developments of the solution procedure of Stokes
flow problems toward realistically simulating mantle convection, for example, in the treatment of highly varying
viscosity, nonlinear rheology, transport of material, spherical shell geometry and adaptive grid resolutions [1-11]. In
addition, recent advances of the technology allow one to conduct high resolution simulations, in which the coupled
system of geodynamical processes in different time and spatial scales – such as global mantle convection and local
deformation of a tectonic plate – can be directly and consistently reproduced within the same numerical framework
[e.g. 12-13].
Here, I present newly developed solution method of the Stokes flow system with a free surface under a selfgravitating field designed for massively parallel vector supercomputer systems, especially for Earth Simulator 2
(ES2) [14], in order to investigate the evolution of the solid earth system as a whole with a realistic surface
treatment. The shape of the planet captured by high resolution simulation combined with self-gravitation reproduces,
1877–0509 © 2011 Published by Elsevier Ltd. Open access under CC BY-NC-ND license.
Selection and/or peer-review under responsibility of Prof. Mitsuhisa Sato and Prof. Satoshi Matsuoka
doi:10.1016/j.procs.2011.04.163

Mikito Furuchi / Procedia Computer Science 4 (2011) 1506–1515

1507

for example, self-consistent interactions between surface deformation and the convective motion of the Earth’s
interior.
For the numerical treatment of free surface and self-gravitation, I employ a ‘spherical Cartesian’ approach, which
is used by the simulation study of the core formation [15-18]. Following the spherical Cartesian treatment, sticky air,
which has very small viscosity with zero density to mimic the free surface of the material, is introduced around the
planetary object. Self-gravitation is calculated by the gravity potential equation.
In earlier studies, I and coworkers developed solution algorisms of the low diffusive advection, and the Stokes
flow solver dealing with high viscosity contrast problems [1-3]. In this study, these techniques are applied to the
problems of spherical Cartesian in three dimensions. The surface of the object is advected by the CIP-CSLR method,
which is based on a fully fixed Eulerian mesh and suitable for computation on the vector processors [19-20]. The illconditioned Stokes flow problems due to sticky air and rheological modeling are solved by the iterative solver
utilizing a strong Schur complement preconditioner and mixed precision arithmetic [3, 21].
The parallel programming method of the solution procedure of Stokes problem is also reported in this paper. The
Stokes flow solver presented here involves the Geometric multigrid (GMG) process as preconditioner for the
solution of momentum equations, and its computational cost is dominant in the overall performance of the Stokes
flow solver. Therefore, in order to achieve efficient parallel performance on ES2, careful implementation of GMG
method is required.
In the following part of this paper, I describe an overview of the solution algorithms, and report the current
strategy of the parallel programming method on ES2 supported by numerical experiments.
2. Stokes flow problem with free surface in self gravitation
The simple Stokes flow system with a free surface in the self gravitating field is considered. Fig 1 shows the
initial simulation setup for the layered structure in the box domain Ω ≡ (0, 0, 1) × (0, 1, 0) × (1, 0, 0) of the three
dimensional Cartesian geometry. Free-slip boundary conditions are employed for the all side walls of the
calculation domain. The governing equations are discretized by the finite difference method on a spatially uniform
staggered grid with a size of n = Nx × Ny × Nz . The Stokes flow materials that have a layered structure in the initial
condition are labelled by s = 1,2,K and have density ρ s and viscosity η s . In order to distinguish the special

distribution of the materials, the scalar color functions Φ s discretized at the center of the cell is introduced. Initially,

Φ s = 1 in the s -th layer and Φ s = 0 in other regions.

Fig. 1. (a) Schematic picture of initial simulation setting for three-layered system s=1,2,3; (b) System overview of ES2 consisting of the 160 SMP
nodes

1508

Mikito Furuchi / Procedia Computer Science 4 (2011) 1506–1515

For the solution at any given time step, first the density ρ and viscosity η are calculated from the color
functions by a simple arithmetic average
⎧ρ = Φ s ρ s , η = Φ sη s
Φ sη s ≥ η air
when
⎪
s
s
s
.
(1)
⎨
Φ sη s < η air
ρ = 0, η = ηair
when
⎪
s
⎩

∑

The region of

∑

∑Φ η
s

s

∑
∑

≤ ηair << 1 is filled with sticky air that has properties ρ air = 0 and η air .

s

In order to take into account the self gravitating field, a Poisson equation is solved with the density distributions
∂ 2φ
= 4πGρ ,
(2)
∂x 2
where φ is the gravitational potential and G = 1 is the gravitational constant. The Dirichlet boundary condition of
the potential φ = 0.0 is defined at the distance r = 0.48 from the center of the box (i.e. the surface on the sphere).
This scalar potential leads to the gravitational body force fi in the i-th direction ( i = 1, 2, 3 : x1 = x, xs = y, x3 = z in
three dimensions)
∂φ
.
(3)
f i = − ρg i = − ρ
∂xi
where g i is gravity acceleration.
After the gravity calculation, the momentum equation of the Stokes flow is obtained as,
∂τ ij ∂p
∂ ⎡ ⎛⎜ ∂u j ∂ui ⎞⎟⎤ ∂p
(4)
= fi ,
−
+
=−
+
⎥+
⎢η
∂x j ∂xi
∂x j ⎢⎣ ⎜⎝ ∂xi ∂x j ⎟⎠⎥⎦ ∂xi
with the continuity equation
∂u
(5)
− i =0,
∂xi
where u , p and τ are the velocity vector, pressure and deviatoric stress, respectively. The free slip boundary
∂u j ≠i
= 0 on the wall normal to i-th direction
condition is given by ui = 0 ,
∂xi
In this study, only a linear (Newtonian) constitutive equation is considered. The equations (4) and (5) are solved as
the saddle point problem for the solution of velocity and pressure.
For the time development of the Stokes system, the transports of color functions are calculated by
∂Φ s
∂Φ s
,
(6)
= −u i
∂t
∂ xi
and return to calculate (1) as a next time step solution procedure.

1509

Mikito Furuchi / Procedia Computer Science 4 (2011) 1506–1515

3. Numerical methods
In this section, I will explain an overview of our solution techniques to efficiently solve the equations of the
Stokes system in Sec 2.
The equation of gravitational potential of (2) is solved by using the simple V-cycle GMG method with Jacobi
smoothing iterations as a standalone solver. The standard GMG method is applied for the cell centered discretization
of gravity potential [22]. In general, vector machine architecture is suitable for many iterative operations of cheap
smoother (e.g. Jacobi or SOR method) rather than fewer operations of expensive smoother (e.g. ILU, line- or cellrelaxation method). Here the Jacobi iteration is employed as cheap smoother because it is straightforward to be
optimized for vector processors: colored SOR type method requires complex implementation for a vectorization
with keeping the sequential memory access. From an algebraic point of view, the isotropic Poisson equation of
gravity potential (2) in the spatially uniform discretization creates well-conditioned coefficient matrices to be solved
by the iterative method.
On the other hand, the Stokes flow problem with (4) and (5) involves the anisotropic structure by the varying
viscosity profiles. The discretized form of the incompressible force balance equation of (4) and (5) is given by
r ⎛ K G ⎞⎛ u ⎞ ⎛ f ⎞ r
⎟⎜ ⎟ = ⎜ ⎟ = b
Ax = ⎜⎜ T
(7)
0 ⎟⎠⎜⎝ p ⎟⎠ ⎜⎝ 0 ⎟⎠
⎝G
where u ∈ ℜ m and p ∈ ℜ n are the discrete pressure and velocity solution respectively, K ∈ ℜ m×m is a positive
definite matrix for the gradient of the deviatoric stress term,and G ∈ ℜm×n represents the discretized pressure
gradient. The matrix K can be simplified with discretized vector Laplacian (three scalar value problems for
i = 1, 2, 3 ) when the viscosity is constant. For a realistic simulation setting of solid earth system, however a large
viscosity jump Δη makes the sub-system of K become ill-conditioned vector problem. For the purpose of dealing
with such an ill-conditioned saddle point problem of A , we developed the preconditioned Krylov subspace method
in a fully coupled approach of the velocity-pressure system, which employs right preconditioning
r r
r
r
(8)
AAˆ −1 y = b , x = Aˆ −1 y
with a block upper triangular preconditioner
) ⎛K G ⎞
(9)
A = ⎜⎜
ˆ ⎟⎟ ,
⎝ 0 − S⎠
where Sˆ is preconditioner of the Schur complement S = G T K −1G ∈ ℜ n×n [23]. The Arnoldi type GCR method is
employed as the Krylov subspace method, which is directly applied to (7) as an outer solve with the non-symmetric
preconditioner of (9). The action of the preconditioner,
⎛ zu ⎞ ) −1 ⎛ ru ⎞
⎜ ⎟=A ⎜ ⎟
(10)
⎜r ⎟
⎜z ⎟
⎝ p⎠
⎝ p⎠
is obtained by following procedure
−1
compute z p = −Sˆ rp

Kzu = ru − Gz p .

solve

In (11), a scaled identity matrix with local viscosity (LV)
Sˆ −1 = ηI ,
p

(11)
(12)
(13)

where I p is an identity operator on the pressure space, is employed as the preconditioner of the Schur complement.
In the Stokes flow problem with uniform viscosity equal to one, the Schur complement is known to be spectrally
equivalent to identity operator I p on the pressure space. The preconditioner of (13) is derived by the diagonal
scaling technique with a local viscosity value applied to I p for dealing with highly varying viscosity problems. The
effect of this scaling method via clustering of the eigenvalues of Sˆ −1S is numerically observed and theoretically
analyzed in [24].
The solution of sub-problem of K with a large contrast of matrix elements in (12) is solved as the inner solver
for K −1 , which is also equipped with the pre-conditioned Krylov subspace method (GCR). The use of the full

1510

Mikito Furuchi / Procedia Computer Science 4 (2011) 1506–1515

recurrence relation in GCR method is less vulnerable to rounding errors than Lanzos type methods with three term
recurrence terms, and is preferred to solve the ill-conditioned problem of K . In order to get scalability against an
increasing problem size, multigrid method is employed as preconditioner. In practice, the GMG method is applied to
a staggered grid system of the velocity vector [22]. The weighted Jacobi method is used as a smoother for the
multigrid method. The multi grid level l is defined from the finest grid level ltop to the coarsest grid level lbot = 1. The
number of grid levels ltop is chosen to satisfy ( Nx)1 × ( Ny)1 × ( Nz )1 ≤ 512 where (⋅)1 denotes the size at the bottom
grid level (lbot = 1) in each direction. The cycling scheme of the multigrid method is the V-cycle. The number of preand post-smoothing iterations of V-cycle are given by 2 × N K (l ) and N K (l ) respectively at grid level l , where
N K (l ) = N K × ( ltop + 1 – l ) with N K = 20 . At the coarsest grid level lbot of the GMG calculation, a direct solver
(Gaussian elimination) is applied to obtain the solution exactly up to the double precision limit. During the V-cycle
operations, if we measure sufficiently small error residual after the pre-smoothing at grid level l , the grid level of
GMG process switches to finer grid level l + 1 instead of going down to the l − 1 level to reduce the operation at
coarse grid as an adaptive cycling procedure [8].
Throughout the Krylov subspace iterations applied to the ill-conditioned matrix of K , arithmetic rounding errors
sometimes cause convergence to stagnate. Then in order to solve large viscosity contrast problems, inner solver of
(12) is equipped with a mixed precision method utilizing quad precision arithmetic. In the mixed precision approach,
quad precision arithmetic is applied throughout the GCR algorithm except during the application of the
preconditioner. This procedure of the mixed precision method is consistent with the idea that the preconditioner is
used for a rough estimation of the real solution. The high precision calculation of full recurrence relation in GCR
method is found to avoid the stagnation of convergence, and improves the robustness of the solver [3].
Instead of officially supported quad precision arithmetics (e.g. REAL*16 in Fortran), double-double precision
algorithms (DD) is employed [3, 21], in which operations using a pair of double precision terms emulates that in
quad precision. An advantage of arithmetic calculation in DD precision is provided by the computer architecture.
The loop calculation in DD precision can be vectorized, but that in the normal quad precision cannot. In addition,
the high intensity of DD algorithms is suitable for speeding up of the operation by using a cache-type memory.
The more details of our Stokes flow solver design and its numerical examinations for the problems with locally
and highly varying viscosity will be presented by [3].
For the transport of the material in (6) with its sharp interface, the CIP-CLSR method is employed as a low
diffusive semi-Lagrangian scheme. The CIP-CLSR method employs the spatially integrated values and its interface
values to construct functional profile on each grid cell. By using the obtained subgrid profiles, the interface values
are updated in semi-Lagrangian manner, and the integrated value is advected by the flux form calculation. These
two types of transport processes of the CIP-CLSR method lead to the low diffusive transported profile with a mass
conservation of the cell-integrated values. In addition, the rational function is employed to construct the subgrid
scale profiles, in order to avoid the oscillatory behavior of profile.
In three dimensional calculation, the variable for the CIP-CLSR method are defined on the points, lines, areas and
volumes for each grid cells. A directional splitting method is applied for the three-dimensional staggered grid
system [1, 19-20]. The operation on fully Eulerian mesh of CIP-CLSR method is easy to optimize in
vectorization/parallelization. The more details of our implementation is given by [1].
The quantitative validation of the sticky air treatment with the viscosity jump captured by the CIP-CLSR method
has been reported in our earlier study for the fluid rope coiling event [2].

Mikito Furuchi / Procedia Computer Science 4 (2011) 1506–1515

1511

4. Parallel programming
The application presented here is designed for ES2 which is the vector-parallel supercomputer based on the NEC
SX-9 consisting of 160 nodes of the SMP system with the theoretical peak performance of 131TFlops. Each node of
ES2 has 8 vector processors (CPUs) with 102.4Gflops per CPU (Fig 1 (b)). In this platform, three levels of computer
architectures should be considered for vectorization and parallelization [25]. (i) On each CPU, automatic
vectorization is applied to the inner most loop by the compiler. (ii) Inside each SMP node (intra node),
parallelization on the CPUs is given by the shared memory (i.e. Open MP) or by message passing communications
(MPI). (iii) Between the SMP nodes (inter node), MPI is an available communication method for parallelization.
In the implementation of iteration loop, the solution vector is stored as a one dimensional array. The process of
Jacobi -type smoother for the velocity vector is as follows.
!cdir ON_ADB(u1,u2,u3)
do iik=ijk_s,ijk_e
ip1jk=ijk+1;ijp1k=ijk+nx;ijkp1=ijk+nx*ny; (set other indexes)
calculate u1_n(ijk) by the values of u1, u2 and u3 at grid points around ijk
calculate u2_n(ijk) by the values of u1, u2 and u3 at grid points around ijk
calculate u3_n(ijk) by the values of u1, u2 and u3 at grid points around ijk
enddo

The vector length given by this implementation is large enough for getting an efficient acceleration by the vector
processor, even for a small problem size at the coarse grid level of the GMG method. Here, the arithmetic operations
are also accelerated by improving the memory access using Assigned Data Buffer (ADB). At the first loading of the
vector arrays ‘u1, u2, u3’ in the loop calculation, the data are buffered on ADB by the directive option ‘ON_ADB’.
From the next loading of the data assigned to ADB, the loading speed can be improved and is faster than normal
memory access. The size of the ADB is 256 KB. When data on the ADB exceeds the size limit, it is overwritten by
the newly stored data. Due to this limitation of the capacity, improvement by the ADB generally depends on the size
of problem.
As a parallel programming approach, two types of approaches, the ‘flat-MPI’ and the ‘hybrid’ methods are taken
into account. In the flat-MPI, only MPI communications are considered between the processes on each CPU. On the
other hand, in the ‘hybrid’ method, Open MP and MPI parallelization schemes are used inside a node and between
nodes, respectively. In general, it is not clear which approach shows better parallel performance, because the
overhead of both parallelization methods depends on several factors of the solution environment, for example
hardware design, iterative algorithms and the size of the problem. Then, it is important to select an efficient parallel
approach by the comparison test [25].
In the parallel performance tests of Sec 5, our interest lies in the performance of the smoothing process of the
GMG method. The sequential operation of smoothing from fine to coarse grid levels inherently has difficulty in its
parallel performance, because a decreasing problem size at the coarser grid levels leads to a smaller grid size of the
problem on each process and causes an increasing communication overhead against the arithmetic computation.
For the grid partitioning for parallelization of GMG method, hierarchical parallel process level are introduced
other than the multigrid levels [22]. The discrete grid Nx × Ny × Nz is decomposed into sub domains involving 23M
CPU processes, where M = 1,2,K denotes the process level. Each process is attributed to the problem with a grid
*
*
*
size of Nx / 2 M × Ny / 2 M × Nz / 2 M at the M -th process level, where the notation for the grid size
(⋅)* × (⋅)* × (⋅)* is introduced to distinguish this from the grid size of whole processes. The agglomeration technique
from the M to the M − 1 process level is applied to reduce the available number of CPUs that contribute to the
arithmetic computation, when the problem size per CPU is small enough that communication overhead is greater
than the cost for the idling processes at the M − 1 process level. At the bottom of the multigrid level, the parallelized
Gaussian elimination solver is used with 8 CPUs (1 node).

(

) (

5. Numerical experiment

) (

)

1512

Mikito Furuchi / Procedia Computer Science 4 (2011) 1506–1515

In this section, some numerical experiments are performed to report the parallel performance of the developed
code on ES2, and demonstrate the result of the self-gravitating Stokes flow problem with a free surface solved by
the employed solution method.
First, the parallel performance of the smoother of the GMG method for the inner solver of is presented. The
10,000 times loop iterations of the smoother for the different grid sizes are calculated by a single node (8 CPUs) and
8 nodes (64 CPUs). The obtained performances for these loop calculations including communication overhead, are
summarized with parallel programming models in Table 1.
Table 1. Comparison of exclusive (CPU) time [sec], speed [Gflops] (par single CPU) and averaged vector length for 10000 times smoothing
iterations by 8 nodes (64 CPUs) and 1 node (8 CPUs), in between the flat-mpi and Open MP parallelization methods.
Grid size

256 × 256 × 256
128 × 128×128

8node:64CPUs Flat-MPI

1node: 8CPUs MPI

1node: 8CPUs Open MP

11.28[sec] ;35.81[Gflops]; 256.0

91.71[sec]; 34.20[Gflops]; 256.0

234.56 [sec]; 13.17[Gflops]; 256.0

4.17[sec] ; 12.85[Gflops]; 254.7

11.45[sec]; 35.30[Gflops]; 255.9

12.25 [sec]; 31.99[Gflops]; 255.8

64 × 64 × 64

2.56[sec] ; 2.92[Gflops]; 245.3

2.57[sec]; 20.81[Gflops]; 254.7

2.74 [sec]; 18.08[Gflops]; 254.3

32 × 32 × 32

2.45[sec] ; 0.46[Gflops];195.8

1.36[sec]; 5.49[Gflops]; 245.3

1.54 [sec]; 4.33[Gflops]; 243.0

16 × 16 ×16

1.299[sec] ; 0.15[Gflops];132.1

1.18[sec]; 0.96[Gflops]; 195.8

1.37[sec]; 0.67[Gflops]; 215.1

For the single node jobs, the MPI model shows better performance than Open MP for all grid sizes. The overhead
by the MPI communication seems to be better than the threading overhead of Open MP in this application on ES2.
This observation can justify the use of the flat-MPI as the standard parallel programming method of this code. From
the point of view of the calculation speed against the grid size, the case for 128 ×128×128 ( 64* × 64* × 64* par CPU)
provides the best performance as the single node calculation. In smaller grid size problems than 128 ×128×128 , the
increase of the communication overhead against arithmetic computations is found to slow down the calculation. On
the other hand, the larger grid size calculation of 256 × 256 × 256 becomes also slower than that of 128 ×128×128 .
In the MPI model, this slow down comes from the capacity of the ADB (256 KB). The use of ADB accelerates the
calculation on the grid size of 64* × 64* × 64* by the fast loading of the two sets of stored velocity arrays of 2D
planes on ADB (i.e 2 × ( Nx * + 2) × ( Ny * + 2) × (for u1, u 2, u 3) × (size of double precision) = 2 × 66 × 66 × 3 × 8byte
≈< 256KB ). On the other hands, the available data size on ADB is not sufficient to get such a speed up in the
memory access for the problem with 128* × 128* ×128* . In other words, my implementation of Jacobi relaxation
with ADB is optimized for the grid size of 64* × 64* × 64* .
In the 8 node calculations of Table 1, the performance with flat-MPI for the grid size of 256 × 256 × 256
( 64* × 64* × 64* on each CPU) provides the perfect weak scaling (how solution time varies for a fixed problem size
per CPU) against the single node MPI calculation of 128 ×128×128 with 35.0% of peak performance (2.3Tflops in
total CPU performance). However this good parallel scaling relation and performance quickly breaks as the grid size
decreases, and finally the single node calculation (5.49 Gflops × 8 CPU) outperforms 8 node calculation (0.46
Gflops × 64 CPU) in the grid size 32 × 32 × 32 . From this observation, in order to minimize the loss of efficiency of
calculation on coarse grid operations, the agglomeration scheme is applied to reduce the active CPUs for arithmetic
computation when the local grid size on the CPU is less than 8* × 8* × 8* .
Here, as a practical problem, the time development of a steady state calculation of self gravitating field is
performed for a two-layer system s = 1, 2 with same constant viscosity η s = 1.0 and density ρ s = 1.0 combined
with the sticky air η air = 10 −3 , ρ air = 0 . In the simulation with a grid size of 256 × 256 × 256 , our application
achieved 910.3Gflop using 8 nodes (13.9% of peak performance) with an average vector length of 248.1 and
99.50% vector operation ratio. Smoothing of the GMG method of the inner solver for K −1 consumed 67.3% of the
overall execution time, which are optimized by the agglomeration technique on flat-MPI process discussed above.
Next, the result of the steady state calculation is examined. The panels in Fig 2 provide a simulated deformation
process of the Stokes flow under the self-gravitating field from an initial cube shape Fig 2 (a), to sphere Fig 2 (c)
that is the steady state of self-gravitation. In order to validate this numerically reproduced steady state solution, the
density and acceleration of the steady state are plotted at randomly selected grid points around the surface of the
planet in Fig 3. The good agreement of the numerical solution with the analytical value is found except for the
surface region of the planet around r = 0.397 . Deviation from the analytic value exists within the length of 2-3 grids,

Mikito Furuchi / Procedia Computer Science 4 (2011) 1506–1515

1513

which is consistent with the typical diffusive length of the interface captured by the CIP-CLSR method. These
qualitative observation and quantitative validation results justify the use of spherical Cartesian method with fine grid
resolution on a super computer system.

Fig. 2. Snapshot of the simulated evolution of two layered system in steady state calculation (a) initial state; (b) intermediate state; (c) steady
state layered sphere. Outer half cropped isovolume and inner white isosurface represent color function of s = 1 and s = 2 respectively. Two
layer have the same isoviscous property η1 = η 2 = 1.0 and density ρ1 = ρ 2 = 1.0 , surrounded by the sticky air η air = 10 −3 , ρ air = 0.0

Fig. 3. Comparison of gravity acceleration of steady state (sphere) between analytical result and numerically obtained values with distance r
from the center. Presented numerical result is calculated with grid resolution of 128 × 128× 128 . Density distribution by the CIP-CSLR method
is also shown.

1514

Mikito Furuchi / Procedia Computer Science 4 (2011) 1506–1515

Finally a preliminary simulation result of the core formation process is demonstrated by using a three-layered
system. In this simulation setting, each layer in Fig 1 (a) stands for the proto core, metal rich layer and mantle for
s = 1, 2, 3 respectively [16, 18]. First, I calculate the steady state of the three-layered system (Fig 4 (a)) with uniform
viscosity and density for s = 1, 2, 3 . Then, the core formation simulation starts by switching the viscosity η s and
density ρ s to the target parameters. The initial perturbation, which automatically enters the numerical result of the
initial steady state calculation, induces gravitational instabilities under the self gravitating field. This three layered
gravitational instability experiment simulates how the metal-rich layer sinks to the center and constructs the core.
This type of simulation study plays an important role for the numerical examination of the proposed scenario of the
core formation process. In Fig 4, the soft proto core (isoviscous layer) is considered with η 2 = η3 . On the other hand,
the model in Fig 5 provides a rigid proto core scenario surrounded by soft metal at an initial state by η 2 < η3 .
Different types of core formation processes were shown in the figures of Fig 4 and Fig 5. In Fig 4, the metal
layer enters the central soft proto core from various directions and results in a short wavelength instability mode. On
the other hand, the rigid proto core scenario η 2 < η3 results in the long wavelength instability in Fig 5 (a) followed
by short wavelength modes in Fig 5 (b). Although it may be difficult to observe in the figures, the long wavelength
deformation of the free surface (semi-transparent isosurface) is found in the beginning of overturn process between
Fig 5 (a) and (b).

Fig. 4. Snapshots of the core formation simulation with three layered model in soft core scenario (a) initial state; (b) intermediate RT instability
mode; (c) resultant layered sphere. Outer most semi-transparent isosurface and half cropped white isosurface represent mass density at ρ = 0.2 ,
and color function at Φ 2 = 0.8 respectively. The color on ortho plane shows mass density. Property of each layer are given by
η air = 10 −3 , ρ air = 0.0 , η1 = 10 0 , ρ1 = 0.83 , η 2 = 10 −3 , ρ 2 = 1.67 and η 3 = 10 −3 , ρ 3 = 1.0 .

(

) (

) (

)

(

)

Fig. 5. Snapshots of the core formation simulation with three layered model in rigid core scenario. Initial state is given by Fig 4 (a), and time
steps goes by left to right. Isosurfaces and colored plane are same as Fig 4. Panels are (a) large wavelength instability mode; (b) small
wavelength instability mode; (c) resultant layered sphere. Isosurfaces and colored plane are same as Fig 4. Property of each layer are given by
η air = 10 −3 , ρ air = 0.0 , (η1 = 1.0, ρ1 = 0.83) , η 2 = 10 −3 , ρ 2 = 1.67 and η 3 = 10 −1 , ρ 3 = 1.0 .

(

)

(

)

(

)

Mikito Furuchi / Procedia Computer Science 4 (2011) 1506–1515

1515

6. Conclusion
From these results, even though further studies are required for the quantitative discussion of these simulated
instabilities, the simulation method presented here is found to successfully handle the viscosity jump between rigid
and soft materials of core formation simulation in three dimensions, maintaining a sharp interface between them.
As a next step, a simulation with more realistic geodynamical settings such as a nonlinear rheology and thermal
effect will be performed by using larger number of nodes of ES2. For the further improvement of application, it may
be worthwhile to implement the colored SOR type relaxation method by dividing the array into each color for a
sequence memory access on vector process. I’m also interested in adaptation of the stabilization scheme of the free
surface motion to evolve the simulation with loner time step differences [26].
Acknowledgements
I thank Dave May, Paul J. Tackley, Masanori Kameyama, Seiji Tsuboi and Akira Kageyama for fruitful
discussion and support of this study. Part of this work was performed in the Institute of Geophysics ETH Zurich,
during the research abroad program of the Japan Agency for Marine-Earth Science and Technology. All of the
numerical calculations presented in this paper were performed by Earth Simulator 2 of Japan Agency for MarineEarth Science and Technology.
References
1. M. Furuichi, M. Kameyama, and A. Kageyama, J. Comput. Phys. 227 (2008) 4977–4997
2. M. Furuichi, M. Kameyama, and A. Kageyama, Phys. Earth Planet. Interiors, 176 (2009) 44-53
3. M. Furuichi, D.A. May, P. J. Tackley, J. Comput. Phys (submitted).
4. D.A. May, L. Moresi, Phys. Earth Planet Interiors. 171 (2008) 33–47.
5. P. J. Tackley, S. Xie, Proceedings of the Second MIT Conference on Computational Fluid and Solid Mechanics, Elsevier B.V, Amsterdam
(2003) 1524–1527.
6. P. J. Tackley, Earth Planet Interiors. 171 (2008) 7–18.
7. M. Kameyama, Akira Kageyama, and Tetsuya Sato, Phys. Earth Planet. Interiors, 171, 19-32, 2008
8. M. Kameyama, Journal of the Earth Simulator. 4 (2005) 2–10.
9. J. Brown, Journal of Scientific Computing, 45 1-3 (2010) 48-63.
10. L. Moresi, F. Dufour, H. B. Muhlhaus, J. Comput. Phys. 184 (2003) 476–497.
11. C. Burstedde, O. Ghattas, G. Stadler, T. Tu, and L. C.Wilcox, Comput. Methods Appl. Mech. Engrg, 198 (2009) 1691-1700.
12. G. Stadler, M. Gurnis, C. Burstedde, L. C. Wilcox, L. Alisic, and O. Ghattas, Science, 329 5995 (2010) 1033-1038.
13. M. A. Jadamec and M. I. Billen, Nature, 465 (2010) 338-341.
14. http://www.jamstec.go.jp/es/en/index.html
15. R. Honda, H. Mizutani and T. Yamamoto, J. Geophys. Res., 98 (1993) 2075-2089.
16. T. V. Gerya, D. A. Yuen, Phys. Earth Planet. Interiors, 163 (2007) 83-105.
17. G.J. Golabek, T. V. Gerya, B. J. P. Kaus, R. Ziethe and P. J. Tackley, Geochemistry, Geo-physics, Geosystems, 10 (2009) Q11007.
18. R. Lin, T. V. Gerya, P. J. Tackley, D. A. Yuen, G. J. Golabek, Icarus, 204 (2009) 732-748
19. T. Nakamura, R. Tanaka T. Yabe and K. Takizawa, J.Comput.Phys. (2001) 171–207.
20. F. Xiao, T. Yabe, X. Peng, H. Kobayashi, J. Geophys. Res. 107 (D22) (2002) 4609.
21. D. H. Bailey: High-Precision Software Directory, http://crd.lbl.gov/~dhbailey/mpdist/
22. U. Trottenberg, C. Oosterlee and A. Schuller, Multigrid, Academic Press, New York, (2001)
23. M. Benzi, G.H. Golub and J. Liesen, Numerical solutions of saddle point problems, Acta Numer. (2005), pp. 1–137
24. P. Grinevich and M. A. Olishanskii, SIAM J. Sci. Comput, 31 5 (2009) 3959-3978.
25. K. Nakajima, Applied Numerical Mathematics, Vol.54, (2005) pp.237-255.
26. B. J. P. Kaus, H. Muhlhaus and D. May, Phys. Earth Planet Interiors. 181 (2010) 12–20.

