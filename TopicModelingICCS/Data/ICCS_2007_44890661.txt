Data Set Homeomorphism Transformation Based
Meta-clustering*
Xianchao Zhang, Yu Zong, He Jiang, and Xinyue Liu
School of Software, Dalian University of Technology, Dalian 116621, China
{xczhang, zongyu, jianghe, xyliu}@dlut.edu.cn

Abstract. Clustering analysis is an important data mining technique with a
variety of applications. In this paper, the data set is treated in a dynamic way
and a Data Set Homeomorphism Transformation Based Meta-Clustering
algorithm (DSHTBMC) is proposed. DSHTBMC decomposes the task of
clustering into multiple stages. It firstly constructs a series of homeomorphous
data sets ranging from high regularity to low, and then iteratively clusters each
homeomorphism data set based on the clustering result of the preceding
homeomorphism data set. Since data sets of high regularities are easier to be
clustered, and the clustering result of each homeomorphism data set can be used
to induce high quality clusters in the following-up homeomorphism data set, in
this way, the hardness of the problem is decreased. Two strategies (i.e.,
Displacement and Noising) for data set homeomorphism transformation are
proposed, with classical hierarchical divisive method--Bisecting k-means as
DSHTBMC’s subordinate clustering algorithm, two new clustering algorithms---HD-DSHTBMC-D and HD-DSHTBMC-N are obtained. Experimental results
indicate that the new clustering algorithms are remarkably better than Bisecting
k-means algorithm in terms of clustering quality.
Keywords: Clustering Analysis, Meta-Clustering, Data Set Homeomorphism
Transformation.

1 Introduction
Clustering analysis is an important data mining technique with a variety of
applications in massive data statistic, network analysis and medicinal graph automatic
detection, etc. Clustering divides the data set into different clusters according to the
inherent characteristic of data, making data elements similar in the same cluster and
dissimilar among different clusters. The recent study progress is proposed in [1-2].
Since the 1940’s, a large number of clustering algorithms have been proposed, such as
divisive method (k-means[3], CLARANS[4], FREM[5])
hierarchical method
(CHAMELEON[6], BRICH[7], PDDP[8,9])
grid-based method (WaveCluster[10],

、

*

、

This paper is supported by National of Science Foundation of China under grant number:
60503003.

Y. Shi et al. (Eds.): ICCS 2007, Part III, LNCS 4489, pp. 661–668, 2007.
© Springer-Verlag Berlin Heidelberg 2007

662

X. Zhang et al.

STING[11], CLIQUE[12]) and density-based method (DBSCAN[13], OPTICS[14]). These
algorithms do not change the data set being processed, and they are here called static
algorithms.
In this paper, a clustering strategy is proposed, which transforms data elements’
positions in the data set and clusters the data set in different stages of the position
transformation. Thus a Data Set Homeomorphism Transformation Based MetaClustering (DSHTBMC) is obtained. Different from traditional static clustering
algorithms, DSHTBMC firstly adopts a data set homeomorphism transformation
strategy to construct a series of homeomorphous data sets ranging from high
regularity to low, and then, by iteratively calling a subordinate clustering algorithm,
clusters each homeomorphism data set based on the clustering result of the preceding
homeomorphism data set. Here regularity means the regular extent of data elements
distribution. Since data sets of high regularity are easier to be clustered, and the
clustering result of each homeomorphism data set can be viewed as a pre-clustering of
the next homeomorphism data set, which induces high quality clusters in the latter
one. In the end, a high quality clustering result is obtained in the original data set.
Two strategies for data set homeomorphism transformation are proposed in this
paper. Displacement method views the extent of data elements closing to the
barycenter of the data set as regularity, and constructs homeomorphism data sets by
removing data elements from nearby barycenter to their initial positions gradually.
Noising method views the even extent of data element distribution as regularity, and
constructs homeomorphism data sets by adding random noises ranging from strong to
weak to the original data set, where the noises are used reduce data elements’
differences. Different transformation strategies lead to different DSHTBMCs and the
DSHTBMCs adopting the above two strategies are called DSHTBMC-D and
DSHTBMC-N respectively. The classical hierarchical divisive method--Bisecting kmeans[15] is used as DSHTBMC’s subordinate clustering algorithm, and two new
clustering algorithms----HD-DSHTBMC-D (Hierarchical Divisive DSHTBMC Using
Displacement ) and HD-DSHTBMC-N (Hierarchical Divisive DSHTBMC Using
Noises) are obtained. The performance differences among the new algorithms and the
traditional algorithm Bisecting k-means are demonstrated with a series of
experiments. Experimental results indicate that the new clustering algorithms are
remarkably better than Bisecting k-means algorithm in terms of clustering quality.
This paper is organized as follows. In the next section, relevant notations and
definitions are introduced. In section 3 the framework of Data Set Homeomorphism
Transformation Based Meta-Clustering (DSHTBMC) and two homeomorphism
transformation strategies are described. The principle and framework of hierarchical
divisive DSHTBMC algorithm is discussed in Section 4. In the last section,
experimental results and performance evaluations are demonstrated.

2 Preliminaries
Definition 1. For given data set X = {x1, x2 , , xn }, ∀i ∈{1, 2, , n} , xi = (xi1, xi2,..., xip) is called
a data element of X , and ∀j ∈{1, 2, , p}, xij is an attribute of xi .

Data Set Homeomorphism Transformation Based Meta-clustering

663

Definition 2. For given data set X , clustering is a process of partitioning X into
subsets C = {C1,C2 ,...,Ck} according to inherent characteristic of data, subject to:
∀i, j ∈ {1, 2,

, k} , Ci ≠ φ , C j ≠ φ , Ci ∩ C j = φ ( i ≠ j ) , and

∪i =1Ci = {1,2,
k

, n} . Where Ci is

called a cluster, whose size is denoted by | Ci | .
Definition 3. For given data set X ={x1, x2, , xn} , its barycenter is computed
by wX = (w1X , w2X ,..., wpX ) , ∀j ∈{1,2,..., p}, w Xj =

∑ in= 1 x ij .

1
n

To assess the performance of clustering algorithms, two modes, external quality
evaluation and internal quality evaluation are adopted at present. The former is to
compare whether clustering result of an algorithm is consistent with the result
obtained by professionals based on domain knowledge. The latter is mainly to
compare the qualities of clustering results obtained by different algorithms. Since
external quality evaluation is human dependent, most scholars adopt the mode of
internal quality evaluation, so does this paper.
Sergio M.Savaresi etc.[16] proposed an internal quality evaluation, considering both
similarity inside clusters and dissimilarity among clusters in accordance with the basic
principles of clustering. The relevant definitions are as follows.
Assume that the clustering result is depicted as C = {C1 , C2 ,..., Ck } , and
the barycenter of Ci is denoted by wC . Similarity inside a cluster is measured
by average discrete degree. For a cluster Ci , its average discrete degree is
i

computed as ei = 1 | C i | ∑

j ∈C i

|| x j − w C i ||2 . Dissimilarity among clusters is

measured by distance between clusters, i.e., the minimum value of distances
between the barycenters of a cluster Ci and the other clusters, denoted
by dˆi = min j (|| wC − wC ||) .
i

j

With the above defined average discrete degree and distance, the internal quality
can be computed by:
Q ( C 1 , C 2 , . .. , C k ) =

∑

k
i=1

| C i | ei

dˆ i n

.

(1)

The smaller the value Q ( C 1 , C 2 , ..., C k ) is, the better quality of the obtained
clustering result is[16].
Definition 4. [17] Assume f : X → Y is an one-one mapping, then f is called a
homeomorphism transformation from X to Y . If such homeomorphism
transformation f exists, then X and Y are called homeomorphism equivalence,
denoted by X ≅ Y . Data set X is called original data set, and Y is called a
homeomorphism data set of X .
Homeomorphism is an important concept in Topology with a variety of applications
in mathematics, biology and physics and so on.

664

X. Zhang et al.

3 Data Set Homeomorphism Transformation Based
Meta-clustering
3.1 The Algorithm Framework
The underlying idea of data set homeomorphism transformation based meta-clustering
is: constructing a series of homeomorphous data sets by transforming data positions,
and clustering these homeomorphous data sets. The clustering of each
homeomorphous uses the clustering result of the preceding data set as an initial
solution. To The control the convergence of the algorithm, a homeomorphism factor
is used, which reflects the regular degree of the data sets and can be used to. The
algorithm terminates when homeomorphism factor reaches small enough.
The framework of DSHTBMC is described in algorithm 1.
Algorithm 1. DSHTBMC
Input: the original data set X , the data set homeomorphism transformation f , the
initial value of homeomorphism factor α 0 and its threshold τ , step length λ .
Output: clustering result.
Begin
(1) α = α 0 ;
(2) perform homeomorphism transformation from X by calling f , and a
homeomorphism data set Y is acquired;
(3) Y is clustered by calling the subroutine clustering algorithm , and clustering
result C is obtained;
(4) while ( α > τ )
(4.1) α ' = α − λ ; C′ = ∅ ;
(4.2) the homeomrphism data set Y ′ of X is produced by calling f ;
(4.3) Y ′ is clustered by calling the subroutine clustering algorithm based on the
initial solution C , and the clustering result C′ is acquired;
(4.4) α = α ' , C = C′ ;
(5) return C ;
End.
3.2 Data Set Homeomorphism Transformation Strategy
3.2.1 Displacement
The principle of displacement is that data elements of original data set X are diffused
gradually from nearby barycenter to their original positions, thus a series of
homeomorphism data set with different regularities are constructed. Algorithm 2
describes the framework of displacement. It can be observed from algorithm 2 that
internal data difference of the obtained homeomorphism data set is largened gradually
as the homeomorphism factor varies from big to small. When the homeomorphism
factor equals 1, that is, yij = xij , the original data set is resumed.

Data Set Homeomorphism Transformation Based Meta-clustering

665

Algorithm 2. Displacement
Input: homeomorphism factor α ≥ 1 , original data set X and its barycenter w X .
Output: homeomorphism data set.
Begin
(1) for each xi ∈ X do
for each xij do
if xij ≥ w Xj then
yij = w Xj + ( xij − w Xj )α ;

else
yij = w Xj − ( w Xj − xij )α ;

(2) return Y ;
End.
Power function transformation is adopted to transform position of data
element xij in original data set X , so data of original data set should be normalized
before data set homeomorphism transformation.
3.2.2 Noising
The essential idea of noising is to construct a series of homeomorphism data set with
different regularities by adding noise from strong to weak to the original data set X .
Algorithm 3 elaborates the framework of noising. During the course of the
homeomorphism factor varying from big to small, the noise decreases gradually, and
internal data elements differences of homeomorphism data set get largened little by
little. When the homeomorphism factor equals 0, noise disappears, and then the
original data set is resumed.
Algorithm 3. Noising
Input: original data set X , homeomorphism factor α ≥ 0.0 .
Output: homeomorphism data set.
Begin
(1) for each xij do
(1.1) generate a random number R ∈ {−1,0,1} ;
(1.2) y ij = xij × (1 + Rα ) ;
(2) return Y ;
End.

4 Hierarchical Divisive DSHTBMC
In this section, the classical hierarchical divisive method--Bisecting k-means is used as
subordinate clustering algorithm of DSHTBMC, and a concrete algorithm is proposed.
4.1 Principle and Framework of the Algorithm
As an implementation DSHTBMC, we use the idea of hierarchical divisive clustering
algorithm, and call this implementation as Hierarchical Divisive DSHTBMC (HDDSHTBMC).

666

X. Zhang et al.

Hierarchical divisive clustering is a procedure of constructing hierarchical binary
tree, where non-leaf nodes of binary tree denote mesne divisive clusters, and leaf
nodes denote the clustering result. Correspondingly, HD-DSHTBMC performs
clustering on a homeomorphism data sets clustering tree, which is a special
hierarchical binary tree with the following features:

①
②
③

The root of the tree corresponds to the initial homeomorphism data set with the
highest regularity;
The bottom layer corresponds to original data set, and each leaf node is a cluster;
The rest layers correspond to the other homeomorphism data sets, and in each
layer, a node is a cluster of the corresponding data set.
Note that each layer is more regular than the lower layers.
Algorithm 4 gives the framework of HD-DSHTBMC.
Algorithm 4. HD-DSHTBMC
Input: original data set X , the homeomorphism transformation f , the initial value
of homeomorphism factor α 0 and its threshold value τ , step length λ .
Output: clustering result.
Begin
(1) α = α 0 ;
(2) call f to produce homeomorphism data set Y of original data set X ;
(3) the initial cluster set C = Y ;
(4) while( α > τ ) do
(4.1) α ' = α − λ ; C′ = ∅ ;
(4.2) call f to produce new homeomorphism data set Y ′ ;
(4.3) for each Ci ∈ C do
call Bisecting k-means algorithm to perform division operation for Ci , and
insert the result into C′ ;
(4.4) α = α ' ; C = C′ ;
(5) return C ;
End.
Different clustering algorithms are obtained by performing diverse strategies of
data set homeomorphism transformation in the layers of the tree. The algorithms
adopting displacement and noising are denoted by HD-DSHTBMC-D and HDDSHTBMC-N, respectively.

5 Experimental Results
The contrasts of clustering quality, among Bisecting k-means, HD-DSHTBMC-D and
HD-DSHTBMC-N, are shown in this section. The parameters used in HDDSHTBMC-D algorithm are: α 0 = 1.0 , λ = 1 , τ = 1 , and in HD-DSHTBMC-N:
α 0 = 1.0 , λ = 0.1 , τ = 0.0 .
Fig. 1 compares the qualities of the three algorithms for the same data set of
different cluster numbers. The size of the data set is 15000. The three clustering

Data Set Homeomorphism Transformation Based Meta-clustering

667

algorithms are run and the clustering results, with the number of clusters
4,6,8,10,12,14,16,18 and 20 are obtained. It can be observed from the figure that the
clustering qualities of HD-DSHTBMC-D and HD-DSHTBMC-N are obviously better
than Bisecting k-means method. The clustering qualities of HD-DSHTBMC-D and
HD-DSHTBMC-N are very close, while HD-DSHTBMC-N is slightly better.
Fig. 2 compares the qualities of the three algorithms under the same cluster number
on data sets of different sizes. The sizes of data sets are 1000, 3000, 5000, 7000,
9000, 11000, 13000, 15000. The three different clustering algorithms divide the data
sets into 8 clusters. It can be concluded from the figure that the clustering qualities of
HD-DSHTBMC-D and HD-DSHTBMC-N are remarkably better than Bisecting kmeans method without reference to data size.

Fig. 1. Comparison of Cluster Quality of 3
Algorithms on the Same Data Set with
Different Cluster Numbers

Fig. 2. Comparison of Cluster Quality of 3
Algorithms under the Same Cluster Number
on Different Data Sets

6 Conclusion
Rather than concrete algorithms, the main contribution of this paper is that a novel
idea of clustering, DSHTBMC, is proposed. Somehow like Shell-Sorting, which is a
multi-stage sorting algorithm, DSHTBMC decomposes the task of clustering into
multiple stages. Each stage is an easier clustering problem, and its solution can be
used to induce solution of the next stage. In this way the initial clustering problem is
eased. In contrast with traditional clustering algorithms through which the underlying
data set remains changeless, homeomorphism transformation is employed in
DSHTBMC. Two homeomorphism transformation strategies are introduced, and as an
implementation of DSHTBMC, HD-DSHTBMC is proposed with Bisecting k-means
as the subordinate algorithm. Experimental results show the effectiveness of
DSHTBMC. Further works include designing diversified homeomorphism
transformation strategies, diverse implementations of DSHTBMC, and employing
various subordinate algorithms for different purposes.

668

X. Zhang et al.

References
1. Pavel Berkhin: Survey of Clustering Data Mining Techniques. Technical report, Accrue
Software (2002)
2. Qian Wei-Ning, Zhou Ao-Ying: Analyzing Popular Clustering Algorithms from Different
Viewpoints. Journal of Software (2002)1382-1394
3. Hartigan J, Wong M.: A K-means Clustering Algorithm. Applied Statistics (1979)100-108
4. Raymond T.Ng, Jiawei Han: Efficient and Effective Clustering Methods for Spatial Data
Mining. Proceeding of the 20th VLDB Conference Santiago, Chile (1994)144-155
5. Ordonez C, Omiecinski E. FREM: Fast and Robust EM Clustering for Large Data Sets. In
ACM CIKM Conference (2002)590-599. http://citeseer.ist.psu .edu/536108.html
6. Karypis G, Han EH, Kumar V. CHAMELEON: A Hierarchical Clustering Algorithm
Using Dynamic Modeling. COMPUTER (1999)68-75
7. Zhang T, Ramakrishna R, Livny M. BIRCH: A New Data Clustering Algorithm and its
Applications. Journal of Data Mining and Knowledge Discovery (1997)141-182
8. Boley DL.: Principal Direction Divisive Partitioning. Technical Report TR-97-056, Dept.
of Computer Science, University of Minnesota, Minneapolis, to appear in Data Mining and
Knowledge Discovery (1997)
9. Boley DL.: Principal Direction Divisive Partitioning. Data Mining and Knowledge
Discovery (1998)325-344
10. Sheikholeslami G, Chatterjee S, Zhang A. WaveCluster: A Multi-resolution Clustering
Approach for Very Large Spatial Databases. In .Proceedings of the 24th Conference on
VLDB, New York, NY (1998)428-439
11. Wang W, Yang J, Muntz R. STING: A Statistical Information Grid Approach to Spatial
Data Mining. In Proceedings of the 23rd Conference on VLDB, Athens, Greece (1997)
186-195
12. Agrawal R, Gehrke J, Gunopulos D, Raghavan P.: Automatic Subspace Clustering of High
Dimensional Data for Data Mining Applications. In Proc.1998 ACM-SIGMOD Int. Conf.
Management of Data (SIGMOD’98), Seattle, WA June (1998)94-105
13. Ester M, Kriegel HP, Sander J, Xu X.: A density-based algorithm for discovering clusters
in large spatial database. In Proc.1996 Int. Conf.Knowledge Discovery and Data Mining
(KDD’96), Portland, OR, Aug (1996)226-231
14. Ankerst M, Breunig M, Kriegel HP: Sander J. OPTICS: Ordering points to identify the
clustering structure. In Proc.1999 ACM-SIGMOD Int.Conf Management of Data
(SIGMOD’99), Philadelphia, PA, June (1999) 49-60
15. Sergio M. Savaresi, Daniel L. Boley. On Performance of Bisecting k-means and PDDP. In
Proceedings of the 1st SIAM ICDM, Chicago, IL (2001)1-14
16. Sergio M. Savaresi etc. Choosing the Cluster to Split in Bisecting Divisive Clustering
Algorithms. CSE Report TR 00-055, University of Minnesota (2000)
17. Jinkun Lin. Basic of Topology. Beijing: Science Press (2003)19-22

