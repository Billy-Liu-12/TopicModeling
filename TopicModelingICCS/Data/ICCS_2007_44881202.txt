Concept-Based Question Analysis
for an Efficient Document Ranking
Seung-Eun Shin1, Young-Min Ahn2, and Young-Hoon Seo2
1

Chungbuk National University BK21 Chungbuk Information Technology Center
Cheongju, Chungbuk, 361-763, Korea
seshin@nlp.chungbuk.ac.kr
2
School of Electrical & Computer Engineering, Chungbuk National University
Cheongju, Chungbuk, 361-763, Korea
mania@nlp.chungbuk.ac.kr, yhseo@chungbuk.ac.kr

Abstract. This paper describes a concept-based question analysis for an
efficient document ranking. Our idea is that we can rank efficiently documents
containing answers for questions when we use well-defined concepts because
concepts occurred in questions of same answer type are similar. That is, we can
retrieve more relevant documents if we know the syntactic and semantic role of
each word or phrase in question. For each answer type, we define a concept rule
which contains core concepts occurred in questions of that answer type.
Concept-based question analysis is a process which tags concepts to
morphological analysis result of a user’s question, determines the answer type,
and extracts untagged concepts from it using a matched concept rule. Empirical
results show that our concept-based question analysis can rank documents more
efficiently than any other conventional approach. Also, concept-based approach
has additional merits that it is language universal model, and can be combined
with arbitrary conventional approaches.
Keywords: Concept, Concept rule, Question Analysis, Document Ranking,
Question Answering.

1 Introduction
Information retrieval (IR) techniques used to find information fast and exactly from
tremendous documents have been rapidly developed with the growth and commercial
application of the Internet. However, we can often find that high ranked documents
retrieved from a general web search engine may be far from a user intension.
Therefore, effective retrieval and rank techniques are needed to provide more relevant
documents to users or question answering (QA) systems are demanded for user’s
convenience.
The TREC 2005 QA track contained three tasks: the main question answering task,
the document ranking task, and the relationship task. The goal of the document
ranking task was to create pools of documents containing answers to questions. The
task was to submit, for a subset of 50 of the questions in the main task, a ranked list of
up to 1000 documents for each question [1].
Y. Shi et al. (Eds.): ICCS 2007, Part II, LNCS 4488, pp. 1202 – 1209, 2007.
© Springer-Verlag Berlin Heidelberg 2007

Concept-Based Question Analysis for an Efficient Document Ranking

1203

Current research trends tend to focus on applying a natural language processing
(NLP) technique for efficient document retrieval [2], [3], [4], [5]. However, such
approaches cannot effectively reflect the meaning of sentences because they use only
index terms extracted from the morphological analysis or n-gram method. Results of
all IR systems include many non-relevant documents because the index cannot
naturally reflect the contents of documents and because queries used in IR systems
cannot represent enough information in user’s question [6]. That is, the statistical IR
model cannot understand a user’s intention because it does not consider semantics of
index terms. That is an essential reason of the inaccurate IR.
Question analysis in QA determines answer type corresponding to question type
using named entities, answer type taxonomies, and ontologies such as WordNet in
addition to question analysis in IR systems [7], [8], [9]. The number of answer types
varies widely from single digits to a few thousands. The subdivided classification of
answer type helps to extract more accurate answer by reducing the number of answer
candidates in phase of answer extraction.
This paper describes a concept-based question analysis for an efficient document
ranking in which concept rather than keyword makes an important role on document
retrieval. We define concepts commonly occurred in the same type of questions, and
use those concepts on document ranking to retrieve more relevant documents.

2 Concept-Based Question Analysis
Concept-based question analysis applies NLP techniques on a user’s natural language
questions and analyzes them semantically for an efficient document ranking. For each
answer type, we define a concept rule which contains core concepts occurred in
questions of that answer type. Concept is a well-defined semantic component for each
answer type.
Let’s consider following questions whose answer type is either an author or time.
(Question 1) Who wrote Hamlet?
(Question 2) Who is the author of the novel, “The old man and the sea”?
(Question 3) When was the American Legion founded?
(Question 4) When was Hong Kong returned to Chinese sovereignty?
We can see concepts to be used commonly to represent the information demand of
users in questions (Question 1-2) whose answer type is an author. They are titles of
books (“Hamlet”, “The old man and the sea”), an interrogative pronoun (who), noun
to express the author (author), verb to express the author (write), and noun to express
the genre (novel). Concepts to be used commonly in questions (Question 3-4) whose
answer type is time are objects of an event (American Legion, Hong Kong), verbs to
represent an event (found, return), an interrogative pronoun (when), and nouns to be
related with an event (Chinese, sovereignty).
The statistical model extracts index terms (Who, write, Hamlet) from (Question 1)
and index terms (When, American Legion, found) from (Question 3). Then it
generates queries using index terms and ranks relevant documents by the querydocument similarity. Therefore, it will retrieve the document which includes simply
many index terms to more relevant document than the really relevant document which

1204

S.-E. Shin, Y.-M Ahn, and Y.-H. Seo

includes sentences such as “Shakespeare is the author of Hamlet”. Besides, users
consider the precision at top documents more important than the total precision
because most IR systems offer results over the hundreds of thousands of documents.
To solve this problem, we determine answer types and subtypes of answers, extract
concepts from user’s questions, and use them for an efficient document ranking.
A concept is not simple meaning of a word but the semantic role of a word or
phrase in sentences and is used to represent the user’s intention. We defined subtypes
of the answer and concepts from 643 questions whose answer type is a person in
TREC QA Data and Web, and we constructed a concept dictionary by tagging
concepts manually on those questions and expanded it using a synonym dictionary.
Table 1 shows a sample of subtypes of the answer and concepts. Subtypes of the
answer type ‘person’ are classified by 24 categories such as author, family,
prizewinner, politician, developer, inventor, scholar, entertainer, player etc. We are
currently defining 125 concepts.
Table 1. Sample of subtypes of the answer and concepts
Subtype of the answer
Common
Author
Family
Prizewinner
Politician

Concepts
Nationality, Time, Sex, Person, …
Book_Title, Author Noun, Author Verb, …
Relationship, Base Person, Relationship Info, …
Prize, Prize Noun, Prize Verb, Ceremony/Place, …
Position, Event, Organization, Election Noun, …

Table 2. Sample of concept tags and concept dictionary
Concept
Clue Adverb
Time
Who
Person
Author Noun
Author Verb
Genre
Relationship
Base Person
Book_Title

Tag
%Adverb
%Time
@Who
@Person
@Author_N
@Author_V
@Genre
@Relationship
#Base_Person
#Book_Title

Concept Dictionary
choi-cho(first), ma-ji-mak(last), …
nu-gu(who)
sa-ram, in-mul, bun(person), …
jeo-ja, jak-ga, geul-sseun-i(author), ...
jeo-sul-ha, jeo-jak-ha, sseu(write), …
chaig(book), so-seol(novel), su-pil(essay), …
a-deul(son), bu-in(wife), a-beo-ji, (father), …

Table 2 is a sample of concept tags and concept dictionary. Tag for each concept
consists of ‘Property’ + ‘Concept’. Properties are divided into three properties such as
‘%’, ‘@’, ‘#’. We are currently constructing the concept dictionary which includes
2,039 vocabularies.
Concepts with ‘%’ property are placed relatively free in sentence, and so they are
inappropriate to handle by rules. Some ‘%’ property concepts such as ‘%Adverb’ may
be extracted from concept dictionary, and others such as ‘%Time’ from rules. These
concepts are used importantly in document ranking though they are not used in
concept rule matching.

Concept-Based Question Analysis for an Efficient Document Ranking

1205

Concepts with ‘@’ property are extracted from concept dictionary. That is, we tag
a word or phrase to ‘@’ property concept if it is found in concept dictionary.
Concepts with ‘#’ property are ones to be extracted only from concept rules. Proper
nouns such as title of the book and person name belong to ‘#’ property concepts. ‘#’
property concepts are extracted from question after a concept rule is selected using
‘@’ property concepts.
We defined a concept rule manually for each answer type to extract concepts from
a user’s questions. Such a concept rule is represented as concepts and grammatical
morphemes in order to consider semantic and syntactic structure of the user’s
questions. Fig. 1 shows the BNF notation of the concept rule and characteristics of
Korean considered in our discussion. The concept rule consists of a list of <word
information> which consists of concepts and grammatical morphemes according to
the answer type.
<Concept Rule> ::= <Word List>
<Word List> ::= <Word Information> | <Word List><Word Information>
<Word Information> ::= ‘(’ <Concept> ‘)’ | ‘(’ <Concept> <Grammatical Morpheme> ‘)’
<Concept> ::= ‘@Who’|‘@Person’|‘@Author_N’|‘@Author_V’|‘@Genre’|
‘@Relationship’|‘#Base_Person’|‘#Book_Title’|...
<Grammatical Morpheme> ::= ‘jc’|‘jx’|‘jm’|‘etm’|‘co’|‘ef’|‘oj’|‘co+etm’
jc : case particle
co : copula
jx : auxiliary particle
etm : adnominal transition ending
jm : adnominal case particle
ef : sentence ending
oj : objective case particle
Fig. 1. BNF notation for the concept rule

The following examples are a concept rule for ‘author’ that is represented as an
extended BNF and process of concept-based question analysis for (Question 2).
•

Example of the concept rule for the author
……
3. (#Book_Title co+etm) (@Genre jm )(@Author_N jx?) (@Who)?
4. (#Book_Title jc) (@Author_V etm) (@Person|@Author_N jx?) (@Who)?
5. (#Book_Title jm) (@Author_N jx?) (@Who)?
……

< Concept-based question analysis for (Question 2) >
“No-in-gua ba-da”ran so-seol-ui jeo-ja-neun nu-gu-ib-ni-gga?
(Who is the author of the novel, “The old man and the sea”?)
• Morphological analysis result of (Question 2) :
“No-in-gua ba-da”/nc+i/co+ra-go/ec+ha/pv+neun/etm so-seol/nc+ui/jm jeoja/nc+neun/jx nu-gu/np+i/co+~b-ni-gga/ef
• Word List:
(“No-in-gua ba-da” co+etm) (@Genre jm) (@Author_N jx) (@Who co+ef)
• Concept rule :
(#Book_Title co+etm) (@Genre jm) (@Author_N jx?) (@Who)?
• Result of Concept-based Question Analysis for (Question 2)
•

1206

S.-E. Shin, Y.-M Ahn, and Y.-H. Seo
▪
▪
▪
▪

Answer type : in-mul (Person)
Subtype of the answer : jeo-ja (Author)
Book_Title : No-in-gua ba-da (The old man and the sea)
Genre : so-seol (novel)

We tag concepts to morphological analysis result of a user’s question and construct
the concept list. The concept list is matched by each concept rule. If there is a
matched concept rule, we determine answer type and extract concepts of ‘#’ property
using that concept rule. We select the longest rule when we have several matched
concept rules.
If there is no concept rule to be matched to a user’s questions, we extract concepts
from the user’s questions by following manner. We classified questions whose answer
type is a person whether it contains a verb or not. The question which includes a verb
consists of Event_V (verb to represent an event), Person (noun to represent a person),
Property_N (noun to represent the property of a person such as doctor, author, and so
on), and Who (interrogative). The question which does not include a verb consists of
Property_N, NP (noun phrase), and Who. Therefore, we designed the common
concept rule according to the syntactic structure of Korean natural language questions
as follow.
•

•

Common concept rule for the question which includes a verb
1. (#NP jc) (#Event_V etm) (@Person|@Property_N) (@Who)?
2. (#NP jx) (@Who) (#Event_V ef)
3. (@Who) (#NP jc) (#Event_V ef)
Common concept rule for the question which does not include a verb
1. (#NP jm) (@Property_N) (@Who)?
2. (#NP jc) (@Property_N) (@Who)?
3. (@Property_N) (@Who)?

Although we cannot determine the subtype of the answer, we can extract concepts
by using common concept rules from a user’s question which has not the concept rule
to be applied.

3 Document Ranking
We generate queries that reflect various syntactic structures to represent the answer
and utilize them for an efficient document ranking. We can retrieve a document that
includes generated queries as a relevant document and improve the precision of
document retrieval. Other approaches that do not analyze the user’s question
semantically can hardly obtain queries whose syntactic structure is different from the
user’s question, but our approach can generate queries using the answer type and
concepts as results of concept-based question analysis.
We designed query generation concept rules in order to generate queries, and it is
made of concepts and grammatical morphemes. The query generation concept rules
have concepts and syntactic structures used to represent an answer. We can generate

Concept-Based Question Analysis for an Efficient Document Ranking

1207

queries using the query generation concept rules, concepts, concept dictionary, and
synonym dictionary. Query generation concept rules for an author are examples of
query generation concept rules.
•

Query generation concept rules for an author
……
3. (Book_Title jm) (Author_N)
4. (Book_Title co+etm) (Genre oj) (Author_V)
……

We can generate queries using query generation concept rules and results of the
concept-based question analysis of (Question 2) as below.
< Examples of the query generation >
• Query generation concept rule 3 : (Book_Title jm) (Author_N)
▪ Generated queries : “No-in-gua ba-da”-ui jeo-ja|jak-ga|……|geul-sseun-i
(writer|author of “The old man and the sea”)
• Query generation concept rule 4 : (Book_Title co+etm) (Genre oj) (Author_V)
▪ Generated queries :
“No-in-gua ba-da”-ra-neun so-seol-eul jeo-sul-ha|jeo-jak-ha|……|sseu
(write|wrote|……|to compose the novel, “The old man and the sea”)
Our approach can generate queries which have the same meaning as the original
question, but have different structures such as above example. Generated queries
reflect the syntactic structures of almost all phrases which have answers for that
question. We retrieve a document that includes generated queries as a relevant
document because they consist of concepts and syntactic structures used to represent
an answer.
Formula (1) is a transformation of the cosine coefficient to determine querydocument similarity when our approach is combined with the vector model [10]. We
calculate the query-document similarity by using formula (1) to retrieve a document
which includes generated queries as a relevant document.
if

G G
G G
di ⋅ qge ≠ 0 then sim(di , qu ) = di ⋅ qge
G G
d ⋅q
| d i || qu |

(1)

else sim ( d i , qu ) = G i Gu

G

di : document, qu : user’s question, d i : document vector
G
G
qge : generated query vector, qu : query vector by query expansion
We can rank documents by formula (1) because queries are generated by concepts
and syntactic structures which are used to represent an answer. We can increase the
precision at document retrieval by ranking documents which include a generated
query in the high position.

1208

S.-E. Shin, Y.-M Ahn, and Y.-H. Seo

4 Experimental Results
We randomly selected 100 questions as a test set from natural language questions
whose answer type is a person. They were questions which were used actually for IR
in the Web. We measured the precision at N documents. In our experiments, we used
Google and Yahoo as the IR systems and used only top 30 results of such systems for
the precision at N documents.
Table 3 shows the accuracy of concept-based question analysis. Accuracy of
concept extraction is (the number of concepts which are extracted correctly)/(the total
number of concepts). Table 4 shows the precision at N documents of the document
ranking result.
Table 3. Accuracy of concept-based question analysis

The number of questions
Accuracy of
answer type determination
Accuracy of
subtype determination
Accuracy of
concept extraction

Question which concept
frame is applied
69

Question which common
concept rule is applied
31

1.000

0.903

1.000

We cannot determine the
subtype of the answer

0.918

0.845

Table 4. Precision at N documents of the document ranking result

N

Micro Averaging Precision
Google+
Google
Yahoo
Our approach

Yahoo+
Our approach

0.804(+0.224)
0.580
0.803(+0.219)
0.584
At 3 docs
0.743(+0.186)
0.557
0.770(+0.185)
0.585
At 5 docs
0.604(+0.093)
0.511
0.646(+0.098)
0.548
At 10 docs
0.541(+0.063)
0.478
0.591(+0.068)
0.523
At 15 docs
0.506(+0.048)
0.458
0.541(+0.052)
0.489
At 20 docs
Precision at N documents: The percentage of documents retrieved in the top N
that is relevant. If the number of documents retrieved is fewer than N, then all
missing documents are assumed to be non-relevant.

In case that our approach is applied to Google and Yahoo, the test of precision at N
documents was improved by +0.2215(N=3), +0.1850(N=5) and +0.0955 (N=10). If
our approach is applied to more documents, the precision at N documents can be
improved more than that of table 4. In addition, we found that it is possible to make
document ranking more efficient by analyzing questions based on concepts which are
comparatively short but fully expressing a user’s intentions.

Concept-Based Question Analysis for an Efficient Document Ranking

1209

5 Conclusion and Future Work
In this paper, we proposed a concept-based approach for an efficient document
ranking. Concept-based question analysis extracts concept components from
morphological analysis result for a user’s question, determines answer type, and
generates queries using extracted concepts. And then, we rank document which
include a generated query in the high position as a relevant document.
We applied our concept-based question analysis to document retrieval system,
Google and Yahoo, and obtained a notable improvement (+0.2215, N=3) in the
precision at N documents. Although we make an experiment in the restricted domain
in which questions require a person name as its answer, our concept-based approach
can retrieve more relevant documents than any other conventional approach. Also, our
approach has additional merits that it is a language universal model, and can be
combined with arbitrary conventional approaches in the method that concept-based
approach is used when the given question can be analyzed to one of the defined
concept rules, and other approach is used otherwise.
We plan to expand concept rules for various domains, and expect incremental
performance improvement.
Acknowledgments. This research was supported by the Ministry of Information and
Communication, Korea under the Information Technology Research Center support
program supervised by the Institute of Information Technology Assessment, IITA2006-(C1090-0603-0046).

References
1. Ellen M. Voorhees and Hoa Trang Dang: Overview of the TREC 2005 Question
Answering Track. TREC 2005, (2005)
2. A.T. Arampatzis, T. Tsoris, C.H.A. Koster and Th.P. van der Weide: Phrase-based
Information Retrieval. Journal of Information Processing & Management, 34(6), (1998)
693–707
3. Boris V. Dobrow, N.V. Loukachevitch and T.N. Yudina: Conceptual Indexing Using
thematic Representation of Texts. TREC-6, (1997)
4. Jose Perez-Carballo and Tomek Strzalkowski: Natural language information retrieval:
progress report. Journal of Information Processing & Management, 36(1), (2000) 155–178
5. C. Zhai: Fast Statistical Parsing of Noun Phrases for Document Indexing. In Proceedings
of the Fifth Conference of Applied Natural Language Processing, (1997)
6. S. H. Myaeng: Current Status and New Directions of Information Retrieval Technique.
Communications of the Korea Information Science Society, 24(4), (2004) 6–14
7. A. Ittycheriah, M. Franz, W. Zhu, A. Ratnaparkhi: IBM’s Statistical Question Answering
System. In 9th Text Retrieval Conference, (2000) 229–334
8. S. Haragagiu, M. Pasca, S. Maiorano: Experiments with open-domain with open-domain
textual question answering. In COLLING-2000, (2000) 292–298
9. M. Pasca, S. Harabagui: High Performance Question / Answer. In 24th Annual
International ACM SIGIR Conference on Research and Development in Information
Retrieval, (2001) 366–374
10. G. Salton: Automatic Text Processing, Addison-Wesley, (1989)

