Co-evolving a Neural-Net Evaluation Function
for Othello by Combining Genetic Algorithms
and Reinforcement Learning
Joshua A. Singer
Stanford University

Abstract. The neural network has been used extensively as a vehicle for
both genetic algorithms and reinforcement learning. This paper shows a
natural way to combine the two methods and suggests that reinforcement
learning may be superior to random mutation as an engine for the discovery of useful substructures. The paper also describes a software experiment that applies this technique to produce an Othello-playing computer
program. The experiment subjects a pool of Othello-playing programs to
a regime of successive adaptation cycles, where each cycle consists of an
evolutionary phase, based on the genetic algorithm, followed by a learning phase, based on reinforcement learning. A key idea of the genetic
implementation is the concept of feature-level crossover. The regime was
run for three months through 900,000 individual matches of Othello. It
ultimately yielded a program that is competitive with a human-designed
Othello-program that plays at roughly intermediate level.

1

Background

This paper describes a way to search the space of neural-net functions using an
approach that combines the genetic algorithm with reinforcement learning. The
hybrid approach is founded on the intuition that crossover and gradient descent
exploit diﬀerent properties of the ﬁtness terrain and can thus be synergistically
combined. A secondary idea is that of feature-level crossover: each hidden node
of a three-layer neural-net may be thought of as extracting a feature from the
net’s input. It is these features that are mixed by the crossover operation.
The paper also describes a software experiment that employs this hybrid
approach to evolve a pool of Othello-playing programs. The best evolved program played about as well as kreversi, the hand-crafted Othello program written by Mats Luthman and ported to KDE/LINUX by Mario Weilguni. (See
www.kde.org for more information.) Othello, also known as Reversi, is a popular
two-player board-game whose rules are described brieﬂy in section 1.3.
1.1

Neural Nets as Game-Players

Many people have used neural-nets as the basis for playing games, notably
Tesauro [1], whose TD Gammon program applied reinforcement learning to a
neural-net in self-play and greatly surpassed all previous eﬀorts at computer
V.N. Alexandrov et al. (Eds.): ICCS 2001, LNCS 2074, pp. 377–389, 2001.
c Springer-Verlag Berlin Heidelberg 2001
�

378

J.A. Singer

backgammon, and recently Chellapilla and Fogel [2], who applied a mutationonly genetic algorithm to a pool of neural-nets, producing a net that could play
checkers at near-expert level. Neural-net approaches to Othello in particular include that of Moriarty and Miikkulainen [3], who applied the genetic algorithm
to a ﬂexible encoding representation that allowed them to evolve multi-layer,
recurrent neural-net topologies.
1.2

The Impetus for a Combined Approach

The goal of all learning methods can be roughly characterized as the search for an
optimal point within some space, where optimality usually means maximizing a
ﬁtness function or minimizing a cost function on the space. In the case of Othello,
the search space is a parameterized space of static evaluation functions, and the
point searched for, u∗ , is the parameter whose associated function fu∗ (x) most
closely approximates the true value function of the game. In general, a global
optimum cannot be found, and we content ourselves with methods that can ﬁnd
points that are highly ﬁt (very close to optimal).
There are at least two reasonable ways to search for points with high ﬁtness.
One is to use gradient-descent to ﬁnd a local optimum. This is the method used
by reinforcement learning. The other is to use crossover, and in so doing to make
what we might call the decomposability assumption: that the ﬁtness of a point
in the space can be approximated as the sum of ﬁtness-values associated with
substructures of the point. For example, suppose the search space is Rn with
associated ﬁtness function F , and that we can decompose Rn into the crossproduct of subspaces Rm and Rk , each with their own ﬁtness-functions, F1 , and
F2 , in such a way that when we decompose u ∈ Rn as u = (u1 , u2 ), u1 ∈ Rm ,
u2 ∈ Rk , we ﬁnd that F (u) is approximately equal to F1 (u1 ) + F2 (u2 ). Then
the original ﬁtness terrain, (Rn , F ), is a decomposable ﬁtness terrain.
As an example of how crossover works to improve ﬁtness in a decomposable
ﬁtness terrain, consider two parameters, u = (u1 , u2 ) with ﬁtness-values (50, 1)
for a total ﬁtness of approximately 51, and z = (z1 , z2 ) with ﬁtness-values (2, 60),
for a total ﬁtness of approximately 62. If these are crossed to yield the two
child parameters c1 = (u1 , z2 ) and c2 = (z1 , u2 ), then c1 will have a ﬁtness of
approximately 110.
Crossover has a weakness though: it can’t make something from nothing.
Given a pool of points with low-ﬁtness, possessed of no useful substructures,
crossover is no more likely than macromutation to yield a point with improved
ﬁtness [4].
Gradient-descent does not possess this weakness of crossover. While, in the
standard genetic algorithm, mutation is the motive force for discovering useful
substructures, in this combined approach, gradient-descent can be used to ﬁnd
useful substructures much more quickly. No matter how unﬁt a particular point
is, it can be used as the starting point for gradient-descent, ultimately leading
to a locally optimal point. Furthermore, it is a fairly safe guess that this point,
being a local optimum, possesses at least one good substructure!

Co-evolving a Neural-Net Evaluation Function for Othello

379

Consider then the following scenario: we start with ten randomly chosen
points in the ﬁtness terrain. The ﬁtness of each being poor, we don’t expect
crossover to yield any interesting results. So instead we perform ten gradientdescent searches, yielding ten diﬀerent local optima. Presumably each of these
local optima possesses at least one good substructure, and, since they come
from diﬀerent portions of the ﬁtness terrain, it may well be that they possess
ten diﬀerent good substructures. Now crossover has a lot to work with. If we
cross pairs of points from this pool of local optima, we are very likely to produce
new points of higher ﬁtness than any in the original pool.
We therefore have the following situation: given any point that is not already
a local optimum, we can use gradient-descent to ﬁnd a local optimum, yielding
a new point with improved ﬁtness. And, given any two points with relatively
high ﬁtness, we can use crossover to yield, with high probability, a new point
with improved ﬁtness. But if this new point truly has improved ﬁtness over its
parents, then it can’t reside in the search space near either of its parents, since
its parents are local optima. Therefore, we can use gradient-descent again on this
new point to yield a new local optimum with still better ﬁtness. By interleaving
the two methods, we hope to produce a sequence of points with ever-increasing
ﬁtness.
The impetus then for the combined approach is to achieve a synergistic interplay between reinforcement learning, which ﬁnds local optima, and the genetic
algorithm, which can combine local optima to produce new points with still
higher ﬁtness.
1.3

How Othello Is Played

Othello is played on an 8x8 board. One player plays as White, the other as Black.
Players take turns placing a stone of their color on an unoccupied square until
all the squares have a stone on them, or until neither player has a legal move.
The player with the most stones on the board at the end of the game wins.
Every time a player places a stone, he causes some of the stones that his
opponent has already played to “ﬂip” colors. So, for example, every time White
plays, some black stones on the board change to white stones, and every time
Black plays, some white stones on the board change to black stones. The exact
way in which this stone-ﬂipping occurs is covered in the rules for Othello, which
can be found at http://www.othello.org.hk/tutorials/eng-tu1.html.
The game’s ﬁnal score is v = WW
+B . Under this scoring method, .5 is a tie.
Scores higher than .5 are wins for White, and scores lower than .5 are wins
for Black. In typical game-theoretic fashion, White tries to maximize the score,
while Black tries to minimize it.
1.4

Game-Playing and Evaluation Functions

Virtually all game-playing programs contain a subroutine called an evaluation
function. The evaluation function is an approximation to its game-theoretic counterpart: the value function. To each possible game-state, the value function as-

380

J.A. Singer

signs a value that indicates the outcome of the game if play were to continue
optimally, both players making their best possible moves from that state onward.
In Othello, a game-state consists of two pieces of information: the board-position,
and whose move it is.
At any point during a game, the player whose move it is has several possible
legal moves, each leading to a diﬀerent successor game-state. We denote the set
of successor game states of a given state x by Succ(x). If White knew the value
function, he would select the move leading to the highest-valued successor state.
Similarly, if Black knew the value function, he would select the move leading to
the lowest-valued successor state.
The value function is not discoverable in practice, so instead we approximate
it with what we call an evaluation function. The evaluation function is obtained
by applying the minimax algorithm to what we call a static evaluation function:
a parametrized function from some function space, fu (x) ∈ F. Here, x is a
vector representing a game-state, and u, called a parameter, is a vector of values
that selects a particular function from the family F. For the experiment, we use
neural-nets as static evaluation functions, which means that the parameter u
consists of the weight-values and bias-values that uniquely determine the net.
The evaluation function, which we’ll denote h(x; d, f ), evaluates a gamestate x by expanding the game tree beneath x to some speciﬁed search-depth, d,
applying f to the terminal nodes of the expansion, and then using the minimax
algorithm to trickle these values back up to the top to yield a derived value for
x. Speciﬁcally, for a given static evaluation function f and a ﬁxed depth d, the
evaluation of a game state x is

f (x),
d=0








 � max h(x� , d − 1; f ), d � = 0,

 x ∈Succ(x)
�
White-to-play
h(x; d, f )=






min h(x� , d − 1; f ), d � = 0,


 x� ∈Succ(x)


Black-to-play
1.5

Choosing a Move

When it is a player’s turn to move, it must decide which of the successors of
the current state x it should move to. In the experiment, a player makes a
weighted random choice, based on the relative values of each move, using the
following exponential weighting scheme: let x�1 . . . x�n be the successors x. Then
the probability of selecting the move that leads to successor x�i is:

T h(x� ;d,f )
i

x is a White-to-play state

�e T h(x� ;d,f )

j

e

j
pi =

−T h(x� ;d,f )

i

 �e −T h(x
x is a Black-to-play state
� ;d,f )

j
e
j

Co-evolving a Neural-Net Evaluation Function for Othello

381

where T is a tuning parameter that controls how tight or dispersed the probability distribution is. Higher values of T make it more likely that the best-rated
move will actually be selected. The experiment uses a value of T = 10. At
T = 0, all moves are equally likely to be chosen, regardless of their evaluation.
At T = 10, if move A has an evaluation .1 greater than move B, then move A is
e10·.1 = e times more likely to be chosen than move B.
1.6

Neural-Networks as Evaluation Functions

This project uses neural-networks as static evaluation functions. Each neuralnetwork has a 192-node input layer, three hidden nodes, and a single output
node. A network in learning mode will use backpropagation to modify its weights.
�
Network nodes have standard sigmoid functions g(z)=(1 + e−z )−1 .
The input layer accepts a 192-bit encoding of the game state. When an
Othello-player wants to evaluate a game-state, it passes the properly-encoded
game-state to its neural-network and interprets the neural-network’s output as
the result of the evaluation.
A game state x is encoded as the concatenation of three 64 bit vectors,
q = q1 . . . q64 , r = r1 . . . r64 , and s = s1 . . . s64 . That is, Encode(x) = qrs. These
64 bit vectors are determined as follows:
�
1 square j of x has a white stone
qj =
0 otherwise
�
1 square j of x has a black stone
rj =
0 otherwise
�
1 square j of x is empty
sj =
0 otherwise
For the above scheme, it is inconsequential how one numbers the squares of
the Othello board, so long as they each have a unique number from 1 to 64.

2

Experiment Design

We begin with a pool of players, each with its own randomly drawn neuralnetwork. These players then undergo a regime of alternating evolutionary and
learning phases.
In the evolutionary phase, the current generation of players gives rise to a
next generation of players through the genetic algorithm; that is, through the
mechanisms of mutation, crossover, and replication. Fitness values are assigned
to the players based on their rankings in a single round-robin tournament.
In the learning phase, each player in the newly created generation augments
the talent it has inherited from its parents by playing successive round-robin
Othello tournaments, modifying its static evaluation function f u (x) as it learns
from experience, using the reinforcement learning algorithm.

382

J.A. Singer

A round-robin tournament is simply a tournament in which each player plays
a game against every other player exactly once. In the evolutionary phase, the
result of a single round-robin tournament determines the ﬁtness of the players.
In the learning phase, the outcomes of the successive round-robin tournaments
are irrelevant. The players are in eﬀect just playing practice games.
In both phases, when it is a player’s turn to move, the player determines which
move to make as described in section 1.5. That is, it evaluates the successors of
the current state and chooses one of the successors randomly, based on relative
value. The evaluation is performed as described in section 1.4, with the player’s
neural-net acting as the static evaluation function f , and using a search-depth
d = 1.
At the end of each learning phase, the weights of the neural-nets may be
substantially diﬀerent than they were at the beginning of the learning phase. In
the subsequent evolutionary phase it is these new weights that are carried over
into the next generation via crossover and replication operations. Thus, this
regime is Lamarckian: the experience gained by the nets during their “lifetimes”
is passed on to their progeny.
2.1

Evolutionary Phase

The evolutionary method used in this project is the genetic algorithm. The evolutionary phase pits the players against each other in a round-robin tournament,
assigning to each player as a ﬁtness value the fraction of games that it won in
the tournament. To be precise, the ﬁtness measure is:
f itness =

1
GamesT ied
GamesW on
+ 2
T otalGames
T otalGames

which gives some credit for ties and always falls in [0, 1]. Players in this generation
are then mated to produce the next generation: a new set of players, produced as
the children of players in the current generation via the operations of crossover,
mutation, and replication. The experiment used a generation size of 10.
In crossover, two players from the current generation are selected at random,
but based on ﬁtness, so that players with higher ﬁtness values are more likely
to be selected, and their neural-network evaluation functions are merged—in a
way to be described in detail later—to produce two new child players in the new
generation. In replication, a single player from the current generation is selected
at random, but based on ﬁtness, and copied verbatim into the new generation.
Every child of the new generation thus created is subject to mutation: with a
certain probability, some of the weights or biases on its neural-net are randomly
perturbed.
Because of the selection pressure the genetic algorithm imposes, poorlyperforming players are likely to get culled from the population. More importantly, if two diﬀerent players discover diﬀerent features of the game-state that
are useful for approximating the value function, these features can be combined
via crossover into a child whose neural-net employs both features in its evaluation
function.

Co-evolving a Neural-Net Evaluation Function for Othello

383

Feature-Level Crossover
Given the topology of the neural nets in the experiment, the function they represent is always of the form:
fu (x) = g(b +

3
�

ωi g(bi +

i=1

192
�

wij aj ))

j=1

where aj is the activation of the j th input node, wij is the weight to the ith
hidden node from the j th input node, bi is the bias of the ith hidden node, g is
the sigmoid function, ωi is the weight from the ith hidden node to the output
node, and b is the bias of the output node.
One way to view the above function is as follows:
fu (x) = g(b +

3
�

ωi φi (x))

i=1

where
�

φi (x)=g(bi +

192
�

wij aj )

j=1

The φi can be thought of as features of the raw game state. Their normalized,
weighted sum (plus a bias term) then constitutes the evaluation function. Under
this view of the neural-net’s structure, the hidden nodes, which implement the
functions φi above, each extract a feature of the raw game-state. Depending on
the initial weight-settings of the nets before learning begins, the hidden nodes
of various nets may learn to represent quite diﬀerent features of the raw game
state, and some of these features may be good predictors of the value function.
If we assume that the value function can be approximated by a roughly
linear combination of more or less independent features of the game state, and if
a hidden node is capable of representing such features, then any net possessing
even one such feature will very likely be more ﬁt than a net possessing none,
and a net possessing two diﬀerent features will be more ﬁt still.
Given the above assumptions, the ﬁtness terrain is decomposable, with a
point u ∈ F decomposing as
u = (φ1 , φ2 , φ3 , ω1 , ω2 , ω3 , b)
where the φi are the above feature functions, expressed as vectors whose components are the bias and all incoming weights of the ith hidden node. That is,
the components of φi are the elements {wij , bi }, j = 1 . . . 192.
The crossover operation is structured to take advantage of the presumed
decomposability of the ﬁtness terrain. It combines nets at the feature-level, rather
than at the lowest-, or weight-level. When two nets are selected to mate in
crossover, their φi are pooled to create a set of 6 features, from which three
are drawn at random to be the features of the ﬁrst child. The remaining three
become the features of the second child. In addition, the ωi of each child are

384

J.A. Singer

set to that of the corresponding inherited φi . For example, if a child inherits φ2
from parent 1, then it also inherits ω2 from parent 1. Finally, the b value of each
child is randomly inherited from one of its parents. The important point is that
the set of values that determine a feature are never disturbed through crossover.
All the crossover operation does is determine which combination of the parents’
features the children will inherit.
Mutation
For every new child created, either through crossover or reproduction, there is a
1/10 probability that the child will have a mutation in precisely one of its weights
or node-biases. When a mutation occurs, the current weight or bias is thrown
away and replaced with a new value drawn uniformly on [−.7, .7]. This is a very
low mutation rate, and, unless the mutation is suﬃcient to knock the neural
net’s parameter out of the local bowl in which it resides into a qualitatively
diﬀerent part of the ﬁtness terrain, the next learning-phase will probably just
pull it right back to very nearly the same local optimum it occupied before the
mutation. Mutation plays a minor role in the experiment because the gradientdescent behavior of reinforcement learning, rather than mutation, is supposed
to provide the impetus for the discovery of useful substructures.
2.2

Learning Phase

The learning method used in this project is reinforcement learning. During the
learning phase, the Othello players modify the weights and biases of their neuralnets with every move of every game they play, in accordance with the reinforcement learning algorithm. The method is simple. At each game state, x, P layer1
passes the training pair (x, h(x; fu1 , d)) to its neural-net, and u1 is updated
accordingly. Similarly, P layer2 passes the training pair (x, h(x; fu2 , d)) to its
neural-net. After both players have updated their nets based on game state
x, the player whose turn to move it is chooses a move using the ﬁtness-based
random-selection method described in section 1.5, and play continues.
A round-robin tournament pits every player against every other exactly once.
So, with a pool of 10 players, a tournament consists of 45 games. This experiment
conducts 200 tournaments in each learning phase. The ﬁrst 100 are conducted
with the learning parameter of the nets set to .1 so that their weights will
quickly hone-in on the bottom of the local error bowl. (Not to be confused with
the weight vector parameter u of the net, the learning parameter, λ, is a scalar
that determines how much the net alters u as the result of a single training pair.
Higher values of λ result in greater changes to u. Lower values of λ are used for
ﬁne-tuning, when u is already close to optimal.) Then the next 100 tournaments
are conducted with the learning parameter of the nets set to .01, for ﬁne tuning.
Endgame Calculation
Othello has a nice property: with each move made, the number of empty squares
left on the board decreases by one. Therefore, once the game reaches the point

Co-evolving a Neural-Net Evaluation Function for Othello

385

where there are only a handful of empty squares left on the board, the remaining
game-tree can be completely calculated.
During the learning phase, the players perform endgame calculation when
there are 10 empty squares left. They then perform one ﬁnal learning-step apiece,
using as a target value v(x), the true value of the game-state, obtained from the
endgame calculation. That is, when x represents a game state that has only 10
empty squares left, then the ﬁnal learning pair (x, v(x)) is passed to the neuralnets of both P layer1 and P layer2 and the game ends. (Note that which player
wins the game is irrelevant to the learning phase.)
It is important to stress that whereas all previous learning-steps only use
the approximation h(x; fu , d) for a target, the ﬁnal learning-step uses the true
game-state value, v(x), obtained by performing a complete game-tree expansion
from x and calculating WW
+B at each of the terminal nodes of the expansion.
Speciﬁcally, we have

W
x is a terminal game-state

W +B






�
max v(x� ) not terminal and White-to-play
v(x)= x� ∈Succ(x)






 � min v(x� ) not terminal and Black-to–play
x ∈Succ(x)

3

Results and Analysis

The experiment ran for 100 generations, taking three months to complete, during
which time 900, 000 individual games were played at a rate of approximately
one game every 9 seconds. At the end of the experiment, a ﬁnal round-robin
tournament was played among the 100 best players of each generation, assigning
to each player as a ﬁnal ﬁtness rating the fraction of games won, modiﬁed to
account for ties:
f itness =

1
GamesT ied
GamesW on
+ 2
T otalGames
T otalGames

Unlike while in learning mode, when the experiment used a search-depth of 1,
the ﬁnal tournament used a search-depth of 4. A search-depth of 4 is still very
modest—the nets can calculate that in a fraction of a second—and probably
gives a more robust appraisal of the evaluation functions.
Figure 4 at the end of this paper shows the ﬁtness of these best-of-gen players
as a function of generation number. One can clearly see that ﬁtness steadily
improves for the ﬁrst twelve generations, after which it levels oﬀ at just a little
above .5. The highest-ﬁtness player is BestOf Gen29 , with a rating of of .68.
For some comparison with a human-written program, BestOf Gen29 was
played against kreversi, a well-known program available with KDE for LINUX.
kreversi plays a modiﬁed positional game. A purely positional game assigns
diﬀerent values to each square of the board and evaluates a game state as the

386

J.A. Singer

sum of the values of the squares occupied by white minus the the sum of the
values of the squares occupied by black. kreversi uses a convex combination of
position and piece-diﬀerential to evaluate its game states, with piece-diﬀerential
receiving higher and higher weight as the game progresses. kreversi also uses a
dynamic search-depth: states near the end of the game are searched more deeply.
BestOf Gen29 played a twenty game tournament against kreversi on its level
three setting, which uses a minimum search-depth of 4, with the following results:
as white, BestOf Gen29 won 7 games and lost 3. As black, BestOf Gen29 won
3 games, lost 6, and tied 1.
3.1

Diverse Features Failed to Develop

Casual analysis of the best-of-gen nets fails to reveal that any interesting
crossover took place. In net after net, the three hidden nodes end up with the
same general pattern of weight values. Figures 2 and 3 illustrate this similarity.
It is possible that, given the ﬁxed three-layer topology, there is only one useful
feature to be discovered. On the other hand, it is possible that the three hidden
nodes are diﬀerent in subtle ways not so easily detectable to the eye. They may
in fact provide a useful redundancy, allowing the nets to be more robust. In this
sense, the three hidden nodes are like three slightly diﬀerent Othello-evaluators
that are “bagged” together by the overall network. (Bagging is a method in
statistics of averaging the results of diﬀerent estimators in order to obtain a
lower-variance estimate.)
How to Read Figures 1—3
Figure 1 on the last page shows the values of the weights incoming to the hidden layer of neural-net BestOf Gen1 . Each of the net’s hidden nodes has 192
incoming weights. This set of incoming weights is represented graphically as a
column of Othello boards: a white board, a black board, and an empty board
(3·64 = 192). Since the net has three hidden nodes, the ﬁgure has three columns,
one for each node. The squares of the boards are drawn with ﬁve diﬀerent gray
levels; white codes the highest value range, [0.6, 1.0], and black codes the lowest
value range, [−1.0, −0.6] The color of the square represents the value of the associated incoming weight. Above each column of boards is printed the weight of the
outgoing link on that hidden node. A hidden node with a relatively higher outgoing weight than the other two hidden nodes will participate more signiﬁcantly
in determining the ﬁnal output value of the net.
Figures 2 and 3 are drawn in exactly the same format. Note that the biases
of the nets are not shown on any of the ﬁgures.
Evaluation Functions Learned to Like Corners and Dislike CornerNeighbors
Probably the single most important piece of strategic advice to give any beginning Othello player is: get the corners. An immediate corollary is to avoid

Co-evolving a Neural-Net Evaluation Function for Othello

387

placing a stone on any square adjacent to a corner, since this usually enables
one’s opponent to grab the corner.
Reviewing the feature plots, one can see that, by generation 29, the nets have
learned to value the corners. This is reﬂected by high-values (light shades of gray)
in the corner squares of the white board and low-values (dark shades of gray) in
the corner squares of the black board. Remember that the evaluation functions
always assess the board-position from White’s point of view, so a white-stone in
the corner will be good (high value), whereas a black-stone in the corner will be
bad (low value).
By generation 29, not only have the corners of the white-board become high
values, but the adjacent squares have become low values. Thus, at the very least,
we can conclude that the nets have learned the fundamentals.
The Fitness Plateau
It is perhaps somewhat surprising that the ﬁtness levelled-oﬀ so quickly. One
might have expected ﬁtness to increase steadily throughout all 100 generations.
But one must remember that each generation comprises not only a geneticmixing step, but also 200 learning tournaments, consisting of 45 games each.
Thus, by generation 30, 9000 games are played. It may be that this was enough
for the reinforcement learning phases to reach convergence. Had a more complex
net topology been chosen, it is possible that the nets would have learned more
and longer.
However, it is not surprising in itself that a plateau occurred. Indeed, this
is almost inevitable. Once a fairly high-level of ﬁtness is achieved, subsequent
genetic pairings are likely to either leave ﬁtness unchanged or reduce it, since
there are many ways to go down, and few ways left to climb, the ﬁtness terrain.
And, given that a plateau occurs, the ﬁtness level of those players in the plateau
must perforce be around .5. Since, by assumption, they all have about the same
ﬁtness, the probability that one player from the plateau group beats another
from the plateau group must be roughly .5.

4

Conclusions and Future Directions

The central idea of this paper is that, in domains where it can be applied,
reinforcement learning may be better than random mutation as an engine for
the discovery of useful substructures. A secondary idea is that of feature-level
crossover: that the substructures we wish to discover may be identiﬁed with
the hidden nodes of a neural net. Feature-level crossover is merely a way to do
crossover on neural-nets. It needn’t be coupled with reinforcement learning, nor
restricted to ﬁxed-topology nets.
As a side note, the distinction between the terms substructure and feature is
actually one of genotype and phenotype. In the space of neural-net functions,
the substructures that confer high ﬁtness are simply those that lead to useful
feature-extractors in the phenotype.

388

J.A. Singer

Whether or not the Othello ﬁtness terrain is a decomposable terrain is an
open question. If it is not, then the crossover operation should perform no better than macromutation in ﬁnding points of improved ﬁtness [4]. It is however
reasonable to suspect that the Othello ﬁtness terrain can be characterized by
more or less independent features, because this is how humans analyze board
positions. It is also an open question whether reinforcement learning applied to
Othello can lead to the discovery of features along the lines suggested by this
paper. Although the experiment yielded a fairly good player, there is little indication that diverse features were discovered, or that crossover played a signiﬁcant
role.
Further, any hybrid strategy, being inherently more complex than a single
method, requires justiﬁcation based on its practical worth, the more so because
it seems that mutation alone can still be very eﬀective in ﬁnding points of high
ﬁtness [2,7]. While this paper does not investigate the eﬃcacy of the hybrid
method relative to other methods, that is an important direction for future
research.
Acknowledgments. I would like to thank John Koza for his many helpful
comments and suggestions.

References
1. G. Tesauro, “Temporal Diﬀerence Learning and TD-Gammon”. Communications
of the ACM, vol. 38, no. 3, pp. 58-68, 1995
2. K. Chellapilla and D. B. Fogel, “Evolution, Neural Networks, Games, And Intelligence”. Proceedings of the IEEE, vol. 87, no. 9, pp. 1471-96, 1999
3. D. E. Moriarty and R. Miikkulainen, “Discovering Complex Othello Strategies
Through Evolutionary Neural Networks”. Connection Science, vol. 7, no. 3-4, pp.
195-209, 1995
4. Terry Jones, “Crossover, Macromutation, and Population-Based Search”, Proceedings of the Sixth International Conference on Genetic Algorithms
5. Dimitri Bertsekas and John Tsitsiklis, Neuro-Dynamic Programming. Belmont,
Massachusetts: Athena Scientiﬁc, 1996.
6. John Koza, Genetic Programming. MIT, 1996
7. J. B. Pollack and A. D. Blair, “Co-Evolution in the Successful Learning of
Backgammon Strategy”, Machine Learning, vol. 32, no. 3, pp. 225-40, 1998
8. Brian D. Ripley, Pattern Recognition And Neural Networks. Cambridge University
Press, 1996.
9. Richard Sutton and Andrew Bartow, Reinforcement Learning. Windfall Software,
1999.

Co-evolving a Neural-Net Evaluation Function for Othello

4

6

4

6

4

6

2

8

2

2

2

4

4

4

4

6

2

8

4

6

2

2

2

4

4

4

6

6

6

8

8

8

2

Net 1

4

6

2

8

4

6

4

4

6

8

Feature 2
Weight = 0.600000

2

8

4

6

8

2

2

2

4

4

6

4

6

8

2

2

2

4

4

4

6

6

6

8

8

8

Net 29

4

2

2

8

6

6

8

2

4

6

8

2

4

6

8

8
2

8

4

6

6

8
4

2

2

4

6

8

Fig. 2. the best-of-run player, from
generation 29. Note the high value
placed on the corners

Fig. 1. The best player of generation 1–a random neural-net

Feature 1
Weight = 0.700000

6

4

8

6

8

8
4

2

6

2

8

4

6

8
2

8

4

6

8

6

8

8
2

4

6

6

6

8

Empty

2

8

Black

Black

2

4

6

8

8

8

8

White

4

6

Empty

White

4

6

Feature 3
Weight = 0.660000

2

2

2

2

2

Feature 2
Weight = 0.620000

Feature 1
Weight = 0.760000

Feature 3
Weight = 0.080000

Feature 2
Weight = 0.020000

Feature 1
Weight = 0.070000

2

389

Feature 3
Weight = 0.390000

Results of Round Robin Tournament
Among Best−Of−Gen Champions

2

2

2

4

4

4

6

6

6

8

8

Black

2

4

6

8

6

8

2

2

4

4

4

6

6

6

8

8

8

6

8

2
Empty

4

2

4

2

4

6

8

2

2

4

4

4

6

6

8

8

8

2

4

6

8

4

4

6

6

8

8

2

6

Net 100

0.8

8
2

2

2

0.9

2

4

6

8

0.7
Fraction of Games Won

White

1

0.6

0.5

0.4

0.3

0.2

0.1

0
2

4

6

8

Fig. 3. The best player of generation 100. This neural-net is nearly
as good as net 29, winning 60 percent of its games, and its structure
is very similar to that of net 29

0

10

20

30

40
50
60
Player Generation Number

70

80

90

100

Fig. 4. This graph shows the fraction of games won by each generation’s best player in a round-robin
tournament against the other generations’ best players. The graph
reveals a general trend of increasing ﬁtness through generation 29, at
which point ﬁtness levels oﬀ

