Available online at www.sciencedirect.com

ScienceDirect

This space is reserved for the Procedia header, do not use it
This space
is Computer
reservedScience
for the
Procedia
header, do not use it
Procedia
108C
(2017) 765–775
This space is reserved for the Procedia header, do not use it

International Conference on Computational Science, ICCS 2017, 12-14 June 2017,
Zurich, Switzerland

Fast
Fast Finite
Finite Element
Element Analysis
Analysis Method
Method Using
Using Multiple
Multiple
GPUs
for
Deformation
and
Application
to
GPUs
for Crustal
Crustal
Deformation
and its
its Using
Application
to
Fast Finite
Element
Analysis Method
Multiple
Stochastic
Analysis
Geometry
Uncertainty
Stochastic
Inversion
Analysis with
withand
Geometry
Uncertainty
GPUs forInversion
Crustal Deformation
its Application
to
1
1,2
1,2
3
Takuma
Fujita
Ichimura
Takane
Hori
1 , Kohei
1,2 , Tsuyoshi
1,2 , Uncertainty
3,
Stochastic
Inversion
Analysis
with
Geometry
Takuma Yamaguchi
Yamaguchi
, Kohei
Fujita
,
Tsuyoshi
Ichimura
,
Takane
Hori
,
1,2
1,2

Muneo
and Lalith
Lalith Wijerathne
Wijerathne1,2
Muneo Hori
Hori1,2 ,, and
1
1,2
Takuma Yamaguchi
, Kohei Fujita , Tsuyoshi Ichimura1,2 , Takane Hori3 ,
1
1 The University of Tokyo, Bunkyo, Tokyo, Japan
1,2 of Tokyo, Bunkyo, Tokyo, Japan
The
University
, and hori,
Lalithlalith}@eri.u-tokyo.ac.jp
Wijerathne1,2
Muneo
Hori
{yamaguchi,
fujita,
ichimura,
3
3

3

{yamaguchi, fujita, ichimura, hori, lalith}@eri.u-tokyo.ac.jp
Advanced Institute
for Computational Science, RIKEN, Kobe, Hyogo, Japan
1
Advanced Institute
for Computational
Science,
Kobe, Hyogo, Japan
The University
of Tokyo,
Bunkyo,RIKEN,
Tokyo,
Japan
Japan Agency for Marine-Earth
Science
and Technology,
Yokohama,
Kanagawa, Japan
Japan Agency
for Marine-Earth
Science and
Technology,
Yokohama, Kanagawa, Japan
{yamaguchi,
fujita, ichimura,
hori,
lalith}@eri.u-tokyo.ac.jp
hori@jamstec.go.jp
2
hori@jamstec.go.jp
Advanced Institute for Computational
Science, RIKEN, Kobe, Hyogo, Japan
Japan Agency for Marine-Earth Science and Technology, Yokohama, Kanagawa, Japan
hori@jamstec.go.jp
2
2

Abstract
Abstract
Crustal deformation computation using 3-D high-ﬁdelity models has been in heavy demand due
Crustal deformation computation using 3-D high-ﬁdelity models has been in heavy demand due
to accumulation of observational data. This approach is computationally expensive and more
to
accumulation of observational data. This approach is computationally expensive and more
Abstract
than 1055 repetitive computations are required for various application including Monte Carlo
than
10 deformation
repetitive computations
are required
for various
application
including
Carlo
Crustal
computation using
3-D high-ﬁdelity
models
has been
in heavyMonte
demand
due
simulation, stochastic inverse analysis, and optimization. To handle the massive computation
simulation,
stochastic
inverse analysis,
To handle the massive
computation
to accumulation
of observational
data. and
Thisoptimization.
approach is computationally
expensive
and more
cost, we5 develop a fast Finite Element (FE) analysis method using multi-GPUs for crustal
cost,
we develop
a fast
Finite Element
(FE) analysis
method
using multi-GPUs
for crustal
than 10
repetitive
computations
are required
for various
application
including Monte
Carlo
deformation. We use algorithms appropriate for GPUs and accelerate calculations such as sparse
deformation.
We use algorithms
appropriate
for GPUs and accelerate
as sparse
simulation, stochastic
inverse analysis,
and optimization.
To handlecalculations
the massivesuch
computation
matrix-vector product. By reducing the computation time, we are able to conduct multiple
matrix-vector
product.
reducing
the(FE)
computation
we using
are able
to conduct
cost, we develop
a fast By
Finite
Element
analysis time,
method
multi-GPUs
formultiple
crustal
crustal deformation computations in a feasible timeframe. As an application example, we
crustal
deformation
computations
in a feasible
timeframe.
As an calculations
application such
example,
we
deformation.
We use algorithms
appropriate
for GPUs
and accelerate
as sparse
conduct stochastic inverse analysis considering uncertainties in geometry and estimate coseismic
conduct
stochastic
inverseBy
analysis
considering
uncertainties
geometry
and
matrix-vector
product.
reducing
the computation
time,in we
are able
toestimate
conductcoseismic
multiple
slip distribution in the 2011 Tohoku Earthquake, by performing 360,000 crustal deformation
slip
distribution
in the
2011 Tohoku
by performing
360,000
crustal example,
deformation
crustal
deformation
computations
in Earthquake,
a feasible timeframe.
As an
application
we
computations for diﬀerent 8 × 1077 DOF FE models using the proposed method.
computations
for diﬀerent
8 × 10 considering
DOF FE models
using the
conduct
stochastic
inverse analysis
uncertainties
in proposed
geometry method.
and estimate coseismic
Keywords:
OpenACC,
MPI,
FEM,
Conjugate
Gradientby
Method,
Element-by-Element
Method
slip
distribution
inPublished
the
2011
Tohoku
Earthquake,
performing
360,000 crustal
deformation
© 2017
The Authors.
by
Elsevier
B.V.
Keywords:
OpenACC,
MPI,
FEM,
Conjugate
Gradient Method,
Element-by-Element
Method
7
Peer-review underfor
responsibility
the10
scientific
of theusing
International
Conference
on Computational Science
computations
diﬀerent of
8×
DOFcommittee
FE models
the proposed
method.
Keywords: OpenACC, MPI, FEM, Conjugate Gradient Method, Element-by-Element Method

1
1

Introduction
Introduction

Numerical simulations of crustal deformation have been studied intensively with the aim of
Numerical simulations of crustal deformation have been studied intensively with the aim of
understanding
the state of the crustal structures in relation to earthquake generation processes.
1 Introduction
understanding
the state of the crustal structures in relation to earthquake generation processes.
Most studies have used analytical solutions, with the assumption that the crustal structure is
Most
studies
have
used of
analytical
solutions, with
thebeen
assumption
the crustal
is
Numerical simulations
crustal deformation
have
studied that
intensively
with structure
the aim of
a half-inﬁnite space [8]. However, recent studies have used the three-dimensional (3-D) ﬁnite
aunderstanding
half-inﬁnite space
[8].
However,
recent
studies
have
used
the
three-dimensional
(3-D)
ﬁnite
the state of the crustal structures in relation to earthquake generation processes.
element (FE) method at low resolutions, because simplifying the 3-D heterogeneity of crustal
element
(FE) have
method
low resolutions,
because
simplifying
the 3-D
of crustal
Most studies
usedatanalytical
solutions,
with the
assumption
that heterogeneity
the crustal structure
is
structures may signiﬁcantly impact the results of some cases [6]. In addition, the availability of
structures
may
signiﬁcantly
impact
the
results
of
some
cases
[6].
In
addition,
the
availability
of
a half-inﬁnite space [8]. However, recent studies have used the three-dimensional (3-D) ﬁnite
high-resolution crustal deformation observational data and crustal structure data has increased
high-resolution
crustal at
deformation
observational
and crustal
structure
data has of
increased
element (FE) method
low resolutions,
becausedata
simplifying
the 3-D
heterogeneity
crustal
with the improvement of observation technologies. As a result, the demand is growing for
with
the
improvement
of
observation
technologies.
As
a
result,
the
demand
is
growing
structures may signiﬁcantly impact the results of some cases [6]. In addition, the availabilityfor
of
high-resolution crustal deformation observational data and crustal structure data has increased
1
with the improvement of observation technologies. As a result, the demand is growing for
1
1877-0509 © 2017 The Authors. Published by Elsevier B.V.
Peer-review under responsibility of the scientific committee of the International Conference on Computational Science
10.1016/j.procs.2017.05.223

1

766	

Takuma Yamaguchi
et al. / Procedia Computer Science 108C (2017) 765–775
Fast Crustal Deformation Computation
Method
T. Yamaguchi et al.

methods that can compute elastic crustal deformation using high-resolution 3-D numerical
modelling and that consider the surface geometry and heterogeneity of crustal structures.
When targeting Japan, the domain of analysis is on the order of 103−4 ×103−4 ×102−3 km. If
a numerical model based on high-ﬁdelity crustal structure data with suﬃcient ﬁne discretization
to guarantee convergence of the numerical solution is used, the model would have 108−10 degrees
of freedom (DOF). To handle such a massive computation cost within a realistic timeframe,
there is great interest in developing fast numerical computation methods for large-scale crustal
deformation computations. In these computations, most computation time is spent in solving a
system of linear equations, which is the result of constitutive rules and discretization. Therefore,
creating a faster solver for systems of linear equations would beneﬁt fast numerical simulations.
Multiple crustal deformation computations enable stochastic inverse analysis, optimization,
sensitivity analysis, and Monte Carlo (MC) simulation. In this study, we focused on estimating
the eﬀects of uncertainties included in the model as an example application of our proposed
method. In crustal deformation computations, it is desirable to consider uncertainties, including
those in material properties, geometries, and inputs. However, the computation cost of such
simulations increases depending on the number of repetitive computations required. A previous
study [11] has showed that these approaches require 105−6 forward analyses.
GPUs have recently become common in scientiﬁc computing. It is thought that they are
broadly applicable to numerical simulations with parallel computation. Use of GPU accelerators is expected to speed up simulations. However, GPU calculation performance varies greatly
depending on the algorithm. When conducting 105−6 forward analyses, it is essential to introduce a faster solver because diﬀerence in computation time of each analysis greatly impacts the
whole computation time.
Here, we propose a method for computing elastic crustal deformation using a fast solver
with multiple GPUs. As for the sparse matrix-vector product, which accounts for the largest
proportion of the computation time, we introduce the Element-by-Element (EBE) method and
reduce the amount of memory access. Using a smaller computer comprising GPUs, we reduce
the computation time and perform many computations within a reasonable timeframe. This
method can be used to compute crustal deformation with quantitative uncertainty evaluations.
The reminder of this paper is organized as follows. Subsection 2.1 describes the formulation
of the FE method, and Subsection 2.2 discusses the complete structure of the solver. Subsection
2.3 describes the method for sparse matrix-vector multiplication in the solver. In Section 3,
we discuss the eﬃciency of the proposed method. We estimate the stochastic coseismic slip
distribution using 360,000 crustal deformation computations, which target the 2011 Tohoku
Earthquake and have 8 × 107 DOFs.

2
2.1

Methodology
Summary of FE Formulation

We used the FE method because it is suitable for accurately handling geometric shapes. Coseismic crustal deformation is typically modelled as elastic deformation caused by dislocation
of the fault surface. The target equation is rewritten after discretization with the FE method
as
Ku = f ,
(1)
where K, u, and f are the global stiﬀness matrix, displacement vector, and force vector, respectively. We used the split-node technique [7] in our simulation to introduce fault dislocation to
the FE model because it does not require the modiﬁcation of K.
2

	

Takuma Yamaguchi
et al. / Procedia Computer Science 108C (2017) 765–775
Fast Crustal Deformation Computation
Method
T. Yamaguchi et al.

2.2

Design of Parallel FE Solver Using GPUs

In this study, target applications require 105−6 crustal deformation computations. Therefore the
performance of the solver controls the whole computation time. We extend the MPI parallel
solver based on the Conjugate Gradient (CG) method [4], which our group has developed.
The solver was developed originally for CPU-based large-scale computation environments, and
it is designed based on the CG method with adaptive preconditioning, geometric multi-grid
method, and mixed precision arithmetic, so that the amount of calculation and communication
is reduced. A previous study by Fujita et al. (2016) [2] has achieved acceleration of this solver by
porting a part of codes to GPUs. There are three loops in the target solver: Outer loop and inner
ﬁne and coarse loops in adaptive preconditioning. The computation cost of preconditioning is
much larger than that of the outer loop. The solver by Fujita has been designed focusing
on acceleration of preconditioning and enables reduction of much computation time with low
development cost by computing a large part of the calculation in GPUs.
However, the computational cost of Fujita’s solver can be reduced further. CPU computation in the outer loop has become non-negligible due to the reduced computation time in the
preconditioning part. The proportion of computation time in the outer loop was about 10% in
the CPU-based original solver. The previous performance measurement shows that this proportion of computation time spent in the outer loop has greatly increased twofold (from 10% to
30%) when we use Fujita’s solver with K40 GPUs. Furthermore, the previous solver switches
the computation device according to its loop. Thereby, we have to transfer data from CPUs to
GPUs before inner loop computation and from GPUs to CPUs after inner loop computation.
The performance of data transfer is much lower than that of computation, so data transfer is
one of the bottlenecks of GPU computation.
Therefore, we assign all computations except coeﬃcient calculations to GPUs. The proposed solver is described in Algorithm 1. This approach enables reduction of computation time
due to acceleration of each calculation including the outer loop. Moreover, we do not have to
transfer data before and after the preconditioning part because the computation devices are
ﬁxed. This also reduces the computation cost in the solver. For GPU computation, we use
OpenACC [9], one of the platforms for various accelerators. By using OpenACC, we can operate GPU computation with some directives. On the other hand, we are required to manage
operations that are unsuitable for GPUs to exhibit high performance in GPU computation.
OpenACC unnecessarily transfers data between CPUs and GPUs if there is no explicit instruction for data transfer before and after GPU computation. Therefore, we insert data directives
and control data transfer between CPUs and GPUs. In this solver, data transfer is required at
the beginning, at the end, and in MPI communication. When we call MPI communication, we
assign one GPU per MPI process and use multiple GPUs. MPI communication between other
processes is conducted in CPUs, so we have to perform data transfer between CPUs and GPUs.
We pack and unpack data necessary for data transfer into a new array to minimize the amount
and times of data transfer.

2.3

Sparse Matrix-Vector Multiplication Using GPUs

Sparse matrix-vector product accounts for the largest proportion of the computation cost in
the solver. In GPU computation, we compensate for the memory access latency by activating
more threads than the number of cores. At this time, multiple threads simultaneously access
the global memory. GPUs are designed to exhibit high performance for coalesced memory
accesses, and they take much time to compute some calculations including random accesses
to the memory, such as sparse matrix-vector products. We apply EBE computation [10] for
matrix-vector products. We can obtain the result by adding up the multiplication of the element
3

767

768	

Fast Crustal Deformation Computation
Method
T. Yamaguchi et al.
Takuma Yamaguchi
et al. / Procedia Computer Science 108C (2017) 765–775

1
2
3
4
5
6
7
8

9
10
11
12
13

(a) Outer loop
transfer
∑ u, r from Host to Device
r ⇐ i Kie uie
transfer r from Device to Host
synchronize r by MPI Send, Recv
transfer r from Host to Device
r⇐f −r
β⇐0
i⇐1
while ∥r∥2 /∥f ∥2 > ϵ do
−1
u⇐M r
T

rc ⇐ P r

1
2
3
4
5
6
7

T

uc ⇐ P u

−1

solve uc = Kc rc in (b) with ϵin
c and Nc
u ⇐ Puc
−1

8
9
10

(b) Inner loop
∑ i
e ⇐ i Ke uie
transfer e from Device to Host
synchronize e by MPI ISend, IRecv
transfer e from Host to Device
e⇐r−e
β⇐0
i⇐1
while ∥e∥2 /∥r∥2 > ϵ and N > i do
−1
z⇐M e
ρa ⇐ (z, e)
transfer ρa from Device to Host
synchronize ρa by MPI Allreduce
if i > 1 then
β ⇐ ρa /ρb
end
transfer β from Host to Device
p ⇐ z + βp
∑ i
q ⇐ i Ke pie
transfer q from Device to Host
synchronize q by MPI ISend, IRecv
transfer q from Host to Device
γ ⇐ (p, q)
transfer γ from Device to Host
synchronize γ by MPI Allreduce
α ⇐ ρa /γ
ρb ⇐ ρa
transfer α from Host to Device
e ⇐ e− αq
u ⇐ u + αp
i⇐i+1
end

solve u = K r in (b) with ϵin and N
11
15
u⇐u
if i > 1 then
12
16
γ ⇐ (z, q)
17
transfer γ from Device to Host
13
18
synchronize γ by MPI Allreduce
14
19
β ⇐ γ/ρ
15
20
transfer β from Host to Device
16
end
17
21
p⇐∑
z + βp
18
22
q ⇐ i Kie pie
19
transfer q from Device to Host
23
20
24
synchronize q by MPI ISend, IRecv
21
25
transfer q from Host to Device
22
26
ρ ⇐ (z, r)
23
27
γ ⇐ (p, q)
24
28
transfer ρ, γ from Device to Host
25
29
synchronize ρ, γ by MPI Allreduce
26
30
α ⇐ ρ/γ
27
31
transfer α from Host to Device
32
r ⇐ r − αq
33
u ⇐ u + αp
34
i⇐i+1
end
35 transfer u from Device to Host
Algorithm 1: The iterative solver is calculated at each computation node to obtain a converged solution of Ku = f using an initial solution, u, with a threshold of ∥r∥2 /∥f ∥2 ≤ ϵ. The
in
input variables are K, K, Kc , P, u, f , ϵ, ϵin
c , Nc , ϵ , and N . The other variables are temporal.
( ¯ ) represents single-precision variables, while the others are double-precision variables. P is
a mapping matrix from the coarse model to the target model. All computation steps in this
solver, except MPI synchronization and coeﬃcient computation, are performed in GPUs.
14

4

	

Fast Crustal Deformation Computation
Method
T. Yamaguchi et al.
Takuma Yamaguchi
et al. / Procedia Computer Science 108C (2017) 765–775

stiﬀness matrix and the global vector in this method, as shown in Equation (2);
∑
Pi Ki PiT u,
f=

(2)

i

where Ki and Pi are the element stiﬀness matrix and the mapping matrix to the global vector,
respectively. The element stiﬀness matrix is not stored in the memory and generated from
node connectivity, coordinate, and material property on the ﬂy. We are able to compute using
cache and reduce the computation cost for random memory access because these arrays are
read repeatedly. Moreover, the total amount of memory access in this method is lower than
that when using stored sparse matrices.
When we apply EBE computation to GPUs, we parallelize calculations with multiple threads
by assigning one thread for each element in the mesh. On the other hand, an algorithm is required to sum up local values into the result vector. Atomic functions have been improved
recently, so we can compute with the assistance of high data locality. Hence, we insert atomic
directives and compute without data race between diﬀerent threads. On the other hand, hardware capability of most GPUs in double precision is much worse than that in single precision.
The GPU used in this study, NVIDIA K40, supports atomic add instruction for values in global
memory in single precision, but not in double precision, whereas the latest GPU, NVIDIA P100,
provides it in both single and double precision. Therefore, in K40, we cannot calculate EBE
computation in double precision by inserting atomic directives of OpenACC. In matrix-vector
multiplication in the outer loop, we apply coloring and manage the order of computation so as
to explicitly avoid data competition. Each node is inﬂuenced from surrounding elements and
computation ordering for these elements must be designated for parallel computation. In the
coloring method, elements that do not share nodes are assigned the same color. Then, the computation for elements with the same color can be performed in parallel. We use unstructured
meshes in this study, which are more complex than structured meshes. The performance of
EBE computation with coloring is expected to decrease because many colors are required for
elements in the mesh and parallelism is decreased. The iteration frequency of the outer loop
is much lower than that of the inner loops, so the rise in computation time by computing in
double-precision coloring is suppressed.

3
3.1

Application Example
Problem Setting

We estimate the coseismic slip distribution by multiple crustal deformation computations, to
show the eﬃciency of the proposed method. The target region is described in Figure 1. The
model is composed of four layers and the minimum size of discretization in space is 2,000 m.
Material properties of the layers are listed in Table 1. From these settings, a FE model is
generated similar to the one shown in Figure 2(a)(b). This FE model has about 8 × 107 DOF.
We input small faults by ﬁtting a bicubic B-spline curve as a unit fault slip to the assumed fault.
The locations to input unit fault slips are shown in Figure 2(c). There are 180 small faults in
the target region. To estimate the slip distribution, Green’s functions obtained by inputting
the unit fault slip in the strike and dip directions are required for each small fault. Therefore,
we need to calculate 180 × 2 = 360 Green’s functions per one inverse analysis. By using the
proposed method, the surface displacement is calculated against each input fault slip. We use
GEONET, GPS-A, and S-Net as synthetic observational data. Each location and component
are shown in Figure 1. We can estimate fault slip by solving
Gx = d,

(3)
5

769

770	

Takuma Yamaguchi
et al. / Procedia Computer Science 108C (2017) 765–775
Fast Crustal Deformation Computation
Method
T. Yamaguchi et al.
45˚

45˚

40˚

40˚

35˚

35˚

10m
135˚

140˚

km
0 100

1m

145˚

135˚

(a) x, y components

km
0 100

140˚

145˚

(b) z component

Figure 1: Target area of the application example (black line) and input vectors at observation points.
We use components of GEONET (black arrows), GPS-A (blue arrows), and S-Net (red
arrows).

Table 1: Material properties of the four layers forming the model. Vp : P wave velocity (m/s), Vs : S
wave velocity (m/s), ρ: density (kg/m3 ), E: Young’s modulus(GPa), v: Poisson’s ratio.

Crust layer
Upper-mantle layer
Philippine Sea Plate
Paciﬁc Plate

Vp
5664
8270
6686
6686

Vs
3300
4535
3818
3818

ρ
2670
3320
2600
2600

E
72.3
176
95.4
95.4

v
0.243
0.285
0.258
0.258

where G is an m × n matrix of Green’s function, x is an n-dimensional vector of unknown
displacement for each subfault, d is an m-dimensional vector of the observational data, and m
and n are the number of observation points and the number of unit fault slips, respectively.
These settings for Green’s functions and observational data are the same as those in [1].
Now we consider uncertainties in geometry and apply Monte Carlo simulation by conducting
1,000 inverse analyses for 1,000 diﬀerent FE models. The original fault surface is the same as
that in [1], which is based on the elevation data in [5] and generated by interpolating the top
surface of Paciﬁc Plate and the trench axis. We add uncertainties to these elevation data. First,
we draw two random numbers, r1 and r2 from diﬀerent normal distributions. The sets of (µ,
σ) in these normal distributions are (0, 2) for r1 and (0, 5) for r2 . We also extract grid points
along the trench axis at 40 km intervals in y direction. For each point, we search lines with the
sharpest inclination in z direction. We add the degree r1 to the angles of the obtained lines in
the xy plane. In addition, we vary the direction of lines vertically by adding the degree r2 to
the angles of lines. At this phase of processing, we have obtained new fault lines. Finally, we
interpolate these lines and obtain a new fault surface. In this example application, we generate
1,000 diﬀerent FE models. Crustal deformation is computed 360 times for each FE model, so
a total of 360,000 crustal deformation computations are required.
For the FE mesh construction, we used the method by Ichimura et al. (2016) [3], which
6

	

Takuma Yamaguchi
et al. / Procedia Computer Science 108C (2017) 765–775
Fast Crustal Deformation Computation
Method
T. Yamaguchi et al.
(c) Locations of unit fault slips.
Each grid point corresponds to
the center of one fault slip.

(b) Whole view of FE model.

(a) Close-up view of FE model.

976 km

Crust layer

y

z

x

784 km

Upper-mantle layer
400 km

Philippine Sea Plate

km
0 100

Pacific Plate

Figure 2: FE model and locations of grid points used as inputs for the basis functions.
GPU cluster
Each compute node
MPI Process #0, 1, …, n-1
Start

MPI Process #n
Create mesh
Decompose mesh

Loop i=1, 2, 3, … , # of meshes
Synchronization
Create files for sub-domains

Create next mesh

Compute Green’s functions

Decompose mesh



Solve inversion equation
Loop i
End

Figure 3: Whole structure of the analysis.

enables automated and robust construction directly from digital elevation data of the crustal
structure without creating a computer-aided design model. The geometric resolution of the FE
model can be the same as that of the input digital elevation data. The generated mesh consists of
quadratic tetrahedral elements and triangular-prism infinite elements. Serial calculations such
as numbering of nodes account for a large proportion of computations in FE mesh construction.
These computations are assigned to CPUs because they are ill-suited for GPU computation.
We perform computations following a scheme in which GPUs always calculate Green’s functions
while CPUs are generating a mesh. The whole structure of the analysis is shown in Figure 3.
First, we launch n+1 processes from process #0 to #n. Next, we call MPI comm split and
divide the communicator in two groups. One is composed of only process #n and computes
FE mesh construction. The other group is composed of process #0 to #n−1 and computes
Green’s functions. In this group, one GPU is assigned to each MPI process. After calculating
Green’s functions, all MPI processes synchronize and proceed to the next step.
In the GPU computation, we use a GPU cluster with has eight compute nodes. Each
compute node of the GPU cluster has two CPUs (Intel Xeon E5-2695 v2) with 24 cores and eight
GPUs (NVIDIA Tesla K40). We decompose the FE model into sub-domains for MPI processes
using METIS. The FE model is decomposed into eight sub-domains and each sub-domain is
7

771

772	

Takuma Yamaguchi
et al. / Procedia Computer Science 108C (2017) 765–775
Fast Crustal Deformation Computation
Method
T. Yamaguchi et al.

assigned to one MPI process. Then one compute node of the GPU cluster computes one crustal
deformation analysis using eight MPI processes. In computing the proposed solver, parameters
for judging the convergence of the solution are set as follows: ϵ = 10−16 , ϵ = 1.6 × 10−3 ,
ϵc = 1.6 × 10−3 , N = 200, and Nc = 2000. Threshold values of the inner loops are set so that
the whole computation time is reduced.

3.2

Performance Measurement

First, we show the eﬃciency of the proposed method by measuring performance of each calculation. We measured computation time for EBE multiplication. Multiplication using atomic
functions in single precision required 13.6 ms, and multiplication using coloring requires 33.3
ms. One node in this unstructured mesh is shared with up to 23 elements, leading to 72 colors
for the whole domain. By using atomics, we can utilize high data locality of node connectivity
and obtain higher performance than the coloring version. As for double-precision calculation,
we obtained multiplication by coloring in 47.8 ms.
Next, we measured the whole computation time in the solver. We compared the computation
time in the proposed solver and in Fujita’s solver. In both solvers, we used one compute node
with eight K40 GPUs. CPU computation with Fujita’s solver used OpenMP/MPI hybrid
parallel computation by assigning three cores of CPUs per MPI process. The measurement
results are listed in Table 2 in detail. The computation time for inner loops was slightly
diﬀerent due to the change of computation method in the outer loop. In Fujita’s solver, GPUs
worked only in the preconditioning part, the inner loops. As a result, the computation time in
the outer loop accounted for 51.6% of the entire time in the solver. Thereby, we conﬁrmed that
computation in the outer loop is a bottleneck in the whole computation. In the proposed solver,
the computation time in the outer loop was decreased by one-fourteenth. This acceleration was
mainly due to the introduction of GPU computation. Compared with the previous solver,
sparse matrix-vector multiplication and inner vector product were computed 8.1 times and
17.8 times faster, respectively. As for matrix-vector multiplication, we cannot cover all latency
due to random memory accesses since each thread in GPUs consumes 255 registers and the
number of threads in one Streaming Multiprocessor is restricted to 256 by the total usage of
registers (65536 registers). We think that developing an algorithm for EBE kernel which uses
less registers and improving performance will be our future task. Moreover, NVIDIA P100
GPU can conduct atomic add instructions in double precision. Considering the comparison
of computation performance in single precision, EBE multiplication using atomic functions in
double precision is expected to exhibit higher performance than using coloring. This point is
another issue in the future. For the inner vector product, the computation is memory bandwidth
bound. The bandwidth per MPI process is 14.9 GB/s in CPUs and 288 GB/s in GPUs. The
obtained speedup is reasonable against the CPU/GPU memory bandwidth ratio. In addition,
Fujita’s solver transfers data before and after the inner loops, which took 0.13 s. This data
transfer was eliminated in the proposed solver, so the elimination of data transfer contributed
to the acceleration of computation in the outer loop. In this solver, the outer loop was iterated
only six times. We can conﬁrm that the proportion of computation that need to be performed
in double precision, for which GPUs cannot exhibit high performance, was reduced by using
the inner coarse loop and the inner ﬁne loop. We also measured power consumption during
computation. We used Intel Power Gadget for CPUs and Nvidia-smi for GPUs, respectively.
Power consumption was 201 Watt when we compute with only CPUs and 1267 Watt when we
compute with CPUs and GPUs. We conﬁrmed that computation got 2.27 times more powereﬃcient by introducing GPUs given that outer loop was accelerated 14.3 times. The whole
computation time in the proposed solver was 10.6 s, which was 1.78 times faster than Fujita’s
8

	

Takuma Yamaguchi
et al. / Procedia Computer Science 108C (2017) 765–775
Fast Crustal Deformation Computation
Method
T. Yamaguchi et al.

Table 2: Comparison of the elapsed time.

# of iterations
Inner coarse loop
(SpMV kernel)
Inner fine loop
(SpMV kernel)
Outer loop
(SpMV kernel)
(inner product kernel)
(data transfer for inner loop)
Total time in solver

1516
133
6

-

Elapsed Time
Elapsed Time
in the solver
in the proposed Speedup
by Fujita (2016)
solver
6.14 s
6.49 s
2.69 s
2.92 s
2.96 s
3.37 s
1.92 s
1.99 s
9.71 s
0.68 s
14.3
2.74 s
0.34 s
8.1
0.30 s
0.02 s
17.8
0.13 s
0.00 s
18.8 s
10.6 s
1.78

P1

P1

40.9

P2
P3

5.02

P2
P3

0.00

(i) Average

0.00

(ii) Standard deviation

Figure 4: Estimated coseismic slip distribution (m) for 1,000 cases. Norm of each point is described.

solver.
The computation time per inverse analysis (360 forward analyses + inversion) was 4,312
s. Each mesh generation and decomposition required 1,364 s and this computation time can
be neglected because it was covered up by computation in the solver, except at the ﬁrst time.
This application example (360,000 forward analyses = 360 forward analyses x 1,000 diﬀerent
models) can be computed in a total of nine days.

3.3

Result for Inversion

Fig. 4 shows the average value and standard deviation of coseismic slip distribution for 1,000
cases, and Fig. 5 shows pointwise convergency. There is little diﬀerence after 800 cases, so we can
conclude that MC simulation has converged. The standard deviation of the slip distribution
is 13% of the average value. This indicates that consideration of uncertainties in geometry
is signiﬁcant because the obtained standard deviation is non-negligible when discussing the
coseismic slip distribution and related stress change distribution.

4

Conclusion

We developed a fast method for computation of crustal deformation using a GPU computing
solver. Our target problem required more than 105 crustal deformation computations. To
handle such a massive computation, we computed all calculations in GPUs and reduced the
9

773

Takuma Yamaguchi
et al. / Procedia Computer Science 108C (2017) 765–775
Fast Crustal Deformation Computation
Method
T. Yamaguchi et al.

20
18
norm of slip (m)

774	

P1
P2
P3

16
8

4.6
4.4
4.2
4
1.8
1.6
1.4
1.2
1

6
4
2
100 200 300 400 500 600 700 800 900 1000
# of cases
(i) Average

100 200 300 400 500 600 700 800 900 1000
# of cases
(ii) Standard deviation

Figure 5: Pointwise transitions of average and standard deviation. Extracted points are shown in
Figure 4.

computation cost. Regarding the sparse matrix-vector product, which accounted for the largest
proportion of the computation time, we introduced EBE multiplication. We modified the
algorithm according to hardware capability in single precision and double precision. To test the
proposed method, we constructed 1,000 FE models with 8 × 107 DOF for northeastern Japan
and computed the elastic crustal deformation computation 360,000 times in nine days using a
GPU cluster comprising 16 CPUs and 64 GPUs. Using this method, a stochastic estimation
of coseismic slip distribution, with uncertainties in geometry, was computed within a realistic
timeframe. In future studies, we will apply this method to the optimization of crustal structure.

Acknowledgments
We thank Prof. Yuji Yagi (University of Tsukuba) and Dr. Takeshi Iinuma (JAMSTEC) for
advice on the setting of uncertainty in geometry. We thank Dr. Ryoichiro Agata (The University
of Tokyo) for providing comments on the setting for inverse analyses and Prof. Kengo Nakajima
(The University of Tokyo) for providing comments on results in inverse analyses.

References
[1] Ryoichiro Agata, Tsuyoshi Ichimura, Kazuro Hirahara, Mamoru Hyodo, Takane Hori, and Muneo
Hori. Robust and portable capacity computing method for many finite element analyses of a highfidelity crustal structure model aimed for coseismic slip estimation. Computers & Geosciences,
94:121–130, 2016.
[2] Kohei Fujita, Takuma Yamaguchi, Tsuyoshi Ichimura, Muneo Hori, and Lalith Maddegedara.
Acceleration of element-by-element kernel in unstructured implicit low-order finite-element earthquake simulation using openacc on pascal gpus. In Proceedings of the Third International Workshop
on Accelerator Programming Using Directives, pages 1–12. IEEE Press, 2016.
[3] Tsuyoshi Ichimura, Ryoichiro Agata, Takane Hori, Kazuro Hirahara, Chihiro Hashimoto, Muneo Hori, and Yukitoshi Fukahata. An elastic/viscoelastic finite element analysis method for
crustal deformation using a 3-d island-scale high-fidelity model. Geophysical Journal International, 206(1):114–129, 2016.
[4] Tsuyoshi Ichimura, Kohei Fujita, Pher Errol Balde Quinay, Lalith Maddegedara, Muneo Hori,
Seizo Tanaka, Yoshihisa Shizawa, Hiroshi Kobayashi, and Kazuo Minami. Implicit nonlinear
wave simulation with 1.08 t dof and 0.270 t unstructured finite elements to enhance comprehensive earthquake simulation. In Proceedings of the International Conference for High Performance
Computing, Networking, Storage and Analysis, page 4. ACM, 2015.

10

	

Takuma Yamaguchi
et al. / Procedia Computer Science 108C (2017) 765–775
Fast Crustal Deformation Computation
Method
T. Yamaguchi et al.

[5] Kazuki Koketsu, Yusuke Yokota, Naoki Nishimura, Yuji Yagi, Shin’ichi Miyazaki, Kenji Satake,
Yushiro Fujii, Hiroe Miyake, Shin’ichi Sakai, Yoshiko Yamanaka, et al. A uniﬁed source model for
the 2011 tohoku earthquake. Earth and Planetary Science Letters, 310(3):480–487, 2011.
[6] Timothy Masterlark. Finite element model predictions of static deformation from dislocation
sources in a subduction zone: sensitivities to homogeneous, isotropic, poisson-solid, and half-space
assumptions. Journal of Geophysical Research: Solid Earth, 108(B11), 2003.
[7] HJ Melosh and A Raefsky. A simple and eﬃcient method for introducing faults into ﬁnite element
computations. Bulletin of the Seismological Society of America, 71(5):1391–1400, 1981.
[8] Yoshimitsu Okada. Internal deformation due to shear and tensile faults in a half-space. Bulletin
of the Seismological Society of America, 82(2):1018–1040, 1992.
[9] OpenACC. Openacc. http://www.openacc.org/.
[10] James M Winget and Thomas JR Hughes. Solution algorithms for nonlinear transient heat conduction analysis employing element-by-element iterative strategies. Computer Methods in Applied
Mechanics and Engineering, 52(1-3):711–815, 1985.
[11] Yuji Yagi and Yukitoshi Fukahata. Introduction of uncertainty of green’s function into waveform
inversion for seismic source processes. Geophysical Journal International, 186(2):711–720, 2011.

11

775

