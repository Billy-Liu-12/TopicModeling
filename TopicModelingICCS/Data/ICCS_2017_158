Available online at www.sciencedirect.com

ScienceDirect

This space is reserved for the Procedia header, do not use it
This
spaceComputer
is reserved
for108C
the Procedia
header, do not use it
Procedia
Science
(2017) 1261–1270
This space is reserved for the Procedia header, do not use it

International Conference on Computational Science, ICCS 2017, 12-14 June 2017,
Zurich, Switzerland

Large-scale Nonparallel Support Vector Ordinal Regression
Large-scale Nonparallel Support Vector Ordinal Regression
Solver Vector Ordinal Regression
Large-scale Nonparallel Support
Solver
1,2,3
1,2,3
Huadong Wang , Jianyu Miao Solver
, Seyed Mojtaba Hosseini Bamakan1,2,3 ,
1,2,3
1,2,3
1,2,3
1,2,3
1,2,4
1
1
1

Huadong Wang , Lingfeng
Jianyu Miao
Mojtaba
and Yong
Shi Hosseini Bamakan ,
Niu , ,Seyed
1,2,3
1,2,3 Lingfeng Niu
1,2,3
,
and
Yong
Shi1,2,4
Huadong Wang , Jianyu Miao , Seyed Mojtaba
Hosseini Bamakan1,2,3 ,
Research Center on Fictitious Economy & Data Science,
Chinese Academy
of Sciences, Beijing 100190, China
1,2,3
1,2,4
, and
Yong
Shi
Lingfeng
NiuScience,
2
Key Laboratory
Big DataEconomy
Mining and
Knowledge
Management,
Chinese
Sciences,
Beijing
Research
Center onofFictitious
& Data
Chinese
Academy
of Academy
Sciences, of
Beijing
100190,
China

2
100190, Management,
China
Key Laboratory of Big Data Mining and Knowledge
Chinese Academy of Sciences, Beijing
Research
Center on Fictitious Economy & Data Science, Chinese Academy of Sciences, Beijing 100190, China
3
School of Mathematical Sciences, University
of Chinese
100190,
ChinaAcademy of Sciences, Beijing, 100049, China.
2
4 3 Key Laboratory of Big Data Mining and Knowledge Management, Chinese Academy of Sciences, Beijing
College
Science
and Technology,
of Nebraska
at Omaha,Beijing,
Omaha,100049,
NE 68118,
USA
SchoolofofInformation
Mathematical
Sciences,
University ofUniversity
Chinese Academy
of Sciences,
China.
100190, China
4
{wanghaudong14,miaojianyu15}@mails.ucas.ac.cn,
College
of Information
Science and Technology, University of Nebraska at Omaha, Omaha, NE 68118, USA
3
School of Mathematical Sciences, University of Chinese Academy of Sciences, Beijing, 100049, China.
s mojtabahossini@yahoo.com,
{niulf,yshi}@ucas.ac.cn
{wanghaudong14,miaojianyu15}@mails.ucas.ac.cn,
4
College of Information Science and Technology, University of Nebraska at Omaha, Omaha, NE 68118, USA
s mojtabahossini@yahoo.com, {niulf,yshi}@ucas.ac.cn
{wanghaudong14,miaojianyu15}@mails.ucas.ac.cn,
s mojtabahossini@yahoo.com, {niulf,yshi}@ucas.ac.cn

Abstract
Abstract
Large-scale linear classification is widely used in many areas. Although SVM-based models for ordinal
Large-scale
linear classification
is be
widely
used in
many areas.
SVM-based
models kernels
for ordinal
regression
are proven to
powerful
techniques,
theAlthough
performance
with nonlinear
are
Abstract problem
regression
problem
are
proven
to be powerful
techniques,
thenot
performance
with to
nonlinear
kernels are
often
suffering
from
time
consuming.
Recently,
linear
SVC
only
is
shown
obtain
competitive
Large-scale linear classification is widely used in many areas. Although SVM-based models for ordinal
often
suffering
time
consuming.
Recently,
linear SVC not
is shown
to obtain
competitive
performance
in from
mostare
ofproven
the
cases,
also it techniques,
is considerably
fastonly
during
the
process
of training
regression problem
to bebut
powerful
the performance
with
nonlinear
kernels and
are
performance
in most
ofstudies
the cases,
but on
also
it is SVM-based
considerablyordinal
fast during
the process
ofIntraining
and
testing.
However,
few
focused
linear
regression
models.
this paper,
often suffering from time consuming. Recently, linear SVC not only is shown to obtain competitive
testing.
However,
few
studiescalled
focused
on Nonparallel
linear SVM-based
regression
models. In(NPSVOR),
this paper,
we
propose
a in
new
approach,
linear
Supportordinal
Vector
Ordinal
performance
most
of the cases,
but
also it is considerably
fast
during
the Regression
process of training
and
we
propose
a new
approach,
called
linear An
Nonparallel
Support Vector
Ordinal
Regression
(NPSVOR),
which
can
deal
with
large-scale
problems.
efficient
algorithm
based
on
Alternating
Direction
testing. However, few studies focused on linear SVM-based ordinal regression models. In thisMethod
paper,
which
can deal(ADMM)
with large-scale
problems.
An efficient
algorithm
based
onexperiments
Alternating Direction
Method
of
is designed
to solve
the proposed
model.
Our
are performed
on
weMultipliers
propose a new
approach,
called linear
Nonparallel
Support
Vector
Ordinal Regression
(NPSVOR),
of
Multipliers
(ADMM)
is designed
to solve
the proposed
model.
Our experiments
are performed on
large
document
data
sets
to
demonstrate
the
effectiveness
of
the
proposed
method.
which can deal with large-scale problems. An efficient algorithm based on Alternating Direction Method
large document data sets to demonstrate the effectiveness of the proposed method.
of2017
Multipliers
(ADMM)
is designed
to B.V.
solve
the proposed
Keywords:
Regression,
Support
Vector
Machine,
ADMM model. Our experiments are performed on
©
TheOrdinal
Authors.
Published
by Elsevier
Peer-review
under data
responsibility
of
the scientific
of the of
International
Conference
on Computational Science
large document
sets to demonstrate
thecommittee
effectiveness
the proposed
method.
Keywords:
Ordinal
Regression,
Support
Vector
Machine,
ADMM
Keywords: Ordinal Regression, Support Vector Machine, ADMM

1 Introduction
1 Introduction
Ordinal regression (OR) is a supervised learning problem where each instance is associated with an
1
Introduction
Ordinal
regression
supervised
each refers
instance
is associated
withThis
an
ordinal label.
Since (OR)
labelsisofaOR
can also learning
be calledproblem
ranks, it where
sometimes
to ranking
learning.

ordinal
label.
Since labels
of OR
can
be calledAranks,
it sometimes
refers
to rankinganalysis,
learning.where
This
situation
is ubiquitous
in is
the
of also
text learning
mining.
notable
example
the sentiment
Ordinal regression
(OR)
a field
supervised
problem
where
eachis instance
is associated with
an
situation
is
ubiquitous
in
the
field
of
text
mining.
A
notable
example
is
the
sentiment
analysis,
where
each
textlabel.
assigned
with aoflabel
fromalso
some
i.e.{ very
negative, negative,
neutral,learning.
positive, This
very
ordinal
Since labels
OR can
be levels,
called ranks,
it sometimes
refers to ranking
each
text is assigned with
a label
from some
levels,
i.e.{
very
negative,
neutral, and
positive,
very
positive}.
relation
exists
among
these
labels.
Forisnegative,
Amazon
website
other
onsituation isObviously,
ubiquitous an
in order
the field
of text
mining.
A notable
example
the sentiment
analysis,
where
positive}.
Obviously,
an
order
relation
exists
among
these
labels.
For
Amazon
website
and
other
online
retailers,
consumers
usually
give
their
ratings
and
comments
after
they
bought
the
products.
The
each text is assigned with a label from some levels, i.e.{ very negative, negative, neutral, positive, very
line
retailers,
consumers
usually
give
their
ratings
and
comments
after
they
bought
the
products.
The
rating
scheme
generally
five levels,
represented
by Amazon
different website
number and
of stars.
positive}.
Obviously,
an includes
order relation
existswhich
amongare
these
labels. For
otherOne
onrating
scheme
generally
includes five
levels,
which
are
represented
by different
number
of stars.
One
star
means
the
most
dissatisfaction,
five
stars
means
the
most
satisfaction,
and
so
on.
These
comments
line retailers, consumers usually give their ratings and comments after they bought the products. The
star
means
the most
dissatisfaction,
five stars
means the most satisfaction,
and so on. These comments
are
very
useful
for mining
the quality
consumer
about
products.
rating
scheme
generally
includes
five and
levels,
which preferences
are represented
bythe
different
number of stars. One
are very useful for mining the quality and consumer preferences about the products.
star means the most dissatisfaction, five stars means the most satisfaction, and so on. These comments
1
are very useful for mining the quality and consumer preferences about the products.
1
1877-0509 © 2017 The Authors. Published by Elsevier B.V.
Peer-review under responsibility of the scientific committee of the International Conference on Computational Science
10.1016/j.procs.2017.05.104

1

1262	

Huadong
WangRegression
et al. / Procedia
1261–1270
Large-scale Nonparallel Support Vector
Ordinal
SolverComputer Science 108C (2017)
Huadong
Wang, Jianyu Miao et. al.

Gutiérrez et al. [9] carried out a thorough experimental study and compared sixteen state-of-the-arts
approaches for ordinal regression. The numerical results demonstrate that SVM-based methods, such
as SVOR [3], RedSVM [13], OPBE [15] and SVMOP [17], have shown promising results and achieved
relatively better performance. However, these approaches are nonlinear models, which map the data
points into a high-dimensional space via a nonlinear feature mapping. Due to the high dimensionality,
the computation of the inner product between vectors in the high-dimensional space is memory and
time consuming. Therefore Nonlinear SVM-based methods are only suitable for small-scale and lowdimensional OR problems. In some applications, data appears in a high-dimensional feature space,
such as word frequency vectors or TF-IDF features extracted from text data by using n-gram words. In
this case, the performances are similar with/without nonlinear mapping. So, to reduce the cost of time
and memory, linear SVM-based methods are preferred to deal with large-scale data sets. In this paper,
we aim to solve large-scale OR problems. Lin et al. [11] studied the algorithms of the standard linear
SVC and SVR, and proposed several large-scale algorithms, such as dual coordinate descent algorithm
(DCD) and trust region algorithm. Although Lin’s algorithms have been widely used in text mining,
these algorithms are mainly for general multi-class classification and numerical regression.
In [19], we have proposed a model called Nonparallel Support Vector Ordinal Regression (NPSVOR),
which is solved by the Alternating Direction Method of Multipliers (ADMM) in nonlinear case. Numerical experiments have shown that NPSVOR is superior to the other SVM-based methods. Therefore,
it is necessary to further study its linear model so that it can be well applied to text mining. Inspired
by the good performance of NPSVOR in nonlinear case, we propose NPSVOR in linear case for largescale problems in this paper. To the best of our knowledge, it is the first work considering large-scale
algorithm in ordinal regression (OR) problems. We design a efficient algorithm based on ADMM [2]
to solve NPSVOR in linear case. Numeral experiments reveal that our model has a better performance
compared with the existing state-of-the-arts methods.
The rest of this paper is organized as follows: In Section 2, we describe the nonparallel support vector ordinal regression model. In Section 3, ADMM algorithm and the preconditioned conjugate gradient
procedure will be introduced for each quadratic optimization in NPSVOR. We conduct experiments
in Section 4 and the corresponding results are studied and discussed. Some conclusions are given in
Section 5.

2

Nonparallel Support Vector Ordinal Regression

In ordinal regression, each sample in the training data set is composed of an input vector and an ordinal
label (i.e., rank). Suppose there are p different ordered categories, without loss of generality, we use
consecutive integers 1, 2, · · · , p to denote the ranks in this paper. Let n be the number of samples. Then
the training set can be represented in the following way
S = {(xi , yi )}i=1,··· ,n
where xi ∈ n is the input vector and yi ∈ {1, 2, · · · , p} is the label of xi . Given k ∈ {1, 2, · · · , p},
we define three index sets for each rank k,
Lk = {i|yi < k}, Ik = {i|yi = k} and Rk = {i|yi > k}.
2

	

Huadong
WangRegression
et al. / Procedia
1261–1270
Large-scale Nonparallel Support Vector
Ordinal
SolverComputer Science 108C (2017)
Huadong
Wang, Jianyu Miao et. al.

where yi is the rank of the instance xi . [19] proposed an approach for the ordinal regression problem by
formulating the following primal problem:

1
[|wkT xi + bk | − ε]+
min (wkT wk + b2k ) + C1
wk ,bk 2
i∈Ik




T
T
[1 − (wk xi + bk )]+
+ C2
[1 + (wk xi + bk )]+ +

(1)

i∈Rk

i∈Lk

where C1 , C2 > 0 are model parameters and [a]+ = max(a, 0).
Denote the optimal proximal hyperplane of rank k obtained by (1) as xT wk ∗ + b∗k = 0 for all
k = 1, 2, · · · , p. Let fk (x) = wk∗  x + b∗k , then the prediction rule could be written as
r(x) = 1 +

p−1


[[fk (x) + fk+1 (x) > 0]]

(2)

i=1

where [[·]] represents a 0-1 indicator function.
To emphasis that p proximal hyperplanes are computed independently and are not necessarily parallel with each other, we name our proposed model in (1) as Nonparallel Support Vector Machine for
Ordinal Regression (NPSVOR) in this paper. Figure 1 gives a geometrical illustration of NPSVOR in
2 .

0

1
=
b2
+
0
T x
= 0
w2
b2
0
+ b2= !
T x
=
1
+
!
b2
w2 T x
=
+
w2
x
b2
T
w2 T x+
w2

8

-8
-8

0

8

Figure 1: The construction of proximal hyperplane for rank 2. “”,“” and “♦” stand for rank 1,2,3,
respectively. The hyperplane should be close to the samples with rank 2. The hyperplane should be far
from the samples with other ranks(1 and 3). Support vectors are marked in bold. Samples whose ranks
are lower than 2 and samples whose ranks are higher than 2 should be located at different sides of the
constructing hyperplane

3

ADMM for Linear NPSVOR

In this section, we discuss how to train the linear NPSVOR efficiently. The ADMM is a convex optimization algorithm dating back to the early 1980’s [7, 8]. With the ability of dealing with objective
functions separately and synchronously, ADMM is turned out to be a natural fit in the field of large-scale
data-distributed machine learning and big-data related optimization, and has received significant amount
3

1263

1264	

Huadong
WangRegression
et al. / Procedia
1261–1270
Large-scale Nonparallel Support Vector
Ordinal
SolverComputer Science 108C (2017)
Huadong
Wang, Jianyu Miao et. al.

of attention in the last few years [2]. Therefore, we propose an efficient algorithm based on ADMM to
solve NPSVOR. In this section, we drop the subscript for rank k for simplicity.
In many applications, data appears in a rich dimensional feature space, and the performances are
similar with/without nonlinear mapping. Linear SVM is one of the most popular tools to deal with
such large-scale sparse data [5, 10, 12]. We concentrate on solving linear NPSVOR with ADMM in this
section.
In some applications, we may omit a bias term b in NPSVOR problems. For convenience, one may
extend each instance with an additional dimension to eliminate this term:
x ← [x , 1], w ← [w , b].
Let X be a data matrix X = [x1 , · · · , xn ] . With the auxiliary variables z, the primal model (1) can
be rewritten as:

where

n

1
min w2 +
g(zi ),
w 2
i=1

subject to z = Xw


 C2 [1+zi ]+ ,
C1 [|zi | − ε]+ ,
g(zi )=

C2 [1 − zi ]+ ,

(3)

i∈L
i∈I
i∈R

The scaled augmented Lagrangian of (3) can be formulated as:
Lρ (w, z, u) =

n

1
ρ
2
2
w2 +
g(zi ) + (Xw − z + u − u )
2
2
i=1

(4)

where u is the scaled dual variable and ρ > 0 is the penalty parameter. ADMM consists of the following
iterations:
wt+1 = arg min Lρ (w, z t , ut )

(5a)

z t+1 = arg min Lρ (wt+1 , z, ut )

(5b)

w

z

t+1

u

t

= u +Xw

t+1

−z

t+1

(5c)

In details, the w-update in (5a) is a ridge regression,i.e.
ρ
1
wt+1 := arg min w2 + Xw − z t + ut 2
w 2
2
It is equivalent to solving the following linear equations system:
(I + ρX  X)w = ρX  (z t − ut ).

(6)

where I is the identity matrix. Since the coefficient matrix I + ρX  X is positive definite, several
algorithms can be adopted to solve (6). As stated above, we focus on the large-scale problems. Then the
scale of I + ρX  X will be very large, which can degenerate the efficiency or even can be infeasible
with traditional methods. Instead, we use conjugate gradient method to efficiently solve (6). Given
a preconditioner P and let H = P −1 ∇2 ht (wt )P − , the PCG algorithm to approximately solve the
ADMM sub-problem (5a) is given in Algorithm 1. The main operation is the computation of the product
Hdi , which can be implemented using the idea in (7),


 

.
(7)
Hdi = P −1 P − di + ρ X  X P − di
4

	

Huadong
WangRegression
et al. / Procedia
1261–1270
Large-scale Nonparallel Support Vector
Ordinal
SolverComputer Science 108C (2017)
Huadong
Wang, Jianyu Miao et. al.

It is usually difficult to find a suitable preconditioners such that P −1 ∇2 ht (wt )P − is close to the
identity matrix. One of popular preconditioners is the diagonal matrix of the Hessian matrix. In our
problem, we use the following diagonal matrix as the preconditioner,
P = diag (λ1 , λ2 , . . . , λn ) ,

1/2
where λi = ∇2 ht (wt ) ii = (1 + ρxTi xi )1/2 .

Algorithm 1 PCG for solving the ADMM sub-problem (5a)
1: Given ξt < 1. Let ŝ0 = 0, r 0 = −P −1 ∇ht (w t ) and d0 = r 0
2: for i = 1, 2, · · · do
3:
If r i  ≤ ξt r 0 , then output st = P −1 ŝi and stop.
4:
αi = r i 2 /((di ) Hdi ).
5:
ŝi+1 = ŝi + αi di .
6:
r i+1 = r i − αi Hdi .
7:
βi = r i+1 2 /r i 2 .
8:
di+1 = r i+1 + βi di .
9: end for
The z-update can be split to the component level. The z-update has a closed form solution and can
be expressed as the shifted soft thresholding operation elementwisely, i.e. if i ∈ L, then

and if i ∈ R, then
where

t+1
zit+1 = TC2 /ρ,1,1 (x
+ uti )
i w

(8)

t+1
zit+1 = TC2 /ρ,−1,1 (x
+ uti )
i w

(9)


 a − γθ,
−τ /γ,
Tθ,γ,τ (a) =

a,

γa > θ − τ
γa ∈ [−τ, θ − τ ]
γa < −τ

is a proximal operator. In fact, Tθ,γ,τ (a) = arg min θ[γz + τ ]+ + 12 (z − a)2 .
z

Since Fθ,ε (a) = arg min θ[|z| − ε]+ + 12 (z − a)2 has a closed form solution:
z

then


a − θ,




 ε,
a,
Fθ,ε (a) =


 −ε,


a + θ,

a>θ+ε
a ∈ [ε, ε + θ]
|a| ∈ [0, ε)
a ∈ [−θ − ε, −ε]
a < −θ − ε

t+1
zit+1 = FC1 /ρ, (x
+ uti ), i ∈ I.
i w

It is observed that
Fθ,ε (a)=



Tθ,1,−ε (a),
Tθ,−1,−ε (a),

(10)

a≥0
a<0

We summarize the above discussion in Algorithm 2.
5

1265

1266	

Large-scale Nonparallel Support Vector
Ordinal
SolverComputer Science 108C (2017)
Huadong
Wang, Jianyu Miao et. al.
Huadong
WangRegression
et al. / Procedia
1261–1270

Algorithm 2 ADMM for Linear NPSVOR
1: Set t := 0, choose ρ, , C1 , C2 > 0 and initialize w 0 , z 0 .
2: repeat
3:
Apply the preconditioned conjugate gradient procedure(PCG) to solving the ADMM subproblem (5a), update wt to wt+1 : wt+1 = wt + st .
4:
Update z t+1 by (8),(9) and (10);
5:
Update ut+1 by (5c);
6:
t := t + 1;
7: until ∇ht (w t ) < ∇h0 (w 0 )

4

Numerical Experiments

In this section, we compare linear NPSVOR described in Section 2 and Section 3 with other SVM-based
ordinal regression models. All the experiments are conducted on 3.80GHz ADM FX(tm)-4300 CPU PC
running Linux with 4GB main memory. Our implementations are extended from LIBLINEAR. Our
code is available at https://github.com/smhbamakan/LinearNPSVOR/. We considered
the following datasets in our experiments:
• Amazon product reviews1 , including five different types with different scales of product review
datasets (AmazonMp3, Video,Tablets, Mobilephone, Camera) which originally used in [18].
Each review consists of a overall rating (1-5 stars) and the review text.
• TripAdvisor2 , is a hotel review dataset originally used in [18] and built by Baccianella et al. [1].
It consists of reviews scored on a scale from 1 to 5 stars.
• Treebank3 : Stanford Sentiment Treebank, an extension of movie reviews with one sentence per
review and fine-grained labels (very negative, negative, positive, neutral, very positive) are provided.
• MovieReview [14], a collection of documents whose labels come from a rating scale4 . The
numerical rating converted from the four-star system used by the author (1/2 star was the smallest
unit he employed) is 0.1. Its continuous target variable was discretized to four ordinal scales by
1 : rating ≤ 0.3, 2 : 0.4 ≤ rating ≤ 0.5, 3 : 0.6 ≤ rating ≤ 0.7, 4 : 0.8 ≤ rating. This dataset
are usually used for sentiment analysis.
• LargeMovie5 , a movie review dataset for eight sentiment classification. It provides a set of 25,000
highly polar movie reviews for training, and 25,000 for testing. We combine the training dataset
and testing dataset together in our experiment.
For all the English datasets, stemming, stop word removal and omitting the words that occur less than
three times, the term document frequency is larger than 50% or is shorter than 2 in length are executed
in the preprocessing. Word frequency is used to extract text features from unigrams and bigrams. The
statistics of data sets, such as the numbers of instances, features, nonzero elements, are summarized in
Table 1.
1 The

data is available at http://sifaka.cs.uiuc.edu/˜wang296/Data/index.html
dataset is available for download from http://www.cs.virginia.edu/˜hw5x/dataset.html
3 http://nlp.stanford.edu/sentiment/
4 scale dataset v1.0: http://www.cs.cornell.edu/people/pabo/movie-review-data/
5 http://ai.stanford.edu/ amaas/data/sentiment/
˜
2 The

6

	

Large-scale Nonparallel Support Vector
Ordinal
SolverComputer Science 108C (2017)
Huadong
Wang, Jianyu Miao et. al.
Huadong
WangRegression
et al. / Procedia
1261–1270

Datasets
AmazonMP3
Video
Tablets
Mobilephone
Camera
TripAdvisor
Threebank
MovieReview
Large Movie

Table 1: Data statistics

Instances Features Nonzeros Classes
Class distribution
31000 780725 3861866
5
( 0.1421, 0.0670, 0.0816, 0.2422, 0.4671 )
59156 755794 5115182
5
( 0.1328, 0.0753, 0.1029, 0.2337, 0.4553 )
124288 1131928 11011720
5
( 0.1119, 0.0566, 0.0736, 0.1666, 0.5913 )
185864 1787678 14623069
5
( 0.1921, 0.0752, 0.0946, 0.1844, 0.4546 )
471532 2682569 28649757
5
( 0.0933, 0.0585, 0.0756, 0.2059, 0.5667 )
198980 3535310 4253448
5
(0.0657, 0.0856, 0.1047, 0.3062, 0.4379 )
11855 88424 185051
5
( 0.1274, 0.2649, 0.1891, 0.2642, 0.1562 )
5006 581323 1545880
4
( 0.1229, 0.3102, 0.3991, 0.1678 )
50000 2556445 90119111
8
( 0.2024, 0.0917, 0.0992, 0.1066, 0.0961,
0.1172, 0.0921, 0.1946 )

Two evaluation measures, which are utilized to quantify the accuracy of predicted ordinal scales
{ŷ1 , . . . , ŷn } with respect to true targets {y1 , . . . , yn }, are used. The first one is the mean absolute error
(MAE), which is the average deviation of the prediction from the true target, i.e.
MAE =

n
1
|yˆi − yi |.
n n=1

(11)

The other one is mean squared error (MSE)
MSE =

n
1
(yˆi − yi )2 ,
n n=1

(12)

which is widely used as a evaluation metric to measure the divergences between predicted sentiment
labels [4,16] and ground truth sentiment labels. To examine the testing MAE/MSE, a stratified selection
on each data set is one fifth for testing and the rest for training.
To test the performance of our method, We compare our method linear NPSVOR with the following
three state-of-the-art SVM-based methods:
(1) SVC [12], an naive approach which treats the OR problem as a general multi classification problem. [12] provides the DCD algorithm for solving linear SVM, and take two important accelerated
strategies, the shrinking and random permutation. The method can be founded in LIBLINEAR
software package, using one vs. all strategy to deal with multi-class problems.
(2) RedSVM (Reduction from ordinal ranking to a binary classification using SVM) [13], a threshold
approach that finds p − 1 thresholds that divide the real-valued line into p consecutive intervals
corresponding to the p ranks. [13] proposed the reduction framework which reduce the ordinal
regression to binary classification. It expands the sample (x, y) into a binary classification prob
lem ((x
i , ek ), ỹki ) (i = 1, . . . , l; k = 1, . . . , p − 1), where ỹki = −1 if yi < k, otherwise,
ỹki = 1, and ek is a one-hot vector with dimension p − 1 and only k-th item with one. Then the
DCD algorithm for SVM can also be used to solve RedSVM. The prediction function is defined
p−1

[[fk (x) > 0]], where fk (x) = wT x − bk is the decision function of the k-th
as r(x) = 1 +
k=1

binary classifier.

(3) SVMOP (Support Vector Machine with Ordinal Partition), an ordered binary decomposition
method, decomposes the ordinal regression problem into a series of ordered binary classification
7

1267

1268	

Large-scale Nonparallel Support Vector
Ordinal
SolverComputer Science 108C (2017)
Huadong
Wang, Jianyu Miao et. al.
Huadong
WangRegression
et al. / Procedia
1261–1270

problems [17]. Each binary classification can adopt any classification method, such as decision
tree, adaboost or SVM. SVMOP is a method which takes the standard SVM as the binary classification method. Its prediction function is (13),
r(x) = arg

min

(13)

{k : fk (x) < 0}(fp (x) = −∞).

k∈{1,...,p}

where fk (x) = w
k x is the decision function of the k-th binary classifier.
Since no large-scale algorithm has proposed for linear RedSVM and SVMOP, the DCD algorithm
for SVM is also implemented based on LIBLINEAR. The parameter C is chosen within the range
[2−5 , 2−7 , . . . , 25 ] by five-fold cross validation (CV) on the training set. Using models trained under the
best C, we predict the performance on the testing set. For the fair comparison, we set the parameters
C1 = C2 in the NPSVOR and fix  at the value 0.1. From the suggestion of [19], we choose ρ = 1. In
the preconditioned conjugate gradient procedure, we use ξk = 0.1 in the stopping condition. The results
of MSE and MAE are given in Table 2 and Table 3.
Table 2: Results of MAE for all methods with and without the bias term on all datasets. The best results
are highlighted in boldface.
Datasets
AmazonMP3
Video
Tablets
Mobile Phone
Cameras
TripAdvisor
Threebank
MovieReview
LargeMeiew

SVC
0.5600
0.3622
0.1817
0.4092
0.1410
0.4419
0.9443
0.4625
1.1649

Without Bias
RedSVM
SVMOP
0.5913
0.5081
0.4280
0.3425
0.2131
0.1685
0.4725
0.3859
0.1687
0.1306
0.4986
0.3974
0.8267
0.8469
0.4176
0.4016
1.2426
1.0302

NPSVOR
0.5211
0.3463
0.1722
0.4014
0.1342
0.4028
0.8402
0.4106
1.0495

SVC
0.5624
0.3659
0.1838
0.4203
0.1443
0.4621
0.9502
0.4645
1.1949

With Bias
RedSVM
SVMOP
0.5877
0.5139
0.4453
0.3465
0.2123
0.1686
0.4729
0.3847
0.1673
0.1323
0.4972
0.4109
0.8203
0.8237
0.4176
0.4206
1.2430
1.0468

NPSVOR
0.5382
0.3573
0.1735
0.4040
0.1367
0.4346
0.8267
0.4206
1.0645

Table 3: Results of MSE for all methods with and without the bias term on all datasets. The best results
are highlighted in boldface.
Datasets
AmazonMP3
Video
Tablets
MobilePhone
Cameras
TripAdvisor
Threebank
MovieReview
LargeMeiew

SVC
1.1052
0.6908
0.3362
0.8064
0.2451
0.6405
1.7541
0.5564
3.6085

Without Bias
RedSVM
SVMOP
0.9635
0.8503
0.6813
0.5516
0.3372
0.2644
0.7744
0.6362
0.2593
0.1978
0.6479
0.5049
1.3378
1.3471
0.4595
0.4595
2.8548
2.4508

NPSVOR
0.8231
0.5385
0.2617
0.6320
0.2003
0.5015
1.3041
0.4525
2.3816

SVC
1.1289
0.7070
0.3459
0.8322
0.2599
0.6871
1.7356
0.5544
3.8730

With Bias
RedSVM
SVMOP
0.9713
0.8555
0.6912
0.5616
0.3383
0.2639
0.7816
0.6266
0.2564
0.2008
0.6465
0.5216
1.2041
1.2758
0.4575
0.4805
2.8662
2.4507

NPSVOR
0.8376
0.5495
0.2631
0.6241
0.2027
0.5387
1.2299
0.4705
2.3796

From the results, we can make the following observations:
(1) The NPSVOR with or without offset has a better performance than the SVC and RedSVM on the
MAE. Although the results of NPSVOR are somewhat inferior to SVMOP, NPSVOR achieved
8

	

Large-scale Nonparallel Support Vector
Ordinal
SolverComputer Science 108C (2017)
Huadong
Wang, Jianyu Miao et. al.
Huadong
WangRegression
et al. / Procedia
1261–1270

the best overall result on MSE. This shows that the predicting of NPSVOR is closer to the true
label and reduces the possibility of a larger deviation prediction.
(2) The RedSVM is superior to the RedSVM in the non-linear case [3]. However, from the experimental results in this paper, the performance of the data is very general and even inferior to the
standard SVC in some data sets.
(3) There is a difference performance between the model with bias or without bias. On the whole, the
model without bias is better than the model with bias on the high-dimensional datasets. In other
words, it is better to choose the biased model for the datasets with lower dimensional features.

5

Conclusions

We propose a new approach called NPSVOR in linear case for ordinal regression, which can efficiently
deal with large scale problems. The ADMM algorithm is designed to efficiently solve linear NPSVOR.
It is very simple to implement, and it possesses sound optimization properties. Experimental results
show that our method outperforms the state of the art methods in terms of the measure MSE. There
exists more works that can be down for the further study of the model (NPSVOR). In addition, the
ADMM algorithm designed here is not a distributed algorithm for each sub-problem. We can extend
this algorithm to a distributed form through splitting the training examples or features(see [2] and [6]
for more details).

Acknowledgements
This work is partially supported in part by the National Natural Science Foundation of China under
Grant No.11671379, No.11331012, No.71331005, No.91546201.

References
[1] Stefano Baccianella, Andrea Esuli, and Fabrizio Sebastiani. Multi-facet rating of product reviews. In European Conference on Ir Research on Advances in Information Retrieval, pages 461–472, 2009.
[2] Stephen Boyd, Neal Parikh, Eric Chu, Borja Peleato, and Jonathan Eckstein. Distributed optimization and
statistical learning via the alternating direction method of multipliers. Foundations and Trends in Machine
Learning, 3(1):1–122, 2011.
[3] Wei Chu and S Sathiya Keerthi. Support vector ordinal regression. Neural computation, 19(3):792–815, 2007.
[4] Qiming Diao, Minghui Qiu, Chao Yuan Wu, Alexander J. Smola, Jing Jiang, and Chong Wang. Jointly
modeling aspects, ratings and sentiments for movie recommendation (jmars). In ACM SIGKDD International
Conference on Knowledge Discovery and Data Mining, pages 193–202, 2014.
[5] Rong-En Fan, Kai-Wei Chang, Cho-Jui Hsieh, Xiang-Rui Wang, and Chih-Jen Lin. Liblinear: A library for
large linear classification. The Journal of Machine Learning Research, 9:1871–1874, 2008.
[6] Pedro A Forero, Alfonso Cano, and Georgios B Giannakis. Consensus-based distributed support vector machines. The Journal of Machine Learning Research, 11:1663–1707, 2010.
[7] M. Fortin and R. Glowinski. Chapter III on decomposition-coordination methods using an augmented lagrangian. Studies in Mathematics & Its Applications, pages 97–146, 1983.
[8] D. Gabay. Applications of the method of multipliers to variational inequalities. Augmented Lagrangian
Methods: Applications to the Numerical Solution of Boundary-value Problems, 1983.

9

1269

1270	

Large-scale Nonparallel Support Vector
Ordinal
Solver Computer Science 108C (2017)
Huadong
Wang, Jianyu Miao et. al.
Huadong
WangRegression
et al. / Procedia
1261–1270

[9] Pedro Antonio Gutiérrez, Marı́a Pérez-Ortiz, Javier Sánchez-Monedero, Francisco Fernández-Navarro, and
C Hervás-Martınez. Ordinal regression methods: survey and experimental study. IEEE Transactions on
Knowledge and Data Engineering, 28(1):127–146, 2016.
[10] Chia-Hua Ho and Chih-Jen Lin. Large-scale linear support vector regression. The Journal of Machine Learning Research, 13(1):3323–3348, 2012.
[11] Cho Jui Hsieh, Kai Wei Chang, Chih Jen Lin, S. Sathiya Keerthi, and S. Sundararajan. A dual coordinate
descent method for large-scale linear svm. In ICML, pages 1369–1398, 2008.
[12] Cho-Jui Hsieh, Kai-Wei Chang, Chih-Jen Lin, S Sathiya Keerthi, and Sellamanickam Sundararajan. A dual
coordinate descent method for large-scale linear svm. In Proceedings of the 25th international conference on
Machine learning, pages 408–415. ACM, 2008.
[13] Hsuan-Tien Lin and Ling Li. Reduction from cost-sensitive ordinal ranking to weighted binary classification.
Neural Computation, 24(5):1329–1367, 2012.
[14] Bo Pang and Lillian Lee. Seeing stars: exploiting class relationships for sentiment categorization with respect
to rating scales. In Meeting on Association for Computational Linguistics, pages 115–124, 2005.
[15] Marı́a Pérez-Ortiz, Pedro Antonio Gutiérrez, and César Hervás-Martı́nez. Projection-based ensemble learning
for ordinal regression. Cybernetics, IEEE Transactions on, 44(5):681–694, 2014.
[16] Duyu Tang, Bing Qin, and Ting Liu. Document modeling with gated recurrent neural network for sentiment
classification. In Conference on Empirical Methods in Natural Language Processing, 2015.
[17] Willem Waegeman and Luc Boullart. An ensemble of weighted support vector machines for ordinal regression. International Journal of Computer Systems Science and Engineering, 3(1):47–51, 2009.
[18] Hongning Wang, Yue Lu, and Chengxiang Zhai. Latent aspect rating analysis on review text data: a rating
regression approach. In ACM SIGKDD International Conference on Knowledge Discovery and Data Mining,
pages 783–792, 2010.
[19] Huadong Wang, Yong Shi, Lingfeng Niu, and Yingjie Tian. Nonparallel support vector ordinal regression.
IEEE Transactions on Cybernetics, 2016.

10

