Complex Workﬂow Management of the
CAM Global Climate Model on the GRID
V. Fern´
andez-Quiruelas, J. Fern´
andez, A.S. Coﬁ˜
no, C. Baeza, F. Garc´ıa-Torre,
R.M. San Mart´ın, R. Abarca, and J.M. Guti´errez
University of Cantabria, Spain. SENAMHI, Per´
u. UDEC, Chile.
(On behalf of the EELA team)
valvanuz.fernandez@gestion.unican.es
http://www.meteo.unican.es

Abstract. Recent trends in climate modeling ﬁnd in GRID computing a powerful way to achieve results by sharing computing and data
distributed resources. In particular, ensemble prediction is based on the
generation of multiple simulations from perturbed model conditions to
sample the existing uncertainties. In this work, we present a GRID application consisting of a state-of-the-art climate model (CAM) [1]. The
main goal of the application is providing a user-friendly platform to
run ensemble-based predictions on the GRID. This requires managing
a complex workﬂow involving long-term jobs and data management in a
user-transparent way. In doing so, we identiﬁed the weaknesses of current
GRID middleware tools and developed a robust workﬂow by merging the
optimal existing applications with an underlying self-developed workﬂow.
Keywords: GRID computing, workﬂow, long term jobs, climate models,
CAM model, El Ni˜
no phenomenon, GRID-CAM application.

1

Introduction

GRID technologies emerged in the 90’s as a way to share computer resources
and other scientiﬁc equipment across geographically distributed locations in a
user-transparent way [2]. By sharing computer resources it is meant not only
to share their storage capacity, but also the computer power, which would be
used to run applications. The user transparency relies on what is referred to as
“middleware”, a software layer between the applications and the GRID infrastructure. A number of research and commercial projects have developed diﬀerent
middleware solutions and applications (e.g. the EGEE project [3] is the reference in GRID development in Europe). New applications ported to the GRID
demand new services which are not always available in the existing middleware.
In this paper, we present a new paradigmatic example on the area of numerical
climate simulation which demands solutions in terms of, e.g., job duration and
workﬂow management.
The EU-funded project EELA (E-Infraestructure shared between Europe and
Latin America) aims at bringing the e-Infrastructures of Latin American countries to the level of those of Europe, identifying and promoting a sustainable
M. Bubak et al. (Eds.): ICCS 2008, Part III, LNCS 5103, pp. 471–480, 2008.
c Springer-Verlag Berlin Heidelberg 2008

472

V. Fern´
andez-Quiruelas et al.

framework for e-Science [4]. Among other tasks, EELA aims at identifying new
applications to be ported to the GRID. The present paper describes the new
developments achieved as a result of porting a climate application to the GRID
under the EELA framework with the goal of analysing el Ni˜
no phenomenon,
which is a key factor for Latin-American (LA) climate prediction. El Ni˜
no has
a special interest due to its direct eﬀect in the Paciﬁc coast of South America
and, in particular, in Peru and Chile (EELA LA partners).
We selected a Global Circulation Model (GCM; see Section 2) as the ﬁrst
application to be ported to the GRID, since any further simulation or analysis
step would require a global simulation as starting point. The particular features
of the GCM (experiments lasting beyond proxy certiﬁcates lifetime, control of
jobs, etc) are described in Section 3. Using the existing middleware solutions
(Section 4) we designed a new application developing extra middleware to run
the GCM in the GRID with a speciﬁc workﬂow, solving most of the problems
encountered.

2

Climate Modeling and GRID Computing

Climate models are complicated computer programs which require large amounts
of CPU power. Most of them are parallelized. However, the GRID cannot make
the most of this kind of parallelism, since the latency across geographically distributed computers would render the program completely ineﬃcient.
Apart from computer parallelism, climate science is recently making use of a
large number of simulations, referred to as “ensemble”, of the same phenomenon
in order to assess the uncertainty inherent to the simulation [5,6]. Ensembles
of simulations with varying parameters are also used for sensitivity experiments
and many other applications. Each simulation in an ensemble is independent
of the others and can be run asynchronously. This kind of parametric jobs is
well suited for the GRID, since each simulation can be carried out in diﬀerent
nodes and the results are made available as a uniform data set in the Logical
File Catalogue (LFC; see Section 4 below) [7], ready to be analyzed.
Unlike volunteer computing projects, such as climateprediction.net [8],
where the GCM needs to be simpliﬁed and most of the results thrown away to
avoid the overloading of the volunteer hosts, the GRID allows running a full
state-of-the-art model and store the regular output information.
A GCM poses speciﬁc problems to the GRID (see Section 3), which cannot
be solved by the existing general solutions to easily port legacy applications to
the GRID. Solutions such as GEMLCA [9] use the application to be ported as
a black box and, thus, cannot monitorize intermediate states of the simulation
or manage the delivery of completed output ﬁles to the catalog.
2.1

Climate Model Used

Dynamical climate models are mathematical models that numerically solve the
nonlinear equations governing the atmosphere on a global lattice with horizontal

Complex Workﬂow Management of the CAM Global Climate Model

473

resolutions ranging from 50km to 300km, depending on the application. These
models require a set of initial conditions (values of climate variables – wind, pressure, temperature, etc, – on the lattice points at the starting time) to propagate
the solution forward in time.
In order to analyze the atmospheric part of the global climate system, we
selected the CAM model (Community Atmosphere Model), which is the latest
in a series of atmosphere GCMs developed at NCAR for the weather and climate
research communities [1]. The model can be run either in parallel (using MPI)
or as a single process. The single-process version has been deployed and run
in the EELA testbed with T42 resolution: 128 (longitude) × 64 (latitude) and
27 vertical levels, i.e. 221184 points per time step. The model produces 32 3-D
and 56 2-D variables over the lattice. Therefore it is expensive in CPU-time and
storage capacity. The simulation of a year takes approximately 48 CPU hours
(i.e. 100 years would take 7 CPU months) and produces 197 MB per time step
(i.e. more than 720 GB per century). We are interested on simulating the climate
during 1.5 years to study El Ni˜
no phenomenon. The application we designed aims
to perform sensitivity experiments by running an ensemble of simulations with
varying parameters (related to the sea surface temperature).

3

Requirements and Workﬂow Management

It is currently uncommon the use of GRID computing to run long-term jobs,
due to the high rate of job failure and the CPU-time limitations for the jobs
on the local management system (typically only jobs lasting less than 48 hours
are allowed). These problems become critical for long simulations such as those
performed with climate models and other similar Earth Science applications.
Thus, unlike many other applications ported to GRID, earth science applications
need to make use of advanced techniques in workﬂow management. In particular,
the climate application described in this paper has the following requirements:
1. Failure aware: Due to the nature of GRID there are several reasons which
may cause job failures in the testbed, including heterogeneity of resources,
CPU-time limited queues, etc.
2. Checkpointing for restart: The complexity of the climate model runs may
require jobs to be restarted in a diﬀerent working nodes due, for instance,
to the excessive duration of the job.
3. Monitoring: Since the climate simulations last for a long time, we need to be
aware of the simulation status once it has been sent to the testbed: whether
the model is running or not, which time step is being calculated, which ﬁles
have been uploaded to Storage Elements [10], which is the last restarting
point, etc.
4. Data and Metadata storage: The goal of our application is the generation
of output information that can be easily accessed by users, so data and
metadata should be stored in an appropriate form.
The above requirements made necessary the development of a goal-oriented
workﬂow manager in order to run the experiments and analyze the results with a

474

V. Fern´
andez-Quiruelas et al.

minimum of human intervention. Therefore, we developed the application GRIDCAM which is a “GRID workﬂow management tool for simulating climate with
CAM”.
3.1

The GRID-CAM Application

In this section we brieﬂy introduce and deﬁne the diﬀerent components involved
in a typical climate simulation. We deﬁne an experiment as an ensemble of
simulations (parametric jobs) designed to answer some scientiﬁc question (a
single execution is the simplest experiment); each of these executions is called a
realization and requires a set of input data to run the model in the prescribed
simulation period (typically one year). A particular type of experiments are those
related to climate sensitivity studies. In this case the diﬀerent sets of input data
are obtained from a single one including certain user-deﬁned perturbations to
form the ensemble (perturbed initial or boundary conditions, etc.).
The lowest level component of our application is a job. This component
matches with a standard GRID job and cannot be related one to one with
a realization since realizations cannot be guaranteed to ﬁnish in a single job.
In general, a realization requires several jobs to complete, each one restarted
from the previous one. As the job is running, the model generates information
(ﬁles and metadata) that has to be available from every other component of
the GRID: restart ﬁles (for failure recovery), current simulation time step, number of restarts, job id (for monitoring purposes), statistical information, output
data, etc. Hereinafter, all the data and metadata generated by the models will
be referred to as output information.
Therefore, numerical climate simulation on the GRID requires the management of a complex workﬂow formed by experiments composed of realizations
split across jobs. This workﬂow is not trivially managed by the currently available GRID middleware, so new features are necessary for a proper execution of
climate simulations.

4

Middleware Used in GRID-CAM

The gLite middleware is an integrated set of components designed to enable
resource sharing in GRID [11]. The core components of the gLite architecture
are the following:
– User Interface (UI): It is the access point to the GRID.
– Computer Element (CE): A set of computing resources localized at a site
(i.e. a cluster, a computing farm).
– Worker node (WN): The cluster nodes where the jobs are run.
– Storage Element (SE): Separate service dedicated to store ﬁles.
The Logical ﬁle catalog(LFC) [7] is a secure GRID catalog containing logical
to physical ﬁle mappings. The primary function of the LFC is to provide central
registration of data ﬁles distributed amongst the various Storage Elements [10].

Complex Workﬂow Management of the CAM Global Climate Model

475

On the other hand, AMGA [12] is the gLite Metadata Catalogue, and we just
use it as a classical GRID-enabled database where we store all the status and
metadata information we need.
We also used GridWay [13], which is a GRID meta-scheduler that gives a
scheduling framework similar to that found on local Resource Management systems, supporting resource accounting, fault detection and recovery and the deﬁnition of state-of-the-art scheduling policies. Compared with the LCG workload
management it is much faster and easy to use [14].
Besides the previous existing middleware products, some GRID developments
were necessary in order to deploy the climate application and to develop the
appropriate workﬂow elements. These new components are described in the following sections.
4.1

The Grid Enabling Layer (GEL)

Climate models are mature applications with thousands of lines of code, which
need to be ported to the GRID introducing small modiﬁcations to the code to
perform system calls to speciﬁc applications which are in charge of interacting
with the GRID on behalf of the climate model. To this aim, we developed a new
software layer, referred to as GRID Enabling Layer (GEL), which provides the
model with the ability to interact with the GRID. The slightly modiﬁed source
code of the model plus its GEL conform a fully featured GRID application. Since
climate models are developed by external institutions, this approach is the best
suited to keep up with the most recent updates with the least eﬀort, since only
the small modiﬁcations to interact with the GEL need to be introduced at key
points of any new release.
The GEL provides the following capabilities:
– Realization monitoring: Since our simulations last for a long time, we need
to know their status once they have been sent to the testbed: If the model
is preparing the WN or running, which step of time is calculating, which
ﬁles has uploaded to SE-LFC, which is the last restart pointer, etc. This is
analyzed in detail en the next section.
– Management of restart: Each time CAM dumps a new restart ﬁle, the GEL
uploads the restart ﬁles to the nearest SE and register them in the LFC. It
also publishes the restart ﬁeld associated to this experiment in the AMGA
database. This way, if the job fails and the realization is rescheduled to
another WN, it will continue calculating from this time step.
– Data and Metadata management: In order to store all the output and restart
information generated by the model, we need that the metadata and ﬁles
are permanently registered in a place accessible from any component of the
GRID (AMGA and LFC-SE).
The above issues were solved by introducing Fortran system calls at 4 speciﬁc
points of the CAM source code. These calls execute the GEL scripts which
carry out the previously mentioned tasks. The GEL consists of a series of scripts

476

V. Fern´
andez-Quiruelas et al.

written in higher level languages (Shell, Perl and Python) allowing for a faster
development process and an easier interface with the middleware.
4.2

Workﬂow Design Using GridWay and AMGA

In order to manage the workﬂow needed to build an unattended application
we used GridWay for scheduling the jobs with re-schedule-on-failure capabilities
and AMGA for monitoring.
GridWay Conﬁguration. After considering several job managers, we found
that GridWay meta-scheduler was the one that best fulﬁlled our requirements,
since GridWay is able to detect job failures for any of the problems mentioned in
Section 3, and it is able to re-schedule the failed jobs to another CE. Moreover,
once the re-scheduled job starts to run in the WN, a component developed
within the GRID-CAM application queries the AMGA database to ﬁnd the
latest restart ﬁles for this realization in order to continue the simulation started
for the previous job. We have also adopted an additional monitoring feature
provided by GridWay. For debugging purposes, while the job is running in the
WN, a monitor script (running also in the WN) checks the status of the job.
This monitor can copy the output and error ﬁles of our job to the UI with a
given frequency. In this way, from the UI we monitorize the exact status of each
of our realizations. Additionally, key information for the application workﬂow is
stored in AMGA (Section 4.2).
When an ensemble of simulations is sent to the GRID, each realization of the
ensemble is converted to a GridWay job that is sent to the scheduler. When
GridWay receives the jobs, it searches the CE better suited to our application
needs and chooses the best among them. To do so, it uses a powerful scheduling
policy that takes into account the user requested requirements (memory, CPU,
etc.) and an heuristic scheduling based on the jobs sent in the past. For instance,
if all jobs sent to a CE failed, GridWay will not send jobs to that site again.
The components and ﬂow of our workﬂow design are shown in Fig. 1.
Finally, in order to manage the issue of the expiration of the proxy, which
aﬀects every long lasting job, we used the myproxy credential management system as a provisional solution that is able to extend the authenticated time to
one week. More research is required to deploy longer term unattended climate
simulations.
Monitoring with AMGA. The AMGA database has two diﬀerent tasks in
the application. On one hand, it is used to store the information generated by the
experiments executed in the GRID. On the other hand, it is used for monitoring
purposes, storing all the status information about each of the simulations as
metadata information. The tables and relationships used by GRID-CAM are
shown in Fig. 2. Some of them are also relevant to the workﬂow, as described
below:
– EXPERIMENT: When preparing the experiment, this table is ﬁlled with
the perturbation type used (multiplicative, random, etc), the number of realizations and a description and dates of start and end.

Complex Workﬂow Management of the CAM Global Climate Model

CE

UI

SUSP

Scheduled

CERR

477

WN
Pending >5min

Inactive >5min

Pending
CE

Running
WN

Submit failed

PERF

CPU <30%

MERR

Middleware
failure

SUCC

WALL

Walltime

Fig. 1. Components and ﬂow diagram of GRID-CAM. See Section 5 for details of the
error signals.

– REALIZATION: Each realization can be executed in many diﬀerent nodes.
This table keeps track of current time step, restart ﬁles, id of the current job
executing the realization, etc.
– JOB: This table is used to keep track of the diﬀerent jobs used in an experiment. It stores the timing information, the WN and the realization it
contributed to. Most of this information is stored for statistical purposes.
– OUTPUTFILE: Each realization generates a number of ﬁles as it runs. This
table stores metadata and access information for the ﬁles stored in the catalog. This speeds up the data discovery process.

5

Experimental Results

In order to test the GRID-CAM application, we ran a simple experiment consisting of 100 realizations simulating the climate on El Ni˜
no region during a
period of two years; to this aim, we used diﬀerent initial conditions as input for
the realizations (perturbed sea surface temperatures). The GRID-CAM workﬂow used part of the resources from the EELA testbed and was executed within
an arbitrary week. Therefore, the results reported here are just a particular illustration of the application’s performance and cannot be considered for testbed
comparison or benchmarking purposes.
In order to make our experiment as realistic as possible and to observe the
eﬃciency of the workﬂow manager, we used the full list of sites from the EELA
project. Some of the sites used for this experiment are located in Latin American
countries. This made even more likely the occurrence of errors in the workﬂow
due to network latency problems (this is one of the EELA challenges).
The experiment lasted one week and at the end of this period 89 realizations
concluded successfully, 7 were still running and 4 crashed without ﬁnishing.
Regarding the workﬂow, we obtained the following results:

478

V. Fern´
andez-Quiruelas et al.

Fig. 2. Structure of the AMGA database used to store metadata and status information
for the GRID-CAM application

– GridWay needed to run 1080 Globus jobs to complete the 100 realizations,
from which:
28% SUCC or WALL: Finished OK (reaching the end of the simulation or
exhausting the walltime allowed by the local queue)
31% CERR: Failed in the CE. These failures were due to misconﬁgured CE.
17% SUSP: Suspension timeout. GridWay is conﬁgured to kill a Globus job
if the job is waiting on the CE queue more than 5 minutes.
8% PERF: Killed by our monitor in the WN because the CPU time dedicated
to the job in the WN was lower than a 30%.
16% MERR: Killed by our monitor because the GEL experienced problems
contacting the GRID middleware (SE, CE or AMGA). Most of these jobs
were run in Latin America and the main cause of failure was a network
outage.
– The experiment generated 300GB of output data replicated in 2 diﬀerent
SE in Europe. Metadata of this data was also successfully published in the
AMGA database for later use.
– Our workﬂow failed to manage 4 realizations. After analysing the output,
we discovered that the errors were due to middleware errors that we did not
manage. We have ﬁxed the application to solve this problem.
The amount of CERR errors was caused by the misconﬁguration of 2 sites
where the jobs failed systematically. The rate of SUSP is also higher than expected. During the experiment there were some sites that did not accept jobs
because their queues were collapsed.

6

Conclusions

We presented a successful port of a state-of-the-art global climate model (CAM)
to run unattended on the GRID. The port consisted of two main components:

Complex Workﬂow Management of the CAM Global Climate Model

479

The Grid Enabling Layer to allow CAM interface with the GRID middleware and
a failure-aware workﬂow built on the GridWay meta-scheduler. The application
was tested in a realistic experiment.
The main conclusion of this test is that, although many problems (inherent to
the GRID [15]) arose during the execution of the experiment, the GRID-CAM
workﬂow was able to restart the simulations in most of them, allowing to ﬁnally
obtain nearly 90% of successful complex realizations suitable for a statistical
study of the problem at hand. Part of the identiﬁed errors have been already
corrected, thus the performance of the workﬂow is expected to increase after a
few more tests.
Without the workﬂow developed (and considering only the unrealistic –nearly
useless– case of climate runs running for less than 1 simulated year) the success
rate would drop to a 28%.
GRID-CAM is able to run an ensemble of indeﬁnitely long climate simulations
split in jobs of any duration (as imposed by the local queues at each site) in
an unattended and user-transparent way. This application is general enough to
support a wide range of the experiments currently being run in the climate
science.
Acknowledgments. This work has been partial funded by the EELA project
under the 6th Framework Program of the European Commission (contract no.
026409). J. F. is supported by the Spanish Ministry of Education and Science
through the Juan de la Cierva program.

References
1. Collins, W.D., Rasch, P.J., Boville, B.A., Hack, J.J., McCaa, J.R., Williamson,
D.L., Kiehl, J.T., Briegleb, B., Bitz, C., Lin, S.J., Zhang, M., Dai, Y.: Description of the NCAR Comunity Atmospheric Model (CAM 3.0). Technical Report
NCAR/TN-464+STR, National Center for Atmospheric Research (2004), http://
www.ccsm.ucar.edu/models/atm-cam/docs/description/description.pdf
2. Foster, I., Kesselman, C.: The grid. Blueprint for a new computing infrastructure.
Morgan Kaufmann Publishers, San Francisco (1999)
3. Enabling Grids for E-sciencE (EGEE), http://www.eu-egee.org
4. E-infrastructure shared between Europe and Latin America (EELA), http://www.
eu-eela.org
5. Palmer, T.N.: The economic value of ensemble forecasts as a tool for risk assessment: From days to decades. Quart. J. Royal Meteor. Soc. 128, 747–774 (2002)
6. Hagedorn, R., Doblas–Reyes, F.J., Palmer, T.: The rationale behind the success
of multi–model ensembles in seasonal forecasting - I. Basic concept. Tellus 57A,
219–233 (2005)
7. Oﬃcial Documentation for LFC and DPM,
https://twiki.cern.ch/twiki/bin/view/LCG/DataManagementDocumentation
8. Allen, M.: Do it yourself climate prediction. Nature 401, 642 (1999)
9. Delaitre, T., Kiss, T., Goyeneche, A., Terstyanszky, G., Winter, S., Kacsuk, P.:
GEMLCA: Running legacy code applications as Grid services. Journal of Grid
Computing 3, 75–90 (2005)

480

V. Fern´
andez-Quiruelas et al.

10. Delgado, A., M´endez, P., Donno, F., Sciab´
a, A., Campana, S., Santinelli, R.: LCG-2
user guide (2004), http://edms.cern.ch
11. Delgado, A., M´endez, P., Donno, F., Sciab´
a, A., Burke, S., Campana, S., Santinelli,
R.: gLite 3 user guide (2007),
https://edms.cern.ch/file/722398/1.1/gLite-3-UserGuide.html
12. Koblitz, B., Santos, N., Pose, V.: The amga metadata service. Journal of Grid
Computing (2007) doi 10.1007/s10723-007-9084-6
13. GridWay 5.2 Documentation: User Guide:
http://www.gridway.org/documentation/stable/userguide
14. V´
azquez-Poletti, J.L., Huedo, E., Montero, R.S., Llorente, I.: A comparison between two grid scheduling philosophies: EGEE WMS and GridWay. Multiagent
and Grid Systems 3(4), 429–439 (2007)
15. Neocleous, K., Dikaiakos, M.D., Fragopoulou, V., Markatos, E.: Failure management in grids: The case of the EGEE infrastructure. Technical Report
TR-0055, CoreGRID (2006), http://www.coregrid.net/mambo/images/stories/
TechnicalReports/tr-0055.pdf

