A Jacobi–Davidson Method for Nonlinear
Eigenproblems
Heinrich Voss
Section of Mathematics, Hamburg University of Technology, D – 21071 Hamburg
voss@tu-harburg.de, http://www.tu-harburg.de/mat/hp/voss

Abstract. For the nonlinear eigenvalue problem T (λ)x = 0 we consider
a Jacobi–Davidson type iterative projection method. The resulting projected nonlinear eigenvalue problems are solved by inverse iteration. The
method is applied to a rational eigenvalue problem governing damped
vibrations of a structure.

1

Introduction

In this paper we consider the nonlinear eigenvalue problem
T (λ)x = 0

(1)

where T (λ) ∈ Cn×n is a family of large and sparse matrices depending on a
parameter λ ∈ D ⊂ C. Problems of this type arise in damped vibrations of
structures, vibrations of rotating structures, stability of linear systems with retarded argument, lateral buckling problems or vibrations of ﬂuid-solid structures,
to name just a few. As in the linear case T (λ) = λI − A a parameter λ is called
an eigenvalue of T (·) if problem (1) has a nontrivial solution x = 0 which is
called a corresponding eigenvector.
For linear sparse eigenproblems iterative projection methods such as the
Lanczos, Arnoldi or Jacobi–Davidson methods are very eﬃcient. In these approaches one determines approximations to the wanted eigenvalues and corresponding eigenvectors from projections of the large eigenproblem to lowdimensional subspaces which are generated in the course of the algorithm. The
small projected eigenproblems are solved by standard techniques.
Similar approaches for general nonlinear eigenproblems were studied in [2],
[4], [7], and for symmetric problems allowing maxmin characterizations of the
eigenvalues in [1] and [8].
Ruhe in [4] (with further modiﬁcations and improvements in [2]) linearized
the nonlinear problem (1) by regula falsi and applied an Arnoldi type method to
the varying sequence of linear problems thus constructing a sequence of search
spaces Vk and Hessenberg matrices Hk which approximate the projection of
T (σ)−1 T (λk ) to Vk . Here λk denotes an approximation to the wanted eigenvalue
and σ a shift close to that eigenvalue. Then a Ritz vector of Hk corresponding
to an eigenvalue of small modulus approximates an eigenvector of the nonlinear
M. Bubak et al. (Eds.): ICCS 2004, LNCS 3037, pp. 34–41, 2004.
c Springer-Verlag Berlin Heidelberg 2004

A Jacobi–Davidson Method for Nonlinear Eigenproblems

35

problem (1) from which a new approximation to the corresponding eigenvalue
is obtained. Hence, in this approach the two numerical subtasks reducing the
large dimension to a low one and solving the projected nonlinear eigenproblem
are attacked simultaneously.
In this paper we suggest an iterative projection method for the nonlinear
eigenproblem where the two subtasks mentioned in the last paragraph are handled separately. If Vk denotes a subspace of Cn of small dimension k constructed
in the course of the algorithm we solve the projected nonlinear eigenvalue problem VkH T (λ)Vk z = 0 by a dense solver to obtain an approximate eigenvalue λk
and eigenvector xk = Vk z. After that we expand the space Vk . Similarly as in the
Jacobi–Davidson method for linear eigenproblems the expansion direction vk+1
of Vk is chosen such that xk + αvk+1 for some α ∈ C has a high approximation
potential for the eigenvector we are just aiming at. The projection step and the
expansion step are repeated alternately until convergence.
Here we consider a method of this type where the search space Vk is expanded
by an approximate solution of a correction equation
I−

T (λk )xk xH
k
xH
k T (λk )xk

T (λk ) I −

xk xH
k
xH
k xk

v = −T (λk )xk

in a Jacobi–Davidson like manner. In [7] we proposed an expansion of the search
space by vk+1 = T (σ)−1 T (λk )xk generalizing the residual inverse iteration for
dense nonlinear eigenproblems.
The paper is organized as follows. Section 2. discusses the expansion of the
search space in a Jacobi–Davidson type way. In particular we discuss the approximate solution of the correction equation by a preconditioned Krylov subspace
method. Section 3. reviews solvers of dense nonlinear eigenproblems with special
emphasis on the fact that nonlinear problems are often small perturbations of
linear problems which can be exploited in the solution process. Section 4. contains the Jacobi–Davidson method for nonlinear eigenproblems and Section 5
demonstrates its numerical behavior for a ﬁnite element model of a structure.

2

Expanding the Search Space by Jacobi–Davidson

The Jacobi–Davidson method was introduced by Sleijpen and van der Vorst (cf.
[6]) for the linear eigenproblem Ax = λx, and generalized in a series of papers
with diﬀerent co-authors to general and to polynomial eigenvalue problems (cf.
[5]). Its idea is to construct a correction for a given eigenvector approximation
x in a subspace orthogonal to x. Namely, if V is the current search space and
(θ, u), u = 1, is a Ritz pair of Ax = λx corresponding to V then V is expanded
by a solution t of the so called correction equation
(I − uuH )(A − θI)(I − uuH )t = −(A − θI)u,

t ⊥ u.

If the correction equation is solved exactly then it is easily seen that the new
search space [V, t] contains the vector t˜ = (A − θI)−1 u obtained by one step

36

H. Voss

of shifted inverse iteration, and therefore one can expect quadratic (and in the
Hermitean case even cubic) convergence.
A natural generalization to the nonlinear eigenproblem (1) which was already
suggested in [5] for polynomial eigenvalue problems is the following one: Suppose
that the columns of V ⊂ Ck form an orthonormal basis of the current search
space, and let (u, θ) be a Ritz pair of (1) with respect to V , i.e. V H T (θ)V y = 0,
u = V y. Then we consider the correction equation
I−

puH
uH p

T (θ) I −

uuH
uH u

t = −r,

t⊥u

(2)

where p := T (θ)u and r := T (θ)u.
Equation (2) can be rewritten as T (θ)t − αp = −r where α has to be chosen
such that t ⊥ u. Solving for t we obtain
t = −u + αT (θ)−1 p = −u + αT (θ)−1 T (θ)u,
and u = V y yields t˜ := T (θ)−1 T (θ)u ∈ span[V, t].
Hence, as in the linear case the new search space span[V, t] contains the vector
obtained by one step of inverse iteration with shift θ and initial vector u, and
we may expect quadratic or even cubic convergence of the resulting iterative
projection method, if the correction equation (2) is solved exactly.
It has been observed by Sleijpen and van der Vorst for linear problems that
the correction equation does not have to be solved accurately but fast convergence of the projection method is maintained if the search space is expanded by
an approximate solution, and the same holds true for nonlinear problems. For
the linear problem they suggested to apply a few steps of a Krylov solver with
an appropriate preconditioner.
In the correction equation (2) the operator T (θ) is restricted to map the
subspace u⊥ to T (θ)u⊥ . Hence, if K ≈ T (θ) is a preconditioner of T (θ) then a
preconditioner for an iterative solver of (2) should be modiﬁed correspondingly
to
H
H
˜ := (I − pu )K(I − uu ).
K
H
H
u p
u u
With left-preconditioning equation (2) becomes
˜ −1 r,
˜ −1 T˜(θ)t = −K
K
where

t ⊥ u.

(3)

puH
uuH
T˜(θ) := (I − H )T (θ)(I − H ).
u p
u u

We apply a Krylov solver to equation (3) with initial guess t = 0. For the
linear case this was already discussed in [6], and the transfer to equation (3) is
straightforward.
˜ −1 T˜(θ) maps the space u⊥ into itself, and since the
Since the operator K
initial guess t = 0 is an element of u⊥ , all iterates are contained in this space,

A Jacobi–Davidson Method for Nonlinear Eigenproblems

37

and therefore in each step we have to perform one matrix-vector product y =
˜ −1 T˜(θ)v for some v ∈ u⊥ . To this end we ﬁrst multiply v by T˜(θ) which yields
K
y˜ = (I −

uH T (θ)v
puH
)T
(θ)v
=
T
(θ)v
−
p,
uH p
uH p

˜ = y˜, y ⊥ u.
and then we solve Ky
This equation can be rewritten as Ky − αp = y˜, where α is determined from
the condition y ⊥ u. Thus, we ﬁnally obtain
y = K −1 y˜ −

uH K −1 y˜ −1
K p
uH K −1 p

(4)

which demonstrates that taking into account the projectors in the precondi˜ instead of K, raises the cost of the preconditioned Krylov
tioner, i.e. using K
solver only slightly. To initialize one has to solve the linear system K p˜ = p
and to determine the scalar product α := uH p˜ = uH K −1 p. These computations
have to be executed just once. Afterwards in each iteration step one has to solve
only one linear system Kw = y˜ for w, one has to compute the scalar product
β := uH w = uH K −1 u
˜, and to perform one axpy y = w − (β/α)˜
y to expand the
˜ −1 T˜(θ).
Krylov space of K

3

Solving Projected Nonlinear Eigenproblems

Since the dimensions of the projected eigenproblems are usually small they can
be solved by any method for dense nonlinear eigenproblems like inverse iteration
or residual inverse iteration.
If T (λ) is symmetric or Hermitean such that the eigenvalues are real and can
be characterized as minmax values of a Rayleigh functional then the projected
problem inherits this property, and the eigenvalues can be determined one after
the other by safeguarded iteration. This approach which was discussed for the
Jacobi–Davidson method in [1] and for the Arnoldi method in [8] has the advantage that it is most unlikely that the method converges to an eigenvalue that
has already been found previously.
In the general case the following strategy is similar to safeguarded iteration.
Assume that we want to determine all eigenvalues of problem (1) in the vicinity
of a given parameter σ0 ∈ D, and that already m − 1 eigenvalues closest to
˜ is an approximation to the eigenvalue
σ0 have been determined. Assume that µ
wanted next.
A ﬁrst order approximation of problem (1) is
µ))x = 0, θ = µ
˜ − λ.
T (λ)x ≈ (T (˜
µ) − θT (˜

(5)

This suggests the method of successive linear problems in Algorithm 1 which
was introduced by Ruhe [3], and which converges quadratically.
Of course this method is not appropriate for a sparse problem (1), but in an
iterative projection method the dimension of the projected problem which has

38

H. Voss

Algorithm 1 Method of successive linear problems
1: Start with an approximation µ1 to the m-th eigenvalue of (1)
2: for = 1, 2, . . . until convergence do
3:
solve the linear eigenproblem T (µ )u = θT (µ )u
4:
choose the eigenvalue θ such |σ0 − (µ − θ)| is the m–smallest among the eigenvalues
5:
µ +1 = µ − θ
6: end for

to be solved in step 3. usually is quite small, and every standard solver for dense
eigenproblems applies.
Quite often the nonlinear eigenvalue problem under consideration is a (small)
perturbation of a linear eigenvalue problem. As a numerical example we will
consider a ﬁnite element model of a vibrating structure with nonproportional
damping. Using a viscoelastic constitutive relation to describe the behavior of a
material in the equations of motions yields a rational eigenvalue problem for the
case of free vibrations. A ﬁnite element model obtains the form


J
1
(6)
T (ω) := ω 2 M + K −
∆Kj  x = 0.
1
+
bj ω
j=1
If the damping is not too large the eigenmodes of the damped and the undamped problem do not diﬀer very much although the eigenvalues do. Therefore,
in step 3. of Algorithm 2 it is reasonable to determine an eigenvector y of the
undamped and projected problem (ω 2 V H M V + V H KV )y = 0 corresponding
2
to the m-smallest eigenvalue ωm
, determine an approximate eigenvalue ω
˜ of the
nonlinear projected problem from the complex equation y H V H T (ω)V y = 0 or
eH V H T (σ)−1 T (ω)V y = 0, and correct it by (residual) inverse iteration.

4

Jacobi–Davidson Method for Nonlinear Eigenproblems

A template for the Jacobi–Davidson method for the nonlinear eigenvalue problem
(1) is given in Algorithm 2.
Remarks on some of its steps are inorder:
1. In V preinformation about the wanted eigenvectors (which may be gained
from previous solutions of similar problems) can be introduced into the
method.
If we are interested in eigenvalues close to a given parameter σ and no information on eigenvectors is at hand we can start the Jacobi–Davidson method
with an orthogonal basis V of an invariant subspace of the linear eigenproblem T (σ)u = θu (or T (σ)u = θT (σ)u) corresponding to eigenvalues θ which
are small in modulus.
8. As the subspaces expand in the course of the algorithm the increasing storage and the computational cost for solving the projected eigenproblems may

A Jacobi–Davidson Method for Nonlinear Eigenproblems

39

Algorithm 2 Nonlinear Jacobi–Davidson method
1: Start with an initial basis V , V H V = I; m = 1
2: determine a preconditioner K ≈ T (σ)−1 , σ close to the ﬁrst wanted eigenvalue
3: while m ≤ number of the wanted eigenvalues do
4:
compute an approximation to the m-th wanted eigenvalue λm and corresponding
eigenvector xm of the projected problem V H T (λ)V x = 0
5:
determine the Ritz vector u = V xm and the residual r = T (λm )u
6:
if r / u < then
7:
accept approximate eigenpair (λm , u); increase m = m + 1;
8:
reduce the search space V if necessary
9:
choose an approximation (λm , u) to the next eigenpair
10:
compute the residual r = T (λm )u;
11:
end if
12:
Find an approximate solution of the correction equation
(I −

T (λm )uuH
uuH
)T
(σ)(I
−
)t = −r
uH u
uH T (λm )u

(7)

13:
orthogonalize t = t − V V H t, v = t/ t , and expand the subspace V = [V, v]
14:
determine a new preconditioner K ≈ T (λm ) if necessary
15:
update the projected problem
16: end while

make it necessary to restart the algorithm and to purge some of the basis
vectors. Since a restart destroys information on the eigenvectors and particularly the one the method is just aiming at we restart only if an eigenvector has
just converged. A reasonable search space after restart is the space spanned
by the already converged eigenvectors (or a space slightly larger).
12. The correction equation can be solved by a preconditioned Krylov solver,
e.g.
13. The ﬁrst two statements represent the classical Gram–Schmidt process. It
is advisable to repeat this orthogonalization step once if the norm of t is
reduced by more than a modest factor, say t / vold < 0.25, e.g.
14. We solved the correction equation (7) by a few steps of preconditioned GMRES where we kept the preconditioner for a couple of eigenvalues. We terminated the solver of (7) in the k-th outer iteration for the m-th eigenvalue
if the residual was reduced by at least τk = 2−k , and we allowed at most 10
steps of the solver. If the required accuracy τk was not met after at most 5
iteration steps we updated the preconditioner. However, we allowed at most
one update for every eigenvalue λm .

5

Numerical Experiments

To test the Jacobi–Davidson method we consider the rational eigenvalue problem
(6) governing damped vibrations of a column
{(x, y, z) : 0.8 <

x2 + y 2 < 1, 0 < z < 5}

40

H. Voss

5

residual norm

10

0

10

50

100

150
iteration

200

250

300

Fig. 1. Convergence history without restarts
restart

10

10

8

10

6

residual norm

10

4

10

2

10

0

10

−2

10

−4

10

50

100

150

200

250

iteration

Fig. 2. Convergence history with restarts (ﬁrst 250 iterations)

which is clamped at its bottom z = 0. The instantaneous Young’s modulus is
set to E = 2.06 × 1011 , the instantaneous Poisson’s rate is ν = 0.3, and the
density is set to ρ = 7800. For the nonproportional damping we use in addition
the following parameters, ∆ν = 0.27, and ∆E = 6 × 1010 for 0 < x < 2.5, and
∆E = 5 × 1010 for 2.5 < x < 5. The relaxation constant is set to b = 10−4 .
Discretizing this problem by linear Lagrangian elements we obtained the
rational eigenproblem (6) of dimension 11892, and the bandwidth of the stiﬀness
matrix K was after reducing it by reverse Cuthill–McKee algorithm still 665. For
symmetry reasons we determined only eigenvalues with negative imaginary part,
and we computed 50 of them one after another with decreasing imaginary part.

A Jacobi–Davidson Method for Nonlinear Eigenproblems

41

The nonlinear projected eigenproblems were solved by inverse iteration with an
initial guess obtained from the corresponding undamped projected problem as
explained at the end of Section 3.
The experiments were run under MATLAB 6.5 on a Pentium 4 processor with
2 GHz and 1 GB RAM. We preconditioned by the LU factorization of T (σ), and
terminated the iteration if the norm of the residual was less than 10−2 .
Starting with an eigenvector of the linear eigenproblem Kx = λM x corresponding to the smallest eigenvalue the algorithm without restarts needed 320
iteration steps, i.e. an average of 6.4 iterations per eigenvalue, to approximate
all 50 eigenvalues (including double eigenvalues) with maximal negative imaginary part. To solve the correction equations a total of 651 GMRES steps were
needed, and 6 updates of the preconditioner were necessary. Fig. 1. contains the
convergence history.
Restarting the Jacobi–Davidson process if the dimension of the research space
exceeded 80 the method needed 7 restarts. Again all 50 eigenvalues were found
by the method requiring 422 iterations, 840 GMRES steps, and 16 updates of the
preconditioner. The convergence history in Fig. 2. looks very similar to the one
without restarts, however, after a restart the speed of convergence was reduced
considerably. After a restart an average of 17.1 iterations was necessary to gather
enough information about the search space and to make the method converge,
whereas for the other iteration steps the average number of steps for convergence
was 7.0.

References
1. T. Betcke and H. Voss. A Jacobi–Davidson–type projection method for nonlinear
eigenvalue problems. Future Generation Computer Systems, 20(3):363 – 372, 2004.
2. P. Hager. Eigenfrequency Analysis. FE-Adaptivity and a Nonlinear Eigenvalue Problem. PhD thesis, Chalmers University of Technology, G¨
oteborg, 2001.
3. A. Ruhe. Algorithms for the nonlinear eigenvalue problem. SIAM J. Numer. Anal.,
10:674 – 689, 1973.
4. A. Ruhe. A rational Krylov algorithm for nonlinear matrix eigenvalue problems.
Zapiski Nauchnyh Seminarov POMI, 268:176 – 180, 2000.
5. G.L. Sleijpen, G.L. Booten, D.R. Fokkema, and H.A. van der Vorst. Jacobi-Davidson
type methods for generalized eigenproblems and polynomial eigenproblems. BIT,
36:595 – 633, 1996.
6. G.L. Sleijpen and H.A. van der Vorst. A Jacobi-Davidson iteration method for
linear eigenvalue problems. SIAM J.Matr.Anal.Appl., 17:401 – 425, 1996.
7. H. Voss. An Arnoldi method for nonlinear eigenvalue problems. Technical Report 56,
Section of Mathematics, Hamburg University of Technology, 2002. To appear in BIT
Numerical Mathematics.
8. H. Voss. An Arnoldi method for nonlinear symmetric eigenvalue problems. In Online Proceedings of the SIAM Conference on Applied Linear Algebra, Williamsburg,
http://www.siam.org/meetings/laa03/, 2003.

