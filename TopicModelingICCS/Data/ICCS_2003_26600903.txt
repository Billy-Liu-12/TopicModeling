A Novel Evolutionary Approach to Linear Time-Series
Forecasting Model
1

2

Prakash Vijayan and S. Suresh
1

Department of Mechanical Engineering, Indian Institute Of Technology, Madras
Chennai-600036, India, v_prakash81@yahoo.com
2
Department of Computer Science and Engineering, Indian Institute Of Technology, Madras
Chennai-600036, India, suresh_12may@yahoo.com

Abstract. This paper presents a handshake between the concepts of genetic
algorithms and the forecasting problem to present a novel search based multiphase genetic algorithm to the forecasting problem based on the time series
model. The backbone concept of the paper lies in utilizing the genetic approach
effectively for implementing the Autoregressive process, a linear stochastic model
where a time series is supposed to be a linear aggregation of random shocks. We
propose to utilize the concept of genetic algorithms to transform an initial
population of random suggested solutions to a population that contains solutions
approximating the optimal one. A carefully chosen fitness function acts in the
capacity of a yardstick to appraise the quality of each “chromosome” to aid the
selection phase. We simulated the presented approach on a Pentium IV processor
and obtained results that were very encouraging.

1 Introduction and Previous Work
Quantitative methods for generating forecasts of future outcomes using statistical
procedures involve the examination of current and historically seasonally unadjusted
data. This knowledge can be used to extrapolate the variable of interest. It is assumed
that the process is stable over the forecast time horizon, but valid only for short time
forecasts. Two types of quantitative forecasting models used are time series models
and causal econometric models. Time series models involve a statistical analysis,
which uses only historical data of the variable. Causal models are based on statistical
analysis of data for other related variables and the use of these variables to forecast
the variable of interest. There are several approaches for time series modeling. Some
of them include Moving average, exponential smoothing and ARIMA, which are
linear in nature. These numerous forecasting methods and the empirical findings,
which are often in conflict, have given no clear guidelines as to the most appropriate
methods for forecasting.
Box-Jenkins method [1] is popular because of its generality as it can handle any
stationary or non-stationary time series, with or without seasonal elements. The BoxJenkins method for selecting an appropriate ARIMA model for estimating and
forecasting a univariate time series consist of identification and testing and
application. Although performance is primary concern, the choice of the Box-Jenkins
P.M.A. Sloot et al. (Eds.): ICCS 2003, LNCS 2660, pp. 903–910, 2003.
© Springer-Verlag Berlin Heidelberg 2003

904

P. Vijayan and S. Suresh

model is due to its sound theoretical basis and the numerous research publications
available. Box-Jenkins’ ARIMA models are a class of linear models that were
designed for linear time series. The model has been successfully applied in not only
economic time series forecasting, but also for modeling the empirical dependencies
between successive times between failures as discussed by Walls and Bendell [2]. The
model gives satisfactory predictive performances as shown by Ho and Xie [3]. The
model has been successfully used and tested for International tourist demand
forecasting. This has been discussed in large and compared with different methods by
Guerts and Ibrahim [4], Martin and Witt [5]. A typical problem in the ARIMA model
for a time series is to estimate the coefficients of the Auto Regressive (AR) and the
Moving Averages (MA) processes. Though, it is possible to fit a variety of linear
models for a time series, parsimony is one important factor, which need to be
considered. The number of terms used in the linear model should be a minimum. In
the existing method, as demonstrated by Box Jenkins [1] in the identification stage,
sample correlograms, the autocorrelation and partial autocorrelation coefficients are
plotted to identify the order of the AR model. However, for higher order models,
identification becomes tedious and difficult. An empirical investigation on the
accuracies of the different forecasting methods has been discussed by Makridakis, S
and Hibon, M[6]. The accuracy of a forecasting method is determined by analyzing
the forecast error, which is defined as the actual value minus the forecast (or fitted)
value of the variable for time period t:
et = At - Ft
where, At the actual value at time t; Ft the forecast value at time t + 1.

To the best of our knowledge, this is the first reported genetic approach to solve this
problem. The field of genetic algorithms has a large amount of prior literature. Basics
of genetic algorithms are presented in [9].

2 Auto Regressive Process
In the Autoregressive process, a linear stochastic model, a time series is supposed to
be a linear aggregation of random shocks. However, parameter parsimony is an
important criterion in choosing the right model. The stochastic process, as a linear
model can be represented as

Z t = et + φ1 at −1 + φ 2 at − 2 + φ 3 at −3 + φ 4 at −4 + ........

Z t as a weighted sum of present and past values of the
white noise process at. The value Z t is the deviation from the mean as the process is
stationary and is represented by Z t = Zt ± 
The linear process represents

In the case, where the first ‘k’ weights are non zero, the process becomes an
Autoregressive process of order k, represented by AR(k) or ARIMA (k,0,0). The
process can be written as,

a t = et + φ1 at −1 + φ 2 at −2 + φ 3 at −3 + .........φ k at −k

A Novel Evolutionary Approach to Linear Time-Series Forecasting Model

905

In the above process, the finite sets of weights φ1 , φ 2 , φ 3 .....φ k are non-zero.
The first order Auto Regressive process or the Markov process can be represented as

at = µ + et + φ1 at −1

For stationarity of the process,
-1 < φ1 < 1
Similarly, for a second order Auto regressive process, represented as

a t = µ + et + φ1 at −1 + φ 2 at −2

For stationarity of the process,
-2 < φ1 < 2 and -1 < φ 2 < 1

The permissible values of φ1 and φ 2 are for a stationary Autoregressive process as
shown by Makridakis, Wheelwright and McGee[7], must fall in the triangle as shown
in the figure 1.

3 The Genetic Approach
The algorithm presented in this paper apart from using only the genetic algorithm uses
a search based multi-phase genetic algorithm that proceeds towards the optimal
solution in an excellent manner. We start with a genetic algorithm by setting the
ranges of the coefficients and operate in that range.

1

0

φ2

-1
-2

-1

0

1

φ1
Fig. 1. The triangle shows the range of values that
process.

2

φ1

and

φ2

for a stationary Autoregressive

906

P. Vijayan and S. Suresh

After simulating this genetic algorithm for a sufficiently large number of iterations we
change the range of values that can be taken by the coefficients in the following
manner:

•
•

Observe the best ten solutions based on their fitness values.
Obtain the maximum and minimum values taken by each of the coefficients
and set them as the maximum and minimum values that can be taken by the
respective coefficients during the next execution of the genetic algorithm.

This approach utilizes an intelligent and random methodology and therefore proceeds
towards the optimal solution in an intelligent manner.
It is however essential to note that it is very important that we execute the first few
genetic algorithms for sufficiently large number of iterations. The reason for such a
claim is attributed to the fact that an improper prediction of range at the end of the
first few executions may result in vast deviations of the solution from the optimal
solution and therefore affecting the performance badly.
3.1 Encoding
The encoding phase essentially establishes a mapping over of the solution into a form
that allows the genetic algorithm to analyze it. The encoding provided is an array
named Coeff[N], where N represents the number of past values used for forecasting.
Each element of the array consists of the coefficient taken by each of the past values
in the forecasting equation. For example:
An array Coeff[3] having the values {1.25, 0.35, 0.11) represents the equation of the
form:

a t = 1.25a t −1 + 0.35a t − 2 + 0.11at −3
3.2 Formation of the Initial Population

The formation of the initial population determines the portion of the solution space
explored by the genetic algorithm. Though the size of the initial population is up to
the user, it should not be too small as it would greatly hinder the performance of the
algorithm, while at the same time, a population too large in size would essentially
increment the runtime as well since we would have to take care of quite a lot of
chromosomes. Through extreme experimentation we found that an initial population
of size equal to 10N, where N depicts the number of previous values used for
forecasting gave us the best solution. We generate the initial population in a random
manner but ensuring that the coefficients lie in the appropriate range. We generate
10*N-3 chromosomes using the random approach. The other three chromosomes are
generated by setting all the coefficients in the array Coeff[N] to the maximum,
minimum and the mid value of the ranges of the respective coefficients.
3.3 Genetic Operators
The genetic operators are applied to the population at hand to produce offspring that
are, with a high probability, better than their parents. These operators ensure proper
evolution of the current population towards the desired solution.

A Novel Evolutionary Approach to Linear Time-Series Forecasting Model

907

3.3.1 The Mating Operator
For the mating process to take place, we would require the two parents to interact.
The parents selected for mating are randomly chosen from the set of given population
at hand. However the probability of selection of any chromosome is directly
proportional to its fitness value. We adopt a one-point crossover strategy and the
crossover point is selected randomly. The following example shows how mating is
performed on two chromosomes:
Parent1: {1.5 | 0.5,0.12}
Parent2: {2.3 | 0.88,0.01}
When the above two parents are crossed over at the point shown by the ‘|’ they
produce two offspring:
Child1: {1.5, 0.88, 0.01}
Child2: {2.3, 0.5, 0.12}
3.3.2 The Mutation Operator
The second genetic operator, the mutation operator, gives the genetic algorithm a
greater touch of randomness. The randomness makes sure that no specific kind of
pattern is lost at any point of time in the population. As a result of the mutation
operator, we have the power to redirect an evolution that is converging to a local
maximum, and place it on a path to a better solution. We assign a probability of
mutation to each candidate of the population in a manner that ensures that a higher
probability of mutation would be associated with those chromosomes that are of a low
quality as far as the fitness value is concerned.
3.3.3 Quality Enhancement Operator (QEO)
This is the last genetic operator that we employed in an unconventional manner. This
operator is primarily concerned with improving upon the quality of the chromosome
on which it is operated. The QEO is employed on ¼*(Population size) number of
randomly chosen chromosomes, the probability being same as the mutation
probability. The operator functions in the following manner:
Calculate the value of the function:

a t = Coeff [0]a t −1 + Coeff [1]a t −2 .... + Coeff [ N ]at − N −1
If the value is more than the expected value for most of the given data, then randomly
select any positive coefficient and decrement its value by 25% of its original value. It
is important to note that by doing this, we ensure with a higher probability that the
offspring produced is of a superior quality than its parent. This operator greatly
enhances the efficiency of our algorithm.
3.4 Fitness Value
The sole purpose of the fitness function is to implement the “survival of the fittest”
strategy. The fitness value in our case is solely concentrated on the minimization of
deviation of the solution at hand from the actual solution. This optimization criterion
is framed aptly in the form of a function that evaluates the chromosomes. The

908

P. Vijayan and S. Suresh

function Calculate_fitness(Chromosome C) calculates the fitness value of the
chromosome C in the following manner:

Calculate_fitness(Chromosome C)
Float MaxDev, CurrDev, Fitval
MaxDev = Maxdeviation()
For each series of data available
begin
Calculate a t = Coeff [0]a t −1 + Coeff [1]a t − 2 .... + Coeff [ N ]a t − N −1
CurrDev = CurrDev +
desired value of
End for

(a t − at* ) 2 , where at* depicts the

at .

Fitval = 1 – CurrDev / (Maxdev* Number of series of data)
Return FitVal
End Function
The function Maxdeviation() calculates the square of maximum possible deviation of
any chromosome which is calculated by substituting the upper bound values of each
of the coefficients. Note that the series of data given can be formed by splitting the
given series into smaller data series of requires length. For example:
In case of N=3, a data series 1,2,3,4,5,6,7,8,9,10 can be split into the following data
series: {1,2,3,4}, {2,3,4,5}, {3,4,5,6}, {4,5,6,7}, {5,6,7,8}, {6,7,8,9} and {7,8,9,10}.
3.5 The Main Algorithm
The main algorithm of any phase of the genetic algorithm is as follows:
GA(Ranges of Coefficients)
Create initial population
while solution with the required fitness value has not been found do
Choose Parents for mating process
Pair up Parents Randomly for the mating process
Carry out Mating
Add Offspring to population
Carry out Mutation
Apply the QEO
End While
End Function
Note that, if at any point of time the population size exceeded the limit of 2000
chromosomes we filtered the population in the following manner:

A Novel Evolutionary Approach to Linear Time-Series Forecasting Model

909

•
•

Retain the top 50% of the chromosomes.
From the bottom 50% of the chromosomes select 50% of them in random
manner.
It was observed that such a policy adopted for controlling the population was highly
lucrative.

4 Experimental Section
We implemented our algorithm in C language on a Pentium IV processor. We tested
the working of our algorithm on the IBM common stock closing prices presented in
[11]. We allowed our algorithm to proceed till the best solution presented by our
algorithm remained stable for a significantly long time.
Before executing our algorithm we had to decide upon the various parameters of our
genetic algorithm like the rate of mating, rate of mutation, size of initial population
etc. By varying these parameters we were able to decide upon the best values for these
parameters, which struck an optimal balance between the run-times, and the fitness
values of the best solutions obtained.
The forecast equation obtained by our algorithm is:

∇z t = a t + 0.125(±0.035)at −1 − 0.003(±0.001)at −2
The equation as predicted by the methodology proposed in [1] was:

∇z t = a t + 0.09(±0.05)at −1
It is very clear that the prediction of any value is mainly based on the previous two
values as the coefficients associated with the remaining time lags are very close to
zero. The residual variance of our algorithm is 43.45, which is a significant
improvement over the variance of 52.2 presented in [1]. Figure 2 shows the first
differences obtained by our algorithm and the actual first differences. It is significant
to note that our forecasts were very close to the desired values. The complexity of
executing the algorithm was very less and the run-times were also impressive.
Thereby our algorithm stood well on both the criteria as mentioned in section 1, as far
as the forecasting accuracy and the ease of interpreting results is concerned.
IBM daily stock closing price (Graph 4.3)
30
20
10

Actual

0
1

8

15

22

29

36

43

50

57

64

71

78

85

92

99

106

11 3

1 20 1 27

13 4

- 10

14 1

1 48

15 5

16 2

1 69

1 76

18 3

1 90

1 97

204

21 1

2 18

2 25

23 2

23 9

2 46

25 3

For ecast

-20
-30
-40

T ime

Fig. 2. The first differences obtained by our algorithm and the actual first differences

910

P. Vijayan and S. Suresh

5 Conclusion
The search based multi-phase genetic algorithm proposed in this paper has proved to
be efficient in estimating the coefficients to very high accuracy. The results obtained
were highly encouraging and the complexity associated with the execution was also
very low. This paper opens up a whole new methodology for fixing the coefficients in
the ARIMA model. Further research in this field may be oriented towards developing
evolutionary approaches for identification and fixing the coefficients for the ARIMA
model. There is also a tremendous scope of improvement in developing new
crossover, mating and the fitness operators.

References
[1]

Box GEP, & Jenkins GM. Time series analysis: Forecasting and control, Revised ed.
Holden-Day, San Francisco,1976.
[2] Walls, L. A., & Bendall, A. (1987). Time series methods in reliability. Reliability
Engineering, 18, 239–265.
[3] Ho, S. L., & Xie, M. (1998). The use of ARIMA models for reliability forecasting and
analysis. Computers and Industrial Engineering, 35
[4] Geurts, M.,& Ibrahim, I. (1975). Comparing the Box–Jenkins approach with the
exponentially smoothed forecasting model application to Hawaii tourists. Journal of
Marketing Research,12, 182–188 .
[5] Martin, C. A., & Witt, S. F. (1989). Forecasting tourism demand: A comparison of the
accuracy of several quantitative methods. International Journal of Forecasting, 5,7–19.
[6] Makridakis, S., & Hibon, M. (1979). Accuracy of forecasting: An empirical
investigation. Journal of the Royal Statistical Society A, 142,97–145.
[7] Makridakis. S, Wheelwright SC, McGee VE, Forecasting: Methods and applications,
John Wiley & sons, 1983
[8] Kenneth A. De Jong and William M. Spears , “An Analysis of the Interacting Roles of
Population Size and Crossover in Genetic Algorithms,” In International Workshop
Parallel Problem Solving from Nature, University of Dortmund, pages 38–47(1990).
[9] Kenneth A. De Jong and William M. Spears (1992), “A Formal Analysis of the Role of
Multi-Point Crossover in Genetic Algorithms,” In Annals of Mathematics and Artificial
Intelligence Journal, v5, n1, pages 1–26.
[10] Pinaki Mazumder, Elizabeth Rudnick ,”Genetic Algorithms for VLSI Design, Layout and
Test Automation ,” Prentice Hall PTR, (2002).
rd
[11] Box GEP, & Jenkins GM. Time series analysis: Forecasting and control, 3 ed., Prentice
Hall, pp. 542–543, IBM Common stock closing prices.

