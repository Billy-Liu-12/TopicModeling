Finding Synchronization-Free Parallelism for
Non-uniform Loops

Volodymyr Beletskyy
Faculty of Computer Science, Technical University of Szczecin, Zolnierska 49 st.,
71-210 Szczecin, Poland,
vbeletskyy@wi.ps.pl

Abstract. A technique, permitting us to find synchronization-free parallelism in
non-uniform loops, is presented. It is based on finding affine space partition
mappings. The main advantage of this technique is that it allows us to form
constraints for finding mappings directly in a linear form while known
techniques result in building non-linear constraints which should next be
linearized. After finding affine space partition mappings, well-known code
generation approaches can be applied to expose loop parallelism. The technique
is illustrated with two examples.

1

Introduction

A lot of transformations have been developed to expose parallelism in loops,
minimize synchronization, and improve memory locality in the past
[1],[3],[4],[6],[7],[8],[9],[10],[11],[12],[14],[15],[19]. However, there are
the
following questions. Which of these methods permit us to find synchronization-free
parallelism and what is their complexity for non-uniform loops.
According to a study by Sass and Mutka[18], a majority of the loops in scientific
code are imperfectly nested, and a majority of the performance-increasing techniques
developed in the past assume that loops are perfectly nested, that is, imperfectly
nested loops deserve more attention from the research community.
This paper presents a technique permitting us to find synchronization-free
parallelism in non-uniform loops. We refer to a particular execution of a statement for
a certain iteration of the loops, which surround this statement, as an operation. The
operations of a loop are divided into partitions, such that dependent operations are
placed in the same partition. A partitioning is described by an affine mapping for
each loop statement.
An m-dimensional affine partition mapping for statement s in a loop is an mdimensional affine expression φ s = C s i + c s , which maps an instance of statement s,
indexed by its iteration vector i , to an m-dimensional vector. Given affine mappings,
well-known techniques for generating parallel code can be applied, for example,
[2],[4],[5],[17].

P.M.A. Sloot et al. (Eds.): ICCS 2003, LNCS 2658, pp. 925–934, 2003.
© Springer-Verlag Berlin Heidelberg 2003

926

2

V. Beletskyy

Dependence Analysis

Our algorithm is based on the dependence analysis proposed by Pugh and Wonnacott
[16]. That analysis permits us to extract exact dependence information for any single
structured procedure in which the expressions in the subscripts, loop bounds, and
conditionals are affine functions of the loop indices and loop-independent variables,
and the loop steps are known constants. Dependences are presented with dependence
relations. A dependence relation is a mapping from one iteration space to another, and
is represented by a set of linear constraints on variables that stand for the values of the
loop indices at the source and destination of the dependence and the values of the
symbolic constants. A dependence relation is a tuple relation. An integer k-tuple is a
point in Z k . A tuple relation is a mapping from tuples to tuples.
The basic merits of the dependence analysis proposed by Pugh and Wonnacott are
as follows: i) it is exact; ii) it is valid for both perfectly and imperfectly nested loops;
iii) it permits value-based dependences to be calculated.
A dependence between operations I and J , which are the source and destination
of the dependence, respectively, is value-based if: I is executed before J ; I and

J refer to a memory location M, and at least one of these references is a write; the
memory location M is not written between operation I and operation J .
The dependence analysis by Pugh and Wonnacott is implemented in Petit, a
research tool for doing dependence analysis and program transformations. To carry
out dependence analysis manually, the Omega calculator can be applied [13].
An affine loop nest is non-uniform if it originates non-uniform dependence
relations represented by an affine function f that expresses the dependence sources I
in terms of the dependence destinations J ( I =f( J )) or vice versa.
An algorithm proposed in this paper is applicable for those loops that meet the
restrictions of the dependence analysis proposed by Pugh and Wonnacott [16].

3

Space-Partition Constraints

Our approach is applicable to the following imperfectly nested loop considered in
[19]
do x1 = L1 ,U 1

S 1a :

H 1a ( x 1 )
do x 2 = L2 ,U 2

S 2a :

Sn :

H 2 a ( x1 , x 2 )
…
do x n = Ln ,U n
H n ( x1 ,..., x n )
…

(1)

Finding Synchronization-Free Parallelism for Non-uniform Loops

S 2b :

927

H 2 b ( x1 , x 2 )

S 1b : H 1b ( x1 ) ,
where the loop bounds of x k are affine constraints over surrounding loop variables
x1 ,…, x k −1 and some symbolic integer constants, or formally:

L k = max(l k ,1 , l k , 2 ,...)
U k = max(u k ,1 , u k , 2 ,...),
where

l k , p = Î ( l 0k , p + l 1k , p x1 + ... + l kk −, p1 x k −1 ) / l kk , p Þ
u k , p = Ð ( u 0k , p + u1k , p x1 + ... + u kk −, p1 x k −1 ) / u kk , p à
and all l k , p and u k , p are integer constants, except possibly for l 0k , p and u0k , p , which
may be symbolic constraints but must still be loop invariants in the loop nest. The
ceiling and floor functions are introduced to convert rationals to integers. In general, a
lower(upper) loop bound is a maximum(minimum) of affine constraints with rational
coefficients. This ensures that the space defined by any set of loops in the loop nest is
a convex polyhedron.
In this section, we consider the following task. Given a set of dependences
originated by a loop and presented with dependence relations

D = {I j − > J j , j = 1,2,..., q} ,
find m s -dimensional affine space partition mappings φ s = C s i + c s for each s=1a,
2a, ..., n,...,2b, 1b, such that φ si ( I j ) = φ sk ( J j ) = P m , where si, sk are the statements
which instances originate the source and destination of the dependence I j − > J j , C s
is a matrix of dimensions m s x n, c s is an m s -dimensional vector representing a
constant term, P m is a vector representing the identifier of a processor to execute
the source and destination of the dependence I j − > J j .
Let I j , J

be represented in the following form

j

I
where I j , J

j

= A1 j * i1 j + B1 j , J

j

= A2 j * i 2 j + B 2 j ,

are m1 j and m 2 j -dimensional vectors, respectively, m1 j <=n,
m 2 j <=n, n is the number of the loop nests, B1 j , B 2 j are m1 j and m 2 j dimensional vectors, respectively, i1 j and i 2 j are n-dimensional vectors, A1 j and
j

A2 j are matrices of dimensions m1 j x n and m 2 j x n, respectively.
Let us write matrices A1 j and A2 j in the following form

928

V. Beletskyy
1
2
n
1
2
n
A1 j = [ A1 j A1 j ... A1 j ] , A2 j = [ A2 j A2 j ... A2 j ] ,

where A1i j and Ai2 j , i=1,2,…,n represent the columns of A1 j and A2 j ,
respectively.
If a dependence I j − > J j , j ∈ [1, q] is the self dependence, that is, it is
originated with the same statement s, we seek an affine space partition mapping
φ s = C s i + c s such that the following condition is satisfied

C s I j + cs = C s J j + cs
or

Cs I j − Cs J j = 0 ,
which means that the same processor executes operations I j and J j .
Let us rewrite the equation above as follows
n
2
1
C s ( A1 j i11j + A1 j i12j + ... + A1 j i1nj + B1 j ) −

− C s ( A12 j i 21j + A22 j i 22j + ... + A2n j i 2nj + B 2 j ) = 0 ,
where i11j , i12j ,..., i1nj and i 21j , i 2 2j ,..., i 2nj are the coordinates of i1 j and i 2 j ,
respectively, and transform it to the form
2
n
〈C s , A11 j 〉 i11j + 〈C s , A1 j 〉 i12j + ... + 〈C s , A1 j 〉 i1nj + 〈C s , B1 j 〉 −

(2)

2
n
− 〈C s , A12 j 〉 i 21j − 〈C s , A2 j 〉 i 2 2j − ... − 〈C s , A2 j 〉 i 2nj − 〈C s , B 2 j 〉 = 0 ,

where 〈 x, y 〉 denotes the inner product of vectors
arbitrary row of C s .

x and y , C s represents an

If a dependence I j − > J j , j ∈ [1, q] is originated with two different statements
s1 and s2,

we seek two affine

space

partition mappings φ s1 = C s1 i + c s1 and

φ s 2 = C s 2 i + c s 2 such that the following condition is satisfied
C s1 I j + c s1 = C s 2 J j + c s 2 .
Let us rewrite the above condition as follows
n
2
1
C s1 ( A1 j i11j + A1 j i12j + ... + A1 j i1nj + B1 j ) + c s1 =
n
2
1
C s 2 ( A2 j i 21j + A2 j i 2 2j + ... + A2 j i 2 nj + B 2 j ) + c s 2 ,

and transform it to the form

Finding Synchronization-Free Parallelism for Non-uniform Loops

2
n
〈C s1 , A11 j 〉 i11j + 〈C s1 , A1 j 〉 i12j + ... + 〈C s1 , A1 j 〉 i1nj + 〈C s1 , B1 j 〉 + c s1 −

929

(3)

2
n
− 〈C s 2 , A12 j 〉 i 21j − 〈C s 2 , A2 j 〉 i 2 2j − ... − 〈C s 2 , A2 j 〉 i 2nj − 〈C s 2 , B 2 j 〉 − c s 2 = 0 ,

where C s1 , C s 2 represent an arbitrary row of C s1 , C s 2 , respectively, c s1 , c s 2 are
unknown constant terms which are dependent on C s1 , C s 2 .
Let us introduce an r j -dimensional vector i j which consists of all uncommon
coordinates(having different names) of I j , J j and the coordinates of this vector be

i1j , i 2j ,..., i rj j , r j <= m1 j + m 2 j . Rewrite equations (2) and (3) in the following form
rj

∑ D kj i kj + d j = 0,

(4)

k =1

where

D kj and d j are formed as follows:

i) for self dependences, it is the sum of all those 〈C s1 , A1mj 〉 and − 〈C s1 , A2p j 〉 for
which the following condition holds i1mj = i 2 pj = i kj ; d j = 〈C s1 , B1 j 〉 − 〈C s1 , B 2 j 〉 ;
ii) for dependences originated with two different statements, it is the sum of all
− 〈C s 2 , A2p j 〉 for which the following condition holds
those 〈C s1 , A1mj 〉 and

i1mj = i 2 pj = i kj ; d j = 〈C s1 , B1 j 〉 − 〈C s 2 , B 2 j 〉 + c s1 − c s 2 .
Algorithm. Find affine space partition mappings for a loop originating the
dependences defined by set D.
1.
2.

From each dependence I j − > J j , j = 1,2,..., q , build the constraint in the form
of (4).
Construct a system of linear equations of the form

D kj = 0,
d j = 0, j = 1,2,..., q, k = 1,2,..., r j
which we rewrite as

Ax = 0 ,
where x is a vector representing all the unknown coordinates of C s and
constant terms c s of the affine space partition mappings, s=1a, 2a, ..., n,...,
2b, 1b.
The remaining steps are the same as in the algorithm proposed in [15 ], namely

930

3.

V. Beletskyy

Eliminate all the unknowns c s from A x = 0 with the Gaussian Elimination
algorithm. Let the reduced system be A’ x’= 0 , where x ’ represents the
unknown coordinates of C s .

4.
5.

Find the solution to A’ x’= 0 as a set of basis vectors spanning the null space
of A’.
Find one row of the desired affine partition mapping from each basic vector
found in step 4. The coordinates of C s are formed directly by the basic
vector; the constant terms c s are found from the coordinates of C s using

Ax = 0 .
After finding mappings, well-known techniques for generating parallel code can
be applied, for example, [2],[4],[5],[17] and they are out of the scope of this paper.

4 Examples
Let us illustrate the technique presented by means of the two following examples.
Example 1:
for (i = 1; i <= n; i++)

for (j = 1; j<= n; j++)
for (k = 1; k <= n; k++)
s1: a(j)=b(i);
For this loop, the dependences found with Petit are as follows
output s1: a(j) Å s1: a(j)
{[i,j,k] Å [i,j,k’] : 1 <= k < k’ <= n && 1 <= i <= n && 1 <= j <= n},
output s1: a(j) Å s1: a(j)
{[i,j,k] Å [i’,j,k’] : 1 <= i < i’ <= n&&1 <= j <= n&&1 <= k<=n&&1<= k’ <= n}.
of the form φ s1 = [C 11 C 12 C 13] i . According to our
approach, we first form the following constraint
C 11* i + C 12 * j + C 13 * k = C 11* i + C 12 * j + C 13 * k ’
We seek a mapping

C 11* i + C 12 * j + C 13 * k = C 11* i’ + C 12 * j + C 13 * k’ ,
which we simplify to the following form

C 13 * (k − k ’) = 0,
C 11 (i − i ’) + C 13 (k − k ’) = 0 .

Finding Synchronization-Free Parallelism for Non-uniform Loops

931

The resulting constraint is as follows

C 11 = 0
C 13 = 0 .
The linearly independent solution to this system is
C 11 = 0 , C 12 = 1, C 13 = 0 .

Applying
the
Omega
code
generator
(free
available
at
ftp://ftp.cs.umd.edu/pub/omega) for the transformation of the source loop by means
of the space partition mapping C 1 = [0 1 0 ] , we have got the following parallel code
parfor(p = 1; p <= n; p++)

for(t1 = 1; t1 <= n; t1++)
for(t2 = 1; t2 <= n; t2++)
s1: a(p) = b(t1);
where for and parfor denote serial and parallel loops, respectively. The outer loop
gives space partitioning while the inner loops define the statement instances executed
serially by a given processor p.
Consider the following imperfectly nested loop.
Example 2:
for (i = 1; i <= n; i++){

for (j = 1; j <= n; j++){
for (k = 1; k <= n; k++){
s1: c(i,j,k)=a(N-j,k);
}
s2: a(N-j+1,i)= b(j,k);
}
}
This loop originates the following dependences found with Petit
anti

s1: a(N-j,k) Å s2: a(N-j+1,i)

{[i,j,i] Å [i,j+1] : 1 <= i <= N && 1 <= j < N,},
anti

s1: a(N-j,k) Å s2: a(N-j+1,i)

{[i,j,k] Å [k,j+1] : 1 <= i < k <= N && 1 <= j < N},
flow

s2: a(N-j+1,i)Å s1: a(N-j,k)

{[i,j] Å [i’,j-1,i] : 1 <= i < i’ <= N && 2 <= j <= N}.

932

We

V. Beletskyy

seek

mappings

of

the

form

φ s1 = [C 11 C 12 C 13] i + c1

and

φ s 2 = [C 21 C 22] i + c 2 . Firstly, we form the following constraint
C 11 * i + C 12 * j + C 13 * i + c1 = C 21 * i + C 22 * ( j + 1) + c 2
C 11 * i + C 12 * j + C 13 * k + c1 = C 21 * k + C 22 * ( j + 1) + c 2
C 21 * i + C 22 * j + c 2 = C 11 * i ’+ C 12 * ( j − 1) + C 13 * i + c1
and next transform it to the form

(C 11 − C 21 + C 13) * i + (C 12 − C 22 ) * j + c1 − c 2 − C 22 = 0
C 11 * i + (C 12 − C 22) * j + (C 13 − C 21) * k + c1 − c 2 − C 22 = 0
(C 21 − C 13) * i + (C 22 − C 12) * j − C 11 * i ’+ c 2 − c1 + C 12 = 0 .
On the basis of the equations above, we construct the following constraint

C 11 − C 21 + C 13 = 0
C 12 − C 22 = 0
c1 − c 2 − C 22 = 0
C 11 = 0
C 13 − C 21 = 0
C 21 − C 13 = 0

(5)

c 2 − c1 + C 12 = 0 .
Eliminating c1 , c 2 , we get

C 11 − C 21 + C 13 = 0
C 12 − C 22 = 0
C 12 − C 22 = 0
C 11 = 0
C 13 − C 21 = 0
C 21 − C 13 = 0 .
The linearly independent solution to the system above is
C 11 = 0 , C 12 = 1, C 13 = 1 , C 21 = 1, C 22 = 1 .

From system (5) we find that c1 =1, c 2 =0.
Applying the Omega code generator for the transformation of the source loop by
means of the space partition mappings found, we have got the following parallel code

parfor(p=2; p<= 2*N+1; p++) {
for(t1=1; t1<= N; t1++) {
for(t2=max(-N+p-1,1); t2<=min(-t1+p-1,N); t2++) {

Finding Synchronization-Free Parallelism for Non-uniform Loops

s1(t1,t2,p-t2-1);
}
if (t1 >= 2 && t1 <= p-1
s1(t1,p-t1,t1-1);
}
if (t1 >= 2 && t1 <= p-1
s2(t1,p-t1);
}
for(t2 = max(-t1+p+1,1);
s1(t1,t2,p-t2-1);
}
if (p <= N+1 && t1 <= 1)
s2(1,p-1);
}

933

&& t1 >= -N+p) {

&& t1 >= -N+p) {

t2 <= min(p-2,N); t2++) {

{

}
}
where for and parfor denote serial and parallel loops, respectively; s1 and s2 are
the statements of the source loop.
The outer loop gives space partitioning while the inner loops define the statement
instances which should be executed serially by a given processor p.

5

Related Work and Conclusion

Unimodular loop transformations[3],[19], permitting the outer loop to be parallelized,
find synchronization-free partitions. But unimodular transformations do not allow
such transformations as loop fission, fusion, scaling, reindexing, or reordering.
Techniques presented in [1],[11] enable finding synchronization-free partitioning
only for perfectly nested loops, supposing statements within each loop iteration are
indivisible.
The affine partitioning framework, considered in many papers, for example,
[8],[9],[10],[15], unifies a large number of previously proposed loop transformations.
It is the most powerful framework for the loop parallelization today allowing us to
parallelize loops with both uniform and non-uniform dependences.
Work [15] is most closely related to ours. In contrast to that work and other
known approaches, our technique permits us to form constraints for finding affine
space partition mappings for non-uniform loops directly in a linear form without the
necessity of applying the Farkas lemma to linearize the constraint, and hence it is less
time-consuming than that of work [15] and other known approaches.
In the future research, we plan to extend our technique to find affine time partition
mappings for the non-uniform loops which do not allow synchronization-free
parallelization.

934

V. Beletskyy

References
[1] Amarasinghe, S.P., Lam, M.S.: Communication optimization and code generation for
distributed memory machines. In: Proceedings of the SIGPLAN'93 (1993) 126–138
[2] Ancourt, C., Irigoin, F.: Scanning polyhedra with do loops. In: Proceedings of the Third
ACM/SIGPLAN Symposium on Principles and Practice of Parallel Programming, ACM
Press (1991) 39–50
[3] Banerjee, U.: Unimodular transformations of double loops. In: Proceedings of the Third
Workshop on Languages and Compilers for Parallel Computing (1990) 192–219
[4] Boulet, P., Darte, A., Silber, G.A., Vivien, F.: Loop paralellelization algorithms: from
parallelism extraction to code generation. Technical report (1997)
[5] Collard, J.F., Feautrier, P., Risset, T.: Construction of do loops from systems of afffne
constraints. Technical Report 93–15, LIP, Lyon (1993)
[6] Darte, A., Risset, T., Robert, Y.: Loop nest scheduling and transformations. In Dongarra,
J., Tourancheau, B., eds.: Enviroments and tools for parallel science computing. North
Holland (1993)
[7] Darte, A., Silber, G., Vivien, F.: Combining retiming and scheduling techniques for loop
parallelization and loop tiling. Technical Report 96–34, Laboratoire de l'Informatique du
Parallelisme (1996)
[8] Feautrier, P.: Some efficient solutions to the affine scheduling problem, part i, one
dimensional time. International Journal of Parallel Programming 21 (1992) 313–348
[9] Feautrier, P.: Some efficient solutions to the affine scheduling problem, part ii,
multidimensional time. International Journal of Parallel Programming 21 (1992) 389–420
[10] Feautrier, P.: Toward automatic distribution. Journal of Parallel Processing Letters 4
(1994) 233–244
[11] Huang, C., Sadayappan, P.: Communication-free hyperplane partitioning of nested loops.
Journal of Parallel and Distributed Computing 19 (1993) 90–102
[12] Kelly, W., Pugh, W.: A framework for unifying reordering transformations. Technical
Report CS-TR-2995.1, University of Maryland (1993)
[13] Kelly, W., Maslov, V., Pugh, W., Rosser, E., Shpeisman, T., Wonnacott, D.: The omega
library interface guide. Technical Report CS-TR-3445, University of Maryland (1995)
[14] Lim, W., Lam, M.S.: Communication-free parallelization via affine transformations. In:
Proceedings of the Seventh Workshop on Languages and Compilers for Parallel
Computing (1994) 92–106
[15] Lim, W., Lam, M.S.: Maximizing parallelism and minimizing synchronization with affine
transforms. In: Conference Record of the 24th ACM SIGPLAN-SIGACT Symposium on
Principles of Programming Languages (1997)
[16] Pugh, W., D.Wonnacott: An exact method for analysis of value-based array data
dependences. In: Workshop on Languages and Compilers for Parallel Computing (1993)
[17] Quillere, F., Rajopadhye, S., Wilde, D.: Generation of efficient nested loops from
polyhedra. International Journal of Parallel Programming 28 (2000)
[18] Sass, R., Mutka, M.W.: Enabling Unimodular transformations. In: Proceedings of
Supercomputing'94 (1994) 753–762
[19] Wolf, M.E.: Improving locality and parallelism in nested loops. Ph.D. Dissertation CSLTR-92-538, Stanford University, Dept. Computer Science (1992)

