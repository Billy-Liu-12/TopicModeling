A Test Environment for High Integrity Software
Development
Alejandro Alonso, Juan Antonio de la Puente, and Juan Zamorano
Departamento de Ingenier´ıa de Sistemas Telem´aticos
Universidad Polit´ecnica de Madrid, E-28040 Madrid, Spain
{aalonso,jpuente}@dit.upm.es, jzamora@fi.upm.es

Abstract. The paper describes the architecture and implementation of the dynamic analysis tools that are part of the DOBERTSEE environment. DOBERTSEE
is a low-cost, flexible software engineering environment which enables different
development processes and methods to be supported by integrating different tools.
An XML-based language is used as a basis for integration. The current version
of the environment supports the HRT-HOOD method and Ravenscar Ada, and
includes an extensive set of static and dynamic analysis tools.

1

Introduction

Software engineering environments have hardly lived up to their promises in supporting
extensive development of high-quality software along the whole life cycle of software
products. In spite of the sound technical approaches that can be found in some commercial
systems, such factors as the monolithic nature of many of them, the difficulties that
are often found when trying to adapt them to the particular methods and idioms of
a particular development, and the lack of flexibility in supporting different kinds of
development processes have limited their use in many application areas, in particular in
the high-integrity systems field.
The DOBERTSEE (Dependable On-Board Embedded Real-Time Software Engineering Environment) project [1] was launched by the European Space Agency (ESA)
in 2000 with the aim of developing a new, open software engineering environment for
on-board software which supports up-to-date technology with a low cost, in such a way
that new methods and tools can easily be integrated into it. The project builds on the
results of the previous ECSS-PMod project, in which the requirements of a software
engineering environment (SEE) for supporting the ECSS1 software standards [2] were
developed and a prototype implementation was built [3].
It can be expected that such a SEE can be easily integrated in a company’s practice,
by adapting it to the particular processes, methods, and tools used by the project teams,
while keeping the required investment costs low enough to be affordable even for small
or medium size projects. The technical approach is based on the extensive use of an XML
based language, called CASEML, as the glue between different tools and representations
of software. The environment is targeted to on-board spacecraft software, and is intended
1

European Co-operation for Space Standardization.

J.-P. Rosen and A. Strohmeier (Eds.): Ada-Europe 2003, LNCS 2655, pp. 359–367, 2003.
c Springer-Verlag Berlin Heidelberg 2003

360

A. Alonso, J.A. de la Puente, and J. Zamorano

User interface

External tools

Basic tools

Data repository

Fig. 1. SEE architecture

to be flexible enough to cover different process models and methods in the framework
of the ECSS-E40 standard.
The main focus of this paper is on the dynamic analysis tools which have been developed as part of the DOBERTSEE environment. Section 2 describes the main features
and the overall architecture of DOBERTSEE. Section 3 discusses the role of testing in
the DOBERTSEE software design process and the general approach to testing that has
been followed in the project. The tools themselves are described in Sect. 4. Finally, some
conclusions are presented in Sect. 5

2 The DOBERTSEE Environment
The SEE environment provides a platform in which different tools can be integrated,
together with a basic set of services and a common user interface (figure 1). The basic services include support for life cycle definition, enactment of the ECSS process
model, configuration management, documentation, traceability, and distributed cooperative work. A common data repository is also part of the core environment.
The SEE can thus be seen as a framework into which a variety of software tools can
be integrated. The DOBERTSEE project aims at building a particular instance of SEE
for on-board embedded real-time systems by selecting one such set of tools which are
appropriate for the application domain. The implementation of SEE is based on open
standards such as ASIS [4] and Tcl/Tk [5], so that other tools can be easily integrated
into it.
On board spacecraft software systems typically have high integrity and hard real-time
requirements, for which the development methods and tools have to be used. Vardanega
[6] proposed a design approach for this kind of systems based on extensive use of static
and dynamic analysis as early as possible in the development process. The approach
is based on the use of HRT-HOOD [7,8] as a design method, and a subset of Ada 95
including the Ravenscar profile [9] for implementation, in such a way that static and
dynamic analysis techniques, including response time analysis, [10,11] can be used
in the design process. Figure 2 shows a scheme of the overall design process used in
DOBERTSEE, which explicitly adds dynamic analysis to the original proposal.

A Test Environment for High Integrity Software Development

Execution profile
generator

Ada program

HRT−HOOD
design

361

Design enhancements

Execution profile
RT attributes
and commitments

Platform properties

Static analysis
(including RTA)

Results

Feedback to design

Requirements

Dynamic analysis

Results

Fig. 2. DOBERTSEE design process

The set of tools integrated with the basic SEE in DOBERTSEE is targeted to the
design process, and currently includes:
– STOOD, a graphic HRT-HOOD tool [12].
– GNAT/ORK, an Ada 95 compilation system and run-time system supporting the
Ravenscar profile for ERC-32 computers [13].
– The TSIM simulator for ERC-32 computers [14]
– The RTA tool for response time analysis [15].
– Static analysis tools developed by OEI-UPM.
– Dynamic analysis tools developed by DIT-UPM.
The static and dynamic analysis tools were developed specially for the DOBERTSEE
environment. The next sections describe the dynamic analysis tools and some related
issues.

3 Testing in DOBERTSEE
In spite of the importance of static analysis techniques for the validation and verification
of high integrity systems, dynamic analysis or testing is still of paramount importance
in order to get confidence on the correctness of software systems [10]. Of course testing
has well known limitations, and a systematic approach has to be used in order to get

362

A. Alonso, J.A. de la Puente, and J. Zamorano

useful information from each testing session. Tool support and coverage analysis may
be of great help in performing effective, accurate tests [16].
There are two main categories of tests: black-box, or functional testing, and whitebox, or structural testing. The current DOBERTSEE environment is focused on structural
unit testing, although functional testing can also be supported, as well as some level of
integration testing. DOBERTSEE supports automatic unit testing based on a CASEML
definition of the tests to be performed in each testing session. The test definition includes
the following items:
– General information, such as the name of the unit, the location of the unit code, the
name of the programmer, the name of the tester, time and date of the test, etc.
– Test cases, which are the individual tests to be applied to the unit. Each of the test
cases includes information on the inputs to exercise the unit and expected outputs.
– Coverage metrics to be generated and the required values. An example is to require
that at least a 90% of the statements should be exercised by the execution of the test
cases.
A test is successful if the output of the test cases is the expected and the produced
coverage metrics met those specified.
The following coverage analysis metrics are supported for each test:
– Statement coverage: check that at least a significant number of the statements in the
code have been executed and identify those which have not been executed.
– Module coverage: check that every procedure, function, task, entry, and package
initialization sequence has been executed at least once.
– Loop coverage: check if every loop in the code has been executed zero times, once,
or more than once.
– Decision coverage: check that every decision in the code has been executed with
values true and false at least once.
– Condition coverage: check that every boolean element has been evaluated to true
and false at least once.
– Basis path coverage: check the number of independent paths which are executed in
the test. This metric is closely related to the cyclomatic complexity of a procedure
[17].
A unit is considered to have been successfully tested if the results of the execution of all
the test cases are the expected ones, and the coverage metrics, that have been specified
in the test, are met.
Figure 3 shows the data flow of the operations required to perform automatic unit
testing based on the above principles. The main operations are:
– Code analysis: in order to compute the coverage metrics some information about the
code execution is required (e.g. how many times a loop has been executed, which
statements have been exercised, etc.) This information is obtained by instrumenting
the code, i.e. by inserting additional statements in the original code that store the
required information in appropriate data structures. The information can then be
recovered and analyzed when the instrumented code is executed.

A Test Environment for High Integrity Software Development

code

analyse
code

test
definitions

instrumented
code

generate
test harness

project
state

generate
coverage
metrics

path
enumeration

test harness

363

compile &
execute

execution
traces

test
results

update
project

test
coverage

Fig. 3. Data flow for automatic unit testing

– Test harness generation: a test harness is a program that executes the required test
cases by invoking the tested unit with the appropriated data, and generates a trace of
the execution results for future evaluation. It is important to note that a test harness
may have to be executed on an embedded computer, which makes the whole process
a bit more difficult.
– Code compilation and execution: once a test harness has been produced, it has to be
compiled and run on the target platform or on a simulator. The output of this process
is the result of the execution of all test cases that were included in a test harness,
together with the trace information produced by the instrumented code.
– Coverage metrics generation: The coverage metrics requested by the test definition
are generated out of the execution traces produced by code instrumentation and the
path information provided by the code analysis process.
– Project update: The DOBERTSEE environment keeps information about the state of
the software project under development. The project state has to be updated with the
results of the test execution and the coverage measurements. In particular, the test
definition is updated by including information on the test execution. Other project
information may also be updated as a consequence of the test results.
This set of operations constitute the core of the dynamic analysis activity of the
design process (figure 2).

4 Tool Architecture and Design
4.1

Overview

The automatic testing process described in Sect. 3 is implemented in DOBERTSEE by
means of a driving tool, the test harness tool, which is integrated as a vertical tool in
the SEE architecture (figure 1). The test harness tool is launched from the common SEE

364

A. Alonso, J.A. de la Puente, and J. Zamorano

source code
(Ada)

instruments
(Ada)
code
analyzer
(Ada/ASIS)

instrumenter
(TCL)
code
information
(TCL list)

test
definitions
(CASEML)

instrumented
code
(Ada)
test
analyzer
(TCL)

test
results
(CASEML)

execution
traces
(ASCII)
test harness
generator
(TCL)

compiler & RTS
GNAT/ORK

test harness
(Ada)

Fig. 4. Architecture of dynamic analysis tools

interface and in turn invokes a number of dynamic analysis tools. Figure 4 shows the
overall architecture of the dynamic analysis tools, which closely follows the data flow
diagram depicted in figure 3.
The tools are implemented in a variety of languages, including Ada, Tcl, and CASEML. The GNAT ASIS interface [18] has been used in order to recover semantic information from the Ada compiler, and a new ASIS binding has been developed for the
Tcl tools. The implementation languages have been chosen in order to make it easier to
interface the tools with the basic SEE infrastructure and the GNAT compiler.
4.2 Tcl/ASIS Binding
The Ada Semantic Interface Specification (ASIS) [4] is a standard interface between an
Ada environment and any tool that requires information from it. An Ada environment
includes semantic and syntactic information on Ada units, whose exact contents and
internal representation is compiler-dependent. The purpose of ASIS is to provide an
open and published callable common interface.
For all these reasons, ASIS is a fundamental asset for code analysis. However, as the
main implementation language in DOBERTSEE is Tcl/Tk, it is necessary to call ASIS
functions from this environment. The implementation of a binding for this purpose was
not difficult, as it is possible to call C functions from a Tcl/Tk interpreter. Therefore,
it is possible to call ASIS from a Tcl/Tk interpreter by using the Ada capabilities for

A Test Environment for High Integrity Software Development

365

interfacing with C. There are only some restrictions on the data types to be used, which
are not related to the use of Ada.
Two kinds of capabilities were developed in order to simplify the use of ASIS from
Tcl scripts:
– Generation of Tcl lists: As previously described, the output of the code analyzer is
used by a Tcl program to instrument the code. Therefore, if the analysis results are
output in a format suitable for Tcl, the instrumentation tool code is much simpler.
According to this consideration, the results of the code analysis are stored as Tcl
lists, which are a basic data structure in this language.
– A generic iterator: One of the most powerful components of the ASIS interface
is an iterator, which enables traversing a syntactic tree and performing a given
operation on all its elements. A more robust and adapted version of this iterator
has been implemented for the DOBERTSEE dynamic analysis tools. It includes an
initialization operation which is not part of the standard ASIS iterator, and enables
Tcl variables to be retrieved at the output of a function call.
In addition, a number of packages and Tcl dynamic libraries written in Ada have
been developed in order to support the Tcl/ASIS binding and make its use easier.
4.3

Support for Coverage Metrics

The different kinds of coverage metrics described in Sect. 3 are supported by several
tools. The code analyzer provides the basic information required to instrument the code
according to the required metrics, and the code information for the analysis of the
test results. The test analyzer takes this information and the test execution traces, and
produces the analysis results in SEE compatible format (CASEML).
The implementation of the coverage metrics has been divided in four steps, in accordance with the tool structure depicted in figure 3:
–
–
–
–

Code analysis.
Code instrumentation.
Code compilation and execution.
Results analysis.

Code analysis is based on the generic iterator previously mentioned. There should
be an instantiated package for each supported metric. There are two main procedures
that should be provided for this instantiation:
– Postoperation: it is the procedure in charge of analysing the Ada code in order to
gather the information required for each metric. This information is stored in a Tcl
list for future processing. This list includes the location of Ada statements that are
relevant for the metric under consideration.
– Initialization: it is the procedure that initializes the variables required in the code
analysis for a specific metric.
Code instrumentation is performed by a Tcl program, which calls in turn to a procedure for each of the supported metrics. The inputs are the lists previously generated and

366

A. Alonso, J.A. de la Puente, and J. Zamorano

the source code to be analysed. The goal of the instrumentation is to insert code into the
unit under test, in order to gather information relevant to each metric. This information
is stored in dedicated variables for each metric during the test execution.
The test harness generator takes into account the required metrics, in order to include
the associated packages. When the code is executed, the information produced by the
instrumentation code is stored in a file for future processing.
Code analysis is performed by a Tcl procedure for each metric. These procedures
get the appropriate information from the file with the results and produce the associated
metric. Finally, the tool checks whether the coverage metrics produced are within the
acceptable bounds specified in the test definition.
An important effort was devoted to make the addition of a new metric as simple
as possible. For each of them it is necessary to include the procedures described in this
section. Then, it is simply required to insert the appropriate calls from the tool procedures
that performs the above mentioned steps. The only detail to take into account is to assign
to the new metric an unique name.

5

Conclusions

The DOBERTSEE project has shown that it is possible to build an open, low cost environment for high integrity real-time systems. The use of common tools and notations makes
it easy to integrate new tools into its basic platform, so tailoring the SEE environment
to different application fields.
The test harness tool is a fundamental component of DOBERTSEE. Its design and
implementation are a practical demonstration of the power of the SEE approach, and
provide the basis for the adaptation and extension of the tool to future project needs.
We believe that this approach can extend the use of integrated software engineering
environments to application areas where they are currently seldom used, by allowing
project managers and developers to integrate their preferred tools in the environment and
to adapt its configuration to their particular needs and to the requirements of different
kinds of software processes.
Acknowledgements. The DOBERTSEE project has been funded by ESA/ESTEC under
contract no. 15133/01/ NL/ND. It has been carried out by a consortium including UPM
(Technical University of Madrid), TNI (Techniques Nouvelles d’Informatique), and ESTEC. We would like to thank Juan Garbajosa, the project leader, and Jorge Amador, the
project officer, for the fruitful discussions that we have had on the topics described in
this paper.
We would also like to acknowledge the work of Ignacio Mart´ın, Miguel Mu˜noz,
´
and Miguel Angel
Parra, who have implemented and tested most of the tools that are
described in this paper.

A Test Environment for High Integrity Software Development

367

References
1. Garbajosa, J., Alonso, A., Amador, J., Bolla´ın, M., de la Puente, J.: A low cost software
engineering environment for on-board real-time software. In: Workshop on Advanced RealTime Software Technologies, Aranjuez, Spain (2002)
2. ECSS: ECSS-E-40A Space Engineering — Software. (1999) Available from ESA.
3. GMV-ECSS-UM-01: SEE User Manual. 4.1 edn. (2000) Also available from ESA.
4. ISO: Ada Semantic Interface Specification (ASIS). ISO/IEC- 15291:1999. (1999)
5. Ousterhout, J.: Tcl and the Tk Toolkit. Addison-Wesley (1994)
6. Vardanega, T.: Development of on-board embedded real-time systems: An engineering approach. Technical Report ESA STR-260, European Space Agency (1999) ISBN 90-9092334-2.
7. Burns,A., Wellings,A.: HRT-HOOD:A design method for hard-real-time. Real-Time Systems
6 (1994) 73–114
8. Burns, A., Wellings, A.: HRT-HOOD(TM): A Structured Design Method for Hard Real-Time
Ada Systems. Elsevier Science, Amsterdam (1995) ISBN 0-444-82164-3.
9. Burns, A., Dobbing, B., Romanski, G.: The Ravenscar profile for high integrity real-time
programs. In Asplund, L., ed.: Reliable Software Technologies — Ada-Europe’98. Number
1411 in LNCS, Springer-Verlag (1998)
10. ISO/IEC: Guide for the use of the Ada Programming Language in High Integrity Systems.
(2000) Technical Report TR 15942:2000.
11. Burns, A., Dobbing, B., Vardanega, T.: Guide for the use of the Ada Ravenscar profile in high
integrity systems. Technical Report YCS-2003-348, University of York (2003)
12. TNI: STOOD home page. http://www.tni.fr (2003)
13. de la Puente, J.A., Ruiz, J.F., Zamorano, J.: An open Ravenscar real-time kernel for GNAT.
In Keller, H.B., Ploedereder, E., eds.: Reliable Software Technologies — Ada-Europe 2000.
Number 1845 in LNCS, Springer-Verlag (2000) 5–15
14. Gaisler, J.: TSIM Simulator User’s Manual. Gaisler Research. (2001)
15. de la Puente, J.A.: RTA User’s Guide. DIT-UPM. 1.3 edn. (2001)
16. Ghezzi, C., Jazayeri, M., Mandrioli, D.: Fundamentals of Software Engineering. PrenticeHall, Englewood Cliffs, New Jersey (1991)
17. Watson, A.H., McCabe, T.J.: Structured Testing: A Testing Methodology Using the Cyclomatic Complexity Metric. NIST Special Report 500-235 (1996).
18. Rybin, S., Strohmeier, A., Kuchumov, A., Fofanov, V.: ASIS for GNAT: From the prototype
to the full implementation. In Strohmeier, A., ed.: Reliable Software Technologies — AdaEurope’96: Proceedings. Volume 1088 of Lecture Notes on Computer Science., SpringerVerlag (1996)

