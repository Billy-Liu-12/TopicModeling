Words as Rules: Feature Selection in Text
Categorization
E. Monta˜
n´es, E.F. Combarro , I. D´ıaz, J. Ranilla, and J.R. Quevedo
Artiﬁcial Intelligence Center, University of Oviedo, Spain
ir@aic.uniovi.es

Abstract. In Text Categorization problems usually there is a lot of
noisy and irrelevant information present. In this paper we propose to
apply some measures taken from the Machine Learning environment for
Feature Selection. The classiﬁer used is Support Vector Machines. The
experiments over two diﬀerent corpora show that some of the new measures perform better than the traditional Information Theory measures.

1

Introduction

Text Categorization (TC) [1] consists of assigning a set of documents to a set of
categories. The removal of irrelevant or noisy features [2] improves the performance of the classiﬁers and reduces the computational cost.
The bag of words [1] is the most common document representation, using the
absolute frequency (tf ) to measure the relevance of the words over the documents [3]. Stemming and removing of stop words are usually performed. In this
paper, words occurring in each category are used isolated from the rest [1] (local
sets). The classiﬁcation is tackled using the one-against-the-rest [4] approach and
Support Vector Machines (SVM), since they perform fast and well [3] in TC.
This paper proposes some well-known impurity measures taken from the
Machine Learning (ML) environment to perform Feature Selection (FS).
The rest of the paper introduces these measures, describes the corpora and
the experiments and presents some conclusions and ideas for further research.

2

Feature Selection

FS is commonly performed in TC by keeping the words with highest score according to a measure of word relevance, like Information Theory (IT) measures.
They consider the distribution of the words over the diﬀerent categories. Some
of the most adopted are information gain (IG) [5], expected cross entropy for
text (CET ) [6] and S − χ2 [7], a modiﬁcation of the χ2 statistic [2].
In the measures proposed here, ﬁxed a category c, a word w is identiﬁed with
the rule w → c which says: If w is in a document, then the document belongs
This work has been supported under MCyT and Feder grant TIC2001-3579.
The author acknowledges the support of research project FICYT PR-01-GE-15.
M. Bubak et al. (Eds.): ICCS 2004, LNCS 3036, pp. 666–669, 2004.
c Springer-Verlag Berlin Heidelberg 2004

Words as Rules: Feature Selection in Text Categorization

667

to c. Then, the relevance of w for c is identiﬁed with the quality of the rule
w → c [8].
Many popular rule quality measures are based on the percentage of successes
and failures of the application of the rule. Two examples are the Laplace measure (L) and the diﬀerence (D) [9]. The former is a slight modiﬁcation of the
precision. The latter establishes a balance between the documents containing w
and penalizes the words from documents not belonging to c. They are deﬁned
by
aw,c + 1
L(w → c) =
D(w → c) = aw,c − bw,c
aw,c + bw,c + s
where aw,c is the number of documents of c in which w appears, and bw,c is the
number of documents contaning w but not belonging to c.
We also propose variants that consider the absence of the word in c, penalizing
more aggressively those words which appear in few documents of c. Hence, we
deﬁne
Lir (w → c) =

aw,c + 1
aw,c + bw,c + cw,c + s

Dir (w → c) = aw,c − bw,c − cw,c

where cw,c is the number of documents from c not containing w.

3

Experiments

Before presenting the experiments, we describe the corpora. Reuters-21578 contains short economic news. The distribution of the documents over the categories
is quite unbalanced and the words are little scattered. Considering the Apt´e
split [4] we obtain 7063 training documents and 2742 test documents, assigned
to 90 diﬀerent categories.
Ohsumed is a MEDLINE subset from 270 medical journals. Here, we consider
the ﬁrst 20, 000 MEDLINE documents from 1991 with abstract (the ﬁrst 10, 000
for train and the rest for test) and the 23 subcategories of diseases (C of MeSH1 ).
The words here are quite more scattered than in Reuters and the distribution
of documents over the categories is much more balanced.
In the experiments, the ﬁltering levels (ﬂ) range from 20% to 98%. One-tail
paired t-tests at signiﬁcance level of 95% are conducted between F1 s for each
pair of measures. Table 1 presents the macroaverage and the microaverage of
F1 [1]. Tables 2 and 3 show the t-test results2 .
For Reuters, Lir , Dir and IG produce, in general, the best macroaverage
and microaverage among their variants. This may be because the words are
little scattered, that is, each category has a high percentage of speciﬁc words.
Hence, the best measures are either those which tend to select words frequent in
1
2

Medical Subject Headings http://www.nlm.nih.gov/mesh/2002/index.html
In them, ”+” means that the ﬁrst measure is signiﬁcantly better than the seconnd,
”-” means that the seconnd measure is better than the ﬁrst, ”=” means that there
exists no signiﬁcative diﬀerence.

668

E. Monta˜
n´es et al.
Table 1. Macroaverage and Microaverage of F1 for diﬀerent variants
Reuters

f l(%) L
20 47.15
40 48.31
60 48.40
80 43.73
85 43.74
90 41.64
95 33.45
98 30.43

Lir
47.53
48.90
49.01
48.66
48.40
48.09
47.42
45.73

Macroaverage
D
Dir CET
46.41 46.71 46.92
45.17 45.75 46.90
44.06 44.61 46.89
42.41 43.77 48.55
41.61 44.06 48.84
40.30 42.48 48.12
38.87 41.42 47.61
37.85 40.68 47.55

f l(%) L
20 46.27
40 51.11
60 51.32
80 48.42
85 46.40
90 47.46
95 48.33
98 47.82

Lir
42.49
42.91
43.66
44.31
44.14
44.57
44.17
41.02

Macroaverage
D
Dir CET S − χ2
49.54 50.00 45.92 45.86
50.57 51.52 46.73 46.71
50.01 51.11 47.39 47.28
48.77 50.74 48.14 47.40
48.93 51.17 48.36 47.64
50.26 52.19 48.79 47.85
51.58 52.37 48.91 46.94
52.27 51.44 47.47 44.16

2

S-χ
46.35
47.27
46.67
48.87
47.69
47.87
47.82
46.30

IG
L
46.07 82.71
48.03 81.53
47.84 80.16
48.57 77.69
48.44 76.80
48.69 75.26
49.19 74.15
48.41 74.76
Ohsumed
IG
42.71
43.61
45.21
46.94
47.11
47.54
49.40
49.66

L
53.53
56.92
56.63
53.41
51.32
52.22
53.75
53.08

Lir
85.03
85.16
85.26
85.21
85.05
84.81
84.47
83.43

Microaverage
D
Dir CET
80.99 81.54 83.39
79.42 80.49 83.03
78.91 79.79 83.03
78.68 79.99 83.33
78.63 80.25 83.40
77.77 80.26 82.99
75.63 79.26 83.22
78.40 80.37 83.02

Lir
51.51
51.54
51.99
51.90
51.24
51.15
49.94
45.85

Microaverage
D
Dir CET S − χ2
56.19 56.77 53.46 53.47
56.20 57.25 53.91 53.94
55.52 56.89 54.49 54.35
55.11 57.18 54.80 54.49
53.45 57.54 55.10 54.75
54.36 57.73 55.42 54.93
55.72 58.04 55.69 54.48
56.24 57.00 54.04 51.99

S-χ2
83.35
83.19
83.28
83.40
83.17
83.30
83.57
83.01

IG
84.78
84.96
85.02
85.42
85.53
85.48
85.40
84.76

IG
51.93
52.31
53.13
53.98
54.19
54.59
55.47
54.81

Table 2. t-tests among diﬀerent variants
Reuters
Ohsumed
f l(%) Lir -L Dir -D IG-CET IG-S-χ2 CET -S-χ2 Lir -L Dir -D IG-CET IG-S-χ2 CET -S-χ2
20
=
+
=
=
=
+
=
=
+
+
=
=
+
=
40
=
=
=
=
=
+
=
60
+
+
=
=
=
+
=
+
80
+
+
=
=
=
=
+
=
+
85
+
=
+
+
+
=
=
=
90
+
+
+
+
=
=
=
+
+
95
=
+
+
+
+
+
=
+
+
98

the category (like Lir and Dir ) or those that consider the absence of the word
in the category (like IG). Among them, Lir and IG are statistically better, with
no signiﬁcative diﬀerences between them.
Regarding Ohsumed, L, Dir and CET are, in general, the best measures
among their variants, in macroaverage and in microaverage. Here, there are not
so many speciﬁc words in each category, since the words are slightly scattered.
Hence, the best measures are those that select words frequent in the category,
although they appear in the rest, like L, Dir and CET . Among them, Dir is
statistically better than the rest.

4

Conclusions and Future Work

This paper proposes some measures taken from Machine Learning for Feature
Selection in TC, comparing them with other traditional Information Theory
measures.
The performance of the measures depends on the corpus. For Reuters, which
has an unbalanced distribution of documents and has the words little scattered, it

Words as Rules: Feature Selection in Text Categorization

669

Table 3. t-tests among the best variants
Filtering level
Lir − Dir
Lir − IG
Dir − IG

20 40 60 80 85 90 95 98
Reuters
= + + + + + + +
+ = = = = = - =
= = -

L − Dir
L − CET
Dir − CET

20 40 60 80 85 90 95 98
Ohsumed
- = = + + + = = = = =
+ + + + + + + +

is better to penalize more those words which are not frequent in the category or to
consider the absence of words in each category. For Ohsumed, whose distribution
of documents is more uniform and which has the words highly scattered, it is
better to reinforce the words of each category and not to penalize so much the
words of the rest.
In our future work, we plan to use other classiﬁers and to ﬁnd the optimal
ﬁltering levels for each measure, which may depend on properties of the category.

References
1. Sebastiani, F.: Machine learning in automated text categorisation. ACM Computing
Survey 34 (2002)
2. Yang, T., Pedersen, J.P.: A comparative study on feature selection in text categorisation. In: Proceedings of ICML’97, 14th International Conference on Machine
Learning. (1997) 412–420
3. Joachims, T.: Text categorization with support vector machines: learning with many
relevant features. In N´edellec, C., Rouveirol, C., eds.: Proceedings of ECML-98, 10th
European Conference on Machine Learning. Number 1398, Chemnitz, DE, Springer
Verlag, Heidelberg, DE (1998) 137–142
4. Apte, C., Damerau, F., Weiss, S.: Automated learning of decision rules for text
categorization. Information Systems 12 (1994) 233–251
5. D´ıaz, I., Ranilla, J., Monta˜
n´es, E., Fern´
andez, J., Combarro, E.: Improving performance of text categorization by combining ﬁltering and support vector machines.
(Journal of the American Society for Information Science and Technology (JASIST))
Accepted for publication.
6. Mladenic, D., Grobelnik, M.: Feature selection for unbalanced class distribution and
naive bayes. In: Proceedings of 16th International Conference on Machine Learning
ICML99, Bled, SL (1999) 258–267
7. Galavotti, L., Sebastiani, F., Simi, M.: Experiments on the use of feature selection
and negative evidence in automated text categorization. In: Proceedings of ECDL00, 4th European Conference on Research and Advanced Technology for Digital
Libraries, Lisbon, Portugal, Morgan Kaufmann (2000) 59–68
8. Combarro, E.F., Monta˜
n´es, E., Ranilla, J., Fern´
andez, J.: A comparison of the
performance of svm and arni on text categorization whit new ﬁltering measures
on an unbalanced collection. In: International Work-conference on Artiﬁcial and
Natural Neural Network, IWANN2003, Lecture Notes of Springer-Verlag. (2003)
9. Muggleton, S.: Inverse entailment and prolog. New Generation Computing, Special
issue on Inductive Logic Programming 13 (1995) 245–286

