Parallel Files Distribution
Laurentiu Cucos1 and Elise de Doncker1
Western Michigan University
{lcucos ,elise}@cs.wmich.edu
http://aegis.cs.wmich.edu/~lcucos
http://www.cs.wmich.edu/~elise
Abstract. For parallel computations requiring massive data Input/Output, one of the goals is to maximize the use of the underling
storage topology, in particular exploit the benefit of using local disks.
This paper presents a mechanism to distribute independent data records
from multiple files to multiple processing nodes and vice-versa. An allocation problem is solved using the Max-Flow algorithm. We give timing
results using various read/distribute protocols. One application is to redistributing tasks (regions) for restarting parallel numerical integration
runs.

1

Introduction and Motivation

In order to maintain a minimum transport cost between producers, consumers
and warehouses, it is preferable to have all three within the same system, in
close proximity, or use paths of easy access between one another. In addition,
is desired to have a small number of moves or stops between producers and
consumers.
As in an industrial scenario, parallel computations that produce, store and
later consume large amounts of data, have to follow similar principles.
Let us consider an arbitrary topology of high speed connections between a
number of organizations, each providing a number of machines with either local
storage and/or access to shared partitions. The optimum distribution would be
achieved when the data is generated, saved, and loaded in the same computing
node (keeping all other data transfers at a minimum); however, this may not
always be possible or eﬃcient. For irregular problems, some nodes may generate
more data than they can store, reload and re-process, or just more data than
other nodes. Under these conditions it would be diﬃcult to predict and control
how the data should be distributed.
Addressing mainly scenarios where data need to be read only once, this paper
presents an eﬃcient distribution engine.
As an application we consider redistributing regions in between calls to numerical multivariate integration codes. ParInt (Parallel Integration project) [1,
5] implements adaptive partitioning (as well as other) strategies for multivariate
integration.
Section 2 below outlines a general equal-distribution algorithm, Section 3
gives experimental results and Section 4 concludes the paper.
V.S. Sunderam et al. (Eds.): ICCS 2005, LNCS 3516, pp. 991–994, 2005.
c Springer-Verlag Berlin Heidelberg 2005

992

2

L. Cucos and E. de Doncker

Static Distribution

To avoid synchronization overheads, and since the total amount of work is known,
it is more eﬃcient to compute the data distribution in advance. While in most
cases data can be easily preallocated by distributing equal shares among all
machines, non-trivial interconnection networks require more sophistication.
Consider the following general allocation problem [4]:
Problem: Given a set of n machines, M = {mi }1≤i≤n , a collection of p ﬁles,
S = {sj }1≤j≤p , where sj has |sj | records, and a unidirectional bipartite graph
G = {(M, S), E}, where E = {(vi , uj )} with vi ∈ M and uj ∈ S, which deﬁnes
a mapping between machines and ﬁles, compute the item distribution so that
each machine gets an equal share of records. In case that an equal distribution
cannot be achieved, ﬁnd a solution with records shipped between machines at a
minimum communication cost.
Example: Four computing nodes m1 , m2 , m3 , m4 , participating in a computation requiring data from three ﬁle servers F S1 , F S2 , F S3 ; where m1 , m2 , m4 can
access F S1 , m1 , m3 can access F S2 , and all can access F S3 . All machines are
interconnected by the same type of network.
Solution: An eﬃcient distribution can be achieved using a Min-Cut Max-Flow
algorithm [3] and the following procedure.
1. Augment the graph with two new nodes: source and sink, and edges between
all set-nodes and source, and all machine-nodes and sink.
2. Assign inﬁnite capacities to edges between sets and machines, |Si | between
p
i=1 |Si |
between each machine and the sink.
set Si and source, and
n
3. Apply the Max-Flow algorithm to the newly created graph.
p
4. If the resulting ﬂow is i=1 |Si |, there exists an equal distribution of the
sets; the distribution is given by the ﬂow in the inner edges. Otherwise, the
sets cannot be equally distributed without additional communication and go
to step 5.
5. Equally distribute the records not allocated in step 3 to machines with a
smaller number of records.

3

Experiments

We implemented the system pfd which is a parallel read/write engine to distribute data records from/to ﬁles located over multiple machines. It provides
a transparent and eﬃcient way to incorporate data reading, distributing, and
processing in the same program. The system is layered over the mpi message
passing library [2].
pfd was mainly developed for ParInt, to save/load large numbers of integration regions; however it can be easily integrated in any application that
reads/writes large numbers of independent records.

Parallel Files Distribution

993

Given a collection of records distributed over a number of machines, pfd:
- reads each record exactly once (when the record is processed);
- minimizes the communication and eﬃciently distributes all records among
all participating nodes;
- provides an easy and transparent way to use local disks;
- uses a buﬀer of conﬁgurable size for interprocess communication;
- provides an array of options to read and distribute the data such that:
- each node will process the same number of records (Equal),
- or allow faster nodes to process more data (Compete),
- or, only nodes with direct access to data will process records (Local).
The system is written in C, and can be integrated with the user application
trough a small set of read/write/pack/unpack functions.
We performed a set of experiments reading 1GB of data organized in 8KB
records and using diﬀerent distributions, ranging from all data being stored in

110
Equal
Compete
Local

100

Equal
Compete
Local

120

Total Run Time(sec)

Total Run Time(sec)

100
90
80
70
60

80
60
40
20

50
40

0
0

2

4
6
Number of Processors

8

10

0

120

4
6
Number of Processors

8

10

8

10

20
Compete
Equal
Local

100

Compete
Equal
Local

18
16
Total Run Time(sec)

Total Run Time(sec)

2

80
60
40

14
12
10
8
6
4

20

2
0

0
0

2

4
6
Processors with Data

8

10

0

2

4
6
Number of Processors

Fig. 1. Top-Left: Distribute records from a shared partition; Top-Right: Distribute
records from one non-shared partition; Bottom-Left: Distribute records located in a
subgroup of machines; Bottom-Right: Distribute records shared equally by the participants

994

L. Cucos and E. de Doncker

one shared partition, to all data located in one local disk. Up to 10 readers are
selected from a set of 1.2GHz AMD Athlon processor nodes, communicating on
a Fast Ethernet network.
Figure 1 (Top-Left) shows results for data located in a shared partition. In
this case, the runtime is independent of the read method (Equal, Compete, or
Local), and of the number of processors, and is mainly driven by the network
transfer speed. Note that the shared partition of this experiment was located on
a disk signiﬁcantly faster than the local disks used in the following experiments.
Figure 1 (Top-Right) shows results for data located in one non-shared partition. While the total read time is around 20 sec, the performance quickly
depreciates as more processors are involved in the computation. We attribute
this behavior to the bottleneck at the distribution node.
Figure 1 (Bottom-Left) and (Bottom-Right) show results for data non-uniformly
distributed, and uniformly distributed, respectively, over participating machines.
There is a signiﬁcant improvement in runtime and scalability. In the latter case,
the read time was not aﬀected by the distribution method (Equal, Compete, or
Local).

4

Conclusions

This paper outlines a mechanism to distribute independent data records from
multiple ﬁles to multiple processing nodes and vice-versa. We present a solution to the general allocation problem of sharing the data equally among the
participating compute nodes, given an arbitrary storage topology.
Although using local disks normally binds data producers with consumers,
our system maintains the beneﬁt oﬀered by this connection, while giving at the
same time a set of options to share data with non-producers.

Acknowledgments
This work is supported in part by Western Michigan University, and by the
National Science Foundation under grants ACI-0000442, ACI-0203776, EIA0130857.

References
1. http://www.cs.wmich.edu/parint, ParInt web site.
2. http://www-unix.mcs.anl.gov/mpi/index.html, MPI web site.
3. Cormen, T. H., Leiserson, C. E., and Rivest, R. L. Introduction to Algorithms.
The MIT press, 1994.
4. Cucos, L. Load Sharing Strategies in Distributed Environments. PhD thesis, Western Michigan University, December 2003.
5. Zanny, R., de Doncker, E., Kaugars, K., and Cucos, L. ParInt1.2 User’s
Manual. Available at http: //www.cs.wmich.edu/parint.

