Regularized Knowledge-Based Kernel Machine
Olutayo O. Oladunni and Theodore B. Trafalis
School of Industrial Engineering, The University of Oklahoma
202 West Boyd, CEC 124 Norman, OK 73019 USA
{tayo, ttrafalis}@ou.edu

Abstract. This paper presents a knowledge-based kernel classification model
for binary classification of sets or objects with prior knowledge. The prior
knowledge is in the form of multiple polyhedral sets belonging to one or two
classes, and it is introduced as additional constraints into a regularized knowledge-based optimization problem. The resulting formulation leads to a least
squares problem that can be solved using matrix or iterative methods. To evaluate the model, the experimental laminar & turbulent flow data and the Reynolds
number equation used as prior knowledge were used to train and test the proposed model.

1 Introduction
In data mining applications, incorporation of prior knowledge is usually not considered because most algorithms do not have the adequate means for incorporating explicitly such types of constraints. In the case of Support Vector Machines (SVMs)
model [1], Fung et al. [2], [3] developed explicit formulations for incorporating prior
knowledge in the form of multiple polyhedral sets belonging to one or more categories into a linear and nonlinear classification model.
The main motivation of this paper is to develop a nonlinear kernel approach based
on the Tikhonov regularization scheme for knowledge-based classification discrimination. The proposed model problem is the kernel-based formulation of the multiclassification model in [4] for a two-class classification problem. The feature of the
proposed model is that it leads to a least squares problem that can be solved by solving a linear system of equations that reduces computational time. In contrast, Fung et
al. [2], [3] solve a linear programming optimization problem.
In this work, a combination of the prior knowledge sets together with the concept
of the least squares SVM (LS-SVM) [5] and regularization based LS-SVM [6] results
into a linear system of equations. Our approach is different in the sense that, the proposed minimization problem is an unconstrained optimization problem resulting into
a smaller linear system of equations than those proposed in [5], [6]. By expressing the
normal vector w as a linear combination of the data points, it turns out that in the least
squares knowledge based formulation the prior knowledge(s) for nonlinear classification is the precise implication of the prior knowledge(s) for linear classification. This
differs from the knowledge based model by Fung et al. [3], where the prior knowledge(s) for nonlinear classification is not the precise implication of the prior knowledge(s) for linear classification. In their formulation, the knowledge constraints are
Y. Shi et al. (Eds.): ICCS 2007, Part I, LNCS 4487, pp. 176–183, 2007.
© Springer-Verlag Berlin Heidelberg 2007

Regularized Knowledge-Based Kernel Machine

177

kernelized so as to fit a standard [7] linear programming formulation for nonlinear
kernel classification.
The regularized knowledge-based kernel classification model can be considered as
a least squares knowledge based nonlinear kernel formulation of Fung et al. [3]. Note
that the resulting regularized knowledge-based model problem minimizes the classical
regularization objective function with strict L2 norm functions. This norm guarantees
the differentiability of the objective function, and as result a linear system of equations is derived. In the knowledge based model by Fung et al. [3], the differentiability
property is lost because in its model problem the functions are L1 norm functions,
which further restrain the solution of the model problem and also requires an LP
solver to obtain a solution.
Benefits of the regularized knowledge-based kernel classification model includes:
the reduction of a classification problem to a smaller linear system of equation, the
ability to provide fast solutions which require no special solvers, the ability to provide
explicit solution in terms of the given data and prior knowledge sets, and the ability to
provide effective and robust classifiers as a result of the bounding planes within a
cluster of points (margin increase).

2 Prior Knowledge in Two-Class Classification
Suppose that in addition to the points belonging to a class, there is prior information
belonging to one or two categories. The knowledge sets [2] in an n dimensional space
are given in the form of a polyhedral set determined by the set of linear equalities and
linear inequalities. The polyhedral knowledge set {x ∈ R n | Bx ≤ b} or {x ∈ R n | Bx = b }
should lie in the halfspace {x ∈ R n | x T w ≥ γ + 1} where B ∈ R g ×n , B ∈ R g × n , b ∈ R g
and b ∈ R g are the prior information belonging to class 1. gu or gu is the number of
prior knowledge (equality or inequality) constraints in class 1. d v or d v is the number
of prior knowledge (equality or inequality) constraints in class 2. Therefore, the following implications must hold for a given (w, γ), where w is a normal vector and γ is
the location of the optimal separating plane relative to the origin:
u

u

u

u

{Bx ≤ b ⇒ x w ≥ γ + 1} or {Bx = b ⇒ x w ≥ γ + 1} .
T

T

(2.1)

Using the nonhomogeneous Farkas theorem of the alternative [8] or, using duality in
linear programming (LP) [9], equations (2.1) can be transformed into a set of knowledge constraints [2].

{B u + w = 0,
T

} {

}

b u + γ + 1 ≤ 0, u ≥ 0 or B u + w = 0, b u + γ + 1 ≤ 0 .
T

T

T

(2.2)

To formulate the nonlinear counterpart of the linear classification, the primal variable w is replaced by its equivalent dual representation w = A T Y α , where α is the
vector of dual variables, A = [ A(1)T A(2)T ]T whose rows are the points belonging to
classes 1 and 2, and Y is a diagonal matrix whose diagonals are +1 for points in class
1 and -1 for points in class 2.

178

O.O. Oladunni and T.B. Trafalis

Let A ∈ R m×n and B ∈ R n× k . The kernel K ( A, B) maps R m× n × R n× k into R m× k . The
behavior of a kernel matrix strongly relies on Mercer’s condition [10], [11] for symmetric kernel functions, i.e., a kernel matrix is a positive semidefinite (PSD) matrix.
Using the dual representation of w and applying the kernel definition, a more complex classifier can be determined, and the nonlinear optimal separating hyperplane is
given as:

K ( xT , AT )Y α = γ .

(2.3)

The implications for a given ( A, Y , α , γ ) now becomes the following [3]:

{Bx ≤ b ⇒ x

T

} {

}

A Y α ≥ γ + 1 or Bx = b ⇒ x A Y α ≥ γ + 1 ,
T

T

T

(2.4)

and the prior knowledge constraints can be rewritten as:

{B u + A Yα = 0,
T

T

} or {B

b u + γ + 1 ≤ 0, u ≥ 0
T

T

}.

u + A Y α = 0, b u + γ + 1 ≤ 0
T

T

(2.5)

Notice that there is no kernel present in the prior knowledge, and as a result equation
(2.4) is the precise implications of equations (2.1). In the subsequent section, when
the prior knowledge constraint is incorporated into a classification model, the kernel
will be employed to obtain nonlinear classifiers.

3 Regularized Knowledge-Based Kernel Classification Machine
n

Consider a problem of classifying data sets with prior knowledge in R that are represented by a data matrix A( i ) ∈ R m × n , where i = 1, 2 , and knowledge sets {x | B (1) x ≤ b (1) }
or {x | B (1) x = b (1) } belonging to class 1, {x | B ( 2) x ≤ b (2 ) } or {x | B ( 2) x = b ( 2) } belonging
to class 2. Let A(1) be a m1 × n matrix whose rows are points in class 1, and m is the
number of data in class 1. Let A( 2) be a m2 × n matrix whose rows are points in class
2, and m2 is the number of data in class 2. This problem can be modeled through the
following optimization problem:
i

1

1
2
2
⎡λ
⎢ 2 α + 2 YKYα − γ Ye − e +
⎢
2
1
T
T
T
T
min f NT KKM (α , γ , a, c) = ⎢ ⎡ Bu a + I u A Y α + Bv c − I v A Y α
⎢2 ⎣
α ,γ , a , c
⎢
⎢ 1 ⎡ BbuT a + γ eu + eu 2 + BbvT c − γ ev + ev
⎣⎢ 2 ⎣
R

⎤
⎥
⎥
2
⎤ +⎥ .
⎦ ⎥
⎥
2
⎤ ⎥
⎦ ⎦⎥

(3.1)

Where α is the vector of dual variables; γ is the location of the optimal separating
plane; λ is a regularization parameter; a = [u , u ] is a vector of all multipliers
referring to class 1, and c = [v , v ] is a vector of all multipliers referring to class 2.
T
(1) T
( 2) T T
K = K ( A, A ) is a kernel matrix; matrix A = [ A A ] whose rows are the points
belonging to classes 1 & 2 points respectively; Y is a diagonal matrix whose diagonals are +1 for points in class 1 and -1 for points in class 2; and vector e = [e (1)T e (2 )T ]T
T

T

T

T

T

T

Regularized Knowledge-Based Kernel Machine

179

is a vector of ones. Matrices Bu & Bv are diagonal block matrices whose diagonals
contain knowledge sets belonging to class 1 & 2. The diagonals of Bu are B ∈ R g × n ,
B ∈ R g × n and of Bv are B ∈ R d × n , B ∈ R d × n . Matrices Bbu & Bbv are created
where the diagonals of Bbu are b ∈ R g ×1 , b ∈ R g ×1 and of Bbv are b ∈ R d ×1 ,
b ∈ R d ×1 . Matrices I u = [ I (Tu ) I ( Tu ) ] & I v = [ I (Tv ) I ( Tv ) ] are block matrices, where I ( u ) ,
I ( u ) , I ( v ) , I ( v ) ∈ R n× n are identity matrices. Vectors eu & ev consist of entries
e1 , e1 ∈ R & e2 , e2 ∈ R , where e1 , e1 , e2 , e2 are equal to one. Vectors eu , ev are vectors
of ones, where each entry corresponds to a vector b , b ( b , b ) respectively.
The ( α , γ) taken from a solution of (3.1) generates the nonlinear separating surface
(2.3). Problem (3.1) was formulated using the concept of penalty functions [9], and it is
called the nonlinear Tikhonov regularization [12] knowledge-based kernel machine
classification model (NTRKKM). It is a binary classification formulation of the multiclassification model in [4] to accommodate nonlinearly separable patterns with knowledge sets, and explicit solutions in dual space in terms of the given data can be derived.
Problem (3.1) is slightly different from the knowledge based model by Fung et al. [4],
which is a linear programming formulation. The difference lies in the selection of the
norm distance and the squared error with regularization term (least squares).
Below is the explicit solution to NTRKKM in terms of the given data:
(1)

(1)

u

(2)

v

( 2)

(2)

v

(1)

u

v

(1)

u

T

(2)

u

v

T

(1)

(1)

( 2)

( 2)

γ = h (dα − t ) .

(3.2)

M α = z ⇒ α = M z , where α ∈ R .
−1

m

(3.3)

Note that h , d, t, M , z are defined as follows.
h = ⎡⎣e YYe + eu

⎡⎣e

− Bbu UBbu eu ⎤⎦ + ev

⎡⎣e

−1

− BbvVBbv ev ⎤⎤
⎦⎦

.

(3.4)

T
T
T
d = ⎡⎣eT YK + euT Bbu
UK Bu
Y + evT BbvT VK Bv
Y ⎤⎦
.
T
t = ⎡⎣ eT Ye + euT ⎡⎣ eu − Bbu
UBbu eu ⎤⎦ − evT ⎡⎣ ev − BbvT VBbv ev ⎤⎦ ⎤⎦

(3.5)

T

(

U = Bu BuT + Bbu BbuT

)

−1

T

u

T

(

, V = Bv BvT + Bbv BbvT

)

−1

T

T

v

, K = YKY

.

(3.6)

⎡λ I + K ⎡ K − Yhed ⎤ +
⎤
⎡ K T ⎡ e − Yhet ⎤ +
⎤
⎣
⎦
⎣
⎦
⎢
⎥
⎢
⎥
M = ⎢Y ⎡⎣ K uY − K BuU ⎡⎣ K Bu T Y + Bbu heu d ⎤⎦ ⎤⎦ + ⎥ , z = ⎢YK BuUBbu ⎡⎣ eu − heu t ⎤⎦ − ⎥ .
⎢
⎥
⎢
⎥
⎢
⎥
T
⎢YK VB ⎡ e + he t ⎤ ⎥
⎡
⎤
⎡
⎤
Y
K
Y
K
V
K
Y
B
he
d
−
+
Bv
bv
v
v
⎣
⎦
v
Bv
Bv
bv
v
⎣
⎦
⎣
⎦ ⎦ ⎦⎥
⎣⎢ ⎣

(3.7)

K u = K ( AI , I u A ), K v = K ( AI , I v A ), K Bu = K ( AI uT , BuT ), K Bv = K ( AI vT , BvT )
T
u

T

T
v

T

Minimizing the L2 norm of α guarantees a solution for a positive tradeoff constant.
It is evident from matrix M that only the diagonals change with any change in the tradeoff constant, implying we can always get a diagonally dominant matrix M which can
ensure a solution. However, if the first term in M is replaced with a kernel matrix, then
for any constant, all elements in matrix M also increase. Therefore, we cannot guarantee

180

O.O. Oladunni and T.B. Trafalis

a diagonally dominant matrix M and hence we cannot always guarantee a solution when
minimizing the square norm of the linear combination of w in dual space.
Assume that U, V and M are invertible matrices. Solution (3.3) provides an estimate of the dual variables. Such a linear system is easier to solve than a quadratic or
linear programming formulation. Methods for solving the system include matrix decomposition methods, or iterative based methods [9], [13]. Its solution involves the
inversion of an m × m dimensional matrix.
The decision function for classifying a point x is given by:

⎧⎪ +1, if point (x) is in class A(1)
.
⎪⎩ −1, if point (x) is in class A( 2)

D ( x ) = sign ⎡⎣ K ( xT , A T )Y α − γ ⎤⎦ = ⎨

(3.8)

4 Laminar/Turbulent Flow Pattern Data Set
The fluid flow data uses flow rate, density, viscosity, borehole diameter, drill collar
diameter and mud type to delineate the flow pattern (laminar, +1; turbulent, -1) of the
model. There are 92 data-points and 5 attributes, 46 instances for laminar flow pattern
and 46 instances turbulent flow pattern [14], [15], [16]. The attributes are as follows:
ρ , density of fluid (lbm/gal – continuous variable); q , flow rate (gal/min – continuous variable); (d2+d1), summation of borehole and drill collar (OD) diameter (in –
continuous variable); μp, plastic viscosity (cp – continuous variable); Mud type, water
based mud (1) oil based mud (2) (categorical variable).
Prior Knowledge: In addition to the fluid flow data, the Reynold’s equation [17] for a
Bingham plastic model is used as prior knowledge to develop a knowledge based
classification model. As additional constraints, the prior knowledge will represent the
transition equations which delineates laminar from turbulent flows. Since the flow
pattern data are scaled by taking the natural logarithm of each instance, the prior
knowledge needs to be scaled by also considering a natural logarithm transformation
of the equations. Below is the equivalent logarithmic transformation for Reynold’s
equation [17]:
⇒

⎧ln( ρ ) + ln( q ) − ln( d
⎨
⎩ln( ρ ) + ln( q ) − ln( d

2

+ d ) − ln( μ ) ≤ 1.9156 − ε → x w − γ ≥ +1 (Laminar)

2

+ d ) − ln( μ ) ≥ 1.9156 + ε → x w − γ ≤ −1 (Turbulent)

T

1

p

T

1

,

(4.1)

p

where ε is a deviation factor, a small fraction perturbing the critical Reynold’s number. The dataset where divided up as follows; 50% of the whole data is used as training data, while the remaining 50% was used as testing data.

5 Computational Results
In this section, the results of the analyzed data sets and prior sets described in section 4,
are presented and discussed. The NTRKKM model is used to train the data sets with
prior knowledge. To demonstrate the uniqueness of the formulations, a comparison

Regularized Knowledge-Based Kernel Machine

181

between the models above was conducted. The comparisons were made by evaluating
a performance parameter (misclassification error) defined below:

⎛ Total number of correctly classified points ⎞ .
⎟
Total number of observed points
⎝
⎠

β = 1− ⎜

(5.1)

β represents the overall misclassification error rate, i.e., the fraction of misclassified
points for a given data set. For 100% classification, β = 0 . The tradeoff constant considered is within the interval 0 – 100, and the deviation factor ε = 0.01. Further comparisons were made between the NTRKKM model, LTRKSVM [16] model problem,
the traditional SVM [1], [14], and the mixed integer programming kernel based classifiers (MIPKCps & MIPKCp, [15]). The polynomial kernel, k ( x , x ) = ( x x + 1) , where p is
the degree of the polynomial (p = 2) was used.
Results of the fluid flow pattern data with prior knowledge information trained on
the NTRKKM model, and compared with the LTRKSVM, SVM, MIPKCps, &
MIPKCp model, are shown in Tables 1 & 2. It should be noted that β (error rate) in
the Tables are defined by (5.1), and the computing (cpu) time is measured in seconds.
All computations and experiments were performed using MATLAB [18] for
NTRKKM, LTRKSVM & SVM model [19], and CPLEX OPL [20] for the mixed
integer programming kernel based models.
T

i

P

i

Table 1. Sample and Average random sample validation test error rate for NTRKKM on Fluid
Flow Pattern Data (varying tradeoff, λ)

Table 2. Average error, accuracy & cpu. time of NTRKKM, LTRKSVM, MIPKC & SVM
models

Tables 1 & 2 contain results for the NTRKKM, LTRKSVM, MIPKCps, MIPKCp, &
SVM model on the fluid flow pattern classification data. The models in comparison
generally report promising error rates. The NTRKKM model reports the smallest error
(0). This means that the testing data set was correctly classified (100% classification).

182

O.O. Oladunni and T.B. Trafalis

The error rate (0.0139) for the model with prior knowledge (LTRKSVM) performs in
the same capacity as the data driven mixed integer programming model (MIPKCps).
The computation time of both the NTRKKM and LTRKSVM model are comparable,
and both clearly outperform the other learning models. The SVM models reports the
next best computation time and the MIPKC model reports the worst time.
The fast solution of the LTRKSVM model is due the formulation of the problem
that is analyzed analytically in the primal (input) space, i.e., the dimension n of the
classification problem is equal to the number of attributes (variables) of the data set;
in our case n = 5. The fast solution of the NTRKKM model is due to the small number
of data in the test set (m = 46 test observations) and when possible, the avoidance of
iterative methods in computing its solution. The next best computation time to the
NTRKKM model is the one of the SVM model. Its fast solution is also due to the
small number of test observations (m = 46 test observations). Note that in this case
iterative methods are employed to find its solution. The MIPKCps, MIPKCp models
require more time because their formulation is based on integer and mixed integer
programming techniques which generally perform more computations in obtaining a
solution than its linear counterparts such as SVM, LTRKSVM and NTRKKM.

6 Conclusion
In this paper, a binary classification model called the nonlinear classification Tikhonov
regularization knowledge-based kernel machine (NTRKKM) is described. This model
problem is an unconstrained optimization problem for discriminating between two
disjoint sets. The proposed model can be considered as a least squares formulation of
Fung et al. [3] knowledge-based nonlinear kernel model. The model was applied to the
laminar/turbulent fluid flow data. Comparisons were made with the linear and nonlinear counterpart formulations, and best statistics were obtained by performing training
on the NTRKKM model. The NTRKKM model can be applied to determine the flow
patterns of fluid flow with different rheology. The fluid model in this paper is a
non-Newtonian fluid, pseudoplastic model (Bingham plastic) with two flow patterns
(laminar & turbulent flows). The same concept can be applied to a Newtonian or nonNewtonian fluid with three flow patterns (laminar, transition and turbulent).

References
1. Vapnik, V.: The Nature of Statistical Learning Theory. New-York: Springer-Verlag
(1995)
2. Fung, G., Mangasarian, O.L., Shavlik, J.W.: Knowledge-based Support Vector Machine
Classifiers. Neural Information Processing Systems 2002 (NIPS 2002), Vancouver, BC,
December 10-12, (2002)
3. Fung, G., Mangasarian, O.L., Shavlik, J.W.: Knowledge-based Nonlinear Kernel Classifiers, in Manfred Warmuth and Bernhard Scholkopf (eds), Proceedings Conference on
Learning Theory (COLT/Kernel 03) and Workshop on Kernel Machines, Washington,
D.C., August 24 - 27, (2003) 102-113

Regularized Knowledge-Based Kernel Machine

183

4. Oladunni, O.O., Trafalis, T.B., Papavassiliou, D.V.: Knowledge-based multiclass support
vector machines applied to vertical two-phase flow, ICCS 2006, Lecture notes in Computer
Science, (V.N. Alexandrov et al., eds.), Part I, LNCS 3991. Springer-Verlag, Berlin Heidelberg (2006) 188-195
5. Suykens, J.A.K. and Vandewalle, J.: Least Squares Support Vector Machine Classifiers.
Neural Processing Letters, 9, (1999b) 293-300
6. Pelckmans, K., Suykens, J.A.K., De Moor, B.: Morozov, Ivanov and Tikhonov Regularization based LS-SVMs. In Proceedings of the International Conference On Neural Information Processing (ICONIP 2004), Calcutta, India, Nov. 22-25, (2004)
7. Mangasarian, O.L.: Generalized Support Vector Machines. Advances in Large Margin
Classifiers, (A. Smola, P. Bartlett, B. Schölkopf, and D. Schuurmans, eds.), MIT Press,
Cambridge, MA (2000) ftp://ftp.cs.wisc.edu/pub/dmi/tech-reports/99-14.ps
8. Mangasarian, O.L.: Nonlinear Programming. Philadelphia, PA: SIAM (1994)
9. Bazaraa, M.S., Sherali, H.D., Shetty, C.M.: Nonlinear Programming – Theory and Algorithms. John Wiley & Sons, Inc. (1993)
10. Burges, C.J.C.: A Tutorial on Support Vector Machines for Pattern Classification. Data
Mining and Knowledge Discovery, 2(2): 121-167, (1998)
11. Cristianini, N., Shawe-Taylor, J.: Support Vector Machines and other Kernel-based Learning Methods; Cambridge University Press, (2000)
12. Tikhonov, A.N., Arsenin, V.Y.: Solution of Ill-Posed Problems. Winston, Washington
D.C., (1977)
13. Lewis, J. M., Lakshmivarahan, S., Dhall, S.: Dynamic Data Assimilation. Cambridge University Press, (2006)
14. Trafalis, T.B.; Oladunni, O.: Single Phase Fluid Flow Classification via Neural Networks
& Support Vector Machine. Intelligent Engineering Systems Through Artificial Neural
Networks, (C.H. Dagli, A.L. Buczak, D. L. Enke, M.J. Embrechts, and O. Ersoy, eds.),
ASME Press, 14, (2004) 427-432
15. Oladunni, O.; Trafalis, T.B.: Mixed-Integer Programming Kernel-Based Classification,
WSEAS Transactions on Computers, Issue 7, Vol. 4: 671-678, July (2005)
16. Oladunni, O.; Trafalis, T.B.: Single Phase Laminar & Turbulent Flow Classification in
Annulus via a Knowledge-Based Linear Model. Intelligent Engineering Systems Through
Artificial Neural Networks, (C.H. Dagli, A.L. Buczak, D. L. Enke, M.J. Embrechts, and O.
Ersoy, eds.), ASME Press, 16, (2006) 599-604
17. Bourgoyne, A.T. Jr., Chenevert, M.E., Millheim, K.K., Young, F.S. Jr.: Applied Drilling
Engineering, SPE Textbook Series Vol. 2, Richardson, TX, p. 155, (1991)
18. MATLAB User’s Guide.: The Math-Works, Inc., Natwick, MA 01760, 1994-2003.
http://www.mathworks.com
19. Gunn, S.T.: Support Vector Machine for Classification and Regression, Technical Report,
Department of Electronic and Computer Science, University of Southampton, (1998)
20. ILOG OPL STUDIO 3.7.: Language Manual, ILOG, S.A. Gentily, France and ILOG, Inc.,
Mountain View California, USA. http://www.ilog.com/products/oplstudio/, (2003)

