Concurrent Execution of Multiple NAS Parallel
Programs on a Cluster
Adam K.L. Wong and Andrzej M. Goscinski
School of Information Technology, Deakin University, Geelong, Vic 3216, Australia
{aklwong, ang}@deakin.edu.au

Abstract. Currently, coordinated scheduling of multiple parallel applications
across computers has been considered as the critical factor to achieve high
execution performance. We claim in this report that the performance and costs
of the execution of parallel applications could be improved if not only dedicated
clusters but also non-dedicated clusters were used and several parallel
applications were executed concurreontly. To support this claim we carried out
experimental study into the performance of multiple NAS parallel programs
executing concurrently on a non-dedicated cluster.

1 Introduction
Parallel processing has moved one step closer toward the computing mainstream by
exploiting specialized dedicated clusters and MPI. These dedicated clusters are built
using off-shelf elements (processors, memories and networks), which make them
cost-effective computer systems.
The cost to performance ratio could be improved even further if non-dedicated
clusters are used. These clusters are owned by many institutions, universities,
business, and industry. They are made of not necessarily the fastest computers and
networks, but still form a huge computational power that can be used to solve many
problems that require parallel computing for high performance. Cluster’s PCs in their
working environments are on average idle for much more than 50% of time [2, 4, 12].
Therefore, a cluster has the potential of concurrently supporting a mixture of parallel
and sequential applications of different users, which could lead to the improvement of
the execution performance [14, 15, 7]. We claim that these computer systems can also
be used cost-effectively to concurrently execute several parallel applications
submitted by many competing users.
Parallel applications can share the computational resources of a cluster in two
dimensions: space and time. Space-sharing is usually done by static allocation of
processes to the available computers. Therefore, processes of different parallel
applications can be mapped into different sets of computers of the cluster and the
execution of those processes would not interfere with each other. On the other hand,
time-sharing can be achieved if processes of parallel applications are mapped into the
same computer and the local scheduler schedules these processes to share the CPU
among them.
V.S. Sunderam et al. (Eds.): ICCS 2005, LNCS 3514, pp. 435 – 442, 2005.
© Springer-Verlag Berlin Heidelberg 2005

436

A.K.L. Wong and A.M. Goscinski

The time-sharing approach can provide good response time and good throughput
for applications in a multi-user and multi-programs environment. However, it has
been considered that uncoordinated scheduling of processes from different
applications would seriously hurt the performance of a cluster [1, 3, 5, 17, 13]: the
execution overhead can be up to over 15 times of the program execution time. To our
knowledge, the only existing work which has shown an opposite result is [11].
Therefore, we believe that more experimental studies of this problem will provide not
only a better understanding of the problem, lead industry, business and research
institutions toward parallel processing on their existing clusters, but also form a
background for the development of global scheduling facilities for computer clusters.
The aim of this paper is to confirm our claim by showing the results of our study
into the performance of multiple parallel applications executing concurrently on a
cluster. We also want to demonstrate that the results achieved by other researchers
showing that uncoordinated scheduling of parallel applications would seriously hurt
the performance of a cluster are unsubstantiated. For this purpose experiments were
carried out using the well known and widely used NAS Parallel Benchmarks [9].
The paper is organized as follows. Section 2 presents the related work. Section 3
describes attributes of parallel applications required to carry out the proposed
experiments on a cluster, and introduces the selected NAS benchmarks. Section 4
details the experiments carried out. Section 5 reports on the achieved results and
presents the analysis of these results. Finally, the conclusion is presented in Section 6.

2 Related Work
Parallel computer systems such as the Massively Parallel Processors (MPPs) have a
low communication overhead and therefore the effect of uncoordinated scheduling of
processes from parallel applications executing concurrently on the systems is
significant. Coordinated scheduling such as gang-scheduling [5] is normally used to
alleviate the problem.
Time-sharing is intrinsically supported in a computer cluster via local scheduling.
That means the local scheduler is responsible for time sharing of the CPU among all
the processes which have been allocated to that computer. Processes from a parallel
application can be placed into some or all of the computers in the cluster depending
on the required parallelism. However, processes belonging to the same parallel
application would not be guaranteed to execute at the same time across the computers
in the cluster. Previous studies carried out using stimulation [1, 3, 17] have found that
if the parallel application is communication intensive, uncoordinated scheduling of
processes would lead to a great loss of performance because a process stalls when it
communicates with a non-scheduled process.
[13, 11] present the results of co-scheduling of multiple parallel applications on a
cluster using local scheduling, which are quite different. [13] shows that coscheduling of parallel applications on a cluster worsens their execution performance.
However, that result is difficult to assess as the experiment is not clearly described
and applications used in the experiment are not defined. [11] shows that local
scheduling can out-perform gang-scheduling of parallel applications executing on a
Beowulf cluster with a slow network (100Mbps Ethernet).

Concurrent Execution of Multiple NAS Parallel Programs on a Cluster

437

3 The NAS Parallel Benchmarks (NPB)
We have selected to use the NAS Parallel Benchmarks (NPB) for this study because
they have been widely used to objectively measure and compare the performance of
parallel computer systems. In particular, the NAS Parallel Benchmarks 2.4 [16] can
be used as parallel applications and run on non-dedicated clusters. Each NPB 2.x
provides source codes written with MPI. Furthermore, these benchmarks can be
coarsely divided into two major categories, computation-bound and communicationbound, which is important when carrying out experiments on commonly used clusters,
with slow, 100Mbps, networks.
The NPB suite is a set of eight programs, which were derived from computational
fluid dynamics (CFD) codes [9]. Each of these programs focuses on some important
aspects of highly parallel supercomputing for aerophysics applications. There are two
groups of these applications: five “kernels” and three “simulated computational fluid
dynamics (CFD) applications”. The kernels are relatively compact problems, easy to
implement. They mimic the computational core of different numerical methods used
by CFD applications – each of them emphasizes a particular type of numerical
computation. They provide insight as to the general levels of performance that can be
expected on these specific types of numerical computations. The simulated CFD
applications are more difficult to implement – they indicate the types of actual data
movement and computation required in state-of-the-art CFD application codes.
3.1 Attributes of the Benchmark Programs Affecting the Scheduling Behaviour
The behavior and scheduling study requirements led us to the specification of
benchmark attributes that must be present because they influence the execution
performance of a parallel application. These attributes form a basis of the selection of
benchmarks for our experiments. They are as follows.
• Computation attributes: In general, the problem size of a parallel program is
directly proportional to its execution time. Each of the programs of the NPB suite
comes with several problem sizes (classes): A, B, C, W(orkstation) and S(ample).
Excluding classes W and S, class A is the smallest whereas class C is the largest.
• Communication attributes: Different parallel program has different
communication features. The communication features of the NAS programs can
be considered in two aspects: communication volume and communication
pattern. In respect to the communication volume, the programs can broadly be
classified into three categories: low, medium and high. In respect to the
communication pattern, two forms exist: point-to-point and collective.
• Memory attributes: The size of main memory of a program during execution
affects the scheduling behaviour, in particular could lead to memory swapping.
The memory scheduling behavior of the NAS programs depends on the program
size, characterized by the program class.
• Topology attributes: The software topology of processes of a parallel program
defines the size (number of processes) and the structure (the connections of
processes) of the program. The NAS programs use various software topologies.
FT, MG, CG, LU and IS run well with a power-of-2 number of processes; SP
and BT run well with a square number of processes; and EP runs with any
number of process.

438

A.K.L. Wong and A.M. Goscinski
Table 1. NAS programs with communication specific attributes
Program Name
EP
LU
BT
MG

Communication Volume
Negligible
Low
Medium
High

Communication Pattern
Negligible
Point-to-Point >>> Collective
Point-to-Point >> Collective
Point-to-Point > Collective

3.2 Selected NAS Benchmarks
To evaluate the impact of concurrent execution of multiple parallel applications on
their performance, we carried out an analysis of the NAS programs to identify those
that posses the attributes addressed in Section 3.1. We have found that four programs,
EP, LU, BT and MG, of the NBS suite, represent a broad range of communication
patterns of parallel applications that can commonly be found in real world. The
communication features of these programs are summarized based on [6, 9, 10] and
shown in Table 1.
We have chosen a problem size of class B for the programs listed above to make
sure that (i) the execution time is long enough for the scheduling behaviour to be
observed (only the number of iterations of execution in some programs have been
altered), and (ii) memory swapping would not happen.
We could not satisfy fully the topology requirement. The reason is as follows. The
MG and LU belong to the group of those programs that perform best (require) a
power-of-2 number of processes, and EP can also be executed effectively with the
same number of processes, BT performs best when it runs with a square number of
processes, i.e., 1, 4, 9, 16. We assumed that the loss of performance for executing BT
with eight (processes) rather than nine (processes) will not distort the experiment
outcomes, and carried out the experiments using a power-of-2 number of processes
for each selected NAS program. Thus, the four programs, two kernel (EP and MG)
and two simulated (LU and BT) applications, which posses the attributes to carry out
our study, are selected.

4 Experiments
All of the scheduling experiments were carried out on a cluster which consists of 16
Pentium III computers, each with 383 Mbytes of main memory. The computers are
connected together by a 100Mbps Fast Ethernet network. Each computer runs the Red
Hat Linux operating system with the parallel programming support of MPI.
We decided to use a two level scheduling system, where the upper level is
responsible for allocation of processes of each parallel application to cluster
computers and the lower level schedules local parallel processes (of one or more than
one application) running on each local computer of the cluster. MPI provides initial
allocation of parallel processes to cluster computers. On an individual computer these
processes are scheduled by the Red Hat Linux operating system scheduler.
An MPI application usually can be executed by first constructing a network
topology, which specifies in the topology configuration file (TCF) the number and the
identity of the computers used for the execution. Such a network topology can then be
booted up in the cluster, which basically starts a MPI daemon process on each of the

Concurrent Execution of Multiple NAS Parallel Programs on a Cluster

439

computers specified in the TCF. The MPI daemons are responsible for handling
communications among processes of a parallel application. We used in our
experiment the LAM/MPI implementation [8] as it is one of the most popular
implementations of the MPI specification, and its version is LAM/MPI-6.5.9.
First, we measured the execution times of the four selected NAS parallel programs
(EP, LU, MG and BT) individually. Taking into account the topology attributes
specified in [9]: a power-of-2 of processes for LU and MG, a square number of
processes for BT and any number of processes for EP, we selected a power-of-2 of
processes as the standard topology for all the four programs executing in this
experiment. Since there are 16 computers in our cluster, each of the programs were
run on 2, 4, 8 and 16 computers; all of the programs were compiled for using 16
processes. Then, we measured i) the execution time for each of the selected NAS
benchmarks when multiple copies of itself were executed concurrently on a cluster of
16 computers, and ii) the execution time for each of the selected NAS benchmarks
when one or multiple copies of EP were executed concurrently with itself on the same
cluster. The influence of the local scheduling was then observed and measured.

5 Results and Analysis
Speedup of NAS Programs. Our experimental study was carried out using the
programs of Class B of NPB.

20000
18000

Execution Time (sec.)

16000
14000
12000

EP
LU

10000

BT

8000

MG

6000
4000
2000
0
2

4

8

16

Numbe r of Computers

Fig. 1. Execution Time of NAS programs against different number of computers

Because the number of iterations of execution in the selected NAS parallel
programs were increased, the execution time of each of parallel programs: EP, LU,
BT and MG, against different number of computers: 2, 4, 8 and 16 of our cluster was
measured. The result is presented in Figure 1.
The results we achieved are consistent with the benchmarking results listed in [9].
Besides, our results indicate that using a cluster of 16 Pentium III computers to
execute each of the NAS parallel programs requires around 40 minutes of time. We

440

A.K.L. Wong and A.M. Goscinski

believe that this time duration for an execution of a parallel program not only is
realistic but also long enough for observing the scheduling behaviour when multiple
copies of the programs are executed concurrently on a cluster.
Concurrent Execution of the NAS Benchmark Programs. The purpose of the
experiment was to determine the effect of co-scheduling of multiple NAS programs
on a cluster with particular attention paid to the execution performance of the
programs. Table 2 shows the results of co-scheduling of two and three identical NAS
parallel programs on our 16-computers cluster.
Table 2. Multi-program scheduling of parallel programs from the NBS

Program

Sequential
Execution Time
(SET) (sec.)

Concurrent
Execution Time
(CET) (sec.)

Number of copies

Number of copies

Slowdown = CET/SET
Number of copies

1

2

3

1

2

3

1

2

3

EP

2240

4480

6720

2240

4497

6711

1.00

1.00

1.00

LU

2522

5044

7566

2522

4942

7478

1.00

0.98

0.99

BT

2859

5718

8577

2859

5046

7561

1.00

0.88

0.88

MG

2279

4558

6837

2279

4550

6769

1.00

1.00

0.99

Expecting to observe some worse performance, we used the term slowdown to
refer to the slowing down of a parallel program when multiple copies of the same
program are running concurrently on the same system. It is calculated by dividing the
time (CET) of concurrent execution of multiple copies of a program on a multiprogramming system by the time (SET) of sequential execution of multiple copies of
the program run on the same system. Therefore, a slowdown value of greater than 1
implies a slowing down of the execution, and worse performance.
Table 3 shows the results of co-scheduling of the NAS parallel programs: LU, BT
and MG executing with one, two and three copies of EP. The reason for that is EP is a
computation-bound parallel application while all others are communication-bound
parallel applications. Mixing computation- and communication-bound applications
together can generate a higher degree of randomness for the processes to
communicate and synchronize each other.
As the number of copies of a NAS parallel program increases, the total memory
required for the executions increases significantly, especially in the data-spaces. In
both cases, the memory requirement for programs to execute is carefully controlled to
make sure that no memory swapping would occur. Our results show that no
slowdown is greater than 1 in all of the above scheduling cases which is different
from the results presented in the majority of the current publications.
The results indicate that both computation-bound and communication-bound
parallel applications are not sensitive to co-scheduling on a cluster with a slow
network. From table 2, it can also be seen that the slowdown obtained in LU, BT and
MG is slightly less than 1 which can be explained by the low CPU utilization of the
programs. When multiple copies of those programs are scheduled together, a blocked
process can be scheduled to keep the CPU as busy as possible.

Concurrent Execution of Multiple NAS Parallel Programs on a Cluster

441

Table 3. Mixed multi-program scheduling of parallel programs from the NBS
Sequential
Execution Time
[SET] (sec.)

Program

Concurrent
Execution Time
[CET] (sec.)

Number of copies of EP

Slowdown = CET/SET

Number of copies of EP

Number of copies of EP

1

2

3

1

2

3

1

2

3

LU

4762

7002

9242

4719

6951

9211

0.99

0.99

1.00

BT

5099

7339

9579

4752

6964

9188

0.93

0.95

0.96

MG

4519

6759

8999

4475

6705

8962

0.99

0.99

1.00

6 Conclusions
In this paper we have presented the results of the experimental study into the
performance of multiple parallel applications executing concurrently on a cluster. The
aim was to confirm our claim that the performance of the execution of parallel
applications could be improved when several parallel applications are executed
concurrently, and to show that the claim made by other researchers that uncoordinated
scheduling of parallel applications would seriously hurt the performance of a cluster
is unsubstantiated.
Contrary to the results presented in most of the current literature, we have found
that even if a parallel application is communication intensive, there is no performance
loss of the parallel application due to uncoordinated communications and
synchronizations of processes. Our study of the concurrent execution of multiple
parallel applications on a cluster does not confirm the simulation results reported in
[1, 3, 17], which recommend synchronized scheduling of multiple parallel
applications such as gang-scheduling. The results of our experimental study using real
parallel applications are different from the result shown in [13]; the only result
presented in the current literature which is inline with us is in [11].
We have demonstrated that concurrent execution of multiple parallel applications
on a cluster did not make the execution performance of a parallel application worse.
The execution performance was improved. It seems that this cost-effective scheduling
scheme is particularly useful for computer cluster, especially with a slow network.

References
1. R.H. Arpaci, A.C. Dusseau, A.M. Vahdat, L.T. Liu, T.E. Anderson and D.A. Patterson.
The Interaction of Parallel and Sequential Workloads on a Network of Workstations. In
Proceedings of 1995 ACM Joint International Conference on Measurement and Modeling
of Computing Systems, pages 267-278, May 1995.
2. A. Acharya, G. Edjlali and J. Saltz. The Utility of Exploiting Idle Workstations for Parallel
Computation. In Proceedings of 1997 ACM Sigmetrics International Conference on
Measurement and Modeling of Computer Systems, pages 225-236, May 1997.

442

A.K.L. Wong and A.M. Goscinski

3. C. Anglano. A Comparative Evaluation of Implicit Coscheduling Strategies for Networks
of Workstations. In Proceedings of 9th International Symposium on High Performance
Distributed Computing (HPDC9), pages 221-228, August 2000.
4. W. Becker. Dynamic Balancing Complex Workload in Workstation Networks -- Challenge,
Concepts and Experience. In Proceedings High Performance Computing and Networking
(HPCN) Europe Lecture Notes on Computer Science (LNCS), pages 407-412, 1995.
5. D.G. Feitelson and L. Rudolph. Gang Scheduling Performance Benefits for Fine-Grained
Synchronization. Journal of Parallel and Distributed Computing, 16(4):306-318, 1992.
6. A. Faraj and X. Yuan. Communication Characteristics in the NAS Parallel Benchmarks. In
Proceedings of the 14th IASTED International Conference on Parallel and Distributed
Computing and Systems (PDCS 2002), Nov. 2002.
7. A. M. Goscinski and A. K. L. Wong. Performance Evaluation of the Concurrent Execution
of NAS Parallel Benchmarks with BYTE Sequential Benchmarks on a Cluster. Paper
submitted to the 11th International Conference on Parallel and Distributed Systems,
ICPADS05.
8. The LAM/MPI Homepage. URL: http://www.lam-mpi.org, lasted access: June 2004.
9. NAS Parallel Benchmarks. URL: http://www.nas.nasa.gov/Software/NPB/, last accessed:
Nov. 2004.
10. J. Subhlok, S. Venkataramaiah and A. Singh. Characterizing NAS Benchmark
Performance on Shared Heterogeneous Networks. In 11th International Heterogeneous
Computing Workshop, April 2002.
11. P. Strazdins and J. Uhlmann. Local Scheduling out-performs Gang Scheduling on a Beowulf
Cluster. Technical Report TR-CS-04-01, The Australian National University, 2004.
12. F. Tandiary, S.C. Kothari, A. Dixit and E. W. Anderson. Batrun: Utilizing Idle
Workstations for Large-scale Computing. IEEE Parallel and Distributed Technology,
4(2):41-48, 1996.
13. F.C. Wong, A.C. Arpaci-Dusseau and D.E. Culler. Building MPI for Multi-Programming
Systems using Implicit Information. In Proceedings of the 6th European PVM/MPI User's
Group Meeting, pages 215-222, 1999.
14. A. K. L. Wong and A. M. Goscinski. Scheduling of a Parallel Computation-Bound
Application and Sequential Applications Executing Concurrently on a Cluster – A Case
Study. In Proceedings of the 2rd International Symposium on Parallel and Distributed
Processing and Applications (ISPA04), Dec. 2004.
15. A.K.L. Wong and A.M. Goscinski. The Performance of a Parallel Communication-Bound
Application and Sequential Applications Executing Concurrently on a Cluster – A Case
Study. In Proceeding of the 12th International Conference on Advanced Computing and
Communication (ADCOM-2004), Dec. 2004.
16. Rob F. Van der Wijngaart. The NAS Parallel Benchmarks 2.4 NAS Technical Report
NAS-95-020, NASA Ames Research Center, Moffett Field, CA, 1995.
17. B.B. Zhou, X. Qu and R.P. Brent. Effective Scheduling in a Mixed Parallel and Sequential
Computing Environment. In Proceedings of the 6th Euromicro Workshop of Parallel and
Distributed Processing, pages 32-37, Jan. 1998.

