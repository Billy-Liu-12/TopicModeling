Procedia Computer Science
Volume 51, 2015, Pages 2683–2687
ICCS 2015 International Conference On Computational Science

Scalable Multilevel Support Vector Machines
Talayeh Razzaghi1 and Ilya Safro1
Clemson University, Clemson, SC
(trazzag,isafro)@clemson.edu

Abstract
Solving optimization models (including parameters ﬁtting) for support vector machines on largescale training data is often an expensive computational task. This paper proposes a multilevel
algorithmic framework that scales eﬃciently to very large data sets. Instead of solving the
whole training set in one optimization process, the support vectors are obtained and gradually
reﬁned at multiple levels of coarseness of the data. Our multilevel framework substantially
improves the computational time without loosing the quality of classiﬁers. The algorithms
are demonstrated for both regular and weighted support vector machines for balanced and
imbalanced classiﬁcation problems. Quality improvement on several imbalanced data sets has
been observed.
Keywords: classiﬁcation, scalable support vector machines, multilevel techniques

1

Introduction

Training nonlinear support vector machines (SVM) is often a time consuming task when the
data is big. This problem becomes extremely sensitive when the model selection techniques are
applied as both quality, and scalability of SVM depend on the employed optimization solvers.
In this paper, we focus on SVMs and weighted SVMs (WSVM) for balanced, and imbalanced
data, respectively, that are formulated as the convex quadratic programming (QP). Usually,
the complexity required to solve such SVMs is between O(n2 ) and O(n3 ). We propose a novel
method for eﬃcient solution of (W)SVM. In the heart of this method lies a multilevel algorithmic framework (MF) inspired by the multiscale optimization strategies [1]. The main objective
of MF is to construct a hierarchy of problems (coarsening), each approximating the original
problem but with fewer degrees of freedom. This is achieved by introducing a chain of successive projections of the problem domain into lower-dimensional or smaller-size domains and
solving the problem in them using local processing (uncoarsening). The MF combines solutions
achieved by the local processing at diﬀerent levels of coarseness into one global solution. Such
frameworks have several key advantages such as a linear complexity, relatively easy parallelization, and adaptivity to hybrid methods with other algorithms. These frameworks are extremely
successful in various practical machine learning and data mining tasks such as clustering and
dimensionality reduction.
Selection and peer-review under responsibility of the Scientiﬁc Programme Committee of ICCS 2015
c The Authors. Published by Elsevier B.V.

doi:10.1016/j.procs.2015.05.381

2683

Scalable Multilevel Support Vector Machines

Razzaghi and Safro

Problem Deﬁnition. Let a set of labeled data points be represented by a set J = {(xi , yi )}li=1 ,
where (xi , yi ) ∈ Rn+1 , and l and n are the numbers of data points and features, respectively.
Each xi is a data point with n features, and a class label yi ∈ {−1, 1}. An optimal classiﬁer is
determined by the parameters w and b through solving the convex problem:
min

1
w
2

2

l

+C

ξi

s.t. ∀i = 1, . . . , l

yi (wT φ(xi ) + b) ≥ 1 − ξi and ξi ≥ 0

(1)

i=1

where φ maps training instances xi into a higher dimensional space, φ : Rn → Rm (m ≥ n). The
term slack variables ξi (i ∈ {1, . . . , l}) in the objective function is used to penalize misclassiﬁed
points. This approach is also known as soft margin SVM. The magnitude of penalization
is controlled by the parameter C. The WSVM (an extension of the SVM for imbalanced
classes) assigns diﬀerent weights to each data sample based on its importance, i.e., the objective
|C+ |

|C− |

of (1) is substituted with 12 w 2 + C + {i|yi =+1} ξi + C − {j|yj =−1} ξj , where subsets of J
related to the “majority” and “minority” classes are denoted by C− , and C+ , respectively, i.e.,
J = C+ ∪ C− . The importance factors C − , and C + are associated with majority and minority
classes C− and C+ , respectively. In our solvers we employ the Gaussian kernel, and an adapted
nested uniform design model selection algorithm [3] for tuning C, C + , and C − .

2

Multilevel Support Vector Machines

The proposed MF (see Figure 1) includes three phases: gradual training set coarsening, coarsest support vectors’ learning, and gradual support vectors’ reﬁnement (uncoarsening). Separate
coarsening hierarchies are created for C+ , and C− independently. Each next-coarser level contains a subset of points of the corresponding ﬁne level. These subsets are selected using the
approximated k-nearest neighbor graphs (AkNN). In contrast to the coarsening used in multilevel dimensionality reduction method [6], we found that selecting an independent set only
does not lead to the best computational results. Instead, making the coarsening less aggressive
makes the framework much more robust to the changes in the parameters. After the coarsest
level is solved exactly, we gradually reﬁne the support vectors and the corresponding classiﬁers.
The Coarsening Phase. The coarsening algorithms are the same for both C+ , and C− , so
we provide only one of them. Given a class of data points C, the coarsening begins with a
construction of AkNN G = (V, E), where V = C, and E are the edges of AkNN. The goal is to
select a set of points Vˆ for the next-coarser problem, where |Vˆ | ≥ Q|V | (typically Q = 0.5). The
second requirement for Vˆ is that it has to be a dominating set of V . The coarsening for class C
is presented in Algorithm 1. It consists of several iterations of independent set of V selections
that are complementary to already chosen sets. We begin with choosing a random independent
set (l. 2) using greedy algorithm. It is eliminated from the graph, and the next independent set
is added to Vˆ (l. 5-9). For imbalanced cases, when WSVM is used, we avoid of creating very
small coarse problems for C+ . Instead, already very small class is continuously replicated across
the rest of the hierarchy if C− still requires coarsening. We note that this method of coarsening
will reduce the degree of skewness in the data and make the data approximately balanced at
the coarsest level. The multilevel framework recursively calls the coarsening process until it
creates a hierarchy of r coarse representations {Ji }ri=0 of J . At each level of this hierarchy, the
corresponding AkNNs’ {Gi = (Vi , Ei )}ri=0 are saved for future use at the uncoarsening phase.
The corresponding data and labels at level i is denoted by (Xi , Yi ) ∈ Rk×(n+1) , where |Xi | = k.
2684

Scalable Multilevel Support Vector Machines

Razzaghi and Safro

Algorithm 1 The Coarsening
1:
2:
3:
4:
5:
6:
7:
8:
9:
10:
11:
12:

Input: G = (V, E) for class C
Vˆ ← select maximal independent set in G
ˆ ← V \ Vˆ
U
while |Vˆ | < Q · |V | do
ˆ = ∅ do
while U
ˆ; U
ˆ ←U
ˆ \ {i}
randomly pick i ∈ U
ˆ}
ˆ ←U
ˆ \ {neighbors of i in U
U
Vˆ ← Vˆ ∪ {i}
end while
ˆ ← V \ Vˆ
U
end while
return Vˆ

Figure 1: The MF for (W)SVM.

The Coarsest Problem. At the coarsest level r, when |Jr | << J , we can apply an exact
algorithm for training the coarsest classiﬁer. Processing the coarsest level includes an application of the UD [3] model selection to get high-quality classiﬁers on the diﬃcult data sets.
The Reﬁnement Phase. Given the solution of coarse level i + 1 (the set of support vectors
Si+1 , and parameters Ci+1 , and γi+1 ), the primary goal of the reﬁnement is to update and
optimize this solution for the current ﬁne level i. Unlike many other multilevel algorithms, in
which the inherited coarse solution contains projected variables only, we initially inherit not
only the coarse support vectors but also parameters for model selection. This is because the
model selection is an extremely time-consuming component of (W)SVM, and can be prohibitive
at ﬁne levels. However, at the coarse levels, when the problem is much smaller than the original,
we can apply much heavier methods for model selection without any loss in the total complexity.
Algorithm 2 The Reﬁnement at level i
1:
2:
3:
4:
5:
6:
7:
8:
9:
10:
11:

Input: Ji , Si+1 , Ci+1 , γi+1
if i is the coarsest level then
Calculate the best (Ci , γi ) using UD
Si ← Apply SVM on Xi
end if
Calculate nearest neighbors Ni for support
vectors Si+1 from the existing AkNN Gi
(i)
datatrain ← S(i+1) ∪ Ni

12:

if |datatrain | < Qdt then
C O ← Ci+1 ; γ O ← γi+1
Run UD using the center (C O , γ O )
else

18:
19:

(i)

13:
14:
15:
16:
17:

20:
21:

Ci ← Ci+1 ; γi ← γi+1
end if
(i)
if |datatrain | ≥ Qdt then
(i)
Cluster datatrain into K clusters
∀k ∈ K ﬁnd P nearest opposite-class
clusters
Si ← Apply SVM on pairs of nearest
clusters only
else
(i)
Si ← Apply SVM directly on datatrain
end if
Return Si , Ci , γi

The reﬁnement is presented in Algorithm 2. The coarsest level is solved exactly and reinforced by the model selection (l. 2-5). If i is one of the intermediate levels, we build the set of
(i)
training data datatrain by inheriting the coarse support vectors Si+1 and adding to them some
of their approximated nearest neighbors at level i (l. 6-7) (in our experiments, usually not more
2685

Scalable Multilevel Support Vector Machines

Razzaghi and Safro

Table 1: Benchmark datasets and computational time (in sec.) of multilevel, and regular SVM.

Dataset

rimb

nf

|J |

|C+ |

|C− |

Letter26
Ringnorm
Twonorm
Buzz
Clean (Musk)
Advertisement
ISOLET
cod-rna
Nursery
EEG Eye State
Hypothyroid

0.96
0.50
0.50
0.80
0.85
0.86
0.96
0.67
0.67
0.55
0.94

16
20
20
77
166
1558
617
8
8
14
21

20000
7400
7400
140707
6598
3279
6238
59535
12960
14980
3919

734
3664
3703
27775
1017
459
240
19845
4320
6723
240

19266
3736
3697
112932
5581
2820
5998
39690
8640
8257
3679

Multilevel
Yes
No
ModelSelection
ModelSelection
Yes
No
Yes
No
45
112
333
27
4
21
42
12
4
21
45
12
2329
2400
70452
20386
30
92
167
55
196
104
412
41
69
373
1367
297
172
293
1611
208
63
37
519
42
51
32
447
33
3
3
5
1

(i)

than 5). If the size of datatrain is still small enough (relatively to the existing computational
resources, and the initial size of the data) for applying model selection, and solving SVM on
(i)
the whole datatrain , then we use coarse parameters Ci+1 , and γi+1 as initializers for the current
level, and retrain (l. 9-10,19). Otherwise, the coarse Ci+1 , and γi+1 are inherited in Ci , and γi
(i)
(l. 12). Then, being large for direct application of the SVM, datatrain is clustered into several
clusters, and pairs of nearest opposite clusters are retrained, and contribute their solutions to
Si (l. 15-17). We note that cluster-based retraining can be done in parallel, as diﬀerent pairs of
clusters are independent. Moreover, the total complexity of the algorithm does not suﬀer from
reinforcing the cluster-based retraining with model selection.

3

Computational Results

Discussion and full results of our work can be found in [5]. The multilevel (W)SVM are evaluated on binary classiﬁcation benchmarks from UCI repository. Single SVM and WSVM models
are solved using LIBSVM-3.18 [2], and the AkNN graphs are costructed using FLANN library
[4]. Multilevel frameworks are implemented in MATLAB 2012a, and evaluated on Linux. The
results for multilevel (W)SVM (objectives and running times) should only be considered qualitatively and can certainly be further improved by a more advanced implementation. The
implementation is available at http://www.cs.clemson.edu/~isafro/software.html. Evaluation of the proposed algorithm is done using accuracy (ACC), sensitivity (SN), speciﬁcity
(SP), and the geometric mean of SN and SP (G-mean). The details of the datasets are described in left part of Table 1. We normalize all data prior to classiﬁcation in order to get zero
mean and unitary standard deviation. We perform a 9- and 5-point run design for the ﬁrst and
second stages of the nested UD.
The performance measures of the multilevel (W)SVM (Table 2, left part of the table) are
compared with the regular (one-level) (W)SVM (Table 2, right part of the table). Since several components in the coarsening, and uncoarsening schemes are randomized algorithms, the
average numbers over 100 random runs are reported for each data set. We do not report the
standard deviations because in all experiments they are negligibly small. Bold fonts emphasize
the best G-mean results. Table 2 demonstrates that the quality of multilevel SVM algorithms
is very similar to the quality of the single-level SVM. However, we observed that multilevel
WSVM improves the single-level WSVM for some datasets.
2686

Scalable Multilevel Support Vector Machines

Razzaghi and Safro

The main achievement of the proposed multilevel scheme is its computational time (see
Table 1) that substantially improves that of the single-level (W)SVM when the model selection
techniques must be applied on diﬃcult data sets. We note that for most of the datasets in the
benchmark, using model selection was extremely important for obtaining high-quality results.
Moreover, the observed improvement is not complete, because (similar to many multilevel and
multigrid algorithms) the reﬁnement phase can be easily parallelized at levels where the training
by clusters is employed. In addition, the proposed methodology is very successful for large
imbalanced classiﬁcation problems since it can reduce the degree of skewness in the data and
make the data approximately balanced at the coarse levels.

WSVM

SVM

Table 2: Performance measures for multilevel and regular SVMs and WSVMs. Each cell contains an average over 100 executions. Column ’Depth’ shows the number of levels.
Dataset
Letter26
Ringnorm
Buzz
Clean (Musk)
Advertisement
ISOLET
cod-rna
Twonorm
Nursery
EEG Eye State
Hypothyroid
Letter26
Ringnorm
Buzz
Clean (Musk)
Advertisement
ISOLET
cod-rna
Twonorm
Nursery
EEG Eye State
Hypothyroid

ACC
0.98
0.98
0.94
1.00
0.94
0.99
0.95
0.97
1.00
0.83
0.98
0.99
0.98
0.94
1.00
0.94
0.99
0.94
0.97
1.00
0.87
0.98

SN
0.99
0.98
0.96
1.00
0.97
1.00
0.93
0.98
0.99
0.82
0.98
0.99
0.97
0.96
1.00
0.968
1.00
0.97
0.98
0.99
0.89
0.98

Multilevel
SP
G-mean
0.95
0.97
0.99
0.98
0.85
0.90
0.99
0.99
0.79
0.87
0.83
0.92
0.97
0.95
0.97
0.97
0.98
0.99
0.88
0.85
0.74
0.85
0.96
0.99
0.99
0.98
0.87
0.91
0.99
0.99
0.80
0.88
0.85
0.92
0.95
0.96
0.97
0.97
0.98
0.99
0.86
0.88
0.75
0.86

Depth
8
6
14
5
7
11
9
6
10
6
4
8
6
14
5
7
11
9
6
10
6
4

ACC
1.00
0.98
0.97
1.00
0.92
0.99
0.96
0.98
1.00
0.88
0.99
1.00
0.98
0.96
1.00
0.92
0.99
0.96
0.98
1.00
0.88
0.99

Not Multilevel
SN
SP
G-mean
1.00
0.97
0.98
0.99
0.98
0.98
0.99
0.81
0.89
1.00
0.98
0.99
0.99
0.45
0.67
1.00
0.85
0.92
0.96
0.95
0.96
0.98
0.99
0.98
1.00
1.00
1.00
0.90
0.86
0.88
1.00
0.71
0.83
1.00
0.97
0.99
0.99
0.98
0.98
0.99
0.81
0.89
1.00
0.98
0.99
0.99
0.45
0.67
1.00
0.85
0.92
0.96
0.96
0.96
0.98
0.99
0.98
1.00
1.00
1.00
0.90
0.86
0.88
1.00
0.75
0.86

References
[1] A. Brandt and D. Ron. Chapter 1 : Multigrid solvers and multilevel optimization strategies. In
J. Cong and J. R. Shinnerl, editors, Multilevel Optimization and VLSICAD. Kluwer, 2003.
[2] Chih-Chung Chang and Chih-Jen Lin. Libsvm: a library for support vector machines. ACM
Transactions on Intelligent Systems and Technology (TIST), 2(3):27, 2011.
[3] C.M. Huang, Y.J. Lee, D.K.J. Lin, and S.Y. Huang. Model selection for support vector machines
via uniform design. Computational Statistics & Data Analysis, 52(1):335–346, 2007.
[4] Marius Muja and David G. Lowe. Fast approximate nearest neighbors with automatic algorithm
conﬁguration. In International Conference on Computer Vision Theory and Application VISSAPP’09), pages 331–340. INSTICC Press, 2009.
[5] T. Razzaghi and I. Safro. Fast multilevel support vector machines. ArXiv, abs/1410.3348, 2014.
[6] S. Sakellaridi, Haw ren Fang, and Y. Saad. Graph-based multilevel dimensionality reduction with
applications to eigenfaces and latent semantic indexing. In Machine Learning and Applications,
2008. ICMLA ’08. Seventh International Conference on, pages 194–200, Dec 2008.

2687

