Linearity Analysis for Automatic Diﬀerentiation
Michelle Mills Strout1 and Paul Hovland2
1
2

Colorado State University, Fort Collins, CO 80523
Argonne National Laboratory, Argonne, IL 60439

Abstract. Linearity analysis determines which variables depend on
which other variables and whether the dependence is linear or nonlinear.
One of the many applications of this analysis is determining whether
a loop involves only linear loop-carried dependences and therefore the
adjoint of the loop may be reversed and fused with the computation of
the original function. This paper speciﬁes the data-ﬂow equations that
compute linearity analysis. In addition, the paper describes using linearity analysis with array dependence analysis to determine whether a
loop-carried dependence is linear or nonlinear.

1

Introduction

Many automatic diﬀerentiation and optimization algorithms can beneﬁt from
linearity analysis. Linearity analysis determines whether the dependence between
two variables is nonexistent, linear, or nonlinear. A variable is said to be linearly
dependent on another if all of the dependences along all of the dependence chains
are induced by linear or aﬃne functions (addition, subtraction, or multiplication
by a constant). A variable is nonlinearly dependent on another if a nonlinear
operator (multiplication, division, transcendental functions, etc.) induces any of
the dependences along any of the dependence chains.
One application of linearity analysis is the optimization of derivative code
generated by automatic diﬀerentiation (AD) via the reverse mode. AD is a technique for transforming a subprogram that computes some function into one that
computes the function and its derivatives. AD works by combining rules for
diﬀerentiating the intrinsic functions and elementary operators of a given programming language with the chain rule of diﬀerential calculus. One strategy,
referred to as the forward mode, is to compute partials as the intrinsic functions
are evaluated and to combine the partials as they are computed. For example,
forward mode AD transforms the loop in Figure 1 into the code in Figure 2. Reverse mode AD results in less computation in the derivative code if the number
of independent variables is much larger than the number of dependent variables.
Figures 3 shows the adjoint code after applying the reverse mode. Notice that
the temporary variable a must be promoted to an array to store results needed
in the adjoint computation.
This work was supported by the Mathematical, Information, and Computational Sciences Division subprogram of the Oﬃce of Advanced Scientiﬁc Computing Research,
U.S. Department of Energy under Contract W-31-109-Eng-38.
V.N. Alexandrov et al. (Eds.): ICCS 2006, Part IV, LNCS 3994, pp. 574–581, 2006.
c Springer-Verlag Berlin Heidelberg 2006

Linearity Analysis for Automatic Diﬀerentiation

a =
f =
for
a
t
f
end

d_a = 0.0
a = 0.0
d_f = 0.0
f = 0.0
for i = 1, N
d_a += 2*x[i]*d_x[i]
a += x[i]*x[i]
d_t = cos(a)*d_a
t = sin(a)
d_f += d_t
f += t
end

0.0
0.0
i = 1, N
+= x[i]*x[i]
= sin(a)
+= t

Fig. 1. Example loop

575

Fig. 2. Example loop after forward mode AD

a[0] = 0.0
f = 0.0
for i = 1, N
a[i] = a[i-1] + x[i]*x[i]
t = sin(a[i])
f += t
end
for i = N, 1, -1
a_t = a_f
a_a[i] = cos(a[i])*a_t
a_a[i-1] = a_a[i]
a_x[i] = 2*x[i]*a_a[i]
end

Fig. 3. Example loop after reverse mode
automatic diﬀerentiation

a = 0.0
f = 0.0
for i = 1, N
a += x[i]*x[i]
t = sin(a)
f += t
a_t = a_f
a_a = cos(a)*a_t
a_x[i] = 2*x[i]*a_a
end

Fig. 4. Adjoint code after reversing the
adjoint loop and fusing it with the original computation

Hascoet et al. [6] observed that the forward computation and adjoint accumulation can be fused if the original loop is parallelizable. In fact, a weaker
condition suﬃces: the two computations can be fused whenever there are no loopcarried, nonlinear dependencies and any variables involved in linear loop-carried
dependencies are scalars The example in Figure 1 includes two loop-carried dependence, and both dependences are linear; therefore, the adjoint loop may be
reversed and fused with the original loop as shown in Figure 4.
Such transformations can result in signiﬁcant performance improvements and
storage savings, by eliminating the need to store or recompute overwritten intermediate quantities such as variable a. Data dependence analysis [2, 4, 18, 14] is
used to determine whether a loop is parallelizable. Precise dependence analysis
techniques can determine which variables are involved in a loop-carried dependence. In the example of Figure 1, such techniques can determine that there are
loop-carried dependences involving the variable f and itself and a and itself. In

576

M.M. Strout and P. Hovland

Variable
f
t
a
x
f
linear linear nonlinear nonlinear
t
nonlinear nonlinear
a
linear nonlinear
x

Fig. 5. The variables in the ﬁrst column
depend on the variables in the ﬁrst row
in the speciﬁed way.
indicates no dependence

  no dependence
linear
  nonlinear

Fig. 6. Lattice for linearity analysis

this paper, we present linearity analysis as a technique for determining if the
loop-carried dependence is linear or nonlinear.
The result of linearity analysis is the assignment of a dependence class to
each pair of variables in the program. For the example in Figure 1, the analysis
summarizes the dependences between variables as shown in Figure 5. Conservatively determining whether a loop-carried dependence is linear requires checking
only whether the dependence between the variables involved in the loop-carried
dependence is linear. Figure 5 indicates that f depends on itself linearly and the
same applies to variable a; therefore, both of the loop-carried dependencies are
linear.

2

Formulation of Linearity Analysis as a Data-Flow
Analysis

Linearity analysis can be formulated as a forward data-ﬂow analysis [10]. Dataﬂow analysis involves representing the subroutine to be analyzed as a control
ﬂow graph. A control ﬂow graph contains directed edges between basic blocks
indicating possible control ﬂow in the program. Each basic block b has a set
of predecessors pred(b) and a set of successors succ(b), and the graph contains
unique entry and exit nodes.
Data-ﬂow analysis propagates data-ﬂow information over the control-ﬂow
graph. For linearity analysis, the data-ﬂow information is which variables are
dependent on which other variables and whether that dependence is linear or
nonlinear, which we refer to as the dependence class. The analysis assigns each
ordered pair of variables a value from the lattice shown in Figure 6. For example,
in the statement
x = 3*z + y**2 + w/(v*v),
x has an linear dependence on z, x, z , linear , a nonlinear dependence on
y, x, y , nonlinear , a nonlinear dependence on w, x, w , nonlinear , and a
nonlinear dependence on v, x, v , nonlinear .
The set IN (b) includes the dependence class assignment for each ordered variable pair that is valid at the entry of basic block b in the control-ﬂow graph. A

Linearity Analysis for Automatic Diﬀerentiation
Expression e
k
k anyop k
v
e1 ± e2

e1 ∗ e2
e1 /e2

e1 power 1
e1 power k

577

DEP S(e)
{ v, | v ∈ V }
{ v, linear } ∪ { w, class | v, w , class ∈ IN (b)}
{ v1 , (class1 class2 )
| v1 = v2 and v1 , class1 ∈ DEP S(e1 )
and v2 , class2 ∈ DEP S(e2 )}
∪ { v, class | v, class ∈ DEP S(e1 ) and v ∈
/ DEP S(e2 )}
∪ { v, class | v, class ∈ DEP S(e2 ) and v ∈
/ DEP S(e1 )}
{ v1 , (nonlinear class1 class2 )
| v1 = v2 and v1 , class1 ∈ DEP S(e1 )
and v2 , class2 ∈ DEP S(e2 )}
∪ { v, nonlinear) | v, class ∈ DEP S(e1 ) and v ∈
/ DEP S(e2 ) }
∪ { v, nonlinear) | v, class ∈ DEP S(e2 ) and v ∈
/ DEP S(e1 ) }
{ v, (linear class) | v, class ∈ DEP S(e1 )
{ v, (nonlinear class1 ) | v, class ∈ DEP S(e1 )}

Fig. 7. Deﬁnition of the DEP S set for each expression

transfer function fb (IN (b)) calculates the OU T (b) set, which includes the dependence class assignments valid upon exiting the basic block b. The dependence
class for each variable pair u, v is initialized to , indicating no dependence,
in IN (b) and OU T (b) for all basic blocks in the control-ﬂow graph.
Iterative data-ﬂow analysis visits each node in the control-ﬂow graph computing the IN (b) and OU T (b) sets until the assignment of data dependence class
to each ordered variable pair converges. The set IN (b) is calculated by performing the pairwise meet over all the sets of data-ﬂow facts valid upon exiting
predecessor basic blocks,
IN (b) =

p∈preds(b) OU T (p).

The meet operation is performed on the lattice values assigned to each variable
pair. The semantics of the meet operation is deﬁned by the lattice. For example,
linear
equals linear, and linear nonlinear equals nonlinear.
The set OU T (b) is computed by applying what is referred to as a transfer
function to the IN (b) set. The transfer function fb is ﬁrst deﬁned for each type
of statement assuming only one statement per basic block. If there are multiple
statements in block b, then fb is the composition of all the transfer functions
for the statements. To deﬁne the transfer function fb for linearity analysis, we
deﬁne the set DEP S(e) as a set containing a mapping of each variable to a data
dependence class. When variable v maps to dependence class class, v, class ,
that indicates how an expression e depends upon that variable. The transfer
function fb for the assignment statement x = e is then deﬁned as
OU T (b) = fb (IN (b)) = { x, v , class | v, class ∈ DEP S(e)}.
The DEP S(e) set is deﬁned in Figure 7, where k represents a constant value,
v and w represent variables in the set of all variables V , anyop represents any

578

M.M. Strout and P. Hovland

Entry
a = 0.0
f = 0.0

if (i<=N)

a += x[i]*x[i]
t = sin(a)
f += t

Exit

Fig. 8. Control ﬂow graph for
code in Figure 1

Statement s IN (s)
a+=x[i]*x[i] { v, w ,
| v, w ∈ V }
t=sin(a)
{ a, a , linear , a, x , nonlinear }
{ v, w ,
| v, w ∈ V }
f+=t
{ a, a , linear , a, x , nonlinear ,
t, a , nonlinear , t, x , nonlinear }
{ v, w ,
| v, w ∈ V }
Expression e Deps(e)
0.0
{ a, , f, , i, , t, , f, }
a+x[i]*x[i] { a, linear , x, nonlinear }
sin(a)
{ a, nonlinear , x, nonlinear }
f+t
{ f, linear , t, linear ,
a, nonlinear , x, nonlinear }

Fig. 9. Applying the linearity analysis to certain
statements in the control ﬂow graph in Figure 8

operation, and power represents the power operation. Notice that if a variable
occurs in both subexpressions of a binary operator that DEP S is computed differently than when a variable only occurs in one of the subexpressions. Figure 8
shows the control ﬂow graph for the example program from Figure 1. Figure 9
shows the IN and DEP S sets for some of the statements in the example program. Note that the IN set for a statement in a basic block is equivalent to the
OU T set for the previous statement.
The worst-case complexity of linearity analysis is O(N 4 (E + V )), where N
is the number of variables, E is the number of edges in the control-ﬂow graph,
and V is the number of nodes in the control ﬂow graph. Each pair of variables
has a lattice value associated with it, and there are N 2 pairs. Each lattice value
may be lowered at most twice; therefore, the graph may be visited 2 ∗ N 2 times.
The size of the graph is E + V . When each node is visited, the computation may
apply meet and transfer operations to each variable pair, O(N 2 ).
2.1

Detecting Nonlinear Loop-Carried Dependences

Data dependence analysis provides information about which variable references
are involved in loop-carried dependences. If a particular variable is involved in
a loop-carried dependence, and the variable depends on itself nonlinearly based
on the results of linearity analysis, then the loop may involve a nonlinear loopcarried dependence.
2.2

Limitations

As formulated, linearity analysis is incapable of determining that the loop shown
in Figure 10 has no loop-carried, nonlinear dependences. Speciﬁcally, there is a

Linearity Analysis for Automatic Diﬀerentiation

579

loop-carried dependence between b and c due to c[i] = b[i-1], and there is a nonlinear dependence between b and c due to c[i] = b[i]*x[i]. However, the nonlinear dependence is not loop carried.
for i = 1 to N
One can determine whether there
b[i] = 3*x[i]
are nonlinear, loop-carried dependenc[i] = b[i-1] + b[i]*x[i]
cies if the data-ﬂow analysis is done
end
on a use-by-use basis. This data-ﬂow
problem could be signiﬁcantly more
expensive than basic linearity anal- Fig. 10. Example where the current formuysis. In order to achieve higher pre- lation of linearity analysis combined with
cision at a reasonable cost, while data dependence analysis is overly conserre-using as much analysis as possible, vative
a closer coupling between linearity analysis and data-dependence analysis may
be required.

3

Other Applications

Linearity analysis is also useful for a sort of “predictive slicing.” In a so-called
“pure” derivative computation [8], one wants to compute only the derivatives of
a function and not the function itself. However, by default AD produces code
that computes both the function and its derivatives, primarily because many of
the intermediate function values are required to compute derivatives. However,
when it can be determined that the
dependent variables depend only
a = 0.0
linearly on an intermediate function
for i = 1, N
value, then that intermediate value
a += x[i]*x[i]
is not needed in the derivative coma_t = a_f
putation. Therefore, the generated
a_a = cos(a)*a_t
derivative code may omit the coma_x[i] = 2*x[i]*a_a
putation of these intermediates.
end
This is equivalent to generating the
derivative code, then performing a Fig. 11. Reverse mode AD, computing only
backward slice [17] from the deriva- derivatives via predictive slicing
tive variables. Figure 11 illustrates
the use of predictive slicing on the example of Figure 4. The dependent variable
f depends nonlinearly only on a and x; therefore, t and f do not need to be
computed.
Linearity analysis can be combined with array data ﬂow analysis to identify functions f (x) : Rn → R that can be decomposed into the form: f (x) =
m
i=1 Fi (x) where each Fi is a function of only a few elements of the vector x.
This is the simplest form of partially separable function [13]. The Jacobian of
F (x) : Rn → Rm is sparse and this sparsity can be exploited by using compression techniques [1]. The gradient of f is the sum of the rows of this Jacobian.
Thus, gradients of such functions can be computed eﬃciently using the forward
mode.

580

M.M. Strout and P. Hovland

Linearity analysis is also directly useful in numerical optimization. Optimization algorithms distinguish between linear and nonlinear constraints in order to
reduce the cost of derivative evaluations (the derivatives of linear constraints
are constant), to reduce the problem size via preprocessing, and to improve the
performance of the optimization algorithm. Experimental results from Gould
and Toint [5] indicate that preprocessing of the linear and bound constraints
reduces the number of constraints by 19% and the total time to solution by 11%
on average. Combined with the added savings from fewer constraint evaluations,
derivative evaluations, and faster convergence, the savings can be substantial.
Preliminary experiments indicate that when all constraints can be identiﬁed as
linear, savings of 50% or more are possible.

4

Related Work

Karr [9] and Cousot [3] determine linear equalities and linear inequalities between variables. The focus for such techniques is to ﬁnd program invariants for
use with automated reasoning tools. More recent research [12, 15] discovers a subset of nonlinear relationships, polynomial relationships of bounded degree. None
of these techniques distinguishes between a nonlinear dependence and a lack of
dependence. Therefore, they are not suitable for the types of program optimization we have described. To-be-recorded (TBR) analysis [7] identiﬁes the set of
variables that are needed for derivative computation and thus must be recorded
if overwritten. This analysis is similar to linearity analysis, but includes index
variables, excludes variables that are never overwritten, and does not identify
pairwise dependence. Linearity analysis can be readily extended to polynomial
degree analysis. We have also extended polynomial degree analysis to a restricted
form of rationality analysis. Polynomial degree analysis and rationality analysis
have applications in code validation [11].

5

Conclusions and Future Work

We have presented a formal data-ﬂow formulation for linearity analysis. Linearity analysis has several applications in automatic diﬀerentiation and numerical
optimization. In addition to the applications already discussed, linearity and
polynomial degree analysis have applications in code derivative-free optimization, nonlinear partial diﬀerential equations, and uncertainty quantiﬁcation. We
are implementing linearity and polynomial degree analysis in the OpenAnalysis
framework [16] to provide compiler infrastructure-independent analysis. We are
investigating ways to tightly couple linearity analysis with dependence analysis
to address the limitations discussed in Section 2.2.

Acknowledgments
We would like to thank Todd Munson, Rob Kirby, and Ridgeway Scott for their
suggestions, and the anonymous reviewers and Gail Pieper for their feedback.

Linearity Analysis for Automatic Diﬀerentiation

581

References
1. B. M. Averick, J. J. Mor´e, C. H. Bischof, A. Carle, and A. Griewank. Computing large sparse Jacobian matrices using automatic diﬀerentiation. SIAM J. Sci.
Comput., 15(2):285–294, 1994.
2. U. Banerjee. Dependence analysis for supercomputing. The Kluwer international
series in engineering and computer science. Parallel processing and ﬁfth generation
computing. Kluwer Academic, Boston, MA, USA, 1988.
3. P. Cousot and N. Halbwachs. Automatic discovery of linear restraints among
variables of a program. In POPL ’78: Proceedings of the 5th ACM SIGACTSIGPLAN symposium on Principles of programming languages, pages 84–96, New
York, NY, USA, 1978. ACM Press.
4. P. Feautrier. Dataﬂow analysis of array and scalar references. International Journal
of Parallel Programming, 20(1), February 1991.
5. N. Gould and P. L. Toint. Preprocessing for quadratic programming. Math. Programming, 100(1):95–132, 2004.
6. L. Hasco¨et, S. Fidanova, and C. Held. Adjoining independent computations. In
G. Corliss, C. Faure, A. Griewank, L. Hasco¨et, and U. Naumann, editors, Automatic
Diﬀerentiation of Algorithms: From Simulation to Optimization, Computer and
Information Science, chapter 35, pages 299–304. Springer, New York, NY, 2001.
7. L. Hasco¨et, U. Naumann, and V. Pascual. “To be recorded” analysis in reversemode automatic diﬀerentiation. Future Generation Computer Systems, 21(8), 2005.
8. T. Kaminski, R. Giering, and M. Voßbeck. Eﬃcient sensitivities for the spin-up
phase. In H. M. B¨
ucker, G. Corliss, P. Hovland, U. Naumann, and B. Norris,
editors, Automatic Diﬀerentiation: Applications, Theory, and Tools, Lecture Notes
in Computational Science and Engineering. Springer, 2005.
9. M. Karr. Aﬃne relationships among variables of a program. Acta Informatica,
6(2):133–151, 1976.
10. G. A. Kildall. A uniﬁed approach to global program optimization. In ACM Symposium on Principles of Programming Languages, pages 194–206, October 1973.
11. R. Kirby and R. Scott. Personal communication, 2004.
12. M. M¨
uller-Olm and H. Seidl. Precise interprocedural analysis through linear algebra. In POPL ’04: Proceedings of the 31st ACM SIGPLAN-SIGACT symposium on
Principles of programming languages, pages 330–341, New York, NY, USA, 2004.
ACM Press.
13. J. Nocedal and S. J. Wright. Numerical Optimization. Springer-Verlag, New York,
1999.
14. W. Pugh. Omega test: A practical algorithm for exact array dependency analysis.
Comm. of the ACM, 35(8):102, 1992.
15. E. Rodriguez-Carbonell and D. Kapur. Automatic generation of polynomial loop
invariants: Algebraic foundations. In ISSAC ’04: Proceedings of the 2004 international symposium on Symbolic and algebraic computation, pages 266–273, New
York, NY, USA, 2004. ACM Press.
16. M. M. Strout, J. Mellor-Crummey, and P. Hovland. Representation-independent
program analysis. In Proceedings of the Sixth ACM SIGPLAN-SIGSOFT Workshop
on Program Analysis for Software Tools and Engineering, 2005.
17. M. Weiser. Program slicing. IEEE Trans. Software Eng., 10(4):352–357, 1984.
18. M. Wolfe and C. W. Tseng. The power test for data dependence. IEEE Trans.
Parallel Distrib. Syst., 3(5):591–601, 1992.

