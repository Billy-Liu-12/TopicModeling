A Computational Infrastructure for Reliable
Computer Simulations
J. Tinsley Oden1 , James C. Browne1 , Ivo Babuˇska1 , Kenneth M. Liechti2 , and
Leszek F. Demkowicz1
1

2

Institute for Computational Engineering and Sciences
The University of Texas at Austin
Austin, TX 78712, U.S.A.
{oden, babuska, leszek}@ticam.utexas.edu
browne@cs.utexas.edu
http://www.ticam.utexas.edu
Research Center for the Mechanics of Solids, Structures and Materials
The University of Texas at Austin
Austin, TX 78712, U.S.A.
kml@mail.utexas.edu

Abstract. The reliability of computer simulations of physical events
or of engineering systems has emerged as the single most critical issue
facing advancements in computational engineering and science. Without concrete and quantiﬁable measures of reliability, the conﬁdence and
usefulness of computer predictions are severely limited and the value of
computer simulation, in general, is greatly diminished. This paper describes a mathematical and computational infrastructure for the systematic validation of computer simulations of complex physical systems and
presents procedures for the integration of computer-based veriﬁcation
and validation processes into simulations. A general class of applications
characterized by variational boundary-value problems in continuum mechanics is considered, but the approach is valid for virtually all types
of models used in simulations. To simulate a particular feature or attribute of a physical event, four basic concepts are used: 1) hierarchical
modeling and a posteriori estimation of modeling error, 2) a posteriori
error estimation of approximation error, 3) quantiﬁcation of uncertainty
in the data and in the predicted response, and 4) the development of
dynamic a data management paradigm, based on code composition of
associated code interfaces and use of annotated source code. The result
is a computational framework for computing bounds on, and estimating accuracy of, computer predictions of user-speciﬁed features of the
response of physical systems.

1

Introduction

The systematic study of reliability of computer simulations of physical phenomena is generally divided into two basic processes,Veriﬁcation and Validation (V
and V). As a scientiﬁc discipline, V and V is in its very early stages of development. Veriﬁcation is the process of determining the accuracy with which a
P.M.A. Sloot et al. (Eds.): ICCS 2003, LNCS 2660, pp. 385–390, 2003.
c Springer-Verlag Berlin Heidelberg 2003

386

J.T. Oden et al.

given mathematical model of physics is solved; validation is the process of determining that the mathematical model represents the actual physical system
with suﬃcient accuracy. Here veriﬁcation does not include software engineering
issues of the logical correctness of the program which implements the simulations
(this is so-called code veriﬁcation). Veriﬁcation is thus concerned with estimating and controling numerical approximation error. In recent years, signiﬁcant
progress has been made in this area (see Ainsworth and Oden [1], Babuska and
Strouboulis [3], Oden and Prudhomme [17], Becker and Rannacher [9]).
Validation, until recently, has been primarily an experimental process in
which data from physical observations are compared with computer simulations.
The notion of hierarchical modeling provides a mathematical structure that can
be useful in directing validation studies. In this structure, a class of models of
events of interest is deﬁned in which a “ﬁne” model is identiﬁed which possess
a level of sophistication believed to be high enough to adequately capture the
event of interest with good accuracy. This model, however, may be intractable.
The complexity of this model, may be such that one cannot solve it, even computationally. We then identify (families of) coarse models which are solvable.
Using the ﬁne model as a datum, the error in the solution of coarse models can
be estimated and controlled, with the goal of obtaining a model best suited for
the simulation goal at hand.
Physical data must be collected to characterize the ﬁne model, and this must
be furnished by data generated in actual laboratory tests. In general, these data
will exhibit scatter (uncertainty) – and the basic ﬁne-model data are then random functions. Thus the validation process is complicated by the fact that the
models themselves are stochastic.
In the sections to follow, we outline brieﬂy the program components that
need to be integrated to produce a V and V computational system.

2

Problem Deﬁnition

As a representative physical situation, we consider a heterogeneous solid body D
occupying a region in R3 and heated along its boundary ∂D and subject to an
internal heat source b − σu, u being the temperature ﬁeld, b the heat supplied
per unit volume and σ > 0 a reaction coeﬃcient. The ﬁne mathematical model
is characterized by the equations,
−divA∇u + σu = b in D
n · A∇u = g on ∂D

(1)

g being a prescribed heat ﬂux and n a unit exterior normal. While the temperature ﬁeld u is governed by this system, we are speciﬁcally interested in predicting
certain features of the event, for instance, the temperature or the heat ﬂux at a
point x0 in direction ν interior to D. These quantities of interest are functionals
on u; e.g.
Q(u) = u(x0 )
A weak form of this problem is

or Q(u) = ν · A(x0 )∇u(x0 )

(2)

A Computational Infrastructure for Reliable Computer Simulations

Find u ∈ V such that
B(u, v) = F (v)

∀v ∈ V

387

(3)

where V is an appropriate space of admissible functions, and
(A∇u · ∇v + σuv) dx

B(u, v) =
D

F (v) =

(4)
bv dx +

D

gv ds
∂D

Problem (2) (or, equivalently, (1)) is, in general, intractable. Typical complications that make it impossible or infeasible are:
1) the coeﬃcient (thermal conductivity) A is a highly oscillatory function
characterizing rapid oscillatory variations of mechanical properties at scales
much smaller than the characteristic dimensions of D, and
2) the laboratory data needed to characterize A exhibit much scatter and,
therefore, A is a random function (the data σ, b, and g may also be random).
This uncertainty in the data must be quantiﬁed in some sense. Thus, it is desirable to develop statistical information suﬃcient to determine the probability
density function (PDF) p for A, the variances, covariance, etc.
If p is the PDF, the mean or expected value of the random function f is
∞
E[f ] = −∞ pf dω. The solution u of (1) is, in this case, a random ﬁeld, u =
u(x, ω), x ∈ D and ω ∈ Ω, Ω being an appropriate set of elementary events.
Then, instead of (4), we use
E[A∇u · ∇v + σuv] dx

¯ v) =
B(u,

(5)

D

u, v ∈ V × W , W being a probability measure space.

3

Coarse and Discrete Models

Consistent with the theory of hierarchical modeling, we replace (2) by a simpliﬁed
or coarse model,
u0 ∈ V :

B0 (u0 , v) = F (v) ∀ v ∈ V

(6)

where
(A0 ∇u0 · ∇v + σu0 v) dx

B0 (u0 , v) =

(7)

D

Here A0 is an eﬀective thermal conductivity of the material obtained through
homogenization methods and, for the purpose of this example, is assumed to
be deterministic. The coarse model prediction of the quantity (or quantities) of
interest is Q(u0 ).
In general, problem (7) is also intractable, but it can be solved approximately
using numerical techniques such as ﬁnite elements. Thus, let V h be a member

388

J.T. Oden et al.

of a family of ﬁnite dimensional subspaces of V . We seek an approximation uh0
of u0 satisfying
uh0 ∈ V h :

B0 (uh0 , v) = F (v) ∀ v ∈ V h

(8)

Now, summing up our sources of error, we see that the error on the quantity of
interest Q(u) is,
Q(u) − Q(uh0 ) = Q(u) − Q(u0 ) + Q(u0 ) − Q(uh0 )
modeling error

approaximation error

(9)

= εmod + εapprox
Estimating and controling the error component εmod is regarded as validation process (or as an aide to validation considerations), while controling εapprox
is a veriﬁcation process. A general theory for controling this type of modeling
error in quantities of interest Q through a posteriori error estimation and adaptive modeling has been developed by Oden and Prudhomme [19,17], and Oden
and Vemaganti [15,16]. Techniques for deriving a posteriori error estimates for
εapprox and goal-oriented adaptive meshing have been advanced by Babuska and
Strouboulis [3], Oden and Prudhomme [18,19], Rannacher, Becker and others [9].
Thus we assume that using the data available, it is possible to obtain computable
lower and upper bounds on these components:
upp
γ low
mod ≤ εmod ≤ γ mod ,

upp
γ low
appox ≤ εapprox ≤ γ approx

(10)

Averages of these bounds can yield estimates of the errors (e.g. εmod ≈ 12 (γ low
mod +
γ upp
)).
If
these
estimated
errors
exceed
preset
tolerances,
the
errors
must
be
mod
reduced by adaptive meshing (for εapprox ) (see [17]) and adaptive modeling (for
εmod ).
Another source of error relevant to validation is due to the randomness of the
coeﬃcients A. There are several approaches available to quantify this error. One
direct approach is provided by the perturbation method described by Kleiber
and Hien [14], which, for a ﬁrst order approximation, takes the form
¯
A(x, ω) = A(x)
+ εA1 (x, ω),

u(x, ω) = u
¯(x) + εu1 (x, ω)

(11)

A¯ and u
¯ being mean values of A and u. Then one treats ﬁrst the deterministic
ﬁne problem (1) with the forms (4), involving A¯ instead of A and u
¯ instead of u.
Then Q(u) = Q(¯
u) + εQ(u1 ). The random ﬁeld Q(u1 ) is completely characterized in a stochastic sense, once the probability ﬁeld of A(x, ω) is provided and
deterministic component u
¯ is known. Bounds on the truncation error inherent
in the perturbation (11) can be calculated and used as additional measure of
the modeling error [4]. The second approach is based on the theory of stochastic
functions characterized by Karhunen-Lo`eve expansion (see [5,11,6]). In [7], the
problem of determination of the Karhunen-Lo`eve expansion from experimental data is described. A brief survey [8] of the mathematics of veriﬁcation and
validation is presented when the experimental data [2] are used.

A Computational Infrastructure for Reliable Computer Simulations

4

389

The Integration of Computational Components

Separate codes are written or are available for the component error estimations and adaptivity. The integration of these modules requires a major new
dynamic data structure innovation to coordinate a component-oriented code development process, and requires a dynamic scaling of common data structures.
An integrated compiler and runtime system is being developed to compose appropriate components and map data structures across interfaces (see [10,12,13]).
Annotations of source code is to be used to guide compilation and enable interfacing diﬀerent data structures. Diﬀerent computational models provide for the
speciﬁcation of associative interfaces and the annotation language. The essential
integration components are as follows:
1) Experimental data are collected to fully characterize the ﬁne model, including statistics to give bounds on the data or PDF’s and covariance matrices.
2) Quantities (or a quantity) Q of interest are (is) speciﬁed as the target physical entity to be predicted in the simulation (perhaps in the form of probability
of the predicted values of the quantity).
3) The coarsest model is used to extract a preliminary estimate of Q and
modeling and approximation errors are computed.
4) If the estimated error exceeds the prescribed tolerance, the model is enhanced and the calculation is repeated, each time adapting the computational
model to reduce εapprox until a model yielding results within the preset bounds
is obtained.
5) The truncation error of the perturbation expansion is estimated; if the
total error exceeds a preset tolerance, the data set and the ﬁne model deﬁnition
must be updated. If not, the predicted Q and the probability that it will takes
on values in a given interval are produced as output.
In the absence of suﬃcient data to adequately characterize the random data
ﬁelds, a worse-case analysis can be done giving upper and lower bounds on the
predicted values.
The implementation of this and related algorithms is underway and will be
the subject of forthcoming reports.
Acknowledgment. The assistance of Dr. Yusheng Feng and Dr. Fabio Nobile
on this project is gratefully acknowledged. This work is done under ITR NSF
grant 0205181. The support and encouragement of Dr. Frederica Darema of NSF
is greatly appreciated by the authors.

References
1. Ainsworth, M., Oden, J.T.: A Posteriori Error Estimation in Finite Element Analysis. John Wiley & Sons, New York, (2000)
2. Babuˇska, I., Jerina, K., Li, Y., Smith, P.: Quantitative Assessment of the Accuracy
of Constitutive Laws for Plasticity with Emphasis on Cyclic Deformation. Material Parameter Estimation for Modern Constitutive Equations. MD-Vol.43/AMDVol.168 ASME (1993)

390

J.T. Oden et al.

3. Babuˇska, I., Strouboulis, T.: Finite Element Method and its Reliability. Oxford
Univ. Press (2001)
4. Babuˇska, I., Chatzipantelidis, P.: On Solving Elliptic Stochastic Partial Diﬀerential Equations. Comput. Methods Appl. Mech. Eng. 191 (2002) 4093-4122
5. Babuˇska, I., Tempone, R., Zouraris, G.E.: Galerkin Finite Element Approximations of Stochastic Elliptic Diﬀerential Equations. TICAM Report 02-38, The
University of Texas at Austin, (2002)
6. Babuˇska, I., Liu, K-M.: On Solving Stochastic Initial-Value Diﬀerential Equations.
Math. Models Methods Appl. Sci., (2003) In press
7. Babuˇska, I., Liu, K-M., Tempone, R.: Solving Stochastic Partial Diﬀerential Equations Based on Experimental Data. Math. Models Methods Appl. Sci., (2003) In
press
8. Babuˇska, I.: Mathematics of the Veriﬁcation and Validation in Computational Engineering. In: Proceeding of Int. Conf. on Mathematical and Computer Medeling
in Sci. Eng., Prague Czech Republic (2003)
9. Becker, R., Rannacher, R.: An optimal control approach to a posteriori error
estimation in ﬁnite element methods. In: Isereles, A. (ed.): Acta Numerica, Vol.
10. Cambridge Univ. Press (2001) 1-120
10. Browne, J.C., Berger, E., Dube, A,: Compositional Development of Performance
Models in POEMS. Int. J. High Perf. Comput. Appl. 14(4) (2000) 283–291
11. Deb, M.K., Babuˇska, I., Oden, J.T.: Solution of Stochastic Partial Diﬀerential
Equations Using Galerkin Finite Element Techniques. Comput. Methods Appl.
Mech. Eng. 190 (2001) 6359-6372
12. Edwards, C, Browne, J.C.: Scalable Dynamic Distributed Array and its Application to a Parallel hp-adaptive Finite Element Code. Proceedings of the 1996
POOMA Workshop, Santa Fe, New Mexico, (1996)
13. John, A., Browne, J.C.: Compilation of Constraints Programs with Noncyclic and
Cyclic Dependencies to Procedural Parallel Programs, Int. J. Parallel Prog. 26(1)
(1998) 65-119
14. Kleiber, M. Hien, T.D.: The Stochastic Finite Element Method: Basic Perturbation Technique and Computer Implementation. John Wiley & Sons, (1992)
15. Oden, J.T., Vemaganti, K.: Adaptive Hierarchical Modeling of Heterogeneous
Structures. Predictability: Quantifying Uncertainty in Models of Complex Phenomena. Phys. D., 133 (1999) 404-415
16. Oden, J.T., Vemaganti, K.: Estimation of Local Modeling Error and GoalOriented Modeling of Heterogeneous Materials; Part I: Error Estimates and Adaptive Algorithms. J. Comput. Phys., 164 (2000) 22-47
17. Oden J.T., Prudhomme, S.: Goal-Oriented Error Estimation and Adaptivity for
the Finite Element Method. Comput. Math. Appl., 41(5-6) (2001) 735-756
18. Oden, J.T., Prudhomme, S.: Estimation of Modeling Error in Computational
Mechanics. J. Computat. Phys., 182 (2002) 496-515
19. Prudhomme, S., Oden, J.T.: On Goal-Oriented Error Estimation for Elliptic Problems: Applications to the Control of Pointwise Errors. Comput. Methods Appl.
Mech. Eng., 176 (1999) 313-331

