Models as Arguments: An Approach to
Computational Science Education
D.E. Stevenson
315 McAdams Hall, School of Computing, Clemson University,
PO Box 34097, Clemson, SC 29634-0974
steve@cs.clemson.edu

Abstract. Hardware and software technology have outpaced our ability
to develop models and simulations that can utilize them. Furthermore,
as models move further into unfamiliar territory, the issues of correctness
becomes more diﬃcult to assess. We propose extending classical argumentation structures as the basis for computational science education.

1

Human Centric Computing — The Argument

Computation and computational science has rightly emphasized hardware and
software development. But the hardware is now far more advanced than the
software developers can take advantage of and the scientiﬁc problems are more
complex than those of ﬁfty years ago. Now that computing is pervasive (if not
invasive) it is time for humans to catch up. Kurtzweil’s “singularity” not withstanding, we must modernize human interaction with computing. The futurists
seem to be divided into two camps: pervasive computing and human centric computing (HCC). Once again, however, technology and not people are the focus,
with overriding objectives of making the technology transparent or letting each
sector focus on what it does best.
– The pervasive computing camp argues that IT needs to “get the end user
experience right,” focusing on designing devices that are easier for users to
interact with in order to ﬁnd relevant information.
– The objectives of human-centric computing are not to focus on the devices
themselves, but rather to create an entire solution so that the human, rather
than the device, is always connected.
This is all very exciting: but the human also needs a tune-up, meaning that the
education of the 21st Century computational scientist cannot just be more detail
at a higher level using the same techniques as those used ﬁfty years ago. So far,
the future is about more speed, not about more understanding. The fundamental
question before computational science is still “How do we know it’s right?”
Since it was fair to test machines for intelligence, machines might test humans
with a variant of the Turing test that I will call a Kurzweil Test:
A machine judge engages in a “machine language” conversation with
one human and one machine, each of which try to appear as machines.
G. Allen et al. (Eds.): ICCS 2009, Part II, LNCS 5545, pp. 84–92, 2009.
c Springer-Verlag Berlin Heidelberg 2009

Models as Arguments: An Approach to Computational Science Education

85

All participants are placed in isolated locations. If the judge cannot reliably tell the human from the machine, the human is said to have passed
the test. 1
But we are before the singularity, so the Test is to determine if a machine
is more intelligent than the human. What do we mean by “more intelligent”?
I approach the test by using Bloom’s Taxonomy (Fig. 1), which orders cognitive
tasks from lowest to highest, as the measure of intellectual achievement and ask
questions based on the standard verbs used to identify levels. Now we must ask,
“Kurzweil machines will be able to pass the Turing test, will our students pass
such a Kurzweil test?”
Remembering deﬁne, recall, repeat, reproduce state
Understanding classify, explain, identify, translate, paraphrase
Applying
choose, demonstrate, interpret, operate, solve, write.
Analyzing
appraise criticize, examine, experiment, question, test.
Evaluating
defend, judge, select, value, evaluate
Creating
construct, create, design, develop, formulate, write.
Fig. 1. Bloom’s Taxonomy as modiﬁed by Lorin Anderson

We can hope that the Ph. D. students can pass a Turing test in their ﬁeld, but
undergraduates probably cannot. Firstly, there is a disconnect between student’s
abilities to respond to requests to develop artifacts and teachers’ expectations.
In learning to program, for example, we immediately ask students to create
artifacts when their cognitive abilities are still in the lower three categories.
Secondly, the current idea that critical thinking is an ultimate skill to teach
leaves us short of creating things: critical thinking is analyzing and evaluating,
not creating. In fact, if one looks at classical rhetoric, one ﬁnds that critical
thinking is a subordinate skill to argumentation.
We extend the concepts presented in [2]. Sect. 2 describes computational science problems as we propose they should be solved in a classroom. Sect. 3 discusses a pedagogical approach Systems-Questions-Explanations (SQE). Finally
we introduce the addition of argumentation as a pedagogical approach to computational science (Sect. 4) followed by an example (Sect. 5). We collect our
major points in Sect. 6.

2

The Problem

Computational science in its broadest sense is the modeling of systems. But
system models, by themselves, are not necessarily correct. Aristotle was a proliﬁc
modeler whose models carried the force of law for almost 2,000 years — but he got
the structures of both the universe and biology wrong. Francis Bacon argued in
1

This tongue-in-cheek version is Wikipedia’s explanation of the Turing test [1] with
“human” and “machine” swapped.

86

D.E. Stevenson

his Novum Organon that models must be validated by experiments: the Scientiﬁc
Method. It is now recognized that the Scientiﬁc Method is supported by a third
leg: computing. Correctness is an ever more important now than with Bacon and
the terms verification and validation are used to describe the processes. Recently,
Wilson added reproducibility so we now abbreviate the “correctness” process as
VV&R.
Despite our advanced computer systems, models and simulations are still developed by people. Traditional science and engineering education proceeds by
showing the student a number of models, then asking the students to apply
these models to textbook problems. This pedagogical approach assumes that
students learn to model by exposure. Our data shows that the students are not
learning how to create models.
Experiences with incoming science, technology, engineering, and science
(STEM) students at Clemson indicates that students do indeed have the facts
at hand that we suppose they should have. Many students have high SAT/ACT
scores and high predicted GPAs; yet many never complete a STEM degree. One
possibility is that these students do not have the critical thinking skills required
to analyze scientiﬁc models; however, critical thinking skills testing we have conducted (with IRB approval) shows that these students seem to have the critical
thinking skills of relatively mature thinkers [3]. In other words, we have students
who are at the evaluting stage in Bloom’s Taxonomy but still are not thriving.
The students seem not to understand how to structure an argument, nor are
they able to evaluate their own arguments. As discussed in Parham, Chinn,
and Stevenson [4], students have trouble solving unstructured problems that
have many transitions among the six Bloom elements. By using verbal protocol
approach, we were able to trace the students use of information and metacognition. Reviewing, analyzing, and evaluating student or historical arguments is
one way to practice critical thinking. We can also practice argumentation by
reviewing, analyzing, and evaluating arguments. But argumentation should also
require students to structure an argument then review and self-evaluate it [5].
One approach we are trying is to organize modeling classes around classical argumentation methods. Argumentation is both a process (the discussion) and
a product (the outcomes) consisting of three parts: rhetoric, reasoning and dialectic. Rhetoric means development of knowledge through communications and
dialectic is discovering and testing knowledge through questions and answers. To
conduct an orderly argument, we must share standards and appraisal methods
relating to knowledge and the reasoning schemata.

3

How Do Humans Formulate Models? SQE

The ﬁrst step in solving any problem is to construct a model of that problem,
and this ﬁrst model is a mental model. The same process of modeling is needed to
install new ideas in our personal knowledge bases. The term mental model is used
both in formally and informally. Many disciplines have their own interpretations
of the term model, but we use three interpretations: (1) model of a physical

Models as Arguments: An Approach to Computational Science Education

87

system (physical sciences), (2) models of axioms (logic), and (3) mental models
(psychology). The basic interpretation of model is a set of logical (not necessarily
classical logic) statements made about the modeled phenomenon that are jointly
true — recall that mathematical equations are true or false. The processes for
showing models (or simulations) are true are called veriﬁcation and validation.
The “subversive” purpose of this paper is to propose a practical approach to
VV&R education.
While the results of a modeling study are external artifacts, the process of
developing models is primarily psychological and sociological. Focusing too much
on the external artifacts lends no insight into the thinking process: the reasoning
process that education must hone, something quite diﬀerent than having more
theorems and laws. Questions concerning human reasoning are not new, but
many modern studies started in the mid-1970s. Philip Johnson-Laird [6] based
his concept of mental model on models of explanation put forth by Kenneth Craik
[7]. Craik proposed a three step process: (S1) translation of original problem
statement to an internal representation, (S2) the manipulation and reasoning
to develop a solution, and ﬁnally (S3) the re-translation of the solution to an
external medium.
We discuss S1 and S3 in this section and Step S2 is discussed in 4. Step S1
is a linguistic step, emphasizing two auxiliary steps: association of words with
concepts (mind maps) and the association of concepts and relationships (concept
maps). At the end of the translation step, the internal representation is a graph
of schemata. Schemata are units of knowledge proposed by Immanuel Kant and
ﬁnally popularized by John Anderson in the ACT* models. Work on schemata
and their use in problem-solving in computer science is being studied at Clemson
under the author. Step S3 is interesting in computational science because the
output form is usually quite diﬀerent than the input: complex natural language
in, complex computer system out.
3.1

Psychology

The author began studying the human aspect of modeling and simulation approximately ﬁfteen years ago, primarily studying problem-solving as described
by Sternberg [8]. There are ﬁve aspects of problem solving: modeling (planning),
reasoning, knowledge, critical thinking, and creative thinking. Bushey [3] explored the role of critical thinking, showing that successful computer scientists
are critical thinkers. As research continued into the psychology literature, we
soon focused on the issue of knowledge and how it is used.
Anderson [9] proposed the ﬁrst of a series of models2 collectively called ACT*,
based on the concept that humans hold knowledge in certain memories, that
these units of memories are connected, and that we use pattern-matching to ﬁnd
new connections. The dynamics of how this might occur were partially established by Marshall [10]. While Marshall established some evidence for schemata,
she did not establish how we recognize mistakes. This process is now known
2

These models are an active domain of research in cognitive psychology.

88

D.E. Stevenson

as metacognition, ﬁrst introduced by Flavell [11]. The emerging picture is that
the problem solver recognizes patterns of information that identiﬁes schemata
and their connections with metacognition playing a control function. Schemata
themselves seem to be units of knowledge with ability to determine applicability,
planning, and execution requirements.

4

Systems, Questions, and Explanations

This is still an unsatisfactory explanation of how humans learn to be modelers: how did Einstein happen on his elevator? Our goal is to understand the
tools needed to model a system and inculcate them into the students. This was
discussed in Stevenson [2], which proposes that the modeling is based on three
foci: Systems, Questions, and Explanations (SQE). The reasoning for the SQE
model is as follows: In order to ask a meaningful question, one must ﬁrst have
a system in mind. The system eﬀectively deﬁnes the context, rules, and conditions that the system obeys and provides a vocabulary of variables and values
(at resolution) to provide semantics for both the question and the answer. With
the system and question, one can provide meaningful answers, which themselves
must be explained; justiﬁcation is just one aspect of explanation.

Fig. 2. Simpliﬁed Unit Argumentation Cell. Four fundamental information types.
Claims are the conclusion. Evidence supports claims through inference. Inference is
justiﬁed by warrants.

The development of an argument relies on the structure shown in Fig. 2.
Classically, these parts are logical statements playing four roles:
1. Claim. This is the statement to be the conclusion.
2. Inference. A gap can be ﬁlled by inference based on the structure of the
model. These inferences must be justiﬁed (warranted).
3. Evidence and Hypotheses. A gap may be ﬁlled by hypotheses that must then
be proven.
4. Computation. A gap may imply enough conditions that a computation is
feasible and practical.

Models as Arguments: An Approach to Computational Science Education

89

This particular approach comes from a basic observation: one must argue that
a model is correct and therefore the model is the argument. This model accepts
the closed world view. In the open world models, there are defaults and uncontrolled logical situations; in the closed world, all propositions must be expressed
explicitly. There are ﬁne lines of distinction between claims, evidence, inference,
and warrants that the student must wrestle with, but it is a clear framework.
Finally, the work on mental models [12,13] shows that most people use informal methods of reasoning [5]. Evidence indicates that we reason through logical
models, but not necessarily using classical (simple syllogistic) logic. One approach, for example, might be to integrate new information in the older model,
then deal with the contradictions and incoherencies. The basic problem is that
classical logical deduction only works if we have all the facts and we have a set
of axioms and we have a set of rules of inference. By deﬁnition, as we learn we
do not have those things — they’re a work in progress. This particular issue is
now known as “informal logic”.
4.1

Argumentation

Classically, there are three parts to discourse: logic, dialectic (discussing the
truth of opinions) and rhetoric. A major theme in rhetoric is argumentation,
including debates. [Note: There is no way to do justice to argumentation in such
a short paper. We present a very short overview]. Arguments are decision-making
processes that is carried out in an uncertain environment. An argument consists
of four elements: claims, evidence, inference, and warrants, themselves logical
statements. Claims are the conclusion of an argument and are not necessarily
universal truths and hence the participants values, priorities, and methods of
judgment are crucial. In order to arrive at a comon ground, the participants
must adopt
–
–
–
–

A critical thinking regimen,
A common language and semantics,
Procedural assumptions and norms
A common frame or frames of reference

Fundamental reasoning skills are essential. The Unit Cell (Fig. 2) is eﬀectively
the rationale for accepting the claim when an inferential step should be accepted.
Conclusions (claims) must be justiﬁed (warranted). Unlike formal syntactic justiﬁcations, arguments can be subjective [14], because knowledge can also be
subjective. The adoption of skepticism is required, hence critical thinking is a
key component. Knowledge has degrees of strength and is always provisional:
our understanding could change with a simple experiment.
How can we use this for 21st Century education? The subject provides a
framework to consider the development of a ﬁeld. The following classical subjects
form such a framework [15,16]:
– Evaluating Evidence. This is the study of both good and bad arguments.
This is similar to requiring students to look at models and deciding whether
they believe the evidence. The ﬁrst third of [17] emphasizes these skills.

90

D.E. Stevenson

– Understanding the Issues. Students often do not know what the issue is when
it comes to analyzing a problem. Most problems have a limited number of
issues that must be addressed, but those issues must be apparent to the
students.
– Case Construction. Case construction is the heart of argumentation, by this
I mean choosing the unit cells (Fig. 2)you wish to use. Each subject has a
limited number of standard arguments that any competent practioner understands.
– Inferences and Warrants. While most academic subjects try to use classical
logic (But not intuitionistic logic ... grist for another article), the psychological studies of human reasoning indicate humans do not naturally do so.
Formulating the statement is not the same as warranting (justifying) the
statement. Physics principles such as Conservation of Energy laws ﬁt in this
area.
– Fallacious Reasoning. Fallacies take up a large portion of argumentation
classes, but it is not clear how much time should be taken up in “applied”
classes. Some disciplines such as mathematics have famous fallacies (the pons
asinorum, for example).
Fortunately there are already examples of such approaches in computational
science.
Test Driven Development (TDD). This is a method of developing software
that adopts a test-ﬁrst approach. Instead of writing a complete program ﬁrst
and then testing the program once it has been developed, TDD has the software
programmers constantly testing the software from the ﬁrst step. Tests are created
for most or all of the individual elements of a program, such as the functions,
objects, etc.
Model-Based Techniques. Model-based testing is closely related to modelbased speciﬁcation. Models are used to describe the behavior of the system
under consideration and to guide such eﬀorts as test selection and test results
evaluation. Both testing and veriﬁcation are used to validate models against the
requirements and check that the implementation conforms to the speciﬁcation
model. Of particular importance are formal models with precise semantics, such
as state-based formalisms. Techniques to support model-based testing are drawn
from diverse areas, like formal veriﬁcation, model checking, control and data ﬂow
analysis, grammar analysis, and Markov decision processes.
Patterns as Schemata. The formal descriptions of patterns, as in [18], are
attempts to describe successful solutions to common problems. Patterns exist
across the knowledge spectrum, in all disciplines. Software patterns are a type
of schemata. Educating students to recognize patterns helps connect them with
their own knowledge rather than just resolving a problem. Not only do patterns
teach useful techniques, they help people communicate better, and they help
people reason about what they do and why.

Models as Arguments: An Approach to Computational Science Education

91

This is not very mystical: electrical engineering circuit diagrams are a type of
pattern. Recognition of patterns as decomposition tools in problem solving is a
hallmark of expertise. A person’s collection of schemata concerning a problem
form a graph representing that person’s knowledge: a language about a particular
program providing vocabulary for talking about a particular problem. Patterns
have a context in which they apply.

5

An Example

An example of how this approach can be used actually occurred recently in
one of my classes. This class has a semester-long project with seven milestones.
Starting with Milestone 2, the students must test their own code before I do.
One would think that seniors know how to think about testing such a project
— unfortunately, ours do not. Therefore, the problem to be solved is “how to
thinking about testing.”
What is a model to the problem at hand? They want to claim that they have
completely tested their code. I drew the unit cell on the board and labeled it.
I then asked the students (I use problem-based learning exclusively), “What do
you want to claim?” It took some time for them to respond, “We want you to
accept our milestones as correct.”
“What is your evidence”?
This took considerably longer before they could formulate that the evidence
was actually four separate classes of processing. The project they were working
on requires them to accept input or reject it. While they ﬁnally determined that
there were four classes of input to demonstrate, they never did determine that
they also had to show they should reject some inputs. When I pointed that out,
we have eight separate types of test inputs.
“Why should I believe you”?
Again, this took some discussion, but we ﬁnally agreed that they were presenting direct evidence and could map the speciﬁcation onto the inputs and outputs.

6

Conclusions

Early versions of this paper were laid out in a tableau format to simplify the
task of organizing the material. An interesting observation is that the tableau is
almost exactly what Euclid taught in the Elements. The connection is important,
but our main message is this:
– To develop the best in our students they must reach the “creating” level of
Bloom’s Taxonomy as quickly as possible.
– To be at the creating level means that both critical thinking and argumentation (modeling) must be emphasized.
– Modeling begins with mental modeling.
– Modeling is an inductive process.
Final Comment. It may seem odd that there is little mention of reasoning and
logic. There are several reasons for this, primarily the fact that any discussion
of these subjects is a paper unto itself.

92

D.E. Stevenson

Acknowledgement
The author wishes to acknowledge the discussions with Dr. Donald Chinn, Ms.
Jennifer Parham and the entire group working with the First Generation College
Students at Clemson: Drs. Barbara Speziale, Jeﬀrey Appling, Calvin Williams,
Robert Ballard, Matthew Ohland, and John Wagner. Ms. Sherry Dorris provided insights on how students were reacting to certain subjects. The students
in my Honors class on “Understanding Scientiﬁc Reasoning” were most helpful
by providing feedback and discussing how they actually study (or don’t).

References
1. Turing, A.: Computing machinery and intelligence. Mind LIX (Oct. 1950) 433–460
doi:10.1093/mind/LIX.236.433.
2. Stevenson, D.E.: The problem with problems in computational science and engineering problem-based learning. In: 2006 International Conference on Computational Science and Education. (Aug 2006)
3. Bushey, D.E.: Critical thinking traits of top-tier experts and implications for computer science education. PhD thesis, Clemson University, Clemson, SC (August
2007)
4. Parham, J., Chinn, D., Stevenson, D.E.: Using bloom’s taxonomy to code verbal
protocols of students solving a data structure problem. In: Proceedings of ACM
SE 2009. (2009) Submitted.
5. Walton, D.: Informal Logic: A Pragmatic Approach. Cambridge University Press,
Cambridge (2008)
6. Johnson-Laird, P.: Mental Models. Harvard University Press (1983)
7. Craik, K.: The Nature of Explanation. Cambridge University Press, Cambridge,
England (1943)
8. Davidson, J.E., Sternberg, R.J.: The Psychology of Problem Solving. Cambridge
University Press, Cambridge, UK (2003)
9. Anderson, J.: Language, Memory, and Thought. Erlbaum (1976)
10. Marshall, S.P.: Schemas in Problem Solving. Cambridge University Press (1995)
11. Flavell, J.H.: Metacognitive aspects of problem solving. In Resnick, L.B., ed.: The
Nature of Intelligence. Erlbaum, Hillsdale, NJ (1976) 231–236
12. Johnson-Laird, P.: How We Reason. Oxford University Press (2009)
13. Stenning, K., van Lambalgen, M.: Human Reasoning and Cognitive Science. Bradford Books (2008)
14. Walton, D.: Fundamentals of Critical Argumentation (Critical Reasoning and
Argumentation. Cambridge University Press (2005)
15. Freeley, A.J., Steinberg, D.L.: Argumentation and Debate: Critical Thinking for
Reasoned Decision Making. 10th edn. Wadsworth (1999)
16. Zarefsky, D.: Argumentation: The Study of Eﬀective Reasoning. The Thinking
Company (2005)
17. Giere, R.N., Bickle, J., Mauldin, R.:
Understanding Scientiﬁc Reasoning.
Wadsworth (2005)
18. Mattson, T.G., Sanders, B.A., Massingill, B.L.: Patterns for Parallel Programming.
Addison-Wesley (2005)

