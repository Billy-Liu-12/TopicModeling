Available online at www.sciencedirect.com

Procedia Computer Science 18 (2013) 621 ‚Äì 630

International Conference on Computational Science, ICCS 2013

Analysis of car crash simulation data with nonlinear machine
learning methods
Bastian Bohna , Jochen Garckea,b , Rodrigo Iza-Teranb , Alexander Paprotnyc , Benjamin
Peherstorferd,‚àó, Ulf Schepsmeiere , Clemens-August Tholeb
a Inst.

for Numerical Simulation, University of Bonn, Wegelerstr. 6, 53115 Bonn, Germany
b Fraunhofer SCAI, Schloss Birlinghoven, 53754 Sankt Augustin, Germany
c Inst. for Mathematics, TU Berlin, Stra√üe des 17. Juni 136, 10623 Berlin, Germany
d Department of Informatics, TU M¬®
unchen, Boltzmannstr. 3, 85748 Garching, Germany
e Department of Mathematics, TU M¬®
unchen, Boltzmannstr. 3, 85748 Garching, Germany

Abstract
Nowadays, product development in the car industry heavily relies on numerical simulations. For example, it is used to explore
the inÔ¨Çuence of design parameters on the weight, costs or functional properties of new car models. Car engineers spend
a considerable amount of their time analyzing these inÔ¨Çuences by inspecting the arising simulations one at a time. Here,
we propose using methods from machine learning to semi-automatically analyze the arising Ô¨Ånite element data and thereby
signiÔ¨Åcantly assist in the overall engineering process.
We combine clustering and nonlinear dimensionality reduction to show that the method is able to automatically detect
parameter dependent structure instabilities or reveal bifurcations in the time-dependent behavior of beams. In particular we
study recent nonlinear and sparse grid approaches, respectively. Our examples demonstrate the strong potential of our approach
for reducing the data analysis eÔ¨Äort in the engineering process, and emphasize the need for nonlinear methods for such tasks.
Keywords: analysis of FEM data; machine learning; car crash simulation; nonlinear dimensionality reduction; sparse grids

1. Introduction
Virtual product development based on numerical simulation is nowadays an essential tool in the car industry.
It is used to analyze the inÔ¨Çuence of design parameters on the weight, costs, functional properties, etc of new car
models [1]. We focus on car crash tests where, e.g., the plate thickness of certain components strongly inÔ¨Çuences
the intrusion of the Ô¨Åre wall into the passenger compartment or the acceleration behavior of a model.
We analyze data from car crash simulations with the help of methods from machine learning. Each instance is
a full simulation run, which nowadays consists of more than one million Ô¨Ånite element (FE) nodes and about hundred time steps, and is therefore very high dimensional, i.e. approx. 108 (time √ó nodes). During the development
process of a new car model simulations are performed where design parameters are changed by the engineer. One
simulation run takes about eight hours on a cluster with 32 cores; the number of data points consequently stays
small in comparison, nowadays up to 1,000.
‚àó Corresponding

author. Tel.: +49-89-289-18630 ; fax: +49-89-289-18607 .
E-mail address: pehersto@in.tum.de.

1877-0509 ¬© 2013 The Authors. Published by Elsevier B.V. Open access under CC BY-NC-ND license.
Selection and peer review under responsibility of the organizers of the 2013 International Conference on Computational Science
doi:10.1016/j.procs.2013.05.226

622

Bastian Bohn et al. / Procedia Computer Science 18 (2013) 621 ‚Äì 630



	


	

	

		

Fig. 1. Our workÔ¨Çow for the analysis of crash data. We cluster the Ô¨Ånite element nodes of the car model to gain a Ô¨Årst insight into the behavior
of the crash and then use nonlinear dimensionality reduction methods to reveal even more characteristics of the data.

We propose a general framework for the analysis of Ô¨Ånite element simulation data, see the workÔ¨Çow in Fig. 1.
The use of machine learning methods allows to signiÔ¨Åcantly reduce the analysis eÔ¨Äort during the engineering
process by semi-automatically revealing characteristics of the simulation runs. Currently, an engineer attempts to
identify parameter dependence by sequentially analyzing the time dependent deformations using 3D visualization
software. Thus, one needs to manually look at several hundred simulations individually to detect a trend in the
behavior of the deformations in order to classify all simulation runs, overall a very time consuming task. Therefore
the aim is to have for each simulation run only a few computed attributes left to consider, but which still describe
the car crash eÔ¨Äects in reasonable detail. One can then easily detect those simulation runs which exhibit an extreme
or exceptional behavior and analyze them further. SpeciÔ¨Åcally we investigate how changes in design parameters
can lead to a bifurcation in the behavior of a beam of the model. We show how a low-dimensional embedding of
the data of the simulation runs is able to separate them into two groups which not only divide the data by means
of diÔ¨Äerent behavior but also with respect to diÔ¨Äerent values of one of the input parameters.
Data mining methods have been used for the analysis of simulation data in [2], where Principal Component
Analysis (PCA) is used for the analysis of Ô¨Ånite element results and in [3], where a clustering method is proposed
based on local distance measures between the nodes. In [4] k-means is used on a transformed representation of
the original data set. Data analysis of branching for frontal crash simulations is treated in [5].
The Ô¨Årst step (clustering) is to obtain a Ô¨Åne-grained partition of the FE nodes of the simulation model where
we group nodes into one cluster if they exhibit similar moving patterns and similar moving intensity. Therefore
we employ clustering methods to Ô¨Ånd a partition which reÔ¨Çects the characteristics of the behavior of the FE nodes
during the simulation. With such a clustering of the nodes, it is already possible to e.g. spot regions where the
beams are crushed the most. However, in particular, the clustering can be seen as a pre-processing of the data for
the dimensionality reduction in the next step of our workÔ¨Çow. We obtain more detailed information about the crash
behavior if we apply the dimensionality reduction on each cluster separately instead on the whole data at once.
We compare the results of k-means and spectral clustering with a recently introduced sub-sampling technique
based on sparse grids. Both yield structurally similar results but in contrast to k-means spectral clustering can Ô¨Ånd
clusters with a nonconvex shape and thus is more Ô¨Çexible.
In the second step (dimensionality reduction) of the workÔ¨Çow of Fig. 1 we employ methods to obtain a low
dimensional embedding of the simulation runs which still describes most of their characteristics. Additionally, in
some situations a relation of the computed attributes to physically relevant ones is possible. The standard dimensionality reduction method is PCA, which gives a low-dimensional representation of data which are assumed to
lie in a subspace. But if the data resides in a nonlinear structure PCA gives an ineÔ¨Écient representation with too
many dimensions. Therefore, and in particular in recent years, algorithms for nonlinear dimensionality reduction,
or manifold learning, have been introduced, see e.g. [6]. The goal consists in obtaining a low-dimensional representation of the data which respects the intrinsic nonlinear geometry. Usually, it is advantageous to not only have
the low-dimensional embedding but also to be able to reconstruct, i.e. to pick a point in the low-dimensional space
and compute an approximation of the corresponding simulation run. We employ diÔ¨Äusion maps, a local PCA variant, and a principal manifold learning algorithm based on sparse grids which inherently allows for interpolation.
Finally, in the third step the reduced data is analyzed. By comparing the reconstruction errors of our nonlinear
dimensionality reduction methods with PCA we can detect if the data contains nonlinear eÔ¨Äects. Furthermore,
the comparison of the essential dimension between the clusters allows to identify starting areas of bifurcations,
since an evolving bifurcation will increase the essential dimension of the solution space. We visualize the lowdimensional embedding of the simulation runs which is expected to reÔ¨Çect the crash behavior. So-called ‚Äúslow
variables‚Äù can be obtained which are dominant with respect to the deformation of the car model. In our case the
embedding clearly consists of two groups which makes it obvious that a bifurcation in the behavior of at least one
beam of the simulation model exists.

623

Bastian Bohn et al. / Procedia Computer Science 18 (2013) 621 ‚Äì 630

(b) DiÔ¨Äerent
Ô¨Ä
bending behavior of beams during crash.

(a) Truck from below.

Fig. 2. We consider a Chevrolet C2500 pick-up truck as model problem. In (a) we show the truck from below at time step 17 (after the crash)
and the position of beams nr. 1 to 4. In (b) the four essential beams at time t17 for three simulation runs. The color bar shows the labels of the
beams (physical parts). A diÔ¨Äerent
Ô¨Ä
bending behavior in each simulation run can be recognized.

2. The WorkÔ¨Çow: Clustering, Dimensionality Reduction and Analysis
In this section we describe the steps of the workÔ¨Çow in more detail. We apply two clustering methods and partition the FE nodes with respect to moving patterns during the crash. On these clusters, dimensionality reduction
methods are applied to show the underlying characteristics of the data.
2.1. The TRUCK example
We consider a frontal crash simulation of a Chevrolet C2500 pick-up truck, a model from the National Crash
Analysis Center, with around 66,000 FE nodes and 251 physical parts (called TRUCK).
With LS-DYNA and using 17 time steps we have computed 126 simulations of the whole crash where 9 of
the underlying design parameters are varied, see Fig. 2. Although this model is more than one order of magnitude
less than nowadays used in industry (due to the large computing resources which would be necessary to generate
simulation data for current sizes), it still shows typical issues like buckling and high sensitivity to small parameter
changes. In this paper we will focus our analysis on four parts, namely the longitudinal chassis beams in the
front of the truck, see Fig. 2. These play a key role in the crash behavior and have been selected after consulting
with car engineers. Additionally, we will restrict ourselves to a subsample of time steps, whose selection will be
motivated in the presented numerical results. Overall, it takes a few minutes per time step to apply our workÔ¨Çow
on the TRUCK example given uncompressed, preprocessed data.
Let us now consider a subset of nodes, be it the whole model, a physical part, or a cluster, and denote by
M the number of nodes. With n1 , . . . , n M ‚àà N we denote the Ô¨Ånite element nodes, whose positions in the three
dimensional space are r x1ti , . . . , r xtMi ‚àà R3 , where r ‚àà {1, . . . , R = 126} is the number of the simulation run and
t ,t
ti ‚àà {1, . . . , T = 17} is the current time step. We now deÔ¨Åne the displacement r dli j between time step ti and t j for
r ti ,t j
one Ô¨Ånite element node nl and the displacement vector d of dimension D = 3M for all nodes by
r ti ,t j
dl

t

:= r xlti ‚àí r xl j ‚àà R3

r ti ,t j

d

:=

r ti ,t j T
d1

T
t ,t T

, . . . , r d Mi j

‚àà RD .

(1)

2.2. Clustering
In the Ô¨Årst step of our workÔ¨Çow we partition the nodes of the simulation model into clusters such that nodes
of a cluster behave ‚Äúsimilar‚Äù. To demonstrate why such a clustering is necessary and to describe what is meant by
‚Äúsimilar‚Äù we show in Fig. 2 the four beams for three simulation runs at time step t17 . A bifurcation in the bending
behavior in the rear of the beams over the diÔ¨Äerent
Ô¨Ä
simulations can be clearly recognized. An example even as
simple as this indicates that the original subdivision into parts hardly reÔ¨Çects similarities in terms of node behavior.
The aim of the clustering is now to divide the nodes n1 , . . . , n M into k clusters such that the displacements of the
nodes in one cluster are similar. To obtain a clustering of the nodes which reÔ¨Çects the behavior over all simulation
runs we cluster them all at once, instead of clustering each simulation run r on its own.
The inputs for our clustering algorithms are the number of clusters k ‚àà N and a data matrix X which contains
Euclidean distances of the displacements (1) for all simulation runs 1, 2, . . . , R. Thus, for each feasible pair of two
time steps ti and t j we have
t ,t
X ti ,t j = r dmi j 2
(2)
m=1,...,M, r=1,...,R

624

Bastian Bohn et al. / Procedia Computer Science 18 (2013) 621 ‚Äì 630

Each node n1 , . . . , n M corresponds to one row of the data matrix X ti ,t j .
In the following, we compute a clustering of the selected four beams (see Fig. 2) with k-means in order to
obtain benchmark results and with spectral clustering to show that a modern clustering method, capable of Ô¨Ånding
nonconvex clusters, is necessary. Because spectral clustering becomes computationally infeasible for many data
points very quickly, we employ a recently introduced sub-sampling technique based on sparse grids [7].
2.2.1. k-means
The main idea of the k-means algorithm is to minimize the mean squared distance from each data point to its
nearest centroid (center) given a set of n data points Y := {e1 , . . . , en }, e j ‚àà Rm and an integer k ‚â§ n, i.e.
n

min

arg min
Œº1 ,...,Œºk

j=1

i‚àà{1,...,k}

e j ‚àí Œºi 22 ,

where Œºi are the cluster centers. The corresponding partition into clusters S 1 , . . . , S k is then induced by the Voronoi
cells VŒºi of the centers, i.e. S i := {e j ‚àà VŒºi }, where VŒºi := {z ‚àà Rn | z ‚àí Œºi ‚â§ z ‚àí Œº j ‚àÄ j ‚àà {1, . . . , k}}. One of the
most popular heuristics for solving this problem is based on a simple iterative scheme for Ô¨Ånding a locally minimal
solution. A very eÔ¨Écient approach, which is based on Lloyd‚Äôs algorithm [8], is the Ô¨Åltering method described by
[9]. Even though the algorithm is proven to converge to a solution, it is in most cases just a local minimum.
Therefore, the algorithm is repeated several times. In our setting the repetition rate is 10. The convergence is quite
fast for a suitably chosen tolerance rate. Since most changes occur in the Ô¨Årst few steps, typically an upper bound
for the iteration steps is used which does not eÔ¨Äect the solution decisively. If the number of clusters is signiÔ¨Åcantly
smaller than the number of points k-means is eÔ¨Écient and fast.
2.2.2. Sparse-Grid-Based Sub-Sampling for Spectral Clustering
Spectral clustering has emerged as a promising technique for nonconvex clustering, e.g. it is able to Ô¨Ånd
clusters with a nonconvex shape [10]. Unfortunately, because of the eigenproblem, the computational costs are in
O(M 3 ), where M is the number of data points. Here, we approximate the eigenfunctions of the spectral clustering
operator on a sparse grid, recently introduced in [7]. Thus, the costs of the eigenproblem are in O(G3 ) rather than
O(M 3 ) where G is the number of sparse grid points which is usually distinctly smaller than the number of data
points M. Furthermore, we obtain an explicit mapping between ambient and latent space which means we can
treat out-of-sample points in a natural way. The cluster assignment of an out-of-sample point can be obtained by
simply evaluating the approximated eigenfunctions. Depending on the number of clusters k, we approximate the
eigenfunctions corresponding to the second smallest up to the k-th smallest eigenvalue.
In common out-of sample extensions, see, e.g., [11], a kernel is associated to each data point. These are then
combined in the Nystr¬®om formula to approximate the eigenfunctions. Here we employ ansatz functions associated
to sparse grid points rather than to data points, i.e. the eigenfunctions are approximated by functions
G

f =

Œ±i œÜi ‚àà V := span Œ¶,

Œ¶ := {œÜi | i = 1, . . . , G},

(3)

i=1

where Œ¶ denotes a tensor-product basis ‚Äì commonly consisting of tensorized B-spline functions ‚Äì settled on a
grid. Unfortunately, a straightforward conventional discretization with N grid points in each direction suÔ¨Äers from
the curse of dimensionality: The number of grid points is of the order O(N P ), depending exponentially on the
dimension P. For suÔ¨Éciently smooth functions, sparse grids allow a reduction of the number of grid points by
orders of magnitude to only O(N log(N)P‚àí1 ) while achieving a similar accuracy as in the full grid case [12]. Note
that sparse grids have also been successfully applied for classiÔ¨Åcation and regression tasks in data mining, see e.g.
[13, 14]. Since the cost of a sparse grid function evaluation is independent of the number of data points a cluster
assignment for an out-of-sample point can be obtained very fast, see [7] for details.
For the sparse-grid-based spectral clustering, we Ô¨Årst reduced the data to 7 dimensions with PCA (cf. Sec. 2.4)
and then computed the (Euclidean) distance matrices. We employed a sparse grid with only 799 grid points and,
thus, reduced the size of the eigenproblem from about 7000 to only 799. An exemplary comparison with the
ordinary spectral clustering has shown that this reduction by one order of magnitude does not distinctly inÔ¨Çuence
the clustering result.

625

Bastian Bohn et al. / Procedia Computer Science 18 (2013) 621 ‚Äì 630

(a) L method

(c) w.r.t. X 4,3 , k-means

(e) w.r.t. X 7,6 , k-means

(g) w.r.t. X 17,1 , k-means

(b) density-based method

(d) w.r.t. X 4,3 , spectral

(f) w.r.t. X 7,6 , spectral

(h) w.r.t. X 17,1 , spectral

Fig. 3. Both, the L-method (a) and the density-based heuristic (b), estimate between 6 to 8 clusters. In (c)-(h) clustering of beams with k-means
(top) and spectral clustering (bottom) for displacements (left to right) from t3 to t4 , from t6 to t7 and from t1 to t17 . The illustrations were
performed using the FE model at time state t1 .

2.3. Clustering Results
We now present some results for the four beams of the TRUCK example. We employ two very diÔ¨Äerent
Ô¨Ä
heuristics to estimate the number of clusters k: The statistical-based L-method [15], and a new heuristic based
on density estimation described in [16]. In Fig. 3(a) we plot an evaluation graph which shows the within sum of
squares curve. Corresponding to the L-method the x-value at the knee (intersection of the two regression lines
which Ô¨Åt the curve most closely) is an estimate of the number of clusters [15]. The density-based heuristic assumes
that the number of peaks of an estimated density function of the data is an indicator for the number of clusters.
Thus, we have to look for Ô¨Çat regions in the curve shown in Fig. 3(b), see [16] for more details. Both methods
predict between 6 to 8 clusters. Setting the number of clusters to k = 8, we are on the safe side, since for the
dimensionality reduction in the next step of our work Ô¨Çow it is very important that all nodes in one cluster have a
similar moving pattern and that clusters are not polluted with nodes which behave diÔ¨Äerently,
Ô¨Ä
cf. Sec. 2.5.2.
In Fig. 3(c)-(h) we compare the clustering obtained by k-means (top) and by spectral clustering with the sparsegrid-based sub-sampling (bottom). We see the clustering assignment of the nodes n1 , . . . , n M with respect to X 4,3
(displacements from t3 to t4 ), with respect to X 7,6 and with respect to X 17,1 . Let us Ô¨Årst consider the clustering
obtained by spectral clustering for displacements from t3 to t4 , see Fig. 3(d). There is one big cluster in the rear and
many rather small clusters in the front. This suggests that from time step t3 to t4 only the front parts are crushed.
Indeed, we could verify this by visualizing the simulation runs. We also see, that k-means yields a distinctly
diÔ¨Äerent
Ô¨Ä
cluster assignment, cf. Fig. 3(c). With the k-means method we cannot Ô¨Ånd the small clusters in the front
of the beam. This clearly shows that spectral clustering is advantageous here. For the displacements from time
step t6 to t7 , the clustering obtained by k-means and by spectral clustering looks very similar but we will see in the
following that there are diÔ¨Äerences
Ô¨Ä
with respect to the reconstruction error in the dimensionality reduction step
of our workÔ¨Çow. Notwithstanding the above, both methods yield one big cluster in the front and several smaller
ones in the rear, cf. Fig. 3(e) and Fig. 3(f). It can again be veriÔ¨Åed that this reÔ¨Çects the bending behavior of the
beams, i.e. mostly the rear of the beam bends. Now, if we consider the displacements from time step t1 to t17 we
capture the complete behavior of the crash. This is reÔ¨Çected by the clustering of both methods: We Ô¨Ånd multiple
clusters in the rear but also in the front parts of the beams. Overall, we might conclude, that the clustering seems
reasonable if compared to the actual behavior of the car during the crash.
2.4. Dimensionality Reduction
After a suitable clustering has been performed on the given data set, the resulting partitioning of data is taken
as input for common dimensionality reduction and manifold learning tools. To achieve both, a compact and
cost eÔ¨Écient description of a given simulation data set, the high-dimensional displacement vectors r dti ,t j in the
Euclidean space of dimension D = 3M have to be represented in a lower-dimensional space. To this end, the
intrinsic dimension s of the R simulation vectors has to be found.

626

Bastian Bohn et al. / Procedia Computer Science 18 (2013) 621 ‚Äì 630

The probably most common approach is principal component analysis (PCA), see e.g. [6]. But as this method
is linear, a nonlinear eÔ¨Äect can not be identiÔ¨Åed. As in our application nonlinear eÔ¨Äects emerge in form of higherorder parameter interactions, a purely linear approach does not suÔ¨Éce. Various nonlinear dimension reduction
methods have been introduced in the recent years, see [6] for an overview. In our project we employ a variant of
local tangent space alignment [17] (a simple, localized and intuitive approach), the diÔ¨Äusion map algorithm [18]
(an often successfully used spectral approach), and a principal manifold learning algorithm with sparse grids [12,
19] (to obtain an inherent out-of-sample evaluation).
For the reconstruction experiments we consider the data set r d7,6 , r = 1, . . . , 126. To allow for an evaluation
of the reconstruction results we split the data into a training and a testing set. For the latter, we pick a set
Itest ‚äÇ {1, . . . , 126} of 10 indices at random. The training set is then given by Y := {yr | r ‚àà Itrain } , Itrain :=
{1, . . . , 126} \Itest . In a pre-processing step, we center the points (i.e. training and test data) around the mean of
the training data and transform the result by a lossless PCA on the training data. Note here again that we run our
algorithms for each cluster separately. With the locally linear approximation and the principal manifold learning
algorithms described below, we construct approximations yr to yr , r ‚àà Itest .
2.4.1. Locally Linear Approximation (LLA) Based on Tangent Space Estimation
Tangent space estimation plays an important role in various manifold learning methods, see for example diffusion maps [18] or local tangent space alignment [17]. Apart from that, estimated tangent spaces may serve as
local approximations to the underlying manifold. The basic idea is as follows: the tangent space at each point
in the sample is estimated by means of a weighted, intrinsically local PCA centered at the point, where, roughly
speaking, the weights are chosen in such a manner that the larger the distance of a neighbor, the less its contribution to the error measure. SpeciÔ¨Åcally, given the intrinsic dimension s and a point y from the sample, we obtain
an estimate of the tangent space at y through the principal space of dimension s of the k ‚â• s nearest neighbors
centered at y. The nearest neighbors are with respect to geodesic distances, which we approximate by means of
graph distances in a neighborhood graph. To construct the graph, we invoke the adaptive approach proposed in
[20], which, as opposed to (say) the -rule or k-nearest neighbors [6, pp. 269-272], is capable of handling nonuniformly distributed data. As a reÔ¨Ånement, we deploy a weighted PCA for the local tangent space estimation.
SpeciÔ¨Åcally, it is intuitively clear that the closer a neighbor is to a considered point, the more it should Ô¨Ågure in
the estimation of the tangent space thereat.
Given estimates TÀú y of the tangent spaces at each sample point y ‚àà Y, an approximation to the manifold is
Àú := y‚ààY œÅy (Vy ), where Vy := {z ‚àà Rn | ||z ‚àí y|| ‚â§ ||z ‚àí yÀú || ‚àÄÀúy ‚àà Y} denotes the Voronoi cell of y with
given by M
respect to Y and
œÅy : Rn ‚Üí y + TÀú y , z ‚Üí argmin||q ‚àí z||.
q‚àày+TÀú y

Àú may be estimated as miny‚àà MÀú ||y ‚àí z|| ‚â§ ||œÅargmin||z‚àíy|| (z) ‚àí z||. Distances
The distance of an out-of-sample point z to M
y‚ààY

Àú are estimated in the same way, i.e. we set yr := œÅargmin||y‚àíyr || (yr ).
of test points yr to M
y‚ààY

2.4.2. Principal Manifold Learning with Sparse Grids
We use the general idea of principal manifold learning (PML) [21], i.e. the minimization of a regularized
error functional on a suitable function space, but in contrast to the original approach discretize this function space
by sparse grids. The main advantage of PML in contrast to most other dimensionality reduction methods is the
automatic construction of a discretized representation of the manifold. Thus, a generative model in the sense of
[6], i.e. a function f : [0, 1] s ‚Üí Rn is constructed to model the observed variables from Rn . This allows us to pick
a point in the latent space [0, 1] s and get the corresponding high dimensional point by an application of f .
In our case we look for s1 , . . . , s|Itrain | ‚àà [0, 1] s and f : [0, 1] s ‚Üí R|Itrain | from a discrete function space Vh which
minimize
|Itrain |
1
1 .
||yr ‚àí f (sr )||2 + Œª
| f ( j) |Hmix
(4)
|Itrain | r‚ààI
j=1
train

The regularization parameter Œª > 0 and the latent space dimension s ‚àà N have to be chosen suitably. f ( j) denotes
1
1
is the Hmix
semi-norm of the real-valued, s-variate function f ( j) , see [22].
the j-th component of f and | f ( j) |Hmix

627

Bastian Bohn et al. / Procedia Computer Science 18 (2013) 621 ‚Äì 630

Table 1. Error (see (5)) of PCA, PML and LLA times 102 for each Table 2. Error (see (5)) of PCA, PML and LLA times 102 for each
beam achieved after k-means.
beam achieved after spectral clustering.

Beam

1

2

3

4

Mean

Beam

1

2

3

4

Mean

PCA, s = 1
PML, s = 1
LLA, s = 1
PCA, s = 2
PML, s = 2
LLA, s = 2
PCA, s = 3
PML, s = 3
LLA, s = 3

2.97
1.14
0.50
0.79
0.44
0.43
0.45
0.33
0.36

2.46
1.76
0.86
1.19
0.82
0.73
0.74
0.59
0.65

1.64
1.20
0.49
0.89
0.43
0.44
0.47
0.47
0.38

2.58
1.85
0.82
1.57
0.73
0.70
0.79
0.68
0.58

2.41
1.49
0.67
1.11
0.61
0.58
0.61
0.51
0.49

PCA, s = 1
PML, s = 1
LLA, s = 1
PCA, s = 2
PML, s = 2
LLA, s = 2
PCA, s = 3
PML, s = 3
LLA, s = 3

1.82
1.34
0.47
1.09
0.37
0.42
0.41
0.29
0.34

2.75
2.04
0.78
1.73
0.60
0.70
0.68
0.52
0.61

1.73
1.24
0.46
0.74
0.43
0.40
0.42
0.27
0.37

2.62
1.80
0.72
1.17
0.66
0.59
0.60
0.39
0.52

2.23
1.60
0.61
1.18
0.52
0.52
0.53
0.37
0.46

The minimization is done by an alternating minimization scheme which minimizes (4) for Ô¨Åxed f Ô¨Årst and
for Ô¨Åxed s1 , . . . , s|Itrain | subsequently. This process is iterated as in an expectation-maximization scheme. For Vh
we choose a vector-valued sparse grid ansatz space with domain [0, 1] s , i.e. each component is discretized by a
sparse grid, cf. (3). Thus, the number of coeÔ¨Écients in our sparse grid ansatz space scales like O(N ¬∑ log(N) s‚àí1 ),
cf. Sec. 2.2.2. Note, that the exponential dependence only appears with respect to the latent space dimension s
and the ambient space dimension aÔ¨Äects the computational complexity only linearly. A thorough description of
PML with sparse grids and the minimization steps involved can be found in [19]. For the numerical experiments
we adaptively reÔ¨Åne the discretization spaces according to the dimension adaptive procedure described in [22].
After f is determined, we project the test data runs onto the manifold to determine the corresponding pre-image
points sr ‚àà R s for r ‚àà Itest and to calculate the approximation yr := f (sr ), for details see [19].
2.4.3. DiÔ¨Äusion Maps
As a third nonlinear approach we consider diÔ¨Äusion maps [18]. Here, Ô¨Årst a weighted graph G = (Œ©, W) is
constructed, where the vertices of the graph are the R observed data points r dti ,t j ‚àà RD . To be precise, we consider
the Euclidean norm of the displacement vectors r d1,t j at a Ô¨Åxed time step t j for the simulation runs r = 1, . . . , R.
2
The weight which is assigned to the edge between two data points u ‚àà Œ© and v ‚àà Œ© is given by we (u, v) = e‚àíŒî (u,v)/ ,
where Œî(u, v) is in general an application-speciÔ¨Åc, locally deÔ¨Åned distance measure (the kernel). A common choice
is the Gaussian kernel, which we also apply in our experiments. The value of controls the neighborhood size.
One deÔ¨Ånes a Markov process on the graph where the transition probability for going from u to v is given by
p(u, v) = we (u, v)/ z‚ààŒ© we (u, z). Hence, if the points are similar then we have a high transition probability, and if
they are not then the transition probability is low. The Markov chain can be iterated in q time steps and a so called
‚ÄúdiÔ¨Äusion distance‚Äù Dq can be deÔ¨Åned in a natural way by D2q (u, v) = z‚ààŒ© (pq (u, z) ‚àí pq (v, z))2 /œÜ0 (z), where œÜ0 (z)
is the stationary distribution of the chain.
DiÔ¨Äusion maps now embeds high-dimensional data into the low-dimensional space such that the diÔ¨Äusion
distances Dq are (approx.) preserved. It has been shown in [18] that this is accomplished by the map Œ®q :
v ‚Üí [Œªq1 œà1 (v), Œªq2 œà2 (v), ..., Œªqs œà s (v)], where Œª j and œà j are the eigenvalues and right eigenvectors of the similarity
matrix P = p(u, v)u,v . Furthermore, one can approximate the distance between transition probabilities using the
Euclidean distance in the reduced space. How many terms are used for the embedding, i.e. the dimension s of
the low-dimensional space, depends on the kernel and the value of . In case of a spectral gap, i.e. only the Ô¨Årst
eigenvalues are signiÔ¨Åcant and the rest are small, one can get a good approximation of the diÔ¨Äusion distance with
only a few terms. For details about the theoretical background of the approach we refer to [18].
Note that we also investigated a pre-image approximation to a test point in diÔ¨Äusion maps space based on
nearest neighbors as described in [23, 24], but the results were not competitive with the above nonlinear methods
and therefore omitted in this paper.

628

Bastian Bohn et al. / Procedia Computer Science 18 (2013) 621 ‚Äì 630

2.5. Analysis of Reduced Data
Due to space constraints we can only give an exemplary selection of a our experimental results here. Note
here, that we empirically conÔ¨Årmed, that overall reconstruction on four computed clusters leads to better results
than based on the four physical parts. Therefore, it is very reasonable to consider the application of a clustering
algorithm before running the dimensionality reduction algorithms. Increasing the number of clusters to eight, as
in the following results, increases the performance further.
2.5.1. Reconstruction Error per Beam
To be able to compare LLA and PML with respect to diÔ¨Äerent clustering results, yr is transformed into a
D-dimensional vector r d7,6 by means of the lossless PCA matrix determined in the pre-processing step for each
r ‚àà Itest and for each cluster. We then construct one vector r D7,6 by merging the vectors for all clusters. Thus, the
resulting vector can be evaluated in every node of the TRUCK model. We analogously construct the vector r D7,6
consisting of the vectors r d7,6 for each cluster. Finally, we measure the error of beam i by evaluating
r 7,6 2
||r D7,6
i ‚àí Di ||

1
|Itest | r‚ààI

test

2
||r D7,6
i ||

.

(5)

A sensible evaluation criterion consists in a comparison of our nonlinear approaches with a global PCA, where
we compute reconstructions as yr := argminy‚ààyc +Vs ||yr ‚àí y||, with yc denoting the mean of the training set Y, and
V s denoting the principal subspace of Y ‚àí yc of dimension s. The results for the latent space dimension s = 1, 2, 3
and the PML-regularization parameter Œª = 10‚àí1‚àís can be seen in Tab. 1 and Tab. 2. As the summands in the
regularization term increase exponentially in s an exponential decay in the regularization parameter (Œª = 10‚àí1‚àís )
turned out to be a reasonable choice and the results proved to be stable with respect to it.
Both nonlinear methods perform better than the standard PCA, although the diÔ¨Äerence between the errors
shrinks for increasing s. For s = 1 we observe a clear advantage of the LLA method over the PML approach
and the PCA. For increasing latent space dimension s however, the advantages of the more complex PML method
begin to prevail. The partition of the spectral clustering seems to allow for a slightly lower reconstruction error
than the one of the k-means algorithm in this setting.
2.5.2. Reconstruction Error per Node
Now, we want to compare the reconstruction error per node of a linear PCA approach with the LLA and PML
algorithm. The parameters are the same as in the previous section. In Fig. 4, for every node n1 , . . . , n M , we plot
1
|Itest | r‚ààI

r d 7,6
l

‚àí r dl7,6

2

test

for each of the eight clusters for k-means. Note, that the results for the spectral clustering are not shown here as
they behave similar to the ones in Fig. 4.
We observe, that the reconstruction error in the front of the four beams is always small since there is not much
variation over all simulations for the chosen time step. The performance of the LLA and PML methods in the
rear parts of the beams, where most of the bending for this time step takes place, is signiÔ¨Åcantly better than the
performance of the PCA. As the PML algorithm builds an actual reconstruction of the underlying manifold, it is
able to Ô¨Ånd the intrinsic dimension of the data. Comparing s = 2 to s = 3 here, and in Tab. 1 and 2, the PML
performance does not increase much. Therefore, it seems that two-dimensional nonlinear eÔ¨Äects appear in these
locations. Single nodes still show relatively large errors for s = 3. These nodes belong to small cluster regions
which are surrounded by nodes that belong to other clusters.
2.5.3. Latent Space Variable Separation
In the following, we compare the embedding of the displacements obtained by applying diÔ¨Äusion maps for
t j = 6 on the four given parts at once (cf. Sec. 2.1) and by applying it on the clusters determined by k-means and
spectral clustering in Sec. 2.2. Based on the spectral gap, the data has been embedded into an only two-dimensional

629

Bastian Bohn et al. / Procedia Computer Science 18 (2013) 621 ‚Äì 630

(a) PCA, s = 1

(d) PML, s = 1

(e) PML, s = 2

(b) PCA, s = 2

(f) PML, s = 3

(c) PCA, s = 3

(g) LLA, s = 1

(h) LLA, s = 2

(i) LLA, s = 3

Fig. 4. Mean of the squared l2 errors of the reconstructed displacements over all test runs for the k-means clustering at the time steps ti = 7,
t j = 6 for PCA, PML and LLA. The latent space dimension s increases from left to right. Values greater than 40 are colored red.

(a) all data

(b) cluster 3 spect. clust.

(c) cluster 4 spect. clust.

(d) cluster 1 k-means

(e) cluster 8 k-means

Fig. 5. Embedding in two dimensions w.r.t. thickness in physical part 1 for all data and clusters obtained by spectral clustering and k-means.

space. We visually investigate the embedding together with the color coded value of the design parameters which
were changed in the simulation runs 1, . . . , 126. Having the information in this form allows us to identify so-called
slow variables, i.e. variables which are dominant with respect to the behavior of the deformations. In Fig. 5 we
show how diÔ¨Äusion
Ô¨Ä
maps can detect the slow variable describing the thickness of the physical part 1. Overall, it
can be seen that by applying diÔ¨Äusion
Ô¨Ä
maps on each cluster separately, the embedding reÔ¨Çects the separation of
the simulation runs into two groups clearer than when diÔ¨Äusion
Ô¨Ä
maps is applied on the whole data set at once.
We also found out that the bending behavior of the beams of simulations within each group is similar but it
diÔ¨Äers
Ô¨Ä signiÔ¨Åcantly when comparing simulation runs from diÔ¨Ä
Ô¨Äerent groups. Therefore, we are able to detect the
bifurcation process in the bending behavior and relate it to the thickness of a beam.
3. Conclusion
By applying the presented workÔ¨Çow for the analysis of car crash data on a real-world example we have shown
that we can indeed reveal characteristics of the crash. In the Ô¨Årst step we obtained a cluster assignment of the
nodes where we grouped nodes with similar moving intensity and moving patterns into one cluster. Based on
that, we could verify that the clustering indeed captures which nodes are crushed the most at the current time step.
Additionally, the results have also shown the need for a clustering method able to Ô¨Ånd nonconvex clusters because
only the sparse-grid-based spectral clustering was able to Ô¨Ånd a reasonable clustering in all situations.
In the second step we employed nonlinear dimensionality reduction on the clusters obtained in the Ô¨Årst step.
Since the clustering already roughly reÔ¨Çects the behavior of the crash, the dimensionality reduction methods are
applied on data which is pre-processed to provide a better input than just taking all nodes at once. Our example
shows that e.g. the embedding obtained by diÔ¨Äusion
Ô¨Ä
maps on each cluster separately better reÔ¨Çects the bifurcation
in the behavior of a beam than the embedding obtained by applying the method on all data. With a locally linear
approximation based on tangent space estimation and the principal manifold learning with sparse grids we have
also shown that we can not only obtain a low-dimensional embedding of the crash data but can also quickly
reconstruct simulation runs in order to explore simulations with diÔ¨Äerent
Ô¨Ä
parameter conÔ¨Ågurations. The analysis
of the reduced data has shown that we get a better reconstruction of the simulation runs with our nonlinear methods

630

Bastian Bohn et al. / Procedia Computer Science 18 (2013) 621 ‚Äì 630

than with PCA, in particular we need less dimensions for a good reconstruction. Just as for the clustering, we see
again that nonlinear methods are advantageous.
Overall, by using modern machine learning tools we can provide assistance to car engineers in the design
process of a new car model. Using the nonlinear procedures the number of relevant parameters to be investigated
can be reduced, whereas the combination of clustering and nonlinear dimensionality reduction leads to a better
identiÔ¨Åcation of slow variables. For example, the detection of the bifurcation process with diÔ¨Äusion maps, i.e.
the separation of simulations into groups with diÔ¨Äerent bending behavior and diÔ¨Äerent input parameter values
(thicknesses), allows the engineer to easily identify a group which consists of runs with a desirable behavior in
terms of passenger safety and the corresponding parameter range. We envision that good reconstruction algorithms
can avoid the costly computation of a full numerical simulation for suitable parameter conÔ¨Ågurations, in particular
if indicators like e.g. HIC-index (Head Injury Criterion Index) or Ô¨Åre wall intrusion are of interest. Such a semiautomatic data analysis workÔ¨Çow processing bundles of simulation runs can thereby signiÔ¨Åcantly reduce the time
spent by the engineer analyzing the inÔ¨Çuences of design parameters on the weight, costs or functional properties
of new car models.
Finally, let us note that the introduced workÔ¨Çow is not limited to car crash data, but can be applied to any
collection of numerical simulations. It is worthwhile to investigate these approaches for other numerical data, e.g.
from computations for weather forecasts, oil reservoir simulations or aerodynamic simulation of aircrafts.
References
[1] D.-B. P. Haug E. T., Scharnhorst, FEM-Crash Berechnung eines Fahrzeugfrontalaufpralls, in: VDI Berichte 613, 1981.
[2] S. Ackermann, L. Gaul, M. Hanss, T. Hambrecht, Principal component analysis for detection of globally important input parameters in
nonlinear Ô¨Ånite element analysis, IAM Institute of Applied and Experimental Mechanics, Stuttgart.
[3] L. Mei, C.-A. Thole, Data analysis for parallel car-crash simulation results and model optimization, Sim. Modelling Practice and Theory
16 (3) (2008) 329‚Äì337. doi:10.1016/j.simpat.2007.11.018.
[4] C. A. Thole, L. Nikitina, I. Nikitin, T. Clees, Advanced mode analysis for crash simulation results, in: Proc. 9th LS-DYNA Forum, 2010.
[5] R. Iza-Teran, Enabling the analysis of Ô¨Ånite element simulation-bundles, Journal of Uncertainty QuantiÔ¨ÅcationAccepted.
[6] J. A. Lee, M. Verleysen, Nonlinear dimensionality reduction, Springer, 2007.
[7] B. Peherstorfer, D. PÔ¨Ç¬®uger, H.-J. Bungartz, A sparse-grid-based out-of-sample extension for dimensionality reduction and clustering
with laplacian eigenmaps, in: D. Wang, M. Reynolds (Eds.), AI 2011: Adv. ArtiÔ¨Åcial Intell., Vol. 7106 of LNCS, Springer, 2011, pp.
112‚Äì121.
[8] S. Lloyd, Least squares quantization in pcm, IEEE Trans. Information Theory 28 (2) (1982) 129‚Äì137. doi:10.1109/TIT.1982.1056489.
[9] T. Kanungo, D. M. Mount, N. S. Netanyahu, C. D. Piatko, R. Silverman, A. Y. Wu, An eÔ¨Écient k-means clustering algorithm: analysis
and implementation, IEEE Trans. Pattern Analysis and Machine Intelligence 24 (7) (2002) 881‚Äì892. doi:10.1109/TPAMI.2002.1017616.
[10] U. von Luxburg, A tutorial on spectral clustering, Statistics and Computing 17 (2007) 395‚Äì416.
[11] Y. Bengio, J. Paiement, P. Vincent, Out-of-Sample extensions for LLE, isomap, MDS, eigenmaps, and spectral clustering, in: NIPS 16,
2003, pp. 177‚Äì184.
[12] H.-J. Bungartz, M. Griebel, Sparse grids, Acta Numerica 13 (2004) 1‚Äì123.
[13] J. Garcke, Regression with the optimised combination technique, in: W. Cohen, A. Moore (Eds.), Proceedings of the 23rd ICML ‚Äô06,
ACM Press, 2006, pp. 321‚Äì328. doi:doi:10.1145/1143844.1143885.
[14] D. PÔ¨Ç¬®uger, Spatially Adaptive Sparse Grids for High-Dimensional Problems, Verlag Dr. Hut, 2010.
[15] S. Salvador, P. Chan, Determining the number of clusters/segments in hierarchical clustering/segmentation algorithms, in: 16th IEEE
Internat. Conf. on Tools with ArtiÔ¨Åcial Intelligence (ICTAI)., 2004, pp. 576 ‚Äì 584. doi:10.1109/ICTAI.2004.50.
[16] B. Peherstorfer, D. PÔ¨Ç¬®uger, H.-J. Bungartz, Clustering based on density estimation with sparse grids, in: B. Glimm, A. Kr¬®uger (Eds.), KI
2012: Adv. ArtiÔ¨Åcial Intell., Vol. 7526 of LNCS, Springer, 2012, pp. 131‚Äì142.
[17] Z. Zhang, H. Zha, Principal manifolds and nonlinear dimension reduction via local tangent space alignment, SIAM SISC 26 (1) (2004)
313‚Äì338.
[18] R. Coifman, F. Lafon, DiÔ¨Äusion maps, Applied and computational harmonic analysis 21 (2006) 5‚Äì30.
[19] C. Feuers¬®anger, M. Griebel, Principal manifold learning by sparse grids, Computing 85 (4) (2009) 267‚Äì299.
[20] J. Giesen, U. Wagner, Shape dimension and intrinsic metric from samples of manifolds with high co - dimension, in: Proc. 19th Annual
ACM Symposium on Computational Geometry (SoCG), 2003, pp. 329‚Äì337.
[21] A. J. Smola, S. Mika, B. Sch¬®olkopf, R. C. Williamson, Regularized principal manifolds, J. of Mach. Learn. Research 1 (2001) 179‚Äì209.
[22] B. Bohn, M. Griebel, An adaptive sparse grid approach for time series predictions, in: J. Garcke, M. Griebel (Eds.), Sparse grids and
applications, Vol. 88 of Lecture Notes in Computational Science and Engineering, Springer, 2012, pp. 1‚Äì30.
[23] S. M. Buchman, A. B. Lee, C. M. Schafer, High-Dimensional Density Estimation via SCA: An Example in the Modelling of Hurricane
Tracks, Statistical Methodology 8 (1) (2011) 18‚Äì30. arXiv:0907.0199.
[24] D. Kushnir, A. Haddad, R. Coifman, Anisotropic diÔ¨Äusion on sub-manifolds with application to earth structure classiÔ¨Åcation, Applied
and Computational Harmonic Analysis 32 (2) (2012) 280‚Äì294. doi:10.1016/j.acha.2011.06.002.

