Procedia Computer
Science

Procedia Computer Science 1 (2012) 801–808
Procedia Computer Science 00 (2010) 1–8

www.elsevier.com/locate/procedia

International Conference on Computational Science, ICCS 2010

Improving parallel performance of large-scale watershed
simulations
Paul R. Eller, Jing-Ru C. Cheng, Hung V. Nguyen, and Robert S. Maier
U.S. Army Engineer Research and Development Center Information Technology Laboratory 3909 Halls Ferry Road, Vicksburg, MS 39180

Abstract
A comprehensive, physics-based watershed model with multispatial domains and multitemporal scales has been
developed and used. This paper discusses interfacing the watershed model with PETSc and evaluating the model
performance for a variety of PETSc preconditioners. Both wall-clock time and scalability are compared based on
performance on the Cray XT4 machine, along with tests to verify that all solutions are producing accurate results.
The ﬁndings conclude that the PETSc Conjugate Gradient solver and preconditioners outperform the simple Conjugate Gradient solver and Jacobi preconditioner originally used by the watershed model. Tests show that the HypreBoomeramg preconditioner provides the most signiﬁcant speedup for the watershed model.

c 2012 Published by Elsevier Ltd. Open access under CC BY-NC-ND license.
⃝

Keywords: Watershed Model, MPI, PETSc, Linear Solvers, Preconditioners

1. Introduction
The parallelization of a coupled surface-subsurface watershed model has been developed and used at the U.S.
Army Engineer Research and Development Center (ERDC) for a variety of civil works projects [1]. This comprehensive, physics-based watershed model is capable of accounting for one-dimensional (1-D) cross-section-averaged
channel ﬂow, 2-D depth-averaged overland ﬂow, 3-D subsurface ﬂow, surface-subsurface interaction through inﬁltration/seepage, and other major hydrological processes treated as source/sink terms [2]. The 1-D channel ﬂow is
governed by 1-D Saint-Venant equations, which include one continuity equation and one momentum equation [3].
The 1-D diﬀusive wave ﬂow equation can be stated as
∂A ∂Q ∂A ∂(AV)
+
=
+
= SS + SR − SI + S1 + S2 ,
(1)
∂t
∂x
∂t
∂x
where A is the cross-sectional area at water depth h, V is the cross-sectionally averaged velocity, S S , S R , S I , S 1 , S 2
represent the main-induced source, rainfall, inﬁltration, and contribution from overland and subsurface ﬂows, respectively. The 2-D overland ﬂow is computed by solving the depth-averaged diﬀusive wave equation with the semiLagrangian ﬁnite element method (FEM). The governing equation can be written as
∂h
+∇·q=S +R−E−I
∂t

or

∂h
+ ∇ · [Vh] = S + R − E − I ,
∂t

(2)

where h = overland water depth[L], t = time[t], q = overland ﬂux[L3 /t/L], S = man-induced source[L3 /t/L2 ], R =
rainfall rate[L/t], E = evapotranspiration rate[L/t], I = inﬁltration rate [L/t], and V = overland ﬂow velocity[L/t]. The
c 2012 Published by Elsevier Ltd. Open access under CC BY-NC-ND license.
1877-0509 ⃝
doi:10.1016/j.procs.2010.04.086

2

ITLet/ Procedia
Computer
ScienceScience
00 (2010)
1–8 801–808
P.R. Eller
al. / Procedia
Computer
1 (2012)

802

governing equation of the 3-D subsurface ﬂow through saturated-unsaturated porous media can be derived based on
the conservation law of water mass and can be written as follows.
dθ ∂h
= ∇ · [K · (∇h + ∇z)] + q ,
dh ∂t

(3)

where θ = moisture content[L3 /L3 ], h = pressure head [L], K = the hydraulic conductivity tensor[L/t], z = the potential
head[L], and q = man-induced source [L3 /L3 /t]. Equation (3), the well-known Richards equation, is solved with the
Galerkin FEM that can be found elsewhere [4]. The ﬂuxes between surface and subsurface media are computed by
imposing continuity of ﬂuxes and state variables (e.g., overland water depth and subsurface pressure head). Thus, the
interaction must be simulated by imposing continuity of pressures and ﬂuxes as
hu = h s

and

u

Qu = Q s = n · K · (∇h s + ∇z) ,
s

(4)
u

where h is the water depth[L] on the overland or in the channel, h is the pressure head[L] in the subsurface, Q is
the ﬂux [L3 /L2 /t] from the overland or the channel to the interface, Q s is the ﬂux from the interface to the subsurface
media [L3 /L2 /t], and n is an outward unit vector of the ground surface.
To resolve diﬀerent hydrological processes eﬀectively and eﬃciently, the so-called multiframe approach is adopted.
The partitioning strategy is to have the 2- and 3-D domains partitioned independently, i.e., without any constraints, to
maintain load balancing when solving the 2- and 3-D component, respectively. Because the 1-D component owning
the spatial domain is smaller and requires more frequent computation than the other two components, each processor
owns a copy of the entire 1-D domain to avoid high communication overhead [5]. A coupler has been developed to
coordinate the interaction and communication between 2- and 3-D domains, while a diﬀerent form of coupler is required when 1-D interacts with 2- or 3-D domains [6]. DBuilder [7], a software toolkit encapsulating all the message
passing interface implementation and managing parallel data including the couplers, was developed to leverage the
work of parallel application code developers in the Department of Defense high performance computing community.
The PT (parallel particle tracking) software [8] was also integrated with DBuilder to solve the diﬀusive wave equation
using the semi-Lagrangian ﬁnite element method in the 2-D component.
The watershed model computes nonlinear iterations using the Picard method, which produces a sequence of functions that converge to a solution. The Picard method is written as follows.

x

Yn+1 (x) = y0 +

x0

Y0 (x) = y0 ,

(5a)

f (t, Yn (t))dt for n ≥ 0,

(5b)

y(x) = lim Yn (x)

(5c)

n→∞

This method transforms the initial value problem into an integral equation that is solved iteratively. This method outputs symmetric positive deﬁnite matrices, making the Conjugate Gradient method the most eﬃcient solver. Therefore,
we will be experimenting to ﬁnd the most eﬃcient preconditioner to use with the Conjugate Gradient method.
In the past, the coarse-grain performance evaluation results were presented using three meshes with diﬀerent
spatial resolutions. Each mesh associates with ﬁve test conditions: coupled 2-/3-D ﬂow with 1-D canal stages used as
modal boundary conditions; coupled 1-/3-D model with one canal, two canals, four canals, and ﬁve canals included
in the computation. Both wall-clock time and scalability were compared based on the ERDC DoD Supercomputing
Resource Center (DSRC) Cray XT3 (Sapphire, http://www.erdc.hpc.mil/hardSoft/Hardware/home/XT3) to
determine the adequacy of the parallelization algorithm implemented in the software. A more recent paper by Cheng
et al. [9] shows coarse-grained and ﬁne-grained performance evaluation on the Cray XT3 machine. This work
concluded that the 3-D subsurface ﬂow required a signiﬁcant amount of the total running time. Therefore, this work
will interface the more sophisticated parallel linear solvers and preconditioners provided by PETSc to reduce the
amount of time spent in this part of the simulation and provide results using the ERDC DSRC Cray XT4 (Jade,
http://www.erdc.hpc.mil/hardSoft/Hardware/home/XT4).
In the remainder of this paper, section 2 discusses the interface between the watershed model and PETSc. Section
3 provides a discussion of Jacobi, Block Jacobi, and Hypre-BoomerAMG preconditioners used in these experiments.

Science
00 (2010)
1–8801–808
P.R. Eller etITL
al./ /Procedia
ProcediaComputer
Computer
Science
1 (2012)

3
803

Section 4 discusses the test problem and the obtained results to compare the running time of each preconditioner and
verify that they produce accurate solutions. Section 5 provides conclusions and future work.
2. PETSc Interface
The Portable, Extensible Toolkit for Scientiﬁc Computation (PETSc) [10, 11] provides users with access to a
suite of data structures and routines for parallel scientiﬁc applications. These routines include a wide variety of fast,
scalable linear solvers and preconditioners. A majority of the running time for large 3-D watershed simulations is
spent solving linear systems, providing signiﬁcant opportunity to reduce the simulation running time. Interfacing the
watershed model with PETSc gives users access to many diﬀerent state-of-the-art linear solvers and preconditioners
that can be used to more quickly and more accurately solve linear systems.
Petsc Initialization Layout:
1. Set and compute simulation setup variables
2. Create application ordering
3. Allocate and create split matrix structure arrays
4. Create PETSc matrix using split arrays
5. Create and initialize PETSc right-hand side and solution vectors
6. Create and initialize PETSc solver and preconditioner
7. Create vector scatter using application ordering
Interfacing the watershed model with PETSc requires the development of initialization, solver, and destruction
routines. The initialization routine called once at the beginning of the simulation sets up the structure of the system
matrix, vectors, and other PETSc structures needed to solve the linear system during a single call at the beginning of
the simulation. The watershed model stores matrices in CSR format on each processor. These matrices are converted
into a split array CSR format used by PETSc. This splits a single CSR matrix on each processor into a diagonal block
matrix and an oﬀ diagonal block matrix, each of which are stored in CSR format. Next, the PETSc matrix is created
using the split arrays. The right-hand side and solution vectors are created in addition to the linear solver. The PETSc
Application Ordering software is used to create a mapping between the global index used by the watershed model and
the ordering used by the PETSc solver. Finally, a vector scatter is created that updates the ghost nodes for the ﬁnal
solution on each processor with the new values.
Petsc Solve Layout:
1. Create split arrays for matrix values
2. Set the values for the right-hand side vector
3. Set solver settings
4. Solve linear system
5. Print solver statistics (running time, residual, etc)
6. Set ﬁnal solution ghost node values using a vector scatter
A PETSc solver routine is called in place of the original solver at each time-step. This code converts the system
matrix values into the split array CSR format and then solves the linear system based on command line parameters to
set the linear solver, preconditioner, and other input parameters. Solution statistics such as the number of iterations,
residual, and solver time can be calculated. Finally, the result vector is scattered to provide each processor with
updated values of ghost nodes. A destruction routine is called at the end of the simulation to clean up the PETSc data
structures.
This code is eﬃcient because of the matrix structure placing most of the matrix entries in the diagonal block portion
of the matrix. Additionally, as much work as possible is done in the setup code, limiting the solver code to only the
operations that are needed to solve a speciﬁc matrix at each time-step. This code also limits the communication time
between processors in the solver code to the fast optimized PETSc linear solver routine and a fast vector scatter.

804

ITLet/ Procedia
Computer
ScienceScience
00 (2010)
1–8 801–808
P.R. Eller
al. / Procedia
Computer
1 (2012)

4

3. Preconditioners
The PETSc Jacobi, Block Jacobi, and Hypre-BoomerAMG preconditioners are used to solve the linear system
more quickly than the original in-house-developed Conjugate Gradient solver with Jacobi preconditioner.
Jacobi Preconditioner:
⎛
⎜⎜⎜
⎜⎜⎜
⎜⎜⎜
P = ⎜⎜⎜⎜
⎜⎜⎜
⎜⎜⎝

a11

0

0
..
.

a22
..
.
···

0

···
..
.
..
.
0

0
..
.
0
a MM

⎞
⎟⎟⎟
⎟⎟⎟
⎟⎟⎟
⎟⎟⎟
⎟⎟⎟
⎟⎟⎟
⎠

The linear system Ax = b is left preconditioned, resulting in the linear system P−1 Ax = P−1 b. The Jacobi preconditioner sets P as the diagonal of the matrix.
Block Jacobi Preconditioner:
⎛
⎜⎜⎜ A11
⎜⎜⎜
⎜⎜⎜ 0
P = ⎜⎜⎜⎜ .
⎜⎜⎜ .
⎜⎜⎝ .
0

0
A22
..
.
···

···
..
.
..
.
0

0
..
.
0
A MM

⎞
⎟⎟⎟
⎟⎟⎟
⎟⎟⎟
⎟⎟⎟
⎟⎟⎟
⎟⎟⎟
⎠

The Block Jacobi preconditioner sets P as the block diagonal of the matrix. Each subblock uses an ILU preconditioner, which computes an incomplete LU factorization of the matrix.
Hypre-BoomerAMG [12] (Scalable Linear Solvers project, http://computation.llnl.gov/casc/linear_
solvers/sls_hypre.html) is a parallel Algebraic Multigrid method based on a setup phase and a recursively deﬁned V-cycle. The setup phase creates M grids Ω1 ⊃ ... ⊃ Ω M , grid operators A1 ,...,A M , interpolation operators
k
,k=1,...,M-1, restriction operators Ikk+1 , k=1,...,M-1, and a relaxation scheme for each level.
Ik+1
Hypre BoomerAMG Setup:
1. Start with the ﬁnest grid, k=1
2. Partition grid Ωk into disjoint sets of coarse and ﬁne grid points
3. Set Ωk+1 equal to the coarse grid
k
4. Deﬁne interpolation operator Ik+1
k+1
k
5. Deﬁne restriction operator Ik = (Ik+1
)T
k+1
k+1 k k
6. Deﬁne grid operator A = Ik A Ik+1
7. If the grid Ωk+1 is small enough, stop; if not go to step 2
Starting with the ﬁnest grained mesh Ωk , k=1, the grid is partitioned into disjoint sets of coarse-grid points and
k
(interpolation operator)
ﬁne-grid points, with Ωk+1 being set equal to the coarse grid. The grid transfer operators Ik+1
k+1
k+1
and Ik (restriction operator) are then deﬁned. The grid operators A are then set using the restriction operators.
This is repeated for all M grids.
HypreBoomerAMG recursive V-cycle:
V-cycle(uk , f k )
If k=M
1. Set u M = (A M )−1 f M
Else
1. Relax Ak uk = f k v1 times
2. Set uk+1 = 0

Science
00 (2010)
1–8801–808
P.R. Eller etITL
al./ /Procedia
ProcediaComputer
Computer
Science
1 (2012)

(a) Solver Running Time

5
805

(b) Total Running Time

Figure 1: Comparison of solver running time and total running time for watershed model preconditioners.

3.
4.
5.
6.

Set f k+1 = Ikk+1 ( f k − Ak uk )
Repeat V-cycle algorithm for level k+1 by calling V-cycle(uk+1 , f k+1 )
k
uk+1
Correct solution by uk = uk + Ik+1
Relax Ak uk = f k v2 times

The V-cycle starts with the ﬁnest grid, k=1, and relaxes the linear system Ak uk = f k . Next, coarse-grid correction
is performed by initializing the value of uk+1 and setting the value of f k+1 using the restriction operator. This algorithm
is then called recursively on level k+1 using the new values for u and f by calling V-cycle(uk+1 , f k+1 ). The solution
k
. The linear system is then relaxed again. When k=M, u M is set
is then corrected using the interpolation operator Ik+1
equal to (A M )−1 f M , and the recursion is stopped.
4. Results
Tests are performed on a Cray XT4 containing 2,146 compute nodes with a 2.1-GHz AMD Opteron 64-bit quadcore processor and 8 GB of memory. Nodes are connected in a 3-D torus using a HyperTransport link to a Cray
SeaStar2 communications engine providing high-bandwidth and high-performance interconnects. The test problem
simulates a 2,124,108 node mesh over a period of 24 hours. Tests are performed on 64, 128, 256, and 512 cores. The
PETSc Conjugate Gradient solver is used along with the Jacobi, Block Jacobi, and Hypre-BoomerAMG preconditioners. These PETSc solvers are compared with the in-house Conjugate Gradient method using a Jacobi preconditioner
(ERDC2) originally used by the watershed model.
A number of parameters are set using the command line to tune the linear solver. An absolute tolerance of 10−9
is used as the stopping criteria for all simulations (“-ksp atol 1.0e-9”). The Hypre-Boomeramg simulations use some
additional parameters to tune the solver. The options line includes the parameters “-pc hypre boomeramg max iter 1
-pc hypre boomeramg tol 1.0e-9 -pc hypre boomeramg strong threshold 0.7”. The optimal values for these parameters were found through experimentation. Additional parameters can be passed using the command line to further
tune the solver if needed.
Figures 1a and 1b show that the Hypre-BoomerAMG preconditioner signiﬁcantly outperforms the other preconditioners. The Jacobi and Block Jacobi preconditioners outperform the ERDC2 solver/preconditioner by a signiﬁcant
amount. We also see that the Jacobi preconditioner performs slightly better than the Block Jacobi preconditioner for
fewer processors, and vice versa for larger numbers of processors. This is likely due to the number of blocks increasing as the number of processors increases, allowing Block Jacobi to be computed faster than Jacobi for larger numbers
of processors, but allowing Jacobi to run slightly faster for smaller numbers of processors. ERDC2 and the Jacobi
preconditioner have similar scaling. All four solvers/preconditioners scale badly after 512 processors. This is likely

806

ITLet/ Procedia
Computer
ScienceScience
00 (2010)
1–8 801–808
P.R. Eller
al. / Procedia
Computer
1 (2012)

6

Figure 2: Comparison of solver speedup for watershed model preconditioners.

Table 1: Residual Values

Preconditioner
ERDC2
Jacobi
Block Jacobi
Hypre-BoomerAMG

Preconditioned Residual
9.95193e-08
9.99522e-10
9.32504e-10
7.11265e-10

True Residual
1.209155e-05
1.385051e-05
1.579996e-04
1.636914e-05

Max Absolute Diﬀerence
N/A
3.034952e-03
1.623658e-02
9.171357e-03

due to the decreased amount of work each processor must do and the increased amount of communication between
each processor.
Figure 3a shows the number of iterations per time-step for each preconditioner, with Figure 3b showing the
number of iterations per time-step for Hypre-BoomerAMG on diﬀerent numbers of processors. There are only minor
diﬀerences between the number of iterations per time-step for the Jacobi and Block Jacobi preconditioners and the
ERDC2 solver/preconditioner. The Hypre-BoomerAMG preconditioner uses a much smaller number of iterations than
the Jacobi preconditioners at each time-step. Block Jacobi uses about half as many iterations per time-step as Jacobi,
likely because of Block Jacobi aﬀecting the block diagonal of the system matrix that contains a majority of the data.
This allows Block Jacobi to converge signiﬁcantly faster, although the increased complexity of the preconditioner
increases the running time for each iteration. Jacobi uses signiﬁcantly less iterations per time-step than ERDC2,
likely because of the PETSc implementation being more eﬃcient than the simple ERDC2 solver/preconditioner. Both
Jacobi preconditioners and ERDC2 use fewer iterations later in the simulation, while Hypre-BoomerAMG uses more
in the middle of the simulation. The Jacobi and Block Jacobi simulations for 64, 128, and 256 processors use similar
numbers of iterations per time-step. The Hypre-BoomerAMG preconditioner uses an increasing number of iterations
up to 256 processors.
Table 1 shows the preconditioned residual, true residual, and maximum absolute diﬀerence between the ﬁnal
solution for ERDC2 and each PETSc preconditioner. These values are produced by simulations using 512 processors.
This shows that all of the solvers/preconditioners produce preconditioned residual of about 10−10 and maximum
absolute diﬀerences of about 10−2 to 10−3 . The PETSc preconditioners produce a true residual of about 10−4 to 10−5 .
Overall, all four solvers/preconditioners produce similar results.

Science
00 (2010)
1–8801–808
P.R. Eller etITL
al./ /Procedia
ProcediaComputer
Computer
Science
1 (2012)

7
807

(a) Iterations per time-step for all preconditioners on 512 (b) Iterations per time-step for Hypre-BoomerAMG on 64,
processors
128, 256, and 512 processors

Figure 3: Iterations per time-step

5. Conclusions and Future Work
This work demonstrates that the PETSc Conjugate Gradient solver and preconditioners signiﬁcantly outperform
the original Conjugate Gradient solver and Jacobi preconditioner. The Hypre-BoomerAMG preconditioner solves the
linear systems the quickest and results in the fastest simulation. All of the solvers produce similar results for a given
tolerance level and see some scaling up to 512 processors.
Future work will include further studying diﬀerent preconditioners and preconditioner settings to determine if we
can produce an accurate solution in a shorter amount of time. The matrices produced throughout the simulation will
be tested to see if using diﬀerent preconditioners at diﬀerent times in the simulation will produce a faster simulation.
We will then develop an adaptive solver that can use matrix attributes to determine the best preconditioner to use to
solve the linear system in the shortest amount of time.
References
[1] J.-R. C. Cheng, R. M. Hunter, H.-P. Cheng, D. R. Richards, G. T. Yeh, Parallelization of a watershed model—Phase III: Coupled 1-dimensional
channel, 2-dimensional overland, and 3-dimensional subsurface ﬂows, in: Computational Methods in Water Resources XVI, CMWR CDROM, paper 64, Copenhagen, Denmark, June 19-22,2006.
[2] G.-T. Yeh, G. Huang, F. Zhang, H.-P. Cheng, H.-C. Lin, J.-R. Cheng, E. V. Edris, D. R. Richards, An integrated media, integrated processes
watershed model—WASH123D: Part 1—model descriptions and features, in: Computational Methods in Water Resources XVI, CMWR
CD-ROM, paper 29, Copenhagen, Denmark, June 19-22,2006.
[3] V. P. Singh, Kinematic Wave Modeling in Water Resources, John Wiley & Sons, Inc., New York, 1996.
[4] G.-T. Yeh, H.-P. Cheng, G. Huang, F. Zhang, H.-C. Lin, E. Edris, D. Richards, A numerical model of ﬂow, heat transfer, and salinity,
sediment, and water quality transport in WAterSHed systems of 1-D stream-river network, 2-D overland regime, and 3-D subsurface media
(WASH123D: Version 2.0), Tech. Rep. Draft, Engineer Research and Development Center, U.S. Army Corps of Engineers, MS (2003).
[5] J.-R. C. Cheng, R. M. Hunter, H.-P. Cheng, D. R. Richards, A parallel software development for watershed simulations, in: Computational
Science — ICCS 2005, The Springer Verlag Lecture Notes in Computer Science (LNCS 3514) Series, Springer, 2005, pp. 460–468.
[6] J.-R. C. Cheng, R. M. Hunter, D. R. Richards, Development of a parallel coupler for a watershed model, in: DoD High Performance
Computing Modernization Program, 2006 Users Group Conference, Denver, CO, June 26-29, 2006.
[7] R. M. Hunter, J.-R. C. Cheng, DBuilder: A parallel data management toolkit for scientiﬁc applications, in: Proceedings of the 2005 International Conference on Parallel and Distributed Processing Techniques and Applications (PDATA’05), CSREA Press, Las Vegas, Nevada, USA,
June 27-30, 2005, pp. 825–831.
[8] J.-R. C. Cheng, P. E. Plassmann, Parallel particle tracking framework for applications in scientiﬁc computing, The Journal of Supercomputing
28 (2004) 149–164.
[9] J.-R. C. Cheng, H. V. Nguyen, , H.-P. Cheng, D. R. Richards, Parallel performance of the coupled watershed system with multiple-spatial
domains and multiple-temporal scales, Springer Verlag Lecture Notes in Computer Science (LNCS series), NTNU, Trondheim, Norway,
13-16 May, 2008, to appear.

808

ITLet/ Procedia
Computer
ScienceScience
00 (2010)
1–8 801–808
P.R. Eller
al. / Procedia
Computer
1 (2012)

8

[10] S. Balay, K. Buschelman, W. D. Gropp, D. Kaushik, M. G. Knepley, L. C. McInnes, B. F. Smith, H. Zhang, Portable, extensible toolkit for
scientiﬁc computation, http://www.mcs.anl.gov/petsc (2009).
[11] S. Balay, K. Buschelman, V. Eijkhout, W. D. Gropp, D. Kaushik, M. G. Knepley, L. C. McInnes, B. F. Smith, H. Zhang, PETSc users manual,
Tech. Rep. ANL-95/11 - Revision 3.0.0, Argonne National Laboratory (2008).
[12] V. E. Henson, U. M. Yang, Boomeramg: a parallel algebraic multigrid solver and preconditioner, Applied Numerical Mathematics 41 (2000)
155–177.

