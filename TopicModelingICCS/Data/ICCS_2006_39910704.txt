Constrained Optimization of the Stress Function for
Multidimensional Scaling
Vydunas Saltenis
Institute of Mathematics and Informatics
Akademijos 4, LT-08663 Vilnius, Lithuania
Saltenis@ktl.mii.lt

Abstract. Multidimensional Scaling (MDS) requires the multimodal Stress
function optimization to estimate the model parameters, i.e. the coordinates of
points in a lower-dimensional space. Therefore, finding the global optimum of
the Stress function is very important for applications of MDS. The main idea of
this paper is replacing the difficult multimodal problem by a simpler unimodal
constrained optimization problem. A coplanarity measure of points is used as a
constraint while the Stress function is minimized in the original highdimensional space. Two coplanarity measures are proposed. A simple example
presented illustrates and visualizes the optimization procedure. Experimental
evaluation results with various data point sets demonstrate the potential ability
to simplify MDS algorithms avoiding multidimodality.

1 Introduction
Multidimensional scaling (MDS) [1, 2] is a widely used technique to visualize the
dissimilarity of data points. Objects (n data points) are represented as p-dimensional
vectors Y1 , … , Yn ∈ R p so that the Euclidean distances d ij (Y ) , ( i, j = 1, … , n; i < j )
between the pairs of points correspond to the given dissimilarities δ ij as closely as
possible. Only representations onto a 2-dimensional space are used (p=2) as usual,
since data visualization is the aim. In general, the dissimilarities δ ij need not be

distances between the multidimensional points.
MDS requires the multimodal Stress function optimization to estimate the model
parameters, i.e. the coordinates of points (vectors Yi ) in a lower-dimensional space.
The measure of fit is usually defined by the Stress function:
n

σ (Y ) = ∑ wij (δ ij − dij (Y )) 2 ,
i , j =1
i< j

proposed in [3]. wij are weights that may be different in various types of the Stress
function. In our investigation wij = 1 .
V.N. Alexandrov et al. (Eds.): ICCS 2006, Part I, LNCS 3991, pp. 704 – 711, 2006.
© Springer-Verlag Berlin Heidelberg 2006

Constrained Optimization of the Stress Function for Multidimensional Scaling

705

The aim of MDS is:
n

∑ (δ ij − dij (Y )) 2 .
min
n×d

Y ∈R

i , j =1
i< j

(1)

A substantial shortcoming of MDS is the existence of local minima. The examples
of proved multimodality of the Stress function are constructed (for example, [4, 5]).
The number of different local minima may range from a few to several
thousands. MDS algorithms that minimize Stress cannot guarantee a global
minimum. In general, some advice is to use multiple random starts and select
the solution with the lowest Stress value (the multiple random start method).
A lot of attempts have been made to improve search procedures by a proper choice of
start points, however all the strategies are computationally intensive.

2 Basic Idea
Let in our case the dissimilarities δ ij in (1) be the Euclidean distances d ij (X )
between the m-dimensional points (m>p) with the given coordinates X 1 ,…, X n ∈ R m

and variable vectors Z1 ,…, Z n ∈ R m as distinct from vectors Y1 , … , Yn ∈ R p in (1)

be of the same dimensionality m. Then n × m dimensional constrained minimization
problem may be formulated as:

∑ (dij ( Z ) − dij ( X )) 2
min
n×m

(2)

P( Z ) = 0 .

(3)

Z ∈R

i< j

subject to the constraint

P(Z ) in (3) is some nonnegative coplanarity measure of points Z. If the points in
an m-dimensional (high-dimensional) space lie on a hyperplane, then the coplanarity
measure must be necessarily equal to zero.
If variable coordinates Z i are equal to given coordinates X i in (2), then object
function value is equal to zero and coplanarity measure P( Z ) > 0 . These Z i values
are a start position to constrained optimization (2) and (3) when the influence of
constraint (3) is gradually increased.
The optimal coordinates Z opt of the problem (2), (3) are m-dimensional and the
distances between them d ij ( Z opt ) are the same as that between the p-dimensional
optimal coordinates Y opt obtained from (1):
d ij ( Z opt ) = d ij (Y opt ) .

706

V. Saltenis

3 Coplanarity Measures
3.1 Coplanarity Measure Based on the Volumes of Tetrahedra

One of the possible coplanarity measures is based on the volumes Vijkl of tetrahedra
whose four vertices are multidimensional points Z1 ,… , Z n . We use the sum of
squared volumes of all possible tetrahedra as coplanarity measure:
P( Z ) =

n −3 n − 2

∑ ∑

n −1

∑

n

2
,
∑ Vijkl

i =1 j = i +1 k = j +1 l = k −1

where the volume V is given by the Cayley-Menger determinant [6]:
0

1

1

1

1

0

d ij2

d il2

28 1
1

d 2ji

0

dik2
d 2jk

d ki2

d kj2

0

d kl2

1

d li2

d lj2

dlk2

0

1
2
Vijkl
=1

d 2jl .

For the simplicity, the notation d ij is used instead of d ij (Z ) .
This coplanarity measure was used in our experimental evaluation.
3.2 Coplanarity Measure Based on Point-Plane Distances

Another possible coplanarity measure is consequent upon the coplanarity definition
[7, 8]. The points Z1 ,… , Z n can be tested for coplanarity by finding the point-plane
distances of the points Z i , i = 4, … , n from the plane determined by Z1, Z 2 , Z 3 and
checking if all of them are zero. If so, all the points are coplanar.
The point-plane distance from the plane determined by three points Z1, Z 2 , Z 3 may
be computed [9] as follows:
Di = nˆ ⋅ ( Z k − Z i ) ,
where Z k is any of the three points Z1, Z 2 , Z 3 and nˆ is the unit normal

nˆ =

( Z 2 − Z1 ) × ( Z 3 − Z1 )
.
( Z 2 − Z1 ) × ( Z 3 − Z1 )

Then one of possible coplanarity measures may be introduced:
n

P( Z ) = ∑ Di2 .
i=4

The measure depends on the selection of the three points Z1, Z 2 , Z 3 .

Constrained Optimization of the Stress Function for Multidimensional Scaling

707

4 Constrained Optimization
The optimization problem (2), (3) was solved by the penalty function method [10]. A
constrained optimization problem is transformed into a sequence of unconstrained
optimization problems by modifying the objective function. In our case, we use such a
sequence of unconstrained problems:

⎛

⎞

⎝i< j

⎠

⎜ ∑ (d ( Z ) − d ( X )) 2 + r P 2 ( Z ) ⎟ , (k = 1,2, …) ,
min
ij
ij
k
⎟
n×m ⎜

Z ∈R

where k is the number of sequence, rk is a positive penalty parameter. The problem is
solved with a sequence of parameters rk tending to ∞ :
rk +1 = Δr ⋅ rk .

(4)

The modified problem can be solved by the methods of unconstrained local
optimization: Quasi-Newton and Conjugate gradient methods, in our case.
The Torgerson scaling technique [1, 11] may be used for recovery of coordinates
of dimensionality 2 from the optimal distances dij ( Z opt ) . The method yields an
analytical solution, requiring no iterations.

5 Simple Illustrative Example
In order to visualize and better understand the new approach and the optimization
procedure, a simple unidimensional MDS illustrative example was constructed. It
uses only three data points, two optimization coordinates, and (p=1).
Let the initial distances between three points be: δ12 = δ13 = 5 ; δ 23 = 6 . Only two
of the distances d13 and d 23 will be optimized. The distance d12 will be fixed:
d12 = δ12 = 5 . Then the Stress function in our case is:

σ (d13 , d 23 ) = (d13 − 5) 2 + (d 23 − 6) 2 .
In our example the coplanarity measure based on the volumes of tetrahedra reduces
to linearity measure L, which is based on the area of a triangle with side lengths d12 ,
d13 , d 23 :
L(d13 , d 23 ) = (5 + d13 + d 23 )(5 + d13 − d 23 )(5 − d13 + d 23 )(−5 + d13 + d 23 ) .
L is proportional to the square of the triangle area, calculated by Heron’s formula.
There are three local optima of the constrained optimization problem:
1. d13 = 3 ; d 23 = 8 , with a minimal value of the Stress function σ (3,8) = 8 . The
constrained local optimum is marked by point A in Fig. 1.

708

V. Saltenis

2. d13 = 2 ; d 23 = 3 , with a minimal value of the Stress function σ (2,3) = 18 .
The constrained local optimum is marked by point B.
3. d13 = 8 ; d 23 = 3 , with a minimal value of the Stress function σ (8,3) = 18 . The
constrained local optimum is marked by point C.
The global constrained optimum is the first one.
At the beginning of optimization (point O in Fig.1) the Stress function is equal to
zero, the constraint value is equal to 2304. At each step of constrained optimization,
when increasing the penalty parameter rk , the value of constraint decreases and in the
last step (point A in Fig. 1) it achieves zero value. At the same time the Stress value
increases and, in the last step, achieves the global optimum value 8.
A contour plot diagram demonstrates that, with slightly different data, the global
optimum point may be different and, consequently, the result of constrained
optimization also changes.

Fig. 1. The contour plots of the Stress function σ ( d13 , d 23 ) (dotted contour lines) and of
constraint function L( d13 , d 23 ) (solid contour lines). The points of constrained local minima
are marked as A, B, C. The start point is denoted as O, and transitional points of constrained
optimization are marked as X.

6 Experimental Evaluation
Two types of data sets were used in the experimental investigation: regular and
irregular. The points of regular data sets were the vertices of a multidimensional cube
of various dimensionality and the irregular ones were obtained randomly.

Constrained Optimization of the Stress Function for Multidimensional Scaling

709

All the results of computational experiments with the proposed algorithm were
compared with the results of global optimization obtained using usual MDS random
multistart optimization of the Stress function. The number of multistart local
optimizations was equal to 200-500.
Coplanarity measure based on the volumes of tetrahedra was used.
Table 1 presents the results of investigation with the regular data points of various
dimensionality. The average local optimization error, the number of local minima and
the probability to find a global minimum by multistart random local optimization
were evaluated from the results of multistart local optimization.
The stopping rule of the constrained optimization was: P( Z ) < 10−7 . The precision
of local optimization was 10−8 .
Two local optimization methods were compared. We can see that the interval of
successful values of Δr is greater for Quasi-Newton method. This method was used
in the investigations of Table 2-3.
Table 1. Results of investigation with the regular data points on the vertices of a
multidimensional cube of various dimensionality

Dimensionality
Number of points
Optimal Stress value
Average local optimization error in %
Number of local minima
Probability of finding a global minimum by
multistart random local optimization
Values of Δr from (4)
(Quasi-Newton method)
Values of r1 from (4)
(Quasi-Newton method)
Values of Δr from (4)
(Conjugate gradient method)
Values of r1 from (4)
(Conjugate gradient method)

3
8
2,854261
2,42
6
0.68

4
16
23,089651
0,69
7
0.74

1.1–10000000

1.1–700000

1

1

1.1 – 100

1.1 – 80

1

1

Table 2. Results of investigation with the random data points of dimensionality m=4 (number
of points n=16)

Optimal Stress value
Average local optimization error in %
Number of local minima
Probability of finding a global minimum by
multistart random local optimization
Δr value

r1 value

2,16464
25.47
10
0.25

1.77626
26.02
8
0.39

100

100

0.01

0.01

710

V. Saltenis

Tables 2-3 present two examples of numerous investigations with the random data
points. Table 2 presents the results with the random data points of dimensionality m=4
(number of points n=16). Table 3 presents the results with the random data points of
dimensionality m=6 (number of points n=20).
In all the experiments (not only presented in Tables 1-3) the proposed constrained
optimization achieved the global minimum.
Table 3. Results of investigation with the random data points of dimensionality m=6 (number
of points n=20)

Optimal Stress value
Average local optimization error in %
Number of local minima
Probability of finding a global minimum
by multistart random local optimization
Δr value
r1 value

8.16616
22.47
38
0.37

10.32240
9.29
37
0.38

100
0.01

100
0.01

The proposed optimization procedure is more computation exhaustive. The number
of variables is larger in comparison with the optimization by the usual approach. For
example, the execution time is 5 times greater in comparison to single start of the
usual approach for the data set of Table 2.

6 Conclusions
The new approach replaces the difficult multimodal optimization problem by a
simpler optimization problem that uses the constrained local optimization procedure.
It minimizes the Stress function in the original high-dimensional space subjected to
zero planarity constraint.
This approach eliminates the problem of the initial choice of variables and the
difficulties caused by the Stress function multimodality.
However, the optimization procedure is more computation exhaustive. The number
of variables is larger in comparison with the optimization by the usual approach; the
constrained optimization requires some steps.
We did not test any evaluations of the computational efficiency of the new
approach for various data, neither did we consider possible performance improvement
observations in the paper. These issues remain as a possible trend of further research.

Acknowledgements
The research was partially supported by the Lithuanian State Science and Studies
Foundation, Grant No. C 03013.

Constrained Optimization of the Stress Function for Multidimensional Scaling

711

References
1. Borg, L., Groenen, P.: Modern Multidimensional Scaling: Theory and Applications,
Springer (1997)
2. Cox, T., Cox, M.: Multidimensional Scaling, Chapman and Hall (2001)
3. Kruskal, J.: Nonmetric Multidimensional Scaling: A Numerical Method. Psychometrica,
Vol.29 (1964) 115-129
4. Trosset, M., Mathar R.: On existence of nonglobal minimizers of the STRESS Criterion
for Metric Multidimensional Scaling. Proceedings of the Statistical Computing Section,
American Statistical Association, Alexandria, VA, (1997) 158-162
5. Zilinskas, A., Podlipskyte, A.: On multimodality of the SSTRESS criterion for metric
multidimensional scaling, Informatica, Vol. 14, No. 1, (2003) 121-130
6. Sommerville, D. M. Y.: An Introduction to the Geometry of n Dimensions. New York:
Dover, (1958)
7. Abbott, P. (Ed.). In and Out: Coplanarity. Mathematica J. 9 (2004) 300-302
8. Weisstein, E. W.: Coplanar. MathWorld - A Wolfram Web Resource.
http://mathworld.wolfram.com/Coplanar.html
9. Weisstein, E. W.: Point-Plane Distance. MathWorld - A Wolfram Web Resource.
http://mathworld.wolfram.com/Point-PlaneDistance.html
10. Bertsekas, D. P.: Nonlinear programming. Athena Scientific (1999)
11. Torgerson, W. S.: Theory and methods of scaling. New York: Wiley (1958)

