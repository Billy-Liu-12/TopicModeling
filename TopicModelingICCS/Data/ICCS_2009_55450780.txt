Hardware-Accelerated Sumi-e Painting
for 3D Objects
Joo-Hyun Park, Sun-Jeong Kim, Chang-Geun Song1 , and Shin-Jin Kang2
1

Department of Computer Engineering, Hallym University, South Korea
2
Department of Games, Hongik University, South Korea

Abstract. Brushwork and ink dispersion make it diﬃcult to render 3D
common objects in the style of sumi-e painting. We use sphere mapping
with brush texture and an image processing techniques to simulate brushstrokes and ink dispersion. The whole process is implemented in shaders
running on Graphics Process Unit (GPU) that allows fast and highquality rendering 3D polygonal models in the style of sumi-e painting.
We show several results which demonstrate the practicality and beneﬁts
of our system.

1

Introduction

The sumi-e is an East Asian type of brush painting also known as ink and wash
painting. Only black ink – the same as used in East Asian calligraphy – is used,
in various concentrations. It is non-photorealistic rendering (NPR) which stands
in contrast with to conventional graphics rendering methods of photo-realistic.
The recent tendency of NPR system is simulating painting style and natural
media, e.g. pen and ink, watercolor, charcoal, pastel, hatching, etc. About sumie paintings in NPR, many researches of 2D drawing systems have been shown. In
these areas, the delicate simulations of brush, black ink and paper are presented,
and a 2D image of sumi-e painting is generated accepting the hand drawing of
the users.
There have been a number of systems for sumi-e painting brushwork and
real-time NPR rendering. Early eﬀorts in sumi-e painting focused on a brushwork simulation. Strassmann[11] swept a one dimension texture to show shading
tone. Pham[9] modeled brushstrokes based on variable oﬀset approximation of
uniform cubic B-splines. Using the theory of elasticity, Lee[7] modeled a brush as
a collection of rods with homogenous elasticity along the entire brush. Way[12]
presented a method of synthesizing rock texture in Chinese landscape painting.
With development of GPU technologies, hardware-accelerated rendering skills
began to be adapted to NPR system. Kang[4,5] modeled hardware-accelerated
rendering algorithm for generating sumi-e painting in real-time from 3D meshes.
Chu[2] worked on simulating real-time ink dispersion in absorbent paper using a
ﬂuid ﬂow. Yuan[14] developed a GPU-based rendering and animation system for
automatically generating Chinese painting cartoon from a set of mesh models.
G. Allen et al. (Eds.): ICCS 2009, Part II, LNCS 5545, pp. 780–789, 2009.
c Springer-Verlag Berlin Heidelberg 2009

Hardware-Accelerated Sumi-e Painting for 3D Objects

781

In NPR, many systems have addressed real-time NPR rendering. Majumder[8]
implemented real-time charcoal rendering applied with contrast enhanced operators by using hardware-accelerated bump mapping and Phong shading. Lake[6]
presented a method for cartoon rendering suits for programmable pipeline.
Praun[10] introduced tonal art map and showed that it permitted real-time
rendering of stroke-based texture for hatching rendering. And he also suggested
hardware hatching system with Webb[13] using volume rendering and pixel shading. Kalnins[3] described a way to interactively render stylized silhouettes of
animated 3D models with robust frame-to-frame coherence. Chi[1] developed a
system for 3D NPR stylized and abstract painterly rendering using a multi-scale
segmented sphere hierarchy.
In this paper, we present an interactive system to render 3D objects in the
style of sumi-e painting. For burshstrokes we use sphere mapping with brush
texture which represents brush style and can be changed for drawing a silhouette
in various brush styles. For the interior of 3D models, we shade it using tone
texture which can be modiﬁed to widen or narrow blank spaces in the painting.
To simulate ink dispersion, we use an image processing technique which computes
the weighted average of k-texel-away neighbors and then raises to the α-th power.
Two parameters k and α are used to control the range and density of spreading.
Finally we mix Xuan paper image with the result above to enhance the aesthetic
sense. Our system enables users to interactively control all steps by changing
brush texture, the parameter of tone texture, the value of spreading range and
density, ﬁltering techniques, and Xuan paper image. The whole rendering process
is implemented in shaders running on GPU that allows fast and high-quality
rendering. Our contributions are the follows:
– We propose two methods to simulate brushstorkes and ink dispersion. One
is sphere mapping with brush texture so that brush pattern is mapped on
a silhouette. The other is an image processing technique of the ﬁltering
controlled by the parameters of spreading range and density.
– We build an interactive rendering system by providing users with as many
parameters as possible. As a result, user can draw sumi-e painting as their
favors by determining the values of intuitive parameters.

2

Sumi-e Painting Algorithm

Our real-time system has three steps to render 3D models in the style of
sumi-e painting: silhouette outlining, interior shading, and paper eﬀect. (Fig. 1)
In the ﬁrst step, for simulating brushstrokes a brush pattern is mapped onto a
silhouette of an object using sphere mapping. In the next step, the interior area
of the object is shaded based on diﬀuse reﬂection and color obtained from object texture. The color of a pixel is determined using the tone texture for sumi-e
painting. In the last step, image processing techniques enable to produce paper
eﬀect like ink dispersion and mixing background pattern.

782

J.-H. Park et al.

Fig. 1. System overview

2.1

Silhouette Outlining

A silhouette is a view of an object or scene of the outline and a featureless
interior. A silhouette edge on a 3D object projected onto a 2D plane (display
plane) is the set of points whose outwards surface normal is perpendicular to
the view vector. A vertex v is named a silhouette vertex if its normal vector Nv
is almost perpendicular to the view vector V.
0 ≤ Nv · V ≤
where is the threshold of silhouette extraction and represents the width of
silhouette. The value of can be interactively assigned by users in our system.
The larger the number of is, the thicker the width of silhouette is. If the vertex
v is a silhouette vertex then its color is black. Otherwise its color is white.
One of diﬃculties in rendering 3D objects in the style of sumi-e painting is to
render stylized silhouettes with brushstrokes. Unfortunately the scheme above
cannot simulate brushstrokes. To achieve brush styles, we use sphere mapping
with the brush texture following Kang’s approach [4,5]. Sphere mapping can be
accelerated by current hardware and has an advantage of not requiring addition
calculation for silhouette detection. Also it can show various silhouette drawing
eﬀects easily by changing brush texture image.
In view coordinate system, we denote the vector from the vertex to the camera
ˆ . Since the computation is performed in view space, the
as v, normalized to v
camera is located at the origin and v is equal to −p, where p is the position of the
vertex in view space. The vertex normal n is transformed to view coordinates,
ˆ . The reﬂected vector r(rx , ry , rz ) can be computed as:
becoming n
ˆ )ˆ
ˆ
r = 2(ˆ
n·v
n−v
We deﬁne:
m=2

rx2 + ry2 + (rz + 1)2

Hardware-Accelerated Sumi-e Painting for 3D Objects

(a) extracted silhouette

(b) brushstrokes

783

(c) brush texture

Fig. 2. A silhouette (a) is changed into a silhouette (b) using sphere mapping with the
brush texture (c)

Then the texture coordinates (tu , tv ) are calculated as:
tu =

1
rx
+ ,
m 2

tv =

1
ry
+
m 2

(1)

Fig. 2(b) shows the silhouette produced by sphere mapping with the brush texture Fig. 2(c) . As we mentioned before, we can draw silhouettes with various
brushstrokes by changing brush texture.
2.2

Interior Shading

Our approach to interior shading is similar to cartoon shading done on the
GPU. First, we create a grayscale tone texture that contains the diﬀerent shade
intensities we desire. Fig. 3 shows the tone texture that we use in the rendering
system. The tone texture intensity must increase from left to right. To smooth
the abrupt transitions between shades, we blur the tone texture using Gaussian
function.
Then at each pixel, we perform the standard diﬀuse calculation dot product
ˆ and the light
to determine the cosine of the angle between the normal vector N
ˆ which is used to determine how much light the pixel receives:
vector L,
ˆ ·L
ˆ
s=N

Fig. 3. Tone texture

784

J.-H. Park et al.

Fig. 4. The width of white shade in the tone texture is determined by the parameter
t and simultaneously the region of blank spaces in the painting is determined

If s < 0, that implies the surface receives no light. Therefore if s < 0, we let
s = 0. So s ∈ [0, 1].
Now, we use s to scale our color vector (r, g, b, a) got from object texture so
that pixel colors are darken based on the amount of light that they receive:
diﬀuseColor = s(r, g, b, a)
Instead of using only s as the u texture coordinate for the shade texture, we compute the luminance of darken pixel color and use it as the u texture coordinate
for the tone texture. Therefore the texture coordinates (u, v) are calculated as:
u = min(s(0.3r + 0.59g + 0.11b), 1),

v = 0.5

(2)

In the sumi-e painting, the concepts of implication and simplicity result in remaining a lot of blank spaces. In other words, many parts of the inside of an
object are usually omitted. Our interactive rendering system enables to widen or
narrow blank spaces by changing continuously the ratio of white shade to total
tone texture. Fig. 4 shows that the parameter t determines the width of white
shade in the tone texture. If t > 0.5, then the area of white shade is relatively
larger than that of black shade. It makes wider blank spaces in the interior of
an object than those at t = 0.5. Otherwise, the area of white shade is relatively
smaller than that of black shade. It also makes narrower blank spaces in the
interior of an object than those at t = 0.5. If t = 1, then all the interior of
an object is painted with the white color. In Fig. 4 the bottom row shows the
examples of interior shading without the silhouette for a sphere model using the
corresponding tone texture.
2.3

Paper Eﬀect

After processes of silhouette outlining and interior shading, the color of a pixel
is computed by the multiplication of silhouette and interior colors. We render

Hardware-Accelerated Sumi-e Painting for 3D Objects

785

not to the screen, but to a texture T which is prepared for image processing and
whose resolution is same as that of the rendering window.
T (i, j) = Pixel(i, j) = silhouetteColor(i, j) ⊗ interiorColor(i, j)
where the ⊗ symbol denotes component-wise multiplication.
To simulate ink dispersion, we use an image processing technique to compute
the weighted average of k-texel-away neighbors from T (i, j) in texture space and
then draw it.
1

1

T (i + kx, j + ky)G[x + 1][y + 1]

e(i, j) =

(3)

x=−1 y=−1

where k is the range of spreading eﬀect and G[x][y] denotes a 3 × 3 Gaussian ﬁlter. If k = 1, then the spreading eﬀect is same as the result image after a 3 × 3 Gaussian ﬁltering. The larger the value of k becomes, the further
the ink is dispersed. In our rendering system, the value of α as well as k can
be interactively assigned. The input parameter α plays a role of the density of
the spreading eﬀect:
ﬁlteredColor(i, j) = pow(e(i, j), α)

(4)

where the function pow(b, n) returns bn . Because e(i, j) ∈ [0, 1], the larger
the number of α is, the darker the shade of the spreading eﬀect is. Fig. 5(a) is a
simple silhouette of a sphere model without interior shading. Using Equation (3),
the silhouette color is spreading k texels away (Fig. 5(b)). If the value of α becomes greater in Equation (4), the spreading eﬀect becomes stronger (Fig. 5(c)).
Finally Xuan paper image is transformed into background texture and mixed
with the result above by multiplication.
Pixelfinal (i, j) = T (i, j) ⊗ ﬁlteredColor(i, j) ⊗ backgroundColor(i, j)
Because the end result of our rendering system is stored in the texture, we
render it onto the screen-aligned billboard whose size is same as that of the
rendering window.

(a) silhouette

(b) ink dispersion

(c) high density

Fig. 5. Ink dispersion (b) of a silhouette (a) is simply simulated and darkened because
of high density (c)

786

3

J.-H. Park et al.

Implementation and Results

We have implemented an interactive rendering system using DirectX 9 and HLSL
(High-Level Shading Language). The whole process is developed in shaders running on GPU.
GPU Processing: Our algorithm entirely utilizes programmable GPU vertex
and pixel shaders. Because all steps use texture mapping (brush texture, tone
texture, and background texture), most operations are implemented in pixel
shader. In vertex shader, the position and normal vector of a vertex in world
space are transformed into the view coordinates, and texture coordinates pass
through it. In pixel shader, silhouette extraction, diﬀuse reﬂection, brush texture
mapping, interior shading, Gaussian ﬁltering, pulp and spreading eﬀects, and
mixing background texture are worked on.

(a) original

(b) silhouette outlining

(c) interior shading

(d) paper eﬀect + (b) + (c)

Fig. 6. Sumi-e painting of a octopus model

Hardware-Accelerated Sumi-e Painting for 3D Objects

(a) scorpion

(b) terrain

(c) prawn

(d) spider

787

Fig. 7. Sumi-e paintings generated by our system

Performance: We conducted performance test on a machine with AMD Athlon
64 Dual Core Processor 6000+ CPU and a Geforce 8600GTS GPU with 512MB
video memory. The performance data are shown in Table 1. Since half the processing time is consumed in the ﬁltering step, the performance is independent
of the number of vertices or triangles. The size of rendering window is the most
important factor in the performance. The resolution of rendering window in
Table 1 is 800 × 600. When the resolution is 400 × 300, the average of rendering
performance is 800 ∼ 850 fps. When the resolution is 1920 × 1200, the average
of rendering performance is 100 ∼ 150 fps.
Results: Fig. 2.3 shows an original model and results of each step. Fig. 7 shows
sumi-e paintings of various 3D objects. Our input is X mesh ﬁles and dds image
ﬁles for texture. Also we use 3ds MAX as the modeling tool and export models to
X ﬁles. Users interactively control the brushstroke, shade of interior, spreading
eﬀect by ﬁltering technique, and background paper in order to draw sumi-e
paintings as their favors.

788

J.-H. Park et al.
Table 1. Performance of our system
Object
Octopus
Scorpion
Terrain
Prawn
Spider

4

Vertices
64,014
7,188
16,641
6,316
31,467

Triangles Frame/sec.
60,892
452
10,000
498
32,768
363
7,292
485
62,301
454

Conclusion

This paper presented an interactive system for rendering 3D objects in the style
of sumi-e painting. For burshstrokes, we used sphere mapping with brush texture.
We could draw a silhouette in various brush styles by changing this brush texture.
For the interior of 3D models, we shaded it using tone texture which was modiﬁed
to widen or narrow blank spaces in the painting. To simulate ink dispersion, we
used an image processing technique which computes the weighted average of ktexel-away neighbors and then raises to the α-th power. Two parameters k and α
were used to control the range and density of spreading. Finally we mixed Xuan
paper image with the result above to enhance the aesthetic sense. Our system
enables users to interactively control all steps by changing brush texture, the
parameter of tone texture, the value of spreading range and exponent, ﬁltering
techniques, and Xuan paper image. The whole rendering process is implemented
in shaders running on GPU.
In future work, we want to apply our rendering system to real-time game
like cartoon rendering is already used in games. To do this we try to keep the
coherence of silhouette for animating characters and devise rendering method
for nature phenomena such as water and smoke in the style of sumi-e painting.

References
1. Chi, M., Lee, T.: Stylized and abstract painterly rendering system using a multiscale segmented sphere hierarchy. IEEE Transactions on Visualization and Computer Graphics 12(1), 61–72 (2006)
2. Chu, N., Tai, C.: MoXi: real-time ink dispersion in absorbent paper. ACM Transactions on Graphics (Proceedings of SIGGRAPH 2005) 24(3), 504–511 (2005)
3. Kalnins, R., Davidson, P., Markosian, L., Finkelstein, A.: Coherent stylized silhouettes. ACM Transactions on Graphics (Proceedings of SIGGRAPH 2003) 22(3),
856–861 (2003)
4. Kang, S.-J., Kim, S.-J., Kim, C.-H.: Hardware-accelerated real-time rendering for
3D sumi-e painting. In: Kumar, V., Gavrilova, M.L., Tan, C.J.K., L’Ecuyer, P.
(eds.) ICCSA 2003. LNCS, vol. 2667, pp. 599–608. Springer, Heidelberg (2003)
5. Kang, S.-J., Kim, C.-H.: Real-time 3D sumi-e painting. In: ACM SIGGRAPH 2003
Conference Abstracts and Applications (Technical Sketch) (July 2003)

Hardware-Accelerated Sumi-e Painting for 3D Objects

789

6. Lake, A., Marshall, C., Harris, M., Blackstein, M.: Stylized rendering techniques
for scalable real-time 3D animation. In: Proceedings of NPAR 2000: The 1st International Symposium on Non-Photorealistic Animation and Rendering, June 2000,
pp. 13–20 (2000)
7. Lee, J.: Diﬀusion rendering of black ink paintings using new paper and ink models.
Computers & Graphics 25(2), 295–308 (2001)
8. Majumder, A., Gopi, M.: Hardware accelerated real time charcoal rendering.
In: Proceedings of NPAR 2002: The 2nd International Symposium on NonPhotorealistic Animation and Rendering, June 2002, pp. 59–66 (2002)
9. Pham, B.: Expressive brush strokes. CVGIP: Graphical Models and Image Processing 53(1), 1–6 (1991)
10. Praun, E., Hoppe, H., Webb, M., Finkelstein, A.: Real-time hatching. In: Proceedings of SIGGRAPH 2001: The 28th Annual Conference on Computer Graphics and
Interactive Techniques, August 2001, pp. 581–586 (2001)
11. Strassmann, S.: Hairy brushes. Computer Graphics (Proceedings of SIGGRAPH
1987) 20(4), 225–232 (1986)
12. Way, D., Shih, Z.: The synthesis of rock textures in chinese landscape painting.
Computer Graphics Forum (Proceedings of EUROGRAPHICS 2001) 20(3), 123–
131 (2001)
13. Webb, M., Praun, E., Finkelstein, A., Hoppe, H.: Fine tone control in hardware
hatching. In: Proceedings of NPAR 2002: The 2nd International Symposium on
Non-Photorealistic Animation and Rendering, June 2002, pp. 53–58 (2002)
14. Yuan, M., Yang, X., Xiao, S., Ren, Z.: GPU-based rendering and animation for
Chinese painting cartoon. In: Proceedings of GI 2007: Graphics Interface, May
2007, pp. 57–61 (2007)

