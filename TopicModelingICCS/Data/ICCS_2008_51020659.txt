Using Metaheuristics in a Parallel Computing
Course
´
Angel-Luis
Calvo, Ana Cort´es, Domingo Gim´enez , and Carmela Pozuelo
Departamento de Inform´
atica y Sistemas, Universidad de Murcia, Spain
angelluiscalvo@gmail.com, acc8@alu.um.es, domingo@dif.um.es,
carmela@pozuelo.org

Abstract. In this paper the use of metaheuristics techniques in a parallel computing course is explained. In the practicals of the course diﬀerent
metaheuristics are used in the solution of a mapping problem in which
processes are assigned to processors in a heterogeneous environment, with
heterogeneity in computation and in the network. The parallelization of
the metaheuristics is also considered.

1

Introduction

This paper presents a teaching experience in which metaheuristic and parallel
computing studies are combined. A mapping problem is proposed to the students
in the practicals of a course of “Algorithms and parallel programming” [1]. The
problem consists of obtaining an optimum processes to processors mapping on a
heterogeneous system. The simulated systems present heterogeneity both in the
computational and network speeds, and the processes to map constitute a homogeneous set, which means a HoHe (Homogeneous processes in Heterogeneous
system) model is represented [2]. The mapping problem is NP [3]. Each student
must propose the solution of the mapping problem with some metaheuristic.
The paper is organized in the following way: section 2 explains the course
in which the experience has been carried out; section 3 presents the mapping
problem; in section 4 the application of some of the metaheuristics is explained,
including their parallelization; a test is given to the students to see how the
teaching objectives have been fulﬁlled, and the results of the test are commented
in section 5; ﬁnally, section 6 summarizes the conclusions and outlines possible
future studies.

2

Organization of the Course

The course is part of the ﬁfth year of the studies in Computer Science, at the
University of Murcia, in Spain. The students had studied Algorithms and Data
This work has been funded in part by the Consejer´ıa de Educaci´
on de la Comunidad
de Murcia, Fundaci´
on S´eneca, project number 02973/PI/05.
Corresponding author.
M. Bubak et al. (Eds.): ICCS 2008, Part II, LNCS 5102, pp. 659–668, 2008.
c Springer-Verlag Berlin Heidelberg 2008

660

´
A.-L.
Calvo et al.

Structures, Computer Architecture, Concurrent Programming and Artiﬁcial Intelligence. The course is optional, so the students are high level students who
are interested in the subject. This, together with the fact that a reduced number of students (approximately ﬁfteen per year) take the course, means that
the teaching is personalized and focused on the work of the students. They do
diﬀerent studies and practicals: preparation of a presentation about some algorithmic technique, both sequential and parallel; solution and theoretical and
experimental study of an algorithm to solve a challenging problem sequentially;
and obtaining parallel versions (in shared memory with OpenMP and in messagepassing with MPI) of the sequential algorithms. The course lasts one semester
and it has sequential and parallel parts, which means the parallelism is studied in
approximately two months. This reduced time together with the diﬃculty of an
initial approach to parallelism means that the goal is to introduce the students
to the problems and tools of parallelism, but we do not expect them to be able
to develop new algorithms and carry out detailed experiments, but they must
study and program available algorithms, to adjust them to the proposed problem, to design signiﬁcant experiments and to draw valid conclusions. Thus, the
topics of the course are: Introduction to complexity of problems, Tree traversal
methods, Probabilistic algorithms, Metaheuristics, Matricial algorithms, Models
of parallel programming, Analysis of parallel algorithms and Parallel algorithms.
First, the diﬃculties to solve some problems in a reduced time are stated.
Then, some approximate, heuristics or numerical sequential algorithms are studied, and ﬁnally, the basics of parallel programming are analysed. Each student
will develop sequential and parallel algorithms for the solution of a challenging
problem. The proposed problem is a mapping problem where a set of identical processes is assigned to processors in a heterogeneous system. So, the students tackle a challenging problem in the ﬁeld of parallel programming, and
they work with topics in two parts (sequential approximate methods and parallel computing) of the syllabus. The methods proposed to solve this problem are:
Backtracking or Branch and Bound with pruning based on heuristics (possibly
pruning nodes which would lead to the optimum solution), Backtracking with
tree traversal guided by heuristics, Probabilistic algorithms, Hill climbing, Tabu
search, Scatter search, Genetic algorithms, Ant colony, Simulated annealing and
GRASP. There are a lot of books on algorithms [5,6] and metaheuristics [7,8]
which can be consulted by the students.
Each student makes two presentations: one on the general ideas of the technique assigned, and the other on the parallelization with OpenMP and MPI of
some algorithm which implements this technique. The presentations are previous
to the practical work, so that the students can exchange ideas about some parts
of the problem (the representation of solutions and nodes, the general scheme of
the algorithms, schemes of metaheuristics, possible combinations of techniques,
...) The collaboration of the students is fostered. The experimental comparison
of the diﬀerent techniques developed by the students is positively valued in the
ﬁnal evaluation of the practical. Additionally, at least two individual tutorials
with each student would be organized, prior to each presentation.

Using Metaheuristics in a Parallel Computing Course

3

661

The Assignation Problem

The problem proposed is a simpliﬁed version of a mapping problem in which
the execution time of a parallel homogeneous algorithm (all the processes work
with the same amount of data and have the same computational cost) is used
to obtain the mapping in a heterogeneous system with which the lowest possible
execution time is achieved. The method was proposed in [9]. It is explained (simpliﬁed) to the students after the study of the topics about problem complexity,
probabilistic algorithms and metaheuristics, and the papers in which the method
was introduced and applied, along with other related papers, is made available
to students. The method is summarized below.
The execution time of a parallel algorithm is modelled as a function of some
algorithmic and system parameters [10]:
t(s) = f (s, AP, SP )

(1)

where s represents the problem size. The system parameters (SP ) represent the
characteristics of the system, and can be the cost of an arithmetic operation, the
start-up (ts ) and the word-sending (tw ) time of communications. The algorithmic
parameters (AP ) can be modiﬁed to obtain faster execution times. Some typical
parameters in homogeneous systems are the number of processors to use from
those available, or the number of rows and columns of processes.
The execution time model considered has the form:
t(s, D) = tc tcomp (s, D) + ts tstart (s, D) + tw tword(s, D)

(2)

where D represents the number of processes used in the solution of the problem,
tc the cost of a basic arithmetic operations, tcomp the number of basic arithmetic
operations, tstart the number of communications and tword the number of data
communicated. In a homogeneous system the values of tc , ts and tw are the
same in the diﬀerent processors. In a heterogeneous system it is also necessary
to select the number of processes to use and the number of processes assigned to
each processor. These numbers are stored in d = (d1 , d2 , . . . , dP ), with P being
the number of processors. The costs of a basic arithmetic operation in each one
of the processors in the system are stored in an array tc with P components,
where tci is the cost in processor i. And the costs of ts and tw between each pair
of processors are stored in two arrays ts and tw of sizes P × P , and tsij and twij
are the start-up and word-sending times from processor i to processor j.
The execution time model would be that of equation 2, but with the values
of tc , ts and tw obtained from the formulae:
tc = max{di tci }, tsij =

max {tsij }, twij =

di =0,dj =0

max {twij }

di =0,dj =0

(3)

In the model the cost of a basic operation in a processor is proportional to
the number of processes in the processor, and no interferences are considered
between processes in the same processor.

662

´
A.-L.
Calvo et al.

Obtaining an optimum mapping becomes a tree traversal problem if we consider the tree of all the possible mappings. Figure 1 shows one such tree, with
P = 3. Each level represents the possible processors to which a process can be
assigned. There is no limit to the height of the tree. Because the processes are
all equal, the tree is combinatorial, and because more than one process can be
assigned to a processor, it includes repetitions. The form of the logical tree, and
the representation of the tree or the set to work with must be decided by the
student. Each node in the solutions tree could be represented in at least two
forms. In a representation with a value for each level, the grey node in ﬁgure
1 would be stated by (1, 2, 2, . . .). Because all the processes are equal, it is also
possible to store the number of processes assigned to each processor. So, the grey
node is represented by (1, 2, 0).

Fig. 1. Tree of the mappings of identical processes in a system with three processors

Sequential and parallel algorithms are developed and studied both theoretically and experimentally. The study would include the analysis of how the use of
parallel computing contributes to reduce the execution time and/or the goodness
of the solution. In order to have comparable results they must obtain experimental results with at least the functions:
n2
p(p − 1)
n(p − 1)
+ ts
+ tw
5p
2
2
which corresponds to a parallel dynamic programming scheme [9], and:
tc

tc

2 n3
n2
+√
3 p
p

2n2
√
+ ts 2n p + tw √
p

(4)

(5)

which corresponds to a parallel LU decomposition [11]. The experiments should
be carried out with the values in the ranges: 1 < tc < 5, 4 < tw < 40 and
20 < ts < 100. Small values of ts and tw would simulate the behaviour of
shared memory multicomputers, medium values would correspond to distributed
memory multicomputers, and large values to distributed systems.

Using Metaheuristics in a Parallel Computing Course

4

663

Application of Metaheuristics to the Mapping Problem

In this section the results obtained with three of the methods are shown. Two of
the methods are metaheuristic methods (genetic algorithms and tabu search) and
the other is a backtracking with pruning based on heuristics. In the sequential
algorithms the stress is put on the high algorithmic representation which allows
us to obtain diﬀerent versions only by changing a routine in the scheme. For
metaheuristic techniques a general scheme is studied [12]. One such scheme is
shown in algorithm 1.
Algorithm 1. General scheme of a metaheuristic method
Inicialize(S);
while not EndCondition(S) do
SS =ObtainSubset(S);
if |SS| > 1 then
SS1 =Combine(SS);
else
SS1 = SS;
end
SS2 =Improve(SS1);
S =IncludeSolutions(SS2);
end

4.1

Backtracking with Node Pruning

Backtracking methods were used for this mapping problem in [9]. For the simulation of small systems, backtracking was satisfactory, but for large systems
huge assignation times were necessary. So, the work of the student was:
– For the sequential method:
• To understand the mapping problem and the increment of the assignation time when backtracking is used for large systems, which makes the
backtracking impractical in most cases.
• To develop a backtracking scheme for the proposed mapping problem.
The scheme should include a pruning routine which should be easy to
substitute to experiment with diﬀerent pruning techniques.
• To identify possible techniques to eliminate nodes which in some of the
cases would not lead to the optimum mapping. The most representative
techniques were:
PT1 The tree is searched until a maximum level, and nodes are not
pruned. This method is included as a reference which ensures the
optimum mapping.
PT2 At each step of the execution the lowest value (GLV ) of the modelled execution time of the nodes generated is stored. To decide if
a node is pruned a “minimum value” (N M V ) is associated to it.

664

´
A.-L.
Calvo et al.

When N M V > GLV the node is pruned. In a node corresponding to p processes, N M V is obtained with a greedy method. From
the execution time associated to the node, new values are obtained
by substituting in the model p by p + 1, p + 2, ... while the value
decreases. N M V is taken as the minimum of these values.
PT3 N M V is calculated in a node by substituting in the formula the value
of the number of processes for the maximum speed-up achievable. For
example, in node (0,2,0), with a tree like that in ﬁgure 1, the ﬁrst
processor will not participate in the computation, and with tc =
(1, 2, 4), the relative speed-ups would be sr = (1, 0.5, 0.25), and the
maximum achievable speed-up is 0.75.
PT4 The same value as in the previous case is used for p in the computation part, and the communication part does not vary.
• To carry out experiments to compare the results obtained with the diﬀerent pruning techniques. Initially, experiments were carried out for small
simulated systems (between 10 and 20 processors). The best results were
obtained with PT3. This technique was used in successive experiments.
The main conclusion was that for small systems backtracking with pruning can be used without a large execution time and obtaining a modelled
time not far from the optimum. For big systems, the mapping time is
too large to be applied in a real context. Parallelism could contribute to
reduce the mapping time, so making the technique applicable.
– Diﬀerent schemes were considered to obtain parallel versions, and ﬁnally a
master-slave scheme was used:
• An OpenMP version is obtained in the following way: the master generates nodes until a certain level; slaves are generated and all the threads
do backtracking from the nodes assigned cyclically to them.
• The MPI version works in the same way, but in this case the master
processor sends nodes to the slave processors and these send back the
results to the master.
• The sequential and parallel versions are compared. There is no important
variation in the modelled time. The speed-up achieved is far from the
optimum, and this is because independent backtrackings are carried out,
which means less nodes are pruned with the parallel programs.
4.2

Genetic Algorithm

Genetic algorithms are possibly the most popular metaheuristic techniques. The
students saw this technique in a previous course on Artiﬁcial Intelligence. The
work of the student was:
– For the sequential method:
• To understand the mapping problem and to identify population and
individual representations to apply genetic algorithms to the problem, to
identify the possible forms of the routines in algorithm 1 for the genetic
scheme, and to develop a genetic scheme at a high level. The scheme
must allow easy changing of some parameters or routines.

Using Metaheuristics in a Parallel Computing Course

665

• To experimentally tune the values of the parameters and the routines to
the mapping problem. The main conclusion was that to obtain a reduced
assignation time it is necessary to reduce the number of individuals and
the number of iterations, but on the other hand this reduction would
produce a reduction in the goodness of the solution. Satisfactory results
(both for the assignation time and the goodness of solution) are obtained
from experiments with 10 individuals and with convergence after 10 iterations without improvement. In any case, genetic algorithms do not
seem to be the most adequate metaheuristic for this problem.
– About the parallel versions:
• The OpenMP program works by simply parallelizing the combination of
the population.
• From diﬀerent parallel genetic schemes [13], the island scheme was selected for the message-passing version. The number of generations to
exchange information between the islands is one parameter to be tuned.
• The sequential and parallel versions are compared. With OpenMP the
same mappings are found, but with an important reduction in the assignation time. In MPI this time is not reduced substantially, but better
mappings are normally obtained.
4.3

Tabu Search

Tabu search is a local search technique which uses memory structures to guide
the search. The students saw this technique in a previous course on Artiﬁcial
Intelligence. The work of the student was:
– For the sequential method:
• To understand the mapping problem and to identify set and element
representations to apply tabu search to the problem, to identify how
the routines in algorithm 1 would be for tabu search, and to develop a
tabu search at a high level. The scheme must allow easy change of some
parameters or routines.
• To experimentally tune the values of the parameters and the routines
to the mapping problem. Satisfactory results were obtained when: the
number of iterations a movement is tabu is equal to half the number
of simulated processors (P ); the initial stage is obtained by assigning
P processes to the fastest processors; the number of iterations to begin
the diversiﬁcation phase is three quarters of the maximum number of
iterations.
– About the parallel versions:
• The OpenMP program works by selecting a number of nodes to explore
at each step equal to the number of available processors.
• For the MPI version, diﬀerent tabu techniques have been studied [14].
A pC/RS/MPDS technique was used: each process controls its own
search; knowledge is not shared by the processes; multiple initial solutions; and diﬀerent search strategies are used. To diversify the search,

666

´
A.-L.
Calvo et al.

some processes start with heuristic solutions and others with random
solutions, and the number of iterations a movement is tabu depends on
the number of the process.
• The sequential and parallel versions are compared. In OpenMP the
speedup is satisfactory. In MPI the time is not reduced substantially,
and only a small improvement in the mappings is obtained. A small reduction in the execution time can be achieved by reducing the number
of iterations.

5

Evaluating Teaching

In order to evaluate if the teaching objectives have been fulﬁlled a test has been
prepared. The test has seven statements. In some of them a high value is positive,
but in others the answer is more positive when the value is lower. In that way,
the test must be completed by reading it carefully. Each item is valued from 1 to
5, with 1 meaning total disagreement and 5 total agreement. Two questions are
analyzed in the test: is the combination of the study of sequential approximation
methods and of parallel programming appropriate? is it interesting to use a
problem to guide the teaching?
The statements in the test are:
1. It is suitable to combine the study of sequential algorithms with the study
of parallel computing because in that way two methods to solve high cost
problems are studied together.
2. The combination in a course of the study of sequential methods with parallel
programming means the course is overloaded, and it would be preferable to
have two diﬀerent courses to study the two subjects.
3. The use of a mapping problem in parallel computing and tackling the problem with heuristic methods is useful to clarify ideas about some methods
previously studied in other courses.
4. The mapping problem has made the study of parallel programming diﬃcult, because the problem deals with heterogeneous systems and the parallel
programming practicals have been done in homogeneous systems.
5. The proposal of the mapping problem at the beginning of the course has
been useful because it has motivated and guided the study of parallel programming.
6. The combination of the study of sequential methods and parallel programming has made it more diﬃcult to follow the course because it has not been
possible to study a theme in depth before beginning the next one.
7. The use of a problem to guide the teaching is appropriate because it motivates the study of each one of the themes.
Table 1 shows the positive or negative rating of each item in the test in relation
to the two questions.
The number of students in the course is low. So, no signiﬁcant conclusions
can be drawn from the answers, but some indicators can be observed. Table 2

Using Metaheuristics in a Parallel Computing Course

667

Table 1. Positive or negative rating of each item in relation with the two questions to
be evaluated
item
1 2 3 4 5 6 7
Heuristic-Parallel + - + Problem-Guided
+ -+ +

summarizes the answers. The mean value for each item is shown. Also the means
of the questions about the join study of approximation methods and parallel
programming (H-P) and about the use of a problem to guide the course (PG) are shown. To obtain the mean, in the negative items negative and positive
values are interchanged (1 for 5 and 2 for 4). The conclusion is that the two
teaching objectives have been successfully fulﬁlled.
Table 2. Mean answer for each item and question in the test
1
2
3
4
5
6
7 H-P P-G
3.57 3.14 4.43 2.71 3.57 3.14 4.00 3.40 3.82

6

Conclusions and Possible Future Studies

The paper presents a teaching experience using metaheuristics in combination
with parallel computing in a course of “Algorithms and Parallel Programming”.
With this combination the students work at the same time with two of the topics
of the course, the importance of approximate methods and heuristics is better
understood when working with a challenging problem like the one proposed,
and the diﬃculty and importance of the mapping problem is better understood
when working on the problem with a metaheuristic approach. Furthermore, parallel programming is introduced using the same metaheuristics with which the
mapping problem is tackled, and parallelism at diﬀerent levels and in shared
memory and message-passing is considered. In addition, because all the students
work with the same mapping problem, but each student works with a diﬀerent
mapping algorithm, collaboration between students and common enrichment is
fostered.
The success of the course organization has been evaluated through a test for
the students. The preliminary experience seems to be very positive, and so it
will be continued in successive courses. At the moment other mapping problems
in the ﬁeld of parallel computing are being considered.

References
1. Gim´enez, D.: Web page of the Algorithms and Parallel Programming course at the
University of Murcia, http://dis.um.es/∼ domingo/app.html
2. Kalinov, A., Lastovetsky, A.: Heterogeneous distribution of computations while
solving linear algebra problems on network of heterogeneous computers. Journal
of Parallel and Distributed Computing 61(44), 520–535 (2001)

668

´
A.-L.
Calvo et al.

3. Lennerstad, H., Lundberg, L.: Optimal scheduling results for parallel computing.
In: SIAM News, pp. 16–18. SIAM, Philadelphia (1994)
4. Brucher, P.: Scheduling Algorithms, 5th edn. Springer, Heidelberg (2007)
5. Brassard, G., Bratley, P.: Fundamentals of Algorithms. Prentice-Hall, Englewood
Cliﬀs (1996)
6. Cormen, T.H., Leiserson, C.E., Rivest, R.L.: Introduction to Algorithms. MIT
Press, Cambridge (1990)
7. Dr´eo, J., P´etrowski, A., Siarry, P., Taillard, E.: Metaheuristics for Hard Optimization. Springer, Heidelberg (2005)
8. Hromkoviˇc, J.: Algorithmics for Hard Problems, 2nd edn. Springer, Heidelberg
(2003)
9. Cuenca, J., Gim´enez, D., Mart´ınez-Gallar, J.P.: Heuristics for work distribution of
a homogeneous parallel dynamic programming scheme on heterogeneous systems.
Parallel Computing 31, 717–735 (2005)
10. Cuenca, J., Gim´enez, D., Gonz´
alez, J.: Architecture of an automatic tuned linear
algebra library. Parallel Computing 30(2), 187–220 (2004)
11. Cuenca, J., Garc´ıa, L.P., Gim´enez, D., Dongarra, J.: Processes distribution of homogeneous parallel linear algebra routines on heterogeneous clusters. In: Proc.
IEEE Int. Conf. on Cluster Computing, IEEE Computer Society Press, Los Alamitos (2005)
12. Raidl, G.R.: A uniﬁed view on hybrid metaheuristics. In: Almeida, F., Blesa Aguilera, M.J., Blum, C., Moreno Vega, J.M., P´erez P´erez, M., Roli, A., Sampels, M.
(eds.) HM 2006. LNCS, vol. 4030, pp. 1–12. Springer, Heidelberg (2006)
13. Luque, G., Alba, E., Dorronsoro, B.: Parallel genetic algorithms. In: Alba, E. (ed.)
Parallel Metaheuristics (2005)
14. Crainic, T.G., Gendreau, M., Potvin, J.Y.: Parallel tabu search. In: Alba, E. (ed.)
Parallel Metaheuristics (2005)

