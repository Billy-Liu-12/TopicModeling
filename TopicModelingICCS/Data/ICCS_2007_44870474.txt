Recognition and Optimization of Loop-Carried Stream
Reusing of Scientific Computing Applications on the
Stream Processor
Ying Zhang, Gen Li, and Xuejun Yang
Institute of Computer, National University of Defense Technology, 410073 Changsha China
zhangying@nudt.edu.cn

Abstract. Compared with other stream applications, scientific stream programs
are usually constrained by memory access. Loop-carried stream reusing means
reusing streams across different iterations and it can improve the locality of
SRF greatly. In the paper, we present algorisms to recognize loop-carried
stream reusing and give the steps to utilize the optimization after analyzing
characteristics of scientific computing applications. Then we perform several
representative microbenchmarks and scientific stream programs with and
without our optimization on Isim. Simulation results show that stream programs
optimized by loop-carried stream reusing can improve the performance of
memory-bound scientific stream programs greatly.

1 Introduction
Now conventional architecture has been not able to meet the demands of scientific
computing[1][2]. In all state-of-the-art architectures, the stream processor[3](as
shown in fig. 1) draws scientific researchers’ attentions for its processing
computation-intensive applications effectively[4-8]. Compared with other stream
applications, scientific computing applications have much more data, more complex
data access methods and more strong data dependence.
The stream processor has three level memory hierarchies[12] – local register files
(LRF) near ALUs exploiting locality in kernels, global stream register files (SRF)
exploiting producer-consumer locality between kernels, and streaming memory
system exploiting global locality. The bandwidth ratios between three level memory
hierarchies are large. In Imagine[9][10], the ratio is 1:13:218. As a result, how to
enhance the locality of SRF and LRF and consequently how to reduce the chip-off
memory traffics become key issues to improve the performance of scientific stream
programs constrained by memory access. Fig. 2 shows a stream flows across three
level memory hierarchies during the execution of a stream program. First, the stream
is loaded from chip-off memory into SRF and distributed into corresponding buffer.
Then it is loaded from SRF to LRF to supply operands to a kernel. During the
execution of the kernel, all records participating in kernel and temporary results are
saved in LRF. After the kernel is finished, the records are stored back to SRF. If there
is producer-consumer locality between this kernel and its later kernel, the stream is
saved in SRF. Otherwise, it is stored back to chip-off memory.
Y. Shi et al. (Eds.): ICCS 2007, Part I, LNCS 4487, pp. 474 – 481, 2007.
© Springer-Verlag Berlin Heidelberg 2007

Recognition and Optimization of Loop-Carried Stream

475

Loop-carried stream reusing means reusing streams across different iterations and
it can improve the locality of SRF. In the paper, we present algorisms to recognize
loop-carried stream reusing and give steps to use our method to optimize stream
programs according to the analysis of typical scientific computing applications. We
give the recognition algorism to decide what applications can be optimized and the
steps how to utilize loop-carried stream reusing to optimize stream organization. Then
we perform several representative microbenchmarks and scientific stream programs
with and without our optimization on a cycle-accurate stream processor simulator.
Simulation results show that the optimization method can improve scientific stream
program performance efficiently.

Fig. 1. Block diagram of a stream processor

Fig. 2. Stream flowing across the memory
system

2 Loop-Carried Stream Reusing
Loop-carried stream reusing is defined as that between neighboring loop iterations of
a stream-level program, input or output streams of kernels in the first iteration can be
used as input streams of kernels in the second iteration. If input streams are reused,
we call it loop-carried input stream reusing. Otherwise, we call it loop-carried output
stream reusing. The essential of stream reusing optimization is to enhance the locality
of SRF. Correspondingly, input stream reusing can enhance producer-producer
locality of SRF while output stream reusing can enhance producer-consumer locality
of SRF.

Fig. 3. Example code

Fig. 4. Data trace of array QP references

Then we take code in fig. 3 from stream program MVM as example to depict our
methods, where NXD equals to NX+2. In the paper, we let NX and NY equal to 832.

476

Y. Zhang, G. Li, and X. Yang

Fig. 4 shows the data trace of QP(L), QP(L+NXD) and QP(L-NXD) participating
in loop2 of fig. 3. QP(1668,2499) is QP(L+NXD) of loop2 when J=1, QP(L) of loop2
when J=2, and QP(L-NXD) of loop2 when J=3. So, stream QP can be reused between
different iterations of loop1. If QP(1668,2499) is organized as a stream, it will be in
SRF after loop2 with J=1 finishes. Consequently, when loop2 with J=2 or J=3
running, it doesn’t get stream QP(1668,2499) from chip-off memory but SRF.

Fig. 5. A D-level nested loop

2.1 Recognizing Loop-Carried Stream Reusing
Fig. 5 shows a generalized perfect nest of D loops. The body of the loop nest reads
elements of the m-dimensional array A twice. In the paper, we only consider linear
subscription expressions, and the ith dimension subscription expression of the first
array A reference is denoted as Fi = ∑ Ci, j * I j +Ci,0 , where Ij is an index variable,
1 ≤ j ≤ D , Ci,j is the coefficient of Ij, 1 ≤ j ≤ D , and Ci,0 is the remaining part of the
subscript expression that does not contain any index variables. Correspondingly, the
ith dimension subscription expression of the second array A reference is denoted as
Gi = ∑ C 'i , j *I j +C 'i ,0 . If in the Pth level loop, the data trace covered by the leftmost Q

dimensions1 of one array A read references in the nest with IP=i is the same to that of
the other array A read references in the nest with IP=i+d, where d is a const, and they
are different to each other in the same nest, such as the data trace of array QP in loop2
in fig. 3, the two array A read references can be optimized by input stream reusing in
respect of loop P and dimension Q. Then we give the algorism of Recognizing Loopcarried Input Stream Reusing.
RLISR. Two array references in the same loop body can be optimized by input
stream reusing in respect of loop P and dimension Q if:
(1) when M = 1 , i.e. array A is a 1-D array, the subscript expressions of two array A
references can be written as F1 = ∑ C1, j * I j +C1,0 and G1 = ∑ C '1, j *I j +C '1,0 respectively, and
Q=1 now. The coefficients of F1 and G1 should satisfy following formulas:
∀j : ((1 ≤ j ≤ D ) ∧ C1, j = C '1, j )

(a)

C1,0 = C '1,0 ± d * C '1, P

(b)

∀j : ((1 ≤ j ≤ D ) ∧ (C1, j ≥

1

D

∑ C1, j * (U j − L j ))
j =1

(c)

In this paper, we assume the sequence of memory access is the leftmost dimension first just
like as that in FORTRAN.

Recognition and Optimization of Loop-Carried Stream

477

Formula (1a) and (1b) ensure when the loop indices of outmost P-1 loops are given
and the loop body passes the space of innermost D-P loops, the data trace of one array
A read reference in the nest with IP=i is the same to that of the other array A read
references in the nest with IP=i+d. d is a const and we specify d=1 or 2 according
to[13]. When I1, …, IP-1, are given and IP, …, ID vary, formula(1c) restricts the data
trace of one array A references in the nest with IP=i from overlapping that of the other
in the nest with IP=i +d. This formula ensures the characteristic of stream process, i.e.
data as streams is loaded into the stream processor to process and reloaded into SRF
in batches after process. For the stream processor, the cost of random access records
of streams is very high.
(2) when M ≠ 1 , i.e. array A is a multi-dimensional array, the subscript expressions
of two array A read references should satisfy following conditions:
(d) the Qth dimension subscript expression of one array is gotten by translating the
index IP in the dimension subscript expression of the other array by d, i.e.
GQ = FQ ( I P ± d ) , and,
(e) all subscript expressions of one array A reference are the same with those of the
other except the Qth dimension subscript expression, i.e. ∀i ((i ≠ Q) ∧ ( Fi = Gi )) , and,
(f) for the two array A reference, the innermost index variable in one subscript
expression will not appear in any righter dimension subscript expressions, i.e.
∀i(∋ j(Ci, j ≠ 0 ∧ ∀j' ( j' > j ∧ Ci, j' = 0)) →∀i' (i' > i ∧ Ci', j = 0)) ∧ ∀i(∋ j(C'i, j ≠ 0 ∧ ∀j' ( j' > j ∧ C'i, j' = 0)) →∀i' (i' > i ∧ C'i', j = 0)) .

It can be proved that data access trace of two array references decided by condition
(2) satisfies condition (1), and when Uj-Ij is large enough, they are equivalent.
The algorism of Recognizing Loop-carried Output Stream Reusing is similar to
RLISR except that reusing stream mustn’t change original data dependence. Then we
give the RLOSR algorism without detailed specifications.
RLOSR. We denote the subscript expressions of read references as Fi and those of
write references as Gi. Two array references in loop body can be optimized by output
stream reusing in respect of loop P and dimension Q if:
(3) when M = 1 , the coefficients of F1 and G1 should satisfy following formulas:
∀j : ((1 ≤ j ≤ D ) ∧ C1, j = C '1, j )
(g)
C1,0 = C '1,0 + d * C '1, P

(h)

D

∀j : ((1 ≤ j ≤ D) ∧ (C1, j ≥ ∑ C1, j * (U j − L j ))
i =1

(i)

(4) when M ≠ 1 , the subscript expressions of two array A read references should
satisfy following formulas:
GQ = FQ ( I P + d )
(j)
∀i ((i ≠ Q) ∧ ( Fi = Gi ))

(k)

∀i(∋ j(Ci, j ≠0∧∀j'( j'> j ∧Ci, j' =0))→∀i'(i'>i ∧Ci', j =0))∧∀i(∋ j(C'i, j ≠0∧∀j'( j'> j ∧C'i, j' =0))→∀i'(i'>i ∧C'i', j =0))

(l)

2.2 Optimizing Loop-Carried Stream Reusing
Then we give the step of using input stream reusing method to optimize stream
organization.

478

Y. Zhang, G. Li, and X. Yang

Step A. Organize different array A references in the innermost D-P loops as stream
A1 and A2 according their data traces.
Step B. Organize all operations on array A in the innermost D-P loops as a kernel.
Step C. Organize all operations in the outmost P loops as stream-level program.
When the nest with IP=i of loop P in stream-level program operates on stream A1 and
A2, one of them has been loaded into SRF by the former nest, which means that the
kernel doesn’t get it from chip-off memory but SRF. From the feature of the stream
processor architecture, we can know the time to access chip-off memory is much
larger than that to access SRF, so the method of stream reusing can improve stream
program performance greatly.
The steps to use output stream reusing are analogous to steps above.
In stream program MVM unoptimized, we organize different array QP read in
loop1 as three streams according their data trace ,and. organize operations in loop1 as
a kernel The length of each stream is 832*832. When running, the stream program
must load these three streams into SRF, the total length of which is 692224*3, nearly
three times of that of array QP.
By the stream reusing method above, we organize different array QP read
references in loop2 as three streams according their own data trace, organize
operations in loop2 as a kernel, and organize operations in loop1 except loop2 as
stream-level program. Thus there would be 832*3 streams in the stream program
loop1, and the length of each is 832. So in stream program loop1, stream QP(L),
QP(L+NXD) and QP(L-NXD) of neighboring iterations can be reused. As a result,
the stream program only need load 832 streams with the length of 832 from chip-off
memory to SRF, the total length of which is 692224, nearly 1/3 of that of unoptimized
program.

3 Experiment
We compare the performance of microbenchmarks and several scientific applications
optimized and unoptimized by stream reusing. All applications are run on a cycleaccurate simulator for a single-node Imagine stream processor, Isim[9][10].
Table 1 summarizes the test programs used for evaluation. Microbenchmarks listed
in the upper half of the table stress particular aspects of loop-carried stream reusing,
e.g., if there is an input stream reusing between adjacent loop nests in respect of loop
2 and dimension 2, the benchmark is named P2Q2d1. All microbenchmarks are
stream programs of applications in fig. 6 in FORTRAN code. P2Q2d1, P3Q3d1,
P3Q3d1O and P3Q3d2 are corresponding stream programs of 6(a), 6(b), 6(c) and

Fig. 6. FORTRAN code of applications to be optimized

Recognition and Optimization of Loop-Carried Stream

479

Table 1. Benchmark programs
Name
Description
P2Q2d1 P=Q=2, d=1, and optimized by input stream reusing.
P2Q2d1l2 same application as P2Q2d1 except that we don’t optimize it by stream reusing but
organize array references of the innermost 2 loops as streams
P2Q2d1l3 same as P2Q2d1l2 except that array references of all 3 loops are organized as streams
P3Q3d1 P=Q=3, d=1, and optimized by input stream reusing
P3Q3d1O same as P3Q3d1 except that it is optimized by output stream reusing
P3Q3d2 same as P3Q3d1 except that d=2
QMR.
ab. of QMRCGSTAB, a subspace method to solve large nonsymmetric sparse linear
systems[14] whose coefficient array size is 800*800
MVM
a subroutine of a hydrodynamics application and computing band matrix
multiplication with the size of 832*832
Laplace
calculating the central difference of two-dimension array whose size is 256*256

4.E+08
3.E+08

P 2Q2d1
P 2Q2d1l2
P 2Q2d1l3

6.E+08

1.5E+08

2.E+08
1.E+08

1.E+08

1.0E+08

4.E+08

5.0E+07

2.E+08

0.0E+00

0.E+00

0.E+00

0.E+00
64 128 192 256 320
(b) Store traffics

64 128 192 256 320
(a)Load traffics

5.E+07

64 128 192 256 320
(d) Run time

64 128 192 256 320
(c) Load-store traffics

Fig. 7. With the increase of array size the performance of different stream implementations of
the application in 6(a) in respect of memory traffics(bytes) and run time(cycles)
With

P2
Q
2d
P3 1
Q
P3 3d1
Q
3d
1O
P3
Q
3d
2

0.E+00

(a) Load traffics

1.E+06

6.E+06

2.E+06

5.E+05

3.E+06

1.E+06

0.E+00

0.E+00

0.E+00

(b) Store traffics

(c) Load-store traffics

P2
Q
2d
P3 1
Q
3
P3 d1
Q
3d
1
P3 O
Q
3d
2

2.E+06

P2
Q
2d
P3 1
Q
3
P3 d1
Q
3d
1
P3 O
Q
3d
2

Without

P2
Q
2d
P3 1
Q
P3 3d1
Q
3d
1
P3 O
Q
3d
2

4.E+06

(d) Run time

Fig. 8. Performance of P2Q2d1, P3Q3d1, P3Q3d1O and P3Q3d2 with array size of 64 in
respect of memory traffics(bytes) and run time(cycles)
Without

With

4.E+07

2.E+06

2.E+11

2.E+07

1.E+06

0.E+00

0.E+00

0.E+00

4.E+11

1.4
1.3
1.2
1.1

Store Loadstore
(c) Laplace

Fig. 9. Effects of stream reusing on the memory traffics of
scientific programs

La
pl
ac
e

1.0
Load

M
V
M

Load Store Loadstore
(b) MVM

Q
M
R.

Load Store Loadstore
(a) QMR.

Fig. 10. Speedup of
scientific
programs
with stream reusing

6(d), which is optimized by loop-carried stream reusing. P2Q2d1l2 and P2Q2d1l3 are
corresponding stream programs of 6(a) without optimization. There are 2*N out of
4*N streams that can be reused as N stream in SRF in every microbenchmark except

480

Y. Zhang, G. Li, and X. Yang

P2Q2d1, in which there are 2*N2 out of 4*N2 streams that can be reused as N2 stream
in SRF . Scientific applications listed in the lower half of the table are all constrained
by memory access. 14994 out of 87467 streams in QMR. can be reused as 4998
streams in SRF, 3 out of 8 streams in MVM can be reused as 1 stream in SRF, and 3
out of 5 streams in Laplace can be reused as 1 stream in SRF.
Fig. 7 shows the performance of different stream implementations of the
application in 6(a) with the increase of array size. Fig. 7(a) shows chip-off memory
load traffics, fig. 7(b) shows store traffics, fig. 7(c) shows the total chip-off memory
traffics, and fig. 7(d) shows the run time of these implementations. In fig. 7(a), the
load traffics of P2Q2d1 are nearly 2/3 of the other two implementations whatever the
array size is. This is because input loop-carried stream reusing optimization finds the
loop-carried stream reusing, improves the locality of SRF and consequently reduces
the load memory traffics. In fig. 7(b) the store traffics of different implementations
are the same because there is only input stream reusing, which has no effect on store
traffics. From fig. 7(c), we can see that because loop-carried stream reusing reuses 2
input streams as one stream in SRF, it cut down the total memory traffics obviously.
In fig. 7(d), when the array size is 64, the run time of P2Q2d1 is larger than the other
two implementations. When the array size is 128, the run time of P2Q2d1 is a little
larger than the other two implementations. The reason for above is that when the array
size is small, the stream length of P2Q2d1 is much shorter than and the number of
streams are larger than the other two implementations. As a result, the overheads to
prepare to load streams from chip-off memory to SRF weigh so highly that they can’t
be hidden, including the time the host writes SDRs(Stream Descriptor Register) and
MARs(Memory Access Register).With the increase of the array size, the run time of
P2Q2d1 is smaller and smaller than the other two implementations. This is because
with the increase of the stream length, the overheads to load streams into SRF weigh
more and more highly and consequently the overheads to prepare loading streams can
be hidden well. The memory traffics of P2Q2d1 are the least and consequently the
P2Q2d1 program performance is the highest.
Fig. 8 shows the performance of P2Q2d1, P3Q3d1, P3Q3d1O and P3Q3d2 with
array size of 64. Fig. 8(a) shows chip-off memory load traffics, fig. 8(b) shows store
traffics, fig. 8(c) shows the total chip-off memory traffics, and fig. 8(d) shows the run
time of them. These applications are representative examples of loop-carried stream
reusing. In fig. 8(a), 8(b) and 8(c), chip-off memory load, store and total traffics have
similar characteristics as those in fig. 7. In fig. 8(d), the performance of all
applications except P2Q2d1 have been improved by stream reusing optimization. The
reason for the reduction of P2Q2d1 performance has been given above. The results
show that these representative applications optimized by loop-carried stream reusing
all get similar performance increase as that in fig. 7.
Fig. 9 shows the effects of stream reusing on the memory traffics of scientific
programs used in our experiments. Fig. 10 shows the speedup yielded by scientific
applications with stream reusing over without. All these applications are optimized by
input stream reusing. From results, we can see that because all these applications are
constrained by memory access, the improvement of application performance brought
by stream reusing is nearly in proportion to the amount of streams that can be reused.

Recognition and Optimization of Loop-Carried Stream

481

4 Conclusion and Future Work
In this paper, we give a recognition algorism to decide what applications can be optimized
and the steps how to utilize loop-carried stream reusing to optimize stream organization.
Several representative microbenchmarks and scientific stream programs with and without
our optimization are performed on Isim which is a cycle-accurate stream processor
simulator. Simulation results show that the optimization method can improve the
performance of scientific stream program constrained by memory access efficiently.
In the future, we are devoted to developing more programming optimizations to
take advantage of architectural features of the stream processor for scientific
computing applications.

References
1. W. A. Wulf, S. A. McKee. Hitting the memory wall: implications of the obvious.
Computer Architecture News, 1995. 23(1): 20-24.
2. D. Burger, J. Goodman, A. Kagi. Memory bandwidth limitations of future
microprocessors. In Proceedings of the 23rd International Symposium on Computer
Architecture, Philadelphia, PA, 1996.78-89.
3. Saman Amarasinghe, William. Stream Architectures. In PACT 2003, September 27, 2003.
4. Merrimac – Stanford Streaming Supercomputer Project, Stanford University,
http://merrimac.stanford.edu/
5. William J. Dally, Patrick Hanrahan, et al., "Merrimac: Supercomputing with Streams",
SC2003, November 2003, Phoenix, Arizona.
6. Mattan Erez, Jung Ho Ahn, et al., "Merrimac - Supercomputing with Streams",
Proceedings of the 2004 SIGGRAPH GP^2 Workshop on General Purpose Computing on
Graphics Processors, June 2004, Los Angeles, California.
7. Wang Guibin, Tang Yuhua, et al., "Application and Study of Scientific Computing on
Stream Processor", Advances on Computer Architecture (ACA’06), August 2006,
Chengdu, China.
8. Du Jing, Yang Xuejun, et al., "Implementation and Evaluation of Scientific Computing
Programs on Imagine", Advances on Computer Architecture (ACA’06), August 2006,
Chengdu, China.
9. MScott Rixner, Stream Processor Architecture. Kluwer Academic Publishers. Boston,
MA, 2001.
10. Peter Mattson. A Programming System for the Imagine Media Processor. Dept. of
Electrical Engineering. Ph.D. thesis, Stanford University, 2002
11. 11.Ola Johnsson, Magnus Stenemo, Zain ul-Abdin. Programming & Implementation of
Streaming Applications. Master’s thesis, Computer and Electrical Engineering Halmstad
University, 2005.
12. Ujval J. Kapasi, Scott Rixner, et al., Programmable Stream Processor, IEEE Computer,
August 2003
13. Goff, G., Kennedy, K., and Tseng, C.-W. 1991. Practical dependence testing. In
Proceedings of the SIGPLAN '91 Conference on Programming Language Design and
Implementation. ACM, New York.
14. A Quasi-Minimal Residual Variant Of The Bi-Cgstab Algorithm For Nonsymmetric
Systems (1994) T. F. Chan, E. Gallopoulos, V. Simoncini, T. Szeto, C. H. TongSIAM
Journal on Scientific Computing.

