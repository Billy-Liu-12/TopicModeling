Self-Organizing Hybrid Neurofuzzy Networks
1

1

1

Sung-Kwun Oh , Su-Chong Joo , Chang-Won Jeong , and Hyun-Ki Kim

2

1

School of Electrical, Electronic and Information Engineering, Wonkwang University,
South Korea
{ohsk, scjoo, mediblue}@wonkwang.ac.kr
2

Department of Electrical Engineering, University of Suwon, South Korea
{hkkim}@mail.suwon.ac.kr

Abstract. We introduce a concept of self-organizing Hybrid Neurofuzzy Networks (HNFN), a hybrid modeling architecture combining neurofuzzy (NF) and
polynomial neural networks(PNN). The development of the Self-organizing
HNFN dwells on the technologies of Computational Intelligence (CI), namely
fuzzy sets, neural networks, and genetic algorithms. The architecture of the
Self-organizing HNFN results from a synergistic usage of NF and PNN. NF
contribute to the formation of the premise part of the rule-based structure of the
Self-organizing HNFN. The consequence part of the Self-organizing HNFN is
designed using Self-organizing PNN. We also distinguish between two types of
the Self-organizing HNFN architecture showing how this taxonomy depends on
connection points. Owing to the specific features of two combined architectures, it is possible to consider the nonlinear characteristics of process and to
get output performance with superb predictive ability. The performance of the
Self-organizing HNFN is quantified through experimentation that exploits standard data already used in fuzzy modeling. These results reveal superiority of the
proposed networks over the existing fuzzy models.

1 Introductory Remarks
With the continuously growing demand for models for complex systems inherently
associated with nonlinearity, high-order dynamics, time-varying behavior, and imprecise measurements there is a need for a relevant modeling environment. Efficient
modeling techniques should allow for a selection of pertinent variables and a formation of highly representative datasets. The models should be able to take advantage of
the existing domain knowledge (such as a prior experience of human observers or
operators) and augment it by available numeric data to form a coherent dataknowledge modeling entity. The omnipresent modeling tendency is the one that exploits techniques of CI by embracing fuzzy modeling, neurocomputing, and genetic
optimization. In this study, we develop a hybrid modeling architecture, called Selforganizing Hybrid Neurofuzzy Networks (HNFN). In a nutshell, Self-organizing
HNFN is composed of two main substructures, namely a neurofuzzy (NF) and a polynomial neural network(PNN). From a standpoint of rule-based architectures, one can
P.M.A. Sloot et al. (Eds.): ICCS 2003, LNCS 2660, pp. 877–885, 2003.
© Springer-Verlag Berlin Heidelberg 2003

878

S.-K. Oh et al.

regard the NF as an implementation of the antecedent part of the rules while the consequents (conclusion parts) are realized with the aid of PNN. The role of the NF is to
interact with input data, granulate the corresponding input spaces (viz. converting the
numeric data into representations at the level of fuzzy sets). NF has two connection
points for combination with PNN. By considering two types of connection methods,
PNN can be used effectively and organized dynamically (through a growth process) to
meet the required approximation error. The role of the PNN is to carry out nonlinear
transformation at the level of the fuzzy sets (and corresponding membership grades)
formed at the level of NF. The PNN is constructed on a basis of a Group Method of
Data Handling (GMDH [1]) method and is a flexible and versatile structure [9]. In this
network, the number of layers and number of nodes in each layer are not predetermined (unlike in case of most neural-networks) but can be generated in a dynamic
fashion. To assess the performance of the proposed model, we exploit a nonlinear
function. Furthermore, the network is directly contrasted with several existing fuzzy
models.

2 The Architecture and Development of Self-Organizing HNFN
In this section, we elaborate on the architecture and a design process of the Selforganizing HNFN. These networks result as a synergy between two other general
constructs such as NF [2] and PNN [9].
2.1 The Architecture of Self-Organizing HNFN
The topology of the Self-organizing HNFN is constructed by combining NF for the
premise part of the Self-organizing HNFN with PNN being used as the consequence
part of Self-organizing HNFN. As visualized in Fig. 1, NF can be designed by using
space partitioning in terms of individual input variables. We are concerned with a
granulation carried out in terms of fuzzy sets defined in each input variable.
µij(xi)
x1

p

w11

aij

p

∑

fi(xi)

p

xi

p

wi1

p

wij

p

win

∑

aij

y^

∑
e BBT

e BBS

Fig. 1. NF structure with two connection points of interaction with PNN

Self-Organizing Hybrid Neurofuzzy Networks

879

Also, NF structure has two possible connection points. The location of this point
implies the character of the network (viz. its flexibility and learning capabilities). Note
that the first connection point allows perceiving each linguistic manifestation of the
original variables (viz. these variables are transformed by fuzzy sets and normalized).
The location of the other connection point implies that the PNN part of the network
does not “see” the individual fuzzy sets in the input space. We use PNN in the consequence structure of the Self-organizing HNFN. The PNN algorithm based on the
GMDH method can produce an optimal nonlinear system by selecting significant input
variables among dozens of these and forming various types of polynomials. PNN is
used in selecting the best ones in partial descriptions (PDs) according to a discrimination criterion. PDs use regression polynomials, Table 1. Successive layers of the Selforganizing HNFN are generated until we reach a structure of the best performance.
Table 1. Types of regression polynomial

The following types of the polynomials are used
• Bilinear = c0 + c1 x1 + c2 x2
• Biquadratic-1 = Bilinear+ c3 x12 + c4 x22 + c5 x1 x2
• Biquadratic-2 = Bilinear +c3 x1 x2
The Self-organizing HNFN is an architecture combined with the NF and PNN as
shown in Fig. 2. These networks result as a synergy between two other general constructs such as NF and PNN. According to the alternative of two connection points,
combination of NF and PNN is implemented for generation of Self-organizing HNFN
architecture such as Fig. 2. Also each type of Self-organizing HNFN includes two
architectures, namely basic and modified architectures. Moreover, for each architecture of the topology we identify two cases.
• Generic type of Self-organizing HNFN;Combination of NF and PNN by connection
point 1
• Advanced type of Self-organizing HNFN; Combination of NF and PNN by connection point 2
(a) Basic architecture – The number of input variables of PDs of PNN is same in
every layer.
(b) Modified architecture – The number of input variables of PDs of PNN is different in each layer.
Case 1. The polynomial order of PDs of PNN is same in every layer.
nd
Case 2. The polynomial order of PDs in the 2 layer or higher of PNN has a different or modified.

880

S.-K. Oh et al.
Premise part(NF)

x1

x2

p w

A11

Premise part(NF)

Consequence part(PNN)

Layer 2
Layer 4
Layer 3
Layer 1
Layer 1

PD
PD

aij
PD

PD
PD

A12

p

PD
PD

PD

A21

p

PD

PD

PD

PD

A22

p

PD

PD

Consequence part(PNN)

Layer 2
Layer 4
Layer 3
Layer 1
Layer 1
Layer k

x1
•••

PD
PD
PD

^y

x2

PD

p w

A11

PD
PD

aij
PD

PD

Layer k

p

PD

PD

A12

PD
PD

PD

• • • PD

A21

p

PD

PD

PD

PD

A22

p

PD

PD

PD

PD

PD

PD

Generic Type

xp

Partial Description
(Polynomial Function)

xq

x1

p

A21
A22

p
p

Partial Description
(Polynomial Function)

Generic Type

zi=f(xp,xq) - case 1

xp

zi’=f ’(xp,xq) - case 2

xq

Partial Description
(Polynomial Function)

PD

∑ fi

PD
PD

PD
PD

∑

x1

PD
PD
PD
NOP

Layer 5

NOP

•••

PD
PD
PD

y^

x2

A11

p

A12

p

A21

p

A22

p

Partial Description
(Polynomial Function)

PD

•••

NOP

NOP

NOP

NOP

NOP

NOP

PD

PD

PD

PD

PD

PD

PD

PD

PD

PD

PD

xp

Partial Description
(Polynomial Function)

zi=f(xp) - case 1
zi’=f ’(xp) - case 2

xp

Partial Description
(Polynomial Function)

zi=f(xp)

PD

PD
NOP

PD

zi=f(xp)

zi’=f ’(xp,xq,xr) - case 2

PD
PD

∑

Advanced Type

zi=f(xp,xq,xr) - case 1

PD
PD

NOP

xp

Partial Description
(Polynomial Function)

∑ fi

Layer 5

NOP

NOP
NOP

xp
xq
xr

zi=f(xp,xq)

••

p

A12

xq

••

x2

A11

xp

zi=f(xp,xq)

^y

PD

xp

Partial Description
(Polynomial Function)

PD

PD

y^

PD

Advanced Type

zi=f(xp) - case 1
zi’=f ’(xp) - case 2

(a) Basic architecture of Self-organizing HNFN

(b) Modified architecture of Self-organizing
HNFN
Fig. 2. Self-organizing HNFN architecture; Combination of NF and PNN using two connection
points

In order to enhance the learning of the Self-organizing HNFN and augment its performance of a Self-organizing HNFN, we use genetic algorithms (GAs) to adjust
learning rate, momentum coefficient and the parameters of the membership functions
of the antecedents of the rules. GAs is a stochastic search technique based on the principles of evolution, natural selection, and genetic recombination by simulating “survival of the fittest” in a population of potential solutions (individuals) to the problem
at hand. GAs are capable of globally exploring a solution space, pursuing potentially
fruitful paths while also examining random points to reduce the likelihood of setting
for a local optimum. The main features of genetic algorithms concern individuals
viewed as strings, population-based optimization (search through the genotype space)
and stochastic search mechanism (such as selection and crossover). A fitness function
(or fitness, for short) used in genetic optimization is a vehicle to evaluate a performance of a given individual (string) [3].
2.2 The Design of Self-Organizing HNFN
The design procedure for each layer in the premise and the consequence of Selforganizing HNFN is as follows (refer to Fig. 1 and Fig. 2).
The premise of Self-organizing HNFN
[Layer 1] Input layer.
[Layer 2] Computing activation degrees of linguistic labels.

Self-Organizing Hybrid Neurofuzzy Networks

881

[Layer 3] Normalization of a degree activation (firing) of the rule.
[Layer 4] Multiplying a normalized activation degree of the rule by connection weight.
aij = µ ij ( xi ) × wij
(1)
If we choose Connection point 1 for combining architecture of the NF and PNN, aij
is given as the input variable of the PNN. If we choose Connection point 2, aij corresponds to the input signal to the output layer of NF viewed as the input variable of the
PNN.
th
[Layer 5] Fuzzy inference for output of the rules: The output of each node in the 5
layer of the premise part of Self-organizing HNFN is inferred by the center of gravity
method. If we choose Connection point 2, see Fig. 1 and 2, fi is the input variable of
PNN that is the consequence part of Self-organizing HNFN
n

n

f i ( xi ) =

n

∑a = ∑µ
ij

j =1

j =1

ij ( xi ) ⋅ wij

=

∑µ

ij ( xi ) ⋅ wij

j =1

(2)

n

∑µ

ij ( xi )

j =1

The consequence of Self-organizing HNFN
[Step 1] Configuration of input variables: x1=a11, x2=a12,…, xn=aij (n=i×j).
[Step 2] Forming a PNN structure: We select input variables and fix an order of a PD.
[Step 3] Estimate the coefficients of a PD: The vector of coefficients of the PD’s (Ci)
in each layer is produced by the standard least-squares method and expressed in the
form
(3)
C i = ( X Ti X i ) −1 X Ti Y
Where i denotes node number. This procedure is implemented repeatedly for all
nodes of the layer and also for all layers of consequence part of Self-organizing
HNFN.
[Step 4] Choosing PDs with the highest predictive capability: Each PD is constructed
and evaluated using the training and testing data, respectively. Then we compare the
values of the performance index and select PDs with the highest predictive capabilities
(lowest values of the performance index). We may use i) the predetermined number W
of the PDs or ii) go for all of them whose performance index is lower than a certain
prespecified value. Especially the method of (ii) uses the threshold criterion θ to select
the node with the best performance in each layer.
θ = Emin + δ
(4)
where θ is a new value of the criterion, δ is a positive constant (increment) and Emin
denotes the smallest value obtained in the each layer.
[Step 5] Termination condition: It is prudent to take into consideration a stopping
condition (Emin ≥ Emin*) for better performance and the number of iterations (size of the
network) predetermined by the designer. Where Emin is a minimal identification error
at the current layer while Emin* denotes a minimal identification error at the previous
layer.
[Step 6] Determining new input variables for the next layer: The outputs of the preserved PDs serve as new inputs to the next layer. In other words, we set x1i=z1i, x2i=z2i,

882

S.-K. Oh et al.

…, xWi=zWi. The consequence part of Self-organizing HNFN is repeated through steps
3-6.

3 Simulation Results
This section presents simulation results of the Self-organizing HNFN for the time
series data of gas furnace utilized by Box and Jenkins [4-8]. Consider a gas furnace
system in which air and methane are combined to form a mixture of gases containing
CO2 (carbon dioxide). We use 2 inputs 1 output, [u(t-3), y(t-1); y(t)] and 3 inputs 1
output, [u(t-2) , y(t-2), y(t-1); y(t)] for evaluating the proposed models. Where, u(t)
denotes the flow rate of methane gas and y(t) stands for the carbon dioxide density.
We consider 296 pairs of the original input-output data. 148 out of 296 pairs of inputoutput data are used as learning set; the remaining part serves as a testing set. The
performance index (PI) is defined by (5).

PI =

1
N

N

∑( y

p

− yˆ p ) 2

(5)

p =1

GAs help optimize learning rate, momentum coefficient, and the parameters of the
membership function in premise part of Self-organizing HNFN. GAs use serial
method of binary type, roulette-wheel as the selection operator, one-point crossover,
and an invert operation in the mutation operator [3]. GAs was run for 100 generations
with a population of 60 individuals. Each string was 10 bits long. The crossover rate
was set to 0.6 and probability of mutation was equal to 0.1. Fig. 3 shows the preferred
architecture in generic type of the Self-organizing HNFN that is composed of NF and
PNN with 3→4 inputs-Type 3→2 topology. The way in which learning has been realized is shown in Fig. 4 where both training and testing data are illustrated. Fig. 5 and 6
show the preferred architecture in advanced type of the Self-organizing HNFN.
PD7

u(t-2)

y(t-2)

y(t-1)

w

PD3

PD8

PD4

PD10

PD8

PD11

A11

p

A12

p

PD10

PD12

PD11

PD13

A21

p

PD12

PD15

p

PD13

PD18

A22

PD15

PD19

A31

p

PD16

PD20

PD17

PD26

A32

p

PD18

PD27

PD19

PD28

PD20

PD29

PD4
PD5
PD7

PD14

PD8

PD7

PD12

PD10

PD21

PD18

PD22

PD30

PD12

PD23
PD27
PD30
PD6

PD30

Fig. 3. Modified architecture-case 2 in generic type of the Self-organizing HNFN

y

Self-Organizing Hybrid Neurofuzzy Networks
0.9

1

0.8

0.9

883

0.8

0.7

0.7

Premise Part :
NF

0.5

0.6

Consequence Part :
PNN

Testing errors

Training errors

0.6

0.4
0.3

Premise Part :
NF

0.5

Consequence Part :
PNN

0.4
0.3

0.2

0.2

0.126
0.1
0

0.096

0.1

0.024

0.0161
0

100

200
300
Iteration

400

500

1

2

3

4

100

5

Layer

(a) Training errors (PI)

200
300
Iteration

400

500

1

2

3

4

5

Layer

(b) Testing errors (E_PI)

Fig. 4. Learning procedure of modified architecture-case 2 in generic type of the Selforganizing HNFN
PD1

u(t-2)

y(t-2)

y(t-1)

A11

p w

A12

p

A21

p

A22

p

A31

p

A32

p

PD2

fi

PD1

PD1

PD3

PD1

∑

y

PD17

PD4

PD6
PD8
PD12
NOP2

∑

NOP3

NOP18

NOP4

NOP19

NOP18

NOP5

∑
PD2

PD6

PD3

PD7

PD4

PD8

PD22

PD35

Fig. 5. Modified architecture-case 2 in advanced type of the Self-organizing HNFN

1.1
1

0.8

0.9

0.7

0.8

0.6

Premise Part :
NF

0.5

Training errors

Training errors

1
0.9

Consequence Part :
PNN

0.4

0.7

0.4

0.2

0.3
0.2

RPRTV
0

100

200

300

Iteration

400

500

RPSTX

RPRSYY
1

2

Layer

3

4

5

Consequence Part :
PNN

0.5

0.3

0.1

Premise Part :
NF

0.6

RPSSU

0.1
100

200

300

Iteration

400

500

1

2

3

4

5

Layer

(a) Training errors (PI)
(b) Testing errors (E_PI)
Fig. 6. Learning procedure of modified architecture-case 2 in advanced type of the Selforganizing HNFN

884

S.-K. Oh et al.

Table 2 contains a comparative analysis including several previous models. Compared with these models, the Self-organizing HNFN comes with high accuracy and
improved prediction capability.
Table 2. Comparison of identification error with previous modeling methods

Our
model
(HNFN)

Model
Lin and Cunningham’s model[4]
Kim’s model[5]
Oh’s Fuzzy model[7]
Oh’s Adaptive FNN[8]
Oh and Pedrycz’s Fuzzy model[6]
Basic
architecture
Generic
type
Modified
architecture
[u(t-3), y(t-1);
y(t)]
Basic
architecture
Advanced
type
Modified
architecture
Basic
architecture
Generic
type
Modified
architecture
[u(t-2) , y(t-2),
y(t-1); y(t)]
Basic
architecture
Advanced
type
Modified
architecture

Case 1
Case 2
Case 1
Case 2
Case 1
Case 2
Case 1
Case 2
Case 1
Case 2
Case 1
Case 2
Case 1
Case 2
Case 1
Case 2

PI
0.071
0.034
0.020
0.021
0.020
0.0216
0.0235
0.0181
0.0176
0.0231
0.0237
0.0194
0.0199
0.0164
0.0172
0.0172
0.0161
0.0198
0.0205
0.0178
0.0177

E_PI
0.261
0.244
0.264
0.332
0.271
0.270
0.268
0.246
0.250
0.270
0.277
0.267
0.264
0.104
0.103
0.106
0.096
0.121
0.119
0.115
0.113

4 Conclusions
In this study, we have introduced a class of Self-organizing HNFN regarded as a modeling vehicle for nonlinear and complex systems. Self-organizing HNFN is constructed by combining NF with PNN. In this sense, we have constructed a coherent
platform in which all components of CI are fully utilized. The model is inherently
dynamic - the use of the PNN is essential to the process of “growing” the network by
expanding the depth of the network. A comprehensive design procedure was developed. The series of experiments helped compare the network with other fuzzy models
- in all cases the previous models came with higher values of the performance index.

Acknowledgements. This paper was supported by the Korea Science & Engineering
Foundation (KOSEF: grant No. R05-2000-000-00284-0) and partially by Wonkwang
University in 2002.

Self-Organizing Hybrid Neurofuzzy Networks

885

References
1.

A. G. Ivahnenko: The group method of data handling: a rival of method of stochastic approximation. Soviet Automatic Control. Vol. 13, No. 3 (1968) 43–55
2. T. Yamakawa: A New Effective Learning Algorithm for a Neo Fuzzy Neuron Model. 5th
IFSA World Conference (1993) 1017–1020
3. D. E. Goldberg: Genetic Algorithms in search, Optimization & Machine Learning.
Addison-Wesley (1989)
4. Y. Lin, G. A. Cunningham III.: A new approach to fuzzy-neural modeling. IEEE Trans.
Fuzzy Systems. Vol. 3, No. 2 (1995) 190–197
5. E. Kim, H. Lee and M. Park: A simply identified Sugeno-type fuzzy model via double
clustering. Information Sciences. Vol. 110 (1998) 25–39
6. S.K. Oh, and W. Pedrycz.: Fuzzy Identification by means of Auto-Tuning Algorithm and
Its Application to Nonlinear Systems. Fuzzy Sets and System. Vol. 115, No. 2 (2000)
205–230
7. B.J. Park, W. Pedrycz and S.K Oh: Identification of Fuzzy Models with the Aid of Evolutionary Data Granulation. IEE Proc.-Control Theory and Applications. Vol. 148, Issue 05
(2001) 406–418
8. S.K. Oh, B.J. Park and C.S. Park: On-line Modeling of Nonlinear Process Systems using
the Adaptive Fuzzy-Neural Networks. The Transactions of The Korean Institute of Electrical Engineers. Vol. 48A, No. 10 (1999) 1293–1302
9. S.K. Oh and W. Pedrycz: The Design of Self-organizing Polynomial Neural Networks.
Information Sciences. Vol. 141, Issue 3-4 (2002) 237–258
10. B.J. Park, W. Pedrycz and S.K. Oh: Fuzzy Polynomial Neural Networks: Hybrid Architectures of Fuzzy Modeling. IEEE Trans. on Fuzzy Systems. Vol. 10, No. 5 (2002) 607–
621

