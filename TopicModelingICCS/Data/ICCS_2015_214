Procedia Computer Science
Volume 51, 2015, Pages 2832‚Äì2837
ICCS 2015 International Conference On Computational Science

Challenges and Solutions in Executing Numerical Weather
Prediction in a Cloud Infrastructure
Emmanuell D. CarreÀú
no, Eduardo RoloÔ¨Ä, Philippe O. A. Navaux
Informatics Institute, Federal University of Rio Grande do Sul (UFRGS), Porto Alegre, RS, Brazil
{edcarreno,eroloff,navaux}@inf.ufrgs.br

Abstract
Cloud Computing has emerged as a solution to perform large-scale scientiÔ¨Åc computing. The
elasticity of the cloud and its pay-as-you-go model present an interesting opportunity for applications commonly executed in clusters or supercomputers. This paper presents the challenges
of migrating and performing a numerical weather prediction (NWP) application in a cloud
computing infrastructure. We compared the execution of this High-Performance Computing
(HPC) application in a local cluster and the cloud using diÔ¨Äerent instance sizes. The experiments demonstrate that, although processing and networking create a limiting factor, storing
input and output datasets in the cloud present an attractive option to share results and ease
the deployment of a test-bed for a weather research platform. Results show that cloud infrastructure can be used as a viable HPC alternative for numerical weather prediction software.
Keywords: Cloud Computing, e-Science, Numerical Weather Prediction, HPC

1

Introduction

Numerical Weather Prediction (NWP) is a type of high-performance computing application that
requires a large amount of computing resources. This large-scale scientiÔ¨Åc computing has been
performed for years on costly machines that are beyond the budget limits for many research
groups. Most of the time, those systems are underutilized and produce a Ô¨Ånancial burden in upfront and running costs, such as electricity and maintenance. The cloud computing paradigm
has provided an alternative to access large infrastructures and resources. In the cloud, the
possibility of paying only for the amount of resources used brings convenience for academia
and the industry. The pay-as-you-go concept could be a viable option for institutions lacking
computing resources.
Previous work in running High-Performance Computing (HPC) applications in the cloud
looks mostly at small HPC benchmarks [4] or bag-of-tasks applications [3, 2, 6], which are
simple to migrate or run in most cases. We look at a full and complex HPC application, with
complicated requirements, as most of the software used in HPC has been optimized to run
2832

Selection and peer-review under responsibility of the ScientiÔ¨Åc Programme Committee of ICCS 2015
c The Authors. Published by Elsevier B.V.

doi:10.1016/j.procs.2015.05.443

Challenges and Solutions in Executing NWP in a Cloud Infrastructure

CarreÀú
no, RoloÔ¨Ä and Navaux

on speciÔ¨Åc architectures and requires a particular set of conditions to operate in a proper and
eÔ¨Écient way. One of the current problems with those legacy applications is the challenge in
the migration to new infrastructures. The usage of virtualization may help to solve some of the
challenges.
The goal of this paper is to execute a numerical weather application in the cloud while testing
the possibility of sharing research results and datasets between various weather research centers.
To produce a tangible outcome, we performed a series of experiments regarding processing,
scaling and networking performance. The main contributions of this paper are, Ô¨Årst, to share
the experience obtained in migrating a legacy application to a cloud environment, presenting
its challenges, providing our solutions and discussing future opportunities. Second, we explain
how an NWP application and its workÔ¨Çow may be adapted to a cloud infrastructure. Finally,
we evaluate the performance of this migrated application in the cloud by comparing it to a
cluster of local machines.

2

NWP in the Cloud: Challenges and Solutions

In infrastructure-as-a-service (IaaS) cloud environments, the users have the responsibility to
set up the entire software stack, starting from the OS to the compilers and required libraries,
before they can compile and execute their applications. The source code of HPC applications
often lacks proper documentation, code comments and most of the time has been optimized for
a very speciÔ¨Åc type of machine or architecture. Thus, migrating the complete application code
becomes a challenging and not-so-trivial task.
One of the most widely employed numerical models in the regional weather centers in Brazil
is BRAMS[1]. Multiple weather research centers in South America use the consolidated datasets
produced by the Brazilian Weather Forecast and Climate Studies (CPTEC) for their regional
NWP simulations using BRAMS. One of the challenges we found while migrating this application to the cloud was the access to the legacy Ô¨Åle-system BRAMS used. In the cloud, the
storage access depends on the features available by the cloud provider. We opted to use the
Azure File Service as other options required an intrusive modiÔ¨Åcation of optimized input/output mechanisms. It is important to automate most of the tasks using the cloud environment to
minimize unnecessary costs, from the creation of an initial VM instance to the Ô¨Ånal deployment
steps. These steps need to be completely automated to allow a real pay-as-you-go usage of an
NWP application in the cloud. Due to the way that cloud services are charged, the time and
resources spent conÔ¨Åguring and compiling the applications need to be taken into account.
Figure 1 depicts the changes introduced in the BRAMS workÔ¨Çow to adapt it to the cloud.
Contrary to a physical grid or cluster in which all machines are powered on even while idling,
in the pay-as-you-go model the machines are not available until provisioned.
	

	




	




	

!


	
	

	%
	!&' %
	!( 

&

( 
(		%



*
	

&

)






"#
$
%
	!%	





&


	







	




+,




-
	





 	







	

!

Figure 1: BRAMS execution workÔ¨Çow. The white boxes are the steps to perform a simulation
in an environment with physical machines already provisioned like a grid or cluster, gray boxes
indicate the steps introduced to run BRAMS on demand in the cloud.
2833

Challenges and Solutions in Executing NWP in a Cloud Infrastructure

3

CarreÀú
no, RoloÔ¨Ä and Navaux

Evaluation Methodology

To compare the performance of BRAMS in a cloud environment, we ran a series of experiments
in a local cluster and compared it with the version running on the Azure platform. Microsoft
Azure was chosen because a previous work [5] has shown that performance and cost eÔ¨Éciency
for HPC in Azure were better when compared to other providers. The experiment allowed us
to check performance and scalability of BRAMS in those two environments and helped us to
identify possible issues related to the migration to the cloud.
HPC applications require low network latencies. Because cloud network conditions changes
on each VM allocation, we design an experiment to collect networking data by measuring the
latencies between the VMs. We created and deleted 8 VM instances of BRAMS and analyzed
the latencies between each node.
In a Ô¨Ånal experiment, we analyze the time spent in the provisioning and controlling operations in the cloud service, namely to start, stop, delete and create instances. Cloud services are
charged by the number of machines allocated even if they are not performing useful computing,
because of this, we wanted to know how much time was spent on each one of those operations.

3.1

Environment

The characteristics of each execution environment are described in this section. A summary of
the hardware details and characteristics of the machines used is depicted in Table 1.
Azure oÔ¨Äers multiple options regarding IaaS 1 . We use the compute instances that were
closest to the characteristics of the local machines. As can be seen in Table 1, most of the
instance types used include 8 CPU cores. To minimize communication overhead in the HPC
application, we avoid using the smaller Linux compute instances available for our subscription.
Only D14 and A9 include 16 CPU cores. Those 16 CPU cores instances were used with 8 cores
during some of the scaling tests to obtain comparable Ô¨Ågures. The names of the instance types
were deÔ¨Åned by the cloud service provider.

4

Experiments and Results

The experiments consisted of executing BRAMS to perform a forecast of 72 hours with a spatial
resolution of 50 km, covering a square perimeter of 6000 km, an area of 1500x1500 km. All of
the experiments were executed on the same Azure datacenter location, using aÔ¨Énity groups in
order to achieve a better performance as the VM is as physically close to each other as possible.
Table 1: Hardware details of the machines and the virtual instances used for performance
experiments. Networking bandwidth limits on the VMs are imposed by the service provider.
Characteristics
Processor model
Processor speed (GHz)
Number of CPU cores
Memory (GB)
Networking (Mbps)

Local System
Xeon E5310
1.6
8
8
1000

D13

D14 A4
A7
Xeon E5-2660
2.2
8
16
8
8
56
112
14
56
1000 2000 800 2000

1 http://msdn.microsoft.com/en-us/library/azure/dn197896.aspx

2834

A8
A9
Xeon E5-2670
2.6
8
16
56
112
5000 10000

Challenges and Solutions in Executing NWP in a Cloud Infrastructure

4.1

CarreÀú
no, RoloÔ¨Ä and Navaux

Performance Results

Performance results of BRAMS execution are shown in Figure 2. The experiment consisted of
running a complete weather forecast scaling horizontally from 1 node up to 8 nodes both locally
and in Azure.
In some scenarios, the local machines were faster. However, the results diÔ¨Äer accordingly
to the instance sizes. The A8 and A9 instances show faster execution times for most of the
experiments, except when using one node, wherein D14 took less time. In that case, D14 Ô¨Ånished
in 33% of the time and A8 and A9 in 48%. D13 and D14 did well at Ô¨Årst, but they got worse
as we added more nodes to the experiments. A7 spent 47% more than the local machines with
64 processes. Interestingly, A7 has better speciÔ¨Åcations than A4, but even so the performance
of A7 is worse than A4 in all the experiments but the Ô¨Årst. The CPU frequency of the local
machines is 37.5% lower than in the A4 and A7 instances, as shown in the results, however this
fact did not reÔ¨Çect an equivalent advantage as in no scenario those instances Ô¨Ånished in less
time than the local machines. We found that the amount of performance degradation due to
virtualization was higher than expected for the A4 and A7 instances.
Generally speaking, performance became worse as more nodes were added to the experiment
for all the instance sizes. This behavior can be attributed to the fact that BRAMS is a CPUbound application with also a high amount of communication between processes and nodes.
Network performance prevents a better scaling of the cloud cluster. The best results overall
were obtained by the A8 instance, Ô¨Ånishing in 860.58s with 64 processes. The A8 instance size
is not the best given its speciÔ¨Åcations, but on average, it outperformed A9.
The variability of these experiments was very small, on average 2.92 seconds (0.16%) on
Azure and 1.43 seconds (0.05%) on the local cluster. These results and their consistency
show that BRAMS behavior was uniform between experiments, and that it is possible to scale
BRAMS horizontally using the Azure infrastructure even with the penalty imposed by the
virtualization overhead.
A4

A7

A8

A9

D13

D14

Execution time (normalized)

150%
125%
100%
75%
50%
25%
0%
8

16

24

32
40
Number of processes

48

56

64

Figure 2: Execution time of a 72 hours integration forecast running on the local cluster and in
the diÔ¨Äerent cloud instance sizes, horizontal axis shows the number of processes. Values were
normalized to the execution time of the local cluster. Lower values mean faster execution times.
2835

100
90
80
70
60
50
40
30
20
10
0

CarreÀú
no, RoloÔ¨Ä and Navaux

200
175
150
Time [s]

Latency [ms]

Challenges and Solutions in Executing NWP in a Cloud Infrastructure

125
100
75
50
25
0

New
from
frontend

New
between
nodes

Reallocated Reallocated
from
between
frontend
nodes

(a) Network Latency.

Creating

Deleting

Starting
Stopping
Deallocated Reallocated

(b) Provisioning/Controlling.

Figure 3: Results of experiments. Fig. 3a shows network latency between the nodes and
frontend. Fig. 3b Shows the time spent on the deployment operations in the cloud service.

4.2

Network Latency

For HPC applications like NWP, the network latency impacts the scalability and performance.
Results of this experiment are shown in the Figure 3a. The high variability suggests high
networking traÔ¨Éc in the datacenter. Reallocation of nodes caused higher latencies than creating new instances. Contrasting with the latencies obtained in Azure, the local cluster had an
average latency of 12ms. The relatively high latencies create a signiÔ¨Åcant issue for the deployment of larger BRAMS simulations in Azure. The networking latencies in this tightly coupled
application are a limiting factor for running bigger experiments due to the performance loss
compared with the gains in raw processing power.

4.3

Provisioning and Controlling Operations

The life cycle of a VM instance includes the steps to create, start, reboot, stop and delete an
instance. These operations are an essential part of the pay-as-you-go model since each deployed
VM has to pass by some of them. Cloud services begin charging since the creation of the VM
to the release of its resources, not only for the actual computing. We noticed node creation
spends more than twice the time of the other three operations. The time spent in each of
these operations is presented in Figure 3b. We found a signiÔ¨Åcant variability in the time spent
on these operations, of up to 16% in the case of stopping nodes. Time spent in operations
not related to an actual processing usage could impose an adoption barrier to make in-depth
analysis of HPC applications in Azure. In this non-processing state, all idle VMs generate costs
that are expected to be reduced by the usage of a cloud infrastructure.

5

Conclusions and future work

This paper presented the challenges in running an HPC application in a cloud computing infrastructure using IaaS. We present our solutions for the process of porting BRAMS to the
Microsoft Azure Cloud platform. Performance results shows that cloud infrastructure oÔ¨Äers
possibilities to move applications from legacy code and that it is possible to replicate a cluster
while reducing the complexity when running HPC applications. Variability between experiments was low in Azure, which is an important characteristic for HPC as it leads to a higher
2836

Challenges and Solutions in Executing NWP in a Cloud Infrastructure

CarreÀú
no, RoloÔ¨Ä and Navaux

predictability of experiments, both in the time it takes and how much it will cost.
We expect that performing this kind of experiments in other cloud providers as Amazon
EC2 and Google Compute shows similar results, as most of the commodity hardware in their
datacenters still uses high latency network links. This network-intensive usage limitation is not
unique of BRAMS, as it is present in more HPC applications using the distributed memory
paradigm. We hope that the experience obtained can be applied to port other HPC applications.
Cloud computing is an evolving, promising alternative, as long as providers upgrade their
hardware and keep lowering the prices of their services, driven by the competitive market.
One of the primary expected contributions of our work is to create the possibility of small
research centers in South America to perform weather prediction of their regions, without
depending on supercomputers, and with limited, or absent, upfront costs. For the future,
we intend to perform a full analysis of the cost of running a simulation of BRAMS. We also
wish to capture more metrics to cover all the aspects of the execution and try to improve the
performance of this HPC application in a cloud environment.

Acknowledgments
We would like to thank the members of the CPTEC, as their help, expertise and advices were of
great value. This research has been partially supported by CNPq under grant 132123/2013-4.

References
[1] CPTEC-INPE. Brazilian Regional Atmospheric Modelling System (BRAMS). http://www.cptec.
inpe.br/brams. Accessed August 10, 2014.
¬¥
[2] Sergio Hern¬¥
andez, Javier Fabra, Pedro Alvarez,
and Joaqu¬¥ƒ±n Ezpeleta. Cost evaluation of migrating
a computation intensive problem from clusters to cloud. In J¬®
orn Altmann, Kurt Vanmechelen, and
OmerF. Rana, editors, Economics of Grids, Clouds, Systems, and Services, volume 8193 of Lecture
Notes in Computer Science, pages 90‚Äì105. Springer International Publishing, 2013.
[3] Wei Lu, Jared Jackson, and Roger Barga. AzureBlast: A Case Study of Developing Science Applications on the Cloud. In Proceedings of the 19th ACM International Symposium on High Performance
Distributed Computing, HPDC ‚Äô10, pages 413‚Äì420, New York, NY, USA, 2010. ACM.
[4] E. RoloÔ¨Ä, F. Birck, M. Diener, A. Carissimi, and P.O.A. Navaux. Evaluating high performance
computing on the windows azure platform. In Cloud Computing (CLOUD), 2012 IEEE 5th International Conference on, pages 803‚Äì810, June 2012.
[5] E. RoloÔ¨Ä, M. Diener, A. Carissimi, and P.O.A. Navaux. High performance computing in the
cloud: Deployment, performance and cost eÔ¨Éciency. In Cloud Computing Technology and Science
(CloudCom), 2012 IEEE 4th International Conference on, pages 371‚Äì378, Dec 2012.
[6] Mikael Fernandus Simalango and Sangyoon Oh. Feasibility study and experience on using cloud
infrastructure and platform for scientiÔ¨Åc computing. In Borko Furht and Armando Escalante,
editors, Handbook of Cloud Computing, pages 535‚Äì551. Springer US, 2010.

2837

