Predictability of Rules in HIV-1 Protease
Cleavage Site Analysis
Hyeoncheol Kim1 , Tae-Sun Yoon1 , Yiying Zhang2 ,
Anupam Dikshit2 , and Su-Shing Chen2
1

2

Dept. of Computer Science Education, Korea University,
Seoul, 136-701, Korea
hkim@comedu.korea.ac.kr
Computer & Info. Sciences and Engineering, University of Florida,
Gainesville, FL. 32611, U.S.A.
suchen@cise.ufl.edu

Abstract. Symbolic rules play an important role in HIV-1 protease
cleavage site prediction. Recently, some studies have done on extraction
of the prediction rules with some success. In this paper, we demonstrated
a decompositional approach for rule extraction from nonlinear neural
networks. We also compared the prediction rules to the ones extracted
by other approaches and methods. Empirical experiments are also shown.

1

Introduction

HIV-1 protease is an enzyme in the AIDS virus that is essential to its replication.
Understanding HIV-1 protease cleavage site speciﬁcity is very desirable, because
eﬃciently cleaved subtrates are also excellent templates for synthesis of tightly
binding chemically modiﬁed inhibitors. HIV-1 protease inhibitor drugs are small
molecules that bind to the active site in HIV-1 protease and stay there, so that
the normal functioning of the enzyme is prevented.
Recently, many machine learning methods have been applied to the problem
of HIV-1 protease cleavage site prediction [2, 3, 8, 9, 12, 7]. Neural networks have
been successful for the cleavage site prediction yielding a prediction accuracy
of 90-92%, which is superior than decision tree C5.0 of 85.5% and similar to
Gaussian SVM [2, 8, 3]. Linear classiﬁers such as a perceptron and linear SVM
were also used yielding accuracy of 88-92% while MLP produced accuracy of
86-92% according to size of training dataset [9].
Narayanan et al. addressed importance of symbolic interpretation in HIV-1
protease cleavage site prediction, and they demonstrated symbolic rule generation using decision trees [8]. Rules can guide future in vitro experiments which
take into account various patterns of residues and identify the most important
positions to mutate in the n-residue regions to test for cleavability [8, 9]. Even
though their experiments showed that neural networks outperformed decision
trees, they used a decision tree to generate symbolic rules because MLP is hard
to interprete. R¨
ognvaldsson et al. used a linear model such as a simple perceptron
or LSVM to generate rules because it is more easily interpreted than a nonlinear
V.N. Alexandrov et al. (Eds.): ICCS 2006, Part II, LNCS 3992, pp. 830–837, 2006.
c Springer-Verlag Berlin Heidelberg 2006

Predictability of Rules in HIV-1 Protease Cleavage Site Analysis

831

one such as MLP. R¨
ognvaldsson et al. used a so-called exhaustive search method
to generate rules from simple linear perceptron, in which they generated all possible candidate combinations and tested each one with a trained perceptron that
is used as a function. However, this method is not practical for nonlinear model
(i.e., MLP).
In this paper, we applied a decompositional approach to extract symbolic rules
from trained nonlinear neural networks (i.e., MLP), which is computationally
practical and safe. Also we compared the extracted rules with others from various
methods in terms of their predictability.

2

Rule Discovery Using Neural Networks

As addressed in [8], machine learning algorithm needs to take into account combined inﬂuence of attributes on classiﬁcation. Algorithms such as MLP do so by
adjusting all the connection weights for each instance to ensure more accurate
classiﬁcation, while decision tree looks for just one attribute (i.e., the most important residue position) at each decision point in the tree on which to split the
samples. However the knowledge learned by MLP can not be converted easily
into symbolic rules.
In neural networks, since the knowledge learned through training is distributed to connection weights that are interconnected nonlinearly, it is very
hard to interpret the black box into human-understandable symbolic forms. In
recent years, there have been studies on rule extraction from feed-forward neural networks [1, 4, 5, 6, 10, 11]. The extracted rules provide human users with the
capability to explain how the patterns are classiﬁed and may provide better
insights about the domain.
There are two approaches of rule extraction from neural networks(i.e., MLP):
pedagogical (black-box) approach and decompositional (white-box) approach.
Pedagogical approaches take a trained neural network as a black-box and observe input patterns and their corresponding output patterns. Generic methods
of the approaches take into account all possible combinations of an input vector
which are then presented to the trained neural network for their validity evaluation. For our cleavage site prediction problem, domain size would be 208 and the
number of all possible candidate rules would be 218 . Also there are some complicated eﬃciency issues on how to evaluate a candidate rule and how to generate
maximally general and accurate rules. Obviously, pedagogical approaches are
not eﬀective when the size of the neural network increases. In order to overcome
this limitation, decompositional approaches apply heuristically guided search to
the process of rule extraction.
Decompositional approaches to rule extraction from a trained neural network
(i.e., MLP) involves the following phases:
1. Intermediate rules are extracted at the level of individual units within the
network (i.e., hidden and output nodes).
2. The intermediate rules from each unit are aggregated to explain the logical
mapping between the input and output nodes of the network.

832

H. Kim et al.
threshold = -1

w=3

x1

4

x2

-6

1

x3

2

x4

x5

Fig. 1. An individual rule with ﬁve incoming connection weights and a threshold

At the ﬁrst phase, it searches valid and maximally general (i.e., size of a
combination is as small as possible) combinations by ﬁnding the combinations
of weights of a node that will make it active. It is illustrated in ﬁgure 1 in which
a non-input unit with 5 binary inputs, their corresponding incoming connection
weights and a threshold is given. For example, a combination (x1 , x2 ) is a valid
rule because its weight-sum (i.e., 3 + 4 = 7) is always greater than the threshold
(i.e., −1) regardless of the values of other incoming units.
However the process in phase 1 is inherently combinatorial. For n binary
inputs, the number of combinations is 3n (i.e., exponential complexity) which
is computationally very expensive as the n grows. Thus, heuristics have been
proposed to reduce the search space. We used Kim’s OAS algorithm for our experiments, which orders incoming weights by their contributions to node activity and then searches valid and maximally-general rules using tree-based search
structure in O(n log n) most cases and less than O(2n−1 ) worst case [6]. Our implementation of the OAS algorithm uses two parameters to control the quantity
and quality of the rules extracted: k and CF threshold. At each node, it generates k rules with the highest CF values among all the rules whose CF values are
greater than the CF threshold. The CF value is deﬁned as the output value of
sigmoid function in the node. CF threshold is normally set to 0.8 for positive
rules.
Decompositional approaches take each node in the network as a concept unit
and the concepts in the concept node are logically aggregated with other concepts from other nodes. Thus, the approaches using the parameters can generate
general rules excluding many other trivial rules. Therefore the rules extracted
by this approach can be diﬀerent from the rules by black-box approaches.

3
3.1

Experimental Results
Datasets

Cai and Chou [2] published a dataset with 362 peptides in 1998, which has
been used as the standard dataset in many publications of this research domain
[8, 9, 12]. The data set includes 362 examples with 114 positives (i.e., cleaved) and

Predictability of Rules in HIV-1 Protease Cleavage Site Analysis

833

Table 1. HIV-1 protease cleavage site datasets. Default cleavability represents probability of the cleaved of a default rule (i.e., “Whatever it is, it is always cleaved”).
Dataset
Cai et al.
New dataset
Combined

Cleaved Uncleaved Total
114
281
395

248
111
359

362
392
754

Default
cleavability
31.49%
71.68%
52.52%

248 negatives (i.e., non-cleaved). Recently, You et al. collected 384 more peptides
to create a 746-peptide dataset [12]. Since the new dataset is not available to
public domain currently, we created our own new dataset in a similar way by
collecting and reﬁning peptide sequences from published literatures containing
experimental data on oligopeptide sequences that have been exposed to the HIV1 protease [12]. We collected 392 new examples and thus created a 754-peptide
dataset without duplication. In this article, we call the datasets as a 362-dataset,
a 392-dataset and a 754-dataset, respectively. Table 1 shows their distributions.
A sequence in the datasets is composed of 8 positions and each position represents one of 20 diﬀerent amino acids. A peptide is denoted by p4 p3 p2 p1 p1 p2 p3 p4
where each p represents one of 20 amino acids. The scissile bond is located between positions p1 and p1 . For our experiments, we use 9 attributes for each
octapeptide: location 1 though location 8 and class attribute (i.e., cleavagability). Location 1 corresponds to p4 and location 8 to p4 . We also used orthonormal
encoding scheme that maps the sequence to a sparse orthonormal representation.
Each amino acid pi is then represented by a 20 bit vector with only one bit set
to one and the rest 19 bits to zero. This places an octamer in one of the corners
of a 20 ∗ 8 = 160 dimensional hypercube.
Like other cleavage site prediction domains, the HIV-1 protease cleavage domain has a problem of dataset size. That is, the number of available sequences
2160 . And there is no eviis too small compared to domain size: 362
28
dence that the 362 or 754 examples are representative for the domain. Thus, a
question comes to us that classiﬁers created from just 362 or 754 examples can
really classify any previously unknown instance that is picked from the space of
208 instances.
3.2

Rule Extraction and Comparison

In our experiments, we ﬁrst extract rules using diﬀerent approaches and methods,
and then compare quality of the rules with their predictability. The predictability
of a classiﬁer means how well a classiﬁer created by a small number of examples
can predict other instances previously unknown.
We used the 362-dataset as a training set, and the 392-dataset as a test set.
Since the rules published in other papers have been created on the 362-dataset
[8, 9, 7], we used the same training dataset to generate our rules and used the
new 392-dataset for testing all the rules together.

834

H. Kim et al.

Table 2. The rules extracted by See5 [8]. Positions @1-4 represent the left hand sequence in front of cleavage point and positions @5-8 the right hand sequence. TP for
true positives and FP for false positives. Rule 1, 2, 3 are generated by normal mode
and rule 4 is generated by boosting mode.
DT Rules
on 362-dataset

1
2
3
4

P/FP
P/FP
P/FP
on 362-dataset on 392-dataset on combined set
(362/248)
(392/111)
(754/359)
F@4, cleavage
35/5
42/4
77/9
L@4, cleavage
38/9
47/4
85/13
Y@4 and P@5, cleavage
32/5
44/14
76/19
E@6, cleavage
38/6
17/0
55/6

Narayanan, et al. [8] tried a decision tree C5.0 to extract prediction rules for
HIV cleavage site, which are listed in table 2. Position 4 and 5 are considered to
be important attributes by the decision tree. A decision tree searches the most informative attribute ﬁrst and divides examples by the attributes, and then do the
same with the subgroup of the examples. The most informative attribute is determined by information gains of attributes, and the information gain is obtained by
amino acid’s frequency in the examples. Thus, the extracted rules are attribute
(i.e., position)-oriented and frequency-based. Frequency-based (or probabilitybased) mining normally requires a training set to be large enough and/or training
examples to be high domain-representative. With the 362-dataset whose quantity and quality of examples are not good enough, quality of the rules extracted
directly from the examples might not be guaranteed.
We also tried association rule mining which is another frequency(or
probability)-based rule mining method. While decision trees search for attributes
(i.e., positions) one by one to generate rules, association rule mining searches for
a set of attributes that are associated each other. The association is deﬁned by
frequency of co-occurrence of the attributes in a set. The frequency-based association rules are shown in table 3. The frequency-based method is statistically
meaningful when the size of data samples are large enough. Since the size HIV
data set is not large enough, the generated rules does not cover all of meaningful
patterns in the domain as seen in table 3.
A feed-forward neural network trained by backpropagation is not a frequentbased attribute-by-attribute method. A perceptron uses gradient descent-based
learning for linear classiﬁcation, and multi-layered perceptron is a set of perceptrons that are networked in a layered architecture and thus it provides nonlinear
separability by combining linear separable perceptrons. R¨
ognvaldsson et al.
Table 3. Association rules
1
2

Assoc. Rules on a 362-dataset support count
conﬁdence
correlation
P@5, cleavage
60
80.0% (60/12)
2.54
E@6, cleavage
38
84.2% (38/6)
2.67

Predictability of Rules in HIV-1 Protease Cleavage Site Analysis

835

Table 4. Rules extracted from a simple linear perceptron trained on a 362-dataset [9].
6 other rules containing E@6 are subsumed to the rule1.

1
2
3
4
5

SP Rules
P/FP
P/FP
P/FP
on 362-dataset
on 362-dataset on 392-dataset on combined set
E@6, cleavage
38/6
17/0
55/6
F@4 and P@5, cleavage
17/0
3/0
20/0
F@4 and Q@6, cleavage
14/0
8/0
22/0
F@4 and V@3, cleavage
0/0
20/1
20/1
L@4 and Q@6, cleavage
3/0
1/0
4/0

used a exhaustive rule search method which is a generic black box approach to
generate rules from a trained simple perceptron which is a linear model [9, 12].
In an exhaustive rule search, all possible combinations of an input vector are
taken into account in the process of rule generation. For each combination (i.e.,
candidate rule), it is presented to the trained neural network to get its validity.
Even for 2-position rules only (i.e., size of antecedent of a rule is 2), there are
8
C2 ∗ 20 ∗ 20 = 11, 200 candidate rules to test. They generated all the cases and
ranked them by their probabilities obtained from a trained linear perceptron.
Table 4 lists the 2-position rules. They showed top 10 rules [9], but we generalized
6 rules containing “E@6” in their antecedents into a 1-position rule (i.e., rule1 in
table 4). Since the rules in table 4 are top-10 rules with the highest probabilities
among 11,200 combinations, rules with very high accuracy are ﬁrst selected.
Table 5. The rules extracted by a neural network trained on a 362-dataset. CF threshold at each hidden unit was set to 0.6 for the top table and 0.8 for the bottom table.
The lower the CF threshold is, the more general rules are generated.
With CF threshold = 0.6
NN Rules
P/FP
on a 362-dataset on 362-dataset on
1 P@5, cleavage
60/12
38/9
2 L@4, cleavage
38/6
3 E@6, cleavage
78/31
4 N@3, cleavage
33/13
5 V@3, cleavage
35/5
6 F@4, cleavage
28/22
7 A@6, cleavage
With CF threshold = 0.8
NN Rules
P/FP
on a 362-dataset
on 362-dataset
1 E@6, cleavage
38/6
17/0
2 F@4 and P@5, cleavage
17/1
3 N@3 and F@4, cleavage
8/1
4 N@3 and A@1, cleavage
2/1
5 N@3 and A@6, cleavage
4/2
6 V@3 and A@6, cleavage

P/FP
P/FP
392-dataset on combined set
66/28
126/40
47/4
85/13
17/0
55/6
96/30
174/61
138/13
171/26
42/4
77/9
83/3
111/25
P/FP
P/FP
on 392-dataset on combined set
17/0
55/6
3/0
20/0
13/1
30/2
1/1
9/1
2/0
4/1
56/1
60/3

836

H. Kim et al.

However, the exhaustive method is not practical at all in terms of its computational complexity, and it is also very diﬃcult to generate rules from non-linear
classiﬁers such as MLP. Thus, it is not practical to use a black-box approach to
generate rules from a non-linear model such as MLP.
We used a decompositional approach to extract rules from MLP neural networks with 8 hidden units. The extracted rules from the MLP with two diﬀerent
CF threshold parameter values are listed in table 5. We could extract more general rules from neural networks. Notice that the rules “A@6 ” and “V@3 and
A@6 ” show low accuracy on training set but very high prediction accuracy on
test set. Frequency-based methods such as decision trees can not generate this
kind of rules because the methods are more dependent on training examples.
The rule“E@6 ” was discovered by any methods, and the rule “F@4 and P@5 ”
which is already well known by domain experts was discovered by neural network
methods.

4

Conclusion and Discussion

Narayanan et al. [8] used a decision tree to generate symbolic rules for HIV-1 protease cleavage site prediction because they could not interpret neural network
knowledge in symbolic format even though neural networks outperform decision trees. Because of the small number of examples available, probability (or
frequency)-based mining (i.e., decision trees and association rule mining) might
be misleading if the examples are not high domain-representative. R¨
ognvaldsson
et al. [9, 12] used linear perceptron to generate symbolic rules because nonlinear
multilayered perceptron (i.e., MLP) is hard to be interpreted. They had to show
that linear model performs as good as the nonlinear model before they used
the linear model to generate rules. However, the exhaustive method they used
is not computationally practical and have some eﬃciency issues on rule generation. Also we don’t have any clear evidence that the HIV-1 protease cleavage
site prediction problem is naturally linear problem.
In this paper, we applied a decompositional approach to extract symbolic
rules from a trained nonlinear neural network(i.e., MLP), and compared the
rules to the ones extracted by other methods. Experimental results show that
a neural network works better especially when only small number of training
examples are available. The extracted rules from the MLP seems to have good
generalization capability and thus predictability.

References
1. Robert Andrews, Joachim Diederich, and Alan B. Tickle. Survey and critique of
techniques for extracting rules from trained artiﬁcial neural networks. KnowledgeBased Systems, 8(6):373–389, 1995.
2. Yu-Dong Cai and Kuo-Chen Chou. Artiﬁcial neural network model for predicting
HIV protease cleavage sites in protein. Advances in Engineering Software, 29:119–
128, 1998.

Predictability of Rules in HIV-1 Protease Cleavage Site Analysis

837

3. Yu-Dong Cai, X.J. Liu, X.B. Xu, and Kuo-Chen Chou. Support vector machines
for predicting hiv protease cleavage sites in protein. journal of Computational
Chemistry, 23:267–274, 2002.
4. LiMin Fu. Neural Networks in Computer Intelligence. McGraw Hill, Inc., 1994.
5. LiMin Fu. Rule generation from neural networks. IEEE Transactions on Systems,
Man, and Cybernetics, 24(8):1114–1124, 1994.
6. Hyeoncheol Kim. Computationally eﬃcient heuristics for if-then rule extraction from feed-forward neural networks. Lecture Notes in Artiﬁcial Intelligence,
1967:170–182, 2000.
7. Alessandra Lumini and Nanni Loris. Machine learning for hiv-1 protease cleavage
site prediction. Proceedings of Artiﬁcial Intelligence and Application (AIA2005),
2005.
8. Ajit Narayanan, Xikun Wu, and Z. Rong Yang. Mining viral protease data to
extract cleavage knowledge. Bioinformatics, 18(Suppl.1):S5–S13, 2002.
9. Thorstein R¨
ognvaldsson and Liwen You. Why neural networks should not be used
for HIV-1 protease cleavage site prediction. Bioinformatics, 20(11):1702–1709, July
2004.
10. Rudy Setino and Huan Liu. Understanding neural networks via rule extraction.
Proceedings of the 14th International Conference on Neural Networks, (1):480–485,
1995.
11. Ismali A. Taha and Joydeep Ghosh. Symbolic interpretation of artiﬁcial neural
networks. IEEE Transactions on Knowledge and Data Engineering, 11(3):443–463,
1999.
12. Liwen You, Daniel Garwicz, and Thorstein R¨
ognvaldsson. Comprehensive bioinformatic analysis of the speciﬁcity of human immunodeﬁciency virus type 1 protease.
Journal of Virology, 79(19):12477–12486, Oct 2005.

