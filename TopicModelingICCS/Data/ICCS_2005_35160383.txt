A Toolbox Supporting Collaboration
in Networked Virtual Environments
Christoph Anthes and Jens Volkert
GUP, Institute of Graphics and Parallel Programming,
Johannes Kepler University, Altenbergerstrasse 69, A-4040 Linz, Austria
canthes@gup.uni-linz.ac.at

Abstract. A growing interest in Collaborative Virtual Environments
(CVEs) can be observed over the last few years. Geographically dislocated users share a common virtual space as if they were at the same
physical location. Although Virtual Reality (VR) is heading more and
more in the direction of creating lifelike environments and stimulating all
of the users senses the technology does not yet allow communication and
interaction as it is in the real world. A more abstract representation is
suﬃcient in most CVEs. This paper provides an overview on tools which
can be used to enhance communication and interaction in CVEs by visualising behaviour. Not only is a set of tools presented and classiﬁed,
an implementation approach on how to use these tools in a structured
way in form of a framework is also given.

1

Introduction

CVEs have emerged in various forms in the recent years. Interaction and communication in these environments are realised in many diﬀerent ways. The users
can send text messages, use audio and video communication; they can change
attributes of the simulation, can share data and might even be able to collaboratively manipulate this data. Our research concentrates on environments using
desktop VR or immersive VR systems such as Head Mounted Displays (HMDs)
or CAVEs [4]. In these environments the users can manipulate objects with
natural interaction techniques and experience the feeling of co-presence.
Performing collaborative tasks in Networked Virtual Environments (NVEs)
is more diﬃcult than performing them in a real world environment through the
limitations of the interfaces. Diﬀerent factors such as latencies, abstract graphical representation, limited ﬁeld of view (FOV) and precision of input devices
make natural interaction a challenging task. VR technology on the other hand
provides many possibilities to enhance or even to completely change the users
perception of the Virtual Environment (VE). Collaboration to the level that two
users can manipulate the same object simultaneously [3] combined with natural
interaction oﬀers new challenges in the area of computer supported cooperative
work (CSCW).
Although some systems support this collaboration on a technical level, i.e.
they provide networking, allow for concurrent object manipulation or provide
V.S. Sunderam et al. (Eds.): ICCS 2005, LNCS 3516, pp. 383–390, 2005.
c Springer-Verlag Berlin Heidelberg 2005

384

C. Anthes and J. Volkert

audio communication. However no tools exist so far which allow a rich enhancement of the VE to support collaboration on a more abstract level by visualising additional information about the users and the interaction. Worlds can be
shared and manipulated, but an abstract toolset visualising collaboration is not
available yet. This paper presents a set of tools which can be used to support
collaborative work in immersive NVEs. The tools are described in their functionality as well as their beneﬁt for collaboration. All of these tools augment the
environment in a way which is only available through VR technology. Communication and interaction could be improved through the use of the these tools.
This approach tries not to overcome technical issues such as lag or limited FOV,
it rather enhances the VE with additional information which is not available in a
real world environment. The toolbox is integrated into a CVE framework, which
provides support for collaboration on a technical level.
This section has given an introduction on the topic of NVEs and collaboration. The following section shows an insight into the area of related work. Section
three presents diﬀerent tools which have been identiﬁed as supportive for collaborative tasks. The fourth section will give an overview on the architecture of
the collaborative toolbox. The last section concludes the paper and shows some
future work in the area.

2

Related Work

Numerous research has been done in the area of NVEs and CVEs. Good overviews
on these topics are given in [8], [9] and [11] on a technical level. They describe
how VEs have to be designed to be scalable and interactive. Psychological aspects of collaborative tasks in NVEs have been examined [16]. Users perform
placing tasks to build up cube, while their behaviour is analysed. Aspects of
collaboration have been researched in a medical visualisation in [6]. Audio and
video communication are used in this scenario with several diﬀerent projection
VR systems. Advantages and disadvantages of diﬀerent VE systems and diﬀerent
communication methods are analysed.
Cooperative object manipulation has been researched by many authors. Good
examples are given in [14] where diﬀerent attributes of single objects can be
changed simultaneously. In other scenarios such as the piano carriers [15] the
users are able to manipulate the same object simultaneously in a way that they
manipulate the same attribute concurrently. In these examples the position of
the manipulated object results from a transformation based on both user inputs.
This is concurrent object manipulation in a three dimensional environment is
described in [10] as the highest level of collaboration. All these scenarios incorporate complex communication and interaction, but none of them provides
abstract tools in form visualisation of collaborative activities.
Many NVEs exit but none exploits the support of collaboration the level
desired. CAVERNSoft G2 [5] provides a collaborative geometry viewer. The
users have the possibility to share a scene in a VE. While discussing the scene

A Toolbox Supporting Collaboration in Networked Virtual Environments

385

they are able to manipulate it sequentially or they can indicate areas of the
shared world using a virtual pointing device.
CVEs usually have avatar representations which represent an embodiment
of the user. One example approach using avatars is given in [13]. There is full
anatomically based body articulation possible which allows the use of postures
and gestures in a CVE. Avatars allow dislocated users in a VE the feeling of
co-presence. Good overviews on the design of avatars are given in [1].

3

Tools

Diﬀerent categories or classes of tools can be identiﬁed for the support of collaboration in immersive VEs. They range from visual to audio support. The ﬁeld
of haptics becomes more and more important in the area of collaboration but
is not researched in this project. The tools presented are categorized into 5 different areas. Gaze visualisation enhances the local users information about the
remote users perception of the world. Object higlighting allows users to clearly
show interest on certain objects in the VE to remote users. An overview of the
whole VE is shown to the users by the map tool and complex movements can be
stressed by the use of the movement visualisation tool. Sonﬁcation is another aspect which is important especially when feedback is needed and textual messages
would decrease the feeling of immersion.
3.1

Gaze Visualisation

Gaze visualisation helps the collaborating partners to see of what the dislocated
users are possibly aware of. This tool is especially useful, when the FOV of the
users is limited, which is the case when HMDs or desktop VR systems are used.
Since most VR systems do not analyse the actual gaze of the user a rough
approximation is used. It is based on the gaze vector of the avatar. On desktop
systems the centre of the screen is assumed as the centre of interest of the user.
In immersive environments, where head tracking is available the gaze vector is
based on orientation and position of the head sensor. On a desktop system the
centre of the screen is used for approximation of the users gaze. Locally the gaze
centre can be displayed by a crosshair.
Frustum Visualisation. This type of gaze visualisation enables other participants in the environment to see what parts of the scene the remote user could
see. The frustum is displayed with a very high transparency value not to irritate
the observing users or to hide objects from them. In the case of desktop VR or
HMDs one frustum is displayed. If several camera orientations are used for the
diﬀerent display walls as in the case of CAVEs or multiple screen installations
the setup data of the display walls is transferred to the remote users and several frustums, one for each used screen, are displayed. Using stereo display the
frustums of the left and the right eye are merged together.
Gaze Vector Visualisation. The gaze vector indicates not only the direction
the user is looking; it indicates as well the distance of the far clipping plane

386

C. Anthes and J. Volkert

from the viewing frustum. A line is displayed from the avatars gaze centre in the
direction of the gaze. The line stops at the far clipping plane.
Parameters of the gaze visualisation are color, opacity and color range which
will allow diﬀerent frustum or vector visualisations for diﬀerent remote users.
3.2

Object Highlighting

The possibility to highlight objects in a NVE allows users more easily to show
to each other about which object they are discussing or with what they intend
to interact. In [12] several interaction techniques were suggested for object selection. These interaction techniques could easily be used for displaying an interest
in certain objects. CVEs could proﬁt from the use of two particular ways for
highlighting of objects.
At-a-Distance by Pointing. The user points at a certain object with his
input device. The position and orientation of the wand are measured, a ray is
cast in the scene and the ﬁrst object of which the bounding volume is hit is
highlighted. On a desktop VR environment either a picking algorithm is applied
to check where the user points with the mouse or the avatars hand movement is
implemented by emulating a tracked users hand.
Gaze Directed. The user looks at a certain object which will be highlighted.
For this the gaze vector calculated by the mechanisms described in the previous
section are be used. The ﬁrst bounding volume which is hit by the gaze vector
is highlighted.
These selection techniques can be used continuously during the whole simulation or they can be toggled by input signals from a wand, a mouse or a keyboard.
Several possibilities arise as well in the way how the highlighting is done. The
whole object or it’s bounding volume can be highlighted by a colour change or
a change of the objects opacity. Instead of color change ﬂashing of the color can
be applied to get more attention. If ﬂashing of an object is enabled, the color
change is activated after a the object is selected for certain amount of time.
Parameters of the object highlighting are similar to the ones given in gaze
visualisation. Color, opacity and color range highlight the objects geometry or
the objects bounding volume. The time to activate ﬂashing after selection has
to be set as well.
3.3

Movement Visualisation

Complex movements can be visualised in many ways. In the area of cartoons
which represent their own reality in a more abstract way complex motion is visualised by the use of motion lines. The support of this framework is given by
motion lines and shadow objects. Considering two dimensional CSCW applications the use of motion lines has been suggested by [7].
Two diﬀerent types of motion visualisation are given by the framework. Lines
at object vertices can be drawn as well as shadow objects can be displayed.

A Toolbox Supporting Collaboration in Networked Virtual Environments

387

Motion Lines. Motion Lines are drawn between two positions in the time line.
A point taken into account for representation of a motion line is a vertex from a
moved object. Using the motion lines it is not only possible to display the area
where the object was moved. By changing color, opacity or length of the motion
line it is possible to display the speed as well.
Shadow Objects. A shadow object is a copy of the geometry of the actual
object with a higher transparency value. Options in this tool allow the amount
of shadow objects displayed in a set timeframe with constant transparency or
increasing transparency over the time. Shadow objects can be connected with
motion lines from one time frame to another.
Since the use of shadow objects and motion lines can produce a high amount
of geometry data displayed it is possible to display bounding volumes or draw
lines between the diﬀerent bounding volumes instead of using the whole geometries. Less computational power is required in that case especially if the original
object consists of a high amount of polygons.
Movement visualisation is not only important for moved objects; it can be a
powerful tool to display avatar movement as well. The movement of the whole
avatar as a single object can be displayed, in the case of immersive VEs as the
movement of its hand can be displayed as well.
The parameters for the movement visualisation describe how long a line or
a shadow object shall be displayed. A fading factor speciﬁes the opacity of the
fading object.
3.4

Maps

Maps in NVEs can support spatial orientation and allow users to easily navigate
to their desired location in the world [2]. Using landmarks in larger environments
or ﬂoor plans in smaller environments allows users to discuss where they meet
in the VE.
The map has diﬀerent levels of detail depending on the current zoom factor.
The smaller the displayed area is the more detail is given. An abstract representation of large geometries is given from a top perspective. This is realised by
projecting the bounding volumes of large objects like buildings and landmarks
on the map. The next level of detail shows the geometries of the large objects.
Additionally it is possible to enhance the map with textual information. If the
VE simulates a city, street names and names of buildings can be given.
Supporting collaboration an avatar radar is implemented which displays all
surrounding avatars. If one avatar speaks to the client via voice connection the
avatar can be highlighted on the map for a set period of time. The names of
the avatars are displayed on the map as well. If a users avatar is interacting
with objects in the scene a zoom on the map should automatically indicate an
interaction. The focus on the map displays the interacting avatar form a top
view with the object he is interacting with. Diﬀerent states of the user in the
environments can be visualised by diﬀerent colours or textual output. Avatars
can be highlighted in a diﬀerent way when they are working with objects, when
they are talking, when they are moving or when they are idle.

388

C. Anthes and J. Volkert

Parameters of the map include initial scale, level of detail, avatar visualisation
and additional textual information.
3.5

Audio

Soniﬁcation of additional information can be useful in NVEs. If avatars are entering or leaving the environment the users should be notiﬁed. A voice notiﬁcation
that a user has entered the environment is given. In order to not to reduce the
immersion, status messages can be given via audio rather than being displayed
in a status window. If a collision with an object has happened or a user has
picked up or selected an object audio feedback provides a useful addition to the
visual cue. The volume of the audio can be set as a parameter. Diﬀerent aspects
of the soniﬁcation can be turned on and oﬀ, some of these values are user entry
notiﬁcation, interaction messages, system messages and collision messages.

4

Toolbox Architecture

The core of the toolbox is formed by the toolbox object which contains a list
of available tools. Depending on the tool changes are performed once, like gaze
visualisation where the frustum or the gaze vector are added to the scene graph,
or several times like object highlighting, where it has to be constantly checked
whether a new object has been selected or not. Each tool uses an update method
which will be called once or every frame. This is deﬁned by an update ﬂag
which can be set. Desiging your own tools becomes a very easy task, by deriving
from existing tool classes and overriding the update method. All tools in the
framework can be activated or deactivated individually. The conﬁguration ﬁle

Fig. 1. Toolbox Architecture

A Toolbox Supporting Collaboration in Networked Virtual Environments

389

is stored locally in an XML format providing the intial setup. The diﬀerent
attributes of the tools can be changed dynamically during runtime. Additional
tools can be activated or already active tools can be turned oﬀ. The users have
to be able to choose which tools suit them best.
The tool setup can aﬀect the VE in three diﬀerent ways. The tools can be
activated for the avatars representing the remote users locally, which allows
enhancements of the perception of what the remote users are doing. Examples
were discussed in section three. The second possibility allows the users to enable
some of their tools locally, an example would be in the area of soniﬁcation if a
user is notiﬁed if he picks up or uses an object in the VE. The last possibility
allows the user to share his conﬁguration ﬁle with other users. They could get
information which tools the user has activated. Figure 1 gives a rough overview
of the communication between the diﬀerent components of the toolbox and other
modules in the CVE.

5

Conclusions and Future Work

The paper has described a selection of tools which can be used to improve
collaborative activities in a NVE by augmenting the VE. The users working with
these tool are able to perceive the environment with additional information such
as the gazing direction, viewing area, movement visualisation of the dislocated
user. The tools represented were explained in functionality as well as in their
meaning for collaboration. Modular design allows users to select the type of
additional information they want to have displayed depending on their needs.
The framework including this toolbox can be used as a testbed for the analysis of
human behaviour in CVEs. Additional logging mechanisms which record tracking
data and other user input can be used to replay the whole simulation after the
collaborative task is completed. For analysis it can be heplful to enable the
frustum visualisation afterwards for example.

References
1. Norman I. Badler, Cary B. Phillips, and Bonnie L. Webber. Simulating Humans:
Computer Graphics Animation and Control. Oxford University Press, New York,
NY, USA, 1992.
2. Douglas A. Bowman, Elizabeth T. Davis, Larry F. Hodges, and Albert N. Badre.
Maintaining spatial orientation during travel in an immersive virtual environment.
Presence: Teleoperators and Virtual Environments, 8(6):618–631, 1999.
3. Wolfgang Broll. Interacting in distributed collaborative virtual environments. In
Virtual Reality Annual International Symposium (VRAIS), pages 148–155, Los
Alamitos, March 1995. IEEE.
4. Carolina Cruz-Neira, Daniel J. Sandin, Thomas A. Defanti, Robert V. Kenyon, and
John C. Hart. The cave: Audio visual experience automatic virtual environment.
Communications of the ACM, 35(6):64–72, June 1992.

390

C. Anthes and J. Volkert

5. CAVERNsoft G2: A Toolkit for High Performance Tele-Immersive Collaboration.
Kyoung park and yong cho and naveen krishnaprasad and chris scharver and
michael lewis and jason leigh and andrew johnson. In Virtual Reality Software
and Technology (VRST), pages 8–15, Seoul, Korea, 2000. ACM Press.
6. Gernot Goebbels and Vali Lalioti. Co-presence and co-working in distributed collaborative virtual environments. In Proceedings of the 1st international conference
on Computer graphics, virtual reality and visualisation, pages 109–114, Camps Bay,
Cape Town, South Africa, 2001. ACM Press.
7. Carl Gutwin. Traces: Visualizing the immediate past to support group interaction.
In Conference on Human-Computer Interaction and Computer Graphics, pages
43–50, May 2002.
8. Chris Joslin, Igor S. Pandzic, and Nadia Magnenat Thalmann. Trends in networked
collaborative virtual environments. Computer Communications, 26(5):430–437,
March 2003.
9. Michael R. Macedonia and Michael J. Zyda. A taxonomy for networked virtual
environments. IEEE MultiMedia, 4(1):48–56, Jan-Mar 1997.
10. David Margery, Bruno Arnaldi, and Noel Plouzeau. A general framework for cooperative manipulation in virtual environments. In M. Gervautz, A. Hildebrand,
and D. Schmalstieg, editors, Virtual Environments, pages 169–178. Eurographics,
Springer, June 1999.
11. Maja Matijasevic. A review of networked multi-user virtual environments. Technical report tr97-8-1, Center for Advanced Computer Studies, Virtual Reality and
Multimedia Laboratory, University of Southwestern Lousiana, USA, 1997.
12. Mark R. Mine. Virtual environment interaction techniques. Tr95-018, University
of North Carolina, Chapel Hill, NC 27599-3175, 1995.
13. Igor Pandzic, Tolga Capin, Elwin Lee, Nadia Magnenat Thalmann, and Daniel
Thalmann. A ﬂexible architecture for virtual humans in networked collaborative
virtual environments. Computer Graphics Forum, 16(3), September 1997.
14. Marcio S. Pinho, Doug A. Bowman, and Carla M.D.S. Freitas. Cooperative object manipulation in immersive virtual environments: Framework and techniques.
In Virtual Reality Software and Technology (VRST), pages 171–178, Hong Kong,
November 2002.
15. Roy A. Ruddle, Justin C. D. Savage, and Dylan M Jones. Symmetric and asymmetric action integration during cooperative object manipulation in virtual environments. ACM Transactions on Computer-Human Interaction, 9:285–308, 2002.
16. Josef Widestr¨
om, Ann-Soﬁe Axelsson, Ralph Schroeder, Alexander Nilsson, and
˚
Asa Abelin. The collaborative cube puzzle: A comparison of virtual and real
environments. In Collaborative Virtual Environments (CVE), pages 165–171, New
York, NY, USA, 2000. ACM Press.

