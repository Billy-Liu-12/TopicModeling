Grammatical Inference as a Tool
for Constructing Self-learning Syntactic Pattern
Recognition-Based Agents
Janusz Jurek
IT Systems Department, Jagiellonian University
Straszewskiego 27, 31-110 Cracow, Poland
jjurek@wzks.uj.edu.pl
http://www.wzks.uj.edu.pl/ksi/jwj

Abstract. Syntactic pattern recognition-based agents have been proven
to be a useful tool for constructing real-time process control intelligent
systems. In the paper the problem of self-learning schemes in the agents
is discussed. Learning capabilities are very important if practical applications of the agents are considered, since the agents should be able to
accumulate knowledge about the environment and ﬂexible react to the
changes in the environment. As it is shown in the paper, the learning
scheme in the agents can be based on a suitable grammatical inference
algorithm.

1

Introduction

Syntactic pattern recognition-based agents have been proven to be a useful tool
for constructing real-time process control intelligent multi-agent systems [6]. In
[2,3] we describe an example of such system: a multi-agent system implemented
for the purpose of the on-line monitoring, diagnosing, and controlling of a very
complex industrial-like equipment (a high energy physics detector). The system
had to perform four diﬀerent tasks:
–
–
–
–

monitoring and identifying the equipment behaviour,
analysing and diagnosing the equipment behaviour,
predicting consequences of the equipment behaviour,
controlling, i.e. taking proper actions (eg. setting operating modes for equipment components).

The decision about the multi-agent architecture of the system has been caused
by the fact that only fully-decentralised, distributed intelligent system consisting
of autonomous components acting in a parallel way can meet the hard realtime constraints: the necessity of performing four mentioned above tasks simultaneously and on-line with respect to hundreds of elementary components of the
equipment. The system has been built of diﬀerent types of agents correspondingly
to the layers of analysis (i.e. the whole equipment layer, modules (subdetectors)
layers, and components layers). We have chosen the syntactic pattern recognition
M. Bubak et al. (Eds.): ICCS 2008, Part III, LNCS 5103, pp. 712–721, 2008.
c Springer-Verlag Berlin Heidelberg 2008

Grammatical Inference for Self-learning Agents

713

approach as a base for constructing lowest layer agents of reactive type. They
are responsible for monitoring and recognition of the behaviour of particular
components.
The main part of syntactic pattern recognition agents is a parser. The parser
performs syntax analysis of the symbolic description of a component behaviour.
The knowledge about the possible behaviour of a component is remembered
in the agent in the form of a formal string grammar. The grammar is used to
construct a control table for the parser. The advantages of using the syntactic pattern recognition approach consist in very good computational properties
(it is possible to implement an advanced parser of the linear, O(n), computational complexity) and the capability to analyse and recognise even exceedingly
complex patterns representing a process in time series. These two advantages
make the approach better in case of constructing real-time process control intelligent systems than many other computationally ineﬃcient classical artiﬁcial
intelligence methods [10].
However, after several years of practical experiments with syntactic pattern
recognition-based agents we have identiﬁed one important problem of the implementation of such agents: the diﬃculty in providing a self-learning feature.
Learning capabilities are very important if we consider practical applications of
the agents, since the agents should be able to accumulate knowledge about the
environment (i.e. the possible behaviour of a component which is being monitored) and ﬂexible and autonomously react to the changes in the environment.
The learning scheme in the agents can be based on a suitable grammatical inference algorithm (i.e. the algorithm of automatical deﬁnition of a grammar from
the sample of a pattern language). Unfortunately grammatical inference is still
one of the main open issues in the syntactic pattern recognition area.
The research into grammatical inference of string grammars started almost
forty years ago. The ﬁrst, basic results were published in the seventies. During
next years the methods of grammatical inference were improved in the aspects of
the ”quality” of generated grammar, and the computational eﬃciency. Although
many diﬀerent results have been already achieved (especially in case of regular grammars) [4,5], there is still lack of models of inferencing context-sensitive
grammars, that is grammars of big generative/discriminative power. Most methods already developed are based on the identiﬁcation of structures in words in
a sample and the deﬁnition of the sequence of the structures. The dependencies
between numbers of symbols in words are not examined. This results from the
fact that the discriminative power of grammars we are able to inferred is too
weak to reﬂect such dependencies. The only one practical method that allows to
consider such quantitative information (in the context of syntactic pattern recognition) has been developed by Alqu´ezar and Sanfeliu [1]. They have proposed a
new type of grammars (ARE grammars, characterised by a big discriminative
power) and a proper grammatical inference algorithm. Although the method is
very innovative, it has some disadvantages. In particular, the time complexity
of the grammatical inference is exponential, which makes the model unsuitable
for the application in a real-time environment.

714

J. Jurek

In the paper we present the recent results of the research into constructing a
self-learning algorithm for syntactic pattern recognition-based agents. In section
2 we describe the general scheme of the agents. Section 3 contains the deﬁnitions
corresponding to the string grammars which we use as a knowledge base in
the agents. The description of the model of the self-learning of the agents (via
grammatical inference) is included in section 4. Section 5 contains presentation
of some experimental results. Conclusions are included in the ﬁnal section.

2

Syntactic Pattern Recognition-Based Agents

There are three main elements of syntactic pattern recognition-based agents: a
knowledge base (in the form of a formal grammar), a parser (a control table for
the parser is constructed on the base of a grammar being the knowledge base),
and a self-learning module (based on a grammatical inference algorithm used to
update the grammar). The general scheme of a syntactic pattern recognitionbased agent is shown in Fig. 1.

Knowledge Base
(Grammar)
Control table
for the parser

Symbolic description
of component behaviour
(String)

Component
Behaviour
Recognition
(Parsing)

Recognised
behaviour

Unrecognised
behaviour

Self-Learning
(Grammatical
inference)

Modified
grammar

Fig. 1. A general scheme of a syntactic pattern recognition-based agent

A syntactic pattern recognition-based agent is of reactive type: it should be
able to respond timely to changes in its environment (changes in the behaviour
of a component which is being monitored) and to acquire knowledge about the
environment. The agent receives symbolic data (a string of symbols) representing
characteristics of a component behaviour. The analysis of the behaviour is made
by the parsing of a received string. The results are outputted to the higher layer
agents with diﬀerent architecture which are beyond the scope of the paper [2].

Grammatical Inference for Self-learning Agents

715

When a received string cannot be recognised by the parser, then the self-learning
activity in the agent should take place. The inability to recognise the string pattern
means that the current grammar should be replaced by a broaden one which generates the missing pattern. Self-learning can be done via grammatical inference.
The result is an updated grammar which becomes a new knowledge base.
The most important decision concerning the implementation of the agent has
been the choice of GDPLL(k) grammars [8] as a formal string grammar which
is used to store knowledge about a component behaviour and to construct a
control table for the parser. The deﬁnition of GDPLL(k) grammar and its brief
characteristics are included in next section.

3

Quasi Context Sensitive GDPLL(k) Grammars

Let us introduce two basic deﬁnitions [3,8].
Deﬁnition 1. A generalised dynamically programmed context-free grammar is
a six-tuple: G = (V, Σ, O, P, S, M ), where V is a ﬁnite, nonempty alphabet; Σ ⊂
V is a ﬁnite, nonempty set of terminal symbols (let N = V \Σ); O is a set of basic
operations on the values stored in the memory; S ∈ N is the starting symbol; M
is the memory; P is a ﬁnite set of productions of the form: pi = (μi , Li , Ri , Ai ) in
which μi : M −→ {T RU E, F ALSE} is the predicate of applicability of the production pi deﬁned with the use of operations (∈ O) performed over M ; Li ∈ N
and Ri ∈ V ∗ are left- and right-hand sides of pi respectively; Ai is the sequence
of operations (∈ O) over M , which should be performed if the production is to
be applied.
Deﬁnition 2. Let G = (V, Σ, O, P, S, M ) be a generalised dynamically programmed context-free grammar. The grammar G is called a Generalised Dynamically Programmed LL(k) grammar, GDPLL(k) grammar, if: 1) the LL(k) condition
of deterministic derivation is fulﬁlled, and: 2) the number of steps during derivation of any terminal symbol is limited by a constant.
As we mentioned in Section 2, GDPLL(k) grammars have been chosen to store
knowledge about components behaviour in the agents. The choice has resulted
from two main features of GDPLL(k) grammars:
1. GDPLL(k) grammars are characterised by very good discriminative properties since their generative power is stronger than context-free grammars
and ”almost” as big as context-sensitive grammars [8]. It means that the
grammars can store knowledge even about highly complicated patterns representing a component behaviour.
2. It is possible to implement a parser for the languages generated by
GDPLL(k) grammars which performs syntax analysis in the linear time.
This feature is crucial if we consider embedding of the agents in real-time
applications.

716

J. Jurek

In case of GDPLL(k) grammars these two contradictory requirements are
balanced. However, as we noticed in Section 2, there is one more requirement
concerning a class of grammars to be used as a knowledge base in the agents: the
ability to construct a grammatical inference algorithm (preferably of polynomial
time complexity) needed to provide self-learning feature in the agents. Grammatical inference is a well-known open problem in the syntactic pattern recognition
area. It is especially diﬃcult in case of grammars stronger than than context-free
ones. We have done the research into the problem of the grammatical inference
of GDPLL(k) grammars for a few years [7,9]. The algorithms developed during
last years have been mostly dedicated to particular applications. Recently, we
have achieved results that allow us to implement a self-learning system which
can be used more generally: in many possible practical applications.

4

Grammatical Inference

Firstly, let us present main deﬁnitions needed to discuss the grammatical inference algorithm [9].
Deﬁnition 3. Let A is a set of all (terminal) symbols which appear in the
sample, Z set of integer variables. Polynomial speciﬁcation of a language is of
p (n )
the form: Lp (A, Z) = Si j k where: pj is a polynomial of a variable nk ∈ Z;
variable nk can be assigned only values greater or equal 1; Si , called polynomial
structure, is deﬁned in a recursive way:
a) Si = (ai1 ...air ), where aij ∈ A. (Si is a basic polynomial structure), or:
p (n )
p (n )
b) Si = (Si1i1 i1 ...Sirir ir ), where Sik is deﬁned as in a) or b). (Si is a complex
polynomial structure).
Extended polynomial speciﬁcation of a language Lep (A, Z) is deﬁned in the
following way:
a) Lep (A, Z) = Lp (A, Z) or:
b) Lep (A, Z) = L1ep (A1 , Z1 ) L2ep (A2 , Z2 ) ... Lzep (Az , Zz ), or:
c) Lep (A, Z) = L1ep (A1 , Z1 ) + L2ep (A2 , Z2 ) + ... + Lzep (Az , Zz ),
if proper conditions of the unambiguity of the speciﬁcation are fulﬁlled in case of
all Zi and Ai (see: [9]).
Example 1. Let A = {a, b, c} be a set of terminal symbols, Z = {n, m} be a
2
3
set of integer variables. Then: Lp (A, Z) = ((ab)2n+1 cn (ba)2m )n+2 (ab)m is an
example of polynomial speciﬁcation of a language. The polynomial structures in
the speciﬁcation are the following:
2

S1 = ((ab)2n+1 cn (ba)2m )n+2 (ab)m
2
S11 = (ab)2n+1 cn (ba)2m
S12 = ab
S111 = ab
S112 = c
S113 = ba

3

p1 ≡ 1
p11 (n) = n + 2
p12 (m) = m3
p111 (n) = 2n + 1
p112 (n) = n2
p113 (m) = 2m

Grammatical Inference for Self-learning Agents

717

Extended polynomial speciﬁcation of a language is built as a catenation (case
”b” in Deﬁnition 3) or alternatives (case ”c” in Deﬁnition 4) of polynomial
2
speciﬁcations. Then: Lep (A, Z) = ((ab)2n+1 + cb2m )abn can be an example of
extended polynomial speciﬁcation of a language.
Our approach to the inferencing GDPLL(k) grammars is based on the following
method. The input is the sample of a language (containing a new string pattern representing an unrecognised behaviour). We divide the inference process
into two phases. The ﬁrst one is responsible for extraction of the features of
the sample and generalisation of the sample. The result is the extended polynomial speciﬁcation of a language. In the second phase, a GDPLL(k) grammar is
generated on the basis of the extended polynomial speciﬁcation of the language.
Now, let us present the ﬁrst phase: the method of creating the extended
polynomial speciﬁcation of a language from the sample. Let us notice that our
method is canonical. It can be used for the detection of basic features only, and
for relatively simple generalisation of the sample. The canonical character of the
method may broaden the area of its possible applications.
Let the sample of a language be a set of m words: Sample = {w1 , ..., wm }.
Let Σ be a common alphabet for all words in the sample. Words belonging to
the sample will be written in the form: w = an1 1 an2 2 ... ank k .
Deﬁnition 4. Let us consider two words: w1 = ap11 ap22 ... apkk and w2 =
bq11 bq22 ... bql l . We say that words w1 i w2 are sequentially equivalent, if the
following conditions are fulﬁlled: k = l and ai = bi for i = 1, ..., k. The template
of an extended polynomial speciﬁcation is an expression constructed accordingly
to the deﬁnition of extended polynomial speciﬁcation Lep , which is built with
sets of sequentially equivalent words instead of polynomial speciﬁcations Lp .
Example 2. Let A = {a, b, c} be a set of terminal symbols. Two words: a2 b4 a2 c7
and a6 b2 a3 c5 are sequentially equivalent, since both words are built with the
same sequence of diﬀerent symbols: a, b, a, c.
The algorithm of the construction of the extended polynomial speciﬁcation of
a language can be divided into three steps. In the ﬁrst step the template of
the extended polynomial speciﬁcation is built. In the second step each set of
sequentially equivalent words (in the template) is generalised in the form of a
polynomial speciﬁcation. In the last step the extended polynomial speciﬁcation
is generated (by applying the results of step 2 to the template).
Algorithm 1. The algorithm of the construction of the extended polynomial
speciﬁcation of the language from Sample = {w1 , ..., wm } consists of three steps:
Step 1. We construct the template of the extended polynomial speciﬁcation.
Let V {v1 , ..., vn } be a set which is a variable in the template of an extended
polynomial speciﬁcation which is being constructed, where v1 , ..., vn are words
or fragments of words belonging to Sample. Initially, let the template of an

718

J. Jurek

extended polynomial speciﬁcation be the variable: V {w1 , ..., wm }. While there
is such a set V in the template, which is not the set of sequentially equivalent words, we transform the template according to two rules. The ﬁrst one is:
V {ws1 a1 wr1 , ..., wsn an wrn } := V {ws1 , ..., wsn } V {a1 wr1 , ..., an wrn }, if wsi are
sequentially equivalent (for i = 1, ..., n) and there are such symbols ai and
aj diﬀerent than the last symbol of ws1 that ai = aj . The second one is:
V {a1 w1 , ..., an wn } := V {a11 w11 , ..., a1k w1k } + ... + V {as1 ws1 , ..., ask wsk },
if for any two symbols axi , axj : axi = axj , and for any two symbols axi , ayj ,
where x = y : ai = aj . The result is the template in which all V are sets of
sequentially equivalent words.
Step 2. We generalise each set of sequentially equivalent words (in the template) in the form of a polynomial speciﬁcation. Let P = {w1 , ..., wq } be a
set of sequentially equivalent words. Let the word wi ∈ P be of the form:
ni
n
n
wi = a1 i1 a2 i2 ... ak k . The canonical algorithm of the construction of polynomial speciﬁcation Lp (A, Z) for P is the following. For Lp (A, Z) we deﬁne: A =
p (x )
p (x )
{a1 , ..., ak }, and Z = {x1 , ..., xk }. We assume that Lp (A, Z) = a11 1 ... akk k .
For each j = 1, ..., k we search for the linear function of variable xj , which
describes dependencies between numbers nij (where: i = 1, ..., q). Let the function be of the form: pj (xj ) = dj xj + rj . Then we look for linear dependencies
between xj = x1 , ..., xk allowing to reduce Z set. For each pair xg and xh (where
g = 1, ..., k, h = 1, ..., k) we check whether xh = xg + s for each words in P and a
constant value s. If the result of the test is positive, we store a new function for
the index h, and we delete variable xh from Z set. As the outcome of this step
we get polynomial speciﬁcation Lp (A, Z) such that all words from P belong to
Lp (A, Z), as well as all other words which are considered to be ”similar”.
Step 3. We deﬁne the extended polynomial speciﬁcation by inserting polynomial
speciﬁcations (generated in step 2) to the template of the extended polynomial
speciﬁcation (generated in step 1).
The algorithm above is of polynomial complexity. The extended polynomial speciﬁcation being the result of the algorithm is the input for the second phase.
Now, let us present the algorithm of automatic generation of a GDPLL(k)
grammar from polynomial speciﬁcation of a language (as the main algorithm
of the second phase). The algorithm can be divided into two steps. In the ﬁrst
step we deﬁne a separate grammar for each polynomial speciﬁcation which is included in the extended polynomial speciﬁcation. In the second step we construct
a target GDPLL(k) grammar for extended polynomial speciﬁcation on the basis
of the grammars constructed in the ﬁrst step.
Algorithm 2. The algorithm of automatic generation of a GDPLL(k) grammar from polynomial speciﬁcation of a language consists of two steps:
Step 1. We deﬁne a separate grammar for each polynomial speciﬁcation which
is included in the extended polynomial speciﬁcation in the following way.
Let Lp (A, Z) be a polynomial speciﬁcation of a language. We will construct

Grammatical Inference for Self-learning Agents

719

a GDPLL(k) grammar G = (V, Σ, O, P, S, M ) generating the language. Let
Σ := A. For each variable n ∈ Z in Lp (A, Z) we deﬁne two variables: vn and
dn in the grammar memory M . For each polynomial structure S in Lp (A, Z) we
deﬁne: a nonterminal symbol XS ∈ V , and two memory variables: cS and eS in
M . For each S p(n) structure in Lp (A, Z) we deﬁne a set of productions in P in
the following way. We use memory variables to implement ”loops” during derivation. Current value of n (in exponent expression) is stored in vn . Variable cS is a
counter of repetitions of S structure. Variable eS contains the current evaluation
of exponent expression for S structure. Boolean variable dn stores information
whether n is ﬁxed or not (i.e. if the value of n has been determined before).
Operations on the memory deﬁned for each production are responsible for ”programming” proper number of repetitions during derivation. (Formal deﬁnition
of the set of production is described in [7]).
Step 2. We construct a target GDPLL(k) grammar for extended polynomial
speciﬁcation in the following way. Let Lep (A, Z) be an extended polynomial
speciﬁcation of language L. Let us assume that Lep (A, Z) is built by m polynomial speciﬁcations Lip (Ai , Z i ) (we will use Lip to denote Lip (Ai , Z i )). Let us
assume that for each Lip a grammar Gi = (V i , Σ i , O, P i , S i , M i ) has been constructed (in step 1) in such a way that: N i ∩ N j = ∅ and M i ∩ M j = ∅ for
i = j. Now we deﬁne: V := V i , Σ := Σ i , P := P i and M := M i ,
for i := 1, ..., m. Then for each extended polynomial speciﬁcation Ljep (where
j ≥ m) included in the construction of the speciﬁcation Lep (A, Z) we deﬁne:
if Ljep = Ljp , then a new symbol E j is added to N ; if Ljep = Ljep1 Ljep2 ... Ljepz ,
then a new production p: E j −→ E j1 ... E jz is added to P , where symbols
E j represent adequate extended polynomial speciﬁcations of the language; if
Lep = L1ep + L2ep + ... + Lzep , then z new productions: E j −→ E j1 , ...,
E j −→ E jz are added to P .
The result of Algorithm 2 is a new (updated) deﬁnition of a GDPLL(k) grammar being a new knowledge base for the agent. Both algorithms presented above
are of polynomial computational complexity, so the whole grammatical inference
process is also computationally eﬃcient (the time complexity of the whole grammatical inference is O(m3 ∗ n3 ), where m is the number of words in a sample, n
is the maximum length of a word in a sample).

5

Implementation and Experimental Results

All grammatical inference algorithms presented in previous section (being the
realization of the learning self-learning scheme in the syntactic pattern
recognition-based agents) have been implemented and tested. The experimental implementation has been prepared in C++ language with the use of the
object-oriented approach.
Two aspects of the grammatical inference module functioning have been veriﬁed: the correctness of the results of the inference, and the time eﬃciency.

720

J. Jurek

Table 1. Inference of the grammar generating language: {an bn an : n = 1, 2, . . .}
Sample
1
2
3
4
5
6

Number of words
3
7
10
50
100
1000

Maximum word length
9
30
90
900
900
900

Time (s)
27 × 10−5
28 × 10−5
29 × 10−5
33 × 10−5
39 × 10−5
246 × 10−5

In order to test the module we have used mainly such samples that describe
context-sensitive languages (intuitively: samples in which context dependencies
concerning the number of given symbols in a word can be observed).
A typical example of a strong context-sensitive language (a language which
cannot be generated by a context-free grammar) is the language: {an bn an :
n = 1, 2, . . .}. The context property of the language can be easily proven on
the basis of the well-known pumping lemma. We have prepared a rich set of
test data for this language. The set has included samples having from 3 to
1000 words. In all cases grammatical inference module has recognised proper
dependencies inside words (the same number of a, b, and c symbols in a word)
and dependencies between words (all words are of the same pattern an bn an ).
Then a proper GDPLL(k) grammar has been automatically generated.
The results of time tests of the grammatical inference module are shown in
Table 1. The tests have been performed on relatively old PC (processor: AMD
Athlon XP 2600+ 2.1 GHz, Microsoft Windows XP Professional system).
All the experiments and tests conﬁrm that the algorithms of grammatical
inference are of good (polynomial) computational complexity. Usually, execution
time is much better than predicted on the base of (pessimistic) estimation O(m3 ∗
n3 ), where m is the number of words in a sample, n is the maximum length of
a word in a sample.

6

Concluding Remarks

In the paper we have presented the recent results of the research into construction
of syntactic pattern recognition-based agents. Such agents have been designed
as a tool for the implementation of real-time process control intelligent multiagent systems. The process diagnostic and control is a natural application for
agents, since process controllers are themselves autonomous reactive systems. On
the other hand, the real-time requirements make the construction of the agents
particularly diﬃcult [11,12,13].
Although we have proven practical usefulness of the syntactic pattern recognition-based agents by the application in a real-time process control system
[2,6], we have observed that there is a signiﬁcant problem concerning the deﬁnition of the universal self-learning method in the agents. Let us notice, that
learning capabilities are very important if we consider practical applications of
the agents, since they should be able to gain knowledge about the changes in

Grammatical Inference for Self-learning Agents

721

the environment. In the paper we have shown that the learning scheme in the
agents can be based on a grammatical inference algorithm. If a new (unrecognised) pattern of the behaviour in the monitored process appears, an updated
grammar (being the knowledge base in the agents) will be generated. We have
developed the algorithm that can automatically produce the deﬁnition of a very
strong (in the sense of discriminative power) quasi-context sensitive grammar.
The algorithm is of polynomial computational complexity.
The method presented in the paper still needs a thorough practical evaluation.
We are going to test it in the environment of several diﬀerent applications where
the need of a self-learning feature of the agents is particularly noticeable. The
discussion of the results will be a subject of further publications.

References
1. Alqu´ezar, R., Sanfeliu, A.: Recognition and learning of a class of context-sensitive
languages described by augmented regular expressions. Pattern Recognition 30,
163–182 (1997)
2. Flasi´
nski, M.: Automata-Based Multi-agent Model as a Tool for Constructing
Real-Time Intelligent Control Systems. In: Dunin-Keplicz, B., Nawarecki, E. (eds.)
CEEMAS 2001. LNCS (LNAI), vol. 2296, pp. 103–110. Springer, Heidelberg (2002)
3. Flasi´
nski, M., Jurek, J.: Dynamically Programmed Automata for Quasi Context
Sensitive Languages as a Tool for Inference Support in Pattern Recognition-Based
Real-Time Control Expert Systems. Pattern Recognition 32, 671–690 (1999)
4. De La Higuera, C.: Current Trends in Grammatical Inference. In: Amin, A., Pudil,
P., Ferri, F.J., I˜
nesta, J.M. (eds.) SPR 2000 and SSPR 2000. LNCS, vol. 1876, pp.
28–31. Springer, Heidelberg (2000)
5. De La Higuera, C.: A bibliographical study of grammatical inference. Pattern
Recognition 38, 1332–1348 (2005)
6. Jurek, J.: Syntactic Pattern Recognition-Based Agents for Real-Time Expert Systems. In: Dunin-Keplicz, B., Nawarecki, E. (eds.) CEEMAS 2001. LNCS (LNAI),
vol. 2296, pp. 161–168. Springer, Heidelberg (2002)
7. Jurek, J.: Towards grammatical inferencing of GDPLL(k) grammars for applications in syntactic pattern recognition-based expert systems. In: Rutkowski, L.,
Siekmann, J.H., Tadeusiewicz, R., Zadeh, L.A. (eds.) ICAISC 2004. LNCS (LNAI),
vol. 3070, pp. 604–609. Springer, Heidelberg (2004)
8. Jurek, J.: Recent developments of the syntactic pattern recognition model based on
quasi-context sensitive languages. Pattern Recognition Letters 26, 1011–1018 (2005)
9. Jurek, J.: Generalisation of a Language Sample for Grammatical Inference of
GDPLL(k) Grammars. In: Computer Recognition Systems 2. Advances in Soft
Computing series, pp. 282–288. Springer, Heidelberg (2007)
10. Negnevitsky, M.: Artiﬁcial Intelligence. A Guide to Intelligent Systems. AddisonWesley, Reading (2002)
11. Niederberger, C., Gross, M.: Hierarchical and Heterogenous Reactive Agents for
Real-Time Applications. Computer Graphics Forum 22, 323–331 (2003)
12. Russell, S.J., Norvig, P.: Artiﬁcial Intelligence: A Modern Approach, 2nd edn.
Prentice-Hall, Englewood Cliﬀs (2002)
13. Soto, I., Garijo, M., Iglesias, C.A., Ramos, M.: An agent architecture to fulﬁll
real-time requirements. In: Proceedings of the Fourth International Conference on
Autonomous Agents, Barcelona, Spain, June 03–07, 2000, pp. 475–482 (2000)

