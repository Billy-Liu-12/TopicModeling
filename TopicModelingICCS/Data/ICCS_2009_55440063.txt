A Parallel High-Order Discontinuous Galerkin
Shallow Water Model
Claes Eskilsson1, Yaakoub El-Khamra1, David Rideout2 , Gabrielle Allen1 ,
Q. Jim Chen1 , and Mayank Tyagi1
1

2

Louisiana State University, Baton Rouge LA 70803, USA
Perimeter Institute for Theoretical Physics, Waterloo Ontario N2L 2Y5, Canada

Abstract. The depth-integrated shallow water equations are frequently
used for simulating geophysical ﬂows, such as storm-surges, tsunamis and
river ﬂooding. In this paper a parallel shallow water solver using an unstructured high-order discontinuous Galerkin method is presented. The
spatial discretization of the model is based on the Nektar++ spectral/hp
library and the model is numerically shown to exhibit the expected exponential convergence. The parallelism of the model has been achieved
within the Cactus Framework. The model has so far been executed successfully on up to 128 cores and it is shown that both weak and strong
scaling are largely independent of the spatial order of the scheme. Results are also presented for the wave ﬂume interaction with ﬁve upright
cylinders.

1

Introduction

This paper presents a ﬁrst step towards a community coastal modeling toolkit
for nearshore surface water waves based on spectral/hp element methods. The
ultimate goal is a scalable, parallel, non-hydrostatic wave solver, based on multilayered Boussinesq-type equations including time-dependent bathymetry and
sediment transport. In this paper we outline the ongoing work of the parallel implementation of non-dispersive two-dimensional shallow water equations
(SWE). As Boussinesq-type equations are higher-order extensions to the SWE,
the SWE constitute the natural initial stepping stone. A driving force behind
this eﬀort is storm surge modeling and the SWE model presented here can be
used as an eﬃcient hydrodynamic core for such simulations.
There are several motivations for using spectral/hp element methods (i.e. ﬁnite element methods of arbitrarily high order) rather than more traditional
methods. Spectral/hp elements provide a ﬂexible setting where both the element size and the polynomial order within the elements can be altered. Further,
high-order methods tend to be more computationally eﬃcient for long-time integrations compared to low-order methods due to their inherently small numerical diﬀusion/dispersion error. The discontinuous Galerkin (DG) ﬂavour of
spectral/hp elements was choosen mainly since the resulting global mass matrix is block-diagonal. Thus, any explicit time-stepping scheme can update the
G. Allen et al. (Eds.): ICCS 2009, Part I, LNCS 5544, pp. 63–72, 2009.
c Springer-Verlag Berlin Heidelberg 2009

64

C. Eskilsson et al.

numerical solution in an element-by-element fashion. The DG method is also
able to incorporate well established shock-capturing techniques from the ﬁnite
volume (FV) framework.
SWE models are most frequently based on shock-capturing FV methods, but
over the last decade several DG SWE models have been presented. Later studies
emphasize the use of high-order schmes [5,4,11,6] and the present state-of-theart DG SWE models rely on adaptivity: [1] include h-type adaptivity while [13]
illustrates the beneﬁts of p-type adaptivity. In contrast to a recent study [12] that
addresses the parallel performance of a low-order DG SWE model, the present
study is addressing the performance of high-order schemes.
The coastal DG wave model is based on the spectral/hp element library Nektar++ [9,10], while parallelism has been achieved by adding support for unstructured two-dimensional meshes into the computational framework Cactus [8,7].
This separation of programming tasks allows coastal engineers to focus on developing coastal code using Nektar++ and the computational scientists to focus on
parallelism, performance and scalability of the unstructured mesh driver. Further, the adoption of an underlying parallel framework provides a methodology
to develop interoperable modules to add additional solvers and models leading
to a comprehensive toolkit for modeling coastal environments.

2

Shallow Water Equations

The shallow water equations are a set of non-linear hyperbolic equations. The
equations are derived under the assumption of hydrostatic pressure, and thus
the SWE are only valid for long waves (the rule of thumb being that the still
water depth to wavelength ratio should be less than 1/20). The conservation
form of the SWE read
∂U
+ ∇ · F(U) = S(U) ,
(1)
∂t
where U = [H , Hu , Hv]T is the vector of conserved variables and F(U) =
[E(U) , G(U)]T is the ﬂux vector deﬁned by:
⎤
⎤
⎡
⎡
Hu
Hv
⎦.
Huv
E = ⎣ Hu2 + gH 2 /2 ⎦ ,
G=⎣
Huv
Hv 2 + gH 2 /2
Here H(x, t) = η(x, t) + d(x) is the total water depth, η(x, t) is the free surface
elevation and d(x) is the still water depth; u(x, t) = [u(x, t) , v(x, t)]T denotes
the depth-averaged velocity in the x- and y-direction, respectively, while g is the
acceleration of gravity.
The source terms S(U) can contain forcing due to e.g. bathymetry, bottom
friction, atmospheric pressure, Coriolis force, wind stresses and diﬀusion. The
objective of this study is to assess the performance of the computational core
rather than present any solution of real-life engineering cases, and we focus here
on the homogeneous version of the SWE.

A Parallel High-Order DG Shallow Water Model

3
3.1

65

Numerical Scheme
DG Discretization

Let Ωh denote the partition of the computational domain, Ω, into N nonoverlapping elements Ωe with elemental boundaries Γe . The diameter of the
element Ωe is given by he and subsequently h = max(h1 , . . . , hN ). We introduce
the discrete polynomial space
Vδ = v ∈ L2 (Ω) : v|Ωe ∈ P p (Ωe ), ∀Ωe ∈ Ωh ,
where P p is the space of polynomials of degree at most p in the element Ωe .
We proceed by multiplying eq. (1) with a piecewise smooth test function q(x)
and integrate over the local element Ωe . We then approximate U and q with
polynomial expansions of order p:

Ωe

qδ

∂Uδ
dx +
∂t

Ωe

qδ ∇ · F(Uδ ) dx =

Ωe

qδ S(Uδ ) dx .

After applying the divergence theorem and exchanging the boundary ﬂux term
with a numerical ﬂux, we can state the discrete DG method as: ﬁnd Uδ ∈ Vδ
such that for all qδ ∈ Vδ and for all Ωe ∈ Ωh
Ωe

qδ

∂Uδ
dx −
∂t

Ωe

+
Γe

∇qδ · F(Uδ ) dx
ˆ δ ) · n dS =
qδ F(U

Ωe

qδ S(Uδ ) dx ,

(2)

ˆ denotes the continuous numerical
where n is the outward unit normal to Γe and F
ﬂux used to couple the elements together. In this study we use the popular HLLC
Riemann solver [14] as the numerical ﬂux.
The approximation of an arbitrary variable fδ ∈ Vδ in the local element Ωe
can be expressed as
e
Ndof
−1

˜fe [i]φi (x) ,

fδ (x, t) =

x ∈ Ωe ,

i=0
e
where ˜fe [i] is the time-dependent vector consisting of the Ndof
elemental degrees
of freedom of expansion coeﬃcients and φi (x) are the trial functions.
Introducing the elemental mass matrix

Me [p][q] =
Ωe

φep (x) φeq (x) dx ,

and the elemental weak derivative matrices
Dex [p][q] =

Ωe

∂φep (x) e
φq (x) dx ,
∂x

Dey [p][q] =

Ωe

∂φep (x) e
φq (x) dx ,
∂y

66

C. Eskilsson et al.

e
where 0 ≤ p, q ≤ Ndof
− 1 , we can write eq. (2) in elemental form as

Me

˜
∂U
˜ − Dey G(U)
˜ + f e = Me S(U)
˜ ,
− Dex E(U)
∂t

in which
f e [p] =
Γe

ˆ δ ) · n dS .
φp (x) F(U

In contrast to continuous Galerkin methods, for DG methods the global matrices are just a concatenation of local elemental matrices. Hence, using explicit
time-stepping schemes the solution can be computed in an element-by-element
fashion.
Following the standard Galerkin formulation we use the same functions for the
test and trial functions. Former studies concerned with high-order DG methods
for SWE have primarily used either the modal orthogonal Prioli-KoornwinderDubiner (PKD) basis [4,11] or the nodal electrostatic basis [5,6]. Here we use
the C 0 modiﬁed PKD basis [10], which can be decomposed into boundary and
interior modes. For this choice of basis the elemental mass matrix is sparse, but
on the other hand there are only the boundary modes involved in the computation of the boundary integral. More importantly this choice of expansion basis
gives the possibility to use static condensation if a global equation system must
be solved [10]. Admittedly, the present SWE solver does not involve a global
solve, but this should be an important feature in the future development of a
Boussinesq solver.
The semi-discretized equations are advanced in time using a second- or thirdorder TVD Runge-Kutta method and all boundary conditions are enforced
weakly through the use of the Riemann solver.
3.2

Computational Approach

The discontinuous Galerkin scheme outlined in section 3.1 was ﬁrst implemented
in a stand-alone serial code using the open-source spectral/hp library Nektar++
[9]. Nektar++ provides the fundamental tools associated with a high-order ﬁnite
element method, such as the calculation of expansion functions, inner products
and diﬀerentiation. With regard to the high-order discretization our work has
been focused on implementing a solver structure for time-dependent problems.
This includes a SWE class containing functions for the evaluation of the ﬂux
vector, numerical ﬂuxes, equation dependent boundary condition, various source
terms, etc.
We rely on the Cactus Framework [8] to provide parallelization and to adopt
an extensible component approach to code development. Cactus is an open
source problem solving environment designed for scientists and engineers needing
to develop collaborative code for large scale parallel machines. Cactus comprises
sets of components (or thorns) which are invoked by the Cactus Flesh which collects information from the thorns to deﬁne grid variables, parameters, methods
and scheduling. The modular structure of Cactus enables parallel computation

A Parallel High-Order DG Shallow Water Model

67

Fig. 1. Architectural diagram showing Cactus thorns and their connection to the external Nektar++ and Zoltan packages

across diﬀerent architectures and collaborative code development between different groups.
To integrate the serial SWE solver into Cactus several new thorns were developed (see Figure 1). First, an unstructured mesh driver (thorn UMDriver) was
developed which provides the underlying parallel layer and the Cactus Conﬁguration Language was extended to support grid functions on unstructured meshes.
The UMDriver (which is still under development) uses the Zoltan [3] library to
provide mesh partitioning, load balancing and mesh migration. Another thorn
LocalToGlobal provides local reindexing of elements, edges and vertices.
The core thorn for the coastal modeling toolkit in Cactus is CoastalWave.
Mirroring the methodology for other domain speciﬁc toolkits in Cactus, this
thorn deﬁnes the generic variables, parameters, and methods for coastal models, allowing models providing the same functionality to be swapped (e.g. the
SWE will be interoperable with the Boussinesq solver) and allowing additional
components to be added into the workﬂow (e.g. phase-averaging wave models to
compute radiation stresses).
Thorn Nektar++ initializes and populates the data structures of the Nektar++
library. Thorn SWE thorn contains the actual SWE solver based on routines
deﬁned in the SWE class. Finally, thorn MeshReader provides a simple ASCII
mesh ﬁle reader, and also allows users to register their own mesh readers.

4
4.1

Computational Results
Convergence

Consider the simple case of a linear standing wave with a wavelength of 10 m in
a square 10 by 10 m basin. The still water depth is 0.5 m. In order to compare
with the analytical solution we here use the linearized SWE. We compute the
solution for one wave period using 10 000 time steps. Table 1 shows the error
and order of convergence measured in the L2 norm.

68

C. Eskilsson et al.
Table 1. L2 error and order of convergence for the linear standing wave
N = 16
error
3.3676E-03
1.7754E-04
2.1303E-05
3.4041E-07

p
1
2
3
4

N = 64
error
order
6.4677E-04 2.38
2.1496E-05 3.05
2.2351E-06 3.25
9.7791E-09 5.12

N = 256
error
order
1.6054E-04 2.01
2.5909E-06 3.05
6.3190E-08 5.14
3.0563E-10 5.00

In general we have optimal convergence O(hp+1 ), although for p = 3 there is
an instance of O(hp ) convergence. This is due to the use of a simpliﬁed numerical
ﬂux (componentwise averaging) in the linear scheme. The use of averaging is
known to sometimes produce sub-optimal convergence for odd p. Nevertheless,
for p-type reﬁnement we obtain the expected exponential convergence, illustrated
by the approximate straight lines in Figure 2.

-2

10

N = 16
N = 64
N = 256

-4

L2 error

10

-6

10

-8

10

-10

10

10-12

0

1

2

3

4

5

6

7

8

9

1

Polynomial order p

Fig. 2. Illustration of exponential convergence for the linear standing wave

4.2

Weak Scaling

Weak scaling indicates the ability of a code to scale up a problem on more
cores, increasing the domain size or grid reﬁnement while keeping a constant
load on each core. We consider two series of meshes – consisting of 100 and
900 quadrilaterals per core, respectively. Both sets are run with three diﬀerent
polynomial orders: p = 4 , 6 , 8. The largest run (900 elements per core on 128
cores) thus contains 218 700 unique degrees of freedom per core, or a total of
roughly 28 million degrees of freedom. We execute the model for one hundred
time steps. Scaling tests were performed on the LONI “Queen Bee” Linux cluster
(comprising 668 nodes, each containing dual Quad Core Xeon 64-bit 2.33GHz
processors with an Inﬁniband interconnect).

A Parallel High-Order DG Shallow Water Model

69

Table 2. Weak scaling using 100 quadrilaterals [Top table] and 900 quadrilaterals
[Bottom table] per core. Solution time is wall clock time in seconds while parallel
eﬃciency is presented as an percentage.
Numbers
of
cores
1
2
4
8
16
32
64
128

p=4
Solution Parallel
time
eﬀ.
5.00
—
5.56
89.8
6.16
81.2
7.17
69.7
7.53
66.3
7.75
64.5
8.44
59.2
9.06
55.2

p=6
Solution Parallel
time
eﬀ.
7.01
—
7.77
90.3
8.54
82.1
9.98
70.3
10.35
67.8
10.63
66.0
11.45
61.3
12.13
57.8

p=8
Solution Parallel
time
eﬀ.
10.6
—
11.7
90.6
12.7
83.4
14.7
72.1
15.3
69.0
16.0
66.1
16.5
64.0
17.2
61.4

Numbers
of
cores
1
2
4
8
16
32
64
128

p=4
Solution Parallel
time
eﬀ.
134.3
—
140.6
95.6
150.5
89.3
159.8
84.1
167.4
80.3
165.0
81.4
167.1
80.4
170.6
78.7

p=6
Solution Parallel
time
eﬀ.
153.1
—
160.1
95.6
169.5
90.3
181.9
84.1
185.9
82.3
190.0
80.6
194.5
78.7
191.4
80.0

p=8
Solution Parallel
time
eﬀ.
185.7
—
193.7
95.8
203.5
91.3
218.6
85.0
222.5
83.5
226.0
82.2
227.8
81.5
237.6
78.2

Table 2 shows the solution time (without time spent in I/O, initializing and
partitioning of the mesh) for the 100 and 900 element series. The parallel eﬃciency
decreases for both series, although more rapidly for the 100 element series. This is
due to the fact that ghost elements are currently used rather than ghost edges, so
that the workload per core is not actually constant: a two-core partition has 100
plus an additional 10 ghost elements per core; four partitions has 100 elements
plus 20 ghost elements per core, etc. This increase in computational overhead is
especially pronounced for scaling tests with a low workload per core as well as for
tests using less than eight cores. The parallel eﬃciency can be regarded as largely
independent with regard to polynomial order p – the slight increase in eﬃciency
for higher p can be attributed to the higher workload per core.
Scaling for the total execution time shows a larger decrease in eﬃciency, indicating that the initialization and mesh partitioning routines also require further
attention to improve scalability. A core contributor to this eﬀect is due to the fact
that currently each core stores information about the indices on the entire mesh.
4.3

Strong Scaling

Strong scaling indicates the ability to decrease the total run time for a particular
problem, scaling across more cores while keeping the overall problem size ﬁxed.

70

C. Eskilsson et al.
3

3

10

p =8
p =6
p =4
map 4
Map 5
Map 6

2

10

1

10

1
1

Wall clock time [sec]

Wall clock time [sec]

10

p =8
p =6
p =4
map 4
Map 5
Map 6

2

10

1

10

1
1

50

Number of cores

100

150 20

50

100

150 20

Number of cores

Fig. 3. Strong scaling. [Left] Solution time and [Right] total execution time.

Fig. 4. Cylinder test case: [Top] computational mesh including iso-contours of the
surface elevation at t = 20 seconds; [Bottom left] snapshot of surface elevation at
t = 9.9 seconds and [Bottom right] snapshot of surface elevation at t = 20 seconds

We assess the strong scaling using a ﬁxed size problem of 12 800 quadrilaterals.
As for the weak scaling tests we use p = 4 , 6 , 8 and run the problem for 100 time
steps. Figure 3 shows the reduction in solution time as the number of processors
are increased. The curves in Figure 3 asymptotically approaches the 1:1 slope,
indicating that the wall clock time is halved as the number of cores are doubled.

A Parallel High-Order DG Shallow Water Model

4.4

71

Wave Interacting with Cylinders

Consider a numerical wave ﬂume that is 125 m long and 40 m wide, with an array of
ﬁve cylinders (all having a 4 m diameter). The computational domain is discretized
into roughly 6 000 triangular elements of polynomial order p = 6 (roughly 500 000
degrees of freedom), see the top image in Figure 4, heavily clustered around the
cylinders. The initial condition is given by Laitone’s ﬁrst order solitary wave solution using an amplitude of 0.05 m centered at x = 0. We run the model for 25
seconds, using 10 000 steps. The non-linear SWE model is executed using 64 cores
and the simulation generally takes 0.1 second per time step.
The lower images in Figure 4 shows the evolution of the solitary wave, including wave run-up, scattering, diﬀraction, reﬂection as well as interaction between the scattered waves. Note that the solution correctly remains symmetric
throughout the simulation.

5

Concluding Remarks

We have outlined a spectral/hp DG method for solving the SWE and described
it’s integration with the Cactus Framework. This work concludes the ﬁrst phase
of a program to develop a scalable, parallel, non-hydrostatic wave solver, based
on multi-layered Boussinesq-type equations capable of including time-dependent
bathymetry and sediment transport. An important step has been taken in designing the integration of the underlying spectral/hp solver apparatus into Cactus to
form the basis of a community framework for coastal modeling. This necessitated
the development of an unstructured mesh driver for Cactus, which through the
modular framework can now be used by other application domains.
Much work remains to be done. Comparing to the weak scaling results of e.g.
[2] it is clear that the scaling of the SWE is not yet optimal. The main reasons for
the suboptimal behaviour are: (i) additional communication overhead due to the
use of complete ghost elements rather than just using ghost edges; (ii) current
ineﬃciencies in the Cactus UMDriver thorn, e.g. in duplicating the physical
space variables.
That the scaling was found independent of polynomial approximation is encouraging, as we can expect scalability also for higher-order schemes. Planned
future work on the unstructured mesh driver includes improving scaling on large
numbers of cores, adaptive mesh reﬁnement, a hyperslabbing interface and dynamic load balancing. On the coastal modeling side, subroutines for e.g. ﬂooding/drying and breaking waves will be added to allow for real-life engineering
applications.

Acknowledgements
The authors gratefully acknowledge the contributions of Prof. S.J. Sherwin,
Mr. P. Vos (Imperial College London) and Prof. R.M. Kirby (University of
Utah) to the DG SWE solver. Financial support was provided by DOD/ONR

72

C. Eskilsson et al.

(N00014-07-1-0955), NSF/EPSCOR (EPS-0701491) and the Floating Point Systems Endowed Chair at LSU. This research was supported in part by the Perimeter Institute for Theoretical Physics. Computer resources were provided by the
Louisiana Optical Network Initiative (LONI).

References
1. Bernard, P.-E., Chevaugeon, N., Legat, V., Deleersnijder, E., Remacle, J.-F.:
High-order h-adaptive discontinuous Galerkin methods for ocean modelling. Ocean
Dyn. 57, 109–121 (2007)
2. Biswas, R., Devine, K.D., Flaherty, J.: Parallel, adaptive ﬁnite element methods
for conservation laws. Appl. Numer. Math. 14, 255–283 (1994)
3. Devine, K., Boman, E., Heaphy, R., Hendrickson, B., Vaughan, C.: Zoltan Data
Management Services for Parallel Dynamic Applications. Comp. Sci. Eng. (2002)
4. Eskilsson, C., Sherwin, S.J.: A triangular spectral/hp discontinuous Galerkin
method for modelling 2D shallow water equations. Int. J. Num. Meth. Fluids 45,
605–623 (2004)
5. Giraldo, F.X., Hesthaven, J.S., Warburton, T.: Nodal high-order discontinuous
Galerkin methods for the spherical shallow water equations. J. Comp. Phys. 181,
499–525 (2002)
6. Giraldo, F.X., Warburton, T.: A high-order triangular discontinuous Galerkin
oceanic shallow water model. Int. J. Num. Meth. Fluids 56, 899–925 (2008)
7. Goodale, T., Allen, G., Lanfermann, G., Mass´
o, J., Radke, T., Seidel, E., Shalf, J.:
The Cactus framework and toolkit: Design and applications. Vector and Parallel
Processing. Springer, Heidelberg (2003)
8. http://www.cactuscode.org
9. http://www.nektar.info
10. Karniadakis, G.E., Sherwin, S.J.: Spectral/hp Element Methods for Computational
Fluid Dynamics. Oxford Sci. Publ. (2005)
11. Kubatko, E.J., Westerink, J.J., Dawson, C.: hp discontinuous Galerkin methods for
advection dominated problems in shallow water ﬂow. Comp. Meth. Appl. Mech.
Eng. 196, 437–451 (2006)
12. Kubatko, E.J., Bunya, S., Dawson, C., Westerink, J.J., Mirabito, C.: A performance
comparison of continuous and discontinuous ﬁnite element shallow water models.
J. Sci. Comp. (in press)
13. Kubatko, E.J., Bunya, S., Dawson, C., Westerink, J.J.: Dynamic p-adaptive RungeKutta discontinuous Galerkin methods for the shallow water equations. Comput.
Meth. Appl. Mech. Engrg. (in press)
14. Toro, E.T.: Shock-Capturing Methods for Free-Surface Shallow Flows. John Wiley,
Chichester (2001)

