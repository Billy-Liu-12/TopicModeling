An Incremental Learning Algorithm Based on
Rough Set Theory
Yinghong Ma

and Yehong Han

School of Management, Shandong Normal University, Jinan, 250014, P.R. China
yinghongma71@163.com

Abstract. A decision table is a pair S = (U, A), where U and A are
nonempty ﬁnite sets called the universe and primitive attributes respectively. In order to computing the minimal rule sets on the decision table
in which a new instance is added, a classiﬁcation of the new instances
and a criteria for the minimum recalculation are given, an incremental
learning algorithm is presented and this algorithm is proved that can be
used to the consistent and the inconsistent decision tables. The complexity of this algorithm is also obtained.
Keywords: rough sets, incremental learning, algorithm.
AMS Subject Classiﬁcations: TP18.

1

Introduction and Notations

Rough set theory was introduced by Z. Pawlak in 1982, it has been used in a
variety of domains [1] [2] [3], such as machine learning, pattern recognition, decision support system and expert systems and so on. The incremental learning is
a branch of machine learning. One of the main researches on rough sets theory is
how to compute the attribute’s reduction on a decision table, but those research
is limited by the static data, but in real life, it is not such case, a new instance
is allowed to put in S. Computing a minimal rule set on a decision table when
the new instance is added, all the data in the decision table will be recalculated
by the classical method [4]. In [5], there are three cases when a new instance x is
added to S, x conﬁrms the actual knowledge; x contradicts to the actual knowledge; x is completely new. But this category is not a partition of S ∪ {x}. In [6] ,
a new category method based on decision logical was introduced, unfortunately,
it isn’t including all the cases.
Knowledge representation system is a pair S =< U, A >, where U and A are
all nonempty ﬁnite sets called the universe and primitive attributes respectively.
Every primitive attribute a ∈ A is a total function a : U → Va , where Va is the
set of values of a, called the domination of a. Let C, D ⊂ A be the condition and
This work is carried out under the Taishan Scholar project of Shandong China and
the Doctor Foundation of Shandong(2006BS01015).
Corresponding author.
Y. Shi et al. (Eds.): ICCS 2007, Part III, LNCS 4489, pp. 444–447, 2007.
c Springer-Verlag Berlin Heidelberg 2007

An Incremental Learning Algorithm Based on Rough Set Theory

445

decision attributes of A respectively. A KR-system with distinguished condition
and decision attributes will be viewed as a decision table, denoted by S = (U, A),
the decision table often is often called a decision logic. Using the decision logic
language(DL-language), the following notations are deﬁned.
The set of formulas in decision logic language is a set satisfying the following
conditions: (1) The expression of the forms (a, v) (in short av called elementary
formulas) are formulas of the DL-language for any a ∈ A and v ∈ Va ; (2) If θ, ψ
are formulas of the DL-language, then ¬θ, θ ∨ ψ, θ ∧ ψ, θ → ψ and θ ≡ ψ are
formulas too. If x ∈ U satisﬁes formula θ in S = (U, A), denoted by x |=S θ. If
θ is a formula, then the set |θ|S , deﬁned as |θ|S = {x ∈ U : x |=S θ}, is called
the meaning of the formula θ in S. Let P = {a1 , a2 , · · · , an } and P ⊆ A. Then
(a1 , v1 ) ∧ (a2 , v2 ) ∧ · · · ∧ (an , vn ) is called a P -basic formula. If θ is a P -basic
formula and R ⊆ P , we denote the R-basic formula obtained from the formula
θ by removing all elementary formulas (a, va ) such that a ∈ P − R by θ/R. The
other notation and deﬁnition not be given here can be ﬁnd in [5].

2

An Algorithm for Incremental Learning

The Category of New Instances. Denoted the new instance by x and the new
decision table by S , let M , R be a minimal decision rule-set and a condition
attribute reduction set of S respectively. The new deﬁnitions and notations are
given.
x is called matching M , if and only if all the rules θ → ψ such that θx → θ in
M imply ψx ≡ ψ, denote the set of thus x by Im . The set Mx = {i|i ∈ U, θx →
θi , ψx ≡ ψi } is called the matching domain of x, where θi → ψi is the RD-rule
reduction of object i. x is called totally contradicts to M if and only if all the
rules θ → ψ such that θx → θ in M imply ψx = ψ. x is partially contradicts
to M if there exist a rule in M satisfying θx → θ such that ψx ≡ ψ and there
exist an other rule in M satisfying θx → θ such that ψx = ψ. Denote the set of
those x by Ic . The set Cx = {i|i ∈ U, θx → θi , ψx = ψi } is called the contradict
domination of x. If there is no rule θ → ψ such that θx → θ in M , then x is
called completely new in M . Denote the set of thus x by In .
By the above deﬁnitions, the following property holds.
Proposition 1. Let I denote all the new instances for a decision table. Then
{Im , Ic , In } is a partition of I.
Proposition 2. Let x be a new instance. If there is no an object y ∈ Cx such
that y |=S θx /R in S. Then the reduction of condition attributes of S is R.
Proof: There is no an object y ∈ Cx such that y |=S θx /R in S, therefore,
there is no any RD-rule θ → ψ such that θx /R ≡ θ and ψx = ψ in S.
Set U = U {x} and S = (U , A). Then P OSR (D) = P OSR (D) {x} and
P OSC (D) = P OSC (D) {x}, where P OSR (D) and P OSC (D) are R-Positive
region and C-Positive region of D on U respectively. Hence P OSC (D) =
P OSR (D) since P OSC (D) = P OSR (D). Therefor the reduction of condition
attributes of S is R because R is the D-independent subfamily of C in U .

446

Y. Ma and Y. Han

Proposition 3. If there is an object y ∈ Mx , y |=S θx in decision table S, then
the decision table S = S, and its reduction of attributes and minimal set of rules
will not change.
The proof of the above proposition is obvious.
Proposition 4. If there is an object y ∈ Cx such that y |=S θx and there is
no other object i ∈ Cx (i = y) such that i |=S θx /R. Then There is a condition
attributes reduction R of S such that R ⊆ R.
Proof: If there is unique y ∈ Cx such that y |=S θx in S, then P OSC (D) =
P OSC (D) − {y|y |=S θx }. There is no any other i ∈ Cx such that θx /R, then
P OSR (D) = P OSR (D) − {y|y |=S θx }. Hence, P OSC (D) = P OSR (D) . Because the size of positive region is reduced when the new instance is add to S,
R may be not the D-independent subfamily of C in U . There is a reduction R
of condition attributes of S such that R ⊆ R.
Proposition 5. If there are two objects y ∈ Cx satisfying y |=S θx at least, then
R is still a condition attributes reduction of S .
Proof: If there are two objects y1 , y2 ∈ Cx satisfying yi |=S θx (i = 1, 2), then
P OSR (D) = P OSR (D) and P OSC (D) = P OSC (D). Hence P OSC (D) =
P OSR (D) . The positive region doesn’t change in S , R is the D-independent
subfamily of C in U . So a condition attribute reduction S is still R.
Proposition 6. Let θi → ψi be a RD-rule reduction of object i in S, and
i∈Cx {j|θj ≡ θh , h ∈ Cx , j ∈ Mx } (where θh and θj are C-basic formulas
of h and j respectively). Let W be the set of all minimal rule sets of S . If an
attribute reduction R in S is also an attribute reduction in S . Then there is a
minimal rule-set M ∈ W such that θi → ψi ∈ M .
Proof: because i∈Cx {j|θj ≡ θh , h ∈ Cx , j ∈ Mx } (where θh and θj are Cbasic formulas of h and j respectively). Clearly, there is no θx → θi nor ψx = ψi .
Hence there is a minimal rule-set M ∈ W such that θi → ψi ∈ M .
According to the above propositions, the following corollaries can be got.
Corollary 1. If a new instance x matches to M , then M is also a minimal
decision rule-set of S .
Corollary 2. If a new instance x is completely new in M , then R is a condition
attributes reduction of S and a minimal set of rules of S is M {θx → ψx |θx →
ψx is a reduction of θx /R → ψx }.

3

Algorithm for Incremental Learning

The algorithm for incremental learning
Input: S = (U, A), R, M , x and its CD-basic rule θx → ψx .
Output: M ( the minimal rule set of S ).

An Incremental Learning Algorithm Based on Rough Set Theory

447

Algorithm: Set M1 = M2 = M3 = ∅;
∀θ → ψ in M
If θx → θ is false, then M1 = M1 {θ → ψ}
Else if ψx ≡ ψ, then M2 = M2 {θ → ψ}
Else M3 = M3 {θ → ψ}.
If M2 = M3 = ∅, then M = M {θx → ψx }.
Else if M3 = ∅,then M = M .
Else compute Cx = {i|i is an object which rule is in M3 }.
If ∃y ∈ Mx such that y |=S θx , then M = M ;
Else if ∃y ∈ Cx such that y |=S θx /R,then R = {R}
else, ∃yi ∈ Cx (i ≥ 2) such that yi |=S θx , then R = {R};
if there is unique y ∈ Cx such that y |=S θx , then compute R ;
Else compute R .
If R ∈ R ,then compute CCx = {θj → ψj | for all i ∈ Cx , j ∈ Mx , θi ≡ θj }
M1 M2 − CCx .
and M . Set M = M
Else simply the rules in S , then minimal rule set is M .
End for
OUTPUT M .
Let m and n be the size of C and U respectively. If x matches to M or is
completely new in M , the time-complexity of the algorithm is O(mn). If x is
contradicts to M and there no any object in Cx such that y |=S θx /R or there
are two objects yi ∈ Cx such that yi |=S θx (i = 1, 2), the time-complexity of
the algorithm is O(mn2 ). Those two cases are the best cases of the algorithm.
But in other situations, the eﬃciency of this algorithm is O(2m n2 ).

Acknowledgements
The authors are indebted to the anonymous referees for their detailed corrections
and suggestions.

References
1. Mollestad T, A rough set approach to data mining, extracting a logic default rules
from data, Norwegian University of Science and Technology, 1997.
2. Tong Linyun, An Liping, Incremental learning of decision rules based on rough set
theory, Proceedings of the 4th World Congress on Intelligent Control and Automation, Shanghai, Press of East China University of Science and Technology, 2002:
420-425.
3. Liping An, Jianyong Zhang, Lingyun Tong, Knowledge acquisition, incremental
modiﬁcation and reasoning based on rough set theory in expert systems, Computer
engineering and design, 25(1)(2004): 42-45.
4. Z. Pawlak, Rough sets and intelliegent data analysis, Information Sciences,
147(2002): 1-12.
5. Z. Pawlak, ROUGH SETS: Theoretical Aspects of Reasoning about Data, Kluwer
Academic Publishers, 1991.
6. Ning Shan, Wojciech Ziarko, Data-based acquisition and incremental modiﬁcation
of classiﬁcation rules, Computational Intelligence, 11(2)(1995): 357-370.

