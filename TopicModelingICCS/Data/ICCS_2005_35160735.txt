A Superconvergent Monte Carlo Method for
Multiple Integrals on the Grid
Soﬁya Ivanovska, Emanouil Atanassov, and Aneta Karaivanova
Institute for Parallel Processing - Bulgarian Academy of Sciences,
Acad. G. Bonchev St., Bl.25A, 1113 Soﬁa, Bulgaria
{sofia, emanouil, anet}@parallel.bas.bg

Abstract. In this paper we present error and performance analysis of
a Monte Carlo variance reduction method for solving multidimensional
integrals and integral equations. This method combines the idea of separation of the domain into small subdomains with the approach of importance sampling. The importance separation method is originally described in our previous works [7, 9]. Here we present a new variant of
this method adding polynomial interpolation in subdomains. We also discuss the performance of the algorithms in comparison with crude Monte
Carlo. We propose eﬃcient parallel implementation of the importance
separation method for a grid environment and we demonstrate numerical experiments on a heterogeneous grid. Two versions of the algorithm
are compared - a Monte Carlo version using pseudorandom numbers and
a quasi-Monte Carlo version using the Sobol and Halton low-discrepancy
sequences [13, 8].

1

Introduction

Consider the problem of approximate calculation of the multiple integral
f (x)p(x) dx,

I[f ] =

p(x) ≥ 0,

G

p(x) dx = 1,
G

where p (x) is a probability density function. The crude Monte Carlo quadrature
is based on the probability interpretation of the integral:
IN [f ] =

1
N

N

f (xn ),

(1)

n=1
1

where {xn } is distributed according to p(x). The error is proportional to σ[f ]N − 2 :
N [f ]

where

⎛
σ[f ] = ⎝

∼ σ[f ]N −1/2 ,
⎞1/2

(f (x)p(x) − I[f ])2 dx⎠

G
V.S. Sunderam et al. (Eds.): ICCS 2005, LNCS 3516, pp. 735–742, 2005.
c Springer-Verlag Berlin Heidelberg 2005

.

736

S. Ivanovska, E. Atanassov, and A. Karaivanova

There are various ways to improve the convergence rate of Monte Carlo integration. The following theorem, due to Bachvalov, establishes a lower bound for
both Monte Carlo and deterministic integration formulae for smooth functions:
Theorem 1. (Bachvalov [4, 5]) There exist constants c(s, k), c (s, k), such that
for every quadrature formula IN [f ] which is fully deterministic and uses the
function values at N points there exists a function f ∈ Csk such that
k

Es

f (x) dx − IN [f ] ≥ c(s, k) f N − s

and for every quadrature formula IN [f ], which involves random variables and
uses the function values at N points, there exists a function f ∈ Csk , such that
2

E
Es

1/2
1

k

≥ c (s, k) f N − 2 − s .

f (x) dx − IN [f ]

From this theorem it follows that Monte Carlo methods have advantage over
deterministic methods, especially in high dimensions. In order to obtain the
optimal convergence rate for functions with bounded k-th order derivatives, one
widely used technique is stratification. Split the integration region G into M
subdomains:
M

M

Dj ,

G=

Di ∩ Dj = ∅,

i = j,

Nj = N,

j=1

j=1

p(j) (x) = p(x)/pj ,

pj =

p(x) dx.
Dj
(j)

In the subdomain Dj use a Monte Carlo integration formula INj [f ], which
(j)

(j)

utilizes Nj random points ξ1 , . . . , ξNj . The stratiﬁed Monte Carlo formula is
M

IN [f ] =
j=1

pj (j)
I [f ] .
Nj Nj

The error of the stratiﬁed method is given by
⎛
N

∼ N −1/2 σs ,

M

σs2 =

2

σ (j) ,
j=1

⎜
σ (j) = ⎝

⎞ 12
f (x) pj (x) − INj [f ]

2

⎟
dx⎠ .

Dj
k

In our method we achieve σ (j) = O(N − s ), if the function f is in Csk , and
therefore we attain the optimal convergence rate (see Sec. 2).
Another possible idea for improving the convergence of Monte Carlo methods is to replace the pseudorandom numbers with terms of a low-discrepancy

A Superconvergent Monte Carlo Method

737

sequence [6, 12]. The most widely used measure for estimating the quality of the
s
distribution of a low-discrepancy sequence τ = {xi }N
i=1 in E is its discrepancy
AN (τ, I)
− µ (E) .
N

∗
DN
= sup

I⊂Es

Low-discrepancy sequences have order of convergence of the discrepancy
O N −1 logs N , which becomes order of convergence of the corresponding quasiMonte Carlo method for integrating a function over Es :
QN [f ] =

1
N

N

f (xi ) .

(2)

j=1

Therefore quasi-Monte Carlo methods oﬀer higher conversion rate (at the expense of more stringent smoothness requirements). Formula 2 can be considered
as a quasi-Monte Carlo version of formula 1. Designing a quasi-Monte Carlo
version of a Monte Carlo method is not always straight-forward. We studied
the possibility of using the Halton and Sobol low-discrepancy sequences in our
algorithm in Sec. 2.
Monte Carlo methods usually show good parallel eﬃciency. In Sec. 3 we
describe our parallelization approach, which is suitable for grid environments.
We obtained good parallel eﬃciency in both Monte Carlo and quasi-Monte Carlo
versions.

2

Description of Our Algorithm

We introduce adaptive division of the domain into smaller subdomains, so that
smaller subdomains are used where the function values are larger. In every subdomain we approximate the function by a polynomial, using function values at
some ﬁxed points. The input parameters of our algorithm are the dimension s,
the smoothness order k, the number of base points M , the number of steps N ,
the number of random points in each cube m, the number of points used for one
application of crude Monte Carlo method R. First we select M points a1 , . . . , aM
in the unit cube Es , following the procedure, discussed in [2], and compute the respective coeﬃcients c1 , . . . , cM . For every coordinate xj , we estimate the values of
|f (x1 , . . . , xj−1 , xj , xj+1 , . . . , xs )| dx1 , . . . , dxj−1 dxj+1 , . . . , dxs

g(xj ) =
Es−1

at points xj = Nr , using crude Monte Carlo algorithm with R points. Then we
approximate the one-dimensional function g by a piece-wise linear function g˜,
using these values. For every coordinate we choose N + 1 points ξri , so that
i
= 1, and
ξ0i = 0, ξN
i
ξr+1

ξri

g˜(t) dt =
i
ξr−1

g˜(t) dt,
ξri

r = 1, . . . , N − 1.

738

S. Ivanovska, E. Atanassov, and A. Karaivanova

These steps can be considered as pre-processing, and can be done in parallel,
if we use more than one CPU. Using the points ξri , we partition the cube Es into
N s subdomains Dj . Approximate Dj f (x) dx by the formula:
1
µ (Dj )
m

m

M

(f (ηi ) − Lj (f, ηi )) + µ (Dj )
i=1

ci f (Tj (ai )),
i=1

where Tj is the canonical linear transformation that maps Es onto Dj , the points
ηi ∈ Dj are uniformly distributed random points, and Lj (f, ηi ) is the polynomial
approximation to f , obtained using the values f (Tj (ai )) (see [2]). Summing these
unbiased estimates, we obtain an estimate of the integral over Es . The variance
of this estimate is a sum of the variances σj2 in every subdomain Dj . The order
of convergence is O N −k−s/2 , when using O(N s ) points and so it is optimal
in the sense of Theorem 1. An aposteriory estimate of the error is obtained by
using the empirical variances σ˜j .
The number of function evaluations is N s (m + M ) + Rs(N − 1). In most
practical situations the time for performing them dominates in the whole computation.
We studied also quasi-Monte Carlo variants of the algorithm. In Monte Carlo
methods one uses pseudorandom number generators in order to sample uniformly distributed random variables. The idea of quasi-Monte Carlo methods is
to replace the pseudorandom numbers with terms of a low-discrepancy sequence.
When integrating a function in the s-dimensional unit cube, the dimension of
the low-discrepancy sequence is frequently higher than s, if a more complex algorithm is used. We note that when used in parallel computations, low-discrepancy
sequences can oﬀer exact reproducibility of results, unlike Monte Carlo. In our
algorithm the coordinates of the r-th term of the sequence can be used to produce the coordinates of the points ηi . Since we have s coordinates and m points,
the constructive dimension of the algorithm becomes sm. We tested two of the
most popular families of low-discrepancy sequences - the sequences of Sobol and
Halton. Since the rate of convergence of the quasi-Monte Carlo algorithm has a
factor of logsm N , we did not observe improvement with respect of the Monte
Carlo version (see Sec. 4).

3

Parallel Implementation of the Algorithm Suitable for
Computational Grids

Monte Carlo methods are inherently parallelizable, and both coarse-grain and
ﬁne-grain parallelism can be exploited. We take into account the fact that the
Grid is a potentially heterogeneous computing environment, where the user
does not know the speciﬁcs of the target architecture. Therefore parallel algorithms should be able to adapt to this heterogeneity, providing automated
load-balancing. Monte Carlo algorithms can be tailored to such environments,
provided parallel pseudo-random number generators are available. The use of

A Superconvergent Monte Carlo Method

739

quasi-Monte Carlo algorithms poses more diﬃculties. In both cases the eﬃcient
implementation of the algorithms depends on the functionality of the corresponding packages for generating pseudorandom or quasirandom numbers.
As a package of parallel pseudo-random generators we used the SPRNG
([10]). For generating the scrambled Halton and Sobol sequences, we used our
ultra-fast generators ([1, 3]), which provide the necessary functionality:
– portable
– use assembly language for best performance on various Intel Pentium and
AMD processors
– provide a fast-forward operation, important for our parallelization approach.
Our parallelization is based on the master-slave paradigm, with some ramiﬁcations. We divide the work into chunks, corresponding to the subdomains,
which are requested from the master process by the slave processes. In order
to increase the eﬃciency, the master also performs computations, while waiting for communication requests. Thus we achieve overlap of computations and
communications, and we do not lose the possible output of the master process.
When using low-discrepancy sequences, we take care in both master and slave
processes to fast-forward the generator exactly to the point that is needed. The
scrambling that is provided by the generators enables aposteriory estimation of
the error in the quasi-Monte Carlo case.

4

Numerical Experiments

Our numerical tests are based on the next two examples, which are taken from
paper of Moskowitz and Caﬂisch [11].
Example 1. The ﬁrst example is Monte Carlo integration over E5 = [0, 1]5 of
the function
5

ai x2i

f1 (x) = exp

2 + sin(

i=1

5
j=1,j=i

xj )

2

,

where a = (1, 12 , 15 , 15 , 15 ).
Example 2. The second example is Monte Carlo integration over E7 = [0, 1]7
of the function
2 π
2 π
2 π
f2 (x) = e1−(sin ( 2 x1 )+sin ( 2 x2 )+sin ( 2 x3 ))
× arcsin sin(1) +

x1 + · · · + x7
200

.

Table 1 shows the results for 5-dimensional integral. The smoothness that is
used is 4 or 6. And just for comparison we show results with crude Monte Carlo
method that uses the same number of functional values as our algorithm. In

740

S. Ivanovska, E. Atanassov, and A. Karaivanova

Table 1. Results for Example 1 with number of cubes N and number of points per
cube 10
N k SPRNG
2 4 5.15e-04
6 7.06e-05
3 4 8.01e-05
6 2.15e-07
4 4 1.21e-05
6 2.95e-07
5 4 1.78e-06
6 2.79e-08

Sobol
3.16e-04
1.43e-05
4.38e-05
7.41e-07
1.01e-05
3.45e-07
3.03e-06
3.61e-08

Halton
3.72e-04
2.34e-05
4.77e-05
3.19e-07
2.09e-05
1.35e-07
2.87e-06
4.09e-08

crude MC
1.52e-02
7.40e-03
5.51e-03
2.68e-03
2.68e-03
1.31e-03
1.54e-03
7.48e-04

Table 2. Results for Example 2 with number of cubes N and number of points per
cube 10
N k SPRNG
2 4 1.25e-05
6 9.49e-07
3 4 1.31e-06
6 7.57e-08
4 4 2.94e-08
6 5.99e-09
5 4 1.23e-07
6 8.43e-10

Sobol
3.12e-05
3.61e-07
5.30e-06
7.61e-08
1.82e-06
1.07e-08
3.21e-07
1.61e-09

Halton
3.29e-05
7.34e-07
4.67e-06
8.61e-08
4.73e-07
1.44e-08
5.43e-07
1.20e-08

both examples the errors obey the theoretical laws, i.e., the convergence rate is
1
k
1
O N − 2 for the crude Monte Carlo method, and O N − s − 2 for our adaptive
method, when smoothness k is used. Table 3 shows CPU time for our algorithm
when N is equal to 14 and the number of random points per cube m is equal
to 40. We used diﬀerent computers for these tests - three of the computers have
Pentium processors running at diﬀerent clock speeds and one has a Motorola
G4 processor. All of the computers had the Globus toolkit installed. In Table 4
we compare the estimated time for running our algorithm on all those computers in parallel, in case of perfect parallel eﬃciency, with the measured execution
time. We used the MPICH-G2 implementation of MPI, which is the most general
approach for running parallel jobs on computational grids. In this way we successfully utilized machines with diﬀerent endianness in the same computation.
We obtained roughly the same accuracy with low-discrepancy sequences as with
pseudo-random numbers, due to the high eﬀective dimension. The CPU time of
our implementations of the quasi-Monte Carlo versions was frequently smaller,
due to the eﬃcient generation algorithms. Quasi-Monte Carlo algorithms are
more diﬃcult to parallelize under such constraints, but this can be done if the
generators of the low-discrepancy sequences have the necessary functionality.

A Superconvergent Monte Carlo Method

741

Table 3. Time and eﬃciency for Example 1 with number of points 14 and number
of points per cube 40

SPRNG
Sobol
Halton

P4/2.8GHz P4/2GHz P4/2GHz G4/450MHz
102
117
118
413
91
106
96
393
91
106
106
393

Table 4. Parallel eﬃciency measurements

SPRNG
Sobol
Halton

5

Estimated Time Measured Time Eﬃciency
34
41
83%
30
34
88%
31
34
91%

Conclusions

An adaptive Monte Carlo method for solving multiple integrals of smooth functions has been proposed and tested. This method is an improved version of the
importance separation method. The importance separation method is combined
with polynomial interpolation in the subdomains. A quasi-Monte Carlo version
of the algorithm was also studied. The obtained results are not an eﬃcient parallel implementation of the algorithm has been achieved using a version of the
master-slave computing paradigm, enabling the execution of the programs in
heterogeneous Grid environments. The ideas of this implementation can be extended to other Monte Carlo and quasi-Monte methods.
Acknowledgments. This work is supported by the Ministry of Education and
Science of Bulgaria under Grant # I-1405/04.

References
1. E. Atanassov. Measuring the Performance of a Power PC Cluster, Computational
Science - ICCS 2002 ( P. Sloot, C. Kenneth Tan, J. Dongarra, A. Hoekstra - Eds.),
LNCS 2330, 628–634, Springer, 2002.
2. E. Atanassov, I. Dimov, M. Durchova. A New Quasi-Monte Carlo Algorithm
for Numerical Integration of Smooth Functions, Large-Scale Scientiﬁc Computing
(I. Lirkov, S. Margenov, J. Wasniewski, P. Yalamov - Eds.), LNCS 2907, 128–135,
Springer, 2004.
3. E. Atanassov, M. Durchova. Generating and Testing the Modiﬁed Halton Sequences, Numerical Methods and Applications (I. Dimov, I. Lirkov, S. Margenov,
Z. Zlatev - Eds.), LNCS 2542, 91–98, Springer, 2003.
4. N.S. Bachvalov. On the approximate computation of multiple integrals, Vestnik
Moscow State University, Ser. Mat., Mech., Vol. 4, 3–18, 1959.

742

S. Ivanovska, E. Atanassov, and A. Karaivanova

5. N.S. Bachvalov. Average Estimation of the Remainder Term of Quadrature Formulas, USSR Comput. Math. and Math. Phys., Vol. 1(1), 64–77, 1961.
6. R.E. Caﬂisch. Monte Carlo and quasi-Monte Carlo methods, Acta Numerica, Vol.
7, 1–49, 1998.
7. I. Dimov, A. Karaivanova, R. Georgieva and S. Ivanovska. Parallel Importance
Separation and Adaptive Monte Carlo Algorithms for Multiple Integrals, Numerical Methods and Applications (I. Dimov, I. Lirkov, S. Margenov, Z. Zlatev - Eds.),
LNCS 2542, 99–107, Springer, 2003.
8. J. H. Halton. On the eﬃciency of certain quasi-random sequences of points in
evaluating multi-dimensional integrals, Numer. math., 2, 84–90, 1960.
9. A. Karaivanova. Adaptive Monte Carlo methods for numerical integration, Mathematica Balkanica, Vol. 11, 391–406, 1997.
10. M. Mascagni. SPRNG: A Scalable Library for Pseudorandom Number Generation.
Recent Advances in Numerical Methods and Applications II (O. Iliev, M. Kaschiev,
Bl. Sendov, P.S. Vassilevski eds.), Proceeding of NMA 1998, World Scientiﬁc, Singapore, 284–295, 1999.
11. B. Moskowitz and R.E. Caﬂisch. Smoothness and dimension reduction in quasiMonte Carlo methods, J. Math. Comput. Modeling, 23, 37–54, 1996.
12. H. Niederreiter. Random number generation and quasi-Monte Carlo methods,
SIAM, Philadelphia, 1992.
13. I.M. Sobol. On the distribution of point in a cube and the approximate evaluation
of integrals, USSR Computational Mathematics and Mathematical Physics, 7, 86–
112, 1967.

