Modeling the Risk Process in the XploRe
Computing Environment
Krzysztof Burnecki and Rafal Weron
Hugo Steinhaus Center for Stochastic Methods, Wroclaw University of Technology,
Wyspia´
nskiego 27, 50-370 Wroclaw, Poland
{burnecki,rweron}@im.pwr.wroc.pl
http://www.im.pwr.wroc.pl/˜hugo

Abstract. A user friendly approach to modeling the risk process is presented. It utilizes the insurance library of the XploRe computing environment which is accompanied by on-line, hyperlinked and freely downloadable from the web manuals and e-books. The empirical analysis for
Danish ﬁre losses for the years 1980-90 is conducted and the best ﬁtting
of the risk process to the data is illustrated.

1

Introduction

The simulation of risk processes is vital for insurance companies. It boils down
to generating aggregated claims for the calculation of losses that may occur.
Since claims arrive at random times, the number of claims up to a given time
is typically assumed to be driven by a stochastic process. In its simplest form
the risk process utilizes the homogeneous Poisson process as the claim arrival
process, however, more general processes like the non-homogeneous Poisson, the
mixed Poisson, the Cox (or doubly stochastic Poisson) and the renewal process
are also considered.
The risk process {Rt } of an insurance company can be approximated by the
sum of the initial capital of the company and the so-called premium function
(representing income from sold insurance policies) minus the aggregated claim
process (expressing liabilities resulting from claims covered by the previously
sold insurance policies) [3,8,9,14]. The latter is typically modeled by a sum of
random length, governed by the claim arrival point process, where the summands
– representing the claim severities – form an independent sequence of positive
i.i.d. random variables.
Since parameter estimation and simulation schemes can be tedious a number
of computer packages have been written to automate the process. In this paper
we want to present a novel solution which takes the form of a library of procedures of the XploRe system combined with on-line, hyperlinked manuals and
e-books.
XploRe is a computing environment which oﬀers a combination of classical
and modern statistical procedures, in conjunction with sophisticated, interactive graphics. XploRe is also a high level object-oriented programming language. With all the standard features like recursion, local variables, dynamic data
M. Bubak et al. (Eds.): ICCS 2004, LNCS 3039, pp. 868–875, 2004.
c Springer-Verlag Berlin Heidelberg 2004

Modeling the Risk Process in the XploRe Computing Environment

869

structures, loops, and conditional execution it provides a platform for advanced
statistical and econometric analysis, research, as well as teaching [10,11].
The statistical methods of XploRe are provided by various procedures and
scripts called quantlets. Quantlets are combined into libraries called quantlibs.
Among other these include: ﬁnance, econometrics, wavelets, generalized (partial)
linear models, time series analysis and ﬁltering, neural networks, non- and semiparametric methods, and teachware. Recent additions to this family comprise
the stable distributions and insurance libraries [6]. Proprietary methods can be
incorporated into XploRe, enabling the user to easily extend the environment.
One of the most outstanding features of the XploRe environment is the unique combination of computing capabilities and on-line, hyperlinked manuals and
books. A variety of electronic statistical and econometric volumes are available
from the XploRe web page (www.xplore-stat.de) in the html and pdf ﬁle formats.
All books contain a large number of quantlets which illustrate the theoretical
content. One of the newest additions is the ”Statistical Tools for Finance and
Insurance” e-book [6], which oﬀers the reader over a hundred methods and procedures related to insurance. Some of these quantlets will be utilized in this
paper.
The paper is organized as follows. In section 2 we brieﬂy recall the methods
of simulating two possible choices for the claim arrival process, namely the homogeneous Poisson process (HPP) and the non-homogeneous Poisson process
(NHPP). In section 3 we discuss a number of claim severities distributions and
present the methods for judging the goodness-of-ﬁt. Finally, in section 4 we
conduct the empirical analysis for Danish ﬁre losses for the years 1980-90 and
illustrate the best ﬁtting risk process for two choices of the claim severities distribution.

2

Claim Arrival Process

In this section we focus on the eﬃcient simulation of the claim arrival point
process. Typically this process is simulated via the arrival times {Ti }, i.e. moments when the ith claim occurs, or the inter-arrival times (or waiting times)
Wi = Ti − Ti−1 , i.e. the time periods between successive claims.
2.1

Homogeneous Poisson Process

A continuous-time stochastic process {Nt : t ≥ 0} is a (homogeneous) Poisson
process with intensity (or rate) λ > 0 if (i) {Nt } is a point process, and (ii)
the times between events are i.i.d. exponential random variables with intensity
λ, i.e. with mean 1/λ. Therefore, successive arrival times T1 , T2 , . . . , Tn of the
Poisson process can be generated by a simple algorithm consisting of generating
independent exponential random variables with intensity λ and taking a cumulative sum of them [3,15]. In the insurance library of XploRe this procedure is
implemented in the simHPP.xpl quantlet.

870

K. Burnecki and R. Weron

Since the expected value of the homogeneous Poisson process ENt = λt,
it is natural to deﬁne the premium function as a linear function of the form
c(t) = (1 + θ)µλt. Here µ = EXk is the expected value of the claim size and
θ > 0 is the relative safety loading which ”guarantees” survival of the insurance
company. With such a choice of the risk function we obtain the classical form of
the risk process [3,8,9]:
Nt

Rt = u + (1 + θ)µλt −

Xi .

(1)

i=1

The nonnegative constant u represents the initial capital of the company and
{Xi } is the i.i.d. claim severities sequence.
2.2

Non-homogeneous Poisson Process

In real life situations the homogeneous Poisson process may be too simplistic. In
order to obtain a more reasonable description of reality we might want to include
the possibility of a variable (eg. increasing) size of the portfolio of insurance
contracts or seasonality in the number of claims. For modeling such phenomena
the non-homogeneous Poisson process (NHPP) is much better. To distinguish it
from the HPP we denote it by Mt . The NHPP can be thought of as a Poisson
process with a variable intensity deﬁned by the deterministic intensity (rate)
function λ(t). Note that the increments of a NHPP do not have to be stationary.
In the special case when λ(t) takes the constant value λ, the NHPP reduces to
the homogeneous Poisson process with intensity λ.
The simulation of a NHPP is slightly more complicated than in the homogeneous case. The ﬁrst approach, the so-called ”integration method”, is based on the observation that for a NHPP with rate function λ(t) the increment
Mt − Ms , 0 < s < t, is distributed as a Poisson random variable with intensity
t
λ = s λ(u)du [9]. Hence, the distribution function Fs of the waiting time Ws is
given by:
Fs (t) = P (Ws ≤ t) = 1 − P (Ws > t) = 1 − P (Ms+t − Ms = 0) =
= 1 − exp −

s+t
s

λ(u)du

t

= 1 − exp −

λ(s + v)dv

.

(2)

0

If the function λ(t) is such that we can ﬁnd an explicit formula for the inverse
Fs−1 then for each s we can generate a random quantity X with the distribution
Fs by using the inverse transform method. Otherwise, we have to apply numerical
schemes and the algorithm becomes relatively slow.
The second approach, known as the ”thinning” or ”rejection method”, is
based on the following observation [2,15]. Suppose that there exists a constant
λ such that λ(t) ≤ λ for all t. Let T1∗ , T2∗ , T3∗ , . . . be the successive arrival times
of a HPP with intensity λ. If we accept the ith arrival time with probability
λ(Ti∗ )/λ, independently of all other arrivals, then the sequence T1 , T2 , . . . of the
accepted arrival times (in ascending order) forms a sequence of the arrival times

Modeling the Risk Process in the XploRe Computing Environment

871

of a NHPP with rate function λ(t). The resulting algorithm [3] is implemented
in the insurance library of XploRe in the simNHPP.xpl quantlet.
Since the mean-value function, i.e. the expected value of the process Mt , is
given by:
t

EMt =

λ(s)ds ,

(3)

0

in the non-homogeneous case the premium function is typically deﬁned as
t
c(t) = (1 + θ)µ 0 λ(s)ds and the risk process takes the form:
t

Rt = u + (1 + θ)µ

Mt

λ(s)ds −

0

3

Xi .

(4)

i=1

Claim Severities Distribution

The derivation of claim size distributions from the loss data could be considered
to be a separate discipline in its own [7]. The objective is to ﬁnd a distribution
function F which ﬁts the observed data in a satisfactory manner. The approach
most frequently adopted in insurance is to ﬁnd a suitable analytic expression
which ﬁts the observed data well and which is easy to handle, see e.g. [5].
The claim distributions, especially describing property losses, are usually
heavy-tailed. Note, that in the actuarial literature such distributions are deﬁned
as having tails heavier than exponential. The lognormal, Pareto, Burr and Weibull distributions are typical candidates considered in applications [13]. Let us
brieﬂy recall them.
If the random variable X is normally distributed with mean µ and variance
σ 2 , then the distribution of Y = eX is lognormal. Its distribution function (d.f.)
is given by FLN (x) = Φ([ln x−µ]/σ), where Φ(x) is the standard normal d.f., and
in the insurance library of XploRe is implemented in the cdfln.xpl quantlet.
The lognormal law is very useful in modeling of claim severities. It has a thick
right tail and ﬁts many situations well.
One of the most frequently used analytic claim size distributions is the Pareto law which is deﬁned by FP (x) = 1 − (λ/[λ + x])α with α, λ > 0, see the
cdfPareto.xpl quantlet. The ﬁrst parameter controls the thickness of the tail:
the smaller the α, the heavier the tail. Empirical experience has shown that the
Pareto formula is often an appropriate model for the claim size distribution,
particularly when exceptionally large claims may occur [4,7]. However, there is
a need to ﬁnd heavy tailed distributions which oﬀer yet greater ﬂexibility. Such
ﬂexibility is oﬀered by the Burr distribution, which is just a generalization of
the Pareto law. Its distribution function is given by FB (x) = 1 − (λ/[λ + xτ ])α
with all three parameters (α, λ, and τ ) being positive real constants, see the
cdfBurr.xpl quantlet.
Another frequently used analytic claim size distribution is the Weibull disα
tribution which is deﬁned by FW (t) = 1 − e−λt with α, λ > 0. Observe, that
the Weibull distribution is a generalization of the exponential law. In XploRe it
is implemented in the cdfWeibull.xpl quantlet.

872

K. Burnecki and R. Weron

Once the distribution is selected, we must obtain parameter estimates. In
what follows we use the moment and maximum likelihood estimation approaches.
The next step is to test whether the ﬁt is adequate. This is usually done by
comparing the ﬁtted and empirical distribution functions. More precisely, by
checking whether values of the ﬁtted distribution function at sample points form
a uniform distribution. In the next section we apply the χ2 , the KolmogorovSmirnov (KS), the Cramer-von Mises (CM), and the Anderson-Darling (AD) test
statistics [1,5,16]. Generally, the smaller the value of the statistics, the better
the ﬁt. These test statistics are implemented in the quantlets chi2stat.xpl,
kstat.xpl, cmstat.xpl, and adstat.xpl, respectively [6].

4

Empirical Analysis

-1
Log(1-F(x))
-2

40
0

20

Losses (DKK million)

60

0

We conducted empirical studies for Danish ﬁre losses recorded by Copenhagen
Re. The data, see Fig. 1, concerns major Danish ﬁre losses in Danish Krone
(DKK), occurred between 1980 and 1990 and adjusted for inﬂation. Only losses in proﬁts connected with the ﬁres were taken into consideration. In order
to calibrate the risk process we had to ﬁt both the distribution function F of
the incurred losses {Xi } and the claim arrival process. First we studied the

1980

1985
Time

1990

0

4

8
12
Losses (DKK million)

16

20

Fig. 1. Left panel : Illustration of the major Danish ﬁre losses in millions of Danish
Krone (DKK), occurred between January 1, 1980 and December 31, 1990 and adjusted
for inﬂation. Right panel : Logarithm of the right tails of the empirical claim sizes
distribution function F (x) (thick solid line) together with lognormal (dotted line) and
Burr (thin solid line) ﬁts

loss sizes. We ﬁtted lognormal, Pareto, Burr, and Weibull distributions using
the estln.xpl, estPareto.xpl, estBurr.xpl, and estWeibull.xpl quantlets,
respectively. The results of the parameter estimation and test statistics are

Modeling the Risk Process in the XploRe Computing Environment

873

Table 1. Parameter estimates and test statistics for the ﬁre loss amounts
d.f.:

Lognormal

Pareto

Burr

Weibull

Gaussian

Para- µ = 12.704 α = 2.4189 α = 0.8935 α = 0.6963 µ = 4.7332e5
meters: σ = 1.4271 λ = 1.0261e6 λ = 1.1219e7 λ = 8.9740e-5 σ = 6.7224e5
τ = 1.2976
χ2
KS
CM
AD

56.109
0.0373
0.1687
1.0533

73.879
0.0397
0.2878
2.7712

48.493
0.0413
0.1438
0.8221

129.24
0.0783
1.5245
10.638

592.98
0.2433
6.3655
78.410

0.5
Autocorrelation

400

0

300
0

100

200

Mean-value function

500

600

1

700

presented in Table 1. For illustration purposes we also added parameter estimates for the Gaussian distribution. The lognormal distribution with parameters µ = 12.7036 and σ = 1.4271 and the Burr distribution with α = 0.8935,
λ = 1.1219 · 107 and τ = 1.2976 produced the best results, see Fig. 1. Hence,
we chose them for further analysis. Next, we ﬁtted the claim arrival process.

0

1

2

3

4

5
6
Time (years)

7

8

9

10

11

0

5

10

15
Time lag (qtr)

20

25

30

Fig. 2. Left panel : The aggregate number of losses of the ﬁre data (dotted line), the
HPP (thick solid line), and the NHPP (thin solid line). Right panel : Autocorrelation
function of the diﬀerenced quarterly ﬁre losses data revealing no seasonality or signiﬁcant dependencies. Horizontal dashed lines represent the 95% conﬁdence intervals for
white noise

We started the analysis with the homogeneous Poisson process Nt with intensity
λ1 . Studies of the quarterly numbers of losses and the interoccurence times of
the catastrophes led us to the conclusion that the homogeneous Poisson process

874

K. Burnecki and R. Weron

1000
800
600

Capital (DKK million)

200

400

300
200
0

0

100

Capital (DKK million)

400

1200

500

with the annual intensity λ1 = 57.72 gave the best ﬁt. However, as we can see
in Fig. 2, the ﬁt is not very good suggesting that the HPP is too simplistic and
causing us to consider the NHPP.
The data reveals no seasonality but a clear increasing trend can be observed in the number of quarterly losses, see Fig. 2. We tested diﬀerent exponential and polynomial functional forms, but a simple linear intensity function
λ2 (s) = c+ds yielded the best ﬁt. Applying a least squares procedure we arrived
at the following values of the parameters: c = 13.97 and d = 7.57. Both choices of the intensity function, λ1 and λ2 (s), are illustrated in Fig. 2, where the
accumulated number of ﬁre losses and mean-value functions for all 11 years of
data are depicted. The simulation results are presented in Fig. 3. We consider a

0

1

2

3

4

5
6
Time (years)

7

8

9

10

11

0

1

2

3

4

5
6
Time (years)

7

8

9

10

11

Fig. 3. Simulation results for a non-homogeneous Poisson process with lognormal (left
panel ) and Burr (right panel ) claim sizes. The thick solid line is the ”real” risk process, i.e. a trajectory constructed from the historical arrival times and values of the
losses. The thin solid line is a sample trajectory. The dotted lines are the sample
0.01, 0.05, 0.25, 0.50, 0.75, 0.95, 0.99-quantile lines. Clearly, the Burr distribution describes the extreme losses much better

hypothetical scenario where the insurance company insures losses resulting from
ﬁre damage. The company’s initial capital is assumed to be u = 100 million kr
and the relative safety loading used is θ = 0.5. We chose two models of the risk
process whose application is most justiﬁed by the statistical results described
above: a non-homogeneous Poisson process with lognormal claim sizes and a
non-homogeneous Poisson process with Burr claim sizes.
In both subplots of Fig. 3 the thick solid line is the ”real” risk process,
i.e. a trajectory constructed from the historical arrival times and values of the
losses. The thin solid line is a sample trajectory. The dotted lines are the sample
0.01, 0.05, 0.25, 0.50, 0.75, 0.95, 0.99-quantile lines based on 10000 trajectories of

Modeling the Risk Process in the XploRe Computing Environment

875

the risk process. Recall that the function x
ˆp (t) is called a sample p-quantile line if
ˆp (t) is the sample p-quantile, i.e. if it satisﬁes Fn (xp −) ≤ p ≤
for each t ∈ [t0 , T ], x
Fn (xp ), where Fn is the sample distribution function. Quantile lines are a very
helpful tool in the analysis of stochastic processes. For example, they can provide
a simple justiﬁcation of the stationarity (or the lack of it) of a process, see [12].
In Fig. 3 they visualize the evolution of the density of the risk process. Clearly,
if claim severities are Burr distributed then extreme events are more probable
to happen than in the lognormal case, for which the historical trajectory falls
even outside the 0.01-quantile line. This suggests that Burr distributed claim
sizes are more adequate for modeling the ”real” risk process.
Acknowledgements. The authors thankfully acknowledge the support of
the State Committee for Scientiﬁc Research (KBN) Grant No. PBZ-KBN
016/P03/99.

References
1. D’Agostino, R.B., Stephens, M.A.: Goodness-of-Fit Techniques. Marcel Dekker,
New York (1986)
2. Bratley, P., Fox, B.L., Schrage, L.E.: A Guide to Simulation. Springer-Verlag, New
York (1987)
3. Burnecki, K., H¨
ardle, W., Weron, R.: An Introduction to Simulation of Risk Processes. In: Teugels, J., Sundt, B. (eds.): Encyclopedia of Actuarial Science. Wiley,
Chichester (to appear)
4. Burnecki, K., Kukla, G.: Pricing of Zero-Coupon and Coupon CAT Bonds. Applicationes Mathematicae 30(3) (2003) 315–324
5. Burnecki, K., Kukla, G., Weron, R.: Property Insurance Loss Distributions. Physica A 287 (2000) 269–278
6. Cizek, P., H¨
ardle, W., Weron, R. (ed.): Statistical Tools for Finance and Insurance.
Springer, Heidelberg (2004)
7. Daykin, C.D., Pentikainen, T., Pesonen, M.: Practical Risk Theory for Actuaries.
Chapman&Hall, London (1994)
8. Embrechts, P., Kl¨
uppelberg, C., Mikosch, T.: Modelling Extremal Events. Springer,
Berlin (1997)
9. Grandell, J.: Aspects of Risk Theory. Springer, New York (1991)
10. H¨
ardle, W., Hlavka, Z., Klinke, S. (ed.): XploRe Application Guide. Springer, Heidelberg (2000)
11. H¨
ardle, W., Klinke, S., M¨
uller, M. (ed.): XploRe Learning Guide. Springer, Heidelberg (2000)
12. Janicki, A., Weron, A.: Simulation and Chaotic Behavior of α-Stable Stochastic
Processes. Marcel Dekker, New York (1994)
13. Panjer, H.H., Willmot, G.E.: Insurance Risk Models. Society of Actuaries, Schaumburg (1992)
14. Rolski, T., Schmidli, H., Schmidt, V., Teugels, J.L.: Stochastic Processes for Insurance and Finance. Wiley, Chichester (1999)
15. Ross, S.: Simulation. 3rd edn. Academic Press, San Diego (2001)
16. Stephens, M.A.: EDF Statistics for Goodness-of-Fit and Some Comparisons. Journal of the American Statistical Association 69 (1974) 730–737

