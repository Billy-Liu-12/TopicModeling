Available online at www.sciencedirect.com

Procedia Computer Science 9 (2012) 2016 – 2019

International Conference on Computational Science, ICCS 2012

Evaluation of two acceleration techniques in a multithreaded 2D
Poisson equation solver
Andrés Vidala,*, Damian Dechevb,c, Alain Kassaba
a

Department of Mechanical, Material and Aerospace Engineering, University of Central Florida, Orlando, FL32816, USA
b
Department of Electrical Engineering and Computer Science, University of Central Florida, Orlando, FL32816, USA
c
Scalable and Secure Systems R&D Department, Sandia National Laboratories, Livermore, CA 94551, USA

Abstract
Two acceleration techniques, based on additive corrections are evaluated with a multithreaded 2D Poisson equation solver. The
popular multigrid algorithm with 2-level grid is compared with the traditional block-correction strategy. In both, single-processor
and distributed architectures, block correction is faster than the multigrid due mainly to the smaller cost that the solution of a 1D
linear system has over one 2D linear system. Results in both cluster tested show that block correction can reduce significantly the
computing time in the solution of very large linear systems. These calculations confirm that the Red/Black ordering is effective
only if data fit entirely in cache memory.
"Keywords: Multithreaded solver; Poisson equation; Multigrid; Block-correction;"

1. Introduction

The efficient solution of large linear systems derived from the Poisson equation problem imposes many
challenging issues. The rise of multicore processors enables new opportunities to speed-up convergence. This
objective can be achieved by means of the modification of the existing methods as well as the development of new
procedures. A variety of factors impact performance including: the implementation strategy, the programming
language selected, the compiler chosen and the optimization techniques. The most common way to improve
convergence is by the use of acceleration techniques based on additive corrections. The popular multigrid algorithm
[9] and its predecessor block-correction procedure [5] have been extensively used to improve the rate of
convergence. In the literature, several efficient implementations can be found but all those methods were developed
for serial computers and, all implementations in multicore processors are done as serial execution units using a
communication protocol such as Message Passing Interface (MPI), Parallel Virtual Machine (PVM) and others. As

* Corresponding author. Tel.: +1-407-823-5778; fax: +1-407-823-0208.
E-mail address: Andres.Vidal@knights.ucf.edu.
c

Sandia National Laboratories is a multi-program laboratory managed and operated by Sandia Corporation, a wholly owned subsidiary of Lockheed Martin
Corporation, for the U.S. Department of Energy's National Nuclear Security Administration under contract DE-AC04-94AL85000.

1877-0509 © 2012 Published by Elsevier Ltd. Open access under CC BY-NC-ND license.
doi:10.1016/j.procs.2012.04.226

Andrés Vidal et al. / Procedia Computer Science 9 (2012) 2016 – 2019

2017

explained in [11], better performance can be achieved if a SOR serial solver is modified to be executed as several
independent units (threads) having access to common data. This work is an extension of the procedure developed in
[11] by studying the performance of multigrid and block-correction algorithms in the multithreaded solver.

2. Related work
The block correction procedure was a popular procedure in the 1980’s but little has been done to developed
parallel versions of it. On the other hand, the multigrid technique has been implemented not only in the Poisson
equation problem but in all problems in Computational Fluid Dynamics (CFD). We are not aware of the existence
of a comprehensive study of the scalability of the multigrid algorithm. In some cases, a super-linear speed-up is
achieved and it is frequently claimed as a characteristic of the method. For example, in [4], a parallel multigrid
procedure is presented, in which the multigrid procedure is done globally. In this work, a super-linear speed-up is
obtained with a large number of processors. When analyzing the size of the problem and the number of processors
used in [4], the problem fits perfectly in cache memory. In another work [7], a system of 1,048,576 equations is
solved with a parallel multigrid procedure. In this paper, a super-linear speed-up is claimed when 64 processors are
used. For that number of nodes, there are only 32,768 equations per processor. That fits in cache memory. In [6], a
parallel multigrid algorithm is developed and scalability up to 3,500 processors is presented. When analyzing the
time versus the number of processors, there is a small increment in time as the number of processors increases. If the
speed-up is computed, we realize that the gains are not as impressive. We are aware of some efficient parallel
implementations of the multigrid algorithm, applied to CFD. In [1] and [2], a general procedure is presented but the
paper does not present a thorough study of the scalability of the procedure. In [3], a super-linear speed-up is claimed
but the paper lacks an explanation of the conditions in which this speed-up has been observed. In [12], a parallel
multigrid algorithm is presented for the solution of a single problem. Finally, in [10] a global parallel multigrid
procedure is shown with a super-linear speed-up when the number of processors is large. (Note: how do we compare
to [10]?)
3. Multithreaded solver
The base multicore solver [11] relies on the execution of multiple concurrent threads. The tasks are distributed
into two main categories: intensive floating point operations (FPU) and non-intensive FPU operations. This scheme
optimizes the math unit, a very important aspect for problems where most of the time is spent in floating point
calculations:
Start a predefined number of solvers (threads)












Main thread (non FPU intensive ops)







	



	




In order to maximize performance, the following considerations have been observed: First, the number of solvers
plus the main thread must be equal to the number of physical threads (for Intel and AMD platforms); second, each
solver should access continuous portions of data. This improves cache memory usage; and finally, performance is
boosted with Red/Black ordering only if data fit entirely in cache memory. Under these conditions, in some cases is
possible to have super-linear speed-up. This is clearly a hardware issue.

2018

André s Vidal et al. / Procedia Computer Science 9 (2012) 2016 – 2019

4. Acceleration techniques
The idea of multigrid algorithm is to use different grids of different sizes. All those grids are identified to a
specific level. The first level will contain the finest grid and, as well as the level number is increased, the size of the
grid is reduced. The finest grid will contain the solution to the problem while the coarse grids will contain the
corrections to the immediate lower grid level [9]. The multigrid algorithm is widely used in fluid problems with
unstructured grids, where the data cannot be stored in a rectangular array [1, 2, 3, 10 and 12]. On the other hand, the
objective of block correction is the same as multigrid but the corrections are performed in lines [5]. Since the
correction equation is a tri-diagonal linear system, the fast and efficient Thomas algorithm can be used to obtain the
corrections in a small fraction of the time of the iterative solver. Due to its simplicity, block correction is very
efficient but it is limited to structured grid problems.
5. Test problem and results
The test problem is

∇ 2T = 0 with the boundary conditions T (0, y ) = 10 , T (1, y ) = 30 , T ( x,0) = 20 ,

T ( x,1) = 40 . The sub-domain method was selected for the parallelization of the problem since allows a complete
distribution of data and work load between nodes. Tests were performed in two different architectures:

Euler: Cluster with 64 nodes, each one with 2 AMD Opteron 248, 2.20 GHz, 1 MB cache, Rocks 5.1 / CentOs 5.2 operating system and g++
compiler version 4.1.2. MPI: Open MPI. Nodes/Processors available for tests: 50/100. Type of interconnection: GigabitEthernet (5)
Hilbert: Cluster with 64 nodes, each one with 2 Intel Xeon dual-cores, 3.00 GHz, 2 MB cache, disabled hyper-threading, Rocks 5.1 / CentOs
5.2 operating system and g++ compiler version 4.1.2. MPI: Open MPI. Nodes/Processors available for tests: 50/200. Type of
interconnection: GigabitEthernet (5a)

Following a similar procedure as done in [11], eight distributed procedures are evaluated:
DBC: Distributed solver with block-correction. Block correction is performed first, and one iteration on the fine grid is done later on.
Finally, communication is performed between nodes.
DBCRB: The same procedure DBC but with Red/Black ordering.
DMBC: Distributed multicore with block correction. The main thread performs corrections and data communications after one
iteration of the first solver. The solvers iterate continuously until stop condition is activated.
DMBCRB: The same procedure DMBC but with Red/Black ordering.
DMu: Distributed solver with multigrid correction. First, some iterations are performed on the fine grid and then the correction system
is solved. At the end of each iteration on the fine grid, data exchange is done.
DMuRB: The same procedure DMu but with Red/Black ordering.
DMMu: Distributed multicore with multigrid correction. The main thread monitors convergence and performs communication. Each
solver performs some iterations on the fine grid and then, solves its part of the correction equation.
DMMuRB: The same procedure DMMu but with Red/Black ordering.

The benchmark case was the standard serial solver, which is not necessarily the best implementation. Figure 1
shows the results in the cluster Euler. The results are similar to the ones observed in [11], in the sense that in a
dualcore cluster, better performance is obtained with the serial implementation than with the multicore one. In both
DBC and DBCRB, speed-up stabilizes at 30 nodes but, as soon as the data fit in L2 cache memory, performance
improves significantly. It is important to underline that this increment is a hardware issue and not a charasteristic of
the method. For this architecture, both serial solutions DMU and DMuRB perform better than the multicore ones. It
is interesting to observe that the DMu solution has a peak in about 40 nodes an then the speed-up decays because of
the overhead that the communication with 100 processors have. This behavior is consistent with the 2-grid level
implementation in [8]. Once the data fit in cache memory, the solver DMuRB performance increases a bit. Results
for the solution of a linear system of 22,680,000 equations on cluster Hilbert are shown in figure 2. First of all, the
difference in performance is greater than those in the cluster Euler, confirming the advantage of the multicore
scheme proposed in this work and in [11].
6. Conclusions
Two additive correction procedures have been tested in a multicore environment, multigrid and block
correction. The traditional block correction scheme has proven to be very adaptable to multicore processor

Andrés Vidal et al. / Procedia Computer Science 9 (2012) 2016 – 2019

2019

architecture. For the quad-core processor, the multicore version has demonstrated an excellent speed up. In all cases,
block correction scheme proved to be robust in the reduction of the number of iterations and the total calculation
time. On the other hand, multigrid does not allow for a significant reduction of a procedure’s computation time since
the solution of a 2D problem, even with a reduced number of points, is still expensive.

Fig. 1. Speed-up for 13,320,000 equations on Euler cluster: a) Block-correction; b) Multigrid

Fig. 2. Speed-up for 22,680,000 equations on Hilbert cluster: a) Block-correction; b) Multigrid

The improvements, due mainly to the optimization of the iterator, show an increment in speed up consistent with
the results observed when studying the multicore solver described in [5]. The distribution of the tasks of the multigrid
algorithm among all threads does not seem to deliver performance gains.
7. References
1. N. B. Cheikh, B. B. Beya and T. Lili, Numerical Heat Transfer, Part B: Fundamentals, 52:2 (2007), 131,
2. M. Darwish, D. Asmar and F. Moukalled, Numerical Heat Transfer, Part B: Fundamentals, 45:1 (2004) 49.
3. M. Darwish, T. Saad and Z. Hamdan, Numerical Heat Transfer, Part B: Fundamentals, 54:2 (2008) 157.
4. G. Haase, M. Kuhn and S. Reitzinger, SIAM J. Sci. Comput., 24:2 (2002) 410.
5. B. R. Hutchinson and G. D. Raithby, Numerical Heat Transfer 9 (1986) 511.
6. W. Joubert and J. Cullum, Electronic Transactions on Numerical Analysis, 23 (2006) 105.
7. K. S. Kang, Max-Planck-Institut für Plasmaphysik, Parallelization of the Multigrid Method on High Performance Computers, 2010.
8. S. Lu, University of California, San Diego, Scalable Parallel Multilevel Algorithms for Solving Partial Differential Equations, Dissertation,
2004.
9. G. Meurant, Elsevier (eds.), Computer Solution of Large Linear Systems, 1999.
10. M. Soria, C. D. Pérez-Segarra and A. Oliva, Numerical Heat Transfer, Part B: Fundamentals, 41:2 (2002) 117.
11. A. Vidal, A. Kassab, D. Mota and D. Dechev, A Multithreaded Solver for the 2D Poisson Equation. To appear in HPC 2012.
12. J. Yan and F. Thiele, Numerical Heat Transfer, Part B: Fundamentals, 34:3 (1998) 323.

