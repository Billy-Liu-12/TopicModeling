Advanced High Performance Algorithms
for Data Processing
Alexander V. Bogdanov and Alexander V. Boukhanovsky
Institute for High Performance Computing and Information Systems,
St. Petersburg, Russia
{bogdanov, avb}@csa.ru, http://www.csa.ru

Abstract. We analyze the problem of processing of very large datasets on parallel systems and find that the natural approaches to parallelization fail for two
reasons. One is connected to long-range correlations between data and the other
comes from nonscalar nature of the data. To overcome those difficulties the
new paradigm of the data processing is proposed, based on a statistical simulation of the datasets, which in its turn for different types of data is realized on
three approaches - decomposition of the statistical ensemble, decomposition on
the base of principle of mixing and decomposition over the indexing variable.
Some examples of proposed approach show its very effective scaling.

1 Introduction
The amount of data, generated by scientific and technological activity of humanity is
increasing by the order of magnitude every couple of years. The new features, which
become evident in the last years are the nonhomogeneous types of the data and the
need of determination of the detailed characteristics of the process, including higher
order moments of it. Presence of long-range correlations and nonscalar nature of data
make it very difficult to use large parallel computer systems for their processing.
Even when it becomes possible, the effect of parallelization can be very small, because of the bad load balancing. To build the effective algorithms several steps should
be taken, that make it possible to decrease the dimension of the problem, to find the
variable, that make it possible to make uniform indexing of data through all set and
finally to simulate the initial process by the relevant procedures, that can be mapped
effectively onto the large computer system.
Due to the complexity of the natural and technological phenomena it is possible to
formulate several approaches within proposed paradigm for solution of pertinent
problem. That is why we give also the algorithms for determination of the optimal
approach for given multiprocessor system. The same technology can be used for
control of the load balancing.

M. Bubak et al. (Eds.): ICCS 2004, LNCS 3036, pp. 239–246, 2004.
© Springer-Verlag Berlin Heidelberg 2004

240

A.V. Bogdanov and A.V. Boukhanovsky

2 Challenges in Parallel Data Processing
The problem of the processing of very large datasets can be illuminated for the simplest probabilistic models, e.g. – model of random value (RV) or multivariate random
value (MRV). These models allow the use of the classical statistical approaches for
the parallel processing of independent data flows on the transputers or multicomputer
farms. The development of the parallel algorithms for processing of a more complicated data models, e.g. sets of time series (TS) ζ (t ) or spatiotemporal random fields

Ξ (r, t ) (STRF), is not so easy, as seems.
The main source of difficulties of the parallelization of the statistical algorithms is
the correlations in the multivariate data. It is the result of the multiscale variability,
nonstationarity and inhomogeneity for both the natural and technical complex systems. The two types of such correlations could be considered [4]:
− Spatiotemporal dependence of the data in different points r at the time t , as
result of the non-local effects (non-stationary behavior and spatial inhomogeneity).
− Intra-element dependence, as the result of the axiomatic representation of the
multivariate random data (as the system of scalar values, Euclidean of affine vectors, functions etc.).
Multivariate statistical analysis (MSA) is the traditional tool for the development of
the statistical models of the correlated data (e.g. [2]). Its goals are the reduction of the
dimensionality, determination of correlations and description of inhomogeneity of the
multivariate statistical sample. Nowadays classical MSA is proposed for model of
MRV, but for TS and STRF the principal approaches were not generated yet, in spite
of some specific approaches for certain classes of the data [9,19]. Hence, the more
general approach on the base of the functional analysis is needed for formalization of
both types of the dependence.
Moreover, the generalization of MSA procedures for more general models is associated with the complicate statistical inference tools. It crucially restricts the possibility to obtain the simple and transparent analytical expressions even for the simplest
statistical estimates. Hence, the computational tools of the statistics on the base of
Monte-Carlo simulation must be widely used [24].
Thus, the principal problem of the development of high-performance statistical algorithms is not in extensive code optimization only. The development of the adequate
parallel models for statistical description of the multivariate data is of the prime importance. This approach must take into account both the spatiotemporal and intraelement variability of the data. Only such approach allows to achieve the direct intrinsic mapping to architecture of the parallel computer system.

3 Regenerative Paradigm of Multivariate Statistics
The problem of the development of the parallel statistical models could be solved in
the frame of regenerative paradigm of the computational statistics [23]. It means, that

Advanced High Performance Algorithms for Data Processing

241

the result of any data processing could be considered as the imitation model (algorithm for Monte-Carlo simulation) for the initial dataset. It allows simulate the largesize ensemble of the data realizations for the numerical studies of different features of
the data, especially – non-observable events etc. [5].
This paradigm leads to the promising new possibilities for the development of the
parallel algorithms for both the statistical analysis and synthesis. It allows constructing the intrinsic parallel models for the dependent data, when the parallelization of
the classical statistical procedures is impossible. Thus, the problem of parallel decomposition may be solved on the level of the imitative model.
The development of the parallel statistical models for TS and STRF variability is
possible to represent as the next four stages.
Reduction of the dimensionality for the initial data Ξ (r, t ) ∈ H in the linear space
H . The goal of this stage is the construction of the set of most informative indexes,
characterizing the sample variability. This gives the system of the linear operators
I k : H → X , where dim( X ) ≤ dim( H ) , and allows to project the initial data set on a
subspace. The sequential (in order p ) application of the hierarchy of the operators

I k( p ) : H p × H p +1 → R × H p +1, p = 1,2,... ,

(1)

allows not only to “fold” the multivariate data space to R , but also simplify the probabilistic data model. E.g., in accordance with Eqn. (1) the representation of non-scalar
STFR reduced to analysis of the set of TS, and further – to MRV. This principal step
makes it possible the complete use of the traditional techniques MSA MRV.
Identification of the model. The Eqn. (1) allows to express the dependence between
non-scalar components of Ξ (r, t ) ∈ H in terms of the system of scalar indexes

Z = {z k (t )} . These indexes may be treated as the MRV or system of TS. For the
quantitative description of the temporal ( t ) and intra-element ( k ) dependencies of
these data the model of linear stochastic dynamic system has been considered [1]:
LZ = RE + BΗ .
(2)
Here L, R, B – are the linear differential operators, Е – is the multivariate white
noise (independent realizations of random value), and Η is the set of driving stochastic factors (predictors). The objects Z (t ), Ε (t ), Η (t ) are multivariate, and possible dim(Ξ ) ≠ dim(Η ) . The Eqn. (2) is the generalization for different regression
models for TS, e.g. ARMA [13], dynamic [22] and spectral [12] regressions. This
way also allows intrinsically extend the qualitative correlation theory of the RV on
the TS and STRF, because Eqn. (2) is justified the terms of functions of partial, multiple and canonical correlations. These characteristics reflect the non-local dependencies of TS on the whole time interval. The results of the qualitative analysis are used
for the identification of the set of model parameters ϑ (coefficients of the L, R, B) on
the initial data sample.
Statistical synthesis. The Eqn. (2) may be treated as the algorithm for Monte-Carlo
simulation of multivariate TS Z = {z k (t )} , using the advanced numerical techniques
[20]. Hence it is considered as the milestone for the construction of the hierarchy of
the stochastic operators J ( p ) of Monte-Carlo procedure opposite to Eqn. (1). It al-

242

A.V. Bogdanov and A.V. Boukhanovsky

G
lows synthesizing the large-size ensemble Ξ (r , t ) ∈ H on the base of the estimated
parameters ϑ .
Verification, scenarios and forecast. The procedure of verification (error analysis)
is proposed as the technique for qualitative control of the statistical model. It allows
to establish the degree of the model adequacy to initial data. The verification based on
the statistical comparison of the simulated and sample characteristics has not been
used for the identification of the parameters ϑ . The elements of the verified simulated ensemble may be treated as the statistical scenarios of the non-observable
events, in respect to the probability of its occurrence [5]. On the base of statistical
scenarios the different statistical problems for the inferences, control, monitoring and
statistical forecast may be solved.
In view of parallel processing, the principal feature of I ( p ) , J ( p ) construction is
the intrinsic formalization of the parallel algorithm, using the possibility of the elimination of the correlations between data in computational procedure.

4 Principles of Intrinsic Parallelization
Generally, there is no unique way to parallel formalization of all the types of statistical models, due to complexity of the mathematical tools. But the mapping of the
statistical algorithms on the parallel architecture may be based on the three principles
[7]. These principles allow classifying the methods of statistical processing and
Monte-Carlo simulation by means of the natural way of intrinsic parallelization.
Decomposition of the statistical ensemble. This principle reflects the postulate of
the independence of sample elements. It allows dividing the sample on the independent fragments and process these data in parallel. The resulting computational algorithm is rather homogenic. Hence, the most statistical procedures for both RV and
MRV models may be, in principle, adopted for parallel architecture. The main problem of the ensemble decomposition is the further integration of the estimates, obtained on the different processors. If each parallel estimate is treated as the realization
of RV, the theory of small sample may be adopted for the formalization of the results
of parallel processing.
Decomposition on the base of principle of mixing. This is the modification of statistical ensemble decomposition for the model of TS with local dependence between
data. The idea of mixing principle [15] is the possibility to consider the values Ξ (t )
and Ξ (s ) independently, when t − s >> 1 . Hence, the realization of TS may be divided on the set of uncrossed fragments. Each fragment simulated by Eqn. (2) in
parallel. After that, the matching of the parallel fragments may be organized as binary
tree algorithm, when Eqn. (2) is considered as the boundary problem, where boundary conditions are the values, obtained on the previous step.
Decomposition of the indexing variable. This principle corresponds to alternative
way for dependence elimination in stochastic model. It is important for the multivariate data, e.g. – for model of inhomogeneous STRF Ξ (r, t ) , where the mixing princi-

Advanced High Performance Algorithms for Data Processing

243

ple is out of consideration. The general approach is based on the specific construction
of the operators I ( p ) , J ( p ) of data transformation, thus the values of the transformed
data for different values of index variable (e.g. - r ) could be computed independently. If the operators in Eqn. (1) may be expressed by means of orthogonal expansions technique [3,16]:

Ξ (r, t ) = ∑ z k (t )φ k (r, t ) ,

def

I k ≡ z k (t ) = (Ξ , φ k ) r ,

(3)

k

the values of Ξ (r, t ) for different r may be expressed in parallel. Hence, the spatial
domain r ∈ ℜ allows the intrinsic decomposition on the fragments has been processed in parallel.
Let us note, that the development of rather complicated models of computational
multivariate statistics for the data with both the intra-element, spatial and temporal
(spatiotemporal) dependence, not allows using only one principle of the parallel decomposition. Usually some combinations are used for multivariate data in respect to
features of the each type of variability.

5 Performance Analysis and Load Balancing Optimization
Non uniqueness of the principles of intrinsic parallel decomposition, agglomeration
and communications require the use of the specific techniques [11] for the preliminary quantitative analysis of the parallel performance for statistical algorithms. The
main object of the analysis is the parallel speed-up S . Let us consider, that the initial
dataset is characterized by means of set of parameters χ (sample size, dimension of
the data, number of model coefficients etc.), and the architecture of the parallel computer system is characterized by parameters (t s , t w , t c ) (latency, communication time,
and computation time respectively) [10]. It allows developing of the analytical model
for comparison of the two (or more) statistical algorithms for the parallel processing.
The isoefficiency surface in terms of S p for two algorithms (“A” and “B”) may be
expressed in implicit form:
S A (t s , tc , t w , p, χ ) = S B (t s , tc , t w , p, χ ) .

(4)

When (t s , t w , tc ) is fixed (for concrete architecture of computer system), the Eqn.
(4) describe the surface in space ( p, χ ). This surface may be treated as the barrier for
the success domains for each algorithm.
The Eqn. (4) allows formulating of the so call “concurrence principle”, as the criterion of the selection of most effective algorithm. The realization of this principle is
the intellectual technology of mapping that allows to take into account both features
of the initial data and specific features of the parallel architecture. Hence, it provides
the possible scalability of the algorithm, and the code may be effectively ported for
different parallel systems.
The use of the theoretical performance model (Eqn. (4)) is possible only for computational systems with rather simple topology of the network, and low number of

244

A.V. Bogdanov and A.V. Boukhanovsky

processors (not more 256). Generally, these models do not take into account the technical features of MPP-systems, e.g. the possibility of communications via more, than
one router. The different ways for model improvement are rather specific [21].
Hence, the problem of practical scalability for large number of processors requires the
special techniques of parallel load balancing, instead of analytical models (4). For the
development of algorithms for scheduling and load balancing the same principles of
decomposition, as for parallel statistical models, may be used.

6 Applications
Statistical analysis and simulation of different natural and technical complex events in
frame of regenerative approach requires the development of the different models
associated with specific parallel representation. Let us consider two crucial computational problems.
6.1 Estimation of the Extreme Waves in the Storm once T-years
The stochastic model of multiscale (synoptic, annual, year-to-year) spatiotemporal
variability of sea wave fields is considered in [6]. Using of this model for estimation
of T-years waves require a lot of computational resources. E.g., even for Barents Sea
0
0
on the gridpoint (0.5 ×1.5 ) for statistical estimation of 100-years waves the computa11
tions of 10 values are required. The principle of the decomposition on indexing
variable is the best (in accordance with Eqn. (4)) for these data. In the fig. 1(a,b) the
results of the computation of 10- and 100-years significant wave height extremes in
different points of the sea are shown. Let us note, that the estimates of field extremes
(e.g. – simultaneous values in the few points) may be obtained by means of regenerative approach only, because all other methods are extensively oriented on rare events
in a fixed point [17].
6.2 Simulation of Multivariate ECG Signal Variability
Another application of the regenerative approach is the stochastic simulation of ECG
signal for criterion formulation of the pathological cardio-dynamics [8]. The most
known models of ECG have been developed for unichannel signal for the purposes of
the classification and discrimination [14]. But arising of the new multichannel cardiomonitors with extremely high resolutions require using parallel computations for
real-time simulations. The most crucial elements of the model are the estimation of
the basic functions {φ k } in Eqn. (3) on the sample data, and the direct Monte-Carlo
simulation. In the fig. 1(c) the absolute computational time for different stages of
modeling are shown for the different number of the processors. In the fig. 1(d) the
result of dynamic balancing of the computational algorithm (in term of the parallel
efficiency E p ) are shown in dependence of value m - number of sub-samples that
geometrically distributed on all the processors.

Advanced High Performance Algorithms for Data Processing

(c)

245

(d)

Fig. 1. Result of the parallel stochastic models applications; (a,b) – estimation of combinations
of significant wave heights once 10- and 100-years in the points A and B in a Barents sea:
(74N,30E-74N,35E) (a) and (74N,30E-74N,40E) (b). Points are the annual extremes on hydrodynamic modeling [18]; (c,d) – performance of the stochastic model of multilead ECG: Total
computational time of model stages (c), and parallel efficiency for different regimes of the
dynamic load balancing (d).

7 Conclusions
The new paradigm of the data processing is proposed, based on a statistical simulation of the datasets, which in its turn for different types of data is realized on three
approaches – decomposition of the statistical ensemble, decomposition on the base of
principle of mixing and decomposition over the indexing variable. In practical realization the combination of only one or two of those approaches can be used, that open
effective possibilities for processing of very large data on multiprocessors computer
system. The concurrence principle is proposed for choosing the most effective algorithm and to make load balancing more easily. Some examples of proposed approach
show, that even for most complex natural processes it is possible to find the approach
for determination most sensitive characteristics of the phenomenon.

246

A.V. Bogdanov and A.V. Boukhanovsky

References
1. Adomian G. Stochastic systems. Academic Press, NY (1983).
2. Anderson T.W., Gupta S.D., Styan G.P.H. A bibliography of multivariate statistical analysis. Robert E. Krieder Pub. Company, Hantington, NY (1977)
3. Blais J.A.R. Estimation and spectral analysis. Univ. of Calgary Press (1988)
4. Boukhanovsky A.V., Degtyarev A.B., Rozhkov V.A. Peculiarities of computer simulation
and statistical representation of time–spatial metocean fields. LNCS 2073, Springer–
Verlag, (2001), pp.463–472.
5. Boukhanovsky A.V. Multivariate stochastic models of metocean fields: computational
aspects and applications. LNCS 2329, Springer–Verlag (2002), pp. 216-225
6. Boukhanovsky A.V., Krogstad H., Lopatoukhin L., Rozhkov V., Athanassoulis G., Stefanakos Ch. Stochastic simulation of inhomogeneous metocean fields. Part II: Synoptic variability and rare events. LNCS 2658, Springer-Verlag (2003), pp. 223-233.
7. Boukhanovsky A., Ivanov S. Stochastic simulation of inhomogeneous metocean fields.
Part III: High-performance parallel algorithms. LNCS 2658, Springer-Verlag (2003),
pp.234-243.
8. Boukhanovsky et al. Telemedicine complex on the base of supercomputer technologies.
Proceeding of X Russian Scientific Conference “Telematica-2003”, vol. 1 (2003), pp. 288289 (in Russian).
9. Dempster A.P. Elements of continuous multivariate analysis. Addison-Wesley Pub. Company, Reading (1969)
10. Foster J. Designing and Building Parallel Programs. Addison-Wesley (1995).
11. Gerbessiotis A.V. Architecture independent parallel algorithm design: theory vs practice.
Future Generation Computer Systems, 18 (2002), pp. 573-593.
12. Hamon B.V., Hannan E.J. Estimating relations between time series. J. of Geophysical
Research, v. 68 (21) (1963), pp. 6033-6042.
13. Jenkins G.M., Watts D.G. Spectral analysis and its application. Holden-Day, SanFrancisco (1969).
14. Koski A. Modelling ECG signals with hidden Markov models. Artificial intelligence in
medicine (8) (1996), pp. 453-471.
15. Leadbetter M., Lindgren G., Rootzen H. Extremes and related properties of random sequences and processes. Springer-Verlag, NY, (1986).
16. Loeve M. Fonctions aleatories de second odre. C.R. Acad. Sci. 220, (1945).
17. Lopatoukhin L.J., Rozhkov V.A., Ryabinin V.E., Swail V.R., Boukhanovsky A.V.,
Degtyarev A.B. Estimation of extreme wave heights. JCOMM Technical Report, WMO/TD
#1041 (2000).
18. Lopatoukhin L.J. et al. The spectral wave climate in the Barents sea. Proceedings of Int.
Conf OMAE’02, Oslo, Norway, June 23-28 (2002) (CD-version).
19. Lutkepohl H. Introduction to multivariate time series analysis. Springer-Verlag (1991)
20. Ogorodnikov V.A., Prigarin S.M. Numerical modelling of random processes and fields:
algorithms and applications. VSP, Utrecht, the Netherlands (1996).
21. Pandle S., Agrawal D.P. (Eds.) Compiler Optimization for Scalable PC, LNCS 1808,
Springer-Verlag (2001).
22. Pesaran M.H., Slater L.J. Dynamic regression: theory and algorithms. Ellis Horwood Limited, NY (1980).
23. Rubinstein R.Y. Simulation and the Monte-Carlo method. John Wiley & Sons (1981)
24. Yakowitz S.J. Computational Probability and Simulation. Addison-Wesley (1977)

