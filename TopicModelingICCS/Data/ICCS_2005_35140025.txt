Computing Orthogonal Decompositions of
Block Tridiagonal or Banded Matrices
Wilfried N. Gansterer
Institute of Distributed and Multimedia Systems,
University of Vienna, Austria

Abstract. A method for computing orthogonal URV/ULV decompositions of block tridiagonal (or banded) matrices is presented. The method
discussed transforms the matrix into structured triangular form and has
several attractive properties: The block tridiagonal structure is fully exploited; high data locality is achieved, which is important for high eﬃciency on modern computer systems; very little ﬁll-in occurs, which leads
to no or very low memory overhead; and in most practical situations observed the transformed matrix has very favorable numerical properties.
Two variants of this method are introduced and compared.

1

Introduction

In this paper, we propose a method for computing an orthogonal decomposition
of an irreducible symmetric block tridiagonal matrix
⎞
⎛
B1 C1
⎟
⎜ A1 B2 C2
⎟
⎜
⎟
⎜
.
..
⎟ ∈ Rn×n
A
B
(1)
Mp := ⎜
2
3
⎟
⎜
⎟
⎜
.
.
.. .. C
⎠
⎝
p−1
Ap−1 Bp
with p > 1. The blocks Bi ∈ Rki ×ki (i = 1, 2, . . . , p) along the diagonal are
quadratic (but not necessarily symmetric), the oﬀ-diagonal blocks Ai ∈ Rki+1 ×ki
and Ci ∈ Rki ×ki+1 (i = 1, 2, . . . , p − 1) are arbitrary. The block sizes ki satisfy
p
1 ≤ ki < n and i=1 ki = n, but are otherwise arbitrary.
We emphasize that the class of matrices of the form (1) comprises banded
symmetric matrices, in which case the Ci are upper triangular and Ci = Ai .
Alternatively, given a banded matrix with upper and lower bandwidth b, a
block tridiagonal structure is determined by properly selecting the block
sizes ki .
Motivation. Banded matrices arise in numerous applications. We also want to
highlight a few situations where block tridiagonal matrices occur. One example
from acoustics is the modelling of vibrations in soils and liquids with diﬀerent
V.S. Sunderam et al. (Eds.): ICCS 2005, LNCS 3514, pp. 25–32, 2005.
c Springer-Verlag Berlin Heidelberg 2005

26

W.N. Gansterer

layers using ﬁnite elements [1]. Every ﬁnite element is only linked to two other
elements (a deformation at the bottom of one element inﬂuences the deformation
at the top of the next element). Consequently, when arranging the local ﬁnite
element matrices into a global system matrix, there is an overlap between these
local matrices resulting in a block tridiagonal global matrix. Another example
is the block tridiagonalization procedure introduced by Bai et al. [2]. Given
a symmetric matrix, this procedure determines a symmetric block tridiagonal
matrix whose eigenvalues diﬀer at most by a user deﬁned accuracy tolerance
from the ones of the original matrix.
In this paper, we present two orthogonal factorizations of Mp —a URV and a
ULV factorization, where U and V are orthogonal matrices, and R and L have
special upper and lower triangular structure, respectively. These decompositions
are important tools when block tridiagonal or banded linear systems have to be
solved. This aspect is discussed in more detail in Section 3.
Although many research activities on ULV/URV decompositions have been
documented in the literature (see, for example, [3, 4, 5]), we are not aware of work
speciﬁcally targeted towards block tridiagonal matrices. A distinctive feature of
the approach described in this paper is that it fully exploits the special structure
deﬁned in Eqn. (1).
Synopsis. The algorithm for computing the two orthogonal decompositions and
their properties are described in Section 2, important applications are summarized in Section 3, and concluding remarks are given in Section 4.

2

Factorization of Block Tridiagonal Matrices

In this section, we summarize two closely related methods for computing an
orthogonal factorization of the block tridiagonal matrix Mp deﬁned by
Eqn. (1).
The basic idea behind the algorithms is to eliminate oﬀ-diagonal blocks one
after the other by computing singular value decompositions of submatrices and
performing the corresponding update. Depending on how the submatrices are
chosen, there are two basic variants of this factorization, a URV and a ULV
factorization. In the ﬁrst one, Mp is transformed into upper triangular structure, whereas in the second one, Mp is transformed into lower triangular structure. A comparison between the two variants for speciﬁc applications is given in
Section 3.2.
2.1

URV Decomposition

Based on the SVD of the block comprising the ﬁrst diagonal and the ﬁrst subdiagonal block of Mp ,
B1
A1

= U1

we can transform Mp into

Σ1
0

V1 =

U11 U13
U12 U14

Σ1
0

V1

Orthogonal Decompositions of Block Tridiagonal

⎛

Σ1
⎜ 0
⎜
⎜
⎜
⎜
⎜
⎝

27

⎞

C˜1 F˜1
˜2 C˜2
B

⎟
⎟
⎟
⎟
.
A2 B3
⎟
⎟
.. ..
.
. Cp−1 ⎠
Ap−1 Bp
..

by updating with U1 from the left and with V1 from the right. The existing
blocks C1 , B2 , and C2 are modiﬁed in this process (indicated by a tilde),
C˜1 = U11 C1 + U12 B2

(2)

˜2 = U13 C1 + U14 B2
B
C˜2 = U14 C2 ,

(3)
(4)

and one block in the second upper diagonal is ﬁlled in:
F˜1 = U12 C2 .

(5)

Next, we continue with the SVD of the subblock comprising the updated
diagonal block and the subdiagonal block in the second block column:
˜2
B
A2

= U2

Σ2
0

V2 .

Again, we perform the corresponding updates on C˜2 , B3 , C3 , the ﬁll-in of F˜2
with U2 from the left, and the update of the second block column with V2 from
the right. Then, we compute the SVD of
˜3
B
A3

,

update with U3 and with V3 , and so on.
After p − 1 such steps consisting of SVD and update operations, followed by
the SVD of the diagonal block in the bottom right corner
˜p = Up ΣVp
B
we get a factorization
Mp = U RV

(6)

with upper triangular R (see Fig. 1). More speciﬁcally,
⎞
⎛
Σ1 C˜1 F˜1
⎟
⎜
Σ2 C˜2 F˜2
⎟
⎜
⎟
⎜
.. ..
..
⎟
⎜
.
.
.
R=⎜
⎟,
⎜
˜
˜
Σp−2 Cp−2 Fp−2 ⎟
⎟
⎜
⎠
⎝
C˜
Σ
p−1

p−1

Σp

(7)

28

W.N. Gansterer

Fig. 1. URV decomposition of Mp

U is orthogonal and represented as the product
⎛

⎞

U11 U13
⎜ U12 U14
⎜
⎜
I
⎜
U =⎜
I
⎜
⎜
..
⎝
.
⎛

⎞

I

⎟ ⎜ U21 U23
⎟ ⎜
⎟ ⎜ U22 U24
⎟ ⎜
⎟×⎜
I
⎟ ⎜
⎟ ⎜
..
⎠ ⎝
.
I

I

⎛

⎞

⎛

⎟
⎟
⎟
⎟
⎟ × ···
⎟
⎟
⎠
I

I

⎞

⎟
..
⎜ I
⎟ ⎜
⎟
.
⎜
⎟ ⎜
⎜
⎟
⎜
⎟ ⎜
.
⎟
.
.
⎜
⎟ ⎜
.
.
⎟,
.
··· × ⎜
⎟×⎜
⎟
⎜
⎟ ⎜
I
⎟
⎜
⎟ ⎜
I
⎟
1
3
⎝
Up−1 Up−1 ⎠ ⎝
⎠
I
2
4
Up−1 Up−1
Up
and V is also orthogonal and block diagonal:
V = block-diag(V1 , V2 , . . . , Vp ).
The correctness of this decomposition algorithm can be veriﬁed directly by multiplying out (6) and taking into account the respective relationships corresponding
to Eqns. (2, 3, 4, 5).
2.2

ULV Decomposition

We may as well start the process described in the previous section with the SVD
of the block comprising the ﬁrst diagonal and the ﬁrst super diagonal block of
Mp ,
B1 C1 = U1 Σ1 0 V1 = U1 Σ1 0

V11 V13
V12 V14

.

Orthogonal Decompositions of Block Tridiagonal

29

Fig. 2. ULV decomposition

The corresponding updates with U1 from the left and with V1 from the right
will transform Mp into
⎛

Σ1
⎜ A˜1
⎜
⎜
⎜ F˜1
⎜
⎜
⎝

⎞

0
˜2 C2
B

⎟
⎟
⎟
⎟.
.
A2 B3
⎟
⎟
.. ..
.
. Cp−1 ⎠
Ap−1 Bp
..

Next, we compute the SVD
˜2 C2 = U2 Σ2 0 V2 ,
B
update again, compute the next SVD in the next block row, etc.
After p − 1 such steps, again consisting of SVD and update operations, followed by the SVD of the diagonal block in the bottom right corner
˜p = Up ΣV ,
B
p
this time we get a factorization
Mp = U LV

(8)

with lower triangular L and orthogonal U and V (see Fig. 2).
2.3

Properties

The decompositions produced by the algorithms described in the previous section
have the following important properties.
– p − 1 lower or upper oﬀ-diagonal blocks are “pushed” onto the other side
of the diagonal blocks, ﬁlling in the p − 2 blocks Fi in the second upper
or lower block diagonal. This implies that there is no increase in memory
requirements, except when the transformation matrices U or V need to be
stored.

30

W.N. Gansterer

– The blocks along the diagonal of R and L are diagonal matrices, whose
entries are non-increasing in each block (because they are computed as the
singular values of another matrix).
– In the URV decomposition, explicit representation of U requires considerable
computational eﬀort because of the overlap between consecutive block rows,
whereas explicit representation of V is available directly because of its simple
block diagonal structure.
– In the ULV decomposition, we have the opposite situation: Analogously to
the previous argument, explicit representation of V requires considerable
computational eﬀort, whereas explicit representation of U is available directly.
– The factorizations (6) and (8) are not guaranteed to be rank-revealing (cf. [3]).
However, our experience and current work shows that in practice it turns
out to be rank-revealing in most cases and that it is very useful even if it
turns out not to be rank-revealing [6].
– The arithmetic complexity of the algorithms described in Sections 2.1 and 2.2
is roughly c1 n maxi ki2 + c2 maxi ki3 with moderate constants c1 and c2 . Consequently, the factorizations (6) and (8) can be computed highly eﬃciently,
especially so for small block sizes ki (“narrow” Mp ).

3

Applications

The investigation of various applications of the orthogonal decompositions described in this paper is work in progress. In the following, we summarize the
basic directions of this approach.
Clearly, one idea is to utilize the decompositions described here in the context of solving linear systems. Compared to standard methods for solving block
tridiagonal (in particular, banded) linear systems, the approaches proposed in
this paper do not involve pivoting. Consequently, they are expected to have
important advantages in terms of eﬃciency in most practical situations. Some
rare numerically “pathological” cases, which may require slightly more expensive
techniques, are currently subject of intense research.
Our motivation comes from very speciﬁc linear systems arising in the context
of the block divide-and-conquer eigensolver.
3.1

Block Divide-and-Conquer Eigensolver

The block divide-and-conquer (BD&C ) eigensolver [7, 8] was developed in recent
years for approximating the eigenpairs of symmetric block tridiagonal matrices
Mp (Bi symmetric and Ci = Ai in Eqn. (1)).
This approach proved to be highly attractive for several reasons. For example,
it does not require tridiagonalization of Mp , it allows to approximate eigenpairs
to arbitrary accuracy, and it tends to be very eﬃcient if big parts of the spectrum
need to be approximated to medium or low accuracy. However, in some situations
eigenvector accumulation may become the bottleneck of BD&C—when accuracy

Orthogonal Decompositions of Block Tridiagonal

31

requirements are not low and/or when not the entire spectrum, but only small
parts of it have to be approximated.
With this in mind, we are currently developing a new approach for computing
approximate eigenvectors of a symmetric block tridiagonal matrix as an alternative to eigenvector accumulation [6]. This new approach has the potential of
signiﬁcantly improving arithmetic complexity, proportionally reducing the computational eﬀort if only a (small) part of the spectrum but not the full spectrum
needs to be computed, and being highly parallelizable and scalable for large
processor numbers.
3.2

Eigenvector Computation

We know that the eigenvector x corresponding to a eigenvalue λ of Mp is the
solution of
(9)
(Mp − λI) x = 0.
Since the shift operation preserves block tridiagonal structure, we can use one
of the algorithms presented in this paper compute an orthogonal decomposition
of the matrix S := Mp − λI and exploit it for solving the linear system (9)
eﬃciently.
URV vs. ULV. In this speciﬁc context, there is a clear preference for one of
the variants discussed in Section 2, as outlined in the following.
If we want to solve Eqn. (9) based on one of the decompositions described
in this paper, we need to ﬁrst solve Ry = 0 or Ly = 0 for the URV or the ULV
decomposition, respectively. Then, we need to form x = V y to ﬁnd a solution of
Eqn. (9).
As discussed in Section 2.3, in the URV decomposition explicit representation of V is available directly and it has simple block diagonal structure, whereas
multiplication with V from the ULV decomposition requires considerably higher
computational eﬀort. This clearly shows that the URV decomposition is preferable for this application context.
Backsubstitution. The rich structure of the matrix R resulting from the orthogonal decomposition described in this paper obviously allows for a very eﬃcient backsubstitution process when solving a system Ry = b.
In the special context of eigenvector computations, many speciﬁc numerical
aspects have to be taken into account in order to achieve certain requirements in
terms of residuals and orthogonality properties (cf. [9] and several publications
resulting from this work).

4

Conclusion

We have presented two related algorithms for computing orthogonal URV or
ULV decompositions of general block tridiagonal matrices. We have shown that
the matrices R and L can not only be computed eﬃciently, but that they also
have very attractive structural properties.

32

W.N. Gansterer

Because of these properties the methods introduced in this paper are expected
to have numerous applications. In particular, we expect them to be integral part
of a new approach for computing approximate eigenvectors of symmetric block
tridiagonal matrices.
Current and Future Work
Our current work focusses on reliable and eﬃcient methods exploiting the decompositions presented in this paper for approximating eigenvectors in the context
of the block divide-and-conquer eigensolver for symmetric block tridiagonal matrices. This goal requires careful investigation of several important numerical
aspects of the decompositions described here, especially in the backsubstitution
process. Our ﬁndings will be summarized in [6].
Moreover, we are also investigating other application contexts for the decompositions presented here.

Acknowlegdements
We would like to thank Robert C. Ward for many inspiring discussions when
developing the concepts presented here and the anonymous referees for their
comments.

References
1. Acoustics Research Institute of the Austrian Academy of Sciences: Vibrations in
soils and liquids – transform method for layers with random properties. FWFProject P16224-N07 (2004)
2. Bai, Y., Gansterer, W.N., Ward, R.C.: Block tridiagonalization of “eﬀectively”
sparse symmetric matrices. ACM Trans. Math. Softw. 30 (2004) 326–352
3. Stewart, G.W.: Updating a rank-revealing ULV decomposition. SIAM J. Matrix
Anal. Appl. 14 (1993) 494–499
4. Park, H., Elden, L.: Downdating the rank-revealing URV decomposition. SIAM J.
Matrix Anal. Appl. 16 (1995) 138–155
5. Stewart, M., Van Dooren, P.: Updating a generalized URV decomposition. SIAM
J. Matrix Anal. Appl. 22 (2000) 479–500
6. Gansterer, W.N., Kreuzer, W., Ward, R.C.: Computing eigenvectors of block tridiagonal matrices. (2005) in preparation.
7. Gansterer, W.N., Ward, R.C., Muller, R.P.: An extension of the divide-and-conquer
method for a class of symmetric block-tridiagonal eigenproblems. ACM Trans.
Math. Softw. 28 (2002) 45–58
8. Gansterer, W.N., Ward, R.C., Muller, R.P., Goddard, III, W.A.: Computing approximate eigenpairs of symmetric block tridiagonal matrices. SIAM J. Sci. Comput. 25 (2003) 65–85
9. Dhillon, I.S.: A New O(n2 ) Algorithm for the Symmetric Tridiagonal Eigenvalue/Eigenvector Problem. PhD thesis, Computer Science Division (EECS), University
of California at Berkeley (1997)

