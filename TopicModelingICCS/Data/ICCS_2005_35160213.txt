Autonomic Job Scheduling Policy
for Grid Computing
J.H. Abawajy
School of Information technology,
Deakin University, Geelong, VIC, 3217, Australia

Abstract. Autonomic middleware services will play an important role
in the management of resources and distributed workloads in emerging distributed computing environments. In this paper, we address the
problem of autonomic grid resource scheduling and propose a scheduling
infrastructure that is capable of self-management in the face of dynamic
behavior inherent to this kind of systems.

1

Introduction

Grid computing is a network of geographically distributed heterogeneous and
dynamic resources spanning multiple administrative domains [5]. Grids can potentially furnish large computational and storage resources to solve large-scale
problems. However, the need to integrate many independent and heterogeneous
subsystems into a well-organized virtual distributed systems introduces new levels of complexity making the underlying Grid environment inherently large, complex, heterogeneous and dynamic. Also it makes the systems highly susceptible
to a variety of failures. Some of these failures include node failure, interconnection network failure, scheduling middleware failure, and application failure. Due
to the inherent complexity and vulnerabilities, achieving large-scale distributed
computing in a seamless manner on Grid computing systems introduces not only
the problem of eﬃcient utilization and satisfactory response time but also the
problem of fault-tolerance [1].
One way to address these problems is to make Grid middleware technologies to embrace the concept of self-conﬁguring systems that are able to act autonomously and adapt to changes in application or user needs. There is a synergy
towards designing and building computing systems that are capable of running
themselves, adjusting to varying circumstances, and preparing their resources to
handle most eﬃciently the workloads we put upon them [7]. This new frontier of
designing and building next generation computing systems has become known
as autonomic computing [7]. The overall goal of autonomic computing is to make
systems anticipate needs and allow users to concentrate on what they want to
accomplish. The question is then how to design and develop autonomic grid resource management infrastructure that is capable of self-management (i.e., selfcontrol, self-healing, self-conﬁguration, self-optimization, and self-protection) in
the face of dynamic behavior inherent to this kind of systems?
V.S. Sunderam et al. (Eds.): ICCS 2005, LNCS 3516, pp. 213–220, 2005.
c Springer-Verlag Berlin Heidelberg 2005

214

J.H. Abawajy

Within this broad problem area, we address the problem of autonmous Grid
resource scheduling in the presence of a variety of failures. By autonomous Grid
resource scheduling we mean a schediling infrastructure that is capable of acting autonomously and adapt to changes in application or resource failures. We
propose an autonomic scheduling infrastructure that is capable of proactively
detect and rectify potential faults as applications are executing.
The rest of the paper is organized as follows. In Section 2, related work
is presented. This section also establishes the fact that, to a large extent, the
problem considered in this paper has not been fully addressed in the literature.
Section 3 presents the proposed autonomic scheduling policy. Finally, Section 4
presents the conclusion and future directions.

2

Related Work

The human body is self-healing. For example, broken bones mend, and cuts heal.
The concept of developing the next generation of computing systems should be
driven by the conceptual similarity between biological systems and digital computing systems [6]. Hence, the objectives of autonomic computing is to build
computing systems and services that are capable of managing themselves; that
can anticipate their workloads and adapt their resources to optimize their performance. [6][7][10].
Enhancing the core services of a Grid middleware technologies with autonomic capabilities so that the functions are self-managing is an important
research area. There has been some work towards autonomic grid computing
[8][11][6] [9]. Liu, and Parashar [11] present an environment that supports the
development of self-managed autonomic components, dynamic and opportunistic
composition of these components using high-level policies to realize autonomic
applications, and provides runtime services for policy deﬁnition, deployment and
execution. In [6], an autonomic architecture to achieve automated control and
management of networked applications and their infrastructure based on XML
format speciﬁcation is presented.
Our main focus in this paper is on autonomic grid resource scheduling middleware. Existing approaches statically allocate or release resources to Grid applications. Moreover, although fault-tolerance is one of the desirable properties
of any grid scheduling algorithm, unfortunately it has not been factored into
the design of most existing scheduling strategies for Grid computing systems.
Research coverage of fault tolerant scheduling is limited as the primary goal for
nearly all scheduling algorithms developed so far has been high performance by
exploiting as much parallelism as possible. Achieving integrated scheduling and
fault-tolerance goal is a diﬃcult proposition as the job scheduling and faulttolerance are diﬃcult problems to solve in their own right.
Clearly, there is a need for a fundamental change in how scheduling middleware for the next generation Grid computing are developed and managed.
We believe that the ability to self-manage while eﬀectively exploiting the variably sized pools of resources in an scalable and transparent manner must be

Autonomic Job Scheduling Policy for Grid Computing

215

an integral part of Grid computing scheduling middleware. We are not aware of
any work that is currently concentrate on autonomic Grid resource management
in general and scheduling middleware in particular. To this end, we propose
a scheduling infrastructure that is capable of dynamically scheduling resources
while at the same time capable of self-heal to various types of failures.

3

Autonomic Scheduling Middleware Infrastructure

In this section, we discuss the proposed autonomic scheduling policy based on
a hierarchical approach shown in Figure 1. Note that hierarchical scheduling
policies have been used in various platforms such as cluster computing [2] and
grid computing [4].
3.1

System Architecture

The core system architecture is designed around L-levels of virtual hierarchy,
which we refer to as a virtual organization tree, as shown in Figure 1. At the top
of the virtual organization tree, there is a system scheduler while at the leaf level
there is a local scheduler (LS) for each node. In between the system scheduler
and the local schedulers, there exists a hierarchy of middle schedulers(CS).

SUBMIT JOB

SUBMIT JOB

User Layer

Resource Layer

SS

CS

CS

CS

CS

CS

CS

CS

CS

CS

CS

CS

CS

LS

LS

LS

LS

LS

LS

LS

LS

LS

LS

LS

LS

P1

P2

P3

P4

P1

P2

P3

P4

P1

P2

P3

P4

cluster 1

cluster 5

cluster 8

Fig. 1. An example of virtual organization tree (SS: System scheduler; CS: middle
scheduler; LS: Local scheduler; and Pi : node i)

216

J.H. Abawajy

We refer to all processors reachable from a given node in the virtual organization tree as its partition-reach. We associate a parameter called base load level
with each node in the virtual organization tree. For non-leaf nodes, the base load
level is set to zero. For all the leaf-level nodes, the base load level is the same
as the multiprogramming level (MPL) of the node. The MPL parameter of a
node controls the maximum number of tasks that can concurrently execute at
any given time [2].
3.2

Autonomic Scheduling Middleware

Within the hierarchical structure shown in Figure 1, an autonomic Grid resource
scheduler can be viewed as a collection of autonomic schedulers that can manage their internal behaviors and their relationships and interactions with other
schedulers and the system. The Autonomic Scheduling (ASP) policy automatically replicats jobs and tasks over several grids and processors, keep track of the
number of replicas, instantiate them on-demand and delete the replicas when
the primary copies of the jobs and tasks successfully complete execution. In
this paper, we assume that every middle scheduler in the system is reachable
from any other middle scheduler unless there is a failure in the network or the
node housing the cluster scheduler. A scheme to deal with node, scheduler and
link failures is discussed in [3] [4]. Also, without lose of generality, we assume
that all incoming jobs are submitted to the system scheduler where they are
placed in the job wait queue until a placement decision is made. Figure 2 shows
the pseudo-code of the ASP policy. As shown in Figure 2, the ASP policy has
three main components namely; Self-scheduling, Job and Task Scheduling and
Fault Management components. These components are described in detail in the
following subsection.
3.3

Self-scheduling

ASP is demand-driven where nodes in the system look for work when their load
is below a given threshold. Speciﬁcally, whenever the current load level of a nonroot node in the cluster tree falls below its base load level, the node sends a
Request for Computation (RFC) message asking for Treq units of computation
to its parent. After sending RFC message to its parent, the node updates its base
load level to ensure that it can have only one outstanding RFC at any given time.
When a parent with no unscheduled work receives a RFC from a child, if there
is a pending RFC at the parent, the new RFC is backlogged and processed when
work becomes available. Otherwise, the RFC recursively ascends the cluster tree
until the RFC reaches either the system scheduler or a node that has unassigned
jobs. In the later case, a set of jobs/tasks are transferred down the hierarchy
along the path the RFC has traveled. This amount is determined dynamically
during parent and child negotiations and the number of unscheduled jobs as
described in the following section.

Autonomic Job Scheduling Policy for Grid Computing

217

SHS algorithm
1. Self-Scheduling
(a) IF Current load level falls below base load level
Treq = base load level − current load.

(1)

(b) Update base load level
ENDIF
2. Job/Task Assignment
(a) Determine ideal number of jobs/tasks that can be scheduled
Ttarget = Tr × number of tasks queued

(2)

where Tr is the transfer factor and is computed as follows:
Tr =

partition-reach of the child node
partition-reach of the parent node

(3)

(b) Adjust the number of jobs that will actually be transferred down one level to
the child as follows:
Ttarget =

min(Treq , number of tasks queued) ifTreq > Ttarget
min(Treq , child partition-reach)
Otherwise.

(4)

where Treq is the is the number of computations requested by the child scheduler.
(c) Finally, if the target size determined in Equation 2 is smaller than the Treq
from the child, then the parent will send the lesser of Treq and the number
of tasks queued. Otherwise, the set of tasks sent to the child is the lesser of
Ttarget and child partition-reach.
(d) Send a replica of the job to each site and update job table and backup scheduler.
3. Monitor the job execution
FOR replica-interval DO
(a) Prompt all remote SMs for job status
(b) Determine the number of healthy replicas (i.e., h)
(c) IF any replica is done THEN
i. Tell all remote SMs to terminate their replica
ii. Update job table
(d) ELSEIF (h > n) THEN
i. Select last replica from set to terminate
ii. Update job table
(e) ELSE
i. Pick next site from set to terminate
ii. Inform the remote SM to execute the job
iii. Update job table
ENDIF
ENDFOR

Fig. 2. Self-healing scheduling algorithm

218

J.H. Abawajy

3.4

Job and Task Placement

First, we determine an ideal number of jobs/tasks that can possible be sent to a
child scheduler as shown in Equation 2. Once the target number, Ttarget , of jobs
that can possibly be transferred to a child is determined, the algorithm then
considers the size of the RFC from the child as a hint to adjust the number of
jobs that will actually be transferred down one level to the child. Finally, if the
target size is smaller than the Treq from the child, then the parent will send the
lesser of Treq and the number of tasks queued. Otherwise, the set of tasks sent
to the child is the lesser of Ttarget and child partition-reach. Note that this is a
dynamic load distribution algorithm that changes the size of the batch at run
time, allocating large size to larger clusters while smaller clusters are allocated
small size.
With respect to which jobs are dispatched, ASP favors jobs that have their
replicas within the partition reach of the requesting schedulers. If there are
no such jobs, then jobs belonging to the left sibling of the requesting node is
searched. If this fails the jobs of the right sibling of the requesting node are
selected. This process continues until the exact number of jobs to be sent to
the requesting node is reached. The motivation for this job selection scheme is
that we minimize replica management overheads (e.g., the replica instantiation
latency) in case the original job fails. We also reduce the job transfer latency
as we have to only send control messages to the child scheduler if the replica is
already located there. Finally, it reduces the time that a child scheduler waits for
the jobs to arrive, which increases system utilization. After dispatching the jobs
to a child, the parent informs the backup scheduler about the assignment and
then updates the application status table (AST) to reﬂect the new assignment.
3.5

Failure Management

A fail-over strategy is used when a link or a node failure is detected. Link failure
is addressed by rerouting traﬃc from the failed part of the network to another
portion of the network. For non-root nodes, the child scheduler is informed to
communicate through its closest sibling of the parent. If the failed node is the
root, then we choose the closest functional and reachable sibling. When the
failed link is repaired, traﬃc is rerouted over the primary route. Similarly, when
a node is repaired, if the node used to be the primary scheduler, then the node
that replaced it is told to send to the recovered node and all former children are
also notiﬁed to update their parent id. The node then rejoins the system and
provides services immediately after recovery. If it was a backup node, recovery
is not necessary. A detailed discussion of the fail-over strategy is given in [3][4].
Job Replication. The replica creation and placement is to ensures that a job
and its constituent task are stored in a number of locations in the cluster tree.
The policy maintains some state information for failure and recovery detections
in Application Status Table (AST). Jobs are replicted over clusters while tasks
are replicated over processors. Speciﬁcally, when a job with fault-tolerance requirement arrives into the system, SHS undertakes the following steps: (1) create

Autonomic Job Scheduling Policy for Grid Computing

219

a replica of the job; (2) keep the replica and send the original job to a child that
is alive and reachable; and (3) update the application status table (AST) to
reﬂect where the job replicas are located. This process recursively follows down
the cluster tree until we reach the lowest level cluster scheduler (LCS) at which
point the replica placement process terminates.
Replica Monitoring. The SHS monitors applications at job-level (between
non-leaf nodes and their parents) and at task-level (between leaf nodes and
their parents). A monitoring message exchanged between a parent and a leaflevel node is called a report while that between non-leaf nodes is called a summary. A report message contains status information of a particular task running on a particular node and sent every REPORT-INTERVAL time units. In
contrast, the summary message contains a collection of many reports and sent
every SUMMARY-INTERVAL time periods such that REPORT-INTERVAL <
SUMMARY-INTERVAL.
When a processor completes execution of a task, the report message contains
a FINISH message. In this case, the receiving scheduler deletes the corresponding
replica and informs the backup scheduler to do the same. When the last replica
of a given job is deleted, the job is declared as successfully completed. In this
case, the cluster scheduler immediately sends a summary message that contains
the COMPLETED message to the parent scheduler, which deletes the copy of
the job and forward the same message to its parent. This process continues
recursively until all replicas of the job are deleted.
Failure Detection and Recovery. After each assignment, the children periodically inform their parents the health of the computations as discussed above.
If the parent does not receive any such message from a particular child in a given
amount of time, then the parent suspects that the child has failed. In this case,
it notes this fact in the AST and sends a request for report message to the child.
If a reply from the child has not been received within a speciﬁc time frame,
the child is declared dead. When a failure is detected, a recovery procedure is
initiated to handle the failure. The recovery mechanism restarts a replica of the
failed primary task as soon as possible.

4

Conclusion and Future Directions

In this paper, we presented a scalable framework that loosely couples the dynamic job scheduling approach with the hybrid (i.e., passive and active replications) approach to schedule jobs eﬃciently while at the same time providing
fault-tolerance. The main advantage of the proposed approach is that fail-soft
behaviour (i.e., graceful degradation) is achieved in a user-transparent manner.
Furthermore, being a dynamic algorithm estimations of execution or communication times are not required. An important characteristic of our algorithm is
that it makes use of some local knowledge like faulty/intact or busy/idle states
of nodes and about the execution location of jobs. Another important charac-

220

J.H. Abawajy

teristic of the proposed approach is that they are applicable for a wide variety
of target machines including Grid computing.
We are currently implementing and studying the performance of the proposed policy. In the proposed self-healing distributed framework, the latency of
detecting the errors might be aﬀected by message traﬃc in the communication
network. To address this problem, we intend to develop an on-line mechanism
to dynamically measure the round-trip time of the underlying network and calculate the error latency accordingly.
Acknowledgement. I appreciate the help of Maliha Omar without whose help
this paper would not have been realized.

References
1. J. H. Abawajy. Fault-tolerant scheduling policy for grid computing systems. In
Proceedings of IEEE 18th International Parallel & Distributed Processing Symposium (IPDPS04), pages 238–146, 2004.
2. Jemal H. Abawajy and Sivarama P. Dandamudi. Parallel job scheduling on multicluster computing systems. In Proceedings of IEEE International Conference on
Cluster Computing (CLUSTER’03), pages 11–21, 2003.
3. Jemal H. Abawajy and Sivarama P. Dandamudi. A reconﬁgurable multi-layered
grid scheduling infrastructure. In Hamid R. Arabnia and Youngsong Mun, editors,
Proceedings of the International Conference on Parallel and Distributed Processing
Techniques and Applications, PDPTA ’03, June 23 - 26, 2003, Las Vegas, Nevada,
USA, Volume 1, pages 138–144. CSREA Press, 2003.
4. Jemal H. Abawajy and Sivarama P. Dandamudi. Fault-tolerant grid resource management infrastructure. Journal of Neural, Parallel and Scientific Computations,
12:208–220, 2004.
5. Ian T. Foster, Carl Kesselman, and Steven Tuecke. The anatomy of the grid enabling scalable virtual organizations. CoRR, cs.AR/0103025, 2001.
6. Salim Hariri, Lizhi Xue, Huoping Chen, Ming Zhang, Sathija Pavuluri, and Soujanya Rao. Autonomia: An autonomic computing environment. In IEEE International Performance Computing and Communications Conference, 2003.
7. Paul Horn. Autonomic computing. The Economist print edition, September 19,
2002.
8. Gang HUANG, Tiancheng LIU, Hong MEI, Zizhan ZHENG, Zhao LIU, and Gang
FAN. Towards autonomic computing middleware via reﬂection. In COMPSAC,
2004.
9. Gail Kaiser, Phil Gross, Gaurav Kc, Janak Parekh, and Giuseppe Valetto. An
approach to autonomizing legacy system. In IBM Almaden Institute Symposium,
2002.
10. Jeﬀrey O. Kephart and David M.Chess. The vision of autonomic computing. IEEE
Computer, pages 41–50, 2003.
11. H. Liu and M. Parashar. A component based programming framework for autonomic applications. In In Proceedings of the International Conference on Autonomic Computing, 2004.

