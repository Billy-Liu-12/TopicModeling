Procedia Computer Science
Volume 80, 2016, Pages 366–375
ICCS 2016. The International Conference on Computational
Science

Leveraging Latent Sentiment Constraint in Probabilistic
Matrix Factorization for Cross-domain Sentiment
Classi cation
Jiguang Liang∗, Kai Zhang∗ , Xiaofei Zhou†, Yue Hu, Jianlong Tan, Shuo Bai
National Engineering Laboratory for Information Security Technologies,
Institute of Information Engineering, University of Chinese Academy of Sciences
{liangjiguang, zhangkai, zhouxiaofei, huyue, tanjianlong, baishuo}@iie.ac.cn

Abstract
Sentiment analysis is concerned with classifying a subjective text into positive or negative according
to the opinion expressed in it. The performance of traditional sentiment classi cation algorithms rely
heavily on manually labeled training data. However, not every domain has the labeled data because the
labeling work is time-consuming and expensive. In this paper, we propose a latent sentiment factorization (LSF) algorithm based on probabilistic matrix factorization technique for cross-domain sentiment
classi cation. LSF works in the setting where there are only labeled data in the source domain and
unlabeled data in the target domain. It bridges the gap between domains by exploiting the sentiment
correlations between domain-shared and domain-speci c words in a two-dimensional sentiment space.
Experimental results demonstrate the superiority of our method over the state-of-the-art approaches.
Keywords: latent sentiment space, cross-domain sentiment classi cation, probabilistic matrix factorization

1

Introduction

Sentiment analysis, aiming to automatically assign a sentiment label (e.g., positive or negative) to a
document, has become a popular topic for many research communities in recent years. Traditional
supervised classi cation algorithms [12, 1, 10] have been proved promising and widely used in sentiment classi cation. However, not every domain has the labeled data because the labeling work is
time-consuming and expensive. Moreover, it has been shown that sentiment classi cation is highly sensitive to the domain because users are used to use different words to express their sentiments in different
domains. A classi er trained using manually labeled opinion documents in one domain often performs
poorly in other domains. Thus, cross-domain sentiment classi cation technique is needed.
In general, the original domain with labeled training data is called the source domain, and the new
domain which is used for testing is called the target domain. The goal of cross-domain sentiment classication is to adapt the sentiment knowledge from a labeled source domain to an unlabeled target domain.
∗ These

authors contributed to the work equllly and should be regarded as co- rst authors;
author: Xiaofei Zhou, email: zhouxiaofei@iie.ac.cn

† Corresponding

366

Selection and peer-review under responsibility of the Scientiﬁc Programme Committee of ICCS 2016
c The Authors. Published by Elsevier B.V.

doi:10.1016/j.procs.2016.05.353

Leveraging Latent Sentiment . . .

Jiguang Liang et al.

The key idea to bridge the gap between domains is to discover and exploit the shared sentiment information across domains. Domain-independent features and domain-speci c features are often used as a
bridge to reduce the gap between domains for knowledge transfer. However, most existing works only
explicit the labeled documents in the source domain and do not utilize the sentiment relationships between words for prediction. In fact, the sentiment information of words is useful to sentiment modeling
and is easy to obtain from the existing resources.
In this paper, in order to bridge the gap between the source and target domain, and adopt sentiment
associations of words more effectively in cross-domain sentiment classi cation, we propose a latent sentiment factorization (LSF) algorithm based on probabilistic matrix factorization technique. Speci cally,
LSF rst maps the words and the documents in both domains into a uni ed two-dimensional space with
the help of domain-shared words. Then it utilizes the sentiment polarities of the labeled documents in
the source domain and prior sentiment information of words to constrain the latent space and further
constrain to the positive-neagtive space. The polarities of the documents in the target domain are learnt
simultaneously with the solving process of the model. Experimental results indicate that LSF is indeed
promising in obtaining better performance than the state-of-the-art methods.
The rest of the paper is organized as follows. At rst, we describe the problem we study and give
some de nitions. Subsequently, we introduce our proposed model. Further, we experimentally evaluate
the effectiveness of our proposed model. Finally, we review some related works and conclude our work.

2

Problem De nition and Preliminaries

Suppose we have a corpus C =< D W > where D = DS DT , W = WS WT , DS with size NS
and WS with size MS are referred to the document set and word set of the source domain, respectively.
Similarly, DT with size NT and WT with size MT are referred to the document and word set of the
target domain, respectively. Q
R2×NS is the sentiment polarity matrix of DS where, Q1i = 1
represents that the polarity of di in DS is positive, and Q2i = 1 indicates that di expresses negative
sentiment. The domain-shared word set with size MO is denoted by WO = WS WT . In this way, the
source and target domain-speci c word sets Ws and Wt have Ms = MS MO and Mt = MT MO
words, respectively. The documents in DS are represented as a word-document joint probability matrix
RMO ×NS and B
RMs ×NS . Then we
S
RMS ×NS . S is composed of two submatrices: A
A
have S =
. That is, A is the domain-shared word-document matrix of DS and B is the domainB
speci c word-document matrix. Similarly, DT word-document joint probability matrix is denoted as
C
T
RMT ×NT . T =
where C
RMO ×NT and D
RMt ×NT . Each element in S and T
D
denotes a tf-idf weight. Let O R2×MO , X R2×Ms and U R2×Mt be latent sentiment matrices
of WO , Ws and Wt , respectively. Y
R2×NS and V
R2×NT represents the sentiment distribution
of DS and DT in latent sentiment space. That is, all words and documents are learnt and represented by
two-dimension sentiment (positive and negative) distributions.
The idea of LSF is to derive high quality sentiment representations of all words and documents by
utilizing the probabilistic matrix factorization technique to factorize S and T using O X Y U and V .

3
3.1

Proposed Approach
Formulating Source and Target Domain Data

For the joint probability matrix S from the source domain, we de ne the conditional distribution as
367

Leveraging Latent Sentiment . . .

P (S O X Y

2
S)

Jiguang Liang et al.

= P (A O Y
MO NS

=

2
A )P (B

X Y

2
B)
(A)

2 Iij
A )]

[N (Aij OiT Yj

Ms N S

[N (Bij XiT Yj

(B)

2 Iij
B )]

(1)

i=1 j=1

i=1 j=1

2
) is the probability density function of the Gaussian distribution with mean and
where N (
(A)
(B)
2
variance , and Iij Iij is the indicator function that is equal to 1 if domain-shared/domain-speci c
word wi appears in source domain document dj and equal to 0 otherwise. The prior distributions over
the word and document feature vectors are assumed to zero-mean Gaussian:
2
O)

P (O

MO

=

N (Oi 0

2
O I)

(2)

N (Xi 0

2
X I)

(3)

i=1
2
X)

P (X

Ms

=
i=1
NS

2
Y

P (Y

)=

N (Yj 0

2
Y

I)

(4)

j=1

Then, through a simple Bayesian inference, the posterior distribution of O X and Y given S can be
modeled as
P (O X Y S

2
S

P (S O X Y
= P (A O Y

2
2
O
X
2
S )P (O

2
A )P (B

X

2
Y)
2
2
2
O )P (X X )P (Y
Y)
2
2
2
)P (O O
)P (X X
)P (Y
Y B

(5)
2
Y

)

Similarly, given the joint probability matrix T in the target domain and the zero-mean spherical
Gaussian priors that are placed on T O U and V , we can also obtain the posterior distribution
P (O U V T

2
T

P (T O U V
= P (C O V

2
2
O
U
2
T )P (O

2
C )P (D

U

2
V)
2
2
2
O )P (U U )P (V
V
2
2
V D
)P (O O
)P (U

)

(6)
2
U )P (V

2
V

)

In Eq.(5) and Eq.(6), O is shared across domains. In this way, O is used as the bridge of knowledge
transformation from the source domain to the target domain.

3.2

Modeling Words Sentiment Prior

Given a sentiment word wi , if we do not have any clue about its polarity, a natural guess is that wi
sentiment polarity is kept consistent with the general purpose sentiment lexicon which contains words
that are almost always positive or negative in any domain. Suppose we have a general purpose sentiment
lexicon L. We de ne a 2 MO matrix E and a MO 1 matrix I (E) : for each word wi in WO , if wi
(E)
(E)
is a sentiment word in L, we set Ii
= 1; otherwise, Ii
= 0; if wi is a positive sentiment word,
(F )
(G)
we set E1i = 1; if wi is a negative word, we set E2i = 1. We also de ne F , G, Ii and Ii which
(E)
have similar meanings with E and Ii for domain-speci c words in Ws and Wt . Based on the above
sentiment prior intuition, we obtain
P (O E

2
O)

MO

=

[N (Oi Ei
i=1

368

(E)

2
Ii
O I)]

(7)

Leveraging Latent Sentiment . . .

Jiguang Liang et al.

P (X F

Ms

2
X)

(F )

[N (Xi Fi

Ii
2
X I)]

[N (Ui Gi

Ii
2
U I)]

=

(8)

i=1

P (U G

2
U)

Mt

=

(G)

(9)

i=1

3.3

Modeling Words Sentiment Consistency

Intuitively, two frequently co-occurrence words in the same domain are more likely to have similar
sentiment than those of two randomly selected words. This phenomenon is called sentiment consistency
[8]. Inspired by this, we assume that for each domain, the polarities of two co-occurring words tend to
be similar and the possibility of similar is proportional to the number of co-occurrence frequency. To
formalize this intuition, we formulate the following equations on domain-speci c word vectors:
P (Xi

Xj

2
x)

2

=

[N (Xki

Xkj 0

[N (Uki

Ukj 0

(X)

2 Sij
x]

(10)

k=1

P (Ui

Uj

2
u)

2

=

(U )

2 Sij
u]

(11)

k=1

where S (X) RMs ×Ms and S (U ) RMt ×Mt are word-word sentiment consistency con dence matrices for the source and target domains. For each domain, con dence matric is constructed with (i j)th
entry
(·)
)Cos(wi wj )
(12)
Sij = N mi(wi wj ) + (1
where N mi( ) is the normalized mutual information function and Cos( ) is the cosine similarity function. wi is the vector of wi . Here, we use Word2Vec1 to train the word vector. Word2Vec can enable
words with similar semantic properties close to each other in the vector space. is employed to control
the contribution of each factor and in this paper we set = 0 5.

3.4

Modeling Source Domain Documents Labels

Y and Q are the predicted and real sentiment distribution matrices of DS , respectively. We except that
the predicted sentiment distribution is t to the real distribution. Then the Gaussian distribution of Y
given Q can be de ned as
P (Y Q

2
Y

NS

)=

N (Yj Qj

2
Y

I)

(13)

j=1

3.5

Full Objective Function

By incorporating all the above factors, we model the problem of cross-domain sentiment classi cation
using the graphical model described in Figure 1. Based on Figure 1, we de ne the posterior distribution

1 http://word2vec.googlecode.com/svn/trunk/word2vec.c

369

Leveraging Latent Sentiment . . .

Jiguang Liang et al.

Figure 1: Graphical Model for LSF.
as
P (O, X, Y, U, V |S, T, Ω)
2
2
2
2
2
2
∝ P (A|O, Y, σA
)P (X|F, σX
)
)P (C|O, V, σC
)P (D|U, V, σD
)P (O|E, σO
)P (B|X, Y, σB
Ms M s

P (Xi −

Xj |σx2 )

i=1 j=1

Mt Mt

P (Ui −

Uj |σu2 )P (Y

|Q, σY2

2
)P (U |G, σG
)P (V

|σV2

(14)

).

i=1 j=1

Maximizing the log-posterior P (O, X, Y, U, V |S, T, Ω) is equivalent to minimizing the following
sum-of-squared cost function:
min

O,X,Y,U,V

J (S, T, O, X, Y, U, V )

= I (A)
+ λC I (C)

(A − OT Y )

2
F

(C − OT V )

+ λO O − E
2
F
T

2
F

+ λY Y − Q

+ λx Tr(X T ζX) + λu Tr(U ξU ) + λV V
where σ = σA = σB , λC =
λx =

σ2
2
σx

σ2
2 .
σu
(U )

and λu =

+ I (B)

σ2
2 ,
σC

λD =

2
F

(B − X T Y )

+ λD I (D)

2
F
T

+ λX X − F

(D − U V )

2
F

2
F

+ λU U − G

2
F

(15)

2
F

σ2
2 ,
σD

λO =

σ2
2 ,
σO

λX =

σ2
2 ,
σX

λY =

σ2
2
σY

, λU =

σ2
2
σU

, λV =

is the Hadamard Product. Tr(·) denotes the trace of a matrix. ζ = X − S

σ2
2 ,
σV
(X)

and ξ = U − S
are the Laplacian matrices. X and U are diagonal matrices with i-th element
(X)
(U )
Ms
Mt
Sij .
Xii = j=1 Sij and Uii = j=1

3.6

Solution

Since J is not concave, it is hard to obtain the global solution by applying the nonlinear optimization
techniques. In this work, we adopt an alternative iterative optimization algorithm, which can converge
to a local optimal solution, to solve the problem in Eq.(15). In each round of iteration, we update O, X,
Y , U and V with the following updating rules.

X←X
370

Y B T + λX F
Y XT Y

T

+ λX X + λx (ζX)

(16)

Leveraging Latent Sentiment . . .

Jiguang Liang et al.

Y

U

Y

OA + XB + Y Q
OOT Y + XXT Y ) +

DV

V

O

DV D
T

U

V

O

UTV

T

+

+

UG

u(

U) +

Y

(18)
UU

C OC + D U D
OOT
V
+ DU U T V +
C

Y AT +
Y OT Y

T

+

CV

CT +

CV

OT V

T

(17)

Y

V

V

OE

+

(19)

(20)
OO

denotes element-wise square root, A = I (A) A, B = I (B) B,
where ·· is element-wise division,
(C)
(D)
C, D = I
D, OT Y = I (A) OT Y , XT Y = I (B) X T Y , OT V = I (C) OT V
C=I
(D)
T
U V.
and U T V = I
It is easy to verify that the updating rules do satisfy the above KKT condition. Furthermore, since
each element in A B C D and is nonnegative, so O X Y U and V are nonnegative during the
updating process. Until now, we prove the correctness of the updating rules. It can be proved that the
updating rules are guaranteed to converge. Since the proof process is similar to that in [4], to save space,
we omit the detailed proof of the convergence of the updating rules.

4

Experimental Analysis

4.1

Dataset Description

In this work, we use Amazon products reviews dataset collected by Blitzer et al. [2] for comparison
experiments. The reviews are about four product domains: books (B), dvds (D), electronics (E) and
kitchen (K). Detailed statistics of the datasets are summarized in Table 1. For each domain, 80% of
the data is randomly selected as the training set and the rest as the testing set. Following the literatures
B, E
B, K
B, B
D,
[11, 15], we construct 12 cross-domain sentiment classi cation tasks: D
E
D, K
D, B
E, D
E, K
E, B
K, D
K, E
K where the word before the arrow
represents the source domain and the word after the arrow represents the target domain. To achieve
better results, unlabeled data is used to learn the con dence matrices for the two domains. Besides, the
general purpose sentiment lexicon is generated based on MPAQ2 subjective lexicon.

4.2

Benchmark Approaches

We compare our method with ve baseline algorithms, including three state-of-the-art approaches: SCL
[2], SFA [11] and TCT [15]. The other two baselines are NoTransf and Upperbound where NoTransf
is a classi er trained directly with the source domain training data and UpperBound is an in-domain
classi er trained with labeled data from the target domain. For NoTransf, Upperbound, SCL and SFA,
2 http://mpqa.cs.pitt.edu/

371

Leveraging Latent Sentiment . . .

Table 1: Amazon review statistics.
Domain
#Reviews #Pos. #Neg.
Books
2,000
1,000 1,000
DVD
2,000
1,000 1,000
Electronics
2,000
1,000 1,000
Kitchen
2,000
1,000 1,000

Jiguang Liang et al.

#Unlab.
4,465
3,586
5,681
5,945

we use libsvm3 as the sentiment classi er. We use the same parameter settings as in the original papers.
For LSF, with the optimization result of V , the sentiment polarity of di in the target domain can be
predicted with
(21)
arg max V1i V2i
The evaluation of cross-domain sentiment classi cation methods is conducted on the test set in the
target domain without labeled training data in the same domain. We report the average results of 5
random times.

Figure 2: Comparison results for cross-domain sentiment classi cation on the Amazon product benchmark of 4 domains.

Figure 3: Results for for cross-domain sentiment classi cation under varying values of
x.

4.3
4.3.1

C

O

Q

and

Results and Discussion
Comparison

Figure 2 presents the performance comparison between our method and other approaches on the four
domain datasets. In the gure, each group of bars represents a cross-domain sentiment classi cation
task. Each bar in speci c color represents a speci c approach. The horizontal lines are accuracies of
3 http://www.csie.ntu.edu.tw/

372

cjlin/libsvm/

Leveraging Latent Sentiment . . .

Jiguang Liang et al.

Figure 4: Number of iterations versus the objective function value.
Upperbound. In this gure, parameters C and D are set to 0 1 and other parameters are set to 0 01.
From the gure, it can be easily observed that the four domains can be roughly classi ed into two
groups: B and D domains are similar to each other, as are E and K, but the two groups are different
from each other. Performing a cross-domain classi er from B domain to D domain is much easier than
performing it from E or K domain to D domain. Similar observations have also been found in [11] and
[15].
As illustrated, our method achieves better performance than other approaches in terms of accuracy
in all the 12 tasks. It demonstrates that our proposed LSF can not only utilize the co-occurrence relationship and sentiment relationship between domain-shared and domain-speci c words to reduce the gap
between domains from the semantic and sentiment perspectives, but also use co-occurrence relationship
among domain-speci ed words to constrain each other s sentiment polarity. Besides, it is not surprising
to nd that the state-of-the-art methods SCL, SFA and TCT obtain signi cant improvement compared
to NoTransf. TCT performs better than SCL and SFA while SFA performs slight better than SCL. However, the performances of SCL and SFA highly rely on the auxiliary tasks that selecting pivot features
and non-pivot features, and domain-independent features and domain-speci c features, respectively. In
TCT, the sentiment distributions of topics are learnt from the source domain which may make TCT has
relatively poor adaptation to the target domain.
4.3.2

Parameter Sensitivity Analysis

In this section, we analyze how the changes of the parameters can affect the cross-domain sentiment
classi cation accuracy. Figure 3 presents the performance change of our method in terms of C , O ,
Y and x . The impact of D , X and U , u generally shares the same trend as the impact of C ,
O and x , respectively. As mentioned above, the four domains can be roughly divided into two classes
([B D], [E K]). Hence we only illustrate the results of the four parameters ranging from 10−6 to 10
here due for two representative tasks (B
D and B
E) to the space limitation. Here, B
D
represents inter-class cross-domain sentiment classi cation, meanwhile, B
E represents intra-class
cross-domain sentiment classi cation. As can be seen, in order to achieve satisfactory performance, it
is better to set C = 0 1, O = 1, Y = 10 and x = 0 01 for the inter-class classi cation task, and set
C = 1, O = 0 1, Y = 1 and x = 0 001 for the intra-class classi cation task.
4.3.3

Convergence Analysis

Here, we also empirically check the convergence property of the proposed iterative algorithm. Figure
4 shows the convergence curves of LSF on the four representative tasks. From the gure, we can see
that the value of objective function decreases along with the iterating process, which agrees with the
theoretic analysis.
373

Leveraging Latent Sentiment . . .

5

Jiguang Liang et al.

Related Work

Cross-domain sentiment classi cation means to perform sentiment classi cation of opinion documents
in multiple domains. Here, we brie y present a few recent advances of cross-domain sentiment classi cation.
Blitzer et al. (2006) proposed the structural correspondence learning (SCL) algorithm to exploit
domain adaptation techniques for sentiment classi cation. SCL uses unlabeled data and frequentlyoccurring pivot features from both source and target domains to nd correspondences between pivot
and non-pivot features. However, the performance of SCL highly relies on the heuristically selected
pivot features which limits the transfer ability of SCL for cross-domain sentiment classi cation. Pan
et al. (2010) proposed a method called spectral feature alignment (SFA) that is similar to SCL at the
high level. With the help of domain independent words as the bridge, SFA bridges the gap between the
domains by aligning domain-speci c words from different domains into uni ed clusters. Those domain
independent words are like pivot words in SCL and can be selected similarly. However, like SCL, any
missing key domain-independent word may lead to poor performance. Along the same line, He et al.
[6] used joint topic modeling to identify opinion topics which are similar to clusters in SFA from both
domains to bridge them. The resulting topics which cover both domains are used as additional features
to augment the original set of features for classi cation [9].
More recently, Zhou et al. (2015) proposed a topical correspondence transfer (TCT) algorithm to
conduct cross-domain sentiment classi cation. Unlike SCL and SFA exploiting relations among words,
TCT utilizes the topic-level relationships. Topics in TCT can be depicted by two-dimensional sentiment
distribution. TCT exploits the sentiment labels of the source domain documents to label the latent
topics sentiment polarities. Then the topics are used to predict the target domain documents sentiment
polarities. In this way, the topical correspondences behind the shared topics is used as a bridge to
reduce the gap between domains for knowledge transfer. However, to obtain a better performance, it is
better to set the number of topic more than 300 for TCT which may leads to relatively large amount of
computation. In fact, rather than map topic into two-dimensional sentiment space, it would be better to
map words into sentiment space. The polarity of word is intuitive and there are many external sentiment
resources are available to used.
Besides, there are many other studies that proposed different approaches, e.g. graph ranking [13],
sentiment sensitive thesaurus [3], deep learning [5], POS-based ensemble model [14] and active learning
[7], to address cross-domain sentiment classi cation from different perspectives.
Nevertheless, research on cross-domain sentiment classi cation is still in its infancy and there are
still some unsolved problems in this eld.

6

Conclusion

In this paper, we studied how to exploit the sentiment associations between domain-shared and domainspeci c words for cross-domain sentiment classi cation. Along this line, we proposed a probabilistic
matrix factorization based method called LSF. To capture the features in the sentiment level for classi cation, LSF maps words and documents in the source and target domains into a two-dimensional
sentiment (positive and negative) space. The polarities of the target domain documents are learnt with
the solution of the optimization problem. Experiments conducted on Amazon datasets with four types
products indicate that LSF obtains better performance than several baselines including SCL, SFA, and
achieves an accuracy which is competitive with TCT for cross-domain sentiment classi cation.
Acknowledgments. This work was supported by Strategic Priority Research Program of Chinese
Academy of Sciences (XDA06030200).
374

Leveraging Latent Sentiment . . .

Jiguang Liang et al.

References
[1] David M. Blei, Andrew Y. Ng, and Michael I. Jordan. Latent Dirichlet Allocation. Journal of Machine
Learning Research, 3:993–1022, 2003.
[2] John Blitzer, Ryan Mcdonald, and Fernando Pereira. Domain adaptation with structural correspondence
learning. In 2006 Conference on Empirical Methods in Natural Language Processing (EMNLP ), pages 120–
128, 2006.
[3] Danushka Bollegala, David Weir, and John Carroll. Using multiple sources to construct a sentiment sensitive
thesaurus for cross-domain sentiment classi cation. In Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies (ACL-HLT), pages 132–141, Portland,
Oregon, USA, June 2011. Association for Computational Linguistics.
[4] Chris Ding, Tao Li, and Michael I. Jordan. Nonnegative matrix factorization for combinatorial optimization:
Spectral clustering, graph matching, and clique nding. In Eighth IEEE International Conference on Data
Mining (ICDM), pages 183–192, 2008.
[5] Xavier Glorot, Antoine Bordes, and Yoshua Bengio. Domain adaptation for large-scale sentiment classication: A deep learning approach. In Lise Getoor and Tobias Scheffer, editors, Proceedings of the 28th
International Conference on Machine Learning (ICML), pages 513–520, New York, NY, USA, 2011. ACM.
[6] Yulan He, Chenghua Lin, and Harith Alani. Automatically extracting polarity-bearing topics for cross-domain
sentiment classi cation. In Proceedings of the 49th Annual Meeting of the Association for Computational
Linguistics: Human Language Technologies - Volume 1, HLT 11, pages 123–131, Stroudsburg, PA, USA,
2011. Association for Computational Linguistics.
[7] Shoushan Li, Yunxia Xue, Zhongqing Wang, and Guodong Zhou. Active learning for cross-domain sentiment
classi cation. In Twenty-Third International Joint Conference on Arti cial Intelligence(IJCAI), pages 2127–
2133, 2013.
[8] Jiguang Liang, Xiaofei Zhou, Yue Hu, Li Guo, and Shuo Bai. Conr: A novel method for sentiment word
identi cation. In Proceedings of the 23rd ACM International Conference on Conference on Information and
Knowledge Management (CIKM), pages 1943–1946, 2014.
[9] Bing Liu. Sentiment analysis and opinion mining, 2012.
[10] Tomas Mikolov, Kai Chen, Greg Corrado, and Jeffrey Dean. Ef cient estimation of word representations in
vector space. CoRR, abs/1301.3781, 2013.
[11] Sinno Jialin Pan, Xiaochuan Ni, Jian tao Sun, Qiang Yang, and Zheng Chen. Cross-domain sentiment classication via spectral feature alignment. In 19th International World Wide Web Conference (WWW), 2010.
[12] Bo Pang, Lillian Lee, and Shivakumar Vaithyanathan. Thumbs up?: sentiment classi cation using machine
learning techniques. pages 79–86, 2002.
[13] Qiong Wu, Songbo Tan, and Xueqi Cheng. Graph ranking for sentiment transfer. In Proceedings of the
ACL-IJCNLP 2009 Conference Short Papers, ACLShort 09, pages 317–320, Stroudsburg, PA, USA, 2009.
Association for Computational Linguistics.
[14] Rui Xia and Chengqing Zong. A pos-based ensemble model for cross-domain sentiment classi cation. In
Proceedings of 5th International Joint Conference on Natural Language Processing, pages 614–622, Chiang
Mai, Thailand, November 2011. Asian Federation of Natural Language Processing.
[15] Guangyou Zhou, Yin Zhou, Xiyue Guo, Xinhui Tu, and Tingting He. Cross-domain sentiment classi cation
via topical correspondence transfer. Neurocomputing, 159:298–305, 2015.

375

