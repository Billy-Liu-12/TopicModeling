On Polynomial and Polynomial Matrix Interpolation
Petr Hušek and Renata Pytelková
Department of Control Engineering,
Faculty of Electrical Engineering
Czech Technical University in Prague
Technická 2, 166 27, Praha 6, Czech Republic
{husek,dvorako}@fel.cvut.cz

Abstract. The classical algorithms for computations with polynomials and
polynomial matrices use elementary operations with their coefficients. The
relative accuracy of such algorithms is relatively small and for polynomials of
higher order and polynomial matrices of higher dimension the executing time
grows very quickly. Another possibility is to use symbolic manipulation
package but even this is applicable only for moderate problems. This paper
improves a new method based on polynomial interpolation. Its principle is as
follows [1]: firstly a sufficient number of interpolation points is chosen, then
the interpolated object is evaluated in these points and finally it is recovered
from both series of values. The choice of interpolation points is crucial to have
a well-conditioned task. Typically, a random choice of real points leads to a
badly conditioning for higher order of interpolated polynomial. However, a set
of complex points regularly distributed on the unit circle in the complex plane
gives a perfectly conditioned task. Moreover very efficient algorithm of fast
Fourier transform can be used to recover the resulted polynomial or polynomial
matrix. The efficiency is demonstrated on determination of inverse to
polynomial matrix.

1 Introduction
Solution of many technical problems leads to computations with polynomials or
polynomial matrices. The classical algorithms, which are based on manipulations with
coefficients, are distinguished for small relative accuracy and low efficiency. The
algorithms based on interpolation-evaluation techniques represent a computationally
efficient way to deal with such computations. The interpolation theory for polynomial
case is very old. Nevertheless even the problem is very well studied all the algorithms
meet the problem of badly conditioned task and could be used only for polynomials of
lower degree (up to 30). The generalization to the matrix case appeared only recently.
The most general way, which includes all other interpolation scheme, was introduced
by Antsaklis [1]. However, the problem of badly conditioned task remained unsolved.
In this paper the conditioned number of corresponding task is improved by
choosing a special set of interpolation points in both, polynomial and polynomial
matrix case. Moreover, the fast Fourier transform (FFT) algorithm is used for

evaluation of the interpolated object in the prescribed set of points and its recovery
from these values. The proposed algorithm is very efficient and numerically reliable.

2 Polynomial Case
Firstly let us remind, that the term interpolation can be used to denote many different
mathematical tools. The technique, which will be here understood as interpolation, is
described below. The basic idea of all interpolation methods can be summarized in
the three following steps:
1. A sufficient number of interpolation points is chosen.
2. The interpolated object is evaluated in this set of points.
3. The interpolated object is recovered from both series of values.
The simplest example of interpolation technique is univariate polynomial
interpolation. The corresponding procedure can be stated by the following theorem.
THEOREM 1 (Univariate polynomial interpolation [3])
   

Given K distinct generally complex scalars sj, =
, and K corresponding
complex values bj, there exists a unique polynomial r(s) of degree  =  −  for
which


 

	
(1)♦
    
=   =   .


That is, an nth degree polynomial r(s) can be uniquely represented by the
   


=  + interpolation points (or pairs) (sj, bj), =
 . To see this, the nth
#

"

-

,

+ *

degree polynomial   =  ! +   +  +    can be written as ' & = / )

9 :
9
8
where / = [1 4 2 1 3 2 5 2 1 0 ] is × ; + 676 row vector of the coefficients. The
equations can then be written as

ACB



=@ 

F


H

H

F I
K

L

D I
I −

J

F D
K

D I
F D −


 E
=[




I G

J
G

E
D

]= ?

.

&
>

)

.
)

& % (
$

== +<

(2)

The matrix V isP called
Vandermonde matrix and it is non-singular if and only if the
N
O Q O
M
K scalars sj, =
are distinct. In that case the equation (2) has a unique
solution r; that is, there exists a unique polynomial r(s) of degree n which satisfies
(1). This proves the theorem 1.
From numerical point of view the solution of equation (2) depends on the condition
number of matrix V. This problem is studied in the next section.

3 Condition Number of Vandermonde Matrix
It is well-known fact that the Vandermonde matrix (2) for vector of real numbers is
very badly conditioned. Typically, for interpolation points regularly distributed in the

interval [-1,1] the critical MATLAB condition number 1016 is achieved for
=  .
An immense improvement of the condition number of the Vandermonde matrix can
be surprisingly achieved using complex interpolation points even if it seems to be
contraproductive.
Let us choose the vector of interpolation points as
(1)

[       ] = 	  ω  ω    ω  −
  

[

]

where ω denotes a primitive K-th root of 1, that is



 
 

ω =  ω ≠ 
=   − .

(4)

Such a set of interpolation points is called Fourier points. The corresponding
Vandermonde matrix Ω (referred to as Fourier matrix) has the following form:




=
$










%



ω
   −    
(5)

ω
.
"

   
ω − − 
6
3 2
5
for
≤ ≤ 1 where ω = 4 ω is a


−

ω
ω


%
ω
ω
$
$
%#
 
  
!
ω − ω −
./
0
-. − ' , + *
Observe that since ∑ = ( ωω = − )
; each column appearing in the matrix Ω is
primitive root of 1, the Euclidean norm of
:;
<
9
C >@?BA
equal to 7 . Moreover since ∑ = D ω =
= ≤ ≤ 8 , the matrix Ω is unitary
and
(6)
HJILKNM G O F = E .
&

It means that the Fourier matrix Ω is perfectly conditioned and its inverse can be
determined as its complex conjugate
(7)
R −Q R P
=
.
The vector of coefficients of interpolated polynomial can be then obtained as the
solution of (2) as
(8)
\ = Z [ Y X S = Z ωW T U V S .

[ ]

Another important fact is that in order to evaluate a univariate polynomial in the
Fourier points, that corresponds to step 1 of the general interpolation-evaluation
algorithm mentioned above, the fast Fourier transform (FFT) algorithm could be
applied. The algorithm is well studied, very efficient and numerically reliable.
Moreover, as the Fourier matrix is unitary, the inverse FFT algorithm can be used to

recover the polynomial coefficients from its values (step 3 of that algorithm or
equivalently the product in (8)).
The use of the described algorithm will be shown on an example in section 6 on
determination of determinant of polynomial matrix.
In the following section a generalization of the algorithm described above to
polynomial matrix case is introduced.

4 Polynomial Matrix Case
The result obtained above can be generalized to polynomial matrix case in many
different ways. Nevertheless all of them can be seen as special cases of the basic
polynomial matrix interpolation procedure introduced by the following theorem.
      	
         
! "#
$#% #
  

=
=
Let
where
are non-negative
integers; let ( ' ≠ & and bj denote (m×1) and (p×1) complex vectors respectively and
sj complex scalars.
THEOREM 2 (Polynomial matrix interpolation [3]), *
+ +
) and non-negative integers
Given interpolation (points) triplets (sj, aj, bj), =
3 5
4
7 3 5
4
/ 102 0
. with = ∑ + . such that the ∑ + . 6 × matrix
di , =
? < : 9? 7= ; > ;0@ 0? 7= < > < 8
(9)
=
6
6
B
C
has full rank, there exists a unique D × A polynomial matrix Q(s), with ith column
F HGI G
degree equal to di, =
E for which
U QL M PT M
(10)
O R J
K
=S M N = N N .
Q(s) can be written as

Z WY X

=

Z\[ W Y V

(11)

_
_ a
where Q b × ∑ c + ` ]^] contains the coefficients of the polynomial entries. Q must
satisfy
fhg
(12)
d =e d
o m
kn l p n m
j j
=
where
i . Since SK is non-singular, Q and therefore Q(s) are uniquely
determined.
For s = r = q SK is turned to Vandermonde matrix. In the polynomial matrix case,
we shall call it block Vandermonde matrix because of its structure, that will be shown
later.
Both theorems expand step 3 in the general description of interpolation methods
introduced before. They show, that the resulted polynomial or polynomial matrix can
be found as a solution of the matrix equation (6) or (8) respectively. It means, that the
step 3 is the same for every problem solvable by interpolation techniques. The whole
difference consists in step 2 which of course can be performed by algorithms working

with constant matrices. These algorithms are usually very efficient and therefore
guarantee good numerical properties of results computed in a short time.

5 Block Vandermonde matrix
Let us focus on the block Vandermonde matrix SK appearing in (12) defined by (9).
According to theorem 2 the choice of interpolation triplets (sj, aj, bj) is arbitrary. The
only condition is the full rank of matrix SK. The postmultiplication of interpolated
matrix by the vectors aj makes it possible to vary interpolation triplets not only by
points sj but also by the vectors aj. This is compensated by a higher number of points
than they are necessary for element by element interpolation procedure. In section 3 it
was shown that Vandermonde matrix has some appropriate properties if evaluated in
the Fourier points. Let us try to choose such triplets (sj, aj, bj) to maintain this
structure in the block Vandermonde matrix.
 =   + ,  =      , 
 =     	 , d = 0 , q = 1,  , r ,  =  +    :
Let
∑
0

!%
# % 
"
  =   ,   =   for & ∈ ∑
+$ ∑ " +$ ;
" 
" 

'%
#
(
+.
/
+
+
/
+
&
) = −∑ " −$ + , 1 0 = , , , , , , *
" '
−

let

=

=

(13)

−

=

where 1 is on the ith position.
This set of interpolation triplets requires only r different interpolation points sq,
which is the lowest possible number and it corresponds to the column-wise
interpolation procedure. The block Vandermonde matrix SK has the following form:

53
where each

D @A? =BAC A>



=




4 8 6
6 4 7 <=
; : :
6 9 6

6
6

;

4 2








(14)

is the Vandermonde matrix

N

O G


H M
= J

H MF E


N

HL I
J K
H LF E I

I

N



H FE
J  .
H F F E E 


(15)

From (14) it immediately follows that the inverse to SK appearing in solution of
(12) can be determined as





−


 −
 
= 	

 







−











	 
  .
 − 
 

(16)

If the interpolation points sj are chosen as the Fourier
*
 
 

0 ) $
" #&%' ! / ( + 
 
 =
, =  .   where
  =  
and -− =
3

3
3
A
9 =
?
3


3

ω
6
ω
?
8;
ω

3

ones then

3

6

ω
5
ω
?
48;
ω
<

@>

@

@


8;
ω 
48; 2
ω 
= 
8 ;8 ; 
ω 

1
: 7 + .

(17)

Again the fast Fourier transform algorithm can be used for evaluation of Q(s) in
these points and its recovery from them.

6 Example
Let us use the described procedure to determine inverse to polynomial matrix A(s),
J G
K
K +F 
K +
L IK H  K J
G
F K J E 
=
+B  .
C K
K J +C K +C
 D
+ 

The inverse to polynomial matrix A(s) can be determined as
e ^ c _ − ` XZY$[]\O^ e W c TUTOV − ` PQSR N e N c MOM b N a M − ` d N a M
=
=
where adj stands for adjoint to a matrix.
Firstly let us compute the determinant r(s) using procedure described in section 2.
The first step consists in estimation of degree n of r(s). It is important to note that if
the estimated degree is higher than the real one, the coefficients corresponding to high
powers will be determined as zero because due to theorem 1 there is only one
polynomial of nth degree determined by g + f points (in fact it should be “of nth or
less degree”). Obviously, the estimated degree cannot be less than the real one. The
simplest choice is k = ∑hi = l j i , where ci are the column degrees of A(s) (the highest
degrees appearing in the column).
r
p
Here n = m and the Fourier matrix Ω is of the seventh ( = q + = o ) order. Now
vu
w
we have to evaluate matrix A(s) in the interpolation points t ω t x t ω s where ω is
seventh root of 1. This step is performed by FFT algorithm. Next the vector of values

  



 

 



	 








 



=
ω 
 

ω  is computed using
of the determinants
standard procedure for computing determinant of a constant matrix. The recovery of
all the coefficients of r(s) (solution of (8)) is performed via inverse FFT applied on
the vector b.

 
 
 

 
 

=−
−
−
−
−
− −  . Now let us
The algorithm gives !
, ) * (
#
+
#
*
"" .
= $%'&
determine the polynomial matrix
As in the previous procedure firstly the column degrees di of Q(s) have to be
0 1
1
5
3 4 2 .
3 2 6 2
= - − / =
estimated. The simplest choice is
where / = 798;: / . In our
B C
?
> @
= = A = = = < . Let us choose the interpolation points according to (13) as the
case
B C
>
> @
G
+ = D scalar complex points H I = = = = D ,
Fourier points. We need F = 798E:
M R
P K Q
K

R O J

=

M L
K

ωK

ω
K

Q

N J

interpolation triplets is

where ω is the fifth root of 1. The total number of
U

Y

X

= ∑V = P

V

LTS

+W =

. The procedure continues as follows.

The polynomial matrix A(s) is evaluated in the points
a

\ R I [

]E^'_ \ `

R I
K

Z

L

=

\ R I [[

K Q

K

S

. Next the values

L

Z

S

K
=
= K Q K . Finally, the
of Q(s) are computed in those points:
solution of (12) can be determined by applying inverse FFT algorithm on each entry
a \ R I [
of
.
The algorithm gives

a

\ R [

−

=



b R N

−
R N

d R g

−

b

g

− R
b
+
R

e R f

−R
g
b
+

−
f
R

f

d R

−

S
R

g
b R

−R

N

−

g

+

b R f

+R

b R g

S R

f

−

+
b
+

c R f

b R g

d

+
L
R
+
b
b
− R −

−

b R N

−R

+

+R
g

b R

g

+R

−
f

b


c 
− 
L

+


−

c R f

7 Conclusion
In the paper the general concept of algorithms on polynomials and polynomial
matrices based on interpolation-evaluation methods was presented. The algorithms
are very efficient and numerically reliable because of using the FFT algorithm, very
well studied algorithms for constant matrices and perfectly conditioned task. The
method was used for computing inverse to polynomial matrix. The experiments reveal
that the algorithm is able to deal with polynomial matrices of high dimension
with
l ikj
ikj
× h
elements of high degrees. For example, determinant of a polynomial matrix
with elements of degrees 30 is computed in 2 seconds1. Adjoint to the same matrix is
determined in 10 seconds. The only drawback of interpolation-evaluation methods
consists in big storage capacity of a computer needed to store all the values of
interpolated object in interpolation points for some algorithms. For example, to
determine the determinant above one need to store 810900 complex values (even if
the determinant is of degree 401 only). However, to determine the adjoint above the

1

All the computations was done on Pentium II, 64MB, 120MHz.

same number of values has to be stored as the total number of coefficients of the
adjoint.

Acknowledgements
This work has been supported by the research program No. J04/98:212300013
”Decision Making and Control for Manufacturing” of the Czech Technical University
in Prague (sponsored by the Ministry of Education of the Czech Republic).

References
1. Antsaklis, P.J. and Gao, Z.: Polynomial and Rational Matrix Interpolation: Theory and
Control Applications. International Journal of Control. Vol. 58, No. 2, pp.349-404 (1993)
2. Bini, D. and Pan, V.: Polynomial and Matrix Computations, Vol. 1, Fundamental
Algorithms.


	Birkhäuser,
  !"Boston
$#&%')(+(1994)
*-,/.01)231 ,54768.9254;:
1<:=">@?.;.9A15BC*-DAE8GFHD-IJI94K*4;:L,/EM=:ON1 ,5P
Linear Input-Output Submodels: Polynomial Approach, in: Proc. of 11th IFAC Workshop
Control Applications of Optimization (CAO 2000), St. Petersburg, Russia (2000)
4. Hušek, P. and Štecha, J.: Rational Interpolation. In: Proceedings of the 1st Europoly
Workshop - Polynomial Systems Theory & Applications. Glasgow. Europoly-the European
Network of Excellence no. CP97-7010 (1999) 121-126

