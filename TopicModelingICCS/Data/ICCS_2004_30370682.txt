Adaptive Transfer Functions in
Radial Basis Function (RBF) Networks
Günther A. Hoffmann
Humboldt University Berlin, Department of Computer Science,
Rechnerorganisation und Kommunikation ROK, 10099 Berlin, Germany
gunho@informatik.hu-berlin.de

Abstract. The quality of Radial Basis Functions (RBF) and other nonlinear
learning networks such as Multi Layer Perceptrons (MLP) depend significantly
on issues in architecture, learning algorithms, initialisation heuristics and
regularization techniques. Little attention has been given to the effect of
mixture transfer functions in RBF networks on model quality and efficiency of
parameter optimisation. We propose Universal Basis Functions (UBF) with
flexible activation functions which are parameterised to change their shape
smoothly from one functional form to another. This way they can cover
bounded and unbounded subspaces depending on data distribution. We define
UBF and apply them to a number of classification and function approximation
tasks. We find that the UBF approach outperforms traditional RBF with the
Hermite data set, a noisy Fourier series and a non φ-separable classification
problem, however it does not improve statistically significant on the MackeyGlass chaotic time series. The paper concludes with comments and issues for
future research.

1 Introduction and Objectives
The quality of Radial Basis Functions (RBF) and other nonlinear learning networks
such as Multi Layer Perceptrons (MLP) depends significantly on architecture, learning algorithms, initialisation heuristics and regularization techniques. Little attention
has been given to the effect of flexible transfer functions in RBF networks on model
quality and efficiency of parameter optimisation. Even though Gaussian and sigmoidal transfer functions prevail in literature, there is little a priori reason why models
solely based on these transfer function will provide optimal model quality. In this
paper we investigate the effects of adaptive transfer functions, which replace the
Gaussian transfer functions in RBF.
The rest of the paper is organized as follows: Before we describe the universal
basis function (UBF) approach in more detail in chapter three we review previous
attempts and the state of the art of universal transfer functions in chapter two. RBF
have been described and reviewed extensively in literature [1][6][13]. We will not
revisit this topic. We will briefly comment on the problem of parameter optimisation
in chapter four. We will then investigate the effect of different types of transfer func-

M. Bubak et al. (Eds.): ICCS 2004, LNCS 3037, pp. 682–686, 2004.
© Springer-Verlag Berlin Heidelberg 2004

Adaptive Transfer Functions in Radial Basis Function (RBF) Networks

683

tions on selected problems such as the Hermite data set, the chaotic Mackey-Glass
time series, a noisy Fourier series and a classification problem in chapter five. In
chapter six we discuss our results and outlay future work.

2 Related Work
A straight forward integration of MLP and RBF was carried out in [3]. In [4] and [5]
an extensive overview of transfer functions is given. Little practical expertise has
been published. To the best of our knowledge a systematic comparison of transfer
functions is yet missing.

3 Universal Basis Functions: RBF with Mixed Transfer Functions
We propose flexible kernels as in (2) to replace the standard Gaussian kernels (1) in
RBF. We call this approach universal basis functions (UBF). These novel kernels can
be parameterised to change their shape smoothly from one functional form to another.
The Gaussian transfer function is a special case of UBF.

G (r ) = e − r

2

/ 2σ 2

(1)

The kernels we propose consist of a mixture of two activation functions. To combine
different types of activation functions in one kernel, additive and multiplicative mixtures have been proposed in [3][5]. We propose a mixture of activation functions by
smoothly morphing one activation function into the other as in (2). We combine
Gaussian (4), sigmoid (5) and multiquadratic (6) activation functions.

G (x; r, σ, ω′) = ω′Φ1 (x; r, σ ) + (1 − ω′ )Φ 2 (x; r, σ )

(2)

1 
ω′ = tanh  ω  ; ω ∈ (− ∞, ∞ )
2 

(3)

with

and

Φ ∈ Φ1′ ..3
Φ1′ = e − r

2

/ 2σ 2

(

Φ ′2 = tanh r / σ 2
Φ′3 = r 2 + σ 2

(4)

)

(5)
(6)

684

G.A. Hoffmann

If we do have information about the data distribution we can incorporate this knowledge by giving a bias towards a specific activation function by initialising the slider
value ω′ accordingly. However, as frequently is the case knowledge about the data
distribution is either difficult to obtain or not available at all. In this case we start with
an educated guess. If no further information is available to justify a bias towards a
particular activation function we set ω′ = 0.5.

4 Parameter Optimisation

Θ = {t ,σ , ω ′} with kernel
positions t, kernel widths σ and the UBF slider value ω′ which controls the shape

In UBF we have to optimise the joint set of parameters

of the kernels. We employ a global stochastic optimisation procedure, the derandomised Evolution Strategy with full covariance matrix adaptation (CMA-ES). This procedure is a stochastic optimisation scheme which has been described in detail in [7]
and [14].

5 Results
Figure 1 depicts the mean square error of the classification problem as a function of
the UBF slider value ω′ . For a Gaussian RBF kernel network we get a straight line
because the kernel shape does not change with a varying ω′ . For a mixture UBF
kernel approach we clearly get an optimal kernel shape minimizing the models classification error at ω′ ≈ 0.2 implying a mildly Gaussian bias.

0.05

0.10

RMSE

0.15

Gaussian kernel only
UBF Gaussian / sigmoidal mixture

-1

0

1

2

UBF Slider

Fig. 1. The categorization error in terms of root-mean-square-errors (RMSE) is shown as the
straight line for the RBF model. The UBF mixture model shows different errors as we
smoothly change the activation function from Gaussian to sigmoidal. A UBF slider value larger
than one, signals a strong Gaussian influence, a value below zero signals sigmoid influence.

Adaptive Transfer Functions in Radial Basis Function (RBF) Networks

685

6 Discussion and Future Work
In this paper we have proposed Universal Basis Functions (UBF) as an extension to
Radial Basis Functions (RBF). We investigated the models generalisation capabilities
in terms of means square errors. We compared UBF and RBF. We find that classification of a non trivial dichotomy and function approximation of the Hermite data set
and a noisy Fourier series the UBF approach significantly outperforms the RBF approach. We also find that for the Mackey-Glass data series the UBF approach does
not significantly outperform the traditional RBF approach. UBF offer the possibility
to adapt the transfer function locally to the search space. RBF are a special case of
UBF. This is achieved by evolving a transfer function which is best suited for covering the local parameter space. The activation function Φ can thereby take on any
form ranging smoothly from a Φ 1 to a Φ 2 . For a Gaussian Φ 1 = Φ 2 we get the
traditional RBF network. If the data distribution is known the optimal activation
function might be chosen based on analytical and theoretical consideration. However,
in high dimensional space such knowledge about the data distribution might be difficult to obtain or not available at all. In these cases UBF might be a powerful extension of RBF. There are a number of open issues which we will address in future research. Further work is in progress to analyse the theoretical limits of UBF. Furthermore we will investigate how to reduce the complexity of the parameter space. Also
currently we work with only one mixture value ω′ for all kernels and all dimensions.
We expect a higher impact on model quality if we introduce a full ω′ -matrix allowing for adaptation of the activation function for each kernel.

References
1.
2.
3.

4.
5.

6.
7.

8.

Bishop C. M. (1995) Neural Networks for Pattern Recognition; Clarendon Press
Cybenko G. (1989) Approximation by Superposition of a Sigmoidal Function; Mathematics of Control Signals and Systems 2, pp. 303-314
Dorffner G. (1994) A Unified Framework for MLPs and RBFNs: Introducing Conic Section Function Networks; Dept. of Medical Cybernetics and Artificial Intelligence University of Vienna
Duch W., Jankowski N. (1999) Survey of Neural Transfer Functions; Neural Computing
Surveys 2,163-212
Duch W., Jankowski N. (2001) Transfer functions: hidden possibilities for better neural
networks; ESANN'2001 proceedings - European Symposium on Artificial Neural Networks Bruges (Belgium), pp. 81-94
Girosi F., Jones M., Poggio T. (1993) Regularization Theory and Neural Networks; MIT
Cambridge; AI Memo 1430
Hansen, Müller, Koumoutsakos (2003); Reducing the time complexity of the derandomized Evolution Strategy with Covariance Matrix Adaptation (CMA-ES); Evolutionary
Computation 11(1):1-18, MIT Press
Hoffmann G. A. (1997b); Evolving Parameter Sets for Conditional Density Models; Intl.
Symposium on Multi-Techn. Inf. Proc.; Taiwan

686
9.

10.
11.
12.
13.
14.

G.A. Hoffmann
Hutchinson James M., Lo Andrew, Poggio Tomaso (1994) A Nonparametric Approach to
Pricing and Hedging Derivative Securities Via Learning Networks; MIT AI Memo No.
1471 or C.B.C.L. Paper No. 92
Jordan M. I., Bishop C. M. (1996); Neural Networks; AI Memo No. 1562; MIT
Lapedes A., Farber R. (1987) Nonlinear Signal Processing using Neural Networks; Prediction and System Modeling; Proceedings of Real Time Systems Symposium
Park J., Sandberg W. (1991); Universal Approximation using radial basis function networks; Neural Computation vol 3:(2):246-257
Poggio T., Girosi F. (1989); A Theory of Networks for Approximation and Learning; AI
Memo 1140, MIT Cambridge
Ostermeier A., Gawelczyk A., Hansen N. (1993); A Derandomized Approach to Self Adaptation of Evolution Strategies; Technische Universität Berlin, Technical Report: TR-93003

