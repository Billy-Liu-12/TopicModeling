Procedia Computer
Science
Procedia
Computer
(2012)1–9
1825–1833
Procedia
Computer Science
Science 001 (2010)
www.elsevier.com/locate/procedia

International Conference on Computational Science, ICCS 2010

Interpretative Adjoints for Numerical Simulation Codes using MPI
Michel Schanen, Uwe Naumanna,, Laurent Hasco¨etb , Jean Utkec
a LuFG

Informatik 12: Software and Tools for Computational Engineering, RWTH Aachen University
b Projet TROPICS, INRIA Sophia-Antipolis
and Computer Science Division, Argonne National Laboratory, Argonne

c Mathematics

Abstract
An essential performance and correctness factor in numerical simulation and optimization is access to exact derivative information. Adjoint derivative models are particularly useful if a function’s number of inputs far exceeds the
number of outputs. The propagation of adjoints requires the data ﬂow to be reversed, implying the reversal of all
communication in programs that use message-passing. This paper presents recent advances made in developing the
adjoint MPI library AMPI. The described proof of concept aims to serve as the basis for coupling other overloading
AD tools with AMPI. We illustrate its use in the context of a speciﬁc overloading tool for algorithmic diﬀerentiation
(AD) for C++ programs. A simpliﬁed but representative application problem is discussed as a case study.
c 2012 Published by Elsevier Ltd. Open access under CC BY-NC-ND license.
⃝
Keywords: Adjoint MPI, Automatic Diﬀerentiation, Source Transformation

1. Motivation
Automatic (or algorithmic) diﬀerentiation (AD) [1] is a technique for transforming implementations of multivariate vector functions y = F(x), where F : IRn → IRm , into programs that compute directional derivatives y˙ = F · x˙
or adjoints x¯ = (F )T · y¯ with machine accuracy. F = F (x) denotes the Jacobian of F at point x. Several AD
tools have been developed over the years (see www.autodiff.org), and numerous successful applications reported
[2, 3, 4, 5, 6]. Adjoint codes generated by reverse mode AD are of particular interest in the context of large-scale
nonlinear optimization (m = 1, n >> 1) as they allow for gradients of F to be computed at a typically small constant
multiple of the cost of evaluating F itself. The downside of adjoint code is that its eﬃcient implementation is a highly
challenging exercise for most real-world numerical simulation programs.
The given implementation of F is assumed to decompose into a single assignment code (SAC) at every point of
interest as follows:
for j = n + 1, . . . , n + p + m
v j = ϕ j (vi )i≺ j .

(1)

where i ≺ j denotes a direct dependence of v j on vi . The result of each elemental function ϕ j is assigned to a
unique auxiliary variable v j . The n independent inputs xi = vi , for i = 1, . . . , n, are mapped onto m dependent
Email addresses: {naumann,schanen}@stce.rwth-aachen.de (Michel Schanen, Uwe Naumann),
laurent.hascoet@sophia.inria.fr (Laurent Hasco¨et), utke@mcs.anl.gov (Jean Utke)

c 2012 Published by Elsevier Ltd. Open access under CC BY-NC-ND license.
1877-0509 ⃝
doi:10.1016/j.procs.2010.04.204

1826

M. Schanen et al. / Procedia Computer Science 1 (2012) 1825–1833

Michel Schanen, Uwe Naumann, Laurent Hasco¨et, Jean Utke / Procedia Computer Science 00 (2010) 1–9

2

outputs y j = vn+p+ j , for j = 1, . . . , m, and involve the computation of the values of p intermediate variables vk , for
k = n + 1, . . . , n + p.
For given adjoints of the dependent and independent variables, reverse mode AD propagates adjoints backward
through the SAC as follows.
for i ≺ j and j = n + p + m, . . . , n + 1
∂ϕ j
(vk )k≺ j .
v¯ i = v¯ i + v¯ j ·
∂vi

(2)

The variables v¯ j are assumed to be initialized to y¯ j for j = n + p + 1, . . . , n +
p + m and to zero for j = 1, . . . , n + p. A forward evaluation of the SAC is
v3 = v1 · v2
performed to compute all intermediate variables whose values are required
v4 = cos (v3 )
for the adjoint propagation in reverse order. The elemental functions in the
v¯ 3 = − sin (v3 ) · v¯ 4
SAC are processed in reverse order in the second part of Equation (2). See
v¯ 2 = v1 · v¯ 3
Figure 1 for a simple example. The two entries of the gradient are computed
v¯ 1 = v2 · v¯ 3
by setting v¯ 4 = 1. The correctness of this approach follows immediately
from the associativity of the chain rule of diﬀerential calculus. The problem
Figure 1: Adjoint code
of performing this data ﬂow reversal within limited memory as eﬃciently as
possible is known to be NP-complete [7].
AD tools can be separated into two categories depending on the method of implementation. Source code transformation parses the given code and produces a semantically transformed derivative code typically in the same programming language. Alternatively, operator and function overloading can be used to store an internal representation of the
SAC, followed by the interpretation of this tape to propagate the required adjoints.
The reverse propagation of adjoints in the second part of Equation (2) implies the reversal of any communication
in a message-passing setup. If the given implementation of F uses MPI, then sends must become receives, receives
become sends, and so forth. First foundations for this approach were laid in [8]. The focus of the present paper is on
coupling a further extended version of the adjoint MPI library with an existing overloading tool for AD (Section 2).
The approach is veriﬁed with a simpliﬁed version of a real-world case study in Section 3. Conclusions are drawn
(Section 4), followed by a discussion of ongoing and potential future activities.
2. Adjoints by Tape Interpretation
Our AD library dco (derivative code by overloading) uses overloading in C++ based on a user-deﬁned data type
to generate a tape in the form of an array tape of s entries.
1
2
3
4
5
6
7

class t a p e e n t r y {
i n t oc ;
/ / o p e r a t i o n code
double v ; / / f u n c t i o n v a l u e
double a ; / / a d j o i n t v a l u e
i n t arg1 ; / / f i r s t argument
i n t arg2 ; / / second argument
};

All arithmetic operators and the relevant intrinsic functions of C++ are overloaded for variables of the user-deﬁned
type. The extended semantics of the elemental functions results in the storage (also recording) of its operation code,
the computed value, and the indexes of its (up to two) arguments. The interpreter propagates the values of the adjoints
backwards through the tape.

1
2
3
4

void i n t e r p r e t t a p e ( ) {
f o r ( i n t i =s ; i > =0; i −−)
switch ( tape [ i ] . oc ) {
...

M. Schanen et al. / Procedia Computer Science 1 (2012) 1825–1833

Michel Schanen, Uwe Naumann, Laurent Hasco¨et, Jean Utke / Procedia Computer Science 00 (2010) 1–9
5
6
7
8
9
10
11
12
13
14
15
16
17
18

1827
3

case MUL : {
tape [ tape [ i ] . arg1 ] . a+=
tape [ tape [ i ] . arg2 ] . v ∗ tape [ i ] . a ;
tape [ tape [ i ] . arg2 ] . a+=
tape [ tape [ i ] . arg1 ] . v ∗ tape [ i ] . a ;
break ;

}

case SIN : {
tape [ tape [ i ] . arg1 ] . a+=
cos ( tape [ tape [ i ] . arg1 ] . v ) ∗ tape [ i ] . a ;
break ;

}
}

}

Function
v1 = 12
v2 = π
v3 = v1 · v2
v4 = cos (v3 )
set dep(v4 )

Tape
1:[ASG, 12 ,-,-,-]
2:[ASG,π,-,-,-]
3:[MUL, π2 ,-,1,2]
4:[ASG, π2 ,-,3,-]
5:[COS,0,-,4,-]
6:[ASG,0,-,5,-]
7:[DEP,0,-,6,-]
7:[DEP,0,1,6,-]
6:[ASG,0,1,5,-]
5:[COS,0,1,4,-]
4:[ASG, π2 ,-1,3,-]
3:[MUL, π2 ,-1,1,2]
2:[ASG,π,- 12 ,-,-]
1:[ASG, 12 ,-π,-,-]

Adjoint

v¯ 4 = 1
v¯ 3 = − sin (v3 ) · v¯ 4 = −1
v¯ 2 = v1 · v¯ 3 = − 12 , v¯ 1 = v2 · v¯ 3 = −π

Figure 2: Tape generation and interpretation
The taping and interpretation mechanism is illustrated in Figure 2. Each operation is recorded according to the
deﬁnition of class tape entry. The last operation of the forward section is a call of set dep to declare v4 as dependent.
Hence, the interpreter is run with v¯ 4 = 1. An extension of dco to MPI requires the taping of MPI calls as well as their
1
2
3
4
5
6
7
8
9

MPI Routine
send(V)
recv(V)
isend(V,r)
irecv(V,r)
wait(r)
waitall(r[])
awaitall(r[])
bcast(V)
reduce(V)

Forward AMPI
send(V)
recv(V)
isend(V,r)
irecv(V,r)
wait(r)
waitall(r[])
bcast(V)
reduce(V)

Backward AMPI
recv(V)
send(V)
wait(V,r)
wait(V,r)
isend(r) || irecv(r)
[isend(r) || irecv(r)]
[wait(r)]
root: recv(V), not root: send(V)
bcast(V)

Table 1: AMPI Library: Implemented and tested routines
correct reversal. A signiﬁcantly enhanced implementation of the adjoint MPI (AMPI) library proposed in [8] has

1828

M. Schanen et al. / Procedia Computer Science 1 (2012) 1825–1833

Michel Schanen, Uwe Naumann, Laurent Hasco¨et, Jean Utke / Procedia Computer Science 00 (2010) 1–9

Process 1
Code
v1 = 12
v2 = π
v3 = v1 · v2
send(v3 )

Process 2

Tape
1:[ASG, 12 ,-,-,-]
2:[ASG,π,-,-,-]
3:[MUL, π2 ,-,1,2]
4:[ASG, π2 ,-,3,-]
5:[SEND, π2 ,-,4,-]

Code

recv(v3 )
v4 = cos (v3 )
set dep(v4 )
v¯ 4 = 1

recv(¯v3 )
v¯ 2 = v1 · v¯ 3 = − 12
v¯ 1 = v2 · v¯ 3 = −π

4

5:[SEND, π2 ,-1,4,-]
4:[ASG, π2 ,-1,3,-]
3:[MUL, π2 ,-1,1,2]

v¯ 3 = − sin (v3 ) · v¯ 4
= −1
send(¯v3 )

Tape

1:[RECV, π2 ,-,-,-]
2:[COS,0,-,1,-]
3:[ASG,0,-,2,-]
4:[DEP,0,-,3,-]
4:[DEP,0,1,3,-]
3:[ASG,0,1,2,-]
2:[COS,0,1,1,-]
1:[RECV, π2 ,-1,-,-]

2:[ASG,π,− 12 ,-,-]
1:[ASG, 12 ,−π,-,-]
Figure 3: dco and AMPI

been developed. The current state of the AMPI library is summarized in Table 1. Refer to [8] for details on awaitall.
We simpliﬁed the listed MPI calls by omitting parameters that are not relevant for this discussion. From a user
perspective MPI structure and speciﬁcation should be preserved as much as possible. AMPI provides for every MPI
routine a version to be called in the forward section of the adjoint code and its matching implementation for the reverse
section. Let us extend our simple example such that v3 = v1 · v2 is computed by one process and v4 = cos(v3 ) by a
second one. Assume blocking communication for exchanging the value of v3 . New opcodes are provided within dco
to represent AMPI calls by tape entries.
Figure 3 shows the tape generation and interpretation phases for the two processes. Each process has its own tape.
The interpretation is similar to the serial case; the only diﬀerence is in the SEND and RECV entries. Passing v3 from
process 1 to process 2 yields the communication of v¯ 3 from process 2 to process 1 during the interpretation of the tape.
The corresponding AMPI routines are called by the interpreter.
A fundamental diﬀerence exists between MPI routines and the elemental operations: MPI uses requests to link
variables with their communication. Consequently, an AMPI request contains memory for the function value, adjoint
value, operation code, destination, communicator, and request. As an example we consider a nonblocking communication using isend, irecv, and wait. For the sake of brevity, we restrict the AMPI request data to the operation code
and the communicated value (double precision) in addition to the original MPI Request (Figure 4). A tape-speciﬁc
dco extension of AMPI request is used to link wait operations to their respective nonblocking communication.
1
2
3
4
5
6

typedef s t r u c t AMPI Request {
MPI Request r ;
i n t oc ;
double v ;
...
} AMPI Request ;

1
2
3
4
5

typedef s t r u c t AMPI dco Request {
AMPI Request r ;
int a;
...
} AMPI dco Request ;

Figure 4: Customized Requests

M. Schanen et al. / Procedia Computer Science 1 (2012) 1825–1833

Michel Schanen, Uwe Naumann, Laurent Hasco¨et, Jean Utke / Procedia Computer Science 00 (2010) 1–9

Process 1
Code
v1 = 12
v2 = π
v3 = v1 · v2
r.v = v3 , r.a = 5
r.oc = isend
isend(r.v, r)
wait(r)

Tape
1:[ASG, 12 ,-,-,-]
2:[ASG,π,-,-,-]
3:[MUL, π2 ,-,1,2]
4:[ASG, π2 ,-,3,0]
5:[ISEND, π2 ,-,4,-,r]
6:[WAIT,r.v,-,r.a,-,r]
≡ [WAIT, π2 ,-,5,-,r]

v¯ 2 = − 12
v¯ 1 = −π

6:[WAIT, π2 ,0,5,-,r]
5:[ISEND, π2 ,r.v,4,-,r]
≡ [ISEND, π2 ,-1,4,-,r]
4:[ASG, π2 ,-1,3,-]
3:[MUL, π2 ,-1,1,2]

5

Process 2
Code

Tape

irecv(r.v, r)
r.oc = irecv
r.a = 1
wait(r)
v3 = r.v = π2
v4 = cos (v3 )

1:[IRECV,-,-,-,-,r]

set dep(v4 )
v¯ 4 = 1

irecv(r.v, r)
wait(r)
v¯ 3 = r.v

1829

v¯ 3 = −1 =
− sin (v3 ) · v¯ 4
r.v = v¯ 3
isend(r.v, r)
wait(r)

2:[WAIT,r.v,-,r.a,-,r]
≡ [WAIT, π2 ,-,1,-,r]
3:[COS,0,-,2,-]
4:[ASG,0,-,3,-]
5:[DEP,0,-,4,-]
5:[DEP,0,-,4,-]
4:[ASG,0,1,3,-]
3:[COS,0,1,2,-]
2:[WAIT,0,-1,1,-,r]
1:[IRECV,-,-,-,-,r]

2:[ASG,π,− 12 ,-,-]
1:[ASG, 12 ,−π,-,-]
Figure 5: Tape generation and interpretation with AMPI Request

This mechanism is explained best with the help of an example. Consider Figure 5. We use a ﬂattened notation
to access the relevant entries within AMPI dco Request (r.oc, r.v, r.a). The ﬁrst four tape entries are generated by
process 1 as in Figure 3. The isend operation is recorded next, together with the value to be communicated ( π2 ), the
point of its deﬁnition (tape entry 4), and the AMPI dco Request r, whose value is set to r.v = π2 and r.a = 5 in order
for the upcoming wait to be able to link with the current isend. W.l.o.g., we omit any additional computation between
the isend/irecv - wait pairs. The tape entry for the wait operation contains the value r.v and the tape index r.a = 5 of
the corresponding isend retrieved from the associated AMPI dco Request r.
Process 2 receives the value π2 and sets the AMPI dco Request r correspondingly; that is, r.v = π2 and r.a = 1.
This information is used to generate the tape entry for the associated wait operation; that is, the value is set to r.v = π2 ,
and the index of its sole argument becomes r.a = 1. The cosine of π2 obtained from tape entry 2 is found to be zero
and is recorded correspondingly. This step completes the tape generation.
The relevant part of the interpretation starts with process 2. The r.v ﬁeld is now used to convey the adjoint value.
In order to compute the desired gradient, the adjoint ﬁeld of the tape entry corresponding to the dependent variable v4
is set to one. Interpretation of the COS entry yields v¯ 3 = − sin(v3 ) · v¯ 4 = −1 · 1 = −1, which is stored in the adjoint
ﬁeld of the WAIT entry. Interpretation of the latter amounts to sending the adjoint value to process 1. The associated
irecv becomes a wait for the completion of this communication. Process 1 receives the adjoint v¯ 3 = r.v = −1. The
corresponding wait is followed by the remaining interpretation of the MUL entry to get v¯ 1 and v¯ 2 .

1830

M. Schanen et al. / Procedia Computer Science 1 (2012) 1825–1833

Michel Schanen, Uwe Naumann, Laurent Hasco¨et, Jean Utke / Procedia Computer Science 00 (2010) 1–9

6

3. Case Study: Heat Equation
We consider a simple data assimilation problem involving the one-dimensional heat equation. A bar of given
length is heated on one side for some time. The simulated temperature distribution is compared with available measurements at a number of discrete points. The initial temperature distribution within the bar is to be estimated such
that the discrepancy between simulated and measured values is minimized. This case study has been designed to
illustrate the use of the AMPI library with dco. We do not report on the runtime statistics of the parallel version, nor
do we discuss any numerical properties of this problem in detail.
We aim to solve the optimization problem minIR f, where
nx

f =
i=0

T (1, xi ) − T˜ i

2

such that
T t = c · T xx

and where

for 0 ≤ x, t ≤ 1

i
for i = 0, . . . , n x
nx
T = T (t, x) : IR2 → IR
T˜ ∈ IRnx (observations)

xi =

T 0 = T (0, x) = f (x) for 0 < x < 1 (initial condition)
T (t, 0) = α for 0 ≤ t ≤ 1 (left boundary condition)

T (t, 1) = β

for 0 ≤ t ≤ 1

(right boundary condition).

Discretization in space is done by centered ﬁnite diﬀerences with step size δx. Explicit Euler is used for time integration with time step
(δx)2
δt ≤
2·c
to ensure stability [9] and to get
T kj+1 − 2 · T kj + T kj−1
− T kj
T k+1
j
=c·
δt
(δx)2
and hence
δt
· (T kj+1 − 2 · T kj + T kj−1 )
(δx)2
n2
= T kj + c · x · (T kj+1 − 2 · T kj + T kj−1 ),
nt

= T kj + c ·
T k+1
j

where n x = (δx)−1 and nt = (δt)−1 .
The cost function is implemented in C++ as shown in Listing 1. For parallelization the bar is decomposed
into numprocs elements (lines 3-6). Each process computes one time step on its element (lines 8-10), followed by
a synchronization with the neighboring elements (lines 11-33). This simple setup is illustrated in Figure 6. The
individual contributions mpicost to the overall costs are ﬁnally reduced to cost (lines 34-38).
Two adjustments must be made to the original dco code. All variables of type active must have their types changed
to AMPI dco double, and the names of all MPI ∗ routines must become AMPI ∗.
i
for i = 0, . . . , n x . The results
We consider the case c = 103 , n x = 200, nt = 1000, α = 2, β = 0, and f (xi ) = 2 − 100
are plotted in Figure 7. The substantial discrepancy between the originally simulated and the observed temperature
distributions is reduced by applying a simple steepest descent method. Starting from an initial cost of 227 for T 0 = 0,
the algorithm performs 401 iterations (taking ∼70 seconds on our PC in the serial case and ∼35 seconds when using
4 processes) to decrease the norm of the gradient to a value less than 10−3 .

M. Schanen et al. / Procedia Computer Science 1 (2012) 1825–1833

Michel Schanen, Uwe Naumann, Laurent Hasco¨et, Jean Utke / Procedia Computer Science 00 (2010) 1–9

1831
7

Figure 6: Synchronization
1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
21
22
23
24
25
26
27
28
29
30
31
32
33
34
35
36
37
38
39
40

AMPI double c o s t ( i n t & nx , i n t & nt , AMPI double& d e l t a t , AMPI double& c , AMPI double ∗ temp ,
AMPI double ∗ temp obs ) {
AMPI double b u f [ 4 ] ; AMPI double m p i c o s t = 0 ; AMPI Request r e q u e s t [ 4 ] ; AMPI Status s t a t u s
[4];
AMPI double c o s t =0;
i n t m p i j =( i d ∗ ( nx / numprocs ) ) +1;
i n t mpi nx = ( ( i d +1) ∗ ( nx / numprocs ) ) +1;
i f ( i d == numprocs − 1)
mpi nx −− ;
f o r ( i n t i =0 ; i <= n t ; i ++) {
f o r ( i n t j = m p i j ; j < mpi nx ) ; j ++) {
temp [ j ] = temp [ j ] ∗ c ∗ nx ∗ d e l t a t ∗ ( temp [ j +1] − 2 ∗ temp [ j ] + temp [ j − 1 ] ) ;

}

/ / r e c i e v e from r i g h t & send t o r i g h t
i f ( i d ! = numprocs − 1 ) {
b u f [ 2 ] = temp [ mpi nx − 1 ] ;
AMPI Isend (& b u f [ 2 ] , 1 , MPI DOUBLE , i d +1 ,0 ,AMPI COMM WORLD,& r e q u e s t [ 2 ] ) ;
AMPI Irecv (& b u f [ 1 ] , 1 , MPI DOUBLE , i d +1 ,0 ,AMPI COMM WORLD,& r e q u e s t [ 1 ] ) ;

}

/ / r e c i e v e from l e f t & send t o l e f t
i f ( i d != 0) {
b u f [ 0 ] = temp [ m p i j ] ;
AMPI Isend (& b u f [ 0 ] , 1 , MPI DOUBLE , i d − 1 ,0 ,AMPI COMM WORLD,& r e q u e s t [ 0 ] ) ;
AMPI Irecv (& b u f [ 3 ] , 1 , MPI DOUBLE , i d − 1 ,0 ,AMPI COMM WORLD,& r e q u e s t [ 3 ] ) ;

}

i f ( i d ! = numprocs − 1) {
AMPI Wait (& r e q u e s t [ 1 ] , & s t a t u s [ 1 ] ) ;
AMPI Wait (& r e q u e s t [ 2 ] , & s t a t u s [ 2 ] ) ;
temp [ mpi nx ] = b u f [ 1 ] ;

}

i f ( i d != 0) {
AMPI Wait (& r e q u e s t [ 0 ] , & s t a t u s [ 0 ] ) ;
AMPI Wait (& r e q u e s t [ 3 ] , & s t a t u s [ 3 ] ) ;
temp [ m p i j − 1] = b u f [ 3 ] ;

}

}

f o r ( i n t j = m p i j ; j < mpi nx ; j ++) {
m p i c o s t = m p i c o s t +( temp [ j ] − temp obs [ j ] ) ∗ ( temp [ j ] − temp obs [ j ] ) ;

}

AMPI Reduce(& mpi cost , &cost , 1 , AMPI DOUBLE , MPI SUM , 0 , MPI COMM WORLD) ;
return cost ;

}

Listing 1: heat example

1832

M. Schanen et al. / Procedia Computer Science 1 (2012) 1825–1833

Michel Schanen, Uwe Naumann, Laurent Hasco¨et, Jean Utke / Procedia Computer Science 00 (2010) 1–9

8

(a) Discrepancy between original simulation of heat distribution (b) Simulation of heat distribution with optimized initial values
starting from T 0 = 0 and observed values

Figure 7: Results
4. Conclusion and Outlook
Although the AMPI library is still under development, we have been able to verify its current version as robust and
user-friendly in the context of both source transformation [8] and overloading tools for automatic diﬀerentiation in
adjoint mode. Ongoing eﬀorts focus on rigorous testing by application to a number of large-scale numerical simulation
codes from the Earth and atmospheric sciences.
Second-order methods for nonlinear optimization require second derivatives in the form of Hessians or projections
thereof. For example, a single iteration is performed to push the value of the gradient of f below 10−9 , as
f (x0 ) =

nx
i=0

T nt (xi0 ) − T˜ i

2

is quadratic and
T 0j = 0
T k+1
= T kj + c ·
j

n2x
· (T kj+1 − 2 · T kj + T kj−1 )
nt

for j = 1, . . . , n x − 1 and k = 0, . . . , nt − 1. Consequently, its gradient ∇ f is linear, yielding a constant Hessian ∇2 f.
With x0 = 0 we get the solution x = x j+1 = x j for j = 1, . . . from the linear system
∇2 f (x0 ) · x = −∇ f (x0 )
(e.g., by Gauss), since according to Newton’s algorithm (see, e.g., [10])
x = x0 − ∇2 f (x0 )

−1

· ∇ f (x0 )

.

Unfortunately, the inverse heat propagation problem is ill-posed, leading to a largely meaningless solution unless
regularization [11] is applied. Consequently we solve the regularized problem
(∇2 f (x0 ) + α · Inx ) · x = −∇ f (x0 )
with an appropriately chosen regularization parameter α > 0, yielding an acceptable loss in accuracy. While the accumulation of the (dense but constant) Hessian dominates the computation, the overall serial runtime of approximately
45 seconds signiﬁcantly undercuts that of the corresponding steepest descent algorithm. The latter takes several hours
to reduce the residual below 10−9 .

M. Schanen et al. / Procedia Computer Science 1 (2012) 1825–1833

Michel Schanen, Uwe Naumann, Laurent Hasco¨et, Jean Utke / Procedia Computer Science 00 (2010) 1–9

1833
9

The eﬃcient computation of second derivatives of MPI codes requires further extension of the AMPI library in
order to be able to communicate second-order adjoint information. Refer to [12] for further details on the computation
of directional derivatives for message-passing programs. Feasibility studies are under way. The coupling of the AMPI
library with the popular overloading AD tool for C++ ADOL-C [13] is planned.
Acknowledgment
This work was supported by the Fond National de la Recherche of Luxembourg under grant PHD-09-145.
References
[1] A. Griewank, A. Walter, Evaluating Derivatives. Principles and Techniques of Algorithmic Diﬀerentiation (2nd Edition), SIAM, Philadelphia,
2008.
[2] M. Berz, C. Bischof, G. Corliss, A. Griewank (Eds.), Computational Diﬀerentiation: Techniques, Applications, and Tools, Proceedings
Series, SIAM, Philadelphia, 1996.
[3] C. Bischof, M. B¨ucker, P. Hovland, U. Naumann, J. Utke (Eds.), Advances in Automatic Diﬀerentiation, no. 64 in LNCSE, Springer, Berlin,
2008.
[4] M. B¨ucker, G. Corliss, P. Hovland, U. Naumann, B. Norris (Eds.), Automatic Diﬀerentiation: Applications, Theory, and Tools, no. 50 in
Lecture Notes in Computational Science and Engineering, Springer, Berlin, 2005.
[5] G. Corliss, A. Griewank (Eds.), Automatic Diﬀerentiation: Theory, Implementation, and Application, Proceedings Series, SIAM, Philadelphia, 1991.
[6] G. Corliss, C. Faure, A. Griewank, L. Hasco¨et, U. Naumann (Eds.), Automatic Diﬀerentiation of Algorithms – From Simulation to Optimization, Springer, New York, 2002.
[7] U. Naumann, DAG reversal is NP-complete, J. Discr. Alg.To appear. Appeared online on Elsevier’s ScienceDirect as
doi:10.1016/j.jda.2008.09.008.
[8] J. Utke, L. Hasco¨et, P. Heimbach, C. Hill, P. Hovland, U. Naumann, Toward Adjoinable MPI, in: Proceedings of the 23rd IEEE International
Parallel & Distributed Processing Symposium, IEEE Computer Society, Washington, DC, USA, 2009.
[9] M. Heath, Scientiﬁc Computing. An Introductory Survey, McGraw-Hill, New York, 1998.
[10] C. T. Kelley, Solving Nonlinear Equations with Newton’s Methods, SIAM, Philadelphia, 2003.
[11] A. Tikhonov, On the stability of inverse problems, Dokl. Akad. Nauk SSSR 39 (5) (1943) 195–198.
[12] P. Hovland, C. Bischof, Automatic Diﬀerentiation for Message-Passing Parallel Programs, in: IPPS ’98: Proceedings of the 12th. International
Parallel Processing Symposium on International Parallel Processing Symposium, IEEE Computer Society, Washington, DC, USA, 1998.
[13] A. Griewank, D. Juedes, J. Utke, Algorithm 755: ADOL-C: A package for the automatic diﬀerentiation of algorithms written in C/C++,
ACM Transactions on Mathematical Software 22 (2) (1996) 131–167.

