Available online at www.sciencedirect.com

Procedia Computer Science 9 (2012) 312 – 320

International Conference on Computational Science, ICCS 2012

Genetic Algorithm Characterization for the Quality Assessment of
Forest Fire Spread Prediction
Andr´es Cencerrado, Ana Cort´es, Tom`as Margalef
Computer Architecture and Operating Systems Department. Escola d’Enginyeria, Universitat Aut`onoma de Barcelona. 08193 Bellaterra,
Barcelona (Spain)

Abstract
When an emergency occurs, hazard evolution simulators are a very helpful tool for the teams in charge of making
decisions. These simulators need certain input data, which deﬁnes the characteristics of the environment where the
emergency is taking place. This kind of data usually constitutes a big set of parameters, which have been previously
recorded from observations, usually coming from remote sensors, pictures, etc. However, this data is frequently
subject to a high degree of uncertainty, as well as the results produced by the corresponding simulators. Hence, it is
also necessary to pay attention to the simulations’ quality and reliability. In this work we expose the way we deal with
such uncertainty. Our research group has previously developed a two- stage prediction methodology that introduces
an adjustment stage in order to deal with the uncertainty on the simulator input parameters. This method signiﬁcantly
improves predictions’ quality, however, in order to be useful, a good characterization of the adjustment techniques
has to be carried out so that we are able to choose the best conﬁguration of them, given certain restrictions regarding
resources availability and time deadlines. In this work, we focus on forest ﬁres spread prediction as a real study case,
for which Genetic Algorithms (GA) have been demonstrated to be a suitable adjustment strategy. We describe the
methodology used to characterize the GA and we also validate it when assessing in advance the quality of the ﬁre
spread prediction.
Keywords: Forest Fire Simulation, Data Uncertainty, Emergency Management, Genetic Algorithm

1. Introduction
As it is well stated, tornados, ﬂoods, forest ﬁres, and other natural hazards may dramatically threaten people’s lives,
because of the diﬀerent kind of drawbacks they can generate, such as disturbing people’s daily activities, economic
losses, and even breaking the peace of a whole country. For this reason, any eﬀort oriented to minimize the impacts
of natural catastrophes is welcome. Due to the diﬃculty on predicting the occurrence of these phenomena, most of
the research eﬀorts are focused on predicting their evolution through time, relying on some physical or mathematical
models.
Email addresses: acencerrado@caos.uab.es (Andr´es Cencerrado), ana.cortes@uab.es (Ana Cort´es), tomas.margalef@uab.es
(Tom`as Margalef)

1877-0509 © 2012 Published by Elsevier Ltd. Open access under CC BY-NC-ND license.
doi:10.1016/j.procs.2012.04.033

AndrÈs Cencerrado et al. / Procedia Computer Science 9 (2012) 312 – 320

313

Nevertheless, environmental hazards represent very diﬃcult systems to simulate. Theoretical and model-related
issues aside, many simulators lack precision on their results because of the inherent uncertainty of the data needed to
deﬁne the state of the system. This uncertainty is due to the diﬃcult to gather precise values at the right places where
the catastrophe is taking place, or because the hazard itself distorts the measurements. So, in many cases the unique
alternative consists of working with interpolated, outdated, or even absolutely unknown values. Obviously, this fact
results in a lack of accuracy and quality on the provided predictions.
To overcome the just mentioned input uncertainty problem, we have developed a two-stage prediction strategy,
which, ﬁrst of all, carries out a parameter adjustment process by comparing the results provided by the simulator and
the real observed disaster evolution. Then, the underlying simulator is executed taking into account the adjusted parameters obtained in the previous phase in order to predict the evolution of the particular hazard for a later time instant.
A successful application of this method mainly depends on the eﬀectiveness of the adjustment technique that has been
carried out. In this sense, our research group has developed several solutions for input parameters optimization, all
of them characterized by an intensive data management: use of statistical approach based on exhaustive exploration
of previous ﬁres databases [5], application of evolutionary computation [8], calibration based on domain-speciﬁc
knowledge [4], and even solutions coming from the merge of some of the above mentioned [7].
It has been demonstrated that the above mentioned adjustment techniques contribute to improve the quality of the
predictions. However, a characterization of the adjustment techniques has to be carried out so that we are able to deal
with eventual restrictions. These restrictions will be set up at an earlier time instant before starting the calibration
and prediction stages, and they may be related to time deadlines, computational resources availability, or both. Therefore, such a characterization must deliver the appropriate parameter settings for executing the underlying adjustment
strategy when a certain quality degree prediction is, a priori, requested.
This work describes the characterization of GA as adjustment process within the two-stage prediction framework.
By means of GA characterization, we state the capability to set up, before starting the whole prediction method, the
populations size and the number of GA’s iterations that must be executed to satisfy the requested prediction quality.
This paper is organized as follows. In the next section, an overview of how the two-stage prediction method
works is given. In Section 3, we expose how this framework could be generalized to any natural hazard, and the
methodology to assess in advance the quality of predicitions is described. In Section 4, the experimental study is
reported and, ﬁnally, the main conclusions are included in Section 5.
2. Two-stage Prediction Method
Nowadays, scientiﬁc community rely on High Performance Computing (HPC) environments in order to solve
most of the present scientiﬁc problems. Nevertheless, applications which solve such problems still need so much
computational resources and time, specially dynamic, event-driven systems simulations.
In the ﬁeld of physical systems modeling, speciﬁcally forest ﬁre behavior modeling, there exist several ﬁre propagation simulators [9, 10, 11], based on some physical or mathematical models [1], which main objective is to try to
predict the ﬁre evolution. These simulators need certain input data, which deﬁne the characteristics of the environment
where the ﬁre is taking place in order to evaluate its future propagation. This data usually consists of the current ﬁre
front, terrain topography, vegetation type, and meteorological data such as humidity, wind direction and wind speed.
Some of this data could be retrieved in advance and with noticeably accuracy as, for example, the topography of the
area and the predominant vegetation types. However, there is some data that turns out very diﬃcult to obtain with
reliability. For instance, to get an accurate ﬁre perimeter is very complicated because of the diﬃculties involved in
getting, at real time, images or data about this matter. Other kind of data sensitive to imprecisions is that of meteorological data, which is often distorted by the ﬁre itself. However, this circumstance is not only related to forest
ﬁres, but it also happens in any system with a dynamic state evolution (e.g. ﬂoods [18], thunderstorms [19, 20], etc.).
These restrictions concerning uncertainty in the input parameters, added to the fact that these inputs are set up only at
the very beginning of the simulation process, become an important drawback because as the simulation time goes on,
variables previously initialized could change dramatically, misleading simulation results. In order to overcome these
restrictions, we need a system capable of dynamically obtaining real time input data in those cases that is possible
and, otherwise, properly estimating the values of the input parameters needed by the underlying simulator.

314

AndrÈs Cencerrado et al. / Procedia Computer Science 9 (2012) 312 – 320

(a) Classic prediction

(b) Two-stage prediction

Figure 1: Prediction Methods
The classic way of predicting forest ﬁre behavior, which is summarized in Figure 1(a), takes the initial state of the
ﬁre front as input, as well as the input parameters given for a certain time instant. The simulator then returns the ﬁre
spread prediction for a later time instant.
Comparing the simulation result with the real ﬁre propagation (later on the experimental section, we describe in
more detail how this comparison is conducted), the forecasted ﬁre front tends to diﬀer to a greater or lesser extent
from the real ﬁre line. One reason for this behavior is that the classic calculation of the simulated ﬁre is based on
one single set of input parameters aﬄicted with the before explained insuﬃciencies. To overcome this drawback, a
simulator independent data-driven prediction scheme was proposed to optimize dynamic model input parameters [3].
Introducing a previous adjustment step as shown in Figure 1(b), the set of input parameters is optimized before every
prediction step. The solution proposed comes from reversing the problem: how to ﬁnd a parameter conﬁguration such
that, given this conﬁguration as input, the ﬁre simulator would produce predictions that match the actual ﬁre behavior.
Having detected the simulator input that better describes current environmental conditions, the same set of parameters,
could also be used to describe best the immediate future, assuming that meteorological conditions remain constant
during the next prediction interval. Then, the prediction becomes the result of a series of automatically adjusted input
conﬁgurations. It is worth highlighting that, since this two stages spread prediction scheme for forest ﬁre described in
Figure 1(b) constitutes a simulator-independent prediction method, the same technique could be extrapolated to any
kind of natural disasters by only exchanging the underlying simulator.
This prediction scheme has been demonstrated to deliver better predictions than the classical approach. This is due
to the fact that this scheme relies on High Performance Computing platforms by means of executing a huge number
of problem simulations on the adjustment stage. Therefore, in order to be operative during a real hazard ocurrence, it
is necessary to be able to provide in advance reliable prediction results. For this purpose, as we have just mentioned,
we need access to a huge number of computing elements. This leads to the necessity of deploying a way to set up in
advance:
• The prediction scheme settings, in particular, the calibration policy’s specﬁc paremeters, for a required prediction’s quality. This is specially relevant when the ongoing hazard may threaten urban areas and even human
lives.
• The computational resources needed to deliver a required prediction’s quality, given a certain time constraints.
This paper focuses on the ﬁrst point and, in particular, we present the characterization of the Genetic Algorithm
(GA) as a calibration scheme. In the next section, we describe in detail how this characterization has been performed.

3. Calibration Stage: Genetic Algorithm
Although the subsequent study has been performed for the case of forest ﬁre spread prediction, as we have previosly mentioned, it could be extrapolated to any other hazard by simply exchanging the underlying simulator. How-

AndrÈs Cencerrado et al. / Procedia Computer Science 9 (2012) 312 – 320

315

ever, we need to work under certain assumptions in order to bound the problem and to be able to deliver reliable
solutions. Such assumptions are the following:
• We rely on the two-stage prediction strategy under stable environmental conditions.
• We focus on those emergencies where the corresponding simulators present high input-data sensitivity.
• We assume scenarios where the computational resources are dedicated. Currently, we are working on adapting
tools that allow urgent execution of tasks in distributed-computing environments, e.g. SPRUCE [14].
It is obvious that in the two-stage prediction strategy, the adjustment process plays the main role. Previous studies
demonstrated that the quality of this simulation is directly correlated to the quality obtained at the end of the adjustment
process [5]. Thus, it is absolutely necessary to have a good characterization of this process in order to be able to
evaluate the adjustment quality we can reach under certain conditions.
For the particular case of forest ﬁre spread prediction, Genetic Algorithm (GA) has been proved to be a good
adjustment strategy. As it is well known, GA works in an iterative way. It starts with an initial population of individuals
which will be evolved through several iterations in order to guide them to better search space areas. Operators such
as elitism, selection, crossover and mutation are applied to every population to obtain a new one better than the
previous one. Every individual from a population is ranked according to a predeﬁned ﬁtness function. The ﬁtness
function in the case of forest ﬁres spread prediction is the diﬀerence between real and simulated ﬁre spread (the
error formula we use will be detailed in Section 4). The iterative nature of GA leads to an eventually near-optimal
solution in the adjustment stage after a certain number of GA iterations. For this reason, it is mandatory to analyze
the GA convergence for the particular case of forest ﬁre spread prediction, as well as to be able to extract a general
characterization of its behaviour. The analysis of GA convergence is reported in the next section.
4. Genetic Algorithm Convergence Study
In this section, we present the experimental studies carried out to fulﬁll the need of being able to select, in advance,
the best settings for the adjustment method, Genetic Algorithm (GA) in this case, given a certain prediction quality
constraint. Many beneﬁts have been previously reported from the use of Genetic Algorithms as a calibration technique
[8]. However, by its own nature, this method constitutes a great challenge for the matter we deal with in this work.
On the one hand, this technique allows to obtain diﬀerent degrees of quality on the solutions obtained, which allows
us to be able to adapt to eventual restrictions (deadlines, available resources, etc.). On the other hand, this ﬂexibility
turns it harder to characterize in order to choose a correct conﬁguration of the method for each case.
Parameters such as number of generations, individuals per population, elitism factor, mutation probability, and
so on, aﬀect the quality of the winner individual, i.e. the ﬁnal solution we will deliver at the end of the adjustment
process.
For the characterization of GA, we have carried out massive executions to obtain a proper statistical analysis.
Subsequently, we expose the details of the conducted experimental study.
4.1. Test bed description
All the experiments reported in this section have been performed using FARSITE [9] as ﬁre spread simulator.
This experiment uses the GIS data from the benchmark provided by FARSITE (the Ashley project). Based on this
benchmark, we set a reference ﬁre with a duration ﬁve hours. All the simulations carried out in this study take these
ﬁrst ﬁve hours of spread as the adjustment time interval, and every initial simulation setting (i.e. every individual in
the GA) is conﬁgured according to the probability distributions and their associated parameters shown in Table 1 for
each type of input parameter (i.e. each gene of each individual). As regards wind speed and wind direction, these
probabilities correspond to the ones used in [17]. Vegetation models correspond to the 13 standard Northern Forest
Fire Laboratory (NFFL) fuel models [2].
The ﬁnal ﬁre front of the reference ﬁre is taken at the end of each simulation in order to calculate the diﬀerence
between it and the simulated one. We call this diﬀerence adjustment error, and it is calculated by means of the
following formula:

316

AndrÈs Cencerrado et al. / Procedia Computer Science 9 (2012) 312 – 320

(UnionCells − InitCells) − (IntersectionCells − InitCells)
(1)
RealCells − InitCells
This equation calculates the diﬀerences in the number of cells burned, both missing or in excess, between the
simulated and the real ﬁre. UnionCells is the union of the number of cells burned in the real ﬁre and the cells burned
in the simulation, IntersectionCells is the intersection between the number of cells burned in the real ﬁre and in the
simulation, RealCells are the cells burned in the real ﬁre and InitCells are the cells burned at the starting time.
Regarding the GA conﬁguration, in this particular set of experiments we ﬁxed both the elitism factor (10%), and
the number of generations (5), and we performed a study based on the results obtained from the evolution of 50
populations composed of 100 individuals for each case, and setting the mutation probability to 10%.
As regards the computational platform, all the experiments carried out in this work were done on a cluster of 8
x Dell PowerEdge M600 nodes, each of which counting on 2xQuad-Core Intel Xeon E5430, 2.66GHz, 2x6MB L2
cache memory (2x2) and 16 GB RAM Fully Buﬀered DIMMs 667MHz, running Linux version 2.6.16.
E=

Input
Vegetation
model
Wind
Speed
Wind Direction
Dead fuel
moisture
Live fuel
moisture

Distribution
Uniform

μ,σ
—

Min,Max
1,13

Normal

12.83,6.25

—

Normal

56.6,13.04

—

Uniform

—

0,1

Uniform

—

0,4

Table 1: Input parameters distributions description.

4.2. Statistical Study
From the obtained results, a statistical study carrying out the Kolmogorov-Smirnov, Anderson-Darling, and Chisquared tests allowed us to determine that the probability distribution which better ﬁts the obtained data is the Logistic
distribution, which resembles the normal distribution, but presents higher kurtosis. Its probabilty density function is
the following one:
pd f (x; μ, s) =

e−(x−μ)/s
s(1 + e−(x−μ)/s )2

(2)

In this equation, x is the random variable (which corresponds to the obtained adjustment error), μ is the location
factor, which is analogous to the mean value in a normal distribution, and s is the scale factor, which is proportional
to the standard deviation, both of them needed to deﬁne such probability distribution. Although the probability
distribution of the data is the same in the whole evolution process, these factors vary depending on the iteration of the
GA we are evaluating. So, Figure 2 depicts the diﬀerent probabilty density functions for each generation. Figure 3
shows the obtained values for both the location and scale factors at each generation of the evolution process.
By means of these probability density functions we are able to guarantee, with diﬀerent degrees of certainty, the
maximum adjustment error we will obtain given a certain conﬁguration of the GA (in this particular case, populations
of 100 individuals, and both elitism and mutation factor set to 10%). Besides, since the number of evolved generations
has a direct impact on both the available resources and time needed to perform the adjustment process, it is worth
highlighting the fact that we are able to give this guarantee taking into account the number of generations we are able
to execute. Symbols G1-G5 correspond to the number of generation within the evolution process.
Table 2 shows the diﬀerent maximum adjustment errors (considering the adjustment time interval [0 hours - 5
hours]) for which we have diﬀerent degrees of guarantee, depending on the number of generations the GA iterates.

AndrÈs Cencerrado et al. / Procedia Computer Science 9 (2012) 312 – 320

317

Figure 2: Probability density functions for the obtained errors at each generation of the evolution process

Figure 3: Location and scale values obtained after the evolution of 50 populations for the calibration interval ( 0hours
- 5hours )
Here, guarantee degree stands for the probability to obtain an adjustment error lesser or equal than the speciﬁed value,
on the basis of the above presented probabilty density function (Equation 2).
Figure 4 also depicts this information, from a guarantee degree of 95% down to 75%. As it is easily understandable, the lesser the error requested is, the lesser the degree of guarantee, for the same number of iterations of the
GA.
Considering a real situation, where the quality of the prediction is a parameter ﬁxed by the decision control centre
in charge of making the appropriate decisions about how to ﬁght against the ongoing ﬁre, this information turns out to
be very important, since we are able to give a certain guarantee of quality in the ﬁnal prediction, taking into account
how many evolution steps (i.e. how many generations) we can perform. This, as previously stated, will be determined
by the available computational resources and time to deliver a prediction. In the subsequent subsection we expose a
validation experiment which conﬁrms this study.
4.3. Experimental Validation
Once the above exposed statistical analysis has been carried out, we present a validation test by means of which
we can prove that this characterization methodology is suitable for the problem we tackle.

318

AndrÈs Cencerrado et al. / Procedia Computer Science 9 (2012) 312 – 320

Guarantee
degree
95%
90%
85%
80%
75%
70%
65%
60%
55%
50%

G1

G2

G3

G4

G5

0.763
0.680
0.628
0.589
0.557
0.529
0.504
0.480
0.457
0.434

0.726
0.644
0.593
0.555
0.523
0.496
0.471
0.447
0.425
0.403

0.649
0.572
0.525
0.489
0.459
0.434
0.410
0.388
0.367
0.347

0.528
0.458
0.414
0.381
0.353
0.330
0.308
0.288
0.268
0.249

0.419
0.367
0.334
0.309
0.289
0.271
0.255
0.240
0.226
0.211

Table 2: Maximum adjustment errors and degrees of guarantee, depending on the number of GA generations

Figure 4: Maximum adjustment errors and degrees of guarantee
For this purpose, we carried out the adjustment process, considering the adjustment time interval [0 hours - 5
hours], for ﬁve new random populations (p0 - p4) in the case of a new diﬀerent ﬁre. Speciﬁcally, as in the test bed
described in Section 4.1, we used FARSITE as the ﬁre propagation simulator, as well as the same GIS data, but we
changed completely the conditions of the reference ﬁre, so its spread varied signiﬁcantly.
As regards the GA conﬁguration, the ﬁve new populations were composed of 100 individuals, both elitism and
mutation factor were set to 10%, and again we performed ﬁve-generation evolution process.
Table 3 shows the obtained errors for each population. In order to contrast this data with the guaranteed errors
exposed in Table 2, let us consider three cases of diﬀerent guarantee degrees in our estimations before the adjustment
process was carried out:
• Estimation with 95% degree of guarantee: in this case, all of our estimations were right, with the exception of
the errors obtained at generation 1 of populations p1, p3 and p4, which present very high adjustment errors. This
fact was expected, and it is understandable, since because of the random features of genetic algorithms, only
one step of evolution is not enough at all to determine any kind of signiﬁcant approach to a suitable solution, so
is not enough to give an estimation with any degree of guarantee.
• Estimation with 85% degree of guarantee: again, we failed in the estimations of populations p1, p3 and p4 at
generation 1. However, for the rest of generations, we only failed in one estimation at generation 4 (population
p2), and one in generation 5 (population p0), which is acceptable taking into account this guarantee degree.

AndrÈs Cencerrado et al. / Procedia Computer Science 9 (2012) 312 – 320

Population
p0
p1
p2
p3
p4

G1
0.601
0.993
0.433
0.823
0.894

G2
0.376
0.450
0.433
0.332
0.343

G3
0.376
0.450
0.433
0.105
0.343

G4
0.376
0.125
0.433
0.105
0.323

319

G5
0.376
0.096
0.309
0.105
0.323

Table 3: Adjustment errors for populations p0-p4 for the calibration interval ( 0hours - 5hours )
• Estimation with 75% degree of guarantee: in this case, we failed in populations p0, p1, p3 and p4 at the end of
generation 1. At the end of generation 4, we failed in our estimations in the cases of populations p0 and p2. At
the end of generation 5, we failed in three cases: populations p0, p2 and p4. In this speciﬁc case of guarantee
degree we have obtained worse results, since we expected a 25% probability of fail, that is, one or two fails for
a set of ﬁve populations. However, the diﬀerence between the guaranteed error and the ones obtained for those
populations is so small, what leads us to consider that in a greater set of populations, the guarantee degree in
our estimations would ﬁt the results obtained.
These three diﬀerent cases validate our proposal, so we are able to establish, in advance, adjustment error boundaries in our prediction framework. This turns out to be very important for the ﬁnal predictions, since the adjustment
error and ﬁnal prediction error in our two-stage prediction method are highly correlated [5]. Besides, we introduce a
degree of certainty in our future predictions, which is very valuable at the time of making decisions.
Moreover, this methodology allows to assess diﬀerent possibilities regarding the number of generations to perform
in the GA process, which is also very useful, depending on the amount of computational resources and time available
to perform a prediction.
5. Conclusions
Natural hazard management is undoubtedly a relevant application area in which the Computational Science can
play a very important role. In this kind of phenomena, it is usual to have to deal with high degrees of uncertainty on
the input parameters, which may lead us to important losses as regards predictions’ quality.
In this work, we detail our two-stage prediction method by the use of which it has been demonstrated, in previous
works, that we are able to relieve the drawbacks produced by such uncertainty, and therefore, enhancing the quality of
prediction. However, for this strategy to be useful, it is necessary to follow a proper methodology so that we are able
to estimate, in advance, how it will perform.
This work constitutes a part of a project which consists of determining in advance, how a certain combination of
natural hazard simulator, computational resources, adjustment strategy, and frequency of data acquisition will perform,
in terms of execution time and prediction quality.
Since we are dealing in the area of natural hazards management, it is absolutely necessary to be able to assess
in advance the quality of the predictions that will be delivered by means of our prediction framework. This is very
important for the control centers to make the appropriate decisions in each case.
In this work, we focus on the speciﬁc case of forest ﬁres, as a one of the most worrisome natural disasters, and the
experimental studies have been done using the FARSITE simulator.
We have exposed our methodology to characterize a well-known Artiﬁcial Intelligence technique as adjustment
strategy, Genetic Algorithms, which has demonstrated to be a powerful technique to perform the adjustment process
in our two-stage prediction method.
For this purpose, we have carried out a statistical study based on a huge set of simulations. Then, we have identiﬁed
the probability distribution which corresponds to the obtained results, so that we can rely on its probability density
function in order to establish certain degrees of guarantee in our adjustment errors estimations.
Furthermore, this study allows us to make decisions about which speciﬁc setting of the GA is the most appropriate
given time and resource availability restrictions, and it allows us to tackle this problem in diﬀerent ways, by designing
diﬀerent policies to optimize the use of the available computational resources. This constitutes part of our ongoing and

320

AndrÈs Cencerrado et al. / Procedia Computer Science 9 (2012) 312 – 320

future work, and includes, for example, the ability to group fastest simulations in subsets of computational resources,
allocating the slowest ones in other dedicated subsets, according to the speciﬁc needs of each case.
Acknowledgment
This research has been supported by the MICINN-Spain under contract TIN2007-64974.
References
[1] R. C. Rothermel. How to Predict the Spread and Intensity of Forest and Range Fires, USDA FS, Ogden TU, Gen. Tech. Rep. INT-143,
pp. 1–5. 1983.
[2] F. A. Albini. Estimating wildﬁre behavior and eﬀects. Gen. Tech. Rep. INT-GTR-30. Ogden, UT: U.S. Department of Agriculture, Forest
Service, Intermountain Forest and Range Experiment Station. 1976.
[3] B. Abdalhaq, A methodology to enhance the Predction of Forest Fire Propagation, PhD Thesis dissertation. Universitat Aut`onoma de
Barcelona (Spain). June 2004.
[4] K. Wendt, A. Cort´es and T. Margalef, Knowledge-guided Genetic Algorithm for input parameter optimisation in environmental modelling,
Procedia Computer Science 2010, Volume 1, Issue 1, International Conference on Computational Science (ICCS 2010), pp. 1361–1369.
[5] G. Bianchini, A. Cort´es, T. Margalef and E. Luque, Improved Prediction Methods for Wildﬁres Using High Performance Computing A
Comparison, LNCS, Volume 3991, pp. 539–546, 2006.
[6] G. Bianchini, M. Denham, A. Cort´es, T. Margalef and E. Luque, Wildland Fire Growth Prediction Method Based on Multiple Overlapping
Solution, Journal of Computational Science, Volume 1, Issue 4, pp. 229–237. Ed. Elsevier Science. 2010.
[7] R. Rodr´ıguez, A. Cort´es and T. Margalef, Injecting Dynamic Real-Time Data into a DDDAS for Forest Fire Behavior Prediction, Lecture
Notes in Computer Science, Volume 5545, Issue 2, pp. 489–499, 2009.
[8] M. Denham, A. Cort´es A. and T. Margalef, Computational Steering Strategy to Calibrate Input Variables in a Dynamic Data Driven Genetic
Algorithm for Forest Fire Spread Prediction, Lecture Notes in Computer Science, Volume 5545, Issue 2, pp. 479–488, 2009.
[9] M. A. Finney, FARSITE: Fire Area Simulator-model development and evaluation, Res. Pap. RMRS-RP-4, Ogden, UT: U.S. Department of
Agriculture, Forest Service, Rocky Mountain Research Station, 1998.
[10] A. Lopes, M. Cruz and D. Viegas FireStation - An integrated software system for the numerical simulation of ﬁre spread on complex toography.
Environmental Modelling and Software 17, Issue 3, pp. 269–285. 2002.
[11] FIRE.ORG - Public Domain Software for the Wildland ﬁre Community. http://www.ﬁre.org.
[12] F. Darema, Dynamic Data Driven Applications Systems: A New Paradigm for Application Simulations and Measurements, ICCS 2004, LNCS
3038, Springer Berlin / Heidelberg, pp. 662–669. 2004.
[13] Dynamic Data Driven Application Systems homepage. http://www.dddas.org.
[14] P. Beckman, S. Nadella, N. Trebon and I. Beschastnikh, SPRUCE: A System for Supporting Urgent High-Performance Computing, GridBased Problem Solving Environments, Volume 239/2007, pp. 295–311. 2007.
[15] G. Holmes, A. Donkin and I. H. Witten. Weka: A machine learning workbench, Proceedings of the Second Australia and New Zealand
Conference on Intelligent Information Systems, Brisbane, Australia. pp. 357–361. 1994.
[16] J. R. Quinlan. Improved use of continuous attributes in c4.5, Journal of Artiﬁcial Intelligence Research, Volume 4, pp. 77–90. 1996.
[17] R. E. Clark, A. S. Hope, S. Tarantola, D. Gatelli, P. E. Dennison and M. A. Moritz, Sensitivity Analysis of a Fire Spread Model in a Chaparral
Landscape, Fire Ecology, Volume 4, Issue 1, pp. 1–13. 2004.
[18] H. Madsen and F. Jakobsen Cyclone induced storm surge and ﬂood forecasting in the northern Bay of Bengal, Coastal Engineering, Volume
51, Issue 4, pp. 277–296. 2004.
[19] S. D. Aberson, Five-day tropical cyclone track forecasts in the North Atlantic basin, Weather and Forecasting, Volume 13, pp. 1005–1015.
1998.
[20] H. C. Weber, Hurricane Track Prediction Using a Statistical Ensemble of Numerical Models, Monthly Weather Review, Volume 131, pp. 749–
770. 2003.

