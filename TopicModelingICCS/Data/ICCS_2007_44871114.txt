Dynamic Tracking of Facial Expressions Using
Adaptive, Overlapping Subspaces
Dimitris Metaxas, Atul Kanaujia, and Zhiguo Li
Department of Computer Science, Rutgers University
{dnm,kanaujia,zhli}@cs.rutgers.edu

Abstract. We present a Dynamic Data Driven Application System
(DDDAS) to track 2D shapes across large pose variations by learning
non-linear shape manifold as overlapping, piecewise linear subspaces.
The learned subspaces adaptively adjust to the subject by tracking the
shapes independently using Kanade Lucas Tomasi(KLT) point tracker.
The novelty of our approach is that the tracking of feature points is
used to generate independent training examples for updating the learned
shape manifold and the appearance model. We use landmark based shape
analysis to train a Gaussian mixture model over the aligned shapes and
learn a Point Distribution Model(PDM) for each of the mixture components. The target 2D shape is searched by ﬁrst maximizing the mixture
probability density for the local feature intensity proﬁles along the normal followed by constraining the global shape using the most probable
PDM cluster. The feature shapes are robustly tracked across multiple
frames by dynamically switching between the PDMs. The tracked 2D
facial features are used deform the 3D face mask.The main advantage of
the 3D deformable face models is the reduced dimensionality. The smaller
number of degree of freedom makes the system more robust and enables
capturing subtle facial expressions as change of only a few parameters.
We demonstrate the results on tracking facial features and provide several empirical results to validate our approach. Our framework runs close
to real time at 25 frames per second.

1

Introduction

Tracking deformable shapes across multiple viewpoints is an active area of research and has many applications in biometrics, facial expressions analysis and
synthesis for deception, security and human-computer interaction applications.
Accurate reconstruction and tracking of 3D objects require well deﬁned delineation of the object boundaries across multiple views.
Landmark based deformable models like Active Shape Models(ASM)[1]have
proved eﬀective for object shape interpretation in 2D images and have lead
to advanced tools for statistical shape analysis. ASM detects features in the
image by combining prior shape information with the observed image data. A
major limitation of ASM is that it ignores the non-linear geometry of the shape
manifold. Aspect changes of 3D objects causes shapes to vary non-linearly on a
hyper-spherical manifold.
Y. Shi et al. (Eds.): ICCS 2007, Part I, LNCS 4487, pp. 1114–1121, 2007.
c Springer-Verlag Berlin Heidelberg 2007

Dynamic Tracking of Facial Expressions

1115

A generic shape model that would ﬁt any facial expression is diﬃcult to train,
due to numerous possible faces and relative feature locations. In this work we
present a generic framework to learn non-linear shape space as overlapping piecewise linear subspaces and then dynamically adapting the shape and appearance
model to the Face of the subject. We do this by accurately tracking facial features across large head rotations and re-training the model speciﬁc to the subject
using the unseen shapes generated from KLT tracking. We use the Point Distribution Models(PDM) to represent the facial feature shapes and use ASM to
detect them in the 2D image. Our generic framework enables large scale automated training of diﬀerent shapes from multiple viewpoints. The shape model is
composed of the Principal Components that account for most of the variations
arising in the data set. Our Dynamic Data Driven framework continuously collects diﬀerent shapes by tracking feature points independently and adjusts the
principal components basis to customize it for the subject.

2

Related Work

A large segment of research in the past decade has focused on incorporating nonlinear statistical models for learning shape manifold. Murase et. al. [2] showed
that pose from multiple viewpoint when projected onto eigenspaces generates a
2D hypersphere manifold. Gong et. al [3] used non-linear projections onto the
eigenspace to track and estimate pose from multiple viewpoints. Romdhani et al.
[4] proposed an ASM based on Kernel PCA to learn shape variation of face due
to yaw. Several prominent work exist on facial feature registration and tracking, use appearance based models(AAM)[5,6]. [5] uses multiple independent 2D
AAM models to learn correspondences between features of diﬀerent viewpoints.
The most notable work in improving ASM to learn non-linearities in the training
data is by Cootes et. al[7] in which large variation is shapes is captured by parametric Gaussian mixture density, learned in the principal subspace. Unlike [5],
our framework does not require explicit modeling of head pose angles. Although
we use multivariate gaussian mixture model to learn initial clusters of the shape
distribution, our subspaces are obtained by explicitly overlapping the clusters.

3

Learning Shape Manifold

An Active Shape Model(ASM) is a landmark based model that tries to learn
a statistical distribution over variations in shapes for a given class of objects.
Changes in viewpoint causes the object shapes to lie on a hyper-sphere and
cannot be accurately modeled using linear statistical tools.
Face shape variation across multiple aspects is diﬀerent across human subjects. It is therefore inaccurate to use a static model to track facial features
for diﬀerent subjects. Our approach to dynamically specialize the learned shape
manifold to a human subject provides an elegant solution to this problem. However tracking shapes across multiple aspects requires modeling and synthesis of
paths between the source and target shapes lying on a non-linear manifold. In

1116

D. Metaxas, A. Kanaujia, and Z. Li

our framework non-linear region is approximated as a combination of multiple
smaller linear subregions. For the ﬁrst frame, we search the shape subspace iteratively by searching along the normals of the landmark points and simultaneously
constraining it to lie on the shape manifold. The path between the source shape
and the target shape is traversed by searching across multiple subspaces that
constitute the non-linear shape surface. For the subsequent frames, we track the
facial features independent of the prior shape model. The tracked shapes are used
to learn Principal Components of the shape and appearance models that capture
the variations speciﬁc to the human subject face. As a pre-requisite for shape
analysis, all the 2D planar shapes are aligned to the common co-ordinate system using Generalized Procrustes Analysis[8]. The tangent space approximation
Ts projects the shapes on a hyper-plane normal to the mean vector and passing
through it. Tangent space is a linear approximation of the general shape space so
that the Procrustes distance can be approximated as euclidean distance between
the planar shapes. The cluster analysis of shape is done in the global tangent
space. We assume a generative multivariate Gaussian mixture distribution for
both the global shapes and the intensity proﬁle models(IPMs). The conditional
density of the shape Si belonging to an N-class model p(Si |Cluster) =
N

γj (2π)−( 2 ) Cj
N

j=1

−1/2

1
exp{− (Si −(μj +Pj bj ))T Cj −1 (Si −(μj +Pj bj ))} (1)
2

We also assume a diagonal covariance matrix Cj . γj are the cluster weights and
(μj , Pj , bj ) are the mean, eigen matrix and eigen coeﬃcients respectively for the
principle subspace deﬁned for each cluster. The clustering can be achieved by the
EM algorithm with variance ﬂooring to ensure suﬃcient overlapping between the
clusters. For each of the N clusters we learn a locally linear PDM using PCA and
using the eigenvectors to capture signiﬁcant variance in the cluster(98%). The intensity proﬁles for the landmark points also exhibit large variation when trained
over multiple head poses. The change in face aspects causes the proﬁles to vary
considerably for the feature points that are occluded. The multivariate Gaussian
mixture distribution(1) is learned for the local intensity proﬁles model(IPM) in
order to capture variations that cannot be learned using a single PCA model.
Overlapping Between Clusters: It is important that the adjacent clusters
overlap suﬃciently to ensure switching between subspaces during image search
and tracking. We can ensure subspace overlap by using boundary points between
adjacent clusters to learn the subspace for both the clusters. These points can
be obtained as nearest to the cluster center but not belonging to that cluster.

4

Image Search in the Clustered Shape Space

Conventional ASM uses an Alternating Optimization(AO) technique to ﬁt the
shape by searching for the best matched proﬁle along the normal followed by
constraining the shape to lie within the learned subspace. The initial average

Dynamic Tracking of Facial Expressions

1117

Fig. 1. Iterative search across multiple clusters to ﬁt the face. The frames correspond
to iteration 1(Cluster 1), iter. 3(Cluster 5), iter. 17(Cluster 7), iter. 23(Cluster 6) and
ﬁnal ﬁt at iter. 33(Cluster 6) for level 4 of the Gaussian pyramid.

shape is assumed to be in a region near to the target object. We use robust
Viola-Jones face detector to extract a bounding box around the face and use its
dimensions to initialize the search shape. The face detector has 99% detection
rate for faces with oﬀ-plane and in-plane rotation angles ±30o. We assign the
nearest Clusteri to the average shape based on the mahalanobis distance between
the average shape and the cluster centers in the global tangent space. The image
search is initiated at the top most level of the pyramid by searching IPM along
normals and maximizing the mixture probability density (1) of the intensity
gradient along the proﬁle. The model update step shifts the shape to the current
cluster subspace√by truncating the eigen coeﬃcients to lie within the allowable
variance as ±2 λi . The shape is re-assigned the nearest cluster based on the
mahalanobis distance and the shape coeﬃcients are re-computed if the current
subspace is diﬀerent from the previous.
The truncation function to regularize the shapes usually generates discontinuous shape estimates. We use the truncation approach, due to its low computational requirement and faster convergence. The above steps are performed
iteratively and converges irrespective of the initial cluster of the average shape.

5

Dynamic Data Driven Tracking Framework

We track the features independent of the ASM by Sum of Squared Intensity
Diﬀerence(SSID) tracker across consecutive frames[9]. The SSID tracker is a
method for registering two images and computes the displacement of the feature
by minimizing the intensity matching cost, computed over a ﬁxed sized window around the feature. Over a small inter-frame motion, a linear translation
model can be accurately assumed. For an intensity surface at image location
I(xi , yi , tk ), the tracker estimates the displacement vector d = (δxi , δyi ) from
new image I(xi + δx, yi + δy, tk+1 ) by minimizing the residual error over a window W around (xi , yi ) [9].
W

[I(xi + δx, yi + δy, tk+1 ) − g.d − I(xi , yi , tk )] dW

(2)

The inter-frame image warping model assumes that for small displacements of
intensity surface of image window W, the horizontal and vertical displacement
of the surface at a point (xi , yi ) is a function of gradient vector g at that point.

1118

D. Metaxas, A. Kanaujia, and Z. Li
0.5
Cluster 1

Principal Component 2

0.4

Cluster 5
Cluster 6

0.3

Cluster 7
0.2

Frames Shown

0.1
0
−0.1
−0.2
−0.3

−0.2

−0.1

0
0.1
Principal Component 1

0.2

0.3

0.4

Fig. 2. (Best Viewed in Color)Tracking the shapes across right head rotation.(Top)
The cluster projections on 2D space using 2 principal modes(for visualization)and the
bounded by hyper-ellipsoid subspace. The right head rotation causes the shape to vary
across the clusters. The red circles corresponds to the frames 1, 49, 68, 76, 114, 262
and 281. The entire tracking path lies within the subspace spanned by the hyperellipsoids.(Bottom) The images of the tracking result for the frames shown as red
markers in the plot.

The tracking framework generates a number of new shapes not seen during
the training for ASM and hence provides independent data for our dynamic
data driven application systems. Both the appearance (IPMs) and the shape
models are composed of Principal Vector basis that are dynamically updated as
we obtain new shapes and IPMs for the landmark points. For the shape Xi+1 at
time step (i + 1), the covariance matrix Ci , is updated as
Ci+1 = ((N + i) −

K
K
) ∗ Ci +
∗ Xi+1 T Xi+1
N +i
N +i

(3)

where N is the number of training examples and i is the current tracked frame.
The updated covariance matrix Ci+1 is diagonalized using power method to
obtain new set of basis vectors. The subspace corresponding to these basis vectors
encapsulates the unseen shape. The sequence of independent shapes and IPMs for
the landmarks are used to update the current and neighboring subspaces, and the
magnitude of updates can be controlled by the predeﬁned learning rate K. The
number of PCA basis vectors(eigenvectors) may also vary as a result of updation
and specialization of the shape and the appearance model. Fig. 3 illustrates the
applicability of our adaptive learning methodology to extreme facial expressions
of surprise, fear, joy and disgust (not present in training images). For every frame
we align the new shape Yt to the global average shape Xinit and re-assign it to
the nearest Clusteri based on mahalanobis distance. Finally after every alternate
frame we ensure that the shape Yt obtained from tracking is a plausible shape
by constraining the shape to lie on the shape manifold of the current cluster.
Fig. 2 shows the path (projection on 2 principal components) of a shape(and

Dynamic Tracking of Facial Expressions

1119

Fig. 3. 2D Tracking for extreme facial expressions

the corresponding cluster) for a tracking sequence when the subject rotates the
head from frontal to full right proﬁle view and back. The entire path remains
within the plausible shape manifold spanned by the 9 hyper-ellipsoid subspaces.

6

Deformable Model Based 3D Face Tracking

Deformable model based 3D face tracking is the process of estimation, over time,
of the value of face deformation parameters (also known as the state vector of
the system) based on image forces computed from face image sequences. Our
objective is to build a dynamically coupled system that can recover both the
rigid motion and deformations of a human face, without the use of manual
labels or special equipment. The main advantage of deformable face models is
the reduced dimensionality. The smaller number of degree of freedom makes the
system more robust and eﬃcient, and it also makes post-processing tasks, such
as facial expression analysis, more convenient based on recovered parameters.
However, the accuracy and reliability of a deformable model tracking application
is strongly dependent on accurate tracking of image features, which act as 2D
image force for 3D model reconstruction. Low level feature tracking algorithms,
such as optical ﬂows, often suﬀer from occlusion, unrealistic assumptions etc.
On the other hand, model based 2D feature extraction method, such as active
shape model, has been shown to be less prone to image noises and can deal with
occlusions. In this paper, we take advantage of the coupling of the 3D deformable
model and 2D active shape model for accurate 3D face tracking. On the one
hand, 3D deformable model can get more reliable 2D image force from the 2D
active shape model. On the other hand, 2D active shape model will beneﬁt from
the good initialization provided by the 3D deformable model, and thus improve
accuracy and speed of 2D active shape model. The coupled system can handle
large rotations and occlusions. A 3D deformable model is parameterized by a set
of parameters q. Changes in q causes geometric deformations of the model. A
particular point on the surface is denoted by x(q; u) with u ∈ Ω. The goal of a
shape and motion estimation process is to recover parameter q from face image
sequences. To distinguish between shape estimation and motion tracking, the
parameters q can be divided into two parts: static parameter qs , which describes
the unchanging features of a particular face, and dynamic parameter qm , which
describes the global (rotation and translation of the head) and local deformations
(facial expressions) of an observed face during tracking. The deformations can
also be divided into two parts: Ts for shape and Tm for motion (expression), such

1120

D. Metaxas, A. Kanaujia, and Z. Li

Fig. 4. 3D tracking results of deformable mask with large oﬀ-plane head rotations

˙
that x(q; u) = Tm (qm ; Ts (qs ; s(u))) The kinematics of the model is x(u)
=
˙ where L = ∂x
L(q; u)q,
is
the
model
Jacobian.
Considering
the
face
images
∂q
under a perspective camera with focal length f , the point x(u) = (x, y, z)T
projects to the image point xp (u) = fz (x, y)T . The kinematics of the new model
is given by:
x˙p (u) =

∂xp
∂xp
˙
x(u)
=(
L(q; u))q˙ = Lp (q; u)q˙
∂x
∂x

(4)

where the projection Jacobian matrix is
∂xp
f /z 0 −f x/z 2
=
0 f /z −f y/z 2
∂x

(5)

which converts the 2D image forces to 3D forces. Estimation of the model parameters q is based on ﬁrst order Lagrangian dynamics [10], q˙ = fq Where the
generalized forces fq are identiﬁed by the displacements between the actual projected model points and the identiﬁed corresponding 2D image features, which
in this paper are the 2D active shape model points. They are computed as:
(Lp (uj )T fimage (uj ))

fq =

(6)

j

Given an adequate model initialization, these forces will align features on the
model with image features, thereby determining the object parameters. The
dynamic system is solved by integrating over time, using standard diﬀerential
equation integration techniques:
˙
q(t + 1) = q(t) + q(t)Δt

(7)

Goldenstein et. al showed in [11] that the image forces fimage and generalized
forces fq in these equations can be replaced with aﬃne forms that represent
probability distributions, and furthermore that with suﬃciently many image
forces, the generalized force converges to a Gaussian distribution. In this paper,
we take advantage of this property by integrating the contributions of ASMs
with other cues, so as to achieve robust tracking even when ASM methods and
standard 3D deformable model tracking methods provide unreliable results by
themselves.

Dynamic Tracking of Facial Expressions

7

1121

Conclusion

In this work we have presented a real time DDDAS framework for detecting and
tracking deformable shapes across non-linear variations arising due to aspect
changes. Detailed analysis and empirical results have been presented about issues
related to the modeling non-linear shape manifolds using piecewise linear models.
The shape and appearance model updates itself using new shapes obtained from
tracking the feature points. The tracked 2D features are used to deform the 3D
face mask and summarize the facial expressions using only a few parameters.
This framework has many application in face-based deception analysis and we
are in the process of performing many tests based on relevant data.

Acknowledgement
This work has been supported in part by the National Science Foundation under
the following two grants NSF-ITR-0428231 and NSF-ITR-0313184.
Patent Pending
The current technology is protected by patenting and trade marking oﬃce, ”System and Method for Tracking Facial Features,”, Atul Kanaujia and Dimitris
Metaxas, Rutgers Docket 07-015, Provisional Patent #60874, 451 ﬁled December, 12 2006. No part of this technology may be reproduced or displayed in any
form without the prior written permission of the authors.

References
1. Cootes, T.: An Introduction to Active Shape Models. Oxford University Press
(2000)
2. Murase, H., Nayar, S.: Learning and recognition of 3D Objects from appearance.
IJCV (1995)
3. Gong, S., Ong, E.J., McKenna, S.: Learning to associate faces across views in
vector space of similarities to prototypes. BMVC (1998)
4. Romdhani, S., Gong, S., Psarrou, A.: A Multi-View Nonlinear Active Shape Model
Using Kernel PCA. BMVC (1999)
5. Cootes, T., Wheeler, G., Walker, K., Taylor, C.: View-Based Active Appearance
Models. BMVC (2001)
6. Edwards, G.J., Taylor, C.J., Cootes, T.F.: Learning to Identify and Track Faces
in Image Sequences. BMVC (1997)
7. Cootes, T., Taylor, C.: A mixture model for representing shape variation. BMVC
(1997)
8. Goodall, C.: Procrustes methods in the statistical analysis of shape. Journal of
the Royal Statistical Society (1991)
9. Tomasi, C., Kanade, T.: Detection and Tracking of Point Features. Technical
Report CMU-CS-91-132 (1997)
10. Metaxas, D.: Physics-Based Deformable Models: Applications to Computer Vision,
Graphics and Medical Imaging. Kluwer Academic Publishers (1996)
11. Goldenstein, S., Vogler, C., Metaxas, D.: Statistical Cue Integration in DAG Deformable Models. PAMI (2003)

