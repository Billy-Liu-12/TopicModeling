Information Granulation-Based Multi-layer Hybrid
Fuzzy Neural Networks: Analysis and Design
1

1

2

Byoung-Jun Park , Sung-Kwun Oh , Witold Pedrycz , and Tae-Chon Ahn

1

1

School of Electrical, Electronic and Information Engineering,
Wonkwang University, Korea
{lcap, ohsk, tcahn}@wonkwang.ac.kr
2
Department of Electrical and Computer Engineering, University of Alberta,
Edmonton, AB T6G 2G6, Canada
and Systems Research Institute, Polish Academy of Sciences, Warsaw, Poland
pedrycz@ee.ualberta.ca

Abstract. In this study, a new architecture and comprehensive design methodology of genetically optimized Hybrid Fuzzy Neural Networks (gHFNN) are
introduced and a series of numeric experiments are carried out. The gHFNN architecture results from a synergistic usage of the hybrid system generated by
combining Fuzzy Neural Networks (FNN) with Polynomial Neural Networks
(PNN). FNN contributes to the formation of the premise part of the overall
network structure of the gHFNN. The consequence part of the gHFNN is designed using PNN.

1 Introductory Remarks
Efficient modeling techniques should allow for a selection of pertinent variables and a
formation of highly representative datasets. The models should be able to take advantage of the existing domain knowledge and augment it by available numeric data
to form a coherent data-knowledge modeling entity. The omnipresent modeling tendency is the one that exploits techniques of Computational Intelligence (CI) by embracing fuzzy modeling [1-6], neurocomputing [7], and genetic optimization [8].
In this study, we develop a hybrid modeling architecture, called genetically optimized Hybrid Fuzzy Neural Networks (gHFNN). In a nutshell, gHFNN is composed
of two main substructures driven to genetic optimization, namely a fuzzy set-based
fuzzy neural network (FNN) and a polynomial neural network (PNN). From a standpoint of rule-based architectures, one can regard the FNN as an implementation of the
antecedent part of the rules while the consequent is realized with the aid of a PNN.
The role of the FNN is to interact with input data, granulate the corresponding input
spaces. In the first case (Scheme I) we concentrate on the use of simplified fuzzy inference. In the second case (Scheme II), we take advantage of linear fuzzy inference.
The role of the PNN is to carry out nonlinear transformation at the level of the fuzzy
sets formed at the level of FNN. The PNN that exhibits a flexible and versatile structure [9] is constructed on a basis of Group Method of Data Handling (GMDH [10])
M. Bubak et al. (Eds.): ICCS 2004, LNCS 3037, pp. 188–195, 2004.
© Springer-Verlag Berlin Heidelberg 2004

Information Granulation-Based Multi-layer Hybrid Fuzzy Neural Networks

189

method and genetic algorithms (GAs). The design procedure applied in the construction of each layer of the PNN deals with its structural optimization involving the selection of optimal nodes (polynomial neurons; PNs) with specific local characteristics
(such as the number of input variables, the order of the polynomial, and a collection of
the specific subset of input variables) and addresses specific aspects of parametric
optimization. To assess the performance of the proposed model, we exploit a wellknown time series data. Furthermore, the network is directly contrasted with several
existing intelligent models.

2 Conventional Hybrid Fuzzy Neural Networks (HFNN)
The architectures of conventional HFNN [11,12] result as a synergy between two
other general constructs such as FNN and PNN. Based on the different PNN topologies, the HFNN distinguish between two kinds of architectures, namely basic and
modified architectures. Moreover, for the each architecture we identify two cases. In
the connection point, if input variables to PNN used on the consequence part of
HFNN are less than three (or four), the generic type of HFNN does not generate a
highly versatile structure. Accordingly we identify also two types as the generic and
advanced. The topologies of the HFNN depend on those of the PNN used for the
consequence part of HFNN. The design of the PNN proceeds further and involves a
generation of some additional layers. Each layer consists of nodes (PNs) for which
the number of input variables could the same as in the previous layers or may differ
across the network. The structure of the PNN is selected on the basis of the number of
input variables and the order of the polynomial occurring in each layer.

3 The Architecture and Development of Genetically Optimized
HFNN (gHFNN)
The gHFNN emerges from the genetically optimized multi-layer perceptron architecture based on fuzzy set-based FNN, GAs and GMDH. These networks result as a
synergy between two other general constructs such as FNN [13] and PNN [9].
3.1 Fuzzy Neural Networks and Genetic Optimization
We use FNN based on two types of fuzzy inferences, that is, simplified (Scheme I)
and linear fuzzy inference-based FNN (Scheme II) as shown in Fig. 1. The notation
used in Fig.1 requires some clarification. The “circles” denote units of the FNN while
“N” identifies a normalization procedure applied to the membership grades of the
input variable xi. The output of the “∑” neuron is described by a nonlinear function
fi(xi). Finally, the output of the FNN is governed by the following expression.
m

yˆ = f1 ( x1 ) + f 2 ( x2 ) + " + f m ( xm ) = ∑ f i ( xi )
i =1

(1)

190

B.-J. Park et al.
Layer 2 Layer 3
Layer 1

µ1j

Layer 2
Layer 1

Layer 5

N
N

x1

Layer 4

w1j

Connection point 2
∑

f1(x1)

ws11
1

wij

∑

fi(xi)

∑

y^

xm

µij
xi

N
N
N

wmj

∑ f (x )
m m

Connection point 1

(a) Scheme I; Simplified fuzzy inference

1

Layer 5

N

∏

N

∏

∑ f1(x1)

Connection point 2

Cy11

Layer 6

∑

∑

w12

N

µmj

ws12

Layer 4
Layer 3

w11

Layer 6

N
N

xi

µ12

x1

N

µij

µ11

N

∏

N

∏

wsij

∑

wij

∑

∑

Cy12

y^

∑ fi(xi)

Cyij

Connection point 1

(b) Scheme II; Linear fuzzy inference

Fig. 1. Topologies of fuzzy set-based FNN

We can regard each fi(xi) given by (1) as the following mappings (rules).
Scheme I – Rj : If xi is Aij then Cyij=wij
(2)
Scheme II - Rj : If xi is Aij then Cyij=wsij + wij xi Rj
(3)
Rj is the j-th fuzzy rule while Aij denotes a fuzzy variable of the premise of the
fuzzy rule and represents a membership function µij. wij is a constant in (2), and wsij is
a constant and wij is an input variable consequence of the fuzzy rule in (3). They express a connection existing between the neurons as visualized in Fig. 1. Mapping from
xi to fi(xi) in (2) is determined by the fuzzy inferences and a standard defuzzification.
z
z
(4)
f i ( xi ) = ∑ j =1 µij ( xi ) ⋅ wij ∑ j =1 µij ( xi )
The learning of FNN is realized by adjusting connections of the neurons and as
such it follows a standard Back-Propagation (BP) algorithm [14]. For the simplified
fuzzy inference-based FNN, the update formula of a connection in Scheme I is as
follow.
∆wij = 2 ⋅η ⋅ ( y p − yˆ p ) ⋅ µij ( xi ) + α ( wij (t ) − wij (t − 1))
(5)
Where, yp is the p-th target output data, yˆ p stands for the p-th actual output of the
model for this specific data point, η is a positive learning rate and α is a momentum
coefficient constrained to the unit interval. The inference result and the learning algorithm in linear fuzzy inference-based FNN use the mechanisms in the same manner as
discussed above.
Genetic algorithms (GAs) are optimization techniques based on the principles of
natural evolution. In essence, they are search algorithms that use operations found in
natural genetics to guide a comprehensive search over the parameter space [8]. In
order to enhance the learning of the FNN and augment its performance of a FNN, we
use GAs to adjust learning rate, momentum coefficient and the parameters of the
membership functions of the antecedents of the rules.
3.2 Genetically Optimized PNN (gPNN)
When we construct PNs of each layer in the conventional PNN [9], such parameters
as the number of input variables (nodes), the order of polynomial, and input variables
available within a PN are fixed (selected) in advance by the designer. This could have

Information Granulation-Based Multi-layer Hybrid Fuzzy Neural Networks

191

frequently contributed to the difficulties in the design of the optimal network. To
overcome this apparent drawback, we introduce a new genetic design approach; especially as a consequence we will be referring to these networks as genetically optimized PNN (to be called “gPNN”). The overall genetically-driven optimization process of PNN is shown in Fig. 2.
Configuration of input variables for consequence part
& initial information concerning GAs and gPNN
Initialization of population

GAs

Generation of a PN by a
chromosome in population
Reproduction
Roulette-wheel selection
One-point crossover
Invert mutation

Evaluation of PNs(Fitness)
x1 = z1, x2 = z2, ..., xW = zW
The outputs of the preserved PNs
serve as new inputs to the next
layer

Elitist strategy &
Selection of PNs(W)
No

Stop
condition
Yes

Generate a layer of gPNN
A layer consists of optimal PNs
selected by GAs
Stop
condition

No

Yes

gPNN
gPNN is organized by GMDH and
GAs
END

Fig. 2. Overall genetically-driven optimization process of PNN

4 The Algorithms and Design Procedure of gHFNN
The premise of gHFNN: FNN (Refer to Fig. 1)
[Layer 1] Input layer.
[Layer 2] Computing activation degrees of linguistic labels.
[Layer 3] Normalization of a degree activation (firing) of the rule.
[Layer 4] Multiplying a normalized activation degree of the rule by connection. If we
choose Connection point 1 for combining FNN with gPNN as shown in Fig. 1, aij is
given as the input variable of the gPNN.
aij = µ ij × Cyij = µ ij × Cyij
(6)
Simplified : Cyij = wij

 Linear : Cyij = wsij + wij ⋅ xi

(7)

[Layer 5] Fuzzy inference for the fuzzy rules. If we choose Connection point 2, fi is
the input variable of gPNN.
[Layer 6; Output layer of FNN] Computing output of a FNN.
The consequence of gHFNN: gPNN (Refer to Fig. 2)
[Step 1] Configuration of input variables.
If we choose the first option (Connection point 1), x1=a11, x2=a12,…, xn=aij (n=i×j).
For the second option (Connection point 2), we have x1=f1, x2=f2,…, xn=fm (n=m).

192

B.-J. Park et al.

[Step 2] Decision of initial information for constructing the gPNN.
[Step 3] Initialization of population.
[Step 4] Decision of PNs structure using genetic design. We divide the chromosome
to be used for genetic optimization into three sub-chromosomes as shown in Fig. 4(a).
Selection of node(PN) structrue by chromosome

i) Bits for the selection of
the no. of input variables

Related bit items

Bit structure of subchromosome divided
for each item

1

0

1

0

Decoding
(Decimal)

Genetic
Design

1

ii) Bits for the selection
of the polynomial order

1

Decoding
(Decimal)

Normalization
(less than
Max)

Normalization
(1 ~ 3)

Selection of
the order of
polynomial

Selection of
no. of input
variables(r)

1

1

iii) Bits for the selection
of input variables

0

1

1 1 0

1

1

1 1 1

1

r

Decoding
(Decimal)

Decoding
(Decimal)

Normalization
(1 ~ n(or W))

Normalization
(1 ~ n(or W))

Decision of
input variables

Decision of
input variables

Selection of input variables

(Type 1~Type 3)

PN

Selected PN

Fig. 3. The PN design using genetic optimization
Table 1. Different forms of regression polynomial forming a PN
Number of inputs
Order of the polynomial
1 (Type 1)
2 (Type 2)
2 (Type 3)

2

3

4

Bilinear
Biquadratic-1
Biquadratic-2

Trilinear
Triquadratic-1
Triquadratic-2

Tetralinear
Tetraquadratic-1
Tetraquadratic-2

[Step 5] Evaluation of PNs.
[Step 6] Elitist strategy and selection of PNs with the best predictive capability.
[Step 7] Reproduction.
[Step 8] Repeating Step 4-7.
[Step 9] Construction of their corresponding layer.
[Step 10] Check the termination criterion (performance index).
E ( PI or EPI ) =

1 n
∑ ( y p − yˆ p )2
n p=1

(8)

[Step 11] Determining new input variables for the next layer.
The gPNN algorithm is carried out by repeating Steps 4-11.

5 Experimental Studies
The performance of the gHFNN is illustrated with the aid of a time series of gas furnace [14]. The delayed terms of methane gas flow rate, u(t) and carbon dioxide density, y(t) are used as system input variables. We utilizes 3 system input variables such
as u(t-2), y(t-2), and y(t-1). The output variable is y(t).

Information Granulation-Based Multi-layer Hybrid Fuzzy Neural Networks

193

Table 2. Performance index of gHFNN for the gas furnace
Fuzzy
Inference

Premise part
No. of rules
PI
(MFs)

EPI

CP

Consequence part
No. of
Layer
Input No.
inputs
1
2
3
4
5
1
2
3
4
5
1
2
3
4
5
1
2
3
4
5

01
Simplified

6
(2+2+2)

0.0248 0.126
02

01
Linear

6
(2+2+2)

0.0256 0.143
02

u(t-2)
1

y(t-2)
1

N

∏

N

∏

4
4
4
4
4
3
4
4
4
4
4
4
3
4
3
3
4
2
4
4

1

1
26
25
28
28
2
15
11
23
7
1
16
26
1
21
3
13
·
11
26

3
3
16
26
1
·
13
17
27
26
5
30
·
13
·
·
6
·
5
25

3
2
3
2
2
3
2
2
1
3
3
3
1
1
3
3
2
2
1
1

EPI

0.0220
0.0209
0.0205
0.0190
0.0175
0.0221
0.0194
0.0190
0.0188
0.0182
0.0218
0.0197
0.0196
0.0193
0.0191
0.0232
0.0196
0.0194
0.0188
0.0184

0.135
0.135
0.131
0.128
0.125
0.135
0.126
0.116
0.114
0.112
0.136
0.124
0.121
0.119
0.117
0.130
0.120
0.115
0.113
0.110

PN3
2 3
PN4
2 3

N

∏

N

∏

∑

PN6
3 2

N

∏

N

∏

PN19
4 2
PN6
2 2

PN8
2 1

∑
∑

∑
∑

2
28
1
2
6
3
2
2
6
12
3
24
16
24
18
2
15
30
21
3

PI

∑

∑
∑

PN11
2 2

y(t-1)

5
15
5
1
18
2
11
28
2
1
6
6
4
22
11
1
12
19
2
13

T

∑

yˆ

PN30
4 2

PN15
2 2
PN17
1 3

Fig. 4. Optimal topology of genetically optimized HFNN for the gas furnace (in case of linear
fuzzy inference)

Table 2 summarizes the results of the optimized architectures according to connection
points based on each fuzzy inference method. In Table 2, the values of the performance index of output of the gHFNN depend on each connection point based on the
individual fuzzy inference methods. The optimal topology of gHFNN is shown in Fig.
4. Fig. 5 illustrates the optimization process by visualizing the performance index in
successive cycles. It also shows the optimized network architecture when taking into
consideration gHFNN based on linear fuzzy inference and connection point (CP) 2,
refer to Table 2. Table 3 contrasts the performance of the genetically developed network with other fuzzy and fuzzy-neural networks studied in the literatures.

194

B.-J. Park et al.
0.8
: PI
: E_PI

0.7

Performance Index

Consequence part;
gPNN

1st layer

0.6

2nd layer

0.5

Premise part;
FS_FNN

3rd layer

0.4

4th layer
5th layer

0.3
0.2
E_PI=0.143
E_PI=0.115

E_PI=0.110

0.1
PI=0.0256

0

200
Iteration

400

500

PI=0.0194

150

300
450
Generation

PI=0.0184

600

750

Fig. 5. Optimization procedure of gHFNN by BP learning and GAs
Table 3. Comparison of performance with other modeling methods
Model

PI

EPI

No. of rules

Box and Jenkin’s model [14]
Pedrycz’s model [1]
Xu and Zailu’s model [2]
Sugeno and Yasukawa's model [3]
Kim, et al.'s model [15]
Lin and Cunningham's mode [16]
Simplified
Complex [4]
Linear
Simplified
Hybrid [6]
Fuzzy
(GAs+Complex)
Linear
Simplified
HCM+GAs [5]
Linear
Simplified
FNN [13]
Linear
Generic [11]
SOFPNN
Advanced [12]

0.710
0.320
0.328
0190
0.034
0.071
0.024
0.023
0.024
0.017
0.022
0.020
0.043
0.037
0.017
0.019
0.018
0.018
0.015
0.018

0.244
0.261
0.328
0.306
0.329
0.289
0.333
0.264
0.264
0.273
0.250
0.264
0.254
0.112
0.258
0.110

2
4
4(2×2)
4(2×2)
4(2×2)
4(2×2)
6(3×2)
6(3×2)
6(3+3)
6(3+3)
4 rules/5th layer
6 rules/5th layer
4 rules/5th layer
6 rules/5th layer
4 rules/5th layer
6 rules/5th layer

Proposed model
(gHFNN)

Simplified
Linear

6 Concluding Remarks
The comprehensive design methodology comes with the parametrically as well as
structurally optimized network architecture. 1) As the premise structure of the
gHFNN, the optimization of the rule-based FNN hinges on genetic algorithms and
back-propagation (BP) learning algorithm: The GAs leads to the auto-tuning of vertexes of membership function, while the BP algorithm helps obtain optimal parameters of the consequent polynomial of fuzzy rules through learning. And 2) the gPNN
that is the consequent structure of the gHFNN is based on the technologies of the
extended GMDH and GAs: The extended GMDH is comprised of both a structural
phase such as a self-organizing and evolutionary algorithm, and a parametric phase of

Information Granulation-Based Multi-layer Hybrid Fuzzy Neural Networks

195

least square estimation-based learning, moreover the gPNN architecture is driven to
genetic optimization, in what follows it leads to the selection of the optimal nodes. In
the sequel, a variety of architectures of the proposed gHFNN driven to genetic optimization have been discussed. The experiments helped compare the network with
other intelligent models – in all cases the previous models came with higher values of
the performance index.
Acknowledgement. This work was supported by Korea Research Foundation Grant
(KRF-2003-002-D00297)

References
1.
2.
3.
4.
5.

6.
7.

8.
9.
10.
11.

12.

13.
14.
15.
16.

W. Pedrycz: An identification algorithm in fuzzy relational system. Fuzzy Sets and Systems, 13 (1984) 153-167
C. W. Xu, Y. Zailu: Fuzzy model identification self-learning for dynamic system. IEEE
Trans. on Syst. Man, Cybern., SMC-17(4) (1987) 683-689
M. Sugeno, T. Yasukawa: A Fuzzy-Logic-Based Approach to Qualitative Modeling.
IEEE Trans. Fuzzy Systems, 1(1) (1993) 7-31
S.-K. Oh, W. Pedrycz: Fuzzy Identification by Means of Auto-Tuning Algorithm and Its
Application to Nonlinear Systems. Fuzzy Sets and Systems, 115(2) (2000) 205-230,.
B.-J. Park, W. Pedrycz, S.-K. Oh: Identification of Fuzzy Models with the Aid of Evolutionary Data Granulation. IEE Proceedings-Control theory and application, 148(5) (2001)
406-418
S.-K. Oh, W. Pedrycz, B.-J. Park: Hybrid Identification of Fuzzy Rule-Based Models.
International Journal of Intelligent Systems, 17(1) (2002) 77-103
K. S. Narendra, K. Parthasarathy: Gradient Methods for the Optimization of Dynamical
Systems Containing Neural Networks. IEEE Transactions on Neural Networks, 2 (1991)
252-262
Z. Michalewicz: Genetic Algorithms + Data Structures = Evolution Programs. (1996)
Springer-Verlag, Berlin Heidelberg
S.-K. Oh, W. Pedrycz, B.-J. Park: Polynomial Neural Networks Architecture: Analysis
and Design. Computers and Electrical Engineering, 29(6) (2003) 653-725
A. G. Ivahnenko: The group method of data handling: a rival of method of stochastic
approximation. Soviet Automatic Control, 13(3) (1968) 43-55
B.-J. Park, D.-Y. Lee, S.-K. Oh: Rule-Based Fuzzy Polynomial Neural Networks in Modeling Software Process Data. International Journal of Control, Automation and Systems,
1(3) (2003) 321-331
H.-S. Park, S.-K. Oh: Rule-based Fuzzy-Neural Networks Using the Identification Algorithm of GA hybrid Scheme. International Journal of Control, Automation and Systems,
1(1) (2003) 101-110
S.-K. Oh, W. Pedrycz, H.-S. Park: Hybrid Identification in Fuzzy-Neural Networks.
Fuzzy Sets and Systems, 138(2) (2003) 399-426
D. E. P. Box, G. M. Jenkins: Time Series Analysis, Forecasting, and Control, 2nd edition
(1976) Holden-Day, SanFrancisco
E. Kim, H. Lee, M. Park, M. Park: A Simply Identified Sugeno-type Fuzzy Model via
Double Clustering. Information Sciences, 110 (1998) 25-39
Y. Lin, G. A. Cunningham III: A new Approach to Fuzzy-neural Modeling. IEEE Transaction on Fuzzy Systems, 3(2) 190-197

