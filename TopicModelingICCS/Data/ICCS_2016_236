Procedia Computer Science
Volume 80, 2016, Pages 1692–1701
ICCS 2016. The International Conference on Computational
Science

GPU-Accelerated Extreme Learning Machines for
Imbalanced Data Streams with Concept Drift
Bartosz Krawczyk
Department of Systems and Computer Networks,
Wroclaw University of Technology,
Wyb. Wyspia´
nskiego 27, 50-370 Wroclaw, Poland.
bartosz.krawczyk@pwr.edu.pl

Abstract
Mining data streams is one of the most vital ﬁelds in the current era of big data. Continuously
arriving data may pose various problems, connected to their volume, variety or velocity. In
this paper we focus on two important diﬃculties embedded in the nature of data streams: nonstationary nature and skewed class distributions. Such a scenario requires a classiﬁer that is
able to rapidly adapt itself to concept drift and displays robustness to class imbalance problem.
We propose to use online version of Extreme Learning Machine that is enhanced by an eﬃcient
drift detector and method to alleviate the bias towards the majority class. We investigate three
approaches based on undersampling, oversampling and cost-sensitive adaptation. Additionally,
to allow for a rapid updating of the proposed classiﬁer we show how to implement online Extreme
Learning Machines with the usage of GPU. The proposed approach allows for a highly eﬃcient
mining of high-speed, drifting and imbalanced data streams with signiﬁcant acceleration oﬀered
by GPU processing.
Keywords: Data streams, Imbalanced data, Concept drift, Big data, Extreme learning machines, GPU.

1

Introduction

Contemporary computer systems store and process enormous amounts of data. Current predictions point out that the volume of stored information will be doubling every two years. E-mails,
social webs, on-line shopping etc. produce ever-growing data, which may carry valuable hidden
information. Therefore, three main issues in big data analytics must be addressed (known as
3Vs: Volume, Velocity, Variety): how to eﬃciently transfer such volumes [14], how to store
them and how to extract meaningful knowledge from them [15, 21]. In this work we are mainly
focusing on the Velocity, because designing big data analytical tools must take into consideration that most of data in modern systems arrives continuously [7] as so-called data stream [9].
Furthermore, the nature and characteristics of data, i.e., statistical dependencies characterizing
an analyzed data stream, can change. A special analytical model which can cope with and
1692

Selection and peer-review under responsibility of the Scientiﬁc Programme Committee of ICCS 2016
c The Authors. Published by Elsevier B.V.

doi:10.1016/j.procs.2016.05.509

GPU Extreme Learning Machines for Imbalanced Data Streams with Concept Drift

B. Krawczyk

adapt to such non-stationary characteristics is required in such cases [11]. This problem can be
connected with the skewed class distributions, leading to scenario when not only properties of
object changes but also ratios between class distributions [16].
Among a plethora of solutions dedicated to data stream mining with concept drift we may
distinguish the following approaches: (i) based on external module for detecting shift and drifts
in the stream [3], (ii) based on adaptive classiﬁers using windowing technique [17], (iii) based on
online classiﬁers that are able to process stream sample-by-sample [10], and (iv) using classiﬁer
ensembles [18, 20, 19].
In this work we propose a new model based on online Extreme Learning Machines that is
able to handle imbalanced and drifting data streams. Extreme Learning Machines are selected
due to their low computational complexity which makes them suitable for mining high-speed
data. However, to achieve even more rapid training and update of our classiﬁer we propose
a GPU-based implementation of the online classiﬁer that is able to signiﬁcantly speed-up the
entire process. This makes our classiﬁer suitable for big data stream mining [2]. In order to
capture the changes in arriving data we employ an eﬃcient ADWIN drift detector that decides
whether to update the current model or discard it and train new one from scraps. To alleviate
the problem of skewed distributions we propose three approaches utilizing either data-level or
cost-sensitive techniques.
Experiments carried out on a set of big, imbalanced and evolving streams prove the high
quality of our proposal for adapting online Extreme Learning Machines for this task and show
that GPU-based computing allows for a rapid analysis of data streams.

2

GPU-accelerated extreme learning machines for imbalanced drifting streams

In this section details of the proposed GPU-based Extreme Learning classiﬁers for imbalanced
and drifting data streams will be given.

2.1

Extreme learning machines

Extreme Learning Machines (ELMs) [8] are random-based single-layer feedforward neural networks trained in a randomized manner in order to reduce their computational complexity. In
last two decades there was a number of signiﬁcant developments in the ﬁeld of neural network
training algorithms. However, most of these approaches suﬀered from the extended computational time required for eﬀective execution and a large number of parameters to be set. ELMs
are among emerging trends in neural network learning that aim at alleviating the training complexities with the usage of randomly drawn weights for neurons in the hidden layer. One must
note here that despite the emerging popularity of ELMs-based approaches this concept can be
actually traced further down in the literature to the proposals of Random Vector Functional
Link [13].
Let us now present the concept of ELMs. Let us assume to have n labeled objects in a ddimensional feature space and a set of M class labels. A single-layer feedforward neural network
with N hidden neurons can be described by the following equation:
N

Bi f (wi · x + bi ),

y=

(1)

i=1

1693

GPU Extreme Learning Machines for Imbalanced Data Streams with Concept Drift

B. Krawczyk

where f () represents the activation function, x is the considered object, wi stand for input
weights associated with i-th hidden neuron, bi is the bias of i-th hidden neuron and Bi are
weights assigned to output neurons.
When considering all of n training points we use this equation in a matrix form:
Y = HB,
where H is the matrix storing outputs of hidden layer
⎛
f (w1 · x1 + b1 ) f (w2 · x1 + b2 )
⎜f (w1 · x2 + b1 ) f (w2 · x2 + b2 )
⎜
H=⎜
..
..
⎝
.
.
f (w1 · xn + b1 )

f (w2 · xn + b2 )

(2)
for each input object:
···
···
..
.

⎞
f (wN · x1 + bN )
f (wN · x2 + bN )⎟
⎟
⎟,
..
⎠
.

···

f (wN · xn + bN )

(3)

and B = (B1 , B2 , · · · BN )T and Y = (y1 , y2 , · · · yn ). To calculate weights assigned to outputs
B we need to compute the Moore-Penrose generalized inverse of the matrix H that will be
denoted as H−1 .
One must note that due to the random nature of ELMs we may face a high variance in their
behavior. To counter this drawback one may use regularization, which has been reported as
having crucial eﬀect on the quality of ELMs.
In order to regularize ELMs we use an orthogonal projection to get the Moore-Penrose
pseudoinverse of H:
∗
(4)
H−1 = (HT H)−1 HT
where HT is a transposed matrix H. This allows us to add a ridge parameter λ1 to the diagonal of (HT H), which is known as the ridge-regression regularization approach [5]. Applying
it leads to obtaining a more stable solution.
After regularization one can calculate the matrix of output weights in step 3 of ELMs
training according to:
B=

I
+ HT H
λ

−1

HT Y,

(5)

where I is an identity matrix of equal size to H.
The ELM algorithm is summarized in a pseudo-code form in Algorithm 1.
Algorithm 1 Extreme Learning Machine with regularization
1: Randomly generate the bias matrix b = (b1 , b2 , · · · bN )T
2: Randomly generate the weight matrix W = (w1 , w2 , · · · wN )T
3: Calculate H using Eq. 3
4: Calculate Moore-Penrose pseudoinverse of H using Eq. 4
5: Calculate the matrix of output weights B using Eq. 5

2.2

Online extreme learning machines

Due to their low computational complexity, good generalization abilities and short response
time ELMs have captured the attention of data stream community, proving themselves to be
suitable for managing streaming and evolving objects [12]. We will present the background
1694

GPU Extreme Learning Machines for Imbalanced Data Streams with Concept Drift

B. Krawczyk

of online ELMs (oELMs) that are able to eﬃciently update their structure in either batch or
sample-by-sample manner [10].
This algorithm is based on Recursive Least Squares approach [6] for sequentially updating
H. The oELMs proceeds in two steps. Firstly an initial ELM model is being trained. This
requires a labeled training set representative to the analyzed stream. The size of this sample
depends on the application of availability of labeled examples. however, it was proven that
ELMs can initialize themselves even with small-sample training sets. This step uses standard
ELM training procedure. In the second step ELM enters the online phase that will allow it to
update its structure using incoming examples or data chunks. In accordance with incremental
learning principles each example will be processed only once and after being processed by oELM
it can be discarded. This reduces both computational complexity and storage requirements,
two important factors in stream mining.
The oELM algorithm is summarized in a pseudo-code form in Algorithm 2.
Algorithm 2 Online Extreme Learning Machine
1: T R0 ← starting labeled training set
2:
3:
4:
5:
6:
7:
8:
9:
10:
11:
12:
13:
14:
15:

procedure Initialize(T R0 )
Randomly generate the bias matrix b = (b1 , b2 , · · · bN )T
Randomly generate the weight matrix W = (w1 , w2 , · · · wN )T
Calculate starting H0 using Eq. 3
M0 = (HT0 H0 )−1
Calculate the starting matrix of output weights B0 = M0 H−1
0 Y0
k←0
end procedure

procedure Update
while stream = TRUE do
k ←k+1
Get new object xk
16:
hk = [f (w1 · xk + b1 ), · · · , f (wN · xk + bN )]T
18:
19:
20:

Mk−1 hk hT
k Mk−1
1+hT
k Mk−1 hk
Bk−1 + Mk hk ykT − hTk Bk−1

Mk = Mk−1

17:

Bk =
end while
end procedure

2.3

Schemes for handling imbalanced and drifting streams

In this paper we discuss two diﬃculties embedded in the nature of data stream mining: nonstationary nature of data and skewed class distributions. Each of these factors individually
poses a signiﬁcant challenge to the pattern classiﬁcation systems. But when combined they
may lead to a highly challenging task. Discussed oELMs are suitable for processing high-speed
data streams, but display no robustness to either class imbalance or concept drift. Therefore, to
obtain an eﬃcient classiﬁcation system we must augment them with procedures that will allow
to compensate for changing and skewed environment. Let us discuss in details the proposed
components of our framework:
1695

GPU Extreme Learning Machines for Imbalanced Data Streams with Concept Drift

B. Krawczyk

Concept drift. In order to detect changes in the stream we either require a self-adaptive
learner or an external module known as drift detector. In this work we will use the eﬃcient
Adaptive Data Stream Sliding Window (ADWIN) drift detector [3]. It maintains a variablelength window of arriving examples, where the window size is directly related to the hypothesis
that were no changes in the average values stored within the window. The only parameter
used by ADWIN is a conﬁdence bound, which points to how conﬁdent the user needs to be in
the output of the algorithm. This is a standard parameter used in all methods dedicated to
random processes. Additional advantage of ADWIN is its reduced computational complexity.
It does not store the entire window of objects, but instead compresses it using the exponential
histogram technique. For a window of length L ADWIN requires has only O(logW ) time and
memory complexities per analyzed object.
Imbalanced data. In order to handle the skewed distribution between classes we propose to
investigate three approaches based on data modiﬁcation and classiﬁcation error:
• Undersampling (oELMunder ). In this strategy each obtained chunk of data is balanced
by a random undersampling of the majority class. This solution has a low computational
cost and reduced memory cost by shrinking the size of processed chunk. However, due to
its random nature we can discard useful objects from the majority class.
• Oversampling (oELMover ). In this strategy each obtained chunk of data is balanced by
a random oversampling of the minority class. This approach preserves all objects from
both classes, artiﬁcially boosting the quantity of underrepresented group. This way we
do not have a risk of removing valuable samples. However, this also leads to an increased
memory consumption and processing costs as we increase the size of the processed chunk.
• Cost-sensitive (oELMcost ). In this approach we use the moving threshold paradigm of
neural networks. In binary imbalanced problem the continuous outputs of two neurons
in the ﬁnal layer of oELM for the object x can be denoted as ymaj (x) and ymin (x),
where ymaj (x) + ymin (x) = 1 and both outputs are bounded within [0,1]. We may
apply the cost-sensitive modiﬁcation of minority class output to counter the bias towards
∗
∗
the majority class. New output ymin
(x) is computed as ymin
(x) = ηymin Cost[min, maj],
where Cost[min, maj] is the cost penalty and η is a normalization parameter such that the
new output is still bounded within [0,1]. This way we boost the recognition of the minority
class without any data modiﬁcations or imposing additional computational costs. Output
is adjusted during the classiﬁcation phase. In order to calculate the Cost[min, maj] and
adapt it to changes in data we propose for it be equal to the imbalance ratio (IR) from
the previous chunk.
Please note that we do not used more sophisticated preprocessing techniques due to their
increased computational complexities. We aim at real-time classiﬁcation of incoming chunks
and therefore at reducing as much as possible the processing time required for it.
The proposed method is summarized in a pseudo-code form in Algorithm 3.

2.4

GPU-based oELM acceleration

Fast data processing is crucial when dealing with real-time data streams. Therefore, oELMs
are an attractive proposition due to their signiﬁcantly reduced computational complexity in
comparison to standard neural networks or many reference classiﬁers. However, in order to
process massive and changing streaming data we require rapid machine learning methods taking
advantage of modern computing environments. Therefore, we decided to focus our attention on
1696

GPU Extreme Learning Machines for Imbalanced Data Streams with Concept Drift

B. Krawczyk

Algorithm 3 Online Extreme Learning Machine for imbalanced and drifting data streams
1: DS 0 ← starting labeled training set
2: i ← 0
3: Train initial oELM according to Algorithm 2.INITIALIZE and selected imbalance handling
method
4:
5:
6:
7:
8:
9:
10:
11:
12:
13:
14:
15:
16:
17:
18:
19:
20:
21:
22:
23:
24:
25:

while stream = TRUE do
i←i+1
Get new data chunk DS i
if cost-sensitive = TRUE then
∗
Classify DS i using oELM with ymin
and ymaj
else
Classify DS i using oELM with ymin and ymaj
end if
Obtain true labels for DS i
if undersampling = TRUE ∨ oversampling = TRUE then
DS i ← undersampling(DS i ) ∨ oversampling(DS i )
end if
if ADWIN detects signiﬁcant error change in oELM then
Rebuild classiﬁer according to Algorithm 2.INITIALIZE using only DS i
else
Update oELM with DS i according to Algorithm 2.UPDATE
end if
if cost-sensitive = TRUE then
Update Cost[min, maj] with the new IR
end if
end while

GPU-based implementation of extreme learning classiﬁers. There exists an eﬃcient R package
allowing for transferring the training procedure onto GPU using NVIDIA CUDA Basic Linear
Algebra Subroutines (cuBLAS) library [1]. This allows for signiﬁcant speed up of the most
computationally costly operations such as calculating the matrix storing outputs of hidden
layer H and its Moore-Penrose pseudoinverse (steps 3 and 4 of Algorithm 1). This however was
not studied in case of real-time classiﬁcation and continuously updated classiﬁers.
We propose to run the oELM using GPU processing applied to updating Mk and Bk (steps
17 and 18 of Algorithm 2) using cuBLAS library. This allows us to achieve a signiﬁcant speedup when processing continuously arriving chunks of data. Additionally, in case of ADWIN
detecting a concept drift the new classiﬁer being trained also uses GPU for calculating H0 , M0
and B0 (steps 6, 7 and 8 of Algorithm 2), thus allowing a rapid adaptation to shifts and drifts
in evolving data streams.

3

Experimental investigations

The aim of this experimental study is to verify the quality of the proposed modiﬁcations of
oELMs for handling drifting and imbalanced data streams. Additionally, we wanted to establish
the inﬂuence of GPU-based processing on the computational time required for the proposed
models to adapt to data streams.
1697

GPU Extreme Learning Machines for Imbalanced Data Streams with Concept Drift

3.1

B. Krawczyk

Data sets

There is an abundance of benchmarks for comparing machine learning algorithms working in
static environments. However, for non-stationary data streams there is still just a few publicly
available data sets to work with 1 . Most of them are artiﬁcially generated ones, with only
some real-life examples. Additionally, most of them have not been exploited in the context
of imbalanced learning. Hence we decided to prepare ﬁve datasets that are characterized by
large-scale number of examples, concept drift presence and varying class imbalance. Increased
class imbalance was achieved by over-generating representatives from majority class. Details of
used benchmarks can be found in Table 1.
Table 1: Details of data stream benchmarks used in the experiments.
Data set
Objects
Features
Classes
IR
Drift type
HYP
LED
Poker
RBFB
SEA

3.2

1
1
1
1
1

010
000
000
000
000

000
000
000
000
000

10
24
10
20
3

2
2 (0 vs 1-9 )
2 (0 vs 1-9)
2
2 (0 vs 1-3)

100.00
9.00
9.00
24.00
50.00

incremental
gradual
unknown
blips
sudden

Set-up

The used oELM has optimized number of hidden neurons ∈ [10;100] using the initial training
set and sigmoid activation function is being applied.
Classiﬁers are trained on given chunk and tested on incoming one. Then we fuse incoming
chunk with the existing one and repeat the procedure for new data. The data block size used
for creating data chunks was d = 2500 for all the data sets.
For evaluating classiﬁers in imbalanced non-stationary scenario we use the prequential AUC
[4] 2 .
The experiments were performed on a machine equipped with an Intel Core i7-4700MQ
Haswell @ 2.40 GHz processor and 16.00 GB of RAM with installed NVidia GTX295 GPU. All
experiments were conducted in R3 environment, with the usage of RMOA4 package.

3.3

Results and discussion

Obtained classiﬁcation performances with respect to prequential AUC are given in Table 2.
The averaged memory consumption is reported in Table 3, while averaged time required for
updating classiﬁers with new chunk can be found in Table 4.
When analyzing performance of classiﬁers for data streams one must take into consideration
several criteria in order to gain a broader view on the problem. When we concentrate only on the
classiﬁcation eﬃcacy expressed in terms of AUC we can see that standard oELM with ADWIN
detector cannot handle imbalanced and drifting streams properly. Out of three investigated
approaches the combination of oELM and oversampling returns superior classiﬁcation quality
on skewed streams. Undersampling does not deliver as good results which can be explained by
1 http://en.wikipedia.org/wiki/Concept

drift

2 http://www.cs.put.poznan.pl/dbrzezinski/software.php
3 http://www.r-project.org/

4 https://github.com/jwijﬀels/RMOA

1698

GPU Extreme Learning Machines for Imbalanced Data Streams with Concept Drift

B. Krawczyk

Table 2: Averaged prequential AUC [%] obtained by the examined classiﬁers. The oELM model
uses ADWIN drift detector, while remaining ones combine ADWIN with methods for handling
imbalanced data.
Data set oELM oELMunder oELMover oELMcost
HYP
LED
Poker
RBFB
SEA

62.09
53.80
77.37
66.34
76.14

75.32
57.32
86.34
78.92
84.78

77.30
62.99
91.09
81.32
88.01

77.86
63.76
91.38
80.58
88.18

Table 3: Averaged memory consumption [MB] displayed by the examined classiﬁers.
Data set

oELM

oELMunder

oELMover

oELMcost

HYP
LED
Poker
RBFB
SEA

1.23
0.28
0.23
2.16
0.72

0.64
0.16
0.13
1.35
0.51

4.42
1.01
1.18
4.87
1.46

1.23
0.28
0.23
2.16
0.72

Table 4: Comparison of averaged updating times [s.] per chunk displayed by the examined
classiﬁers computed wit the usage of CPU and GPU.
Data set

oELM

oELMunder

HYP
LED
Poker
RBFB
SEA

13.04
13.38
13.13
13.32
12.21

12.02
12.93
12.15
12.36
11.16

CPU
oELMover
26.56
19.56
20.89
19.41
17.58

oELMcost

oELM

oELMunder

13.04
13.38
13.13
13.32
12.21

1.32
1.40
1.35
1.39
1.20

1.19
1.21
1.21
1.20
1.10

GPU
oELMover
2.88
2.15
2.94
2.14
1.98

oELMcost
1.32
1.40
1.35
1.39
1.20

high imbalance ratio in most of the problems. Here undersampling may remove useful objects
that represent current state of the concept and may point out to the potential drift occurring.
Thus it is possible to discard an useful information which inﬂuences the observed classiﬁcation
process. Cost-sensitive oELM in most cases achieves AUC performance very similar to the
oversampling model.
However, when we take into account memory consumption and time complexity required
to update a model for each chunk our conclusions must change. Oversampling-based approach
is characterized by highest memory and time requirements due to its need to generate, store
and process chunks of increased size (which is especially vivid for cases with high IR). On the
other hand undersampling-based approach is characterized by the lowest requirements due to
signiﬁcant reduction of the data chunk size being analyzed. Cost-sensitive approach work on
unaltered set of objects, applying only modiﬁcation to its output. Therefore, its requirements
are identical as in case of standard oELM.
This allows us to conclude that the cost-sensitive oELM is the most suitable choice for
processing drifting and imbalanced data streams. It oﬀers classiﬁcation accuracy comparable
to oversampling solution while maintaining balanced time and memory requirements.
1699

GPU Extreme Learning Machines for Imbalanced Data Streams with Concept Drift

B. Krawczyk

When comparing processing times we can see that simple delegation of matrix-based operation to GPU allows us to achieve roughly 10 times computing acceleration regardless of
the method used. This proves that using GPU-based oELMs allow for real-time mining of
high-speed data streams.
Combination of oELM improved with ADWIN detector, cost-sensitive output modiﬁcation
and realized on GPU oﬀers an adaptive, skew-insensitive and rapid classiﬁer for imbalanced
and drifting data streams.

4

Conclusions

In this paper we have proposed a novel approach for handling imbalanced and drifting data
streams using online Extreme Learning Machines enhanced with drift detector and data-level
or cost-sensitive modiﬁcations. We showed that such additions allows oELM to eﬃciently
adapt to non-stationary properties of incoming objects, while alleviating the inﬂuence of skewed
distributions on its performance. Additionally, an eﬃcient GPU-based implementation was
proposed for rapid training and updating of online classiﬁers. This allowed for achieving 10
times speed-up when compared to standard CPU-based implementation. This issue was crucial
in term of mining high-speed data streams.
Experimental study carried on streams with varying drift types and imbalance ratios showed
that cost-sensitive oELMs oﬀer the best trade-oﬀ between their classiﬁcation accuracy and time
/ memory requirements.
These ﬁndings encourage us to continue works on GPU-based online extreme learning for
drifting and imbalanced data streams. Our future works will concentrate on proposing eﬃcient ensemble architectures using discussed classiﬁer model, handling streams where classes
constantly evolve between majority and minority concepts and using active learning to reduce
the cost of supervision.

Acknowledgments
This work was partially supported by the Polish National Science Center under the grant no.
DEC-2013/09/B/ST6/02264. The work was also partially supported by EC under FP7, Coordination and Support Action, Grant Agreement Number 316097, ENGINE European Research
Centre of Network Intelligence for Innovation Enhancement (http:// engine.pwr.wroc.pl/).

References
[1] M. Alia-Martinez, J. Antonanzas, F. Anto˜
nanzas-Torres, A. V. Pern´ıa-Espinoza, and R. UrracaValle. A straightforward implementation of a gpu-accelerated ELM in R with NVIDIA graphic
cards. In Hybrid Artiﬁcial Intelligent Systems - 10th International Conference, HAIS 2015, Bilbao,
Spain, June 22-24, 2015, Proceedings, pages 656–667, 2015.
[2] A. Bifet, G. De Francisci Morales, J. Read, G. Holmes, and B. Pfahringer. Eﬃcient online evaluation of big data stream classiﬁers. In Proceedings of the 21th ACM SIGKDD International
Conference on Knowledge Discovery and Data Mining, Sydney, NSW, Australia, August 10-13,
2015, pages 59–68, 2015.
[3] A. Bifet and R. Gavald`
a. Learning from time-changing data with adaptive windowing. In Proceedings of the Seventh SIAM International Conference on Data Mining, April 26-28, 2007, Minneapolis, Minnesota, USA, pages 443–448, 2007.

1700

GPU Extreme Learning Machines for Imbalanced Data Streams with Concept Drift

B. Krawczyk

[4] D. Brzezinski and J. Stefanowski. Prequential AUC for classiﬁer evaluation and drift detection in
evolving data streams. In New Frontiers in Mining Complex Patterns - Third International Workshop, NFMCP 2014, Held in Conjunction with ECML-PKDD 2014, Nancy, France, September
19, 2014, Revised Selected Papers, pages 87–101, 2014.
[5] P. Buteneers, K. Caluwaerts, J. Dambre, D. Verstraeten, and B. Schrauwen. Optimized parameter
search for large datasets of the regularization parameter and feature selection for ridge regression.
Neural Processing Letters, 38(3):403–416, 2013.
˙
[6] E. K. P. Chong and S. H. Zak.
An introduction to optimization, volume 76. John Wiley & Sons,
2013.
[7] B. Cyganek and S. Gruszczy´
nski. Hybrid computer vision system for drivers’ eye recognition and
fatigue monitoring. Neurocomputing, 126:78–94, 2014.
[8] S. Ding, Zhao. H., Y. Zhang, X. Xu, and R. Nie. Extreme learning machine: algorithm, theory
and applications. Artif. Intell. Rev., 44(1):103–115, 2015.
[9] J. Gama. A survey on learning from data streams: current and future trends. Progress in AI,
1(1):45–55, 2012.
[10] G.-B. Huang, N.-Y. Liang, H.-J. Rong, P. Saratchandran, and N. Sundararajan. On-line sequential
extreme learning machine. In IASTED International Conference on Computational Intelligence,
Calgary, Alberta, Canada, July 4-6, 2005, pages 232–237, 2005.
[11] D. Jankowski and K. Jackowski. Evolutionary algorithm for decision tree induction. In Computer Information Systems and Industrial Management - 13th IFIP TC8 International Conference,
CISIM 2014, Ho Chi Minh City, Vietnam, November 5-7, 2014. Proceedings, pages 23–32, 2014.
[12] B. Mirza, Z. Lin, and N. Liu. Ensemble of subset online sequential extreme learning machine for
class imbalance and concept drift. Neurocomputing, 149:316–329, 2015.
[13] Y.-H. Pao, G. H. Park, and D. J. Sobajic. Learning and generalization characteristics of the
random vector functional-link net. Neurocomputing, 6(2):163–180, 1994.
[14] M. Przewozniczek, R. Goscien, K. Walkowiak, and M. Klinkowski. Towards solving practical
problems of large solution space using a novel pattern searching hybrid evolutionary algorithm an elastic optical network optimization case study. Expert Syst. Appl., 42(21):7781–7796, 2015.
[15] I. Triguero, S. del R´ıo, V. L´
opez, J. Bacardit, J. M. Ben´ıtez, and F. Herrera. ROSEFW-RF:
the winner algorithm for the ecbdl’14 big data competition: An extremely imbalanced big data
bioinformatics problem. Knowl.-Based Syst., 87:69–79, 2015.
[16] S. Wang, L. L. Minku, and X. Yao. Resampling-based ensemble methods for online class imbalance
learning. IEEE Trans. Knowl. Data Eng., 27(5):1356–1368, 2015.
[17] M. Wo´zniak. A hybrid decision tree training method using data streams. Knowl. Inf. Syst.,
29(2):335–347, 2011.
[18] M. Wo´zniak. Application of combined classiﬁers to data stream classiﬁcation. In Computer Information Systems and Industrial Management - 12th IFIP TC8 International Conference, CISIM
2013, Krakow, Poland, September 25-27, 2013. Proceedings, pages 13–23, 2013.
[19] M. Wo´zniak, P. Cal, and B. Cyganek. The inﬂuence of a classiﬁers’ diversity on the quality of
weighted aging ensemble. In Intelligent Information and Database Systems - 6th Asian Conference,
ACIIDS 2014, Bangkok, Thailand, April 7-9, 2014, Proceedings, Part II, pages 90–99, 2014.
[20] M. Wo´zniak, M. Gra˜
na, and E. Corchado. A survey of multiple classiﬁer systems as hybrid systems.
Information Fusion, 16(1):3–17, 2014.
[21] M. Wo´zniak, A. Kasprzak, and P. Cal. Weighted aging classiﬁer ensemble for the incremental
drifted data streams. In Flexible Query Answering Systems - 10th International Conference, FQAS
2013, Granada, Spain, September 18-20, 2013. Proceedings, pages 579–588, 2013.

1701

