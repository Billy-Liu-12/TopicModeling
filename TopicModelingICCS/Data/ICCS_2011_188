Available online at www.sciencedirect.com

Procedia Computer Science 4 (2011) 558–566
p

International Conference on Computational Science, ICCS 2011

Community Earth System Model Data Management:
Policies and Challenges
Gary Strand*
National Center for Atmospheric Research, PO Box 3000, Boulder, CO 80307-3000 USA

Abstract
The data management plan of the Community Earth System Model (CESM)[1] from the National Center for Atmospheric
Research (NCAR) is given historical context and its policies, definitions, and features are detailed. The drivers of CESM data
management are discussed, including the upcoming Coupled Model Intercomparison Project 5 (CMIP5), the ongoing Earth
System Grid (ESG) project, and the strategies to address these drivers are mentioned. Future plans and strategies to address
CESM data management needs and requirements are noted. The significant challenges resulting from the use of CESM output in
the areas of metadata, preservation, curation, provenance, and other aspects of data management are considered.
Keywords: NCAR, CESM, CCSM, data management, climate model, ESG, CMIP

1. Introduction
The Community Earth System Model (CESM)[2, 3] from the National Center for Atmospheric Research (NCAR)
is a state-of-the-art Earth system model. It comprises components representing various processes important in the
climate system, such as atmospheric chemistry, radiation and dynamics, ocean biogeochemistry and dynamics, sea
and land ice, dynamic vegetation, the carbon/nitrogen cycle, and others. CESM is a true community model; its
development is the result of close collaboration between NCAR scientists and universities worldwide, as well as
scientists from other US national laboratories and its source code is freely available. CESM and its predecessors
have been used by thousands of climate researchers worldwide for their scientific analyses of the many aspects of
the global climate system. CESM has also participated in national and international assessments and has been used
by other scientific communities, university researchers, policymakers, government agencies, and private citizens
(see Figure 1). NCAR has always strived to make the output from the simulations as publicly available as practical
and as a result they have been widely used. The requirements from scientists, assessments, and this broad and global
user community of CESM output present significant challenges in all areas of data management: access,
distribution, metadata (information about the data), data standards, curation (who owns data and is responsible for
maintenance and support), provenance (how the data were created), and more. These extensive requirements have
contributed to the development of CESM through the careful consideration and application of standards,
conventions, and best practices. This paper uses the terms “output” and “data” interchangeably, even if in a technical
sense climate model output are not “data” because they are not measurements of observed phenomena.

1877–0509 © 2011 Published by Elsevier Ltd. Open access under CC BY-NC-ND license.
Selection and/or peer-review under responsibility of Prof. Mitsuhisa Sato and Prof. Satoshi Matsuoka
doi:10.1016/j.procs.2011.04.058

Gary Strand / Procedia Computer Science 4 (2011) 558–566

559

Figure 1: Geographic distribution of the CESM user community – each orange dot can represent many researchers and users of
CESM data. (Background image courtesy NASA; user locations from ESG-II metrics).

2. Historical background
CESM is the latest in a series of NCAR-based climate models, the lineage of which extends back several
decades. Until the early 1990s, the management of model output was informal, and access was limited to a handful
of scientists, most located at NCAR. The ability to distribute the data was constrained. Electronic distribution via the
internet did not yet exist, and the user community was mostly limited to the scientists who developed and
implemented the models of the time.
Beginning in the mid-1990s, two climate models - the Parallel Climate Model (PCM), funded by the US
Department of Energy (DOE), and the Climate System Model 1.0 (CSM1), funded by the US National Science
Foundation (NSF), existed at NCAR. Both models required that their output be made as publicly available as
practical. For PCM this was accomplished by transferring the model output from the various computing centers at
which PCM was executed to the archival system at the National Energy Research Supercomputing Center (NERSC).
NERSC’s archival system had an open access policy, so individuals who wished access to the PCM output could
request an account to access the archive. After the PCM principal investigators had examined the application, access
was granted. This process resulted in the distribution of PCM data to a community of roughly 100 users between
1999 and 2004. Similarly, access to CSM1.0 output required an account at NCAR to access the archival system
there. The online disk resources for even the modest output volumes of the era were not available.
For both models these distribution practices were limited because of the need for manual registration and
knowledge of the software to interact with the archive system. However, these access methods did result in the first
distribution of climate model output to a much broader community. These experiences also provided the initial
indications that NCAR climate modeling efforts would need to formalize their data management policies, because
these user communities were not intimately tied to NCAR’s climate modeling efforts. The user communities needed
assistance to gain understanding of the meaning, proper use, and applicability of the data. Support of the community
for data issues became more important.
In the late 1990’s the limitations of these techniques of data distribution became apparent as the user community
grew and their needs expanded. For PCM, these changes led to the decision to capitalize on the Earth System Grid I
project. ESG-I was conceived in 2000 to utilize Data Grid technologies to move and replicate terascale datasets
efficiently and rapidly between computing centers. These abilities were a natural fit for PCM data. The new project
was christened Earth System Grid-II[4, 5] (ESG-II) in late 2000 and was intended to provide the same abilities for
PCM output. ESG-II was designed to remove the requirement that a user of PCM output needed separate accounts to
download and use the data because ESG-II was web-based. With ESG-II, a registered user could browse the
available data, search for the data of interest, and make a simple request to download the data. ESG-II made the

560

Gary Strand / Procedia Computer Science 4 (2011) 558–566

archival system invisible to the user, allowing access to data without knowing the details. At the same time, NCAR
developed the Community Data Portal (CDP)[6] for the data from the CSM and its descendants. The CDP enabled
abilities to access data similar to ESG-II. The major difference between ESG-II and the CDP was that CSM data was
located exclusively at NCAR, while PCM data was located at NCAR and other DOE supercomputing centers.
One consequence of the implementation of both the CDP and ESG-II was that the CCSM (renamed from CSM in
2002) and PCM began to formalize their data management plans and policies - a significant outcome of the ability
of a broader user community to access the model output. With the number of users for both CCSM2 and PCM
output approaching a few hundred, the previous reliance by the model developers and scientists on users’ deep
understanding of the data was no longer valid. PCM and CCSM2 merged in 2004 to become a single model,
CCSM3[7], which was released in June of that year. With a larger user community, and with the experiments for the
third Coupled Model Intercomparison Project (CMIP3) under way, a formal data management plan was developed.
The initial plan was released in 2005 and it defined several important aspects of CESM data management.
3. The CESM Data Management and Data Distribution Plan
The overall intent of the CESM data management and distribution plan is to enable the global user community
the easiest access to CESM data as is possible, and at no cost to the data user. Any changes to the plan are passed to
the CESM Scientific Steering Committee (SSC), the panel ultimately responsible for management of the entire
CESM project, for its approval. As a living document, the CESM data management plan is periodically revised,
often in accompaniment to model code releases. The CESM version 1 code was released in mid-2010 and the most
recent updates and modifications to the data management plan were approved by the SSC in early 2011.
The plan includes the definitions of the categories of CESM data, ownership rights and responsibilities, release
timelines and retention policies, format standards, metadata requirements, naming conventions, quality assurance,
access policies, user registration and auditing, distribution limitations, and other aspects of CESM data management.
The CESM data management plan addresses access, provenance, governance, curation, and data and metadata
standards. Table 1 summarizes the major aspects of the CESM data management plan related to the simulation type.
Table 1. Summary of CESM simulation types and aspects of the data management plan that differ for each kind of simulation.

CESM simulation
type

Simulated Time

Development

days to years

Ownership

Principal Investigator
and/or
CESM Working Group
Production

years to millenia

Access and Release Timelines

Retention Period

limited to Working Group members

3 years

< 6 months, WG members
6-12 months, WG members and
collaborators

3 years for all data,
reduced set for 4
additional years

> 12 months, public access

x CESM simulation type
The two broad categories of CESM output are development simulations and production simulations.
Development simulations are primarily short-term (days of model time to a few years), executed to test and
evaluate new or updated parameterization schemes, software engineering developments, improved input datasets,
modernized model components, and other changes. There may be dozens, even hundreds, of these short
simulations, which means the provenance information is critical.
Production simulations are typically much longer in duration – from decades to millennia of simulated time, but
most often of century-scale length. These production simulations are the most extensively used by the CESM
user community, so all aspects of the CESM data management plan apply.

Gary Strand / Procedia Computer Science 4 (2011) 558–566

561

x Ownership
The CESM data management plan notes that the data belong to the principal investigators (PI). PIs are defined as
the SSC or the CESM Working Group (WG) that has undertaken the simulation. The owners have final approval
over the release, access, curation, and preservation of the model output, within the constraints of the CESM data
management plan. The data from development are restricted to close collaborators. These data are primarily for
testing, investigating, and evaluating model developments. For production runs, data are open to public access,
after a preliminary period of restriction.
x Access
Initially, all CESM data are classified as “protected”. Access is restricted to a small group of CESM scientists,
model developers, and close collaborators, for a time period of no greater than six months from the completion of
the simulation. This six-month period can be extended, but only with the approval of the SSC. The six-month
“embargo” allows for a thorough examination of simulation results via detailed diagnostics and analyses. Once
the data have been examined and determined to meet the requirements for wider access, the data are classified as
“CESM access”. A larger group of officially affiliated (via Working Group membership) users are allowed
access to the data for a period extending from six months to one year from the end of the simulation. This interim
period is often used to write papers on the simulation results, allowing the data owners first priority for
publication. For example, the Journal of Climate published a special issue about the CCSM3 in 2004 and will
publish special issues on CESM. After one year or the publication of papers (whichever is sooner) the data is
classified as open, with the sole condition that registration with ESG is mandatory.
x Curation
One aspect of climate model output is that as time passes, the data often become less utilized because of the
development of the next generation of models. This is the opposite of observational data, which typically become
more valuable with time. With this in mind, all CESM data are to be retained on archival storage for a period of
no less than three years for development data and for no less than seven years overall for production output.
Additional consideration is given to the costs of archived output, because some archival systems impose a time
factor in assessing charges for storage. These upper time limits for storage of data may increase because research
on the impact of model improvements on model fidelity to observations, and comparisons between model
versions, is now being done. These data stewardship practices ensure that CESM data will be properly
maintained and available.
x Data quality
The PI of each simulation is responsible for data quality and assurance, via the respective Working Groups or
collaborations with several of the Working Groups. Questions about data quality are to be answered as promptly
as possible by the responsible PI.
x Provenance
Provenance is indirectly addressed by the CESM data management policy, in that all simulations are strongly
encouraged to follow good practice in recording the model configuration, computing platforms, input datasets,
and other information. CESM uses a run database that lists the information necessary to reproduce a simulation.
The specifications include model version, software repository tag, computing hardware, component set, input
data used, and other important information. One aspect of climate model output that differentiates them from
observational data is that the data can be reproduced at a later time – with important caveats. Factors such as
changes in computing hardware and software can make bit-for-bit reproducibility difficult or impossible. These
changes often put an upper time limit on the ability to continue or repeat a specific simulation. Statistical testing
can be used to determine if a reproduced simulation, even if not bit-for-bit, has the same climate as the original
simulation. Additionally, records of all the steps involved in creating post-processed data are recorded, both by
attributes within each netCDF file as well as log files of the post-processing exercise itself. These logs are stored
in a subversion repository in case there are questions about the post-processing.
x Data format
All CESM data, both model output and post-processed are in netCDF format and are designed to follow standard
best-practice netCDF conventions. Since netCDF is a very widely used data format, this requirement enables
CESM data to be used by a large variety of common software packages. CESM data is thus not tied to any one
specific piece of analysis or visualization software.

562

Gary Strand / Procedia Computer Science 4 (2011) 558–566

x Metadata
CESM data is developed to follow the Climate and Forecast[8] (CF) metadata conventions as closely as possible.
A number of key personnel in the CESM project have played key roles in the development of the CF metadata
standards. Following CF assists in understanding the model output. Given the large size of the CESM user
community, this is critical. CESM code can be changed to accommodate the CF standard. Any additional
metadata requirements are strictly followed, such as those imposed by model intercomparison projects, detailed
below. The provenance information kept in the run database is critical for these additional metadata.
x Metrics
The CESM data management plan requires that usage metrics are saved. Periodic analyses of these metrics are
done to understand the usage patterns of CESM data. Metrics reports also allow CESM managers to know who
the biggest users are, which data are downloaded most often, the geographic distribution of users, and other
measures. The metrics reports can also assist in network throughput analysis, and are used by the ESG developers
to maintain and enhance overall system use. This allows for increased efficiency and reliability of the system . If
data from a given simulation are downloaded frequently, arrangements can be made to ensure the data stay on
disk, to avoid repeated reads from archival tape. These metrics are also used to determine the utilization of
current hardware and software, and form part of the basis for future hardware and software needs. Lastly, they
serve as an indicator of the status of ESG – if failures occur, the system needs maintenance and if necessary,
software upgrades.
4. Challenges impacting CESM data management
As noted above, early CESM data management was informal. The first model assessment projects did not greatly
impact data management practices of the time because the volume of data requested was modest - a few hundred
Megabytes (MB) of data from a few fields (surface temperature, precipitation, and a few other quantities), easily
extracted and stored on a few compact discs.
A crucial challenge was the decision by the WCRP/CLIVAR Working Group on Coupled Modelling (WGCM) in
2003 to create a large set of coordinated climate model experiments, the Coupled Model Intercomparison Project 3.
The data from these experiments were used in the IPCC Fourth Assessment Report (AR4), published in 2007. This
project pushed the creation of the original formal CESM data management plan. An earlier version of CESM,
CCSM3, was used for these CMIP3 experiments. Approximately 200 Terabytes (TB) of model output were
generated by CCSM3 for CMIP3. About 10 TB met the requirements defined by the WGCM[9] and was postprocessed from the raw model output. These data were sent to the Program for Climate Model Diagnosis and
Intercomparison (PCMDI) using 1 TB external disks in time for their inclusion in multimodel analyses for the IPCC
AR4 Working Group 1 (WG1) report. The output from NCAR comprised the single largest submission of any of the
modeling groups to CMIP3. The CMIP3 data archive[10, 11] represented the largest and most comprehensive set of
climate model output ever assembled. As of early 2011, the total download volume from the CMIP3 archive was
approximately 1 PB (1,000 TB) and its easy access enabled at least 750 scientific papers to be written based upon
these data. By any measure, the CMIP3 multi-model archive has been an unqualified success.
From the perspective of CESM data management, multiple lessons were learned from this exercise. The process
of transforming CESM output to meet the WGCM requirements took approximately 16 months, from the first
release of the WGCM requirements in February 2004 until the last disk was sent from NCAR to PCMDI in August
2005. The WGCM requirements specified which model fields were to be submitted, their CF-compliant names and
other metadata, how the netCDF files were to be named, and a long list of other requirements. The processing
required the development of a number of software packages and the provision of hardware to accomplish the task in
a timely manner to meet the WG1 report deadline. The requirements set forth by the WGCM, particularly the
metadata requirements, have influenced the means by which CESM output is processed and details of simulations
are recorded and managed. As a result the workflow of CESM is more efficient, because the data are post-processed
almost as the model code executes.
Partly due to the scientific knowledge gained from the CMIP3 multi-model archive, the upcoming IPCC Fifth
Assessment Report (IPCC AR5) mandates a larger set of coordinated experiments, called CMIP5[12]. There was a
CMIP4 project, but it was of much smaller scale and involved only simulations of the 20 th century climate in which
the various forcings (solar insolation, volcanoes, aerosols and so on) were imposed in various combinations to help

563

Gary Strand / Procedia Computer Science 4 (2011) 558–566

assess the impact of those forcings on the climate system. In addition to the IPCC AR5 simulations, the CMIP5
experimental design[13] requires additional simulations for investigating and understanding climate models and
climate processes. CESM simulations are underway in anticipation of publication of the report in mid-2013. The
scale and scope of the requested data have greatly expanded, as have the metadata requirements.
For CMIP3, 12 kinds of simulations were carried out. For CMIP5, the number of simulations requested is now
six times larger – a total of 72 different kinds of simulations. There are three major classes of simulations:
atmospheric sensitivity tests, long-term simulations, and for the first time, decadal-scale climate prediction.
Updates of previous intercomparison projects, such as the Paleoclimate Modelling Intercomparison Project 2
(PMIP2) and the Cloud Feedback Model Intercomparison Project (CFMIP) are incorporated into CMIP5. These
additional MIPs partly account for the larger set of simulations. There are also simulations intended to test the latest
Earth System Models, ESMs, of which CESM is one. The WGCM expects that the modeling groups will perform a
number of ensembles of each simulation. An ensemble is a set of simulations that have the same boundary
conditions (solar input, radiatively-important gases, aerosols, and the like) but different initial conditions. Table 2
summarizes the increase in the number of fields requested for the long-term simulations.
Table 2. Comparison of the number of variables from CMIP3 and CMIP5 requested for each type of data and each time resolution.

subdaily
CMIP3

daily

CMIP5

CMIP3

monthly
CMIP5

Atmosphere

9

100

18

75

Land surface

0

3

0

5

Ocean

0

1

0

3

Sea ice

0

0

0

4

Totals

9

104

18

87

CMIP3
47

annual

CMIP5

CMIP3

Totals

CMIP5
8

CMIP3
74

CMIP5

223

0

406

9

82

0

0

9

90

12

127

0

79

12

210

4

40

0

0

4

44

72

472

0

87

99

750

564

Gary Strand / Procedia Computer Science 4 (2011) 558–566

The extensive ongoing use of the CMIP3 data archive will be replicated for the CMIP5 archive. The one major
difference between the CMIP3 and CMIP5 archives is that all CMIP3 data are located at and served by PCMDI, but
the CMIP5 output will be served to the analysis community by an international federation of ESG gateways.
Frequently used fields will be replicated at several data centers. These centers are located at various sites, including
PCMDI, NCAR, the Max Planck Institute, the Australian National University, the British Atmospheric Data Centre,
and others. This federation should improve the reliability and robustness of the data delivery system and will
provide the global user community faster access. The necessity during the submission phase of CMIP3 to ship 1 TB
disks from the modeling centers to PCMDI has been eliminated by this federation of ESG sites.
A more dramatic demonstration of the steep increase in data output driven by the increased data requirements of
CMIP5 is illustrated in Figure 2. It shows the total cumulative archived data volume from both CCSM3 CMIP3
simulations and CESM CMIP5 simulations, between December 2003 and December 2010:

Figure 2. Comparison of the cumulative volumes of CCSM3 and CESM archived data.

The far larger simulation set size, many more requested fields, more frequent time sampling periods, and finer
spatial resolution of CESM compared to CCSM3 will result in a significantly larger contribution from CESM to the
CMIP5 project. The expected volume will be approximately 400 TB – 40 times the submission from CCSM3 for
CMIP3. While more-capable hardware and more personnel resources are now available for processing CESM output
that meets the requirements for CMIP5 in a reasonable timeframe, the increase in volume alone will be a significant
challenge.
Due to supercomputing performance reasons, all fields (variables such as surface temperature, ocean salinity, soil
moisture, sea ice thickness, etc.) are saved in a single netCDF file for each time sample or time range as the model
executes. One interesting result from the existence of the CDP and ESG-II was that analysis of the usage metrics
revealed that most of the data downloaded was from post-processed data, not this original model output.
Additionally, most analysis of model output is done on specific fields across a range of simulations, for the duration
of the simulations, rather than on large groups of fields at the same model time from a single simulation. These two
data usage patterns led to the enactment of routine post-processing of model output, apart from the processing
required for assessments. This transposition, from all fields at each time to all times for each field, requires a number
of steps: data are read in, any derivations or calculations are performed, metadata is added, and the post-processed
data are written out to disk, one file per field per time sample. The files are then concatenated along their time
dimension, resulting in one netCDF file for each field containing time samples for that field. Lastly, these postprocessed data are written to archival storage. These operations are not necessarily CPU-intensive, but they do

Gary Strand / Procedia Computer Science 4 (2011) 558–566

565

require substantial disk resources and memory. This post-processing is fundamentally limited by the rate at which
the data can be moved to and from archival tape.
Developments are underway to enable this processing to be more efficiently completed using various techniques.
The demanding input/output load isn’t well suited to supercomputing architectures, so specialized post-processing
hardware is preferred. These systems are built with large memory, fast archival access, and very large disk
resources.
Additionally, the common tools used to analyze and visualize these large datasets, such as NCL (NCAR Graphics
Command Language), must be upgraded to take advantage of parallel data analysis computers.
In addition to the increase in the number of simulations and fields for CMIP5 compared to CMIP3, the metadata
requirements are much greater. For CMIP3, additional metadata was encoded in each submitted netCDF file,
including global attributes such as “institution”, “project_id”, “contact” and others. Each modeling group was also
requested (but not strictly required) to provide documentation on the climate model itself, using a simple template
document[14]. While the information was useful to users of the CMIP3 multimodel archive, the different modeling
groups didn’t always provide sufficient documentation. This shortcoming has been addressed with a project called
“METAFOR” (Common Metadata for Climate Modelling Digital Repositories)[15]. A detailed questionnaire has
been developed by METAFOR that must be completed by each modeling group submitting data for CMIP5. The
questionnaire asks detailed questions about the climate models themselves, to assist in understanding how and why
the different models respond to the forcings of the CMIP5 experimental design. This requirement has helped CESM
more carefully document the model itself as well as the simulations.

5. The future of CESM data management
In the short term, CESM data will be given digital object identifiers (DOIs). NCAR has started the process for it
to become a DOI registration authority. Once this is complete, CESM data will be assigned DOIs, however, the level
of granularity for these assignments is yet to be determined. It will be impractical to assign a DOI to individual
CESM files, as there are far too many of them. One possible solution is to assign a DOI to each simulation or set of
files from each component (atmosphere, ocean, sea ice, etc.) from each simulation. This project will enable CESM
data to be citable. As noted above, the CESM data management and distribution plan is under revision with the
release of CESM version 1.
Over the longer term, new versions of CESM will be released, incorporating the latest understanding of climate
processes, improved parameterizations, and new components. The CESM code will continue to be refined and
enhanced to make the most efficient use of future supercomputing architectures. There will always be data
management challenges from the increasing volume of output, future assessments, and the needs of the CESM data
user community. The terabytes of model output will become petabytes and data management will become more
demanding. As CESM evolves, these changes may alter the details of CESM’s data management plan, but the
guiding principles underlying it will remain unchanged – easy, open, and transparent access to the results of CESM
simulations.
6. Conclusions
The NCAR CESM continues to be a leader in making access to output publicly and widely available. Its data
management plan has enabled the project to be a major contributor to the progress of climate science. By requiring
and enforcing good data management practices, the CESM data management and distribution plan enables
thousands of users worldwide to effectively use CESM data for their scientific research, potential policy-making
decisions, and other research activities.

566

Gary Strand / Procedia Computer Science 4 (2011) 558–566

Acknowledgments
The author would like to acknowledge Trey White and Julie Arblaster for their valuable comments and
suggestions. This work is supported by the Office of Biological and Environmental Research, US Department of
Energy, as part of its Climate Change Prediction Program (CCPP); and by NCAR. The National Science Foundation
sponsors NCAR.

References
1. CESM Data Management and Distribution Plan: http://www.cesm.ucar.edu/management/docs/data.mgt.plan.2011.pdf
2. Gent P. R., G. Danabasoglu, L. Donner, M. Holland, E. Hunke, S. Jayne, et al., 2011: The Community Climate System Model version 4. J.
Climate, Revised.
3. The Community Earth System Model home page: http://www.cesm.ucar.edu
4. Williams, D.N., R. Ananthakrishnan, D.E. Bernholdt, S. Bharathi, D. Brown, M. Chen, et al., 2009: The Earth System Grid: Enabling Access
to Multimodel Climate Simulation Data. Bull. Amer. Meteor. Soc., 90, 195-205. doi:10.1175/2008BAMS2459.1
5. Earth System Grid: http://www.earthsystemgrid.org
6. Community Data Portal: http://cdp.ucar.edu
7. Collins, W.D., C.M. Bitz, M.L. Blackmon, G.B. Bonan, C.S. Bretherton, J.A. Carton, et al., 2006: The Community Climate System Model:
CCSM3. J. Climate, vol. 19, 2122‑2143.
8. Climate and Forecast Conventions: http://cf-pcmdi.llnl.gov
9. CMIP3 home page: http://www-pcmdi.llnl.gov/ipcc/about_ipcc.php
10. CMIP3 overview page: http://www-pcmdi.llnl.gov/ipcc/about_ipcc.php
11. Meehl G.A., C. Covey, T. Delworth, M. Latif, B. McAvaney, J.F.B. Mitchell, et al., 2007: THE WCRP CMIP3 Multimodel Dataset: A New
Era in Climate Change Research, BAMS, 88, 1383-1394, DOI: 10.1175/BAMS-88-9-1383
12. CMIP5 home page: http://cmip.llnl.gov/cmip5
13. CMIP5 experiment design document: http://cmip.llnl.gov/cmip5/docs/Taylor_CMIP5_design.pdf
14. CMIP3 model documentation page: http://www-pcmdi.llnl.gov/ipcc/model_documentation/ipcc_model_documentation.php
15. METAFOR home page: http://metaforclimate.eu

