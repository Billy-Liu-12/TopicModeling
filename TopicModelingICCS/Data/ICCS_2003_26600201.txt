Parallel Program Debugging with MAD – A Practical
Approach
Dieter Kranzlm¨uller1 and Axel Rimnac2
1

GUP, Dept. for Graphics and Parallel Processing
2
Institute for Theoretical Physics
Joh. Kepler University Linz
Altenbergerstr. 69, A-4040 Linz/Austria, Europe
{dieter.kranzlmueller,axel.rimnac}@jku.at

Abstract. Debugging parallel programs can be very time-consuming and tedious,
because the multiplicity of communicating processes increases the complexity of
a program and the probability of incorrect behavior. Solutions are provided by
debugging tools, which try to offer meaningful ways to investigate errors and their
original causes. The MAD environment is a debugging toolset, which focuses on
parallel and distributed programs. This paper discusses the application of MAD
to real-world programs with practicability and usability as the main goals. The
observations are based on a series of debugging sessions conducted for a specific
application of theoretical physics, with the opinions of the tool developer on one
side and the view of the application developer on the other side.

1

Introduction

Computational Science and Engineering (CSE) is strongly related to High Performance
Computing (HPC), because HPC promises the performance needed by many CSE applications. The power of HPC architectures is usually provided by parallel and distributed
systems, which represent a serious challenge for CSE application developers. The traditional software engineering process has to be adapted to cope with several concurrently
executing and communicating processes, which attempt to jointly solve a given problem.
This increases the complexity of parallel and distributed CSE applications and requires
dedicated support from software development tools.
A basic necessity for software is correctness, which is verified during the testing and
debugging phase of the software lifecycle. The program’s behavior during runtime is
inspected and compared to the given specification. When deviations from the specified
behavior are observed, the developer tries to locate the original cause for the anomaly
and eventually corrects the program’s code. These activities are supported by so-called
debugging tools, which provide techniques such as breakpointing and inspection for
program comprehension [12].
Although many parallel debuggers exist, the majority of error detection work is still
performed by placing “print” statements in the program and inspecting their output
during execution. This seems almost archaic compared to the functionality provided
by sophisticated debugging tools, such as DDBG [4], P2D2 [7], DETOP [13], and
Totalview [5].
P.M.A. Sloot et al. (Eds.): ICCS 2003, LNCS 2660, pp. 201–210, 2003.
c Springer-Verlag Berlin Heidelberg 2003

202

D. Kranzlm¨uller and A. Rimnac

One reason for this situation may be that placing print statements is rather trivial
compared to the learning curve of using a new debugging tool. Another reason may
be that the textual representations chosen by the majority of debugging tools may be
inappropriate to express the complexity of parallel and distributed programs [11]. Finally,
there may be a discrepancy between the views of the tool developer and the application
programmer on what is important about parallel debugging, or rather how and what the
debugging tool should support.
This paper describes a case study of parallel program debugging for a particular
computational science program. The idea was to investigate the usefulness of an existing debugging tool when developing a theoretical physics program. The approach
was rather informal, with the two persons involved sitting together and detecting errors.
Consequently, the following article contains an example set of errors detected during
the course of debugging. Nevertheless, the findings of this cooperation seem to be an
interesting point for a discussion about the practical usefulness of software development
tools.
The paper is organized as follows. The next section describes the case study, an
application from theoretical physics, and the MAD environment. Section 3 introduces
the required preparations for error detection and presents simple automatic debugging
features, which revealed several errors in the program. Section 4 shows some more
advanced debugging features by using array visualization, while a summary and an
outlook on future work conclude this paper.

2

Case Study: Debugging with the MAD Environment

2.1 Target Application
The application chosen as a target for analyzing the debugging process under “real world”
conditions is a program that is currently used in the investigation of the scattering of
single helium (4 He) particles on a macroscopic slab system consisting of 4 He particles
near absolute zero temperature. Helium near absolute zero is known to be the only
terrestrical liquid that shows purely quantum mechanical effects. Hence, the description
of scattering has to be done in the framework given by the well-known Schr¨odinger
equation. Therefore, the quantity containing maximum information on the system is the
quantum mechanical wave function of the given many-body system.
The calculations are done in the so-called Correlated Basis Function (CBF) approximation which describes the energy loss of an incident particle to the target slab in terms
of two-phonon decay processes, i.e., an incident particle may stick in the system by
losing its energy to two quasiparticle excitations that can be viewed as surface waves on
the system. The incident particle may also lose just part of its energy and leave the slab
either as a reflected or transmitted particle1 .
The equation for the quantities of major interest, namely the probabilities of elastic
reflection and transmission as well as the probability of inelastic scattering takes the
form [2]
1

A classical analog would be the process of throwing a small stone into a lake. At this process
the stone may be reflected elastically or stick in the liquid and sink.

Parallel Program Debugging with MAD – A Practical Approach

H1 (z)ψ(z) +

Σ(z, z , ω)ψ(z )dz = h
¯ω

S(z, z )ψ(z )dz

203

(1)

which is a generalized one-dimensional Schr¨odinger equation. Here, ψ(z) is the quantum
mechanical wave function of the incident particle. H1 (z) and S(z, z ) depend on the
structure of the static background liquid, ω is the energy of the incident particle and
Σ(z, z , ω) is the so-called nonlocal optical potential accounting for the possibility of
the incident particle to lose energy. The self energy matrix Σ(z, z , ω) is given by
Σ(z, z , ω) =
m,n

Vmn (p, q, z)Vmn (p, q, z )
dp dq
¯h(ωm (q) + ωn (p) − ω)

(2)

where the matrix elements Vmn (p, q, z) are known ground state quantities and the denominator displays the dispersion relations of so-called Feynman excitation [3]. In determining Σ, parallelization is done by data-decomposition along indices m, n and summing
up all contributions at the end of the calculation. This is by far the most time-consuming
part of the computation.
Practical calculations have revealed that the execution time for a single run is about
12 hrs on 8 processors of an SGI Origin 3800 (equipped with R12000 processors at 450
MHz). This clearly justifies the use of a HPC system and parallel software engineering
for application development. The program is written in FORTRAN 90 and uses the
Message Passing Interface standard MPI [10] for communication and synchronization.
The following sections describe the development of the application described above
from a debugging point of view.
2.2 The MAD Environment
The Monitoring And Debugging environment MAD [8] is a toolset for debugging parallel programs based on the message-passing paradigm. The principal idea of MAD
is to display observed program interactions as space-time diagrams and to apply this
graphical representation for error detection. Based on this idea, two kinds of errors can
be distinguished [9]:
– Local errors represent incorrect program behavior without influence of parallelism.
They occur on a single process and do not involve communication or interaction with
other processes. These kinds of errors can be investigated with traditional sequential
debuggers.
– Global errors are established due to the parallel nature of the application. They
are connected to operations which would not exist in sequential programs. As a
consequence, the occurrence of incorrect behavior and its original source are often
located on different processes. Due to these characteristics, the usability of sequential
debuggers is questionable for tracking global errors.
For the remainder of this paper, we will focus on global errors, because their existence
is determined by the parallelism constructs introduced when developing programs for an
HPC system. Nevertheless, it is obvious that debugging local errors may also be needed,
for example when tracing back sources of global errors.

204

D. Kranzlm¨uller and A. Rimnac

The debugging process of MAD is performed using a series of independent modules.
The two most important modules of MAD are
– the monitoring tool NOPE - NOndeterministic Program Evaluator, and
– the visualization tool ATEMPT - A Tool for Event ManiPulaTion.
While these core modules sufficiently cover the majority of debugging activities,
several other analysis tools extend the functionality of MAD. Examples include trace
file converters for various formats, additional visualization modules, and a control- and
data flow analysis tool [8].

3

Program Instrumentation and Automatic Error Detection

The initial step for program debugging is instrumentation, which is applied to insert
code containing monitoring functionality into the target program. The monitoring code
is then responsible for extracting program state data for subsequent analysis activities.
In MAD, monitoring is performed with NOPE, which is inserted into the target program
through source code instrumentation. While source code instrumentation is less sophisticated than other techniques, e.g. dynamic instrumentation [6], its simplicity is a major
advantage. The drawback with this method is that the source code has to be recompiled
in order to integrate the monitoring functionality. However, this is probably acceptable
since it does not require more effort than inserting print statements into the program.
With the monitoring tool NOPE, instrumentation is performed in two ways:
(1) Automatic instrumentation is used for all functions known a priori. This includes
the library function calls of MPI as well as some system functions. The monitoring
code is inserted by replacing the include file mpif.h with its counterpart of NOPE
- monitor.h - which replaces the original function calls through preprocessor directives.
(2) Manual instrumentation is required whenever behavior needs to be observed, which
is specific to the target application. This includes switching on and off the monitoring
functionality as well as monitoring of user-defined data structures. In this case, the
user has to insert monitoring functionality in the code in the same way as print
statements are used.
For the target application, we applied both kinds of instrumentation. While automatic
instrumentation was turned on for the complete debugging session, manual instrumentation required several iterations to extract the state information of interest.
The most important characteristic of instrumentation seems to be simplicity. A debugging tool will only be utilized, if instrumentation and monitoring is less complicated
than placing print statements.
After recompiling the program with the monitoring functionality of NOPE included,
another program execution is initiated. The state data extracted by NOPE is stored in
tracefiles, which can afterwards be analyzed with the visualization tool ATEMPT. Instead
of exhaustively discussing the features of MAD, the following paragraphs describe a few
situations, that occurred during the debugging sessions of the target application.

Parallel Program Debugging with MAD – A Practical Approach

205

Situation 1: Early in the debugging session, the program hangs for no apparent reason. The diagnosis print statements indicate that the execution of the
program is blocked at some point, but does not reveal the exact location in the
source code nor the actual reason thereof.
Among the reasons why software hangs, the most typical case is a deadlock due to
an unfulfilled condition at a blocking operation. An example from MPI is the blocking
receive MPI Recv, which holds a process’ execution until a specific message arrives. If
the message does not arrive - for whatever reason - the process’ execution is blocked
forever. This kind of error is a so-called isolated receive [9].
The problem with this kind of situations is that blocked executions and program
failures are difficult to observe. The reason is based on the fact, that the monitoring tool
needs to store the monitoring data to disk. Yet, if the operation of a process is blocked,
the important information may still be in some intermediate monitoring or file buffer. If
the program’s execution is terminated, the data from the file buffer is lost.

Fig. 1. Missing trace data due to program failure.

An example of this phenomenon is shown in Figure 1. The left screenshot displays
the program’s execution as observed during execution with the ATEMPT visualization
tool. The program hangs due to a blocking receive on process P0, which is actually not
shown in Figure 1. Instead, it seems as if the program terminated without any reason.
With NOPE, tracefile flushing can manually be turned on or off at any place during
the program’s execution. After turning tracefile flushing on and restarting the program,
the right screenshot is observed. The program still hangs for the reason mentioned above,
but the cause of the hanging is clearly visible on process P0. The applied correction was
to correct the iteration counter for the blocking receive.
Please note, that the visualization of an error provides only a first indication of
the incorrect behavior. A pre-requisite for correcting the error is identification of the
corresponding line of source code. The appropriate feature in MAD is source code
connection, which is provided when clicking on the visual representation of an event. A
new window opens, displaying event attributes and the filename and line number of the
responsible statement.
A different solution to this problem is described in [14]. To avoid the problem of file
buffering, shared memory buffers and an external reader process are provided. This way,

206

D. Kranzlm¨uller and A. Rimnac

the interesting information can be stored in the tracefiles by a second process, which is
not affected by the blocking operation. However, while the benefits of this approach are
obvious, its implementation is more difficult than turning on trace file flushing.
In addition to isolated receives, ATEMPT is also able to automatically detect other
anomalies in communication patterns. This includes isolated send events or events with
different message length as in the following example:
Situation 2: The results of the application contained incorrect values if the
number of elements in a particular vector was not divisible by an integer.
In this case, the automatic detection of ATEMPT revealed the incorrect communication pair even before the incorrect results have been verified. Figure 2 presents an
example of a communication pair, where the amount of data provided by the sender process P1 does not fit into the message buffer at the receiver P0. Following the receive on
P0, the process is missing some data and possibly accessing incorrect message buffers.

Fig. 2. Different message length at sender and receiver.

The automatic detection of communication errors as described above revealed most
MPI related errors in the program with a few executions and correction cycles. We
believe, that the benefits of these simple techniques are most usable during debugging.

4 Advanced Debugging Techniques – Array Visualization
Besides errors in communication statements, parallelization of a program often involves
semantic changes to the program’s behavior, including partitioning and distribution of
data. In this context, a special feature of MAD for debugging is array visualization
(similar functionality is provided by the Prism debugger [1]). With array visualization is
it possible to graphically display the contents of up to three dimensions of an arbitrary
array. The array itself may be distributed across the available processes using High
Performance Fortran-like distribution styles (block, cyclic,. . . ).
Using array visualization requires manual instrumentation to specify the structure
of the array local to each process. The monitor NOPE provides dedicated function calls
for array visualization. However, due to the number of possibilities for distributing an

Parallel Program Debugging with MAD – A Practical Approach

207

array across multiple processes, these function calls may represent a major obstacle for
the in-experienced debugging user.
The most simple approach of instrumenting a program for array visualization is
achieved by placing the function call
monARRAYTRACE(void *array, trcArrayInfo info)
The variable array is a pointer to the start address of the local array, while info
describes the structure of the local array and its relation to the parts on the other processes.
This description includes the variable type of the elements, the number of dimensions and
the size of the array in each dimension. The distribution of the array onto the processes
is expressed in HPF-like descriptions. Further details can be specified with overlapping
areas at process borders.
An example of an array visualization event is shown in the top-left screenshot of
Figure 3. After a broadcast from process P0, every process provides its corresponding
part of the global matrix as indicated by the line across all processes. By clicking on the
array visualization event, an additional window opens and displays the reconstructed
global array as shown with the examples in Figure 3.
Situation 3: The array visualization feature has been used to display a 3D-array
during processing within the target application. Each process provides only the
local array, which is then assembled into the global array. The values stored
in the array are displayed as a heat-diagram, with different values indicated by
different colors.
With the help of the array visualization feature, distribution errors can often be spotted
immediately. While the parallel program should usually ensure a smooth gradient across
process borders, errors in array limits or processing are often observed.
The top-right screenshot of Figure 3 shows the 3D-array as computed by the application. For illustration and to emphasize the incorrect behavior, we changed the source
code to store a simple function into the parts of the array distributed across all processes.
As shown in the middle-left screenshot, the computation of the distributed array seems
to be wrong. Stripes along dimension x indicate, that the user did not specify the correct
array dimensions on each process. This is underlined by the distribution map of the
array as shown in the middle-right screenshot. The global array is distributed across the
8 processes blockwise along dimension z.
The bottom-left screenshot of Figure 3 displays the global array after correcting
the array boundaries on each process. The disturbance at the process borders has been
removed and the computation seems correct. In order to further investigate the 3D-array,
mechanisms for modifying the array visualization are provided. An example is shown
in the bottom-right screenshot, where a slice of the array along dimension z has been
removed in order to display the inside of the matrix.
The array visualization proved a valuable tool during the course of debugging. While
the instrumentation of arrays is certainly too difficult for the in-experienced users, the
graphical display of distributed data structures represents a huge benefit. Achieving the
same knowledge with print statements is certainly a much tougher task.

208

D. Kranzlm¨uller and A. Rimnac

Fig. 3. Examples of array visualization.

Parallel Program Debugging with MAD – A Practical Approach

5

209

Conclusions and Future Work

Debugging parallel and distributed CSE applications is a difficult task, which requires
dedicated support from software tools. By applying the MAD environment for a realworld application, several discrepancies in viewpoints from the tool developer to the
application programmer occurred. While tool developers seem to be interested in solving
the most complex problems of program debugging, application programmers seem to
be dissatisfied with the functionality of available debugging tools. In fact, the common
practice of using print statements for debugging seems to arise from the simplicity of
the approach.
On the positive side of our experience with real-world applications, several features
of MAD proved vital during the debugging process. The automatic analysis features
revealed a series of minor bugs in the code, which were easily traced back based on
the space-time diagram and its integrated source code connection. Among the advanced
analysis techniques, the verification of data distribution routines with array visualization
provided substantial benefits, even though the instrumentation was sometimes tricky.
While debugging the theoretical physics application is now closed, several improvements of the MAD environment are being planned based on the insights provided by
this work. On one hand, the automatic analysis activities will be enhanced by pattern
analysis, which is expected to reveal repeated communication patterns or minor deviations thereof. On the other hand, the array visualization work will be extended to include
additional distributed data structures and their visualization, as well as to simplify the
instrumentation process.
Acknowledgments. This work has been supported by a series of colleagues at the
Johannes Kepler University, including Prof. Jens Volkert and Prof. Eckhard Krotschek.
We gratefully acknowledge J. Messner from the Zentraler Informatikdienst (ZID) of the
Johannes Kepler Universit¨at Linz for providing part of his thesis to the MAD project
and for assistance in the course of the debugging procedure described in this paper.

References
1. Allen, D., Bowker, R., Jourdenais, K., Simons, J., Sistare, S., and Title, R.: "The Prism
Programming Environment", Proc. Supercomputer Debugging Workshop 91, Albuquerque,
New Mexico, USA, pp. 1–7 (November 1991).
2. Campbell, C.E., Krotscheck, E., and Saarela, M.: "Quantum Sticking, Scattering, and Transmission of 4 He atoms from Superfluid 4 He Surfaces" Physical Review Letters Vol. 80, No.
10, pp. 2169–2172 (1998)
3. Clements, B.E., Krotscheck, E., and Tymczak, C.J.: "Multiphonon excitations in boson quantum films" Physical Review B Vol. 53, No. 18, pp. 12253–12275 (1996)
4. Cunha, J.C., Lourenco, J.M., Antao, T.: "An Experiment in Tool Integration: the DDBG
Parallel and Distributed Debugger", EUROMICRO Journal of Systems Architecture, Vol. 45,
No. 11, pp. 897–907 (1999).
5. Etnus, LLC: "TotalView Debugger", http://www.etnus.com/Products/TotalView
(January 2003)

210

D. Kranzlm¨uller and A. Rimnac

6. Hollingsworth, J.K, Miller, B.P., Cargille, J.: "Dynamic Program Instrumentation for Scalable
Performance Tools", Proc. SHPCC, 1994 Scalable High Performance Computing Conference,
Knoxville, TN, USA, pp. 841–850 (May 1994).
7. Hood, R.: "The p2d2 Project: Building a Portable Distributed Debugger", Proc. SPDT’96,
ACM SIGMETRICS Symposium on Parallel and Distributed Tools, Philadelphia, USA, pp.
127–136 (May 1996).
8. Kranzlm¨uller, D., Grabner, S., Volkert, J.: "Debugging with the MAD Environment", Parallel
Computing, Vol. 23, No. 1-2, pp. 199–217 (Apr. 1997)
9. Kranzlm¨uller, D.: "Event Graph Analysis for Debugging Massively Parallel Programs", PhD
Thesis, GUP, Joh. Kepler University Linz, http://www.gup.uni-linz.ac.at/˜dk/thesis (Sept.
2000).
10. Message Passing Interface Forum: "MPI: A Message-Passing Interface Standard - Version
1.1", http://www.mcs.anl.gov/mpi/ (June 1995).
11. Pancake, C.M.: "Visualization Techniques for Parallel Debugging and Performance-Tuning
Tools", in: Zomaya, A.Y., " Parallel Computing: Paradigms and Applications", Intl. Thomson
Computer Press, pp. 376–393 (1996).
12. Rosenberg, J.B.: "How Debuggers Work: Algorithms, Data Structures, and Architecture",
John Wiley & Sons, New York (1996).
13. Wism¨uller, R., Oberhuber, M., Krammer, J., and Hansen, O.: "Interactive Debugging and
Performance Analysis of Massively Parallel Applications", Parallel Computing, Vol. 22, No.
3, pp. 415–442 (March 1996).
14. Wism¨uller, R., Dozsa, G., and Drotos, D.: "Using OMIS for On-line Monitoring in the
GRADE Programming Environment", Proc. DAPSYS 98, Austrian-Hungarian Workshop on
Distributed and Parallel Systems, Budapest, Hungary, pp. 177–184 (Sept. 1998).

