An Enhanced Speech Emotion Recognition
System Based on Discourse Information
Chun Chen, Mingyu You, Mingli Song, Jiajun Bu, and Jia Liu
College of Computer Science, YuQuan Campus, ZheJiang University
Hangzhou, P.R.CHINA, 310027
{chenc, roseyoumy, brooksong, bjj, liujia}@zju.edu.cn

Abstract. There are certain correlation between two persons’ emotional
states in communication, but none of previous work has focused on it. In
this paper, a novel conversation database in Chinese was collected and
an emotion interaction matrix was proposed to embody the discourse information in conversation. Based on discourse information, an enhanced
speech emotion recognition system was presented to improve the recognition accuracy. Some modiﬁcations were performed on traditional KNN
classiﬁcation, which could reduce the interruption of noise. Experiment
result shows that our system makes 3% - 5% relative improvement compared with the traditional method.

1

Introduction

Researches on understanding and modelling human emotions have attracted increasing attention in the artiﬁcial intelligence ﬁeld. As a major indicator of human emotions, speech plays an important role in detecting aﬀective states. Accurate emotion recognition from speech signals will beneﬁt the human-machine
interaction [1]. It has broadly potential applications in areas such as education,
consumer service and entertainment.
There are lots of researches that attempt to characterize emotional states of
human speech. Most previous eﬀorts involving speech emotion recognition have
tended to focus on either lexical [2, 3] or acoustic information [4, 5, 6]. These
studies usually used non-natural speech, including short isolated utterances, expressed by professional actors. Systems embedding lexical features into emotion
recognition assumed that certain words correlated with emotional states. But as
we know, the relationship between words and emotions is fuzzy and sometimes
confused. One word may indicate several possible emotions and one emotional
state can be conveyed by diﬀerent words. Besides, lexical information always
needs manual transcription for each utterance, which is diﬃcult to be realized
automatically.
On the other hand, researches on emotion detection of spoken dialog system
tried to classify more natural occurring emotion of actual users. In order to
achieve better performance, lexical and acoustic feature sets were augmented
with additional features such as context and discourse information[7, 8, 9, 10].
Contextual factors really have inﬂuences on emotion identiﬁcation, which can
V.N. Alexandrov et al. (Eds.): ICCS 2006, Part I, LNCS 3991, pp. 449–456, 2006.
c Springer-Verlag Berlin Heidelberg 2006

450

C. Chen et al.

be found in the work of Crystal[11, 12, 13]. Liscombe[7] et al. included prosodic
context, lexical context and discourse context as contextual features in emotion
prediction and increased classiﬁcation accuracy by 2.6%. Contributions made by
context information indicate that emotion is expressed in a language environment. Traditional operation extracting prosodic features from isolated utterances
is inconsistent with emotion perception of human beings. Contextual features focus on utterances of single person, but dialogs appear frequently in daily life.
Many studies paid attention to dialog system and added discourse information
into emotion recognition[8, 9, 10]. Ang et al.[8] included discourse features such as
turn location within the conversation and dialog acts of the current turn (repeat,
repair, neither). Lee et al.[9] labeled dialog acts as rejection, repeat, rephrase,
ask-start over or none of the above. The addition of discourse information added
approximately 3% relative improvement over using lexical and prosodic features
alone. But the discourse features considered were only based on the categorization of users’ responses when interacting with machine. Few work paid attention
to the dialog between two persons. But it is known that the emotions of two
talkers have certain correlation.
This paper presents research on simultaneously recognizing two talkers’ emotional states using discourse information. In addition to extracting typical
acoustic features, we combined emotional correlation between two persons in dialog into speech emotion recognition. Using this extended method, we observed
an increase in prediction accuracy.

2

Conversation Corpus and Discourse Information

In the ﬁeld of emotion recognition, there are ongoing debates concerning how to
deﬁne basic emotion categories. Diﬀerent researchers have diﬀerent opinions and
some psychologists even argue against the categorical labels for human emotions.
In this paper, we just focus on the archetypal emotions - happy, sad, fear, angry,
surprise and neutral. Besides, how to obtain amount of realistic data for research
is another hard task. Most studies in speech emotion recognition asked subjects
to simulate certain emotions with neutral semantic content[14]. Since those data
sets are limited to utterances of single person in archetypal emotions, results
based on them may still have distance to real-life scenarios. On the other hand,
it’s hard for real data to cover all the emotion categories needed and the noise in
real environment is also a problem. With aforementioned analysis, we should turn
to sources containing conversations and happening in a relatively quiet background. The living theater and broadcast drama are suitable sources for dialog
corpus in controlled environment. They also embody most emotional correlation
of conversation in daily life. As the beginning of dialog corpus collection, we tried
to extract 4000 conversations from Chinese drama lasting hundreds of hours totally. Our data corpus covers the six archetypal emotions mentioned above and
contains dialogs covering man to man, man to woman and woman to woman.
In table 1, we mark two persons having conversation as A and B. A represents
the ﬁrst person and B denotes the second one. The left part of table 1 presents

An Enhanced Speech Emotion Recognition System

451

Table 1. Distribution of The Other’s Emotion When Given One Person’s Emotional
State (The abbreviated emotion labels in line are same to those in column)
A
Ne.(%) An. Fe. Ha. Sa. Su.
Neutral 26.67 22.22 6.67 6.67 11.11 26.67
Angry 32.56 46.51 2.33 2.33 4.65 11.63
Fear 41.18 35.29 5.88 0 11.76 5.88
Happy
30
0
5 40
5
20
Sad
20 26.67 0
0
40 13.33
Surprise 36
32
0 24
4
4

B
Ne. An. Fe. Ha. Sa. Su.
Neutral 23.53 27.45 13.73 11.76 5.88 17.65
Angry 20.83 41.67 12.5 0 8.33 16.67
Fear
50 16.67 16.67 16.67 0
0
Happy 16.67 5.56 0 44.44 0 33.33
Sad 29.41 11.76 11.76 5.88 35.29 5.88
Surprise 48
20
4
16
8
4

A’s emotion distribution when given the emotional state of B. Similarly, emotion
distribution of B is shown at the right part of table 1. As an example, we look
into the ﬁrst line of part A. This line exhibits A’s emotion distribution when the
emotional state of B is neutral. From the listed numbers, we ﬁnd that when B is
neutral, the probability of A on neutral is 26.67%. Probabilities of A on angry,
fear, happy, sad and surprise are 22.22%, 6.67%, 6.67%, 11.11% and 26.67%,
respectively. Diﬀerent probability on each emotion category implies that B will
aﬀect the emotional state of A in dialog and vice versa. For example, when
B is happy, A is likely to be also happy considering the corresponding high
probability. We were wondering if we could embed this information into speech
emotion recognition in order to improve the recognition accuracy. A detailed
experiment is designed in the following section.

3

An Enhanced Speech Emotion Recognition

Based on the analysis above, we propose an enhanced speech emotion recognition system in Figure 1. The system is composed of two parts - training and

Fig. 1. System Overview

452

C. Chen et al.

testing. In the oﬀ-line training phase, we collected 4000 dialogs into the conversation database. The corpus includes thousands of people instead of two in
Figure 1 which is just for simpliﬁcation. An initialized emotion interaction matrix was trained to embody the emotion correlation between people in dialog in
the training course. When new data came into the system for testing, acoustic
features were extracted and gender classiﬁcation was performed based on pitch
analysis. Then a method combining emotion interaction matrix was proposed
to recognize emotional states of both persons using those acoustic features. The
whole process can be divided into three steps mentioned below.
3.1

Acoustic Features Extraction

The conversation corpus we collected is sampled at 16kHZ frequency and 16 bits
resolution with monophonic Windows PCM format. In this study, we extracted
48 prosodic and 16 formant frequency features. Prosody is mainly related to the
rhythmic aspects of speech, and believed to be the primary indicator of speakers’
emotion state. The extracted prosodic features include: max, min, mean, median
of Pitch (Energy); mean, median of Pitch (Energy) rising/ falling slopes; max,
mean, median duration of Pitch (Energy) rising/ falling slopes; mean, median
of Pitch (Energy) plateaux at maxima/ minima; max, mean, median duration
of Pitch (Energy) plateaux at maxima/ minima. Here, if the ﬁrst derivative is
approximately zero and the second derivative is positive, the point belongs to a
plateau at a local minimum. If the second derivative is negative, it belongs to
a plateau at a local maximum. We also investigated formant frequency features
which are widely used in speech processing applications. Statistical properties
including max, min, mean, median of the ﬁrst, second, third, and fourth formant
were extracted.
3.2

Dimensionality Reduction

Because high dimensional data can dramatically raise computational complexity and decrease classiﬁcation accuracy in speech emotion recognition, the 64dimensional acoustic features extracted above should be compressed. Principal
Component Analysis (PCA) was employed as dimensionality reduction method
in our study. PCA tends to ﬁnd a t-dimensional subspace whose basis vectors
correspond to the maximum variance direction in the original s-dimensional
space(t
s). Original data set is projected into the t-dimensional subspace
with projection matrix WP CA .
In the experiment, speaker independent emotion recognition was investigated
because of the thousands of speakers involved in conversation database. But gender classiﬁcation was still performed on speech data because of the diﬀerence of
acoustic features between female and male. 10-fold cross-validation method was
adopted considering the conﬁdence of recognition results. So, 7200(90%*4000*2)
64-dimensional vectors were used to train PCA. We used a 7200*64 matrix X to
represent these vectors. After X was normalized and mean-subtracted, we got
matrix Y. Y T Y formed a covariance matrix M which was 64*64. Eigenvalues and

An Enhanced Speech Emotion Recognition System

453

eigenvectors were computed for M. Eigenvectors corresponding to the largest t
eigenvalues were selected to create the PCA projection matrix WP CA . t was
the number of eigenvalues that guaranteed energy E was greater than 0.9. Here
energy E was deﬁned in equation(1):
t

Et =

64

λj /
j=1

λj

(1)

j=1

where λj was the jth eigenvalue. In our experiment, t equaled to 27. 3600(7200/2)
training conversations and 400(800/2) testing ones were both projected into
subspace using WP CA .
3.3

Emotion Recognition Based on Discourse Information

Having the low dimensional features, K-Nearest-Neighbor(KNN) was adopted
to classify the data into six emotional states. K-Nearest-Neighbor is a simple
classiﬁcation which range the testing data into the class most of its k nearest
neighbors belonging to. It is the classical implementation of KNN. We made
some modiﬁcations to KNN for the sake of our enhanced recognition system.
K nearest neighbors were calculated for testing utterance which was same to
KNN classiﬁcation. In the process of K nearest neighbors’ calculation, Euclidean
distance was adopted as the distance measurement between feature vectors of
training utterance and testing utterance. K nearest neighbors belonged to M
classes {C1 , C2 , · · · , CM }. The probability of belonging to Ci was deﬁned by:
Pi = Ni /Nk where Ni denoted the number of nodes belonging to Ci and Nk
equaled to k which was the total number of nodes. So the probabilities for M
classes was {P1 , P2 , · · · , PM } and we used Pm1 to be the highest probability
and Pm2 to be the second. In classical KNN, Cm1 with Pm1 is selected as the
recognition result for test utterance. However, we’d like to make our decision
based on Pm1 and Pm2 instead of depending on Pm1 alone.
Diﬀerent from conventional methods, the emotional states of two persons in
dialog were recognized together in our system. Let us use MA to stand for the
emotion interaction matrix of A when given emotional state of B and MB to
A
A
and Pm
were the largest two distribution probabilities
stand for that of B. Pm
1
2
B
B
of person A’s neighbors and Pm
and Pm
were those of B’s. There were four
1
2
A
A
B
B
situations based on diﬀerent Pm1 , Pm2 , Pm
and Pm
listed below. We used a
1
2
constant T h to represent the threshold of comparison.
A
A
B
B
− Pm
≥ T h and Pm
− Pm
≥ Th
(1) Pm
1
2
1
2
A
A
B
In this case, we believed Cm1 corresponding to Pm
and Cm
corresponding to
1
1
B
A
as
Pm1 were outstanding ones among those candidates. So we just chose Cm
1
B
the emotion recognition result for person A and Cm1 for person B.
A
A
B
B
(2) Pm
− Pm
< T h and Pm
− Pm
≥ Th
1
2
1
2
B
B
was selected as the recognition result
In this case, Cm1 corresponding to Pm
1
of person B just as situation(1). But for person A, there wasn’t such class with
prominent performance, in other words, it was not sure which class should be

454

C. Chen et al.

A
A
selected as the result. In our system, we selected Cm
corresponding to Pm
i
i
deﬁned in equation(2) as the recognition result for person A.
2

A
A
B
B × P
= arg max(Pm
× (MA )mA
Pm
m1 )
i
i
i m1
i=1

(2)

Here, we embedded emotion interaction information mentioned above into the
speech emotion recognition system. Such method could save those candidates
in {C1 , C2 , · · · , CM } which might have lower probabilities because of noise. As
human beings, we also use this rule in emotion perception. If we are not sure
about the other’s emotional state, we’d like to judge it by our experience on
what emotion would be most likely.
A
A
B
B
− Pm
≥ T h and Pm
− Pm
< Th
(3) Pm
1
2
1
2
This case is similar to situation(2) excepting for using matrix MB instead of
MA . Here we omit the detailed operations.
A
A
B
B
− Pm
< T h and Pm
− Pm
< Th
(4) Pm
1
2
1
2
In this case, both A and B could not ﬁnd sure recognition results. When recognizA
A
corresponding to Pm
deﬁned in equation(3)
ing emotional state of person A, Cm
i
i
was selected as the result.
2

A
A
B
Pm
= arg max(Pm
× (MA )mA
B × P
mj )
i
i
i mj
i,j=1

(3)

B
deﬁned in equaSimilarly, person B’s recognition result depended on Pm
j
tion(4).
2

B
A
B
Pm
= arg max(Pm
× (MB )mA
B × P
mj )
j
i
i mj
i,j=1

4

(4)

Experiment Result

In our experiment, we set k to 10 in K-Nearest-Neighbor searching and Threshold T h to 20%. In order to evaluate the performance of our new method, we

Fig. 2. Recognition Accuracy for person A and person B in our method (Accuracy in
Conversational X) and traditional method (Accuracy in Single X)

An Enhanced Speech Emotion Recognition System

455

Fig. 3. Recognition Accuracy on Diﬀerent Choice of k

also included a traditional emotion recognition process. In the traditional way,
emotions of two persons in dialog were recognized separately and classical KNN
classiﬁcation was employed. The recognition accuracy of six emotions using our
enhanced emotion recognition method and the traditional method is shown in
Figure 2. From the ﬁgure, we can ﬁnd out that the enhanced speech emotion
recognition system outperforms traditional method on almost all of the basic
emotions. On the average, our system is observed 5% relative improvement on
person A and 3% on person B compared with traditional method. For person
A, traditional method has diﬀerent performance on diﬀerent emotion. The accuracy on emotion fear is 30% lower than that on neutral, which will impact on the
system performance. Our method balances the performance on each emotional
state and improve the recognition accuracy totally.
Besides, the performance on diﬀerent choice of k in K-Nearest-Neighbor
searching is compared in Figure 3. None of the k achieves outstanding result
compared with other choices. But k = 10 always has acceptable performance,
especially on the emotion fear of person A. In addition to performance, simple
computation is also an advantage of k = 10.

5

Conclusion and Future Work

This paper presents an enhanced speech emotion recognition system based on
discourse information between human beings. Instead of hurriedly choosing one
class as the recognition result, all possible classes were investigated. Experiment
result shows that the enhanced method makes improvements at almost all of
the emotional states and balanced the performance on every emotion. As we
expected, interaction information used in the communication of humans did
help the emotion recognition of computers.
The emotion interaction matrix we collected is only a beginning work. Because
of the small conversation database, the correlation of emotional states between
talking people can not be well indicated. More eﬀorts should be put on the
collection of conversation database and the discovery of emotional relationship
between talking people.

456

C. Chen et al.

Acknowledgement
The work is partly supported by National Natural Science Foundation of China
(60203013). And We thank Cheng Jin, Qi Wu and Weiguang Wang for their
generous help to our experiment and paper.

References
1. A. Mehrabian, ”Communication without words”, Psychology Today, 2(4), pp. 5356, 1968
2. Z. J. Chuang and C. H. Wu, ”Emotion recognition from textual input using an
emotional semantic network”, in Proceedings of ICSLP, Denver, Colorado, USA,
2002, pp. 2033C2036.
3. B. Schuller, G. Rigoll and M. Lang, ”Speech emotion recognition combining
acoustic features and linguistic information in a hybrid support vector machinebelief network architecture ”, in Proc. IEEE International Conference on Acoustics,
Speech, and Signal Processing, Volume 1, pp. 577-580, May 2004.
4. D. Ververidis, C. Kotropoulos and I. Pitas, ”Automatic emotional speech classiﬁcation”, in Proc. IEEE International Conference on Acoustics, Speech, and Signal
Processing, Volume 1, pp. 593-596, May 2004.
5. C. Lee, S. Narayanan, and R. Pieraccini, ”Recognition of negative emotions from
the speech signal”, in Proc. Automatic Speech Recognition Understanding, Dec.
2001.
6. J. Yuan, L. Shen, and F. Chen, ”The acoustic realization of anger, fear, joy, and
sadness in chinese”, in Proceedings of ICSLP, Denver, Colorado, USA, 2002, pp.
2025C2028.
7. J. Liscombe, G. Riccardi and D. Hakkani-T¨
ur, ”Using Context to Improve Emotion
Detection in Spoken Dialog Systems”, In the Proceedings of EUROSPEECH’05,
September, 2005.
8. J. Ang, R. Dhillon, A. Krupski, E. Shriberg, and A. Stolcke, ”Prosody-based automatic detection of annoyance and frustration in human-computer dialog”, in
Proceedings of ICSLP, Denver, Colorado, USA, 2002, pp. 2037C 2039.
9. C. M. Lee and S. Narayanan, ”Towards detecting emotions in spoken dialogs”,
IEEE Transactions on Speech and Audio Processing, in press, 2004.
10. A. Batliner, K. Fischer, R. Huber, J. Spilker, and E. N¨
oth, ”How to ﬁnd trouble
in communication”, Speech Communication, vol. 40, pp. 117C143, 2003.
11. D. Crystal, ”Prosodic Systems and Intonation in English”, Cambridge University
Press, 1969
12. D. Crystal, ”The English Tone of Voice”, Edward Arnold, 1975
13. D. Crystal, ”The Cambridge Encyclopaedia of the English Language”, Cambridge
University Press, 1995
14. M. You, C. Chen and J. Bu, ”CHAD: A CHINESE AFFECTIVE DATABASE”,
in Proc. Aﬀective Computing and Intelligent Interaction, pp. 542 - 549, 2005

