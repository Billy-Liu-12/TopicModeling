Parallel Calculating of the Goal Function in
Metaheuristics Using GPU
Wojciech Bo˙zejko, Czeslaw Smutnicki, and Mariusz Uchro´
nski
Institute of Computer Engineering, Control and Robotics
Wroclaw University of Technology
Janiszewskiego 11-17, 50-372 Wroclaw, Poland
{wojciech.bozejko,czeslaw.smutnicki,mariusz.uchronski}@pwr.wroc.pl

Abstract. We consider a metaheuristic optimization algorithm which
uses single process (thread) to guide the search through the solution
space. Thread performs in the cyclic way (iteratively) two main tasks:
the goal function evaluation for a single solution or a set of solutions
and management (solution ﬁltering and selection, collection of history,
updating). The latter task takes statistically 1-3% total iteration time,
therefore we skip its acceleration as useless. The former task can be accelerated in parallel environments in various manners. We propose certain
parallel small-grain calculation model providing the cost optimal method.
Then, we carry out an experiment using Graphics Processing Unit (GPU)
to conﬁrm our theoretical results.

1

Introduction

Almost all combinatorial optimization tasks formulated for scheduling problems
are strongly NP-hard. Currently known exact solution algorithms (dedicated to
ﬁnd global optimum) own exponential computational complexity which causes
unacceptable long solution time for instances that come from practice. In this
context one can propose two, not mutually conﬂicted, approaches, which allow
one to solve large-size instances in acceptable time: (1) approximate methods
(chieﬂy metaheuristics), (2) parallel methods. The best hybrid combination of
both is the one which we really needed.
The most promising metaheuristic algorithms search solution space in a certain intelligent way. Quality of the best solutions generated by these algorithms
strongly depends on the number of analyzed solution, and thus on the running
time. Time and quality have opposing tendency in such a sense, that ﬁnding
a better solution requires a signiﬁcant computation time growth. Through the
parallel processing one can increase the number of checked solutions (per time
unit). In this paper there are proposed several solutions algorithms dedicated to
a single solution analysis employed in widely used metaheuristics.
In the scope of a single-thread search, dedicated fundamentally for uniform
multiprocessor system of small granularity, one can distinguish a few parallel
approaches taking into account various design technologies and diﬀerent needs
applied by modern discrete optimization algorithms, namely: (a) single solution
G. Allen et al. (Eds.): ICCS 2009, Part I, LNCS 5544, pp. 1014–1023, 2009.
c Springer-Verlag Berlin Heidelberg 2009

Parallel Calculating of the Goal Function in Metaheuristics Using GPU

1015

analysis (dedicated for simulated annealing SA, simulated jumping SJ, random
search RS), (b) local neighborhood analysis (for tabu search TS, adaptive memory search AMS, descending search DS), (c) analysis of population of distributed
solutions (for genetic approach GA, scatter search SS). In each case special attention should be paid to eﬃciency, cost and speedup of methods depending on
the used parallel computing environment. For each algorithm, theoretical evaluation of its numerical properties as well as comparative analysis of potential
beneﬁts from proposed approaches are expected. In this paper we deal chieﬂy
with the approach (a).
In this paper we use the following notions which are fundamental in the parallel computing area, see e.g. [4]: theoretical parallel architectures, theoretical
models of parallel computations, granularity, threads, cooperation, speed up,
eﬃciency, cost, cost optimality, computational complexity, real parallel architectures and parallel programming languages.
This work constitutes the continuation of authors research on constructing
eﬃcient algorithms applied to solve hard combinatorial problems ([2,5,6]).

2

Permutation Flow Shop Problem

We consider, as the test case, the well-known in the scheduling theory, strongly
NP-hard problem, called the permutation ﬂow-shop problem with the makespan
criterion and denoted by F ||Cmax . Skipping consciously the long list of papers
dealing with this subject we only refer the reader to the recent reviews and the
best up-to-now algorithms [5,6].
The problem has been introduced as follows. There is n jobs from a set J =
{1, 2, . . . , n} to be processed in a production system having m machines, indexed
by 1, 2, . . . , m, organized in the line (sequential structure). A single job reﬂects
one ﬁnal product (or sub product) manufacturing. Each job is performed in m
subsequent stages, in a common way for all tasks. The stage i is performed by
machine i, i = 1, . . . , m. Each job j ∈ J is split into a sequence of m operations
O1j , O2j , . . . , Omj performed on machines in turn. The operation Oij reﬂects
processing of job j on the machine i with the processing time pij > 0. Once
started job cannot be interrupted. Each machine can execute at most one job at
a time; each job can be processed on at most one machine at a time.
The sequence of loading jobs into system is represented by a permutation
π = (π(1), . . . , π(n)) on the set J. The optimization problem is to ﬁnd the
optimal sequence π ∗ so that
Cmax (π ∗ ) = min Cmax (π).
π∈Π

(1)

where Cmax (π) is the makespan for permutation π and Π is the set of all permutations. Denoting by Cij the completion time of job j on the machine i we have
Cmax (π) = Cm,π(n) . Values Cij can be found by using the recursive formula
Ciπ(j) = max{Ci−1,π(j) , Ci,π(j−1) } + piπ(j) , i = 1, 2, . . . , m, j = 1, . . . , n, (2)

1016

W. Bo˙zejko, C. Smutnicki, and M. Uchro´
nski

with initial conditions Ciπ(0) = 0, i = 1, 2, . . . , m, C0π(j) = 0, j = 1, 2, . . . , n.
Computational complexity of (2) is O(mn).
Values Cij from the equation (2) can be also determined by using a graph
model of the ﬂow shop problem. For a given sequence of jobs execution π ∈ Π
we create a graph G(π) = (M × N , F 0 ∪ F ∗ ), where M = {1, 2, . . . , m},
N = {1, 2, . . . , n}, F 0 =
arcs (vertical) and F ∗ =

m−1 n

{((s, t), (s + 1, t))} is a set of technological

s=1 t=1
m n−1

{((s, t), (s, t + 1))} is a set of sequencing arcs

s=1 t=1

(horizontal). Arcs of the graph G(π) have no weights, but each vertex (s,t) has
weight ps,π(t) . The time Cij of completing job π(j), j = 1,2,...,n on the machine
i, i = 1,2,...,m equals the length of the longest path from the vertex (1,1) to the
vertex (i,j), including the weight of the last one. For the F ||Cmax problem the
value of the criterion function for the ﬁxed sequence π equals the length of the
critical path in the graph G(π).

1
1

2

2

3

...

n

jobs

(1,1)

(1,2)

(1,3)

n-1
(1,n-1)

(2,1)

(2,2)

(2,3)

(2,n-1)

(2,n)

(3,1)

(3,2)

(3,3)

(3,n-1)

(3,n)

(m,1)

(m,2)

(m,3)

(m,n-1)

(m,n)

(1,n)

3
M
m
machines

Fig. 1. Graph G(π)

3

Searching

We consider a solution method which uses only one thread to manage the search
process. The process executes cyclic iterations consisting of: (1) numerical calculations (i.e. the goal function value determination), (2) managing functions (i.e.
solution selection, calculations memory realization, solution acceptation). Activities connected with managing take statistically 1-3% of the iteration’s time.
Therefore, its acceleration by using parallel environment gives us insigniﬁcant
beneﬁt. From the other side, the numerical calculations acceleration by implementing in the parallel or distributed architecture may signiﬁcantly improve the
eﬃciency of the solution space’s search algorithm.
We take advantage of the following well-known facts for the PRAM parallel
computer model:

Parallel Calculating of the Goal Function in Metaheuristics Using GPU

1017

Fact 1. Sequence of prefix sums (y1 , y2 , ..., yn ) of input sequence (x1 , x2 , ..., xn )
such, that
yk = yk−1 + xk = x1 + x2 + ... + xk for k = 2, 3, ..., n
where y1 = x1 can be calculated in time O(log n) on the EREW PRAM machine
with O(n/ log n) processors.
From what we have stated above we can assume that the sum of n values
can be calculated in time O(log n) on O(n/ log n) – processors EREW PRAM
machine.
Fact 2. The minimal and the maximal value of input sequence (x1 , x2 , ..., xn )
can be determined in the time O(log n) on the EREW PRAM machine with
O(n/ log n) processors.
Fact 3. The value of y = (y1 , y2 , . . . , yn ) where yi = f (xi ), x = (x1 , x2 , . . . , xn )
can be calculated on the CREW PRAM machine with n processors in a time
O(c) = O(1), where c is a time needed to calculate the single value of yi = f (xi ).
Fact 4. The problem formulated in the previous fact can be calculated in the
time O(log n) on O(n log n) processors.
If we do not have such a big number of processors we can use such a fact to keep
the same cost:
Fact 5. If the algorithm A works on p – processors PRAM in the time t, then
for every p < p exists an algorithm A for the same problem which works on p
– processors PRAM in time O(pt/p ).
In each iteration we have to ﬁnd a goal function value for a single ﬁxed π.
Calculations can be spread into parallel processors in a few ways.
Theorem 1. For a fixed π the value of criterion function for problems F ||Cmax
and F ||Csum can be found on the CREW PRAM machine in the time O(n + m)
by using m processors.
Proof. Without the loss of generality one can assume that π = (1, 2, . . . , n).
Calculations of Ci,j by using (2) have been clustered. Cluster k contains values
Cij such that i + j − 1 = k, k = 1, 2, . . . , n + m − 1 and requires at most m
processors. Clusters are processed in the order k = 1, 2, . . . , n+m−1. The cluster
k is processed in parallel on at most m processors. The calculations sequence
is shown in Fig. 2 on the background of the grid graph commonly used for the
ﬂow shop problem. Values linked by dashed lines constitute a single cluster.
n
The value of Cmax criterion is simple Cm,n . To calculate Csum = j=1 Cm,j we
need to add n values Cm,j , which can be done sequentially in n iterations or in
parallel by using m processors with the complexity O(n/m + log m). Finally, the
computational complexity of determining the criterion value for F ||Cmax and
F ||Csum problems is O(n + m) by using m processors.

1018

W. Bo˙zejko, C. Smutnicki, and M. Uchro´
nski

Cm,n
Cm,n-1

2
1

processor 1
1

processor 2

2

.
.
.

Cm,2
m

Cm,1

processor m

Fig. 2. Computing Cij order using m threads

nm
n
Fact 1. Speedup of method from Theorem 1 is O( n+m
), eﬃciency is O( n+m
)1 .

Theorem 2. For a fixed π the value of criterion function for problems F ||Cmax
and F ||Csum can be found on the CREW PRAM machine in the time O(n + m)
nm
by using O( n+m
) processors.
Proof. Without the loss of generality one can assume that π = (1, 2, . . . , n).
We based on the scheme of calculations shown in Fig. 2. Let p ≤ m be the
number of used processors. The calculation process will be carried out for levels
k = 1, 2, . . . , d, d = n+m−1 in this order. On the level k we perform a calculation
of nk values Ci,j such that i + j − 1 = k, dk=1 nk = nm.
We cluster nk elements on the level k into

nk
p

groups; ﬁrst

nk
p

groups

contain p elements each, whereas the remaining elements (at most p) belong to
the last group. Parallel computations on the level k are performed in the time
O( npk ). The total calculation time is equal to the sum over all levels and is of
order
d
d
nm
nm
nk
nk
+1 =
+d=
+ n + m − 1.
(3)
≤
p
p
p
p
k=1

k=1

We are seeking for the number of processors p, 1 ≤ p ≤ m, for which eﬃciency
of parallel algorithm is O(1), which ensures cost optimality of the method. The
1

Evaluation is true with a certain constant multiplier.

Parallel Calculating of the Goal Function in Metaheuristics Using GPU

1019

value p can be found from the following condition
1
p

nm
p

nm
= c = O(1)
+n+m−1

(4)

for some constant c < 1. After a few simple transformations of (4) we get
p=

nm
n+m−1

1
−1
c

= O(

nm
).
n+m

(5)

nm
Setting p = O( n+m
) we obtain the total calculation time of Cij values equals

O(

nm
nm
+ n + m − 1) = O( nm + n + m) = O(n + m).
p
n+m

(6)

nm
Fact 2. Speedup of the method based on Theorem 2 is O( n+m
), cost is O(nm).

The method is cost optimal and allows one to control eﬃciency as well as speed of
calculations by choosing the number of processors and adjusting the parameters
of calculations to the real number of parallel processors existing in the system.
Besides, Theorem 2 provides the “optimal” number of processors that ensures the
cost optimality of this method. This number can be set by a ﬂexible adaptation
of the number of processors to both sizes of the problem, namely n and m
simultaneously.

efficiency

1,000

m=10

0,800

m=20
0,600

m=30
m=40

0,400

m=50

0,200

0,000

number of jobs n

Fig. 3. Eﬃciency of the method from Theorem 2

4

Experimental Results

The parallel algorithm for the considered problem of calculating makespan in
permutation ﬂow shop problem was coded in C (CUDA) for GPU, ran on the

1020

W. Bo˙zejko, C. Smutnicki, and M. Uchro´
nski

Tesla C870 GPU (512 GFLOPS) with 128 streaming processor cores and tested
on the benchmark problems of Taillard. The benchmark set contains 120 particularly hard instances of 12 diﬀerent sizes. For each size (group) n × m: 20 × 5,
20 × 10, 20 × 20, 50 × 5, 50 × 10, 50 × 20, 100 × 5, 100 × 10, 100 × 20, 200 × 10,
200 × 20, 500 × 20, a sample of 10 was provided. The considered algorithm
showed as Algorithm 1 uses m GPU processors for calculating makespan. Its sequential version is obtained by assigning p = 1 and it is also executed on GPU.
Algorithm 2 presents details of the parallel method coded in CUDA.
m=5

m = 10

m = 20

1,2
0,97

1
ts [ms]

0,8
0,6

0,49

0,4
0,2

0,20
0,05

0,10

0,25

0,49
0,24

0,12

0
20

50
n

100

Fig. 4. The one thread algorithm computational times

Algorithm 1. Parallel algorithm of the makespan calculating
parfor i = 1 to p (for each processor)
for j = 1 to n + m − 1 do
x=j−i+1
if x ≥ 1 and x ≤ n then
Cx,i = max{Cx−1,i , Cx,i−1 } + px,i
end if
end for
end of parfor.

Each multiprocessor of the TESLA C870 GPU has on-chip memory of the
four following types:
– one set of local 32-bit registers per processor,
– a parallel data cache or shared memory that is shared by all scalar processors
cores and is where the shared memory space resides,
– a read-only constant cache that is shared by all scalar processor cores and
speeds up reads from the constant memory space, which is a read-only region
of device memory,
– a read-only texture cache that is shared by all scalar processor cores and
speeds up reads from the texture memory space, which is a read-only region
of device memory.

Parallel Calculating of the Goal Function in Metaheuristics Using GPU

1021

Algorithm 2. Parallel algorithm for makespan calculating coded in CUDA
__global__ void cmax(int *c, int n, int m, int *cmax) {
int idy;
int idx = blockIdx.x * blockDim.x + threadIdx.x + 1;
if(idx<=m) {
for(int j=1;j<m+n;j++) {
idy = j - idx + 1;
if(idy>=1&&idy<=n) {
c[idx*(n+1)+idy]=max(c[(idx-1)*(n+1)+idy],
c[idx*(n+1)+(idy-1)]) + tex2D(tex,idy ,idx);
if(idx==m&&idy==n) cmax[0]=c[idx*(n+1)+idy];
} } } }
int main() { //Kernel invocation
int blockSize = 16;
int nBlocks = M/blockSize + (M%blockSize == 0?0:1);
cmax<<< nBlocks, blockSize >>> (devC, N, M, devCmax);
}

There is no need to copy table with processing times before each calculating of
makespan so this table was copied once to very fast read-only texture memory.
Therefore timings are measured without this preparing time. Texture memory
is cached. Table C was allocated in global memory. After calculations makespan
on GPU value of Cmax is copied to the CPU. This operation is very fast (only
one-element table is copied).
The sequential algorithm using one GPU processor was coded with the aim
of determining the speedup value which can be obtained by a parallel algorithm.
Table 1 shows computational times for the sequential and the parallel algorithm
as well as speedup. The value of relative speedup s can by found by the following
Table 1. Experimental results for Taillard’s instances
n×m
20 × 5
20 × 10
20 × 20
50 × 5
50 × 10
50 × 20
100 × 5
100 × 10
100 × 20
200 × 10
200 × 20
500 × 20

p
5
10
20
5
10
20
5
10
20
10
20
20

tp [ms]
0.0271
0.0309
0.0386
0.0480
0.0518
0.0601
0.0835
0.0874
0.0968
0.1582
0.1697
0.3919

ts [ms]
0.0526
0.1022
0.2014
0.1244
0.2469
0.4909
0.2449
0.4885
0.9740
0.9716
1.9403
4.8392

speedup s
1.94
3.31
5.22
2.59
4.76
8.16
2.93
5.59
10.06
6.14
11.43
12.35

1022

W. Bo˙zejko, C. Smutnicki, and M. Uchro´
nski

m=5

m = 10

m = 20

0,12
0,097

0,1
tp [ms]

0,084

0,087

0,08
0,060

0,06
0,04

0,048

0,052

0,039
0,0270,031

0,02
0
20

50

100

n

Fig. 5. The parallel algorithm computational times

expression s = ttps , where ts constitutes the computational time of sequential
algorithm and tp - computational time of parallel algorithm. Figure 4 shows
computational times of the sequential algorithm for diﬀerent sizes of problem.
The algorithm computational time increases with the size of a problem. For the
ﬁxed number of jobs 100% increasing of the number of machines results in 100%
increasing computational time.
Figure 4 shows computational times of the parallel algorithm for diﬀerent
sizes of the problem. For the ﬁxed number of jobs the increase of the number of
machines results in a small computational time increase. Increase size of problem
results in increasing the speedup of parallel algorithm in comparison with the
sequential algorithm. Figure 6 conﬁrms theoretical eﬀectiveness on the basis of
Theorem 2 shown on Figure 3. Obtained experimental results are fully common
with the theoretical analysis.
1

effectiveness

0,8
0,6
0,4
0,2
0
0

100

200

300

400

n

Fig. 6. Experimental eﬀectiveness

500

Parallel Calculating of the Goal Function in Metaheuristics Using GPU

5

1023

Conclusions

We propose taking advantage of modern many-core computing processors GPU
to accelerate local search algorithm’s work. Results, for a classic ﬂow shop
scheduling problem, allow us to obtain a speedup which is proportional to the
number of machines m from the problem deﬁnition. Theorems presented in this
paper can be easily extended to the EREW PRAM model, with exclusive read,
which requires an additional O(log n) time, however the architecture of the used
GPU allows us to implement CREW algorithms (with a possibility of concurrent read). On the other side the shared memory usage is connected with a huge
time latency (400-600 cycles of the clock) comparing to the local memory of a
processor or the textures (constant) memory. Therefore, a further acceleration
of the algorithm is possible under condition of the methods adaptation to the
GPU memory access speciﬁc.

References
1. Aarts, E.H.L., Verhoeven, M.: Local search. In: Dell’Amico, M., Maﬃoli, F.,
Martello, S. (eds.) Annotated bibliographies in Combinatorial Optimization, Wiley, pp. 163–180. Wiley, Chichester (1997)
2. Bo˙zejko, W.: Parallel scheduling algorithms, PhD Thesis, Report 29/2003, Institute
of Engineering Cybernetics, Wroclaw University of Technology 1–205 (2003)
3. Fiechter, C.N.: A parallel tabu search algorithm for large traveling salesman problems. Discrete Applied Mathematics 51, 243–267 (1994)
4. Grama, A., Kumar, V.: State of the Art in Parallel Search Techniques for Discrete
Optimization Problems. IEEE Transactions on Knowledge and Data Engineering 11,
28–35 (1999)
5. Nowicki, E., Smutnicki, C.: A fast tabu search algorithm for the permutation ﬂow
shop problem. European Journal of Operational Research 91, 160–175 (1996)
6. Nowicki, E., Smutnicki, C.: Some aspects of scatter search in the ﬂow-shop problem.
European Journal of Operational Research 169, 654–666 (2006)
7. Porto, S.C., Ribeiro, C.C.: Parallel tabu search message passing synchronous strategies for task scheduling under precedence constraints. Journal of Heuristics 1, 207–
223 (1995)
8. Porto, S.C., Ribeiro, C.C.: A tabu search approach to task scheduling on heterogeneous processors under precedence constraints. International Journal of High Speed
Computing 7, 45–71 (1995)
9. Taillard, E.: Benchmarks for basic scheduling problems. European Journal of Operational Research 64, 278–285 (1993)

