The Data-Flow Equations of Checkpointing
in Reverse Automatic Diﬀerentiation
Benjamin Dauvergne and Laurent Hasco¨et
INRIA Sophia-Antipolis, TROPICS team,
2004 Route des lucioles, BP 93, 06902 Sophia-Antipolis, France
Abstract. Checkpointing is a technique to reduce the memory consumption of adjoint programs produced by reverse Automatic Diﬀerentiation. However, checkpointing also uses a non-negligible memory space
for the so-called “snapshots”. We analyze the data-ﬂow of checkpointing, yielding a precise characterization of all possible memory-optimal
options for snapshots. This characterization is formally derived from the
structure of checkpoints and from classical data-ﬂow equations. In particular, we select two very diﬀerent options and study their behavior
on a number of real codes. Although no option is uniformly better, the
so-called “lazy-snapshot” option appears preferable in general.

1

Introduction

Mathematical derivatives are a key ingredient in Scientiﬁc Computation. In particular, gradients are essential in optimization and inverse problems. The methods to compute gradients can be classiﬁed in two categories. In the ﬁrst category,
methods use CPU more intensively because several operations are duplicated.
This can be through repeated tangent directional derivatives, or through reverse
Automatic Diﬀerentation using the “Recompute-All” strategy. This is not the
context of this paper. In the second category, methods spare duplicated operations through increased memory use. This encompasses hand-coded resolution of
the “adjoint equations” and reverse Automatic Diﬀerentation using the “StoreAll” strategy, which is the context of this work.
Being a software transformation technique, reverse AD can and must take
advantage from software analysis and compiler technology [1] to minimize these
eﬃciency problems. In this paper, we will analyze “checkpointing”, an AD technique to trade repeated computation for memory consumption, with the tools of
compiler data-ﬂow analysis. Checkpointing oﬀers a range of options that inﬂuence the resulting diﬀerentiated code. Our goal is to formalize these options and
to ﬁnd which ones are optimal. This study is part of a general eﬀort to formalize
all the compiler techniques useful to reverse AD, so that AD tools can make the
right choices using a ﬁrmly established basis.

2

Reverse Automatic Diﬀerentiation

Automatic Diﬀerentiation by program transformation takes a program P that
computes a diﬀerentiable function F , and creates a new program that comV.N. Alexandrov et al. (Eds.): ICCS 2006, Part IV, LNCS 3994, pp. 566–573, 2006.
c Springer-Verlag Berlin Heidelberg 2006

The Data-Flow Equations of Checkpointing in Reverse AD

567

putes derivatives of F . Based on the chain rule of calculus, AD iserts into P
new “derivative” statements, each one corresponding to one original statement
of P . In particular, reverse AD creates a program P that computes gradients.
In P , the derivative statements corresponding to the original statements are
executed in reverse order compared to P . The derivative statements use some
of the values used by their original statement, and therefore the original state→
−
ments must be executed in a preliminary “forward sweep” P , which produces
the original values that are used by the derivative statements forming the “back←
−
ward sweep” P . This is illustrated by Fig. 1, in which we have readily split P
in three successive parts, U Upstream, C Center, and D Downstream. In our
context, original values are made available to the backward sweep through PUSH
and POP routines, using a stack that we call the “tape”. Not all original values

time:
t1
t2 PUSH

U

C

forward sweep

D

C

backward sweep

D

t3

POP

U

Fig. 1. Basic structure of reverse diﬀerentiated programs

are required in the backward sweep. Because of the nature of diﬀerentiation,
values that are used only “linearly” are not required. The “To Be Recorded”
(TBR) analysis [2, 6] ﬁnds the set of these required values denoted by Req.
Set Req evolves as the forward sweep advances. For example in Fig. 1, TBR
←
−
←
−
analysis of U ﬁnds the variable values required by U (i.e. actually use( U )),
→
−
←
−
which must be preserved between the end of U and the beginning of U . To
this end, each time a required value is going to be overwritten by a statement,
it is PUSH’ed beforehand, and it is POP’ped just before the derivative of this
statement.
Although somewhat complex, reverse AD can be easily applied by an automatic tool, and has enormous advantages regarding the number of computation
steps needed to obtain the gradient [4, chapter 3].
In [5], we studied the data-ﬂow properties of reverse diﬀerentiated programs,
in the basic case of Fig. 1, i.e. with no checkpointing. We formalized the structure
of these programs and derived specialized data-ﬂow equations for the “adjoint
liveness” analysis, which ﬁnds original statements that are useless in the diﬀerentiated program, and for the TBR analysis. In this paper, we will focus on the
consequences of introducing checkpointing. In this respect this paper, although
stand-alone, is a continuation of [5].

568

3

B. Dauvergne and L. Hasco¨et

The Equations of Checkpointing Snapshots

Checkpointing modiﬁes the diﬀerentiated code structure to reduce the peak
memory consumption. When code fragment C is “checkpointed” (notation [C]),
the adjoint now written [C]; D is formally deﬁned by the recursive rewrite rule:
Req

[C]; D = PUSH(Sbk);
PUSH(Snp);
C;
(1)

ReqD D
POP(Snp);
ReqC C
POP(Sbk);

Boxes show terms to be rewritten, whereas terms outside boxes are plain pieces
of code. This new code structure is sketched in Fig.2, to be compared with
Fig. 1. Now, C is ﬁrst run in its original version, so that the tape consumed by

time:
U
t1
PU
SH
t2
t3
t4

Sbk Snp

CHECKPOINTING
C

D

C
D

POP
U

C
Fig. 2. Checkpointing in reverse AD

→ ←
−
−
→ ←
−
−
D D; D (“;” denotes code sequence) is freed before execution of C
C; C.
The peak memory consumption for [C]; D is thus reduced to the maximum of
→
−
→
−
the peak after C and the peak after D. However, duplicate execution of C
requires that “enough” variables (the “snapshot” Snp) are preserved to restore
the context of execution. This also uses memory space, although less than the
→
−
tape for C . To not lose the beneﬁt of checkpointing, it is therefore essential we
ﬁnd the smallest snapshot for a ﬁxed C, and in further studies the placement of
C that uses least memory.
This proves tricky: a larger snapshot can mean smaller tapes, and conversely.
Therefore, unlike what happens with no checkpoints, there is no unique best
choice for these sets. There are several “optimal” choices, among which none
is better nor worse than the others. Our goal is to establish the constraints
that deﬁne and link the “snapshot” and “tape” sets, and to characterize all
the optimal choices. For our AD tool tapenade, we settled on one solution (cf
Sect. 4) that our benchmarks indicated as a mean best choice.

The Data-Flow Equations of Checkpointing in Reverse AD

569

Four Unknown Sets of Variables. Let’s examine checkpointing deﬁnition (1)
in more detail. The rewrite context Req is the incoming “required set” of variables
imposed by U , that must be preserved across execution of Req [C]; D. On the
other hand, ReqD and ReqC are the sets of variables that C and D will
be required to preserve, respectively. For us, ReqD and ReqC are unknowns,
to be determined together with the snapshot. About the snapshot itself, due
to the stack structure, there are two places where variables may be restored
→
−
←
−
from the stack: before C and before U . Therefore we introduce two snapshot
sets: Snp, the “usual snapshot”, contains variables to be restored just before
C, thus ensuring that their value is the same for both executions of C. Sbk,
←
−
the “backward snapshot”, contains variables to be restored just before U . Thus,
whatever happens to these variables during Req [C]; D, their value is preserved
←
−
for U . Using Sbk instead of Snp and ReqC may improve memory traﬃc. In total,
we have four “unknown” sets to choose: ReqD , ReqC , Sbk and Snp. Those sets
must respect constraints parameterized upon Req, ReqD , ReqC , Sbk, Snp, and
upon the ﬁxed data-ﬂow sets use (variables used) and out (variables partly
written) of the code fragments C, D, C, and D. These constraints will guarantee
that checkpointing preserves the semantics, i.e. the computed derivatives.
Two Necessary and Suﬃcient Conditions. Fig. 1 shows the diﬀerentiated
program in the reference case with no checkpointing. This reference program is
assumed correct. All we need to guarantee is that the result of the diﬀerentiated
program, i.e. the derivatives, remain the same when checkpointing is done. This
can be easily formulated in terms of data-ﬂow sets. We observe that the order of
the backward sweeps is not modiﬁed by checkpointing. Therefore the derivatives
are preserved if and only if the original, non-diﬀerentiated variables that are used
during the backward sweeps hold the same values. In other words, the snapshot
and the tape must preserve the use set of C between time t1 and t3 i.e.
⎛
⎞
PUSH(Sbk);
⎜ PUSH(Snp); ⎟
⎜
⎟
⎟ use(ReqC C) = ∅
(2)
out ⎜
⎜ C;
⎟
⎝ ReqD D; ⎠
POP(Snp);
←
−
and the use set of U , which is Req by deﬁnition, between time t1 and t4 i.e.
⎞
⎛
PUSH(Sbk);
⎜ PUSH(Snp); ⎟
⎟
⎜
⎟
⎜ C;
⎟
⎜
⎜
(3)
out ⎜ ReqD D; ⎟
⎟ Req = ∅ .
⎜ POP(Snp); ⎟
⎟
⎜
⎝ ReqC C; ⎠
POP(Sbk);
The rest is purely mechanical. Classically, the out set of a code sequence is:
out(A; B) = out(A) ∪ out(B) ,

570

B. Dauvergne and L. Hasco¨et

except in the special case of a PUSH/POP pair, which restore their argument:
out(PUSH(v); A; POP(v)) = out(A) \ {v} .
Also, the mechanism of reverse AD ensures that the variables in the required context are actually preserved, and this does not aﬀect the variables used. Writing
for short A ∅ A, we have:
out(Req

A) = out(A) \ Req

use(Req

A) = use(A) .

Also, a PUSH alone overwrites no variable. Therefore, equation (2) becomes:
out(C) ∪ (out(D) \ ReqD ) \ Snp

use(C) = ∅

(4)

and equation (3) becomes:
out(C) ∪ (out(D) \ ReqD ) \ Snp ∪ out(C) \ ReqC

\ Sbk

Req = ∅ . (5)

From (4) and (5), we obtain equivalent conditions on Sbk, Snp, ReqD and ReqC :
Sbk ⊇ (out(C) ∪ (out(D) \ ReqD )) \ Snp
∪ (out(C) \ ReqC ) ∩ Req
Snp ⊇ out(C) ∪ (out(D) \ ReqD ) ∩ use(C) ∪ (Req \ Sbk)
ReqD ⊇ (out(D) \ Snp) ∩ use(C) ∪ (Req \ Sbk)
ReqC ⊇ (out(C) \ Sbk) ∩ Req .
Notice the cycles in these inequations. If we add a variable into Snp, we may be
allowed to remove it from ReqD , and vice versa: as we said, there is no unique best
solution. Let’s look for the minimal solutions, i.e. the solutions to the equations
we obtain by replacing the “⊇” sign by a simple “=”.
Solving for the Unknown Sets. Manipulation of these equations is tedious
and error-prone. Therefore, we have been using a symbolic computation system
(e.g. Maple [8]). Basically, we have inlined the equation of, say, Snp into the
other equations, and so on until we obtained ﬁxed point equations with a single
unknown X of the form
X = A ∪ (X ∩ B) ,
whose solutions are of the form “A plus some subset of B”. The solutions are
expressed in terms of the following sets:
Snp0
Opt1
Opt2
Opt3

= out(C) ∩ (use(C) ∪ (Req \ out(C)))
= Req ∩ out(C) ∩ use(C)
= Req ∩ out(C) \ use(C)
= out(D) ∩ (use(C) ∪ Req) \ out(C) .

(6)

The Data-Flow Equations of Checkpointing in Reverse AD

571

−
For each partition of Opt1 in two sets Opt+
1 and Opt1 , and similarly for Opt2
and Opt3 , the following is a minimal solution of our problem:

Sbk
Snp
ReqD
ReqC

+
= Opt+
1 ∪ Opt2
= Snp0 ∪ Opt−
∪ Opt+
2
3
=
Opt−
3
−
= Opt−
1 ∪ Opt2 .

(7)

Any quadruplet of sets (Sbk, Snp, ReqD , ReqC ) that preserves the derivatives
(compared to the no-checkpoint code) is equal or larger than one of these minimal
solutions. Notice that Opt1 ⊆ Snp0 , and Snp0 , Opt2 , and Opt3 are disjoint.

4

Discussion and Experimental Results

The ﬁnal decision for sets Sbk, Snp, ReqD , and ReqC depends on each particular
context. No strategy is systematically best. We looked at two options.
We examined ﬁrst the option that was implemented until recently in our
AD tool tapenade [7]. We call it “eager snapshots”. This option stores enough
variables in the snapshots to reduce the sets ReqD and ReqC as much as possible, therefore reducing the number of subsequent PUSH/POP in D and C. Equations (7) show that we can even make these sets empty, but experiments showed
that making ReqD empty can cost too much memory space in some cases.
As always, the problem behind this is undecidability of array indexing: since
we can’t always tell whether two array indexes designate the same element or
not, the “eager snapshot” strategy may end up storing an entire array whereas
only one array element was actually concerned.
−
Therefore “eager snapshot” chooses Opt−
1 and Opt2 empty but
Opt+
3 = out(D) ∩ (use(C) \ Req) \ out(C)
Opt−
3 = out(D) ∩ Req \ out(C)
which gives:
Sbk = Req ∩ out(C)
Snp = (out(C) ∩ (use(C) ∪ Req \ out(C)))∪
(out(D) ∩ use(C) \ Req \ out(C))
ReqD = out(D) ∩ Req \ out(C)
ReqC = ∅ .

(8)

Notice that intersection between Sbk and Snp is nonempty, and requires a special
stack mechanism to avoid duplicate storage space.
We examined another option that is to keep the snapshot as small as possible,
therefore leaving most of the storage work to the TBR mechanism inside D and
C. We call it “lazy snapshots”, and it is now the default strategy in tapenade.
Underlying is the idea that the TBR mechanism is eﬃcient on arrays because
when an array element is overwritten by a statement, only this element is saved.

572

B. Dauvergne and L. Hasco¨et

+
+
Therefore, “lazy snapshot” chooses all Opt+
1 , Opt2 , and Opt3 empty, yielding:

Sbk
Snp
ReqD
ReqC

=∅
= out(C) ∩ (Req ∪ use(C))
= out(D) ∩ (Req ∪ use(C)) \ out(C)
= out(C) ∩ Req .

(9)

We ran tapenade on our validation application suite, for each of the two
options. The results are shown in Table 1. We observe that lazy snapshots perform better in general. Actually, we could show the potential advantage of eager
snapshots only on a hand-written example, where the checkpointed part C repeatedly overwrites elements of an array in Req, making TBR mechanism more
expensive than a global snapshot of the array. On real applications, however,
this case is rare and lazy snapshots work better.
Table 1. Comparison of the eager and lazy snapshot approaches on a number of small
to large applications
Code
Domain
Orig. time Adj. time Eager (8) Lazy (9)
OPA
oceanography
110 s
780 s 480 Mb 479 Mb
STICS agronomy
1.8 s
35 s 229 Mb 229 Mb
UNS2D CFD
2.7 s
23 s 248 Mb 185 Mb
SAIL
agronomy
5.6 s
17 s 1.6 Mb 1.5 Mb
THYC thermodynamics
2.7 s
12 s 33.7 Mb 18.3 Mb
LIDAR optics
4.3 s
10 s 14.6 Mb 14.6 Mb
CURVE shape optim
0.7 s
2.7 s 1.44 Mb 0.59 Mb
SONIC CFD
0.03 s
0.2 s 3.55 Mb 2.02 Mb
Contrived example
0.02 s
0.1 s 8.20 Mb 11.72 Mb

Whatever the option chosen, equations (7) naturally capture all interactions
between successive snapshots. For example, if several successive snapshots all
use an array A, and only the last snapshot overwrites A, it is well known that A
must be saved only in the last snapshot. However, when an AD tool does not
rely on a formalization of checkpointing such as the one we introduce here, it
may very well happen that A is stored by all the snapshots.

5

Conclusion

We have formalized the checkpointing technique in the context of reverse AD by
program transformation. Checkpointing relies on saving a number of variables
and several options are available regarding which variables are saved and when.
Using our formalization and with the help of a symbolic computation system, we
found that no option is strictly better than all others and we could specify all the

The Data-Flow Equations of Checkpointing in Reverse AD

573

possible optimal options. This gives us safer and more reliable implementation
in AD tools.
We selected two possible optimal options and implemented them in the AD
tool tapenade. Experience shows that the option called “lazy snapshots” performs better on most cases.
However, we believe that for reverse AD of a given application code, the option
chosen need not be identical for all checkpoints. This formal description of all
the possible options allows us to look for the best option for each individual
checkpoint, based on static properties at this particular code location. In this
regard, we used symbolic computation again and came up with a very pleasant
property: for a given checkpoint, whatever the optimal option chosen for the
snapshot, the out set of this piece of code turns out to be always the same:
out(C; D) = out(C) ∪ ((out(D) ∪ out(C)) \ use(C)) \ Req .
If checkpoints are nested, this out set is what inﬂuences possible enclosing checkpoints. Therefore the choice of the optimal option is local to each checkpoint.
One of the current big challenges of reverse AD is to ﬁnd the best possible
placement of nested checkpoints. This was found [3] for one simple model case.
For arbitrary programs, our formulas show that the segmentation of a code into
the subsections U , C, and D has substantial impact on the memory usage, and
they can help ﬁnding good such segmentations.

References
1. A. Aho, R. Sethi, and J. Ullman. Compilers: Principles, Techniques and Tools.
Addison-Wesley, 1986.
2. C. Faure and U. Naumann. Minimizing the tape size. In G. Corliss, C. Faure,
A. Griewank, L. Hasco¨et, and U. Naumann, editors, Automatic Diﬀerentiation of
Algorithms: From Simulation to Optimization, Computer and Information Science,
chapter 34, pages 293–298. Springer, New York, NY, 2001.
3. Andreas Griewank. Achieving logarithmic growth of temporal and spatial complexity in reverse automatic diﬀerentiation. Optimization Methods and Software,
1:35–54, 1992.
4. Andreas Griewank. Evaluating Derivatives: Principles and Techniques of Algorithmic Diﬀerentiation. Number 19 in Frontiers in Appl. Math. SIAM, Philadelphia,
PA, 2000.
5. L. Hasco¨et and M. Araya-Polo. The adjoint data-ﬂow analyses: Formalization,
properties, and applications. In H. M. B¨
ucker, G. Corliss, P. Hovland, U. Naumann,
and B. Norris, editors, Automatic Diﬀerentiation: Applications, Theory, and Tools,
Lecture Notes in Computational Science and Engineering. Springer, 2005.
6. L. Hasco¨et, U. Naumann, and V. Pascual. “to be recorded” analysis in reverse-mode
automatic diﬀerentiation. Future Generation Computer Systems, 21(8), 2004.
7. L. Hasco¨et and V Pascual. Tapenade 2.1 user’s guide. Technical report 0300, INRIA,
2004. http://www.inria.fr/rrrt/rt-0300.html.
8. Darren Redfern. The Maple handbook, Maple V, release 4. Springer, 1996.

