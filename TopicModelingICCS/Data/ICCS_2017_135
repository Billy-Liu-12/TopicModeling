Available online at www.sciencedirect.com

ScienceDirect

This space is reserved for the Procedia header, do not use it
This space
is reserved for the Procedia header, do not use it
Procedia Computer Science 108C (2017) 68–78
This space is reserved for the Procedia header, do not use it

International Conference on Computational Science, ICCS 2017, 12-14 June 2017,
Zurich, Switzerland

Graph Ranking Guarantees for Numerical Approximations
Graph Ranking Guarantees for Numerical Approximations
to Katz for
Centrality
Graph Ranking Guarantees
Numerical Approximations
to Katz Centrality
2
Eisha Nathan11 , Geoffrey Sanders
, James
Fairbanks33 , Van Emden Henson22 , and
to Katz
Centrality
Eisha Nathan , Geoffrey Sanders2 , James Fairbanks
, Van Emden Henson , and
1
David A. Bader1
David
A. Bader
2
Eisha Nathan
, Geoffrey Sanders
, James
Fairbanks3 , Van Emden Henson2 , and
1
School of Computational Science and Engineering,1 Georgia Institute of Technology
1
David
A. Bader Georgia Institute of Technology
School of Computational{enathan3,bader}@gatech.edu
Science
and Engineering,
1

2
1
2
2

{enathan3,bader}@gatech.edu
Center for Applied Scientific
Computing, Lawrence Livermore National Laboratory
School of
Science
and Engineering,
Institute
of Technology
Center
forComputational
Applied Scientific
Computing,
Lawrence Georgia
Livermore
National
Laboratory
{sanders29,henson5}@llnl.gov
{enathan3,bader}@gatech.edu
3{sanders29,henson5}@llnl.gov
Georgia Tech Research Institute
3
Center for Applied Scientific
Computing,
Lawrence
Livermore National Laboratory
Georgia
Tech Research
Institute
james.fairbanks@gtri.gatech.edu
{sanders29,henson5}@llnl.gov
james.fairbanks@gtri.gatech.edu
3
Georgia Tech Research Institute
james.fairbanks@gtri.gatech.edu

Abstract
Abstract
Graphs and networks are prevalent in modeling relational datasets from many fields of research.
Graphs
networks
are prevalent
in modeling
fromKatz
many
fields of research.
By
usingand
iterative
solvers
to approximate
graph relational
measures datasets
(specifically
Centrality),
we can
Abstract
By
using
iterative
solvers
to
approximate
graph
measures
(specifically
Katz
Centrality),
we can
obtain
a
ranking
vector
consisting
of
a
number
for
each
vertex
in
the
graph
identifying
its
Graphs
and
networks
are prevalent
inofmodeling
relational
datasets
from
many
fields
of research.
obtain
a
ranking
vector
consisting
a
number
for
each
vertex
in
the
graph
identifying
its
relative
importance.
We
use
the
residual
to
accurately
estimate
how
much
of
the
ranking
from
By
usingimportance.
iterative solvers
to approximate
graph
measures
(specifically
Katz of
Centrality),
wefrom
can
relative
We
use
the
residual
to
accurately
estimate
how
much
the
ranking
an
approximate
ranking
given
the vertex
exact solution.
Usingidentifying
probabilistic
obtain
a rankingsolution
vector matches
consistingthe
a number
forby
each
in the graph
its
an
approximate
solution
matches
theofanalysis
ranking
given
by
the exact of
solution.
Using probabilistic
matrix
norms
and
applying
numerical
to
the
computation
Katz
Centrality,
we obtain
relative
importance.
We
use
the
residual
to
accurately
estimate
how
much
of
the
ranking
from
matrix
norms
applying numerical
analysis to
the computation
of Katz
Centrality,
we obtain
bounds
on theand
accuracy
the approximation
compared
to exact
the exact
solution
with
respect
to
an
approximate
solution of
matches
the ranking given
by the
solution.
Using
probabilistic
bounds
on
the
accuracy
of
the
approximation
compared
to
the
exact
solution
with
respect
to
the
highly
ranked
nodes.
This
relates
the
numerical
accuracy
of
the
linear
solver
to
the
data
matrix
norms
and applying
numerical
analysis
to the computation
of
Katz
Centrality,
we
obtain
the
highly
ranked
nodes.
This
relates
the
numerical
accuracy
of
the
linear
solver
to
the
data
analysis
accuracy
of finding
the approximation
correct ranking.compared
In particular,
answer
the question
of which
bounds
on
the accuracy
of the
to thewe
exact
solution
with respect
to
analysis
accuracy
of
finding
the
correct
ranking. In particular,
we
answer
the
question
of which
pairwise
rankings
are
reliable
given
an
approximate
solution
to
the
linear
system.
Experiments
the
highly
ranked
nodes.
This
relates
the
numerical
accuracy
of
the
linear
solver
to
the
data
pairwise
rankings
arenetworks
reliable given
approximate
solution and
to the
linear system. million
Experiments
on
manyaccuracy
real-world
toan
several
million
vertices
edges
analysis
of finding
theup
correct
ranking.
In particular,
weseveral
answerhundred
the question
of which
on
many
real-world
networks
up
to
several
million
vertices
and
several
hundred
million
edges
validate
our
theory
and
show
that
we
are
able
to
accurately
estimate
large
portions
of
the
pairwise
rankings
areand
reliable
given
an
approximate
solution
to the
linear large
system.
Experiments
validate
our
theory
show
that
we
are
able
to
accurately
estimate
portions
of
the
approximation.
By
analyzing
convergence
error,
we
develop
confidence
in
the
ranking
schemes
on
many real-world
networks convergence
up to severalerror,
million
vertices
and
several in
hundred
million
edges
approximation.
By
analyzing
we
develop
confidence
the
ranking
schemes
of
data mining.
validate
our theory and show that we are able to accurately estimate large portions of the
of data mining.
approximation.
By
analyzing
convergence
error, we
develop
confidence
Keywords:
graphs,
data
analysis,
numerical
katz
centrality,
rankingin the ranking schemes
©
2017 The Authors.
Published
by Elsevier
B.V. accuracy,
Keywords:
graphs,
data analysis,
accuracy,
katzInternational
centrality,Conference
ranking on Computational Science
Peer-review
under
responsibility
of the numerical
scientific committee
of the
of data mining.
Keywords: graphs, data analysis, numerical accuracy, katz centrality, ranking

1 Introduction
1 Introduction
Graphs are a very popular means of representing massive amounts of relational data. One of
1
Introduction
Graphs
a veryquestions
popular means
representing
massive
amounts
data.theOne
of
the mostare
popular
arisingoffrom
the analysis
of large
graphsofisrelational
to determine
most
the
most popular
questions
arising
fromimportance
the analysisis ofreferred
large graphs
is to determine
the most
important
in a graph.
Vertex
to asofcentrality,
centrality
Graphs
arevertices
a very popular
means
of representing
massive
amounts
relationaland
data.
One of
important
vertices
in
aprovide
graph.
Vertex
importance
is referred
to as centrality,
and
centrality
scores
can
be
used
to
rankings
on
the
vertices
of
a
graph.
While
there
exist
many
the
most
popular
questions
arising
from
the
analysis
of
large
graphs
is
to
determine
the
most
scores
can be used
to provide
rankings
on focus
the vertices
ofCentrality
a graph. because
While there
exist
many
such
centrality
measures,
in
this
work
we
on
Katz
of
its
analytical
important
vertices
in
a
graph.
Vertex
importance
is
referred
to
as
centrality,
and
centrality
such centrality measures, in this work we focus on Katz Centrality because of its analytical
scores can be used to provide rankings on the vertices of a graph. While there exist many
1
such centrality measures, in this work we focus on Katz Centrality because of its analytical
1
1877-0509 © 2017 The Authors. Published by Elsevier B.V.
Peer-review under responsibility of the scientific committee of the International Conference on Computational Science
10.1016/j.procs.2017.05.021

1

	

Numerical Approximations to Katz
EishaCentrality
Nathan et al. / Procedia Computer Science 108C (2017) 68–78

Nathan et al.

tractability. Efficiently solving for the Katz centrality in a graph involves solving a linear system.
Obtaining an exact solution via direct methods is prohibitively computationally expensive, since
we are required to take the inverse of a matrix. The most accurate way to obtain the exact
solution would be by Cholesky decomposition, which costs O(n2 ), where n is the number of
vertices in the graph. In many real networks the amount of data is massive and n can be
as large as millions or billions of vertices, so direct methods such as these do not scale and
are impractical. Moreover, there is no technique to compute an exact solution for a general
graph in finite precision arithmetic, so in practice, iterative methods are often used to obtain an
approximate solution. Iterative methods optimally cost O(m), where m is the number of edges
in the graph, but to achieve this optimal complexity the number of iterations must be limited.
Many real-world graphs are sparse and m  n2 [1]. In this paper we provide theoretical
guarantees (Theorem 1) on the accuracy of an approximate solution compared to the exact
solution to certify rankings in the approximation, and explain how they can be used to limit
the number of iterations in the iterative solver.
This work bridges the gap between the fields of numerical analysis and data mining by
understanding the effect that the error in a numerical problem has on the confidence of the
data analysis problem of ranking. We solve the data mining problem of ranking by solving
the numerical problem of obtaining a solution to a linear system. Using iterative methods to
obtain an approximate solution to this linear system inherently gives rise to some error in the
approximation. We explain how this error affects how much of the approximate solution is
accurate with respect to the unknown exact solution to the ranking problem. Additionally,
we develop practical algorithms that leverage our theory. For many application purposes it
is primarily the highly-ranked vertices that are of interest. Consider performing a web search
with Google. Typically anyone running a web search has enough human resources to examine
the top. In a Twitter graph, we might wish to identify the most influential voices in a subset of
Twitter users, or in a network modeling disease spread an analyst would be interested in finding
sites of disease origin. These queries are answered by examining the highly ranked vertices in
the graph.
In this paper, we obtain bounds on the accuracy of the approximation compared to the
exact solution using properties of error analysis on linear solvers. We validate our theoretical
guarantees of certifying the accuracy of the top ranked vertices across several real-world networks and show that our method is able to find these vertices in a fraction of the time compared
to the standard approach of running to machine precision. The main goal of the work is to
improve our understanding of how numerical accuracy affects data analysis accuracy.

1.1

Contributions

This paper makes the following contributions:
• A new error bound (Theorem 1) on elements of a ranking vector to provide graph ranking
guarantees to the computation of Katz Centrality.
• A new stopping criterion for iterative solvers to identify top ranked vertices in a graph
that reduces runtime compared to running a solver to machine precision.
• Empirical evidence of a tighter probabilistic upper bound on A2 compared to deterministic Gershgorin bounds for real-world graphs.
• Demonstrations that these bounds provide practical results in real datasets.
2

69

70	

Numerical Approximations to Katz
EishaCentrality
Nathan et al. / Procedia Computer Science 108C (2017) 68–78

1.2

Nathan et al.

Related Work

Many data analysis problems are answered by solving an induced numerical problem. We
present how numerical error in the approximation to the solution of a linear system affects
the solution to the original ranking problem. Several centrality measures can be expressed as
functions of the adjacency matrix of a graph [2]. Pagerank [13] is a common method for ranking
vertices in graphs, where a high score means random walks through the graph tend to visit the
highly ranked vertices. Similarly the exponential-based centrality measure weights walks of
1
length k by a factor of k!
[7]. Here, we address the ranking problem for Katz Centrality [9],
a centrality metric that measures the affinity between vertices as a weighted sum of the paths
between them.
Solving for many linear algebra based centrality measures directly is generally intractable so
iterative solvers are used to approximate them [4]. Understanding the error in the approximate
solution to the numerical problem is key to understanding the error in the data mining problem.
Ranking vertices in graphs and finding the top ranked vertices is of very practical relevance to
data analysts [6]. We focus on approximating the Katz score of the vertices in the graph to
a high enough accuracy to certify that the top of the ranking vector is accurate compared to
the exact solution. Several other methods for approximating Katz scores across the network
only examine paths up to a certain length [5] or employ low-rank approximation [12]. In [3],
the authors provide theoretical guarantees for pairwise Katz scores and provide an algorithm
to find the Katz scores from one vertex to the rest of the graph with reduced complexity. Our
work differs in that we provide confidence as to which portion of the global ranking is correct
and use the size of the residual to provide an accurate estimation of the ranking.
The main contribution of this paper is bounding the error between the approximate and
exact solutions to accurately certify top portions of the ranking with thorough experimentation
to validate our results. We derive the bound and provide error analysis in Section 2. Numerical
experiments validating the bound including analysis of both precision and performance of our
method are presented in Section 3. Finally, in Section 4, we conclude and discuss further uses
of this work.

2

Definitions and Theory

Let G = (V, E) be a graph, where V is the set of n vertices and E the set of m edges. Denote the
n×n adjacency matrix A of G with entries A(i, j) = 1 if there exists an edge from vertex i to j, 0
otherwise. In this work we deal with undirected, unweighted graphs so ∀i, j, A(i, j) = A(j, i) and
all edge weights are 1, although all the theory presented in this paper can easily be generalized
to the weighted case. The matrix 2-norm A2 is given by the largest singular value, σmax .
Katz centrality rankings quantify the ability of a vertex to initiate walks around the network.
The number of walks of length k from vertex i to j is Ak (i, j). The Katz score of vertex i counts
the number of closed walks ending at vertex i, while penalizing long walks through the network
by multiplying by a fixed user-chosen factor
for each edge used, where α ∈ [0, 1/A2 ). The
α
∞
k−1 k
A 1, where ei is the ith canonical basis
Katz centrality of vertex i is given by eTi
k=1 α
vector and 1 is the n × 1 vector of all ones. In practice the Neumann formula [16] is employed
to turn this series
∞ into a linear solver and we compute the Katz Centrality of all vertices in the
graph as c = k=1 αk−1 Ak 1 = A(I − αA)−1 1.
The iterative method we use is conjugate gradient (without a preconditioner) [14], although
the theory applies to other iterative techniques. Conjugate gradient is a popular technique to
approximate the solution x in a linear system M x = b, given M and b. Let M = I −αA so that
we solve the system M x = 1 for x and c is obtained with a simple matrix vector multiplication
3

	

Numerical Approximations to Katz
EishaCentrality
Nathan et al. / Procedia Computer Science 108C (2017) 68–78

Nathan et al.

as c = Ax in O(m). At each step k of the iterative solver we obtain new approximations
x(k) and c(k) to the exact solutions x∗ and c∗ respectively. The error at each step is denoted
as the difference between the exact and approximation, x∗ − x(k) 2 and the residual norm
as rk = 1 − M x(k) 2 , where  · 2 denotes the 2-norm. In practice as the exact solution is
not known, typical stopping criteria for the iterative solver use the residual norm, terminating
when it hits machine precision, rk ≈ 10−15 . The problem is more ill-conditioned and harder
as α → 1/A2 and typically requires more iterations to terminate and converge to machine
precision.

2.1

Error Analysis

We make the observation that if our goal is identification of the highly ranked vertices in a
graph, we ought to focus on ranking accuracy not numerical accuracy. This is because the
error in the data analysis problem of ranking is dfferent than the error in numerical problem of
solving the linear system: the relative ranking of vertices can be correct even without a fully
correct centrality vector. We theoretically guarantee the accuracy of the solution to numerical
problem needed to successfully answer the data mining question of ranking.
When M −1 1 is approximated, there will be differences between the approximate solution and
the exact solution. We prove that these differences along with the ranking values can indicate
how far down the ranking we can go before the approximation error makes it unreliable. Define
b(k) = π (k) c(k) , where π (k) is the permutation such that b(k) is the vector c(k) ordered in
(k)
(k)
decreasing order so that bi ≥ bi+1 .
Theorem 1. Let A, M , x(k) , c(k) , x∗ , c∗ , b(k) , b∗ , and rk be as previously defined. Define
λmin (M ) to be the smallest eigenvalue of the matrix M . Let σup be any upper bound on A2 .
(k)
(k)
Then for any i < j, the ranking of vertex i above j is correct if |bi − bj | > 2k for k =
σup
λmin (M ) rk .
Proof. Using foundations of error analysis in linear solvers, we can bound the point-wise error in
the ranking, which will then provide a sufficient error gap in the elements of the approximation
to the ranking vector.
b∗ − b(k) ∞ = c∗ − c(k) ∞ ≤ c∗ − c(k) 2

= Ax∗ − Ax(k) 2 ≤ A2 x∗ − x(k) 2

= A2 M −1 1 − x(k) 2 ≤ A2 M −1 2 1 − M x(k) 2

A2
σup
1 − M x(k) 2 ≤
rk
λmin (M )
λmin (M )
=: k
≤

(k)

Since bi
(k)
bi

−

(k)
bj

(k)

− b∗i < k and b∗j − bj

> 2k , then

b∗i

−

b∗j

(k)

< k , this means that b∗i − b∗j > bi

(k)

− bj

− 2k . If

> 0 meaning that the ranking of vertex i above j is correct.

We observe in practice that this bound is tight enough to produce relevant results in many
practical applications and lends itself to the development of a new stopping criterion for iterative
solvers when identifying the highly ranked vertices in a graph.
4

71

72	

Numerical Approximations to Katz
EishaCentrality
Nathan et al. / Procedia Computer Science 108C (2017) 68–78

2.2

Nathan et al.

New Stopping Criterion

Current methods for identifying the top vertices in a graph involve running an iterative solver to
machine precision to obtain an approximation of c∗ . We introduce a new stopping criterion to
find these top vertices that typically provides results much faster than existing methods, based
off of the theory developed in Theorem 1. Furthermore, our method provides theoretically
sound guarantees as to the correctness of the top vertices, unlike the common method of simply
running a solver to machine precision and blindly hoping the resulting vector is good enough
for the desired data mining task.
Suppose a user desires a set of j vertices containing the top R highly ranked vertices in a
graph, with precision φ0 . How large does j need to be before we can accurately certify that
the top vertices are in the set? We are not concerned with the internal ordering of this set, but
rather that the top R vertices are contained somewhere within the set of j vertices. We answer
this question using our theory. At each iteration of conjugate gradient, the current solution c(k)
is ordered in decreasing order to produce the vector b(k) as described earlier. We find the first
(k)
(k)
position j > R in b(k) where we find the necessary gap of |bR − bj | > 2k . The precision for
R
. If for this value of j we have the desired precision
these values of R and j is defined as φ = j−1
φ0 then we terminate, else we iterate again using conjugate gradient to obtain a more accurate
approximation. This procedure is given in Algorithm 1, for an adjacency matrix A, upper
bound σup on A2 , number of top vertices R, desired precision φ0 , and maximum number of
iterations kmax . Intuitively the precision shows how far past position R we must travel down
the vector to find the necessary gap to ensure we are returning the top R vertices in the graph.
Conjugate gradient can be organized to return x(k) , c(k) , and the residual norm rk at each
iteration (denoted CGiteration in Algorithm 1).
Algorithm 1: Obtain top R vertices in network with precision φ0
1

2
3
4
5
6
7

Function Top R
Data: A, σup , R, φ0 , kmax
Result: Set of j vertices s.t. top R vertices are contained within this set
k = 0; j = ∞
M = I − αA
R
while j−1
< φ0 and k < kmax do
(k) (k)
x , c , rk = CGiteration(M, x(k−1) )
b(k) = π (k) c(k)
σ
k = λminup(M ) rk

8
9

2.3

(k)

(k)

j = argmini>R |bR − bi | > 2k
k += 1

Bounds on A2

We obtain a tight bound on k which allows us to certify that the ranking of vertex i above j is
correct if the gap between two elements in the ranking vector is greater than our error bound,
(k)
(k)
|bi − bj | > 2k . Conjugate gradient readily provides the residual norm rk at each iteration,
and λmin (M ) can be computed provided α is chosen in the given range. To certify portions of
the ranking vector, we desire k to be as small as possible to find places in the vector where the
(k)
(k)
necessary gap |bi − bj | exists. Obtaining a tight bound on A2 is key to bounding k ; we
present and compare two methods of bounding A2 .
5

	

Numerical Approximations to Katz
EishaCentrality
Nathan et al. / Procedia Computer Science 108C (2017) 68–78

Nathan et al.

The Gershgorin
Circle Theorem [15] bounds the eigenvalues of the symmetric matrix A.

Let Ti = j=i |aij |, the sum of the nondiagonal entries in row i. Then D(aii , Ti ) is the closed
interval centered at aii with radius Ti and every eigenvalue λ ∈ σ(A) must lie within at least
one interval D(aii , Ti ), where σ(A) is the spectrum of A. Since the diagonal entries aii of A are
0, the discs are all centered around the origin and ∀i, Ti = di = the degree of vertex i. We then
have A2 = max λi < max Ti = dmax , where dmax is the largest degree in the graph. While
this provides a basis for an upper bound of the matrix 2-norm of A, many real-world graphs
such as social networks have a scale-free distribution and thus contain vertices with a very
large degree. Therefore, this is often a non-optimal bound. By using just a few matrix-vector
multiplications applied to random vectors, one can compute tighter bounds with high certainty.
We next examine probabilistic matrix norm bounds [8] and consider replacing the true bound
σup with an estimate of a bound with some probability. These bounds are developed using
the polynomials p, q implicitly formed as a part of the Lanczos bidiagonalization process with
starting vector v1 , which is chosen randomly with unit norm. The defining relations of Lanczos
bidiagonalization are stated as αm u(m) = Av(m) − βm−1 u(m−1) and βm v(m+1) = AT u(m) −
αm v(m) for β0 = 0, u0 = 0. The recurrent polynomials are: γj+1 pj (t) = qj (t) − βj pj−1 (t) and
T

T

βj+1 qj+1 (t) = tpj (t) − γj+1 qj (t) where γj = u(j) Av(j) and βj = u(j) Av(j+1) for p−1 (t) = 0
and q0 (t) = 1. The bound, stated in Theorem 2, is due to [8]. The result is an upper bound
σup (θ) for A2 with probability 1-θ, where θ is the user-chosen probability of bound failure.
 1 x−1
1
(1 − t)y−1 dt.
Define δ = θ · 21 B( n−1
2 , 2 ) where B is Euler’s Beta function, B(x, y) = 0 t
Theorem 2. [8] Suppose we have carried out k steps of the Lanczos bidiagonalization process
with starting vector v1 , and let θ ∈ (0, 1). Then the largest zero of the polynomials,
f1 (t) = qk (t2 ) − 1/δ, f2 (t) = tpk (t2 ) − 1/δ
with δ given above, is an upper bound σup (θ) for A2 with probability at least 1-θ.

As a result of thorough experimentation, for all bounds used in this paper, we select values
of θ=0.01 and k=10. For k=10, in order to calculate σup (0.01) we are required to calculate the
largest root of a tenth order polynomial. Since this does not change regardless of problem size
n, this calculation is asymptotically a fixed cost. We use Python’s Sympy package to calculate
the roots of these polynomials.
The deterministic Gershgorin bounds yield large values of A2 , rendering these bounds
useless. On average, these bounds return estimates of A2 that are 30.9× greater than the
true 2-norm. In contrast, the probabilistic bounds presented in Theorem 2 return estimates of
A2 that are only on average 1.07× greater than the true 2-norm, meaning that these are able
to be used for practical purposes.

3

Results

In this section we present comparisons to existing methods for identifying the top ranked vertices
with respect to performance and experiments validating our bound with respect to precision. We
are interested in determining if our method correctly identifies the set of top vertices and if so,
how much faster we are able to certify this set. The common method of iterating to machine
precision does not theoretically certify this set but our theory can be used on the machine
precision solution as well. We conduct experiments on 39 graphs from the KONECT [10] and
SNAP [11] datasets, including social networks, autonomous systems, citation, co-authorship,
web, co-purchasing, and road graphs.
6

73

74	

Numerical Approximations to Katz
EishaCentrality
Nathan et al. / Procedia Computer Science 108C (2017) 68–78

Nathan et al.

Table 1: Graphs used in experiments. Columns are numbers of vertices (|V |), number of edges (|E|), number of
iterations using new stopping criterion (IA ), and number of iterations using old stopping criterion (IE ).
Graph
blogs
USAirports
UCIrvineMessages
figeys-protein
OpenFlights
reactome
as20000102
p2p-Gnutella06
p2p-Gnutella05
oregon1 010331
p2p-Gnutella04
oregon2 010331
ca-AstroPh
cit-cora
p2p-Gnutella25
ca-CondMat
as-caida20071105
cit-HepPh
p2p-Gnutella30
email-Enron
slashdot-threads
p2p-Gnutella31
soc-Slashdot0902
recordlabel
libimseti
web-Stanford
dblp
web-NotreDame
com-amazon
cit-citseer
soc-twitter
stack-overflow
country
web-Google
youtube
as-skitter
flickr-links
roadNet-CA
livejournal

3.1

|V |
1,224
1,574
1,899
2,239
2,939
6,327
6,474
8,717
8,846
10,670
10,876
10,900
18,771
21,201
22,687
23,133
26,475
34,546
36,682
36,692
50,835
62,586
82,168
168,268
220,970
281,903
317,080
325,729
334,863
384,413
465,017
545,196
590,112
875,713
1,134,890
1,696,415
1,715,255
1,965,206
7,489,073

|E|
19,025
28,236
59,835
6,452
30,501
147,547
13,233
31,525
31,839
22,002
39,994
31,180
198,050
91,500
54,705
186,936
106,762
421,578
88,328
367,662
140,778
147,892
948,464
233,286
17,359,346
2,312,497
1,049,866
1,497,134
925,872
1,751,463
834,797
1,301,942
637,134
5,105,039
2,987,624
11,095,298
15,550,782
2,766,607
112,307,385

IA
14
22
338
8
30
70
6
22
15
6
12
8
159
13
7
57
13
56
13
42
9
10
10
7
67
22
108
16
10
10
37
11
8
18
9
27
48
1,332
28

IE
41
62
784
32
81
144
23
47
46
24
40
27
348
43
36
120
41
141
52
102
32
37
28
24
189
44
257
56
31
38
97
33
25
41
28
63
106
2,069
65

New Stopping Criterion

We first analyze the effect of our stopping criterion on reducing the number of iterations taken
by an iterative solver to identify the top R vertices in a network. Denote the number of
iterations taken by conjugate gradient to machine precision as IE and the number of iterations
used with our new stopping criterion as IA . Table 1 shows basic information about each graph
used in the experiments as well as raw iteration counts using both the existing and our new
stopping criterion. Figure 1a plots a histogram of the reduction in iterations IIEA for R = 100
and φ0 = 0.95. In all cases we obtain a reduction of at least 2×, an average of 3.07× reduction,
and up to a maximum reduction of 5.14× the number of iterations using our method. This
shows that we are able to identify the top R = 100 in a fraction of the time using our stopping
criterion compared to running until machine precision, while providing a theoretical guarantee
that these vertices are in the top of the ranking vector. This is especially significant because
7

	

Eisha Nathan et al. / Procedia Computer Science 108C (2017) 68–78
Numerical Approximations to Katz Centrality

75
Nathan et al.

(a) Histogram of the reduction in iterations from com-

(b) Correlation between α and residual obtained after
puting ranking vector to machine precision versus usterminating at stopping criterion. Larger values of s
ing new stopping criterion.
yield larger values of α and indicate harder problems.
Figure 1: Performance results using new stopping criterion for R = 100.

running to machine precision can sometimes take hundreds or thousands of iterations as seen
in Table 1. Next we investigate on what problems our method proves to be the most useful.
1
, the problem becomes more ill-conditioned. Since α ∈ (0, 1/A2 ), we
We know as α → A
2
apply our stopping criterion to the different graphs for various α in this range. Figure 1b plots
the relationship between α and the residual norm obtained when the solver terminates using
1
our criterion. We use α = s σup (0.01)
, substituting the bound σup (0.01) obtained in Section 2 for
A2 , for s ∈ (0, 1). For each value of s, the averaged residual norm is plotted across graphs.
When running to machine precision, the residual norm upon termination is typically rk ≈ 10−15 .
The average residual norm across all trials using our stopping criterion is 2.89 × 10−4 . We see
that we never have to iterate until machine precision using our new stopping criterion if we are
interested in only the top vertices in a graph. Regression analysis of these results (plotted as
the green line in Figure 1b) shows a strong linear correlation with a slope of 3.21 and mean
sum of squares of 0.83. The linear relationship suggests that we need less accurate approximate
1
solutions for harder problems as α → A
to obtain the top vertices in the graph. Typically the
2
harder problems tend to take thousands of iterations to converge with the standard stopping
criterion of iterating until a residual norm of 10−15 , but with our stopping criterion we can
converge faster at a lower tolerance to solve the desired data mining task. The low residual
norm suggests we are able to certify the top R correctly with low fidelity solutions and we
are able to use this technique to turn harder linear algebra problems into easier data mining
problems.

3.2

Accuracy of Approximation

Using the theory presented in Section 2, we show that we are able to accurately identify sets
R
. Intuitively, we calculate
of highly ranked vertices. Recall the precision φ is given by φ = j−1
the ratio of the number of returned vertices that are relevant and in the desired top to the
total number returned. A value close to 1 indicates we have obtained the top R vertices with
very few false positives. We evaluate the final precision φ for various values of R ∈ [10, 10000].
For example, φ for R = 100 represents the precision we obtain using our theory to return the
smallest set containing the top 100 ranked vertices in the graph. We find the first index j > 100
(k)
(k)
where |bR − bj | > 2k . Figure 2a plots a scatterplot of φ values versus various values of R.
For clarity, the y-axis starts at 0.8. We evaluate the precision on the 39 real-world networks
and on each network test on values of R ranging from 10 to 10000, dependent on network size.
Each data point is averaged over all the datasets for that particular value of R. In the majority
of cases we obtain a precision very close to 1, with an average of 0.98. This means we are able
to accurately certify large portions of the ranking vector and with our theory, do so with few
8

76	

Eisha Nathan et al. / Procedia Computer Science 108C (2017) 68–78
Numerical Approximations to Katz Centrality

Nathan et al.

(a) Precision values on all graphs for various R.

(b) Histogram of P values for different networks.
Figure 2: Qualitative results using new stopping criterion.

false positives. Note that we examine the number of top vertices in graphs instead of a top
percentage. From a data analysis standpoint, an analyst is more likely to be concerned with
the top 100 vertices, for example, across multiple graphs rather than the top 5% of vertices
(which will result in vastly different numbers of vertices dependent on graph size and in some
cases may be impossible to parse given available human resources).

3.3

Perfect Ordering of Top

We have shown that we are successfully able to efficiently identify sets of top ranked vertices in
networks for various set sizes. Experimentation shows that the theory is sound across several
real-world networks. While the previous experiments are only concerned with returning the
top set of vertices, here we impose the additional constraint of perfect ordering of this set. We
not only want the most highly ranked vertices, but we also want them in the correct ordering
as given by the exact solution. Recall the example of a web-Google graph given in Section
1. In this use case, it is important to ensure the ordering of these results is correct. We are
able to apply the theory from Theorem 1 in this application and provide a guarantee on how
many vertices we can accurately certify are in the correct ordering in the top of the ranking
vector compared to the exact solution. In this case, we look at the gaps between successive
vertices i and i+1 to ensure each pairwise comparison of vertices has the necessary gap to prove
the correctness of the relative ordering. Running a solver to machine precision to identify top
sets in networks cannot in fact provide any theoretical guarantee of how many vertices in the
approximation are in the correct ordering compared to the exact solution. In this experiment,
(k)
(k)
we are interested in finding P such that P = argmaxi |bi − bi+1 | > 2k , where P is the number
of vertices in the top of the vector in the correct order compared to the exact solution. We
traverse the sorted ranking vector b(k) after 10 iterations of conjugate gradient to find the first
place where the gap of 2k is not satisfied. When this occurs, we know that the previous vertices
are in the correct ordering since each pair-wise comparison of previous vertices satisfied the gap.
Figure 2b plots the distribution of P values for the different networks, with values of 0
omitted. Note the x-axis is on a log-scale. In most cases, we are able to accurately certify at
least hundreds of vertices, with an average across all datasets of P = 903. For cases where
we are only able to guarantee 1 or 0 vertices, we offer a possible explanation. If there are
vertices with the same ranking at the top of the exact solution, our theory will not be able to
go beyond this point because the necessary gap does not exist. Regardless, from a data analysis
standpoint, the numbers of vertices able to be accurately certified in the exact order in the top
validate our theory being used in this use case. For example, in a web-Google graph, a user will
only be concerned with the top 75-100 results, meaning that relative ordering of these results is
9

	

Numerical Approximations to Katz
EishaCentrality
Nathan et al. / Procedia Computer Science 108C (2017) 68–78

Nathan et al.

very important. Therefore our ability to accurately certify hundreds of vertices in the correct
order is very applicable.

4

Conclusions

This work relates the two research areas of numerical accuracy of solvers and network analysis
by understanding how the error in a solver affects the data analysis problem of ranking. By
treating the problem of ranking vertices in a graph as understanding numerical accuracy in
a linear solver, we present how the error in the numerical problem affects the solution to the
original data analysis problem of ranking. Our aim in this work was to provide theoretical
guarantees to bound the error in an approximate solution from an iterative method to the
exact Katz Centrality scores of vertices in a network. Using this theory, we are able to identify
the most central vertices with high confidence without accurately computing the centrality
scores for every vertex and therefore reduce computation time.
The result of our analysis is a reduction in the number of iterations taken to solve the data
analysis problem of ranking in graphs while maintaining a high precision rate in identifying
top vertices. We demonstrate this on several real-world networks using conjugate gradient
as the iterative method, giving high confidence that the important portion of the ranking is
correct. We present experiments validating the theory as a stopping criterion that can be
used in conjunction with any iterative solver, leading to significant algorithmic improvements.
When using the theory to identify top ranked vertices we are able to do so with very few false
positives. Finally, we also show perfect recall of the top vertices with respect to the exact
solution is possible with our theory. The results from this paper can also be applied to any
linear solver based ranking. Identifying top ranked vertices by Katz is one example in practice
presented in this work, but the theory is generalizable to other linear algebra based ranking
metrics. This paper draws the following quantitative conclusions:
• Our stopping criteria leads to an average of 3.07× reduction in iterations with a maximum
of 5.14× reduction across several graphs when identifying the top 100 vertices.
• The average residual norm when we terminate at our stopping criterion is 10−4 , which
provides a practical tolerance for iterative solvers compared to that of machine precision.
• Regression analysis of the performance of our method as a function of α shows that our
method tends to reduce cost more on harder problems.
• We obtain a 0.98 precision when identifying the top R vertices (for R ∈ [10, 10000]) in a
graph which shows that our method returns high quality results.
• Applying the theory to several real datasets shows we are able to guarantee that large
portions of the ranking vector are in the correct ordering compared to the exact solution.
Experiments show that we are able to answer the data analysis question of identifying the top
ranked vertices in the graph before needing to converge to machine precision to answer the
numerical problem. Future work will study the impact of these guarantees in a personalized
setting, specifically studying Katz scores from a specific seed set of vertices, and will extend
our theory for directed graphs.

5

Acknowledgments

Eisha Nathan is in part supported by the National Physical Science Consortium Graduate
Fellowship. This work is in part performed under the auspices of the U.S. Department of Energy
by Lawrence Livermore National Laboratory under Contract DE-AC52-07NA27344 with release
number LLNL-CONF-728271.
10

77

78	

Numerical Approximations to Katz
EishaCentrality
Nathan et al. / Procedia Computer Science 108C (2017) 68–78

Nathan et al.

References
[1] Réka Albert, Hawoong Jeong, and Albert-László Barabási. Internet: Diameter of the world-wide
web. Nature, 401(6749):130–131, 1999.
[2] Michele Benzi, Ernesto Estrada, and Christine Klymko. Ranking hubs and authorities using matrix
functions. Linear Algebra and its Applications, 438(5):2447–2474, 2013.
[3] Francesco Bonchi, Pooya Esfandiar, David F Gleich, Chen Greif, and Laks VS Lakshmanan.
Fast matrix computations for pairwise and columnwise commute times and katz scores. Internet
Mathematics, 8(1-2):73–112, 2012.
[4] Ulrik Brandes and Christian Pich. Centrality estimation in large networks. International Journal
of Bifurcation and Chaos, 17(07):2303–2318, 2007.
[5] Kurt C Foster, Stephen Q Muth, John J Potterat, and Richard B Rothenberg. A faster katz status
score algorithm. Computational & Mathematical Organization Theory, 7(4):275–285, 2001.
[6] KA Hawick and HA James. Node importance ranking and scaling properties of some complex
road networks. 2007.
[7] Nicholas J Higham. Functions of matrices: theory and computation. Siam, 2008.
[8] Michiel E Hochstenbach. Probabilistic upper bounds for the matrix two-norm. Journal of Scientific
Computing, 57(3):464–476, 2013.
[9] Leo Katz. A new status index derived from sociometric analysis. Psychometrika, 18(1):39–43,
1953.
[10] Jérôme Kunegis. Konect: the koblenz network collection. In Proceedings of the 22nd International
Conference on World Wide Web, pages 1343–1350. ACM, 2013.
[11] Jure Leskovec and Andrej Krevl. SNAP Datasets: Stanford large network dataset collection, June
2014.
[12] David Liben-Nowell and Jon Kleinberg. The link-prediction problem for social networks. Journal
of the American society for information science and technology, 58(7):1019–1031, 2007.
[13] Lawrence Page, Sergey Brin, Rajeev Motwani, and Terry Winograd. The pagerank citation ranking: bringing order to the web. 1999.
[14] Yousef Saad. Iterative methods for sparse linear systems. Siam, 2003.
[15] RS Varga. Gershgorin and his circles in springer series in computational mathematics, 36, 2004.
[16] Dirk Werner. Funktionalanalysis. Springer, 2006.

11

