New Bounds in Secret-Key Agreement: The Gap
between Formation and Secrecy Extraction
Renato Renner1 and Stefan Wolf2
1

Department of Computer Science, ETH Zurich, Switzerland.
renner@inf.ethz.ch
2
DIRO, Universit´e de Montr´eal, Canada.
wolf@iro.umontreal.ca

Abstract. Perfectly secret message transmission can be realized with
only partially secret and weakly correlated information shared by
the parties as soon as this information allows for the extraction of
information-theoretically secret bits. The best known upper bound
on the rate S at which such key bits can be generated has been the
intrinsic information of the distribution modeling the parties’, including
the adversary’s, knowledge. Based on a new property of the secret-key
rate S, we introduce a conditional mutual information measure which
is a stronger upper bound on S. Having thus seen that the intrinsic
information of a distribution P is not always suitable for determining
the number of secret bits extractable from P , we prove a diﬀerent
signiﬁcance of it in the same context: It is a lower bound on the
number of key bits required to generate P by public communication.
Taken together, these two results imply that sometimes, (a possibly
arbitrarily large fraction of) the correlation contained in distributed information cannot be extracted in the form of secret keys by any protocol.
Keywords. Information-theoretic security, secret-key agreement, reductions among primitives, information measures, quantum entanglement
puriﬁcation.

1
1.1

Information-Theoretic Secret-Key Agreement from
Distributed Information
From Partially Secret Weak Correlations to Perfect Secrecy

The unarguably strongest type of security in cryptography is based on information theory (rather than computational complexity theory) and withstands
attacks even by an adversary whose capacities (of both computation and memory) are unlimited. According to a famous theorem by Shannon [11], such a
high level of security can be achieved only by parties sharing an unconditionally
secret key initially; a result by Maurer [6] states that such a key can, even interactively, not be generated from scratch, i.e., without any prior correlation between
the legitimate partners Alice and Bob that is not completely under control of or
accessible to the adversary (or environment) Eve. The meaning of the previous
E. Biham (Ed.): EUROCRYPT 2003, LNCS 2656, pp. 562–577, 2003.
c International Association for Cryptologic Research 2003

New Bounds in Secret-Key Agreement

563

sentence might not be entirely clear; in fact, determining its exact interpretation
is one of this paper’s main concerns.
In [6] and [9], the pessimistic statement that information-theoretic security
is possible only when given information-theoretic security to start with was relativized by showing that the required prior correlation and privacy can both be
arbitrarily weak, still allowing for the generation of an unconditionally secret key
(and hence unbreakably secure message transmission [13]). This was shown both
in a very general but purely classical scenario, where Alice and Bob share classical information, as well as in a setting where they start with shared quantum
states and try to generate a “quantum key” (the privacy of which is guaranteed
by the laws of physics).
1.2

State of the Art: Facts and Conjectures

In the classical scenario, the starting point is a probability distribution PXY Z ,
where X, Y , and Z represent Alice’s, Bob’s, and Eve’s pieces of information,
respectively. The intrinsic information between X and Y given Z (the mutual
information shared by Alice and Bob as seen from Eve’s best choice of all possible points of view), has been shown to be an upper bound to the secret-key
rate (a quantiﬁcation of Alice and Bob’s ability of generating a secret key by
starting from independent repetitions of the random experiment characterized
by PXY Z ). No better bound on this rate has been found so far, and it has even
been conjectured that the two quantities might always be equal [8].
The parallels between the classical and quantum scenarios of informationtheoretic secret-key agreement pointed out in [3], [2], however, suggest that a
well-known phenomenon on the quantum side, called bound entanglement [4], [5],
has a classical counterpart. Bound entanglement is a kind of correlation between
Alice and Bob inaccessible to Eve but nevertheless of no use for generating a
secret (quantum) key. Unfortunately, the existence of such bound information,
which would contradict the mentioned conjecture on the classical side of the
picture, could not be proven so far. We make a signiﬁcant step towards such
a proof in the present paper by showing that the gap between the intrinsic
information and the secret-key rate can in fact be arbitrarily large.
1.3

Contributions and Outline of This Paper

The contributions of this paper are the following. Based on a new property of
the secret-key rate (Section 3.1), a new conditional information measure, the
reduced intrinsic information, is proposed and shown to be a stronger upper
bound on the secret-key rate (Sections 3.2 and 3.3). The analysis of a particular
class of distributions shows that this new quantity, and hence also the secret-key
rate, can be smaller than the intrinsic information by an arbitrarily large factor;
this is used to prove the existence of the phenomenon of bound information
asymptotically (Section 3.4).
Because of this gap between intrinsic information and the secret-key rate,
one might think that intrinsic information was simply not the right measure to

564

R. Renner and S. Wolf

be used in this context. This is certainly true in a way, but untrue in another: We
show (in Section 4) that the same intrinsic-information measure is a lower bound
on what we will call “information of formation,” namely the amount of secret-key
bits required to generate a certain distribution by public communication (which
is the inversion of the process of extracting key bits from the distribution). In
Section 4.3, we sketch how to generalize this to a “calculus of secret correlations.”
In the light of Section 4, the results of Section 3 imply that there exist
distributions requiring asymptotically more secret-key bits to be generated than
can be extracted from them. The somewhat counter-intuitive existence of such
a gap has its counterpart on the quantum side: Two important measures for
quantum entanglement [10], namely entanglement of formation on one side and
distillability on the other, can diﬀer by an arbitrary factor (where the former is
always at least as big as the latter).

2

Measuring the Extractable Secret Correlation of a
Distribution

2.1

The Secret-Key Rate

The model of information-theoretic key-agreement by common (classical) information has been introduced by Maurer [6] as an interactive generalization of
earlier settings by Wyner [14] and Csisz´
ar and K¨
orner [1]. Alice, Bob, and Eve
are assumed to have access to many independent outcomes of X, Y , and Z,
respectively, where the random experiment is characterized by a discrete probability distribution PXY Z . The amount of extractable secret correlation of such
a distribution was quantiﬁed by the maximal length of a highly secret key that
can be generated by Alice and Bob using public but authenticated two-way
communication.
Deﬁnition 1. [6], [7] Let PXY Z be the joint distribution of three discrete random variables X, Y , and Z. The secret-key rate S(X; Y ||Z) is the largest
real number R such that for all ε > 0 there exists N0 ∈ N such that for
all N ≥ N0 there exists a protocol between Alice (knowing N realizations
X N := (X1 , . . . , XN ) of X) and Bob (knowing Y N ) satisfying the following
conditions: Alice and Bob compute, at the end of the protocol, random variables
SA and SB , respectively, with range S such that there exists a random variable
S with the same range and1
H(S) = log |S| ≥ RN ,
Prob [SA = SB = S] > 1 − ε ,
I(S; CZ N ) < ε .

(1)
(2)
(3)

Here, C stands for the entire protocol communication.
1

H and I stand for the usual Shannon entropy and mutual information measures,
respectively. All logarithms in the paper are binary.

New Bounds in Secret-Key Agreement

565

Similar models have been deﬁned for the case where Alice and Bob start with
a shared quantum state instead of classical information and try to generate, by
classical communication, unitary operations on their local quantum systems, and
local measurements, quantum keys, i.e., maximally entangled quantum states.
This paper is concerned with the classical model, but some of its results are
inspired by the parallels between the two settings pointed out in [3]. Although
we will come back to the quantum setting later in the paper in order to illustrate
a further correspondence between the models, it can be fully understood without
any knowledge in quantum physics or quantum information processing.
2.2

Measuring S: Information Bounds and Bound Information

The secret-key rate as a measure of the amount of extractable secret correlation in a distribution has been studied extensively in the literature. The objective is to express S(X; Y ||Z) = S(PXY Z ), which is deﬁned asymptotically
and by a maximization over the set of all possible protocols, in terms of simpler, non-asymptotically deﬁned properties of the distribution PXY Z such as the
information-theoretic quantities I(X; Y ) or I(X; Y |Z).
The following lower bound is a consequence of a random-coding argument
by Csisz´ar and K¨
orner [1] and states that an initial advantage can be used for
extracting a secret key.
Theorem 1. [1], [6], [7] For any distribution PXY Z , we have
S(X; Y ||Z) ≥ max {I(X; Y ) − I(X; Z) , I(Y ; X) − I(Y ; Z)} .

(4)

Inequality (4) is not tight: S(X; Y ||Z) can be positive even when the right-hand
side of (4) is negative [6], [9].
The best upper bound on S(X; Y ||Z) known so far is the intrinsic information
I(X; Y ↓ Z) between X and Y , given Z, which is, intuitively, the remaining
mutual information between Alice and Bob after Eve’s choice of her best possible
viewpoint.
Deﬁnition 2. [8] Let PXY Z be a discrete probability distribution. Then the
intrinsic conditional mutual information between X and Y given Z is
I(X; Y ↓Z) :=

inf

XY →Z→Z

I(X; Y |Z) .

The inﬁmum in Deﬁnition 2 is taken over all Z such that XY → Z → Z
is a Markov chain, i.e., over all channels PZ|Z that can be chosen by Eve for
processing her data. (A recent unpublished result states that if |Z| is ﬁnite, then
the inﬁmum is a minimum, taken by a channel PZ|Z which does not extend the
alphabet, i.e., |Z| ≤ |Z|.)
Theorem 2. [8] For any distribution PXY Z , we have
S(X; Y ||Z) ≤ I(X; Y ↓Z) .

(5)

566

R. Renner and S. Wolf

The question whether the bound (5) is tight or not has been open. Motivated
by the phenomenon of bound (non-distillable) quantum entanglement, distributions for which S = 0 holds although I(X; Y ↓ Z) is positive are said to have
bound information, but have not been shown to exist. In fact, it has not even
been known whether there exist distributions for which equality does not hold
in (5).
Deﬁnition 3. [3] Let PXY Z be such that I(X; Y ↓Z) > 0 but S(X; Y ||Z) = 0
hold. Then PXY Z is said to have bound information.
Let (PXY Z )n = PX(n) Y(n) Z(n) be a sequence of distributions such that
I(X(n) ; Y(n) ↓ Z(n) ) ≥ c > 0
for all n, but
S(X(n) ; Y(n) ||Z(n) ) → 0 if n → ∞ .
Then, the sequence (PXY Z )n is said to have asymptotic bound information.

3

A New Bound on the Secret-Key Rate

3.1

The Limited Eﬀect of Adversarial Side Information: A Bit
Harms Just a Bit

The secret-key rate S(X; Y ||Z) is one particular measure for quantifying the
amount of conditional correlation in a distribution. Another, simpler, such measure is the conditional mutual information I(X; Y |Z). As a preparation for the
rest of this paper, we show that the secret-key rate has an important property in
common with the conditional information, namely that additional side information accessible to the adversary cannot reduce the quantity in question by more
than the entropy of this side information. For the conditional information, this
property reads as follows (where X, Y , Z, and U are arbitrary discrete random
variables):
I(X; Y |ZU ) ≥ I(X; Y |Z) − H(U ) .2

(6)

It is less surprising that this property holds for the secret-key rate than that it
does not hold for the intrinsic information, as we will see in Section 3.2.
2

A stronger version of this inequality is
|I(X; Y |ZU ) − I(X; Y |Z)| ≤ H(U )
and implies immediately Shannon’s above-mentioned theorem if we interpret X as
the message, Y as the ciphertext, and U as the key (Z being trivial):
| I(X; Y |U ) − I(X; Y ) | ≤ H(U ) .
=H(X)

=0

New Bounds in Secret-Key Agreement

567

Theorem 3. Let X, Y , Z, and U be arbitrary discrete random variables. Then
S(X; Y ||ZU ) ≥ S(X; Y ||Z) − H(U ) .
Proof. Let R < S(X; Y ||Z). Then there exists, for all ε > 0 and suﬃciently
large N , a protocol (with communication C) such that Alice and Bob (knowing
X N and Y N ) end up with SA and SB , respectively, and such that there exists
S satisfying (1), (2), and (3). Let E denote the event SA = S ∨ SB = S, let E
be its complement and χE its characteristic random variable, satisfying χE = E
if E occurs and χE = E otherwise. We then have3
H(SA ) ≥ H(SA |χE )
≥ PχE (E) · H(SA |χE = E)
≥ (1 − ε) · H∞ (SA |χE = E)
≥ (1 − ε)(− log(1/|S|(1 − ε)))
= (1 − ε)(log |S| + log(1 − ε)) ,
H(SA |SB ) ≤ H(SA |SB χE ) + H(χE )
≤ PχE (E) log |S| + h(ε)
≤ ε log |S| + h(ε) ,
I(SA ; CZ N U N ) = I(SA ; CZ N ) + I(SA ; U N |CZ N )
≤H(U N )

≤ I(S; CZ N ) + H(SA |S) + N · H(U )
≤ ε + ε log |S| + h(ε) + N · H(U ) ,
and, because of (1),
I(SA ; SB )

−

I(SA ; CZ N U N ) ≥ (R − H(U ) − Θ(ε))N

H(SA )−H(SA |SB )

holds for suﬃciently large N . According to the bound (4), secret-key agreement
with respect to the new random variables SA , SB , and [CZ N U N ] is possible
at a rate S(SA ; SB ||CZ N U N ) ≥ (R − H(U ) − Θ(ε))N . Since N realizations of
(X, Y, [ZU ]) are required to achieve one realization of (SA , SB , [CZ N U N ]), we
have
S(X; Y ||ZU ) ≥ S(SA ; SB || CZ N U N )/N ≥ R − H(U ) − Θ(ε) .
This concludes the proof because ε > 0 and R < S(X; Y ||Z) were arbitrary. ✷

3

Here, h(p) := −(p log p+(1−p) log(1−p)) is the binary entropy function, and H∞ (X)
is the min-entropy of a distribution PX , deﬁned as H∞ (X) := − log maxx∈X PX (x).
We make use of the fact that for all X, H∞ (X) ≤ H(X) holds.

568

3.2

R. Renner and S. Wolf

The Intrinsic-Information Bound Is Not Tight

Let us now show the more surprising fact that the intrinsic information does
not have the property studied in the previous section. Clearly, an immediate
consequence will be that equality cannot always hold in (5) since two measures
with diﬀerent properties cannot be identical.
Let PXY ZU be the following distribution with the ranges X = Y = {0, 1, 2, 3}
and Z = U = {0, 1}.
PXY ZU (0, 0, 0, 0) = PXY ZU (1, 1, 0, 0) = PXY ZU (0, 1, 1, 0) = PXY ZU (1, 0, 1, 0) = 1/8 ,
PXY ZU (2, 2, 0, 1) = PXY ZU (3, 3, 1, 1) = 1/4 .

All the other probabilities are 0. We represent this distribution graphically. (Note
that Z and U are uniquely determined by X and Y .)
X 0 1 2 3
Y
1/8 1/8 0 0
0
1
1/8 1/8 0 0
2
0 0 1/4 0
3
0 0 0 1/4
Z ≡ X + Y (mod 2) if X, Y ∈ {0, 1}
Z ≡ X (mod 2) if X ∈ {2, 3}
U = X/2 .
The intuition behind the “weirdness” of this distribution is as follows. Given
that Eve does not know the bit U , she has the additional disadvantage (besides
not knowing this bit, which can hence directly be used as a secret-key bit by
Alice and Bob) not to know how to process the bit Z (i.e., whether to forget
it or not) in order to minimize I(X; Y |Z). This explains both why the intrinsic
information reduces by more than one bit when Eve learns U , as well as why
the secret-key rate is only 1: Eve has the potential to destroy all the correlation
besides U (if only she knew U . . . ).
Lemma 1. For the given distribution, we have
I(X; Y ↓Z) = 3/2 , I(X; Y ↓ZU ) = 0 , and H(U ) = 1 .
Proof. We show ﬁrst I(X; Y ↓Z) = 3/2. Let PZ|Z be an arbitrary discrete binaryinput channel, let z ∈ Z, and let p := PZ|Z (z, 0), q := PZ|Z (z, 1). Then the
distribution PXY |Z=z is given by the following table.
Y
0
1
2
3

X

0

1

p
q
4(p+q) 4(p+q)
q
p
4(p+q) 4(p+q)

0
0

0
0

2

3

0
0

0
0
0

p
2(p+q)

0

q
2(p+q)

New Bounds in Secret-Key Agreement

569

We have
I(X; Y |Z = z) = 1 +

1
2

1−h

p
p+q

+

1
h
2

p
p+q

= 3/2 .

Thus, I(X; Y |Z) =
z PZ (z) · I(X; Y |Z = z) = 3/2 . Since the channel was
arbitrary, it follows that I(X; Y ↓Z) = 3/2.
We now show that I(X; Y ↓ZU ) = 0. Consider the following channel PZU |ZU :
PZU |ZU (∆|(0, 0)) = PZU |ZU (∆|(1, 0)) = PZU |ZU (0|(0, 1)) = PZU |ZU (1|(1, 1)) = 1 .

It is easy to check that I(X; Y |ZU ) = 0 holds, hence I(X; Y ↓ZU ) = 0.

✷

From Lemma 1, combined with Theorem 3, we can immediately conclude
that the given distribution satisﬁes
I(X; Y ↓Z) = 3/2 ,
but
S(X; Y ||Z) ≤ S(X; Y ||ZU ) + H(U ) ≤ I(X; Y ↓ZU ) + 1 ≤ 1 .

(7)

In fact, equality holds in (7), i.e.,
S(X; Y ||Z) = 1 ,
since the bit called U is a secret-key bit known only to Alice and Bob. We
emphasize that this is the ﬁrst example of a distribution PXY Z for which it is
known that S(X; Y ||Z) = I(X; Y ↓Z) holds.
Corollary 1. For the given distribution we have
1 = S(X; Y ||Z) < I(X; Y ↓Z) = 3/2 .
3.3

A New Information Measure which Is a Stronger Bound on S

A generalization of this reasoning immediately leads to a new conditional information measure which bounds the secret-key rate from above.
Deﬁnition 4. Let PXY Z be a discrete distribution. Then the reduced intrinsic
information of X and Y given Z, denoted by I(X; Y ↓↓Z), is deﬁned by
I(X; Y ↓↓Z) :=

inf (I(X; Y ↓ZU ) + H(U )) .

PU |XY Z

The inﬁmum is taken over all conditional probability distributions PU |XY Z .
Lemma 2. The reduced intrinsic information has the following properties. Let
X, Y , Z, and U be arbitrary random variables.
1. I(X; Y ↓↓Z) ≤ I(X; Y ↓Z) ,
2. I(X; Y ↓↓ZU ) ≥ I(X; Y ↓↓Z) − H(U ) .

570

R. Renner and S. Wolf

Proof.
1. Choose U with H(U ) = 0 in Deﬁnition 4.
2. Let PXY ZU be a ﬁxed distribution. Then
I(X; Y ↓↓Z) =
≤
≤

inf (I(X; Y ↓ZV ) + H(V ))

PV |XY Z

inf

PU

|XY (Z,U )

PU

|XY (Z,U )

inf

(I(X; Y ↓ZU U ) + H(U U ))

(8)

(I(X; Y ↓ZU U ) + H(U ) + H(U |U ))

≤ I(X; Y ↓↓ZU ) + H(U ) .
In the second step of (8), the inﬁmum is restricted to random variables
V = [U U ] containing U .
✷
The most important property of I(X; Y ↓↓ Z), however, is that it is an upper bound on the secret-key rate (that is strictly stronger than I(X; Y ↓ Z)).
Corollary 2 follows directly from Theorem 3, Corollary 1, and Lemma 2.
Corollary 2. For every distribution PXY Z , we have
S(X; Y ||Z) ≤ I(X; Y ↓↓Z) ≤ I(X; Y ↓Z) ;
for some distributions PXY Z , we have
I(X; Y ↓↓Z) < I(X; Y ↓Z) .

3.4

The Gap Can Be Arbitrarily Large: Asymptotic Bound
Information

We have seen in the previous section that the intrinsic information is not always
a tight upper bound on the secret-key rate. In Section 4 we will show, however,
that I(X; Y ↓Z) nevertheless remains an interesting and meaningful measure of
correlated secrecy since it indicates the amount of perfectly secret bits required
to synthesize a certain distribution by public communication.
Before this, we show that the gap between the secret-key rate (or the reduced
intrinsic information) and the intrinsic information can be arbitrarily large; this
will imply the existence of asymptotic bound information.
Let (PXY Z )n be the following distribution (where X = Y = {0, 1, . . . , 2n−1}
and Z = {0, 1, . . . , n − 1}).

New Bounds in Secret-Key Agreement

Y
0
..
.

X

0 · · · n − 1 n n + 1 · · · 2n − 1
1
2n2

···

..
.

n−1
n
n+1
..
.

571

···
0 ···
0 ···
..
.

1
2n2

2n − 1 0 · · ·

1
2n2

0
..
.

0
..
.

···

..
.

0
..
.

1
2n2

0

0
0

···
···

0
0
0
..
.

0
0
..
.

1
2n

0
..
.

1
2n

0

0

0

..

.
···

1
2n

Z ≡ X + Y (mod n) if X, Y ∈ {0, . . . , n − 1}
Z ≡ X (mod n) if X ∈ {n, . . . , 2n − 1} .
Lemma 3. For the distribution (PXY Z )n , we have
1
I(X(n) ; Y(n) ↓ Z(n) ) = 1 + log n
2
and
I(X(n) ; Y(n) ↓↓ Z(n) ) = 1 .
The proof of Lemma 3 is very similar to the proof of Lemma 1.
Proof. The index n is omitted. Let ﬁrst PZ|Z be an arbitrary discrete n-ary-input
channel, let z ∈ Z, and let pi := PZ|Z (z, i) for i = 0, . . . , n − 1. Then
1
log n − H
2
1
= 1 + log n .
2

I(X; Y |Z = z) = 1 +

p0
pn−1
,... ,
pi
pi

+

1
2

H

p0
pn−1
,... ,
pi
pi

Hence I(X; Y |Z) = 1 + (log n)/2, and, since the channel was chosen arbitrarily,
I(X; Y ↓Z) = 1 + (log n)/2.
On the other hand, consider the bit
U := X/n

satisfying H(U ) = 1 .

Then I(X; Y ↓ ZU ) = 0 (the corresponding channel PZU |ZU is an immediate
generalization of the channel from the proof of Lemma 1), and
Y ↓ZU ) + H(U ) = 1
hold. In fact, equality holds in (9) since S(X(n) ; Y(n) ||Z(n) ) ≥ H(U ) = 1.

(9)
✷

Let now (QXY Z )n be the following sequence of distributions derived from
(PXY Z )n (where all the alphabets are extended by the “erasure symbol” ∆):
(QXY Z )n (x, y, z) =

1
log n (PXY Z )n (x, y, z)
1 − log1 n

if x = ∆ , y = ∆ , z = ∆ ,
if x = y = z = ∆ .

572

R. Renner and S. Wolf

For this new distribution, we have
I(X(n) ; Y(n)↓Z(n) ) =

1
1
· 1 + log n
log n
2

>

1
2

but
S(X(n) ; Y(n) ||Z(n) ) =

1
→0.
log n

In other words, the intrinsic information can be larger than some positive constant c = 1/2 whereas the secret-key rate is, at the same time, arbitrarily close
to 0.
Theorem 4. There exist sequences of distributions with asymptotic bound information.

4

Building a Distribution from Secret-Key Bits, and
Reductions Among Correlations

4.1

Information of Formation . . .

In Section 3 we have shown that the intrinsic information measure is not a
tight bound on the rate at which secret-key bits can be extracted from repeated
realizations of a random experiment PXY Z . Despite this fact, and although we
have even derived a better (generally smaller) upper bound, intrinsic information
nevertheless remains an important quantity for measuring the secret correlation
in PXY Z . We will show that it is a lower bound on the number of secret-key bits
required to generate, using public communication, random variables distributed
according to PXY Z . This means that certain distributions require asymptotically
strictly more secret bits to be formed than can be extracted from them.
We ﬁrst deﬁne the information of formation which can be seen as the classical
analog to entanglement of formation. It is, roughly speaking, the rate at which
secret bits are required to synthesize a distribution which is, in terms of the
provided privacy, at least as good as PXY Z from Alice and Bob’s point of view.
Deﬁnition 5. Let PXY Z be the joint distribution of three discrete random variables X, Y , and Z. The information of formation of X and Y , given Z, denoted
by Iform (X; Y |Z), is the inﬁmum of all numbers R ≥ 0 with the property that
for all ε > 0 there exists N0 such that for all N ≥ N0 , there exists a protocol
between Alice and Bob with communication C and achieving the following: Alice
and Bob, both knowing the same random RN -bit string S, can ﬁnally compute X and Y , respectively, such that there exist random variables X N , Y N ,
and Z N jointly distributed according to (PXY Z )N (this is the distribution corresponding to n-fold independent repetition of the random experiment PXY Z )
and a channel PC|Z N such that
Prob [(X , Y , C) = (X N , Y N , C)] ≥ 1 − ε
holds.

New Bounds in Secret-Key Agreement

573

Intuitively, the conditions of Deﬁnition 5 imply that with high probability, Alice’s and Bob’s random variables X and Y are distributed according to
N
PXY
and the entire communication C can be simulated by an Eve knowing the
corresponding Z N . This latter condition formalizes the fact that the protocol
communication C observed by Eve does not give her more information than
ZN .
4.2

...

Is Bounded from below by Intrinsic Information

Theorem 5. For every distribution PXY Z , we have
Iform (X; Y |Z) ≥ I(X; Y ↓Z) .
Remark. Note that I(X; Y ↓ Z) is a stronger bound on Iform (X; Y |Z) than
I(X; Y ↓↓ Z), and that both I(X; Y ) and I(X; Y |Z) are not lower bounds on
Iform (X; Y |Z). This can be seen with the examples where X is a random bit
and X = Y = Z holds (I(X; Y ) = 1 but Iform (X; Y |Z) = 0), and where X
and Y are independent random bits and Z = X ⊕ Y (I(X; Y |Z) = 1 but
Iform (X; Y |Z) = 0).
Proof of Theorem 5. The crucial observation here is that local data processing
as well as sending messages cannot increase the intrinsic information of Alice’s
and Bob’s pieces of information given what Eve knows.
We have, for arbitrary random variables A, B, E, and Ci (where Ci stands
for the i-th message sent and A, B, and E for Alice’s, Bob’s, and Eve’s complete
knowledge before Ci is sent), that
I(ACi ; BCi ↓ ECi ) ≤ I(A; B ↓ E) .
This can be seen as follows. Let us assume without loss of generality that Ci is
sent by Alice, i.e., I(BE; Ci |A) = 0 holds. Then
I(ACi ; BCi ↓ ECi ) ≤ inf I(ACi ; BCi | ECi )
PE|E

= inf I(A; B | ECi )
PE|E

= inf (H(B; ECi ) − H(B; AECi ))
PE|E

≤ inf (H(B; E) − H(B; AE))
PE|E

= inf I(A; B | E)
PE|E

= I(A; B ↓ E) .
In the fourth step, we have made use of the fact that the message Ci is generated
by Alice.

574

R. Renner and S. Wolf

Let now R > Iform (X; Y |Z) and ε > 0, and let a protocol as in Deﬁnition 5
be given. At the beginning of the protocol, which uses RN secret-key bits to
start with, the intrinsic information equals RN . Hence we get
RN ≥ I(X ; Y ↓ C) .
Let now E be the event that (X , Y , C) = (X N , Y N , C) holds, E its complementary event, and χE its characteristic random variable. Then we have, by
deﬁnition, PχE (E) = Prob [E] ≤ ε, and we can conclude4
RN ≥ I(X ; Y ↓ C)
˜
= inf I(X ; Y |C)
PC|C
˜

˜ E ) − h(ε))
≥ inf (I(X ; Y |Cχ
PC|C
˜

˜ E) − h(ε))
≥ inf (PχE (E) · I(X ; Y |C,
PC|C
˜

˜ E) − h(ε))
= inf (PχE (E) · I(X N ; Y N |C,
PC|C
˜

˜ E ) − Pχ (E) · I(X N ; Y N |C,
˜ E) − h(ε))
= inf (I(X N ; Y N |Cχ
E
PC|C
˜

˜ E ) − ε · min(log |X N |, log |Y N |) − h(ε))
≥ inf (I(X N ; Y N |Cχ
PC|C
˜

˜ − 2h(ε) − ε · N · min(log |X |, log |Y|)
≥ inf I(X N ; Y N |C)
PC|C
˜

= I(X N ; Y N ↓ C) − 2h(ε) − ε · N · min(log |X |, log |Y|)
≥ I(X N ; Y N ↓ Z N ) − 2h(ε) − ε · N · min(log |X |, log |Y|)
= N · I(X; Y ↓ Z) − 2h(ε) − ε · N · min(log |X |, log |Y|) .
In the second last step we have used the fact that C can be generated by sending
Z N over a channel PC|Z N (which is true by deﬁnition).
Since ε > 0 and R > Iform (X; Y |Z) were arbitrary, this concludes the proof.
✷
For every distribution PXY Z we now have
S(X; Y ||Z) ≤ I(X; Y ↓↓Z) ≤ I(X; Y ↓Z) ≤ Iform (X; Y |Z) ,

(10)

and for some, equality does not hold in the second inequality of (10), hence
S(X; Y ||Z) = Iform (X; Y |Z) .
4

For simplicity, the proof is only given for the case where at least one of the ranges
X or Y is ﬁnite. The proof for the general case is somewhat more involved and
uses a separation of the ranges into a ﬁnite and an inﬁnite part such that the random variables X and Y , restricted to the inﬁnite part, have only negligible mutual
information.

New Bounds in Secret-Key Agreement

575

These distributions have the property to require asymptotically more secret bits
to be generated than can be maximally extracted from them. The rest of the
correlation seems to be used up, or trapped in X, Y , and Z forever.
4.3

Generating a Secret Correlation from Another: In General,
There Are Losses

The concepts of key agreement and formation of a distribution studied above
can be seen as special cases of a general secret-correlations calculus: Given the
outcomes of (many independent realizations) of a random experiment PXY Z , is
it possible to construct instances of another distribution PX Y Z (or a distribution less favorable for Eve, i.e., where Eve has less information in the sense of
Deﬁnition 5), and if yes, at which rate
Rsec (PX

Y Z

← PXY Z ) ?

By similar arguments as used in the proof of Theorem 5, one obtains the bound
Rsec (PX

Y Z

← PXY Z ) ≤

I(X; Y ↓Z)
.
I(X ; Y ↓Z )

(11)

(Note, as above, that the same bound does not hold if the intrinsic information
is replaced by, for instance, the usual conditional mutual information.)
Theorems 2 and 5 are now special cases of inequality (11): If PBB∆ denotes
the distribution of a secret bit (PBB∆ (0, 0, δ) = PBB∆ (1, 1, δ) = 1/2), then we
have
Rsec (PBB∆ ← PXY Z ) = S(X; Y ||Z)
and, since I(B; B↓ ∆) = 1, inequality (11) is nothing else but Theorem 2. On
the other hand, we have
Rsec (PXY Z ← PBB∆ ) =

1
,
Iform (X; Y |Z)

and inequality (11) turns into Theorem 5.
The results of Section 3 imply that equality does generally not hold in (11).
Even worse, for every ε > 0 there exist distributions PXY Z and PX Y Z with
Rsec (PX

Y Z

← PXY Z ) · Rsec (PXY Z ← PX

Y Z

)<ε.

Like currency exchange, “exchanging secret correlations” is (even asymptotically) not loss-free.

5

Concluding Remarks and Open Questions

We have shown that some partially-secret correlations PXY Z are wasteful with
secret bits in the sense that some of the key bits that would be required to

576

R. Renner and S. Wolf

construct the correlation cannot be extracted. More speciﬁcally, for some distributions even an arbitrarily large fraction of them are lost.
A similar phenomenon is well-known in the scenario of quantum key agreement. Let ρ be a mixed quantum state between Alice and Bob. The entanglement
of formation (the classical analog of which is deﬁned in Section 4) is the rate at
which EPR pairs (maximally entangled quantum bits) are required asymptotically in order to construct (a state close to) ρ by classical communication and
local quantum operations. The distillability (its classical analog is the secret-key
rate) on the other hand is the number of EPR pairs (or states very close to
EPR pairs) that can (asymptotically) be distilled from ρ by local quantum operations and classical communication. (The adversary does not have to be taken
into account here since EPR pairs are pure states and hence not entangled with
the environment.) The entanglement of formation and the distillability are two
measures for entanglement which can diﬀer by an arbitrarily large factor.
Let us conclude with two questions. First, is S(X; Y ||Z) = I(X; Y ↓↓Z) possible? Secondly, is Iform (X; Y |Z) > 0 but S(X; Y ||Z) = 0 possible? We believe
that the answer to the second question is yes. Some examples of distributions
that might have this property were presented in [3]; they are the classical translations of bound entangled quantum states. Another candidate, which (at ﬁrst
sight) is not related to bound entanglement, arises when the distribution of Section 3.2 is modiﬁed as follows.
Y
0
1
2
3

X

0

1

2

3

1/8 1/8 a a
1/8 1/8 a a
a a 1/4 0
a a 0 1/4

Z ≡ X + Y (mod 2) if X, Y ∈ {0, 1} ,
Z ≡ X (mod 2) if X, Y ∈ {2, 3} ,
Z = (X, Y ) otherwise.
(The distribution must be re-normalized.) We have I(X; Y ↓Z) > 0 for any a ≥ 0,
but√
it seems that S(X; Y ||Z) vanishes if a is chosen large enough (probably a ≥
1/4 2, in which case the correlation useful for key agreement seems destroyed).
An indication that this is indeed so is the fact that the “translation” of this
distribution to a quantum state turned out to be a bound entangled state (one
that has not been known previously) [12].
Acknowledgments. The distribution analyzed in Section 3.2 was proposed by
Juraj Skripsky. The authors thank Nicolas Gisin and Ueli Maurer for interesting
and helpful discussions on the subject of this paper. The ﬁrst author was supported by the Swiss National Science Foundation (SNF), and the second author
was supported by Canada’s NSERC.

New Bounds in Secret-Key Agreement

577

References
1. I. Csisz´
ar and J. K¨
orner, Broadcast channels with conﬁdential messages, IEEE
Transactions on Information Theory, Vol. IT-24, pp. 339–348, 1978.
2. N. Gisin, R. Renner, and S. Wolf, Linking classical and quantum key agreement:
is there a classical analog to bound entanglement?, Algorithmica, Vol. 34, pp. 389–
412, 2002.
3. N. Gisin and S. Wolf, Linking classical and quantum key agreement: is there “bound
information”?, Proceedings of CRYPTO 2000, Lecture Notes in Computer Science,
Vol. 1880, pp. 482–500, Springer-Verlag, 2000.
4. M. Horodecki, P. Horodecki, and R. Horodecki, Mixed-state entanglement and
distillation: is there a “bound” entanglement in nature?, Phys. Rev. Lett., Vol. 80,
pp. 5239–5242, 1998.
5. P. Horodecki, Separability criterion and inseparable mixed states with positive
partial transposition, Phys. Lett. A, Vol. 232, p. 333, 1997.
6. U. Maurer, Secret key agreement by public discussion from common information,
IEEE Transactions on Information Theory, Vol. 39, No. 3, pp. 733–742, 1993.
7. U. Maurer and S. Wolf, Information-theoretic key agreement: from weak to strong
secrecy for free, Proceedings of EUROCRYPT 2000, Lecture Notes in Computer
Science, Vol. 1807, pp. 352–368, Springer-Verlag, 2000.
8. U. Maurer and S. Wolf, Unconditionally secure key agreement and the intrinsic
conditional information, IEEE Transactions on Information Theory, Vol. 45, No. 2,
pp. 499–514, 1999.
9. U. Maurer and S. Wolf, Towards characterizing when information-theoretic secret key agreement is possible, Proceedings of ASIACRYPT ’96, Lecture Notes in
Computer Science, Vol. 1163, pp. 196–209, Springer-Verlag, 1996.
10. S. Popescu and D. Rohrlich, Thermodynamics and the measure of entanglement,
quant-ph/9610044, 1996.
11. C. E. Shannon, Communication theory of secrecy systems, Bell System Technical
Journal, Vol. 28, pp. 656–715, 1949.
12. F. Spedalieri, personal communication, 2003.
13. G. S. Vernam, Cipher printing telegraph systems for secret wire and radio telegraphic communications, Journal of the American Institute for Electrical Engineers, Vol. 55, pp. 109–115, 1926.
14. A. D. Wyner, The wire-tap channel, Bell System Technical Journal, Vol. 54, No. 8,
pp. 1355–1387, 1975.

