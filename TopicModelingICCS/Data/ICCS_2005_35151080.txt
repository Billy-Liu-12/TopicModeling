User Experiences with Nuclear Physics
Calculations on a H2O Metacomputing System
and on the BEgrid
P. Hellinckx, K. Vanmechelen, G. Stuer, F. Arickx, and J. Broeckhove
Department of Mathematics and Computer Science,
University of Antwerp, BE-2020 Antwerp, Belgium
peter.hellinckx@ua.ac.be

Abstract. We report on user experiences gathered with quantum nuclear scattering calculations on a H2O based metacomputing system
and on the BEgrid, the Belgian EGEE Grid infrastructure. We compare quantifiable aspects such as Grid utilization but also ease-of-use
and the experience encountered in porting the application to each of the
Grid environments.

1

Introduction

While Grids are becoming a fact of life in High Performance Computing (HPC)
it is clear that the term stands for a whole range of middleware systems with
strongly varying characteristics. Although traditional Grid systems often host an
extensive stack of Grid services, many applications do not necessarily require explicit coordination and centralized services for authentication, registration, and
resource brokering. For these applications, a lightweight model in which individuals and organizations share their superﬂuous resources on a peer-to-peer basis,
seems to be more suitable [1].
In order to test this claim, we have implemented a quantum physics, nuclear
3-cluster scattering problem using both a lightweight and a heavyweight Grid.
We chose this test case, of which the details will be described later, for several
reasons. First, the computation has both a CPU-intensive and a data-intensive
part. Secondly, the computation is too extensive to solve on a single machine, yet
it does not require the full blown Grid infrastructure. Thirdly, this application ﬁts
an algorithmic approach that is rather common in many scientiﬁc computations.
This contribution presents the results of this comparative study. Our choice
for a lightweight Grid system is H2O [2], as we have considerable experience
using this middleware. Our choice for a heavyweight Grid is the BEgrid which
hosts the Belgian part of the EGEE project. As a member of the BEgrid consortium, our research group has experience in its procedures and maintenance,
Research supported in part by the Ministerie van de Vlaamse Gemeenschap, Afdeling
Technologie en Innovatie as part of the Vlaamse pilot-gridinfrastructuur project.
V.S. Sunderam et al. (Eds.): ICCS 2005, LNCS 3515, pp. 1080–1088, 2005.
c Springer-Verlag Berlin Heidelberg 2005

User Experiences with Nuclear Physics Calculations on a H2O

1081

as well as in gridifying applications. In what follows, the design, experimental results and a performance comparison between both platforms will be
discussed.

2

Problem Statement

To obtain physical properties of quantum systems, such as atoms, nuclei or
molecules, one needs to solve the so-called Schr¨odinger equation. In order to
solve it, proper boundary conditions must be chosen. The solutions, the so-called
“wave-functions” then allow for the calculation of physical quantities. The equation and its boundary conditions are usually too complex to be solved for manybody systems (e.g. a nucleus), and approximations have to be introduced. One
approach is to expand the wave-function on a discrete, inﬁnite-dimensional, set
of basis states. Substitution of this approximation in the equation and boundary
conditions, leads to a much simpler matrix equation in the expansion coeﬃcients
to be solved.
The matrix formulation can be further simpliﬁed by choosing expansion bases
with speciﬁc properties. The Modiﬁed J-Matrix (MJM) model [3] is such an
approach, and has been applied to 3-cluster nuclear systems [4]. We consider it
here to obtain scattering results for a 3-particle conﬁguration of a triton and
2 neutrons of 5 H. The calculations essentially consist of two steps: (1) a CPUintensive calculation of the matrix elements in the matrix equation, and (2) the
solution of the matrix equation. Step 2 can be obtained in reasonable time on a
single node, and we will therefore only consider the gridiﬁcation of step 1.
The oscillator expansion basis for the solution of a 3-cluster problem, is enumerated by a set of indices. These are the hypermoment K, describing the three
cluster geometry; relative angular momenta l1 and l2 between the three clusters
coupled to the total angular momentum L, a constant of motion; the oscillator
index n. The number of (l1 l2 ) combinations depends on both K and L. The
essential matrix to be determined is the energy matrix, denoted by
ˆ Kj , (l1 l2 )j L, nj = Ki , li , ni H
ˆ Kj , lj , nj
Ki , (l1 l2 )i L, ni H

(1)

ˆ is the Hamiltonian, or energy, operator, and i and j distinguish basis
where H
states; the right hand side in (1) simpliﬁes the notation, omitting L as an overall
constant, and replacing the combination (l1 l2 )i by li .
The theory to obtain (1) [4] is well beyond the scope of this paper, but it can
be broken down to
ˆ Kj , l j
Ki , l i H

R(Ki , li , lr , t)

=
t

lr

ls

ˆ Kj , l s
Ki , l r H

t

R(Kj , ls , lj , t)
(2)

1082

P. Hellinckx et al.

where
stands for a matrix over all ni , nj indices. The R factors are socalled Raynal-Revai coeﬃcients as discussed in [4], and the nature and range of
index t depends on the nucleus (5 H) and its cluster decomposition (t + n + n).
The granularity of the problem is clear from (2), and reduces the problem to
ˆ Kj , ls
a fork and join algorithm by calculating all independent
Ki , lr H
t

matrices for ﬁxed Ki , Kj and all allowed combinations lr , ls , t (the fork), followed
by a summation to obtain (2) (the join). In this paper we discuss a calculation
for L = 0 and K = 0, 2, 4, .., 16, a range of l1 = l2 = 0, 1, .., K/2 values, and 45 t
values. All of the computational code components are implemented in Fortran90
using the Intel v7 compiler.
A farmer-worker distribution algorithmic model seems an evident choice for
this problem, because all of the fork tasks are independent of one another. The
tasks are ﬁle based, meaning that they get their input from a series of ﬁles and
write their results into one. All input ﬁles except one, a conﬁguration ﬁle which
contains the particular indices for the current computation, have a constant
content for all tasks.

3

H2O

H2O [2] is developed by Emory University and is a component-based, serviceoriented framework for distributed metacomputing. H2O is designed to cater
for lightweight, general purpose, loosely coupled networks where resources are
shared on a P2P basis. Adopting a provider-centric view of resource sharing,
it provides a lightweight, general-purpose, Java based, conﬁgurable platform.
H2O adopts the microkernel design philosophy: resource owners host a software
backplane (called a kernel) onto which owners, clients, or third-party resellers
may load components (called pluglets) that deliver value added services. It is
this last aspect that makes H2O unique: by separating the service provider from
the resource owner, it becomes possible for any authorized third party to deploy
new services in a running kernel.
3.1

Software Architecture

Since H2O basically stands for an empty service container, no infrastructural
Grid components exist. In order to keep the H2O approach as lightweight as
possible, we implemented the infrastructural components which are absolutely
necessary: a lookup service (LUS), a ﬁle transfer pluglet (FTPl), a broker, and
executors. Figure 1 shows the basic architecture of our framework.
Before the calculation can start, the infrastructure has to be set up. This
comes down to starting the LUS, and initiating the H2O kernel on all participating nodes. These kernels will register themselves with the LUS and deploy
the FTPl. From this point on, the system is ready to start the calculation.

User Experiences with Nuclear Physics Calculations on a H2O

1083

LUS

User
start h2O kernel

Kernel

register

FTPl

register

Broker

retrieve kernels

WorkerThread
*[for each kernel]

stage constant resources

deploy executor

Executor

get next config while(configs available)
stage variable resources

execute binary
retrieve result

Fig. 1. Architecture of the framework

The ﬁrst step in distributing the problem involves initializing the broker
component which is responsible for the overall progress of the calculation. It will
locate all available kernels using the LUS and instantiate W orkerT hreads for
all of them. Each one is responsible for the communication with one particular
kernel. They start by staging the constant-content input ﬁles and the Fortran
binary using the FTPl, followed by deploying the executor pluglet. Next, as long
as there are sets of indices left to calculate, they will generate conﬁguration ﬁles,
transfer them to the kernel, start the executor, and retrieve the result. Finally,
when all required combinations have been calculated, this process stops and the
results can be recombined.

4

BEgrid

The BEgrid project involves an infrastructural collaboration between several
universities and research institutions in Belgium. It is currently running LCG
2.2 middleware [5] and includes sites from the universities of Antwerp, Ghent,
Leuven and Brussels as well as other research oriented institutions such as the
Flanders Marine Institute. More institutions are expected to join on a short term

1084

P. Hellinckx et al.

basis. BEgrid is part of the North European ROC region of the EGEE project.
Many sites are currently in deployment phase, 205 CPUs will be operational in
the beginning of 2005.

4.1

Software Architecture

The LCG middleware comprises of a number of functional modules that cooperate to provide workload, data and information management services. A resource broker matches job requirements, which are expressed in JDL (Job Description Language), to the available resources. The user has the ability to state
complex infrastructural and operational job requirements using a standardized
information schema called GLUE. The broker also supports user-deﬁned ranking
of resources that fulﬁll the hard job requirements, based on soft QoS requirements such as the average queued time for a job at a compute resource. After
a computing resource is selected by the broker, the job is forwarded to the
computing element (CE) associated with that resource. The computing element
virtualizes compute resources such as PBS, LSF or Condor farms, providing
uniform access to heterogeneous compute back-ends.
In the same way as the CE provides a virtualization layer for compute resources, a storage element (SE) delivers such a layer for heterogeneous storage
resources. A replica management service (RMS) supports ﬁle based data replication and orchestrated access to ﬁles stored across SEs. Although jobs may
interact with the RMS to access large data ﬁles, small I/O requirements can be
fulﬁlled by specifying input and output sandboxes. These allow the user to indicate which ﬁles should be staged in and out before and after the computation.
All interactions with the Grid infrastructure take place from a user interface
node which supports APIs, command line, and graphical interfaces to BEgrid.
The LCG middleware supports a batch oriented computing model, and is
currently in process of providing better support for data dependencies between
jobs and more tightly coupled computations. This does not aﬀect the mapping
of our problem to the LCG based computing infrastructure as jobs are trivially
parallel. A characteristic that does aﬀect our job distribution scheme is the fact
that long delays have been observed when submitting jobs and receiving their
results on a zero loaded computing resource. The computation time for individual
indices also varies widely from a few seconds to a few hours. Therefore it was
deemed necessary to group multiple calculations in a single job, in order to
mitigate the performance penalty induced by these delays. To determine the
size of these groupings we monitored the number of queued jobs over all CE
queues during the computation and adjusted the group size accordingly. The
idea is that as long as all CE queues are suﬃciently loaded to keep the compute
resources busy, the submission delay will not aﬀect the total wall-clock time of
the computation.
All ﬁle staging was performed using the input and output sandboxes since
the calculations are compute intensive. After jobs ﬁnish, a polling component on
the user interface fetches the job output.

User Experiences with Nuclear Physics Calculations on a H2O

5
5.1

1085

Experimental Results
Speedup

The distributed computations on the H2O framework were performed on a local
cluster containing a heterogeneous set of 20 PC-nodes with an Intel processor
ranging from 1.7 to 2.66 GHz, part of a 100Mb LAN. For the BEgrid tests, we
used 24 3.2 GHz Pentium 4 nodes at Antwerp and 8 dual-Opteron/244 nodes (1.8
GHz) at Leuven. This amounts to 33 normalized 3.2 CPUs which is the maximum
reachable speedup factor on this setup. Both sites use 1 GigE interconnect. The
user interface and resource broker were both located at Antwerp.
Table 1 shows the turn-around time of the distributed computations for different values of K, and compares these to the pure sequential execution timing.
The speedup factor which denotes the ratio between sequential and distributed
execution time is also shown for each K. The table depicts low speedup ﬁgures
for computations with K < 10 both for H2O and BEgrid. More time is spent
deploying the jobs than processing them. The reason is that most of the tasks
with low K have a very small execution time. From K = 10 onwards, the picture
changes, as the mean execution time for these jobs rises signiﬁcantly. The results
are almost optimal for K > 12 on H2O. For H2O, the low speedup for small K
is a direct consequence of our naive implementation of the Grid infrastructure
components. Prefetching the data ﬁles will probably boost speedup for small K.
For BEgrid, low speedup values can be attributed to job submission delays
which are in the order of seconds, and result fetching delays which are in the order
of minutes. These make it diﬃcult to achieve optimal resource utilization in the
beginning and at the end of the computation. The delays were not found to be
caused by data transmission overhead but rather by the workload management
system itself.
Table 1. Speedup achieved on the available H2O and LCG resources
H2O
LCG
K sequential distributed speedup sequential distributed speedup
0
2s
55s
0.04
1s
7m57s
0.00
2
9s
1m26s
0.11
3s
9m00s
0.01
4
1m11s
2m24s
0.49
35s
16m27s
0.04
6
9m01s
4m00s
2.25
5m19s
16m59s
0.32
8
56m31s
8m25s
6.72
30m44s
18m54s
1.63
10 4h52m56s
20m22s
14.38 2h55m43s
18m42s
9.40
12 21h33m12s 1h12m52s
17.75 12h37m12s
49m36s
15.27
14 82h38m21s 4h11m24s
19.72 49h39m20s 2h01m05s
24.61
16 285h04m20s 14h23m59s
19.80 173h37m26s 6h07m31s
28.30

5.2

Utilization

Turn-around timing is not an acceptable measure if we wish to compare distributive platforms, such as the H2O and LCG middlewares, as the amount of

1086

P. Hellinckx et al.

available resources varies signiﬁcantly from one run to another. Therefore, we
consider the utilization rate of the participating nodes, which is deﬁned as the
percentage of time the processor is performing the actual computation. This
measure provides an indication of the overhead introduced by both the middleware and data transfers, and is largely independent of the hardware used. Figure
2 shows the utilization as a function of K, which indicates strongly decreasing
overhead for increasing K-values. For H2O, the main part of the overhead can
be attributed to data transfers. The remaining part, 0.03% to 0.07% of the total turn-around time, is middleware overhead. For BEgrid, job submission and
result fetching delays were the main causes for overhead.
100
H2O
LCG

Utilization (%)

80

60

40

20

0
0

2

4

6

8
K

10

12

14

16

Fig. 2. Processor Utilization Comparison between H2O and BEgrid

6

H2O vs. BEgrid

The main merits of H2O are that it is lightweight and easy to deploy. As it is Java
based, it functions on any desktop commodity PC available, limiting hardware
investments. Furthermore, job submission is very responsive, which allows for
the submission of small jobs, without creating too much overhead. Finally, its
component based architecture allows for on-demand deployment of custom made,
and highly optimized Grid components such as specialized schedulers, LUS, etc.
The latter advantage also turns out to be a shortcoming since every component has to be constructed from scratch. As the available documentation is
rather scarce, this results in an initially steep learning curve. In the current
implementation, we opted for a minimalistic approach which kept the infrastructural components simple, but unfortunately also unscalable and, in general,
not reusable. High quality, scalable, robust, and general purpose components
would require an important implementation investment. Finally, though H2O
may be written in Java, typical computational components are not. Thus special provisions have to be made to ensure that kernels hosted on Linux, Solaris,
Mac and Windows platforms can participate in the computation.

User Experiences with Nuclear Physics Calculations on a H2O

1087

One of the main merits of distributing the computation on BEgrid is the
eﬃciency of the implementation process itself: only 230 lines of bash scripting
code were needed to capture all job distribution and execution logic. Though the
highly parallel and independent nature of the distributed subtasks is certainly of
inﬂuence, this is also directly related to the fact that core Grid services for workload, data and information management as well as core security provisions, are
delivered ’out-of-the-box’. Another important advantage is the fact that coding
an application to a model and infrastructure accepted on a European scale, opens
the possibility for accessing a large scale infrastructure. On the other hand, one
should be aware that gaining access to a general purpose (inter)national Grid
resources still requires “social” interactions for the application to be accepted at
remote sites.
A potential shortcoming of BEgrid is the high job submission and result
fetching delay which may aﬀect applications that have to face a ﬁne grained level
of job distribution. Although our approach of wrapping multiple application level
jobs into a single Grid job mitigated these eﬀects, it was still diﬃcult to achieve
a good load balance at the beginning and near the end of the total computation.

7

Conclusions

The main conclusion of this work is that both Grid platforms are well suited for
gridifying a highly parallelizable CPU-intensive application.
In its default conﬁguration, H2O is best suited for distributive computations
on a limited scale. It is highly responsive, but only provides the base Javaoriented infrastructure to develop Grid applications. It does allow for a more
scalable approach, but requires all necessary Grid components for this to be
custom made. In order to overcome this drawback, a pluggable H2O based Grid
infrastructure is currently under development.
BEgrid provides a platform with standardized well-deﬁned services for distributive processing. This delimited set of services to approach the Grid is a
merit of the system, because of its straightforward usage, as well as a shortcoming because of its inherent limitations. With respect to computing resources,
it features international scalability, which is an important asset for large scale
distributed applications.

References
1. Jonathan Chin, Peter V. Coveney: Towards tractable toolkits for the Grid: a plea
for lightweight, usable middleware. http://www.realitygrid.org/lgpaper.html.
2. V. Sunderam, D. Kurzyniec: Lightweight Self-Organizing Frameworks for Metacomputing. In 11th Int. Symp. on High Performance Distributed Computing, 2002.

1088

P. Hellinckx et al.

3. J. Broeckhove, F. Arickx, W. Vanroose and V. Vasilevsky: The Modified J-Matrix
method for Short-Range Potentials. J. Phys. A: Math. Gen 37 (2004) 1-13
4. V. S. Vasilevsky, A. V. Nesterov, F. Arickx and J. Broeckhove: The Algebraic Model
for Scattering in Three–s–cluster Systems: Theoretical Background. Phys. Rev. C63
(2001) 034606:1–16
5. A. D. Delgado et al., LCG-2 User Guide. https://edms.cern.ch/file/454439/LCG2-Userguide.pdf.

