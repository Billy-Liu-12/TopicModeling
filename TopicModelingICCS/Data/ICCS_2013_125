Available online at www.sciencedirect.com

Procedia Computer Science 18 (2013) 1312 – 1321

International Conference on Computational Science, ICCS 2013

Automatic Tuning of Compiler Optimizations and Analysis of
their Impact
Dmitry Plotnikova , Dmitry Melnika,∗, Mamikon Vardanyana , Ruben Buchatskiya ,
Roman Zhuykova , Je-Hyung Leeb
a ISP
b Samsung

RAS, 25 Alexander Solzhenitsyn st., Moscow 109004, Russia
Electronics Co., Ltd., Next-Generation Computing Lab., Suwon, Korea

Abstract
Modern compilers can work on many platforms and implement a lot of optimizations, which are not always tuned well for
every target platform. In the paper we present the Tool for Automatic Compiler Tuning (TACT), which helps to identify such
underperforming compiler optimizations. Using GCC for ARM, we show how this tool can be used to improve performance of
several popular applications, and how the results can be further analyzed to ﬁnd places for improvement in the GCC compiler
itself.
c 2013 The Authors. Published
©
Published by
by Elsevier
ElsevierB.V.
B.V. Open access under CC BY-NC-ND license.
Selection and/or
peer-review
under
responsibility
organizers
of the
2013
International
Conference
Computational
and peer
review under
responsibility
of of
thethe
organizers
of the
2013
International
Conference
on on
Computational
Science
Science.
Keywords: Compiler Optimization; Automatic Performance Tuning; GCC

1. Introduction
The initial motivation [1] for developing the TACT tool was the task of improving the GCC compiler for
ARMv7 architecture. Given the complexity of GCC, its long development history, the number of supported
target platforms and optimizations, usually its performance can be improved by tuning the compiler optimization
parameters for the speciﬁc target platform. However, it isn’t obvious which compiler optimizations out of dozens
provided by the compiler (most of those are platform-independent) may beneﬁt from such tuning.
The traditional approach to compiler performance tuning implies choosing some test applications, ﬁnding
their hot spots, analyzing the generated assembly code, and then tracing portions of suboptimal code back to
optimizations that have generated it (or those that we believe should have worked, but didn’t), and then improving
those optimizations. This work requires much eﬀort and experience, but still some optimization opportunities
could be overlooked, because the code produced by an optimizing compiler is diﬃcult to analyze.
∗ Corresponding

author. Tel.: +7-495-912-0754
Email addresses: leitz@ispras.ru (Dmitry Plotnikov), dm@ispras.ru (Dmitry Melnik), mamikon@ispras.ru (Mamikon
Vardanyan), ruben@ispras.ru (Ruben Buchatskiy), zhroma@ispras.ru (Roman Zhuykov), jehyung.lee@samsung.com (Je-Hyung
Lee)

1877-0509 © 2013 The Authors. Published by Elsevier B.V. Open access under CC BY-NC-ND license.
Selection and peer review under responsibility of the organizers of the 2013 International Conference on Computational Science
doi:10.1016/j.procs.2013.05.298

Dmitry Plotnikov et al. / Procedia Computer Science 18 (2013) 1312 – 1321

It could be helpful to compare performance with another compiler, which could be tuned better for the target
platform, so to estimate how much the original compiler could be improved from the current level, and then
compare generated code and ﬁx the deﬁciencies found. In case of ARM, there are at least LLVM and ARMCC
compilers available for this architecture, but comparing the code generated by two optimizing compilers could
be even more complicated than analyzing the output code from one of them. Also, as we found later, GCC’s
performance is already substantially better than ARMCC’s1 , and also though LLVM may outperform GCC on
some tests, is still far from being a reference compiler for ARM.
This way, we came to the idea of using automatic tuning for improving the compiler: it can provide an
estimate of the achievable performance level for a given application and give some hints on which optimizations
can be improved. The information from the automatic tuning is especially valuable if it shows that the test case
improves from disabling an optimization that was enabled in the default compiler conﬁguration, or if it ﬁnds a new
value for an optimization’s parameter. As GCC includes in the -O2 optimization level only conservative enough
optimizations, the speedup coming from disabling such optimization could point to an architecture-speciﬁc issue
(or test-speciﬁc issue, but the former case is more valuable). The subsequent improvement of the cases found with
an automatic parameter tuning comes at much less eﬀort, because there is already a code path in the compiler
that yields better code, and, in this case, we just need to analyze why the default optimization parameters perform
worse, and what does it take to generate better code with the default optimization level. Also, ﬁnding suboptimal
pieces of code by comparing assembly dumps from runs of the same compiler which diﬀer only in a single
optimization is a much easier task than ﬁnding that code by manual analysis.
Initially, we started automatic performance tuning with ACOVEA [2]. This is a simple open-source tool that
uses a genetic algorithm to search the compiler optimization parameters space. It was included in Gentoo Linux
repository to provide its users with a tool for determining optimal compilation ﬂags for building their system. We
improved ACOVEA with support for cross-compilation, and it has helped us to ﬁnd [1] ﬁrst performance issues
in GCC for ARM. However, we came across few drawbacks that made this tool inconvenient to use for the task.
In order to use an automatic tuning tool eﬀectively to ﬁnd places for improvement in the compiler, we believe
that the following features are important. First, it should provide tools for tuning results analysis, so to determine what compiler options from the resulting long obscure parameter string contribute most to the performance
improvement found with tuning. Second, it needs support for error checking to ﬁlter out compiler options that
cause incorrect program behavior or build error. Third, it needs support for parallel compilation and execution on
several devices to speedup the tuning. Fourth, the tool should not assume anything about the compiler internals,
e.g. options that are included in default optimization levels, or should not require custom patches for the compiler,
because most compiler optimization work for open-source compilers such as GCC or LLVM happen on nightly
repository snapshots, which are changing fast.
To meet the above requirements, we developed the Tool for Automatic Compiler Tuning (TACT). In this paper,
we give an overview of its features and discuss how it can be used in work on compiler improvement, as well as
regular application tuning.
2. Related Work
A lot of work is dedicated to the automatic search for optimal compiler settings. In this section, we discuss
the available tools for automatic tuning, and give an overview of known approaches to the problem.
One of the well known approaches is iterative compilation. In works [3, 4, 5, 6, 7] the authors are considering
diﬀerent strategies for searching optimization space as well as diﬀerent optimization goals.
Bodin et al. [3] describe a method for solving the three numerical parameters optimization problem. The
algorithm measures the performance for points in optimization space located at equal intervals. The points lying
between the current minimum and average value of performance are added to a queue. Then these points are
sequentially removed from the queue and their neighborhood is again split into spaced intervals and similarly
1 On SPEC2000 INT, GCC is on average 5% better than ARMCC at -O2 in ARM mode, and 50% better (geomean) for GNU Go, CxImage
and C-Ray applications in Thumb-2 mode, though in the latter case ARMCC parameters were tuned with TACT to rule out the diﬀerence in
set of -O2 optimizations

1313

1314

Dmitry Plotnikov et al. / Procedia Computer Science 18 (2013) 1312 – 1321

investigated. The process is repeated until the speciﬁc number of points has been evaluated, and the point with
the minimum execution time is reported. In their work, the authors show that it is possible to ﬁnd a solution close
to optimal (within 0.3%), while considering less than 0.25% of the possible combinations of options and ﬁnding
the minimum after studying less than 1% of combinations. However, it appears diﬃcult to determine the size of
each step, as it depends on the program, on the size of the input data and on the target architecture. Reducing
the size of the step can help to achieve a better result, but it also greatly increases the run time; in addition, the
algorithm can get stuck in a local minimum. Also, it’s not clear how well this algorithm scales to the large number
of dimensions, if used to optimize compiler with hundreds of exported optimization parameters (in case of GCC,
it’s around 200, around half of them having numerical values).
In COLE [4], the authors investigated the standard levels of compiler optimization, and searched for Paretooptimal levels for performance and compile time. They show that using the genetic algorithm it’s possible to ﬁnd
the set of compiler options that are more Pareto-eﬀective (i.e. better performance for the same compile time or
vice versa) than the standard optimization levels, which once were set up manually and are not reviewed too often.
The authors used SPEC2000 INT, which is a popular benchmark suite for evaluating the compiler performance.
However, it doesn’t guarantee that the parameters selected for SPEC still will be optimal for other applications.
Bashkansky et al. [8] introduced interesting modiﬁcations to the genetic algorithm. They shaped population
size as a function of the generation number. In the paper they considered random number of entities, constant
number on each population, linear decrease, L-shaped and exponential decrease. Experimental evaluations have
shown that extending the random coverage in the beginning and focusing on convergence in the end improves the
cost/performance eﬃciency. Maximum performance gain was achieved by L-shaped and Exponential policies.
Pekhimenko et al. [9] focused on decreasing the compile time for the static commercial compiler TPO (Toronto
Portable Optimizer), while preserving the execution time. They extended the compiler with a class to collect static
program features such as total operations, ﬂoat operations, levels of nested loops and others. It is shown that
using logistic regression machine learning technique to predict set of transformations and values for their heuristic
parameters can decrease the average compile time by at least a factor of two with a negligible increase in the
execution time (1%).
Likely the most related work to ours is the MILEPOST project [10]. To our knowledge, only the MILEPOST
open-source project is currently freely available for public use (besides ACOVEA [2] that is no longer being
developed. It uses machine learning methods to ﬁnd set of the best compiler optimization parameters for a program
based on its features. A variety of static and dynamic parameters are evaluated for each function of the program,
which form their feature vectors. During the search of optimization space for a program, these vectors are saved
along with the corresponding set of optimizations and the measured result. Then, MILEPOST can provide the
best known set of optimizations for previously searched programs with similar features. Using MILEPOST GCC,
one can even control the internal order of optimization passes in the GCC compiler, which is otherwise not
controllable by user. Authors also suggest the idea of a collective program optimization, where anyone can share
their experiences with the community [11]. Such teamwork gives the ability to quickly get suﬃciently larger
amount of tuning data for various programs. However, MILEPOST doesn’t ﬁt too well into our requirements.
First, it’s focused on using machine learning techniques, which implies ﬁnding a solution based on the previous
knowledge. However, GCC evolves fast, and the data obtained for the previous compiler revision may quickly
become outdated, so we would have to run training phase very often. Second, the GCC plugin it uses is GCC
version speciﬁc, so updating compiler versions is complicated.
3. Overview of the Tool for Automatic Compiler Tuning
In the following subsections we describe the basic concepts of TACT and outline its main features.
3.1. Genetic Algorithm Core
The TACT evolutional search core supports multiple optimization objectives, so it can tune either for a single
optimization parameter, or for two selected parameters simultaneously, for example, for performance and code
size (or compile time).
The implemented method is based on SPEA2 [12], which provides an eﬀective way for ﬁnding Pareto-optimal
front of compile option sets. First, it reads the conﬁguration ﬁle with the compiler options to be tuned, and

Dmitry Plotnikov et al. / Procedia Computer Science 18 (2013) 1312 – 1321

Send flags for
evaluation

Host machine

Genetic Algorithm Core
Population 1
Population 2
...
Population N

Task Queue
Management
System
Get
results
of building and
running
Allocate pool
and build
application with
cross-compiler

Run Target devices
pool #k
Test Board 1
Get
Result
Test Board N

Access
compiled
binaries and
run
application

Build Pool Directories
(shared via NFS)
Pool 1: building
Pool 2:

Waiting for
test board

...
Pool N: running

Fig. 1. TACT Components and Operational Scheme

generates a pool of strings (”chromosomes”) consisting of random2 set of compiler options. This pool of string
represents one population, and there can be several of them. Then, each chromosome from every population
is evaluated: the target application is compiled with a given option string, it executes on a target test board,
and the measured performance is returned to TACT, along with application’s binary size. In case of a singleparameter optimization, the compile strings corresponding to the best-performing (or smallest size) conﬁgurations,
are added to the archive – a pool that stores predeﬁned number of best conﬁgurations among all generations. If
the archive is full, then the newly added conﬁgurations will push the inferior ones out of the archive. In case of
the multi-objective tuning, the archive holds the best Pareto-optimal set, and to limit the number of points in the
archive the clustering technique is used. Then, the compiler option conﬁgurations from the archive built on the
previous generation are used to produce conﬁgurations for the next generation: two ”parent” conﬁgurations are
chosen randomly from the archive (in case of a single optimization parameter the conﬁgurations with the better
parameter values have greater chance to be picked for crossbreeding), and the new option set is constructed by
choosing at random option values either from the ﬁrst or the second parent. At this stage, a limited number of
chromosomes can migrate from one population to another. Then, the mutation pass runs on all chromosomes,
which can randomly change a single option to the opposite value or shift a parameter value by a random number.
After that, the process is repeated from the evaluation step until the ﬁxed number of generations passes or a speciﬁc
condition on best achieved result is satisﬁed. Figures 2 and 3 show how the tuning converges over generations for
the multi- and the single-parameter cases respectively.
3.2. Operational Scheme
As TACT was designed primarily for automatic tuning on embedded systems running Linux, the system for
tuning typically consists of one multi-core x86 host for cross-compilation and one or more target devices (in our
case ARM CPU boards). There are two requirements: that all devices can access NFS mount from host, and
that SSH server is installed on targets (the latter is used for running applications). Bare-metal targets without
NFS/SSH could be also supported by extending the task manager with a routine to send binaries to and request
to run them using the available communications. Also, the task management module can reside on a separate
machine accessible from other compile hosts through SSH – this way available targets can be shared among
tuning sessions of several users.
The overview of TACT operation is shown on Fig 1. The genetic algorithm core handles all operations that
involve evolution of compiler options and interacts with other components only by sending requests for evaluation
of compile strings to the task queue management system. This component is responsible for building application,
for sending the request to run the application binary on a free target test board, and for synchronization of these
2 To reduce the tuning time, numerical parameter values (in case of GCC --param) can be initialized with random values normally distributed around their defaults values, or evenly distributed within parameter range to increase diversity in the ﬁrst population. This setting is
controlled by conﬁguration option.

1315

1316

Dmitry Plotnikov et al. / Procedia Computer Science 18 (2013) 1312 – 1321
700000

-Os
-O3
-O2
generation-30
generation-26
generation-21
generation-16
generation-11
generation-6
generation-1
final 6-point Pareto bound

650000

600000

Size (bytes)

550000

500000

450000

400000

350000

300000

5.5

6

6.5

7

7.5

8

8.5

9

9.5

10

Performance
(points)
Run time (seconds)

Fig. 2. Evolution of Pareto graphs for tuning x264 by performance and code size simultaneously

processes. The build of an application is performed in one of the build pool directories. A build pool directory is
shared across the host and the target devices via NFS and holds all the data that belongs to a single evaluation run:
the program build tree, the installation tree, the temporary run logs and the proﬁle data (if compiling with proﬁling
support). After the application is built in the allocated pool, the system waits for the available target device to run
application from the speciﬁed pool. If all devices are busy, the task waits until one becomes free, keeping the pool
locked. After a device frees up, the run order is accepted and upon completion the performance value is reported
back. The task queue manager frees the pool and sends the result back to genetic algorithm core.
3.3. Uniﬁed Structure for Application Deployment
TACT provides a benchmarking framework resembling that of SPEC CPU, which includes a directory structure for deploying application sources, shared libraries and resources, a common speciﬁcation for scripts that
build and run applications, verify correctness of run results, a common data format speciﬁcation for exchanging
run results and generating reports. In order to add a new application for tuning, the user just needs to copy the
directory structure from a template application, to deploy the application source, and to adjust conﬁguration ﬁles
and build/run scripts for the speciﬁc application.
3.4. Parallel Build and Execution
Parallel compilation and execution support greatly speeds up the tuning process. The parallelism is exploited
on two levels. First, we allow building application at the same time as the tool awaits for execution result of the
previously compiled application. Second, we allow using several test boards in the tuning process to run tuned
applications simultaneously on all of them.
Note that the test boards used for tuning should not be necessarily of the very same model or the same CPU
speed. This is because each test board is assigned its own population, so results are compared only with those
obtained on the same test board, so evolution branches progress independently. However, migrations between
populations are allowed so the best GCC ﬂag combinations are spread through other populations and continue
their competition with the ”native species” of those populations. This competition is fair since after migration the
compile string will be evaluated again on the new test board.
3.5. Error Handling
TACT handles the following types of errors: compile-time errors (internal compiler errors, incompatible ﬂag
combinations, etc.), runtime errors (segmentation faults due to miscompilations), execution timeout and output
hash mismatch. In all these cases the failing ﬂag combination is eliminated from further evolution. The output

Dmitry Plotnikov et al. / Procedia Computer Science 18 (2013) 1312 – 1321

hash can be calculated either by the target application itself (e.g. for the libevas benchmark we have added the
evaluation of the CRC32 checksum of the frame buﬀer image after each test), or calculated by the user script. The
timeout value for the application is speciﬁed in its conﬁguration ﬁle.
Interestingly, some miscompilations may cause program to execute faster (e.g. once we encountered a bug in
the loop optimization that incorrectly reduced the number of iterations), and as at the time TACT didn’t have output
hash veriﬁcation the tuning always converged to the ﬂag combination causing such ”proﬁtable” miscompilation.
3.6. Multiple-objective Tuning
TACT supports tuning for multiple optimization objectives simultaneously. In this case, on each generation it
evolves the Pareto-optimal set of conﬁgurations for the given criteria. Out of the box, it can tune simultaneously
for code size and performance, while other user-speciﬁed optimization criterion can be added with scripting.
Multiple-objective tuning not only allows to achieve ultimate performance or code size values, but allows
ﬁnding an acceptable tradeoﬀ between them. This tuning type allows for a ﬁxed threshold of one parameter to
ﬁnd the optimal value of the second one.
Fig 2 shows evolution of Pareto frontiers for tuning of the x264 application3 . Though the eﬀect of the GCC’s
-Os option (optimize for size) can’t be reproduced with a combination of parameters plus -O2, for the performance
or code size of other baselines (-O2, -O3) the tool was able to ﬁnd solutions with better value of the second
parameter. For example, for the performance level of -O3 it has found a solution in which code size is 1/3 less.
3.7. Support for Proﬁle-Guided Compilation
TACT can also tune applications with proﬁle-guided optimizations. The tool’s task queue manager allows
interleaving two execution stages (proﬁle collection and ﬁnal evaluation) of diﬀerent build pool directories to
minimize the idle time of the test boards.
4. Tuning Speed and Convergence
The number of runs required to get a suﬃcient speedup depends on the application itself, the compiler, the set
of ﬂags chosen for tuning, the evolution parameters (the mutation and migration rates, the number of generations
and populations, and the number of ﬂag conﬁgurations in one population).
Upper and lower charts on Fig 3, show the maximum and the average speedup for each generation, respectively. Vertical axis measures the performance gain compared to the base optimization level (-O2) in fractions of
the maximum speedup achieved during tuning. The value of 1.0 on both graphs corresponds to 16.62% performance improvement for C-Ray, 18.19% for libevas and 13.76% for x264, while zero corresponds to the baseline
performance (-O2). These charts do not include the best conﬁgurations found on previous generations that are
stored in the archive, nor runs failed due to a compilation error or a miscompile.
While tuning starts with the negative average performance gain (and may stay lower than the baseline for a
substantial number of iterations), still for all three applications the solution better than the baseline was found
already at the ﬁrst generation. The conﬁguration that gives at least 50% of the maximum performance gain (found
in 30 iterations) for x264 was achieved on the 1st generation, for libevas on the 2nd, and for C-Ray on the 5th.
The 80% level was achieved at 9th, 14th, and 10th generations respectively.
Though the convergence of the solution is not required to achieve substantial performance improvements, from
these graphs we can anticipate a little more improvement if we would continue tuning past 30 generations, since
best values kept improving in the last two generations.
Our tuning setup involved 200 GCC ﬂags, 80 ﬂag conﬁgurations in a population, 2 populations being run in
parallel each on its own testboard, and used 4 compilation pools. The mutation rate was set to 3%. Assuming that
the average application takes less than a minute for a single run, and that the compile host has enough parallelism
(and there are enough compile pools) to ﬁnish a compilation before an execution ﬁnishes, one generation takes
just 80 minutes to complete. Thus, large tuning sessions providing two-digit speedup values can be completed in
1-2 days. In TACT, the number of generations to run can be set either statically or dynamically. In the latter case
the tuning stops when the best value does not improve for the speciﬁed number of generations.
3 Please note that in order to show the relevant part of the graph in better details the axis are shown at the oﬀset and are not originating at
zero.

1317

1318

Dmitry Plotnikov et al. / Procedia Computer Science 18 (2013) 1312 – 1321

Fig. 3. Best (top) and average (bottom) performance improvement for each generation, 0 corresponds to the -O2 optimization level, 1 corresponds to the maximum performance achieved with tuning

5. Analyzing Tuning Results
Typically, the user of a performance tuning tool is only interested in reproducing the best tuning result by
passing the exact compilation string found by the tool as CFLAGS. For the compiler developer the greatest interest
is where the speedup comes from, e.g. which compiler optimizations are involved in the observed speedup, which
parameters make them work better than with the default ones, which default optimizations were disabled to obtain
top performance, etc. In this section we describe the after-tuning analysis tools provided by TACT, which can be
useful for a compiler developer.
5.1. Normalizing the Flags Set
The results of automatic performance tuning are hard to analyze, since tuning tools usually provide a long
and obscure string that may include hundreds of compiler ﬂags. In TACT, we chose to explicitly include every
parameter or option being tuned, no matter if it’s already included or not in a baseline level. The other option
was to make the tuning tool aware of the default set of optimizations and parameters for chosen base level of the
speciﬁc version of the compiler. As for results interpretation, we are mostly interested in the diﬀerence between
the optimal compile string compared to the -O2 default set of optimizations, the resulting compile string ﬁrst
should be normalized, excluding the ﬂags corresponding to optimizations already enabled in the -O2. These also
include the parameters that are speciﬁc only to the optimizations that are disabled in the resulting set.
The ﬁltering is based on comparing MD5 hashes of application binary compiled with diﬀerent sets of compiler
ﬂags. If the hashes match, then ﬂags that constitute the diﬀerence can be omitted, i.e. normalized ﬂag set satisﬁes
the following4 :
∀ﬂag ∈ FlagSet : hash(FlagSet \ ﬂag) hash(FlagSet).
TACT can ﬁlter ﬂags based on the above condition. To optimize the number of required compilations, we
are trying to throw out several options at a time, using binary search. Also, we automatically maintain historical
database of signiﬁcance for every option. This characteristic represents the probability of whether omitting the
option from the compile string will yield the binary hash change. This database is updated after each ﬁltering run
for the same application. We sort the option set according to ﬂag signiﬁcance, so that insigniﬁcant ﬂags are more
likely to be grouped together and ﬁltered out earlier. The described normalization procedure typically reduces the
number of ﬂags by the factor of 3 to 6.
4 This doesn’t cover ﬂags that can be excluded only together with each other (e.g. mutually exclusive ﬂags), but GCC doesn’t have many
such ﬂags anyway, and they can be ﬁltered with other methods (see Section 5.2).

Dmitry Plotnikov et al. / Procedia Computer Science 18 (2013) 1312 – 1321

220
200
180
160
140
120
100
80
60
40
20
0
1

2

3
C-Ray

4

5

700
650
600
550
500
450
400 of flags in
Number
350 set
original
300
250
Number of required
200
runs
150
100
50
0
1
2

1319

Number of flags in
original set
Number of required runs

3

4

5

libevas

Fig. 4. Number of runs required to reduce conﬁgurations consisting of the given number of ﬂags

5.2. Reducing the Resulting Flag Set Based on Performance
Though after the normalization the resulting ﬂag set contains only those ﬂags that aﬀect the compiled binary,
many of them do not signiﬁcantly aﬀect the application’s performance. To identify ﬂags that aﬀect performance
the most, we do further performance-based reducing of the resulting conﬁgurations.
The procedure starts with the original normalized ﬂag set, and then on each step it tries to drop a single ﬂag,
so that its exclusion improves5 the performance. If such ﬂag can not be found, then the ﬂag with the least negative
impact is excluded. This procedure repeats until only the base ﬂags are left (usually -O2).
For the ﬂag set consisting of N ﬂags it requires N − 1 steps, and on each step the tool performs as many runs
as there are currently ﬂags left in the set, looking for the least signiﬁcant. So the total number of runs required
by a straightforward implementation of the reducing algorithm does not exceed (N − 1)(N − 2)/2, and usually is
less, since we continue traversing the ﬂag set from the ﬂag dropped on the last step if the impact of such operation
was positive. Also, to speedup the process, we’re ﬁrst trying to remove ﬂags in large groups, which are formed
based on the performance impact the ﬂag has shown when reducing the previous conﬁguration. This way, ﬂags
with historically least negative eﬀect (for this tuning session) have priority. If removing the whole group decreases
performance, then we again consider the worse half of the group. We continue the dichotomy until a better set is
found or only one option is left.
After that, we proceed with a straightforward algorithm described above. Such a method helps to suﬃciently
reduce number of runs. Fig 4 shows how the number of runs required to reduce conﬁgurations of comparable
size changes when sequentially reducing ﬁve best conﬁgurations. For C-Ray, the minimum number of iterations
required to reduce all ﬁve conﬁgurations without reusing of previous knowledge or grouping would be 2671, but
in our experiments it was 773. For libevas, the ﬁrst conﬁguration was lucky to drop most of its options at the
ﬁrst reducing step, so we didn’t get many recommendations recorded for the second conﬁguration. However, both
graphs show a tendency of decreasing number of runs for the subsequent conﬁgurations.
Fig 5 shows how much the resulting compiler ﬂag set can be reduced for chosen applications. Out of the
original 200 compilation ﬂags, after initial ﬂag normalization by binary hash only 33-62 signiﬁcant options were
left. Then, the resulting set is further reduced to 12-27 ﬂags by the method described in this section (ﬁrst bar).
Out of those, 4-14 options provide 80% of achieved improvement (second bar), so after manual inspection some
of those may be included in the application’s Makeﬁle. And 4-17 options in results were actually optimizations
turned oﬀ from the conﬁguration speciﬁed by the -O2 optimization level, or parameters that diﬀer from their
default values (third bar). Flags in the latter category are the most interesting for compiler developer, because
they may point at potential problems in the compiler default optimizations or to their suboptimal tuning for the
application or the target architecture.

1320

Dmitry Plotnikov et al. / Procedia Computer Science 18 (2013) 1312 – 1321
-*

,/

,*

+/
		

+*
	2*6


/

		 
&%%$'&%%'


*


"

%



+0#16

+0#,6

	




,-#,6

-.#.6

,0.

,-#36

	

+/#/6

Fig. 5. Number of ﬂags in the resulting set after reducing by performance, the number of most signiﬁcant ﬂags, and the number of most
interesting ﬂags for analysis

Score

%Prev

25.319 0.00%
25.627 1.22%
25.689 0.24%
25.622 -0.26%
26.839 4.75%
28.431 5.93%
29.561 3.98%
30.639 3.64%
...
31.515 0.32%
31.637 0.38%
31.638 0.00%
*31.674 0.11%
31.565 -0.34%

%Base

%Best

Flags diff

Reduction cost

0.00%
1.22%
1.46%
1.20%
6.00%
12.29%
16.76%
21.01%

-20.06%
-19.09%
-18.89%
-19.11%
-15.26%
-10.24%
-6.67%
-3.27%

-O2
-DEVAS_UNROLL_FACTOR=0
-mvectorize-with-neon-quad
-mfpu=neon
-ftree-vectorize
-fvectorize-misaligned
-fprefetch-loop-arrays
--param l1-cache-line-size=64

-27.10%
-21.10%
-19.27%
-19.03%
-13.53%
-9.25%
-6.31%

24.47%
24.95%
24.96%
25.10%
24.67%

-0.50%
-0.12%
-0.11%
-0.00%
-0.34%

-fno-if-conversion2
-funwind-tables
-fno-thread-jumps
-fno-expensive-optimizations
-fno-tree-ter

-0.27%
-0.45%
-0.35%
-0.11%

Fig. 6. Example of performance-based ﬂag reducing with libevas

5.3. Analyzing the Resulting Reduced Flags
The reducing procedure described in the previous section allows one to estimate the relative importance of
compiler ﬂags for the performance. Given that our sequential exclusion algorithm saves ﬂags with most impact
for last steps, the earlier the ﬂag was dropped, the less importance it has.
The example of relative importance based on reducing results by performance for libevas tuning is shown
on Fig 6. The lines on the ﬁgure correspond to reducing procedure steps in the reverse order, i.e. the reducing
procedure starts with a set of approximately 50 compiler ﬂags (this state corresponds to the last line), and on
its ﬁrst step drops the worst performing option -fno-tree-ter, winning 0.34% with that. This state (with one
excluded option) corresponds to the previous line (as it’s the best result for reducing, it’s marked with asterisk).
We can also interpret this ﬁgure another way: starting with -O2, we are walking down the list and adding up to
compile string one ﬂag per line. Then, the meaning of each line would be the gain from adding a new ﬂag to the
previous line conﬁguration. The gain values shown in three columns are evaluated relative to the previous line,
base, and the best result respectively.
The reduction cost in the last column is the value of slowdown measured with the best conﬁguration excluding
the correspondent option. All reduction costs are calculated on the step marked with an asterisk and are shown
just as additional characteristic of ﬂag importance.
5 The presence of such ”harmful” ﬂags in the ﬁnal tuning result can be attributed to measuring inaccuracy or insuﬃcient number of tuning
runs. The typical positive gain from the performance-based reducing is 0.2-2%.

Dmitry Plotnikov et al. / Procedia Computer Science 18 (2013) 1312 – 1321

1321

From this table it’s clear that manual loop unrolling in the application source (controlled by macro -DEVAS
UNROLL FACTOR) was turned oﬀ by the tuning as a prerequisite in order for GCC loop optimizations to work; that
an experimental patch for misaligned access support for ARM NEON (guarded by the custom ﬂag -fvectorizemisaligned) immediately adds almost 6% to standard GCC vectorization; the proper setting of the cache line
size according to ARMv7 speciﬁcation (--param l1-cache-line-size=64) in prefetching contributes to the
performance almost as much as enabling the prefetching itself. Note that the correct value for cache line size, as
well as values for other binary options, were evolved during the tuning, and only later we have found that 64 is
indeed the correct value for this parameter in ARMv7 architecture.
6. Conclusion
In this paper we have described a method for fast searching of possible compiler deﬁciencies for a given
platform. We have proposed the requirements for an automatic tool to aid compiler analysis and optimization.
We have presented our Tool for Automatic Compiler Tuning (TACT), which has the following main features: it
has the tools for analysis of tuning results that allow to identify compiler optimizations that contribute the most
to the improvement, supports parallel compilation and execution on several devices to speedup the tuning, and is
capable of performing multi-objective optimization to evolve Pareto-front of optimal conﬁgurations.
Using TACT tool, we have tuned few popular open-source applications C-Ray, Crafty Chess, libevas (part of
Enlightenment Foundation Libraries), SciMark, x264 and zlib. All applications were tuned on ARM Cortex-A9
boards, using all of approximately 200 options and parameters available in GCC 4.8. The tuning has resulted in
15-34% speedup of these applications, while 80% of this improvement can be achieved with 4-17 options. Such
results can be obtained in a period of a several hours to a few days, depending on the test application, target
hardware and tuning parameters.
We used our results for improvement of the GCC compiler. We developed three patches that were accepted
into GCC mainline. Also, steady appearance of the ﬂag that disabled Global Common Subexpression Elimination
in results for many applications and subsequent analysis of this problem draw our attention to an old patch in
Linaro GCC, that yield improvement of SPEC 2000 INT by 4% for GCC mainline on ARM.
We are now continuing development of TACT, and we plan to release it as open source software later this year.
References
[1] D. Melnik, A. Belevantsev, D. Plotnikov, S. Lee, A case study: optimizing gcc on arm for performance of libevas rasterization library,
in: Proceedings of International Workshop on GCC Research Opportunities (GROW-2010), Pisa, Italy, 2010.
URL http://ctuning.org/dissemination/grow10-03.pdf
[2] Acovea project.
URL http://freecode.com/projects/acovea
[3] F. Bodin, T. Kisuki, P. M. W. Knijnenburg, M. O’Boyle, E. Rohou, Iterative compilation in a non-linear optimisation space (1998).
[4] K. Hoste, L. Eeckhout, Cole: compiler optimization level exploration, in: Proceedings of the 6th annual IEEE/ACM international
symposium on Code generation and optimization, CGO ’08, ACM, New York, NY, USA, 2008, pp. 165–174.
[5] J. Cavazos, M. F. P. O’Boyle, Automatic tuning of inlining heuristics, in: Proceedings of the 2005 ACM/IEEE conference on Supercomputing, SC ’05, IEEE Computer Society, Washington, DC, USA, 2005, p. 14.
[6] E. Park, S. Kulkarni, J. Cavazos, An evaluation of diﬀerent modeling techniques for iterative compilation, in: Proceedings of the 14th
international conference on Compilers, architectures and synthesis for embedded systems, CASES ’11, ACM, New York, NY, USA,
2011, pp. 65–74.
[7] G. G. Fursin, M. F. P. O’Boyle, P. M. W. Knijnenburg, Evaluating iterative compilation, in: Proceedings of the 15th international
conference on Languages and Compilers for Parallel Computing, LCPC’02, Springer-Verlag, Berlin, Heidelberg, 2005, pp. 362–376.
[8] G. Bashkansky, Y. Yaari, Black box approach for selecting optimization options using budget-limited genetic algorithms, SMART’07
(2007) pp. 1–16.
[9] G. Pekhimenko, A. D. Brown, Eﬃcient program compilation through machine learning techniques, in: Proceedings of the The Fourth
International Workshop on Automatic Performance Tuning (iWAPT), Tokyo, Japan, 2009.
[10] G. Fursin, Y. Kashnikov, A. Memon, Z. Chamski, O. Temam, M. Namolaru, E. Yom-Tov, B. Mendelson, A. Zaks, E. Courtois, F. Bodin,
P. Barnard, E. Ashton, E. Bonilla, J. Thomson, C. Williams, M. O’Boyle, Milepost gcc: Machine learning enabled self-tuning compiler,
International Journal of Parallel Programming 39 (2011) 296–327.
[11] G. Fursin, O. Temam, Collective optimization: A practical collaborative approach, ACM Transactions on Architecture and Code Optimization 7 (2010) 20:1–20:29.
[12] E. Zitzler, M. Laumanns, L. Thiele, Spea2: Improving the strength pareto evolutionary algorithm.

