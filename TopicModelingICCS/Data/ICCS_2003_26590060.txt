An Efficient Navigation Method for Virtual Endoscopy
Using Volume Ray Casting
Byeong-Seok Shin and Suk Hyun Lim
Inha University, Department of Computer Science and Engineering
253 Yonghyeon-Dong, Nam-Gu, Inchon, 402-751, Korea
bsshin@inha.ac.kr
q2011498@inhavision.inha.ac.kr

Abstract. In virtual endoscopy, it is important to devise the navigation method
that enables us to control the position and orientation of virtual camera easily
and intuitively without collision to organ wall. We propose an efficient navigation algorithm that calculates depth information while rendering a scene for
current frame with volume ray casting method, then computes new viewing
specification of camera for the next frame using the depth information. It can
generate a camera path in real-time, allows us to control the camera with ease,
and avoids collision with objects efficiently. In addition, it doesn’t require preprocessing stage and extra storage for maintaining spatial data structures. Experimental result shows that it is possible to navigate through human colon
when we apply our method to virtual colonoscopy.

1 Introduction
Optical endoscopy is a less-invasive diagnosis method. We can directly examine the
pathologies of internal organs by putting endoscopy camera into human body. It offers the highest quality images than any other medical imaging methods. However it
has some disadvantages of causing patients discomfort, limited range of exploration
and serious side effect such as perforation, infection and hemorrhage.
Virtual endoscopy is regarded as a good alternative of optical endoscopy. After acquiring cross-sectional images of human abdomen with CT or MRI, we can reconstruct three dimensional volume models and provide visualizations of inner surface of
the human organ, which has pipe-like shape such as colon, bronchi, and blood vessels.
Since the virtual endoscopy is non-invasive examination, there is no discomfort and
side effects. It is adequate for mass screening of pathological cases and training tool
for doctors and medical students.
In order to implement virtual endoscopy based on volume rendering technique, we
should devise a method that generates high quality perspective images within short
time. However, it is more important to detect collision between virtual camera and
organ wall in real-time, and let the camera smoothly move along the center-line of
human cavity. Since the organs to be examined such as colon and bronchus have
P.M.A. Sloot et al. (Eds.): ICCS 2003, LNCS 2659, pp. 60–69, 2003.
© Springer-Verlag Berlin Heidelberg 2003

An Efficient Navigation Method for Virtual Endoscopy Using Volume Ray Casting

61

complex structures, it is difficult to move forward endoscopy camera and change its
orientation even for experienced doctors. So, fast and accurate navigation is essential
for efficient diagnosis using virtual endoscopy. Previous methods for navigation have
some problems to demand a lot of computation cost in preprocessing step and it
doesn’t allow user to change the camera position and orientation during navigation, or
to require extra storages for spatial data structures such as a distance map or a potential field.
In this paper, we propose an efficient algorithm that generates depth information
while rendering a scene in current frame with volume ray casting, then determines
camera orientation for the next frame using the information. Since the depth information can be estimated without additional cost during ray casting, it computes camera
path in real-time. In addition, it doesn’t require extra memory and preprocessing step.
Related works is summarized in the next section. In section 3, we present our algorithm in detail. Experimental results and remarks are shown in section 4. Lastly, we
summarize and conclude our work.

2 Related Work
Volume ray casting is the most famous volume rendering method [1]. After firing a
ray from each pixel on the view plane into volume space, it computes color and opacity on sample points along the ray using the corresponding voxel values and opacity
transfer function. Then it determines final color for the pixel by blending them. Although it takes long time to make an image due to randomness in memory reference
pattern, it produces high-quality images in comparison to the other methods. Also it
can make perspective images such as endoscopic image. It is possible to improve its
rendering speed using some acceleration technique such as template based rendering
[2], shear-warp decomposition [3], and space-leaping [4].
In order to implement realistic virtual environment, several kinds of interactions
between objects should be considered. Especially, collision detection is more important than any other components for visual realism [5]. We have to devise appropriate
way to specify the surface of volumetric objects since they do not have explicitly
defined surfaces unlike geometric objects. Usually we define object boundaries indirectly using the opacity transfer function, which assigns higher opacity values to
voxels regarded as boundaries, and lower opacities to the others. This process is
called classification. Several spatial data structure might be used to detect collision
between an object and the indirectly specified surfaces.
Occupancy map has the same resolution as the target volume dataset and each cell
of the map stores identifiers for objects that occupy the cell [6]. As the objects change
its position, the values of each cell should be updated. If a cell has two or more identifiers, it is recognized as collision of those objects. This method requires large amount
of storage as original volume dataset, and it should update its contents whenever an
object change its position and size.
Three dimensional distance map can be used for collision detection and avoidance
[7]. A distance map is a 3D spatial data structure that has the same resolution of its

62

B.-S. Shin and S.H. Lim

volume data and each point of the map has the distance to the nearest boundary voxel
of an object. We can exploit the distance information generated in preprocessing time
for collision detection without extra cost. The smaller the distance values the larger
the collision probability and vice versa. Asymptotic complexity of collision detection
is O(1) , since we have to refer the distance map only once. Due to the occupancy
map, this method requires large amount of storage and considerably long preprocessing time.
Navigation method for virtual endoscopy can be classified into three categories;
manual navigation [8], planned navigation [9] and guided navigation [10]. In manual
navigation, we can directly control the virtual camera to observe anywhere we want
to examine. However the user might feel discomfort since camera movement is entirely dependent on user’s control, and collision with organ wall may occur if the user
misleads the camera.
Planned navigation calculates entire navigation path in preprocessing time using
several path generation algorithm, then moves through the camera along the precalculated path in navigation step. It can fly through the desired area without user
intervention. However it requires a lot of computation cost in preprocessing step and
it doesn’t allow users to control the camera position and orientation intuitively.
Guided navigation is a physically-based method, which makes an spatial data
structures such as potential field in preprocessing step and determines camera orientation by considering attractive force that directs from starting point to target point,
repulsive force from organ surface, and user’s input. It guarantees that a camera arrives to target point and move the camera to anywhere the user want to place intuitively without collision against organ wall. However it is very hard to implement and
it demands a lot of cost to make and maintain the vector volume just like a potential
field.

3 Navigation Method Using Volume Ray Casting
In order to implement efficient navigation, it have to satisfy the following requirements. (1) It should avoid collision between organ surface and virtual camera. (2) It
should guarantee smooth movement of camera, and camera orientation should not
change abruptly. (3) It should allow users to control the camera conveniently and
intuitively. (4) It should not require preprocessing step, or minimize that if needed.
(5) It should not require extra storages.
Guided navigation algorithms mentioned in the previous section is regarded as object-space method since they exploit overall information of entire volume dataset.
Even though it guarantees accurate navigation, this is a reason why it takes a long
preprocessing time. In virtual endoscopy, it requires information only for a small part
of entire volume since field of view is restricted in cavities of a volume data set. Our
navigation algorithm computes camera orientation using voxels visible in current
viewing condition, thus we define it as image-space method.

An Efficient Navigation Method for Virtual Endoscopy Using Volume Ray Casting

63

In rendering point of view, navigation is a kind of animation sequence that moves
a camera along a smooth path. Here, we explain how to determine camera orientation
for the next frame fi+1, using the information obtained in rendering time for current
frame fi. We determine the direction of rotation using the largest depth value, and
angle (magnitude) of rotation based on the smallest value. Figure 1 shows the procedure of our method. Rendering process is the same as in the conventional animation.
Our method has another pipeline to specify the viewing condition of camera. It generates depth information while rendering and computes direction of rotation and rotation angle by using the depth information.

Fig. 1. A procedure for determining reliable navigation path

3.1 Calculating Depth Information

Volume ray casting fires a ray from each pixel on the view plane into volume space,
then it computes color and opacity on sample points along the ray and determines
final color for the pixel by blending them. Empty space (cavities) of volume data has
low opacity. On the contrary organ wall has relatively high opacity value. While
sampling voxel values along a ray, we can compute the distance from a pixel to the
nearest non-transparent voxel using the number of samples and unit distance between
two consecutive samples. Figure 2 depicts how to obtain distance value for each pixel.
In our application, the distance can be regarded as the depth value from a camera to
organ wall.
Actually, we need only the maximum and minimum depth value for the following
steps. Let di,j be a depth value of a pixel pi,j, and Ri,j be the direction of ray for that
pixel. Minimum and maximum depth value dmin and dmax can be defined as follows:

64

B.-S. Shin and S.H. Lim

d min = min(

M ,N

M ,N

i , j =0

i , j =0

U d i , j ) , d max = max( U d i , j ) .

(1)

where M and N are horizontal and vertical resolution of final image respectively. Also
we store the direction of a ray fired from a pixel that has the maximum depth value as
a form of unit vector, which is denoted as Rmax.

Fig. 2. Computing distance from a pixel to the nearest non-transparent voxel (is potentially
regarded as organ boundary)

3.2 Determining the Direction of Rotation of Virtual Camera

Depth values are inverse proportional to the possibility of collision. Larger depth
value means that obstacles are located far away from the current camera position. On
the contrary, smaller the depth value, higher the collision possibility since the camera
is near the organ wall. In this case, we have to rotate the camera rapidly to avoid
collision. That is, we can minimize the collision possibility by altering the camera
orientation to the direction of ray that has the maximum depth value.
We can define the direction of rotation ri for the next frame fi+1 can be defined as
follows:

ri = R max − VPN i .

(2)

where Rmax is a unit vector computed in the previous step, and VPNi is also a unit
vector that represents a view plane normal for current frame fi . Figure 3 shows how
to determine the direction of rotation using Rmax .

An Efficient Navigation Method for Virtual Endoscopy Using Volume Ray Casting

65

Fig. 3. Determination of the direction of rotation using the maximum distance value

3.3 Calculating the Rotation Angle
After determining the direction of rotation ri, we have to calculate rotation angle
using minimum depth value dmin. As we mentioned before, smaller dmin implies that a
camera is located near the organ wall, so we should increase the rotation angle along
ri. On the contrary, larger dmin decreases the angle.
It is simple but has a problem of instability. Since our method changes the camera
orientation in every frame, unnecessary changes might occur in open area as shown in
Figure 4. This causes camera trembling, so it cannot satisfy the requirement (2). To
solve the instability problem, we define threshold depth value dth, then we do not alter
camera orientation in sufficiently open area by considering the value. If dmin is larger
than dth, it is regarded as in open area and we have to keep the camera orientation
unchanged.
We can determine the rotation angle θ based on dth and dmin as follows:

if d min > d th
d th −d min
.
arctan(
) otherwise

d th
0


θ =

(3)

Figure 5 shows how to determine the rotation angles according to the difference between dth and dmin.
In consequence, we can obtain the final camera orientation VPNi+1 for the text
frame as in the follow equation:

VPN i +1 =

ri tan θ + VPN i
.
| ri tan θ + VPN i |

(4)

66

B.-S. Shin and S.H. Lim

Fig. 4. An example of trembling of camera orientation in open area. It should be fixed along
the desired navigation path.

Fig. 5. Determination of the rotation angle using the minimum distance value

4 Experimental Results
In this section we will check whether our method offers reliable navigation through
human colon with collision avoidance. Also we will compare the preprocessing and
rendering time of a typical object-space navigation method based on distance transformation and our navigation algorithm. All of these methods are implemented on a

An Efficient Navigation Method for Virtual Endoscopy Using Volume Ray Casting

67

PC which has Pentium IV 1.7GB CPU, 1GB main memory, and ATI RADEON 9000
graphics accelerator. The volume data used for the experiment is a clinical volume
obtained by scanning a human abdomen with a spiral CT (computed tomography)
with resolution 512 × 512 × 541 .
Figure 6 depicts the result of the method that does not perform collision detection
and avoidance. In this case, a camera would bump against colon wall unless a user
manually changes its direction. Figure 7 shows the example of using collision
avoidance feature of our method in virtual colonoscopy. As you can see, the camera
could move forward along the colon cavity automatically.
Table 1 shows the comparison of preprocessing time and rendering time of a
method that does not perform any kind of collision avoidance (NONE), an objectspace guided navigation method using distance map (DIST) and our image space
navigation method (OURS). While an object-space method spends long time for generating a distance map in preprocessing step, an image-space method doesn’t need
preprocessing time at all. In rendering time, the difference between NONE and OURS
is very small (about 6.51 %), which implies that our method requires small amount of
additional cost to calculate camera orientation during rendering. Also our navigation
method is not so slow (only 4.91 %) even in case of comparing it with DIST.

Fig. 6. This is an example of not using collision detection and avoidance feature. A camera
would bump against colon wall unless a user manually changes its direction

68

B.-S. Shin and S.H. Lim

Fig. 7. This is a result of our navigation method. The camera could move forward along the
colon cavity automatically

Table 1. comparison of preprocessing time and rendering time of a method that does not perform any kind of collision avoidance (NONE), an object-space guided navigation method
using distance map (DIST) and our image space navigation method (OURS) for
512 × 512 × 541 volume data set

Preprocessing time
Rendering time

DIST
(msec)
186880
265

NONE
(msec)
–
261

OURS
(msec)
–
278

OURS /
DIST (%)
n/a
4.91

OURS /
NONE (%)
n/a
6.51

Object space method doubles up the memory consumption since it should maintain
the distance map that has the same size as the original volume data set. On the contrary, our method does not require additional storage. For manipulating
512 × 512 × 541 volume dataset, about 142Mbytes memory is need for volume data
and the sample size storage is required for the distance map.

An Efficient Navigation Method for Virtual Endoscopy Using Volume Ray Casting

69

5 Conclusion
We propose an efficient navigation method that reflects user’s control and prohibits a
camera from bump against organ wall, when the camera moves through a human
cavity such as colon and bronchus. This method makes it possible to perform collision-free navigation and gives us smooth camera movement without trembling. Since
it exploits depth information generated in the previous rendering stage, it doesn’t
require preprocessing step as well as extra storage at all. Using this method, we can
go forward and backward anywhere we want to see without consideration about collision between camera and organ wall. Experimental results show us that our method
satisfies most of requirement for guided navigation. Future work should be focused
on improving this method to obtain more smooth and natural path for camera movement.

References
1.

Levoy, M.: Display of Surfaces from Volume Data,• IEEE Computer Graphics and
Applications, Vol. 8, No. 3 (1988) 29–37
2. Yagel, R. and Kaufman, A.: Template-based volume viewing, Computer Graphics Forum
(Eurographics 92 Proceedings), Cambridge, UK (1992) 153–167
3. Lacroute, P. and Levoy, M.: Fast volume rendering using a shear-wqrp factorization of the
viewing transformation, Computer Graphics (SIGGRAPH 94 Proceedings), Orlando, Florida (1994) 451–458
4. Yagel, R. and Shi, Z.: Accelerating volume animation by space-leaping, Proceedings of
IEEE Visualization 1993 (1993) 62–69
5. He, T. and Kaufman, A.: Collision detection for volumetric objects, Proceedings of Visualization 1997 (1997)27–34
6. Gibson, S.: Beyond volume rendering: visualization, haptic exploration, and physical
th
modeling of voxel-based objects, 6 Eurographics Workshop on Visualization in
Scientific Computing, (1995)
7. De Assis Zampirolli, F. and De Alencar Lotufo, R.: Classification of the distance transformation algorithms under the mathematical morphology approach•, Proceedings XIII
Brazilian Symposium on Computer Graphics and Image Processing (2000) 292–299
8. Gleicher, M. and Witkin, A.: Through-the lens camera control, ACM SIGGRAPH (1992)
331–340
9. Hong, L., Kaufman, A., Wei, Y., Viswambharan, A., Wax, M. and Liang, Z.: 3D virtual
colonoscopy, IEEE Symposium on Biomedical Visualization (1995) 26–32
10. Hong, L., Muraki, S., Kaufman, A., Bartz, D. and He, T.: Virtual voyage: Interactive
navigation in the human colon, ACM SIGGRAPH (1997) 27–34

