Procedia Computer Science
Volume 51, 2015, Pages 2769–2773
ICCS 2015 International Conference On Computational Science

Federated Big Data for resource aggregation and load
balancing with DIRAC
V´ıctor Fern´andez1 , V´ıctor M´endez2 , and Tom´as F. Pena3
1

2
3

Department of Particle Physics, University of Santiago de Compostela (Spain)
victormanuel.fernandez@usc.es
Computer Architecture and Operating Systems (CAOS), Universitat Aut´
onoma de Barcelona
(Spain) vmendez@caos.uab.es
Research Center in Information Technologies (CiTIUS), University of Santiago de Compostela
(Spain) tf.pena@usc.es

Abstract
BigDataDIRAC is a Big Data solution with a Distributed Infrastructure with Remote Agent
Control (DIRAC) access point. Users have the opportunity to access multiple Big Data resources scattered in diﬀerent geographical areas, such as access to grid resources. This approach
opens the possibility of oﬀering not only grid and cloud to the users, but also Big Data resources
from the same DIRAC environment. In this work, we describe a system to allow access to a
federation of Big Data resources, including load balancing, using DIRAC. Our results demonstrate the ability of BigDataDIRAC to manage jobs driven by dataset location stored in the
Hadoop File System (HDFS) of the Hadoop distributed clusters. DIRAC is used to monitor the
execution, collect the necessary statistical data, and upload the results from the remote HDFS
to the SandBox Storage machine. Performance results demonstrate that BigDataDIRAC load
balancing is able to aggregate resources in an eﬃcient manner.
Keywords: Big Data federation, DIRAC, MapReduce, Hadoop, Cloud Computing

1

Introduction

More than ever, scientiﬁc communities need to access large computational resources. Scientists are generating datasets that are increasing exponentially in both complexity and volume,
making their analysis a signiﬁcant challenge.
The DIRAC interware [8], is a platform providing a complete solution to scientiﬁc communities requiring access to distributed resources, including clouds, grids, and local clusters [6].
FG DIRAC service main communities are from lifescience, particularly the biomedical image
applications of CREATIS project [3]. DIRAC EGI service main community is WeNMR project,
processing applications for NMR spectroscopy and structural biology. There are other DIRAC
instances for single and multiple communities [2], mainly in the ﬁelds of high energy physics.
Selection and peer-review under responsibility of the Scientiﬁc Programme Committee of ICCS 2015
c The Authors. Published by Elsevier B.V.

doi:10.1016/j.procs.2015.05.430

2769

Federated BD for resource aggregation and LB with DIRAC

Fern´
andez, M´endez and Pena

Big Data technologies have arisen as viable alternatives for problems involving processing
very large amounts of data. The MapReduce programming model [4] abstracts the common difﬁculties linked to distributed processing on large clusters, providing an easy-to-use programming
model that features automatic parallelization, scalability, and data locality-based optimizations.
In [5], BigDataDIRAC was proposed as a solution which gives users the opportunity to
access multiple Big Data resources scattered in diﬀerent geographical areas, such as access to
grid resources, including a centralized data catalog to integrate Big Data clusters in a coherent
way. In this work, we analyzed the use of BigDataDIRAC to allow access to a federation of Big
Data cluster, including load balancing among them. Obtained results show that BigDataDIRAC
load balancing is able to eﬃciently aggregate remotely distributed resources.
The rest of this work is organized as follows. Section 2 presents our solution for the federated workload management. In Section 3 the experimental setup is described and results are
analyzed. Our conclusions are presented in Section 4

2

Federated BigDataDIRAC workload management

The integration of geographically distributed Big Data clusters across diﬀerent organizations
presents a major workload management problem. The general aim can be expressed as an
optimization problem. Optimization theory usually identiﬁes sub-problem heterogeneity to
deal with two major components, intensiﬁcation and diversiﬁcation. Intensiﬁcation is focusing
the solution search in a local region to ensure the convergence to the optimal. On the other
hand, at a global scale, diverse solutions explore the search scale to avoid local suboptimal.
Federated BigDataDIRAC proposes an on-line model for global scheduling to adapt well to
the dynamic conditions of the distributed Big Data clusters. The diversiﬁcation component of
the optimization problems can be addressed in a simple way by load balancing (LB).
DIRAC job matchmaking considers the precise running conditions at the resource and last
moment requests to the system. The DIRAC WMS provides a single scheduling mechanism for
jobs with very diﬀerent proﬁles. To achieve an overall optimization, it organizes pending jobs
in task queues with similar requirements. A group job priority is assigned to each task queue.
Figure 1 shows the schema for scheduling in BigDataDIRAC, based in the Task Queue
matching with available resources and pushing jobs to the corresponding cluster. BigDataDIRAC scheduling pushes to each cluster with one waiting job, if there is pending workload
in the task queues corresponding to those cluster resources, and maintains the rest of the jobs in
the central DIRAC task queues to later push the job to the appropriate cluster. This dynamic
matchmaking is able to provide system LB between distributed resources. The LB aim is to
optimize resource usage and a common metric is the total makespan. At the same time, the
general load conditions of the resource service will aﬀect the particular job completion times,
so in an indirect way it also provides a job response average time optimization. Section 3 shows
diﬀerent use cases to test the optimization capabilities of the proposed WMS for federated BigDataDIRAC. The main hypothesis is that the federated BigDataDIRAC workload management
is able to take advantage of the aggregated computing power of distributed Big Data clusters.

3

Usecase tests and results

There are three main components in our experimental setup. The ﬁrst one consists in the
DIRAC central servers, running at the University of Santiago de Compostela (USC) in Spain,
including the DIRAC main server, the DIRAC user interface, and the Web Portal. Additionally,
2770

Federated BD for resource aggregation and LB with DIRAC

BigData
Directors
Hadoop v1.1.0

Job

Job

)

Job

Job

Task queue 2
Job
Job
Task queue 1
Job

Job

queues
running

Job

waiting

Hadoop v1.1.2

Stochastic
matching
job
Task queue N

Enqueue job

JobTracker

No waiting job

Find job
(cluster requirements)
BigData
Scheduler
Agent

Big Data cluster A

WorkLoad Management System

SS
H

BigData
Monitoring
Agent

Fern´
andez, M´endez and Pena

Job

sh
(pu

Hadoop v2.2.0
Hadoop v1.1.x
interactive
Hadoop v2.2.0
interactive
Hive v0.12

Job

Big Data cluster B
JobTracker
queues
running

Job

waiting

Job

Big Data cluster C
JobTracker
queues

New job X
Requirements

running

Job

waiting

Job

Figure 1: BigDataDIRAC scheduling system schema.

we use two Hadoop clusters running in two diﬀerent institutions: the BIGDATA.usc.es cluster
at USC and the BIGDATA.cesga.es at the Galician Supercomputing Center (CESGA), also
in Spain. For better portability of the setup, all DIRAC components and the Hadoop clusters
were created in cloud environments. In the performance evaluation tests the main hypothesis of
Section 2: The federated BigDataDIRAC WMS is able to take advantage of aggregated computing
power of distributed Big Data clusters by eﬃcient global load balancing will be evaluated. For
this purpose, two usecases were tested to characterize typical user patterns: Big Data jobs with
high IO throughput compared with CPU, and classical MapReduce jobs to ﬁlter lines including
a searched substring (grep), with higher CPU consumption compared to IO.
For the ﬁrst usecase, jobs with high IO throughput compared with CPU, a modiﬁed version
Terasort [7] (a very extended performance test for Hadoop clusters) has been created. This Java
job is composed of a randomwriter run, followed by a sort of the produced outputs for the ﬁrst
run to complete a terabyte sorting benchmark. In our particular usecase, some modiﬁcations
have been introduced. The benchmark was separated in two decoupled runs to keep record
of the system performances for diﬀerent use patterns. Therefore, the randomwriter usecase
analyses the high IO throughput pattern and storing results in the DIRAC data catalog for
future use of the produced ﬁles. The original randomwriter has also been modiﬁed to allow
diﬀerent ﬁle size productions, to run benchmarks in the gigabyte magnitude.
Two tests were run, both submitting a single randomwriter job per Big Data cluster. The
ﬁrst test produces a 2 GB random content ﬁle, splitting the ﬁle in two parts. The ﬁrst part is
created with a job running in BIGDATA.usc.es producing a 1 GB ﬁle, the second is another
1 GB ﬁle created in BIGDATA.cesga.es. Both jobs were submitted to DIRAC at the same time.
Table 1a shows results for the split federated 2 GB production, including metrics of Hadoop
jobs runtime (HRT), the corresponding DIRAC jobs response time (RT) and the makespan,
which is the response time for both jobs submitted to DIRAC at the same time.
2771

Federated BD for resource aggregation and LB with DIRAC

Site
USC
CESGA
Makespan

HRT
2264
950

RT
2424
1029
2424

Site
USC
CESGA

(a) Federated BigDataDIRAC: Split Randomwrite 2 GB (1 GB/site)

Fern´
andez, M´endez and Pena

HRT
4364
1900

RT
4433
2052

(b) Single cluster: Randomwrite 2 GB

Table 1: Randomwrite results (sec.)

To compare this federated performance, another test was run for a complete 2 GB random
write ﬁle in a single cluster, and repeated for USC and CESGA clusters to test diﬀerent powers
of the single clusters. This comparison test is shown in Table 1b, where the job response time
corresponds to the 2 GB makespan of the federated case of Table 1a.
Randomwrite results obtained by federated BigDataDIRAC shows a performance constrained by the cluster with USC less power. The rule can be expressed as: given a randomwrite
problem of size S, it can be split in N Hadoop clusters using BigDataDIRAC, obtaining a performance improvement of processing leastPowerCluster(S/N). Thus, with an additional aggregated
cluster of similar power than USC, the federated makespan would be about 4433/3.
The hypothesis federated BigDataDIRAC workload management is able to take advantage of
aggregated computing power of distributed Big Data clusters by an eﬃcient global load balancing
has been validated for this usecase, showing improvement by leastPowerCluster(S/N). Actually,
S/N performance is not exact, because there is some DIRAC overhead corresponding to the
diﬀerence between Hadoop runtime and DIRAC response time. Analyzing the federated single
jobs of 1 GB, we have overhead ratios of 7.1% (USC) and 8.3% (CESGA), while for 2 GB ﬁles
production we have ratios of 1.6% (USC) and 8.0% (CESGA).
Site
CESGA
USC
Average
Makespan

File size
0.5 GB
0.5 GB
HRT
242

Jobs exec.
10
10
RT
652
1104

(a) Federated BigDataDIRAC: Split Grep
1 GB

Site
CESGA
Average
Makespan

File size
1 GB
HRT
190

Jobs exec.
10
RT
759
1656

(b) Single cluster BigDataDIRAC: Grep 1 GB

Table 2: Grep results
The second usecase Grep for higher CPU than IO consumption is based on a 1 GB text
ﬁle, which is processed to obtain the lines which include the search substring. Table 2a shows
results for the split federated 1 GB Grep. In this case, the federated strategy was to split the
1 GB dataset in two ﬁles of 0.5 GB, one in CESGA and the other in USC. To process the
grep, it is necessary to submit the job to both Big Data clusters, so two jobs (one in each
cluster) are necessary to complete a single grep. The complete makespan is composed of 10
greps, corresponding to 20 DIRAC jobs, which were submitted at the same time and processed
stochastically as described in Section 2. The results of a complete 1 GB grep were taken from
sequential CESGA and USC job submissions, independently of when they were split processed.
Table 2a summarizes the second DIRAC job of the pairs, including the runtime addition, the
2772

Federated BD for resource aggregation and LB with DIRAC

Fern´
andez, M´endez and Pena

response time from the pair submission to the latest job end, and the total makespan from the
larger DIRAC job response time. The concept was to test not only the replication processing
strategy, as in the previous sort usecase, but also possible split processing strategies. Table 2b
shows the equivalent 1 GB Grep for a single cluster submission in the most powerful cluster.
Comparing the total makespan, the hypothesis federated BigDataDIRAC workload management is able to take advantage of aggregated computing power of distributed Big Data clusters
by an eﬃcient global load balancing has been validated with an improvement of 50%.

4

Conclusions

We presented the aggregation of Big Data resources in a federated context with DIRAC as the
connector framework. The use of the DIRAC framework has reduced the development time,
allowing, at the same time, to incorporate additional monitoring and accounting information.
The performance of our proposal was evaluated with two job patterns: high IO, and high
CPU. We veriﬁed the main hypothesis: BigDataDIRAC can take advantage of aggregated
computing, showing eﬃcient global load balancing scheduling with datasets of 0.5,1,2 GB.
The code of the new components used by DIRAC in the proposed BigDataDIRAC architecture have been described and published in a public git repository [1]. The next steps in this
work will be to test our proposed system with large size and real world datasets, and aggregate
Hadoop Streaming for running jobs written in diﬀerent languages, such as Python or Perl.

5

Acknowledgements

This work was supported by projects FPA2010-21885-C02-01/02 and TIN2013-41129-P.

References
[1] BigDataDIRAC, github repository. https://github.com/vfalbor/BigDataDIRAC. [Online; accessed Jan 2015].
[2] A. Casaj´
us, K. Ciba, V. Fern´
andez, R. Graciani, V. Hamar, V. M´endez, S. Poss, M. Sapunov,
F. Stagni, A. Tsaregorodtsev, and M. Ubeda. Status of the DIRAC project. Journal of Physics:
Conference Series, 396(3):032107, 2012.
[3] CREATIS project. http://www.creatis.insa-lyon.fr/site/. [Online; accessed Jan 2015].
[4] J. Dean and S. Ghemawat. MapReduce: simpliﬁed data processing on large clusters. Communications of the ACM, 51(1):107–113, 2008.
[5] V. Fern´
andez-Albor, V. M´endez-Munoz, and T. F. Pena. BigDataDIRAC: deploying distributed
Big Data applications. In 15th IEEE/ACM International Symposium on Cluster, Cloud and Grid
Computing (CCGRID). IEEE, 2015.
[6] T. Fiﬁeld, A. Carmona, A. Casaj´
us, R. Graciani, and M. Sevior. Integration of cloud, grid and
local cluster resources with DIRAC. In Journal of Physics: Conference Series, volume 331, page
062009. IOP Publishing, 2011.
[7] O. O’Malley. Terabyte sort on Apache Hadoop. http://sortbenchmark.org/YahooHadoop.pdf,
May 2008. [Online; accessed Jan 2015].
[8] A. Tsaregorodtsev, M. Bargiotti, N. Brook, A. Casaj´
us, et al. DIRAC: A community grid solution.
In Journal of Physics: Conference Series, volume 119, page 062048. IOP Publishing, 2008.

2773

