Available online at www.sciencedirect.com

ScienceDirect
Procedia
Computer Science 108C
(2017) 1019–1029and Kronecker
Separable
Covariance
Matrices
Separable
Covariance
Matrices
Separable Covariance Matrices and
and Kronecker
Kronecker
Approximation
Separable Covariance
Matrices
and
Kronecker
Approximation
Approximation
Separable
Covariance
Matrices
and
Kronecker
International
Conference
on
Computational
Science,
ICCS
2017,
12-14 June 2017,
Separable Covariance
Matrices
and
Kronecker
Approximation
Raja Zurich,
Velu and
Kris Herman
Separable
Covariance
Matrices
and
Kronecker
Switzerland
RajaApproximation
Velu and
Kris Herman
Separable
Covariance
Matrices
and
Kronecker
RajaApproximation
Velu and
Kris Herman
Separable
Covariance
Matrices
and
Kronecker
Syracuse
University
Separable Covariance
Matrices
and
Kronecker
RajaApproximation
Velu
and
Kris
Herman
Syracuse
University
Approximation
{rpvelu,kgherman}@syr.edu
Syracuse
University
Raja
Velu
and
Kris
Herman
Approximation
{rpvelu,kgherman}@syr.edu
Raja
Velu
and Kris Herman
Approximation
{rpvelu,kgherman}@syr.edu
Syracuse University
Raja Velu
and Kris Herman
Raja
Velu
and
Kris
Herman
Syracuse
University
{rpvelu,kgherman}@syr.edu
Raja
Velu
and
Kris
Herman
Syracuse
University
Raja
Velu
and
Kris
Herman
Raja
Velu
and
Kris
Herman
{rpvelu,kgherman}@syr.edu
Raja
Velu
and
Kris
Herman
Abstract
Syracuse
University
{rpvelu,kgherman}@syr.edu
Syracuse University

Abstract
Syracuse
University
{rpvelu,kgherman}@syr.edu
Syracuse
University
When a model structure allows
for Abstract
the error
covariance matrix to be written in the
{rpvelu,kgherman}@syr.edu
Syracuse
University
{rpvelu,kgherman}@syr.edu
When a model structure allows
for
the
error
covariance matrix to be written in the
Syracuse
University
{rpvelu,kgherman}@syr.edu
Abstract
form
of
the
Kronecker
product
of
two
positive
definite
covariance
matrices,
estimation
When a model structure allows
for the error
covariance
matrix
to be the
written
in the
{rpvelu,kgherman}@syr.edu
form of the Kronecker product{rpvelu,kgherman}@syr.edu
of two positive
definite
covariance matrices, the estimation
Abstract
of the
relevant
parameters
is
intuitive
and
easy
to
carry
out.
In
many
time
series
models,
form
of
the
Kronecker
product
of
two
positive
definite
covariance
matrices,
the
estimation
When
a
model
structure
allows
for
the
error
covariance
matrix
to
be
written
in the
Abstract
of the relevant parameters is intuitive and easy to carry out. In many time series models,
thethe
covariance
matrix
does
have
apositive
separable
(1993)
of
parameters
isnot
intuitive
and
tostructure.
carry
out.Van
In Loan
many
time
series
models,
When
model
structure
allows
theeasy
error
covariance
matrix
toand
be Pitsanis
written
in the
form
ofrelevant
thea Kronecker
product
of
twofor
definite
covariance
matrices,
the
estimation
Abstract
the When
covariance
matrix
does
not
have
a
separable
structure.
Van
Loan
and
Pitsanis
(1993)
model structure
allows
for Abstract
theproducts.
error covariance
matrixwetoapply
be written
in the
provide
anaapproximation
with
Kronecker
Incovariance
this
paper,
their
method
Abstract
the
covariance
matrix
does
not
have
separable
structure.
Van
Loan
and
Pitsanis
(1993)
form
ofrelevant
the
Kronecker
product
of
twoapositive
definite
matrices,
the
estimation
of
the
parameters
is
intuitive
and
easy
to
carry
out.
In
many
time
series
models,
Abstract
provide
ana
with
Kronecker
Incovariance
this paper,
weto
their
method
When
aapproximation
model structure
structure
allows
for
theproducts.
error
covariance
matrix
toapply
be the
written
in the
form
of the
Kronecker
product
of
twofor
positive
definite
matrices,
estimation
Abstract
When
model
allows
the
error
covariance
matrix
be
written
in
to
estimate
the
parameters
of
a
multivariate
regression
model
with
autoregressive
errors.
provide
anaapproximation
with
Kronecker
In this
paper,
wetoand
apply
their models,
method
Abstract
of
the
relevant
parameters
isnot
intuitive
and
easy
tostructure.
carry
out.Van
In Loan
many
time
series
the
covariance
matrix
does
have
apositive
separable
(1993)
When
model
structure
allows
for
theproducts.
error
covariance
matrix
be Pitsanis
written
in the
the
to
estimate
the
parameters
of
a
multivariate
regression
model
with
autoregressive
errors.
form
of
the
Kronecker
product
of
two
definite
covariance
matrices,
the
estimation
of
the
relevant
parameters
is
intuitive
and
easy
to
carry
out.
In
many
time
series
models,
When
a
model
structure
allows
for
the
error
covariance
matrix
to
be
written
in
the
form
of
the
Kronecker
product
of
two
definite
covariance
matrices,
the
estimation
When
a
model
structure
for
the
error
covariance
matrix
to
be
written
in
the
An
illustrative
example
is with
also
to
estimate
the
parameters
ofallows
aprovided.
multivariate
regression
model
with
autoregressive
errors.
the
covariance
matrix
does
not
have
apositive
separable
structure.
Van
Loan
and
Pitsanis
(1993)
provide
an
approximation
Kronecker
products.
In
this
paper,
we
apply
their
method
form
of
the
Kronecker
product
of
two
positive
definite
covariance
matrices,
the
estimation
When
a
model
structure
allows
for
the
error
covariance
matrix
to
be
written
in
the
An
illustrative
example
is
also
provided.
of
the
relevant
parameters
is
intuitive
and
easy
to
carry
out.
In
many
time
series
models,
the
covariance
matrix
does
not
have
a
separable
structure.
Van
Loan
and
Pitsanis
(1993)
form
of
the
Kronecker
product
of
two
positive
definite
covariance
matrices,
the
estimation
of
the
relevant
parameters
is
intuitive
and
easy
to
carry
out.
In
many
time
series
models,
form
of
the
Kronecker
product
of
two
positive
definite
covariance
matrices,
the
estimation
An
illustrative
example
is
also
provided.
provide
an
approximation
with
Kronecker
products.
In
this
paper,
we
apply
their
method
to
estimate
the
parameters
of
a
multivariate
regression
model
with
autoregressive
errors.
of
the
relevant
parameters
is
intuitive
and
easy
to
carry
out.
In
many
time
series
models,
form
of
the
Kronecker
product
of
two
positive
definite
covariance
matrices,
the
estimation
the
covariance
matrix
does
not
have
a
separable
structure.
Van
Loan
and
Pitsanis
(1993)
Keywords:
dimension–reduction,
multivariate
regression
approximation
ofPitsanis
covariance
matrices.
provide
an
approximation
with
Kronecker
products.
Inand
this
paper,
we and
apply
their models,
method
of
the
relevant
parameters
isnot
intuitive
and
easy
tostructure.
carry
out.
In Loan
many
time
series
models,
the
covariance
does
have
a
separable
Van
(1993)
of
the
relevant
parameters
is
easy
to
carry
out.
In
many
time
series
to
estimate
thematrix
parameters
ofintuitive
amultivariate
multivariate
regression
model
autoregressive
errors.
Keywords:
dimension–reduction,
regression
and
approximation
ofPitsanis
covariance
matrices.
An
illustrative
example
is with
also
provided.
the
covariance
matrix
does
have
a and
separable
Van
Loan
and
(1993)
of
relevant
parameters
isnot
and
easy
tostructure.
carry
out.
Inwith
many
time
series
provide
an
approximation
Kronecker
products.
Inand
this
paper,
we
apply
their models,
method
to the
estimate
thematrix
parameters
ofintuitive
amultivariate
multivariate
regression
model
with
autoregressive
errors.
the
covariance
does
not
have
a
separable
structure.
Van
Loan
and
Pitsanis
(1993)
Keywords:
dimension–reduction,
regression
approximation
of
covariance
matrices.
provide
an
approximation
with
Kronecker
products.
In
this
paper,
we
apply
their
method
the
covariance
matrix
does
not
have
a
separable
structure.
Van
Loan
and
Pitsanis
(1993)
An illustrative
exampledoes
is with
also
provided.
provide
an
approximation
Kronecker
products.
In
this
paper,
we
apply
their
method
the
covariance
matrix
not
have
a
separable
structure.
Van
Loan
and
Pitsanis
(1993)
to
estimate
theexample
parameters
of a
amultivariate
multivariate
regression
model
withwe
autoregressive
errors.
Anestimate
illustrative
is with
also
provided.
provide
an
approximation
Kronecker
products.
In
this
paper,
apply
their
method
Keywords:
dimension–reduction,
regression
and
approximation
of covariance
matrices.
to
the
parameters
of
multivariate
regression
model
with
autoregressive
errors.
provide
an
approximation
with
Kronecker
products.
In
this
paper,
we
apply
their
method
to
estimate
theexample
parameters
of aprovided.
multivariate
regression
model
withwe
autoregressive
errors.
provide
an
approximation
with
Kronecker
products.
Inand
this
paper,
apply
their method
An
illustrative
isElsevier
also
to
estimate
the
parameters
of
a
multivariate
regression
model
with
autoregressive
errors.
© 2017
The
Authors.
Published
byis
B.V.
Keywords:
dimension–reduction,
multivariate
regression
approximation
of covariance
matrices.
An
illustrative
example
also
provided.
to
estimate
the
parameters
of
a
multivariate
regression
model
with
autoregressive
errors.
An
illustrative
example
is
also
provided.
Keywords:
regression
and
approximation
of covariance
matrices.
to
theexample
parameters
of amultivariate
multivariate
regression
model
with autoregressive
errors.
Peer-review
underdimension–reduction,
responsibility
ofis
scientific
committee
of the International
Conference
on
Computational
Science
Anestimate
illustrative
isthealso
also
provided.
An
illustrative
example
provided.
1
Introduction
Keywords:
dimension–reduction,
multivariate
regression and
and approximation
approximation of
of covariance
covariance matrices.
matrices.
An illustrative
example is also multivariate
provided. regression
Keywords:
dimension–reduction,
1
Introduction
Keywords: dimension–reduction, multivariate regression and approximation of covariance matrices.
1 Keywords:
Introduction
dimension–reduction,
multivariate
regression
and approximation of
of covariance
covariance matrices.
matrices.
Keywords:
dimension–reduction,
multivariate
regression
and
Keywords:
dimension–reduction,
multivariate
regression
and approximation
approximation
covariance
matrices.
Regression
methods
are perhaps the
most widely
used statistical
tools in of
data
analysis.
When

1
Introduction
Regression
methods are perhaps the most widely used statistical tools in data analysis. When
several
response
variables
are studied
simultaneously,
are in the
sphere
of multivariate
reRegression
methods
are perhaps
the most
widely used we
statistical
tools
in data
analysis. When
1
Introduction
several
response variables are studied simultaneously, we are in the sphere of multivariate re1
Introduction
gression.
There
are
two
practical
concerns
regarding
this
general
multivariate
regression
model.
several
response
variables
are
studied
simultaneously,
we
are
in
the
sphere
of
multivariate
reRegression
methods
are
perhaps
the
most
widely
used
statistical
tools
in
data
analysis.
When
1
Introduction
gression.
There
are two
concerns
regarding
thisstatistical
general multivariate
regression
model.
1
Introduction
Regression
methods
are practical
perhaps
the
most
widely used
tools
in data
analysis.
When
1
Introduction
First,
the
accurate
estimation
of
all
the
regression
coefficients
may
require
a
relatively
large
gression.
There
are
two
practical
concerns
regarding
this
general
multivariate
regression
model.
several
response
variables
are
studied
simultaneously,
we
are
in
the
sphere
of
multivariate
reRegression
methods estimation
are perhapsofthe
widely used
statisticalmay
toolsrequire
in dataa analysis.
1
Introduction
First,
the
accurate
all most
the
regression
coefficients
relativelyWhen
large
1
Introduction
several
response
variables
are
studied
simultaneously,
we
are
in
the
sphere
of
multivariate
re1
Introduction
number
of
observations,
which
might
involve
some
practical
limitations.
Second,
even
if
the
First,
the
accurate
estimation
of
all
the
regression
coefficients
may
require
a
relatively
large
gression.
There
are
two
practical
concerns
regarding
this
general
multivariate
regression
model.
Regression
methods
are
the
widely
used
statistical
tools
in
analysis.
several
variables
are studied
simultaneously,
we
are limitations.
in the
sphere
of multivariate
renumberresponse
of observations,
which
might
involve
some
practical
Second,
even When
if the
Regression
methods
are perhaps
perhaps
the most
most
widely
used
statistical
tools
in data
data
analysis.
When

Regression
methods
are practical
perhaps
the
widely
used
statistical
tools
in data
analysis.
When
gression.
There
are
two
concerns
regarding
this
general
multivariate
regression
data
are
available,
interpreting
simultaneously
a large
number
ofmay
the
regression
coefficients
can
number
of
observations,
which
might
involve
some
practical
limitations.
Second,
evenmodel.
iflarge
the
First,
the
accurate
estimation
of
all most
the
regression
coefficients
require
a multivariate
relatively
several
response
variables
are
studied
simultaneously,
we
are
in
the
sphere
of
reRegression
methods
are
perhaps
the
most
widely
used
statistical
tools
in
data
analysis.
When
gression.
There
are
two
practical
concerns
regarding
this
general
multivariate
regression
model.
Regression
methods
are
perhaps
the
most
widely
used
statistical
tools
in
data
analysis.
When
data
are
available,
interpreting
simultaneously
a large
number
ofmay
the
regression
coefficients
can
several
response
variables
are
studied
simultaneously,
we
are
in
the
sphere
of
multivariate
reseveral
response
variables
are
studied
simultaneously,
we
are
in
the
sphere
of
multivariate
reRegression
methods
are
perhaps
the
most
widely
used
statistical
tools
in
data
analysis.
When
First,
the
accurate
estimation
of
all
the
regression
coefficients
require
a
relatively
large
become
unwieldy.
Achieving
parsimony
in
the
number
of
unknown
parameters
may
be
desirable
data
are
available,
interpreting
simultaneously
a
large
number
of
the
regression
coefficients
can
number
of
observations,
which
might
involve
some
practical
limitations.
Second,
even
if
the
gression.
There
are
two
practical
concerns
regarding
this
general
multivariate
regression
model.
several
response
variables
are
studied
simultaneously,
we
are
in
the
sphere
of
multivariate
reFirst,
the
accurate
estimation
of
all
the
regression
coefficients
may
require
a
relatively
large
several
response
variables
are
studied
simultaneously,
we
are
in
the
sphere
of
multivariate
rebecome
unwieldy.
Achieving
parsimony
in
the
number
of
unknown
parameters
may
be
desirable
gression.
There
are
two
practical
concerns
regarding
this
general
multivariate
regression
model.
gression.
There
are
two
practical
concerns
regarding
this
general
multivariate
regression
model.
several
response
variables
are
studied
simultaneously,
we
are
in
the
sphere
of
multivariate
renumber
of
observations,
which
might
involve
some
practical
limitations.
Second,
even
if
the
both
from
the
estimation
and
interpretation
points
of
view
of
multivariate
regression
analysis.
become
unwieldy.
Achieving
parsimony
in
the
number
of
unknown
parameters
may
be
desirable
data
are
available,
interpreting
simultaneously
a
large
number
of
the
regression
coefficients
can
First,
the
accurate
estimation
of
all
the
regression
coefficients
may
require
a
relatively
large
gression.
There
are
two
practical
concerns
regarding
this
general
multivariate
regression
model.
number
of
observations,
which
might
involve
some
practical
limitations.
Second,
even
if
the
gression.
There
are
two
practical
concerns
regarding
this
general
multivariate
regression
model.
both
from
the
estimation
and
interpretation
points
of
view
of
multivariate
regression
analysis.
First,
the
accurate
estimation
of
all
the
regression
coefficients
may
require
a
relatively
large
First,
the
accurate
estimation
of
all
the
regression
coefficients
may
require
a
relatively
large
gression.
There
are
two
practical
concerns
regarding
this
general
multivariate
regression
model.
data
are
available,
interpreting
simultaneously
a
large
number
of
the
regression
coefficients
can
We
address
these
concerns
by
assuming
that
the
regression
coefficient
matrix
is
of
lower
rank
both
from
the
estimation
and
interpretation
points
of
view
of
multivariate
regression
analysis.
become
unwieldy.
Achieving
parsimony
in
the
number
of
unknown
parameters
may
be
desirable
number
of
observations,
which
might
involve
some
practical
limitations.
Second,
even
if
the
First,
the
accurate
estimation
of
all
the
regression
coefficients
require
aa isrelatively
large
data
are
available,
interpreting
simultaneously
a large
number
ofmay
the regression
coefficients
can
First,
the
accurate
estimation
of
all
the
regression
coefficients
may
require
relatively
We
address
these concerns
by
assuming
the
regression
coefficient
matrix
ofbe
lower
rank
number
of
observations,
which
might
involve
some
practical
limitations.
Second,
even
if
the
number
of
observations,
which
might
involve
some
practical
limitations.
Second,
even
iflarge
the
First,
the
accurate
estimation
of
allvalue
the
regression
coefficients
may
require
amay
large
become
Achieving
parsimony
inthat
thepoints
number
of
unknown
parameters
desirable
and
itare
isunwieldy.
estimated
through
singular
decomposition
of an
appropriately
weighted
full-rank
We
address
these
concerns
by
assuming
that
the
regression
coefficient
matrix
isrelatively
ofbe
lower
rank
both
from
the
estimation
and
interpretation
of
view
of
multivariate
regression
analysis.
data
available,
interpreting
simultaneously
a
large
number
of
the
regression
coefficients
can
number
of
observations,
which
might
involve
some
practical
limitations.
Second,
even
if
the
become
unwieldy.
Achieving
parsimony
in
the
number
of
unknown
parameters
may
desirable
number
of
observations,
which
might
involve
some
practical
limitations.
Second,
even
if
the
and
it
is
estimated
through
singular
value
decomposition
of
an
appropriately
weighted
full-rank
data
are
available,
interpreting
simultaneously
a
large
number
of
the
regression
coefficients
can
data
are
available,
interpreting
simultaneously
a
large
number
of
the
regression
coefficients
can
number
of
observations,
which
might
involve
some
practical
limitations.
Second,
even
if
the
both
from
the
estimation
and
interpretation
points
of
view
of
multivariate
regression
analysis.
regression
coefficient
matrix.
and
it
is
estimated
through
singular
value
decomposition
of
an
appropriately
weighted
full-rank
We
address
these
concerns
by
assuming
that
the
regression
coefficient
matrix
is
of
lower
rank
become
unwieldy.
Achieving
parsimony
in
the
number
of
unknown
parameters
may
be
desirable
data
are
available,
interpreting
simultaneously
a
large
number
of
the
regression
coefficients
can
both
from
the
estimation
and
interpretation
points
of
view
of
multivariate
regression
analysis.
data
are
available,
interpreting
simultaneously
a
large
number
of
the
regression
coefficients
can
regression
coefficient
matrix.
become
unwieldy.
Achieving
parsimony
in
the
number
of
unknown
parameters
may
be
desirable
become
unwieldy.
Achieving
parsimony
in
the
number
of
unknown
parameters
may
be
desirable
data
are
available,
interpreting
simultaneously
a
large
number
of
the
regression
coefficients
can
We
address
these
concerns
by
assuming
that
the
regression
coefficient
matrix
is
of
lower
rank
regression
coefficient
matrix.
and
it
is
estimated
through
singular
value
decomposition
of
an
appropriately
weighted
full-rank
both
from
the
estimation
and
interpretation
points
of
view
of
multivariate
regression
analysis.
become
unwieldy.
Achieving
parsimony
the
number
of
unknown
parameters
may
desirable
We
address
these
concerns
by
assumingin
that
the
regression
coefficient
matrix
is ofbe
lower
rank
become
Achieving
parsimony
in
the
number
unknown
parameters
may
be
desirable
both
the
estimation
and
interpretation
of
view
of
multivariate
regression
analysis.
both
from
the
estimation
and
interpretation
points
of of
view
of
multivariate
regression
analysis.
become
unwieldy.
Achieving
parsimony
inthat
thepoints
number
of
unknown
parameters
may
be
desirable
and
itfrom
isunwieldy.
estimated
through
singular
value
decomposition
of an
appropriately
weighted
full-rank
regression
coefficient
matrix.
We
address
these
concerns
by
assuming
the
regression
coefficient
matrix
is
of
lower
rank
both
from
the
estimation
and
interpretation
points
of
view
of
multivariate
regression
analysis.
1.1
Multivariate
Linear
Regression
Model
and
Least
Squares
Estimaand
it
is
estimated
through
singular
value
decomposition
of
an
appropriately
weighted
full-rank
both
from
the
estimation
and
points
of
of
multivariate
regression
analysis.
We
address
these
concerns
by
assuming
that
the
regression
coefficient
is
rank
We
address
these
concerns
by interpretation
assuming
that
the Model
regression
coefficient
matrix
is of
of lower
lower
rank
both
from
the
estimation
and
interpretation
points
of view
view
of
multivariate
regression
analysis.
1.1
Multivariate
Linear
Regression
and
Leastmatrix
Squares
Estimaregression
coefficient
matrix.
and
it
is
estimated
through
singular
value
decomposition
of
an
appropriately
weighted
full-rank
We
address
these
concerns
by
assuming
that
the
regression
coefficient
matrix
is
of
lower
rank
1.1
Multivariate
Linear
Regression
Model
and
Least
Squares
Estimaregression
coefficient
matrix.
We
address
these
concerns
by
assuming
that
the
regression
coefficient
matrix
is
of
lower
rank
tor
and
it
is
estimated
through
singular
value
decomposition
of
an
appropriately
weighted
full-rank
and
it
is
estimated
through
singular
value
decomposition
of
an
appropriately
weighted
full-rank
We
address
these
concerns
by
assuming
that
the
regression
coefficient
matrix
is
of
lower
rank
tor
regression
coefficient
matrix.
and
it
is
estimated
through
singular
value
decomposition
of
an
appropriately
weighted
full-rank
1.1
Multivariate
Linear
Regression
Model
and
Least
Squares
Estimaand
it
is
estimated
through
singular
value
decomposition
of
an
appropriately
weighted
full-rank
tor
regression
coefficient
matrix.
regression
coefficient
matrix.
and
it
is
estimated
through
singular
value
decomposition
of
an
appropriately
weighted
full-rank
We
consider
the
general
multivariate
linear
model,
1.1
Multivariate
Linear
Regression
and Least Squares Estimaregression
coefficient
matrix.
regression
coefficient
matrix.
tor
We
consider
the general
multivariate
linear model,Model
1.1
Multivariate
Linear
Regression
regression
coefficient
matrix.
We
consider
the general
multivariate
linear model,Model and Least Squares Estimator
1.1
Multivariate
Linear
Model
Yk =Regression
CX
k = 1, .and
. . , T, Least
(1)
tor
1.1
Multivariate
Linear
Regression
Model
and
Least Squares
Squares EstimaEstimak + model,
k,
We consider
the general Linear
multivariate
linear
1.1
Multivariate
Model
Squares
Yk =Regression
CX
k = 1, .and
. . , T, Least
(1)
k + k ,
1.1
Multivariate
Linear
Regression
Model
and
Least
Squares EstimaEstimator
1.1
Multivariate
Linear
Regression
Model
and
Least
EstimaCX
k = 1, .and
. . , T, Least Squares
(1)
Yk =Regression
We consider
the
general
multivariate
linear
k + model,
k,
tor
1.1
Multivariate
Linear
Model
Squares
Estimator

We
consider
the
general
multivariate
linear
model,
m
×k 1+ vector
of kresponse
variables,
Xk = (x1k , . . . , xnk
)
where Y
tor
k = (y1k , . . . , ymk ) isYkan
=
CX

,
=
1,
.
.
.
,
T,
(1)
tor
k
=
(y
,
.
.
.
,
y
)
is
an
m
×
1
vector
of
response
variables,
X
=
(x
,
.
.
.
,
x
where
Y
We
consider
the
multivariate
linear
model,
k
1kgeneralmk
k
1k
nk )
tor

We
multivariate
linear
model,
is
anconsider
nY×
1 the
vector
variables,
coefficient
matrix,
(y1kgeneral
, . .of
. , ypredictor
) isYkan
m
×k 1+ vector
of km
response
Xk = (x1k
, . . . , x and
)
where
=
CX
C
=×1,n. . regression
. , variables,
T,
(1)
k =
mk
k , is an
We
general
multivariate
linear
model,
is
anconsider
n×
1 the
vector
variables,
coefficient
matrix, nk
and
CX
=×1,n. . regression
. , T,
(1)
We
the
general
multivariate
linear
model,
k + C
k , is an km
 of predictor
 Yk =
We
consider
the
general
multivariate
linear
model,
=consider
(
,
.
.
.
,

)
is
the
m
×
1
vector
of
random
errors,
with
mean
vector
E(
)
=
0
We
is
n
×
1
vector
of
predictor
variables,
C
is
an
m
×
n
regression
coefficient
matrix,
and
=
(y
,
.
.
.
,
y
)
is
an
m
×
1
vector
of
response
variables,
X
=
(x
,
.
.
.
,
x
)
where
Y
k an
1k
mk
k
k
1k
mk
k
1k
nk

general
multivariate
linear
CX
+ofmodel,
,,
k errors,
=
1,
.. .. ..with
,, T,
(1)
(Y1k , =
. . .the
, mk
the
×kkan
1=
vector
mean vector
and
k =consider
krandom
m Y
k ), .=
=
CX
=
1,
T,
(1)
Y
(y
, ))Cov(
..of
.is
, ypredictor
)=
m
×kkk×1+
vector
of k
response
variables,
Xkk are
= E(
(x
. . ,00x
)
where
k
k , .1
1k
mk
1k
nk
=
CX
+

,
k
=
1,
.
.
.
,
T,
(1)
Y
)
Σ
,
an
m
m
positive-definite
matrix.
The
assumed
to
be
covariance
matrix
= (
.
.
,

is
the
×
1
vector
of
random
errors,
with
mean
vector
E(
)
=
where
is
nY1k
×
vector
variables,
C
is
an
m
×
n
regression
coefficient
matrix,
and
mis
k
k
k

k an
mk
k
(y1k , Cov(
.. . , ymk
is
an
m
×k×1+
vector
of k
response
Xkk are
= (x
)
=
CX
kkpositive-definite
,,
=
1,
.. .. ..matrix.
,, variables,
T,
(1)
Y
k =
1k , . . . , x
nk
Σ
,
an
m
m
The
assumed
to
be
covariance
matrix
k
k ) )=

=
CX
+
k
=
1,
T,
(1)
Y
k
k
is
an
n
×
1
vector
of
predictor
variables,
C
is
an
m
×
n
regression
coefficient
matrix,
and
m
CX
C
, is T
km
=×1,n
. . regression
.with
, variables,
T, mean
(1)
independent
different
assume
that
vector
observations
are
) )=
ΣY×
,1=
an
m×k×1+m
matrix.
The
available,
areE(
assumed
tonk
be
covariance
= (
, .1matrix
. .for
, 1k
the
vector
ofvector
random
errors,
vector
0x
k an
kshall
kpositive-definite
kk.
k
1k
mk
k ),and
(y
,, )Cov(
....of
..is,, yypredictor
is
an
m
of
response
X
(x
..=
.. .. ,,define
))
where
We
k =
mk
k =
1k
is
nY
×
vector
variables,
an
coefficient
matrix,
and

independent
for
different
k.
We
shall
assume
that
T
vector
observations
are
available,
and
define
=
(y
)
is
an
m
×
1
vector
of
response
variables,
X
=
(x
,
x
where
Y
k
mk
k
1k
nk
(y
,×)Cov(
..T
.is, data
ypredictor
)=
is
an
massume
××1m
ofas
response
variables,
Xavailable,
==
(x
..=
....,define
)].
where
Y1k
=m(
.1matrix
. .for
, 1k
the
×shall
1variables,
vector
ofvector
random
errors,
with
mean
vector
E(
0,x
and
k an
m
kT, =
1k
mk
kX
1k
nk
mk
k1),,and
the
×
and
n
matrices,
respectively,
Y
=
[Y
,
.
.
.
,
Y
]
and
[X
,
X
independent
different
k.
We
that
T
vector
observations
are
)
Σ
,
an
m
positive-definite
matrix.
The
are
assumed
to
be
covariance
1
T
T
k
k

is
n
×
vector
of
C
is
an
m
×
n
regression
coefficient
matrix,
and
=
(y
,
.
.
.
,
y
)
is
an
m
×
1
vector
of
response
variables,
X
=
(x
.
.
.
,
x
))].
where
Y
k
mk
k
1k
nk
=m(
.1and
. .vector
, 1k
the
×an
1variables,
vector
ofvector
random
errors,
with
mean
vector
E(
k an
(y
,,×)Cov(
.. ..T
..is,, data
yypredictor
))=
is
m
×
1
of
response
variables,
X
=
(x
......,,0,x
where
Y
m
1k
mk
k1),,..=
the
×
n
matrices,
respectively,
as
Y
=
[Y
,
.
.
.
,
Y
]
and
=
[X
X
is
n
×
of
C
is
an
m
×
n
regression
coefficient
matrix,
and
kT, =
1k
mk
kX
1k
nk
1
T
T
is
an
n
×
1
vector
of
predictor
variables,
C
is
an
m
×
n
regression
coefficient
matrix,
and
=
(y
is
an
m
×
1
vector
of
response
variables,
X
=
(x
,
.
x
)].
where
Y
)
Σ
,
an
m
×
m
positive-definite
matrix.
The

are
assumed
to
be
covariance
matrix
 T data
kT, .1and
1k
mk
1k
nk
k

k
the
m
×
n
×
matrices,
respectively,
as
Y
=
[Y
,
.
.
.
,
Y
]
and
X
=
[X
,
.
.
.
,
X
independent
for
different
k.
We
shall
assume
that
T
vector
observations
are
available,
and
define
1
T
1
T
=
(
.
.
,

)
is
the
m
×
1
vector
of
random
errors,
with
mean
vector
E(
)
=
0

is
an
n
×
vector
of
predictor
variables,
C
is
an
m
×
n
regression
coefficient
matrix,
and
1877-0509
2017
The
Authors.
by
Elsevier
1k
mk )
k ) = 0toand
Σ×
,1variables,
an
mB.V.
×m
positive-definite
matrix.
The
k areE(
assumed
be
covariance
matrix
Cov(
is
n
×
1
vector
of
predictor
C
is
an
m
×
n
regression
coefficient
matrix,
 Published
k) =
=
(
,
.
.
.
,

is
the
m
vector
of
random
errors,
with
mean
vector
kkk ©an
1k
mk
k
=
(
,
.
.
.
,

)
is
the
m
×
1
vector
of
random
errors,
with
mean
vector
E(
)
=
0
is
an
n
×
1
vector
of
predictor
variables,
C
is
an
m
×
n
regression
coefficient
matrix,
and
independent
for
different
k.
We
shall
assume
that
T
vector
observations
are
available,
and
define
Peer-review
under
responsibility
of the
committee
of the
International
Conference
the
m(×1k
T
and
ndifferent
×)Cov(
data
matrices,
respectively,
Y =observations
[Ywith
Yon
] Computational
and
X=
[XkkScience
, .=. . define
Xand
].
1 , . . . ,mean
TThe
1)and
Tbe
 T
))scientific
=m
Σ
,,1
an
m
×
m
positive-definite
matrix.
available,
assumed
covariance
.. ..for
,, mk
the
×shall
vector
of
random
errors,
vector
E(
00,to
kk =
kk.
k are
1k ,, ..matrix
mk
independent
We
assume
that
T as
vector
are
the
vector
of
random
errors,
mean
vector
E(
 is
Σ
m
×
matrix.
are
assumed
be
covariance
matrix
k
k
10.1016/j.procs.2017.05.184
)=
=m
Σ×
,1
an
m
×m
m
positive-definite
matrix.
The
available,
are=
assumed
to
covariance
Cov(
=m(
(×1k
. .for
, mk
is data
the
m
×
1 an
vector
of positive-definite
random
errors,
with
vector
E(
=. . define
0,to
and
k =
the
and
ndifferent
×))Cov(
Tis
matrices,
respectively,
as
Y =observations
[Ywith
YTThe
] are
and
[Xkk1))and
, .=
Xand
].
kk.

kX
1kT, .matrix
mk
1 , . . . ,mean
Tbe
independent
We
shall
assume
that
T
vector
)
=
Σ
,
an
m
×
m
positive-definite
matrix.
The

are
assumed
to
be
covariance
matrix
Cov(
k

k
the
m × T matrix
and
ndifferent
×Cov(
T data
matrices,
respectively,
as
Y =observations
[Ymatrix.
] are
andavailable,
=assumed
[X1and
, . . . define
,to
XTbe
].
)) =
Σ
,, an
m
×
m
positive-definite
are
covariance
1 , . . . , YTThe
independent
for
We
shall
assume
that
T
vector
kk.

kX
independent
for
different
k.
We
shall
assume
that
T
vector
observations
are
available,
and
define
=
Σ
an
m
×
m
positive-definite
matrix.
The

are
assumed
to
be
covariance
matrix
Cov(
kk. We

kX = [X and
the
m
×
T
and
n
×
T
data
matrices,
respectively,
as
Y
=
[Y
and
,, .. .. .. define
,, X
independent
for
different
shall
assume
that
T
vector
observations
available,
1 ,, .. .. .. ,, Y
T ]] are
1and
T ].
independent
for
different
k.
We
shall
assume
that
T
vector
observations
are
available,
define
the
m
×
T
and
n
×
T
data
matrices,
respectively,
as
Y
=
[Y
Y
and
X
=
[X
X
the m × T and
× T data
matrices,
respectively,
Y =observations
[Y11 , . . . , YTT ] are
andavailable,
X = [X11and
, . . . define
, XTT ].
].
independent
for n
different
k. We
shall assume
that T as
vector
the
m
×
T
and
n
×
T
data
matrices,
respectively,
as
Y
=
[Y
,
.
.
.
,
Y
]
and
X
=
[X
,
.
.
.
, XTT ].
1
T
1
the
m
×
T
and
n
×
T
data
matrices,
respectively,
as
Y
=
[Y
,
.
.
.
,
Y
]
and
X
=
[X
,
.
.
.
the m × T and n × T data matrices, respectively, as Y = [Y1 , . . . , YT ] and X = [X1 , . . . ,, X
X ].
].

1020	

Separable Covariance Matrices Raja Velu et al. / Procedia Computer Science 108C (2017) 1019–1029 Velu and Herman

We assume that m+n ≤ T and that X is of full rank n = rank(X) < T . We arrange the error
vectors k , k = 1, . . . , T , into an m × T matrix  = [1 , 2 , . . . , T ], similar to the data matrices.
We also will consider the errors arranged into an mT × 1 vector e = ((1) , . . . , (m) ) , where
(j) = (j1 , . . . , jT ) is the vector of errors corresponding to the jth response variable in the
regression equation (1). Then we have that
E(e) = 0,

Cov(e) = Σ ⊗ IT ,

(2)

where A ⊗ B denotes the Kronecker product of two matrices A and B. That is, if A is an m × n
matrix and B is a p × q matrix, then the Kronecker product A ⊗ B is the mp × nq matrix with
block elements A ⊗ B = (aij B), where aij is the (i, j)th element of A. We will observe that
this Kronecker structure of the error covariance matrix leads to an elegant, one-step solution
in obtaining the efficient estimates.
The unknown parameters in model (1) can be estimated by the method of least squares.
Rewrite the model (1) as
Y = CX + .
(3)
The least squares criterion is given as
e e = tr( ) = tr [(Y − CX)(Y − CX) ] = tr


T


k=1 k k



,

(4)

where tr(A) denotes the trace of a square matrix A; that is, the sum of the diagonal elements
of A. The least squares (LS) estimate of C is the value C̃ that minimizes the criterion in (4)
and yields a unique solution for C as
C̃ = Y X  (XX  )−1 =




1
TYX




1
T XX

−1

.

(5)

It is noted that the rows of the matrix C are estimated in (5) by the least squares regression
of each response variable on the predictor variables and therefore the covariances among the
response variables do not enter into the estimation. Let  = Y − CX. An unbiased estimator
of Σ is obtained as
T
1
1 
ˆˆ =
Σ =
ˆk ˆk .
(6)
T −n
T −n
k=1

We summarize the main results on the LS estimate C̃ in the following.
Result 1.1 For the model (1) under the normality assumption on the k , the least squares
estimator C̃ = Y X  (XX  )−1 is the same as the maximum likelihood estimator of C, and the
distribution of the least squares estimator C̃ is such that
vec(C̃  ) ∼ N (vec(C  ), Σ ⊗ (XX  )−1 ).

(7)


Note, in particular, that this result implies that the jth row of C̃, C̃(j)
, which is the vector of least
squares estimates of regression coefficients for the jth response variable, has the distribution
C̃(j) ∼ N (C(j) , σjj (XX  )−1 ).
The inference on the elements of the matrix C can be made using the result in (7). In
practice, because Σ is unknown, a reasonable estimator such as the unbiased estimator in (6)
is substituted for the covariance matrix in (7).

2

	

Separable Covariance Matrices Raja Velu et al. / Procedia Computer Science 108C (2017) 1019–1029 Velu and Herman

2

The Basic Reduced-Rank Model and Background

In many practical situations, there is a need to reduce the number of parameters in model (1)
and we approach this problem through the assumption of lower rank of the matrix C in model
(1). More formally, in the model Yk = CXk + k we assume that
rank(C) = r ≤ min(m, n).

(8)

The rank condition (8) has a practical implication. The matrix C can be written as a product
of two lower dimensional matrices that are of full ranks. Specifically, C can be expressed as
C = AB

(9)

where A is of dimension m × r and B is of dimension r × n, but both have rank r. Note that
the r columns of the left matrix factor A in (9) can be viewed as a basis for the column space
of C, while the r rows of B form a basis for the row space. The model (8) can then be written
as
Yk = A(BXk ) + k ,
k = 1, . . . , T,
(10)
where BXk is of reduced dimension with only r components. A practical use of (10) is that
the r linear combinations of the predictor variables Xk are sufficient to model the variation in
the response variables Yk . Hence, there is a gain in simplicity and interpretation through the
reduced-rank regression modeling. Various applications of the basic reduced-rank model are
given in Reinsel and Velu (1998)[8].

2.1

Estimation of Parameters in the Reduced-Rank Model

The reduced-rank regression model (8) has the following parameters: the matrix A which is of
dimension m × r, the matrix B which is of dimension r × n. To obtain a particular unique set
of parameter values for A and B from the elements of C, certain normalization conditions are
typically imposed.
As we shall see shortly, reduced-rank estimation will be obtained as a certain reduced-rank
approximation of the full-rank least squares estimate of the regression coefficient matrix, via
Eckart–Young Theorem (Eckart and Young, 1936)[2], which presents the tools for approximating
a full-rank matrix by a matrix of lower rank. The solution is related to the singular value
decomposition of the full-rank matrix.
Estimation of A and B in (10) is based on the following result (Brillinger, 1981, [1], Section
10.2) which essentially uses Theorem 2.1.
Theorem 2.1 For any positive-definite matrix Γ, an m × r matrix A and r × n matrix B, for
r ≤ min(m, n), which minimize
tr{E[Γ1/2 (Y − ABX)(Y − ABX) Γ1/2 ]}

(11)

are given by
A(r) = Γ−1/2 [V1 , . . . , Vr ] = Γ−1/2 V,

B (r) = V  Γ1/2 Σyx Σ−1
xx ,

(12)

where V = [V1 , . . . , Vr ] and Vj is the (normalized) eigenvector that corresponds to the jth
1/2
largest eigenvalue λ2j of the matrix Γ1/2 Σyx Σ−1
(j = 1, 2, . . . , r).
xx Σxy Γ

3

1021

1022	

Separable Covariance Matrices Raja Velu et al. / Procedia Computer Science 108C (2017) 1019–1029 Velu and Herman

Proof: See Reinsel and Velu (1998)[8], p 28.
The elements of the reduced-rank approximation of the matrix C are given as
 r



−1
Vj Vj Γ1/2 Σyx Σ−1
C (r) = A(r) B (r) = Γ−1/2
xx = PΓ Σyx Σxx

(13)

j=1

for any Γ, but need not be symmetric. When r = m,
where
m PΓ is an idempotent matrix
(r)
V
V
=
I
and
therefore,
C
reduces to the full-rank coefficient matrix, C = Σyx Σ−1
m
xx .
j=1 j j
Rao (1979)[7] has shown a stronger result that the solution for A and B in Theorem 2.1 also
simultaneously minimizes all the eigenvalues of the matrix in (11) provided that Γ = Σ−1
yy .

3

Reduced–Rank Model with Autogressive Errors

The multivariate reduced-rank regression model assumes that the error terms in the model are
independent. When the data are in the form of time series the assumption of serial independence
of errors is often not appropriate. In these circumstances, it is important to account for serial
correlation in the errors. See Velu and Reinsel (1987)[9].
The assumption we make on the errors is that they follow a multivariate autoregressive
process. We consider the estimation problem with the autoregressive error assumption in the
time domain. As an example, we present an analysis of some macroeconomic data of the United
Kingdom. Consider the multivariate regression model for the vector time series {Yt },
Yt = CXt + Ut ,

t = 1, 2, . . . , T,

(14)

where Yt is an m × 1 vector of response variables, Xt is an n × 1 vector of predictor variables,
and C is an m × n regression coefficient matrix. As in Section 2, we assume that C is of reduced
rank. We assume further that the error terms Ut satisfy the stochastic difference equation
Ut = ΦUt−1 + t

(15)

where U0 is taken to be fixed. Here the t are independently distributed random vectors with
mean zero and positive-definite covariance matrix Σ , and Φ = (φij ) is an m × m matrix of
unknown parameters, with all eigenvalues of Φ less than one in absolute value. We can have a
more general autoregressive model for Ut , but for illustration for we consider only AR(1); but
results for AR(2) model, are available. When the regression matrix C is assumed to be of lower
rank r, following the discussion in Section 2, write C = AB with appropriate normalization
conditions. Estimation of A and B are outlined in the next section.

3.1

Maximum Likelihood Estimators

We now consider an estimation procedure that will yield an efficient estimator. From the model
(14) and (15), we have
Y − ΦY −1 = CX − ΦCX −1 + 
(16)
where Y −1 = [Y0 , . . . , YT −1 ] and X −1 = [X0 , . . . , XT −1 ] are the lagged data matrices. We first
briefly consider estimation of parameters C and Φ in the full-rank version of the model, that
is, without the rank restriction on C.
4

	

Separable Covariance Matrices Raja Velu et al. / Procedia Computer Science 108C (2017) 1019–1029 Velu and Herman
1
1 
We consider minimization of the criterion ST (C, Φ) = 2T
tr( Γ) = 2T
e (Γ ⊗ IT )e, where

−1
 = Y − ΦY −1 − CX + ΦCX −1 , e = vec( ), and the choice Γ = Σ is taken. From (16) we
have
vec(Y ∗ ) = X ∗ vec(C  ) + vec( )

where Y ∗ = Y − ΦY −1 and X ∗ = (Im ⊗ X  ) − (Φ ⊗ X −1 ) is an mT × mn matrix. It follows
from standard results for the linear regression model that, for given values (estimates) Φ̃ and
Σ̃ of Φ and Σ , the full-rank estimate of C is obtained as the (estimated) generalized least
squares (GLS) estimate, given by
∗

∗

vec(C̃  ) = G̃−1 X̃ (Σ̃−1
 ⊗ IT )vec(Ỹ )/T,
∗

∗

∗

(17)

∗



where G̃ = X̃ (Σ̃−1
 ⊗IT )X̃ /T , with Ỹ = Y − Φ̃Y −1 and X̃ = (Im ⊗X )−(Φ̃⊗X −1 ). The
1

−1
ˆ
covariance matrix of the full-rank estimate is estimated by Cov[vec(
C̃ )] = T G̃ . Conversely,
for given value (estimate) C̃ of C, the corresponding estimate of Φ to minimize the above


criterion ST (C, Φ) is given by Φ̃ = Ũ Ũ −1 (Ũ −1 Ũ −1 )−1 , where Ũ = Y − C̃X = [Ũ1 , . . . , ŨT ],
Ũ −1 = [Ũ0 , . . . , ŨT −1 ], with Ũt = Yt − C̃Xt , t = 1, . . . , T , and Σ is estimated as Σ̃ = ˜˜ /T ,
where ˜ = Ũ − Φ̃Ũ −1 . The estimation scheme is thus performed most conveniently by an
iterative procedure which alternates between estimation of C through (17), for given estimate
Φ̃, and estimation of Φ as indicated for given estimate C̃. The most common starting value is
the least squares estimate of C, given in (12), that ignores the AR(1) errors structure.
Now for estimation of the reduced-rank model with C = AB, let θ = (α , β  , φ ) , where
α = vec(A ), β = vec(B  ), and φ = vec(Φ ).

Theorem 3.1 Consider the model (14) and (15) under the stated assumptions, with C = AB,
where A and B satisfy the normalization conditions with h(θ) = 0 denote the r2 × 1 vector of
normalization conditions. The first r(r +1)/2 components of h(θ) are of the form αi Σ−1
 αj −δij ,
i ≤ j, where αi denotes the ith column of the matrix A and δij = 1 for i = j and 0 otherwise,
and the remaining r(r − 1)/2 elements are βi Σxx βj , i < j, where βi denotes the ith row of B.
Assume the limits
T
1
Xt Xt = Σxx = Γx (0),
T →∞ T
t=1

lim

T
1
Xt−1 Xt = Γx (1)
T →∞ T
t=2

lim

exist almost surely, and define
−1

 −1
 −1
G = (Σ−1
 ⊗ Γx (0)) − (Σ Φ ⊗ Γx (1) )− (Φ Σ ⊗ Γx (1)) + (Φ Σ Φ ⊗ Γx (0))
 −1
= Cov[vec(Xt t Σ−1
 − Xt−1 t Σ Φ)].

Let θ̂ denote the estimator that absolutely minimizes
√ ST (C, Φ) subject to normalization conditions. Then as T → ∞, θ̂ → θ almost surely and T (θ̂ − θ) has a multivariate normal distribution with zero mean vector 
and covariance
W = (Bθ +Hθ Hθ )−1Bθ (Bθ +Hθ Hθ )−1 , where
 matrix

⊗
B
A ⊗ In
0
G
0
I
m
Bθ = M 
M, M =
, Hθ = {(∂hj (θ)/∂θi )} is
0
0
I
0 Σ−1
⊗
Σ
⊗
Im
u
m

the matrix of first derivatives of the constraints and is of order r(m + n) + m2 × r2 , and
Σu = ΦΣu Φ + Σ , which can be obtained from vec(Σu ) = [Im2 − Φ ⊗ Φ]−1 vec(Σ ) (see Reinsel
and Velu (1998)[8], p 99).
5

1023

1024	

Separable Covariance Matrices Raja Velu et al. / Procedia Computer Science 108C (2017) 1019–1029 Velu and Herman

3.2

Alternative Estimators

It is important to note that because of the structure of the G matrix, which is not in Kronecker
form, the solution gets iterative. If there is a structure to the true parameter matrix, we should
be able to detect it by using the full-rank estimator directly with simpler structure as well. The
alternative estimators we propose below is essentially based on this simple concept. This will
also clarify the need for Kronecker’s approximation of G, which is the main focus of this paper.
To motivate the alternative estimators we first recall the solution to the reduced-rank regression problem for the single regressor case under independent errors. The sample version of
the criterion which led to the solution reduces to finding A and B by minimizing
tr[Γ1/2 (C̃ − AB)Σ̂xx (C̃ − AB) Γ1/2 ] = [vec{(C̃ − AB) }] (Γ ⊗ Σ̂xx )vec{(C̃ − AB) },

(18)

where C̃ = Σ̂yx Σ̂−1
xx is the full-rank least squares estimator. Using the Eckhart–Young Theorem,
the problem is solved explicitly. This is not the case with an AR(1) error model since the fullrank estimate of C, as displayed in (17) of does not have an analytically simple form.
For given Φ and Σ (≡ Γ−1 ), minimization of the efficient criterion (18) reduces to the
problem of finding A and B such that
[vec{(C̃ − AB) }] G̃ vec{(C̃ − AB) }

(19)

is minimized, where C̃ is the corresponding full-rank GLS estimate, vec(C̃  ) = G̃−1 X ∗ (Σ̃−1
 ⊗
∗
IT )vec(Y ∗ )/T , and G̃ = X ∗ (Σ̃−1
 ⊗ IT )X /T . Now comparison of this latter expression with
(18) indicates that an explicit solution via the Eckhart–Young Theorem is not possible unless the
matrix G̃ can be written as the Kronecker product of two matrices of appropriate dimensions.
This can happen only under rather special circumstances, for instance, when Φ = ρI. Hence,
the solution is not simple as in the standard independent errors case.
Motivated by a desire to obtain initial estimates for the reduced-rank model (for various
choices of the rank r) which are relatively easy to compute in terms of the full-rank estimator C̃
of model (14)–(15). Velu and Reinsel (1987) [9] considered an alternate preliminary estimator.
This estimator essentially is chosen to minimize (19), but with G̃ replaced by (Γ ⊗ XX  /T ),
the weights matrix that appears in (18), the inefficient least squares criterion, with Γ = Σ−1
u .
This proposed estimation procedure, it should be kept in mind, does not have any overall
optimality properties, but is motivated by the standard model (see Section 2) and computational
convenience.
We can obtain the alternative estimators Ã∗ and B̃ ∗ using the Eckhart–Young Theorem.
1/2
−1/2

From the results in Section 2, it follows that Ã∗ = Σ̃u Ṽ(r) and B̃ ∗ = Ṽ(r)
Σ̃u C̃, where
−1/2

−1/2

Ṽ(r) = [Ṽ1 , . . . , Ṽr ] and the Ṽj are normalized eigenvectors of the matrix Σ̃u C̃ Σ̂xx C̃  Σ̃u
corresponding to its r largest eigenvalues. This involves as easily noticed one-step eigendecomposition unlike the iterative procedure adapted for the calculation of efficient estimator.

4

Kronecker Approximation

Statistical models of multivariate data collected over time assume generally that the covariance
are separable functions of the cross-sectional variance and the temporal variance. As shown
in Section 3, separable covariance matrices lead to fast computation of the parameters and
may be especially useful for dimension-reduction aspects of large data sets. We describe the
nearest Kronecker product approximation of a matrix of general form, in the Frobenius norm
suggested by Van Loan and Pitsianis (1993)[4]. The algorithm is simple to use and the solution
6

	

Separable Covariance Matrices Raja Velu et al. / Procedia Computer Science 108C (2017) 1019–1029Velu and Herman

preserves the positive definiteness and symmetry required for the covariance matrices as in the
approximation proposed in Section 3.
Let G be a covariance matrix of dimension mn × mn; as illustrated in Section 3, this could
be a sum of several Kronecker products or simply a matrix that is not in the form of a Kronecker
product. The solution to the approximation of G by G1 ⊗ G2 , i.e. to minimize:
G − G1 ⊗ G2 F = R(G) − Vec(G1 ) ⊗ Vec(G2 )F

(20)

where R(G) is a matrix that is rearranged from the columns and rows of G so that
GF = R(G)F . The rearrangement function is R(G) = [Vec (G11 ), Vec (Gm1 ), Vec (G21 ),
· · · , Vec (Gmm )], where Gkl is the k, lth n × n block of G. The rearranged matrix R(G) is
m2 × n2 and can be rectangular.
It is shown that minimizing (20) is essentially reduced to a rank one approximation of
the rectangular matrix, R(G). By singular value decomposition of R(G), U  R(G)V = ∆ =
2
2
2
2
diag(δ1 , · · · , δq ), where U ∈ Rn ×n and V ∈ Rm ×m are orthogonal matrices and the singular
values are arranged in descending order and are positive; observe q = min{m2 , n2 }. The solution
to (20) is given by,


Vec(G1 ) = δ1 U1 and Vec(G2 ) = δ1 V1
(21)
where U1 and V1 are the first columns of U and V matrices. Application of this method
in various statistical contexts are given in Genton (2007)[3], Mitchell, Genton and Gumpertz
(2006)[6], and Werner, Jansson and Stoika (2008) [5]. Genton (2007)[3] defines an error index
to measure the error in the approximation as:

q
δi2
G − G1 ⊗ G2 F
= i=2
(22)
KG (G1 , G2 ) =
q
2
GF
i=1 δi
The index is bounded below by zero and above by

5


1 − 1/q.

Example on U.K. Economy – Basic Data and Their Descriptions

To illustrate the procedures, we consider the macroeconomic data of the United Kingdom which
has been previously analyzed by Velu and Reinsel (1987) [9]. Quarterly observations, starting
from the first quarter of 1948 to the last quarter of 1956, are used in the following analysis. The
endogenous (response) variables are y1 = index of industrial production, y2 = consumption of
food, drinks and tobacco at constant prices, y3 = total unemployment, y4 = index of volume
of total imports, and y5 = index of volume of total exports, and the exogenous (predictor)
variable are x1 = total civilian labor force, x2 = index of weekly wage rates, x3 = price index
of total imports, x4 = price index of total exports, and x5 = price index of total consumption.
The relationship between these two sets of variables is taken to reflect the demand side of the
macrosystem of the economy of the United Kingdom. Time series plots of the resulting data are
given in Figure 1. Initially the least squares regression of Yt = (y1t , y2t , y3t , y4t , y5t ) on Xt =
(x1t , x2t , x3t , x4t , x5t ) was performed without taking into account any autocorrelation structure
on the errors. In order to get a rough view on the rank of the regression coefficient matrix,
−1/2
1/2
the singular values of Σ̃uu C̃ Σxx are calculated and they indicate a possible deficiency in the
rank of C, matrix. However, the residuals resulting from the least squares regression indicated
7

1025

1026	

Separable Covariance Matrices Raja Velu et al. / Procedia Computer Science 108C (2017) 1019–1029 Velu and Herman

Figure 1: Quarterly macroeconomic data for the U.K.
a strong serial correlation in the errors. Therefore, we consider incorporating an autoregressive
errors structure in the analysis. A first-order autoregressive error model is constructed for the
purpose of illustration, although one may argue towards a second-order model. A formal test
for determining the rank of the matrix is provided in Velu and Reinsel (2007)

5.1

Computational Results

In order to study the effectiveness of the Kronecker’s approximation of G in (20), we compute
the trace and the determinant of the error covariance matrix, Σuu for various choices of C̃ and
for ranks, r = 1, 2, · · · , 5. The choices we consider here are: least Squares Estimator, C̃LS in (5),
that assumes that errors are independent and efficient Estimator, C̃Eff in (17), that accounts
for the autocorrelated errors.
The reduced-rank estimators, as we may recall are the projections of these matrices to lower
dimensions with different weight matrices. These are as stated in Theorem 2.1, Theorem 3.1 and
the alternative calculations based on the Kronecker’s product structure. In the first alternative,
the weight matrix is taken to be, Σ̃−1
uu ⊗ Σ̃xx and in the second, the approximation that results
in G̃ ∼ G̃1 ⊗ G̃2 is used.
The difference between the various alternatives explored here lies in the choice of the estimators of the ‘C’ matrix and the estimators of the appropriate weight matrices. It is known that
the efficient estimators are obtained when all the model considerations are taken into account
and these estimators naturally employ the inverse of the variance of the efficient estimators
as the weighing matrix. If Γ1 and Γ2 are positive definite weight matrices, the reduced-rank
1/2
1/2
estimators of ‘C’ is obtained from the projection of Γ1 CΓ2 in the lower dimension; results
we present below refer to the following alternatives:
8

	

Separable Covariance Matrices Raja Velu et al. / Procedia Computer Science 108C (2017) 1019–1029 Velu and Herman

1. Least-Squares Estimator: C̃LS ; here Γ1 = Σ̂−l
uu ; Γ2 = Σ̂xx
2. Efficient Estimator: ĈEff : Here we consider three alternatives:
(a) Weight matrix, G, not in Kronecker-form
(b) Weight matrices: Γ1 = Σ̂−l
uu ; Γ2 = Σ̃xx
(c) Weight matrices: Γ1 = G̃1 ; Γ2 = G̃2
1/2

−1/2

Note that C̃ (r) = P C̃, where the projection matrix P = Γ1 Ṽr Ṽr Γ1
and Ṽ(r) is the m×r
1/2
1/2
matrix of eigenvectors corresponding to the first ‘r’ eigenvalues of Γ1 C̃Γ2 C̃  Γ1 .
It can be noted from Figure 1, many of the y and x series are non-stationary, but in the
regression set-up considered here that is not taken into account. Some select results are given
below:




5.36

C̃Eff

 1.24

= −5.54
 3.00
9.50

Φ̃Eff =



−0.18
−0.12
−0.25

0.15
−0.94

−0.89
−0.09
0.26
−0.50
−0.37

−0.33
−0.45
−0.31
0.39
0.80

−0.06
0.13
−0.35
−0.11
0.72

−0.12
−0.03
0.86
−0.27
−0.11

−0.42
−0.32
1.18
−0.13
−0.87

−0.14
−0.11
0.60
−0.08
−0.27

−0.30
−0.08
0.24
−0.49
0.37

0.27
0.17
−0.09
−0.05
1.30

−0.70
−0.34
0.37
−0.41
−1.61

9.76
 2.84
−14.28

4.97
5.89

2.84
2.97
−6.21
2.00
0.55

−14.28
−6.21
77.79
−10.27
−0.03

4.97
2.00
−10.27
14.12
8.93

−0.79
0.16
1.35
−1.45


−0.15 , G̃2 = 0.55

0.88
0.06
0.98
0.87

1.35
12.37
4.54
7.93
7.98

2.51
1.90
 1.38
0.55 


(2)
−1.04 , C̃Eff = −1.14

 0.72
1.39
8.26
0.37



0.17
0.08 
0.13  ,

−0.10
0.27

Σ̃Final
=




1.54
0.51 

−1.10
1.97 
−0.12



5.89
0.55 
−0.03

8.93
29.42

As discussed rank two approximation is taken to be optimal resulting in considerable reduction in the number of parameters. The resulting coefficient matrix from Theorem 3.1 is given
below:
The G̃-matrix is of dimension 25 × 25 and is not reproduced here. Now the results of the
Kronecker’s product; the G̃ matrix is decomposed into G̃1 ⊗ G̃2 and they are given below:




6.61

 0.20

G̃1 =  0.30
−1.42
−0.79

0.20
25.59
−0.05
−0.92
−1.45

0.30
−0.05
0.22
0.53
−0.15

−1.42
−0.92
0.53
2.40
0.06

The resulting rank(2) approximation of C matrix from

4.37 −0.62 0.10
 1.81 −0.24 0.06

(2)
C̃KR = 
−6.15 0.23 −0.55
 2.88 −0.65 −0.09
8.73 −0.43 0.72

0.55
4.54
4.82
5.57
3.56

0.88
7.93
5.57
7.60
5.74

0.87
7.98

3.56
5.74
5.33

this decomposition is given below:

−0.55 1.70
−0.23 0.63 

0.89 −0.48

−0.32 1.81 
−1.24 0.98
(2)

The approximation performs well compared to the efficient coefficient matrix, C̃Eff . The
effectiveness of this approximation crucially depends on how well G̃ is approximated by the
Kronecker product G̃1 ⊗ G̃2 . The eigenvalues of G̃ and the eigenvalues of G̃1 ⊗ G̃2 are virtually
indistinguishable (see Figure 2); Figure 2 contains these and the eigenvalues of Σ̂−1
uu ⊗ Σ̂xx and
Σ̂−1
 ⊗ Σ̂xx , the other weighing matrices used in various approximations.
Some comments are in order. It is clear from Figure 2 that G̃, the correct weighing matrix is
very well approximated by G̃1 ⊗ G̃2 . The distribution of eigenvalues is almost indistinguishable.
9

1027

1028	

Separable Covariance Matrices

Raja Velu et al. / Procedia Computer Science 108C (2017) 1019–1029

Velu and Herman

Figure 2: Eigen Plot.
But it is also clear that these two weighing alternatives exhibit higher variation than the other
−1
weighing matrices, Σ̃−1
uu ⊗ Σ̃xx and Σ̃ ⊗ Σ̃xx , that are used to arrive at the lower rank approximates of the coefficient matrix. To make the comparisons more meaningful we compute the
AIC-criterion (see Reinsel and Velu (1998) [8]) for various ranks of C̃ with alternative weighing
matrices. The AIC is defined as




∗ 2
AIC(r) = ln Σ(r)
c  + n
T

(23)

where n∗ , the number of independent parameters is defined as, n∗ = r(m + n − r), where ν is
the rank. Figure 3 provides the plot of AIC(r) for the efficient matrix for various alternative
−1
weighing matrices: AIC1 for Σ̃−1
uu ⊗ Σ̂xx , AIC2 for Σ̃c ⊗ Σ̃xx and AIC3 for G̃1 ⊗ G̃2 . The AIC
plot clearly indicates the weighing matrix based on Kronecker approximates yields uniformly
better results.

6

Future Work

In this paper, we have explored the approximation of a general larger covariance matrix by
a Kronecker product of two separable smaller covariance matrices, using the classical work of
Van Loan and Pitsianis (1993) [4]. The approximation works very well in a set-up where a
multivariate regression model with vector autogressive error of order 1. This can be extended
to higher order processes and as well as to situations where the covariance matrix is not of
Kronecker’s form (see for example, Chapter 7, Reinsel and Velu (1998) [8], seemingly unrelated
regression models with reduced ranks). In Van Loan and Pitsainis (1993) [4], (see Section 5.5)
that if G̃ matrix is the sum of Kronecker products (which is the case in our extractions) it can
be approximated by the Kronecker product of two 
matrices; each is a linear combination of
p
the matrices involved in G̃; more precisely, if G̃ = i=1 (Gi ⊗ Fi ) then G̃ ∼ (B̃ ⊗ C̃) where

10

	

Separable Covariance Matrices

Raja Velu et al. / Procedia Computer Science 108C (2017) 1019–1029
Velu and Herman

Figure 3: Eigen Plot.
B̃ = α1 G1 + · · · + αp Gp and C̃ = β1 F1 + · · · + βp Fp . Studying the properties of α’s and β’s
would be useful.

References
[1] D.R. Brillinger. Time Series: Data Analysis and Theory, expanded ed. Holden-Day: San Francisco,
1981.
[2] C. Eckart and G. Young. The approximation of one matrix by another of lower rank. Psychometrika,
1:211–218, 1936.
[3] M.C. Genton. Separable approximation of space-time covariance matrices. Environmetrics, 18:681–
695, 2007.
[4] C. Van Loan and N.P. Pitsianis. Approximation with Kronecker products, in: M.S. Moonen, G.H.
Golub (Eds.), Linear Algebra for Large Scale and Real Time Applications. Kluwer Publications,
Dordrecht, 1993.
[5] K. Werner, M. Jansson and P. Stoica. On estimation of covariance matrices with kronecker product
structure. IEEE Transactions on Signal Processing, 56:436–491, 2008.
[6] M.W. Mitchell, M.G. Genton and M.L. Gumpentz. A likelihood ratio test for separability of
covariance. Journal of Multivariate Analysis, 97:1025–1043, 2006.
[7] C.R. Rao. Separation theorems for singular values of matrices and their applications in multivariate
analysis. Journal of Multivariate Analysis, 9:362–377, 1979.
[8] G.C. Reinsel and R.P. Velu. Multivariate Reduced-Rank Regression: Theory and Applications.
Springer: New York, 1998.
[9] R.P. Velu and G.C. Reinsel. Reduced rank regression with autoregressive errors. Journal of Econometrics, 35:317–335, 1987.

11

1029

