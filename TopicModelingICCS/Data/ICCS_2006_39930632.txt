Error Analysis of a Monte Carlo Algorithm for
Computing Bilinear Forms of Matrix Powers
Ivan Dimov, Vassil Alexandrov, Simon Branford, and Christian Weihrauch
Centre for Advanced Computing and Emerging Technologies
School of Systems Engineering, The University of Reading
Whiteknights, PO Box 225, Reading, RG6 6AY, UK
{v.n.alexandrov, s.j.branford, i.t.dimov, c.weihrauch}@reading.ac.uk
Institute for Parallel Processing, Bulgarian Academy of Sciences
Acad. G. Bonchev 25 A, 1113 Soﬁa, Bulgaria
ivdimov@bas.bg

Abstract. In this paper we present error analysis for a Monte Carlo
algorithm for evaluating bilinear forms of matrix powers. An almost Optimal Monte Carlo (MAO) algorithm for solving this problem is formulated. Results for the structure of the probability error are presented and
the construction of robust and interpolation Monte Carlo algorithms are
discussed.
Results are presented comparing the performance of the Monte Carlo
algorithm with that of a corresponding deterministic algorithm. The two
algorithms are tested on a well balanced matrix and then the eﬀects of
perturbing this matrix, by small and large amounts, is studied.
Keywords: Monte Carlo algorithms, matrix computations, performance
analysis, iterative process.

1

Introduction

The problems of inverting a real n × n matrix (MI), solving a system of linear
algebraic equations (SLAE) or ﬁnding extreme eigenvalues are of unquestionable importance in many scientiﬁc and engineering applications: e.g real-time
speech coding, digital signal processing, communications, stochastic modelling,
and many physical problems involving partial diﬀerential equations. The direct
methods of solution for SLAE require O(n3 ) sequential steps when using the
usual elimination or annihilation schemes (e.g. Gaussian elimination, GaussJordan methods) [7] and, similarly, the direct methods for MI or calculating
extreme eigenvalues can be computationally expensive. Consequently the computation time for very large problems, or for real-time solution problems, can be
prohibitive and this prevents the use of many established algorithms.
It is known that Monte Carlo methods give statistical estimates for elements
of the inverse matrix, or for components of the solution vector of SLAE, by
Partially supported by the Bulgarian Ministry of Education and Science, under grant
I-1405/2004.
V.N. Alexandrov et al. (Eds.): ICCS 2006, Part III, LNCS 3993, pp. 632–639, 2006.
c Springer-Verlag Berlin Heidelberg 2006

Error Analysis of a Monte Carlo Algorithm

633

performing random sampling of a certain random variable, whose mathematical
expectation is the desired solution [8, 9]. The problem of variance estimation,
in the optimal case, has been considered for extremal eigenvalues [10]. In this
paper we extend this by considering bilinear forms of matrix powers, which can
be used to formulate solutions for all three problems, and study the eﬀects, on
the Monte Carlo algorithm, of perturbing a well balanced matrix.
The idea of Monte Carlo and an algorithm for the problem of bilinear forms
of matrix powers is presented in Section 2; in Section 3 we detail the implementation we used for the algorithm; the results of the experimental runs of this
implementation are also presented in this section; and we conclude the work in
Section 4.

2

Formulation of the Monte Carlo Algorithm

2.1

Bilinear Forms of Matrix Powers

We are interested in the bilinear form of matrix powers:
(v, Ak h).

(1)

For x, the solution of a SLAE Bx = b then
k

(v, x) =

v,

Ai h ,

(2)

i=0

where the Jacobi Over-relaxation Iterative Method has been used to transform
the SLAE into the problem x = Ax + h. In cases where the Neumann series does
not converge a resolvent method can be used [5, 6].
Matrix inversion is equivalent to solving (2) n times Bcj = e(j) , j = 1, . . . , n
for the special case where cj ≡ (c1j , . . . , cnj )T and e(j) ≡ (0, . . . , 0, 1 , 0, . . . , 0)T .
j

For an arbitrary large natural number k the Rayleigh quotient can be used to
obtain an approximation for λ1 , the dominant eigenvalue, of a matrix A:
λ1 ≈

(v, Ak h)
.
(v, Ak−1 h

Thus it is clear that having an eﬃcient way of calculating (1) is important.
This is especially important in cases where we are dealing with large and/or
sparse matrices.
2.2

Almost Optimal Markov Chain Monte Carlo

We shall use the so-called Almost Optimal Monte Carlo (MAO) algorithm studied in [2, 3, 1, 5, 4]. Here we give a brief presentation of MAO.
Suppose we have a Markov chain
T = α0 → α1 → α2 → . . . → αk → . . .

634

I. Dimov et al.

with n states. The random trajectory (chain) Tk of length k starting in the state
α0 is deﬁned as follows:
Tk = α0 → α1 → . . . → αj → . . . → αk ,

(3)

where αj means the number of the state chosen, for j = 1, . . . , k.
Assume that
P (α0 = α) = pα , P (αj = β|αj−1 = α) = pαβ ,
where pα is the probability that the chain starts in state α and pαβ is the
transition probability to state β after being in state α. Probabilities pαβ deﬁne
a transition matrix P . We require that
n

n

pα = 1

pαβ = 1, for any α = 1, 2, ..., n.

and

α=1

(4)

β=1

We will consider a special choice of density distributions pi and pij deﬁned as
follows:
pi =

2.3

|vi |
,
v

n

v

|vi |

=

and pij =

i=1

|aij |
,
ai

n

ai =

|aij |.

(5)

j=1

Monte Carlo Algorithm for Computing Bilinear Forms of
Matrix Powers (v, Ak h)

The pair of density distributions (5) deﬁnes a ﬁnite chain of vector and matrix
entrances:
vα0 → aα0 α1 → . . . → aαk−1 αk .
(6)
The latter chain induces the following product of matrix/vector entrances and
norms:
Akv

k

= vα0

aαs−1 αs ;

Akv

k

= v

s=1

×

aαs−1

.

s=1

Note, that the product of norms Akv is not a norm of Akv . The rule for creating
the value of Akv is the following: the norm of the initial vector v, as well as
norms of all row-vectors of matrix A visited by the chain (6), deﬁned by densities
(5), are included. For such a choice of densities pi and pij we have
E{hαk } =

sign{Akv }
v, Ak h .
Akv

The standard deviation σ{hαk } is ﬁnite. Since random variable
θ(k) = sign{Akv }×

Akv

hαk

(7)

Error Analysis of a Monte Carlo Algorithm

635

is an unbiased estimate of the form (v, Ak h), (7) can be used to construct a MC
algorithm.
Let us consider N realizations of the Markov chain Tk (3) deﬁned by the pair
(k)
of density distributions (5). Denote by θi the ith realization of the random
(k)
variable θ . Then the value
N
(k)

θ¯(k) =
i=1

θi

= sign{Akv }

Akv

N

{hαk }i

(8)

i=1

can be considered as a MC approximation of the form (v, Ak h). The probability
error of this approximation can be presented in the following form:
1

RN = (v, Ak h) − θ¯(k) = cp σ{θ(k) }N − 2 ,
(k)

(9)

where the constant cp only depends on the probability P = P r{|θ¯ − J| ≤ RN }
(where J is the exact solution of the problem, RN is the probability error and
0 < P < 1) and does not depend on N and on θ(k) . Because of the ﬁniteness of
the standard deviation the probability error is always ﬁnite.
In fact, (8) together with the sampling rules using probabilities (5) deﬁnes
a MC algorithm. The expression (8) gives a MC approximation of the form
(k)
(v, Ak h) with a probability error RN . Obviously, the quality of the MC algorithm depends on the behaviour of the standard deviation σ{θ(k) }. So, there is
a reason to consider a special class of robust MC algorithms.
2.4

Robust and Interpolation Monte Carlo Algorithms

Definition. A MC algorithm for which the standard deviation does not increase
with the increasing of the matrix power k is called a robust MC algorithm.
Thus, if the MC algorithm is robust, then there exists a constant M such that
lim σ{θ(k) } ≤ M.

k→∞

Definition. A MC algorithm for which the probability error is zero is called an
interpolation MC algorithm.
So, using the the following notations:
ˆ = {h2 }n ,
h
i i=1

v¯ = {|vi |}ni=1 ,

A¯ = {|aij |}ni,j=1

we can prove that
σ 2 {θ(k) } = Akv

h − (v, Ak h)2 ,
v¯, A¯k ˆ

where σ 2 {θ(k) } is the variance. A ﬁnite and small variance means that the associated Monte Carlo method should converge, and the probable error will be
small.

636

I. Dimov et al.

Now we can formulate an important result that gives a suﬃcient condition for
constructing an interpolation MC algorithm.
Let h = (1, . . . , 1), v = ( n1 , . . . , n1 ) and
⎞
⎛1
1
n ... n
⎟
⎜
A = ⎝ ...
⎠.
1
n

...

1
n

Then MC algorithm deﬁned by density distributions (5) is an interpolation MC
algorithm. Matrices A of the above form are stochastic matrices.

3

Experimental Results

For this paper two programs were implemented; one for the proposed Monte
Carlo algorithm and one for a deterministic matrix-vector and vector-vector
multiplication solving the same problem. The algorithms were written in Fortran
90 and compiled with the Intel Fortran 9.0 compiler. For all real numbers double
precision was used to lower the inﬂuence of rounding errors. Random numbers
were generated with the help of the Fortran 90 RANDOM NUMBER() sub-routine
which creates random numbers with a uniform distribution.
The results were generated on a SGI Prism equipped with 8 x 1.5 GHz Itanium
II processors and 16 GByte of main memory. For the input data the vectors h
and v and matrix A were generated for the sizes 100, 1000 and 5000. The vector
h was ﬁlled with ones and v was ﬁlled with n1 . The matrix A was ﬁlled with
elements of size n1 and then perturbed by 2, 5, 10, 50 and 90%. The norm of
such matrices is around 1. For comparison random non-balanced matrices were
generated too. In Figure 1 we show the results for diﬀerent perturbations for the
Monte Carlo algorithm and for the deterministic code. We see that the results
are very close for perturbations of up to 10% whereas the results for 50 and 90%
diﬀer up to 2% for matrices of size 1000 and 5000 and diﬀer up to 14% for a
matrix of size 100.

(v, Ak h)

1

0.1

n=100, k=5, chains=100
n=1000, k=5, chains=100
n=5000, k=5, chains=100
n=100, k=5, chains=1000
n=1000, k=5, chains=1000
n=5000, k=5, chains=1000
deterministic n=100, k=5
deterministic n=1000, k=5
deterministic n=5000, k=5
0.01
1

10
Pertubation in %

100

Fig. 1. The dependence of MC results on perturbation of A

Error Analysis of a Monte Carlo Algorithm

637

0.2
0.18

n=1000, pert=2%
n=1000, pert=10%
n=1000, pert=90%
n=5000, pert=2%
n=5000, pert=10%
n=5000, pert=50%
n=1000, non-balanced
n=5000, non-balanced

0.16

MC relative error

0.14
0.12
0.1
0.08
0.06
0.04
0.02
0
2

3

4

5

6

7

8

9

10

Power of A

Fig. 2. The dependence of MC relative error on power of A
1e-04
n=100, k=5, chains=100
n=1000, k=5, chains=100
n=5000, k=5, chains=100
n=100, k=5, chains=1000
n=1000, k=5, chains=1000
n=5000, k=5, chains=1000

1e-05

Variance

1e-06

1e-07

1e-08

1e-09
1

10
Pertubation in %

100

Fig. 3. The dependence of MC variance on perturbation of A

Since the deterministic computations were performed with a double precision
we accept the results obtained as exact results and use them to analyse the
accuracy of the results produced by our Monte Carlo code. In Figure 2 the
relative error of the results for Monte Carlo algorithm is shown. The Monte
Carlo relative error was computed by using the following formulas:
MC error = |MC result - exact result|,
MC error
MC relative error =
.
exact result
From Figure 2 we can see that the error increases linearly if k is increasing. The larger the matrix is, the smaller the inﬂuence of the perturbation. For
comparison, the results for non-balanced matrices were included.
The variance of the results for the diﬀerent perturbations are shown in
Figure 3. In this ﬁgure we compare results for diﬀerent sizes of the matrix and
diﬀerent chain lengths. Again it is obvious that the inﬂuence of the perturbation

638

I. Dimov et al.
1e-04
1e-05

n=1000, pert=2%
n=1000, pert=10%
n=1000, pert=90%
n=5000, pert=2%
n=5000, pert=10%
n=5000, pert=50%

1e-06
1e-07

MC error

1e-08
1e-09
1e-10
1e-11
1e-12
1e-13
1e-14
2

3

4

5

6
Power of A

7

8

9

10

Fig. 4. The dependence of MC error on power of matrices with ”small” spectral norms
( A ≈ 0.1)

is a lot bigger for smaller matrix of size 100. But over all a variance of 10e-06
is a good result and shows that the Monte Carlo algorithm works well with this
kind of balanced matrices.
In order to test the robustness of the Monte Carlo algorithm, a re-run of
the experiments was done with matrices of norm smaller than 1. Therefore the
randomly generated matrices were re-used and their elements were divided by
10. The results for these experiments are shown in Figure 4 and Figure 5.
In Figure 4 the Monte Carlo errors for matrix size of n = 1000 and chain
length of 1000 are shown. We can see that the Monte Carlo algorithm is very
robust because with an increasing k the error is decreasing enormously.
The variance shown in Figure 5 is 1010 smaller than the variance shown in
Figure 3.
1e-14
n=100, k=5, chains=1000
n=1000, k=5, chains=1000
n=5000, k=5, chains=1000
n=100, k=5, chains=100
n=1000, k=5, chains=100
n=5000, k=5, chains=100

1e-15

Variance

1e-16

1e-17

1e-18

1e-19
1

10
Pertubation in %

100

Fig. 5. The dependence of MC variance on perturbation of matrices with ”small”
spectral norms ( A ≈ 0.1)

Error Analysis of a Monte Carlo Algorithm

4

639

Conclusion

In this paper we have analysed the error and robustness of the proposed MC
algorithm for computing bilinear form of matrix powers (v, Ak h). We have shown
that with increasing the perturbations the error and the variance are increasing
too. Especially small matrices have a high variance. For a rising power of A an
increase of the relative error can be observed. The robustness of the Monte Carlo
algorithm with balanced matrices with matrix norms much smaller than 1 has
been demonstrated. In these cases the variance has improved a lot compared to
cases were matrices have norms close to 1. We can conclude that the balancing of
the input matrix is very important for MC computations. A balancing procedure
should be performed as an initial (preprocessing) step in order to improve the
quality of Monte Carlo algorithms. For matrices that are ”close” in some sense
to the stochastic matrices the accuracy of the MC algorithm is very high.

References
1. V. Alexandrov, E. Atanassov, I. Dimov, Parallel Quasi-Monte Carlo Methods for
Linear Algebra Problems, Monte Carlo Methods and Applications, Vol. 10, No. 3-4
(2004), pp. 213-219.
2. I. Dimov, Minimization of the Probable Error for Some Monte Carlo methods.
Proc. Int. Conf. on Mathematical Modeling and Scientiﬁc Computation, Albena,
Bulgaria, Soﬁa, Publ. House of the Bulgarian Academy of Sciences, 1991, pp. 159170.
3. I. Dimov, Monte Carlo Algorithms for Linear Problems, Pliska (Studia Mathematica Bulgarica), Vol. 13 (2000), pp. 57-77.
4. I. Dimov, T. Dimov, T. Gurov, A New Iterative Monte Carlo Approach for Inverse Matrix Problem, Journal of Computational and Applied Mathematics, Vol.
92 (1997), pp. 15-35.
5. I.T. Dimov, V. Alexandrov, A New Highly Convergent Monte Carlo Method for
Matrix Computations, Mathematics and Computers in Simulation, Vol. 47 (1998),
pp. 165-181.
6. I. Dimov, A. Karaivanova, Parallel computations of eigenvalues based on a Monte
Carlo approach, Journal of Monte Carlo Method and Applications, Vol. 4, Nu. 1,
(1998), pp. 33-52.
7. G.V. Golub, C.F. Van Loon, Matrix computations (3rd ed.), Johns Hopkins Univ.
Press, Baltimore, 1996.
8. I.M. Sobol Monte Carlo numerical methods, Nauka, Moscow, 1973.
9. J.R. Westlake, A Handbook of Numerical matrix Inversion and Solution of Linear
Equations, John Wiley & Sons, inc., New York, London, Sydney, 1968.
10. M. Mascagni, A. Karaivanova, A Parallel Quasi-Monte Carlo Method for Computing Extremal Eigenvalues, Monte Carlo and Quasi-Monte Carlo Methods (2000),
Springer, pp. 369-380.

