On Eﬀective Computation of the Logarithm of
the Likelihood Ratio Function for Gaussian
Signals
Maria V. Kulikova
Ulyanovsk State University, L. Tolstoy Str. 42, 432970 Ulyanovsk, Russia
KulikovaMV@ulsu.ru

Abstract. In this paper we consider a new way to calculate the logarithm of the Likelihood Ratio Function for Gaussian signals. This approach is based on the standard Kalman ﬁlter. Its eﬃciency is substantiated theoretically, and numerical examples show how such a method
works in practice.

1

Introduction and Problem Statement

One of the most important trends of the modern control theory for linear discrete stochastic systems is a fault detection problem. A wide class of the wellknown algorithms applied to this situation is based on a concept that is usual in
mathematical statistics, namely the logarithm of the Likelihood Ratio Function
(Log-LRF) [1]:
def

Log-LRF = ln

fθ1 (z)
= ln fθ1 (z) − ln fθ0 (z)
fθ0 (z)

(1)

where fθ (z) is a parametrized probability density and z is a measurement vector.
Thus, the eﬃcient evaluation of Log-LRF is very important, with this point
of view, for practical use. It follows from (1) that the announced goal can be
attained by developing an algorithm for eﬀective calculation of the logarithm of
the Likelihood Function (Log-LF) ln fθ (z).
In the paper, we consider the following linear discrete system with the white
noise:
xt+1 = Φt xt + Γt wt ,
(2a)
zt = Ht xt + vt

(2b)

where xt ∈ Rn , wt ∈ Rq , zt ∈ Rm , {w0 , w1 , . . .} and {v1 , v2 , . . .} are mutually
independent zero-mean Gaussian sequences of independent vectors. Without loss
of generality, their covariances Q and R are assumed to be reduced to identity
matrices; i.e., Q = I and R = I. The latter can be easily done by normalizing
This work was supported in part by the Russian Ministry of Education (grant
No. T02-03.2-3427).
P.M.A. Sloot et al. (Eds.): ICCS 2003, LNCS 2658, pp. 427–435, 2003.
c Springer-Verlag Berlin Heidelberg 2003

428

M.V. Kulikova

the input noise in (2a) and the measurements in (2b). We suppose also that the
both sequences mentioned above are independent of the Gaussian initial x0 with
a mean x
¯0 and a covariance matrix P0 .
It is easy to see that the Likelihood Function (LF) for the t-th measurement
zt in system (2) is given by the formula
−1/2

m

f (zt |Zt−1 ) = [(2π) |Σt |]

1
T −1
exp − [zt − Ht x
ˆ−
ˆ−
t ] Σt [zt − Ht x
t ] ,
2

where Σt = Ht Pt− HtT +I, provided that measurements Zt−1 = {z1 , z2 , . . . zt−1 }
def

def

def

have been processed. We further denote the updating sequence by νt = zt −
Ht x
ˆ−
t . It is characterized by the covariance matrix Σt . Then Log-LF has the
following form:
ln f (zt |Zt−1 ) = −

2

1
m
1
ln(2π) − ln |Σt | − νtT Σt−1 νt .
2
2
2

(3)

Algorithm of Eﬃcient Evaluation of Log-LRF

When calculating Log-LRF, most of the execution time is spent for ﬁnding two of
the summands, namely ln |Σt | and νtT Σt−1 νt (see formulas (1) and (3)). With the
aim to derive the eﬀective way for evaluating Log-LF, we consider the standard
algorithm of Kalman ﬁlter [2]. We call such computation the vector treatment
because the measurement vector zt at the moment t is being processed as a
whole.
Let us now consider another algorithm for computing ln |Σt | and νtT Σt−1 νt
which is also formulated on the basis of the Kalman ﬁlter. We call the new
approach the scalar treatment since the measurement vector zt at the moment t
is being processed in a component-wise way. This method is given as follows:
Algorithm 1
−
0
0
0
I. Set initial values: x
ˆ0t = x
ˆ−
t ; Pt = Pt ; δt = 0; ∆t = 0.
II. Compute for k = 1, 2, . . . , m:
(k)

(k−1)

αk = ht Pt
(k)

Kt
(k)

Pt

(k−1)

= Pt
(k)

= zt

(k)

=x
ˆt

νt
x
ˆt
(k)

δt

(k−1)

= δt

(k−1)

= Pt

(k)

(k)

(k) (k)

(k−1)

(k) (k−1)

− ht x
ˆt

(k−1)

(4a)

(ht )T /αk ;

− K t h t Pt

(k)

+ ln αk ;

(ht )T + 1;

;

;

(4b)

(k) (k)

(4c)

+ K t νt ;
(k)

∆t

(k−1)

= ∆t

(k)

+ (νt )2 /αk .

On Eﬀective Computation of the Logarithm

429

III. Obtain results:
(m)

ln |Σt | = δt

νtT Σt−1 νt = ∆t

(m)

;

;

(m)

Pt+ = Pt

.

First of all it is necessary to show equivalence of the two methods mentioned
above for evaluation of Log-LF: the vector treatment (standard algorithm) and
the scalar treatment (according to the proposed algorithm 1). This result is given
by
(k)

Theorem 1 Let the (1 × n)-matrix ht
(k)

zt

be the k-th row of Ht , and the scalar

(k)

= ht xt + vt

be the k-th element of zt in (2b). Then values of the both summands ln |Σt |and
νtT Σt−1 νt in the right-hand side of formula (3) can be obtained by the scalar
treatment (algorithm 1) that is used instead of vector treatment.
Proof. For convenience, we ﬁx an arbitrary time point t and omit the time from
all the formulas. If we now consider the structure of matrix Σ given by the
def
formula Σ = H T P (0) H + I then we have
 T (0)

h1 P h1 + 1 hT1 P (0) h2 · · · hT1 P (0) hm
 hT2 P (0) h1 hT2 P (0) h2 + 1 · · · hT2 P (0) hm 
def 

Σ = 
.
..
..
..
..


.
.
.
.
hTm P (0) h1

hTm P (0) h2

· · · hTm P (0) hm + 1

Further we split the proof of Theorem 1 into two parts.
Part I. Here, we want to prove validity of the formula
m

ln |Σ| =

ln(αk )

(5)

k=1

where the magnitudes αk , k = 1, . . . , m, are calculated by (4a). To complete it,
we apply induction with respect to the dimension of matrix Σ as follows:
1. Let m = 2. Then we need to verify the equality
ln |Σ| = ln(α1 ) + ln(α2 )
or, equivalently,
|Σ| = α1 α2 ,
and αk , k = 1, 2, satisfy formula (4a).
The simple substitution gives
(0)

α1 α2 = σ11
(0)

(0)

σ22 −

1 (0)
σ
α1 12

2

(0) (0)

(0)

= σ11 σ22 − σ12

2

= |Σ|,

where σij denotes the (i, j)-th element of the matrix Σ. Thus, we conclude that
Part I of Theorem 1 is valid when m = 2.

430

M.V. Kulikova

2. Let now (5) be true for m = s. Then the formula
s

|Σ| =

αk

(6)

k=1

with αk , k = 1, . . . , s, calculated by (4a) takes place. Further we want to justify
(6) when m = s + 1.
def

To do this, we consider the matrix Σ (1) = H T P (1) H + I where P (1) is
determined in algorithm 1. The latter yields the formula
Σ (1) = H T P (0) H −

1 T (0)
H P h1 hT1 P (0) H + I
α1

or

Σ

(1)

1

...

0



0

2

(0)
(0)
(0)
(0)
(0) (0)
(0) 
. . . σ2,s+1 − σ21 σ1,s+1 /σ11 
σ22 − σ12 /σ11
 0
.
=

...
...
...


2
(0)
(0)
(0)
(0)
(0)
(0)
(0)
0 σs+1,2 − σs+1,1 σ12 /σ11 . . . σs+1,s+1 − σs+1,1 /σ11

We see that the matrix Σ (1) has been obtained from the matrix Σ by the Gauss
(0)
method with the pivot σ11 .
From the structure of Σ (1) and determinant’s properties it follows
(0)

|Σ| = σ11 |Σ (1) |.

(7)

Let us now denote the minor of matrix Σ (1) obtained by omitting the ﬁrst row
and the ﬁrst column in this matrix as Σ ∗ . Then the formula
(0)

∗
σij
= σi+1,j+1 −
(0)

(0)

σi+1,1 σ1,j+1
(0)

σ11

,

i, j = 1, . . . , s,

is valid for each element of the matrix Σ ∗ .
By the inductive hypothesis, we derive
s

|Σ ∗ | =

αk∗

(8)

k=1

because the matrix Σ ∗ is of dimension s × s. Here, the magnitudes αk∗ , k =
1, . . . , s, are calculated by (4a) provided that one iteration has been done. Therefore, (7), (8) and the notation introduced above lead to the ﬁnal result
|Σ| = σ11 |Σ (1) | = σ11 |Σ ∗ | = α1
(0)

s

(0)

k=1

αk∗ = α1

s+1

s+1

αk =
k=2

αk ,
k=1

where αk , k = 1, . . . , s + 1, satisfy (4a), that completes the proof. Thus, Part I
of the theorem has been justiﬁed.

On Eﬀective Computation of the Logarithm

431

Part II. In this Part, it is necessary to prove validity of the formula
ν T Σ −1 ν =

m
k=1

νk2
αk

(9)

where the left-hand side of (9) means the vector treatment, as in equations of
the standard Kalman ﬁlter, and the right-hand one implies the component-wise
treatment by algorithm 1. We now apply induction with respect to the dimension
of matrix Σ as well as in Part I.
1. Let m = 2. We need to verify the equality
2

ν T Σ −1 ν =
k=1

νk2
αk

or, equivalently,
ν2
ν2
ν12 A11 + 2ν1 ν2 A12 + ν22 A22
= 1 + 2
|Σ|
α1
α2

(10)

where Aij is a relevant cofactor of the matrix Σ. Here we have also used the
symmetry of matrix Σ.
Since the left-hand side of (10) means the vector treatment and the righthand one implies the component-wise treatment, we further denote the i-th
component of vector ν in the left-hand side by (νi )v and the i-th component of
vector ν in the right-hand side by (νi )cw . Then it is easy to see that
(ν1 )v = (ν1 )cw = z 1 − hT1 x0 ,
but (ν2 )v = (ν2 )cw because
(ν2 )v = z 2 − hT2 x0 ±hT2 x1 = z 2 − hT2 x1 + hT2 (x1 − x0 )
= (ν2 )cw + hT2 x0 + K 1 ν1 − x0 = (ν2 )cw + hT2 K 1 ν1 .
Thus, we can transform the vector treatment (left-hand side of formula (10))
to the component-wise treatment by substitution of the above results. We obtain
(ν12 )v A11 + 2(ν1 )v (ν2 )v A12 + (ν22 )v A22 /|Σ|
= (ν12 )cw σ22 − 2(ν1 )cw (ν2 )cw σ12 − 2(ν12 )cw σ12 hT2 K 1 + (ν22 )cw σ11
+2(ν1 )cw (ν2 )cw σ11 hT2 K 1 + σ11 (ν12 )cw hT2 K 1

2

/|Σ|

where K 1 is calculated by algorithm 1; i.e., K 1 = P (0) h1 /α1 . Now we take
into account that from formula (5), proved in Part I of the theorem, it follows
|Σ| = α1 α2 . Finally, we yield the formula
2(ν1 )cw (ν2 )cw σ11 hT2 P (0) h1
(ν22 )cw σ11
2(ν1 )cw (ν2 )cw σ12
−
+
|Σ|
|Σ|
α1 |Σ|

432

M.V. Kulikova

+

(ν 2 )cw
(ν12 )cw
2σ 2
σ2
(ν 2 )cw
σ22 − 12 + 12 = 1
+ 2
|Σ|
σ11
σ11
α1
α2

which completes the proof of (10) when m = 2.
2. Let us assume that Theorem 1 is true for m = s; i.e., the formula
s

ν T Σ −1 ν =

k=1

νk2
αk

(11)

with αk , k = 1, . . . , s, computed by (4a) takes place for any square matrix Σ of
dimension s. Our goal now is to check (9) for m = s + 1.
First of all it is easy to see that


s+1
s+1
1

ν T Σ −1 ν =
νj
νi Aij 
|Σ| j=1 i=1


s+1
s+1
s+1
1 
=
νj
νi Aij + 2
νi ν1 Ai1 + ν12 A11  .
|Σ| j=2 i=2
i=2

(12)

Here, as above, Aij means the relevant cofactor of the symmetric matrix Σ. Let
us apply the ﬁrst step of Gaussian elimination to Σ and denote the new matrix
ˆ We have to show that formula (11) holds for the matrix Σ.
ˆ
by Σ.
On the other hand, from formula (12) we obtain


s+1
s+1
ˆ −1 ν = 1 
νT Σ
νj
νi Aˆij 
ˆ
|Σ|
j=2

+

1
2
ˆ
|Σ|

i=2

s+1

νi ν1 Aˆi1 + ν12 Aˆ11
i=2

ˆ Having
where Aˆij , i, j = 1, . . . , s + 1, are relevant cofactors of the matrix Σ.
ˆ
ˆ
used the structure of matrix Σ we easily conclude that A1j = 0, j = 2, . . . , s + 1.
Thus,


s+1

ˆ −1 ν = 1 
νj
νT Σ
ˆ
|Σ|
j=2

s+1

νi Aˆij + ν12 Aˆ11  .

(13)

i=2

Then, taking into account the introduced notation, (6) and the formula
s+1

αk

Aˆ11 =
k=2

which follows from the proof of Part I immediately, (13) becomes
s+1
T

ˆ −1

ν Σ

ν=

j=2

νj

s+1
i=2

ˆ
|Σ|

νi Aˆij
+

ν12
.
α1

(14)

On Eﬀective Computation of the Logarithm

433

Let now the minor derived by omitting the ﬁrst row and the ﬁrst column
ˆ be denoted as Σ∗ . We note that Aˆij = σ11 A∗
in the matrix Σ
i−1,j−1 , i, j =
2, . . . , s + 1, where A∗ij is the relevant cofactor of the matrix Σ∗ . It means that
the following equality is valid:
s+1

s+1

νj
j=2

ˆ T Σ −1 ν∗ .
νi Aˆij = σ11 |Σ∗ |ν∗T Σ∗−1 ν∗ = |Σ|ν
∗
∗

i=2

Here, ν∗ = [ν2 , ν3 , . . . , νs+1 ]T .
Having remembered that the inductive hypothesis takes place for the matrix
Σ∗ of dimension s × s, we come to the formula
ν∗T Σ∗−1 ν∗ =

s
k=1

[νk∗ ]
αk∗

2

(15)

where the magnitudes αk∗ , k = 1, . . . , s, are calculated by (4a) provided that
one iteration has been done. Therefore, from the notation introduced above and
formulas (14), (15) it follows
ˆ −1 ν =
νT Σ

s+1
k=2

νk2
ν2
+ 1 =
αk
α1

s+1
k=1

νk2
αk

where αk , k = 1, . . . , s + 1, satisfy (4a). That proves Part II of the theorem.
Thus, Theorem 1 has been completely justiﬁed.
Now we show the eﬃciency of the scalar treatment (Algorithm 1) for evaluation of Log-LF. In this context, we calculate and compare the total number of
the arithmetical operations: +, -, /, * needed for the computation of Log-LF as
by the vector treatment as by the scalar one (see Table 1).
According to the data of Table 1, we can conclude that the usage of the
scalar treatment allows the total number of arithmetical operations to be reduced
by the power of 2 with respect to m (i.e., with respect to the dimension of
measurement vector zt ). The latter increases the speed of computation of LogLRF by Algorithm 1 signiﬁcantly.

3

Numerical Examples

As a test problem, we consider the example from [4]. Let system (2) be deﬁned
as follows:




0.75 −1.74 −0.3 0.0 −0.15
0.0 0.0 0.0
 0.09 0.91 −0.0005 0.0 −0.008 
 0.0 0.0 0.0 




 xt +  24.64 0.0 0.0  wt ,
0.0
0.0
0.95
0.0
0.0
xt+1 = 




 0.0 0.0
 0.0 0.835 0.0 
0.0 0.55 0.0 
0.0 0.0
0.0
0.0 0.905
0.0 0.0 1.83

434

M.V. Kulikova

Table 1. Total number of arithmetical operations for evaluation of Log-LF with the
both treatments (vector and scalar)
Treatment

Total number of operations
Multiplications and Divisions
Subtractions

Additions
Vector

3n3 +n2 (2+q+3m)+
n(q 2 +2m2 +2m+1)+
2m2 + m

n2 + m

3n3 + n2 (1 + q + 3m) + n(q 2 + 2m2 +
2m) + m3 + m2 + 2m

Scalar

2n3 +n2 (2+q+3m)+
n(q 2 + 3m) + 3m

mn2 + m

2n3 + n2 (1 + q + 4m) + n(q 2 + 4m) +
2m

zt =

10001
x + vt
01010 t

where {wt } and {vt } are zero-mean white noise sequences with covariance matrices Qt = I and Rt = I (I denotes the identity matrix).
Table 2 shows the results of numerical experiments which conﬁrm Theorem 1
in practice. We see that the diﬀerences between the both methods do not exceed
the computer accuracy. All software for the numerical tests was written in Pascal.
Table 2. Values of Log-LF computed by the vector and scalar treatments
Discrete Time Vector Treatment (y1 )
-5.42
0
10000
-4.46
20000
-5.84
30000
-6.35
40000
-4.47
50000
-4.42
60000
-4.51
70000
-4.82
80000
-9.79
90000
-7.83
100000
-8.78

Scalar Treatment (y2 )
-5.42
-4.46
-5.84
-6.35
-4.47
-4.42
-4.51
-4.82
-9.79
-7.83
-8.78

y1 − y2
0.0
4.3E-19
4.3E-19
4.3E-19
4.3E-19
0.0
3.8E-19
0.0
3.3E-19
4.3E-19
0.0

∞

Table 3 shows the execution time (in seconds) required for the vector and
scalar treatments in the computation of Log-LF. The advantage of the scalar
algorithm will be especially noticeable on systems with large measurement vectors (i.e., when the dimension m is large). According to the data of Table 3, we
conclude that the gain in time on the 40000-th iteration is approximately two
seconds for our example. (Note that m = 2 there, and all the experiments were
developed with processor Intel Pentium-II.) It is obvious that increasing of the
parameter m results the greater proﬁt of Algorithm 1 in execution time.

On Eﬀective Computation of the Logarithm

435

Table 3. Execution time (in sec.) for the evaluation of Log-LRF computed by the
vector and scalar treatments
Discrete Time Vector Treatment (t1 )
0
0.0
10000
4.91
20000
9.80
30000
13.39
40000
17.59
50000
22.32
60000
26.79
70000
31.13
80000
35.75
90000
40.28
100000
44.87

Scalar Treatment (t2 )
0.0
3.65
7.87
11.93
15.69
19.59
23.37
27.37
31.28
35.50
38.97

|t1 − t2 |
0.0
1.26
1.21
1.46
1.90
2.73
3.42
3.76
4.47
5.23
5.90

Some additional experiments for practical determination of parameters
changes in linear dynamic systems were developed for the methods from [1].
The data obtained also conﬁrm the eﬃciency of the scalar treatment presented
in the paper.

4

Conclusion

Thus, we have developed the new eﬀective way for evaluation of Log-LRF for
Gaussian signals that is very important for a number of problems in practice.
The necessary theory has been given and the eﬃciency has been investigated
on the test examples. We have shown clearly that the vector treatment and the
scalar one produce the same result, but the execution time is smaller for the latter
method. That is why we recommend this algorithm for practical implementation
in real time problems.
Acknowledgment. The author would like to thank Prof. I.V. Semoushin for
the problem statement and valuable notes.

References
1. Basseville, M., Nikiforov, I.: Detection of Abrupt Changes: Theory and Applications.
Prentice-Hall, Englewood Cliﬀs, New Jersey, 1993
2. Bierman, G.J.: Factorization methods for discrete sequential estimation. Academic
Press, New York, 1977
3. Newbold, P.M., Ho, Yu-Chi.: Detection of changes in the characteristics of a GaussMarkov process. IEEE Trans. Aerosp. and Electr. Syst. AES-4 (1968) No. 5, 707–
718
4. Semoushin, I.V.: Practical method of fault detection in navigational systems. (in
Russian) Izvestiya VUZov. Ser. Priborostroenie. (1981) No. 3, 53–56.

