Available online at www.sciencedirect.com

Procedia Computer Science 18 (2013) 1106 – 1115

International Conference on Computational Science, ICCS 2013

Distributed Multiscale Computations using the MAPPER
framework
Mohamed Ben Belgacema,∗, Bastien Choparda,∗, Joris Borgdorﬀb , Mariusz Mamo´nskic ,
Katarzyna Rycerzd,e , Daniel Harezlake
a University

of Geneva, Route de Drize 7, 1227 Carouge, Switzerland
Science, Faculty of Science, University of Amsterdam, Amsterdam, the Netherlands
c Pozna´
n Supercomputing and Network Center, Pozna´n, Poland
d Instutute of Computer Science, AGH University of Science and Technology, Krakow,
´
Poland
e CYFRONET, AGH Univerisity of Science and Technology, Krakow,
´
Poland

b Computational

Abstract
We present a global overview of the methodology developed within the MAPPER European project to design, implement and
run a multiscale simulation on a distributed supercomputing infrastructure. Our goal is to highlight the main steps required
when developing an application within this framework. More speciﬁcally, we illustrate the proposed approach in the case
of hydrology applications. A performance model describing the execution time of the application as a function of its spatial
resolution and the hardware performance is proposed. It shows that Distributed Multiscal Computation is beneﬁcial for large
scale problems.
Keywords: Multiscale, application, distributed, grid, computation, MAPPER framework

1. Introduction
Multiscale applications are an important challenge for computational science and many domains of research.
Most phenomena involve many spatial or temporal scales, and the interaction between various physical processes.
It is therefore important to provide an eﬀective methodology for designing, implementing and running multiscale
applications. There are rather few frameworks that oﬀer general tools and concepts to address such a challenge.
Methodological papers such as [1, 2, 3] are important in this perspective as they propose examples of a conceptual
approach. However, there is still a lack of a consensus vision among the many communities dealing with multiscale applications. A recent review [4] of diﬀerent domains shows that existing multiscale projects tend to use
radically divers approaches to build and run their applications, mostly connecting their codes with hand-written
scripts and run computation on single sites.
Running simulations across multiple computing sites has been done in previous works, as illustrated for instance in [5, 6, 7]. These approaches oﬀer user friendly API and convenient grid submission procedures but do
not propose a full methodology based on theoretical concepts and a formalism as discussed below.
∗ Corresponding

author. Tel.: +41 (0)22 379 02 19 / 01 90 ; fax: +41 (0)22 379 02 50.
Email addresses: mohamed.benbelgacem@unige.ch (Mohamed Ben Belgacem), bastien.chopard@unige.ch (Bastien Chopard),
J.Borgdorff@uva.nl (Joris Borgdorﬀ), mamomski@man.poznan.pl (Mariusz Mamo´nski), kzajac@agh.edu.pl (Katarzyna Rycerz),
d.harezlak@cyfronet.pl (Daniel Harezlak)

1877-0509 © 2013 The Authors. Published by Elsevier B.V. Open access under CC BY-NC-ND license.
Selection and peer review under responsibility of the organizers of the 2013 International Conference on Computational Science
doi:10.1016/j.procs.2013.05.276

Mohamed Ben Belgacem et al. / Procedia Computer Science 18 (2013) 1106 – 1115

1107

A few year ago, some of us contributed to the development of a multiscale framework, coined CxA for Complex Automata. The initial goal was the coupling of diﬀerent Cellular Automata or Lattice Boltzmann models
with diﬀerent spatial and temporal scales [8, 9, 10]. This approach has been successfully applied to the so-called
In-Stent Restenosis biomedical problem [11]. It also gave rise to a Multiscale Modeling Language (MML)) [12],
oﬀering a powerful way to describe a multiscale problem in terms of its components and their couplings.
This approach is now further developed and generalized within the European project MAPPER 1 . The goal
is to show that a wide range of applications can be described, implemented and run within the formalism. An
important aspect of the project is the execution phase where the concept of Distributed Multiscale Computing
(DMC) is proposed [13, 14]. In short, the idea is that the diﬀerent components of a multiscale-multiphysics
application can be distributed on several supercomputing facilities and run in a tightly and/or loosely coupled
way. DMC allows the users to access at once, and for one speciﬁc application, much more computing resources
than available in a single supercomputing center.
The goal of this paper is to present the sequence of actions that are proposed in the MAPPER framework, from
building to executing a multiscale-multiphysics application on the European distributed computing infrastructure,
such as EGI and PRACE [15, 16]. The process will be illustrated for the case of a speciﬁc problem taken from the
MAPPER application portfolio, namely the simulation of irrigation canals or rivers. The other applications studied
in MAPPER are in-stent restenosis [11, 17], cerebrovascular blood ﬂow [18], clay-polymer nanocomposites [19],
nuclear fusion, and gene regulatory networks.
Hydrology is a class of problem for which multiscale tools are needed. River and water way management
is a main concern of our modern society, in particular for clean electricity production and agriculture irrigation.
One of our interests is to simulate a section of the Rhone river (13 km), from the Geneva city down to Verbois
water dam. In collaboration with the Geneva electricity company, we are investigating the possibility to study
speciﬁc questions, namely, the way to ﬂush sediments in some critical area by changing the water levels. In order
to address this problem, a multiscale, multiphysics approach is required as one cannot aﬀord a fully resolved 3D,
free surface ﬂow with sediment transport along the full 13 km. We are thus developing a numerical approach
where some sections are described with a 1D shallow water solver whereas more critical sections are solved by
a 3D free surface model with sediment. The MAPPER framework allows us to easily connect these diﬀerent
components and run them on a distributed grid of supercomputers [20].
Note that for hydrology applications at least, MAPPER oﬀers a problem solving environment, that is a framework in which several multiscale, multiphysics components can be predeﬁned and coupled at will to build any
desired complex hydrology system. We described this feature as a “lego based philosophy”.
2. Formalism
We refer the reader to [12, 10, 13, 14, 21] for a detailed discussion of the theoretical concepts underlying our
approach. Here we would like to adopt the point of view of the end-user aiming at developing, or adapting, his
application within the proposed framework.
The MAPPER formalism is based on four steps, as illustrated in Fig. 1. These steps are called here Modeling,
computation design, multiscale implementation and execution. Each of them is discussed below.
2.1. Modeling
In this step, a given multiscale-multiphysics phenomena is decomposed in several coupled single-scale submodels, by selecting only the most relevant scales of the problem, and selecting the relevant physical processes
that take place in the system. From a practical point of view submodels correspond to numerical solvers, with
given spatial and temporal resolution. By coupling these submodels in an appropriate way, one expects a good
enough approximation of the original problem with all scales, with yet a substantial reduction of computational
needs.
The coupling between the submodels is realized by components called mappers (junctions) or conduits. They
implement the required scale bridging techniques and transfer of data. Note that the knowledge of the diﬀerent
1 http://www.mapper-project.eu

1108

Mohamed Ben Belgacem et al. / Procedia Computer Science 18 (2013) 1106 – 1115

Fig. 1. The pipeline of actions of the MAPPER framework, from modeling to execution.

scales of the system is in the junctions only, making the submodels autonomous and independent solvers, blind to
the coupling structure.
The decomposition in submodels and their couplings is a diﬃcult yet central task. It is usually done according
to the speciﬁc knowledge that the scientist has of the given problem. This process is however facilitated by
representing the system on a Scale Separation Map (SSM), as suggested for instance in [10].
An important aspect of the formalism is that submodels are assumed to be described by a generic time loop
containing only a few abstract operators, named initialization, observation, computation, and boundary condition.
The coupling between submodels can then be thought of as a link between a pair of such fundamental operations.
In order to be MAPPER compatible, a submodel should have input and output ports for the above generic
operatiors. At the programming level, these ports are implemented with a coupling framework called MUSCLE
[22, 23].
In our approach, the links between submodels are also computational processes. Submodels do not have to
know about the rest of the system, only links know the properties and scales of the submodels they connect. Simple
links simply transfer data from one side to another, but they can also be programmed to do some transformation
(ﬁltering or interpolation). An important type of link is provided by a component called mapper which may have
several inputs or several outputs.
Several coupling patterns can be identiﬁed between submodels. They depends on the relation between the
submodels in the SSM [10] and can be cyclic or acyclic [13].
The conceptualization of a multiscale application according to the above methodology can be expressed, independently of the implementation, with MML, our mutiscale modeling language [12]. Within this language, the
submodels, their scales and their couplings are described in an abstract way. Fig. 2 illustrates a graphical version
of the MML formulation of a physical system where two canal sections are connected through a gate.
2.2. Computation Design
MAPPER provides tools that implement the MML speciﬁcation: MaMe (Mapper Memory) and MAD (Mapper
Application Designer) [24]. MaMe is based on a semantic integration technology described in [25] and composed
of a database and web based interface that allow end-users to easily describe submodels and junctions, to deﬁne
the operators and conduits operations, and to catalog them into the database.
The MAD tool is also a web interface which reads the MaMe database, displays submodels/mappers as icons

1109

Mohamed Ben Belgacem et al. / Procedia Computer Science 18 (2013) 1106 – 1115





(a) Example of real water canal

 	
	




(b) MML conceptualization of the reality
Fig. 2. Modeling step

and enables users to drag-and-drop in a graphical design space in order to compose a cyclic or acyclic computational workﬂow.
It is worth noticing here that the submodels/junctions
/
can be implemented in diﬀ
ﬀerent languages (C++, Java,
etc.) and have no information regarding the other components, thus, illustrating the “Lego based philosophy” of
our approach.





	





	







Fig. 3. Computation design step

Fig. 3 illustrates the design step of a multiscale application where a computational workﬂow is designed on the
ﬂy. The connection between components is handled by the deﬁned operators and conduits (arrows in the ﬁgure)
and inter-scale data are transmitted throughout conduits accordingly to the data type described in the MaMe
database.
The MAD tool generates a conﬁguration ﬁle, named CxA ﬁle in reference to the theoretical concepts mentioned above. This ﬁle describes the coupling of all the components as illustrated in Listing 1 for the case of two
canal sections separated by a gate. Here, the ﬂow in the canal sections are computed by two submodels, SW1D1
and SW1D2, both implementing a Lattice Boltzmann 1D shallow water ﬂow simulation (hence the name SW1D).
Listing 1. Example of CxA ﬁle
1
2
3
4
5
6
7
8
9
10
11
12
13
14
15

# declare kernels
cxa.add kernel(’SW1D1’, ’com.unige.irigcan.kernel.d1.SW1Dkernel’)
cxa.add kernel(’SW1D2’, ’com.unige.irigcan.kernel.d1.SW1Dkernel’)
cxa.add kernel(’Gate’, ’com.unige.irigcan.junction.Gate kernel’)
# conﬁgure connection scheme
cs = cxa.cs
cs.attach(’SW1D1’=>’Gate’) {tie(’f out’,’f1 in’)}
cs.attach(’SW1D2’=>’Gate’) {tie(’f out’,’f2 in’)}
cs.attach(’Gate’=>’SW1D1’) {tie(’f1 out’,’f in’)}
cs.attach(’Gate’=>’SW1D2’) {tie(’f2 out’,’f in’)}
#parameters
cxa.env[’SW1D1:dx’]=0.05
cxa.env[’SW1D2:dx’]=0.025
cxa.env[’SW1D1:dt’]=0.025
cxa.env[’SW1D2:dt’]=0.0125

1110

Mohamed Ben Belgacem et al. / Procedia Computer Science 18 (2013) 1106 – 1115

The computational workﬂow submodels are declared on lines 2-4. Each component is identiﬁed by a unique
name and the binary ﬁle that implements it. Lines 7-10 establish the coupling between the ports of each submodel.
Here for instance, “f out” and “f in” are both boundary operators from which the canal section “SW1D1” sends
and receives boundary data, respectively. In lines 12-15, we describe the input parameters needed by submodels
binaries.
2.3. Multiscale Implementation
Multiscale applications involve interactions between diﬀerent scientiﬁc codes which iteratively exchange data
at diﬀerent scale, or of diﬀerent nature. As mentioned earlier, the actual coupling of the submodels can be implemented with the MUSCLE API [22, 23]. MUSCLE has both a Java and C/C++ API and allows coupling of MPI
code with non-MPI one. Data transfers between submodels is handled mainly by send()/receive() methods calls
of the MUSCLE API.
Listing 2 illustrates the code of the “SW1D 1” submodel, developed in Java language. The declarations of the
conduits and ports names are in lines 3-4 and 8-9. In this example, the data type of the exchange is array of
doubles and f_in and f_out are the same as in the CxA ﬁle.
Listing 2. Example of Java submodel
1

public class SW1D 1 extends muscle.core.kernel.CAController {

2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
21
22
23

}

private ConduitEntrance<double[]> f out;
private ConduitExit<double[]> f in;
...
@Override
protected void addPortals() {
f out = addEntrance(”f out”, double[].class);
f in = addExit(”f in”,double[].class);
}
@Override
protected void execute() {
double [] data tosend;
boolean cond=false;
while (!cond){
//computation: collide and stream
data=readBoundary(...);//get boundary data
f out.send(data); // send boundary data
data=(double[]) f in.receive();// receive boundary data
//observation ... (cond==true)?
}
}

Let us consider again the example of two canal sections connected through a gate (Fig. 3). If the upstream
and downstream canals have a diﬀerent spatial and temporal resolutions, grid reﬁnement techniques must be used
for the coupling. In this case, the gate (junction) can be programmed to handle two diﬀerent frequencies of
send()/receive() operations for each side. We have implemented such a coupling in [26] using the grid reﬁnement algorithm presented in [27]. The gate submodel was run on a separate computer node, thus illustrating the
possibility of having a distributed simulation.
2.4. Execution
In this step the computation is launched over local or distributed grid e-infrastructures. The MUSCLE library
should be installed on all the computing nodes. Its role is to start the submodels described in the CxA ﬁle, to
establish the communication between them and to handle inter-scale data transfer. In case of local computation,
all the submodels can be run on the same machine or distributed on machines within in the same local area
network. In case of large distributed execution, involving machines on a wide area network, MUSCLE library
uses the MUSCLE Transport Overlay (MTO), a kind of proxy that allows data transfers between submodels even
if they are running on diﬀerent supercomputers. In this case, MTO instances provide access points to the local
area networks corresponding to the computing nodes. The conﬁgured MTO instances accept only connections
coming from remote master nodes and transmit data intended for another supercomputer to the relevant MTO.
For applications having cyclic communications, distributed simulations require coordination and eﬃcient
scheduling of submodels submitted at the same time on the distributed computing sites (supercomputers and

Mohamed Ben Belgacem et al. / Procedia Computer Science 18 (2013) 1106 – 1115

1111

clusters). On top of MUSCLE , the QoSCosGrid middleware is used (QCG) [28] for the submission. It supports
an advance reservation mechanism, essential to block several computing resources for the same period of times.
Job submission can be made through the command line or through the GridSpace [29, 30] web based environment. The latter allows the end-user to reserve graphically time slots on the available EGI [15] and PRACE [16]
resources, and to export the computation description directly from the MAD tool.
3. Simulation results
Here we evaluate the time overhead induced by the MAPPER approach when performing computations of hydrodynamical problems. The MAPPER tools we consider are: MUSCLE API, QCG middleware, and PALABOS
toolkit 2 .
Our benchmark consist of an MPI based lattice Boltzmann (LB) 3D cavity ﬂow problem, developed using
PALABOS. The pseudo-code of the algorithm is shown in listing 3
Listing 3. pseudo-code of the caviy3d example
1
2
3
4
5
6
7

Finit()
While (it++<2000){
CollideAndStream()
GetBoundaryData()
SendReceiveBoundaryData()
}
end

• The CollideAndS tream() operation consists in a LB computing operations.
• The GetBoundaryData() operation retrieves boundary data scattered over the same single cluster/
r supercomputer nodes.
• The S endReceiveBoundaryData() operation sends/receives
/
the selected boundary data too/from other submodels.
• The U pdateBoundaryData() operation updates the boundary data scattered over the same single cluster/
rsupercomputer nodes.
As depicted in Fig. 4, the simulation scenario consists in:
1. Running a monolithic simulation (T
T mono ) over one cluster.
2. Splitting the cavity into two equal sections (left and right) and coupling them using the MUSCLE API.
Simulations are ﬁrst performed on the same single cluster and, then, over two distributed clusters using the
QCG grid middleware.
3. Measuring the wall-clock time for the execution and data transfer in the distributed simulation and comparing it with the monolithic time.

Fig. 4. Conﬁguration of the monolithic and MUSCLE simulations

From the theoretical point of view, and inspired by the work of [31], we elaborated a simple performance
prediction model, that approximates the execution time based on the computing resources and the cyclic coupling
strategy.
2 http://www.palabos.org/
g

1112

Mohamed Ben Belgacem et al. / Procedia Computer Science 18 (2013) 1106 – 1115

Let us consider a multiscale application involving n submodels {sk }1≤k≤n that run in a tightly coupled way.
The application has a computation domain length L x (Fig. 4) and runs over p cores. We deﬁne T mono (L x , p) as
the monolithic computation time of the application without using MUSCLE and T muscle (L x , p) as the required
MUSCLE computation time for the application over p cores. The goal of this simple performance model is to
evaluate T muscle (L x , p) based on T mono (L x , p).
Each submodel sk of this application has a computation domain length L x,k and runs over pk cores. Note
that p = nj=1 pk and L x = nk=1 L x,k . At each iteration, each submodel sk exchanges boundary data Dk with its
neighbors. Let tk (L x,k , pk ) be the required time to compute the submodel sk and we write
tk (L x,k , pk ) = T mono (L x,k , pk ) + T com (Dk )

(1)

Where T mono (L x,k , pk ) is the monolithic time required to process the submodel computation domain and T com (Dk )
the time required to exchange the boundary data. For large problem, we assume that the MPI inter-processor communications are negligible in regard to the computation and we deﬁne T mono (L x,k , pk ) = T serial (L x,k )/pk , where
T serial (L x,k ) is the time required to process sk submodel computation domain on one core processor. We write
T mono (L x,k , pk )

=

T com (Dk , Δx)

=

T serial (L x,k )
pk
Ly .Lz
β × Δx
2

L x,k .Ly .Lz
×
Δx3 .pk
1
β1 × Δx2 × I

=α×
×I

=

I

=

α1 .L x,k
pk

×

1
Δx3

×I

(2)

Where I is the number of iterations to perform and α1 and β1 are coeﬃcients, independent of Δx, but depending
respectively on the speed of the processors and the bandwidth of the network. They can be obtained from technical
speciﬁcations or by running the application and by ﬁtting the execution time to the above formula. As we can see
in the eq. (2), the T mono (L x,k , pk ) depends on Δx3 whereas T com (Dk ) depends on Δx2 . From eq. (1) and eq.(2) we
obtain
tk (L x,k , pk ) = (

α1 .L x,k
pk

×

1
Δx3

+ β1 ×

1
)
Δx2

×I

(3)

It is obvious that T muscle (L x , p) will be determined by the slowest submodel, i.e:
T muscle (L x , p) = max {T mono (L x,k , pk ) + T com (Dk )}

(4)

1≤k≤n

For a given cyclic multiscale application comprising n submodels, let’s now consider a uniform computation
domain discretization, where Δx is the same among all the submodels. Also, let us assume that all submodels
run on the same type of computing resources. As a result, submodels will exchange the same boundary data
size at each iteration and T com (Dk ) = T com will be constant. In addition, since T muscle (L x , p) depends on the
slowest submodel, a uniform decomposition of the submodels computation domains, for which T mono (L x , p) =
T serial (L x )/p, L x,k = L x /n and pk = p/n = q, leads to T mono (L x , p) = T mono (L x,k , pk ) for all k, and minimizes
T muscle (L x , p). Thus, by combining
T mono (L x , p)
max {T mono (L x,k , pk ) + T com (Dk )}

1≤k≤n

= T mono (L x,k , pk )
= T mono ( Lnx , np ) + T com

= T mono ( Lnx , np )
= T mono ( Lnx , q) + T com

= T mono ( Lnx , q)

(5)

with eq. (2), we ﬁnally obtain
T mono (L x , p) = αq2 × Δx1 3 × I
T muscle (L x , p) = ( αq2 × Δx1 3 × (1 +

β1 .q
α2

× Δx)) × I

(6)

and, hence, the total communication fraction gain of the application can be written as
εmuscle (L x , p) =

monolithic execution time
T mono (L x , p)
=
MUSCLE execution time
T muscle (L x , p)

(7)

In our benchmark problem, the total computational domain has a length L x of 4500 meters, a width Ly of 100
meters and a depth Lz of 25 meters. The LB implementation is based on C++ MPI and simulation is carried out
with diﬀerent value of Δx ranging from 0.4(m) to 2(m) with a step of 0.2(m). Regarding the LB parameters, each
lattice cell contains 19 distribution functions with DOUBLE precision data type (8 bytes). The size of one lattice
cell is equal to 19 × 8 = 152 Bytes. To treat the oﬀ-lattice boundary condition, we need for each submodel the
e = 3 boundary cells along the x-axis, including all the corresponding y-axis and z-axis cells.

1113

Mohamed Ben Belgacem et al. / Procedia Computer Science 18 (2013) 1106 – 1115
Table 1. LB grid size based on Δx and boundary data size

Δx(m)
nx
ny
nz
Dk (KB)

0.4
11250
250
63
7182

0.6
7500
167
42
3198

0.8
5625
125
32
1824

1
4500
100
25
1140

1.2
3750
84
21
804

1.4
3215
72
18
590

1.6
2813
63
16
459

1.8
2500
65
14
414

2
2250
50
13
269

Table 1 shows the diﬀerent lattice grid sizes and the size of the boundary data Dk used for the computations as
a function of Δx and LB distribution functions. The quantities n x , ny and nz represent the number of lattice sites
along the x-axis, y-axis and z-axis of the cavity3D section, respectively.
The monolithic execution (T mono ) is carried out over 20 cores on the same Gordias cluster (located at hepia 3 ,
Switzerland), having the following conﬁguration:
• 28 nodes with 12 GB and 8 cores each (64bits Intel-Xeon(R) CPU, 2.40GHz) with OpenMPI environment.
• InﬁniBand (IB) based network interface communication (20 Gb/sec (4X DDR))
• Ethernet network interface with a speed of (1Gb/s)
local
For the “mapperized” computation, we performed a local computation (T muscle
) on the Gordias cluster and a
grid
distributed one (T muscle ) over two geographically distributed EGI clusters (“Galera” and “Reef” located in Poland).

3.1. Local execution
In order to run the local simulation on the “Gordias” cluster, we ﬁrstly run the MUSCLE manager, a single
daemon which manages the registration of the running submodels, in a separate node. Then, we run the left and
the right sections over 10 cores each.
The total number of iterations we performed is equal to 2000 for both the monolithic and MUSCLE local simulations. For each iteration, we measured the computation clocktime of the “boundary” and “collideAndStream”
operations. We repeated the simulation with diﬀerent Δx values.
Results are shown in the ﬁrst and second columns of table 2. The diﬀerence of the execution clocktime
local
for the case Δx=0.4 was 106 seconds over a total elapsed time of 5498 seconds. For the
between T mono and T muscle
case Δx = 1.4, it was 141 seconds over a total elapsed time of 370 seconds. This suggests that the time overhead
of MUSCLE on the same cluster (collideAndStream and data transfer operations) vary slowly with Δx and can
be averaged to 120 seconds, for all values of Δx. This behavior can be explained by the stability of the network
speed connecting the nodes on the “Gordias” cluster.
3.2. Distributed execution
For the distributed execution, we kept the same conﬁguration (number of cores, same executable and input
data, etc.), but used two separate clusters: “Galera” and “Reef” (both are IntelXeon cluster). Compared to the
grid
local
(one cluster), the distributed execution T muscle
is mainly inﬂuenced by the speed of the
local execution T muscle
network connection between clusters, in addition to the size of the boundary data. This can also be observed on
grid
grid
grid
local
= T mono /T muscle
and on the comparison between muscle
and muscle
depicted
the distributed eﬃciency values muscle
in table 2. Besides, we can observe that we obtain a small time overhead with small values of dx, and vice versa.
local
Based on the measurements in table 2, we observe that the execution times T muscle
and T mono follow the
local
and T mono
performance model proposed in eq. (6) (data not shown). Furthermore, these measured values of T muscle
allow us to determine the values of α2 and β1 . As a result, we can rewrite eq. (6) as
−3

T mono (Δx) = 0.874.10
× Δx3 × I
q
local
T muscle (Δx) = T mono (Δx)(1 + 0.43 × Δx)

(8)

Note that these numerical values are valid for p = 20 cores and depend also on the computation performance
of the Gordias cluster, namely, the speedup behavior of the code parallelization and the network connection speed.
3 http://hepia.hesge.ch/

1114

Mohamed Ben Belgacem et al. / Procedia Computer Science 18 (2013) 1106 – 1115

It is clear that the beneﬁt of a distributed execution becomes evident when the size of such problem exceeds
the capability of one computer or cluster in term of computing resources, i.e, for small Δx or large computation
domain (L x ,Ly ,Lz ). The beneﬁt disappears when the overhead of the distributed computation becomes larger than
the gains of faster processing, particularly, when the problem size is not large enough.
Table 2. Local eﬃciency

Δx
0.4
0.6
0.8
1
1.2
1.4
1.6
1.8
2

local
muscle

T mono (s)
5392.54
1746.9
846.53
490.94
321.81
228.34
166.85
125.69
97.76

and distributed eﬃciency

grid
muscle

local
T muscle
(s)
5498.85
1939.85
1015.09
654.78
407.62
369.37
255.68
246.26
209.47

(2000 iterations)
grid
T muscle
(s)
9249
4111
2229
1403.84
1186.74
811.129
589.5
533.12
684.7

local
muscle

0.98
0.90
0.83
0.74
0.78
0.61
0.65
0.51
0.46

grid
muscle

0.58
0.42
0.37
0.36
0.27
0.28
0.28
0.23
0.14

Note that the present performance analysis compares T muscle and T mono for a ﬁxed number of cores. In practical
situations however, MUSCLE and distributed computations will be used not to have the same number of processors
split on several clusters, but to access extra processors to solve a larger problem. Thus, despite its computational
overhead, MAPPER enables end-users no only to run computations that were out range with local computing
resource, but also to build eﬃcient and scalable multiscale applications that can easily run on a distributed grid
infrastructure.
4. Conclusion
In this paper, we have described the steps required by the MAPPER framework to design and run a multiscale
application on a possibly distributed computing infrastructure. The proposed methodology is easy and formal
enough to be applied to a large range of multiscale applications. We have shown that MAPPER provides userfriendly tools that hide the technical complexity of running a multiscale application on grid e-infrastructures. We
have demonstrated that a distributed execution becomes beneﬁcial when the size of such problem exceeds the
capability of one computer or cluster in term of computing resources. Finally we have also presented the “legobased philosophy” oﬀered by the MAPPER framework to build, out of predeﬁned components, a wide range of
simulations in a given ﬁeld of application. We are currently developing 3D-3D couplings with diﬀerent spatial
and temporal resolutions, as well as 1D-3D couplings in order to adjust the resolution and the numerical method
to the need of each submodels.
Acknowledgment
This research is funded by the European project FP-7 MAPPER under grant agreement RI-261507. Numerical
simulations were carried out in part on the Pl-Grid infrastructure, part of the European grid infrastructure. We
would like to thank Jonas L¨att and all the MAPPER project consortium for their work and helpful discussions.
References
[1] W. E, X. Li, W. Ren, E. Vanden-Eijnden, Heterogeneous multiscale methods: A review, Commun. Comput. Phys. 2 (2007) 367–450.
[2] G. Ingram, I. Cameron, K. Hangos, Classiﬁcation and analysis of integrating frameworks in multiscale modelling, Chemical Engineering
Science 59 (2004) 2171–2187.
[3] J. O. Dada, P. Mendes, Multi-scale modelling and simulation in systems biology, Integrative Biology 2 (3) (2011) 86–96. doi:10.
1039/c0ib00075b.
[4] D. Groen, S. Zasada, P. Coveney, Taxonomy of multiscale computing communities, in: e-Science Workshops (eScienceW), 2011 IEEE
Seventh International Conference on, 2011, pp. 120 –127. doi:10.1109/eScienceW.2011.11.

Mohamed Ben Belgacem et al. / Procedia Computer Science 18 (2013) 1106 – 1115

1115

[5] D. Groen, S. P. Zwart, T. Ishiyama, J. Makino, High-performance gravitational n-body simulations on a planet-wide-distributed supercomputer, Computational Science & Discovery 4 (1) (2011) 015001.
[6] S. Manos, M. Mazzeo, O. Kenway, P. V. Coveney, N. T. Karonis, B. Toonen, Distributed mpi cross-site run performance using mpig, in:
Proceedings of the 17th international symposium on High performance distributed computing, HPDC ’08, ACM, New York, NY, USA,
2008, pp. 229–230. doi:10.1145/1383422.1383459.
[7] H. Bal, K. Verstoep, Large-scale parallel computing on grids, Electronic Notes in Theoretical Computer Science 220 (2) (2008) 3 – 17,
proceedings of the 7th International Workshop on Parallel and Distributed Methods in veriﬁCation (PDMC 2008).
[8] A. Hoekstra and E. Lorenz and J.-L. Falcone and B. Chopard, Towards a Complex automata formalism for multuscale modeling, Int. J.
Multiscale Multiscale Computational Engineering 5(6) (2008) 491–502.
[9] A. Hoekstra, F. J.-L, A. Caiazzo, B. Chopard, Multi-scale modeling with cellular automata: The complex automata approach, in: H. U.
et al. (Ed.), ACRI 2008, Vol. LNCS 5191, Springer-Verlag Berlin Heidelberg 2008, 2008, pp. 192–199.
[10] A. Hoekstra, A. Caiazzo, E. Lorenz, J.-L. Falcone, B. Chopard, Complex automata: Multi-scale modeling with coupled cellular automata,
in: J. Kroc, P. M. Sloot, A. G. Hoekstra (Eds.), Simulating Complex Systems by Cellular Automata, Vol. 0 of Understanding Complex
Systems, Springer Berlin Heidelberg, 2010, pp. 29–57. doi:10.1007/978-3-642-12203-3_3.
[11] A. Caiazzo, D. Evans, J.-L. Falcone, J. Hegewald, E. Lorenz, B. Stahl, D. Wang, J. Bernsdorf, B. Chopard, J. Gunn, R. Hose,
M. Krafczyk, P. Lawford, R. Smallwood, D. Walker, A. Hoekstra, A complex automata approach for in-stent restenosis: two-dimensional
multiscale modeling and simulations, J. of. Computational SciencesDOI: 10.1016/j.jocs.2010.09.002.
[12] J.-L. Falcone, B. Chopard, A. Hoekstra, MML: towards a Multiscale Modeling Language, Procedia Computer Science 1 (1) (2010)
819–826.
[13] J. Borgdorﬀ, E. Lorenz, A. Hoekstra, J. Falcone, B. Chopard, A Principled Approach to Distributed Multiscale Computing, from Formalization to Execution, in: e-Science Workshops (eScienceW), 2011 IEEE Seventh International Conference on, 2011, pp. 97–104.
[14] J. Borgdorﬀ, J.-L. Falcone, E. Lorenz, C. Bona-Casas, B. Chopard, A. G. Hoekstra, Foundations of distributed multiscale computing:
Formalization, speciﬁcation, and analysis, J. Parallel and Distributed Computing.
[15] EGI, http://www.egi.eu/.
[16] PRACE, http://www.prace-ri.eu/.
[17] J. Borgdorﬀ, C. Bona-Casas, M. Mamonski, K. Kurowski, T. Piontek, B. Bosak, K. Rycerz, E. Ciepiela, T. Gubała, D. Harezlak,
M. Bubak, E. Lorenz, A. G. Hoekstra, A Distributed Multiscale Computation of a Tightly Coupled Model Using the Multiscale Modeling
Language, Procedia Computer Science 9 (2012) 596–605. doi:doi:10.1016/j.procs.2012.04.064.
[18] D. Groen, J. Borgdorﬀ, C. Bona-Casas, J. Hetherington, R. W. Nash, S. J. Zasada, I. Saverchenko, M. Mamonski, K. Kurowski, M. O.
Bernabeu, A. G. Hoekstra, P. V. Coveney, Flexible composition and execution of high performance, high ﬁdelity multiscale biomedical
simulations, CoRR abs/1211.2963.
[19] J. Suter, D. Groen, L. Kabalan, P. V. Coveney, Distributed multiscale simulations of clay-polymer nanocomposites, Materias Research
Society Proceedings 1470. arXiv:http://journals.cambridge.org/article_S1946427412010093.
[20] S. J. Zasada, M. Mamonski, D. Groen, J. Borgdorﬀ, I. Saverchenko, T. Piontek, K. Kurowski, P. V. Coveney, Distributed Infrastructure
for Multiscale Computing, in: 2012 IEEE/ACM 16th International Symposium on Distributed Simulation and Real Time Applications
(DS-RT), IEEE, 2012, pp. 65–74. doi:10.1109/DS-RT.2012.17.
[21] B. Chopard, J.-L. Falcone, A. G. Hoekstra, J. Borgdorﬀ, A Framework for Multiscale and Multiscience Modeling and Numerical Simulations, in: C. Calude, J. Kari, I. Petre, G. Rozenberg (Eds.), LNCS 6714, Springer, Berlin, Heidelberg, 2011, pp. 2–8.
[22] J. Hegewald, M. Krafczyk, J. Tike, A. Hoekstra, B. Chopard, An Agent-Based Coupling Platform for Complex Automata, in: M. Bubak,
G. van Albada, J. Dongarra, P. Sloot (Eds.), Computational Science - ICCS 2008, Vol. 5102 of Lecture Notes in Computer Science,
Springer Berlin / Heidelberg, 2008, pp. 227–233.
[23] MUSCLE2, http://apps.man.poznan.pl/trac/muscle.
[24] MAD software, http://www.mapper-project.eu/web/guest/mad-mame-ew.
[25] T. Gubala, M. Bubak, P. M. Sloot, Semantic Integration of Collaborative Research Environments, Information Science Reference IGI
Global, 2009, Ch. XXVI, pp. 514–530.
[26] M. Ben Belgacem, B. Chopard, A. Parmigiani, Coupling method for building a network of irrigation canals on a distributed computing
environment, in: G. C. Sirakoulis, S. Bandini (Eds.), Cellular Automata, Vol. 7495 of Lecture Notes in Computer Science, Springer
Berlin Heidelberg, 2012, pp. 309–318.
[27] D. Lagrava, O. Malaspinas, J. Latt, B. Chopard, Advances in multi-domain lattice Boltzmann grid reﬁnement, Journal of Computational
Physics 231 (14) (2012) 4808 – 4822.
[28] Bartosz Bosak and Jacek Komasa and Piotr Kopta and Krzysztof Kurowski and Mariusz Mamo´nski and Tomasz Piontek, New Capabilities in QosCosGrid Middleware for Advanced Job Management, Advance Reservation and Co-allocation of Computing Resources – Quantum Chemistry Application Use Case, in: M. Bubak, T. Szepieniec, K. Wiatr (Eds.), Building a National Distributed
e-Infrastructure–PL-Grid, Vol. 7136 of Lecture Notes in Computer Science, Springer Berlin / Heidelberg, 2012, pp. 40–55.
[29] E. Ciepiela, D. Harezlak, J. Kocot, T. Barty´nski, M. Kasztelnik, P. Nowakowski, T. Gubala, M. Malawski, M. Bubak, Exploratory programming in the virtual laboratory, in: Computer Science and Information Technology (IMCSIT), Proceedings of the 2010 International
Multiconference on, 2010, pp. 621 –628.
[30] Ciepiela, E. and Nowakowski, P. and Kocot, J. and Hare¸z˙ lak, D. and Gubała, T. and Meizner, J. and Kasztelnik, M. and Barty´nski,
T. and Malawski, M. and Bubak, M., Managing entire lifecycles of e-science applications in the gridspace2 virtual laboratory–from
motivation through idea to operable web-accessible environment built on top of pl-grid e-infrastructure, Building a National Distributed
e-Infrastructure–PL-Grid (2012) 228–239.
[31] L. Axner, J. Bernsdorf, T. Zeiser, P. Lammers, J. Linxweiler, A. Hoekstra, Performance evaluation of a parallel sparse lattice boltzmann
solver, Journal of Computational Physics 227 (10) (2008) 4895 – 4911. doi:10.1016/j.jcp.2008.01.013.

