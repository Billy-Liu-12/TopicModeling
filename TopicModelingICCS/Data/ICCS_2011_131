Available online at www.sciencedirect.com
Procedia Computer Science 00 (2009) 000–000

Procedia
Computer
Science
www.elsevier.com/locate/procedia

Procedia Computer Science 4 (2011) 382–391

International Conference on Computational Science, ICCS 2011

Efficient Probabilistic Latent Semantic Indexing
using Graphics Processing Unit
Eli Koffi KOUASSI*1, Toshiyuki AMAGASA*1,*2, Hiroyuki KITAGAWA*1,*2
*1 Graduate School of Systems and Information Engineering
*2 Center for Computational Sciences
University of Tsukuba
1-1-1 Tennodai, Tsukuba, Ibaraki 305-8573, JAPAN

Abstract

In this paper, we propose a scheme to accelerate the Probabilistic Latent Semantic Indexing (PLSI), which is an
automated document indexing method based on a statistical latent semantic model, exploiting the high parallelism of
Graphics Processing Unit (GPU). Our proposal is composed of three techniques: the first one is to accelerate the
Expectation-Maximization (EM) computation by applying GPU matrix-vector multiplication; the second one uses
the same principles as the first method, but deals with the sparseness of co-occurrence of words and documents; and
the third one is to use the concurrent kernel execution, which is available on NVIDIA Fermi architecture, in order to
speed up the process. We compare the performance of the proposed scheme with the non-parallelized
implementation. The results show that our method could be more than 100 times faster than the CPU-based
implementation in our environment. By dealing with the sparseness of the data, we could not only process more
documents and words using GPU, but we could also keep more data on the device memory so that we can avoid
massive data copy transfer between the host and the device susceptible to reduce the execution performance.
Keywords: Graphic Processing Unit (GPGPU), Clustering, Algorithms, Probabilistic Latent Semantic Indexint (PLSI), Expectation Maximization
(EM) Algorithm;

1. Introduction
The importance of text analysis is increasing in many applications including computational sciences, due to the
growing needs of information retrieval and text mining from large text databases. In such processes, the automated

*

* Corresponding author. Tel.: +81-298-53-5051; fax: +81-298-53-5522.
E-mail address: keli@kde.cs.tsukuba.ac.jp(K.K. Eli), amagasa@cs.tsukuba.ac.jp(T. Amagasa), kitagawa@cs.tsukuba.ac.jp(H.Kitagawa)

1877–0509 © 2011 Published by Elsevier Ltd. Open access under CC BY-NC-ND license.
Selection and/or peer-review under responsibility of Prof. Mitsuhisa Sato and Prof. Satoshi Matsuoka
doi:10.1016/j.procs.2011.04.040

383

Eli Koffi KOUASSI et al. / Procedia Computer Science 4 (2011) 382–391
Eli Koffi KOUASSI, Toshiyuki AMAGASA, Hiroyuki KITAGAWA/ Procedia Computer Science 00 (2011) 000–000

indexing of documents is often used as a fundamental tool to process a document collection for subsequent
document analysis. It actually maps a document to statistical latent classes, and therefore it can also be used to
perform dimension reduction or clustering.
Latent Semantic Indexing (LSI) [1] is a commonly used technique of automated document indexing. It is based
on the mathematical principle of Singular Value Decomposition (SVD), and has successfully been used in many
applications. However, it is known that LSI has some drawbacks: 1) the resulting (reduced) dimensions are often
difficult to interpret; 2) the input is a bag-of-words, which are not capable of incorporating structural information
from the text; 3) ambiguous terms create noise in the vector space; and 4) SVD is computationally expensive.
To cope with these problems, Hofmann [2] proposed Probabilistic Latent Semantic Indexing (PLSI). PLSI is a
statistically funded method, which is based on the co-occurrence of terms and documents with a latent class.
Consequently, it has a more robust statistical foundation, and is able to provide a proper generative data model. In
addition, it can deal with domain specific synonymy and polysemy. PLSI has proven to be effective and has been
used in many applications. However since PLSI is based on Expectation-Maximization (EM) algorithm [3], it still
suffers from long execution time when dealing with large datasets.
Many approaches have been trying to solve this drawback. One of the latest works is proposed by Wan et al. [4].
They exploit a distributed computing environment by Message Passing Interface (MPI) to process PLSI, while
trying to reduce the memory required by redesigning the EM implementation.
Meanwhile, for the last several years, General-purpose computation on graphics processing units (GPGPU) has
been gaining much public attention as a new computational platform. The idea is to exploit GPUs for not only
graphical processing, but also general-purpose computation. GPGPU now covers diverse range of applications,
such as physical simulation, audio processing, video processing, cryptography processing, and so on [5][6][7][8].
However, text processing on GPU has not yet been studied very well, because available memory on GPU is
generally limited, while text processing requires much memory spaces.
In this paper, we propose a scheme for processing PLSI using GPU. As mentioned earlier, the technical challenge
is twofold: how to deal with massive text data using limited memory on GPU and how to speedup PLSI using the
functionality of GPU. Our contribution can be summarized as following: 1) reinterpretation of EM the algorithm
using matrix vector multiplication to make the best use of GPU instructions; 2) reorganization of the steps of the EM
algorithm to save the space and to reduce the complexity; and 3) exploiting a dedicated data structure for sparse
matrixes called occurrence matrix to cope with data sparseness. The above approach helps to process thousands of
documents with thousands of unique terms in seconds. The feasibility of the proposed scheme is demonstrated by
experiments.
The rest of this paper is organized as follows. Section 2 gives some preliminary knowledge that is required for
the subsequent discussion. In Section 3, we briefly mention some related works. In Section 4 we present our
proposal, and experiments and results are shown in Section 5. The last section of this paper concludes our work and
shows the future works.

2. Preliminaries
2.1. Probabilistic Latent Semantic Indexing (PLSI)
The core of PLSI is the aspect model [2], which is a latent or "hidden" model associating the hidden classes
z k  {z1 , z 2 ,..., z K } with the co-occurrence of documents d i  {d1 , d 2 ,..., d D } and words

w j  {w1 , w2 ,..., wW } . If we suppose that the terms are chosen independently of the documents for each hidden
class, the probability form of this model can be expressed as

P(di , w j )  k P( zk ) P(di | zk ) P(w j | zk )
A standardized way to solve (1) is maximizing the log likelihood function as expressed in Equation (2)

(1)

384 	

Eli Koffi KOUASSI et al. / Procedia Computer Science 4 (2011) 382–391
Eli Koffi KOUASSI, Toshiyuki AMAGASA, Hiroyuki KITAGAWA/ Procedia Computer Science 00 (2011) 000–000

D

W

    n(d i , w j ) log P (d i , w j )

(2)

i 1 j 1

where n(d, w) is the number of occurrence of the word w in the document d.
A standard procedure to solve this problem is the Expectation Maximization (EM) algorithm [3]. The EM is an
iterative method composed of two steps. While the values of the log likelihood are estimated in the Expectation (E)
step based on the values of the parameters in (1), these parameters are updated in the maximization (M) step. The
EM stops when a predefined condition is reached. In the case of PLSI, the E step defines the probability a word in a
particular document is explained by the hidden class z, and the M step updates the probabilities. Equations (3) shows
the E step and equations (4) to (6) show the M step. In real applications, tempered EM is used to avoid over fitting.
(E):

P( z k | d i , w j ) 

P( w j | z k ) P( z k | d i )

(3)

m1 P(w j | zm ) P( zm | di )
K

 n(d , w ) P( z | d , w )
|z )
  n(d , w ) P( z | d , w )
D

(M):

P( w j

k

D

i 1

i 1

i

j

k

i

j

(4)

W

i

l 1

l

k

i

l

 n( d , w ) P ( z | d , w )
)
  n( d , w ) P ( z | d , w )
W

P(d i | z k

m1

P( z k

i

j 1
W

D

j

k

m

j 1

i

j

j

k

m

  n( d , w ) P ( z | d , w )
)
  n( d , w )
D

W

i 1

j 1
D

i 1

i

j

k

i

W

j 1

i

j

(5)

j

(6)

j

3. Related Work
Many researchers are now interested in GPU because of its attractive performance benefit. One of the most
famous among them is physical simulation on GPU, such as fluid dynamics and water condensation [6]. In the area
of signal and image processing, many works has been presented, such as segmentation [7], image thresholding [8].
GPU has been successfully applied to problems in databases and data mining. Bakkum et al. propose a method to
accelerate SQL queries using CUDA [9]. Zhan et al. accelerate text mining using CUDA [10]. Govindaraju et al.
propose a framework called GPUTeraSort, which is to provide a fast sorting mechanism large databases using
CUDA [11].
However, we found that few works exploit GPU for text analysis. This is due to the fact that processing text data
using GPU faces some technical challenges. One major challenge is that text data are in many cases voluminous,
while the available memory on GPU is quite limited. Wu et al. present one of the related works in this category.
They propose to cluster large data point using CUDA [12]. Many other research works can be found in the survey
on GPU by Owens et al. [5].
3.1. Parallel Latent Semantic Analysis using GPU
Cavanagh et al. proposed to accelerate LSA using GPU [13]. Their idea is to speed up the most time-consuming
part of LSA, which is the singular value decomposition (SVD), by parallelization using GPU. Specifically, they use

Koffi KOUASSI
et al.
/ Procedia
ComputerProcedia
Science 4Computer
(2011) 382–391
Eli Koffi KOUASSI, Eli
Toshiyuki
AMAGASA,
Hiroyuki
KITAGAWA/
Science 00 (2011) 000–000

385

the Lanczos algorithm because of its efficiency. Also, they use CUBLAS, which is a GPU version of the famous
numerical computation library called Basic Linear Algebra (BLAS) [14]. As a result, they can process big matrices
such as 4000-by-4000 in seconds. Our work is different from this work, because our target is PLSI. In addition, their
algorithm assumes that the number of latent classes is divisible by 16, which is not flexible when we apply it to reallife applications. Our approach, on the other hand, can deal with any number of latent classes; also the underlying
indexing model is different.
3.2. Parallel and Distributed Latent Semantic Indexing
Regarding research works on parallelizing PLSI, to the best of our knowledge, there have been only few works. In
[3] Wan et al. attempt to cope with the drawbacks of PLSI, that is, it considerably consumes computing resources in
terms of both execution time and internal memory. Specifically, their approach is based on a former work by Hong
et al. [15], which proposes a shared-memory parallelization using OpenMP. Wang et al., in addition, combine
distributed memory parallelization using MPI. They also consider the sparseness of the co-occurrence of terms and
documents and retouch the EM algorithm in order to save memory for computation. Our work is different from
those approaches, because we exploit GPU for speeding up PLSI.
4. Parallelization of PLSI on GPU
As mentioned earlier, processing text data on GPU poses several technical challenges: 1) how to make the best
use of GPU facilities to speed up EM algorithm and 2) how to deal with large text dataset in the limited memory on
GPU. This section presents our proposed scheme for executing PLSI using GPU.
4.1. Accelerating EM Algorithm using Matrix-vector Multiplications
One of the features of GPU is that it is able to execute SIMD operations, such as vector multiplication, very
efficiently with high parallelism. For this reason, it is crucial to make the best use of such mechanisms in our PLSI
implementation to achieve better performance. In fact, in this work, we reorganize the EM algorithm to make use of
the vector multiplication operation. This is desirable in the sense that we can use off-the-shelf numerical
computation packages, such as CUBLAS [14], in the implementation.
The pseudo-code of the proposed scheme is shown in Figure 1. Our proposed scheme is based on the following
observations. When we look into the EM algorithm, we notice that we can categorize the components in the formula
into the following two groups; the components which depend on iterations and the components which depend on
latent classes. In the first group, the components remain unchanged during iteration and are shared by all latent
classes. In the other group, in contrast, the components vary depending on the latent classes. According to this
analysis, we separate the computations and combine some steps of the EM algorithm for efficiency on both time and
memory space. We also implement the equations at the matrix and vector level so that the computations can be
done more efficiently.
In the following discussion, we introduce some matrix notation of the terms in the above formulas: P(z) = [zkk] is a
K-by-K matrix, P(d|z) = [bkj] is a K-by-D matrix, and P(w|z) = [ckj] is a K-by-W matrix. Also, suppose that the (x, y)
element of matrix M is referred to as Mx, y. Then, in Equation (1),Γ(dx, wy) is computed by:

(d x , wy )  (([zkk ] *[bki ])T *[ckj ]) x, y

(7)

where MT is the transpose of matrix M. The denominator Γ(d,w) is common for all latent classes in each iteration,
and is denoted as a D-by-W matrix.
In the meantime, the numerator varies according to the class for any i  {1,2,...,K } . It is computed according to
Equation (8), where P(zk) is the probability of the latent class k, and P(d|zk) and P(w|zk) are vectors of size D and W
representing the probability of d and w under class z, respectively. These vectors can easily be extracted from [bki]
and [ckj]:

zk (d x , wy )  ([zkk ] *[bki ]T *[ckj ])x, y

(8)

386 	

Eli Koffi KOUASSI et al. / Procedia Computer Science 4 (2011) 382–391

Eli Koffi KOUASSI, Toshiyuki AMAGASA, Hiroyuki KITAGAWA/ Procedia Computer Science 00 (2011) 000–000

1: ComputeTotalOccurrence
2: For each iteration
3:
ComputeExpectationDenominator
4:
For each latent class
5:
FinalizeExpectationForCurrentClass
6:
ComputeWordScoreForCurrentClass
7:
ComputeDocScoreForCurrentClass
8:
UpdateProbabilitiesForCurrentClass
9: End latent class Loop
10: End iteration Loop
Fig. 1. Pseudo-code of our proposed implementation of PLSI.

Finally, E step can be computed from the denominator Γ and the numerator Ψ. As can be seen from Equations (4),
(5), and (6), and Figure 2 as well, we successfully cut down inherent computation by multiplying each pair (di,wj)
with its occurrence n(di,wj) when computing the final step of the expectation, because the probability calculated in
Equation (1) is never used alone but always multiplied with the occurrence n(di,wj). Equation (9) gives the E step for
a given class m and Figure 2 shows the final image of the result.

P( z m | d x , w y ) 

([ z kk ]m,m [bki ]T * [ckj ]) x , y
(([z kk ] * [bki ])T * [c kj ]) x , y

( d1, w1 )
(d1, w2 )
n( d1, w1 )
n(d1, w2 )
( d1, w1 )
( d1, w2 )
(d 2 , w1 )
(d 2 , w2 )
n(d 2 , w1 )
n(d 2 , w2 )
(d 2 , w1 )
(d 2 , w2 )
...
...
(d D 1, w1 )
(d D 1, w2 )
n(d D 1, w1 )
n(d D 1, w2 )
(d D 1, w1 )
(d D 1, w2 )
D1 (d D , w1 )
 (d D , w2 )
n(d D , w1 )
n(d D , w2 )
(d D , w1 )
(d D , w2 )

n( d x , w y )

(9)

... ... ...

(d1, wW 1 )
n(d1, wW 1 )
( d1, wW 1 )

... ... ...

...

... ... ...

( d1, wW )
n(d1, wW )
(d1, wW )
(d 2 , wW )
n(d 2 , wW )
(d 2 , wW )
...

...
(d D 1, wW 1 )
... ... ...
n(d D 1, wW 1 )
...
(d D 1, wW 1 )
 (d D , wW 1 )
 (d D , wW )
... ... ...
n(d D , wW 1 )
n(d D , wW )
(d D , wW 1 )
(d D , wW )

Fig. 2. The final step of expectation multiplied by the word occurrence.

As for the details of our implementation, we exploit parallel array reduction method. Belloch [16] has proposed a
method of parallel array reduction method optimized for CUDA, and an implementation is publicly available as a
library named CUDPP [18]. We use this library to implement the M step. Specifically, we use it in the following
way: 1) using whole matrix reduction in order to normalize the probabilities; 2) using the row multi scan to compute
the probabilities of documents; and 3) using row multi scan to transpose the expectation matrix in order to compute
the probabilities of words.
Having computed the expectations, the next step is the maximization step. The process is relatively easy and can
be implemented efficiently using GPU, because no loop is required. One thread is dedicated to update the
probability of one document, while another thread is used to update the probability of one word. These are done in
parallel for each latent class

Eli Koffi KOUASSI et al. / Procedia Computer Science 4 (2011) 382–391
Eli Koffi KOUASSI, Toshiyuki AMAGASA, Hiroyuki KITAGAWA/ Procedia Computer Science 00 (2011) 000–000

387

4.2. Exploiting the Data Sparseness
As shown in Figure 2, we need to refer to the co-occurrence matrix of term and document, [n(d, w)], by which we
can obtain the occurrence of word w in d. However, in practical, this type of matrix is a quite sparse. Table 1 shows
the statistics from the well-known training sets that we used in our experiments.
Table 1 Sparseness of text datasets
Dataset

#Docs

#Words

#Occurrence

Occupancy

CRAN

1398

7867

63813

0.58%

MED

1033

10062

28013

0.32%

CISI

1460

9765

34947

0.29%

Because the total memory size of GPU is strictly limited, it is crucial to exploit sparse matrix representation
techniques to deal with large document dataset. There are many kinds of such approaches [19] for GPU. We adopt
the Compressed Sparse Row (CSR) format for its simplicity and memory efficiency. CSR stores column indices and
nonzero values in arrays as indices and data. It stores row information as pointer, named ptr. Taking a D-by-W
matrix, for example, ptr is of D+1 length and indices and data are of the number of nonzero elements in the matrix.
Figure 3 illustrates an example of CSR representation of a simple matrix. For the space limitation, we do not show
how we implemented it on GPU. We actually store only nonzero elements in CSR, which is inspired by the scalar
representation in [19].

Fig. 3. Compressed Row Matrix Representation example

4.3. Optimization on Fermi Architecture
As shown in Figure 1, we can separate the class-dependent components from the class-independent components
in the modified EM implementation. When having a closer look at the class-dependent part, we observe that the
computation of expectations and updates concerning a class do not interfere that regarding other classes. Here is a
chance for further optimization. As the recently released NVIDIA Fermi architecture offers the possibility to execute
multiple kernels concurrently, we are able to do the process in parallel according to the number of latent classes. In
order to avoid global memory being overwritten, we design the system so that each kernel has its own working
space. Actually, this is not memory efficient, and therefore limits its application to dataset of modest size. Coping
with this problem is a part of our future work.

388 	

Eli Koffi KOUASSI et al. / Procedia Computer Science 4 (2011) 382–391
Eli Koffi KOUASSI, Toshiyuki AMAGASA, Hiroyuki KITAGAWA/ Procedia Computer Science 00 (2011) 000–000

5. Experiments
We conducted a number of experiments in order to prove the effectiveness of our method. Specifically, we compare
the CPU-based implementation with the GPU version.
5.1. Dataset
We use both synthetic and real world data. We used the CRAN, MED, and CISI training set, which are publicly
available [21] from Cornell University. We apply standardized stemming and stop-word elimination to the training
set in order to reduce noise. The dataset specifications, such as number of documents and numbers of words, are
summarized in Table 1.
We also generate synthetic data to test the proposed scheme under controlled environments; we fix the number of
documents and words per document, while we change the percentage of nonzero elements in the documents. We
also change the number of latent classes to see the impact.
5.2. Experimental Environment
The hardware specification for the CPU version of our implementation is as follows: Intel Quad-core Xeon
E5620 processors with 6.4GT/s (Intel QuickPath Interconnect) and 8MB shared cache. We use C++ (GCC 4.4.1) to
implement the algorithms. The operating system is CentOS 5.4 and the GPU is Tesla C2050. It is Fermi architecture
with 3GB memory, 14 multiprocessors, and 32 cores per processor. It is connected by PCIe, whose speed is
144GB/s.
5.3. Experimental Results
5.3.1. Synthetic Data.

Using synthetic data, we conduct two different experiments. First, we test the impact of the sparseness against the
execution time with varying the number of non-zero elements in the occurrence matrix. We fix the number of latent
classes to 32, the occurrence matrix is of size 1000-by-5000, and the number of concurrent kernels is 16. In Figure 4
(a), we observe that the number nonzero elements affect the execution time.
In the second experiment, we measure the execution time varying the number of concurrent kernels. NVIDIA
allows at most 16 kernels to be executed concurrently. We show how parallelizing the class execution through
concurrent kernel execution is advantageous in terms of performance. We fixed the latent class to 32, the occurrence
matrix is of size 1000-by-5000, and the ratio of non-zero elements to 1%. Figure 4 (b) shows the overall execution
time. As it can be seen, the execution time decreases as the number of concurrent kernels increases. However, the
performance saturates when the number of concurrent kernels is 8.

Fig. 4. (a) Impact of Sparseness on execution time; (b) Execution time reduced by Concurrent kernel

Eli Koffi KOUASSI et al. / Procedia Computer Science 4 (2011) 382–391
Eli Koffi KOUASSI, Toshiyuki AMAGASA, Hiroyuki KITAGAWA/ Procedia Computer Science 00 (2011) 000–000

389

5.3.2. Real World Data
We execute both CPU and GPU version of PLSI varying the number of latent classes from 8 to 50. The execution
times, all in milliseconds, are summarized in Tables 2, 3, and 4. Figure 5 shows the speed up of GPU version over
the CPU version. Observe that GPU version is considerably faster than CPU version. As for the concurrent kernel
execution, due to the inefficient global-memory utilization, we cannot execute two or more kernels concurrently for
Med and CISI dataset (Tables 2 and 3). That is why the performance is slightly worse than non-concurrent version
due to some overhead to monitor the status of concurrent kernels. However, with CRAN dataset, we can execute 4
kernels concurrently, thereby obtaining even better performance than non-concurrent GPU version.
Table 2 Med dataset experimental results.
Latent Class

CPU[ms]

GPU[ms]

GPU w/concurrent kernel exec.[ms]

4

646.33

23.09

23.70

8

2357.67

44.21

45.69

16

6155.33

86.95

89.51

32

13967.00

167.73

172.59

50

26575.70

262.04

270.00

Table 3: CISI dataset experimental results
Latent Class

CPU[ms]

GPU [ms]

GPU w/concurrent kernel exec[msl.

4

907.67

30.89

31.31

8

3148.67

59.00

60.30

16

8261.67

116.66

118.34

32

19076.70

223.54

228.51

50

36209.70

349.33

357.80

Table 4 CRAN dataset experimental results
Latent Class

CPU[ms]

GPU Sparse[ms]

GPU w/concurrent kernel exec[ms].

4

511.00

31.88

23.77

8

2022.33

60.50

44.83

16

5673.00

119.21

88.26

32

13384.30

232.09

169.91

50

25685.00

362.77

267.21

390 	

Eli Koffi KOUASSI et al. / Procedia Computer Science 4 (2011) 382–391
Eli Koffi KOUASSI, Toshiyuki AMAGASA, Hiroyuki KITAGAWA/ Procedia Computer Science 00 (2011) 000–000

Fig. 5. Proposed Method Speed up. (a) Med dataset ; (b) CISI dataset; (c) CRAN dataset

6. Conclusion and Future Work
In this paper we showed a scheme of parallelizing PLSI for the execution using GPU. Our implementation, which
uses the matrix-vector implementation of EM, is more efficient both in memory and execution time. In the next we
showed how to use a data structure for sparse matrix representation in GPU. Concurrent execution of multiple
kernels is a feature available only in NVIDIA Fermi architecture, and we also mentioned how to make use of the
feature. Experimental evaluation showed that our GPU-based implementation is faster than CPU-based
implementation in the 1 to 2 orders of magnitude.
In the near future, we plan to deal with the memory utilization problem to accommodate more concurrent kernels,
as well as to be able to handle huge datasets. We also plan to execute the folding-in operations to make a full and
complete application of the parallel PLSI on GPGPU.
Acknowledgement
This work has been partly supported by the Grant-in-Aid for Scientific Research on Priority Areas by MEXT
(#21013004). The authors are also grateful to Eklou Kouakou Damien and Djelloul Boukhelef for their useful
comments to this paper.

References
[1] Scott Deerwester, Susan T. Dumais, George W. Furnas, Thomas K. Landauer, Richard Harshman, Indexing by
Latent Semantic Analysis. Journal of the American Society for Information Science 41 (6): 391–407 (1990)

Eli Koffi KOUASSI et al. / Procedia Computer Science 4 (2011) 382–391
Eli Koffi KOUASSI, Toshiyuki AMAGASA, Hiroyuki KITAGAWA/ Procedia Computer Science 00 (2011) 000–000

391

[2] Thomas Hofmann, Probabilistic Latent Semantic Indexing, Proceeding of the Twenty-Second Annual
International SIGIR Conference on Research and Developpment in Information Retrieval (1999).
[3] A.P. Dempster, N.M. Laird, D.B. Rubin, Maximization likelihood from incomplete data via EM algorithm,
Journal of the Royal Statistical Society. Series B(Methodological), Vol. 39, No. 1 (1977),pp.1-38
[4] R. Wan, V.N. Ahn, and H. Mamitsuka, Efficient Probabilistic Latent Semantic Analysis through Parallelization,
AIRS (2009), LNCS 5839, pp. 432-443,2009
[5] John D. Owens, D. Luebke, N. Govindaraju, M. Harris, J. Kruger J. Lefohn et al., A Survey of GeneralPurpose Computation on Graphics Hardware, Computer Graphics Forum, 26: 80-113(200’)
[6] Harris M.J., BaxerⅢ W., S.T., Lastra A., Simulation of cloud dynamics on graphics hardware, Graphics
Hardware 2003, pp.92-101 (2003)
[7] Lefohn A.E., Kniss J. M., Hansen C.D., Whitaker R.T. A stream narrow-band algorithm: Interactive computation
and visualization surfaces. IEEE Trans. Visual. Comput. Graph. pp.: 422-433 (2004).
[8] Yan R., Welch G., Fast Image segmentation and smoothing using commodity graphics hardware, J. Graph Tools.
7(4):91-100.
[9] Peter Bakkum and Kevin Skadron, Accelerating SQL Database Operations on a GPU using CUDA, 94-103.
(2010)
[10] Yongpeng Zhang, Franck Mueller, Xiaohui Cui, Thomas Potok, GPU-Accelerated Text Mining, EPHAM’09
(2009).
[11] N.K. Govindaraju, Jim Gray, Ritesh Kumar, Dinesh Manacha, GPGPUTeraSort: High Performance Graphics
Co-processor Sorting for Large Database Management, ACM-SIGMOD (2006)
[12] Ren Wu, Bin Zhang, M. Hsu, Clustering Billions of Data points Using GPUs. UCHPC-MAW (2009)
[13] J. M. Cavanagh, T. E. Potok, Xiaohui Cui, Parallel Latent Semantic Analysis using a Graphics Processing Unit,
GECC’2009, pp.2505-2509 (2009)
[14] CUBLAS, The NVidia SDK linear algebra
http://www.nvidia.com/content/cudazone/cuda_sdk/Linear_Algebra.html
[15] Hong C., Chen W., Shan J., Chen Y., Zhang Y. Parallelization and characterization of probabilistic latent
semantic analysis. In: Pro. 37th International Conference on Parallel Processing, pp. 628-635(2008)
[16] Guy E. Belloch, Vector Models for Data-Parallel Computing. The MIT Press (1990)
[17] Guy E. Belloch. Prefix Sums and Their Applications, “Synthesis of Parallel Algorithms", Edited by John H.
Reif, Morgan Kaufmann (1991).
[18] CUDPP home page: http://gpgpu.org/developer/cudpp
[19] N. Bell, M. Garland, Efficient Sparse Matrix-Vector Multiplication on CUDA. NVIDIA Technical Report
(2008)
[21] Training dataset download web site: ftp://ftp.cs.cornell.edu/pub/smart/

