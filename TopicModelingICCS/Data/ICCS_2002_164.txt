Techniques for Estimating the Computation and
Communication Costs of Distributed Data Mining•
Shonali Krishnaswamy1, Arkady Zaslavsky1, Seng Wai Loke2,
1
School of Computer Science and Software Engineering
900 Dandenong Road, Monash University, Caulfield East, Victoria –3145, Australia.
{shonali.krishnaswamy, arkady.zaslavsky}@csse.monash.edu.au
2
School of Computer Science and Information Technology
RMIT University, GPO Box 2476V, Melbourne, Victoria-3001, Australia.
swloke@cs.rmit.edu.au

Abstract. Distributed Data Mining (DDM) is the process of mining distributed
and heterogeneous datasets. DDM is widely seen as a means of addressing the
scalability issue of mining large data sets. Consequently, there is an emerging
focus on optimisation of the DDM process. In this paper we present cost
formulae for estimating the communication time in distributed data mining for
various scenarios. We also propose application run time estimation as a method
for determining the response times of the data mining computations. We have
developed a novel rough sets based algorithm for predicting application run
times. In this paper we present an overview of this technique and focus on its
application for estimating the computational cost of distributed data mining.

1 Introduction
Distributed data mining (DDM) addresses the specific issues associated with the
application of data mining in distributed computing environments. These
environments are typically characterised by the physical and/or geographical
distribution of users, data, hardware and mining software [1]. DDM is widely seen as
a means of addressing the scalability issue of mining large data sets. The mining of
small sub sets of data and merging the results is known to be more scalable than
mining a large data set [3]. There are predominantly two architectural models used in
the development of DDM systems, namely, client-server (CS) and software agents.
The “agents” category can be further divided on the basis of whether the agents have
the ability to migrate in a self-directed manner or not (i.e. whether the agents exhibit
the characteristic of mobility or not). The functionality implemented in each of the
architectural models is comparable in that distributed data sets are mined and the
•

THE WORK REPORTED IN THIS PAPER HAS BEEN FUNDED IN PART BY THE COOPERATIVE RESEARCH CENTRE PROGRAM THROUGH THE DEPARTMENT OF
INDUSTRY, SCIENCE AND TOURISM OF THE COMMONWEALTH GOVERNMENT
OF AUSTRALIA.

knowledge is integrated. However there are some fundamental differences in the
manner and location of the mining process.
The CS model is characterised by the presence of one or more data mining servers
[1]. In order to perform mining, the user requests’ are fed into the data mining server
which collects data from different locations and brings it into the data mining server.
The mining server houses the mining algorithms and is generally a high performance
server. The advantage of this model is that the DDM server has well defined
computational resources which has the ability to handle resource intensive data
mining tasks. However, this model for distributed data mining has a high
communication overhead as it involves transfer of huge quantities of data.
The agent-based, particularly involving mobile agents, is a popular approach to
constructing distributed data mining systems and is characterised by a variety of
agents co-ordinating and communicating with each other to perform the various tasks
of the data mining process. Mobile agent technology is seen as being able to address
the specific concern of increasing scalability and enhancing performance by moving
code instead of data and thereby reducing the communication overhead incurred in
the CS model [11]. However, the absence of dedicated data mining servers and the
lack of control over available computational resources at remote sites are limitations.
There is an emerging focus on efficiency and optimisation of response time in
distributed data mining [11][15][20. This can be attributed to two reasons. Firstly,
DDM aims to improve the scalability of mining large data sets. Scalability in turn is
intrinsically linked with better performance in terms of response times. Secondly, the
emergence of several Internet-based data mining service providers is driving the need
to improve the efficiency of the distributed data mining process [12]. Response time
is an important Quality of Service (QoS) metric in web-based data mining services for
both clients and service providers. The advantage for the clients is that it helps to
impose QoS constraints on the service level agreements and the benefit for the
service-providers is that it facilitates optimising resource utilisation and scheduling.
In general, the principle that governs the optimisation process is the need to reduce
the time taken to perform a distributed data mining task by:
1. Identifying factors that affect the performance in distributed data mining (such as
datasets, communication and processing)
2. Computing/assigning a “cost” (where cost has an inverse relationship with
performance) to those factors for alternate scenarios or strategies
3. Choosing a strategy that involves the least cost and thereby optimises the
performance
A significant issue in the success of an optimisation model is the computation of the
cost of different factors in the DDM process. In this paper, we present mathematical
cost models that facilitate identification of the different cost components in the DDM
process for various strategies such as client-server and mobile agents. These cost
models form the basis for apriori estimation of the response time for a given task. In
conjunction with the cost models, we have developed a technique for estimating the
run time of data mining tasks (which is one of the cost components in the DDM
process). The experimental evaluation of this technique establishes its estimation
accuracy and validity.
In the following sections of this paper, we present cost models for distributed data
mining including cost formulae for estimating the communication time and our

technique for apriori estimation of the run time of data mining algorithms. The paper
is organised as follows. In section 2 we review related work in the field of distributed
data mining with a particular focus on optimisation techniques. Section 3 describes
the cost components in different DDM techniques and presents the cost formulae for
estimating these components. In section 4, the experimental results of our data mining
task run time estimation technique are analysed. Finally, in section 5, we conclude by
discussing the current status of our project and the future directions of this work.

2 Related Work
Research in distributed data mining can be divided into two broad categories:
development of distributed data mining algorithms which focus on efficient
techniques for knowledge integration and development of distributed data mining
architectures which focus on the processes and technologies that support construction
of software systems to perform distributed mining.
The core concept of distributed data mining algorithms is that each local data set is
mined individually and the results obtained are combined to produce a global model
of the data. Several surveys of distributed data mining algorithms and knowledge
integration techniques have been written including [4]. Some studies, such as [7],
focus on parallel and distributed data mining algorithms, where, parallel implies
parallelisation of sequential data mining algorithms.
Several DDM systems using either agent-based architectures or the client-server
model have been proposed. Systems that use the agent paradigm include: Parallel
Data Mining Using Agents (PADMA) [8], InfoSleuth [14], Java Agents for MetaLearning (JAM) [19], Besizing Knowledge through Distributed Heterogeneous
Induction (BODHI) [9], Papyrus [17] and Distributed Agent-based Mining
Environment (DAME) [12]. DDM systems based on the client-server paradigm
include DecisionCentre [1] and IntelliMiner [15].
Work on improving performance of DDM systems by using an optimal/costefficient strategy has been the focus of [15][20]. IntelliMiner [15] is a client-server
distributed data mining system, which focuses on scheduling tasks between
distributed processors by computing the cost of executing the task on a given server
and then selecting the server with the minimum cost. The cost is computed based on
the resources (i.e. number of data sets) needed to perform the task. For example, if a
server A requires datasets X and Y and another server B already has dataset X and
only requires Y, then the second server is preferred. Thus, the cost of performing a
task on a server is the number of resources that it has to acquire in order to compute
the given task. While this model takes into account the overhead of communication
that is increased by having to transfer more datasets it ignores several other cost
considerations in the DDM process such as processing cost and size of datasets.
Consider the following cases with respect to the above example:
1. Dataset X is fairly small and dataset Y is very large. Should server A then
necessarily have a higher cost assigned to it and server B be chosen?
2. Server A has considerably better computational power than server B. Should server
B still be chosen as the preferred server?

In [20], the optimisation is motivated by the consideration that mining a dataset
either locally or by moving the entire dataset into a different server is a naïve
approach. They use a linear programming approach to partition datasets and allocate
the partitions to different servers. The allocation is based on the cost of processing
(which is assigned manually in terms dollars) and the cost of transferring data (which
is also assigned manually in terms of dollars). The manual assignment of cost requires
expert knowledge about different factors that affect the DDM process and
quantification of their respective impact, which is a non-trivial task.
Thus the computation of the cost is an important question for any optimisation
model. In the models discussed, the cost is either assigned manually or computed
using a simple technique, which only takes into account the availability of datasets. In
the following sections of this paper, we present a technique for computing the cost
based on the response time for different mining strategies. We use apriori estimates of
the response time for factors such as communication and processing.

3 Cost Components in the Distributed Data Mining Process
In general, optimisation models in DDM attempt to reduce the response time by
choosing strategies that facilitate faster communication and/or processing. We
propose apriori estimates of the response time as the basis for computing the cost of
distributed data mining. In this section we describe the different cost components of
the DDM process and present cost formulae for estimating their response times. The
response time of a DDM task broadly consists of three components:
1. Communication: The communication time is largely dependent on the DDM model
and varies depending on whether the task is performed using a client server
approach or using mobile agents. In the client server model the communication
time is principally the time taken to transfer data from distributed servers to a high
performance machine where the data mining is performed. In the mobile agent
model, the communication time revolves around the time taken to transfer mobile
agents carrying data mining software to remote datasets and the time taken to
transfer results from remote locations for integration.
2. Data Mining. This is the time taken to perform data mining on the distributed data
sets and is a core factor irrespective of the DDM model.
3. Knowledge Integration. This is the time taken to integrate the results from the
distributed datasets.
The response time for distributed data mining is as follows:
T = tddm + tki
(1)
In (1), T is the response time, tddm is the time taken to perform mining in a distributed
environment and tki is the time taken to perform knowledge integration. The response
time for tddm in turn can be represented as:
tddm = tdm + tcom
(2)
In (2), tdm is the time taken to perform data mining and tcom is the time involved in the
communication. Depending on the model used for distributed data mining (i.e. mobile
agent or client server) and the different scenarios within each model, the factors such
as communication that determine tddm will change. This results in a consequent

change in the actual cost function that determines tddm. The modelling and estimation
of the knowledge integration (tki) variable in Eq. (1) is non-trivial as it depends on
several unknown factors such as the size and type of results produced by data mining.
In fact, one of the primary reasons or rationales for data mining is the discovery of
hidden and unknown patterns in the data. Thus, we do not present cost formulae or
estimation techniques for the response time of knowledge integration.
This paper focuses on the estimation of tddm. In the following discussion we present
the different distributed data mining scenarios and the cost functions to determine the
response time tddm for each case.
3.1 Estimating the Communication Cost
The communication cost in the DDM process varies depending on whether the
client-server strategy is followed or the mobile agent model is used.
3.1.1 Mobile Agent Model
This case is characterised by a given distributed data mining task being executed in
its entirety using the mobile agent paradigm. The core steps involved include:
submission of a task by a user, dispatching of mobile agent (or agents) to the
respective data server (or servers), data mining and the return of mobile agent(s) from
the data resource(s) with mining results. This model is characterised by a set of
mobile agents traversing the relevant data servers to perform mining. In general, this
can be expressed as m mobile agents traversing n data sources. There are three
possible alternatives within this scenario. The first possibility is m = n, where the
number of mobile agents is equal to the number of data servers. This implies that one
data mining agent is sent to each data source involved in the distributed data mining
task. The second option is m < n, where the number of mobile agents is less than the
number of data servers. The implication of having fewer agents than servers is that
some agents may be required to traverse more than one server. We do not consider
the third case of m > n since this is in effect equivalent to the case 1 above where
there is a mobile agent available per data server. Each of the above alternatives has its
own cost function. These cost models are described as follows.
Case 1. Equal number of mobile agents and data servers (m=n).
This is a case where data mining from different distributed data servers is
performed in parallel. The algorithm used across the different data servers can be
uniform or varied. The system dispatches a mobile agent encapsulating the data
mining algorithm (with the relevant parameters) to each of the data servers
participating in the distributed data mining activity.
Let n be the number of data servers. Therefore, the number of mobile agents is n
(since m=n). In order to derive the cost function for the general case involving n data
servers and n data mining agents, we first formulate the cost function for the case
where there is one data server and one data mining agent.
Let us consider the case where data mining has to be performed at the ith data
server (i.e 1≤ i ≤ n ). The cost function for the response time to perform distributed
data mining involving the ith data server is as follows:

tddm= tdm(i) + tdmAgent(AC, i) + tresultAgent(i,AC)
The communication terms in the above cost estimate are are tdmAgent(AC, i) and
tresultAgent(i,AC). That is,
tcom= tdmAgent(AC, i) + tresultAgent(i,AC)
(3)
tdmAgent(AC, i). In our cost model, the representation tmobileAgent(x, y) refers to the
time taken by the agent mobileAgent to travel from node x to node y. Therefore
tdmAgent(AC, i) is the time taken by the mobile agent dmAgent (which is the agent
encapsulating the mining algorithm and the relevant parameters) to travel from the
agent centre (AC) to the data server (i). In general, the time taken for a mobile agent
to travel depends on the following factors: the size of the agent and the bandwidth
between nodes (e.g. in kilobits per second). The travel time is proportional to the size
of the agent and is inversely proportional to the bandwidth (i.e. the time taken
increases as the agent size increases and decreases as the bandwidth increases). This
can be expressed as follows:
tdmAgent(AC, i) ∝ size of dmAgent
(4)
tdmAgent(AC, i) ∝ 1 / bandwidth
(5)
From (4) and (5):
tdmAgent(AC, i) = ( k * size of dmAgent ) / (bandwidth between AC and i)
In the above expression for the time taken by the data mining agent to travel from
the agent centre to the data server, k is a constant. In [11] the size of an agent is given
by the following triple,
size of an agent = < Agent State, Agent Code, Agent Data>
where, agent state is the execution state of the agent, agent code is the program
that is encapsulated within the agent that performs the agent’s functionality and agent
data is the data that the agent carries (either as a result of some computation
performed at a remote location or the additional parameters that the agent code
requires). On adapting the above representation to express the size of the data mining
agent (dmAgent), we now have,
size of an dmAgent = <dmAgent state, data mining algorithm, input
parameters>
tresultAgent(i, AC). This is the time taken for the data mining results to be transferred
from the data server (i) to the agent centre (AC). However, estimating this component
apriori is not feasible as the size of results obtained from a data mining task is usually
unknown.
We now extend the cost estimate for the general case characterised by n mobile
agents and n distributed data sources. Let there be n data sources that need to be
accessed for a particular distributed data mining exercise. Thus, n mobile agents
encapsulating the respective mining algorithms and parameters (i.e. one to each of the
data sources) are dispatched concurrently. Mining is performed at each of the sites in
parallel and the results are returned to the central server. Since the mining is
performed at the distributed locations concurrently, the total communication cost is
equal to the time taken by the mobile agent that takes the longest time to travel to its
respective remote location. Therefore,
tcom = max(tdmAgent(AC, i)), where i = 1..n
(5)

Case 2. Fewer mobile agents than data servers (m<n).
This is the second case, where the number of mobile agents (m) available for
distributed data mining is less than the number of data sources (n) participating in a
distributed data mining task (i.e. m < n). In order to derive the cost formula for the
general case involving m data mining agents and n data sources, we first develop the
cost model for the situation involving one agent and n servers.
The principal difference between this scenario (where there are lesser agents than
data sources) and the previous case (where there was one agent/data source) is that in
the latter distributed data mining is performed concurrently in all the data sites. In the
former, certain mobile agents are assigned to more than one data site for mining.
Hence such an agent must travel to all the specified servers where it has to mine. We
make an assumption that the mining agent returns to the agent centre only after it has
accomplished its task in all the respective data sources assigned to it and that results
are sent to the central server directly. This allows us to estimate the communication
cost more effectively.
Before considering the general case of m agents and n data sources, we first
develop the cost model for the situation where there is only one mobile agent for n
distributed data sites. The agent is dispatched from the agent centre to the first of the n
data sites. From there on, the agent completes the mining task at each site and travels
to the next data site and so on, until each of the n sites have been mined. The mining
agent then finally returns to the central server. Therefore, the communication cost in
this scenario is expressed in equation (7) as follows:

t com = t

dmAgent

(AC, 1) +

n - 1 tdmAgent (i, i + 1) + t dmAgent (n, AC)
i =1

In the above cost estimate, the term tdmAgent(AC, 1) is the time taken for the data
mining agent to travel from the agent centre to the first data site (the data servers
being numbered from 1 through n). The second term is the time taken to travel
iteratively from server i to server i+1 as i ranges from 1 to (n-1). The final term is the
time taken for the for the agent to return to the central server from the nth data site.
We now extend the above cost estimate for the general case involving m mobile
agents and n DDM sites. The n data sites are numbered from 1 through n. Let ds be
labelling of a set of data sites. Therefore ds = { 1,2,…,n}. Let m be the number of data
mining agents available. The set ds is divided into m subsets – ds1, ds2,… ,dsm with the
following property: dsi ⊆ ds (i.e. dsi ⊆ {1,2,..., n} , 1 ≤ i ≤ m). The sets dsi (1 ≤ i ≤
m ) have the additional property of independence. That is, dsi ∩ dsj = φ, i ≠ j. A
corollary of the above is that the sum of the cardinalities of the subsets ds1, ds2,… ,dsm
is equal to the cardinality of ds. This implies that |ds1| + |ds2| +… + |dsm| = |ds|.
Thus, the n data servers are divided into m subsets and the ith data mining agent is
assigned the task of mining the data sites in the subset dsi (where 1 ≤ i ≤ m ).
Distributed data mining can occur concurrently at one level (i.e. there are m different
mining agents operating at the same time). However, each of these m agents could be
assigned several sites to mine (i.e. the agent has to travel to different sites). Thus the
ith agent (where 1 ≤ i ≤ m) has to travel to and perform mining in the number of sites
specified in |dsi|. The total time taken to mine is therefore the time taken by the agent,
which takes the maximum time interval to complete its task. The communication cost
estimate for the ith agent’s response time is as follows in equation (8):

t com (i)

= t

t dmAgent

(ds

dmAgent

i

(AC,

ds i (1))

( ds ), AC)

+

− 1
t dmAgent
j = 1

ds

i

(j,

j + 1)

+

i

In the above expression, the first term is the time taken by the data mining agent to
travel from the agent centre to the first data server in its path (i.e. the ith subset dsi).
The term involving the summation is the time taken for the agent to travel to the
respective data sites within the set assigned to it (excluding the final site). The second
last term is the time taken to mine at the last data site in its path. The final term in the
expression is the time taken for the agent to travel from the last site on its path to the
agent centre.
Since there are m agents operating concurrently, the time tcom is the time taken by
the agent requiring the longest travel time. Thus,
tcom = max(tcom(i) ), i = 1..m
(9)
In (9), tcom(i) is estimated from equation (8).
3.1.2 Client Server Model
The communication cost formulae for the response time in DDM systems that use
the traditional client-server paradigm is presented in this section. Typically, data
from distributed sources is brought to the data mining server – a fast, parallel server and then mined. Let there be n data sites from which data has to be mined. Let si be
the data set obtained from the ith site (where 1 ≤ i ≤ n). The communication time for
DDM for the data set si from the ith site is as expressed in equation (11) as follows:
tcom(i) = tdataTransfer(i,DMS, si), 1 ≤ i ≤ n
(10)
The term tdataTransfer(i,DMS, si) is the time taken to transfer the data set (si) from the ith
site to the DDM server (DMS) and is estimated as follows:
tdataTransfer(i,DMS, si) = size of si / ( bandwidth between i and DMS )
(11)
The data transfer can be a significant addition when the data volumes are large and/or
the bandwidth is low. In this section, we have presented the cost formulae for the
communication component of the DDM process for different mining strategies. We
now present our techniques for estimating the cost of performing data mining.

3.2 Estimating the Data Mining Cost
Regardless of the DDM strategy adopted, there is a need to estimate the time taken
to perform data mining (tddm) either at a remote location using mobile agents or at a
centralised server using the client server paradigm. In this section, we present
application run time estimation techniques as an approach to estimating apriori the
time taken to perform data mining. We have developed a novel rough sets based
approach to estimating the run time of applications [13].
The motivation for our work in this area comes from application run time
estimation techniques proposed by [18]. Application run time prediction algorithms
including [2][5][18] operate on the principal that applications that have similar
characteristics have similar run times. Thus, a history of applications that executed in
the past along with their respective run times is maintained in order to estimate the

task run time. Given an application for which the run time has to be predicted, the
steps involved are:
1. Identify “similar” applications in the history
2. Compute a statistical estimate (e.g. mean, linear regression) of the run times of the
“similar” applications and use this as the predicted run time.
The fundamental problem is the definition of similarity. There can be diverse
views on the criteria that make two applications similar. For instance, it is possible to
say that two applications are similar because the same user on the same machine
submitted them or that two applications are similar because they have the same
application name and are required to operate on data of the same size. Thus, as
discussed by [18], the basic question that needs to be addressed is the development of
techniques that can effectively identify similar applications. Such techniques must be
able to accurately choose the attributes of applications that best determine similarity.
The obvious test for such techniques is the prediction accuracy of the estimates
obtained by computing a statistical estimate of the run times of the applications
identified as being similar. Thus, the closer the predicted run time is to the actual run
time, the better is the prediction accuracy of the technique. Several statistical
measures can be used for computing the prediction (using the run times of similar
applications that executed in the past) including measures of central tendency. Studies
by [18] have shown that the mean performs very well as a predictor.
Early work in this area by [2][5] proposed the use of “similarity templates” of
application characteristics to identify similar tasks in the history. A similarity
template is a set of attributes that are used as the basis for comparing applications in
order to determine if they are similar or not. Thus, for histories recorded from parallel
computer workloads, [2] selected the queue name as the characteristic to determine
similarity. Applications that were assigned to the same queue were deemed similar. In
[5] several templates were used for the same history including: (user, application
name, number of nodes and age) and (user, application name). Thus, in order to
estimate the run time for a new task, the following steps are performed:
1. The template to be used is defined/selected
2. The history is partitioned into different categories where each category contains
applications which have the same values for the attributes specified in the template.
That is, if the template used is (user, application name), then tasks that had the
same user and the same application name are placed into a distinct category.
3. The current task (whose run time has to be predicted) is matched against the
different categories in the history to determine which set it belongs to.
The run time is predicted using the run times of the applications in the similar
category. This technique obviously has the limitation of requiring a sufficient history
in order to function. It was proposed by [18] that manual selection of similarity
templates had the following limitations:
• It is not always possible to identify the characteristics that best determine similarity
• It is not generic. Thus, while a particular set of characteristics may be appropriate
for one domain, it is not always applicable to other domains.
They proposed automated definition and search for templates and used genetic
algorithms and greedy search techniques. They were able to obtain improved
prediction accuracy using these techniques. For a detailed description of the template
identification and search algorithms readers are referred to [18].

We have developed a rough sets based algorithm to address the problem of
automatic selection of characteristics that best define similarity to estimate application
run times. Rough sets provide an intuitively appropriate theory for identifying good
“similarity templates” (or sets of characteristics on the basis of which applications can
be compared for similarity). For a detailed explanation of the theoretical soundness of
a rough sets based approach to identify similarity templates and the algorithm itself,
readers are referred to [13]. In [13], we have established the improved prediction
accuracy of our approach. In this paper, we present a brief overview of our algorithm
for use as a means to costing the data mining component in the DDM process.
3.2.1 Rough Sets Algorithm for Estimating tdm
A data set in rough sets is represented as a table called Information System, where
each row is an object and each column is an attribute. The attributes are partitioned
into condition attributes and decision attributes. The condition attributes determine the
decision attribute. The history, as shown in figure 1, is a rough information system,
where the objects are the previous applications whose run times (and other properties)
been recorded.

Figure 1: A Task History Modelled as a Rough Information System
The attributes in the information system are the properties about the applications
that have been recorded. The decision attribute is the application run time that has
been recorded. The other properties that have been recorded constitute the condition
attributes. This model of a history intuitively facilitates reasoning about the recorded
properties so as to identify the dependency between the recorded attributes and the
run time. Thus, it is possible to concretise similarity in terms of the condition
attributes that are relevant/significant in determining the decision attribute (i.e. the run
time). Thus, the set of attributes that have a strong dependency relation with the run
time can form a good similarity template. The fact that rough sets operate entirely on
the basis of the data that is available in the history and require no external additional
information is of particular importance as the lack of such information (beyond
common sense and intuition) was the bane of manual similarity template selection

techniques. Having cast the problem of application run time as a rough information
system, we now examine the fundamental concepts that are applicable in determining
the similarity template.
Degree of Dependency. Using rough sets it is possible to measure the degree of
dependency between two sets of attributes. The measure takes values [0,1] and higher
values represent stronger degrees of dependency. It is evident that the problem of
identifying a similarity template can be stated as identifying a set of condition
attributes in the history that have a strong degree of dependency with the run time.
This measure computes the extent of the dependency between a set of condition
attributes and the decision attribute and therefore is an important aspect of using
rough sets for identifying similarity templates.
Significance of Attributes. The significance of the attribute is the extent by which
the attribute alters the degree of dependency between a set of condition and decision
attributes. If an attribute is “important” in discerning/determining the decision
attribute, then its significance value, which is measured in the range [0,1], will be
closer to 1. The similarity template should consist of a set of properties that are
important for determining the run time. The significance of an attribute allows
computing the extent to which an attribute affects the dependency between a set of
condition attributes and a decision attribute. This measure allows quantification of the
impact of individual attributes on the run time, which in turn results in identification
and consequent elimination of attributes that do not impact on the run time and
therefore should not be the basis for comparing applications for similarity.
Reduct. A reduct consists of the minimal set of condition attributes that have the
same discerning power as the entire information system. All superfluous attributes are
eliminated from a reduct. According to [10], while it is relatively simple to compute a
single reduct, the general solution for finding all reducts is NP-hard. A similarity
template should consist of the most important set of attributes that determine the run
time without any superfluous attributes. In other words, the similarity template is
equivalent to a reduct which has the most significant attributes included.
It is evident that rough sets theory has highly suitable and appropriate constructs
for identifying the properties that best define similarity for estimating application run
time estimation. A similarity template should have the following properties:
• It must include attributes that have a significant impact on the run time
• It must eliminate attributes that have no impact of the run time
This ensures that the criteria on which applications are compared for similarity
have a significant bearing in terms of determining run time. As a consequence,
applications that have the same characteristics with respect to these criteria will have
similar run times. We have informally discussed the aspects of rough theory that are
important for identifying similarity templates. We now present our rough sets
algorithm for identifying similarity templates in figure 2.
Our technique for applying rough sets to identify similarity templates centres round
the concept of a reduct. A reduct by definition is a set of condition attributes that are
minimal (i.e. contain no superfluous attributes) and yet preserve the dependency
relation between the condition and decision attributes by having the same
classification power as the original set of condition attributes. As explained before,
conceptually a reduct is equivalent to a similarity template in terms of comprising

attributes that have the best ability to determine the run time and thereby are best
suited to form the basis for determining similarity.
1.

Let A={a1, a2,….,an}be the set of condition attributes and D be the set of decision
attributes.
2. Let C be the D-Core
3. REDUCT = C
4. A1 = A – REDUCT
5. Compute the Significances of the Attributes (SGF) in A1 and sort them in
ascending order
6. For i = |A1| to 0
K(REDUCT, D) = K(REDUCT, D) + SGF(ai)
If K(REDUCT,D) = K(A,D)
REDUCT = REDUCT ∪ ai
Exit
End If
K(REDUCT, D) = K(REDUCT, D) - SGF(ai)
End For
7. K(REDUCT, D) = K(REDUCT, D) + SGF(a|A1|)
8. While K(REDUCT,D) is not equal to K(A,D)
Do
REDUCT = REDUCT ∪ ai (where SGF(ai) is the highest of
the attributes in A1)
A1 = A1 - ai
Compute the degree of dependency K(REDUCT,D)
End
9. |REDUCT| -> N
10. For i = 0 to N
If ai is not in C (that is the original set of attributes of the
REDUCT at the start and SGF(ai) is the least)
Remove ai from REDUCT
End If
Compute degree of dependency K(REDUCT, D)
If K(REDUCT, D) not equal to K(A,D)
REDUCT ∪ ai -> REDUCT
End If
11. End For
12. End if

Figure 2: Reduct Generating Algorithm
An information system can have several reducts and there are several techniques
for computing reducts [10]. In computing a reduct for use as a similarity template in
application run time estimation we require the reduct to consist of attributes that are
“significant” with respect to the run time. For this purpose, we use a variation of the
reduct generation algorithm proposed by [6]. The algorithm proposed by [6] was
intended to produce reducts that included user specified attributes. It computes
reducts by iteratively using the most significant attribute. The modified algorithm we
use to compute the reduct for use as a similarity template is shown in figure 2. This
algorithm treats the D-core (note: the core of a rough information system is the
intersection of all reducts and the D-core is the core with respect to the set of decision
attributes) as the initial attribute set of the reduct. The attribute significances for the
remaining attributes are then computed and sorted in ascending order. Each attribute
(starting with the most significant) is added to the initial reduct and the value of the
dependency between the initial reduct and the decision attribute is incremented by the

SGF value of the attribute that is under consideration. This identifies any reduct of
size |D-core + 1|. This is one of the variations we introduce to the original algorithm
developed by [4]. From our experiments we found that it is not unusual for the D-core
to combine with the a single attribute (typically the most significant attribute) to form
a reduct. This step identifies reducts of size |D-core + 1|. This iteration is
computationally inexpensive as it involves only simple additions and no analysis.
Thus, if a reduct of size |D-core + 1| exists, we find it without further computation and
since we start with the most significant attribute, we attempt to find reducts that
involve attributes with the highest significances. In the absence of such a reduct, the
most significant attribute is added to the initial reduct and the dependency is
computed as discussed. Then attributes (starting from the next most significant one)
are iteratively added to the “new” reduct and the dependency between the new reduct
and the decision attributes is re-computed. This process of re-computation of the
dependency is expensive as it involves finding equivalence classes and the previous
iteration we initiated was principally an attempt to avoid it. This is done until the
dependency between the reduct (which is constantly growing due to the addition of
attributes) and the decision attributes is the same as the dependency between the
condition attributes and the decision attributes. Thus, at the end of the iteration, the
reduct is the set of attributes, which have some significance (or are useful in
determining the decision attributes). However, at this stage the reduct might not be
minimal – as it could contain attributes that may be redundant. However, reducts have
to be minimal and so inorder to eliminate superfluous attributes, the algorithm
removes an attribute and checks whether the removal changes the dependency. A
superfluous attribute will not affect the dependency and can be removed. The
computationally most expensive component of this algorithm is the need to determine
equivalence classes several times since this is necessary to determine the degree of
dependency. As an implementation enhancement, we have introduced the notion of an
“incremental equivalence class”, which enables us to reduce the number of
comparisons that have to be made when computing equivalence classes.
We have discussed how important it is to be able to compute the cost of processing
and communication for any optimisation model. We have presented our rough sets
approach to application run time estimation techniques as a way of estimating the cost
of processing as it allows prediction of the time taken to perform mining at a given
location. Obviously, the success of such a technique is dependent on the prediction
accuracy. In the following section of the paper we present experimental results of our
technique to estimate the run time of data mining tasks.

4 Experimental Results and Analysis
We have presented cost formulae for estimating the communication time in
distributed data mining and identified our rough sets based application run time
techniques as suitable for estimating the time taken to perform data mining, thereby
presenting theoretical models for computing the cost of performing DDM in terms of
response time. However, the viability of using these cost models in a DDM optimiser
depends on the accuracy of the estimation techniques. Thus, the estimated

communication times and predicted data mining task run times must be close to actual
run time for these cost formulae to form the basis for effective optimisation. There are
two questions that need experimental validation:
1. The accuracy of the cost formulae for estimating the communication time for
mobile agent and client server strategies.
2. The accuracy of the run time estimation algorithm for predicting the time taken to
perform data mining.
At this stage we are implementing mobile agents to perform distributed data mining
and do not have experimental results to validate the communication cost models.
However, we have implemented our rough sets algorithm and in this section present
results from experiments on estimating the run times of data mining tasks.
We compiled a history of data mining tasks by running several data mining
algorithms on a network of distributed machines and recording information about the
tasks and the environment. We executed several runs of data mining jobs by varying
the parameters of the jobs such as the mining algorithm, the data sets, the sizes of the
data sets and the machines on which the tasks were run. The algorithms used were
from the WEKA package of data mining algorithms [21]. We generated several data
sets of sizes varying from 1MB to 20MB. The data mining jobs were executed on
three distributed machines with different physical configurations and operating
systems. Two machines had Windows 2000 and one machine had Sun OS 5.8. One of
the windows machines was a Pentium III with 833 Mhz processor and 512 MB
memory, while the other was a Pentium II with 433 Mhz processor and 128 MB
memory. The third machine was a Sun Sparc with 444 Mhz processor and 256 MB
memory. The rational for building a history using a distributed network of nodes was
twofold. Firstly we wanted to obtain a diverse history and test the prediction accuracy
given a varied history. Secondly, it represents a realistic scenario for DDM, where a
distributed network of servers would be used.
For each data mining job, the following information was recorded in the history:
the algorithm, the file name, the file size, the operating system, the version of the
operating system, the IP address of the local host on which the job was run, the
processor speed, the memory, the start and end times of the job. Currently we record
only static information about the machines, however we are currently implementing a
feature to enable recording dynamic information such as memory usage and CPU
usage. The history was used to conduct experiments using the process described in
section 3. We used histories with 100 and 150 records and as before each
experimental run consisted of 20 tests. The mean error we recorded was 0.34 minutes
and the mean error as a percentage of the mean run time was 8.29. The mean error is
less than a minute and the error as a percentage of the actual run times is also very
low, which indicates that we obtained very good estimation accuracy for data mining
tasks. This good performance accuracy is illustrated in figure 3, which presents the
actual and estimated run times from one of our experimental runs. The reduct that our
algorithm selected as a similarity template included the following attributes:
algorithm, file name, file size and operating system. It must be noted that we used an
automated data set generator to generate files of different sizes and the file names
recorded included the full path.

10
9
8
7
6
5
4
3
2
1
0

ESTIMATE

19

17

15

13

9

11

7

5

3

ACTUAL

1

Run Time (Minutes)

Comparison of Actual and Estimated Run
Times

Experiment #

Figure 3: Actual Vs. Estimated Run Times
We have presented experimental results that demonstrate the performance
accuracy of our rough sets algorithm for estimating application run times of data
mining tasks. We are currently focussing on incorporating dynamic system
information into the history and also experimenting on larger histories.

5 Conclusions and Future Work
This paper has focussed on computing the cost of the distributed data mining
process by estimating the response time of the communication and computational
components. Accurate costing is vital for optimisation of the DDM process and we
address the need for good cost estimation techniques. We have developed cost
formulae for estimating the response time of the communication for different
strategies (including client server and mobile agent techniques) and a rough sets based
application run estimation algorithm for predicting the run times of data mining tasks.
We have experimentally validated the prediction accuracy of our rough sets
algorithm, which is shown to have low mean errors and high accuracy. The cost
models presented in this paper are generic and can be applied to any optimisation
strategy. We are currently implementing the mobile agent model for DDM so as to
experimentally validate the cost formulae for estimating the communication time.
Future directions of this work include the development of a DDM optimiser based on
the cost models presented in this paper.

References
1.Chattratichat,J., Darlington, J., Guo,Y., Hedvall,S., Köhler,M., and Syed,J., (1999), “An
Architecture for Distributed Enterprise Data Mining”, in Proceedings of the 7th
International Conference on High Performance Computing and Networking (HPCN
Europe’99), Amsterdam, The Netherlands, Springer-Verlag LNCS 1593.

2.Downey,A,B., (1997), “Predicting Queue Times on Space-Sharing Parallel Computers”,
Proc. of the 11th Intl. Parallel Processing Symposium (IPPS), Geneva, Switzerland, April.
3.Freitas,A., and Lavington,S., (1996), “Parallel Data Mining for Very Large Databases”,
Proceedings of the International Conference on High-Performance Computing and
Networking (HPCN’96), Brussels, April, Springer-Verlag, LNCS 1067, pp.158-163.
4.Fu,Y., (2001), “Distributed Data Mining: An Overview”, in Newsletter of the IEEE
Technical Committee on Distributed Processing, Spring 2001, pp.5-9.
5.Gibbons,R., (1997), “A Historical Application Profiler for Use by Parallel Schedulers”,
Lecture Notes in Computer Science (LNCS), 1291, Springer-Verlag, pp.58-75.
6.Hu,X., (1995), “Knowledge Discovery in Databases: An Attribute-Oriented Rough sets
Approach”, PhD Thesis, University of Regina, Canada.
7.Kamath,C., (2001), “The Role of Parallel and Distributed Processing in Data Mining”, in
Newsletter of the IEEE Technical Committee on Distributed Processing, Spring 2001,
pp.10-15.
8.Kargupta,H., Hamzaoglu,I. and Stafford,B., (1997b), “Scalable, Distributed Data Mining
Using An Agent Based Architecture”, in Proceedings of the Third International Conference
on Knowledge Discovery and Data Mining, Newport Beach, California, (eds), David
Heckerman, Heikki Mannila, Daryl Pregibon, and Ramasamy Uthurusamy, AAAI Press, pp.
211-214.
9.Kargupta,H., Park,B., Johnson,E., Riva Sanseverino,E., Di Silvestre,L., and Hershberger,D.,
(1998), “Collective Data Mining From Distributed Vertically Partitioned Feature Space”, in
KDD-98 Workshop on Distributed Data Mining, New York, USA, AAAI Press.
10.Komorowski,J., Pawlak,Z., Polkowski,L., and Skowron, A., (1998), “Rough sets: A
Tutorial”, in Rough-Fuzzy Hybridization: A New Trend in Decision Making, (eds) S.K.Pal
and A.Skowron, Springer-Verlag, pp. 3-98.
11.Krishnaswamy,S., Loke,S,W., & Zaslavsky,A. (2000b). “Cost Models for Heterogeneous
Distributed Data Mining”, Proc. of the Twelfth Intl. Conf. on Software Engineering and
Knowledge Engineering (SEKE), pp.31-38.
12.Krishnaswamy,S., Zaslavsky,A., & Loke,S,W. (2000a), “An Architecture to Support
Distributed Data Mining Services in E-Commerce Environments”, Proc. of the Second Intl.
Workshop on Advanced Issues in E-Commerce and Web-Based Information
Systems(WECWIS), pp.238-246.
13.Krishnaswamy,S., Loke,S,W., & Zaslavsky,A, (2002a), “Application Run Time Estimation:
A Quality of Service Metric for Web-based Data Mining Services”, To Appear in ACM
Symposium on Applied Computing (SAC 2002), Madrid, March.
14.Martin,G., Unruh,A., and Urban,S., (1999), “An Agent Infrastructure for Knowledge
Discovery and Event Detection”, Technical Report MCC-INSL-003-99, Microelectronics
and Computer Technology Corporation (MCC).
15.Parthasarathy,S., and Subramonian,R., (2001), “An Interactive Resource-Aware Framework
for Distributed Data Mining”, in Newsletter of the IEEE Technical Committee on
Distributed Processing, Spring 2001, pp.24-32.
16.Pawlak,Z.,(1982), “Rough sets”, International Journal of Computer and Information
Sciences, Issue. 11, pp. 413-433.
17.Ramu,A,T., (1998), “Incorporating Transportable Software Agents into a Wide Area High
Performance Distributed Data Mining Systems”, Masters Thesis, University of Illinois,
Chicago, USA.
18.Smith,W., Taylor,V., and Foster,I.,(1999), "Using run-time predictions to estimate queue
wait times and improve scheduler performance", Lecture Notes in Computer Science
(LNCS), 1659, Springer-Verlag, pp.202-229.
19.Stolfo,S,J., Prodromidis,A,L., Tselepis, L., Lee,W., Fan,D., and Chan,P,K., (1997), “JAM:
Java Agents for Meta-Learning over Distributed Databases”, in Proceedings of the Third
International Conference on Data Mining and Knowledge Discovery (KDD-97), Newport

Beach, California, (eds) David Heckerman, Heikki Mannila, Daryl Pregibon, and
Ramasamy Uthurusamy, AAAI Press, pp. 74-81.
20.Turinsky,A., and Grossman,R., (2000), “A Framework for Finding Distributed Data Mining
Strategies that are Intermediate between centralized Strategies and In-place Strategies”,
Workshop on Distributed and Parallel Knowledge Discovery at KDD-2000, Boston, pp.1-7.
21.Witten,I,H., and Eibe,F., (1999), “Data Mining: Practical Machine Learning Tools and
Techniques with Java Implementations”, Morgan Kauffman.

