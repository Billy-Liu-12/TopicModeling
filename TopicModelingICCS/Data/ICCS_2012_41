Available online at www.sciencedirect.com

Procedia Computer Science 9 (2012) 1817 – 1826

International Conference on Computational Science, ICCS 2012

Data parallel skeletons in Java
Herbert Kuchen, Steﬀen Ernsting1,∗
Department of Information Systems
University of Muenster
Leonardo-Campus 3
48149 Muenster, Germany

Abstract
In the past years, multi-core processors and clusters of multi-core processors have emerged to be promising approaches to meet the growing demand for computing performance. They deliver scalable performance, certainly at
the costs of tedious and complex parallel programming. Due to a lack of high-level abstractions, developers of parallel applications have to deal with low-level details such as coordinating threads or synchronizing processes. Thus,
parallel programming still remains a diﬃcult and error-prone task. In order to shield the programmer from these lowlevel details, algorithmic skeletons have been proposed. They encapsulate typical parallel programming patterns and
have emerged to be an eﬃcient and scalable approach to simplifying the development of parallel applications. In this
paper, we present a Java binding of our skeleton library Muesli. We point out strengths and weaknesses of Java with
respect to parallel and distributed computing. A matrix multiplication benchmark demonstrates that the Java Generics
deliver poor performance, thus the Java implementation is unable to compete with the C++ implementation in terms
of performance.
Keywords: algorithmic skeletons, parallel programming, distributed programming, programming environments,
message passing, Java for HPC

1. Introduction
In the past decades, the strategy for raising the performance of microprocessors was to push a single core to
continuously increased clock rates and to introduce complex and highly power consuming features like out-of-order
execution and branch prediction. This led to a power problem because the power required by those microprocessors
grew at a faster rate than the frequency and along with it the performance. In order to meet the growing demand of
computing performance, chip designers nowadays have turned to multi-core processors. Raw performance increases
now come from increasing the number of cores instead of pushing the frequency to its limits. On the one hand, this
strategy results in high performance gains while, at the same time, keeping power consumption on a rather constant
level. On the other hand, this strategy also leads to a signiﬁcant growth in complexity of programming multi-core
∗

Email address: s.ernsting@uni-muenster.de (Steﬀen Ernsting)
author

1 Corresponding

1877-0509 © 2012 Published by Elsevier Ltd. Open access under CC BY-NC-ND license.
doi:10.1016/j.procs.2012.04.200

1818

Herbert Kuchen and Steffen Ernsting / Procedia Computer Science 9 (2012) 1817 – 1826

processors. Today, parallel programmers have to deal with rather low-level concepts like multi-threading and message
passing. Due to the lack of high-level abstractions, portable parallel programming can be a very complicated and
error-prone task. Developers not only have to deal with to some extent completely diﬀerent computing platforms
and programming approaches, they additionally have to deal with message passing standards such as MPI [1] and
multithreading concepts like OpenMP [2] or the Java concurrency API. In order to shield the programmer from lowlevel details and thus reducing complexity, algorithmic skeletons have been proposed [3]. They encapsulate typical
parallel computation and communication patterns, thus hiding low-level details of parallelism from the programmer.
In this paper, we present the skeleton library JMuesli, a Java binding of the data parallel part of our skeleton
library Muesli [4, 5]. In order to guarantee platform independence, JMuesli makes only use of pure Java features and
does not make use of any native code through the Java Native Interface (JNI). Not least because C/C++ compiles to
machine code, which can natively be executed, and Java is interpreted by a virtual machine, Java is commonly known
for delivering worse performance than C/C++ does. But with the development of eﬃcient techniques like just-in-time
(JIT) compilation and faster (and concurrent) garbage collectors, this position is at least questionable. In this paper, we
want to ﬁgure out where the strengths and weaknesses of Java are with respect to parallel and distributed computing
in comparison to C++.
The remainder of this paper is structured as follows: related work is discussed in section 2, section 3 introduces
the C++ skeleton library Muesli, pointing out its concepts and beneﬁts. Section 4 describes in detail how the concepts
of Muesli are implemented in Java. Additionally, with a small program for matrix multiplication, we demonstrate
how to implement parallel applications using JMuesli. Section 5 provides experimental results containing measured
runtimes and speedups of the matrix multiplication benchmark. Finally, section 6 concludes the paper and gives a
brief outlook to future work.
2. Related Work
In [3] Cole establishes the algorithmic skeleton approach by introducing an initial set of algorithmic skeletons:
ﬁxed degree divide and conquer, iterative combination, cluster, and task queue. He discusses on general implementation details independent of whether the underlying host language is of functional or imperative nature and elaborates
on the strengths and weaknesses of the skeletal approach.
Today, there are several approaches to skeleton programming with diverging programming paradigms. Most of
which provide skeletons in terms of program libraries for imperative programming languages like C/C++ and Java
on top of MPI or other distribution libraries. In [6] Gonz´alez-V´elez and Leyton provide a state of the art overview
of algorithmic skeleton frameworks and libraries. They describe diﬀerent approaches in detail and summarize recent
research on the topic of skeletal programming.
According to our approach of providing algorithmic skeletons in terms of C++/Java class libraries, we will brieﬂy
discuss recent similar approaches to skeletal programming. However, with Eden [7] and Higher-Order Divide-andConquer (HDC) [8] there are notable skeleton approaches based on functional languages like Haskell. Another worth
mentioning approach to skeletal programming is the Pisa Parallel Programming Language (P3 L) [9], a structured
high-level parallel language providing skeleton constructs to coordinate the parallel or sequential execution of C code.
The Edinburgh Skeleton Library (eSkel) [10] was one of the ﬁrst libraries to implement algorithmic skeletons. As a
structured parallel programming library, eSkel oﬀers various skeletal programming constructs in terms of C functions
on top of MPI. In its current version eSkel supports the skeletons pipeline, farm, butterﬂy (divide & conquer), deal
(sort of farm), and haloswap and aims to adress the several issues raised by skeletal programming. However, a new
version eSkel2 is currently developed in order to address more concepts for more ﬂexibility.
SkeTo [11] is a C++ library on top of MPI. It oﬀers skeletons for manipulating data structures such as lists, trees
and matrices. It also provides an optimization mechanism called fusion transformation, which merges two successive

Herbert Kuchen and Steffen Ernsting / Procedia Computer Science 9 (2012) 1817 – 1826

1819

function calls into a single one. But, to the best of our knowledge, SkeTo does not provide explicit communication
skeletons.
JaSkel [12] is a Java based skeleton library, which oﬀers the skeletons farm, pipe, and heartbeat in terms of abstract
classes. There are sequential, concurrent, and distributed variants of each skeleton. In order to switch variants, the
programmer needs to change the signature of the implementing class. In JaSkel, distribution is based on AspectJ [13],
an aspect-oriented Java extension.
Lithium [14] and its successor Muskel [15] are Java based skeleton libraries implemented on top of RMI. They
provide task and data parallel skeletons by exploiting the so-called macro-data ﬂow technique. To enhance loadbalancing, Lithium has been extended to provide some RMI optimization mechanisms. Additionally, Muskel provides
mechanisms for load balancing and also oﬀers performance-oriented features, like for instance quality of service
(QoS). Recent research addresses skeletal extensibility, autonomic components, and behavioral skeletons.
Calcium [16] and Skandium [17] are Java based skeleton libraries greatly inspired by Lithium and Muskel. Both
task and data parallel skeletons are available and fully nestable. Calcium also provides ﬁle access and a performance
tuning model to identify code that is responsible for weak performance. Whereas Calcium is enabled for distributed
computing by making use of the Java grid middleware ProActive, Skandium only supports shared memory systems
using Java threads for parallelization.
None of the above mentioned Java skeleton libraries make use of state of the art message passing libraries for
distributed computing such as MPJE. Techniques like RMI rely on a client-server model and therefore may introduce
scalability problems when it comes to hundreds or thousands of nodes.
3. The Muenster Skeleton Library
The Muenster Skeleton Library Muesli is a C++ template library oﬀering various task and data parallel skeletons.
For an eﬃcient support of multi- and many-core computer architectures as well as clusters of both, Muesli makes
use of OpenMP [2] and MPI [1]. Additionally, Muesli provides data parallel skeletons for GPU processing using
CUDA [18]. Since there is not yet a quality CUDA binding for Java, we will subsequently consider only data parallel
skeletons for multi-node, multi-core or many-core computers.
In Muesli, two classes of data parallel skeletons can be distinguished: computation skeletons and communication
skeletons. Both computation skeletons such as map, zip, fold, and scan and their variants as well as communications skeletons such as permutePartition, broadcast, and gather (and their variants) are provided by Muesli
as member functions of distributed data structures, i.e. classes that represent distributed data structures. At present,
there are distributed data structures for arrays, matrices and sparse matrices oﬀered by Muesli, namely:
template < typename T > class DistributedArray {...}
// one - d i m e n s i o n a l
template < typename T > class DistributedMatrix {...}
// two - d i m e n s i o n a l
template < typename T > class D i s t r i b u t e d S p a r s e M a t r i x {...} // two - d i m e n s i o n a l
Listing 1: Distributed data structures oﬀered by Muesli.

Whereas the class DistributedArray represents a one-dimensional (ﬁxed) polymorphic distributed array, the classes
DistributedMatrix and DistributedSparseMatrix represent two-dimensional (ﬁxed) polymorphic distributed
arrays (or matrices). The template parameter T denotes the type of the elements stored by the distributed data structure. By instantiating the parameter T, arbitrary element types can be generated. As the name suggests, distributed data
structures are distributed among several MPI processes, each storing a (local) partition of the whole data structure.
The distributed data structures deliver a transparent and ﬂexible automated data distribution mechanism. While ﬁgure
1 (a) shows the programmer’s perspective of a matrix called global view, (b) shows the block-wise distribution of a
matrix into sub-matrices (local partitions), which is referred to as local view. However, programmers are interested
in having a global view of the data structures instead of dealing with local partitions. Therefore, the default view is
the global view: every skeleton call aﬀects the whole data structure and whenever programmers have to deal with

1820

Herbert Kuchen and Steffen Ernsting / Procedia Computer Science 9 (2012) 1817 – 1826

indices, most of these indices are global indices. The local view is just an internal representation of the whole data
structure. But, since in some cases it may be useful for programmers to deal with local indices, there are variants of
skeletons using local indices. Also, some communication skeletons require that the programmer be aware of this data
distribution scheme. Additionally, there are getters and setters for global and local indices. When calling computation
5
7
0
8

3
0
6
5

(a)

4
8
2
4

(

(

1
5
9
3

P0

1
5

5 3
7 0

4
P1
8

P2

9
3

0 6
8 5

2
P3
4

(b)

Figure 1: (a) Global view of a matrix. (b) Local view of a matrix after distribution among 4 processes.

skeletons, distributed data structures are processed as a whole, e.g. the map skeleton applies a given user function
to each element of a distributed data structure and replaces the element with its result. The fold skeleton combines
all elements of a distributed data structure successively by means of a given associative binary user function. When
skeletons involve communication between processes (fold, scan), this is handled within these skeletons and thus is
fully transparent to the programmer. The programmer just needs to be aware of the data structure as a whole and
not of its decomposition into local partitions when using computation skeletons. In order to avoid ineﬃciency, there
is no other implicit communication, e.g. by accessing elements of remote partitions. For explicit communication,
distributed data structures provide various communication skeletons, e.g. for permuting and broadcasting local partitions among processes or for gathering the whole data structure. For the proper use of communication skeletons,
the programmer needs to be aware that distributed data structures are decomposed into local partitions, but not of the
resulting communication details. Making use of these communication concepts signiﬁcantly minimizes the risk of
deadlocks, thus signiﬁcantly reducing development time.
A key concept of Muesli is that the argument functions of skeletons can be partial applications rather than just
C++ functions. Since a skeleton deﬁnes some parallel algorithmic structure, where the details are ﬁxed by an appropriate argument function with a ﬁxed number of arguments, this feature provides skeletons with more expressiveness:
with an argument function being a partial application, the details of a skeleton can depend themselves on additional
arguments, which are computed at runtime. The auxiliary function curry is used to transform ordinary C++ functions into partial applications. Within this process, a number of arguments are ﬁxed to some values (arguments of the
curry function), returning a function requiring less arguments than the original function.
4. Implementation
As already mentioned before, JMuesli is a pure Java implementation of the C++ skeleton library Muesli. As we
want to achieve a comparison between Java and C++ in parallel and distributed computing (and skeletal programming), JMuesli entirely relies on the core Java API. We do not exploit operating system or platform speciﬁc features
using JNI. This way, we also guarantee platform independence and universal portability.
4.1. Parallel and Distributed Computing in Java
Java is a platform independent programming language, which can be used on any machine or combination of
diﬀerent machines that support Java. Additionally, garbage collection and the implicit serialization mechanism help
programmers writing reliable code a lot. With the Java concurrency API, Java oﬀers a built-in support for multithreading. Lots of easy-to-use features like thread pools and thread synchronization make a major contribution to
simplifying parallel programming in Java. Last but not least, with the Java Development Kit (JDK) Java provides a
huge standard library with lots of useful tools.
As a language designed with networking in mind, Java naturally provides techniques for distributed computing.
On the one hand there are APIs for distributed objects and remote procedure calls like RMI and CORBA, on the other

Herbert Kuchen and Steffen Ernsting / Procedia Computer Science 9 (2012) 1817 – 1826

1821

hand there are rather low-level point-to-point communication techniques using the java.net or java.nio sockets,
respectively. While the former is known to deliver poor performance, the latter, as said, is a rather low-level package,
which involves complex socket programming. With MPJ Express (MPJE) [19] there is a quality implementation of
Java bindings for the MPI standard. The system implements thread-safe communication in Java via message passing.
At runtime, programmers may choose between pluggable transport devices either based on Java NIO, which is a pure
Java implementation, or based on high performance interconnects like Myrinet, which are implemented using the Java
Native Interface (JNI). For the benchmark, we used the Java NIO transport device.
4.2. Parallelization
In JMuesli, parallelism is achieved in two steps. For inter-node parallelization, the SPMD programming model
delivered by MPI (MPJE) is exploited in a way that each process only stores a (local) partition of a distributed data
structure. As pointed out in section 3, in Muesli, a distributed data structure is a simple template class, which provides
computation and communication skeletons as well as various auxiliary methods. In order to maintain polymorphism
when porting the distributed data structures to Java, we make use of Java Generics.
public class DistributedArray <T >{...}
public class DistributedMatrix <T >{...}
public class DistributedSparseMatrix <T >{...}
Listing 2: Distributed data structures oﬀered by JMuesli.

With assistance of a process identiﬁer delivered by MPJE, each process can independently calculate its own
partition, which is locally stored by a generic array. Since it is not allowed to straightforward create a generic array in
Java, the Java Reﬂection API is used to overcome this limitation. Listing 3 illustrates generic array creation in Java.
1
2

// not allowed :
T [] a = new T [ mySize ];

3
4
5

// allowed :
import java . lang . reflect . Array ;

6
7
8
9

public <T > T [] getGenericArray ( Class type , int size ) {
return ( T []) Array . newInstance ( type , size ) ;
}

10
11
12

// Call :
T [] a = g et Ge n er ic Ar r ay ( Integer . class , mySize ) ;
Listing 3: Generic array creation in Java.

For intra-node parallelization, Java thread pools are used to distribute the workload among a number of participating threads. The most important reason of using a thread pool over creating a new thread for each task is thread
creation and destruction overhead. Usually skeletal programs contain multiple skeleton calls. Creating and destoying
a number of threads for each skeleton call would lead to a signiﬁcant overhead. In order to reduce this thread creation
and destruction overhead to a minimum, a thread pool abstraction layer is introduced by the class ThreadPool. This
class is implemented using the Singleton pattern to ensure that only a single thread pool instance is created for a single
JMuesli instance. This thread pool instance is shared among all skeleton calls of a single program. However, there is
no competition for resources, as skeleton calls are sequential. Only their execution is done in parallel.
Listing 4 demonstrates how the mapInPlace skeleton, which simply applies an argument function to each element
of a distributed data structure and replaces the corresponding element with the result of that function call, is implemented using a thread pool. As previously mentioned, a skeleton deﬁnes some parallel algorithmic structure where
the details are ﬁxed by an appropriate argument function. Since Java is class-based and lacks ﬁrst-class functions
(methods), the mapInPlace skeleton expects a function object (or often called functor) containing a method that represents the argument function. FunctionObj1 indicates, that its argument function expects a single argument, i.e. the
corresponding element of the distributed data structure. The generic type parameter T is deﬁned by the corresponding
distributed data structure class. It indicates the return and the argument type of the function object. However, implementation details of function objects are considered closely in section 4.3. In line 7 of listing 4, threads are started

1822

Herbert Kuchen and Steffen Ernsting / Procedia Computer Science 9 (2012) 1817 – 1826

by submitting tasks to the thread pool. Here, a task is an instance of an implementation of the runnable interface
provided with the JDK. Finally, threads are joined using a CountDownLatch. The await() method blocks until every
thread has called the countDown() method.
1
2
3

public void mapInPlace ( FunctionObj1 <T , T > fo1 ) {
int start ;
CountDownLatch cdl = new CountDownLatch ( numThreads ) ;

4

for ( int i = 0; i < numThreads ; i ++) {
start = i * nLocal / numThreads ;
threadpool . submit ( new MapWorker ( start , cdl , e l e m e n t s P e r T h r e a d [ i ] , fo1 ) ) ;
}

5
6
7
8
9

try {
cdl . await () ;
} catch ( I n t e r r u p t e d E x c e p t i o n e ) {
...
}

10
11
12
13
14
15

}
Listing 4: Parallelizing the mapInPlace skeleton with Java threads.

4.2.1. Combining MPJ Express and Java Threads
Whereas the mapInPlace skeleton introduces a very straightforward parallelization without any interprocess communication, in this section, we demonstrate how interaction of intra-node and inter-node parallelization is handled.
Consider the fold skeleton, which successively combines all elements of a distributed data structure by means of
a given associative binary user function. Listing 5 presents the scheme of the fold skeleton. At ﬁrst, each process folds its local elements in line 6. Similarly to the mapInPlace skeleton presented in the previous section, Java
threads are used to do this in parallel. Next, the local results have to be distributed among all processes in line 8.
This is accomplished by the allgather communication routine. Note that for portability reasons, we want to keep
the communication interface interchangeable and we therefore do not make use of collective communication routines
provided by MPJE. Instead, we have implemented eﬃcient collective communication routines by exclusively making
use of simple send and receive methods as well as hypercube communication. However, we did not observe any major
performance drawbacks. Finally, each process folds all local results to a single global result. Note that, again, Java
threads are used for intra-node parallelization.
1
2
3
4

public T fold ( FunctionObj2 <T , T , T > fo2 ) {
T localResult ;
T [] globalResults ;
T result ;

5

localResult = localFold () ;

6
7

allgather ( localResult , globalResults )

8
9

result = foldGloba lResults ( globalResults ) ;

10
11

return result ;

12
13

}
Listing 5: Scheme of the fold skeleton.

4.3. Higher-Order Functions and Partial Applications
Because skeletons take functions as arguments and are functions themselves, they can be called higher-order
functions. But, since methods are not ﬁrst-class in Java, it is not possible to pass them as arguments to other methods.
Therefore, skeleton argument functions have to be implemented as classes, which contain a method that actually
implements the desired argument function. With the function objects, we deﬁne a set of interfaces containing an
abstract method f(), which has to be implemented by the programmer and can then be called by the skeletons.

Herbert Kuchen and Steffen Ernsting / Procedia Computer Science 9 (2012) 1817 – 1826

1823

Listing 6 exemplarily presents the abstract class FunctionObj1. It provides an interface for a skeleton argument
function that expects a single argument. In case of additional arguments being required, JMuesli provides interfaces
for skeleton argument functions that expect up to six arguments.
Listing 6 also illustrates how partial applications are implemented. A function object that shall be called with less
arguments than expected, has to return another function object with the same functionality and the given arguments
ﬁxed. For that purpose, each function object oﬀers a reference to the next higher function object (line 3). The hierarchy
is deﬁned by the number of expected arguments. Through this reference, a lower function object can forward a
function call to the next higher function object. The method curry in lines 11 to 17 implements this procedure. It
creates and returns an instance of the next lower function object, thereby ﬁxing one argument. The returned (lower)
function object oﬀers a reference to the current (higher) function object in order to be able to forward calls of method
f() (line 14). Note that the argument arg has to be declared final (read-only) so that the anonymous inner class
may refer to it. For simplicity reasons, the curry method is only capable of ﬁxing a single argument with a single
invocation. This is not a drawback in ﬂexibility, since the same result can be achieved by ﬁxing one argument after
another.
1

public abstract class FunctionObject1 <T , A1 > extends FunctionObj {

2

protected FunctionObj2 fo2 ;

3
4

protected FunctionObj1 ( FunctionObj2 fo2 ) {
this . fo2 = fo2 ;
}

5
6
7
8

public FunctionObj0 <T > curry ( final A1 arg ) {
return new FunctionObj0 <T >( this ) {
public T f () {
return ( T ) fo1 . f ( arg ) ;
}
};
}

9
10
11
12
13
14
15
16

public abstract T f ( A1 arg1 ) ;

17
18

}
Listing 6: Abstract class FunctionObj1 represents a higher order function in Java.

4.4. Serialization
Serialization is an important concept for distributed computing in general and for our skeleton library JMuesli in
particular. Whenever a distributed data structure is parameterized with an arbitrary data type, elements have to be
serialized before being transferred over network. Fortunately, for a programmer there is no real eﬀort on serializing
objects in Java. Java provides the so-called marker interface Serializable. Each class that implements this interface
can be serialized. Same holds for every subclass of a class implementing this interface.
4.5. Case Study: Matrix Multiplication
Cannon’s algorithm [20] for matrix multiplication of two n × n-matrices assumes the matrices to be split into p
submatrices (local partitions) of size m × m, where p denotes the number of processes and m = n/ #processes. Initially the submatrices of A and B are shifted cyclically in horizontal and vertical direction, respectively. Submatrices
of row i (column j) are shifted i ( j) positions to the left (upwards). The core of the algorithm is the repeated local
submatrix multiplication followed by a cyclic shift of the rows and columns of A and B by one position in horizontal
(leftwards) and vertical direction (upwards), respectively. After #processes steps, each process has calculated a single submatrix of the resulting matrix. Listing 7 presents the implementation of Cannon’s algorithm using computation
and communication skeletons oﬀered by JMuesli.
At ﬁrst, the initial shifting is performed by the communication skeletons rotateRows and rotateCols in lines
21 and 22. When called with a function object as argument, these skeletons calculate the number of positions each

1824

Herbert Kuchen and Steffen Ernsting / Procedia Computer Science 9 (2012) 1817 – 1826

submatrix has to be shifted by applying this function object to the row and column indices of the submatrices, respectively. According to the Negate function object, a submatrix of row i (column j) is shifted i ( j) positions to the left
(upwards). When called with the argument -1 (lines 29, 30), submatrices of row i (column j) are shifted one position
to the left (upwards). The result matrix C is instantiated in line 24. Next, the scalar product function object scproduct
(lines 1 to 9) is partially applied to A and B. The getter methods getGlobalLocal and getLocalGlobal (line 5) treat
the ﬁrst argument as a global index and the second argument as a local index and vice versa. In order to ﬁx the ﬁrst
two arguments (A and B) of this function object, the auxiliary function curry is used twice. The result is a function
object that has two arguments ﬁxed and expects another three arguments. When passed to the mapIndexInPlace
skeleton (line 28) this function object is applied to each element of the result matrix. In addition to the elements of
the result matrix, it is provided with the row and column indices of these elements. This way, the local submatrix
multiplications are performed.
1

2

3
4
5
6
7
8
9

class Scprod extends FunctionObj5 < Integer , DistributedMatrix < Integer > , DistributedMatrix <
Integer > , Integer , Integer , Integer > {
public Integer f ( DistributedMatrix < Integer > A , DistributedMatrix < Integer > B , Integer Cij ,
Integer i , Integer j ) {
Integer sum = Cij ;
for ( int k = 0; k < A . getLocalRows () ; k ++) {
sum += A . getGlobalLocal (i , k ) * B . getLocalGlobal (k , j ) ;
}
return sum ;
}
}

10
11
12
13
14
15

class Negate extends FunctionObj1 < Integer , Integer > {
public Integer f ( Integer arg1 ) {
return - arg1 ;
}
}

16
17

18
19

public DistributedMatrix < Integer > matmult ( DistributedMatrix < Integer > A , DistributedMatrix <
Integer > B ) {
Negate negate = new Negate () ;
Scprod scproduct = new Scprod () ;

20

A . rotateRows ( negate ) ;
B . rotateCols ( negate ) ;

21
22
23

DistributedMatrix < Integer > C = new DistributedMatrix < Integer >(...) ;
FunctionObj3 < Integer , Integer , Integer , Integer > fo3 = scproduct . curry ( A ) . curry ( B ) ;

24
25
26

for ( int i = 0; i < A . getBlocksInRow () ; i ++) {
C . mapIndexInPlace ( fo3 ) ;
A . rotateRows ( -1) ;
B . rotateCols ( -1) ;
}
return C ;

27
28
29
30
31
32
33

}
Listing 7: Implementation of Cannon’s algorithm using JMuesli.

5. Experimental Results
In order to ﬁgure out whether the Java implementation JMuesli is able to compete with the C++ implementation
of our skeleton library Muesli, we have implemented the above-described matrix multiplication benchmark using data
parallel computation and communication skeletons. The benchmarks has been conducted on PALMA, a multi-node,
multi-core cluster computer at the University of Muenster. The compute nodes consist of two Intel Xeon Westmere
X5650 processors with a total of 12 cores (6 cores each). The JVM used is the IcedTea6 JVM.

Herbert Kuchen and Steffen Ernsting / Procedia Computer Science 9 (2012) 1817 – 1826

1825

Table 1: Execution times (in seconds) for the matrix multiplication benchmark using JMuesli’s (Java) skeletons. Matrices are from size n × n with
n = 8192, np and nt denote the number of processes and threads per process, respectively. Speedups (relative to a single core) given in parenthesis.

nt/np
1
4
8
12

1
49134.10 (1.00)
14543.08 (3.38)
7737.47 (6.35)
5291.54 (9.29)

4
10968.30 (4.48)
3505.44 (14.02)
2016.84 (24.36)
1410.17 (34.84)

16
2117.75 (23.20)
654.27 (75.10)
402.29 (122.14)
332.42 (147.81)

64
488.84 (100.51)
193.57 (253.83)
125.88 (390.32)
101.57 (483.74)

Table 2: Execution times (in seconds) for the matrix multiplication benchmark using Muesli’s (C++) skeletons. Matrices are from size n × n with
n = 8192, np and nt denote the number of processes and threads per process, respectively. Speedups (relative to a single core) given in parenthesis.

nt/np
1
4
8
12

1
8633.22 (1.00)
2431.89 (3.55)
1189.37 (7.20)
861.05 (10.02)

4
1975.12 (4.37)
549.93 (15.70)
272.87 (31.64)
182.35 (47.34)

16
432.44 (19.97)
112.49 (76.75)
55.23 (156.31)
36.74 (234.98)

64
76.87 (112.31)
19.45 (443.87)
10.28 (839.81)
7.41 (1165.08)

Tables 1 and 2 show the JMuesli (Java) and Muesli (C++) results, respectively, for the matrix multiplication
benchmark. It was noticeable that the JMuesli implementation with the use of Java Generics faced some dramatic
performance and scalability problems. The JMuesli runtimes exceeded the Muesli runtimes by a factor of around 6 to
14. The more processes used, the bigger the gap in performance between the two implementations. As a consequence
thereof, the speedups achieved by the Muesli implementation exceeded those achieved by the JMuesli implementation
by a factor of approximately 1.5 to 2. This is a huge advantage for the C++ implementation. But what is the reason
for this poor performance of the Java implementation? In order to elaborate on this issue, we have implemented an
int variant of the distributed matrix.
Table 3 shows the JMuesli results when using an int implementation of the class DistributedMatrix instead
of the generic version. In comparison with the generic variant, the int variant provided much better performance.
Runtimes decreased by a factor of approximately 3 to 7 and speedups increased by a factor of approximately up to 2.
In comparison with the Muesli results, runtimes and speedups of JMuesli and Muesli conformed to each other, being
in the same order of magnitude. But there was still a clear advantage for the C++ implementation Muesli, amounting
to a factor of two in runtime.
The comparison between the generic and the int implementation revealed a dramatic performance impact of Java
Generics. Considering the implementation details of Java Generics closely, this performance impact is not surprising.
Whereas C++ templates are translated to specialized representations for each generic type (code specialization), Java
Generics are translated to a single representation for all generic types (code sharing). A technique called type erasure
maps various instantiations of the generic type onto the single byte code representation by adding type checks and type
conversions where required. Note that these type checks and conversions then happen at runtime, introducing a slight
overhead whenever a generic declaration is invoked. Furthermore, this technique prevents generic types from being
instantiated with primitive data types, which actually means that programmers are required to use wrapper classes. In
total, all these drawbacks sum up to a huge overhead introduced by the Java Generics.
6. Conclusion and Future Work
In this paper, we have presented a Java implementation (JMuesli) of the data parallel part of our C++ skeleton
library Muesli. It provides an eﬃcient and scalable high-level approach to simplify portable parallel and distributed
programming. Due to the exclusive use of pure Java, JMuesli is innately portable across a variety of platforms (that
support Java). Parallel applications developed with JMuesli can even be run on heterogeneous grids that make use of

1826

Herbert Kuchen and Steffen Ernsting / Procedia Computer Science 9 (2012) 1817 – 1826

Table 3: Execution times (in seconds) for the matrix multiplication benchmark using an int implementation of JMuesli’s (Java) skeletons.
Matrices are from size n × n with n = 8192, np and nt denote the number of processes and threads per process, respectively. Speedups (relative to
a single core) given in parenthesis.

nt/np
1
4
8
12

1
14024.75 (1.00)
3537.14 (3.96)
1770.30 (7.92)
1198.43 (11.70)

4
2999.07 (4.68)
815.19 (17.20)
386.07 (36.33)
260.17 (53.91)

16
560.92 (25.00)
141.50 (99.11)
73.74 (190.19)
60.54 (231.66)

64
93.24 (150.41)
30.91 (453.71)
20.62 (680.13)
15.83 (885.93)

diﬀerent operating systems and hardware architectures. Further, the large JDK, built-in exception handling, garbage
collection, and other features provided by the Java platform make parallel and distributed programming with JMuesli
very comfortable. Besides all these advantages, the use of pure Java also carries some disadvantages: experimental
results showed that the Java Generics are not able to compete with C++ templates in the matter of performance and
scalability. In contrast to C++ templates, Generics can not be parameterized with primitive data types. The restriction
to wrapper classes and the implementation of Java Generics introduce a huge overhead, which clearly reﬂected in
the results. Even when restricting to primitive data types, the Java implementation is not able to compete (in terms
of performance) with the C++ implementation. There is still a clear advantage in performance and scalability for
the C++ implementation. Besides providing more data parallel skeletons to raise the level of expressivity, future
work could be directed to also implement in Java the task parallel skeletons provided by Muesli. But for any further
development of JMuesli, it is essential to ﬁnd a concept to overcome the performance issues raised by the Java
Generics.
References
[1] W. Gropp, W. Lusk, A. Skjellum, Using MPI - Portable Parallel Programming with the Message-Passing Interface, MIT Press, 1996.
[2] B. Chapman, G. Jost, R. van der Pas, Using OpenMP - Portable Shared Memory Parallel Programming, Scientiﬁc and Engineering Computation, MIT Press, 2008.
[3] M. Cole, Algorithmic Skeletons: Structured Management of Parallel Computation, MIT Press, 1989.
[4] H. Kuchen, A skeleton library, in: Proceedings of the 8th International Euro-Par Conference on Parallel Processing, Euro-Par ’02, SpringerVerlag, London, UK, 2002, pp. 620–629.
[5] S. Ernsting, H. Kuchen, Data Parallel Skeletons for GPU Clusters and Multi-GPU Systems, in: Proceedings of the International Conference
on Parallel Computing (ParCo 2011, to appear), ParCo ’11, 2011.
[6] H. Gonz´alez-V´elez, M. Leyton, A survey of algorithmic skeleton frameworks: high-level structured parallel programming enablers, Software:
Practice and Experience 40 (12) (2010) 1135–1160.
[7] R. Loogen, Y. Ortega-Mall´en, R. Pe˜na-Mar´ı, Parallel functional programming in eden, Journal of Functional Programming 15 (3) (2005)
431–475.
[8] C. Herrmann, C. Lengauer, Hdc: A higher-order language for divide-and-conquer, Parallel Processing Letters 10 (2) (2000) 239–250.
[9] B. Bacci, M. Danelutto, S. Orlando, S. Pelagatti, M. Vanneschi, P3l: A structured high-level parallel language, and its structured support,
Concurrency: practice and experience 7 (3) (1995) 225–255.
[10] A. Benoit, B. Cole, S. Gilmore, J. Hillston, Flexible Skeletal Programming with eSkel, in: Proceedings of the 11th International Euro-Par
Conference, Vol. 3648, Springer, 2005, pp. 761–770.
[11] Y. Karasawa, H. Iwasaki, A Parallel Skeleton Library for Multi-core Clusters, in: International Conference on Parallel Processing, 2009, pp.
84–91.
[12] J. Sobral, A. Proenc¸a, Enabling jaskel skeletons for clusters and computational grids, in: Cluster Computing, 2007 IEEE International
Conference on, IEEE, 2007, pp. 365–371.
[13] G. Kiczales, E. Hilsdale, J. Hugunin, M. Kersten, J. Palm, W. Griswold, An overview of aspectj, ECOOP 2001Object-Oriented Programming
(2001) 327–354.
[14] M. Aldinucci, M. Danelutto, P. Teti, An advanced environment supporting structured parallel programming in java, Future Generation Computer Systems 19 (5) (2003) 611–626.
[15] M. Aldinucci, M. Danelutto, P. Dazzi, Muskel: an expandable skeleton environment, Scalable Computing: Practice and Experience 8 (4).
[16] D. Caromel, M. Leyton, Fine tuning algorithmic skeletons, Euro-Par 2007 Parallel Processing (2007) 72–81.
[17] M. Leyton, J. M. Piquer, Skandium: Multi-core Programming with Algorithmic Skeletons, IEEE Euro-micro PDP 2010.
[18] Nvidia Corp., NVIDIA CUDA C Programming Guide, Nvidia Corporation, 2010.
[19] A. Shaﬁ, B. Carpenter, M. Baker, Nested parallelism for multi-core hpc systems using java, J. Parallel Distrib. Comput. 69 (6) (2009) 532–545.
[20] M. J. Quinn, Parallel computing (2nd ed.): theory and practice, McGraw-Hill, Inc., New York, NY, USA, 1994.

