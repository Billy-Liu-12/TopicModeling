High-Dimensional Clustering Method for High
Performance Data Mining*
Jae-Woo Chang and Hyun-Jo Lee
Research Center of Industrial Technology
Dept. of Computer Engineering, Chonbuk National University,
Chonju, Chonbuk, 561-756, South Korea
jwchang@chonbuk.ac.kr, hjlee@dblab.chonbuk.ac.kr

Abstract. Many clustering methods are not suitable as high-dimensional ones
because of the so-called ‘curse of dimensionality’ and the limitation of available
memory. In this paper, we propose a new high-dimensional clustering method
for the high performance data mining. The proposed high-dimensional
clustering method provides efficient cell creation and cell insertion algorithms
using a space-partitioning technique, as well as makes use of a filtering-based
index structure using an approximation technique. In addition, we compare the
performance of our high-dimensional clustering method with the CLIQUE
method which is well known as an efficient clustering method for highdimensional data. The experimental results show that our high-dimensional
clustering method achieves better performance on cluster construction time and
retrieval time than the CLIQUE.
Keywords: High-Dimensional Clustering, Data Mining.

1 Introduction
Data mining is concerned with the extraction of interesting knowledge from a large
amount data, i.e. rules, regularities, patterns, constraints. Clustering, one of the most
important research topics in data mining, is the process of grouping data into classes
or clusters, in such a way that objects within a cluster have high similarity to one
another, but are very dissimilar to objects in other clusters [1]. The existing clustering
methods have a critical drawback that they do not work well for clustering highdimensional data because their retrieval performance is generally degraded as the
number of dimension increases. In this paper, we propose an efficient highdimensional clustering method for the high performance data mining. Our highdimensional clustering method provides a cell creation algorithm to make cells by
splitting each dimension into a set of partitions, and provides a cell insertion
algorithm to construct clusters as cells with more density than a given threshold and
insert them into an index structure. By using an approximation technique, we also
propose a new filtering-based index structure to have fast accesses to the clusters.
*

This work is financially supported by the Ministry of Education and Human Resources
Development (MOE), the Ministry of Commerce, Industry and Energy (MOCIE) and the
Ministry of Labor (MOLAB) though the fostering project of the Lab of Excellency.

Y. Shi et al. (Eds.): ICCS 2007, Part III, LNCS 4489, pp. 621–628, 2007.
© Springer-Verlag Berlin Heidelberg 2007

622

J.-W. Chang and H.-J. Lee

The rest of this paper is organized as follows. The next section discusses related
work on high-dimensional clustering methods. In Section 3, we propose a new highdimensional clustering method. In Section 4, we analyze the performances of our
high-dimensional clustering method. Finally, we draw our conclusion in Section 5.

2 Related Work
The existing high-dimensional clustering methods can be roughly classified into two
groups, such as grid-based and partitioning [2]. CLIQUE[3] and MAFIA[4] belong to
the grid-based approach while PROCLUS[5], FINDIT[6], and DOC[7] belong to the
partitioning approach. In this section, we discuss the typical clustering methods for
each approach. First, CLIQUE(CLustering In QUEst) was proposed as a densitybased clustering method. CLIQUE automatically finds subspaces(grids) with highdensity clusters. CLIQUE produces identical results irrespective of the order in which
input records are presented, and it does not presume any canonical distribution of
input data. Input parameters are the size of the grid and a global density threshold for
clusters. Next, PROCLUS was the first top-down clustering method while CLIQUE
scales linearly with the number of input records and it has good scalability as the
number of dimensions in the data. PROCLUS samples the data, then selects a set of k
medoids, and iteratively improves the clustering. PROCLUS is biased toward clusters
that are hyper-spherical in space. While cluster may be found in differ subspaces, the
subspaces must be of similar sizes since the user must input the average number of
dimensions for the clusters.

3 An Efficient High-Dimensional Clustering Method
Since the existing clustering methods assume that a data set is resident in main
memory, they are not efficient for handling large amounts of data. As the
dimensionality of data is increased, the number of cells increases exponentially, thus
causing the dramatic performance degradation. To remedy that effect, we propose an
efficient high-dimensional clustering method for the high performance data mining.
The overall architecture of our clustering method is shown in Figure 1.
Large high
dimensional
input data

Cell
creation
algorithm

Cells
containing
data

Cell
insertion
algorithm

Filtering
Based
index
structure

Fig. 1. Overall architecture of our clustering method

3.1 Cell Creation Algorithm
Our cell creation algorithm makes cells by splitting each dimension into a group of
sections using a split index. Density based split index is used for creating split
sections and is efficient for splitting multi-group data. Our cell creation algorithm first
finds the optimal split section by repeatedly examining a value between the maximum

High-Dimensional Clustering Method for High Performance Data Mining

623

and the minimum in each dimension. The split index value is calculated by Eq. (1)
before splitting and Eq. (2) after splitting. Using Eq. (1), we can determine the split
index value for a data set S in three steps: i) divide S into C classes, ii) calculate the
square value of the relative density (Pj) of each class, and iii) subtract from one all the
square values of the densities of C classes. Using Eq. (2), we compute a split index
value for S after S is divided into S1 and S2. Here the number of data in S is n and the
number of data belonging to S1(S2) is n1 (n2). If the split index value is larger than the
previous value before splitting, we actually divide S into S1 and S2. Otherwise, we
stop splitting. Secondly, our cell creation algorithm creates cells being made by the
optimal split sections for n-dimensional data. As a result, our cell creation algorithm
creates fewer cells than the existing clustering methods using equivalent intervals.
Split(S) = 1 −

C

∑P j

2

(1)

j =1

Split ( S ) =

n1
n
Split ( S1 ) + 2 Split ( S 2 )
n
n

(2)

If a data set has n dimensions and the number of the initial split sections in each
dimension is m, the conventional cell creation algorithms make mn cells, but our cell
creation algorithm makes only K1*K2*...*Kn cells (1 ≤K1, K2, .. ,Kn≤m).
3.2 Cell Insertion Algorithm
For cell insertion, we first obtain the cells created using the cell creation algorithm.
Secondly, we construct clusters as the cells with more density than a given cell
threshold and store them into a cluster information file. A record in the cluster
information file consists of a cluster id and the number of data belonging to the
cluster. The cluster information file for large, high-dimensional data is too large to
reside in memory. Thirdly, we calculate the frequency of a section in all dimensions
whose frequency is greater than a given section threshold. Finally, we set to '1' the
bits corresponding to the sections with a high frequency in an approximation
information file. We set to '0' the other bits for the remainder sections. We calculate
the frequency of data in a cell. Finally, the cell threshold and the section threshold are
shown in Eq. (3).
NR × F
⎛
⎜λ =
NI
⎜
Section threshold = ⎜ NI : the number of input data
⎜
⎜ NR : the number of sections per dimension
⎜ F : minimum section frequency being regarded as '1'
⎝
Cell threshold (τ ) : positive integer

(3)

3.3 Filtering-Based Index Structure
When the number of the created cells is very large due to large and high-dimensional
data, it may take much time to answer users’ queries. In order to reduce time to

624

J.-W. Chang and H.-J. Lee

respond to the queries, it is possible to construct a new filtering-based index scheme
using the approximation information file. Figure 2 shows an example of a filteringbased index scheme containing both the approximation information file and cluster
information file, assuming two-dimensional data. Let assume that K clusters are
created by our cell-based clustering method and the numbers of split sections in X
axis and Y axis are m and n, respectively. The following equation, Eq.(4), shows the
retrieval times (C) when the approximation information file is used and when it is not
used. We assume that is an average filtering ratio in the approximation information
file. D is the number of dimensions of input data. P is the number of records per page.
R is the average number of records in each dimension. When the approximation
information file is used, the retrieval time decreases as decreases. For high-dimension
data, our two-level index scheme using the approximation information file is an
efficient scheme because the K value increases exponentially in proportion to
dimension D. The size of the approximation information file is dependent on both the
number of dimensions of data and the numbers of split sections in each dimension.
Because we deal with high-dimensional data, we keep the approximation information
file in disk, rather than in memory.
i) Retrieval time without the use of an approximation information file
C = ⎡K P ⎤ 2 (Disk I/O accesses)
ii) Retrieval time with the use of an approximation information file
C = ⎡(D * R ) P ⎤ *α + (1 − α ) ⎡K P ⎤ 2 (Disk I/O accesses)

(4)

Figure 2 shows our filtering-based index scheme used to answer a query when a
cell threshold and a section threshold are 1, respectively. For a query Q1, we
determine 0.6 in X axis as the third section and 0.8 in Y axis as the fourth section. In
the approximation information file, the value for the third section in X axis is '1' and
the value for the 4-th section in Y axis is '0'. Because one of sections’ values is '0', Q1
can be discarded without searching the corresponding cluster information file. For a
query Q2, the value of 0.55 in X axis and the value of 0.7 in Y axis belong to the third
section, respectively. Because the third bit for X axis and the third bit for Y axis have
'1' in the approximation information file, we calculate a cell number and obtain its cell
Y
1

1
c13

0.75

c14

0.6

0.8

Q1

0.55

0.7

Q2

cell number
c9

c10

c11

X
Y

c12

0.5

1 0 1 1
1 1 1 0
2

c5

c6

c1

c2

c7

c8

Approximation
Information
file

0.25

0

0.25

c4

c3

0.5

c4

1

c5

2

c9

1

c11

3

c12

1

c16

c15

0.75

구성된
Constructed
셀 구조
cells

1

X

저장
Index
인덱스
structure
구조

Fig. 2. Filtering-based index scheme

Cluster
Information
file

High-Dimensional Clustering Method for High Performance Data Mining

625

frequency by accessing the cluster information file. As a result, we obtain a cell
number 11 and its frequency 3 for Q2.

4 Performance Analysis
For our performance analysis, we implemented our clustering method on Linux server
with dual processors. We make use of one million data (16-dimensional one) created
by Synthetic Data Generation Code for Classification in IBM Quest Data Mining
Project [8]. A record in our experiment is composed of both numeric type attributes,
like salary, commission, age, hvalue, hyears, loan, tax, interest, cyear, balance, and
categorical type attributes, like level, zipcode, area, children, ctype, job. The factors
of our performance analysis are cluster construction time, precision, and retrieval
time. We compare our cell-based clustering method (CBCM) with the CLIQUE
method, which is one of the most efficient high-dimensional clustering methods. For
our experiment, we make use of three data sets, one with random distribution, one
with standard normal distribution (variation=1), and one with normal distribution of
Table 1. Methods used for performance comparison (MI:Maximal Interval)
Methods
CBCM-5R
CLIQUE-5R
CBCM-10R
CLIQUE-10R
CBCM-5SND
CLIQUE-5SND
CBCM-10SND
CLIQUE-10SND
CBCM-5ND(0.5)
CLIQUE-5ND(0.5)
CBCM-10ND(0.5)
CLIQUE-10ND(0.5)

Description
CBCM for data set with random distribution(MI = 5)
CLIQUE for data set with random distribution (MI=5)
CBCM for data set with random distribution (MI=10)
CLIQUE for data set with random distribution (MI=10)
CBCM with standard normal distribution (MI=5)
CLIQUE with standard normal distribution (MI=5)
CBCM with standard normal distribution (MI=10)
CLIQUE with standard normal distribution (MI=10)
CBCM with normal distribution of variation 0.5 (MI=5)
CLIQUE with normal dist. of variation 0.5 (MI=5)
CBCM with normal distribution of variation 0.5 (MI=10)
CLIQUE with normal dist. of variation 0.5 (MI=10)

Time (unit: second)

800
CBCM-10R
600

CLIQUE-10R
CBCM-10SND

400
CLIQUE-10SND
CBCM-10ND(0.5)
CLIQUE-10ND(0.5)

200

0
20

40

60

80

100

Number of records (unit: 10,000)

Fig. 3. Cluster Construction Time

626

J.-W. Chang and H.-J. Lee

variation 0.5. We also use 5 and 10 for the interval of numeric attributes. Table 1
shows methods used for performance comparison in our experiment.
Figure 3 shows the cluster construction time when the interval of numeric
attributes equals 10. It is shown that the cluster construction time increases linearly in
proportion to the amount of data. This result is applicable to large amounts of data.
The experimental result shows that the CLIQUE requires about 700 seconds for one
million items of data, while our CBCM needs only 100 seconds. Because our method
creates smaller number of cells than the CLIQUE, our CBCM method leads to 85%
decrease in cluster construction time. The experimental result with the maximal
interval (MI)=5 is similar to that with MI=10.

30

20

10
0

CBCMCBCM CLIQ
-5
-10
UE-5

Random

CLIQ CBCM CBCM CLIQ
UE-10-5
-10
UE-5

CLIQ CBCM CBCM CLIQ
UE-10-5
-10
UE-5

Standard Normal Dist.

CLIQ
UE-10

Normal Dist. (0.5)

Fig. 4. Retrieval Time

Figure 4 shows average retrieval time for a given user query after clusters were
constructed. When the interval of numeric attributes equals 10, the CLIQUE needs
about 17-32 seconds, while our CBCM needs about 2 seconds. When the interval
equals 5, the CLIQUE and our CBCM need about 8-13 seconds and 1 second,
respectively. It is shown that our CBCM is much better on retrieval performance than
the CLIQUE. This is because our method creates a small number of cells by using our
cell creation algorithm, and achieves good filtering effect by using the approximation
information file. It is also shown that the CLIQUE and our CMCM require long
retrieval time when using a data set with random distribution , compared with normal
distribution of variation 0.5. This is because as the variation of a data set decreases,
the number of clusters decreases, leading to better retrieval performance.
Figure 5 shows the precision of the CLIQUE and that of our CBCM, assuming that
the section threshold is assumed to be 0. The result shows that the CLIQUE achieves
about 95% precision when the interval equals 10, and it achieves about 92% precision
when the interval equals 5. Meanwhile, our CBCM achieve over 90% precision when
the interval of numeric attributes equals 10 while it achieves about 80% precision
when the interval equals 5. This is because the precision decreases as the number of
clusters constructed increases.
Because both retrieval time and precision have a trade-off, we estimate a measure
used to combine retrieval time and precision. To do this, we define a system
efficiency measure in Eq. (5). Here EMD is the system efficiency of methods (MD)
shown in Table 1 and Wp and Wt are the weight of precision and that of retrieval time,

High-Dimensional Clustering Method for High Performance Data Mining

627

Time (unit:

second)

100

90

80

70
CBCM CBCM CLIQ CLIQ CBCM CBCM CLIQ
-5
-10
UE-5 UE-10 -5
-10
UE-5

Random

CLIQ CBCM CBCM CLIQCLIQ
UE-10-5
-10
UE-5UE-10

Standard Normal Dist.

Normal Dist. (0.5)

Fig. 5. Precision

respectively. PMD and TMD are the precision and the retrieval time of the methods
(MD). PMAX and TMIN are the maximum precision and the minimum retrieval time,
respectively, for all methods.
EMD = W p •

PMD
1
+ Wt •
PMAX
TMAX / TMIN

(5)

1
0.8
0.6
0.4
0.2
0
CBCMCBCM CLIQ
-5
-10
UE-5

Random

CLIQ CBCM CBCM CLIQ CLIQ
UE-10 -5
-10
UE-5 UE-

Standard Normal Dist.

CBCM CBCM CLIQ
-5
-10
UE-5

CLIQ
UE-10

Normal Dist. (0.5)

Fig. 6. System efficiency

Figure 6 depicts the performance results of methods in terms of their system
efficiency. The importance of the precision and that of the retrieval time depend on an
application situation, but the precision is more important than the retrieval time in
general. Thus, it is reasonable that we should set the weight of the precision to be
three times greater than that of retrieval time, i.e., Wp=0.75, Wt =0.25. The
performance result shows that our CBCM outperforms the CLIQUE with respect to
the system efficiency, regardless of the data distribution of the data sets. Especially,
the performance of our CBCM with MI=10 is the best.

628

J.-W. Chang and H.-J. Lee

5 Conclusion
The conventional clustering methods are not efficient for large, high-dimensional
data. In order to overcome the difficulty, we proposed a new efficient clustering
method with two features. The first one allows us to create the small number of cells
for large, high-dimensional data. To do this, we calculate a section of each dimension
through split index and create cells according to the overlapped area of each fixed
section. The second one allows us to apply an approximation technique to our
clustering method for the high performance data clustering. For this, we use a twolevel index structure which consists of both an approximation information file and a
cluster information file. For performance analysis, we compare our high-dimensional
clustering method with the CLIQUE method. The performance analysis results show
that our clustering method shows slightly lower precision, but it achieves good
performance on retrieval time as well as cluster construction time. Finally, our highdimensional clustering method shows a good performance on system efficiency which
is a measure to combine both precision and retrieval time. As future work, it is
required to study on the parallelism of our high-dimensional clustering method in
order to achieve higher performance on retrieval.

References
1. Han, J., Kamber, M.: Data Mining: Concepts and Techniques. Morgan Kaufmann (2000).
2. Gan, G., Wu, J.: Subspace Clustering for High Dimensional Categorical Data. ACM
SIGKDD Explorations networks, Vol. 6, Issue 2 (2004) 87-94.
3. Agrawal, R., Gehrke, J., Gunopulos, D., Raghavan, P.: Automatic Subspace Clustering of
High Dimensional Data Mining Applications. Proc. of ACM SIGMOD (1998) 94-105.
4. Nagesh, H., Goil, S., Choudhary, A.: A Scalable Parallel Subspace Clustering Algorithm for
Massive data Sets. Proc. of Int. Conf. on parallel Processing (2000) 477-486.
5. Aggrawal, C., Wolf, J., Yu, P., Procopiuc, C., Park, J.: Fast Algorithms for Projected
Clustering. Proc. of ACM SIGMOD (1999) 61-72.
6. Woo, K., Lee, J.: FINDIT: A Fast and Intelligent Subspace Clustering Algorithm using
Dimension Voting. PhD thesis, Korea Advanced Institute of Sci.&Tech, Dept. of CS
(2002).
7. Procopiuc, C., Jones, M., Agrawal, P., Murali, T.: A More Carlo Algorithm for Fast
Projective Clustering. Proc. of ACM SIGMOD (2002) 418-427.
8. http://www.almaden.ibm.com/cs/quest

