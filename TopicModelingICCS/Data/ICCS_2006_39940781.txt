Support Vector Machines for Regression
and Applications to Software Quality Prediction
Xin Jin1, Zhaodong Liu1, Rongfang Bie1,*, Guoxing Zhao2,3,and Jixin Ma3
1
College of Information Science and Technology,
xinjin796@126.com, liuzd661@163.com, rfbie@bnu.edu.cn
Beijing Normal University, Beijing 100875, P.R. China
2
School of Mathematical Sciences, Beijing Normal University, Beijing 100875, P.R. China
3
School of Computing and Mathematical Science,
The University of Greenwich, London SE18 6PF, U.K
G.Zhao@gre.ac.uk,
j.ma@gre.ac.uk

Abstract. Software metrics are the key tool in software quality management.
In this paper, we propose to use support vector machines for regression applied to software metrics to predict software quality. In experiments we compare this method with other regression techniques such as Multivariate Linear
Regression, Conjunctive Rule and Locally Weighted Regression. Results on
benchmark dataset MIS, using mean absolute error, and correlation coefficient as regression performance measures, indicate that support vector machines regression is a promising technique for software quality prediction. In
addition, our investigation of PCA based metrics extraction shows that using
the first few Principal Components (PC) we can still get relatively good
performance.

1 Introduction
Software quality management, which is an important aspect of software project development, is an ongoing comparison of the actual quality of a product with its expected quality [16]. Software metrics are the key tool in software quality management. Many researchers have sought to analyze the connection between software
metrics and code quality [8][13][14][15][17][23]. The methods they used fall into
four mainly categories: association analysis (association rules), clustering analysis (kmeans, fuzzy c-means), classification analysis (decision trees, layered neural networks, Holographic networks, logistic regression, genetic granular classification [12])
and prediction analysis (linear regression).
In this paper we propose to use support vector machines for regression to predict
software quality. Support vector machine technique has attracted many researchers in
optimization and machine learning areas [19,22]. In the case of regression, the objective is to choose a hyperplane with small norm while simultaneously minimizing the
sum of the distances from the data points to the hyperplane.
*

Corresponding author.

V.N. Alexandrov et al. (Eds.): ICCS 2006, Part IV, LNCS 3994, pp. 781 – 788, 2006.
© Springer-Verlag Berlin Heidelberg 2006

782

X. Jin et al.

The remainder of this paper is organized as follows. In Section 2, we describe the
software metrics and benchmark dataset we used. Section 3 presents the support vector regression method. Section 4 describes three comparison algorithms. Section 5
introduces PCA. Section 6 presents the performance measures and the experiment
results. Conclusions are covered in Section 7.

2 Software Metrics
We investigated the twelve software metrics, as shown in Table 1, which are used in
the famous benchmark dataset MIS [15,12]. Simple counting metrics such as the
number of lines of source code or Halstead’s number of operators and operands describe how many “things” there are in a program. More complex metrics such as
McCabe’s cyclomatic complexity or Bandwidth attempt to describe the “complexity”
of a program, by measuring the number of decisions in a module or the average level
of nesting in the module, respectively.
Table 1. Description of the MIS dataset with a detailed characterization of the software
metrics [18]
Metrics
LOC
CL
TChar
TComm
MChar
DChar
N
N’
NF
V(G)
BW

Changes

Detailed Description
Number of lines of code including comments, declarations and the main
body of the code
Number of lines of code, excluding comments
Number of characters
Number of comments
Number of comment characters
Number of code characters
Halstead’s Program Length N = N1+N2, N1 is the total number of
operators, N2 is the total number of operands
Halstead’s Estimate of Program Length N’= n1log1n1 + n2log2n2, n1 is the
number of unique operators, n2 is the number of unique operands
Jensen’s Estimate of Program Length Metric log1n1! + log2n2!
McCabe’s Cyclomatic Complexity Metric, where V(G) = e−n+2, and e
represents the number of edges in a control graph of n nodes
Belady’s Bandwidth measure BW = (ΣiiLi)/n, Li represents the number of
nodes at level “i” in a nested control flow graph of n nodes. This measure
indicates the average level of nesting or width of the control flow graph
representation of the program
Number of changes

In this study, MIS is represented by a subset of the whole MIS data with 390 modules written in Pascal and FORTRAN. These modules consist of approximately
40,000 lines of code. Our goal is to develop a prediction model of software quality in
which the number of modifications (changes) is projected on a basis of the values of
the 11 software metrics that is used to characterize a software module. We cast the
problem in the setting of regression, the explanatory variables are the first eleven soft-

Support Vector Machines for Regression and Applications to Software Quality Prediction 783

ware metrics and the dependent variable is the number of changes. Software modules,

which have no changes, could be deemed to be fault-free, while software modules
with the number of changes being too big, for example, over 10, can be sought as
potentially highly faulty modules.

3 Support Vector Machine Regression (SVR)
Specifically, the ε-insensitive support vector regression will be used for predicting
software quality. In the ε-insensitive support vector regression, our goal is to find a
function f(x) that has an ε-deviation from the actually obtained target yi for all training
data and at the same time is as flat as possible [10]. Suppose f(x) takes the following
form:
f(x) = wx + b
w ∈ X,b ∈ ℜ.
(1)
Then, we have to solve the following problem:

min

1 2
w
2

yi − wxi − b ≤ ε

Subject to

(2)

wxi + b − yi ≤ ε

In the case where the constraints are infeasible, we introduce slack variables. This
case is called soft margin formulation, and is described by the following problem.

min
Subject to

l
1 2
w + C ∑ (ξ i + ξ i* )
2
i =1

yi − wxi − b ≤ ε + ξi

ξi,ξi* ≥ 0

wxi + b − yi ≤ ε + ξi*

Cf0

(3)

where C determines the trade-off between the flatness of the f(x) and the amount up to
which deviations larger than ε are tolerated. This is called ε-insensitive loss function:

⎧ 0
if ξ ≤ ε ⎫
ξε =⎨
⎬
⎩ ξ − ε if ξ f ε ⎭

(4)

By constructing the Lagrangian function, we formulate the dual problem as
l

l

l

l

i =1

i =1

Max − 1 ∑∑ (λi − λi* )(λ j − λ*j )xi x j − ε ∑ (λi + λi* ) + ∑ yi (λi − λi* )

2

Subject to

i =1 j =1

∑ (λ − λ ) = 0
*
i

i

(5)

λi ,λi* ∈ (0 ,C )

At the optimal solution, we obtain
l

w* = ∑ (λi − λi* )xi

(6)

i =1

l

f(x) = ∑ (λi − λi* )xi x + b*
i =1

(7)

784

X. Jin et al.

We compute the optimal value of b from the complementary slackness conditions:

b* = yi − w* xi − ε λ i ∈ ( 0 , C )
*
*
and b = yi − w xi + ε λi∗ ∈ ( 0 ,C)

(8)

In some case, we need to map input space into feature space and try to find a hyperplane in the feature space by using the trick of kernel functions:
Max −

l
l
1 l l
(λi − λi* )(λ j − λ*j )K(xi ,x j ) − ε ∑ (λi + λi* ) + ∑ yi (λi − λi* )
∑∑
2 i =1 j =1
i =1
i =1

(9)

At the optimal solution, we obtain
l

w* = ∑ (λi − λi* )K(xi ),

(10)

i =1

l

f(x) = ∑ (λi − λi* )K(xi , x) + b

(11)

i =1

where K(.,.) is a kernel function. Any symmetric positive semi-definite function,
which satisfies Mercer's conditions, can be used as a kernel function in the SVMs
context [10, 20]. In this paper, we use the linear kernel [11].

4 Regressors for Comparison
4.1 Multivariate Linear Regression
Multivariate Linear Regression (MLR) finds a set of basis vectors wxi and corresponding regressors βi in order to minimize the mean square error of the vector y. The
basis vectors are described by the matrix Cxx-1Cxy. A low-rank approximation to this
problem can be defined by minimizing
2
M
⎡
⎤
T
ε = E ⎢ y − ∑ βi wxi xwyi ⎥
i =1
⎥⎦
⎣⎢
2

(12)

where M=dim(y), N<M and the orthogonal basis wyi span the subspace of y which
gives the smallest mean square error given the rank N.
4.2 Conjunctive Rule
Conjunctive Rule (CR) consists of antecedents "AND"ed together and the consequent
(prediction value) for the regression. This learner selects an antecedent by computing
the Information Gain of each antecendent and prunes the generated rule using Reduced Error Prunning (REP) or simple pre-pruning based on the number of antecedents [1]. The Information is the weighted average of the mean-squared errors of both
the data covered and not covered by the rule. In pruning, the weighted average of the
mean-squared errors on the pruning data is used.
4.3 Locally Weighted Regression
Locally Weighted Regression (LWR) is a memory-based method that performs a
regression around a point of interest using only training data that are local to that

Support Vector Machines for Regression and Applications to Software Quality Prediction 785

point [3,4]. We consider here a form of locally weighted regression that is a variant of
the LOESS model [5]. The LOESS model performs a linear regression on points in
the data set, weighted by a kernel centered at x. The kernel shape is a design parameter for which we use the Linear.

5 Principal Component Analysis
Principal Component Analysis (PCA) is a famous multivariate data analysis method
that is useful in linear feature extraction [6,7]. The PCA finds a linear transformation
y=Wx such that the retained variance is maximized. Each row vector of W
corresponds to the normalized orthogonal eigenvector of the data covariance matrix.
One simple approach to PCA is to use singular value decomposition (SVD). Let us
denote the data covariance matrix by Rx (0) = E{x (t)xT (t)}. Then the SVD of Rx (0)
gives Rx (0) =UDUT, where U= [Us,Un] is the eigenvector matrix (i.e. modal matrix)
and D is the diagonal matrix whose diagonal elements correspond to the eigenvalues
of Rx (0) (in descending order). Then the PCA transformation from m-dimensional
data to n-dimensional subspace is given by choosing the first n column vectors, i.e., n
principal component vector y is given by y=UsTx.

6 Experiment Results
10-fold cross-validation on the benchmark MIS dataset, available on [9], is used for
estimating prediction performance.
6.1 Performance Measures
We use the following performance measures:
Mean Absolute Error (MAE): MAE provides a measure of how close a prediction
model is to the actual data.

MAE =

1 n
∑ | ai − pi |
n i =1

(13)

where ai and pi is the actual and predicted value for the ith test case. MAE ranges
from 0 to infinity, with 0 corresponding to the ideal. the smaller the MAE the better.
Correlation Coefficient (CC): CC is a measure of how well trends in the predicted
values follow trends in past actual values [21]. It measures how well the predicted
values from a forecast model "fit" with the real-life data. A perfect fit gives a CC of
1.0.
CC =

∑

n
i =1

( pi − p )( ai − a )

∑ i=1 ( pi − p)2 ∑ i=1 (ai − a )2
n

(14)

n

The higher the CC the better.
6.2 Results
Fig.1 shows MAE of different regressors on the original MIS metrics and the PCA
extracted data. On the original metrics, SVR with the linear kernel (SVR_L) get the

786

X. Jin et al.

best performance by achieving a minimum of 3.98 MAE. As shown in Fig.1, using
the first few Principal Components (PC) we can still get relatively good performance.
For CRule, using PCs is even better than using the original metrics.
Mean Absolute Error
6

MIS

5.5

MIS_PCA1

5

MIS_PCA2

4.5
4

MIS_PCA3

3.5
3
MLR

SVR_L

CRule

LWR_L

Regr essor

Fig. 1. MAE of different regressors on the original MIS metrics and the PCA extracted data.
MLR= Multivariate Linear Regression, SVR_L=Support Vector Regression with Linear kernel,
CRule=Conjunctive Rule, LWR_L=Locally Weighted Regression with Linear kernel.
MIS_PCA1 means using the first Principal Component, MIS_PCA2 means using the first two
Principal Components, etc.

Fig.2 shows CC of different regressors on the original MIS metrics and the PCA
extracted data. SVR with Linear kernel get the best performance by achieving a
maximum of 0.77 CC. For CRule, using PCs is better than using the original metrics.

Cor r el at i on Coef f i ci ent

0. 8

MI S
MI S_PCA1
MI S_PCA2
MI S PCA3

0. 75
0. 7
0. 65
0. 6
0. 55
0. 5
MLR

SVR_L

CRul e

LWR_L

Regr es sor

Fig. 2. CC of different regressors on the original MIS metrics and PCA extracted data. MLR=
Multivariate Linear Regression, SVR_L=Support Vector Regression with Linear kernel,
CRule=Conjunctive Rule, LWR_L=Locally Weighted Regression with Linear kernel.
MIS_PCA1 means using the first Principal Component, MIS_PCA2 means using the first two
Principal Components, etc.

7 Conclusions
In this paper we propose to use support vector machines for regression applied to
software metrics to predict software quality. Comparison is done with three other

Support Vector Machines for Regression and Applications to Software Quality Prediction 787

techniques: Multivariate Linear Regression, Conjunctive Rule and Locally Weighted
Regression. Results on benchmark dataset MIS, using MAE and CC as performance
measures, indicate that support vector machines regression is a promising technique
for software quality prediction. The investigation of PCA based metrics extraction
show that using the first few Principal Components we can still get relatively good
performance.

Acknowledgments
This work was supported by the National Science Foundation of China under the
Grant No. 10001006 and No. 60273015.

References
1. I.Witten, E.Frank: Data Mining –Practical Machine Learning Tools and Techniques with
Java Implementation. Morgan Kaufmann (2000)
2. Friedman, J.H.: Stochastic Gradient Boosting. Technical Report, Stanford University
(1999)
3. Atkeson, C., A. Moore, S. Schaal: Locally Weighted Learning. AI Reviews (1996)
4. Cohn, D.A., Ghahramani, Z., Jordan, M.I.: Active Learning with Statistical Models. Journal of Artificial Intelligence Research, Vol. 4, pp. 129-145 (1996)
5. Cleveland, W., Devlin, S., Grosse, E.: Regression by Local Fitting. Journal of Econometrics, 37, pp. 87-114 (1988)
6. Zeng, X.Y., Chen, Y.W., et al: A New Texture Feature based on PCA Maps and Its Application to Image Retrieval. IEICE Trans. Inf. and Syst., E86-D 929-936 (2003)
7. Diamantaras, K.I. and Kung, S.Y.: Principal Component Neural Networks: Theory and
Applications. John Wiley & Sons, INC (1996)
8. D. Garmus, D. Herron: Measuring The Software Process, Prentice Hall, Upper Saddle
River, NJ (1996)
9. MIS: http://www.win.tue.nl/~jromijn/2IW30/2IW30_statistics/LYU/DATA/CH12 (2006)
10. Theodore B. Trafalis, Huseyin Ince: Support Vector Machine for Regression and Applications to Financial Forecasting, ijcnn, p.6348, IEEE-INNS-ENNS International Joint Conference on Neural Networks (IJCNN'00) Volume 6 (2000)
11. A.J. Smola and B. Scholkopf, A Tutorial on Support Vector Regression, NEUROCOLT2
Technical Report Series, NC2-TR-1998-030 (1998)
12. Witold Pedrycz, Giancarlo Succi: Genetic Granular Classifiers in Modeling Software
Quality. Journal of Systems and Software 76(3): 277-285 (2005)
13. W. Pedrycz, G. Succi, M.G. Chun, Association Analysis of Software Measures, Int. J of
Software Engineering and Knowledge Engineering, 12(3): 291-316 (2002)
14. K.H. Muller, D.J. Paulish, Software Metrics, IEEE Press/Chapman & Hall, London, 1993.
15. J. C. Munson, T. M. Khoshgoftaar: Software Metrics for Reliability Assessment, in Handbook of Software Reliability and System Reliability, McGraw-Hill, Hightstown, NJ, 1996.
16. 16.Scott Dick, Aleksandra Meeks, Mark Last, Horst Bunke, Abraham Kandel: Data Mining in Software Metrics Databases. Fuzzy Sets and Systems 145(1): 81-110 (2004)
17. W. Pedrycz, G. Succi, P. Musilek, X. Bai: Using Self-Organizing Maps to Analyze Object
Oriented Software Measures. J. of Systems and Software, 59, 65-82 (2001)

788

X. Jin et al.

18. P. K. Simpson: Fuzzy Min-Max Neural Networks. Part 1: Classification, IEEE Trans.
Neural Networks, Vol. 3, pp. 776-786 (1992)
19. M.S. Bazaraa, H.D. Sherali, and C.M. Shetty: Nonlinear Programming: Theory and Algorithms, John Wiley &Sons Inc., New York (1993)
20. C. Cortes and V. Vapnik, Support Vector Networks, Machine Learning, 20, 273-297
(1995)
21. Correlation Coefficient: http://www.neatideas.com/cc.htm (2006)
22. T. Joachims, Making Large-Scale SVM Learning Practical, Technical Report, LS-8-24,
Computer Science Department, University of Dortmund (1998)
23. R. Subramanyan and M.S. Krishnan: Empirical Analysis of CK Metrics for ObjectOriented Design Complexity: Implications for Software Defects, IEEE Trans. Software
Eng., Vol. 29, pp. 297-310, Apr (2003)

