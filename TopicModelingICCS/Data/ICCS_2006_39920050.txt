Numerical Modelling of Poroviscoelastic
Grounds in the Time Domain Using a Parallel
Approach
Arnaud Mesgouez1 , Ga¨elle Lefeuve-Mesgouez1, Andr´e Chambarel1 ,
and Dominique Foug`ere2
1
UMR A Climate, Soil and Environment, Universit´e d’Avignon,
Facult´e des Sciences, 33 rue Louis Pasteur, F-84000 Avignon, France
{arnaud.mesgouez, gaelle.mesgouez, andre.chambarel}@univ-avignon.fr
2
UMR 6181 Mod´elisation et Simulation Num´erique en M´ecanique et G´enie des,
Proc´ed´es, 38 rue F. J. Curie, Tech. de Chˆ
ateau-Gombert, F-13451 Marseille, France
fougere@l3m.univ-mrs.fr

Abstract. In this paper, we present a parallelized ﬁnite element code
developed to study wave propagation phenomena, speciﬁcally in porous
soils problems which usually require millions of degrees of freedom. The
parallelization technique uses an algebraic grid partitioning managed by
a Single Program Multiple Data (SPMD) programming model. Message
Passing Interface (MPI) library speciﬁcation is the standard used to exchange data between processors. The architecture of the code is explained
and numerical results show its performance.

1

Introduction

The study of the mechanical wave propagation in porous media is a subject of
great interest in diverse scientiﬁc ﬁelds ranging from environmental engineering
or vibration isolation to geomechanics. At the macroscopic scale, the medium is
considered as a two-phase continuum. The Biot theory is known as the reference
theory to deal with the macroscopic mechanical wave propagation phenomenon,
see Biot [1] or Coussy [2] for instance.
Theoretical works are restricted to simple geometries. Consequently, they have
to be completed by numerical approaches such as Finite Element or Boundary Element Methods, allowing the study of more complex problems to better represent
the ground. The diﬃcult study of transient regimes in geomechanics has been
treated numerically by several authors but only for speciﬁc cases, Zienkiewicz
and Shiomi [3], Simon et al. [4] and Gajo et al. [5] for example. In particular, in
many cases, the tortuosity and the physical damping parameters are not taken
into account.
Moreover, even with an eﬃcient and optimized ﬁnite element code, only a
restricted range of problems can be treated. As a matter of fact, solution of
practical problems (for instance, realistic 3D geometries, and problems with
short pulse load needing ﬁne meshes for representing well the high frequencies)
V.N. Alexandrov et al. (Eds.): ICCS 2006, Part II, LNCS 3992, pp. 50–57, 2006.
c Springer-Verlag Berlin Heidelberg 2006

Numerical Modelling of Poroviscoelastic Grounds

51

usually requires millions of degrees of freedom. This is often virtually out of capabilities of contemporary sequential computers either because of lack of memory
or abundantly long computation time. In all these cases, parallel programming
techniques may be a good solution to overcome the computational complexity.
Nevertheless, to our knowledge, no parallelization of numerical approach on
the complete Biot theory in the transient domain exists in the literature. In fact,
papers presenting parallel computing of the ﬁnite element method often deal with
elastodynamic problems using the domain decomposition method, Papadrakakis
and Bitzarakis [6], Lirkov [7] and Bohlen [8].
In this paper, the authors propose a parallelized version of a ﬁnite element
C++ code speciﬁcally developed at the CSE Laboratory to study transient wave
propagation. This approach includes the whole Biot theory with all the couplings
which represent the interactions between the solid and ﬂuid phases. The sequential version has previously been presented at ICCS 2005, Mesgouez et al. [9].
The study of the diﬀerent time-consuming parts yields a parallelization technique using an algebraic grid partitioning managed by a SPMD programming
technique. MPI standard library is used to exchange data between processors.
For this, numerical results, obtained for a two-dimensional problem, include the
analysis of speed-up and eﬃciency on a SGI Origin 3800. Complementary results
compare the performance obtained by another supercomputer (AMD-ATHLON
cluster), on which MPICH and SCI-MPI implementations of MPI standard is
used. First results with a three-dimensional geometry solving a problem with
more than 2,000,000 unknowns are obtained.

2
2.1

Mechanical and Numerical Works
Spatial Scales and Macroscopic Approach

When we focus our attention on the description of a porous medium, the ﬁrst
question to be put is that of the spatial scale of analysis: indeed, two approaches
are conceivable. The ﬁrst one is situated at the microscopic scale. The characteristic length size is the dimension of the pore. In this conﬁguration, the solid
matrix is partially or completely ﬁlled with one or several viscous ﬂuids. One
geometric point is thus located in one of the diﬀerent identiﬁable solid or ﬂuid
phases. Mechanical equations of each phase and mixture with compatible interface conditions are written. They correspond to those of linear elasticity in
the solid and those of Stokes in the ﬂuid. This approach deals with problems
like interface modelling or description of microscopic geological structures. Homogenization is then obtained through asymptotic developments or averaging
procedures and leads to a macroscopic description of the porous medium, see
Terada and al. [10] or Coussy et al. [11] for instance. We obtain thus the famous set of macroscopic mechanical equations for a representative elementary
volume. In this macroscopic spatial description, the porous medium is seen as
a two-phase continuum. This scale, we study here, is well adapted to most of
practical geomechanical problems.

52

A. Mesgouez et al.

Writing ui and Ui respectively the macroscopic solid and ﬂuid displacements
components, Biot’s equations can be written with usual notations as follows:
¨i
σij,j = (1 − φ)ρs u
¨i + φρf U
φ ˙
¨i
p,i = − K (Ui − u˙ i ) + ρf (a − 1)¨
ui − aρf U

(1)

σij = λ0v εkk δij + 2μv εij − βpδij
1
p
−φ (Uk,k − uk,k ) = βuk,k + M

(3)
(4)

(2)

σij are the total Cauchy stress tensor components and p is the pore pressure.
The soil’s characteristics are: λ0v and μv (drained viscoelastic equivalent Lam´e
constants), ρs and ρf (solid grains and ﬂuid densities), φ (porosity), K (hydraulic
permeability representing the viscous coupling), a (tortuosity standing for the
mass coupling), M and β (Biot coeﬃcients including the elastic coupling).
2.2

Finite Element Formulation and Numerical Resolution

To determine the solid and ﬂuid displacements in the ground, we develop a
numerical code based on the ﬁnite element method for the space integration,
coupled to a ﬁnite diﬀerence method for the time integration. The main steps
are:
– some boundary and initial conditions are associated to the previous partial
diﬀerential system. Some modiﬁcations on the ﬁeld equations are done in
order to lead to a Cauchy’s problem.
– integral forms are obtained using the weighted residual method. They are
then spatially and analytically discretized and lead to a time diﬀerential
system. The global diﬀerential system to be solved can be written as
[M ]

d
{W (G) } + [K]{W (G) } = {F (G) }
dt

(5)

[M ] and [K] are respectively the global mass and stiﬀness matrixes. {W (G) }
and {F (G) } are the global vectors of unknowns and solicitation. With the
developed technique, the mass matrix is diagonal and can be easily inverted.
– the backward ﬁnite diﬀerence method modiﬁed with an upward time parameter is used to obtain an approximate solution of the problem.
2.3

Structure of the Code and Parallelization

The sequential code called FAFEMO (Fast Adaptive Finite Element Modular
Object), developed to solve the previous problem, constitutes an eﬃcient code
to deal with transient 2D problems and small 3D ones. The use of a matrix free
technique, not necessary for small cases, becomes interesting for huge ones. An
expert multigrid system is also used to optimize the problem size and yields a
modiﬁcation of the global matrixes at each time step. The two previous techniques lead to a high performance level both for the storage and the CPU costs.

Numerical Modelling of Poroviscoelastic Grounds

53

Table 1. Time proﬁle of the 2D sequential code
reading of the data ﬁles
and element class
7.45%

elementary matrixes
class
90.60%

building-resolution
class
1.95%

Element class:
Master processor

Construction of the element
and the associated functions
Single heritage

Elementary matrixes class:
Construction of [Ke] [Me] {fe}

Parallelization
Elements 1 to n/p:
processor 1

Elements (i-1)*n/p to
i*n/p: processor i

Master processor

Elements (p-1)*n/p to
n: processor p

Slave processors
Communications
Single heritage

Master processor

Building resolution class:
Assembling and resolution

Solver

Fig. 1. Structure of the parallelized version of the C++ ﬁnite element code

The C++ code is organized in three classes connected by a single heritage: element, elementary matrixes and building-resolution classes.
For huge problems, the elementary vectors have to be calculated and assembled for each time step since they are too expensive in terms of Input/Output
cost to be stored. In order to treat 3D problems and to perform intensive 2D
parametric studies, we propose a parallelization of the code to reduce the time
calculation.
The Unix/Linux gprof tool draws a time proﬁle of the sequential code. For a
two-dimensional application, the elapsed time is divided as presented in Table 1,
for each of the three classes.
The part which is the largest consumer of elapsed time clearly appears to
be the elementary matrixes class. This can be explained as the elementary matrixes have to be calculated for each time step. Besides, as we use a matrix free
technique with a diagonal mass matrix, the resolution part is more eﬃcient and
needs little computational time. Moreover, the process of construction of [Ke ],
[Me ] and {fe } is iterative and independent element by element. This independent and time-consuming loop can thus be divided into several processors by
distributing the n elements between p equitable parts.

54

A. Mesgouez et al.

We use a grid partitioning based on algebraic decomposition which is performed randomly without any geometric factors. Several advantages are:
– unlike the domain decomposition method, this technique does not need any
particular interface management. This is particularly important when the
expert multigrid system is activated, or when the geometry is changed.
– moreover, when the size of the grid is modiﬁed, the algebraic distribution
of the elements leads to an equitable load balancing between processors at
each time step.
– another advantage of this approach is that the implementation is as close to
the original sequential solver as possible.
A SPMD programming model manages the algebraic distribution of the different ﬁnite elements of the grid and MPI standard library is used to exchange
data concerning the elementary matrixes between master and slave processors.
The architecture of the parallelized version of the code is summarized on ﬁgure 1.

3

Results

In this section, numerical results are presented for a 2D problem involving
200,000 unknowns to estimate the performance of the parallelized version of
the FAFEMO code.
3.1

SGI Origin 3800

We have worked on SGI Origin 3800 installed at the National Computer Center
of Higher Education (CINES, Montpellier, France). SGI ORIGIN is a shared
global memory based on cc-NUMA architecture (cache-coherent Non-Uniform
Access Memory) composed of 768 processors MIPS R14000 (500 MHz, 512 Mo
RAM), distributed among 80 nodes. The internal network (Bristled hypercube)
give a speed of 1,6 GB/s.
Figures 2 and 3 present the evolution of the elapsed times and the speed-up
depending on the number of processors (up to 64). The main conclusions are:
1400

12

1200

10

8

Speed-up

Elapsed times

1000

800

600

6

4
400

2

200

0

0
0

10

20

30

40

50

60

0

10

20

30

40

50

60

Number of processors

Number of processors

Fig. 2. Elapsed times in minutes up to 64
processors on SGI Origin 3800

Fig. 3. Speed-up up to 64 processors on
SGI Origin 3800

Numerical Modelling of Poroviscoelastic Grounds

55

250

5,0
4,5
200

Speed-up

Elapsed times

4,0

150

100

3,5
3,0
2,5
2,0
1,5

50

1,0

0

1

2

3

4

5

6

7

8

9

Number of processors

Fig. 4. Elapsed times in minutes up to 8
processors on cluster CHOEUR

0,5
0

1

2

3

4

5

6

7

8

9

Number of processors

Fig. 5. Speed-up up to 8 processors on
cluster CHOEUR

– the elapsed time is reduced considerably for a few number of processors and
the communications do not aﬀect the performance of the parallelization.
For instance for 8 processors, the speed-up value equals 6.45. Compared to
literature results on elastodynamic problems, these results are pretty good.
– for a larger number of processors, the performance of the code is not so
interesting. Actually, the maximum of the speed-up curve corresponds to an
optimal value of the number of processors situated between 20 and 30.
3.2

Cluster and Comparison

We have also worked on cluster CHOEUR, installed at the Laboratoire de Mod´elisation et Simulation Num´erique en M´ecanique et G´enie des Proc´ed´es (MSNMGP/L3M) of the CNRS and Aix-Marseille University. It is an AMD-ATHLON
cluster, built upon 20 AMD-760MP 1800 bi-processors nodes interconnected with
gigabit Ethernet, and 6 AMD-760MPX 1800+ bi-processors nodes (1530 Mhz,
1 GB RAM) interconnected with SCI-Dolphin High Speed 2,5 Gb network. The
cluster is managed by an extra master node (2KMhz, 2 GB RAM).
Figures 4 and 5 present the elapsed times and the speed-up. Results are limited to 8 processors. As previously, the results show a real gain for the time
calculation. Nevertheless, we can not visualize the maximum of speed-up for
cluster CHOEUR which does not provide enough processors to reach it.
The elapsed times on SGI Origin 3800 are much longer than the ones obtained
on cluster CHOEUR: for a single processor, more than 1300 min (i.e. 22 h) on
SGI Origin 3800 and only 230 min (i.e. less than 4 h) on CHOEUR, because
processors on CHOEUR are more recent and eﬃcient.
The speed-up and the eﬃciency are better on SGI than on CHOEUR: for
instance for 5 processors, the speed-up obtained on SGI equals 4.3 and 3.7 on
CHOEUR. Calculation time is longer dominating compared to communications
time on SGI due to less eﬃcient processors.
Moreover, the inter-processor communication process is a very important parameter to take into account. Particularly, due to the hierarchical architecture of
the memory of SGI (NUMA, non uniform memory access), the communication
times decrease with the number of processors, but only over a limited range of

56

A. Mesgouez et al.

processors. Concerning CHOEUR, communications are optimal since the limits
of the mother board PCI bus are not reached.
Consequently, the diﬀerence in performance between SGI and CHOEUR is
mainly due to the diﬀerence of quality of the two kinds of processors and the
management of communications.
3.3

Physical Example

Previous 2D results, obtained with the sequential version of the code, have been
presented for instance in [9] and [12]. Parametric studies on the inﬂuence of the
diﬀerent couplings and an approach of heterogeneous soil have been carried out.
The interest of the parallelized version is to be able to lead parametric study
more rapidly and to tackle problem much more expensive in terms of storage and
CPU costs, like 3D geometries. The 3D problem presented here is a 2,300,000
unknowns problem and concerns an homogeneous poroviscoelastic semi-inﬁnite
half-space ground subjected to an impulsional load applied on the center of the
surface. Figure 6 presents for instance the solid displacement contour levels for a
given time on the surface of the ground. The propagation of the Rayleigh wave
is clearly perceptible and corresponds to the dark area. The ﬁrst compressional
wave has almost reached the boundary. Figure 7 shows similar results for the
ﬂuid phase for a high hydraulic permeability value: note that the two behaviors
are strongly uncoupled. These results allow great perspectives for the study of
3D heterogeneous soils.

Fig. 6. Contour levels of the solid displacements (dimensionless time = 0.6)

4

Fig. 7. Contour levels of the ﬂuid displacements (dimensionless time = 0.6)

Conclusion and Further Works

A parallelized ﬁnite element code has been presented to study wave propagation
phenomena in poroviscoelastic grounds. In fact, the applications are wider and
can concern for instance porous bones or foams. Besides, the code can treat all
propagation wave phenomena: a version studying electromagnetic wave propagation have been developed in the same way.

Numerical Modelling of Poroviscoelastic Grounds

57

With the parallelized code, one can consider the work to be continued: i) a
study of more complex ground : an approach of partially saturated ground is
under progress ii) an analysis of 2D heterogeneous random media mixing two
kinds of soils has shown the existence of diﬀerent thresholds for ﬂuid and solid
phases: we would like to do similar analysis for a 3D geometry in order to know
if conclusions are identical or not iii) a study of the coupling of electromagnetic
and seismic waves, which represents a hard problem because of the two diﬀerent
time scales.
Moreover, concerning the numerical point of view, some improvements can
still be done: for instance a compressed message passing technique for internode communication could allow a signiﬁcant reduction of the communications
time.
To achieve these aims, an access of more computational resources at the
CINES has been asked for 2006. Besides, SGI will be soon replaced by a massive
parallel supercomputer which will allow more eﬃcient abilities.

References
1. Biot, M.A.: Theory of propagation of elastic waves in a ﬂuid-saturated porous solid.
I- Low-frequency range. J. Acoust. Soc. Am. 28(2) (1956) 168–178
2. Coussy, O.: M´ecanique des milieux poreux. (1991) Paris: Ed. Technip
3. Zienkiewicz, O.C., Shiomi, T.: Dynamic behaviour of saturated porous media: the
generalized Biot formulation and its numerical solution. Int. J. Numer. Anal. Methods Geomech. 8 (1984) 71–96
4. Simon, B.R., Wu, J.S.S., Zienkiewicz, O.C., Paul, D.K.: Evaluation of u-w and
u-π ﬁnite element methods for the dynamic response of saturated porous media
using one-dimensional models. Int. J. Numer. Anal. Methods Geomech. 10 (1986)
461–482
5. Gajo, A., Saetta, A., Vitaliani, R.: Evaluation of three and two ﬁeld ﬁnite element
methods for the dynamic response of saturated soil. Int. J. Numer. Anal. Methods
Geomech. 37 (1994) 1231–1247
6. Papadrakakis, M., Bitzarakis, S.: Domain decomposition PCG methods for serial
and parallel processing. Adv. Eng. Softw. 25 (1996) 291–307
7. Lirkov, I.: MPI solver for 3D elasticity problems. Math. Comput. Simul. 61 (2003)
509–516
8. Bohlen, T.: Parallel 3-D viscoelastic ﬁnite diﬀerence seismic modelling. Comput.
Geosci. 28 (2002) 887–899
9. Mesgouez, A., Lefeuve-Mesgouez, G., Chambarel, A.: Simulation of transient mechanical wave propagation in heterogeneous soils. Lect. Notes Comput. Sc. 3514
(2005) 647–654
10. Terada, K., Ito, T., Kikuchi, N.: Characterization of the mechanical behaviors of
solid-ﬂuid mixture by the homogenization method. Comput. Methods Appl. Mech.
Eng. 153 (1998) 223–257
11. Coussy, O., Dormieux, L., Detournay, E.: From Mixture theory to Biot’s approach
for porous media. Int. J. Solids Struct. 35 (1998) 4619–4635
12. Mesgouez, A., Lefeuve-Mesgouez, G., Chambarel, A.: Transient mechanical wave
propagation in semi-inﬁnite porous media using a ﬁnite element approach. Soil
Dyn. Earthq. Eng. 25 (2005) 421–430

