A Leave-One-Out Bound for ν−Support Vector
Regression
Qin Ruxin1 , Chen Jing1, , Deng Naiyang1 , and Tian Yingjie2
1

2

College of Science, China Agricultural University, 100083, Beijing, China
jing quchen@163.com
Research Center on Data Technology & Knowledge Economy, Chinese Academy of
Sciences, 100080, Beijing, China

Abstract. An upper bound on the Leave-one-out (Loo) error for
ν−support vector regression (ν−SVR) is presented. This bound is based
on the geometrical concept of span. We can select the parameters of
ν−SVR by minimizing this upper bound instead of the error itself, because the computation of the Loo error is extremely time consuming.
We also can estimate the generalization performance of ν−SVR with the
help of the upper bound. It is shown that the bound presented herein
provide informative and eﬃcient approximations of the generalization
behavior based on two data sets.

1

Introduction

Support Vector Machines (SVMs) have been proven to be powerful and robust
methodologies for learning from empirical data. They have been successfully
applied to many real-world problems, such as classiﬁcation problem, regression
problem. It depends critically on the kernel and the parameters. One of the most
reasonable approaches is to select the kernel and the parameters by minimizing
the bound of Leave-one-out (Loo) error. However, the computation of the Loo
error is extremely time consuming. Therefore, an eﬃcient strategy is to minimize
an upper bound of the Loo error, instead of the error itself. In fact, for Support
Vector Classiﬁcation (SVC) and ε−support vector regression (ε−SVR), some
useful bounds have been proposed, see e.g. [1],[2],[3].
This paper is concerned with ν−support vector regression (ν−SVR). More
precisely, we will induce a bound for the standard ν−SVR which corresponds
to the famous ν−Support Vector Classiﬁcation (ν−SVC) algorithms. In our
experiments, the bound and their corresponding true Loo errors of some standard
benchmark are compared. The results show the validity of the approach proposed
in this paper.
This paper is organized as follows. In section 2, we propose the ν−SVR algorithm. In section 3, we induce Loo bound for ν−SVR, and in section 4 experiments are performed to verify the validity of our method.
Supported by the National
10631070,60573158,10601064).
Corresponding author.

Natural

Science

Foundation

Y. Shi et al. (Eds.): ICCS 2007, Part III, LNCS 4489, pp. 669–676, 2007.
c Springer-Verlag Berlin Heidelberg 2007

of

China(No.

670

2

R. Qin et al.

ν−Support Vector Regression(ν − SV R)

In this section, we propose the standard of ν − SV R. Consider a regression
problem with a training set
T = {(x1 , y1 ), · · · , (xl , yl )} ∈ (X × Y)l ,

(1)

where, xi ∈ X = Rn , yi ∈ Y = R, i = 1, · · · , l. Suppose that the loss function is
selected to be the ε−insensitive loss function.
c(x, y, f (x)) = |f (x − y)|ε = max{0, |y − f (x)| − ε}.

(2)

In SVM framework, the input space is ﬁrst mapped to a higher dimensional
space H by
x = Φ(x),

(3)

T¯ = {(x1 , y1 ), · · · , (xl , yl )} ∈ (H × Y)l ,

(4)

and the training set T turns to be

where, xi = Φ(xi ) ∈ H = Rn , yi ∈ Y = R, i = 1, · · · , l.
The dual optimization problem of standard ν−SVR based on the training set
T is
l

max

α(∗) ∈R2l

(∗)

W (α

l

1
)=−
(α∗ − αi )(α∗j − αj )K(xi · xj ) +
(α∗i − αi )y(5)
i
2 i,j=1 i
i=1

l

(αi − α∗i ) = 0 ,

s.t.

(6)

i=1
(∗)

0

αi

C
, i = 1, · · · , l ,
l

(7)

l

(αi + α∗i )

C ·ν ,

(8)

i=1

where α(∗) = (α1 , α∗1 , · · · αl , α∗l )T , and K(xi , xj ) = (xi · xj ) = (Φ(xi ) · Φ(xj )) is
the kernel function. Thus, the algorithm can be established as follows:
Algorithm 1. ν−Support Vector Regression(ν−SVR)
1) Given a training set T deﬁned in (1);
2) Select a kernel function K(·, ·) and parameters ν and C;
0
0∗ T
3) Solve problem (5)∼(8) and get its solution α0 = (α01 , α0∗
1 , · · · αl , αl ) ;
0
4) Compute the threshold b by KKT conditions(ref [4]), and construct the
decision function as
m

f (x) = (w0 · x) + b0 =

0
0
(α0∗
i − αi )K(xi , x) + b
i=1

(9)

A Leave-One-Out Bound for ν−Support Vector Regression

3

671

The LOO Bound

In this section, we give the deﬁnition of this error, and then estimate its bounds.
Deﬁnition 1. Given a regression algorithm. Consider the training set (1) and
the ε−insensitive loss function (2). Let fT |t (x) be the decision function obtained
by the algorithm from the training set T |t = T \ {xt , yt }, then the Loo error of
the algorithm 1. with respect to the loss function and the training set T is deﬁned
as
l
1
RLoo (T ) =
dt ,
(10)
l t=1
where,
dt =

0,
1,

if c(xt , yt , fT |t (xt )) = 0,
otherwise.

(11)

Obviously, the Loo error is the proportion of xi (i = 1, · · · , l) out of the ε−
zone. It is similar with the Loo error in [1],[2],[3], the computation cost of the
Loo error is very expensive if l is large. In fact, for a training set including l
numbers of training points, the computing of the Loo error implies l times of
training. So ﬁnding a more easily computed approximation of the Loo error is
necessary. An interesting approach is to estimate an upper bound of the Loo
error, such that this bound can be computed through only one time of training.
Next, we derive the bounds of the above algorithms respectively. In order to
educe the Loo bound of ν − SV R, we present the deﬁnition of ν−span.
Deﬁnition 2. The ν−span for tth support vector is deﬁned as
ˆt
Sν2 (t) := min{ xt − x

2
xt
F |ˆ

∈ Λνt }

(12)

where
n

Λνt = {

n
0(∗)

λi = 1, and ∀i ∈ Mt (α0 ), αi

λi xi :
i∈Mt (α0 )

i∈Mt (α0 )

0(∗)

+ αt

λi ∈ [0,

C
]},
l

(13)
and Mt (α0 ) is the margin support vector in the solution α0 .
Now we derive an upper bound of the Loo error for Algorithm 1. Obviously,
its Loo bound is related with the training set T |t = T \ {xt , yt }, t = 1, · · · , l. We
label the solution of primal and dual problem as wt , ξt(∗) , εt , bt , αt(∗) .
According to (2), we say that an error is made in point xl when
|fT |t (xt ) − yt | ≥ εt .

(14)

Obviously, fT |t (xt ) = yt when xt is not support vector. So the error will only be
occur on support vector. Next, we induce two inequalities which will be used.
The ﬁrst inequality is
(15)
W (αt ) ≥ W (α0 − δ),

672

R. Qin et al.

where, αt is the optimal solution of (5)∼(8) based on the additional constraint
0(∗)

αt

= 0,

(16)

and
n
0
∀i ∈ Mt (α0 ) : δi∗ = −α0∗
t λi , δi = −αt λi , where

λi xi ∈ Λνt ,
i∈Mt (α0 )

∀i ∈
/ Mt (α ) ∪ {t} :
0

δi∗
δt∗

= 0, δi = 0,
0
= α0∗
t , δt = αt .

(17)

This inequality (15) follows the solution αt being the best legitimate solution for
0(∗)
which αt
= 0, since α0 − δ represents a subset of the possible solutions with
0(∗)
αt = 0.
The second inequality is
W (α0 ) ≥ W (αt + γ),

(18)

where, α0 is the optimal solution of (5)∼(8), and the value of γ satisﬁed the
follow conditions:
(∗)

γi

C
,
l
∗
∗
γt γt = 0, γt + γt = 0,
t(∗)

= 0, if αi

= 0 or

(19)

and
(γi − γi∗ ) = 0,
i∈Mt (αt )

(γi − γi∗ )yi = 0,
i∈Mt (αt )

(γi + γi∗ ) ≤ 0,
i∈Mt (αt )

C
C
∗
αti + γi ∈ [0, ], αt∗
].
i + γi ∈ [0,
l
l

(20)

It is easy to prove the existence of γ satisﬁed (19)∼(20).
Combining equations (15) and (18) yields
W (α0 ) − W (α0 − δ) ≥ W (αt + γ) − W (αt ).

(21)

To derive a bound from (21), we make use of the following lemma.
Lemma 1. If the support vector xt is left out, then Sν2 (t) is deﬁned, and
1 0∗
0
0 2 2
W (α0 ) − W (α0 − δ) = −ε|α0∗
t − αt | + (αt − αt ) Sν (t),
2

(22)

in which Sν2 (t) is ν−span.
Lemma 2. If αt misclassiﬁes (xt , yt ), it follows that
W (αt + γ) − W (αt ) ≥ Ω,

(23)

A Leave-One-Out Bound for ν−Support Vector Regression

673

where
Ω=−

C 2 Dt2
,
2l2

Dt2 := maxi∈T¯|t xt − xi .

(24)
(25)

For support vectors xt misclassiﬁed by (wt , bt )(or αt ), we combine Lemma 1
and Lemma 2 and inequation (21) to obtain
Lemma 3. If αt misclassiﬁes (xt , yt ), it follows that
1 0∗
0
0 2 2
−ε|α0∗
t − αt | + (αt − αt ) Sν (t) ≥ Ω.
2

(26)

All the proof of the listed lemma are in the section of appendix.

4

Estimates of the LOO Error

We write L(T ) as the number of errors made by the Loo procedure. Thus the
Loo error becomes
L(T )
.
(27)
eLOO =
l
The result of Lemma 3 is a suﬃcient condition. That is to say, the inequality
(26) may be occur when (xt , yt ) is not misclassiﬁed. So we have the following
theorem.
Theorem 1. If the Λνt is nonempty, then the Loo error is bounded according to
L(T )
1
1 0∗
0
0 2 2
≤ card{t : −ε|α0∗
t − αt | + (αt − αt ) Sν (t) ≥ Ω}.
l
l
2

(28)

where card{·} is the cardinality.
1
0
0∗
0 2 2
The value of 1l card{t : −ε|α0∗
t − αt | + 2 (αt − αt ) Sν (t) ≥ Ω} is the Loo
bound for ν−SVR.

5

Experiments

In this section, we will compare the Loo bound with the true Loo error. We
consider two classical data bases - ”Sinc” data set and ”Boston Housing Data”.
5.1

Results on Sinc Data Base

”Sinc” data is generated from function sincx = sinx
x with added Gaussian noise
and includes 100 instances uniformly distribute on [−10, 10].
Here we choose the Radial Basis Kernel
K(x, x ) = exp(

−||x − x ||2
),
σ2

(29)

674

R. Qin et al.

where σ is the kernel parameter, So the chose parameters in Algorithm 2.1
include C, ν, σ. and in our experiments, we choose the three parameters from
the following sets:
C ∈ P1 = {0.01, 0.1, 0.2, 0.5, 1, 2, 5, 10, 20, 50, 100, 200, 500, 1000},
ν ∈ P2 = {0.05, 0.08, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1},
σ ∈ P3 = {0.01, 0.1, 0.5, 1, 2, 3, 4, 5, 6, 10, 20, 50, 100, 200, 1000, 10000}.
We ﬁrst describe the consequences of varying the C in S1 . We used ν =
0.5, σ = 2 for our experiments. Applying these parameters in Algorithm 2.1 and
using Deﬁnition 3.1, the Loo errors are computed. On the other hand, according
to Theorem 4.1, the corresponding Loo error bounds are obtained. Both the
Loo errors and the Loo error bounds are showed in Fig.1, where ”∗” denotes
Loo error and ”◦” denotes Loo bound. To be clearly visible, the values of C are
changed into log(C).
Similarly, we chose C = 1, σ = 2 for our experiments with ν in S2 , the
compared result is showed in Fig.2. We chose C = 50, ν = 0.3 for our experiments
with σ in S3 , the compared result is showed in Fig.3. To be clearly visible, the
values of σ are changed into log(σ).
1

1
Loo error
Loo bound

0.9

1
Loo error
Loo bound

0.9

0.8

0.8

0.8

0.7

0.7

0.7

0.6

0.6

0.6

0.5

0.5

0.5

0.4

0.4

0.4

0.3

0.3

0.3

0.2

0.2

0.2

0.1

0.1

0

0

−4

−2

0

2
log(C)

Fig. 1.

5.2

4

6

Loo error
Loo bound

0.9

0.1
0

0.2

0.4

ν

0.6

Fig. 2.

0.8

1

0
−5

0

5

10

log(σ)

Fig. 3.

Results on Boston Housing Data

Boston Housing Data is a standard regression testing problem. This data set
includes 506 instances, each of which has 13 attributes and a real-valued output.
Here we randomly choose 100 instances for training, and the kernel function and
parameters are all same to the experiment on Sinc Data. We choose the three
parameters from the following sets:
C ∈ S1 = {0.01, 0.1, 1, 2, 5, 8, 10, 20, 30, 50, 100, 200, 500, 1000, 10000},
ν ∈ S2 = {0.05, 0.1, 0.15, 0.2, 0.25, 0.3, 0.4, 0.5, 0.55, 0.6, 0.7, 0.8, 0.85, 0.9},
σ ∈ S3 = {0.01, 0.1, 0.5, 1, 20, 50, 100, 200, 1000, 2000, 5000, 8000, 10000}.
We chose ν = 0.3, σ = 1 for our experiments with C in P1 , the compared
result is showed in Fig.4. The compared results of C = 1000, σ = 5, ν ∈ S2 and
C = 50, ν = 0.3, σ ∈ P3 is respectively showed in Fig.5 and Fig.6.

A Leave-One-Out Bound for ν−Support Vector Regression
1

1
Loo error
Loo bound

0.9

1
Loo error
Loo bound

0.9
0.8

0.8

0.7

0.7

0.7

0.6

0.6

0.6

0.5

0.5

0.5

0.4

0.4

0.4

0.3

0.3

0.3

0.2

0.2

0.2

0.1

0.1

0

0

−2

0

2

4

6

0.1
0

0.2

0.4

ν

log(C)

Fig. 4.

Loo error
Loo bound

0.9

0.8

−4

675

0.6

0.8

0
−5

1

0

5

10

log(σ)

Fig. 5.

Fig. 6.

From the above ﬁgures, we can see that Loo bound which we proposed is
really the upper bounds of the corresponding true Loo errors. Especially, they
almost has the same trends with the corresponding true Loo errors according to
variant parameters. So if we want to choose optimal parameters in Algorithm
2.1 for real problem, we only need to compute the proposed Loo bounds instead
of Loo errors and it will cost much less time.

References
1. Joachims,T.: Estimating the generalization performance of an SVM eﬃcientily[R].
In P.Langley, editor, Proceedings of the 17th International Conference on Machine
Learning, 431-438, San Franscisco, California, MorganKaufmann (2000)
2. Vapnik, V., Chapelle, O.: Bounds on error expectation for support vector machines.
Neural Computation[J], 12(9), (2000)
3. Tian, Y.J., Deng, N.Y.: Leave-one-out Bounds for Support Vector Regression[R].
IAWTIC. (2005)
4. Scholkopf, B., Smola, A.J.: Learning with Kernels- Support Vector Machines, Regularization, Optimization, and Beyond. The MIT Press, (2002)

Appendix
The Proof of Lemma 1
According to (17), we obtain
0
W (α0 ) − W (α0 − δ) = (α0∗
t − αt )(

λi (f (xi ) − yi ) + yt − f (xt ))

i∈Mt (α0 )

1
+ (α0∗
− α0t )2 xt −
2 t

λi xi 2 .
i∈Mt

(α0 )

0
0
For any margin SVs xi , if α0∗
i > 0, αi = 0, ξi = 0, f (xi ) − yi = ε; if αi > 0,
∗
= 0, ξi = 0, yi − f (xi ) = ε. So we yield

α0∗
i

0
(α0∗
t − αt )(
i∈Mt

0
λi (f (xi ) − yi ) + yt − f (xt )) = −ε|α0∗
t − αt |.
(α0 )

(30)

676

R. Qin et al.

Finally, substituting in the deﬁnition of the ν−span from equation (12) gives
us
1 0∗
0
0 2 2
W (α0 ) − W (α0 − δ) = −ε|α0∗
t − αt | + (αt − αt ) Sν (t).
2

(31)

The Proof of Lemma 2
From (19)∼(20), we obtain
l

(γj∗ − γj )fT |t (xj ) −

W (αt + γ) − W (αt ) = −
j=1

1
2

l

(γi∗ − γi )xi

2

. (32)

i=1

Based on (19),
l

(γi∗ − γi )xi

2

= (γt∗ − γt )2 xt −

i=1

i∈Mt (αt )

γi − γi∗
xi
γt∗ − γt

2

≤ (γt∗ − γt )2 Dt2 .(33)

where Dt2 is deﬁned in (25).
By KKT condition,
l

(γj∗ −γj )fT |t (xj ) =

γj∗ (yj +εt )+
αt∗
j >0,j=t

j=1

(−γj )(yj − εt ) + (γt∗ − γt )fT |t (xt )
αtj >0,j=t

(γj∗ −γj )yj + εt

=
j=t

(γj∗ +γj ) + (γt∗ − γt )fT |t (xt ).
j=t

(34)
According to (20),
(γj∗ − γj )yj + εt
j=t

(γj∗ + γj ) ≤ −yt (γt∗ − γt ) − εt (γt∗ + γt ).

(35)

j=t

When xl is misclassiﬁed, according to (14),
fT |t (xt ) > yt + εt ,

(36)

fT |t (xt ) < yt − εt .

(37)

When (36) or (37) occurs(i.e. this kind of error is occur), we let γt >
or γt = 0, γt∗ > 0: according to (34), (35) and (36) or (37)

0, γt∗

=0

l

(γj∗ − γj )fT |t (xj ) ≤ −2εt γt .
(∗)

(38)

j=1

Combining (32), (33) and (38),we obtain
1
W (αt + γ) − W (αt ) ≥ 2εt γt∗ − (γt∗ − γt )2 Dt2 ≥ Ω,
2
where Ω is deﬁned in (24).

(39)

