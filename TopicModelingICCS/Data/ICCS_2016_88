Procedia Computer Science
Volume 80, 2016, Pages 1612–1623
ICCS 2016. The International Conference on Computational
Science

SC-ESAP: A Parallel Application Platform for Earth
System Model ∗
Jinrong Jiang1 , Tianyi Wang1 , Xuebin Chi1 , Huiqun Hao1 , Yuzhu Wang2 , Yiran
Chen1 , and He Zhang3
1

2

Computer Network Information Center, Chinese Academy of Sciences, Beijing, China
jjr@sccas.cn chi@sccas.cn
Institute of Remote Sensing and Digital Earth, Chinese Academy of Sciences, Beijing, China
3
Institute of Atmospheric Physics, Chinese Academy of Sciences, Beijing, China

Abstract
The earth system model is one of the most complicated computer simulation software in the
human development history, which is the basis of understanding and predicting the climate
change, and an important tool to support the climate change related decisions. CAS-ESM,
Chinese Academy of Science Earth System Model, is developed by the Institute of Atmospheric
Physics(IAP) and its cooperators. This system contains the complete components of the climate system and ecological environment system including global atmospheric general circulation
model(AGCM), global oceanic general circulation model(OGCM), ice model, land model, atmospheric chemistry model, dynamic global vegetation model(DGVM), ocean biogeochemistry
model(OBM) and regional climate model(RCM), etc. Since CAS-ESM is a complex system
and is designed as a scalable and pluggable system, a parallel software platform(SC-ESSP) is
needed. SC-ESSP will be developed as an open software platform running on Chinese earth
system numerical simulation facilities for diﬀerent developers and users, which requires that the
component models need to be standard and uniﬁed, and the platform should be pluggable, high
performance and easy-to-use. To achieve this goal, based on the platform of Community Earth
System Model(CESM), a parallel software application platform named SC-ESAP is designed for
CAS-ESM, mainly including compile and run scripts, standard and uniﬁed component models,
3-D coupler component, coupler interface creator and some parallel and optimization work. A
component framework SC-ESMF will be developed based on the framework SC-Tangram for
the more distant future.
Keywords: parallel computing; earth system model; software platform; CAS-ESM

∗ This project is supported by the Key Research Program of CAS(No.KJZD-EW-TZ-G09-04) and the National
High Technology Research and Development Program of China(No.2014AA01A302)

1612

Selection and peer-review under responsibility of the Scientiﬁc Programme Committee of ICCS 2016
c The Authors. Published by Elsevier B.V.

doi:10.1016/j.procs.2016.05.493

SC-ESAP: A Parallel Application Platform for Earth System Model

1

Jiang, Wang, Chi and et al.

Introduction

In the past decades, climate simulation has achieved great success. It has been able to provide
eﬀective climate prediction and forecasting information. However, as the climate change and its
impact intensiﬁes, people and the decision makers from all over the world are demanding more
accurate global and local forecasting information. Therefore, countries, including the United
States, the European Union and Japan, are formulating strategies to boost the development
of Earth System Model and its dedicated high performance computer system. Alliance and
uniﬁcation are encouraged to meet the need of the country and to stay competitive around the
world. Edwards [9] deeply discussed the concept of fundamental infrastructure and software
being the computing platform. From a decade ago, scientists have realized the beneﬁt of
sharing the developing software platform among diﬀerent research groups of Climate Model,
for instance, the Community CESM [5, 10, 3], previously named as CCSM, and its Coupler
(CPL) [13, 2] developed by NCAR, the Flexible Modeling System (FMS) from Geophysical
Fluid Dynamics Laboratory (GFDL), OASIS [11] from Europe and the Earth System Modeling
Framework (ESMF) [17] introduced the concept of ultra-architecture, in which a componentbased way of composition is used [1, 8].
China has been actively developing Earth System Model [15]. In 2013, we(IAP, ICT, CNIC,
SUGON) proposed the project named the earth system numerical simulation facilities into
the national long-term guideline of national major scientiﬁc and technological infrastructure
construction projects (2012-2030). Within this project, not only the dedicated high performance
computers will be established but also the Chinese Academy of Science Earth System Model
(CAS-ESM) as well as the supporting software platform(SC-ESSP) will be developed. The
CAS-ESM contains whole climate system and ecological environment components. It integrates
Atmospheric Model, Ocean Model, Seaice Model, Land Model, Atmospheric Chemistry and
Aerosol Model, DGVM, OBM and WRF [4]. In addition, Space Weather Model(SWM) will
be added to the system in the future. Also, the coupling between diﬀerent pairs of component
models will be realized. Therefore, the simulation research of Atmosphere, Current, Land
Surface Procedures and Ecology will be more accurate. In order to promote the alliance,
uniﬁcation and cooperation of distributed research of Earth System Model and the sharing of
simulating infrastructure in China, the supporting software platform need to provide convenient
coupling, matching and compiling processes. A fast and convenient plan to tackle the problem
is to establish SC-ESSP based on CESM, which is named as SC-ESAP. Since the component
models are demanded to be plugged in and out dynamically, the total number of component
models will be larger than it is in CESM. Also, the number is not the same in diﬀerent tests.
Thus, using the original CESM as the framework will be insuﬃcient for the following reasons.
First, CESM does not support component models such as WRF. Since there is no interfaces
for them, new interfaces should be developed for them. Second, for a particular component
model, interface may be inappropriate if the coupling variables change. Also, for a particular
component model, the diﬀerent combination of models may require diﬀerent coupling variables.
In addition, CESM does not support 3-D coupling. The code of the component models are
diﬀerent for diﬀerent combination, which causes that there are several editions of models and
thus it limits their development and feasibility of sharing. Besides, it is complicated to use CPL7
and MCT, because interfaces need to be altered manually to adjust the component model. It
is a tedious work and only experts are able to complete it. At last, CESM does not support
dynamically plugging, which is unable to fulﬁll various users’ demand. Therefore, we design a 3D coupling component to support 3-D coupling, an auto-creator for coupling interfaces to create
diﬀerent interfaces, and to standardize and unify the component models. Correspondingly, the
1613

SC-ESAP: A Parallel Application Platform for Earth System Model

Jiang, Wang, Chi and et al.

compilation script and ﬁle system will be expanded and improved. Moreover, we are keen on
developing components like Earth System modeling framework based on SC TANGRAM [6], a
general purpose framework.
The rest of this paper is organized as follows. Section 2 gives a brief introduction of CASESM and SC-ESSP. Section 3 to Section 6 introduces our main work about SC-ESAP. In
Section 7, some discussions are given and some results of simulation of CAS-ESM are illustrated.

2

Model and Platform

The CAS-ESMv1.0 is designed and developed based on the CESM version 1.0. The CAS-ESM
[12] is composed of seven separate component models and one central coupler component. It can
be employed to conduct fundamental research for the Earth’s climate states. In the CAS-ESM
system, in addition to the original component models of CESM, some models developed by
Chinese researchers are added, for example, IAP AGCM4.0 [16] is an atmospheric model developed by the IAP, LASG/IAP Climate System Ocean Model (LICOM) [7] Version 2.0 is an ocean
model developed by the State Key Laboratory of Numerical Modeling for Atmospheric Sciences
and Geophysical Fluid Dynamics (LASG) of the IAP, the Common Land Model (CoLM) is a
land component model developed by Beijing Normal University, the DGVM [18] is a model
developed by IAP, the Global Environmental Atmospheric Transport Model (GEATM) is an
atmospheric chemical component model developed by the IAP, and the Ocean Biogeochemistry
General Circulation Model(OBGCM) is ocean biogeochemistry model developed by IAP. The
Advanced Research WRF version 3.2 is put into the CAS-ESM modeling system. In the future,
more component models will be added to the CAS-ESM platform. The model structure of
the CAS-ESM system is presented in Figure 1. The models in dotted ovals are optional. The
CAM/
IAP-AGCM
/datm...

IAPDGVM

CoLM/
CLM/
dlnd...
WRF/
GRAPES...

CICE/
dice...

LICOM/
POP/
d
docn...

SC-ESSP
3-D Coupler

CPL7

GEATM
...

OBGCM

GLC
...

Figure 1: Model Structure of the CAS-ESM
SC-ESSP is the supporting software platform for the earth system model CAS-ESM, including
SC-ESAP and SC-ESMF. SC-ESAP is a simple platform based on CESM for short-term demands of CAS-ESM, and SC-ESMF is a componentized framework based on run-time system
for long-term goals. The major work of SC-EASP will be discussed in this paper. Some alreadyequipped functions of CESM are employed in CAS-ESM, such as MCT and cpl7, parallel I/O,
MPI Communication Library, Compilation Scripts, etc.. Moreover, a 3-D coupling method for
some component models such as WRF and GEATM is introduced in the system. Also a coupler
interface creator is developed for easily plugging new models to the CAS-ESM and realizing
diﬀerent combinations of the models on one uniﬁed platform. The structure of SC-ESSP is
shown in Figure 2.
1614

SC-ESAP: A Parallel Application Platform for Earth System Model

Jiang, Wang, Chi and et al.

SC-ESSP

SC-PESMF

MPI
communication

SC-ESAP

Parallel I/O

Runtime
Environment

Compiling and
Runing Scripts

Bottom layer data
and tool component

Model
Component

CESM

CPL7

Numerical
Algorithm lib
Coupling
Component
User Interface
Component

Improved

CPL
Standard
and Unified
component
models

Improved

Coupling
Interface Creator
3D-Coupler

ATM,
OCN,
...

Figure 2: The Structure of SC-ESSP

3

Compilation Scripts

Compilation scripts ﬁle system is a complex and important part in CAS-ESM. It provides the
integration of a variety of component models and the functions of the user interface. At the
same time, compilation scripts ﬁle system also has the function of uniﬁed version and model
extension. This scripts ﬁle system references the structure of CESM similar system and adds
some new component models and functions.

3.1

Structure of Compilation Scripts File System

Scripts are written with Perl and Shell. As is shown in Figure 3, there are two main modules which are shared-compilation-scripts module and component-compilation-scripts module.
Shared-compilation-scripts module is used to control the compilation process and copy other
scripts and conﬁguration ﬁles. Users can create new cases, conﬁgure and build their cases
by calling these scripts. Besides, component compilation scripts are independent compilation
scripts of each component model. For CAS-ESM developers, they need to write these scripts to
copy namelists and save conﬁguration information of each component model. The XML format
ﬁle which is called and analyzed by scripts is used in the system to set related variables. Users
can change the conﬁguration of their case just by modifying these XML ﬁles.

3.2

Addition of A New Component Model

CAS-ESM is an extensible platform for the earth system model. Users can add a new component
model to CAS-ESM by the following steps.
Firstly, shared-compilation-scripts module should be modiﬁed to support the new component model. On one hand, users need to add some new component sets to conﬁg compsets.xml
and some new grids to conﬁg grid.xml. This new component model may be used by these
component sets and grids. Other environment variables which are used by this new component
model can be added in conﬁg deﬁnition.xml. As ConﬁgCase.pm is used to read and parse these
xml ﬁles, it should also be modiﬁed to add some group variables. On the other hand, users
1615

SC-ESAP: A Parallel Application Platform for Earth System Model
config_compset.xml
config_grid.xml
config_machines.xml
config_definition.xml
ConfigCase.pm
g
p
env_case.xml
env_mach_pes.xml
env_build.xml
env_conf.xml
env_run.xml
_
SourceMods

create_newcase

case directories

configure

$case.$mach.build

Jiang, Wang, Chi and et al.

LockedFiles
Buildconf
...

Tools
**.buildexe.csh
build
b
ild
ldexe
*.buildnml.csh
*conf

$component.cpl7.template
Component bld
Definition files(XML)

Figure 3: Structure of Shared-compilation-scripts Module and Component-compilation-scripts
Module
should modify script create newcase to copy the new component compilation scripts and other
new component xml ﬁles. If this new component model needs to change some environment
variables before conﬁguration, try to modify conﬁgure script ﬁle.
Secondly, component-compilation-scripts module should be written by users. $component.cpl7.template is the most important component compilation script ﬁle which is used for
creating namelist script, creating build script, reading and parsing component xml ﬁle. Deﬁnition ﬁles are some XML ﬁles which deﬁne some variables like processor division, supported
blocks division and so on.
Of course, the model added to the CAS-ESM should be ﬁrstly uniﬁed for diﬀerent combinations. There are several kinds of component models which are atmosphere, land, ocean, sea
ice, land ice and regional model, etc. After our work, all these parts can be freely combined
with active models and optional or data models.

4

3-D Coupling Component

The current CAS-ESM system employs CPL7 to complete the coupling processes between different component models. Model Coupling Toolkits (MCT) are used to provide functions for
transferring data between diﬀerent models through coupler. With proper interfaces, component
models can be easily plugged in to the whole system. However, in the current implementation
of CAS-ESM, some coupling procedures, such as the coupling between AGCM and WRF, involve 3-D data coupling. In the cases of 3-D couplings, CPL7 only plays as a role of data
communicating tool, pushing the coupling data from one component to the other without the
necessary processing of it. The processing procedures of data are integrated in the component
models. For instance, in the case of WRF, there is a pre-processing module that handle the
received data to adjust it into the form that WRF needs. Therefore, the current mechanism of
3-D coupling is ineﬃcient and we are working on the optimization.
The current implementation of CAS-ESM that uses CPL7 to complete 3-D couplings needs to
1616

SC-ESAP: A Parallel Application Platform for Earth System Model

Jiang, Wang, Chi and et al.

be altered and the 3-D couplings should not be using CPL7 any longer. CPL7 is not appropriate
for the following reasons. Here, the coupling of AGCM-WRF is taken as an example. First, there
are unnecessary steps caused by using CPL7 for 3-D coupling in the current implementation.
The data processing steps in CPL7 are redundant since WRF has its own pre-processing step
to handle the raw data received from coupling. In addition, because of the huge size of 3-D
data, the converting and processing procedures could involve enormous calculation. Moreover,
the model of WRF itself contains mass integral computation. Thus, it is unwise to put two
modules with mass computation together in a particular component. The more concentrated
the load is, it is more likely to cause load imbalance which leads to ineﬃciency.
The plan of optimization is to introduce a new dedicated coupler component for 3-D couplings. The 3-D coupler will be based on MCT and it will be serving as a component of the
earth system. With the 3-D coupler set up, the pre-processing procedures of WRF will be
moved into the 3-D coupler’s module. Therefore, the workload of WRF component will be
spread to the 3-D coupler. In addition, in this way, the 3-D coupler and component model are
able to run concurrently in a pipeline to achieve higher eﬃciency.
Figure 4 illustrates the coupling ﬂow of 3-D coupling using the newly introduced 3-D coupler.
As is shown, the pre-process steps (METGRID and REAL in the ﬁgure) of WRF is completed
by the 3-D coupler. Thus, the mass computational work of mapping and rearranging of 3-D
data is separated from the integration, which also contains heavy computation work. Since
the 3-D coupler and WRF employ distinct computational resource, ideally, the pre-process and
integration can be run concurrently in a pipeline. In this way, higher eﬃciency can be achieved
because of higher level of parallelism and less load imbalance. In current implementation
atm_cpl_dt
Atmosphere
state variables

Surface and Soil

IAP-AGCM
CPL7

CoLM/LICOM
/CICE

3D-Coupler
pler
wrf_cpl_dt

metgrid

Coupling Flow

concurrently
in a pipeline
REAL

integration

WRF

Figure 4: The Flow of 3-D Coupling for WRF
of CAS-ESM, the direct 3-D coupling of AGCM-WRF has been realized when AGCM and
WRF are of the same processor decomposition. According to test results, the time of the
communication step between AGCM and WRF has been reduced by almost 50%. The coupling
of AGCM-WRF with diﬀerent processor decomposition is under development. In this plan,
a new component, serving as the exclusive 3-D coupler between AGCM and WRF. Also, the
processors that AGCM and WRF use are none overlapping. The communication are realized
by calling communicating functions among the combined communication group of AGCM and
3-D coupler as well as the one of WRF and 3-D coupler. Thus, theoretically, AGCM and WRF
can run concurrently.
1617

SC-ESAP: A Parallel Application Platform for Earth System Model

5

Jiang, Wang, Chi and et al.

CAS-ESM Coupler Interface Auto-creator

Coupler is an important part of the Earth System Model for model development. It connects
several separate interoperable component models in order to simultaneously simulate the variations of components models and interactions among the atmosphere, land surface, oceans, sea
ice and other components of the climate system. However, CESM does not include WRF and
other component models like DGVM. CAS-ESM is composed by component models which are
inserted into CESM with the CPL coupler interface. However, when component model experts
add component models into CAS-ESM, they not only need to read coupler interface but also
to understand the complex internal coupler of the system, the repeated work of which can be
replaced by automation.
Three parts need to be automatically generated. Firstly, the ﬁle under the name of
ccsm comp mct.F90 is the top level of coupler. It includes many component models attributes
such as MCT global segment maps and control ﬂags, and some functions which would call middle level functions. Secondly, the middle level consists of functions such as mapping and merge.
The number of mapping and merge functions depends on the number of component models.
Diﬀerent from top level coupler, component models from mapping function needs to interact
with other models; as that from merge function, it needs to gather some models. Thirdly, the
active module coupler of any component models needs to be automatically generated although
models are in endless variety.
In order to generate codes automatically, three methods have been proposed. Firstly, reusing
the duplicate codes so as to get uniﬁcation codes into template ﬁles, and replace diﬀerent
attributes with labels like“{c}”. An annotation line under label “{list}” in front of the special
labels exists to prompt users. Secondly, User conﬁguration ﬁles are provided to component
model experts to ﬁll up the corresponding contents of variables including the kind of arrays
which covers component model attributes. One-dimensional array can be used to replace top
level label from template ﬁles and loop traversal. Mapping and merge functions would require
multi-dimensional array. Both one-way and two-way nesting exist in mapping, so the array
includes one component model, another component model and their nesting directions. One
component merges with one or more others, so the lengths of each array element will not be
the same. Dictionary which consists of mapmapping instead of array beneﬁts merge variable
expressions. Thirdly, the contents being replaced by arrays provided by user conﬁguration
ﬁles from the special labels in the template ﬁles can be inserted into ﬁnal codes, to complete
automated generation. Python programming language with regular library can be used to
complete replacement. Moreover, XML inserting modules are provided for experts to insert
special codes which cannot get from template ﬁles.
In addition, in this way, the codes can be automatically generated when component model is
accessed to CAS-ESM coupler. This would then beneﬁt component model experts in a way that
they need only to ﬁll user conﬁguration ﬁles rather than to understand the complex internal
coupler of system.

6

Model Optimization Work

The timeliness and accuracy of the models are equally important in the practical simulation.
The integrated CAS-ESM has higher computational complexity, so some work are done to
improve the parallel eﬃciency. The followings are some of them.
1618

SC-ESAP: A Parallel Application Platform for Earth System Model

Jiang, Wang, Chi and et al.

Template Files
Top Level:
Controller Interface

ccsm_comp_mct.F90

Middle Level:
Function Interface

map_{c1}{c2}.F90
mrg_{c1}{c2}.F90
seq_*.F90

Model Level:
Component Model
Interface

{ccc}_comp_mct.F90

User
configuration
files

Figure 5: Flow Chart of CAS-ESM Coupler Auto Created

6.1

2-D domain decomposition algorithm for IAP AGCM4.0

The original IAP AGCM4.0 uses one-dimensional domain decomposition method, which prevents it from running on more than dozens of CPU cores. The computation of the physical
process in the IAP AGCM4.0 needs to use related data on the grid points in the vertical direction k and has to be done in sequence. Therefore, the computation task of the physical process
can be decomposed in the horizontal direction. The computation of the dynamic core on each
grid point needs to use not only related data on the grid points in the vertical direction k and
direction j of longitudinal circle but also related data on the grid points in the direction i of
latitudinal circle. Meanwhile, a part of the computation in the direction i or k needs to be
done in sequence. To solve this, the hybrid 2-D decomposition method is used. When the computation in the direction i needs to be done in sequence, the global domain is decomposed by
latitude and vertical level, as is shown in Figure 6 left. When the computation in the direction
k needs to be done in sequence, the decomposition way of the task is shown in Figure 6 right.
Obviously, it is necessary to convert the data from a kind of decomposition way to another one
during the whole computing of the dynamical core.
k

k

j

i

i

Figure 6: Hybrid Two-dimension Domain Decomposition
After the two-dimension domain decomposition for the IAP AGCM4.0, it can scale reasonably to more than 1000 CPU cores. To evaluate parallel performance of the IAP AGCM4.0,
an ideal climate simulation experiment for 61 model days is designed to run on Sugon cluster.
In this case, the computing time of the IAP AGCM4.0 is shown in Table 1. The result indicates that the parallelization of the IAP AGCM4.0 with MPI reduces the computing time of
2787.82 seconds on 32 cores to 739.07 seconds on 256 cores, for a speedup of about 3.8. But
the computing time on 512 cores begins to increase.

6.2

Solving load imbalance of METGRID

During assessing the feasibility and capability of the CAS-ESM in simulation, it is found that the
METGRID module of the WRF in the CAS-ESM exists the load imbalance. It is very crucial
1619

SC-ESAP: A Parallel Application Platform for Earth System Model

AGCM(Py ∗ Pz )
32 ∗ 1
32 ∗ 2
32 ∗ 4
32 ∗ 8
32 ∗ 16
42 ∗ 26

Nodes(Cores)
2(32)
4(64)
8(128)
16(256)
32(512)
69(1092)

Jiang, Wang, Chi and et al.

Computing time
2787.82
1577.49
1038.58
739.07
775.03
1519.98

Table 1: Computing time (s) of the IAP AGCM4.0 with default decomposition

for improving real-time computation performance of the CAS-ESM to solve the METGRID
load imbalance and improve its processing speed for massive data. The idea of the optimizing
algorithm is that each process stores the traversal record of executing the ﬁrst for-loop code
for all the vertexes before the search extrap (a function of the METGRID module) is called for
the ﬁrst time. When the search extrap is called later, each process reads the traversal record
instead of executing the ﬁrst loop again [14]. In this way, the total amount of the ﬁrst loop
in the search extrap can be decreased eﬀectively. Then, the problem of the load imbalance is
solved eﬀectively.
In the 5 days of simulation for the CAS-ESM, 64 CPU cores are used to compute ﬁrstly.
Figure 7 illustrates the METGRID running time of all the 64 MPI ranks before and after the
optimizing. The computing speed of the METGRID after the optimizing is about 10 times
faster than before and the total computing time of the CAS-ESM with WRF decreases to 1/3.

METGRID running time of each MPI rank
9000
before optimizing
after optimizing

8000
7000

Running time(s)

6000
5000
4000
3000
2000
1000
0

0

10

20

30
40
MPI rank

50

60

70

Figure 7: Comparison of the Run Time for each MPI Rank Before and After Optimization

6.3

An improved grid decomposition algorithm for LICOM

The Ocean model LICOM is a very important component model in CAS-ESM. Figure 8 shows
the original and the improved grid decomposition methods of LICOM. Here, N is grid number,
np is MPI process number, and n equals to the greatest integer no more than N/np. In the
original method, N −(n+1)∗(np−1) should be bigger than zero, so the MPI process number np
is limited. The improved method let the grids divide equally to the MPI processes and spread
the rest grids evenly to several processes, which can expand the process number limitation.
1620

SC-ESAP: A Parallel Application Platform for Earth System Model
Pes
np-1 N-(n+1)x(np-1)
n+1
np-2
...
.
n+1
1
n+1
0

n
n
...
n+1
n+1

Jiang, Wang, Chi and et al.

Pes
np-1
np-2
.
1
0

Figure 8: Comparison of Original and Current Grid Decomposition Method of LICOM

7

Result and Discussion

7.1

Long-term Climate Simulation

This experiment is setting up with pre-industrial (1800 AD) climate forceings. CAS-ESM is
spun up for 50 years. To ensure the continuity of the climate system, we select the data
from years 11-50 for analysis. Annual mean 2-meter air temperature and sea surface temperature(SST) are displayed in Figure 9 (a) and (c). It is shown that the simulation result is
consistent with the Japanese 25-year reanalysis (JRA-25) dataset and Hadley Centre Sea Ice
and Sea Surface Temperature data set(HadlSST).

DDQQXDOPHDQPHWHUDLU
WHPSHUDWXUHE\&$6(60

EDQQXDOPHDQPHWHUDLU
WHPSHUDWXUHRI-5$

FDQQXDOPHDQ667E\&$6(60

GDQQXDOPHDQ667RI+DGO667

Figure 9: Comparision of The Model Simulation Result and Observation

7.2

Torrential Rainfall Simulation

To evaluate the coupling performance of the CAS-ESM with WRF, the paper uses the CAS-ESM
to simulate the extreme precipitation event over Beijing on 21 July 2012 (00:00 UTC 21 July
2012 to 00:00 UTC 22 July 2012). The 1◦ ∗ 1◦ resolution National Centers for Environmental
Prediction Final analysis (NCEP-FNL) data at 12:00 UTC 20 July 2012 is used to provide the
1621

SC-ESAP: A Parallel Application Platform for Earth System Model

Jiang, Wang, Chi and et al.

initial conditions. Figure 10 shows the daily-accumulated rainfall in the simulation, which is
close to the observation. It means that the CAS-ESM can be used for the regional weather
forecast.
Then, the simulating results for the CAS-ESM with the default decomposition strategy on
64, 128, 256, 512 and 1024 CPU cores respectively are analyzed to test the scalability of the
system. Comparing with the 64 CPU cores, the parallel eﬃciency of the CAS-ESM on 1024 CPU
cores can reach about 70% when the grid size of WRF is 1024 ∗ 1024 ∗ 30, as shown in Figure 11.
On the whole, the CAS-ESM has desirable parallel performance and strong scalability.

Figure 10: Daily-accumulated Rainfall in the Simulation

Parallel efficiency of the CAS-ESM

Speedup of the CAS-ESM

110

16
256*256*30
512*512*30
1024*1024*30

100
90

256*256*30
512*512*30
1024*1024*30
ideal

14
12
10

70
Speedup

Parallel efficienc(%)

80

60
50

8
6

40
4
30
2

20
10

0

200

400

600
Number of cores

800

1000

1200

0

0

200

400

600
Number of cores

800

1000

1200

Figure 11: Parallel eﬃciency and Speedup of the CAS-ESM with diﬀerent grid scales.

7.3

Discussion and conclusion

We have established a software platform SC-ESAP for the earth system model CAS-ESM based
on CESM for the purpose of multi-disciplinary crossing research and collaboration development.
In SC-ESAP, a compilation script referencing the CESM system is designed for the CAS-ESM,
the CPL7 interface is used and modiﬁed as 2-D coupler and a new 3-D coupler is developed
based on MCT, a coupler interface auto-creator is ﬁnished to decrease the operational diﬃculty
of adding a new component models into CAS-ESM. Finanlly, CAS-ESM is constructed with SCESAP and some parallel optimization algorithms are developed to improve its computational
eﬃciency. However, there are a lot of places need to be improved such as standardization
ﬂow of 3-D coupler and coupler interface auto-creator, synergy development among compiler
scripts, coupler and coupler creator, more experiments to verify its validity and robustness,
etc.. There is some advanced software platform technology can be introduced for CAS-ESM.
1622

SC-ESAP: A Parallel Application Platform for Earth System Model

Jiang, Wang, Chi and et al.

We have started the work to build the component like Earth System modeling framework
based on SC TANGRAM. The current component models need to be reconstructed and it is a
challenging and long-term work.

References
[1] Knobel C. Understanding infrastructure: Dynamics, tensions, and design. 2007.
[2] Anthony P. Craig, Mariana Vertenstein, and Robert Jacob. A new ﬂexible coupler for earth system
modeling developed for ccsm4 and cesm1. International Journal of High Performance Computing
Applications, 26(1):31–42, 2011.
[3] Kluzek E. Cesm research tools: Clm4 in cesm1. 0.4 user’s guide documentation. 2011.
[4] Juanxiong He, Minghua Zhang, Wuyin Lin, Colle Brian, Ping Liu, and Andrew M. Vogelmann.
The wrf nested within the cesm: Simulations of a midlatitude cyclone over the southern great
plains. Journal of Advances in Modeling Earth Systems, 5(3):611–622, 2013.
[5] Marika Holland and David A Bailey. 2012 community earth system model (cesm) tutorial - proposal
to doe. In Name of conference: 2012 CESM Tutorial Location of conference: National Center for
Atmospheric Research, Boulder, CO Date of conference: July 30 - August 3, 2012, 2013.
[6] Laxmikant V. Kale and Sanjeev Krishnan. Charm++ : A portable concurrent object oriented
system based on c++*. In Conference on Object Oriented Programming Systems, pages 91–108,
2010.
[7] Pengfei Lin, Hailong Liu, Wei Xue, Huimin Li, Jinrong Jiang, Mirong Song, Yi Song, Fuchang
Wang, and Minghua Zhang. A coupled experiment with licom2 as the ocean component of cesm1.
Journal of Meteorological Research, 30(1):76–92, 2016.
[8] Gerald A. Meehl, Moss Richard, Karl E. Taylor, Eyring Veronika, Ronald J. Stouﬀer, Bony Srine,
and Stevens Bjorn. Climate model intercomparisons: Preparing for the next phase. Eos Transactions American Geophysical Union, 95(9):77–78, 2014.
[9] Edwards P N. A vast machine: Computer models, climate data, and the politics of global warming.
Mit Press, 2010.
[10] Konor C Randall D, Heikes R. A ﬂexible atmospheric modeling framework for the cesm. Technical
report, 2014.
[11] R. Redler, S. Valcke, and H. Ritzdorf. Oasis4-a coupling software for next generation earth system
modelling. Geoscientiﬁc Model Development & Discussions, 3(1):87–104, 2010.
[12] Hongchuan Sun and Guangqing Zhou. Assessments of the climate system model (cas-esm-c) using
iap agcm4 as its atmospheric component. Chinese Journal of Atmospheric Sciences, 36(2):215–233,
2012.
[13] Craig T. Cpl7 user’s guide. 2011.
[14] Yuzhu Wang, Jinrong Jiang, Huang Ye, and Juanxiong He. A distributed load balancing algorithm
for climate big data processing over a multi-core cpu cluster. Concurrency and Computation:
Practice and Experience, pages n/a–n/a, 2016. CPE-15-0370.R2.
[15] Pu Yifen Wang Huijun, Zhu Jiang. The earth system simulation. SCIENTIA SINICA Physica,
Mechanica & Astronomica, 44(10)(1116-1126), 2014.
[16] He Zhang and Zhaohui Lin. The computational scheme and the test for dynamical framework of
iap agcm-4. Chinese Journal of Atmospheric Sciences, 33(6):1267–1285, 2009.
[17] S Zhou, A da Silva, B Womack, and et al. Prototyping of the esmf using doe’s cca. NASA Earth
Science Technology Conference, 2003(24-26), 2003.
[18] Jia Wen Zhu, Xiao Dong Zeng, L. I. Fang, and Xiang Song. Preliminary assessment of the common
land model coupled with the iap dynamic global vegetation model. Atmospheric & Oceanic Science
Letters, 7(6):505–509, 2014.

1623

