Procedia Computer Science
Volume 29, 2014, Pages 565–575
ICCS 2014. 14th International Conference on Computational Science

Experiments on a Parallel Nonlinear Jacobi–Davidson
Algorithm
Yoichi Matsuo1∗, Hua Guo2 , and Peter Arbenz3
1

Keio University, School of Fundamental Science and Technology, Graduate School of Science and
Technology, 3-14-1 Hiyoshi, Kohoku, Yokohama, Kanagawa, 223-8522, Japan
matsuo@math.keio.ac.jp
2
Systransis AG, Riedstrasse 1, 6343 Risch, Switzerland
3
ETH Zurich, Computer Science Department, Universit¨
atstr. 6, 8092 Zurich, Switzerland
arbenz@inf.ethz.ch

Abstract
The Jacobi–Davidson (JD) algorithm is very well suited for the computation of a few eigenpairs of large sparse complex symmetric nonlinear eigenvalue problems. The performance of
JD crucially depends on the treatment of the so-called correction equation, in particular the
preconditioner, and the initial vector. Depending on the choice of the spectral shift and the
accuracy of the solution, the convergence of JD can vary from linear to cubic. We investigate
parallel preconditioners for the Krylov space method used to solve the correction equation.
We apply our nonlinear Jacobi–Davidson (NLJD) method to quadratic eigenvalue problems
that originate from the time-harmonic Maxwell equation for the modeling and simulation of
resonating electromagnetic structures.
Keywords:

1

Introduction

In this paper, we consider solving constrained complex-valued nonlinear eigenvalue problems,
T (λ)x = Ax + λRx − λ2 M x = 0,

C T x = 0,

(1)

where A, R and M are large sparse complex symmetric matrices. Such eigenvalue problems
occur in the analysis of resonant cavities that includes loss mechanisms in the model, of dielectric
resonator antennas (DRA), or of plasmonic nanostructures, considering the dispersive dielectric
properties of metals in the optical region of the electromagnetic spectrum [5, 9, 10, 22]. These
models can be described by the time-harmonic Maxwell equations in 3-dimensional space [10],
2
∇ × μ−1
r ∇ × E (x) + iλσZ0 E (x) − λ εr E (x) = 0,
∇ × (εr E (x)) = 0, x ∈ Ω,
∗ Most

(2)

of the work was done during this author’s visit to the Computer Science Department at ETH Zurich

Selection and peer-review under responsibility of the Scientiﬁc Programme Committee of ICCS 2014
c The Authors. Published by Elsevier B.V.
doi:10.1016/j.procs.2014.05.051

565

Parallel Nonlinear Jacobi–Davidson Algorithm

Y. Matsuo, H. Guo, and P. Arbenz

where Ω ⊂ R3 is a bounded domain, λ is the complex wavenumber in free space, εr and μr
are magnetic permittivity and electric permeability, respectively. Then, a 3-dimensional ﬁnite
element discretization of equation (2) by N´ed´elec tetrahedral elements yields complex-valued
large sparse nonlinear eigenvalue problems of the form (1), cf. [11].
The Jacobi–Davidson (JD) algorithm suggested by Sleijpen and van der Vorst [16] is very
well suited for the computation of a few eigenpairs of such problems. There are numerous
variants available for linear and nonlinear eigenvalue problems [1–4, 7, 13, 20, 21]. In [2, 3], JD
algorithms for large scale generalized real- and complex-valued symmetric eigenvalue problems
Ax = λBx have been considered.
The JD algorithm is related to the Newton as well as to the Rayleigh quotient iteration [13,
17]. Since the correction equation is in general solved only approximately these methods are
‘inexact’. However, they are ‘accelerated’ by employing a search space instead of a single vector
iterate. The treatment of the so-called correction equation is crucial for the success of JD,
in particular, the shift strategy. Mostly, the correction equation is solved by a Krylov space
method. Then, the choice of the preconditioner is important.
Betcke and Voss [4, 20] proposed a JD algorithm for solving nonlinear eigenproblems. We
closely follow their algorithm but try to improve its behavior for our set of problems investigating
mainly shifting strategies and preconditioners. In section 2, the NLJD algorithm is presented.
In section 3, we discuss how the choice of the shift in the correction equation and the initial
vector aﬀect the speed of convergence. We show examples that explain the properties of the
NLJD algorithm. In section 4 we present some realistic electromagnetics simulations. We draw
our conclusions in section 5.

2

The nonlinear Jacobi–Davidson algorithm

The Jacobi–Davidson algorithm was suggested by Sleijpen and van der Vorst [16] for the computation of a few eigenvalues close to a given target τ . Shortly after this seminal paper, variants
of JD for all kinds of linear and polynomial eigenvalue problems were derived [7, 21]. In this
section, we discuss the NLJD algorithm suggested by Voss et al. [4, 20] for solving nonlinear
eigenproblems.
H
Assume that (ϑk , y k ) is the eigenpair of the projected eigenvalue problem Vk−1
T (λ)Vk−1 y =
0 closest to the target τ . Then, (ϑk , uk ) with uk = Vk−1 y k is a Ritz pair for the eigenvalue
problem (1). The subspace Vk−1 is expanded by the solution of the so-called correction equation
for the NLJD algorithm,
T˜ (σk ) t =

I−

pk uH
k
uH
k pk

T (σk ) I −

uk uH
k
uH
k uk

t = −r k ,

t ⊥ uk .

(3)

The solution t is orthogonalized against Vk−1 and, after normalization, appended to Vk−1 such
that Vk = [Vk−1 , t]. This procedure is repeated until the residual T (ϑk )uk 2 is small enough.
The NLJD algorithm has mostly been applied to nonlinear eigenproblems of the form (1).
We developed our own parallel NLJD solver in Femaxx with only a few modiﬁcations from the
algorithms of Voss [20]. Details of our NLJD method are given in Algorithm 1. The diﬀerence
between this solver and Voss’s [4, 20] are the projection steps 3 and 18, where we enforce the
vectors to satisfy the divergence-free condition C T x = 0. The actual projection is
(I − Y H −1 C T ) t = t − Y z,
T

Hz = C T t,

(4)

where Hz = C t is solved to high accuracy by Jacobi preconditioned Bi-CGstab. In step 7,
the projected eigenvalue problem is solved by the method of successive linear problems [15] or
566

Parallel Nonlinear Jacobi–Davidson Algorithm

Y. Matsuo, H. Guo, and P. Arbenz

Algorithm 1: The nonlinear Jacobi–Davidson algorithm
Data: Matrices T, Y, H, and C, target τ , initial vector v0 , tolerance ε
Result: An eigenpair (λ, x) closest to the target τ
1 begin
2
Determine a preconditioner M ≈ T (τ );
3
Project v 0 = I − Y H −1 C T v 0 ;
4
Set initial search space V0 =[v 0 ];
5
while not converged do
6
k = k + 1;
7
compute a eigenpair (ϑk , xk ) of the projected eigenproblem V H T (ϑk−1 ) V y = 0,
closest to the target τ .;
8
Ritz vector uk = V xk ;
9
Residual r k = T (ϑ) uk ;
10
if r k < ε then
11
accept the approximate eigenpair (ϑk , xk );
12
end
13
pk = T (ϑk ) u;
14
Reduce the search space Vk if necessary;
15
Choose σk = τ or ϑk ;
16
Solve the correction equation to ﬁnd the vector expanding the search space by a
Krylov space method with preconditioner M ;
17
18
19
20
21
22

I−

p k uH
k
uH
k pk

uk uH
k
uH
k uk
−1 T

T (σk ) I −

tk = −r k ;

Project tk = I − Y H C tk ;
Orthogonalize tk against the current search space Vk ;
Expand the search space Vk+1 = [Vk , tk ];
end
end

the QZ algorithm after linearization. In steps 16 and 17, the correction equation is solved by
˜ . With M ≈ T (σk ) ≈ T (τ ) we set
the Bi-CGstab method with preconditioner M
˜ :=
M

I−

p k uH
k
p
uH
k k

M

I−

uk uH
k
u
uH
k k

.

(5)

˜ are maps from span{uk }⊥ to span{pk }⊥ . Therefore it makes
Notice that both T˜(σk ) and M
˜
sense to precondition (3) by M ,
˜ + T˜ (σk ) t = −M
˜ + r,
M

tk ⊥ uk .

(6)

We start the Bi-CGstab iteration for solving (5) with the initial guess 0 which trivially satisﬁes
H
˜ + T˜w. Since I − ukHuk w = 0, we get
the constraint. In each step, we compute z = M
uk uk

w
˜ = T˜w =

I−

p k uH
k
uH
k pk

T (σk )w = T (σk )w −

uH
k T (σk )w
pk .
uH
k pk

(7)
567

Parallel Nonlinear Jacobi–Davidson Algorithm

Y. Matsuo, H. Guo, and P. Arbenz

˜ z = w,
˜ z ⊥ uk . Again, since z ⊥ uk , we get
Then we solve M
z = M −1 w
˜−

−1
w
˜ −1
uH
k M
M pk .
H
−1
uk M pk

(8)

˜ by means of equations (7)–
We can solve equation (5) without computing the pseudoinverse of M
(8). In the ﬁrst few NLJD iterations the approximation far from the desired eigenpair. Therefore, it is not necessary to solve the correction equation accurately. We stop the iteration after
a small number of steps or if the variable stopping criterion by Fokkema et al. [7],
r˜i

2

< 1.2−k r˜ i 2 ,

(9)

is satisﬁed.

3

Solving the correction equation

Let us for a moment assume that T (σk ) in the correction equation (3) is symmetric and that
the correction equation can be solved exactly by Gaussian elimination. If σk = ϑk then
tk = αk T (ϑk )−1 pk − T (ϑk )−1 r k ,

αk =

−1
rk
uH
k T (ϑk )
,
H
−1
uk T (ϑk ) pk

(10)

where αk is determined by the constraint tH
k uk = 0. Equation (10) is mathematically equivalent
with a step of the cubically convergent Rayleigh quotient iteration [13]. The Rayleigh quotient
iteration works very well if uk has a large component in the direction of the sought eigenvector.
Otherwise, there is slow or even no convergence. In other words, if u is far from the desired
eigenvector, the search space is poorly expanded and the iteration stagnates.
To stabilize the iteration we keep the shift ﬁxed in the early phase of the NLJD iteration,
σk = τ . This makes (10) a shift-and-invert iteration that converges stably but slowly, i.e.,
linearly to the eigenvector associated with the eigenvalue closest to the target τ . For the
performance of NLJD it is important to get a suitable approximation of the desired eigenpair
and switch from the ﬁxed to variable shifts at the right moment. In the next section we
experiment with variants on how to do this, cf. [8].
By means of three small examples we illustrate how the choice of the shift in the correction
equation aﬀects the convergence of the NLJD algorithm. These examples are inspired by [6,16]
and turned into quadratic eigenproblems. Note that we do not impose constraints on the
solutions. We investigate several variants of the NLJD algorithm: (1) we keep the shift ﬁxed
throughout the iteration, σk = τ ; (2) we vary the shift σk = ϑk in all steps; (3) we keep
the shift ﬁxed initially and start varying it at step 5. All examples have been executed with
Matlab 7.10.0. The tolerance ε is set to 10−8 for Examples 1, 2, and 3, and to 10−6 for
Example 4, respectively. The initial vector is a random vector except Example 4. In each
example, each procedure ﬁnds the same eigenpair.
Example 1. Here, T ∈ R1000×1000 is a symmetric sparse matrix. The matrix A has diagonal
elements ajj = j. The elements on sub- and superdiagonal are all equal to 0.5. The matrix R is
tridiagonal with elements (−1, 2, −1). The matrix M has nonzero diagonal elements mj,j = 1
and two further nonzeros m1000,1 = m1,1000 = 0.5. We set the target τ = 40 and obtain the
eigenvalue 38.32. The closest eigenvalue is 34.99. We employ the NLJD algorithm with variable
shift (NLJD-1), with ﬁxed shift (NLJD-2), and with switching the shift at step 5 (NLJD-3). The
correction equation is solved exactly in each step. Figure 1 gathers the convergence histories.
568

Parallel Nonlinear Jacobi–Davidson Algorithm

Figure 1: Convergence history for Example 1.

Y. Matsuo, H. Guo, and P. Arbenz

Figure 2: Convergence history for Examples
2 and 3.

The NLJD algorithm with the variable shift converges cubically. However, since the initial
vector is far from the desired eigenvector, NLJD-1 needs many iteration steps to get into the
region of fast converge. NLJD-2 converges linearly. The initial steps of NLJD-3 coincide with
those of NLJD-2. After 5 steps the shift is adapted, σk = ϑk . From this moment we observe
cubic convergence.
Example 2. Now, T ∈ C1000×1000 . The matrices A and R are tridiagonal. The nonzero
elements in the jth row of A are (0.5 + 0.5ı, j − 0.5ı, 0.5 + 0.5ı), those in the jth row of R
are (−1, 2 + jı, −1). The matrix M has nonzeros on the diagonal mj,j = 1 + ı and two more
nonzeros m1000,1 = m1,1000 = 0.5 + 0.5ı. We set τ = 0. All variants of NLJD ﬁnd the eigenvalue
−0.126 + 0.341ı. The nearest eigenvalue is −0.319 + 0.478ı, They behave very similarly as in
Example 1, see Figure 2. However, NLJD-1 gets in the region of cubic convergence much
quicker. Therefore, NLJD-2 is clearly the slowest solver, in terms of iteration count.
Example 3. Since it is not practical for large scale problems to solve the correction equation
exactly we now experiment with an iterative solver. We solve the correction equation for the
problem of Example 2 by GMRES(25) with ILU(0) preconditioner. We build the preconditioner
M from T (τ ). We stop GMRES if either r˜i 2 < 1.2−k r˜ 0 2 or after 50 iteration steps. So, we
restart at most once. We denote the NLJD algorithm with variable shift by “NLJD-4”, with
ﬁxed shift by “NLJD-5”, and with switching shift by “NLJD-6”. Again, we switch from ﬁxed
to variable shift at step 5. The convergence history is given in Figure 2. We can see similarities
between Example 2 and Example 3. The speed of convergence of NLJD-2 and NLJD-5 is linear.
However, NLJD-5 is so slow that it does not converge within 30 outer iteration steps. If we
set the tolerance for the GMRES to be smaller, for instance r˜ i 2 < 2.0−k r˜ 0 2 , the speed
increases. On the other hand, we can still see quadratic convergence for NLJD-6 and also
NLJD-4 since the initial guess is close enough to the solution.
Example 4. We also show how a good initial vector can promote the convergence. According to Voss [20], if no information on the desired eigenvector is available, it may be helpful to
execute a few Arnoldi steps for the linear eigenproblem
T (τ )y = ϑy,

(11)

and choose the eigenvector corresponding to the smallest eigenvalue as the initial vector. In
these experiments, we executed 5 steps to get an initial vector. We solved again the problem of
569

Parallel Nonlinear Jacobi–Davidson Algorithm

Y. Matsuo, H. Guo, and P. Arbenz

Example 2 and employed NLJD-4, NLJD-5, and NLJD-6 with the improved initial vector. The
convergence history is illustrated in Figure 3. NLJD-4 converges cubically to the eigenvalue
in just 3 steps. NLJD-6 also converges cubically after switching the target. The speed of
convergence of NLJD-5 is improved, but it is still not competitive.
These results show that if we solve the correction equation by an iterative solver accurately
enough, we can still see superlinear convergence. Although these problems are artiﬁcial they
show quite nicely the convergence behavior of Jacobi–Davidson. We can say that switching the
shift from ﬁxed to variable can improve the speed of convergence considerably. We however
did not investigate when to actually change the strategy. An initial phase with a ﬁxed shift is
certainly beneﬁcial for a stable convergence.

4

Experiments with realistic problems

In this section, we experiment with problems that originate in electromagnetics simulations.
The computations are executed with Femaxx, a software package that has continuously been
developed since 2002, in a collaboration between ETH Zurich and the Paul Scherrer Institute
(PSI) [1, 2, 8–10]. The time-harmonic Maxwell equations (2) are solved in geometrically complicated domains Ω with curved boundaries. The problems are discretized by the ﬁnite element
method employing linear and quadratic N´ed´elec ﬁnite elements [11], which are mandatory in order to properly represent the electromagnetic ﬁeld vector. The large sparse eigenvalue problems
are solved by the Jacobi–Davidson QZ algorithm [18] or the NLJD algorithm of Algorithm 1.
All solvers are parallelized for distributed memory parallel architectures by means of the Trilinos
framework which uses the MPI for communication [19].
In the previous version of Femaxx, only the NLJD algorithm with ﬁxed shift and diagonal
(Jacobi) preconditioner are implemented [9, 10]. We implemented new preconditioners and
incorporated variable shift strategies into NLJD. The problems that we discuss now have been
executed on 32 cores of the Merlin4 cluster at PSI. Merlin4 has 30 compute nodes (with a
total of 360 cores). The nodes have dual Intel Xeon X5670 2.93 GHz CPUs with 24 or 48 GB
RAM. We show numerical experiments with ﬁxed shift (indicated by “f”), and with variable
shift (indicated by “s”). “blkj-” refers to the Block-Jacobi preconditioner. Here, a block is the
local diagonal portion of the respective matrix. In the tables, “nitout ” and “nitin ” denote the
number of outer and inner iteration steps, respectively. “tprec ” denotes the time for building
the preconditioner. In all experiments we compute a single eigenpair.

4.1

Ohmically lossy material

We ﬁrst consider the half-ﬁlled rectangular resonator [9, 22], where one half of the complete
volume is ﬁlled with ohmically lossy material and the other half is vacuated. It is modeled by
the Maxwell equations (2). We apply perfect electric conductor (PEC) boundary conditions
which implies that the tangential electric ﬁeld vanishes identically on the cavity wall. We
approximate the problem by quadratic N´ed´elec elements with 1’852’810 degrees of freedom and
76’854’188 non-zero elements. The ﬁnite element discretization yields a constrained complexvalued symmetric quadratic eigenproblem of the form (1). The matrix A is real-valued, the
matrix M is complex-valued, and R is a purely imaginary matrix. C is a complex-valued
rectangular matrix. Setting τ = 100 we ﬁnd the eigenvalue 119.704 + 108.913ı. The nearest
eigenvalue is 137.883 + 80.981ı. As indicated in Algorithm 1, the computed eigenpairs (λ, x)
satisfy the divergence-free condition, i.e., C T x = 0. We use the M-orthogonal projector P
570

Parallel Nonlinear Jacobi–Davidson Algorithm

Figure 3: Convergence history for Example 4.

Y. Matsuo, H. Guo, and P. Arbenz

Figure 4: Convergence history of the halfﬁlled rectangular resonator

Table 1: Statistics for the half-ﬁlled rectangular resonator
Prec
diag-f
blkjilu-f
blkjlu-f

nitout
52
29
23

nitin
1306
366
160

t
538.67
409.00
453.44

tprec
0.07
7.3
46.58

Prec
diag-s(35)
blkjilu-s(20)
blkjlu-s(15)

nitout
48
27
20

nitin
757
394
244

t
534.13
427.17
582.85

tprec
0.08
7.66
41.71

in (4) to enforce the constraint [8]. This implies that all vectors in the search space satisfy the
divergence-free condition.
We solve the half-ﬁlled rectangular resonator by the NLJD with ﬁxed or switching shift.
The correction equation is solved by the Bi-CGstab method with Jacobi (diagonal) and BlockJacobi preconditioners. With Block-Jacobi preconditioners, each local block is solved by either
the ILU(0) or the LU factorization. In Femaxx we use hierarchical bases for the N´ed´elec and
the Lagrange basis functions. Therefore it is natural to use hierarchical basis preconditioners
for solving the correction equation [1]. We employ the NLJD and restart after every 10 iteration
steps. The tolerance ε for the residual norm is set to 10−2 . In Bi-CGstab we execute at most
50 iterations for the correction equation. For the Jacobi preconditioner, we switch to variable
shifts after 35 outer iteration. For the Block-Jacobi ILU and LU preconditioners, the switch is
after 20 and 15 steps, respectively.
The numerical results are presented in Table 1. The convergence histories are found in
Figure 4. Considering these results, the switching shift strategy reduces the number of outer
iterations. In this example it is much more diﬃcult to ﬁnd a good approximation of the desired
eigenpair than in the previous examples. Although we can reduce the number of outer iteration
steps by switching shifts, the gain is little. Since the preconditioner is recomputed in every
iteration step, the single step becomes much more expensive. This is in particular visible
with the LU factorizations (blkj-lu). Nevertheless, if the reduction of the iteration steps is big
enough, then also the overall execution time is reduced. Clearly, it is very important to switch
from ﬁxed to variable shift at the right moment. The blkj-ilu approach is a good compromise
as the (re-)computation of the preconditioner is not excessively expensive.
571

Parallel Nonlinear Jacobi–Davidson Algorithm

Figure 5: Convergence history for the halfﬁlled rectangular resonator.

Y. Matsuo, H. Guo, and P. Arbenz

Figure 6: Convergence history for DRA
problem.

Experiments with an improved initial vector
Now we discuss numerical experiments with an improved initial vector. Voss [20] suggested to
generate a ‘good’ initial vector by executing a few steps of the Arnoldi algorithm for the linear
eigenvalue problem
T (τ )y = ϑy,

(12)

and select the eigenvector corresponding to the smallest eigenvalue as the initial vector for the
nonlinear problem. In our experiments, we executed 5 Arnoldi steps to get an initial vector.
Execution times in the tables include the time for constructing the initial vector. Constructing
an initial vector is referred to by “a”.
We solve again the half-ﬁlled rectangular resonator [9, 22] with quadratic elements with
1’852’810 degrees of freedom and 76’854’188 non-zero elements by the Bi-CGstab algorithm with
Jacobi and Block-Jacobi preconditioners. In this experiment the shift is ﬁxed for all solvers.
Except for the initial vector, every parameter is the same as with the half-ﬁlled rectangular
resonator. The results are found in Table 2. Here we can see improvements. Comparing with
the results with ﬁxed shift in Table 1, the NLJD with diagonal preconditioning the number
of outer iterations is reduced by about 40%, the overall time by about 10%. The NLJD with
Block-Jacobi preconditioning reduces the number of the outer iteration by about 35% and the
overall time by about 5%. Since we need time to construct a initial vector, the reduction of
the overall time is less. Nevertheless, by executing the Arnoldi steps, the initial vector can
get signiﬁcant components in the direction of the desired eigenvector and convergence to an
unwanted eigenvector is less probable.
Then we executed the NLJD with switching shifts and Arnoldi type initial vector. The
results are listed on the right side of Table 2. Comparing with the left side of Table 1, the
number of outer iteration steps is further reduced. However, again, the increase of the number
of inner iteration steps per outer iteration step counteracts this improvement. Figure 5 shows
the convergence histories of NLJD with ﬁxed and switching shift, and improved initial vector.
The speed of convergence of the NLJD with switching shift and good initial vector is improved.

572

Parallel Nonlinear Jacobi–Davidson Algorithm

Y. Matsuo, H. Guo, and P. Arbenz

Table 2: Statistics for the half-ﬁlled rectangular resonator with a good initial vector
Prec
diag-f-a
blkjilu-f-a
blkj-lu-f-a

nitout
27
17
14

nitin
597
340
150

t
500.85
398.43
419.62

tprec
0.07
8.53
43.63

Prec
diag-s(20)-a
blkjilu-s(12)-a
blkjlu-s(10)-a

nitout
24
15
12

nitin
677
396
230

t
540.13
487.11
626.12

tprec
0.08
7.8
42.56

Table 3: Statistics of the DRA with a random (left) or improved (right) initial vector. The
shift is ﬁxed.
Prec
diag-f
blkilu-f
blkjlu-f

4.2

nitout
27
16
17

nitin
474
316
223

t
130.27
71.46
85.36

tprec
1.18
1.92
2.92

Prec
diag-a
blkilu-a
blkjlu-a

nitout
16
9
11

nitin
258
270
179

t
80.45
56.94
60.73

tprec
1.15
2.02
3.12

Dielectric resonant antenna with a good initial vector

Now we consider the problem of rectangular dielectric resonator antennas (DRAs) in the microwave region. Here we solve the time-harmonic Maxwell equations (2) with ﬁrst order absorbing boundary conditions (ABC) [11]
n × ∇ × E(x) = −ik n×(n × E(x)),
√
where k = k0 μr εr is the wavenumber of the surrounding medium. DRAs have been analyzed
theoretically [12,14]. The complex-valued matrices A, R, and M are composed of real symmetric
element matrices multiplied by a (varying) complex factor. With quadratic N´ed´elec elements
there are 439’188 degrees of freedom and 1’895’460 non-zero elements. With the target τ = 0.1
we ﬁnd the eigenvalue 0.117 + 0.00134ı. The closest eigenvalue is 0.162 + 0.0392ı. Again, we
execute 5 Arnoldi steps to get an improved initial vector. The numerical results are presented
in Table 3. The convergence histories are found in Figure 6. By using a good initial vector,
the NLJD with diagonal preconditioning reduces about 40% the number of outer and inner
iteration steps, as well as the overall time. The NLJD with Block-Jacobi preconditioning also
exhibit considerable improvements with respect to the number of outer and inner iterations,
and overall time.

5

Conclusions

In this paper, we discussed how the shift in the correction equation and the initial vector aﬀect
the speed of convergence of the NLJD. Clearly, switching the shift reduces the number of outer
iteration steps and improves the speed of convergence. However, the recomputation of the
preconditioner consumes a lot of computing time. Therefore, choosing the right instance when
to switch from the ﬁxed to the variable shift is critical. In our experiments, the switching helped
to improve the execution times of the solver, however by at most 20%.
On the other hand, improving the initial vector by solving an auxiliary linear eigenvalue
573

Parallel Nonlinear Jacobi–Davidson Algorithm

Y. Matsuo, H. Guo, and P. Arbenz

problem was very successful. For instance in the DRA problem, choosing a good initial vector
reduced the overall solution time by 40%.
Compared with plain diagonal preconditioning, Block-Jacobi preconditioning improves the
speed of convergence. It reduces the number of inner and outer iterations leading to substantial
reductions of execution time – in spite of the fact that constructing these preconditioners needs
more time.
We plan to apply the techniques developed in this paper to improve solution times to the
simulation of further resonating electromagnetic structures such as plasmonic nanostructures.
In particular we will investigate more sophisticated techniques for obtaining good starting
vectors.

References
[1] P. Arbenz, M. Beˇcka, R. Geus, U. Hetmaniuk, and T. Mengotti. On a parallel multilevel preconditioned Maxwell eigensolver. Parallel Comput., 32(2):157–165, 2006.
[2] P. Arbenz and R. Geus. Multilevel preconditioned iterative eigensolvers for Maxwell eigenvalue
problems. Appl. Numer. Math., 54(2):107–121, 2005.
[3] P. Arbenz and M. E. Hochstenbach. A Jacobi–Davidson method for solving complex-symmetric
eigenvalue problems. SIAM J. Sci. Comput., 25(5):1655–1673, 2004.
[4] T. Betcke and H. Voss. A Jacobi–Davidson-type projection method for nonlinear eigenvalue problems. Future Gener. Comput. Syst., 20(3):363–372, 2004.
[5] S.J. Cooke and B. Levush. Eigenmodes of microwave cavities containing high-loss dielectric materials. In 17th Particle Accelerator Conference, pages 2431–2433, May 1997.
[6] M. Crouzeix, B. Philippe, and M. Sadkane. The Davidson method. SIAM J. Sci. Comput.,
15:62–76, 1994.
[7] D. R. Fokkema, G. L. G. Sleijpen, and H. A. van der Vorst. Jacobi–Davidson style QR and QZ
algorithms for the partial reduction of matrix pencils. SIAM J. Sci. Comput., 20(1):94–125, 1998.
[8] R. Geus. The Jacobi–Davidson algorithm for solving large sparse symmetric eigenvalue problems.
PhD thesis no. 14734, ETH Zurich, 2002.
[9] H. Guo. 3-dimensional eigenmodal analysis of electromagnetic structures. PhD thesis, ETH Zurich,
2013.
[10] H. Guo, B. Oswald, and P. Arbenz. 3-dimensional eigenmodal analysis of plasmonic nanostructures. Opt. Express, 20(5):5481–5500, 2012.
[11] J. Jin. The Finite Element Method in Electromagnetics. John Wiley, New York, NY, 2nd edition,
2002.
[12] R. K. Mongia and A. Ittipiboon. Theoretical and experimental investigations on rectangular
dielectric resonator antennas. IEEE Trans. Antennas Propag., 45(9):1348–1356, 1997.
[13] Y. Notay. Combination of Jacobi–Davidson and conjugate gradients for the partial symmetric
eigenproblem. Numer. Linear Algebra Appl., 9:21–44, 2002.
[14] A. Okaya and L. F. Barash. The dielectric microwave resonator. Proc. IRE, 50:2081–2092, 1962.
[15] A. Ruhe. Algorithms for the nonlinear eigenvalue problem. SIAM J. Numer. Anal., 10(4):674–689,
1973.
[16] G. L. G. Sleijpen and H. A. van der Vorst. A Jacobi–Davidson iteration method for linear eigenvalue
problems. SIAM J. Matrix Anal. Appl., 17(2):401–425, 1996.
[17] G. L. G. Sleijpen and H. A. van der Vorst. The Jacobi–Davidson method for eigenvalue problems
and its relation with accelerated inexact Newton scheme. In S. D. Margenov and P. S. Vassilevski,
editors, Second IMACS International Symposium on Iterative Methods in Linear Algebra, pages
377–389, New Brunswick, NJ, 1996. IMACS.

574

Parallel Nonlinear Jacobi–Davidson Algorithm

Y. Matsuo, H. Guo, and P. Arbenz

[18] G. L. G. Sleijpen and H. A. van der Vorst. Jacobi–Davidson method. In Z. Bai, J. Demmel,
J. Dongarra, A. Ruhe, and H. van der Vorst, editors, Templates for the Solution of Algebraic
Eigenvalue Problems: A Practical Guide, pages 238–246. SIAM, Philadelphia, PA, 2000.
[19] The Trilinos Project Home Page. http://trilinos.sandia.gov/.
[20] H. Voss. A Jacobi–Davidson method for nonlinear and nonsymmetric eigenproblems. Comput.
Struct., 85(17-18):1284–1292, 2007.
[21] J. Demmel Z. Bai, J. Dongarra, A. Ruhe, and H. van der Vorst. Templates for the Solution of
Algebraic Eigenvalue Problems: A Practical Guide. SIAM, Philadelphia, PA, 2000.
[22] Y. Zhu and A. C. Cangellaris. Robust ﬁnite-element solution of lossy and unbounded electromagnetic eigenvalue problems. IEEE Trans. Microwave Theory Tech., 50(10):2331–2338, 2002.

575

