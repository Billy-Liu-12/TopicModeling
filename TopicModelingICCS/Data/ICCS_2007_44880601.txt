Efficient Parallel Tree Reductions on
Distributed Memory Environments
Kazuhiko Kakehi1 , Kiminori Matsuzaki2 , and Kento Emoto3
1

Division of University Corporate Relations
Department of Mathematical Informatics
3
Department of Creative Informatics
University of Tokyo
{kaz,kmatsu,emoto}@ipl.t.u-tokyo.ac.jp
2

Abstract. A new approach for fast parallel reductions on trees over distributed
memory environments is proposed. By employing serialized trees as the data representation, our algorithm has a communication-efficient BSP implementation
regardless of the shapes of inputs. The prototype implementation supports its real
efficacy.
Keywords: Tree reduction, parentheses matching, BSP, parallel algorithm.

1 Introduction
Research and development of parallelization have been intensively done toward matrices or one dimensional arrays. Looking at recent trends in applications, another data
structure has also been calling for efficient parallel treatments: the tree structure. Emergence of XML as a universal data format, which takes the form of a tree, has magnified
the impact of parallel and distributed mechanisms toward trees in order to reduce computation time and mitigate limitation of memory. However, parallel tree computation
over distributed memory environments is not so straightforward as it looks.
Consider, as a simple and our running example, a computation maxPath to find the
maximum of the values each of which is a sum of values in the nodes from the root to
each leaf. When it is applied to the tree in Fig. 1, the result should be 12 contributed by
the path of values 3, −5, 6 and 8 from the root. Parallelization of such a simple computation under distributed memory environments requires consideration from two aspects.
The first is the underlying data representations, including its distribution among processors to guarantee performance toward trees of arbitrary shapes. The second is derivation
of the parallel algorithm. As associativity often helps parallelization, we need to exploit
the similar property under trees which suitably work for the data representations.
This paper gives a clear solution for parallel tree reductions with its start point to
use serialized forms of trees. Their Notable examples are the serialized (streamed) representations of XML or parenthesized numeric expressions which are obtained by tree
traversals. The first problem mentioned above is naturally resolved by this choice, since
distribution of serialized data among processors is much simpler than that of trees. As
for the second point, we present an efficient parallel algorithm for tree reductions satisfying extended distributivity. As instances of serialized trees, parallelization of parentheses matching problems, which figures out correspondence between brackets, have
Y. Shi et al. (Eds.): ICCS 2007, Part II, LNCS 4488, pp. 601–608, 2007.
c Springer-Verlag Berlin Heidelberg 2007

602

K. Kakehi, K. Matsuzaki, and K. Emoto
3
4

-5
6

-2

5

2

1

8

-6

se

en

clo

op

tree traversal /
parsing

internal node

-1

leaf
4

(1,/)

(0,3)
(1,4)

(1,/)
(1,/)
(2,6)
(2,1)
(3,/)
(4,4)
(3,/)
(3,/)
(1,/)
(2,-6)
(2,/)
(1,5)
(2,/)
(0,/)
(1,-5)
(3,-2)
(3,8)
(3,-1)
(4,/)
(1,2)
(2,/)

=

aligned by depth
0

3

1
4

-5

5
6

1
-2

8

-1

2

2
-6

3
4

4

Fig. 1. A rose tree (upper left), its serialized representation as a sequence of pairs (middle) and
another representation according to the depth (lower)

plenty of work ( [1, 2, 3, 4, 5] to mention a few); our algorithm, with good resemblance
to the one under BSP [6], has also a BSP implementation with three supersteps.
This paper is organized as follows. After this Introduction, Sect. 2 observes our tree
representations and tree reductions. Section 3 develops the algorithm. Our algorithm
consists of three phases, where the first two perform computation along with parentheses matching, and the last reduces a tree of size less than twice of the number of
processors. Section 4 supports our claims by some experiments. We conclude this paper in Sect. 5 and mentioning future directions. We refer the interested reader to our
technical report [7] for the omitted details due to the space limitation of this paper.

2 Preliminaries
This section observes the underlying frameworks for our parallelization: tree structures,
their serialized form, tree homomorphism, and extended distributivity.
2.1 Trees and Their Serialized Representation
We treat trees with unbound degree (trees whose nodes can have an arbitrary number
of subtrees); Fig. 1 shows an example. As was explained in Introduction, our internal
representation is to keep tree-structured data in a serialized manner. The sequence of
the middle in Fig. 1 is our internal representation of the example tree. Like XML serialization or parentheses expressions, it is a combination of a preorder (for producing the
open elements) and a postorder traversal (for producing the close elements afterwards).
We assume well-formedness, with which sequences are guaranteed to be parsed back
into trees, and without loss of information we simplify close elements to be “/”. For
the convenience of later discussion, we assign the information of the depth in the tree
to each node. The figure also depicts their presentation according to the depth.

Efficient Parallel Tree Reductions on Distributed Memory Environments

603

2.2 Tree Homomorphism and Extended Distributivity
We use the framework called tree homomorphism [8], which specifies recursive tree
reductions h using h , ⊕, and associative ⊗ with its unit ι⊗ :
h( Node a [t1 , . . . , tn ] ) = a ⊕ ( h(t1 ) ⊗ · · · ⊗ h(tn ) ) ,
h( Leaf a )
= h (a) .
The computation maxPath mentioned in Introduction is also a tree homomorphism:
maxPath( Node a [t1 , . . . , tn ] ) = a + ( maxPath(t1 ) ↑ · · · ↑ maxPath(tn ) ) ,
maxPath( Leaf a )
= id (a) = a .
Here, id is the identify function, and ↑ returns the bigger of two numbers whose unit is
−∞. When it is applied to the tree in Fig. 1, the result should be 12 = 3 + (−5) + 6 + 8.
We can apply parallelization of tree homomorphism over serialized trees based on
list homomorphism [3]. This naive approach, however, suffers from the factor of tree
depths. We need to review an additional property called extended distributivity [9]. This
property is explained, by introducing a new operator defined as (a, b, c) e = a ⊕
(b ⊗ e ⊗ c), as follows: for any triples (au , bu , cu ), (al , bl , cl ), and any expression e,
there exists a triple (a, b, c) which satisfies
(au , bu , cu )

( (al , bl , cl )

e ) = (a, b, c)

e.

Efficient parallel reduction requires these computations as well as ⊗ and ⊕ to be done
in constant time. Our running example satisfies these properties as the following calculation shows.
(au , bu , cu ) ( (al , bl , cl ) e )
= au + (bu ↑ (al + (bl ↑ e ↑ cl )) ↑ cu )
= (au + al ) + ((−al + bu ↑ bl ) ↑ e ↑ (cl ↑ −al + cu ))
= ( (au + al ), (−al + bu ↑ bl ), (cl ↑ −al + cu ) ) e
For some other examples under these formalizations, see our previous work [9].

3 Parallel Computation over the Serialized Trees
This section develops a parallel algorithm for tree homomorphism which satisfies extended distributivity. We explain our algorithm in three phases: (1) the first phase applies
tree homomorphism toward segments (consecutive subsequences) in each processor as
much as possible; (2) after communications among processors the second phase performs further reduction using extended distributivity, producing a binary tree as a result
whose internal nodes are specified as triples, and size is less than twice of the number
of processors; finally (3) the third phase reduces the binary tree into a single value.
We use N and P to the number of tree nodes and available processors, respectively.
During the explanation we assume P = 4. The algorithm resembles the analysis of
parentheses matching problems under BSP [5].
First phase. Each processor applies tree homomorphism to its given segment of size
2N/P. The process is summarized as Routine 1. This process leaves fragments of results,

604

K. Kakehi, K. Matsuzaki, and K. Emoto

5
1

2

5
-6

1
3

-1

2
-∞ -6
-∞

4

Fig. 2. An illustrating example of the first phase—applying tree homomorphism maxPath to a
segment of Fig. 1 (lined and dashed ovals indicate values obtained by reduction and −∞ (the
unit of ↑), respectively)

in arrays asp , bsp , csp and an integer dp for each processor number p. The array asp
is to keep the open elements without their corresponding close element. Each element
of asp can have subtrees before the next one in asp , and their reduced values are kept
in bsp . Similar treatments are done to unmatched close elements, leaving values in csp
(we remove unmatched close elements thanks to the absence of values). The integer dp
denotes the shallowest depth in processor p. While both of elements in asp and bsp are
listed in a descending manner, those of csp are in ascending manner; the initial elements
of asp and bsp and the last one of csp are at height dp (except for cs0 and csP−1 whose
last element at depth 0 is always ι⊗ and therefore is set to be eliminated).
Routine 1. Prepare arrays as, bs, cs (behaving as stacks growing from left to right),
and an integer variable d. A temporary t is used. Sequence (d0 , a0 ), . . . , (dn−1 , an−1 )
(n ≥ 1) is given.
• Set d ← d0 . If a0 is “/” then cs ← [ι⊗ , ι⊗ ], as ← [ ], bs ← [ ]; else cs ← [ι⊗ ],
as ← [a0 ], bs ← [ι⊗ ].
• For each i in {1, . . . , n − 1}
• if ai is not “/” (namely a value), then push ai to as and ι⊗ to bs;
else • if as is empty, then push ι⊗ to cs, and set d ← di ;
else • pop a from as and b from bs.
• if b = ι⊗ (implying a is a leaf) then t ← h (a ); else t ← a ⊗ b ;
• if bs is not empty, then pop b from bs and push b ⊗ t to bs;
else pop c from cs and push c ⊗ t to cs.
• If P = 1 then remove ι⊗ at the bottom of cs0 and csP−1 .
In Fig. 2 we show a case of the illustrating segment from the 10th element (3, −1) to
the 21st (2, −6) of the sequence in Fig. 1. We have, as depicted:
cs = [−1 + id (4), id (1), id (5)]
= [3, 1, 5],

as = [2, 6],
bs = [−∞, −∞],

d = 1.

Please note that we regard absence of subtrees as an empty forest to which the tree
homomorphism returns −∞, the unit of ↑ (at depth 2 and 3 kept in bs). When we
distribute the whole sequence in Fig. 1 evenly among four processors (6 elements for
each), the results by this phase is shown in Fig. 3, left.
A linear routine produces as, bs, cs and d. The worst case in terms of the size of the
results occurs when an sequence of only open elements is given, resulting in two arrays
as and bs of the length 2N/P for each.

Efficient Parallel Tree Reductions on Distributed Memory Environments
proc #0

proc #1

proc #2

proc #3

4

-5
-∞

↑

6
-∞

-2

8
-∞

cs0 =
as0 =
bs0 =
d0 =

-∞

-∞

1
-1

-∞
4

-∞

(3,4,-4)

0

↑

3

1
5

-4
-∞

-∞

605

(1,-∞,-5)

2
3

-∞↑5

4
-2

8↑3

[]
cs1 = [−∞, 8] cs2 = [−∞, −∞, 1, −∞] cs3 = [−∞, −4]
[3, −5, 6, 2]
as1 = [−1]
as2 = [5]
as3 = [ ]
[4, −∞, −∞, −∞] bs1 = [4]
bs2 = [−∞]
bs2 = [ ]
0
d1 = 3
d2 = 1
d3 = 0

Fig. 3. Triples between two processors (left) and the resulting tree (right) after the second phase

Second phase. The second phase matches data fragments kept in each processor into
triples using communication between processors. Later, we reduce consecutive occurrences of triples into a value, or into one triple by extended distributivity.
When we look carefully at Fig. 3, we notice that 3 in as0 at depth 0 now has five parts
at depth 1 as its children: the value 4 in bs0 , a subtree spanning from processors 0 to 2
whose root is −5 in as0 , the value −∞ in cs2 , a subtree from processor 2 to 3 whose
root is 5 in as2 , and the value −4 in cs3 . As these subtrees need reducing separately, we
focus on the leftmost and the rightmost values in bs0 and cs3 (we leave the value −∞
in cs2 for the time being). We notice that the group of the value 3 in as0 with these two
values in processors 0 and 3 forms a triple (3, 4, −4).
Similarly, two elements in as0 at depth 1 and 2, with two elements each in bs0 and
cs2 at depth 2 and 3, respectively, form two triples (−5, −∞, 1) and (6, −∞, −∞). The
former triple indicates a tree that awaits the result of one subtree specified by the latter.
This situation is what extended distributivity takes care of: we can merge two triples
(a sequence of triples in general) into one: (−5, −∞, 1) ( (6, −∞, −∞) e ) =
( (−5 + 6), (−6 − ∞ ↑ −∞), (−∞ ↑ −6 + 1) ) e = (1, −∞, −5) e for any e.
In this way, the group of data fragments in two processors turn into one triple.
Such groups from two adjacent processors are reduced into a single value without any missing subtrees in between: instead of treating using extended distributivity,
the values −2 in as0 and −1 in as1 at depth 3, and 5 in as2 at depth 2 with their
corresponding values in bsi and csi+1 (i = 0, 1, 2) turn into values id (−2) = −2,
−1 + (4 ↑ −∞) = 3, id (5) = 5, respectively.
We state the following lemma to tell the number of resulting groups in total.
Lemma 1. The second phase produces groups of the number at most 2 P − 3.
The proof sketch is as follows. Let Rp be the the number of groups among p processors.
As Fig. 3 indicates, R2 = 1, and for p > 2 we derive Rp ≤ 1 + Rj + Rp−j+1
with 1 < j < p; hence we have Rp ≤ 2p − 3. This lemma guarantees that, after
transactions of data fragments among processors, one processor needs to take care of at
most two such groups. The computed value at the shallowest depth dp in each processor
are associated to their right for later computation by ↑ (8 in cs1 , −∞ in cs2 ; see Fig. 3).
pl ↔pr
denotes a
The following Routine 2 figures out groups among processors. M[d
u ,dl ]
group between processors pl and pr whose data fragments span from the depth du until
dl (∞ in dl indicates “everything starting from du ”); M[d↔
is inserted as a dummy
u ,dl ]

606

K. Kakehi, K. Matsuzaki, and K. Emoto

group in case the same d appear among more than two consecutive processors, and
assumed to be reduced into ι , a virtual left unit of (namely ι
e = e).
Routine 2. A stack is used whose top is referred as (ps , ds ). Sequence (p, dp ) is given
in the ascending order of p.
• Push the first pair (0, 0) on a stack
• For each i in {1, . . . , P − 1}
• prepare a variable d ← ∞.
ps ↔i
, set d ← ds and pop from the stack;
• while di < ds , produce M[d
s ,d]
ps ↔i
• if di = ds , then produce M[d
and M[d↔
, and pop from the stack.
i ,ds ]
s ,d]

ps ↔i
.
else produce M[d
i ,d]
• push (i, di ).
↔
• Finally eliminate the last mating pair (that is M[0,0]
).

We summarize this phase. This phase consists of two steps. In the first all processors
figure out the groups by Routine 2 and allocation of groups by any deterministic rule
through sharing their depth information di . They require O(P) communication and computation costs in total. The second step is to apply further computation toward each
group. In the worst case it is possible that a processor sends out its whole data fragments in it to all other P − 1 processors, and receives two groups and evaluates each
into one triple or value. Since the size of fragments in one processor and that of each
group are at most O(N/P), the cost for data transaction and computation is bound by
O(N/P).
Third phase. This last phase compiles obtained triples or values and reduce them into
a single value. As Fig. 3 shows, the obtained triples and values in the previous phase
↔
). We collect triples or
form a binary tree of size 2 P − 3 (including dummies by M[0,0]
values in one processor, and apply tree reduction in O(P) time.
In summary, we have three communication rounds (two in the second phase, one in
the third phase). We conclude this section by stating the following theorem.
Theorem 1. Tree homomorphism with extended distributivity has a BSP implementation with three supersteps of at most O(P + N/P) communication cost for each.

4 Experiments and Discussion
We performed experiments using our prototype implementation using C++ and MPI.
Specifications of our PC cluster are shown in Table 1, left. We simulated a simple query
on rose trees where local computation of ⊗ and ⊕ were matrix multiplication-like operations over matrices of the size 10 × 10. We prepared randomly generated trees of
three types, namely (RS) shallow, (RD) deep, and (M) a monadic tree (a tree-view of
a list). The tree size was constrained by the memory size a machine has. The height
of RS came from observations on XML documents [10]. We executed the program over
each type using 2i processors (i = 0, . . . , 6).
Table 1 summarizes the results. First, it is natural that no difference existed in terms
of costs of initial data distribution. As their plots in the left of Fig. 4 indicates, our
algorithm exhibited good scalability for both of (RS) and (RD). Results of (M) fell

Efficient Parallel Tree Reductions on Distributed Memory Environments

607

Table 1. Specification (left) and execution times (right) of our experiments
machine 2.8 GHz CPU, 2GB memory
network Gigabit Ethernet
software Linux 2.4.21,
gcc 2.96, mpich 1.2.7
data: randomly generated trees of size
2,000,000
(RS) with maximum height 7
(RD) with maximum height of 5,000
(M) a monadic tree

P

1
2
4
8
16
32
64

Deep (RD)
Shallow (RS)
Monadic (M)
dist. comp. total comp. total comp. total
— 23.5 23.5
22.8 22.8
N.A. N.A.
0.48 12.3 12.7
12.6 13.0
N.A. N.A.
0.56
6.13 6.70
5.81 6.36 60.1 60.7
0.62
3.18 3.80
3.79 4.41 20.9 21.5
0.70
1.81 2.51
1.96 2.65 14.8 15.5
0.77
1.11 1.88
1.04 1.81
8.36 9.14
0.83
0.68 1.52
0.57 1.40
4.47 5.30

70
Deep (RD)
Shallow ( RS )
Monadic (M)

25

Relative Speedups

Execution Time (sec)

30

20
15
10
5
0

Deep (RD)
60 Shallow ( RS )
linear
50
40
30
20
10

0

10

20
30
40
50
Number of Processors

60

70

0

0

10

20
30
40
50
Number of Processors

60

70

Fig. 4. Plots of Table 1 by total execution time (left) and by speedups of computation time (right)

behind the other two: it ran out of memory until 2 processors (the result by 4 processors seems also affected), and its execution was around seven times slower than other
cases. The reason is that there are no reducible subtrees in the first phase, which incurs
heavy data transactions and costly computation by extended distributivity. It should be
noted that the similar BSP algorithm for all nearest smaller values problem, generalization of parentheses matching, toward average cases is shown not to suffer from heavy
transactions [5]. While we need to develop a similar theoretical proof to our algorithm,
experiments on random trees (RS and RD) suggest that our algorithm works efficiently
toward average cases.
We make a brief comparison with existing approaches of parallel tree computations.
One common approach under distributed memory environments has their basis in list
ranking, and their parallel tree contractions require expensive costs of O(N/P log P)
[11, 12, 13]. Using a technique called m-bridge [14] for initial data distribution, our
existing library implements parallel tree contractions in O(N/P + P) over trees kept as
linked structures [15, 16, 9]. In comparison with them, the approach in this paper has
two notable advantages. First, it can be coded as simple for-loops over one-dimensional
arrays and benefits compiler optimizations well. We also observed that high memory locality by serialized forms brought considerable performance improvements, especially
when the required computation was memory-intensive where cache effects become important. Second, the data distribution process in this paper is really small compared to
the cost of m-bridge, in which traversal over linked structure is involved.

608

K. Kakehi, K. Matsuzaki, and K. Emoto

5 Concluding Remarks
In this paper we have developed a new approach to reduce a tree in parallel. The algorithm has been shown to run scalably as well as fast in theory and practice, by exploiting
the serialized trees and a property called extended distributivity.
At the moment we have only analyzed cost for the worst case. It is our interesting
task to analyze average cases, or go a step further to the theory of heterogeneous cases.
Acknowledgment. This research was partially supported by the Ministry of Education, Culture, Sports, Science and Technology, Grant-in-Aid for Young Scientists (B),
17700026, 2005–2007.

References
1. Berkman, O., Schieber, B., Vishkin, U.: Optimal doubly logarithmic parallel algorithms
based on finding all nearest smaller values. Journal of Algorithms 14 (1993)
2. Prasad, S., Das, S., Chen, C.: Efficient EREW PRAM algorithms for parentheses-matching.
IEEE Transactions on Parallel and Distributed Systems 5(9) (1994)
3. Cole, M.: Parallel programming with list homomorphisms. Parallel Processing Letters 5
(1995)
4. Kravets, D., Plaxton, C.: All nearest smaller values on the hypercube. IEEE Transactions on
Parallel and Distributed Systems 7(5) (1996)
5. He, X., Huang, C.: Communication efficient BSP algorithm for all nearest smaller values
problem. Journal of Parallel and Distributed Computing 16 (2001)
6. Valiant, L.: A bridging model for parallel computation. Communication of the ACM 33(8)
(1990)
7. Kakehi, K., Matsuzaki, K., Emoto, K., Hu, Z.: A practicable framework for tree reduction
under distributed memory environments. Technical Report METR 2006-64, Department of
Mathematical Informatics, University of Tokyo (2006)
8. Skillicorn, D.B.: Parallel implementation of tree skeletons. Journal of Parallel and
Distributed Computing 39(2) (1996)
9. Matsuzaki, K., Hu, Z., Kakehi, K., Takeichi, M.: Systematic derivation of tree contraction
algorithms. Parallel Processing Letters 15(3) (2005) (Original version appeared in Proc. 4th
International Workshop on Constructive Methodology of Parallel Programming, 2004.).
10. Mignet, L., Barbosa, D., Veltri, P.: The XML web: a fist study. In Proceedings of the Twelfth
International World Wide Web Conference, ACM Press (2003)
11. Mayr, E.W., Werchner, R.: Optimal routing of parentheses on the hypercube. Journal of
Parallel and Distributed Computing 26(2) (1995)
12. Mayr, E.W., Werchner, R.: Optimal tree contraction and term matching on the hypercube
and related networks. Algorithmica 18(3) (1997)
13. Dehne, F., Ferreira, A., C´aceres, E., Song, S., Roncato, A.: Efficient parallel graph algorithms
for coarse-grained multicomputers and BSP. Algorithmica 33(2) (2002)
14. Reid-Miller, M., Miller, G.L., Modugno, F.: List ranking and parallel tree contraction. In
Reif, J.H., ed.: Synthesis of Parallel Algorithms. Morgan Kaufmann (1993)
15. SkeTo Project Home Page. http://www.ipl.t.u-tokyo.ac.jp/sketo/
16. Matsuzaki, K., Emoto, K., Iwasaki, H., Hu, Z.: A library of constructive skeletons for
sequential style of parallel programming (invited paper). In: Proceedings of the First
International Conference on Scalable Information Systems, IEEE Press (2006)

