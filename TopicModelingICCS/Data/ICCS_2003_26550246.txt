Evidential Volume Approach for Certiﬁcation
Silke Kuball and Gordon Hughes
Safety Systems Research Centre, Department of Computer Science
University of Bristol, Merchant Venturers Building, Woodland Road
Bristol BS8 1UB, UK
silke@cs.bris.ac.uk

Abstract. In this paper we describe an approach to capture the degree
of compliance of a product with the international standard for functional
safety of E/E/PE systems, IEC 61508. We call this the evidential volume of an assessment scenario. It is based on compiling observed evidence
according to assigned weighting factors, which describe the relative importance of each piece of evidence. The evidential volume can by itself
be used as an indicator to compare diﬀerent assessment scenarios. This
could form the basis for improved consistency in assessment. We suggest a model to relate the evidential volume to the probability of having
achieved a product of required safety integrity. Developing such a relationship can lead to a decision-aid on acceptance or rejection or can be
used to decide whether additional evidence, such as statistical testing
could be used to achieve target safety integrity. The model we suggest
is based on the Success Likelihood Index Model (SLIM) and it poses an
initial step towards decision-support for assessment.

1

Introduction

Systems comprising software and/or programmable electronic systems are increasingly used within safety-critical applications such as nuclear protection systems or avionics. From this stems an increased need for guidance on the development and assessment of safe and reliable systems. One of the routes that
support the development and maintenance of safe and reliable systems is the
use of industry standards such as DO-178B (avionics), IEEE/EIA 12207 (defense), IEC 61508 (all industry sectors) PES Guidelines (nuclear). Even though
the use of standards and guidelines itself does not garantuee reliability, it helps
to implement techniques that are considered to be the basis of safe and reliable
products. Thus, the use of standards can be seen as a technique complementary
to more quantitative methods such as statistical testing (ST), which estimate
a reliability ﬁgure but do not directly scrutinize the process of design and development. Our aim is to work towards combining both routes of assessment:
standards/certiﬁcation and ST. Therefore, we set out to consider how the wide
range of tasks performed when following a standard can be transformed into a
quantitative ﬁgure measuring something like the foundation on which ST methods then start to operate. On the long-term, the following questions should be
answered: “Can we combine the evidence seen during certiﬁcation and during
J.-P. Rosen and A. Strohmeier (Eds.): Ada-Europe 2003, LNCS 2655, pp. 246–257, 2003.
c Springer-Verlag Berlin Heidelberg 2003

Evidential Volume Approach for Certiﬁcation

247

ST into one reliability measure?” “Does the groundwork performed in design
and development for any particular component look sound enough to justify
launching into the eﬀort of ST?” Due to the interests of the industry we work
with, we focus our considerations here on two standards used by this industry:
IEC 61508 [1] and the PES Guidelines [2]. However, it should be noted that
any other standard could be inserted throughout the paper, the methodology
proposed would remain the same.
The international functional safety standard IEC 61508 (Functional safety of
E/E/PE safety-related systems) aims at providing guidance on the development
and assessment of safe and reliable systems consisting of hardware, software
or a combination of both. It describes the overall safety lifecycle of a system or
component as consisting of 16 phases and provides generic guidance for all safety
lifecycle activities. The achievement of functional safety is supported through a
wide rande of measures, techniques and principles relating both to the process
and the product. IEC 61508 uses the concept of Safety Integrity Levels (SILs),
which are linked to upper bounds on the tolerated system probability of failure
on demand (pfd). Examples for claims on the pfd for diﬀerent SILs are:
SIL2 :

pf d ∈ [10−3 , 10−2 [,

SIL3 :

pf d ∈ [10−4 , 10−3 [.

(1)

Starting with a Hazard and Risk Analysis, the safety requirements are identiﬁed for each safety function and then later translated into SILs for components
implementing the function. For each SIL, the set of measures, techniques and
principles recommended in [1] changes, becoming more stringent for higher SILs.
Throughout this paper, we refer to the techniques/principles listed in [1] as requirements. Implementation of the applicable requirements does not guarantee
having developed a component of tolerable failure probability, but it shows that
all reasonable activities to prevent intolerably high failure probabilities have been
carried out. Thus we are reasonably conﬁdent in having achieved a component of
required safety integrity. This is based on the fact that the standard is a compilation of the best knowledge and understanding currently available from experts
in the ﬁeld. Thus the assumptions that following a standard supports a reliability claim is an expression of expert judgement. When certifying a system, expert
judgement comes in again: an assessor makes the decision whether or not to
accept a product based on the evidence he/she has seen. This decision is mostly
seen as a binary process: accept or reject. In the case where each requirement
has been demonstrably met, the decision for acceptance seems obvious. However,
in the case where not all expected evidence is available, maybe because it was
replaced by alternative evidence or a technique wasn’t documented or simply
not applied, the decision will be less clear. Depending on the type of information missing, the quality of alternative evidence provided, the overall picture of
evidence, the assessor will decide in favour of or against acceptance. It is such
cases that seem to potentially beneﬁt from decision-support because decisions
here are based on subjective indiviudal judgement and individual experiences
that cannot easily be replicated or compared between scenarios, thus allowing
for inconsistencies to arise. It is the aim of this paper to explore the possibility

248

S. Kuball and G. Hughes

of a quantitative measure expressing the “overall picture” of evidence available
with respect to a standard, a measure for the total volume of related evidence.
This measure should be repeatable and provide a high degree of consistency
between assessment scenarios. It could be used alongside expert judgement and
aid in deciding on acceptance or rejection of a system. In this paper we explore
a possible way of achieving such a quantitative decision-aid.

2

Background: Structure of Evidence

In Fig. 1, we have sketched part of the structure of evidence contained in IEC
61508. The output generated during one phase serves in most cases as input
to succeeding phases. The structure in Fig. 1 is laid out over 4 levels. At level
4 the requirements are listed. The remaining levels are used to indicate that
phases can consist of sub-phases, which again can consist of sub-phases. For
example, the Level 1 phase “Software Realisation” contains among others the
sub-phases “Safety Requirements Allocation” and “Design and Development”.
Level 2 phase “Design and Development” contains a series of sub-phases building
on each other such as “Software Architecture”, “Support Tools & Programming
Languages (PLs)”, “Code Implementation” etc. At the ﬁnal level a set of requirements is listed for each phase in the form of clauses and tables. For example:
requirements listed under “Support tools & PLs” and “Code Implementation”
are: “Programming language unambiguously deﬁned”, “No dynamic objects”,
“Limited use of pointers”. Please note that Fig. 1 does not attempt to replicate
the safety lifecycle as given in [1], it only shows an excerpt for illustrative pur-

Level 1

Level 2

Level 3

Level 4

Concept

Requirements:

Hazard & Risk
Analysis

Determine hazards and
hazardous events
Evaluate likelihoods of
hazardous events
Evaluate or estimate EUC risk
Etc.

Safety Requirements
Allocation

Software
Realisation

S/w safety req.
allocation
S/w safety
validation
planning
S/w safety
validation

S/w Design and
development

Architecture
Support tools & PLs
Module design
Code Implementation
Module tests etc

Requirements:
PL unambiguously def.
Coding Standards
No dynamic objects
Limited use of pointers
Etc.

Fig. 1. Excerpt from safety lifecycle as described in IEC 61508.

Evidential Volume Approach for Certiﬁcation

249

poses. The PES Guidelines contain a similar structure of phases and sub-phases
but are a smaller and more accessible document tailored to the needs of PES
in Nuclear Safety Applications. Examples of requirements listed here in subphase “Design for Safety” are: “Avoid use of interrupts, pointers, tasking. Safe
language subsets (e.g. SPARK Ada) help to reinforce these restrictions”, “Semantics of language should support Formal Proof”, “Use safe language subsets:
no dangerous constructs that may be ambiguous”.
Using a language that meets the requirements listed above thus seems to
support the creation of a safe and reliable system/component. If all languagesensitive objectives and guidance are satisﬁed then the language is seen as suitable for the purpose of a safety-critical system of given target integrity. But, how
does the particular evidence pertaining to coding and programming-languages
contribute to the overall system quality? How does the evidence collected during
assessment on both process and product add up to build conﬁdence in system
safety? In some way, the degree to which a sub-phase complies with the requirements listed will be indicative of the quality achieved for its parent phase, i.e.
the next level down in Fig. 1, and ﬁnally this will contribute to the overall compliance of the product with the safety claim. If important evidence is missing,
this will aﬀect the trust we have in the product reliability which is expressed
through the probability of failure on demand (pfd). For example, the use of C as
a programming language may aﬀect the degree to which conformance has been
achieved within the phases “Code Implementation (IEC 61508) or “Design for
Safety” (PES Guidelines). The use of SPARK Ada on the other hand lends itself
well to achieving conformance within these phases and to achieve this rather
eﬃciently. Depending on the relative importance of these phases in the context
of the overall safety lifecycle, this will more or less aﬀect our belief in having
achieved an end-product of suﬃcient reliability. Due to the hierarchical structure
of evidence in the safety lifecycle, a ﬂaw in one of the phases, e.g. lacking rigour
in chosing programming techniques will ﬁlter down and potentially undermine
success of the project, even if all the subsequent phases were carried out to high
standard and in compliance with the requirements. Even though all requirements
are important, there will most likely be some, the absence of which would inﬂuence an assessor’s conﬁdence in the product more severely than others. Hereby
absence could mean that a recommended technique was not documented or not
implemented and not repaced by a suitable alternative technique. Also, if a large
number of pieces of evidence are missing that would by themselves not aﬀect our
belief in the end-product, then the total mass of missing evidence may create
again a considerable plunge in conﬁdence leading to rejection of a product. It
seems beneﬁcial to formalize these concepts through the introduction of a quantitative measure to aid diﬀerent assessors to come to the same conclusion (accept
or reject) for the same set of evidence. In order to make a step towards such a
formalization, we introduce the following approach.

250

3

S. Kuball and G. Hughes

Evidential Volume Approach

Let Pj be used to indicate a phase in the overall safety-lifecycle. We assume
that for Pj a set of requirements R1j , ..., Rnj is listed. An example could be the
“Code Implementation” sub-phase in Fig. 1 and the set of requirements listed
for that sub-phase. An assessor observes whether a requirements has been met
or not met. Let I1j , ..., Inj be an indicator taking on the value 1 if the assessor
observes that Rij is met and 0 otherwise. One could also allow for a value between 0 and 1 being assigned to express the quality by which a requirement has
been met, however, at this stage we model the observation of an assessor as a
binary value. The set Iij , i = 1, ..., n describes the whole set of evidence present
at the end of the assessment. However, when the assessor makes his/her decision
on whether to accept or reject a system or component, not only the number of
requirements met will be taken into account, but the relative importance of the
evidence present and of the evidence missing has to be accounted for as well. We
model this by introducing a set of weights, w1j , ..., wnj attached to requirements
R1j , ..., Rnj . These weights describe a ranking of the importance of diﬀerent
requirements in the whole set of listed requirements with respect to achieving
success in phase Pj . Success could mean for example: “code implementation has
been performed in accordance with the recommendations in the standard”. We
n
assume the weighting factors to be normalized such that
i=1 wij = 1. The
technique that is considered most important would receive the highest weight, if
no preference can be given to any technique, all weights are chosen to be equal.
The weighting factors should be elicited through discussions with a panel of experts. This process would have to be undergone once for each SIL in order to ﬁx
weights for each requirement depending on what the total set of requirements
for that SIL is and depending on the level of recommendation the requirement
receives in IEC 61508 for that particular SIL. The weights thus identiﬁed could
then be used for every assessment scenario based on IEC 61508. From the assessor’s observations I1j , ..., Inj and the generic set of weights w1j , ..., wnj we
compile a measure for the degree of compliance of the product under assessment
with the principles of IEC 61508:
Deﬁnition 1. Let Pj be a sub-phase of IEC 61508 for which requirements
(R1j , ..., Rnj ) are listed. Let (I1j , ..., Inj ) be a set of indicator values assigned
by the assessor with Iij = 1 if Rij is met and 0 otherwise, i = 1, ..., n. Let
(w1j , ..., wnj ) be weights describing the relative importance of R1j , ..., Rnj in
achieving success in phase P j. Then,
n

wij · Iij .

EVj := fj (Iij , wij , i = 1, ..., n) =

(2)

i=1

is called evidential volume for Pj .
Generally fj is an aggregation function, which can adopt diﬀerent forms. At the
level of aggregating single requirements, we have chosen the additive form. In the
next section we will brainstorm some possibilities of assigning weighting factors
to each evidence unit and again comment on the choice of aggregation functions.

Evidential Volume Approach for Certiﬁcation

3.1

251

Assigning Weighting Factors

Weighting factors represent a ranking between requirements. They should be
assigned by a panel of international experts for each SIL or equivalent integrity
target. There are diﬀerent ways of assigning such weights. If no preference can
be given to any phase or requirement, then the weights pertaining to one level
could be all chosen equal. Weights can be identiﬁed through pairwise comparisons. Within the phase “Code Implementation” “Use of coding standard” could
be considered as twice as important as “Limited Use of Pointers” due to a level of
recommendation identiﬁed in the standard. Another requirement “No Dynamic
Objects” could be for example equally weighted as “Use of coding standard” and
if they represented a full set of recommended techniques, the assumption that
they should add up to 1 would be enough to calculate the weights. Weights could
also be thought of initially as qualitative statements. Discussion of this work with
our industrial sponsors led to identiﬁcation of three preliminary ranking-levels for
requirements: “Important”, “Less Important” and “Very Important” depending
on the safety integrity target(s) for which the requirements were listed. Deﬁning
relations between the rankings (e.g. “Important=2*Less Important”, “Very Important=3*Less Important”) can then lead to an identiﬁcation of weights. These
are however initial ideas that have not been validated yet and require more input
from experts in order to devise a suitable way of allocating weights.
A further possiblility to allocate weights is to use an approach introduced
by Li et al (2000) [3]. Here Multi Attribute Utility Theory is used for the ranking of software engineering measures with respect to their ability to contribute
to reliability. This approach seems very suitable to be used in the context of
ranking requirements within IEC 61508 for their importance when it comes to
generating a reliable product. In [3] for a set of software engineering measures a
score is elicited from experts for a set of attributes. Example measures are error
distribution and failure rate. Examples of chosen attributes that were chosen
are credibility, experience, repeatability, validation, cost, beneﬁt and relevance
to reliability. Measures are rated with respect to an attribute on a scale from 0
to 5 or 6, the high scores representing the highest relevance. The scores given by
the experts are compiled into a single number using a linear additive aggregation
equation with equal weights. In the context of IEC 61508 the same or similar attributes could be used to assess the requirements listed. Each requirement could
be rated according to the set of attributes and the total score achieved from a
panel of experts would then point towards an appropriate weighting factor.
3.2

Aggregation Functions

The choice of aggregation functions depends on the structure of phases and the
relationship between phases. There are several levels of aggregation needed. First
of all on the sub-phase level we need to aggregate observations on the single requirements. Requirements listed for one sub-phase consist of diﬀerent techniques
that are all perceived to contribute to successful implementation of the same aspect of the safety lifecycle. For example in the sub-phase code implementation,

252

S. Kuball and G. Hughes

the techniques such as no dynamic objects, use of coding standards, limited
use of pointers etc. provide a diﬀerent component of a reasonable programming
regime. Thus these techniques contribute all to some degree to the successful
implementation of that particular sub-phase. If one is missing, still some good
practice has been followed and thus a certain quality of programming achieved.
For this level of aggregation, we have chosen the linear aggregation function of
Eq. (1). We choose the linear function here because the set of requirements can
be basically seen as separate evidence contributing to some degree to the same
measure, e.g. quality of code implementation. After having established the evidential volumes for each phase, we need to aggregate the evidential volumes
of the phases into one overall evidential volume. In the overall safety lifecycle,
phases heavily depend on each other: the input from one phase serves as output
to subsequent phases. If one phase is completely missing, and is not replaced or
compensated for by an alternative technique1 the entire process is invalid, if one
phase is ﬂawed this cannot be compensated for by other phases. The same relationship holds for example between the sub-phases of “Software Realisation”.
This should be captured through the aggregation function used to combine the
EVs of diﬀerent phases into one EV. Aggregation functions should be chosen to
comply with experts’ belief. To achieve this, a set of axioms could be formulated
reﬂecting the experts’ beliefs. Possible axioms that seem reasonable are given
below, see also [4]
i)
ii)
iii)
iv)
va)
vb)

The calculation of EV is uniquely determined.
EV shall be not larger than the largest value EVj and not smaller than the
smallest value EVj .
Small improvements in EVj , j ∈ {1, ..., m} do not make EV smaller.
EV shall take on its highest possible value (1) only when all EVj take on
the highest possible value.
EV shall take on the smallest possible value (e.g. 0) only if all EVj take on
the smallest possible value OR
EV shall take on the smallest possible value if at least one EVj takes on the
smallest possible value.

Axiom vb) constitutes a “K.O.”-criterion, which means that the total EV is
0 already when one Iij = 0, i.e. when one EVj is missing. This means that if
evidence for one phase is completely missing, this cannot be compensated for by
another phase having been performed exceptionally well. Such a criterion could
apply to certain levels of aggregation, as an example, in Fig. 1, it could apply
to levels 1 and 3 of the depicted evidence structure. A possibility of modelling
such a criterion is via a multiplicative aggregation function.
m

EV =
j=1
1

α

EVj j .

(3)

Hereby, replacement of a technique with an alternative method should be performed
according to the principles acceptable in the context of the standard.

Evidential Volume Approach for Certiﬁcation

253

Hereby αj , j = 1, ..., m are again weighting factors chosen to fulﬁl a condition
m

such as
j=1

αj = 1. They express the importance of phase Pj in the entire process,

or the severity of loss in conﬁdence when putting no demonstrable eﬀort into
that phase. When using the function in Eq. (3), a change in the factor with the
largest αj will be noticed more than a change in a factor with a smaller αj . For
the factor with largest αj , the larger the change, the higher the impact on the
overall evidential volume. Other aggregation functions are possible such as
EV =

min (EVj )αj .

j=1,...,m

Note that the exact choice of aggregation function models our prior belief about
how the quality of the overall product is inﬂuenced by the quality of sub-phases.
Therefore, the aggregation functions too need to be checked with and agreed
upon by experts. Depending on the “score” obtained in a speciﬁc sub-phase, the
place the sub-phase takes in the overall safety lifecycle and the weight assigned
to it, the outcome of a sub-phase will inﬂuence the overall evidential volume.
This can help decide what the beneﬁt of a speciﬁc technique (e.g. SPARK Ada)
is in the context of the overall evidence gathered. It can also help to compare
techniques by investigating the eﬀort needed to obtain a certain EV with diﬀerent
techniques, i.e. how eﬃcient are they in achieving a high EV. For example, the
application of a technique such as SPARK Ada lends itself well to fulﬁllment
of the requirements of sub-phase “Code Implementation”. At the same time, it
supports static analysis [5] and thus the sub-phase “Software Veriﬁcation”, thus
may contribute to obtaining a high EV in two phases.

4

Use of Evidential Volume in Decision-Making

The weighting factors in the evidential volume approach should be generically
speciﬁed through consensus of a suitable team of experts. For a given SIL, they
should be a ﬁxed set of numbers. Thus, once they are speciﬁed, the outcome
of an assessment as measured by the evidential volume is repeatable and can
be compared to the outcome of other assessment scenarios. For all products for
which the same EV is achieved, the same decision outcome should be made:
accept or reject.
Potentially, the evidential volume can be used to formulate a prior belief on
the reliability of the product under assessment, which could form the basis of
deciding whether or not to perform further more quantitative assessment such
as statistical testing. Such a prior belief could be for example expressed in the
form of a probability of success or a conﬁdence in having achieved the required
integrity target. A possible approach to link the EV to a success probability,
is based on the success likelihood index model (SLIM) [6]. In SLIM, a success
n

likelihood index S is calculated via S =
i=1

I[P SFi ] · wi . Hereby, P SFi are

“performance shaping factors” contributing to the task at hand. wi describe

254

S. Kuball and G. Hughes
n

the relative importance of P SFi for the task, hereby

i=1

wi = 1. I[P SFi ] rate

the quality of P SFi in the current context. The EV is a similar compilation of
weights and ratings for a set of factors, all inﬂuencing the quality of the ﬁnal
(human) task of producing a system of suitable reliability. Thus, we interpret the
EV as a success likelihood index. The use of SLIM in the context of reliability
estimation is also mentioned in [7]. Using SLIM, the following relation can be
established between EV and a probability of success P success [6].
Log(P success ) = a · EV + b.

(4)

P success can be deﬁned as the probability of having successfully met the claimed
integrity target, i.e. P success := P r(pf d < Integrity Target). The parameters a
and b in (4) are determined through the boundary conditions
EV = 0 ⇒ P success = PLower , EV = 1 ⇒ P success = PU pper ,
Log(PU pper ) = a · 1 + b.
Log(PLower ) = a · 0 + b,

(5)

SLIM is originally used to calculate human error probabilities. The task of developing a software product from a given speciﬁcation to a certain level of reliability
is a human task, thus use of this model seems suitable. It has to be noted that
even though SLIM is based on experimental data and theoretical considerations
[6], it is to be seen as a model compiling expert judgement and thus prior belief
rather being derived from statistical inference. Thus it is important to check the
message the model conveys against expert beliefs for example for a series of example scenarios. In (5), for two extreme cases such as EV=0 or 1 one formulates
a prior belief about P success . This could take on the following form. We consider
the example of aiming at a SIL 2 system, i.e. pf d < 0.001, see (1). In the case of
EV=1, we have encountered the “ideal” assessment scenario were all evidence is
demonstrably met. In this case, because the standard comprises the consensus
on what is considered to be sound development practice, and because compliance with the standard is associated with complying with a claimed integrity
level, one could model PU pper as for example 0.99. This would be the same level
of conﬁdence that one would aim at with statistical testing. Other values for
PU pper are possible, the importance is on checking it with expert belief. The
value PLower could be modelled as follows. In the absence of all evidence, we
could assume that we do not know anything about the system’s pfd. This can
be modelled through a uniform prior distribution over the pfd, which results
in P r(pf d < 0.001) = 10−3 . Thus a possible choice is PLower = 10−3 . The
assumption PLower = 0 is not suitable due to the Log(PLower ) in Eq. (5).
Using (4) and (5), one can then calculate P success for EV ∈]0, 1[. From this relationship one can potentially determine the utility or risk of accepting a system,
and a cut-oﬀ point, below which a system should be rejected. In the following
we shall perform some example calculations. We use the examples of having a
a) SIL2 target and b) SIL3 target. We deﬁne boundary conditions as described
above, and calculate success probabilities for diﬀerent EVs.

Evidential Volume Approach for Certiﬁcation

255

Example: We deﬁne P success := P r(pf d < T SIL ). Hereby T SIL is the safety
integrity target for the given SIL, which can be interpreted as the lower bound
of the pfd interval in (1), see also [1], Part 1, Table 2. We assume the following
boundary conditions:
1) SIL 2. P success := P r(pf d < 0.001). PLower = 0.001, PU pper = 0.99.
2) SIL 3. P success := P r(pf d < 0.0001). PLower = 0.0001, PU pper = 0.99.
We obtain the following results:
EV P success
0.95 0.7
SIL 2:
0.9 0.497
0.8 0.249

EV P success
0.95 0.624
SIL 3:
0.9 0.394
0.8 0.15

Using SLIM with the above boundary conditions yields for SILs 2 and 3
a relatively small estimated success probability already when 5 − 10% of the
total evidential volume is missing. This may be considered as too pessimistic by
experts, in which case the model needs to be revised to capture these beliefs.
However, 5% of the total evidential volume missing may be considered as a case
where a considerable decrease in conﬁdence in the product from assessment data
alone is justiﬁed. In this case, it could be investigated whether a higher level of
conﬁdence can be achieved by adding evidence such as statistical testing. In
order to investigate what can cause a loss of 5 − 10% of evidential volume, we
have considered an example scenario. We assume a series of 14 phases Pj , each
containing a list of nj requirements. The evidential volume of Pj is calculated
nj

as EVj =

s=1

ws · Is . In this example, we assume that the weights ws , s =

1, ..., nj within each phase are equal. The total evidential volume is calculated as
14

α

EV =
j=1

EVj j . For the weights α1 , ..., α14 , we establish a set of comparative

equations.
α1 = 1 −

14
j=1

αj ; α1 = 3 · α3 ; α3 = 3 · α2 ; α4 = 0.5 · α3 ;

α5 = α6 = α7 = ... = α10 = α4 ; α11 = 2 · α4 ; α12 = ... = α14 = α11 .
From these conditions, the following set of weights are calculated:
α1 =

18
2
6
3
6
, α2 =
, α3 =
, α4 =
= . . . = α10 , α11 =
= . . . = α14 .
71
71
71
71
71

For the numbers nj of requirements within P1 , ..., P14 , we have assumed:
n1 = 11, n2 = 5, n3 = 12, n4 = 3, n5 = 6, n6 = 5, n7 = 2, n8 = 4, n9 = 5,
n10 = 8, n11 = 10, n12 = 8, n13 = 13, n14 = 3.
This gives us a total of 95 requirements. We have now assumed the following
scenarios:

256

S. Kuball and G. Hughes

1) 3 requirements are missing in P1 (large weight) and 3 requirements are missing in P13 . Both P1 and P13 possess large weights. The resulting total evidential
volume is EV = 0.91, this yields a success probability of ca. 40% if we are
performing SIL 3 assessment, and 50% if we were assessing for SIL 2. Approximately 6% of all requirements are missing, mostly in important phases, and the
resulting EV has decreased by ca. 10%.
2) A total of 12 requirements is missing, but P1 is complete. The missing requirements are distributed over 9 phases. This lead to a total evidential volume
of 89%, thus not a large change in comparison to case 1), however double the
amount of requirements are missing.
Thus, in the compilation of the evidential volume, the number of missing
requirements, their weights and the weights of the phases they are associated
with within the overall lifecyle is of importance. The model introduced here
describes the belief that missing a small amount of important evidence may
already lead to a considerable decrease in conﬁdence in the product, which then
has to be dealt with by revising the development process, generating additional
evidence, such as statistical testing, or else rejecting the product. Linking the
evidential volume to a success probability is useful because it can serve as the
basis of calculating the utility of a decision, for example by multiplying the risk
involved when accepting a product that is not safe enough with (1-P success ) and
similarly multiplying the risk (or cost) of rejecting a safe product unnecessarily
with P success and comparing the ﬁgures. The success probability can also form
the basis of building a prior belief function for statistical testing. This could
be used to predict the probability of the system under test to pass statistical
reliability testing.

5

Outlook

The intention of this paper is to introduce the EVA to the reliability community
for discussion and feedback. The model introduced here, needs to be discussed
with experts in order to be validated as a suitable model for expert belief. This
requires iterative calculations of example scenarios and this should be helped by
tool-support. We have started to develop a simple support-tool that contains the
requirements listed in the PES Guidelines and a set of preliminary weights. The
tool takes as inputs the decision whether any particular requirement is “met”,
“not applicable” or “missing”. If the requirement is rated as “met”, a set of
security questions pops up to determine whether the evidence has been directly
met or through some allowed alternative route. If not all relevant questions can
be answered with “yes” but still “requirement met” is ticked, a warning message
will be issued. The tool calculates the EV for all main phases as well as the
overall EV. The purpose of this simple prototype tool is to distribute it to a
set of experts as a basis of discussion of the approach and to demonstrate how
the approach works. This will help identify the strengths and limitations and
provide useful feedback on how to take this work further. Working with a tool
can also be useful when discussing the weights in the approach. Example EV’s for

Evidential Volume Approach for Certiﬁcation

257

known assessment scenarios can be calculated and the results compared with the
actual assessment decision taken. This can shed some light on suitable choices
of weights. On the longer term, a tool for the calculation of an EV - based
on a set of weights assigned by a panel of experts from the relevant industry,
academia and government institutions- might also be beneﬁcial to developers of
safety-critical components to assess the degree of potential compliance achieved
with their product at any stage of the development cycle in a user-friendly way.
To propose how to setup a panel of experts is beyond the scope of the project
performed here. This could potentially be part of a future project. No application
to real cases has been performed yet, but hopefully at some stage we will be able
to perform example calculations on real assessment scenarios and examine how
well the results obtained with the EVA ﬁt the original assessment decisions. The
ﬁrst step still to be performed is however to arrive at a useable set of weights.

6

Conclusions

In this paper we have described a model to capture the degree of compliance
of a product with an industrial standard such as IEC 61508. This ﬁgure, the
evidential volume can by itself be used as an indicator to compare diﬀerent
assessment scenarios. This could form the basis for improved consistency in
assessment. A model was suggested for relating the evidential volume to the
probability of having achieved a product of required safety integrity. Developing
such a relationship can lead to a decision-aid on acceptance or rejection or can be
used to determine the amount of additional evidence, such as statistical testing,
required to help with demonstrating successful product development. The model
described here is based on SLIM, it poses an initial step towards decision-support
for assessment and poses the basis on which experts can start formulating their
prior belief arising from assessment quality.

References
[1] “Functional safety of electrical/electronic/programmable electronic safety-related
systems, International Electrotechnical Commission,” 1998.
[2] L. Winsborrow and A. Lawrence, “Guidelines for using programmable electronic
systems in nuclear safety and nuclear safety-related applications,” British Energy
Generations Ltd., 2001.
[3] M. Li, C. Smidts, and R. W. Brill, “Ranking software engineering measures related
to reliability using expert opinion,” Proceedings of ISSRE 2000, 2000.
[4] H. Dehlinger, “Deontische Fragen, Urteilsbildung, Bewertungssysteme,” Arbeitsbericht aus dem Fachgebiet Design Theorien and Methoden, vol. 7/94.
[5] R. Chapman, “Industrial experience with SPARK,” Proceedings of ACM SigAda
2000, pp. 64–68.
[6] G. Salvendy, Handbook of Human Factors and Ergonomics. Wiley Interscience,
1997.
[7] C. Smidts and D. Sova, “An architectural model for software reliability quantiﬁcation: sources of data,” Reliability Engineering and System Safety, vol. 64, pp. 279–
290, 1999.

