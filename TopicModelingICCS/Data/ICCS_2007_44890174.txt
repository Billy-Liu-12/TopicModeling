Boundary Processing of HHT Using Support Vector
Regression Machines
Wu Wang, Xueyao Li, and Rubo Zhang
National Laboratory on Machine Perception, Peking University, Beijing, P.R. China
College of Computer Science and Technology, Harbin Engineering University,
Harbin150001,China
wangwu131@163.com
Abstract. In order to better restrain the end effects in Hilbert-Huang Transform,
support vector regression machines (SVRM), which have the superiority in the
time series prediction, are adopted to extend the data at the both ends. In the
application of SVRM, the parameters have a great influence on the performance
of generalization. In this paper the influence of parameters is discussed, and
then an adaptive support vector regression machine is proposed based on the
particle swarm optimization (PSO) algorithm. With the parameters optimized
by PSO, SVRM can be characterized as self-adaptive and high generalization
performance in applications. Experiments show that this method can solve the
problem of selecting parameters properly. Contrast to the neural networks
methods and HHTDPS designed by Huang et al., end effects can be restrained
better and the Intrinsic Mode Functions have less distortion.
Keywords: End effects, Hilbert-Huang Transform, Support vector regression
machines, Particle swarm optimization.

1 Introduction
Hilbert-Huang Transform (HHT) is a powerful data analysis method for nonlinear and
non-stationary data[1][2], which is based on empirical mode decomposition (EMD).
Although Hilbert-Huang transform is a powerful analysis method for non-stationary
signals, but end effects are its unavoidable problem[1][4][5]. If the end effects aren’t
restrained effectively, it can produce large swings and eventually propagate inward and
corrupt the whole data[1]. So it is necessary to restrain the end effects effectively. Many
new methods have proposed in recent years, such as time series prediction based on
neural networks[3], mirror extending method[4], waveform matching method[5],
constructing two periodic series from the original data by even and odd extension[6].
Among these methods, the neural networks have relatively better effects than other
methods[7]. The neural networks extending method can work well for non-stationary
signals[3], but the local minimum point, over learning and the excessive dependence on
experience about the choice of structures and types are its inevitable limitation. A
potential solution to the above problems is to use Support vector regression machines
(SVRM). In applications, the performance of SVRM is sensitive to its parameters[8], so
in this paper the particle swarm optimization (PSO) algorithm is proposed to optimize
the parameters. More importantly, with the parameters optimized by PSO, this method
can be self-adaptive to nonlinear and non-stationary data.
Y. Shi et al. (Eds.): ICCS 2007, Part III, LNCS 4489, pp. 174–177, 2007.
© Springer-Verlag Berlin Heidelberg 2007

Boundary Processing of HHT Using Support Vector Regression Machines

175

2 Parameter Selection in SVRM Based on PSO
In the application of Support vector regression machines, there are three important
parameters: the regularization parameter C, the tube size ε and the kernel

parameter σ . All of them have a great influence on the generalization performance
of the SVRM. In this paper, PSO is proposed to optimize these parameters. Compared
to other parameter selection methods, this approach is simple but faster[9]. In PSO,
the potential solutions, called particles, fly through the problem space with a velocity,
which is dynamically adjusted according to its own flying experience and its
companions’ flying experience. Every particle is evaluated by the fitness value. The
best previous position of the particle is recorded and represented as pbest. The
position of the best particle among all the particles in the population is represented by
the symbol gbest. According to the pbest and gbest, the particle updates its velocity
and position as the following equation:
2

vid = w* vid + c1 * rand () * ( pbestid − xid ) + c2 * Rand () * ( gbest - xid )
xid = xid + vid , i = 1, 2, " M

(1)

where d is the dimension of every particle, which is equivalent to the number of
parameters to optimize. Rand ( ) and the Rand ( ) are two random numbers generated
independently. w is inertia weight, c1 and c2 are learning factors and the M is the
population size.
To get better generalization ability, it is necessary to make the SVRM model have
lower error rate and a simple structure. Therefore, the fitness function can be defined
as follows:

Fi = ferror + K ×

Nsv
× (1 − ferror )
N

(2)

where f error is the error rate in the training data sets, N sv is the number of the support
vectors(SV) and N is the training data number. K ∈ [0,1] is used to make a balance
between the precision and the complexity.

3 Implementation and Experimental Results
In order to make a comparison between the SVRM and RBF network in forecasting
effect, one speech segment of 731 data points and the SNR=5dB was picked as the
training samples, which is shown in Fig.1. The data were forecast 150 data points from
the back endpoint purposely, but in applications, we only need to forecast very less
points. The results by SVRM and RBF neural network are shown in Fig. 2 and Fig. 3,
where the dashed line represents the signal forecast by SVRM or RBF network and the
solid line represents the actual signal. Figures show the forecasted series by RBF network
have a more serious distortion. The mean absolute percentage error (MAPE) of the
SVRM and RBF network were 1.1012 and 1.3470. The result also demonstrates that
the forecasting method based on SVRM is superior to that based on neural network.

176

W. Wang, X. Li, and R. Zhang

Fig. 1. The noised speech signal with SNR=5dB

Fig. 2. The noised speech signal after the SVRM predicting

Fig. 3. The noised speech signal after the RBF neural network predicting

(a)

(b)

Fig. 4. The EMD decomposition results of the speech signal after being forecast by the SVRM
(a) and the HHTDPS (b)

Boundary Processing of HHT Using Support Vector Regression Machines

177

We also conducted our experiments on a speech material of 0.7608s long. Fig.4
shows the IMFs results extracted form our method and decomposed by the HHT Data
Processing System (HHTDPS). The HHTDPS is an engineering spectral analysis tool
developed at Goddard Space Flight Center (GSFC) of NASA. The HHTDPS used is
Version 1.4, which is the latest version of HHT software.
Fig. 4 shows the IMF components after forecasting by SVR and got by HHTDPS
respectively. From the Fig.4 (b) we can see the fourth and sixth IMF components in
HHTDPS have an explicit distortion at the starting point, while this distortion can be
restrained effectively by our algorithm, shown in Fig.4(a).

4 Conclusion
In this paper, we proposed a method to restrain the end effects in HHT by time series
prediction based on adaptive support vector regression machine. For the end effects in
Hilbert–Huang transform, the original time series can be forecast by SVRM, and the
end effects can be released by abandoning the extended data. With the parameters
optimized by PSO, SVRM can be self-adaptive to nonlinear and non-stationary data. It
is helpful for SVRM to get high generalization performance in time series prediction.
Therefore, SVRM is a very suitable forecasting method for the time series. Our
experiments show end effects in HHT can be restrained effectively by this method.
Acknowledgments. This work is supported by the NSFC No. 60475016 and the
Foundational Research Fund of Harbin Engineering University HEUF04092. We also
appreciate NASA Goddard to approve us to use and evaluate HHTDPS software.

References
1. Norden E. Huang .The Empirical Mode Decomposition and the Hilbert spectrum for nonlinear
and non-stationary time series analysis J. proc.R.Soc. Lond.A (1998) 454,903-995.
2. Norden E.Huang. A confidence limit for the Empirical Mode Decomposition and Hilbert
spectral analysis; J.proc.R.Soc.Lond 2003(459), 2317-2345.
3. Y.J. Deng, W. Wang, Boundary processing technique in EMD method and Hilbert
transform, Chinese Science Bulletin 46 (11) (2001) 257–263.
4. J.P. Zhao， D.J. Huang, Mirror extending and circular spline function for empirical mode
decomposition method, Journal of Zhejiang University(Science),2001, 2(3) 247-252
5. Q. Gai, X.J. Mao, H.Y. Zhang, et al., New method for processing end effect in local wave
method, Journal of Dalin University of Technology, Chinese 42 (1) (2002) 115–117.
6. K. Zeng and M.X. He, A Simple Boundary Process Technique for Empirical Mode
Decomposition, IGARSS '04. Proceedings, 6(2004) 4258 – 4261.
7. B.J. Xu, J.M. Zhang, X.L. Xu, J.W. Li, A study on the method of restraining the ending
effect of empirical mode decomposition, Transactions of Beijing Institute of Technology,
26(3)(2006) 196-200.
8. C. C. Chuang, S. F. Su, J. T. Jeng, and C. C. Hsiao, Robust support vector regression
networks for function approximation with outliers, IEEE Trans. Neural Netw., vol. 13, no.
6, pp. 1322–1330, Nov. 2002.
9. Kennedy, I. and Eberhan, R. C. Paticle swarm optimization. Proceedings of IEEE
lnlemational Conference on Neural Networks. Piscalaway, NJ. pp. 1942-1948, 1995.

