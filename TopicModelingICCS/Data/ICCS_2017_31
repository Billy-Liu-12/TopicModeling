Available online at www.sciencedirect.com

ScienceDirect
ProcediaisComputer
Science
108CProcedia
(2017) 1463–1472
This space
reserved
for the
header, do not use it

This space is reserved for the Procedia header, do not use it

International Conference on Computational Science, ICCS 2017, 12-14 June 2017,
Zurich, Switzerland

Accelerating Parallel Multicriterial Optimization Methods
Accelerating
Multicriterial
Optimization
Methods
Based onParallel
Intensive
Using of Search
Information
Based on Intensive Using of Search Information
Gergel V.P. and Kozinov E.A.
GergelState
V.P.University
and Kozinov
E.A.
Lobachevsky
of Nizhni
Novgorod
Nizhni Novgorod, Russia
Lobachevsky State University of Nizhni Novgorod
gergel@unn.ru, evgeny.kozinov@itmm.unn.ru
Nizhni Novgorod, Russia
gergel@unn.ru, evgeny.kozinov@itmm.unn.ru

Abstract
In the present paper, an efficient parallel method for solving complex multicriterial optimization
Abstract
problems, which the optimality criteria can be multiextremal, and the computing of the criteria
In the present paper, an efficient parallel method for solving complex multicriterial optimization
values can require a large amount of computations in, is proposed. The proposed approach is
problems, which the optimality criteria can be multiextremal, and the computing of the criteria
based on the reduction of the multicriterial problems to the global optimization ones using the
values can require a large amount of computations in, is proposed. The proposed approach is
minimax convolution of the partial criteria, the dimensionality reduction with the use of the
based on the reduction of the multicriterial problems to the global optimization ones using the
Peano space-filling curves, and the application of the efficient parallel information-statistical
minimax convolution of the partial criteria, the dimensionality reduction with the use of the
global optimization methods. The intensive use of the search information obtained in the
Peano space-filling curves, and the application of the efficient parallel information-statistical
course of computations is provided when conducting the computations. The results of the
global optimization methods. The intensive use of the search information obtained in the
computational experiments demonstrated such an approach to allow reducing the computation
course of computations is provided when conducting the computations. The results of the
costs of solving the multicriterial optimization problems essentially – tens and hundreds times.
computational experiments demonstrated such an approach to allow reducing the computation
costs
solving
thePublished
multicriterial
optimization
problems essentially – tens and hundreds times.
©
2017ofThe
Authors.
by Elsevier
B.V.

Peer-review
responsibility
the scientific committee
of theparallel
International
Conference
on Computational
Science
Keywords: under
decision
making, of
multicriterial
optimization,
computing,
dimensionality
reduction,
criteria convolution, global optimization algorithms, computational complexity
Keywords: decision making, multicriterial optimization, parallel computing, dimensionality reduction,
criteria convolution, global optimization algorithms, computational complexity

1 Introduction
1
Introduction
The multicriterial
optimization (MCO) problem statements are used widely for modeling the
complex decision making problems – see, for example, the monographs [2, 3, 14, 15] and the
The multicriterial optimization (MCO) problem statements are used widely for modeling the
reviews of the research and application results [4, 6, 12, 13]. Solving the MCO problems it
complex decision making problems – see, for example, the monographs [2, 3, 14, 15] and the
should be taken into account that the partial optimality criteria are usually contradictory.
reviews of the research and application results [4, 6, 12, 13]. Solving the MCO problems it
As a result, solving the MCO problems is reduced to finding some compromised solutions,
should be taken into account that the partial optimality criteria are usually contradictory.
which the achievement of the best values with respect to the partial criteria is coordinated (the
As a result, solving the MCO problems is reduced to finding some compromised solutions,
compromised solutions, which cannot be improved with respect to all partial criteria are usually
which the achievement of the best values with respect to the partial criteria is coordinated (the
called nondominated, efficient or Pareto-optimal solutions). At that, the understanding of an
compromised solutions, which cannot be improved with respect to all partial criteria are usually
desired compromise may change in the course of computations that leads to the necessity to
called nondominated, efficient or Pareto-optimal solutions). At that, the understanding of an
find several different efficient solutions – in the limiting case, the whole set of the nondominated
desired compromise may change in the course of computations that leads to the necessity to
solutions (the Pareto domain).
find several different efficient solutions – in the limiting case, the whole set of the nondominated
solutions (the Pareto domain).
1
1877-0509 © 2017 The Authors. Published by Elsevier B.V.
Peer-review under responsibility of the scientific committee of the International Conference on Computational Science
10.1016/j.procs.2017.05.051

1

1464	

Accelerating Parallel Multicriterial
Optimization
Methods
Gergel and Kozinov
Gergel
V.P. et al. / Procedia
Computer Science 108C (2017) 1463–1472

The necessity to find several efficient solutions (or the whole Pareto domain) increases
the computational complexity essentially. One must take into account also that the partial
criteria can have a complex multiextremal form, and computing the criteria values can appear
to be computation costly. In such conditions, finding even a single compromised solution
requires considerable computations whereas computing several efficient solutions becomes a
computational problem of the ultimate level of computational complexity. Overcoming this
problem becomes possible with the use of huge computational capabilities of the supercomputing
systems. And, besides, to provide the efficient computations, a full utilization of the search
information obtained in the course of computations is necessary.
Further structure of the paper is as follows. In Section 2, the multicriterial optimization
problem statement is given. In Section 3, the basics of the developed approach are presented:
the reduction of the multicriterial problems to the global optimization ones by means of the
minimax convolution of the partial criteria and the dimensionality reduction with the use of the
Peano space-filling curves. In Section 4, a parallel multicriterial global optimization algorithm
utilizing the search information accumulated in the course of computations is presented. Section
5 involves the results of the numerical experiments. In Conclusions, the obtained results are
discussed, and main directions of further investigations are outlined.

2

Multicriterial Optimization Problem Statement

A multicriterial (multi-objective) optimization (MCO) problem can be defined as follows:
f (y) = (f1 (y), f2 (y), . . . , fs (y)) → min, y ∈ D,

(1)

where y = (y1 , y2 , . . . , yN ) is the vector of varied parameters, N is the dimensionality of the
multicriterial optimization problem being solved and D is the search domain representing a
N -dimensional hyperparallelepiped
D = {y ∈ RN : ai ≤ yi ≤ bi , 1 ≤ i ≤ N }
with given boundary vectors a and b. Without loss of the generality, the partial criteria values
in the problem (1) are supposed to be non-negative, and the decrease of these ones corresponds
to the increase in the efficiency of the considered solutions y ∈ D.
As a solution of the MCO problem, any efficient solution (a partial solution) can be considered. In the general case, when solving a MCO problem, finding the whole set of nondominated
solutions (the Pareto domain) P D(f, D) (a full solution of the MCO problem) may be required.
Within the present paper, the MCO problems will be considered in application to the
most complex decision making problems, which the partial criteria fi (y), 1 ≤ i ≤ s can be
multiextremal, and obtaining the criteria values at the points of the search domain y ∈ D can
require a considerable amount of computations in. Let us suppose also the partial criteria fi (y),
1 ≤ i ≤ s to satisfy the Lipschitz condition
|fi (y  ) − fi (y  )| ≤ Li y  − y  , y  , y  ∈ D, 1 ≤ i ≤ s.

3

(2)

Reduction Schemes for Multicriterial Problems

The proposed approach consists in the successive reduction of solving the MCO problems to
solving a series of simpler optimization problems. In Subsection 3.1, a method of the scalarization of the partial criteria, which reduces the MCO problem to a set of the global optimization
2

	

Accelerating Parallel Multicriterial
Optimization
Methods
Gergel and Kozinov
Gergel
V.P. et al. / Procedia
Computer Science 108C (2017) 1463–1472

ones is presented. In Subsection 3.2, the dimensionality reduction with the use of the Peano
space-filling curves that allows reducing the multidimensional optimization problems to the
reduced one-dimensional global optimization ones is considered.
One can note the possibility to use a great number of efficient and widely used optimization
methods for solving the MCO problems as an advantage of such an approach.

3.1

Reducing the Multicriterial Problems to the Scalar Optimization
Problems

A widely used approach for solving the MCO problems is the scalarization approach, which
some methods of convolution of the set of partial criteria fi (y), 1 ≤ i ≤ s into a integrated
scalar criterion F (λ, y) are applied – see, for example, [3, 4]. In the present paper, it is proposed
to use a minimax convolution of the partial criteria in the form:
F (λ, y) = max (λi fi (y)),
1≤i≤s

(3)

at given values of coefficients λi , 1 ≤ i ≤ s, which the condition should be satisfied for
s

λ∈Λ⊂R :

s

i=1

λi = 1, λi ≥ 0, 1 ≤ i ≤ s.

The coefficients λi , 1 ≤ i ≤ s be understood as the indices of importance of the partial
criteria – the higher the value of the coefficient λi for any particular partial criterion, the
higher the contribution of this partial criterion into the integrated scalar criterion F (λ, y).
It should be stressed that solving a MCO problem may require solving several problems (3)
when changing the understanding of the importance of the partial criteria. The possibility to
determine several efficient variants (or the whole Pareto domain) at reasonable computation
costs becomes a key challenge in solving the complex multicriterial optimization problems.

3.2

Reduction of the Multicriterial Problems to the One-dimensional
Global Optimization Problems

In the general case, finding the numerical estimates of the globally-optimal extrema implies
constructing a coverage of the search domain D – see, for example, [5, 7, 16, 18, 19, 20]. This
fact complicates the computational schemes of the global optimization methods considerably
since it implies a complex computational analysis of a large amount of multidimensional search
information. As a result, many optimization algorithms utilize some methods of dimensionality
reduction [11, 16, 17, 18, 19].
Within the frames of the information-statistical approach, the Peano curves or evolvents
y(x) mapping the interval [0, 1] onto a N -dimensional hypercube D unambiguously are used for
the dimensionality reduction – see, for example, [17, 18, 19]. As a result of such reduction, the
initial multidimensional global optimization problem (3) is reduced to a one-dimensional one:
F (λ, y(x∗ )) = min F (λ, y(x)).
x∈[0,1]

(4)

The considered dimensionality reduction scheme reduces a multidimensional problem with
the Lipschitzian minimized function to a one-dimensional problem, which the corresponding
functions satisfy the uniform Hölder condition (see [18, 19]) i. e.
|F (λ, y(x )) − F (λ, y(x ))| ≤ H|x − x |1/N , x , x ∈ [0, 1],

(5)
3

1465

Gergel V.P. et al. / Procedia Computer Science 108C (2017) 1463–1472

1466	

Accelerating Parallel Multicriterial Optimization Methods

Gergel and Kozinov

√
where the constant H is defined by the relation H = 4L N , L is the Lipschitz constant from
(2), and N is the dimensionality of the optimization problem (1).
The algorithms for the numerical construction of the approximations of the Peano curves
are presented in [18].

4

Parallel Computations for Global Optimization Problems

The approach presented in Section 3 allow reducing the solving of the MCO problem (1) to
the solving a series of the reduced multiextremal problems (4). And, therefore, the problem
of development of the methods for solving the MCO problems is resolved by the possibility of
a wide application of the global optimization algorithms. Taking into account that the MCO
problems can be computationally expensive the optimization methods should take advantage
of the computational potential of high performance supercomputing systems.
In further description of the parallel computation methods, we will use a simpler notation
of the reduced one-dimensional problems (4) as
ϕ(x) = F (λ, y(x)) : x ∈ [0, 1].

4.1

(6)

Parallel Algorithm of Multicriterial Global Optimization for the
Computational Systems with Shared Memory

Within the framework of the proposed approach, the Parallel Multidimensional Algorithm of
Global Search (PMAGS) [1, 9, 11, 10, 18, 19] is proposed to apply for solving every next reduced
one-dimensional problem (4). The general computational scheme of this algorithm consists in
the following.
Let p be the number of parallel computational units (processors or cores) of a computational
system with shared memory being used and let the minimized function values to be computed
at any p points of the interval [0, 1] (hereafter such computations will be called trials). Further,
let k, k > 1 parallel iterations of the global optimization be competed (p trials are assumed to
be performed within every parallel iteration). The choice of the trial points at the next (k +1)th
parallel iteration is determined by the following rules.
Rule 1. Renumber the points from the set of all trial points executed earlier by the lower
indices in the order of increasing coordinate value
0 = x0 < x1 < · · · < xi < · · · < xkp < xkp+1 = 1,

(7)

the points x0 , xkp+1 are introduced additionally for the convenience of further consideration,
the minimized function values z0 , zk+1 at these points remain undefined.
Rule 2. Compute the current estimate of the Hölder constant H from (5)

rM, M > 0
|zi − zi−1 |
(8)
m=
, M = max
1≤i≤kp
i
1,
M =0
as the relative differences of the minimized functions ϕ(x) from (6) on the set of executed trial
points xi , 1 ≤ i ≤ kp from (7). Hereafter i = (xi − xi−1 )1/N , 1 ≤ i ≤ kp + 1. The constant r,
r > 1 is the reliability parameter of the algorithm.
4

	

Gergel V.P. et al. / Procedia Computer Science 108C (2017) 1463–1472

Accelerating Parallel Multicriterial Optimization Methods

1467

Gergel and Kozinov

Rule 3. Compute the characteristics R(i) for each interval (xi−1 , xi ), 1 ≤ i ≤ kp + 1, where
R(i) = i +

(zi −zi−1 )2
m2 i

i−1 )
− 2 (zi +z
, 1 ≤ i ≤ kp,
m

(9)

zi
R(i) = 2i − 4 m
, i = 1,

R(i) = 2i − 4 zi−1
m , i = kp + 1
Rule 4. Arrange the characteristics of the intervals obtained in (9) in the decreasing order
R(t1 ) ≥ R(t2 ) ≥ · · · ≥ R(ti ) ≥ · · · ≥ R(tkp+1 )

(10)

and select p intervals with the indices tj , 1 ≤ j ≤ p having the maximal values of characteristics.
Rule 5. Execute new trials (the computations of the minimized function values) at the
points xkp+j ,1 ≤ j ≤ p located in the intervals with the maximal characteristics from (10)
xk+j =

xtj +xtj −1
2

1
− sign(ztj − ztj −1 ) 2r
[

x

k+j

=

xtj +xtj −1
, tj
2

|ztj −ztj −1 | N
] ,1
m

< tj < kp + 1,
(11)

= 1, tj = kp + 1.

The termination condition, according to which the trials are terminated, is defined by the
condition
(12)
tj < ε, 1 ≤ j ≤ p,
which should be checked for all intervals from (10), which the scheduled trials are executed in.
The parameter ε > 0 is the predefined accuracy of the problem solution. If the termination
condition (12) is not satisfied, the iteration index k is incremented by 1 (the number of the
executed trials is incremented by p), and the next global optimization iteration is performed.
The characteristics R(i), 1 ≤ t ≤ kp+1 computed in (9) can be interpreted as some measures
of importance of the intervals with respect to the location of the global minimum point in these
ones. Then, the scheme of choosing the intervals for executing the next trials given by (10)(11) becomes clear – the points for the next global optimization iteration are selected in the
intervals, which the location of the global minimum point is the most probable in.
The conditions of convergence of the presented algorithm are considered, for example, in
[18]. Thus, at an appropriate estimate of the Hölder constant ( m > 22−1/N H, m is from (8)
and H is from (5)) the algorithm converges to all existing global minimum points.

4.2

Accelerating the Computations Based on Intensive Using of
Search Information

Solving the multicriterial optimization problems may require a considerable volume of computations. The reason of the computational complexity consists in the fact that in order to find
several efficient solutions may require multiple solving of the global optimization problems (3)(4). The overcoming of this complexity can be provided by utilization of the search information
obtained in the course of computations.
As a result of the scalarization of the partial criteria (3), the dimensionality reduction (4),
and the necessity of the ordered representation (see Rule 1 of PMAGS), the search information
can be represented as a search state matrix (SSM)
Ak = {(xi , zi , vi )T : 0 ≤ i ≤ kp + 1}

(13)
5

1468	

Accelerating Parallel Multicriterial
Optimization
Methods
Gergel and Kozinov
Gergel
V.P. et al. / Procedia
Computer Science 108C (2017) 1463–1472

where xi , 0 ≤ i ≤ kp + 1 are the reduced points of the executed global optimization iterations,
zi , 1 ≤ i ≤ kp + 1 are the values of the scalar criterion of the current optimization problem (4),
and vi , 1 ≤ i ≤ kp + 1 are the calculated values of the partial criteria fi (y), 1 ≤ i ≤ s.
Introducing SSM causes a crucial impact on an essential reduction of the computational
complexity of solving the multicriterial optimization problems. The optimization methods can
use SSM for the adaptive performing the next optimization iterations taking into account the
results of previous computations. And, the most important, the availability of SSM allows
transforming the results of all previous computations zi , 1 ≤ i ≤ kp stored in SSM to the

values of the current optimization problem (4) for new values of the convolution coefficients λj ,
1 ≤ j ≤ s i.e.


zi = max (λj vi (j)), 1 ≤ i ≤ k,
1≤j≤s

without any time-consuming computations of the partial criteria values. And, therefore, all
search information can be utilized in the continued computations.
The PMAGS algorithm expanded with the possibility to use the search information Ak
from (13) will be called hereafter Parallel Multidimensional Algorithm of Multicriterial Global
Search (PMAMGS).

5

Results of Computational Experiments

The computational experiments have been carried out on the Lobachevsky supercomputer at
Lobachevsky State University of Nizhni Novgorod (the operating system – CentOS 6.4, the
supercomputer management system – SLURM). A computational node included 2 Intel Sandy
Bridge E5-2660 2.2 GHz processors, 64 Gb RAM. The CPUs had 8 cores (i. e. total 16 cores
were available per a node).
Prior to describe the computational experiments, let us consider the results of comparing
the proposed approach to a number of other multicriterial optimization algorithms presented
in [8]. In the comparison, the bi-criteria two-dimensional problem proposed in [5]
f1 (y) = (y1 − 1)y22 + 1, f2 (y) = y2 , 0 ≤ y1 , y2 ≤ 1,

(14)

was used. The construction of a numerical approximation P DA(f, D) of the Pareto domain
P D(f, D) was considered as the solution of the MCO problem. In order to evaluate the quality
of the approximation, the completeness and the uniformity of coverage of the Pareto domain
were compared using the following two indices [5, 20]:
• The hypervolume index (HV) defined as the volume of the subdomain of the values of the
vector criterion f (y) dominated by the points of the Pareto domain approximation. The
value of this index can be obtained as the total volume of the intersection of the hyperparallelepipeds, the vertices of which are the points of the Pareto domain approximation,
and some reference point f̂ .This index features the completeness of an approximation of
the Pareto domain (the larger the value, the more complete the coverage of the Pareto
domain).
• The distribution uniformity index (DU) of the points from the Pareto domain approximation defined by the relations:
p
p
di
2
DI = i=1 (di − d) , d = i=1 ,
p
di = min (yi , y), yi , y ∈ DP A, yi = y, 1 ≤ i ≤ p,
6

	

Accelerating Parallel Multicriterial
Optimization
Methods
Gergel and Kozinov
Gergel
V.P. et al. / Procedia
Computer Science 108C (2017) 1463–1472

Table 1: The results of computational experiments from
Method
Iterations PDA points
MC
500
67
SEMO
500
104
NUC
515
29
BLO
498
68
MAMGS+ 370
100

[8] for the test problem (14)
HV
DU
0.300 1.277
0.312 1.116
0.306 0.210
0.308 0.175
0.316 0.101

where p is the number of points in the Pareto domain approximation. This index features
the uniformity of a coverage of the Pareto domain (the less the value, the more uniform
the coverage of the Pareto domain).
Within the frames of this experiment, five multicriterial optimization algorithms have been
compared:
• The Monte-Carlo (MC) method, in which the trial points were chosen randomly and
uniformly within the search domain D,
• The genetic algorithm SEMO from the PISA library [12, 20],
• The Non-Uniform Coverage (NUC) method [12],
• The Bi-objective Lipschitz Optimization (BLO) method proposed in [20],
• The MAMGS (the serial version of PMAMGS) algorithm proposed in the present paper.
For the first three algorithms from the above list, the results of experiments from [12] have been
used. For the BLO method, the results of experiments were presented in [20]. For MAMGS
the following parameters were used: the reliability parameter r = 3 and the accuracy ε = 0.05.
Total 50 problems (4) with various convolution coefficients λ distributed uniformly in Λ from
(3) have been solved by MAMGS.
The results of experiments from [8] are presented in Table 1. The results have demonstrated that the MAMGS algorithm has a considerable advantage as compared to considered
multicriterial optimization methods even when solving relatively simple MCO problems.
In the executed series of experiments, the solving of bi-criteria two-dimensional MCO problems i.e. N = 2, s = 2 has been performed. The multiextremal functions defined below:
AB =
CD =



7
i=1



7
i=1

7

j=1 [Aij aij (y1 , y2 )

7

j=1 [Cij aij (y1 , y2 )

+ Bij bij (y1 , y2 )]

2

− Dij bij (y1 , y2 )]

φ(y1 , y2 ) = −(AB + CD)1/2

2

were used as the problem criteria. Here
aij (y1 , y2 ) = sin(πiy1 ) sin(πjy2 ),

bij (y1 , y2 ) = cos(πiy1 ) cos(πjy2 )

are defined in the range 0 ≤ y1 , y2 ≤ 1, and the parameters −1 ≤ Aij , Bij , Cij , Dij ≤ 1 are
independent values distributed uniformly and randomly. The minimization of such functions
arises, for example, in the problem of evaluation of the maximum strain (determined by the
strength) in a thin plate at the transverse load.
7

1469

1470	

Gergel V.P. et al. / Procedia Computer Science 108C (2017) 1463–1472
Accelerating Parallel Multicriterial Optimization Methods
Gergel and Kozinov

Figure 1: Approximation of the Pareto domain for a MCO problem to be solved
Table 2: The results of the series of experiments on solving the bi-criteria two-dimensional
MCO problems
Iterations
Speedup
p
PMAGS PMAMGS
PMAGS PMAMGS - 1 PMAMGS - 2
1
2
3
4
5
6
1
17161,9
1773,4
1
1
9,7
2
9019,4
1103,1
1,9
1,6
15,6
5
4117,2
550,1
4,2
3,2
31,2
10
1801,7
179,6
9,5
9,9
95,6
25
820,2
77,5
20,9
22,9
221,4
In order to provide more justified conclusions on the efficiency of the developed approach,
the solving of 100 multicriterial problems formed with the use of the multiextremal functions
of the above problem family has been performed. For solving the optimization problems, the
reliability parameter r = 2 and the search accuracy ε = 0.001 were used. In every experiment
performed, the problem (3) has been solved for 50 various convolution coefficients with the use of
the search information and without the one. The obtained results of experiments were averaged
over the number of solved MCO problems (i. e. the results of experiments are presented below
in average per one MCO problem, which the global search of the Pareto optimal solutions for
50 problems (3) has been performed within). As an example, an approximation of the Pareto
domain obtained for a problem to be solved is shown in Figure 1.
The results of numerical experiments are presented in Table 2.
To explain the presented results, let us remind that PMAGS is a parallel algorithm without
the use of the search information while PMAMGS uses the search information intensively.
The results of experiments demonstrate an essential reduction of the amount of computations
by means of the use of the search information obtained in the course of the global optimization.
Even for the serial algorithm without any use of the parallel computations, this reduction reaches
more than 9 times (Column 6). In order to account for this result, two variants of obtained
speedup are given in Table 2: one relative to the serial algorithm utilizing the search information
(Speedup 1, Column 5) and another one relative to the initial serial algorithm (Speedup 2,
Column 6). As follows from the presented data, the speedup of the parallel computations
with the use of the search information was 22.9 when using 25 computational cores. One
should note a considerable increase of the speedup of the parallel computations relative to the
initial algorithm without the use of the search information – the speedup was 221.4 for 25
computational cores. In addition, the obtained results are presented also in the visual graphical
8

	

Gergel V.P. et al. / Procedia Computer Science 108C (2017) 1463–1472
Accelerating Parallel Multicriterial Optimization Methods
Gergel and Kozinov

Figure 2: Diagrams for demonstrating the results of the numerical experiments
form in Figure 2.
In Figure 2, the bar chart demonstrates the number of parallel iterations executed by the
methods (columns 2 and 3 in Table 2), the corresponding numbers are shown on the left axis.
The line plots present the speedups of parallel computations (columns 5 and 6 in Table 2), the
corresponding scale is shown on the right axis.

6

Conclusions

In the present paper, an efficient parallel method for solving complex multicriterial optimization problems, which the criteria of optimality can be multiextremal, and the computing of the
criteria values can require a large volume of computations in, has been proposed. The basis of
the proposed approach consists in reduction of the multicriterial problems to the global optimization ones by means of the minimax convolution of the partial criteria, the dimensionality
reduction with the use of the Peano space-filling curves, and the application of the efficient
information-statistical global optimization methods.
The key aspect of the developed approach consists in the overcoming of large computations
costs of the global optimization of the set of efficient solutions when solving the multicriterial
optimization problems. A considerable increase of the efficiency and essential reduction of
the amount of computations are provided by means of intensive using of the search information
obtained in the course of computations. Within the framework of developed approach, a method
for transforming all available search information to the values of current scalar problem of global
optimization being solved have been proposed. The search information is used by the parallel
optimization methods for the adaptive planning of the performed global optimization iterations.
The results of the computational experiments demonstrated such an approach to allow reducing
the computational costs of solving the multicriterial optimization problems essentially – by tens
and hundreds times.
In conclusion, one can note that the developed approach is a promising one and requires
further investigations. First of all, it is necessary to continue carrying out the computational
experiments on solving the multicriterial optimization problems with a large number of partial criteria of efficiency and for a greater dimensionality of the optimization problems being
solved. It is necessary also to estimate the possibility to organize the parallel computations with
the use of a greater number (hundreds and thousands) of processors in the high performance
computational systems with the distributed memory.
9

1471

1472	

Accelerating Parallel Multicriterial
Optimization
Methods
Gergel and Kozinov
Gergel
V.P. et al. / Procedia
Computer Science 108C (2017) 1463–1472

7

Acknowledgements

This work has been supported by Russian Science Foundation, project No 16-11-10150 ”Novel efficient methods and software tools for time-consuming decision making problems using superiorperformance supercomputers.”

References
[1] K. Barkalov, V. Gergel, and I. Lebedev. Use of Xeon Phi coprocessor for solving global optimization
problems. LNCS, 9251:307–318, 2015.
[2] Y. Collette and P. Siarry. Multiobjective Optimization: Principles and Case Studies (Decision
Engi-neering). Springer-Verlag, Berlin, Heidelberg, 2011.
[3] M. Ehrgott. Multicriteria Optimization. Springer-Verlag, Berlin, Heidelberg, 2001, 2nd ed., 2010.
[4] G. Eichfelder. Scalarizations for adaptively solving multi-objective optimization problems. Comput. Optim., 44:249–273, Appl. 2009.
[5] Yu.G. Evtushenko and M.A. Posypkin. A deterministic algorithm for global multi-objective optimization. Optimization Methods and Software, 29(5):1005–1019, 2014.
[6] J. Figueira, S. Greco, and (Ed.). Multiple criteria decision analysis: State of the art surveys.
Springer, New York, 2005.
[7] C.A. Floudas and M.P. Pardalos. Recent Advances in Global Optimization. Princeton University
Press, 2016.
[8] V. Gergel and E. Kozinov. Efficient multicriterial optimization based on intensive reuse of search
information. (to be published).
[9] V. Gergel and I. Lebedev. Heterogeneous parallel computations for solving global optimization
problems. Procedia Computer Science, 66:53–62, 2015.
[10] V. Gergel and S. Sidorov. A two-level parallel global search algorithm for solution of computationally intensive multiextremal optimization problems. LNCS, 9251:505–515, 2015.
[11] V.P. Gergel, V.A. Grishagin, and A.V. Gergel. Adaptive nested optimization scheme for multidimensional global search. Journal of Global Optimization, 66(1):1–17, 2015.
[12] C. Hillermeier and J. Jahn. Multiobjective optimization: survey of methods and industrial applications. Surv. Math. Ind., 11:1–42, 2005.
[13] A. Mardani, A. Jusoh, K. Nor, Z. Khalifah, N. Zakwan, and A. Valipour. Multiple criteria decisionmaking techniques and their applications – a review of the literature from 2000 to 2014. Economic
Research-Ekonomska Istrazivanja, 28(1):516–571, 2015.
[14] R.T. Marler and J.S. Arora. Multi-Objective Optimization: Concepts and Methods for Engineering.
VDM Verlag, 2009.
[15] K. Miettinen. Nonlinear Multiobjective Optimization. Springer, 1999.
[16] J.D. Pinter. Global optimization inaction (continuous and Lipschitz optimization: algorithms,
implementations and applications). Kluwer Academic Publishers, Dortrecht, 1996.
[17] Y.D. Sergeyev, R.G. Strongin, and D. Lera. Introduction to global optimization exploiting space
filling curves. Springer, 2013.
[18] R. Strongin and Ya. Sergeyev. Global optimization with non-convex constraints. Sequential and
parallel algorithms. Kluwer Academic Publishers, Dordrecht, 2000.
[19] R.G. Strongin. Numerical methods in multiextremal problems: information-statistical algorithms.
Nauka, Moscow (in Russian), 1978.
[20] A. Zilinskas and J. Zilinskas. Adaptation of a one-step worst-case optimal univariate algorithm of
bi-objective lipschitz optimization to multidimensional problems. Commun Nonlinear Sci Numer
Simulat, 21:89–98, 2015.

10

