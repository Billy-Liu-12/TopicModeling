Procedia Computer Science
Volume 51, 2015, Pages 1292–1302
ICCS 2015 International Conference On Computational Science

DiamondTorre GPU implementation algorithm of the
RKDG solver for ﬂuid dynamics and its using for the
numerical simulation of the bubble-shock interaction
problem
Boris A. Korneev1 and Vadim D. Levchenko2
1

Moscow Institute of Physics and Technology, Dolgoprudny, Moscow Region, Russia
boris.korneev@phystech.edu
2
Keldysh Institute of Applied Mathematics, Moscow, Russia
lev@keldysh.ru

Abstract
In this paper the solver based upon the RKDG method for solving three-dimensional Euler
equations of gas dynamics is considered. For the numerical scheme the GPU implementation
algorithm called DiamondTorre is used, which helps to improve the performance speed of calculations. The problem of the interaction of a spherical bubble with a planar shock wave is
considered in the three-dimensional setting. The obtained calculations are in agreement with
the known results of experiments and numerical simulations. The calculation results are obtained with the use of the PC.
Keywords: ﬂuid dynamics; Euler equations; RKDG method; DiamondTorre algorithm; GPU computing; bubble-shock interaction problem

1

Introduction

One of the ﬁelds of research in computational ﬂuid dynamics is the numerical simulation of
fully three-dimensional problems with precise and detailed results. It requires using high-order
numerical methods and modern high-performance computing resources. In this paper the GPU
implementation algorithm called DiamondTorre for the RKDG numerical method of solving
ﬂuid dynamics problems in three-dimensional case is introduced.
The RKDG method (Runge-Kutta discontinuous Galerkin method) for numerical solution
of Euler equations of ﬂuid dynamics is considered in this paper [1]. RKDG method is the highorder, non-oscillating and explicit method. It has some common features with the ﬁnite volume
methods [2] and the ﬁnite element Galerkin method [3]. For the numerical ﬂuxes construction,
the exact or approximate Riemann solvers are used, which makes the RKDG closer to the
1292

Selection and peer-review under responsibility of the Scientiﬁc Programme Committee of ICCS 2015
c The Authors. Published by Elsevier B.V.

doi:10.1016/j.procs.2015.05.314

DiamondTorre GPU implementation algorithm of the RKDG CFD solver for the bubble-shock
interaction problem solving
Korneev B.A., Levchenko V.D.

Godunov methods [4]. Except its good calculating properties, the method has clear logical
structure and is easy to code.
For the GPU implementation of chosen numerical method, a new algorithm called DiamondTorre is developed by the authors. Special attention is paid to the large scale problems having
the numerical domain too large to be fully kept in the GPU memory. The algorithm structure
is associated with the CUDA tools. Results of the performance testing show high eﬃciency of
the algorithm compared to the maximum possible derived from the peak GPU performance in
FLOPs and the numerical scheme needs.
In this paper the application of the solver to the problem of interaction between the shock
wave and the bubble of gas with the diﬀerent density in the three-dimensional setting is investigated. Due to its fully three-dimensional setting and complex behavior of the solution, it can
be treated as a tough validation test for the solver. Also this problem is linked to the important
tasks such as the description of turbulent combustion in the jet engines, interaction of fuel slurry
with a shock wave of the piston in the internal combustion engines, the non-surgical removal
of kidney stones (lithotripsy), the research of sonoluminescence, some problems in astrophysics
and others. This problem is being numerically simulated with the use of high-performance
computing [5, 6, 7]. The works are known, where the RKDG method is used for that [7]. In
this work the numerical results are obtained using the PC with 32GB memory and GTX Titan
GPU. Results of the numerical experiments are shown for two diﬀerent initial conditions.
The paper is organised in the following way. In section 2 the RKDG numerical method is
described. Section 3 deals with the implementation algorithm. Results of the bubble-schock
interaction problem simulation are presented in section 4, followed by the concluding section 5.

2
2.1

RKDG method
Governing equations

The system of Euler equations of ﬂuid dynamics of the inviscid compressible ﬂow can be written
as
∂U ∂Fj (U)
+
= 0,
(1)
∂t
∂xj
where U = (ρ, ρui , E)T is the vector of conservative variables, E = ρ(ε + 12 ui ui ) is the total
energy, Fj (U) = (ρuj , ρui uj + pδij , (E + p)uj )T are the convective or Eulerian ﬂuxes.
We also should add the equation of state to make the internal energy, the pressure and the
density functionally depended like ε = ε(p, ρ).
This system is being solved in a three-dimensional domain Π ∈ R3 on a time interval [0, T ]
with the given initial and boundary conditions
U(x, 0) = U0 (x), x ∈ Π, U(x, t) = UΓ (x, t), x ∈ ∂Π, t ∈ [0, T ].

2.2

(2)

DG space discretization
N

The domain Π is divided into N non-overlapping elements or cells Sp , p = 1, . . . , N , p=1 Sp =
Π, Sp ∩ Sq = ∅, p = q. We will construct the approximate solution Uh for every Sp in form of
the expansion
(3)
Uh (x, t) = un (t)ϕn (x)
of the basis functions Vp = {ϕn (xi )}kn=1 . The coeﬃcient un (t) = [u1n (t), . . . , u5n (t)]T is only
time-dependent.
1293

DiamondTorre GPU implementation algorithm of the RKDG CFD solver for the bubble-shock
interaction problem solving
Korneev B.A., Levchenko V.D.

After inserting (3) into (1), we force the residual to be orthogonal to all basis functions.
Therefore, after applying the divergence theorem, we get such an equality for the variables un (t)
in every Sp , n = 1, . . . , k, p = 1, . . . , N :
u˙ n

Sp

ϕn ϕm dV +

∂Sp

hj (Uh (xint ), Uh (xext ))ϕm nj dΣ −

Sp

Fj (Uh )

∂ϕm
dV = 0,
∂xj

(4)

The values Fj (Uh ) at x ∈ ∂Sp are redeﬁned by the numerical ﬂux hj (·, ·), which depends
on the variables according to the current cell side points xint and the neighboring ones xext ,
and can be the exact or approximate Riemann solver [1]. The numerical ﬂux used in this work
is the HLLC ﬂux [8].
Integrals in (4) are approximated with the quadrature rules. Therefore such an explicit
system of the ordinary diﬀerential equations can be obtained
u˙ = Lu,
u(0) = u0 .

2.3

(5)

Time integration and limiting

The system (5) is solved by the Runge-Kutta scheme. On every stage the special limiter is
applied.
1. u0 = ΛP (u0 )
2. ∀n = 0, . . . , M − 1:
• u(0) = un ,
• ∀i = 1, . . . , K + 1 : u(i) = Λ

i−1
l=0

αil u(l) + βil Δtn L(u(l) ) ,

• un+1 = uK+1 .
P is the projection operator of the initial conditions on the discrete space. Coeﬃcients αil , βil
should provide the approximation order k + 1 and satisfy the TVD condition. The time step
Δtn is taken from the CFL condition [1].
Λ is the slope limiter, which manages to suppress the spurious oscillations near the discontinuities. The result of the work of the limiter depends on the values in the current cell and
adjacent ones.
In this work the piecewise linear basis, the 2 order Runge-Kutta scheme and the minmod
limiter [1] is used, so the scheme has the second order of accuracy in smooth regions.

3

The algorithm of implementation

For the eﬀective GPU implementation algorithm of the numerical method the modiﬁed version
of the DiamondTorre algorithm is used. In [9] it was previously applied to the PIC numerical
method for the numerical simulation of Vlasov-Maxwell equations of plasma.
The DiamondTorre algorithm is adopted to the heterogeneous computer architecture and
capable to solve the tasks with the numerical domain larger than the amount of GPU memory
with high ratio of calculation speed to the theoretically possible. CUDA as the programming
tool for GPGPU coding is used.
1294

DiamondTorre GPU implementation algorithm of the RKDG CFD solver for the bubble-shock
interaction problem solving
Korneev B.A., Levchenko V.D.

3.1

Initial remarks

For every numerical cell, the computation with the method can be divided into four main steps:
1. Calculating the numerical ﬂuxes and performing the 1 stage of the Runge-Kutta
2. Limiting after the 1 stage
3. Calculating the numerical ﬂuxes and performing the 2 stage of the Runge-Kutta
4. Limiting after the 2 stage
On every step we need some data only from the current cell and neighboring or adjacent ones.
In this work the structured grid is used, so every cell has 6 neighbors according to the number
of the cell faces. In this case the numerical stencil has a form of the cross.
We use numerical cells containing 4 basic coeﬃcients of every 5 values of U saved as ﬂoat
types with single presision, so cell size is 80B. Let the grid be arranged as the three-dimensional
array. Then it can be converted to the two-dimensional array of vectors by the third coordinate.
For this vector supercell, the computation requires the data from only 4 adjacent supercells on
every step. Every supercell can be denoted by the cartesian coordinate (x, y), according to the
array index.

3.2

The Torre concept and the based-upon algorithm

The further description of the algorithm of the calculation is based on a concept of the Torre
(the tower, Spanish). The Torre with the coordinates (x, y) and the height H TH (x, y) is deﬁned
as the following procedure:
• height = 0;
• while (height < H) do
1.

– Calculate all the numerical ﬂuxes of the cell C(x, y) (the one with the coordinates (x, y)), including the ﬂuxes on the x-axis between the C(x, y) and
C(x, y − 1) and between C(x, y + 1) and C(x, y), the same for the ﬂuxes on the
y-axis
– Calculate the ﬂuxes on the z-axis and perform the ﬁrst stage of the RungeKutta on (x, y)
– x += 1
– Calculate the ﬂuxes on the z-axis and perform the ﬁrst stage of the RungeKutta on (x, y)

2.

– Perform the limiter on (x, y)
– x += 1
– Perform the limiter on (x, y)

3. The same as 1 but for the second stage of the Runge-Kutta
4. The same as 2
5. height += 1
1295

DiamondTorre GPU implementation algorithm of the RKDG CFD solver for the bubble-shock
interaction problem solving
Korneev B.A., Levchenko V.D.

• done
Increasing the height means the growth in time.
The Torre is schematically described in the ﬁgure 1(a). R1 and R2 mean ﬁrst and second
stages of the Runge-Kutta, respectively, L1 and L2 mean limiting after the ﬁrst or second
stages. The commands of the Torre are skipped if the coordinates go beyond the numerical
domain. If the coordinates correspond to the boundary of the numerical domain, the boundary
condition is used.
t

Window of computations (GPU)

Ny-1
ti+H

L2 L2

R1 R1

ti+H-1

computing
direction

computing
direction

computing
com
omput
mput
put
uting
ut
ng
direction
direc
ec
ec
ction
ct
t

ti+1

1

L2 L2
R2 R2

Copying back to CPU

L1 L1

Copying from CPU to GPU

Ny-2

R2 R2

L1 L1
ti

R1 R1

computational window
x-1 x

x+4H+1

(a)

x

0

i-1 i

y

y
z

Nx-2

x

(b)

Nx-1

z

x

...

i+4H

Whole domain (CPU)

(c)

Figure 1: (a) the Torre in the cross-section xt; (b) the DiamondTorre algorithm described in
the plane xy; (c) the memory usage in the DiamondTorre algorithm.
The DiamondTorre algorithm is deﬁned in the following way. Let the integer H > 0 be the
height of all the Torres. Initially, the stripe of Torres TH (N x, 0), TH (N x, 2), . . . , TH (N x, N y)
is calculated. Then the stripe of Torres TH (N x−1, −1), TH (N x−1, 1), . . . , TH (N x−1, N y −1)
is performed. Further, all the numerical domain is covered with the Torres this way, similar to
the checkerboard pattern.
In the ﬁgure 1(b) the scheme of the DiamondTorre algorithm is demonstrated in the projection on the plane xy. Only the bottom side of the Torres are drawn. The calculation of the
ﬂuxes is marked as small diamonds with two cut vortices and the performing of the RungeKutta stage is marked as the square points. The elements of each Torre performed in a single
stripe have the same color and are independent from each other, so they can be calculated in
parallel. The result of one iteration of the DiamondTorre algorithm with the Torres of height
H is equal to performing H simple time steps.
CUDA is used for coding the algorithm. Each Torre is calculated into single CUDA block.
The calculations by the z-axis in the block (or the Torre as well) are performed in parallel by
the CUDA threads. The numerical ﬂuxes at z-axis are stored in the shared memory of the
block. The extra data needed to be stored such as the numerical ﬂuxes at x-axis and y-axis at
every stage, the temporary results from the Runge-Kutta stages and the limiters are kept in
the global memory of the GPU. The amount of this memory is conditioned by the height of the
Torres and is not more than (4 · H + 2) · N y · sizeof (supercell) for every extra array of data.
So the calculations are performed not in the part of the whole domain called computational
window. In the ﬁgures 1(a) and 1(c) this so-called window is shown.
This window is moving from right to left during every iteration, and the memory exchange
with the main grid kept in the CPU memory is needed. The memory usage during the algorithm
and the exchange zones are shown in the ﬁgure 1(c). It appears that before performing the stripe
1296

DiamondTorre GPU implementation algorithm of the RKDG CFD solver for the bubble-shock
interaction problem solving
Korneev B.A., Levchenko V.D.

of the Torres with the coordinate x, the only one slice of the data array C(x−1, y), y ∈ 0, N y − 1
is needed to be copied to the GPU and the result is ready to be sent back only at the slice
C(x + 4H, y), y ∈ 0, N y − 1. The total amount of exchange is 2 · N x · N y · sizeof (supercell) for
one DiamondTorre iteration (H timesteps) and it does not depend on the H. It means that the
negative inﬂuence of memory exchange on the performance can be hidden if H is big enough,
which is proved in the next subsection.

3.3

Algorithm performance testing

In the ﬁgure 2 the results of the solver run on several GPU is shown. Domain grid size for the
test problem is 512 × 512 × 1024 cells. One can see that performance grows while increasing H,
reaching its maximum value. It should be noted that only for one GPU from presented (GTX
Titan), its global memory amount is enough to reach the maximum performance.
45

GTX 750 Ti
GTX Titan
GTX 970

Performance, 106 cells/sec

40
35
30
25
20
15
10

2GB
5

0

5

4GB
10

6GB GDDR
15

20

25

Number of oors in the Torre

Figure 2: Speed of calculations depending on the height of the Torres
Using the CUDA proﬁler, it was measured that about 104 FMA per cell is needed to compute
using the the numerical scheme, which corresponds with analytical estimations. So, taking into
the account the peak performance of used GPUs (for example, see [10]), it appears that the
gained performance is up to 20% to the theoretically possible.
Further calculations in this work are performed using the PC having GTX Titan GPU and
32GB of CPU memory. Most of the simulations are made using the grid of 512 × 512 × 1024
cells or smaller.

4
4.1

Bubble-shock interaction problem solution
Problem statement

The computational domain is a box ﬁlled with an ideal gas at rest with density ρ = ρ0 and
pressure p = p0 , the speed of sound there is equal to a0 . Inside this parallelepiped the spherical
region of radius R = R0 is deﬁned, with the center at the point (x0 , y0 , z0 ), where the pressure
is equal to the external, and the density diﬀers from the density of the ambient gas (pB = p0 ,
ρB = ρ0 ). On the left, at the coordinate x = xL , there is a plane shock wave. Its front moves
from left to right with the speed of v∗ > a0 , behind the shock wave the unknown quantities
are determined using the Rankine-Hugoniot conditions. Parameters of the shock waves are
given by the Mach number M = v∗ /a0 , the density of the bubble is characterized by the
Atwood number At = (ρB − ρ0 ) /(ρB + ρ0 ) . Due to the symmetry, to reduce the size of the
computational domain, the task is set to the quarter, as shown in ﬁgure 3, planes xy and yz
1297

DiamondTorre GPU implementation algorithm of the RKDG CFD solver for the bubble-shock
interaction problem solving
Korneev B.A., Levchenko V.D.

have the symmetry boundary condition, other planes have the boundary condition of the free
ﬂow [6].

Figure 3: The computational domain at the initial time
The dimensionless units of measure are chosen connected with the radius of a bubble, the
background sound velocity and density by the following expressions
˜, y = R0 y˜, z = R0 z˜;
x = R0 x
u = a0 u
˜, v = a0 v˜, w = a0 w;
˜
˜
t = tR0 /a0 ;
ρ = ρ0 ρ˜;
p = ρ0 a20 p˜ = p˜p0 /γ.
They are denoted by the tilde at the top.

4.2

Mesh-sensitive eﬀects research

The numerical solution, as it is proved in [6], has the mesh-sensitive features. To search for it
and to deﬁne the proper parameters of the mesh to be used further, the following numerical
experiment is set.
The same task is solved using the mesh with the width of 64, 128, 256 and 512 cells. Results
are compared with each other at the same speciﬁed time moment. In ﬁgures 4(a) —4(d) the
part of the numerical solution of density in the diametrical cross-section of the bubble is shown
at the same ﬁxed time moment.
It is easy to see that the mesh shredding leads to the display of new slight details of the
solution, especially near the vortices.

(a)

(b)

(c)

(d)

Figure 4: The dependence of the solution on the mesh. The mesh width (a) is 64 cells, (b) is
128 cells, (c) is 256 cells, (d) is 512 cells.
Further the numerical experiments are held using the width of the grid equal to 512 or
smaller.
1298

DiamondTorre GPU implementation algorithm of the RKDG CFD solver for the bubble-shock
interaction problem solving
Korneev B.A., Levchenko V.D.

4.3

The case of a rareﬁed gas in a bubble

Figure 5 shows the results of simulating the interaction of a helium bubble in air (At = −0.757)
with a shock with Mach M = 3. In the paper [6] the similar setting is considered. The mesh
used is 512 × 512 × 1024 cells.

(a)

(b)

(c)

(d)

(e)

(f)

(g)

(h)

(i)

(j)

(k)

(l)

Figure 5: At = −0.757, M = 3; vorticity (top) and density (bottom) at the diﬀerent time
moments: (a) t˜ = 0.00, (b) t˜ = 0.22, . . . , (l) t˜ = 2.44

In the top half of each ﬁgure the vorticity scalar product by the unit normal vector to the
plane of the ﬁgure (∇ × V) · n is shown (further it is called simply the vorticity), and density
ρ is shown on the bottom one in the same diametrical cross-section. Scales for vorticity and
density are at the top and bottom, respectively. At the initial stage of interaction, reﬂected in
ﬁgures 5(a)–5(c), the shock wave penetrates into the cavity, deforming the surface of the bubble,
while the reﬂected rarefaction wave occurs. During the further evolution of the process shown
in ﬁgures 5(c)–5(e), the initial perpendicular wave, the wave-precursor and the Mach stem are
connected to the so-called triple point. After passing through the bubble, wave initiates the
formation of the cylindrical-ring vortex structure, moving behind the shock wave, which mainly
consists of low-density gas. The following process is shown in ﬁgures 5(e) — 5(l).
1299

DiamondTorre GPU implementation algorithm of the RKDG CFD solver for the bubble-shock
interaction problem solving
Korneev B.A., Levchenko V.D.

4.4

The case of a dense gas in a bubble

The series of drawings 6(a) — 6(p) and 7(a) — 7(f) show the results of simulating the interaction
of the bubble with freon R12 (At = 0.613) in the air with a shock with Mach M = 5. In the
paper [6] this statement is also considered. In the top half of each ﬁgure the vorticity is shown
and density is shown on the bottom one in the diametrical cross-section. Scales for vorticity
and density are at the top and bottom, respectively. The mesh size is 448 × 448 × 896 cells.

(a)

(b)

(j)

(c)

(k)

(d)

(l)

(e)

(m)

(f)

(n)

(g)

(o)

(h)

(i)

(p)

Figure 6: At = 0.613, M = 5; vorticity (top) and density (bottom) at the early time moments:
(a) t˜ = 0.0, (b) t˜ = 0.1, . . . , (o) t˜ = 1.4, (p) t˜ = 1.5.
It should be noted that in this statement the process develops in the diﬀerent way from the
Air-He case. The proﬁle of the shock wave becomes concave and focused while passing through
the bubble. The bubble becomes deformed and pressured. In addition, the development of
the unstable vortex structures behind the shock and exhausting from the bubble is obtained.
At past times the process seems to have turbulent character with lots of unstable vortices (see
ﬁgures 7(a) — 7(f)). Obtained results qualitatively correspond with ones presented in [6].

5

Concluding remarks

In the paper the DiamondTorre GPU implementation algorithm for constructing the CFD solver
based on the RKDG numerical method is supposed. It provides solving huge tasks having the
numerical domain larger than GPU memory. The tests have shown that performance of the
algorithm is up to 4.5 · 107 cells per second on GTX Titan.
In this work the problem of bubble-shock interaction in fully three-dimensional statement
is considered. The mathematical model used for the task description is rather simpliﬁed, for
example, viscosity, multicomponentness are not taken into account. However, this statement is
basic and its study is important for the extensions and applications. In the paper it is shown
1300

DiamondTorre GPU implementation algorithm of the RKDG CFD solver for the bubble-shock
interaction problem solving
Korneev B.A., Levchenko V.D.

(a)

(b)

(c)

(d)

(e)

(f)

Figure 7: At = 0.613, M = 5; vorticity (top) and density (bottom) at the past time moments:
(a) t˜ = 2.0, (b) t˜ = 2.5, . . . , (e) t˜ = 4.0, (f) t˜ = 4.5.
that this problem can be successfully simulated on the PC, which means that dealing with more
complex settings is also can be more aﬀordable.
Presented algorithm has a signiﬁcant limitation that it requires the structured grid use. To
develop the described approach to solve problems with complex geometries, some extra research
is to be held.

Acknowledgements
The work is partly funded by the Russian Foundation of Basic Research grants 12-01-00490-a
and 12-01-00708-a and the ”UMNIK” grant of the Foundation for Assistance to Small Innovative
Enterprises. Also MIPT 5—100 Program is gratefully acknowledged.

References
[1] B. Cockburn and C. Shu. Runge-Kutta Discontinuous Galerkin Methods for Convection-dominated
Problems. Technical report, NASA, 2000.
[2] J. Caper and H. Atkins. A ﬁnite-volume high-order ENO scheme for two-dimentional hyperbolic
systems. Computational Physics, 106:62–76, 1993.
[3] J. N. Reddy and J. I. Gartling. The ﬁnite element method in heat transfer and ﬂuid dynamics.
C.R.C. Press, 1994.
[4] S.K. Godunov. A diﬀerence method for numerical calculation of discontinuous solutions of the
equations of hydrodynamics. Matematicheskii Sbornik, 89(3):271–306, 1959.
[5] Babak Hejazialhosseini, Diego Rossinelli, and Petros Koumoutsakos. 3D shock-bubble interactions
at mach 3. arXiv preprint arXiv:1210.3822, 2012.
[6] John H.J. Niederhaus, J.A. Greenough, J.G. Oakley, D. Ranjan, M.H. Anderson, and R. Bonazza.
A computational parameter study for the three-dimensional shock–bubble interaction. Journal of
Fluid Mechanics, 594:85–124, 2008.
[7] Leandro D Gryngarten and Suresh Menon. Shock-bubble interaction simulations using a new
two-phase discontinuous galerkin method. In 49th AIAA Aerospace Sciences Meeting including
the New Horizons Forum and Aerospace Exposition, no. AIAA-2011-294, Orlando, FL, 2011.
[8] E.F. Toro. Riemann Solvers And Numerical Methods for Fluid Dynamics: A Practical Introduction.
Springer, 2009.

1301

DiamondTorre GPU implementation algorithm of the RKDG CFD solver for the bubble-shock
interaction problem solving
Korneev B.A., Levchenko V.D.
[9] A.Yu. Perepelkina, V.D. Levchenko, and I.A. Goryachev. 3D3V plasma kinetics code DiamondPIC for modeling of substantially multiscale processes on heterogenous computers. In 41st EPS
Conference on Plasma Physics, 2014.
[10] GeForce. Relative GPU performance of GTX 970. http://www.geforce.com/hardware/
desktop-gpus/geforce-gtx-970/performance.

1302

