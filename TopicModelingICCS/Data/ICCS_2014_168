Procedia Computer Science
Volume 29, 2014, Pages 20–29
ICCS 2014. 14th International Conference on Computational Science

Optimizing Shared-Memory Hyperheuristics on top of
Parameterized Metaheuristics
Jos´e-Mat´ıas Cutillas-Lozano and Domingo Gim´enez
Departamento de Inform´
atica y Sistemas, University of Murcia, Spain
{josematias.cutillas,domingo}@um.es

Abstract
This paper studies the auto-tuning of shared-memory hyperheuristics developed on top of a
uniﬁed shared-memory metaheuristic scheme. A theoretical model of the execution time of
the uniﬁed scheme is empirically adapted for particular metaheuristics and hyperheuristics
through experimentation. The model is used to decide at running time the number of threads
to obtain a reduced execution time. The number of threads is diﬀerent for the diﬀerent basic
functions in the scheme, and depends on the problem to be solved, the metaheuristic scheme,
the implementation of the basic functions and the computational system where the problem is
solved. The applicability of the proposal is shown with a problem of minimization of electricity
consumption in exploitation of wells. Experimental results show that satisfactory execution
times can be achieved with auto-tuning techniques based on theoretical-empirical models of the
execution time.
Keywords: Shared-memory, Auto-tuning, Metaheuristics, Hyperheuristics

1

Introduction

Uniﬁed schemes for metaheuristics facilitate the development of metaheuristics and their application [2]. Hyperheuristics [7] can be developed on top of parameterized metaheuristic schemes.
Their application requires the application of a large number of metaheuristics to obtain satisfactory conﬁgurations of the parameters, which means satisfactory metaheuristics. So, eﬃcient
hyperheuristics require the application of high performance computing strategies. There is a
large number of parallel strategies that can be applied to diﬀerent metaheuristics in parallel
environments of diﬀerent characteristics [1, 10, 11]. In our work, the parallelization of diﬀerent
metaheuristics is tackled through a uniﬁed parameterized shared-memory metaheuristic scheme
[2], and so the diﬀerent metaheuristics that ﬁt the scheme are parallelized in a uniﬁed way.
The auto-tuning problem of sequential and parallel routines has been studied in diﬀerent
ﬁelds [8, 9, 12], and this paper considers the application of auto-tuning methodologies to a
parameterized shared-memory metaheuristic scheme and to hyperheuristics based on it. A
previous work [6] studies the modeling of the execution time of shared-memory parameterized
20

Selection and peer-review under responsibility of the Scientiﬁc Programme Committee of ICCS 2014
c The Authors. Published by Elsevier B.V.
doi:10.1016/j.procs.2014.05.002

Shared-Memory Hyperheuristics

J.-M Cutillas-Lozano and D. Gim´enez

Initialize(S,ParamIni,ThreadsIni)
while (not EndCondition(S,ParamEnd)) do
SS=Select(S,ParamSel)
SS1=Combine(SS,ParamCom,ThreadsCom)
SS2=Improve(SS1,ParamImp,ThreadsImp)
S=Include(SS2,ParamInc,ThreadsInc)
end while
Algorithm 1: Parameterized shared-memory metaheuristic scheme

metaheuristics, and how this work extends the application of the model to hyperheuristics,
and studies the application of auto-tuning techniques based on the theoretical model of the
execution time.
The rest of the paper is organized as follows. Section 2 summarizes the ideas of the common parameterized, shared-memory scheme for metaheuristics and its use for hyperheuristics
development. In Section 3 the modeling of the execution time of the diﬀerent basic and combined/hybridized metaheuriscs in the scheme is analyzed theoretically. Section 4 summarizes
the problem of minimization of electricity consumption in exploitation of wells, used as a test
case, and shows the experimental results obtained with this problem and with a hyperheuristic
to obtain satisfactory metaheuristics for it. Section 5 concludes the paper and oﬀers some future
research lines.

2

Parameterized Shared-Memory Schemes for Metaheuristics and Hyperheuristics

The use of a uniﬁed parameterized scheme for metaheuristics facilitates the development of
metaheuristics and their application. However, selecting the appropriate values of parameters
to apply a satisfactory metaheuristic to a particular problem can be diﬃcult and is computationally demanding. These values can be selected with a hyperheuristic method also developed
with the parameterized metaheuristic scheme. This section summarizes the main ideas of a
parameterized metaheuristic scheme (a more complete description can be found in [3]) and a
shared-memory version [2], and it comments on the development of hyperheuristics on top of
the metaheuristic scheme.

2.1

A Parameterized Shared-Memory Scheme for Metaheuristics

Algorithm 1 presents a parameterized shared-memory scheme of metaheuristics. The scheme
includes a set of basic functions whose instantiation, by selecting the appropriate values of the
Metaheuristic parameters (P aramX), determines a particular metaheuristic. For the sharedmemory version, each function is parallelized independently and includes Parallelism parameters
(T hreadsX) with which the number of threads to use in each basic function is selected. The
optimum values of the parallelism parameters to reduce the execution time depends on the particular metaheuristic (the values of the metaheuristic parameters) and changes for the diﬀerent
basic functions in the scheme.
The meaning and number of the metaheuristic parameters depend on the basic metaheuristics considered and on the implementation of the basic functions. The basic metaheuristics we
have included until now in the scheme are: GRASP, Genetic Algorithms (GA), Scatter Search
(SS) and Tabu Search (TS). The explanation and justiﬁcation of the metaheuristic parameters is
21

Shared-Memory Hyperheuristics

J.-M Cutillas-Lozano and D. Gim´enez

not the goal of this work. Those included in our implementation of the scheme are enumerated,
so that they can be referenced in the rest of the paper:
• Initialize: IN EIni elements are generated, and a subset with F N EIni elements
is selected for the iterations. Some initial elements are improved. A parameter P EIIni
indicates the percentage of elements to be improved, and the intensity of the improvement
is represented by a parameter IIEIni. The parameter ST M Ini is used for the extension
of Tabu short-term memory in the initialization improvement.
• EndCondition: The end condition is common to the diﬀerent metaheuristics. It consists
of a maximum number of iterations (M N IEnd) or a maximum number of iterations
without improving the best solution (N IREnd).
• Select: The elements are grouped into two sets, the best and worst according to the
objective function. The number of best elements is N BESel and that of worst elements
N W ESel.
• Combine:
The total number of elements to obtain by combination is
2(N BBCom+N BW Com+N W W Com), where the three parameters represent the number of combinations of the best with the best elements, the number of the best with the
worst and the number of the worst with the worst.
• Improve: As in the improvement in the initialization, P EIImp, IIEImp and SM IImp
represent the percentage of elements to be improved, the intensiﬁcation of the improvement and the short-term memory in the improvement of the elements generated in the
combination; and P EDImp, IDEImp and SM DImp represent the corresponding values
in a diversiﬁcation.
• Include: The N BEInc best elements are maintained in the reference set, and the others
are selected from the remaining elements. LT M Inc is a Tabu parameter (long-term
memory) that allows the tracking of individuals most frequently explored.
There is a set with 20 metaheuristic parameters with which it is possible to adapt the metaheuristics to the target problem. The parallelism parameters represent the number of threads
to use in each function. Diﬀerent number of threads may be selected for diﬀerent functions. In
this way, the appropriate number of threads in each function can be selected depending on the
cost of the function, which depends on the values of the metaheuristic parameters. The parallel
scheme is used here to apply a common auto-tuning technique to select the optimum number
of threads, so obtaining low execution times. Two basic parallel schemes are identiﬁed. In the
ﬁrst scheme the elements of a set are treated independently, and the number of threads to work
with in a loop is selected. The second scheme has two parallelism levels and can be used to
obtain ﬁne or grained parallelism by varying the number of threads at each parallelism level.
The parallelism parameters included in our implementation are:
• Initialize: A for loop is used to generate the initial set of elements, so a one-level
scheme is used, with a number of threads T GEIni. The improvement follows the twolevel scheme, with two parameters (T I1Ini and T I2Ini) for the number of threads.
• Combine: Pairs of elements are combined in a loop, so we have T CP Com threads to
work with in the combination loop.
22

Shared-Memory Hyperheuristics

J.-M Cutillas-Lozano and D. Gim´enez

• Improve: As in the improvement in the initialization, two-level parallelism functions are
used for the improvement of the elements. Three improvement functions are considered:
for the improvement of elements in the reference set, for those obtained in the combination,
and for those obtained though diversiﬁcation. So there are six parallelism parameters:
T R1Imp, T R2Imp, T C1Imp, T C2Imp, T D1Imp, T D2Imp.
• Include: A one-level parallelization with T IEInc threads is considered for the inclusion
of elements.
There is a set of 11 parallelism parameters in 4 basic functions, with three parts of the
code with loop parallelization (the initialization, the combination and the inclusion) and four
improvements where two-level parallelism is used. For auto-tuning, the execution time in these
seven parts will be modeled independently, but with a common methodology in which the
values of the metaheuristic parameters are considered. The number of threads to use in each
part will be selected independently from the diﬀerent models, thus depending on the values of
the metaheuristic parameters.

2.2

Hyperheuristics Based on Parameterized Metaheuristic Schemes

The same parameterized scheme of metaheuristics is used for the implementation of hyperheuristics, which will work by searching the metaheuristic space determined by the values of
the metaheuristic parameters, and which can in turn use the same metaheuristic scheme for
hyperheuristics development. An element is represented by an integer vector M etaheurP aram
that encodes the set of parameters that characterizes a metaheuristic. The number and meaning of the parameters is determined by the basic metaheuristics integrated in the scheme, the
implementation of the basic functions and the parameters that can be varied in the functions.
In our implementation, an element has 20 components, with values in ranges which depend
on the possible values of the parameter itself and also on some restrictions imposed on the
hyperheuristic to reduce the search space.
The set of individuals constitutes the reference set, which means a set of metaheuristics, with each metaheuristic being the combination/hybridation of basic metaheuristics given
by the values in M etaheurP aram. The ﬁtness value in the hyperheuristic for an element
M etaheurP aram is based on that obtained when the metaheuristic with the parameters in
M etaheurP aram is applied to the problem for which the metaheuristic is being obtained.
The metaheuristic scheme is used at two levels: for the hyperheuristic (HMS) and for the
application of the metaheuristics (MS) determined from the metaheuristic parameters for each
element of the reference set of metaheuristics. Thus, parallelism can be applied in the hyperheuristic and the metaheuristics, with a total of four parallelism levels, although it is preferable
to parallelize at a high level, and parallelism is usually only applied in the hyperheuristic.
Furthermore, due to the higher computational cost of hyperheuristics, ﬁne and coarse grained
parallelism in the improvement parts in the scheme are explored to obtain the combination with
the lowest execution time, while for metaheuristics in general the parallelization is preferable
only at the highest level.

3

Modeling and Auto-Tuning Methodology

To reduce the execution time it is necessary to select the values of the parallelism parameters
appropriately. A theoretical model of the execution time is obtained for each function, and the
23

Shared-Memory Hyperheuristics

J.-M Cutillas-Lozano and D. Gim´enez

number of threads of a loop or the number of threads in the ﬁrst and the second parallelism
levels is established.
The auto-tuning process used in [4] for linear algebra routines is adapted to the metaheuristic scheme. A problem of minimization of electricity consumption in exploitation of wells
[6] is used to show the modeling and auto-tuning methodology. The metaheuristics used to
show how the methodology works are Genetic algorithm, Scatter Search, GRASP and Tabu
Search, but the same methodology could be applied with another set of basic metaheuristics
and implementations of the basic functions in the metaheuristic scheme. The process has three
phases:
• Design: The routine is developed together with its theoretical execution time. Because
two types of parallelism have been identiﬁed, two basic models can be used. For one-level
routines, where N E elements are treated (generated, combined or evaluated) the model
is:
tone−level =

kg · N E
+ kp · p
p

(1)

where kg represents the cost of the work with one individual, kp the cost of generating
one thread, and p is the number of threads.
For the two-level routines in the improvement functions, with a number of elements N E,
a probability of improvement IP and an intensiﬁcation II, the model is:
ttwo−levels =

ki · N E · IP · II
+ kp,1 · p1 + kp,2 · p2
100 · p1 · p2

(2)

where ki represents the cost of one step in the improvement of one element, kp,1 and kp,2
the costs of generating threads at the ﬁrst and second level, and p1 and p2 the numbers
of threads at each level.
The model for each function in the scheme is obtained by substituting the values of N E,
IP and II for the corresponding metaheuristic parameters in the function. The models
so obtained are very simple and do not consider some architectural aspects like memory
or threads allocation, but their simplicity facilitates their use, and satisfactory results
are obtained. Furthermore, in some execution environments those system-architecture
aspects can not be considered when the code runs; for example, when sending the job
to a queue, the system decides the cores where the threads are mapped and the data
allocation.
• Installation: When the shared-memory parameterized scheme is installed in a particular
system, the value of the parameters inﬂuenced by the system are estimated. The system
parameters are the kg , ki , kp , kp,1 and kp,2 for each function. The estimation can be made
through experimentation with each basic function in the metaheuristic, for some values
of the metaheuristic parameters and parallelism parameters and least square adjustment.
We give an example of the work done in this phase by summarizing the results of the
installation of the scheme in an HP Integrity Superdome SX2000 with 128 cores of Intel Itanium-2 dual-core Montvale with shared-memory. We call this system Ben. The
optimum number of threads varies with the number of elements, and we are interested
in selecting a number of threads close to the optimum from a small number of elements
(for low installation times). As an example for one-level routines, in the initialization in a
24

Shared-Memory Hyperheuristics

J.-M Cutillas-Lozano and D. Gim´enez

hyperheuristic the model in equation 1 is used, and parameters kg and kp in the model are
obtained by least-squares with N E = IN EIni = 5. The values obtained are kg = 0.577
and kp = 0.0491 (in seconds). For a two-level routine, like the routine to improve elements
after the initial generation, the values of the parallelism parameters are obtained by leastsquares with experiments with parameters for the hyperheuristic N E = IN EIni = 10,
IP = P EIIni = 100 and II = IIEIni = 1. The results are ki = 1.21, kp,1 = 0.104 and
kp,2 = 0.0989 (in seconds).
• Execution: At execution time, the number of threads in each basic function is selected
from the theoretical execution time, with the values of the metaheuristic parameters being
those of the metaheuristic (or hyperheuristic) we are experimenting with and the values of
the system parameters estimated in the installation phase. The number of threads which
gives the theoretical minimum execution time is obtained by minimizing the corresponding
equation after substituting in it the values of the metaheuristic and system parameters.
The values of the parallelism parameters are obtained as a function of the metaheuristic
parameters, so the equations are valid for the diﬀerent metaheuristics or hyperheuristics
in the particular computational system where the scheme is installed.
As an example, for the initial generation of the reference set, for the experiments carried
out in our system, the optimum number of threads is:

popt =

√
kg
· IN EIni = 3.43 · IN EIni
kp

(3)

and for the improvement of the generated elements the optimum number of threads in
the two-level function is:
p1,opt = 0.479 ·
p2,opt = 0.505 ·

4

√
3
√
3

IN EIni · P EIIni · IIEIni

(4)

IN EIni · P EIIni · IIEIni

(5)

Experimental Results

For the experiments, we consider a problem of electricity consumption in exploitation of wells.
A deeper analysis of the problem can be found in [5]. The water system consists of a series of
pumps (B) of known power, located in wells, that draw water ﬂow along a daily time range R.
The total ﬂow is the sum of the ﬂows contributed by each well. The pumps may be running or
idle at a given time. The pumps operate electrically and the electricity has a daily cost which
should be minimal:
R

B

Ce =

Ti Pj Ni Xij

(6)

i=1 j=1

where Ce represents the cost of the electricity consumed by the combination of pumps selected
in a day; Ti is the cost of the electricity in the range i; Pj is the electric power consumed
by the pump j; Ni is the number of hours of pump operation in the time slot i; and Xij
represents a binary element of a matrix with values 1 or 0 for pump on or oﬀ. An element for
25

Shared-Memory Hyperheuristics

IN EIni
100
500

threads
exp
mod
48
55
121
122

exp
27
77

J.-M Cutillas-Lozano and D. Gim´enez

speed-up
mod
exp-auto
27
25
61
75

(a) MS one-level parallel routine

Comb.
c1
c2

threads
exp
mod
89
67
128
150

exp
35
78

speed-up
mod
exp-auto
17
21
51
78

(b) MS two-level parallel routine

Table 1: Comparison of the highest experimental speed-up (exp), the modeled speed-up (mod) and
that obtained when using the auto-tuning methodology (exp-auto), (a) for IN EIni = 100 and 500
in the one-level parallel routine and (b) for two combinations of IN EIni, P EIIni and IIEIni (c1:
100,50,10; c2: 500,100,5) in the two-level parallel routine of Initialize, when applying the MS in Ben.

threads
speed-up
IN EIni exp mod exp mod exp-auto
20
22 15 11
8
8
100
24 34 12 17
12

threads
speed-up
Comb. exp mod exp mod exp-auto
c1
9×8 6×7 14 15
11
c2
9×4 8×9 15 24
14

(a) HMS one-level parallel routine

(b) HMS two-level parallel routine

Table 2: Comparison of the highest experimental speed-up (exp), the modeled speed-up (mod) and
that obtained when using the auto-tuning methodology (exp-auto), (a) for IN EIni = 20 and 100 in the
one-level parallel routine and (b) for two combinations of IN EIni, P EIIni and IIEIni (c1: 50,50,1;
c2: 100,50,1) in the two-level parallel routine of Initialize, when applying the HMS in Ben.

the metaheuristic solving the problem is represented by the binary matrix, X, of size B × R,
which encodes the set of pumps distributed in diﬀerent time slots. The set of individuals
constitutes a population. Not all possible combinations result in feasible individuals, and each
time an individual is generated or modiﬁed, ﬁve constraints are evaluated: demand satisfaction,
minimum ﬂow maintenance, compliance with maximum exploitation volumes for each well,
maintaining the average conductivity below the limit and compliance with maximum depths of
dynamic levels. This means in some cases that obtaining a new individual is time-consuming.
Furthermore, for large exploitation systems the number of wells and of time ranges can be large.
So, to apply diﬀerent metaheuristics eﬃciently a shared-memory parameterized scheme is used,
and the inclusion of an autotuning methodology in the scheme is studied.
To validate the auto-tuning methodology, the optimum number of threads and the maximum
speed-up achieved are calculated from the models for diﬀerent metaheuristic parameters using
the system parameters obtained in the installation. These system parameters were calculated
using small values of the metaheuristic parameters to reduce the installation time. For the
application of the MS, the values were IN EIni = 20 for the initialization and IN EIni = 20,
P EIIni = 50, IIEIni = 20 for the improvement in the initialization. For the HMS these values
were IN EIni = 5, IN EIni = 10, P EIIni = 100 and IIEIni = 1. The diﬀerences between
the values of the parameters of MS and HMS are explained by the higher execution time of the
hyperheuristic, which makes it necessary to have lower values for a low installation time.
Tables 1 and 2 compare the results of the initial generation of the reference set (a) and of
the improvement of elements (b) for two parameter combinations using the MS and the HMS.
The number of threads selected with the auto-tuning methodology is not far from the best
values obtained experimentally and, as a consequence, the speed-up achieved with auto-tuning
is not far from the experimental maximum and the auto-tuning methodology is useful for the
reduction of the execution time.
Since the metaheuristic scheme is the same, similar results would be expected in both
26

Shared-Memory Hyperheuristics

ks · 104
kp,1 · 103
kp,2 · 103

J.-M Cutillas-Lozano and D. Gim´enez

One-level parallel
Ini
Com
4.43
5.69
3.96
2.61
-

routines
Inc
1.44
58.3
-

Two-level parallel routines
Imp-Ini
Imp-Ref
Imp-Com
Div
6.05
6.01
1.20
107
3.08
1.91
3.38
63.8
1.56
8.38
23.2
0.915

(a) MS model constants

ks · 102
kp,1 · 102
kp,2 · 102

One-level parallel routines
Ini
Com
Inc
1.45
2.91
0.296
0.541
0.679
2.44
-

Two-level parallel routines
Imp-Ini
Imp-Ref
Imp-Com
Div
15.7
25.5
52.3
26.2
5.66
2.55
8.86
2.22
7.09
3.25
34.8
4.84

(b) HMS model constants

Table 3: Values of the constants of the model for all the functions when applying (a) the MS and (b)
the HMS, in Saturno.

m1
m2
h1
h2

m1
m2
h1
h2

INEIni
20
100
10
20

NWWCom
10
5
10
5

FNEIni
20
50
10
20
PEIImp
100
50
100
20

PEIIni
50
100
100
50
IIEImp
20
10
1
3

IIEIni
20
10
1
3

STMIni
7
7

SMIImp
7
7

NBESel
10
25
5
10

PEDImp
50
100
100
20

NWESel
10
25
5
10

IDEImp
20
20
1
3

NBBCom
20
100
20
50

SMDImp
7
7

NBWCom
5
20
5
10

NBEInc
10
25
10
10

LTMInc
7
7

Table 4: Values of the metaheuristic parameters used for the auto-tuning experiments when applying
the MS (m1 and m2) and the HMS (h1 and h2), in Saturno.

cases, although there may be diﬀerences due to diﬀerent implementations. For example, in the
improvement function of the MS, the second level was used to start more threads to work on
the improvement of the ﬁtness function (more neighbors are analyzed) but not to reduce the
execution time, so the number of threads of second level is constant.
So far we have checked the validity of the auto-tuning methodology in a large system like
Ben, where the access to the shared-memory supposes an additional delay in the execution time.
For more general conclusions, the complete auto-tuning process has been analyzed in a smaller
NUMA system called Saturno comprising 24 cores (4 hexa-cores), with IntelXeon E7350 processors. Table 3 gives the values of the constants of the model obtained in the installation phase in
Saturno for MS and HMS. Taking into account the values of these constants, we can verify the
validity of the methodology of auto-tuning. Table 4 gives the values of typical metaheuristic
parameters used in the MS and the HMS. While the metaheuristic executes problem instances
directly, the hyperheuristic is more time-consuming, since it executes diﬀerent metaheuristics
at the same time to optimize the resolution of the problem. So, these parameter values have
been chosen because they produce metaheuristics and hyperheuristics of intermediate size, so
allowing for a comprehensive study in a relatively reduced time.
Table 5 shows the optimum number of threads of the ﬁrst and second level of parallelism obtained with auto-tuning in Saturno for all the functions in the scheme for the four metaheuristic
parameter combinations in Table 4. Results are presented for the application of the MS and
27

Shared-Memory Hyperheuristics

m1
m2

h1
h2

J.-M Cutillas-Lozano and D. Gim´enez

One-level parallel routines
TGEIni
TCPCom
TIEInc
2
3
1
3
5
1

level
p1
p2
p1
p2

TI1Ini
6
14

One-level parallel routines
TGEIni
TCPCom
TIEInc
5
12
3
7
17
4
-

Two-level parallel routines
TR1Imp
TC1Imp
TD1Imp
11
5
6
9
5
13

TI Ini
3
3
5
4

Two-level parallel routines
TR Imp
TC Imp
TD Imp
5
9
6
4
2
3
5
10
7
4
2
3

Table 5: Values of the parallelism parameters for four metaheuristic parameter combinations, in
Saturno. Optimum number of threads of the ﬁrst level of parallelism for all the functions when applying
the MS (m1 and m2). Optimum number of threads (levels one and two of parallelism) for all the
functions when applying the HMS (h1 and h2).

m1
m2
h1
h2

max
1
5
3
4

max
2

2
6
4
5

auto
2
6
5
5

Table 6: Speed-ups for various metaheuristic parameter combinations when applying all the functions
in the MS (m1 and m2 in Tables 4 and 5) and all the functions in the HMS (h1 and h2 in Tables 4
and 5). Experimental values obtained with the maximum number of threads available (max), values
obtained with half of the maximum number of threads ( max
) and values obtained with auto-tuning
2
(auto), in Saturno.

the HMS. The value of the second level parameter was ﬁxed to 1 for the MS because the second
level was used to start more threads to work on the improvement of the ﬁtness function, but
not to reduce the execution time.
Finally, in Table 6 we can see the speed-up achieved with the optimal number of threads given
by the model, and this is compared with the value achieved when parallelism parameters are
selected without a systematic method. We consider the maximum number of threads available
and the half of the maximum number of threads. Other values could have been chosen, but this
is a good approximation for comparison with our auto-tuning methodology. In all the cases the
speed-up obtained with the auto-tuning methodology improves or equals the values obtained
with a number of threads selected in a non-optimal manner. This result is also observed in each
function separately, which gives us an idea of the validity of the methodology.

5

Conclusions and Future Work

An auto-tuning methodology for parameterized shared-memory metaheuristic schemes that
can in turn be applied to hyperheuristics based on metaheuristic schemes has been shown. A
problem of minimization of electricity consumption in exploitation of wells has been used as a
test case, but the methodology can be applied to other problems. As far as we know, this is the
ﬁrst time that auto-tuning techniques are applied to parallel hyperheuristics. The methodology
provides satisfactory values for the number of threads to use in the application of the parallel
28

Shared-Memory Hyperheuristics

J.-M Cutillas-Lozano and D. Gim´enez

metaheuristics and hyperheuristics in NUMA systems.
The validity of the hyperheuristic approach should be analyzed with other applications, and
more research into the implementation of the hyperheuristic to improve the metaheuristic it
ﬁnds should be carried out. One possibility to improve the application of the hyperheuristic is
to determine search ranges for each metaheuristic parameter, so reducing the possible values
of the elements in the metaheuristic with which the hyperheuristic is implemented. For this,
statistical analyses like those in [3] can be used.
Similar parameterized, parallel metaheuristic schemes together with the corresponding autotuning methodology should be developed for message-passing or GPU. This could be preferable
for the application of hyperheuristics with large reference sets or with a high ﬁtness function
cost.

5.1

Acknowledgments

This work was supported by the Spanish MINECO, as well as European Commission FEDER
funds, under grant TIN2012-38341-C04-03. The authors acknowledge the resources and assistance provided by the Supercomputing Center of the Scientiﬁc Park Foundation of Murcia.

References
[1] E. Alba. Parallel Metaheuristics: A New Class of Algorithms. Wiley-Interscience, 2005.
[2] F. Almeida, D. Gim´enez, and J.-J. L´
opez-Esp´ın. A parameterized shared-memory scheme for
parameterized metaheuristics. The Journal of Supercomputing, 58(3):292–301, 2011.
[3] F. Almeida, D. Gim´enez, J.-J. L´
opez-Esp´ın, and M. P´erez-P´erez. Parameterised schemes of metaheuristics: basic ideas and applications with Genetic algorithms, Scatter Search and GRASP.
IEEE Trnsactions on Systems, Man and Cybernetics, Part A: Systems and Humans, 43(3):570–
586, 2013.
[4] J. Cuenca, D. Gim´enez, and J. Gonz´
alez. Architecture of an automatic tuned linear algebra library.
Parallel Computing, 30(2):187–220, 2004.
[5] L.-G. Cutillas-Lozano. Metaheur´ıstica aplicada a la optimizaci´
on de los criterios de producci´
on de
aguas subterr´
aneas. Sondea Project (in Spanish). Final-studies dissertation, University of Alicante,
2012.
[6] L.-G. Cutillas-Lozano, J.-M. Cutillas-Lozano, and D. Gim´enez. Modeling shared-memory metaheuristic schemes for electricity consumption. In Distributed Computing and Artiﬁcial Intelligence
- 9th International Conference, pages 33–40, 2012.
¨
[7] G. Kendall G. Ochoa E. Ozcan
E. K. Burke, M. Hyde and J. Woodward. A Classiﬁcation of
Hyper-heuristic Approaches, pages 449–468. Springer, 2010. In Michel Gendreau, Jean-Yves
Potvin, editors, Handbook of Meta-heuristics.
[8] M. Frigo and S. G. Johnson. FFTW: An Adaptive Software Architecture for the FFT. In IEEE
International Conference on Acoustics, Speech, and Signal Processing, volume 3, pages 1381–1384,
1998.
[9] T. Katagiri, K. Kise, and H. Honda. Eﬀect of auto-tuning with user’s knowledge for numerical
software. In J. L. Gaudiot S. Vassiliadis and V. Piuri, editors, Proceedings of the First Conference
on Computing Frontiers, pages 12–25, 2004.
[10] G. Luque and E. Alba. Parallel Genetic Algorithms: Theory and Real World Applications.
Springer, 2011.
[11] El-Ghazali Talbi. Metaheuristics - From Design to Implementation. Wiley, 2009.
[12] R. Clinton Whaley, A. Petitet, and J. Dongarra. Automated empirical optimizations of software
and the ATLAS project. Parallel Computing, 27(1-2):3–35, 2001.

29

