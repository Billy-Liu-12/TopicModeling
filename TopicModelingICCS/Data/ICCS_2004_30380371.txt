Balanced RM2: An Improved Data Placement
Scheme for Tolerating Double Disk Failures in
Disk Arrays
Dae-Woong Kim, Soon-Ho Lee, and Chan-Ik Park
Department of Computer Science and Engineering/PIRL
Pohang University of Science and Technology
Pohang, Kyungbuk 790-784, Republic of Korea
cipark@postech.ac.kr

Abstract. There is a growing demand for high data reliability beyond
what current RAIDs can provide and users need various levels of data reliability. In order to meet these requirements, an eﬃcient data placement
scheme called RM2 has been proposed in [1], which enables a disk array
system to tolerate double disk failures. However, RM2 has high data reconstruction overhead due to data dependency between parity groups in
the case of double disk failures, suﬀering from large performance ﬂuctuation according to disk failure patterns. To minimize the reconstruction
overhead, this paper presents an improved data placement scheme called
Balanced RM2. Experimental results show that the performance ﬂuctuation of the Balanced RM2 becomes much less than that of the RM2
regardless of read or write. It is also shown that Balanced RM2 provides
better performance than RM2 by 5 ∼ 20% in the case of double disk
failures.

1

Introduction

There has been much research on improving I/O performance, known as data
declustering and disk striping, in disk array systems [2,3,4]. However, incorporating a large number of disks into a disk array system makes the system more
prone to failure than a single disk would [3].
In general, high reliability can be achieved by redundant information. For
example, RAID (Redundant Arrays of Independent Disks) enables a disk array
system to tolerate a single disk failure by maintaining parity information. In
order to meet reliability requirement beyond what RAID can provide, that is,
tolerate more than one disk failures, there has been much research on high reliability of disk arrays [1,5,6]. Blaum et al. have proposed EVENODD in [6]. This
The authors would like to thank the Ministry of Education of Korea for its support
toward the Electrical and Computer Engineering Division at POSTECH through its
BK21 program. This research was also supported in part by HY-SDR IT Research
Center and in part by grant No. R01-2003-000-10739-0 from the Basic Research
Program of the Korea Science and Engineering Foundation.
M. Bubak et al. (Eds.): ICCS 2004, LNCS 3038, pp. 371–378, 2004.
c Springer-Verlag Berlin Heidelberg 2004

372

D.-W. Kim, S.-H. Lee, and C.-I. Park

method maintains two types of redundant information: horizontal redundancy
and diagonal redundancy. Note that EVENODD requires only two additional
disks (which is optimal). However, EVENODD has an apparent restriction; the
number of information disks has to be a prime number. Park has proposed RM2
in [1] which does not exhibit any explicit I/O bottleneck by enforcing the even
distribution of parity information across all disks. However, RM2 has high data
reconstruction overhead due to data dependency between parity groups in the
case of double disk failures, suﬀering from large performance ﬂuctuation according to disk failure patterns.
This paper presents an improved data placement scheme called Balanced
RM2 which distributes the load of data reconstruction as evenly as possible over
all the remaining disks under disk failures.
The rest of the paper is organized as follows. Section 2 describes the RM2
architecture. Section 3 presents the Balanced RM2 architecture and its data
reconstruction method. Section 4 gives experimental results for various cases.
Finally, concluding remarks and future works are given in Section 5.

2

Backgrounds

A disk array is assumed to consist of N disks each of which handles independent
requests in parallel. The address space of a disk array is logically structured
with a set of stripe units. A stripe unit is a basic unit of data striping consisting
of a group of logically contiguous blocks. It is placed consecutively on a single
disk before placing another stripe unit on a diﬀerent disk. A stripe consists of N
stripe units each of which has the same physical address in each disk. If a stripe
contains only data units, then it is called a data stripe. If a stripe consists of
parity units only, then it is called a parity stripe. A parity group is deﬁned as a
set of data units and a parity unit, where the parity unit is computed from the
set of data units. We deﬁne a stripe group as a set of data stripes and a parity
stripe, which covers all stripe units in a given parity group. In RM2, data and
parity mapping of a stripe group are deﬁned by an N × N redundancy matrix.
In the redundancy matrix, each column corresponds to a disk and each row
corresponds to a parity group. Each column has one entry with a value of −1
and two entries with a value of k for 1 ≤ k ≤ M − 1 where M is the size of a
stripe group. Let RMi,j be an element of the redundancy matrix with a value
ranging from −1 to M − 1. According to the value of RMi,j , the interpretation
diﬀers as follows:
– RMi,j = −1: A parity unit of the disk j belongs to the parity group i.
– RMi,j = 0: No meaning.
– RMi,j = k for 1 ≤ k ≤ M − 1: The k-th data unit of the disk j belongs to
the parity group i.
Figure 1 represents the data layout of the RM2 architecture and its corresponding 7 × 7 redundancy matrix when the number of disks is seven. Note that
each disk in RM2 has equal amount of parity information. There are two data

Balanced RM2: An Improved Data Placement Scheme

373

Fig. 1. Data layouts of RM2 and its redundancy matrix for N = 7 and M = 3 where
N is the number of disks and M is the size of a stripe group

stripes and one parity stripe in a stripe group, i.e., M = 3. The amount of disk
space used to store parity information is represented by the reciprocal of the
size of a stripe group (i.e., 1/M ), called redundancy rate. For example in Figure
1, the redundancy rate is 1/3 since a stripe group consists of two data stripes
and one parity stripe, i.e., the size of a stripe group is 3. Pi represents the parity unit in the i-th parity group and Di,j means that the data unit is involved
in computing parity information of both the i-th and j-th parity groups. For
example, in order to compute the parity unit P5 , we have to consider the four
data units D2,5 , D4,5 , D5,6 , and D5,1 . And, the data unit D8,11 belongs to both
parity groups P8 and P11 .

3

Balanced RM2

The main problem of RM2 is that it has high data reconstruction overhead
due to data dependency between parity groups in the presence of double disk
failures. To minimize the reconstruction overhead, this section describes how to
place data and parity to address the data dependency problem.
3.1

Dual-Symmetry

In RM2, c-independence was introduced to formulate how to place In order
to deal with the eﬃciency of data reconstructing operation, we deﬁne dualsymmetry of RM2.
Deﬁnition 1 (Dual-Symmetry). The column vector of a redundant matrix is
dual-symmetric if every positive entries are symmetrically positioned both around
the entry with a value of −1 (parity unit) and around a sequence of zero entries
simultaneously. And the redundant matrix is said to be dual-symmetric if its
column vector is dual-symmetric.

374

D.-W. Kim, S.-H. Lee, and C.-I. Park

Fig. 2. Data layouts of the Balanced RM2 and its redundancy matrix for N = 7 and
M =3

For example, Figure 1(b) shows a redundancy matrix that is c-independent,
but whose column vector is not dual-symmetric. Figure 2(b) represents the redundancy matrix that is both c-independent and dual-symmetric.
For the Balanced RM2 architecture, it is necessary for a redundancy matrix
to meet the following two conditions:
– any pair of columns is c-independent.
– the column vector is dual-symmetric.
The ﬁrst condition guarantees that all failed data blocks can be reconstructed in
the case of one or two disk failures [1]. And, the second condition enforces that
the load of data reconstruction are evenly distributed, which will be described
in detail later.
Figure 2 represents the data layout of the Balanced RM2 architecture and
its redundancy matrix when N = 7 and M = 3. It looks similar to that of RM2
(see Figure 1), but note that all stripe units of a parity group are symmetrically
positioned around the parity unit.
3.2

Recovery Points and Sequences

A recovery point is deﬁned as a parity group that has only one inaccessible data
unit owing to disk failures. If the inaccessible data unit is a parity unit, then the
recovery point will not be used as a starting sequence for data reconstruction.
Otherwise, it can be regarded as a starting sequence for data reconstruction. A
recovery path is deﬁned as the sequence of all parity groups reachable from a
recovery point. For example shown in Figure 2(c), parity groups P G0 and P G5
become recovery points when disks D2 and D3 fail. Thus, two recovery paths
{P G0, P G4, P G2} and {P G5, P G1, P G3} are found. If a recovery path ends
up with another recovery point, then it is called a bi-directional recovery path.
Otherwise, it is called a uni-directional recovery path.
A recovery sequence is deﬁned as the shortest recovery path which starts from
a recovery point and ends up with a parity group including the failed data unit.
Following the recovery sequence, we can reconstruct the failed data unit. The
length of a recovery sequence represents the number of parity groups involved in

Balanced RM2: An Improved Data Placement Scheme

375

Table 1. The length of minimum and maximum recovery sequence depending on the
distance between two failed disks, where N = 7 and M = 3
Distance between
RM2
two failed disks minimum maximum
1
4
1
2
1
2
3
1
1
4
1
1

Balanced RM2
minimum maximum
1
2
1
1
1
1
1
1

the recovery sequence. For example, the recovery sequence length in RAID5 is
one, that is, accessing only one parity group is enough for data reconstruction.
For example shown in Figure 2, consider the case where we want to reconstruct the ﬁrst data unit, D4,2 , in the disk D3 when the disks D2 and D3 fail
(denoted as shaded columns). Both the parity group P G2 and P G4 are the parity groups that include the failed data unit D4,2 since RM2,3 and RM4,3 have
a value of one in the redundancy matrix. From the parity group P G4, we can
ﬁnd the second data unit of D2, D4,0 , as a pivot because P G4 has another failed
data unit. Then, the recovery sequence includes the parity group P G0 because
P G0 also depends on the data unit selected as a pivot. Since P G0 has only one
broken data unit, P G0 is then a recovery point that can be a starting sequence of
data reconstruction. Thus, we get the recovery sequence that includes the parity
group P G4 and P G0. However, from the parity group P G2, we cannot further
construct a recovery sequence because the pivot for the parity group P G2 is a
parity unit.
We know that the length of a recovery sequence has eﬀects on the data
reconstruction performance in the case of disk failures.
Lemma 1. The Balanced RM2 has at least two recovery points in the case of
double disk failures.
Theorem 1. The length of any recovery sequence in the Balanced RM2 is at
most M − 1, where M is the size of a stripe group.
For the proof of Lemma 1 and Theorem 1, please refer to [7].
Consequently, the Balanced RM2 architecture is expected to provide better
performance than RM2 in the case of double disk failures since the worst case of
recovery sequence lengths in the RM2 is 2(M − 1) while only M − 1 in Balanced
RM2, where M is the size of a stripe group. Table 1 shows the minimum and
maximum recovery sequence depending on the distance between two failed disks.

4

Performance Evaluation

This section evaluates the performance, in terms of throughput and response
time, of RM2, EVENODD, and Balanced RM2 under a highly concurrent workload of small reads and writes.

376

D.-W. Kim, S.-H. Lee, and C.-I. Park

Table 2. Disk operation overheads in each disk array architecture according to the
number of disk failures. CR and CW represent the cost of read operations and write
operations, respectively. N denotes the number of disks and G represents the size of a
parity group. L and L∗ denote the number of parity groups involved in reconstructing
the failed data unit in RM2 and Balanced RM2 each other
Disk arrays

Single failure
Double failures
Read cases
Write case
Read case
Write case
EVENODD (N − 1) CR (N − 1) CR + 2CW 2 (N − 1) CR 2 (N − 1) CR + 2CW
RM2
(G − 1) CR (G − 1) CR + 2CW L (G − 1) CR L (G − 1) CR + 2CW
Balanced RM2 (G − 1) CR (G − 1) CR + 2CW L∗ (G − 1) CR L∗ (G − 1) CR + 2CW

Simulation Environments. Our experiments were run on RAIDframe [8], a
simulator in which various RAID architectures can be modeled and evaluated.
RAIDframe’s synthetic workload generator makes highly concurrent sequences of
user I/O requests for disk accesses from multiple client threads. Each individual
user request is converted into a set of read and write accesses on the speciﬁc disk
in the disk array.
The individual disk of the array was modeled with HP 2247 whose revolution
time, single cylinder seek time and average seek time are 11.12 ms, 2.5 ms
and 10 ms, respectively. The stripe unit was conﬁgured to be of 8 sectors, i.e.,
4 Kbytes. For disk queue management, the shortest-seek-time-ﬁrst (SSTF)
queueing policy is adopted in a disk queue that can hold 10 outstanding
requests. The synthetic workload generator incessantly makes random and
independent 4 Kbyte accesses aligned to the stripe unit boundaries. These
workloads are chosen because small requests are typically the most demanding
for many RAID architectures [9,10,11].
Each disk array is conﬁgured as N = 15 and M = 3. The experiments
are performed according to the increase of client threads, which shows us the
saturation point of throughput in each disk array architecture.
Analytic Comparison. Table 2 shows how many disk operations are needed
to serve small-sized user requests in each architecture depending on the number
of disk failures. Note that each array has diﬀerent values in the size of a parity
group as well as the length of a recovery sequence. For example, the size of
a parity group in the RM2 architecture is the same as that in the Balanced
RM2 architecture whereas the RM2 has the longer recovery sequence than the
Balanced RM2.
Comparison under Double Disk Failure. In the case of read operations,
it is expected that the RM2 and the Balanced RM2 have much better performance than the EVENODD due to the declustering eﬀect [10]. In detail, the
Balanced RM2 architecture provides better performance than the RM2 architecture since it needs smaller number of parity groups in reconstructing the
failed data unit. Figure 3(a) shows that the Balanced RM2 architecture reaches

Balanced RM2: An Improved Data Placement Scheme

377

Fig. 3. Performance comparison for the case of double disk failures

Fig. 4. Throughput according to the distance between two failed disks

its point of saturation at about 740 reqs/sec whereas the RM2 architecture at
about 580 reqs/sec and the EVENODD architecture at about 460 reqs/sec.
In the case of the write operations, RM2 and Balanced RM2 have non trivial
performance advantage over EVENODD as shown in Figure 3(b). Furthermore,
the performance of Balanced RM2 is slightly better than RM2 due to the smaller
number of parity groups involved in reconstructing the failed data unit. In the
case of write operations, the Balanced RM2 architecture reaches its point of
saturation at about 194 reqs/sec, compared to about 190 reqs/sec for the RM2
architecture and 175 reqs/sec for the EVENODD architecture.
Comparison According to the Distance between Two Failed Disks.
Figure 4 demonstrates how the distance between two failed disks aﬀects on the
performance of read and write operation in the case of double disk failures. It
is shown that Balanced RM2 provides a better throughput than RM2. Remember that the maximum length of a recovery sequence in the Balanced RM2 is
(M − 1) whereas 2(M − 1) in RM2. The Balanced RM2 architecture has better performance than the EVENODD architecture because of the declustering
eﬀect. However, the declustering eﬀect becomes negligible due to long recovery
sequences in the case of adjacent disk failures.

378

5

D.-W. Kim, S.-H. Lee, and C.-I. Park

Conclusions

This paper has presented an improved data and parity placement scheme called
Balanced RM2 to minimize high data reconstruction overhead in RM2. Minimizing data reconstruction overhead is achieved by reducing the number of parity
groups involved in reconstructing the failed data unit.
The experiments have been conducted by the RAIDframe under a highly
concurrent workload of small-sized reads and writes. It has been shown that the
Balanced RM2 provides better performance than the RM2 and the EVENODD
in data reconstruction operations.
The data and parity placement schemes using the redundancy matrix are
applicable to many research ﬁelds for disk arrays. Currently, we are considering
the data and parity placement schemes to tolerate n disk failures in disk arrays.

References
1. Park, C.I.: Eﬃcient placement of parity and data to tolerate two disk failures
in disk array systems. IEEE Transactions on Parallel and Distributed Systems 6
(1995) 1177–1184
2. Kim, M.Y., Tantawi, A.N.: Asynchronous disk interleaving. IEEE Transactions
on Computers 40 (1991)
3. Patterson, D.A., Gibson, G.A., Katz, R.H.: A case for redundant array of inexpensive disks (raid). In: ACM SIGMOD Conference Proceedings, Chicago, Illinois
(1988)
4. Narasimha, R.A.L., Banerjee, P.: An evaluation of multiple-disk i/o systems. IEEE
Transactions on Computers 38 (1989) 1680–1690
5. Alvarez, G.A., Burkhard, W.A., Cristian, F.: Tolerating multiple failures in raid
architectures with optimal storage and uniform declustering. In: Proceedings of
the 24th Annual ACM/IEEE International Symposium on Computer Architecture(ISCA ‘97), Denver, Colorado (1997)
6. Blaum, M., Brady, J., Bruck, J., Menon, J.: Evenodd: An eﬃcient scheme for tolerating double disk failures in raid architectures. IEEE Transactions on Computers
44 (1995)
7. Kim, D.W., Lee, S.H., Park, C.I.: Balanced rm2: An improved data placement
scheme for tolerating double disk failures in disk arrays. Technical Report CSESSL-2003-08, System Software Lab., Department of Computer Science and Engineering, Pohang University of Science and Technology (2003)
8. Courtright, W.A., Gibson, G., Zelenka, J.: Raidframe: Rapid prototyping for disk
arrays. In: Proceedings of the 1996 Conference on Measurement and Modeling of
Computer Systems (SIGMETRICS). Volume 24. (1996) 268–269
9. Stodolsky, D., Gibson, G., Holland, M.: Parity logging: Overcoming the small write
problem in redundant disk arrays. In: Proceedings of International Symposium on
Computer Architecture, San Diego CA (1993)
10. Holland, M., Gibson, G.A.: Parity declustering for continuous operation in redundant disk arrays. Journal of Distributed and Parallel Databases 2 (1994)
11. Courtright, W.A., Gibson, G., Zelenka, J.: A structured approach to redundant
disk array implementation. In: Proceedings of the International Symposium on
Performance and Dependability. (1996)

