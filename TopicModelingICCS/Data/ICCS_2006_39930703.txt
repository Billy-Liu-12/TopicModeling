Learning in a Multi-agent System as a Mean for
Eﬀective Resource Management
´ zy´
Bartlomiej Snie˙
nski and Jaroslaw Ko´zlak
AGH University of Science and Technology, Institute of Computer Science
Krak´
ow, Poland
{sniezyn, kozlak}@agh.edu.pl

Abstract. In this paper symbolic, supervised learning is used in a multiagent system for resource management. Environment is a Fish Bank
game, where agents manage fishing companies. Rule induction is applied
to generate ship allocation and cooperation rules. In this article system
architecture and learning process are described and experimental results
comparing performance of several types of agents are presented. The
results obtained confirm that applying a supervised learning algorithm
in a multi-agent system may improve resource management.

1

Introduction

One of the main problems in a modern society is a problem of eﬃcient resource
management. It may concern diﬀerent domains like logistics, ecology, production
planning or computer network designing. The management is especially diﬃcult
for common, renewable resources, i.e. the ones whose quantity in the environment
increases until reaching a given maximum value, and the speed of this increase
depends on the quantity of resources.
An exhaustion of common resources can be considered as an example of a
crisis situation, which makes it impossible to regenerate them in the future and,
as a consequence, makes the functioning of entities, which need these resources,
impossible. Hence, it is important to guarantee a resource consumption on the
appropriate level, and to minimize the probability of crisis situations arising as
well as their scale.
The goal of our work is to provide a strategy of resource management and to
guarantee that a system will stay in a safe state, i.e. the one, which does not
lead to resources exhaustion.
To analyze such situations it is useful to apply multi-agent approach, which
allows for modelling of activities of autonomous, rational entities – agents, which
cooperate or compete one with another. Agents try to realize theirs goals in a way
that is as good as possible, whereas a cooperation may be necessary to avoid
an exhaustion of resources and appearing a crisis situation. The cooperation
may occur as an auto-modiﬁcation of actions concerning resources consumption
performed by an agent or by negotiations and contracts.
V.N. Alexandrov et al. (Eds.): ICCS 2006, Part III, LNCS 3993, pp. 703–710, 2006.
c Springer-Verlag Berlin Heidelberg 2006

704

´ zy´
B. Snie˙
nski, J. Ko´zlak

One of the main problems in the development of such systems is designing
an appropriate strategy for resource allocation and determination, when negotiations and contracts should be introduced and which character they should
have. Applying learning algorithms allows to overcome such problems. One can
implement an agent that is not perfect, but it improves its performance.
In this paper results of application of symbolic, supervised learning in a multiagent system are presented. As an environment Fish Bank game is used [1]. It
is a simulation where agents run ﬁshing companies that must decide how much,
and where to ﬁsh.
In the following sections learning in the multi-agent systems is described,
developed system and a learning process used is presented, and experimental
results are analyzed.

2

Learning in Multi-agent Systems

The problem of learning in multi-agent systems may be considered as a union
of research on multi-agent systems and on machine learning. Machine learning
focuses mostly on research on isolated process performed by one intelligent module. The multi-agent approach concerns the systems composed of autonomous
elements, called agents, whose actions lead to the realization of given goals. In
this context, learning is based on the observation of the inﬂuences of activities,
performed to achieve the goal by an agent itself or by other agents. Learning
may proceed in a traditional – centralized (one learning agent) or decentralized manner. In the second case more than one agent is engaged in the learning
process [2].
The learning process is strictly associated with such aspects of agents as
reasoning and decision making. The most popular learning technique in multiagent systems is reinforcement learning that allows to learn what action should
be executed in a current situation. Other techniques can be also applied. Learning
process can be based on the symbolic knowledge representation (rules, decision
trees), neural networks, models coming from game theory as well as optimization
techniques (like the evolutionary approach, tabu search etc.).
An important feature of cooperative learning is the necessity of the interactions between agents using a suitable protocol. Thanks to these interactions,
an agent may acquire additional knowledge useful to its decision making and
allowing it to make a common decision in accordance with the preferences of
the agents. An example may be a solution presented in [3]. Here, agents may
come forward with proposals, which are expressed by an additional parameter
which describes the conﬁdence of the agent about them. In their response, the
other agents participating in the negotiation process may accept the proposal
(conﬁrm), reject it (disagree), propose modiﬁcations (modify) or do not have an
opinion on the subject (no opinion). In each case, except the last one, agents
also declare their conﬁdence concerning their evaluation.

Learning in a Multi-agent System

3

705

System Description

Although Fish Banks game is designed for teaching people eﬀective cooperation in using natural resources [4] it suits to using in multi-agent systems very
well [1, 5]. In this research the game is a dynamic environment providing all necessary resources, action execution procedures, and time ﬂow (game rounds). Each
round consists of the following steps: ships and money update, ship auctions,
trading session, ship orders, ship allocation, ﬁshing, and ﬁsh number update.
Agents represent players that manage ﬁshing companies. Each company aims
at collecting maximum assets expressed by the amount of money deposited at a
bank account and the number of ships. The company earn money by ﬁshing at
ﬁsh banks.
Environment provides two ﬁshing areas: coastal and a deep-sea. Agents can
also keep their ships at the port. Cost of ﬁshing at the deep-sea is the highest.
Cost of staying at port is the lowest but such ship does not catch ﬁsh.
Initially, it is assumed that the number of ﬁsh in both banks is close to the
bank’s maximal capacity. During the game the number of ﬁsh in every bank
changes according to the following equation:
ft+1 = ft + bft (1 −

ft
) − Ct ,
fmax

(1)

where ft is a ﬁsh number at a time t, b is a birth rate (0.05 value was used in
experiments), fmax is a maximum number of ﬁsh (equal 4000 for a deep sea, and
2000 for a coastal area), Ct = nct is a total ﬁsh catch: n is a number of ships of
all players sent to the bank, and ct is a ﬁsh catch for one ship at the time t:
ct = cmax wt

ft
,
fmax

(2)

where cmax is a maximal catch (equal 25 for a deep sea, and 15 for a coastal
area), and wt is a weather factor at a time t, which is a random number between
0.8 and 1.0. As we can see, at the beginning of game, when ft is close to fmax ,
ﬁshing at the deep sea is more proﬁtable.
Usually exploration overcomes birth and after several rounds the ﬁsh number
can decrease to zero. It is a standard case of ”the tragedy of commons” [6]. It
is more reasonable to keep ships at the harbor then, therefore companies should
change theirs strategies.
Three types of agents are implemented: learning agent, predicting agent, and
random agent. The ﬁrst one uses experience to allocate ships, the second one
uses previous ﬁshing results to estimate values of diﬀerent allocation strategies,
third one allocates ships randomly. All types of agents may observe the following
aspects of the environment: new ships that they receive from a shipyard, money
earned in the last round, ships allocations of all agents, and ﬁshing results (ct )
for a deep sea and an inshore area. All types of agents can execute the following
two types of actions: order ships, allocate ships.

706

´ zy´
B. Snie˙
nski, J. Ko´zlak

Order ships action is currently very simple. It is implemented in all types of
agents in the same way. At the beginning of the game every agent has 10 ships.
Every round, if it has less then 15 ships, there is 50% chance that it orders two
new ships.
Ships allocation is based on the method used in [1]. Allocation strategy is represented by a triple (h, d, c), where h is the number of ships left in a harbor, d and
c are numbers of ships sent to a deep sea, and a coastal area respectively. Both
types of agents generate a list of allocation strategies for h = 0%, 25%, 50%, 75%,
and 100% of ships that belong to the agent. The rest of ships (r) is partitioned;
for every h the following strategies are generated:
1. All: (h, 0, r), (h, r, 0) – send all remaining ships to a deep sea or coastal area,
2. Check: (h, 1, r − 1), (h, r − 1, 1) – send one ship to a deep sea or coastal area
and the rest to the other,
3. Three random strategies: (h, x, r − x), where 1 ≤ x < r is a random number
– allocate remaining ships in a random way,
4. Equal: (h, r/2, r/2) – send equal number of ships to both areas.
The random agent allocates ships using one of strategies chosen by random.
Learning agent does the same in the ﬁrst game, but in the following games it
chooses strategy with the highest rating. Strategy rating is generated using rules
that allow to classify allocation as good or bad taking into account allocation
parameters and environment parameters (ﬁsh catch at the deep sea and at the
coastal area in the previous round).
Every strategy s gets a value v according to the formula:
v(s) = α good(s) + bad(s),

(3)

where good(s) and bad(s) are numbers of rules, which match the strategy and
current environment parameters, with consequence good and bad, respectively,
and α is a weight representing importance of rules with consequence good. The
rules are learned using agent experience (see section 4).
If there is more then one strategy with the same value, one occurring earlier
in the list is chosen.
Predicting agent uses the following formula to estimate value of the strategy:
v(s) = income(s) + η ecology(s),

(4)

where income(s) represents prediction of the income based on the previous ﬁshing results, ecology(s) represents ecological eﬀects of the action s (the value is
low if ﬁshing is performed in the area with low ﬁsh population), and η represents
importance of the ecology factor.
Software used in experiments is written in Prolog, using Prologix compiler [7].
Every agent is a separate process. It can be executed on a separate machine.
Agents communicate with the environment using Linda blackboard.

Learning in a Multi-agent System

707

Table 1. Predicates used to describe the event
Predicate
rate(R)
harbor(N)
alloc(A)
prevCatchDeep(D)

Argument values
good, bad
100%, 75%, 50%, 25%, 0%
100%-0%, 75%-25%, 50%50%, 25%-75%, 0%-100%
integer numbers

prevCatchCoastal(C) integer numbers

4

Description
Rating of the allocation strategy
Fraction of ships left in a harbor
Allocation: ship fraction sent to
deep-ship fraction sent to coastal
Number of fish cought by every ship
on a deep sea
Number of fish cought by every ship
on a coastal area

Learning

To support learning AQ21 program is used [8]. It is a machine learning software
that allows to generate attributional calculus rules for given examples. In a standard mode it produces a complete and consistent description of the data, but it
can also provide rules that are not complete and/or consistent. The main advantage of this program is that generated rules are not ordered and the knowledge
is easy to interpret for human what makes experimental results easier to check
and can be useful in Fish Bank application to teach people. Of course, other
methods of classiﬁer learning can be used.
4.1

Ship Allocation Learning

The AQ21 program generates a classiﬁer that is used to rate ship allocation
strategies. Input attributes for the classiﬁer are allocation parameters (ratios of
ships that are kept in a harbor, sent to the deep sea, and sent to a coastal area)
and environment parameters. Target attribute is a rating of the allocation in a
given environment state. It has two values: good and bad. Predicates used to
describe the event are presented in Tab. 1.
Training events are generated from agents observations. Every round the
learning agent stores ship allocations of all agents, and ﬁsh catch in the previous round. The strategy of an agent with the highest income is classiﬁed as
good, and the strategy of an agent with the lowest income is classiﬁed as bad.
If in some round all agents get the same income, none strategy is classiﬁed, and
as a consequence, none of them is used in learning.
At the end of each game the learning agent uses events, which were generated
during all games played so far, to learn a new classiﬁer that is used in the next
game.
4.2

Cooperative Learning

The next step in model development is to introduce cooperative learning of
agents to avoid depleting of ﬁsh. There are three main aspects of negotiations
that are considered:

708

´ zy´
B. Snie˙
nski, J. Ko´zlak

– selection of the time point when negotiation is performed. If an agent discovers that its situation is worse than in a previous tour, it starts a negotiation;
– selection of a proposal to be negotiated. The selection may be performed by
random with preference to decision which proved to be the most advantageous in the most of situations or for the most situations in similar cases;
– decision on whether the negotiated proposition should be accepted or rejected. Agents evaluate a list of proposals (each agent accepts or rejects
them) and then select the one with the highest degree of acceptance. If an
agent does not have any reason to accept or reject negotiation results (i.e.
does not know if the consequences of a particular condition is good or bad
for it) then it makes a decision by lot. Proposal has to get a majority of
the agents to be valid. At the end of game an agent assesses whether the
accepted proposition improved or worsened its situation and classiﬁes the
proposal as good or bad for the given system state.
To represent the agent situation, the wealth of the agent, numbers of ﬁsh on
the banks, and/or trends concerning the change of wealth or the change of the
number of ﬁsh caught should be taken into account.
These attributes are used to generate rules that are used to rate proposals.
Leaning rules after a game can improve negotiation strategies. As for ship allocation learning, AQ21 learning program can be applied.

5

Experimental Results

To test how learning changes an agent performance four series of experiments
were performed. Four agents took part in every series. Every series consisted of
ten simulations, and each simulation consisted of the sequence of 15 games.
In the ﬁrst series there were three random agents and one learning agent (with
weight α = 1). Performance of the agents measured as a balance at the end of
every game is presented in Fig. 1-(a).
In the second series one learning (α = 1), one predicting and two random
agents were used. Performance of agents is presented in Fig. 1-(b).
In the third and fourth series there were two learning and two random agents.
In both series, the ﬁrst learning agent (LA1) has α = 1. In the former series,
presented in Fig. 1-(c), second learning agent (LA2) has α = 2, in the latter,
presented in Fig. 1-(d), LA2 has α = 3.
In all experiments average balance of the learning agents grows with the
agent’s experience, while performance of the predicting agent decreases slightly
(because of the learning agent competition). Performance of random agents generally doesn’t change.
Examples of rules learned are presented in Fig. 2. Capital letters represent
variables that can be uniﬁed with any value. Predicate member checks if its ﬁrst
argument belongs to the list that is a second argument. It is used to represent
an internal disjunction (expression of the form x = v1 ∨ v2 ∨ . . . ∨ vn ). Remaining
predicates are explained in the previous section. These rules (in the form of
clauses) can be interpreted in the following way.

Learning in a Multi-agent System
(a)

(b)
40

35
30
25

LA
RA1
RA2
RA3

20
15
10
5
0
1

2

3

4

5

6

7

8

9

Balance at the end of game [K$]

Balance at the end of game [K$]

40

35
30
25

15
10
5
0
-5

10 11 12 13 14 15

LA
PA
RA1
RA2

20

1

2

3

4

5

6

7

8

9

10 11 12 13 14 15

Game nr

Game nr

(c)

(d)

30

35

25
20

LA1
LA2
RA1
RA2

15
10
5
0
1

2

3

4

5

6

7

8

9

10 11 12 13 14 15

Balance at the end of game [K$]

Balance at the end of game [K$]

709

30
25
20

LA1
LA2
RA1
RA2

15
10
5
0
-5

1

Game nr

2

3

4

5

6

7

8

9

10 11 12 13 14 15

Game nr

Fig. 1. Comparison of performance of learning agents (LA, LA1, LA2) and other agents
using random strategy of ship allocation (RA1, RA2, RA3) or prediction (PA); values
for learning and predicting agents are presented with the standard deviation
(a)
rate(bad) :harbor(B),
member(B,[25,50,75]),
prevCatchDeep(C),
C >= 16,
prevCatchCoastal(10).

(b)
rate(good) :alloc(B),
member(B,[100%-0%,75%-25%]),
prevCatchDeep(C),
C >= 18,
C =< 21,
prevCatchCoastal(D),
D =< 10.

Fig. 2. Examples of rules (in the form of Prolog clauses) learned by the agent

Clause (a): it is a bad decision to keep at a harbor 25, 50, or 75 percent of
ships if previous catch at a deep sea is greater or equal to 16, and previous catch
at a coastal area is 10.
Clause (b): it is a good decision to send 100% ships to a deep sea or 75% to
a deep sea and 25% to a coastal area if previous catch at a deep sea is greater
or equal to 18, and smaller or equal to 21, and previous catch at a coastal area
is smaller or equal to 10.
Experimental results show that the learning agent performance increases
rapidly at the beginning of the learning process, when generated rules are used
instead of a random choice. Next it increases slowly, because new examples do
not contain any signiﬁcant knowledge. The performance stabilizes at the end of
the process.

710

´ zy´
B. Snie˙
nski, J. Ko´zlak

Weight α has no signiﬁcant inﬂuence on the learning agent performance. However values greater then 1 seem to be a little bit better then α = 1.
As we can see in Fig. 1-(b), the predicting agent performs better then the
learning agent. It suggests, that there is a space for improvement of the learning
method. Further research is necessary to check if it is possible to learn such a
good strategy.

6

Conclusion and Further Research

The results obtained conﬁrm that applying a symbolic machine learning algorithm in a multi-agent system may improve resource management.
Fish Banks environment is complex enough to apply supervised learning, when
direct performance feedback is available (e.g. income at the end of the round),
and, if there is no such information, and feedback is available at the end of game
(reinforcement learning can be used in such situation). Testing the latter case is
a subject of further research.
Acknowledgments. The authors thank prof. Ryszard S. Michalski, George Mason
University for providing AQ21 program, and Arun Majumdar, Vivomind Intelligence Inc. for providing Prologix system, which was used in the implementation.

References
1. Kozlak, J., Demazeau, Y., Bousquet, F.: Multi-agent system to model the fishbanks
game process. In: The First International Workshop of Central and Eastern Europe
on Multi-agent Systems (CEEMAS’99), St. Petersburg (1999)
2. Sen, S., Weiss, G.: Learning in multiagent systems. In Weiss, G., ed.: A Modern
Approach to Distributed Artificial Intelligence. The MIT Press (1999)
3. Sian, S.: Adaptation based on cooperative learning in multi-agent systems. In
Demazeau, Y., Muller, J.P., eds.: Decentralised AI (Vol. 2). (1991) 257–272
4. Meadows, D., Iddman, T., Shannon, D.: Fish Banks, LTD: Game Administrator’s
Manual. Laboratory of Interactive Learning, University of New Hampshire, Durham,
USA (1993)
5. Sniezynski, B., Kozlak, J.: Learning in a multi-agent approach to a fish bank game.
In: Multi-Agent Systems and Applications IV: Proc. of CEEMAS 2005. Volume 3690
of Lecture Notes in Computer Science. (2005) 568–571
6. Hardin, G.: The tragedy of commons. Science 162 (1968) 1243–1248
7. A. Majumdar, P. Tarau, J.S.: Prologix: Users guide. Technical report, VivoMind
LLC (2004)
8. Wojtusiak, J.: AQ21 User’s Guide. Reports of the Machine Learning and Inference
Laboratory, MLI 04-3. George Mason University, Fairfax, VA (2004)

