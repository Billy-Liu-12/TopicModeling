Towards Efficient Parallel Image Processing on Cluster
Grids Using GIMP
Pawel Czarnul, Andrzej Ciereszko, and Marcin Fra˛czak
Faculty of Electronics, Telecommunications and Informatics
Gdansk University of Technology, Poland
pczarnul@eti.pg.gda.pl, {cierech,marcin.f}@wp.pl
http://fox.eti.pg.gda.pl/∼pczarnul

Abstract. As it is not realistic to expect that all users, especially specialists in the
graphic business, use complex low-level parallel programs to speed up image processing, we have developed a plugin for the highly acclaimed GIMP which enables
to invoke a series of filter operations in a pipeline in parallel on a set of images
loaded by the plugin. We present the software developments, test scenarios and
experimental results on cluster grid systems possibly featuring single-processors
and SMP nodes and being used by other users at the same time. Behind the GUI,
the plugin invokes a smart DAMPVM cluster grid shell which spawns processes
on the best nodes in the cluster, taking into account their loads including other user
processes. This enables to select the fastest nodes for the stages in the pipeline.
We show by experiment that the approach prevents scenarios in which other user
processes or even slightly more loaded processors become the bottlenecks of the
whole pipeline. The parallel mapping is completely transparent to the end user
who interacts only with the GUI. We present the results achieved with the GIMP
plugin using the smart cluster grid shell as well as a simple round robin scheduling
and prove the former solution to be superior.

1

Introduction

While both the cluster and grid architectures ([1]) and real grid software based on Globus
([2]) as well as image processing tools become mature and available for a wider range
of systems and network topologies, it is still a difficult task to merge the two worlds
in open NOWs. We investigate available solutions and make an attempt to process in
parallel both loosely coupled and pipelined images on NOW systems. In the context of
parallel image processing there is a need for an easy-to-use graphical user interface of
a familiar application like Adobe Photoshop or the GIMP and an efficient but not too
complex a tool for selection of best resources. Ideally, as we managed to achieve, the
second stage is completely hidden from the user provided the prior configuration of the
parallel environment. Following the Sun Grid Engine terminology ([3]) we define the
cluster grid as a set of distributed resources providing a single point of entry (running
GIMP in our approach) to users in an institution.
Work partially sponsored by the Polish National Grant KBN No. 4 T11C 005 25
M. Bubak et al. (Eds.): ICCS 2004, LNCS 3037, pp. 451–458, 2004.
c Springer-Verlag Berlin Heidelberg 2004

452

2

P. Czarnul, A. Ciereszko, and M. Fra˛czak

Related Work

System or application level solutions which could assist in running either sequential or
parallel image processing codes in parallel, range from system-level Mosix ([4]) which
features process migration for load balancing for Linux boxes, cluster management
systems like LoadLeveler, PBS, LSF ([5]) Condor ([6]) for exploiting idle cycles in
shared networks up to Sun Grid Engine ([3]) which enables both queueing submitted
HPC tasks and launching interactive processes on least loaded nodes allowing grid
computing. There are libraries and environments like MPI, PVM ([7]) available for
programmers. MatlabMPI ([8]) implements the MPI-like programming API for Matlab
scripts to be run on parallel systems using the file system for communication. Although
the latter three can assist in writing parallel image processing, none of them is by design
coupled to specific parallel applications, including graphic design.
In regard to support for multithreaded image processing, in applications like Adobe
Photoshop on SMP machines some filters like Gaussian Blur, Radial Blur, Image Rotate,
Unsharp Mask can potentially benefit from many processors ([9]). However, some filters
which can be accomplished fast can run even slower on SMP boxes than on a single
processor due to the large synchronization overhead compared to the processing time.
It has been confirmed that the multithreaded approach can speed up processing on the
latest Intel HyperThreading processors ([10]) when running filters in Photoshop 7.0.
[11] presents threaded GIMP plugins implementing Gaussian Blur.
[12] assumes, similarly to our approach, that it is unrealistic to expect the knowledge on parallelization issues from graphic specialists and provides the programmer
with the architecture in which data parallel image processing applications can be coded
sequentially, automatically parallelized and run on a homogeneous cluster of machines.
[13] presents a skeleton based approach for data parallel image processing in which
only algorithmic skeletons coded in C/MPI need to be chosen for particular given lowlevel image operators to produce a parallel version of the code.
Finally the Parallel Image Processing Toolkit (PIPT, [14]) provides an extendable
framework assisting image processing offering low-level API which can be incorporated
into parallel MPI programs. This allows operations on chunks of images in parallel.
Similarly, [15] describes a library of functions SPMDlib meant to help the development
of image processing algorithms and a set of directives SPMDdir which are mapped by
a parser to the library which provides an easy-to-use and high-level API.

3

Our Approach for Cluster Parallel Image Processing

In order to show the benefits of cluster computing in an open multi-user NOW, we have
implemented an extension to the DAMPVM environment ([16], [17]) which provides the
end user with a smart cluster shell which starts a shell command or a set of commands on
best nodes in the network. Compared to Sun Grid Engine, it supports any platform PVM
can run on, not only Solaris Operating Environments (on SPARC processors) and Linux
x86 for Sun Grid Engine 5.3. Moreover, the DAMPVM sources can be modified to use
a cluster even more efficiently. In view of this, the presented solution can easily use the
monitoring/filtering infrastructure ([18]) and the divide-and-conquer features ([17]) of

Towards Efficient Parallel Image Processing on Cluster Grids Using GIMP

453

DAMPVM. Thus it has been naturally extended to enable the GIMP plugin to launch
processes on least loaded processors in a parallel environment incorporating run-time
changes into decision making. The plugin can make use of the GIMP support for reading
and writing various graphic formats like TIFF, PNG, JPEG etc. and leave the computing
part for the efficient C++ code using PVM or other means of communication like MPI.
To prove the usefulness of this approach we have tested the following scenarios:
1. Starting command-line multi process conversion (using ImageMagick’s convert
utility) of large images in a network with varying loads. The cluster shell selects the
least loaded nodes for convert to be run and thus optimizes the wall time of the
simulation.
2. A GIMP plugin which invokes parallel pipelined processing using PVM applying
a sequence of filters on images read by GIMP, all implemented by us within this
work1 . In corresponding external conditions, we prove that the allocation of pipeline
processes to processors using the DAMPVM cluster shell can lead to a noticeable
reduction of the execution time compared to round-robin/random allocation. This is
visible even in lightly loaded networks where the DAMPVM shell chooses the least
loaded nodes. In the pipeline, even small, difficult to notice processor loads appear
to be a bottleneck for the pipeline which justifies our approach.
3.1

Speed Measurement and Task Queueing

In a heterogeneous network different but comparable processors like AMD Athlon XPs,
MPs, Intel Pentium 4 (HT) can process different codes at different speeds. Secondly, the
load of other users must be measured and filtered in order to hide high-frequency peaks
corresponding to short-lived but CPU-bound actions like starting Mozilla etc. The latter
was implemented in the DAMPVM runtime before ([18]). Within the scope of this work
it was extended with precise speed measurement for specific code operations to reflect
the real CPU-bound long application which follows the measurement. This can be the
same processing command used on smaller images.
We applied the algorithm shown in Figure 1 to the DAMPVM cluster shell. When
there are idle processors in the mixed single-processor, SMP system (as indicated by
DAMPVM schedulers) the relevant information is stored in an array and the idle processors are assigned tasks successively with no delays. After the procedure has been
completed, a 10-second delay is introduced to allow for the increased load being detected by the DAMPVM runtime. When all processors are still busy, every 2-second time
slot a load check is performed. The values result from arbitrary delays to monitor load
coded in the DAMPVM runtime. This scheme allows to use idle processors at once and
queue pending supposedly processor and disk-bound image processing tasks in order
not to overload the processor with too many processes resulting in process switching
overhead. The load is monitored on every host by DAMPVM schedulers ([18]) and then
collected asynchronously by a cluster manager using ring fashion communication. The
load information includes: machine speed, the number of CPUs per node, idle percentages of the processors, CPU load by other users and the system, the number of processes,
file storage available on the node, link start-up times and bandwidth.
1

Download software from http://fox.eti.pg.gda.pl/∼pczarnul

454

P. Czarnul, A. Ciereszko, and M. Fra˛czak

Fig. 1. Task Queueing in DAMPVM Remote Shell

3.2 A GIMP Plugin for Pipelined Operations on Clusters
The main idea of the plug-in is to apply a set of up to ten filters to a large number of
images. The images should be placed on one machine with the GIMP, connected via
network to a cluster of computers running PVM, a cluster grid. The architecture of the
plug-in consists of three layers (Figure 2):
– a Script-Fu wrapper,
– a supervisory GIMP-deployed plug-in,
– slave node modules in the proposed parallel pipelined architecture.

The GIMP

Slave node 1

GIMP library

Script−Fu wrapper
(user data collection)
PVM communication
function calls

Supervisory module

Slave node n

Fig. 2. GIMP Plugin’s Architecture

Fig. 3. GIMP’s Pipelined Plugin Interface

The GIMP provides developers with an easy to use scripting language called ScriptFu, that is based on a scheme programming language. From the Script-Fu level you can
run any of the GIMP library function including GIMP plug-ins properly registered in
the PDB (GIMP’s Procedural DataBase). Script-Fu also aids developers in creating of
plug-in interfaces. The wrapper’s function is to gather data from the user and relay them
to the supervisory GIMP-deployed plug-in.

Towards Efficient Parallel Image Processing on Cluster Grids Using GIMP

455

Both, the supervisory module and the slave module, are programs written in C using
PVM for communication. The supervisory module calls the GIMP’s library functions to
open each image for processing, acquires raw picture data from the GIMP’s structures,
and passes it to the first of the pipeline nodes. This allows to combine the easy-to-use
interface with the underlying parallel architecture. The images are then processed in
the created slave node pipeline. The pipeline can be created in two ways: either by
using PVM (Pipeline plug-in) in accordance with a static list of hosts created by the
user, or dynamically, with assistance of PVM based DAMPVM remote shell (Pipeline
dampvmlauncher plug-in) thus taking advantage of the speed and load information of
individual nodes in creating the slave node pipeline.
The slave node module implements a series of image filters (3x3, 5x5 matrix area
filters and simple non-context filters). Each slave applies a filter to the image and passes
it on through the pipeline.
The interfaces for both plug-ins are identical, the difference lies in the code that is
implemented in the supervisory module. The user invokes the plugin from the GIMP’s
context menu and specifies a path name for the files intended for filtering and the number
of filters that will be applied to all the images. Figure 3 shows the plugin window
superimposed on the context menu invoking it. Slide bars let the user specify the types
of filters at the stages of the pipeline.

4

Experimental Results

We have performed lab experiments to prove that image processing on cluster grids can
be successfully assisted by the load-aware remote DAMPVM shell. We used a cluster
of 16 Intel Celeron 2GHz, 512MB machines interconnected with 100Mbps Ethernet.
4.1

Parallel Image Conversion with a Cluster Shell

In this case, we tested the ability of
the DAMPVM runtime and the remote
shell to detect least loaded nodes, spawn
tasks remotely, queue pending tasks as
described above and submit them when
processors become idle. These features
were tested in the scenario with one node
loaded with other CPU-bound processes
for which the results are shown in Figure 4. We ran ImageMagick’s convert
utility to convert a 4000 × 4000 48MB
Fig. 4. Scaled Speed-up for Remote Shell
TIFF image to Postscript. On 1 procesconvert Runs
sor, we ran the command without the remote shell. On P > 1 processors in Figure 4 there were P + 1 processors available,
one of which was overloaded. The remote shell omitted the overloaded node. The scaled
speed-up was computed as the single-processor execution time multiplied by P divided

456

P. Czarnul, A. Ciereszko, and M. Fra˛czak

by the time achieved on P processors with the load on the P + 1-th processor. The latter processor was chosen randomly. Less than ideal values but showing small overhead
result from additional load monitoring, spawn and the queueing procedure.
4.2

Pipelined Image Filtering as a GIMP Plugin Using a Cluster

In this case we compared launching pipelined computations using a static allocation of
pipeline stages to processors and the dynamic allocation using the remote shell.
In the case that work is distributed in an unloaded and homogeneous network the
efficiency of both plugins should be similar. However, in a normal network environment
such conditions are hard to achieve and thus it is expected that total work times will
be considerably shorter for the DAMPVM module. It is not only in academic examples
in which some node(s) is overloaded but also in a seemingly idle network. In the latter
case, some system activities or a user browsing the Internet contribute to the processor
usage and such a node becomes effectively the pipeline bottleneck if selected as a stage
of the pipeline. The remote shell enabled the plugin to avoid placing pipeline nodes on
machines that are already loaded with work (e.g. the computer running the GIMP).

Fig. 5. Execution Time for Various Pipeline
Allocation Methods by Image Size

Fig. 6. Execution Time for Various Pipeline
Allocation Methods by Number of Images

The variables in the pipelined simulations are as follows:
–
–
–
–

the number of stages/processors in the pipeline (P ),
the number of images to process (N ),
the size of the images: may be of similar or different sizes,
the type of filters at the stages: may be uniform or different in regard to the processing
time.

Figure 5 presents results for 3 × 3 matrix filters (taking same time to complete) on
P = 10 processors for N = 25 800 × 600 and 1600 × 1200 bitmaps while Figure 6
shows results for for 5 × 5 matrix filters (taking same time to complete) on P = 10
processors for N = 25 and N = 50 800 × 600 bitmaps. In all cases there were 14 idle
processors available. On one of them there was the GIMP running, acting as a master
host, on another one a user performed simple editing in Emacs. In the “Static not loaded”

Towards Efficient Parallel Image Processing on Cluster Grids Using GIMP

457

case, the allocation was done statically by listing the available hosts in a file, from which
10 successive processors were chosen. The master host at the end of 14-processor list
was thus omitted. In the second test (“Static random”) the master host was acting as
the first stage in the pipeline, both reading images through GIMP, processing it and
passing to the second stage. Especially for large images this becomes a bottleneck since
the master host also saves the results to the disk. Finally, in the “Dynamic with shell”
example, the remote shell was launched to start 10 slave node processes on least loaded
nodes. It automatically omitted both the master host and the processor busy with text
editing although seemingly idle. Compared to the “Static not loaded” case it is visible
that even small additional loads in the static allocation scenario slow down the pipeline,
justifying the use of the remote shell.
The best theoretical speed-up of the pipeline is strictly bounded and, assuming no
overhead for communication which is more apparent for larger bitmaps, can be estimated
NP
as N −1+P
(N = 25, P = 10) which approximates to 250
34 ≈ 7.35. The time obtained
for 1 processor and 25 800 × 600 bitmaps and the 10-stage pipeline was 439s giving
speed-up 5.49. This is due to costly communication and synchronization. It must be
noted also that on 1 processor all the processes run concurrently which means that there
are costly context switches and slow downs due to the GIMP’s disk operations.

5

Summary and Future Work

We have presented software developments combining advanced pipelined filter processing of images selected in GIMP with parallel clusters showing improvement in the
execution time when used with a load-aware remote shell rather than static process assignment. It is easy to select the proposed as well as add new filters to customize the
graphic flow, for example, to perform the common sequence of operations on images
transformed to thumbnails for WWW use, usually transformed from TIFF: adjust levels
(can be pipelined itself), convert to the 16-bit format, adjust contrast, brightness, possibly
saturation, scale the image, apply the unsharp mask, convert to JPEG.
As it is a more practical pipeline flow, we are planning to implement such a pipeline
and execute it on a cluster of 128 2-processor machines at the TASK center in Gdansk,
Poland as well. Note that the proposed approach can be used widely for pipelined image
conversion for WWW gallery creation or assist in pipelined sequences of advanced
graphic designers working with the GIMP. The implementation could also be extended
to other popular applications like Adobe Photoshop.

References
1. Foster, I., Kesselman, C., Tuecke, S.: The Anatomy of the Grid: Enabling Scalable Virtual
Organizations. International Journal of High Performance Computing Applications 15 (2001)
200–222 http://www.globus.org/research/papers/anatomy.pdf.
2. Globus: Fundamental Technologies Needed to Build Computational Grids (2003)
http://www.globus.org.
3. Sun Microsystems Inc.: Sun Grid Engine 5.3. Administration and User’s Guide. (2002)
http://wwws.sun.com/software/gridware/faq.html.

458

P. Czarnul, A. Ciereszko, and M. Fra˛czak

4. Barak, A., La’adan, O.: The MOSIX Multicomputer Operating System for High Performance
Cluster Computing. Journal of Future Generation Computer Systems 13 (1998) 361–372
5. Platform Computing Inc.: PLATFORM LSF, Intelligent, policy-driven batch application
workload processing (2003) http://www.platform.com/products/LSF/.
6. Bricker, A., Litzkow, M., Livny, M.: Condor Technical Summary. Technical report, Computer
Sciences Department, University of Wisconsin-Madison (10/9/91)
7. Wilkinson, B., Allen, M.: Parallel Programming: Techniques and Applications Using Networked Workstations and Parallel Computers. Prentice Hall (1999)
8. Kepner, J.: Parallel Programming with MatlabMPI. MIT, Lexington, MA, U.S.A. (2003)
http://www.ll.mit.edu/MatlabMPI/.
9. Marc Pawliger: Multithreading Photoshop (1997)
http://www.reed.edu/∼cosmo/ pt/tips/Multi.html.
10. Mainelli, T.: Two cpus in one? the latest pentium 4 chip reaches 3 ghz and promises you
a virtual second processor via intel’s hyperthreading technology. PC World Magazine (Jan
2003)
11. Briggs, E.: Threaded Gimp Plugins (2003)
http://nemo.physics.ncsu.edu/∼briggs/gimp/.
12. Seinstra, F., Koelma, D., Geusebroek, J., Verster, F., Smeulders, A.: Efficient Applications in
User Transparent Parallel Image Processing. In: Proceeding of International Parallel and Distributed Processing Symposium: IPDPS 2002 Workshop on Parallel and Distributed Computing in Image Processing, Video Processing, and Multimedia (PDIVM’2002), Fort Lauderdale,
Florida, U.S.A. (2002) citeseer.nj.nec.com/552453.html.
13. Nicolescu, C., Jonker, P.: EASY-PIPE - An "EASY to Use" Parallel Image Processing Environment Based on Algorithmic Skeletons. In: Proceedings of the 15th International Parallel
and Distributed Processing Symposium (IPDPS’01), Workshop on Parallel and Distributed
Image Processing, Video Processing, and Multimedia (PDIVM’2001), San Francisco, California, USA (2001)
http://csdl.computer.org/comp/proceedings/ipdps/2001/0990/03/
099030114aabs.htm.
14. Squyres, J.M., Lumsdaine, A., Stevenson, R.L.: A Toolkit for Parallel Image Processing. In:
Proceedings of SPIE Annual Meeting Vol. 3452, Parallel and Distributed Methods for Image
Processing II, San Diego (1998)
15. Oliveira, P., du Buf, H.: SPMD Image Processing on Beowulf Clusters: Directives and
Libraries. In: Proceedings of International Parallel and Distributed Processing Symposium
(IPDPS’03), Workshop on Parallel and Distributed Image Processing, Video Processing, and
Multimedia (PDIVM’2003), Nice, France (2003)
http://csdl.computer.org/comp/proceedings/ipdps/2003/1926/00/
19260230aabs.htm.
16. Czarnul, P.: Programming, Tuning and Automatic Parallelization of Irregular Divide-andConquer Applications in DAMPVM/DAC. International Journal of High Performance Computing Applications 17 (2003) 77–93
17. Czarnul, P.: Development and Tuning of Irregular Divide-and-Conquer Applications in
DAMPVM/DAC. In: Recent Advances in Parallel Virtual Machine and Message Passing
Interface. Number 2474 in Lecture Notes in Computer Science, Springer-Verlag (2002) 208–
216 9th European PVM/MPI Users’ Group Meeting, Linz, Austria, September/October 2002,
Proceedings.
18. Czarnul, P., Krawczyk, H.: Parallel Program Execution with Process Migration. In: International Conference on Parallel Computing in Electrical Engineering (PARELEC’00),
Proceedings, Quebec, Canada (2000)

