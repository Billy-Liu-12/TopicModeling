Available online at www.sciencedirect.com

Procedia Computer Science 9 (2012) 508 – 517

International Conference on Computational Science, ICCS 2012

Eﬀectiveness of Hybrid Workﬂow Systems for Computational
Science
Beth Plale∗, Eran Chinthaka Withana∗, Chathura Herath∗, Kavitha Chandrasekar∗, Yuan Luo∗
School of Informatics and Computing
Indiana University, Bloomington

Abstract
The workﬂow and its supporting systems are integral to computational science. Tailored to loosely coupled, and
largely coarse-grained tasks, the workﬂow replaces the script as a way to automate the multiple steps of a large scale
model. Workﬂow reuse has been at the subworkﬂow level but this restricts, over the long run, a workﬂow to running
on the system on which it was developed. A scientist wanting to use two workﬂows developed by two diﬀerent
people and for diﬀerent workﬂow systems will need to have access to both workﬂow systems. The contribution this
paper makes is a qualitative and quantitative study of the tradeoﬀs of a hybrid workﬂow solution that utilizes multiple
workﬂow systems and solutions to execute a single workﬂow. Our results indicate that the major tradeoﬀs are not in
performance as much as they are in complexity.

1. Introduction
The workﬂow and its supporting systems are integral to computational science [1]. Tailored to loosely coupled,
and largely coarse-grained tasks, the workﬂow replaces the script as a way to automate the multiple steps of a large
scale model, including moving data, identifying and locating input data sources, transforming data from one form to
another suitable for model digestion, and assimilation of data from multiple sources into a single form.
Workﬂow systems often provide default activities that serve as out-of-the-box tasks that can be used to construct
a workﬂow. These activities may be domain independent, such as third party data movement, or targeted towards a
particular science domain such an activity that carries out a BLAST gene sequence matching activity [2]. The choice
of workﬂow activities supported by a system often depends on the targeted use of the system. A workﬂow system
might be specialized to work extremely well when tasks are jobs that run on large-scale compute resources such as
Teragrid/xSEDE [3], or a cloud platform. Sharing of workﬂow activities through sites such as myExperiment.org
[4] further enhance the usefulness of a particular workﬂow system by allowing domain speciﬁc sets of tasks to be
constructed by one into a workﬂow or subworkﬂow and then shared with many. But workﬂow scripts usually run on
a single workﬂow engine.
∗ Corresponding

author
Email addresses: plale@cs.indiana.edu (Beth Plale), echintha@cs.indiana.edu (Eran Chinthaka Withana),
cherath@cs.indiana.edu (Chathura Herath), kavchand@cs.indiana.edu (Kavitha Chandrasekar), yuanluo@cs.indiana.edu (Yuan
Luo)

1877-0509 © 2012 Published by Elsevier Ltd. Open access under CC BY-NC-ND license.
doi:10.1016/j.procs.2012.04.054

Beth Plale et al. / Procedia Computer Science 9 (2012) 508 – 517

509

Workﬂow reuse has been at the subworkﬂow level as has been noted by the stewards of the myExperiment workﬂow repository. That is, users share portions of workﬂows, and these workﬂows are being picked up for adoption
at higher rates than are full workﬂows. For simplicity in exposition, we henceforth refer to a sub-workﬂow simply
as a ’workﬂow’. Since workﬂow scripts usually run on a single engine, it is likely that any long-term guarantee that
a workﬂow system makes to reproducibility of its workﬂows will be speciﬁc to the system on which the workﬂow
was ﬁrst created. But this limits over the long run a workﬂow to running on the system on which it was developed.
A scientist wanting to use two workﬂows developed by two diﬀerent people and for diﬀerent workﬂow systems will
need to have access to both workﬂow systems.
Hybrid computing of the form of utilizing more than one workﬂow system to carry out a single task has been
explored in the WS-VLAM system [5]. WS-VLAM integrates support for multiple workﬂow systems through a
single event service bus, the VL-e Workﬂow Bus, that carries data and control to and from multiple remote workﬂow
systems. Such an approach seems highly desirable, but what is the cost? We explore that question fully in this paper.

Figure 1: Dimensions of hybrid workﬂow systems. Activities B and F are called from System 1 by instructing Systems 2 and 3 respectively to
execute the activities. B, while seen as a single activity in System 1, is actually a workﬂow. F is an activity that runs on grid middleware. Diagram
adapted from [6].

We undertake a performance evaluation of hybrid computing of the kind demonstrated by WS-VLAM that involves
multiple workﬂow systems and evaluate the approach qualitatively and quantitatively. To our knowledge there is no
good comparative data on the costs, both quantitative and qualitative, of what it takes to support such a strategy. Our
study ﬁxes the high level system at a user desktop workﬂow engine, and explores the performance and programming
impact of various forms of remote activity. The structure adopted is best illustrated in Figure 1. System 1 is hosted on
a user’s local machine, and workﬂows are run locally. Activities A and E run locally. Activity B is a call from System
1 to System 2 to execute the activity. B, while seen as a single activity in System 1, is actually a workﬂow made up
of activities C and D. Activity F is executed by a call from System 1 to System 3, instructing the latter to execute
the activity. F, which is seen as an activity in System 1 calls out to grid middleware to carry out the execution of the
activity.
In our evaluation we hold System 1 constant at the local desktop workﬂow system, speciﬁcally, the Trident Scientiﬁc Workﬂow Workbench [7] running on top of Workﬂow Foundation. Trident was chosen because it is easy to write
new activities, and because as a Windows solution, it brings with it platform diversity, since System 2 and System
3 are mostly Linux in our setup. In more detail, using the model in Figure 1 to illustrate, for the System 2 case we
evaluate both the Kepler workﬂow system [8], and the Apache ODE [9] workﬂow tool. Trident is well suited to a
desktop environment, hence was chosen for System 1. Kepler is often used to run workﬂows on the grid and Apache
ODE is widely used in enterprise environments. At System 2, these two workﬂow systems represent two categories
of workﬂow systems [10], as Kepler uses Graph-based modelling and ODE with it BPEL-based orchestration follows
Language-based modelling for workﬂow composition. With regards to Workﬂow scheduling, all the three workﬂow
systems use a centralized scheduling scheme. This taxonomy allows our results to be transferrable to other workﬂow
systems. For the System 3 case where remote services are invoked directly we evaluate GFac [11] and the Opal
toolkit [12].While GFac dynamically provides a generic interface to services on the grid, Opal wraps an application
statically to expose an interface speciﬁc to the application. These two examples cover the typical wrapping methods

510

Beth Plale et al. / Procedia Computer Science 9 (2012) 508 – 517

for applications on grid.
The contribution this paper makes is a qualitative and quantitative study of the tradeoﬀs of a hybrid workﬂow
solution. As our results indicate, the remote engine model compares favorably to the remote grid middleware in terms
of performance. The complexity of maintaining such a hybrid system may in the end outweigh any beneﬁts that could
be accrued from such a ﬂexible solution.
The remainder of this study is organized as follows. Section 2 discusses related work. Section 3 details architectural aspects. Section 4 discusses the workload and Section 5 discusses the quantitative and section 6 qualitative
results. Section 7 discusses future work.
2. Related Work
A wide range of workﬂow systems [10, 13] have been developed for scientiﬁc research and their maturity and
established user base inﬂuenced the design constraints of this study. While most systems are general purpose [10],
others have a stronger domain focus [2]. Triana [14] targets complex workﬂows for domains like signal, text and
image processing. Some of these systems are optimized for supercomputing resources (e.g. Grids [10], Dagman [15],
Wings [16]). Pegasus [17], in combination with DAGMan [18] treats parametric sweeps as ﬁrst class operations.
Others are better suited to desktop or single user environments such as Taverna [2] and Trident [7].
Researchers have examined ways to standardize and evaluate workﬂow systems. Jinde et al. [19] study interoperability amongst business workﬂows management systems. There have been eﬀorts to standardize workﬂow interoperability; Hayes et al. 2002 [20] proposes standards to which the workﬂow systems adhere to achieve business workﬂow
interaction, yet these standards imposed limitations that arise in a particular business setting. The AFRICA [21] framework supports interoperability among workﬂow systems using SOA based XML message interactions. Though these
eﬀorts are promising, diﬀerent workﬂow representations (e.g. directed graphs, petri-nets, UML diagrams), unique domain and compute resource speciﬁc requirements within diﬀerent workﬂow environments have complicated attempts
at interoperability. Select workﬂows systems [2], [22] coordinate workﬂow tasks designed for another workﬂow
system.
Performance evaluation often examines an aspect of performance, such as Wang et al. [23] performance evaluation
of virtual machine-based Grid workﬂow system. Stratan et al. [24] evaluate grid workﬂow engines on characteristics
like overhead, raw performance, stability, scalability and reliability on diﬀerent workloads. We compare two models
based on overall performance and overhead quantitatively. Gillmann et al. [25] benchmark workﬂow systems based
on e-commerce scenarios found in enterprises. Truong et al. [26] deﬁne the various granularities at which a grid
workﬂow system can be evaluated. Our study is focussed on the strengths of layered workﬂow engines and services.
3. Architectural Organization
The architectural organization is of the infrastructure used in the study, not of a working infrastructure. It can be
viewed as a combination of cases, a few of which might be used in practice. While exercising the whole architecture
may be unrealistic in a real setting, the cases individually and in small-group combinations are realistic. There are
four primary components of the architecture, which are reﬁnements to the model of Figure 1 and these are:
Baseline execution: The baseline execution does not employ hybrid workﬂows, and instead runs workﬂows
locally, within System 1 in Figure 2. The top level workﬂow engine is the Trident Scientiﬁc Workﬂow Workbench
and it runs workﬂows locally. Trident calls out to the Workﬂow Foundation to orchestrate execution.
Remote workﬂow engine: where an activity in the local system contacts a remote workﬂow engine, illustrated
by activities AR and AT in the ﬁgure.
Remote grid/cloud middleware: where activities in the local system contact grid/cloud middleware directly.
Since there is no orchestration at the remote system, the remote grid/cloud services are capable of executing only one
task of a workﬂow. Shown in the black dotted lines is GRAM executing one of the three services of a workﬂow.
High performance compute resources are the high performance compute resources such as Teragrid, OpenScienceGrid, or a supercomputer.
To illustrate the distinctions, using terminology of Figure 1, baseline execution is carried out by Trident running
on a Windows machine and using Workﬂow Foundation. Trident activities A p and Aq invoke two workﬂow tasks
under the control of Workﬂow Foundation running on the same host or host cluster as Trident.

Beth Plale et al. / Procedia Computer Science 9 (2012) 508 – 517

511

Figure 2: Types of interoperability: local execution within System 1 is shown by activity AP and AQ invoking a workﬂow through WorkﬂowFoundation. Activity AR communicates with the Kepler remote engine to run a subworkﬂow on Big Red (solid black lines). Activity AS contacts grid
services directly to invoke nodes individually (dotted black lines). Activity AT invokes the ODE workﬂow engine to run a subworkﬂow on Big Red
(solid red lines). Activity AU contacts grid services directly to invoke nodes individually (dotted red lines).

The remote workﬂow engine case is illustrated by activity AR ’s invocation of the Kepler workﬂow system to
orchestrate a workﬂow. Kepler uses the resource manager, Opal, to submit jobs to the supercomputer, Big Red; Opal
submits using Globus GRAM [27]. The remote grid/cloud middleware case is AS initiating a remote grid/cloud service
directly. Cases 4 and 5, AT and AU , are second examples of cases AR and AS respectively. AT invokes the Apache
ODE workﬂow engine which schedules jobs using GFAC, and WS-GRAM [27] with job communication facilitated
through a wrapper service that allows a web services infrastructure to communicate with legacy Fortran codes. AU
invokes remote grid/cloud services directly.
AR invokes Kepler which contacts the Opal Toolkit to execute a subworkﬂow on the Big Red supercomputer
through Globus Gram. Activity AT invokes the Apache ODE workﬂow engine that contacts GFac to execute a subworkﬂow on Big Red using Sigiri resource manager [28].
4. Workﬂow workloads
The core workﬂows of the study cover four cases over two workﬂow patterns as follows:
1. Data Intensive, Sequential Workﬂow
2. Data Intensive, Parallel Workﬂow
3. Compute Intensive, Sequential Workﬂow
4. Compute Intensive, Parallel Workﬂow
The sequential workﬂow is made up of 5 identical tasks executed sequentially. For the compute-heavy version
Linpack Java version is used. Linpack solves linear equations, and we conﬁgured it to carry out execution on a 5000 x
5000 matrix, using double precision ﬂoating point coeﬃcients. The inputs and outputs are small, on the order of 5KB.
The data-intensive workﬂow carries out applies a cryptographic hash function (i.e., MD5) on a 1.5 GB ﬁle. Because
there are no natural data dependencies in this workﬂow, a 1.5 GB ﬁle is copied from one location to another before
each MD5 service is performed. The parallel workﬂow executes the same computations (MD5 or Linpack) in parallel.
The workﬂow is sandwiched on either end by scatter and gather nodes.
A sequence diagram shown in Figure 3 temporally depicts activity along a vertical timeline of invocation sequences for the Kepler/Opal stack, showing communication between workﬂow engines and grid services.
5. Evaluation
The experimental performance evaluation is focused on exposing performance overheads of the hybrid model, and
does so through the following metrics:

512

Beth Plale et al. / Procedia Computer Science 9 (2012) 508 – 517

Figure 3: Sequence diagram showing workﬂow engine interaction between Trident and Kepler. Solid lines are events, dotted lines are remote grid
services events, and gray lines are notiﬁcations.

1. Overhead of remote engine execution. Measured by the time to invoke workﬂow instance on local machine
versus time to invoke workﬂow through a remote workﬂow engine. This is the cost in time incurred by using a
second workﬂow engine.
2. Diﬀerence between workﬂow execution through a second workﬂow engine versus directly accessing remote
grid services.
3. Variability in approaches for subworkﬂow stacks: gives variation for given service time or latency. Best and
worst case can often vary signiﬁcantly.
The test environment has the Trident Workﬂow Workbench and the Workﬂow Foundation running on a Windows
2008 R2 Enterprise edition server. The underlying machine has an Intel E7540 CPU, which consists of four 2.0 GHz
processors, each with six cores (24 cores total); the machine has 128 GB RAM. We use Trident version 1.2.1 and
Workﬂow Foundation 4. The remote services, shown as System 2 in Figure 1, are run on a cluster consisting of 16
dual-socket, 2 core (4 total cores/node) MD Opteron system with 16 GB of memory per node running 64-bit Red Hat
Enterprise Linux. Nodes are connected via Gigabit Ethernet. Local disks are 73 GB and nodes in the cluster have GB
Ethernet connectivity to a 24 TB NAS that is front-ended by a ﬁle server running NFS v4.
The remote workﬂow tasks are executed on a 1024 IBM JS21 Blade server (IU Big Red), each with two dual-core
PowerPC 970 MP processors, 8GB of memory per node at Indiana University. Compute nodes are connected via
Gigabit Ethernet to a GPFS ﬁle system hosted on 16 IBM p505 Power5 systems. We submit jobs to run on the system
through IBM’s LoadLeveler resource manager. LoadLeveler, in turn, relies on Adaptive Computing’s Moab scheduler
to dispatch user jobs to appropriate and available compute nodes. In our experiment, we use BigRed queue set up for
experimental use. At 4 node, 16 core/node capacity, it is small but oﬀers low queue latencies.
The data intensive workﬂows make use of the underlying Lustre Distributed File System for ﬁle movement within
the same site. The workﬂow suites used in the evaluation are capable of moving ﬁles between remote sites using
GridFTP and RFT, but the data intensive workﬂows in this evaluation focus on data movement within the same site.
Every activity in the Data Intensive Sequential Workﬂow copies the data ﬁle to its input working directory before
it starts processing and once it ﬁnishes the processing copies the output ﬁle to its output working directory. So
every activity in Data Intensive Sequential Workﬂow will copy its input ﬁle from the output working directory of the
previous activity. The ﬁrst activity would copy the input ﬁle from the input parameter to the workﬂow.
The Data Intensive Parallel Workﬂow is launched with the location of the data ﬁles as the inputs to the workﬂow.
Each activity in the workﬂow copies the input data ﬁles to its input working directory and produces its output ﬁles to
its output working directory. The last activity of the Data Intensive Parallel Workﬂow gathers all the outputs from the
parallel activities to a single output directory. Each test is executed ﬁve times and minimum, maximum and median
values are computed.
The secondary workﬂow systems are Apache ODE workﬂow engine and the Kepler workﬂow system. XBaya [29]
mediates between Trident and the Apache ODE workﬂow engine. ODE uses GFac to manage interactions with
application services. GFac instantiates workﬂow tasks if needed; it moves data for Grid resources and interacts with

Beth Plale et al. / Procedia Computer Science 9 (2012) 508 – 517

513

job submission and resource management providers to schedule jobs to Grid resources. We set up our experiment so
that GFac uses Sigiri to submit jobs to the HPC resource. Kepler uses Opal actors to interact with job submission
components. We conﬁgure Opal to use GRAM actors to submit and manage jobs. Kepler invokes an Opal actor which
is a generic web service client to launch job and query job status. The Opal actor submits a job to the Opal server
which submits the job to the resource manager in BigRed. Once a job is done, the output data is staged from BigRed
to Opal Server. When the Opal job status is COMPLETE, the actor will be terminated.
The remote grid services we use are GFac and Opal. Trident interacts directly either with GFac or Opal server to
get the jobs scheduled and executed. Trident also uses a custom ﬁle movement utility, implemented as an Opal service,
for the ﬁle movement to the supercomputer resources, in the Opal-based workﬂows case. In the case of GFac/Sigiri
remote grid services, GFac serves as data mover.
5.1. Baseline execution case
We capture the baseline measurement of workﬂow overhead by running a workﬂow under Trident and on Trident’s
host machine, then compare that against the same workﬂow executed by a secondary workﬂow engine. In order to
have a fairer comparison, we ignore waiting time in the job queue in the remote case.
For the compute-intensive workﬂows, the remote ODE workﬂow engine instance saw only 2.4% higher execution time than local execution. The remote grid service instance using GFac/Sigiri, on the other hand, had 15.5%
higher execution time than local machine execution. This diﬀerence is because of the overhead in invoking GFac,
the service factory, for dynamic web service creation. Kepler workﬂow and Opal/GRAM grid services showed only
5% higher execution time than local machine execution. The diﬀerence in time can be attributed to the diﬀerence in
architecture between local and workﬂow and remote grid execution, namely, the overheads for invocation of remote
workﬂow engine, application scheduling and invocation on the grid. The 10.5% higher overhead for GFac/Sigiri versus Opal/GRAM can loosely be seen as the overhead of dynamic service creation. The service creation has to be done
manually for Opal before any service invocation. Because of this the overhead of Opal invocations does not include
service creation. In order to ensure network latencies did not inﬂuence our measurements, logs for remote invocation
were constantly monitored for anomalies. To ensure availability of the remote engine during the experiments, it was
set up as a persistence service, and monitored for availability. Also, the queue wait times were measured and removed
from the overhead to remove the eﬀect of varying queue wait times. Other side-eﬀects like degree of utilization were
prevented by setting up the remote engines as a dedicated service for experimentation.

(a) ODE and GFac/Sigiri: compute intensive workﬂow

(b) Kepler and Opal/GRAM: compute intensive workﬂow

Figure 4: Compute intensive workﬂows

5.2. Remote access case
Recall that we have four remote cases two where Trident works with a secondary workﬂow engine (Kepler and
ODE), and two where Trident executes a remote grid task directly using Opal or Sigiri. Service time for all four cases

514

Beth Plale et al. / Procedia Computer Science 9 (2012) 508 – 517

broken out by major workﬂow stages is shown Figure 4. In Figure 4(a), the Gfac/Sigiri case has notable Service
Creation and Trident Termination latencies. The ODE stack has the ﬂexibility to reuse a dynamic service, but without
ODE, Trident must ask GFac to dynamically start a new service for every task invocation. Too, in the Gfac/Sigiri
case, Trident has more activities to manage. This higher number of workﬂow activities within Trident contributes to
the latency captured as Trident Termination. Since ODE uses an internal data movement tool (i.e., GFac) and every
component in the synthetic workﬂow consumes the same input ﬁle, the tools move the ﬁle for the ﬁrst activity and
re-use the same location for the other nodes in the workﬂow. But for the Gfac/Sigiri case, ﬁles are moved for every
service invocation adding more overhead to the workﬂow execution.
In Figure 4(b), performance of Kepler is compared to Opal/GRAM remote grid task case for the compute intensive
workﬂow. Unlike GFac which create services dynamically, Opal uses pre-deployed web services and hence Service
Creation overhead exists neither in Kepler-Opal workﬂow nor in the Opal/GRAM case. In the Opal/GRAM case,
Trident has more activities to manage and track. It can be observed that the total execution times shown across the
four case in Figure 4 all lie within a range of 800-820 seconds for all four invocation types. The data intensive
workﬂow can be found in a longer technical report.

(a) ODE and Kepler: compute intensive workﬂow, sequential

(b) ODE and Kepler: data intensive workﬂow, sequential

Figure 5: Execution time variability per execution step.

5.3. Execution variability
An important aspect of system performance is the variability in performance observed over repeated runs. Figure 5
captures variation in execution times for ODE and Kepler solutions under both the sequential compute and sequential
data workﬂows. The error bars represent the logarithm of minimum and maximum values of a given measurement
and the plotted value represent the logarithmic median of the measured values.
The workﬂows on the ODE and Kepler stacks take approximately the same amount of time (16 sec), but examining
the deviation between minimum and maximum values, the ODE stack has a smaller deviation (106 sec) to Kepler’s
(1481 sec). The large deviation of the latter can be attributed to submission overheads. The ODE stack keeps the
submission overhead within a small range. Kepler uses GRAM as its job submission middleware and GRAM takes
a varying amount of time to get the job scheduled and executed in the compute resource. During this experiment
we also experienced job failures caused by diﬀerent GRAM failures and had to re-run our experiments on multiple
occasions. Additionally, the ”Application Invocation” overhead in ODE is signiﬁcantly lower than other stacks.
Higher application invocation overhead in Kepler could be attributed to ineﬃcient workﬂow activity and service
handling within the Opal actors. For data intensive sequential workﬂows the ODE and Kepler stacks show similar
performance numbers (80 sec). Kepler shows the lowest deviation (575 sec) with ODE at 650 sec. Opal stack
shows signiﬁcant overhead during the shutting down phase of the service (Service Termination). Further investigation
revealed that this is due to the implicit log ﬁle movements in Opal server.

Beth Plale et al. / Procedia Computer Science 9 (2012) 508 – 517

515

In addition to the overhead, the overall performance in a grid environment is inﬂuenced by the degree of utilization
of grid. In case of submission to a grid, we had a reservation on a queue and the queue wait times showed a standard
deviation of only around 8 seconds, for an average queue wait time for 460 seconds. As for the execution time, we
observed a very low standard deviation of only 7 seconds for an average execution time of 810 seconds.
6. Qualitative Aspects of Hybrid Model
Hybrid models of interoperability can also be interestingly examined on qualitative aspects, and do that in this
study examining several dimensions: 1.) Model of computation - execution model supported by a system, 2.) Level
of control at desktop, and ﬁnally 3.) Complexity of the system. Each is discussed in sections below.
6.1. Model of Computation
The Model of Computation (MOC) [6, 30] captures the interaction between activities. Intuitively, the MOC gives
interpretation to the edges that connect two vertices of a workﬂow graph or between workﬂow systems. Elmroth
et al. [6] state that ”sub-workﬂows are seen as black boxes and their internal MoC is not important to workﬂows
higher up in the hierarchy”, meaning that we need not consider the internal edges of the subgraph (sub-workﬂow).
But the black-box nature of the workﬂow model has advantages and disadvantages. The advantage can be seen in
Trident. Trident is a control-ﬂow workﬂow system. All scheduling decisions are based on static information and
this information is used to generate an Actor/activity ﬁring schedule in the form of a Windows Workﬂow Foundation
run-time script before it starts execution. But the disadvantage of a black box model is lack of control and problem
solving strategies over the entire workﬂow if something goes wrong. Too, how integrated is the sub workﬂow into the
top level workﬂow script? Are the semantics of the input and output edges of the sub workﬂow representable at the
higher level. We did not attempt to quantify the issues with semantics and error propagation during the course of the
evaluation, but clearly recognize how important the problem is to simplifying the overall user experience.
Finally, for a more extensible system, the lower level workﬂow systems should use a uniform interface when
communicating with the top level workﬂow system. In WS-VLAM [5] this is accomplished by means of a common
event bus, VL-e workﬂow bus. We did not implement this in our testbed.
6.2. Level of Control at Top Level System
What are the limits to a layered workﬂow system model? There are several we note here. Workﬂow engines often
depend on select services to carry out tasks needed during workﬂow execution, and can be limited by this dependency.
These tasks include data movement services, authentication and authorization frameworks, job submission and monitoring services. This functionality can limit the generality of the workﬂow engine, because it can only perform the
kinds of data movement it supports. But through the remote grid model, a user can use any data movement framework
she chooses and also has the ability to optimize data movements and data placement in compute resource.
If the top level workﬂow engine supports checkpointing and recovery operations, when a workﬂow fails at a
certain node, the workﬂow can be restarted from that node. But if the failed node represents a multi-task workﬂow,
the failure can take time to recover. For example, if a certain experiment contains 10 tasks to be executed and if it is
implemented as set of components, then a failure at the 8th node is not excessively costly, because the workﬂow can
be restarted to run at the 8th node. But if the same experiment is implemented as a workﬂow, and if the 3rd to 8th
component are in the subworkﬂow, failure of the 8th node requires the workﬂow to be restarted from the 3rd task.
The ODE workﬂow engine, particularly through its instantiation in the OGCE tool suite [31] is limited by its
ability to add new workﬂow activities. For functionality to be added, it must be exposed as a Web service. Both
Kepler and Trident workﬂow engines on the other hand are primarily targeted to desktop based workﬂow executions
and thus provide ﬂexibility to incorporate new functionality easily into the system. With the actor model in Kepler
and activity model in Trident, a user can program any functionality into the workﬂow, enabling it to support a wide
variety of functionality.
In experience in the research lab and in the classroom, Trident is easy to use. Programming new workﬂow activities
requires writing small C# activities. With its user-friendly interface and programming model, we believe the tool could
appeal to the scientist who is comfortable with a Windows platform and inclined to write and deploy new workﬂow
functionalities that execute within their local compute resources. We conducted a heuristic evaluation [32] of Trident,

516

Beth Plale et al. / Procedia Computer Science 9 (2012) 508 – 517

Swift, VisTrails, Kepler, Taverna, and Ode and found that Trident had the lowest time to get a custom workﬂow up
and running.
6.3. Architectural Simplicity
The remote grid service approach, in contrast to the remote workﬂow engine approach, gives the user more control
over the execution of the experiment. But with this control comes maintenance overhead and complexity, because the
user has to manage all the components and their interactions. For example, if any of the interfaces to the remote tasks
change, the workﬂow author has to update the components to account for these changes. In the remote workﬂow engine solution, these changes would be handled inside the calling workﬂow engine infrastructure. From our experience
with the LEAD [33] science gateway, grid middleware adds complexity to the architecture. Handling security issues
is also a known concern with these systems. This work becomes more complex with the reliability issues of these
middleware components [34]. When Trident is expected to interact directly with Sigiri and Opal, the workﬂow author
is responsible for handling the complexities for each activity, including authentication mechanisms, fault tolerance
and checkpointing. In the hybrid model, the user must provide perfect conﬁguration parameters for the next workﬂow
engine to function properly. As we discussed in section 5, the cost of failure of a workﬂow at a secondary workﬂow
system can be higher compared to the remote grid service approach. This gap further widens proportional to number
of nodes in the workﬂow.
7. Conclusion
The results of our study show that the remote workﬂow engine approach generally outperforms the remote
grid/cloud middleware approach. This is due in part to the fact that the workﬂow engines we selected are currently
in use in scientiﬁc experiments and so have many optimizations built into them. For example, the dynamic service
creation and lifecycle management within ODE stack through the generic factory service signiﬁcantly reduces the
overhead in handling application service invocations. Additionally, intelligent ﬁle movement services substantially
reduces overheads in data intensive workﬂows. We can also safely generalize the results of our performance evaluation to diﬀerent test workﬂows. This is because the overheads we see are largely associated with initialization and
termination of the components in the setup like the local workﬂow engine, remote workﬂow engine and remote grid
services and these overheads will hold for diﬀerent test samples, with variations only in the workﬂow execution time.
Grid middleware is responsible for a signiﬁcant amount of overhead in a scientiﬁc workﬂow stack scheduling jobs
into super-computing resources. The job failures and the higher variation of overheads we experienced during our
evaluation suggests that the instability and unpredictable behavior of grid middleware components have high impact
on the scientiﬁc workﬂow systems that are using them. However eﬃcient and optimized these workﬂow stacks are,
the issues in middleware can make these workﬂow suites uncertain, if not unusable.
Similar to other workﬂow systems, Trident facilitates workﬂow runs in Windows based environments. But we
think more improvements are necessary to make this toolkit more useable among the scientiﬁc research community.
For example, Trident lacks the support for parallel execution constructs. Even though activities are picked up and
scheduled in parallel, parallel workﬂows are executed sequentially.
Because scientiﬁc applications are written using a wide variety of programming and scripting languages, they are
often wrapped and exposed using interoperable methods in order to be invoked by workﬂows engines. We used two
generic service factories, GFac and Opal, to wrap applications. GFac exposes an application deployed in a supercomputing resource. But with the increased usage of cloud computing platforms for scientiﬁc workﬂow executions we
intend to expand our evaluation to include cloud computing platforms using other application wrapping methods.
8. Acknowledgements
This project was funded in part by the National Science Foundation grants NSF CSR-0720580 and NSF EIA0202048, and a gift from Microsoft. Our thanks to Felix Terkhorn for thoughtful discussions.

Beth Plale et al. / Procedia Computer Science 9 (2012) 508 – 517

517

References
[1] E. Deelman, Y. Gil, Managing large-scale scientiﬁc workﬂows in distributed environments: Experiences and challenges, in: 2nd IEEE Int’l
Conf. on e-Science and Grid Computing, IEEE Computer Society, 2006.
[2] T. Oinn, M. Greenwood, et al., Taverna: lessons in creating a workﬂow environment for the life sciences, Concurrency and Computation:
Practice and Experience 18 (10) (2006) 1067–1100.
[3] C. Catlett, The philosophy of TeraGrid: building an open, extensible, distributed TeraScale facility, in: ACM Int’l Symp. on Cluster Computing and the Grid, IEEE Computer Society, 2002.
[4] C. Goble, D. DeRoure, myexperiment: social networking for workﬂow-using e-scientists, in: 2nd workshop on workﬂows in support of
large-scale science, ACM, New York, NY, USA, 2007, pp. 1–2.
[5] Z. Zhao, S. Booms, A. Belloum, C. Laat, B. Hertzberger, Vle-wfbus: A scientiﬁc workﬂow bus for multi e-science domains, 2nd IEEE Int’l
Conf. on eScience and Grid Computing eScience06 (2006) 11–11.
[6] E. Elmroth, F. Hern´andez, J. Tordsson, Three fundamental dimensions of scientiﬁc workﬂow interoperability: Model of computation, language, and execution environment, Future Generation Computer Systems 26 (2) (2010) 245–256.
[7] R. Barga, J. Jackson, N. Araujo, D. Guo, N. Gautam, Y. Simmhan, Trident scientiﬁc workﬂow workbench, in: IEEE Int’l Conf. on e-Science,
IEEE Computer Society, 2008, pp. 317–318.
[8] I. Altintas, C. Berkley, E. Jaeger, M. Jones, B. Ludascher, S. Mock, Kepler: An extensible system for design and execution of scientiﬁc
workﬂows, in: 16th Int’l Conf. on Scientiﬁc and Statistical Database Management, IEEE Computer Society, 2004, pp. 423–424.
[9] Apache ode (orchestration director engine), http://ode.apache.org/.
[10] J. Yu, R. Buyya, A taxonomy of scientiﬁc workﬂow systems for grid computing, ACM Sigmod Record 34 (3) (2005) 44–49.
[11] G. Kandaswamy, D. Gannon, A Mechanism for Creating Scientiﬁc Application Services on Demand from Workﬂows, in: Int’l Conference
on Parallel Processing Workshops, 2006, pp. 25–32.
[12] S. Krishnan, L. Clementi, J. Ren, P. Papadopoulos, W. Li, Design and evaluation of opal2: A toolkit for scientiﬁc software as a service, in:
2009 Congress on Services - I, IEEE Computer Society, Washington, DC, USA, 2009, pp. 709–716.
[13] Y. Han, A. Sheth, C. Bussler, A taxonomy of adaptive workﬂow management, in: Workshop of ACM Conference on Computer Supported
Cooperative Work, 1998.
[14] I. Taylor, M. Shields, I. Wang, A. Harrison, The triana workﬂow environment: Architecture and applications, Workﬂows for e-Science (2007)
320–339.
[15] C. Team, Dagman (directed acyclic graph manager)Http://www.cs.wisc.edu/condor/dagman.
[16] Y. Gil, V. Ratnakar, E. Deelman, G. Mehta, J. Kim, Wings for pegasus: Creating large-scale scientiﬁc applications using semantic representations of computational workﬂows, in: Proceedings of the National Conference on Artiﬁcial Intelligence, Vol. 22, AAAI Press, Menlo Park,
CA, 2007, p. 1767.
[17] E. Deelman, G. Singh, M. Su, et al., Pegasus: A framework for mapping complex scientiﬁc workﬂows onto distributed systems, Scientiﬁc
Programming 13 (3) (2005) 219–237.
[18] M. Litzkow, M. Livny, M. Mutka, Condor - a hunter of idle workstations, in: 8th Int’l Conf. of Distributed Computing Systems, 1988, pp.
104–111.
[19] Z. Jinde, Study on interoperability of workﬂow management systems, Journal of University of Electronic Science and Technology of China
2.
[20] J. Hayes, E. Peyrovian, S. Sarin, M. Schmidt, K. Swenson, R. Weber, Workﬂow interoperability standards for the internet, Internet Computing
4 (3) (2002) 37–45.
[21] M. Zur Muehlen, A framework for xml-based workﬂow interoperability–the AFRICA project, in: Americas Conference on Information
Systems, 2000.
[22] P. Kacsuk, G. Sipos, Multi-grid, multi-user workﬂows in the P-GRADE grid portal, Journal of Grid Computing 3 (3) (2005) 221–238.
[23] L. Wang, M. Kunze, J. Tao, Performance evaluation of virtual machine-based grid workﬂow system, Concurrency and Computation: Practice
and Experience 20 (2008) 1759–1771.
[24] C. Stratan, A. Iosup, D. H. J. Epema, A performance study of grid workﬂow engines, in: 9th IEEE/ACM Int’l Conf. on Grid Computing,
GRID ’08, IEEE Computer Society, 2008, pp. 25–32.
[25] M. Gillmann, R. Mindermann, G. Weikum, Benchmarking and conﬁguration of workﬂow management systems, in: 7th Int’l Conf. Cooperative Information Systems, 2000, pp. 186–197.
[26] H.-L. Truong, S. Dustdar, T. Fahringer, Performance metrics and ontologies for grid workﬂows, Future Gener. Comput. Syst. 23 (6) (2007)
760–772.
[27] I. Foster, Globus Toolkit Version 4: Software for Service-Oriented Systems, IFIP International Conference.
[28] E. Chinthaka, B. Plale, Sigiri: Uniform research abstraction for grids and clouds, to appear Concurrency and Computation: Practice and
Experience.
[29] S. Shirasuna, A dynamic scientiﬁc workﬂow system for the web services architecture, Ph.D. thesis, Indiana University (2007).
[30] A. Goderis, C. Brooks, I. Altintas, E. Lee, C. Goble, Heterogeneous composition of models of computation, Future Generation Computer
Systems 25 (5) (2009) 552–560.
[31] The open grid computing environments portal and gateway toolkit, http://www.collab-ogce.org.
[32] B. Plale, G. Fox, S. Kowalczyk, K. Chandrasekar, escience workﬂows 9 years out: Converging on a vision, Tech. rep., Pervasive Technology
Institute, Indiana University, Bloomington, Indiana (2011).
[33] K. Droegemeier, D. Gannon, D. Reed, B. Plale, et al., Service-oriented environments for dynamically interacting with mesoscale weather,
Computing in Science and Engineering 7 (6) (2005) 12–29.
[34] S. Marru, S. Perera, M. Feller, S. Martin, Reliable and scalable job submission: LEAD science gateways testing and experiences with WS
GRAM on Teragrid resources, in: TeraGrid Conference, 2008.

