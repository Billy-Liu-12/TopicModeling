An Ensemble Kalman-Particle
Predictor-Corrector Filter for Non-Gaussian
Data Assimilation
Jan Mandel1,2 and Jonathan D. Beezley1,2
2

1
University of Colorado Denver, Denver, CO 80217-3364, USA
National Center for Atmospheric Research, Boulder, CO 80307-3000, USA
{jon.beezley.math,jan.mandel}@gmail.com

Abstract. An Ensemble Kalman Filter (EnKF, the predictor) is used
make a large change in the state, followed by a Particle Filer (PF, the
corrector), which assigns importance weights to describe a non-Gaussian
distribution. The importance weights are obtained by nonparametric
density estimation. It is demonstrated on several numerical examples
that the new predictor-corrector ﬁlter combines the advantages of the
EnKF and the PF and that it is suitable for high dimensional states
which are discretizations of solutions of partial diﬀerential equations.
Keywords: Dynamic data driven application systems, data assimilation,
ensemble Kalman ﬁlter, particle ﬁlter, tracking, non-parametric density
estimation, Bayesian statistics.

1

Introduction

Dynamic Data Driven Application Systems (DDDAS) [1] aim to integrate data
acquisition, modeling, and measurement steering into one dynamic system.
Data assimilation is a statistical technique to modify model state in response
to data and an important component of the DDDAS approach. Models are
generally discretizations of partial diﬀerential equations and they may have easily
millions of degrees of freedom. The model equations themselves are posed in
functional spaces, which are inﬁnitely dimensional. Because of nonlinearities,
the probability distribution of the state is usually non-Gaussian.
A number of methods for data assimilation exist [2]. Filters attempt to ﬁnd
the best estimate from the model state and the data up to the present. We
present a combination of the Ensemble Kalman Filter (EnKF) [3] and the
Sequential Importal Sampling (SIS) particle ﬁlter (PF) [4]. The EnKF is a
Monte-Carlo implementation of the Kalman Filter (KF). The KF is an exact
method for Gaussian distributions. However, it needs to maintain the state
covariance matrix, which is not possible for large state dimension. The EnKF
and its variants [6,7] replace the covariance by the sample covariance computed
from an ensemble of simulations. Each ensemble member is advanced in time by
the model independently until analysis time, when the data is injected, resulting
in changes in the states of the ensemble members. Particle ﬁlters also evolve a
G. Allen et al. (Eds.): ICCS 2009, Part II, LNCS 5545, pp. 470–478, 2009.
c Springer-Verlag Berlin Heidelberg 2009

An Ensemble Kalman-Particle Predictor-Corrector Filter

471

ensemble of simulations, but they assign to each ensemble member a weight and
the analysis step updates the weights.
The KF and the EnKF represent the probability distributions by the mean
and the covariance, and so they assume that the distributions are Gaussian.
This shows in the tendency of EnKF to smear distributions towards unimodal,
as illustrated in Sec. 3 below. So, while the EnKF has the advantage that it
can make large charges in the state and the ensemble can represent an arbitrary
distribution, the EnKF is still essentialy limited to Gaussian distributions. On
the other hand, the PF can represent non-Gaussian distributions faithfully, but it
only updates the weights and cannot move ensemble members in the state space.
Thus a method that combines the advantages of both without the disadvantages
of either is of interest. The design of more eﬃcient non-Gaussian ﬁlters for largescale problems has been the subject of signiﬁcant interest, often using Gaussian
mixtures and related approaches [8].
The predictor-corrector ﬁlter presented here uses an EnKF as a predictor to
move the state distribution towards the correct region and then a PF as corrector
to adjust for a non-Gaussian character of the distribution. Nonparametric density
estimation is used to compute the weights in the PF. The combined predictorcorrector method appears to work well on problems where either EnKF of
PF fails, and it does not degrade the performace of the EnKF for Gaussian
distributions. Predictor-corrector ﬁlters were ﬁrst formulated in [9,10]. Related
results and some probabilistic background can be found in [11].

2

Formulation of the Method

A common procedure to construct an initial ensemble is as a sum with random
coeﬃcients [12],
m

λn dn ϕn ,

u=

dn ∼ N (0, 1) ,

{dn } independent,

(1)

n=1

where {ϕn } is an orthonormal basis in the space state V = Rm equipped with the
Euclidean norm · . The elements of V are column vectors of values of functions
on a mesh in the spatial domain. The basis functions ϕn are smooth for small
n and more oscillatory for large n. If the the coeﬃcients λn → 0 suﬃciently
fast, the series (1) converges and u is a random smooth function in the limit as
m → ∞. The sum (1) deﬁnes a Gaussian random variable with the eigenvalues
of its covariance matrix equal to λ2k . Possible choices of {ϕk } include a Fourier
basis, such as the sine or cosine functions, or bred vectors [2]. On the state space
V , we deﬁne another norm by
m

u

2
U

=

1 2
c ,
κ2 n
n=1 n

m

u=

cn ϕn .

(2)

n=1

Note that if κn = 1, · U is just the original norm · on V . We generally use κn
adapted to the smoothness of the functions in the initial ensemble, λn /κn → 0
as n → ∞.

472

J. Mandel and J.D. Beezley

A weighted ensemble of N simulations (uk , wk )N
k=1 is initialized according to
(1), with equal weights wk = 1/N . The ensemble members are advanced by the
model and at given points in time, new data is injected by an analysis step. The
data consists of vector d of measurements, observation function h (u) = Hu,
also called forward operator, here assumed to be linear, which links the model
state space with the data space, and data error distribution, here assumed to be
Gaussian with zero mean and known covariance R. The value of the observation
function Hu is what the data vector would be in the absence of model and data
errors. The value of the probability density of the data error distribution at
the data vector d for a given value of the observation function Hu is called data
likelihood and denoted by p(d|u). The probability distribution of the model state
before the data is injected is called the prior or the forecast, and the distribution
after the data is injected is called the posterior or the analysis. Assuming the
forecast probability distribution has the density pf , the density pa of the analysis
is found from the Bayes theorem,
pa (u) ∝ p (d|u) pf (u) ,

(3)

where ∝ means proportional.
Instead of working with densities, the probability distributions are
approximated by weighted ensembles. We will call the following analysis step
algorithm EnKF-SIS.
Predictor. Given a forecast ensemble
ufk , wkf
the members

uak

N
k=1

N

,

wkf ≥ 0,

wkf = 1,
k=1

of the analysis ensemble are found from the EnKF,

uak = ufk + K(dk − Hufk ),

dk ∼ N (d, R) ,

K = QH T HQH T + R

−1

where dk are randomly sampled from the data distribution, and Q is the forecast
ensemble covariance,
N

wk uk − uf

Q=

uk − uf

T

N

,

wkf ufk .

uf =

k=1

(4)

k=1

This is the EnKF from [3], extended to weighted ensembles by the use of the
weighted sample covariance (4).
Corrector. The analysis members uak are thought of as a sample from some
proposal distribution, with density pp . Ideally, the analysis weights wka should be
computed from the SIS update as [4]
wka ∝ p (d|uak )

pf (uak )
.
pp (uak )

However, the ratio of the densities is not known, so it is replaced by a
noparametric estimate inspired by [13], giving

An Ensemble Kalman-Particle Predictor-Corrector Filter

wka

∝

p (d|uak )

: uf −ua
k
:

ua −ua
k

U

≤hk

wkf

≤hk

1
N

U

473

N

wka = 1.

,
k=1

The bandwidth hk is the distance from uak to the N 1/2 -th nearest member ua ,
measured in the · U norm.

3

Numerical Results

Figure 1 demonstrates a failure of EnKF for non-Gaussian distributions, while
SIS and EnKF-SIS do ﬁne. We construct a bimodal prior in 1D by ﬁrst sampling
from N (0, 5) and assigning the weights by
2

2

wf (xi ) = e−5(1.5−xi ) + e−5(−1.5−xi ) .

(a) Prior and data likelihood densities

(b) Posterior from EnKF

(c) Posterior from SIS

(d) Posterior from EnKF-SIS

Fig. 1. Data assimilation with bimodal prior. EnKF fails to capture the non-Gaussian
features of the posterior, but both SIS and EnKF-SIS represent the nature of the
posterior reasonably well.

474

J. Mandel and J.D. Beezley

The data likelihood is Gaussian. The ensemble size was N = 100.
The next 1D problem demonstrates that EnKF-SIS is doing better that either
EnKF or SIS alone in ﬁltering for the stochastic diﬀerential equation [14]
du
= 4u − 4u3 + κη,
dt

(5)

where η(t) is white noise. The parameter κ controls the magnitude of the
stochastic term.
The deterministic part of this diﬀerential equation is of the form
du
= −f (u) ,
dt
where the potential f (u) = −2u2 + u4 . The equilibria are given by f (u) = 0;
there are two stable equilibria at u = ±1 and an unstable equilibrium at u = 0.
The stochastic term of the diﬀerential equation makes it possible for the state to
ﬂip from one stable equilibrium point to another; however, a suﬃciently small κ
insures that such an event is rare.
A suitable test for an ensemble ﬁlter will be to determine if it can properly
track the model as it transitions from one stable ﬁxed point to the other. From
Fig. 1, it is clear that EnKF will not be capable of describing the bimodal nature

Fig. 2. Ensemble ﬁlters mean and optimal ﬁlter mean for stochastic ODE (5). EnKFSIS was able to approximate the optimal solution better than either SIS or EnKF
alone.

An Ensemble Kalman-Particle Predictor-Corrector Filter

(a)

(b)

(c)

(d)

475

Fig. 3. EnKF smears non-Gaussian distribution. The horizontal axis is the spatial
coordinate x ∈ [0, π]. The vertical axis is the value of function u. The level of shading
on each vertical line is the marginal density of u at a ﬁxed x, computed from a
histogram with 50 bins. While EnKF completely ignores the non-Gaussian character
of the posterior and centers the distribution around u = 0, EnKF-SIS shows darker
bands at the edges.

of the state distribution so it will not perform well when tracking the transition.
Also, when the ensemble is centered around one stable point, it is unlikely that
some ensemble members would be close to the other stable point. It is known
that SIS can be very slow in tracking the transition and EnKF can do better
[14]. Figure 2 demonstrates that EnKF can outperform both.
The solution u of (5) is a random variable dependent on time, with density
p(t, u). The evolution of the density in time is given by the Fokker-Planck
equation, which was solved numerically on a uniform mesh from u = −3 to
u = 3 with the step Δu = 0.01. At the analysis time, the optimal posterior
density was computed by multiplying the probability density of u by the data
likelihood following (3) and then scaling so that again pdu = 1, using numerical

476

J. Mandel and J.D. Beezley

(a)

(b)

(c)

(d)

Fig. 4. SIS cannot make a large update. The horizontal axis is the spatial coordinate
x ∈ [0, π]. The vertical axis is the value of function u. The level of shading on each
vertical line is the marginal density of u at a ﬁxed x, computed from a histogram with
50 bins. While EnKF and EnKF-SIS create ensembles that are attracted to the data
value u(π/2) = 7, SIS cannot reach so far because there are no such members in this
relatively small ensemble of size N = 100.

quadrature by the trapezoidal rule. The data points were taken from one solution
of this model, called a reference solution, which exhibits a switch at time t ≈ 1.3.
The data error distribution was normal with the variance taken to be 0.1 at
each point. To advance the ensemble members and the reference solution, we
have solved (5) by the explicit Euler method with a random perturbation from
1/2
N (0, (Δt) ) added to the right hand side in every step [16]. The simulation
was run for each method with ensemble size 100, and assimilation performed for
each data point.
Finally, typical results for ﬁltering in the space of functions on [0, π] of the
form
d

u=

cn sin (nx)
n=1

(6)

An Ensemble Kalman-Particle Predictor-Corrector Filter

477

are in Figs. 3 and 4. The ensemble size was N = 100 and the dimension of
the state space was d = 500. The Fourier coeﬃcients were chosen λn = n−3 to
generate the initial ensemble from (1), and κn = n−2 for the norm in the density
estimation.
Figure 3 again shows that EnKF cannot handle bimodal distribution. The
prior was constructed by assimilating the data likelihood
⎧
⎨ 1/2 if u(π/4) and
u (3π/4) ∈ (−2, −1) ∪ (1, 2)
p(d|u) =
⎩
0 otherwise
into a large initial ensemble (size 50000) and resampling to the obtain the forecast
ensemble size N = 100 with a non-Gaussian density. Then the data likelihood
u (π/2) − 0.1 ∼ N (0, 1) was assimilated to obtain the analysis ensemble.
Figure 4 shows a failure of SIS. The prior ensemble sampled from Gaussian
distribution with coeﬃcients λn = n−3 using (1) and (6), and the data likelihood
was u (π/2) − 7 ∼ N (0, 1).

4

Conclusion

We have demonstrated the potential of a predictor-corrector ﬁlter to perform a
successful Bayesian update in the presence of non-Gaussian distributions, large
number of degrees of freedom, and large change of the state distribution. Open
questions include convergence of the ﬁlter in high dimension when applied to
multiple updates over time, mathematical convergence proofs for the density
estimation and for the Bayesian update, and performance of the ﬁlters when
applied to systems with a large number of diﬀerent physical variables and modes,
as is common in atmospheric models.

Acknowledgements
This work was supported by the National Science Foundation under grants CNS0719641, ATM-0835579, and CNS-0821794.

References
1. Darema, F.: Dynamic data driven applications systems: A new paradigm for
application simulations and measurements. In: Bubak, M., van Albada, G.D., Sloot,
P.M.A., Dongarra, J. (eds.) ICCS 2004. LNCS, vol. 3038, pp. 662–669. Springer,
Heidelberg (2004)
2. Kalnay, E.: Atmospheric Modeling, Data Assimilation and Predictability.
Cambridge University Press, Cambridge (2003)
3. Burgers, G., van Leeuwen, P.J., Evensen, G.: Analysis scheme in the ensemble
Kalman ﬁlter. Monthly Weather Rev. 126, 1719–1724 (1998)
4. Doucet, A., de Freitas, N., Gordon, N. (eds.): Sequential Monte Carlo in Practice.
Springer, Heidelberg (2001)

478

J. Mandel and J.D. Beezley

5. Evensen, G.: The ensemble Kalman ﬁlter: Theoretical formulation and practical
implementation. Ocean Dynamics 53, 343–367 (2003)
6. Anderson, J.L.: An ensemble adjustment Kalman ﬁlter for data assimilation.
Monthly Weather Rev. 129, 2884–2903 (1999)
7. Mitchell, H.L., Houtekamer, P.L.: An adaptive ensemble Kalman ﬁlter. Monthly
Weather Rev. 128, 416–433 (2000)
8. Bengtsson, T., Snyder, C., Nychka, D.: Toward a nonlinear ensemble ﬁlter for high
dimensional systems. J. of Geophysical Research - Atmospheres 108(D24), STS
2–1–10 (2003)
9. Mandel, J., Beezley, J.D.: Predictor-corrector ensemble ﬁlters for the assimilation of
sparse data into high dimensional nonlinear systems. CCM Report 232, University
of Colorado Denver (2006),
http://math.ucdenver.edu/ccm/reports/rep232.pdf
10. Mandel, J., Beezley, J.D.: Predictor-corrector and morphing ensemble ﬁlters for
the assimilation of sparse data into high dimensional nonlinear systems. In:
11th Symposium on Integrated Observing and Assimilation Systems for the
Atmosphere, Oceans, and Land Surface (IOAS-AOLS), CD-ROM, Paper 4.12,
87th American Meteorological Society Annual Meeting, San Antonio, TX (2007),
http://ams.confex.com/ams/87ANNUAL/techprogram/paper_119633.htm
11. Mandel, J., Beezley, J.D.: Predictor-corrector ensemble ﬁlters for data assimilation
into high-dimensional nonlinear systems (2009) (in preparation)
12. Evensen, G.: Sequential data assimilation with nonlinear quasi-geostrophic model
using Monte Carlo methods to forecast error statistics. J. of Geophysical
Research 99 (C5), 143–162 (1994)
13. Loftsgaarden, D.O., Quesenberry, C.P.: A nonparametric estimate of a multivariate
density function. Ann. Math. Stat. 36, 1049–1051 (1965)
14. Kim, S., Eyink, G.L., Restrepo, J.M., Alexander, F.J., Johnson, G.: Ensemble
ﬁltering for nonlinear dynamics. Monthly Weather Rev. 131, 2586–2594 (2003)
15. Miller, R.N., Carter, E.F., Blue, S.T.: Data assimilation into nonlinear stochastic
models. Tellus 51A, 167–194 (1999)
16. Higham, D.J.: An algorithmic introduction to numerical simulation of stochastic
diﬀerential equations. SIAM Rev. 43, 525–546 (2001)

