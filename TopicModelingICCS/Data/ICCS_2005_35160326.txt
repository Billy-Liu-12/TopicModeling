A Grid-Based Bridging Domain Multiple-Scale Method
for Computational Nanotechnology
Shaowen Wang1, Shaoping Xiao2,3, and Jun Ni1,2,4
1

Academic Technology -- Research Service, Information Technology Services
2
Department of Mechanical and Industrial Engineering
3
Center for Computer-Aided Design
4
Department of Computer Science,
The University of Iowa, Iowa city, IA, 52242, USA
{Shaowen-wang, shaoping-xiao, jun-ni}@uiowa.edu

Abstract. This paper presents an application-level Grid middleware framework
to support a bridging domain multi-scale method fornumerical modeling and
simulation in nanotechnology. The framework considers a multiple-length-scale
model by designing specific domain-decomposition and communication algorithms based on Grid computing. The framework is designed to enable researchers to conductlarge-scale computing in computational nanotechnology
through the use of Grid resources for exploring microscopic physical properties
of materials without losing details of nanoscale physical phenomena.

1 Introduction
In the study of nano-materials or nano-devices, a molecular dynamics (MD) model [1]
plays an important role. However, many existing MD algorithms have limitations
either on length or on time scales due to the lack of enough compute power. Even the
most powerful high performance computing (HPC) system is still not powerful
enough to perform a complete MD simulation [2]. Therefore, an innovative computational methodology for MD simulations is urgently needed.
Recently-developed concurrent multiscale methods [3] take into consideration of
multiple length scales in predicting macroscopic properties. For example, a bridging
domain coupling method [4] performs a linkage between the continuum and molecular models. However, if the ratio of mesh size in the continuum domain to the lattice
space in the molecular domain is too large, there is a difficult to eliminate a spurious
phenomenon in which a nonphysical wave reflection occurs at the interface of different length scales. Therefore, concurrent multiscale methods also require a tremendous
computing power. In addition, a small time step (the order of femtosecond) must be
selected to meet numerical stability of MD stimulation at the molecular model. Such
requirement causes to waste compute time during simulations. A feasible and efficient
method is needed to conduct a complete modeling and simulation of large nano systems within long time scales. This requirement stimulates the authors to develop a
Grid-based Bridging Domain Multiscale Method (GBDMM).
V.S. Sunderam et al. (Eds.): ICCS 2005, LNCS 3516, pp. 326 – 333, 2005.
© Springer-Verlag Berlin Heidelberg 2005

A Grid-Based Bridging Domain Multiple-Scale Method

327

Grid computing [5-7] enables users to assemble large-scale, geographicallydistributed, heterogeneous computational resources together for intensive MD
computations. This assemblage is dynamically orchestrated using a set of protocols
as well as specialized software referred to as Grid middleware. This coordinated
sharing of resources takes place within formal or informal consortia of individuals
and/or institutions that are often called Virtual Organizations (VO) [8]. In a similar
way, Grid application-specific middleware must be developed for MD related
multi-scale methods. We intend to develop a micro/macro coupling method and
Grid application middleware for multiscale computation in nano-science and
technology.
Section 2 reviews the bridging domain coupling method and describes its extensions in the GBDMM. Section 3 addresses domain-decomposition and communication algorithms for the GBDMM. Section 4 presents the framework of Grid nanomiddleware for GBDMM’s implementation.

2 Grid-Enhanced Bridging Domain Multiscale Method
(GBDMM)
2.1 Bridging Domain Coupling Method
The bridging domain coupling method contains a continuum domain, Ω C , which is
overlapped with a molecular domain, Ω M , through a bridging domain, Ω int (Fig. 1).
The scaling of the molecular and continuum Hamiltonians is performed in the bridging domain. The Lagrange multiplier method is used to conjunct the molecular domain with the continuum domain via constraint condition(s), u C (X I , t ) − u M
I (t ) = 0 ,
where u is the displacement. The total Hamiltonian of the system can be written as

H=

∫ β ( X)(K

Ω

C

)

(

)

+ U C dΩ C + ∑ (1 − β ( X I ) ) K MI + U MI +
I

C

(1)

∑ λ ( X ) (u ( X , t ) − u ( t ))
C

I

I∈Ω

I

M
I

int

where β (X ) equals zero in Ω M and unity in Ω C except the bridging domain.
Within the bridging domain, Ω int , it smoothly varies from zero to one. If an irregular
bridging domain is considered, a nonlinear function β (X ) can be introduced. The
function can be derived from a signed distance function, or defined by radial basis
functions from a set of points. K and U are the kinetic and potential energies. λ (X )
is a Lagrange multiplier field that can be obtained from the finite element approximation as shown in Fig. 1. Based on the classical Hamiltonian mechanics, the discrete
equations of Equation (1) can be expressed as

328

S. Wang, S. Xiao, and J. Ni

Fig. 1. The bridging domain coupling model for a graphite sheet

CI = f IextC − f Iint C − f ILC
M Iu
m I 
u IM = f IextM − f Iint M − f ILM

where

Fig. 2. Length scales covered in the BDMM

in ΩC ,
in Ω M

(2)

M I and m I are the modified nodal and atom masses; f Iext and f Iint are the

external and internal forces, independently; f IL is the force due to constraints in the
bridging domain. During simulation, velocities are obtained independently in the
continuum and molecular domains from Equation (2) without the consideration of
constraint forces. Constraint conditions are then used to calculate the Lagrange multipliers. Finally, the constraint forces in Equation (2) are used to correct the nodal/atom
velocities in the bridging domain, thus eliminating the wave reflection automatically.
2.2 Multiscale Considerations in GBDMM
Based on the bridging domain coupling method, we proposed a Bridging Domain
Multiscale Method (BDMM), which bridges from nanoscale to microscale to macroscale (Fig. 2). A bridging domain multiscale model contains a macro-scale domain
(~10-3 m) as a linear elasticity domain in which linear finite element methods (FEM)
can be used. The material properties can be obtained from the Representative Volume
Element (RVE) technique with molecular dynamics simulation. The BDMM embeds
a micro-scale domain (~10-6 m) in the macro-scale domain. It models physical behaviors of materials using either nonlinear FEM [9] or extended FEM [10] (if crack
propagation is considered at the microscale). Furthermore, a quasi-continuum technique [11] is implemented with the FE methods at the microscale, so that the constitutive equations can be constructed based on the atomic level energy. A sub-domain in a
microscale domain can be treated as a nanoscale (molecular) domain (~10-9 m) in
which molecular dynamics is employed. The BDMM couples various scales without
grading down mesh sizes. Such coupling leads to a straightforward implementation of
different time steps via a multiple-time-step algorithm.

A Grid-Based Bridging Domain Multiple-Scale Method

329

3 Domain Decomposition and Communication Algorithms
To perform simulations using GBDMM, an efficient algorithm of domain decomposition must be developed. An entire physical domain with multiple length scales specified can be hierarchically divided into multiple sub-domains. The first generation
M
(FG) sub-domains, such as Ω 01 (see Fig3), are generated based on variation in
length scales. These sub-domains are allocated on heterogeneous Grid computing
resources. Such first-generational domain decomposition is designed to avoid high
network latency, and hence produce a loosely-networked computation on Grids.
Each FG sub-domain can further be divided into a number of second generation
(SG) sub-domains being parallel-processed on different computational nodes within a
Grid resource. A bridging sub-domain is only shared between two SG sub-domains,
which belong to two different length scales, respectively. Although one SG subdomain can be overlapped with two or more SG sub-domains, the bridging subdomains are not overlapped with each other.
There are two types of communications in the GBDMM (see Fig. 4). The interdomain communication among the SG sub-domains takes place within an interconnection within a Grid resource, performing a traditional parallel computation. This
computation guarantees the consistency and integrity of SG sub-domains and boundary information for solving the governing equations of atom motions. The procedure
for solving equations of atom motion on each Grid cluster is independent. After solving the equations at each time step, communication takes place between two clusters.
The bridging coupling techniques are then applied to correct the trial velocities of
nodes or atoms in each bridging sub-domain independently.
For example, a simple GBDMM have two sub-domains Ω A and Ω B in the molecular domain, allocated to a single Grid resource (often a computing cluster), and
Ω C and Ω D in the continuum domain, allocated to another Grid resource. The
communication between the group of Ω A and Ω B , and the group of Ω C and Ω D
takes place on networks in a Grid environment. Ω A and Ω B share an atom E .
int
Bridging domains Ω int
A and Ω B perform energy and force calculations. The results
int
from Ω B are used to calculate the body potential functions and inter-atomic forces in
Ω A , since Ω A and Ω int
B share with same atoms. The size of bridging domains depends on the selected potential functions, especially the cutoff distance for Van der
Waals potential functions.
The continuum sub-domains, Ω C and Ω D share a boundary node F as shown in
Fig. 4. F C and F D represent the same node F in different sub-domains. Unlike
inter-domain communications (which do not require bridging domains), the communication between continuum sub-domains, Ω C and Ω D exchange internal forces of
boundary nodes through the networks among Grid resources. For instance, the internal force of node F C , calculated in the sub-domain Ω C is fetched to the sub-domain
Ω D to form an external force on the node of F D , but in an opposite direction. A
similar procedure is conducted to pass the internal force of node F D as the negatively external force of node F C . Therefore, the motions of node F , updated from
Ω C and Ω D are consistent.

330

S. Wang, S. Xiao, and J. Ni

Fig. 3. Domain decomposition for the
GBDMM method

Fig. 4. A demonstration for domain communications

After equations of motion are solved independently on each cluster, bridgingdomain communication takes place independently in each bridging sub-domain, such
B
B
B
as Ω AC and Ω BD . For instance, the trial velocities of atoms in Ω BD of Ω B are
transferred to Ω D , while the trial velocities of nodes in Ω BBD of Ω D are transferred
to Ω B . The bridging domain coupling technique is then applied to correct the trial
B
B
velocities of atoms in Ω BD of Ω B as well as in Ω BD of Ω D , respectively on each
mater nodes.

4 Framework of Grid Middleware for GBDMM Implementation
An application-level Grid middleware framework is designed (in Fig. 5) to enable a
GBDMM implementation. In this framework, middleware manages available computing resources, schedules decomposed domains to appropriate Grid resources (i.e.
clusters). It contains (1) a task scheduling advisor (TSA) that takes the result of domain decomposition as input to produce scheduling plans and achieve highperformance simulation through load-balancing; and (2) an information broker (IB)
that leverages Grid information services to provide the task scheduling advisor with a
resource discovery query functions [12].
The framework is centered on the TSA that is used to schedule sub-domains to an
appropriate set of Grid resources discovered to achieve optimal performance: tasks
are allocated to balance computations across the available set of resources. The subdomains are converted to the tasks that are placed in Grid-resource queues. The TSA
is designed to achieve high levels of performance by balancing tasks across available
resources. It determines the correspondence between tasks and the available Grid
resources. Within the TSA, several static scheduling strategies [13] such as Min-min
and Max-min have been implemented. The fine granularity of the domain decomposition in GBDMM is designed to achieve high level parallelism. Static scheduling

A Grid-Based Bridging Domain Multiple-Scale Method

331

strategies are developed to assign tasks based on computational intensity information
for each sub-domain, as well as the variability in the computing capacity of each Grid
resource. The workflow of the nano-middleware enhanced GBDMM with Grid computing is shown as Fig. 6.

Fig. 5. The Grid nano-middleware architecture. Note: Globus is a software project, the
purpose of which is to develop protocols and
services for computational grids. Condor is a
high throughput computing system

Fig. 6. The work flow of GBDMM

TSA is used to schedule sub-domains to an appropriate set of Grid resources discovered to achieve optimal performance: tasks are allocated in a way that balances
computations across the available selection of resources. The sub-domains information is converted to the tasks that are placed in Grid-resource queues. The TSA is
designed to achieve high levels of performance by balancing tasks across available
resources. It determines the correspondence between tasks and the available Grid
resources. The TSA deploys both static and dynamic scheduling, planning to operate
static and dynamic scheduling on tasks upon the computing recourse available. The
TSA dynamically facilitate the swap of tasks between computing resources according
to a dynamic performance evaluation. The fine granularity of the domain decomposition in GBDMM is carefully design to achieve high level parallelism. Static scheduling strategies are developed to assign tasks based on computational intensity information for each sub-domain, as well as the variability in the computing capacity of each
Grid resource.

332

S. Wang, S. Xiao, and J. Ni

Since Grid resources is very a complex set (resources and network complexities),
an IB is needed to perform a self-organized-resource discovery. The IB for the nanomiddleware is designed based on a self-organized grouping (SOG) method that manages the Grid complexity. Therefore, the IB contains three strategies: 1) to control
resource heterogeneity during a certain period of time; 2) to capture resource dynamics through a publishing mechanism and aggregating mechanism that handles resource dynamically; (3) to enable the assemblage of large number of resources for
applications across VO boundaries.
The developed information broker is based on a self-organized resource discovery
method. It is well known that currently, the Grid represents an extremely complex
distributed computing environment for developing applications because: 1) Grid resources (e.g., CPU, network, storage, and special-purpose instrument) are heterogeneous; 2) Grid resources are dynamic, and they tend to have faults that may not be predictable; 3) Grids are often distributed across security domains with large number of
resources involved. Due to this complexity, it has been a great challenge for developing efficient methods for Grid resource discovery that refers to the process of locating
satisfactory resources based on user requests. Grid resource discovery must handle the
search for desirable subsets of large number of accessible resources, the status and
availability of which dynamically change.
The workflow of the nano-middleware enhanced GBDMM with Grid computing is
shown as Fig. 6. The workflow demonstrates the integration of computational
nanotechnology and computer science in this multi-disciplinary project.

5 Conclusion and Future Work
The GBDMM is an extension of the bridging domain coupling method that allows
multiple length/time scales to be treated. The GBDMM’s domain decomposition and
communication are designed to be applicable to distributed Grid resources. The Grid
nano-application-middleware includes a task scheduling advisor and an information
broker. This middleware enables the GBDM to schedule resources, distribute tasks,
and achieve load balancing. Our ongoing work is to implement and apply GBDMM to
study mechanical properties of nanostructured materials.

References
1. Rountree C. L., Kalia R. K., Lidorikis E., Nakano A., Van B. L., Vashishta P: Atomistic
aspects of crack propagation in brittle materials: Multimillion atom molecular dynamics
simulation. Annual Review of Materials Research 32 (2002) 377-400
2. Abraham F. F., Walkup R., Gao H., Duchaineau M., Rubia T. D., Seager M.: Simulating
materials failure by using up to one billion atoms and the world’s fastest computer: brittle
fracture, Proceedings of the National Academy of Science of the United States of America.
99(9) (2002) 5777-5782
3. Muralidharan K., Deymier P. A., Simmons J. H.: A concurrent multiscale finite difference
time domain/molecular dynamics method for bridging an elastic continuum to an atomic
system, Modelling and Simulation in Materials Science and Engineering, 11(4) (2003)
487-501

A Grid-Based Bridging Domain Multiple-Scale Method

333

4. Xiao S. P., Belytschko T.: A bridging domain method for coupling continua with molecular dynamics, Computer Methods in Applied Mechanics and Engineering, 193 (2004)
1645-1669
5. Foster I., Kesselman C.: The Grid: Blue Print for a New Computing Infrastructure. Morgan
Kaufmann Publishers, Inc. San Francisco, CA (1999)
6. Foster I.: The Grid: A new infrastructure for 21st century science, Physics Today (2003)
February, 42-47
7. Berman F., Fox G., Hey T.: The Grid, past, present, future, Grid Computing, Making the
Global Infrastructure a Reality. edited by R. Berman, G. Fox, and T. Hey John Wiley &
Sons, West Sussex, England (2003)
8. Foster I., Kesselman C., Tuecke S., The anatomy of the grid: enabling scalable virtual
organizations, International Journal Supercomputer Applications. 15(3) (2002)
9. Belytschko T., Liu W. K., Moran B.: Nonlinear Finite Elements for Continua and Structures, Wiley, New York (2000)
10. Belytschko T., Parimi C., Moës N., Sukumar N., Usui S.: Structured extended finite
element methods for solids defined by implicit surfaces, International Journal for Numerical Methods in Engineering. 56 (2003) 609-635
11. Tadmor E. B., Ortiz M., Phillips R.: Quasicontinuum analysis of defects in solids, Philosophy Magazine A. 73 (1996) 1529-1563
12. Wang, S., Padmanabhan, A., Liu, Y., Briggs, R., Ni, J., He, T., Knosp, B. M., Onel Y.: A
multi-agent system framework for end-user level Grid monitoring using geographical information systems (MAGGIS): architecture and implementation. In: Proceedings of Lecture Notes in Computer Science. 3032 (2004) 536-543
13. Braun, T. D., Siegel, H. J., Beck, N., Boloni, L. L., Maheswaran, M., Reuther, A. I.,
Robertson, J. P., Theys, M. D., Yao, B., Hensgen, D., and Freund, R. F.: A comparison of
eleven static heuristics for mapping a class of independent tasks onto heterogeneous
distributed computing systems. Journal of Parallel and Distributed Computing. 61(2001)
810-837

