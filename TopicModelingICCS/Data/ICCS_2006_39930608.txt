Small WebComputing Applied to Distributed
Monte Carlo Calculations
P.A. Whitlock1 , Dino Klein1 , and Marvin Bishop2
1

Department of Computer and Information Sciences, Brooklyn College
2900 Bedford Avenue, Brooklyn, NY 11210-2889
whitlock@its.brooklyn.cuny.edu
2
Department of Mathematics/Computer Science, Manhattan College
Riverdale, New York 10471
marvin.bishop@manhattan.edu

Abstract. The software package, Small WebComputing (SWC), has
been applied to a Monte Carlo simulation of a system of hard hyperspheres in a variety of dimensions. The SWC environment was chosen
because once the framework is embedded in the application code, the
user has the choice of running the distributed computations as a set of
applets, as parallel threads on a symmetric multiprocessor or as independent processes distributed over a network. A description of the software
and a discussion of its ongoing evolution is presented.

1

Introduction

The properties of hard hyperspherical systems in a variety of dimensions have
been studied by us using Monte Carlo methods[1, 2, 3]. In order to study systems
with a minimum of several thousand hyperspheres a parallel or distributed computation was employed. The Small WebComputing (SWC) protocol developed
by Ying and co-workers[4, 5, 6] was selected for the project because of several
attractive features. Firstly, by simply incorporating several new classes into the
Java simulation code, the actual distribution of the code is performed by the
SWC software. The user is only responsible for the division of the calculation
into tasks that can be run in parallel. Secondly, the distributed tasks can be
run as Java applets and the sole requirement is that the host computer has the
appropriate Java libraries, i.e. there is no need to upload the complete SWC software onto every host. Thirdly, the distributed tasks can also be run as threads
or conventional distributed processes depending on the computers available.
An overview of the existing SWC software, a description of the Monte Carlo
application, and a discussion of the ongoing improvements are given in this
paper.

2

The Original SWC Software

The Small WebComputing framework was developed to provide master-worker
MIMD parallel programming software for a user[5]. It was meant to have an
V.N. Alexandrov et al. (Eds.): ICCS 2006, Part III, LNCS 3993, pp. 608–615, 2006.
c Springer-Verlag Berlin Heidelberg 2006

SWC Applied to Distributed Monte Carlo Calculations

609

easy to use design that separated the programming interface from the underlying hardware. To achieve its goals, the SWC software was written in Java. The
use of Java and Java applets provided inherent security through the ”sandbox”
security model[7, 8]. A set of extensible classes that sustain the framework for the
user’s computation were provided. In the original version of SWC, upper level
communication, e.g. between the Master process and the server, used the Transmission Control Protocol (TCP) and lower level communication used the User
Datagram Protocol (UDP). Eager scheduling[9, 10] was employed to guarantee
load balancing and task completion.
The SWC framework envisions a Computation as composed of three levels: the
Master, the Router, and the Worker components. A Computation that utilizes
the original SWC framework must contain a Master, one or more Routers, and
one or more Workers. The user programs the Master component so that it divides
the computation into smaller, independent tasks, referred to as Work Units.
The Workers are responsible for processing the Work Units (completing the
task that the Work Unit represents) and returning Result Units to the Master.
The Router component is provided by the software and its purpose is to act
as a liaison between the Master and the Workers. All communication with the
Workers is channeled through the Routers which protects the Master from being
ﬂooded by messages from Workers. Thus, the communication is tiered, with
the Master only communicating with the Routers, the Routers communicating
with both the Master and the Workers, and the Workers communicating only
with the Routers (no inter-Worker transmission of data is allowed). Under eager
scheduling Computations are guaranteed to ﬁnish. Any incomplete task, i.e. no
Result Unit has yet been returned for the task, is kept on the task queues by
the Routers and reassigned to idle Workers that request Work Units. The SWC
software is completely thread based and therefore can easily take advantage of
multiple CPU host computers.
In order to create a computation, the user must extend and implement the
following abstract classes and interfaces:
SWCWorkUnit: a class implementing this interface will contain data regarding the task that needs to be done
SWCResultUnit: a class implementing this interface will contain results from
the completed task
SWCMaster: the implementation of this abstract class must generate Work
Units, collect Result Units, and output ﬁnal computation results
SWCWorker: the implementation of this abstract class processes the Work
Unit and creates a Result Unit which is returned to the Master.
It is important to note that the concrete class, SWCRouter, which functions
as the Router in the system, does not need to be extended since it does not
perform any computation speciﬁc operations. In the original version of SWC,
the Router could not be shared among multiple computations because a Router
was connected to one Master managing a single Computation.

610

3

P.A. Whitlock, D. Klein, and M. Bishop

Small WebComputing 2: An Improved Version of the
Software

The second version of the Small Web Computing (SWC2) framework[11] improved the usability of the software. Computation speciﬁcity was completely
removed from the Master and Worker, and the framework hosted multiple, independent, concurrent Computations. The Router’s functionality was still conﬁned
to transferring Work or Result Units among the Master and the Workers. This
lack of speciﬁcity was achieved by incorporating a ”pull model” of communication between each level of the framework. A Worker process requested (pulled)
a Work Unit from a Router, while the Router requested Work Units from the
Master. Computation completion was still guaranteed through eager scheduling.
SWC2 employed TCP for all communication between the components. The
original SWC framework relied on UDP as the protocol for communicating with
Workers. This choice, coupled with the inability to fragment Work or Result
Units at the framework level, limited the size of Work or Result Units to 64
kilobytes. The decision to use UDP may have been due to the implementation
of the TCP classes in the JDK version current when the SWC framework was
originally written. In Table 3 of Ying, Arnow and Clark[4] which gives elapsed
times for 100 iterations of transmissions of various data sizes, the TCP tests failed
to transmit 4K and 64K data instances. Another diﬃculty with the use of UDP
was the unreliable delivery of Work Units and Result Units. The SWC framework
contained code to check periodically for the delivery of UDP packets. This code
was the source of timing bottlenecks and bugs which hindered the early usages of
the software. Since TCP guarantees delivery, the SWC2 framework does not need
to test for delivery and perform retransmission of Work and Result Units. These
changes simpliﬁed the software and lead to greater transparency of the SWC2
source code. There is no longer a restriction on the size of the data transmitted.
Exhaustive testing has shown that there is no problem with using TCP for large
data sets or frequently sent transmissions in SWC2.

4

The Monte Carlo Application

The behavior of multi-dimensional hyperspherical systems is governed by the
Boltzmann distribution function, f (R):
f (R) =

exp[−
exp[−

φ(rij )/kb T ]
φ(rij )/kb T ]dR

(1)

where R is the d-dimensional vector, ri = (xi1 , xi2 , · · · , xid ), i = 1, · · · , M of the
d*M coordinates of the centers of mass of the M hyperspheres in the simulation box, kb is Boltzmann’s constant and T is the absolute temperature of the
system. The pair potential, φ(rij ) represents the interaction between two hard
hyperspheres; rij =| ri − rj | and
φhs (rij ) =

∞, when rij < σ
0, when rij ≥ σ

(2)

SWC Applied to Distributed Monte Carlo Calculations

611

where σ represents the contact distance between the two hyperspheres or the
hypersphere diameter. The hyperspheres are initially placed in a simulation box
on a lattice. The side lengths of this box are determined by the number of
hyperspheres and the number density, ρ, the number of particles per unit volume.
The Metropolis [12] method was developed to assist in the Monte Carlo evaluation of integrals associated with complex physical systems. It guarantees the
asymptotically correct sampling of an integrand, interpreted as a probability
distribution function, by performing random walks in the many dimensional
conﬁguration space of the problem. These walks are generated by proposing a
move from the current position of a hypersphere, X, to a new position, X . The
new position is chosen from a probability distribution function, H(X | X). In
the present calculation, the new position is randomly chosen from a hyperbox
surrounding the current position of the center of mass of the hypersphere. The
proposed new position is then accepted or rejected based upon the probability
p(X | X) deﬁned as:
p(X | X) = min(1, q(X | X)

(3)

H(X | X )f (X )
H(X | X)f (X)

(4)

where
q(X | X) =

If the new position is not accepted, the hypersphere remains at its current location. The acceptance ratio, the number of accepted moves divided by the
number of total moves, is monitored. At higher densities, it is necessary to use
much smaller trial moves in order to have a “reasonable” conﬁguration change.
Each pass of the random walk consists of one attempted move for each of the
M hyperspheres. The move may or may not be accepted but is always counted
in the averaging. As the random walk proceeds, a recursive relationship develops between the phenomenological distribution functions, fn (R), represented by
each step of the random walk. As long as the system is ergodic and obeys detailed
balance [12, 13, 14], fn (R) → f (R) is guaranteed to be true as n, the number of
passes, becomes large.
The Monte Carlo averaging procedure is performed to compute expectation
values, < A >, of physical properties, A(R):
< A >=

A(R)f (R)dR
.
f (R)dR

(5)

In the Metropolis algorithm the successive positions of the hyperspheres are
not independent; it takes many passes to converge from the initial state to an
equilibrated one sampled from f (R). Thus, some number of passes in the random
walk must be discarded. Typically, on the order of 103 − 104 passes are needed
to reach the equilibrated state. Once the asymptotic distribution function is
sampled, there is still serial correlation between each step in the random walk
and this will aﬀect the determination of the statistical error of the results. One
method of dealing with these statistical correlations is to divide the random walks

612

P.A. Whitlock, D. Klein, and M. Bishop

into blocks and use the block averages in the error analysis. Another approach is
to perform totally independent sets of random walks and average the individual
results together. The latter method is ideally suited for implementation with the
SWC software.
Ying and co-workers[5] original vision was that the programmer would divide
their computation into small tasks which would require an hour or two on any
computer. This would not burden the volunteer host as it would likely be available for this amount of time. In the present case, for lower dimensional systems
such as one or two dimensions, a random walk converges in one or two hours. The
time for calculating a converged random walk increases dramatically in higher
dimensions. A four dimensional system with 4096 hyperspheres typically took
six hours and forty one minutes to complete ten parallel instances of a random
walk consisting of thirteen thousand passes. However, an identically sized system
in six dimensions, 4096 six dimensional hyperspheres, took thirty three hours to
complete ten parallel instances of a random walk of eleven thousand passes at a
similar number density. These large times for individual tasks require processors
which are available for long periods of time and such tasks are probably not
suited for running on widely scattered volunteer hosts.
While some of the calculations of the hard hypersphere systems have been
deployed over a heterogeneous set of computers via the Internet, most of the
production runs have been carried out on a network of Sun workstations. This
mode of computation was selected because it was easier to harness a large group
of networked workstations than to access a large number of volunteer computers.
When volunteer hosts are requested to run the applet tasks, the version of the
Java Virtual Machine running on each machine can be an issue. However, the
applets did run successfully on PCs using Internet Explorer or Netscape running
Windows, PCs using Mozilla running Linux and PCs using Netscape running
Solaris. In the setting of separated colleagues collaborating on a calculation and
thus able to negotiate the choice of browser and JVM, the applet version of the
Worker is a feasible method of performing calculations.
When a computationally intensive code is written in Java, there is always
a concern that the calculations will run substantially slower than if they were
implemented in a diﬀerent language. Extensive testing was done to compare the
Java simulation code using the original SWC libraries against a serial C++ code.
The serial C++ code and the parallel SWC code were run on the same processor
and gave the exact same results. In a comparison of two, ﬁve dimensional runs
involving 3125 hard hyperspheres with 3000 total passes, the serial code took 2
hours and 24 minutes, whereas the Java code with just one Worker took 2 hours.
Similar timing results were obtained in all test cases, conﬁrming our decision to
use the SWC software.

5

Evolution of SWC2 to Version 3

After several years of experience with the software, major improvements are
needed. In the SWC2 organization, no communication can occur directly between

SWC Applied to Distributed Monte Carlo Calculations

613

Workers. Each task has to be independent of the other tasks. To extend the hypersphere computation to dimensions greater than eight will require a major
change in the way the the random walks are performed. The easiest alteration
would be to divide each of the parallel random walks into tasks representing a
thousand or so passes and to then serialize the random walk into sets of tasks.
This would require the transmission of the intermediate positions of the hyperspheres in the random walk, between the Master and the Worker, to continue the
walk. This can be accomplished with SWC2. An example of organizing a computation to allow for multiple exchanges of data between the Master and the Workers is given by the sample factorial computation in the SWC2 documentation[11].
For very large numbers of particles, a commonly used alternative is to divide the
domain space of the hard hypersphere system into subdomains containing sets of
neighboring hyperspheres[15, 16]. In the present Computations, the domain space
is already partitioned into hypercells in order to eﬃciently detect hypersphere
overlap. In a domain decomposition paradigm, the subdomains would represent
distributed tasks and could be run on diﬀerent Workers. If a hypersphere moved
suﬃciently far, it would need to be transferred to a diﬀerent subdomain. Since
the originating Worker would not know the location of the receiving Worker, a
broadcast to all Workers might be needed. Alternatively, a barrier might be necessary to ensure that the Workers are approximately all at the same step in the
random walk before a hypersphere is transferred. Workers would then need to
check for a message at every move[17]. This is a well-known problem in parallel
simulations[18] and can lead to great ineﬃciencies in the simulation[19].
The most fundamental change in the newest version, SWC3, is the communication system. It is being modiﬁed from a ”pull model” to a fully asynchronous
system. Messages can be sent between the Master and the Workshops, which
manage workers. These messages can be control messages or other kinds of messages sent among Computations and Workshops.
The system allows, to limited extent, for communication to occur between a
Computation and its Workshops, or among the Workers within a given Workshop. Restrictions on the type of communication are required in order to sustain
the assumption that Work Units processed by diﬀerent Workers, at diﬀerent
points in time, will produce results that are statistically identical.
These new communication capabilities are supported by the Router component, which exhibits enhanced functionality and responsibility. The Routers in
the SWC3 system now communicate among themselves in order to locate Workers for inter-Worker messaging. Since reliability is not required for inter-router
queries, the faster and more compact UDP protocol is used. Worker messages
are still reliably sent using TCP connections.
Another beneﬁt of the new communication system using control messages is
the ability to terminate Workers on demand. In the previous version, SWC2,
it was impossible to terminate speciﬁc Workers. If a Computation ﬁnished, but
some of its replicated Work Units were still being processed, the user would
have to wait needlessly. The user’s alternative was to terminate the SWCWorker
process, with the risk of negatively impacting another Computation. When a

614

P.A. Whitlock, D. Klein, and M. Bishop

Computation terminates in SWC3, a notiﬁcation message is sent to all Routers,
which is subsequently transmitted to all Workshops. The notiﬁcation causes
Routers to dispose of any Work Units created by the now completed Computation, while Workshops will terminate any associated Workers.
Finally, a new interface is being developed in which Computations may periodically checkpoint, i.e. save a snapshot of their state. The purpose of this new
functionality is to reduce the amount of lost work in the event that the Master
terminates unexpectedly because if a Master is deleted, all the Computations
are lost.

6

Conclusion

The Small WebComputing framework has been successfully used in the investigation of large hypersphere systems distributed on local area networks. To achieve
the next stage of the investigation, dimensions higher than eight and tens of thousands of hyperspheres, requires changes in the organization of the computation
code. How well this will work over the Internet with its inherent communication
latency and a higher frequency of lost tasks needs careful evaluation.

Acknowledgments
We wish to thank the Brooklyn College Computing Center and the Manhattan
College Computing Center for their support. One of us, D.K., was partially
supported by PSC/CUNY Award # 65358-0036.

References
1. Whitlock, P.A., Klein, D., and Bishop, M.: A parallel Monte Carlo Simulation of a
5-Dimensional hard sphere system using SWC. CUNY Ph.D. Program in Computer
Science Technical Report, TR-200405, http://www.cs.gc.cuny.edu/tr/
2. Bishop, M., Whitlock, P.A. and Klein, D.: The Structure of Hyperspherical Fluids
in Various Dimensions. J. Chem. Phys. 122 (2005) 074508
3. Bishop, M. and Whitlock, P.A.: The Equation of State of Hard Hyperspheres in
Four and Five Dimensions. J. Chem. Phys. 123 (2005) 014507
4. Ying, K., Arnow, D. and D. Clark: Evaluating Communication Protocols for WebComputing. In Proceedings of the 1999 International Conference on Parallel and
Distributed Processing Techniques and Applications, CSREA Press, Las Vegas,
June (1999)
5. Arnow, D., Weiss, G., Ying, K. and Clark, D.: SWC:A Small Framework for WebComputing. In Proceedings of the International Conference on Parallel Computing,
Delft, Netherlands, August (1999)
6. Ying, K.M.: WebComputing: Design and Performance. Ph.D. dissertation, Computer Science, City University of New York (2000)
7. http://java.sun.com/sfaq/veriﬁer.html
8. McGraw, G. and Felten,E.W.: Securing Java: Getting Down to Business with Mobile Code, 2nd Edition. John Wiley & Sons, New York, (1999) Chapter 2.

SWC Applied to Distributed Monte Carlo Calculations

615

9. Baratloo, A., Karaul, M., Kedem, Z.M. and Wijckoﬀ, P.: Charlotte: Metacomputing
on the Web. Future Generation Computer Systems. 15 (1999) 559
10. Neary, M.O. and Cappello, P.: Advanced Eager Scheduling for Java-Based Adaptively Parallel Computing. In Proc. of the 2002 Joint ACM - ISCOPE Conference
on Java Grande (2002) 56
11. http://www.sci.brooklyn.cuny.edu/whitlock/swc2docs/
˜
12. Metropolis, N., Rosenbluth, A.W., Rosenbluth, M.N., Teller, A.H. and Teller, E.:
Equations of state calculations by fast computing machines. J. Chem. Phys. 21
(1953) 1087
13. M.H. Kalos and P.A. Whitlock, Monte Carlo Methods, John Wiley and Sons, Inc.,
New York, 1986, pp. 73–86.
14. W.W. Wood, Monte Carlo studies of simple liquid models, in: H.N.V. Temperley,
J.S. Rowlinson and G.S. Rushbrooke, (Eds.), The Physics of Simple Liquids, NorthHolland, Amsterdam, 1968, Chapter 5.
15. Barnes, J.E. and Hut, P.: A Hierarchical O(NlogN) Force Calculation Algorithm.
Nature 324 (1986) 446–449
16. Greengard, L. and Rokhlin, V.: A Fast Algorithm for Particle Simulations. J. Comp.
Phys 73 (1987) 325–348
17. Wilkinson, B. and Allen, M.: Parallel Programming, Second Edition. Pearson Prentice Hall, Upper Saddle River, NJ (2005) Chapter 6
18. Jeﬀerson, D.R.: Virtual Time, ACM Transactions on Programming Languages and
Systems 7 (1985) 404–425
19. Jones, K. and Das, S.R.: Combining Optimism Limiting Schemes in Time Warp
Based Parallel Simulations. In: D.J. Medeiros, E.F. Watson, J.S. Carson and M.S.
Manivannan, eds., Proceedings of the 1998 Winter Simulation Conference

