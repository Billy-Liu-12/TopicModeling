Available online at www.sciencedirect.com

Procedia Computer Science 18 (2013) 1700 – 1709

2013 International Conference on Computational Science

Labor Market Forecasting by Using Data Mining
Yas A. Alsultanny*
Arabian Gulf University, Manama, Kingdom of Bahrain

Abstract
Data mining approach was used in this paper to predict labor market needs, by implementing Naïve Bayes Classifiers,
Decision Trees, and Decision Rules techniques. Naïve Bayes technique implemented by creating tables of training; the sets
of these tables were generated by using four factors that affect
continuity in their jobs. The training tables used
to predict the classification of other (unclassified) instances, and tabulate the results of conditional and prior probabilities to
test unknown instance for classification. The information obtained can classify unknown instances for employment in the
labor market. In Decision Tree technique, a model was constructed from a dataset in the form of a tree, created by a process
known as splitting on the value of attributes. The Decision Rules, which was constructed from Decision Trees of r
rules
gave the best results, therefore we recommended using this method in predicting labor market.
© 2013 The Authors. Published by Elsevier B.V. Open access under CC BY-NC-ND license.
Selection and
peer
review under
responsibility
of the
organizers
of the
International
Conference
on Computational
Selection
and/or
peer-review
under
responsibility
of the
organizers
of 2013
the 2013
International
Conference
on Computational
Science
Science
Keywords: Data Mining; Forecastingk; Decision rules; Naïve Bayesk; Knowledge discovery;

1. Introduction
The information plays an important role in linking between education institutions and the labor market. Data
Mining (DM) approach in human resources (HR) is to analyze data in large databases, and became useful tool
to human resources professional by extracting knowledge based on patterns of data from large databases, not
only performing data analysis on large dataset, but also gain competitive advantage, by managing human
resource in organization, and can bridge the knowledge gab.
The human resources data can help organizations to support their decision making processes, by known
useful information in human resources databases. The organization can extract the behavior and potential of
their people from HR data;
Mining tools, a massive database can be analyzed in minutes [1]. The role of information technology (IT) in the

* Corresponding author. Tel.: + 973 39193807; fax: +973 17239484.
E-mail address: alsultanny@hotmail.com.

1877-0509 © 2013 The Authors. Published by Elsevier B.V. Open access under CC BY-NC-ND license.
Selection and peer review under responsibility of the organizers of the 2013 International Conference on Computational Science
doi:10.1016/j.procs.2013.05.338

Yas A. Alsultanny / Procedia Computer Science 18 (2013) 1700 – 1709

1701

organizations becomes an important factor and fundamental to support business processes in organizations. The
information technology had major impact in human resource utilization in the sectors of the economy, such as
manufacturing and financial services. Today, the amount of data in databases has increased rapidly, and need
new techniques and tools of data analysis to extract knowledge from this data. This leads to growth of new
techniques and tools of data analysis such as Data Mining (DM) to acquire knowledge.
2. Data Mining
There are vast amounts of data recorded every day on automatic recording devices such as computers with
the amount doubling every three years [2]. The huge volumes of data are examined in no more than the most
superficial way, which lead to rich data with poor knowledge. As a result, Data Mining is becoming a major
tool for analyzing large amounts of data, usually in a data warehouse and analyzing web data, and enables
organizations calculated decisions by assembling, accumulating, analyzing and accessing corporate data [3].
Data Mining is the process of traveling through data to find unknown relationships among the data that are
interesting to the user of the data [4]. The business intelligent approach uses a financial data mining technique
to assess the feasibility of financial forecasting compared to a regression model using an ordinary least squares
estimation method [5].
There are many steps involved in Data Mining process, a number of researchers have been proposed process
models for implementing Data Mining analytics on a database. Each process consists of several steps [6].
Feelders, et al., [7] presented six steps in the Data Mining process that are related to Data Mining human
resources, these steps are; Problem Definition, Acquisition of Background Knowledge, Selection of Data, Preprocessing of Data, Analysis and Interpretation, and Reporting and Use. Another approach is CRoss-Industry
Standard Process for Data Mining (CRISP-DM). CRISP-DM breaks the process of Data Mining into six major
phases [8]: Business Understanding, Data Understanding, Data Preparation, Modeling, Evaluation, and
Deployment. Choosing the right method of data mining depended on the type of data in the database. Larose
[9] stated that data mining tasks classified into six categories; Descriptive Mining, Estimation Mining,
Prediction Mining, Classification Mining, Clustering Mining, and Association Mining.
3. Data Mining and Human Resources
used to support decision-making processes. Furthermore,
Human Resource Information System applications can provide opportunity to apply Data Mining techniques by
providing a large amount of data which are requirement for Data Mining [1]. Ranjan et. al, [3] presented how
does Data Mining plays a role in Human Resources Management systems, and showed that Data Mining
discovers and extracts useful patterns from large data set to find observable patterns in Human Resources. As a
result of their study, they concluded that Data Mining has quikly emerged as a tool can allow organisation to
explore their information assets. Jantan, et al, [10] suggested the potential HR system architecture for talent
forecasting by using Knowledge Discovery in Database (KDD) or Data Mining. There are many variables
affecting manpower in any market, in this paper the variables that have a major effect on the manpower in labor
market are classified to independent and dependent variables.
Independent Variables: The independent variables are classified as shown in Table 1.
Dependent Variables: The dependent variables were classified as follows;
o Very shortage: indicates that there is a huge shortage in this specialty.
o Shortage: indicates that there is a shortage in this specialty.
o No need: indicates that there is a sufficiency in this specialty.
o Available: indicates that there is surplus in this specialty.

1702

Yas A. Alsultanny / Procedia Computer Science 18 (2013) 1700 – 1709

Table 1: Classification of independent variables
Variable no.

Period of study

Description
Medical, 7 years
Engineering, 5 years
Other college, 4 years
Diploma, 2 years
Salary
Description
Very high
Salary above 1300$
High
Salary from 1000$ to 1300$
Medium
Salary from 600$ to 900$
Low
Salary under 600$
Distance from employee home to working location
Description
Very far
Distance above 80 Km
Far
Distance between 50Km to 80 Km
Normal
Distance less 50 Km
Working hours
Description
One
working 35 hours in a week
Two
working in the morning and afternoon
Shift
working in shifts{morning, afternoon, and night}
Over
more than the rate of allowable working hours
Very long
Long
Normal
Short

1

2

3

4

4. Implementation and Results
Three methods of data mining were implemented in this paper are;
4.1. Naïve Bayesian Classifier
Naïve Bayesian Classifier create dataset such as the training set, that constitutes the results of a sample of
trials that can be used to predict classification of other (unclassified) instances, and the testing set to evaluate
the accuracy of training set. Naïve Bayes classification algorithm can be explained as follows: Given a set of k
which have prior probabilities
,
mutually exclusive and exhaustive classifications ,
, respectively, attributes ,
, of instance have values ,
respectively, the posterior
probabilities of class occurring for the specified instance proportional to [11]:
P(ci)*

n

P(a j

v j class

(1)

ci )

j 1

Making the assumption that the attributes are independent, the value of this expression can be calculated
using the product:
*

*

(2)

Calculate this product for each value of from 1 to and choose the classification that has the largest value.
Table 2 shows the training set, which was designed to describe the HR conditions for hiring some unspecified
manpower. This table is the training table; their variables can be changed according to the labor market.
Table 2: HR training set
No
Working hours
1
One
2
One

Period of study
Very long
Very long

Salary
Very high
High

Distance
Far
Far

Class
Shortage
Shortage

1703

Yas A. Alsultanny / Procedia Computer Science 18 (2013) 1700 – 1709

3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
21
22
23
24

Shift
Shift
Two
Two
One
One
Shift
Shift
Two
Two
One
One
One
One
Shift
Two
One
One
One
Shift
Two
Over

Very long
Very long
Very long
Very long
Long
Long
Long
Long
Long
Long
Normal
Normal
Normal
Normal
Normal
Normal
Short
Short
Short
Short
Short
Short

Very high
High
Very high
High
Very high
High
Very high
High
High
Low
High
Medium
Medium
Medium
High
High
High
High
Low
High
High
High

Far
Very far
Far
Very far
Far
Very far
Far
Very far
Very far
Very far
Medium
Medium
Far
Very far
Very far
Very far
Far
Very far
Far
Very far
Very far
Very far

Shortage
Very shortage
Shortage
Very shortage
Shortage
Shortage
Shortage
Very shortage
Very shortage
Very shortage
No need
Shortage
Shortage
Shortage
Shortage
Shortage
Available
No need
Shortage
No need
No need
Shortage

To implement the Naïve Bayesian classifier technique, the following four mutually exclusive and exhaustive
events in HR dataset are defined:
E1
E3

Huge shortage for this specialty (very shortage)
Sufficiency for this specialty (available)

E2 Shortage for this specialty (shortage)
E4 Surplus for this specialty (no need)

The probability of an event is usually indicated by a capital letter , and to calculate the probabilities that
and
occur and divide by the total number of the events,
must be counting the value of each event
each of these probabilities is between 0 and 1 inclusive, as it has to be qualify as a probability. They also satisfy
a second important condition: the sum of the four probabilities has to be 1. In this case:
P(E1)+ P(E2)+ P(E3)+ P(E4)=1

(3)

The dataset table was created such as that shown in Table (2). It was designed to provide a training set. Each
row of the training set is called an instance. An instance comprises the values of a number of attributes and the
corresponding classification. The training set constructed from random sample of employees and constitutes the
results of a sample of trials that can be used to predict the classification of other (unclassified) instances. The
training set consists of 24 instances, each recording the value of the four attributes as well as the classification.
Using classifications: (very shortage for this specialty, shortage for this specialty, sufficiency for this specialty,
and surplus for this specialty) correspond to the events E1, E2, E3 and E4 described previously.
The instances in the HR dataset record not only the classification but also the values of four attributes:
working hours, period of study, salary and distance. Using conditional probability to make effective use of the
additional information represented by the attribute values to obtain reliable estimates of the four classifications
(Very shortage, Shortage, No need, and Available). The probabilities of the four events are as follows:
P (E1) = 0.2083

P (E2) = 0.5833

P (E3) = 0.1667

P (E4) = 0.0417

The conditional probability of the event occurring is the probability of an event occurring, if knowing that an
attribute has a particular value (or that several variables have particular values) and is written as follows;
P(class= Shortage | salary=High) =5/14=0.3571

P(class = Shortage|salary = High) is also called a posterior probability, which is the probability that can be
calculated for the classification after have been obtained the information that the salary is High. Where the
prior probability is probability of having shortage of employee specialty, calculated by using the frequency of
on time in the training set divided by the total number of instances and is calculate as;
P(class=Shortage) =14/24 =0.5833

1704

Yas A. Alsultanny / Procedia Computer Science 18 (2013) 1700 – 1709

The Naïve Bayes algorithm combines the prior probability and conditional probabilities in a single formula
by using equation (1) and equation (2 Choose the classification with the largest value after applying this
algorithm. Tabulate all the conditional and prior probabilities for HR dataset as shown in Table 3:
Table 2: Conditional and prior probabilities: HR dataset

Working Type = One
Working Type = Two
Working Type = Shift
Working Type = Over
Period = Very long
Period = Long
Period = Normal
Period = Easy
Salary = Very high
Salary = High
Salary= Medium
Salary = Low
Distance = Very fare
Distance = Fare
Distance = Normal
Prior Probability

Class=Very shortage
0/5=0
3/5=0.6
2/5=0.4
0/5=0
2/5=0.4
3/5=0.6
0/5=0
0/5=0
0/5=0
4/5=0.8
0/5=0
1/5=0.2
5/5=1
0/5=0
0/5=0
5/24=0.2083

Class= Shortage
8/14=0.5714
2/14=0.1429
3/14=0.2143
1/14=0.0714
4/14=0.2857
3/14=0.2143
5/14=0.3571
2/14=0.1429
5/14=0.3571
5/14=0.3571
3/14=0.2143
1/14=0.0714
5/14=0.3571
8/14=0.5714
1/14=0.0714
14/24=0.5833

Class=No need
2/4=0.5
1/4=0.25
1/4=0.25
0/4=0
0/4=0
0/4=0
1/4=0.25
3/4=0.75
0/4=0
4/4=1
0/4=0
0/4=0
3/4=0.75
0/4=0
1/4=0.25
4/24=0.1667

Class= Available
1/1=1
0/1=0
0/1=0
0/1=0
0/1=0
0/1=0
0/1=0
1/1=1
0/1=0
1/1=1
0/1=0
0/1=0
0/1=0
1/1=1
0/1=0
1/24=0.0417

Using the probabilities that were obtained from Table (3) to classifying a series of unknown instances, by
calculating all the prior probabilities and also all the conditional probabilities involving one attribute, though
not all of them may be required for classifying any particular instance. Using the values of the columns in
Table 3, to determine the following posterior probabilities for each possible classification for the unknown
instance:
Working hours Period of study Salary Distance
Class
Shift

Long

High

Very fare

?????

To classify the class of the example, compute a probability for each class based on the probability
distribution in the training data. The following procedure will be followed;
Class = Very shortage 0.2083 x 0.4 x 0.6 x 0.8 x 1 = 0.03999
Class = No need 0.1667 x 0.25 x 0 x 1 x 0.75 = 0

Class = Shortage 0.5833 x 0.2143 x 0.2143 x 0.3571 x 0.3571 = 0.000341
Class = Available 0.0417 x 0 x 0 x 1 x 0 = 0

Choose the classification with the largest value. This means that the unknown instance will be classified as
Very shortage.
To classify an employee that hold bachelor degree in science, working 35 hours in a week, earn 700$ in a
month, and the distance from employee house to work location is 10 Km. Using the values of the columns in
Table (3) to classify the class by calculating the probabilities, the following procedure will be followed;
Class = Very shortage 0.2083 x 0 x 0 x 0 x 0 = 0
Class = No need 0.1667 x 0.5 x 0.25 x 0 x 0 = 0

Class = Shortage 0.5833 x 0.571 x 0.357 x 0.214 x 0.571 = 0.01452
Class = Available 0.0417 x 1 x 0 x 0 x 1 = 0

Choose the classification with the largest value. The instance will be classified as Shortage. The results
show by applying Naïve Bayesian classifier, which lead to creating dataset table to classify a series of unknown
instances for employment. The dataset table can be change and modify the over time easily to handle constantly
changing of labor market demands, and can give good results for employment in the labor market.
4.2. Decision Tree
Decision tree is an approach of representing a series of rules that lead to a class or value, and consider the
most popular Data Mining technique. It is a flow-chart-like tree structure, where each internal node denotes a
test on an attribute, each branch represents an outcome of a test, and leaf nodes represent classes or class
distributions, the topmost node in a tree is the root node [12]. Decision Tree is method of constructing a model
from a dataset in the form of a decision tree or (equivalently) a set of decision rules by using very powerful

1705

Yas A. Alsultanny / Procedia Computer Science 18 (2013) 1700 – 1709

algorithm called TDIDT, which stands for Top-Down Induction of Decision Trees. Creating Decision Trees by
a process known as splitting on the value of attributes that used entropy and information gain to construct the
decision tree. If S be set consisting of s data samples and suppose the class label attribute has m distinct values
and si be the number of samples of S in class Ci. The expected
defining m distinct classes, Ci
information needed to classify a given sample is given by [11]:
I(s1, s2

m)

=-

m

(4)

pi log2 pi

i 1

Where pi the probability than an arbitrary sample is belongs to class Ci and is estimated by si/s. When
attribute A have v distinct values, {a1, a2
v}. Attribute A can be used to partition S into v subsets, {S1, S2,
v}, where Sj contains those samples in S that have value aj of A. If A were selected as the test attribute
(i.e., best attribute for splitting), then these subsets would correspond to the branches grown from the node
containing the set S, and sij be the number of samples of class Ci in a subset Sj. The entropy, or expected
information based on the partitioning into subsets by A is given by [11]:
v

E ( A)

s1 j

s

j 1

The term

v
j 1

s1 j

s2 j ... smj

I ( s1, s2 ,....,sm )

(5)

s 2 j ... s mj acts as the weight of the jth subset and is the number of samples in the subset (i.e.,
s

having value aj of A) divided by the total number of samples in S. The smaller the entropy value is, the greater
the purity of the subset partitions. The encoding information that would be gained by branching on S (i.e.,
Gain(A) is the expected reduction in entropy caused by knowing the value of attribute (A), is given by:
-E(A)

(6)

The algorithm computes the information gain of each attribute. The attribute with the highest information
gain is chosen as the test attribute for the given set S. A node is created and labeled with the attribute, branches
are created for each value of the attribute, and the samples are partitioned accordingly. Using Table 2 HR
training set, the class attribute has four distinct values (Very shortage, Shortage, Available, No need), therefore,
there are four distinct classes (m=4). Let C1 corresponding to the class Very shortage, class C2 corresponding
to Shortage, C3 corresponding to the class Available, and class C4 corresponding to No need. There are 5
samples of class Very shortage, 14 samples of class Shortage, 4 samples of class Available, and 1 sample of
class No need. To compute the information gain of each attribute, by using equation (4), the expected
information needed to classify a given sample is:
Next, compute the entropy of each attribute. Start with the attribute {Working hours}. Need to look at the
distribution of class samples for each value of {Working hours}. Compute the expected information for each of
these distributions. Furthermore, using equation (5), the expected information needed to classify a given sample
if the samples are partitioned according to {Working hours}, and using equation (6) to compute the gain in
information from such a partitioning. The calculation is as follows:
Working hours:
One:
,

,

,

1706

Yas A. Alsultanny / Procedia Computer Science 18 (2013) 1700 – 1709

Two:

,

,

,

Shift:

,

,

,

Over:

,

,

Period:
Very long:

,

Long:

,

,

,

,

,

Salary:
Very high:

,

,

High:

,

,

Medium:

,

,

,

Distance:
Very far:

,

,

,

Low:

,

,

,

Short:

Medium:

,

,

Normal:

Far:

,

,

,

,

,

,

,

,

,

,

,

,

,

Yas A. Alsultanny / Procedia Computer Science 18 (2013) 1700 – 1709

After compute

,
,
and
. Since {
} has the highest information gain among the attributes, it is selected
}, and branches are grown for each of the
as the test attribute. A node is created and labeled with {
attribute's values. The node created is called root node (initial node). Table 4 summarize the results of the
process of selecting attribute by using entropy applied on HR training set to generate decision tree model.
Table 3: Information Gain values for splitting on each of the four attributes

i.Initial node
ii.node{Period}
Branch:Very. long
node{Distance} Branch:Very far
node{Distance} Branch:Far
iii.node{Period}
Branch:Long
node{Working hour} Branch{One}
node{Working hour } Branch{Two}
node{Working hour} Branch{Shift}
node{Salary} Branch:Very. high
node{Salary} Branch:High
iv.node{Period}
Branch:Normal
node{Distance} Branch:Very far
node{Distance} Branch:Far
node{Distance} Branch:Medium
node{Salary} Branch:High
node{Salary} Branch:Medium
v.node{Period}
Branch:Short
node{Working hour} Branch:Over
node{Working hour} Branch:Two
node{Working hour} Branch:Shift
node{Working hour} Branch:One
node{Salary} Branch:High
node{Salary} Branch:Low
node{Distance} Branch:Very far
node{Distance} Branch:Far

Working
Period
hours {Gain} {Gain}
0.315
0.54

Salary
{Gain}
0.393

Distance
{Gain}
0.436

0.251

--

0.459

0.918

---

---

---

---

0.666

---

0.459

0.459

--0
---

--0
---

--0
---

--0
---

0.109

--

0.19

0.316

------

------

------

------

0.666

--

0.316

0.584

---------

---------

---------

---------

Decision
create root node {Period}
create node {Distance}
end with class {Very shortage}
end with class {shortage}
create node {Working hours}
end with class {shortage}
end with class {Very shortage}
create node {Salary}
end with class {shortage}
end with class {Very shortage}
create node {Distance}
end with class {Shortage}
end with class {Shortage}
create node {Salary}
end with class {Normal}
end with class {Shortage}
create node {Working hours}
end with class {Shortage}
end with class {No need}
end with class {No need}
create node {Salary}
end with class {Shortage}
create node {Distance}
end with class {No need}
end with class {Available}

4.3. Decision Rules
rules.
The third technique is decision rules that extracted from its Decision Tree, by applying are executed. If condition is False,
When an instant is tested, if condition is true, the statements following
(if any) is evaluated in turn. When a True condition is found, the statements following the
each
are executed. If none of the
statements are True (or there are no
clauses), the
associated
are executed. After executing the statements following
or
, execution
statements following
. Finally, using Weka1 Data Mining software to determine the
continues with the statement following
accuracy of each technique to evaluate the effectiveness of classifiers, the model generated by the training set is
applied to the test set. Classification accuracy is standard metric used to evaluate classifiers is classification
accuracy [13], which is simply the percentage of the instances where the method resulted in the correct
prediction to the predictive accuracy of the three Data Mining techniques, and also using Kappa statistic which

1707

1708

Yas A. Alsultanny / Procedia Computer Science 18 (2013) 1700 – 1709

measure the agreement of predictions with the actual class. A kappa of 1 indicates almost perfect agreement,
whereas a kappa of 0 indicates agreement equivalent to chance, the scale of Kappa Measure is;

One of the most attractive aspects of decision trees lies in their interpretability, especially with respect to the
construction of decision rules. Decision rules can be constructed from a decision tree simply by traversing any
rules. For decision rules,
given path from the root node to any leaf. Decision rules come in the form the antecedent consists of the attribute values from the branches taken by the particular path through the tree,
while the consequent consists of the classification value for the target variable given by the particular leaf node.
The decision rules were constructed as follows.

Decision rules to predict the needs of the market labour, the
rules generate as follows: If employee is engineering and work 35 hours in a week, the decision is shortage.
Decision rules determines what action to execute according to the condition of the incoming data, the employee
is engineering which means the period is difficult and working 35 hours in a week, which means the working
rules which indicate that there is
hours in category one, the decision that obtain from applying
shortage in engineering specialty.
If employee is teacher, the distance from employee house to work location is 25 Km, and the salary earn is 800. The decision in
this case is indicate that there is shortage in teacher specialty because the only rule can apply on this case is shown below and
leads to shortage in teacher specialty.
If employee is physician and the distance from employee house to work location is 50 Km, the decision is very shortage.
Because the employee is a physician and distance is more than 30 Km, the decision rule that applied is indicate that there huge
shortage in physicians specialty.

Comparison between the three Data Mining techniques to determining which technique provides the best
1
Data Mining
predictor needs for the labor market. Measure the accuracy for each classifier
software version 3.6.1. Table 5 shows the calculated values of the accuracy for each technique.
Table 4: The accuracy result
Technique
Accuracy
Kappa statistic

Naïve Bayes
83.33%
0.7126

Decision Tree
100%
1

Decision Rules
75 %
0.5814

Yas A. Alsultanny / Procedia Computer Science 18 (2013) 1700 – 1709

From the above table, when Decision tree is applied on the HR data set showed improvement in accuracy
percentage (100%) of the instances of correct prediction, and agreement of predictions with the actual class
from Kappa statistic measure is (1), which means that the Decision tree technique is the efficient and more
accurate classification technique among the over two classification on the HR data.
5. Conclusion
Data Mining is one of the emerging technologies in the field of data analysis tools, and has a significant
impact in Human Resources in discovering patterns and information hidden in databases in order to use this
information to gain knowledge that help the decision-makers to develop strategies and future plans of labor
Naïve
market.
Bayes classification which creates a dataset table to classify a series of unknown instances for employment.
The dataset table can be changed and modified, easily to handle constantly changing of labor market demands.
The information that obtained from applying Naïve Bayesian classifier HR data set can determine the needs
and the shortages of specialists in the labor market by classify unknown instances for employment
automatically. The second technique is Decision Tree which where a model was constructed from a dataset in
the form of a tree. It can be used to predict the values of other instances not in the training set, and the accuracy
is typically high in Decision Tree. The third technique is Decision Rules which extracted from Decision Tree,
contains 16 rules to understand complexity of decision tree. The results indicate that the three Data Mining
techniques showed to be a workable approach to predicting the labor market needs. The comparison between
the three techniques showed that the Decision Tree technique had the highest accuracy.
Implementing data mining methods on Human Resources data can handle the large amounts of data and
transform it to useful information to improve the process of decision-making in organizations. Moreover, it can
lead to easier understanding of the needs of the organization.
References
[1] Long, L. K., and Troutt, M. D. (2003), Data Mining for Human Resource Information Systems. In J. Wang, Data Mining:Opportunities
and Challenges (pp. 366-388). Idea Group Publishing.
[2] Lyman, P., and Varian, H. R. (2003), How Much Information. Berkley CA: School of Information Management and Systems,
University of California, USA.
[3] Ranjan, J., Goyal, D. P., and Ahson, S. I. (2008), Data Mining Techniques for Better Decisions in Human Resource Management
Systems. Journal of International J. Business Information Systems , vol. 3, no. 5, pp.464-481.
[4] Wang, H., and Wang, S. (2008), A knowledge Management Approach to Data Mining Process for Business Intelligence. Journal of
Industrial Management and Data Systems , vol.108, no.5, pp. 622-634.
[5] Tjung Luna C., Ojoung Kwon, Tseng K. C. and Geist Jill Bradley, Forecasting Financial Stocks using Data Mining, Global Economy
and Finance Journal Vol. 3, No. 2, September 2010. pp. 13 26.
[6] Pittman, K. (2008), Comparison of Data Mining Techniques used to Predict Student Retention. Phd thesis, Nova Southeastern
University, USA.
[7] Feelders, A., Daniels, H., and Holsheimer, M. (2000), Methodological and Practical Aspects of Data Mining. Journal of Information
and Management, vol.37, pp.271-281.
[8] CRISP-DM The Cross Industry Standard Process for Data Mining Blog (2011), Retrieved May 15, 2011, from
http://crispdm.wordpress.com
[9] Larose, Daniel T. (2005), Discovering Knowledge in Data an Introduction to Data Mining, John Wiley and Sons, Inc.
[10] Jantan, H., Hamdan, A. R., and Othman, Z. A. (2009), Knowledge Discovery Techniques for Talent Forecasting in Human Resource
Application. Proceedings of World Academy of Science, Engineering and Technology, vol.38, pp.803-811.
[11] Bramer, M. (2007), Principles of Data Mining. Springer.
[12] Han, J., and Kamber, M. (2001), Data Mining: Concepts and Techniques. Morgan Kaufmann.
[13] Perlich, C., Provost, F., and Simonoff, J. (2003), Tree Induction vs. Logistic regression: A Learning-curve Analysis. Journal of
Machine Learning Research, vol.4, pp.211-255.

1709

