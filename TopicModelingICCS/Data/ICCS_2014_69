Procedia Computer Science
Volume 29, 2014, Pages 2272–2283
ICCS 2014. 14th International Conference on Computational Science

Performance of Unidirectional Hierarchization for
Component Grids Virtually Maximized
Philipp Hupp
Department of Computer Science, ETH Z¨
urich, Z¨
urich, Switzerland
philipp.hupp@inf.ethz.ch

Abstract
The sparse grid combination technique provides a framework to solve high-dimensional numerical problems with standard solvers. To combine the component grid solutions of the combination
technique either interpolation and sampling or a change of basis from the full grid basis to the
hierarchical basis is required. We implement a memory eﬃcient hierarchization algorithm for
the component grids of the sparse grid combination technique performing this change of basis.
By exploiting the structure of the component grids, this implementation comes within a factor
of 1.5 of the runtime achievable for large grids by any hierarchization algorithm implementing the unidirectional principle. The implementation outperforms the currently fastest generic
software StructuredSG [2] by a factor between 5.8x and 41x for problems larger than 30MiB.
Keywords: sparse grids, combination technique, hierarchization, unidirectional principle, high-dimensional, performance, operational intensity, memory bound

1

Introduction

Solving high-dimensional numerical problems, in particular high-dimensional partial diﬀerential equations (PDEs), is one of the grand challenges of current and future high-performance
computing systems. As an example consider the simulation of hot fusion plasmas in fusion
reactors. The behavior of the plasma is modeled using either a gyrokinetic approach or the
Vlasov-Maxwell equations. Both PDEs have to jointly treat, besides time, three spatial dimensions and two or three, respectively, velocity dimensions. Hence, in each time step a 5respectively 6-dimensional PDE has to be solved. Classical discretizations fully suﬀer from the
”curse of dimensionality”, i.e. the number of unknowns of the discretization grows exponentially with respect to the dimension. A 5-dimensional problem with 1024 degrees of freedom
in each dimension or about 1015 unknowns in total would require roughly 8 PetaBytes to only
store the data in double precision. Hence, simply because of the massive amount of data,
high-dimensional problems are currently not feasible with classical discretizations.
Hierarchical discretizations can come to our rescue. Sparse grids [18] can reduce the number
of grid points by 8 orders of magnitude in the above example while the accuracy of the solu2272

Selection and peer-review under responsibility of the Scientiﬁc Programme Committee of ICCS 2014
c The Authors. Published by Elsevier B.V.
doi:10.1016/j.procs.2014.05.212

Performance of Unidirectional Hierarchization for Component Grids Virtually Maximized
Regular
grid basis

Hierarchical
basis

Gather

Hierarchical
basis

Component
Component
Dehierarchize
Grids
Grids

Scatter

Sparse
Grid

Hierarchize

P. Hupp

Solve
−

−

+

+

Figure 1: A standard solver can be employed on the component grids. The gather step assembles
the sparse grid solution from the component grid solutions and the scatter step distributes the
joint solution back to the component grids. The base change from the regular grid basis to the
hierarchical basis facilitates the gather and scatter steps.

tion only deteriorates slightly [1] given certain smoothness assumptions. We want to employ
the sparse grid combination technique, an extrapolation scheme that assembles the sparse grid
solution as linear combination of the solutions of many coarse and anisotropic full grids called
component grids. The sparse grid combination technique has several big advantages: First, the
hierarchical approach breaks the global communication requirements of conventional discretizations such that the component grids can be solved independently and thus embarrassingly in
parallel. Second, as the component grids are coarse, anisotropic and classical discretizations,
standard solvers can be employed. Further, the combination technique can ensure requirements
for next-generation computing such as fault tolerance.
While the sparse grid combination technique breaks the global communication requirements,
communication is still necessary. After every or at least few time steps [3] of a time dependent PDE, the solutions of the component grids should be combined to the sparse grid solution (gather) and the joint solution distributed back to the component grids (scatter). This
introduces a synchronization barrier which requires a reduced but global communication leaving
the gather and scatter steps as the remaining communication bottleneck.
To take advantage of the hierarchical structure of sparse grids in this remaining communication step, the component grid solutions need to be transfered from the full grid basis to the
hierarchical sparse grid basis. This base change is called hierarchization. For communication
schemes that rely on the hierarchical representation of the component grid solutions [8, 7],
hierarchization is on the performance critical path as shown in Fig. 1. Hierarchization is also
one of the most fundamental algorithms for sparse grids. Like all sparse grid algorithms it is
typically implemented taking advantage of the unidirectional principle.
The implementation of the unidirectional hierarchization algorithm combiHier presented in
this paper is tuned speciﬁcally for component grids. By exploiting the additional structure of
component grids in comparison with regular or adaptive sparse grids, combiHier comes within
a factor of 1.5 of the runtime achievable for large grids by any hierarchization algorithm implementing the unidirectional principle. The implementation is able to outperform the currently
fastest generic software StructuredSG [2] by a factor between 5.8x and 41x for problems larger
than 30MiB. Hence writing code speciﬁcally for component grids seems worthwhile when the
combination technique should be established for time dependent PDEs. Further, we point out
that hierarchization according to the unidirectional principle has bounded operational intensity
for large problems. This result is not limited to component grids but holds for any (sparse)
2273

Performance of Unidirectional Hierarchization for Component Grids Virtually Maximized

P. Hupp

grid. Hence, if higher operational intensity is desired, the unidirectional principle needs to be
avoided and algorithms cannot be restricted to work on 1-dimensional subproblems only.
The rest of the paper is organized as follows: the next section covers related work. In
Sect. 3 the unidirectional principle as well as the hierarchization algorithm are discussed. Sect. 4
describes the implementation of the unidirectional hierarchization algorithm combiHier . Sect. 5
presents and discusses the experiments and Sect. 6 concludes the paper.

2

Related Work

Sparse grids [16, 18, 1] and the sparse grid combination technique [4] have been used successfully to solve high-dimensional problems. Few experiments have been conducted using the
combination technique in a time step setting but those show promising results [3]. Recently
global communication schemes for the combination technique taking advantage of the hierarchical structure of sparse grids [8, 7] have been derived. This stresses the need for an eﬃcient
hierarchization algorithm for component grids.
Several software packages [14, 11, 12, 9, 2] can hierarchize sparse grids. All of them implement the unidirectional principle. Further, all these approaches focus on hierarchization of
regular or adaptive sparse grids which inherent a complicated structure and on which navigation is tedious. The standard software for sparse grids SG++ [14] focuses on spatially adaptive
sparse grids [15] and handles those using level-index vectors as keys in a hash table to index grid
points. SG++ is able to hierarchize the anisotropic component grids as a special case. CDS [11]
presents a bijective mapping to navigate on regular sparse grid with small memory footprint. It
also performs extensive comparisons with prior software which it all outperforms. fastSG [12]
extends the bijective mapping of CDS to truncated grids which cover the anisotropic grids of
the combination technique as special case. rSG [9] eﬃciently hierarchizes regular sparse grids
using a dynamic data layout. It is in particular eﬃcient for high dimensions, up to thousands,
and small levels. A work phase hierarchizes the data with respect to the ﬁrst dimension and a
rotate phase rearranges the data such that role of the dimensions change. These two operators
are applied alternatingly d times to yield the d-dimensional hierarchization. StructuredSG [2]
also employs a dynamic memory layout. It splits the sparse grid into hierarchical subspaces and
rotates these subspaces to enable vectorization. StructuredSG outperforms CDS and provides
hence the fastest hierarchization algorithm for sparse grids up to date. As StructuredSG can
also hierarchize the anisotropic grids of the combination technique it is used to benchmark the
new implementation.
None of these software packages specializes in the anisotropic component grids of the combination technique. Hence, also none of them can fully exploit the regular structure of the
component grids. Hierarchization of component grids has been in the focus for the ﬁrst time
in a preliminary version of this work [6] focusing on the case when the 1-dimensional poles are
arranged in a breadth-ﬁrst search layout.
Eﬃcient evaluation schemes for sparse grids, preliminary for dehierarchization, the inverse
basis transformation, are addressed by CDS, fastSG and StructuredSG. Further, CDS and
StructuredSG include GPU implementations.
To analyze the performance of the implementation the Rooﬂine Model [17] is employed. To
generate the rooﬂine plots and to measure the data transfer between main memory and cache
perfPlot [13] is used.
2274

Performance of Unidirectional Hierarchization for Component Grids Virtually Maximized

3

P. Hupp

The Unidirectional Hierarchization Algorithm

This section ﬁrst describes component grids and the unidirectional principle. Then an unidirectional hierarchization algorithm is presented. The section closes with a bandwidth and an
operational intensity bound for unidirectional hierarchization and the number of ﬂops performed
by the presented unidirectional hierarchization algorithm.

3.1

Component Grids

Component grids are full but coarse grids. A component grid is completely described by its level
vector ∈ Nd describing how often dimensions i ∈ {1, . . . , d} has been reﬁned. A component
grid of reﬁnement level li consist of 2li −1 grid points in dimension i and does not have boundary
points. If li = 1, the component grid consists of a single grid point in dimension i.

3.2

The Unidirectional Principle and Poles

The unidirectional principle is the enabling algorithmic principle regarding sparse grids. Instead
of working in d-dimensional space the problem is decomposed into d distinct phases each dealing
with 1-dimensional subproblems. This breaks the complicated d-dimensional data dependencies
of the sparse grid. We call these one dimensional subproblems poles. A pole in dimension i
consists of all points of the grid which only diﬀer in the i.th component, i.e. lie on a line parallel
to the i.th coordinate axis.

3.3

The Unidirectional Hierarchization Algorithm

Hierarchization is one of the most basic tasks for sparse grids. The input is any grid, e.g. an
anisotropic component grid or a regular sparse grid, that has a value assigned to each grid
point. Think of a piecewise multilinear, i.e. piecewise linear with respect to every dimension,
function f that is sampled at the grid points. The task is to calculate the coeﬃcients of f in
the hierarchical basis, called hierarchical surpluses, at all grid points. In case of a piecewise
linear basis this base transformation is achieved by Algo. 1. The outer loop iterates over the d
dimensions and constitutes the unidirectional principle. The inner three loops update the whole
data set once. Hereby, the second loop splits the data set into the 1-dimensional poles in the
current work dimension dd. The inner two loops perform the updates on these 1-dimensional
poles bottom up in a daxpy like fashion.
Algorithm 1: The Unidirectional Hierarchization
Algorithm for a d-dimensional component grid x of
level vector (l1 , l2 , . . . , ld ).
for dd ← 1 to d do
foreach 1-dim pole P in direction dd do
for ← ldd , (ldd − 1) to 2 do
forall the xi on level of pole P do
x[i] = x[i] − 0.5 ∗ x[leftPredec.(i, d)]
x[i] = x[i]−0.5∗x[rightPredec.(i, d)]

l=1
l=2
l=3
l=4
Figure 2: A 1-dimensional grid
(i.e. pole) and the respective binary
tree. Hierarchical predecessors are
indicated by solid and dashed arrows.

2275

Performance of Unidirectional Hierarchization for Component Grids Virtually Maximized

P. Hupp

To understand the updates performed on a pole consider the grid points of the pole as
the nodes of a complete binary tree in in-order traversal as depicted in Fig. 2. Hence we can
assign a level to each point of the pole. Algo. 1 updates the pole bottom up, level by level.
The left (right) hierarchical predecessor of a node v is on the left (right) side of v and with
respect to the in-order traversal the closest predecessor of v in the binary tree. The second
hierarchical predecessor does not exist for the outermost grid points of each reﬁnement level. If
a node possesses only one hierarchical predecessor, the instruction referring to the non-existent
predecessor is ignored.

3.4

Bounds and Flop Count for Unidirectional Hierarchization

This section derives a bandwidth bound and an operational intensity bound for unidirectional
hierarchization assuming piecewise linear basis functions and analyzes the number of ﬂoating
point operations performed by Alg. 1.
A bandwidth bound for the unidirectional principle. The unidirectional principle
sweeps d times over the data. In each sweep the data is read, updated and written back.
When the input is signiﬁcantly larger than the cache, no intermediate results are reused between sweeps. (In theory, data up to cache size could be reused. This becomes irrelevant as
the data is signiﬁcantly larger than the cache.) Therefore, given the memory bandwidth of the
processor, a lower bound for the runtime of any hierarchization algorithm implementing the
unidirectional principle working on input data signiﬁcantly larger than the cache is
2d · (number of grid points) · (size of datatype)
.
bandwidth

(1)

An operational intensity bound for unidirectional hierarchization. In each sweep of
the unidirectional principle at most 4 ﬂoating point operations are performed by Algo. 1 to
update one grid point. Given the assumption that no reuse is performed between the iterations
of the unidirectional principle, at least the grid point itself must be read from main memory and
its updated value written back. Hence the operational intensity of unidirectional hierarchization
is upper bounded by
4 ﬂops
.
(2)
2 · (size of datatype)
lops
In case of doubles the operational intensity is therefore bounded by 0.25 fbyte
.
To calculate the hierarchical surplus for basis functions diﬀerent from the piecewise linear
ones discussed here, not 4 but a diﬀerent number of ﬂops per grid point would be required in
each sweep of the unidirectional principle. Hence, while the general approach stays valid, the
value of the operational intensity bound (2) would change if diﬀerent basis functions would be
employed.

Flop count.
forms

To hierarchize a d-dimensional component grid of level (l1 , . . . , ld ) Alg. 1 per⎛
⎞
d

F (d, ) = 2 ·
i=1

⎝ 2li +1 − 2 · li − 2 ·

d

2 lj − 1 ⎠

(3)

j=1, j=i

ﬂops. Observe that the ﬂop count of Alg. 1 could easily be reduced by roughly 14 , if desired.
Whenever both hierarchical predecessors exist, their values could be added ﬁrst, multiplied
2276

Performance of Unidirectional Hierarchization for Component Grids Virtually Maximized

P. Hupp

by −0.5 and then added to the value of x[i]. As the experiments are going to show that the
hierarchization according to the unidirectional principle is memory bound and not compute
bound, no experiments have been conducted with a reduced ﬂop count version of the code.

4

Implementation of the Unidirectional Hierarchization
Algorithm

This section discusses the implementation of Alg. 1. First, we specify the index computations
and navigation on the data layout performed by Alg. 1. Then optimizations of the algorithm
are discussed.
The implementation of the unidirectional hierarchization algorithm for component grids
discussed in this paper works in-place as Alg. 1. The input and output layout is standard
row-major order as usual for full grids.

4.1

Basic Navigation on the Data Layout

Alg. 1 does not specify how to calculate the start of the next pole, how to loop over a certain
level of a pole and how the hierarchical predecessors can be computed. For adaptive or regular
sparse grids, a lot of eﬀort has been spent to derive eﬃcient ways to navigate on sparse grids. As
we are considering the anisotropic but full component grids, this navigation can be performed
on the ﬂy using integer strides. Integer strides allow an eﬃcient implementation and also reduce
the memory overhead to a minimum.
For dimension dd, there are 1≤j≤d, j=dd 2lj − 1 poles. The lexicographic ﬁrst point of
the p.th pole in dimension dd is given by
p
· jumpdd + (p
stridedd
dd−1

mod stridedd )

dd

li
2li − 1 and jumpdd =
for stridedd =
i=1
i=1 2 − 1 . Hence it is left to navigate
ldd
within a pole. A pole in dimension dd consists of 2 − 1 points in total or 2k−1 points
for level k ∈ {1, . . . , ldd }. The lexicographic ﬁrst point of pole p on level k is at oﬀset
2ldd −k − 1 · stridedd from the lexicographic ﬁrst point of the pole. Within level k, the points
are at stride 2ldd −k+1 · stridedd . If present, the hierarchical predecessors of a point are at stride
±2li −k · stridedd from the point itself.
The second loop of Alg. 1 is parallelized using OpenMP as all poles can be handled independently of each other. As hierarchization of all poles requires the same number of ﬂoating point
operations static scheduling with maximum chunk size was used. The unoptimized version of
combiHier implements Alg. 1 using these speciﬁcations looping over the poles in their natural
order.

4.2

Optimizing the Unidirectional Hierarchization Algorithm

The unoptimized code has been optimized using basic block optimizations and manual vectorization. Hierarchizing in the ﬁrst dimension has a special character and is not optimized
further.
For work dimension dd ≥ 2 consider the arrangement of the poles as depicted in Alg. 2 for
d = 2 and dd = 2. The poles (orange boxes) are orthogonal to the data layout and thus the data
layout is perfect for block optimizations and vectorization (vector registers dashed). Loading
2277

Performance of Unidirectional Hierarchization for Component Grids Virtually Maximized

P. Hupp

Algorithm 2: Unrolled Unidirectional Hierarchization Algorithm for a d-dimensional component grid x of level vector (l1 , l2 , . . . , ld ), using basic blocks of size block.
// Treat the first dimension as before
Hierarchize Dimension 1 as in Algorithm 1
// Basic block optimizations for dd ≥ 2
for dd ← 2 to d do
foreach basic block B of block poles in dim dd do
for ← ldd , (ldd − 1) to 2 do
forall the i such that xi is on level of the ﬁrst pole of the basic block B do
for j ← 1 to block do // Execute once for each pole in block B
x[i + j] = x[i + j] − 0.5 ∗ x[leftPredecessor(i + j, d)]
x[i + j] = x[i + j] − 0.5 ∗ x[rightPredecessor(i + j, d)]

a point of a pole automatically loads the whole cache line and page of that point. Both, cache
line and page, span over diﬀerent poles as we are working in row major layout. The relative
position within its pole, however, is the same for all points of the same cache line and page
(assuming the ﬁrst dimension is reﬁned suﬃciently) as we are working on the anisotropic but
full component grids. In particular,
x[rightPredecessor(i + j, d)] = x[rightPredecessor(i, d) + j] .
The same holds for the left predecessor. Hence, the hierarchical predecessors of points that are
contiguous in memory are contiguous in memory as well. This observation can be exploited
to increase memory reuse. Instead of handling the poles after each other, basic blocks of
block poles are created. Within the ﬁrst pole of a basic block we still work level by level
bottom up and within a level position by position. The innermost loop, however, iterates over
all poles of that basic block as diﬀerent poles are at unit stride. Hence we ﬁnish the update of
those contiguous points for several poles before we move on to the next grid point of the ﬁrst
pole of that basic block. This results in Alg. 2. StructuredSG [2] employs a similar strategy to
improve locality but can only treat hierarchical subspaces, which are of much smaller size, as
basic blocks. Instead of parallelizing the loop over the poles the loop over the basic blocks is
parallelized for the optimized version.
The sequence of computations and the data layout are now ideal for vectorization. The
scalar operations can simply be replaced by their corresponding vector operations.
Basic block optimizations as well as vectorization are simpliﬁed if the grid is padded such
that the number of grid points in the ﬁrst dimension is a multiple of the block size block and
the size of the vector registers. This padding aligns all points of the ﬁrst pole of a basic block
to a multiple of the vector register size and hence allows to use aligned loads and stores.

5

Experimental Results and Discussion

This section ﬁrst describes the experimental setup. Then the experiments examining strong
scaling, the size of the basic blocks, grids of diﬀerent sizes and anisotropic grids are presented
and a benchmark is performed. The implementation of combiHier is publicly available [5] and
was veriﬁed for correctness using SG++ [14].
2278

Performance of Unidirectional Hierarchization for Component Grids Virtually Maximized

5.1

P. Hupp

Experimental Setup

All experiments were performed on an Intel Xeon E3-1240 running at 3.4GHz under Fedora 19.
TurboBoost was disabled. The machine consists of 4 cores and hyperthreading was disabled.
The 4 cores share 8MiB L3 cache and 32GiB main memory. As compiler icc version 14.0.1
with the ﬂags -std=c++0x -openmp -xHost -O3 was used. gcc version 4.8.2 was installed for
library support and OpenMP version 3.1 employed. All experiments were performed in double
precision. For vectorization the 4-way AVX registers were used. The largest data sets examined
were roughly 8GiB (| |1 = 30). If the level sum decreases by one, the size of the data set halves
approximately. For d = 4 we had to limit the experiments to li = 7 (| |1 = 28) resulting in
data sets of roughly 2GiB. The other maximum reﬁnement level were li = 15 for d = 2, li = 10
for d = 3 and li = 6 for d = 5. To choose the block size as a power of 2, the ﬁrst dimension
was padded with 1 grid point such that there were 2l1 grid points in the ﬁrst dimension. The
standard size used for the basic blocks was 214 for d = 2, 512 for d = 3, 128 for d = 4 and 64
for d = 5.
The bandwidth was measured with the stream benchmark [10] and peaked at 21.1 GiB/s
when at least 2 cores where used. The number of grid points in the unpadded component grid
d
is i=1 2li − 1 . To calculate the bandwidth bound this number of grid points is used in (1)
instead of the size of the larger, padded grid.
All measurements for the rooﬂine plots were conducted with perfPlot [13] guaranteeing cold
cache measurements. For small basic blocks we observed that measurements from the ﬂop
counters diﬀered by up to a factor of 2 from the value given by (3). As perfPlot measures issued
but not executed or retired ﬂops we attribute this to wrong branching for small basic blocks.
(For basic blocks of size 4 the loop body needs only to be executed once.) Hence the measured
ﬂops were replaced by the value calculated with (3). As a consequence the performance numbers
in the rooﬂine plots are indirectly proportional to runtime.
For all other measurements the wall clock time was stopped using the system clock of the
C++ chrono library and warm cache measurements were performed. These experiments were
repeated 10 times and the average results are shown. When error bars are plotted, these report
the minimum and maximum over the 10 runs.

5.2

Experimental Results

Scaling: the optimized code is already bandwidth bound. The unoptimized code
scales strongly. Figure 3 shows the wall clock time of the unoptimized and optimized code for
diﬀerent dimensions and maximum reﬁnement level. While the unoptimized code scales almost
perfectly for d = 3, 4, 5, the optimized code does not beneﬁt from using more than 3 cores.
The performance of the optimized code is already very close to the bandwidth bound and the
overhead created by thread synchronization slows the algorithm down when 4 cores are used.
The runtimes of the optimized code for 3 and 4 cores as well as the bandwidth bound (1) and
runtime
the ratio bandwidth
bound are summarized in Tab. 1. This table shows that the optimized version
is within a factor of 1.5 of the bandwidth bound for all dimensions and respective maximum
reﬁnement level. Further, this factor decreases as the dimension increases as hierarchizing the
ﬁrst dimension is not optimized. As the optimized version performs best when 3 cores are
selected all upcoming measurements are performed using 3 cores and the optimized code.
Basic block optimizations: basic blocks should be large. Fig. 3 shows performance
for grids of maximum reﬁnement level when the size of the basic blocks is varied. If the ﬁrst
dimension is only moderately reﬁned, the maximum block size should be chosen (d = 4, 5).
2279

Performance of Unidirectional Hierarchization for Component Grids Virtually Maximized
Scaling of the Optimized and Unoptimized Code:
Optimized Code is Bandwidth Bound - Unoptimized Code Scales Strongly
Runtime (wallclock - chrono) [s]

Dim 5
Unoptimized Version Level=(6,6,6,6,6)
Bandwidth Bound
Optimized Version

25
20
15

Dim 3
Level=(10,10,10)

Dim 2
Level=(15,15)

Dim 4
Level=(7,7,7,7)

5

Basic Block Optimizations: Basic Blocks Should be Chosen Large
(Calculated Flop Count – Annotation: Basic Block Size ’BLOCK’)

128
512

1
0.9
0.8
0.7

th
dwid
Ban

0.6

4 Dim

1

2

3

4

1

2

3

4

1

2

Number of processors

3

4

1

2

3

4

0.4
0.1

Dim 2, Level = (15,15)

64

Dim 4,
Level = (7,7,7,7)
4

32768
4

1024

Dim 5,
Level = (6,6,6,6,6)

3, Level = (10,10,10)

0.5
4

0

)
ycle
s/C
Byte
(6.2

16384

Operational Intensity
for large levels
(0.25 Flops/Byte)

10

Performance [(Calculated Flops)/Cycles]
(indirectly proportional to wall clock time)

30

2

P. Hupp

0.2
Operational Intensity [(Calculated Flops)/(Bytes transfered)]

0.3

Figure 3: Left: Scaling of the unoptimized and optimized code for diﬀerent dimensions. Perfect
scaling for the unoptimized code is indicated by dashed lines. Right: Varying the size block of
the basic blocks for component grids of diﬀerent dimensions and maximum reﬁnement level li .
d = 2, li = 15 d = 3, li = 10 d = 4, li = 7 d = 5, li = 6
Grid Size (i ∈ {1, . . . , d})
Bandwidth Lower Bound (1)
1.52 s
2.27 s
0.73 s
3.50 s
Number of Processors
P =3 P =4 P =3 P =4 P =3 P =4 P =3 P =4
Runtime
2.15 s 2.12 s 3.33 s 3.79 s 0.84 s 0.86 s 4.19 s 4.85 s
Runtime
1.41 1.39
1.47 1.66
1.15 1.18
1.20 1.39
Bandwidth Bound
Table 1: Comparing the runtime of the optimized code for 3 & 4 cores to the bandwidth bound.
If the ﬁrst dimension is reﬁned very ﬁnely, very large basic blocks may decrease performance
(d = 2, 3). The best block size improves performance between 1.5 and 3 for all dimensions
compared to basic blocks of size 4. For d = 2, 3 increasing the size of the basic blocks also
increases operational intensity as larger basic blocks increase temporal locality. For d = 4, 5
this eﬀect does not appear as the poles are shorter and the cache can hold suﬃcient entries such
that data is not evicted while ﬁnishing a basic block. Also, for large basic blocks the algorithm
performs close to the bandwidth (1) and the operational intensity bound (2) in all dimension.
Varying the grid size: large problems are bandwidth bound. Fig. 4 shows the performance for diﬀerent dimensions and reﬁnement levels. The left plot shows that the bound
on operational intensity limits the performance achievable for large data sets to less than 1/16
of the available AVX peak performance. The right plot enlarges the crucial part of the ﬁgure.
For small levels the code is able to beat the operational intensity upper bound (2) as the data
sets are small enough to ﬁt into cache and do not need to be written back to main memory
between the loops of the unidirectional principle. As soon as the data sets do no longer ﬁt into
cache (d = 2 & li = 11, d = 3 & li = 7, d = 4 & li = 6, d = 5 & li = 5) the operational
ﬂops
. Again, one
intensity abruptly drops below the operational intensity bound (2) of 0.25 cycle
can observe that the performance is close to the bandwidth bound (1) and to the operational
intensity bound (2) for large grids.
Isotropic grids: baseline and upper bound for anisotropic grids. As the combination
technique also requires anisotropic grids, we compare the time spent hierarchizing an isotropic
grid with times spent when a highly anisotropic grid of the same reﬁnement level is hierarchized.
The experiments are conducted for d = 6 and the level sum is held ﬁx at | |1 = 30. The
2280

Performance of Unidirectional Hierarchization for Component Grids Virtually Maximized
Varying the Grid Size: Large Problems are Bandwidth Bound
(Calculated Flop Count – Annotation: Reﬁnement Level li for all i)
AVX Peak Performance
(32.0 Flops/Cycle)

5
2
1

10
15

Performance [(Calculated Flops)/Cycles]
(indirectly proportional to wall clock time)

cle)
s/Cy
Byte
(6.2
th
id
dw
Ban

10

7
6

3 Dimension 5
3 Dimension 4

0.5

3 Dimension 3

0.2
0.1

Operational Intensity
for large levels
0.05 Dimension 2
3
(0.25 Flops/Byte)
0.1
0.2
0.3
0.5
1
2
Operational Intensity [(Calculated Flops)/(Bytes Transfered)]

Varying the Grid Size (Plot Enlarged): Large Problems are Bandwidth Bound
(Calculated Flop Count – Annotation: Reﬁnement Level li for all i)
3

cle)
s/Cy
Byte
(6.2
th
dwid
Ban

2

1

10 15 5

0.6
0.5

Dimension 5
7

0.8

5

Dimension 4

6 7
6

11

0.4
0.3

4

Dimension 3
Operational Intensity
for large levels
(0.25 Flops/Byte)

Performance [(Calculated Flops)/Cycles]
(indirectly proportional to wall clock time)

30

P. Hupp

10

6
3

Dimension 2
3

3

5

0.2
0.3
0.4
0.5
Operational Intensity [(Calculated Flops)/(Bytes transfered)]

0.6

Figure 4: Increasing the level for diﬀerent dimensions (the right plot enlarges the critical region).
Anisotropic Grids for Dim. 6: Isotropic Grids can Serve as Upper Bound

5

Runtime (wallclock - chrono) [s]

2

1

0

Isotropic Baseline

3

40
Speedup over StructuredSG
30

Anisotropic Grids

4

(5,5,5,5,5,5)

Speedups over StructuredSG: Speedups of 5.8x to 41x for Grids > 30 MB

Bandwidth bound
differs as number
of grid points differs

Dim 4
Dim 3

10
8
6
5
4
3

l2=15

l3=15

l4=15

l5=15

l6=15

(15,3,3,3,3,3) (3,15,3,3,3,3) (3,3,15,3,3,3) (3,3,3,15,3,3) (3,3,3,3,15,3) (3,3,3,3,3,15)

Level Vector (Level Sum = 30)

2.7x

20
15
10
8
6
5
4
3

Dim 2

2

l1=15

40
30

Dim 5

20
15

2

5.8x

41x 8.2x

1

1

0.0001

0.001

0.01

0.1

1

10

100

1000

Number of Grid Points in Million

Figure 5: Left: Comparing the hierarchization of isotropic and anisotropic grids of same dimension and level sum. Right: Speedups of combiHier over StructuredSG [2].

anisotropic grid has reﬁnement level li = 5 for all i. For the anisotropic grids, one dimension
is reﬁned to level 15 while all other dimensions have reﬁnement level 3. For l1 ∈ {3, 5},
block was chosen as 2li and for l1 = 15 block was set to 214 . The bandwidth bound diﬀers
for the isotropic and anisotropic grids as the number of grid points diﬀers.
Fig. 5 shows that hierarchizing the isotropic grid takes longer than hierarchizing any of the
anisotropic grids. Hence the experiments conducted for the isotropic grids can serve as an upper
bound for the runtime needed to hierarchize an anisotropic grid of the same reﬁnement level
and dimension. In addition, hierarchizing the anisotropic grids is faster if the reﬁned dimension
is low. In particular, if the ﬁrst dimension is reﬁned, the runtime is very close to the bandwidth
bound as a large reﬁnement level in the ﬁrst dimension enables large basic blocks.
Benchmark: exploiting the regular structure of component grids allows to outperform generic software by about 1 order of magnitude for large grids. To show
the competitiveness of combiHier it is benchmarked against StructuredSG, the currently fastest
hierarchization code. As StructuredSG is not limited to component grids it cannot exploit
their full structure. While combiHier uses a full grid layout, StructuredSG splits a component
grid according to its hierarchical subspaces. Whether this subspace splitting is beneﬁcial or
creates additional overhead depends on the application in which the software should be used.
StructuredSG was compiled with its standard ﬂags -openmp -O3 -std=c++0x -fno-strict-aliasing
-funroll-loops -xHost using icpc version 14.0.1. StructuredSG is run on all 4 cores in parallel as it
2281

Performance of Unidirectional Hierarchization for Component Grids Virtually Maximized

P. Hupp

was shown that StructuredSG scales very well. The sparse grid level n was set to the minimum
d
such that the sparse grid was large enough to truncate correctly, namely n = −d + 1 + i=1 li .
The speedups of combiHier over StructuredSG are depicted in Fig. 5 and are at least 2.7x
for all measured cases. When the grid consists of at least 4 Million grid points, i.e. is at
least 30 MiB large, the speedups are above 5.8x. The speedups further increase up to 8.2x for
d = 2 and 41x for d = 5 as the grid size grows. Further, for large grids of the same size, the
speedups are increasing with higher dimension. The error bars compare the minimum runtime
of StructuredSG over 10 runs with the maximum runtime of combiHier over 10 runs and vice
versa. For grids with more than 100’000 grid points, i.e. a size roughly above 1MiB, the
performance of both codes is stable and the deviation from the mean run time is negligible.

6

Conclusions

The experiments have shown that the presented implementation combiHier speciﬁcally tuned
for the hierarchization of component grids achieves close to maximum operational intensity and
comes within less than a factor of 1.5 from the bandwidth bound (1). Further, combiHier is
able to outperform generic software by a factor between 5.8x and 41x for problems larger than
30MiB. Hence, writing code speciﬁcally for component grids seems worthwhile.
Further, we have shown that hierarchization according to the unidirectional principle has
bounded operational intensity for large problems. This result holds for any (sparse) grid and is
not limited to component grids. Similarly, a bound on operational intensity can be deduced for
every algorithm employing the unidirectional principle, not only hierarchization. The operational intensity bound is going to depend on the kind of basis functions employed as well as the
algorithm under consideration. For operational intensity beyond that bound the unidirectional
principle needs to be avoided and algorithms cannot be restricted to work on 1-dimensional
subproblems.
Acknowledgements. Special thanks go to my advisor Riko Jacob for many helpful discussions. Thanks a lot to Markus P¨
uschel and his TA Georg Ofenbeck during whose lecture I started this work [6]. Also, I am grateful to Gerrit Buse who granted me access to
StructuredSG [2] and who was always available for discussions.

References
[1] H.-J. Bungartz and M. Griebel. Sparse grids. Acta Numerica, vol. 13:147–269, 04 2004.
[2] G. Buse, R. Jacob, D. Pﬂ¨
uger, and A. Murarasu. A non-static data layout enhancing parallelism
and vectorization in sparse grid algorithms. In ISPDC 2012, Munich, June 2012. IEEE.
[3] M. Griebel, W. Huber, and C. Zenger. Numerical turbulence simulation on a parallel computer
using the combination method. In Flow simulation on high performance computers II, Notes on
Numerical Fluid Mechanics 52, pages 34–47, 1996.
[4] M. Griebel, M. Schneider, and C. Zenger. A combination technique for the solution of sparse grid
problems. In Iterative Methods in Linear Algebra, pages 263–281. IMACS, Elsevier, 1992.
[5] P. Hupp. code: combiHier. https://github.com/PhilippHupp/componentGridHierarch.git.
[6] P. Hupp. Hierarchization for the sparse grid combination technique. CoRR, abs/1309.0392, 2013.
[7] P. Hupp, M. Heene, R. Jacob, and D. Pﬂ¨
uger. Global communication schemes for the numerical
solution of high-dimensional PDEs. in preparation.

2282

Performance of Unidirectional Hierarchization for Component Grids Virtually Maximized

P. Hupp

[8] P. Hupp, R. Jacob, M. Heene, D. Pﬂ¨
uger, and M. Hegland. Global communication schemes for the
sparse grid combination technique. In Parallel Computing - Accelerating Computational Science
and Engineering (CSE), volume 25 of Advances in Parallel Computing. IOS Press, March 2014.
[9] R. Jacob. Eﬃcient regular sparse grid hierarchization by a dynamic memory layout. Proceedings
of the 2nd Workshop on Sparse Grids and Applications, July 2012.
[10] J. D. McCalpin. Memory bandwidth and machine balance in current high performance computers.
IEEE TCCA Newsletter, pages 19–25, dec 1995.
[11] A. Murarasu, J. Weidendorfer, G. Buse, D. Butnaru, and D. Pﬂ¨
uger. Compact data structure and
scalable algorithms for the sparse grid technique. PPoPP ’11, pages 25–34. ACM, 2011.
[12] A. F. Murarasu, G. Buse, D. Pﬂ¨
uger, J. Weidendorfer, and A. Bode. fastsg: A fast routines library
for sparse grids. Procedia CS, 9:354–363, 2012.
[13] G. Ofenbeck, R. Steinmann, V. Caparros, D. G. Spampinato, and M. P¨
uschel. Applying the
rooﬂine model. In IEEE ISPASS, 2014.
[14] D. Pﬂ¨
uger. Spatially Adaptive Sparse Grids for High-Dimensional Problems. PhD thesis, 2010.
[15] Dirk Pﬂ¨
uger. Spatially adaptive reﬁnement. In Jochen Garcke and Michael Griebel, editors, Sparse
Grids and Applications, LNCSE, pages 243–262, Berlin Heidelberg, October 2012. Springer.
[16] S. Smolyak. Quadrature and interpolation formulas for tensor products of certain classes of functions. Soviet Mathematics, Doklady, 4:240–243, 1963.
[17] S. Williams, A. Waterman, and D. Patterson. Rooﬂine: an insightful visual performance model
for multicore architectures. Commun. ACM, 52(4):65–76, April 2009.
[18] C. Zenger. Sparse grids. In Wolfgang Hackbusch, editor, Parallel Algorithms for Partial Diﬀerential
Equations, volume 31 of Notes on Numerical Fluid Mechanics, pages 241–251. Vieweg, 1991.

2283

