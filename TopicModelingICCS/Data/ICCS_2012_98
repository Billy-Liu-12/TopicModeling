Available online at www.sciencedirect.com

Procedia Computer Science 9 (2012) 1240 – 1248

International Conference on Computational Science, ICCS 2012

Knowledge-based Support Vector Machine Classiﬁers via Nearest
Points
Xuchan Jua , Yingjie Tianb,∗
a Institute

of Systems Science, Academy of Mathematics and Systems Science, Chinese Academy of Science
b Research Center on Fictitious Economy and Data Science, CAS, Beijing, China

Abstract
Prior knowledge in the form of multiple polyhedral sets or more general nonlinear sets was incorporated into
Support Vector Machines (SVMs) as linear constraints in a linear programming by Mangasarian and his co-worker.
However, these methods lead to rather complex optimization problems that require fairly sophisticated convex optimization tools, which can be a barrier for practitioners. In this paper we introduce a simple and practical method to
incorporate prior knowledge in support vector machines. After transforming the prior knowledge into lots of boundary points by computing the shortest distances between the original training points and the knowledge sets, we get an
augmented training set therefore standard SVMs and existing powerful SVM tools can be used directly to obtain the
solution fast and exactly. Numerical experiments show the eﬀectiveness of the proposed approach.
Keywords: support vector machines, knowledge, nearest point, classiﬁcation, machine learning.

1. Introduction
Support Vector Machines (SVMs) play the major role among data mining tasks[1, 2, 3], they construct hyperplanes
which can be used for classiﬁcation, regression, or other problems. For classiﬁcation problems, a good separation is
achieved by the hyperplane that make the margin larger then the generalization error of the classiﬁer lower. At the
same time, the primal problem may be stated in its dual form, therefore for discriminating the problem which is
not linearly separable, kernel functions can be introduced. Many tools and methods of SVMs emerge because of
demands, such as LIBSVM, SVMlight, algorithms chunking , decomposition and SMO, which make SVMs solve
problems eﬃciently and eﬀectively.
Knowledge-based support vector machines(KBSVMs) have been proposed by Mangasarian and his co-workers[4,
5, 6]. They introduce prior knowledge in the form of polyhedral sets or more general nonlinear sets into the reformulation of a support vector machine classiﬁer. Firstly, for the prior polyhedral knowledge sets supposed to belong
to one of two categories, the powerful tool, theorems of the alternative[7], is used to convert them to be linear inequalities and added to the 1-norm SVMs as some constraints. When the problem is linearly inseparable, nonlinear
kernel classiﬁers are directly introduced, and using the same theorem, incorporating polyhedral knowledge sets into
∗ Corresponding

author
Email address: tyj@gucas.ac.cn ( Yingjie Tian)

1877-0509 © 2012 Published by Elsevier Ltd. Open access under CC BY-NC-ND license.
doi:10.1016/j.procs.2012.04.135

Xuchan Ju and Yingjie Tian / Procedia Computer Science 9 (2012) 1240 – 1248

1241

a nonlinear kernel classiﬁer can be done. For the nonlinear prior knowledge, they incorporated it into a nonlinear
classiﬁer without kernelization by the conversation to linear inequalities imposed on a linear programming.
However, their methods lost the superiority of the conventional SVMs, i.e., they can not derive the dual problems
of the primal problems with only inner products, their methods for nonlinear cases were all obtained based on the
approximate nonlinear kernel classiﬁers directly. Furthermore, if the number of the knowledge sets is large, the
number of the constraints added to the primal problem increases, the primal problem becomes diﬃcult to solve or
even turns to be infeasible. If we consider the linearly separable problems with convex knowledge sets, we may
notice that only the boundaries of these sets take eﬀect to the optimal separating hyperplane, more precisely, only the
nearest points of the sets to each other and to other training points are important. So we can search for these points by
computing the shortest distances between the training points and prior knowledge sets. After that, we add these nearest
points to the original training set. For this augmented training set, lots of existing SVMs toolboxes and methods can
be applied, which help us get the solution exactly and fast. For the nonlinear problems with linear knowledge sets,
our methods still works with the introduction of the kernel functions.
The paper is organized as follows. Section 2 introduces Mangasarian’s KBSVMs and our method when both the
prior knowledge and the kernel are linear. In section 3, we describe nonlinear problems with linear knowledge solved
by both methods. Section 4 concludes the paper.
We describe our notation now. All vectors will be column vectors unless transposed to a row vector by a prime
except that w is a row vector. The scalar (inner) product of two vectors x and y in the n-dimensional real space Rn
will be denoted by x y. For x ∈ Rn , ||x|| p denotes the p–norm, p = 1, 2, · · · , ∞. The notation A ∈ Rm×n will signify a
real m × n matrix. For such a matrix, A will denote the transpose of A. A vector of ones in a real space of arbitrary
dimension will be denoted by e. Thus for e ∈ Rm and y ∈ Rm the notation e y will denote the sum of the components
of y. For A ∈ Rm×n and B ∈ Rn×k , a kernel K(A, B) maps Rm×n × Rn×k into Rm×k . If x and y are vectors in Rm , then
K(x , y) is a real number. That is, K(x , A ) is a row vector in Rm and K(A, A ) is an m × m matrix.
2. Linear Knowledge-Based Linear Support Vector Machine via Nearest Points
In this section, we propose the Knowledge-Based Support Vector Machine via Nearest Points for the case of linear
knowledge and linear kernel.
2.1. KBSVM
Consider the Knowledge-Based Support Vector Machine Classiﬁer. It will classify m1 points in the n-dimensional
input space Rn . These points are represented by the m1 -by-n matrix A. Each point Ai in the class A+ or A− is speciﬁed
by a m1 × m1 diagonal matrix D with +1 or −1 entries. For this classiﬁcation problem, it can be separated by two
bounding planes according the following regulation:
Ai w ≥ γ + 1,
Ai w ≤ γ − 1,

Dii = 1,

(1)

Dii = −1.

(2)

More succinctly, (1) and (2) can be rewritten:
D(Aw − e γ) ≥ e.

(3)

where e is a vector of ones.
For this problem, the linear programming support vector machine [3, 11] with a linear kernel is given by the
following linear programming with parameter ν > 0:
min
w,γ,y

s.t.

1
|w||21 + νe y,
2
D(Aw − eγ) + y ≥ e,
y ≥ 0,

(4)
(5)
(6)

where w,γ are variables, y is a slack variable, and ν is a parameter. Intuitively, we hope maximizing the distance of
the two planes which often is called the “margin”, and minimizing empirical error e y with weight ν.

1242

Xuchan Ju and Yingjie Tian / Procedia Computer Science 9 (2012) 1240 – 1248

Now if some prior knowledge is provided, and suppose that the prior knowledge is as the following type. All
points x lying in the polyhedral set determined by the linear inequalities
Bx ≤ b

(7)

belong to class A+ . Hence it must lie in the halfspace
wx ≥ γ + 1.

(8)

Bx ≤ b ⇒ wx ≥ γ + 1.

(9)

Therefore, we have the implication

This implication cannot be a constraint directly. So it must be converted. Assuming that the implication (9) holds for
a given (w, γ), it follows that (9) is equivalent to
Bx ≤ b,

wx < γ + 1

x.

has no solution

(10)

This statement can be converted by Farkas theorem[7] as follows
B u + w = 0,

∃u satisﬁes

b u + γ + 1 ≤ 0,

u ≥ 0.

(11)

If we are given k + 1 knowledge sets
k
l

A+ : {x|Bi x ≤ bi }, i = 1, · · · , k,
A− : {x|C j x ≤ c j }, j = 1, · · · , l,

sets belonging to
sets belonging to

(12)
(13)

follow the Farkas theorem[7], (12) and (13) will be converted to the following statements: There exist ui , i = 1, · · · , k,
v j , j = 1, · · · , l, such that
Bi ui + w = 0,
C j v j − w = 0,

bi ui + γ + 1 ≤ 0,

ui ≥ 0,

i = 1, · · · , k,

(14)

c v − γ + 1 ≤ 0,

v j ≥ 0,

j = 1, · · · , l.

(15)

j

j

Incorporating the knowledge sets (14) and (15) into the problem (4)∼(6) as constraints, adding error slack variables
ri , ρi , i = 1, . . . k, s j , σ j , j = 1, . . . l, and minimizing these errors meanwhile, the ﬁnal problem solved in KBSVM is
formulated as
k

min

νe y + μ((

l

e r i + ρi ) + (
i=1

s.t.

e s j + σ j )) + e p,

(16)

j=1

D(Aw − eγ) + y ≥ e,
−p ≤ w ≤ p,
−ri ≤ Bi ui + w ≤ ri ,
bi ui + γ + 1 ≤ ρi ,

(17)
(18)
(19)
i = 1, · · · , k,

(20)

−s ≤ C v − w ≤ s ,

(21)

c v − γ + 1 ≤ ρ , j = 1, · · · , l,
y, ui , ri , ρi , vi , s j , σ j ≥ 0,

(23)

j

j

j

j

j

j

j

(22)

where w, γ, p, ui , v j are variables, and ri , ρi , s j , σ j , y are trade-oﬀ variables with weight μ and ν.
2.2. Knowledge-Based Support Vector Machine Classiﬁer via Nearest Points (NPKBSVM)
First, let’s consider a simple classiﬁcation problem with only one positive knowledge set Bx ≤ b and one negative
knowledge set Cx ≤ c in R2 as shown in Fig.1(a). If we apply algorithm KBSVMs, we will get the separating
straightline wx = γ in Fig.1(a). In fact, the separating straightline wx = γ can be also derived by bisecting the nearest

1243

Xuchan Ju and Yingjie Tian / Procedia Computer Science 9 (2012) 1240 – 1248
8

8

6 wx=γ

6

Bx≤b

6

wx=γ

wx=γ

Bx≤b

2

Cx≤c

4

2

x2

4
x2

x2

4

8

Cx≤c

2

0

0

0

−2

−2

−2

−2

0

2
x1

4

6

8

−2

0

(a)

2
x1

4

6

8

(b)

−2

0

2
x1

4

6

8

(c)

Figure 1: Geometrical illustration of NPKBSVM in R2 .

points between these two convex sets, which enlighten us to solve the knowledge-based problem by computing the
nearest points instead of converting the knowledge to some constraints.
For a more complicated classiﬁcation problem with extra training points and the two knowledge sets in R2 as
shown in Fig.1(b), KBSVMs get the separating line wx = γ. In fact, we can also derive the same separating line
following two steps: (1) compute the shortest distance between each training point and each knowledge set, and the
shortest distance between the two sets, then get the nearest points on the boundaries of the sets; (2) apply standard
SVMs to search for the separating line based on the augmented training set including the original training points and
the nearest points. We will ﬁnd in Fig.1(c) that the approximate outlines of the two sets are composed by the nearest
points, and the role of these sets to the separating line are embodied by these outlines, especially two points (red point
on wx = γ − 1 and green point on wx = γ + 1).
In other words, we use some important points (nearest points) to replace the boundaries and the boundaries to
replace the prior knowledge. The points will represent our prior knowledge, and then be added to the original training
set. These important points can be computed by only solving the following small convex quadratic programming
problems (QPPs):
|| x¯ − Ai ||22 ,

(24)

B x¯ ≤ b,

(25)

min

|| x˜ − Ai ||22 ,

(26)

s.t.

C x˜ ≤ c, ,

(27)

min

|| x¯ − x˜||22 ,

(28)

s.t.

B x¯ ≤ b,

(29)

C x˜ ≤ c.

(30)

min
x¯

s.t.
x˜

and
x¯, x˜

Suppose that we get m2 nearest points including ( x¯i , +1), i = 1, · · · , l+ and ( x˜i , −1), i = 1, · · · , l− , then add them to A
and D to get the augmented training set A∗ and D∗ . For this new training set, we can apply the standard SVM[1, 2]:
min
w,γ,y

s.t.

1
|w||22 + νe y,
2
D∗ (A∗ w − eγ) + y ≥ e,

(32)

y ≥ 0,

(33)

(31)

where w, γ are variables, y is the slack variable. Of course, problem (33)-(35) can be stated in a dual form and the
kernel functions are used directly for nonlinear case. In addition, we can choose diﬀerent weighting to original dataset
A and the new added points to tradeoﬀ them.

1244

Xuchan Ju and Yingjie Tian / Procedia Computer Science 9 (2012) 1240 – 1248

2.3. Numerical Experiments
In order to demonstrate the eﬃciency of our linear NPKBSVM method, some experiments are performed for three
datasets and several measurable indicators are compared with KBSVMs: True Positive (T P), True Negative (T N),
False Positive (FP), False Negative (FN), True Positive rate (rtp )
rtp =

tp
;
tp + f n

(34)

rtn =

tn
;
tn + f p

(35)

rf p =

fp
;
f p + tn

(36)

rfn =

fn
;
tp + f n

(37)

True Negative rate (rtn )

False Positive rate (r f p )

False Negative rate (r f n )

Matthew Correlation Coeﬃcient (MCC)
MCC = √

T P × T N − FP × FN
;
(T P + FP)(T P + FN)(T N + FP)(T N + FN)

(38)

and the ROC curve.
Firstly, we use a 2-dimensional artiﬁcial dataset. The prior knowledge sets are based on two feature x1 and x2 and
in the form of triangles
1
9
3
( x1 − x2 ≥ 1) ∧ ( x1 + x2 ≥ ) ∧ (4x1 + x2 ≤ 21) ⇒ class1,
2
3
2
(x2 ≥ 0) ∧ (2x1 − x2 ≥ 2) ∧ (2x1 + x2 ≤ 6) ⇒ class − 1.

(39)
(40)

The original training points and the two prior knowledge are shown in Figure 1(b). By calculating the shortest
distances between all points and knowledge sets (39)∼(40), we get some important points which may become support
vectors to replace the prior knowledge sets, and add them to the original set. They are shown in Figure 1(c). All prior
knowledge sets therefore are interpreted as point sets. It is to see the separating planes in Figure 1(b) and Figure 1(c)
are the same. If the positive prior knowledge set locates in the upper right corner or the negative prior knowledge
locates in the lower left corner, the new points computed by this method will not become support vectors. That is, the
prior knowledge sets do not work.
The second dataset is heart dataset [12] using a less than 50% cut oﬀ diameter narrowing for predicting present or
absent of heart disease, which has 13 features and 270 instances. The rules depend upon two features from the dataset
namely oldpeak (feature 10) and thal (T) (feature 13). The rules are given as
(T ≥ 4)Λ(O ≥ 2) ⇒ Presence of heart disease,
(T ≤ 3)Λ(O ≤ 1) ⇒ Absence of heart disease.

(41)
(42)

By calculating the nearest points by problems (24)∼(25), (26)∼(27) and (28)∼(30), we got 397 points, of which 233
points belong to class +1 and 164 points belong to class −1. A new training set including 667 points is thus obtained by
adding them to the original dataset. We use the 667 points as the training set and the original 270 points as the testing
set. Table 1 gives out the values of T P, FP, FP, FN, rtp , rtn , r f p , r f n , and the value of MCC is 0.6769. Fig.2(a) shows
the ROC curve. Table 2 reports the comparison between our method with 1-norm ( w 1 ) KBSVM(KBSVM1) and
2-norm ( w 2 )(KBSVM2), the optimal penalty parameters are chosen by grid searching and ten-fold cross validation.

1245

Xuchan Ju and Yingjie Tian / Procedia Computer Science 9 (2012) 1240 – 1248
Table 1: The valve of T P, FP, FP, FN, rtp , rtn , r f p , r f n of heart dataset

number
rate

tp
93
77.5%

fn
27
22.5%

fp
16
10.67%

ROC curve

tn
134
89.33%
ROC curve

0.8

0.8

0.6

0.6
rtp

1

rtp

1

0.4

0.4

0.2

0.2

0
0

0.5
rfp

1

0
0

(a)

0.5
rfp

1

(b)

Figure 2: The ROC curves of heart dataset and WPBC dataset

The results indicate that the performance of NPKBSVM can match KBSVM, and most importantly, we no longer need
to solve a problem with many constraints, and can make use of many toolboxes and algorithms to solve the standard
linear SVM. It is more fast and practical.
The third dataset is a subset of Wisconsin breast cancer prognosis dataset WPBC using a 60-month cut-oﬀ for
predicted recurrence or non-recurrence of disease [12], which has 33 feature and 194 instances. 46 points are positive
and 148 points are negative. We use the prognosis rules given by doctors [13] which depend on two features from the
dataset: tumor size(T) (feature 31) and lymph node status(L) (feature 32). Tumor size is the diameter of the excised
tumor in centimeters, and lymph nodes. The rules are given below:
(L ≥ 5)Λ(T ≥ 4) ⇒ Recur,

(43)

(L = 0)Λ(T ≤ 1.9) ⇒ Non Recur.

(44)

Similar with the above experiments, we got 324 new points, of which 171 points belong to class +1 and 153 points
belong to class −1. Adding them to the original dataset, we can obtain 518 points as a new training set. Since the
training set is not balance, we divided it into 3 groups. Every group consists of the remaining 46 positive points and a
third of remaining negative points. Using 518 points as a training set and every group as a testing set, we can get an
average accuracy. Table 3 shows the values of T P, FP, FP, FN, rtp , rtn , r f p , r f n , and the MCC is 0.5861. Fig.2(b)
shows the ROC curve for the WPBC dataset. Table 4 shows the comparison results between KBSVM1, KBSVM2
and NPKBSVM for the WPBC dataset, which also shows that NPKBSVM gets a good performance.
3. Linear Knowledge-Based nonlinear Support Vector Machine via Nearest Points
In this section, we extend the Knowledge-Based Support Vector Machine via Nearest Points for the case of linear
knowledge and nonlinear kernel.
3.1. Mangasarian’s Knowledge-Based Nonlinear Kernel Classiﬁers
To obtain a more complex classiﬁer, one resort to a nonlinear separating surface in Rn instead of the linear separating surface wx = γ deﬁned as follows [3]
K(x , A )Du = γ,

(45)

1246

Xuchan Ju and Yingjie Tian / Procedia Computer Science 9 (2012) 1240 – 1248
Table 2: Test accuracy comparison between KBSVM1, KBSVM2 and NPKBSVM

method
rate

KBSVM1
85.33%

KBSVM2
71.48%

NPKBSVM
85.07%

Table 3: The valve of T P, FP, FP, FN, rtp , rtn , r f p , r f n of WPBC dataset

tp
33
71.74%

number
rate

fn
13
28.26%

fp
6
13.64%

tn
38
86.36%

where K is an arbitrary nonlinear kernel as deﬁned in the introduction. Suppose that the polyhedral {x|Bx ≤ b} must
lie in the halfspace {x|wx ≥ γ + 1}, then the implication can be got
Bx ≤ b ⇒ wx ≥ γ + 1,

(46)

By letting w take on its dual form w = A Du [3, 10], the implication (46) becomes
Bx ≤ b ⇒ x A Du ≥ γ + 1,

(47)

Bx ≤ b ⇒ K(x , A )Du ≥ γ + 1.

(48)

this implication is then kernelized as

Based on the theorem of the alternative [7], (48) is equivalent to the following system of linear equalities that have a
solution s for a given u and γ
K(A, B )s + K(A, A )Du = 0, b s + γ + 1 ≤ 0,
s ≥ 0.

(49)
(50)

If k + l knowledge sets (12)∼(13) are given, it can also be converted as follows: There exist si , i = 1, · · · , k, t j ,
j = 1, · · · , l, such that
K(A, Bi )si + K(A, A )Du = 0, bi si + γ + 1 ≤ 0, si ≥ 0, i = 1, · · · , k,
K(A, C j )t j − K(A, A )Du = 0, c j t j − γ + 1 ≤ 0, t j ≥ 0, j = 1, · · · , l,

(51)
(52)

These equations are added to the standard linear programming formulation of a nonlinear kernel classiﬁer [5] and the
ﬁnal problem solved turns to be
k

min

νe y + μ((

l

i=1

s.t.

e z2j + ξ2j )) + e r,

e zi1 + ξ1i ) + (

D(K(A, A )Du − eγ) + y ≥ e,
−r ≤ u ≤ r,
−zi1 ≤ K(A, Bi )si + K(A, A )Du ≤ zi1 ,
b s +γ+1
i

i

−z2j ≤
j j

(53)

j=1

≤ ξ1i ,
j j

i = 1, · · · , k,

K(A, C )t − K(A, A )Du ≤ z2j ,

c t − γ + 1 ≤ ξ2j , j = 1, · · ·
y, r, zi1 , z2j , si , t j , ξ1i , ξ2j ≥ 0,

, l,

where u, γ, r, si , t j are variables, y, zi1 , z2j , ξ1i , ξ2j are slack error variables with with weight ν and μ.

(54)
(55)
(56)
(57)
(58)
(59)
(60)

1247

Xuchan Ju and Yingjie Tian / Procedia Computer Science 9 (2012) 1240 – 1248
Table 4: Test accuracy comparison between KBSVM1, KBSVM2 and NPKBSVM

method
rate

KBSVM1
73.08%

KBSVM2
70.28%

NPKBSVM
71.85%

3.2. Linear Knowledge-Based Nonlinear SVMs via Nearest Points
Diﬀerent with the above nonlinear KBSVM, we only need to change problems (24)∼(25), (26)∼(27), and (28)∼(30)
to be following kernelized problems
min
x¯

s.t.
min
x˜

s.t.

K( x¯ , x¯) − 2K( x¯ , Ai ) + K(Ai , Ai ),

(61)

B x¯ ≤ b,

(62)

K( x˜ , x˜) − 2K( x˜ , Ai ) + K(Ai , Ai ),

(63)

C x˜ ≤ c,

(64)

and
min
x

s.t.

K( x¯ , x¯) − 2K( x¯, x˜ ) + K( x˜ , x˜),

(65)

B x¯ ≤ b,
C x˜ ≤ c,

(66)
(67)

respectively, where K(·, ·) is the kernel function. After getting the nearest points and then the augmented training set
A∗ and D∗ , standard SVM with the nonlinear kernel K(·, ·) is easily applied.
3.3. Numerical Experiments
We test our method on the classical checkerboard dataset [10, 14, 15, 16] which consists of 1000 points taken from
16-square checkerboard. We ﬁrst took a subset of 16 points only, each one is the “center” of one of the 16 squares.
Here, we use RBF kernel to model the separating surface. Fig.3(a) shows the results of C-SVM with RBF kernel.
Similar to the discussion in [5], we thus obtained a fairly poor Gaussian-based representation of the checkerboard
with correctness of only 87.50% on a testing set of uniformly randomly generated 800 points (400 positive and 400
negative) labeled according to the true checkerboard pattern.
1

1

1

0.8

0.8

0.8

0.6

0.6

0.6

0.4

0.4

0.4

0.2

0.2

0.2

0

0

0

−0.2

−0.2

−0.2

−0.4

−0.4

−0.4

−0.6

−0.6

−0.6

−0.8

−0.8

−1
−1

−0.8

−0.6

−0.4

−0.2

0

(a)

0.2

0.4

0.6

0.8

1

−1
−1

−0.8

−0.8

−0.6

−0.4

−0.2

0

0.2

0.4

0.6

0.8

1

−1
−1

−0.8

−0.6

−0.4

−0.2

(b)

0

0.2

0.4

0.6

0.8

1

(c)

Figure 3: Comparison between nonlinear SVM, KBSVM, and NPKBSVM

On the other hand, same with the method in [5], two knowledge sets, i.e., the leftmost two squares of the bottom
row of squares are provided in conjunction with these same 16 points, we obtained the corresponding separating
surfaces by KBSVM and NPKBSVM shown in Fig.3(b) and Fig.3(c), the correctness are 97.5% and 98% respectively
on the 800-point testing set.

1248

Xuchan Ju and Yingjie Tian / Procedia Computer Science 9 (2012) 1240 – 1248

4. Conclusion
In this paper we introduce a simple and practical method, knowledge-based SVM via nearest points (NPKBSVM)
to incorporate prior knowledge in support vector machines. After transforming the prior knowledge into lots of
boundary points by computing the shortest distances between the original training points and the knowledge sets,
we get an augmented training set therefore standard SVMs and existing powerful SVM tools can be used directly to
obtain the solution fast and exactly. Numerical experiments for linear or nonlinear classiﬁers with linear knowledge
sets show the eﬀectiveness of the proposed approach. Nonlinear classiﬁers with nonlinear knowledge sets can also be
solved by this method and under our consideration.
Acknowledgment
This work has been partially supported by grants from National Natural Science Foundation of China( NO.70921061,
NO.10601064), the CAS/SAFEA International Partnership Program for Creative Research Teams, the President Fund
of GUCAS, and the National Technology Support Program 2009BAH42B02.
References
[1] V.N.Vapnik. The Nature of Statistical Learning Theory. Springer, New Work, second edition, 2000.
[2] V.Cherkassky and F.Mulier. Learing from Data-Concepts, Theory and Methods. John Wiley and Sons, New York,1998.
[3] O.L.Mangsarian. Generalized support vector machines. In A.Smola, P.Bartlett, B.Scholkopf, and D.Schuurmans, editors, Advances inLarge
Margin Classiﬁers, pages 135-146, Cambridge, MA, 2000.
[4] G.Fung, O.L.Mangasarian and J. Shavlik. Knowledge-based support vector machine classiﬁers. Technical Report 01-09, Data Mining Institute,
Computer Sciences Department, University of Wisconsin, Madison, Wisconsin, November 2001.
[5] G.Fung, O.L.Mangasarian and J. Shavlik. Knowledge-based nonlinear kernel classiﬁers. Technical Report 03-02, Data Mining Institute, Computer Sciences Department, University of Wisconsin, Madison, Wisconsin, March 2003.
[6] O.L.Mangasarian and Edward W.Wild. Nonlinear knowledge-based classiﬁers. Technical Report 06-04, Data Mining Institute, Computer
Sciences Department, University of Wisconsin, Madison, Wisconsin, August 2006.
[7] O.L.Mangasarian. Nonlinear programming. SIAM, Philadelphia,PA,1994.
[8] Y.-J.Lee, O.L.Mangasarian and W.H.Wolberg. Survival-time classiﬁcation of breast cancer patients. Technical Report 01-03, Data Mining
Institute, Computer Science Department, University of Wisconsin, Madison, Wisconsin, March 2001.
[9] R. O. Duda, P. E. Hart and D. G. Stork. Pattern Classiﬁcation, 2nd ed.
[10] Y.-J.Lee and O.L.Mangasarian. SSVM: A smooth support vector machine. Computation Optimization and Application, 20:5-22,2001, Data
Mining Institute, University of Wisconsin, Technical Report 99-03. New York: Wiley, 2001.
[11] P.S.Bradley and O.L.Mangasarian. Feature selection via concave minimization and support vector machines. In J.Shavlik,editor, Machine
Learning Proceedings of the Fifteenth International Conference(ICML’98), 1998.
[12] C.L.Blake, C.J.Merz. UCI Repository for Machine Learning Databases. Department of Information and Computer Sciences, University of
California, Irvine, 1998.
[13] Y.-J.Lee, O.L.Mangasarian and W.H.Wolberg. Survival-time classiﬁcation of breast cancer patients. Technical Report 01-03, Data Mining
Institute, Computer Science Department, University of Wisconsin, Madison, Wisconsin, March 2001.
[14] T. K. Ho and E. M. Kleinberg. Building projectable classiﬁers of arbitrary complexity. In Proceedings of the 13th International Conference
on PatternRecognition, pages 880-885, Vienna, Austria, 1996.
[15] T. K. Ho and E. M. Kleinberg. Checkerboard dataset, 1996.
[16] Y.-J.Lee and O.L.Mangasarian. RSVM: Reduced support vertor machines. Technical Report 00-07, Data Mining Institute, Computer Sciences
Dapartment, University of Wisconsin, Madison, July 2000, Proceedings of the FirstSIAM International Conference on Data Mining, Chicago,
April 5-7, 2001.

