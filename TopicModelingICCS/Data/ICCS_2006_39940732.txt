On the Solution of Skew-Symmetric Shifted
Linear Systems
T. Politi1 and A. Pugliese2
1

2

Dipartimento di Matematica, Politecnico di Bari,
Via Amendola 126/B, I-70126 Bari (Italy)
politi@poliba.it
School of Mathematics, Georgia Institute of Technology,
Atlanta, GA 30332 U.S.A.
pugliese@math.gatech.edu

Abstract. In this paper we consider the problem of solving a sequence
of linear systems with coeﬃcient matrix Aα = I + αA (or Aα = αI + A),
where α is a real paramater and A is skew-symmetric matrix. We propose
to solve this problem exploiting the structure of the Schur decomposition of the skew-symmetric matrix and computing the Singular Value
Decomposition of a bidiagonal matrix of halved size.

1

Introduction

In this paper we consider the solution of a sequence of linear systems of the form
Aα xα = bα

(1)

Aα = I + αA

(2)

where
or
Aα = αI + A
with I identity matrix of order n, α positive real parameter belonging to ]0, αmax ]
and A is a real skew-symmetric matrix of order n. The question is how to solve
eﬃciently the linear systems for subsequent values of the shift α. The goal is
to obtain a solution procedure that is cheaper, in terms of total solution costs,
trying to save much computations as possible. In [2, 3] the authors consider the
solution of a sequence of linear systems (1)-(2) having A symmetric and positive
deﬁnite. In this case they try to obtain an eﬃcient preconditioning matrix to
save computational operations when the parameter α changes, observing that
using the same preconditioner for diﬀerent values of α brings to a very slow
convergence. Sequences of linear systems of the form (1)-(2) arise in diﬀerent
ﬁelds of applied mathematics. If we consider the numerical solution of a Kortwegde Vries partial diﬀerential equation using a particular discretization (see [4]) a
diﬀerential system
y (t) = A(y)y(t)
V.N. Alexandrov et al. (Eds.): ICCS 2006, Part IV, LNCS 3994, pp. 732–739, 2006.
c Springer-Verlag Berlin Heidelberg 2006

On the Solution of Skew-Symmetric Shifted Linear Systems

733

with initial condition y(0) = y0 has to be solved. Applying numerical schemes
of the integration, at each step, a sequence of linear systems (1)-(2) has to be
solved.
The paper is organized as follows: in Section 2 we derive the structure for a
decomposition of matrix (2), in Section 3 we analyze the computational cost of
the proposed method, while in Section 4 possible applications are described.
The following theorem is a general result that will be useful later.
Theorem 1. Let A be the following n × n, n even, square matrix
A=

B C
−C B

with C, B ∈ IRm×m symmetric matrices and m = n/2. If A is nonsingular then
A−1 retains the same structure of A, i.e.
A−1 =

X Y
−Y X

with X, Y ∈ IRm×m symmetric matrices.

2

The Decomposition of the Matrix I + αA

The factorization technique that we describe in this section has the same of
starting point of the one shown in [5] in order to compute the exponential of
the skew-symmetric matrix A times a unitary norm vector v, and, in general, to
compute analytic matrix functions. First of all we consider the problem to ﬁnd
the Schur decomposition of the skew-symmetric matrix A. Hence we consider the
real skew symmetric matrix A ∈ IRn×n , a vector q 1 ∈ IRn , such that q1 2 = 1,
the Krylov matrix K(A, q 1 , m) = [q 1 Aq 1 A2 q 1 . . . Am−1 q 1 ] ∈ IRn×m and the
Krylov subspace Km = span{q1 , Aq 1 , . . . , Am−1 q 1 }. The following result states
the conditions for the existence of a Hessenberg form of A (see [6]).
Theorem 2. Suppose Q = [q 1 q 2 . . . q n ] ∈ IRn×n is an orthogonal matrix. Then
Q AQ = H is an unreduced Hessenberg matrix if and only if R = Q K(A, q1 , n)
is nonsingular and upper triangular.
Thus, when K(A, q 1 , n) is of full rank n, from the QR factorization of K(A, q 1 , n),
it follows that an unreduced Hessenberg form H of A exists. The Hessenberg
decomposition, A = QHQ , is essentially unique when the ﬁrst column q 1 of Q
is ﬁxed and its unreduced Hessenberg form H is skew-symmetric and possesses
the following tridiagonal structure
⎤
⎡
0 −h2 0 . . . 0
. ⎥
⎢
⎢ h2 0 −h3 . . . .. ⎥
⎥
⎢
⎥
⎢
(3)
H = ⎢ 0 ... ... ... 0 ⎥ .
⎥
⎢
⎥
⎢ . .
⎣ .. . . hn−1 0 −hn ⎦
0 . . . 0 hn 0

734

T. Politi and A. Pugliese

For skew-symmetric matrices the reduction to the above tridiagonal form can
be performed using the following Lanczos tridiagonalization process:
Let q 1 be a vector of IRn with q1 = 1 and set h1 = 0 and q 0 = 0.
for j = 1 : n
w j = Aq j + hj q j−1
hj+1 = wj
q j+1 = w j /hj+1
end
We have to notice that, in exact arithmetic, the above algorithm is equivalent
to the Arnoldi process applied to skew symmetric matrices. It provides an orthogonal matrix Q = [q 1 q 2 . . . q n ] ∈ IRn×n such that Q AQ = H where H is
the tridiagonal matrix (3). Moreover, it allows one to take full advantage of the
possible sparsity of A due to the matrix-vector products involved. However, in
ﬂoating-point arithmetic the vectors q j could progressively lose their orthogonality, in this case, the application of a re-orthogonalization procedure is required
[6, 7]. We suppose n to be even, but the case of n odd may be approached in a
similar way. Let us consider the permutation matrix
P = [e1 , e3 , . . . , en−1 , e2 , e4 , . . . , en ],
where ei is the i−th vector of the canonical basis of IRn . Then, if H is as in (3)
we have
0 −B
P HP =
,
(4)
B
0
where B is the lower bidiagonal square matrix of size m = n/2 given by:
⎤
⎡
h2 0 . . . . . . 0
⎢ −h3 h4 . . .
0 ⎥
⎥
⎢
⎢
.. ⎥
.
.
.
.
⎢
.
. ⎥
B = ⎢ 0 −h5 .
⎥.
⎥
⎢ . . .
.. .. h
⎦
⎣ ..
n−2 0
0 . . . 0 −hn−1 hn

(5)

Since all diagonal and subdiagonal entries of B are non zero, the m singular
values of B are distinct and diﬀerent from zero.
Let us consider the singular value decomposition of B
B = U ΣV ,
with Σ = diag(σ1 , σ2 , . . . , σm ) and σ1 > σ2 > . . . σm > 0, and the orthogonal
n × n matrix
U 0
W =
.
0 V
Hence
W P HP W =

U
0
0 V

0 −B
B
0

U 0
0 V

=

0 −Σ
.
Σ 0

On the Solution of Skew-Symmetric Shifted Linear Systems

735

The matrix I + αA can be decomposed as
I + αA = QP W (I + αΣ)W P Q
where
Σ=

(6)

O −Σ
.
Σ O

If we have to solve the sequence of linear systems (1)-(2) we could exploit the
factorization (6) computing vector xα , for a given value α through the following
steps:
1. Compute y α = Q bα ;
2. Compute cα = W P y α ;
3. Solve the linear system
(I + αΣ)uα = cα

Im −αΣ
uα = cα
αΣ Im

⇔

(7)

4. Compute xα = QP W uα .
The solution of (7) is very simple, in fact exploiting Theorem 1 it is easy to
compute the inverse of the matrix I + αΣ. In fact:
Im −αΣ
αΣ Im
where

−1

=

X Y
−Y X

X = (Im + α2 Σ 2 )−1
Y = αΣ(Im + α2 Σ 2 )−1

and, explicitly
2 −1
)
X = diag (1 + ασ12 )−1 , (1 + ασ22 )−1 , . . . , (1 + ασm
2 −1
Y = diag ασ1 (1 + ασ12 )−1 , ασ2 (1 + ασ22 )−1 , . . . , ασm (1 + ασm
.
)

Decomposing the vectors uα and cα as
uα =

u1
,
u2

cα =

c1
c2

with u1 , u2 , c1 , c2 ∈ IRm , the solution of system (7) is
u1 = (Im + α2 Σ 2 )−1 (c1 + αΣc2 )
u2 = (Im + α2 Σ 2 )−1 (c2 − αΣc1 ).

736

3

T. Politi and A. Pugliese

Some Remarks on the Computational Cost

In this section we consider the computational cost of the algorithm described
in the previous section for the solution of a single linear system and the cost to
solve a diﬀerent linear system changing just the vector bα and the parameter
α. We suppose that matrix A has a dense structure, that is the worst case from
the computational point of view. It is obvious that this process cannot be used
when we have to solve a small number of linear systems since it involves the
computation of the Schur decomposition of a matrix of order n and of all the
factor of the singular value decomposition of a bidiagonal matrix of order n/2. It
is well-known (see [6]) that the computational cost of the Schur decomposition
of A is
2n3 + 3n2 + 7n
plus n − 1 square roots, while the cost of the SVD decomposition of a bidiagonal
matrix of order m is
12m2 + 30m + 2m square roots,
and, in this particular case is 3n2 + 15n plus n square roots. In the following
we shall neglect the number of square roots. In Table 1 we recall the number
of operation required by the decompositions we need to compute just once. In
Table 2 we recall the computational costs required by the operations that we
need to perform at each step (i.e. when the value of parameter α changes).
Table 1. Computational costs for the diﬀerent steps of the algorithm
Step

Flops
3

Schur Decomposition
SVD Decomposition

2n + 3n2 + 7n
3n2 + 15n

Total

2n3 + 6n2 + 22n

We observe that the lower cost of the solution of the linear system is due to the
special structure of the coeﬃcient matrix. If we have to solve Kα linear systems
(with Kα diﬀerent values of α) the global computational cost is
N1 = 2n3 +6n2 +22n+Kα 5n2 + 4n = 2n3 +(6+5Kα)n2 +2(11+2Kα)n. (8)
Now we compare the computational with a similar algorithm, which starts considering the same Schur decomposition of A but solves, for a ﬁxed value of α a
tridiagonal non-symmetric system. In detail we consider the decomposition
I + αA = Q(I + αH)Q

On the Solution of Skew-Symmetric Shifted Linear Systems

737

Table 2. Computational costs for each step the algorithm (α ﬁxed)
Step
Step
Step
Step
Step

Flops
1
2
3
2

(Matrix-vector product)
(Matrix-vector product)
(Solution of the linear system)
(Matrix-vector products)

2n2
n2 /2
4n
5n2 /2
5n2 + 4n

Total

hence the single linear system becomes
Q(I + αH)Q xα = bα .
Once this decomposition has been computed the following steps need to be
carried on:
1.
2.
3.
4.

Compute
Compute
Solve the
Compute

y α = Q bα ;
the LU decomposition of the tridiagonal matrix I + αH;
linear system (I + αH)uα = y α ;
xα = Quα .

Also for this algorithm we recall in Table 3 the computational costs for the single
steps.
Table 3. Computational cost for the second algorithm
Step
Schur decomposition of A
Step 1 (Matrix-vector product)
Step 2-3 (LU decomposition + linear solver)
Step 4 (Matrix-vector products)

Flops
3

2n + 3n2 + 7n
2n2
11n
2n2

The total computational cost for Kα linear systems is
N2 = 2n3 + 3n2 + 7n + Kα 4n2 + 11n = 2n3 + (3 + 4Kα)n2 + (7 + 11Kα)n. (9)
Both costs have the same coeﬃcient for n3 hence it is important to analyze
the coeﬃcients for n2 and n. Comparing (8) and (9) and in particulat their dependence on the number Kα of systems that must be solved we observe that
the computational cost N1 of the ﬁrst algorithm, for large Kα , will be greater
than N2 . Just for small values of n, i.e. when the linear coeﬃcient for n, N1 is
smaller than N2 . In order to underline this phenomenon in Figure 1 we have
sketched the number of ﬂops required by the two algorithms (solid line for the

738

T. Politi and A. Pugliese
6

10

5

10

4

Flops

10

3

10

2

10

1

10

0

10

20

30

40
50
60
70
Number of systems of dimension n

80

90

100

Fig. 1. Computational costs for considered algorithms

ﬁrst, dashed line for the second), taking diﬀerent value for n = 2, 4, . . . , 20,
and Kα .

4

Applications

In this section we describe just an eﬃcient application of the algorithm described
in Section 2. Considering the Kortvieg-de Vrijes partial diﬀerential equation
ut = −uux − δ 2 uxxx,
with periodic boundary conditions u(0, t) = u(L, t), where L is the period and δ
is a small parameter. As shown in [4], appropriate methods of space discretization
lead to solve a set of ODEs of the form
y = A(y)y,

y(0) = y0 ,

evolving on the sphere of radius y0 , where y(t) = (u0 (t), u1 (t), . . . , uN −1 (t)) ,
ui (t) ≈ u(iΔx, t) for i = 0, 1, . . . , N − 1, and where Δx = N2 is the spatial step of
[0, 2]. For instance if we consider the space discretization method in [8] we have
1
δ2
g(y) −
P
(10)
6Δx
2Δx3
where both g(y) and P are two N × N skew symmetric matrices given by:
⎧
ui−1 + ui
if j = i + 1
⎪
⎪
⎪
⎪
if i = 1, j = N
⎨ −(u0 + uN −1 )
if i = j + 1
[g(y)]i,j = −(uj−1 + uj )
⎪
⎪
u
+
u
if
i = N, j = 1
⎪
0
N
−1
⎪
⎩
0
otherwise.
A(y) = −

On the Solution of Skew-Symmetric Shifted Linear Systems

and

⎡

0 −2 1

0 ...
.
⎢
⎢ 2 0 −2 1 . .
⎢
⎢
.
⎢ −1 2 0 −2 . .
⎢
⎢
⎢ 0 −1 2 0 . . .
P =⎢
⎢ . . . . .
⎢ .. . . . . . . . .
⎢
⎢
.. ..
⎢ 0
. . 2
⎢
⎢
..
⎣ 1 0
. −1
−2 1

0 −1 2
..

.

..

.

..

.

0

739

⎤

⎥
0 −1 ⎥
⎥
⎥
0 ⎥
⎥
. . .. ⎥
. . ⎥
⎥.
⎥
1 0 ⎥
⎥
⎥
−2 1 ⎥
⎥
⎥
0 −2 ⎦

2
0 . . . 0 −1 2

0

Applying the Backward Euler Method a system like (1) must be solved at each
step, especially the paramter δ is time dependent and a splitting technique for
(10) is used using an explicit method for g(y) and an implicit one for P .

References
1. Bai Z., Golub G., Ng M.K.: Hermitian and skew-Hermitian splitting methods for
non-Hermitian positive deﬁnite linear systems. SIAM J. Matr. Anal. 24 (3) (2003)
603–626
2. Benzi M., Bertaccini D.: Approximate inverse preconditioning for shifted linear systems. BIT 43 (2003) 231–244
3. Bertaccini D.: Eﬃcient solvers for sequences of complex symmetric linear systems.
ETNA 18 (2004) 49–64
4. Chen J.B., Munthe-Kaas H., Qin M.Z.: Square-conservative schemes for a class of
evolution equations using Lie group methods. SIAM J. Num. Anal. 39 (6) (2002)
2164–2178
5. Del Buono N., Lopez L., Peluso R.: Computation of the exponential of large sparse
skew-symmetric matrices. SIAM J. Sci. Comp. 27 (2005) 278–293
6. Golub G.H., Van Loan C.F.: Matrix Computation. The John Hopkins Univ. Press,
Baltimore, (1996)
7. Saad Y.: Analysis of some Krylov subspace approximation to the matrix exponential
operator, SIAM J. Numer. Anal. 29 (1) (1992) 209–228
8. Zabusky N.J., Kruskal M.D.: Interaction of solitons in a collisionless plasma and the
recurrence of initial states. Phys. Rev. Lett. 15 (1965) 240–243

