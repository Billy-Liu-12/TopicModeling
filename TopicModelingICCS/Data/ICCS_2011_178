Available online at www.sciencedirect.com

Procedia Computer Science 4 (2011) 2186–2195

International Conference on Computational Science, ICCS 2011

Performance prediction of ocean color Monte Carlo simulations
using multi-layer perceptron neural networks
Tamito Kajiyamaa,∗, Davide D’Alimonteb , Jos´e C. Cunhaa
a CITI,

DI/FCT, Universidade Nova de Lisboa, Quinta da Torre, 2829–516 Caparica, Portugal
DI/FCT, Universidade Nova de Lisboa, Quinta da Torre, 2829–516 Caparica, Portugal

b CENTRIA,

Abstract
A performance modeling method is presented to predict the execution time of a parallel Monte Carlo (MC) radiative transfer simulation code for ocean color applications. The execution time of MC simulations is predicted using
a multi-layer perceptron (MLP) neural network regression model trained with past execution time measurements in
diﬀerent execution environments and simulation cases. On the basis of the MLP performance model, a complementary job-environment mapping algorithm enables an eﬃcient utilization of available high-performance computing
resources minimizing the total execution time of the simulation jobs distributed in multiple environments.
Keywords: performance modeling, radiative transfer, marine optics, non-linear regression.

1. Introduction
Predicting the execution time of scientiﬁc and engineering simulation codes is fundamental for an eﬃcient job
scheduling and comprehensive utilization of available high-performance computing (HPC) resources. However, the
inherent complexity of simulation codes coupled with a growing number of performance factors in modern computer
architectures aﬀect the execution time of the codes and make performance prediction a challenging problem.
The objective of the present study is to develop a performance prediction method for a Monte Carlo (MC) radiative
transfer code for ocean color (OC) simulations, hereafter referred to as MOX [1, 2]. The MOX code simulates inwater and above-water light ﬁelds at high spatial resolution in an extended two-dimensional domain by tracing a
number of photons and tracking their trajectories. The scope of the MOX development is theoretical quantiﬁcation
of uncertainties due to environmental eﬀects and measurement protocols over in situ radiometric OC data products in
coastal waters. Simulated light ﬁelds are subject to MC intrinsic statistical noise, so that a large number of photons has
to be traced in order to achieve the accuracy required for a thorough investigation of uncertainty budgets. Therefore,
the MOX code has been parallelized to permit large-scale OC simulations in an acceptable amount of time by means
of HPC facilities. A key challenge in performance modeling of the MOX code is to address the high variability and
non-linearity of the execution time which is a function of various input simulation parameters (including sea-surface
geometry, seawater inherent optical properties, and illumination conditions) and performance factors of execution
∗ Corresponding

author.
Email address: t.kajiyama@di.fct.unl.pt (Tamito Kajiyama)

1877–0509 © 2011 Published by Elsevier Ltd. Open access under CC BY-NC-ND license.
Selection and/or peer-review under responsibility of Prof. Mitsuhisa Sato and Prof. Satoshi Matsuoka
doi:10.1016/j.procs.2011.04.239

Tamito Kajiyama et al. / Procedia Computer Science 4 (2011) 2186–2195

2187

environments (such as the number of CPU cores, CPU clock frequency, and memory bandwidth). The present work
addresses this performance modeling problem through a learning-from-data approach based on multi-layer perceptron
(MLP) neural networks [3]. The rationale of the MLP-based approach is two-fold. One is the high non-linearity of
the MOX execution time, which makes it hard to model the MOX performance in an analytical form and prevents the
use of linear schemes. The other reason is that MOX applications require up to thousands of simulation jobs with a
wide range of input simulation parameters. An example of case studies is to model the response of radiometer systems
for in situ marine optics under a variety of ﬁeld measurement conditions. The underpinning for learning the MOX
execution time using MLP is that a subset of the simulation jobs with a reduced number of photons with respect to the
photon population size required in real case studies can be used to set up an MLP model of the overall relationship
between MOX input parameters and expected execution time.
MLP neural networks have been employed for a variety of applications including performance modeling [4],
job scheduling [5], load balancing [6], design space exploration [7, 8], and OC inversion schemes [9, 10, 11]. A
common ﬁnding among the previous studies is that a key to successful MLP applications is empirical tuning of MLP
algorithms in terms of network architecture, data pre-processing, and application-speciﬁc feature selection. Although
the advantages of MLP regression schemes are widely recognized, developing MLP algorithms usually requires a
thorough adaptation of the generic neural computing framework to speciﬁc application cases. The present work hence
investigates the MLP applicability to the MOX execution time modeling. The novelty of the approach presented
here is to divide the MOX execution time into linear and non-linear components, and employ an MLP algorithm to
represent the non-linear dependency of the MOX execution time on the input simulation parameters and performance
factors of computing environments. The linear variability of the MOX execution time is instead expressed analytically
as a function of the number of traced photons.
This section concludes with a description of the components of the MOX code needed to discuss the methods
and results of the study. Section 2 presents an MLP-based performance prediction method for MOX, as well as a
mapping algorithm for scheduling simulation jobs distributed to multiple execution environments. Section 3 assesses
the prediction capability of the proposed methods through experimental results. Finally, Section 4 summarizes the
outcomes of the present work and gives future research directions.
1.1. MOX: A parallel Monte Carlo radiative transfer code for ocean color simulations
The scope of this section is to overview basic MOX features, whereas a detailed description can be found in
the literature [1, 2]. MOX has been developed to quantify uncertainties due to environmental perturbations (e.g.,
light focusing and defocusing eﬀects and sun/sky glint eﬀects, both induced by sea-surface waves) and measurement
protocols (e.g., deployment speed and data acquisition frequency of in-water optical proﬁling systems, and radiometer
tilt) over in situ OC data products in coastal waters, which are optically more complex than open oceans. MOX
permits a theoretical quantiﬁcation of uncertainty budgets through virtual optical proﬁling of simulated light ﬁelds
under controlled conditions, enabling various accuracy assessments of in situ radiometric measurements. Accurate in
situ data products are of critical importance for validation of remote sensing OC observations and vicarious calibration
of satellite sensors [12, 13] in the context of current space missions, including MERIS on-board of ESA’s ENVISAT
platform, as well as MODIS and SeaWiFS on-board of NASA’s Aqua and OrbView-2 satellites, respectively.
MOX computes two-dimensional representations of in-water and above-water light ﬁelds by tracing a number of
photons and tracking their trajectories. Modeled radiometric quantities include downward irradiance (Ed ), upward
irradiance (Eu ), and up-welling radiance (Lu ). Figure 1 shows a MOX simulation domain and a schematic of photon
trajectory. The simulation domain is represented as layers of horizontal photon-collecting bins. Each radiometric
quantity is associated with a matrix whose (i, j) entry accumulates the radiometric value detected by the j-th bin of
the i-th layer. Photon tracing is performed as follows. First, a photon is generated at a random point lying on the top
boundary of the domain. The photon has a travel direction and photon weight. The source of a photon is either the sun
(direct light) or the sky (diﬀuse light), determined as a function of the diﬀuse-to-total ratio. If the photon comes from
the sun, the initial travel direction is set equal to the sun zenith angle and the initial photon weight is unity. Otherwise,
a sky radiance distribution model is used for sampling the initial travel direction and photon weight. The photon
travels straight until it hits the sea surface. Across the air-water interface, the travel direction and photon weight are
updated on the basis of the Snell and Fresnel equations. Each time the photon intersects with a photon-collecting
bin, the photon weight is added to the corresponding matrix entry. In seawater of attenuation c [m−1 ], absorption a
[m−1 ], and volume scattering function (VSF) β(θ) [m−1 sr−1 ], the optical distance l [m] that the photon travels before it

2188

Tamito Kajiyama et al. / Procedia Computer Science 4 (2011) 2186–2195
Origin (light source)
Reflection

Sea surface

Refraction

Collecting bin
Absorption

Scattering
Scattering

Lambertian bottom reflection

Figure 1: Schematic of MOX simulation domain and photon trajectory.

undergoes an absorption or scattering event follows the exponential probability density function (pdf) p(l) = exp(−l),
where l = cτ is the optical distance and τ [m] is the geometrical distance. Values of l are sampled by solving P(l) = q
for l, where P is the cumulative distribution function (CDF) of l (i.e., the integral of p from 0 to l), and q is a random
number uniformly distributed between 0 and 1. The scattering coeﬃcient b (i.e., the integral of β over all directions)
˜ sin(θ). Scattering angles θ are sampled by solving
and scattering phase function β˜ = β/b deﬁne the pdf p(θ) = 2πβ(θ)
P(θ) = q for θ, where P is the CDF of θ (i.e., the integral of p from 0 to θ). At the end of each sampled distance l,
the photon weight is scaled by the single scattering albedo ω = (c − a)/c. The photon tracing ends when the photon
weight becomes smaller than a threshold. Then, a new photon is started until a certain number of photons is traced.
MC radiative transfer simulations belong to the class of embarrassingly parallel problems, since each photon
trajectory can be computed independently of the others. On this basis, two parallel versions of the MOX code were
developed using MPI and OpenMP, respectively. In both versions, parallelization has been done as follows. The total
number of photons Np is evenly divided into Nc CPU cores. Each CPU core computes the trajectories of Np /Nc photons
using a diﬀerent random number seed to generate independent matrices for Ed , Eu , and Lu . For each radiometric
quantity, matrices on the Nc CPU cores are gathered and summed up at the end of the simulation. This matrix
summation is performed by a reduction operation, which involves Nc − 1 matrix additions.
The execution time of the parallel MOX code accounts for three components: initialization, photon tracing, and
matrix summations. The initialization time, spent for reading a simulation conﬁguration ﬁle and preparing zero-ﬁlled
radiometric matrices, is negligible regardless of input simulation parameters. Experimental results have shown that
the photon tracing time is dominant (more than 99 %) when considering the number of photons per CPU core typically
required for application studies, and thus the time spent for matrix summations can also be neglected with respect to
the photon tracing time. On the other hand, the photon tracing time may vary signiﬁcantly depending on various input
simulation parameters, including the seawater inherent optical properties (i.e., attenuation, absorption, and VSF),
illumination conditions (i.e., the sun zenith angle, the diﬀuse-to-total ratio, and the sky radiance model), sea-surface
geometry (expressed by wave heights and wavelengths), dimensions and spatial resolution of the simulation domain,
and photon weight threshold. The photon tracing time also depends on performance factors of execution environments
(such as CPU clock frequency, memory bandwidth, and memory latency). Prediction of the photon tracing time thus
requires a dedicated performance model.
2. Methods
2.1. Performance prediction with MLP
This section presents an MLP scheme for predicting the execution time of the MOX code based on a learningfrom-data approach. The proposed method exploits the fact that there is a linear dependence between the execution
time and photon population size. A typical MOX application study requires a set of MOX simulation jobs with a large
number of photons (up to 1010 ). Therefore, a set of performance history data can be created for training an MLP by
running a fraction of the simulations with a smaller number of photons (e.g., 106 ). The time spent for building the
MLP training dataset is several orders of magnitude smaller than the total amount of computation time required for the

2189

Tamito Kajiyama et al. / Procedia Computer Science 4 (2011) 2186–2195

Table 1: Photon tracing time T t [in seconds; averaged over ﬁve
measurements on Milipeia (see Section 3.2)] as a function of the
number of photons per core Nppc (set to a power of ten). Ratio
r = T t (Nppc )/T t (Nppc /10) shows that T t is proportional to Nppc when
Nppc ≥ 105 .
Nc
4
4
4
4
4
4
4

Np
4 × 101
4 × 102
4 × 103
4 × 104
4 × 105
4 × 106
4 × 107

Nppc
101
102
103
104
105
106
107

Tt
1.984 × 10−1
3.172 × 10−1
1.284 × 100
1.036 × 101
1.011 × 102
9.988 × 102
1.007 × 104

Table 2: Photon tracing time T t [in seconds; averaged over ﬁve measurements on Milipeia (see Section 3.2)] as a function of the number
of CPU cores Nc (set to a power of two). Ratio r = T t (Nc /2)/T t (Nc )
is approximately two for all Nc values, showing that r is independent
of Nc .
Nc
4
8
16
32
64
128

r
—
1.599
4.048
8.07
9.752
9.884
10.08

Np
128 × 106
128 × 106
128 × 106
128 × 106
128 × 106
128 × 106

Nppc
32 × 106
16 × 106
8 × 106
4 × 106
2 × 106
1 × 106

Tt
3.150 × 104
1.612 × 104
7.986 × 103
4.085 × 103
2.079 × 103
1.046 × 103

r
–
1.95
2.02
1.95
1.96
1.99

large-scale MOX simulations. As documented in the next paragraph, MLP results derived from a reduced number of
photons can be linearly extrapolated to obtain execution time estimates of the large-scale MOX simulations, enabling
a prediction-based job-environment mapping (see Section 2.2). The proposed method accounts only for the photon
tracing time that is the dominant component of the MOX execution time.
Let Np and Nc denote the total number of photons and the number of CPU cores, respectively. The time T t spent
for photon tracing in a MOX simulation with a ﬁxed set of input parameters is proportional to the number of photons
per core Nppc = Np /Nc when Nppc is large enough. Table 1 shows the proportionality of T t as a function of Nppc .
These T t measurements were obtained using four CPU cores of the Milipeia cluster (see Section 3.2) with Nppc set to
a power of ten. It can be observed that the ratio r = T t (Nppc )/T t (Nppc /10) gets close to ten, which means that T t is
proportional to Nppc when Nppc ≥ 105 . Table 2 shows another set of T t measurements on Milipeia as a function of Nc
(set to a power of two) with Np = 128 × 106 . Ratio r = T t (Nc /2)/T t (Nc ) is approximately two for all Nc values, which
shows that r is independent of Nc . Therefore, T t can be determined solely by Nppc .
Based on these results, the expected execution time T t of a MOX simulation job in a particular computing environment is expressed as a function of a set of input simulation parameters, the number of CPU cores Nc , and the number
of photons per core Nppc as follows:
1. A training dataset of execution time samples is built by running a set of MOX simulations with Nˆ c CPU cores
Nppc .
and a reduced number of photons per core Nˆ ppc
2. An MLP is built from the training dataset and then used for predicting the time Tˆ t to be spent for tracing Nˆ ppc
photons using Nˆ c CPU cores and the given set of input simulation parameters.
3. Tˆ t is linearly extrapolated to obtain the time prediction T t as
T t = Nˆ c /Nc Nppc /Nˆ ppc Tˆ t .

(1)

The MLP applied for learning Tˆ t is a two-layer feed-forward network trained by the standard back-propagation
procedure [3]. The MLP consists of a layer of d input units, another layer of M hidden units, and one output unit.
The ﬁrst layer of the MLP computes M linear combinations of d MOX input parameter values xi (i = 1, . . . , d) and
transforms them using hyperbolic tangent function to give M hidden-unit output values z j ( j = 1, . . . , M):
⎞
⎛ d
⎟⎟
⎜⎜⎜
(1) ⎟
⎟⎟ ,
(2)
z j = tanh ⎜⎜⎜⎝ w(1)
x
+
b
i
ji
j ⎟
⎠
i=1

(1)
where w(1)
ji and b j are ﬁrst-layer weights and biases, respectively. The z j are then transformed by second-layer
(2)
weights w(2)
to give the output value y (i.e., modeled execution time):
j and bias b
M
(2)
w(2)
j zj + b .

y=
j=1

(3)

2190

Tamito Kajiyama et al. / Procedia Computer Science 4 (2011) 2186–2195

Input
S = {s1 , . . . , sNs }: a set of simulation jobs.
E = {e1 , . . . , eNe }: a set of execution environments.
f (si , e j ): a cost function that gives a scalar value representing the cost of running si on e j .
Output
Q1 , . . . , QNe : environment-wise sets of jobs.
Begin
1: Let U = S be a set of simulation jobs that are not assigned to any execution environment.
2: Let Q j = ∅ be a set of simulation jobs assigned to environment e j , and t j = 0 be the total cost of running the jobs
assigned to e j ( j = 1, . . . , Ne ).
3: Repeat the following steps until U becomes empty:
ˆ
4: Let Eˆ ⊆ E be the set of environments having the smallest t j , and Nˆ e be the number of environments in E.
ˆ
5: For each e j ∈ E, ﬁnd the job si ∈ U with the largest cost function value fi j = f (si , e j ).
6: Among the Nˆ e job-environment pairs, ﬁnd the pair (si , e j ) with the smallest fi j .
7: Assign the job si to the environmnet e j by removing si fom U, adding si to Q j , and adding fi j to t j .
End
Figure 2: The job-environment mapping algorithm.

The training dataset consists of Nt patterns (xn , tn ), where xn = (x1n , . . . , xdn ) is a vector of input values, tn is a target
value (i.e., measured execution time), and n labels each pattern in the training dataset (n = 1, . . . , Nt ). The target
values tn are assumed to be aﬀected by i.i.d. additive Gaussian noise with zero mean and constant variance. The
values of the weights and biases in the MLP are then determined by minimizing a sum-of-squares error function E
deﬁned as
N
1 t
w2i ,
(4)
E=
{y(xn ; w) − tn }2 + α
2 n=1
w ∈w
i

where w is the set of the weights and biases, and α is a non-negative value called the regularization parameter for
reducing the risk of over-ﬁtting during the learning stage [3, 14].
2.2. Job-environment mapping
A job-environment mapping algorithm is hereafter presented based on the proposed performance prediction
method. The objective of the mapping algorithm is to minimize the cost of running a set of MOX simulations distributed to multiple execution environments. Let S = {s1 , . . . , sNs } be a set of Ns simulation jobs, E = {e1 , . . . , eNe }
be a set of Ne execution environments that are exclusively available for the simulations, and f be a cost function that,
given a pair of simulation job si ∈ S and execution environment e j ∈ E, returns a scalar value fi j = f (si , e j ) that
represents the cost of running si using all CPU cores of e j . The cost function f is deﬁned by means of the MOX
performance prediction method, assuming that the cost of running a job depends on the time spent for the execution
of the job. Figure 2 shows the job-environment mapping algorithm that takes S, E, and f as inputs, and yields Ne
environment-wise sets of jobs Q1 , . . . , QNe such that Q1 ∪ . . . ∪ QNe = S and Qi ∩ Q j = ∅ (i
j). The mapping
algorithm employs a greedy strategy to assign jobs to environments in descending order of cost.
3. Results
After discussing feature selection, data pre-processing, and execution environments used for experiments, this section assesses the prediction accuracy of the MLP execution time model as well as the capability of the job-environment
mapping algorithm.
3.1. Feature selection and data pre-processing
Selected input variables for the MLP regression are attenuation c [m−1 ], absorption a [m−1 ], particulate backscatter
fraction Bp = bbp /bb (i.e., the ratio of backscattering by particles to total scattering by particles, used to parameterize the Fournier-Forand (FF) scattering phase function [15]), and sun zenith angle. Among many MOX simulation

Tamito Kajiyama et al. / Procedia Computer Science 4 (2011) 2186–2195

2191

parameters, these four have been identiﬁed as the most important parameters to account for the MOX execution time
when considering diﬀerent water types and illumination conditions. Attenuation and absorption coeﬃcients were determined by a speciﬁc water type model [16] characterized by three classes of optically signiﬁcant constituents found
in natural waters: phytoplankton (quantiﬁed in terms of Chlorophyll-a concentration; referred to as Chl), non-pigment
particulate matter (NPPM), and colored dissolved organic matter (CDOM). To account for a wide range of application
study scenarios involving diﬀerent water types, we consider Chl-dominated open ocean (Case 1) waters, as well as
NPPM- and CDOM-dominated coastal (Case 2) waters. Speciﬁcally, the three water types were parameterized by
Chlorophyll-a concentration ranging from 0.05 to 50 [mg m−3 ], NPPM concentration from 0.05 to 50 [g m−3 ], and
attenuation by CDOM at 440 nm from 0.05 to 5 [m−1 ], respectively. These water type parameters were sampled at
regular intervals in a log10 scale and used to determine attenuation and absorption coeﬃcients at 510 nm. Resulting coeﬃcients fall in the following ranges: 0.117 ≤ c ≤ 7.24 and 0.0404 ≤ a ≤ 0.510 for Chl-dominated Case 1
waters, 0.0662 ≤ c ≤ 27.5 and 0.0380 ≤ a ≤ 0.883 for NPPM-dominated Case 2 waters, and 0.328 ≤ c ≤ 1.83
and 0.0609 ≤ a ≤ 1.57 for CDOM-dominated Case 2 waters. Bp values were sampled from 0.0001 to 0.5 at regular
intervals on a log10 scale, while sun zenith angles were taken from 0 to 80 degrees at regular intervals in a linear scale.
In order to allow for an eﬀective MLP learning, the following data pre-processing options have been considered:
(a) z-score transformation z(xi ) = (xi − μ)/σ, where μ and σ are mean and standard deviation of a set of input
values xi , respectively; (b) log10 transformation; and (c) both log10 and z-score transformations in this order. Zscore transformation is a linear normalization commonly used for MLP applications with the aim of arranging input
and target values to have a similar range of values [3]. The use of log transformation is examined for two reasons.
First, water type parameters for determining attenuation and absorption coeﬃcients, as well as Bp values for deﬁning
FF VSFs are sampled at regular intervals in a log10 scale over several orders of magnitude, in order to ensure an
unbiased prediction performance when considering diﬀerent water types. Second, the MOX execution time contains
an intrinsic variability that is a multiplicative factor of the execution time. For instance, when a MOX simulation takes
a short execution time (e.g., 100 minutes), the deviation from the mean execution time of several runs is small (e.g., 2
minutes). If a MOX simulation requires a longer execution time (e.g., 10 hours), then the deviation also gets larger
(e.g., 12 minutes). By log transformation, this intrinsic variability becomes an additive factor, which is consistent with
the assumption of additive noise (described in Section 2.1).
The dependence of accuracy on MLP architectures was examined with the number of hidden units from 2 to 20.
The MLP model was implemented using Netlab neural network toolbox [14] in GNU Octave. The regularization
parameter α in Eq. (4) was set to 10−2 on the basis of a set of trials.
3.2. Execution environments
Two computing environments were used for collecting execution time measurements of MOX simulations. One is
the Milipeia cluster (University of Coimbra, Portugal) consisting of 130 compute nodes. Each node has two dual-core
AMD Opteron 275 2.2 GHz processors and 8 GB RAM, and runs on CentOS 4.4 (x86 64). The MPI version of the
parallel MOX code was used on Milipeia (compiled with GCC 4.4.0 and MPICH2 1.2.1p1). The other is a desktop PC
(Dell Precision T3500), hereafter referred to as Xeon-PC, that has a quad-core Intel Xeon E5520 2.27 GHz processor
and 6 GB RAM, and runs on Ubuntu Linux 9.10 (x86 64). The OpenMP version of the MOX code was used on
Xeon-PC (compiled with GCC 4.4.1).
3.3. Prediction accuracy of the MLP execution time model
Prediction accuracy of individual simulation jobs was evaluated as a function of (1) water types, MLP architectures, and data pre-processing options, and (2) training dataset population size (i.e., the number of training samples).
3.3.1. Eﬀects of water types, MLP architectures, and pre-processing options
For each of the three water types (Chl-dominated Case 1 waters, NPPM-dominated Case 2 waters, and CDOMdominated Case 2 waters), the prediction accuracy of individual simulation jobs was evaluated as follows. First,
a training dataset Dt containing Nt = 900 execution time measurements was built by running Nt MOX simulations
considering combinations of ten pairs of attenuation and absorption coeﬃcients (determined for diﬀerent water types),
ten Bp values, and nine sun zenith angles, using 32 CPU cores of the Milipeia cluster. Next, Nn = 10 MLPs were
trained using this dataset. To validate the MLP prediction capability, another dataset Dv with Nv = 100 samples

2192

Tamito Kajiyama et al. / Procedia Computer Science 4 (2011) 2186–2195

10
8
6
4

8
6
4
2

0

0

4

5
7
9
12
Number of hidden units

15

20

(a) Chl-dominated Case 1 waters.

log10
z-score
log10 & z-score

12

10

2
3

14

log10
z-score
log10 & z-score

12
Mean relative error [%]

Mean relative error [%]

14

log10
z-score
log10 & z-score

12

Mean relative error [%]

14

10
8
6
4
2

3

4

5
7
9
12
Number of hidden units

15

0

20

(b) NPPM-dominated Case 2 waters.

3

4

5
7
9
12
Number of hidden units

15

20

(c) CDOM-dominated Case 2 waters.

Figure 3: Dependency of prediction accuracy on water types, MLP architectures (expressed by the number of hidden units), and data pre-processing
options. For each of three water types (Chl-dominated Case 1 waters, NPPM-dominated Case 2 waters, and CDOM-dominated Case 2 waters),
prediction accuracy was evaluated as a function of the number of hidden units and data pre-processing option (applying only log10 , only z-score,
or both log10 and z-score transformations).

was built with randomly selected attenuation and absorption coeﬃcients, Bp values, and sun zenith angles. For each
validation sample, relative error ik of modeled execution time was computed as
k
i

yki − ti

= 100 ×

ti

,

(5)

where ti is the true execution time of the i-th sample (i = 1, 2, . . . , Nv ), and yki is the i-th modeled execution time
using the k-th MLP (k = 1, 2, . . . , Nn ). Overall prediction accuracy was evaluated through mean relative error μ and
standard deviation σ , deﬁned respectively as
μ =
and

1 1
Nv Nn

⎛
⎜⎜⎜ 1 1
σ = ⎜⎜⎜⎝
Nv Nn

Nv

Nv

Nn
k
i

(6)

i=1 k=1

Nn
k
i

i=1 k=1

−μ

⎞1/2
⎟⎟
2⎟
⎟⎟⎟
⎠

.

(7)

Figure 3 shows the MLP prediction accuracy as a function of the number of hidden units and data pre-processing
option for each water type. The horizontal axis is the number of hidden units, while the vertical axis is mean relative
error μ with error bar indicating standard deviation σ . Throughout the three water types, the pre-processing option
achieving the best prediction results was that based on both log10 and z-score transformations, which resulted in mean
relative errors μ < 2 % in the cases of ﬁve or more hidden units.
3.3.2. Eﬀects of training dataset population size
Prediction accuracy of individual simulation jobs was also evaluated as a function of the number of training
samples. For each water type, six diﬀerent training dataset population sizes Nt ∈ {n3 | n = 2, . . . , 7} were tested as
follows. For each population size, a training dataset with n3 samples was derived from the training dataset Dt (Nt =
900) in the previous section by selecting those samples taken considering combinations of the following parameters:
(a) n pairs out of the ten pairs of attenuation and absorption coeﬃcients (determined by diﬀerent water type parameter
values); (b) n values out of the ten Bp values; and (c) n angles out of the nine sun zenith angles. For instance,
the derived dataset with 43 training samples in Chl-dominated Case 1 waters was populated with the execution time
measurements taken considering the following input parameters: four pairs of attenuation and absorption coeﬃcients
determined by Chlorophyll-a concentration at 0.05, 0.5, 5, and 50 mg m−3 ; four Bp values 1.0 × 10−4 , 1.7 × 10−3 ,
2.9 × 10−2 , and 5.0 × 10−1 ; and four sun zenith angles at 0, 30, 50, and 80 degrees. Then, Nn = 10 MLPs were trained
with the derived dataset. Data pre-processing was performed by applying both log10 and z-score transformations in

Tamito Kajiyama et al. / Procedia Computer Science 4 (2011) 2186–2195
40

Chl-dominated Case 1 waters
NPPM-dominated Case 2 waters
CDOM-dominated Case 2 waters

35
Mean relative error [%]

2193

30
25
20
15
10
5
0

23

33
43
53
63
Number of training samples

73

Figure 4: Dependency of prediction accuracy on training dataset population size. Data pre-processing was done by applying both log10 and z-score
transformations in this order, and ﬁve hidden units were used to build MLPs.

this order, and the number of hidden units was set to ﬁve. Finally, the MLP prediction accuracy was assessed with
the validation dataset Dv (Nv = 100) described in the previous section. Figure 4 shows the eﬀects of the training
dataset population size on prediction accuracy evaluated through mean relative error [Eq. (6)] and standard deviation
[Eq. (7)]. It is observed that prediction accuracy improvements quickly converge as the population size Nt increases.
In the cases of Nt = 73 , mean relative errors were less than 2 % for all water types.
3.4. Prediction accuracy of the job-environment mapping algorithm
The prediction performance of the job-environment mapping algorithm was assessed considering four execution
environments and a set of simulation jobs. The execution environments, hereafter referred to as Opteron1, Opteron2,
Xeon1, and Xeon2, are clusters of homogeneous compute nodes. Opteron1 and Opteron2 are subclusters of compute
nodes in Milipeia. The total number of CPU cores in Opteron1 and Opteron2 is set equal to 32 and 64, respectively.
Because of the lack of access to a large cluster of Xeon processors, Xeon1 and Xeon2 are supposed to be clusters
whose compute nodes are identical to Xeon-PC (Section 3.2). The total number of CPU cores in Xeon1 and Xeon2 is
set equal to 16 and 32, respectively. The simulations trace 109 photons in Chl-dominated Case 1 waters.
Two training datasets of 900 samples were built using 32 CPU cores of Milipeia and four CPU cores of XeonPC, respectively, with the input simulation parameters described in Section 3.3.1 in both cases. The total number of
photons was set equal to 107 for Milipeia and 2 × 106 for Xeon-PC. The dataset collected using Milipeia was used for
building an MLP to predict job execution time in Opteron1 and Opteron2. The execution time T t of a simulation job for
tracing 109 photons is determined by projecting the output Tˆ t of the MLP using Eq. (1) with Nˆ c = 32, Nˆ ppc = 107 /Nˆ c ,
and Nppc = 109 /Nc , where Nc = 32 for Opteron1 and Nc = 64 for Opteron2. Similarly, the training dataset collected
with Xeon-PC was used for training another MLP to model the execution time of simulation jobs in Xeon1 and Xeon2.
The modeled execution time is then extrapolated using Eq. (1) with Nˆ c = 4, Nˆ ppc = 2 × 106 /Nˆ c , and Nppc = 109 /Nc ,
where Nc = 16 for Xeon1 and Nc = 32 for Xeon2.
3.4.1. Variability in the total execution time of mapped jobs
The variability in the total execution time of mapped jobs was evaluated through 100 trials of mappings between
the four environments and 25 simulation jobs. Two validation datasets (each with execution time measurements of
100 simulation jobs) were build using 32 CPU cores of Milipeia and four CPU cores of Xeon-PC, respectively. The
simulation jobs were deﬁned by 100 ﬁxed sets of randomly selected attenuation and absorption coeﬃcients, Bp values,
and sun zenith angles. The total number of photons Nˆ p was set equal to 107 for Milipeia and 2 × 106 for Xeon-PC. For
each trial, 25 simulation jobs were randomly selected from the ﬁxed set of 100 jobs for validation, and the execution
time for tracing Np = 109 photons was predicted for each job and execution environment. The simulation jobs were
then scheduled on the four environments by the mapping algorithm. Table 3 shows an example of mapping results.
For each mapped job si with execution time measurement tˆi for tracing Nˆ p photons, the relative error of the modeled
execution time was computed using Eq. (5) with ti = tˆi × Np /Nˆ p (instead of measuring ti by tracing Np photons).
Expected error in the modeled total execution time was evaluated by the quadrature sum of relative errors of the

2194

Tamito Kajiyama et al. / Procedia Computer Science 4 (2011) 2186–2195

Table 3: An example of mapping results between 25 simulation jobs and four environments. For each job assigned to an environment, the relative
error of the modeled execution time on the environment was computed using Eq. (5). Overall error in the modeled total time was evaluated by the
quadrature sum of relative errors of the jobs assigned to each environment.
Environment
Xeon1
Xeon2
Opteron1
Opteron2

Number of jobs
4
9
4
8

Total time [sec]
70783.3
77136.1
77487.7
74455.0

Error [%]
1.32
4.45
7.65
4.91

Table 4: Variability of job-environment mapping results evaluated through 100 trials of mappings between 25 simulation jobs and four environments. For each trial, 25 jobs were randomly selected from a ﬁxed set of 100 execution time samples for validation. The variability is quantiﬁed
through the mean μt and standard deviation σt of modeled total execution time, coeﬃcient of variation ct = 100 × σt /μt in percentage, as well as
the mean μe and standard deviation σe of quadrature sums of relative errors.
Environment
Xeon1
Xeon2
Opteron1
Opteron2
All

Total time [sec]
μt
σt
74542.4
5788.5
74688.3
3307.9
75407.1
3027.8
72385.6
3252.3
74255.9
4147.8

ct
7.77
4.43
4.02
4.49
5.59

Error [%]
μe
σe
3.44
1.12
4.80
0.88
4.33
1.38
5.71
1.23
4.57
1.42

assigned jobs for each environment. Table 4 shows the variability in the total execution time of mapped jobs assessed
through the 100 trials of job-environment mappings. The variability was quantiﬁed through the mean μt and standard
deviation σt of modeled total execution time, and coeﬃcient of variation ct = 100 × σt /μt in percentage. The ct values
computed for each environment were less than 8 %, whereas the ct value computed over all environments was less
than 6 %, both of which indicate a considerably small variability. Moreover, the mean μe and standard deviation σe of
quadrature sums of relative errors indicate that the predicted total execution time is expected to be accurate.
3.4.2. Validation of job-environment mapping accuracy
Finally, the accuracy of the total execution time predicted by the job-environment mapping algorithm was validated
as follows. First, the 100 execution time samples of the validation dataset described in the previous section were
divided into four distinct sets of 25 jobs. For each job set, the 25 jobs were mapped to the four execution environments.
Then, the jobs assigned to Opteron1 and Opteron2 were executed on Milipeia using 32 and 64 CPU cores, respectively,
and the actual execution time for tracing 109 photons was measured. Figure 5 (a) and (b) show a scatter plot for
Opteron1 and Opteron2, respectively. Based on the measured and modeled total execution time for each job set, the
mean μ and standard deviation σ were computed over the four job sets (shown in the form μ ± σ in the ﬁgure). The
accuracy of the mapping over all job sets was quantiﬁed by root mean square error (RMSE) of the modeled total
execution time, which was 506.4 seconds in Opteron1 and 703.7 seconds in Opteron2. Remarkably, these results
demonstrate that the accuracy of the proposed performance prediction methods corresponds to a relative error of a
few percent.
4. Conclusion
This paper presented an MLP performance prediction model and complementary job-environment mapping algorithm for the MOX radiative transfer code for ocean color simulations. The MLP model was capable of accurately
predicting the execution time of MOX simulations with mean relative errors less than 2 % over all water types considered. The demonstrated prediction accuracy of the proposed methods constitutes a reliable foundation for scheduling a
set of large-scale MOX simulations distributed to multiple HPC environments. On this basis, our future work concerns
the development of an adaptive MOX simulation framework that encompasses various runtime support tools including
performance monitoring of running simulations, early termination of photon tracing by online accuracy evaluation of
simulated light ﬁelds, and dynamic load balancing through runtime rescheduling of waiting simulation jobs.

2195

Tamito Kajiyama et al. / Procedia Computer Science 4 (2011) 2186–2195
24000

22000

14000
Environment = Opteron1 (32 cores)

Environment = Opteron2 (64 cores)

Number of job sets = 4

Number of job sets = 4

Measured total time = 75686.4 ± 1099.7 [sec]

Measured total time = 72668.8 ± 2837.9 [sec]

Modeled total time = 75845.0 ± 1440.9 [sec]

12000

RMSE of modeled total times = 506.4 [sec]

Modeled total time = 72094.8 ± 3279.1 [sec]
RMSE of modeled total times = 703.7 [sec]

Measured [sec]

Measured [sec]

20000

18000

10000

8000

16000
Job set #1 (4 jobs)
Job set #2 (4 jobs)
Job set #3 (4 jobs)

14000

Job set #1 (8 jobs)
Job set #2 (7 jobs)
Job set #3 (8 jobs)

6000

Job set #4 (4 jobs)
12000
12000

14000

16000

18000
20000
Modeled [sec]

(a) Opteron1

22000

Job set #4 (7 jobs)
24000

4000
4000

6000

8000
10000
Modeled [sec]

12000

14000

(b) Opteron2

Figure 5: Validation of the modeled total execution time in Opteron1 and Opteron2.

Acknowledgment. The authors are grateful to Dr. Giuseppe Zibordi for his continuous and unconditional support from
in situ marine radiometry perspectives. This work was carried out in the context of the Geo-Info project funded by
the Portuguese Foundation for Science and Technology (FCT) of the Ministry of Science, Technology and Higher
Education (MCTES). Numerical experiments for this paper were partly conducted with the Milipeia cluster of the
University of Coimbra, Portugal, through a grant-in-aid project entitled “Large-scale parallel Monte Carlo simulations
for Ocean Colour applications.” Additional funding was granted through the ESA contract number C22576.
References
[1] D. D’Alimonte, G. Zibordi, T. Kajiyama, J. C. Cunha, Monte Carlo code for high spatial resolution ocean color simulations, Applied Optics
49 (26) (2010) 4936–4950.
[2] T. Kajiyama, D. D’Alimonte, J. C. Cunha, G. Zibordi, High-performance ocean color Monte Carlo simulation in the Geo-Info project, in:
Proc. PPAM 2009, Lecture Notes in Computer Science 6068, Springer, 2010, pp. 370–379.
[3] C. M. Bishop, Neural Networks for Pattern Recognition, Oxford University Press, 1995.
[4] E. Ipek, B. R. de Supinski, M. Schulz, S. A. McKee, An approach to performance prediction for parallel applications, in: Proc. Euro-Par
2005, LNCS 3648, Springer, 2005, pp. 196–205.
[5] Y. Park, S. Kim, Y.-H. Lee, Scheduling jobs on parallel machines applying neural network and heuristic rules, Computers & Industrial
Engineering 38 (1) (2000) 189–202.
[6] I. Ahmad, K. Mehrotra, C. K. Mohan, S. Ranka, A. Ghafoor, Performance modeling of load balancing algorithms using neural networks,
Concurrency: Practice and Experience 6 (5) (1994) 393–409.
[7] F. Altiparmak, B. Dengiz, A. A. Bulgak, Buﬀer allocation and performance modeling in asynchronous assembly system operations: An
artiﬁcial neural network metamodeling approach, Applied Soft Computing 7 (2007) 946–956.
[8] E. Ipek, S. A. McKee, R. Caruana, B. R. de Supinski, M. Schulz, Eﬃciently exploring architectural design spaces via predictive modeling,
ACM SIGOPS Operating Systems Review 40 (5) (2006) 195–206.
[9] H. Schiller, R. Doerﬀer, Neural network for emulation of an inverse model operational derivation of Case II water properties from MERIS
data, International Journal of Remote Sensing 20 (9) (1999) 1735–1746.
[10] D. D’Alimonte, G. Zibordi, Phytoplankton determination in an optically complex coastal region using a multilayer perceptron neural network,
IEEE Transactions on Geoscience and Remote Sensing 41 (12) (2003) 2861–2868.
[11] D. D’Alimonte, G. Zibordi, J.-F. Berthon, Determination of CDOM and NPPM absorption coeﬃcient spectra from coastal water remote
sensing reﬂectance, IEEE Transactions on Geoscience and Remote Sensing 42 (8) (2004) 1770–1777.
[12] G. Zibordi, D. D’Alimonte, J.-F. Berthon, An evaluation of depth resolution requirements for optical proﬁling in coastal waters, Journal of
Atmospheric and Oceanic Technology 21 (7) (2004) 1059–1073.
[13] G. Zibordi, J.-F. Berthon, D. D’Alimonte, An evaluation of radiometric products from ﬁxed-depth and continuous in-water proﬁle data from
moderately complex waters, Journal of Atmospheric and Oceanic Technology 26 (2009) 91–106.
[14] I. T. Nabney, Netlab: Algorithms for Pattern Recognition, Springer, 2002.
[15] C. D. Mobley, L. K. Sundman, E. Boss, Phase function eﬀects on oceanic light ﬁelds, Applied Optics 41 (6) (2002) 1035–1050.
[16] E. Leymarie, D. Doxaran, M. Babin, Uncertainties associated to measurements of inherent optical properties in natural waters, Applied Optics
49 (28) (2010) 5415–5436.

