Numerical Experiments on the Solution of the
Inverse Additive Singular Value Problem
G. Flores-Becerra1, 2 , Victor M. Garcia1 , and Antonio M. Vidal1
1

Departamento de Sistemas Inform´
aticos y Computaci´
on,
Universidad Polit´ecnica de Valencia,
Camino de Vera s/n, 46022 Valencia, Espa˜
na,
{gflores, vmgarcia, avidal}@dsic.upv.es
2
Departamento de Sistemas y Computaci´
on,
Instituto Tecnol´
ogico de Puebla,
Av. Tecnol´
ogico 420, Col. Maravillas, C.P. 72220, Puebla, M´exico

Abstract. The work presented here is an experimental study of four
iterative algorithms for solving the Inverse Additive Singular Value Problem (IASVP). The algorithms are analyzed and evaluated with respect
to diﬀerent points of view: memory requirements, convergence, accuracy
and execution time, in order to observe their behaviour with diﬀerent
problem sizes and to identify those capable to solve the problem eﬃciently.

1

Introduction

Inverse problems are of interest for diﬀerent applications in Science and Engineering, such as Geophysics, Computerized Tomography, Simulation of Mechanical Systems, and many more [4] [9] [11] [12]. Two speciﬁc inverse problems are
the Inverse Eigenvalue Problem (IEP) and the Inverse Singular Value Problem
(ISVP). The goal of these problems is to build a matrix with some structure
features, and with eigenvalues (or singular values) previously ﬁxed. In this paper we study a particular case of the ISVP, the Inverse Additive Singular Value
Problem (IASVP), which can be deﬁned as:
Given a set of matrices A0 , A1 , ..., An ∈ m×n (m ≥ n) and a set of real
numbers S ∗ = {S1∗ , S2∗ , ..., Sn∗ }, where S1∗ > S2∗ > ... > Sn∗ , ﬁnd a vector c =
[c1 , c2 , ..., cn ]t ∈ n , such that S ∗ are the singular values of
A(c) = A0 + c1 A1 + ... + cn An .

(1)

There are several well known methods for the IEP. Friedland et al. [8] propose several methods for the IEP based in Newton‘s method, named Method
I, Method II, Method III and Method IV. Chen and Chu [2] proposed the
Lift&Project method, where the IEP is solved as a series of minimum squares
problems.
The methods for the IASVP are usually derived from those for the IEP. In
[3], Chu proposes two methods for the IASVP, one of them (The ”discrete”
V.S. Sunderam et al. (Eds.): ICCS 2005, LNCS 3514, pp. 17–24, 2005.
c Springer-Verlag Berlin Heidelberg 2005

18

G. Flores-Becerra, V.M. Garcia, and A.M. Vidal

method) is obtained by adapting the Method III for the IEP to the IASVP; we
shall name this method as MIII. The convergence of MIII was proved in [5].
The MI [7] and LP [6] methods for the IASVP are other methods derived of
Method I and Lift&Project, respectively. On the other hand, the simplest way
to set up a Newton iteration for resolution of the IASVP is to write directly
the problem as a system of nonlinear equations; we call this a Brute Force (BF)
approach.
Some experimental results of IASVP resolution were given for size of problem
smaller than 6 (by example, m = 5 and n = 4 in [3] and [5]); so, it is necesary
to study the behaviour of the algorithms for greater sizes.
The goal of this work has been to make a fair comparison among some of
these methods with larger problem sizes than those used in previous studies. To
do so, we have implemented FB, MI, MIII and LP methods. The performances
of these algorithms have been analyzed and evaluated, for diﬀerent values of
m and n, regarding memory requirements, convergence, solution accuracy and
execution time, through an experimental study, to determine those of better
characteristics to solve the IASVP.
In section 2 the algorithms are brieﬂy described; the performance of the
algorithms is analyzed in section 3, and, ﬁnally, the conclusions are given in
section 4.

2

Resolution Methods for IASVP

2.1

Brute Force (BF)
∗

Let c be a solution of the IASVP; then, the singular value decomposition of
A(c∗ ), must be
(2)
A(c∗ ) = P diag(S ∗ )Qt
with P ∈ m×n and Q ∈ n×n , orthogonals.
A system of nonlinear equations of the form F (z) = 0, can be built by using
(2) and the orthogonality of P and Q. In this system, the unknown vector would
be z = [Q1,1 , Q1,2 , ..., Qn,n , P1,1 , P1,2 , ..., Pm,n , c1 , c2 , ..., cn , ]t , n unknowns of c,
mn unknowns of P 1 and n2 unknowns of Q, then F (z) = 0 is F (Q, P, c) = 0,
where
F(i−1)n+j (Q, P, c) = (A0 + c1 A1 + · · · + cn An − P S ∗ Qt )i,j ; i = 1 : m; j = 1 : n;
(3)
(4)
Fmn+(i−1)n+j−i+1 (Q, P, c) = (P t P − Im )i,j ; i = 1 : n; j = i : n;
Fmn+n n+1 +(i−1)n+j−i+1 (Q, P, c) = (Qt Q − In )i,j ; i = 1 : n; j = i : n;
2

(5)

A nonlinear system with mn + n2 + n equations and mn + n2 + n unknowns
has been deﬁned from (3), (4) and (5); its solution can be approximated through
Newton’s method, computing a succession of (Q(0) , P (0) , c(0) ), (Q(1) , P (1) , c(1) ),
1

Only mn unknowns of P because we need only min{m, n} singular values.

Numerical Experiments on the Solution of the IASVP

19

..., (Q(k) , P (k) , c(k) ) that approximates to the solution of F (Q, P, c) = 0. Then,
if z (k) is the k-th element of this succession, the (k + 1)-th element is given by
the expression [10]:
z (k+1) = z (k) − J(z (k) )−1 F (z (k) )

(6)

where J(z (k) ) is the Jacobian matrix of F (z) evaluated at z (k) . The Jacobian
matrix of F (Q, P, c) is
⎤
⎡
J1,1 0 0
= ⎣ 0 J2,2 0 ⎦
J(Q, P, c) = ∂F∂zr (z)
t
r=1:mn+n2 +n ; t=1:mn+n2 +n
J3,1 J3,2 J3,3
n+1
2
where J1,1 , J2,2 , J3,1 , J3,2 y J3,3 are blocks of size n n+1
2 × n , n 2 × mn,
2
mn × n , mn × mn and mn × n, respectively, such that

for i = 0 : n − 1; j = 1 : n; row = 1 +
(J1,1 )row,col

i−1
k=0 (n

− k); col = i + (j − 1)n + 1

= 2Qj,i+1

(J1,1 )row+1:row+n−i−1,col = Qtj,i+2:n
(J1,1 )row+a,col+a

= Qj,i+1

for i = 0 : n − 1; j = 1 : m; row = 1 +
(J2,2 )row,col

a = 1 : n − i − 1;
i−1
k=0 (n

− k); col = i + (j − 1)n + 1

= 2Pj,i+1

t
(J2,2 )row+1:row+n−i−1,col = Pj,i+2:n

(J2,2 )row+a,col+a

= Pj,i+1

a = 1 : n − i − 1;

and for i = 1 : m; j = 1 : n; t = 1 : n
(J3,1 )(i−1)n+j,(j−1)n+t = St∗ Pi,t
(J3,2 )(i−1)n+j,(i−1)n+t = St∗ Qj,t
(J3,3 )(i−1)n+j,t

= (At )i,j

This iterative method converges quadratically to the solution of F (Q, P, c) if
the initial guess Q(0) , P (0) and c(0) is close enough to the solution [10].
2.2

MI

As mentioned before, the MI method for the IASVP follows the ideas from
Method I for IEP [8]. First, for any c we can obtain the singular value decomposition of (1) as A(c) = P (c)S(c)Q(c)t . Then, the IASVP can be stated as

20

G. Flores-Becerra, V.M. Garcia, and A.M. Vidal

ﬁnding the solution of the nonlinear system in c: F (c) = [Si (c) − Si∗ ]i=1,n = 0. If
Newton’s method is applied to solve this nonlinear system, the Jacobian matrix
is needed; it can be obtained as in [7]: J = [pti Aj qi ]i,j=1,n , and the Newton’s
iteration (6) is given by the expression [7]
J (k) c(k+1) = b(k)
where b(k) = S ∗ − pi

(k)t

close enough to c∗ [10].
2.3

(k)

A0 qi

i=1,n

(7)

. MI converges quadratically to c∗ if c(0) is

LP

LP is developed in a similar way to Lift&Project in [2]. Let us deﬁne Γ (S ∗ ), the
set of matrices in m×n , (m ≥ n), which can be written in the form P S ∗ Qt ,
where P ∈ m×n and Q ∈ n×n are orthogonal matrices; and let Λ(c) be the set
of matrices that can be expressed as in (1). The goal is to ﬁnd the intersection
of both sets, using distance minimization techniques. The distance between two
matrices U and V is deﬁned as d(U, V ) = U − V F .
LP is an iterative algorithm, with two stages for each iteration:
1) The Lift stage, which consists in, given c(k) (given A(c(k) ) ∈ Λ(c)) ﬁnd
(k)
X ∈ Γ (S ∗ ) such that d(A(c(k) ), X (k) ) = d(A(c(k) ), Γ (S ∗ )). This is achieved by
computing the singular value decomposition of A(c(k) ), P (k) S (k) Q(k)t , and then
computing X (k) = P (k) S ∗ Q(k)t , which turns out to be the element of Γ (S ∗ )
closest to A(c(k) ) [7].
2) The Projection stage consist in, given X (k) ∈ Γ (S ∗ ), ﬁnd c(k+1) (ﬁnd
A(c(k+1) ) ∈ Λ(c)) such that d(X (k) , A(c(k+1) )) = d(X (k) , Λ(c(k+1) )). This is
achieved by ﬁnding c(k+1) as the solution of the nonlinear least squares problem minc(k+1) A(k+1) − P (k) S ∗ Q(k)t 2F . This problem can be solved by equating
the gradient of A(k+1) − P (k) S ∗ Q(k)t 2F to zero and solving the linear sys(k)
tem resulting Atr c(k+1) = btr , where [7] Atr = [tr (Ati Ar )]r,i=1,l and btr =
t
(k)
t
tr Ar X − A0 r=1,l . This LP algorithm converges to a stationary point
in the sense that [7] A(k+1) − X (k+1)
2.4

F

≤ A(k) − X (k)

F.

MIII

The method MIII described here is the method presented in [3] by Chu. This
method ﬁnds the intersection of Γ (S ∗ ) and Λ(c), deﬁned in (2.3), using an iterative Newton-like method. In the iteration k, given X (k) ∈ Γ (S ∗ ), there exist
matrices P (k) and Q(k) such that X (k) = P (k) S ∗ Q(k)t and the tangent vector to
Γ (S ∗ ) which starts from the point X (k) and crosses A(c(k+1) ), can be expressed
as
(8)
X (k) + X (k) L(k) − H (k) X (k) = A(c(k+1) )
where L(k) ∈ n×n and H (k) ∈ m×m are skew-symmetric matrices. Because
X (k) = P (k) S ∗ Q(k)t , (8) can be expressed as
˜ (k) − H
˜ (k) S ∗ = W (k) ,
S∗ + S∗L

(9)

Numerical Experiments on the Solution of the IASVP

21

˜ (k) = Q(k)t L(k) Q(k) , H
˜ (k) = P (k)t H (k) P (k) , W (k) = P (k)t A(c(k+1) )Q(k) .
where L
Equating the diagonal elements of (9), we obtain the linear system (7), that
calculate c(k+1) (A(c(k+1) ) and W (k) ). Equating the oﬀ-diagonal elementos of
˜ (k) [3].
˜ (k) and L
(9), we calculate H
In order to calculate X (k+1) from A(c(k+1) ), e.g. P (k+1) and Q(k+1) , matrix A(c(k+1) ) must be lifted to a point in Γ (S ∗ ). Then X (k+1) is deﬁned as
X (k+1) = P (k+1) S ∗ Q(k+1)t and P (k+1) and Q(k+1) are orthogonal matrices
which can be approximated by P (k+1) ≈ P (k) R and Q(k+1) ≈ Q(k) T , being
−1
and T =
R and T the Cayley transforms: R = I + 12 H (k) I − 12 H (k)
I + 12 L(k)

3

I − 12 L(k)

−1

. See [1] and [3] for details.

Numerical Experiments

In order to observe the behaviour of the algorithms when m, n > 5, they are analyzed experimentally, evaluating memory requirements, convergence, accuracy
and eﬃciency. By each analyzed aspect, we compare the algorithms to determine
those most suitable to solve the IASVP.
The numerical experiments have been carried out taking matrices sizes of
m = n = {5, 10, 15, 20, 25, 30, 50}, using random values for matrices and vectors
of the IASVP and taking diﬀerent initial guesses c(0) .
The algorithms have been implemented in Matlab and executed in a 2.2 GHz
Intel Xeon biprocessor with 4 GBytes of RAM and operating system Linux Red
Hat 8.
Memory Requirements. The storage required for n + 1 matrices of size m × n
(Ai , i = 1 : n), two vectors of size n (S ∗ , c) and the vectors and matrices
required by each algorithm (singular values and vectors, Jacobians matrices,
skew-symmetric matrices, ...), can be seen in Table 1. The FB method has the
greatest memory needs, thereby is hardly feasible to implement for m > 50. The
best algorithms from this point of view are MI and LP.
Table 1. Memory Requirements in KBytes(KB), MBytes(MB), GBytes(GB) and
TBytes(TB)
m=n
BF
MI
LP
MIII

12
1.2
1.2
1.7

4
KB
KB
KB
KB

366
12
12
15

10
KB
KB
KB
KB

30
28 MB
250 KB
250 KB
270 KB

205
1.1
1.1
1.2

50
100
MB
4 GB
MB 8.3 MB
MB 8.3 MB
MB 8.6 MB

500
2 TB
1 GB
1 GB
1 GB

1000
32 TB
8 GB
8 GB
8.1 GB

Convergence. The convergence of the algorithms BF, MI and MIII is really
(0)
sensitive to the initial guess. When the initial guess is taken as ci = c∗i + δ
(i = 1 : n), for small δ such as δ = 0.1 all of them converge; but when δ = 1.0

22

G. Flores-Becerra, V.M. Garcia, and A.M. Vidal

(0)

Fig. 1. Accuracy of Algorithms for diﬀerent values of m, with ci

= c∗i + 0.1 (i = 1 : n)

they only converge for sizes m < 50 and when δ = 10.0 they only converge for
m < 10. Then, as, the initial guess is farther from the solution, these algorithms
converge only for smaller problem sizes. This phenomenon is not suﬀered by LP,
since it has the property A(k+1) − X (k+1) F ≤ A(k) − X (k) F [7].
Solution Accuracy. The accuracy of the solution is measured by obtaining the
relative errors of the obtained singular values with respect to the correct ones
(Figure 1), with δ = 0.1. MIII gives the approximations S ∗ , while LP gives the
worst; however, the LP errors are smaller than 1e − 6.
Time Complexity estimates and Execution Times. The time complexity
estimates are a function of the problem size (m,n) and of the number of iterations to convergence (K); this estimates can be split in two, the cost of the
start-up phase (T (m, n)startU p ) and the cost of each iteration inside the main

Fig. 2. Execution times (seconds) required to reach convergence, with diﬀerent values
of m and δ = 0.1

Numerical Experiments on the Solution of the IASVP

23

Table 2. Number of iterations required for convergence with δ = 0.1
m=n 4
5
10
15
20
25
30
40
50
BF
3
2
3
3
3
3
3
3
3
2
2
2
3
3
3
3
3
3
MI
365 206 561 3398 2629 1998 4048 6304 5303
LP
3
2
2
3
3
3
3
3
4
MIII

loop (T (m, n)loop ) so that the time complexity can be written as: T (m, n, K) =
T (m, n)startU p +KT (m, n)loop . Then, the estimates for each method (for m = n)
can be written as:
T (m, K)BF

≈ (62/3)m3 + O(m2 ) + K{(16/3)m6 + O(m5 )}

T (m, K)M I

≈ (44/3)m3 + O(m) + K{2m4 + (m3 )}

T (m, K)LP

≈ m4 + O(m3 ) + K{(56/3)m3 + O(m2 )}

T (m, K)M III ≈ (56/3)m3 + O(m2 ) + K{2m4 + O(m3 )}
The time estimate of BF shows that it is not an acceptable algorithm. On
the other hand, LP has the smallest time estimate in the iterative part, but it
needs many more iterations for convergence (and execution time) than the other
methods. This can be checked in Table 2, where the number of iterations K is
shown for some experiments, and in Figure 2, where the execution times are
shown.

4

Conclusions

We have implemented and analyzed a set of algorithms for resolution of the
IASVP and we have analized their behaviour for diﬀerent values of m (m > 5).
The study carried out shows that the FB approach must be discarded as a
practical algorithm given its high costs, both in memory (O(m4 )) and execution
time (O(m6 )).
From the memory point of view, MI and LP are those of smaller requirements
of memory; from the execution time point of view, MI is most eﬃcient. The
highest accuracy of the solutions, in most of the tested cases, is reached with
MIII.
However, MI and MIII, like BF, have the drawback of being too sensitive to
the quality of the initial guess. This sensitivity becomes worse as the size of the
problem increases. In contrast, LP does not suﬀer this problem.
Since none of the algorithms possess properties good enough on its own, it
seems desirable to combine several algorithms in a single one. A possible followup of this work is the development of this kind of algorithms; some approaches
can be found in [2] and [7].

24

G. Flores-Becerra, V.M. Garcia, and A.M. Vidal

The problem of the high costs of storage and execution times for problems of
larger size (m > 50) can be tackled by parallelizing the algorithms, so that any
processor involved stores only a part of the data structures and executes part of
the calculations.

Acknowledgement
This work has been supported by Spanish MCYT and FEDER under Grant
TIC2003-08238-C02-02 and DGIT-SUPERA-ANUIES (M´exico).

References
1. Chan, R., Bai, Z., Morini, B.: On the Convergence Rate of a Newton-Like Method
for Inverse Eigenvalue and Inverse Singular Value Problems. Int. J. Appl. Math.,
Vol. 13 (2003) 59-69
2. Chen, X., Chu, M.T.: On the Least Squares Solution of Inverse Eigenvalue Problems. SIAM, Journal on Numerical Analysis, Vol. 33, No. 6 (1996) 2417-2430
3. Chu, M.T.: Numerical Methods for Inverse Singular Value Problems. SIAM, Journal Numerical Analysis, Vol. 29, No. 3 (1992) 885-903
4. Chu, M.T.: Inverse Eigenvalue Problems. SIAM, Review, Vol. 40 (1998)
5. Bai, Z., Morini, B., Xu, S.: On the Local Convergence of an Iterative Approach for
Inverse Singular Value Problems. Submitted
6. Flores G., Vidal A.: Paralelizaci´
on del M´etodo de Elevaci´
on y Proyecci´
on para la
Resoluci´
on del Problema Inverso de Valores Singulares. Primer Congreso Internacional de Computaci´
on Paralela, Distribuida y Aplicaciones, (2003).
7. Flores, G., Vidal A.: Parallel Global and Local Convergent Algorithms for Solving
the Inverse Additive Singular Value Problem. Submitted
8. Friedland, S., Nocedal, J., Overton, M.L.: The Formulation and Analysis of Numerical Methods for Inverse Eigenvalue Problems. SIAM, Journal on Numerical
Analysis, Vol. 24, No. 3 (1987) 634-667
9. Groetsch, C.W.: Inverse Problems. Activities for Undergraduates. The mathematical association of America (1999)
10. Kelley, C.: Iterative Methods for Linear and Nonlinear Equations. SIAM (1995)
11. Neittaanmki, P., Rudnicki, M., Savini, A.: Inverse Problems and Optimal Design
in Electricity and Magnetism. Oxford: Clarendon Press (1996)
12. Sun, N.: Inverse Problems in Groundwater Modeling. Kluwer Academic (1994)

