Available online at www.sciencedirect.com

ScienceDirect
Procedia Computer Science 108C (2017) 838–847

International Conference on Computational Science, ICCS 2017, 12-14 June 2017,
Zurich, Switzerland

A wrapper around parallel MUMPS solver
to reduce
its memory
and
execution
time
A wrapper
around usage
parallel
MUMPS
solver
for finite
element usage
methodand
computations
to reduce
its memory
execution time
for finite element method computations
Maciej Paszyński1 and Antônio Tadeu Azevedo Gomes2
1

AGH University, Kraków, Poland

1
2
Maciej
Paszyński
AntônioComputing,
Tadeu Azevedo
National
Laboratoryand
for Scientific
Petropolis, Gomes
Brasil
2

1
maciej.paszynski@agh.edu.pl,
atagomes74@gmail.com
AGH University, Kraków,
Poland
2
National Laboratory for Scientific Computing, Petropolis, Brasil
maciej.paszynski@agh.edu.pl, atagomes74@gmail.com

Abstract
In this paper, we present a wrapper around MUMPS solver, called Hierarchical Solver Wrapper
(HSW),
Abstractthat is tailored to domain decomposition-based parallel finite element method computations
In distributed
this paper, memory
we present
a wrapper
around
MUMPS
solver,
calledMUMPS
Hierarchical
Solver entries
Wrapper
on
systems.
It offers
the same
interface
as parallel
with matrix
in
(HSW), thatformat
is tailored
to domain
parallel multiple
finite element
methodThe
computations
coordinate
provided
in a decomposition-based
distributed fashion among
processors.
algorithm
on distributed by
memory
systems.utilizes
It offersmultiple
the same
interface instances
as parallelofMUMPS
with
matrix
in
implemented
the wrapper
sequential
MUMPS
solver
to entries
compute
coordinate
format provided
in a distributed
fashion among
multiple
processors.
TheLU
algorithm
Schur
complements
over subdomains.
Next, it deallocates
sequential
MUMPS
solvers and
factors,
and
it calls the
MUMPS
solver
feededsequential
with the Schur
complements,
implemented
by parallel
the wrapper
utilizes
multiple
instances
of MUMPSstored
solverin todistributed
compute
manner.
In the backward
substitution stage
local LUMUMPS
factors before
local
Schur complements
over subdomains.
Next,ititrecomputes
deallocates the
sequential
solverssolving
and LUthe
factors,
and it callsThe
thewrapper
parallel has
MUMPS
solverwith
feeded
with the Schurisogeometric
complements,analysis
stored computations,
in distributed
problems.
been tested
three-dimensional
manner.
In theitbackward
substitution
stageusage
it recomputes
the local LU
factors
before solving
local
and
we show
reduces both
the memory
and the execution
time,
in comparison
withthe
a single
problems.
The wrapper
parallel
MUMPS
call. has been tested with three-dimensional isogeometric analysis computations,
and we show it reduces both the memory usage and the execution time, in comparison with a single
© 2017 The Authors. Published by Elsevier B.V.
Keywords:
parallel call.
direct solver; memory minimization; wrapper
parallel MUMPS
Peer-review
under responsibility of the scientific committee of the International Conference on Computational Science
Keywords: parallel direct solver; memory minimization; wrapper

1 Introduction
multi-frontal solver [1,2] is a popular algorithm for factorization of linear systems resulting
1 The
Introduction

from mesh-based computations with the finite element method. The MUMPS solver [3] is a popular
The multi-frontal
solver
is a popular algorithm
for factorization
of linearsolver
systems
resulting
parallel
implementation
of [1,2]
the multi-frontal
solver algorithm.
The MUMPS
works
with
from mesh-based
computations
with [4],
the and
finiteit element
method.
MUMPS
solver [3]
popular
symmetric
and asymmetric
matrices
is optimized
usingThe
different
techniques
[5].isIta operates
in
in-core
and out-of-core
environments
The algorithm.
MUMPS The
solverMUMPS
employssolver
several
ordering
parallel
implementation
of the
multi-frontal[6].
solver
works
with
algorithms.
These
include nested
dissections
available through
METIStechniques
interface [8],
[9],
symmetric and
asymmetric
matrices
[4], and it[7]
is optimized
using different
[5]. PORD
It operates
in in-coredegree
and out-of-core
environments
[6]. Theversions
MUMPS
solver
employs
several
minimum
[10] or several
of its approximate
[11,12],
or the
orderings
basedordering
on the
algorithms.
include
nested
dissections
METISmatrix.
interface [8], PORD [9],
topology
of These
the adaptive
grid
[13] to
minimize [7]
the available
bandwidththrough
of the global
minimum degree [10] or several of its approximate versions [11,12], or the orderings based on the
topology of the adaptive grid [13] to minimize the bandwidth of the global matrix.
1877-0509 © 2017 The Authors. Published by Elsevier B.V.
Peer-review under responsibility of the scientific committee of the International Conference on Computational Science
10.1016/j.procs.2017.05.096

	

Maciej Paszyński et al. / Procedia Computer Science 108C (2017) 838–847

The algorithm implemented by the HSW wrapper is based on the idea of sub-structuring originally
described in [14]. The computational mesh is partitioned into subdomains, where we compute local
Schur complements by using sequential MUMPS solvers. Later, we translate the Schur complements
into lists of non-zero entries and deallocate the local systems. We call the parallel MUMPS solver to
solve the skeleton problem, and finally, we restore the local systems by enforcing Dirichlet boundary
conditions on the interfaces and recomputing the LU factors for the interior of the subdomains. In
case of symmetric systems, we use LLT factors instead. This simple strategy of deallocating local
systems and restoring them by fast recomputations with Dirichlet boundary conditions allows
reducing not only the memory usage but also the execution times. This observation comes along with
the one already presented in [15], where the authors showed that just performing static condensations
over subdomains before calling the parallel MUMPS solver reduces its execution time.
We further investigate this idea, incorporating the aforementioned strategy to reduce the memory
usage of the solver, which is important when dealing with large systems, encountered especially
during higher order finite element method computations. Our exemplary numerical experiments are
concerned with the isogeometric analysis (IGA-FEM), where we use the higher order B-spline basis
functions over three-dimensional grids [16,17]. Nevertheless, the wrapper is not restricted to IGAFEM computations, and can be used in other domain decomposition-based problems.

2 Strategy to minimize the memory usage
In the proposed strategy, the computational domain is subdivided into subdomains by the user, as
it is illustrated in Figure 1. The user aggregates local element matrices into a single subdomain matrix.
It is done by browsing elements one by one, and aggregating element local matrices into one
subdomain local matrix. The resulting subdomain local matrices generated on each subdomain are of
the form
A i Bi   xi   bi 
(1)
C A i  x i   b i 
s   s   s 
 i
with subscript i referring to entries associated with the interior of the subdomain, and s to entries
associated with the boundary of the subdomain. The panel (a) of Figure 2 illustrates the subdomain
local matrices pertaining to each of the subdomains in Figure 1.

Fig. 1. Example of domain decomposition

The subdomain local matrices have to be stored as lists of non-zero entries in coordinate format.
Additionally, the user must denote the entries whose rows are located in the interface between
subdomains. This is done by setting boolean flags in an additional list INTERFACE_loc_in:
integer, dimension(:), pointer :: IRN_loc_in
integer, dimension(:), pointer :: JCN_loc_in
(2)
double precision, dimension(:), pointer :: A_loc_in

839

Maciej Paszyński et al. / Procedia Computer Science 108C (2017) 838–847

840	

logical, dimension(:), pointer :: INTERFACE_loc_in
The lists of non-zero entries are submitted to several sequential, local MUMPS solvers running
concurrently and independently of each other, one for each subdomain. The fully assembled degrees
of freedom, related to the interior and external boundary of the subdomains (as denoted by the user),
are eliminated by executing partial forward elimination on each local MUMPS solver.
*

 bi* 
 A i B i   x i   b i  U i B i   x i  
(3)


 i* 


*
C A i  x i   b i   
i
i
 0 A s  x s  
s   s   s 
 i

b s 
This is illustrated on the panel (b) in Figure 2. In the next step, the Schur complement
*

contributions A s i , as returned by the local MUMPS solvers, are transformed into lists of non-zero
entries stored internally by the wrapper.
integer, dimension(:), pointer :: SCHUR_IRN_loc
integer, dimension(:), pointer :: SCHUR_JCN_loc
double precision, dimension(:), pointer :: SCHUR_A_loc

(4)

The local MUMPS solvers are then deallocated to save memory. At this point, we keep the
original lists of non-zero entries (2) and the Schur complements compressed into lists of non-zero
entries (4). Then, the wrapper calls a single, parallel instance of the MUMPS solver to compute the
global skeleton problem. Figure 3 depicts the processor-mapping relationship between the several
independent computations of the local MUMPS solvers and the single parallel computation of the
global MUMPS solver.
The global skeleton problem, which is stored in a distributed manner by the parallel instance of
the MUMPS solver, is of the form
ˆ x  bˆ
A
(5)
s

p

ˆ   P A i*P T
A
i s
i

(6)

*
bˆ   Pi b s i PiT

(7)

i 1
p

i 1

where Pi stands for the permutation matrices, transforming subdomain’s local ordering of degrees
of freedom located on the interface into the global ordering on the interface
i
i
i
i
Pi : M ninterface
 ninterface
 M ninterface
 ninterface
(8)
This is illustrated on the panel (c) in Figure 2. Notice that our skeleton problem (5-7)
corresponds to the so-called Schur complement
ˆ  A  C T A 1 B
A
(9)



s



I

I

bˆ  b s  CI A I b I
of the global system
B1 
A1

...
... 


Ap Bp


C1 ... C p A s 
T





I

1

(10)
 x1   b1 
 ...   ... 
   
  
x p  b p 

xs 
 
b s 


(11)

	

Maciej Paszyński et al. / Procedia Computer Science 108C (2017) 838–847

841

Fig. 2. Illustration on the execution of the HSW wrapper algorithm on the Fichera model problem

Fig. 3. Several local sequential MUMPS solvers and one parallel MUMPS solver

expressed in the compressed form
 A I B I  x I  b I 
C T A  x   b 
s  s  s
 I

(12)

In other words, the Schur complement matrix (6) and the corresponding right-hand-side vector
(7) can be obtained by executing partial forward eliminations on subdomains and summing up
*

*

renumbered sub-matrices A s i and sub-vectors b s i , which is expressed in (6) and (7).
In the next step, the global interface problem (5) is solved using the global parallel MUMPS
solver. This is illustrated on the panel (d) in Figure 2.

842	

Maciej Paszyński et al. / Procedia Computer Science 108C (2017) 838–847

The interface problem solution is broadcasted into subdomains. Now, we want to substitute the
local solution to the right-hand side with the Schur complement sub-matrix replaced by the identity
matrix.
*
*
*
U i B i *   x i  

 bi

 U i B i   x i  
 bi 
(13)















*
*
i
i


1
T
i
i
x
x

P
x
P
0
1   s   i
s i 

 
b s 
 0 A s   s  
Nevertheless, this is not possible at this point since we have already de-allocated the local
systems. To be able to perform the backward substitutions, the following system of equations must be
reconstructed, where we replace the interface nodes by Dirichlet boundary conditions:
U11 U12   x1  bˆ 1 
(14)
  
 0
I  x 2  xˆ 2 

We can regenerate U11 , U12 and b̂1 by executing the full forward elimination on the original
matrix, with A 22 sub-matrix replaced by the identity matrix, and the bottom part of the right-handside b 2 replaced by the solution x̂ 2 obtained from the father node:

bˆ 1 

A11 A12   x1  b1  U11 U12   x1  
(15)
  

 x   xˆ   

x
0
I
0
I

 2  2 
 2 
xˆ 2 

This corresponds to the definition of the Dirichlet boundary conditions over the local interface.
Note that this full forward elimination is much faster than the computations of the Schur complement,
because the new system is sparser. Namely, it takes around 10 percent of the original factorization
time.
Finally, the backward substitution is executed on (15), and the local contribution to the solution
x1 is obtained. This strategy allows us to reduce memory usage since the local systems are not stored
in memory but recomputed.

3 Testing and calling the wrapper
In this section, we show how to call the HSW wrapper from FORTRAN. We have also
implemented an analogous C language interface. The full wrapper code can be obtained from
http://home.agh.edu.pl/paszynsk/files/HSW
We focus on a simple one-dimensional heat transfer problem with Dirichlet boundary conditions,
solved with the finite element method with four elements using linear basis functions at vertices and
second order polynomials at the interiors of elements. The global matrix is the following:

	

Maciej Paszyński et al. / Procedia Computer Science 108C (2017) 838–847

0
0
0
0
0
0
1 0
0 1 / 3 0
0
0
0
0
0

0
0
0
0
0 1  2 1
0 0
0 1/ 3 0
0
0
0

0 0
0
1 2 1
0
0

0
0
0 1/ 3 0
0
0 0
0 0
0
0
0
1 2 1

0
0
0
0
0 1/ 3
0 0
0 0
0
0
0
0
0
0

0 u0  0
0 u1  0
   
0 u2  0
0 u3  0
    
0 u4   0

0 u5  0
   
0 u6  0

0 u7  0
   
1 u8  1

843

(16)

The global matrix is partitioned according to the integration over the four finite elements into
four local systems, which are submitted to the wrapper:

1 0
0 1 / 3

0 1
 1 1
 0 1/ 3

1
 0

0  u2  0
0  u0  0  1 1
   
   

0 u1   0, 0 1 / 3 0  u3   0,



  
  
 1 
1  1 
u4  0
u2  0  0
0  u4  0  1 1 0 u6  0
   
   
0  u5   0,  0 1 / 3 0 u7   0



 1 u6  0  0
0 1 u8  1

The FORTRAN code for calling the wrapper with the example above is listed below:
program main
use test_HierarchicalSolver
implicit none
INTERFACE
subroutine run_HMS_FORTRAN(N_in,NZ_in,NZ_loc_in,
.
IRN_loc_in,JCN_loc_in,A_loc_in,RHS_in)
integer :: N_in, NZ_in, NZ_loc_in
integer, dimension(:), pointer :: IRN_loc_in
integer, dimension(:), pointer :: JCN_loc_in
double precision, dimension(:), pointer :: A_loc_in
logical, dimension(:), pointer :: INTERFACE_loc_in
double precision, dimension(:), pointer :: RHS_in
end subroutine run_HMS_FORTRAN
END INTERFACE
include "mpif.h"
integer :: N_in, NZ_in, NZ_loc_in
integer, dimension(:), pointer :: IRN_loc_in
integer, dimension(:), pointer :: JCN_loc_in
double precision, dimension(:), pointer :: A_loc_in
logical, dimension(:), pointer :: INTERFACE_loc_in
double precision, dimension(:), pointer :: RHS_in
integer :: i1,i2
call mpi_init(i1)
call mpi_comm_size(MPI_COMM_WORLD,nrproc,i1)

(17)

844	

Maciej Paszyński et al. / Procedia Computer Science 108C (2017) 838–847

call mpi_comm_rank(MPI_COMM_WORLD,myrank,i2)
N_in=9;
NZ_in=20;
NZ_loc_in=5
allocate(IRN_loc_in(NZ_loc_in))
allocate(JCN_loc_in(NZ_loc_in))
allocate(INTERFACE_loc_in(NZ_loc_in))
allocate(A_loc_in(NZ_loc_in))
if(myrank.eq.0)then
IRN_loc_in = (/1,2,3,1,3/)
JCN_loc_in = (/1,2,3,3,1/)
A_loc_in = (/1,1,1,0,-1/)
INTERFACE_loc_in =
(/.true.,.false.,.true.,.true.,.true./)
allocate(RHS_in(9))
RHS_in = (/0,0,0,0,0,0,0,0,1/)
else if(myrank.eq.1)then
IRN_loc_in = (/3,4,5,3,5/)
JCN_loc_in = (/3,4,5,5,3/)
A_loc_in = (/1.d0,1.0/3.d0,1.d0,-1.d0,-1.d0/)
INTERFACE_loc_in =
(/.true.,.false.,.true.,.true.,.true./)
else if(myrank.eq.2)then
IRN_loc_in = (/5,6,7,5,7/)
JCN_loc_in = (/5,6,7,7,5/)
A_loc_in = (/1.d0,1.d0/3.d0,1.d0,-1.d0,-1.d0/)
INTERFACE_loc_in =
(/.true.,.false.,.true.,.true.,.true./)
else if(myrank.eq.3)then
IRN_loc_in = (/7,8,9,7,9/)
JCN_loc_in = (/7,8,9,9,7/)
A_loc_in = (/1.d0,1.d0/3.d0,1.d0,-1.d0,0.d0/)
INTERFACE_loc_in =
(/.true.,.false.,.true.,.true.,.true./)
endif
call run_HMS_FORTRAN
. (N_in,NZ_in,NZ_loc_in,
. IRN_loc_in,JCN_loc_in,A_loc_in,INTERFACE_loc_in,RHS_in)
call mpi_finalize(i1)
end
The wrapper is partitioned into several routines, which are called one after another. It first creates
local and global MPI communicators in routine phaseI_1_create_communicators. Later, it
instantiates local MUMPS solvers over each subdomain in phaseI_2_create_local_mumps,
and computes global to local mappings in phaseI_3_translate_global_to_local,
necessary to translate input provided with global numbering into local numbering supported by the
local MUMPS solvers. Next, in phaseI_3prim_distribute_rhs it distributes the right-handside provided according to MUMPS standard as a single vector on the host processor. It inputs the
vector
of
interface
variables
to
form
the
Schur
complement
in
phaseI_4_submit_Schur_to_mumps. The local phase is finished with the computation of the
local Schur complement matrices in phaseI_5_compute_Schur. The Schur complement

	

Maciej Paszyński et al. / Procedia Computer Science 108C (2017) 838–847

matrices are transformed into the lists of non-zero entries in phaseI_6_get_Schur. The local
MUMPS instances are shut down, and the global MUMPS instance is created in
phaseI_7_create_global_mumps. The Schur complements are provided to the global solver
in phaseI_8_submit_Schur_to_global_mumps, and the skeleton problem is computed in
phaseI_9_global_solution.
The
solution
is
scattered
into
processors
in
phaseI_10_scatter_solution. The global MUMPS instance is deallocated, and the local
systems are restored by imposing Dirichlet boundary conditions and calling partial factorization in
phaseI_11_initialize_mumps_for_local_solve
and
phaseI_12_submit_problem_to_mumps. Finally, the local solution is computed in
phaseI_13_compute and collected at host processor according to the standard MUMPS interface
in phaseI_14_return_solution. The full description of the code that chains these routines is
not possible here, since it has around 1,500 lines. However, it can be downloaded from
http://home.agh.edu.pl/paszynsk/files/HSW/HSW.F.
This decomposition of the wrapper into several routines enables for a simple implementation of
unit tests, which has been done for the provided one-dimensional heat transfer example in
http://home.agh.edu.pl/paszynsk/files/HSW/test_HSW.F.

4 Numerical results
We have tested the presented wrapper on the uniform three-dimensional grids with higher order
B-spline basis functions utilized in the isogeometric finite element method computations [17,18]. We
compare the wrapper against the call to parallel MUMPS with distributed entries. In all the tests we
have utilized eight nodes of ATARI Linux cluster at Department of Computer Science, AGH
University:
mpirun -np 8
-host node3,node4,node5,node6,node7,node8,node9,node10 ./code
The numerical tests are summarized in Table 1.
The first test concerned a mesh with 64x64x64 finite elements with quadratic B-splines. The
global number of unknowns is equal to N=274,625 and the total number of non-zero entries
NZ=113,688,414. The call to parallel MUMPS with distributed lists of non-zero entries resulted in
1,606s of computations with 5,137MB per processor. The call to our HSW wrapper gives 990s of
total execution time. In the local factorization phase, it used 4,385MB per processor, 216MB per
processor in the global phase, and a maximum of 904MB per process in the backward substitution
phase.
The second test concerned a mesh with 48x48x48 finite elements with cubic B-splines. The
global number of unknowns is equal to N=117,649 and the total number of non-zero entries
NZ=292,464,640. The call to parallel MUMPS with distributed lists of non-zero entries resulted in
898s of computations with 2,477MB per processor. The call to our HSW wrapper gives 588s of total
execution time. In the local factorization phase, it used 2,332MB per processor, 197MB per processor
in the global phase, and a maximum of 1,055MB per process in the backward substitution phase.
The third test concerned a mesh with 36x36x36 finite elements with quartic B-splines. The global
number of unknowns is equal to N=50,653 and the total number of non-zero entries NZ=
542,752,875. The call to parallel MUMPS with distributed lists of non-zero entries resulted in 678s of
computations with 3,258 MB per processor. The call to our HSW wrapper gives 397s of total
execution time. In the local factorization phase, it used 2,158MB per processor, 770MB per process in
the global phase, and a maximum of 333MB per process in the backward substitution phase.

845

Maciej Paszyński et al. / Procedia Computer Science 108C (2017) 838–847

846	

The savings in terms of memory were expected, however the savings in terms of the execution
time were unexpected here, and this may be related to the fact that exchanging compressed Schur
complements between sub-domains may be more efficient than factorization of the global matrix
stored in a distributed manner. Nevertheless, we would need to further study the details about the
internal implementation of the MUMPS solver to understand the reason for this behavior.
ID

Mesh size

B-spline
order

N

NZ

#1
#2
#3

64x64x64
48x48x48
36x36x36

2
3
4

274625
117649
50653

113688414
292464640
542752875

Table 1: Numerical results

MUMPS
Time Memory
[s]
[MB]
1606
898
678

5137
2477
3528

HSW
Memory
(local,global, b.s.)
[s]
[MB]
990
4385,216,904
588
2332,197,1055
397
2158,770,333

Time

5 Conclusions
In this paper, we presented a simple wrapper around parallel MUMPS solver with distributed
entries provided as lists of non-zero entries. The wrapper allows reducing the memory usage and the
execution time of the solver in the case of domain decomposition-based finite element method
computations. The interface to the wrapper is similar to MUMPS, only requiring as additional
information the identification of those entries located on the interface between subdomains. From the
presented numerical experiments performed for three-dimensional regular grids with B-spline basis
functions we demonstrate that the wrapper allows for a reduction of up to 50% in memory usage per
processor and of up to 70% of total execution time.
Future work will involve a development of the multi-level hierarchical substructuring solver,
where we create a binary tree of communicators, and generalize the proposed strategy into multilevels. We also plan to construct a hybrid direct-iterative solver, where on the second level the Schur
complement matrices will be submitted to an iterative solver. We also intend to apply HSW to other
domain decomposition-based methods than the isogeometric analysis: in particular, we are
progressing work on the integration of HSW with a solver that implements the family of Multiscale
Hybrid Mixed (MHM) methods [19,20] Finally, we may also consider incorporating the HSW solver
into PETSc environment [21,22]. Future work will also involve theoretical derivation of the upper
bounds for memory and execution time gain.

6 Acknowledgement
This work was supported by the National Laboratory for Scientific Computing (LNCC) through
the PCI Program from the Brazilian Ministry of Science, Technology and Innovation (MCTIC). This
work was supported by National Science Centre, Poland, grant no. 2015/17/B/ST6/01867. This work
has been partially funded by the European Union’s Horizon 2020 Program (2014-2020) and by
MCTIC through Rede Nacional de Pesquisa (RNP) under the HPC4e project (http://www.hpc4e.eu),
grant no. 689772.

	

Maciej Paszyński et al. / Procedia Computer Science 108C (2017) 838–847

References
[1] I. S. Duff, J. K. Reid, The multifrontal solution of indefinite sparse symmetric linear systems. ACM
Transactions on Mathematical Software, 9 (1983) 302-325.
[2] I. S. DSuff, J. K. Reid, The multifrontal solution of unsymmetric sets of linear systems. SIAM Journal on
Scientific and Statistical Computing, 5 (1984) 633-641.
[3] MUlti-frontal Massively Parallel Sparse direct solver MUMPS http://mumps.enseeiht.fr/
[4] P.R. Amestoy, I.S. Duff and J.-Y. L'Excellent, Multifrontal parallel distributed symmetric and
unsymmetric solvers, Computer Methods in Applied Mechanics and Engineering, 184 (2000) 501-520.
[5] P. R. Amestoy, C. Ashcraft, O. Boiteau, A. Buttari, J.-Y. L'Excellent and Cl. Weisbecker, Improving
multifrontal methods by means of block low-rank representations SIAM Journal on Scientific Computing, 37(3)
(2015) A1451-A1474
[6] P. Amestoy, I.S. Duff, Y. Robert, F.H. Rouet and B. Ucar, On computing inverse entries of a sparse matrix
in an out-of-core environment, SIAM Journal on Scientific Computing 34(4) (2012) 1975-1999
[7] G. Karypis, V. Kumar, A fast and high quality multilevel scheme for partitioning irregular graphs, SIAM
Journal of Scientiffic Computing, 20, 1 (1998) 359-392.
[8] G. Karypis, V. Kumar, METIS - Unstructured Graph Partitioning and Sparse Matrix Ordering System,
Version 2.0, Technical Report (1995)
[9] J. Schulze, Towards a tighter coupling of bottom-up and top-down sparse matrix ordering methods, BIT,
41, 4 (2001) 800.
[10] P. Heggernes, S.C. Eisenstat, G. Kumfert, A. Pothen, The Computational Complexity of the Minimum
Degree Algorithm, ICASE Report No. 2001-42, (2001).
[11] P. R. Amestoy, T. A. Davis, I. S. Du, An Approximate Minimum Degree Ordering Algorithm, SIAM
Journal of Matrix Analysis & Application, 17, 4 (1996) 886-905.
[12] G.W. Flake, R.E. Tarjan, K. Tsioutsiouliklis, Graph clustering and minimum cut trees, Internet
Mathematics 1 (2003), 385-408.
[13] A. Paszyńska, Volume & neighbors algorithm for finding elimination trees for three-dimensional hadaptive grids, Computer Methods in Applied Mechanics and Engineering 68(10) (2014) 1467-1478
[14] M. Paszyński, Fast solver for mesh-based computations, Taylor & Francis, CRC Press 2016
[15] Walsh T., Demkowicz L., A Parallel Multifrontal Solver for hp-Adaptive Finite Elements. TICAM
Report 99-01, 1999
[16] Geng P., Oden T. J., van de Geijn R. A., A Parallel Multifrontal Algorithm and Its Implementation.
Computer Methods in Applied Mechanics and Engineering 149 (2006) 289-301
[17] T .J. R. Hughes, J. A. Cottrell, Y. Bazilevs, Isogeometric analysis: CAD, finite elements, NURBS, exact
geometry and mesh refinement, Computer methods in applied mechanics and engineering 194(39) (2005) 41354195.
[18] J. A. Cottrell, T. J. R. Hughes, Y. Bazilevs, Isogeometric Analysis: Toward Unfication of CAD and FEA
John Wiley and Sons, (2009)
[19] C. Harder, D. Paredes, and F. Valentin, A family of multiscale hybrid-mixed finite element methods for
the darcy equation with rough coefficients, Journal of Computational Physics, 245 (2013) 107 – 130
[20] C. Harder, D. Paredes, and F. Valentin, On a multiscale hybrid-mixed method for advective-reactive
dominated problems with heterogeneous coefficients, Multiscale Modeling & Simulation, 13(2) (2015) 491–518 .
[21] S. Balay, S. Abhyankar, M. F. Adams, J. Brown, P. Brune, K. Buschelman, V. Eijkhout, W. D. Gropp, D.
Kaushik, M. G. Knepley, L. Curfman McInnes, K. Rupp, B. F. Smith, H. Zhang, PETSc Web Page,
http://www.mcs.anl.gov/petsc (2014)
[22] S. Balay, S. Abhyankar, M. F. Adams, J. Brown, P. Brune, K. Buschelman, V. Eijkhout, W. D. Gropp, D.
Kaushik, M. G. Knepley, L. Curfman McInnes, K. Rupp, B. F. Smith, H. Zhang, PETSc User Manual, Argonne
National Laboratory ANL-95/11 - Revision 3.4 (2013)

847

