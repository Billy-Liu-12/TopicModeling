Available online at www.sciencedirect.com

Procedia Computer Science 4 (2011) 2068–2075

International Conference on Computational Science, ICCS 2011

Developing an Automated Mechanism for Cluster Computing in
Computerized Classroom
Shuen-Tai Wang1, Chin-Hung Li1, Hsi-Ya Chang1, Ying-Chuan Chen1, Ching-Hsien Hsu2
2

1
National Center for High-Performance Computing, Taiwan
Department of Computer Science and Information Engineering,Chung Hua University,Taiwan

Abstract
Scientific computing has become one of the key players in the advance of modern science and technologies. In the meantime, due
to the success of developments in processor fabrication, the computing power of Personal Computer (PC) is not to be ignored as
well. Lots of high throughput type of applications can be satisfied by using the current desktop PCs, especially for those in
computerized classrooms, and leave the supercomputers for the demands from large scale high performance parallel
computations. The goal of this work is to develop an automated mechanism for cluster computing to utilize the computing power
such as resides in computerized classroom. The PCs in computerized classroom are usually setup for education and training
purpose during the daytime, and shut down at night. After well deployment, these PCs can be transformed into a pre-configured
cluster computing resource immediately without touching the existing education/training environment installed on these PCs.
Thus, the training activities will not be affected by this additional activity to harvest idle computing cycles. To echo today's
energy saving issues, a dynamic power management is also developed to minimize energy cost. This development not only
greatly reduces the management efforts and time to build a cluster, but also implies the reduction of the power consumption by
such a mechanism.
Keywords: Personal Computer, cluster computing, computerized classroom, dynamic power management

1. Introduction
Due to the successful achievement in current processor design and fabrication, the computing power of Personal
Computer (PC) has become noticeable. In the meantime, even the performance of individual laptop exceeds that of
previous mainframe. The growth in computing power of the PC is a direct result of recent advances in computer
science. Many high throughput types of applications can be satisfied by using the current desktop PCs, especially for
those in computerized classroom [1], thus, leaving supercomputers to perform the work of large-scale parallel
computations.
Today, the computerized classroom is an unheeded resource or underutilized in academic institutes and colleges,
but it is a potential computing resource to support some computations. The PCs in computerized classroom are
usually setup during the daytime for education and training purposes and then shut down at night. One interesting
topic can be raised is how to exploit the computing resource of the power-off PCs, and without touching the existing
education/training environment installed on these PCs. Thus, the training activities will not be affected by this

1877–0509 © 2011 Published by Elsevier Ltd. Open access under CC BY-NC-ND license.
Selection and/or peer-review under responsibility of Prof. Mitsuhisa Sato and Prof. Satoshi Matsuoka
doi:10.1016/j.procs.2011.04.226

Shuen-Tai Wang et al. / Procedia Computer Science 4 (2011) 2068–2075

2069

additional activity to harvest idle computing cycles.
NPACI Rocks [2] is a full cluster package distribution that uses Red Hat kickstart to install the compute nodes.
Full automation of the PC cluster installation is its major advantage. But for the PCs in computerized classroom, by
deploying Rocks directly or other cluster distributions [3,4] will format all the hard drives. Furthermore, considering
these packages will provide more quickly image backup/restore functions, but the installation time of real system
may not be efficient for such situation of computerized classroom. ClassCloud [5] can switch the PCs into virtual
platform, and it also focuses on investigation of computerized classroom. The goal of ClassCloud is to build up a
basic cloud computing environment for testing. It uses Xen [6] to construct a virtual computing resource. Although
adopting virtual machine delivers high flexibility, it has a native performance issue, and it can’t reduce power
consumption. In [7] authors described a system to make on-the-fly partitioning of a physical cluster into multiple
independent virtual clusters. It works well for on-demand configurable dynamic virtual clusters, which associate
variable shares of cluster resources, but it is mainly designed for Grid environment and may not meet the
requirements of gathering the idle computing power in computerized classroom.
In this paper, we propose a mechanism to enable automated and rapid deployment for cluster computing in
computerized classroom. We have implemented it as a cluster-enabled toolbox - "Phantom Cluster" to solve the
above issues by deploying pre-configured cluster computing environment that can utilize the existing computing
power residing in computerized classroom. The Phantom Cluster was developed by National Center for Highperformance Computing (NCHC) [8]. It can transform the PCs into a cluster-computing resource immediately that
can be used at night when the PCs are normally not in use without touching the existing education/training
environment. This can be achieved by "Root over NFS" diskless environment. So it does not touch the client hard
drive, therefore, existing Operating Systems, along with all the software and applications installed on them, are
preserved. In addition to automation and manageability, many middleware are packaged into the Phantom Cluster,
such as resource management system, dynamic power management, accounting management, monitoring tools, and
the Message Passing Interface (MPI) [9] libraries for parallel program. On the other hand, we also integrated some
grid-related middleware for collaborating the geographically distributed Phantom Clusters to perform reliable and
efficient sharing of computing resources, and provide single entry interface for users for submitting, monitoring and
controlling jobs in distributed environment.
This paper presents an effective way to obtain the existing computing power residing in computerized classroom
for cluster computing. It works well especially for computationally bound applications. The rest of this paper is
organized as follows. Section 2 gives a description of hardware/software architecture. Section 3 gives some details
of the Phantom Cluster. Performance evaluation and analysis will be presented in section 4. Finally, section 5
presents the conclusion and future work.
2. System Architecture
Figure 1 shows the hardware architecture of the Phantom Cluster. The Phantom system now consists of a head
node and numerous computing nodes. The head node is a login node located in the top of the Phantom's architecture
for users to provide a single entry interface for submitting, monitoring and controlling jobs. Each head node has two
Ethernet interfaces. One is assigned a public IP address and connected to internet. The other is connected to the local
classroom switch. Both head node and compute node are installed with the same operating system to avoid the
problem of missing library files.
Figure 2 shows the software stack of the Phantom Cluster. It can be divided into two major parts: Diskless
Module and Phantom Module. First, the Phantom Module depends on the proper setups of diskless environment.
Above the diskless layer, several software are packaged into the Phantom Module for the Phantom system, such as
local queuing system, accounting software, monitoring tools, power management module, cloud-enabled extension
and MPI libraries. Most of the software are widely accepted and are tailored and tuned for helping automation and
manageability. We also wrapped the pre-configured software into a package for some Linux distributions, including
CentOS, Fedora, and Red Hat. With this one-size-fits-all solution staged, we can generally eliminate the efforts of
building the Phantom system.

2070

Shuen-Tai Wang et al. / Procedia Computer Science 4 (2011) 2068–2075

Fig. 1. Hardware architecture.

Fig. 2. Software architecture.

After well deployed, all PCs in the computerized classroom could be scheduled. Users can login the head node
and submit their jobs to batch queue system anytime. The batch queue system will fetch the jobs and schedule to the
applicable computing nodes by its scheduling policy. And then even when the work horses of the Phantom Cluster is
not available at the moment. Submitted jobs will be queued and wait for computing resources to become available,
typically during night time when the classroom PCs are free from their duty assigned to perform. When the PCs are
available, the Phantom will fetch the suitable jobs, parse the requirements, and remote power on exact number of the
PCs. After jobs are completed, the outputs will be sent back to the head node. While the Phantom Cluster with no
further job request, it will power off the PCs that arranged by batch queue system automatically. These features
greatly reduce the management effort and time required to build a cluster as well as power consumption.
3. Phantom Cluster
The Phantom Cluster is the base of the Phantom system. It provides a pre-configured cluster-computing
environment for utilizing the computing power resides in a computerized classroom. The special characteristics of
the Phantom Cluster are described in the following.
3.1. Diskless Remote Boot
The diskless remote boot is a PC without disk drives, which employs network remote booting to load its
operating system from a server. We implemented such mechanism to managing the deployment of the Linux
operating system across many computing nodes for the Phantom Cluster. It uses PXE/etherboot [10], NFS, and NIS
to provide services to PCs so that it is not necessary to install operating system on the PC’s hard drives individually,
and so on, it does not touch the hard drives, therefore, the original operating system installed on the PCs will be
unaffected. During the deployment of Diskless Module, the administrator needs to power on PCs one by one and
gather their MAC addresses. The diskless module will collect MAC addresses and generate the associated
configuration file for DHCP and PXE/etherboot services. Furthermore, the list of MAC addresses will be the target
of power management mentioned in the following section. After diskless environment being valid, the head node
will become a powerful server, which provides initial RAM disk, IP address for PXE boot, NFS and NIS service for
a computerized classroom.
3.2. Dynamic Power Management
To echo today's energy saving issues, we also developed a new approach to reduce energy utilization in the
Phantom Cluster. We do this work on the integration of resource management and remote power management [11]
that aims at reducing power consumption such that they suffice for meeting the minimizing quality of service
required by the Phantom Cluster. In particular, our approach relies on recalling services dynamically onto
appropriate amount of the PCs according to user's job request and temporarily shutting down the computers after

Shuen-Tai Wang et al. / Procedia Computer Science 4 (2011) 2068–2075

2071

finish in order to conserve energy. As shown in Figure 3, users can submit their jobs to the Phantom Cluster anytime,
even if the cluster is not available at the time the job is submitted. The submitted jobs are queued and then wait for
the computing resources to become available, typically at night time when the classroom PCs are free from their
assigned daytime duty. When the PCs become available, the head node of the Phantom Cluster fetches the
applicable jobs, parses the requirements, and remotely powers on the correct number of PCs by Wake-on-LAN [12]
protocol. After the job completed, the Phantom Cluster will then power the PCs down. Our implementation
currently relies on checking the local queuing system (i.e. Torque [13]) job pool and then decides to shut down
which compute nodes when no new job was submitted. Powering off idle PCs can significantly save more energy
than always keeping all PCs running.

Fig. 3. Scenario of dynamic power management.

Fig. 4. Software architecture.

3.3. Collaborative Phantom Clusters
NCHC has three business units located at three science parks in Taiwan now, and each business unit has two
computerized classrooms. After a proper deployment of Phantom Clusters in six computerized classrooms, we hope
that these distributed Phantom Clusters can collaborate and form a resource pool for users. We refer to Grid
architecture to achieve this work. Unlike the complex of standard Grid middleware, the Phantom’s head nodes are
all in our administrative domain, adopted the same operating system and user/group accounts. So we could bypass
the credential management service in this environment. For performing reliable and efficient sharing of computing
resources between Phantom Clusters, we employed the meta-scheduler - GridWay [14] to provide a scheduling
functionality.
GridWay is an open-source community project and it is highly modular, allowing adaptation to different
infrastructures. We customized the prolog, wrapper, and epilog behavior in GridWay; moreover, we modified
EM_MAD module to replace Globus [15] GRAM functions. Security is always the top priority for online services.
We've replaced the GridFTP [16] with general SFTP/SCP service to migrate user data between head nodes. By
using GridWay which makes central job submission and job dispatch to computerized classrooms in different
geographical locations possible. Figure 4 shows our approach how to work with the Phantom Cluster services. Like
the way using a typical PC cluster, users need to prepare simple job script and submit it to the Dispatch Manager via
command line interface. The Dispatch Manager will parse the job script by its scheduling policy, and then will be
transparently converted to equivalent Torque job script on head nodes. User's job files will be archived as a tar file
and transferred to head node's working directory for execution when Dispatch Manager dispatched them.
Hosting the meta-scheduler prevents us from installing additional software on remote head nodes. This also
means that Phantom has the scalability for new computerized classrooms to join it. So we could add new
computerized classrooms to raise Phantom's total computing capacity in the future.
3.4. Classroom Management
In order to enable Phantom Clusters work tightly with computerized classrooms in different locations. It is

2072

Shuen-Tai Wang et al. / Procedia Computer Science 4 (2011) 2068–2075

necessary to develop environmental controlling tools. In each computerized classroom in NCHC, it has already
installed Clonezilla [17] for OS image cloning and deployment for education and training course. The Clonezilla has
its own DHCP service to clone or backup OS images. But due to the DHCP protocol constrains, it is not allowed to
have two DHCP servers under the same subnet at one time. Accordingly, we have to let the head node has the ability
to start/stop the DHCP service of Clonezilla server automatically. The workflows of our classroom management are
depicted in Figure 5. We arranged a central database to store the weekly education/training time table data. The
Classroom-Ctrl tool client-side implementation on each head node will query this database every hour. This helps
role of a classroom to change over time and facilitates our lecturers to give full-day or half-day courses. In other
words, we could schedule the accessibility of each classroom. Figure 6 illustrates the more detail workflow about
how we control the DHCP service in a classroom. Before switching for cluster computing, the Classroom-Ctrl tool
will shut down all PCs via UDP datagram, and stop the Clonezilla server's DHCP service. A few minutes later,
Classroom-Ctrl tool will start head node's DHCP service to handle the boot-time installation requests from diskless
clients. Each time slot for computing or training activity will last at least for two hours. In the same way, five
minutes before the expiration on computing, the head node will power off its diskless nodes and stop its own DHCP
service. The Classroom-Ctrl tool will try to start the Clonezilla's DHCP service again before time is up. With this
workflow process, not only night time but also empty time slots, local PC classrooms will be ready for computing
most of time.

Fig. 5. Classroom management workflow.

Fig. 6. Head node workflow.

3.5. Account Management
The account system in the Phantom is quite different than a regular PC cluster. How to keep accounts and
passwords on head nodes of collaborative Phantom Clusters consistent with those on all nodes is a challenge for us.
Rsync is one of the most efficient Linux utility to replicate two servers. The benefit of rsync is that it could work
over SSH to offer a secure channel. We use rsync to synchronize the users’ account in the Phantom. Any update of
account data will be propagated to all head nodes, and then the head node updates its diskless clients via YP service.
How to overcome the obstacle of host SSH key should be taken into consideration, too. We arranged an expect
script that will generate and swapped the public SSH keys among front and head nodes during new user's first time
login process. With these preparations, job migration will be fulfilled through a password-less tunnel.
4. Performance Evaluation and Analysis
In this section, we employ HPL (High Performance Linpack) [18] and IOR [19] to evaluate the performance of
the Phantom Cluster in a computerized classroom, and compare the results with two production-run clusters in
NCHC. These two self-made clusters are Siraya Cluster and our recently built GTX GPU Cluster. The details of
hardware specification are listed in Table 1.
Table 1. Hardware specification of the clusters

2073

Shuen-Tai Wang et al. / Procedia Computer Science 4 (2011) 2068–2075

Phantom Cluster

Siraya Cluster

GTX GPU Cluster

Processors

Intel Core2 Duo 2.83GHz x 1

AMD Opteron 275 Dual Core
2.2GHz *2

Intel X5660 Six-Core
2.8GHz *2

Total Memory

8G DDR2 SDRAM

8GB DDR400 Registered/ECC
SDRAM

48GB DDR3
Registered/ECC SDRAM

Network Interface

Gigabit Ethernet x 1

Gigabit Ethernet x 2

Gigabit Ethernet x 2

10Gbps InfiniBand

40Gbps InfiniBand

Total Nodes

Desktop PC x 40

1U Rack-mount server x 80

4U Rack-mount server x 8

Total Watts

9.6KW

66KW

11.2KW

HPL is the widely used benchmark for evaluating performance of supercomputers. However, the performance of
PC cluster is largely application-dependent. We also use the NCHC benchmark suite [20] which contains five
benchmarks, namely hubksp, nonh3d, bem3d, ns3d and jcg3d, picked from four application domains. The hubksp
program comes from physics, and nonh3d is an atmospheric science case. Both bem3d and ns3d are applications of
parallel computing in the field of CFD (Computational Fluid Dynamics). The last one, jcg3d is from computational
solid mechanics. These jobs helped us to distinguish the performance in different problem domains.
Table 2 shows the results of HPL Rmax and Efficiency on 80 CPU-Cores. We observed that the GTX GPU
Cluster presents the best result among these machines, while Siraya is the worst one. Although this diskless based
Phantom Cluster is not fine-tuned in its networking configurations, but it is capable of running parallel computations.
Table 3 is the IOR benchmark results. IOR is a benchmark tools for evaluating the performance of parallel file
system. The results from this table reveal that the Phantom has poor results for parallel write operation due to its
single Ethernet network channel.
Table 2. HPL Rmax and Efficiency
Cluster Name (Network)

Network Topology

Network Switch

Rmax in GFlops

Efficiency (%)

Phantom (GbE)

Star

Cisco SLM2048

271.1

30

Siraya (IB)

Fat-tree

Voltaire ISR 9288

271.05

77

Siraya (GbE)

Star

Nortel 5510-48T

228.8

65

GTX GPU Cluster (IB)

Fat-tree

Voltaire ISR 9024

761.6

85

GTX GPU Cluster (GbE)

Star

DXS-3350SR

618.2

69

Table 3. IOR Tests
IOR Parameters

Cluster Name (Network)

Max Write
(MB/sec)

Max Read
(MB/sec)

api = MPIIO, access = single-shared-file, ordering in a file =
sequential offsets, ordering inter file = no tasks offsets, clients = 8 (4
per node), repetitions = 1, xfersize = 4 MiB, blocksize = 1 GiB,
aggregate filesize = 8 GiB

Phantom (GbE)

2.84

10.12

Siraya (GbE)

4.63

12.35

GTX GPU Cluster (GbE)

4.7

14.33

From sequential job tests listed in Table 4, GTX GPU Cluster went beyond Phantom while running on single core.
But, the Phantom Cluster still proved that it could provide basic capability for scientific computing. According to
the floating point operations per clock cycle information, the Intel Core2 Duo doubles the Siraya AMD Opteron
processor. For sequential jobs, Siraya apparently will be the worst one due to its less powerful processor. In terms of
parallel processing, we evaluate hubksp and nonh3d jobs on many processors. Figure 7 shows the performance
comparison of Siraya and Phantom. It is delightful to see our Phantom took less time than Siraya while executing
nonh3d. For parallel hubksp jobs, Siraya surpassed Phantom on 32 cores. There is a plausible reason for results of
Phantom while running the hubksp project beyond 16 cores. The main cause for a drop in performance is NFS
network contention with the computing channel over the same switch in the diskless environment. Both Siraya and

2074

Shuen-Tai Wang et al. / Procedia Computer Science 4 (2011) 2068–2075

GTX GPU Cluster have dedicated sub-networks for monitoring, computing and file-system to avoid network traffic
blocking. Initially, desktop PCs are not for high performance computing. However, for those non-urgent computing
jobs, the above results will suffice to say that Phantom will be enough.
Table 4. Performance of NCHC Benchmark Suit
bem3d

hubksp

jcg3d

nonh3d

ns3d

Phantom

46m27s

126m2s

44m24s

59m58s

162m14s

Siraya

94m50s

189m3s

102m1s

149ms36s

272m15s

GTX GPU Cluster

45m79s

37m6s

37m54s

28m22s

37m32s

Fig. 7. Performance of NCHC benchmark suit.

Considering the results in Figure 8, Phantom has better potential in performance-power ratio than Siraya, mainly
because of a group of inferior desktops. Generally speaking, single desktop PC consumes less power than a rackmount machine. Figure 9 shows that we can reduce power consumption after launching Phantom's dynamic power
management. The first case is to keep all PCs powered on all the time. The other case is that PCs were powered off
when no requests. However, running other applications or different submission frequencies will cause distinct
results, so we submitted the same workload (240 HPL jobs; 10 jobs per hour) in three days for testing cases to
sustain objectivity. By enabling dynamic power management, we roughly saved 1790 watts compared to the always
powered-on case. If we keep running these two cases for years, the distinction will be more obvious.

Fig. 8. Performance/Power ratio comparison.

Fig. 9. Power consumption comparison.

The Phantom Cluster takes advantage of existing idle time of computerized classrooms to do computing. It makes
sense to neglect the purchasing cost. Besides, the power consumption of booting stage will obviously be reduced in
diskless mode. Running Phantom will be more economical than staging a traditional PC cluster. We can reduce
electricity and disk failure cost for online service. To sum up, C-P ratio of the Phantom Cluster will far surpasses
installing brand new machines.
5. Conclusion and Future Work
The Phantom Cluster can utilize the untapped computing power that resides in numerous PCs of computerized

Shuen-Tai Wang et al. / Procedia Computer Science 4 (2011) 2068–2075

2075

classroom. It works especially for sequential or parallel computationally bound applications very well. But it does
not apply to communication or I/O bound applications which need high-performance file system or local scratch
space on compute nodes. To echo today's energy saving issues, the Phantom Cluster includes a built-in dynamic
power management that wakes up the idle PCs when computing power is needed and then powers them down when
the job is done. The modified meta-scheduler is also integrated into the Phantom Cluster to perform reliable and
efficient sharing of computing resource resides in geographically distributed computerized classrooms. For general
science applications, the benchmark results indicate that Phantom not only has the advantage of better
cost/performance ratio but also has the potential to outperform high-end machines. The Phantom Cluster also greatly
reduces the time required to build and manage a cluster. In the future, we plan to add Cloud middleware that the
Phantom can be extended for Cloud Computing. It also will soon be planned to deliver freely as one ISO image to
provide a Drop & Go cluster computing solution.
Acknowledgment
This work is supported by National Science Council, R.O.C., under the contract number of NSC-99-2221-E-492017.
References
1.
2.
3.
4.
5.
6.
7.

8.
9.
10.
11.
12.
13.
14.
15.
16.
17.
18.
19.
20.

A. Apon, R. Buyya, H. Jin, and J. Mache, "Cluster Computing in the Classroom: Topics, Guidelines, and Experiences"
P. M. Papadopoulous, M. J. Katz, and G. Bruno, "NPACI Rocks: Tools and techniques for easily deploying manageable Linux clusters," In
IEEE Cluster 2001, October 2001.
Open Cluster Group, "OSCAR: A packaged cluster software stack for high performance computing." http://www.openclustergroup.org.
Scyld Beowulf. http://www.scyld.com/.
C. Y. Tu, W. C. Kuo, Y. T. Wang, Steven Shiau,"E2CC: Building energy efficient ClassCloud using DRBL," in Grid '09: Proceedings of the
10th annual international conferenceon Grid Computing. Banff, AB, Canada: IEEE Computer Society, 2009, pp. 189-195.
P. Barham, B. Dragovic, K. Fraser, S. Hand, T. Harris, A. Ho, R. Neugebauer, I. Pratt, and A. Warfield, "Xen and the Art of Virtualization,"
In Proc. 19th SOSP, Lake George, NY, Oct 2003.
J. S. Chase, D. E. Irwin, L. E. Grit, J. D. Moore, and S. E. Sprenkle,.Dynamic Virtual Clusters in a Grid Site Manager,. in
HPDC ’03:Proceedings of the 12th IEEE International Symposium on High Performance Distributed Computing, Washington, DC, USA, p.
90, IEEE Computer Society, 2003.
NCHC, National Center for High-performance Computing, http://www.nchc.org.tw
Message Passing Interface Forum, "MPI: A message-passing interface standard," International Journal of Supercomputer Applications 8 (3/4)
(1994) 165–414
PXELINUX, http://syslinux.zytor.com/wiki/index.php/PXELINUX.
A. DiRienzo, John A. Medeiros, M. Whitlock, E. Wages, and J. Highfield, "Concepts for Computer Center Power Management,"
International Supercomputing Conference, 2010.
Wake-on-LAN, http://en.wikipedia.org/wiki/Wake-on-LAN
TORQUE Resource Manager, http://www.clusterresources.com/products/torque-resource-manager.php.
P. Armstrong, "Building a Scheduler Adapter for the GridWay Metascheduler," Faculty of Engineering Summer 2006 Work Term Report
I. Foster and C. Kesselman, "Globus: A Metacomputing Infrastructure Toolkit," International Journal of Supercomputer Applications 11 2
(1997), pp. 115-128.
The Globus Project: the GridFTP protocol, http://www.globus.org/datagrid/gridftp.html.
S. Shiau, "The Clonezilla project," http://clonezilla.nchc.org.tw and http://clonezilla.sourceforge.net.
A. Petitet, R. C. Whaley, J. J. Dongarra, and A. Cleary. "HPL - A Portable Implementation of the High-performance Linpack Benchmark for
Distributed Memory Computers," http://www.netlib.org/benchmark/hpl/.
IOR, http://www.llnl.gov/asci/purple/benchmarks/limited/ior/.
K.C. Huang, H.Y. Chang, C.Y. Shen, C.Y. Chou, S.C. Tcheng, "Benchmarking and Performance Evaluation of NCHC PC Cluster, "The
Fourth International Conference on High-Performance Computing in the Asia-Pacific Region-Volume 2, 2000.

