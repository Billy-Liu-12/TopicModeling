Low-Cost Fault-Tolerance Protocol for
Large-Scale Network Monitoring
JinHo Ahn1 , SungGi Min2, , YoungIl Choi3 , and ByungSun Lee3
1

3

Dept. of Computer Science, College of Information Science, Kyonggi University
San 94-6 Yiuidong, Paldalgu, Suwonsi Kyonggido 442-760, Republic of Korea
jhahn91@hanmail.net
2
Dept. of Computer Science & Engineering, Korea University
5-1 Anamdong, Sungbukgu, Seoul 136-701, Republic of Korea
sgmin@korea.ac.kr
Network Technology Lab., Electronics and Telecommunications Research Institute
Yusong P.O. Box 106, Taejon 305-600, Republic of Korea
{yichoi,bslee}@etri.re.kr

Abstract. Distributed hierarchical network monitoring model has been
proposed to solve scalability problem of centralized model. In this distributed model, a top-level monitoring manager, called main manager,
obtains aggregate management information from mid-level managers,
named domain managers, forming a hierarchical structure. However, if
some of monitoring managers crash, network elements cannot be continuously and correctly monitored until the managers are repaired. To address this important, but previously unresolved issue, this paper presents
a new fault-tolerance protocol for domain managers, named DM F T P ,
allowing the managers to eﬃciently utilize their organization structure.
Therefore, this protocol can minimize failure detection overhead and the
number of live managers aﬀected by each manager node crash. Also, it
tolerates concurrent manager failures and, after the failed managers have
been repaired, ensures their immediate and consistent recovery.

1

Introduction

Network monitoring is an ability to collect traﬃc information from networked
systems in a timely manner [4]. The traﬃc information is generally used in these
systems for a variety of purposes such as quality of service, scheduling, capacity
planning and traﬃc ﬂow prediction and so on. As the systems scale up, the
importance of the ability signiﬁcantly increases because it is very diﬃcult to
control and manage the systems without any monitoring mechanism.
Monitoring systems should be designed to minimize network traﬃc incurred
by them and latency in extracting the necessary information from the networked
systems [5]. These systems can be classiﬁed into centralized and distributed(or
This work was supported by Electronics and Telecommunications Research Institute
Grant.
Corresponding author. Tel.:+82-2-3290-3201; fax:+82-2-953-0771.
P.M.A. Sloot et al. (Eds.): ICCS 2003, LNCS 2659, pp. 504–513, 2003.
c Springer-Verlag Berlin Heidelberg 2003

Low-Cost Fault-Tolerance Protocol for Large-Scale Network Monitoring

505

decentralized) based on their organization model. In the centralized monitoring model, a single manager collects information from all agents on monitored
nodes and processes it. The model can be simply implemented, but become a
performance bottleneck of the entire systems. Thus, the model is widely used to
manage relatively small-scale networks. In order to pursue increased performance
and scalability, the distributed monitoring model can be used for large-scale networked systems. In this model, monitoring managers are largely classiﬁed into
main manager and domain manager depending on the role of each manager,
and hierarchically organized. In other words, a hierarchical structure is formed
with the main manager in top, several levels of domain managers in the middle and a collection of agents at the bottom. Thus, administrators can delegate
monitoring tasks across a hierarchy of managers [2]. Each lowest-level domain
manager collects management information from some agents. Similarly, a set of
domain managers act as agents to a higher-level domain manager or to the main
manager. The main manager obtains and processes aggregate and pre-ﬁltered
information from the domain managers. Several existing network monitoring
systems based on this model were proposed to use more than one among some
technologies such as SNMP, Distributed Object, Mobile Agent and so on [4].

Main Manager M
M1

M2

……

Mk

Fig. 1. A cluster of redundant main managers

However, suppose that some of managers crash in this model. In this case,
network devices or nodes aﬀected by the failures cannot be continuously and
correctly monitored before the failed managers are recovered. To the best of our
knowledge, few among previous works consider this important issue. Especially,
if the main manager fails, the entire monitoring system may never play its role.
To address the issue, our research group has performed developing a scalable and
eﬃcient fault-tolerance strategy for the model [1]. In this strategy, main manager
fault tolerance is achieved by applying existing scalable replication-based protocols [3] to a cluster of redundant main managers like in Fig. 1 because the main
manager should process very large volume of management information and perform signiﬁcantly complex decision-making process for various applications. But,
another fault-tolerance protocol is required for a hierarchy of domain managers
due to their more lightweight role compared with that of the main manager.
For this purpose, this paper proposes a scalable fault-tolerance protocol for domain managers, called DM F T P (Domain M anager F ault-T olerance P rotocol),
to eﬃciently utilize their hierarchical structure based on the assumption that

506

J. Ahn et al.

the main manager is failure-free. The protocol results in low failure detection
overhead by each domain manager periodically sending a domain manager advertisement message only to its immediate super manager. Thus, when some of
domain managers fail even concurrently, the protocol enables their immediate
super managers to take over them. This behavior can achieve minimizing the
number of live managers aﬀected by the failures. Also, after failed managers
have been recovered, it allows them to immediately play their pre-failure roles in
order to improve entire monitoring system performance degraded by the failures.
The rest of the paper is organized as follows. In Sect. 2, we describe our
designed protocol DM F T P and in Sect. 3, prove its correctness. Finally, Sect. 4
concludes this paper.

2

Domain Manager Fault-Tolerance Protocol (DMFTP)

The proposed protocol DM F T P is composed of three components, failure detection, takeover and recovery. Every manager i should maintain three variables,
M M ngri , P arenti and SubM ngrsi , for the protocol. M M ngri is the main manager’s identiﬁer needed when domain manager i recovers after repaired. P arenti
is the immediate super manager’s identiﬁer of domain manager i. SubM ngrsi
is a tree for saving the identiﬁer and timer of every sub-manager of main or domain manager i. Its node is a tuple (id, timer, sub). timer for each sub-manager
id is used so that manager i detects whether sub-manager id is alive or failed
currently, and is initialized to a. sub for sub-manager id is a sub-tree for all submanagers of the domain manager id. In the next subsections, basic concepts and
algorithms of the three components of DM F T P are described in details. Due
to space limitation, there are no formal descriptions of the proposed protocol in
this paper. The interested reader can ﬁnd the descriptions in [1].
2.1

Failure Detection and Takeover

Every domain manager i periodically sends each domain manager advertisement
message only to its immediate super manager P arenti . Thus, monitoring the
advertisement messages of its sub managers, the super manager P arenti can detect which managers fail or are alive among them. For example, Fig. 2(a) shows
a hierarchy of managers consisting of a main manager M and twelve domain
managers and Fig. 2(b), domain manager advertisement message interaction between four managers M , D1 , D6 and D12 . In Fig. 2(b), domain managers D1 ,
D6 and D12 periodically transmit each a domain manager advertisement message to their immediate super managers M , D1 and D6 by invoking procedure
Rcv DmMngrAM(). In this case, each super manager resets timer for each
corresponding sub manager to a like in Figs. 3(a)–3(c). Therefore, the protocol
DM F T P results in low failure-free overhead incurred by failure detection by
eﬃciently utilizing the hierarchical structure of managers.
In DM F T P , each super manager sm decrements the timer for every sub
manager by one every certain time interval. If there are some sub managers

Low-Cost Fault-Tolerance Protocol for Large-Scale Network Monitoring

507

which cannot send each a domain manager advertisement message to sm until
their timers become zero, sm suspects that the managers failed. In this case, it
executes the takeover procedure of DM F T P . For example, if domain manager
D6 fails like in Fig. 4, manager D1 cannot receive any advertisement message
from D6 . Thus, the timer for D6 eventually becomes zero like in Fig. 4(c) and so,
D1 invokes procedure Takeover(). This procedure forces three sub managers
of D6 , D10 , D11 and D12 , to call procedure Update Parent(). In this case, like
in Fig. 4(d), the sub managers set all P arenti to D1 as their immediate super
manager. Afterwards, they periodically send each an advertisement message to
D1 like in Fig. 4(b).
Also, the protocol DM F T P performs the consistent takeover process even if
domain managers fail concurrently. To illustrate this claim, Fig. 5 indicates an
example that concurrent failures of two domain managers D1 and D6 occur. In
this case, main manager M can receive no advertisement message from the domain manager D1 like in Fig. 5(b). Thus, the timer for D1 of M is ﬁnally expired
like in Fig. 5(c) and then M executes procedure Takeover(), which causes sub
managers of D1 , D4 , D5 and D6 , to invoke procedure Update Parent(). However, M cannot receive any acknowledgement of the procedure from the failed
manager D6 like in Fig. 5(b). In this case, it sets the timer for D6 to zero and
then forces the sub managers of D6 to perform procedure Update Parent().
After having completed the takeover procedure, M can receive each advertisement message from D10 , D11 and D12 respectively every certain interval like in
Fig. 5(b).
2.2

Recovery

This section describes the recovery algorithms of DM F T P allowing repaired
domain managers to play their pre-failure roles. For example, Fig. 6 shows that
failed domain manager D6 in Fig. 5 is recovered by executing DM F T P . In this
example, after the manager D6 has been repaired, it performs procedure Recovery(). This procedure ﬁrst forces the main manager M to invoke procedure
Get Parents() like in Fig. 6(b) in order to obtain a list of super managers of D6
known to be alive by M . In this case, the received list is {M } because the manager D1 previously failed. Thus, D6 requires from M a tree of sub managers it
managed before the failure by remotely calling procedure Get SubMngrs() of
M . At this point, M resets the timer for D6 to a like in Fig. 6(c) and then causes
three sub managers, D10 , D11 and D12 , to remotely invoke Update Parent()
so that the sub managers set all P arenti to D6 as their immediate super manager. After having completed this procedure, D6 obtains the tree from M like
Fig. 6(d). Hereafter, they periodically send each an advertisement message to
D6 like in Fig. 6(b).
Next, DM F T P can allow each repaired manager to be recovered consistently
even if its immediate super manager fails concurrently when the recovery process
is started. To illustrate this feature, Fig. 7 shows an example that D1 crashes as
soon as D6 in Fig. 4 performs procedure Recovery(). In this case, D6 receives
a list of super managers of D6 , {M, D1 }, from M because M has not detected

508

J. Ahn et al.
M

D1

D4

D2

D5

D6

D10

D11

D3

D7

D8

D9

D12

(a) a hierarchy of managers
M

D1

D6

D12

Rcv_DmMngrAM()
Rcv_DmMngrAM()
Rcv_DmMngrAM()

(b) Domain manager advertisement message interaction between four managers
Fig. 2. In case of no manager failure in DM F T P (continued)

D1 ’s failure yet. Then, D6 ﬁrst invokes procedure Get SubMngrs() of the
lowest level super manager in the list, D1 , to obtain a tree of its monitored sub
managers. However, like in Fig. 7, D6 cannot get the tree from the currently
crashed manager, D1 . Thus, it requires the tree from the secondly lowest level
super manager, M , by calling procedure Get SubMngrs() of M . At this point,
M can detect D1 ’s failure by either D6 or the timer for D1 . In the ﬁrst case,
it sets the timer for D1 to zero and then forces the sub managers of D1 to set
all P arenti to M by invoking their procedure Update Parent() respectively.
Also, M resets the timer for D6 to a and then causes D6 ’s sub managers to set
their P arenti to D6 . Then, D6 gets the tree from M . Afterwards, D4 , D5 and
D6 periodically send each an advertisement message to M , and D10 , D11 and
D12 , to D6 like in Fig. 7.

Low-Cost Fault-Tolerance Protocol for Large-Scale Network Monitoring
ParentM = nil

D4

a

nil

D10

a

nil

SubMngrsM

D5

a

nil

D11

a

nil

D6

a

D12

a

nil

id

509

timer sub

D1

a

D2

a

D7

a

nil

D3

a

D8

a

nil

D9

a

nil

(a) Data structures for the main manager M
MMngrD1 = M

D4
ParentD1 = M
SubMngrsD1

a

nil

D10

a

nil

D5

a

nil

D11

a

nil

D6

a

D12

a

nil

(b) Data structures for D1
MMngrD6 = M
ParentD6 = D1
SubMngrsD6

MMngrD12 = M

D10

a

nil

D11

a

nil

D12

a

nil

(c) Data structures for D6

ParentD12 = D6
SubMngrsD12

nil

(d) Data structures for
D12

Fig. 3. In case of no manager failure in DM F T P

3

Correctness

This section shows theorem 1 and 2 to prove the correctness of our takeover
algorithm and recovery algorithm in DM F T P . Due to space limitation, the
proof of the two theorems is omitted. The interested reader can ﬁnd them in [1].
Theorem 1. Even if multiple domain managers crash concurrently, our takeover
algorithm enables another live managers to monitor all the network elements
previously managed by the failed ones.
Theorem 2. After every repaired domain manager has completed our recovery
algorithm, it can manage its monitored network elements before it has failed.

510

J. Ahn et al.
M

D1

D4

D2

D5

D6

D10

D11

D7

D3

D8

D9

D12

(a) a hierarchy of managers
M

D1

D6

D12

Rcv_DmMngrAM()
Rcv_DmMngrAM()
Chk_Timer()
Update_Parent()

Rcv_DmMngrAM()

(b) Domain manager advertisement message interaction between four managers
MMngrD1 = M
ParentD1 = M
SubMngrsD1

D4

a

nil

D10

a

nil

D5

a

nil

D11

a

nil

D6

0

D12

a

nil

(c) Data structures for D1
MMngrD12 = M
ParentD12 = D1
SubMngrsD12

nil

(d) Data structures for
D12
Fig. 4. In case of D6 ’s failure in DM F T P

Low-Cost Fault-Tolerance Protocol for Large-Scale Network Monitoring

511

M

D1

D4

D2

D5

D6

D10

D11

D7

D3

D8

D9

D12

(a) a hierarchy of managers
M

D1

D6

D12

Rcv_DmMngrAM()

Chk_Timer()
Update_Parent()
Update_Parent()
Rcv_DmMngrAM()

(b) Domain manager advertisement message interaction between four managers
ParentM = nil
SubMngrsM
id

D4

a

nil

D10

a

nil

D5

a

nil

D11

a

nil

D6

0

D12

a

nil

timer sub

D1

0

D2

a

D7

a

nil

D3

a

D8

a

nil

D9

a

nil

(c) Data structures for M
MMngrD12 = M
ParentD12 = M
SubMngrsD12

nil

(d) Data structures
for D12
Fig. 5. In case of concurrent failures of two domain managers D1 and D6 in DM F T P

512

J. Ahn et al.
M

D1

D4

D2

D5

D6

D10

D11

D7

D3

D8

D9

D12

(a) a hierarchy of managers
M

D1

D6

D12

Get_Parents()
Get_SubMngrs()
Update_Parent()
Rcv_DmMngrAM()

(b) Domain manager advertisement message interaction between four managers
ParentM = nil

D4

SubMngrsM
id

a

nil

D10

a

nil

D5

a

nil

D11

a

nil

D6

a

D12

a

nil

timer sub

D1

0

D2

a

D7

a

nil

D3

a

D8

a

nil

D9

a

nil

(c) Data structures for M
MMngrD6 = M
ParentD6 = M
SubMngrsD6

MMngrD12 = M

D10

a

nil

D11

a

nil

D12

a

nil

(d) Data structures for D6

ParentD12 = D6
SubMngrsD12

nil

(e) Data structures
for D12

Fig. 6. An example of recovering domain manager D6 in ﬁgure 5

Low-Cost Fault-Tolerance Protocol for Large-Scale Network Monitoring
M

D1

D6

513

D12

Get_Parents()
Get_SubMngrs()
Get_SubMngrs()
Update_Parent()
Update_Parent()
Rcv_DmMngrAM()
Rcv_DmMngrAM()

Fig. 7. In case of D1 ’s failure while recovering D6 in ﬁgure 4

4

Conclusion

In this paper, a low cost fault-tolerance protocol for domain managers, DM F T P ,
was designed based on their hierarchical structure. This protocol can minimize
failure detection overhead and the number of live managers aﬀected by each
manager node crash. Also, it tolerates concurrent manager failures and, after
the failed managers have been repaired, ensures their immediate and consistent
recovery.
Integrating DM F T P with existing replication-based protocols [3] for the
main manager is a research work requiring further investigation because the role
of the main manager fault-tolerance protocols is very important for DM F T P to
work eﬀectively [1]. Thus, we are currently examining which replication protocols
are appropriate for DM F T P through various experiments.

References
1. J. Ahn, S. Min, Y. Choi and B. Lee. A Novel Fault-Tolerance Strategy for LargeScale Network Monitoring. Technical Report KU-CSE-02-049, Korea University,
2002.
2. G. Goldszmidt and Y. Yemini. Delegated Agents for Network Management. IEEE
Communication Magazine, 36(3):66–70, March 1998.
3. R. Guerraoui and A. Schiper. Software-Based Replication for Fault Tolerance. IEEE
Computer, 30(4):68–74, 1997.
4. J. Philippe, M. Flatin and S. Znaty. Two Taxonomies of Distributed Network
and System Management Paradigms. Emerging Trends and Challenges in Network
Management, 2000.
5. R. Subramanyan, J. Miguel-Alonso and J.A.B. Fortes. A scalable SNMP-based
distributed monitoring system for heterogeneous network computing. In Proc. of
the 12nd ACM/IEEE International Supercomputing Conference. Dallas, Texas, Nov.
2000.

