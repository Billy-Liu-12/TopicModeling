Available online at www.sciencedirect.com

Procedia Computer Science 4 (2011) 898–907

International Conference on Computational Science, ICCS 2011

Development of a High-speed Eigenvalue-solver for Constant
Plasma Monitoring on a Cell Cluster System
Noriyuki Kushidaa,1,, Ken-ichi Fujibayashib , Hiroshi Takemiyab
a Center

for Computational Science and E-systems, Tokai Research and Development Center, Japan Atomic Energy Agency
b Center for Computational Science and E-systems, Japan Atomic Energy Agency

Abstract
We developed a high speed eigenvalue solver that is an essential part of a plasma stability analysis system for fusion
reactors on a Cell cluster system. In order to achieve continuous operation of fusion reactors, we must evaluate the state
of plasma within the characteristic conﬁnement time of the plasma density and temperature in fusion reactors. This is
because we can prevent plasma from being disrupted by controlling the conﬁning magnetic ﬁeld, if we can determine
the state of the plasma within the characteristic conﬁnement time. Therefore, we introduced a Cell processor that
has high computational power and high performance/cost, in order to achieve constant monitoring of fusion reactors.
Furthermore, we developed a novel eigenvalue solver, which usually consumes most of the plasma evaluation time, to
achieve high performance of our Cell cluster system. The eigensolver is based on the conjugate gradient (CG) method
and was designed by considering three levels of parallelism, which we refer to as Intra-processor, Inner-processor, and
SIMD parallel. In addition, we developed a new CG acceleration method, called locally complete LU. This method
has the same acceleration performance as complete LU, which is one of the best acceleration methods, without any
reduction in parallel performance. Finally, we succeeded in obtaining our target performance: we were able to solve
a block tri-diagonal Hermitian matrix containing 1024 diagonal blocks, where the size of each block was 128 × 128,
within a second. Therefore, we have found a suitable candidate for achieving a satisfactory monitoring system.
Keywords: PowerXCell8i, Cell cluster, Eigensolver, Preconditioned Conjugate Gradient Method, Parallel
Computing, Plasma stability analysis

1. Introduction
In this study, we developed a high speed eigenvalue solver on a Cell cluster system, which is an essential component of a plasma stability analysis system for fusion reactors such as International Thermo-nuclear Experimental
Reactor[1]. The Japan Atomic Energy Agency (JAEA) has been developing a plasma stability analysis system, in
order to achieve sustainable operation. In Fig. 1, we illustrate an overview of the plasma stability analysis system.
The plasma stability analysis system works as follows:
(1) Obtain the current magnetic ﬁeld status of the exterior of the reactor.
Email address: kushida.noriyuki@jaea.go.jp (Noriyuki Kushida)
author

1 Corresponding

1877–0509 © 2011 Published by Elsevier Ltd. Open access under CC BY-NC-ND license.
Selection and/or peer-review under responsibility of Prof. Mitsuhisa Sato and Prof. Satoshi Matsuoka
doi:10.1016/j.procs.2011.04.095

Noriyuki Kushida et al. / Procedia Computer Science 4 (2011) 898–907

899

(2) Analyze the plasma state using numerical simulation.
(3) Judge the state of the plasma (Plasma is stable/unstable, when the smallest eigenvalue λ is greater/smaller than
zero).
(4) If the plasma is unstable, the operating conditions are changed, in order to stabilize the plasma.

Fusion Reactor
Data sender

Data receiver

Reactor controller

Cell cluster

λ > 0: stable
λ < 0: unstable

Matrix generation
Eigensolver

Result sender

Figure 1: Illustrative overview of the plasma stability system

The main component of the plasma stability analysis system is the plasma simulation program MARG2D [2]. MARG2D
consists of roughly two parts: One is the matrix generation part; the other is the eigensolver. In order to achieve sustainable operation of fusion reactors, we must evaluate the state of the plasma every two to three seconds. This is
because the characteristic conﬁnement time of the density and temperature in a fusion reactor is from three to ﬁve
seconds[3], and we can prevent plasma from disruption by controlling the conﬁning magnetic ﬁeld. Moreover, we
estimated that we must determine the state of the plasma within half of the characteristic conﬁnement time, by taking
into account the time for data transfer, and other such activities. Since we must solve for the plasma state within
a quite short time interval, a high-speed computer is essential. In particular, the eigensolver consumes the greatest
amount of the computation time of MARG2D. Therefore, we focused on the eigensolver in this study.
A massively parallel supercomputer (MPP), which obtains its high calculation speed by connecting many processing units and is the current trend for heavy duty computation, is inadequate for following two reasons.
(1) We cannot dedicate MPPs to the monitoring system.
(2) MPPs have a network communication overhead.
We elaborate on the above two points. Firstly, with regard to the ﬁrst point, when we consider developing the plasma
monitoring system, we are required to utilize a computer during the entire fusion reactor operation. That is because
fusion reactor must be monitored continuously and without delay. For this reason MPPs are inadequate because they
are usually shared with a batch job system. Furthermore, using an MPP is unrealistic, because it is high price, requires
huge exclusively-designed building, and so forth. Therefore, MPPs could not be dedicated to such a monitoring
system. Next, we discuss the latter point. MPPs consist of many processing units that are connected via a network. The
data transfer performance of a network is lower than that of main memory. In addition, there are several overheads that
are ascribable to introducing a network, such as the time to synchronize processors, and the time to call communication
functions. These overheads are typically from O(n) to O(n2 ), where n is the number of processors. Even though the
overheads can be substantial with a large number of processors, they are usually negligible for large-scale computing,
because the net computational time is quite long[4]. However, the monitoring system is required to terminate within
such a short period that network overheads can be dominant. Moreover, the entire time for computation can be longer
when the number of processors increases. Thus, we cannot utilize MPPs for the monitoring system.
In order to deal with the above diﬃculties, we introduced a Cell cluster system into this study. A cell processor
is faster than a traditional processor, hence we could obtain suﬃcient computational power with a small number of

900

Noriyuki Kushida et al. / Procedia Computer Science 4 (2011) 898–907

PPE

SPE
SPE

SPE
SPE

SPE
SPE

SPE
SPE

SPU

SPU

SPU

SPU

LS

LS

LS

LS

PPU
L1 Cache

L2 Cache

MFC

MFC

MFC

MIC
MIC

DDR2
Main
memory

Flex
Flex
IO
IO

External
device

MFC

Element Interconnect Bus
MFC

MFC

MFC

MFC

LS

LS

LS

LS

SPU

SPU

SPU

SPU

SPE
SPE

SPE
SPE

SPE
SPE

SPE
SPE

Figure 2: Overview of the PowerXCell 8i processor

processors. Thus, we were able to establish the Cell cluster system at much cheaper cost, and we can dedicate it to
monitoring. Moreover, our Cell cluster system requires less network overhead. Therefore, it should be suitable for the
monitoring system.
The Cell processor obtains its greater computational power at the cost of more complex programming. Therefore,
we also introduce our newly developed eigensolver in the present paper. The details of our Cell cluster system and the
eigenvalue solver, are described in the following sections (Sections 2 and 3). Moreover, the performance is evaluated
in Section 4.
2. Cell Cluster
2.1. PowerXCell 8i
PowerXCell 8i, which has a faster double precision computational unit than the original version, is a kind of Cell
processor[5]. An overview of PowerXCell 8i is shown in Fig. 2. In the ﬁgure, PPE denotes a Power PC Processor
Element. The PPE has a PPU that is a processing unit equivalent to a Power PC, and also includes a second level
cache memory. SPE denotes a Synergetic Processor Element, which consists of a 128 bit single instruction multiple
data processing unit (hereinafter referred to as SIMD), In earlier studies [6], the processing unit was called an SPU.
together with a local store (LS) and a memory ﬂow controller (MFC), which handles data transfer between LS and
main memory. Each SPE provides 12.8 GFLOPS in double precision, therefore the total performance reaches over
100 GFLOPS. The PPE, SPE, and main memory are connected with an Element Interconnect Bus (EIB). EIB has four
buses and its total bandwidth reaches 204.8 Gigabytes per second. Note that the total bandwidth of EIB includes not
only the data transfer between the processing unit and the main memory but also data transfer among processing units.
Therefore, we usually consider the practical bandwidth of PowerXCell 8i to be 25.6 Gigabytes per second, which is
the maximum access speed of main memory.
2.2. Clustering of QS22
For this study, we constructed a Cell cluster system using QS22[7] blades, (developed by IBM), together with the
Mpich2 library[8]. We illustrate an overview of our Cell cluster system in Fig. 3. QS22 contains two Cell processors
and both can access a common memory space; thus in total, sixteen SPEs are available in one QS22 blade. In addition,
two QS22s are connected by a gigabit Ethernet. The Message passing interface (MPI) speciﬁcation is the standard for
the communication interface for distributed memory parallel computing and the Mpich2 library is one of the most well
known implementations of MPI on commodity oﬀ the shelf clusters. Originally, the MPI speciﬁcation was developed
for a computer system with one processing unit and one main storage unit. This model is simple but not suitable for
a Cell processor, because the SPEs have their own memory and therefore do not recognize a change of data in main
memory. Thus, we combined two kinds of parallelization; the ﬁrst is parallelization among blades using Mpich2, and
the second is parallelization among SPEs. We observe, however, that a PPE only communicates to other blades using
Mpich2 and SPEs do not relate to communication. Moreover, the SIMD processing unit of SPE itself is a kind of

Noriyuki Kushida et al. / Procedia Computer Science 4 (2011) 898–907

QS22

QS22

SPE 6

SPE 7

SPE 5

Main
Memory

PowerXCell8i

SPE 5

SPE 4

SPE 3

SPE 2

SPE 1

SPE 0

SPE 7

SPE 6

PPE
SPE 5

SPE 4

SPE 3

SPE 7

Giga-bit
Ethernet

PPE
SPE 2

SPE 4

PPE

Main
Memory

SPE 1

SPE 6

PPE

SPE 3

SPE 2

SPE 1

SPE 0

SPE 7

PowerXCell8i
SPE 6

SPE 5

SPE 4

SPE 3

SPE 2

SPE 1

SPE 0

PowerXCell8i

SPE 0

901

PowerXCell8i

Figure 3: Overview of our Cell cluster system

parallel processor. Then we must consider three levels of parallelization, in order to obtain better performance of the
Cell cluster: (1) MPI parallel, (2) SPE parallel,(3) SIMD parallel
3. Eigensolver
Although there are numerous eigenvalue solver algorithms, only two are suitable for our purposes, because only
the smallest eigenvalue is required for our plasma stability analysis system. One candidate is the Inverse power
method, and the other is The conjugate gradient method (hereafter referred to as CG). The inverse power method is
quite simple and easy to implement; however, it requires solving the linear equation at every iteration step, which
is usually expensive in terms of time and memory. It is fortunate that the computational cost of lower/upper (LU)
factorization and backward/forward (BF) substitution of block tri-diagonal matrices is linear of order n. However, this
is just for the sequential case. We are forced to incur additional computational cost with parallel computing, especially
for MPI parallel. According to the articles[9][10], the computational cost of LU factorization increases with a small
number of processors and is at least twice as great as the sequential computational cost. In our estimation, such an
inﬂation of computational cost was not acceptable for our system. On the other hand, CG is basically well suited to
distributed parallel computing, in that the computational cost for one processor linearly decreases as the number of
processors that are actually used, increases. For these reasons, we employ CG as the eigenvalue solver. Details of the
conjugate gradient method, including parallelization and the convergence acceleration technique that we developed
are described in the following sections.
3.1. Preconditioned Conjugate Gradient
CG is an optimization method used to minimize the value of a function. If the function is given by
λ (x) =

(x, Ax)
.
(x, x)

(1)

the minimum value of λ (x)corresponds to the minimum eigenvalue of the standard eigensystem Ax = λx, and the
vector x is an eigenvector associated with the minimum eigenvalue. Here ( , ) denotes the inner product. The CG
algorithm, which was originally developed by Knyazev[11] and Yamada and others was modiﬁed[12][13] (as shown
in Algorithm 1). In the ﬁgure, T denotes the preconditioning matrix; the details are introduced later. Several variants
of the conjugate gradient algorithm have been developed and have been tested for stability. According to the literature, Knyazev’s algorithm achieved quite good stability by employing Ritz method, expressed as the eigenproblem for
S A v = μS B v, in the algorithm. Yamada’s algorithm is mathematically equivalent to Knyazev’s algorithm; however,

902

Noriyuki Kushida et al. / Procedia Computer Science 4 (2011) 898–907

it requires only one matrix-vector multiplication, which is one of the most time consuming steps of the algorithm,
whereas Knyazev’s algorithm requires three such multiplications. Therefore, in the present study, we employ Yamada’s algorithm. Let us consider the preconditioning matrix T . The basic idea of preconditioning is to transform the
coeﬃcient matrix close to the identity matrix by operating by an inverse of T that approximates the coeﬃcient matrix
A in some sense. Even if a higher degree of approximation of T to A provides a higher convergence rate for CG, we
usually stop short of achieving T = A, because the computational eﬀort can be extremely expensive. Additionally, an
inverse of T is not constructed explicitly because the computational eﬀort can also be large. Although the matrix T −1
appears in the algorithms, the algorithm only requires solving the linear equation. We usually employ triangular matrices, or some multiples thereof, for T , because we can such a system with Backward/Forward(BF) substitutions. It is
fortunate that complete LU factorization for block tri-diagonal matrices can be obtained at reasonable computational
cost; we employed complete LU factorization to construct the preconditioning matrix T .
Algorithm 1 Algorithm of the conjugate gradient method as introduced by Yamada et al.
1. Let x0 be the initial guess, and p0 := 0
2. x0 := x0 / x0
3. X0 := Ax0
4. μ−1 := (x0 , X0 )
5. W0 := X0 − μ−1 x0
6. for i = 0, 1, 2, 3, . . ., until convergence do
7.
Wk := Awk
8.
S A := {wk , xk , pk }T {Wk , Xk , Pk }
9.
S B := {wk , xk , pk }T {wk , xk , pk }
10.
Find the smallest eigenvalue μ and corresponding eigenvector v of S A v = μS B v,v = {v1 , v2 , v3 }
11.
μk := (μ + (xk + Xk ))/2
12.
xk+1 := v1 wk + v2 xk + v3 pk
13.
xk+1 := xk+1 / xk+1
14.
pk+1 := v1 wk + v3 pk
15.
pk+1 := pk+1 / pk+1
16.
Xk+1 := v1 Wk + v2 Xk + v3 Pk
17.
Xk+1 := Xk+1 / Xk+1
18.
Pk+1 := v1 Wk + v3 Pk
19.
Pk+1 := Pk+1 / Pk+1
20.
wk := T −1 (Xk+1 − μk xk+1 )
21.
wk+1 := wk+1 / wk+1
22. end for

3.2. Parallelization of the conjugate gradient method
Almost the entire algorithm of CG consists of the following three operations: (1)Matrix – vector multiplication,(2)Vector dot product,(3)Scalar – vector multiplication
Note that the normalization of vectors can be carried out using (2) and (3). Preconditioning is not discussed in this
section but is discussed later. As mentioned previously, three levels of parallelization have to be considered for the
Cell cluster. These three kinds of parallelization for these three operations are individually discussed in the following
sub-sections.
3.2.1. MPI Parallel
In this sub-section, the parallelization between two QS22 blades is considered. In the present study, this parallelization was achieved using MPI, therefore, we refer to this situation as MPI parallel. By considering the three
operations, we assigned the memory and vectors as shown in Fig. 4. We assumed that the matrix has 6 × 6 blocks.

Noriyuki Kushida et al. / Procedia Computer Science 4 (2011) 898–907

903

Blade1
x(1)

A(1,1) A(1,2)

Blade1
x(2)

A(2,1) A(2,2) A(2,3)
A(3,2) A(3,3) A(3,4)
A(4,3) A(4,4) A(4,5)

Blade2

x(3)

x(3)

x(4)

x(4)

A(5,4) A(5,5) A(5,6)

x(5)

A(6,5) A(6,6)

x(6)

Blade2

Figure 4: Memory conﬁguration of matrix and vector on our Cell cluster

The big square which is shown on the left of the ﬁgure is the matrix A and the two rectangles on the right are the
vector x. The hatched small squares are non-zero blocks and the white blocks are complete zero blocks. The non-zero
blocks have a location indicator. The blocks for matrices are m × m and for vectors are 1 × m. Firstly, we consider
matrix – vector multiplication. Since the matrix is used for matrix – vector multiplication, we divided the matrix by
rows; namely, the 1st to the 3rd rows of the matrix are stored in Blade 1, and the 4th to the 6th rows are stored in
Blade 2. This is also the same for the vector, (1st to 3rd components are in Blade 1 and 4th to 6th in Blade 2), but
extra storage area is allocated in order to carry out the matrix – vector operation. The reason for extra storage can be
simply explained. When we consider the multiplication of A by x, all the calculations except those for A(3, 4) can be
done with x(1), x(2) and x(3) on Blade 1, but then will never terminate if x(4) is absent. Therefore, x(4) must be sent
from Blade 2 to Blade 1 before the multiplication. In addition, the same situation occurs for Blade 2. Secondly, we
consider the vector dot product. The way to compute it is quite simple; we calculate partial sums locally on blades
and exchange them with each other. More precisely, considering vectors x and y, the total product S total can be written
as
S total = S Blade1 + S Blade2 ,
where

3

S Blade1 =

6

x(i)y(i), S Blade2 =
i=1

x(i)y(i),
i=4

It is obvious that both S Blade1 and S Blade2 can be computed on the local blade, and the total can be obtained just by
exchanging S Blade1 and S Blade2 . Scalar – vector multiplication can be performed with no communication.
3.3. SPE parallel
The QS22 blade has 16 SPEs and each SPE can run in parallel with each other. Originally, each SPE could
contain its own instruction set; however, we consider them to have the same instruction set in the present study. In
other words, we employ single instruction multiple data parallelization among SPEs. Firstly, we consider matrix –
vector multiplication. The computation of matrix – vector multiplication is performed in a block wise manner. In
detail, when y := Ax, is computed, each SPE computes y(i) := y(i) + A(i, j) ∗ x( j) as an unit. We observe that this
computation can be done in parallel with diﬀerent i. Next we consider the vector dot product. The strategy is quite
similar to MPI parallel; however, the parallelization is based on SPE. Each SPE computes part of the product and PPE
sums the results. Moreover, scalar – vector multiplication can be done just by subdividing the range of computation.
3.4. SIMD parallel
The SIMD processor is the processing unit that computes two or more ﬂoating point values at the same time
and SIMD parallel is the smallest parallelization unit in the present study. The SIMD processor provides better
computational performance than the traditional processing unit; however, there are two big hindrances to realizing
peak performance. One is that the SIMD processor always computes two ﬂoating-point values; otherwise we must

904

Noriyuki Kushida et al. / Procedia Computer Science 4 (2011) 898–907

1

2

3

4

1

C(1,1) C(1,2) C(1,3)

0

2

C(2,1) C(2,2) C(2,3)

0

3

C(3,1) C(3,2) C(3,3)

0

x(1)
x(2)
x(3)
0

Figure 5: Memory conﬁguration for SIMD processing

incur a huge penalty [6]. In other words, we have to consume more time to complete one ﬂoating-point calculation
than to make two ﬂoating-point calculations. In order to explain the penalty, we consider the quite simple operation
c := a+b. SPE needs around 60 clock cycles (In this case, data load and store operations are included). However, if we
apply a technique which enables us to use the SIMD processor, we obtain a result within 30 clock cycles. Therefore,
we incur twice as much time cost for one ﬂoating-point calculation as for an SIMD calculation.
The other point is that two ﬂoating point values that are processed must be arranged in contiguous address spaces.
In order to avoid the penalty, we added extra zeros (zero padding) when we cannot compute two ﬂoating point values.
In Fig. 5, matrix – vector multiplication is used as an example of zero padding. In the ﬁgure, the multiplication of
a 3 × 3 block matrix and the corresponding vector is considered. In this study, the block matrix is stored in a row
oriented form (the addresses of C(1, 1) and C(1, 2) are contiguous) , because the performance of LU factorization of
the block matrix, which is required for preconditioning and which dominates the total computational time, was better
than using storage in a column oriented form, in our preliminary test. Furthermore, the performance of the entire
matrix – vector multiplication is comparable between row and column oriented versions. Because the matrix is stored
in a row oriented form, the matrix – vector multiplication can be computed as an iteration of vector dot products. Now,
consider the computation of the ﬁrst row and vector x. The matrix originally has three components in a row, therefore,
The ﬁrst two components can be computed without diﬃculty. On the other hand, the third component is by itself and
extra zeros should be added just after the third component. This zero padding should be applied to the vector not only
in the case of matrix – vector multiplication but also for the vector dot product and scalar – vector multiplication.
This technique for the usage of the SIMD processor, as described here, could also be used for block-wise matrix –
matrix multiplication, LU factorization, and BF substitution, all of which are required for LU factorization of the
block tri-diagonal matrix.
3.5. LU factorization on parallel distributed memory
Let us consider LU factorization of a block tri-diagonal matrix, which is employed for preconditioning of CG in
the present study. As mentioned above, preconditioning is just an accelerator for CG convergence and we do not need
to achieve complete LU factorization. In the other words, an approximate LU can be acceptable for our purposes.
Then we have to consider the balance between the computational eﬀorts of preconditioning and the gain from the
acceleration of CG convergence. According to studies [9][10], the computational eﬀort for complete LU factorization
using distributed parallel memory is twice as much as it is on a single computer for the situation of our ﬁnal goal (for
which the number of diagonal blocks is 1024 and each block is 128 × 128). Furthermore, this parallelization requires
communication; therefore the total time of calculation can be longer than twice that of the sequential case. On the
other hand, we can obtain nearly complete LU factorization by ignoring a few blocks; when we ignore A(3, 4) and
A(4, 3) in Fig. 4, we obtain two block tri-diagonal matrices that are independent of each other. Since the absence of
these blocks disconnects the relation between the matrices on the blades, we refer to this technique as localization.
Under this realistic condition, we ignore just two of 3000 blocks. In addition, we observed that the convergence rate of
complete LU and locally complete LU are the same for our test problem (See section 4.3). Finally, we have employed
CG with locally complete LU preconditioning as the eigensolver.

Noriyuki Kushida et al. / Procedia Computer Science 4 (2011) 898–907

905

4. Performance Evaluation
In this section, we evaluate the performance of our eigensolver on the Cell cluster. Parallel performance of
each type of parallelization was examined, in order to determine their beneﬁcial eﬀects. Parallel performance with
SIMD was not examined, because the serialized computation that corresponds to the SIMD computation could not be
obtained.
Speed-up Ratio

Elaspsed Time

14

16

10
8
8

6
4

Speed-up Ratio

Elapsed Time ( sec. )

12

2
0

1 2

4

8

Number of SPEs

1
16

Figure 6: Computation time to CG convergence and speed-up ratio for various numbers of SPEs

4.1. SPE parallel
We evaluated the parallel performance of our solver with respect to SPE (inner QS22) parallel. In Fig. 6, the
computational time for CG convergence (referred to as elapsed time) is shown with circles and the speed-up ratio is
shown with squares. Here, the speed-up ratio S is deﬁned as S = t1 /tn , where tn is the elapsed time with n SPEs, and
t1 is the elapsed time with 1 SPE.
The evaluation was carried out for one QS22; thus the number of SPEs changes from 1 to 16. The properties of
the matrix are as follows. The number of diagonal blocks is 1024, and the size of each block is 128 × 128. Random
numbers were taken as the non-zero components of the matrix. We provided numbers that were distributed in the
range of 1.0 to 10.0 as diagonal components, 0.0 to 10−6 as non-diagonal components. We stopped the CG iteration
when the residual norm value became less than the criterion 1.0 × 10−2 .
It can be said that our parallel implementation strategy for SPE works well, because the elapsed time decreases
as the number of SPEs increases. However, the speed-up ratio is far from ideal. The speed-up ratio should be
linear with respect to the number of processors in the ideal case. This deterioration comes from a deﬁciency in
memory bandwidth. In our experience, the memory bandwidth of the Cell processor is not suﬃcient for some matrix
operations[14]. In extreme cases, we did not obtain the speed-up discussed in earlier articles. Considering these facts,
we conclude that our implementation as discussed above yields a suﬃciently good performance.
4.2. MPI parallel
In Fig. 7, the elapsed time and the speed-up ratio for several numbers of QS22s are shown. The elapsed time
decreases when the number of QS22s increasing; thus we conclude that our intra-QS22 parallel implementation is
successful. Moreover, the speed-up ratio is close to ideal; i.e., we can obtain more speed by adding extra QS22s.
4.3. Eﬀect of localization
We show the convergence histories of CG, Complete LU preconditioned CG, and Locally complete LU preconditioned CG, in Fig. 8. In this section, locally complete LU preconditioned CG is performed with two QS22s, and the
others are performed with one QS22. No obvious diﬀerence between complete LU and locally complete LU can be
observed. Moreover, preconditioning accelerates convergence, in fact, making it occur four times as fast as normal.

906

Noriyuki Kushida et al. / Procedia Computer Science 4 (2011) 898–907

Elapsed Time

1.6

Speed-up Ratio

4

1.4
3

1

Speed-up Ratio

Elapsed Time ( sec. )

1.2

0.8
0.6

2

0.4
0.2
0

1

2
3
Number of QS22s

4

1

Figure 7: Computation time to CG convergence and speed-up ratio for various numbers of QS22s

1e+1

Residual Norm

Locally Complete LU
Complete LU
Without Preconditioning
1e+0

1e-1

1e-2

0

10

20
30
40
Number of Iterations

50

Figure 8: Convergence history of several CG methods

5. Precision of the estimated eigenvalue
For the purpose of plasma monitoring, the time available for eigenvalue solving is one second. In this section, we
discuss the precision of the estimated eigenvalue that can be obtained within one second. In table 1, the numbers of
iterations which can be performed within one second and the error of the resulting minimum eigenvalue are tabulated.
In the table, we solved same problem by changing the number of QS22s. The error is deﬁned as |λtrue − λestimated |. As
can be observed in Fig. 8, the convergence behavior of our solver is smooth. Therefore, we can obtain a more precise
result if we use more computing power. In the present study, we can compute around 5 more iterations when we add
one extra QS22. Moreover, the estimated eigenvalue becomes more precise when we add QS22s. Finally, the error
was reduced to O(10−6 ), when we use four QS22s.
6. Conclusion
In the present paper, we introduced a high-speed eigenvalue solution system that is required for the plasma stability
analysis system of fusion reactors. We constructed the Cell cluster system using QS22 blades and the Mpich2 library
and developed the eigensolver taking into consideration the architecture of Cell processor. The eigensolver was based
on the CG, and locally complete LU factorization was employed for preconditioning. By considering the architecture
of the Cell cluster system, we developed three levels of parallelization: MPI parallel, SPE parallel, and SIMD parallel.
By use of these techniques, we achieved solution of the target eigensystem within the target time; we succeeded

Noriyuki Kushida et al. / Procedia Computer Science 4 (2011) 898–907

907

Table 1: Number of iterations and error of the estimated eigenvalue obtained within one second.

# of QS22
1
2
3
4

# of iterations
1
6
10
15

Error
1.71E-03
4.60E-05
2.31E-05
7.54E-06

to solving a system with 1024 diagonal blocks where each block was 128 × 128, within one second. This result
suggests the potential of using our Cell cluster system to carry out plasma monitoring. The algorithm we developed
should work well on GPGPU and homogeneous many-core processors like intel Xeon processors as well. However,
according to our preliminary estimation, Cell may show the best performance for the entire monitoring at the present
time. Additionaly, we are optimistic because the essentials of these new devices are the same and we can port our code
to them. We can select the best hardware on moment to moment basis. Another possible way is building a special
purpose hardware, but the cost seems a big problem[15].
To complete our stability analysis system, we consider the following two aspects as future work:
1. Conﬁrm the convergence behavior of CG for the actual MARG2D matrices and modify our solver to handle the
generalized eigenvalue problem. According to the article [11], the modiﬁcation required should be quite small.
2. Develop a high-speed matrix generation system on the Cell cluster. Matrix generation requires considerable computation time and the total time for both matrix generation and the eigensolver should be suﬃciently small. By
our estimation, the goal is to complete a simulation within two to three seconds and we now have a basis for
achieving this result.
Acknowledgment
We are grateful to JSPS for the Grant-in-Aid for Young Scientists (B), No. 21760701. We also thank Dr. M. Yagi
and Dr. N. Aiba for their fruitful advice on plasma stability analysis, and Mr. A. Tomita @ FIXSTARS Co. for his
extensive knowledge of the CELL.
[1] ITER project web page, available at http://www.iter.org/default.aspx
[2] S. Tokuda and T. Watanabe, ”A new eigenvalue problem associated with the two-dimensional Newcomb equation without continuous spectra”,
vol. 6, 3012, 1999.
[3] S.—Tokuda, ”Development of Eigenvalue Solver for an MHD Stability Analysis Code”, 5th Burning Plasma Simulation Initiative Annual
Meeting, 2006.
[4] N. KUSHIDA and H. OKUDA, ”Optimization of the Parallel Finite Element Method for the Earth Simulator”, Journal of Computational
Science and Technology, vol.2, No. 1, pp 81-90, 2008.
[5] Cell Broadband Engine processor based systems White Paper, available at http://www.irisa.fr/orap/Constructeurs/Cell/
cell be systems whitepaper.pdf
[6] M. Scarpino, ”Programming the Cell procssor for Games, Graphics, and Computation”, Pearson Education, 2009.
[7] Prodcut information of QS22, available at http://www-03.ibm.com/systems/info/bladecenter/qs22/
[8] MPICH2 web page: http://www.mcs.anl.gov/research/projects/mpich2/
[9] A. van der Ploeg, ”Reordering strategies and LU-decomposition of block tridiagonal matrices for parallel processing”, technical report NMR9618, CWI, October, 1996.
[10] A. van der Ploeg, ”Parallelization of a block tridiagonal solver in HPF on an IBM sp2”, Lecture Notes in Computer Science, vol 1401, pp
242–251, 1998.
[11] A. V. Knyazev, ”Toward the optimal eigensolver: Locally optimal block preconditioned conjugate gradient method”, SIAM J. Sci. Comput.,
23, pp. 517–541, 2001.
[12] S. Yamada, T. Imamura, and M. Machida, ”Preconditioned Conjugate Gradient Method for Large-scale Eigenvalue Problem of Quantum
Problem”, Transactions of JSCES, Vol. 2006, 20060027, 2006. (In Japanese)
[13] S. Yamada, T. Imamura, and M. Machida,”16.477 TFLOPS and 159-Billion-dimensional Exact-diagonalization for Trapped FermionHubbard Model on the Earth Simulator”, Proc. of SC2005, 2005.
[14] N. Kushida, H. Takemiya, ”Optimization of Finite Element Method for the Cell processor” Transacsions of JSCES, Vol. 2010, 20100002,
2010. (In Japanese)
[15] S. Tokuda,”Development of Eigenvalue Solver for an MHD Stability Analysis Code”, 5th Burning Plasma Simulation Initiative Annual
Meeting, available at http://bit.ly/f2Fqax

