Resource Management in a Multi-agent System
by Means of Reinforcement Learning
and Supervised Rule Learning
´ zy´
Bartlomiej Snie˙
nski
AGH University of Science and Technology, Institute of Computer Science
Krak´
ow, Poland
Bartlomiej.Sniezynski@agh.edu.pl

Abstract. In this paper two learning methods are presented: reinforcement learning and supervised rule learning. The former is a classical
approach to a learning problem in multi-agent systems. The latter is
a novel approach, according to the author’s knowledge, which has several advantages. Both methods are used for resource management in a
multi-agent system. The environment is a Fish Bank game, where agents
manage ﬁshing companies. Both learning methods are applied to generate ship allocation strategy. In this article the system architecture and
learning processes are described and experimental results comparing the
performance of implemented types of agents are presented.
Keywords: Machine learning, multi-agent systems, resource management.

1

Introduction

Resource allocation is a vital problem in multi-agent systems. Agents try to realize theirs goals in a way that is as good as possible, whereas a cooperation may
be necessary to avoid an exhaustion of resources and appearing a crisis situation.
One of the main problems in the development of such systems is designing an
appropriate strategy for resource allocation.
Applying learning algorithms allows to overcome such problems. One can
implement an agent that is not perfect, but improves its performance. This is
why machine learning term appears in a context of agent systems for several
years. There are many learning methods that can be used to generate knowledge
or strategy. Choosing an appropriate one, which ﬁts a given problem, sometimes
is a diﬃcult task.
In multi-agent systems the most common technique is reinforcement learning [1]. It allows to generate a strategy for an agent in a situation, when the
environment provides some feedback after the agent has acted. Feedback takes
the form of a real number representing reward, which depends on the quality of
the action executed by the agent in a given situation. The goal of the learning
is to maximize estimated reward.
Y. Shi et al. (Eds.): ICCS 2007, Part II, LNCS 4488, pp. 864–871, 2007.
c Springer-Verlag Berlin Heidelberg 2007

Resource Management in a Multi-agent System

865

Symbolic, supervised learning is not so widely used in multi-agent systems.
There are many methods belonging to this class that generate knowledge from
data. Here a rule induction algorithm is used. It generates a rule-based classiﬁer,
which assigns a class to a given example. As an input it needs examples, where
the class is assigned by some teacher. In the paper it is shown that it is possible
to generate such examples in the case of learning resource allocation. Using this
method instead of reinforcement learning has several advantages.
As an environment the Fish Banks game is used [2]. It is a simulation where
agents run ﬁshing companies that must decide how much, and where to ﬁsh.
In the following sections learning in the multi-agent systems is described,
developed system and both learning methods are presented, and experimental
results are analyzed.

2

Learning in Multi-agent Systems

The problem of learning in multi-agent systems may be considered as a union of
research on multi-agent systems and on machine learning. Machine learning focuses mostly on research on an isolated process performed by one intelligent module. The multi-agent approach concerns the systems composed of autonomous
elements, called agents, whose actions lead to the realization of given goals. In
this context, learning is based on the observation of the inﬂuences of activities,
performed to achieve the goal by an agent itself or by other agents. Learning
may proceed in a traditional – centralized (one learning agent) or decentralized manner. In the second case more than one agent is engaged in the learning
process [1].
So far agent systems with learning capabilities were applied in many domains:
to train agents playing in RoboCup Challenge [3], adapt user interfaces [4], take
part in agent-based computational economics simulations [5], analyze distributed
data [6].

3

System Description

The Fish Banks game is originally designed for teaching people eﬀective cooperation in using natural resources [7]. However; it suits using in multi-agent
systems very well [2,8]. The game is a dynamic environment providing all necessary resources, action execution procedures, and time ﬂow represented by game
rounds. Each round consists of the following steps: ships and money update,
ship auctions, trading session, ship orders, ship allocation, ﬁshing, ﬁsh number
update.
Agents represent players that manage ﬁshing companies. Each company aims
at collecting maximum assets expressed by the amount of money deposited at
a bank account and the number of ships. The company earns money by ﬁshing
at ﬁsh banks. The environment provides two ﬁshing areas: coastal and deep-sea.
Agents can also keep their ships at the port. The cost of deep-sea ﬁshing is the
highest. The cost of staying at the port is the lowest but such ship does not

866

´ zy´
B. Snie˙
nski

catch ﬁsh. Initially, it is assumed that the number of ﬁsh in both banks is close
to the bank’s maximal capacity. Therefore, at the beginning of game deep-sea
ﬁshing is more proﬁtable.
Usually exploration overcomes birth and after several rounds the number can
decrease to zero. It is a standard case of ”the tragedy of commons” [9]. It is more
reasonable to keep ships at the harbor then, therefore companies should change
theirs strategies.
3.1

Agents

Four types of agents are implemented: reinforcement learning agent, rule learning
agent, predicting agent, and random agent. The ﬁrst one uses learned strategy to
allocate ships, the second one uses rules induced from the experience to classify
actions and chose the best one, third agent type uses previous ﬁshing results
to estimate values of diﬀerent allocation actions, the last one allocates ships
randomly.
All types of agents may observe the following aspects of the environment:
arriving of new ships bought from a shipyard, money earned in the last round,
ships allocations of all agents, and ﬁshing results for deep sea and inshore area.
All types of agents can execute the following two types of actions: order ships,
allocate ships.
Order ships action is currently very simple. It is implemented in all types of
agents in the same way. At the beginning of the game every agent has 10 ships.
Every round, if it has less then 15 ships, there is 50% chance that it orders two
new ships.
Ships allocation is based on the method used in [2]. The allocation action is
represented by a triple (h, d, c), where h is the number of ships left in a harbor,
d and c are numbers of ships sent to a deep sea, and a coastal area respectively.
Agents generate a list of allocation strategies for h = 0%, 25%, 50%, 75%, and
100% of ships that belong to the agent. The rest of ships (r) is partitioned; for
every h the following candidates are generated:
1. All: (h, 0, r), (h, r, 0) – send all remaining ships to a deep sea or coastal area,
2. Check: (h, 1, r − 1), (h, r − 1, 1) – send one ship to a deep sea or coastal area
and the rest to the other,
3. Three random actions: (h, x, r − x), where 1 ≤ x < r is a random number –
allocate remaining ships in a random way,
4. Equal: (h, r/2, r/2) – send equal number of ships to both areas.
The random agent allocates ships using one of the candidates chosen by random. Predicting agent uses the following formula to estimate the value of the
action:
(1)
v(a) = income(a) + η ecology(a),
where income(a) represents the prediction of the income under the assumption
that in this round ﬁshing results will be the same as in the previous round,
ecology(a) represents ecological eﬀects of the action a (the value is low if ﬁshing

Resource Management in a Multi-agent System

867

begin
if it is the ﬁrst game then Q := 0;
a1 := random action; execute a1 ;
foreach round r > 1 do
xr := current state;
if random() > the probability of exploration then a := random action;
else ar := arg maxa Q(a, xr ); qr := Q(ar , xr );
execute ar ; xr+1 := state after action execution;
i:= income (money earned by ﬁshing - costs);
Qmax
r+1 := maxa Q(a, xr+1 );
Δ := γQmax
r+1 + I − Q;
Q(ar , xr ) := Q(ar , xr ) + βΔ;
end
end
Fig. 1. Algorithm used by the reinforcement learning agent

is performed in the area with low ﬁsh population), and η represents importance
of the ecology factor.
3.2

Learning Agents

Reinforcement learning agent chooses action by random in the ﬁrst round. In
the following rounds, action with the highest predicted value (Q) is chosen. Q is
a function that estimates value of the action in a given state:
Q: A×X → ,

(2)

where A = {(h, d, c) : h, d, c ∈ {0%, 25%, 50%, 75%100%}, d + c = 1} is a set of
all possible ship allocation actions, and X = {(dc, cc) : dc ∈ {1, 2, . . . 25}, cc ∈
{1, 2, . . . , 15}} is a set of states, which represent catch in both areas in the
previous round. As we can see, the set of ship allocation actions is diﬀerent
than one of the other agents. Q-learning algorithm [10] is used to update the Q
function after each round (beginning from the second one to have a catch data).
The algorithm of the agent is presented in Fig. 1.
At the beginning Q is initialized as a constant function 0. To provide suﬃcient
exploration, in a game number g a random action is chosen with probability 1/g
(all actions have the same probability then). Therefore random or the best action
(according to Q function) is chosen and executed. Income, which represents
feedback from the environment, is calculated, and Q is updated taking into
account the reward. The γ ∈ [0, 1] parameter represents the importance of the
future rewards, β ∈ (0, 1) is used to control the speed of change of Q.
The rule learning agent also randomizes actions in the ﬁrst game, but in the
following games it chooses action with the highest rating. The action rating
is generated using rules, which are stored in the KB. Rules allow to classify
the allocation as good or bad taking into account allocation parameters and
environment parameters (ﬁsh catch at the deep sea and at the coastal area in
the previous round). The algorithm of the agent is presented in Fig. 2.

868

´ zy´
B. Snie˙
nski

begin
if it is the ﬁrst game then KB := ∅; T := ∅;
foreach round r do
if it is the ﬁrst game or round then a:= random action;
else a:= action with the highest rating using KB;
execute a; observe actions of other agents and results;
T := T ∪ {(best-action-data, good), (worst-action-data, bad)};
end
KB := generate knowledge from training examples T ;
end
Fig. 2. Algorithm used by the rule learning agent

Every action a gets a value v according to the formula:
v(a) = α good(a) − bad(a),

(3)

where good(a) and bad(a) are numbers of rules, which match the action and
current environment parameters, with consequence good and bad, respectively,
and α is a weight representing the importance of rules with consequence good.
If there is more then one action with the same value, one occurring earlier in
the list is chosen. As a consequence, actions with smaller h are preferred.
Training examples are generated from agents observations. Every round the
learning agent stores ship allocations of all agents, and the ﬁsh catch in the
previous round. The action of an agent with the highest income is classiﬁed as
good, and the action of an agent with the lowest income is classiﬁed as bad. If
in some round all agents get the same income, none action is classiﬁed, and as a
consequence, none of them is used in learning.
At the end of each game the learning agent uses training examples T , which
were generated during all games played so far, to learn a new classiﬁer (KB),
which is used in the next game.
To support rule induction the AQ21 program is used [11]. It is a machine
learning software that allows to generate attributional calculus rules for given
examples. In standard mode it produces a complete and consistent description of
the data, but it can also provide rules that are not complete and/or consistent.
The main advantage of this program is that the generated knowledge is easy to
interpret for human what makes the experimental results easier to check and can
be useful in Fish Bank application to teach people. Of course, other methods of
rule (or even classiﬁer) learning can be used.
3.3

Implementation

The software used in experiments is written in Prolog, using Prologix compiler [12]. Every agent is a separate process. It can be executed on a separate
machine. Agents communicate with the environment using Linda blackboard.

Resource Management in a Multi-agent System

869

Prologix is an extension of BinProlog that has many powerful knowledge-based
extensions (e.g. agent language LOT, Conceptual Graphs and KIF support).

4

Experimental Results

Three series of experiments were performed to test how learning inﬂuences the
agent’s performance. Four agents took part in every series. Each series consisted
of the sequence of 15 games and it was repeated ten times.
In the ﬁrst series there were three random agents and one reinforcement learning agent (with γ = 1 and β = 0.1). The performance of agents measured as a
balance at the end of every game is presented in Fig. 3-a.
In the second series there were three random agents and one rule learning
agent (with weight α = 1). The performance of these agents is presented in
Fig. 3-b.
In the third series one learning (α = 1), one predicting and two random agents
were used. The performance of agents is presented in Fig. 3-c.
In all experiments average balance of both types of learning agents increases
with the agent’s experience, while the performance of the predicting and random
agents decreases slightly (because of the learning agents competition). Reinforcement learning agent was a little bit worse then a rule learning agent, but tuning
of its parameters (β, γ) and taking into account actions of other agents during
learning should increase its performance.
Examples of rules learned are presented in Fig. 4. Capital letters represent
variables that can be uniﬁed with any value. Predicate member checks if its ﬁrst
argument belongs to the list that is a second argument. It is used to represent
an internal disjunction (expression of the form x = v1 ∨ v2 ∨ . . . ∨ vn ). Remaining predicates represent the catch in the previous round (prevCatchDeep,
prevCatchCoastal) and action (harbor, deep_coastal_ratio).
These rules (in the form of clauses) can be interpreted in the following way.
Clause (a): it is a bad decision to keep at a harbor 25, 50, or 75 percent of
ships if the previous catch at deep-sea is greater or equal to 16, and the previous
catch at coastal area is 10.
Clause (b): it is a good decision to send 100% ships to a deep sea or 75% to
a deep sea and 25% to a coastal area if previous catch at deep-sea is greater or
equal to 18, and smaller or equal to 21, and previous catch at coastal area is
smaller or equal to 10.
Experimental results show that the rule learning agent performance increases
rapidly at the beginning of the learning process, when generated rules are used
instead of a random choice. Next it increases slowly, because new examples do
not contain any signiﬁcant knowledge. The performance stabilizes at the end of
the process.
As we can see in Fig. 3-c, the predicting agent performs better then the
learning agent. It suggests, that there is a space for improvement of the learning
method. Further research is necessary to check if it is possible to learn such a
good strategy.

´ zy´
B. Snie˙
nski

870

Balance at the end of game [K$]

(a)
35
30
25
RLA
RA1
RA2
RA3

20
15
10
5
0
1

2

3

4

5

6

7

8

9 10 11 12 13 14 15

Game nr

(b)

(c)
40

35
30
25

LA
RA1
RA2
RA3

20
15
10
5
0
1

2

3

4

5

6

7

8

9

10 11 12 13 14 15

Balance at the end of game [K$]

Balance at the end of game [K$]

40

35
30
25
LA
PA
RA1
RA2

20
15
10
5
0
-5

1

2

3

4

5

6

7

8

9

10 11 12 13 14 15

Game nr

Game nr

Fig. 3. Comparison of performance of reinforcement learning agent (RLA), rule learning (LA) and other agents using random strategy of ship allocation (RA1, RA2, RA3)
or prediction (PA); values for learning and predicting agents are presented with the
standard deviation
(a)
rate(bad) :harbor(B),
member(B,[25,50,75]),
prevCatchDeep(C),
C >= 16,
prevCatchCoastal(10).

(b)
rate(good) :deep_coastal_ratio(B),
member(B,[100%-0%,75%-25%]),
prevCatchDeep(C),
C >= 18,
C =< 21,
prevCatchCoastal(D),
D =< 10.

Fig. 4. Examples of rules (in the form of Prolog clauses) learned by the agent

5

Conclusion and Further Research

As we can see, both learning algorithms can be applied for learning resource allocation in a multi-agent system. Their performance is much better then a random
strategy, but there is still a space for improvement. Both of the techniques have
some advantages and disadvantages.
These two methods use diﬀerent knowledge representation. Reinforcement
learning uses the action value function, which is diﬃcult to analyze especially
in a case of a large domain. Rules are usually much easier to interpret (unless

Resource Management in a Multi-agent System

871

there are too many of them). Therefore, if learned knowledge is analyzed by a
human, rule induction seems to be a better choice.
A disadvantage of reinforcement learning is necessity of tuning its parameters
(γ, β, and exploration method). The choice has a high impact on the results.
What is more, due to necessary exploration, the algorithm’s performance is less
stable (there is a high variance).
On the other hand, reinforcement learning works well even if the reward is
delayed. Additionally, it does not need information about other agents’ actions.
Hence it is more universal. Rule learning can be modiﬁed not to use this data,
but it will probably result in slower learning. It will be tested in the future.
Other future works will concern applying other learning algorithms, and cooperation learning. Also applying both methods at the same time for diﬀerent
aspects seems to be an interesting issue.

References
1. Sen, S., Weiss, G.: Learning in multiagent systems. In Weiss, G., ed.: A Modern
Approach to Distributed Artiﬁcial Intelligence. The MIT Press (1999)
2. Kozlak, J., Demazeau, Y., Bousquet, F.: Multi-agent system to model the ﬁshbanks
game process. In: The First International Workshop of Central and Eastern Europe
on Multi-agent Systems (CEEMAS’99), St. Petersburg (1999)
3. H.Kitano, M.Tambe, P.Stone, M.Veloso, S.Coradeschi, Osawa, E., Matsubara, H.,
Noda, I., Asada, M.: The RoboCup synthetic agent challenge 97. In: International
Joint Conference on Artiﬁcial Intelligence (IJCAI97), Nagoya, Japan (1997) 24–29
4. Lashkari, Y., Metral, M., Maes, P.: Collaborative interface agents. In: AAAI.
(1994) 444–449
5. Tesfatsion, L.: Agent-based computational economics: Growing economies from
the bottom up. Artiﬁcial Life 8 (1) (2001) 55–82
6. Stolfo, S.J., Prodromidis, A.L., Tselepis, S., Lee, W., Fan, D.W., Chan, P.K.: Jam:
Java agents for meta-learning over distributed databases. In: KDD. (1997) 74–81
7. Meadows, D., Iddman, T., Shannon, D.: Fish Banks, LTD: Game Administrator’s Manual. Laboratory of Interactive Learning, University of New Hampshire,
Durham, USA (1993)
8. Sniezynski, B., Kozlak, J.: Learning in a multi-agent approach to a ﬁsh bank
game. In: Multi-Agent Systems and Applications IV: Proc. of CEEMAS 2005.
Volume 3690 of Lecture Notes in Computer Science. (2005) 568–571
9. Hardin, G.: The tragedy of commons. Science 162 (1968) 1243–1248
10. Watkins, C.J.C.H.: Learning from Delayed Rewards. PhD thesis, King’s College,
Cambridge (1989)
11. Wojtusiak, J.: AQ21 User’s Guide. Reports of the Machine Learning and Inference
Laboratory, MLI 04-3. George Mason University, Fairfax, VA (2004)
12. A. Majumdar, P. Tarau, J.S.: Prologix: Users guide. Technical report, VivoMind
LLC (2004)

