Available online at www.sciencedirect.com

ScienceDirect

This space is reserved for the Procedia header,
Procedia is
Computer Science
108C Procedia
(2017) 1281–1291
This
This space
space is reserved
reserved for
for the
the Procedia header,
header,
This space is reserved for the Procedia header,

A
A
A
A

do
do
do
do

not
not
not
not

use
use
use
use

it
it
it
it

International Conference on Computational Science, ICCS 2017, 12-14 June 2017,
Zurich, Switzerland

Hash
Hash
Hash
Hash
1
2
2
2
2

Based
Based
Based
Based

Method for Large Scale Nonparallel
Method
for
Scale
Method
for Large
Large Prediction
Scale Nonparallel
Nonparallel
Vector
Machines
Method
for Large Prediction
Scale Nonparallel
Vector
Vector Machines
Machines
Prediction
1,2,3
Xuchan Machines
Ju1,2,3 and Tianhe
Wang4∗
Vector
Prediction
4∗
Xuchan Ju1,2,3 and Tianhe Wang4∗
Xuchan Ju
Xuchan Ju

Support
Support
Support
Support

and Tianhe Wang
and Tianhe Wang

Postdoctoral Programme of Agricultural
Bank of China, Beijing
100005, China.
1,2,3
4∗

1
Postdoctoral
Programme
of
Bank
China,
100005,
China.
1
School
of Economics
and Management,
Tsinghua
University,
Beijing
100083,
China.
Postdoctoral
Programme
of Agricultural
Agricultural
Bank of
of
China, Beijing
Beijing
100005,
China.
3
School
of
Economics
and
Management,
Tsinghua
University,
Beijing
100083,
China.
Institute
of
Internet
industry,
Tsinghua
University,
Beijing
100083,
China.
1
School
of Economics
and Management,
Tsinghua
University,
Beijing
100083,
China.
Postdoctoral
Programme
of Agricultural
Bank
of
China,
Beijing
100005,
China.
3
Tsinghua
University,
Beijing
100083,
China.
3 Institute of Internet industry,
juxuchan@126.com
Institute
of
Internet
industry,
Tsinghua
University,
Beijing
100083,
China.
School
of Economics
and
Management,
Tsinghua
University,
Beijing
100083,
China.
4
juxuchan@126.com
Beijing Branch, Bank
of Nanjing, Beijing 100036, China.
3
juxuchan@126.com
Institute
4 of Internet industry, Tsinghua University, Beijing 100083, China.
of Nanjing, Beijing 100036, China.
4 Beijing Branch, Bank
yangpu617@126.com
4

Beijing Branch, Bank
of Nanjing, Beijing 100036, China.
juxuchan@126.com
yangpu617@126.com
yangpu617@126.com
Beijing Branch, Bank of Nanjing, Beijing 100036, China.
yangpu617@126.com

Abstract
Abstract
Recent
years have witnessed more and more success of hash methods for building efficient clasAbstract
Recent
years
have
more
and
success
for
building
classifiers,
but
forwitnessed
prediction
in machine
thismethods
paper, we
hash based
Recent
yearsless
have
witnessed
more
and more
morelearning.
success of
ofInhash
hash
methods
for propose
building aefficient
efficient
clasAbstract
sifiers,
but
less
for
prediction
in
machine
learning.
In
this
paper,
we
propose
a
hash
based
method
for
large
scale
nonparallel
support
vector
machine
prediction(HNPSVM).
Our
key
idea
sifiers,
but
less
for
prediction
in
machine
learning.
In
this
paper,
we
propose
a
hash
based
Recent years have witnessed more and more success of hash methods for building efficient clasmethod
for
large
scale
nonparallel
support
vector
machine
prediction(HNPSVM).
Our
key
idea
of
this
method
is
that
we
use
an
approximal
decision
function
instead
of
exact
decision
function
method
for
large
scale
nonparallel
support
vector
machine
prediction(HNPSVM).
Our
key
idea
sifiers, but less for prediction in machine learning. In this paper, we propose a hash based
of
this
method
is
that
we
use
an
approximal
decision
function
instead
of
exact
decision
function
by
computing
the
Hamming
distance
between
hashing
the
normal
to
the
hyperplane
of
the
of
this
method
is
that
we
use
an
approximal
decision
function
instead
of
exact
decision
function
method for large scale nonparallel support vector machine prediction(HNPSVM). Our key idea
by
computing
the
Hamming
distance
between
hashing
the
normal
to
the
hyperplane
of
the
classifier
and
the
features.
This
method
benefits
nonparallel
support
vector(NPSVM)
predicby
computing
the
Hamming
distance
between
hashing
the
normal
to
the
hyperplane
of
the
of this method is that we use an approximal decision function instead of exact decision function
classifier
and
the
features.
This
method
benefits
nonparallel
support
vector(NPSVM)
prediction
in
three
aspects.
First,
it
enhances
the
prediction
accuracy
using
an
flexible
and
general
classifier
and
the
features.
This
method
benefits
nonparallel
support
vector(NPSVM)
predicby computing the Hamming distance between hashing the normal to the hyperplane of the
tion
in
aspects.
it
thereduce
prediction
accuracy
using
an
flexible
and
method.
Second,
the First,
proposed
HNPSVM
storage
cost
owing
to
compact
binary
tion
in three
three
aspects.
First,
it enhances
enhances
prediction
accuracy
usingvector(NPSVM)
an the
flexible
and general
general
classifier
and
the features.
This
method the
benefits
nonparallel
support
predicmethod.
Second,
the
proposed
HNPSVM
reduce
storage
cost
owing
to
the
compact
binary
hash
representation.
Last,
HNPSVM
can
speed
up
the
computation
of
classification
function.
method.
Second,
the
proposed
HNPSVM
reduce
storage
cost
owing
to
the
compact
binary
tion in three aspects. First, it enhances the prediction accuracy using an flexible and general
hash
representation.
Last,
HNPSVM
can
speed
up
the
computation
of
classification
function.
Moreover,
we
prove
that
the
classification
results
of
a
hash
based
NPSVM
classifier
converge
hash
representation.
Last,
HNPSVM
can
speed
up
the
computation
of
classification
function.
method. Second, the proposed HNPSVM reduce storage cost owing to the compact binary
Moreover,
we
prove
the
classification
results
ofnumber
aa hash
based
NPSVM
classifier
converge
to
therepresentation.
results
the that
exact
NPSVM
classifier
as the
of binary
functions
tends to
Moreover,
we of
prove
that
classification
results
hash
based
NPSVM
classifier function.
converge
hash
Last,the
HNPSVM
can speed
upofthe
computation
ofhash
classification
to
the
results
of
the
exact
NPSVM
classifier
as
the
number
of
binary
hash
functions
tends
infinity.
Several
experiments
on
large
scale
data
sets
show
the
efficient
of
our
method.
to
the
results
of
the
exact
NPSVM
classifier
as
the
number
of
binary
hash
functions
tends to
to
Moreover, we prove that the classification results of a hash based NPSVM classifier converge
infinity.
Several
experiments
on
large
scale
data
sets
show
the
efficient
of
our
method.
infinity.
Several
experiments
on
large
scale
data
sets
show
the
efficient
of
our
method.
to2017
the The
results
of scale
the
exact
NPSVM
classifier
as support
the number
binary hash
functions tends to
Keywords:
Large
data by
sets,
Hash,B.V.
Nonparallel
vectorofmachine,
Prediction
©
Authors.
Published
Elsevier
Keywords:
Large
scale
data
sets,
Hash,
Nonparallel
support
vector
machine,
Prediction
Peer-review
under
responsibility
of
the
scientific
committee
of
the
International
Conference
on
Computational
infinity.
Several
experiments
on
large
scale
data
sets
show
the
efficient
of
our
method. Science
Keywords: Large scale data sets, Hash, Nonparallel support vector machine, Prediction
Keywords: Large scale data sets, Hash, Nonparallel support vector machine, Prediction

1 Introduction
1
1 Introduction
Introduction
Recent explosion of data brings a great challenge for traditional classification algorithms in
1
Introduction
Recent
dataincreasing
brings a scale
greatofchallenge
traditional
classification
algorithms
in
machineexplosion
learning. of
With
data sets,formuch
larger memory
and longer
runtime

Recent explosion of data brings a great challenge for traditional classification algorithms in
machine
learning.
With
scalebut
of data
much larger
memory
and
longer
runtime
are
required
not only
in increasing
training
also sets,
in prediction
phase.
However,
improved
machine
learning.
With
increasing
data
sets,
larger
memory
and most
longer
runtime
Recent
explosion
of
data
brings phase
a scale
greatofchallenge
formuch
traditional
classification
algorithms
in
are
required
not
only
in
training
phase
but
also
in
prediction
phase.
However,
most
improved
algorithms
are
designed
for
train
classifiers
on
such
huge
amounts
of
data
and
less
for
speeding
are required
not only
in increasing
training phase
also sets,
in prediction
phase.
However,
machine
learning.
With
scalebut
of data
much larger
memory
and most
longerimproved
runtime
algorithms
are
designed
for train
on such
huge
of
and
for
speeding
up
the
prediction
phase.
algorithms
are
train classifiers
classifiers
such
huge amounts
amounts
of data
data
and less
less
for improved
speeding
are
required
notdesigned
only
in for
training
phase buton
also
in prediction
phase.
However,
most
up
the
prediction
phase.
A
notable
similarity
search
techniques
called
hashing
in
many
real
applications
[6,
18,
2, 19,
up the prediction
phase.for train classifiers on such huge amounts of data and less for speeding
algorithms
are designed
A
notable
similarity
search
techniques
called
hashing
in
many
real
applications
[6,
18,
2, 19,
21,
23]
have
been
used
in
training
classifiers[9,
11,
10].
The
main
idea
of
hashing
techniques
A notable
similarity
up the
prediction
phase.search techniques called hashing in many real applications [6, 18, 2, 19,
21,
23]
have
been
used
in
training
classifiers[9,
11,
10].
The
main
idea
of
hashing
techniques
21,∗A
23]notable
have been
used in
training
classifiers[9,
10]. in
The
main
idea
of hashing[6,techniques
similarity
search
techniques
called 11,
hashing
many
real
applications
18, 2, 19,
Corresponding
author(Tianhe
Wang)
author(Tianhe
Wang) classifiers[9, 11, 10]. The main idea of hashing techniques
21,∗∗ Corresponding
23]
have
been
used
in
training
Corresponding author(Tianhe Wang)
∗ Corresponding

author(Tianhe Wang)

1877-0509 © 2017 The Authors. Published by Elsevier B.V.
Peer-review under responsibility of the scientific committee of the International Conference on Computational Science
10.1016/j.procs.2017.05.133

1
11
1

1282	

HNPSVM

Xuchan
Ju and Tianhe Wang
Xuchan Ju et al. / Procedia Computer Science 108C (2017)
1281–1291

is to use binary code representation for data with expectation of preserving the neighborhood
structure measured from the original feature space into a Hamming space, where the storage cost
and time for training can be substantially reduced. However, less efforts of hashing technique are
spent in the counterpart scalability issue: how can big trained classifiers be efficiently applied
on the prediction phase using hashing techniques.
Inspired by applying hash techniques into support vector classifier(SVC) for prediction[12],
we give an algorithm about how to apply hash techniques into an exact classifier, nonparallel
support vector machine(NPSVM)[16, 15, 17], for prediction. The basic idea of our method is
that we use an approximal decision function instead of exact decision function by computing
the Hamming distance between hashing the normal to the hyperplane of the classifier and the
features.
The proposed hash based method for nonparallel support vector machines prediction(HNPSVM) framework opens the door of tackling large scale data sets prediction by NPSVM. This
framework has several desirable properties. First, it enhances the prediction accuracy using an
flexible and general method, NPSVM. Second, it reduces storage cost owing to the compact
binary hash representation. Third, HNPSVM can speed up the computation of classification
function. Moreover, we prove that the classification results of a hash based NPSVM classifier
converge to the results of the exact NPSVM classifier as the number of binary hash functions
tends to infinity.
The structure of this paper is organized as follows: in section 2, the background material
is given and in section 3 we introduce linear NPSVM and HNPSVM. In section 4, we discuss
the property and efficiency of HNPSVM. Experiments are reported in section 5. Finally We
conclude our work in section 6.

2

Related Work

In this section, we give several methods working on linear and nonlinear SVM for speeding up
prediction time.
Linear SVM for fast prediction: Locally Linear SVM(LLSVM)[8] shows each sample
is represented by a linear combination of its neighbors to speed up prediction phase. Hashbased support vector machines approximation for large scale prediction[12] via locality sensitive
hashing to build efficient hash-based classifiers that are applied in a first stage in order to
approximate the exact results and filter the hypothesis space.
Nonlinear SVM for fast prediction: In order to reduce the size of the training set, fewer support vectors in the model can result in faster prediction speed. Squashing approach[13]
use clustering and grouping nearby points to reduce the size of training set. To improve both
training and prediction speed, Nyström method[20, 22] which form kernel approximations are
proposed. Some approaches are designed to reduce support vectors in the testing phase. Incremental greedy method[14] can achieve a better performance. Recently, an adaptive kernel
approximation for large-scale data sets[1] use randomized algorithm for speeding up prediction time. Local Deep Kernel Learning(LDKL)[5] learns a tree-based primal feature which can
efficiently encode non-linearities while speeding up prediction exponentially. DC-pred++[4]
method adds ”pseudo landmark points” to the Nyström approximation to speeding up prediction time without much additional prediction cost under a divide-and-conquer framework.
2

	

HNPSVM

3
3.1

Xuchan
Ju and Tianhe Wang
Xuchan Ju et al. / Procedia Computer Science 108C (2017)
1281–1291

Hash Based Method for Large Scale Nonparallel Support Vector Machine Prediction(HNPSVM)
Linear Nonparallel Support Vector Machine

Consider the binary classification problem with the training set
T = {(x1 , +1), · · · , (xp , +1), (xp+1 , −1), · · · , (xp+q , −1)},

(1)

(w+ · x) + b+ = 0 and (w− · x) + b− = 0

(2)

where xi ∈ Rn , i = 1, · · · , p + q. Let A = (x1 , · · · , xp ) ∈ Rp×n , B = (xp+1 , · · · , xp+q ) ∈
Rq×n , and n = p + q. NPSVM seeks two nonparallel hyperplanes

by solving two convex quadratic programming problems (QPPs):

min

(∗)

w+ ,b+ ,η+ ,ξ−

s.t.

p
p+q


1
2
∗
w+  + C1
(ηi + ηi ) + C2
ξj ,
2
i=1
j=p+1

(w+ · xi ) + b+  ε + ηi , i = 1, · · · , p,
− (w+ · xi ) − b+  ε + ηi∗ , i = 1, · · · , p,
(w+ · xj ) + b+  −1 + ξj ,

(3)

j = p + 1, · · · , p + q,

ηi , ηi∗  0, i = 1, · · · , p,
ξj  0, j = p + 1, · · · , p + q,
and
min

(∗)

w− ,b− ,η− ,ξ+

s.t.

p
p+q


1
2
∗
w−  + C3
ξj ,
(ηi + ηi ) + C4
2
j=1
i=p+1

(w− · xi ) + b−  ε + ηi ,
i = p + 1, · · · , p + q,

− (w− · xi ) − b−  ε + ηi∗ ,
i = p + 1, · · · , p + q,
(w− · xj ) + b−  1 − ξj , j = 1, · · · , p,

(4)

ηi , ηi∗  0, i = p + 1, · · · , p + q,
ξj  0, j = 1, · · · , p,

where xi , i = 1, · · · , p are positive inputs, and xi , i = p + 1, · · · , p + q are negative inputs,
(∗)
Ci  0, i = 1, · · · , 4 are penalty parameters, ξ+ = (ξ1 , · · · , ξp ) , ξ− = (ξp+1 , · · · , ξp+q ) , η+ =
(∗)
 ∗ 
 ∗ 
∗
∗
, η+ ) = (η1 , · · · , ηp , η1∗ , · · · , ηp∗ ) , η− = (η−
, η− ) = (ηp+1 , · · · , ηp+q , ηp+1
, · · · , ηp+q
) ,
(η+
are slack variables.
In order to get the solutions of problems (3) and (4), we need to solve their dual problems:
1 
θ Λθ + κ θ,
2
s.t. e θ = 0,
0  θ  C̄,

min
θ

(5)

3

1283

1284	

Xuchan
Ju and Tianhe Wang
Xuchan Ju et al. / Procedia Computer Science 108C (2017)
1281–1291

HNPSVM

where

e=

∗

 

 
, α+
, β−
) , κ = (εe
θ = (α+
+ , εe+ , −e− ) ,


 
(−e
+ , e+ , −e− ) , C̄

Λ=



H1
H2

H2
H3

=


 
(C1 e
+ , C1 e + , C2 e − ) ,




AA −AA
,
, H1 =
−AA AA


AB 
H2 =
, H3 = BB  ,
−AB 


(6)
(7)

(8)

and
1 
γ Λγ + κ γ,
2
s.t. e γ = 0,

min
γ

(9)

0  γ  C̃,
where

e=


 
∗

 
) , κ = (εe
, α−
, β+
γ = (α−
− , εe− , −e+ ) ,


 
(−e
− , e− , −e+ ) , C̃

Λ=



Q1
Q
2

Q2
Q3

=


 
(C3 e
− , C3 e − , C4 e + ) ,





BB  −BB 
, Q1 =
,
−BB  BB 


BA
Q2 =
, Q3 = AA ,
−BA

(10)
(11)

(12)

By solving (5), we can get (w+ ,b+ ).
w+ =

p

i=1

(αi∗ − αi )xi −

p+q


βj xj

(13)

j=p+1

Then choose a component of α+ , α+j ∈ (0, C1 ), compute
b+ = −(w+ · xj ) + ε

(14)

∗
∗
, α+k
∈ (0, C1 ), compute
or choose a component of α+

b+ = −(w+ · xk ) − ε

(15)

or choose a component of β− , β−m ∈ (0, C2 ), compute
b+ = −(w+ · xm ) − 1

(16)

By solving (9), we can get (w− ,b− ).
w− =
4

p+q


i=p+1

(αi∗ − αi )xi −

p

j=1

βj xj

(17)

	

HNPSVM

Xuchan
Ju and Tianhe Wang
Xuchan Ju et al. / Procedia Computer Science 108C (2017)
1281–1291

Then choose a component of α+ , α+j ∈ (0, C3 ), compute
b− = −(w− · xj ) + ε

(18)

∗
∗
or choose a component of α+
, α+k
∈ (0, C3 ), compute

b− = −(w− · xk ) − ε

(19)

or choose a component of β− , β−m ∈ (0, C4 ), compute
b− = −(w− · xm ) − 1

(20)

Then construct the decision functions
f+ (x) = w+ · x + b+ ,

(21)

f− (x) = w− · x + b− ,

(22)

and
separately, a new point x ∈ Rn is therefore predicted to the class k(k = −, +) by
arg min

k=−,+

|fk (x)|
,
 k 

(23)

where
+ = θ Λθ, − = γ  Λγ.

3.2

(24)

Hash Based Method for Large Scale Nonparallel Support Vector
Machine Prediction(HNPSVM)

Let ||w+ || = 1 and ||w− || = 1 are L2 -normalized, where w+ and w− are obtained from (13) and
−
(17) respectively. We can build another hyperplane classifier g(x) = w ·x+b, where w = w+ −w
2
b+ −b−
n
and b = 2 . A new point x ∈ R is predicted to the class by
Class = sgn(g(x))

(25)

We normalize all the features, that is ||x||2 = 1. Suppose we are given a binary hash function
family F: Rd → {−1, 1} and select a function f (x) from it.
f (x) = sgn(ω · x)

(26)

If ω is a random hyperplane from a zero-mean multivariate Gaussian N (0, I), this hash function obeys the locality sensitive hash(LSH) property[7]. From a result from Goemans and
Williamson[3] shows that for any two points p, q ∈ Rd , we have
P r[f (p) = f (q)] = 1 −

1
p·q
cos−1 (
)
π
||p||||q||

(27)

The collision probability p(x, w) between f (x) and f (w) is equal to
p(x, w) = P r[f (x) = f (w)] = 1 −

1
x·w
cos−1 (
)
π
||x||||w||

(28)
5

1285

1286	

Xuchan
Ju and Tianhe Wang
Xuchan Ju et al. / Procedia Computer Science 108C (2017)
1281–1291

HNPSVM

By concatenating D hash functions from F, any x and w can be hashed into a D-length
binary hash code FD (x) and FD (w). We define the Hamming distance between any point x
and w as dH (FD (x), FD (w)) which is a random variable distributed according to a binomial
distribution B(p̄, D) and the success probability p̄ of each Bernoulli trial is equal to 1 − p(x, w).
An estimator of p(x, w) can be computed as
p̂(x, w) = 1 −

dH (FD (x), FD (w))
D

(29)

Now we derive an estimation of the classifier ĝ(x).
g(x) = sgn(w · x + b)

(30)

From (28) and ||x|| = 1, we can obtain
ĝ(x) = sgn(cos(π(1 − p̂(x, w)))||w|| + b)
= sgn(cos(π(1 − p̂(x, w))) +

||w||
)
b

−b
) − π(1 − p̂(x, w)))
||w||
−b
D
) − dH (FD (x), FD (w)))
= sgn( cos−1 (
π
||w||
= sgn(cos−1 (

If we define rw,b =

D −b
π ( ||w|| ),

and then a hash-based NPSVM classifier is

ĝ(x) = sgn(rw,b − dH (FD (x), FD (w)))

4
4.1

(31)

(32)

Property and Efficiency of HNPSVM
Property of HNPSVM

In this section, we give a theorem to illustrate the property of HNPSVM. Theorem 1. The
classification results of approximal NSPVM classifier ĝ(x) converge to the classification results
of exact NPSVM classifier g(x) when binary hash code length D tends to infinity.
Proof. Suppose that g(x) = −1, that is w · x + b < 0, and according to (28), we can obtain
p(x, w) < 1 −
Since rw,b =

D −b
π ( ||w|| ),

−b
1
cos−1 (
).
π
||w||

(33)

the following inequality is established.
rw,b < D(1 − p(x, w))

(34)

The probability pmis = P r[ĝ(x) = g(x)] of a misclassification is equal to
pmis = P r[ĝ(x) = 1] = P r[dH (FD (x), FD (w)) < rw,b ] = FH (rw,b )

(35)

where FH is the cumulative distribution function of the discrete random variable dH which
satisfies binomial distribution B(1 − p(x, w), D). According to (34), we can derive rw,b  <
D(1 − p(x, w)). And according to Hoeffding’s inequality, the upper bound of FH can be derived
FH (rw,b ) ≤
6

(D(1 − p) − rw,b )2
1
exp(−2
)
2
D

(36)

	

Xuchan
Ju and Tianhe Wang
Xuchan Ju et al. / Procedia Computer Science 108C (2017)
1281–1291

HNPSVM

that is

1
2r2
exp(−2(1 − p)2 D −
+ 4r(1 − p))
2
D
When D tends to infinity, the misclassification probability tends to zero.
pmis (D) ≤

lim pmis (D) = 0

D→∞

(37)

(38)

On the other hand, if we suppose that g(x) = 1, the approximal classifier can be written as
ĝ(x) = sgn(d¯H (FD (x), FD (w)))

(39)

where d¯H (FD (x), FD (w)) = D − dH (FD (x), FD (w)). d¯H satisfies to a binomial distribution
B(p(x, w), D). Then the probability of a misclassification is equal to
pmis = P r[ĝ(x) = −1] = P r[d¯H (FD (x), FD (w)) < D − rw,b ] = F̄H (D − rw,b )

(40)

where F̄H is the cumulative distribution function of the binomial discrete random variable d¯H .
Since g(x) = 1, then
−b
1
)
(41)
p(x, w) > 1 − cos−1 (
π
||w||
and

rw,b > D(1 − p(x, w)),

(42)

D − rw,b < Dp.

(43)

D − rw,b  < Dp.

(44)

which can be written as
Then
According to Hoeffding’s inequality, we can derive
pmis (D) ≤

(Dp − (D − rw,b ))2
1
exp(−2
)
2
D

(45)

When D tends to infinity, the misclassification probability tends to zero.
lim pmis (D) = 0

D→∞

(46)

In order to illustrate the validity of Theorem 1, we show experiment results on subsets of the
Covtype and Webspam data set. The binary hash code length is D = 64, 128, 256, 512, 1024
and we compute the prediction accuracy using exact NPSVM, Hash-based SVM for prediction(HSVM), and HNPSVM. The results are shown in Figure 1.

4.2

The efficiency of HNPSVM

Although the prediction complexity of approximal NSVM is as the same as exact one, HNPSVM
still has several incomparable advantages.
1. The space requirement is much lower when dealing with a large number of features. If
suppose we use a double precision for w and b, the memory usage for a single exact classifier
is SN P SV M = 8 + 8d bytes and for a single HNPSVM is SHN P SV M = 8 + D
8 . Therefore, if we
take D = 128, and d = 200, the memory usage required by exact NPSVM is almost 100 times
higher than HNPSVM.
7

1287

Xuchan
Ju and Tianhe Wang
Xuchan Ju et al. / Procedia Computer Science 108C (2017)
1281–1291

HNPSVM

1

1

0.98

0.9
exact NPSVM
HSVM
HNPSVM

0.85

prediction accuracy

0.95
prediction accuracy

1288	

0.96
0.94
exact NPSVM
HSVM
HNPSVM

0.92
0.9

0.8

0

200

400

600

800

binary hash code length

1000

1200

0.88

(a) Covtype

0

200

400

600

800

binary hash code length

1000

1200

(b) Webspam

Figure 1: Validity of Theorem 1

Datasets
Letter
Covtype
Usps
Webspam
Kddcup99
a9a

Table 1: Data sets
Number of training samples
12,000
522,910
7291
280,000
4,898,431
32,561

description
Number of testing samples
6,000
58,102
2007
70,000
311,029
16,281

d
16
54
256
254
125
123

2. HPSVM fist takes linear NPSVM to solve classification problem which can obtain much
exact model and in turn provides important guidance for prediction accuracy.
3. HNPSVM speeds up the computation of the classification function. The Hamming
distance between w and x can be much faster than their inner product computation, especially
for high dimensional data.

5

Experimental Results

In order to illustrate the efficiency of HNPSVM, we compare several methods for prediction
including linear and nonlinear state-of-the-art methods on six data sets. All the experiment are
conduced on a PC with an Intel Core I5 processor and 8G RAM.
Experimental data sets: We use six publicly available data sets which can be downloaded
from the homepage of Lin Chih-Jen or the UCI data repository as shown in Table 1 and d
represents the number of features. All samples are scaled in [0, 1].
Comparable methods: We adopt three comparable methods to illustrate the performance
of HNPSVM including one linear method and two nonlinear methods. They are:
1. HSVM: Hash-based SVM[12] is a linear approximal method for prediction. It speeds up
the prediction phase of linear SVM via locality sensitive hashing.
2. DC-Pred++: This method[4] applies weighted kmeans to select landmark points which
generate pseudo-landmark points in Nyström approximation for fast prediction.
8

	

Xuchan
Ju and Tianhe Wang
Xuchan Ju et al. / Procedia Computer Science 108C (2017)
1281–1291

HNPSVM

3. LDKL: The Local Deep Kernel Learning(LDKL)[5] method is a popular method recently
as it performs much better than state-of-the-art kernel approximation.
Table 2 shows the comparison results in terms of test time and test accuracy using HNPSVM,
HSVM, DC-Pred++, and LDKL on several data sets. Binary hash code length D sets to be
256. However, kernel machines usually achieve better performance compared to linear machine,
HNPSVM speeds up the prediction with less prediction loss. Compared with HSVM, HNPSVM
can achieve better test accuracy in a shorter time. Note that the actual prediction time is
normalized by linear prediction time. For example, 0.80x means that the actual prediction
time is equal to 0.80×(time for linear SVM prediction time).
Table 2: Results on the benchmark data sets
Datasets
Letter
Covtype
Usps
Webspam
Kddcup
a9a

HNPSVM
Time
Accuracy
0.080x
92.11%
0.077x
90.39%
0.045x
93.53%
0.038x
94.17%
0.071x
89.23%
0.075x
79.57%

HSVM
Time
Accuracy
0.082x
90.23%
0.079x
87.13%
0.043x
91.21%
0.045x
92.13%
0.063x
86.51%
0.072x
77.43%

DC-Pred++
Time
Accuracy
12.8x
95.90%
18.8x
95.19%
14.40x
95.56%
20.50x
98.4%
11.80x
92.3%
12.5x
83.9%

LDKL
Time
Accuracy
29x
95.78%
35x
89.53%
12.01x
95.96%
23x
95.15%
26x
92.2%
32x
81.95%

Moreover, we further compare HNPSVM with HSVM, DC-pred++, and LDKL on four data
sets in Figure 2. X-axis is the prediction time dividing linear prediction and in order to show
the results clearly we adopt its log value. Y-axis shows the prediction accuracy.

6

Conclusion

In this paper, we have proposed a hash based method for large scale nonparallel support vector
machine for prediction. This method opens the door of tackling large scale data sets prediction
by NPSVM. By hashing the normal to the hyperplane of the classifier and the features and
then computing the Hamming distance between them, HNPSVM reduces storage cost and
computation of classification function with less prediction accuracy loss. Compared with some
state-of-the-art methods like HSVM, DC-pred++, and LDKL, HNPSVM can achieve a higher
test accuracy in a shorter time. Therefore, HNPSVM is a powerful approach for predicting
large scale data sets.

Acknowledgment
This work has been partially supported by grants from National Natural Science Foundation of
China (Nos. 61472390, 71331005, and 91546201), Major International (Regional) Joint Research
Project (No. 71110107026) and the Beijing Natural Science Foundation (No. 1162005).
9

1289

Xuchan Ju et al. / Procedia Computer Science 108C (2017)
1281–1291
Xuchan
Ju and Tianhe Wang

HNPSVM

96

96
94
Test Accuracy(%)

Test Accuracy(%)

94
92
90
HNPSVM
HSVM
DC−pred++
LDKL

88
86
−1

0

1

2

3

log(prediction time/linear prediction time)

92
90
88
86

HNPSVM
HSVM
DC−pred++
LDKL

84
4

82
−1

0

1

2

3

log(prediction time/linear prediction time)

(a) Letter

4

(b) Covtype

100

93
92

98

91
96

Test Accuracy(%)

Test Accuracy(%)

1290	

94
92

88
−2

0

2

log(prediction time/linear prediction time)

(c) Webspam

89
88
87

HNPSVM
HSVM
DC−pred++
LDKL

90

90

HNPSVM
HSVM
DC−pred++
LDKL

86
4

85
−1

0

1

2

3

log(prediction time/linear prediction time)

4

(d) Kddcup

Figure 2: Comparison HNPSVM with HSVM, DC-pred++, and LDKL.

References
[1] Michele Cossalter, Rong Yan, and Lu Zheng. Adaptive kernel approximation for large-scale nonlinear svm prediction. In Proceedings of International Conference on Knowledge Discovery and
Data Mining, 2011.
[2] Thomas Dean, Mark A. Ruzon, Mark Segal, Jonathon Shlens, Sudheendra Vijayanarasimhan, and
Jay Yagnik. Fast, accurate detection of 100,000 object classes on a single machine. In Proceedings
of Conference on Computer Vision and Pattern Recognition(CVPR), 2013:1814-1821.
[3] Michel X. Goemans and David P. Williamson. Improved approximation algorithms for maximum cut and satisfiability problems using semidefinite programming. The Journal of the ACM,
1995(42):1115-1145.
[4] Cho-Jui Hsieh, Si Si, and Inderjit S. Dhillon. Fast prediction for large-scale kernel machines.
Advances in Neural Information Processing Systems (NIPS), 2014.
[5] Cijo Jose, Prasoon Goyal, and Parv Aggrwal. Local deep kernel learning for efficient non-linear
svm prediction. In Proceedings of the 30 th International Conference on Machine Learning, 2013.
[6] Weihao Kong and WuJun Li. Double-bit quantization for hashing. In Proceedings of the Twenty-

10

	

HNPSVM

Xuchan Ju et al. / Procedia Computer Science 108C (2017)
1281–1291
Xuchan
Ju and Tianhe Wang

Sixth AAAI Conference on Artificial Intelligence (AAAI), 2012.
[7] Brian Kulis and Kristen Grauman. Kernelized locality-sensitive hashing for scalable image search.
IEEE International Conference on Computer Vision, 2009.
[8] L’ubor Ladický and Philip H.S. Torr. Locally linear support vector machines. In Proceedings of
the 28 th International Conference on Machine Learning, 2011.
[9] Ping Lin, Anshumali Shrivastava, Joshua Moore, and Arnd Christian König. b-bit minwise hashing
for large-scale learning. in Big Learning 2011: NIPS 2011 Workshop on Algorithms, Systems, and
Tools for Learning at Scale, Neural Information Processing Foundation, 2011.
[10] Wei Liu. Large-scale machine learning for classification and search. Doctoral Dissertations, Graduate School of Arts and Sciences, Columbia University, 2012.
[11] Yadong Mu, Gang Hua, Wei Fan, and Shih-Fu Chang. Hash-svm: scalable kernel machines for
large-scale visual classification. In Proceedings of Conference on Computer Vision and Pattern
Recognition(CVPR), 2014.
[12] Saloua Litayem Ouertani, Alexis Joly, and Nozha Boujemaa. Hash-based support vector machines
approximation for large scale prediction. British Machine Vision Conference (BMVC), 2012(86):111.
[13] Dmitry Pavlov, Darya Chudova, and Padhraic Smyth. Towards scalable support vector machines
using squashing. In Proceedings of International Conference on Knowledge Discovery and Data
Mining, 2000.
[14] Bernhard Schölkopf, Phil Knirsch, Alex Smola, and Chris Burges. Fast Approximation of Support
Vector Kernel Expansions, and an Interpretation of Clustering as Approximation in Feature Spaces.
Springer Berlin Heidelberg, 1998.
[15] Yingjie Tian, Xuchan Ju, and Zhiquan Qi. Efficient sparse nonparallel support vector machines
for classification. Neural Comput & Applic, 2013(24):1089-1099.
[16] Yingjie Tian, Xuchan Ju, Zhiquan Qi, and Yong Shi. Improved twin support vector machine.
SCIENCE CHINA Mathematics, 2014(2):417-432.
[17] Yingjie Tian, Xuchan Ju, Zhiquan Qi, Yong Shi, and XiaoHui Liu. Nonparallel support vector
machines for pattern classification. IEEE TRANSACTIONS ON CYBERNETICS, 2013(44):10671079.
[18] KaiYu Tseng, YenLiang Lin, YuHsiu Chen, and Winston H. Hsu. Sketch-based image retrieval on
mobile devices using compact hash bits. In Proceedings of the 20th ACM international conference
on Multimedia, 2012:913-916.
[19] Jianfeng Wang, Jingdong Wang, Nenghai Yu, and Shipeng Li. Order preserving hashing for
approximate nearest neighbor search. In Proceedings of the 21th ACM international conference on
Multimedia, 2013.
[20] Christopher K. I. Williams and Matthias Seeger. Using the nyström method to speed up kernel
machines. In Proceedings of Conference on Neural Information Processing Systems(NIPS), 2001.
[21] B Xu, J Bu, Y Lin, C Chen, X He, and D Cai. Harmonious hashing. In Proceedings of the 23rd
International Joint Conference on Artificial Intelligence (IJCAI), 2013:1820-1826.
[22] Kai Zhang, Ivor W. Tsang, and James T. Kwok. Improved nystrom low-rank approximation
and error analysis. In Proceedings of International Conference on Knowledge Discovery and Data
Mining, 2008.
[23] Peichao Zhang, Wei Zhang, WuJun Li, and Minyi Guo. Supervised hashing with latent factor
models. In Proceedings of theTwenty-Eighth AAAI Conference on Artificial Intelligence (AAAI),
2014.

11

1291

