Available online at www.sciencedirect.com

Procedia Computer Science 18 (2013) 1535 – 1544

2013 International Conference on Computational Science

Employing an adaptive projection-based interpolation to prepare
discontinuous 3D material data for finite element analysis
Damian Goika, Marcin Sienieka*
a

a

,

Madeja

AGH University of Science and Technology, Krakow, Poland

Abstract
In this paper we propose an adaptive 3D image data pre-processing technique for generating a continuous approximation of an
input image representing some material data, along with a finite element mesh aligned to the properties of the material, which
can be used as the initial mesh for a further hp-adaptive finite element analysis. First, we introduce the projection-based
interpolation operator, we explain some design considerations, useful for reproducing this work, then we present a benchmark
problem used as a proof of concept and we conclude with numerical results for this exemplary problem, obtained with our
implementation of the discussed method.

© 20133 The Authors. Published by Elsevier B.V. Open access under CC BY-NC-ND license.
Selection and
peerpeer-review
review under
responsibility
of theoforganizers
of theof
2013
Conference
on Computational
and/or
under
responsibility
the organizers
the International
2013 International
Conference
on Computational
Science
Science
Keywords: finite element method, projection-based interpolation, image processing

1. Introduction
1.1. Motivation
It can be generally stated that recently observed needs of the automotive and aerospace industries for new
metallic materials that can meet strict requirements regarding weight/property ratio constitute a driving force for
development of modern steel grades. Complicated thermomechanical operations are applied to obtain highly
sophisticated microstructures with combination of e.g. large grains, small grains, inclusions, precipitates, multiphase structures etc. These microstructure features and interactions between them at the micro-scale level during

* Corresponding author. Tel.:+48-725-567-100.
E-mail address: msieniek@agh.edu.pl.

1877-0509 © 2013 The Authors. Published by Elsevier B.V. Open access under CC BY-NC-ND license.
Selection and peer review under responsibility of the organizers of the 2013 International Conference on Computational Science
doi:10.1016/j.procs.2013.05.321

1536

Damian Goik et al. / Procedia Computer Science 18 (2013) 1535 – 1544

manufacturing or exploitation stages eventually result in elevated material properties at the macro-scale level. To
support experimental research on these materials, a numerical material model that can take mentioned
microstructure features explicitly into account during FE analysis of processing and exploitation conditions has to
used. One of the solutions to deal with the explicit representation of microstructure features during numerical
analysis is an approach based on the Digital Material Representation (DMR) ([2]). However, there are two major
issues that have to be addressed in this methodology. The first is development of algorithms for creation of
structures that can represent real morphology of single and two phase microstructures ([1]). The second that is
addressed in the present work is a problem of meshing of the created DMR as due to the nature of obtained
microstructure a significant solution gradients (strain, stress etc.) are expected during numerical modeling. A
robust and reliable algorithms capable to properly refine finite elements along mentioned microstructure features
have to be developed. One of the solution is developed within the work space projection approach.
Interpolants computed using projection based interpolation operator were originally used as convergence
estimates for the hp-adaptive Finite Element Method, but the spectrum of their applications is much wider. In this
paper we discuss their use as a basis for approximation of geometry in 3D. More specifically, given a 3D input
bitmap, the goal is to come up with its representation as linear combination of the Finite Element basis functions.
This transformation is necessary to prepare the input image for future computations (where it is treated as a
material function) using the Finite Element Method as well as to generate initial mesh in accordance with material
function variation.
The three dimensional PBI algorithm technique presented in this paper is a generalization of the two
dimensional one described in [3, 4]. For a summary of alternative techniques for approximating different classes
of locally regular functions see [17]. The paper is concluded with numerical results concerning the PBI
approximation of the three dimensional two phase material.
2. Projection-based interpolation
Our goal is to find a representation of the input function
other words, we are looking for the coefficients
interpolates the function in vertices

in the base

of a

of the linear combination
for each vertex

and minimizes the

space. In
which

norm

on other nodes.
This could be solved globally as a linear system. We are solving it, however, as a sequence of sub-problems
local to particular elements. The consequences of this decision are twofold: On one hand, what we are obtaining is
only a suboptimal solution. On the other hand, computations can be performed in parallel and are trivially
distributable (thanks to small amount of local data required to do the computations). This approach has already
proven effective for our 2D experiments ([3]), but its applicability for 3D problems was not investigated yet.

Fig. 1 Naming of element's nodes. The space constrained by faces, edges and vertices is called interior
and constitutes the fourth type of a
node to which shape functions can be attached.

1537

Damian Goik et al. / Procedia Computer Science 18 (2013) 1535 – 1544

The methodology we are going to utilize is the projection based interpolation (PBI) technique originally
introduced by [9]. The 3D projection problem in the PBI algorithm is partitioned into the following steps, related
to finite element vertices, edges, faces and interiors (compare the naming scheme of element nodes in Fig. 1)
For each element we are looking for coefficients in a particular order. We start with vertices, since their
coefficients are the most straightforward to compute. There is only once function per vertex with a support on it
and the interpolating function is required to be equal to the interpolant which yields:

(1)
On nodes other than vertices, the input function cannot be represented exactly, so instead we are trying to
minimize the representation error. First, on each one of the 12 edges:

(2)
signifies the number of edge shape functions in space with supports on edge . Such a problem
where
on
can be reduced to a linear system and solved with a linear solver, but if we assume the adaptation order
each node, for each edge there exists only one shape function with a support on it. Not only is this restriction
justified performance-wise (one local equation instead of a system), but it also suffices in most cases, according to
our experiments. Thus equation (2) reduces to:

(3)
where

(4)

(5)
and as

where

is constant:

1538

Damian Goik et al. / Procedia Computer Science 18 (2013) 1535 – 1544

(6)
which leads to:

(7)

is constant and can be omitted in minimization.

since
Let be

a bilinear, symmetric form defined as:

(8)
and

be a linear form:

is reducible to solving
, we obtain:

It is proven that minimizing
applying this lemma to problem (7) for

(9)
(see [5]). By

(10)
and finally as

and

:

(11)
The next step consists in an optimization on faces:

1539

Damian Goik et al. / Procedia Computer Science 18 (2013) 1535 – 1544

(12)
where

vanishes on vertices and edges. This leads to:

(13)
Finally, an analogical optimization in the interior of the finite element:

(14)
(where

vanishes everywhere except from the interior) yields:

(15)
It is worth noting that using this method the global matrix is not constructed at all. Thanks to the
restriction,
we have a single equation over each vertex, edge, face and interior. This algorithm requires a computational cost
linear with respect to the mesh size, because it involves constant number of operations for each vertex, edge, face
and interior.
3. Adaptability
In a single step we perform the above computations on each one of the elements. This way we are obtaining a
global interpolation function (composed of locally-optimal interpolations). Should the quality of the solution be
unsatisfactory, the algorithm can be repeated several times on a refined mesh.
3.1. H-adaptation
The approach we took for the mesh adaptation relies solely on so called h-adaptation. In this technique, we are
refining a solution over an element by breaking it into eight elements of equal sizes that fill the space of the
original element. Then, in order to satisfy the 1-irregularity constraint [5] we divide each neighboring (i.e. having
a common face or edge) element provided that its edges are two times longer than the edge of the original

1540

Damian Goik et al. / Procedia Computer Science 18 (2013) 1535 – 1544

element. The enforcement rule propagates as it is possible that the newly created elements have their neighbors
too big as well. Note that throughout the whole adaptation process, an element can have a vertex in common with
an element at most four times larger and an edge or face in common with an element at most two times larger.
3.2. Ensuring continuity
In order to ensure continuity of the global solution function, several steps need to be made (see [15] for
details). Although it can be easily proven that for elements of equal sizes, the solution on their common faces,
edges and vertexes is equal, the situation changes when an element is placed next to another one of a different
size. In order to guarantee the continuity under such circumstances, for each basis function bound to some node
(vertex, edge or face) of a bigger neighbor, we transform computed coefficient associated with this basis function
into a set of different coefficients that are used on the smaller neighbor and its basis functions. This way we
ensure that combination of basis functions multiplied by their coefficients has equal values on the common node
(vertex, edge or face) from the perspective of both the bigger and the smaller neighbor. This transformation works
regardless of elements' relative positions. In order to assign a particular basis function and its coefficient equal
values on the common node from the perspective of the smaller neighbor, you need to use some functions of the
same and lower approximation levels. For instance, a shape function corresponding to an edge node must be
composed of smaller element's edge and vertex shape functions. Conversely, a face function must be composed of
smaller element's face, edge and vertex functions. Note that for a smaller neighbor node joint with bigger one,
formulas from chapter 2 are not used at all.
3.3. Multithread H adaptation algorithm
norm of a difference between the solution and the
Our solution adapts the mesh as long as a value of
interpolated function is greater than the desired error rate. While mesh adaptation is sequential, computation of
base function coefficients is performed by more than one thread in a given moment. It is essential to compute
bigger elements before smaller because of the reasons mentioned above.
A big picture of what our implementation of the above does is presented on Alg. 1.
function pbi (mesh, desiredErrorRate){
elementsToCompute = getInitialElement()
while(elementsToCompute.size() != 0){
//concurrent part
for ( elementsOfEqualSize in elementsToCompute){
computeCoefficientsConcurrent(elementsOfEqualSize)
computel2 NormConcurrent(elementsOfEqualSize)
}
// sequential part
for( element in elementsToCompute ){
if( element.error > desiredError)
//this step involes possible neighbour's divisions
divide(element)
}
elementsToCompute = getAllNewElements()
}
}
Alg. 1. Adaptive algorithm for computing PBI.

Damian Goik et al. / Procedia Computer Science 18 (2013) 1535 – 1544

1541

4. Agent-based approach
As mentioned before, the presented algorithm acts mostly locally on subsequent parts of the domain. Inter-part
communication is required only by:
me
enforce the regularity rules during the mesh refinement process.
accumulated globally
On the other hand the method is computation-intensive, so some parallelization is needed in many scenarios. As
the adaptive algorithm can produce very unbalanced meshes, load balancing is usually desirable.
Having taken these requirements into consideration, we decided to express the PBI algorithm in agent-oriented
paradigm. By using an agent platform, which implements an automatic load balancing and grain control (e.g.
Octopus described in [6, 7, 8]) we gain even load of tasks and though efficient usage of the machine. To do so, it
is necessary to parallelize the computations using domain decomposition. Each agent performs PBI on its own
slice of the mesh and is capable to divide and delegate the task to another agent when needed. Effectively we
managed to introduce only three different types of agent's roles in our project:
Slave Agent - performs the actual computations
Master Agent - manages an interface between its children
Root Agent - manages the highest-level interface

Fig. 2 Exemplary agent communication tree. The blocks illustrate how a 3D mesh is divided between lower-level agents.

To use the above approach to mesh distribution, it is necessary to develop an efficient method of mesh
refinement that is run on all agents simultaneously. That is why, the mesh into sub-meshes that will be later
distributed among different agents. Optimal division would create two meshes of similar amount of elements
using the smallest possible number of faces on the division path. The latter condition stems from the fact that the
amount of communication necessary to synchronize the sub-meshes is proportional to the amount of faces on the
boundary. Since it is very hard to find an optimal path in reasonable computing time, heuristic algorithms have to
be used.
The main influence that has driven us during the design of mesh distribution algorithm is the choice of base
functions. Some of them span across multiple faces and during the mesh division process we will have to divide
across some of them. To deal with that limitation, the division path should chosen so that all the split shape
functions will be based on a face, an edge or a vertex of that path. In the case where a 3D bitmap is the input, this
will enable us to calculate the coefficients for each base function separately on both sub-meshes and their values
will be the same.

1542

Damian Goik et al. / Procedia Computer Science 18 (2013) 1535 – 1544

In the agent-based approach, each agent represents some part of the whole mesh. Depending on the role of an
agent it can contain a single sub-mesh (slave agent) or represents an interface between sub-meshes of two
different agents (master agent). In the latter case, the subagents can also have role of master and create a binary
tree. The master is responsible for routing synchronization data from both subagents to each other and up the tree.
In case of distributed mesh refinement, the main information that has to be synchronized is the list of boundary
faces (faces that were on the division path) that will be split during the refinement process. During the refinement
process, each boundary face splitting information is sent to the master agent above. The master agent decides if
the border split is internal if so, it is forwarded to the other subagent or external it is again send up the tree.

Fig. 3 The solution to the PBI problem in consecutive steps of PBI algorithm.

Damian Goik et al. / Procedia Computer Science 18 (2013) 1535 – 1544

1543

5. Numerical results
The numerical example consists in approximating a three dimensional two phase material, using the adaptive
projection based interpolation technique described above. The solution to our benchmark problem indicates
compact areas that belong to one of two groups (red or blue). Fig. 3 represents subsequent results on a surface of
the three-dimensional domain, obtained by starting the computations with smaller desired error rate. One can see
that adaptation starts at the boundaries of the elements. Then it propagates due to the 1-irregularity rule. In our
case, an element that entirely lies in one of the two areas adapts only because of the 1-irregularity rule, even
though we can accurately interpolate the function over it from the beginning. The solution on the elements that
contain boundaries of two or more areas will always have interpolation error, depending on their size. It is worth
noting that the green color, in this particular situation denoting bad interpolation quality, quickly vanishes and it is
always located somewhere close to the boundaries of the areas. Even though in this specific case of a 2-phase
material, refining only elements on the interface between phases (without recoursing to such an advanced hadaptive algorithm) would suffice, this is not true for a general case of more complex materials.

Fig. 4 The comparison between the input data (left panel) and numerical results (right panel)

Fig. 5. represents total error (in terms of the norm) after a subsequent series of adaptations in six iterations.
Computations lead to accurate solution. The comparison of the numerical results with input data are presented in
Fig. 4.

Fig. 5 Decrease of the difference between the PBI projection and the input data in particular iterations.
The horizontal axis denotes number of iterations, the vertical axis denotes log of the
norm.

6. Conclusions and future work
In this paper we have presented a simple, yet powerful approach for computing continuous approximations of
discrete 3D bitmaps, leveraging the PBI operator. Thanks to the ability to compute the coefficients of the
For practical
in
applications of the projections mechanism we refer to
For future work, a support for p-adaptation could be developed. Such an algorithm would require solving a
system of equations rather than a single equation f

1544

Damian Goik et al. / Procedia Computer Science 18 (2013) 1535 – 1544

and potentially higher quality-to-number-of-degrees-of-freedom ratio. It remains unclear, whether this would
improve the quality-to-computational cost ratio as well, which makes such an experiment even more interesting.
Apart from that we plan to investigate the extension of this research to a hybrid solver such as a direct solver
interfaced with an iterative solver as described in [10, 11, 12]. We are also planning to investigate the application
of agent-based model summarized in [13, 14] for the implementation of the methodology described in this paper.
Acknowledgements
This work has been funded by Polish National Science Center with funds allocated with the decision DEC2011/03/N/ST6/01397 and with the grant NN 519 447 739.
References
[1] L. Madej, 2010, Development of the modeling strategy for the strain localization simulation based on the
Digital Material Representation, DSc dissertation, AGH University Press, Krakow
, 2011: Digital Material Representation as an efficient tool for
[2] L. Madej, L. Rauch, K
strain inhomogeneities analysis at the micro scale level, Archives of Civil and Mechanical Engineering, 11,
pp. 661-679.
[3] M. Sieniek, P. Gurgul, M. Skotniczny, K. Magiera, M. Paszynski, 2011: Agent-oriented image processing with
the hp-adaptive projection-based interpolation operator, Procedia Computer Science, vol. 4, pp. 1844-1853.
[4] P. Gurgul, M. Sieniek, M. Skotniczny, K. Magiera, 2011, Application of multi-agent paradigm to hp-adaptive
projection based interpolation operator, Journal of Computational Science, in press.,
dx.doi.org/10.1016/j.jocs.2011.07.002
[5] L. Demkowicz, J. Kurtz, D. Pardo, M. Paszynski, W. Rachowicz, A. Zdunek , 2007: Computing with HpAdaptive Finite Elements, vol. 2: Frontiers: Three Dimensional Elliptic and Maxwell Problems with
Applications, Chapman & Hall/ CRC
[6] J. Momot, K. Kosacki, M. Grochowski, P. Uhruski, R. Schaefer, 2004: Multi-Agent System for Irregular
Parallel Genetic Computations, Lecture Notes in Computer Science, vol. 3038, Springer, pp. 623-630.
[7] M. Grochowski, R. Schaefer, 2006: Architectural Principles and Scheduling Strategies for Computing,
Fundamenta Informaticae, vol. 71, IOS Press, pp. 15-26.
[8] M. Grochowski, R. Schaefer, P. Uhurski, 2004: Diffusion Based Scheduling in the Agent-Oriented Computing
System; Lecture Notes in Computer Science, vol. 3019; Springer, pp. 97-104.
[9] L. Demkowicz, A Buffa, 2005: H1, H(curl), and H(div)-conforming projection-based interpolation in three
dimensions Quasi-optimal p-interpolation estimates, Compuer Methods in Applied Mechanics and
Engineering, vol. 194, pp. 267-296.
2008: Scalability analysis for a multigrid linear equations solver, Lecture Notes in Computer
Science , vol. 4967, pp. 1265 1274.
, 2010: Finite element numerical integration on GPUs, Lecture Notes in
Computer Science, vol. 6067, pp. 411 420.
[12]
Finite element numerical integration on PowerXCell processors, Lecture Notes in
Computer Science, vol. 6067, pp. 517 524.
[13] K. Cetnarowicz; P. Gruer, V. Hilaire et al., 2002: A formal specification of M-agent architecture, Proc.
Multi-Agent Systems CEEMAS 2001, Berlin, Heidelberg, vol. 2296, pp. 62-72.
[14] K. Cetnarowicz, 2009: From algorithm to agent, Computational Science ICCS 2009, LNCS 5545 Springer
Verlag, pp. 825-834.
[15] L. Demkowicz, J.T. Oden, W. Rachowicz, O. Hardy, Toward a universal h-p adaptive finite element strategy,
part 1. Constrained approximation and data structure, Computer Methods in Applied Mechanics and
Engineering, Volume 77, Issues 1 2, December 1989, Pages 79-112
[16] M. Sieniek, P. Gurgul, M.
, 2013: Employing adaptive finite elements to model squeezing of a
layered material in 3D, International Journal of Materials, Mechanics and Manufacturing, IACSIT Press (in
press)
[17] L. Demaret, A. Iske, 2012: Optimal N-term Approximation by Linear Splines over Anisotropic Delaunay
Triangulations, Preprint

