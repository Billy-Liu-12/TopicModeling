Available online at www.sciencedirect.com

Procedia Computer
Procedia
ComputerScience
Science001 (2009)
(2012)000–000
535–544

Procedia
Computer
Science
www.elsevier.com/locate/procedia
www.elsevier.com/locate/procedia

International Conference on Computational Science, ICCS 2010

Component-based design for adaptive large-scale infectious disease
simulation
Thorsten Matthias Riechersa,*, Shyh-hao Kuob, Rick Siow Mong Gohb, Terence Hungb
b

a
RWTH Aachen University, Templergraben 55, 52056 Aachen, Germany
Institute of High Performance Computing, 1 Fusionopolis way #16-16 Connexis, Singapore 138632

Abstract
Component-based design improves productivity by concentrating development efforts on one component at a time without
having to worry about a change having an application-wide effect. In this paper, we demonstrate the usefulness of componentbased approach in the development of an infectious disease simulator. Specifically, we have explored the possibility of self
performance tuning at runtime through the use of hot-swappable components by incrementally develop optimised component
variants easily. The application has achieved 4 times speedup using dynamic kernel adaptation and a further 5.3 times speedup
through parallelisation on a multicore and GPU server.
c 2012 Published by Elsevier Ltd. Open access under CC BY-NC-ND license.
⃝
Keywords: Multi-agent systems; Infectious disease simulator; Component-based software engineering; Compositional adaptation; Heterogeneous
computing

1. Introduction
In this paper, we present our experience in implementing a large-scale infectious disease simulator using a
component-based design approach. We have implemented runtime adaptation of simulation kernels to improve
overall performance. Our approach allows one to make better utilization of heterogeneous computing resources,
such as GPU [1,2] and heterogeneous CPUs [3-5] while maintaining portability at the compositional level.
With high performance computing platforms moving towards heterogeneous multi-core architectures, the optimal
configuration of any algorithmic task in a parallel program has now become dependent on the processor core it is
assigned on for execution [6,7]. In order to harness the power provided by these heterogeneous cores, one has to go
beyond static assignments of small code snippets to different processors, and consider the need to make these
resource assignments during runtime. In this paper, we present our extension to a single thread, component-based
infectious disease simulator by the authors [8] to a heterogeneous architecture parallel processing platform. In this
extension, we demonstrate a simulator design that is capable of adapting the random number generator component at
run-time by utilising a GPU when present, and implements the same functionality in the CPU when the GPU is not
available. Furthermore, this component was also able to supplement the GPU when the demand for random number
* Corresponding author. Tel.: +49-176-22718602.
E-mail address: thorsten.riechers@rwth-aachen.de.

c 2012 Published by Elsevier Ltd. Open access under CC BY-NC-ND license.
1877-0509 ⃝
doi:10.1016/j.procs.2010.04.057

536

T.M. Riechers et al. / Procedia Computer Science 1 (2012) 535–544
T. M. Riechers, S. Kuo, R.S.M. Goh, T. Hung / Procedia Computer Science 00 (2010) 000–000

is too high due to an increase number of simulator threads. All of the above was achieved without compromising the
elegance of the simulator design by encapsulating these adaptation rules inside a component.
Because different kernel implementations may have very different performance characteristics depending on the
input data, it is this observation that rationalizes the proposed technique of dynamic kernel adaptation – to optimize
applications by using the most efficient kernel implementation that may change during runtime. As the dynamic
kernel adaptation approach is to change the implementation of a compute-intensive kernel within an application
during runtime, it is convenient if the application is designed with a component-based framework and the kernels are
implemented as components.
The rest of this paper is organized as followed. Firstly, we give a brief description of the background to our work.
The next two sections that follow present the component-based adaptive runtime system and the infectious disease
simulator respectively. The performance results are presented and discussed before concluding this paper.
2. Related Work
In this paper, we present our experience in applying the Common Component Architecture (CCA) framework [9]
to a large-scale agent-based infectious disease simulator. This simulator is a tool that we have developed to model
and simulate the spread of infectious diseases such as the Influenza A (H1N1) [10], as well as the effects of the
control strategies to mitigate the spread. For this approach, we have developed and implemented a generalised
Component-based Optimiser for the purpose of compositional adaptation for scientific applications [11].
Compositional adaptation refers to software that is composed of several parts; some of these parts have multiple
instances that can be adapted. The specific parts can be selected at compile-time based on the specific hardware
architecture, or in our case, these parts takes the form of software components and can be “hot-swapped” during
program execution.
There exist a few component-based frameworks that allow compositional adaptation of components. A
comprehensive list can be found in [11]. For our work, we use the Ccaffeine Framework [12] which implements the
Common Component Architecture (CCA) specification [9]. We have chosen to work with the CCA specification
because it has specified API for creating and destroying components during runtime and is well suited for
implementing the dynamic component substitution idea [13]. Another important aspect to consider for choosing a
framework to perform dynamic compositional adaptation is the decision rules. Again, the Ccaffeine Framework
provides componentised interface for measuring component execution time using the TAU library [14].
Although our implementation utilizes the Ccaffeine Framework, the approach can be used in any component
framework that supports dynamic switching of components (i.e. runtime component substitution), and provides
some basic instrumentation that can be fed back into the system at runtime. One example of such framework is PCOM2 [15] developed at the University of Texas. The supporting mechanism for runtime adaptation for it can be
found in [16].
Instead of considering multiple implementations to provide the same functionality as different components that
can be “hot-swapped”, another possible way of achieving runtime adaptation is to introduce the concept of a
component container holding a substitution set of a family of components with the same functionality and different
qualities (e.g. execution time and accuracy) [13,17]. Such a “dynamic component” selects the most appropriate
component implementation in which the selection is based on meta- and monitored data that each component
provides. This approach allows the adaptation rules to be encapsulated within this component container and
arguably provides a cleaner interface. In a system built with two or more adaptive components, our approach is to
provide a centralised adaptation component that can jointly adapt multiple components with similar functionality.
We also refer this approach as dynamic kernel adaptation.
3. Generalised component-based optimiser
In this section, we shall discuss the design of our component-based Optimiser that supports compositional
adaptation of components. The goal is to allow the simple creation of multiple implementations of kernels with
similar functionality and that the supporting runtime system will allow dynamic adaptation through runtime
component substitution. The component substitution mechanism ensures that the most appropriate kernel
implementation is chosen based on runtime conditions and the processor platform the application is executed on.

T.M. Riechers et al. / Procedia Computer Science 1 (2012) 535–544

537

T. M. Riechers, S. Kuo, R.S.M. Goh, T. Hung / Procedia Computer Science 00 (2010) 000–000

Most appropriate refers to the best performing kernel implementation on available hardware resources. The
component-based Optimiser is shown in Fig. 1. When a user starts the simulation, an architecture specific Optimiser
is constructed dynamically and then executed. While the Simulator is executing, performance data is fed through the
Monitor component (an instance of the TAU component) to a Decision component which checks the performance
data and decides if the simulator need to be reconfigured for better performance. The actual mechanism of invoking
the API for creating and destroying components within the simulator is implemented in the Builder component. As
shown in the figure, there is also another component called Accelerator that interfaces with the Builder component.
The purpose and functionality of each of these components will be described in more detail subsequently. But
firstly, we shall briefly discuss the Simulator component which contains the most compute-intensive kernels, and
which is where dynamic kernel adaptation occurs.

Fig. 1. This figure shows a Component-based Optimiser that allows the creation of multiple implementations of kernels with similar
functionality. It enables dynamic adaptation through runtime component substitution in which the most appropriate kernel implementation
is chosen based on runtime conditions and the processor platform the application is executed on.

The Simulator component shown in Fig. 1 is made up of multiple smaller components but the details are not
shown for simplicity. The most compute-intensive component within Simulator is referred to as the computational
kernel and in our case study infectious disease simulation application, it is the component that is responsible for
updating the progress of disease spread among all individuals in a simulation population (more details of the kernel
are provided in next section). For a specific kernel of a simulation, there may exist more than one implementation to
achieve the same functionality. Multiple implementation variations can differ not just in the architecture it is
compiled for, but may also differ algorithmically – e.g. various sorting algorithms. An important concept to bear in
mind is that multiple instances only require to offer similar functionality and produce similar results. However, due
to the specifications and nature of hardware architectures, e.g. different floating point precision capabilities, these
implementations may not necessarily produce identical numerical results – e.g. matrix multiplication on CPU and
GPU.
For our case study application, we first refactor our original application (already written in C++) into the
Simulator component which is made up of several smaller components. The advantage of component-based
software engineering with respect to dynamic kernel adaptation is that components have well-defined standardized
interfaces. This allows the compute-intensive kernels to be encapsulated into a component which can be
dynamically swapped easily.
The supporting runtime framework responsible for creating and destroying the components in the Simulator is
then constructed. It also helps to make decision on the optimal way of configuring it. This runtime system is built
using CCA components and consists of the following:
(1) The Builder component dynamically modifies the composition of the Simulator. It encapsulates framework
specific function calls for dynamically building and modifying the Simulator. It also implements a portable interface
for component substitution which is used by the Driver component.
(2) The Decision component implements the decision rules for swapping the implementations of the kernel. For
each adaptive component, it selects from a family of implementations which have the same functionality but have
different performance (such as execution time, memory-usage, energy efficiency and accuracy). The Decision
component requires two types of information to make its decision: (a) performance metric to validate the decision,

538

T.M. Riechers et al. / Procedia Computer Science 1 (2012) 535–544
T. M. Riechers, S. Kuo, R.S.M. Goh, T. Hung / Procedia Computer Science 00 (2010) 000–000

and (b) simulation-specific observations (such as the size of an array) that can be used to predict performance. This
information can be used by this component to learn the conditions under which an implementation performs best in
order to build up the decision rules. Because our approach separates this component from the Monitor and the
Builder, we can experiment with different implementations effortlessly.
(3) The Monitor component gathers the data required by the Decision component and its implementation depends
on the Simulator.
(4) The Accelerator component examines the surrounding system and processor platform. It detects the available
resources such as multicore processors and GPUs so that the Builder component can select pool of appropriate
implementations that cater for the particular system used.
(5) The Driver component simply coordinates the optimization process.
By constructing the runtime in a component-based framework, certain core elements can be developed and
changed separately, e.g. the decision rules can be implemented and modified without knowledge of the Simulator’s
implementation, which is handled by the Builder component. Furthermore, this structure can be easily applied to
other simulations to achieve dynamic adaptation by changing only the Builder component. While the Monitor
component appears to be part of the Simulator, it is implemented as a proxy component that sits at the component
interface and thus it is possible to automatically generate the Monitor in a systematic way. Finally, the Decision
component may implement generic rules to optimize the application based on some gradient search methods. Since
the Monitor component abstracts from Simulator specific metrics, the decision rules can be formulated in a general
way such that they can be adapted easily. This will likely save time and improve the productivity of developers.
4. An agent-based infectious disease simulator
Simplistically, an agent-based simulator for infectious diseases can be viewed as a large collection of simple state
machines. Each of these state machines represents an individual in the population and is known as an agent. Agents
interact with each other through pre-defined contacts. The aggregation of all the contacts forms a graph with agents
as nodes and contacts as edges. This graph is sometimes referred to as the contact network. The states of the agent is
reflective of the disease progression such as susceptible (S), exposed (E), infectious (I), and recovered (R). This is
referred to as the SEIR-model [18]. A typical simulation may consist of millions of agents representing a densely
populated city. By introducing an infectious individual into this population, other agents that are connected to this
infected individual will change state from S to E and potentially become infectious based on some probabilistic
mode. To simulate this, the computer has to check through each individual and perform state transitions based on the
contacts and result of a random number generator.
A typical run of the simulation terminates when there are no more infectious individuals. Profiling of the
simulator without the adaptation framework reveals that approximately 80% of the total execution time is spent in
updating the agent status and traversing the contact network. Next, we describe in detail the computational kernel
used in updating the states in the simulation.
4.1. Two different kernels
Once an individual is in the exposed state, the state changes autonomously irrespective of its contact list (edges).
For agents in the infectious and susceptible state, the simulation needs to check through the contact list of these
individuals. In the case of a contact between a susceptible and an infectious individual, it makes a probabilistic
decision if an infection actually takes place. We have identified two different kernels for performing this update and
they are described below.

T.M. Riechers et al. / Procedia Computer Science 1 (2012) 535–544

539

T. M. Riechers, S. Kuo, R.S.M. Goh, T. Hung / Procedia Computer Science 00 (2010) 000–000

4.1.1. The infecting loop (I-Loop)
While the simulator examines the contact network, it will check each agent’s state and perform the autonomous
state transitions. In addition, if it is an infectious individual, we will go through the contact list (edges) of this
individual and attempt to infect those that are in the list and who are susceptible (see Fig. 2a).
4.1.2. The to-be-infected (or susceptible) loop (S-Loop)
In this method, the simulator performs the autonomous update as in the previous method. Additional computation
is performed only on the susceptible individuals by computing the probability of this susceptible individual being
infected by checking its contact list for infectious individuals (see Fig. 2b).

Fig. 2. (a) I-Loop: an infectious individual (1) is infecting susceptible contacts (7, 10, 146 etc.); (b) S-Loop: a susceptible individual (1) is
infected by infectious contacts (10, 146, 112 etc.)

4.2. The serial implementation of the simulator
4.2.1. Experimental setup and Results
The system used in our experiments is an Intel dual quadcore 3.0GHz with 32GB DDR2 memory with a Tesla
GPU card on the PCI express bus. The 8 cores has the same access to the main 32GB of memory and can be
considered as two groups of 4 cores with a shared cache for each group. The Tesla GPU device has a memory of
4GB and data has to be transferred through the PCI express bus to share data with the CPU. We use the Ccaffeine
Framework, version 0.8.8 as our component framework and instrumentation is provided through the TAU
performance component version 1.7.3a [14] with TAU version 2.17.3 [19].
For completeness, the parameters used in the infectious disease simulation are given below. Note that the
parameters used are for benchmarking purposes and may not reflect the property of any specific disease. In order to
eliminate the possibility of early termination of our simulation, ten infectious individuals are introduced at the
beginning of the simulation run with the probability of infecting other individuals in the population through contact
(transmissibility of the disease) set to be sufficiently high at 0.01, which is a realistic number. This will almost
guarantee that the simulation will blow up to a full epidemic which is the special case where performance tuning is
most critical. Other important simulation parameters used include: the average time to become infectious after
exposure is 1 timestep, and the average time to recover after becoming infectious is 6 time steps. The simulator
performs a synchronous update of the individual’s state and at any instance.
Execution time (runtime) and system memory usage are recorded for different population size and different
degree of contact (i.e. average number of people that an individual comes into contact with per day). We will refer
to a simulation from the introduction of the infectious individuals to the state whereby no one is infectious as a
single epidemic. A typical run of the simulation consists of collecting statistical information from many epidemics

540

T.M. Riechers et al. / Procedia Computer Science 1 (2012) 535–544
T. M. Riechers, S. Kuo, R.S.M. Goh, T. Hung / Procedia Computer Science 00 (2010) 000–000

in order to obtain a good average. With population size similar to that of Singapore (5 million), a typical run of the
reference design takes about one day on the above mentioned system. In a typical study of a disease, one would
perform a run to obtain the typical characteristics of the disease. With further study of the result, multiple “control
strategies” will be proposed which results in modification to the contact networks. This modified contact network is
then fed into the system for another run and the result will be compared. With a turnaround time of around a day in a
reasonably powerful server, there is definitely requirement to improve the execution time.
We compare the execution time (Fig. 3a) and memory usage (Fig. 3b) of four different single threaded
implementations:
(1) The “Original” implementation is the reference C++ implementation of the simulator. All other results are
normalized to this and the results of Original do not appear in the figures. The Original’s results are represented as
the 100% mark.
(2) The “Static” implementation refers to the equivalent component-based implementation of Original without
the adaptive runtime. This shows the relative overhead in a component based design.
(3) The “Dynamic” implementation embeds the Simulator’s component-based implementation into the runtime.
This will incur an addition runtime composition overhead compare with the Static version. In this mode adaptation
is disabled.
(4) The last column (“Adapt”) refers to the same setup as Dynamic but with adaptation enabled.

Fig. 3. (a) Execution time comparison of the four implementations. The population size varies from 100,000 to 5 million. The degree of
contact is 50 or 100. (b) Memory usage comparison of the four implementations.

4.2.2. Discussion
For the serial implementation it was observed that there is a small overhead from refactoring the original code
into components. Without adaptation, a penalty of 10% extra execution time was observed at the 4 million and 5
million population size by the component-based approach. There is also a constant memory overhead of 200Mbyte,
which can be attributed to the extra storage needed in managing the arrays in the component interface. Nevertheless,
these overheads are a small price to pay when considering the benefit provided by being able to adaptively optimize
the simulation as we observe 4 times (degree of contact 100) and 2 times (degree of contact 50) speedup in terms of
execution time for the “Adapt” implementation as shown in Fig. 3a.
With dynamic adaptation enabled, we observe up to 4 times speedup compared to the Original simulator. From
the description of the two kernels, it is obvious that the execution time for each individual state update depends on
the progress of the disease. When a disease is first introduced into a community, there are only a small number of
infectious people. Therefore it is more efficient to use the infecting loop (I-Loop) as the kernel. As the disease
progresses and more individuals of the population is either infectious or recovered (we assume a period of
immunity), it becomes more efficient to use the to-be-infected loop (S-Loop) kernel.

541

T.M. Riechers et al. / Procedia Computer Science 1 (2012) 535–544
T. M. Riechers, S. Kuo, R.S.M. Goh, T. Hung / Procedia Computer Science 00 (2010) 000–000





4

S-LOOP

3
2

200
EXECUTION TIME [s]

EXECUTION TIME [s]

5

I-LOOP

1

S-LOOP

150

100

I-LOOP
50

0

0
10

20

30

40

50

60

TIMESTEPS

20

30

40

50

60

70

80

TIMESTEPS

Fig. 4. (a) 100k/100 execution time of I-Loop and S-Loop; (b) 5M/50 execution time of I-Loop and S-Loop

For both kernels, the execution time per timestep is shown for 100k/100 in Fig. 4a. From this graph, we confirm
our assumptions on the association of the performance of the two kernels with the state of the population (i.e.
number of susceptible and infectious). Fig. 4a reveals that the optimal swapping point is at the intersection of the
graphs (timestep 24). The grey highlighted area under the graphs corresponds to the optimal execution time with
perfect adaptation.
The execution time characteristics of the two kernels for 5M/50 are shown in Fig. 4b. The S-Loop performs less
well for a lower degree of contact (e.g. 50), the speedup reduces to 2. This is due to a shift of intersection point
(timestep 51), and increases the highlighted area to about 50% (Fig. 4b) of the area under the I-loop. This
corresponds to the 2 times speedup observed for this case.
4.3. The multi-threaded implementation of the simulator
In order to obtain further speedup, we created components with an OpenMP multi-threaded implementation of
the I-Loop and S-Loop algorithm. These are implemented as substitutable alternatives to their respective serial
counterparts. In this approach, the parallelisation aspect can be isolated into implementing these alternative
components without affecting the rest of the system. As these loops are the computationally expensive part of the
simulation, efficient parallelisation would have the largest impact when exploiting multi-core architectures.
Fig. 5a compares the execution time for different numbers of threads normalised to the serial implementation;
runtime component substitution remains enabled. It is important to note that this is normalized to a single thread
adaptive implementation which already has 4 times speedup over the original reference implementation, i.e. non-

Fig. 5. (a) Normalized Execution time of the multi-threaded simulator with a shared serial RNG for different numbers of threads (testcase
1M/100, normalized to the implementation without OpenMP); (b) Speedup of the multi-threaded simulator with different RNG components
for different numbers of threads.

542

T.M. Riechers et al. / Procedia Computer Science 1 (2012) 535–544
T. M. Riechers, S. Kuo, R.S.M. Goh, T. Hung / Procedia Computer Science 00 (2010) 000–000

adaptive and non-componentised version. The system used and the experimental setup is the same as described in
section 4.2.
4.3.1. Multi-buffered random number generation
In this section, we discuss our final version of random number generator component that is optimised for multibuffered access without lock and has the ability to dynamically adjust generation rates. In this component, we use
one CPU thread to serve multiple simulation threads by maintaining multiple input and output buffers. This thread
may perform the generation in a GPU if available. The chores of maintaining multiple buffers as well as the GPGPU
DMA details are encapsulated into a component interface so that it appears as a serial RNG with a much higher
generation speed. Since the rate at which random numbers are consumed by the infectious disease simulation
threads are not constants, we implement multiple buffers (two per simulation threads plus one for temporary storage
for the GPGPU) to ensure that there is always enough random numbers available when requested by the simulator
threads. This way, we avoid the locking situation described previously.
However, a single GPGPU still have its limitations. By increasing the number of simulation threads in our
system, we can reach a point where the simulation threads consume more random numbers than can be generated.
At this point, our system compensates by creating another appropriately seeded (to ensure a relative prime generator
polynomial as described in [20]) random number generator thread to be executed in the CPU. In a similar way, if a
second GPGPU is available, then it can be utilised. All of this optimisation details are encapsulated and can be made
into substitutable components without system wide side effects.
In summary, we have created two random number generator components with identical serialised interface. They
differ in their architecture specific implementation. The Builder component in the adaptive runtime (Fig. 1b) is
responsible for inserting the correct component according to the system setup, i.e. use the GPU specific
implementation mentioned above if a GPU is present and available. Currently, this decision is performed once just
prior to programme execution and although the component implementations can be substituted to another during
simulation, we have not found a compelling reason to do so.

Fig. 6. Multi-buffered random number generator

4.3.2. Discussion
In Fig. 5a, each additional thread reduces the total execution time of the simulation. However, the reduction is not
reflective of the marginal increase in computation power. Ideally, one would expect the bar graphs to resemble that
of 1/x where x is the number of threads. Through a quick examination of the scalability plot of the speedup vs. cores
Fig. 5b, we note that the speedup start to levelling off at around 5 to 6 cores. This is due to the use of a single
random number generator component that is shared among all the simulation threads. In order to remove this
bottleneck, we have to be able to scale up the speed of random number generation according to the number of
threads. Next, we will touch briefly on random number generators and the reasons behind our design choice.
Typically, random number generators (RNG) maintain an internal state in the form of a sequential state machine.
The state variable is updated every time a random number is generated. One can view the generated random number
as a simple hash of the internal states. Thus, RNGs with a large set of internal states can generate longer sequence of
random numbers without repeating. Due to the finite nature of the internal states, all RNG will eventually repeat.

T.M. Riechers et al. / Procedia Computer Science 1 (2012) 535–544

543

T. M. Riechers, S. Kuo, R.S.M. Goh, T. Hung / Procedia Computer Science 00 (2010) 000–000

Therefore, if we implement two RNG in parallel, we have to ensure that the states of these two RNG are setup in
such a way that these two sequences will not overlap.
In our case study application, profiling of the single thread simulation reveals that the generation of random
numbers account for 17% of the total execution time. When the system is scaled to two threads, concurrent
execution can only be performed on 83% of the code and both threads have to share the same random number
generator. If we assume that the RNG is the only sequential part of the calculation, Amdahl’s Law will predict that a
speed limit at around 5 to 6 threads. This may account for the observed scalability issue.
In order to improve the scalability, one may assign each thread their own RNG. While this approach will improve
scalability, the simulator may have a compromised randomness configuration critical for a stochastic simulation [20]
and would require an architecture dependent builder component in the runtime. In order to maintain the same
component interface as the serial implementation so that parallelisation can be a decision made by the adaptive
runtime, we have implemented an accelerator aided, adaptive random number generator component that can
optionally utilise a GPU if it is present.
To demonstrate the improvement in scalability, we plot the speedup (single threaded execution time / multithreaded execution time) as a function of the number of simulation threads in Fig. 5b. From this graph, we see that
the GPU RNG version has better speedup than a single serial RNG CPU. At 8-threads, the GPU RNG version
achieves a speedup of 5.3 times and that there is a marked improvement over the serial RNG version. Considering
the 4 times speedup over the original implementation, the overall speedup through dynamic kernel adaptation and
parallelisation is up to as much as 21 times on a server with 8-core with a GPU.
5. Conclusion
We have designed and developed a generalised component-based optimizer that allows the creation of multiple
implementations of kernels easily. The component-based runtime system allows dynamic adaptation through
runtime component substitution in which the most appropriate kernel implementation is chosen based on runtime
conditions and the processor platform the application is executed on.
We demonstrate the value of our approach through a case study of a component-based infectious disease
simulator. Our experimental results shows that the component-based simulator with dynamic adaptation has
performance gain of 2 to 4 times as compared to the original non-component-based code in the single threaded
implementation. This gain is independent of the gain that can be obtained through careful parallelisation. We then
further improve the performance of the system by parallelising one of the components without changing the rest of
the simulation. The results reveal that another factor-of-4 improvement (over and above that of adaptation gain) can
be obtained through simple parallelisation. Finally, we created another alternative implementation for the random
number generator component to allow better scaling and demonstrated better speedup. This final implementation
allows a total of up to 21 times speedup over that of the single threaded implementation on a server with 8-core and
one GPU, and is capable of completing a simulation run in about an hour compared with a day.
Throughout the development of this adaptive software, we have managed to maintain the same component
configuration. The various optimisation to provide speedups, whether it is kernel adaptation or accelerating RNG
generation when a GPU is available, has been implemented as alternative components without having to change the
overall system design. We believe that this approach greatly increases the maintainability of the code.
References
1. J. A. Anderson, C. D. Lorenz, A Travesset, General purpose molecular dynamics simulations fully implemented on graphics processing units,
J. Comput. Phys. 227, 10, May 2008.
2. J. A. van Meel, A. Arnold, D. Frenkel, S. F. Portegies Zwart, R. G. Belleman, Harvesting graphics power for MD simulations, Molecular
Simulation 34 (3), 2008, pp. 259-266.
3. M. Gschwind, H. P. Hofstee, B. Flachs, M. Hopkins, Y. Watanabe, T. Yamazaki, Synergistic Processing in Cell's Multicore Architecture,
IEEE Micro, vol. 26, no. 2, March/April 2006, pp. 10-24.
4. Cray XT5hTM Supercomputer, http://www.cray.com/Assets/PDF/products/xt/CrayXT5hBrochure.pdf, January 2009.
5. K. J. Barker, K. Davis, A. Hoisie, D. J. Kerbyson, M. Lang, S. Pakin, J. Sancho, Entering the petaflop era: the architecture and performance of
Roadrunner. In Proceedings of the 2008 ACM/IEEE Conference on Supercomputing, Volume 00, Austin, Texas, November 2008.

544

T.M. Riechers et al. / Procedia Computer Science 1 (2012) 535–544
T. M. Riechers, S. Kuo, R.S.M. Goh, T. Hung / Procedia Computer Science 00 (2010) 000–000

6. S. Balakrishnan, R. Rajwar, M. Upton, K. Lai, The Impact of Performance Asymmetry in Emerging Multicore Architectures, isca, 32nd
Annual International Symposium on Computer Architecture (ISCA'05), 2005, pp. 506-517.
7. R. Kumar, D. M. Tullsen, N. P. Jouppi, P. Ranganathan, Heterogeneous Chip Multiprocessors, Computer, vol. 38, no. 11, November 2005, pp.
32-38.
8. T. M. Riechers, S. Kuo, R. S. M. Goh, H. Soh, T. Hung, A. M. Malik, A Case Study on Dynamic Kernel Adaptation in a Component-based
Infectious Disease Simulator, Workshop on Component-Based High Performance Computing, Supercomputing Conference, 2009.
9. CCA specification, http://cca-forum.org/specification/, 2009
10. C. Fraser, C. A. Donnelly, et al., Pandemic Potential of a Strain of Influenza A (H1N1): Early Findings, Science 324 (5934), 1557.
11. P. K. McKinley, S. M. Sadjadi, E. P. Kasten, B. H.C. Cheng, Composing Adaptive Software, Computer, vol. 37, no. 7, July 2004, pp. 56-64.
12. B. A. Allan, R. Armstrong, The Ccaffeine Framework: Composing and debugging applications interactively and running them statically,
Compframe 2005, 2005
13. L. McInnes, J. Ray, R. Armstrong, T. Dahlgren, A. Malony, B. Norris, S. Shende, J. Kenny, and J. Steensland, Computational Quality of
Service for Scientific CCA Applications: Composition, Substitution, and Reconfiguration, Argonne National Laboratory preprint
ANL/MCS-P1326-0206, February 2006
14. N. Trebon, A. Morris, J. Ray, S. Shende, A. D. Malony, Performance Modeling using Component Assemblies, Concurrency and
Computation: Practice and Experience 19(5), 2007, pp. 685-696
15. N. Mahmood, G. Deng, J. C. Browne, Compositional Development of Parallel Programs, Springer, Berlin / Heidelberg, 2004, pp. 109-126
16. N. Mahmood, Y. Feng, J. C. Browne, Evolutionary performance-oriented development of parallel programs by composition of components,
Proceedings of the 5th international Workshop on Software and Performance, Palma, Illes Balears, Spain, July 2005.
17. P. Hovland, K. Keahey, L. C. McInnes, B. Norris, L. F. Diachin, P. Raghavan, A Quality of Service Approach for High-Performance
Numerical Components. Proceedings of Workshop on QoS in Component-Based Software Engineering, Software Technologies Conference,
Toulouse, France, 2003.
18. P. E. Lekone, B. F. Finkenstädt, Statistical Inference in a stochastic epidemic SEIR model with control intervention: Ebola as a case study,
Biometrics, 2006 (62), pp. 1170-1177.
19. S. Shende and A. D. Malony, The TAU Parallel Performance System, International Journal of High Performance Computing Applications,
Volume 20, Number 2, Summer 2006. pp. 287-311.
20. Makoto Matsumoto and Takuji Nishimura, Dynamic Creation of Pseudorandom Number Generators, Monte Carlo and Quasi-Monte Carlo
Methods 1998, Springer, 2000, pp 56-69

