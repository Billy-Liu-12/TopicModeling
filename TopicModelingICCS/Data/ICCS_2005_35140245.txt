Multi-pass Mapping Schemes for Parallel Sparse
Matrix Computations
Konrad Malkowski and Padma Raghavan
Department of Computer Science and Engineering,
The Pennsylvania State University,
343K IST Building, University Park, PA 16802-6106
{malkowsk, raghavan}@cse.psu.edu

Abstract. Consider the solution of a large sparse linear system Ax = b
on multiprocessors. A parallel sparse matrix factorization is required in
a direct solver. Alternatively, if Krylov subspace iterative methods are
used, then incomplete forms of parallel sparse factorization are required
for preconditioning. In such schemes, the underlying parallel computation is tree-structured, utilizing task-parallelism at lower levels of the
tree and data-parallelism at higher levels. The proportional heuristic has
typically been used to map the data and computation to processors.
However, for sparse systems from ﬁnite-element methods on complex domains, the resulting assignments can exhibit signiﬁcant load-imbalances.
In this paper, we develop a multi-pass mapping scheme to reduce such
load imbalances and we demonstrate its eﬀectiveness for a test suite of
large sparse matrices. Our scheme can also be used to generate improved
mappings for tree-structured applications beyond those considered in this
paper.

1

Introduction

Many computational science and engineering applications concern the numeric
solution of models based on nonlinear partial-diﬀerential-equations on complex
domains, which are discretized using ﬁnite-element or ﬁnite-diﬀerence methods.
When implicit or semi-implicit schemes are used in the solution process, the
total application time can be dominated by the time required for the solution of
the underlying sparse linear systems of the form Ax = b. Consequently, eﬀective
parallel sparse linear system solution is of critical signiﬁcance in such large-scale
applications.
A solution to Ax = b, where A is sparse, can be achieved using either direct
methods or preconditioned iterative methods. In parallel sparse direct solvers,
The work was supported in part by the National Science Foundation through grants
NSF ACI-0102537 and NSF CCF-0444345, and by the Director, Oﬃce of Science,
Division of Mathematical, Information, and Computational Sciences of the U.S. Department of Energy under contract number DE-AC03-76SF00098.
V.S. Sunderam et al. (Eds.): ICCS 2005, LNCS 3514, pp. 245–255, 2005.
c Springer-Verlag Berlin Heidelberg 2005

246

K. Malkowski and P. Raghavan

a Cholesky (A = LLT ) or an LU (A = LU ) factorization is ﬁrst computed and
then used for triangular solution [1, 2, 4, 6, 8, 16]. For preconditioning, incomplete
counterparts of both types of factorizations can be utilized to compute a sparse
approximation to the factors to accelerate the convergence of an iterative method
such as Conjugate Gradients or GMRES [7, 10, 17, 18, 19]. Eﬃcient implementations on distributed memory multiprocessors require data and task assignments
that can balance the computational load for the factorization step among processors. The computations in the factorization step are tree-structured and are
formulated bottom-up on a supernodal tree using either eﬀectively dense panels
of columns in a left-looking panel scheme [1] or using dense triangular matrices
in a multifrontal scheme [8, 9, 13, 16].
In this paper, we focus on mapping tree-structured computations typical of
sparse factorizations where assignments generated by the popular proportional
mapping [5, 15] scheme often exhibit large imbalances. Our main contribution
is the formulation of a new multi-pass reﬁnement scheme that can substantially
improve the quality of the assignment and thus the performance of parallel factorization codes. In the next section, we provide a brief review of parallel sparse
factorization. In Section 3, we begin with a review of the original proportional
mapping scheme [15] and then describe our new multi-pass schemes. In Section 4, we provide an empirical evaluation of the performance of our multi-pass
schemes and the original proportional mapping. In Section 5, we summarize our
contributions and discuss further extensions and applications.

2

Parallel Tree-Structured Sparse Factorization

Sparse matrix factorization and its incomplete variations typically require a four
step process: (1) ordering to compute a ﬁll-reducing numbering, (2) symbolic
factorization to determine the nonzero structure of the factor, (3) numeric factorization, and, (4) triangular solution. The ﬁrst ordering step is also critical for
determining the parallelism and the total computational costs over all remaining steps. A well-established practice is to compute orderings, using for example,
nested dissection techniques that recursively partition the graph of A using vertex separators [3, 11]. After this step, the parallelism available for the subsequent
factorization and triangular solution step can be represented by a tree. This tree
can be weighted to represent computation costs and the tree can be mapped to
processors to enable load-balanced computation of the factorization step.
We provide a brief overview of parallel sparse Cholesky factorization using a
small example to illustrate the main ideas, which are described in greater detail
in the survey article by Heath et al. [8]. Figure 1 concerns the sparse matrix
A of a ﬁve-point 7 × 7 ﬁnite-diﬀerence grid, which is widely used as a model
problem in this area. The columns of the Cholesky factor L of this matrix, can
be grouped into supernodes [12]. A supernode is a set of consecutive columns
that have nested sparsity structure, and can essentially be treated as a dense
block, see for example, the last seven columns in Figure 1. As a consequence of
sparsity in L, columns in a supernode need not be updated by columns in all

Multi-pass Mapping Schemes for Parallel Sparse Matrix Computations
Sparse Matrix, 7X7 grid ND Order

L of 7X7 grid ND Order

247

43-49

0

43-49

5

40-42

5

10

10

15

15

20

20

25

25

30

30

43-49

19-21
40-42

19-21

35

35

40

40

43-49
28-30

7-9

37-39

16-18

6

3

12

15

24

27

36

33

45

45

5

10

15

20

25

30

35

40

45

50

1
0

5

10

15

20

25

30

35

40

45

50

2

4

5 10

11 13

14

22

23 25

26 31

32

34

35

Fig. 1. The structure of a sparse matrix A from a 7 × 7, 5-point ﬁnite-diﬀerence
grid reordered to reduce ﬁll (left), the structure of L shown with a recursive partition
(middle) and a multifrontal scheme on the binary supernodal tree (right)

preceding supernodes in the numeric factorization, instead columns in a supernode v are updated only by columns in supernodes within the subtree rooted at
v [12]. The sparsity structure of L can be viewed in terms of eﬀectively dense
column-blocks or alternatively, in a recursive manner in terms of submatrices, as
shown in Figure 1. The two types of cache-eﬃcient numeric factorizations are a
column-block scheme [14] and a multifrontal scheme [2]. The two schemes diﬀer
in how they compute and apply updates to columns in a given supernode from
columns in earlier supernodes. In a multifrontal scheme, dense triangular matrix
operations are used to factor the columns in a supernode and to accumulate
and propagate updates from these columns to those at the parent and ancestor
supernodes, as shown in Figure 1. Multifrontal schemes typically lead to eﬃcient
parallel implementations [6, 16].
Parallel implementations of both left-looking or multifrontal factorization depend on the supernodal tree which can be weighted to represent the corresponding computation and communication costs [5, 6, 16]. For illustrative purposes,
view this tree as a complete binary tree with more leaves than the number of
processors P and with all nodes having the same computational cost. Now, at
some level l = log2 P , P disjoint nodes can be identiﬁed and the subtrees rooted
at these nodes can be assigned to distinct processors. These subtrees represent
disjoint local computations at processors with ideal task-parallelism. At a level
higher than this one, disjoint processor groups of size 2 can cooperate to perform
data-parallel computations at the supernode and so on, until all processors participate at the root. This is known as the balanced subtree to processor mapping.
The proportional mapping scheme is a generalization of this scheme to derive
assignments for practical problems where the supernodal tree can be highly irregular and the computation at a node and total computations in subtrees can
vary dramatically.

248

3

K. Malkowski and P. Raghavan

Multi-pass Mapping Schemes

In this section, we begin with an overview of the original proportional mapping
scheme [15] and continue with the formulation of our multi-pass assignment
schemes. The latter seek to reﬁne and improve an assignment obtained from the
proportional mapping, which is used in the ﬁrst step.
To provide a precise statement of our mapping schemes, we start with a
deﬁnition of the weighted supernodal tree and a valid mapping, i.e., an assignment of computations represented by the tree to a set of processors. Consider
a supernodal tree T (r) = (V, E, N W, SW ) with V vertices, E edges, rooted at
r ∈ V with two weighting functions SW and N W representing a suitable measure of computational costs. For each vertex v ∈ V , N W (v) is the nodal weight,
corresponding to the cost of computations at the node v. The subtree weight
of v, SW (v) is deﬁned as the sum of nodal weights of all vertices in T (v), the
subtree rooted at v. A mapping M = (T, P ) indicates an assignment of a set of
P disjoint subtrees T (v0 ), T (v1 ), · · · T (vp−1 ) (including all leaf vertices in V ) to
processors 0, 1, · · · (P − 1). These disjoint subtrees represent local task parallel
computations; computations at an interior vertex v are shared equally among
all processors assigned subtrees in T (v). However, such computation cannot proceed until all processors can synchronize at the internal node. Consequently, load
imbalances among processors along diﬀerent paths leading to a vertex v, result
in some processors remaining idle until all can synchronize and proceed with the
computations at v.
The proportional mapping [15] is a recursive scheme with an initial assignment of P processors to the root r and thus T (r). Consider T (v) the subtree at v,
which has been assigned p processors. If p = 1, the recursion terminates; otherSW (c)
wise for p > 1, for each child c of v assign pc to T (c) where pc = p× SW (v)−N
W (v) .
Some rounding scheme must be used to ensure that pc is an integer number of
processors and that c,(c,v)∈E pc = p. We experimented with several rounding
schemes to select the one that leads to the best mappings for our test collection.
In this scheme, at a vertex v with T (v) assigned p processors, we compute for
SW (c)
SW (c)
ˆ
each child c, pˆc = p × SW (v)−N
W (v) , and the projected load W (c) =
pˆc .
Next, we compute p˜ = p − pˆ where pˆ = c,(c,v)∈E pˆc . Let c1 , c2 , · · · cp˜ be the
ˆ (ci ); for
children vertices of v with the p˜ highest values of the projected load W
these vertices we set pc ← pˆc + 1 while for the others, we set pc ← pˆc . Our
multi-pass assignment scheme uses this proportional mapping as the ﬁrst step.
Rounding eﬀects, incurred to ensure an integer number of processors at each
node, can be exaggerated by irregular subtree weights resulting in assignments
with processor loads that are not well-balanced. Consequently, the highest load
at a processor, i.e., the critical path weight, can be substantially higher than the
ideal of SW (r)/P as shown in the next section in Figure 2.
Our multi-pass schemes attempt to reﬁne the assignment from the proportional mapping in order to improve the worst load at a processor. We therefore
start with a precise deﬁnition of this quantity. Consider any mapping M =
(T (r), P ) of the supernodal tree T (r) = (V, E, N W, SW ) with disjoint sub-

Multi-pass Mapping Schemes for Parallel Sparse Matrix Computations
Original Proportional Mapping

249

Robin Hood, Multi−Pass Mapping

40

40

35

35

30

30

25

25

critical overload (%)

critical overload (%)

Robin Hood
Multi−Pass

20

15

20

15

10

10

5

5

0

10

15

20

25

30

35
40
processors

45

50

55

60

65

0

10

15

20

25

30

35
40
processors

45

50

55

60

65

Fig. 2. Critical overload for the original proportional heuristic (left) and and the Robin
Hood and multi-pass schemes (right) for augustus7 on 8 – 64 processors

trees T (v0 ), T (v1 ), · · · T (vP −1 ) assigned to processors 0, 1, · · · (P − 1). Let π(v)
denote the number of processors assigned to a node v; note that π(r) = P .
Let path(i) denote the intermediate vertices in T from the parent of vi to
the root r. Now the workload of a processor pi , 0 ≤ i ≤ (P − 1) is given
W (pi ) = SW (T (vi )) + v∈path(vi ) N W (v)/π(v). An ideal assignment would
lead to the ideal load I(M ) = SW (r)/P . The heaviest load at a processor, which
corresponds to a critical path is given by H(M ) = maxpi ,0≤i≤P −1 {W (pi )}; likewise, we deﬁne the lightest load L(M ) = minpi ,0≤i≤P −1 {W (pi )}; Our goal is
compute mappings where H(M ) is close to I(M ) for a given T and P .
We next present two reﬁnement schemes, followed by our ﬁnal multi-pass
scheme which enables their eﬀective combination.
A Robin Hood Reﬁnement Scheme. Assume that an assignment M has
been provided either from proportional mapping or the application of one or
more reﬁnement schemes. First, determine H(M ) and L(M ) corresponding to
the heaviest and the lightest loads at a processor. Let ph and pl denote the
corresponding processors assigned to subtrees T (h) and T (l), rooted at vertices
h and l respectively. We consider ph , the overworked processor to be “poor ”and
the lightly loaded pm to “rich.” Our Robin Hood scheme, removes a processor
assigned along the lightest path, path(l) and uses it to reﬁne the mapping of
the local subtree T (h) and thus reduce the load along critical path path(h). The
Robin Hood scheme is typically applied four times to allow reﬁnement of paths
with weights close to the weight of the critical path, and the best assignment is
retained.
Iterative Correction with Processors in Reserve. Assume we are seeking
an assignment M = (T (r), P ) with P processors. We ﬁrst obtain a mapping with
˜ = (T (r), P˜ ) with P˜ processors where P˜ < P . Let Pˆ = P − P˜ denote the
M
remaining processors “held in reserve.” Our correction algorithm proceeds thus in
˜ with P1 = P˜ processors.
Pˆ iterations, starting with a mapping M1 initialized to M
At iteration i, compute H(Mi ) and let ph be the corresponding processor with the

250

K. Malkowski and P. Raghavan

heaviest load. Reﬁne the mapping of T (h) by adding another processor to vertex
h to obtain a new mapping Mi+1 with Pi+1 = Pi + 1 processors. A disadvantage
of this scheme is that improvements depend on the initial choice of P˜ .
A Multi-pass Mapping Scheme. We now combine the two reﬁnement schemes
with the original proportional mapping to present our ﬁnal multi-pass mapping
scheme. The ﬁrst pass is the proportional mapping scheme. In the second pass,
this mapping is reﬁned using the Robin Hood scheme. Let this result in a mapˆ with the speciﬁed number of processors, P . Compute H(M
ˆ ); if this quanping M
ˆ ),
tity is more than the ideal load (SW (r)/P ), then compute P˜ = SW (r)/H(M
¯ with P˜
where P˜ < P . Apply the proportional mapping to obtain a mapping M
˜
processors. Reﬁne it using the Robin Hood scheme to obtain a new mapping M
with P˜ processors. Finally, reﬁne this mapping using iterative correction with
Pˆ = P − P˜ processors in reserve. This deﬁnes our overall multi-pass mapping
scheme.

4

Empirical Results

In this section, we empirically evaluate the quality of assignments for performing
parallel sparse Cholesky factorization. We report on the improvements observed
when our schemes are used to reﬁne the assignments computed by the original
proportional scheme. We use a collection of well-known sparse matrices from
ﬁnite-element analysis of three dimensional structures and shells, and one problem from computational ﬂuid dynamics.
Our test suite of matrices and best observed improvements are shown in
Table 1. We consider the factorization of these matrices using 8-64 identical pro-

Table 1. Description of test matrices, relative critical loads (RCL), and numeric factorization times using the original proportional scheme and the multi-pass scheme.
Each matrix - processor pair corresponds to the best observed improvements of the
RCL metric using our multi-pass scheme. The column labeled “Error” indicates the
diﬀerence between predicted and observed execution times
Matrix Characteristics
Matrix Rank |A| |L|
(103 ) (104 ) (104 )
bmw7st1
bmwcra1
bmw3 2 1
augustus7
augustus5
af shell3
cfd2

141
148
227
1,060
134
504
123

374
539
575
518
64
904
160

9,134
18,924
17,998
79,995
4,598
11,425
7465

Relative Critical Load and Numeric Factorization Time
Proc- Proportional Map
Multi-pass Map
essors RCL Time
RCL Predicted Observed Error
(sec)
(sec)
(sec)
(%)
24
134 97
114 82.52
80.72
2
14
146 493
129 435.6
454
4
24
130 263
122 246
242
2
24
133 1,989
117 1,749
1,653
6
26
145 32.42
117 26.16
25.97
1
26
137 43.65
116 36.96
40.11
9
23
154 75.67
118 57.98
57.84
0

Multi-pass Mapping Schemes for Parallel Sparse Matrix Computations

251

cessors after ordering using a nested dissection scheme. The supernodal trees
were weighted to represent computational costs (for ﬂoating point operations,
and not for communication or for other integer operations) in a parallel multifrontal scheme, for example, such as the scheme in the DSCPACK software [16].
Our results concern the quality of mappings as deﬁned by the heaviest workload at a processor, i.e., the critical path weights. Consider T (r) corresponding
to a speciﬁc problem. For each mapping M of T (r) using some P processors in
the range 8 — 64, we focus on heaviest load at a processor, i.e., the critical path
cost, as the main metric indicative of the quality of the mapping. The closer this
metric, H(M ) (deﬁned in the earlier section) is to the ideal load (I = SW (r)/P ),
the better is the quality of the mapping. Over the range of processors, problems
and mapping the actual value of this metric can vary signiﬁcantly making direct
comparisons diﬃcult. We therefore use the following two scaled forms: (i) the
)
× 100, and (ii) the critical overload,
relative critical load, (RCL) deﬁned as H(M
I
H(M )−I
(CO) deﬁned as
× 100.
I
We begin with some experiments (reported in the right half of Table 1) to verify
that the relative critical load (and the critical overload) metric corresponds well
to the actual performance of the numeric factorization step. We used the DSCPACK software [16] with two diﬀerent mappings, one from the original proportional scheme and the other from our multi-pass scheme. Our experiments were
performed on a cluster with 81 dual-processor compute nodes with AMD Athlon
MP2200+ processors, with 71 nodes having 1GB of main memory, and 10 nodes
having 2GB of main memory, and a 9 × 9 torus Scali interconnect. Table 1 shows
the relative critical load (RCL) and CPU time for numeric factorization using the
original proportional mapping. We then compute assignments using our multi-pass
scheme and their corresponding values of the relative critical load (also reported
in Table 1). Using the relative critical load metrics for the original and multi-pass
mappings and the CPU time for numeric factorization with the proportional mapping, we can project the estimated numeric factorization time for the new assignment (reported in the column labeled “Predicted” time). We next performed numeric factorization with the new assignments and observed actual CPU time. As
shown in Table 1, these observed times are in close agreement with our predicted
values, thus indicating that it is valid to use the relative critical load metric and
the closely related critical overload metric to evaluate the quality of assignments.
Figure 2 plots the critical overload metric for the original proportional mapping, the Robin Hood scheme and our ﬁnal multi-pass scheme for augustus7 on
8 through 64 processors. The plots indicate that Robin Hood scheme can improve
the assignments produced by the original proportional mapping. However, as expected, these improvements are not as substantial as the improvements from our
ﬁnal multi-pass scheme. Consequently, in the remainder of this section, we focus
on more detailed comparisons between the quality of assignments produced by
the original proportional scheme and our ﬁnal multi-pass scheme.
Figure 3 (left) indicates how our scheme can improve the worst mapping
generated by the original scheme. For each matrix, we select the instance with
the largest value of the critical overload metric from an assignment by the pro-

252

K. Malkowski and P. Raghavan
Best Improvement

60

50

50

40

40
critical overload (%)

critical overload (%)

Worst critical overload

60

30

30

20

20

10

10

0

bmw7st1

bmwcra1

bmw321

augustus5 augustus7

afshell3

0

cfd2

bmw7st1

bmwcra1

bmw321

augustus5 augustus7

cfd2

afshell3

Fig. 3. Quality of assignments for worst load instances (left) and the best observed
improvement instances (right); each group of two bars indicates the critical overload
from the original proportional scheme and our multi-pass scheme. Average values are
indicated by horizontal lines
Multi−Pass Mapping
Original Proportional Mapping

250
250

200
cumulative critical overload (%)

cumulative critical overload (%)

200

150

150

100

100

50

50

0
15

20

25

30

35

40
45
processors

50

55

60

65

0
15

20

25

30

35

45
40
processors

50

55

60

65

Fig. 4. Cumulative critical overload over all matrices for each processor, from assignments using the original proportional scheme (left) and from our multi-pass scheme
(right). Each patch in a stacked bar represents the critical overload for one matrix.
Average values are indicated by horizontal lines

portional scheme. For this instance, i.e., problem-processor pair, we show the
value of the overload metric when our multi-pass scheme is used. Our multi-pass
scheme successfully reduced the worst case overload from almost 60% to 27%
for the bmw7st1 matrix. On average, the metric is halved from nearly 50% for
the original to 25% for our multi-pass scheme. Figure 3 (right) shows the best
observed improvement from our new multi-pass scheme when compared to the
original mapping for each matrix in the test set. Assignments from our multipass scheme reduced the critical overload for bmw7st1 from approximately 60%
to 27% and from 55% to 17% for cfd2. Additionally, on average over these in-

Multi-pass Mapping Schemes for Parallel Sparse Matrix Computations

253

Multi−Pass Mapping
Original Proportional Mapping

1400
1400

1200

cumulative critical overload (%)

1200

cumulative critical overload (%)

1000

1000

800

600

800

600

400
400

200
200

0
0

bmw7st1

bmwcra1

bmw321

problems augustus7
augustus5

afshell3

bmw7st1

bmwcra1

bmw321

problems augustus7
augustus5

afshell3

cfd2

cfd2

Fig. 5. Cumulative critical overload for 16 – 64 processors for each matrix, from assignments using the original proportional scheme (left) and from our multi-pass scheme
(right). Each patch in a stacked bar represents the critical overload for one processor
size in the range 16 – 64. Average values are indicated by horizontal lines

stances, the critical overload from the proportional scheme was 46%, whereas it
was reduced to 17% from our multi-pass schemes.
We next consider the overall quality of the assignments produced by the
original proportional scheme and our multi-pass scheme for the 7 matrices using
16 – 64 processors. We now consider a cumulative form of the critical overload
metric shown as a stacked bar in Figures 4 and 5. In Figure 4, each stacked bar
represents the critical overload value summed over all 7 matrices for a speciﬁc
number of processors. Figure 4 clearly indicates that our multi-pass scheme
signiﬁcantly improves the quality of mapping over all problems for the entire
range of processors. On average, the cumulative critical overload is reduced from
a value of approximately 150 to 100 through the use of our multi-pass scheme.
Figure 5 shows the improvements for each matrix cumulatively over the range
of processors; each stacked bar represents the critical overload value summed
over all 49 processor sizes (from 16 — 64) for a speciﬁc matrix. Now the average
value from the original scheme is approximately 1050 which is reduced to under
725 by our multi-pass mapping scheme. These results show that our multi-pass
schemes are indeed eﬀective in producing assignments that substantially improve
the balance of loads among processors.

5

Conclusions

In this paper, we have presented a multi-pass scheme that improves workload
distribution among processors in tree-structured computation. Through experiments using trees weighted to represent sparse multifrontal factorizations on distributed memory multiprocessors we show that our multi-pass mapping scheme
can signiﬁcantly improve the quality of assignments.

254

K. Malkowski and P. Raghavan

Our scheme can be applied with a more complex weighting scheme to take into
account both computation and interprocessor computation costs. Our scheme
can also be used with a weighting function to model memory requirements to
produce more balanced assignments. Additionally, we can compute assignments
for applications where the triangular solution costs following the factorization
are dominant, i.e., one factorization is followed by solutions for a sequence of
right-hand-side vectors [20]. We can also extend the weighting schemes to model
parallel incomplete factorization for preconditioning [18] and the subsequent
application of the preconditioner using tree-structured parallel schemes.

References
1. J. Demmel, S. C. Eisenstat, J. R. Gilbert, X. S. Li, and J. W. H. Liu. A supernodal
approach to sparse partial pivoting. Technical Report CSL–94–14, Xerox Palo Alto
Research Center, 1995.
2. I.S. Duﬀ. Parallel implementation of multifrontal schemes. Parallel Computing,
3:193–204, 1986.
3. A. George and J. W. H. Liu. An automatic nested dissection algorithm for irregular
ﬁnite element problems. SIAM J. Numer. Anal., 15:1053–1069, 1978.
4. J. A. George and J. W. H. Liu. Computer Solution of Large Sparse Positive Deﬁnite
Systems. Prentice-Hall Inc., Englewood Cliﬀs, NJ, 1981.
5. L. Grigori and X. S. Li. A new scheduling algorithm for parallel sparse LU factorization with static pivoting. In Proceedings of the 2002 ACM/IEEE conference on
Supercomputing, pages 1–18. IEEE Computer Society Press, 2002.
6. A. Gupta, F. Gustavson, M. Joshi, G. Karypis, and V. Kumar. PSPASES:
An eﬃcient and scalable parallel sparse direct solver, 1999.
See
http://www-users.cs.umn.edu/$\sim$mjoshi/pspases.
7. A. Gupta, V. Kumar, and A. Sameh. Performance and scalability of preconditioned conjugate gradient methods on the CM-5. In R. F Sincovec, D. E. Keyes,
M. R. Leuze, L. R. Petzold, and D. A. Reed, editors, Proceedings of the Sixth
SIAM Conference on Parallel Processing for Scientiﬁc Computing, pages 664–674,
Philadephia, PA, 1993. SIAM Publications.
8. M. T. Heath, E. Ng, and B. W. Peyton. Parallel algorithms for sparse linear
systems. SIAM Review, 33:420–460, 1991.
9. M. T. Heath and P. Raghavan. Performance of a fully parallel sparse solver. Int.
J. Supercomputing Appl., 11:49–64, 1997.
10. M. T. Jones and P. E. Plassman. An improved incomplete Cholesky factorization.
ACM Trans. Math. Software, 21:5–17, 1995.
11. G. Karypis and V. Kumar. METIS: Unstructured graph partitioning and sparse
matrix ordering system. Technical report, Department of Computer Science, University of Minnesota, Minneapolis, MN, 1995.
12. J. W. H. Liu. The role of elimination trees in sparse factorization. SIAM J. Matrix
Anal. Appl., 11:134–172, 1990.
13. J. W. H. Liu. The multifrontal method for sparse matrix solution: theory and
practice. SIAM Review, 34:82–109, 1992.
14. E. Ng and B. W. Peyton. A supernodal Cholesky factorization algorithm for
shared-memory multiprocessors. SIAM J. Sci. Comput., 14:761–769, 1993.

Multi-pass Mapping Schemes for Parallel Sparse Matrix Computations

255

15. A. Pothen and C. Sun. A mapping algorithm for parallel sparse Cholesky factorization. SIAM J. Sci. Comput., 14(5):1253–1257, 1993.
16. P. Raghavan. DSCPACK: Domain-Separator Codes for the parallel solution of
sparse linear systems, 2002. Software for solving sparse linear systems on multiprocessors an d NOWs using C and MPI. Package has a two stage parallel nested
dissection, higher-level BLAS, fast repeated solves and an easy t o use parallel
interface. See http://www.cse.psu.edu/Dscpack.
17. P. Raghavan, K. Teranishi, and E. Ng. Scalable parallel preconditioning with
incomplete factors. In Proceedings of the Seventh SIAM Conference on Applied
Linear Algebra. 2000.
18. P. Raghavan, K. Teranishi, and E. Ng. A latency tolerant hybrid sparse solver
using incomplete Cholesky factorization. Numerical Linear Algebra, 10:541–560,
2003.
19. Y. Saad. Iterative Methods for Sparse Linears Systems. PWS Publishing Co.,
Boston, MA, 1996.
20. C. Yang, P. Raghavan, L. Arrowood, B. Sumpter, and D . Noid. Large-scale
normal coordinate analysis on distributed memory mu ltiprocessors and nows. Int.
J. Supercomputing Appl., 1(4):409–424, 2002.

