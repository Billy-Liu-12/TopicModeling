Fast Quadruple Precision Arithmetic Library on
Parallel Computer SR11000/J2
Takahiro Nagai1 , Hitoshi Yoshida1 , Hisayasu Kuroda1,2 ,
and Yasumasa Kanada1,2
1

Dept. of Frontier Informatics, The University of Tokyo,
2-11-16 Yayoi Bunkyo-ku Tokyo, Japan
{takahiro.nagai,hitoshi.yoshida}@klab.cc.u-tokyo.ac.jp
2
The Information Technology Center, The University of Tokyo,
2-11-16 Yayoi Bunkyo-ku Tokyo, Japan
{kuroda,kanada}@pi.cc.u-tokyo.ac.jp

Abstract. In this paper, the fast quadruple precision arithmetic of
four kinds of basic operations and multiply-add operations are introduced. The proposed methods provide a maximum speed-up factor of
5 times to gcc 4.1.1 with POWER 5+ processor used on parallel computer SR11000/J2. We also developed the fast quadruple precision vector
library optimized on POWER 5 architecture. Quadruple precision numbers, which is 128 bit long double data type, are emulated with a pair of
64 bit double data type on POWER 5+ prosessor used on SR11000/J2
with Hitachi Optimizing Compiler and gcc 4.1.1. To avoid rounding errors in computing quadruple precision arithmetic operations, emulation
needs high computational cost. The proposed methods focus on optimizing the number of registers and instruction latency.

1

Introduction

Some numerical methods require much more computation complexity due to
rounding errors as increasing the scale of a problem. For example, CG method,
one of the solutions for linear equation Ax=b and using Krylov subspace, is
aﬀected by computation errors on a large scale problem. Floating point arithmetic operations generate rounding errors because a real number is approximated
with the ﬁnite number of signiﬁcant ﬁgures. To reduce errors in ﬂoating point
arithmetic, quadruple precision arithmetic, e.g. higher precision arithmetic is
required.
Quadruple precision number, which is a 128 bit long double data type, can
be emulated with a pair of 64 bit double precision numbers on POWER 5 architecture by the run-time routine. The cost of the quadruple precision operations
takes much more than the double precision operations. In this paper, we present
the fast quadruple precision arithmetic of four basic arithmetic operations, i.e.
{+, −, ×, ÷} and multiply-add operation, and vector library for POWER 5 architecture based machine such as parallel computer SR11000/J2. We implemented
M. Bubak et al. (Eds.): ICCS 2008, Part I, LNCS 5101, pp. 446–455, 2008.
c Springer-Verlag Berlin Heidelberg 2008

Fast Quadruple Precision Arithmetic Library

447

Table 1. IEEE 754 data type of 64 bit double and 128 bit long double on SR11000/J2
Data type

Total bit Exponent Exponent range Signiﬁcand number of signiﬁcant
length bit length
bit length ﬁgures in decimal
IEEE 754
64
11
−1022 ∼ 1023
52
about 16.0
128
11 × 2
−1022 ∼ 1023
52 × 2
about 31.9
SR11000/J2

fast quadruple precision arithmetics and made up a quadruple precision vector library including four basic arithmetics and multiply-add operations. We achieved
high performance that maximum speed up factor of 5 times to gcc 4.1.1.

2

128-Bit Long Double Floating Point Data Type

POWER5+ processors, which are CPUs of SR11000/J2, have 64 bit ﬂoating
point registers. In 64 bit architecture, to store ﬂoating point data with quadruple
precision, a pair of 64 bit registers is used by the software. Quadruple precision
can handle up to about 31 decimal digits precision number, compared to (1 +
16 handled by double precision. The point to notice is that the
52)×log102
exponent range is the same as that of double precision. Although the precision
is greater, the magnitude of representable numbers is the same as 64 bit double
precision numbers. That is, while 128 bit data type can store numbers with more
precision than 64 bit data type, it does not store numbers of greater magnitude.
The details are as follows. Each pair of 64 bit numbers has two 64 bit ﬂoating
point data type with sign, exponent and signiﬁcand. We show the data format
with IEEE 754 standard explained in Table 1[5]. Typically, the low-order part
has a magnitude that is less than 0.5 units in the last place of the high-order
part, so the value of two parts never overlap and the entire signiﬁcand of the
low-order number adds precision beyond the high-order number.

3

Quadruple Precision Arithmetic

All of algorithms as follows are double or quadruple precision data type based on
the round-to-nearest rounding mode. We express the ﬂoating operations using
{⊕, , ⊗, } for {+, −, ×, ÷} respectively. For example, ﬂoating point addition
a + b = ﬂ(a + b)+err(a + b) exactly, then we use a ⊕ b = ﬂ(a + b) to denote the
result of addition, and err() is the error caused by the operation.
Now, we explain quadruple precision arithmetic operations consisted of two
kinds of basic algorithms, Quick-TwoSum() and TwoSum(). These 64 bit double
precision algorithms are already used and implemented on gcc 4.1.1 for 128 bit
long double data type[3] and they have explained in papers[1,2,4,8].
This type of 128 bit long double data type does not support IEEE special
numbers, NaN and INF. The quadruple precision algorithms introduced in this
paper are not satisﬁying the IEEE compliance.

448

3.1

T. Nagai et al.

About Precision

There are two kinds of quadruple precision algorithms for addition operation for
the accuracy.
– Accuracy of 106 bit signiﬁcand
– Accuracy of about 106 bit signiﬁcand permitting a few bits rounding errors
in the last part
Compared with these algorithms, the latter method is realized with the half
number of instructions of the former one by permitting a few bits rounding error
in the last part. This reason is extra instructions in error compensation. In this
paper, we select the latter algorithm by focusing on speeding up of quadruple
precision arithmetic.
We have already quantitatively analyzed the quadruple precision arithmetic
of addition and multiplication[11]. Here we introduce the quadruple precision
algorithms optimized and implemented as vector library.
3.2

Addition

Quadruple precision addition algorithm of Quad-TwoSum(a, b), which is consisted of ﬂoating point addition TwoSum(), computes (sH , sL ) = ﬂ(a + b). Here,
(sH , sL ) is a part of s, s = sH + sL . Each of sH and sL is 64 bit data type and indicates high-order and low-order part respectively. Then, a, b, s are 128 bit data
type. We do not need to consider separating the quadruple precision numbers
because each number is stored into memory as 64 bit data type automatically.
TwoSum() algorithm[2] also computes s =ﬂ(c + d) and e =err(c + d). We have
Quad-TwoSum(a, b){
(t, r) ←TwoSum(aH , bH )
e ← r ⊕ aL ⊕ b L
sH ← t ⊕ e
sL ← t sH ⊕ e
return (sH , sL )
}

TwoSum(c, d){
s←c⊕d
v←s c
e ← (c (s v)) ⊕ (d
return (s, e)
}

v)

to pay attention that both of c and d are not 128 bit data type but 64 bit double
data type.
Quad-TwoSum() is addition routine for quadruple precision numbers with
error considerations using TwoSum() algorithm.
Then the number of operation steps is 11 Flop (FLoating point OPeration),
sum of Two-Sum 6 Flop and 5 Flop from addition and subtraction operations.
We see that this quadruple precision algorithm requires 11 times more operations
compared to 1 Flop of double precision. Flop indicates the number of ﬂoating
point operations.

Fast Quadruple Precision Arithmetic Library

3.3

449

Multiplication

Quadruple precision multiplication algorithm Quad-TwoProd(a, b) computes
(pH , pL ) = ﬂ(a × b). (pH , pL ) is a part of p, p = pH + pL . Then, a, b, p are also
128 bit data type.
Quad-TwoProd(a, b){
m1 ← aH ⊗ b L
t ← aL ⊗ b H ⊕ m 1
p H ← aH ⊗ b H ⊕ t
e ← aH ⊗ b H p H
pL ← e ⊕ t
return (pH , pL )
}
Some processors have FMA (Fused Multiply-Add) instruction set that can compute expressions such as a × b ± c with a single rounding error. It is a merit of
this instruction that there are not double rounding errors for addition following
multiplication operation. FMA instruction is comparatively fast because it is implemented on hardware as well as addition or multiplication instruction. A series
of POWER processor can handle FMA instruction, so we made up multiplication algorithm using FMA instruction. It costs 8 Flop in quadruple precision
multiplication operation.
3.4

Division

Quadruple precision division Quad-TwoDiv(a, b) computes (qH , qL ) = ﬂ(a ÷ b).
(qH , qL ) is a part of q, q = qH + qL . Then, a, b, q are also 128 bit data type.
Quad-TwoDiv(a, b){
d1 ← 1.0 bH
m1 ← aH ⊗ d1
e1 ← −(bH ⊗ m1 aH )
m1 ← d1 ⊗ e1 ⊕ m1
m2 ← −(bH ⊗ m1 aH )
m2 ← aL ⊕ m 2
m2 ← −(bL ⊗ m1 m2 )
m3 ← d1 ⊗ m2
m2 ← −(bH ⊗ m3 m2 )
m2 ← d1 ⊗ m2 ⊕ m3
qH ← m1 ⊕ m2
e2 ← m1 qH
qL ← m2 ⊕ e2
return (qH , qL )
}
This algorithm is based on the Newton-Raphson method. The number of operation steps is 18 Flop and 1 double division operation. The deﬁnition of Flop

450

T. Nagai et al.

does not include the double division operation because it is costly compared to
the cost of double precision addition and multiplication operations.
This algorithm is applicable in the usual case that special numbers such as
NaN, INF are not generated by ﬁrst operation of double division (1.0 b).

4

Speeding-Up the Quadruple Precision Arithmetic

We quantitatively evaluate each algorithms of addition and multiplication on
parallel computer SR11000/J2 at Information Technology Center, the University
of Tokyo. In terms of the number of operations, addition takes 11 Flop and
multiplication takes 8 Flop. Division takes 18 Flop and 1 double precision data
division. From the analysis in term of the number of addition and multiplication
operations, it is possible to speed up by reducing data dependency between
instructions under condition that the each instruction latency such as fadd, fmul,
fmadd of processors is the same clocks as others.
And we have considered the multiply-add operation in quadruple precision
with the combination of multiplication and addition.

5

Optimizing Quadruple Precision Arithmetic

First, the theoretical peak performance is 9.2 GFlops in one processor on
SR11000/J2. Quadruple precision arithmetic operations are rarely aﬀected by
the delay of data transfer from main memory to register because computation
time of one quadruple precision operation is large. To get high performance, it is
most important to increase throughput and hide instruction latency by pipelining
the operations for vector data. To realize pipeline processing, we focus on the
loop unrolling.
We see that latency of ﬂoating point instructions on POWER 5+ such as
fadd, fmul, fsub, fabs and fmadd is 6 clocks. Throughput is 2 clocks for fmadd
and 1 clock for others. Fig.1 shows the pipeline processing in case of instruction
latency is 6 clock.

Fig. 1. Pipelining for 6 clock instruction latency

Fast Quadruple Precision Arithmetic Library

5.1

451

Hiding Instruction Latency

Because of loop unrolling, we can optimize performance by way of hiding instruction latency. Data dependency of quadruple precision arithmetic operations is
solved by loop-unrolling, which lines up same instructions as follows. An example of solution is shown below. Here, f r means a 64 bit ﬂoating point register in
POWER architecture. In Problem(), there is data dependency in three instructions, {+, ×, ÷}. It is possible to hide latency by loop unrolling like Solution(),
whose unrolling size is 2.
Solution(){
f r1 ← f r2 + f r3
f r9 ← f r7 + f r8
f r5 ← f r1 × f r4
f r11 ← f r9 × f r10
f r7 ← f r5 ÷ f r6
f r13 ← f r11 ÷ f r12
}

Problem(){
f r1 ← f r2 + f r3
f r5 ← f r1 × f r4
f r7 ← f r5 ÷ f r6
}

5.2

Number of Registers

Loop unrolling prevents from stall of CPU resource among instructions. As
POWER 5+ processor has 32 logical registers, we used the full logical registers. In fact, there are 120 physical registers and they are utilized by the register renaming function. If m is the number of registers needed for 1 quadruple
operation,
maximum unrolling size = 32 / m
(1)
Quadruple precision addition needs 4 registers in 1 operation of ci = ai + bi ,
that is, m is 4. We can realize maximum unrolling size of 8.
maximum unrolling size = 32 / 4 = 8

(2)

In a similar way, quadruple precision multiplication of ci = ai × bi also needs
4 registers, then m is 4. The maximum unrolling size is 8.
Quadruple precision division of ci = ai / bi needs 5 registers in 1 operation,
then m is 5. Maximum unrolling size is 32/5 = 6. To attain unrolling size 8 as
well as addition or multiplication operation, we store 1 register data into memory
and reload when it is needed. This method achieves unrolling size of 32/4 = 8.

6

How to Use Quadruple Precision Arithmetic
Operations Library

We have discussed algorithms and how to optimize quadruple precision arithmetic for vector data in sections 3, 4 and 5. The interfaces of each quadruple precision arithmetic operations are shown in this section. This library is especially
eﬀective for vector data and implemented in C with optimized assembler-code.
Users specify the include ﬁle ”quad vec.h” in C and call each arithmetic function
in library. We have to note here that it is easy for adaptation to FORTRAN.

452

T. Nagai et al.
Table 2. Compile option
compile option
Optimizing C Compiler 01-03/C
cc -Os +Op -64
-noparallel
No paralleled
-roughquad
Quadruple precision (add, multiply, div)
gcc -maix64 -mlong-double-128 -O3
gcc 4.1.1

– Addition ci = ai + bi
void qadd vec(long double a[],long double b[],long double c[],int n)
– Subtraction ci = ai − bi
void qsub vec(long double a[],long double b[],long double c[],int n)
– Multiplication ci = ai × bi
void qmul vec(long double a[],long double b[],long double c[],int n)
– Division ci = ai / bi
void qdiv vec(long double a[],long double b[],long double c[],int n)
– Multiply-Add ci = s × bi + ci (s : constant)
void qmuladd vec(long double *s,long double b[],long double c[],int n)

Here are the sample program routine computing matrix-multiplication in size
N using qmuladd vec() described above.
long double a[N][N], b[N][N], c[N][N];
···

for(i=0;i<N;i++)
for(j=0;j<N;j++)
qmuladd vec(&a[i][j], &b[j][0], &c[i][0], N);
···

7

Numerical Experiment

We implemented and evaluated four kinds of arithmetic operations, addition,
multiplication, division and multiply-add operation. Subtract is same operation as addition except for sign. Our proposed methods were optimized with
assembler-code and compared with Hitachi Optimizing Compiler of SR11000/J2
[10] and gcc 4.1.1. OS is IBM AIX version 5.3 with large page setting[9]. Compile
options are shown in Table 2.
The experimented data size is six patterns, i.e. size of L1 cache, half of L2,
L2, half of L3, L3 and out of L3. We measured the MQFlops value (1 quadruple
precision operation in 1 second is deﬁned as 1 QFlops). Figures from Fig.2
to Fig.9 show the quadruple precision arithmetic operation performances. The
eﬀective clocks, which is the clocks in each loop unrolling size in one loop in our
proposed method, is shown in Fig.2 and the computational performance is shown
in Fig.3 in addition. Our proposed methods in quadruple precision arithmetic
operations show high performance in all of data ranges. Performances of our
proposal and Hitachi optimizing compiler in quadruple precision addition are

Fast Quadruple Precision Arithmetic Library

Fig. 2. Eﬀective Clocks in our proposed
addition

Fig. 3. MQFlops in addition

Fig. 4. Eﬀective Clocks in our poposed
multiplication

Fig. 5. MQFlops in multiplication

Fig. 6. Eﬀective Clocks in our proposed division

Fig. 7. MQFlops in division

453

almost same. Operations in gcc 4.1.1 are much slow because its execution calls
library in each steps and it takes much cost in function overhead.
From the result of quadruple precision addition, our proposed method attained
73.70/38.34 1.9 times speed up than that of Hitachi optimizing compiler and
73.70/13.96 5.3 times speed up than that of gcc 4.1.1 when data size is just on

454

T. Nagai et al.

Fig. 8. Eﬀective Clocks in multiply-add

Fig. 9. MQFlops in multiply-add

Fig. 10. Matrix multiplication using multiply-add arithmetic operation

L1 cache. At the end, matrix-multiplication result using optimized multiply-add
operation is shown in Fig.10.

8

Concluding Remarks

In this paper, fast quadruple precision arithmetic of four kinds of basic arithmetic
operations and multiply-add operation are developed and evaluated. The proposed methods provide a maximum speed-up 5 times faster for vector data than
gcc 4.1.1 with POWER 5+ processor on parallel computer SR11000/J2. Even
though proposed method in quadruple precision addition operation is almost the
same with Hitachi optimizing compiler in performance, other quadruple precision arithmetic operations’ results show high performance in all of data ranges.
We developed the fast quadruple precision library for vector date optimized on
POWER 5 architecture. Quadruple precision arithmetic operations are costly,
compared with double precision operations because compensating rounding errors. Then we applied the best optimization of hiding latency to ﬁt the number
of registers by loop unrolling to quadruple precision arithmetic operation.

Fast Quadruple Precision Arithmetic Library

455

As a future work, we have to develop quadruple precision library, which will be
available in various architecture such as Intel and AMD. POWER architecture as
well as PowerPC has FMA instructions which can operate in same clocks as add
or multiply. Especially, in some environment where there is no FMA instruction,
we have to develop fast algorithm in quadruple precision arithmetic operations.

References
1. Dekker, T.J.: A Floating-Point Technique for Extending the Available Precision.
Numer. Math. 18, 224–242 (1971)
2. Knuth, D.E.: The Art of Computer Programming, 2nd edn. Addison-Wesley Series
in Computer Science and Information. Addison-Wesley Longman Publishing Co.,
Inc, Amsterdam (1978)
3. The GNU Compiler Collection, http://www.gnu.org/software/gcc/index.html
4. A fortran-90 double-double library,
http://www.nersc.gov/∼ dhbailey/mpdist/mpdist.html
5. ANSI/IEEE754-1985 Standard for Binary Floating-Point Arithmetic (1985)
6. Akkas, A., Schulte, M.J.: A Quadruple Precision and Dual Double Precision
Floating-Point Multiplier. In: DSD 2003: Proceedings of the Euromicro Symposium on Digital Systems Design, pp. 76–81 (2003)
7. Hida, Y., Li, X.S., Bailey, D.H.: Algorithms for quad-double precision ﬂoating point
arithmetic. In: Proceedings of the 15th Symposium on Computer Arithmetic, pp.
155–162 (2001)
8. Bailey, D.H.: High-Precision Floating-Point Arithmetic in Scientiﬁc Computation.
In: Computing in Science and Engineering, vol. 07, pp. 54–61. IEEE Computer
Society, Los Alamitos (2005)
9. AIX 5L Riﬀerences Guide Version 5.3 (IBM Redbooks). IBM Press (2004)
10. Optimizing C User’s Guide For SR11000. Hitachi, Ltd. (2005)
11. Nagai, T., Yoshida, H., Kuroda, H., Kanada, Y.: Quadruple Precision Arithmetic
for Multiply/Add Operations on SR11000/J2. In: Proceedings of the 2007 International Conference on Scientiﬁc Computing CSC, Worldcomp 2007, Las Vegas, pp.
151–157 (2007)

