Procedia Computer Science
Volume 80, 2016, Pages 1033–1041
ICCS 2016. The International Conference on Computational
Science

Denormalize and Delimit: How not to Make Data
Extraction for Analysis More Complex than Necessary
Alex F. Bokov1*, Laura Manuel1, Catherine Cheng1, Angela Bos1, and
Alfredo Tirado-Ramos1*
1
The University of Texas Health Science Center at San Antonio,
Department of Epidemiology and Biostatistics, Clinical Informatics Research Division
informatics@uthscsa.edu

Abstract
There are many legitimate reasons why standards for formatting of biomedical research data are
lengthy and complex (Souza, Kush, & Evans, 2007). However, the common scenario of a
biostatistician simply needing to import a given dataset into their statistical software is at best underserved by these standards. Statisticians are forced to act as amateur database administrators to pivot
and join their data into a usable form before they can even begin the work that they specialize in
doing. Or worse, they find their choice of statistical tools dictated not by their own experience and
skills, but by remote standards bodies or inertial administrative choices. This may limit academic
freedom. If the formats in question require the use of one proprietary software package, it also raises
concerns about vendor lock-in (DeLano, 2005) and stewardship of public resources.
The logistics and transparency of data sharing can be made more tractable by an appreciation of the
differences between structural, semantic, and syntactic levels of data interoperability. The semantic
level is legitimately a complex problem. Here we make the case that, for the limited purpose of
statistical analysis, a simplifying assumption can be made about structural level: the needs of a large
number of statistical models can often be met with a modified variant of the first normal form or 1NF
(Codd, 1979). Once data is merged into one such table, the syntactic level becomes a solved problem,
with many text based formats available and robustly supported by virtually all statistical software
without the need for any custom or third-party client-side add-ons. We implemented our
denormalization approach in DataFinisher, an open source server-side add-on for i2b2 (Murphy et al.,
2009), which we use at our site to enable self-service pulls of de-identified data by researchers.
Keywords: relational databases, electronic health records, data extraction, data formats, data transformation,
health services research

Selection and peer-review under responsibility of the Scientiﬁc Programme Committee of ICCS 2016
c The Authors. Published by Elsevier B.V.

doi:10.1016/j.procs.2016.05.403

1033

Denormalize and Delimit: How not to Make Data ...

Bokov, Manuel, Cheng, Bos, Tirado-Ramos

1 Introduction
Healthcare systems are grappling with truly difficult problems of pulling heterogeneous sources of
patient data into usable data warehouses, implementing data models and terminologies to support
federated queries against these data warehouses, and adopting standards for combining the results of
these queries. While the challenges of multi-site data interchange are being addressed, biostatisticians
could already be conducting useful analysis of patient populations at their own home institutions far
beyond feasibility counts and clinical trial recruitment. The problem of extracting EHR data in a form
that is usable to biostatisticians has been unnecessarily complicated by conflation with the separate
problems of data interchange and terminology alignment and by overreach of standards bodies into
recommending specific vendors and even versions of analytic software instead of focusing on
statistical methodology and open formats. Here we present a use-tested approach to data extraction
that is deliberately limited in scope to data structure, uses site-level rules for handling different types
of data elements, and does not constrain the statistician's choice of analytic software.

2 Many Statistical Methods Converge on One Simple Format
There are countless models and methodologies for analyzing data, but a large number of them fall
into four basic classes: categoric, cross-sectional, longitudinal, and time-to-event. Categorical data is
analyzed by calculating various types of correlation coefficients on frequency counts organized in
contingency tables, and sometimes this is all that can be done for a large population. But much richer
data is collected during a typical patient visit and that data is being underutilized. In addition to
categoric variables (such as demographic and insurance information) of which binary variables (such
as diagnosis and procedure codes) are a special case, there are numeric variables (such as lab values
and drug dosages). There are often complex interactions between variables, and ignoring these
amounts to ignoring heterogeneous treatment effects. Therefore, such data is best analyzed using
general linear models, which encompass ANOVA and various forms of linear regression. A set of
isolated visits could be considered multivariate cross sectional data, but isolated visits are the
exception, not the rule. A typical patient record is a time series of visits. The outcomes observed
during any visit may be correlated with outcomes during previous visits by the same patient
(autocorrelation), there may be individual-by-covariate interactions, and a confounding effect of some
individuals having more datapoints than others (Seaman, Pavlou, & Copas, 2014). To mitigate these
biases, mixed-effect or panel data models can be used. Sometimes the outcome of interest is how
much time one can expect to elapse for a patient from a particular population before an event of
interest (e.g. diagnosis, recurrence, death) or, equivalently, what is the risk that an event of interest
occurs during a given time interval. For such questions, time-to-event or survival models are used.
This may seem like a bewildering array of statistical models to support, but the cross sectional,
longitudinal, and time-to-event models-- require their input data to be structured in the same way
regardless of whether the statistician is using SAS (SAS Institute, 2011), Stata (StataCorp, 2015), R (R
Core Team, 2015), Julia (Bezanson, Karpinski, Shah, & Edelman, 2012), or Python/Pandas
(McKinney, 2010, 2013). Formally, this logical format is known as first normal form (1NF) (Codd,
1979) with the additional constraint that the data be contained in a single table, so that the statistician
does not need to do any joins outside the database. In short, for the purposes of this discussion by 1NF
we mean:
x

1034

Each patient visit is represented by one row, and each patient will have as many rows
“belonging” to her as she has visits in the result-set (the definition of visit may change, to
allow aggregating multiple visits over a researcher-defined time-unit, but this must be an
explicit decision by the clinical domain expert or biostatistician).

Denormalize and Delimit: How not to Make Data ...

x

Bokov, Manuel, Cheng, Bos, Tirado-Ramos

Each column represents a distinct data element. Some columns can be discrete variables
(sex, race, insurance type). Some might be indicator (true/false) variables (presence or
absence of a particular diagnosis or procedure code or group of codes), but really these
are a special case of discrete variable. Other columns can be numeric variables (age at
visit, BMI, heart rate). Some variables will remain the same over many rows (such as
patient demographics), while others will be different at each visit (such as vital signs)

Complex interface and message protocol specifications may be necessary for federated queries, but
communicating the results of a query to a statistical software package is trivial: they are all capable of
reading CSV and most other common text-based tabular formats. If informaticians are writing SAS or
R code for manipulating data client-side on behalf of statisticians, the time of both is getting wasted.

Table 1 Simulated data in the same format as would be used to extract multiple visits from relational
EHR records and condense them to a single table in 1NF. Constant values such as patient id and
demographic information repeat for each date the patient was seen (VS_DATE). Then remaining columns
are time-varying data elements. FRAC and SPRN are indicator variables that report for each visit whether
one or more diagnosis codes in the 820-829.99 and 844-845 respective ICD9 ranges were linked (lower
limb fractures and lower limb sprains). The rest of the columns are numeric data elements which
represent vital signs. Such data can trivially be exported to a delimited text format which most regression,
longitudinal, and survival models in SAS, Stata, R, Julia, and Pandas can consume directly. We do not
exclude null values because the handling of missing data (e.g. by simple exclusion or by various
imputation or interpolation methods) really is a decision that belongs in the statistical domain, but most
statistical software also has dedicated functionality for this problem.

3 Methods
i2b2 (Murphy et al., 2009) is a powerful, extensible, and widely-used open source framework for
biomedical data warehouses with support for biorepositories, patient registries, and multi-site
federated queries (Weber et al., 2009). Most existing methods for extracting visit data from i2b2
(Bauer et al., 2015; Chang & Mendis, 2014; Murphy et al., 2009; Post et al., 2013) rely on
manipulating XML returned from the ReST API of the i2b2 web service. For a large dataset, there can
be a significant overhead from translating RDBMS tables into XML, manipulating the XML, and then
converting it back to tabular form for export. This would impact service both to the user requesting the
data and to all other users. Moreover, the i2b2 Export Data (Murphy et al., 2009) and the ExportXLS
(Chang & Mendis, 2014) plugins are not capable of representing a single 1NF table indexed by patient
and visit. AIW i2b2 Export (Post et al., 2013) may support such a structure, but with the materials
currently available from the project it was not possible to deploy export components to a current i2b2
instance under a standard configuration, so a full evaluation could not be done at this time. The IDRT
Additional Data plugin (Bauer et al., 2015) does not directly support visits; the tables it produces are
indexed by patient. It may be possible to pass multiple queries to this plugin in a manner that would
cause the summary view to be equivalent to the 1NF structure we described, but we were unable to
test this because most queries resulted in errors on a current i2b2 instance under a standard

1035

Denormalize and Delimit: How not to Make Data ...

Bokov, Manuel, Cheng, Bos, Tirado-Ramos

configuration. Finally, there is a suite of tools called SEINE (Adagarla et al., 2015) that uses a plugin
called DataBuilder to extract a subset of interest from the i2b2 star schema into a stand-alone SQLite
database and then upload each user-selected data element into REDCap (Harris et al., 2009) as a
separate instrument, in effect using REDCap to self-join the data. In practice we found that the
REDCap upload component silently drops many data elements not explicitly anticipated by the
software and most site-specific customizations would require patching the source code. We did,
however, use the DataBuilder component of SEINE as the entry point for our own work, because by
operating on an intermediate data file generated by DataBuilder, we can decouple the denormalization
task from the server-side data-extraction task, and can leverage DataBuilder's existing user interface.
It should be noted that the above projects evaluated above are all under active development and
their current incompatibility with our requirements does not imply future incompatibility.
Furthermore, these projects have useful functionality orthogonal to extraction of analytic data. For
example, the IDRT and AIW projects both offer powerful toolkits of which the data export plugins are
only a small part. The IDRT or AIW projects may be highly useful to sites that have not yet deployed
data warehouses, because their respective toolkits offer powerful features for installing i2b2, mapping
terminologies to an i2b2 metadata schema, and populating an i2b2 instance with data. Likewise,
unmodified SEINE is an excellent choice for sites that use REDCap in the same way that the authors'
site does and do not require a lot of flexibility in specifying statistical models of longitudinal data.
Our method for denormalizing data from the approximations of 2NF and 3NF typically used in
databases to a single table in 1NF is functionally somewhat similar to the left outer joins approach of
Dinu et al. (Dinu, Nadkarni, & Brandt, 2006) in that we use dynamic SQL, although the two
codebases are unrelated. Our approach differs in that it constrains the problem space to i2b2, is
modular and configurable, and is accompanied by re-usable code available for researchers who wish to
deploy it in their own i2b2 data warehouses. Denormalization proceeds in the following steps (Figure
1):
x
x

Create a 'scaffold' table which contains all unique patient and visit indexes from an EAV
(entity attribute value) table (Johnson, 1996) in the source data.
Parse the desired data elements out of the researcher's query and dynamically generate
SQL left join statements (to the above scaffold table, the number of join statements
proportional to the number of data elements). To avoid the joins-per-query limit of the
database, these statements are broken into chunks smaller than the limit, each chunk
creating an intermediate temporary table (also dynamically named), and then these
intermediate tables are joined into the final result.

One complication is that the exact contents of each left join may need to depend on the the data
element being joined. For example, when we join a collection of codes as one data element, we
assume that the researcher who specified them wants a single indicator variable representing “at least
one of these codes was referenced during this visit”. On the other hand, there is no meaningful way to
aggregate a panel of different lab tests each of which returns a numeric value. If a lab panel is selected
as a single data element, its component assays get split into separate columns. Some data elements use
codes together with modifiers (e.g. historic vs active versions of the same diagnosis, different relatives
for the same family history of disease, or different reactions to the same allergen). Certain numeric
data elements can also have modifiers, units, and threshold indicators among many other pieces of
contextual information. Such semantic rules are likely to vary from one site to another, so they are
supplied in a configuration file that the local informatics team can customize to their business rules
and terminology. We have several generic pre-defined rules that catch most codes and numeric values.
Each data element will create at least one primary column (actual data) and one or more supporting
columns (e.g. units, certain modifiers, quality control flags). These columns are output adjacent to
each other and their names differ only in their suffixes, which is technically a departure from strict
1NF for the sake of readability, but is not essential to the functioning of our code.

1036

Denormalize and Delimit: How not to Make Data ...

Bokov, Manuel, Cheng, Bos, Tirado-Ramos

Figure 1 Simplified diagram of data transformation from i2b2 EAV table to a single, analysis-ready
output table as executed by the DataFinisher i2b2 plugin that implements the approach described here.
OBSERVATION_FACT and CONCEPT_DIMENSION are tables that exist in star schema of the i2b2
datamart and the thick line between them represents a foreign key. VARIABLE is a table created from a
query that a researcher created by dragging branch or leaf nodes from i2b2's ontology to a data request
form; each such cluster of nodes is represented by one row in the VARIABLE table. The site-specific
configuration file “ruledefs.cfg” contains rules for matching data elements (e.g. regular expression
matches on concept code, cardinality, presence or absence of certain ancillary fields) and altering the way
that dynamic SQL is generated for those elements. Solid arrows represent transformation of data via static
SQL. Dashed arrows represent dynamic SQL created from one table and creating the additional tables
they point to. Oval nodes represent tables created by DataFinisher, where DF_DTDICT is a data
dictionary generated from a researcher's query and data contained in i2b2; DF_JOINME is the scaffold
table described in the Methods section; DF_RULES is a table created in-database from “ruledefs.cfg”;
DF_DYNSQL is a table containing snippets of dynamic SQL that are combined and executed to create a
series of temporary result tables collectively called TXXX (in the actual tables XXX is a zero-padded
integer, and this is done to avoid limitations on the number of joins permitted in a single SQLite
statement). The TXXX tables are then themselves joined one final time to create the analysis-read output
table that is provided to the statistician as a .csv file. For purposes of readability, several intermediate
tables were omitted.

1037

Denormalize and Delimit: How not to Make Data ...

Bokov, Manuel, Cheng, Bos, Tirado-Ramos

Another deliberate departure from from 1NF is in handling a data element that is not matched by
any existing rule. We tokenize all information associated with that data element during each patientvisit into a JSON-like (Crockford, 2006) string and that becomes the value in the column of a not-yetsupported data element. The reason for this design decision is two-fold. First, it makes the existence of
unsupported data elements obvious, to facilitate the writing of additional rules to support in
consultation with a domain expert, e.g. a medical records coder. Second, it preserves all information
about the data element, since its unsupported status implies we cannot yet be confident about which
attributes can be useful for analysis.
One might argue that we are not reducing complexity-- we are merely shifting the complexity out
of the data file and into the code that generates it. Indeed, that is exactly what we are doing,
deliberately, in keeping with the software design principles of modularity (Brooks, 1975) and
separation of concerns (Hürsch & Lopes, 1995). Generic data manipulation functionality is defined
once at the code level and site-specific rules tuning the behavior of these generic functions are
specified once at the configuration level. To the extent that advances are made in data interoperability
at the semantic level, the need for site-specific customization will diminish. But semantic data
alignment between different sites needs to be recognized as a distinct problem from the structural and
syntactic design of extracting data that is already conformant to a specific data model.

4 Implementation
Our implementation is for the i2b2 framework. However the general problem being solved is that
of programmatically converting relational data (in the case of i2b2, mostly the
OBSERVATION_FACT EAV table) into 1NF (with minor departures as described in Methods). The
same approach can be applied to other shared data models commonly used in health services research
such as MiniSentinel (Curtis et al., 2012), PCORNet CDM (Stang et al., 2010), and OMOP (Stang et
al., 2010). Data warehouses using such models contain tables that, though segregated by thematic
similarity, are still rough approximations of EAV, with the patient or the patient-visit as the entity, and
attributes that represent related data elements. Therefore, though the implementation of our data
extraction approach targets i2b2, a similar principle might be useful for developing extraction tools
targeting other common data models.

1038

Denormalize and Delimit: How not to Make Data ...

Bokov, Manuel, Cheng, Bos, Tirado-Ramos

Figure 2 Screenshot of DataFinisher and DataBuilder (UI modified slightly from the original SEINE
interface). To request deidentified EHR data for a patient-set created using i2b2's standard query
functionality, a researcher pulls EHR concepts from the hierarchical list on the left to the “Other
Observations” field. Each such concept will expand to a set of adjacent columns in a context-sensitive
manner governed by a rules file customized by the site informatics team.

Our
code
is
available
as
an
open
source
project
called
DataFinisher
(https://github.com/UTHSCSA-CIRD/datafinisher/).DataFinisher depends on and extends another
i2b2 plugin called DataBuilder (https://informatics.kumc.edu/work/browser/heron_extract), which was
written at University of Kansas Medical Center as part of the SEINE system (Adagarla et al., 2015)
and is used to support the efforts to the Greater Plains Collaborative Clinical Data Research Network
(CDRN) (Waitman, Aaronson, Nadkarni, Connolly, & Campbell, 2014) that is part of PCORNet
(Fleurence et al., 2014). DataBuilder accepts online data requests from i2b2 users (Figure 2) and
provides to them the records of interest for the patient cohort they request. However, the output of
DataBuilder alone was essentially a SQLite file containing a subset of the i2b2 star schema.
DataFinisher condenses that relational structure into a single, analysis-ready 1NF table as described
above with some additional quality control columns. We already are using DataFinisher in production
at our institution to provide de-identified EHR data on a self-service basis to researchers. This saves
researchers time, gives them greater autonomy over their projects, and reduces the technical support
burden on the informatics team.

1039

Denormalize and Delimit: How not to Make Data ...

Bokov, Manuel, Cheng, Bos, Tirado-Ramos

5 Conclusions
Terminology alignment and interoperability of data models are critical problems that must be
solved for the emerging field of health informatics to achieve its full potential but an idea from
computer science that health services researchers need to take more seriously is that standards should
focus on interfaces, not implementations (Liskov & Wing, 1994). Health services researchers and
perhaps informaticians may not fully appreciate the amount of time statisticians spend performing
transformations that can be done more efficiently within a database. At the same time, many
statisticians may not be aware that better options are even available for the asking. Here we attempt to
fill this gap by explicitly presenting a practical approach to denormalization that supports a large
number of statistical use-cases, accompanied by software that uses this approach with i2b2, a data
warehouse platform that is widely used in health informatics research.
We hope that as development on DataFinisher continues it will call the attention of decisionmakers in health service research to the need for small, open, modular tools and formats that do not
attempt to be all things for all people and do not needlessly constrain researchers' decisions about what
analytical software to use.

6 Acknowledgements
This work was supported by PCORI grants CDRN-1306-04631 and CDRN-1501-26643, as well as
NIH grant 1P30AG044271-01.

References
Adagarla, B., Connolly, D. W., McMahon, T. M., Nair, M., VanHoose, L. D., Sharma, P., …
Waitman. (2015). SEINE: Methods for Electronic Data Capture and Integrated Data Repository
Synthesis with Patient Registry Use Cases. Manuscript in preparation. Retrieved from
http://hdl.handle.net/2271/1303
Bauer, C. R. K. D., Ganslandt, T., Baum, B., Christoph, J., Engel, I., Löbe, M., … Sax, U. (2015).
Integrated Data Repository Toolkit (IDRT): A Suite of Programs to Facilitate Health Analytics on
Heterogeneous Medical Data. Methods of Information in Medicine, 55(2), 125–135.
http://doi.org/10.3414/ME15-01-0082
Bezanson, J., Karpinski, S., Shah, V. B., & Edelman, A. (2012). Julia: A Fast Dynamic Language
for Technical Computing. CoRR, abs/1209.5145. Retrieved from http://arxiv.org/abs/1209.5145
Brooks, F. P. (1975). The mythical man-month: essays on software engineering. Reading, Mass:
Addison-Wesley Pub. Co.
Chang, W., & Mendis, M. (2014). ExportXLS (Version 3.3) [I2b2]. University of Massachusetts
Medical School. Retrieved from https://i2b2exportxlsv3p3.codeplex.com/
Codd, E. F. (1979). Extending the database relational model to capture more meaning. ACM
Transactions on Database Systems (TODS), 4(4), 397–434.
Crockford, D. (2006). The application/json media type for javascript object notation (json).
Curtis, L. H., Weiner, M. G., Boudreau, D. M., Cooper, W. O., Daniel, G. W., Nair, V. P., …
Brown, J. S. (2012). Design considerations, architecture, and use of the Mini-Sentinel distributed data
system: USE OF THE MINI-SENTINEL DISTRIBUTED DATABASE. Pharmacoepidemiology and
Drug Safety, 21, 23–31. http://doi.org/10.1002/pds.2336
DeLano, W. L. (2005). The case for open-source software in drug discovery. Drug Discovery
Today, 10(3), 213–217. http://doi.org/10.1016/S1359-6446(04)03363-X

1040

Denormalize and Delimit: How not to Make Data ...

Bokov, Manuel, Cheng, Bos, Tirado-Ramos

Dinu, V., Nadkarni, P., & Brandt, C. (2006). Pivoting approaches for bulk extraction of Entity–
Attribute–Value data. Computer Methods and Programs in Biomedicine, 82(1), 38–43.
http://doi.org/10.1016/j.cmpb.2006.02.001
Fleurence, R. L., Curtis, L. H., Califf, R. M., Platt, R., Selby, J. V., & Brown, J. S. (2014).
Launching PCORnet, a national patient-centered clinical research network. Journal of the American
Medical Informatics Association, 21(4), 578–582. http://doi.org/10.1136/amiajnl-2014-002747
Harris, P. A., Taylor, R., Thielke, R., Payne, J., Gonzalez, N., & Conde, J. G. (2009). Research
electronic data capture (REDCap)—A metadata-driven methodology and workflow process for
providing translational research informatics support. Journal of Biomedical Informatics, 42(2), 377–
381. http://doi.org/10.1016/j.jbi.2008.08.010
Hürsch, W. L., & Lopes, C. V. (1995). Separation of concerns.
Johnson, S. B. (1996). Generic data modeling for clinical repositories. Journal of the American
Medical Informatics Association, 3, 328–339.
Liskov, B. H., & Wing, J. M. (1994). A behavioral notion of subtyping. ACM Transactions on
Programming Languages and Systems, 16(6), 1811–1841. http://doi.org/10.1145/197320.197383
McKinney, W. (2010). Data Structures for Statistical Computing in Python. In S. van der Walt & J.
Millman (Eds.), Proceedings of the 9th Python in Science Conference (pp. 51 – 56).
McKinney, W. (2013). Python for data analysis. Beijing: O’Reilly.
Murphy, S., Churchill, S., Bry, L., Chueh, H., Weiss, S., Lazarus, R., … others. (2009).
Instrumenting the health care enterprise for discovery research in the genomic era. Genome Research,
19(9), 1675–1681.
Post, A. R., Kurc, T., Cholleti, S., Gao, J., Lin, X., Bornstein, W., … Saltz, J. H. (2013). The
Analytic Information Warehouse (AIW): A platform for analytics using electronic health record data.
Journal of Biomedical Informatics, 46(3), 410–424. http://doi.org/10.1016/j.jbi.2013.01.005
R Core Team. (2015). R: A Language and Environment for Statistical Computing. Vienna, Austria:
R Foundation for Statistical Computing. Retrieved from https://www.R-project.org/
SAS Institute,. (2011). SAS system. Cary, NC.
Seaman, S., Pavlou, M., & Copas, A. (2014). Review of methods for handling confounding by
cluster and informative cluster size in clustered data. Statistics in Medicine, 33(30), 5371–5387.
http://doi.org/10.1002/sim.6277
Souza, T., Kush, R., & Evans, J. P. (2007). Global clinical data interchange standards are here!
Drug Discovery Today, 12(3-4), 174–181. http://doi.org/10.1016/j.drudis.2006.12.012
Stang, P. E., Ryan, P. B., Racoosin, J. A., Overhage, J. M., Hartzema, A. G., Reich, C., …
Woodcock, J. (2010). Advancing the Science for Active Surveillance: Rationale and Design for the
Observational Medical Outcomes Partnership. Annals of Internal Medicine, 153(9), 600.
http://doi.org/10.7326/0003-4819-153-9-201011020-00010
StataCorp. (2015). Stata Statistical Software: Release 14. College Station, TX: StataCorp LP.
Waitman, L. R., Aaronson, L. S., Nadkarni, P. M., Connolly, D. W., & Campbell, J. R. (2014). The
Greater Plains Collaborative: a PCORnet Clinical Research Data Network. Journal of the American
Medical Informatics Association, 21(4), 637–641. http://doi.org/10.1136/amiajnl-2014-002756
Weber, G. M., Murphy, S. N., McMurry, A. J., MacFadden, D., Nigrin, D. J., Churchill, S., &
Kohane, I. S. (2009). The Shared Health Research Information Network (SHRINE): A Prototype
Federated Query Tool for Clinical Data Repositories. Journal of the American Medical Informatics

1041

