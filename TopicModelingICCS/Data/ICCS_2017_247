Available online at www.sciencedirect.com

ScienceDirect

This space is reserved for the Procedia header, do not use it
This Procedia
space isComputer
reserved
for 108C
the Procedia
header, do not use it
Science
(2017) 1743–1752
This space is reserved for the Procedia header, do not use it

International Conference on Computational Science, ICCS 2017, 12-14 June 2017,
Zurich, Switzerland

Solution of Few-Body Coulomb Problems with
Solution of Few-Body Coulomb Problems with
LatentofMatrices
on Coulomb
Multicore Processors
Solution
Few-Body
with
Latent Matrices
on MulticoreProblems
Processors
1,2
2,3
Luis Biedma
, Flavio
Colavecchia
, and Enrique
S. Quintana-Ortı́4
Latent
Matrices
on Multicore
Processors
Luis Biedma1,2 , Flavio Colavecchia2,3 , and Enrique S. Quintana-Ortı́4
1

FAMAF, Universidad Nacional de Córdoba, Argentina

11,2 , Flavio Colavecchia2,3 , and Enrique S. Quintana-Ortı́4
Luis
Biedma
2
FAMAF,
Universidad Nacional
Consejo
Nacional
de Investigaciones
Cientı́ficasdeyCórdoba,
Técnicas Argentina
(CONICET), Argentina
2 3
Consejo

4
4
4

Nacional
de Investigaciones Cientı́ficas y Técnicas (CONICET), Argentina
Div. Fı́sica
1 Atómica, Molecular y Óptica, Centro Atómico Bariloche, Argentina
3
FAMAF, Molecular
Universidad
Nacional
de Córdoba,
Argentina
Div.
Fı́sica
Atómica,
y Óptica,
Centro
Atómico
Bariloche,
Argentina Spain
Depto.
Ingenierı́a
y
Ciencia
de Computadores,
Universidad
Jaume
I (UJI), Castellón,
2
Consejo
Nacional
de
Investigaciones
Cientı́ficas
y
Técnicas
(CONICET),
Argentina
Depto.
Ingenierı́a
y
Ciencia
de
Computadores,
Universidad
Jaume
I
(UJI),
Castellón,
Spain
3
Div. Fı́sica Atómica, Molecular y Óptica, Centro Atómico Bariloche, Argentina
Depto. Ingenierı́a y Ciencia de Computadores, Universidad Jaume I (UJI), Castellón, Spain

Abstract
Abstract
We re-formulate a classical numerical method for the solution of systems of linear equations
We
re-formulate
a classical
numerical
method
for the
solution
of systems that
of linear
equations
to
tackle
problems
with latent
data, that
is, linear
systems
of dimension
is a priori
unAbstract
to
tackleThis
problems
with
latent
data, in
that
is,
linear of
systems
of dimension
that is afor
priori
unknown.
type
of
systems
appears
the
solution
few-body
Coulomb
problems
Atomic
We
re-formulate
numerical
for the
solution of
systemsproblems
of linearfor
equations
known.
ThisPhysics,
typeaofclassical
systems
appears
inmethod
the solution
of
few-body
Coulomb
Atomic
Simulation
in
the
form
of
multidimensional
partial
differential
equations
(PDEs)
that
to
tackle problems
with
that is, linear systems
of dimension
that is (PDEs)
a priorithat
unSimulation
Physics,
insolution
thelatent
formofdata,
ofa multidimensional
partial
differential
equations
require
the
numerical
sequence
of recurrent
dense
linear
systems
of growing
scale.
known.
This
type
of
systems
appears
in
the
solution
of
few-body
Coulomb
problems
for
Atomic
require
thedimension
numerical solution
of a sequence
dense linearthousands
systems ofofgrowing
scale.
The
large
these
systems,
with of
uprecurrent
to several
unknowns,
is
Simulation
Physics, inof
form
of multidimensional
partialhundred
differential
equations
(PDEs) that
The
large
dimension
ofthe
these
systems,
withimplementation
up to several
hundred
thousands
ofthe
unknowns,
is
tackled
in
our
approach
via
a
task-parallel
of
a
solver
based
on
QR
factorrequire
the
numerical
solution
of a sequenceimplementation
of recurrent dense
systems
growing
scale.
tackled
in
our
approach
via a task-parallel
of a linear
solver
based
onof
the
QR
factorization.
This
method
is
parallelized
using
the
OmpSs
framework,
showing
fair
strong
and
weak
The
large
dimension
of
these systems,
with
to several hundred
thousands
of unknowns,
is
ization.
This
is parallelized
using
theup
OmpSs
showing
fair strong
and weak
scalability
on method
aapproach
multicore
processor
equipped
with 12 framework,
Intel of
cores.
tackled
in
our
via
a
task-parallel
implementation
a
solver
based
on
the
QR
factorscalability on a multicore processor equipped with 12 Intel cores.
ization.
This
method
is parallelized
using
OmpSs
framework,
showing fair multicore
strong and
weak
Keywords:
Few-body
problems,
latent matrices,
factorization,
task-parallelism,
CPUs
©
2017 The
Authors.
Published
by Elsevier
B.V. theQR
Keywords:
Few-body
problems,
matrices,
QR
task-parallelism,
multicore CPUs
Peer-review
of thelatent
scientific
committee
offactorization,
the
Conference on Computational
Science
scalabilityunder
on aresponsibility
multicore
processor
equipped
with
12International
Intel cores.
Keywords: Few-body problems, latent matrices, QR factorization, task-parallelism, multicore CPUs

1 Introduction
1 Introduction
A key method to solve many of the differential equations that describe the physical world is to
1
A
keyIntroduction
method
to solve
differential
that
describe
the
world
is to
expand
the solution
in amany
basisofofthe
functions.
Thisequations
transforms
the
problem
of physical
obtaining
the solu-

expand
thedifferential
solution in equation
a basis ofinto
functions.
This transforms
the problem
of expansion.
obtaining the
solution
ofmethod
the
the calculation
of thethat
coefficients
its
Usually
A
key
to solveequation
many of into
the differential
equations
describeof
the
physical
world
is to
tion
of
the
differential
the
calculation
of
the
coefficients
of
its
expansion.
Usually
the
basisthe
is solution
mathematically
defined
in an infinite-dimensional
vector
space.
The physicist
thus
expand
in
a
basis
of
functions.
This
transforms
the
problem
of
obtaining
the
soluthe
basis
is mathematically
defined
inrelevant
an infinite-dimensional
vector
space.
Theunder
physicist
thus
faces
the
challenge
of
to
compute
the
magnitudes
of
the
physical
system
scrutiny,
tion
ofthe
the
differential
equation
into
the
calculation
of theofcoefficients
of system
its expansion.
Usually
faces
challenge
of
to
compute
the
relevant
magnitudes
the
physical
under
scrutiny,
choosing
basis
large enough
such that
calculation is meaningful,
with The
the restrictions
of
the
basis a
mathematically
defined
in anthe
infinite-dimensional
vector space.
physicist thus
choosing
ais basis
large enough
such
that
the
calculation
is
meaningful,
with of
thethe
restrictions
of
memory
and
computer
power
at
hand.
Moreover,
the
optimal
dimension
basis
set
is
faces
the and
challenge
of to compute
the
relevant
magnitudes
of the physical
system
under
scrutiny,
memory
computer
power
at
hand.
Moreover,
the
optimal
dimension
of
the
basis
set
is
usually
not
known
a
priori,
and
many
trial-and-error
runs
need
to
be
performed.
choosing
a basis
large
enough
such
that
the calculation
is meaningful,
with the restrictions of
usually
not
known
a
priori,
and
many
trial-and-error
runs
need
to
be
performed.
To establish
the problem
in at
detail,
let Moreover,
us considerthe
a differential
operator Â(r)
in basis
an infinitememory
and computer
power
hand.
optimal dimension
of the
set is
To establish
the space
problem
in detail,
let
us consider
a differential
operator
Â(r)
in an
infinitedimensional
vector
given
by
the
variables
r
=
{r
,
r
,
.
.
.
,
r
},
and
a
general
1
2
n
usually not known a priori, and many trial-and-error runs need to be performed. non-homodimensional
vector
space given by the
variablesthat
r =space
{r1 , rgiven
rn }, and a general non-homo2 , . . . ,by:
geneous
partial
differential
To establish
the problemequation
in detail,(PDE)
let us in
consider
a differential
geneous
partial differential
equation
(PDE)
in
that space
given by:operator Â(r) in an infinitedimensional vector space given by the variables r = {r1 , r2 , . . . , rn }, and a general non-homoÂ(r)Ψ(r) = Ψ0 (r).
(1)
geneous partial differential equation (PDE)
in that
given by:
Â(r)Ψ(r)
= Ψspace
(1)
0 (r).
Â(r)Ψ(r) = Ψ0 (r).
1877-0509 © 2017 The Authors. Published by Elsevier B.V.
Peer-review under responsibility of the scientific committee of the International Conference on Computational Science
10.1016/j.procs.2017.05.019

(1)1
1
1

1744	

Solution of Few-Body CoulombLuis
Problems
onal.Multicore
Biedma et
/ ProcediaProcessors
Computer Science 108C (2017) 1743–1752 L. Biedma et al.

For this problem, we are interested in the solution Ψ(r) for a given set of boundary conditions.
Consider an orthonormal basis set given by the functions {φn (r)} such that the internal product
is < φn , φm >= δnm . Thus, the functions Ψ(r) and Ψ0 (r) in this basis can be expanded as:


xn φn (r),
Ψ0 (r) =
Ψ(r) =
bn φn (r),
(2)
n

n

which include all the infinite elements of the basis. To make use of these expressions in a
computer application and obtain a numerical solution to equation (1), it is necessary to select
only N elements of the basis, such that
Ψ(r) ≈ Ψ(N ) (r) =

N


n=1

xn φn (r),

(N )

Ψ0 (r) ≈ Ψ0 (r) =

N


bn φn (r).

(3)

n=1

In this way, equation (1) is projected into the selected truncated basis, obtaining a linear system
for the coefficients x(N ) = {x1 , x2 , . . . , xN }:
A(N ) x(N ) = b(N ) ,

(4)
(N )

where the matrix A(N ) , of size N × N , has elements Anm =< φn , Âφm >; and the vector
b(N ) = {b1 , b2 , . . . , bN } is the approximate representation of the initial condition Ψ0 in the
truncated basis.
The usual work-flow to obtain the best solution Ψ(r) is to solve the system (4) for increasing
values N1 < N2 < N3 . . . of N until an adequate convergence is achieved. This convergence
and the associated stopping criterion are usually determined by the precision required in the
calculation of a physical magnitude, which is bounded by experimental data.
This naive procedure presents several drawbacks. First, one can choose to compute the
complete matrix A(N ) for each N1 < N2 < N3 . . ., with the result that many elements of the
matrix are repeatedly computed as all the elements of A(Ni ) are included for all the calculations
with N ≥ Ni , for i = 1, 2, . . .. To avoid these costly recalculations, the different matrices A(Ni )
can be saved, but then that data will have to be retrieved from files, since memory constraints are
reached very rapidly for medium-sized matrices. In this last scenario, the calculation is limited
by the slow I/O transfer rate from disk. Second, although numerical linear algebra packages
provide the functionality to tackle a substantial amount of problems, they all need to know
in advance the size, structure and data of the matrices in any of their many implementations:
serial, multi-threaded and/or distributed algorithms.
In this work, we introduce a new concept of matrices, that we qualify as latent. The main
features of these matrices are: 1) The dimension of the latent matrix is not known a priori ;
2) a latent matrix has a well-known structure of blocks or tiles; 3) each element of a latent
matrix can be computed with a well-defined numerical algorithm; and 4) the latency of the
computation of each element can be determined, for a given computational setup.
The concept of latent matrices is useful to classify the problem stated in the preceding paragraphs. However, it raises the question of designing adequate numerical linear algebra algorithms and software for them. In this work we move towards the exploration of these algorithms
for the solution of few-body Coulomb problems, making the following specific contributions:
• We re-formulate the solution of a system of linear equations with latent data into a matrix
factorization updating problem, which can be performed via a “lazy” QR factorization.
• We analyze the parallelism of the problem, proposing an asynchronous solution that
overlaps the execution of multiple updating problems/QR factorizations to expose a higher
degree of task concurrency.
2

	

Solution of Few-Body CoulombLuis
Problems
onal.Multicore
Biedma et
/ ProcediaProcessors
Computer Science 108C (2017) 1743–1752 L. Biedma et al.

• We tackle the large-scale of the linear systems appearing in few-body Coulomb problems
via a task-parallel implementation based on the OmpSs programming model.
• We evaluate the performance of our task-parallel solution for latent linear systems on a
12-core Intel-based server, showing reasonable scalability.

2

Physics Problems with Latent Matrices

One of the most important examples of latent matrices arises in Atomic Collisions Physics. This
field is devoted to the calculation of collisions processes between photons, electrons, ions, atoms
and molecules, which are present in a variety of applications, ranging from energy production
from plasma reactors to the study of atmospheric reactions.
For example, the simplest atomic collision is the elastic scattering of an electron from a
Hydrogen nucleus. In this process, two charged particles interact with each other following the
rules of the long-range Coulomb interaction within the quantum world. Adding one particle to
that system leads to a six-dimensional PDE which is analytically unsolvable. The most accurate
framework to deal with this physical system relies on the numerical solution of the Schrödinger
equation for the few-body problem that represents the collision. This has been the method of
choice for rather small, but very complex systems at the atomic scale. Even nowadays, the
problem is still extremely difficult to solve.
Let us consider the Schrödinger equation for continuum states that results from collisions in
a three-body system, and can be obtained from the solution of the time-independent problem
(H − E) Ψ = W Ψ0 ,

(5)

where Ψ0 is the initial state of the process and E is the total energy of the system. The (possibly
differential) operator W is responsible for the transitions from the initial, known state Ψ0 to the
continuum, collisional unknown state Ψ. For the case of two electrons and an atomic nucleus,
one considers a coordinate system given by the vectors {r1 , r2 }, which represent the position of
each electron with respect to the nucleus. Therefore, equation (5) is a six-dimensional PDE. The
approach to solve this problem numerically is to use a spectral method, where one introduces
a suitable basis set to represent the solution Ψ. One of the main features of this procedure is
that the physical considerations can be introduced directly into the basis elements [1, 2, 3].
L
Any spectral method in the electronic coordinates introduces a basis set {Ξnm
(r1 , r2 )} to
expand the wave function Ψ:
Ψ(r1 , r2 ) =

N
M


L
xnm Ξnm
(r1 , r2 ).

nm

This represents the application of equation (2) for this particular atomic system. At this point
we need to point out that the equation implicitly depends on the total angular momentum L.
This fact is represented by the superscript in the definition of the basis. The Hamiltonian is a
latent matrix in this basis set, and can be computed as:

 L
L
HL
n m nm = Ξn m |H − E| Ξnm ,

which is a square matrix of NH × NH elements.
The solution of the linear system Hx = b gives the set of coefficients x = {xnm } that
determines the solution Ψ. The right-hand side vector of this linear system, b, is the projection
into the basis of the action of operator W on the initial state Ψ0 .
3

1745

1746	

Solution of Few-Body CoulombLuis
Problems
onal.Multicore
Biedma et
/ ProcediaProcessors
Computer Science 108C (2017) 1743–1752 L. Biedma et al.

The elements of the Hamiltonian matrix HL
n m nm are a priori six-dimensional integrals in
the coordinate space spanned by the set {r1 , r2 }. However, the complexity of the calculation
of these elements can be reduced if physical symmetries are taken into account when the basis
elements are selected. Let us assume that we make use of spherical coordinates for each electron:
L
is defined as a
ri = (ri , θi , ϕi ) for i = 1, 2. With this choice, each element of the basis Ξnm
product of two one-electron functions [4]:
ΞnLa nb (r1 , r2 ) =

Sna la (r1 ) Snb lb (r2 ) LM
Yla lb (
r1 , 
r2 ),
r1
r2

where 1 ≤ na ≤ Na and 1 ≤ nb ≤ Nb , and {Na , Nb } are the number of elements for each
one-electron basis set. The indexes la and lb represent the angular momentum of each electron,
and satisfy |L − la | ≤ lb ≤ L + la . This implies that, for a given L, there are several pairs of
values (la , lb ) that can be included in the basis.
From the computational point of view, this problem is latent in the basis size parameters
{Na , Nb } as well as in the number of pairs (la , lb ). This means that the optimal values of these
parameters are not known a priori, and usually a trial-and-error procedure is performed until
the required accuracy is achieved.
Typically, several pairs of angular momenta are used for a given L (for example, from 6 to
16 [5]), and the basis size Na = Nb includes several dozens of elements. Therefore, the size of
the linear system to be solved can feature several hundred thousands of unknowns. To conclude
this section, the Hamiltonian is a dense matrix with complex entries that needs to be computed
in double-precision arithmetic, raising the memory requirements to hundreds of Gbytes [6].

3

Linear Algebra Methods for Latent Linear Systems

We next describe in detail the linear algebra problem underlying the few-body Coulomb problems, connecting the linear systems with latent data to the updating of a dense QR factorization.
Solution of latent linear systems. The discussion in Section 2 exposes that, in order to
tackle the target problem, we need to solve a sequence of linear systems of the form Hx = b,
organized in a cycle of “levels”. Thus, we generate and solve the problem at level s; next, we
test whether the solution obtained at that level is satisfactory for the global problem; and in
case it is not, we proceed to the next level s + 1, repeating the cycle (trial-and-error).
Let us define the process formally. Assume the linear system to be solved initially (i.e, at
level 0) is given by
(6)
A(0) x(0) = b(0) ,
where A(0) is the m0 ×m0 coefficient matrix, b(0) is the right-hand side vector, x(0) is the soughtafter solution, and b(0) , x(0) are both of size m0 . In case the solution of (6) is not satisfactory,
we then extend the linear system to
 (0)
  (0)   (0) 
A
B (0)
y
b
A(1) x(1) = b(1) ≡
=
,
(7)
C (0) D(0)
z (0)
c(0)
where A(1) is n1 × n1 , n1 = m0 + m1 , and the components of (7) are partitioned accordingly.
This dependence between levels 0 and 1 can be generalized to two consecutive levels, s−1 and
s, in the form of the algorithm for the solution of recurrent latent linear systems (RLLS) stated
in Figure 1. The dense linear system in Step 2 there, corresponding to equation (6), can be
solved via any conventional factorization algorithm, such as the LU or QR factorizations [7], for
4

	

Solution of Few-Body CoulombLuis
Problems
onal.Multicore
Biedma et
/ ProcediaProcessors
Computer Science 108C (2017) 1743–1752 L. Biedma et al.

1.
2.
3.
3.1

3.2

Generate the initial problem data: A(0) , b(0) .
Solve the m0 × m0 dense linear system A(0) x(0) = b(0) .
Solve the sequence of levels:
for s = 1, 2, . . . until convergence
Generate the problem data for the s-th level:
B (s−1) of size ns−1 × ms , C (s−1) of size ms × ns−1 , D(s−1) of size
m s × ms ,

(s−1)
where ns−1 = s−1
with ms components.
k=0 mk ; and c
Solve the ns × ns linear system for the s-th level:
 (s−1)
  (s−1)   (s−1) 
A
b
y
B (s−1)
A(s) x(s) = b(s) ≡
=
.
z (s−1)
c(s−1)
C (s−1) D(s−1)
end for

Figure 1: Algorithm RLLS.
a cost of 2m30 /3 or 4m30 /3 floating-point arithmetic operations (flops), respectively. (Hereafter,
we neglect minor order terms in the flop expressions such as, e.g., the cost required to solve the
triangular systems after the matrix factorization. Furthermore, for simplicity, we assume real
arithmetic though the actual calculations for a few-body problem operate with complex data.)
The solution of the linear system in Step 3.2 is more challenging. Concretely, assuming
we rely on the QR factorization, a naive solution that tackles Step 3.2 in isolation will require
4n3s /3 flops, with this figure rapidly growing with the level index s and the dimension of the
subproblems mj , j = 0, 1, 2, . . ..
Updating a QR factorization. A more elaborate solution can avoid the large overhead of
the previous naive approach by exploiting that A(s−1) is a leading principle submatrix of A(s) to
leverage an existing factorization of the former. In linear algebra, this is known as an updating
problem, and efficient high performance solutions have been proposed for the (incremental) QR
factorization [8] and the LU factorization (with incremental pivoting) [9]. Due to the potential
instability of the incremental pivoting technique required in the latter, in the remainder of this
work we focus on the numerically-stable incremental QR factorization, reviewing the underlying
procedure next.
Assume we have already computed the factorization A(s−1) = Q(s−1) R(s−1) = QR, and
consider the partitioning


QT A(s−1)
C (s−1)

B (s−1)
D (s−1)



=



R(s−1)
C (s−1)

B (s−1)
D (s−1)







= 


R00

C0

R01
R11

...
...
..
.

C1

...

R0,s−1
R1,s−1
..
.
Rs−1,s−1
Cs−1

B0
B1
..
.
Bs−1
D





 , (8)


where the tiles on the diagonal Rk,k , of dimension mk × mk , k = 1, 2, . . . , s − 1, are all upper triangular. The structure-aware procedure to update this matrix factorization (UMF) is
then illustrated in Figure 2. Upon completion, the coefficient matrix (8) has been reduced to
upper triangular form R(s) via a sequence of orthogonal transformations represented by Q(s) .
Therefore, by applying the same sequence of transformations to b(s) , we can next use backward
substitution to solve R(s) x(s) = ((Q(s) )T b(s) ) for x(s) .
The integration of the update procedure UMF into algorithm RLLS produces a lazy variant
of the QR factorization where, at each step of the decomposition (level of the RLLS solver), we
5

1747

1748	

Biedma et
/ ProcediaProcessors
Computer Science 108C (2017) 1743–1752 L. Biedma et al.
Solution of Few-Body CoulombLuis
Problems
onal.Multicore
1.
1.1
1.2

2.
2.1

2.2

2.3
3.

Apply previous transformations to B (s−1) :
for j = 0, 1, . . . , s − 1
Bj := QT
j,j Bj
for i = j +1, j + 2, .. . , s −1
Bj
Bj
:= QT
,
i,j
Bi
Bi
end for
end for
Factorize new row block in [C | D]:
for j = 0, 1, . . . , s − 1




R̄s,j
Rj,j
= Qs,j
Compute the QR fact.
Cj
0
and set Rj,j := R̄s,j .
for p = j + 1,
 j + 2, . .. , s − 1 
Rj,p
Rj,p
:= QT
.
s,j
Cp
Cp
end for




Bj
Bj
:= QT
.
s,j
D
D
end for
Compute the QR fact. D = Qs,s R̄s,s , and set D := R̄s,s .

Cost (in flops):
2m3j
4((mj + mi )m2j − m3j /2)

2(ms m2j + 4m3j /3)
4((mj + ms )m2j − m3j /2)
4((mj + ms )m2j − m3j /2)
4m3s /3

Figure 2: Structure-aware procedure UMF (application at step s of algorithm RLLS). Here
Q00 = Q(0) denotes the orthogonal factor resulting from the initial factorization of A(0) performed as part of Step 2 of algorithm RLLS.
only update the elements on the s-th column and s-th row of the matrix above and to the left
of the (s, s) element, respectively. For performance, this is formulated as a blocked algorithm.
Exploiting the upper triangular structure of A(s−1) produces a considerable reduction in the
cost with respect to the naive approach; see the column for the costs in Figure 2. Furthermore,
due to the use of orthogonal transformations, the structure-aware procedure is stable [7].

4

Parallel Tools for the Solution of Latent Systems

The solution of conventional (i.e. non-latent) dense linear systems, on a general-purpose multicore processor, can be performed using the appropriate computational kernels (routines) from
multi-threaded dense linear algebra libraries such as Intel MKL, PLASMA [10], or libflame [11].
Concretely, the parallel routines in these libraries can be easily applied to the solution of the
initial system in (6). For the recurrence defining the latent systems, they can also be leveraged
to compute in parallel the individual computational kernels composing the structure-aware procedure UMF and the subsequent solve (see Figure 2). However, this approach is constrained in
the amount of the parallelism that is exploited, which does not lead to a scalable solution.
In this section we discuss an alternative strategy that exposes additional task-parallelism in
the recurrence of latent linear systems, improving the scalability of the complete process. We
open the section with a detailed analysis of the task-parallelism of this process. Then, we offer
a glimpse of the OmpSs programming model that underlies our parallel solution.
Task-parallel solution of latent systems. Consider the structure-aware procedure UMF in
Figure 2 applied at level s = 4. Let us denote each one of the factorizations in Step 2.1 there
by Fj , j = 0, . . . , 3; and the application of the orthogonal transformations comprised by Qs,j
to each one of the submatrices in Step 2.2, by Uj,p , p = j + 1, j + 2, . . . , 3. Furthermore, for
simplicity, let us skip the application of the transformations in Step 2.3 as well as the operations
6

	

Biedma et
/ ProcediaProcessors
Computer Science 108C (2017) 1743–1752 L. Biedma et al.
Solution of Few-Body CoulombLuis
Problems
onal.Multicore

UMF
F

U01

0

F

U12

1

U02

F

U23

2

F
3

U13

U03

Figure 3: (Simplified) TDG for the structure-aware procedure UMF applied to matrix A(s) at
level s = 4.

GD

Level
0

GD

Level
s−1

GD

Level
s
UMF
F

0

MF

UMF

UMF

TS

TS

TT

TT

U01

F

1

U02

U12

F

2

U23

F

3

U13

U03

TS

TT

..

..

Figure 4: (Simplified) TDG for algorithm RLLS.

in Step 1 during the following discussion.
Figure 3 represents the data dependencies appearing in the matrix factorization update in
the form of a task dependency graph (TDG), where the nodes identify the operations (tasks or
computational kernels) and the edges specify the dependencies. This plot reports the concurrency available in this step of the algorithm as that present solely in the updates Uj,p for each
independent value of p. Additional parallelism can be exposed by dividing C (s−1) /B (s−1) in (8)
into finer-grain block columns/rows of width/height td < mj . (Note that this induces a partitioning of R(s−1) that presents diagonal blocks of dimension td × td .) However, as td diminishes,
the update of these blocks becomes a memory-bound kernel, offering very low performance.
An appealing opportunity to expose ampler task-parallelism comes from considering the
solution of the full recurrence of latent linear systems. In particular, Figure 4 depicts the levels
and steps appearing in the solution process (see Figure 1), using the following notation:
– GD: data generation (Steps 1 and 3.1);
– MF: matrix factorization (Step 2, only in level 0) and UMF for the update version (Step 3.2);
– TS: application of orthogonal transformations to the right-hand side and solution of the
subsequent upper triangular system (following MF in level 0 or UMF in levels 1,2,. . . ); and
– TT: termination test (stopping criterion).
For GD, it is possible to partition the problem data into multiple independent blocks, yielding an
embarrassingly-parallel execution of this step. In contrast, TS involves operations with a minor
cost and low parallel performance. Unfortunately, this organization of algorithm RLLS presents
data dependencies between each pair of consecutive steps, and a control dependency (for the
stopping criterion) between each pair of consecutive levels, that introduce a synchronization
7

1749

1750	

Solution of Few-Body CoulombLuis
Problems
onal.Multicore
Biedma et
/ ProcediaProcessors
Computer Science 108C (2017) 1743–1752 L. Biedma et al.

point each:
Level s − 1
Level s
Level 0









GD → MF → TS → TT → · · · → GD → UMF → TS → TT → GD → UMF → TS → TT → · · ·

The approach we propose here is to merge GD with MF (in level 0) or UMF (in levels 1 and
higher) so that the first task accessing a block of C (s−1) during the matrix factorization update
becomes also responsible for generating the data for that block. For the simple TDG in Figure 3
this means that task F0 generates the data for C0 ; and tasks U0,p , p = 1, 2, 3, those for Cp .
More importantly, proceeding in this manner we break the synchronization point between
each two consecutive levels by allowing level s to commence its execution even before that of
level s − 1 has been completed. This approach can then leverage idle processor cores, performing an speculative execution of tasks pertaining to future levels, which may or may not be
necessary depending on the outcome of the stopping test. The test for level s thus becomes an
asynchronous signal that can stop the execution of tasks in future levels s + 1, s + 2, . . .. To
favor an early solution of the problem though, we prioritize the executions of the tasks in the
lower levels.
To close this discussion, we remark that a solution based on existing task-parallel dense
linear algebra libraries, such as libflame or PLASMA, is impractical due to 1) the need to
exploit the special structure of A(s) in order to reduce the cost of factorization update; and 2)
the need of an explicit handling of the solution process to expose additional task-parallelism.
OmpSs implementation of algorithm RLLS. OmpSs is a task-based parallel programming
model developed at Barcelona Supercomputing Center [12] that has been successfully applied
in the past to the solution of dense linear systems via standard matrix decomposition such as
the Cholesky, QR and LU factorizations, on multicore systems; see, e.g., [13].
At execution time, the OmpSs runtime system detects data dependencies between tasks,
with the help of the programmer via OpenMP-like compiler directives (pragmas) annotated
with clauses that indicate the task operands’ directionality (input, output or input/output).
OmpSs then generates a task graph, which is leveraged to schedule the tasks to the cores,
exploiting the inherent task-level parallelism while fulfilling the graph dependencies.
Our task-parallel solver relies on a sequential implementation of algorithm RLLS and the
structure-aware UMF procedure in C, with routines for the kernels/computational tasks offering
clean interfaces. From that starting point, we add OmpSs pragmas (#pragma omp task) to
identify tasks, directionality clauses (in, out and inout) to indicate the character of the routine
matrix arguments, and priorities to guide the execution order of the tasks. No attempt to exploit
other type of parallelism (e.g., inside the tasks, via calls to a multi-threaded implementation of
BLAS) is currently done in our approach.

5

Experimental Results

The goal of this section is to expose the performance benefits of leveraging a task-parallel
programming model, such as OmpSs, for the computation of the lazy QR factorization operating
with latent matrices. For this reason, our experiments are designed to assess the scalability of
the OmpSs factorization code, in a simplified yet practical scenario. In particular, for the
evaluation of the task-parallel lazy QR factorization, we generated square latent matrices of
orders n = 14, 400, 20,160, 28,800, 34,560, 40,320, 44,640, and 49,152 for the largest instance.
For simplicity, for each problem dimension we set the dimension of all levels to be the same:
m = m0 = m1 = m2 = . . ., with values m=600, 840, 1,200, 1,440, 1,680, 1,860, and 2,048
8

Solution of Few-Body CoulombLuis
Problems
onal.Multicore
Biedma et
/ ProcediaProcessors
Computer Science 108C (2017) 1743–1752 L. Biedma et al.

Strong Scalability
10
9

Weak Scalability
10

LazyQR

9

LazyQR

8
GFLOPS per core

8
GFLOPS per core

	

7
6
5
4
3

7
6
5
4
3

2

2

1

1

0

0
1

2

3

4 5 6 7 8 9
Number of threads/cores

10 11 12

1

2

3

4 5 6 7 8 9
Number of threads/cores

10 11 12

Figure 5: Performance of the task-parallel lazy QR factorization.

growing proportionally with n. This implies that the number of levels s required for convergence
is assumed to equal 24 for all problem instances. The memory consumption (footprint) is n2
plus a minor factor, with n being the largest problem size that is factorized. The evaluation is
performed in terms of GFLOPS (billions of flops per second), using the standard cost for the
QR factorization.
All the experiments in this section were performed in IEEE double precision arithmetic, on a
server equipped with two Intel E5645 hexa-core processors (for a total of 12 cores), at 2.40 GHz,
and 48 Gbytes of DDR3 RAM. Our codes were linked with Intel MKL (composer xe 2011 sp1)
for the BLAS kernels and OmpSs (version 16.06).
Figure 5 reports the GFLOPS rate attained by the OmpSs version of the lazy QR factorization routines on the Intel 12-core server for two different configurations, corresponding to strong
and weak scalability tests. For the evaluation of the strong scalability, we set the problem size
to the largest instance, that is n=49,152 (and therefore m=2,048), and then vary the number
of threads/cores from 1 to 12. In this scenario, we should observe a decrease in the GFLOPS
throughput as the number of cores is increased, reflecting a situation where the problem dimension (n × n) per core is gradually reduced. The left-hand side plot in Figure 5 shows that
this is indeed the case. However, the decay is small and mostly occurs for the executions with
4, 6, and 12 threads. Overall the reduction from 1 thread/core and 8 GFLOPS to 12 threads/cores and slightly less than 7 GFLOPS is about 15%, showing a fairly strong scalability for
the task-parallel lazy QR factorization. The speed-up with c cores is directly obtained from
the results of this experiments by dividing the GFLOPS rates with c cores, multiplied by c, by
that obtained with a single core.
For the weak scalability test (in the right-hand side plot in Figure 5), we run the smallest
problem instance (n=14,400, m=600) on a single thread/core, and then gradually increase the
number of threads/cores while maintaining the problem dimension per core constant. Note that
this roughly corresponds to the dimensions that we enumerated in the opening paragraph of this
section, with n=14,400, 20,160,. . . ,49,152 respectively executed using 1, 2,. . . , 12 threads/cores.
For this experiment, we observe a rapid increase of the GFLOPS rate for up to 10 threads/cores,
but an stagnation for the execution of the largest problem on 12 threads/cores. While this may
be a bit disappointing, we believe that a finely-tuned problem-dependent optimization of this
case (e.g., by adjusting the blocking parameters used internally in the building blocks of the
UMF procedure) could solve this problem.
9

1751

1752	

Solution of Few-Body CoulombLuis
Problems
onal.
Multicore
Biedma et
/ ProcediaProcessors
Computer Science 108C (2017) 1743–1752 L. Biedma et al.

6

Concluding Remarks and Future Work

We have re-formulated the solution of linear systems with latent data arising in few-body
Coulomb problems as a matrix factorization update, which we address via a lazy QR factorization algorithm. Our solution exploits task-parallelism and addresses the reduced degree of
parallelism of this problem by proposing an asynchronous execution of multiple linear systems,
increasing the concurrency and, therefore, avoiding otherwise idle cores (i.e., wasted computational resources). This approach can thus produce a faster execution, at the expense of
performing some speculative calculations that may not be part of the final solution.
As part of future work, we plan to exploit complementary thread-parallelism existing inside
the computational building blocks that compose the factorization.

Acknowledgements
The researcher from UJI was supported by project TIN2014-53495-R of the MINECO and
FEDER, and project P1-1B2015-26 of UJI. We thank Rocio Carratalá, from UJI, for her help
with the evaluation of the building blocks.

References
[1] I. Bray, A. Kheifets, D. V. Fursa, Electrons and photons colliding with atoms: development and
application of the convergent close-coupling method, Journal of Physics B: Atomic, Molecular and
Optical Physics 35 (2002) R117.
[2] M. S. Mengoue, M. G. K. Njock, B. Piraux, Y. V. Popov, S. A. Zaytsev, Electron-impact Double
Ionization of He by Applying the Jacobi Matrix Approach to the Faddeev-Merkuriev equations,
Physical Review A 83 (2011) 052708.
[3] A. Frapiccini, J. M. Randazzo, G. Gasaneo, F. D. Colavecchia, A boundary adapted spectral
approach for breakup problems, Journal of Physics B: Atomic, Molecular and Optical Physics
43 (10) (2010) 101001.
[4] J. M. Randazzo, A. Frapiccini, L. U. Ancarani, G. Gasaneo, F. D. Colavecchia, Generating Optimal
Sturmian Basis Functions for Atomic Problems, Physical Review A 81 (2010) 042520.
[5] M. Baertschy, T. N. Rescigno, W. Isaacs, X. Li, C. McCurdy, Electron-impact ionization of atomic
hydrogen, Physical Review A 63 (2) (2001) 022712.
[6] F. D. Colavecchia, Accelerating spectral atomic and molecular collisions methods with graphics
processing units, Computer physics communications 185 (7) (2014) 1955–1964.
[7] G. H. Golub, C. F. V. Loan, Matrix Computations, 3rd Edition, The Johns Hopkins University
Press, Baltimore, 1996.
[8] B. C. Gunter, R. A. van de Geijn, Parallel out-of-core computation and updating the QR factorization, ACM Trans. Math. Soft. 31 (1) (2005) 60–78.
[9] E. S. Quintana-Ortı́, R. A. van de Geijn, Updating an LU factorization with pivoting, ACM Trans.
Math. Softw. 35 (2) (2008) 11:1–11:16.
[10] PLASMA project home page, http://icl.cs.utk.edu/plasma.
[11] F. G. Van Zee, libflame: The Complete Reference, www.lulu.com, 2012.
[12] OmpSs project home page, http://pm.bsc.es/ompss.
[13] R. M. Badia, J. R. Herrero, J. Labarta, J. M. Pérez, E. S. Quintana-Ortı́, G. Quintana-Ortı́, Parallelizing dense and banded linear algebra libraries using SMPSs, Concurrency and Computation:
Practice and Experience 21 (18) (2009) 2438–2456.

10

