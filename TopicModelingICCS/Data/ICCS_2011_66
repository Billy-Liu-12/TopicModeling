Available online at www.sciencedirect.com

Procedia Computer
Science
Procedia
Computer
Science
4 (2011)
2287–2296
Procedia
Computer
Science
00 (2011)
1–10

International Conference on Computational Science, ICCS 2011

Strategies for Fault Tolerance in Multicomponent Applications
Aniruddha G. Sheta,1 , Wael R. Elwasifa , Samantha S. Foleya , Byung H. Parka , David E. Bernholdta,∗, Randall
Bramleyb
a Computer

Science and Mathematics Division, Oak Ridge National Laboratory, Oak Ridge, TN, USA
b Dept. of Computer Science, Indiana University, Bloomington, IN, USA

Abstract
This paper discusses on-going work with the Integrated Plasma Simulator (IPS), a framework for coupled multiphysics simulations of plasmas, to allow simulations to run through the loss of nodes on which the simulation is
executing.
While many diﬀerent techniques are available to improve the fault tolerance of computational science applications
on high-performance computer systems, checkpoint/restart (C/R) remains virtually the only one that see widespread
use in practice. Our focus here is to augment the traditional C/R approach with additional techniques that can provide
a more localized and tailored response to faults based on the ability to restart failed tasks on an individual basis, and
the use of information external to the application itself in order to guide decision-making, in many cases avoiding the
need to stop and restart the entire simulation. This capability involves several features within the IPS framework, and
leverages the Fault Tolerance Backplane, a publish/subscribe event service to disseminate fault-related information
throughout HPC systems, to obtain information from the Reliability, Availability and Serviceability (RAS) subsystem
of the HPC system.
This work is described in the context of Cray XT-series computer systems for concreteness, but is applicable
to other environments as well. As part of the analysis of this work, we discuss the requirements to generalize this
approach to other complex simulation applications beyond the Integrated Plasma Simulator.
Keywords:
application fault tolerance, computational science, multiphysics framework

1. Introduction
Fault tolerance (FT) is often discussed as an important consideration in modern high-performance computational
science and engineering (CSE). Starting recently, the growth in the computational capabilities of high-end systems
now comes almost entirely from increasing the number of processors, and concerns about fault tolerance are exacerbated. Today’s largest (petascale) systems report mean times between failures (MTBFs) in the range of a few days
∗ Corresponding

Author
Email addresses: elwasifwr@ornl.gov (Wael R. Elwasif), foleyss@ornl.gov (Samantha S. Foley), parkbh@ornl.gov (Byung
H. Park), bernholdtde@ornl.gov (David E. Bernholdt), bramley@cs.indiana.edu (Randall Bramley)
1 Current address: Intel Research Laboratory, Bangalore, India

1877–0509 © 2011 Published by Elsevier Ltd. Open access under CC BY-NC-ND license.
Selection and/or peer-review under responsibility of Prof. Mitsuhisa Sato and Prof. Satoshi Matsuoka
doi:10.1016/j.procs.2011.04.249

2288 	

Aniruddha G. Shet et al. / Procedia Computer Science 4 (2011) 2287–2296
A.G. Shet et al. / Procedia Computer Science 00 (2011) 1–10

2

[1, 2]. Planning for the first generation of exascale machines suggests that maintaining failure rates at this level will
be challenging, requiring significant new eﬀorts on fault tolerance throughout the system [3, 4].
Although a variety of other techniques have been developed (see Section 5), few are widely used today, at least in
part, because of their intrusiveness into HPC applications. As a practical matter, checkpoint/restart (C/R) is the one
approach in wide use today for resilience of high-performance computational science and engineering codes.
In this paper, we explore several strategies applications can use to augment primary resilience capabilities, like
C/R, in the context of a framework for coupled multiphysics simulation of fusion plasmas. One strategy exploits the
“multiple-program multiple-data” (MPMD) nature of the coupled simulation for a more localized task re-execution
(restart) capability. The second strategy is more general, and connects the simulation framework to information
external to the application about the status and health of the computer system to allow the framework to make betterinformed decisions about restarts (or other responses to faults). Although the implementation described is simplified
by leveraging certain characteristics of the types of applications we are pursuing, the concepts presented are more
broadly applicable, and their generalization may point towards useful lines of future research.
In Section 2 of this paper, we briefly describe the Fault Tolerance Backplane (FTB), which we use to make fault
information available to the application. Then in Section 3, we present the Integrated Plasma Simulator (IPS), a
framework for coupled multiphysics simulations of plasmas. We will briefly describe its architecture, as well as its
resilience-related features, including the interface to the FTB. Section 4 provides more specific details of how the
fault tolerance strategies we have implemented can augment the widely-used checkpoint/restart approach, providing
additional resilience to the application. After a short discussion to place this work in the context of the much broader
field of fault tolerance (Section 5), we close with a recap of the key features of the environment that enable our fault
tolerance approach and discuss how the approach might translate to other hardware environments and other simulation
frameworks (Section 6).
2. The Fault Tolerance Backplane
The goal of the Coordinated Infrastructure for Fault Tolerant Systems (CIFTS) project [5] is to explore how wider
availability of fault information can facilitate more eﬀective responses to faults. Currently, detection and reaction
to faults are often limited to the specific subsystems where the trouble originates. This may be acceptable if the
subsystem can successfully correct or mask the fault so that other subsystems are not aﬀected, but often this is not
possible. However, if fault awareness is communicated to the other layers of the software stack, it may be possible
to create solutions that can handle faults more rapidly and less expensively, than if treated in isolation. Coordination
across multiple subsystems may allow more eﬀective or more useful (to the users) responses to faults.
The first step along this path is to make fault information more accessible throughout HPC systems. To this end, the CIFTS project has
FTB client
FTB client
developed the Fault Tolerance Backplane (FTB) [6], a publish/subscribe
Connect
Connect
event service intended to make it easy for subsystems throughout the enSubscribe
Publish event
tire software stack, and across HPC systems, or even entire computer
FTB Agent
centers, to discover and make information available about the health and
behavior of the system. As shown in Figure 1, a network of FTB agents
FTB Agent
FTB Agent
form the backbone of the FTB, routing messages between publishing and
FTB Agent
subscribing clients, as well as maintaining the resilience of the FTB itFTB Agent
self. Redundant FTB bootstrap servers help clients join the network. The
philosophy behind the FTB promotes the integration of any and all parts
of the HPC environment as FTB clients, including the hardware ReliabilFigure 1: A schematic of the architecture of the
ity, Availability and Serviceability (RAS) systems; I/O systems; schedFault Tolerance Backplane, from [6]. See text
ulers; batch managers; accounting systems; operating system compofor further details.
nents; communication, numerical, checkpoint/restart, and other libraries,
and applications themselves.
The FTB provides a relatively small interface (API) for clients. Operations include joining and leaving the FTB,
subscribing and unsubscribing to specific categories of events, and publication. Both polling and callback mechanisms
are available for event delivery. The FTB event names are strings, which live in a hierarchical namespace with
wildcarding.
Bootstrap

Aniruddha G. Shet et al. / Procedia Computer Science 4 (2011) 2287–2296
A.G. Shet et al. / Procedia Computer Science 00 (2011) 1–10

2289
3

3. The Integrated Plasma Simulator
The computational goal of the Center for Simulation of Radio Frequency Wave Interactions with Magnetohydrodynamics (SWIM) [7] is to develop a framework to enable coupled multiphysics simulations to gain a better
understanding of the behavior of plasmas and how to control them for fusion energy production. The project is also
designed to serve as a prototype for next generation frameworks for comprehensive “whole device” modeling of fusion
plasmas.
Most of the physics applications that make up the coupled simulations of interest to SWIM have been in use
and development for decades, and continue to be developed and used for other projects besides SWIM. Therefore,
it is important that the applications be changed as little as possible for SWIM. These physics applications also vary
significantly in their parallel scalability, including codes which are strictly sequential, modestly scalable (typically
tens of processors), and highly scalable (hundreds or thousands of processors). All of the parallel applications of
interest to SWIM currently use MPI for communication.
To address the project’s requirements, we developed the Integrated Plasma Simulator (IPS) [8, 9, 10], a lightweight
component framework based on the concepts of the Common Component Architecture (CCA) [11], implemented in
Python. IPS components are unmodified executables of physics applications, wrapped with Python. The wrapper,
along with small “helper” executables, adapt the application’s native inputs and outputs to an interface the IPS expects.
Components generally exchange small amounts of data (typically a few megabytes) via a “plasma state” file [12],
a set of special purpose netCDF files containing plasma specific data and a Fortran library to access them. The
IPS framework provides a small set of services to components, such as data (file) management, resource and task
management, configuration information, and an event service (Figure 2).
IPS simulations are typically time-stepped, with
multiple components run at each time step in sequence dictated by their data dependencies. The details of these simulations vary with the physics of interest, but in current SWIM simulations, time steps
range from a few minutes to an hour or more of wall
clock time, and involve many hundreds to a few thousand steps.
The IPS is designed to execute in self-contained
fashion as a batch job or interactively. The core of
the IPS framework runs as a single Python process
which may be run on a head node, service node, or
in the compute partition depending on the system architecture and the capabilities or limitations of each
class of nodes. Components are spawned as separate processes, which in turn invoke the underlying
Figure 2: An illustration of the relationships between the framework,
components, services, and tasks in the Integrated Plasma Simulator (IPS).
physics codes. These computational tasks, which are
in general parallel, are launched using appropriate
mechanisms for the host system, such as mpiexec or aprun (Cray). The current version of the framework supports
“concurrent multitasking” or MPMD execution through non-blocking versions of both component method invocations
and launching of underlying physics tasks [9]. The IPS’s resource manager (RM) tracks utilization of nodes from the
pool allocated to the job by the batch system and processes resource requests from the task manager (TM) using a
simple first-come first-served (FCFS) algorithm with backfill.
The IPS has a number of features related to fault tolerance, which were included in the design of the current
version. Although some of these features have been mentioned in previous papers describing the IPS, this is the first
publication to discuss them in detail.
The data manager implements a simulation-wide C/R capability, in cooperation with components, which are
expected to provide checkpoint() and restart() methods. Checkpoints can be scheduled on intervals based on
either the wall clock time or the physics time step. When a checkpoint is triggered, the checkpoint() method
is invoked on each component participating in the simulation. As with the data exchanged between components,
SWIM’s checkpoint sizes are quite modest as well. In addition to resilience, the IPS C/R capability is also used to

2290 	

Aniruddha G. Shet et al. / Procedia Computer Science 4 (2011) 2287–2296
A.G. Shet et al. / Procedia Computer Science 00 (2011) 1–10

4

facilitate scientific workflows, such as running a simulation until it achieves a steady state and restarting diﬀerent runs
from that point to examine diﬀerent perturbations to the plasma.
The IPS event service (ES) similarly has multiple uses; for fault tolerance it serves as a bridge between the FTB and
the IPS framework and components, making fault information available via the FTB available to any interested part
of the IPS. As we will describe in the next section, this enables the IPS to make better-informed decisions about, for
example, the handling of failed computational tasks. The IPS resource manager (RM) is designed to use FTB events
to track the status of computational resources allocated to it as the simulation progresses. Similarly, the task manager
(TM) can use FTB information to determine whether a failed task has a chance of succeeding if it is re-executed.
The final resilience “feature” used in this work is the fact that computational tasks in the IPS are strongly independent of each other, each being launched as a separate mpiexec (or equivalent). This aspect of the design of the
IPS is a reflection of the loosely coupled nature of SWIM simulations and the desire, discussed above, to preserve
the individual physics codes in binary form as the basis for SWIM components. Based on this, the IPS task manager
incorporates a re-execution option, which allows failed tasks to be relaunched a configurable number of times in an
attempt to overcome faults. Or if information is available from the FTB, more intelligent decision-making is possible.
4. Examples of Fault Tolerance in the IPS
In this section we illustrate how the Integrated Plasma Simulator can take advantage of task re-execution strategies
and external fault information to provide users with improved resilience over the simple C/R approach. To be concrete,
we will use the Cray XT as the hardware platform for these examples, though the approach is not fundamentally
platform-specific.
4.1. Simulation Scenario
The IPS is designed to be extremely flexible with respect to the construction and execution of simulations. Using
it’s MPMD capabilities, multiple tasks within a simulation can be run concurrently (data dependencies permitting, of
course), and multiple distinct simulations can be run simultaneously in a single IPS invocation [13]. For this purpose,
we have chosen a very simple simulation scenario which does not use any of the MPMD features of the framework in
order to focus our presentation on the fault tolerance.
The chosen simulation, the “ANT” scenario of [13], involves three components which compute the eﬀects of
radio-frequency heating (AORSA [14]) and neutral particle beam injection (NUBEAM [15, 16]) on the plasma and
evaluate transport eﬀects within it (TSC [17, 18]). The simulation spans 1000 physics time steps, in which TSC,
AORSA, and NUBEAM are each executed in sequence. In each step, TSC runs for 130 seconds on 1 core, AORSA
for 1020 s on 1024 cores, and NUBEAM for 1020 s on 512 cores. This amounts to roughly 36 minutes of wall clock
time per step, and a total of 25.1 days to complete the entire simulation.
To examine the utility of task re-execution in IPS simulations, we have extended our IPS simulator, RUS [13],
to support the injection of node failure events. Following [19, 20], we assume a 4 year mean time between failures
per node, following a Weibull distribution with exponent 0.7. The results reported are the average of 100 simulated
executions.
We provision jobs with 5% extra nodes (48 additional cores for the ANT scenario) to accommodate some node
failures before requiring a resubmission through the batch queue system. Failed tasks are retried at most three times
before the simulation is aborted. There is a 60 s delay before re-executing a task to allow the compute nodes time to
clear and reset. The IPS resource manager removes failed nodes from the resource pool for the duration of the job.
As previously mentioned, checkpointing in the IPS involves copying of key files, which for this scenario takes about
20 s. We consider various checkpoint intervals ranging from 2 to 39 physics time steps, or approximately 1.2 hours to
23.4 hours of wall clock time.2

2 Note that 19 and 39 time steps in our ANT scenario are approximately 12 and 24 hours, which are common limits on the length of individual
batch jobs at many computer centers.

Aniruddha G. Shet et al. / Procedia Computer Science 4 (2011) 2287–2296
A.G. Shet et al. / Procedia Computer Science 00 (2011) 1–10

2291
5

4.2. Comparison of Checkpoint/Research and Task Re-Execution
Figure 3 compares the additional cost of a complete 1000-step fault tolerant execution for C/R only and using C/R and task re-execution (T/R) together compared to the baseline cost of a (hypothetical) fault-free execution.
In the C/R-only case, we see the rapid rise in the amount of rework required
as the checkpoint interval lengthens. In the C/R + T/R case, task re-execution
is able to handle all faults until the pool of extra nodes is exhausted and the
batch job has to be resubmitted to get a new set of nodes. For 100 trials, the
C/R-only strategy required an average of 1.05 batch resubmissions in order
to complete the 1000 steps, while for the C/R + T/R strategy averaged 0.85
batch resubmissions.
These results clearly illustrate the eﬀectiveness of re-executing tasks,
when possible, over restarting the entire simulation after rolling back possibly several time steps. Next, we explore how more sophisticated responses
to failures can be facilitated by integrating information about the nature of
the faults.

Figure 3: Extra cost incurred for completion
of 1000-step ANT simulation (relative to a
hypothetical fault-free execution) using only
checkpoint/restart (C/R) or C/R and task reexecution (T/R) fault tolerance strategies.

4.3. Cray XT RAS and FTB
A standard part of most modern purpose-built HPC systems is a Reliability, Availability and Serviceability (RAS) subsystem. This is a set of
sensors, controllers, computers, and networks which are independent of the
main HPC system and used to monitor and control its operation at the most basic levels. Such systems monitor
everything from basic hardware sensors (e.g. temperatures, voltages, fan speeds) to the liveness of the network and
operating system. This information is typically collected on a management system where it is logged and monitored.
The RAS system usually also provides functionality to control and manage the system, including power, booting, and
the status of resources (i.e. whether nodes are in or out of service).
The Cray RAS is considered by administrators to be relatively verbose, capable of generating more than 200
diﬀerent events. Many of these events are informational, or warnings of transient conditions, only a subset represent
faults that would interrupt a running task. The IBM Blue Gene/P RAS system is generally similar, and is capable of
generating 40 unique messages which are logged to a relational database [21].
By integrating the RAS subsystem with the FTB, and the FTB with the IPS framework, RAS events become
visible to any interested component or framework service.

4.4. Analysis of Faults and Responses
As with any system, the Cray has a variety of faults that take a node out of service until it is repaired. For example:
the machine check exception event indicates the failure of a CPU or a memory module, node voltage fault and verty
health check fault indicate failures of diﬀerent voltage regulator modules, a link inactive event indicates a problem
with the high-speed interconnect, and operating system kernel panic events also take the node down for diagnosis.
When such events occur, the task running on the node, for example the AORSA code, will fail, which is detected
by the IPS task manager. The RAS event (passed via the FTB to the IPS’s internal event service) will trigger the
IPS resource manager to mark the node as failed. This insures that the failed node will not be assigned to any future
resource allocation requests, and of course, reduces the total number of nodes available to the simulation running
within the IPS. As part of re-launching a replacement AORSA task the TM will obtain a new allocation from the RM,
and reset the component’s working directory to its initial state.
Another class of faults will interrupt jobs running on a node, but do not require the node to be taken out of service.
One notable example of this on the hardware side comes from the IBM Blue Gene/L, where single-bit memory errors
were detected, but could not be corrected [22] (on the Blue Gene/P, as well as on the Cray XT, single-bit errors are
detected and corrected without interruption of the application). On the Cray XT, most such errors are on the software
side of the system, such as out of memory (OOM) errors or segmentation faults. In general, the IPS can respond to the
task failure in the same manner as described above, except that there is no change to the resource pool, since nodes

2292 	

Aniruddha G. Shet et al. / Procedia Computer Science 4 (2011) 2287–2296
A.G. Shet et al. / Procedia Computer Science 00 (2011) 1–10

6

are not disabled by these events. However software-related failures should be analyzed carefully to determine whether
re-execution is, in fact, an appropriate response.
For example, a segmentation fault in a user code is generally an indication of a bug in the software. In many cases,
the same problem would recur if the task were re-executed, so the preferred response would be to abort the simulation
and let the application developer debug their code. On the other hand, in the real world, it is also not uncommon
to find applications with acknowledged but undiagnosed bugs that cause the code to fail only occasionally, and for
which re-execution is the accepted response.
The out of memory (OOM) event is produced by user software trying to allocate more memory than is available on
a node (HPC systems like the Cray XT do not have virtual memory). These errors are quite common as users push their
simulations to the limits of available resources. If the application itself does not provide direct controls over memory
usage, the practical solution to this problem is often to use fewer cores per node, thus providing more memory on a
per-process basis. Since the IPS allows configuration of the number of processes to launch per node, the IPS can adapt
its configuration based on OOM events to reduce the number of processes per node (with a corresponding increase in
the number of nodes requested) before rerunning the task.
As an aside, it might seem odd for user software errors like segmentation faults and OOM to appear as events in
the RAS system—most developers will be familiar with such errors appearing on workstations as exit codes directly
from the application, easily accessed without a RAS system. In HPC systems, however, it is easy for exit code
information to be lost to the user because of the computational environment. Consequently, the RAS system is often
the only practical way to obtain reliable information about occurrences of such errors, and except for the FTB, such
information is often only be available to system administrators.
5. Related Work
Fault tolerance is an extremely broad area, which would require numerous review articles to cover adequately. For
the purposes of this paper, we briefly outline some of the approaches to fault tolerance most likely to be relevant to
HPC CSE applications and discuss how they relate to our work. Even with this restriction, space limitations to not
allow citation of all relevant work. Therefore, we have selected certain key papers and recent papers which themselves
cite much of the other relevant work.
As already mentioned, the most commonly used fault tolerance technique in HPC is checkpoint/restart [23]. As it
is usually implemented, C/R provides a recovery strategy after faults occur within the system which force the application to terminate prematurely. The costs of the C/R approach include the additional resources spent periodically taking
and storing checkpoints, the restart time, and the compute time lost between the last usable checkpoint and the point
at which the failure occurred. Coordinated checkpointing, which is the most common approach generally requires
restarting the entire application for recovery. Uncoordinated checkpointing is more amenable to recovery of a single
task or process, but incurs additional overheads such as message logs and the need to retain more checkpoints [23].
At present, coordinated checkpoints, either under application control or system control can generally be implemented
with little impact on existing applications, and are the de facto standard for application resilience in computational
science and engineering today. Eﬀorts to improve the performance and scalability of C/R generally involve reduction
of the size of checkpoints through memory exclusion [24, 25], and optimizing the I/O, which has inspired a wide
range of activity [26, 27, and references therein].
A variety of algorithm-based fault tolerance (ABFT) techniques have been explored over the years, including
coding (checksum) techniques for specific operations (usually linear algebra) [28, 29, and references therein], and
exploitation of “naturally fault tolerant” (NFT) versions of algorithms [30]. Coding techniques incur overheads due
to the additional storage and computation involving the checksums, as well as during recovery, though they oﬀer the
advantage of being able to detect certain kinds of errors (e.g. data corruption rather than fail-stop faults) that might
otherwise go undetected. The additional cost of naturally fault tolerant approaches occurs primarily during recovery
(longer time to solution), though the NFT version of an algorithm may not be optimal in terms of performance or
resource utilization. Most ABFT algorithms are designed to “run through” faults rather than requiring the simulation
be stopped and restarted. Despite the fact that these approaches have been known for some time, they are not very
well developed, and to our knowledge have seen little or no “production” use in HPC CSE. There are several likely
reasons for this, including the fact that the approach has to be worked out on a per-algorithm basis, and only a limited

Aniruddha G. Shet et al. / Procedia Computer Science 4 (2011) 2287–2296
A.G. Shet et al. / Procedia Computer Science 00 (2011) 1–10

2293
7

number of algorithms have been developed thus far. Additionally, many ABFT approaches are very intrusive, and
would require major modifications to existing applications.
Replication-based approaches execute multiple instances of computational tasks to protect against failures. The
additional cost of such an approach is, of course, the cost of each replica beyond the first, though as with ABFT
approaches, it is generally possible for the application to run through faults rather than spend additional time in an
explicit restart or recovery phase. N-modular redundancy is a well-known technique for both fault detection and
fault tolerance which is based on replicated execution. The additional cost of replication-based approaches makes
them unattractive unless failure rates are high [31]. They are routinely used in “volunteer” distributed computing
projects (e.g. SETI@Home) and similar settings, but they are not common in traditional HPC applications. While it
should in principle be possible to implement many replication-based approaches in a fashion that is transparent to the
application, the required infrastructure has not been available in the traditional HPC context. Recently, several MPI
implementations designed to support process replication have been reported [32, 33, 34, 35, 36], which could greatly
simplify the development of replicated applications.
Our work has both similarities and diﬀerences with these approaches. Our task re-execution strategy can be
viewed as the restart of a failed task from a “checkpoint” which is, in this case, implicit in the time-stepped nature
of the simulation. Because of the isolation between tasks, we do not need the complexities usually associated with
uncoordinated checkpointing. Although we would not categorize our work as algorithm-based fault tolerance, there
are characteristics of the algorithms (i.e. the loose coupling of the physics) that helps make our approach feasible (see
Section 6 for further discussion).
Regarding our strategy of using external fault information to guide decisions about task re-execution, we are aware
of only one other paper where such a connection is made. Ferreira et al. use the RAS system on the Cray Red Storm
to track the status of nodes in their rMPI resilient MPI implementation [37]. They make no mention of dealing with
faults other than node failures.
6. Discussion
In this work, we have demonstrated two strategies which can be utilized by applications to augment the resilience
provided by checkpoint/restart (or other major approaches). One strategy relies on the multicomponent, loosely
coupled nature of our application to enable task-level re-execution which has less overhead and is more localized
than a whole-simulation restart. The second strategy involves the use of external information about faults to guide
decisions within the framework about whether restarts (at the task or whole-simulation level) are likely to be useful.
To provide these capabilities, we leverage a number of services both outside and within the IPS:
• The Reliability, Availability, and Serviceability subsystem of the platform, which provides definitive information about faults occurring in the system;
• The Fault Tolerance Backplane, which is a general publish/subscribe event service for the dissemination of fault
information throughout HPC systems with, in this case, the RAS system and the application as FTB clients;
• Services within the Integrated Plasma Simulator application framework, including:
– An Event Service, which in this case bridges events from the FTB to other framework services;
– A Resource Manager which uses fault information to track the availability of compute nodes within its
allocation;
– A Task Manager which implements policies for handling faults (e.g., re-executing failed tasks) based on
configuration parameters and live fault information.
– A Data Manager which can reset the execution environment (working directory contents) back to that
appropriate to restart the failed task.
By working together, these services allow the IPS to recover from task failures by re-executing the task automatically, without user intervention or manual termination followed by restart (or requeue) the entire job. The IPS can
also adapt to the execution characteristics of individual tasks by modifying the deployment (processes per node) to
give more memory per process in response to a history of out of memory errors.

2294 	

Aniruddha G. Shet et al. / Procedia Computer Science 4 (2011) 2287–2296
A.G. Shet et al. / Procedia Computer Science 00 (2011) 1–10

8

6.1. Prospects for Generalization to other Platforms and Applications
The capabilities provided by the IPS are quite general, in the sense that there are a few distinct classes of faults
that the framework responds to. All such faults will interrupt a running task, but they may or may not take the node
out of service. Some faults warrant special processing or adaptation of the task as part of the retry process (like out of
memory). Given a RAS system or other source of fault information, the messages available on that system can be used
to classify faults according to the categories above. The “portability” of the IPS’s fault tolerance would be facilitated if
there were “standard” messages for common faults across diﬀerent systems. Such standardization could be provided
at the FTB level, so that the FTB component interfacing with the RAS subsystem would map the system-specific
faults to the standard general messages, avoiding the need to reconfigure the application on a per-system basis. The
CIFTS project is currently working with the MPI community on such standards, and plans similar activities for other
subsystems.
Multicomponent coupled multiphysics simulations are becoming an increasingly prominent part of the way modern computational science is done, driven by a synergy between the increasing power of computers and increasing
scientific understanding of individual phenomena and their inability to fully reproduce real world observations. The
fact that the IPS provides a strong isolation between diﬀerent tasks by launching them as separate mpiexecs is a
distinctive feature of our coupled application that clearly facilitates the basic task re-execution strategy. However the
approach we have described here could also be applied to other simulation frameworks, with some caveats. One key
requirement is that a single component or task within the framework to is able to fail without taking down the entire
simulation. This requirement may be manifest at several diﬀerent levels. Simulations of interest to SWIM are loosely
coupled, so components exchange data infrequently (generally at the beginning and end of each task). Other simulations with more intensive coupling between the components may not provide the required separation, or it may be that
the interruption of one task will require others tightly connected to it to be terminated and restarted with it. Another
level is the communication infrastructure used by the framework. In the IPS, individual tasks generally use MPI for
parallelism, but separate tasks do not use MPI to communication with each other. Given the MPI standard’s default
requirement that the failure of one process in a parallel job causes the entire job to abort, this could pose problems for
simulation frameworks in which diﬀerent components are linked by MPI or another communication layer with similar
fault intolerance properties. It is worth noting that there are plans to improve the MPI standard’s fault tolerance in the
forthcoming version 3 [38], and there are several MPI implementations with (non-standard) fault tolerance capabilities
that might be useful in this context [35, 36] though to our knowledge, none of these implementations are currently
in production use in the HPC community. The use of file-based data exchange in the IPS is another convenience for
this approach. Not only does in-memory data exchange generally lead to tighter connections between tasks, as noted
above, it also requires more vigilant checkpointing (or other mechanisms) to insure that the data required to retry a
failed task is available. Finally, the dynamic task management and resource allocation is key to the ability of the IPS to
accommodate node failures, though applications with more static approaches might be able to achieve similar results
by over-provisioning the job and stopping and restarting the whole simulation.
6.2. Future Work
Overall, this work represents a modest, but practical and useful fault tolerance capability beyond the widely-used
checkpoint/restart (which the IPS also supports) and suggests other adaptations and responses that we have not yet
pursued.
There are many diﬀerent sources of fault information throughout the HPC environment that an application might
be able to make use of. For example, there is essentially no information about the health of the filesystems available,
in our testbed. Developing an FTB client for the file server would remedy this. If appropriate information were
available, the IPS might be extended utilize alternate filesystems in response to a failure or performance degradation.
Cooperation between the application and the system might facilitate recovery system-wide by temporarily relieving
or shifting loads from trouble resources to good ones.
Another possible IPS enhancement would be negotiating a diﬀerent level of parallelism for a task in response to
node failures reducing the size of the resource pool.
When the application has more awareness of the state of the system it is running on, other services may also be
useful. For example, currently it is common for users to over-provision their batch requests with additional nodes
in anticipation of failures during the run. If instead, the system resource manager maintained a system-wide pool

Aniruddha G. Shet et al. / Procedia Computer Science 4 (2011) 2287–2296
A.G. Shet et al. / Procedia Computer Science 00 (2011) 1–10

2295
9

of “spare” nodes, held in reserve to replace failed nodes in running jobs, the need to over-provision individual jobs
might be eliminated. The IPS resource manager is already capable of adding new nodes to the resource pool, so the
only modification required to take advantage of such a capability would be to generate the request when a node fails
(assuming it is not automatic).
6.3. Conclusion
Looking forward, we anticipate that fault detection (which we have not addressed in this work) and fault tolerance
will become increasingly important as the HPC community builds out towards the exascale and beyond. There are
many questions as to how much further it will be possible to push checkpoint/restart, and so it is increasingly important
that computer scientists and computational scientists to work together to identify new approaches to augment the fault
tolerance of applications. Our contribution suggests that with thoughtfully designed frameworks, as are increasingly
being used for many types of simulations, there is potential for solutions that do not require significant modifications
to user code to be eﬀective.
7. Acknowledgments
We would like to thank Scott Hampton, Thomas Naughton, and especially Josh Hursey for many helpful discussions and for comments on the manuscript. We also thank the staﬀ of the National Center for Computational Sciences
at ORNL, especially Don Maxwell for patiently answering our many questions.
This work has been supported by the U. S. Department of Energy, Oﬃce of Science, Oﬃces of Advanced Scientific Computing Research (ASCR) and Fusion Energy Sciences (FES). It has also been supported by by the ORNL
Postmasters and Postdoctoral Research Participation Programs and the ORNL Higher Eduction Research Experiences
Program which are sponsored by ORNL and administered jointly by ORNL and by the Oak Ridge Institute for Science and Education (ORISE). This research also used resources of the Oak Ridge Leadership Computing Facility at
ORNL. ORNL is managed by UT-Battelle, LLC for the U. S. Department of Energy under Contract No. DE-AC0500OR22725. ORISE is managed by Oak Ridge Associated Universities for the U. S. Department of Energy under
Contract No. DE-AC05-00OR22750.
References
[1] J. Rogers, Deploying large scale XT systems at ORNL, Proceedings of the Cray User’s Group Conference, http://www.cug.org/
5-publications/proceedings_attendee_lists/CUG09CD/S09_Proceedings/pages/authors/16-18Thursday/16B-Rogers/
rogers_slides.pdf (2009).
[2] L. D. Crosby, Parallel IO and fault tolerance, Virtual School of Computational Science and Engineering. Summer School 2009: Scaling to
Petascale, http://www.greatlakesconsortium.org/events/scaling/files/Parallel_IO_Fault_Tolerance-2.pdf (August
2009).
[3] DOE
exascale
initiative
technical
roadmap,
http://extremecomputing.labworks.org/hardware/collaboration/
EI-RoadMapV21-SanDiego.pdf (Dec 2009).
[4] F. Cappello, A. Geist, B. Gropp, L. Kale, B. Kramer, M. Snir, Toward exascale resilience, International Journal of High Performance Computing Applications 23 (4) (2009) 374 –388.
[5] Coordinated Iinfrastructure for Fault Tolerant Systems, http://www.mcs.anl.gov/research/cifts/.
[6] R. Gupta, P. Beckman, H. Park, E. Lusk, P. Hargrove, A. Geist, D. K. Panda, A. Lumsdaine, J. Dongarra, CIFTS: A Coordinated Infrastructure
for Fault-Tolerant Systems, in: Proceedings of 38th International Conference on Parallel Processing (ICPP) 2009, 2009.
[7] Center for Simulation of RF Wave Interactions with Magnetohydrodynamics, http://cswim.org/.
[8] W. Elwasif, D. E. Bernholdt, A. G. Shet, S. S. Foley, R. Bramley, D. B. Batchelor, L. A. Berry, The design and implementation of the SWIM
Integrated Plasma Simulator, in: Parallel, Distributed and Network-Based Processing (PDP), 2010 18th Euromicro International Conference
on, 2010, pp. 419–427.
[9] S. S. Foley, W. R. Elwasif, A. G. Shet, D. E. Bernholdt, R. Bramley, Incorporating concurrent component execution in loosely coupled
integrated fusion plasma simulation, in: Component-Based High-Performance Computing (CBHPC), Karlsruhe, Germany, 2008.
[10] W. R. Elwasif, D. E. Bernholdt, L. A. Berry, D. B. Batchelor, Component framework for coupled integrated fusion plasma simulation, in:
HPC-GECO/CompFrame – Joint Workshop on HPC Grid Programming Environments and Components and Component and Framework
Technology in High-Performance and Scientific Computing, Montreal, Canada, 2007.
[11] B. A. Allan, R. Armstrong, D. E. Bernholdt, F. Bertrand, K. Chiu, T. L. Dahlgren, K. Damevski, W. R. Elwasif, T. G. W. Epperly, M. Govindaraju, D. S. Katz, J. A. Kohl, M. Krishnan, G. Kumfert, J. W. Larson, S. Lefantzi, M. J. Lewis, A. D. Malony, L. C. McInnes, J. Nieplocha,
B. Norris, S. G. Parker, J. Ray, S. Shende, T. L. Windus, S. Zhou, A component architecture for high-performance scientific computing, Intl.
J. High-Perf. Computing Appl. 20 (2) (2006) 163–202.
[12] D. McCune, Plasma State, http://w3.pppl.gov/ntcc/PlasmaState/.

2296 	

Aniruddha G. Shet et al. / Procedia Computer Science 4 (2011) 2287–2296
A.G. Shet et al. / Procedia Computer Science 00 (2011) 1–10

10

[13] S. S. Foley, W. Elwasif, D. E. Bernholdt, A. G. Shet, R. Bramley, Many-task applications in the integrated plasma simulator, in: MTAGS ’10:
Proceedings of the 3rd Workshop on Many-Task Computing on Grids and Supercomputers, 2010, in press.
[14] E. F. Jaeger, L. A. Berry, E. D’Azevedo, D. B. Batchelor, M. D. Carter, K. F. White, H. Weitzner, Advances in full-wave modeling of radio
frequency heated, multidimensional plasmas, Physics of Plasmsa 9 (5) (2002) 1873–1881.
[15] R. J. Goldston, D. C. McCune, H. H. Towner, S. L. Davis, R. J. Hawryluk, G. L. Schmidt, New techniques for calculating heat and particle
source rates due to neutral beam injection in axisymmetric tokamaks, J. Computat. Phys. 43 (1) (1981) 61–78.
[16] A. Pankin, D. McCune, R. Andre, G. Bateman, A. Kritz, The tokamak Monte Carlo fast ion module NUBEAM in the National Transport
Code Collaboration library, Computer Phys. Comm. 159 (3) (2004) 157–184.
[17] S. C. Jardin, N. Pomphrey, J. Delucia, Dynamic modeling of transport and positional control of tokamaks, J. Computat. Phys. 66 (2) (1986)
481–507.
[18] S. Jardin, M. Bell, N. Pomphrey, TSC simulation of Ohmic discharges in TFTR, Nucl. Fusion 33 (3) (1993) 371–382.
[19] R. Riesen, K. Ferreira, J. Stearley, See applications run and throughput jump: The case for redundant computing in HPC, in: Dependable
Systems and Networks Workshops (DSN-W), 2010 International Conference on, 2010, pp. 29 –34.
[20] B. Schroeder, G. Gibson, A large-scale study of failures in high-performance computing systems, in: Dependable Systems and Networks,
2006. DSN 2006. International Conference on, 2006, pp. 249 –258.
[21] IBM system Blue Gene solution: Blue Gene/P system administration, http://www.redbooks.ibm.com/abstracts/sg247417.html
(Aug. 2010).
[22] J. N. Glosli, D. F. Richards, K. J. Caspersen, R. E. Rudd, J. A. Gunnels, F. H. Streitz, Extending stability beyond CPU millennium: a micronscale atomistic simulation of Kelvin-Helmholtz instability, in: Proceedings of the 2007 ACM/IEEE conference on Supercomputing, ACM,
Reno, Nevada, 2007, pp. 1–11.
[23] E. N. M. Elnozahy, L. Alvisi, Y. Wang, D. B. Johnson, A survey of rollback-recovery protocols in message-passing systems, ACM Comput.
Surv. 34 (3) (2002) 375–408.
[24] J. S. Plank, Y. Chen, K. Li, M. Beck, G. Kingsley, Memory exclusion: optimizing the performance of checkpointing systems, Software:
Practice and Experience 29 (2) (1999) 125–142.
[25] S. Agarwal, R. Garg, M. S. Gupta, J. E. Moreira, Adaptive incremental checkpointing for massively parallel systems, in: ICS ’04: Proceedings
of the 18th annual international conference on Supercomputing, ACM, New York, NY, USA, 2004, pp. 277–286.
[26] X. Dong, N. Muralimanohar, N. Jouppi, R. Kaufmann, Y. Xie, Leveraging 3D PCRAM technologies to reduce checkpoint overhead for future
exascale systems, in: Proceedings of the Conference on High Performance Computing Networking, Storage and Analysis, ACM, Portland,
Oregon, 2009, pp. 1–12.
[27] J. Bent, G. Gibson, G. Grider, B. McClelland, P. Nowoczynski, J. Nunez, M. Polte, M. Wingate, PLFS: a checkpoint filesystem for parallel
applications, in: SC ’09: Proceedings of the Conference on High Performance Computing Networking, Storage and Analysis, ACM, New
York, NY, USA, 2009, pp. 1–12.
[28] K. Huang, J. Abraham, Algorithm-Based fault tolerance for matrix operations, Computers, IEEE Transactions on C-33 (6) (1984) 518–528.
[29] G. Bosilca, R. Delmas, J. Dongarra, J. Langou, Algorithm-based fault tolerance applied to high performance computing, J. Parallel Distrib.
Comput. 69 (4) (2009) 410–416.
[30] C. Engelmann, A. Geist, Super-Scalable algorithms for computing on 100,000 processors, in: V. Sunderam, G. van Albada, P. Sloot, J. Dongarra (Eds.), Computational Science – ICCS 2005, Vol. 3514 of Lecture Notes in Computer Science, Springer Berlin / Heidelberg, 2005, pp.
313–321, 10.1007/11428831 39.
[31] R. Zheng, J. Subhlok, A quantitative comparison of checkpoint and restart and replication in volatile environments, Tech. Rep. US-CS-08-06,
Department of Computer Science, University of Houston, Houston, TX (June 23 2008).
[32] R. Brightwell, K. Ferreira, R. Riesen, Transparent redundant computing with MPI, in: R. Keller, E. Gabriel, M. Resch, J. Dongarra (Eds.),
Recent Advances in the Message Passing Interface, Vol. 6305 of Lecture Notes in Computer Science, Springer Berlin / Heidelberg, 2010, pp.
208–218, 10.1007/978-3-642-15646-5 22.
[33] T. LeBlanc, R. Anand, E. Gabriel, J. Subhlok, VolpexMPI: an MPI library for execution of parallel applications on volatile nodes, in: M. Ropo,
J. Westerholm, J. Dongarra (Eds.), Recent Advances in Parallel Virtual Machine and Message Passing Interface, Vol. 5759 of Lecture Notes
in Computer Science, Springer Berlin / Heidelberg, 2009, pp. 124–133, 10.1007/978-3-642-03770-2 19.
[34] S. Genaud, C. Rattanapoka, P2P-MPI: a Peer-to-Peer framework for robust execution of message passing parallel programs on grids, Journal
of Grid Computing 5 (1) (2006) 27–42.
[35] R. Batchu, Y. S. Dandass, A. Skjellum, M. Beddhu, MPI/FT: a Model-Based approach to Low-Overhead fault tolerant Message-Passing
middleware, Cluster Computing 7 (4) (2004) 303–315.
[36] G. Fagg, J. Dongarra, FT-MPI: Fault tolerant MPI, supporting dynamic applications in a dynamic world, in: J. Dongarra, P. Kacsuk, N. Podhorszki (Eds.), Recent Advances in Parallel Virtual Machine and Message Passing Interface, Vol. 1908 of Lecture Notes in Computer Science,
Springer Berlin / Heidelberg, 2000, pp. 346–353, 10.1007/3-540-45255-9 47.
[37] K. Ferreira, R. Riesen, R. Oldfield, J. Stearley, J. Laros, K. Pedretti, T. Kordenbrock, R. Brightwell, Increasing fault resiliency in a messagepassing environment, Tech. Rep. SAND2009-6753, Sandia National Laboratories, Albuquerque, New Mexico and Livermore, California,
http://www.cs.sandia.gov/~rolf/Papers/Misc/ferreira_09_increasing.pdf (October 2009).
[38] MPI
Forum’s
fault
tolerance
working
group
wiki,
https://svn.mpi-forum.org/trac/mpi-forum-web/wiki/
FaultToleranceWikiPage.

