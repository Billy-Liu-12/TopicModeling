Quasi Analog Formal Neuron and Its Learning
Algorithm Hardware
Karen Nazaryan
Division of Microelectronics and Biomedical Devices,
State Engineering University of Armenia,
375009, Terian Str. 105, Yerevan, Armenia
nakar@freenet.am

Abstract. A version of learning algorithm hardware implementation for a new
neuron model – quasi analog formal neuron (QAFN) is considered in this paper.
Due to the presynaptic interaction of “AND” type, wide functional class (including all Boolean functions) for the QAFN operating is provided based on
only one neuron. There exist two main approaches of neurons, neural networks
(NN) and their learning algorithm hardware implementations: analog and digital. The QAFN and its learning algorithm hardware are based on those two approaches simultaneously. Weight reprogrammability is realized based on
EEPROM technique that is compatible with CMOS technology. The QAFN and
its learning algorithm hardware are suitable to implement in VLSI technology.

1 Introduction
An interest in the artificial neural networks and neurons has been greatly increased
especially in the last years. Research show, that the computing machines based on
NNs can easily solve such difficult problems which require not only standard algorithmic approaches. These neural networks and neurons are based on models of biological neurons [1, 2].
Thus, a biological neuron model is presented in fig.1 (below, the word neuron is
implied as the artificial model of biological neuron). A neuron, has one or more inputs
x1, x2, x3, ... xn and one output y. Inputs are weighted, i.e. input xj is multiplied by
weight wj. Weighted inputs of the neuron s1, s2, s3, ... sn are summed by resulting algebraic sum S. In general, a neuron has a threshold value, which also is summed algebraically with S. The result of this whole sum is the argument of f activation function
of the neuron.
One of the most attractive features of NNs is their capability to be learned and
adapted to solve various problems. That adaptability is provided by using learning
algorithms, the aim of which is to find the optimum weight sets for the neurons. There
exist two main approaches of learning algorithm performance: hardware and software.

V.N. Alexandrov et al. (Eds.): ICCS 2001, LNCS 2074, pp. 356–365, 2001.
© Springer-Verlag Berlin Heidelberg 2001

Quasi Analog Formal Neuron and Its Learning Algorithm Hardware

357

In general, neural models differ by the output activation function, input and weight
values. For instance, activation function of classical formal neuron (FN) is Boolean
one, inputs accept binary and weights - integer values [1, 3]. In analog neuron model,
the output function, inputs and weights accept analog values, etc. [4, 5, 6].
x1

w1

x2

w2

.
.
.
xn

�
� n
y = f(S) = f�� � wj x j + �(-1)��
j
=
1
�
�

s1
s2

+

y

f(S)

sn
wn

�
-1

Fig. 1. Mathematical model of biological neuron

2 The QAFN Model
Currently, there exist many neural models and variants of their hardware implementations. This work considers a learning algorithm hardware for the new neuron model
named QAFN (fig.2) [7, 8], which is based on classical FN and enables the increase of
FN functional possibilities.

c1
c2
c3
:
c�
x1
x2
x3
:

xn

PI

c1
c2
c3
:
c�

w1 +
w2 +
w3 +
w�

+

I

S+
w0+

Subtractor
III
(�a1)

c0=1

- (�a2)
c1
c2
c3
:
c�

�

w1
w2 �
w3 �
w� �

w0�

II

S�

Fig. 2. Quasi analog formal neuron

fQAFN

358

K. Nazaryan

On the other hand, research shows [8, 9, 10, 11] that the proposed QAFN model
simplifies ways of hardware implementation, with simultaneous improvement of some
technical parameters and features: operating speed, accuracy, consuming area in semiconductor crystal, etc.
Thus, QAFN realizes the following mathematical equation:
�

�

j� 0

j� 0

f QAFN � a 1 � c j w �j � a 2 � c j w �j ,

(1)

where the activation function fQAFN is a quasi analog one, synaptic cj inputs accept
+
logic 1 and 0 values, weights wj and wj� accept integer nonnegative (0,+1,+2,+3…)
values, a1 and a2 are constants, � is the number of synaptic inputs (fig.3).
The separation of I and II summing parts enable easily to form positive and negative weights, since those parts have identical hardware solutions [9, 10, 11]. Input
interaction or presynaptic interaction (PI) unit realizes a logical input interaction between values of vector X=(x1, x2, ... xn), by forming logical values of vector C=(c1, c2,...
c�). Values of c-s are all possible variants of logical interactions between input values,
n
including all inputs without interactions. Consequently, �=2 -1 (where n is the number
of information inputs x). For instance, in n=3 case, values of c-s are the followings:
N=3, �=7,
c1=x1
c2=x2
c3=x3
c4=x1�x2
c4=x1�x2
c5=x1�x3
c5=x1�x3
c6=x2�x3
c6=x2�x3
c7=x1�x2�x3
c7=x1�x2�x3

(c1,c2,... c7)

(x1,x2,x3)

Without input interaction

Input interaction of logic
AND or OR type

Due to PI, the operating wide functional class of QAFN model is provided for both
Boolean and quasi analog functions, based only on one neuron (without network construction) [1, 9, 12, 13]. This is one of the most important advantages of QAFN model.
The input interaction type (“AND” or “OR”) mathematically is not significant for
providing QAFN functioning. It should be taken into consideration from the hardware
implementation point of view. In this model, the weight values are defined by the
weight binary word. Consequently, the weight storing has a simple realization by
using EEPROM technique.
From a learning point of view, the neural networks are usually taught easier if the
weights of the neurons are allowed to assume both positive and negative values [5].
One of the developed versions of QAFN [9], which provides that opportunity for all
weights, is illustrated in fig.3. In this case a1=a2=1 and only one of the summing parts

Quasi Analog Formal Neuron and Its Learning Algorithm Hardware

359

(I) or (II) are used, and positive or negative weighted influence is defined by switching
the weighted inputs s1, s2, s3 ... either to positive or negative input of the subtractor:

�� c j w �j if w �j � 0;
s j � c j w j � c j (w � w ) � �
�
�
��� c j w j if w j � 0.
�
j

�
j

(2)

The most significant bit (MSB) of weight binary word defines the sign of the given
weight, as it is done in the known computing systems. MSB controls the output of a
switch (IV) to the corresponding positive or negative input of the subtractor (III),
depending on the weight sign value.
For example, if the number of bits in the weight binary word is eight, the values of
wj are the following:

w (j7) w (j6)
0
0
:
0
0
0
1
1
:
1
1

w (j0)

. . . . . .

1
1

1
1

1
1

1
1

1
1

1
1

1
0

0
0
0
1
1

0
0
0
1
1

0
0
0
1
1

0
0
0
1
1

0
0
0
1
1

1
0
0
1
1

0
1
0
1
0

0
0

0
0

0
0

0
0

0
0

0
0

1
0

wj
+127
+126
:
+2
+1
0
�1
�2
:
�127
�128

Thus, QAFN enables to overcome functional drawbacks of classical neurons, increase functional possibilities and adaptability, and improve technical parameters
(speed, features, used area etc.).

3 The QAFN Learning
3.1 The Learning Algorithm
A brief description of a learning algorithm for QAFN will be considered here. The
output function f(k) for the k-th combination of input x-s, generating C(k)=(c1(k), c2(k),
... c�(k)) synaptic inputs, is given by:
�

f (k ) � S(k ) � � w j (k ) � c j (k )
j� 0

(3)

360

K. Nazaryan

If f(k) is equal to the desired value for the k-th combination of input variables, we
will say that C(k) belongs to the �1 functional class, otherwise – C(k) belongs to the �2
functional class [12, 14]. Due to the presynaptic interaction unit of QAFN, �1 and �2
functional classes may be overlapped [9].
.

fQAFN
III
�(�a2) + (�a1)

II

S�

s1-

w1<0 w1�0
IV
s1

w1

sj-

I

sj+

cj

w�<0 w��0
IV
s�

w�

s1+

c1

wj<0 wj�0
IV
sj

wj
s �-

S+

s�+

c�

Fig. 3. QAFN block scheme with weight switches

Let W(k) be an arbitrarily selected initial weight vector. Two learning multitudes
are given that represent �1 and �2 functional classes (they may be linearly unseparable), respectively. In this case the k-th step of the neuron learning is the following [12,
14]:
1. If X(k)�� 1 and W(k)�C(k)<0, then the W(k) weight vector is replaced by
W(k+1)= W(k)+��C(k) vector;
2. If X(k)�� 2 and W(k)�C(k)�0, then the W(k) weight vector is replaced by
W(k+1)= W(k)���C(k) vector;
3. Otherwise, the W(k) weight vector is not changed, i.e. W(k+1)= W(k).
where � is the correcting factor. � is a real positive constant value. It influences on the
learning rate. It is advised to choose value � depending on the given problem.
The learning process is accomplished at the kr+� step, that is

Quasi Analog Formal Neuron and Its Learning Algorithm Hardware

W(kr ) � W(kr �1) � W(kr � 2) � ... � W(kr � �) ,

361

(4)

where � is the number of training multitudes [14].
The above described neuron learning algorithm converges after finite number of iterations that is proved in [14].
The brief description of the learning algorithm for QAFN is considered below.
Assigning the output error

E � T�A,

(5)

where T and A are target (desired) and actual outputs, respectively. Considering that T
and A accept quasi analog values and x-s, consequently c-s � logic 0 or 1, the learning
algorithm for the QAFN, based on the algorithms described in [1, 12, 13, 14], can be
written in the following way:
W (k � 1) � W (k ) � �(k ) � C(k ) ,

(6)

�� � if E � 0 �T(k) � A(k)� (2nd item);
�
�(k ) � � 0 if E � 0 �T(k) � A(k)� (3th item);
�� � if E � 0 �T(k) � A(k)� (1st item).
�

(7)

where

Here, � corresponds to the unit weight value.
It is obvious that E/� is always equal to an integer value.
Schematically, it can be presented as shown in fig.4. The output error generator
(OEG) generates the �(k), comparing target (T) and actual (A) values (see equation
(7)).
3.2 The Learning Hardware
Computation of synaptic weight values is included within the chip by the digital and
analog circuit blocks, and the synaptic weight values are stored in the digital memory.
Before the illustration of the hardware implementation details, a technical particularity should be taken into consideration. The problem is that from the engineering
point of view, it is very difficult to provide the exact analog value equal to the target
one because of the technological mismatch of the semiconductor components. In order
to overcome that problem, the following is recommended to take into consideration:
U t arg et �

U� � U�
2

�U= U � U�
+

(8)

(9)

where U , U� are the bounds of higher and lower voltage levels, between which the
actual output value of QAFN is assumed equal to the target one. Of course, �U should
+

362

K. Nazaryan

be as small as possible considering the hardware implementation properties, in order
to reach the ideal case. It is obvious that �U and unit weight should have the same
order of their values.
+
Thus, learning target value is given by the analog values of U and U� considering
that the �U difference should be the same for all desired output values and input com+
+
binations. A voltage level-shifting circuit block (controlled by only U ) to generate U
�
and U values can solve that problem.
The hardware of QAFN learning algorithm is illustrated in fig.5. A simple OEG is
designed to generate up/down signals. A couple of the voltage comparators generate a
digital voltage code comparing the target and actual analog values. That code is then
used in the up/down signal generation during clock signal appearance. The corresponding digital weight (for which cj=1) is increased/decreased by the up/down
counter, accordingly. Up/Down signal generation is shown in Table 1. Each value of
the weight word bits is stored in a memory cell and is allowed to renew due to
EEPROM (electrically erasable-programmable read only memory) technique [15, 16].
To refresh the weights, an access (EN - enable) to the triggers of the corresponding
counter, is required. The operational modes of the neuron are presented in Table 2.

“1”
c1

w1
w2
w3
:
w�

AND c2
c3

X

:

c�

PI

w0

fQAFN = S

�

� Output
Error
Generator

Learning
Algorithm

A
T

X
Fig. 4. Schematic representation of QAFN learning
Table 1. Up/Down signal generation

Clk

Uactual, Utarget

cj

up

down

�
�
�
�

-

0
1
1
1

0
�
0
0

0
0
�
0

�

Uactual < U
+
Uactual > U
+
U�<Uactual<U

Learning
Algorithm
item 3
item 1
item 2
item 3

Quasi Analog Formal Neuron and Its Learning Algorithm Hardware

363

Table 2. The neuron operational modes
CS

Terminal
OE
UPR

Weight
storage

Counter (cnt.)
access to
Trigger
trigger inputs outputs
W or ki ng M ode
Roff
Training Mode
On
Roff

C om m e nt s

1

1

5v

read

1

1

5v

read

0

0

5v

Roff

Off

cnt.

0

1

-

Off

cnt.

0

0

18v
pulse
18v
pulse

Previous weight setting in
counter
Learning algorithm realization
Previous weight erasing

write

Off

cnt.

New weight storing

4 Conclusions
The neuron suggested in this paper could be used in complex digital-analog computational systems, in digital filters, data analyzers, controllers, complex Boolean and
quasi – analog functioning, digital – analog signal processing systems, etc., where high
speed and adaptability are crucial. Weight discretization is the most reliable way of
eliminating leakage problems associated with, for instance, capacitive analog storage.
Using static digital storage is a convenient solution, it is, however, quite area consuming. Boolean full-functionality and wide functional class for quasi – analog functions are provided by only one QAFN, without network construction, which is one of
the most important advantages of the neuron model. A convenient hardware is designed for the neuron learning algorithm performance. The fabrication mismatches of
the semiconductor components are taken into consideration, designing a convenient
output error generator, based on some engineering approaches. Since the weight correcting value is fixed compatibly with the QAFN hardware and the learning algorithm
is performed for only one neuron, the learning rate is relatively high, which depends
only on clock signal frequency and possibilities of hardware implementation limitations. The hardware approach of the QAFN learning algorithm implementation provides a substantial high speed for the learning process. Reprogrammability of the
weights is realized due to EEPROM technique. The CMOS QAFN and its learning
algorithm are suitable to implement in VLSI technology.

364

K. Nazaryan

Upr
OE
CS

Utarget= (U++ U�)/2
�U= U+� U�
EEPROM EEPROM
Cell
Cell
(0)
(1)
(wj )
(wj )

EEPROM
Cell
( B�1 )
( wj
)

�-th floating gate
weight storage

1-st floating gate
weight storage

U+

w1

Input/Output Control

(0)

wj

fQAFN

(1)

wj

wj

:

�1

20 21 22 ... 2(B )
trigger outputs

( B�1)
wj

j-th counter
EN up down

trigger inputs
�1
20 21 22 ... 2(B )

1-st counter
up
down

QAFN

clock

&

&

&

&
OEG

c�

cj

� +

� +

j-th floating gate
weight storage

c1

U�

... w�

up
down

�-th counter

Fig. 5. Schematic of QAFN learning algorithm hardware with analog/digital circuit blocks and
digital weight storage.

References
1.

Mkrtichyan, S.: Computer Logical Devices Design on the Neural Elements. Moscow:
Energia (1977), (in Russian).
2. Mkrtichyan, S.: Neurons and Neural Nets. Moscow: Energia, (1971) (in Russian).
3. Mkrtichyan, S., Mkrtichyan, A.S., Lazaryan, A.F., Nazaryan, K.M. and others. Binary
Neurotriggers for Digital Neurocomputers. Proc of NEUREL 97, Belgrade, (1997) 34-36.
4. Mead, C.: Analogue VLSI Implementation of Neural Systems. Kluwer, Boston (1989).
5. Haykin, S.: Neural Networks. A Comprehensive Foundation. Macmillan, New York,
(1994)
6. Chua, L., and Yang, L.: Cellular Neural Networks: Theory. IEEE Trans. Circuits and
Systems, vol 35, N 5, (1988). 1257-1290.
7. Mkrtichyan, S., and Nazaryan, K.: Synthesis of Quasi Analog Formal Neuron. Report of
Science Academy of Armenia, N3, Yerevan: (1997). 262-266 (in Armenian).
8. Nazaryan, K.: Research and Design of Formal Neurons (MSc thesis). State Engineering
University of Armenia, Yerevan (1997) (in Armenian).
9. Nazaryan, K..: Circuitry Realizations of Neural Networks (PhD thesis). State Engineering
University of Armenia, Yerevan (1999) (in Armenian).
10. Nazaryan, K.: Circuit Realization of a Formal Neuron: Quasi Analog Formal Neuron. In
Proc of NEUREL 97. Belgrade, (1997) 94-98.

Quasi Analog Formal Neuron and Its Learning Algorithm Hardware

365

11. Nazaryan, K.,: The Quasi Analog Formal Neuron Implementation Using a Lateral Weight
Transistor. Proc of NEURAP 98. Marseilles, (1998). 205-209.
12. Mkrtichyan, S., Lazaryan, A.: Learning Algorithm of Neuron and Its Hardware Implementation. In Proc of NEUREL 97. Belgrade, (1997) 37-42.
13. Mkrtichyan, S., Navasardyan, A., Lazaryan, A., Nazaryan, K. and others.: Adaptive
Threshold Element. Patent AR#472, Yerevan, (1997). (in Armenian).
14. Tu, J. and Gonzalev, R.: Pattern Recognition Principles, Mir, Moscow (1978). (in Russian).
15. Holler, M., Tam, S., Castro, H., Benson, R.: An Electrically Trainable Artificial Neural
Network (ETANN) with 10240 Floating Gate Synapses. In Proc of IEEE/INNS Int. Joint
Conf. of Neural Networks 2, (1989) 191-196.
16. Lee, B., Yang, H., Sheu, B.: Analog Floating-Gate Synapses for General-Purpose VLSI
Neural Computation. IEEE Trans. on Circuit and Systems 38, (1991) 654-658.

