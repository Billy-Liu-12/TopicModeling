Orthogonal method for linear systems.
Preconditioning
Henar Herrero1, Enrique Castillo2 and Rosa Eva Pruneda1
1

Departamento de Matemáticas, Universidad de Castilla-La Mancha,
13071 Ciudad Real, Spain.
2
Departamento de Matemáticas, ETSI de Caminos
Universidad de Cantabria, Santander, Spain.

Abstract. We discuss the complexity of an orthogonal method to solve
linear systems of equations. One of the advantages of the orthogonal
method is that if we modify one equation of a linear system, we can obtain
the new solution with a few number of operations using the solution of the
initial system. We compare this advantage in relation to other methods.
Based on this method we introduce a technique to reduce the condition
number.

1

Introduction

Castillo et al [1] have introduced a pivoting transformation to obtain the orthogonal of a given vectorial subspace. This method is applied to solve a long list of
problems in linear algebra including systems of linear equations. Nowadays interest is focussed on iterative methods [2], [3], [4], [5] because they are more suitable
for large systems. But, they present some difficulties such as conditioning, and
for some problems a direct method can be more useful. The direct methods
arising from this transformation have complexity identical to that associated
with the Gaussian elimination method (see Castillo et al [1]). However, they are
specially suitable for updating solutions when changes in rows, columns, or variables are done. In fact, when changing a row, column or variable, a single step
of the process allows obtaining (updating) the new solution, whithout the need
of starting again from scratch. Therefore a drastic reduction in computational
effort is obtained.
The paper is structured as follows. In the second section we introduce the
pivoting transformation. In the third one this transformation is applied to the
resolution of a linear system of equations. In the fourth one we study the main
advantage of the method, i.e. the updating of solutions. In the fifth section we
study the complexity of the method related to its updating facilities. In the
last one we introduce an strategy to reduce the condition number based on this
orthogonal procedure.

2

Pivoting Transformation

The main tool to be used in this paper consists of the so called pivoting transformation, which transforms a set of vectors V j = {v1j , . . . , vnj } into another set
of vectors Vj+1 = {v1j+1 , . . . , vnj+1 } by
 j j

if k = j,
 vk /tj ;
tjk j
(1)
vkj+1 =
j

 vk − j vj ; if k 6= j,
tj

where tjj 6= 0 and tjk ; k 6= j are arbitrary real numbers. In what follows we
consider that the vectors above are the columns of a matrix V j .
This transformation
can
i be formulated in matrix form as follows. Given a
h
matrix Vj = v1j , . . . , vnj , where vij , i = 1, . . . , n, are column vectors, a new
matrix Vj+1 is defined via

Vj+1 = Vj M−1
j ,

(2)

where M−1
is the inverse of the matrix
j
T

Mj = (e1 , . . . , ej−1 , tj , ej+1 , . . . , en ) ,

(3)

where ei is the ith column of the identity matrix, the transpose of tj being
defined by
tTj = aTj Vj
(4)
for some predestined vector aj .
Since tjj 6= 0, the matrix Mj is invertible. It can be proved that M−1
is the
j
identity matrix with its jth row replaced by
t∗j =

1 

tjj


−tj1 , . . . , −tjj−1 , 1, −tjj+1 , . . . , −tjn .

This transformation is used in well-known methods, such as the Gaussian
elimination method. However, different selections of the t-values lead to completely different results. In this paper we base this selection on the concept of
orthogonality, and assume a sequence of m transformations associated with a
set of vectors {a1 , . . . , am }.
We can resume the main properties of this pivoting transformation (see [6])
as follows:
1. Given a matrix V, the pivoting transformation transforms its columns without changing the linear subspace they generate, i.e., L(V j ) ≡ L(Vj+1 ).
2. The pivoting process (2) with the pivoting strategy (4) leads to the orthogonal decomposition of the linear subspace generated by the columns of V j

with respect to vector aj . Let aj 6= 0 be a vector and let tjk = aTj vkj ; k =
1, . . . , n. If tjj 6= 0, then
aTj Vj+1 = eTj .
(5)
In addition, the linear subspace orthogonal to aj in L(Vj ) is


j+1
j+1
{v ∈ L(Vj )|aTj v = 0} = L v1j+1 , . . . , vj−1
, vj+1
, . . . , vnj+1 ,

and its complement is L(vjj+1 ). In other words, the transformation (2) gives
the generators of the linear subspace orthogonal to aj and the generators of
its complement.
3. Let L{a1 , . . . , an } be a linear subspace. We can sequentially use pivoting
transformation (2) to obtain the orthogonal set to L{a1 , . . . , an } in a given
subspace L(V1 ). Let tji be the dot product of aj and vij . Then assuming,
without loss of generality, that tjj 6= 0, we obtain
!
tj1 j
tjn j
j
j
j
j
L(V ) = L v1 − j vq , . . . , vq , . . . , vn − j vq
tq
tq


j+1
j+1
j+1
= L v1 , . . . , v n
= L(V ),
and



j+1
L(a1 , . . . , aj )⊥ ≡ {v ∈ L(V1 )|aT1 v = 0, . . . , aTj v = 0} = L vj+1
, . . . , vnj+1 .
In addition, we have

aTr vkj+1 = δrk ; ∀r ≤ j; ∀j,

(6)

where δrk are the Kronecker deltas.
4. The linear subspace orthogonal to the linear subspace generated by vector a j
is the linear space generated by the columns of Vk ; for any k ≥ j + 1 with
the exception of its pivot column, and its complement is the linear space
generated by this pivot column of Vk .
5. The linear subspace, in the linear subspace generated by the columns of V 1 ,
orthogonal to the linear subspace generated by any subset W = {ak |k ∈ K}
is the linear subspace generated by the columns of V` , ` > maxk∈K with
the exception of all pivot columns associated with the vectors in W , and
its complement is the linear subspace generated by the columns of V ` , ` >
maxk∈K which are their pivot columns.
In the following theorem we prove how the orthogonal transformation can be
used, not only to detect when a vector is a linear combination of previous vectors
used in the pivoting process, but to obtain the coefficients of such combination.
Theorem 1 (Linear combination). Let L{a1 , . . . , an } be a linear subspace.
Applying sequentially pivoting transformation (2), if tjk = 0 for all k = j, . . . , n,
then the aj vector is linear combination of previous vectors used in the process,
aj = ρ1 a1 + . . . + ρq−1 aj−1 ,

(7)

where ρi = tji = aj • vij , and vij is the corresponding column pivot associated to
the vector ai , for all i = 1, . . . , j − 1.
Proof. If tjk = 0 for all k = j, . . . , n,
aj ∈ L{a1 , . . . , aj−1 }⊥⊥ ≡ L{a1 , . . . , aj−1 },

(8)

then ∃ ρ1 , . . . , ρj−1 , sucht that aj = ρ1 a1 + . . . + ρj−1 aj−1 . For i = 1, . . . , n, we
calculate the dot product
aj • vij = (ρ1 a1 + . . . + ρq−1 aq−1 ) • vij = ρ1 (a1 • vij ) + . . . + ρj−1 (aj−1 • vij ) (9)
and using property 4 of Section 2, ai • vij = 1 and ak • vij = 0 for all k 6= i, k =
1, . . . , j − 1, we obtain aj • vij = ρi .

3

Solving a linear system of equations

Consider now the complete system of linear equations Ax = b:
a11 x1 +a12 x2 + · · · +a1n xn = b1
a21 x1 +a22 x2 + · · · +a2n xn = b2
···
···
···
···
···
am1 x1 +am2 x2 + · · · +amn xn = bm

(10)

Adding the artificial variable xn+1 , it can be written as:
a11 x1
a21 x1
···
am1 x1
am1 x1

+a12 x2
+a22 x2
···
+am2 x2
+am2 x2

+ · · · +a1n xn
+ · · · +a2n xn
···
···
+ · · · +amn xn
+ · · · +amn xn

−b1 xn+1 = 0
−b2 xn+1 = 0
··· ···
−bm xn+1 = 0
−bm xn+1 = 0

(11)

xn+1 = 1

(12)

System (11) can be written as:
(a11 , · · · , a1n , −b1 )(x1 , · · · , xn , xn+1 )T = 0
(a21 , · · · , a2n , −b2 )(x1 , · · · , xn , xn+1 )T = 0
·································
···
(am1 , · · · , amn , −bm )(x1 , · · · , xn , xn+1 )T = 0

(13)

Expression (13) shows that (x1 , . . . , xn , xn+1 )T is orthogonal to the set of
vectors:
{(a11 , . . . , a1n , −b1 )T , (a21 , . . . , a2n , −b2 )T , . . . , (am1 , . . . , amn , −bm )T }.
Then, it is clear that the solution of (11), is the orthogonal complement of
the linear subspace generated by the rows of matrix Ā (= (A| − b), i.e. the
column -b is added to the matrix A):
L{(a11 , . . . , a1n , −b1 )T , (a21 , . . . , a2n , −b2 )T . . . , (am1 , . . . , amn , −bm )T }⊥

Thus, the solution of (10) is the projection on IRn of the intersection of the
orthogonal complement of the linear subspace generated by
{(a11 , . . . , a1n , −b1 )T , (a21 , . . . , a2n , −b2 )T , . . . , (am1 , . . . , amn , −bm )T }.
and the set {x|xn+1 = 1}.
To solve
 system1 (13)
	 we apply1 the orthogonal algorithm to the A rows with
V1 ≡ L v11 , . . . , vn+1
where vi = ei ; i = 1, . . . , n + 1, and ei is the vector
with all zeroes except for the ith component, which is one.
If we consider the j−th equation of system (11), i.e., aj x = bj , after pivoting
with the corresponding associated vector, we can obtain the solution of this
equation, Xj ,

	
Xj ≡ x ∈ Vk |xn+1 = 1 ∧ aTj • x = 0, k = j + 1, . . . m .
(14)
In fact, we consider the solution generated by the columns of the corresponding
table except the used as pivot (see Section 2, property 5), and we impose the
condition xn+1 = 1.
In each step of the computational process we must deal with one of the
following situations:
1. The new equation aTj •x = 0 is a linear combination of the previous equations:
This occurs when
aTj • vij = 0, i = 1, . . . , n + 1.

(15)

2. The system of equations is incompatible: This occurs when
j
6= 0.
aTj • vij = 0, ∀i = 1, . . . , n ∧ aTj • vn+1

(16)

In this case xn+1 = 0 6= 1 and the system is incompatible.
3. The new equation is not a linear combination of the previous equations but
is compatible with them:
aTj • vij = tji 6= 0, for some i 6= n + 1.

(17)

In this case we realize a new pivotation.
	

After this sequential process we have Xm ≡ L v1m , . . . , vnmm , and then the
solution of the initial system becomes
	

(18)
v̂nmm + L v̂1m , . . . , v̂nmm −1 ,

where v̂ is the vector obtained from v by removing its last component (see table
2 for an example).

When L v̂1m , . . . , v̂nmm −1 degenerates to the empty set, we have uniqueness
of solution. For this to occur, we must have m = n and |A| 6= 0; that is, the
coefficients matrix must be a square nonsingular matrix.
If we are interested in to obtain the solution of any subsystem, we will take
the intersections of the corresponding solutions of each equation. Note that we
have to keep the orthogonal and the complement set in each step in order to
apply this process.

4

Modifying equations

Once the redundant equations have been detected in a system of linear equations,
which is easy using the orthogonal method to solve the system (see Theorem 1),
we can suppose without lost of generality, a linear system not redundant. In this
section we show how to update the solution of this system next to modify one
equation, with only one extra iteration of the orthogonal method.
We consider a not redundant linear system Ax = b. We perform the orthogonal transformation to solve it. Now, we modify the jth equation. To obtain the
new solution we need the ortogonal subspace corresponding to the new equation
instead of the old equation. Like the orthogonal method can be used with any
space V, we take the last table of the process to solve the original system and
we introduce the modified equation to make the extra pivoting transformation.
Using as pivot the column pivot associated with the row changed, and taking
into account properties 1 and 4 from Section 2, we will obtain the new solution.

5

Complexity

Castillo et al [1] studied the number of operations of the orthogonal method to
solve linear system of equations. The conclusion of this study is that when we
use this method to obtain the final solution of a linear system of equations, we
remove in each step the complement space, and the number of operations is the
same that in the Gaussian elimination method, that is, around 2n3 /3 operations
(see [7]). We want in this section to study the number of operations of the
complete process, keeping the complement space, in order to update solutions
after modifying an equation, and to compare this results with de Gauss process.
The number of operations required to solve a linear system of equations with
the Gaussian elimination method (see [8]) is 2n3 /3 + (9n2 − 7n)/6, with the
orthogonal method it is (2n3 − 5n + 6)/3 (see Ref.[1]). The number of operations
of the orthogonal method keeping the complement space is 2n3 − n.
If we modify an equation, we need to realize an extra iteration to update a
solution, the number of necessary operations in this case is 4n2 − 2n.
If we want to make several modifications in the original system and to obtain
the new solution for each modification, with the Gaussian elimination method
we have to perform the complet process each time. With the orthogonal method
we need to make the complete process once (2n3 − n operations) and one update
for each modification (4n2 − 2n operations). The number of operations that we
are comparing, when the size of the system is n and the number of updates is k,
are
2n3 − n + [k(4n2 − 2n)]
for the orthogonal method, and
k[(2/3)n3 + (9n2 − 7n)/6]
for the Gaussian elimination method.

Table 1. Required number of products and sums for making an extra iteration of the
orthogonal method.

Extra iteration
Products or divisions

Sums

Dot Products

n2

n(n − 1)

Normalization

n

–

Pivoting

n(n − 1)

n(n − 1)

Total

2n2

2(n2 − n)

From these formulas we can conclude that for k > 3 the number of operations
required for the orthogonal method is smaller and it is worthwhile to use it
as compared with the Gaussian elimination method. This fact can be useful
for nonlinear problems solved by means of iterations on linear approaches, i.e.
Newton methods.

6

Reducing the condition number

In this section we propose a way to obtain an equivalent system of linear equations with very reduced condition number for a full matrix. This kind of matrices
appears very often when using spectral methods for solving partial differential
equations. The idea uses the procedure explained in section 3 for solving a linear
system. It is based on the guess observed in several cases that a matrix is ill conditioned when several of the rows of the matrix A are in very close hyperplanes,
i.e. when tg(ai ,ˆaj )  1. Therefore the solution is to rotate one of the vectors,
but keeping it into the same hyperplane L(āi , āj ).
As an example, using the orthogonal method for the following system:
0.832x1 + 0.448x2 = 1,
0.784x1 + 0.421x2 = 0.

(19)

L(v11 , v21 ) = L(a1 )⊥ ; in table 2 Iteration 2 we observe that a2 is almost orthogonal to one of the generators of L(a1 )⊥ and this is the source of ill conditioning
(K(A) = 1755).
We propose the substitution of a2 by a π/2 rotation of a2 , ag2 = gπ/2 (a2 )
such that (gπ/2 (a2 ), −b2 ) ∈ L(ā1 , ā2 ). In this way the calculation of L(ā1 , ā2 )⊥
is equivalent to that of L(ā1 , āg2 )⊥ .
To calculate āg2 we solve a linear system āg2 = αā1 + βā2 .

Table 2. Iterations for solving Eq. (19). Pivot columns are boldfaced.
Iteration 1

Iteration 2

a1

v1

v2

v3

a2

v11

v21

v31

0.832

1

0

0

0.784

1

-0.538

-1.202

0.448

0

1

0

0.421

0

1

0

–1

0

0

1

0

0

0

1

0.832

0.448

–1

0.784

–7.92e-004

–0.942

Output
v12

v22

v32

1

-0.538

-438.542

0

1

816.667

0

0

1

Table 3. Condition number of the matrices equivalent to the Hilbert matrix of size
n = 10 rotated in the rows from m to n.
m

9

8

7

6

5

4

3

2

K 1010 7 · 108 4 · 107 2 · 106 6 · 104 2 · 103 337 315

If we apply this technique to the system (19), the new matrix is


0.8320
0.4480




−0.2261 0.4210
and K(A) = 1.9775, the condition number has been drastically reduced.
In the case of a system n × n to be sure that all the vectors are in different
hyperplanes we proceed as follows:
Starting by i = n − 1 till i = 1, we compare (aii , aii+1 ) with (aji , aji+1 ),
j = i + 1, ..., n, for the close cases (tg(ai ,ˆaj )  1) the corresponding vector
(aji , aji+1 ) is rotated to (−a2ji+1 /aji , aji+1 ) in such a way that the subspaces in
IRn+1 are kept. In this way we can afirm that any two row vectors of the new
matrix are in very different hyperplanes and the condition number is reduced. In

table 3 this fact can be observed: K(A) = 1013 for the starting Hilbert matrix,
by increasing the number of rows where this technique is applied, the condition
number is reduced till K = 314 when all the rows are involved.

References
1. Castillo, E., Cobo, A., Jubete, F., Pruneda, R.E.: Orthogonal Sets and Polar
Methods in Linear Algebra: Applications to Matrix Calculations, Systems of
Equations and Inequalities, and Linear Programming. John Wiley and Sons, New
York (1999)
2. Golub, G.H., van Loan, C.F.: Matrix Computations. Johns Hopkins University
Press, London (1996)
3. Axelsson, O.: Iterative Solution Methods. Cambridge University Press, Cambridge
(1996)
4. Kelley, C.T.: Iterative Methods for Linear and Nonlinear Equations. SIAM, Phyladelphia (1995)
5. Duff, I.S., Watson, G.A. (eds): The state of the art in numerical analysis. Oxford
University Press, Oxford (1997)
6. Castillo, E., Cobo, A., Pruneda, R.E. and Castillo, C.: An Orthogonally-Based
Pivoting Transformation of Matrices and Some Applications. SIAM Journal on
Matrix Analysis and Applications 22 (2000) 666-681
7. Atkinson, K.E.: An Introduction to Numerical Analysis. Jonhn Wiley and sons,
New York (1978)
8. Infante del Rı́o, J.A., Rey Cabezas, J.M.: Métodos numéricos. Teorı́a, problemas
y prácticas con Matlab. Pirámide, Madrid (1999)

