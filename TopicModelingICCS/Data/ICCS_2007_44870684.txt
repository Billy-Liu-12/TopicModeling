Type-Based Query Expansion for Sentence Retrieval
Keke Cai, Chun Chen, Jiajun Bu, and Guang Qiu
College of Computer Science, Zhejiang University
Hangzhou, 310027, China
{caikeke, chenc, bjj, qiuguang}@zju.edu.cn

Abstract. In this paper, a novel sentence retrieval model with type-based
expansion is proposed. In this retrieval model, sentences expected to be relevant
should meet with the requirements both in query terms and query types. To
obtain the information about query types, this paper proposes a solution based
on classification, which utilizes the potential associations between terms and
information types to obtain the optimized classification results. Inspired by the
idea that relevant sentences always tend to occur nearby, this paper further reranks each sentence by considering the relevance of its adjacent sentences. The
proposed retrieval model has been compared with other traditional retrieval
models and experiment results indicate its significant improvements in retrieval
effectiveness.
Keywords: Sentence retrieval, query type identification, query expansion.

1 Introduction
Sentence retrieval is to retrieve query-relevant sentences in response to users’ queries.
It has been widely applied in many traditional applications, such as passage retrieval
[1], document summarization [2], question answering [3], novelty detection [4] and
content-based retrieval presentation [5]. A lot of different approaches have been
proposed for sentence retrieval. Most of them, however, have not been proven
efficient enough. The main reason is due to the limited information expressed in
sentences. To improve the performance of sentence retrieval, besides the key words in
queries and sentences, additional features that are helpful for indicating sentences’
relevance should be explored.
Query type, which expresses relevant information satisfying users’ information
need, has been effectively used in some applications involving sentence retrieval,
such as question-answering that looks for sentences containing the expected type of
answer, and novelty detection where sentences involving information of special type
will be considered more relevant. However, little efforts have been made to
incorporate such information into the process of keyword-based information retrieval,
where the most difficulty is the identification of query-relevant information types.
This paper proposes a new sentence retrieval model, in which the information
about query types is explored and incorporated into the retrieval process. The idea is
similar to that of query expansion. The difference is that this model expands each
query with the relevant information types instead of the concreted terms. In this paper,
Y. Shi et al. (Eds.): ICCS 2007, Part I, LNCS 4487, pp. 684 – 691, 2007.
© Springer-Verlag Berlin Heidelberg 2007

Type-Based Query Expansion for Sentence Retrieval

685

query types are defined as the types of the expected information entities, such as
persons, locations, numbers, dates, times and etc, which are considered necessary for
satisfying user’s request or information need. Therefore, in the retrieval process,
sentences expected to be relevant should meet with the requirements both in query
terms and query types. To achieve such a retrieval model, the most important factor is
the identification of query types. This paper proposes a solution based on
classification. This classification model makes a full use of the theory of information
association, with the purpose to utilize the potential associations between terms and
information types to obtain the optimized classification results. In addition to term
and type information described above, another type of information is also considered
in the evaluation of sentence relevance, that is, the proximity between sentences. The
idea underneath is that relevant sentences always tend to occur nearby. Then, each
sentence is further re-ranked by considering the relevance of its adjacent sentences.
The remainder of the paper is structured as follows: Section 2 introduces the
related studies in sentence retrieval. Section 3 describes the proposed sentence
retrieval model and the classification approach for query type identification. In
Section 4, the experimental results are presented. Section 5 concludes the paper.

2 Related Works
Most exiting approaches for sentence retrieval are based on term matching between
query and sentence. They are essentially the applications of algorithms designed for
document retrieval [6] [7] [8]. However, compared with documents, sentences are
much smaller. Thus, the performance of typical document retrieval systems on the
retrieval of sentences is significantly worse. Some systems try to utilize linguistic or
other features of sentences to facilitate the detection of relevant sentences. In the
study of [5], factors used for ranking sentences include the position of sentence in the
source document, the words contained in sentence and the number of query terms
contained in sentence. In [9], semantic and lexical features are extracted from the
initial retrieved sentences to filter out possible non-relevant sentences.
In addition to the mining of features in sentences, some systems concentrate on the
studies of features in queries. One of the most significant features about queries is the
query type. In most cases, query type is defined as the entity types of the expected
answer. For example, in [4], queries are described as patterns that include both query
words and the required answer types. Then, these patterns are used to retrieve
sentences. Sentences without the expected type of named entities will be considered
irrelevant. In the domain of question answering, query type is also an important factor
for sentence relevance evaluation. Given a question, the question is analyzed for their
expected answer type and then submitted to retrieve sentences that contain the key
words from the question or the tokens or phrase that are consistent with the expected
answer type of the question. Studies have shown the positive effects of query type on
sentence retrieval, which however, in the context of keyword-based retrieval, has not
been fully utilized. The most difficulty is the identification of query types, which
becomes one of the focuses of our studies in this paper.

686

K. Cai et al.

3 Sentence Retrieval Model Involved with Query Type
The proposed sentence retrieval model with type-based query expansion measures
sentence relevance from two perspectives: lexical similarity and type similarity.
Lexical similarity is an indicator of the degree to which query and sentence discuss
the same topic. It is always evaluated according to term matching between query and
sentence. Given two terms, their similarity can be viewed from different point of
views, such as, synonymy, variant, co-occurrence or others. Since this paper expects
to pay more attention to the application of query type, we adopt the most basic
definition for lexical similarity. If two terms are exactly the same, they are lexical
similarity. Type similarity is actually to evaluate the coincidence between the query
types and information types related to sentence. From sentence perspective, the
related information types are defined as the types of entities contained in sentence and
can be identified by using the existing named entity recognition technique. However,
from query perspective, the identification of query types is a little more difficult. In
this paper, this problem is solved by a solution based on classification.
3.1 Information Association Based Classification
Inspired by the theory of Hyperspace Analogue to Language (HAL) [10], a novel
classification approach is proposed to solve the problem of query type identification.
In this approach, a special type of information association is explored, with the
purpose of reflecting the dependencies between terms and information types. When
such kind of associations is incorporated into query classification, information types
that have the most probabilities to be associated with a query can be identified. The
implementation of this classification is based on the information model reflecting the
Associations between Terms and Information Types (ATIT).
Construction of the ATIT Model. The construction of the ATIT model consists of
two steps.
The first step is to construct the HAL model given the large document corpus. The
HAL model is constructed in the traditional way (See [10] for more details). Let T =
{t1, t2 … tn} be the term set of the document corpus, the constructed HAL is finally
represented as a n*n matrix MHAL, in which each term of T is described as a ndimension term vector Vi HAL = {MHAL(i,1), …, MHAL(i,n)}, where MHAL(i, j) describes
the association strength between the terms ti and tj.
The second step realizes the construction of ATIT model based on HAL model. In
the ATIT model, each term ti is expected to be described as a m-dimension vector
Vi ATIT = {MATIT(i,1), …, MATIT(i,m)}, where MATIT(i,j) represents the association
strength between the term ti and the information type cj. The construction of ATIT can
be further divided into three sub-steps: Firstly, entity type related to each term ti is
discovered. In this paper, it is realized by named entity recognition (NER) [11].
However, it is noted that not all entities of all types can be well identified. To solve
this problem, the manual NER approach is adopted, which is to use human generated
entity annotation results to realize a part of these entity recognitions. Secondly, based
on the association information provided by HAL, the association strengths between

Type-Based Query Expansion for Sentence Retrieval

687

terms and information types are calculated. Let terms of information type cj be Tc j =
{ tc j 1 , …, tc j k }, the association strength between term ti and cj is calculated as:
c jk

∑M

M ATIT (i, j) =

HAL ( i , p )

.

(1)

p =c j 1

Implementation of Classification. The ATIT information model makes it possible to
identify the relationships between query terms and information types. Then, the
probability of query Q being relevant to information type cj can be evaluated by:
P (c j | Q ) =

∑ P(c j | ti ) P(ti | Q)

.

ti∈Q

(2)

where P(ti | Q) means the probabilities of ti in Q. Since that queries are normally short,
each P(ti | Q) can be approximately assigned an equal value, i.e., P(ti | Q) = 1/|Q|,
where |Q| is the number of terms in the query. The probability P(cj | ti ) represents the
association strength of the information type cj with respect to the term ti. According to
Bayesian formula, it can be transformed into:
P(c j | ti ) =

P (t i | c j ) * P ( c j )

.

P (t i )

(3)

where P(cj) and P(qi) are respectively the prior probability of category cj and query
term qi. Here, we set them to be constants. P(ti | cj) represents the conditional

probability of ti. Based on the previous constructed ATIT model, it is defined as:
P(qi | c j ) =

M ATIT (i, j )
n

∑ M ATIT (i, j )

.

(4)

i =1

Probability of query Q being relevant to information type cj can be evaluated by:
rank

P (c j | Q ) =

∑

ti∈Q

M ATIT (i, j )
n

∑ M ATIT (i, j)

.

(5)

i =1

3.2 Relevance Ranking of Sentence
Experiments in [12] show that although there is no significant difference in the
performance of traditional retrieval models when implemented in sentence retrieval,
TFIDF technique performs consistently better than the others across different query

688

K. Cai et al.

sets. Thus, in this paper, we decided to use the vector space model (VSM) with tf.idf
weighting as the retrieval framework. The retrieval model is formulated as:
sim( S , Q) = λ

∑WS ,l *WQ,l + (1 − λ ) ∑WS ,t *WQ,t

l∈LS ∧ LQ

t∈TS ∧TQ

.

(6)

where, the parameter λ is used to control the influence of each component to sentence
retrieval. Let A be the sentence S or the query Q, then LA and TA respectively represent
the term vector and the type vector with respect to A. WA,l denotes the weight of term l
in A and can be defined as log(Ltfl+1)*log(N /Ldfl+1), where Ltfl is the frequency of
term l in A, N is the total number of sentences and Ldfl is the number of sentences
containing the term l. WS,,t denotes the weight of the information type t in sentence S
and can be defined as log(Ttft+1)*log(N/Tdft+1), where Ttft is the frequency of entities
in S with type t and Tdft is the number of sentences containing entities of type t. WQ,,t
denotes the weight of the information type t in query Q. It is the normalized
probability of each query type and defined as:
WQ ,t =

P (t | Q )

∑ p(t | Q)

.

(7)

t∈TQ

In our relevant sentence retrieval model, a sentence that contains not only query
terms but also the entities of the expected information types will be considered more
relevant. This is the main difference between our type-based expansion retrieval
model and other word-based approaches. Since this method makes an estimation of
the most possible query context by query relevant information types, the more
accurate relevance judgment is expected. However, it is noted that in most cases this
approach is more effective for long sentences than short sentences. To solve this
problem, further optimization is considered. Conclusions in [9] show that query
relevant information always appear in several continuous sentences. Inspired by this
idea, this paper proposes to re-rank the sentence according to the following rules.
For sentence Si that has information of the expected types, but contain few query
terms, if the sentence Sj that is before or after Si has many terms in common with the
query, but do not contain the expected information types, the ranks of sentences Si and
Sj will be improved by: sim’(Si, Q) = sim(Si, Q)+α*sim(Sj, Q), sim’(Sj, Q) = sim(Sj,
Q)+β*sim(Si, Q), where 0≤α, β≤1 and sim(Si, Q) and sim(Sj, Q) are the initial
relevance values of Si and Sj evaluated by formula 6.

4 Experiments
We use the collections of the TREC Novelty Track 2003 and 2004 for the evaluation
of our proposed retrieval method. To validate its applicability to keyword-based
retrieval, we select the TREC topic titles to formulate the queries.
The first part of experiments is to verify the potential associations between query
and information of certain types. Table 1 gives the statistical results. In this table, the
second and third row shows the distribution of entity information in relevant
sentences. Signification information about information entities has been discovered in

Type-Based Query Expansion for Sentence Retrieval

689

most relevant sentences. It implies its large potentialities in sentence retrieval. In
Table 1, P(N) represents the probability of queries concerned with N types of entities.
It is discovered that most queries involve one or two different types of information
entities. This further approves the assumption of the underlying associations between
query and information types, which, when considered carefully, will improve the
effectiveness of sentence retrieval.
Table 1. Statistical information about entities and entity types in relevant sentences

TREC 2003
# of relevant sentences 311.14
per query
# of entities per relevant 1.703
sentence
P(N)
N=0 N=1
18% 38%

TREC 2004
166.86
1.935
N=2
30%

N>2
14%

N=0
32%

N=1
30%

N=2
26%

N>2
12%

Another part of experiments is to compare the classification performance of our
proposed information association based approach (IA) with word pattern based
approach (WP) applied in [4] and the machine learning classification approach (ML),
which in this paper is support vector machine approach. We select the data obtained at
the website (http://l2r.cs.uiuc.edu/~cogcomp/Data/QA/QC/) as the training and test
data. Since these data are in the form of question, to make them applicable to our
classification case, we have transformed them to be the form of keyword query. These
formed queries contain the kernel terms in questions excepting the interrogative
terms. Table 2 respectively shows the experimental results measured by macro
precision, macro recall and macro F1-measure. As shown in Table 2, word pattern
based approach has a higher classification precision, but a lower recall. It on one hand
shows the relative accuracy of the defined word pattern and on the other hand its
limited coverage. Compared with the machine learning, our proposed approach has a
similar classification precision, but achieves a higher recall. In our experiments, we
discovered that some valued relationships between term and information type have
not been identified as expected. It is for that reason that a large amount of needed
entities information cannot be properly recognized by the applied entity recognition
technique. The study of entity recognition technique is out of the scope of this paper.
However, it is believed that with the perfection of entity recognition technique,
information association based classification approach can achieve better performance.
The purpose of the results shown in Tables 3 is to compare the performance of the
proposed sentence retrieval model with type-based query expansion (TPQE) to other
three retrieval approaches, including TFIDF model (TFIDF), BIR model (BIR), KLdivergence model (KLD) and the traditional term-based expansion sentence retrieval
model (TQE). Statistical analysis of the retrieval results shows that in the context of
sentence retrieval the application of traditional document retrieval algorithm is
actually to detect the existence of query terms in sentence or not. As shown in Table 3
there is no significant difference in the performances of these traditional retrieval
models. However, since most sentences have fewer words, TQE approach always add
noise to the sentence retrieval process. Sentences containing expansion terms may not

690

K. Cai et al.

be as relevant to the original query as sentences containing the original query terms.
Table 3 shows that performance of this approach. Comparatively speaking, our
proposed method considers query expansion from another perspective. It is helpful to
identify the most relevant information of query and therefore avoids the introduction
of large noise. As shown in Table 3, our proposed approach does do better than all
other approaches.
Table 2. Performances of different classification approaches

Macro precision
0.7230
0.6980
0.6895

WP
ML
IA

Macro recall
0.4418
0.5181
0.5903

Macro F1
0.4990
0.5778
0.6081

Table 3. Performance comparison in finding relevant sentences for 50 queries in TREC 2003 &
TREC 2004

Database
TFIDF
BIR
KLD
TQE
TPQE

50 queries in TREC 2003
0.349
0.292
0.330
0.304
0.385

50 queries in TREC 2004
0.228
0.178
0.2833
0.212
0.249

Table 4. Retrieval performance with or without consideration of proximity

Database
Methods
P@5
P@10
P@20
P@30
P@50
P@100

50 queries in TREC 2003
TPQE
TPQE_PX
0.6324
0.6989
0.6559
0.6860
0.5326
0.6639
0.5919
0.6400
0.5534
0.5896
0.5026
0.5438

50 queries in TREC 2004
TPQE
TPQE_PX
0.3920
0.4380
0.3875
0.4200
0.3704
0.4020
0.3680
0.4160
0.3542
0.3976
0.3185
0.3413

With the hypothesis that relevant sentences always exist in close proximity to each
other, we further propose the proximity-based optimization method for re-ranking the
sentences. This optimization scheme focuses on the distributions of query terms and
query types involved in the proximity sentences, with the hope to reveal some
relevant sentences, which when work together, can provide the integrated information
satisfying user’s information need. Table 4 illustrates the experimental results, where
P@n means the precision at the top n ranked documents. As shown in this table 4,
retrieval with consideration of sentence proximity achieves clear improvement in
retrieval effectiveness. It further validates the relevancies among adjacent sentences.

Type-Based Query Expansion for Sentence Retrieval

691

5 Conclusion
Compared with the traditional sentence retrieval model, the features of this proposed
retrieval model include: it views the information of query type as a factor for
identifying sentences’ relevance; it re-ranks each sentence by considering the
relationships between adjacent sentences. The proposed retrieval model has been
compared with other traditional retrieval models. Experiment results indicate that it
produces significant improvements in retrieval effectiveness.

References
1. Salton G., Allan J., Buckley C.: Automatic structuring and retrieval of large text files.
Communication of the ACM, Vol. 37(2). (1994) 97-108
2. Daumé III H., Marcu D.: Bayesian query-focused summarization. In Proceedings of the
21st International Conference on Computational Linguistics and 44th Annual Meeting of the
Association for Computational Linguistics. Sydney, Australia (2006) 305–312
3. Li X.: Syntactic Features in Question Answering. In Proceedings of 26th Annual
International ACM SIGIR Conference on Research and Development in Information
Retrieval. Toronto, Canada. (2003) 383-38
4. Li X., Croft W.: Novelty detection based on sentence level patterns. In Proceedings of
2005 ACM CIKM International Conference on Information and Knowledge Management.
Bremen, Germany. (2005) 744-751
5. White R., Jose J., Ruthven I.: Using top-ranking sentences to facilitate effective
information access. Journal of the American Society for Information Science and
Technology. Vol. 56(10). (2005) 1113-1125
6. Larkey L., Allan J., Connell M., Bolivar A., Wade, C.: UMass at TREC 2002: Cross
Language and Novelty Tracks. In Proceedings of 11th Text REtrieval Conference.
Gaithersburg, Maryland. (2002) 721–732
7. Schiffman B.: Experiments in Novelty Detection at Columbia University. In Proceedings
of 11th Text REtrieval Conference. Gaithersburg, Maryland. (2002) 188-196
8. Zhang M., Lin C., Liu Y., Zhao L., Ma S.: THUIR at TREC 2003: Novelty, Robust and
Web. In Proceedings of 12th Text REtrieval Conference. Gaithersburg, Maryland. (2003)
556-567
9. Collins-Thompson K., Ogilvie P., Zhang Y., Callan J.: Information filtering, Novelty
Detection, and Named-Page Finding. In Proceedings of 11th Text REtrieval Conference.
Gaithersburg, Maryland. 107-118
10. Lund K., Burgess C. Producing High dimensional Semantic Spaces from Lexical Cooccurrence. Behavior Research Methods, Instruments, & Computers, Vol. 28, (1996) 203208
11. Andrew B. A Maximum Entropy Approach to Named Entity Recognition. Ph.D. thesis,
New York University, (1999)
12. Allan J., Wade C., Bolivar A.: Retrieval and Novelty Detection at the Sentence Level. In
Proceedings of 26th Annual International ACM SIGIR Conference on Research and
Development in Information Retrieval. Toronto, Canada. (2003) 314-321

