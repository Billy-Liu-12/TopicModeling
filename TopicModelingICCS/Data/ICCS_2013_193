Available online at www.sciencedirect.com

Procedia Computer Science 18 (2013) 1235 – 1244

International Conference on Computational Science, ICCS 2013

DD-OceanVar: a Domain Decomposition fully parallel Data
Assimilation software for the Mediterranean Forecasting System
Luisa D’Amorea,∗, Rossella Arcuccib , Luisa Carracciuoloc , Almerico Murlib,d
a University

of Naples, Federico II, Complesso Universitario Monte S.Angelo, 80126 Naples, ITALY
Euro-Mediterranean Center on Climate Change, via Augusto Imperatore, 16 - 73100 Lecce, ITALY
c Istituto di Chimica e Tecnologia di Polimeri, CNR, Via Campi Flegrei, 34, 80078 Naples, ITALY
d SPACI, Southern Partnership for Advanced Computational Infrastructures, c/o Complesso Universitario Monte Sant’Angelo, Via Cintia,
80126, Naples, ITALY
b CMCC,

Abstract
OceanVar is a Data Assimilation (DA) software which is being used in Italy within the Mediterranean Forecasting System
(MFS) to combine observational data (Sea level anomaly, sea-surface temperatures, etc.) with backgrounds produced by
computational models of ocean currents for the Mediterranean Sea (namely, the NEMO framework). OceanVAR is based
on a three-dimensional variational approach. We describe computational eﬀorts aimed to design a fully parallel OceanVar
software, based on Domain Decomposition (DD), which involves modiﬁcation of the variational scheme on each sub domain.
Our approach aims to face to the ever greater multi-level parallelism and scalability of the current and of the next generation
of leadership computing facility systems, while fulﬁlling the speciﬁc requirements of OceanVar within the MFS.
Keywords: Data Assimilation; Domain Decomposition; parallel software;

1. Introduction
The Mediterranean Forecasting System (MFS) is a daily 10-day forecast system in operational use since 1998,
and its ocean general circulation model (OGCM) is based on the Ocean Parallelise (OPA) code, which has subsequently been set up for the Mediterranean Sea (NEMO framework) [1]. Within the MFS, OceanVar is the
operational assimilation software involving a wide range of observational data, from satellite-observed sea level
anomalies (SLAs) and sea-surface temperatures (SSTs), to temperature and salinity proﬁles from expendable
bathytermographs (XBTs, ) and Argo-ﬂoat proﬁlers. The heat ﬂux, on the other hand, is corrected by relaxing the
modeled surface-layer temperatures towards the satellite-observed SST data.
This work describes the computational eﬀorts towards the design of a fully parallel OceanVAR software to be
coupled with NEMO within the MFS. The parallel approach is based on Domain Decomposition (DD) involving
the physical domain, the regularization functional operator and the vector solution. The software will be therefore
named as DD-OceanVar (Domain Decomposition OceanVar).
We show a simulated test case aimed to validate the eﬀectiveness of the proposed approach both in terms of the
numerical results and in terms of execution time. We are currently working on the employment of DD-OceanVar
into the real conﬁguration of the NEMO model within the MFS.
∗ Corresponding

author. Tel.: +39-081-675625 ; fax: +39-081-7662106.
E-mail address: luisa.damore@unina.it.

1877-0509 © 2013 The Authors. Published by Elsevier B.V. Open access under CC BY-NC-ND license.
Selection and peer review under responsibility of the organizers of the 2013 International Conference on Computational Science
doi:10.1016/j.procs.2013.05.290

1236

Luisa D’Amore et al. / Procedia Computer Science 18 (2013) 1235 – 1244

2. The domain decomposition approach
To implement a Data Assimilation algorithm in oceanography, it is natural to think about Domain Decomposition
(DD) approach: each sub domain of the decomposition could be associated to a geographical area which could
have its own observational and computational resources.
In NEMO, the DD approach agrees with the physical characteristics of the ocean model: the 3D domain is laid out
on local processor memories following a 2D horizontal topological splitting. Further, each sub domain computes
its own surface and bottom boundary conditions and it has a side wall overlapping interface which deﬁnes the
lateral boundary conditions for computations in the inner sub domain. The overlapping area consists of the two
rows at each edge of the sub domain. After a computation, a communication phase starts: each processor sends
to its neighboring processors the updated values of the points corresponding to the interior overlapping area of its
neighboring sub domain (i.e. the innermost of the two overlapping rows). The communication is done through
message passing.
In DD-OceanVar we use this DD approach and the same overlapping area for the exchange of information on
boundary conditions. These information, which are obtained on the edges, are crucial to solve the problem of
data assimilation on sub domains as explained in the next section. About observations, in [2] are described three
conceivable strategies for their spatial distribution. In DD-OceanVar we use, at this stage, one of these strategies
named ”model distribution”. Moreover, our idea is to reformulate the DA regularization functional according to
the physical domain decomposition. In this way we obtain a set of DA-subproblems, each one deﬁned in a sub
domain, that should be solved concurrently.
2.1. DD-OceanVAR computational model
Let tk , k = 0, 1, . . . , n be a sequence of observation times and, for each k, let x Mk ≡ x M (tk ) ∈
denoting the state of the Mediterranean sea system at time tk , where:

N

be the vector

xk = [T, S , η, u, v] ,
and T is the three-dimensional temperature ﬁeld, S the three-dimensional salinity ﬁeld, η the two-dimensional
free surface elevation, and u,v are the total horizontal velocity components and where with we denote the vector
transposed.
At each time step tk , let:
yk = Hk (xk )
be the observations (Hk : N → p is the non-linear operator collecting the observations at time tk [7]). The
computational model of OceanVAR is the following non-linear least square problem [3]:
xDA (tk ) = argmin xk J(xk )

(1)

where
J(xk ) = Hk (xk ) − yk

2
R

+ xk − x Mk

2
B

(2)

xDA is the so-called analysis (i.e. the estimation of the vector xk at time tk ), R and B are the covariance matrices,
whose elements provide the estimate of the errors on yk and on x Mk , respectively.
Let us consider the following overlapping decomposition of the physical domain Ω:
N

Ω=

Ωi

(3)

i=1

such that Ωi ∩ Ω j = Ωi j 0 if Ωi and Ω j are adjacent. According to this decomposition we consider J(x) onto
the sub domain Ωi , i.e. Ji (x). The new issue induced by DD is to join the solution on the overlapping domains
correctly. For that, we add a term to the cost function Ji (·) to enforce coupling between the domains [4]. The
computational model of DD-OceanVar on Ωi is:

1237

Luisa D’Amore et al. / Procedia Computer Science 18 (2013) 1235 – 1244

xDAi (tk ) = argmin x Ji (xk )

(4)

where
Ji (xk ) = Hki (xki ) − yki

2
Ri

+ xki − xMki

2
Bi

+ xki /Ωi j − xk j /Ωi j

2
Bij

(5)

where xDAi , xMi , yi , xi , Ri , Bi , Hi (xi ) are the quantities deﬁned in (1) on Ωi and xi /Ωi j , x j /Ωi j ,Bij are the restriction
on Ωi j of the quantities deﬁned in (1).
Let
x˜ DAi =

xDAi
0

on
on

Ωi
Ω − Ωi

(6)

be the vector extension of xDAi on Ω, we deﬁne the DD-analysis as:
n
DD
xDA
=

x˜ DAi

(7)

i=1

So, let d = [yk − H(xk )] be the misﬁt, by using the following linearization of H:
H(x) = H(z) + H(x − z)
where H is the matrix obtained by the ﬁrst order approximation of the Jacobian of H and, by setting vi = ViT δxi ,
the preconditioned (see [5]) cost function is:
1 T
1
1
+
− T
+
−
vi vi + (Hi Vi vi − di )T R−1
i (Hi Vi vi − di ) + (Vi j vi − Vi j vi ) (Vi j vi − Vi j vi )
2
2
2
where v+i and v−i are shown in Figure 1.
Ji (vi ) =

(8)

Fig. 1. quantities v+i and v−i

On each subdomain of Ω, the operator Ji (∀i = 1, ...), is minimized using the L-BFGS method (Figure 2).

2.2. DD-OceanVAR algorithm
Parallel algorithm performs, on each sub domain Ωi , the DA computations needed to obtain xDAi : at the end
of each iteration the exchange of boundary conditions is performed. The parallel algorithm running on each
processor is described in Algorithm 1.
The main aspect of this algorithm is that it requires only few modiﬁcations on the existing sequential algorithm
(see [6]). These modiﬁcations consist in changing the cost function Ji (·) and adding the exchange of the boundary
conditions at each step of minimization process.
So, the basic idea of the DD-OceanVAR software is to split the discretization mesh into several smaller meshes
(see Figure 3) and solve the DA problem by addressing independent local problems. Each processor has its own

1238

Luisa D’Amore et al. / Procedia Computer Science 18 (2013) 1235 – 1244

Fig. 2. DD-OceanVAR solutions computed solving concurrently the minimizing problem of each Ji with i = (1, 1), (1, 2), (2, 1), ... deﬁned in
subdomains Ωi

Algorithm 1 The DD-OceanVAR scheme
1: Acquire the observations yi and the model vector xMi
2: Deﬁne the operators Hi
3: Compute di ← yi − Hi xMi
4: Estimate the operators Ri and Bi
5: Compute the matrix Vi from Bi
6: Deﬁne the initial value of xDAi
7: Compute viopt ← V+
i xDAi
8: repeat
9:
Send and Receive the boundary conditions from the adjacent domains needed during Ji (v) evaluation
10:
Compute Jiopt ← Ji (viopt )
11:
Compute gradJi opt ← ∇Ji (viopt )
12:
Compute new values for viopt by the L-BFGS steps from Jiopt and gradJi opt
13: until (Convergence on the viopt values are obtained)
14: Compute xDAi ← xMi + Vi viopt

local memory and solves the model equation over a sub domain. The sub domain boundary conditions are deﬁned
by data exchange between processors which is exploited by explicit statements of the message passing paradigm
(see point 9 of Algorithm 1).

Fig. 3. 2D domain decomposition distribution of the discretization mesh

1239

Luisa D’Amore et al. / Procedia Computer Science 18 (2013) 1235 – 1244

From the modeler’s point of view, the algorithm running on each processor/sub domain is almost identical to the
”mono-processor/domain” code.
Let us denote the processors number as nproc = p × q. We assume a uniform decomposition along the (i, j)-axis,
that is the i-axis is divided by p and the j-axis by q. Hence, if the size of Ω is n x × ny × nz , then the size of sub
domains are nloc x , nlocy , nlocz where:
nx
+ 2o x
nloc x =
(9)
p
ny
(10)
nlocy =
+ 2oy
q
nlocz = nz

(11)

These dimensions include the internal domain and the overlapping rows.

Fig. 4. sub-domains with overlapping

By using n xpp and nypp to denote the position of the (1, 1) grid-point of each sub domain in the global domain,
each element of the local array xloc corresponds to the element of the global array xglob , as following:
xglob (i + n xpp − 1; j + nypp − 1; k) = xloc (i; j; k)

(12)

where 1 < i < nloc x , 1 < j < nlocy , and 1 < k < nlocz .
About observations, we distribute them according to the ”model distribution”. Sub domains are associated with
diﬀerent geographic regions. We carry out a check on the geographical location of the observed data to attribute
the observations to the processor related to the geographical region in which the measurement was made.
It is worth noting that in a more general framework, DD approach should be applied to each sub domain, in such
a way each sub domain is processed in parallel and so on. Furthermore, each mono-domain code could be a
multi-level parallel code (see [5]) instead of the sequential original code. By this way, we refer to fully parallel
implementation, too.
3. Validation of the DD approach: a case study
In order to validate the proposed DD approach we described some results related to the quality of the numerical
results and in terms of reduction in computation time 1 . All considerations are made on the basis of a case study
1 All tests are performed on an AMD Opteron 6278@2.4GHz based system, with 512 GB of RAM memory, available in the context of the
SCoPE Computing Infrastructure at the University of Naples Federico II.

1240

Luisa D’Amore et al. / Procedia Computer Science 18 (2013) 1235 – 1244

Table 1. Estimation of the magnitude of the elements of e Mk for diﬀerent values of n.

n
64
128

htrue − h Mk ∞
4.999763e-03
4.999910e-03

utrue − u Mk ∞
4.999357e-03
4.999468e-03

vtrue − v Mk ∞
4.999357e-03
4.999505e-03

based on a shallow water model as a simpliﬁed version of NEMO forecasting one: for that reason the shallow
water model can be considered a consistent tool to get a proof of concept of the validity of the approach.
The shallow model equations are derived from the principles of conservation of mass and conservation of momentum. The independent variables are the time, t, and two space coordinates, x and y. The dependent variables are
the ﬂuid height or depth, h, and the two-dimensional ﬂuid velocity ﬁeld, u and v. With the proper choice of units,
the conserved quantities are mass, which is proportional to h, and momentum, which is proportional to u and v.
We assume that n x = ny = n and nz = 3, so the problem size is N = n2 ∗ 3. Finally, the time step used for the
temporal discretization of the model is dt = 0.01.
The domain decomposition used is coherent with the software NEMO where, ﬁxed p and q, the local dimensions
of sub domains (considering overlapping) are deﬁned as in (9) and (10).
We use as reference values the solution to the forecasting model denoted by xtruek , where k = 0, 1, ..., n counts the
time step tk :
xtruek = [htrue , utrue , vtrue ] ,
For each time step, the operator H deﬁned in (4) is assumed to be a piecewise linear interpolation operator whose
coeﬃcients are computed using the points of model domain nearest the observation values.
The standard deviation of the observation error is generally assumed equal to 0.5 [6]. So, the covariance matrix R
is a diagonal matrix with elements equals to 0.5.
Let
x Mk = [h Mk , u Mk , v Mk ] ,
be the vector containing the numerical solution of the forecasting model at time tk and the error vector e Mk =
|xtruek − x Mk | (see Table 1 for an estimation of the magnitude of the elements of e Mk ).
Then the covariance matrix B is given by the relationship:
B = e Mk eTMk .
To compute the matrix V in (8) we apply the TSVD (Truncated SVD) to the matrix B. So, we get
1

1

1

1

B = UdU T = Ud 2 d 2 U T = (Ud 2 )(Ud 2 )T
1

and by posing V = Ud 2 we get V such that

B = VV T .

Observation values are randomly chosen among the values of x Mk .
Table 2 shows the quantities htrue − hComp ∞ , utrue − uComp ∞ and vtrue − vComp
hComp = h Mk + dh;

uComp = u Mk + du;

∞

such that

vComp = v Mk + dv

where dh, du and dv are obtained by DA performed starting from h Mk , u Mk and v Mk .
We meant evaluate how well the DA solution [h, u, v]Comp approximates the model solution [h, u, v]true . We observe that the results performed by using DD-OceanVar are consistent with those shown in Table 1 i.e. all the values of dh, du and dv have the same magnitude of the error between the model solution and the numerical model solution. Moreover the DA solution become more and more accurate (i.e. the values of [h, u, v]true − [h, u, v]Comp ∞
decrease) as the number of sub domains increases (i.e. the dimension of each sub domain decreases).

1241

Luisa D’Amore et al. / Procedia Computer Science 18 (2013) 1235 – 1244

Table 2. Values of htrue − hComp

∞

, utrue − uComp

∞

e vtrue − vComp

∞

for diﬀerent values of nproc and of n.

n
64

nproc = p × q
nproc = 1, p = 1, q = 1
nproc = 2, p = 2, q = 1
nproc = 4, p = 2, q = 2
nproc = 8, p = 4, q = 2

htrue − hComp ∞
5.778788e-03
5.455278e-03
5.239452e-03
5.120223e-03

utrue − uComp ∞
5.778319e-03
5.454835e-03
5.243863e-03
5.127610e-03

vtrue − vComp ∞
5.778319e-03
5.454488e-03
5.244196e-03
5.127936e-03

128

nproc = 1, p = 1, q = 1
nproc = 2, p = 2, q = 1
nproc = 4, p = 2, q = 2
nproc = 8, p = 4, q = 2

−
6.442963e-03
5.850961e-03
5.484419e-03

−
6.442394e-03
5.850444e-03
5.472367e-03

−
6.442441e-03
5.850183e-03
5.478658e-03

Table 3 shows the execution time required for the calculation of the quantities dh, du e dv. Times include the
deﬁnition and the evaluation, on each sub domain Ωi , of the functionals Ji . In order to evaluate the performance
of the parallel algorithm we state the following:
Proposition The scale-up factor of the described DD approach is S nproc = nproc2
Proof: The most expensive computation of Algorithm 1 is the TSVD needed to obtain Vi , requiring O(N 3 ) ﬂops,
TS VD
(Nloc ) for performing the TSVD on each sub domain
on a problem of size N. Hence, the computation time T loc
ny
nx
of size Nloc = nz × p × q is:
⎞
⎛
⎜⎜⎜
ny 3 ⎟⎟⎟
n
x
TS VD
×
T loc (Nloc ) = O ⎜⎜⎝ nz ×
⎟⎟ .
p
q ⎠
DD
In the DD approach we perform nproc TSVD on problems of size Nloc , hence the total computation time T nproc
should be:
⎞
⎛
3
⎜⎜⎜
n x ny ⎟⎟⎟⎟
DD
⎜
×
T nproc = O ⎜⎝ p × q nz ×
⎟.
p
q ⎠

or else:
DD
T nproc

⎛
⎜⎜⎜ nz × n x × ny
⎜
= O ⎜⎜⎜⎜
⎝ (p × q)2

3⎞

⎛
⎟⎟⎟
⎜⎜⎜ nz × n x × ny
⎟⎟⎟
⎜
⎟⎟⎠ = O ⎜⎜⎜⎜⎝
(nproc)2

3⎞

⎟⎟⎟
⎟⎟⎟
⎟⎟⎠ .

Than the scale-up factor, of the parallel algorithm, computed respect to nproc1 is:
DD
=
S nproc,nproc
1

DD
T nproc
1
DD
T nproc

=

nproc2
nproc21
♣

In Table 3 we observe that as nproc increases the execution time scales as O(nproc2 ), according to the scale-up
factor.
Finally, in Figure 5 a graphical representation of the quantities characterizing the solution is showed for n = 64.
4. Conclusions and future work
We proposed a Domain Decomposition based approach to Data Assimilation. The novel approach consists on
a new formulation of the three dimensional variational scheme on each subdomain. Our idea is to “decompose”

1242

Luisa D’Amore et al. / Procedia Computer Science 18 (2013) 1235 – 1244

Table 3. Total execution time and scale-up factor for diﬀerent values of nproc and of n.

n
64

nproc = p × q
nproc = 1, p = 1, q = 1
nproc = 2, p = 2, q = 1
nproc = 4, p = 2, q = 2
nproc = 8, p = 4, q = 2

Total execution (secs) - T nproc
5.3888e+03
9.5397e+02
2.0800e+02
6.6651e+01

T 1 /T nproc
1.0
5.6
25.9
80.8

DD
S nproc,1
1
4
16
64

n
128

nproc = p × q
nproc = 1, p = 1, q = 1
nproc = 2, p = 2, q = 1
nproc = 4, p = 2, q = 2
nproc = 8, p = 4, q = 2

Total execution (secs) - T nproc
−
6.0008e+04
6.2254e+03
2.3894e+03

T 2 /T nproc
−
1.0
9.6
25.1

DD
S nproc,2
−
1
4
16

the DA functional according to the physical domain decomposition obtaining a set of DA-subproblems, each one
deﬁned in a sub domain, to be solved concurrently. We use a case study based on the shallow water model to
build a simulated case (a sort of “phantom experiment”) to verify the correctness of the solution. We are currently
working to the implementation of this approach in the actual conﬁguration of the NEMO model within the MFS.

1243

Luisa D’Amore et al. / Procedia Computer Science 18 (2013) 1235 – 1244

Fig. 5. Values of htrue , hComp , htrue − hComp
nproc = 4, p = 2, q = 2 and n = 64

∞

, utrue , uComp , utrue − uComp

∞

, vtrue , vComputed e vtrue − vComp

∞

obtained by setting

1244

Luisa D’Amore et al. / Procedia Computer Science 18 (2013) 1235 – 1244

References
[1] The NEMO System Home Page - http://www.nemo-ocean.eu
[2] J. Rantakokko, Strategies for parallel variational data assimilation - Parallel Computing 23, 1997, pp. 2017-2039 - Elsevier
[3] S. Dobricic, N. Pinardi, An oceanographic three-dimensional variational data assimilation scheme - Ocean Modelling 22, 2008, pp.
89-105, Elsevier
[4] Y. Tremolet, F. X. Le Dimet, Parallel algorithms for variational data assimilation and coupling models - Parallel Computing 22, 1996,
pp. 657-674, Elsevier
[5] L. D’Amore, R. Arcucci, L. Marcellino and A. Murli, HPC computation issues of the incremental 3D variational data assimilation scheme
in OceanVar software - Journal of Numerical Analysis, Industrial and Applied Mathematics vol. 7, no. 3-4, 2012, pp. 91-105 .
[6] N. Gustafsson, X. Huang, X. Yang, K. Mogensen, M. Lindskog, O. Vignes, T. Wilhelmsso and S. Thorsteinsson, Four-dimensional
variational data assimilation for a limited area model - Tellus - vol. 24, 2012
[7] L.D’Amore, R.Arcucci, L.Marcellino, A.Murli - A Parallel Three-dimensional Variational Data Assimilation Scheme - Numerical Analysis and Applied Mathematics, AIP C.P. 1389, 1829-1831

