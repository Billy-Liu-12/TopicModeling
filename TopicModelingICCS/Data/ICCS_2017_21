Available online at www.sciencedirect.com

ScienceDirect

This space is reserved for the Procedia header, do not use it
This Procedia
space isComputer
reserved
for 108C
the Procedia
header, do not use it
Science
(2017) 2049–2058
This space is reserved for the Procedia header, do not use it

International Conference on Computational Science, ICCS 2017, 12-14 June 2017,
Zurich, Switzerland

A Posterior Ensemble Kalman Filter Based On A Modified
A Posterior Ensemble Kalman Filter Based On A Modified
Cholesky
Decomposition
A Posterior Ensemble
Kalman
Filter Based On A Modified
Cholesky
Decomposition
1
2
Elias D. Nino-Ruiz
, Alfonso Decomposition
Mancilla11 , and Juan C. Calabria1,
Cholesky
1
1, 2
Elias D. Nino-Ruiz , Alfonso Mancilla , and Juan C. Calabria
1

Universidad del Norte, Barranquilla, ATL, Colombia

1
1
1
Elias D. Nino-Ruiz
, Alfonso
Mancilla
, and
Juan
C. Calabria1, 2
Universidad
del Norte,
Barranquilla,
ATL,
Colombia
{enino,
amancill,
calabriaj}@uninorte.edu.co
2
2
2

{enino, amancill,
calabriaj}@uninorte.edu.co
Universidad
Simon Bolivar,
Barranquilla, ATL, Colombia
1
Universidad
del Norte,
Barranquilla,
ATL,
Colombia
Universidad
Simon
Bolivar,
Barranquilla,
ATL,
Colombia
jcalabria@unisimonbolivar.edu.co
{enino,
amancill, calabriaj}@uninorte.edu.co
jcalabria@unisimonbolivar.edu.co
Universidad Simon Bolivar, Barranquilla, ATL, Colombia
jcalabria@unisimonbolivar.edu.co

Abstract
Abstract
In this paper, we propose a posterior ensemble Kalman filter (EnKF) based on a modified
In
this paper, we propose a posterior ensemble Kalman filter (EnKF) based on a modified
Cholesky
Abstractdecomposition. The main idea behind our approach is to estimate the moments of
Cholesky
decomposition.
The main
behind
approach
is to estimate
the moments
of
the
analysis
distribution
based
on anidea
ensemble
of our
model
realizations.
Thebased
method
as
In
this
paper,
we propose
a posterior
ensemble
Kalman
filter (EnKF)
on proceeds
a modified
the
analysis
distribution
based
on
an
ensemble
of
model
realizations.
The
method
proceeds
as
follows:
initially,
an estimate
the precision
background
error covariance
matrix
computed
Cholesky
decomposition.
Theof
idea behind
our approach
is to estimate
the is
moments
of
follows:
initially,
an estimate
ofmain
the precision
background
error
covariance
matrix
is
computed
via
a
modified
Cholesky
decomposition
and
then,
based
on
rank-one
updates,
the
Cholesky
the
analysis
distribution
based
on
an
ensemble
of
model
realizations.
The
method
proceeds
as
via
a modified
Cholesky
decomposition
and then, based
on
updates,
Cholesky
factors
ofinitially,
the inverse
background
covariance
matrixerror
arerank-one
updated
in matrix
order the
to
obtain
an
follows:
an estimate
of theerror
precision
background
covariance
is computed
factors
of
the
inverse
background
error
covariance
matrix
are
updated
in
order
to
obtain
an
estimate
of
the
inverse
analysis
covariance
matrix.
The
special
structure
of
the
Cholesky
factors
via
a modified
Cholesky
decomposition
and
then,The
based
on rank-one
updates,
the Cholesky
estimate
of
the
inverse
analysis
covariance
matrix.
special
structure
of
the
Cholesky
factors
can
be exploited
in order
to obtain aerror
matrix-free
implementation
of the EnKF.
Oncetothe
analysis
factors
of the inverse
background
covariance
matrix are updated
in order
obtain
an
can
be exploited
inisorder
to obtain
aposterior
matrix-free
implementation
of thecan
EnKF.
Once the
analysis
covariance
matrix
estimated,
the
mode
of
the
distribution
be
approximated
and
estimate
of
the
inverse
analysis
covariance
matrix.
The
special
structure
of
the
Cholesky
factors
covariance
matrix
is estimated,
the posterior
of the distribution
can
be approximated
and
samples
about
itinare
taken
in order
to buildmode
the
posterior
ensemble.
Experimental
are
can
be exploited
order
to obtain
a matrix-free
implementation
of the EnKF.
Once thetests
analysis
samples
about
it
are
taken
in
order
to
build
the
posterior
ensemble.
Experimental
tests
are
performed
making
use
of
the
Lorenz
96
model
in
order
to
assess
the
accuracy
of
the
proposed
covariance
matrix
isuse
estimated,
the posterior
mode
of thetodistribution
can be approximated
and
performed
making
of
the
Lorenz
96
model
in
order
assess
the
accuracy
of
the
proposed
implementation.
reveal
that,
the accuracy
of the proposed
similar
samples
about it The
are results
taken in
order
to build
the posterior
ensemble.implementation
Experimental is
tests
are
implementation.
The
results
reveal
that,
the
accuracy
of the filter
proposed
implementation
is
similar
to
that
of
the
well-known
local
ensemble
transform
Kalman
and
even
more,
the
use
of our
performed
making
use
of
the
Lorenz
96
model
in
order
to
assess
the
accuracy
of
the
proposed
to
that of the
well-known
localofensemble
transform
Kalman
filter
and even
the use of our
estimator
reduces
the results
impact
sampling
errors
during
assimilation
of more,
observations.
implementation.
that, the
accuracy
of the
the proposed
implementation
is similar
estimator reducesThe
the impactreveal
of sampling
errors
during
the
assimilation
of observations.
to2017
thatThe
of Authors.
the
well-known
local
ensemble
Kalman
filter
and even
more, the use of our
Keywords:
Ensemble
Kalman
Posterior
Ensemble,
Modified
Cholesky
Decomposition
©
Published
byFilter,
Elsevier
B.V. transform
Keywords:
Ensemble
Kalman
Posterior
Ensemble,
Modified
Cholesky
Decomposition
Peer-review
responsibility
of Filter,
theofscientific
committee
ofduring
the International
Conference
Computational Science
estimator under
reduces
the
impact
sampling
errors
the assimilation
ofonobservations.
Keywords: Ensemble Kalman Filter, Posterior Ensemble, Modified Cholesky Decomposition

1 Introduction
1 Introduction
To be concise, Data Assimilation is the process by which imperfect numerical forecasts are
1
To
beIntroduction
concise,
Datareal
Assimilation
is the process
by practice,
which imperfect
forecasts
are
adjusted
according
noisy observations
[1]. In
Gaussiannumerical
errors are
commonly

adjusted
according
real noisy
observations errors
[1]. Induring
practice,
Gaussian errors
are commonly
assumed
for
background
and observational
assimilation
of observations
[2].
To
be concise,
Data Assimilation
is the process
by
whichthe
imperfect
numerical
forecasts are
assumed
for
background
and
observational
errors
during
the
assimilation
of
observations
[2].
While
observational
errors
can
be
well-estimated
in
the
context
of
operational
data
assimilation,
adjusted
according errors
real noisy
observations
[1]. inInthepractice,
Gaussian
errors
areassimilation,
commonly
While
observational
can
be
well-estimated
context
of
operational
data
background
correlations
can be hard errors
to approximate,
owingoftoobservations
the size of the
assumed
for error
background
and observational
during themainly,
assimilation
[2].
background
error
correlations
can be
hard
to approximate,
mainly,
owing
to the
size offilter
the
vector
state
which,
typically,
ranges
in
the
order
of
millions
[3].
In
the
ensemble
Kalman
While
observational
errors
can
be
well-estimated
in
the
context
of
operational
data
assimilation,
vector
state
typically,
ranges
in the orderutilized
of millions
[3]. Intothe
ensemble Kalman
filter
(EnKF),
an which,
ensemble
of model
realizations
in order
estimate
of
background
error
correlations
can
be hard tois
approximate,
mainly,
owing tothe
themoments
size of the
(EnKF),
an
ensemble
of
model
realizations
is
utilized
in
order
to
estimate
the
moments
of
the
underlying
background
error
distribution
[4].
Since
the
ensemble
size
is
constrained
by
vector
state which,
typically,error
ranges
in the order
In the ensemble
Kalman filter
the underlying
background
distribution
[4].of millions
Since the[3].ensemble
size is constrained
by
(EnKF), an ensemble of model realizations is utilized in order to estimate the moments of
1
the underlying background error distribution [4]. Since the ensemble size is constrained by
1
1877-0509 © 2017 The Authors. Published by Elsevier B.V.
Peer-review under responsibility of the scientific committee of the International Conference on Computational Science
10.1016/j.procs.2017.05.062

1

2050	

Elias
D. Nino-Ruiz
et al. / Procedia
Computer Science 108C (2017)
A Posterior EnKF Based On
A Modified
Cholesky
Decomposition
Nino,2049–2058
Mancilla and Calabria

computational aspects, localization methods can be utilized in order to mitigate the impact
of sampling errors [5, 6]. Efficient formulations of the EnKF account for some sort of implicit
localization during the analysis step in order to damp out spurious correlations [7, 8, 9]. For
instance, in the EnKF based on a modified Cholesky decomposition [10], a band estimate of the
inverse background error covariance matrix can be obtained, on the fly, during the assimilation
of observations. Even more, this precision matrix can be expressed in terms of Cholesky factors
which are composed by a diagonal matrix and a band lower triangular matrix. We think that,
these factors can be updated in order to estimate the Cholesky factors of the inverse analysis
covariance matrix. This covariance matrix can be estimated by applying a series of rank-one
updates on the Cholesky factors of the inverse background error covariance matrix. With such
covariance matrix, samples from the posterior error distribution can be approximately taken
with low-computational efforts.
This paper is organized as follows: in section 2 efficient EnKF formulation are discussed,
section 3 presents the proposed EnkF implementation, in section 4 the accuracy of the proposed
EnKF is assessed and compared with well-known EnKF formulations and finally, conclusions
are stated in section 5.

2

Preliminaries

In this section, efficient EnKF implementations in order to avoid the impact of sampling errors
on the analysis innovations are discussed. Inflation aspects and other sources of misestimation
of model states and ensemble collapsing are well-studied in [11, 12].
In the ensemble Kalman filter, an ensemble of model realizations,


Xb = xb[1] , xb[2] , . . . , xb[N ] ∈ Rn×N ,
(1)

is utilized in order to estimate, the moments of the background error distribution,


x ∼ N xb , B ,

via the empirical moments of the ensemble (1), therefore,
xb ≈ xb =

N
1  b[i]
·
x ∈ Rn×1 ,
N i=1

(2a)

and
B ≈ Pb =

1
· ∆X · ∆XT ∈ Rn×n ,
N −1

(2b)

where n is the model dimension, N is the ensemble size, xb[i] ∈ Rn×1 is the i-th ensemble
member, for 1 ≤ i ≤ N , xb ∈ Rn×1 is well-known as the background state while B ∈ Rn×n
stands for background error covariance matrix, xb is the ensemble mean, and Pb is the ensemble
covariance matrix. Likewise, the matrix of member deviations ∆X ∈ Rn×N reads,
∆X = Xb − xb · 1TN .

(3)

When an observation y ∈ Rm×1 is available, the analysis ensemble can be computed as follows,
Xa = Xb + Z ∈ Rn×N ,
2

(4)

	

Elias
D. Nino-Ruiz
et al. / Procedia
Computer Science 108C (2017)
A Posterior EnKF Based On
A Modified
Cholesky
Decomposition
Nino,2049–2058
Mancilla and Calabria

where Z ∈ Rn×N can be obtained by the solution of the linear system of equations,

 −1
+ HT · R−1 · H · Z = HT · R−1 · ∆Y ,
Pb

(5)

H ∈ Rm×n is a linearized observational operator, R ∈ Rm×m is the estimated data-error
covariance matrix, the matrix of innovations on the observations ∆Y ∈ Rm×N reads,
∆Y = y · 1TN + E − H · Xb ,

(6)

and the columns of E ∈ Rm×N are samples from a zero-mean Normal distribution with covariance matrix R. The forecast is the approximated by propagating the ensemble (4) until new
observations are available,


b[i]
a[i]
(7)
xnext = Mtcurrent →tnext xcurrent , for 1 ≤ i ≤ N ,

where xa[i] ∈ Rn×1 denotes the i-th analysis member, and Mcurrent→next : Rn×1 → Rn×1 is
an imperfect numerical model (i.e., a model which mimics the behaviour of the ocean and/or
atmosphere).
In operational data assimilation, ensemble members can come at high computational efforts
[13] and therefore, the ensemble moments (2) are corrupted by sampling noise [14]. Hence,
localization methods are commonly utilized in the EnKF context in order to mitigate the impact
of sampling errors. One of the best EnKF implementations is the local ensemble transform
Kalman filter (LETKF) [15, 16]. In the LETKF, the analysis is approximated in the ensemble
space,
xa = xb + ∆X · aa ∈ Rn×1 ,

where,


aa = Q · VT · R−1 · y − H · xb ∈ RN ×1 ,

(8)

V = H · ∆X ∈ Rm×N , and an estimate of the analysis covariance matrix in such space reads,
−1

∈ RN ×N ,
Q = (N − 1) · I + VT · R−1 · V

with I ∈ RN ×N being the identity matrix in the ensemble space. This covariance matrix can
then be utilized in order to build an ensemble about the posterior model of the distribution.
In this context, localization methods are performed by using domain decomposition [17]: each
model component is surrounded by a local box of radius r and only local information (i.e.,
observed components) are utilized during the assimilation step.
In the ensemble Kalman filter based on a modified Cholesky decomposition (EnKF-MC)
[18] background error correlations are estimated via the Cholesky decomposition proposed by
Bicket and Levina in [19]. This provides an estimate of the inverse background error covariance
matrix of the form,
 −1 = LT · D · L ∈ Rn×n ,
B

(9)

where L ∈ Rn×n is an unitary lower-triangular matrix, and D ∈ Rn×n is a diagonal matrix.
 −1 , in addition, the
Even more, when only local effects are considered during the estimation of B
matrix L is sparse with only a few non-zero elements per row. Typically, the number of nonzero elements are some function of the radius of influence during the estimation of background
3

2051

2052	

A Posterior EnKF Based On
A Modified
Cholesky
Decomposition
Nino,2049–2058
Mancilla and Calabria
Elias
D. Nino-Ruiz
et al. / Procedia
Computer Science 108C (2017)

error correlations. For instance, in the one-dimensional case, the radius of influence denotes
the maximum number of non-zero elements, per row, in L. The EnkF-MC is then obtained by
replacing in the estimator (9) in (4). Given the structure of the Cholesky factors, the EnKF-MC
can be seen as a matrix-free implementation of the EnKF.
Recall that, the precision analysis covariance matrix reads,
A−1 = B−1 + HT · R−1 · H ∈ Rn×n .

(10)

and since HT · R−1 · H ∈ Rn×n can be written as a sum of m rank-one matrices, the factors
(9) can be updated in order to obtain an estimate of the inverse analysis covariance matrix. In
the next section, we propose an ensemble Kalman filter implementation based on this idea.

3

Proposed Method

Before we start, we make the assumptions [8, 20] that, in practice, the data error covariance
matrix R has a simple structure, the observation operator H is sparse and therefore, it can be
applied efficiently, and that the number of model components n is several times the ensemble
size N . We want to estimate the moments of the analysis distribution,
x ∼ N (xa , A) ,
based on the background ensemble (1), where xa is the analysis state and A ∈ Rn×n is the
analysis covariance matrix. Consider the estimate of the inverse background error covariance
matrix (9), the precision analysis covariance matrix (10) can be approximated as follows,
 −1 = B
 −1 + X · XT ,
A−1 ≈ A

(11)

where X = HT · R−1/2 ∈ Rn×m . The matrix (11) can be written as follows,
 −1 = LT · D · L +
A

m

i=1

xi · xTi ,

where xi denotes the i-th column of matrix X, for 1 ≤ i ≤ m. Consider the sequence of factors
updates
T

L(i) · D(i) · L(i)

=
=
=



L(i−1)

T

· D(i−1) · L(i−1) + xi · xTi

T 

L(i−1) · D(i−1) + pi · pTi · L(i−1)
T



L̃(i−1) · L(i−1) · D̃(i−1) · L̃(i−1) · L(i−1) ,



 −1 = L(0) T · D(0) · L(0) , and
where L(i−1) · pi = xi ∈ Rn×1 , for 1 ≤ i ≤ m, B

T
D(i−1) + pi · pTi = L̃(i) · D̃(i) · L̃(i) ∈ Rn×n .

4

(12)

	

A Posterior EnKF Based On
A Modified
Cholesky
Decomposition
Nino,2049–2058
Mancilla and Calabria
Elias
D. Nino-Ruiz
et al. / Procedia
Computer Science 108C (2017)

We can make use of the Dolittle’s method in order to compute the factors D̃(i) and L̃(i) in (12),
it is enough to note that,
 ˜
 

 ˜
1
0
0 ... 0
1 l21 ˜l31 . . . ˜ln1
d1 0 0 . . . 0
0 1 ˜l32 . . . 0   0 d˜2 0 . . . 0   ˜l21 1
0 . . . 0

 

 
0 0
˜ln2  ·  0 0 d˜3 . . . 0  ·  ˜l31 ˜l32 1 . . . 0
1
.
.
.

 

 
.
  .

 .
..
..
..
.. . .
..
.. . .
..
.
.
 ..




˜
. 0
. ln3
. 0
.
.
.
.
.
.
.
.
˜ln1 ˜ln2 ˜ln3 . . . 1
0 0
0 ... 1
0 0 0 . . . dn

 

 



=



L̃(i) T

p21

d1 +
 p2 · p1

 p3 · p1

 ..
 .
pn · p1


D̃(i)

p 1 · p2
d2 + p22
p 3 · p2
..
.

p1 · p 3
p2 · p 3
d3 + p23
..
.

...
...
...
..
.

p1 · pn
p2 · pn
p3 · pn
..
.

pn · p2

pn · p 3


...

dn + p2n

D(i) +pi ·pT
i



L̃(i)




.




After some math simplifications, the next equations are obtained,
d˜k = p2k + dk −
and

n


q=k+1

2
,
d˜q · ˜lqi



n

1
˜lkj =
· pk · pj −
d˜q · ˜lqi · ˜lqj  ,
d˜k
q=k+1

(13a)

(13b)

for 1 ≤ k ≤ n, and 1 ≤ j ≤ k − 1. The set of equations (13) can be used in order to derive
an algorithm for rank-one update of Cholesky factors, the updating process is shown in the
Algorithm 1.
Algorithm 1 Rank-one update for the factors L(i−1) and D(i−1) .
1:
2:
3:
4:
5:
6:
7:
8:
9:
10:
11:
12:

function Upd Cholesky factors(L(i−1) , D(i−1) , xi )
T
Compute pi from L(i) · pi = xi .
for k = n → 1 do
Compute d˜k via equation (13a).
Set lkk ← 1.
for j = 1 → k − 1 do
Compute ˜lkj according to (13b).
end for
end for
Set L(i) ← L̃(i−1) · L(i−1) and D(i) ← D̃(i) .
return L(i) , D(i)
end function

 −1 for all column vectors in X,
Algorithm 1 can be used in order to update the factors of B
this process is detailed in the Algorithm 2. Once the updating process has been performed, the
5

2053

2054	

A Posterior EnKF Based On
A Modified
Cholesky
Decomposition
Nino,2049–2058
Mancilla and Calabria
Elias
D. Nino-Ruiz
et al. / Procedia
Computer Science 108C (2017)

 −1 = L(m) T · D(m) · L(m) .
Algorithm 2 Computing the factors L(m) and D(m) of A
function Compute analysis factors(L(0) , D(0) , H, R)
Set X ← HT · R−1/2 .
for i = 1 → m do


Set [L(i) , D(i) ] ← Upd Cholesky factors L(i−1) , D(i−1) , xi
5:
end for
6:
return L(m) , D(m)
7: end function
1:
2:
3:
4:

resulting factors form an estimate of the inverse analysis covariance matrix,
T

 −1 = L(m) · D(m) · L(m) ∈ Rn×n .
A

(14a)

From this covariance matrix, the posterior mode of the distribution can be approximated as
follows,
xa = xb + z ∈ Rn×1 ,

(14b)

where


L(m)

T

· D(m) · L(m) · z = q ,

(14c)



with q = HT · R−1 · y − H · xb ∈ Rn×1 . Notice, the linear system (14c) involves lower and
upper triangular matrices and therefore, xa can be estimated without the needing of matrix
inversion. Once the posterior mode is computed, the analysis ensemble is built about it. Note
 reads,
that, A
−1 
−T
−1 

 = L(m)
· L(m)
,
· D(m)
A

 can be approximated as follows,
and therefore a square root of A
−1 
−1/2

 1/2 = L(m)
· D(m)
∈ Rn×n ,
A

(15)

which can be utilized in order to build the analysis ensemble,
Xa = xa · 1TN + ∆Xa ,

(16)

where ∆Xa ∈ Rn×N is given by the solution of the linear system,

1/2
L(m) · D(m)
· ∆Xa = W ∈ Rn×N ,

(17)

and the columns of W ∈ Rn×N are formed by samples from a multivariate standard normal
distribution. Again, since L(m) is lower triangular, the solution of (17) can be obtained readily.
6

	

A Posterior EnKF Based On
A Modified
Cholesky
Decomposition
Nino,2049–2058
Mancilla and Calabria
Elias
D. Nino-Ruiz
et al. / Procedia
Computer Science 108C (2017)

4

Experimental Results

In this section, we assess the accuracy of the P-EnKF and compare it against that of the LETFK
implementation proposed by Hunt in [16]. The numerical model is the Lorenz 96 model [21]
which mimics the behaviour of the atmosphere. This model is described by the next set of
ordinary differential equations:
dxk
= −xk−1 · (xk−2 − xk+1 ) − xk + F, for 1 ≤ k ≤ n ,
dt

(18)

where n = 40 is the number of model components and F is an external force. It is well-known
that when F equals 8.0, the Lorenz 96 model exhibits a chaotic behaviour which makes it
attractive as a toy problem for testing weather prediction methods [22]. The experimental
settings are described below:
• An initial random solution is propagated for a long time period in order to obtain a
reference solution dynamically consistent with the model (18).
• The initial background errors follows a Normal distribution N (0, σB · I). Three different
values for σB are considered during the numerical experiments σB ∈ {0.05, 0.10, 0.15}.
This perturbed state is propagated in time in order to make it consistent with the physics
and dynamics of the numerical model (18). A similar procedure is performed in order to
obtain a background state and an initial ensemble.
• The assimilation windows consists of 15 equidistant observations. The frequency of observations is 0.5 time units which represents 3.5 days in the atmosphere.
• The number of observed components is 50% the dimension of the vector state.
• Three ensemble sizes are tried during the experiments N ∈ {20, 40, 60}.
• As a measure of quality, the L-2 norm of the analysis state and the reference solution is
computed across assimilation steps.
• Varying the reference solution, 100 runs are performed for each pair (N, σB ).
The average of the error norms of each pair (N, σB ) for the LETKF and the P-EnKF
implementations are shown in the Table 1. As can be seen, in average across 100 of runs,
the performance of the proposed EnKF implementation outperforms that of the LETKF in
terms of L − 2 norm of the error. Even more, the P-EnKF seems to be invariant to the initial
background error σB since, in all cases, when the ensemble size is increased a better estimation
of the reference state x∗ at different observation times is obtained. This can also obey to the
estimation of background error correlations via the modified Cholesky decomposition [19] since
it is drsaticaly improved whenever the ensemble size is increased as is pointed out by Bickel
and Levina in [23]. In such case, the error decreases by O (log(n)/N ). This is crucial in the
P-EnKF formulation since estimates of the precision analysis covariance matrix are obtained
by rank-one updates on the inverse background error covariance matrix. On the other hand, in
the LETKF context, increasing the ensemble size can improve the accuracy of the method but,
that is not better than the one shown by the P-EnKF.
Some plots of the L-2 norm of error for the P-EnKF and the LETKF across different
configurations and runs can be seen in figure 1. Note that, the error of the P-EnKF decreases
aggressively since the earlier iterations. In the LETKF context, the accuracy is similar to that
of the P-EnKF only at the end of the assimilation window.
7

2055

Elias
D. Nino-Ruiz
et al. / Procedia
Computer Science 108C (2017)
A Posterior EnKF Based On
A Modified
Cholesky
Decomposition
Nino,2049–2058
Mancilla and Calabria

45

45

Background
Analysis

30

30
− X* ||

35

k

25

||X

k

25
20

20

15

15

10

10

5
0

2

4

6

8
10
Assimilation Step(k)

12

14

5
0

16

(a) LETKF σB = 0.05, N = 60
45

2

4

6

8
10
Assimilation Step(k)

12

14

16

(b) P-EnKF σB = 0.05, N = 60
45

Background
Analysis

40

Background
Analysis

40
35

30

30
− X* ||

35

k

k

||Xk − X* ||

Background
Analysis

40

35

k

||Xk − X* ||

40

25

||X

k

25
20

20

15

15

10

10

5
0

2

4

6

8
10
Assimilation Step(k)

12

14

5
0

16

45

2

4

6

8
10
Assimilation Step(k)

12

45

Background
Analysis

40

14

16

(d) P-EnKF σB = 0.10, N = 60

(c) LETKF σB = 0.10, N = 60

Background
Analysis

40
35

30

30
− X* ||

35

k

k

||Xk − X* ||

k

25

25

||X

2056	

20

20

15

15

10

10

5
0

2

4

6

8
10
Assimilation Step(k)

12

14

(e) LETKF σB = 0.15, N = 60

16

5
0

2

4

6

8
10
Assimilation Step(k)

12

14

16

(f) P-EnKF σB = 0.15, N = 60

Figure 1: L − 2 norm of the error for the LETKF and the P-EnKF implementations at different
observation times. For each configuration, 100 of runs are performed. The assimilation window
consists of 15 evenly distributed observations.
8

	

Elias
D. Nino-Ruiz
et al. / Procedia
Computer Science 108C (2017)
A Posterior EnKF Based On
A Modified
Cholesky
Decomposition
Nino,2049–2058
Mancilla and Calabria

σB
0.05

0.10

0,15

N
20
40
60
20
40
60
20
40
60

LETKF
22,6166
20,5671
20,0567
23,1742
20,9513
18,5048
24,8201
21,1314
20,8487

P-EnKF
21,2591
18,2548
17,8824
21,0725
18,3542
17,8240
20,9059
18,1731
17,7590

Table 1: Average of L − 2 norm of errors for 100 of runs of each configuration (σB , N ) for the
compared filter implementations.

Acknowledgement
This work was supported by the Applied Math and Computer Science Laboratory (AML-CS)
at Universidad del Norte.

5

Conclusions

We propose a posterior ensemble Kalman filter based on a modified Cholesky decomposition.
The proposed method estimates the posterior moments of the error distribution based on an
ensemble of model realizations. An estimate of the inverse background error covariance matrix
via a modified Cholesky decomposition is updated making use of rank-one matrices with information brought by the data error correlations in order to estimate the precision covariance
matrix. This matrix is utilized in order to compute the posterior mode of the error distribution
and then, samples are taken about it. This implementation is matrix-free making it attractive for practical implementations. Experimental settings are performed making use of the
Lorenz 96 model and different observations and ensemble configurations. The results obtained
by the proposed method are compared against those obtained by the local ensemble transform
Kalman filter (LETKF). The results reveal that, the use of the proposed implementation can
mitigate the impact of sampling errors and even more, the accuracy of the proposed EnKF
implementation is similar to that of the LETKF.

References
[1] Geir Evensen. The Ensemble Kalman Filter: Theoretical Formulation and Practical Implementation. Ocean Dynamics, 53(4):343–367, 2003.
[2] Geir Evensen. Data Assimilation: The Ensemble Kalman Filter. Springer-Verlag New York, Inc.,
Secaucus, NJ, USA, 2006.
[3] Milija Zupanski. Theoretical and Practical Issues of Ensemble Data Assimilation in Weather and
Climate. In SeonK. Park and Liang Xu, editors, Data Assimilation for Atmospheric, Oceanic and
Hydrologic Applications, pages 67–84. Springer Berlin Heidelberg, 2009.
[4] S. Gillijns, O.B. Mendoza, J. Chandrasekar, B. L R De Moor, D.S. Bernstein, and A Ridley. What
is the Ensemble Kalman Filter and How Well Does It Work? In American Control Conference,
2006, pages 6 pp.–, June 2006.

9

2057

2058	

A Posterior EnKF Based OnElias
A Modified
Cholesky
Decomposition
Nino,2049–2058
Mancilla and Calabria
D. Nino-Ruiz
et al. / Procedia
Computer Science 108C (2017)

[5] Poterjoy Jonathan, Zhang Fuqing, and Yonghui Weng. The Effects of Sampling Errors on the
EnKF Assimilation of Inner-Core Hurricane Observations. Monthly Weather Review, 142(4):1609–
1630, 2014.
[6] Jeffrey L. Anderson. Localization and Sampling Error Correction in Ensemble Kalman Filter Data
Assimilation. Monthly Weather Review, 140(7):2359–2371, 2012.
[7] Christian L. Keppenne. Data Assimilation into a Primitive-Equation Model with a Parallel Ensemble Kalman Filter. Monthly Weather Review, 128(6):1971–1981, 2000.
[8] Michael K. Tippett, Jeffrey L. Anderson, Craig H. Bishop, Thomas M. Hamill, and Jeffrey S.
Whitaker. Ensemble square root filters. Monthly Weather Review, 131(7):1485–1490, Jul 2003.
[9] EliasD. Nino Ruiz, Adrian Sandu, and Jeffrey Anderson. An Efficient Implementation of the
Ensemble Kalman Filter Based on an Iterative Sherman–Morrison Formula. Statistics and Computing, pages 1–17, 2014.
[10] Peter J. Bickel and Elizaveta Levina. Regularized estimation of large covariance matrices. Ann.
Statist., 36(1):199–227, 02 2008.
[11] Hong Li, Eugenia Kalnay, and Takemasa Miyoshi. Simultaneous estimation of covariance inflation and observation errors within an ensemble kalman filter. Quarterly Journal of the Royal
Meteorological Society, 135(639):523–533, 2009.
[12] Jeffrey L Anderson. An adaptive covariance inflation error correction algorithm for ensemble
filters. Tellus A, 59(2):210–224, 2007.
[13] P. F. J. Lermusiaux. Adaptive modeling, adaptive data assimilation and adaptive sampling.
Physica D Nonlinear Phenomena, 230:172–196, June 2007.
[14] Ahmed H. Elsheikh, Mary F. Wheeler, and Ibrahim Hoteit. An iterative stochastic ensemble
method for parameter estimation of subsurface flow models. Journal of Computational Physics,
242:696 – 714, 2013.
[15] Craig H. Bishop and Zoltan Toth. Ensemble Transformation and Adaptive Observations. Journal
of the Atmospheric Sciences, 56(11):1748–1765, 1999.
[16] Edward Ott, Brian R. Hunt, Istvan Szunyogh, Aleksey V. Zimin, Eric J. Kostelich, Matteo
Corazza, Eugenia Kalnay, D. J. Patil, and James A. Yorke. A local ensemble kalman filter for
atmospheric data assimilation. Tellus A, 56(5):415–428, 2004.
[17] Edward Ott, Brian Hunt, Istvan Szunyogh, Aleksey V Zimin, Eic J. Kostelich, Matteo Corazza,
Eugenia Kalnay, D. J. Patil, and James A. Yorke. A Local Ensemble Transform Kalman Filter
Data Assimilation System for the NCEP Global Model. Tellus A, 60(1):113–130, 2008.
[18] Elias D Nino-Ruiz, Adrian Sandu, and Xinwei Deng. A parallel ensemble kalman filter implementation based on modified cholesky decomposition. In Proceedings of the 6th Workshop on Latest
Advances in Scalable Algorithms for Large-Scale Systems, page 4. ACM, 2015.
[19] Peter J Bickel and Elizaveta Levina. Covariance regularization by thresholding. The Annals of
Statistics, pages 2577–2604, 2008.
[20] Jan Mandel and Jonathan D Beezley. Predictor-corrector and morphing ensemble filters for the
assimilation of sparse data into high-dimensional nonlinear systems. University of Colorado at
Denver and Health Sciences Center, Center for Computational Mathematics, 2006.
[21] Ibrahim Fatkullin and Eric Vanden-Eijnden. A computational strategy for multiscale systems with
applications to lorenz 96 model. Journal of Computational Physics, 200(2):605–638, 2004.
[22] Elana J Fertig, John Harlim, and Brian R Hunt. A comparative study of 4d-var and a 4d ensemble
kalman filter: Perfect model simulations with lorenz-96. Tellus A, 59(1):96–100, 2007.
[23] David R. Bickel and Marta Padilla. A Prior-free Framework of Coherent Inference and Its Derivation of Simple Shrinkage Estimators. Journal of Statistical Planning and Inference, 145(0):204–221,
2014.

10

