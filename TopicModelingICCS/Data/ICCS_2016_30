Procedia Computer Science
Volume 80, 2016, Pages 2231–2235
ICCS 2016. The International Conference on Computational
Science

Algorithmic Diﬀerentiation of Numerical Methods:
Second-Order Adjoint Solvers for Parameterized Systems of
Nonlinear Equations
Niloofar Saﬁran1 , Johannes Lotz1 , and Uwe Naumann1
LuFG Informatik 12: Software and Tools for Computational Engineering, RWTH Aachen, Germany
[safiran, lotz, naumann]@stce.rwth-aachen.de

Abstract
Adjoint mode algorithmic (also know as automatic) diﬀerentiation (AD) transforms implementations of multivariate vector functions as computer programs into ﬁrst-order adjoint code. Its
reapplication or combinations with tangent mode AD yields higher-order adjoint code. Second derivatives play an important role in nonlinear programming. For example, second-order
(Newton-type) nonlinear optimization methods promise faster convergence in the neighborhood
of the minimum through taking into account second derivative information. The adjoint mode
is of particular interest in large-scale gradient-based nonlinear optimization due to the independence of its computational cost on the number of free variables. Part of the objective function
may be given implicitly as the solution of a system of n parameterized nonlinear equations. If
the system parameters depend on the free variables of the objective, then second derivatives
of the nonlinear system’s solution with respect to those parameters are required. The local
computational overhead as well as the additional memory requirement for the computation of
second-order adjoints of the solution vector with respect to the parameters by AD depends on
the number of iterations performed by the nonlinear solver. This dependence can be eliminated
by taking a symbolic approach to the diﬀerentiation of the nonlinear system.
Keywords: Second-Order Adjoint Nonlinear Solver, Algorithmic Diﬀerentiation

1

Introduction and Foundations

In this paper we consider two alternative approaches to evaluating second-order adjoints of numerical simulation programs with an embedded iterative solver for a system of n parameterized
nonlinear equations. Naive application of algorithmic diﬀerentiation (AD) [9, 11] yields a computational cost for second-order adjoints of the solution vector with respect to the parameters
which depends on the number of nonlinear iterations ν. Symbolic diﬀerentiation (SD) is based
on the assumption of having reached the exact solution of the nonlinear system. The derived
Selection and peer-review under responsibility of the Scientiﬁc Programme Committee of ICCS 2016
c The Authors. Published by Elsevier B.V.

doi:10.1016/j.procs.2016.05.388

2231

2nd -Order Adjoint Solvers for Parameterized Systems of Nonlinear Equations Saﬁran, Lotz, Naumann

formulations allow an implementation which reduces memory requirement and computational
cost by an order of complexity.
The focus of this paper is on adjoint mode AD. Analogous results were derived for tangent
mode AD in [13]. In the following section we recall some aspects from [12] which this work
is based on. Further related work by others includes [3, 4, 6, 8] as well as [9, section 15.5].
We consider the computation of second-order adjoints for solvers of systems of n nonlinear
equations depending on m parameters and described by the residual
r = F (x(λ), λ) : IRn × IRm → IRn .

(1)

For a given λ ∈ IRm , a primal solution vector x ∈ IRn is sought such that r = 0. Without loss
of generality, the nonlinear solver is assumed to be embedded into the unconstrained convex
nonlinear programming problem (NLP) minz f (z) , z ∈ IRq for a given objective function
f : IRq → IR. Second-order derivative-based NLP solvers such as Ipopt [14] use the gradient
and the Hessian of the objective y = f (z) with respect to the free variables z ∈ IRq . Hence, both
ﬁrst and second derivatives of the nonlinear solver are required. As in [12], f is decomposed
as y = f (z) = p(S(x0 , P (z))), where P : IRq → IRm denotes the part of the computation that
precedes the nonlinear solver S : IRn × IRm → IRn and p : IRn → IR maps the result x onto the
scalar objective y.
We recall some signiﬁcant elements of AD described in further detail in [9, 11]. Without
loss of generality, the following discussion is based on Equation (1). Let u = (x, λ) ∈ IRh
and h = n + m. AD yields semantical transformations of the given implementation of F as a
computer program into ﬁrst and higher (k-th) derivative code. F is assumed to be k times
continuously diﬀerentiable. Extensions of AD to nondiﬀerentiable functions have been the
subject of ongoing research for several years; see, for example, [7, 10]. AD provides two basic
models, the tangent and the adjoint model. Tangents of a variable are denoted by superscripts,
e.g. x(1) for ﬁrst-order tangents, and adjoints are denoted by subscripts, e.g. x(1) for ﬁrst-order
adjoints. Higher-order mixed projections carry consequently mixed super- and subscripts, e.g.
(2)
(x,λ)
x(1) for second-order tangents of adjoints. With the Jacobian ∇F = ∇F (u) ≡ ∂F
∂(x,λ) of a
multivariate vector function r = F (u), F : IRh → IRn , the tangent model is given as IRh → IRn
deﬁned by r(1) →< ∇F (u), u(1) >≡ ∇F (u) · u(1) . The whole Jacobian can be accumulated
with machine accuracy at a computational cost of O(h) · Cost(F ) by letting u(1) range over
the Cartesian basis vectors in IRh . The adjoint model is given as IRn → IRh deﬁned by
r(1) →< r(1) , ∇F (u) >≡ ∇F (u)T · r(1) . As opposed to the tangent model, the adjoint model
can compute the whole Jacobian at a computational cost of O(n) · Cost(F ) by letting r(1)
2

F (x,λ)
range over the Cartesian basis vectors in IRn . With the Hessian ∇2 F = ∇2 F (u) ≡ ∂∂(x,λ)
of
2
r = F (u) (a symmetric 3-tensor, i.e. [∇2 F ]k,i,j = [∇2 F ]k,j,i ) the tangent-over-adjoint model is
deﬁned by
(2)

(2)

u(1) =< r(1) , ∇F (u) > +

n

r(1)
i=1

i

· ∇2 [F ]i · u(2) .

(2)

The whole Hessian can be accumulated with machine accuracy at a computational cost of
(2)
O(h · n) · Cost(F ) by setting r(1) = 0 and letting r(1) and u(2) range over the Cartesian basis
vectors in IRn and IRh , respectively. Re-application of tangent or adjoint mode AD yields higher
derivative information. This way, AD can compute projections of derivative tensors of arbitrary
order. Our AD tool dco/c++ is used to implement the theoretical results in this article. dco/c++
is an AD overloading tool written in and for C++. Apart from arbitrary tangent and adjoint
projections, it facilitates the deﬁnition of ﬁrst- and higher-order tangent and adjoint versions
2232

2nd -Order Adjoint Solvers for Parameterized Systems of Nonlinear Equations Saﬁran, Lotz, Naumann

of user-deﬁned high-level intrinsics, for example, second-order adjoint nonlinear solvers. The
technical details of the given implementation in dco/c++ are similar to the description in [12]
and therefore omitted here.

2

Second-Order Adjoint Nonlinear Solver

We consider two approaches to the generation of second-order adjoint solvers for systems of
parameterized nonlinear equations. The ﬁrst approach is a fully algorithmic adjoint version of
the solver, that computes second-order adjoints of the approximation of the solution, which
is actually computed by the algorithm. In this setting, tangent-over-adjoint AD is applied to
the individual statements of the primal nonlinear solver. The memory requirement as well
as the computational overhead grows with the number of operations performed by the primal
nonlinear solver. The resulting memory requirement is likely to exceed the available resources for
most real-world applications. Checkpointing techniques can help keeping the required memory
feasible at the expense of additional primal function evaluations [9, 11]. Reverse accumulation
[3] limits the memory requirement to that induced by a single iteration of the primal nonlinear
solver. Alternatively, one is a coupled approach uses a symbolic adjoint version of the solver,
based on the assumption that the exact primal solution has been reached. The nonlinear system
F (x, λ) = 0 can be diﬀerentiated symbolically in this case according to the implicit function
theorem. The discrepancies in the numerical results computed by second-order algorithmic
and symbolic adjoint nonlinear solvers depend on the accuracy of the approximation of the
primal solution. The following theorem shows that second-order adjoints of the paramters
can be computed by solving one linear system with the Jacobian and two linear systems with
the transposed Jacobian of the residual as system matrix followed by two evaluations of the
second-order adjoint residual.
Theorem 2.1. Let for r = F (x(λ), λ) : IRn × IRm → IRn and for a given λ ∈ IRm , a vector
(2)
x ∈ Rn be sought such that F (x(λ), λ) = 0. Second-order adjoints λ(1) ∈ IRm of the solution
x ∈ IRn of the system of parameterized nonlinear equations F (x, λ) = 0 with respect to the
parameter vector λ ∈ IRm are equal to
(2)

λ(1) =< z,

∂2F
x(2)
,
∂λ∂(x, λ) λ(2)

> + < z(2) ,

∂F
>,
∂λ

(3)

∂F T
∂x

∂z
· z = −x(1) . Furthermore, z(2) =< ∂λ
, λ(2) >∈ IRn satisﬁes
(2)
x
(2)
∂F T
∂2F
∂x
> −x(1) , where x(2) = ∂λ
· z(2) = − < z, ∂x∂(x,λ)
,
· λ(2) is evaluated by a
∂x
λ(2)
(2)
(2)
ﬁrst-order symbolic tangent version of the nonlinear equation [12], i.e. ∂F
= − ∂F
.
∂x · x
∂λ · λ

where z ∈ IRn solves

Proof. First-order adjoints λ(1) of the solution x of F (x(λ), λ) = 0 with respect to λ are equal
T

n

T

∂Fi
∂F
to λ(1) =< z, ∂F
i=1 zi · ∂λ , where z is the solution of the linear system ∂x ·z = −x(1) ,
∂λ >=
see [12]. The directional derivative of λ(1) with respect to λ in direction λ(2) becomes
(2)

λ(1) =

n
i=1

(2)

zi

·

∂Fi T
∂ 2 Fi
x(2)
·
+ zi ·
λ(2)
∂λ
∂λ∂(x, λ)

T

(4)

where x(2) is evaluated according to the Equation given above. Similarly, the directional deriva2233

2nd -Order Adjoint Solvers for Parameterized Systems of Nonlinear Equations Saﬁran, Lotz, Naumann

tive of the linear system
n
i=1

∂F T
∂x
(2)

zi

·z=
·

n
i=1

i
zi ∂F
∂x

T

= −x(1) is given by

∂Fi T
∂ 2 Fi
x(2)
·
+ zi ·
λ(2)
∂x
∂x∂(x, λ)

T

(2)

= x(1)

(5)

yielding the linear system for z(2) .
(2)

The amount of memory required for evaluating the symbolic second-order adjoint λ(1) is
roughly equal to the amount of memory required by the primal nonlinear solver, i.e, ∼ O(n2 )
for the Jacobian matrix. In Equation (3), the complexity of evaluating the right hand side is
O(1) · Cost(F ). The Jacobian needs to be factorized only once for the ﬁrst-order adjoint and
can be reused for evaluating the second-order adjoint. Solving the linear system Equation (2.1)
by forward/backward substitution at a cost of O(n2 ), the complexity for the overall overhead
(2)
induced by evaluating the symbolic second-order adjoint λ(1) becomes proportional to n3 .

3

Results and Conclusion

Figure 1 shows results based on an one-dimensional time-dependent partial diﬀerential equation.
Newton’s method is used as the nonlinear solver in the implicit timestepping scheme, where
in each iteration the Jacobian of the residual is factorized as part of a direct approach to the
solution of the embedded linear system. The ﬁgure clearly shows the advantage in terms of run
time and memory requirements of the symbolic approach.
100

10,000

10

100

memory

run time

1,000

10
SD
AD
FD

1
0.1

1
SD
AD

0.1

0.01
10

20

50

n

100

200

400

10

20

50

n

100

200

400

Figure 1: Run time (in seconds) and memory requirements (in MB) for the 1D reference
problem using symbolic (SD) and algorithmic (AD) approaches to diﬀerentiation as well as
ﬁnite diﬀerences (FD). Missing values indicate failure to converge within 5 hours.
AD is based on the knowledge of partial derivatives of a set of elemental functions [9] used
to build up an evaluation procedure for a given target function. Traditionally, the built-in
functions and arithmetic operators of programming languages have been serving as elementals.
Deeper insight into the mathematical structure of numerical simulations yields higher-level
elemental functions including linear and nonlinear solvers, optimizers, integrators for ordinary
and partial diﬀerential equations. First- and higher-order numerical methods generate the
need for ﬁrst and higher derivatives of these new elemental functions. They ﬁnd application
within a wide range of modern methods in Computational Science, Engineering, and Finance
ranging from sensitivity and error analysis to adjoint approaches to the solution of inverse
2234

2nd -Order Adjoint Solvers for Parameterized Systems of Nonlinear Equations Saﬁran, Lotz, Naumann

problems. A rich collection of related articles can be found in the proceedings of the last three
international conferences on AD [1, 2, 5]. Software tools for AD1 need to exhibit a high level
of ﬂexibility and extensibility in order to facilitate the integration of new elemental functions.
Diﬀerentiated versions of numerical libraries will soon have to provide appropriate naming and
calling conventions for adjoints and adjoints of numerical methods. For example, the Numerical
Algorithms Group Ltd.2 is in the process of expanding the NAG Library with both ﬁrst- and
second-order tangent and adjoint versions for a suitable subset of the more than 1500 numerical
routines. dco/c++ keeps adopting them as new elementals.

Acknowledgments
N. Saﬁran was ﬁnancially supported by the Diversity Fund of the RWTH Aachen. J. Lotz was
ﬁnancially supported by the BeProMod project, which is part of the NRW-Strategieprojekt
BioSC funded by the Ministry of Innovation, Science and Research of the German State of
North Rhine-Westphalia.

References
[1] C. Bischof, M. B¨
ucker, P. Hovland, U. Naumann, and J. Utke, editors. Advances in Automatic
Diﬀerentiation, volume 64 of Lecture Notes in Computational Science and Engineering. Springer,
Berlin, 2008.
[2] M. B¨
ucker, G. Corliss, P. Hovland, U. Naumann, and B. Norris, editors. Automatic Diﬀerentiation:
Applications, Theory, and Implementations, volume 50 of Lecture Notes in Computational Science
and Engineering. Springer, New York, NY, 2005.
[3] B. Christianson. Reverse accumulation and attractive ﬁxed points. Optimization Methods and
Software, 3(4):311–326, 1994.
[4] J. Davies, B. Christianson, L. Dixon, R. Roy, and P. van der Zee. Reverse diﬀerentiation and the
inverse diﬀusion problem. Adv. Eng. Softw., 28(4):217–221, 1997.
[5] S. Forth, P. Hovland, E. Phipps, J. Utke, and A. Walther, editors. Recent Advances in Algorithmic
Diﬀerentiation, volume 87 of Lecture Notes in Computational Science and Engineering. Springer,
Berlin, 2012.
[6] J. Gilbert. Automatic diﬀerentiation and iterative processes. Optimization Methods and Software,
1:13–21, 1992.
[7] A. Griewank. On stable piecewise linearization and generalized algorithmic diﬀerentiation. Optimization Methods and Software, 28(6):1139–1178, 2013.
[8] A. Griewank and C. Faure. Reduced functions, gradients and Hessians from ﬁxed-point iterations
for state equations. Numerical Algorithms, 30(2):113–139, 2002.
[9] A. Griewank and A. Walther. Evaluating Derivatives: Principles and Techniques of Algorithmic
Diﬀerentiation. Number 105 in Other Titles in Applied Mathematics. SIAM, Philadelphia, PA,
2nd edition, 2008.
[10] K. Khan and P. Barton. Evaluating an element of the Clarke generalized Jacobian of a composite
piecewise diﬀerentiable function. ACM Transactions on Mathematical Software, 39(4):23–1, 2013.
[11] U. Naumann. The Art of Diﬀerentiating Computer Programs. An Introduction to Algorithmic
diﬀerentiation. Number 24 in Software, Environments, and Tools. SIAM, Philadelphia, PA, 2012.
[12] U. Naumann, J. Lotz, K. Leppkes, and M. Towara. Algorithmic diﬀerentiation of numerical
methods: Tangent and adjoint solvers for parameterized systems of nonlinear equations. ACM
Transactions on Mathematical Software, 2015. To appear.
[13] N. Saﬁran, J. Lotz, and U. Naumann. Second-order tangent solvers for systems of parameterized
nonlinear equations. Procedia Computer Science, 51(0):231 – 238, 2015.
[14] A. W¨
achter and L. Biegler. On the implementation of an interior-point ﬁlter line-search algorithm
for large-scale nonlinear programming. Mathematical Programming, 106(1):25–57, 2006.
1 Refer

to the AD community’s web portal www.autodiff.org for further information.

2 www.nag.co.uk

2235

