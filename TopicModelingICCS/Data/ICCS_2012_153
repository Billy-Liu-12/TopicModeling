Available online at www.sciencedirect.com

Procedia Computer Science 9 (2012) 1657 – 1666

International Conference on Computational Science, ICCS 2012

On the Way of Applying Urgent Computing Solutions to Forest
Fire Propagation Prediction
Andr´es Cencerrado1 , Ana Cort´es, Tom`as Margalef
Computer Architecture and Operating Systems Department. Escola d’Enginyeria, Universitat Aut`onoma de Barcelona. 08193 Bellaterra,
Barcelona (Spain)

Abstract
A quick response becomes crucial in natural hazard management. When an emergency occurs, hazard evolution
simulators are a very helpful tool for the teams in charge of making decisions. To perform the simulations, they rely on
data which usually constitutes a big set of parameters, which have been previously recorded from observations, usually
coming from remote sensors, images, etc. However, this data is frequently subject to a high degree of uncertainty.
This data uncertainty also produces uncertainty in simulators’ results. To overcome this drawback, in previous works
we developed a two-stage prediction method, which has been demonstrated to improve noticeably the quality of the
predictions. The time incurred in performing this strategy, however, may vary signiﬁcantly depending on diﬀerent
factors. As it is well known, the execution time of a particular simulator depends on the speciﬁc setting of the
input parameters. Moreover, decision control centers in charge of making decisions to ﬁght against the ongoing
disaster require a certain degree of quality in the ﬁnal prediction. Depending on how demanding are the quality and
time constraints, the two-stage strategy may need the support of Urgent Computing solutions, in order to meet the
requirements. In this work, we focus on forest ﬁres spread prediction, as a real application case of study, and we
expose our methodology to characterize both the time needed and the expected quality of the two-stage prediction
method, so that we are able to determine the amount of computational resources needed to respond eﬃciently to
an eventual emergency. This way, we emphasize the need of Urgent Computing mechanisms to be able to put into
practice this method at real time.
Keywords: Forest Fire Simulation, Data Uncertainty, Urgent Computing

1. Introduction
Nowadays, scientiﬁc community rely on High Performance Computing (HPC) environments in order to solve
most of the present scientiﬁc problems. Because of the computational advances, these problems have become more
demanding, as a consequence of the fact that it is possible at the present to achieve goals which were inconceivable
in the past. Nevertheless, a lot of these applications still need much computational resources and time, specially
Email addresses: acencerrado@caos.uab.es (Andr´es Cencerrado), ana.cortes@uab.es (Ana Cort´es), tomas.margalef@uab.es
(Tom`as Margalef)
1 Corresponding author

1877-0509 © 2012 Published by Elsevier Ltd. Open access under CC BY-NC-ND license.
doi:10.1016/j.procs.2012.04.183

1658

AndrÈs Cencerrado et al. / Procedia Computer Science 9 (2012) 1657 – 1666

dynamic, event-driven simulations. In much of these cases the challenge is not only to solve them but to get the
results satisfying demanding time constraints.
In order to satisfy computing-intensive problems in the minimum possible time, it is possible to resort to distributed
computing infrastructures, e.g. grid computing, cloud computing. These kind of infrastructures allow the users
to count on many computing resources managed in a decentralized way. Natural hazards evolution prediction is
an area that can beneﬁt from this kind of infrastructures since when a natural hazard is happening there are strict
deadlines. Urgent Computing facilities can guarantee response time and resource availability needed to accomplish
such requirements [9].
One of the most signiﬁcant natural hazards is forest ﬁre. Every year forest ﬁres cause signiﬁcant damages around
the world. This kind of hazard provokes looses from the ecological, economical, social and human point of view.
Therefore, when an emergency occurs a quick response is crucial to minimize its eﬀects. In this context, the accurate
prediction of the propagation of forest ﬁre is critical to use the available resources to ﬁght against the ﬁre in the most
eﬃcient way.
In the ﬁeld of forest ﬁre behavior modeling, there exist several ﬁre propagation simulators [11, 12, 18], based
on some physical or mathematical models [15], which main objective is to try to predict the ﬁre evolution. These
simulators need certain input data, which deﬁne the characteristics of the environment where the ﬁre is taking place,
in order to evaluate its future propagation. This data usually consists of the current ﬁre front, terrain topography,
vegetation type, and meteorological data such as humidity and wind direction and wind speed. The classic way of
predicting forest ﬁre behavior, which is summarized in Figure 1(a), takes the initial state of the ﬁre front as input, as
well as the input parameters given for a certain time instant. The simulator then returns the ﬁre spread prediction for
a later time instant. Comparing the simulation result with the real ﬁre advance, the forecasted ﬁre front tends to diﬀer
to a greater or lesser extent from the real ﬁre line. One reason for this behavior is that the classic calculation of the
simulated ﬁre is based on one single set of input parameters. Some of this data could be retrieved in advance and with
noticeably accuracy, as, for example, the topography of the area and the predominant vegetation types. However, there
is some data that turns out very diﬃcult to be obtained with reliability. For instance, getting an accurate ﬁre perimeter
is very complicated because of the diﬃculties involved in getting, at real time, images or data about this matter. Other
kind of data sensitive to imprecisions is that of meteorological data, which is often distorted by the ﬁre itself. However,
this circumstance is not only related to forest ﬁres, but it also happens in any system with a dynamic state evolution
throughout time (e.g. ﬂoods [13], thunderstorms [2, 16], etc.). These restrictions concerning uncertainty in the input
parameters, added to the fact that these inputs are set up only at the very beginning of the simulation process, become
an important drawback. As the simulation time goes on, variables previously initialized could change drastically,
misleading simulation results. In order to overcome these restrictions, we need a system capable of dynamically
obtaining real time input data in those cases that is possible and, otherwise, properly estimating the values of the input
parameters needed by the underlying simulator.
To tackle this problem, our research group developed, in previous works, a two-stage prediction methodology
[1, 5, 8]. This methodology introduces an adjustment stage. In the adjustment stage the actual propagation of the
hazard is observed and then a search is carried out to determine the values that best reproduce the actual propagation
of the hazard. Such values are used in the prediction stage (see Figure1(b)). This method noticeably increases the
quality of the results, but it requires Urgent Computing solutions to be eﬃcient. The same methodology is also
applicable to any other natural hazard such as hurricanes, ﬂoods or tsunamis.
The search space is multidimensional and very huge and, therefore, some evolutionary methods, such as genetic
algorithms, tabu search or simulated annealing, have been introduced to accelerate the search. These methods are
iterative and require the execution of many simulations per iteration. However, when the natural hazard is taking
place, response time is critical and it is necessary to determine its future propagation beforehand. So, we face three
main interrelated issues:
• The quality of the prediction is directly related to the quality of the adjustment, and the quality of the adjustment
depends on the number of iterations of the evolutionary algorithm, as well as on the number of scenarios tested
per iteration.
• The amount of computing resources determines the amount of simulations that can be simultaneously executed
per iteration at the adjustment stage.

AndrÈs Cencerrado et al. / Procedia Computer Science 9 (2012) 1657 – 1666

(a) Classic prediction

1659

(b) Two-stage prediction
Figure 1: Prediction Methods

• The response time to provide a prediction is a critical point and seriously limits the number of iterations that
can be executed at the adjustment stage.
So, in a real emergency, the response time to provide the result of the prediction is ﬁxed by the decision control
center. In the same way, the quality of the prediction is also a parameter ﬁxed by the decision control center. Under
these constraints, it is necessary to determine the computing resources required to fulﬁll the time and quality constraints. Therefore, the goal of this work is to develop a methodology that determines the required resources to ensure
a certain prediction quality in a given ﬁxed time. Once the amount of resources is determined, Urgent Computing
facilities must be used to ensure the expected results.
This paper is organized as follows. In Section 2, we expose the characterization of the adjustment stage and the
methodology to assess in advance both the time needed and the prediction quality expected. Section 3 shows the
experimental results and, ﬁnally the main conclusions are included in Section 4.
2. Characterization of the adjustment stage: Quality assessment and execution time estimation
Because of the dangerous consequences it may involve, it is clear that when dealing with emergency simulation,
it is extremely necessary to maximize the compromise between urgency, i.e. being able to satisfy strict deadlines, and
accuracy, i.e. satisfy restrictions concerning the quality of the predictions. For this purpose, in order to be operative
during a real hazard occurrence we need access to a huge number of computing elements. This leads to the necessity
of deploying a way to set up in advance:
• The prediction scheme settings, in particular, the adjustment policy’s speciﬁc parameters, for a required prediction’s quality. This is specially relevant when the ongoing hazard may threaten urban areas and even human
lives.
• The computational resources needed to deliver a required prediction’s quality, given certain time constraints.
In order to bound the problem, we work under certain assumptions:
• We rely on the two-stage prediction strategy under stable environmental conditions.
• We focus on those emergencies where the corresponding simulators present high input-data sensitivity.
• We assume scenarios where the computational resources are dedicated. For this, we need support from tools
that allow urgent execution of tasks in distributed-computing environments, e.g. SPRUCE [4].

1660

AndrÈs Cencerrado et al. / Procedia Computer Science 9 (2012) 1657 – 1666

In the two-stage prediction strategy exposed in previous section, the adjustment process plays the main role.
Previous studies demonstrated that the quality of the prediction is directly correlated to the quality obtained at the
end of the adjustment process [5]. Thus, it is absolutely necessary to have a good characterization of the adjustment
process in order to be able to evaluate the adjustment quality we can reach under certain conditions. To accomplish
this goal it is necessary to characterize the evolutionary algorithm in such a way that it is possible to determine,
beforehand, the number of iterations and the number of scenarios per iteration that should be executed to ensure a
certain prediction quality. Since each scenario implies one execution of the simulation kernel, it is also necessary to
characterize this simulation kernel to estimate the time required to run each simulation.
Having characterized the evolutionary algorithm and the simulation kernel, then it is possible to determine the
necessary computing resources to execute a certain number of iterations with a certain number of simulations per
iteration, having each simulation an estimated execution time. Thus, our methodology allows determining the required
computing resources to reach a certain prediction quality in a given time. In this work, we expose the methodology
developed to perform such characterization for the particular case of forest ﬁre spread, as well as a set of experiments
aimed to validate our proposals, highlighting how our framework would beneﬁt from Urgent Computing mechanisms.
2.1. Evolutionary Algorithm Characterization
For the particular case of forest ﬁre spread prediction, Genetic Algorithm (GA) has proved to be a good adjustment
strategy. As it is well known, GA works in an iterative way. It starts with an initial population of individuals which
will be evolved through several iterations in order to guide them to better search space areas. Every individual from
a population is ranked according to a predeﬁned ﬁtness function. The ﬁtness function, in the case of forest ﬁres
spread prediction, is the symmetric diﬀerence between real and predicted ﬁre spread divided by the real burned area.
Operators such as elitism, selection, crossover and mutation are applied to every population to obtain a better one. The
iterative nature of GA leads to an eventually near-optimal solution in the adjustment stage after a certain number of GA
iterations. However, there are several parameters of the GA that inﬂuence its convergence. The number of individuals
per iteration, the number of iterations, the elitism, the crossover and mutation probability aﬀect the convergence of
the GA, as it is subsequently stated:
• The number of iterations of the GA is related to the quality of the adjustment, but on the other hand it is also
related to the execution time. More iterations means a better adjustment, but it also implies more computing
time.
• The number of individuals (population) per iteration is related to the convergence speed, but it is also related to
the computing resources required. A larger population converges faster than a smaller one, but it requires more
computing resources to execute all the individuals in parallel.
• There are internal parameters of the GA, such as the mutation probability or the amount of elitism, that aﬀect
both the convergence and the quality of the adjustment, although they do not have a direct inﬂuence on the
computing resources required or on the time per iteration.
In this subsection, we present the experimental studies carried out to fulﬁll the need of being able to select, in
advance, the best settings for the adjustment method, Genetic Algorithm (GA) in this case, given a certain prediction
quality constraint.
The experiments were based on the FARSITE ﬁre simulator [11]. In this case, we set a reference ﬁre with a
duration of ﬁve hours. All the simulations carried out in this study take these ﬁrst ﬁve hours of spread as the adjustment
time interval. The initial values of the parameters of each individual are randomly generated taking into account the
range of possible values and the probability distribution for each parameter [6].
Regarding the GA conﬁguration, in this particular set of experiments, we ﬁxed the elitism factor to 10%, and
the number of generations to 5, and we performed a study based on the results obtained from the evolution of 50
populations composed of 10, 25, 50 and 100 individual. For each case, the mutation probability was set to 10%.
From the obtained results, a statistical study carrying out the Kolmogorov-Smirnov, Anderson-Darling, and Chisquared tests allowed us to determine that the probability distribution which better ﬁts the obtained data is the Logistic
distribution, which resembles the normal distribution, but presents higher kurtosis. Its probability density function is
the following one:

AndrÈs Cencerrado et al. / Procedia Computer Science 9 (2012) 1657 – 1666

1661

Figure 2: Probability density functions for the obtained errors at each generation of the evolution process (populations of 100 individuals).

pd f (x; μ, s) =

e−(x−μ)/s
s(1 + e−(x−μ)/s )2

(1)

In this equation, x is the random variable (which corresponds to the obtained adjustment error), μ is the location
factor, which is analogous to the mean value in a normal distribution, and s is the scale factor, which is proportional
to the standard deviation, both of them needed to deﬁne such probability distribution. Although the probability
distribution of the data is the same in the whole evolution process, these factors vary depending on the iteration of
the GA we are evaluating. So, Figure 2 depicts the diﬀerent probability density functions for each generation, for the
particular case of 100 individuals per population.
By means of these probability density function, we are able to guarantee, with diﬀerent degrees of certainty, the
maximum adjustment error we will obtain given a certain conﬁguration of the GA. Besides, since the number of
evolved generations has a direct impact on both the available resources and time needed to perform the adjustment
process, it is worth highlighting the fact that we are able to give this guarantee taking into account the number of
generations we are able to execute.
So, for example, we can evaluate the maximum adjustment error that can be guaranteed in 90% of the adjustment
processes. Table 1 shows the diﬀerent maximum adjustment errors (considering the adjustment time interval [0 hours
- 5 hours]) depending on the number of individuals per population, and the number of iterations of the GA.
Individuals per population
10
25
50
100

G1
2.96
1.39
1.15
0.68

G2
2.43
1.13
0.74
0.64

G3
1.75
1.05
0.68
0.57

G4
1.22
0.87
0.62
0.46

G5
1.12
0.80
0.54
0.36

Table 1: Maximum adjustment errors for a 90% degree of guarantee, depending on the number of GA generations, and the number of individuals
per population.

Considering a real situation, the quality of the prediction is a parameter ﬁxed by the decision control center in
charge of making the appropriate decisions about how to ﬁght against the ongoing ﬁre. So, it is possible to ﬁx
the quality of the adjustment and then determine the guarantee degree of reaching such error in a given number of
iterations. Figure 3 shows the guarantee degree for errors of 0.33 and 0.5 considering populations of 50 and 100
individuals.

1662

AndrÈs Cencerrado et al. / Procedia Computer Science 9 (2012) 1657 – 1666

Figure 3: Guarantee degrees for adjustment errors 0.33 and 0.5 (populations composed of ﬁfty and one hundred individuals).

This information turns out to be very signiﬁcant, since we are able to establish error thresholds taking into account
how many individuals we can simulate in parallel, and how many evolution steps (i.e. how many iterations) we can
perform. This information will determine the necessary computational resources and time to deliver a prediction.
This results highlight the need to be able to count on as many individuals (i.e. as many simulations per iteration)
as possible in order to perform a prediction with guarantees. In practice, this fact implies to be able to have access to
more computational resources at the moment of dealing with the emergency.
Furthermore, the capability of executing in parallel as many simulations as possible not only has a direct impact on
the quality of the results, but also in the time needed to deliver them. In Table 1, one can easily notice that populations
composed of 100 individuals present errors hardly achievable by populations of ten individuals, even extending the
adjustment process to more than ﬁve generations. This fact happens because of the existence of local minimums in
the search space. It is also noticeable that the evolution of populations of 100 individuals are always more favourable
than populations of 50 individuals, even performing one less iteration of the GA. Nevertheless, in order to beneﬁt
from a 100-individual conﬁguration of GA, it is compulsory to be able to simulate them in a parallel way, otherwise
this conﬁguration would shoot up the time needed to give the prediction. Hence, this fact highlights the need to count
on eﬃcient Urgent Computing solutions so that we can tackle this problem in a feasible way.
2.2. Simulation kernel characterization
As it has been mentioned above, once the GA has been characterized, the number of iterations and the number
of individuals per iteration that must be executed to reach a certain quality index can be determined. The next component of the methodology that must be characterized is the simulation kernel. The matter concerning the simulation
time estimation may be tackled by means of carrying out large sets of executions of the underlying simulator, and
then analyzing its behavior from the obtained results, and then, applying knowledge-extraction techniques from the
information they provide. We record the execution times from the experiment, and then we establish a classiﬁcation
of the input parameters according the elapsed times they produce. At this moment, we are capable to apply machine
learning techniques to determine the classiﬁcation criteria and, therefore, given a new set of input parameters, to be
able to estimate how much the execution will last.
This learning process is carried out oﬄine, i.e. the classiﬁcation rules are established prior to the hazard occurrence. Therefore, at the moment of the urgency management, we only have to apply the classiﬁcation technique,
which involves a negligible cost of computational time.
As in the previous section, we have used FARSITE, as a ﬁre spread simulator. This experiment uses the GIS
data from the benchmark provided by FARSITE (the Ashley project), and in every case, a simulation of 30 hours is
performed.
The probability distributions and their associated parameters are the ones described in [6]. Regarding wind speed
and direction, the ones used in [10] were considered. The vegetation models correspond to the 13 standard Northern
Forest Fire Laboratory (NFFL) fuel models [3].

AndrÈs Cencerrado et al. / Procedia Computer Science 9 (2012) 1657 – 1666

1663

Once established the distribution of each input parameter, a set of 12000 diﬀerent combinations of input data sets
was generated, so as to build the training database, and the simulations of each scenario were performed in the same
computational platform.
As regards the computational platform, all the experiments were carried out on a cluster of 8 x Dell PowerEdge
M600 nodes, each of which counting on 2xQuad-Core Intel Xeon E5430, 2.66GHz, 2x6MB L2 cache memory (2x2)
and 16 GB RAM Fully Buﬀered DIMMs 667MHz, running Linux version 2.6.16.
From the point of view of emergency prediction, it is crucial to have the execution time under control, so we may
deal with cases that drastically slow down the prediction process. A predicted execution time for a certain simulation
with an error on the order of hundreds of seconds would be prohibitive, so, from cases like this one, there arises the
need to be able to predict how the simulator is going to behave and, therefore, the need to use an eﬃcient classiﬁcation
technique.
In order to tackle this requirement, we chose decision trees as a classiﬁcation method, to be able to estimate, in
advance, the execution time of FARSITE, given a new set of input parameters.
The decision trees used in this research were the ones generated by the C4.5 algorithm [14], speciﬁcally, the J48
open source Java implementation of the C4.5 algorithm in the Weka data mining tool [17]. The data obtained from
the 12000 executions was used as a training set, and 1000 new instances were generated according to the probability
distributions of each parameter to be used as a test set.
The number of classes, and the execution time intervals they represent, were determined taking into account where
our work is framed, i.e. the intervals chosen for each class are those that in a real emergency situation would matter (it
has no sense, for example, to classify by intervals of 10 seconds when predicting forest ﬁre spread). For this purpose,
we bound diﬀerent classes by analyzing the diﬀerent local minimums the histogram presents, so that we minimize the
classiﬁcation errors due to values too close to the boundaries. As a result, the deﬁned classes are the following, where
ET stands for execution time:
• Class A: ET ≤ 175 seconds.
• Class B: 175 seconds < ET ≤ 500 seconds.
• Class C: 500 seconds < ET ≤ 875 seconds.
• Class D: 875 seconds < ET ≤ 3600 seconds.

Real Class

The results of applying decision trees to the test set are summarized in Table 2. Here, one of the main aspects to
highlight is the prominence of the main diagonal, which means that perfect matches are predominant over the whole
set of predictions. Furthermore, one can notice that the values decrease as one moves away from the main diagonal.
In total, we obtained a 95.7% of correct classiﬁcations.

A
B
C
D

A
341
10
0
0

Predicted Class
B
C
11
0
358
6
6
108
0
5

D
1
1
3
150

Table 2: Correspondence between real and predicted classes.

Obviously, this represents a very good result, which allow us to predict, in advance, how much a simulation will
last, with a very high degree of certainty. It is worth mentioning that, in order to be conservative, given a certain input
setting for a simulation, we assign the upper-bound value of its class to the expected execution time it will produce.

1664

AndrÈs Cencerrado et al. / Procedia Computer Science 9 (2012) 1657 – 1666

3. Experimental study
In this section, an experimental validation of the GA characterization is reported. Following the methodology
exposed in Section 2.1, we are able to select, in advance, the best settings for the GA, given a certain prediction
quality constraint. The subsequent experiments are aimed to recreate a real situation where diﬀerent quality and time
restrictions are imposed. As mentioned above, in the presence of restrictions, we have to eﬀectively deal with the
speciﬁc conﬁgurations of the GA used as adjustment technique, since the number of iterations (generations) have a
direct impact on both the quality of the results and execution time. In the same way, the number of individuals aﬀects
the convergence speed and the amount of computational resources required.
In order to validate this methodology, an experiment consisting of the evolution of 10 populations of 50 individuals,
and 10 populations of 100 individuals was carried out. Moreover, in this experiment, only individuals which execution
time is previously classiﬁed, at the most, as Class C (see Section 2.2), were allowed to be executed (a study of how
this action aﬀects to the quality of the adjustment is reported in [7]). Table 3 shows the obtained values, in terms of
quality of adjustment and time spent to achieve it.
Taking into account the probability density function exposed in Section 2.1 (Equation 1), let us consider two
diﬀerent cases, regarding adjustment quality requirements:
• Case A: an adjustment error of 0.33 is required. In this case, the only suitable conﬁguration for the GA would
be a population of 100 individuals. As can be seen in Figure 3, the probability density function tell us that we
can achieve this quality with a guarantee degree of 82.4% at the ﬁfth generation, and with 64.7% at the fourth
generation.
• Case B: an adjustment error of 0.5 is required. In this case, a conﬁguration of 100 individuals would be
appropriate, so we can achieve it at the fourth generation with a 91.3% degree of guarantee. A conﬁguration with
50 individuals would also be suitable, since we can reach such quality at the ﬁfth generation with a guarantee
degree of 86.7%.

Individuals

50

100

Population
p0
p1
p2
p3
p4
p5
p6
p7
p8
p9
p0
p1
p2
p3
p4
p5
p6
p7
p8
p9

G1
0.831
0.431
0.763
0.602
0.602
0.627
0.843
0.544
0.503
0.829
0.848
0.333
0.861
0.865
0.166
0.993
0.823
0.588
0.825
0.830

G2
time
711 s
759 s
560 s
813 s
794 s
801 s
862 s
856 s
809 s
790 s
710 s
870 s
835 s
787 s
855 s
863 s
860 s
839 s
734 s
865 s

0.698
0.431
0.763
0.602
0.602
0.572
0.843
0.541
0.503
0.828
0.148
0.330
0.344
0.520
0.166
0.450
0.332
0.558
0.287
0.322

G3
time
714 s
778 s
730 s
718 s
530 s
791 s
761 s
858 s
705 s
792 s
799 s
717 s
742 s
867 s
866 s
868 s
860 s
802 s
728 s
809 s

0.698
0.431
0.564
0.598
0.602
0.572
0.624
0.541
0.312
0.026
0.148
0.330
0.344
0.393
0.166
0.450
0.105
0.558
0.287
0.322

G4
time
715 s
813 s
609 s
537 s
668 s
784 s
776 s
580 s
772 s
805 s
865 s
813 s
806 s
773 s
861 s
828 s
708 s
780 s
864 s
681 s

0.512
0.431
0.563
0.598
0.602
0.572
0.624
0.489
0.312
0.026
0.148
0.330
0.342
0.393
0.164
0.125
0.105
0.558
0.235
0.322

G5
time
738 s
458 s
669 s
800 s
694 s
866 s
755 s
605 s
681 s
844 s
799 s
818 s
809 s
805 s
728 s
672 s
866 s
770 s
781 s
646 s

0.512
0.427
0.563
0.496
0.412
0.422
0.624
0.449
0.311
0.026
0.146
0.330
0.342
0.393
0.161
0.095
0.105
0.317
0.235
0.315

time
768 s
575 s
573 s
692 s
558 s
718 s
834 s
707 s
623 s
780 s
640 s
814 s
779 s
764 s
786 s
740 s
828 s
767 s
818 s
863 s

Table 3: Adjustment errors obtained for 10 diﬀerent populations, and elapsed times for each generation. Populations composed of 50 and 100
individuals.

AndrÈs Cencerrado et al. / Procedia Computer Science 9 (2012) 1657 – 1666

1665

As it can be observed in Table 3, populations composed of 100 individuals fulﬁll the requirements in both cases.
Considering Case A, at the end of the ﬁfth generation, only two populations exceeded the error of 0.33 (populations
p2 and p3). At the end of the fourth generation, three populations exceeded that error (populations p2, p3 and p7).
These results comply with the established degrees of guarantee (82.4% and 64.7%, respectively). Case B is also
fulﬁlled at the fourth generation, since only one population (p7) presented an error higher than 0.5, being 91.3% the
corresponding degree of guarantee.
As for populations composed of 50 individuals, for this speciﬁc experimental set, Case B (error 0.5) is not satisﬁed
in three cases (p0, p2 and p6). Given the previously mentioned guarantee degree of 86.7%, one might expect only one
or two negative cases, but taking into account the size of this experimental sample, and the fact that the error produced
by population p0 (0.512) is very close to our target, the results are acceptable.
These results not only validate our proposals, but also emphasize how important the amount of available resources
is. It is obvious that the more individuals populations have, the faster the convergence to a good solution is. This, in
terms of execution time, becomes a key point to be considered when dealing with emergencies.
Let us suppose that, in addition to the previously mentioned quality requirements, a strict deadline of one hour for
carrying out the adjustment process is imposed. As stated in Section 2.2, Class C individuals last, at the latest, 875
seconds. This deadline would prevent us from iterating more than four generations, considering we have the capacity
to execute all the individuals in a parallel way. Obviously, this capacity depends on the number of computational
resources we have access at the moment of dealing with the ongoing ﬁre.
Thus, in case of having access to 100 computational resources (i.e. being able to carry out 100 simulations in
parallel), both Case A and B are satisﬁed with good degrees of guarantee. However, a limitation of 50 computational
resources would make us to signiﬁcantly decrease our degree of guarantee to only 48.8% in Case A (error 0.33, see
Figure 3), and 78.3% in Case B (error 0.5). This important drop in the adjustment quality’s degree of guarantee would
be prohibitive, specially in cases where human lives are threatened.
These results clearly highlight the need of Urgent Computing solutions for our two-stage prediction method to be
eﬀective.
4. Conclusions
Since we are dealing in the area of natural hazards management, it is absolutely necessary to be able to assess
in advance the quality of the predictions that will be delivered by means of our prediction framework. This is very
important for the control centers to make the appropriate decisions in each case. However, in this kind of situations,
the time needed to give a prediction becomes critical.
In this work, we focus on the speciﬁc case of forest ﬁres, as a one of the most worrisome natural disasters, and the
experimental studies have been done using the FARSITE simulator. We present how we deal with the prediction time
assessment by means of the use of decision trees as classiﬁcation technique, with outstanding results, and we have also
exposed our methodology to characterize another well-known Artiﬁcial Intelligence technique as adjustment strategy,
the Genetic Algorithms, which has demonstrated to be a powerful technique to perform the adjustment process in
our two-stage prediction method. For these purposes, we have carried out statistical studies based on huge sets of
simulations, which allow us to make decisions about which speciﬁc setting of the GA is the most appropriate given
time and resource availability restrictions.
This work emphasizes the need to count on eﬃcient Urgent Computing techniques in order to be able to fulﬁll the
requirements from the decision control centers, both in terms of quality and time restrictions.
Moreover, these studies allow us to tackle this problem in diﬀerent ways, by designing diﬀerent policies to optimize the use of the available computational resources. This constitutes part of our ongoing and future work, and
includes, for example, the ability to group fastest simulations in subsets of computational resources, allocating the
slowest ones in other dedicated subsets, according to the speciﬁc needs of each case.
Acknowledgement
This research has been supported by the MICINN-Spain under contract TIN2007-64974 and contract TIN201128689-C02-01.

1666

AndrÈs Cencerrado et al. / Procedia Computer Science 9 (2012) 1657 – 1666

References
[1] B. Abdalhaq, A. Cort´es and T. Margalef, Enhancing wildland ﬁre prediction on cluster systems applying evolutionary optimization techniques,
Future Generation Comp. Syst., Volume 21(1). pp. 61–67. 2005.
[2] S. D. Aberson, Five-day tropical cyclone track forecasts in the North Atlantic basin, Weather and Forecasting, Volume 13, pp. 1005–1015.
1998.
[3] F. A. Albini. Estimating wildﬁre behavior and eﬀects. Gen. Tech. Rep. INT-GTR-30. Ogden, UT: U.S. Department of Agriculture, Forest
Service, Intermountain Forest and Range Experiment Station. 1976.
[4] P. Beckman, S. Nadella, N. Trebon and I. Beschastnikh, SPRUCE: A System for Supporting Urgent High-Performance Computing, GridBased Problem Solving Environments, Volume 239/2007, pp. 295–311. 2007.
[5] G. Bianchini, M. Denham, A. Cort´es, T. Margalef and E. Luque, Wildland Fire Growth Prediction Method Based on Multiple Overlapping
Solution, Journal of Computational Science, Volume 1, Issue 4, pp. 229–237. Ed. Elsevier Science. 2010.
[6] A. Cencerrado, A. Cort´es and T. Margalef, Prediction Time Assessment in a DDDAS for Natural Hazard Management: Forest Fire Study
Case, Procedia Computer Science 2011, Volume 4, International Conference on Computational Science (ICCS 2011), pp. 1761–1770. 2011.
[7] A. Cencerrado, A. Cort´es and T. Margalef, Prediction Time Assessment in a DDDAS for Natural Hazard Management: Forest Fire Study
Case, Proceedings of Knowledge Representation and Automated Reasoning Workshop, 22nd International Joint Conference on Artiﬁcial
Intelligence 2011, pp. 31–45. 2011.
[8] A. Cencerrado, R. Rodr´ıguez, A. Cort´es and T. Margalef, Urgency versus Accuracy: Dynamic Data Driven Application System for Natural
Hazard Management, International Journal of Numerical Analysis and Modeling, Volume 9, Number 2, pp. 432–448. 2012.
[9] Urgent Computing: Exploring Supercomputing’s New Role, CTWatch Quarterly Journal, Volume 4, Number 1. 2008.
[10] R. E. Clark, A. S. Hope, S. Tarantola, D. Gatelli, P. E. Dennison and M. A. Moritz, Sensitivity Analysis of a Fire Spread Model in a Chaparral
Landscape, Fire Ecology, Volume 4(1), pp. 1–13. 2004.
[11] M. A. Finney, FARSITE: Fire Area Simulator-model development and evaluation, Res. Pap. RMRS-RP-4, Ogden, UT: U.S. Department of
Agriculture, Forest Service, Rocky Mountain Research Station, 1998.
[12] A. Lopes, M. Cruz and D. Viegas FireStation - An integrated software system for the numerical simulation of ﬁre spread on complex toography.
Environmental Modelling and Software 17(3), pp. 269–285. 2002.
[13] H. Madsen and F. Jakobsen Cyclone induced storm surge and ﬂood forecasting in the northern Bay of Bengal, Coastal Engineering, Volume
51, Issue 4, pp. 277–296. 2004.
[14] J. R. Quinlan. Improved use of continuous attributes in c4.5, Journal of Artiﬁcial Intelligence Research, Volume 4, pp. 77–90. 1996.
[15] R. C. Rothermel. How to Predict the Spread and Intensity of Forest and Range Fires, USDA FS, Ogden TU, Gen. Tech. Rep. INT-143,
pp. 1–5. 1983.
[16] H. C. Weber, Hurricane Track Prediction Using a Statistical Ensemble of Numerical Models, Monthly Weather Review, Volume 131, pp. 749–
770. 2003.
[17] G. Holmes, A. Donkin and I. H. Witten. Weka: A machine learning workbench, Proceedings of the Second Australia and New Zealand
Conference on Intelligent Information Systems, Brisbane, Australia. pp. 357–361. 1994.
[18] FIRE.ORG - Public Domain Software for the Wildland ﬁre Community. http://www.ﬁre.org.

