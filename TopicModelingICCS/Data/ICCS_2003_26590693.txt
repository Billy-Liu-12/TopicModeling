An Augmented Lanczos Algorithm for the
Eﬃcient Computation of a Dot-Product of a
Function of a Large Sparse Symmetric Matrix
Roger B. Sidje1 , Kevin Burrage1 , and B. Philippe2
1

Department of Mathematics
University of Queensland
Brisbane QLD 4072, Australia
{rbs,kb}@maths.uq.edu.au
2
INRIA/IRISA
Campus de Beaulieu
35042 Rennes Cedex, France
philippe@irisa.fr

Abstract. The Lanczos algorithm is appreciated in many situations due
to its speed and economy of storage. However, the advantage that the
Lanczos basis vectors need not be kept is lost when the algorithm is used
to compute the action of a matrix function on a vector. Either the basis
vectors need to be kept, or the Lanczos process needs to be applied twice.
In this study we describe an augmented Lanczos algorithm to compute
a dot product relative to a function of a large sparse symmetric matrix,
without keeping the basis vectors.

1

Introduction

To compute the action of a matrix-function on a vector, f (A)v, where A is a
large sparse symmetric matrix of order n, and f is a function for which A is
admissible (i.e., f (A) is deﬁned), the general principle of the Lanczos method
relies on projecting the operator A onto the Krylov subspace Km (A, v) =
Span{v, Av, . . . , Am−1 v} and approximating the action of the function in the
subspace. Here, m is the prescribed dimension of the Krylov subspace and in
practice m
n. The classical Lanczos Algorithm 1 will start with the vector v
and perform a three-term recurrence scheme to construct a symmetric tridiagonal matrix Tm and an orthonormal basis Vm of Km (A, v):


α1 β2



 β2 α2 . . .
 , Vm = [v1 , v2 , . . . , vm ]

Tm = 

.
.
.. .. β 

m
βm αm
and for any j = 1, . . . , m we have the fundamental relations
P.M.A. Sloot et al. (Eds.): ICCS 2003, LNCS 2659, pp. 693–704, 2003.
c Springer-Verlag Berlin Heidelberg 2003

694

R.B. Sidje, K. Burrage, and B. Philippe

VjT Vj = I
AVj = Vj Tj + βj+1 vj+1 eTj
T
Vj AVj = Tj

(1)

where Vj = [v1 , . . . , vj ], Tj is the jth leading submatrix of Tm , ej is the jth column of the identity matrix of appropriate size. The pseudo-code for the Lanczos
process is indicated below.
Algorithm 1: Lanczos(m, A, v)
β = v 2 ; v1 := v/β ; β1 := 0 ;
for j := 1 : m do
p := Avj − βj vj−1 ;
αj := vjT p ; p := p − αj vj ;
βj+1 := p 2 ; vj+1 := p/βj+1 ;
endfor
From this, v = βVm e1 where β = v 2 . To compute w = f (A)v, the underlying
principle is to use the approximation w ≈ w where
w = βVm f (Tm )e1 .

(2)

Thus, the distinctive feature is that the original large problem is converted to
the smaller problem (2) which is more desirable. Moreover the special structure
of Tm (symmetric tridiagonal) means that, when possible, the smaller matrixfunction action f (Tm )e1 can be handled in an eﬃcient, tailor-made manner.
Theory and practice over the years have conﬁrmed that this simpliﬁed process
is amazingly eﬀective for a wide class of problems. In general however, the basis
Vm = [v1 , . . . , vm ] has to be kept for the ﬁnal evaluation of w = βVm f (Tm )e1 .
The basis Vm is a dense matrix of size n × m, and when n is very large, such
a storage is seen as a drawback in certain conﬁgurations. In speciﬁc contexts
such as linear systems (conjugate gradient-type methods) or eigenvalue problems (where eigenvectors are not needed), it is possible to avoid storing the basis
vectors and this is considered as a signiﬁcant strength of the Lanczos scheme. In
principle however, it is also possible to avoid this storage by applying the Lanczos process twice. But the perception that Lanczos storage savings are lost when
evaluating f (A)v has prompted some authors to oﬀer alternative methods outside the Krylov-based paradigm. For instance, Bergamaschi and Vianello [2] used
Chebyshev series expansions to compute the action of the exponential operator
in the case of large sparse symmetric matrices arising after space discretization
of 2D and 3D parabolic PDEs by ﬁnite diﬀerences and ﬁnite elements. The conﬁguration used for their experiments was a 600Mhz Alpha station with 128MB
of RAM. Although the Chebyshev series method requires an a priori estimation
of the leftmost and rightmost eigenvalues of A, and is known to converge slower
than the Krylov approach, they showed that when memory is at premium, some
advantages of the Krylov method may need to be traded.
In this study, we present an augmented Lanczos algorithm which alleviates
the drawback, allowing the Krylov framework to be fully retained when the issue

An Augmented Lanczos Algorithm

695

at hand is really to compute uT f (A)v for given vectors u and v, i.e., when we
are interested in a weighted sum of the action of matrix-function on a vector. As
a particular example, taking u = ei and v = ej allows retrieving the ij-th entry
of f (A). Assuming the matrix is positive deﬁnite, this was used to bound entries
of a matrix function [1,5,6]. In this study however, we do not assume that the
matrix is positive deﬁnite. Another example worth mentioning is the evaluation
of the norm f (A)v 2 which can be handled as a special case. Thus, compared
to a method such as the Chebyshev method, the beneﬁts are two-fold:
– Economy of storage: the Lanczos basis vectors need not be saved. This alleviates the primary drawback that motivated the Chebyshev method, and
– Potential re-usability: the computed data can be re-used to get {uTp f (A)v}
for a family of vectors {up }. Note that because the matrix is symmetric, computing {uTp f (A)v} is algorithmically equivalent to computing {uT f (A)vq }
(but not equivalent to computing {uTp f (A)vq } which would need several
sweeps). The computed data can also be re-used to get uT fp (A)v for a family of functions {fp }. This can be useful when the argument matrix depends
on another variable, e.g., the matrix may belong to the parameterized family
A(t) = tA where t is the ‘time’ variable. As a concrete example, evaluating
T T
u f (tA)v may involve discretizing the integration domain and comput0
ing the matrix function at several knots. Sometimes also, the matrix may
vary in terms of rank-updates – such problems can be seen in Bernstein and
Van Loan [3] though in the unsymmetric context. A careful implementation
of our scheme can oﬀer a suitable re-usable framework where appropriate.

2

The Augmented Lanczos Algorithm

The principle of augmenting a Krylov subspace is well-known in the literature
and has been used in several other contexts. Diﬀerences between approaches usually lie in the strategy by which the augmentation is made. Probably, the most
popular Krylov algorithm obtained via augmentation is the GMRES algorithm
of Saad and Schultz [11] which resulted from taking full advantage of the information computed by the Arnoldi procedure (or FOM – Full Orthogonalization
Method). Our proposed augmented scheme falls in a class of similar approaches.
To get uT f (A)v, the idea is to augment the Lanczos basis Vm as follows. Deﬁne
the orthogonal projector onto Vm as Pm = Vm VmT , and let
Vˆm+1 = [Vm | vˆm+1 ]
where
vˆm+1 =

(I − Pm )u
,
(I − Pm )u 2

ˆ where
and take an approximation in this augmented subspace as uT w ≈ uT w
T
w
ˆ = β Vˆm+1 f (Vˆm+1
AVˆm+1 )e1 .

696

R.B. Sidje, K. Burrage, and B. Philippe

Now, we have the symmetric matrix
VmT Aˆ
vm+1
VmT
T
A[Vm | vˆm+1 ] = T m
T
T
vˆm+1
vˆm+1 AVm vˆm+1
Aˆ
vm+1

T
AVˆm+1 =
Vˆm+1

and expanding further, we get
T
T
T
vˆm+1
AVm = vˆm+1
(Vm Tm + βm+1 vm+1 eTm ) = βm+1 vˆm+1
vm+1 eTm

and letting
and

T
vm+1
βˆm+1 ≡ βm+1 vˆm+1

(3)

T
α
ˆ m+1 ≡ vˆm+1
Aˆ
vm+1

(4)

we see that




T
ˆ
ˆ
ˆ
Vm+1 AVm+1 ≡ Tm+1 = 



0
..
.

Tm
βˆm+1

··· 0

0

0
βˆm+1
α
ˆ m+1




.



Hence Tˆm+1 remains tridiagonal. Furthermore, just as with the standard Lanczos
process, the work at one step is a dovetail update from the work at the previous
step, without the need to store the entire set of basis vectors. Indeed VmT u =
T
T
T
u]T = [(Vm−1
u)T , vm
u]T and (I − Pm )u = (I − Vm VmT )u = (I −
[v1T u, . . . , vm
T
T
vm vm ) · · · (I − v1 v1 )u can be computed incrementally as basis vectors become
available. We now give additional details on how α
ˆ m+1 can be eﬃciently updated
instead of using just (4). Direct calculation shows that
T
Aˆ
vm+1 =
α
ˆ m+1 ≡ vˆm+1

=

uT (I − Pm )A(I − Pm )u
(I − Pm )u 22

T
T
u)(vm
u)
uT Au − uT Vm Tm VmT u − 2βm+1 (vm+1
·
2
(I − Pm )u 2

It is therefore clear that we also have
T
T
T
u − 2βm (vm
u)(vm−1
u)
uT Au − uT Vm−1 Tm−1 Vm−1
α
ˆm =
·
2
(I − Pm−1 )u 2
But further calculation shows that
T
T
T
T
uT Vm Tm VmT u = uT Vm−1 Tm−1 Vm−1
u + 2βm (vm+1
u)(vm
u) + αm (vm
u)2 ,

and hence we end up with the updating formula
α
ˆ m+1 (I − Pm )u

2
2

=α
ˆ m (I − Pm−1 )u

2
2

T
T
T
− 2βm+1 (vm+1
u)(vm
u) − αm (vm
u)2

in which we also have
(I − Pm )u

2
2

= (I − Pm−1 )u

2
2

T
+ (vm
u)2 .

Therefore, the vector u interacts with A only through one extra matrix-vector
product needed to form the initial α
ˆ 2 . And from there subsequent updates are
made by scalar recurrences using quantities readily available.

An Augmented Lanczos Algorithm

697

Remarks
Rather than adding the augmented vector u at the end, we could have put it
at the beginning. And then, we could either 1) orthogonalize u against each
incoming vector as before, or 2) orthogonalize each incoming vector against u.
1. In the ﬁrst case, we would get V˜m+1 = [ˆ
vm+1 | Vm ], with Vm unchanged
and vˆm+1 resulting from orthogonalizing u against each incoming vector as
before. This is therefore equivalent to a permutation V˜m+1 = Vˆm+1 Pm+1
where Pm+1 = [em+1 , e1 , . . . , em ] is the (m + 1)-by-(m + 1) permutation
matrix that produces the eﬀect. It follows that
T
T
T
T
AV˜m+1 = Pm+1
AVˆm+1 Pm+1 = Pm+1
V˜m+1
Vˆm+1
Tˆm+1 Pm+1

and this shows that the tridiagonal form is destroyed. If we leave out vˆm+1
we recover the classical Lanczos quantities, whereas our primary interest in
this study is to explore an augmented approach and summarize our ﬁndings.
2. If u is added at the beginning and each incoming vector is orthogonalized
against it, the resulting matrix from iterating on {u, v, Av, . . .} is not tridiagonal (although the restriction onwards from v is). Augmenting at the beginning is also what is often termed as locking or deﬂation when used as a means
to accelerate the convergence of eigenvalue problems or linear systems.
It is worth noting that when dealing with the same augmented subspace
Km (A, v) + {u}, there exists transformation matrices between the computed
bases. Our proposed approximation scheme oﬀers the advantage of retaining the
standard Lanczos elements. Existing Lanczos implementations would therefore
provide a good starting code base and as we shall see, the scheme is also well
suited for establishing further results. It should also be clear such an augmented
scheme can be applied to the Arnoldi procedure. But, by itself, the Arnoldi
procedure already needs all basis vectors to perform the Gram-Schmidt sweeps.

3

Alternative Approaches

To compute uT f (A)v, our motivation was to avoid keeping the entire set of
Lanczos basis vectors. Nor did we want to apply the process twice and incur
the associated cost. Aside from leaving out u and embedding uT Vm in the classical Lanczos scheme as noted earlier, these goals could be achieved by other
approaches such as the block Lanczos algorithm and the bi-orthogonal Lanczos
algorithm in the ways outlined below.
If started with [v, u], the block Lanczos computes a block tridiagonal matrix
and an orthonormal basis V2m of Km (A, v) + Km (A, u). Since u and v can be
T
T
u and v = V2m V2m
v, an apexpressed in the computed basis as u = V2m V2m
T
proximation to u f (A)v could be retrieved without storing all of V2m . However,
doing a block Lanczos in this context is twice the cost of the augmented scheme,
although the approach satisﬁes a particular theoretical characterization as we
shall see below.

698

R.B. Sidje, K. Burrage, and B. Philippe

Another approach is the bi-orthogonal Lanczos algorithm which is usually
meant for unsymmetric problems. But it is possible to deliberately apply it
here to simultaneously compute a basis of Km (A, v) and a basis of Km (A, u)
from where to approximate uT f (A)v. Below we brieﬂy summarize this wellknown algorithm using its general, unsymmetric notation. We do not dwell here
on the obvious notational simpliﬁcations that can be made since the matrix is
symmetric.
Algorithm 2: Bi-orthogonal Lanczos
β := v 2 ; α := (uT v)/β ;
v1 := v/β ; u1 := u/α ;
for j := 1 : m do
p := Avj ; q := AT uj ;
if j > 1 then
p := p − γj vj−1 ; q := q − βj uj−1 ;
endif
αj := uTj p ;
p := p − αj vj ; q := q − αj uj ; s := q T p ;
βj+1 := |s| ;
γj+1 := sign(s) βj+1 ;
vj+1 := p/βj+1 ; uj+1 := q/γj+1 ;
endfor
Upon completion of this algorithm, we get a tridiagonal matrix


α1 γ2



 β2 α2 . . .
,

Tm = 

.
.
.. .. γ 

m
βm αm
and Vm ≡ [v1 , . . . , vm ] is a basis of Km (A, v), Um ≡ [u1 , . . . , um ] is a basis of
Km (AT , u). The bases are bi-orthogonal, i.e.,
T
Vm = VmT Um = I
Um

(5)

and furthermore we have the relations
AVm = Vm Tm + βm+1 vm+1 eTm
T
Um
AVm = Tm

T
AT Um = Um Tm
+ γm+1 um+1 eTm .

(6)
(7)
(8)

From these, we see that v = βVm e1 and u = αUm e1 and the approximaT
T
tion is computed as uT f (A)v = αβeT1 Um
f (A)Vm e1 ≈ αβeT1 f (Um
AVm )e1 =
T
αβe1 f (Tm )e1 . And this relationship allows us to avoid storing the bases explicitly. In our symmetric context, we replace AT with A. Each step requires two
matrix-vector products, so it is twice as expensive as of the augmented scheme,
but as we shall see below the bi-orthogonal Lanczos algorithm turns out to have
certain theoretical characteristics similar to that of the block Lanczos.

An Augmented Lanczos Algorithm

4

699

Some Properties

The notation used here is that introduced earlier for each relevant method.
We ﬁrst start with the proposed augmented process. In analogy with (1) the
augmented process satisﬁes the following relations
T
Vˆm+1
Vˆm+1 = I
T
AVˆm+1 = Tˆm+1
Vˆm+1
AVˆm+1 = Vˆm+1 Tˆm+1 + (βm+1 vm+1 − βˆm+1 vˆm+1 )eTm − βˆm+1 vm eTm+1 +

vm+1 eTm+1
+ (A − α
ˆ m+1 I)ˆ
T
= Vˆm+1 Tˆm+1 + (I − vˆm+1 vˆm+1
) βm+1 vm+1 eTm +
T
+ (I − vm vm
)Aˆ
vm+1 eTm+1

where em and em+1 are of length m + 1. We have already seen that the ﬁrst and
second relations hold by construction. To get the third relation, we write
vm+1 ]
AVˆm+1 = A[Vm | vˆm+1 ] = [Vm Tm + βm+1 vm+1 eTm | Aˆ
and
T
Vˆm+1 Tˆm+1 = [Vm | vˆm+1 ] ˆ m T
βm+1 em
= [Vm Tm + βˆm+1 vˆm+1 eT
m

βˆm+1 em
α
ˆ m+1
| βˆm+1 vm + α
ˆ m+1 vˆm+1 ]

and the relation comes after some algebraic manipulation. The last follows from
T
T
T
the fact that α
ˆ m+1 = vˆm+1
Aˆ
vm+1 and βm+1 vˆm+1
vm+1 = βˆm+1 = vm
Aˆ
vm+1 .
The basis Vm and the tridiagonal matrix Tm computed by the original Lanczos process are known to satisfy p(A)v1 = Vm p(Tm )e1 for any polynomial p
of degree ≤ m − 1 (see, e.g., [9, Lemma 3.1]). This is still valid with the augmented process since it fully retains the original Lanczos elements. Thus we have
p(A)v1 = Vˆm+1 p(Tˆm+1 )e1 for any polynomial p of degree ≤ m − 1. In fact, we
now show that we can extend this property to a higher degree in our context.
Proposition 1. At the m-th step of the augmented Lanczos process, we have
uT p(A)v1 = uT Vˆm+1 p(Tˆm+1 )e1 for any polynomial p of degree ≤ m.
Proof. Since the result is already true for a degree ≤ m−1, it remains to establish
the degree m. Let pm (z) = a0 + zqm−1 (z) be a polynomial of degree m in which
qm−1 is a polynomial of degree m − 1 and a0 = pm (0) is a scalar. We have
pm (A)v1 = a0 v1 + Aqm−1 (A)v1 = a0 v1 + AVˆm+1 qm−1 (Tˆm+1 )e1
T
) βm+1 vm+1 eTm +
= a0 v1 + Vˆm+1 Tˆm+1 + (I − vˆm+1 vˆm+1
T
+ (I − vm vm
)Aˆ
vm+1 eTm+1

qm−1 (Tˆm+1 )e1

T
= Vˆm+1 pm (Tˆm+1 )e1 + (I − vˆm+1 vˆm+1
) βm+1 vm+1 eTm +
T
+ (I − vm vm
)Aˆ
vm+1 eTm+1 qm−1 (Tˆm+1 )e1

700

R.B. Sidje, K. Burrage, and B. Philippe

Since Tˆm+1 is a tridiagonal matrix of order m + 1 and qm−1 is a polynomial of degree m − 1, we have eTm+1 qm−1 (Tˆm+1 )e1 = 0. Furthermore uT (I −
T
vˆm+1 vˆm+1
)vm+1 = 0.
Proposition 1 means that the approximation is exact if the minimal degree
of v with respect to the matrix A is ≤ m. In general, the a priori error bounds
obtained using functional calculus (as in [7]) for the usual Lanczos scheme for
approximating f (A)v can be transposed here. In particular, it is known from the
theory of matrix functions [8, Chap.6] that
f (A)v = pn−1 (A)v

(9)

where pn−1 is the Hermite interpolation polynomial of f at λ(A), the spectrum
of A. Hence,
uT w
ˆ = βuT Vˆm+1 f (Tˆm+1 )e1 = βuT Vˆm+1 pm (Tˆm+1 )e1 = uT pm (A)v
with pm being the Hermite interpolation polynomial (of degree m) at the modiﬁed ‘Ritz’ values 1 . The error can be written in a polynomial form as
|uT f (A)v − βuT Vˆm+1 f (Tˆm+1 )e1 | = |uT pn−1 (A)v − uT pm (Tˆm+1 )v|
The theory of Krylov subspaces [10] shows that λ(Tˆm+1 ) not only approximates
a larger subset of λ(A) as m increases but also approximates it more accurately.
The next results shed some light on the behavior of the bi-orthogonal Lanczos
process and the block Lanczos process.
Proposition 2. Let Tm be the resulting tridiagonal matrix at the m-th step
of the bi-orthogonal Lanczos algorithm, then uT1 p(A)v1 = eT1 p(Tm )e1 for any
polynomial p of degree ≤ 2(m − 1).
Proof. Any polynomial p of degree ≤ 2(m − 1) can be factored as
p(z) = q(z)r(z) + s(z) where q, r, s are polynomials of degree <= m −
1. From this, if follows that uT1 p(A)v1 = (q(A)u1 )T r(A)v1 + uT1 s(A)v1 =
T
(Um q(Tm )e1 )T Vm r(Tm )e1 + eT1 Um
Vm sm (Tm )e1 and the bi-orthogonality property (5) yields the result.
Proposition 3. Let T2m be the resulting block-tridiagonal matrix at the m-th
step of the block Lanczos algorithm started with MGS(v, u) where MGS stands for
the Modiﬁed Gram-Schmidt procedure, then uT p(A)v = β(ω1 eT1 +ω2 eT2 )p(T2m )e1
for any polynomial p of degree ≤ 2(m − 1) where ω1 and ω2 are the coeﬃcients
relative to u from the MGS(v, u) procedure.
Proof. The block Lanczos procedure will generate an orthonormal basis for
Km (A, v) + Km (A, u), hence the assertion is already true for any polynomial
of degree ≤ m − 1. The rest of the proof proceeds as in Proposition 2 using the
basis of dimension 2m generated by the block Lanczos algorithm.
1

These are not really the Ritz values since the tridiagonal matrix is augmented in a
special way. But since the eigenvalues of Tˆm+1 embed the Ritz values according to
Cauchy’s interleaving theorem, they remain close.

An Augmented Lanczos Algorithm

701

We should draw the attention of the reader on the fact that an apparent
higher polynomial order of an approach is not synonymous with higher accuracy.
A higher degree polynomial does not necessarily convey a better approximation
to a matrix function. An illustrative case in point is the Taylor series expansion
which can still converge slowly even after summing a large number of terms. As
relation (9) showed, the underlying approximations are going to depend on how
good the spectrum of A is approximated in the respective situations. Since the
three approaches usually yield comparable approximations to the spectrum, the
augmented approach stands out due to its cheaper cost.

5

Numerical Experiments

There are a number of matrix functions that arise frequently in the literature.
In our ﬁrst experiment, we consider the sine function f1 (x) = sin x. In the
second experiment, we consider an example from Druskin
√ and Knizhnerman [4]:
the exponential function with square-root f2 (x) = e−θ x , θ ≥ 0. This example
results from the boundary value problem
Aw −

d2 w
= g(θ)v.
dθ2

The function f2 is the solution of the boundary value problem w(0) = v,
w(+∞) = 0 with g = 0. References in Druskin and Knizhnerman [4] include
applications where this problem arises, for example in geophysical computed
tomography. In our experiments, we used the functions on symmetric matrices
from the Harwell-Boeing collection.
Let fexact be the computed value of uT f (tA)v. For the purpose of the experiments, it was computed by diagonalizing the matrix. Each ﬁgure includes three
error curves corresponding to fexact −fmth approx 2 fexact , where the m-th approximation, fmth approx , is either the approximation at the m-th iteration with
the proposed augmented scheme (plain curve with circles), the approximation
with the bi-orthogonal Lanczos (dashed curved with triangles), or the classical
approximation uT w
˜ = βuT Vm f (tTm )e1 (dotted curve with crosses).
On the reported examples, all methods tend to converge very well as generally observed with Krylov-based approximations to matrix functions. The ﬁrst
example has some interesting observations. The bi-orthogonal Lanczos scheme
exhibits faster convergence at the beginning but is subsequently caught up by
the other methods. What is even more noticeable is that the convergence of the
bi-orthogonal scheme stalls at some point. Further investigation showed us that
a marked lost of orthogonality started in the bi-orthogonal process and so the
process was contaminated from that point onwards (since we were not dealing
with a production code, our testings did not involve re-orthogonalization techniques that may be needed with Lanczos-based algorithms). Since in general,
the bi-orthogonal scheme is more sensitive to lost of orthogonality, that is yet
another unfavorable argument against using it without cross-checking.

702

5.1

R.B. Sidje, K. Burrage, and B. Philippe

Example 1
0

100

200

300

400

500

600

0

100

200

300
400
nz = 2474

500

600

1
0
−1

0

500

1000

1500

2000
2500
Spectrum

3000

3500

4000

4500

Sparsity pattern and spectrum
Relative error in the approximation to uTsin(A)v for A of order n=662

2

10

0

10

−2

Relative error at the m−th iteration

10

−4

10

−6

10

−8

10

−10

10

−12

10

−14

10

0

5

10

15
m

20

25

30

Error curves
Fig. 1. We consider f1 (x) = sin x and compute uT f1 (tA)v for random u and v, and a
662-by-662 matrix A known as 662bus in the Harwell-Boeing collection. The sparsity
pattern and spectrum are shown on the left side (the spectrum lies on the real line
since A is symmetric). We took t = 0.01, for A is of large norm.

An Augmented Lanczos Algorithm

5.2

703

Example 2
0
50
100
150
200
250
300
350
400
450
0

100

200
nz = 5172

2

3
Spectrum

300

400

1
0
−1

0

1

4

5

6
5

x 10

Sparsity pattern and spectrum
Relative error in the approximation to uTf (A)v for A of order n=468
2

0

Relative error at the m−th iteration

10

−1

10

−2

10

−3

10

0

5

10

15
m

20

25

30

Error curves
√

Fig. 2. We consider f2 (x) = e−θ x , θ = 0.01 and compute uT f2 (A)v where u and v
are randomly generated, and A is the 468-by-468 matrix known as nos5 in the HarwellBoeing collection

704

6

R.B. Sidje, K. Burrage, and B. Philippe

Conclusion

Evaluating f (A)v with the usual Lanczos process has the drawbacks of the storage of the basis vectors or the application of the process twice. Despite the
reputed convergence properties of the method, its drawbacks deter some users
from using the process in certain situations. We have presented a cost-eﬀective
approach for evaluating uT f (A)v using a suitably augmented Lanczos process
that does not have these drawbacks. The proposed method still has the advantage of preserving the original elements, and thus it inherits from the same
foundation and oﬀers the recognized quality of Krylov-based methods for approximating matrix functions. Some properties were established and placed in
the broader perspective of alternative methods such as the block Lanczos and
the bi-orthogonal Lanczos algorithms. Numerical results were made to test the
quality of the method on some frequently used matrix functions.

References
1. M. Benzi and G. Golub. Bounds for the entries of matrix functions with applications to preconditioning. BIT Numerical Mathematics, 39(3):417–438, 1999.
2. L. Bergamaschi and M. Vianello. Eﬃcient computation of the exponential operator
for large, sparse, symmetric matrices. Numer. Linear Algebra Appl., 7:27–45, 2000.
3. D. S. Bernstein and C. F. Van Loan. Rational matrix functions and rank-1 updates.
SIMAX, 22(1):145–154, 2000.
4. V. Druskin and L. Knizhnerman. Extended krylov subspaces: Approximation of
the matrix square root and related functions. SIMAX, 19(3):755–771, 1998.
5. G. Golub and G. Meurant. Matrices, moments and quadrature. In D. F. Grifﬁths and G. A Waston, editors, Numerical Analysis 93. Pitman Research Notes in
Mathematics 303, Longman Scientiﬁc and Technical, 1993.
6. G. Golub and G. Meurant. Matrices, moments and quadrature. ii. how to compute
the norm of the error in iterative methods. BIT, 37(3):687–705, 1997.
7. M. Hochbruck and Ch. Lubich. On Krylov subspace approximations ot the matrix
exponential operator. SIAM J. Numer. Anal., 34(5):1911–1925, October 1997.
8. R. A. Horn and C. R. Johnson. Topics in Matrix Aanalysis. Cambridge University
Press, Cambridge, 1991.
9. Y. Saad. Analysis of some Krylov subspace approximations to the matrix exponential operator. SIAM J. Numer. Anal., 29(1):208–227, 1992.
10. Y. Saad. Numerical Methods for Large Eigenvalue Problems. John Wiley & Sons,
Manchester Univ. Press, 1992.
11. Y. Saad and M. H. Schultz. GMRES: A generalized minimal residual algorithm for
solving nonsymmetric linear systems. SIAM J. Sci. and Stat. Comp., 3(7):856–869,
July 1986.

