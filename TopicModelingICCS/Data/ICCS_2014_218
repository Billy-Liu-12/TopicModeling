Procedia Computer Science
Volume 29, 2014, Pages 821–830
ICCS 2014. 14th International Conference on Computational Science

COFADMM: A Computational features selection with
Alternating Direction Method of Multipliers
Mohammed El Anbari1 , Sidra Alam2 , and Halima Bensmail1∗
1

Qatar Computing Research Center
melanbari@qf.org.qa, hbensmail@qf.org.qa
2
Carnegie Mellon University @ Qatar
sidra.m.alam@gmail.com

Abstract
Due to the explosion in size and complexity of Big Data, it is increasingly important to be
able to solve problems with very large number of features. Classical feature selection procedures involves combinatorial optimization, with computational time increasing exponentially
with the number of features. During the last decade, penalized regression has emerged as an
attractive alternative for regularization and high dimensional feature selection problems. Alternating Direction Method of Multipliers (ADMM) optimization is suited for distributed convex
optimization and distributed computing for big data. The purpose of this paper is to propose
a broader algorithm COFADMM which combines the strength of convex penalized techniques
in feature selection for big data and the power of the ADMM for optimization. We show that
combining the ADMM algorithm with COFADMM can provide a path of solutions eﬃciently
and quickly. COFADMM is easy to use, is available in C, Matlab upon request from the
corresponding author.
Keywords: features selection, lasso, least angle regression algorithm, coordinate descent algorithm,
ADMM Algorithm

1

Introduction

During Big data era: wiki, WSJ, white house, McKinsey report to name a few, many challenges
are raised related to the methodology when dealing with the big number of variables, in the
eﬃciency when we face data with large sample size and/or large number of variables, and in the
memory when sample size is large and needs a distributed computing to solve it via MapReduce
or Hadoop. In fact, ADMM has been proposed for a variety of machine learning problems of
recent interest, including the Lasso, sparse logistic regression, basic pursuit, covariance matrix
estimation, support vector machines among many others as an easy and elegant tool of optimization. For regression problems of type y = Xβ + ε, where X = (x1 , ..., xp ) is an n × p matrix
∗ corresponding

author.

Selection and peer-review under responsibility of the Scientiﬁc Programme Committee of ICCS 2014
c The Authors. Published by Elsevier B.V.
doi:10.1016/j.procs.2014.05.074

821

COFADMM Algorithm

El Anbari, Alam, Bensmail

of p feature vectors of dimension n, ε is a random error vector with E(ε) = 0 and β is a vector of
unknown parameters to be estimated, a great deal of attention has been given to the estimation
and feature selection in high-dimensional linear regression models. Several important methods
among others have been proposed. Most of these methods are based on penalized least squares,
which perform estimation and feature selection in a continuous fashion by shrinking some of the
regression coeﬃcients towards zero and setting some of them to exactly zero. The most popular
regularization approach is the least absolute shrinkage and selection operator called Lasso [10],
which is based on the penalized least squares by the 1 penalty on the vector parameter. [5]
developed an eﬃcient algorithm called Lars (Least Angle Regression) to ﬁnd the entire solution
path for the Lasso. Despite its good properties, the Lasso has serious limitations namely (a) it
selects at most n features if p > n; (b) produces signiﬁcant bias towards 0 for large regression
coeﬃcients; (c)fails to do grouped selection and tends to select one feature from a group and
ignore the others in the presence of highly correlated features; (d)ignores a structured data as
in the case of highly ordered features.
Many methods have been proposed to deal with these limitations. The most popular among
these methods is the elastic net [12], which uses both the Ridge and Lasso constraints. In the
same spirit, [1] proposed a method called Oscar, which is based on the penalized least squares
with a penalty function combining the 1 and the pairwise ∞ norms. Oscar forces some of the
grouped coeﬃcients to be identically equal, encouraging correlated features that have similar
eﬀect on the response to form clusters represented by the same coeﬃcients. However, the computation of the Oscar estimates is based on a sequential quadratic programming algorithm which
is slow for large p. In the same setting, [7] considered the Smooth-Lasso procedure (SLasso), a
modiﬁcation of the Fused-Lasso procedure [9], in which a second 1 Fused penalty is replaced
by the smooth 2 norm penalty. From an algorithmic point of view, to ﬁnd the solutions in [12],
[6], [3] and [7], each of the corresponding optimization problem can be seen as a Lasso problem
by introducing new observations, and then use Least Angle Regression algorithm (LARS) or
coordinate-descent (Gauss-Seidel) algorithm. It is interesting to note that (i) for p >> n the
augmented data set has p+n observations and p variables, which can slow the computation considerably; (ii) if the original design matrix is normalized, there is no guarantees the augmented
design matrix will behave similarly, which can cause a loss of a part of the interpretation of the
big data; and (iii) the coordinate-descent algorithm proceeds by ”one at a time” philosophy,
e.g. it minimizes the loss function of βj while maintaining components {βk , k = j} ﬁxed at
their actual values, in this case we cannot develop Gauss-Seidel for a grouped variable selection
problem. To overcome these limitations, we derive a uniﬁed alternating direction method of
multipliers based algorithm (COFADMM) to handle Big Data features selection with lasso-type
estimator. We propose a doubly regularized model with a general penalty term of the form:
p

ω
ˆ j |βj |,

(μ/2)β t Qβ + λ

(1)

j=1

where λ, μ ≥ 0 are two tuning parameters, ω
ˆ = (ˆ
ω1 , ..., ω
ˆ p )t and Q = (qij )1≤i,j≤p are weights
associated with the 1 and 2 norms respectively, which are ﬁxed in advance.
The advantage of our algorithm are: (1)Provide a general frame to deal with the limitations of
un-weighed versions of lasso-type estimates. A weighted version possesses the oracle properties
of selecting the subset of interesting variables with a proper choice of the weights and increasing
the number of hits and decreasing the number of false positives. (2) Combine the strengths of
Lasso and a quadratic penalty designed to capture additional structure on the features in high
dimensional setting. (3) Develop an easy and fast algorithm using the ”Alternating Direction
Method of Multipliers” approach to ﬁnd optimal estimator without augmenting or normalizing
822

COFADMM Algorithm

El Anbari, Alam, Bensmail

data.
In the following, we emphasize on the advantage of using ADMM on a general lasso-type model
with a general penalty term and we will show later that this approach is powerful as it provides
fast and optimal solution (closed form).We will discuss brieﬂy some results on its convergence
and stoping criterion in Section 2. The problem formulation and its corresponding ADMM
algorithm are considered in Section 3. Section 4 is devoted to numerical experimentations on
an artiﬁcial and a real data with a large number of features. We end the paper with a brief
discussion in Section 5.

2

Alternating Direction Method of Multipliers

Recently, the alternating direction method of multipliers (ADMM) has been revisited and successfully applied to solving large scale problems arising from diﬀerent applications. In this
section we give an overview of ADMM. Consider the following optimization problem:
minimize
subject to

f (β) + g(ξ)
β − ξ = 0,

(2)

where f and g are two convex functions and β, ξ ∈ Rp . In this optimization problem, we have
two sets of variables, with separable objective. The augmented Lagrangian for this problem is:
Lτ (β, ξ, δ) = f (β) + g(ξ) + δ t (β − ξ) + (τ /2) β − ξ 22 ,
where δ is the dual variable for the constraint β − ξ = 0 and τ > 0 is a penalty parameter.
The augmented Lagrangian methods were developed in part to bring robustness to the dual
ascent method, and in particular, to yield convergence without strong assumptions like strict
convexity or ﬁniteness of f and g.
At iteration k, the ADMM algorithm consists of the three steps:
β k+1

:=

arg min Lτ (β, ξ k , δ k ),

ξ k+1

:=

arg min Lτ (β k+1 , ξ, δ k ), //ξ-minimization

(4)

δ k+1

:=

δ k + τ (β k+1 − ξ k+1 ). //dual-update

(5)

β

//β-minimization

(3)

ξ

• In the ﬁrst step of the ADMM algorithm, we ﬁx ξ and δ and minimize the augmented
Lagrangian over β.
• In the second step, we ﬁx β and δ and minimize the augmented Lagrangian over ξ.
• Finally, we update the dual variable δ.
If we consider the scaled dual variable η = (1/τ )δ and the residual r = η − ξ, the ADMM
algorithm can be expressed on its scaled dual form as (we will use the scaled form in the
paper):
β k+1

:=

arg min f (β) + (τ /2) β − ξ k + η k

ξ k+1

:=

arg min g(ξ) + (τ /2) β k+1 − ξ + η k

η k+1

:=

η k + β k+1 − ξ k+1 .

β
ξ

2
2

;
2
2

(6)
;

(7)
(8)

823

COFADMM Algorithm

Stopping criteria

El Anbari, Alam, Bensmail

The primal and dual residuals at iteration k have the forms:
ekpri = (β k − ξ k ),

ekdual = −τ (η k − η k−1 ).

The ADMM algorithm terminates when the primal and dual residuals satisfy stopping criterion.
A typical stopping criterion is given in [2] where the authors propose to terminate when ekpri ≤
pri
, ekdual ≤ dual . The tolerances pri > 0 and dual > 0 can be chosen using an absolute
√
√
and relative criterion, such as pri = p abs + rel max{ β k 2 , η k 2 }; and dual = p abs +
rel
k
abs
rel
τ η 2 , where
> 0 and
> 0 are absolute and relative tolerances. A reasonable value
for the relative stopping criterion is rel = 10−3 or 10−4 , while abs depends on the scale of the
typical variable (see [2] for details).

3

Problem formulation and method

In this section we derive an eﬃcient Alternating Direction Method of Multipliers algorithm for
a class of Lasso-type estimators with a general penalty term of the form (1). To check if a
variable is important or not, we estimate its coeﬃcient βˆ that minimizes (1) and is a solution
of the generic problem:
1
βˆCOFADMM (λ, μ) = arg min y − Xβ
β 2

2
2

p

+

μ t
β Qβ + λ
ω
ˆ j |βj |,
2
j=1

(9)

where λ, μ are two non negative tuning parameters, Q is a positive semi-deﬁnite matrix. Equation (9) combines the strengths of regularized techniques of type Lasso and a quadratic penalty
designed to capture additional structure on the features. When ω
ˆ j = 1, it is straightforward to
show that all type of lasso models (Lasso, Enet, Slasso, L1Cp and Wfusion) are particular case
of (9) using an augmented data reparameterization of the form
X∗(n+p)×p =

X
√ t
μL

; Q = LLt ;

∗
y(n+p)
=

y
0

,

Therefore any eﬃcient algorithm developed to ﬁnd the whole solution path of the Lasso like
least angle regression or coordinate descent algorithm can be applied. Unfortunately, the good
properties of the two optimization techniques are overshadowed by the diﬃculties (i), (ii) and
(iii). To deal with those problems, we propose to solve (9) using the ADMM algorithm. The
idea is simple and straightforward. First, we propose to re-write (9) on the following ADMM
form:
1
y − Xβ
2

2
2

p

ω
ˆ j |ξj |

+ (μ/2)β t Qβ + λ
j=1

subject to β − ξ = 0.

(10)

p
If we write f (β) = (1/2) y − Xβ 22 + (μ/2)β t Qβ, g(ξ) = λ j=1 ω
ˆ j |ξj | and ω
ˆ j = (|βˆj | + 1/n)−1
then (9) becomes (2). Here we can see that f and g are two convex functions. Applying the
ADMM algorithm to (10), we have to do the following three steps at each iteration:

824

COFADMM Algorithm

El Anbari, Alam, Bensmail

The β-minimization step.
This step updates β k by:
β k+1

:=

arg min f (β) + (τ /2) β − ξ k + η k

:=

arg min (1/2) y − Xβ

:=

β

β

Xt X + μQ + τ Ip

−1

2
2

2
2

+ (μ/2)β t Qβ + (τ /2) β − ξ k + η k

× Xt y + τ (ξ k − η k )

2
2

(11)

The ξ-minimization step.
This step updates ξ k by:
ξ k+1

:=
:=

arg min g(ξ) + (τ /2) β k+1 − ξ + η k 22
ξ
⎧
⎨ p
τ k+1
β
ω
ˆ j |ξj | +
− ξ + ηk
arg min λ
ξ ⎩
2
j=1

2
2

⎫
⎬
⎭

.

We show in the appendix that the solution consists of updating each component ξjk for j = 1, ..., p
by:
ξjk+1

:=

sign(βjk+1 + ηjk ) max |βjk+1 + ηjk | −

:=

S λωˆ j βjk+1 + ηjk ,

λˆ
ωj
,0
τ
(12)

τ

where

⎧
⎨ a − κ if a > κ
0 if |a| ≤ κ
Sκ (a) = (1 − κ/|a|)+ a =
⎩
a + κ if a < −κ

is the soft thresholding function introduced and analyzed by [4]. The dual-update step is
straightforward and consists of updating η k by η k+1 := η k + β k+1 − ξ k+1 .
It is worth to notice that since τ > 0, μ ≥ 0, Xt X and Q are positive semi-deﬁnite matrices,
(Xt X + μQ + τ Ip ) is always invertible. If p > n, let M = μQ + τ Ip , to alleviate the cost of
calculations, we can exploit the Woodbury formula for (Xt X + M )−1 . The following algorithm
shows the complete details of COFADMM:
Tuning parameters selection In practice, it is important to select appropriate tuning parameters in order to obtain a good prediction precision and to control the amount of sparsity
in the model. Choosing the tuning parameters can be done via minimizing an estimate of the
out-of-sample prediction error. If a validation set is available, this can be estimated directly.
Lacking a validation set one can use 10-fold cross validation. In our experimentations λ takes
100 logarithmically equally spaced values, μ ∈ {0, 0.1, 1, 10, 100} and γ ∈ {0.5, 1, 2.5, 5, 25}.
Algorithm 1 describes COFADMM with ADMM steps and Table 1 summarizes COFADMM as
a general form of diﬀerent lasso-type model.
825

COFADMM Algorithm

El Anbari, Alam, Bensmail

Initialize the variables: β 0 = 0, ξ 0 = 0, η 0 = 0 ;
Select a scalar τ > 0 ;
while k = 0, 1, 2, ... until convergence do
if j = 0 to p then
−1
Xt y + τ (ξ k − η k ) ;
β k+1 := (Xt X + μQ + τ Ip )
ξjk+1 := sign(βjk+1 + ηjk ) max |βjk+1 + ηjk | −

λˆ
ωj
τ ,0

;

:= η + β
−ξ
η
else
M = μQ + τ Ip ;
Use Woodbury for (Xt X + M )−1
end
end
Algorithm 1: Description of COFADMM with ADMM steps.
k+1

Method
M1
M2

k

k+1

k+1

TP
λ ≥ 0, μ = 0
λ ≥ 0, μ ≥ 0

M3

λ ≥ 0, μ ≥ 0

M4

λ ≥ 0, μ ≥ 0
⎛

M5

γ>0

1
p

Q
Identity matrix of order p
⎛
1 −1 0 · · ·
0
⎜
..
.
.
⎜ −1 2 −1
.
.
⎜
⎜
.
..
Q = ⎜ 0 −1 2
0
⎜
⎜ .
.
.
.
..
..
. . −1
⎝ ..
0 ···
0 −1 1
1
2 s=i 1−ρ
i=j
2 ,
is
qij =
ρij
−2 1−ρ2 .
i=j
w1k

⎜
⎜ −s12 w12
×⎜
⎜
..
⎝
.
−s1p w1p

−s12 w12
w2k
..
.
···

ij

···

···
..
.
−sp−1,p wp−1,p

⎞
⎟
⎟
⎟
⎟
⎟
⎟
⎟
⎠

−s1p w1p
..
.
−sp−1,p wp−1,p
wpk

⎞
⎟
⎟
⎟
⎟
⎠

Table 1: Summary of the ﬁve regularization methods as particular case of COFADMM.
M1 states for COFADMMLasso , M2 for COFADMMEnet , M3 COFADMMSlasso , M4 for
COFADMML1cp and M5 for COFADMMW f usion . TP is used for tunning parameters.

4

Performance

All the experiments are performed on a PC machine with Intel Core i7 CPU and 8 GB RAM
under Matlab R2009a. In this section, we present one artiﬁcial study and a Glioblastoma
microarray data set analysis to illustrate the performance of the COFADMM algorithm under
various conditions (time for big data and selection of variables with collinearity). In both
numerical experimentations τ is ﬁxed and μ ∈ {0, 0.1, 1, 10, 100}, so we can catch few initial
factorizations of (Xt X + μQ + τ Ip )−1 to make subsequent iterations much cheaper.
826

COFADMM Algorithm

4.1

El Anbari, Alam, Bensmail

Performance on artiﬁcial data.

This example is the same considered in [2]. The explanatory matrix has p = 50000 features
and n = 15000 observations. The data is generated as follows. We ﬁrst choose Xij ∼ N (0, 1)
and then normalize the columns to have unit 2 norm. The regression coeﬃcient β ∈ Rp is
generated with 100 nonzero components, each sampled from a N (0, 1). The noise vector is
a N (0, 10−3 In ). In terms of timings, it is clear that COFADMM is the winner (see Table
2), followed by lasso-type models with Gauss-Seidel, while lasso-type models with LARS are
very slow for this high dimensional example. The coordinate-descent slows down under high
correlation. We also note that despite the computational advantage, coordinate-descent suﬀers
from the limitations (ii) and (iii) cited in the introduction.
Method
COFADMM-Lasso
COFADMM-Enet
COFADMM-L1cp
COFADMM-Wfusion
COFADMM-Slasso

Tuning parameters
λ = λ0 = Xt y ∞
1
λ = λ0 , μ = 100
1
λ = λ0 , μ = 100
1
λ = λ0 , μ = 100 , γ = 25
1
λ = λ0 , μ = 100

N o.
15
35
38
37
34

T
8.00
5.12
6.13
6.73
5.44

Method
Lasso
Enet
L1cp
Wfusion
Slasso

T(GS)
57.58
6.30
67.53
4.56
3.79

T(LARS)
800.88
> 7602.35
> 24h
> 24h
> 24h

Table 2: N o. is the number of iteration to convergence of COFADMM and T (GS) and T (LARS)
are the time in seconds of lasso-type models using Gauss-Seidel and LARS.

4.2

Performance on Real Data.

American Association of Neurological Surgeons (AANS) deﬁne astrocytoma as the most invasive
type of glial tumor, because of their rapid growth potential, and their spread to nearby brain
tissue. the median survival rate is three months without therapeutic intervention, and by
optimal therapy, resection, radiation and chemotherapy, the survival can be extended to ﬁfteen
months, and fewer than 25% of patients surviving up to two years [8].
Global gene expression data from a set of clinical tumor samples of n = 111 are obtained
by high-density Aﬀymetrix arrays. Expression values of 360 genes are available. We use the
logarithm of time to death as the response variable. As mentioned before, due to the large
number of the genes, traditional algorithms fail to analyze this data. Regularization methods
of type-lasso selected no more than 40 genes and failed to include the groups of signiﬁcant genes
while COFADMMLasso , COFADMMEN et , COFADMML1CP and COFADMMW f usion selected
45, 53, 58 and 50 respectively including signiﬁcant genes. Due to the grouping eﬀect, our method
selected signiﬁcant genes and revealed that some of the genes that are negatively associated
with survival are expressed in neurons (VSNL1), in cytoplasm and/or nucleus (S100A4), and
in plasma membrane (RGS3). In fact, (VSNL1) is a member of recovering family and neuronal
calcium-sensor protein, (S100) proteins is a group of calcium-binding proteins which has a
crucial role in motility features of tumor astrocytes by modiﬁcations in organization of actin
cytoskeleton and the expression of its diﬀerent regulators. Finally, (RGS) gene regulates the
duration of cell signaling and is ubiquitous negative regulators of G signaling by stimulating
the rate of GTP hydrolysis on G protein alpha subunits. Moreover, Bassoon (BSN) gene which
was identiﬁed to be positively associated with the patients’ survival by COFADMM has two
double zinc ﬁnger domains located in the amino terminal part and three coiled-coil domains
that may play a role in its interaction with other presynaptic proteins, and harbors a stretch
of polyglutamine encoded by CAG repeats. The localization of this protein suggest its role
827

COFADMM Algorithm

El Anbari, Alam, Bensmail

in the structural and functional organization of the synaptic vesicle cycle, and the release of
neurotransmitter by calcium-triggered exocytosis [11].

5

Conclusion and Outloook

In this paper, we have adapted the ADMM algorithm for a class of Lasso-type estimators in the
context of linear regression models and proposed a model with a general penalty term to guard
against sparsity for high-dimensional features, a very challenging problem of big data. We have
seen that the proposed algorithm can provide eﬃciently the estimators for a grid of values of the
tuning parameters. It has been demonstrated through simulated and a Glioblastoma data set,
that the proposed algorithm gives good performances in both prediction and feature selection
viewpoints in a very competitive computational time. The algorithm is particularly useful for
situations such that the number of regressors is much larger than sample size. The adaptation
of this algorithm to other penalized techniques in the context of linear models; extensions to
generalized linear models and support vector machine with more complex penalties is a work
in progress.

6

Acknowledgments

We thank Zaki Mohammed from RPI for his critical comments which helped improve the quality
of the manuscript.

References
[1] H.D. Bondell and B. J Reich. Simultaneous regression shrinkage, feature selection and clustering
of predictors with oscar. Biometrics., 64:115–123, 2007.
[2] E. Chu B. Peleato Boyd, S.N. Parikh and J. Eckstein. Distributed optimization and statistical
learning via the alternating direction method of multipliers. Foundations and Trends in Machine
Learning, 3(1):1–122, 2011.
[3] Z. J. Daye and X. J. Jeng. Shrinkage and model selection with correlated variables via weighted
fusion. J. Neurochem., 53:1284–1298, 2009.
[4] D. Donoho and I Johnstone. Ideal spatial adaptation by wavelet shrinkage. Biometrika., 81(3):425–
455, 1994.
[5] T. Johnstone I. Efron, B. Hastie and R. Tibshirani. Least angle regression. Annals of Statistics.,
32:407–499, 2004.
[6] M. El Anbari and A. Mkhadri. Penalized regression with a combination of the l1 norm and the
correlation based penalty. Sankyha B Journal., pages 1–21, 2013.
[7] M. Hebiri and S. van De Geer. The smooth-lasso and other l1 + l2 penalized methods. Electronic
Journal of Statistics., 5:1184–1226, 2011.
[8] Philipp-Niclas Pfenning. RGS4, CD95L and B7H3: Targeting evasive resistance and the immune
privilege of glioblastoma. PhD thesis, Faculties for the Natural Sciences and for Mathematics of
the Ruperto-Carola University of Heidelberg, Germany, 2011.
[9] M. Rosset S. Zhu J. Tibshirani, R. Sunders and K Knight. Sparsity and smoothness via the fused
lasso. Journal of the Royal statistical Society, B., 67:91–108, 2005.
[10] R. Tibshirani. Regression shrinkage and selection via the lasso. Journal of the Royal statistical
Society, B., 58:267–288, 1996.

828

COFADMM Algorithm

El Anbari, Alam, Bensmail

[11] Langnaese K. Richter K. Kindler S. Soyke A. Wex H. Smalla K.H. Kmpf U. Frnzer J.T. Stumm
M. Garner C.C. Gundelﬁnger E.D. Tom Dieck S., Sanmarti-Vila L. Bassoon, a novel zinc-ﬁnger
cag/glutamine-repeat protein selectively localized at the active zone of presynaptic nerve terminals.
The Journal of Cell Biology., 142(2):499–509, 1998.
[12] H. Zou and T. Hastie. Regularization and variable selection via the elastic-net. Journal of the
Royal statistical Society, B., 67:301–320, 2005.

Appendix
Proof of the β-minimization step
β k+1

=

arg min (1/2) y − Xβ

2
2

+ (μ/2)β t Qβ + (τ /2) Aβ + Bξ k − c + η k

=

arg min (1/2) y − Xβ

2
2

+ (μ/2)β t Qβ + (τ /2) β + η k − ξ k

β

β

2
2

2
2

Let T1 (β) = 12 y − Xβ 22 + μ2 β t Qβ, T2 (β) = τ2 β + η k − ξ k 22 , and υ = η − τ ξ, then we have
T1 (β) = 12 β t (Xt X + μQ)β − yt Xβ + 12 y 22 and T2 (β) = τ2 β t β + 2(η k − ξ k )t β + η k − ξ k 22 .
So, the β-minimization step is equivalent to minimize
T (β)

T1 (β) + T2 (β)
1
1 t
β Xt X + μQ + τ Ip β + τ υ k − yt X β +
=
2
2

=

y

2
2

+ τ υk

2
2

The partial diﬀerential of T with respect to β is (Xt X + μQ + τ Ip ) β + τ υ k − yt X . Since
β k+1 is the minimizer of T , we must have (Xt X + μQ + τ Ip ) β k+1 + τ υ k − yt X = 0. Finally,
−1
β k+1 = (Xt X + μQ + τ Ip )
Xt y + τ υ k
Proof of the ξ-minimization step For the ξ-minimization step, we invoke subdiﬀerential
calculus. Recall that it consists of updating ξ k by:
ξ k+1

=
=

τ k+1
β
arg min g(ξ) +
− ξ + η k 22
ξ
2
⎧
⎫
⎨λ p
⎬
1
ω
ˆ j |ξj | + ξ − (β k+1 + η k ) 22 ,
τ arg min
⎭
ξ ⎩τ
2
j=1

So the ξ-minimization step is equivalent to minimize the function h(ξ) = (1/2) ξ − dk 22 +
p
λ
ˆ j |ξj | over ξ where dk = β k+1 + η k . The optimization problem above is convex. For a
j=1 ω
τ
minimizer ξ ∗ of h(.), it is necessary and suﬃcient that the subdiﬀerential (denoted ∂h(ξ ∗ )) at
ξ ∗ contains zero. Now, for each index j, either ξj∗ = 0 or ξj∗ = 0. We begin with the case ξj∗ = 0.
λˆ
ω
This means that the ordinary ﬁrst derivative at ξ ∗ has to be zero: ξj∗ − dkj + τ j sign(ξj∗ ) = 0,
λˆ
ω
then ξj∗ = dkj − τ j sign(ξj∗ ).
Since we assume ξj∗ = 0, this means that either dkj >

sign(ξj∗ ) = sign(dkj ). In both cases, we
So now we know, that for each index j,
λˆ
ω
dkj by τ j towards 0.

λˆ
ωj
λˆ
ωj
k
τ or dj < − τ , and this means that
λˆ
ω
λˆ
ω
have: ξj∗ = dkj − τ j sign(ξj∗ ) = sign(dkj ) |dkj | − τ j .
λˆ
ω
if |dkj | is greater than τ j , then ξj∗ is simply shrinking

829

COFADMM Algorithm

El Anbari, Alam, Bensmail

On the other hand, if ξj∗ = 0, the subdiﬀerential at ξ ∗ has to include the zero element. That is:
λˆ
ω
λˆ
ω
if ξj∗ = 0 : ξj∗ −dkj + τ j e = 0 for some e ∈ [−1, 1]. But this is equivalent to |dkj | ≤ τ j if ξj∗ = 0.
Putting all the cases together, we get that: ξj∗ = sign(dkj ) max |dkj | −

λˆ
ωj
τ ,0

where Sκ (.) is the soft thresholding function introduced and analyzed by [4].

830

= S λωˆ j (dkj ),
τ

