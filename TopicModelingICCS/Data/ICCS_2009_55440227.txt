Support for Urgent Computing Based on
Resource Virtualization
´
Andr´es Cencerrado, Miquel Angel
Senar, and Ana Cort´es
Departamento de Arquitectura de Computadores y Sistemas Operativos
08193 Bellaterra (Barcelona), Spain
andres.cencerrado@caos.uab.es
{miquelangel.senar,ana.cortes}@uab.es

Abstract. Virtualization technologies provide ﬂexible execution environments that could bring important beneﬁts for computational problems with strong deadlines. Large Grid infrastructures are becoming
available nowadays and they could be a suitable environment to run
such on-demand computations that might be used in decision-making
processes. For these computation, we encounter the need to deliver as
much resources as possible at particular times. These resources may be
provided by diﬀerent institutions belonging to a grid infrastructure but
there are two important issues that must be satisﬁed. Firstly, all resources must be correctly conﬁgured and all the components needed by
the application must be properly installed. If there is something small
missing that is required then applications will fail. Secondly, the execution of urgent applications must be made quickly in order to produce
useful results in time. If applications must wait in a queue, results might
be useless because they are obtained too late. To address these issues, we
describe a job management service, based on virtualization techniques,
that avoids conﬁguration problems and increases the number of available resources to run applications with critical deadlines. We describe
the main components of our service that can be used on top of common
batch queue systems and we show some experimental results that prove
the beneﬁts of applying time-sharing techniques on the virtual machines
to increase the performance of urgent computations.

1

Introduction

Nowadays, scientiﬁc community rely on computational resources in order to solve
most of the present scientiﬁc problems. In order to satisfy computing-intensive
problems in the minimum possible time, since the beginning of the 90s it is
possible to resort to distributed computing, because of the deployment of grid
infrastructures. These kind of infrastructures allow the users to count on many
computing resources managed in a decentralized way.
Besides, there is an emerging area in the scientiﬁc applications ﬁeld, Urgent
High Performance Computing (Urgent HPC), which requires a great capacity
This work is supported by the MEC-Spain under contracts TIN 2007-64974.
G. Allen et al. (Eds.): ICCS 2009, Part I, LNCS 5544, pp. 227–236, 2009.
c Springer-Verlag Berlin Heidelberg 2009

228

´ Senar, and A. Cort´es
A. Cencerrado, M.A.

of computing resources at a given time. Urgent HPC applications could take
beneﬁt from a grid environment prepared to respond eﬃciently, because of the
great resources oﬀer and variety it can supply. In fact, there are several projects
in progress which main goal is to get response from a multiple-resource sites as
quick as possible [1], but due to the grid environments nature, it is necessary to
propose new management strategies that provide not only a quick response, but
a more eﬃcient use of the available resources for the results to be more accurate
in a shorter period of time.
Grid computing can be considered as a consolidated ﬁeld in high performance
computing, however, it still presents serious limitations from the point of view of
Urgent HPC requests. Response time is an usual handicap in such environments.
Each administrative domain, in a grid infrastructure, has its own components
that take care about security and scheduling issues. All these components introduce considerable delay to the jobs starting, which is a clear penalty for urgent
computing applications.
Nevertheless, time response is not the only constraint, compatibility between
the application to be launched and the execution platform is a very important
handicap as well. Libraries, permissions and the host operating system are examples of factors that can compromise the compatibility and, in consequence, the
successful execution of the jobs. Many times, users are not aware of the requirements of their own jobs. This kind of ignorance usually leads to a considerable
waste of time trying to guarantee the correct execution of the application on the
remote sites and sometimes it turns out to be an impossible task.
Moreover, most of scientiﬁc applications rely on models/simulators that do
not provide accurate results and the quality of the results will depend on how
many times the underlying simulation can be executed before a deadline. Therefore, in those cases, as much executions we can deliver at a given time, the
better the results become. Consequently, we focus on increasing the number of
suitable working nodes for the underlying urgent application at a given time
independently on the sites’ particular conﬁguration.
Virtual Machines (VMs) are becoming popular in grid computing as they
provide a way to abstract the grid resources and allow grid applications to be
run without worrying about the underlying platform of the resource. In this work,
we propose a job-management service based on virtualization techniques, that
avoid conﬁguration problems and increases the number of available resources to
run applications with critical deadlines.
This paper is organized as follows. In the next section, the main features of
virtual machines are described. In section 3, we describe the architecture of the
job management architecture. The experimental study is reported in section 4
and, ﬁnally, the main conclusions are included in section 5.

2

Virtual Machines for Urgent Computing

As we have previously mentioned, virtual machines provide an abstraction of the
underlying computing resources allowing any kind of application to be executed

Support for Urgent Computing Based on Resource Virtualization

229

without misleading problems. In particular, VMs provide several important advantages:
a) Physical platform isolation: thanks to the abstraction carried out by
the virtual machine monitor (VMM), tasks execution does not depend on
the host device features. Thus, any VM instance on which user can execute
his application successfully will be useful and, therefore, any working node
(WN) in the site can satisfy application demands.
b) Multiplexation capability: depending on the features of the host platform, it can support multiple VM instances execution, which could provide
better performance per WN.
c) Possibility of deploying new subpool of WNs: a VM, like a real physical machine, can execute the software needed to become a new WN into
the site, able to receive and process diﬀerent tasks as well as physical WNs
do. Furthermore, the fact of having multiple VMs as a part of the working
nodes set of the site, allow us to deploy a new subset, or subpool, of virtual
WNs. The idea besides that is the one referred as to VM recycling. Instead of
starting and stopping a VM each time the submitted job requires this kind of
execution environment, VM recycling deals with the idea of not stopping the
VM and keeping it alive waiting for new tasks to be executed. This recycling
strategy saves time, as the time needed to instantiate the VM is skipped.
As one can see, these features solve, or at least relieve, the main constraints
of grid environments when dealing with an urgent HPC application. How each
one of the above mentioned handicaps are overcome is subsequently listed.
– Compatibility constraints elimination: in order to be able to execute the
application in any grid site, it is suﬃcient to have a VM that emulates a
trusted system on which the desired application executes successfully.
– Response time reduction: as it has been previously mentioned, VM recycling
permit to deploy subpools of virtual WNs that could be managed in a diﬀerent way than the physical ones, establishing, for example, diﬀerent priority
polices, even up to dedicating it exclusively for particular applications.
– Turnaround time reduction: because of the multiplexation capability, it is
possible to carry out more executions per time unit, reducing the turnaround
time of each individual task/application execution compared to a standard
batch execution.
As stated previously, this work is focused on eﬃcient managing strategies of
the grid-available resources. To achieve this goal, we propose a grid site local-level
architecture design based on the use of Virtual Machines (VMs), as a suitable
platform for grid infrastructures in order to allow an eﬃcient and successful
processing of urgent scientiﬁc applications under such environments.
Although this approach seems very promising, it also raises new diﬃculties
and new challenges that must be explored, for instance, it is needed an accurate
analysis of possible performance degradations as well as collateral eﬀects. These
aspects will be studied in the subsequent sections.

230

3

´ Senar, and A. Cort´es
A. Cencerrado, M.A.

Architecture Design

Virtual machines in grid computing can be implemented in diﬀerent ways depending on the purpose and intention of the grid. We shall now brieﬂy describe
two of the most signiﬁcant approaches according to our work:
1.-Virtual Machines as Grid Infrastructure: this method allows Grids to be set
up and deployed easily. Grid middleware is installed and conﬁgured within virtual
machines, and users can create and use a Grid infrastructure by installing and
deploying these virtual machine images. Machines are added and removed from
the Grid when users start and terminate their virtual machines. One example of
this virtual machine grid computing implementation is Grid Appliance [2].
2.- Virtual Machines running on Grid Infrastructures: this method is based
on the use of an already existing infrastructure for supporting grid computing.
Virtual Machines are executed as processes using the grid middleware just like
normal applications. Some examples are In-VIGO [3] and Maestro-VC [4].
Our proposal ﬁts the second approach, taking into account the possibility of
dealing with virtual resources as new WNs inside the site, as well. Figure 1 depicts a modular schema of the proposed architecture for an urgent job execution
site based on virtual resources. In this approach, Grid middleware layer is based
on EGEE gLite framework [5]. As one can see, there are new resource components as VMs which have been deployed as a consequence of the execution of
an ordinary task in a physical node, consisting of VM instance starting. These
virtual nodes have the capability to become new WNs into the site. Our contribution, by means of this design, consists of the Virtual Spaces Job Manager
(VSJM) component description, which is the responsible for the appropriate
deployment and management of these virtual resources.
VSJM manages the virtual resources subpool, establishing direct connection
with it, in order to be able to diversify eﬃciently incoming virtual-resource demanding jobs. This component is constituted by three subcomponents:
– Execution manager: responsible for carrying out scheduling tasks for this
sort of jobs.
– VM manager: the subcomponent in charge of VM instantiation, by submitting a VM starting up job to the Local Resource Management System
(LRMS), stopping, recycling and even migrating it.
– Job manager: this component will deal with the submission and control of
tasks to be executed in virtual resources.
In order to include to the system the capability to correctly attend incoming
requests, we lean on an extension of the EU-Datagrid Job Description Language
[6], that is, the users, by means of a JDL ﬁle, may specify which virtual environment should be provided for their jobs. Figure 2 shows a simpliﬁed example
of an extended JDL ﬁle where we include two new attributes: the VMId, which
univocally speciﬁes what VM image should be deployed, and the UrgencyLevel,

Support for Urgent Computing Based on Resource Virtualization

231

Fig. 1. Virtual resources-based site architectural design approach
Executable
Arguments
JobType
InputSandBox
VMId
UrgencyLevel

=
=
=
=
=
=

"fusion_app";
"-n";
"Normal";
{"fusion_app"};
e97315e469fb;
1;

Fig. 2. Extended JDL job description

which speciﬁes, in a 0 to 2 scale, the emergency degree of the incoming job, in
order to eﬃciently attend this kind of jobs.
Based on this extended JDL ﬁle, users can order the instantiation of a VM
image specifying its VMId, which had been established once the image had been
deposited on the VM repository. Our objective is to adapt this component to
the Globus Virtual Workspaces virtual repositories management system [7] [8].
Thus, jobs get into the system through the Grid middleware layer, which determines whether jobs will be executed in a physical WN, steering them directly
to the LRMS, or jobs will be executed in a virtual WN, steering them in this
case to the VSJM module. Additionally, jobs that demand a virtual environment,
which is not already deployed, will cause the VM manager to act submitting an
ordinary job to the LRMS consisting of VM starting up.
This architectural approach constitutes a solution that provides useful services
for urgent computing, from which outstand the following ones:
– Allowing usual job execution: new functions added by our system do
not interfere in the way the site operated previously.

232

´ Senar, and A. Cort´es
A. Cencerrado, M.A.

– Allowing VM instances executions (oﬀered by the site or supplied
by the user): our approach contemplates the possibility of oﬀering a series of VM images available in the site (stored in a storage element), and
accepting user images as well, depending on authorization policies.
– VMs capability to become WNs and constituting a new WN subset: our system submits VMs images as normal jobs to the Local Resource
Management System without requiring any special feature.
– Transparent job submitting to VM instances: our system includes
non-intrusive techniques that allow the automatic instantiation of a resource
management system (such as Condor [9]) when a VM image is started. This
resource management system does not interfere with the Local Resource
Management System deployed in the site and is used only by our Job Manager in order to submit jobs directly to VM instances. By exploiting disk
images inclusion techniques, there is no need to modify the users’ VM image
nor to make them install any extra software in their images.
– Eﬃcient VMs recycling management: the automatic deployment of a
resource management system when each VM is started allows the execution
of multiple jobs in the same VM instance. Therefore, signiﬁcant saving of
time is obtained by avoiding the overhead incurred in VM instantiation.
– Basic infrastructure that can be able to dynamically increase the
number of resources in case of emergency: our system enables the
development of intelligent scheduling strategies that can determine in an
automatic and dynamic way when it is necessary to deploy new VM instances
in order to attend to the incoming tasks load, without the need for the user
to send VMs execution jobs.

4

Performance Analysis and Experimentation

This section describes some experimental results obtained with a ﬁrst prototype
that supports the basic functionality of the job management service described
in the previous section. The goal is to demonstrate feasibility of the design and
show the potential use of the virtual machine multiprogramming mechanism. Our
prototype is interfaced to Condor as Local Resource Management System. The
current stable version of Condor (7.0.4) provides direct support for managing
VM images (which includes transferring images from a submission machine to
an execution machine, starting and stopping the virtual machine, and so on).
Condor was running on a testbed made of 12 Pentium IV machines, running
Linux. The Grid middleware used for accessing the testbed was based on gLite.
And we extended the existing gLite’s job manager with additional services that
provide a basic control of virtual images and application jobs.
We carried out a set of experiments with this prototype to measure the performance of two applications running in a virtualized mode and the eﬀect of
virtual machine multiprogramming on application performance. Our multiprogramming scheme allows the execution of two or more virtual machines on a
physical machine. Each virtual machine can be used for a diﬀerent application

Support for Urgent Computing Based on Resource Virtualization

233

thus allowing urgent jobs to time share the CPU with other jobs running on a
diﬀerent virtual machine. A graceful degradation in performance is observed but
this scheme does not require to kill or checkpoint a running job when an urgent
job enters into the system.
We have used two computing-intensive applications to carry out the experimental analysis presented in this section. The ﬁrst one is a Dynamic Data Driven
Genetic Algorithm for ﬁre spread prediction (DDDGA) [10]. The second one is
BLAST [11], a well known biology application, based on searching regions of local
similarity between genetic sequences. Both applications are compute-intensive.
However, in our experiments we have used DDDGA as an example of an urgent
job that was time shared with a non-urgent job (BLAST).
There is no appreciable cost (in terms of execution time and overhead) derived
from the use of VMs as computational tool instead of a physical machine in the
case of compute-intensive applications [12]. Table 1 shows execution times of
both applications under VM and under native host.
Table 1. DDDGA and BLAST execution times (minutes)
Native host

Virtual Machine

BLAST DDDGA BLAST DDDGA
58.5

10

62

11

We carried out another experiment to demonstrate the fact that it is possible
to implement an adaptive scheduling system which is able to exploit virtual
resources depending on the emergency or priority of the incoming tasks. As
VMM we have used Xen [12], which provides useful techniques to control the
VMs behavior, such as CPU limitation. The experiment presented in this work
consists on evaluating how many executions of urgent incoming jobs (DDDGA)
can be carried out in the same time of another task already in execution, without
the need neither to suspend it nor to refuse the incoming requests, depending on
the CPU limitation balance between the virtual working nodes instances. Results
are shown on table 2. The time incurred by Condor to transfer the VM image is
not included. table 2 only shows the actual time incurred in the execution of each
job once the corresponding VM was started. It is worth noting that, according
to our architecture design, each instance of a job does not require to run in a
diﬀerent VM. In general, only a small set of VM can be started and then multiple
instances of the job will run there until the whole set of jobs is completed, thus
limiting the overall overhead incurred in VM transfer and initialization. In this
experiment, DDDGA and BLAST run simultaneously on the same machine but
a diﬀerent amount of CPU was given to each VM. The CPU amount ranged
from a minimum of 10% to a maximum of 90% (for instance, ﬁrst row in table
2 shows the results of DDDGA running in a VM that was limited to 10%, while
BLAST was running in a VM that was using 90% of the physical CPU). The
same experiment was conducted on diﬀerent machines of our testbed and table
2 shows average results.

234

´ Senar, and A. Cort´es
A. Cencerrado, M.A.
Table 2. CPU limitation experimental results (execution times in minutes)

CPU limit

#DDDGA #BLAST Whole

Whole

DDDGA mean BLAST mean

distribution

executions executions DDDGA

BLAST

execution

(DDDGA VM-

set execution set execution time

BLAST VM)

execution
time

time

time

10%-90%

1

2

128

132

128

66

25%-75%

2

1

82

82.5

41

82.5

50%-50%

5

1

126

129

25.2

129

75%-25%

18

1

277

287

15.39

287

90%-10%

49

1

591

602

12.06

602

As seen in table 2, it is possible to get an important beneﬁt by taking into
account applications behavior in each case of CPU limitation. A hypothetic
situation could be the fact of having a set of non-urgent BLAST jobs queued
and/or in execution, and an urgent request for executing as many instances of
DDDGA as possible. The results obtained indicate that if the system balanced
properly the use of CPU on the virtual resources many urgent tasks could be
completed while non-urgent ones will be kept in execution, without having to
suspend them. For instance, when DDDGA was running with 90% of CPU, the
overhead on the execution time of each job was nearly negligible, compared to
the case where DDDGA was running alone (Table 1). A total of 49 instances of
DDDGA were completed at the time taken by BLAST to complete one instance.
Obviously, performance degradation of BLAST was signiﬁcantly higher, but this
is an acceptable situation in a scenario in which an urgent application arrives into
the system and needs to harness as many computational resources as possible.
Further exploitation of this multiprogramming mechanism requires the study
of new scheduling policies that make an eﬃcient use of it in order to get the
maximum performance for each node in the grid site. Open issues that can be
explored in the future include:
a) To establish useful scheduling policies for virtual resources exploitation.
b) To satisfy, by means of these techniques, multiple time constraints (i.e. jobs
deadlines) thanks to intelligent strategies based on real-time CPU and memory limitation for jobs in execution in order to provide a suitable environment
for incoming tasks, respecting both the old jobs and the new jobs deadlines.
Furthermore, the addition of some sampling mechanism will enable our system to give feedback to the users about execution time estimations, taking into
account how many resources could be devoted to their jobs in a given period of
time, and how would be possible to resize them in terms of CPU limitation.

5

Conclusions and Future Work

This work addresses some of the problems related to execution of urgent jobs
in a distributed computing environment. We have studied and proposed an

Support for Urgent Computing Based on Resource Virtualization

235

architecture for a job management service, based on virtual resources, that local sites should present in order to attend successfully and eﬃciently incoming
computing-intensive scientiﬁc applications. By using virtualization techniques,
our system has several advantages because urgent codes will be able to run
without worrying about applications’ libraries and architecture dependencies.
The system includes separated services that are responsible, on the one hand,
for managing virtual machine instantiation and, on the other hand, for job execution on the corresponding virtual machine instances. One of our goals was to
keep interoperability with already existing Local Resource Management Systems
(LRMS) and cluster middleware. No changes are required to existing LRMS, and
the only additional software required on each cluster are the necessary components needed by the underlying virtualization engine (Xen, VMware...).
A ﬁrst prototype of our service has been built and tested on a cluster managed by Condor. Several experiments were conducted to evaluate the potential
beneﬁts that can be obtained when a multiprogramming mechanism was used
to share the execution of multiple virtual machines on a single host. Our experiments demonstrate that having control over virtual resources and balancing
the amount of CPU devoted to each virtual machine enables the development of
new scheduling strategies which could be used to manage the execution of urgent
jobs by adapting the system in a way that can satisfy the incoming requests and
demands.
Future research includes the exploitation of previous knowledge about applications behavior, that may allow the system to perform eﬃcient scheduling
policies [13]. Furthermore, we are working on deﬁning and implementing additional feasible features of the architectural design proposed, which deal with
issues such as automatic and dynamic VM activation according to the workload,
the ability of virtual resources booking, or multi-core architectures exploitation
in order to satisfy scientiﬁc computing intensive parallel applications.

References
1. Homepage: Spruce (November 2008), http://spruce.teragrid.org/
2. Homepage: Grid appliance user interface (November 2008),
http://www.grid-appliance.org/
3. Adabala, S., Chadha, V., Chawla, P., Figueiredo, R., Fortes, J., Krsul, I., Matsunaga, A., Tsugawa, M., Zhang, J., Zhao, M., Zhu, L., Zhu, X.: From virtualized
resources to virtual computing grids: the in-vigo system. Future Gener. Comput.
Syst. 21, 896–909 (2005)
4. Kiyanclar, N., Koenig, G.A., Yurcik, W.: Maestro-vc: A paravirtualized execution
environment for secure on-demand cluster computing. In: CCGRID 2006: Proceedings of the Sixth IEEE International Symposium on Cluster Computing and the
Grid, 28 (2006)
5. Webpage: Egee - glite (November 2008), http://glite.web.cern.ch/glite/
6. Pacini, F.: Jdl attributes speciﬁcation (November 2008),
https://edms.cern.ch/document/590869

236

´ Senar, and A. Cort´es
A. Cencerrado, M.A.

7. Keahey, K., Foster, I.T., Freeman, T., Zhang, X.: Virtual workspaces: Achieving
quality of service and quality of life in the grid. Scientiﬁc Programming 13, 265–275
(2005)
8. Keahey, K., Foster, I.T., Freeman, T., Zhang, X., Galron, D.: Virtual workspaces
in the grid. In: Cunha, J.C., Medeiros, P.D. (eds.) Euro-Par 2005. LNCS, vol. 3648,
pp. 421–431. Springer, Heidelberg (2005)
9. Homepage: Condor project (November 2008), http://www.cs.wisc.edu/condor/
10. Denham, M., Cort´es, A., Margalef, T., Luque, E.: Applying a dynamic data driven
genetic algorithm to improve forest ﬁre spread prediction. In: Bubak, M., van
Albada, G.D., Dongarra, J., Sloot, P.M.A. (eds.) ICCS 2008, Part III. LNCS,
vol. 5103, pp. 36–45. Springer, Heidelberg (2008)
11. Homepage: Blast: Basic local alignment search tool (November 2008),
http://blast.ncbi.nlm.nih.gov
12. Barham, P., Dragovic, B., Fraser, K., Hand, S., Harris, T., Ho, A., Neugebauer,
R., Pratt, I., Warﬁeld, A.: Xen and the art of virtualization. In: Proceedings of
the 19th ACM Symposium on Operating Systems Principles (SOSP 2003), Bolton
Landing, NY, USA. ACM, New York (2003)
13. Sherwood, T., Perelman, E., Hamerly, G., Calder, B.: Automatically characterizing
large scale program behavior. In: Proceedings of the Tenth International Conference on Architectural Support for Programming Languages and Operating Systems
(ASPLOS 2002), San Jose, California, pp. 45–57 (October 2002)

