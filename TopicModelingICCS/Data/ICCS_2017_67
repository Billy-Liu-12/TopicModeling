Available online at www.sciencedirect.com

ScienceDirect
Procedia Computer Science 108C (2017) 2282–2286

Column-wise Guided Data Imputation

Column-wise
GuidedScience,
DataICCS
Imputation
International
Conference on Computational
2017, 12-14 June 2017,
Column-wiseZurich,
Guided
Data
Imputation
Switzerland
*
Alessio Petrozziello
Ivan Jordanov
Column-wise
Guided
Data
Imputation
* and
Alessio
Petrozziello
and
Ivan Jordanov
Column-wise
Data
Imputation
University
ofGuided
Portsmouth,
Portsmouth,
U.K.
*
University
of Portsmouth,
Portsmouth, U.K.
Alessio.petrozziello@port.ac.uk,
Ivan.jordanov@port.ac.uk
Alessio
Petrozziello
* and Ivan Jordanov
Alessio.petrozziello@port.ac.uk,
Ivan.jordanov@port.ac.uk
Alessio
Petrozziello
Ivan Jordanov
University
of Portsmouth,
Portsmouth,
U.K.
* and
Alessio
Petrozziello
and
Ivan Jordanov
University
of Portsmouth,
Portsmouth,
U.K.
Alessio.petrozziello@port.ac.uk,
Ivan.jordanov@port.ac.uk
University of Portsmouth,Ivan.jordanov@port.ac.uk
Portsmouth, U.K.
Alessio.petrozziello@port.ac.uk,
Alessio.petrozziello@port.ac.uk, Ivan.jordanov@port.ac.uk

Abstract
Abstract
This paper investigates data imputation techniques for pre-processing of dataset with missing
This
paper
data is
imputation
techniques
pre-processing
dataset estimating
with missing
values.
The investigates
current literature
mainly focused
on theforoverall
accuracy, of
evaluated
the
Abstract
values.
The
current
literature
is
mainly
focused
on
the
overall
accuracy,
evaluated
estimating
the
Abstract
missing
values
on
the
dataset
at
hand,
however
the
predictions
can
be
suboptimal
when
considering
This paper investigates data imputation techniques for pre-processing of dataset with missing
Abstract
missing
values
on theliterature
dataset
atis
hand,
however
the
can
be suboptimal
when
considering
This
paper
investigates
dataeach
imputation
techniques
pre-processing
of
dataset
with
missing
the
model
performance
for
feature.
To address
this
problem,
a Column-wise
Guided
Data
values.
The
current
mainly
focused
onpredictions
thefor
overall
accuracy,
evaluated
estimating
the
Thismodel
paper
investigates
data
imputation
techniques
pre-processing
of
dataset
with
missing
the
performance
for
feature.
To address
this
problem,
aselection
Column-wise
Guided
Data
values.
The
current
literature
mainly
focused
onpredictions
thefor
overall
accuracy,
evaluated
the
Imputation
method
(cGDI)
iseach
proposed.
Its
main
novelty
resides
in the
ofwhen
theestimating
most
suitable
missing
values
on the
dataset
atis
hand,
however
the
can
be suboptimal
considering
values.
The
current
literature
mainly
focused
onpredictions
thethis
overall
accuracy,
evaluated
the
Imputation
method
(cGDI)
proposed.
Its
main
novelty
resides
in the
of
theestimating
most
suitable
missing
values
on the
dataset
atis hand,
however
the
can
be suboptimal
when
considering
model
from
a multitude
ofisimputation
techniques
for
each
individual
feature,
through
a learning
the model
performance
for
each
feature.
To address
problem,
aselection
Column-wise
Guided
Data
missing
values
on
the
dataset
at
hand,
however
the
predictions
can
be
suboptimal
when
considering
model
from
a
multitude
of
imputation
techniques
for
each
individual
feature,
through
a
learning
the
model
for
To performance
address
problem,
aselection
Column-wise
Guided
Data
process
onperformance
the known
data.
To feature.
assessItsthe
of the
proposed
technique,
empirical
Imputation
method
(cGDI)
iseach
proposed.
main
noveltythis
resides
in the
of the most
suitable
the model
for
each
To performance
address
problem,
aselection
Column-wise
Guided
Data
process
onperformance
the
known
data.
To feature.
assess
the
of
the
proposed
technique,
Imputation
method
(cGDI)
proposed.
Its
main
novelty
resides
in the
of
the
most
suitable
experiments
been
conducted
on 13
publicly
available
datasets.
The
results
show
that
cGDI
model
from
ahave
multitude
ofisimputation
techniques
for this
each
individual
feature,
through
aempirical
learning
Imputation
method
(cGDI)
is
proposed.
Its
main
novelty
resides
in
the
selection
of
the
most
suitable
experiments
have
been
conducted
on
13
publicly
available
datasets.
The
results
show
that
cGDI
model
from
a
multitude
of
imputation
techniques
for
each
individual
feature,
through
a
learning
outperforms
twoknown
baselines
andTohasassess
always
or of
greater
estimationtechnique,
accuracy over
four
process on the
data.
thecomparable
performance
the proposed
empirical
model
from
ahave
multitude
of
imputation
techniques
for problem
each
individual
feature,
through
aover
learning
outperforms
two
baselines
and
comparable
or of
greater
estimation
accuracy
foura
process
on the
known
data.
Tohas
assess
the
performance
the
proposed
technique,
empirical
state-of-the-art
methods,
widely
applied
to
solve
the
at hand.
Furthermore,
has
experiments
been conducted
onalways
13
publicly
available
datasets.
The
results
showcGDI
that
cGDI
process
on
the
known
data.
To
assess
the
performance
of
the
proposed
technique,
empirical
state-of-the-art
methods,
widely
applied
to
solve
the
problem
at
hand.
Furthermore,
cGDI
has
experiments
have
been
conducted
on
13
publicly
available
datasets.
The
results
show
that
cGDI
straightforward
implementation
and
any
other
known
imputation
technique
can
be
easily
added.
outperforms two baselines and has always comparable or greater estimation accuracy over foura
experiments
have
been conducted
on
13other
publicly
available
datasets.
The
results
that
cGDI
straightforward
implementation
any
known
imputation
can accuracy
be show
easily
added.
outperforms
twomethods,
baselines
and and
has
always
comparable
or greater
estimation
over
foura
state-of-the-art
widely
applied
to
solve
the
problem
attechnique
hand.
Furthermore,
cGDI
has
©
2017
The
Authors.
Published
by
Elsevier
B.V.
outperforms
two
baselines
and
has
always
comparable
or
greater
estimation
accuracy
over
foura
Keywords: Missing
Data; Data
Imputation;
Multitude
ofthe
imputation
models.
state-of-the-art
methods,
widely
applied
to solve
problem
attechnique
hand. Furthermore,
cGDI
has
straightforward
implementation
and
any
other
known
imputation
can
be
easily
added.
Peer-reviewMissing
undermethods,
responsibility
of theapplied
scientific
committee
of problem
the International
Conference
on Computational
Science
state-of-the-art
widely
to
solve
the
at
hand.
Furthermore,
cGDI
has
Keywords:
Data;
Data
Imputation;
Multitude
of
imputation
models.
straightforward implementation and any other known imputation technique can be easily added. a
straightforward
implementation
and anyMultitude
other known
imputation
technique can be easily added.
Keywords: Missing
Data; Data Imputation;
of imputation
models.

1 Introduction
1 Introduction
Most real world datasets contain missing data due to either sensors failures or human errors and
1dealing
Introduction
Most
realitworld
datasets contain
datapre-processing
due to either sensors
failures
orstatistical
human errors
and
with
is an important
step in missing
the dataset
phase, since
most
analysis
1dealing
Introduction
with
it
is
an
important
step
in
the
dataset
pre-processing
phase,
since
most
statistical
analysis
techniques,
data
reduction
tools,
and
machine
learning
methods
require
complete
sets.
The
real world datasets contain missing data due to either sensors failures or human errors and
1 Most
Introduction
Keywords: Missing Data; Data Imputation; Multitude of imputation models.
Keywords: Missing Data; Data Imputation; Multitude of imputation models.

techniques,
tools,
machine
learning
methods
require
complete
sets.
The
Mostwith
realdata
world
datasets contain
missing
datapre-processing
due
tointo
either
sensors
failures
orstatistical
human
errors
and
mechanisms
are usually
categorized
three
groups
(Enders,
2010):
MCAR
dealing
itof
is missingness
anreduction
important
step
inand
the dataset
phase,
since
most
analysis
Mostwith
real
world
datasets
contain
missing
datapre-processing
due
tointo
either
sensors
failures
orstatistical
human
errors
and
mechanisms
are usually
categorized
three
groups
(Enders,
2010):
MCAR
dealing
itof
is missingness
anreduction
important
step
inand
the
dataset
phase,
since
most
analysis
(Missing
Completely
at Random),
MAR
(Missing
At
Random)
and
MNAR
(Missing
Not
At
techniques,
data
tools,
machine
learning
methods
require
complete
sets.
The
dealing
with
it
is
an
important
step
in
the
dataset
pre-processing
phase,
since
most
statistical
analysis
(Missing
Completely
at Random),
MAR
(Missing
Atinto
Random)
and
MNAR
(Missing
Not
At
techniques,
data
reduction
tools,
and
machine
learning
methods
require
complete
The
Random) and
themissingness
approaches
for
with
missingness
include
(Enders,
2010):
simple
deletion
mechanisms
of
are dealing
usually
categorized
three
groups
(Enders,
2010):sets.
MCAR
techniques,
data
reduction
tools,
and
machine
learning
methods
require
complete
sets.
The
Random)
themissingness
approaches
for
with
missingness
include
(Enders,
2010):
simple
deletion
mechanisms
of
are dealing
usually
categorized
three
groups
(Enders,
2010):
MCAR
(list-wise,
attribute,
and
deletion);
univariate
imputation
(Random
Guessing,
and
Mean/
(Missing and
Completely
at pairwise
Random),
MAR
(Missing
Atinto
Random)
and
MNAR
(Missing
Not
At
mechanisms
of
areal.,
usually
categorized
three
groups
(Enders,
2010):
MCAR
(list-wise,
attribute,
and
deletion);
univariate
imputation
(Random
Guessing,
andincludes
Mean/
(Missing
Completely
at pairwise
Random),
MARwith
(Missing
Atinto
Random)
and MNAR
Not
At
Median
(Sarro
etfor
2016);
and
multivariate
imputation.
The
last(Missing
group
Random)Imputation)
and
themissingness
approaches
dealing
missingness
include
(Enders,
2010):
simple
deletion
(Missing
Completely
at pairwise
Random),
MAR
(Missing
At
Random)
and different
MNAR
(Missing
Not
At
Median
(Sarro
etforal.,
2016);
and
multivariate
imputation.
The
lastalgorithms
group
Random)Imputation)
and
the approaches
dealing
with
missingness
include
(Enders,
2010):
simple
deletion
methods
that
consider
the
correlation
of the
attributes.
Inimputation
this
work,
four
of this
(list-wise,
attribute,
and
deletion);
univariate
(Random
Guessing,
andincludes
Mean/
Random)
and
the
approaches
for
dealing
with
missingness
include
(Enders,
2010):
simple
deletion
methods
that
consider
the
correlation
of
the
attributes.
In
this
work,
four
different
algorithms
of
this
(list-wise,
attribute,
and
pairwise
deletion);
univariate
imputation
(Random
Guessing,
and
Mean/
family
are
investigated:
Multiple
Imputation
Chained
Equations
(MICE);
Bagged
Tree
Imputation
Median Imputation) (Sarro et al., 2016); and multivariate imputation. The last group includes
(list-wise,
attribute,
and
pairwise
deletion);
univariate
(Random
Guessing,
and
Mean/
family
are
investigated:
Imputation
Chained
Equations
(MICE);
Bagged
Tree
Imputation
Median
Imputation)
(Sarro
etImputation
al.,
2016);
and
multivariate
imputation.
The
lastalgorithms
group
includes
(BTI);
K-Nearest
Neighbour
(Jordanov,
Petrov,
Petrozziello,
2016)
methods
that
consider
theMultiple
correlation
of the(KNNI)
attributes.
Inimputation
this work,
four&
different
of and
this
Median
Imputation)
(Sarro
etImputation
al.,
2016);
and
multivariate
imputation.
The
lastalgorithms
group
includes
(BTI);
K-Nearest
Neighbour
(KNNI)
(Jordanov,
Petrov,
Petrozziello,
2016)
methods
that
consider
theMultiple
correlation
of the
attributes.
In this work,
fouret&
different
of and
this
Bayesian
Principal
Component
Analysis
Imputation
(bPCA)
(Schmitt
al.,
2015).
These
methods
family are
investigated:
Imputation
Chained
Equations
(MICE);
Bagged
Tree
Imputation
methods
that
consider
theMultiple
correlation
of the
attributes.
In years
this work,
fouret&
different
algorithms
of and
this
Bayesian
Principal
Component
Analysis
Imputation
(bPCA)
(Schmitt
al.,
2015).
These
methods
familybeen
are
investigated:
Imputation
Chained
Equations
(MICE);
Bagged
Tree
Imputation
have
widely
applied
and Imputation
compared
in(KNNI)
the
past (Jordanov,
few
showing
discordant
results
(Schmitt
(BTI);
K-Nearest
Neighbour
Petrov,
Petrozziello,
2016)
family
are
investigated:
Multiple
Imputation
Chained
Equations
(MICE);
Bagged
Tree
Imputation
have
widely
applied
compared
in(KNNI)
the past (Jordanov,
few
showing
discordant
(Schmitt
(BTI);
K-Nearest
Neighbour
Imputation
Petrov,
Petrozziello,
2016)
anda
et
al.,been
2015),
(Jordanov
et and
al., 2016).
Most
approaches
ofyears
dealing
withet&
missingness
would
select
Bayesian
Principal
Component
Analysis
Imputation
(bPCA)
(Schmitt
al.,
2015).results
These
methods
(BTI);
K-Nearest
Neighbour
Imputation
(KNNI)
Petrov,
Petrozziello,
2016)
anda
et
al.,been
2015),
(Jordanov
et and
al., 2016).
Most
approaches
ofyears
dealing
withet&
missingness
would
select
Bayesian
Principal
Component
Analysis
Imputation
(bPCA)
(Schmitt
al.,
2015).However,
These
methods
single
method
thatapplied
outperforms
the
others
on (Jordanov,
afew
given
performance
measure.
while
have
widely
compared
inbased
the past
showing
discordant
results
(Schmitt
Bayesian
Principal
Component
Analysis
Imputation
(bPCA)
(Schmitt
et
al.,
2015).
These
methods
single
method
that
outperforms
the
others
based
on
a
given
performance
measure.
However,
while
have
widely
applied
and
compared
in approaches
the pastacross
fewofyears
showing
discordant
(Schmitt
aet
given
approach
might
have
best performance
the
whole
dataset,
it does results
not
mean
that ita
al.,been
2015),
(Jordanov
et
al.,the
2016).
Most
dealing
with
missingness
would
select
have
been
widely
applied
and
compared
inbased
the
past
years
showing
discordant
results
(Schmitt
asingle
given
approach
have
best
performance
the
whole
dataset,
it does
not
mean
that
ita
et
al.,be
2015),
(Jordanov
et
al.,
2016).
Most
approaches
of
dealing
with
missingness
would
select
will
superior
atmight
the
level
ofthe
each
individual
feature.
In
the
proposed
approach,
instead
of selecting
method
that
outperforms
the
others
onacross
afew
given
performance
measure.
However,
while
et
al.,
2015),
(Jordanov
et
al.,
2016).
Most
approaches
of
dealing
with
missingness
would
select
will
be
superior
at
the
level
of
each
individual
feature.
In
the
proposed
approach,
instead
of
selecting
single
method
that
outperforms
the
others
based
on
a
given
performance
measure.
However,
while
a given approach might have the best performance across the whole dataset, it does not mean that ita
*single method that outperforms the others based on a given performance measure. However, while
aCorrespondent
given
approach
have
best
performance
across
theproposed
whole dataset,
it does
not mean
that it
author
will
be superior
atmight
the level
ofthe
each
individual
feature.
In the
approach,
instead
of selecting
*
*a given approach might have the best performance across the whole dataset, it does not mean that it
Correspondent
author
will
be superior
at the level of each individual feature. In the proposed approach, instead of selecting
*will be superior at the level of each individual feature. In the proposed approach, instead of selecting
Correspondent author

*
Correspondent author
1877-0509
* © 2017 The Authors. Published by Elsevier B.V.
Correspondent
author
Peer-review
under responsibility
of the scientific committee of the International Conference on Computational Science
10.1016/j.procs.2017.05.008

	

Alessio Petrozziello et al. / Procedia Computer Science 108C (2017) 2282–2286

a single method which outperforms the others on the whole dataset, a column-wise selection is used
to choose the best imputation method for each attribute of the dataset. To do that, we initially use
the subset with complete data only. Then, we artificially introduce a percentage of missing data in
it, which subsequently is imputed using the above mentioned methods. For each feature, the method
that produced the lowest estimation error is then used to impute the missing values for the
correspondent attribute. We propose a Column-wise Guided Data Imputation (cGDI) method which
performance is compared with two baseline techniques (Random Guessing and Median Imputation)
and four state-of-the-art methods (MICE, BTI, KNNI, and bPCA). The cGDI is extensively tested
and validated on 13 publicly available datasets with a large degree of diversity (size and number of
attributes) and its performance is assessed and compared with the other techniques using Wilcoxon
Signed-rank test for statistical significance (Cohen et al., 2013). The rest of the paper is organized
as follows. Section 2 proposes the cGDI method. Section 3 discusses the empirical study carried
out. The results of this investigation are critically analyzed in Section 4 and finally, in Section 5
conclusion is given.

2 The Proposed Method
All methods described in (Schmitt et al., 2015), (Jordanov et al., 2016) have been widely used
for solving missing data problems. However, while a given approach may produce low estimation
error for the whole dataset at hand, this does not mean that the method produces the best results
(min error) for every individual feature (usually, for some of the features other methods give better
estimates). The investigated here Column-Wise Data Imputation (cGDI) is an approach which
ensemble “weak” models choosing the best one for each feature (column) of the dataset (accepting
that in the same time the chosen model may be 'weak' for the other features). In other words, when
building an ensemble, the best imputation method for each feature of the dataset is selected among
the “weak” techniques, and then included into the ensemble. During the learning phase, the
algorithm is trained on artificially introduced missing data, and then, the combination of methods
that performed best, is used to impute the missing values in the initial dataset. The complete subset
(without missingness) is used for training the model introducing a percentage of MCAR (e.g., 25%)
in each column. Once the data are imputed with each “weak” technique, an error function (e.g.,
RMSE, MAE) is used to select the best imputation method for each column of the dataset. To cope
with the random nature of the algorithm and to ensure a more robust choice, this process is iterated
for a given number of times, and the algorithm which produced the lowest median overall error for
each feature is then chosen. For example, let's assume a set of m imputation methods (M1, ..., Mm
 S) and dataset (X) composed of v variables and n samples, where k of them (0 < k < n) contain
at least one missing value. Once the n-k complete samples (X' subset) are separated from those with
missing values, a % of MCAR is added to each variable of X' (e.g., 25%). The missing data in X'
are separately imputed using all methods of S, and the estimation error (e.g., RMSE) is calculated
for each feature. This process is repeated I times (e.g., I = 5), and for each variable in X', the
imputation algorithm scoring the lowest median error is added to the ensemble (E). The ensemble
of those techniques is then used to estimate the missing values of the whole set X. In particular, 
Mi  E, i = 1,..,m, the dataset X is entirely imputed, and only the imputed values for the features
where Mi scored the lowest error are saved, discarding the others. Since X is imputed independently
with each technique, the order of imputation is irrelevant, enabling the process to be parallelized.

3 Empirical Study
In this section, the design of the empirical study used to test and validate the proposed approach
is presented. Firstly, the research questions that promoted this study are discussed, then the
validation criteria and performance metrics are analysed, followed by the description of
experimental settings and used datasets.
After extensive research of the literature to identify one imputation method able to win on every
dataset, we found that there are discordant performance results regarding the four discussed data
imputation techniques. Furthermore, a preliminary empirical analysis (see Section 4) highlighted

2283

Alessio Petrozziello et al. / Procedia Computer Science 108C (2017) 2282–2286

2284	

that the performance of the considered techniques vary for different datasets as well as for each
feature. These findings led to the investigation of our ensemble idea.
The proposed method (cGDI) is compared with the given univariate baseline and multivariate
state-of-the-art (KNN, BTI, MICE and bPCA) imputation methods to assess its performance on the
missing data estimation task. The results are reported in Section 4.
Variety of metrics employed for comparing and evaluating data imputation and predictive
models can be found in the literature (Sarro, Petrozziello, & Harman, 2016). Among them, Mean
Squared Error (MSE) and variants as Root Mean Squared Error (RMSE) and Normalized Root
Mean Squared Error (NRMSE) are the most largely used. These metrics measure the difference
between predicted and actual values while the two variants are used to mitigate the magnitude
problem (taking the root of the error) and normalize the errors in the interval [0, 1]. The Mean
Absolute Error (MAE) is argued to be more accurate and informative than the RMSE (Willmott,
2005), successively refuted by (Chai & Draxler, 2014), who states that the two measures picture
different aspects of the error and therefore they should both be used to assess the results. The
Standard Accuracy (SA) is argued to be good baseline estimation measures (Whigham et al., 2015).
Some of these measures are used to evaluate the performance of the proposed method in this work.
In particular, as suggested in (Willmott, 2005) and (Chai & Draxler, 2014), RMSE and MAE are
implemented to compare the estimated missing values and the original ones, reflecting the average
performance of the imputation method. Furthermore, the RMSE is employed as error function for
the training phase of the cGDI. SA is used to compare the proposed model with the univariate
baseline imputation techniques (discussed earlier). In particular, SA compares the prediction against
the mean of a random sampling of the training response values (SA = 1 –
(RMSE(predicted,actual)/RMSE(randomGuess,actual))).
Model
KNNI
BTI
MICE
bPCA
cGDI

Hyper-parameters
K = 10, Distance = Euclidean
#Trees = 200
#Iterations = 10
Method = Bayesian, Nboot = 5, Lstart = 1000, L = 100
#Iteration of training set = 5, Error function = RMSE

Table 1 Hyper-parameters setting

To validate the proposed method, a k-fold cross validation is applied, splitting the dataset into
independent training and test sets. The test set is generated using a uniform sampling without
repetitions, and the rest of the data is left as a training set. Since the Shapiro Test showed that many
of our patterns came from non-normally distributed populations, the statistical Wilcoxon Signed
Rank Test was used to prove which method is giving better performance (Cohen et al., 2013).
Furthermore, the used test does not make any assumptions about the underlying distribution of the
data. In this work, the following NULL hypothesis is tested: “The RMSEs (MAEs) provided by
model Mi are significantly smaller than the errors provided by model Mj'', (i, j = 1,…n, where n is
the number of investigated models), using confidence level α=0.05.
A 5-fold (80% training and 20% testing) cross validation is used to validate the proposed
method. To calibrate the model during the training phase, 25% MCAR is added to each attribute of
the training set, subsequently imputed using the five imputation techniques and the accuracy is
evaluated using the RMSE. This process is run 5 times and for each attribute, the imputation model
achieving the lowest median error (preferred to the mean due to robustness to outliers) is selected.
Lastly, the ensemble of selected techniques is used to impute the data on the independent test set
and the results are compared to all the other methods. Table 1 shows the hyper-parameters used for
each algorithm.
Thirteen publicly available datasets from KEEL repositories (Alcalá-Fdez, et al., 2011) are used
in this work, namely Contraceptive, Yeast, Red wine, Car, Titanic, Abalone, White Wine, Page
Block, Ring, Two Norm, Pen Based, Nursery, and Magic04. The selection of these datasets from the
repositories’ “classification” area was driven by the intent to cover different application domains
and data characteristics. In particular, the datasets differ in the number of instances (1484 to 19020),
the number of features (3 to 20), and range and type of the features (real, integer and categorical).

	

Alessio Petrozziello et al. / Procedia Computer Science 108C (2017) 2282–2286

The selected datasets do not have missing values by default. The introduction of synthetic MCAR
guarantees the reliability of the estimation through the experiments and the assessment and
evaluation of the results.

4 Results and Discussion
Preliminary results highlighted that the performance of the considered techniques vary for
different datasets as well as for each feature. The imputation with the single techniques showed that
for each attribute there is a different winner; hence, if one method is selected as the overall “best” it will not be superior for every feature of the dataset. As a whole, the KNNI prevailed on one
dataset (Pen Based), BTI on four datasets (Car, Red Wine, White Wine, and Ring), and bPCA on
six datasets (Contraceptive, Yeast, Titanic, Abalone, Two Norm, and Magic04). In rare cases, the
dataset at hand would have the same best imputation model for all the features (e.g., Car and Two
Norm), and normally each feature will have different best imputation model.
Dataset
Abalone
Pen Based
Page Block
Magic04
Contraceptive
Red Wine
White Wine
Titanic
Two Norm
Yeast
Car
Ring
Nursery

cGDI
0.68
0.54
0.49
0.47
0.39
0.37
0.36
0.35
0.34
0.33
0.29
0.31
0.30

KNNI
0.62
0.56
0.41
0.42
0.24
0.33
0.34
0.26
0.24
0.24
0.12
0.24
0.09

BTI
0.57
0.49
0.43
0.41
0.36
0.33
0.34
0.34
0.32
0.32
0.29
0.29
0.25

MICE
0.66
0.47
0.39
0.32
0.18
0.23
0.16
0.05
0.07
0.06
-0.01
-0.02
0.00

bPCA
0.72
0.45
0.46
0.45
0.38
0.32
0.34
0.34
0.34
0.33
0.29
0.29
0.29

Median
0.28
0.27
0.25
0.28
0.26
0.30
0.28
0.28
0.30
0.28
0.25
0.28
0.23

Table 2 Standard Accuracy (SA) values achieved by cGDI, the baseline (Median Imputation) and stateof-the-art (KNNI, BTI, MICE, SVD and bPCA) techniques over the 13 datasets for 5-fold cross validation with
25% MCAR. Higher values represent better estimation over the random guess

The SA values given in Table 2 show superior results for the imputation carried out with our
model. It outperformed the baseline methods Random Guessing (SAcGDI > 0) and the Median
Imputation (SAcGDI > SAMedian). The Mean Imputation was omitted in favor of the Median
Imputation, since the latter is considered less biased to outliers. To finally assure that the proposed
method is outperforming the baselines, a Wilcoxon test for statistical significance is run, testing the
NULL hypothesis “The RMSEs provided by cGDI are significantly smaller than the errors produced
by the models Random Guessing and Median Imputation”. The results proved cGDI being better
than both with p-value < 0.05 over all 13 datasets.
Model
cGDI
bPCA
BTI
KNNI
MICE

win
40 (37)
31 (24)
26 (19)
15 (19)
3 (4)

tie
9 (12)
9 (14)
12 (15)
3 (11)
5 (8)

loss
3 (3)
12 (14)
14 (18)
34 (22)
44 (40)

Table 3 RMSE (MAE) significance test for 5-fold cross validation with 25% MCAR in the test set. Each
row shows how many times the model Mi is better (win), comparable (tie), or worse (loss) than the other
models in a Wilcoxon Signed Rank Test, with NULL Hypothesis “The RMSEs (MAEs) provided by Mi are
significantly smaller than the errors provided by the other models”

The Standard Accuracy analysis (Table 2) shows that the cGDI method not only outperforms
the baselines, but it is also comparable, and even better than the state-of-the-art algorithms. As it
can be seen from the table, the SAcGDI is higher than the SA of the other methods in 45 out of the 52

2285

Alessio Petrozziello et al. / Procedia Computer Science 108C (2017) 2282–2286

2286	

cases, comparable in 5 out of the 52 cases, and worse in only 2 case. To validate the significance of
the difference, the Wilcoxon test is run justifying the NULL hypothesis “The RMSEs provided by
cGDI are significantly smaller than the errors achieved by the state-of-the-art methods”. Results in
Table 3 show that the imputation improvement achieved by cGDI is significant (p-value < 0.05) in
40 out of the 52 cases, comparable in 9 out of the 52 cases and worse in 3 cases only. As suggested
in (Willmott, 2005) the same NULL hypothesis was tested using the MAE metric. The cGDI resulted
significantly better in 37 cases, comparable in 12 and worse in only 3 cases. The second-best
imputation method (bPCA) for RMSE is significantly better in 31 out of the 52 cases, comparable
in 9 and worse in 12 case, which shows an improvement for cGDI of 17% over the best single
method. For the MAE hypothesis, bPCA results are significantly better in 24 out of the 52 cases,
comparable in 14 and worse in 14 cases, showing 25% superiority for the cGDI over the bPCA.

5 Conclusion
Missing data represents an important problem for datasets used in machine learning tasks,
statistical analysis, and any other process requiring a complete set. Several models have been
proposed in the literature, mainly focusing on the overall imputation accuracy. An initial analysis
carried on 13 datasets showed that a model scoring the lowest overall error does not necessarily
provide the best imputation for each feature of the dataset. For this reason, a Column-wise Guided
Data Imputation method is introduced and proposed in this paper. Its novelty lies in its approach,
which pairs each method with a feature in an attribute-wise fashion. The cGDI divides the complete
records (without missing values) from those with missingness, and selects (using learning applied
to the complete subset), the most suitable imputation method for each feature. The imputation
performance is evaluated with three widely used imputation tasks metrics (SA, RMSE, and MAE).
The results are statistically assessed using the Shapiro Test to check the distribution normality, and
the non-parametric Wilcoxon Signed Rank Test, validating the following NULL hypothesis: “The
RMSEs (MAEs) provided by model Mi are significantly smaller than the errors provided by model
Mj'' (i, j = 1,…n, where n is the number of investigated models), using confidence level α=0.05.
The Standard Accuracy analysis shows cGDI to always have better accuracy than the two baselines
and to produce superior estimation over the single state-of-the-art methods in 41 out of the 52 cases.
The Wilcoxon on MAE and RMSE shows improvements of 25% and 17% respectively for the cGDI
over the second best performing algorithm (bPCA). The results achieved in this work strongly
suggest that the use of the proposed approach can be beneficial when considering multivariate
imputation as a way of dealing with missingness. Another advantage of the cGDI approach is its
straightforward implementation and easy incorporation of other known imputation methods.

References
Alcalá-Fdez, J., Fernandez, A., Luengo, J., Derrac, J., García, S., Sánchez, L., & Herrera, F. (2011). KEEL
Data-Mining Software Tool: Data Set Repository, Integration of Algorithms and Experimental Analysis
Framework. Journal of Multiple-Valued Logic and Soft Computing, 17(2-3), 255-287.
Chai, T., & Draxler, R. (2014). Root mean square error (RMSE) or mean absolute error (MAE)?--Arguments
against avoiding RMSE in the literature. Geoscientific Model Development, 7(3), 1247-1250.
Cohen, J., Cohen, P., West, S. G., & Aiken, L. S. (2013). Applied multiple regression/correlation analysis for
the behavioral sciences. Routledge.
Enders, C. K. (2010). Applied missing data analysis. Guidford: Guildford Press.
Jordanov, I., Petrov, N., & Petrozziello, A. (2016). Supervised Radar Signal Classification. Neural Networks
(IJCNN), 2016 Int. Joint Conf. on (p. 1464-1471). Vancouver: IEEE.
Sarro, F., Petrozziello, A., & Harman, M. (2016). Multi-Objective Software Effort Estimation. Software
Engineering (ICSE), 2016 IEEE/ACM 38th IEEE Int. Conf. on, (p. 619-630). Austin.
Schmitt, P., Mandel, J., & Guedj, M. (2015). A comparison of six methods for missing data imputation. Journal
of Biometrics & Biostatistics, 6(1), 1-6.
Whigham, P. A., Owen, C. A., & Macdonell, S. G. (2015). A baseline model for software effort estimation.
ACM Trans. on Software Engineering and Methodology (TOSEM), 24(3), 20.
Willmott, C. J. (2005). Advantages of the mean absolute error (MAE) over the root mean square error (RMSE)
in assessing average model performance. Climate research, 30(1), 79-82.

