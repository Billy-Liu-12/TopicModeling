Procedia
Computer
Science

Available online at www.sciencedirect.com

Procedia
Computer Science
Science 00
(2009) 000–000
Procedia
Computer
1 (2012)
2227–2235

www.elsevier.com/locate/procedia
www.elsevier.com/locate/procedia

International Conference on Computational Science, ICCS 2010

Computational intelligence based architecture for cognitive agents
Anna T. Lawniczaka, Bruno N. Di Stefanob *
a

Department of Mathematics and Statistics, University of Guelph, Guelph, Ontario, N1G 2W1, Canada
b
Nuptek Systems Ltd, Toronto, Ontario, M5R 3M6, Canada

Abstract
We discuss some limitations of reflexive agents to motivate the need to develop cognitive agents and propose a hierarchical,
layered, architecture for cognitive agents. Our examples often involve the discussion of cognitive agents in highway traffic
models. A cognitive agent is an agent capable of performing cognitive acts, i.e. a sequence of the following activities:
“Perceiving” information in the environment and provided by other agents, “Reasoning” about this information using existing
knowledge, “Judging” the obtained information using existing knowledge, “Responding” to other cognitive agents or to the
external environment, as it may be required, and “Learning”, i.e. changing (and, hopefully augmenting) the existing knowledge
if the newly acquired information allows it. We describe how computational intelligence techniques (e.g., fuzzy logic, neural
networks, genetic algorithms, etc) allow mimicking to a certain extent the cognitive acts performed by human beings. The order
with which the cognitive actions take place is important and so is the order with which the various computational intelligence
techniques are applied. We believe that a hierarchical layered model should be defined for the generic cognitive agents in a style
akin to the hierarchical OSI 7 layer model used in data communication. We outline in broad sense such a reference model.

c 2009
⃝
2012Published
Publishedby
byElsevier
ElsevierB.V.
Ltd. Open access under CC BY-NC-ND license.
©
Keywords: Agent ; Agent Modeling; Cognitive Agent; Computational Intelligence

1. Introduction
Agent-based modeling can be traced back to Von Neumann and Ulam, [1], and later to John Conway, [2],
Thomas Schelling, [3], and Robert Axelrod, [4]. Since then there has been an extremely wide body of research using
agent-based models in investigation of both natural and man-made systems. These models accurately approximate
the behaviour of simple or simplified natural and man-made systems by mimicking and often bio-mimicking simple
entities, but they fall short of mimicking the behaviour of “sophisticated” animals and humans.
This inability to behave as sophisticated animals is mostly due to the simplicity of the computational paradigms
employed. In these traditional agent-based models an agent can be defined as “an autonomous entity capable of
interacting with its environment and other agents”. For instance in most highway traffic agent-based models each

* Corresponding author. Tel.: 001-519-824-4120 ext 53287; fax: 001-519-837-0221.
E-mail address: alawnicz@uoguelph.ca .

c 2012 Published by Elsevier Ltd. Open access under CC BY-NC-ND license.
1877-0509 ⃝
doi:10.1016/j.procs.2010.04.249

2228

A.T. Lawniczak, B.N. Di Stefano / Procedia Computer Science 1 (2012) 2227–2235
A.T. Lawniczak and B.N. Di Stefano / Procedia Computer Science 00 (2010) 000–000

car is an agent and the road is the environment. If properly designed, each car is capable of moving on the road
according to the rules of traffic and the laws of physics, avoiding accidents with other cars (i.e., other agents) and
with obstacles on the road, e.g. bridge pillars, trees, etc. (i.e., the environment). Many such models are an extension
of simple 1-D Cellular Automata (CA) models. In these models, the highway being modeled is discretised into a 1-D
array consisting of equal cells. Typically, there is one cell for every 7.5 m of road. This is the most common length
chosen for a cell in the literature, see for instance, e.g. [5], [6], and [7]. As we wrote in [8], “This has been chosen
because it corresponds to the space occupied by the typical car plus the distance to the preceding car in a situation
of dense traffic jam. The traffic jam density is given by 1000/7.5m approximately equal to 133 vehicles/km, a figure
deemed realistic by traffic engineers”. If we assume that each cell may contain either a “1” or a “0” and that each
“1” represents the presence of a vehicle and each “0” the absence of a vehicle, we can visualize vehicles moving
forward in the absence of vehicles ahead of them, or otherwise slowing down and even stopping. When modeling
multilane highway traffic, each lane can be modeled with a 1-D CA, hopping from one CA cell to the equally
numbered cell of another CA when changing lane. Alternatively a 2-D CA can be used, [8]. A review of highway
traffic CA models can be found in [5]. They model highway traffic with two loops, an “external” time loop and an
“internal” space loop. The time loop simulates the passing of time, assigning a number of seconds to each time step.
The space loop “scans” the representation of the physical highways, where distances are converted into cells, for
every cell checking if the cell is occupied by a car or not. If it is not occupied by a car the next cell is examined,
while if the cell is occupied by a car the “rule of transition” (also called “rule of motion”) is applied. To be able to
model different traffic behaviour the internal space loop is often replaced by various consecutive space loops, each
one of them required to perform a single operation, as it may be required, e.g., change lane to the left, change lane to
the right, accelerate, decelerate, brake, exit the highway, etc. At the beginning of each space loop the current state of
the highway is stored in what is called an “old buffer”. All changes of position of vehicles are stored into a “new
buffer”. At the end of one space loop or at the end of each space loop the “new buffer” fully represents the state of
the highways. The “new buffer” is copied into the “old buffer” and the next step of the time loop or the next space
loop is executed, as it may apply. Thus, the CA is a like a database where all information about the current state of
the model is recorded. The car is the agent that is invoked in each instance in which a car is present in a cell. This
agent knows how to interpret all information about all possible situations and is able to decide what to do next.
Indeed a large number of highway traffic models able to deal with variable speed, variable acceleration, multilane
traffic, urban areas, intersection of various type (e.g., yield, traffic light, roundabout, etc.) exist, see for instance [6],
[8], [10], [11], [12], [13], [14], and [15].
The ability of agents to act autonomously is limited to the predefined environment and to the predefined
situations to which the agent is expected to respond, because agents can only act in a situation compatible with the
way they are designed. For instance, an agent representing a car in highway traffic model of Canadian highways,
where people are expected to drive on the right hand side of the road, could not be transferred to a highway traffic
model of British highways, where people are expected to drive on the left hand side of the road. The results of such
transfer would be unpredictable and most probably the “Canadian agent” would “cause accidents” and would get
into a stalled state, incapable of moving. In fact the behaviour of the agent is provided by means of a finite state
machine or a set of finite state machines represented either by the Mealy Model (i.e., a finite state machine
generating an output based on its current state and an input; also called “finite state transducer”) or by the Moore
Model (i.e., a finite state machine generating an output based on its current state alone). A problem with all finite
state machines is that their design, verification, validation, coding, and testing becomes progressively harder when
trying to prepare the finite state machine for all possible scenarios beyond a small number.
“Cognitive agents” partially solve this problem by performing “Cognitive Acts” (i.e., a sequence consisting of all
of the following acts: Perceiving, Reasoning, Judging, Responding, and Learning) as opposed to Agents, who
perform “Reflexive Acts” (i.e., Perceiving and Responding), [16].
The rest of this paper is structured as follows. Section 2 discusses functionality & performance requirements of
cognitive agents. Section 3 describes the need for cognitive agents implementation standards. In Section 4 we
presents our proposal for the architecture of an application independent software implementation of a generic
cognitive agent able of providing the required functionality and performance. Section 4 consists of a number of
subsections detailing each layer of a layered hierarchical model akin to the OSI 7 layer Reference Model used in
data communications. We then outline our future plan of work.

A.T. Lawniczak, B.N. Di Stefano / Procedia Computer Science 1 (2012) 2227–2235

2229

A.T. Lawniczak and B.N. Di Stefano / Procedia Computer Science 00 (2010) 000–000

2. Cognitive agents
A cognitive agent is an agent capable of performing cognitive acts, i.e. a sequence of the following activities:
1) Perceiving information in the environment and provided by other agents,
2) Reasoning about this information using existing knowledge,
3) Judging the obtained information using existing knowledge,
4) Responding to other cognitive agents or to the external environment, as it may be required,
5) Learning, i.e. changing (and, hopefully augmenting) the existing knowledge if the newly acquired
information allows it.
All five steps are required to define an act as a cognitive act. If steps (2), (3), and (5) were missing, the act would
be defined as a reflexive act. Thus, any agent could perform sequences of perception and response. Only a cognitive
agent can perform one of the two following sequences (“cognitive acts”):
x “perception”, “reasoning”, “judging”, and “response”
x “perception”, “reasoning”, “judging”, “response”, and “learning”.
A cognitive agent “car” (or “driver”) could be developed and tested in a simulated country where people drive on
the right hand side of the road and then deployed in a simulated country where people drive on the left hand side of
the road. This agent would be able to “perceive” that other agents go along the left hand side of the road. It would be
able to perceive it observing that a certain number of other agents go along the left hand side. Probably, the number
of other agents to be observed would not be very high, maybe 3 or 4. The reasoning of this agent would be simple,
almost child-like (e.g., “if they do this, I do the same….”). The key different between the car (or driver)
implemented as a reflexive agent and the car (or driver) implemented as a cognitive agent is that the former knows
only how to drive on the right side of the simulated road while the latter will drive on the side where the majority of
the cars on the road are driving. A cognitive agent has the ability to react to unexpected situations and the ability to
reason about these unexpected situations in order to react to them.
The functionality and performance of cognitive agents requires replacing the finite state machine typical of all
agents with more complex functional blocks, built using computational intelligence methodology, i.e. applying one
or more of the following methodologies: fuzzy logic, neural networks, evolutionary computation, and various type
of bio-mimicry. Both cognitive and reflexive agents, can easily be implemented as a “class” (using the ObjectOriented paradigm), where the “class” is a generic abstract concept (e.g., car) while an “object” is a specific instance
of that concept (e.g., the specific car plated “Ontario 682 MCW”). An agent can be fully represented by an object of
the appropriate class. Like an object of a class, a valid (i.e., correctly and properly designed) agent is an entity
characterized by: State, Behaviour, and Identity, [17]. State is the set of variables identifying the current condition of
an agent (e.g., vehicle position, velocity, and acceleration in the case of the highway traffic simulation model).
Behaviour is the set of actions that the agent can take (e.g., stop, move, accelerate, and change lane). Identity is what
distinguishes an agent from all others (e.g., serial number, vehicle identification number, plate number, etc).
The Behaviour is implemented by means of “member functions” capable of implementing “reasoning”,
“judging”, and “learning”. The Identity of an agent can be implemented with a “write-only-once” variable
guaranteed to contain some unique information. One can design the agent in such a way that the agent has an ID
assigned at instantiation time using some unique combination of events, which uniquely identify that agent (e.g., a
combination of agent instantiation counter, time counter, and some other variable semantically meaningful in the
application domain of the model.
All animal species, from insects to mammals, so far outperform digital computers in performing cognitive
functions, [18]. The reason for this gap is that animal species use a mix of analog and digital functions emphasizing
“distributed, event-driven, collective, and massively parallel mechanisms, and make extensive use of adaptation, self
organization, and learning”, [18]. Indeed, all human beings drive cars with a certain amount of dealing with
imprecise, incomplete, and vague information. “Fuzzy logic is a precise logic of imprecision and approximate
reasoning.” [19]. Moreover, most of the time, human beings reason and act, as a consequence of their reasoning, without
taking any precise measurement and without performing any precise computation (e.g., we park cars successfully with a
high degree of approximation). Human beings unconsciously map what they perceive (e.g., hear, see, etc.) to an idealized
view of reality. They apply approximate rules to this approximate view of reality and, eventually, produce an approximate
result of their inference. Fuzzy inference, fuzzy reasoning, operates in a similar way, [20], [21], [22], and [23]. A

2230

A.T. Lawniczak, B.N. Di Stefano / Procedia Computer Science 1 (2012) 2227–2235
A.T. Lawniczak and B.N. Di Stefano / Procedia Computer Science 00 (2010) 000–000

description of how a fuzzy inference system works is beyond the scope of this paper. A detailed description can be found
in [24], [24], and [26].
The “Fuzzy Approximation Theorem” proved in 1990 by Bart Kosko, shows that if the input space is compact and the
analytically unknown function is continuous, then an additive fuzzy system uniformly approximates the analytically
unknown function, see [27], [28], [29], [30], [31], and [32]. Because of this theorem, in theory we can translate most
equations of interest into rules. In practice this is not always feasible because of the exponential explosion of the number
of rules due to the level of precision that is required in some applications. Selection of number and shapes of input and
output membership functions, fuzzification and defuzzification algorithms, are still open to the individual’s preference and
are often based on heuristic experience. Neurofuzzy, [33], and connectionist systems, [34], have been used also. Using
genetic algorithms to design fuzzy inference systems is possible and is still the area of research (see for example [35],
[36], and [37]).
Judging can be seen as the systematic assignment of value to available information. Subsets of this activity often
involve:

x “feature extraction”,
x “identification”,
x “estimation”.
Informally, one can say that, in the presence of data that is too large to be processed with available means, “feature
extraction” is equivalent to simplifying the amount of resources required to describe a large set of data accurately, [26].
“Identification” is the construction of a model, mathematical or a rule-based, of a dynamic system using experimental data
from the system itself, [26]. “Estimation” is an algorithmic approximation of some input data which is usable even if the
input data is imprecise, incomplete, and/or uncertain.” [26]. Fuzzy logic and neural networks have shown good results for
feature extraction, identification, and estimation, see for instance [26], [35], [37], [38], and [39].
Neural Networks have been successfully used as learning algorithms (i.e., supervised learning, unsupervised learning,
and reinforcement learning).

3. The need of standards
Most research on cognitive agents focuses on some specific cognitive activity, because developing full fledged
cognitive agents involves a major effort. There is a need of standards allowing integration of different subsets of
cognitive activities developed by different scientists so that individual scientists can concentrate on the modules
where they feel that they can achieve best results.
A number of standard formats for the exchange of messages among agents have been developed. They have been
called “agent communication languages” (ACLs), [40]. The best known ACL is KQML (Knowledge Query and
Manipulation Language), [41], developed by the ARPA (Advanced Research Projects Agency) Knowledge Sharing
Effort, [42]. KQML is a language and protocol for communication among software agents and knowledge-based
systems. This and other standards are based on the fact that “To make agents understand each other they have to not
only speak the same language, but also have a common ontology. An ontology is a part of the agent's knowledge
base that describes what kind of things an agent can deal with and how they are related to each other.”, [43].
KQML is comprised of two parts: the actual “the knowledge query and manipulation language” and the “knowledge
interchange format” (KIF). A detailed description and discussion of KQML is beyond the scope and the available
space of the paper. Interested readers can find further information in [40], [41], [42], and [43]
FIPA (Foundation for Intelligent Physical Agents) “is an IEEE Computer Society standards organization that
promotes agent-based technology and the interoperability of its standards with other technologies.” [44]. As stated
in their web site, “FIPA specifications represent a collection of standards which are intended to promote the
interoperation of heterogeneous agents and the services that they can represent. The life cycle of specifications
details what stages a specification can attain while it is part of the FIPA standards process.”, [45]. Additionally,
“Each specification is assigned a specification identifier as it enters the FIPA specification life cycle.”, [46], and
“The specifications themselves can be found in the Repository. ”, [47]. FIPA has produced and continues to produce
standards for agent communication, agent management, agent message transport, interaction protocols,
communicative acts, content and languages.
The emphasis of these agent communication languages appears to be on the software techniques rather than on
the computational intelligence.

A.T. Lawniczak, B.N. Di Stefano / Procedia Computer Science 1 (2012) 2227–2235

2231

A.T. Lawniczak and B.N. Di Stefano / Procedia Computer Science 00 (2010) 000–000

We agree that, in order to be able to communicate, agents must have agreed on a common set of terms, an
“Ontology”, i.e. a formal representation of a set of concepts within a domain and the relationships. However, we feel
that there should be also a standard set of cognitive acts performed and that they should be performed in a predefined order.
4. Proposal for the architecture for a generic cognitive agent

The order with which the cognitive actions take place is important and so is the order with which the various
computational intelligence techniques are applied. We believe that a hierarchical layered model should be defined
for the generic cognitive agents in a style akin to the hierarchical OSI 7 layer model used in data communication. If
the interface between layers is kept sufficiently generic, one could achieve interoperability similar to the one
possible with the OSI 7 layer model in data communication.
We propose:
x A Perceptual Layer, akin to the physical layer in the OSI 7 layer model
x A Reasoning Layer, to be implemented with one or more fuzzy inference blocks
x A Judging Layer, capable of performing “feature extraction”, “identification”, “estimation”, (each module to be
implemented by means of neuro-fuzzy blocks)
x A Response Layer, akin to the Network Layer
x A Learning Layer, to be implemented with a combination of neural networks and genetic algorithms.
4.1. Perceptual Layer
The Perceptual Layer is the one most difficult to standardize because its function is to perceive information in the
environment and provided by other agents. The implementation of this layer depends on the level of “granularity of
the senses” assumed for the agent. In a hardware implementation of the cognitive agent, this layer would include all
software drivers and software filters for data sensors and transducers and would have the ability to sense and
interpret images, sounds, or other contents of the environments.
This layer would be also responsible for approximating the information acquired by the agent. The need of
approximating acquired information is not immediately clear. In fact in a software model, the acquired information
is always crisp (i.e., precise) because it is read from some software variable. For instance an agent car (or driver)
driving along a simulated highway can see a car ahead and can read or calculate the exact distance of the car ahead.
For instance, it can read that there is a car ahead at a distance of 20 cells (i.e., 150 m). If we are trying to embed
human-like behaviour in a cognitive agent, this is not useful information. No human being will think of the car
ahead as being at 150 m ahead. Thus, the Perceptual Layer should make the acquired information less precise than
normally available in digital computers and should convert it to a representation that is at the same time approximate
and accurate. Thus, the car ahead will be presented as being either “close” or “far”, where “close” and “far” depend
on how fast the car is going. “Close” and “far” depend on how fast the trailing car will come in physical proximity
of the leading car. The meaning of “close” and “far” are context dependent, i.e. they depend on the velocity of the
trailing car with respect to the velocity of the leading car.
The Perceptual Layer should also produce a symbolic abstract representation of the world so that the cognitive
agent can reason on a model of reality. A “Judging Block”, not to be confused with the above Judging Layer, could
be included inside the Perceptual Layer with the purpose of performing “feature extraction”, “identification”, and
“estimation” to create a simplified representation of reality.
If generality is to be achieved, a standard XML, [48], interface between the Perceptual Layer and the Reasoning
Layer should be defined. This interface should include, but not be limited to:
x Information about the number of input and output variables
x The range of values that each variable can acquire, including, where applicable, values consistent with the
“computing with words” paradigm, [49] and [50], that is, if the input and output variables can be described by a
list of linguistic attributes, these attributes should be listed and described in an unambiguous way (i.e., a small
dictionary may have to be transferred between the Perceptual Layer and the Reasoning Layer)
x A symbolic representation of the outside world, both the environment and the other agents present in the
environment.

2232

A.T. Lawniczak, B.N. Di Stefano / Procedia Computer Science 1 (2012) 2227–2235
A.T. Lawniczak and B.N. Di Stefano / Procedia Computer Science 00 (2010) 000–000

4.2. Reasoning Layer
The purpose of the proposed Reasoning Layer is thinking in a coherent way about the information received from
the Perceptual Layer using existing knowledge. This layer makes decisions using the available information and
existing knowledge and passes information about this decision to the Judging Layer.
Fuzzy inference is a good way of imitating what human beings do. Human beings can reason “in an environment
of imprecision, uncertainty, incompleteness of information, partiality of truth and possibility.” [19]. Moreover, most
of the time, human beings reason and act, as a consequence of their reasoning, without taking any precise
measurement and without performing any precise computation (e.g., we park cars successfully with a high degree of
approximation). Human beings unconsciously map what they perceive (e.g., hear, see, etc.) to an idealized view of
reality. They apply approximate rules to this approximate view of reality and, eventually, produce an approximate
result of their inference. Fuzzy inference operates in a similar way, [20], [21], [22], and [23].
The first phase, called “Fuzzification”, interprets the crisp (i.e., discrete) value of each input variable using the
prior knowledge stored in the membership functions of the corresponding fuzzy set. Each membership function is a
form of prior knowledge. It provides the degree of truthfulness of belonging to a set for each input value in the
universe of discourse. There is a membership function for any semantically identifiable subset of the input variables
in the universe of discourse. For each crisp input value a vector is generated, with as many items as the number of
membership functions for that variable. This means that for every crisp input value a number of different assertions
may be made and that the crisp input value may belong to a number of different subsets of the variable in the
universe of discourse.
The next phase is called “Rule Evaluation”. In this phase the relative strength of all active (“firing”) rules is
evaluated. Using the prior knowledge stored in the “Rule Base”, the vector containing the “Fuzzy Outputs is
generated”. The “Rule Base” is a form of prior knowledge. Then we have the “Defuzzification”, the phase where
each conclusion obtained by the inference engine, expressed in terms of (output) fuzzy sets is converted into a crisp
output, using the prior knowledge stored in the membership functions of the corresponding output fuzzy set.
Thus, using fuzzy inference is equivalent to make decisions using the available information (i.e., the information
received from the Perceptual Layer) and existing knowledge, the prior knowledge available in the form of
membership functions for all input variables, rule base, and membership functions for all output variables.
The prior knowledge can be modified by using information acquired from other layers, in particular the Judging
Layer. Also, it is conceivable that the various forms of prior knowledge may be generated by means of genetic
algorithms by observing the behaviour of the fuzzy inference engine.
Fuzzy inference is not the only method that can be used to make decision. Other methods can be employed. For
instance, in some applications pattern recognition may be employed as a simple way of making decisions, [51].
Let’s imagine that a cognitive agent car (or driver) must decide if the road is jammed after the next exit and must
decide if it is good idea to exit the expressway and use a parallel side road or a service road. In this case one of many
pattern recognition algorithms can be used to decide if the viewable part of the expressway fits the definition of
jammed or not.
4.3. Judging Layer
The scope of the Judging Layer is to assign suitable values to available information. This layer receives
information from the Reasoning Layer but also sends information back to the Reasoning Layer. Subsets of this
activity often involve: “feature extraction”, “identification”, and “estimation”.
In spite of the fact that the Perceptual Layer is expected to reduce the precision of the information, there may still
be a problem of too much information. The precision reduction performed by the Perceptual Layer is meant to
convert precise information available because we use a digital computer to the kind of information that a human
being would use, e.g. 150 m becomes “near” when a vehicle is moving at approximately 100 km/h. The problem of
too much information is instead intrinsic to the nature of data. For instance, images, both still and moving, contain a
lot of details that we do not need for the purpose of the decision we have to make. As an example we can think of a
car parking algorithm when dealing with side parking between two parked vehicles. If we use a vision system, we
have access to the full image of the two cars that are already parked. There will be a lot of details, such as colour,
car make logo, plate number, etc. In reality, for the purpose of parking the car, we care only about detecting the

A.T. Lawniczak, B.N. Di Stefano / Procedia Computer Science 1 (2012) 2227–2235

2233

A.T. Lawniczak and B.N. Di Stefano / Procedia Computer Science 00 (2010) 000–000

edges of the cars already parked so that we can park safely between those two cars without touching the edge of
either car. Typical feature extraction algorithms used in image processing are: edge detection, corner detection, blob
detection, and ridge detection. Informally, one can say that, in the presence of data that is too large to be processed
with available means, “feature extraction” is equivalent to simplifying the amount of resources required to describe
a large set of data accurately, [26]. Beside neurofuzzy techniques, dimensionality reduction techniques may be
applicable, e.g.: principal components analysis, multifactor dimensionality reduction, nonlinear dimensionality
reduction, kernel PCA, latent semantic analysis, partial least squares, and independent component analysis.
“Identification” is the construction of a model, mathematical or a rule-based, of a dynamic system using
experimental data from the system itself, [26]. Three approaches are possible: white box, grey box, and black box.
White box models are based on first principles, e.g. using equations from physics. These models tend to be complex
and often impossible to obtain in reasonable time. “Jin et al. describe grey box modeling as assuming a model
structure a priori and then estimating the model parameters. This model structure can be specialized or more
general so that it is applicable to a larger range of systems or devices. The parameter estimation is the tricky part
and Jin et al. point out that the search for a good fit to experimental data tend to lead to an increasingly complex
model. Jin et al. then define a black box model as a model which is very general and thus containing little a priori
information on the problem at hand and at the same time being combined with an efficient method for parameter
estimation.”, [52] and [53].
“Estimation” is an algorithmic approximation of some input data which is usable even if the input data is imprecise,
incomplete, and/or uncertain.” [26]. A simple estimation may be to find an upper and lower bound such that the true, but
unknown, value is contained between those to bounds. Because of the “Fuzzy Approximation Theorem”, if the input space
is compact and the analytically unknown function is continuous, fuzzy systems are good candidates to solve estimation
problems, see [27], [28], [29], [30], [31], and [32]. Fuzzy logic and neural networks have shown good results for feature
extraction, identification, and estimation, see for instance [26], [35], [37], [38], and [39].
It is conceivable that the Judging Layer receives information from the Reasoning Layer but also sends information back
to the Reasoning Layer, because the Reasoning Layer may require more refined, i.e. processed, information to make
decision. A standard XML, [48], bidirectional interface between the Reasoning Layer and the Judging Layer should

be defined.
4.4. Response Layer
The Response Layer is supposed to receive information from the Judging Layer, apply whatever rules are
applicable to the situation at hand, and instruct the Perceptual Layer to issue the appropriate response.
The actual modus operandi of this layer depends on the type of agent and the type of environment. If we assume
that there is no interaction with other agents but that the interaction is only with the environment, this layer could be
just a collection of finite state machines. In fact the Reasoning Layer and the Judging Layer would be responsible
for processing the information is such a way as to provide most of the required intelligence. For instance, in the case
of the car cognitive agent most responses would be of the type “move ahead”, “accelerate”, “decelerate”, “change
lane to the left”, “change lane to the right”, “exit from the expressway”, etc. Most of these responses would be
selected with a “If … Then…” and “If … Then…. Else” decision.
In the case of a cognitive agent expected to exhibit higher level of intelligence, a certain amount of additional
“reasoning” and “judging” could be implemented in this layer. This would be required in the case of cognitive agent
car (or driver) able to operate both in a country where people drive on the right hand side of the road and in a
country where people drive on the left hand side of the road. Various forms of AI would apply, e.g. [54] and [55].
4.5. Learning Layer
The purpose of the Learning Layer is to monitor the operation of the entire agent and to derive new knowledge
from the operation of the agent. Thus, the Learning Layer would be responsible for modifying what we called “prior
knowledge” in the description of the Reasoning Layer, i.e. the membership functions and the rule base. To do so, the
Learning Layer must be implemented with algorithms that allow the cognitive agent to evolve behaviours based on
data available from the other layers. Indeed, the “Learning Ability” could be implemented in each one of the other
layers and tailored to the specific learning applicable to that layer.

2234

A.T. Lawniczak, B.N. Di Stefano / Procedia Computer Science 1 (2012) 2227–2235
A.T. Lawniczak and B.N. Di Stefano / Procedia Computer Science 00 (2010) 000–000

A review of all learning algorithms and methodologies, both within computational intelligence and statistics, is
beyond the scope of this paper.
4.6. Evolution
Some authors add “evolution” to the cognitive acts that a cognitive agent can perform. Evolution can be defined
as “learning extended over generation of agents”. This is clearly not a cognitive act per se but is the ability to
transfer knowledge or attitude over multiple generations of agents.
This step is normally not considered because reproduction of agents is assumed to be difficult or even impossible
to achieve. While this is true for hardware implementations, it is not true for software implementations. In fact
software cognitive agents can reproduce themselves. This can be achieved in two ways: either by “forking” a
process or by using a “copy constructor” at cognitive agent instantiation time. We prefer the second solution because
it requires less memory usage. With this method it is possible to pass to each cognitive agent, “at birth” (i.e.,
instantiation time), all accumulated knowledge of the species of cognitive agents to which it belongs. Every table or
any algorithm that has been built by all existing (i.e., previously instantiated and deployed) cognitive agents is
periodically copied to a template (i.e., not deployed) cognitive agent and subsequently copied to every newly
instantiated cognitive agent. This is a form of collectively acquired and transmitted experience of the species. Using
a forking mechanism would be akin to genetic mutation transmitted from parent to child and would resemble more
accurately evolution, but it would result in slower evolutionary transmission of knowledge.
A philosophical advantage of accommodating evolution with a copy constructor is that it does not imply
necessarily a genetically inherited knowledge, but it could imply very early childhood training. For instance,
somebody born at the end of the 19th century in an industrialised country had to learn how to deal with the telephone
at some point in life, while somebody born at the beginning of the 21st century knows how to deal with the telephone
almost from birth. It is not genetically inherited but it is a form of early childhood training. It can be handled with a
“copy constructor” at cognitive agent instantiation time.
5. Future work
We plan on developing a software test bed to explore various types of topologies of fuzzy approximators and
neural network learning networks, adding the ability to evolve these two functional blocks by means of genetic
algorithms. We will then connect together these blocks in hierarchical fashion to simulate the full spectrum of
cognitive acts, i.e. “perception”, “reasoning”, “judging”, “response”, and “learning”. We will identify a number of
reference problems and apply our software test bed to the reference problems. Our investigations will not be limited
to the above mentioned computational intelligence methodologies, but will include other types of biomimicry,
learning, and pattern recognition algorithms.
Acknowledgements
The authors acknowledge the contribution of Prof. Konstantinos N. Plataniotis who invited them to a “Cognitive
Agents Roundtable” at IPSI (“Identity, Privacy and Security Institute”, at the University of Toronto), on March 13,
2009. The authors were inspired by the problems discussed at this roundtable and decided to search a solution for
some of these problems. A. T. Lawniczak acknowledges partial financial support from the Natural Science and
Engineering Research Council (NSERC) of Canada. B. N. Di Stefano acknowledges full financial support from
Nuptek Systems Ltd.
References
1.
2.
3.

J. Von Neumann, and A. W. Burks, Theory of self-reproducing automata, Urbana, University of Illinois Press, 1966. [Note: 19521953 manuscripts by Von Neumann postumously edited and published by A. W. Burks].
M. Gardner, Scientific American 223:4 (April 1970) 120.
T.C. Schelling, Journal of Mathematical Sociology, Vol. 1 (1971).

A.T. Lawniczak, B.N. Di Stefano / Procedia Computer Science 1 (2012) 2227–2235

2235

A.T. Lawniczak and B.N. Di Stefano / Procedia Computer Science 00 (2010) 000–000
4.
5.
6.
7.
8.
9.
10.
11.
12.
13.
14.
15.
16.
17.
18.
19.
20.
21.
22.
23.
24.
25.
26.
27.
28.
29.
30.
31.
32.
33.
34.
35.
36.
37.
38.
39.
40.
41.
42.
43.
44.
45.
46.
47.
48.
49.
50.
51.
52.
53.
54.
55.

R. Axelrod, The Complexity of Cooperation: Agent-Based Models of Competition and Collaboration, Princeton: Princeton University
Press, 1977.
S. Maerivoet and B. De Moor, Physics Reports, vol. 419, nr. 1 (2005) 1.
K. Nagel and M. Schreckenberg, J. Physique I 2 (1992) 2221.
M.E. Larraga, J.A. del Rio, A. Schadschneider, J. Phys. A: Math. Gen. 37 (2004) 3769.
A. T. Lawniczak and B. N. Di Stefano, in Automata 2008: Theory and Applications of Cellular Automata, editors A. Adamatzky, R.
Alonso-Sanz, A. Lawniczak, G. Martinez, K. Morita, T. Worsch, Luniver Press, 2008, pp. 527-541.
B. Chopard and M. Droz, Cellular Automata Modelling of Physical Systems, Cambridge University Press, 1998.
N. Boccara, Modeling Complex Systems. Springer-Verlag New York, Inc., 2004.
B. Chopard, P.A. Queloz and P.O. Luthi, in Proc. of the 3rd European Connection Machine Users Meeting, Parma, Italy, 1995.
B. Chopard, P.O. Luthi, and P.A. Queloz, J.Phys. A, Vol.29 (1996) 2325.
B. Chopard, A. Dupuis and P.O. Luthi, in Proceedings of Traffic and Granular Flow, 1997.
J. Esser and M. Schreckenberg, Int. J. Mod. Phys. C, 8 (1997) 1025.
P.M. Simon and K. Nagel, Phys.Rev. E, 58 (1998) 1286.
J. Ferber, Multi-Agent Systems. An Introduction to Distributed Artificial Intelligence, Addison Wesley, London, 1999, page 8.
G. Booch, Object-Oriented Analysis and Design With Applications, Addison-Wesley Professional, 1993, Chapter 3.
G. Indiveri, E. Chicca, R. J. Douglas, “Artificial Cognitive Systems: From VLSI Networks of Spiking Neurons to Neuromorphic
Cognition”, Cognitive Computing, Springer Science+Business Media, LLC 2009, 9 pages (published on-line 14 January 2009).
L.A. Zadeh, IEEE Computational Intelligence Magazine, Volume 3, Issue 3 (August 2008) 11.
L.A. Zadeh, Information and Control, Vol. 8 (1965) 338.
L.A. Zadeh, Information Sciences, Vol. 8 (1975) 199, 301; Vol. 9 (1975) 43.
L. A. Zadeh, Synthese 30 (3-4) (1975) 407.
L.A. Zadeh, Fuzzy Sets and Systems, Vol. 1, No. 1 (1978) 3.
G.J. Klir and B. Yuan, Fuzzy Sets and Fuzzy Logic: Theory and Applications, 1/e, Prentice Hall PTR, 1995.
H.-J. Zimmermann, Fuzzy Set Theory - And Its Applications, Third Edition, Kluwer Academic Publishers Group, 1996.
M. Kevin, Passino and S. Yurkovich, Fuzzy Control, Addison Wesley Longman, Menlo Park, CA, 1998.
B. Kosko, Fuzzy Engineering, 1/e, Prentice Hall, 1996.
B. Kosho, in Proceedings of the First IEEE Conference on Fuzzy Systems (FUZZ-92), pages 1153-62, San Diego, March 1992.
B. Kosko, Fuzzy Thinking – The New Science of Fuzzy Logic, Hyperion/Disney Books, 1993, Chapter 10, page 161.
J. A. Dickerson and B. Kosko, in Proc. World Congr. Neural Networks (WCNN-93), vol. 2 (July 1993) 9.
J. A. Dickerson and B. Kosko, IEEE Trans. Syst., Man, Cybern., pt. B, vol. 26 (1996) 542.
B. Kosko and S. Mitaim , IEEE Transactions on Fuzzy Systems, vol. 9, no. 4 (2001) 637.
B. Kosko, Neural Networks and Fuzzy Systems: A Dynamical Systems Approach to Machine Intelligence, Englewood Cliffs, NJ:
Prentice Hall, 1992.
N. Kasabov, IEEE Computational Intelligence Magazine, Volume 3, Issue 3 (2008) 23.
H. Ishibuchi, K. Nozaki, N. Yamamoto and H. Tanaka, IEEE Transactions on Fuzzy Systems, Vol 3, Issue 3 (1995) 260.
K.M. Chow and A.B. Rad, Fuzzy Sets and Systems, Volume 132 , Issue 2 (2002) 147.
S. Mitra and Y. Hayashi, IEEE Transactions on Neural Networks, Volume 11, Issue 3 (2000) 748.
J.J. Buckley and L.J. Jowers, Simulating Continuous Fuzzy Systems”, Series: Studies in Fuzziness and Soft Computing , Vol. 188
(2006) Chapter 3, “Fuzzy Estimation”.
W. Yu, X. Li, IEEE Transactions on Fuzzy Systems, Volume 12, Issue 3 (2004) 411.
M. Wooldridge, An Introduction to MultiAgent Systems - Second Edition, John Wiley & Sons, 2009.
http://en.wikipedia.org/wiki/Knowledge_Query_and_Manipulation_Language
http://en.wikipedia.org/wiki/DARPA_knowledge_Sharing_Effort
http://en.wikipedia.org/wiki/Agent_communication_language
http://www.fipa.org/
http://www.fipa.org/specifications/lifecycle.html
http://www.fipa.org/specifications/identifiers.html
http://www.fipa.org/repository/index.html
http://www.w3.org/TR/REC-xml/
L.A. Zadeh, IEEE Trans. on Fuzzy Systems, vol. 4 (1996) 103.
L.A. Zadeh, IEEE Trans. on Circuits and Systems–I: Fundamental Theory and Applications, vol. 4 (1999) 105.
S. Theodoridis, K. Koutroumbas, Pattern Recognition, Fourth Edition, Academic Press, 2008
Jin, Sain, Pham, Spencer, Ramallo, in Proceedings of the American Control Conference Arlington, VA June 25-27, 2001
http://en.wikipedia.org/wiki/System_identification
M. Margaliot, IEEE Computational Intelligence Magazine, Volume 3, Issue 3 (2008) 38.
J. Weng and W.-S. Hwang, IEEE Computational Intelligence Magazine, Volume 1, Issue 3 (2006) 15.

