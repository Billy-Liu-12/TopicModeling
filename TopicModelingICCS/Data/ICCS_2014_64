Procedia Computer Science
Volume 29, 2014, Pages 1816–1824
ICCS 2014. 14th International Conference on Computational Science

In Need of Partnerships – An Essay about the
Collaboration between Computational Sciences
and IT Services
1

2

Anton Frank1, Ferdinand Jamitzky1, Helmut Satzger 1,
and Dieter Kranzlmüller1,2*

Leibniz Supercomputing Centre (LRZ), Garching n. Munich , Germany
MNM-Team, Ludwig-Maximilians-Universität München (LMU), Munich, Germany
kranzlmueller@mnm-team.org

Abstract
Co mputational Sciences (CS) are challenging in many aspects, not only fro m the scientific do main
they address, but especially also fro m the needs of the most sophisticated IT in frastructures to perform
their research. Often, the latest and most powerful supercomputers, high-performance networks and
high-capacity data storages are utilized for CS, wh ile being offered, developed and operated by experts
outside CS. This standard service approach has certainly been useful for many domains, but more and
more o ften it represents a limitation to the needs of CS and the restrictions of the IT services. The
partnership initiative ȧ CS established at the Leibniz Superco mputing Centre (LRZ) moves the
collaboration between Co mputational Scientists and IT service providers to a new level, moving fro m
a service-centered approach to an integrated partnership. The interface between them is a gateway to
an improved collaboration between equal partners, such that future IT services address the
requirements of CS in a better, optimized, and more efficient way. In addition, it sheds some light on
future professional development.
Keywords: Computational Science, IT Services, Interfacing, Collaboration, Partnership

1 Introduction
Simu lation is accepted as the third p illar of research, where scientists utilize co mputers to advance
scientific breakthrough in a variety of domains. Scientific co mputing determines the usage of
computing resources to analyze and solve scientific problems . Scientific areas that strongly depend on
scientific co mputing methods are subsumed under the term “Co mputational Sciences”. Please note
*

Corresponding author

1816

Selection and peer-review under responsibility of the Scientiﬁc Programme Committee of ICCS 2014
c The Authors. Published by Elsevier B.V.
doi:10.1016/j.procs.2014.05.166

In Need of Partnerships...

A. Frank, F. Jamitzky, H. Satzger and D. Kranzlm¨
uller

that the plural is used on purpose, as we indicate not only the scientific field of co mputational science,
but also instead any scientific field, which apply large-scale co mputing resources for their scientific
work. In most scientific fields , the computer is accepted as probably the most powerful tool of many
scientific do mains today, and at the same time, the limitat ion s of the computational power represent
often the limitations of the scientific problem solving process. These facts are well -known in many
scientific do mains and have lead to the installation of mo re and more powerfu l co mputers, so -called
supercomputers, and the presentation of the Top 500 supercomputer list † every 6 months shows a turnover rate of around 50 percent of the machines on the list, denoting a steady need for these machines.
But how are computational scientists using these machines? In most cas es, they start the
development of the simu lation codes on their o wn desktop mach ine, somet imes even their own
departmental cluster. In this case, the master of the machines, running, operating and maintaining
them, is a person with in their research group, potentially sitting next door, who can be asked for
support, change installations and modify the existing systems, and might even accommodate the needs
of the scientists in future purchases. However, once the software runs and scales, the scientific
breakthrough is limited by the available local hardware and the scientist needs to access machines
fro m a local, reg ional, national or even international provider, again either directly or v ia jo int efforts
such as EGI ‡ , PRACE § or XSEDE ** or via established commercial providers. In most cases, the
scientist needs to adhere to an established application process, where computational needs have to be
justified by providing a proposal, which is subsequently evaluated by experts of the steering
committee according to s cientific quality as well as expected efficiency of resource usage. The
steering committee decides, where access is granted and how many core hours are provided to the
respective scientist. Once these cycles are obtained, the scientist may log in to the ma chine, co mpile
the code, run it and download the results fro m so me storage. In case of problems, there is a support
hotline addressable via e-mail, phone, or a trouble ticket system to help the user.
Taking the same scenario fro m the view of the local, regional, national or international IT service
provider, who is running, operating and maintain ing the IT systems dedicated for computational
science simulat ions: The IT service p rovider waits until a user submits a proposal and until it has been
positively evaluated by the expert committee. At this point, new access credentials are init ialized and
provided to the user, so that the scientist can access the machines. Meanwhile, the IT systems have
been installed, put into operation, and are being maintained on a regular basis. Failures that occur are
corrected and updates to the software and hardware are installed, often without users noticing it. The
only interface to the user is the support hotline, which tries to solve the problems of scientists when
using the machines.
If all works s moothly, then this process describes (with minor deviat ions) the most common
scenario, and in practice, this approach has often supported the delivery of the latest scientific
breakthroughs. However, today’s situation has changed in several ways:
x

x

Using remote resources is increasing with the availability of h igh -speed networks, wh ich
allo w accessing computational resources almost anywhere on the planet. Th is increases the
number of systems a user is able to access, making portability and accessibility important
aspects.
Using the machines is getting more and more co mplicated, especially with large nu mbers of
cores, sophisticated and interconnected software stacks, and often specific characteristics of
these machines.

†

www.top500.org
www.egi.eu
§
www.prace-project.eu
**
www.xsede.org
‡

1817

In Need of Partnerships...

x
x
x
x

A. Frank, F. Jamitzky, H. Satzger and D. Kranzlm¨
uller

Code porting and optimizat ion often eats up more work t ime than is gained by performance
improvements; supercomputers are often oversubscribed leading to increased queue waiting
times, therefore delaying research rather than speeding it up.
Operating and running IT resources is often more cost-expensive than the original capital
expenditure, meaning that it is cheaper to buy a new mach ine than to run it during the
projected life time.
The provisioning of the largest IT machines on the planet requires dedicated buildings with
specialized cooling and power equip ment, and it takes much longer to install a new co mputer
if a new infrastructure building needs to be built.
The costs of supercomputers are often determined by their energy and environ mental costs,
and already today, the execution of a single code can lead to tremendous power bills.
Therefore, users need to be made aware of their energy envelope to ensure that energy is
saved whenever possible.

These issues have to be taken into consideration for both, computational scientists using these
mach ines and IT service providers. We believe that the current approach is insufficient, specifically
for state-of-the-art research, and that the current customer-provider relat ionship needs to be replaced
by a partnership, where both collaborators work on equal footing. As we will shown, only such an
integrated approach will ensure that computational scientists get access to the most appropriate tool for
their research, while IT services providers are enabled to operate their infrastructures at optimum.
This paper describes the partnership init iative π CS established at the Leibniz Superco mputing
Centre (LRZ) to address the issues described above. The paper is structured as follows: We introduce
the petaflop-scale machine SuperM UC as a representative of one of the most powerful IT
infrastructures today in Section 2. The architecture and specifics of SuperMUC are introduced,
focusing on the challenges for the computational scientist when using these machines . Section 3
describes our Extreme Scaling Workshop Series, which has been established as an incentive for
scientists to bring their codes to the largest machines available. At the same time, this ev ent and its
results demonstrate the importance of working more closely with scientists, not only t o get their codes
running, but also to ensure their optimal execution. Finally, Sect ion 4 describes the concept of the
partnership initiative, its current implementation as well as some first examples, where we try to
provide this new initiat ive to potential partners. The entire paper is less a research paper in scientific
computing, but more an essay that highlights difficult ies and issues of computational sciences in
today’s world. We hope to in itiate so me d iscussion about the issues and potential improve ments for
future CS applications and future IT infrastructures.

2 Using the Peta-Scale System SuperMUC
The most-powerful system at the LRZ today is the peta-scale machine SuperMUC, which became
operational in 2012 and represents even today one of the top 10 fastest computers on the planet –
according to the Linpack benchmark and the Top 500 supercomputer list (Meuer, Strohmaier,
Dongarra, & Simon, 2013). The first phase of SuperMUC contains 155,656 cores in 9,421 co mpute
nodes, mo re than 300 Terabyte of main memory, and a h igh-speed Infiniband FDR network. The
theoretical peak performance of SuperMUC is mo re than 3 Petaflop/s (=10 15 Floating point operations
per second). A sketch of the system is shown in Figure 1.

1818

In Need of Partnerships...

A. Frank, F. Jamitzky, H. Satzger and D. Kranzlm¨
uller

Figure 1: SuperM UC System Architecture

SuperMUC has been set-up as part of the German strategy on supercomputing imp lemented by the
Gauss Centre for Superco mputing (GCS) (Müller & et al., 2014) in o rder to strengthen the German
position in Europe and world-wide by delivering outstanding compute power and integrating it into
the European High Performance Co mputing ecosys tem. As such, SuperMUC is part of the European
IT infrastructure for research, and LRZ acts as a Tier-0 centre of PRA CE, the Partnership for
Advanced Co mputing in Europe (PRACE AISBL, 2014), and there as a Eu ropean Centre for
supercomputing. The SuperMUC machine is therefore available to all Eu ropean researchers with the
goal to further expand the frontiers of science and engineering.
For the users, it is then important to access SuperMUC and get their codes running . For this, a
project proposal†† is required, where necessary information such as amount of CPU time, temporary
and permanent disk space is specified. New users also have the possibility to ask for a small test
account to learn more about SuperMUC’s architecture ‡‡ - see Figure 1 and to investigate the suitability
of SuperMUC’s architecture for their code. Pro jects with more than 35 million core hours have to be
submitted via the Gauss Call for Large-Scale Projects §§ , where the user has the possibility to choose
between different GCS machines, and provides a description about the scientific project through a
web-based questionnaire.
In fact, the financial costs of running a mach ine such as SuperMUC should not be taken too easily.
Let us estimate the amount of power needed for the machine and its surrounding infrastructure: While
the electrical power consumption is kept belo w 2.3 Megawatts for phase 1 of SuperM UC, it still
represents a large portion of the financial investment needed to operate this large-scale infrastructure.
Assuming that the current price for electricity in Germany is somewhere around 18 eurocents per
kilowatt-hour, the costs of operation of SuperMUC fo r one year add u p to more than 3.6 M illion Euro
††
‡‡
§§

https://www.lrz.de/services/compute/supermuc/projectproposal/
http://www.lrz.de/services/compute/supermuc/access_and_login/
https://www.lrz.de/services/compute/supermuc/calls/

1819

In Need of Partnerships...

A. Frank, F. Jamitzky, H. Satzger and D. Kranzlm¨
uller

per year for the machine only, without any other equipment in the room. Additional costs include
especially running the auxiliary services on SuperMUC, the support personnel, and specifically the
cooling equipment.
Breaking this down for a very simplified small example, we assume that we have a code running
on 10,000 cores for a day (24 hours), the power bill (again just for the core system) would be approx.
650 Euros. A commercial cloud provider would charge much more and with the efficiency of
SuperMUC, we assume that today this is the lowest possible price. However, the scientist does not
have to pay for using the supercomputer, not even if the code contains an error and the run has to be
repeated after correction. Instead, the user can utilize SuperMUC as long as the scientific requirements
as specified in the original proposal are met and enough cycles as assigned by the scientific co mmittee
are available.
However, the goal of the IT provider is d ifferent. With the fact that funding is limited, even the
provisioning of a powerfu l mach ine such as SuperMUC has its limitat ions. One examp le is the optimal
clock frequency of the processor. If the machine is operated at the upper limit of the clock frequency,
it has quite different power consumption co mpared to operation at the lower clock frequency.
However, a higher clock frequency does not necessarily result in better performance. Instead, if the
application is memo ry bound, meaning that the memory access determines the perfo rmance of the
application, an increased clock frequency does not reduce the execution time, as the processor spends
lots of time waiting on memory accesses to be completed.
The problem is now that the user has to know these limitations and has to adapt the usage behavior
to the optimal execution mode of the IT infrastructure. Additionally, the user is required to port the
application to the system and to scale it to a specific minimu m nu mber of cores , ignoring other
difficult tasks such as data access and s torage, network bandwidth, and the fact that all these resources
are usually shared with other users .

3 The Extreme Scaling Workshop
Using machines such as SuperMUC with more than hundred thousand cores is certainly a
challenge, but if it can be managed, then it might allow unprecedented scientific applications and
ground breaking results. With the complexity of the architecture increasing as described above, we can
also observe that the usage of the IT infrastructure gets even more difficult, especially if new
operational restrictions are put into place which need to be respected by the users. The increasing
numbers of processors provides a simple but straightforward examp le. It is a well-known fact that
using twice as many cores does not necessarily mean a doubling of perfo rmance. Instead, it means that
overheads need to be taken into consideration and codes need to be made scalable. This might be
feasible for small numbers of cores, but certainly beco mes challenging on todays supercomputer
architectures with more than 10,000 or even 100,000 cores. Th is issue of scalability has been known in
the community for quite a while and has been a constant source of worries. Yet, in retrospect it seems
that the biggest necessity is to motivate the users to address this issue by themselves. At the LRZ, we
started the Extreme Scaling Workshop series with the idea to offer an incentive to users, to adapt their
codes to the ever-increasing numbers of processors.
The First SuperMUC Extreme Scaling Workshop took place in July 2013 and was a huge success.
As participants, we had 15 international projects, which wanted to port their code to a large Peta -scale
mach ine like SuperMUC. Of course, we couldn’t just offer access to SuperMUC for anybody, so we
required as a prerequisite that the codes were already scaling up to 4 islands on SuperMUC , (32,768
cores). Four islands are also the maximu m that regular users can allocate during regular Supe rMUC
operation, and as such, we selected the best scaling codes already running on the system. To
successfully comp lete the workshop, the participants had to demonstrate scaling on more than 64000
cores, which means doubling the number of cores being used so far.

1820

In Need of Partnerships...

A. Frank, F. Jamitzky, H. Satzger and D. Kranzlm¨
uller

The participating software pro jects came fro m a variety of different computatio nal science
domains, including software packages such as LAMMPS (Plimpton, Tho mpson, & Cro zier, 2013),
Vertex (Dannert, Marek, & Rampp, 2013), GA DGET (Springel, Naoki, & White, 2001), WaLBerla
(Feichtinger, Göt z, Donath, Iglberger, & Rüde, 2009), BQCD (Nakamu ra & Stüben, 2010), Gro macs
(Lindahl, van der Spoel, & Hess, 2013), APES, SeisSol (Käser, Castro, Hermann, & Pelties, 2010),
CIA O. Co rresponding scientific do mains were mo lecular modeling, plas ma physics, quantum physics,
computational fluid dynamics, astrophysics, seismology, and combustion simulation.
The incentive to scale up the application was to use more than the regular 4 islands of SuperMUC
for a part icular p rogram run. In addition, the cycles consumed during the workshop were not
accounted for the users. The entire SuperMUC system had been reserved for 2.5 days. At most, 16
islands of SuperMUC (appro x. 128,000 cores) have been used . The remaining islands were not
available for the workshop. The computing t ime used during these two days summed up to 6.1 million
core hours, which is in the order of the allocation for a typical project on SuperMUC. Fu rthermore, the
participants were invited to attend the LRZ M ini-Sy mposiu m at the ParCo conference in September
2013 including a publication about their successful program run.

Name
Linpack
Vertex
GROMACS
Seissol
waLBerla
LAMMPS
APES
BQCD

MPI
IBM
IBM
IBM, Intel
IBM
IBM
IBM
IBM
Intel

# cores
128000
128000
64000
64000
128000
128000
64000
128000

Description
TFlop/s/island TFlop/s max
TOP500
161
2560
Plasma Physics
15
245
Molecular Modelling
40
110
Geophysics
31
95
Lattice Boltzmann
5.6
90
Molecular Modelling
5.6
90
CFD
6
47
Quantum Physics
10
27

Table 1: Results from the First SuperM UC Extreme Scaling Workshop

The results of the workshop are shown in Table 1, where 7 codes were able to scale up to at least
64,000 co res. The first row contains the results of the Linpack benchmark on 128,000 cores, which
reaches 2,560 TFlop/s and therefore comes close to the peak performance of the system. (Note: The
theoretical peak performance on 16 islands of SuperMUC is 2,690 TFlop/s.) The names of the
software packages are shown in colu mn 1, while the applied M PI version is shown in colu mn 2.
Colu mn 3 contains the maximu m nu mber of cores being used by that particular software system, while
a description of the code is shown in Column 4. The achieved performance per island is shown in the
fifth colu mn, while the sixth colu mn contains the maximu m performance for t hat particular software
package as achieved with the nu mber of cores indicated in colu mn 3. Colu mn 6 was also used as a
sorting key. It is, of course, interesting to see that the actual sustained performance is far fro m the peak
performance and fro m the performance of Linpack on the system, but this is not unexpected as other
top 10 systems report similar values for their applications.
The most important result, however, was the fact that codes could indeed be scaled up to almost
the full machine, wh ich means that new scientific models with larger data sets, higher resolutions or
higher co mputational co mplexity are now feasible. Of course, there were a number of other lessons
learned with this experiment:
x
x

Hybrid codes, using both MPI and OpenMP, are still s lo wer on SuperMUC than pure MPI
(e.g. GROMACS), but applications scale to larger core counts (e.g. VERTEX)
Core p inning needs a lot of experience by the programmer and is currently not feasible for
most applications

1821

In Need of Partnerships...

x
x

A. Frank, F. Jamitzky, H. Satzger and D. Kranzlm¨
uller

Parallel I/O still remains a challenge for many applications, both with regard to stability and
speed.
Several stability issues with the parallel file system GPFS were observed for very large jobs
due to writ ing thousands of files to a single directory. This will be imp roved in the upcoming
versions of the application codes.

Additionally, several groups used the experiences fro m the workshop and improved their codes
even further. By the time of this writing, our colleagues from Astrophysics were able to perform the
world ’s largest simu lation of supersonic compressible turbulence with a nu merical grid resolution of
4,0963 points, and the earthquake simu lation SeisSol (mo re than 10^12 unknowns) obtained almost 1
Petaflop/s of sustained performance on SuperMUC.

4 The Partnership Initiative π CS
The Extreme Scaling Workshop at LRZ has demonstrated that the close and personal collaboration
between the users and the IT is essential for success . The optimizat ions of the code and the scaling to
higher nu mbers of p rocessors have been a result of the collaboration between CS and IT experts . Th is
confirms our hypothesis that computational sciences need a much better cooperation between the
application experts and the systems experts, not only to ut ilize the systems as good as possible, but
also to bring the applications to a new level. This intensified collaboration is also the key idea and
therefore further improved through our partnership initiative π CS, which is a new in itiative at the LRZ
to address the special requirements of computational scientists. Below we describe π CS in more detail
to demonstrate, how the gap between CS and IT experts can be reduced.
The primary goal of π CS is to provide individual and tailored support for scientists with co mplex IT
questions in addition to our regular support structures. As a consequence, we should be able to provide
an optimal IT infrastructure involving up-to-date IT technologies for state-of-the-art research to a
larger group of users than before. The users benefit fro m access to the latest technologies and they are
able to better influence future develop ments for future IT services with their o wn requirements. This
ensures coordinated planning of future research projects and corresponding IT infrastructure by
involvement of both parties in the early stages of research projects.
The offers from the IT provider, additional to standard infrastructure and services are:
x
x
x
x
x

Exclusive resources, specialized infrastructure, prototypes, test environments
Software development and quality management support
Tailo red consulting, train ing, workshops, etc. as well as part icipation in academic lectures or
courses
Technology expertise, connection to computer science and mathematics through the IT
providers competence network in IT/CSE
Facilitation between scientific areas with similar IT requirements

For examp le, in a partnership, it will be possible to tailor train ing much better to the needs of the
scientists by specifically addressing their problems. An example is a jo int lecture for students of the
respective field, where the IT provider exp lains the necessary steps to access the complex
infrastructures and helps with the first steps when using these machines.
The most important role in this new fo rm of relat ionship will be the assignment of a dedicated
contact person at the IT provider, which is focused on collaboration with a specific application group
and has the freedom for research. Th is contact is preferably a domain scientist within the same field of
research and with an in -depth knowledge of the operations at the IT provider, or an expert in IT
service provisioning with basic knowledge in the area at hand. In practice this means that the contact

1822

In Need of Partnerships...

A. Frank, F. Jamitzky, H. Satzger and D. Kranzlm¨
uller

point between the user and the IT provider is a person, who “speaks” both languages, the domain
specific language and the computing center language. This is a basic necessity for making th is
interface useful and simplifies the collaboration on both ends.
As a result, there should be an improved learning relat ionship on both sides again, bringing in
domain know-how to the IT provider while bridging gaps in utilizing the IT providers services. To
establish this new form o f relat ionship, the contact person s hould be included in the scientific working
groups and research actions, even working on jo int publications and joint scientific results. In fact, the
IT prov ider should also appear as a partner in jo int proposals for funding, and both partners should
agree on mutual dissemination and outreach activities.
Fro m such an intense scientific relationship we expect the development of new and innovative
services which serve not only the original scientific partners, but will also be generalized to other
scientific areas facing similar challenges. In fact, by collaborating with key user commun ities, we
expect to obtain valuable requirements for future IT services, where mo re and more users will benefit
in the future.

5 Conclusions
The challenges of Computational Sciences exist not only within the field of co mputer science, but
specifically also in the usage of computers for leading-edge domain science. The Extreme Scaling
Workshop at LRZ marks a first highlight on our way to a new kind of collaboration between
computational scientists and IT providers. Besides the lessons learned fro m this workshop, it also
supports the idea that a closer fo rm o f collaboration is needed, specifically for state-of-the-art research
in computational science. The individual support provided to the users leads to better results in the
scientific do main and better utilizat ion of the IT infrastructure, wh ich is beneficial to all part ies
involved. Clearly, the idea of a collaborating partnership needs to be supported by respective measures
when educating young scientists. This requires first of all a change in attitude, which needs to be
communicated to the next generation of domain scientists and IT experts.
The partnership init iative π CS at LRZ has been initiated in autumn 2013 after a long strategic
discussion on how to imp rove the wo rkflo w for co mputational scientists. At LRZ, it has been decided
on focusing on a four init ial scientific do mains, namely astrophysics, geo sciences, life sciences, and
energy research. By putting the focus of thes e areas on environ mental research, we are also able to
improve the society at large, assuring that the investments in IT for the computational sciences are
visible to the general public. However, the π CS is only the latest step in our quest to support the users
of our infrastructure. We are constantly trying to improve not only our service portfolio, but also the
way it is utilized by our “customers”, and we are using this experience to imp rove the overall situation
for future utilization.

6 Acknowledgements
The authors are thankful to the following colleagues at the Leibniz Superco mputing Centre (LRZ):
Prof. Dr. Arndt Bode, Dr. Nicolay Hammer, Dr. Matthias Breh m, Dr. A lexander Block, Dr. Markus
Müller, Dr. Carmen Navarrete, Dr. David Brayford, Dr. Anupam Karmakar, Dr. Herbert Huber, Dr.
Reinhold Bader, and Dr. Helmut Heller for their support and valuable input.

1823

In Need of Partnerships...

A. Frank, F. Jamitzky, H. Satzger and D. Kranzlm¨
uller

References
Carlisle, D. (2010, April). graphicx: Enhanced support for graphics. Retrieved fro m
http://www.ctan.org/tex-archive/ help/Catalogue/entries/graphicx.html
Dannert, T., Marek, A., & Rampp, M . (2013, Oktober). Porting Large HPC Applications to GPU
Clusters: The Codes GENE and VERTEX. Retrieved from http://arxiv.org/abs/1310.1485
Feichtinger, C., Götz, J., Donath, S., Iglberger, K., & Rüde, U. (2009). WaLBerla: Exp loiting
Massively Parallel Systems for Lattice Bolt zmann Simu lations. In R. Trobec, M. Vajtersic, & P.
Zinterhof, Parallel Computing (pp. 241-260). London: Springer.
Käser, M., Castro, C., Hermann, V., & Pelt ies, C. (2010). SeisSol – A Software for Seismic Wave
Propagation Simulat ions. In S. Wagner, M. Stein metz, A. Bode, & M. M. Müller, High Performance
Computing in Science and Engineering, Garching/Munich 2009 (pp. 281-292). Berlin Heidelberg :
Springer.
Lindahl, E., van der Spoel, D., & Hess, B. (2013, December). Gro macs. Retrieved fro m
www.gromacs.org
Müller, C. A., & et al. (2014, January). Gauss Centre for Supercomputing. Retrieved fro m
http://www.gauss-centre.eu/
Meuer, H., St rohmaier, E., Dongarra, J., & Simon, H. (2013, November). Top500 Supercomputing
Sites. Retrieved from http://www.top500.org/
Nakamu ra, Y., & Stüben, H. (2010, October). BQCD - Berlin quantum chromodynamics program.
Retrieved from http://arxiv.org/abs/1011.0199
Plimpton, S., Tho mpson, A., & Cro zier, P. (2013, December). LAMMPS Molecular Dynamics
Simulator. Retrieved from lammps.sandia.gov
PRA CE A ISBL. (2014, January). PRACE Research In frastructure. Retrieved fro m www.p raceri.eu
Springel, V., Naoki, Y., & White, S. D. (2001). GADGET: a code for co llisionless and
gasdynamical cosmological simulations. New Astronomy , 6, pp. 79-117.

1824

