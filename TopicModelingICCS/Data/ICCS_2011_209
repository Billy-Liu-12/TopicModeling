Available online at www.sciencedirect.com

Procedia Computer Science 4 (2011) 869–877

Application of graphics processing unit (GPU) to software in
elementary particle/high energy physics ﬁeld
J. Kanzaki
KEK, Oho 1-1, Tsukuba, Ibaraki 305-0801, Japan

Abstract
We apply graphics processing unit (GPU) to software in elementary particle and high energy physics ﬁeld. We
develop GPU versions of programs for fast calculations of cross sections of Standard Model (SM) processes at the
Large Hadron Colliders (LHC) and those for numerical integration of multi-dimensional diﬀerential cross sections
of these processes via the Monte Carlo method. We summarize our activity on the development of programs which
run on GPU and the results on their performances. Compared with the CPU programs, we obtain around 50 times on
average better performance on GPU. We expect that application of GPU to more general programs which are used in
jobs on GRID will greatly reduce the computing resources necessary for large scale reconstructions and simulations
for the LHC analysis.
Keywords:

1. Introduction
The CERN Large Hadron Collider (LHC) started its operation in 2009 and the experiments at LHC are now
accumulating a huge amount of data. For reconstruction and analysis of accumulated data and also for generation
of simulated data for physics analysis, increase of necessary computing resources to process data within reasonable
time is a serious problem for the LHC experiments. Currently, this problem is solved by the use of GRID computing,
which utilizes computing resources around the world via network. A large number of reconstruction and simulation
jobs for the LHC experiments are processed on GRID.
Recently, a new eﬀort to improve the computing performance further using the high level of parallelism of GPU
has started. By installing GPU’s to the personal computing environment the drastic improvement in computing time
can be expected. Also, their installation to computers on GRID will largely reduce the total computing time which is
necessary to process large scale jobs.
We start developing software that can be executed on GPU for elementary particle and high energy physics. We
expect that further improvement of performance of software which is now used in reconstructions and simulations can
be achieve by parallelizing it by the use of GPU.
2. Computing environment
2.1. GPU
GPU (Graphics Processing Unit) was originally developed to quickly perform the multitude of calculations necessary to render complex moving images on computer displays. It achieves this prompt display output by using many

1877–0509 © 2011 Published by Elsevier Ltd. Open access under CC BY-NC-ND license.
Selection and/or peer-review under responsibility of Prof. Mitsuhisa Sato and Prof. Satoshi Matsuoka
doi:10.1016/j.procs.2011.04.092

870

J. Kanzaki / Procedia Computer Science 4 (2011) 869–877

multi-processors. This property of GPU as a highly parallel processor can be used not only for graphics applications
but also for general purpose computations. GPUs have already been used in scientiﬁc applications which require a
large number of calculations to process data in the ﬁelds of astrophysics and ﬂuid dynamics. Recently, GPUs have
also been used in elementary particle and high energy physics [1, 2, 3]. In these studies programs run on GPU were
shown to be 50 times faster than those run on CPU on average.
We have tested our programs mainly on two GPUs by NVIDIA: the GeForce GTX280 and GTX285. First we
introduced GTX280 and started developing programs and then upgraded the GPU to GTX285. The reason why
we selected NVIDIA’s GPUs is because NVIDIA introduced the software development system, CUDA [4]. CUDA
permits working with familiar programming languages (C/C++) while developing software that can run on a GPU.
Speciﬁcations of these two GPUs are given in Table 1. The GeForce GTX280 was introduced in Jun/2008 with 1GB
global memory and 16k registers. The GeForce GTX285 was introduced in Jan/2009 with the same architecture as
GTX280 with more global memory, 2GB, and with higher clock frequency. Both GPUs have 30 streaming multiprocessors (SM). Since each SM has 8 streaming processors (SP), they have 240 SP in total.
These graphics cards are connected with PCI Express2×16 bus to a host PC which controls them by a Linux of the
Fedora10 (64bit) operating system. The parameters of the host computer we used for this study are summarized in
Table. 2.
In order to compile programs for the GPU, we used the CUDA version 2.3 toolkit which is obtained from the
NVIDIA site [5]. The programs in FORTRAN and C were compiled using GNU compilers gfortran and gcc respectively. The versions of the compilers are summarized in Table. 3.
Table 1: Parameters of GPUs

GeForce
GTX285
Number of
multiprocessors
Number of cores
Total amount of
global memory
Total amount of
constant memory [kB]
Total amount of shared
memory per block [kB]
Total number of registers
available per block [k]
Clock rate [GHz]

Table 2: Host PC environment

CPU
L2 Cache
Memory
Bus Speed
OS

Core i7 2.67GHz
8MB
6GB
1.333GHz
Fedora 10 (64 bit)

GeForce
GTX280
30

240
2GB

1GB
64
16
16

1.48

1.30

Table 3: development environment

nvcc
CUDA Driver
CUDA Runtime

Rel. 2.3 (V0.2.1221)
Ver.2.30
Ver 2.30

gcc
gfortran

4.3.2 (Red Hat 4.3.2-7)
4.3.2 (Red Hat 4.3.2-7)

J. Kanzaki / Procedia Computer Science 4 (2011) 869–877

871

3. Basic test with amplitude calculations
3.1. Amplitude computation on GPU
For the basic test of the performance of programs executed on GPU, we developed programs which computed production cross sections of physics processes at LHC. In these programs, total cross section is computed as a weighted
average of diﬀerential cross sections at many phase space points generated randomly.
Amplitude of physics processes at each phase space point is computed with a library, HEGET [1], which is
converted from a FORTRAN subroutine library, HELAS [6], to C functions for the use on GPU. The weighted average
of diﬀerential cross section at each phase space point, obtained from squared amplitudes and phase space weight, over
generated phase space points gives total cross sections of physics processes. We prepared two types of programs:
• a C program which generates phase space points and computes diﬀerential cross sections with a sequential loop,
and
• a CUDA program which calculates diﬀerential cross sections at each phase space point in parallel on GPU.
These programs has the following structure:
1.
2.
3.
4.

initialization,
event generation for the computation of diﬀerential cross section,,
total cross section calculation, and
ﬁnalization.

For both programs, diﬀerential cross sections computed at step 2 are averaged at the CPU side (step 3).
The “event generation” part (step 2) consists of the following parts:
• generation of phase space points from random numbers,
• cuts on kinematic variables,
• calculation of weights from parton distribution functions (PDF), and
• computation of scattering amplitudes.
In the C programs, the “event generation” part is executed as a loop on events, and in the CUDA programs, it is
executed on GPU.
Source codes for the amplitude calculation for various physics processes are generated by MadGraph/MadEvent [7, 8, 9] with FORTRAN. We develop a program which converts FORTRAN codes with HELAS subroutines
into C codes with HEGET functions. Computed total cross sections are checked with the FORTRAN programs,
MadGraph/MadEvent.
In order to check their performance, we compared computing time necessary for the “event generation” part of both
programs. In the GPU programs, time for data transfer between CPU and GPU is included in the time measurement.
3.2. Results for QED processes
In order to study the properties of programs running on GPU, we started from QED processes, n-photon productions, at the LHC energy.
uu → n photons (n = 2 ∼ 8)
(1)
Since the ﬁnal state of these processes is very simple, we can compare amplitude programs not only converted from
outputs of FORTRAN programs (MadGraph), as described in the previous section, but also those handwritten using
permutations of ﬁnal state photons.
Processing time for each event and ratios of processing time for CPU and GPU computations are plotted in Fig. 1
and 2 In Fig. 1, the measured processing time for one event of n-photons production processes is shown for the GPU
(GTX280) and the CPU. They are plotted versus the number of photons in the ﬁnal state.

872

J. Kanzaki / Procedia Computer Science 4 (2011) 869–877

180

Event Processing Time on GTX280

160

u u → photons

103

Ratio of Processing Time

Processing Time / Event [μ sec]

104

Permutation

102

CPU
10

MadGraph

1

Permutation

MadGraph (divided)

GPU

-1

10

2

3

140
120
100
Permutation

80

MadGraph (divided)

60
40
20

MadGraph
10-2

CPU / GPU(GTX280)
u u → photons

MadGraph

4
5
6
Number of Photons

7

0

8

Figure 1: Event processing times for the GPU and CPU.

2

3

4
5
6
Number of Photons

7

8

Figure 2: Ratio of event processing times (CPU/GPU).

The upper two lines show the event processing time for the computation on CPU. The programs with the MadGraph generated HELAS codes run faster than those based on the permutation amplitude especially for larger number
of photons in the ﬁnal state. For nγ = 8, only the permutation-based program can be compiled on CPU.
In all cases the program on GPU computes one event much faster than the CPU. For the computation on GPU
the HEGET programs converted from the MadGraph-generated HELAS codes runs faster than those based on the
permutations of a single amplitude.
The ratios of event processing time between the CPU and GPU are shown in Fig. 2. The solid curves give the ratios
between the permutation amplitudes on the CPU (the upper CPU points in Fig. 1) and those on the GPU (the upper
GPU points in Fig. 1). The performance ratio is about 120 for nγ = 2 and 3, and it goes down to about 45 for nγ = 6,
7 and 8. A higher-performance ratio is found when we compare the event process time of the MadGraph-generated
amplitude on CPU (the lower CPU points in Fig. 1) and that on GPU (the lower GPU points in Fig. 1). Ratios above
150 are found for nγ = 3 and 4, going down to about 45 for nγ = 7. For nγ = 6 and 7, the HEGET programs from the
MadGraph-generated HELAS codes are too big for the present CUDA compiler, and the results are shown for the GPU
program with multiple kernel calls for one event (denoted as “MadGraph (divided)” on the Figure). Nevertheless, the
performance ratio is about 60 for nγ = 6 and about 45 for nγ = 7. Even more surprisingly, though the divided amplitude
includes more program lines for repeated parts of diagrams than the original amplitude program and requires data
transfers for each reputation of kernel call, the performance is better for the divided amplitude with multiple kernel
calls than the single kernel computation for nγ = 5, with the performance ratio exceeding 100.
4.5
0.9

MadGraph

4

Effect of Unrolling
Unrolled / No-unrolling

Ratio of Processing Time

Ratio of Processing Time

0.8
3.5
3
2.5

Permutation

2
1.5

Ratio of Processing Time

1

Double / Single Precision

0.5

0.7
0.6
0.5

Unroll One Perm

0.4

MadGraph

0.3

Unroll Two Perm
MadGraph (divided)

0.2
0.1

0

2

3
4
Number of Photons

5

0

3

4

5
6
Number of Photons

7

8

Figure 3: Ratio of event processing times (double/single precision)
Figure 4: Eﬀect of unrolling while loops in “permutation” amplitudes
on a GTX280.
on a GTX280.

Compared to CPU where double precision programs run even faster than single precision programs, with the

J. Kanzaki / Procedia Computer Science 4 (2011) 869–877

873

architecture of GPU which we used for the tests in this paper, the support for double precision computation is not
enough. It has only one eighth double precision arithmetic logic unit to single precision one. In Fig. 3, the ratio of
single and double precision computations are plotted. Fig. 3 shows the ratio of event processing time between double
and single precision computations. The program with the double precision computations runs 3-4 times slower than
that with single precision. These ratios include not only speed of ﬂoating arithmetic calculations but also integer
arithmetic computations, memory access and data transfer speed. In comparison, the diﬀerence between single and
double precision computation time on CPU is small.
In the permutation amplitude program, a combination of integer numbers for successive permutations is generated
sequentially within a while loop of the C-language. Since programs with loops (divergent loops) or branches lower
the eﬃciency of parallel computing on GPU, there is a possibility of improving the performance of the permutation
program by spreading out, or unrolling, a part of the while loop in the permutation.
We therefore tested the eﬀect of unrolling the while loop for the permutation generation. Fig. 4 shows the relative
performance of the programs after unrolling a part of the while loop as compared to the processing time presented in
Fig. 1 as “GPU permutation”.
We ﬁnd that the eﬀect of unrolling is signiﬁcant, especially for large nγ processes. The improvement of the
execution speed becomes a factor of 3 for nγ = 7 and 8. Because the event processing time does not change signiﬁcantly
by unrolling the while loop on CPU, the CPU/GPU ratio plotted in Fig. 2 for ‘permutation‘ amplitudes grows from
∼ 45 to ∼ 150 for nγ = 7 and 8 after unrolling two permutations.
Also plotted as a reference in Fig. 4 are the relative performances of the computation with the converted MadGraph
amplitudes as compared to those of permutation amplitudes. It is clearly seen from Fig. 4 that by reducing the while
loop the performance of the program with permutation improves signiﬁcantly for nγ ≥ 5, approaching that of the
MadGraph based programs. The presence of the while loop is probably the main cause of the poor performance of the
permutation amplitudes as compared to the MadGraph generated ones, as shown in Fig. 1.
3.3. Results for SM processes
After the basic tests with QED (and also QCD) processes [1], HEGET is extended from basic interactions for
massless particles to all SM interactions [10].
Among SM processes tested in [10], ratios of event process time between CPU and GPU for four processes are
shown in Figs. 5-8. They are plotted against the number of jets in the ﬁnal state. For the tests with SM processes, we
used the GeForce GTX285 graphic card.
In general the ratio of processing times for process with smaller number of external partons is relatively small. For
these processes with simple ﬁnal states, relative weight of the computations which do not depend on the complexity
of the process, ex. The access to the parton distribution function (PDF) data, is larger than that for processes with
more ﬁnal state particles. For processes with the simplest ﬁnal states, ud → W + and uu → Z, the ratio is ∼ 60.
For process with the maximum number of jets in the ﬁnal states, the ratios also become smaller. Execution of
a large program on GPU which has larger number of local memories costs higher than that on CPU. The ratios for
tt+ 3-jets (except for gg → tt + ggg) are around 30.
Maximum ratios of the processing time appear in between. For single vector boson productions, they become
around 100, and for top quark associated Higgs boson productions they are about 70.
4. Monte Carlo integration and event generation
4.1. Monte Carlo integration (BASES/VEGAS)
The Monte Carlo integration technique is a very powerful tool to integrate complex functions with multi-dimensions.
The program VEGAS [11] and its variants are widely used for Monte Carlo integration. They are based on an iterative
and adaptive Monte Carlo scheme. In these programs each axis of variable is divided into grids, thus the integrand
volume is divided into hyper cubes. Monte Carlo integration is performed in each hypercube and variances from
the hypercubes are used to deﬁne new grid spacings which are used in the next iteration step. The variance of total
integral is reduced iteration by iteration. BASES [12] is one variant of VEGAS that was developed at KEK, and has
been widely used in particle physics calculations for colliders.

874

J. Kanzaki / Procedia Computer Science 4 (2011) 869–877

150

150

WW + jets
Ratio of Processing Time

Ratio of Processing Time

W + jets

100

+

u d → W + jets

50

+

u g → W d + jets
+

u u → W u d + jets

100

0

-

+

-

+

+

+

-

u u → W W u u + jets
u u → W W d d + jets
g g → W W u u + jets

1
2
3
Number of Jets in Final State

0

4

Figure 5: Processing time ratio for W + + n-jet production for
W + → l+ νl (l = e, μ)

0

1
2
Number of Jets in Final State

3

Figure 6: Processing time ratio for W + W − + n-jet production for
W + (W − ) → l+ νl (l− ν¯ l ) with l = e, μ.
150

150

H t t + jets
Ratio of Processing Time

t t + jets
Ratio of Processing Time

-

+

u g → W W u + jets

g g → W+ d u + jets
0

+

u u → W W + jets

50

100

u u → t t + jets

50

g g → t t + jets
u g → t t u + jets

u u → H t t + jets
g g → H t t + jets
u g → H t t u + jets

100

u u → H t t u u + jets

50

u u → t t u u + jets
0

0

1
2
Number of Jets in Final State

3

Figure 7: Processing time ratio for tt + n-jet production for t →
bl+ νl (l = e, mu).

0

0

1
Number of Jets in Final State

2

Figure 8: Processing time ratio for Htt + n-jet production for t →
bl+ νl (l = e, μ) and H → τ+ τ− .

In order to apply GPU to more general computations in high energy physics, we developed the GPU version of
VEGAS/BASES (gVEGAS and gBASES) and tested their performance.1
Multi-dimensional integration programs, VEGAS and BASES, have the following common structure:
1.
2.
3.
4.

initialize parameters,
generate N points consisting of a set of k random numbers within a k-dimensional hyperspace,
evaluate an integrand function at the generated space point,
sum up values of the integrand function and their squares for all phase space points and also within each hyper
cube, and compute their averages and variances,
5. optimize grid spacing after accumulating N function values,
6. repeat 2-5 steps up to M iterations or until the desired accuracy is reached.
In BASES, after M iterations (grid optimization phase) are done, further iteration steps are executed in order to
improve the accuracy of the integration (integration phase). The results of this integration phase are used for event
generation by the program SPRING [12].
In order to transfer the whole system of the Monte Carlo integration onto GPU, all programs should be written
in CUDA [4]. Because both VEGAS and BASES are originally written in FORTRAN, we ﬁrst converted them to C
codes and then transformed them further to CUDA codes.
Especially, VEGAS is written in the old style of FORTRAN with many “GOTO” statements. It had to be rewritten
in a modern style of FORTRAN control structures with a reduced number of “GOTO”’s. Then, FORTRAN source
1 Sample source codes of gVEGAS are available on the web page: http://madgraph.kek.jp/KEK/GPU/gVEGAS/example/. The source
codes of gBASES will also become available soon together with the event generation package, SPRING,

875

J. Kanzaki / Procedia Computer Science 4 (2011) 869–877

codes should be carefully converted into C codes with a special care for the diﬀerence of array indexing between two
programming languages.
For the development of the GPU programs based on the converted C programs, the structure of the program should
be carefully considered to achieve the good performance by the parallelization. Because the generation of space points
(step 2) and the evaluation of an integrand function (step 3) can be executed independently for each point in phase
space, these steps can be parallelized on GPU. In the original CPU programs, the accumulations and summations of
the integrand function values (step 4) are included in a loop over phase space points for the function evaluation (step
3). Because the accumulations and summations are not independent between points in multi-dimensional space, they
have to be separated from the function evaluation step (step 3) on GPU. Hence, the generation of phase space points
(step 2) and the evaluation of the integrand function at all phase space points (step 3) are computed on GPU in parallel,
and their values are transferred to CPU memory. The computed function values are accumulated on CPU and the grid
parameters are optimized based on the accumulated information (steps 4-5). These steps are iterated and the variance
of the integral is reduced.
Due to the limited support for double precision calculations on GPU that we used for this study [2], ﬂoating point
computations on GPU were done in single precision.
4.2. Results for Monte Carlo integration on GPU
In this paper, we report results on the following processes
ud → W + (→ μ+ νμ ) + n gluons (n = 0 ∼ 4)

(2)

with semi-realistic ﬁnal state cuts at LHC. We compared the results and performances for the programs in FORTRAN
and C on the CPU, and on GPU using the computation of cross sections of these processes. For the test of Monte
Carlo integration on GPU, we used the GeForce GTX285 graphic card.
In Table 4 the measured processing time for a single function call is listed for each program. As explained above,
the processing time per single function call is obtained by dividing the total computation time by number of function
calls per iteration (N in step 2).
No. of gluons
0
1
2
3
4

VEGAS [μsec]
FORTRAN
C
1.32 (63.8) 1.06 (51.2)
2.19 (68.8) 1.73 (54.6)
4.19 (84.2) 2.96 (59.5)
11.1 (101) 7.00 (63.6)
72.1 (77.8) 37.4 (40.4)

GPU
0.0207
0.0318
0.0497
0.110
0.927

BASES [μsec]
FORTRAN
C
1.78 (68.7) 1.39 (53.5)
2.97 (75.0) 2.24 (56.6)
4.97 (88.3) 3.35 (59.6)
11.7 (103) 7.02 (62.2)
61.6 (66.2) 31.8 (34.2)

GPU
0.0260
0.0396
0.0563
0.113
0.931

Table 4: Processing time for a single function call in VEGAS and BASES on CPU with FORTRAN or C, and on GPU with CUDA. Numbers in
the parentheses of the FORTRAN and C columns are the ratio of processing time relative to that of GPU.

Numbers in parentheses in the FORTRAN and C columns in Table 4 are the ratio of the processing time as
compared to that of the GPU. About a factor of 50 times more sampling is possible with the GPU as compared to
the C programs on CPU with the same computing time. During the comparison of processing time, we ﬁnd that the
original FORTRAN codes run slower than the C-versions. Because the total processing time of the CPU programs
is dominated by the function (amplitude) computation, this FORTRAN-to-C ratio originates from the diﬀerence in
processing time of the amplitude computation. We ﬁnd that arithmetic operations of complex numbers are faster
in the C programs than in the FORTRAN programs. In particular, the addition of complex numbers which appears
frequently in the amplitude computation is processed about 60% faster in the C programs. We use in-line functions for
the computations of complex numbers in C, which have better eﬃciency compared with built-in complex functions in
FORTRAN.
In Fig. 9 the processing time for a single function call is plotted versus the number of gluons in the ﬁnal state. In
Fig. 10 ratios of the processing time between programs on CPU (FORTRAN/C) and those on GPU are plotted. The
diﬀerences between processing time for VEGAS and BASES are small.

J. Kanzaki / Procedia Computer Science 4 (2011) 869–877

120
103

u d → W+ + gluons

Ratio to Total Processing Time on GPU

Processing Time / Function Call [μ sec]

876

FORTRAN

102

C
10
VEGAS
BASES

1

10-1

0

1

GPU

2

3

Number of Gluons in Final State

4

Figure 9: Processing time of a single function call for ud → W + (→
μ+ νμ ) + n-gluons.

FORTRAN

VEGAS
100

BASES

80

60

40

C
20

0

0

1

2

3

4

Number of Gluons in Final State

Figure 10: Processing time ratios of FORTRAN and C programs to
the corresponding GPU program

Programs which are executed on GPU can run about 50 times faster than those in C on CPU, and even greater
compared to the original FORTRAN code running on CPU.
4.3. Event generation
We also developed the event generation program based on SPRING, which was a part of BASES, on GPU
(gSPRING). Original algorithm of SPRING for the event generation is very simple as a sequential generation of
required number of events. In order to transfer SPRING to GPU, the development of new algorithm optimized for parallel processing is necessary. In Fig. 11 the ratio of total processing time both for Monte Carlo integrations (BASES)
and event generations (SPRING) is shown. Event statistics is increased in order to reproduce smooth distributions of
kinematic variables of ﬁnal state particles. Especially for the case with 3 and 4 gluons which require long processing
time the improvement in processing time becomes around 100. In Fig. 12 the comparison of pT of generated μ’s in
ud → W + (→ μ+ νμ ) + 3-gluons for FORTRAN, C and gSPRING on GPU is shown. We ﬁnd that the agreement of these
distributions generated by three SPRING programs is quite well.

Ratio to Total Processing Time on GPU

140
120

Processing Time Ratio
+
u d → W + n-gluons

Fortran

30000

C

SPRING
100

GPU

25000

BASES

80

20000

60
15000
40
+

ud→ W g g g

10000
20

p (μ)
T

5000
0

0

1

2

3

Number of Gluons in Final State

4
0
0

10

20

30

40

50

60

70

80

90
100
p (μ ) [GeV]
T

Figure 11: Processing time ratios of FORTRAN and C programs
to the corresponding GPU program for Monte Carlo integration
(BASES) and event generation with (SPRING) increased event statistics of ud → W + (→ μ+ νμ ) + n-gluons process.

Figure 12: Comparison of pT of μ’s generated by various SPRING
programs (FORTRAN, C and GPU) for ud → W + (→ μ+ νμ )+3-gluons

5. Summary
We have developed GPU versions of the cross section calculation programs for physics processes at LHC and
the Monte Carlo integration programs which are widely used in elementary particle physics. We compared their

J. Kanzaki / Procedia Computer Science 4 (2011) 869–877

877

performance with those of CPU programs. Compared with FORTRAN and C programs on CPU, GPU versions of
these programs show more than 50 times better performance in execution time on average. We expect that application
of GPU to more general programs in elementary particle/high energy physics which are used for jobs on GRID will
greatly reduce the computing resources necessary for large scale reconstructions and simulations for the LHC analysis.
The large number of multi-processors of GPU is eﬀective to reduce computing time of these programs. And, if more
than 10 times reduction in computing time is achieved, the requirement for disk spaces to keep intermediate data
should also be reduced by serializing successive analysis steps in a single program.
6. Acknowledgment
This work is supported by the Grant-in-Aid for Scientiﬁc Research from the Japan Society for the Promotion of
Science (No.20340064 and No.22740155).
[1] K. Hagiwara, J. Kanzaki, N. Okamura, D. Rainwater and T. Stelzer, Eur. Phys. J. C66 (2010) 477, eprint arXiv:0908.4403; K. Hagiwara,
J. Kanzaki, N. Okamura, D. Rainwater and T. Stelzer, Eur. Phys. J. C70 (2010) 513, eprint arXiv:0909.5257.
[2] J. Kanzaki, to be published in Eur. Phys. J. C, eprint arXiv:1010.2107.
[3] W. Giele, G. Stavenga and J. Winter, FERMILAB-PUB-10-025-T, Feb 2010, eprint: arXiv:1002.3446.
[4] http://www.nvidia.com/content/global/global.php
[5] http://www.nvidia.com/object/cuda home new.html
[6] K. Hagiwara, H. Murayama and I. Watanabe, Nucl. Phys. B367 (1991) 257; H. Murayama, I. Watanabe and K. Hagiwara, KEK-Report 91-11,
1992.
[7] T. Stelzer and W. F. Long, Comput. Phys. Commun. 81 (1994) 357.
[8] F. Maltoni and T. Stelzer, JHEP 0302 (2003) 027.
[9] J. Alwall, P. Demin, S. de Vissher, R. Frederix, M. Herquet, F. Maltoni, T. Plehn, D. Rainwater, T. Stelzer, JHEP 0709 (2007) 028.
[10] K. Hagiwara, J. Kanzaki, Q. Li, N. Okamura and T. Stelzer, in preparation.
[11] G. P. Lepage, J. Comput. Phys. 27 (1978) 192.
[12] S. Kawabata, Comput. Phys. Commun. 41 (1986) 127, Comput. Phys. Commun. 88 (1995) 309.

