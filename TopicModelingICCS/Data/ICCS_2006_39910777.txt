A Novel Supervised Information Feature Compression
Algorithm
Shifei Ding1,2 and Zhongzhi Shi2
1

College of Information Science and Engineering,
Shandong Agricultural University, Taian 271018 P.R. China
2
Key Laboratory of Intelligent Information Processing, Institute of Computing Technology,
Chinese Academy of Sciences, Beijing 100080 P.R.China
sfding@sdau.edu.cn; shizz@ics.ict.ac.cn

Abstract. In this paper, a novel supervised information feature compression
algorithm is set up. Firstly, according to the information theories, we carried out
analysis for the concept and its properties of the cross entropy, then put forward
a kind of lately concept of symmetry cross entropy (SCE), and point out that the
SCE is a kind of distance measure, which can be used to measure the difference
of two random variables. Secondly, We make the SCE separability criterion of
the classes for information feature compression, and design a novel algorithm
for information feature compression. At last, the experimental results
demonstrate that the algorithm here is valid and reliable, and provides a new
research approach for feature compression, data mining and pattern recognition.

1 Introduction
Feature extraction or compression is one of the most importmant steps in pattern
recognition, data mining, machine learning and so on. In order to choose a subset of

the original features by reducing irrelevant and redundant, many feature selection
algorithms have been studied[1,2]. The literature contains several studies on
feature selection for unsupervised learning in which he objective is to search for a
subset of features that best uncovers “natural” groupings (clusters) from data
according to some criterion. For example, principal components analysis (PCA) is
an unsupervised feature extraction method that has been successfully applied in the
area of face recognition, feature extraction and feature analysis[3,4]. But the
method of PCA is effective to deal with the small size and high-dimensional
problems, and gets the extensive application in Eigenface and feature extraction. In
high-dimensional cases, it is very difficult to compute the principal components
directly[5].
Now an important question is how to deal with supervised information feature
compression. In this paper, the authors are going on studying this field on the
basis of these studies. Firstly, we study and discuss the cross entropy theory, and
point out its shortage, then put forward a new concept of symmetry cross entropy
(SCE), and prove that the SCE is a kind of distance measure. Secondly, we regard
SCE as class separability criterion, and design a new algorithm of information
V.N. Alexandrov et al. (Eds.): ICCS 2006, Part I, LNCS 3991, pp. 777 – 780, 2006.
© Springer-Verlag Berlin Heidelberg 2006

778

S. Ding and Z. Shi

feature compression. At last, our experimental results indicate that the proposed
algorithm here is efficient and reliable.

2 Feature Compression Algorithm
In order to set up information feature compression algorithm, we firstly discuss the
following new concept of symmetry cross entropy and feature compression theorem.
2.1 Symmetry Cross Entropy
Shannon put forward the concept of information entropy for the very first time in
1948[6]. The cross entropy (CE), or the relative entropy, is used for measuring
difference information between the two probability distributions. But the CE satisfies
only nonnegativity, normalization and dissatisfies symmetry and triangle inequation.
For this reason, we carry out the improvement, and give the following definition.
Definition 1 Suppose that P = ( p1 , p 2 , " , p n ) and Q = (q1 , q 2 , " , q n ) are two

probability vectors of discrete random variable X , and H ( P || Q ) and
CE of P to Q and Q to P respectively. Then we define

H (Q || P ) are

D ( P, Q) = H ( P || Q ) + H (Q || P)

(1)

It is called Symmetric Cross Entropy (SCE) of P and Q .
We can prove that the SCE satisfies three basic properties as follows.
1) Nonnegativity: D( P, Q ) ≥ 0 , D( P, Q) = 0 ⇔ P = Q ;
2) Symmetry: D ( P, Q) = D (Q, P) ;
3) Triangle inequation: Suppose that W = ( w1 , w2 ,", wn ) is another probability
vector, then D ( P , Q ) ≤ D ( P ,W ) + D (W , Q ) .
Therefore, the SCE is a kind of distance measurement, which can be used to
measure the difference of two random variables. In order to compute conveniently,
we can use the function as follows in accordance with the above D ( P, Q) , i.e.
n

H ( P, Q) =

∑( p

i

− qi ) 2

(2)

i =1

Doing like this, we don't affect the results to select d optimal features.
2.2 Compression Theorem

Suppose that { X (j1) } ( j = 1,2, " , N1 ) and { X (j2) } ( j = 1,2, " , N 2 ) are squared
normalization pattern vectors which belongs to two classes. The kth feature
component of X (j i ) is denoted by x (jki ) (i = 1,2; k = 1,2, " , n) . The square mean of each

A Novel Supervised Information Feature Compression Algorithm

component for every class is γ k(i ) =

1
Ni

Ni

∑(x

779

, Where i = 1,2; j = 1,2,", n .

(i ) 2
jk )

j =1

Obviously γ k(i ) ≥ 0 , and then
n

∑
k =1

γ k(i ) =

n

Ni

∑ ∑
k =1

1
Ni

( x (jki ) ) 2 =

j =1

n

∑γ

Namely γ k(i ) ≥ 0 and

(i )
k

Ni

n

∑ N ∑ (x
1

j =1

(i ) 2
jk )

i k =1

=

Ni

∑N
j =1

1

=1

(3)

i

= 1 . Therefore, we can comprehend {γ k(i )} as the

k =1

probability distribution defined by X (j i ) . Suppose that the (k , l ) element of symmetric
matrix G ( i ) (i = 1,2) is g kl(i ) =
every components of γ

(i )

1
Ni

Ni

∑x

(i ) (i )
jk x jl

. Record γ (i ) = (γ 1(i ) , γ 2(i ) , " , γ n( i ) ) , then

j =1

is element of G ( i ) (i = 1,2) in diagonal line. Let

s = s (γ (1) , γ ( 2) ) =

n

∑ (γ

(1)
k

− γ k( 2) )2

(4)

k =1

According to discussion above, we can get the following compression theorem.
Theorem 1 Suppose that A = G (1) − G ( 2) , and the function s defined by formula (4),
then the SCE=maximum if and only if the coordinate system as s =maximum is
composed of the d eigenvectors in correspondence with eigenvalues satisfied
definite conditions of the matrix A .
2.3 Feature Compression Algorithm

In order to explain the effect of information feature compression, we make the
eigenvalues λi (i = 1,2, " , n) of matrix A arrange λ12 ≥ λ22 ≥ " ≥ λ2d ≥ " ≥ λ2n . The
n

total variance sum of square is denoted by Vn =

∑λ

2
k

, and then the variance square

k =1

ratio is defined as V = Vd Vn , which can measure the degree of information
compression. According to the discussion above, we can get the algorithm of
information feature compression based on SCE as follows.
Step 1 Carry out square normalization processing for the two classes original data,
and get data matrix x (i ) (i = 1,2) .
Step 2 Carry out centralization for obtained data matrix x , and then calculate the
symmetric matrix G (i ) (i = 1,2) and difference matrix A .
Step 3 Calculate all eigenvalues corresponding to all eigenvectors of the matrix
A = G (1) − G ( 2) .

780

S. Ding and Z. Shi

Step 4 Construct information compression matrix T = (u1 , u2 ,", ud ) .
Step 5 Compress data matrix x based on y (i ) = T ′x (i ) (i = 1,2) , and so we reach the
purpose of optimal compression of information feature.

3 Conclusions
According to the definition of cross entropy, we propose a new concept of the SCE,
and point out that the SEC is a kind of distance measure. Based on SCE, we set up a
novel separability criterion, which can be used to measure the difference degree
between the random variables, and construct a compression algorithm for information
feature based on SCE.

Acknowledgements
This study is supported by the National Natural Science Foundation of China
under grant No.60435010, the China Postdoctor Science Foundation under grant
No.2005037439, and the Doctoral Science Foundation of Shandong Agricultural
University under grant No. 23241.

References
1. Duda, R.O.,Hart, P.E. (eds.): Pattern Classification and Scene Analysis. Wiley, New York
(1973)
2. Fukunaga, K. (ed.): Introduction to Statistical Pattern Recognition. Academic Press, 2nd ed.,
New York (1990)
3. Hand, D.J. (ed.): Discrimination and Classification. Wiley, New York (1981)
4. Nadler, M., Smith, E.P. (eds.): Pattern Recognition Engineering. Wiley, New York (1993)
5. Yang, J., Yang, J.Y.: A Generalized K-L Expansion Method That Can Deal With Small
Sample Size and High-dimensional Problems. Pattern Analysis Applications 6(6) (2003)
47-54
6. Shannon, C.E.: A Mathematical Theory of Communication. Bell Syst. Tech. J. 27 (1948)
379-423

