Procedia Computer Science
Volume 51, 2015, Pages 1545–1554
ICCS 2015 International Conference On Computational Science

An Algebraic Approach to Combining Classiﬁers
Philippe J. Giabbanelli1 and Joseph G. Peters2
1

University of Cambridge, Cambridge, United Kingdom. pg438@cam.ac.uk
2
Simon Fraser University, Burnaby, Canada. peters@cs.sfu.ca

Abstract
In distributed classiﬁcation, each learner observes its environment and deduces a classiﬁer. As
a learner has only a local view of its environment, classiﬁers can be exchanged among the
learners and integrated, or merged, to improve accuracy. However, the operation of merging is
not deﬁned for most classiﬁers. Furthermore, the classiﬁers that have to be merged may be of
diﬀerent types in settings such as ad-hoc networks in which several generations of sensors may be
creating classiﬁers. We introduce decision spaces as a framework for merging possibly diﬀerent
classiﬁers. We formally study the merging operation as an algebra, and prove that it satisﬁes
a desirable set of properties. The impact of time is discussed for the two main data mining
settings. Firstly, decision spaces can naturally be used with non-stationary distributions, such as
the data collected by sensor networks, as the impact of a model decays over time. Secondly, we
introduce an approach for stationary distributions, such as homogeneous databases partitioned
over diﬀerent learners, which ensures that all models have the same impact. We also present a
method using storage ﬂexibly to achieve diﬀerent types of decay for non-stationary distributions.
Keywords: Model combination; Non-stationary distributions; Unsupervised meta-learning

1

Introduction

There are numerous examples of processes that take place over a geographical- or time-scale
such that global monitoring requires collaboration among the various information systems that
observe parts of these processes. For example, a forest ﬁre can take place over a large geographical area. While satellites can monitor such an area, the quality is generally not suﬃcient to
detect forest ﬁres until they have grown to a signiﬁcant size. Better response is possible when
ﬁres are monitored by wireless sensor networks [6]. Over a large area or around boundaries, a
ﬁre may be captured by networks reporting to diﬀerent information systems, which should thus
collaborate. Similarly, the spread of a disease can take place over several administrative units
and be observed by diﬀerent health authorities whose information systems could be set to collaborate in order to better guide interventions. Several other applications warranting the need
for collaboration can be found in the series of Workshops on Multiple Classiﬁer Systems [10].
Selection and peer-review under responsibility of the Scientiﬁc Programme Committee of ICCS 2015
c The Authors. Published by Elsevier B.V.

doi:10.1016/j.procs.2015.05.346

1545

An algebraic approach to combining classiﬁers

Giabbanelli and Peters

For both forest ﬁres and diseases, the information systems may not be able to communicate
individual records/observations. Limited battery power of the sensors observing a ﬁre might
limit them to only transmit aggregate information because bandwidth quickly depletes their
batteries. Health authorities may face ethical limitations due to the conﬁdentiality of patient
records. Thus, each information system may only send a summary, or model, based on its observations rather than raw data. We focus on this situation, known as model combination [11].
Solutions for combination have been proposed (e.g., ensemble classiﬁers, meta-learning) and
diﬀer based on whether models are fused or selected ([8], p. 106). These solutions, and particularly fuser design, were recently reviewed in [9]. Intuitively, models are specialized when they
are restricted to instances with speciﬁc features: when a new instance comes in, the appropriate model is thus selected. Alternatively, models may be designed for instances with the same
features and they are then fused. An intermediate case can occur in these examples: there
can be overlaps in the feature space (e.g., two health authorities can observe populations with
very similar demographics), but some models may be more competent in parts of that space
than others (e.g., one health authority had a large sample of minorities whereas this was less
prevalent for the other health authority). To emphasize that our approach is intermediate, we
say that the models are merged rather than selected or fused.
In this paper, we consider that each information system summarizes its data using a classiﬁer. In other words, observations contain predictive attributes (e.g., age, gender) and target
attributes (e.g., sick or susceptible), and a data mining algorithm is used to relate them. The
technical goal is thus to merge classiﬁers. However, this operation is complicated for at least two
reasons. Firstly, classiﬁers can be produced by a variety of algorithms (e.g., decision trees, support vector machines), and we thus have to combine objects with very diﬀerent mathematical
structures. Secondly, classiﬁers can diﬀer in how predictive attributes are connected to target
attributes. Solving these conﬂicts either uses heuristics (e.g., pure meta-learning [2, 12, 5]),
or requires access to the raw observations, which is not feasible in the setting considered here.
Therefore, we have to perform meta-learning with classiﬁers of potentially diﬀerent types.
Contributions of the Paper. Numerous solutions have been proposed for meta-learning
with heterogeneous classiﬁers. However, these solutions are typically evaluated experimentally,
and less attention is paid to their theoretical properties. This has led researchers to suggest
that questions about combining classiﬁers are (still) open “because we do not yet have a scientiﬁc understanding of the classiﬁer combination mechanisms” [7]. This paper contributes to
developing such an understanding by emphasizing a formal analysis of combination as follows:
• We investigate the combination of heterogeneous classiﬁers (e.g., decision trees, rule sets,
support vector machines with linear kernels) under pure meta-learning using a new algebraic framework.
• We show that key properties of the combination can be proved using the algebra, such
as commutativity (i.e., when two models are merged then the result does not depend on
which model came ﬁrst) and non-associativity (i.e., if three or more models are merged
then the order in which the combination is performed aﬀects the result).
• We develop two protocols that are eﬃcient in terms of storage space and that can tune
the dependencies of the impact of a model on the time at which it is used.
Organization of the Paper. In Section 2, we introduce our framework and provide the
technical foundations of classiﬁers. This framework is analyzed in terms of merging in Section 3.
Section 4 is devoted to the impact of time on merging, that is, how the order in which models
are combined aﬀects the ﬁnal result. Finally, we discuss the limitations of our approach.
1546

An algebraic approach to combining classiﬁers

2

Giabbanelli and Peters

A Framework: Decision Spaces

Intuitively, a decision space is an m-dimensional space, in which each dimension corresponds
to one of the m attributes. It contains a set of non-overlapping elements which, if they cover
all of the space, form a partition of the space. A geometrical interpretation of an element is
a subspace deﬁned by an m-polytope. Thus, the only requirement for the classiﬁers considered
here is that their elements have to be polytopes, which is the case for Support Vector Machines
(SVMs) with linear kernels, decision trees, and rule sets. Our study of polytopes is a ﬁrst
step to investigating the theoretical behaviour of pure meta-learning. Further research could
remove the restriction to polytopes to generalize our approach to handle other classiﬁers such as
SVMs with non-linear kernels [1], which are not considered by the current approach. Informally,
non-linear kernels can deﬁne shapes in terms of curves whereas polytopes can only use lines.
Each element, or polytope, has a class distribution vector that speciﬁes the percentage of
instances within the covered space that are assigned to each class. Determining the percentages
by enumerating the instances is straightforward for the three classiﬁers considered here. Any
classiﬁer is capable of producing a label for a given instance, and can be repeatedly prompted
for labels over a discrete space. Therefore, any classiﬁer can provide a class distribution vector.
In Xu’s categorization of the outputs used to combine classiﬁers, our requirement is the most
universal (Type 1) [13]. These concepts are formalized in Deﬁnitions 1 and 2.
Deﬁnition 1. A decision space is an m-dimensional space Dattr1 × · · · × Dattri × · · · × Dattrm
where m is the number of attributes and Dattri is the space covered by the i-th attribute
speciﬁed as a bounded poset (i.e., partially ordered set with a least and a greatest element).
Deﬁnition 2. An element of an m-dimensional decision space D is a subspace of D, i.e., an
m-dimensional polytope (m-polytope). It is identiﬁed by a set of coordinates for each attribute,
and has a class distribution vector V with c components, where c is the number of classes.
The i-th component of the vector is the percentage of instances in class i, which is obtained by
counting all instances that have class i and are within the element’s space. Thus, the content
of the vector sums to 100%. We will refer to the vector V as the value of the element.
The constraints on the structure of a classiﬁer are used by a data mining algorithm to guide
the search. A decision space is a framework and does not result directly from a data mining
algorithm, thus its structure is less constrained than classiﬁers such as decision trees. This
ensures that elements from several kinds of classiﬁers can be converted to elements of a decision
space with no loss of information. To convert a classiﬁer into a decision space, the only data
required besides the classiﬁer itself are the ranges of attributes for the dataset on which the
classiﬁer was trained. These ranges can be deduced in one pass over the dataset by scanning for
the maximum and minimum values. The ranges can also be user-supplied, but should not be
smaller than what is found in the dataset for consistency sake. Thus, given a classiﬁer and the
ranges of the attributes, the main task of the conversion is to extract the individual elements,
or polytopes. The polytopes of three popular classiﬁers, in order of increasing constraints on
the shapes, can be converted as follows:
(1) The elements in a SVM can have the most general shapes as data can be assigned into
regions. Each element of the decision space corresponds to exactly 1 element of the SVM
with the same distribution vector and the same coordinates specifying the space spanned.
(2) A rule in a rule set deﬁnes an axis-parallel rectangle. When lower or upper bounds are
not speciﬁed, the attribute’s range is used instead.
(3) Paths of a decision tree can be converted to rules deﬁning axis-parallel rectangles, and
are then processed as in (2). Note that a decision tree cannot generate all axis-parallel
rectangles. Indeed, a decision tree belongs to the data mining family of divide-and-conquer
1547

An algebraic approach to combining classiﬁers

Giabbanelli and Peters

algorithms that imposes constraints on the search. Intuitively, a cut in the space along
the border of an element, either vertical or horizontal, should not cut any element [3].
Figure 1(b) shows a rule set violating this constraint.

3
3.1

Merge Operator
Preliminary Deﬁnitions

The most fundamental operation on decision spaces is merge. In the following, we use the term
“element” to refer to a geometrical space spanned, and “value” to refer to a class distribution
vector. Given two decision spaces X and Y , we merge them into Z using the following principles:
(1) If an element (subspace) x ∈ X does not intersect with any element y ∈ Y , then the
prediction represented by x has no conﬂicts and can be added to Z. An element y ∈ Y
with no conﬂicts is treated similarly.
(2) If an element y ∈ Y is strictly contained within an element x ∈ X with the same value
(i.e., the same class distribution vector V ), then we consider y to be too specialized
(as explained in Deﬁnition 5) and delete it.
(3) If neither of the ﬁrst two conditions is satisﬁed, then the element x ∈ X intersects with
at least one element of Y and conﬂicts must be resolved.
Prior to establishing the algorithm to merge elements, we introduce the formal notation on
which it relies, and specify the ways that elements can intersect.
• An element x ∈ X has a set of attributes A(x).
• Each attribute a ∈ A(x) covers a (one-dimensional) space S(x, a). It can be an interval
such as [8, 10], or a union of intervals.
• The size of the space covered by x for the attribute a is denoted |S(x, a)|.
• x has a class distribution vector V (x).
Deﬁnition 3. An element x ∈ X subsumes an element y ∈ Y , denoted y x, if A(x) = A(y)
and ∀a ∈ A(x), S(y, a) ⊆ S(x, a).
In other words, an element y is subsumed by an element x when the space covered by y
is included in the space covered by x. We denote strict subsumption by ≺ when ∀a ∈ A(x),
S(y, a) ⊂ S(x, a). The main property of subsumption is established by Theorem 1.
Theorem 1. Let X and Y be two decision spaces. For each y ∈ Y , there is at most one x ∈ X
such that y x.
Proof. The elements of X and Y partition the spaces formed by X and Y, respectively. Thus,
an element (component of a partition) can be fully included in at most one other element.
Deﬁnition 4. The intersection of an element x ∈ X with a decision space Y is denoted
x Y and is the largest subset of elements I = {y1 , . . . , yn } ⊆ Y such that for each yi ∈ I,
∃a ∈ (A(x)∩A(yi )) such that S(x, a)∩S(yi , a) = ∅. Subsumption is a special case of intersection.
The ﬁrst two principles of merging can be handled by the notions in Deﬁnitions 3 and 4.
For the third principle, we resolve each conﬂict between two elements x ∈ X and y ∈ Y by
creating a new element z for each intersection. A simple approach would be to assign to z the
average value of x and y, but it would not account for the spaces covered by x and y (i.e., their
regions of competence). Previous research in combining classiﬁers has also used their local
competence to assign them weights during the combination [14]. Consequently, Deﬁnition 5
speciﬁes how to measure the space covered by an element, which we call its specialization
(also known as competence). Then, Deﬁnition 6 speciﬁes how the value of z is calculated as a
weighted average based on the spaces covered by x and y. Note that Deﬁnition 6 is a classical
1548

An algebraic approach to combining classiﬁers

Giabbanelli and Peters

combination scheme intermediate between classiﬁer fusion and selection (cf. Introduction). In
these schemes, all classiﬁers contribute to the outcome of a given space space with weights
based on their competence for that space [8].
Deﬁnition 5. The specialization of an element x ∈ X is M (x) =

a∈A(X) |S(x,a)|

|A(X)|

.

Intuitively, we sum the sizes of the spaces of the attributes characterizing x and normalize
by the number of attributes. Small values of M indicate specialized predictions based on small
spaces. Other possible metrics could take into account the number of instances, or the class
distribution vector. However, in the case of pure meta-learning investigated in this paper, we
cannot use the number of instances or the class distribution vector in a speciﬁc part of an
element. This could only be done by requiring the original classiﬁer to perform an exhaustive
search in its dataset, and repeated use of such an operation could lead to signiﬁcant overhead.
Therefore, such alternative metrics are not considered here.
Deﬁnition 6. The value of the intersection of two elements x ∈ X and y ∈ Y based on their
M (y)
M (x)
specializations is the class distribution vector V (x⊗y) = V (x)× M (x)+M
(y) +V (y)× M (x)+M (y) .
If the elements predict several attributes, then the class distribution vector for each attribute
is computed using the above formula. Other formulae for V (x ⊗ y) could be designed to suit
application-speciﬁc needs. Indeed, both the element’s space and its value must be taken into
account, but speciﬁc applications may require that the specialization be normalized diﬀerently.
However, care should be taken to ensure that custom formulae satisfy the commutativity, idempotency, and unique identity properties. Indeed, such properties are critical for proving that
the merge algorithm behaves appropriately, as will be shown in Section 3.3.

3.2

Merging Algorithm

The merge operator ⊗ : (X, Y ) → Z is deﬁned in an algorithmic way by Algorithm 1 and is
illustrated in Figure 1(a). First, we apply Merge principle (1): all elements with no intersection
are added. Then, we apply principle (2): each element x ∈ X that is strictly contained in an
element y ∈ Y with the same value is deleted. Finally, principle (3) is applied as follows:
• We consider all possible intersections x Y and handle them sequentially.
• For each y ∈ (x Y ), we create an element z. The space covered by z is the intersection
of the spaces covered by x and y, and the value is V (x ⊗ y) (Deﬁnition 6).
• There will be a “remainder” when the process is ﬁnished if x only has a partial intersection
with Y . Thus, we remove from x the space of each z resulting from the intersection, and
we add the remainder, if any, to the result.
After applying the merge principles for each element x ∈ X, we process the elements y ∈ Y .
Merge principles (1) and (2) are applied to these elements, but principle (3) can be partially
avoided. Indeed, the elements in the intersection of X and Y have already been computed
when processing X. We use a hash map H as a cache, in which the element y is used as a key
and its corresponding value y in the hash map is the space that is updated throughout the
process. When an intersection between x and y is found, the part in the intersection is virtually
removed from y by changing the value for y in H. After all intersections have been computed,
H contains the remainder of each element of Y and thus it can be added directly to the result.
The intersection of rectangles in m-dimensions (for m attributes) can result in spaces that are
not rectangles. For example, in Figure 1(c), decision space X contains four rectangular elements
and Y contains one rectangular element, but the intersection of the element of Y with any
element of X leaves a non-rectangular remainder of Y . Constraining elements to be rectangles
1549

An algebraic approach to combining classiﬁers

Giabbanelli and Peters

Algorithm 1
: (X, Y ) → Z
1: Z ← new decision space
2: A ← ∅ // hashmap: set (y, y’) of keys y and associated values y’
3: for x ∈ X do
4:
if not (x ≺ y ∈ Y and V (x) = V (y)) then
5:
if x Y = ∅ then
6:
Z ← Z ∪ x //x does not conﬂict with any element of Y
7:
else
8:
tmp ← x.space //Conﬂicts: save the initial space of x
9:
for y ∈ x Y do
10:
z ← new element //Create an element for each conﬂict
11:
z.space ← tmp ∩ y.space //z’s space is the one initially shared by x and
the conﬂicting element
12:
z.value ← V (x ⊗ y) //z’s value is based on both x’s and y’s
13:
Z ← Z ∪ z //z is added to the resulting decision space
14:
if (y, y ) ∈ A then
15:
A ← A ∪ (y, y.space \ z.space) //Never registered a conﬂict
16:
else
17:
A ← A \ (y, y ) //Otherwise update the previous record
18:
A ← A ∪ (y, y .space \ z.space)
19:
x.space ← x.space \ z.space //Some of x’s conﬂicts was handled
20:
if x.space = ∅ then Z ← Z ∪ x
21:
//If x has (non conﬂicting) space after solving the conﬂicts then we keep it
22: for y ∈ Y such that (y, y ) ∈ A do
23:
if not (y ≺ x ∈ X and V (y) = V (x)) then
24:
Z ← Z ∪ y //Elements not in a conﬂict and not subsumed are added
25: for (y, y ) ∈ A do
26:
if y = ∅ then Z ← Z ∪ y //add parts of an element free of conﬂict to the result
27: return Z

requires a heuristic that can bias the result since rectangles are only an approximation of the
element’s actual specialization. We avoid this problem by using polytopes instead of rectangles
to provide an exact algebra. Constraining elements to be simple polytopes (e.g., rectangles) for
computational reasons would be an interesting avenue for future work.

3.3

Algebraic Properties

The goal of a merge operator ⊗ is to merge the information of two decision spaces, resolving any
conﬂicts that arise. Thus, it has to obey a set of algebraic properties in order to be consistent:
• Commutativity: merging a decision space with another one should not depend on which
one is ﬁrst but only on the information.
• Identity element: merging a decision space with a decision space that does not contain
any elements should not change anything because there is no new information.
• Idempotence: merging a decision space with itself should not change anything, as there
are neither conﬂicts nor new information.
Theorems 2, 3, and 4 prove that our merge operator satisﬁes these three algebraic properties.
Intuitively, the algorithmic construction of the merge operator is based on unions of geometric
spaces and the resolution of conﬂicts encountered for non-empty intersections. Both the union
1550

An algebraic approach to combining classiﬁers

Giabbanelli and Peters

Figure 1: (a) Merging two decision trees by converting them into decision spaces and creating a union
decision space. (b) A partition of space not allowed by decision trees but allowed by decision spaces.
(c) Intersection of decision spaces X (four rectangular elements) and Y (one rectangular element). (d)
Illustration of Theorem 5. Merging schemes: (e) biased binary scheme, (f) unbiased binary scheme,
and (g) merging scheme with n = 3 × 2 × 2 decision spaces from Theorem 6.

of spaces (i.e., the intersection of geometrical elements) and the resolution of conﬂicts (i.e., the
weighted average of values) satisfy the three algebraic properties. We then show in Theorem 5
that the operator is not associative. We only provide proof outlines in this section and the next
section because of space constraints. The full proofs are available on arXiv [4].
Deﬁnition 7. The set of all decision spaces is denoted by D.
Theorem 2. The ⊗ operator is commutative, i.e., ∀X, Y ∈ D, X ⊗ Y = Y ⊗ X.
Proof. Suppose that there is a z ∈ (X ⊗ Y ). The result can be proved by showing that z also
has to be in (Y ⊗ X). The proof considers all possible cases from which such a z could result:
(1) z results from an x ∈ X that has no intersection. We show that if a y ∈ Y has no intersection
it will also be kept in the result Z.
(2) z results from the intersection of an x ∈ X with a y ∈ Y . We will show that there are no
changes if we consider it as the intersection of a y ∈ Y with an x ∈ X.
(3) z is the remainder of an element x ∈ X. We show that the remainder of an element y ∈ Y will
also be kept in the result Z.
Deﬁnition 8. A decision space E ∈ D is called an identity of D with respect to the ⊗ operator
if and only if ∀D ∈ D, E ⊗ D = D ⊗ E = D.
Theorem 3. There exists a unique identity E ∈ D with respect to the ⊗ operator, and it is an
empty set of polytopes (i.e., the empty space).
Proof. Let X and E be two decision spaces and E = ∅. We ﬁrst prove that ∀X ∈ D, X ⊗E = X,
which leads to E ⊗ X = X using Theorem 2, hence E is an identity element. We complete the
proof by showing that Y ∈ D cannot be an identity for any X ∈ D if Y = E.
Theorem 4. The ⊗ operator is idempotent as X ⊗ X = X.
1551

An algebraic approach to combining classiﬁers

Giabbanelli and Peters

Proof. Let X and Y be two decision spaces such that X = Y . The proof consists of establishing
three facts: (i ) there is no x ∈ X strictly contained in any y ∈ Y , (ii ) each element x intersects
exactly one y, and (iii ) the resulting decision space contains each x ∈ X exactly once.
Theorem 5. The ⊗ operator is not associative: ∃X, Y, Z ∈ D such that (X ⊗ Y ) ⊗ Z =
X ⊗ (Y ⊗ Z).
Proof. In Figure 1(d), the value of the intersection of x and y is diﬀerent when we ﬁrst merge x
with y and then merge with z than when x is merged with the result of merging y and z.

4

The Impact of Merge Order

According to Theorem 5, the ⊗ operator is not associative, so the result of merging more than
two decision spaces depends on the order in which they are merged.
Deﬁnition 9. A merging scheme speciﬁes the order in which a set of decision spaces is merged.
It can be represented as a tree, in which a leaf represents a decision space, an intermediate node
represents the application of the ⊗ operator (hence an intermediate decision space), and the
root represents the ﬁnal result.
Figures 1(e) and 1(f) show two merging schemes. In Figure 1(f), decision spaces are merged
pairwise (bottom layer of the tree), the resulting decision spaces are merged pairwise, and so on,
until a single decision space W is obtained. We can describe this scheme as (((X1 ⊗ X2 ) ⊗ (X3 ⊗
X4 )) · · · ((Xm−3 ⊗ Xm−2 ) ⊗ (Xm−1 ⊗ Xm ))), where m = 2k . This scheme does not introduce
bias into the calculation of the value of W (Corollary 7). Any diﬀerences in the impacts of the
decision spaces on the value of W are based on their relative specializations. In Figure 1(e), the
ﬁrst two decision spaces are merged, then the result is merged with the third decision space, and
so on, until W is obtained. We can describe this scheme as (((X1 ⊗ X2 ) ⊗ X3 ) · · · ⊗ Xm ). This
scheme leads to a bias in the result: each decision space is merged with the result of merging
its predecessors, so its impact is the same as all of its predecessors combined. Thus, the impact
of earlier decision spaces is reduced each time that a new decision space arrives.
Both merging schemes can be desirable depending on the setting. The unbiased scheme can
be used for homogeneous distributions, e.g. databases partitioned among several computation
units where each unit’s model should have the same impact on the result. The biased scheme
can be used for time-changing distributions, e.g. for data streams as learners deduce models at
diﬀerent times, and we may want to favour models representing recent trends. Note that we
could also have an online setting (e.g., data streams), but we might not want the time at which
models are received to have an impact. In this case, the unbiased scheme can be used but some
decision spaces must be stored temporarily: large groupings of decision spaces must be formed
before the merging operation can take place to guarantee an equal impact on the ﬁnal result.
However, the framework developed so far has limitations that can prevent unbiased results.
Indeed, the merging operator is binary and thus an unbiased result can only be achieved if the
number of decision spaces to merge is a power of 2. Furthermore, using powers of 2, the impact
of decision spaces over time can only decay exponentially, whereas other types of decays may be
needed. We will generalize the merge operator to address these needs in the next subsections.

4.1

m-ary Merging

Consider a set {X1 , . . . , Xm } of decision spaces. We extend Algorithm 1 from a binary operator
to an m-ary operator. Increasing the number of geometrical objects that are intersected does
not aﬀect the operator, since the intersection of geometrical objects is associative. Thus, the
extension only involves the computation of the value, which is extended in Deﬁnition 10.
1552

An algebraic approach to combining classiﬁers

Giabbanelli and Peters

Deﬁnition 10. The value of the intersection of m elements x1 ∈ X1 , x2 ∈ X2 , . . . , xm ∈ Xm
based on their specializations is a vector
M (x1 )
M (xm )
V (x1 ⊗ · · · ⊗ xm ) = V (x1 ) × M (x1 )+···+M
(xm ) + · · · + V (xm ) × M (x1 )+···+M (xm ) .
The combination of the handling of geometrical spaces and the computation of the value is
similar to the merging algorithm introduced in Section 3.2:
(1) If an element is strictly included within another one and has the same value, discard it.
(2) Otherwise, for each intersection, create a new element z whose value is computed using
the formula in Deﬁnition 10.
We assume in Theorem 6 and its corollaries that the decision spaces to be merged all cover
the same space so that we can concentrate on the impact of merge order.
Theorem 6. In a merging scheme for m decision spaces X1 , . . . , Xm , all of which cover the
same space, the impact of a decision space Xi on the value of the ﬁnal result is proportional to
the product of the numbers of operands of the ⊗ operators on the path from the leaf representing
Xi to the root of the merging scheme.
Proof. If u1 and u2 are internal nodes of a merging scheme with k1 and k2 operands, respectively,
and suppose that u2 is an operand of u1 . Then each operand of u2 accounts for a fraction
1/(k1 × k2 ) of the value of u1 . The same fraction results if the numbers of operands of u1 and
u2 are exchanged.
Corollary 7. In the merging scheme (((X1 ⊗ X2 ) ⊗ (X3 ⊗ X4 )) · · · ((Xn−3 ⊗ Xn−2 ) ⊗ (Xn−1 ⊗
Xm ))), where m = 2k , each decision space accounts for 2−k of the value of W .
Corollary 8. In the merging scheme (((X1 ⊗ X2 ) ⊗ X3 ) · · · ⊗ Xm ), decision space Xi accounts
for 2−m+i−1 of the value of W if i > 1, and 2−m+1 if i = 1.
Figure 1(g) is a merging scheme for twelve decision spaces in which the impacts of all of the
decision spaces on the value of W are the same according to Theorem 6. Figures 1(f) and 1(e)
show applications of Corollaries 7 and 8, respectively.

4.2

Adjusting the Bias

When decision spaces are merged over time, their impacts decay exponentially (Corollary 8).
In Section 4.1, we used extra storage to change the impacts. We now introduce an approach
that uses no extra storage and provides decision spaces with the same impact. This approach
does not require prior knowledge of the number m of decision spaces to be merged, compared
to the approach in Section 4.1 which needed m to design a merging scheme.
Deﬁnition 11. The β-weighted value of the intersection of two elements x ∈ X and y ∈ Y is
the class distribution vector V (x ⊗ y) = V (x) × β + V (y) × (1 − β), 0 < β < 1.
(x1 )
The value produced using Deﬁnition 6 is a β-weighted value with β = M (xM
and
1 )+M (x2 )
V (x⊗y) depends only on the specializations of x and y. If x and y have the same specialization,
then each accounts for half of the value of the result. The bias in a merging scheme like the one
(x1 )
in Figure 1(e) is entirely the result of the merge order when β = M (xM
is used. We can
1 )+M (x2 )
use diﬀerent values of β with this scheme to produce the ﬁnal value V (((x1 ⊗x2 )⊗x3 ) · · ·⊗xm ) =
M (x1 )
M (xm )
V (x1 ) × M (x1 )+···+M
(xm ) + · · · + V (xm ) × M (x1 )+···+M (xm ) , the same as an m-ary merge.
1 )+···+M (xi )
Theorem 9. Let βi = MM(x(x1 )+···+M
(xi+1 ) be the weight used to compute the value of the i-th
merge, i = 1, 2, . . . , m − 1, in the merging scheme (((X1 ⊗ X2 ) ⊗ X3 ) · · · ⊗ Xm ). Then the
resulting value of the intersection of m elements x1 ∈ X1 , x2 ∈ X2 , . . . , xm ∈ Xm based on their

1553

An algebraic approach to combining classiﬁers

specializations is:
V (((x1 ⊗ x2 ) ⊗ x3 ) · · · ⊗ xm ) = V (x1 ) ×

M (x1 )
M (x1 )+···+M (xm )

Giabbanelli and Peters

+ · · · + V (xm ) ×

M (xm )
M (x1 )+···+M (xm ) .

Proof. The proof is a straightforward induction on the number of merges.

5

Conclusions

Information systems have to collaborate when each of them observes part of a process, in space
or time. Due to technical or ethical limitations, individual observations may not be communicated and summaries such as classiﬁers are instead provided. Our current understanding of
classiﬁer combination mechanisms is limited, and this paper contributes to showing how the
combinations can be analyzed in an algebraic approach. Our methods are designed for a pure
meta-learning framework in which classiﬁers are merged instead of being stored separately, and
observations are never exchanged. We use algebraic methods to prove several desirable properties of the merge operator. We also show that the operator is not associative: the result of
merging several classiﬁers depends on the order that the merge operator is applied. This can
be the desired behaviour for a rapidly evolving situation in which the last information system
to communicate is deemed most representative, and the result is biased to favour recent trends.
However, other types of biases may be needed. For example, several information systems that
are monitoring a situation may observe it at the same time and communicate at diﬀerent times,
but they should still be given equal weight. We introduced two approaches to achieve diﬀerent
biases. The ﬁrst one uses storage space ﬂexibly to customize the bias. The second approach
results in no bias, thereby making the operator associative, and uses no space. Future work can
(i) use our algebraic framework and approach to establish the behaviours of other operators, (ii)
compare the framework’s accuracy and complexity with existing methods for combining heterogenous classiﬁers as reviewed in [9], and (iii) analyze how simpliﬁed shapes (e.g., rectangles)
can reduce the space and time complexity of our approach while impacting its accuracy.

References
[1] B. E. Boser, I. M. Guyon, & V. N. Vapnik. A training algorithm for optimal margin classiﬁers. In
D. Haussler, editor, 5th Annual ACM Workshop on COLT, pages 144–152, 1992.
[2] P. Chan & S. J. Stolfo. Toward parallel and distributed learning by meta-learning - working notes. In
AAAI Workshop on Knowledge Discovery in Databases, pages 227–240. AAAI, 1993.
[3] J. F¨
urnkranz. Separate-and-conquer rule learning. Artif Intell Rev, 13(1):3–54, 1999.
[4] P. Giabbanelli & J. Peters. An Algebra to Merge Heterogeneous Classiﬁers, arXiv 1501.05141v2.
[5] C. Giraud-Carrier, R. Vilalta, & P. Brazdil. Introduction to the special issue on meta-learning. Machine
Learning, 54:187–193, 2004.
[6] M. Hefeeda & M. Bagheri. Forest ﬁre modeling and early detection using wireless sensor networks. Ad hoc
and Sensor Wireless Networks, 7:169–224, 2009.
[7] T. K. Ho. Multiple classiﬁer combination: Lessons and the next steps, pages 171–198. World Scientiﬁc
Publishing, 2002.
[8] L. I. Kuncheva. Combining pattern classiﬁers: methods and algorithms. Wiley & Sons, 2004.
[9] M. Wocniak, M. Grana, & E. Corchado. A survey of multiple classiﬁer systems as hybrid systems. Information Fusion, 16:3–17, 2014.
[10] C. Sansone, J. Kittler, & F. Roli. Proc. 10th international workshop on multiple classiﬁer systems. Lecture
Notes in Computer Science, 6713, 2011.
[11] K. M. Ting & B. T. Low. Model combination in the multiple-data-batches scenario. ECML ’97 Proc. of
the 9th European Conference on Machine Learning, pages 250–265, 1997.
[12] R. Vilalta & Y. Drissi. A perspective view and survey of metalearning. Artif Intell Rev, 18:77–95, 2002.
[13] L. Xu, A. Krzyzak, & C. Y. Suen. Methods of combining multiple classiﬁers and their application to
handwriting recognition. IEEE Trans. on Systems, Man, and Cybernetics, 22:418–435, 1992.
[14] P.P.K. Chan, D.S. Yeung, et al. Dynamic fusion method using Localized Generalization Error Model
Information Sciences, 217:1–20, 2012.

1554

