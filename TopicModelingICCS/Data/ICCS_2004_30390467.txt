A Continuous Approach for the Computation of
the Hyperbolic Singular Value Decomposition
T. Politi
Dipartimento di Matematica, Politecnico di Bari,
Via Amendola 126/B, I-70126 Bari (Italy).
politi@poliba.it

Abstract. In this paper a continuous approach based on the Projected
Gradient Flow technique is presented in order to ﬁnd a generalization of
the Singular Value Decomposition (SVD) of a rectangular matrix called
Hyperbolic SVD. If A is a m × n real matrix with full column rank
and if G is a n × n diagonal sign matrix, i.e. gii = ±1, the Hyperbolic
Singular Value Decomposition of the pair (A, G) is deﬁned as A = U ΣV ,
where U is orthogonal, Σ is diagonal with positive entries and V is
hypernormal (or G-orthogonal), i.e. V T GV = G. In this work we use a
continuous approach based on the projected gradient technique obtaining
two diﬀerential systems, the ﬁrst one evolving on group of orthogonal
matrices and the second on the quadratic group related to G. A numerical
test is reported in order to show the eﬀectiveness of the approach.

1

Introduction

Recently there has been a growing interest in numerical analysis community in
the exploitation of the concept of structure associated to matrices. Examples of
these structures are orthogonality, simplecticity, skew-symmetry and so on. Some
of these properties are related to group structures (and sometimes to Lie-group
structures) or to algebra structure (for example the skew-symmetry) or, more
simplicity to algebraic properties (it is the case of the obliqueness, i.e. matrices
Y such that diag(Y T Y ) = I). In particular a great attention has been devoted
to the numerical solution of diﬀerential equations evolving on matrix groups (see
[7] for quadratic groups, [6] for Lie groups, [3,4] for orthogonal matrices). The
experience in the eﬀective solution of diﬀerential equations on matrix groups
can be used also to solve some linear algebra problems, such as the computation of factorizations of time depending matrices (see [13]) or the computation
of singular value decomposition factors as limit point of continuous ﬂows (see
[2]). In this paper we consider this last problem in the case of the Hyperbolic
Singular Value Decomposition. The work is organized as follows: in Section 2 we
deﬁne the Hyperbolic Singular Value Decomposition and recall some important
features and applications, in Section 3 we use the projected gradient technique
in order to obtain two diﬀerential ﬂows having, respectively, the unitary and the
hypernormal factors as limit point. Finally in the Section 4 a numerical test is
described in order to show the eﬀectiveness of the diﬀerential approach.
M. Bubak et al. (Eds.): ICCS 2004, LNCS 3039, pp. 467–474, 2004.
c Springer-Verlag Berlin Heidelberg 2004

468

2

T. Politi

The Hyperbolic Singular Value Decomposition

In this section we deﬁne the Hyperbolic SVD of a m × n real matrix A with a
couple of its applications, but ﬁrst we give some important deﬁnitions.
Deﬁnition 1. Let G be a m×m diagonal matrix with entries ±1, then a matrix
U ∈ IRm×m is said hypernormal if U T GU = G.
In [5] the hypernormal matrices are called G−orthogonal.
Deﬁnition 2. If G is a m × m diagonal matrix with entries ±1, then a matrix
U ∈ IRm×m is said hyperexchange if
U T GU = G

(1)

where G is another real diagonal matrix with entries ±1.
It is easy to observe that there is a strict relation between hypernormal and
hyperexchange matrices. In fact if V is an hyperexchange matrix there exists
a permutation matrix P such that W = V P is hypernormal. In fact, from (1)
since G and G have the same inertia (i.e. the same number of +1 and −1) and
there exists a permutation matrix P such that P GP T = G, hence
V T GV = G = P GP T

⇒

(V P )T G(V P ) = G

and W = V P is hypernormal. Considering the quadratic group related to matrix
G, i.e. the set
HG = {Y ∈ IRn×n | det(Y ) = 0, Y GY T = G}
we observe that it coincides with the set of hypernormal matrices. Moreover we
shall denote by O(n) as the set of orthogonal matrices of order n. The following
result states the existence of the Hyperbolic SVD (see [10]).
Theorem 1. Let A ∈ IRm×n and G be a square diagonal real matrix of order n
with entries equal to ±1, if the rank of AGAT is equal to min{m, n} then there
exist a m × m unitary matrix U , an hypernormal n × n matrix V and a m × n
diagonal matrix Σ with positive entries such that
A = U ΣV.

(2)

We observe that in [1,8] the matrix V is requested to be an hyperexchange matrix
but we have already shown that the two sets of matrices are strictly related. The
Hyperbolic SVD has some interesting applications: for example in [12] it has been
used to solve the symmetric indeﬁnite eigenvalues problem Ax = λx, where A
is a square matrix. The algorithm proposed consists of two steps:
• A is decomposed by symmetric indeﬁnite factorization A = LGLT (see [9]),
being G a diagonal matrix with entries equal to ±1;

A Continuous Approach

469

• the second step is the computation of the Hyperbolic SVD of L, i.e. L = U ΣV.
Since
A = LGLT = U ΣV GV T ΣU T = U GΣ 2 U T ,
the eigenvalues of A are λi = gii σi2 , while the columns of U are the corresponding
eigenvectors. If G =diag(Ik , −Im−k ) and we divide the matrix A in block form
A = A1 A2 , with A1 ∈ IRm×k and A2 ∈ IRm×(m−k) , then the Hyperbolic
SVD could be used to ﬁnd the eigenvalues of the matrix
H = AGAT = A1 AT1 − A2 AT2 = U GΣ 2 U T
without forming explicitly the matrix.

3

A Gradient Flow Approach

The aim of this section is to use the projected gradient in order to construct
a continuous steepest descent ﬂow on the manifold O(m) × HG . To apply this
approach it is necessary to transform the problem into a minimization one with
manifold constraints. From (2) it is
Σ = U −1 AV −1 = U T AGV T G.
Since Σ must be diagonal it is natural to minimize the distance between the
matrix U T AGV T G and the manifold of real diagonal matrices and to choose
as projection the operator giving the main diagonal of the matrix. Deﬁning the
function
2
(3)
F (U, V ) = U T AGV T G − diag(U T AGV T G) F
where U ∈ O(m) and V ∈ HG and, following the same approach as in [2] for the
usual SVD, we have to solve the constrained problem:
Minimize

F (U, V ) = U T AGV T G − P (A), U T AGV T G − P (A)

subject to U T U = In ,

V T GV = G

where P (A) =diag(U T AGV T G), and A, B denotes the Frobenius inner product of two matrices A, B ∈ IRm×n deﬁned as
A, B = trace AB T =

aij bij .

(4)

i,j

As seen in the previous section the set of hypernormal matrices is the quadratic
group related to matrix G, that is a particular Lie group and having as Lie
algebra the set
hG = {A ∈ IRn×n | AG + GAT = 0}.
We observe that if G is the identity matrix of order n then hG is the set of real
skew-symmetric matrices. A property of the Lie algebra is that it is the tangent

470

T. Politi

space of HG at any hypernormal matrix Q is deﬁned as TQ HG = QhG . If K ∈ hG
and S is a G−self adjoint matrix (i.e. S = GS T G) it is
trace(SK) = 0
then S is orthogonal to any K with respect to metric (4). Then the normal space
NQ HG of HG at any Q, hypernormal matrix, can be expressed as NQ HG = QSG ,
where SG is the set of G−self adjoint matrices. Function F (U, V ) is deﬁned on
the Cartesian product O(m) × HG . Taking H ∈ IRm×m and K ∈ IRn×n , the
Fr´echet derivative of F acting on (H, K) ∈ IRm×m × IRn×n can be considered as
F (U, V ).(H, K) =

∂F
∂F
.K,
.H +
∂U
∂V

(5)

where Λ.η denotes the result of the action by the linear operator Λ on η. We
now calculate each action in (5) as follows. First
∂F
.K = U T AGK T G − diag(U T AGK T G), U T AGV T G − diag(U T AGV T G) =
∂V
= U T AGK T G, U T AGV T G − diag(U T AGV T G) +
− diag(U T AGK T G), U T AGV T G − diag(U T AGV T G) =
= U T AGK T G, U T AGV T G − diag(U T AGV T G) =
= U T AGK T , U T AGV T − diag(U T AGV T ) =
= V GAT U − diag(V GAT U ), KGAT U =
= (V GAT U − diag(V GAT U ))U T AG, K .

It follows, from the Riesz representation theorem, that the partial gradient with
respect to the Frobenius inner product can be represented as
∂F
= (V GAT U − diag(V GAT U ))U T AG.
∂V
Then
∂F
.H = H T AGV T G − diag(H T AGV T G), U T AGV T G − diag(U T AGV T G) =
∂U
= H T AGV T G, U T AGV T G − diag(U T AGV T G) =
= H T AGV T , U T AGV T − diag(U T AGV T ) =
= H T , (U T AGV T − diag(U T AGV T ))V GAT =
= AGV T (V GAT U − diag(V GAT U )), H ,

and

∂F
= AGV T (V GAT U − diag(V GAT U )).
∂U

A Continuous Approach

471

The gradient ∇F (U, V ) now must be projected in the Cartesian product of the
tangent spaces of the two manifolds, i.e. TQ O(m) × TQ HG . The tangent space
TQ O(m) is Qh, where h is the set of real skew-symmetric matrices of order m.
It is well known that any general matrix X ∈ IRn×n can be uniquely splitted as
X=Q

QT X + X T Q
QT X − X T Q
,
+Q
2
2

then the projection PO(m) (X) onto the tangent space TQ O(m) is given by
PO(m) (X) = Q

QT X − X T Q
.
2

Similarly it is easy to verify that any matrix X ∈ IRn×n has a unique splitting
X=Q

1
1
(GQT GX − GX T GQ) + (GQT GX + GX T GQ)
2
2

where Q ∈ HG , GQT GX − GX T GQ ∈ hG and GQT GX + GX T GQ ∈ SG . The
projection of the gradient of F (U, V ) into the tangent space TQ HG is
PHG (X) = Q

GQT GX − GX T GQ
.
2

Hence the diﬀerential systems that must be solved are
∂F
dU
= −PO(m)
dt
∂U
dV
∂F
.
= −PHG
dt
∂V

4

(6)
(7)

Numerical Tests

In this section we show a numerical example obtained applying the projected
gradient ﬂow approach described previously. We consider a 5 × 3 real random
matrix A having as main diagonal of Σ the vector (0.5, 1, 1.5), and taking
G =diag(I2 , −1). The diﬀerential systems (6)-(7) have been solved numerically
in the interval [0, 30]. In Figure 1 we show the behaviour of the objective function
(3), while in Figure 2 we show the behaviour of the diagonal entries of the matrix Xn = UnT AGVnT G. Matrices Un and Vn are the numerical approximations
of the solutions U (t) and V (t), computed at t = tn , obtained ﬁrst integrating
the diﬀerential systems (6)-(7) with the MatLab ode routine ode113 and then
projecting the numerical solutions on the manifolds. For the orthogonal ﬂow
(6) the projection has been computed taking the orthogonal factor of the QR
decomposition (see [3] for more details), while for the ﬂow (7) the hypernormal
factor of hyperbolic QR decomposition has been taken (see [11]). In [3] has been

472

T. Politi

4

3.5

3

2.5

2

1.5

1

0.5

0

0

5

10

15
Time

20

25

30

Fig. 1. Evolution of the objective function.

1.8

1.6

1.4

1.2

1

0.8

0.6

0.4

0.2

0

0

5

10

15
Time

20

Fig. 2. Diagonal elements of matrices Xn .

25

30

A Continuous Approach

−15

1.4

x 10

|| UTn Un−I ||F

1.2
1

0.8
0.6
0.4
0.2
0

0

5

10

15
Time

20

25

30

5

10

15
Time

20

25

30

−14

1.4

x 10

|| VTn GVn−G ||F

1.2
1

0.8
0.6
0.4
0.2
0

0

Fig. 3. Errors in the orthogonal and in the quadratic group related to G.

−5

10

F
n n

|| UTU −I ||

−10

10

−15

10

−20

10

0

5

10

15
Time

20

25

30

0

5

10

15
Time

20

25

30

−5

−10

10

n

n

|| VTGV −G ||

F

10

−15

10

−20

10

Fig. 4.

473

474

T. Politi

proved that the order of the ODE integrator is preserved. The initial conditions
for (6) and (7) are random matrices in the manifolds. We observe that the behaviour of the diagonal elements of Xn approaches the theoretical values already
when t
8. In Figure 3 we show the departure from the orthogonal manifold
and the quadratic group related to G for Un and Vn respectively, computed as
UnT Un − In F and VnT GVn − G F . Finally in Figure 4 we show that the use
of the projection of the numerical solution on the manifolds is necessary. In the
picture the solid lines denote the manifold errors of the numerical solutions for
U (t) and V (t) using the projection while the dashed lines denote the errors given
by the MatLab integrator. The solution is computed by routine ode113 with a
relative tolerance set to 10−6 but it departes from the manifold very soon.

References
1. Bojanczyk A.W., Onn R., Steinhardt A.O.: Existence of the Hyperbolic Singular
Value Decomposition. Lin. Alg. Appl. 185 (1993) 21–30
2. Chu M.T., Driessel K.R.: The projected gradient method for least square matrix
approximations with spectral constraints. SIAM J. Numer. Anal. 27 (1990) 1050–
1060
3. Dieci L., Russell R.D., Van Vleck E.S.: Unitary integrators and applications to
continuous orthonormalization techniques. SIAM J. Numer. Anal. 31 (1994) 261–
281
4. Diele F., Lopez L., Peluso R.: The Cayley transform in the numerical solution of
unitary diﬀerential systems. Adv. Comput. Math. 8 (1998) 317–334
5. Higham N.: J−Orthogonal matrices: properties and generation. SIAM Rev. 45 (3)
(2003) 504–519
6. Iserles A., Munthe-Kaas H., Nørsett S.P., Zanna A.: Lie-group methods. Acta
Numerica 9 (2000) 215–365
7. Lopez L., Politi T.: Applications of the Cayley approach in the numerical solution
of matrix diﬀerential systems on quadratic groups. Appl. Num. Math. 36 (2001)
35–55
8. Onn R., Steinhardt A.O., Bojanczyk A.W.: The Hyperbolic Singular Value Decomposition and Applications. IEEE Trans. Sign. Proc. 39 (7) (1991) 1575–1588
9. Slapniˇcar I.: Componentwise analysis of direct factorization of real symmetric and
Hermitian matrices. Lin. Alg. Appl. 272 (1998) 227–275
10. Slapniˇcar I.: Highly accurate symmetric eigenvalue decomposition and Hyperbolic
SVD. Lin. Alg. Appl. 358 (2003) 387–424
11. Stewart M., Stewart G.W.: On hyperbolic triangularization: stability and pivoting.
SIAM J. Matrix Anal. Appl. 19 (4) (1998) 847–860.
12. Veseli´c K.: A Jacobi eigenreduction algorithm for deﬁnite matrix pairs. Numer.
Math. 64 (1993) 241–269
13. Wright K.: Diﬀerential equations for the analytic singular value decomposition of
a matrix. Numer. Math. 63 (1992) 283–295

