Data Reduction Based on Spatial Partitioning
Gongde Guo, Hui Wang, David Bell, and Qingxiang Wu
School of Information and Software Engineering, University of Ulster
Newtownabbey, BT37 0QB, N.Ireland, UK
{G.Guo, H. Wang, DA.Bell, Q.Wu}@ulst.ac.uk

Abstract. The function of data reduction is to make data sets smaller, while
preserving classification structures of interest. A novel approach to data
reduction based on spatial partitioning is proposed in this paper. This algorithm
projects conventional database relations into multidimensional data space. The
advantage of this approach is to change the data reduction process into a spatial
merging process of data in the same class, as well as a spatial partitioning
process of data in different classes, in multidimensional data space. A series of
partitioned regions are eventually obtained and can easily be used in data
classification. The proposed method was evaluated using 7 real world data sets.
The results were quite remarkable compared with those obtained by C4.5 and
DR. The efficiency of the proposed algorithm was better than DR without loss
of test accuracy and reduction ratio.

1 Introduction
Data reduction is a process used to transform raw data into a more condensed form
without losing significant semantic information. In data mining, data reduction in a
stricter sense refers to feature selection and data sampling [1], but in a broader sense,
data reduction is regarded as a main task of data mining [2]. Data mining techniques
can thus, in this broad sense, be regarded as a method for data reduction. Data
reduction is interpreted as a process to reduce the size of data sets while preserving
their classification structures. Wang, et al [3] propose a novel method of data
reduction based on lattices and hyper relations. The advantage of this is that raw data
and reduced data can be both represented by hyper relations. The collection of hyper
relations can be made into a complete Boolean algebra in a natural way, and so for
any collection of hyper tuples its unique least upper bound (lub) can be found, as a
reduction.
According to the method proposed in [3], the process of data reduction is to find
the least upper bound of the raw data and to reduce it. The process of data reduction
can be regarded as a merging process of simple tuples (raw data) and hyper tuples that
have been generated in the same class to generate new hyper tuples. The success of
each merging operation depends on whether the new hyper tuple generated from this
merging operation covers in same sense a simple tuple of another class. If the new
hyper tuple does cover a simple tuple of another class, the operation is cancelled. The
merging operation repeats recursively until all the data including hyper tuples and
same class simple tuples cannot be merged again. The main drawback of the method
proposed in [3] is its efficiency, since much time is spent in trying probable merge. In
this paper, we introduce the complementary operation of hyper tuples and attempt to
V.N. Alexandrov et al. (Eds.): ICCS 2001, LNCS 2074, pp. 245–252, 2001.
© Springer-Verlag Berlin Heidelberg 2001

246

G. Guo et al.

use the irregular regions to represent reduced data. The main goal of the proposed
algorithm is to improve its efficiency and reduction ratio whilst preserving its
classification accuracy.
The remainder of the paper is organized as follows. Section 2 introduces the
definitions and notation. Section 3 describes the data reduction and classification
algorithm based on spatial partitioning, in which the execution process of the
algorithm is demonstrated by graphical illustration. The experimental results are
described and the evaluation is given in Section 4. Section 5 ends the paper with a
discussion and an indication of proposed future work.

2 Definitions and Notation
In the context of the paper, Hyper relations are a generalization of conventional
database relations in the sense that it allows sets of values as tuple entries. A hyper
tuple is a tuple where entries are sets instead of single values. A hyper tuple is called a
simple tuple, if all its entries have a cardinality of 1. Obviously a simple tuple is a
special case of hyper tuple.
Consider two points pi, pj denoted as pi=(pi1, pi2, �, pin), pj=(pj1, pj2, �, pjn) and
two spatial regions ai, aj denoted as ai=([ti11, ti12], [ti21, ti22], �, [tin1, tin2]), aj=([tj11,
tj12], [tj21, tj22], �, [tjn1, tjn2]) in multidimensional data space, in which [til1, til2] is the
projection of ai to its l-th component, and til2 � til1, l=1, 2, �, n. In this paper, for
simplicity and uniformity, any point pi is represented as a spatial region in
multidimensional data space, viz. pi=([pi1, pi1], [pi2, pi2], �, [pin, pin]). This is often a
convenient and uniform representation for analysis.
Definition 1 Given two regions ai, aj in multidimensional data space, the merging
operation of two regions denoted by ‘ � ’ can be defined as: ai � aj=([min(ti11, tj11),
max(ti12, tj12)], [min(ti21, tj21), max(ti22, tj22)], …, [min(tin1, tjn1), max(tin2, tjn2)]).
The intersection operation ‘ � ’ of two regions in multidimensional data space can
be defined as:
ai � aj =([max(ti11, tj11), min(ti12, tj12)], [max(ti21, tj21), min(ti22, tj22)], …, [max(tin1, tjn1),
min(tin2, tjn2)]). ai � aj is empty, if and only if there exists a value of l such that
max(til1, tjl1) � min(til2, tjl2), where l=1,2,�, n.
A point merging (or intersecting) with a region can be regarded as a special case
according to above definition.
Definition 2 Given a region aj in multidimensional data space denoted as aj = ([tj11,
tj12], [tj21, tj22], �, [tjn1, tjn2]), the complementary operation of aj is defined as: a j =
( [t j11 , t j12 ] , [tj21, tj22], �, [tjn1, tjn2]) � ([tj11, tj12], [t j 21 , t j 22 ] , �, [tjn1, tjn2]) � � � ([tj11,
tj12], [t j 21 , t j 22 ], � , [t jn1 , t jn 2 ]) � ( [t j11 , t j12 ] , [t j 21 , t j 22 ] , [tj31, tj32], �, [tjn1,tjn2]) � ([tj11,tj12],

[t j 21 , t j 22 ] , [t j 31 , t j 32 ] ,�,[tjn1, tjn2]) � � � ([ t j11 , t j12 ], � , [t j ( n �1)1 , t j ( n �1) 2 ], [t jn1 , t jn 2 ]) � � �
( [t j11 , t j12 ] , [t j 21 , t j 22 ] ,�, [t jn1 , t jn 2 ] ), is the region in the multidimensional data space
complementary region aj .
Definition 3 Given a point pi denoted as pi=(pi1, pi2, �, pin) and a region aj denoted as
aj=([tj11, tj12], [tj21, tj22], �, [tjn1, tjn2]) in multidimensional data space, the hyper
similarity of pi, aj denoted as S(pi, aj) is defined as follows:

Data Reduction Based on Spatial Partitioning

247

If aj is a regular region, aj=([tj11, tj12], [tj21, tj22], �, [tjn1, tjn2]), the hyper similarity
S(pi, aj) is equal to the number of l which satisfies tjl1�pil�tjl2, in which l=1, 2, �, n.
If aj is an irregular region, consisting of h regular regions, denoted as
a j ={ a j1 , a j 2 ,�, a jh }, the hyper similarity S(pi, aj) equals the value of max(S(pi, aj1),
S(pi, aj2), �, S(pi, ajh)).
Definition 4 Given a point pi =(pi1, pi2, �, pin) and a region aj in multidimensional
data space, the universal hyper relation ‘�’ is defined as: pi � aj, if and only if the
point pi falls into the spatial region of aj.
If aj is a regular region denoted as aj =([tj11, tj12], [tj21, tj22], �, [tjn1, tjn2]). pi�aj if
for all values of l, tjl1� pil� tjl2, where l=1, 2, �, n.
If aj is an irregular region, consisting of h regular regions, a j ={ a j1 , a j 2 , � , a jn } .
pi� aj, if and only if there exists a regular region a jl where pi�ajl, in which, l=1,2,�, h.
For simplicity, all the data attributes used in this paper for data reduction and
classification are numerical. Set union operation (respectively intersection and
complementation operations) can be used to replace the ‘�’ operation (‘�’ and ‘-’
operations respectively) defined above for categorical data or binary data. In addition,
the standard set inclusion operation and subset operation can be used to replace the
hyper similarity operation and universal hyper relation operation respectively.

3 Data Reduction & Classification Algorithm
Let a training data set Dt={d1, d2, �, dm}, where di =( d i1 , di 2 , � , din ). d i is represented
as di=([di1, di1], [di2, di2], �, [din, din]) using spatial region representation as a point in
multidimensional data space. Supposing that there are k classes in the training data set
and din is a decision attribute, value din�{t1, t2, �, tk}, the spatial partitioning
algorithm is as follows:
1. t=0
2. M it = � {d j } , dj � Dt, i=1, 2, �, k, j=1, 2, �, m
d jn � t i

M it � M tj , i�j, i=1, 2, �, k, j=1, 2, �, k

3.

M it, j

4.

Sit = M it � M it,1 � � � M it, j �1 � M it, j �1 � � � M it, k , i=1, 2, �, k

5.

Dt+1 ={di | di � M it, j , i�j, i=1, 2, �, k, j=1, 2, �, k}

6.
7.
8.

If (Dt+1=�) go to 8
t=t+1, go to 2
Ri ={ S i0 , S i1 , � , S it }, i=1, 2, �, k.

=

Some symbols used in the above algorithm are: S it -the biggest irregular region of
ti class obtained in the t-th cycle; Dt -the training data set used in the t-th cycle; M it the merging region of ti class data obtained in the t-th cycle; M it, j - the intersection of

M it and M tj obtained in the t-th cycle. Ri - the results obtained via running the
algorithm are a series of distributive regions of ti class data, in which, i=1, 2,�, k.

248

G. Guo et al.

Given the training data set shown in Figure 2-1, the running process of the
algorithm in 2-dimensional space is illustrated graphically below.
The training data set includes 30 data points and is divided into three classes of
black, grey and white. The distribution of data points in 2-dimensional data space is
shown in Figure 2-1.
At the beginning of the running process, we merge all the data in the same class
for each individual class and gain three data spatial regions M 10 , M 20 , M 30 represented
by bold line, fine line and broken line respectively. The intersections of
M 10, 2 , M 10, 3 , M 20, 3 are also represented by bold line, fine line and broken line in Figure 22 and Figure 2-3, in which, M 10, 2 � M 10 � M 20 , M 10, 3 � M 10 � M 30 , M 20, 3 � M 20 � M 30 . In
the first cycle, three partitioning regions S10 , S 20 , S 30 shown in Figure 2-4 are obtained.

M 10

M 30

M 20
Fig. 2-1. The distribution of data points

M

Fig. 2-2. Three merging regions

M 10, 3

0
1, 2

S

0
3

S 10

S 20
M 20, 3

Fig. 2-3. Three intersection regions

Fig. 2-4. Three partitioning regions

Data Reduction Based on Spatial Partitioning

249

In which, S10 � M 10 � M 10, 2 � M 10, 3 , S 20 � M 20 � M 10, 2 � M 20, 3 , S 30 � M 30 � M 10, 3 � M 20, 3 .
Obviously, if test data falls into S10 (or S 20 , S 30 ), it belongs to the black class (or the
grey, white class respectively). If it falls into none of S10 , S 20 and S 30 , it should fall
into M 10, 2 , or M 10, 3 or M 20, 3 . If so, it can not be determined which class it belongs to. All
the data in the original data set which belong to M 10, 2 or M 10, 3 or M 20, 3 are taken out and
form a new training data set. This new training data set is partitioned again and
another three merging regions: M 11 , M 21 , M 31 as well as another three intersection
regions: M 11, 2 , M 11, 3 , M 21, 3 are obtained in the second cycle. The process of merging and
partitioning is executed recursively until there is no data in the new training set. This
process is illustrated graphically below from Figure 2-4 to Figure 2-9.

Fig. 2-5

Fig. 2-6

Fig. 2-7

Rblack

Fig. 2-8

R white
Rgrey

Fig. 2-9

Fig. 2-10. The distributive regions of black data

A series of distributive regions of data of each class are obtained via learning from
the training data set. The distributive regions of data of the black class are represented
against a grey background in Figure 2-10.

250

G. Guo et al.

Using irregular regions representing spatial distributive regions of different
classes can give higher data reduction efficiency. The term ‘irregular regions’ in this
paper means the projection of the spatial region to each dimension might be a series
of discrete intervals.
It is probable that the partitioning process cannot continue for some data
distributions because equal merging regions could be obtained in the partitioning
process. See Figure 3-1 for instance. The three merging regions of M black , M grey and

M white are totally equal to each other.
In this situation, one resolution is to select an attribute as a partitioning attribute to
divide the data set into two subsets and then for each subset according to the above
algorithm, continue partitioning until all the data has been partitioned.

M black , M grey , M white

Fig. 3-1. Special training data set

Fig. 3-2. Dividing and partitioning

Figure 3-1 shows that the partitioning cannot continue because
M black � M grey � M white . What we can do is to divide the data set into two subsets on the
Y-axis. We then execute the spatial partitioning operation for each subset
respectively shown in Figure 3.2 to let the partitioning process continue. A series of
distributive regions obtained from the partitioning process can easily be used in
classification.
Given a testing data t, Ri is a set of distributive regions of ti class obtained by
running the partitioning algorithm. We use universal hyper relation � to classify the
testing data t, the partitioning algorithm is as follows:
�
�

If t � Ri viz. t falls into the regions of Ri, then t is classified by the class of Ri.
If there is not such a region of Ri which can satisfy � operation, the class of t can
be classified by using hyper similarity defined above, viz. t is classified by the
class of Rj, where j is defined as S(t, Rj)=max(S(t, R1), S(t, R2), �, S(t, Rk)).

A system called Partition&Classify or P&C for simplicity was developed using the
proposed method, it can classify unlabeled testing data effectively. The algorithm was
evaluated for classification using some real world data sets and the results are quite
remarkable. The experimental results are reported in next section.
This sort of data reduction and classification is very helpful for large databases and
data mining based on some of the following reasons [3]:

Data Reduction Based on Spatial Partitioning

251

� It reduces the storage requirements of data used mainly for classification;
� It offers better understandability for the knowledge discovered;
� It allows feature selection and continuous attribute discretization to be achieved as
by-products of data reduction.

4 Experiment and Evaluation
The ultimate goal of data reduction is to improve the performance of learning, hence
the main goal of our experiment is set to evaluate how well our proposed method
performs for data reduction and to calculate its accuracy of prediction and
performance for some real world data sets. We use the 5-fold cross validation method
to evaluate its prediction accuracy and compare the results obtained from experiment
with some of standard data mining methods.
Seven public databases are chosen from the UCI machine learning repository.
Some information about these databases is listed in Table 1.
Table 1. General information about the data sets

Data set
Aust
Diab
Hear
Iris
Germ
TTT
Vote

NA
14
8
13
4
20
9
18

NN
4
0
3
0
11
9
0

NO
6
8
7
4
7
0
0

NB
4
0
3
0
2
0
18

NE
690
768
270
150
1000
958
232

CD
383:307
268:500
120:150
50:50:50
700:300
332:626
108:124

In Table 1, the meaning of the title in each column is follows: NA-Number of
attributes, NN-Number of Nominal attributes, NO-Number of Ordinal attributes, NBNumber of Binary attributes, NE-Number of Examples, and CD-Class Distribution.
We also selected the C4.5 algorithm installed in the Clementine’ software package
as our benchmark for comparison and the DR algorithm [3] as a reference to data
reduction. A 5-fold cross validation method was used to evaluate the performance of
C4.5, DR and the P&C algorithm, the classification accuracy and the data reduction
ratio were obtained and shown in Table 2. The reduction ratio we used is defined as
follows:
(The number of tuples in the original data set - The number of the biggest irregular
regions in the model) / (The number of tuples in the original data set).
The experimental results in Table 2 show that P&C outperforms C4.5 with respect
to the cross validation test accuracy for the data sets but Vote. For the data sets with
more numerical attributes (e.g. Iris and Diab data sets) P&C excels in ratio of data
reduction compared to DR while preserving the accuracy of classification. Both DR
and P&C were tested on the same PC with Pentium(r)
� ���������, experiments
show that DR has the highest testing accuracy among the three tested algorithms and

252

G. Guo et al.

P&C has more higher reduction ratio than DR. In particular, on average P&C is about
2 times faster than DR.
Table 2. A comparison of C4.5, DR and P&C in testing accuracy and reduction ratio (TATesting Accuracy, RR-Reduction Ratio).

Data set
Aust
Diab
Hear
Iris
Germ
TTT
Vote
Average

TA:C4.5
85.2
72.9
77.1
94.0
72.5
86.2
96.1
83.4

TA:DR
87.0
78.6
83.3
96.7
78.0
86.9
87.0
85.4

TA:P&C
86.9
77.4
82.5
96.7
77.2
86.1
86.3
84.7

RR:DR
70.6
68.6
74.1
94.0
73.1
81.5
99.1
80.1

RR:P&C
69.1
71.1
74.2
97.1
73.2
80.7
98.8
80.6

5 Conclusion
In this paper, we have presented a novel approach to data reduction and classification
(and so data mining) based on spatial partitioning. The reduced data can be regarded
as a model of the raw data. We have shown that data reduction can be viewed as a
process to find the biggest irregular region to represent the data in the same class. It
executes union, intersection and complement operations in each dimension using the
projection of spatial regions in multidimensional data space and represents the raw
data of the same class by the local biggest irregular regions to realize the goal of data
reduction. A series of spatial regions obtained from the learning process can be used
in classification. Further research is required into how to eliminate noise and resolve
the marginal problem to improve testing accuracy as current P&C is sensitive to noise
data and data in marginal areas has lower testing accuracy.

References
1.
2.
3.
4.
5.
6.
7.
8.

Weiss, S. M., and Indurkhya, N. (1997). Predictive Data Mining: A Practical Guide.
Morgan Kaufmann Publishers, Inc.
Fayyad, U. M. (1997). Editorial. Data Mining and Knowledge Discovery – An
International Journal 1(3).
Hui Wang, Ivo Duntsch, David Bell. (1998). Data reduction based on hyper relations. In
proceedings of KDD98, New York, pages 349-353.
Duntsch,I., and Gediga, G. (1997). Algebraic aspects of attribute dependencies in
information systems. Fundamenta Informaticae 29:119-133.
Gratzer, G. (1978). General Lattice Theory. Basel: Birkhauser.
Ullman, J. D. (1983). Principles of Database Systems. Computer Science Press, 2 edition.
Wolpert, D. H. (1990). The relationship between Occam’s Razor and convergent
guessing. Complex Systerms 4:319-368.
Gongde Guo, Hui Wang and David Bell. (2000). Data ranking based on spatial
partitioning. In proceedings of IDEAL2000, HongKong, pages78-84. Springer-Verlag
Berlin Heidelberg 2000.

