Available online at www.sciencedirect.com

Procedia Computer Science 9 (2012) 1234 – 1239

International Conference on Computational Science, ICCS 2012

Regularized Multiple Criteria Linear Programming via Linear
Programming
Zhiquan Qi, Yingjie Tian∗, Yong Shi∗
Research Center on Fictitious Economy & Data Science, Chinese Academy of Sciences, Beijing 100190, China

Abstract
Regularized multiple-criteria linear programming (RMCLP) model is a new powerful method for classiﬁcation and
has been used in various real-life data mining problems. In this paper, instead of solving quadratic programming (QP)
in algorithm RMCLP , we proposed a novel RMCLP via linear programming (LP) for pattern classiﬁcation(called LPRMCLP) . Numerical experiments on benchmark datasets show that the proposed method is comparable to traditional
RMCLP in accuracy, however considerably faster than it.
Keywords: Classiﬁcation, RMCLP, Linear Programming

1. Introduction
Given a independently and identically distributed (i.i.d) training set
T = {(x1 , y1 ), · · · , (xl , yl )} ∈ (Rn × Y)l ,

(1)

where xi ∈ Rn , yi ∈ Y = {−1, 1}, i = 1, · · · , l. Classiﬁcation refers to the construction of a discriminate function
from the input space Rn onto the unordered set of classes Y. Recently, Shi and his colleagues[1] proposed a multiple
criteria linear programming (MCLP) model for classiﬁcation. A lot of experiments show the eﬀectiveness of the
approach. Then various improved algorithms were proposed one after the other [2, 3, 4, 5, 6] and had been applied
to handle many real world data mining problems, such as credit card portfolio management [7, 8, 9], bioinformatics
[10], information intrusion and detection [11], ﬁrm bankruptcy [12, 13], and etc.
In this paper, we proposed a Regularized Multiple-Criteria Linear Programming Via Linear Programming(called
LP-RMCLP). This algorithm can be considered as an improved version of [2]. Their main diﬀerence is that our
algorthm depends on solving linear programming instead of quadratic programming. Experiment results show that
the speed of our algorithm is about 10 times faster than RMCLP on the premise of keeping the classiﬁcation precision.
We describe our notation now. All vectors will be column vectors unless transposed to a row vector by a prime
T
. The inner product of two vectors x and y in the n-dimensional real space n will be denoted by (x · y). For
w ∈ n , w 1 , denotes the 1- norm while w 2 denotes the 2-norm. The notation B ∈ m×n will signify a real m · n
∗ Corresponding

author
Email addresses: qizhiquan@gucas.ac.cn (Zhiquan Qi), tyj@gucas.ac.cn (Yingjie Tian)

1877-0509 © 2012 Published by Elsevier Ltd. Open access under CC BY-NC-ND license.
doi:10.1016/j.procs.2012.04.134

Zhiquan Qi / Procedia Computer Science 9 (2012) 1234 – 1239

1235

matrix. For such a matrix, BT will denote the transpose of B. A vector of all ones or all zeros of arbitrary dimension
will be denoted by e and 0, respectively. For x ∈ n , the notation eT x denotes the sum of the components of x.
The remaining parts of the paper are organized as follows. Section 2 introduces the basic formulation of MCLP
and RMCLP; In section 3 describes in detail our proposed algorithms LP-RMCLP; All experiment results are shown
in the section 4; Last section gives the conclusions.
2. Regularized MCLP for Data Mining
We give a brief introduction of MCLP in the following. For classiﬁcation about the training data [1]. Data
separation can be achieved by two opposite objectives. The ﬁrst objective separates the observations by minimizing
the sum of the deviations (MSD) among the observations. The second maximizes the minimum distances (MMD) of
observations from the critical value. the overlapping of data u should be minimized while the distance v has to be
maximized. However, it is diﬃcult for traditional linear programming to optimize MMD and MSD simultaneously.
According to the concept of Pareto optimality, we can seek the best trade-oﬀ of the two measurements[8, 9]. So
MCLP model can be described as follows:
min
u

s.t.

eT u & max eT v,

(2)

v

(w · xi ) + (ui − vi ) = b,

for {i|yi = 1},

(3)

(w · xi ) − (ui − vi ) = b,

for {i|yi = −1},

(4)
(5)

u, v ≥ 0,

where e ∈ Rl be vector whose all elements are 1, w and b are unrestricted, ui is the overlapping and vi the distance
from the training sample xi to the discriminator (w · xi ) = b (classiﬁcation separating hyperplane). By introducing
penalty parameter c, d > 0, MCLP has the following version
min
u,v

s.t.

ceT u − deT v,

(6)

(w · xi ) + (ui − vi ) = b,

for {i|yi = 1},

(7)

(w · xi ) − (ui − vi ) = b,

for {i|yi = −1},

(8)
(9)

u, v ≥ 0,

A lot of empirical studies have shown that MCLP is a powerful tool for classiﬁcation. However, we cannot ensure
this model always has a solution under diﬀerent kinds of training samples. To ensure the existence of solution, recently,
Shi et.al proposed a RMCLP model by adding two regularized items 12 wT Hw and 12 uT Qu on MCLP as follows(more
theoretical explanation of this model can be found in [2]):

min
z

s.t.

1
1 T
w Hw + uT Qu + deT u − ceT v,
2
2
(w · xi ) + (ui − vi ) = b, for {i|yi = 1},
(w · xi ) − (ui − vi ) = b, for {i|yi = −1},
u, v ≥ 0,

(10)
(11)
(12)
(13)

where z = (wT , uT , vT , b)T ∈ Rn+l+l+1 , H ∈ Rn×n , Q ∈ Rl×l are symmetric positive deﬁnite matrices. Obviously, the
regularized MCLP is a convex quadratic programming. The geometric meaning of the model is shown in Figure.1.
3. Regularized Multiple-Criteria Linear Programming Via Linear Programming(LP-RMCLP)
Choose H, Q to be identity matrix, (10)∼ (13) can be written as

1236

Zhiquan Qi / Procedia Computer Science 9 (2012) 1234 – 1239

Figure 1: Geometric meaning of MCLP.

1
w
2

min

w,u,v,b

2
2

+

1
u
2

l
2
2

l

+d

ui − c

vi ,

(14)

i

i=1

yi ((w · xi ) − b) = vi − ui , i = 1, · · · , l,
ui , vi ≥ 0, i = 1, · · · , l.

s.t.

(15)
(16)

By introducing its Lagrange function
L(w, u, v, b, α, β, η) =

1
w
2

2
2

+

1
u
2

l
2
2

+d

l

ui − c
i=1

vi +

(17)

i

l

l

αi (yi ((w · xi ) − b) + ui − vi ) −
i=1

l

βi ui −
i=1

ηi vi ,

(18)

i=1

where αi , βi , ηi ∈ R are the Lagrange multipliers, Therefore the dual problem of (14)∼ (16) can be formulated as
max

w,u,v,b,α,β,η

s.t.

L(u, v, w, b, α, β, η),

(19)

∇w,u,v,b L(u, v, w, b, α, β, η) = 0,

(20)
(21)

βi , ηi ≥ 0, i = 1, · · · , l.
From equation (20) we get
l

∇w L = w +

yi αi xi = 0,

(22)

∇ui L = ui + d + αi − βi = 0, i = 1, · · · , l,
∇vi L = −c − αi − ηi = 0, i = 1, · · · , l,

(23)
(24)

i=1

l

∇b L = −

yi αi = 0.
i=1

(25)

1237

Zhiquan Qi / Procedia Computer Science 9 (2012) 1234 – 1239

Substituting the above equations into problem (14)∼ (16), the dual problem can be expressed as
−

max
α,u,b

1
2

l

l

yi y j αi α j (xi · x j ) −
i=1 j=1

1
2

l

l

l

ui u j −
i=1 j=1

αi b,

(26)

i=1

l

yi αi = 0, i = 1, · · · , l,

s.t.

(27)

i=1

−d − ui ≤ αi ≤ −c, i = 1, · · · , l.

(28)

From (22), we can obtain w = − li=1 yi αi xi . Now we use 1-norm distance α
and u 1 for u 22 . So the problem (14)∼ (16) can be rewritten as

α,u,v,b

1
α
2

s.t.

yi (−

min

1
u
2

+

1

l
1

+d

1

to replace the 2-norm distance w 22 ,

l

ui − c

vi ,

(29)

i

i=1

l

y j α j (x j · xi ) − b) = vi − ui , i = 1, · · · , l,

(30)

j=1

ui , vi ≥ 0, i = 1, · · · , l.

(31)

By introducing the constraints −λ ≤ α ≤ λ and −π ≤ u ≤ π, the above problem can be further expressed as
min

α,u,v,b,λ,π

l

1
2

λi +
i

l

1
2

l

l

πi + d
i

ui − c

vi ,

(32)

i

i=1

l

s.t.

y j α j (x j · xi ) − b) = vi − ui , i = 1, · · · , l,

yi (−

(33)

j=1

−λi ≤ αi ≤ λi , i = 1, · · · , l,
−πi ≤ ui ≤ πi , i = 1, · · · , l.
ui , vi , λi , πi , ≥ 0, i = 1, · · · , l.

(34)
(35)
(36)

Introduce the kernel function K(x, x ) = (Φ(x) · Φ(x )) to take place of (x · x ), where Φ(·) is a mapping from the
input space Rn to Hilbert space H
Rn → H,
x → Φ(x).

Φ:

(37)

Therefore, the linear programming problem (32)∼ (36) turns to be

min

α,u,v,b,λ,π

1
2

l

λi +
i

1
2

l

l

πi + d
i

l

ui − c
i=1

vi ,

(38)

i

l

s.t.

y j α j K(x j · xi ) − b) = vi − ui , i = 1, · · · , l,

yi (−

(39)

j=1

−λi ≤ αi ≤ λi , i = 1, · · · , l,
−πi ≤ ui ≤ πi , i = 1, · · · , l.
ui , vi , λi , πi , ≥ 0, i = 1, · · · , l.
So we can establish the following algorithm:

(40)
(41)
(42)

1238

Zhiquan Qi / Procedia Computer Science 9 (2012) 1234 – 1239

Algorithm 1 LP-RMCLP
1:

2:
3:
4:
5:

Given a mixed training set:
T = {(x1 , y1 ), · · · , (xl , yl )} ∈ (Rn × Y)l ,
where xi ∈ Rn , yi ∈ Y = {−1, 1}, i = 1, · · · , l. ;
Choose appropriate penalty parameters c, d and kernel function K;
Construct and solve the problem (38) and (42), then get its solution:
αˆ = (αˆ 1 , · · · , αˆ l ) , uˆ = (ˆu1 , · · · , uˆ l ) , vˆ = (ˆv1 , · · · , vˆ l ) ,
ˆ
λˆ = (λˆ 1 , · · · , λˆ l ) , πˆ = (ˆπ1 , · · · , πˆ l ) , b;
Obtain the decision function:
ˆ
f (x) = − li=1 yi αˆ i K(xi · x j ) + b.

4. Numerical Experiments
Our algorithm code was wrote in MATLAB 2010. The experiment environment: Intel Core I5 CPU, 2 GB memory.
The “linprog” function with MATLAB is employed to solve linear programming problem related to this paper.
The testing accuracies of all experiments are computed using standard 10-fold cross validation. c, d and RBF
kernel parameter σ are all selected from the set {2i |i = −7, · · · , 7} by 10-fold cross validation on the tuning set
comprising of random 10% of the training data. Once the parameters are selected, the tuning set was returned to the
training set to learn the ﬁnal decision function.
To demonstrate the capabilities of our algorithm, we report results on UCI dataset. They are respectively “Sonar”,
“Ionosphere”, “Australian”, “Pima-Indian”, “CMC”, “Votes”, “WPBC”. In all experiments, out method compared
with the standard RMCLP. The table 1 give the experiments results
Table 1: The percentage of tenfold testing accuracy of RMCLP and LP-RMCLP.

Dataset
Sonar
Ionosphere
Australian
Pima-Indian
CMC
Votes
WPBC

RMCLP
accuracy
time(second)
78.11± 5.42
6.32
87.64± 6.83
11.45
86.89 ± 5.86
30.12
77.71 ± 6.13
34.26
69.23 ± 5.52
57.54
96.18 ± 4.32
18.66
83.26 ± 3.21
3.11

LP-RMCLP
accuracy
time(second)
79.54± 5.68
45.25
89.24± 5.71
149.32
85.32 ± 5.23
280.46
76.24 ± 6.62
328.34
68.41 ± 5.90
612.27
97.21 ± 4.51
192.87
82.98 ± 3.12
23.45

Table 1 shows that our algorithm’s the training time in the condition of 10-fold cross validation consumed is much
less than the RMCLP while their accuracy are in the same level. Generally speaking, our algorithm is faster than both
of RMCLP over ten times.
5. Conclusion
In this paper, we have proposed a new algorithm: LP-RMCLP, for binary classiﬁcation by solving a standard linear
programming problem. Experiments have shown that our algorithm is considerably faster, usually over ten times, than
RMCLP while the same level of accuracy are kept. Therefore, It is suitable for solving large-scale data sets. Future
research includes comprehensive test of the new algorithm and its application.
6. Acknowledgment
This work has been partially supported by grants from National Natural Science Foundation of China( NO.70921061,
NO.10601064), the CAS/SAFEA International Partnership Program for Creative Research Teams, Major Internation-

Zhiquan Qi / Procedia Computer Science 9 (2012) 1234 – 1239

1239

al(Ragional) Joint Research Project(NO.71110107026), the President Fund of GUCAS.
References
[1] D. Olson, Y. Shi, Introduction to business data mining, Irwin-McGraw-Hill Series: Operations and Decision Sciences, McGraw-Hill, 2006.
[2] Y. Shi, Y. Tian, X. Chen, P. Zhang, Regularized multiple criteria linear programs for classiﬁcation, Science in China Series F: Information
Sciences 52 (10) (2009) 1812–1820.
[3] G. Kou, Y. Shi, S. Wang, Multiple criteria decision making and decision support systems - guest editor’s introduction, Decision Support
Systems 51 (2011) 247–249.
[4] J. Zhang, Y. Shi, P. Zhang, Several multi-criteria programming methods for classiﬁcation, Computers and Operations Research 36 (2009)
823–836.
[5] Y. Shi, H. Lee, A binary integer linear program with multi-criteria and multi-constraint levels, Computers & Operations Research 24 (3)
(1997) 259–273.
[6] J. Li, L. Wei, G. Li, W. Xu, An evolution-strategy based multiple kernels multi-criteria programming approach: The case of credit decision
making, Decision Support System 51(2):292-298.
[7] G. Kou, Y. Peng, Y. Shi, M. Wise, W. Xu, Discovering Credit Cardholders’ Behavior by Multiple Criteria Linear Programming., Annals of
Operations Research 135 (1) (2005) 261–274.
[8] Y. Shi, W. Wise, M. Lou, Multiple criteria decision making in credit card portfolio management, Multiple Criteria Decision Making in New
Millennium, 2001, pp. 427–436.
[9] Y. Shi, Y. Peng, W. Xu, X. Tang, Data Mining via Multiple Criteria Linear Programming: Applications in Credit Card Portfolio Management.,
International Journal of Information Technology and Decision Making 1 (1) (2002) 131–152.
[10] J. Zhang, W. Zhuang, N. Yan, Classiﬁcation of hiv-1 mediated neuronal dendritic and synaptic damage using multiple criteria linear programming, Neuroinformatics 2 (2004) 303–326.
[11] G. Kou, Y. Peng, Y. Shi, Z. Chen, X. Chen, A multiple-criteria quadratic programming approach to network intrusion detection, in: CASDMKM, 2004, pp. 145–153.
[12] W. Kwak, Y. Shi, S. W. Eldridge, G. Kou, Bankruptcy prediction for japanese ﬁrms: using multiple criteria linear programming data mining
approach., International Journal of Business Intelligence and Data Mining 1 (4) (2006) 401–416.
[13] J. He, Y. Zhang, Y. Shi, G. Huang, Domain-driven classiﬁcation based on multiple criteria and multiple constraint-level programming for
intelligent credit scoring., IEEE Transactions on Knowledge and Data Engineering 22 (6) (2010) 826–838.

