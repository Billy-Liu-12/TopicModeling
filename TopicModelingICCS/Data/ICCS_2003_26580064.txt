Continuous Time Markov Decision Processes with
Expected Discounted Total Rewards
Qiying Hu1 , Jianyong Liu2 , and Wuyi Yue3
1

2

College of International Business & Management,
Shanghai University, Shanghai 201800, China.
qyhu@mail.shu.edu.cn
Institute of Applied Mathematics, Academia Sinica, Beijing 100080, China.
3
Dept. of Information Science and Systems Engineering
Konan University, Kobe 658-8501, JAPAN
yue@konan-u.ac.jp

Abstract. This paper discusses continuous time Markov decision processes with
criterion of expected discounted total rewards, where the state space is countable, the reward rate function is extended real-valued and the discount rate is a
real number. Under necessary conditions that the model is well defined, the state
space is partitioned into three subsets, on which the optimal value function is positive infinity, negative infinity, or finite, respectively. Correspondingly, the model
is reduced into three submodels, by generalizing policies and eliminating some
worst actions. Then for the submodel with finite optimal value, the validity of the
optimality equation is shown and some its properties are obtained.

1

Introduction

Markov decision processes (MDP) have been studied well since its beginning in 1960s.
While continuous time MDP (CTMDP) [1], as one of its three basic models, was also
studied well though has some delay with respect to the other two basic models, discrete
time MDP (DTMDP) [2], and semi-Markov decision processes (SMDP) [3]. A new area
is the hybrid system which combines event-driven dynamics and time-driven dynamics,
e.g., see [4]. The criterions include discounted criterion, average criterion, expected total
rewards and mixed criterion, etc. The standard results in MDP with discounted criterion
include the following three aspects. 1) The model is well defined. 2) The optimality
equation holds. 3) A stationary policy achieving the supermum of the optimality equation
will be optimal. In order to obtain these standard results, some conditions should be
required. The general and the most usual method to study a MDP model is first to
present a set of conditions for the model, and then, based on the conditions, show the
standard results 1), 2) and 3) successively.
There are various conditions presented in literature, especially for DTMDP and
SMDP.As for CTMDP with discounted criterion, [5] studied it with unbounded transition
rates by using the general method. In [6], the author studied the CTMDP also with
unbounded transition rates but by using a transformation method, which can transform
the CTMDP into a DTMDP under the discounted criterion. Under this transformation, the
corresponding optimality equations and discounted objective functions for the stationary
This research was supported by the National Natural Science Foundation of China, and by
Institute of Applied Mathematics, Academia Sinica and by GRANT-IN-AID FOR SCIENTIFIC
RESEARCH (No.13650440), Japan.
P.M.A. Sloot et al. (Eds.): ICCS 2003, LNCS 2658, pp. 64–73, 2003.
c Springer-Verlag Berlin Heidelberg 2003

Continuous Time Markov Decision Processes

65

policies in the CTMDP model and the DTMDP model are equivalent. So the results for
CTMDP can be obtained directly from that for DTMDP. In [7], the author studied
CTMDP with bounded transition rates also by a transformation, but under which only
the discounted objectives for stationary policies in the CTMDP model and the DTMDP
model are equivalent. On the other hand, in [8] the author presented a set of conditions
for unbounded reward rate. Recently, in [9] the authors discussed a denumerable-state
CTMDP with unbounded transition and reward rates. But the method they used is a
combination of that of [6] and [8]. In [10], the authors discussed the same model and
same conditions but on average criterion.
But there are few studies about expected total rewards criterion. Though the methods
presented in [6] and [7] may be used to study it with nonpositive or nonnegative rewards,
it is restricted to the stationary policies and can not deal with the general reward rate
function or the negative discount rate.
The various conditions presented in literature are only sufficient for MDP. On the
contrary, we try to study the necessary conditions, i.e., we want to see what results can
be obtained under the condition that the MDP model is well defined. This condition is
only the standard result 1), and is obviously the precondition for studying MDP. It is
interesting to see if the standard results 2) and 3) can be implied by it. In [11], we studied
it for DTMDP with expected discounted total rewards.
This paper is a subsequent one to [11] for CTMDP, where the state space is countable,
the reward rate function is extended real-valued and the discount rate may be any real
number. The criterion is the discounted expected total rewards with no limits on the
discount factor. So it includes the traditional discounted criterion and the expected total
rewards criterion. We first generalize the general Markov policies into piecewise semiMarkov policies. Then under the condition that the model is well defined, we show
that after eliminating some worst actions, the state space S can be partitioned into three
subsets, on which the optimal value function equals +∞, −∞, or is finite, respectively.
According to it, the original MDP model can be decomposed into three corresponding
sub-models. In the one with finite optimal value, the reward rate function is finite and
bounded above at each state, and the validity of the optimality equation is discussed.
The remainder of the paper is organized as follows. Section 2 gives the formulation
of the model and presents two conditions, under which Section 3 decomposes the state
space and the MDP model. Section 4 discusses some properties of the CTMDP model.
In Section 5, the validity of the optimality equation with finite optimal value is shown
and several its properties are discussed. While Section 6 is a concluding section.

2

Model and Conditions

The CTMDP model discussed here is
{S, A(i), qij (a), r(i, a), Uα }

(1)

where the state space S and the action set A(i), available at state i, are countable;
{qij (a)|i, j∈S, a∈A(i)} is the state transition rate family satisfying qij (a) ≥ 0 for
i = j and j qij (a) = 0 for (i, a)∈Γ := {(i, a)|i∈S, a∈A(i)}, and it is assumed that
λ(i) := sup{−qii (a)|a∈A(i)} < ∞ for i∈S; the reward rate function r(i, a) is extended
real-valued; Uα is the objective function for the criterion of expected discounted total
rewards with discount factor α∈(−∞, +∞), and will be defined below.
We suppose that the measure about the time variable t is the Lebesgue measure.

66

Q. Hu, J. Liu, and W. Yue

We define the following policies as in literature, a Markov policy π = (πt , t ≥
0)∈Πm , a stochastic stationary policy π0 ∈Πs , a stationary policy f ∈ F = ×i A(i).
For a policy π = (πt ) and s ≥ 0, we define a policy π s = (πt∗ )∈Πm by πt∗ = πs+t
for t ≥ 0. For any policy π = (πt )∈Πm and t ≥ 0, we define a matrix Q(π, t) =
(qij (π, t)) with qij (π, t) =
a∈A(i) qij (a)πt (a|i) and a vector r(π, t) = (ri (π, t))
with ri (π, t) = a∈A(i) r(i, a)πt (a|i). Thus, qij (π, t) and ri (π, t) are respectively the
state transition rate family and the reward rate function under policy π. If π = π0 ∈Πs ,
then both Q(π0 , t) and r(π0 , t) are independent of t, and will be denoted respectively
by Q(π0 ) = (qij (π0 )) and r(π0 ) = (ri (π0 )).
CONDITION A: For any policy π∈Πm , the Q(π, t)-process {P (π, s, t), 0≤s≤t < ∞}
exists uniquely and is the minimal one; moreover, for any 0≤s≤t≤u < ∞,
∂
P (π, s, t) = P (π, s, t)Q(π, t), P (π, s, u) = P (π, s, t)P (π, t, u),
∂t
Pij (π, s, t) = 1, Pij (π, s, s) = δij , i, j ∈ S.
j

One can find the constructing algorithm for the minimal Q-process in [12] (II. 17) for
stationary case and in [1] for nonstationary case. Condition A is true when qij (a) is
bounded, or under the assumptions presented in [5] when qij (a) is unbounded.
Now, we generalize the concept of policies. Let Y (t) be the state of the process at time
t. Given any integer N , real numbers {ti , i = 1, 2, . . . , N } with 0 = t0 < t1 < . . . <
tN < tN +1 = ∞, and Markov policies {π n,i , n = 0, 1, 2, . . . , N, i∈S} ⊂ Πm , we
define a policy π = (π n,i ; n = 0, 1, 2, . . . , N, i∈S) as follows: for n = 0, 1, 2, . . . , N ,
if Y (tn ) = i, then π n,i is used in time interval [tn , tn+1 ), i.e., the action is chosen
n,i
(·|j) at time t∈[tn , tn+1 ] if Y (t) = j∈S. Such a policy, denoted
according to πt−t
n
n,i
by π = (π ) for short, is called a (finite) piecewise semi-Markov policy, the set of
which is denoted by Πm (s). If all π n,i = f n,i ∈F , then π = (f n,i ) is called a piecewise
semi-stationary policy, the set of which is denoted by Πsd (s).
For such a policy π, if Y (tn ) = i, then the system in [tn , tn+1 ] is a Markov process
with transition probability matrix P (π n,i , s, t). So, the system under it is a special case
of piecewise Markov process (see [13]). In details, for each s and t with 0 ≤ s ≤ t
and i, j∈S, suppose that s∈[tm , tm+1 ] and t∈[tn , tn+1 ] for some m≤n, then the state
transition probability that the system will be in state j at time t provided that the system
is in state i at time s and in state k at time tm is
Pijk (π, s, t) := Pπ {Y (t) = j|Y (s) = i, Y (tm ) = k}
Pij1 (π m,k , s − tm , tm+1 − tm )

=
j1

Pjn−m−1 jn−m (π n−1,jn−m−1 , 0, tn − tn−1 )

·
jn−m

·Pjn−m j (π n,jn−m , tn , t).

(2)

For i, j∈S, let Pij (π, t) = Piji (π, 0, t).
Now, we define the objective function, for a Markov policy π∈Πm , by
∞

Uα (π) =

exp(−αt)P (π, t)r(π, t)dt
0

(3)

Continuous Time Markov Decision Processes

67

where the integral is the Lebesgue integral. It is the expected discounted total rewards
on the whole time axis under π. Let Uα (π, t) := Uα (π t ), for t ≥ 0. Obviously,
∞

Uα (π, t) =

exp(−α(s − t))P (π, t, s)r(π, s)ds

(4)

t

is the expected discounted, to time t, total rewards on the time axis [t, ∞) under π. And
for π = (π n,i )∈Πm (s) with {tn , n = 1, 2, · · · , N } and t ≥ 0, we define inductively
tn+1

Uαn,k (π, t, i) =

Pij (π n,k , t, s)rj (π n,k , s)ds

exp(−α(s − t))

t

j

Pij (π n,k , t, tn+1 )Uαn+1,j (π, tn+1 , j),

+ exp(−α(tn+1 − t))
j

t∈[tn , tn+1 ), n = 0, 1, . . . , N − 1, k, i∈S,
UαN,k (π, t, i) = Uα (π N,k , t − tN , i), t ≥ tN , k, i∈S.

(5)

Uαn,k (π, tn , i)

Uα0,i (π, 0, i).

Let
= 0 for t = tn and k = i, and Uα (π, i) =
Let Uα (π)
be the vector with its i-th component Uα (π, i).
CONDITION B: Uα (π) is well defined (may be infinity) for each π∈Πm (s).
This condition means that :1) j Pij (π, t)rj (π, t), and furthermore, the integral in
Eq. (3), are well-defined for each π ∈ Πm ; 2) j Pij (π, t, s)Uα (π , s, j) is well-defined
for every policies π ∈ Πm and π ∈ Πm (s); 3) the sum in Eq. (5) is well-defined. The
condition is necessary to discuss CTMDP. It is well known that it is true whenever
α > 0 and r(i, a) is bounded above or below; or α ≥ 0 and r(i, a) is nonnegative or
nonpositive. Condition B is assumed to be true throughout the paper.
Conditions A and B hold mean that the CTMDP model (Eq. (1)) is well defined.
Because a policy π∈Πm is also a piecewise semi-Markov policy with arbitrary N
and t1 , t2 , . . . , tN , it follows from Eq. (5) that for π∈Πm , t ≥ 0,
t

Uα (π) =

exp(−αs)P (π, s)r(π, s)ds + exp(−αt)P (π, t)Uα (π, t),
0

which means that P (π, t) can be put out of the integral

∞
,
t

(6)

that is,

∞

exp(−α(s − t))P (π, s)r(π, s)ds
t

∞

= P (π, t)

exp(−α(s − t))P (π, t, s)r(π, s)ds
t

= P (π, t)Uα (π, t).

Eq. (6) is still true for policies π∈Πm (s) by defining r(π, s) adequately.
Let the optimal value function be Uα∗ (i) = sup{Uα (π, i)|π∈Πm (s)} for i∈S. For
ε ≥ 0, π ∗ ∈Πm (s), if Uα (π ∗ , i) ≥ Uα∗ (i) − ε (if Uα∗ (i) < +∞) or ≥ 1/ε (if Uα∗ (i) =
+∞), then π ∗ is called ε-optimal. Here, 1/0 = +∞ is assumed. 0-optimal is simply
called optimal.

3

Eliminating the Worst Actions

First, we introduce some concepts. State j can be reached from state i (and write i → j)
if there are a policy π∈Πm (s) and t ≥ 0 such that Pij (π, t) > 0. It is easy to see that

68

Q. Hu, J. Liu, and W. Yue

i → j iff there are π∈Πm and t ≥ 0 such that Pij (π, t) > 0, or equivalently there are
n ≥ 0, states j1 , j2 , . . . , jn ∈S and f ∈F such that qij1 (f )qj1 j2 (f ) . . . qjn j (f ) > 0. It is
apparent that if i → j and j → k, then i → k. For a subset S0 ⊂ S and a state i, if there
is a state j∈S0 such that i → j, then we say that S0 can be reached from state i, which is
denoted by i → S0 . Let S0∗ = {i|i → S0 } be a set of states that can reach S0 . Because
i → i, so S0 ⊂ S0∗ . A subset S0 of S is called a closed (state) set if qij (a) = 0 for all
¯ S0 , or equivalently, (S − S0 )∗ = S − S0 . Similarly as above, S0
i∈S0 , a∈A(i) and j ∈
¯ S0 and t ≥ 0.
is closed iff Pij (π, t) = 0 for all i∈S0 , π∈Πm (s), j ∈
For any closed subset S0 , if the system’s initial state i∈S0 , then the system will
remain in S0 irrespective of the policies used. Thus, the restriction of CTMDP to S0 ,
S0 -CTMDP := {S0 , (A(i), i∈S0 ), pij (a), r(i, a), Uα }
is also a CTMDP, which is called an induced sub-CTMDP by S0 . Its policies are restriction of the original policies to S0 . It is clear that Condition A and B are also true for
S0
S0 -CTMDP. Let its objective function be Uα (π).
THEOREM 1: For any closed subset S0 ⊂ S, π∈Πm (s) and i∈S0 , Uα (π, i) =
S0
Uα (π, i).
The theorem says that the induced sub-CTMDP by a closed set S0 is equivalent to
the original CTMDP in subset S0 . So, if both S0 and S − S0 are closed, then CTMDP
can be partitioned into two smaller parts:S0 -CTMDP and (S − S0 )-CTMDP. On the
other hand, if S0 is closed while Uα∗ (i) for i∈S − S0 is known, or a (ε-)optimal policy
can be obtained in S − S0 , then one need to discuss only S0 -CTMDP. Thus the state
space is partitioned and reduced.
On the other hand, some actions may be eliminated with no influence on the essential
of the model.
DEFINITION 1: Suppose that A1 (i) ⊂ A(i) for i∈S. We denote by CTMDP the
CTMDP with A(i) being replaced by A1 (i) (a symbol " " is added). If for any policy π of
the (original) CTMDP there is a policy π of the CTMDP such that Uα (π, i)≤Uα (π , i)
for all i, then the CTMDP is equivalent to the CTMDP , and we say that A(i) can be
reduced as A1 (i) for i∈S, or a ∈ A(i) − A1 (i) can be eliminated for i ∈ S.
Now, we denote by U (i) = sup{r(i, a)|a∈A(i)} and L(i) = inf{r(i, a)|a∈A(i)}
respectively the supremum and infimum of the reward rate function r(i, a) over the
action set A(i) for i∈S. Let SU = {i|U (i) = +∞}, S=∞ = {i|there is π∈Πm (s)
such that Uα (π, i) = +∞}, S∞ = {i|Uα∗ (i) = +∞} − S=∞ , S−∞ = {i|Uα∗ (i) =
−∞}, S0 = S − S=∞ − S∞ − S−∞ = {i| − ∞ < Uα∗ (i) < ∞}. These state subsets
have obvious meanings.
LEMMA 1:
1) For i∈SU , there is a policy π0 ∈Πs such that ri (π0 ) = +∞. So Uα (π0 , i) = +∞
and SU ⊂ S=∞ .
2) For i∈S with L(i) = −∞, there is a policy π0 ∈Πs such that ri (π0 ) = −∞ and then
Uα (π0 , i) = −∞.
3) For i∈S, L(i) = −∞ and U (i) = +∞ can not be true simultaneously.
THEOREM 2:
∗
= S=∞ and so S := S − S=∞ is closed.
1) S=∞

Continuous Time Markov Decision Processes

69

2) For i∈S − S−∞ , A(i) can be reduced as
A1 (i) = {a∈A(i)|r(i, a) > −∞ and

qij (a) = 0}.

(7)

j∈S−∞

∗
After the reduction, S−∞
= S−∞ and so S := S − S−∞ becomes closed.
3) For i∈S , A1 (i) can further be reduced as

A2 (i) = {a∈A1 (i)|∃π∈Πm with Uα (π, i) > −∞ and the Lebesgue
measure of {s ∈ [0, t]|πs (a|i) > 0} is positive for each t > 0}.

(8)

∗
After this reduction, S∞
= S∞ , and so S0 := S − S∞ is closed.

By Theorem 1 and 2, S can be partitioned into four subsets: S−∞ , S=∞ , S∞ , S0 .
In S−∞ , each policy is optimal; in S=∞ , there is an optimal policy (in fact, there is
a stochastic stationary optimal policy in SU ); in S∞ , Uα (π, i) < ∞ for each π, while
Uα∗ (i) = ∞, and thus there is no optimal policy, and in S0 , Uα∗ (i) is finite, and S0 is
closed after eliminating some worst actions. So, one can consider only the following
CTMDP:
S0 -CTMDP = {S0 , A2 (i), qij (a), r(i, a), Uα }.

(9)

Because i∈S−∞ when A2 (i) = ∅, Eq. (9) is a CTMDP; furthermore, we have
− ∞ < Uα∗ (i) < +∞, −∞ < r(i, a)≤U (i) < +∞,

∀i, a.

(10)

It is easy to see that all the above results restricted to Πs (s) are also true.
In the remaining of this paper, we discuss mainly the S0 -CTMDP, and so will write
S0 and A2 (i) by S and A(i) respectively for convenience.

4

Some Properties

This section discusses some properties of S0 -CTMDP (Eq. (9)) and simplifies the expression of A2 (i). First, the following lemma is from [12] (II. 15-17).
LEMMA 2: Suppose that P (t) = (pij (t)) is a homogeneous state transition probability
matrix on a countable state space S with a finite transition rate family Q = (qij ). Let
qi = −qii . Then there are nonnegative continuous functions gij (t) for i, j∈S, on [0, ∞),
such that
pij (t) = exp(−qi t)

t
0

exp(qi s)qi gij (s)ds + exp(−qi t)δij ,

i, j∈S, t ≥ 0

where δij denotes the Kronecker delta function, and for s > 0, t ≥ 0,
lim gij (s) = (1 − δij )qij /qi ,

s→0+

gij (s) = 1, gij (s + t) =
j

gik (s)pkj (t).
k

Based on the above lemma, one can proved the following two lemmas.

70

Q. Hu, J. Liu, and W. Yue

LEMMA 3: Suppose that P (t) = (pij (t)), Q and gij (t) are as in Lemma 2, supi qi <
∞, u is a finite nonnegative function in S, Z ⊂ S, i∈S. If
pij (t∗ )uj is finite for
some t∗ > 0, then hi (t) := qi exp(qi t)
and
j∈Z

j∈Z

j∈Z

gij (t)uj is finite and continuous in [0, t∗ ),

qij uj < ∞; otherwise, hi (t) = +∞ for all t > 0.

LEMMA 4: Using the symbols in Lemma 2, Suppose that supi qi < ∞, u is a finite
function in S, t∗ > 0 and i ∈ S. If j pij (t)uj is finite in [0, t∗ ], then its derivative is
well-defined and continuous in [0, t∗ ), and
d
dt

pij (t)uj
j

=
j

d
pij (t)uj =
dt

{−qi pij (t) + qi gij (t)}uj .
j

Having the above several lemmas for pre preparation, now we can prove the following
theorem, where S + := {i∈S0 |Uα∗ (i) ≥ 0} and S − := {i∈S0 |Uα∗ (i) < 0}.
THEOREM 3: 1) P (π, t)Uα∗ < ∞ is well defined for each π∈Πm (s) and t > 0.
2) For π∈Πm (s), t > 0 and i∈S, if j Pij (π, t)Uα∗ (j) = −∞, then Uα (π ∗ , i) =
−∞ for any piecewise semi-Markov policy π ∗ = (π n,j )∈Πm (s) with π 0,i = π and
t1 = t, especially, Uα (π, i) = −∞.
COROLLARY 1: Suppose that there is f ∗ ∈F such that Q(f ∗ ) is bounded, then
∗
j qij (a)Uα (j) < ∞ is well defined for any i∈S and a∈A(i).
COROLLARY 2: Suppose that f ∈F with bounded Q(f ), i∈S, t∗ > 0, [P (f, t)Uα∗ ]i is
finite in [0, t∗ ], then [P (f, t)Uα∗ ]i is differentiable in [0, t∗ ), its derivative is continuous
and
d
dt

Pij (f, t)Uα∗ (j)

=

j

j

[P (f, t)Q(f )]ij Uα∗ (j)

d
Pij (f, t)Uα∗ (j),
dt
Pij (f, t)[Q(f )Uα∗ (j)]j ,

=

j

t ∈ [0, t∗ ).

(11)

j

We conjecture that the result in Corollary 2 is also true for π∈Πm , but it needs that
Lemma 2 holds for a nonhomogeneous Markov process, which is not known to us.
To conclude equations in this section, we give the following theorem on a simplified
expression for A2 (i).
THEOREM 4: If qij (a) is uniformly bounded, then
qij (a)Uα∗ (j) > −∞},

A2 (i) ⊂ {a∈A1 (i)|

i∈S;

(12)

j

moreover, if hi (t) is finite and continuous whenever
then

j∈Z

qij uj is finite in Lemma 3,

qij (a)Uα∗ (j) > −∞},

A2 (i) = {a∈A1 (i)|
j

i∈S.

(13)

Remark 1: 1) By the above theorem, if S − is finite, or Uα∗ (i) is bounded below, then
Eq. (13) is true when qij (a) is uniformly bounded; 2) S − is empty if Uα∗ is nonnegative,
especially, if the reward function is nonnegative; 3) Uα∗ (i) is bounded below if α > 0
and the reward function is bounded below.

Continuous Time Markov Decision Processes

5

71

Optimality Equation

This section shall deal with the standard results 2) and 3) (see Section 1), that is, we
shall show the optimality equation and the optimality of policies achieving the optimality
equation for S0 -CTMDP, under the assumption that {qij (a)} is uniformly bounded, i.e.,
λ = sup{−qii (a)|i ∈ S, a ∈ A(i)} < ∞. For π∈Πm (s), t ≥ 0 and a finite function
u = (u(i)) on S, we define
Uα (π, t, u) =

t
0

exp(−αs)P (π, s)r(π, s)ds + exp(−αt)P (π, t)u

whenever the right hand side is well-defined. Denote Uα∗ (π, t) = Uα (π, t, Uα∗ ) for short,
which is well-defined by Theorem 3. Certainly, Uα∗ (π, t) is the expected discounted total
rewards if π is used in [0, t] and then an optimal policy is used from t.
LEMMA 5: Uα∗ = sup{Uα∗ (π, t)|π∈Πm (s)} for t ≥ 0, and Uα∗ (π, t) is nonincreasing
in t for any π∈Πm (s).
Now, we introduce our third condition.
CONDITION C: For each i ∈ S and a ∈ A(i), there is f and t > 0 such that f (i) = a
and Uα∗ (f, t, i) > −∞.
Remark 2: Two sufficient conditions for Condition C are as follows: 1) the conditions
given in Theorem 4, especially, when S − is finite or Uα∗ is bounded below (see Remark
1); 2) for each i ∈ S, A(i) can be reduced as
A (i) = {a ∈ A(i)|

sup

f ∈F :f (i)=a

Uα (f, i) > −∞},

which means that any action a ∈ A(i) should be eliminated if any stationary policy f
using it will have negative infinite objective value. In fact, if A(i) can be reduced as
A (i), then it follows Lemma 5 that Uα∗ ≥ Uα∗ (f, t) ≥ Uα (f ) > −∞ for each f ∈ F
and t > 0.
THEOREM 5: Under Condition C, Uα∗ satisfies the following optimality equation:
αUα∗ (i) = sup {r(i, a) +
a∈A(i)

qij (a)Uα∗ (j)},

i∈S.

(14)

j

The policy set is generalized here, but it is often our pleasure to restrict an (ε ≥ 0)
optimal policy to a smaller and simpler policy set. To do this, our first result is the
following theorem, which says that the optimality can be restricted to Πm , the set
of Markov policies, iff the optimal value function restricted to Πm also satisfies the
optimality Eq. (14). Let Uαm = sup{Uα (π)|π ∈ Πm }. We affirm that Uαm is finite.
In fact, if Uαm (i0 ) = −∞ for some i0 ∈ S, then it is easy to see from Eq. (5) that
Uα (π, i0 ) = −∞ for each π ∈ Πm (s). Thus Uα∗ (i0 ) = −∞, which is a contradiction.
But Uαm ≤ Uα∗ < ∞, so, Uαm is finite.
THEOREM 6: Uα∗ = Uαm iff Uαm is a solution of the optimality Eq. (14).
In order to obtain some properties for the optimality Eq. (14), we define a set, denoted
by W , of finite functions u = (u(i)) on S satisfying the following conditions: for each
π ∈ Πm (s) and i ∈ S, j Pij (π, t)u(j) < ∞ is well-defined for all t ≥ 0. Moreover,
∗
j Pij (π, t)u(j) > −∞ whenever
j Pij (π, t)Uα (j) > −∞ for each t ≥ 0 and
∗
i ∈ S. W is nonempty for Uα ∈ W . It is clear that Uα (π, t, u) < ∞ is well-defined for
each u ∈ W .

72

Q. Hu, J. Liu, and W. Yue

LEMMA 6: Suppose that ε ≥ 0, β + α ≥ 0, u ∈ W , π∈Πm and i ∈ S. If π and u
satisfy the following two conditions, then u(i)≤Uα (π, i) + (β + α)−1 ε.
αu≤r(π, t) + Q(π, t)u + exp(−βt)εe, a.e. t ≥ 0,
lim inf exp(−αt)
t→∞

Pij (π, t)u(j)≤0.

(15)
(16)

j

THEOREM 7: Suppose that u ∈ W is a solution of the optimality Eq. (14) and i ∈ S.
1) if for some β > −α and each ε > 0, there is a policy π∈Πm (s) with Uα (π, i) > −∞
satisfying Eq. (15) and Eq. (16), then u(i) ≤ Uα∗ (i);
2) if u satisfies the following (23) for each π ∈ Πm (s) with Uα (π, i) > −∞, then
u(i) ≥ Uα∗ (i),
lim sup exp(−αt)
t→∞

Pij (π, t)u(j) ≥ 0.

(17)

j

d
satisfying Eq. (15), but it may
It is clear that there is often a policy π = (ft ) ∈ Πm
be not true that Uα (π, i) > −∞. On the other hand, Uα∗ often satisfies Eq. (17) for
π ∈ Πm (s) with Uα (π, i) > −∞. In fact, by Eq. (6) we know that if Uα (π, i) > −∞,
then j Pij (π, t)Uα∗ (j) is also finite for each t ≥ 0, and

Pij (π, t)Uα∗ (j) ≥ lim sup exp(−αt)

lim sup exp(−αt)
t→∞

j

t→∞

Pij (π, t)Uα (π, t, j) = 0.
j

The following corollary can be proved easily by Theorem 7 and Lemma 6.
COROLLARY 3: Provided that Eq. (14) holds,
1) for any given f ∈ F , if f attains supremum of Eq. (14), f and Uα∗ satisfy Eq. (16)
and Uα (f ) > −∞, then f is optimal;
2) for some π ∗ ∈ Πm (s), if Uα (π ∗ ) is a solution Eq. (14), then π ∗ is optimal;
d
with Uα (π) > −∞, π and Uα∗
3) if for any ε > 0, there is a Markov policy π∈Πm
d
satisfy Eq. (15) and Eq. (16) for each i ∈ S, then Uα∗ = sup{Uα (π)|π∈Πm
};
∗
4) if α > 0, ε ≥ 0, f ∈ F attains the ε-supremum of Eq. (14), f and Uα satisfy Eq. (16),
Uα (f ) > −∞, then f is α−1 ε-optimal; moreover, if such f exists for each ε > 0,
then Uα∗ = sup{Uα (f )|f ∈F };
5) if Uα∗ ≤0, then Uα∗ is the largest solution of Eq. (14) in W satisfying conditions given
in 1) of Theorem 7;
6) Uα∗ is the smallest solution of Eq. (14) in W satisfying Eq. (17) for π ∈ Πm (s) and
i ∈ S with Uα (π, i) > −∞.
COROLLARY 4: For f ∈F and i∈S with Uα (f, i) > −∞,
and

αUα (f, i) = r(i, f ) +

j

qij (f )Uα (f, j) is finite

qij (f )Uα (f, j).

(18)

j

To conclude equations in this section, we discuss the CTMDP model (see Eq. (1))
restricted to Πsd (s), the set of piecewise semi-stationary policies. In this case, Theorem
2 is still true except that “ ≤ U (i)” should be deleted in Eq. (10) and A2 (i), defined by
Eq. (8), should be redefined by
A2 (i) = {a∈A1 (i)|there is f ∈F such that f (i) = a and Uα (f, i) > −∞}.

Continuous Time Markov Decision Processes

73

Thus, Condition C is trivial. By noting that Corollary 2 and 4 also hold for f . The
following theorem can be proved similarly as Theorem 5 and 6.
THEOREM 8: Restricted to Πsd (s), Uα∗d := sup{Uα (π)|π ∈ Πsd (s)} satisfies Eq. (14),
moreover, Uαs := sup{Uα (f )|f ∈ F } satisfies Eq. (14) iff Uα∗d = Uαs .

6

Conclusions

This paper discussed CTMDP with expected discounted total rewards under the necessary conditions that the model is well defined. We partitioned the state space into three
subsets, on which the optimal value is negative infinity, positive infinity and finite respectively. Thus the discussion on the CTMDP could be restricted in the sub-state space
with finite optimal value (we call it a sub-CTMDP). In fact, the reward rate function of
this sub-CTMDP is finite and is bounded above in the action. Finally, we showed, for
this sub-CTMDP, its optimality equation and the optimality of policies achieving the
optimality equation.
Further research may include if we can deal with the state partition and action
elimination directly on the optimality equation such that the optimality equation can
be obtained whenever it is well defined. Also, Condition C may be proved.

References
1. Kakumanu, P.V.: Continuous Time Markov Decision Models with Applications to Optimization Problems. Technical Report 63, Dept. of Oper. Res., Cornell Univ. (1969)
2. Lewis, M.E. and Puterman, M.L.: A Probabilistic Analysis of Bias Optimality in Unichain
Markov Decision Processes. IEEE Trans. on Autom. Contr. 46 (2001) 96–100
3. Lippman, S.A.: On Dynamic Programming with Unbounded Rewards, Mgt. Sci. 21 (1975)
1225–1233
4. Cassandras, C.G., Pepyne, D.L. and Wardi, Y.: Optimal control of A Class of Hybrid Systems.
IEEE Trans. on AC 46 (2001) 398–415
5. Song, J.: Continuous Time Markov Decision Processes with Nonuniformly Bounded Transition Rate Family, Scientia Sinica Series A, 11 (1988) 1281–1290
6. Hu, Q.: CTMDP and Its Relationship with DTMDP. Chinese Sci. Bull. 35 (1990) 710–714
7. Serfozo, R.F.: An Equivalence Between Continuous and Discrete Time Markov Decision
Processes, J. Oper. Res. 27 (1979) 60–70
8. Hou, B.: Continuous-time Markov Decision Processes Programming with Polynomical Reward, Thesis, Institute of Appl. Math. Academic Sinica, Bejing (1986).
9. Guo, X.P. and Zhu, W.P.: Denumerable-state Continuous-time Markov Decision Processes
with Unbounded Transition and Reward Rates under the Discounted Criterion. J. Appl. Prob.
39 (2002) 233–250
10. Guo, X.P. and Zhu, W.P.: Denumerable-state Continuous-time Markov Decision Processes
with Unbounded Cost and Transition Rates under Average Criterion. ANZIAM J. 43 (2002)
541–557
11. Hu, Q. and Xu, C.: The Finiteness of the Reward Function and the Optimal Value Function
in Markov Decision Processes. J. Math. Methods in Ope. Res. 49 (1999) 255–266
12. Chung, K.L.: Markov Chains with Stationary Transition Probabilities. Springer-Verlag (1960)
13. Kuczura, A.: Piecewise Markov Processes. SIAM J. Appl. Math. 24 (1973) 169–181

