Procedia Computer Science
Volume 80, 2016, Pages 1255–1266
ICCS 2016. The International Conference on Computational
Science

Variance-reduced HMM for Stochastic Slow-Fast Systems
Ward Melis1 and Giovanni Samaey1
Department of Computer Science, K.U. Leuven, Celestijnenlaan 200A, 3001 Leuven, Belgium
ward.melis@cs.kuleuven.be

Abstract
We propose a novel variance reduction strategy based on control variables for simulating the
averaged equation of a stochastic slow-fast system. In this system, we assume that the fast
equation is ergodic, implying the existence of an invariant measure, for every ﬁxed value of the
slow variable. The right hand side of the averaged equation contains an integral with respect
to this unknown invariant measure, which is approximated by the heterogeneous multiscale
method (HMM). The HMM method corresponds to a Markov chain Monte Carlo method in
which samples are generated by simulating the fast equation for a ﬁxed value of the slow
variable. As a consequence, the variance of the HMM estimator decays slowly. Here, we
introduce a variance-reduced HMM estimator based on control variables: from the current time
HMM estimation, we subtract a second HMM estimator at the previous time step using the same
seed as the current time HMM estimator. To avoid introducing a bias, we add the previously
calculated variance-reduced estimator. We analyze convergence of the proposed estimator and
apply it to a linear and nonlinear model.
Keywords: multiscale, stochastic, slow-fast, HMM, variance reduction, control variables

1

Introduction

Stochastic diﬀerential equations are ubiquitous in a multitude of real-life applications appearing in diﬀerent scientiﬁc domains, such as climate and environmental sciences [1], molecular
dynamics [2, 4] and bacterial chemotaxis [11]. Many of these applications contain processes
that inherently evolve over multiple time scales. One prototypical example system that was
proposed in [4] to analyze convergence behavior of novel multiscale methods is a singularly
perturbed slow-fast system in which the slow variable is described deterministically, while the
model for the fast variable contains stochastic eﬀects. The general form is as follows:
⎧
⎨dx(t) = f (x, y)dt,
x(0) = x0 ∈ R
1
1
(1)
⎩dy(t) = g(x, y)dt + √ β(x, y)dW (t), y(0) = y0 ∈ R,
ε
ε
where the scalar quantities x(t) : [0, T ] → R and y(t) : [0, T ] → R represent the slow and fast
evolving stochastic processes, respectively. The functions f (x, y), g(x, y) ∈ R are called the
Selection and peer-review under responsibility of the Scientiﬁc Programme Committee of ICCS 2016
c The Authors. Published by Elsevier B.V.

doi:10.1016/j.procs.2016.05.497

1255

Variance-reduced HMM for Stochastic Slow-Fast Systems

Melis,Samaey

drift functions and β(x, y) ∈ R is termed the diﬀusion function. This nomenclature originates
from the eﬀect of these terms in the corresponding Fokker-Planck equation. Furthermore,
W (t) ∈ R denotes a standard Brownian motion. The parameter ε
1 is a positive small-scale
parameter measuring the time scale separation between the fast and slow variable in system
¯ of the slow
(1). In addition, we assume that the fast dynamics is ergodic for every ﬁxed state x
variable, implying the existence and uniqueness of an invariant measure [10].
Often, one is only interested in the evolution of the slow variable of system (1) and not
in a detailed description of the fast variable. However, the fast dynamics cannot be omitted,
since the slow process explicitly depends on the fast variable. Due to the stiﬀness in system (1),
explicit simulation techniques such as the Euler-Maruyama or higher-order Milstein schemes are
computationally prohibitive. In the deterministic setting, implicit solvers such as the backward
Euler and trapezoidal methods are popular choices for stiﬀ systems allowing much larger time
steps than explicit methods. However, when dealing with systems with an invariant measure
diﬀerent from a Dirac distribution centered at the equilibrium state of the fast equation, it is
known that implicit methods fail to capture the correct invariant measure, thus introducing a
bias [7]. To avoid this, it is required to limit the time step of the implicit method by the time
scale separation, thus losing its computational advantage in the stochastic setting.
The diﬃculties related to direct simulation routines can be remedied by exploiting the time
scale separation of system (1): for ε → 0, the averaging principle [10] yields the following
reduced description for the slow variable of system (1):
dX
= F (X),
dt

F (X) =
Y

f (X, y)dμ∞
X (y).

(2)

in which μ∞
X (y) denotes the invariant measure induced by the fast dynamics of system (1)
keeping x = X ﬁxed. Equation (2) is known as the averaged, macroscopic or reduced equation
for the slow variable. The averaging principle transforms the original stochastic slow-fast system
into the deterministic problem (2) written in terms of the slow variable only. Given that
the latter is completely deterministic, equation (2) can be simulated using explicit integration
routines such as the forward Euler or higher-order Runge-Kutta methods. However, before these
schemes can be used, a procedure to calculate F (X) in equation (2) is needed. In general, the
invariant measure is not known explicitly. Then, one needs to resort to numerical methods that
approximate the integral in (2) without explicit knowledge of the invariant measure, such as the
heterogeneous multiscale method (HMM) [12] and the projective integration (PI) method [3].
In essence, the HMM method is a Markov chain Monte Carlo method. The variance of
the HMM estimator can be large since samples generated by the Markov chain are correlated.
Moreover, the variance decays as σf2 /M , with σf2 the variance of f , resulting in very slow
convergence. This can be improved by either increasing the sample size M , or by lowering
the variance σf2 . Since the former choice will lead to a much higher computational cost, we
examine the latter in this work. The variance reduction method we propose is based on a control
variables technique in which a control process is introduced that is strongly correlated with the
original HMM estimator. Consequently, by subtracting these two, corresponding variations
greatly diminish leading to a variance-reduced HMM estimator.
The remainder of this paper is structured as follows. In section 2, we introduce the stochastic
slow-fast systems that we intend to solve numerically. The HMM framework to eﬃciently
integrate these slow-fast systems is described in section 3. In that section, we also present the
variance-reduced HMM framework. Next, in section 4 we study the numerical properties of the
proposed variance reduction method. Numerical results are reported in section 5. We conclude
in section 6 with a brief discussion and ideas for future work.
1256

Variance-reduced HMM for Stochastic Slow-Fast Systems

2

Melis,Samaey

Slow-Fast System

The general form of slow-fast systems we consider is given in equation (1) and describes the
coupled evolution of a scalar slow variable x(t) using a deterministic model, and a scalar fast
variable y(t) evolving according to a stochastic model. We point out that the results obtained
in this work can easily be extended to cases with more fast or slow variables.
In what follows, we will always assume that the fast dynamics of system (1) is ergodic
for all ﬁxed values of the slow variable. Ergodicity implies that the statistical properties of
the ensemble of the stochastic process at a ﬁxed time instance and those of one realization of
the process over an inﬁnite time interval are the same. Consequently, for an ergodic process,
averaging a function with respect to the invariant measure μ∞
X (y) yields the same result as
averaging this function over one inﬁnitely long time path of the process:
F (X) =
Y

1
T →∞ T

f (X, y)dμ∞
X (y) = lim

T

f (X, y(t))dt.

(3)

0

This interchange between averages serves as a base for numerical methods avoiding explicit
knowledge of the invariant measure μ∞
X (y). Here, we consider the following linear and nonlinear
model problems.
Linear system.

In the linear setting, system (1) takes on the following form:
⎧
⎨dx(t) = λx(t) + py(t) dt
1
1
⎩dy(t) = qx(t) − Ay(t) dt + √ dW (t),
ε
ε

(4)

in which the parameters λ, p, q and A are all real scalars. In addition, to ensure that solutions
pq
decay exponentially with time, we require that λ < 0 and A ∈
,2 .
−λ
For this linear system, the invariant measure of the fast equation with respect to every ﬁxed
value X of the slow variable can be calculated analytically as:
2
μ∞
X (y) ∼ N (m∞ , σ∞ ),

m∞ =

q
X,
A

2
σ∞
=

1
,
2A

(5)

2
where m∞ and σ∞
denote the mean and variance of the invariant measure, respectively. Since
the invariant measure is known, the integral in equation (2) can also be calculated analytically,
yielding:
pq
dX
= λ+
X,
X(0) = x0 ,
(6)
dt
A

which is a linear ODE with exact solution X(t) = x0 exp((λ + pq/A)t).
Nonlinear system. As a second example, we consider the following nonlinear stochastic
multiscale system from [9]:
⎧
⎨dx(t) = − y(t) + y(t)2 dt
1
1
⎩dy(t) = − y(t) − x(t) dt + √ dW (t).
ε
ε

(7)

1257

Variance-reduced HMM for Stochastic Slow-Fast Systems

Melis,Samaey

In this case, the dynamics of the slow variable is nonlinear, while the fast variable is described
by a linear Ornstein-Uhlenbeck process. For this process, it is known that the invariant measure
2
μ∞
x (y) is a Gaussian with the following invariant mean m∞ and variance σ∞ :
m∞ = x,

2
σ∞
=

1
.
2

(8)

Since the invariant measure is known, the reduced equation for X can be calculated analytically
as:
dX
1
= − X + X2 +
,
X(0) = x0 ,
(9)
dt
2
which is a nonlinear ODE with solution X(t) = −0.5 − 0.5 tan (0.5t − arctan(2x0 + 1)).

3

Numerical Method

In this section, we construct a variance-reduced numerical scheme to solve the averaged equation
(2). Since this equation is deterministic, any stable explicit ODE solver can be used, such as
the forward Euler and Runge-Kutta methods. Here, we will employ the forward Euler (FE)
method. To that end, we discretize equation (2) on a uniform time mesh with time step Δt,
ˆ n . The forward Euler
and tn = nΔt. The numerical solution on this mesh is denoted by X
scheme is then given by,
ˆ n + ΔtFˆ (X n ),
ˆ n+1 = X
X

ˆ 0 = x0 .
X

(10)

In equation (10), the function F is replaced by an appropriate estimator Fˆ , since, in general,
the integral in equation (2) can not be calculated analytically. Therefore, we ﬁrst introduce
the HMM estimator in section 3.1. Then, we present a novel variance-reduced HMM estimator
based on control variables in section 3.2.

3.1

Heterogeneous Multiscale Method (HMM)

The heterogeneous multiscale method [12] bypasses explicit knowledge of the invariant measure
in equation (2) by exploiting the ergodicity property given in equation (3): F is calculated
by averaging over one inﬁnitely long time path of the fast process of system (1) while keeping
the value of the slow variable ﬁxed. As a result, the HMM estimator boils down to a Markov
chain Monte Carlo estimator: the integral in equation (2) is approximated by a Monte Carlo
method, in which the samples are not drawn from the (unknown) invariant measure, but are
instead generated from a Markov chain. This chain is obtained by simulating the fast equation
using the explicit Euler-Maruyama scheme, which is the stochastic counterpart of the forward
Euler scheme. In that regard, we discretize the fast equation on a uniform time mesh with
ˆ n of the slow variable, the numerical solution at time
time step δt. For a given ﬁxed value X
n,m
n,m
t
= nΔt + m δt is denoted by y
. The Euler-Maruyama scheme is given by [5],
y n,m +1 = y n,m +

δt ˆ n n,m
g(X , y
)+
ε

δt ˆ n n,m m
β(X , y
)ξn ,
ε

m = 0, ..., M − 1,

(11)

n M −1
in which (ξm
)m =0 is a set of mutually independent samples drawn from
√ the standard normal
distribution using a random number generator with seed ωn . The factor δt appears in the last
term of equation (11) due to the scaling property of Brownian motion. The initial condition of

1258

Variance-reduced HMM for Stochastic Slow-Fast Systems

Melis,Samaey

the Euler-Maruyama method is chosen as y 0,0 = y0 , and for all other n > 0 as y n,0 = y n−1,M −1 .
Then, the samples generated by the Markov chain (11) are approximately distributed according
to the desired invariant measure. Since it is more natural to label samples from 1 to M , we use
the trivial substitution m = m + 1 as sample index. The HMM estimator at time instance tn
is calculated as follows:
ˆ n ; ωn ) = 1
FˆHMM (X
M

M

ˆ n , y n,m ),
f (X

(12)

m=1

in which ωn represents the seed that is used in the random number generator. Since we can
only generate ﬁnite sample sizes M , the HMM estimator in equation (12) is a random variable.

3.2

Variance-reduced HMM

In this work, we propose a variance-reduced estimator based on the well-known control variables
technique, in which the variance of the estimator is reduced by introducing a related control
process. The expression of the variance-reduced HMM estimator at time instance tn using
control variables becomes:
¯ n ) = FˆHMM (X
¯ n ; ωn ) − FˆHMM (X
¯ n−1 ; ωn ) − F¯HMM (X
¯ n−1 ) ,
F¯HMM (X

(13)

¯ n ; ωn ) in
in which an overbar denotes a variance-reduced estimator. The ﬁrst term FˆHMM (X
equation (13) coincides with the classical HMM estimator without variance reduction for the
¯ n−1 ; ωn ) represents another HMM
¯ n using seed ωn . The second term FˆHMM (X
slow variable X
¯ n−1 at the previous time instance.
estimation without variance reduction for the slow variable X
¯ n−1 )
However, this term uses the exact same seed ωn as in the ﬁrst term. The last term F¯HMM (X
is the variance-reduced HMM estimator previously calculated.
The proposed estimator exploits information on the noise of the estimator at a previous time
¯ n−1 ; ωn )
instance to improve current time instance estimation. The diﬀerence between FˆHMM (X
n−1
¯
) between brackets in equation (13) corresponds to estimator noise. Since the
and F¯HMM (X
same seed ωn is used in the ﬁrst term and noise calculation at the previous time instance, we
can assume this noise resembles the HMM estimator noise at the current time instance.
The proposed technique can also be viewed from the following perspective. The ﬁrst and
second estimator terms in equation (13) are strongly correlated, as they employ the same seed
ωn . Consequently, the variations of both estimators will be similar. By subtracting these two
terms, we attempt to cancel corresponding variations. To avoid introducing a bias, subtracting
the previous variance-reduced estimator from the obtained diﬀerence is required. In conclusion,
the ﬁrst and second terms in equation (13) are strongly correlated to reduce the variance,
while the terms between brackets have the same expected value, so no bias is introduced in the
variance-reduced estimator. The variance reduction method is illustrated in ﬁgure 1.
The variance-reduced estimator is used in practice as described below. Given the initial
¯ 1 is calculated by taking
¯ 0 = x0 and y0 for the slow and fast variables, respectively, X
values X
one forward Euler step:
¯1 = X
¯ 0 + ΔtF¯HMM (X
¯ 0 ),
X
(14)
0
0
¯
¯
¯
where the initial calculation of FHMM (X ) is either the exact solution F (X ) or a more accurately estimated value of (12) achieved through using many more samples. Then, all other
values of the slow variable are obtained using the forward Euler scheme
¯ N + ΔtF¯HMM (X
¯ N ),
¯ N +1 = X
X

(15)

together with the variance-reduced version of the estimator given in (13).
1259

Variance-reduced HMM for Stochastic Slow-Fast Systems

Melis,Samaey

¯ n)
Fˆ (X

¯ n−1 )
Fˆ (X

¯ n)
F¯ (X

tn−1

tn

¯ n+1 )
Fˆ (X

tn+1

time

Figure 1: Sketch of the proposed variance reduction technique by control variables. Black arrows denote
the estimated value of F by HMM, while blue arrows depict the variance-reduced HMM estimator. The
red dashed arrows indicate that the noise – the diﬀerence between black and blue arrows – is similar
on consecutive time instances (color online).

4

Numerical Properties

In this paper, we only state the main results. A detailed analysis and proofs of the reported
results are postponed to a further publication [8]. The assumptions and approximations underpinning the variance-reduced HMM estimator lead to diﬀerent sources of error. First, when
exploiting the ergodicity property (3), we can only simulate the fast dynamics over a ﬁnite
time interval [0, τ ] with τ = M δt, which leads to a ﬁrst error. Second, we introduce an error
by replacing the (ﬁnite) time integral by a ﬁnite Riemann sum, in which the fast variable is
evaluated at discrete time instances. Third, since the exact solution of the fast equation is not
known explicitly, a time discretization method is used to approximate the solution of this equation, which leads to a discretization error. The ﬁrst and third error sources are deterministic
and belong to the class of systematic errors. The second error is stochastic, since we are taking
a ﬁnite sum of a stochastic process, thus residing in the class of statistical errors. A quantity
that captures the error of an estimator is the mean squared error (MSE), which satisﬁes the
following equality:
E |F¯HMM (x) − F (x)|2 = F˜HMM (x) − F (x)

2

+ E |F¯HMM (x) − F˜HMM (x)|2 ,

(16)

in which we used the following shorthand notation for the expected value of an estimator θ(x):
˜
θ(x)
= E[θ(x)].

(17)

Here, θ(x) is either the original or variance-reduced HMM estimator. The ﬁrst term on the right
hand side of equation (16) is the estimator bias squared, while the second term corresponds to
the estimator variance. Equation (16) allows for a clear distinction between the systematic and
statistical errors of the estimator.
Working out the recursion in equation (13), the variance-reduced HMM estimator at time
instance tN can be written as follows:
¯ N ) = FˆHMM (X
¯ N ; ωN ) +
F¯HMM (X

N −1

¯ n ; ωn ) − FˆHMM (X
¯ n ; ωn+1 )
FˆHMM (X

n=1

¯ 0 ; ω1 ) − F¯HMM (X
¯ 0) .
− FˆHMM (X

(18)

In the following two sections, we look at the convergence of the estimator and pathwise convergence.
1260

Variance-reduced HMM for Stochastic Slow-Fast Systems

4.1

Melis,Samaey

Estimator Convergence

First, we examine the bias of the variance-reduced HMM estimator at time instance tN . Therefore, we take the expectation of both sides of equation (18) over repeated experiments while
keeping M and δt ﬁxed:
¯ N )] = E[FˆHMM (X
¯ N ; ·)] − E[FˆHMM (X
¯ 0 ; ·)] − E[F¯HMM (X
¯ 0 )] .
E[F¯HMM (X

(19)

¯ N ) does
From equation (19), we observe that the variance-reduced HMM estimator F¯HMM (X
N
¯
ˆ
not introduce an additional bias compared to the original estimator FHMM (X ; ωN ) as soon as
¯ 0 ) is unbiased with respect to FˆHMM (X
¯ 0 ; ω0 ).
the initial variance-reduced estimator F¯HMM (X
Furthermore, in the limit of M, τ → ∞ and δt constant we obtain:
¯ N )] = Eμˆ∞ [f (X
¯ N , ·)] − Eμˆ∞ [f (X
¯ 0 , ·)] − Eμ¯∞ [f (X
¯ 0 , ·)] ,
lim E[F¯HMM (X
¯0
¯0
¯N

M →∞
M δt=τ

X

X

(20)

X

¯ N of the
in which μ
ˆ∞
¯ N denotes the invariant measure of the Markov chain with ﬁxed value X
X
∞
slow variable, and μ
¯X¯ 0 corresponds to the invariant measure of samples used in the initial
estimation.
Second, we study the statistical error which is described by the variance of the estimator:
¯ N )] = Var FˆHMM (X
¯ N ; ωN ) − FˆHMM (X
¯ N −1 ; ωN ) − F¯HMM (X
¯ N −1 )
Var[F¯HMM (X

.

(21)

By using Taylor series expansions, it can be shown that equation (21) is further calculated as:
⎞⎞⎞
⎛
⎛
⎤
⎡⎛
m−1
m−1
M
Δt
i
¯ N −1 )⎦ ,
¯ N ) ≈ Var ⎣⎝1 +
⎝BN
⎝fxm + fym
AjN ⎠⎠⎠ F¯HMM (X
Var F¯HMM (X
M m=1
i=1
j=i+1
(22)
in which we used the shorthand notation fxm and fym to denote the partial derivative of the
¯ N , y N,m ) and we introduced:
function f with respect to x and y, respectively, evaluated at (X
N
Am
N =1+

4.2

δt m
g +
ε y

δt m m
β · ξN ,
ε y

m
BN
=

δt m
g +
ε x

δt m m
β · ξN .
ε x

Pathwise Convergence

The variance-reduced HMM solution of the macroscopic equation is written as:
¯ N −1 + ΔtF¯HMM (X
¯ N −1 )
¯N = X
X
¯ 0 + Δt
=X

N −1

¯ n ).
F¯HMM (X

(23)

n=0

Using expression (18) followed by taking expectations of both sides of equation (23) we obtain:
¯ 0 + Δt E[F¯HMM (X
¯ 0 )] +
¯N] = X
E[X

N −1

¯ n ; ωn )]
E[FˆHMM (X

n=1

¯ 0 )] − E[FˆHMM (X
¯ 0 ; ω1 )] .
+ Δt(N − 1) E[F¯HMM (X

(24)
1261

Variance-reduced HMM for Stochastic Slow-Fast Systems

Melis,Samaey

Next, we examine how paths behave by looking at the error E N at time instance tN between
¯ N and the exact solution X(tN ). To show convergence
the variance-reduced HMM solution X
for a ﬁxed time instance tN = N Δt, we calculate an upper bound for the mean-squared error
which tends to 0 for Δt → 0. To that end, we introduce the forward Euler solution of the
averaged equation (2) at time instance tN using the exact expression of the function F , which
we denote by X N . This leads to:
¯ N − X(tN )|
E N = |X
¯ N − X N | + |X N − X(tN )|
≤ |X
≤ |¯
eN
HMM | + O(Δt).

(25)

The ﬁrst term corresponds to the variance-reduced HMM error, while the second term is a
classical result of a forward Euler discretization. The former can be further calculated as:
e¯N
HMM = Δt

N −1

¯ n )) + Δt
es (F¯HMM (X

n=0

N −1

¯ n )) + Δt
eb (F¯HMM (X

n=0

N −1

¯ n ) − F (X n ) . (26)
F (X

n=0

in which we introduced the following shorthand notation for the systematic and statistical error:
¯ n )) = F˜HMM (X
¯ n ) − F (X
¯ n ),
eb (F¯HMM (X

¯ n )) = F¯HMM (X
¯ n ) − F˜HMM (X
¯ n ). (27)
es (F¯HMM (X

To bound the error, we look at the expected value of the error squared. Using Lipschitzcontinuity of F with Lipschitz constant L and applying Gronwall’s inequality, the following
error bound is obtained [3]:
2
E |¯
eN
≤ 3T ΔtS N exp 3T 2 L2 ,
HMM |

(28)

in which we used N Δt = T and
SN =

N −1

¯ n ))
E eb (F¯HMM (X

2

¯ n ))
+ es (F¯HMM (X

2

.

(29)

n=0

This last bound at a ﬁxed time instance tN = N Δt indeed converges to 0 in the limit Δt → 0
which concludes the convergence of the method.

5

Numerical Results

We now apply the proposed variance reduction method to a linear and nonlinear model problem
in sections 5.1 and 5.2, respectively.

5.1

Linear System

Let us ﬁrst demonstrate the variance reduction technique when applied to the linear model
problem given in equation (4). We begin by applying the HMM technique without variance
reduction, in which we approximate the reduced evolution of the slow variable in equation (6)
which is in turn an approximation of the slow variable’s true evolution described in system (4).
ˆ 0 = x0 = 1 and y0 = 1. The
We compute the solution for t ∈ [0, 1] using initial conditions X
system parameters in equation (4) are as follows: λ = −10, p = 4, q = 0.5 and A = 1.
1262

Variance-reduced HMM for Stochastic Slow-Fast Systems

Melis,Samaey

In every iteration of the HMM method, we generate M = 50 samples by simulating the fast
equation of system (4), while keeping the current value of the slow variable ﬁxed. For stability,
the simulation time step δt in the Euler-Maruyama discretization is chosen as δt = ε with
ε = 10−3 . The forward Euler time step used in the discretization of the macroscopic equation
(2) is ﬁxed as Δt = 0.02. The evolution of the variables X and F and their variance are shown
by blue circles in ﬁgure 2. Clearly, the statistical error dominates, thus justifying the need for
variance reduction.
Before applying the variance reduction technique based on control variables introduced in
section 3.2, we ﬁrst specify how the function F is estimated in the initial forward Euler step
of the macroscopic equation shown in equation (14). If this ﬁrst estimation of F is performed
using its exact expression as calculated in equation (6), the variance-reduced HMM estimator
is completely variance-free, thus leading to a deterministic estimator. This result is brieﬂy
illustrated: in the second forward Euler step of the macroscopic equation, combining equation
(15) with (13), the variance of the variance-reduced estimator is obtained as follows:
¯ 1 )] = Var FˆHMM (X
¯ 1 ; ω1 ) − FˆHMM (X
¯ 0 ; ω1 ) − F (X
¯ 0)
Var[F¯HMM (X
=

p2
Var
M2

M

1
0
− ym
) .
(ym

(30)

m=1

¯ 0 ) is evaluated using its exact form, in equation (30) we used that X
¯ 0, X
¯ 1 and
Given that F (X
0
¯
F (X ) are all deterministic quantities. To calculate the sum in equation (30), we ﬁrst subtract
the equations of the two Markov chains, as given in equation (11), from each other resulting in:
δt
δt ¯ 1
¯ 0 ) + δt (ξ m − ξ m ).
(y11,m − y10,m ) + q(X
(31)
−X
1
ε
ε
ε 1
Since we are using the same Brownian path for both Markov chains, the stochastic part of
equation (31) cancels out exactly. Therefore, this diﬀerence between Markov chain generated
samples is completely deterministic and its variance is zero. This continues to hold for all
following forward Euler steps. The results of the variance-reduced estimator are shown in
ﬁgure 2. Green circles represent a simulation with exact initialization, while red and cyan
circles are used for simulations with an HMM estimated initial step using sample sizes of 103
and 104 , respectively, for which only the variance of X and F are shown for clarity.
y11,m+1 − y10,m+1 =

5.2

1−A

Nonlinear System

As a second model problem, we consider the nonlinear stochastic multiscale system given in
equation (7). We begin by applying the HMM procedure without variance reduction. We
ˆ 0 = x0 = y0 = 0.5. In the HMM
calculate the solution for t ∈ [0, 2] using initial conditions X
method, we use M = 50 samples by iterating over the Euler-Maruyama scheme for the fast
dynamics with time step δt = ε and ε = 10−3 . We ﬁx the forward Euler time step as Δt = 0.05.
The results are depicted by blue circles in ﬁgure 3. The HMM method now fails to capture
the correct solution path: a clear bias has developed for both X and F which reappears over
repeated experiments. This is attributed to the diﬀerence between the true measure μ∞
x (y) of
(y)
of
the
Markov
chain
which
possesses
the fast dynamics and the approximate measure μ
ˆ∞
x
2
the following invariant mean m
ˆ ∞ and variance σ
ˆ∞
:
m
ˆ ∞ = x,

2
σ
ˆ∞
=

1
2−

δt
ε

.

(32)

1263

Variance-reduced HMM for Stochastic Slow-Fast Systems
100

1
HMM
HMM + CV
Exact

0.8

10−5
10−10
Var[X]

X

0.6

Melis,Samaey

0.4

HMM
HMM + CV (exact)
HMM + CV (M = 103 )
HMM + CV (M = 104 )

10−15
10−20
10−25

0.2

10−30

0
0

0.2

0.4

0.6

0.8

10−35

1

0

0.2

0.4

2

100

0

10−5

−2

10−10

−4
−6
−8
−10

0.6

0.8

1

0.6

0.8

1

time

Var[F (X)]

F (X)

time

10−15
10−20
10−25
10−30

0

0.2

0.4

0.6
time

0.8

1

10−35

0

0.2

0.4
time

Figure 2: Left: X and F of the HMM method applied to the linear model problem (4) with and
without variance reduction (green and blue circles, respectively). The red line represents the exact
solution. Right: variance of X and F for HMM. Blue circles: no variance reduction; green circles:
variance reduction and exact ﬁrst step; red and cyan circles: variance reduction and estimated ﬁrst
step using M = 103 and M = 104 samples, respectively (color online).

Comparing this result with (8), we conclude that the measures μ∞
ˆ∞
x (y) and μ
x (y) have the same
invariant mean but diﬀerent invariant variance due to the Euler-Maruyama discretization of the
fast equation. Moreover, this diﬀerence disappears for δt → 0. However, when choosing δt = ε
as above, the HMM method yields an estimator for the macroscopic equation with right hand
side F (X) = −(X + X 2 + 1) instead of the desired equation given in (9).
To correct this diﬀerence in invariant measure, we supplement the HMM method with
the Metropolis-Hastings algorithm [6]. This algorithm introduces an accept-reject procedure
for every sample generated by the Markov chain such that the obtained ensemble follows the
desired invariant distribution. The results of the HMM method with the Metropolis-Hastings
algorithm are shown by cyan circles in ﬁgure 3. We observe that the bias has vanished and the
statistical error of the estimator has become the dominant error source.
Unfortunately, when combining the Metropolis-Hastings algorithm with the variance¯ n ; ωn )
reduced HMM estimator, we lose strong correlation between the HMM estimators FˆHMM (X
n−1
¯
ˆ
; ωn ) in equation (13) since samples can be rejected in diﬀerent places in
and FHMM (X
both ensembles. To resolve this, we instead use two classical HMM estimators without the
Metropolis-Hastings extension in equation (13), each producing a bias. However, since both
contain the same bias, subtraction yields a result of O(Δt). In the simulations, we used the
exact expression of F in the initial forward Euler step of the averaged equation. The results
are shown by green circles in ﬁgure 3. We remark that the variance buildup of the variancereduced estimator on the right hand side plots can be countered by occasionally reinitializing
the estimator.
1264

Variance-reduced HMM for Stochastic Slow-Fast Systems
10−1

0.3

10−2

0
Var[X]

−0.3
X

Melis,Samaey

−0.6
−0.9

HMM
HMM + MH
HMM + CV
Exact

−1.2
−1.5
0

0.2 0.4 0.6 0.8

10−3
10−4
10−5

HMM
HMM + MH
HMM + CV (exact)

10−6
1

1.2 1.4 1.6 1.8

10−7

2

0

0.2 0.4 0.6 0.8

time
0

Var[F (X)]

F (X)

1.2 1.4 1.6 1.8

2

1.2 1.4 1.6 1.8

2

100

−0.4
−0.8
−1.2
−1.6
−2

1
time

0

0.2 0.4 0.6 0.8

1
time

1.2 1.4 1.6 1.8

2

10−1
10−2
10−3
10−4

0

0.2 0.4 0.6 0.8

1
time

Figure 3: Left: X and F of the HMM method applied to the nonlinear model problem (7) with and
without variance reduction (green and blue circles, respectively). The red line represents the exact
solution. Right: variance of X and F for HMM. Blue circles: no variance reduction; green circles:
variance reduction and exact ﬁrst step; red and cyan circles: variance reduction and estimated ﬁrst
step using M = 103 and M = 104 samples, respectively (color online).

6

Conclusions

We proposed a novel variance reduction strategy based on control variables for simulating the
averaged equation of a stochastic slow-fast system. We analyzed convergence of the proposed
estimator and applied it to a linear and nonlinear model problem. The numerical tests showed
a signiﬁcant reduction of variance. For future work, it is interesting to apply the variance
reduction technique to metastable systems [1] and systems containing oscillatory functions.

References
[1] Maria Bruna, S. Jonathan Chapman, and Matthew J. Smith. Model reduction for slow-fast
stochastic systems with metastable behaviour. Journal of Chemical Physics, 140(17):1–23, 2014.
[2] Radek Erban, S. Jonathan Chapman, Ioannis G. Kevrekidis, and Tom´
aˇs Vejchodsk´
y. Analysis of
a stochastic chemical system close to a sniper bifurcation of its mean ﬁeld model. SIAM Journal
on Applied Mathematics, 70(3):984–1016, 2009.
[3] Dror Givon, Ioannis G. Kevrekidis, and Raz Kupferman. Strong convergence of projective integration schemes for singularly perturbed stochastic diﬀerential systems. Communications in
Mathematical Sciences, 4(4):707–729, 2006.
[4] Dror Givon, Raz Kupferman, and Andrew Stuart. Extracting macroscopic dynamics: model
problems and algorithms. Nonlinearity, 17(6):55–127, 2004.

1265

Variance-reduced HMM for Stochastic Slow-Fast Systems

Melis,Samaey

[5] Desmond J. Higham. An Algorithmic Introduction to Numerical Simulation of Stochastic Diﬀerential Equations. SIAM Review, 43(3):525–546, jan 2001.
[6] Tony Leli`evre, Gabriel Stoltz, and Mathias Rousset. Free energy computations: A mathematical
perspective. World Scientiﬁc, 2010.
[7] Tiejun Li, Assyr Abdulle, and Weinan E. Eﬀectiveness of implicit methods for stiﬀ stochastic
diﬀerential equations. Communications in Computational Physics, 3(2):295–307, 2008.
[8] Ward Melis and Giovanni Samaey. Variance-reduced HMM for stochastic slow-fast systems. In
preparation, 2015.
[9] Anastasia Papavasiliou and Ioannis G. Kevrekidis. Variance reduction for the equation-free simulation of multiscale stochastic systems. Multiscale Modelling and Simulation, 6(1):70–89, 2007.
[10] Grigorios A. Pavliotis and Andrew Stuart. Multiscale methods: averaging and homogenization.
Springer Science & Business Media, 2008.
[11] Mathias Rousset and Giovanni Samaey. Individual-Based Models for Bacterial Chemotaxis in the
Diﬀusion Asymptotics. Mathematical Models and Methods in Applied Sciences, 23(11):2005–2037,
2013.
[12] Eric Vanden-Eijnden. Numerical techniques for multi-scale dynamical systems with stochastic
eﬀects. Communications in Mathematical Sciences, 1(2):385–391, 2003.

1266

