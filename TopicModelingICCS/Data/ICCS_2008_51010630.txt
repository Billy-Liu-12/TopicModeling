Approximate Clustering of Noisy
Biomedical Data
Krzysztof Boryczko and Marcin Kurdziel
Institute of Computer Science, AGH University of Science and Technology,
al. Mickiewicza 30, 30–059 Krak´
ow, Poland
{boryczko,kurdziel}@agh.edu.pl

Abstract. Classical clustering algorithms often perform poorly on data
harboring background noise, i.e. large number of observations distributed
uniformly in the feature space. Here, we present a new density-based algorithm for approximate clustering of such noisy data. The algorithm
employs Shared Nearest Neighbor Graphs for estimating local data density and identiﬁcation of core points, which are assumed to indicate locations of clusters. Partitioning of core points into clusters is performed
by means of Mutual Nearest Neighbor distance measure. This similarity
measure is sensitive to changes in local data density, and is thus useful
for discovering clusters that diﬀer in this respect. Performance of the
presented algorithm was demonstrated on three data sets, two synthetic
and one real world. In all cases, meaningful clustering structures were
discovered.
Keywords: Cluster analysis, Noisy data, Multidimensional data, Shared
Nearest Neighbor Graph, Mutual Nearest Neighborhood.

1

Introduction

Formerly, research in cluster analysis focused on data sets were almost all observations are believed to be members of some clusters. Even if outlier observations
were accounted for, they were thought as exceptions rather than signiﬁcant fraction of the data set. In recent years however, eﬀorts were made to develop clustering techniques suitable for data sets were outlier observations are so frequent
that they in fact become a noisy background in which clusters are submerged. A
classical example is the DBSCAN algorithm [1], which employs a density-based
deﬁnition of clusters. Density-based notion of clusters was also adopted in [2].
Unlike DBSCAN, which relay on simple counting of points within spheres of some
given radius, this method employ Shared Nearest Neighbor (SNN) graphs for density estimation. Some approaches to noisy data clustering employ data sampling
instead of explicit density estimation. This is the case in CURE [3] algorithm
for example. Yet another approach to this task focus on graph-based cluster
connectivity measures. Typical representatives of this approach are Chameleon
[4] and ROCK [5] algorithms.
M. Bubak et al. (Eds.): ICCS 2008, Part I, LNCS 5101, pp. 630–640, 2008.
c Springer-Verlag Berlin Heidelberg 2008

Approximate Clustering of Noisy Biomedical Data

631

We present a new algorithm for clustering of high-dimensional, noisy data.
The algorithm, named Clustering With Nearest Neighborhood (CWNN), is inspired by ideas presented in [2], [6] and [7]. CWNN employs the SNN graph
to detect the so-called core data points. This allows for explicit handling of
background noise as well as automatic assessment of the number and shapes of
clusters. The strength of the our approach lies in a method for partitioning core
points into origins of clusters. We propose to partition the set of core points
by employing the Mutual Nearest Neighbor (MNN) distance measure computed
over the proximity measure derived from the SNN graph. Experimental results
illustrating performance of this method for data harboring background noise, including multidimensional cases, are demonstrated. It is important to note here,
that perfect discrimination between background noise and data clusters is often
unattainable, especially if they are of comparable densities. Therefore, CWNN
should be seen as an approximate clustering method.
1.1

Mutual Nearest Neighbor Distance Measure

Consider a set of points X = {x1 , x2 , . . . , xn } and a distance metric d(·, ·). For
example, this can be a ﬁnite subset of m-dimensional cube, X ⊂ −γ, γ m ⊂ Rm ,
and the Euclidean distance. Let NGH(xi ) be a list of neighbors of the point xi ,
sorted in an ascending order according to d(·, ·). Further, let Gk = {X, E} be
the k-Nearest Neighbor (k-NN) graph of X, i.e.:
(xi , xj ) ∈ E ⇔ xj ∈ {NGHl (xi ) : l = 1 . . . k}

(1)

where NGHl (xi ) is the l-th element of the list NGH(xi ).
The Mutual Nearest Neighbor distance measure, originally proposed in [7],
estimates proximity between a pair of points on the basis of their rankings in
mutual k-NN lists. In particular, for a pair of points xi , xj ∈ X, such that:
xi = NGHk (xj ) ∧ xj = NGHl (xi )

(2)

the value of the MNN distance measure is equal to:
MNN(X, xi , xj ) = k + l

(3)

For clustering purposes, the MNN distance measure has a strong advantage
over classical distance metrics, e.g. the Euclidean distance, of being more sensitive to changes in local data density [7]. This is illustrated on the example
depicted in Fig. 1. The points form two clusters of diﬀerent densities (marked
by C1 and C2 ). Suppose, that we would like to identify the clusters by simply comparing the distances between the points. A straightforward approach
would be to identify the connected components within the data set, assuming
that any two points are connected when the distance between them is smaller
than some threshold value ε. Using the Euclidean distance, it is impossible to
choose a proper threshold value ε. For ε ≤ 2 each point from the cluster C1
will be assigned to a separate, artiﬁcial cluster. On the other hand, for every

632

K. Boryczko and M. Kurdziel

ε > 2 the whole data set will be assigned to a single cluster. Now, consider
the MNN distance measure. For every two points from the cluster C1 that are
adjacent to each other and are placed in the same column (e.g., points a and
b in Fig. 1) or the same row (e.g., points b and c in Fig. 1) the value of the
MNN distance measure is equal to 2. The same situation occurs in the cluster
C2 . However, the value of MNN distance measure between the points x and y
is equal to: MNN(C1 ∪ C2 , x, y) = 4. We can clearly see this from the lists of
the nearest neighbors of those two points: NGH(x) = [{xn1 , xn2 , y}, yn2 , . . .] and
NGH(y) = [{yn1 , yn2 }, yn3 , {x, yn4 , yn5 }, . . .]. Consequently, this data set can be
properly clustered with the threshold value for the MNN distance measure ε = 3.

a

b

c

xn1

xn2

X

2

C2
Y
yn2

yn1

yn4

yn3

yn5

C1

2

1

Fig. 1. An example data set, made of two clusters that cannot be discovered using
only the Euclidean metric but can be found with the MNN distance measure

1.2

Estimating Proximity in Multidimensional Spaces with Sparse
Shared Nearest Neighbor Graphs

Euclidean metric is not suited for estimating proximity of points in highdimensional spaces (see e.g. [8]). A proximity measure that is better suited for
multidimensional data was proposed in [6]. In this paper, proximity between a
pair of points was deﬁned to be the number of neighbors they share. We employ
this idea in a slightly modiﬁed manner. Consider a k-NN graph Gk = (X, E) of
the input data set X. A graph Sk = (X, E, W ) in which weights given by:
wij = #{xs ∈ X \ {xi , xj } : (xi , xs ) ∈ E ∧ (xj , xs ) ∈ E}

(4)

are assigned to edges (xi , xj ) ∈ E, is called the Shared Nearest Neighbor graph
of X. Provided that the number of shared neighbors depends on how close the
points are (which is a reasonable assumption), their proximity can be deﬁned in
the following way:
(5)
dSk (xi , xj ) = k − wij
The measure dSk (xi , xj ) is well deﬁned only if both the edges (xi , xj ) and (xj , xi )
belong to Sk . If this is not the case, we consider the dSk (xi , xj ) to be inﬁnite.

Approximate Clustering of Noisy Biomedical Data

633

The SNN graph can be used to establish a strong neighborhood relationship
in X [2]. This is done by removing from Sk all edges (xi , xj ) ∈ E for which
wij < t, where t is a threshold value. In the resultant graph, denoted by Skt ,
edges connect strong neighbors. Relation deﬁned in this way has an advantage
of being relatively immune to background noise. In particular, if a suﬃciently
high threshold value t is chosen, the noise points that lie outside of high density
regions (i.e. clusters) will not have any strong neighbors.

2

Clustering Method Based on the SNN Graph and the
MNN Distance Measure

The pseudo-code of CWNN is presented as Algorithm 1. First, the SNN graph
Sk of the input data set is build and sparsiﬁed using a threshold value t. The
resultant sparse SNN graph Skt is used to construct the set of core points Xc ,
i.e. set of data points that have more than td strong neighbors within a sphere
of radius εn . It is assumed, that the core points span regions of data space
with relatively dense distribution of data points. In the next step, a graph Gc
of core points is created, in which points u, v ∈ Xc are connected if and only
if the SNN distance measure dSk (u, v) < ε and the MNN distance measure
MNNSk (Xc , u, v) < tm . Here, MNNSk denotes the MNN distance measure calculated over the proximity measure derived from the SNN graph Sk (eq. 5). To
locate origin of clusters, CWNN identiﬁes connected components in Sk . Finally,
CWNN evaluates each of the non-core data points xi and if d(xi , c) < εn , where
c ∈ Xc is the core point that is closest to xi , then the point xi is assigned to the
cluster represented by c. Otherwise, xi is assigned to the noise cluster CN .
Our approach employs the MNN distance measure for constructing connected
components in the graph of core points. To strengthen this measure against
background noise it is calculated over proximity measure derived from the SNN
graph. As the MNN distance measure is eﬀective in identifying local changes
in data density (see Section 1.1), gradients in the data density should split the
graph of core points into a number of connected components, each one with a
more uniform density distribution. Consequently, the clustering structure should
reveal more information about the analyzed data set. Number of clusters constructed by CWNN is equal to the number of connected components in the graph
of core points. This is controlled by two threshold values: tm for the MNN distance measure and ε for the SNN distance measure. CWNN do not assume any
particular geometry of the clusters. In principle, shapes of clusters depend only
on the shapes of connected components.
Number of points assigned to the noise cluster depends on two factors: the
number of core points identiﬁed by the algorithm and the threshold value εn .
The threshold εn speciﬁes the maximum distance d(·, ·) between a given point
xi and its nearest core point c, which still allows for assigning xi to the cluster
represented by c. The number of core points depends mainly on the distribution
of density within the data set. However, setting a broader initial neighborhood

634

K. Boryczko and M. Kurdziel

Algorithm 1. The CWNN algorithm
INPUT: set of points X = {x1 , x2 , . . . , xn }; distance metric d(·, ·)
PARAMETERS: k, t, td , tm , ε ∈ N+ ; εn ∈ R+
OUTPUT: set of data clusters Ω = {C1 , C2 , . . . , Ck }; noise cluster CN
Gk = build the k-NN graph of X
Sk = build the SNN graph from the graph Gk
Skt = sparsify the SNN graph Sk with the threshold t
Xc = ∅, Vc = ∅
for all xi ∈ X do
ρ = number of strong neighbors of the point xi that lie in a sphere of radius εn
if ρ > td then
Xc = Xc ∪ {xi }
end if
end for
for all (xi , xj ) ∈ Xc × Xc , i < j do
if [dSk (xi , xj ) < ε] & [MNNSk (Xc , xi , xj ] < tm ) then
Vc = Vc ∪ {(xi , xj ), (xj , xi )}
end if
end for
Ω = ﬁnd the connected components of the graph Gc = (Xc , Vc )
for all xi ∈ X \ Xc do
c = ﬁnd the point x ∈ Xc such that: ∀x ∈ Xc \ {x} : d(x, xi ) < d(x , xi )
if d(xi , c) < εn then
Cj = ﬁnd the cluster C ∈ Ω such that c ∈ C
Cj = Cj ∪ {xi }
else
CN = CN ∪ {xi }
end if
end for

(i.e., higher number of neighbors in the k-NN graph) or decreasing the number
of required strong neighbors, td , will increase the number of core points.
The computational complexity of the ﬁrst part of CWNN, i.e. identiﬁcation of
core points, is O(n2 log n + nk log k) due to construction of the k-NN graph and
counting of shared neighbors. The computational complexity of the remaining
part of the algorithm is O(m2 log m + m · n), where m is the number of core
points. However, the number of core points is smaller than the total number of
points: m ≤ n. Consequently, the computational complexity of the whole CWNN
algorithm is approximately O(n2 log n). The memory complexity is O(n2 ).

3

Experimental Results

Three data sets were used to demonstrate the eﬀectiveness of CWNN. The ﬁrst
one, further called Chameleon data set, was taken from [4]. The second one is
a synthetic three-dimensional test set, further called tube data set. Third test
set, i.e. microcalciﬁcation data set, consists of feature vectors constructed by the

Approximate Clustering of Noisy Biomedical Data

635

Table 1. Parameters of CWNN used for clustering of the test data sets
Parameter
k
t
td tm
ε
Chameleon data set
100 75 4 20 25
Tube data set
250 180 13 15 115
Microcalciﬁcation data set 1600 1100 275 1900 1050

εn
10.0
59.0
2.19

authors during work on an algorithm for detecting suspicious lesions in digital
mammograms [9]. Parameters used for clustering the test sets are given in Table 1. To set the value of these parameters we used the following heuristic. First,
we set the value for the number of neighbors in the k-NN graph. This graph
should reveal local properties of the data set. Therefore, we use a small fraction,
i.e. between 1% to 2%, of the total number of points for this parameter. The
upper value is used for multidimensional data. Next, we construct the histogram
of the number of shared neighbors in the k-NN graph and locate its maximum.
The ﬁrst minimum following the maximum corresponds to the number of shared
nearest neighbors above the most frequent one. We set the parameter t to a value
near this minimum, ensuring that strong neighborhood relationship connect only
the truly close points. In the next step, we construct the histogram for the number of strong neighbors. This histogram will usually have a peak corresponding
to the background noise followed by peaks corresponding to clusters. We set the
parameter td to a value after the peak from noisy background, ensuring correct
noise identiﬁcation. The value for εn is set by evaluating all non-noise points.
For each such point we calculate the distance to its furthest strong neighbor. The
histogram of these distances is used to identify most frequent values and εn is set
close to them. The parameters ε and tm are set by inspecting minimal spanning
trees of core points constructed using dSk (·, ·) and MNNSk (Xc , ·, ·) respectively.
Again, we construct histograms of edge lengths in these trees. The minima in
these histograms correspond to edges connecting clusters. First such minimum is
usually a good choice for the value of underlying parameter. Subsequent minima
can be used if a more coarse grained clustering is desirable.
Comparative tests. In [4] four two-dimensional data sets were used for evaluation of the Chameleon algorithm, three of which contain background noise.
We applied CWNN to these three noisy data sets and in each case were able to
obtain the correct clustering. For the lack of space we will report only the result
for the hardest case (in [4] it is called DS4 ).
The Chameleon data set is pictured in Fig. 2a. The clustering given by CWNN
is presented in Fig. 2b. As we can see, CWNN was able to remove the background
noise while preserving the bona ﬁde clusters. We should note here, that these
clusters diﬀer in densities. Furthermore, the density of the rectangular cluster
is near the density of the noise. In addition, clusters lay close to each other.
In particular, triangle-like clusters are nearly adjacent. Nevertheless CWNN did
succeed in clustering this data set. In comparison, according to [4], DBSCAN, a
well known density clustering method, is unable to identify the correct clustering

636

(a)

K. Boryczko and M. Kurdziel

(b)

Fig. 2. (a) Two-dimensional Chameleon data set. (b) Clustering of the Chameleon data
set obtained with CWNN algorithm.

in this data set. Another density clustering method, CURE, is also reported to
fail on this data. Additional results provided on a web page referenced in [4]
show that CLARANS [10], ROCK, and group average hierarchical clustering
are all unable to correctly cluster the Chameleon data set. The Chameleon algorithm itself managed to identify the genuine clusters in this data. However,
this algorithm has no explicit noise removal technique. Therefore, in addition to
the bona ﬁde clusters Chameleon constructed additional, spurious clusters out
of the background noise, which is evident on the Fig. 6 in [4].
Tube data set. The tube data set is pictured in Fig. 3a. It consists of two
cubical clusters, namely cluster A and cluster B, surrounded by a variable density
background noise. Density of data points in the clusters is ﬁve times greater than
the density of the surrounding noise. The density of the noise itself increases
linearly from the right to the left end of the tube, resulting in 4-fold diﬀerence
between the ends.
Result of clustering the tube data set is pictured in Fig. 3b. The two biggest
clusters found by CWNN are depicted in Fig. 3c. CWNN managed to identify
78.8% of noise points (i.e. 16,207 out of 20,560). From the 577 data points in
the cluster A, 516 were found (89.4%). In cluster B, out of the 1078 data points,
1075 points were found (99.7%). CWNN was therefore successful in discovering
both clusters. Some artiﬁcial clusters were created from the background noise,
near cluster B. This is a consequence of the noise density near left end of the
tube being comparable with the density of the cluster A. Therefore, assignment
of the whole noisy background to the noise cluster would result in lost of the
cluster A. Yet, cluster B was not merged with any of the artiﬁcial clusters (see
Fig. 3c). Gradient of the data density at the border of cluster B, that is well
preserved in the set of core points, increases the value of the MNN distance
measure between core points in the cluster and core points in the background.
This prohibits merging of artiﬁcial clusters with cluster B.
Microcalciﬁcation feature vectors data set. The microcalciﬁcation data set
contains feature vectors describing suspicious regions of interest (ROIs) found in
200 high-resolution digital mammograms from the DDSM database [11]. Analysis of such data sets is an important step in design and implementation of

Approximate Clustering of Noisy Biomedical Data

Cluster B
ClusterB

637

ClusterA
ClusterA

(a)

(b)

(c)

Fig. 3. (a) Three-dimensional tube data set containing two clusters enclosed by noisy
background. (b) Clustering of the tube data set obtained with CWNN algorithm. (c)
Two biggest clusters found by CWNN.

Cluster 1

Cluster 2

Cluster 3

Cluster 4

Cluster 5

Cluster 6

Fig. 4. Example mammogram regions of interest (ROIs), corresponding to feature vectors that belong to diﬀerent clusters discovered by CWNN

computer aided detection (CAD) systems. Moreover, CAD systems for screening mammography are among most heavily researched computerized detection
techniques, owing to diﬃculties in recognizing early cancer symptoms on mammogram images. Thus, microcalciﬁcation data set is an example of rather important class of biomedical data.
Each mammogram ROI is described by 27 pixel intensity features, such as:
entropy, contrast, moments of a brightness’ histogram, and other. As the algorithm used for initial selection of ROIs was tuned for high sensitivity, a large
number of false-positive detections was made. Feature vectors of false-positive
ROIs constitute the noise in the data set. Additional details are given in [9].
Out of approximately 93,300 data points in the microcalciﬁcation data set,
approximately 53,000 (i.e. 57%) were assigned to noise by CWNN. Six clusters
were constructed from the remaining data points. Example ROIs corresponding
to feature vectors selected randomly from the six discovered data clusters are
presented in Fig. 4. As it can be seen, clusters number 4, 5 and 6 contain ﬂat ROIs
characterized by low image contrast. The ROIs diﬀer in the average brightness.
Cluster no. 1 contains ROIs with linear structures, usually on a dark background.
The regions of interest from the cluster no. 3 contain similar structures but on a
brighter background. Finally, cluster no. 2 contains round, punctate occlusions
resembling small microcalciﬁcations.

638

4

K. Boryczko and M. Kurdziel

Implementation Notes

We have implemented parallel versions of the most costly routines in CWNN,
namely construction of the k-NN and sparse SNN graphs and calculation of
the MNN distance measure. Parallel version of these routines were designed for
shared memory machines and implemented using the OpenMP1 standard.
Parallelization of the k-NN graph construction is straightforward. In particular, the ﬁrst outer loop of this routine runs over all data points, for each one
calculating the distances to the points with greater indices. There is no data
dependencies between the iterations of this loop and thus they can be directly
split between the threads. In the next step, for each point the distances to the
remaining points are sorted in an ascending order. Loop performing this sorting
can also be parallelized by direct splitting between the threads.
The routine for constructing the sparse SNN graph contains two nested loops.
The outer loop runs over all data points. In the i-th iteration of the outer loop,
the inner loop runs over nearest neighbors y ∈ NGH(xi ) of the point xi , assessing
for each of them whether xi ∈ NGH(y) and counting shared neighbors. These
operations require read-only access to the k-NN graph and therefore do not
impose data dependencies. However, in rare cases threads can compete for write
access to the weights matrix of the SNN graph. We eﬃciently solved this issue
by employing a small hash table of locks that protects elements of the weights
matrix. This enables splitting of the outer loop between the threads.
8000

25

SGI Altix

SGI Altix

20

6000
5000

Speedup

Execution time [s]

7000

4000
3000
2000

15
10
5

1000
0

5

(a)

10
15
20
25
Number of processors

0

30

(b)

4

8

12
16
20
24
Number of processors

28

32

Fig. 5. The execution time (a) and speedup (b) of the parallel implementation of
CWNN algorithm. The results were obtained on the microcalciﬁcation data set.

Calculation of the MNN distance measure requires read-only access to the
weighted graph of core points. The edge weights are the distance measures derived from the SNN graph (see Sections 1.2 and 2). Write access is needed only for
the array storing the results. Therefore, we can employ a parallelization strategy
similar to the one used in distance calculation during construction of the k-NN
graph.
To illustrate the eﬃciency of the parallelization scheme, benchmark runs on
the microcalciﬁcation data set were made. The tests were carried out on the SGI
1

www.openmp.org

Approximate Clustering of Noisy Biomedical Data

639

Altix platform, equipped with 1.5 GHz Intel Itanium 2 processors and running
Linux operating system. The results are presented in Fig. 5. As we can see, the
algorithm scales almost linearly for the number of processors between 2 and 32.

5

Future Work

In the current setup, CWNN can be applied to various types of data provided
that a distance metric d(·, ·) is available for them. However, the performance of
the algorithm in such cases needs further evaluation, which will be the focus of
our future research. Another issue to be studied thoroughly are the methods for
estimating the initial values for the CWNN parameters. Although our experience
shows that with a help of simple heuristic reasonable values for these parameters can be established in few trial runs, an automatic method would make the
algorithm more user-friendly.
Acknowledgements. The authors are grateful to the Professor Witold Dzwinel
for his valuable comments. This work was partly founded by the Polish Committee for Scientiﬁc Research (KBN) grants no. 3T11F01030 and 3T11F01930.

References
1. Ester, M., Kriegel, H., Sander, J., Xu, X.: A density-based algorithm for discovering clusters in large spatial databases with noise. In: Proceedings of the 2nd
International Conference on Knowledge Discovery and Data Mining, pp. 226–231.
AAAI Press, USA (1996)
2. Ert¨
oz, L., Steinbach, M., Kumar, V.: Finding clusters of diﬀerent sizes, shapes,
and densities in noisy, high dimensional data. In: Proceedings of the Third SIAM
International Conference on Data Mining, San Francisco, CA, USA, vol. 47 (2003)
3. Guha, S., Rastogi, R., Shim, K.: Cure: an eﬃcient clustering algorithm for large
databases. In: Proceedings of the 1998 ACM SIGMOD international conference on
Management of data, pp. 73–84. ACM Press, New York (1998)
4. Karypis, G., Han, E., Kumar, V.: Chameleon: hierarchical clustering using dynamic
modeling. IEEE Computer 32(8), 68–75 (1999)
5. Guha, S., Rastogi, R., Shim, K.: Rock: A robust clustering algorithm for categorical
attributes. Information Systems 25(5), 345–366 (2000)
6. Jarvis, R., Patrick, E.: Clustering using a similarity measure based on shared near
neighbors. IEEE Transactions on Computers 22(11), 1025–1034 (1973)
7. Gowda, K., Krishna, G.: Agglomerative clustering using the concept of mutual
nearest neighborhood. Pattern Recognition 10, 105–112 (1978)
8. Aggarwal, C., Hinneburg, A., Keim, D.: On the surprising behaviour of distance
metrics in high dimensional space. In: Van den Bussche, J., Vianu, V. (eds.) ICDT
2001. LNCS, vol. 1973, pp. 420–434. Springer, Heidelberg (2000)
9. Boryczko, K., Kurdziel, M.: Recognition of subtle microcalciﬁcations in highresolution mammograms. In: Proceedings of 4th International Conference on Computer Recognition Systems, Advances in Soft Computing, pp. 485–492 (2005)

640

K. Boryczko and M. Kurdziel

10. Ng, R., Han, J.: Clarans: A method for clustering objects for spatial data mining.
IEEE Transactions on Knowledge and Data Engineering 14(5), 1003–1016 (2002)
11. Heath, M., Bowyer, K., Kopans, D., Kegelmeyer, W., Moore, R., Chang, K., Munishkumaran, S.: Current status of the digital database for screening mammography.
In: Digital Mammography, pp. 457–460. Kluwer Academic Publishers, Dordrecht
(1998)

