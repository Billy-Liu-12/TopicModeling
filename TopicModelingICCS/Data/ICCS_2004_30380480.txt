Automatic Parallel-Discrete Event Simulation
Mauricio Mar´ın
Centro de Estudios del Cuaternario CEQUA
Universidad de Magallanes
Punta Arenas, Chile
Mauricio.Marin@umag.cl

Abstract. This paper describes a software enviroment devised to support parallel and sequential discrete-event simulation. It provides assistance to the user in issues such as selection of the synchronization protocol to be used in the execution of the simulation model. The software
framework has been built upon the bulk-synchronous model of parallel
computing. The well-deﬁned structure of this model allowed us to predict
the running time cost of synchronization protocols in accordance with
the particular work-load generated by the simulation model. We exploit
this feature to automatically generate the simulation program.

1

Introduction

Discrete-event simulation is a widely used technique for the study of dynamic
systems which are too complex to be modelled realistically with analytical and
numerical methods. Amenable systems are those that can be represented as
a collection of state variables and whose values may change instantaneously
upon the occurrence of events in the simulation time. It is not diﬃcult to ﬁnd
real-life systems with associated simulation programs which are computationally
intensive enough to consider parallel computing, namely parallel discrete-event
simulation (PDES) [1], as the only feasible form of computation.
Over the last decade or so, parallel discrete event simulation (PDES) has
been intensively performed on traditionals models of parallel computation [1,7].
Paradoxically, these models have failed to make parallel computation a paradigm
of wide-spread use. Among other drawbacks, they lack realistic cost models for
predicting the performance of programs. As a result, existing simulation software
products have not been built upon frameworks which are able to predict performance and select accordingly the most eﬃcient algorithms for implementing
the parallel simulation program. Usually this kind of software either encapsulate
just one ﬁxed algorithm or leaves to the user the responsability of selecting one
from a set of alternative ones. Neither is convenient since it is known that a
particular algorithm is not eﬃcient for all work-load cases and the target user
not necessarily is an expert on eﬃcient algorithms for parallel simulation. This
has restrained potential users from using existing parallel simulation software.
Partially supported by Fondecyt project 1030454.
M. Bubak et al. (Eds.): ICCS 2004, LNCS 3038, pp. 480–487, 2004.
c Springer-Verlag Berlin Heidelberg 2004

Automatic Parallel-Discrete Event Simulation

481

In this paper we describe a simulation environment whose design is based on
the use of the bulk-synchronous model of parallel computing (BSP model) [8,
11]. The key point is that the model of computing provides a simple way to cost
parallel algorithms in their computation, communication and synchronization
components. This allows us to predict the performance of diﬀerent synchronization protocols in accordance with the particular features of the user deﬁned
simulation model. We use this facility to automatically select the synchronization
protocol that is best suited for the particular simulation.

2

Object-Oriented Approach to Modeling

We describe a simple general purpose modeling methodology (the world-view in
simulation parlance) that we have devised to facilitate the use of our system.
The world is seen as a collection of simulation objects that communicate
with each other via timestamped event-messages. Associated with each object
there is a global instance identiﬁer, called object-id, and a class identiﬁer, called
entity-id. There exists a simulation kernel that is responsible for eﬃciently delivering the messages in strict message timestamps chronological order. Each
simulation object inherits from a base class called Device that provides methods
for the interface with the kernel. In particular, the kernel delivers messages to
the simulation objects by executing the Device’s method cause with parameters
such as event-type, object-id, entity-id, and the simulation time at which the
event takes place in the target object.
For each simulation object, the user must provide an implementation of the
cause method so that events are handled in accordance with the behaviour deﬁned for the particular object. Nevertheless, we enable users to maintain a class
library for pre-deﬁned/common use simulation objects together with a graphical
representation for the user interface. Groups of classes can be tailored to speciﬁc
application domains. The entity-id parammeter allows the user to split the event
processing task into a set of private methods, each handling diﬀerent types of
events for a given type of entity.
In addition, simulation objects contain output channels that they use for
sending messages to other objects connected to those channels. A message is
sent out by executing the Device’s method schedule which takes parammeters
such as the channel id, time at which the event must take place in the target
object, and sender’s object-id and entity-id. At initialization time, all output
channels are connected to their respective target objects. Note that the cause
method could work on input channels as well. However, we have not seen a need
for including in our world-view the notion of input channels yet. In fact, the
combination object-id/entity-id appears to be very ﬂexible as it allows objects
to receive messages from multiple sources without complicating too much the
initialization process.
On the other hand, the notion of output channels makes it easier to the
programmer to work on generic implementations of the object without worrying
about the speciﬁc object connected to the output channel. Actually, all this is

482

M. Mar´ın

Fig. 1.

a tradeoﬀ between generality and simpliﬁcation of the initialization process and
its implicancies in code debuging and maintenance. As an alternative to output
channels, we also support the concept of associative arrays (e.g., C++ maps)
that are used by schedule to go from global object ids to pointers to the actual
simulation objects for direct message delivering.
Currently, we have C++ and Java implementations of this approach.
The graphical user interface is designed to ease the burden of deploying
thousands of simulation objects and deﬁning their communication relations. The
model deﬁnition process starts up with the creation of a project and drawing of
a few objects. Each object is an instance of a given class which can be edited
and/or viewed by using a class editor as shown in ﬁgure 1. More objects of a
given class can be deployed by duplicating existing objects. The user interface
allows the deﬁnition and/or edition of diﬀerent properties associated with the
objects as well as the automatic generation of the simulation program.
The architecture of the simulation framework is as follows. On the upper
layer we have the user interface which allows the simulation model to be deﬁned
in a graphical manner as well as the codiﬁcation of the classes that deﬁne the
behavior of the respective simulation objects. The second layer takes the user
deﬁnitions and generates a speciﬁcation of the model written in a mid-level language. The third layer is in charge of selecting the synchronization protocol that
happears to be most suitable for the simulation model. This is eﬀected by directly

Automatic Parallel-Discrete Event Simulation

483

Fig. 2. Software Architecture

analyzing the mid-level deﬁnitions. The ﬁrst step is to perform a pre-simulation
of the model. The result is a set of parameters that predicts the model behavior.
These parameters are then input to a formula that predicts whether it is better
to simulate the model in parallel or just sequentially in the target parallel computer. The last step is to generate the simulation program by linking the selected
synchronization protocol with the simulation objects deﬁned by the user. Finally
a C++ or java compiler is used to produce the executable simulation program.
Figure 2 shows the relationships among the main components of the simulation environment. Note that users who are not familiar with C++/Java programming can proﬁt from the class library that contains deﬁnitions for most
typical objects in, for example, queuing systems. The library can be increased
by directly including new user-deﬁned class deﬁnitions.
Usually a parallel simulation will have to deal with thousands of simulation
objects. Thus the mid-level languaje only speciﬁes general information about the
objects such as the class they belong to and their communication relations. The
instances themselves are stored in a symbolic manner into a ﬁle to be actually
created later at simulation running time.
The methodology used to automatically generate the simulation program can
be divided into the following major steps (see ﬁgure 2):
(i) Pre-simulation of the simulation model deﬁned by the user. This uses
information of the communication topology among objects deﬁned by the user
and the details about what random number generators are used to send the
messages among them (simulation time of those event messages). The results

484

M. Mar´ın

of the pre-simulation are used to determine a tuple which describes the overall
behaviour of the model in terms of the amount of computation, communication
and synchronization it demands to the BSP computer per unit simulation time
(see next section).
(ii) The tuple obtained in the previous step is plugged into a formula that
predicts the feasible speedup to be achieved by the simulation on the target BSP
computer (next section). The outcome can be a recomendation to simulate the
model just sequentially because the predicted speedup is too modest or even less
than one. The eﬀect of the particular parallel computer hardware is included in
the BSP parameters G and L obtained for the machine via proper benchmarks
(next section).
(iii) In the case that the recomendation is a parallel simulation, the tuple
is now plugged into a set of formulas that predict the running times of a set of
synchronization protocols available in our system for parallel simulation. These
protocols are optimistic and conservative ones and for each of them it is necessary
to include new deﬁnitions into the simulation model. Conservative protocols
need the so-called lookhaead information whereas the optimistic one requires
the speciﬁcation of what are the states variables that need to be handled by
rollbacks.
(iv) The simulation program is generated by putting together objects and
synchronization protocol. During simulation the synchronization protocols have
the avility of adapting themselves to changes in the work-load evolution. Those
protocols also implement a dynamic load balancing strategy we devised to
re-distribute objects onto processors in order to reduce running times (ﬁgure
2). The initial mapping of objects onto the processors is uniformly at random.
During running time a dynamic load balancing algorithm is executed to correct
observed imbalance.

3

Technical Details

In the BSP model of computing both computation and communication take
place in bulk before the next point of global synchronization of processors. Any
parallel computer is seen as composed of a set of P processor-local-memory
components which communicate with each other through messages. The
computation is organised as a sequence of supersteps. In each superstep, the
processors may perform sequential computations on local data and/or send
messages to other processors. The messages are available for processing at their
destinations by the next superstep, and each superstep is ended with the barrier
synchronisation of the processors.
The total running time cost of a BSP program is the cumulative sum of the
costs of its supersteps, and the cost of each superstep is the sum of three quantities: w, h g and l, where (i) w is the maximum of the computations performed by
each processor, (ii) h is the maximum number of words transmitted in messages
sent/received by each processor with each one-word-transmission costing g units
of running time, and (iii) l is the cost of barrier synchronising the processors.

Automatic Parallel-Discrete Event Simulation

485

The eﬀect of the computer architecture is costed by the parameters g and l,
which are increasing functions of P . These values can be empirically determined
by executing benchmark programs on the target machine [8].
We use the above method to cost BSP computations to compute the speedup
Sup under a demanding case for the underlying parallel algorithm. In [4] we
derived the following speed-up expression,
Sup =

1
PB (1 + PM /r) + z PB PM ge + PS le

with P1 ≤ PB ≤ 1, 0 ≤ PM ≤ 1, 0 ≤ PS ≤ 1, r ≥ 1, and
z ≥ 1. The parameter PS is a measure of slackness since 1/PS =
number of simulated events per superstep. The parameter PM accounts for locality as it is the average fraction of simulated events that results in message
transmitions. The parameter PB accounts for load balance of the event processing task. The size of messages is represented by z. In addition, r ≥ 1 is the
event granularity deﬁned with respect to Ce which is the lowest (feasible) cost
of processing an event in the target machine. Finally, ge and le are deﬁned as
ge = g/(r Ce ) and le = l/(r Ce ) for the BSP parameters g and l respectively.
In this way simulation models can be represented by an instance of the tuple
(PB , PS , PM , r, z).
The most popular synchronisation protocols [2,5,6,10] base their operation
on one of two strategies of simulation time advance. Synchronous time advance
(SYNC) protocols deﬁne a global time window to determine the events allowed
to take place in each iteration (superstep). The SYNC protocol advances its time
window forward in each iteration to let more events be processed. On the other
hand, asynchronous time advance (ASYNC) protocols implicitly deﬁne windows
which are local to the simulation objects. Figure 3 describes sequential simulation
algoritms which predict the supersteps executed by each protocol (SYNC and
ASYNC).
The comparative cost of synchronization protocols is calculated as follows.
Let Sps and Spa be the number of supersteps per unit simulation time required
by synchronous time advance (SYNC) or asynchronous time advance (ASYNC)
respectively. We measure load balance (at superstep level) in terms of the event
eﬃciency Ef as follows. If a total of Me
N events are processed during
Me
1
the complete simulation with P processors, then Ef = SumMaxEv
P where
SumMaxEv is the sum over all supersteps of the maximum number of events
processed by any processor during each superstep (i.e., the cumulative sum of
the maximum in each superstep). Both Sp and Ef can be determined empirically
for the simulation model at hand by employing the algorithms shown in ﬁgure 3.
The protocols in our system are optimized BSP realizations of YAWNS [6],
BTB [9], CMB-NM (null messages) [5] and TW [2]. YAWNS and BTB are SYNC
protocols whereas CMB-NM and TW are ASYNC protocols.
In the optimistic protocols we increase the cost of processing each event in
ϕ ≥ 1 units in order to include the eﬀect of state saving. Roll-backs cause resimulation of events thus we consider that this operation increases the total

486

M. Mar´ın

ASYNC
SYNC
Generate N initial pending events;
Generate N initial pending events;
[e.s indicates the minimal superstep at
[ e is an event with time e.t ]
which the event e may take place in
TZ := ∞; [ event horizon time ]
processor e.p.]
SZ ← Φ; [ buﬀer ]
loop
loop
e := NextEvent();
if TimeNextEvent() > TZ then
p := e.p ; [ e occurs in processor p ]
SStep := SStep + 1;
if e.s > SStep[p] then
Schedule(SZ );
SStep[p] := e.s;
TZ := ∞;
endif
SZ ← Φ;
e.t := e.t + TimeIncrement();
endif
e.p := SelectProcessor();
e := NextEvent();
if p = e.p then
e.t := e.t + TimeIncrement();
e.s := SStep[p];
p := e.p ; [ e occurs in processor p ]
else
e.p := SelectProcessor();
e.s := SStep[p] + 1;
if e.p = p then
endif
SZ ← SZ ∪ {e};
Schedule(e);
TZ := MinTime(SZ );
endloop
else
Schedule(e);
The total number of supersteps is the
endif
maximum of the P values in array SStep.
endloop
Fig. 3.

number of simulated events by a factor of φ events with φ ≥ 1. In the asynchronous protocol roll-backs also increase the message traﬃc. The conservative
protocols do not have these overheads thus we set ϕ = 1 and φ = 1. Synchronous
time advance protocols (SYNC) require a min-reduction operation with cost RD
for each event processing superstep.
Deﬁning N and Nm as the number of simulated events and sent messages per
unit simulation respectively on a P -processors BSP computer,
the total cost of
s s
a SYNC protocol (YAWNS and BTB) is given by SYNC = ϕ Eφs Pr N + ENsmP g +
f

f

Sps l + Sps RD , whereas the total BSP cost of the ASYNC protocol (TW and
a
a a
Nm
CMB-NM) is given by ASYNC = ϕ Eφa Pr N + (2 φE−1)
g + c Spa l . where c ≥ 1
aP
f

f

is a factor that signal the average increase of supersteps in CMB-NM. The determination of the synchronization protocol to be suggested to the user takes into
account the following cases. Conservative protocols (YAWNS, CMB-NM) have
higher priority than the optimistic ones (BTB, TW). For the case in which the
observed fan-in/fan-out of the communication topology among the simulation
objects is large enough, the CMB-NM is discarded since this protocol loses eﬃciency dramatically. Also for the cases in which the pre-simulation did not ﬁnd a
suﬃcient amount of “lookahead” in the built-in random number generators for
timestamps increments, the YAWNS and CMB-NM protocols are discarded. On
the other hand, the cost of state saving for the optimistic protocols depends on

Automatic Parallel-Discrete Event Simulation

487

the size of data to be periodically saved. Also roll-backs in BTB are on everage
20% lower than in TW. Thus the best choise is a tradeoﬀ.

4

Conclusions

We have described the overall design of a simulation framework we have developed to support parallel discrete event simulation. The main objective was to
assist users on the complexity associated with the selection of a proper synchronization protocol to conduct the simulation of the user-deﬁned model.
A pre-simulation of the user model produces information about what synchronization protocol is best suited for the execution of the model. Those protocols
are devised upon a model of computing that provides both independence of the
architecture of the parallel computer and a method for determining the cost of
parallel algorithms.
We have tested the suitability of our system using several simulations models.
Those include the synthetic work-load PHold, Wind energy electricity generation systems, hard-particles models, Web crawlers, and a large toriodal queuing
network. In all cases, specially in regular systems, we have observed that our
prediction methodology is very eﬀective in practice (this claim is supported by
the results in [3,4]).

References
1. R.M. Fujimoto. Parallel discrete event simulation. Comm. ACM, 33(10):30–53,
Oct. 1990.
2. D.R. Jeﬀerson. Virtual time. ACM Trans. Prog. Lang. and Syst., 7(3):404–425,
July 1985.
3. M. Mar´ın. Asynchronous (time-warp) versus synchronous (event-horizon) simulation time advance in bsp. In Euro-Par’98 (Workshop on Theory and Algorithms
for Parallel Computation, pages 897–905, Sept. 1998. LNCS 1470.
4. M. Mar´ın. Towards automated performance prediction in bulk-synchronous parallel discrete-event simulation. In XIX International Conference of the Chilean
Computer Science Society, pages 112–118. (IEEE-CS Press), Nov. 1999.
5. J. Misra. Distributed discrete-event simulation. Computing Surveys, 18(1):39–65,
March 1986.
6. D.M. Nicol. The cost of conservative synchronization in parallel discrete event
simulations. Journal of the ACM, 40(2):304–333, April 1993.
7. D.M. Nicol and R. Fujimoto. Parallel simulation today. Annals of Operations
Research, 53:249–285, 1994.
8. D.B. Skillicorn, J.M.D. Hill, and W.F. McColl. Questions and answers about BSP.
Journal of Scientiﬁc Programming, V.6 N.3, 1997.
9. J.S. Steinman. Speedes: A multiple-synchronization environment for parallel discrete event simulation. International Journal in Computer Simulation, 2(3):251–
286, 1992.
10. J.S. Steinman. Discrete-event simulation and the event-horizon. In 8th Workshop
on Parallel and Distributed Simulation (PADS’94), pages 39–49, 1994.
11. L.G. Valiant. A bridging model for parallel computation. Comm. ACM, 33:103–
111, Aug. 1990.

