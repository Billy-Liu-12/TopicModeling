Procedia Computer Science
Volume 80, 2016, Pages 222–232
ICCS 2016. The International Conference on Computational
Science

Induced Dimension Reduction method for solving
linear matrix equations
Reinaldo Astudillo and Martin B. van Gijzen
Delft University of Technology, Delft Institute of Applied Mathematics, The Netherlands
R.A.Astudillo@tudelft.nl and M.B.vanGijzen@tudelft.nl

Abstract
This paper discusses the solution of large-scale linear matrix equations using the Induced Dimension reduction method (IDR(s)). IDR(s) was originally presented to solve system of linear
equations, and is based on the IDR(s) theorem. We generalize the IDR(s) theorem to solve
linear problems in any ﬁnite-dimensional space. This generalization allows us to develop IDR(s)
algorithms to approximate the solution of linear matrix equations. The IDR(s) method presented here has two main advantages; ﬁrstly, it does not require the computation of inverses of
any matrix, and secondly, it allows incorporation of preconditioners. Additionally, we present a
simple preconditioner to solve the Sylvester equation based on a ﬁxed point iteration. Several
numerical examples illustrate the performance of IDR(s) for solving linear matrix equations.
We also present the software implementation.
Keywords: Matrix linear equations, Krylov subspace methods, Induced Dimension Reduction method,
Preconditioner, Numerical software.

1

Introduction

In this work we extended the Induced Reduction Dimension method (IDR(s) [13]) to approximate the solution of linear matrix equations,
k

Aj XBjT = C,

(1)

j=1

where the A1 , A2 , . . . , Ak are in Cn×n , B1 , B2 , . . . , Bk are in Cm×m , C ∈ Cn×m , and X ∈ Cn×m
is unknown. Solving equation (1) is equivalent to solve a linear system of equations. Deﬁning
vec(X) as the vector of order n × m created by stacking the columns of the matrix X, we can
write (1) as,
⎞
⎛
⎝

k

Bk ⊗ Ak ⎠ vec(X) = vec(C).

(2)

j=1

222

Selection and peer-review under responsibility of the Scientiﬁc Programme Committee of ICCS 2016
c The Authors. Published by Elsevier B.V.

doi:10.1016/j.procs.2016.05.313

IDR(s) for solving linear matrix equations

Astudillo and Van Gijzen

Throughout this document, we only consider the case when the coeﬃcient matrix of the system
of linear equations (2) is non-singular, i.e., Eq. (1) has guaranteed the existence and uniqueness
of their solution. For the general case of (1) the conditions to ensure existence and uniqueness
of its solution are not fully established. However, in the cases of the Sylvester and Lyapunov
equation, the conditions for existence and uniqueness of their solution are known. For the
Sylvester equation,
AX + XB = C,
(3)
the condition for the existence and uniqueness of the solution is that the matrices A and −B
do not have any common eigenvalue. The Lyapunov equation,
AX + XAT = C,

(4)

has a unique solution when the eigenvalues of A hold that λi + λj = 0 for 1 ≤ i, j ≤ n.
Linear matrix equations appear in diﬀerent areas such as complex networks, and system and
control theory (see [10] and its references). Another important source of linear matrix equations
is the numerical solution of diﬀerential equations. Discretization of diﬀerential equations lead
to linear systems or parameterized linear systems, and in some cases, they can be rewritten as
a Sylvester equations. In this work, we emphasize this kind of examples. We present numerical
tests of Sylvester equations originated from the discretization of time-dependent linear systems,
and convection-diﬀusion equations.
In this work, we propose a variant of Induced Dimension Reduction method (IDR(s)) for
solving linear matrix equations. IDR(s) was originally proposed in [13] as an iterative method
to solve large, sparse and non-symmetric system of linear equations
Ax = b,

(5)

where A ∈ Cn×n is the coeﬃcient matrix, b is the right-hand side vector in Cn , and x ∈ Cn
is unknown. IDR(s) has been adapted to solve other related problems like solving block linear
systems [5], multi-shift linear systems [15, 2], and eigenvalue problems [6, 1]. IDR(s) is based on
the IDR(s) theorem. In this paper, we generalize the IDR(s) theorem to solve linear problems
in any ﬁnite-dimensional space. Using this generalization, we develop an IDR(s) algorithm to
approximate the solution of linear matrix equations.

1.1

Notation

We use the following notation: column vectors are represented by bold-face, lower case letters
and capital letters denote matrices. For a matrix A, AT represents its transpose and the irepresents the
th column is denoted by Ai . Greek lower case letters represent scalars.
is
the
Frobenius
norm
induced
by
the
Frobenius
inner
classical Euclidean norm, and
F
product A, B F = trace(AT B). Subspaces are denoted by uppercase calligraphic letters with
the exception of A, I, and M that represent linear operators. In is the identity matrix of order
n, and wherever the context is clear the subindex n is eliminated.

2

IDR(s) for linear operators

This section extends the IDR(s) method to solve linear matrix equations of the form (1). We
present an alternative form of the IDR(s) theorem. First, we would like to draw the attention
of the reader to the proof of Theorem 2.1 in [13]. In this proof, the authors only use that Cn
is a linear subspace, and that A is a linear operator on this linear subspace. Using these facts,
223

IDR(s) for solving linear matrix equations

Astudillo and Van Gijzen

we can generalize the IDR(s) theorem to any ﬁnite-dimensional linear subspace D with A as
linear operator deﬁned on the same linear subspace. Corollary 1.1 summarizes this result.
Corollary 1.1. Let A be any linear operator over a ﬁnite dimensional subspace D and I the
identity operator over the same subspace. Let S be any (proper) subspace of D. Deﬁne G0 ≡ D,
if S and G0 do not share a nontrivial invariant subspace of the operator A, then the sequence
of subspace Gj , deﬁned as
Gj ≡ (I − ωj A)(Gj−1 ∩ S)

j = 0, 1, 2 . . . ,

with ωj ’s nonzero scalars, have the following properties,
1. Gj+1 ⊂ Gj , for j ≥ 0 and
2. dimension(Gj+1 ) < dimension(Gj ) unless Gj = {0}.
Proof. The proof is analogous to the one presented in [13].
In [5], Du et al. present another generalization of the original IDR(s) theorem, this generalization is used to derive an IDR(s) method for solving block systems of linear equations.
Corollary 1.1 has a broader scope; we apply this corollary to solve diﬀerent types of linear
matrix equations.
As in [7], we rewrite problems (1) as
A(X) = C,

(6)

k

where A(X) =
j=1 Aj XBj . Using Corollary 1.1, we are able to create residuals Rk =
C − A(Xk ) of the problem (6) in the shrinking and nested subspaces Gj and obtain the approximations Xk . Only changing the deﬁnition of the operator A and the subspace D, we are able
to approximate the solution of the linear matrix equation using IDR(s). Assuming that the
space S is the null space of the set P = {P1 , P2 , . . . , Ps }, and the approximations {Xi }ki=k−s ,
with their respective residuals {Ri }ki=k−s belonging to Gj , IDR(s) creates Rk+1 in Gj+1 and the
approximation Xk+1 using the recursions
s

Xk+1 = Xk + ωj+1 Vk +

γi Uk−i ,
i=1

Rk+1 = Vk − ωj+1 A(Vk ),

and

s

Vk = Rk −

γi Gk−i ,
i=1

where {Gi }ki=k−s ∈ Gj and Uk−i = A(Gk−i ). The coeﬃcient {γj }sj=1 are obtained by imposing
the condition that
Vk ⊥ P,
which is equivalent to solving the s × s system of linear equations,
Mc = f,
where Mi,j = Pi , Gk−s+(j−1)
224

F

and fi = Pi , Rk

F.

IDR(s) for solving linear matrix equations

Astudillo and Van Gijzen

Using the fact that Gj+1 ⊂ Gj , IDR(s) repeats the calculation above to generate s + 1
residuals in Gj+1 with their corresponding approximations. Then, it is possible to create new
residuals in the subsequent space Gj+2 . The parameter ωj might be chosen freely for the ﬁrst
residual in Gj , but the same value should be used for the next residuals in the same space.
There exist diﬀerent options to select the parameter ωj , see for example [12], [16], and [11].
Equivalent to [13], we can select directly Gi = −(Ri − Ri−1 ) and Ui = Xi − Xi−1 . A more
general approach to select Gi was presented in [16]. Assuming that t matrices were already
created in Gj+1 with 1 ≤ t < s + 1, then any linear combination of these matrices is also in
Gj+1 . In order to create the residual Rk+t+1 in Gj+1 , they ﬁrst select vectors Gi as,
t−1

Gk+t = −(Rk+t − Rk+t−1 ) −

βj Gk+i .
i=1

Diﬀerent choices of these parameter yields diﬀerent variants of IDR(s) for solving system of
linear equations. Normally, the values β’s are chosen to improve the convergence or stability
of the IDR(s), for example, in [16] the authors propose the biorthogonal residual variant of
IDR(s) selecting β’s such that the Gk+t is orthogonal to P1 , P2 , . . . , Pt−1 . A quasi-minimal
residual variant of IDR(s) was proposed in [15], choosing the parameters β to force Gk+t to be
orthogonal to G1 , G2 , . . . , Gt−1 . In this work we implement the biorthogonal residual IDR(s),
see [16] for more details.

3

Preconditioning

The use of preconditioners in iterative methods is a key element to accelerate or ensure the
convergence. However, in the context of solving linear matrix equations A(X) = C, there is
not a straightforward deﬁnition of the application of a preconditioner. An option for applying
the preconditioning operation to V is to obtain an approximation to the problem
A(X) = V.
For example in the case of the Sylvester equation, the preconditioner applied to V computes
an approximate solution of
AX + XB = V.
In next section, we present a simple preconditioner for the Sylvester equation based on ﬁxedpoint iteration.

3.1

Fixed-point (FP) and Inexact Fixed-point (FP-ILU) preconditioners for the Sylvester equation

In this section we present a simple preconditioning scheme for the iterative method to solve the
Sylvester equation,
AX + XB = V.
(7)
The solution of equation (7) is also the solution of the ﬁxed-point iteration,
AXk+1 = −Xk B + V

(8)
225

IDR(s) for solving linear matrix equations

Astudillo and Van Gijzen

We propose as preconditioner a few steps of the ﬁxed-point iteration (8). If matrix A is diﬃcult
to invert, we propose the application of few steps of the following iteration,
ˆ k B + V,
ˆ k+1 = −X
MX

(9)

where M is an approximation to the matrix A. This can be considered as an inexact ﬁxed-point
iteration. Particularly, we approximate A using the Incomplete LU factorization. Fixed-point
iterations (8) and (9) do not have the same solution. However, if it is assumed that M is a
good approximation of A and equation (7) is well-conditioned, one can expect that the solution
of the ﬁxed point iteration (9) is close to the solution of the Sylvester equation (7).
We use as preconditioning operator M(V ) the ﬁxed-point iteration (9), or if it is possible to
solve block linear system with A eﬃciently, we use iteration (8). The ﬁxed point iteration (8)
for solving Sylvester equation has been analyzed in [8]. A suﬃcient condition for the iteration
(8) to converge to its ﬁxed point is that A−1 B < 1 when A is non-singular. Using this
result, it is easy to see that the inexact iteration (9) also converge to its ﬁxed point if M is nonsingular and M −1 B < 1. For this reason, we can compute M = LU +E, the incomplete LU
factorization of A, using strategies based on monitoring the growth of the norm of the inverse
factors of L and U like [3, 4], or scaling matrices such that M −1 B < 1 is satisﬁed.

4

Numerical examples

In this section we present four numerical experiments to illustrate the numerical behavior of
IDR(s) for solving matrix equations and compare it with others block Krylov subspace solvers.
The ﬁrst three problems are illustrative small examples for diﬀerent types of matrix equations.
The fourth example considers a more realistic application the ocean circulation simulation
problem [17].
The numerical experiments presented in this section were implemented in Python 2.7 running on GNU/ Debian on an Intel computer with four cores I5 and 32GB of RAM. We use as
stopping criterion,
C − A(X) F
≤ 10−8 .
C F

4.1

Small examples

Example 1: (Solving a Lyapunov equation) In this example, we solve the Lyapunov equation
using IDR(s) for A(X) = C with A(X) = AX + XAT . We compare IDR(s = 4) for matrix
equations with Bi-CGSTAB [14] and GMRES [9]. As matrix A, we choose the negative of
the anti-stable matrix CDDE6 from the Harwell-Boeing collection, and matrix C = ccT , with
c = rand(961, 1). Although for IDR(s) the solution of the Lyapunov equation takes more
iteration (165), this is the faster method regarding CPU-time. IDR(s) consumes 7.52 secs. of
CPU time, while GMRES runs in 17.53 secs. (131 iterations) and Bi-CGSTAB takes 13.32 secs.
(566 iterations).
Example 2: (Solving a time-dependent linear system) We consider the time-dependent linear
system,
dy
= Ay + g(t),
t0 ≤ t ≤ tm
with y(t = t0 ) = y0 .
(10)
dt
226

IDR(s) for solving linear matrix equations

Astudillo and Van Gijzen
(b)
Relative residual norm

Relative residual norm

(a)
106
104
102
100
10−2
10−4
10−6
10−8

IDR(s = 2)
GMRES
Bi-CGSTAB

0
20
40
60
80
100
120
Number of application of the Sylvester operator

100

IDR(s = 2)
GMRES

10−2

Bi-CGSTAB

10−4
10−6
10−8
0
10
20
30
40
Number of application of the Sylvester operator

Figure 1: (Example 2) (a) Residual norm for IDR(s=2), Bi-CGSTAB, and GMRES solving
a Sylvester equation. (b) Residual norm for the preconditioned IDR(s=2), Bi-CGSTAB, and
GMRES using two steps of (8).
Solving (10) with backward Euler with constant time-step δt , we obtain a Sylvester equation,
−AY + Y

D
y0 T
=G+
e ,
δt
δt 1

(11)

where G = [g(t1 ), g(t2 ), . . . , g(tm )]n×m , D is the upper and bidiagonal matrix,
⎤
⎡
1 −1
0 ...
0
⎢0
1 −1 . . .
0⎥
⎥
⎢
⎢ ..
.
.
..⎥
.
..
..
..
D =⎢.
,
.⎥
⎥
⎢
⎦
⎣0
0
0
−1
0
0
0 ...
1 m×m
and e1 represents the ﬁrst canonical vector of order m. Speciﬁcally, We consider the 1D timedependent convection-diﬀusion equation,
du
d2 u
du
−
= 0,
+ω
2
dt
dx
dx

0 ≤ t ≤ 1,

ut0 = 1,

(12)

with convection parameter ω = 1.0 and diﬀusion term = 10−3 , x ∈ [0, 100], with Dirichlet
boundary conditions. We discretized this equation using the central ﬁnite diﬀerences and Euler
backward for time integration, with δt = 0.05 (m = 20), δx = 0.1 (A ∈ C1000×1000 ). Figure 1
show the evolution of the residual norm for IDR(s) and diﬀerent Krylov method without and
with preconditioner (8) respectively.
Example 3: (Solving a multi-shift linear system) Solving a multi-shift linear system of equation,
for i = 1, 2, . . . , m,
(13)
(A − σi I)x = b,
can also be rewritten as a Sylvester equation,
AX − XD = buT ,

(14)

where D = diag([σ1 , σ2 , . . . , σm ]), X ∈ Cn×m , and u = [1, 1, . . . , 1]T ∈ Cm . We consider an
example presented in [15]. We discretize the convection-diﬀusion-reaction equation,
−

u + vT ∇u − ru = f
227

IDR(s) for solving linear matrix equations

Astudillo and Van Gijzen
(b)

10
102
100
10−2
10−4
10−6
10−8

Relative residual norm

Relative residual norm

(a)
4

IDR(s = 2)
GMRES
Bi-CGSTAB

0 100 200 300 400 500 600 700 800
Number of application of the Sylvester operator

2

10

IDR(s = 2)

100

GMRES
Bi-CGSTAB

−2

10

10−4
10−6
10−8
0
5
10
15
20
25
30
35
Number of application of the Sylvester operator

Figure 2: (Example 3) (a) Residual norm for IDR(s=2), Bi-CGSTAB, and GMRES solving
a Sylvester equation. (b) Residual norm for the preconditioned IDR(s=2), Bi-CGSTAB, and
GMRES using two steps of (9).
√
√
with = 1, v = [0, 250/ 5, 500/ 5]T , r = {0.0, 200.0, 400.0, 600.0, 800.0, 1000.0}, and homogeneous Dirichlet boundary conditions in the unit cube using central ﬁnite diﬀerences obtaining a matrix A of size 59319 × 59319. The right-hand-side vector is deﬁned by the solution
u(x, y, z) = x(1 − x)y(1 − y)z(1 − z). Figures 2 shows the behavior of the relative residual
norm for GMRES, IDR(s), and Bi-CGSTAB with and without preconditioner.

4.2

A more realistic example

Example 4:
The previous numerical examples are rather academic, in this example we
consider a more realistic example. We consider a convection-diﬀusion problem from ocean
circulation simulation. The following model,
−r

ψ−β

∂ψ
= (∇ × F )z
∂x

in Ω

(15)

describes the steady barotropic ﬂow in a homogeneous ocean with constant depth. The function
ψ represents the stream function, r is the bottom friction coeﬃcient, β is the Coriolis parameter,
and F is given by
τ
F =
,
(16)
ρH
where τ represents the external force ﬁeld caused by the wind stress, H the average depth of
the ocean, and ρ the water density. The stream function is constant on continent boundaries,
i.e.,
on ∂Ωk for k = 1, 2, . . . , K,
(17)
ψ = Ck
with K is the number of continents. The values of Ck are determined by the integral condition,
r
∂Ωk

∂ψ
ds = −
∂n

∂Ωk

F × s ds.

(18)

We discretize Eqs. (15)-(18) using the ﬁnite elements technique described in [17]. The physical
parameters used can also be found in the same reference. We obtain a coeﬃcient matrix A of
order 42248, and we have to solve a sequence of twelve systems of linear equations,
Axi = bi
228

with i = 1, 2, . . . , 12.

(19)

IDR(s) for solving linear matrix equations

Method
IDR(s = 4)
Bi-CGSTAB
Bi-CG
GMRES(100)

Solving (19) separately
Time [s] # Mat-Vec
17.03
2498
22.66
3124
25.58
4070
42.82
4200

Astudillo and Van Gijzen

Solving (19) as a block linear system
Time [s]
# Mat-Block
12.29
190
15.35
282
20.62
338
38.30
400

Table 1: (Example 4) Comparison of solving (19) as a sequence of linear system or as a matrix
equation (a block linear system). This exempliﬁes one of the advantages of using a blocksolvers over their sequential counterparts, the time reduction due to the extensive use of block
subroutines (BLAS Level 3) over several calls to single vectors routines. Mat-Vec indicates
the number of matrix-vector products and Mat-Block indicates the number of matrix-block
multiplications.

Each of these system of linear equations represent the data for each month of the year. We
compare the time for solving (19) using two approaches, solving all the linear systems separately,
and solving (19) as a linear matrix equation (a block linear system). In all the cases, we applied
incomplete LU with drop tolerance 10−4 as preconditioner. Table 4 shows the time comparison
between the diﬀerent methods using both approaches. Figure 3 shows the solution computed
using IDR(s = 4) for matrix equations.
Degree
4
3
2
1

Matrix size
2594 × 2594
4630 × 4630
10491 × 10491
42249 × 42249

Time [s]
0.02
0.11
0.42
12.29

# Mat-Block
5
18
29
190

Table 2: (Example 4) Solving the ocean model problem as a matrix equation (a block linear
system) using IDR(s = 4) with ILU preconditioner. Degree is the grid size in the ocean model.
In Table 2, the increment in number of matrix-block products is higher than a fact of two if
the grid size is halved. This rather disappointing behaviour seems to be caused by the dropping
strategy in the ILU preconditioner, which gives a worse performance for increasingly ﬁner grids.
To exclude this eﬀect, we next consider diagonal scaling as preconditioner for IDR(s) in Table
3.
Degree
4
3
2
1

Matrix size
2594 × 2594
4630 × 4630
10491 × 10491
42249 × 42249

Time [s]
0.44
1.36
5.09
58.03

# Mat-Block
557
773
1204
2883

Table 3: (Example 4) Solving the ocean model problem as a matrix equation (a block linear
system) using IDR(s = 4) with diagonal preconditioner. One can see a linear increment with a
factor of two in the number of matrix-block products required if the grid size is halved.

229

IDR(s) for solving linear matrix equations

Astudillo and Van Gijzen
December

80

60

40

20

0

−20

−40

−60

−80
0

50

100

150

200

250

300

350

Figure 3: (Example 4) Solution of the ocean problem.

5

Software implementation

The IDR(s) method for matrix equation is implemented in Python 2.7. The main advantages
of using Python are the code portability and ease of use. The software solves general linear
matrix equations (1), it only uses the standard Python libraries Numpy v1.8.2 and Scipy v0.14.
For compatibility reasons with the Krylov subspace methods implemented in the library Scipy,
the software interface is the following:
X, info = idrs(A, C, X0=None, tol=1e-8, s=4, maxit=2000,
M=None, callback=None).
Table 4 describes the input and output parameters.
Following, we illustrate the use of the Python’s idrs function. The user has to provide an
object which deﬁne the application of a linear operator A over a matrix X, in this case we use
example 1 of the numerical tests,
import s c i p y . i o a s i o
import numpy a s np
from s o l v e r import ∗
# IDRs s o l v e r p a c k a g e
A = −i o . mmread ( ’ cdde6 . mtx ’ ) . t o c s r ( )
n = A. shape [ 0 ]
c = np . random . rand ( n , 1 )
C = c . dot ( c . T)
c l a s s LyapOp :
# D e f i n i n g L i n e a r Operator
i n i t ( s e l f , A) :
define
s e l f .A = A
d e f i n e dot ( s e l f , X ) :
return s e l f .A. dot (X) + ( s e l f .A. dot (X. T ) ) . T
Aop = LyapOp (A)
X, i n f o = i d r s ( Aop , C)
230

IDR(s) for solving linear matrix equations

Parameter
A
C
X0
tol
s
maxit
M
callback
X
info

Astudillo and Van Gijzen

Description
(Input object) Object with a dot method
which deﬁnes the linear operator of the problem.
(Input matrix) The right hand side matrix.
(Input matrix) Initial guess.
(Input ﬂoat) Tolerance of the method.
(Input integer) Size of IDR recursion. Bigger s
gives faster convergence, but also makes the method more expensive
(Input integer) Maximum number of iterations to be performed.
(Input object) Object with a solve method
which deﬁnes the preconditioning operator of the problem.
(Input function) User-supplied function.
It is called as callback(X) in each iteration.
(Output matrix) Approximate solution.
(Output integer) 0 if convergence archived. > 0 if convergence not archive.
Table 4: Parameter of the Python’s idrs function

6

Conclusions

In this work we have presented a generalization of the IDR(s) theorem [13] valid for any ﬁnitedimensional space. Using this generalization, we have presented a framework of IDR(s) for
solving linear matrix equations. This document also presents several numerical examples of
IDR(s) solving linear matrix equations, among them, the most common linear matrix equations like Lyapunov and Sylvester equation. In the examples solving Lyapunov and Sylvester
equations, full GMRES required less iteration to converge than IDR and Bi-CGSTAB. However,
IDR(s) presented a better performance in CPU time.
Additionally, we present two preconditioners based on ﬁxed point iteration to solve the
Sylvester equation. The ﬁrst preconditioner is ﬁxed-point iteration,
AXk+1 = −Xk B + C,
that required the explicit inverse or the solving block linear systems with matrix A. Whenever
is not possible the inversion or solving block linear system with matrix A in an eﬃcient way,
we use the inexact iteration
M Xk+1 = −Xk B + C,
where M = LU the incomplete LU factorization of A. Numerical experiments conducted show
a competitive behavior of IDR(s) for solving linear matrix equations.

7

Code availability

Implementations of the Induced Dimension Reduction (IDR(s)) in diﬀerent programming languages like Matlab, FORTRAN, Python and Julia are available to download in the web page
http://ta.twi.tudelft.nl/nw/users/gijzen/IDR.html
231

IDR(s) for solving linear matrix equations

Astudillo and Van Gijzen

References
[1] R. Astudillo and M. B van Gijzen. A Restarted Induced Dimension Reduction method to approximate eigenpairs of large unsymmetric matrices. J. Comput. Appl. Math., 296:24–35, 2016.
[2] M. Baumann and M. B van Gijzen. Nested Krylov methods for shifted linear systems. SIAM J.
Sci. Comput., 37(5):S90–S112, 2015.
[3] M. Bollh¨
ofer. A robust ILU with pivoting based on monitoring the growth of the inverse factors.
Linear Algebra Appl., 338(1–3):201–218, 2001.
[4] M. Bollh¨
ofer. A Robust and Eﬃcient ILU that Incorporates the Growth of the Inverse Triangular
Factors. SIAM J. Sci. Comput., 25(1):86–103, 2003.
[5] L. Du, T. Sogabe, B. Yu, Y. Yamamoto, and S.-L. Zhang. A block IDR(s) method for nonsymmetric linear systems with multiple right-hand sides. J. Comput. Appl. Math., 235(14):4095–4106,
2011.
[6] M. H. Gutknecht and J.-P. M. Zemke. Eigenvalue Computations Based on IDR. SIAM J. Matrix
Anal. Appl., 34(2):283–311, 2013.
[7] M. Hochbruck and G. Starke. Preconditioned Krylov Subspace Methods for Lyapunov Matrix
Equations. SIAM J. Math. Anal. Appl., 16(1):156–171, 1995.
[8] M. Monsalve. Block linear method for large scale Sylvester equations. Comput. Appl. Math.,
27(1):47–59, 2008.
[9] Y. Saad and M.H. Schultz. GMRES: a generalized minimal residual algorithm for solving nonsymetric linear systems. SIAM J. Sci. Stat. Comput., 7:856–869, 1986.
[10] V. Simoncini. Computational methods for linear matrix equations. To appear in SIAM Rev., (To
appear).
[11] V. Simoncini and D. B. Szyld. Interpreting IDR as a Petrov-Galerkin method. SIAM J. Sci.
Comput., 32(4):1898–1912, 2010.
[12] G. L. G. Sleijpen and H. A. van der Vorst. Maintaining convergence properties of bicgstab methods
in ﬁnite precision arithmetic. Numer. Algorithms, 10(2):203–223, 1995.
[13] P. Sonneveld and M. B. van Gijzen. IDR(s): a family of simple and fast algorithms for solving
large nonsymmetric linear systems. SIAM J. Sci. Comput., 31(2):1035–1062, 2008.
[14] H. A. van der Vorst. Bi-CGSTAB: A Fast and Smoothly Converging Variant of Bi-CG for the
Solution of Nonsymmetric Linear Systems. SIAM J. Sci. Stat. Comput., 13(2):631–644, 1992.
[15] M. B. van Gijzen, G. L. G. Sleijpen, and J.-P. M. Zemke. Flexible and multi-shift induced
dimension reduction algorithms for solving large sparse linear systems. Numer. Linear Algebra
Appl., 22(1):1–25, 2015.
[16] M. B. van Gijzen and P. Sonneveld. Algorithm 913: An Elegant IDR(s) Variant that Eﬃciently
Exploits Bi-orthogonality Properties. ACM Trans. Math. Software, 38(1):5:1–5:19, 2011.
[17] M. B. van Gijzen, C. B. Vreugdenhil, and H. Oksuzoglu. The Finite Element Discretization for
Stream-Function Problems on Multiply Connected Domains. J. Comput. Phys., 140(1):30–46,
1998.

232

