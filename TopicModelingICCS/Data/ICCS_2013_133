Available online at www.sciencedirect.com

Procedia Computer Science 18 (2013) 260 – 269

International Conference on Computational Science, ICCS 2013

OSL: an algorithmic skeleton library with exceptions
Joeﬀrey Legauxa,∗, Fr´ed´eric Loulerguea , Sylvain Jubertiea
a LIFO,

Universit´e d’Orl´eans, France

Abstract
Exception handling is a traditional and natural mechanism to manage errors and events that disrupt the normal ﬂow of program
instructions. In most concurrent or parallel systems, exception handling is done locally or sequentially, and cannot guarantee
the global coherence of the system after an exception is caught. Working with a structured parallel model is an advantage
in this respect. Algorithmic skeletons, that are patterns of parallel algorithms on distributed data structures, oﬀer such a
structured model. However very few algorithmic skeleton libraries provide a speciﬁc parallel exception mechanism, and no
C++-based library. In this paper we propose the design of an exception mechanism for the C++ Orl´eans Skeleton Library
that ensures the global coherence of the system after exceptions are caught. We explain our design choices, experiment on the
performance penalty of its use, and we illustrate how to purposefully use this mechanism to extract the results in the course of
some algorithms.
Keywords: Parallel programming; algorithmic skeletons; C++; exceptions

1. Introduction
Due to the increasing complexity of computer programs and systems, abnormal or exceptional events are very
likely to happen. Even in the case of a certiﬁed system, input/output operations, memory allocation or in general
hardware related operations may cause errors. It is preferable that such events do not cause a crash of the programs
or systems. Therefore modern programming languages oﬀer mechanisms to manage such events.
Work on managing exceptions started in the 70’s [1]. Nowadays, exception mechanisms are more than a way
to manage errors: There are structural parts of the languages and the execution models. Three distinct roles are
played by an exception mechanism:
(1) The capability to continue the execution of a program in the absence of a result that failed to be computed.
It may simply be the capability to output the nature of the problem before terminating the execution. One possible
solution for this is to output a special return value and to have a diﬀerent value for each possible error scenario.
This is what is done in C for example with the integer value return by the main function or the return value of some
MPI functions that may indicate errors. However these cases do not belong to the exception mechanism class as
they require an explicit treatment of exceptional cases, for all the calls: This pollutes the code and makes it harder
to read, understand and maintain. An exception mechanism introduces an implicit treatment of exceptional cases.
Code is kept smaller and dealing with exceptional cases using speciﬁc language constructs makes the code clearer.
∗ Corresponding

author. Tel: +33 (0)2 38 41 70 11 Fax: +33 (0)2 38 41 71 37.
E-mail address: joeﬀrey.legaux@univ-orleans.fr.

1877-0509 © 2013 The Authors. Published by Elsevier B.V. Open access under CC BY-NC-ND license.
Selection and peer review under responsibility of the organizers of the 2013 International Conference on Computational Science
doi:10.1016/j.procs.2013.05.189

Joeffrey Legaux et al. / Procedia Computer Science 18 (2013) 260 – 269

(2) When an exception is raised, the normal execution ﬂow is interrupted. The execution ﬂow is routed to a
speciﬁc code to deal with the exceptional case at hand. Therefore an exception should trigger a speciﬁc treatment
without step-by-step going through the call stack as in normal function calls.
(3) A catch or retry mechanism allows to return to a safe program state or to retry with diﬀerent parameters.
To have an idea of the complexity of exception handling in the case of a parallel exception, it may be helpful to
think of exception propagation as a jump in the call stack of a program, or successive jumps to an adequate routine
to handle the exception. In the parallel case there exists one stack by processor or process and these stacks may
be entirely independent if we assume no underlying structure. In the absence of synchronisation mechanisms,
the notion of the execution state at a given time is itself diﬃcult to deﬁne as every machine has its own clock.
There are a lot of work on exception in a concurrent context [2]. But as the authors of [3] indicate there is no best
solution. The exception mechanism should be adapted to the considered concurrent or parallel model. In most
systems, in particular concurrent or parallel ones, exception handling is done locally or sequentially, and cannot
guarantee the global coherence of the system after an exception is caught. In this context, we have the advantage
to work with a structured parallel model: algorithmic skeletons [4].
Skeletal parallel languages or libraries provide a ﬁnite set of algorithmic skeletons that are higher-order functions or patterns that can be executed in parallel. A skeleton often captures the pattern of a classical parallel algorithm such as a pipeline, a parallel reduction, or parallel operations on distributed collections. In such a context
data-structures are considered globally for the whole parallel machine, even in the case of distributed memory machine. This is the global view also advocated by the Chapel language designers [5]. It eases parallel programming
compared to the fragmented view provided by SPMD programming (for example MPI). Usually the sequential
semantics of the skeleton is simple and corresponds to the usual semantics of similar higher-order functions in
functional programming languages or usual operations and iterations on collections in object oriented languages.
The user of algorithmic skeletons has just to compose some of the skeletons to write his parallel application.
Orl´eans Skeleton Library or OSL is an eﬃcient algorithmic skeleton library for C++ that uses meta-programming
techniques to attain good performances when composing skeletons [6].
The contribution of the work described in this paper is: (1) the design of an exception mechanism adapted
to the parallel execution model of Orl´eans Skeleton Library that ensures the global coherence of the system after
exceptions are caught; (2) the implementation of this mechanism in OSL and experiments of the performance
penalty of its use; (3) an illustration of its use to purposefully extract the results in the course of parallel backtracking algorithms.
The paper is organised as follows. In section 2, we give an overview of the Orl´eans Skeleton Library. Section 3
describes the implementation of the exception mechanism inside OSL. Section 4 is devoted to an application that
relies heavily on the exception mechanism for its execution ﬂow. Section 5 studies the performance penalty of
using exceptions. We discuss related work in section 6, and conclude in section 7.
2. OSL: An Overview
Orl´eans Skeleton Library is a C++ library of data-parallel algorithmic skeletons that follow the Bulk Synchronous Parallel [7] model of parallel computation. It is implemented on top of MPI and takes advantage of the
expression templates optimisation techniques to be very eﬃcient yet allowing programming in a functional style.
Programming with OSL is very similar to programming in sequential. The main diﬀerence is that OSL programs operate on distributed data structures called distributed arrays. Distributed arrays are one dimensional
arrays such that, at the time of the creation of the array, data is distributed among the processors. Distributed
arrays are implemented as a template class DArray<T>. A distributed array consists of bsp_p partitions, where
bsp_p is the number of processing elements of the parallel (BSP) machine. Each partition is an array of elements
of type T.
2.1. OSL computation and communication skeletons
To give a quick, yet precise, overview of OSL, ﬁgure 1 presents an informal semantics for the main OSL
skeletons together with their signatures. In this ﬁgure, bsp_p is noted p. A distributed array of type DArray<T>
can be seen “sequentially” as an array [t0 , . . . , tt.size−1 ] where t.size is the (global) size of the (distributed) array t

261

262

Joeffrey Legaux et al. / Procedia Computer Science 18 (2013) 260 – 269

Skeleton
map
zip
reduce
scan
permute
shift
redistribute
getPartition

flatten

Signature
Informal semantics
DArray<W> map(W f(T), DArray<T> t)
map( f, [t0 , . . . , tt.size−1 ]D ) = [ f (t0 ), . . . , f (tt.size−1 )]D
DArray<W> zip(W f(T,U), DArray<T> t, DArray<U> u)
zip( f, [t0 , . . . , tt.size−1 ]D , [u0 , . . . , ut.size−1 ]D ) = [ f (t0 , u0 ), . . . , f (tt.size−1 , ut.size−1 )]D
<T> reduce(T ⊕(T,T), DArray<T> t)
reduce(⊕, [t0 , . . . , tt.size−1 ]D ) = t0 ⊕ t1 ⊕ . . . ⊕ tt.size−1
DArray<T> scan(T ⊕(T,T), DArray<T> t)
scan(⊕, [t0 , . . . , tt.size−1 ]D ) = [⊕0i=0 ti ; . . . ; ⊕t.size−1
ti ]D
i=0
DArray<T> permute(int f(int), DArray<T> t)
permute( f, [t0 , . . . , tt.size−1 ]D ) = [t f −1 (0) , . . . , t f −1 (t.size−1) ]D
DArray<T> shift(int o, T f(T), DArray<T> t)
shift(o, f, [t0 , . . . , tt.size−1 ]D ) = [ f (0), . . . , f (o − 1), t0 , . . . , tt.size−1−o ]D
DArray<T> redistribute(Vector<int> dist, DArray<T> t)
redistribute(dist, [t0 , . . . , tt.size−1 ]D ) = [t0 , . . . , tt.size−1 ]dist
DArray< Vector<T> > getPartition(DArray<T> t)
getPartition([t0 , . . . , tt.size−1 ]D ) = [t0 , . . . , tD(0)−1 ], . . . , [t ji , . . . , t ji +D(i)−1 ], . . . , [t j p−1 , . . . , tn−1 ] E p
where E p (i) = 1 and ji = k=i−1
k=0 D(k)
DArray<T> flatten(DArray< Vector<T> > t)
flatten([a0 , . . . , aa.size−1 ]) = a0 [0], . . . , a0 [a0 .size − 1], a1 [0], . . . , aa.size−1 [aa.size−1 .size − 1] D
where D (i) = D(i−1)≤k<D(i) ak .size and ji = k=i−1
k=0 D(k)
Fig. 1. OSL Skeletons

(and we use the same notation if t is a C++ vector). But as with the getPartition skeleton, the user can expose
the distribution of the distributed array, this informal semantics should also indicates how the array is distributed.
We write the distribution as a subscript D of the distributed array. D is a function from {0, . . . , bsp_p} to N.
The ﬁrst two skeletons, map and zip, are the usual combinators used to apply a function to each element of
a distributed array (resp. of two distributed arrays). The ﬁrst argument of both map and zip could be a C++
functor either extending std::unary_function or std::binary_function. There are two variants: mapIndex and
zipIndex. Each of these variants takes a function with an additional argument: an integer representing the global
index of each element in the distributed array.
Parallel reduction and parallel preﬁx computation with a binary associative operator ⊕ are performed using
respectively the reduce and scan skeletons. Communications are needed to execute both skeletons.
permute and shift are communication skeletons. permute redistributes the array, according to a function f,
bijective on the interval [0, t.size − 1]. The shift skeleton allows to shift elements on the right (the case shown in
the ﬁgure) or the left depending on the sign of its ﬁrst argument. The missing values, at the beginning or the end
of the array, are given by function f.
The next skeleton only modiﬁes the distribution of the distributed array, not its content. redistribute distributes the content of the distributed array according to a vector of integers representing the target distribution.
This skeleton is used to implement derived skeletons such as gather, scatter or balance. The latter redistribute
the array so that it becomes an evenly distributed array, i.e. an array such that the largest local partition has at most
one more element than the smallest local partition.
All the skeletons up to redistribute preserve the distribution. It means that if they are applied to evenly
distributed arrays, the result will be an evenly distributed array. The redistribute skeleton may thus seems
useless. However, some algorithms such as BSP regular sampling sort, require intermediate and ﬁnal results that
are not evenly distributed. To implement such algorithms, two additional skeletons are needed: getPartition and
flatten.
The getPartition skeleton exposes how a distributed array is distributed among the processors. It transforms

Joeffrey Legaux et al. / Procedia Computer Science 18 (2013) 260 – 269

a distributed array of type DArray<T> into a distributed array of type DArray<Vector<T> > and of size bsp_p. In
the resulting distributed array, each processor contains only one element: a C++ vector, the former partition on
this processor. The flatten skeleton is the inverse operation of getPartition.
As a very short OSL program, we compute the variance
xi :

n−1
i=0 (xi

−

n−1
j=0

n

xj

) of a sequence of random variables

double avg = osl::reduce(std::plus<double>(), x) / x.getGlobalSize();
double variance = osl::reduce(std::plus<double>(),
osl::map(std::bind(std::minus<double>(),avg, _2), x));

2.2. OSL skeleton for exception handling
In order to provide parallel error handling to the user, we add a parallel exception handling mechanism to OSL.
This is provided by the forwardExceptions skeleton. This skeleton takes as parameter a skeletal expression and
returns the value of the skeletal expression. If at some processors, the evaluation of the skeletal expression raises
an exception, forwardExceptions will raise a vector of exception pointers. This vector has size bsp_p and at each
index corresponding to a processor that raised an exception, the vector contains the exception pointer otherwise it
contains the null pointer.
Thus we can catch all the exceptions raised by all the processors. If he prefers, the user could choose only
one exception to consider, for example by getting the exception of the lowest rank processor, or randomly. In
this respect the mechanism we provide increases the expressive power of OSL. Syntactically, an OSL program
handling exceptions would look as follows, where da is a deﬁned distributed array of type DArray<int>:
struct f {
int operator()(int i) { if(i%2 == 0) throw std::exception(); else return i; }
};
try
{ forwardExceptions(map(f(), da)); }
catch
( std::vector<std::exception*> exn ) { /* Access to all the exceptions through exn */ }

3. Support for Parallel Exception Handling
The user can already catch exception at the element level in the functions applied by the skeleton calls. He
can also catch exceptions raised by the evaluation of a skeleton expression with the usual sequential try/catch
instructions, however doing this will only catch the exceptions raised locally. This is problematic as it may cause
processes to follow diﬀerent paths of instructions, which could break the consistence or synchronisation implied
by the BSP model. To implement complete exception handling for OSL we need to be able to determine if any
of the current processes that are working in a skeleton call did raise an exception. We thus need to be able to
remotely send exceptions between the processes, but the design of the C++ exception handling mechanism raises
several problems.
Exceptions are naturally polymorphic: Every exception provided by the standard library is derived from the
base class std::exception, and users are encouraged to similarly create their own exception classes by deriving
from std::exception, although the language allows any type of object to be thrown as an exception.
Exceptions in C++ have the same properties as any other object, however the standard deﬁnes that they are
thrown by value, which creates a copy based on their dynamic type, while standard objects are copied using their
static type when passed as a function parameter. Moreover, we can only write the catching blocks by ﬁltering
exceptions according to their static types. The generic catching block catch(...) can be used to catch any type
of exception, but it does not provide access to the exception object, so it is not suited if we want to send the
exception to another process. Processing each type of possible exception separately in its own catch block would
be extremely tedious to write, not easily extensible, and very ineﬃcient as we would exchange separate messages
between processes for every type of exception.

263

264

Joeffrey Legaux et al. / Procedia Computer Science 18 (2013) 260 – 269

We can only achieve this eﬃciently by catching the most generic possible type of exceptions, serialise them,
and exchange them once between the processes. The main diﬃculty lies in the fact that we do not know the
dynamic type of the caught exception, but only its static base type, and we must be able to serialise the object in
one process and deserialise it to create an object of the dynamic derived type on the other processes.
A rather simple solution is to create a complete hierarchy of objects deriving from a virtual base class which
requires the derivatives to implement a serialisation function, an object creation function and a throwing function.
The receiving process can then use an implementation of the classical abstract factory pattern to proceed to the
creation of the exception object of the correct type and propagate it locally [8, 9].
This solution however as a consequent disadvantage: the user is obliged to use a custom hierarchy of exceptions. We want to stay close to the standard and provide the user with the usual exception objects of the standard
library, but those lack the needed serialisation and polymorphic creation functions needed to achieve our goal.
However, the Boost library provides tools that can be used to solve these issues.
3.1. Boost
Boost1 is a set of experimental libraries for C++ providing very advanced functionalities that often go far
beyond the standard use of the language. Eventually, the most popular libraries are submitted to be included in
the standard library when they grow mature enough.
A very interesting library here is boost::serialization. This library provides various means to implement
generic serialisation operators. The key concept is that we deﬁne how an object of a class will be serialised within
an “archive” which can be viewed as an abstract container at this point. Our serialisation function is generic,
and the archive class is a template parameter of this function. We may then implement an actual archive class or
use the provided ones, which typically provide text archives for strings or ﬁle streams, binary archives or XML
archives. The archive concept is also implicitly used in the boost::mpi library, so we can easily send and receive
any object through MPI processes as long as it possesses a serialisation operation.
boost::serialization already provides serialization operators for many common object types, but unfortunately not for the standard exceptions. Writing such an operator for the std::exception base class is pretty
straightforward, as this class does not have any data member:
void serialize(Archive & ar, std::exception & e, const unsigned int version){}

We need some more work for classes such as std::logic_error and its derivates. These do not have a default
constructor, but only a constructor taking a string argument. We need to overload save_construct_data and
load_construct_data operators of boost::serialize to take care of these speciﬁcities.
inline void save_construct_data(Archive & ar, const myclass * c, const BOOST_PFTO unsigned int){
std::string s(c->what());
ar << s; }
template<class Archive>
inline void load_construct_data(Archive & ar, myclass * c, const unsigned int){
std::string s;
ar >> s;
::new(c)myclass(s); }

Now we need to add the possibility to serialise our exceptions through a pointer to the base type std::exception,
as the receiving processes will not have any information about the dynamic types of the exception that may be
retrieved. Fortunately, boost::serialization implicitly manages the serialisation of pointers and includes mechanisms to retrieve derived object from a serialised pointer to its base class. The ﬁrst step is to include in the
serialisation operator the serialisation of the base class:
void serialize(Archive & ar, myclass & c, const unsigned int version){
ar & boost::serialization::base_object<baseclass>(c); }

1 www.boost.org

Joeffrey Legaux et al. / Procedia Computer Science 18 (2013) 260 – 269

265

The boost::serialization library maintains a list of references to the types of the derived classes in order to allow their identiﬁcation from a base pointer and invoke the corresponding code. This can be done
with the provided macro BOOST_CLASS_EXPORT_KEY(classname). A last issue is that, as the serialisation operator is a template function, it may never be instantiated if we only work with pointers to the base class. Again,
boost::serialization provides the macro BOOST_CLASS_EXPORT_IMPLEMENT(classname) which enforces the instantiation of the relevant code.
Serialisation of standard exceptions always follows the same pattern, the only diﬀerence being when a default
constructor, or a string parameter are needed. We provide two convenient macros which allow the users to easily
implement serialisation for their own exceptions as long as they derive from a standard exception without adding
new members or methods.
3.2. Rethrowing
We are now able to catch remote exceptions and exchange them between processes as pointers to std::exception
objects. Thanks to the boost::serialization mechanisms, the pointed objects are eﬀectively created with the
adequate derived type. However, we cannot directly throw again those exceptions after reception due to the throwing mechanism: the exception object is copied by value when thrown. In C++ when an object is copied by value
its static type is used, thus the copy constructor of the base class is used, a slicing phenomenon occurs and a
std::exception object will always be thrown.
What we need is to convert the pointer to the correct derived type before throwing the pointed object. Although
we may retrieve information about the dynamic type of objects with the typeid operator, we can not use it directly
to obtain the type of an object and all casting operations have to be deﬁned statically. A basic solution could be to
write a big switch comparing the typeid of the pointer with each possible exception and proceeding to the relevant
conversion, but this greatly lacks extensibility.
We propose an alternative solution based on the abstract factory pattern [10], whose aim is to be able to create
objects derived from an abstract base type without specifying their exact types.
In our case, we do not want to build objects, but rather to cast an exception pointer to its correct derived type
and throw it. We have implemented a hierarchy of simple functors to fulﬁl this role: an abstract ThrowFunctor
deﬁning the virtual operator rethrow() which casts the pointer to its derived type and throw it, and a derived
functor implementing this operator for each exception type.
To be able to call the correct functor according to the derived type of our exception pointer, we create an
‘abstract throwing factory” class which registers all the exception classes types with their associated functors. A
general rethrow() function in the factory is provided to compare an input pointer with the registered classes and
call the relevant ThrowFunctor
We now only need to have in the abstract throwing factory all the pairs of typeids and functors we want to be
able to recognise. We attribute this role to a “throwing factory” that need to be created for each exception type. Its
constructor simply registers the corresponding pair in the abstract throwing factory, and each throwing factory is
statically called once in order to ensure that its constructor is called exactly once in the program.
Again, since the functor and throwing factory always conduct the same computation for each class of exceptions, we deﬁned a macro to construct them, so that any user can add its own exceptions to the abstract throwing
factory by simply calling the macro OSL_BUILD_THROW_FACTORY(myclass). Finally, we can throw any exception
pointer by simply calling the AbstractThrowingFactory::rethrow() method, as long as the derived type has been
deﬁned previously with the macro.
3.3. Implementation in OSL
In OSL, expressions evaluation is optimised through an automatic fusion implemented with the expression
templates mechanism. Each element of the language (distributed array or skeletons) that can be part of an expression is implemented as a template class of its sub-expressions, which contains an evaluation operator.
By building an OSL expression, we eﬀectively combine the template classes that represent it. The evaluation
operators are declared as inline, so when the compiler creates an instance of a sub-expression, the code of its
evaluation is eﬀectively inserted in the evaluation operator of its including expression. After compilation, we thus
get for our expression a single evaluation operator containing the fused code from all sub-expressions operators,

266

Joeffrey Legaux et al. / Procedia Computer Science 18 (2013) 260 – 269

instead of having a succession of nested calls. The fusion mechanism generally stops when a sub-expression is
a distributed array, whose evaluation operator simply returns its elements. The evaluation operator of a whole
expression is typically called when an assignment operator ’=’ is encountered in the program.
We have solved the diﬃculties of sending and rethrowing remote exceptions, now we may use standard exceptions inside communication skeletons just like any data type. As one of our goals was to be able to inform
all processes when exceptions occurs in any of them, we integrated a proper exception forwarding mechanism
in our library through a skeleton named forwardExceptions. This skeleton should not have any direct eﬀect on
the result of an expression when we apply it, so its return value has to be exactly the return value of the expression being evaluated. We also have to break the fusion optimisation of the expression templates when applying
forwardExceptions for two reasons. The ﬁrst is coherency: If we want to forward only exceptions raised by a
sub-part of the expression, we must ensure that loops of the compiled code are not fused with those of the rest
of the expression outside of the forwardException skeleton. The other reason is eﬃciency and simplicity: When
fusion optimisation occurs, the resulting loop may apply subsequently every skeleton on each element of the distributed array. That would imply that the exception forwarding mechanism would be carried individually for each
element of the distributed array, that would lead to potentially huge communication costs as even if no exception
is raised, we still need to send the serialisation of a null exception pointer.
What we want instead is to determine if an exception happened on a particular process. This can be done using
the Evaluate function. This function forces the evaluation of an expression by assigning the resulting value of the
expression to a temporary value, and then returning the content of this temporary.
The assignment eﬀectively shortcuts the fusion mechanism, but fusion can still take place inside the expression
being evaluated. Evaluate is already used notably by communication skeletons to ensure that the computations
are done before exchanging the data between processes.
The structure of the forwardException becomes thus rather simple: it calls Evaluate on the considered expression, then returns its result. The evaluation happens inside a try/catch block, if a std::exception is caught
(which implicitly includes any exception deriving from this type) we serialise a pointer to it and exchange them
with all other processes through a call to MPI_Alltoall. If no exception is raised, we simply send a NULL pointer.
After the forwardException skeleton, we thus get on each process an array of std::exception pointers containing the exceptions raised on all the processes. If this array does not contain only NULL pointers, we throw it as an
exception object locally on each individual node.
4. Parallel Backtracking Expressed with the Exception Handling Mechanism
Besides being able to recover from errors in parallel programs, our exception forwarding mechanism increases
the expressive power of our library, for example by allowing the implementation of a parallel backtracking algorithm with exceptions. Backtracking is a classical algorithm used to ﬁnd the solution of a given computational
problem. It does so by incrementally building a tree of the partial solutions, and progressively discarding them
as soon as they can be determined to be unable to lead to a ﬁnal solution. This algorithm can be applied as long
as we can build partial solutions and evaluate them, this particularly suits constraint satisfaction problems such as
the eight queens puzzle or sudoku for example.
Exception mechanisms are commonly used to improve the performance as well as the simplicity of programming on those algorithms. The idea is to simply throw an exception when a solution is found, and catch it outside
the backtracking function. This will eﬀectively shortcut the evaluation of all the remaining partial solutions, and
we do not have to propagate the results of the evaluations through the stack of recursive calls.
As we can now catch exceptions from remote processes, we can easily write a parallel version of this algorithm
with a functor:
struct BackTrack{
inline int operator()(Node s1) {
if (is_valid(s1)){
std::vector<Node> children = s1.build_children();
if (children.size() == 0) throw(MyException(s1.value()));
else for (int i = 0; i < children.size(); i++) backtrack(children[i]);
}

267

Joeffrey Legaux et al. / Procedia Computer Science 18 (2013) 260 – 269

}
};

This functor can now be called seamlessly within our library. In order to split the evaluation tree and have
enough elements to provide basic balancing between the processes, we ﬁrst create an array containing children of
the root node of the problem. Then we extract a batch of those nodes into a distributed array. We ﬁnally only have
to use the map skeleton to apply the backtracking algorithm to each element of this distributed array, and iterate
over the batches until a solution is found.
std::vector initArray = rootnode.build_children();
while (!result) {
try {
DArray<Node> array(getSomeNodes(initArray));
auto res = forwardExceptions(osl::map(backtrack(), array));
} catch(std::vector<std::exception *> e){
for (int i = 0; i < bsp_p; i++)
if(e[i] != NULL) { result = true; /* here we can extract the result from the exception */ }
}
}

5. Experiments
We conducted two sets of experiments: We ﬁrst measured the performance overhead caused by the forwarding
of exceptions on a distributed simulation of heat diﬀusion implemented with OSL. We evaluated this overhead
on varying problem sizes and number of processes, when the computation was conducted without any exception
being raised. We then measured the performance of the backtracking sudoku solver and compared it with a version
implemented without the use of exceptions. These experiments were conducted on a cluster of Quad-Core AMD
Opteron 2376 processors.
1.8

18
Overhead on small problem (1000x1000 grid)
Overhead on small problem (3000x3000 grid)

Backtracking without exceptions
Backtracking with exceptions

16

1.4

14

1.2

12
Speedup

Overhead

1.6

1
0.8

10
8

0.6

6

0.4

4

0.2

2

0

0
1

2

4

8

16
24
Number of cores

32

40

Fig. 2. Overhead on a simulation

48

56

1

2

4

8

16
24
Number of cores

32

48

64

Fig. 3. Sudoku backtracking solver

Concerning the overhead, the impact is very low on a small number of processors, but this situation declines as
the number of processes grows. This can be expected, as the cost of the MPI_Alltoall operations increases with
the square of the number of processes, while the computation speed only increases in a sub-linear fashion. At a
certain point, the gains in the simulation are outweighed by the costs of communications to forward exceptions.
On very small problems (such as a grid of size 100 x 100), the cost of communication is immediately higher to
the computation speedup.
The experiments on the backtracking algorithms do not exhibit a very good scalability, due to a very simple
load balancing mechanism. However both algorithms exhibit similar performances, with a slight advantage for the
one with exceptions. This illustrates the advantages of our exception forwarding mechanism regarding the added
expressivity: In the algorithm without exceptions, we have to manually gather the results on each process and then
broadcast the result (or its absence) on each iteration of the main loop. This forces us to add more skeleton calls,
introduce new functors and manage return values in the backtracking function SudokuSolve. This in turn lead to
an approximately 50% increase in the size of the program.

268

Joeffrey Legaux et al. / Procedia Computer Science 18 (2013) 260 – 269

6. Related Work
Error and exception handling mechanisms. In most languages, the exception handling mechanism is imperative:
there is a command to raise an exception and block construction to catch exceptions thrown by the code enclosed
in the block. Such a block is thus protected against (some of the) abnormal events. It is also associated with code
to be executed in case such an event occurs so that this event becomes invisible to the program outside the block.
This kind of mechanism is well adapted to current processors. It is used in languages such as C++, Ada, Java,
C#, Python. It is also compatible with ML type systems and thus is provided in languages such as OCaml, SML
or F#.
Common Lisp oﬀers a system of conditions, very expressive but tedious to use and to implement. This mechanism allows to relaunch execution with modiﬁed parameters. However no strongly typed language oﬀer such a
mechanism, so we will not consider it further. Other mechanisms are well suited to pure functional languages, even
if then do not respect all the criteria given in introduction. This is the case of monadic exception handling [11],
for example in Haskell. This kind of mechanism does not allow to propagate exceptions transparently and makes
the code heavier but is the adequate mechanism for lazy functional languages.
Exception handling in concurrent systems. In concurrent models, exception handling cannot be done at the global
level. Therefore it is often transformed in a kind of auxiliary communication protocol that allows to propagate
process state information and to take decisions at diﬀerent levels of locality depending on the scope of the exception or error. Some systems such as Argus [12] allows to select a subset of processes (for example all the processes
that deﬁned a speciﬁc exception) and synchronise them in case of an exception is raised. It is quite complex in the
implementation, and quite complex to use. The context is also quite diﬀerent from structured parallelism and the
kind of proposed solution is not well adapted to structured parallelism.
Exception handling in structured parallelism. There exist several algorithmic skeleton libraries for various languages: C++ [13, 14], Java [15, 16, 17], OCaml [18] to cite a few. Muesli [14] and SkeTo [13] are the closest
libraries to OSL in terms of skeletons on distributed arrays. However they do not provide an exception handling
mechanism speciﬁc to the skeletons: an uncaught exception at the level of a processor cannot be caught at the
global level, and therefore will lead to a program crash. All the cited libraries but Calcium [17] also do not have
an exception mechanism adapted to their skeletons, and an uncaught exception at the local level may lead to
unexpected behaviour.
In Calcium a system has been speciﬁcally designed to handle exceptions, however its principles are diﬀerent.
In the case of Calcium if two processors raise an exception and the catch block is at the global level, only one
exception will be caught: The behaviour is non-deterministic. In the OSL case, both are caught and the code used
to treat these exceptions could be diﬀerent whether one or two exceptions got caught.
BSML [19] and Eden [20] are parallel functional languages that are often used to program algorithmic skeleton
libraries on top of their parallel primitives. Eden being based on Haskell could use a monadic style of error
handling. BSML has an exception mechanism [21] that could beneﬁt to skeletons implemented in BSML [22]. For
the backtracking algorithm, a divide-and-conquer style, for example using the parallel superposition of BSML [23]
could be possible. However no release of BSML allows the usage of exception handling and superposition in
combination.
Manticore [24] combines CML and NESL and has a speciﬁc exception handling mechanism: this mechanism
implements a sequential semantics. Manticore has both a parallel and a sequential semantics, and the exceptions
behave in parallel as they would behave in sequential. Therefore in the case where two exceptions are raised in
parallel, one has always priority over the other and can be caught: The semantics is deterministic.
7. Conclusion and Future Work
The structured nature of skeletal parallelism allowed us to design a parallel exception handling mechanism
suited to the Orl´eans Skeleton Library. Its implementation in C++ raised some challenges that we were able to
tackle, in particular using the Boost framework. The obtained mechanism oﬀers acceptable performance overhead in case no exception is raised. It also provides additional expressive power as exempliﬁed by a parallel
backtracking algorithm.

Joeffrey Legaux et al. / Procedia Computer Science 18 (2013) 260 – 269

Future work on OSL includes hybrid parallel versions of the library. A OpenMP/MPI version may cause no
additional problems concerning exception handling. A CUDA-based GPU version is more problematic because
neither exception handling nor run-time type information (RTTI) are currently supported for device code.
Acknowledgements
This work is partly supported by ANR (France) and JST (Japan) (project PaPDAS ANR-2010-INTB-0205-02
and JST 10102704). The SPEED machine and Joeﬀrey Legaux’s PhD grant are funded by the Conseil G´en´eral du
Loiret.
References
[1] J. B. Goodenough, Exception handling: issues and a proposed notation, Communications of the ACM 18 (12) (1975) 683–696. doi:
10.1145/361227.361230.
[2] A. B. Romanovsky, C. Dony, J. Knudsen, A. Tripathi (Eds.), Advances in Exception Handling Techniques, Vol. 2022 of LNCS, Springer,
2001. doi:10.1007/3-540-45407-1.
[3] A. Romanovsky, J. Kienzle, Action-oriented exception handling in cooperative and competitive concurrent object-oriented systems, in:
Romanovsky et al. [2], pp. 147–164. doi:10.1007/3-540-45407-1_9.
[4] M. Cole, Algorithmic Skeletons: Structured Management of Parallel Computation, MIT Press, 1989, available at http://homepages.
inf.ed.ac.uk/mic/Pubs.
[5] S. J. Deitz, D. Callahan, B. L. Chamberlain, L. Snyder, Global-view abstractions for user-deﬁned reductions and scans, in: PPoPP, ACM,
New York, NY, USA, 2006, pp. 40–47. doi:10.1145/1122971.1122980.
[6] N. Javed, F. Loulergue, Parallel Programming and Performance Predictability with Orl´eans Skeleton Library, in: International Conference on High Performance Computing and Simulation (HPCS), IEEE, 2011, pp. 257–263. doi:10.1109/HPCSim.2011.5999832.
[7] L. G. Valiant, A bridging model for parallel computation, Comm. of the ACM 33 (8) (1990) 103. doi:10.1145/79173.79181.
[8] M. Rintala, Exceptions in remote procedure calls using C++ template metaprogramming, Softw. Pract. Exper. 37 (3) (2007) 231–246.
doi:10.1002/spe.v37:3.
[9] K. Ba´nczyk, T. Boi´nski, H. Krawczyk, Object serialization and remote exception pattern for distributed C++/MPI application, in: PaCT,
LNCS, Springer, Berlin, Heidelberg, 2007, pp. 188–193. doi:10.1007/978-3-540-73940-1_19.
[10] E. Gamma, R. Helm, R. Johnson, J. Vlissides, Design patterns: elements of reusable object-oriented software, Addison-Wesley Longman
Publishing Co., Inc., Boston, MA, USA, 1995.
[11] P. Wadler, Comprehending monads, in: ACM Conference on LISP and functional programming, ACM, New York, NY, USA, 1990, pp.
61–78. doi:10.1145/91556.91592.
[12] H. E. Bal, Fault-tolerant parallel programming in Argus, Concurrency: Pract. Exper. 4 (1) (1992) 37–55. doi:10.1002/cpe.
4330040104.
[13] K. Matsuzaki, H. Iwasaki, K. Emoto, Z. Hu, A Library of Constructive Skeletons for Sequential Style of Parallel Programming, in:
InfoScale’06: Proceedings of the 1st international conference on Scalable information systems, ACM Press, 2006. doi:10.1145/
1146847.1146860.
[14] P. Ciechanowicz, H. Kuchen, Enhancing Muesli’s Data Parallel Skeletons for Multi-core Computer Architectures, in: IEEE International
Conference on High Performance Computing and Communications (HPCC), 2010, pp. 108–113. doi:10.1109/HPCC.2010.23.
[15] M. Danelutto, P. Dazzi, Joint Structured/Unstructured Parallelism Exploitation in Muskel, in: V. Alexandrov, D. van Albada, P. Sloot,
J. Dongarra (Eds.), ICCS, LNCS, Springer, 2006. doi:10.1007/11758525_124.
[16] H. Kuchen, S. Ernsting, Data Parallel Skeletons in Java, in: ICCS, Vol. 9 of Procedia Computer Science, 2012, pp. 1817–1826. doi:
10.1016/j.procs.2012.04.200.
[17] M. Leyton, L. Henrio, J. M. Piquer, Exceptions for algorithmic skeletons, in: P. D’Ambra, M. R. Guarracino, D. Talia (Eds.), 16th
International Euro-Par Conference, LNCS 6272, Springer, 2010, pp. 14–25. doi:10.1007/978-3-642-15291-7_3.
[18] R. Di Cosmo, M. Danelutto, A “minimal disruption” skeleton experiment: seamless map & reduce embedding in OCaml, in: ICCS,
Vol. 9 of Procedia Computer Science, Elsevier, 2012, pp. 1837–1846. doi:10.1016/j.procs.2012.04.202.
[19] F. Loulergue, F. Gava, D. Billiet, Bulk Synchronous Parallel ML: Modular Implementation and Performance Prediction, in: V. S.
Sunderam, G. D. van Albada, P. M. A. Sloot, J. Dongarra (Eds.), ICCS, Vol. 3515 of LNCS, Springer, 2005, pp. 1046–1054. doi:
10.1007/11428848_132.
[20] R. Loogen, Y. Ortega-Mallen, R. Pena-Mari, Parallel Functional Programming in Eden, Journal of Functional Programming 3 (15)
(2005) 431–475. doi:10.1017/S0956796805005526.
[21] L. Gesbert, F. Gava, F. Loulergue, F. Dabrowski, Bulk Synchronous Parallel ML with Exceptions, Future Generation Computer Systems
26 (2010) 486–490. doi:10.1016/j.future.2009.05.021.
[22] F. Gava, S. Tan, Impl´ementation et pr´ediction des performances de squelettes data-parall`eles en utilisant un langage BSP de haut niveau,
in: S. Conchon, A. Mahboubi (Eds.), Journ´ees Francophones des Langages Applicatifs (JFLA), Studia Informatica Universalis, Hermann, 2011, pp. 39–65.
[23] F. Loulergue, Parallel Superposition for Bulk Synchronous Parallel ML, in: P. M. A. Sloot, al. (Eds.), ICCS, Vol. 2659 of LNCS, Springer
Verlag, 2003, pp. 223–232. doi:10.1007/3-540-44863-2_23.
[24] M. Fluet, M. Rainey, J. Reppy, A. Shaw, Implicitly-threaded parallelism in manticore, in: ICFP, ACM, New York, NY, USA, 2008, pp.
119–130. doi:10.1145/1411204.1411224.

269

