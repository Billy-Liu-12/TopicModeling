Available online at www.sciencedirect.com

Procedia Computer Science 4 (2011) 528–537

International Conference on Computational Science, ICCS 2011

Visualization of numerical simulations of astrophysical and fusion
plasmas with the SDvision code
B. Thooris, D. Pomarède
On behalf of the COAST project team
IRFU, Institut de recherche sur les lois fondamentales de l’Univers
CEA Saclay, 91191 Gif-sur-Yvette CEDEX France

Abstract
The new generation of massively parallel mainframes enabled computational physics to make a quantum leap in complexity and
size of numerical simulations, especially in the domain of astrophysics. The COAST project at CEA/IRFU at Saclay, started in
2005, involves astrophysicists and software engineers developing simulation codes in magneto-hydrodynamics and generic tools
for data structuration and visualization. A dedicated software for visualizing the massive amounts of data produced by these
simulation codes has been developed, the SDvision code, deployed in the framework of IDL Object Graphics. This code is
suitable for interactive and immersive navigation for the analysis of 3D results and also for videos and stereoscopic movies
productions for people at large.
In this paper, we present the capabilities of the code SDvision and some applications in the domain of astrophysics simulations
but also in the domain of fusion plasmas studies. In particular, two challenging simulations have been performed in the
framework of the ‘Grands Defis GENCI/CINES’ on recent supercomputer Jade in CINES in 2010 and we present the
visualization studies for such huge computation results.

KEYWORDS: Astrophysics, plasma physics, visualization

1. Introduction
The COAST (for COmputational ASTrophysics) project [1][2][3][4] is a program of massively parallel
numerical simulations in astrophysics, mixing astrophysicists and software engineers of CEA/IRFU institute.
Computational astrophysics is clearly now a major activity in astrophysics and with the increasing computing power
offered by massively parallel mainframes, simulation has become a major tool in the investigation of complex
physical phenomena. The COAST team is developing 3D magneto-hydrodynamics codes suitable for studying
different scales of the Universe. The scientific objective is the understanding of the formation of structures in the
Universe, including the study of large-scale cosmological structures and galaxy formation, turbulence in interstellar
medium, stellar magneto-hydrodynamics and proto-planetary systems.
* Corresponding author. Tel.: +33 169083386; fax: +33 169083147.
E-mail address: thooris@cea.fr.

1877–0509 © 2011 Published by Elsevier Ltd. Open access under CC BY-NC-ND license.
Selection and/or peer-review under responsibility of Prof. Mitsuhisa Sato and Prof. Satoshi Matsuoka
doi:10.1016/j.procs.2011.04.055

B. Thooris et al. / Procedia Computer Science 4 (2011) 528–537

529

Due to the complexity, the geometry or the size of the simulations, the codes are using different numerical
techniques, regular Cartesian meshes or structures such as Adaptive Mesh Refinement, spherical coordinates or
multi-meshes embedded in the geometry. The post-treatment software, and in particular the visualizing software
tool, must fulfil all these requirements, so a visualization code has been developed inside the COAST team: the
SDvision code [5] , which will be described below. The capabilities of this code allowed us to visualize results of
numerical simulation coming from other domains of physics, and the example of fusion plasmas simulations posttreatment is shown also in this paper.

2. The SDvision visualization tool

The visualization plays a very important role in the development of simulations codes. Fundamental aspects
including domain decomposition, initial conditions, message passing and parallelization, treatment of boundary
limits, can be controlled and evaluated qualitatively through visualization. Once in production phase, visualization is
also used for the validation, the analysis and the interpretation of the results. A complete graphical interface named
SDvision has been developed in order to participate in the development of the simulation codes and visualize the
large astrophysical simulation datasets produced in the context of the COAST program.

2.1. SDvision functionalities
The SDvision graphical interface is implemented as an interactive widget as displayed in Fig. 1in its running state. It
benefits from hardware acceleration through its interface to the OpenGL libraries, including GLSL shaders.
SDvision has been developed in the framework of IDL Object Graphics [6][7]. IDL, the Interactive Data Language,
is a firmly-established software for data analysis, visualization and cross platform application development. IDL
provides a set of tools for developing object-oriented applications. A class library of graphics objects allows to
create applications that provide equivalent graphics functionality regardless of the computer platforms.Fig.2. is
giving an example of programming in IDL, an object-oriented based language.

Fig. 1 Layout of the SDvision widget interface.

530

B. Thooris et al. / Procedia Computer Science 4 (2011) 528–537

Fig. 2: An example of IDL programming in the 50000lines code SDvision.

Other powerful visualization codes exist and are widely used in the astrophysics community, for instance VISIT [8],
VAPOR [9] and PARAVIEW [10]. We developed our own tool from scratch using IDL framework mainly for
historical reasons: IDL is the dominant platform for analysis and visualization in the astrophysics community, and as
a consequence, many home format reading and data handling modules were readily available; also, IDL provides
mathematical and scientific libraries which help both simulations visualization and analysis. And even if using IDL
needs licenses, it exits also a virtual machine mechanism for non-licensees users. About data formats, a migration to
a unique HDF5 format is in progress, but specific readers for binary data are still needed.
Three-dimensional scalar and vector fields distributed over regular mesh grids or more complex structures such as
adaptive mesh refinement data or multiple embedded grids, as well as N-body systems, can be visualized in a
number of different, complementary ways. Various implementations of the visualization of the data are
simultaneously proposed, such as 3D iso-surfaces, volume projections, hedgehog and streamline displays, surface
and image of 2D subsets, profile plots, particle clouds. The difficulty inherent to the hybrid nature of the data and
the complexity of the mesh structures used to describe both scalar and vector fields is enhanced by the fact that
simulations are parallelized. Large-scale simulations are conducted on high-performance mainframes with
potentially thousands of processors associated with a non-trivial domain decomposition.
On Fig. 3, we show an example of multi-objects visualization display: scalar fields (baryonic matter density), vector
fields (baryonic velocity field) and particles (Dark Matter particle cloud).

Fig. 3: simultaneous visualization of different cosmological data coming from a RAMSES simulation.

Parallelism is needed for the processing and the visualization of large data sets; some elements of parallelism are
provided in IDL, for example we benefit from a multiple-CPU implementation of the IDLgrVolume class to render
volume by ray-casting. Fig. 4 shows the performance on local machines for producing frames using different
number of nodes; the test was done for this ray-casting algorithm on a 512^3 grid, producing 1024^2 images.

B. Thooris et al. / Procedia Computer Science 4 (2011) 528–537

531

Fig. 4: comparison of frames per second computing performance using different multi-cores local machines.

In order to assess the performances and limitations of the ray-casting algorithms versus the size of the data, a
benchmark test is conducted using a powerful graphics cluster with 512 GB shared memory and four octocore
processors, amounting to 64 logical computing cores. In this benchmark test, a datacube of bytes with size n^3 is
rotated and produce 1024^2 images. The average fps (frames per second) and the RAM used in this process is
presented in Fig. 5. The algorithm stands up to the highest available memory, enabling visualization of grid up to
nearly 8000^3. The frame rate decreases roughly as the inverse of the cube size.

Fig. 5: Performances of the ray-casting algorithm versus the size of the visualized datacube

2.2. AMR processing
The analysis of results from complex MHD and N-body AMR-Octree code for cosmological simulations (See part
3.1) implies two steps of processing as we need Cartesian grids as input for multithreading processing. The
management of the memory is shown on Fig. 6 and an example of data extraction can be found in Fig. 7. The
highest levels of the AMR resolution are reached by successive and synchronous spatial and resolution zooms, using
an interactive definition of the sub-volume in which the AMR extraction is performed. New algorithms are studied
for direct reconstruction of images from the AMR-Octree structures, to avoid using intermediate Cartesian grids.

Fig. 6: Management of the memory in the case of AMR octree code data extraction

532

B. Thooris et al. / Procedia Computer Science 4 (2011) 528–537

Fig. 7: example of images extracted from high level AMR structure

2.3. Multi-grid processing
In order to visualize data resulting from multi-grids codes, such as the Jupiter code[11], a special module has been
developed in SDvision, an example of such image is shown on Fig. 8.

Fig. 8: example of images extracted from a multi-grids code

2.4. Particle clouds processing
The development of SDvision was particularly focused on the visualization of grid data produced by finite volumes
hydrodynamics codes; the particles clouds are treated as mere 3D scatter plots (See Fig. 9 a representation of stars,
simulation performed by M. Martig ) typically in astrophysics for dark matter and stars. Other codes exist such as
TIPSY [12] and SPLOTCH[13] which are based on more refined algorithms using a computation of the local
particle density.
Fig. 9: example of visualization of particle clouds in SDvision showing stars color-coded against their age.

B. Thooris et al. / Procedia Computer Science 4 (2011) 528–537

533

3. Astrophysical simulations visualizations
3.1. Cosmological structures studies
The RAMSES code [14][15][16] is designed as a N-body and hydrodynamical code based on the Adaptive Mesh
Refinement (AMR) technique. Hybrid simulations are performed using the RAMSES code to study cosmological
large scale structures and galaxy formations. RAMSES has been used in the context of the HORIZON [17] Grand
Challenge Simulation at CEA/CCRT on Platine in September 2007, which was the largest ever N-body
cosmological simulation performed. For the first time, have been performed a simulation of half the observable
universe, with enough resolution to describe a Milky Way-like galaxy with more than 100 dark matter particles. The
RAMSES code has been run on 6144 cores, 18 Tb RAM used for 2 months to simulate 70 billion particles. Another
challenge in computing in astrophysics with RAMSES was the HORIZON “galaxy formation” simulation at
MareNostrum [18]. The characteristics of the run are the following: 1024 3 dark matter particles, 4 billions AMR
cells. 2048 processors were needed for computing, 64 processors dedicated to I/O, 3 weeks of computations, 20 Tb
of data generated and stored. The run performed simulations from large scale filaments to galactic discs. The
visualization of such challenging simulations has been described in previous papers.
RAMSES was used more recently in the studies of galaxies formations [19]. A first example of high-resolution
simulation of a galaxy disk is shown in Fig. 10. The image represents the density of the baryon gas in a galaxy disk.
The simulation was performed on 700 processors at CEA/CCRT on Titane by F. Bournaud.
Fig. 10: Visualization of the AMR density field in a simulation of a galactic disk. The AMR structure up to level 14 is projected
in a 4096x4096x328 grid

Another example of high resolution visualization from RAMSES results is the simulation of the Antennae galaxies
formation [20], starting from the collisions of two spiral galaxies. The movement of the galaxies baryonic gas has
been simulated during 500 million years in a region of 600000 light-years on each side. Fig. 11 represents the
galaxies at the second collision time after 450 million years. The 1000 stored time-steps allowed us to perform a
high-resolution video of the simulation using the SDvision capabilities.

534

B. Thooris et al. / Procedia Computer Science 4 (2011) 528–537

Fig. 11: Visualization of Antennae galaxies simulation by D. Chapon, R. Teyssier and F. Bournaud.

3.2. Interstellar Medium Simulations
As another example of astrophysical numerical analysis code, the HERACLES [21][22][23] 3D code is mixing
hydrodynamics and radiative transfer studies on Cartesian grids, using the finite volumes method. The HERACLES
biggest simulation has been performed in the framework of the Grands Défis CINES 2010 on 2500 processors of the
Jade machine in CINES Computing Center in Montpellier, France. The run simulated the Interstellar Medium
turbulences in a 2000x2000x2000 cube, using 8 billion cells. The simulations generated 15To of data, and allowed
high resolution images and videos for stereoscopic visualization systems, thanks to SDvision. Fig. 12 shows the
turbulence of ISM gas in a 2-phases simulation. Fig 13 shows an immersive view of the turbulence of the gas in ISM
in a isothermal simulation

Fig. 12: Visualization of the HERACLES simulation on a 2000x2000x2000 grid.

B. Thooris et al. / Procedia Computer Science 4 (2011) 528–537

535

Fig. 13: Visualization of the HERACLES simulation on a 2000x2000x2000 grid, immersive view.

4. Fusion plasma simulations visualizations
To understand the behavior of the plasmas in the next generation of tokomaks, like ITER, simulations are performed
with the GYSELA code [24][25], developed at CEA/IRFM Cadarache, with the goal to reduce the turbulences for
improving performances in these machines. The GYSELA simulation performed in the framework of the Grands
Défis CINES 2010 was the largest simulation ever realized on the ITER model. The simulation used 272 billion
cells in the 5-dimentional mesh and had run one month on 8192 processors of the Jade machine in CINES. The code
used 27Go by node and generated more than 6To of data. Fig. 14 represents in 3D the temperature fluctuations
during the turbulence inside the plasma on a poloïdal cut of the torus. Fig. 15 represents in 3D the electrostatic
potential fluctuations inside the torus.

Fig. 14: visualization of the poloidal cross section of the simulated ITER plasma

536

B. Thooris et al. / Procedia Computer Science 4 (2011) 528–537

Fig. 15: Immersive view of the simulated ITER plasma inside the torus

5. Perspectives
The SDvision software package, intended primarily for the visualization of massive cosmological simulations,
has been extended to provide an interactive visual representation of different classes of redshift surveys [26]. A first
study has been carried out with the objective to enable direct comparisons between the low statistics X-ray clusters
samples of the XMM-LSS Survey and the high-statistics photometric redshift catalogues of the CFHTLS [27]. The
various possibilities offered by the tool in terms of filtering of the data, reconstruction of density fields, interactivity
and visual rendering, are opening a new domain of collaborations with astrophysics involved in experiments
collecting actual data.
6. Conclusion
If the development of the SDvision visualization code was basically motivated by the need of analyzing the results
(and sometimes detecting computing bugs) from huge amounts of data with complex structures, the production,
thanks to SDvision , of images, videos and stereoscopic movies in the domain of astrophysics simulation have
caused a lot of requests for communication with the general public. Several movies generated by SDvision have
been screened in exhibitions, museums and in our 3D room for visitors at Saclay. A new dedicated room with 100
seats has been equipped in our laboratory for the projection of astrophysical stereoscopic movies generated by
SDvision. Projects are also in progress with planetariums and museums to present on very high resolution screens
the simulations of different scales of the Universe, from cosmology to planet formation.

Acknowledgements
The creation of SDvision is part of the COAST project and has been motivated by the different simulations
performed by the COAST astrophysicists members: we would like to thank R. Teyssier, E. Audit, A.S. Brun,

B. Thooris et al. / Procedia Computer Science 4 (2011) 528–537

537

F. Masset, S. Fromang, F. Bournaud, D. Chapon, L. Delaye, L. Powell, M. Martig, A. Strugarek and al. for
providing their data to the software engineers.
The visualizations presented were obtained using either our local computing resources at CEA/IRFU or the Cesium
graphics cluster supported by the CEA/CCRT.

References
1.
2.

3.

4.

5.

6.
7.

8.
9.
10.
11.
12.
13.
14.
15.
16.

17.
18.

19.
20.
21.
22.
23.
24.
25.
26.
27.

http://irfu.cea.fr/projets/COAST
The COAST Project: a Program of Massively Parallel Numerical Astrophysics Simulations , B.Thooris, E. Audit, A.S. Brun, Y. Fidaali, F.
Masset, D. Pomarède, R. Teyssier, Proceedings of the HPC-Asia 2009 conference ( march 2-5, Kaohsiung, Taiwan), ISBN:978-986-852280-0, p. 642.
Numerical Simulations of Astrophysical Plasmas. D. Pomarède, B. Thooris, E. Audit, R. Teyssier. Proceedings of the 6th IASTED
International Conference on Modelling, Simulations, and Optimization (MSO2006), Gaborone, Botswana, September 11-13, 2006, ed. H.
Nyongesa, 507-058, Acta Press, ISBN:0-88986-618-X
Numerical Simulations of Astrophysical Plasmas: status and perspectives of the Saclay/DAPNIA software project, E.Audit, D.Pomarède,
R.Teyssier, B.Thooris, Proceedings of the First CalSpace-IGPP International Conference on Numerical Modeling of Space Plasma Flows,
ASTRONUM2006, Palm Springs CA, USA, March 27-30,2006, ed. N.V. Pogorelov and G.P. Zank, the Astronomical Society of the Pacific
Conference Series, vol.359 (2006), 9-14, ISBN:978-1-583812-27-3.
Interactive visualization of astrophysical plasma simulations with SDvision, D. Pomarède, Y. Fidaali, E. Audit, A.S. Brun, F. Masset, R.
Teyssier, Proceedings of the IGPP/DAPNIA International Conference on Numerical Modeling of Space Plasma Flows, ASTRONUM2007,
Paris, France, June 11-15, 2008, ASPCS 386 (2008) 327.
http://www.ittvis.com/idl
Visualization of astrophysical simulations using IDL Object Graphics, D. Pomarède, E. Audit, A.S. Brun, V. Gautard, F. Masset, R.
Teyssier, B. Thooris, Proceedings of the Computer Graphics Imaging and Visualization 2007 Conference, CGIV07, Bangkok, Thailand,
August 14-17, 2007, ed. E. Banissi, M. Sarfraz, and N. Dejdumrong, IEEE Computer Society, ISBN 0-7695-2928-3, p.471-480
https://wci.llnl.gov/codes/visit
http://www.vapor.ucar.edu/
http://www.paraview.org/
http://jupiter.in2p3.fr/
http://www-hpcc.astro.washington.edu/tools/tipsy/tipsy.html
High-performance astrophysical visualization using Splotch, Z. Jin et al., Proceedings of ICCS 2010 Conference, Amsterdam, June 2010.
Cosmological hydrodynamics with adaptive mesh refinement - A new high resolution code called RAMSES, R. Teyssier, Astron.
Astrophys., 385 (2002) 337-364
Monitoring and Control of the RAMSES Simulation Program, R. Osmont, D. Pomarède, V. Gautard, R. Teyssier, B. Thooris. Proceedings
of the CCP2007 Conference on Computational Physics, Brussels, Belgium, September 5-8, 2007
Visualization of Hybrid, N-body and Octree-based Adaptive Mesh Resolution Parallelized Simulations , D. Pomarède, Y. Fidaali, R.
Teyssier, Proceedings of 5th IEEE Conference on Computer Graphics, Imaging and Visualization (august 25-28, Penang, Malaysia),
ISBN:978-0-7695-3359-9, p. 295.
http://www.projet-horizon.fr/
A Visual Insight into the Horizon Simulation at MareNostrum, D. Pomarède, Y. Fidaali, R. Teyssier, Proceedings of the IGPP/DAPNIA
International Conference on Numerical Modeling of Space Plasma Flows, ASTRONUM2008, Saint John, US Virgin Islands, June 8-13,
2008, ASPCS 406 (2009) 317.
Hydrodynamics of high-redshift galaxy collisions: From gas-rich disks to dispersion-dominated mergers and compact spheroids, F.
Bournaud et al. , eprint arXiv:1006.4782, June 2010
The Driving Mechanism of Starbusts in Galaxy Mergers. D. Chapon, R. Teyssier, F. Bournaud. The Astrophysical Journal Letters, Vol.
720, Issue 2, pp. L149-L154(2010)
HERACLES: a three-dimensional radiation hydrodynamics code , González M., Audit E., Huynh P., 2007, A&A, 464, 429
Thermal Condensation in a Turbulent Atomic Hydrogen Flow, E. Audit and P. Hennebelle, Astronomy and Astrophysics, 433, 2005, 1-13.
Enabling Tools and Techniques for the Optimization of the HERACLES Simulation Program, E. Audit, V. Gautard, D. Pomarède, B.
Thooris. Proceedings of the 6th EUROSIM Congress, Ljubljana, Slovenia, September 9-13, 2007
http://www-ljk.imag.fr/membres/CHANT/Documents/GRANDGIRARD_IHP.pdf
Global full-f gyrokinetic simulations of plasma turbulence. V. Grandgirard t al. , Plasma Physics and Controlled Fusion, 49(12B) :B173,
2007
Interactive Visualization of 3D Redshift Surveys with SDvision, D. Pomarède, M. Pierre, Proceedings of IAU symposium 277,
Ouagadougou, Burkina Faso, December 2010.
The XMM-LSS cluster sample and its cosmological applications. Prospects for the XMM next decade
Pierre et al., 2008, AN 329, 143 - arXiv/astro-ph 0712.0262

