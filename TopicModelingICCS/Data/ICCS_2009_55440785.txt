Study of Parallel Linear Solvers for Three-Dimensional
Subsurface Flow Problems
Hung V. Nguyen, Jing-Ru C. Cheng, and Robert S. Maier
U.S. Army Engineer Research and Development Center
Major Shared Resource Center
3909 Halls Ferry Road
Vicksburg, MS 39180-6199
{Hung.V.Nguyen,Ruth.C.Cheng,Robert.S.Maier}@usace.army.mil

Abstract. The performance of an iterative method for solving a system of linear
equations depends on the structure of the system to be solved and on the choice
of iterative solvers in combination with preconditioners. In this paper, the
performance of a specified set of linear solvers and preconditioners provided by
PETSc and Hypre is evaluated based on three data sets from subsurface finite
element flow models. The results show that simple preconditioners are robust
but do not enable convergence behavior that scales with problem size. They
also show that it is important to choose an appropriate type of solver for
different kind of simulations.
Keywords: Sparse parallel solvers, iterative solvers, PETSc, Hypre, and finite
element.

1 Introduction
Simulations of groundwater [3] and watershed [1] flow require the solution of a
nonlinear system of partial differential equations (PDE). Discretization of the PDE
spatial domain on a finite-element mesh results in a set of time-dependent, nonlinear
algebraic equations that may be solved by a Newton or Modified Picard algorithm.
These algorithms require the solution of one or more large sparse systems of linear
equations at each time-step, of the form,
Ax = b

(1)

where A=[ai j] is an n ×n matrix and b a given right-hand-side vector.
WASH123 is a simulation code used for modeling time-dependent surface and
subsurface flows, and the coupling between the two flow regimes [2]. WASH123 uses a
modified Picard algorithm to solve the nonlinear problem of three-dimensional (3-D)
subsurface flow, and the resulting linear systems can be shown to be symmetric and
positive definite (SPD) in all cases. Also, the matrix A may vary over the course of a
time-dependent simulation depending on the forcing, coupling, and boundary
conditions. A significant part of the computation time of a WASH123 simulation is
spent solving the linear system [5], [6]. Therefore, the performance of linear solvers is
of great interest.
G. Allen et al. (Eds.): ICCS 2009, Part I, LNCS 5544, pp. 785–794, 2009.
© Springer-Verlag Berlin Heidelberg 2009

786

H.V. Nguyen, J.-R.C. Cheng, and R.S. Maier

Iterative methods are most often used for linear problems involving a large number
of variables because of the memory and scalability restrictions of direct methods, such
as LU decomposition. Krylov subspace methods are an important class of iterative
solution solvers [4], [8]. This class includes the Conjugate Gradient (CG) method,
which is robust for SPD matrices. Tracy and Gavali have previously tested a wide
range of iterative solvers [4] on matrices arising from the FEMWATER code, a
predecessor of WASH123. Their results suggest that CG remains a competitive
method for matrices in the range of a few million unknowns, but leaves open the
question of the most effective preconditioner. In practice, combining a Krylov
subspace method with a preconditioner is essential, especially for an ill-conditioned
linear system. Therefore, this study tests the effectiveness of iterative solvers
combined with various preconditioners to determine those most effective for
subsurface and coupled watershed flow applications.
The convergence criterion for an iterative method is typically tied to the residual,
rk = b -Axk, where superscript k denotes the kth iteration. The PETSc convergence
criterion was used:
||rk||2 < ε ||b||2

(2)

where ε is relative tolerance, ε=10-12 for linear system A and B; ε=10-16 for linear
system C.

2 Numerical Libraries
Two numerical libraries were used in this study: PETSc [4] and Hypre [9]. Both
provide parallel routines for solving large sparse linear systems. PETSc, the Portable,
Extensible Toolkit for Scientific computation, provides a variety of preconditioners
and Krylov subspace solvers. This study used Jacobi, Block Jacobi (BJacobi),
successive overrelaxation (SOR), and the Additive Schwartz method (ASM) as
preconditioners for CG. BJacobi and ASM were implemented with one block per
processor. The individual blocks were solved with ILU(0). Hypre [9] in addition
provides multigrid preconditioning, and the study also used Hypre’s algebraic
multigrid method, boomerAMG, as a preconditioner for CG. The study also
experimented with the Support Tree Conjugate Gradient (STCG) method [7], as
implemented in PETSc [4]. The STCG method is potentially more effective than CG
where A is a generalized Laplacian [7], but relatively little is known about its
performance in practical applications.

3 Test Problems
The choice of the best iterative solver may strongly depend on the grid size, the aspect
ratio of the grid, and physical parameters of the problem. Typically, the convergence
rate of the iterative solver is strongly connected to the condition number of the matrix,
κ(A). Therefore, this study used three sets of data, which try to depict the important
role of these characters.

Study of Parallel Linear Solvers for Three-Dimensional Subsurface Flow Problems

787

3.1 Linear System A
The FEMWATER code [3] was used to create linear system A. The data set is
generated from a pump-and-treat model developed for remediation of contaminated
groundwater. The 3-D mesh contains 102,996 nodes, 187,902 elements, and 8
material types, for a total of 31 layers. FEMWATER uses Picard linearization, which
produces a symmetric, positive-definite (SPD) linear system. The condition number
of the coefficient matrix A, κ(A)= 2.7 x 106.
3.2 Linear System B
The WASH123D watershed code was constructed based on its first-principle,
physics-based mathematical model [2]. In WASH123D, the cross-section-averaged 1D diffusive wave equation, the depth-averaged 2-D diffusive wave equation, and the
3-D Richards equation were solved with semi-Lagrangian for canal network flow and
overland flow, and Galerkin finite element methods for variably saturated subsurface
flow, respectively [2]. Also, the computational domain is discretized with finite
element meshes; each element can be assigned with a different material type to
account for heterogeneity; and each material may have its own set of physical model
parameters. The 3-D subsurface flow in this coupled watershed model is governed by
the Richards equation, solved by Picard linearization, but employs a modular software
approach.
The 3-D mesh contains 59,409 nodes, 84,996 elements, and includes 17 material
types for a total of six layers. Figure 1 shows the 3-D computational mesh of this
linear system B.
Similar to the linear system A above, the matrix B is symmetric and positive
definite, with condition number κ(B)=3.9 x 109; thus this linear system is
ill-conditioned.

Fig. 1. The 3-D computational mesh of data set B

3.3 Linear System C
Linear system C was also generated by the WASH123 code, from the same model as
linear system B, using finer mesh. The 3-D mesh for this system contains 2,124,108

788

H.V. Nguyen, J.-R.C. Cheng, and R.S. Maier

nodes, 4,018,700 elements, and includes 17 material types for a total of 20 layers. The
resulting matrix is symmetric and positive definite.

4 Test Results
A C code was written to read these data sets and then call the PETSc functions to
create a sparse parallel matrix A in AIJ format, a parallel vector b, and independent
solution xtrue, which was used to compare with solution from PETSc and Hypre for
the purpose of verification. The code was compiled and run on a Cray XT4 containing
2,152 compute nodes; each has 2.1GHz AMD Opteron 64-bit quad-core processors
and 8 GBytes of dedicated memory.
Tables 1-3 compare the convergence and performance of different preconditioners
for CG and STCG methods, respectively. As expected, preconditioning can significantly
increase performance of Krylov subspace solvers. Without preconditioning, the solvers
often fail to converge. Both CG and STCG methods using the Jacobi preconditioner are
robust solvers for all three linear systems but not necessarily the most efficient. ASM is
the most effective preconditioner for linear systems B and C. The ASM preconditioner
option –pc_asm_type basic was employed, which uses the full restriction and
interpolation operator. The option –pc_asm_type restrict is the PETSc default, but it
does not converge for linear systems A, B, or C.
Table 1. Solver time, number of iterations, and residual norm using CG (shaded) and STCG
(white) methods for linear systems A (two cores)
System
PC

None
Jacobi
Bjacobi
SOR
ASM

Residual
Norm
X10-9
1348.94
3.22
10.20
4.08
12.59

CG
Iteration

5700
670
210
272
220

Time
(sec)
39.61
4.73
3.98
5.34
5.02

Residual
Norm
X10-9
1348.94
3.22
3.22
4.08
12.59

STCG
Iteration

5700
670
210
272
220

Time
(sec)
41.03
4.90
4.05
5.43
5.08

Table 2. Solver time, number of iterations, and residual norm using CG (shaded) and STCG
(white) methods for linear system B (two cores)
System
PC

None
Jacobi
Bjacobi
SOR
ASM

Residual
Norm
X10-9
NC
9.05
9.05
6.93
18.93

CG
Iteration

NC
20340
20340
18784
2818

Note: NC means not converge.

Time
(sec)
NC
76.58
78.11
173.28
34.66

Residual
Norm
X10-9
NC
9.05
9.05
6.93
18.93

STCG
Iteration

NC
20340
20340
18784
2818

Time
(sec)
NC
78.75
80.62
176.00
35.07

Study of Parallel Linear Solvers for Three-Dimensional Subsurface Flow Problems

789

Table 3. Solver time, number of iterations, and residual norm using CG (shaded) and STCG
(white) methods for linear system C (two cores)
System
PC

None
Jacobi
Bjacobi
SOR
ASM

Residual
Norm
X10-13
NC
1.37
5.50
1.21
6.24

CG
Iteration

NC
105960
47517
57391
30784

Time
(sec)
NC
17123.40
18269.13
22968.26
15994.27

Residual
Norm
X10-13
NC
1.37
5.50
1.21
6.24

STCG
Iteration

NC
105960
47517
57391
30784

Time
(sec)
NC
17836.93
18611.86
23381.96
16220.26

Note: NC means not converge.

Fig. 2. Relative error versus solver time using CG method for linear system A Relative error ||b
–AXk|| / ||b –AX0||

Fig. 3. Relative error versus solver time using CG method for linear system B Relative error ||b
–AXk|| / ||b –AX0||

790

H.V. Nguyen, J.-R.C. Cheng, and R.S. Maier

Fig. 4. Relative error versus solver time using CG method for linear system C Relative error ||b
–AXk|| / ||b –AX0||

Fig. 5. Relative error versus number of iterations using CG method with Jacobi preconditioner
for linear systems A, B, and C. Relative error ||b –AXk|| / ||b –AX0||.

Figures 2-4 compare convergence of the residual norm for different
preconditioners using the CG method. For linear system A, Block-Jacobi is the most
efficient in terms of solver time, followed by ASM, SOR, and Jacobi. However, for
linear systems B, ASM converges in the fewest iterations and requires the least
amount of time, followed by the Block-Jacobi, SOR, and Jacobi. For linear system C,
ASM converges slightly faster than others, followed by Jacobi, Block-Jacobi, and
SOR.
The convergence behavior of the three linear systems is compared in Figure 5 for
the Jacobi preconditioner. The behavior of the three systems follows the same pattern;
an initial, relatively slow rate of convergence is followed by an abrupt change to a
more rapid rate of convergence. The initial stage appears to be approximately linear,
or slightly faster, in the number of iterations, while in the latter stage, convergence is
faster than quadratic. For these linear systems, which are closely related in terms of
the underlying PDE, the initial rates of convergence are very similar, i.e., the error is
reduced at the same rate for each linear system.

Study of Parallel Linear Solvers for Three-Dimensional Subsurface Flow Problems

791

Fig. 6. Solver time using CG and STCG methods combination with Jacobi, Block Jacobi, SOR,
and ASM preconditioners for linear system A

Fig. 7. Solver time using CG and STCG methods combination with Jacobi, Block Jacobi, SOR,
and ASM preconditioners for linear system B

Figures 6-8 show the scaling of solver time with the number of processors using
CG and STCG methods. For linear system A, the performance scales linearly up to
about 16 processors and then flattens out, indicating that fewer than 5000-10,000
nodes per processor results in lower parallel efficiency. The various preconditioners
have similar scaling behavior for system A. For linear system B, performance scales
erratically in the range from one to four processors but scales linearly in the range
from 4 to 32 processors. Beyond 32 processors, the performance flattens out for all
preconditioners except ASM, which arguably demonstrates linear scaling from 4 to
128 processors. The number of blocks used in the Block-Jacobi and ASM
preconditioners depends on the number of processors; because of this, their solver
time running on one processor may be less than that on two, four, or eight processors.

792

H.V. Nguyen, J.-R.C. Cheng, and R.S. Maier

For linear system C, performance does not scale linearly in the range from one to four
processors. This problem is due to on-chip memory contention since four cores share
memory in the same node on the Cray XT4 system. The performance scales linearly
in the range from 4 to 256 processors.
Figure 9 shows the scaling of solver time with the number of processors using
boomerAMG method from Hypre for linear systems A and B. The preliminary results
are based on a residual tolerance of ε=10-7.

Fig. 8. Solver time using CG and STCG methods combination with Jacobi, Block Jacobi, SOR,
and ASM preconditioners for linear system C

Fig. 9. Solver time using boomerAMG method for linear systems A and B

Study of Parallel Linear Solvers for Three-Dimensional Subsurface Flow Problems

793

5 Summary and Future Work
Jacobi-CG appears to be the most robust method for solving moderately illconditioned SPD problems in the range of a few hundred-thousand unknowns.
However, it is well known that the number of CG iterations will not scale linearly
with matrix dimension. As WASH123 problems become larger, the number of JacobiCG iterations required to achieve a fixed error tolerance will grow, causing the
solution time to increase faster than the number of unknowns. This study found that a
multilevel preconditioner was effective for linear system B but was not effective with
the same preconditioner for the closely related system C. The authors are therefore
continuing to experiment with algebraic multigrid and multilevel preconditioners.
Even though the cost per iteration of these preconditioners is relatively high for
problems in the range of a few hundred-thousand unknowns, they may prove to be
more efficient for problems with tens or hundreds of millions of unknowns, where the
number of CG iterations is likely to be prohibitive. The authors also continue to
investigate the convergence of CG in time-dependent problems, where a reasonable
initial guess is available from the previous time-step, and convergence may be
relatively fast.
Acknowledgments. This study was supported by the Department of Defense (DoD)
6.1 Basic research funding in the Civil Works category. It was also supported in part
by an allocation of computer time from the DoD High Performance Computing
Modernization Program.

References
1. Cheng, J.R.C., Hunter, R.M., Cheng, H.P., Richards, D.R., Yeh, G.T.: Parallelization of a
watershed model|Phase III: Coupled 1-dimensional channel, 2-dimensional overland, and 3dimensional subsurface ows. In: Computational Methods in Water Resources XVI,
Copenhagen, Denmark, CMWR CD-ROM, paper 64, June 19-22 (2006)
2. Yeh, G.T., Huang, G., Zhang, F., Cheng, H.P., Lin, H.C., Cheng, J.R., Edris, E.V.,
Richards, D.R.: An integrated media, integrated processes watershed model WASH123D:
Part 1model descriptions and features. In: Computational Methods in Water Resources XVI,
Copenhagen, Denmark, CMWR CD-ROM, paper 29, June 19-22 (2006)
3. Lin, H.J., Richards, D.R., Yeh, J., Cheng, J., Cheng, H., Jones, N.L.: FEMWATER: A
three-dimensional finite element computer model for simulating density dependent flow and
transport, TR CHL-97-12, U.S. Army Engineer Waterways Experiment Station, Vicksburg,
MS (1997)
4. Balay, S., Buschelman, K., Gropp, W.D., Kaushik, D., Knepley, M.G., McInnes, L.C.,
Smith, B.F., Zhang, H.: PETSc (2008), http://www.mcs.anl.gov/petsc
5. Tracy, T.F., Oppe, C.T., Gavali, S.: Testing Parallel Linear Iterative Solvers for Finite
Element Groundwater Flow Problems. NAS Technical report; NAS-07-007 (September
2007)

794

H.V. Nguyen, J.-R.C. Cheng, and R.S. Maier

6. Cheng, C.J.-R., Nguyen, H., Cheng, H.-P(P.), Richards, R.D.: Parallel Performance of the
Coupled Watershed System with Multiple_Spatial Domains and Multiple-Temporal Scales.
In: PARA2008, 9th International Workshop on State-of-the-Art in Scientific and Parallel
Computing, NTNU, Trondheim, Norway, May 13-16 (2008)
7. Gremban, D.K., Miller, L.G., Zagha, M.: PerfoMarco: Performance Evaluation of a New
Parallel Preconditioner. In: IPPS 1995: Proceeding of the 9th International Symposium on
Parallel Processing (1995)
8. Demmel, J.W.: Applied Numerical Linear Algebra. SIAM, Philadelphia (1997)
9. Falgout, R.D., Meier Yang, U.: hypre: A library of high performance preconditioners. In:
Sloot, P.M.A., Tan, C.J.K., Dongarra, J., Hoekstra, A.G. (eds.) ICCS-ComputSci 2002.
LNCS, vol. 2331, pp. 632–641. Springer, Heidelberg (2002); also available as LLNL
Technical Report UCRL-JC-146175

