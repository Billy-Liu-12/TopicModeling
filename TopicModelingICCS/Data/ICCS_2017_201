Available online at www.sciencedirect.com

ScienceDirect
Procedia
Science
(2017) 1008–1018
This
spaceComputer
is reserved
for108C
the Procedia
header, do not use it
This space is reserved for the Procedia header, do not use it
This space is reserved for the Procedia header, do not use it
This space is reserved for the Procedia header, do not use it

International Conference on Computational Science, ICCS 2017, 12-14 June 2017,
Zurich, Switzerland

Optimizing
the SVD
Bidiagonalization
Process
for
aa Batch
Optimizing
Bidiagonalization
Process
for
Optimizing the
the SVD
SVDof
Bidiagonalization
Process
for
a Batch
Batch
Small
Matrices
Optimizing the SVDof
Bidiagonalization
Process
for
a
Batch
Small
Matrices
of
Small
Matrices
2 , Stanimire
Tingxing Dong11 , Azzam of
Haidar
Tomov2 , and Jack Dongarra234
Small
Matrices
Tingxing Dong , Azzam Haidar2 , Stanimire Tomov2 , and Jack Dongarra234
Tingxing Dong1 , Azzam
Haidar2 , Stanimire Tomov2 , and Jack Dongarra234
1 Radeon Technologies Group, AMD, U.S.A
2 , Stanimire Tomov2 , and Jack Dongarra234
12 Radeon
Tingxing Dong1 , Azzam
Haidar
Technologies
Group,
AMD, U.S.A
1 University
of Tennessee,
Knoxville,
USA
Technologies Group, AMD, U.S.A
2 Radeon
3 Oak
University
of Tennessee,
Knoxville,
USAUSA
Ridge National
Laboratory,
Oak Ridge,
12 Radeon
University
of Tennessee,
Knoxville,
USA
3 Oak
Technologies
Group,
AMD,
U.S.A
4 University
Ridge National
Laboratory,
Oak
Ridge,
USA
3 Oak
of
Manchester,
Manchester,
U.K.
2
Ridge
National
Laboratory,
Oak
Ridge,
USA
4 University
UniversityofofManchester,
Tennessee, Manchester,
Knoxville, USA
U.K.
4
tingxing.dong@amd.com{haidar,tomov,dongarra}@icl.utk.edu
3
University of Manchester, Manchester, U.K.
Oak Ridge National Laboratory, Oak Ridge, USA
tingxing.dong@amd.com{haidar,tomov,dongarra}@icl.utk.edu
4 University of Manchester, Manchester, U.K.
tingxing.dong@amd.com{haidar,tomov,dongarra}@icl.utk.edu
tingxing.dong@amd.com{haidar,tomov,dongarra}@icl.utk.edu

Abstract
Abstract
A
challenging class of problems arising in many GPU applications, called batched problems, involves
Abstract
A
challenging
class of problems
in many
GPU applications,
batched
involves
linear
algebra operations
on manyarising
small-sized
matrices.
We designedcalled
batched
BLASproblems,
(Basic Linear
AlA challenging
class of problems
arising
in many
GPU applications,
called
batched
problems,
involves
Abstract
linear
algebra
operations
on
many
small-sized
matrices.
We
designed
batched
BLAS
(Basic
Linear
Algebra
Subroutines)
routines,
and in
particular
the
Level-2
BLAS
GEMV
andbatched
theBLAS
Level-3
BLAS
GEMM
linear
algebra operations
on many
small-sized
matrices.
We
designed
batched
(Basic
Linear
AlA
challenging
class
of
problems
arising
in
many
GPU
applications,
called
problems,
involves
gebra
Subroutines)
routines,
and
in
particular
the
Level-2
BLAS
GEMV
and
the
Level-3
BLAS
GEMM
routines,
to solve
them.
We
proposed
device functions
andWe
big-tile
settings
in the
our
batched
BLAS
design.
gebra Subroutines)
routines,
and insmall-sized
particular
the
Level-2
BLAS
GEMV
and
Level-3
BLAS
GEMM
linear
algebra
operations
on
many
matrices.
designed
batched
BLAS
(Basic
Linear
Alroutines,
to solve
them. We
proposed different
device functions
and
big-tile
settings
in We
our illustrated
batched BLAS
design.
We
adopted
auto-tuning
to optimize
instances
ofBLAS
GEMV
routines.
our GEMM
batched
routines,
to solve
them.
We
proposed
device functions
and
big-tile
settings
in the
ourLevel-3
batchedBLAS
BLAS
design.
gebra
Subroutines)
routines,
and
in
particular
the
Level-2
GEMV
and
We
adopted
auto-tuning
to optimize
different instancesprogressively
of GEMV routines.
WeGPU.
illustrated
our batched
BLAS
approach
tothem.
optimize
batched bi-diagonalization
on a K40c
TheBLAS
optimization
We adopted
auto-tuning
to optimize
different
instancesand
of big-tile
GEMVsettings
routines.
illustrated
our batched
routines,
to solve
We
proposed
device functions
in We
ourGPU.
batched
design.
BLAS
approach
to
optimize
batched
bi-diagonalization
progressively
on
a
K40c
The
optimization
techniques
inauto-tuning
thistopaper
are
applicable
to the other
two-sided
factorizations
asWe
well.
BLAS
approach
optimize
batched bi-diagonalization
progressively
on a K40c
GPU.
The optimization
We
adopted
to
optimize
different
instances
of
GEMV
routines.
illustrated
our
batched
techniques in this paper are applicable to the other two-sided factorizations as well.
techniques
in thistopaper
are applicable
totwo-sided
the otherfactorization
two-sided
factorizations
as well.
BLAS
optimize
bi-diagonalization
progressively
on Singular
a K40c
GPU.
The optimization
Keywords:
accelerators;
algorithms;
Value Problems
©
2017 approach
TheHardware
Authors.
Publishedbatched
by batched;
Elsevier
B.V.
Keywords:
Hardware
accelerators;
batched;
two-sided
factorization
algorithms;
Singular
Value
Problems
techniques
in
this
paper
are
applicable
to
the
other
two-sided
factorizations
as
well.
Peer-review
under responsibility
of the
scientific
committee
of the International
Conference
on Computational
Keywords: Hardware
accelerators;
batched;
two-sided
factorization
algorithms;
Singular Value
Problems Science
Keywords: Hardware accelerators; batched; two-sided factorization algorithms; Singular Value Problems

1 Introduction
1
1 Introduction
Introduction
The
emergence
of multicore and heterogeneous architectures requires many linear algebra algorithms
1
Introduction
The emergence
of multicore and heterogeneous architectures requires many linear algebra algorithms

to
redesignedoftomulticore
take advantage
of accelerators,
such as GPUs.
A particularly
class of
Thebeemergence
and heterogeneous
architectures
requires
many linearchallenging
algebra algorithms
to
be redesigned
to
take
advantage
of accelerators,
such
as of
GPUs.
Aalgebra
particularly
challenging
class
of
problems,
arising
in
numerous
applications,
involves
the
use
linear
operations
on
many
smallto beemergence
redesignedoftomulticore
take advantage
of accelerators,
such as GPUs.
A particularly
class
of
The
and
heterogeneous
architectures
requires
manyoperations
linearchallenging
algebra
algorithms
problems,
arising
in
numerous
applications,
involves
the
use
of
linear
algebra
on
many
smallsized
matrices.
Their
number
can
beofthousands,
evensuch
millions.
For example,
billions
of on
8x8many
andclass
32x32
problems,
arising
in take
numerous
applications,
involves
the
use
of
linear
algebra
operations
smallto
be
redesigned
to
advantage
accelerators,
as
GPUs.
A
particularly
challenging
of
sized
matrices.
Their number
be thousands,
even millions.
For
example,Also,
billions
of 8x8 and
32x32
eigenvalue
problems
need to can
be
solved
in involves
magnetic
resonance
imaging.
thousands
of matrixsized matrices.
Their
number
can
be thousands,
eventhe
millions.
For
example,
billions
of on
8x8many
and
32x32
problems,
arising
in
numerous
applications,
use
of
linear
algebra
operations
smalleigenvalue
problems
need to be solved
in magnetic
resonance
imaging.
Also, thousands
of matrixmatrix
(GEMM)
and matrix-vector
(GEMV)
are computed
in hydrodynamic
with
eigenvalue
problems
need to can
be solved
in magnetic
resonance
imaging.
Also,
thousands
of matrixsized
matrices.
Their
number
beproducts
thousands,
even millions.
For
example,
billions
ofsimulations
8x8 and
32x32
matrix
(GEMM)
and
matrix-vector
products
(GEMV)
are
computed
in
hydrodynamic
simulations
with
Finite
Element
Method
[4].
Here
the
size
of
matrices
increases
with
the
order
of
the
numerical
methods,
matrix
(GEMM)
and
matrix-vector
products
(GEMV)
are
computed
in
hydrodynamic
simulations
with
eigenvalue
problems
need
to
be the
solved
in
magnetic
resonance
imaging.
Also,
thousands
ofmethods,
matrixFinite
Element
Method
[4].
Here
size
of
matrices
increases
with
the
order
of
the
numerical
and
can
range
from
ten
to
a
few
hundred.
GEMM
is
at
the
heart
of
deep
neural
network
(DNN)
compuFinite
Element
Method
[4].
Here
the
size
of
matrices
increases
with
the
order
of
the
numerical
methods,
matrix
andten
matrix-vector
products
(GEMV)
are
in hydrodynamic
simulations
with
and
can(GEMM)
range from
to treating
a few hundred.
GEMM
is atlarge
the computed
heart
of deep
neural
(DNN)
computations,
where
rather
than
convolution
as one
GEMM
problem,
itofisnetwork
much
more
efficient
to
and
can
range from
ten
to
a few
hundred.
GEMM
is atincreases
the heart
of deep
neural
network
(DNN)
compuFinite
Element
Method
[4].
Here
the
size
of
matrices
with
the
order
the
numerical
methods,
tations,
where
rather
than
treating
convolution
as one large
GEMM
problem,
it is much
more
efficient
to
view
it
as
many
small
GEMMs
[2].
In
an
astrophysics
ODE
solver
[10],
multiple
zones
are
simulated,
tations,
where
rather
than
treating
convolution
as
one
large
GEMM
problem,
it
is
much
more
efficient
to
and
can
range
from
tenGEMMs
to a few [2].
hundred.
GEMM
is at the
heart
of deep
neural
network
(DNN)
compuview
it
as
many
small
In
an
astrophysics
ODE
solver
[10],
multiple
zones
are
simulated,
and
each
zone
corresponds
to
a
small
linear
system
solve
based
on
an
LU
factorization
[10].
If
the
view
it
as
many
small
GEMMs
[2].
In
an
astrophysics
ODE
solver
[10],
multiple
zones
are
simulated,
tations,
where
than treating
convolution
as one large
GEMM on
problem,
is much more[10].
efficient
to
and
each
zonerather
corresponds
to a small
linear is
system
solve
an LUitfactorization
matrix
isassymmetric
and
definite,
theIn
problem
reduced
to abased
batched
Cholesky
factorization
[11, If
3].the
In
and
each
zone
corresponds
to a[2].
small
linear
system
solve
based
on[10],
an LU
factorization
[10].
If
the
view
it
many
small
GEMMs
an
astrophysics
ODE
solver
multiple
zones
are
simulated,
matrix
is symmetric
andtodefinite,
theone
problem
reduced
ton*20
a batched
Cholesky where
factorization
[11,
3]. 32
In
[13],
there
are demands
compute
millionis
25x8
and
SVD
n ranges
from
matrix
is symmetric
and definite,
the problem
is
reduced
to abased
batched
Cholesky
factorization
[11,
3].the
In
and
each
zone
corresponds
to a small
linear
system
solve
onproblems,
an LU factorization
[10].
If
[13],
there
are
demands
to
compute
one
million
25x8
and
n*20
SVD
problems,
where
n
ranges
from
32
to
4096.
[13],
there
are
demands
to
compute
one
million
25x8
and
n*20
SVD
problems,
where
n
ranges
from
32
matrix
to 4096.is symmetric and definite, the problem is reduced to a batched Cholesky factorization [11, 3]. In
to
4096.
[13],
there are demands to compute one million 25x8 and n*20 SVD problems, where n ranges from 32
to 4096.
1877-0509 © 2017 The Authors. Published by Elsevier B.V.
Peer-review under responsibility of the scientific committee of the International Conference on Computational Science
10.1016/j.procs.2017.05.237

	

Tingxing
Dong of
et Matrices
al. / Procedia Computer Science 108C (2017)
Optimizing the SVD Bidiagonalization
for Batch
Dong, 1008–1018
Haidar, Tomov and Dongarra

2

Related Work and Contributions

Efficient batched one-sided factorizations (LU, QR, and Cholesky) were developed in [8] [6] [5]. These
factorizations are compute-bound and rich in Level-3 BLAS operations. Therefore, the main effort in
developing them lied in algorithmically enhancing the percentage of Level-3 BLAS operations, using
techniques such as recursive blocking, parallel swapping, etc.
In contrast to the compute-bound one-sided factorizations, we consider memory-bound two-sided
Householder bi-diagonalizations (GEBRD). These routines are needed for Singular Value Decompositions (SVD) in many applications. Instead of BLAS-3 GEMM, the Householder bi-diagonalization
problem is rich in memory-bound Level-2 BLAS GEMV operations. The goal is to develop efficient
Level-2 BLAS that minimizes memory transactions and maximizes bandwidth. To accomplish this,
we propose device functions and big-tile setting techniques to facilitate data reuse. Because there are
various instances of GEMV to solve, we adopt auto-tuning to find the optimal setting for each instance.
Our main focus is on same-sized problems, that is, all the matrices are of identical size. Variablesized BLAS are also considered, though to a lesser extent. Throughout this paper, our batched routines
are named as MAGMA batched routines, and released through the MAGMA library.
Our main contributions are: 1) Design batched BLAS device functions and kernels, as well efficient
implementations and optimization techniques; and 2) Design two-sided bi-diagonalization for batched
execution based on the batched BLAS approach.

3

Background

The SVD problem is to find orthogonal matrices U, V , and a diagonal matrix Σ with nonnegative elements, such that A = UΣV T , where A is an m × n matrix. The diagonal elements of Σ are singular
values of A, the columns of U are called left singular vectors of A, and the columns of V are called right
singular vectors of A. Such problem is solved by a three-phase process:
1. Reduction phase: orthogonal matrices Q and P are applied on both the left and the right side of
A to reduce it to a bi-diagonal matrix – hence these are called “two-sided factorizations.”
2. Solution phase: a singular value solver further computes the singular values Σ and the left and
 and V T of the bi-diagonal matrix;
right vectors U
3. Back transformation phase: if required, the left and the right singular vectors of A are computed
 and V T by the orthogonal matrices Q and P used in the reduction phase.
by multiplying U

It is well known that the first phase is the most time consuming portion of the SVD problem. Benchmarks show that it consists of more than 70% or 90% of the total time when all singular vectors or only
singular values are computed on modern architectures, respectively [9]. For that, we focus in this paper
on the reduction phase for a batch of small problems, and study its limitations.

4

Householder Bi-diagonalization

The bi-diagonalization factorizes A = UBV ∗ , where U and V are orthogonal matrices, and B is bidiagonal with non-zeros only on the diagonal and upper superdiagonal. This is done by the classic stable
Golub-Kahan method that applies a sequence of Householder transformations [7]. Algorithmically, this
corresponds to a sequence of in-place transformations, where A is overwritten by the entries of the bidiagonal matrix B, as well as by the U and V holding the vectors defining the left and right Householder
reflectors, respectively:
2

1009

Optimizing the SVD Bidiagonalization for Batch of Matrices

Dong, Haidar, Tomov and Dongarra

Tingxing Dong et al. / Procedia Computer Science 108C (2017) 1008–1018

1010	



(0)

a
 11
a(0)
 21
 (0)
a31
(0)
a41

(0)

a12
(0)
a22
(0)
a32
(0)
a42

(0)

a13
(0)
a23
(0)
a33
(0)
a43

 
(0)
a14
b
 11
(0) 
 v1
a24 
 
(0)  → 
a34   v1
(0)
a44
v1

(1)

a12
(1)
a22
(1)
a32
(1)
a42

(1)

a13
(1)
a23
(1)
a33
(1)
a43

 
(1)
b11
a14
(1) 

a24 
  v1
(1)  → 
a34   v1
(1)
v1
a44

b12
(2)
a22
(2)
a32
(2)
a42

u1
(2)
a23
(2)
a33
(2)
a43

 
u1
b11
(2)
a24 
 
 v1
(2)  →
a34   v1
(2)
v1
a
44

b12
b22
v2
v2

u1
b23
b33
v3


u1
u2 

b34 
b44

This algorithm is sequential and rich in Level-2 BLAS GEMV routine calls that are applied in every step
for updating the rest of the matrix. The blocked two-phase algorithm is described in Algorithm 1. The
factorization of the panel Ai x, Ai y proceeds in n/nb steps of blocking size nb . Each step is composed of
BLAS and LAPACK routines, e.g., with the Level-3 BLAS GEMM routine used for the trailing matrix
update, and LAPACK’s LABRD routine used for the panel factorization. LABRD is still sequential
and composed of Level-2 BLAS GEMV. LABRD saves Householder transformations in matrices X and
Y , respectively. Once the transformations are accumulated within the panel, they are applied to the
trailing matrix using Level-3 BLAS operations. The blocked algorithm casts half of flops of the original
sequential algorithm from Level-2 BLAS to Level-3 BLAS GEMM operations.
Algorithm 1 Two-phase implementation of the Householder GEBRD algorithm. Without loss of generality, A is assumed n × n. A(i : j, m : n) is the submatrix of A consisting of i-th through j-th row and
m-th through n-th column with 0-based indexing.
for i ∈ {1, 2, 3, . . . , n/nb } do
{Aix = A(i−1)×nb :(n−1),(i−1)×nb :i×nb }
{Aiy = A(i−1)×nb :i×nb ,(i−1)×nb :(n−1) }
{Ci = Ai×nb :(n−1),i×nb :(n−1) }
Panel Factorize LABRD, reduce Aix and Aiy to bi-diagonal form, returns matrices X, Y to update trailing matrix Ci in the next phase,
U, V are stored in factorized A

Trailing Matrix Update Ci = Ci −V ×Y T − X ×U T with gemm
end for

5

Performance Bound Analysis and Roofline Model

In order to evaluate the performance behavior of the reduction to bi-diagonal and to analyze if there
are opportunities for improvements, we present a performance bound analysis and the associated with it
roofline model. Similar to the one-sided factorizations (LU, Cholesky, QR), the two-sided factorizations
(in particular, the bi-diagonal reduction) are split into a panel factorization and a trailing matrix update.
Unlike the one-sided factorizations, the panel factorization requires computing Level-2 BLAS matrixvector products involving the entire trailing matrix. This requires loading the entire trailing matrix into
memory, and thus, incurring a significant amount of memory-bound operations. The application of twosided transformations creates data dependencies and produces artificial synchronization points between
the panel factorization and the trailing submatrix update. This makes it impossible to overlap the panel
and the trailing submatrix update. Therefore, we can model the performance of our algorithm by the
performances of its basic kernels (as they have to be executed in order).
The algorithm proceed by steps of size nb . We give the detailed panel and update costs per step:
• The panel is of size nb columns. The factorization of every column is primarily dominated by two
matrix-vector products with the trailing matrix. Thus, the cost of a panel is 4 nb l 2 + Θ(n), where
l is the size of the trailing matrix at step i. For simplicity, we omit Θ(n) and roundup the cost of
the panel by the cost of the matrix-vector product;
3

	

Optimizing the SVD Bidiagonalization
for Batch
Dong, 1008–1018
Haidar, Tomov and Dongarra
Tingxing
Dong of
et Matrices
al. / Procedia Computer Science 108C (2017)

• The update of the trailing matrix consists of applying the Householder reflectors generated during
the panel factorization to the trailing matrix from both the left and the right side using Level3 BLAS routines: Ai∗nb :n−1,i∗nb :n−1 ← Ai∗nb :n−1,i∗nb :n−1 − V × Y T − X × U T , where V and U
are the Householder reflectors computed during the panel phase, X and Y are two rectangular
matrices needed for the update and also computed during the panel phase. This update phase can
be performed by two matrix-matrix products using the gemm routine and its cost is 2 × 2 nb k2 ,
where k is the size of the trailing matrix at step i.
For all steps (n/nb ), the trailing matrix size varies from n to nb by steps of size nb , where l varies from
n to nb and k varies from (n − nb ) to 2 nb . Thus, the total cost for the n/nb steps is:
n/nb

n−nb
nb

nb

2nb

≈ 4nb ∑ l 2 + 4nb ∑ k2 ≈

4 3
4 3
3 ngemv + 3 ngemm

≈

8 3
3n .

According to the above equation, we derive below the maximum performance Pmax that can be
reached by the bi-diagonal reduction algorithm. In particular, for large matrix sizes n:
Pmax =

f lops
tmin

=

8 n3
3
4 n3 ∗ 1 + 4 n3 ∗ 1
Pgemv 3
Pgemm
3

=

2∗Pgemm ∗Pgemv
Pgemm +Pgemv

< 2Pgemv , when Pgemm >> Pgemv .

(1)

The performance of the Level-2 BLAS routines such as the batched matrix-vector multiplication
(gemv) is memory bound and very low compared to the Level-3 BLAS dgemm. For example, on
a K40c GPU the performance of batched dgemv is about 40 Gflop/s as shown in Figure 3a, while
for batched dgemm it is about 323 Gflop/s as illustrated in Figure 2a. Thus, one can expect from
Equation (1), that the performance of the reduction algorithm is bound by the performance of the Level-2
BLAS operations. This explains the well known low performance behavior observed for the algorithm.

6

Batched BLAS Design and Implementation for GPUs

In a batched problem that is based on batched BLAS, many small dense matrices must be factorized
simultaneously, meaning that all the matrices will be processed simultaneously by the same kernel.

6.1

Two-level Parallelism and Device-kernel Mode

Our batched BLAS kernels do not make any assumption about the layout of the matrices in memory,
e.g., they are not necessarily stored continuously. The starting addresses of every matrix is stored in
an array of pointers, and the batched kernel takes the array of pointers as input. Note that to use the
array of pointers interface extra memory must be allocated as workspace, compared to the assumption
of consecutive matrix storage. Inside the kernel, each matrix is assigned to a unique batch ID and
processed by one device function. Device functions are low-level and callable only by CUDA kernels.
The device function only sees a matrix by the batched ID and thus still maintains the same interface
as the standard BLAS. Moreover, we use multiple CUDA threads per matrix factorization, which is
different from [14], where only one thread is used. Thus, our batched BLAS is characterized by two
levels of parallelism. The first level is a task-level parallelism among the independent matrices that are
simultaneously processed. The second is fine-grained data parallelism within the computation of each
matrix and its goal is to exploit the SIMT architecture of the GPU through device functions.
The device function is templated with CUDA C++. A number of tunable parameters are selected
– thread blocks size, tile size, etc.; see Section 7.2 – and stored in C++ template parameters. The use
4

1011

1012	

Optimizing the SVD Bidiagonalization
for Batch
Dong, 1008–1018
Haidar, Tomov and Dongarra
Tingxing
Dongof
et Matrices
al. / Procedia Computer Science 108C (2017)

of device functions brings multiple advantages. First, merging kernels can reduce kernel launching
overhead but usually demodulize the BLAS-based structure of LAPACK algorithm. However, device
functions preserve the BLAS interface. Multiple small workload device function can be merged in
one kernel easily but with the BLAS-based LAPACK algorithm structure still gracefully maintained.
Second, since shared memory is alive per kernel life time, multiple device functions can access the
same shared memory to improve data reuse. Merging kernels and data reuse is important to GEBRD.
They are possible because the panel factorization stage has many small computations that if merged
have a good possibility of data reuse, e.g., in reusing the Householder vector. In order to reuse data
in shared memory, we propose a big-tile setting which will be described in the next section. Third,
if the underlying computation is the same, only one copy of device function is maintained for different
kernels. Figure 1 shows that the same GEMV device function can be called by different types of kernels,
standard GEMV (targeting a large matrix instead of many small ones), batched GEMV, LABRD, and
TRSV kernels. Each type of kernel requires optimization accordingly. We use auto-tuning techniques
(see Section 7.2) to find the optimal setting for each particular kernel as shown in the figure.

Figure 1: The same GEMV device function is called by various kernels.

6.2

Data Reuse and Degrees of Parallelism

An important optimization technique in CUDA programming is to load frequently accessed data in
shared memory to perform computations as much as possible before storing back results to the GPU
main memory. However, shared memory is private per thread block. When solving just one large matrix
problem, the matrix is divided into tiles with each tile loaded in shared memory. Different thread blocks
access different tiles in an order determined by the algorithm. Synchronization of the computation of
the tiles is accomplished by ending and re-launching kernels. When one kernel exits, the resulting data
in shared memory must be stored back to the GPU main memory as the shared memory will be flushed.
However, in small-sized batched problems, too many kernel launches should be avoided, especially in
the panel factorization, where each routine has a small workload, and a high probability of data reuse
exists in shared memory (if kernels are merged).
In our design, each matrix is assigned to a thread block. The synchronization is accomplished by
barriers inside the thread block. We call this setting big-tile setting. The naming is from this observation:
if the tile is big enough (e.g., 10,000, that is well beyond the scope of a batched problem) that the whole
matrix is inside the tile, it reduces to the point that one thread block accesses the whole matrix.
However, compared to the big-tile setting, the classic setting with multiple thread blocks processing
one matrix has a higher degree of parallelism as different parts of the matrix are processed simultane5

	

Optimizing the SVD Bidiagonalization
for Batch
Dong, 1008–1018
Haidar, Tomov and Dongarra
Tingxing
Dong of
et Matrices
al. / Procedia Computer Science 108C (2017)

ously, especially for large square matrices. Thus, overall, there is a trade-off. Big-tile setting allows data
to be reused through shared memory, but suffers a lower degree of parallelism. The classic setting has a
higher degree of parallelism, but may lose the data reuse benefits. The optimal setting depends on many
factors, including the algorithm type and matrix size, and is often selected by auto-tuning (as in Section
7.2). Our experience shows that for the panel factorization, the big-tile setting has advantage. While for
the trailing matrix update with GEMM computation, the classic setting is preferred.

6.3

Batched Bi-diagonalization Implementations on GPUs

One approach to the batched problems is to consider that the entire matrix is small enough to fit into
the 48KB shared memory per streaming multiprocessor (SMX) for the high-end NVIDIA K40 (Kepler)
GPUs. However, completely saturating the shared memory per SMX can decrease the performance of
memory-bound routines since only one thread-block will be mapped to that SMX at a time, resulting in a
very low occupancy, and subsequently poor core utilization. The advantages of multiple blocks residing
on the same SMX is that the scheduler can swap out a thread block waiting for data from memory and
push in the next block that is ready to execute [15]. We found that using a small amount of shared
memory per kernel (less than 10KB) provides an acceptable data reuse and reasonable occupancy.
For good performance of Level-3 BLAS in trailing matrix updates, panel width nb needs to be
increased. Yet, increasing nb increases tension as the panel is a sequential operation – a larger panel
width results in larger Amdahl’s sequential fraction which governs the maximum speedup. The best
panel size is by balancing the two factors and obtained by tuning. We discovered empirically that
the best value of nb for one-sided factorizations is 32 [8] [6] [5]. However, 16 or 8 is optimal for
the two-sided bi-diagonalization. A smaller nb is better because the panel operations in the two-sided
factorizations are more significant (50 %) than the panel operations in the one-sided factorizations.
GEBRD panel with LABRD: This provides the batched equivalent of LAPACK’s LABRD routine
that reduces the first nb rows and columns of an m by n matrix A to upper or lower real bi-diagonal
form by Householder transformations, and returns the matrices X and Y that later are used to apply
the transformation to the unreduced trailing matrix. It consists of nb steps where each step calls two
routines generating Householder reflectors (LARFG), one for column and one for row Householder
reflector, and a set of GEMV and scaling SCAL routines. The LARFG involves a norm computation
followed by a SCAL that uses the results of the norm computation in addition to some underflow/overflow checking. The norm computation is a sum reduce and thus a synchronization step. To accelerate
it, we implemented a parallel tree reduction. The Householder reflectors are frequently accessed and
are loaded in shared memory. A set of GEMV routines are called to update the rest of the panel and
matrices X and Y . Since there are nb steps, these routines are called nb times; thus, one can expect that
the performance depends on the performances of Level-2 and Level-1 BLAS operations. Hence, it is a
slow, memory-bound routine.
Trailing matrix updates with GEMM: The update is achieved by two GEMMs with the matrices
X and Y returned from the panel factorization. The first one is a GEMM with a non-transpose matrix
and a transpose matrix (A = A −V ∗Y  ), followed by another GEMM with a non-transpose matrix and
a non-transpose matrix (A = A − X ∗U  ) . The update is directly applied on trailing matrix A. However,
for very small matrices it might be still difficult to extract performance from Level-3 BLAS kernels.

7

Auto-tuning

The efforts of maximizing the performance of BLAS, especially GEMM, generally fall into two directions: writing assembly code and source level code tuning. The vendor libraries (e.g., Intel MKL, AMD
6

1013

1014	

Optimizing the SVD Bidiagonalization
for Batch
Dong, 1008–1018
Haidar, Tomov and Dongarra
Tingxing
Dong of
et Matrices
al. / Procedia Computer Science 108C (2017)

ACML, NVIDIA CUBLAS) supply their own routines on their hardware. To achieve performance, the
GEMM routine is implemented in assembly code. The assembly code usually delivers high performance. A disadvantage is that it is highly architecture-specific. The vendors maintain the performance
portability across different generations of their architectures [16]. Another direction is to explore the
source level code auto-tuning to achieve optimal performance (within a preset kernel design space).
Different from assembly code, source code auto-tuning relies on the compilers to allocate registers and
schedule instructions. The advantage is that source code is architecturally independent and is easy to
maintain. Our effort focuses on source code auto-tuning.

7.1

Batched Level-3 BLAS GEMM

The performance of linear algebra routines highly relies on the Level-3 BLAS GEMM. Our batched
GEMM is modified from the standard MAGMA GEMM [12]. We template the batched GEMM routine.
The template parameters include the number of threads, the size of shared memory, and the data tile
size. The combination of these parameters produces a large search space, which can be powerfully
pruned by constraints then. The derived constraints of the search space include correctness, as well as
hardware constraints and soft constraints. Hardware constraints stem from the realities of the accelerator
architecture, like registers and shared memory size. Invalid kernels violating the hardware requirement
(like over 48KB shared memory) will be discarded. The constraints may be soft in terms of performance.
We require at least 512 threads per GPU SM to ensure a reasonable occupancy. More details about tuning
batched GEMM can be found in [1].
Figure 2a shows our batched DGEMM (denoted as the MAGMA batched) performance against
other solutions after auto-tuning. The number of matrices is 400. The best CPU solution is to parallelize with 16 OpenMP threads on a 16-core Sandy Bridge CPU. Its performance is stable around 100
Gflop/s. The non-batched GPU solution is a loop over the 400 matrices by calling standard GEMM
routine, where the GPU sequentially processes each matrix and relies on the multi-threading per matrix to achieve performance. The non-batched curve linearly grows below size 320 and catches up with
CUBLAS batched GEMM around size 448. Our MAGMA batched GEMM outperforms other solutions.
It is 75 Gflop/s or 30% faster than CUBLAS on average and more than 3× faster than the CPU solution.
The performance fluctuation is caused by the blocking size. When the performance is on peak, all the
threads participate without branching. The trailing matrix always starts from an aligned address (a new
allocated matrix is always aligned in memory) with a step blocking size nb . Therefore, our GEMM
operation runs on the peak performance as much as possible. e always chose the appropriate blocking
nb to let GEMM Figure 2b also show the autotuning of the DGEMV routine for wide matrices meaning
matrices with small number of row (e.g., corresponding to the panel size of GEBRD) and large number
of colunms.

7.2

Different Batched Level 2 BLAS GEMV instances Tuning

In matrix-vector multiplication using a non-transpose matrix (GEMVN), a reduction is performed per
row. Each thread is assigned to a row and a warp of threads is assigned to a column. Each thread iterates
row-wise in a loop and naturally owns the reduction result. Since matrices are stored in column-major
format, the data access in GEMVN by the warp is in a coalescing manner.
However, in GEMV using a transpose matrix (GEMVT), the reduction must be performed on each
column. Assigning a thread to a column will make the reduction easy, but will lead to memory access
in a striding way. To overcome the non-coalescing problem in GEMVT, a two-dimension thread block
configuration is adopted. Threads in x-dimension are assigned per row. These threads access data rowwise to avoid the memory non-coalescing penalty. A loop of these threads over the column is required
7

Optimizing the SVD Bidiagonalization
for Batch
Dong, 1008–1018
Haidar, Tomov and Dongarra
Tingxing
Dong of
et Matrices
al. / Procedia Computer Science 108C (2017)

in order to do the column reduction in GEMVT. Partial results owned by each thread are accumulated
in every step of the loop. At the final stage, a tree reduction among the threads is performed to obtain
the final result, similar to MPI REDUCE.
Threads in y-dimension are assigned per column. A outside loop is required to finish all the columns.
Threads in x-dimension ensure the data access is in a coalescing pattern. Threads in y-dimension preserve the degree of parallelism, especially for the wide matrix (or called fat matrix, with both terms
being interchangeable throughout this paper) where the parallelism is more critical to performance.
dgemv batched fat16
20

15

Gflops

	

10

5

0

Performance bound
Magma id5:8 16 10000
Magma id6:8 32 10000
Magma id4:8 8 10000
Magma id7:8 64 10000
Magma id12:16 16 10000
0

32

64

96

128

160

192

224

256

288

320

352

384

416

448

480

512

matrix size

(a) Performance of our batched DGEMM (K=32) vs. other so-(b) Tuning results of batched DGEMV for a wide matrix where m =
lutions on CPUs or GPUs.
16 and n is reported in the ”x” axis.

Figure 2: Performance of the main batched BLAS routines.

8

Performance

We conducted our experiments on a multicore system with two 8-cores socket Intel Xeon E5-2670
(Sandy Bridge) processors with each running at 2.6 GHz. Each socket has a shared 20 MB L3 cache,
and each core has a private 256 KB L2 and a 64 KB L1 cache. The system is equipped with 64 GB
of memory and the theoretical peak in double precision is 20.8 Gflop/s per core, i.e., 332.8 Glop/s in
total for the two sockets. It is also equipped with an NVIDIA K40c GPU with 11.6 GB GDDR memory
per card running at 825 MHz. The theoretical peak in double precision is 1, 430 Gflop/s. The GPU is
connected to the CPU via PCIe I/O hubs with 6 GB/s bandwidth.
In our testing, we assume the data already resided in the processor’s memory. Unless explicitly
noted, the memory transfer time between processors is not considered. We believe this is a reasonable
assumption since the matrices are usually generated and processed on the same processor. For example,
in the high order FEMs, each zone assembles one matrix on the GPU. The conjugation is performed
immediately, followed by a batched GEMM. All the data is generated and computed on the GPU [4].

8.1

Performance study and Optimization of the bidiagonalization

Since the performance of the batched GEMV on K40c is around 40 Gflop/s (as shown in Figure 3a), the
GEBRD roofline bound is 70 Gflop/s according to equation (1).
Figure 4a demonstrates the performance improvement progress of our implementation. The nonblocked version purely composed of Level 2 BLAS operations does not scale any more after size 256.
8

1015

Optimizing the SVD Bidiagonalization
for Batch
Dong, 1008–1018
Haidar, Tomov and Dongarra
Tingxing
Dong of
et Matrices
al. / Procedia Computer Science 108C (2017)

The first non-optimized blocked version v1 follows the LAPACK’s two-phase implementation as depicted in Algorithm 1 in which the trailing matrix is updated with Level 3 BLAS operations. Additional
memory allocation overhead has to be introduced in order to use the array of pointers interfaces in the
blocked algorithm. Below size 224, the performance of version v1 is even slower than the non-blocked
due to the overhead. Beyond 224, it starts to grow steadily because of GEMM performance. Each
GEMV routine is a device function in v1. The main issue of v1 is that the GEMVs are not optimized
for instances required by the GEBRD. By tuning these GEMVs, as described in Section 7.2, the performance is immediately doubled in version v2. These GEMV routines are called in the form of device
functions in the panel factorization kernel. The column/row vector of Householder reflectors and the
to-be-updated column in matrices X and Y are repeatedly accessed at each step. We load them into fast
on-chip shared memory. In order to reuse and synchronize data in shared memory, one matrix cannot
span multiple thread blocks, and the big-tile setting has to be used for the GEMV device functions in v2.
As discussed in Section 6.2, there is a trade-off between data reuse (with big-tile setting) and the degree
of parallelism (with classic setting). We found there is a switch over at size 128 for the two settings.
We adopt classic setting beyond size 128 and big-tile for size less than 128 for square instances. The
big-tile setting is still adopted for other wide/tall instances because the data caching proves to be more
important. By this switch-over, the performance of version 3 boosts to 50 Gflop/s from 40 Gflops in v2
at size 512.

Memory	Bandwidth	and	Transac5ons	(10^6)	
600	
500	

(30)	

Pad	
Mis-aligned	

(49)	

400	
GB/s	

1016	

300	
(58)	

200	

(64)	

(71)	 (92)	

100	
0	

L1	

L2	

Device	Mem	

(a) Performance of batched DGEMV(transpose) in three situa- (b) Number of transactions (on top of the bar, in millions) and
tions: aligned, mis-aligned, and pad.
achieved bandwidth of the y axis.

Figure 3: Effect of the padding.
By profiling the GEMV time in GEBRD step by step, we find it does not match the optimal performance obtained in our auto-tuning. In Figure 3a, the blue curves depicts the performance of GEMV
transpose of double precision with every matrix being aligned in memory. However, when the algorithm iterates the sub-matrix as in GEBRD factorization, the starting address may not be aligned (green
curve). The performance curve fluctuates because when the starting address of the sub-matrix is aligned
in memory, the peak performance is reached; otherwise, it drops drastically. The fluctuation is more
serious for bigger matrices since most threads are mis-aligned as more threads are used in large size.
To overcome the fluctuation issue, we adopt a padding technique. The starting thread always reads
from the recent upper aligned address. It introduces extra data reading. The extra reading is up to 15
elements per row because 16 threads fit in an aligned 128-byte segment as a double element is of 8
byte. Although more data is read, it is coalescing that the 128-byte segment can be fetched by only
one transaction. Overall the number of memory transactions is reduced as shown in Figure 3b. Since
the number of memory transactions decreases, the bandwidth is improved accordingly. By padding
9

	

Optimizing the SVD Bidiagonalization
for Batch
Dong, 1008–1018
Haidar, Tomov and Dongarra
Tingxing
Dongof
et Matrices
al. / Procedia Computer Science 108C (2017)

(a) Progress of different versions on a K40c GPU.

(b) Performance on Kepler (K40c) and Pascal P100 GPU.

Figure 4: Performance of batched dgebrd.
elements in the multiplied vector as zeros, extra results are computed but finally discarded in the writing
stage. Figure 3a show that our padding technique enables the GEMV in the GEBRD algorithm to run at
a speed close to the aligned speed. By padding, version 4 reaches 56 Gflop/s at size 512 which is 80%
of the upper bound of the performance.
Our original development targeted Kepler GPUs. The recent Pascal GP100-PCIe GPU (16GB)
has 732 GB/s bandwidth HBM2 memory, which is 2.5× improvement over the Kepler K40c’s 288
GB/s GDDR5 memory. Testing our memory-bound GEBRD (v4) on the Pascal GPU, even without any
tuning, reveals a 3-3.5× speedup. This demonstrates that although we targeted a previous generation
GPU, our batched approach is scalable with the future GPU architecture.

9

Acknowledgment

This research was supported by the Exascale Computing Project (17-SC-20-SC), a collaborative effort of the U.S.
Department of Energy Office of Science and the National Nuclear Security Administration. The work was also
partially supported by Nvidia and NSF under grant #1514406.

10

Conclusions and Future Work

GPU improvements have been observed extensively on large dense and sparse linear algebra problems which have
more data parallelism. Small problems, taking advantage of CPU cache reuse, can be implemented relatively easily
for multicore CPUs. On the other hand, the development of small problems on GPUs is not straightforward. We
demonstrated that with a batched approach, small problems can have an advantage over CPUs, as well.
We consider a batched two-sided bi-diagonalization based on the batched BLAS approach. We first adopt
optimal blocking algorithm to maximize the GPU-friendly GEMM operations. We then propose device functions
as the underlying components of batched BLAS kernels. The use of device functions allows the data to be reused
through shared memory and avoids multiple small kernel launches, but without demodulizing the BLAS-based
LAPACK algorithm structure at the same time. The device functions are CUDA C++ templated. Auto-tuning is used
to help find the optimal template setting for different types of kernels, like standard, batched, or different instances
for a type of kernel, like transpose, wide, slim for GEMV. Because the bi-diagonalization algorithm accesses the
matrix by offset leading to mis-aligned memory access in GEMV kernels, we adopt padding to overcome the
performance fluctuation issue. We demonstrate that variable sized batched BLAS can be easily extended from
uniform sized problems by using device function. We achieved 56 Gflop/s, 80% of the upper bound performance
for the bi-diagonalization problem on a Kepler K40c GPU. By testing on Pascal GPU, we show that our approach

10

1017

1018	

Optimizing the SVD Bidiagonalization
for Batch
Dong, 1008–1018
Haidar, Tomov and Dongarra
Tingxing
Dongof
et Matrices
al. / Procedia Computer Science 108C (2017)

has very good scalability on different architectures.
The methods in this paper can be applied to other two-sided factorizations, e.g., the Hessenberg reduction
(GEHRD) and the tri-diagonalization (SYTRD), as well. In addition to the GEMV demonstrated here, GEHRD
and SYTRD have BLAS-2 TRMV (triangular matrix-vector multiplication) and SYMV (symmetric matrix-vector
multiplication) operations, respectively. The same optimization techniques of GEMV are applicable to TRMV and
SYMV.

References
[1] A. Abdelfattah, A. Haidar, S. Tomov, and J. J. Dongarra. Performance, design, and autotuning of batched
GEMM for gpus. In High Performance Computing - 31st International Conference, ISC High Performance
2016, Frankfurt, Germany, June 19-23, 2016, Proceedings, pages 21–38, 2016.
[2] L. Brown.
Accelerate machine learning with the cudnn deep neural network
library,
2015.
at
http://devblogs.nvidia.com/parallelforall/
accelerate-machine-learning-cudnn-deep-neural-network-library/.
[3] N. Corporation. https://devtalk.nvidia.com/default/topic/527289/ help-with-gpu-cholesky-factorization-/.
[4] T. Dong, V. Dobrev, T. Kolev, R. Rieben, S. Tomov, and J. Dongarra. A step towards energy efficient computing: Redesigning a hydrodynamic application on CPU-GPU. In IEEE 28th International Parallel Distributed
Processing Symposium (IPDPS), 2014.
[5] T. Dong, A. Haidar, P. Luszczek, A. Harris, S. Tomov, and J. Dongarra. LU Factorization of Small Matrices:
Accelerating Batched DGETRF on the GPU. In 16th IEEE International Conference on High Performance
and Communications (HPCC 2014), August 2014.
[6] T. Dong, A. Haidar, S. Tomov, and J. Dongarra. A fast batched cholesky factorization on a GPU. In 43rd
International Conference on Parallel Processing, ICPP 2014, Minneapolis, MN, USA, September 9-12, 2014,
pages 432–440, 2014.
[7] G. Golub and W. Kahan. Calculating the singular values and pseudo-inverse of a matrix. 1965.
[8] A. Haidar, T. T. Dong, S. Tomov, P. Luszczek, and J. Dongarra. A framework for batched and gpu-resident
factorization algorithms applied to block householder transformations. In High Performance Computing - 30th
International Conference, ISC High Performance 2015, Frankfurt, Germany, July 12-16, 2015, Proceedings.
[9] H. Ltaief, P. Luszczek, and J. J. Dongarra. High performance bidiagonal reduction using tile algorithms on
homogeneous multicore architectures. ACM Transactions on Mathematical Software, 39(3), May 2013.
[10] O. Messer, J. Harris, S. Parete-Koon, and M. Chertkow. Multicore and accelerator development for a
leadership-class stellar astrophysics code. In Proceedings of ”PARA 2012: State-of-the-Art in Scientific and
Parallel Computing.”, 2012.
[11] J. Molero, E. Garzón, I. Garcı́a, E. Quintana-Ortı́, and A. Plaza. Poster: A batched Cholesky solver for local
RX anomaly detection on GPUs, 2013. PUMPS.
[12] R. Nath, S. Tomov, and J. Dongarra. An improved magma gemm for fermi graphics processing units. Int. J.
High Perform. Comput. Appl., 24(4):511–515, Nov. 2010.
[13] Batched svd, 2015. at https://devtalk.nvidia.com/default/topic/851534/batched-svd-/.
[14] V. Oreste, M. Fatica, N. A. Gawande, and A. Tumeo. Power/performance trade-offs of small batched LU
based solvers on GPUs. In 19th International Conference on Parallel Processing, Euro-Par 2013, volume
8097 of Lecture Notes in Computer Science, pages 813–825, Aachen, Germany, August 26-30 2013.
[15] B. Rymut and B. Kwolek. Real-time multiview human body tracking using gpu-accelerated pso. In Int.
Conf. on Parallel Processing and Applied Mathematics (PPAM 2013), Lecture Notes in Computer Science.
Springer-Verlag, Berlin, Heidelberg, 2014.
[16] Q. Wang, X. Zhang, Y. Zhang, and Q. Yi. Augem: Automatically generate high performance dense linear
algebra kernels on x86 cpus. In Proceedings of the International Conference on High Performance Computing,
Networking, Storage and Analysis, SC ’13, pages 25:1–25:12, New York, NY, USA, 2013. ACM.

11

