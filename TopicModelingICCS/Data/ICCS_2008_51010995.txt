KCK-Means: A Clustering Method Based on Kernel
Canonical Correlation Analysis
Chuan-Liang Chen1, Yun-Chao Gong2, and Ying-Jie Tian3,∗
1

Department of Computer Science, Beijing Normal University, Beijing 100875, China
2
Software Institute, Nanjing University, Nanjing, China
3
Research Centre on Fictitious Economy & Data Science, Chinese Academy of Sciences,
100080, Beijing, China
C.L.Chen86@gmail.com, gyc05@software.nju.edu.cn,
tianyingjie1213@163.com

Abstract. Kernel Canonical Correlation Analysis (KCCA) is a technique that
can extract common features from a pair of multivariate data, which may assist
in mining the ground truth hidden in the data. In this paper, a novel partitioning
clustering method called KCK-means is proposed based on KCCA. We also
show that KCK-means can not only be run on two-view data sets, but also it
performs excellently on single-view data sets. KCK-means can deal with both
binary-class and multi-class clustering tasks very well. Experiments with three
evaluation metrics are also presented, the results of which reflect the promising
performance of KCK-means.
Keywords: Kernel Canonical Correlation Analysis, K-means clustering,
Similarity Measure, Clustering Algorithm.

1 Introduction
Clustering is one of the most commonly techniques which is widely applied to extract
knowledge, especially when lacking any a priori information (e.g., statistical models)
about the data. Generally, the problem of clustering deals with partitioning a data set
consisting of n points embedded in m-dimensional space into k distinct set of clusters,
such that the data points within the same cluster are more similar to each other than to
data points in other clusters [3]. There are two main approaches of clustering
algorithms, hierarchical (e.g., agglomerative methods) and partitional approaches
(e.g., k-means, k-medoids, and EM). Most of these clustering algorithms are based on
elementary distance properties of the instance space [4].
In some interesting application domains, instances are represented by attributes
that can naturally be split into two subsets, either of which suffices for learning [5],
such as web pages which can be classified based on their content as well as based on
the anchor texts of inbound hyperlinks. Intuitively, there may be some projections in
these two views which should have strong correlation with the ground truth. Kernel
∗

Corresponding author.

M. Bubak et al. (Eds.): ICCS 2008, Part I, LNCS 5101, pp. 995–1004, 2008.
© Springer-Verlag Berlin Heidelberg 2008

996

C.-L. Chen, Y.-C. Gong, and Y.-J. Tian

Canonical Correlation Analysis (KCCA) is such a technique that can extract common
features from a pair of multivariate data, which can be used as a statistical tool to
identify the correlated projections between two views. Therefore, KCCA is expected
to be used to measure the similarity between data points excellently. In this paper, we
propose two algorithms based on KCCA which can improve the performances of
traditional clustering algorithms—K-means, namely KCK-means for two-view data
sets and single-view data sets that could not be split naturally. The results of
experiments show that their performances are much better than those of the original
algorithms. Our empirical study shows that these two algorithms can not only perform
excellently on both two-view and single-view data, but also be able to extract better
quality clusters than traditional algorithms.
The remainder of this paper is organized as follows. We demonstrate KCCA and
propose the algorithms in Sect. 2. Performance measures, experiment results and their
analysis are presented in Sect. 3. Finally, Sect. 4 presents the main conclusions.

2 KCK-Means Method
2.1 Canonical Correlation Analysis
Firstly, we briefly review Canonical Correlation Analysis (CCA), then its kernel
extension—Kernel Canonical Correlation Analysis (KCCA).
CCA is computationally an eigenvector problem. It attempts to find two sets of
basis vectors, one for each view, such that the correlation between the projections of
these two views into the basis vectors are maximized. Let X = {x1, x2, … , xl} and Y =
{y1, y2, … , yl} denote two views, i.e. two attribute sets describing the data. CCA finds
projection vectors wx and wy such that the correlation coefficient between wTx X and

wTy Y is maximized. That is [12],
⎛

⎞
⎟ ,
⎜ wT C w wT C w ⎟
x
xx x y
yy y ⎠
⎝
wTx Cxy wy

ρ = arg max ⎜
wx , wy

(1)

⎧⎪ w Cxx wx = 1
,
w.r.t ⎨
⎪⎩ w C yy wy = 1
where Cxy is the between-sets covariance matrix of X and Y, Cxx and Cyy are
respectively the within-sets covariance matrices of X and Y. The maximum canonical
correlation is the maximum of ρ with respect to wx and wy.
Assume that C yy is invertible, then
T
x
T
y

wy =

1

λ

C yy−1C yx wx ,

(2)

and
C xy C yy−1C yx wx = λ 2 Cxx wx .

(3)

KCK-Means: A Clustering Method Based on Kernel Canonical Correlation Analysis

997

By first solving for the generalized eigenvectors of Eq. 3, we can therefore obtain
the sequence of wx ’s and then find the corresponding wy ’s using Eq. 2.
However, in complex situations, CCA may not extract useful descriptors of the data
because of its linearity. In order to identify nonlinearly correlated projections between
the two views, kernel extensions of CCA (KCCA) can be used [12]. Kernel CCA offers
an alternative solution by first projecting the data into a higher dimensional feature
space, i.e. mapping xi and yi to φ ( xi ) and φ ( yi ) respectively (i = 1, 2, … , l). And then

φ ( xi ) and φ ( yi ) are treated as instances to run CCA routine. Let Sx =
{ (φ ( x1 ), φ ( x2 ),..., φ ( xl )) }and Sy = { (φ ( y1 ), φ ( y2 ),..., φ ( yl )) }. Then the directions wx
and wy can be rewritten as the projection of the data onto the direction α and
β ( α , β ∈ ℜl ): wx = S xα and wy = S y β . Let Kx = S xT S x and Ky= S Ty S y be the kernel
matrices corresponding to the two views. Substituting into Eq. 1 we can obtain the
new objective function

ρ = max
α ,β

α T Kx K y β
α T K x2α ⋅ β T K y2 β

.

(4)

α can be solved from
( K x + κ I ) −1 K y ( K y + κ I )−1 K xα = λ 2α ,

(5)

where κ is used for regularization. Then β can be obtained from

β=

1
( K y + κ I ) −1 K xα .
λ

(6)

Let Κx(xi, xj) = φx ( xi )φxT ( x j ) and Κy(yi, yj) = φ y ( yi )φ yT ( y j ) are the kernel functions
of the two views. Then for any for any x* and y*, their projections can be obtained
from P(x*)= Κx(xi, X) α and P(y*)= Κy(yi, Y) β respectively.
A number of α and β (and corresponding λ) can be solved from Eq. 5 and Eq. 6.
If the two views are conditionally independent given the class label, the most strongly
correlated pair of projections should be in accordance with the ground-truth [9].
However, in real-world applications the conditional independence rarely holds, and
therefore, information conveyed by the other pairs of correlated projections should
not be omitted [9].
So far we have considered the kernel matrices as invertible, although in practice
this may not be the case [20]. We use Partial Gram-Schmidt Orthogonolisation
(PGSO) to approximate the kernel matrices such that we are able to re-represent the
correlation with reduced dimensionality [12]. In PGSO algorithm, there is a precision
parameter—η, which is used as a stopping criterion. For low-rank approximations, we
need keep eigenvalues greater than η and the number of eigenvalues we need to
consider is bounded by a constant that depends solely on the input distribution [20].
Since the dimensions of the projections rely on the N×M lower triangular matrix

998

C.-L. Chen, Y.-C. Gong, and Y.-J. Tian

output by PGSO which relies on this stopping criterion, we discuss the influence of η
to our algorithm in Sect. 3. More detail about PGSO is described in [20].
2.2 Two KCK-Means Algorithms

In our method, the similarity between data points is measured partly by the
projections obtained by KCCA and extends the K-means algorithm.
In [7], Balcan et al. showed that given appropriately strong PAC-learners on each
view, an assumption of expansion on the underlying data distribution is sufficient for
co-training to succeed, which implies that the stronger assumption of independence
between the two views is not necessary, and the existence of sufficient views is
sufficient. Similarly, the distance function fsim described below is also calculated
based on the assumption that X and Y are sufficient to describe the data respectively,
which is the same as the assumption of expansion about the co-training method.
Actually, our method is intuitively derived from co-training [10]. Since the two views
are sufficient to describe the data, both of them may be consist of some projections
correlate with the ground truth. So we intend to measure the similarity between
instances using information from two views of data. KCCA is an excellent tool that
can carry out this task. Therefore, measuring by the use of KCCA may be a promising
way of solving the problem of traditional distance measures.
Let m denote the number of pairs of correlated projections that have been
identified, then x* and y* can be projected into Pj(x*) and Pj(y*) (j = 1, 2, … ,m). Let
fsim denote distance functions, which is L2-norm • in this paper. Of course, other
2

similarity distance functions also could be. Based on the projections obtained by
KCCA, a new similarity measure can be defined as follows,
f sim ( xi , x j ) = μ xi − x j

2

m

+ ∑ Pk ( xi ) − Pk ( x j )

2

,

(7)

k =1

where μ is a parameter which regulates the proportion of the distance between the
original instances and the distance of their projections. Based on this similarity
measure, we propose the first algorithm as follows.
Input:
Output:

X and Y, two views of a data set with n instances
k, the number of clusters desired
C1 and C2, two vectors containing the cluster indices of each point of X
and Y.

Process:
1. Identify all pairs of correlated projections, obtaining α i , β i by solving Eqs. 5
and 6 on X and Y.
2. for i = 1, 2, …, l do
Project xi and yi into m pairs projections and obtain P(xi) and P(yi).
3. Get the new data sets by unite X and P(X), Y and P(Y), i.e. Mx = X P(X), My =
Y P(Y).
Fig. 1. KCK-means Algorithm for two-view data sets

KCK-Means: A Clustering Method Based on Kernel Canonical Correlation Analysis

999

Cluster Mx and My respectively as follows:
4. Randomly assign each instance of Mx (My) to one cluster of the k clusters.
5. Calculate the cluster means, i.e., calculate the mean value (both the original
value and the projections’ value) of the instance of each cluster.
6. repeat
7.
(re)assign each instances to the cluster to which the instance is the
most similar by calculating Eq. 7.
update the cluster means.
8.
9. until no change.
Fig. 1. (continued)

However, two-view data sets are rare in real world, which is the cause that though
co-training is a powerful paradigm, it is not widely applicable. In [6], it points out that
if there is sufficient redundancy among the features, we are able to identify a fairly
reasonable division of them, and then co-training algorithms may show similar
advantages to those when they perform on the two-view data sets. Similarly, in this
paper, we try to randomly split the single-view data set into two parts and treat them
as the two views of the original data set to perform KCCA and then KCK-means.
Input:

X , a single-view data set with n instances
k, the number of clusters desired
C, a vector containing the cluster indices of each point of X.

Output:
Process:
1. Randomly spilt X into two views with the same attributes, X1 and X2.
2. Identify all pairs of correlated projections, obtaining D i , E i by solving Eqs. 5 and
6 on X1 and X2.
3. for i = 1, 2, …, l do
Project x1, i and x2, i into m pairs projections and obtain P(x1, i) and P(x2, i).
4. Unite P(X1) and P(X2) into P(X), i.e. P(X) = P(X1)ĤP(X2).
5. Get the new data sets by unite X and P(X), i.e. Mx = XĤP(X).
Cluster Mx:
6. Randomly assign each instance of Mx to one cluster of the k clusters.
7. Calculate the cluster means, i.e., calculate the mean value (both the original
value and the projections’ value) of the instance of each cluster.
8. repeat
9.
(re)assign each instances to the cluster to which the instance is the most
similar by calculating Eq. 7.
10.
update the cluster means.
11. until no change.
Fig. 2. The KCK-means Algorithm for single-view data sets

3 Experiments and Analysis
Two standard multi-view data sets are applied to evaluate the effectiveness of the first
version of KCK-means. They are

1000

C.-L. Chen, Y.-C. Gong, and Y.-J. Tian

Course: The course data set has two views and contains 1,051 examples, each
corresponding to a web page, which is described in [10]. 200 examples are used in
this paper and there are 44 positive examples.
Ads: The url and origurl data sets are derived from the ads data set which is described
in [16] and has two categories. 300 examples are used in this paper, among which 42
examples are positive. In this paper, we construct a two-view dataset by using the url
view and origurl view.
In order to find out how well the second version of KCK-means performs on
single-view data sets, we use three single-view data sets .
F1

A3a: The a3a is a single-view data set derived from Adult Data Set of UCI, which is
described in [11]. It has two categories and 122 features. 3,185 examples are used and
there are 773 positive examples.
W1a: The w1a is a single-view data set derived from web page dataset which is
described in [9]. It has two categories and 300 sparse binary keyword attributes. 2,477
examples are used, among which 72 examples are positive.
DNA: The DNA is a single-view data set which is described in [8]. It has three
categories and 180 attributes. 2,000 examples are used, among which 464 examples
are 1st class, 485 examples are 2nd class, and 1,051 examples are 3rd class.
We use three performance measures, Pair-Precision, Intuitive-Precision and Mutual
Information, to measure the quality of the clusters obtained by the KCK-means.
Pair-Precision: The evaluation metric in [2] is used in our experiments. We evaluate
a partition i.e. the correct partition using
accuracy =

num(correct decisions )
.
n(n − 1) / 2

Mutual Information: Though entropy and purity are suitable for measuring a single
cluster’s quality, they are both biased to favor smaller clusters. Instead, we use a
symmetric measure called Mutual Information to evaluate the overall performance.
The Mutual Information is a measure of the additional information known about one
when given another [1], that is
MI ( A, B) = H ( A) + H ( B ) − H ( A, B) ,
where H(A) is the entropy of A and can be calculated by using
n

H ( A) = −∑ p( xi ) log 2 ( p( xi )) .
i =1

Intuitive-Precision: We choose the class label that share with most samples in a
cluster as the class label. Then, the precision for each cluster A is defined as:
P( A) =

1

1
max( {xi | label ( xi ) = C j } ) .
A

On http://www.csie.ntu.edu.tw/~cjlin/libsvmtools/datasets, all these single-view data sets can
be downloaded.
H

H

KCK-Means: A Clustering Method Based on Kernel Canonical Correlation Analysis

1001

In order to avoid the possible bias from small clusters which have very high
precision, the final precision is defined by the weighted sum of the precision for all
clusters, as shown in the following equation
G

Ak

k =1

N

P=∑

P ( Ak ) ,

where G is the number of categories (classes) and N is the total number of instances.

Fig. 3. Clustering results on two two-view data sets (course and ads, on the left column) and
three single-view data sets (a3a, w1a and DNA, on the right column) using KCK-means
comparing with two traditional clustering algorithms, K-means and Agglom (agglomerative
hierarchical clustering) with three performance measures, P-Precision (Pair-Precision), IPrecision (Intuitive-Precision), and MI (Mutual Information)

The comparison among between KCK-means and K-means, agglomerative
hierarchical clustering, are performed. In order to better reflect the performance of the
three algorithms, for all experiments demonstrated below with the two partitioning
algorithms, K-means and KCK-means, the diagrams are based on averaging over ten

1002

C.-L. Chen, Y.-C. Gong, and Y.-J. Tian

100%

100%
95%

Kmeans(View1)
Kmeans(View2)
Agglom(View1)
Agglom(View2)
KCK-means(View1)
KCK-means(View2)

P-Precision

80%

90%
85%

P-Precision

90%

70%

Kmeans
Agglom
KCK-means

80%
75%

60%

70%

50%

65%

40%

100%

100%

95%

95%

90%

90%

85%

85%

1

9

8

0.

7

5
0.

0.

4
0.

6

3
0.

0.

2
0.

η

0.

1
0.

1

0.
9

0.
8

0.
7

0.
5

0.
6

0.
4

0.
3

0.
2

0.
1

60%

η

80%
Kmeans(View1)
Kmeans(View2)
Agglom(View1)
Agglom(View2)
KCK-means(View1)
KCK-means(View2)

75%
70%
65%
60%

I-Precision

I-Precision

Kmeans
Agglom
KCK-means

80%
75%
70%
65%

1

0.
9

0.
8

9
0.

0.
6

8
0.

η

0.
7

7
0.

0.9

0.
5

6
0.

0.5

0.
4

5
0.

1.0

0.
2

4
0.

0.6

0.
3

3
0.

0.
1

2
0.

1

1
0.

60%

η

0.8
Kmeans

Mutual Information

Mutual Information

0.4

0.7

Kmeans(View1)
Kmeans(View2)
Agglom(View1)
Agglom(View2)
KCK-means(View1)
KCK-means(View2)

0.3
0.2
0.1

Agglom
KCK-means

0.6
0.5
0.4

7

8

9

0.

0.

η

1

6

0.

5

1

0.

9
0.

0.

8
0.

3

7
0.

4

.6
η0

0.

5
0.

2

4
0.

0.

3
0.

1

2
0.

0.

0.3
1
0.

0.

0

Fig. 4. The influence of η on the performance of KCK-means on the two-view data set course
and the single-view data set DNA, where η changes from 0.1 to 1.0, all of the three evaluation
metrics, Pair-Precision, Intutitive-Precision and Mutual Information, are used

clustering runs to compensate for their randomized initialization. And that is also
beneficial for measuring the performance of the second version of KCK-means on the
single-view data sets for its randomly splitting these data sets. The performances of
the three algorithms are showed in Fig. 3.
In Fig. 3, the performances of KCK-means are much better than those of other two
traditional clustering algorithms. On some data sets such as a3a, the Pair-Precision
and Intuitive-Precision of the results of KCK-means are both almost 100%, but PairPrecision and Intuitive-Precision of the results of K-means and agglomerative
hierarchical clustering are 59.74%, 75.73% and 58.87%, 75.73% respectively. KCKmeans also performs excellently on the multi-class data set—DNA and gets 85.03%
Pair-Precision, for K-means and agglomerative hierarchical clustering 72.39% and
67.13% respectively. For other two evaluation metrics, KCK-means is also much
better than those of the others’.

KCK-Means: A Clustering Method Based on Kernel Canonical Correlation Analysis

1003

In our experiments, we also note that when the proportion parameter μ is set to be
very small or even zero, the performance of KCK-means is the best, which means
using the projections obtained from KCCA the similarity between instances already
can be measured good enough. The μ in the experiments described in this paper is all
set to be 10-6.
In Sect. 2.1 we have stated that there is a precision parameter (or stopping
criterion)—η in the PGSO algorithm, on which the dimensions of the projections rely.
Now we demonstrate its influence on the performance of KCK-means. In order to
better measure such influence, we use two data sets, course and DNA, in the
experiments described below. Because course is a two-view data set with two classes,
and DNA is a single-view data set with three classes, then we can combine the
measure of the KCK-means on two-view data set and single-view data set simultaneously. The results are derived on more than ten clustering showed in Fig. 4.
In Fig. 4 we can find that follow the change of η, the performance of KCK-means
changes a little. Furthermore, even considering the influence, the performances of
KCK-means on both data sets are also much better than the other two clustering
algorithms.
However, in the experiments we find when η is larger than some threshold which
depends on given data set the performance of KCK-means descends very much even
worse than those of K-means and agglomerative hierarchical clustering. After
carefully observation, we find in such situations the number of dimensions of
projections is always very small, sometimes even only one dimension. Just as what
we have described in Sect. 2.1, in real-world applications the conditional
independence rarely holds, and therefore, information conveyed by the other pairs of
correlated projections should not be omitted [9]. Therefore, this performance
descending may be caused by lacking information conveyed by the other projections.

4 Conclusion
In this paper, we propose a novel partitioning method, i.e. KCK-means, based on
KCCA and inspiration from co-training. By using KCCA which mines the ground truth
hidden in the data, KCK-means measures the similarity between instances. On two
two-view data sets, course and ads, and three single-view data sets, a3a, w1a and
DNA, the experiments are performed using three performance measures, PairPrecision, Intuitive-Precision and Mutual Information. The results reflect that by
using KCK-means, much better quality of clusters could be obtained than those
obtained from K-means and agglomerative hierarchical clustering algorithms.
However, we also observe that when the number of dimensions of the projections
obtained from KCCA is very small, the performance of KCK-means descends very
much even worse than those of the two traditional clustering algorithms. This reflects
that in real-world applications, we need to consider the information conveyed by the
other pairs of correlated projections obtained from KCCA, instead of only considering
the strongest projection or very few stronger projections. That is, the number of
dimensions of projections obtained from KCCA and then used in KCK-means must be
enough.

1004

C.-L. Chen, Y.-C. Gong, and Y.-J. Tian

Acknowledgments. The research work described in this paper was supported by
grants from the National Natural Science Foundation of China (Project No. 10601064,
70531040, 70621001).

References
1. Butte, A.J., Kohane, I.S.: Mutual information relevance networks: functional genomic
clustering using pairwise entropy measurements. In: Pacific Symposium on Biocomputing,
Hawaii, pp. 415–426 (2000)
2. Wagstaff, K., Claire, C.: Clustering with Instance-level Constraints. In: the 17th
International Conference on Machine Learning, pp. 1103–1110. Morgan Kaufmann press,
Stanford (2000)
3. Khan, S.S., Ahmadb, A.: Cluster center initialization algorithm for K-means clustering.
Pattern Recognition Letters 25, 129–1302 (2004)
4. Kirsten, M., Wrobel, S.: Relational distance-based clustering. In: Page, D.L. (ed.) ILP
1998. LNCS, vol. 1446, pp. 261–270. Springer, Heidelberg (1998)
5. Bickel, S., Scheffer, T.: Multi-View Clustering. In: The 4th IEEE International Conference
on Data Mining, pp. 19–26. IEEE press, Brighton (2004)
6. Nigam, K., Ghani, R.: Analyzing the effectiveness and applicability of co-training. In: the
9th international conference on Information and knowledge management, pp. 86–93. ACM
press, McLean (2000)
7. Balcan, M.F., Blum, A., Yang, K.: Co-training and expansion: Towards bridging theory
and practice. In: The 18th Annual Conference on Neural Information Processing Systems,
pp. 89–96. MIT press, Vancouver (2005)
8. Hsu, C.W., Lin, C.J.: A comparison of methods for multi-class support vector machines.
IEEE Transactions on Neural Networks 13, 415–425 (2002)
9. Zhou, Z.H., Zhan, D.C., Yang, Q.: Semi-supervised learning with very few labeled training
examples. In: The 22nd AAAI Conference on Artificial Intelligence, pp. 675–680. AAAI
press, Vancouver (2007)
10. Blum, A., Mitchell, T.: Combining labeled and unlabeled data with co-training. In: The
Conference on Computational Learning Theory, pp. 92–100. Morgan Kaufmann press,
Madison (1998)
11. Kohavi, R.: Scaling Up the Accuracy of Naive-Bayes Classifiers: a Decision-Tree Hybrid.
In: The Second International Conference on Knowledge Discovery and Data Mining, pp.
202–207. AAAI press, Oregon (1996)
12. Hardoon, D.R., Szedmak, S., Shawe-Taylor, J.: Canonical correlation analysis; An
overview with application to learning methods. Technical report, Department of Computer
Science Royal Holloway, University of London (2003)

