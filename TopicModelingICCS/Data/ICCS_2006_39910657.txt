An Improved Particle Swarm Optimization Algorithm
for Global Numerical Optimization
Bo Zhao
Jiangsu Electric Power Research Institute Corporation Limited
Nanjing 210063, Jiangsu, China
zhaobozju@163.com

Abstract. This paper presents an improved particle swarm optimization algorithm (IPSO) for global numerical optimization. The IPSO uses more particles'
information to control the mutation operation. A new adaptive strategy for
choosing parameters is also proposed to assure convergence of the IPSO.
Meanwhile, we execute the IPSO to solve eight benchmark problems. The results show that the IPSO is superior to some existing methods for finding the
best solution, in terms of both solution quality and algorithm robustness.

1 Introduction
Particle swarm optimization (PSO) is one of the evolutionary computation techniques
based on the social behavior metaphor [1]. It is initialized with a population of random solutions, conceptualized as particles. Each particle in PSO flies through the
search space with a velocity according to its own and the whole population’s historical behaviors. The particles have a tendency to fly toward better search areas over the
course of a search process. In recent years, PSO has been widely used for numerical
optimization and many other engineering problems. Global optimization problems
arise in almost every field of science, engineering, and business. Now, the PSO is
becoming one of the popular methods to address them due to its simplicity of implementation and ability to quickly converge to a reasonably good solution. But, in
global optimization problems, the major challenge is that the POS may be trapped in
the local optima of the objective function. The issue is particularly challenging when
the dimension is high and there are numerous local optima. Therefore, some improved
methods have been proposed, such as the fully informed particle swarm [2], a hybrid
of GA and PSO [3], a cooperative approach to PSO [4], a hierarchical particle swarm
optimizer [5], a multiagent-based PSO [6], and so on.
In the standard PSO algorithm, the neighborhood of a particle consist of all particles, so that the global best position, i.e., the best solution found so far, directly influences its behavior. Hence, for social science context, a PSO system combines a
social-only component model and a cognition-only model. The social-only model
component suggests that individuals ignore their own experience and adjust their
behavior according to the successful beliefs of individuals in the neighborhood. On
the other hand, the cognition-only model component treats individuals as isolated
beings. The real strength of the particle swarm derives from the social interactions
V.N. Alexandrov et al. (Eds.): ICCS 2006, Part I, LNCS 3991, pp. 657 – 664, 2006.
© Springer-Verlag Berlin Heidelberg 2006

658

B. Zhao

among
particles as they search the space collaboratively. So, in this paper, an improved adaptation strategy with enhanced social influence is proposed for PSO algorithm. This
adaptation strategy is similar to the social society in that a group of leaders, i.e. several fittest individuals in a population, play a major role in reproduction process. This
is different from the standard PSO with only one leader, the fittest individual in a
generation, is selected to the reproduction process. The improved particle swarm
optimization (IPSO) has been tested with some benchmark functions. The experimental results show that the IPSO achieves a good performance for test functions, which
illustrate that the IPSO overcomes the problem of premature convergence of the standard PSO method in some degree.

2 Improved Particle Swarm Optimization (IPSO)
2.1 Standard Particle Swarm Optimization Approach (PSO)
PSO is developed through simulation of bird flocking in two-dimension space. According to the research results for a flock of birds, birds find food by flocking. The
observation leads the assumption that information is shared inside flocking. Moreover, according to observation of behavior of human groups, behavior of each individual (agent) is also based on behavior patterns authorized by the groups such as
customs and other behavior patterns according to the experiences by each individual.
The position of each agent is represented by XY-axis position and the velocity is
expressed by vx (the velocity of X-axis) and vy (the velocity of Y-axis). Modification
of the agent position is realized by using the position and the velocity information [7].
Searching procedures by PSO based on the above concept can be described as follows: a flock of agents optimizes a certain objective function. Each agent knows its
best value so far (pbest) and its position. The information is corresponding to personal
experiences of each agent. Moreover, each agent knows the best value so far in the
group (gbest) among pbests. The information is corresponding to knowledge of how
the other agents around them have performed. Namely, each agent tries to modify its
position using the following information:
• The distance between the current position and pbest, pt.
• The distance between the current position and gbest, pˆ t .
The modification can be represented by the concept of velocity. Velocity of each
agent can be modified by the following equation:

vt +1 = c0 vt + c1r1 ( t ) × ( pt − xt ) + c2 r2 ( t ) × ( pˆ t − xt ) ,

(1)

where c0, c1 and c2 are positive constant coefficients, r1 and r2 are uniformly distributed
random numbers in [0,1], vt is the current velocity of the particle at iteration t, xt is current position of the particle at iteration t, vt+1 is the modified velocity at iteration t+1.
Using the above equation (1), a certain velocity that gradually gets close to pbests
and gbest can be calculated. The current position (searching point in the solution
space) can be modified by the following equation:

An IPSO Algorithm for Global Numerical Optimization

x

t +1

= xt + vt +1 ,

659

(2)

where xt+1 is the modified position at iteration t+1.
2.2 Improved Particle Swarm Optimization Approach (IPSO)
Due to the shortage of the social interactions among particles in the standard PSO
algorithm, an improved adaptation strategy with enhanced social interactions is proposed for particle swarm optimization (PSO) algorithm in this section. This adaptation strategy uses more particles' information to control the mutation operation and
extends the original formulas of the PSO method, which can search the global optimal
solution more effectively.
The third term added to the right-hand-side of the velocity equation (1) is derived
from the successes of the others; it is considered a "social influence" term. It is found
that when this effect is removed from the algorithm, performance is abysmal. So the
social interaction is an important factor to improve the PSO performance. To enhance
the social interactions in the algorithm, this paper proposes a new method of improved PSO using some fittest particles' information to modify particle's position and
velocity. Namely, at ith iteration, we rearrange the particles in descending order according to their fitness and select the last n particles to modify particle's position and
velocity. Let pˆ i ,t denote current position of the particle i in these particles at iteration
t. The updating equations of IPSO method can be described in the following:
v t + 1 = c 0 v t + c1 r1 ( t ) × ( p t − x t ) +

x

t +1

n

∑c
i =1

r

2 ,i 2 ,i

( t ) × ( pˆ i , t

− xt ) ,

= xt + vt +1 ,

(3)
(4)

Each particle of IPSO method modifies its position and velocity using the best solution particle achieved and several gbest of neighborhood particles. It is similar to the
social society in that the group of leaders could make better decisions. However, in
standard PSO, only one gbest of neighborhood particles is employed. This process
using some neighborhood particles can be called 'intensifying' and 'enhancing' the
social influence. Based on this understanding, we should intensify these particles that
could lead individuals to better fitness. This reinforces the exploitation and exploration of PSO. As a particle swarm population searches over time, individuals are drawn
toward one another's successes, with the usual result being clustering of individuals in
optimal regions of the space.
2.3 IPSO Algorithm’s Convergence
The form of the recurrence relation of particle's position can be derived as follows:
substituting equation (3) into (4) and from equation vt=xt-xt-1, we have the following
equation:
n
n
⎛
⎞
xt +1 = ⎜1 + c0 − c1r1 ( t ) − ∑ c2.i r2.i ( t ) ⎟ xt − c0 xt −1 + c1r1 ( t ) pt + ∑ c2.i r2.i ( t ) pˆ i ,t ,
i =1
i =1
⎝
⎠

(5)

660

B. Zhao

which is a non-homogeneous recurrence relation that can be solved using standard
recursive techniques. This recurrence relation can be written as a matrix-vector
product,

⎡ xt +1 ⎤ ⎡1 + c0 − η0 − η1
⎢ x ⎥=⎢
1
⎢ t ⎥ ⎢
0
⎣⎢ 1 ⎦⎥ ⎣⎢

−c0 η0 pt + η1 p ⎤ ⎡ xt ⎤
⎥ ⎢x ⎥ ,
0
0
⎥ ⎢ t −1 ⎥
0
1
⎦⎥ ⎣⎢ 1 ⎦⎥

n

n

i =1

i =1

(6)

where η0 = c1 ∗ r1 ( t ) , η1 = ∑ c2.i ∗ r2.i ( t ) , η1 p = ∑ c2.i ∗ r2.i ( t ) ∗ pˆ i ,t . The characteristic

polynomial of the equation (6) is,

( λ − 1) ( λ 2 − (1 + c0 − η0 − η1 ) λ + c0 ) ,
which has a trivial root of λ1=1.0, and two other solutions (7) and (8),

λ2 =

(1 + c0 − η0 − η1 ) + γ
,
2

(7)

λ3 =

(1 + c0 − η0 − η1 ) − γ
,
2

(8)

where

γ=

(1 + c0 − η0 − η1 )

2

− 4c0 ,

(9)

Note that λ1, λ2 and λ3 are both eigenvalues of the equation (6). The explicit form of
the recurrence relation (5) is then given by equation,
xt = k1λ1t + k2 λ2t + k3 λ3t = k1 + k2 λ2t + k3 λ3t ,

(10)

where k1, k2 and k3 are constants determined by the initial conditions of the system at
each iteration.
An important aspect of the behavior of a particle concerns whether its trajectory
(specified by xt) converges or diverges. The conditions under which the sequence

{ xt }t =0
+∞

will converge are determined by the magnitude of the values λ2 and λ3 as com-

puted using equations (7) and (8). From equation (9), it is clear that there are two cases:
1)

Case A: (1 + c0 − η0 − η1 ) < 4c0
2

In this case, r will be a complex number with a non-zero imaginary component. A
complex r results in λ2 and λ3, being complex numbers with non-zero imaginary
components as well. Consider the value of xt in the limit, thus equation (10)
becomes,
xt = k1 + k2 ⋅ λ2 ⋅ e jθ t + k3 ⋅ λ3 ⋅ e jσ t ,
t

t

(11)

An IPSO Algorithm for Global Numerical Optimization

661

where λ2 ⋅ e jθ t and λ3 ⋅ e jσ t are the exponent expression of the trivial roots.
t

t

Clearly, equation (11) explains the sequence

{ xt }t =0
+∞

will converge when

max ( λ2 , λ3 ) < 1 , so, lim xt = k1 + k2 ⋅ λ + k3 ⋅ λ = k1 .
t
2

t →∞

t
3

Case B: (1 + c0 − η0 − η1 ) ≥ 4c0
2

2)

In this case, r, λ2 and λ3, will be real numbers, from equation (10), if

max ( λ2 , λ3 ) < 1 , then the sequence { xt }t =0 converges.
+∞

From the analysis of above two cases, we obtain the convergence condition of the

sequence { xt }t =0 is max ( λ2 , λ3
+∞

) < 1.

One popular choice of updating parameters is c0 = 0.7298 , c1 = 1.49618 and
n

∑c
i =1

2.i

n

= 1.49618 [8]. On account of η0 = c1 ∗ r1 ( t ) and η1 = ∑ c2.i ∗ r2.i ( t ) , so
i =1

η0 ∈ (0,1.49618)
,
η1 ∈ (0,1.49618)
and
η0 + η1 ∈ (0, 2.9924)
.When
η0 + η1 ∈ (0, 0.0212] , from equation (9), it will imply a real-valued γ , which corre1.7298 − η0 − η1 + γ
sponds to case B, then γ ≥ 0 and max( λ2 , λ3 ) =
< 1 . Similarly,
2
η0 + η1 ∈ (0.0212, 2.9924) will result in a complex r value, which corresponds to case
A and from equations (7) and (8) we can assure λ2 = λ3 < 1 . The above analysis
shows that the choices of parameters satisfy the convergence conditions and will
assure convergence of the sequence { xt }t = 0 .
+∞

IPSO method differs from PSO method is that η1 is the sum of the coefficients of n
gbest particles. The degree of importance of each particle is weighed through particle’s
fitness value. The better the particle’s fitness value is, the more important the particle’s
influence is. So the paper proposed the rule of updating parameters chosen as follows:
(12)

1
∧

c2,i =

n

fi
⋅ 1.49618 ,
1

∑
i =1

∧

fi

∧

where f i is the gbest particle’s value. This adaptive strategy of updating parameters
in IPSO can assure convergence of IPSO method and enhance the global convergence
capability of IPSO method.

3 Numerical Experiments and Results
In order to verify the effectiveness and efficiency of the proposed IPSO method, eight
benchmark functions have been used in Table 1.

662

B. Zhao
Table 1. Comparisons of generalization capability with published results

Function
Name

Dimensions
(N)

Initial
Range

30

(-500,500)

F2 ( x ) = ∑ ⎡⎣ xi2 − 10 cos ( 2π xi ) + 10 ⎤⎦

30

(5.12,5.12)

⎛
⎞
⎛1 N
⎞
1 N
F3 ( x) = −20 exp ⎜ −0.2 ∑ xi2 ⎟ − exp ⎜ ∑ cos ( 2π xi ) ⎟
⎜
⎟
⎜
⎟
n i =1
⎝ n i =1
⎠
⎝
⎠
+ 20 + exp(1)

30

(-32,32)

30

(-600,600)

30

(-30,30)

30

(-100,100)

30

(-10,10)

30

(-100,100)

Function Expression
N

Schwefel

(

F1 ( x ) = ∑ − xi sin
i =1

xi

)

N

Rastrigin

i =1

Ackley

Griewank

F4 ( x ) =

1 N 2 N
⎛x ⎞
cos ⎜ i ⎟ + 1
∑ xi − ∏
4000 i =1
⎝ i⎠
i =1
N −1

Rosenbrock

2
2
F5 ( x ) = ∑ ⎡100 ( xi +1 − xi ) + (1 − xi ) ⎤
⎣
⎦
i =1
N

Sphere

F6 ( x ) = ∑ xi2
i =1

Schwefel

N

N

i =1

i =1

F7 ( x ) = ∑ xi + ∏ xi
N ⎛ i

Schwefel

⎞
F8 ( x) = ∑ ⎜ ∑ x j ⎟
⎜
⎟
i =1 ⎝ j =1 ⎠

2

Some parameters must be assigned to before the IPSO method is used to solve
problems in following experiments. Since the test problem dimensions are high, a
moderate population size is set to 70 and the maximal generation is set to 1000. To
evaluate uncertain value combinations of n of the IPSO method, we have been executed 50 times to solve the above test function problem under various value combinations. The results show that the best solution can be obtained by the IPSO method
when n=4. Hence, in the following study, we always choose n=4.
Owing to the randomness in IPSO, the algorithm is executed 50 independent times
on each test function. Table 2 shows the optimal results of IPSO. From Table 2, we
see that the mean function values are equal or close to the optimal ones, and the standard deviations of the function values are relatively small, except for function F2.
These results indicate that the proposed IPSO method can find optimal or close-tooptimal solutions, and its solution quality is quite stable.
Table 3 summarizes the optimal results as obtained by some existing methods.
These existing algorithms include standard particle swarm optimization algorithm
(PSO), conventional genetic algorithm (CGA), orthogonal genetic algorithm with
quantization (QGA/Q), fast evolutionary programming (FEP), evolutionary optimization (EO), enhanced simulated annealing (ESA) and conventional evolutionary
programming with Gaussian mutation operator (CEP/GMO). Since each of these
existing algorithms was executed to solve some of the above-mentioned functions,

An IPSO Algorithm for Global Numerical Optimization

663

Table 3 has included all of the available results for comparison. From Table 3, these
results show that the optimal solutions determined by the IPSO lead to high quality
solutions, which confirms that the IPSO is well capable of determining the global or
close-to-optimal solutions. Firstly, Table 3 compares IPSO with PSO and CGA. For
all numerical optimization functions, IPSO gives better solutions than PSO and
CGA. Compared IPSO with PEP and OGA/Q, we see that FEP and OGA/Q obtain
good performance on numerical optimization problem, even better solutions in some
optimization functions. But the IPSO method is simple and has not complicated
operator, not as PEP and OGA/Q. However, in eight benchmark functions, except for
F2, the optimal solutions obtained by the IPSO are nothing less than these solutions
obtained by PEP and OGA/Q. It indicates that IPSO can, in general, give good mean
solution quality. At the same time, compared IPSO with EO and ESA, as can been
seen, IPSO obtains better solutions in the available results for comparison, and displays a good performance in solving these global numerical optimization problems.
Table 2. Optimal results of IPSO
Mean function
value
-12569.487
11.5312
8.253×10-16
0
9.584×10-1
4.096×10-96
1.654×10-8
0

Test function
F1
F2
F3
F4
F5
F6
F7
F8

Standard deviation
of function value
7.056×10-6
0.1242
7.413×10-17
0
2.761×10-1
1.721×10-96
7.357×10-9
0

Globally minimal
function value
-12569.5
0
0
0
0
0
0
0

Table 3. Comparison between IPSO and some existing algorithms
Mean function value

F1
F2
F3
F4
F5
F6
F7
F8

IPSO

PSO

CGA

OGA/Q

FEP

EO

ESA

CEP/GMO

-12569.487
11.5312
8.25 × 10-16
0
9.58 × 10-1
0
5.40 × 10-22
2.70 × 10-11

-9903.8
26.8639
7.99 × 10-15
0.022
16.770
4.83 × 10-48
1.65 × 10-8
0.324

-9094.75
22.967
2.697
1.258
150.79
4.96
7.93 × 10-1
18.82

-12569.45
0
4.44 × 10-16
0
7.52 × 10-1
0
0
0

-12554.5
4.6 × 10-2
1.8 × 10-2
1.6 × 10-2
-5.7 × 10-4
8.1 × 10-3
1.6 × 10-2

-46.47
-0.40
1911.59
9.88
---

----17.10
----

-120.00
9.10
2.52 × 10-7
86.70
3.09 × 10-7
1.99 × 10-3
17.60

Figure 1 shows the evolution process of F1 obtained using IPSO, PSO and CGA respectively. It is clear for the figure that the solution by IPSO is converged to high
quality solutions at the early iterations. Similar results are obtained for F2 to F8. According to the above analysis, considering together more particles' information to
control the mutation operation, the IPSO method performs better than the PSO, both
in the quality of the solution discovered and in the velocity of convergence, and simulation results show that IPSO outperforms CGA and PSO.

664

B. Zhao

Fig. 1. Optimization procedure by IPSO, PSO and CGA

4 Conclusions
An improved particle swarm optimization algorithm (IPSO) has been developed for
determination of the optimal or close-to-optimal solutions of global numerical optimization problem. The IPSO approach uses more particles' information to control the
mutation operation. The convergence property of IPSO is analyzed using standard
results from the dynamic system theory and guidelines for proper algorithm parameter
selection are derived. A new adaptive strategy for choosing parameters is also proposed to assure convergence of IPSO. Meanwhile, it was found from experimental
results that IPSO could find higher quality solutions reliably with the faster converging characteristics than some existing methods on the problem studies.

References
1. Kennedy, J., Eberhart, R. C.: Particle swarm optimization. In: Proceedings of IEEE International Conference on Neural Networks. (1995) 1942-1948
2. Mendes, R., Kennedy, J.: The fully informed particle swarm: simpler, maybe better. IEEE
Trans. Evolutionary Computation. 3 (2004) 204-210
3. Juang, C.F.: A hybrid of genetic algorithm and particle swarm optimization for recurrent
network design. IEEE Trans. System, Man and Cybernetics-Part B. 2 (2004) 997-1006
4. van den Bergh, F., Engelbrecht, P.: A cooperative approach to particle swarm optimization,.
IEEE Trans. Evolutionary Computation. 3 (2004) 225-239
5. Janson, S., Middendorf, M.: A hierarchical particle swarm optimizer and its adaptive variant. IEEE Trans. System, Man and Cybernetics-Part B, 6 (2005) 1272-1282
6. Zhao, B., Guo, C.Y., Cao, Y.J.: A multiagent-based particle swarm optimization approach
for optimal reactive power dispatch. IEEE Trans. Power Systems. 2 (2005) 1070-1078
7. Kennedy, J., Eberhart, R.C.: Swarm Intelligence. Morgan Kaufmann Publishers (2001)
8. Clerc, M., Kennedy, J.: The Particle Swarm-explosion, stability, and convergence in a multidimensional complex space. IEEE Trans. on Evolutionary Computation. 1 (2002) 58-73

