Procedia Computer Science
Volume 51, 2015, Pages 140–149
ICCS 2015 International Conference On Computational Science

Adaptive Partitioning for Irregular Applications on
Heterogeneous CPU-GPU Chips ∗
Antonio Vilches1 , Rafael Asenjo1 , Angeles Navarro1 , Francisco Corbera1 , Rub´en
Gran2 , and Mar´ıa Garzar´an3
1

Universidad de M´
alaga, Spain. e-mail: {avilches, asenjo, angeles, corbera}@ac.uma.es
2
Universidad de Zaragoza, Spain. e-mail: rgran@unizar.es
3
Dept. Computer Science, UIUC, USA. e-mail: garzaran@illinois.edu

Abstract
Commodity processors are comprised of several CPU cores and one integrated GPU. To fully
exploit this type of architectures, one needs to automatically determine how to partition the
workload between both devices. This is specially challenging for irregular workloads, where each
iteration’s work is data dependent and shows control and memory divergence. In this paper,
we present a novel adaptive partitioning strategy specially designed for irregular applications
running on heterogeneous CPU-GPU chips. The main novelty of this work is that the size of
the workload assigned to the GPU and CPU adapts dynamically to maximize the GPU and
CPU utilization while balancing the workload among the devices. Our experimental results
on an Intel Haswell architecture using a set of irregular benchmarks show that our approach
outperforms exhaustive static and adaptive state-of-the-art approaches in terms of performance
and energy consumption.
Keywords: Heterogeneous CPU-GPU chips, adaptive partitioning, dynamic scheduling, parallel for.

1

Introduction

Hardware vendors such as Intel, Qualcomm, AMD, or NVidia have recently released heterogeneous processors characterized by featuring several CPU cores and a GPU on the same die.
However, the success of these systems will rely on the ability of the software to adapt the
application level parallelism to exploit the underlying available hardware.
We consider the problem of eﬃciently executing the iterations of a parallel loop on heterogeneous CPU-GPU chips by executing on both, the CPU cores and the integrated GPU. This
requires a carefully partitioning of the workload across the CPU cores and the GPU accelerator.
Creating a dynamic and adaptive work distribution mechanism that is portable across processors is challenging and even more when the application exhibits irregularities. We propose a
lazy partitioning of the loop iteration space into chunks (or blocks) of iterations. These chunks
are greedily processed on the CPU cores or on the GPU and their sizes are adaptively computed
∗ This work has been supported by the following Spanish projects: TIN2010-16144, TIN 2013-42253-P,
TIN2013-46957-C2-1-P, Consolider NoE TIN2014-52608-REDC, P11-TIC-08144 and gaZ: T48 research group.

140

Selection and peer-review under responsibility of the Scientiﬁc Programme Committee of ICCS 2015
c The Authors. Published by Elsevier B.V.

doi:10.1016/j.procs.2015.05.213

Adaptive Partitioning for Irregular Applications on Heterogeneous Chips

Antonio Vilches et al.

throughout the execution of the loop. To provide a productive programming interface we have
extended the parallel for template of the TBB task framework [17] to allow its exploitation
on heterogeneous CPU-GPU chip processors. Previous task frameworks that oﬀer support for
heterogeneous CPU-GPU systems, like StarPU [1], OmpSs [3], XKaapi [13], and Concord [11]
implement a variety of static and dynamic scheduling strategies, but do not adapt the size of
the chunk of iterations assigned to the GPU. In contrast, our strategy monitors the throughput
of each computing device (CPU cores and GPU) during the execution of the iteration space
and uses this metric to adaptively resize chunks to optimize overall throughput and to prevent
underutilization and load imbalance of GPU and CPUs due to small or large chunk sizes. We
have found that correctly determining the chunk size for the GPU is highly important. For
instance, if the GPU chunk size is too small, the GPU will not amortize the cost of oﬄoading
the work. If on the other hand, the GPU chunk size is too large, the GPU may suﬀer from
too much memory contention due to uncoalesced memory accesses that can lead to suboptimal
device performance. For these reasons, dynamically ﬁnding the appropriate chunk size for each
device is critical to minimize execution time and energy consumption.
The contributions of the paper are: i) a novel adaptive partitioning algorithm that can be
applied to parallel loops to automatically ﬁnd the appropriate chunk size for the GPU and
the CPU cores; ii) to demonstrate the positive impact of using adaptive GPU and CPU chunk
sizes that dynamically vary based on changes in the throughput of the application; and iii)
the evaluation, using regular and irregular applications, of our partitioning strategiy in terms
of performance and energy consumption, showing that we outperform other state-of-the-art
approaches.

2

Motivation

In this Section, we motivate the need for varying the chunk size when dynamically scheduling
chunks of iterations of a parallel loop of an irregular application on an heterogeneous CPUGPU chip. We discuss two diﬀerent implementations of an n-body problem: a regular approach
called Nbody [10] and an irregular one called Barnes Hut [12]. We have chosen these benchmarks because they use the same inputs and perform similar computations using two diﬀerent
approaches. These two benchmarks consist of a sequential outer loop that represents a sequence
of time-steps. In each time-step all the body-body gravitational interactions are computed. To
that end, each time-step contains a parallel loop that traverses all the bodies and an inner loop
that, for each body of the parallel loop, computes the interactions with other bodies. Nbody
is considered a brute force method that computes all the interactions that the n bodies in the
system induce upon each other. On the other hand, Barnes Hut employs an octree data structure and the number of interactions of each body varies depending on its location. Thus, for
each parallel loop iteration, the inner loop has a diﬀerent number of iterations (or interactions),
resulting in an irregular application.
To understand the impact that the chunk size has in the performance, we ran some experiments where Nbody and Barnes Hut were executed only on the GPU or only on the CPU
cores. The experiments were conducted on a Core i7-4770, 3.4 GHz based on Haswell architecture with an integrated GPU HD 4600, for 75 time-steps and using 100,000 bodies as input.
Figure 1 shows the average GPU throughput for diﬀerent chunk sizes for Nbody (time-steps
0 and 30) and Barnes Hut (time-steps 0, 5, and 30) using log2 scale in the x-axis. For each
time-step, the data were collected by running the iteration space of the parallel loop with a
ﬁxed chunk size (the x-axis). The chunk sizes were set to 20 × 2k , where k goes from 0 to
13, being 20 the number of execution units, nEU , on Haswell’s GPU. Figure 1(a) shows the
behavior of the regular Nbody application, where chunk size = 640 obtains an optimal aver141

Adaptive Partitioning for Irregular Applications on Heterogeneous Chips
100
90
Linear Scale: Samples and fitting
4

2.5
2

3
2
1
0
0

1.5

Samples
Fitting Curve
1
Chunk size

1

80
Average Throughput

3

Throughput

Average Throughput

3.5

2
4

x 10

70
60
50
40

static, ts=0
static, ts=5
static, ts=30
adap, ts=0
adap, ts=5
adap, ts=30

30
20

static, ts=0
static, ts=30

0.5
0
0

Antonio Vilches et al.

BarnesHut: Average Throughput per chunk size

Nbody: Average Throughput per chunk size
4

40

160

640
2560
Chunk size

(a) Nbody

10240

40980

81960

10
0
0

40

160

640
2560
Chunk size

10240

40980

81960

(b) Barnes Hut

Figure 1: Average GPU’s throughput (iter./ms.) for diﬀerent chunk sizes and diﬀerent time-steps.
Note the log2 scale in x-axis.
age throughput and larger chunk sizes have a minimum impact on throughput. This result is
the same across all time-steps. The inner subﬁgure in Figure 1(a) shows some samples of the
throughput for diﬀerent chunk sizes and the logarithmic curve that ﬁts them using linear scale.
We will come back to this topic in section 4. Figure 1(b) shows a very diﬀerent scenario in the
irregular Barnes Hut application. Here, in time-step 0, chunk size = 640 obtains the highest
average throughput, while in time-steps 5 and 30 the highest average throughput is obtained
with chunk size = 1280. For all time-steps, the application takes an important performance
hit beyond those points. The ﬁgure also shows the average throughput when the chunk size
is adaptively selected to the value that provides the highest throughput during the iteration
space execution, for time step 0 (the blue marks ’•’), time-step 5 (the green marks ’•’), and
time-step 30 (the red marks ’•’). For time-step=0 the chunk sizes were selected between [620,
1320], while for time-step=5 and time-step=30 they were between [740, 1560] and [1280, 1760],
respectively. In all cases, w.r.t. the maximum average throughput of the static experiments, an
additional improvement of 5%-7% was observed thanks to the adaptive selection of the chunk
size. These results illustrate that the GPU’s chunk size has a signiﬁcant impact in the GPU performance and that ﬁnding the best value for irregular applications can be challenging, because
the appropriate chunk size might not be a constant, but rather change during the execution.
There are some reasons that explain the GPU throughput behavior shown in Figure 1. For
both, Nbody and Barnes Hut, small chunk sizes result in low throughput due to: i) an insuﬃcient amount of work oﬄoaded to the GPU to exploit all execution units; and ii) the overhead
of data transfer and kernel launch. On the other hand, for Nbody, large chunk sizes achieve
higher utilization and amortize the cost of oﬄoading work to the GPU, as shown in Figure 1(a).
However, for Barnes Hut the chunk size has to be carefully and adaptively selected in order to
make the most out of the GPU. As pointed out in a previous work [4], the access pattern in
Barnes Hut can result in uncoalesced memory accessess, that may represent between 65% to
75% of the total number of issued instructions. Since many threads are trying to concurrently
access uncoalesced memory addresses, contention for the shared memory bandwidth increases.
Therefore, for this kind of codes, a large GPU chunk size might be counter-productive, as
Figure 1(b) shows. Notice that previous works have not performed a sensitivity analysis for irregular applications as the one shown in Figure 1. Thus, those works assume a behavior similar
to that of Nbody and assign large blocks to the GPU (to make sure the throughput is in the
plateau). They do not impose any restriction on how large the GPU block size can be, as they
do not consider that large GPU block sizes can harm performance, as they are not harmful for
regular applications like Nbody.
Regarding the CPU cores, a similar chunk size study for Nbody and Barnes Hut showed
that chunk sizes of 10 or more iterations always obtained the maximum constant throughput,
142

Adaptive Partitioning for Irregular Applications on Heterogeneous Chips

Antonio Vilches et al.

independently of the benchmark regularity. Moreover, our experimental data also show that
large chunk sizes may result in load imbalance between CPU and GPU when they are collaboratively processing the parallel loop, especially at the end of the iteration space. Our scheduler
addresses the described issues: adaptive chunk size selection and load imbalance.

3

Scheduler description

Our scheduler considers loops with independent iterations (parallel for) and features a dynamic
work scheduling policy and an adaptive GPU and CPU chunk partitioning. Our approach
adaptively partitions the whole iteration space into chunks or blocks of iterations. The goal
of the partitioning strategy is to evenly balance the workload of the loop among the compute
resources (GPU and CPU cores) as well as to assign to each device the chunk size that maximizes its throughput during computation. Our scheduler builds on top of an extension of the
TBB parallel for template for heterogenous CPU-GPU systems by Navarro et al. [15]. The
developer can invoke our parallel for function, which has the three following arguments: the
iteration space, the body object of the loop, and the partitioner object. The latter implements
the adaptive partitioning strategy that computes the appropriate chunk size for each compute
device (CPU cores or GPU). This partitioning strategy is the novelty of this paper and is described in Section 4. The user is also required to write a class that processes the chunk on a
CPU core or on the GPU. Our template can work on diﬀerent types of heterogeneous systems,
but our focus in this paper is on architectures with an on-chip GPU. Internally, our scheduler
ﬁrst selects an idle device (GPU or CPU core) and our partitioner computes the number of iterations (chunk size) that will be extracted from the range of the remaining iterations and will
be assigned to that device. The time to compute the chunk on the CPU or GPU is recorded1 .
This is necessary to compute the device’s throughput, which is used by the partitioner described
next. More details of the scheduler can be found in [15].

4

Partitioning strategy

In this Section we explain how our partitioning approach, that we call LogFit, works. We assume
that the execution time can be seen as a sequence of scheduling intervals {tG0 , tG1 . . . tGi−1 , tGi ,
tGi+1 . . .} for the GPU and {tC0 , tC1 . . . tCi−1 , tCi , tCi+1 . . .} for each CPU core. Each computing
device at its ith interval, tGi or tCi , gets a chunk of iterations of size G(tGi ) and C(tCi ),
respectively. The running time for the assigned GPU chunk, T (tGi ), or CPU chunk T (tCi ), is
recorded. This time is used to compute the throughput in the corresponding interval, λG (tGi ) =
G(tGi )/T (tGi ) for the GPU or λC (tCi ) = C(tCi )/T (tCi ) for a CPU core.
First we explain how LogFit computes the recommended chunk size, G(tGi ), for the GPU.
A previous work [2] have found that for regular applications, the GPU throughput follows a
logarithmic curve with respect to the size of the chunk: y = a ln(x) + b, being x the chunk size
and y the estimated throughput. We corroborated that ﬁnding for the Nbody benchmark, as
depicted in the inner subﬁgure of Figure 1(a). For this case, we can accurately compute the
recommended chunk size by ﬁrst collecting some points of the curve (by sampling at runtime
the GPU throughput for diﬀerent chunk sizes) and then applying the least squares ﬁtting of
the collected data to calculate the parameters a and b of the expression a ln(x) + b. A threshold
value, thld, is used to determine the point at which the estimated throughput is stabilized. The
threshold allows us to compute the recommended chunk size as a/thld. This value represents
the size at which the slope of the estimated curve varies less than 1%, and where the throughput
can be considered as near-optimal. In our experiments we use 4 samples and thld = 0.01 (also
1 Note that for the GPU, the time includes the computation time as well as the hostToDevice and deviceToHost times.

143

Adaptive Partitioning for Irregular Applications on Heterogeneous Chips

Antonio Vilches et al.

suggested in [2]). Using this procedure we found that the recommended chunk size for Nbody
is 700, which is corroborated by inspecting Figure 1(a).
Unfortunately, as Figure 1(b) shows, irregular applications do not follow this logarithmic
behavior. However, LogFit heuristic assumes that the near-optimal throughput for irregular
applications can be approximated as the near-optimal throughput computed for each one of
the intervals, tG0 , tG1 , . . . tGi−1 , tGi , tGi+1 , . . .. The near-optimal throughput for each interval
can be estimated now from a logarithmic expression, and from that expression we get the new
recommended GPU chunk size as explained above. The procedure is the following:
• Initially, to ﬁnd the ﬁrst GPU chunk size, G(tG0 ), for the ﬁrst scheduling interval, tG0 , we
sample 4 block sizes, xi = nEU × 2k−1 , k = 1 : 4, being nEU 2 the number of Execution
Units -EU- on the target GPU, and measure their corresponding throughput, yi , getting
a set setSamples = {(x1 , y1 ), (x2 , y2 ), (x3 , y3 ), (x4 , y4 )}. Next, we apply the logarithmic
ﬁtting to that set, as previously explained, to get G(tG0 ) = a/thld.
• After executing the chunk of iterations in the GPU, we get the time, T (tG0 ) and measure
the throughput λG (tG0 ) = G(tG0 )/T (tG0 ). This measured throughput and the corresponding chunk size represent a new updated sample that can be used by our ﬁtting procedure to ﬁnd the recommended GPU chunk size for the next scheduling interval tG1 . To
avoid re-taking four samples in the next scheduling interval, we use the previous set of samples and just replace the fourth sample with the new one, so that the set of samples that we
use to compute G(tG1 ) is now setSamples = {(x1 , y1 ), (x2 , y2 ), (x3 , y3 ), (G(tG0 ), λG (tG0 ))}.
In general, for a given time interval tGi for which we want to compute its recommended
chunk size G(tGi ), we use the set of samples given by {(x1 , y1 ), (x2 , y2 ), (x3 , y3 ), (G(tGi−1 ),
λG (tGi−1 ))}. We have analytically and experimentally validated that re-sampling for each
time interval using small chunk sizes ({(x1 , y1 ), (x2 , y2 ), (x3 , y3 ), (x4 , y4 )}) is not worthwhile: the penalty incurred by processing small chunk sizes at sub-optimal throughput is
not compensated by the throughput improvement obtained due to a more accurate measurements in the scheduling interval. In any case, (G(tGi−1 ), λG (tGi−1 )) already represents
an accurate measurement for the next interval.
The rationale for monitoring the throughput and using this metric to adjust the chunk size
is that if there is a change in the throughput, it is because the workload regime has changed and
we need to adapt the GPU chunk size accordingly. If the throughput has increased with respect
to the previous time interval, this is due to a decrement in the time per iteration to compute
this last assigned chunk, or in other words, either the workload per iteration has decreased or
the number of cycles that the EU have been stalled has been reduced due to an increment in
the number of coalesced memory accesses. In any case, a problem of GPU under-utilization
may appear. Thus, we increase the chunk size in order to increase Thread Level Parallelism
(TLP). If there is not GPU under-utilization, enlarging the chunk size will not worsen the
throughput, otherwise the throughput will likely increase in these scenarios. On the contrary,
if the throughput has fallen, then there has been an increment in the time per iteration to
compute the last chunk, which can be due to either an increment in the workload per iteration
or an increment in the number of cycles that the EU have been stalled that can be produced by
a higher number of uncoalesced memory accesses. In these cases, we reduce the chunk size in
order to alleviate a probable problem of over-provisioned GPU and for the sake of better load
sharing with the CPU.
Regarding the policy to set the CPU core chunk size, LogFit follows the heuristic by Navarro
et al. [15]. Basically, this heuristic is based on a strategy to minimize the load imbalance among
2 nEU

144

= clGetDeviceInfo(deviceId, CL DEVICE MAX COMPUTE UNITS)

Adaptive Partitioning for Irregular Applications on Heterogeneous Chips

Antonio Vilches et al.

the CPU cores and the GPU while optimizing the throughput of the system. To that end, the
optimization model described there recommends that: each time that a chunk is partitioned
and assigned to a device, its size should be selected such that it is proportional to the device’s
eﬀective throughput. Therefore, in our partitioning algorithm the chunk size assigned to a CPU
core, C(tCi ) veriﬁes: C(tCi )/λC (tCi−1 ) = G(tGi )/λG (tGi−1 ), where λC (tCi−1 ) and λG (tGi−1 ) are
the CPU and GPU throughputs of the previously computed scheduling interval, respectively.
In other words, we have: C(tCi ) = G(tGi ) · λC (tCi−1 )/λG (tGi−1 ).

5

Experimental Results

We run our experiments on a quad-core Intel Core i7-4770, 3.4GHz, based on Haswell architecture and featuring the HD-4600 on-chip GPU. We rely on Intel Performance Counter Monitor
(PCM) tool [7] to access the HW counters (which also provide energy consumption in Joules).
Intel TBB 4.2 provides the core template of the heterogenous parallel for. The GPU kernels are implemented with Intel OpenCL SDK 2014. The benchmarks are compiled using Intel
C++ Compiler 14.0 with -O3 optimization ﬂag. We measured time and energy in 10 runs of
the applications and report the average.
We have selected ﬁve benchmarks: Nbody [10] and Barnes Hut [12] presented in Section 2,
plus three other irregular applications, PEPC3 [8], CFD from the Rodinia suite [5], and SpMV
from the SHOC suite [6]. For the Nbody benchmark an input set of 10,000 bodies and 50
time-steps were simulated. For Barnes Hut an input set of 100,000 bodies and 75 time-steps
were computed. PEPC is fed with 100,000 point charges and 75 time-steps. CFD uses the
missile.0.2M input data (which considers 2,000 time-steps) and SpMV processes the GL7d13
sparse matrix from the University of Florida Sparse Matrix Collection that exhibits a triangularlike proﬁle. Barnes Hut and PEPC are examples of coarse grained application (each iteration
executes in 100 and 110 microseconds, respectively, on one Haswell core) while CFD and SpMV
are ﬁne grained ones (0.3 and 7 microseconds per iteration, respectively, on one Haswell core).
Initially, we performed a sensitivity study of the main parameters of the LogFit partitioning
strategy using our benchmarks, ﬁnding that by increasing (decreasing) thld, our approach tends
to ﬁnd smaller (greater) chunk sizes. While smaller chunk sizes quickly increase times, greater
chunks also degrade times but the variation is smaller. We also considered more samples during
the ﬁtting, ﬁnding that by doubling the number of samples the sizes tend to slightly increase
(less than 5%) which slightly degrades time (less than 2%).

5.1

Alternative partitioning strategies

To validate our LogFit approach we compare with three other related strategies: Static, Concord [11] and HDSS [2]. As a baseline, we use a Static partitioner (Oracle-like) that assigns one
big chunk to the GPU and the rest of the iterations to the CPU. The size of this single GPU
chunk is computed by a previous oﬄine search phase that exhaustively looks for a partitioning
of the iteration space between CPU and GPU that minimizes the execution time. This proﬁling
step runs the application 11 times. For each run, the percentage of the iteration space oﬄoaded
as a single chunk to the GPU varies (between 0% –only CPU execution– and 100% –only GPU
execution– using 10% steps). With this approach, proﬁling results for Nbody and Barnes Hut
show that 50% and 70% of the iteration space should be computed on the GPU, respectively.
Notice that this Static approach does not have runtime scheduling overheads, but the proﬁling
step requires 11 runs for each value of nT hreads > 1.

3 PEPC is a Barnes Hut like problem but it computes electrical forces instead of gravitational ones. Another
diﬀerence is that our Barnes Hut implementation sorts the particles to better exploit spatial locality.

145

Adaptive Partitioning for Irregular Applications on Heterogeneous Chips

Antonio Vilches et al.

The other two approaches, Concord and HDSS have a proﬁling phase where the relative
speed of the GPU and the CPU is computed. Concord computes the relative speed with an
online proﬁling phase where the GPU is assigned a ﬁxed chunk size and the CPU cores pick
chunks until the GPU ﬁnishes its chunk. Then, Concord switches to the execution phase where
the relative speed computed during the proﬁling phase is used to partition once the remaining
iterations between the CPU and GPU. Concord also considers re-proﬁling until the relative
speed stabilizes or until a given percentage of the iteration space (50%) is processed. HDSS
uses a training phase to compute the relative speed for both, the GPU and the CPU. It starts
with a small chunk size and increases it gradually while recording the corresponding throughput
of each sample. With the ﬁrst four samples, HDSS ﬁts a logarithmic curve and computes a ﬁrst
recommended GPU chunk size. HDSS keeps iterating in the training phase by adding samples
and recomputing the logarithmic ﬁtting and the relative speed until the diﬀerence between
two consecutive speeds is less than a given threshold (1%) or a ﬁxed percentage (20%) of the
iteration space has been processed. Then, it moves to a completion phase where block sizes
computed in the adaptive phase are no longer used. Instead, HDSS starts assigning chunks to
the CPU and GPU relying on a Modiﬁed Guided Self-Scheduling (MGSS): it ﬁrst assigns the
largest possible chunk size to each device considering its relative speed and gradually reduces
the chunk sizes towards the end of the iteration space to avoid load imbalance.
LogFit departures from these two previously described approaches in two ways. First,
instead of looking for an stable relative speed that may never be found for irregular codes,
LogFit’s main goal is to adaptively select the recommended GPU chunk size that is big enough to
fully occupy the GPU execution units while controlling the number of cycles that the execution
units get stalled due to uncoalesced memory accesses. LogFit considers that the relative speed
varies throughout the irregular code execution. Thus, for each scheduling interval, it also
recomputes the CPU chunk sizes to ensure load balance between the CPU and GPU; Second,
instead of having a steady phase following the adaptive one, LogFit keeps monitoring and
adapting the GPU and CPU chunk sizes, while trying to minimize the overheads of the adaptive
mechanisms, as we demonstrate next. Notice that LogFit and Concord remember information
from one time-step to the next one: LogFit remembers {(x1 , y1 ), (x2 , y2 ), (x3 , y3 )} samples,
whereas Concord remembers the GPU relative speed. HDSS does not re-use information.

5.2

Performance and energy comparison

Figure 2 shows, for our benchmarks and the four diﬀerent partitioning approaches: left - execution time (ms.), center - energy breakdown (Joules), and right - energy vs. performance (Joules
vs. iterations per ms.) on Haswell for diﬀerent number of threads. The energy breakdown
distinguishes the energy consumed on the cores, E CPU, on the GPU, E GPU, and on the uncore components of the chip, E Un. Note that when using only 1 thread, we get the only-GPU
execution. In the energy vs. performance plots, each mark in the lines represents the number of
threads from 1 to 5. In these plots, note that the closer to the right-bottom zone, the better the
tradeoﬀ between energy consumption and throughput. Typically, when increasing the number
of threads, the curves move towards the upper right corner (higher performance and energy
consumption), although there are exceptions, as explained next.
The study with the regular Nbody application aims at assessing the overhead of the adaptive
engine in the three adaptive schedulers w.r.t. the Static approach. As the ﬁgure shows, Concord
and LogFit perform similarly and execution times and energy consumption are close to those of
the Oracle-like Static implementation. Even for this regular benchmark, Concord and LogFit
can be faster than Static. In the ﬁgure this occurs for 3 threads because Static only evaluates 11
diﬀerent partitions, while the adaptive approaches may ﬁnd a ﬁner tuned distribution of work
between CPU and GPU. HDSS pays an additional overhead because it trains at the beginning
146

Adaptive Partitioning for Irregular Applications on Heterogeneous Chips

Antonio Vilches et al.

Figure 2: Results for Nbody, Barnes Hut, PEPC, CFD and SPMV benchmarks for Haswell. Time
and Energy graphs, left to right: Static, Concord, HDSS and LogFit.

of each time-step, while Concord and LogFit can use previously computed information. For 5
threads, Concord, HDSS and LogFit are 8%, 20%, and 4.9% slower than Static, respectively.
These results show that LogFit has an acceptable overhead in comparison with the Static
approach. Moreover, LogFit does not need the oﬄine proﬁling that Static requires.
Regarding the irregular coarse grained Barnes Hut and PEPC benchmarks, all adaptive
approaches outperform the Static one. In terms of execution time, for 5 threads LogFit runs
10% and 17% faster than Static for Barnes Hut and PEPC, respectively. However, Concord
runs just 2% (Barnes Hut) and 7% (PEPC) faster, whereas HDSS is 1% (Barnes Hut) and 8%
(PEPC) faster. As the Energy and Energy vs. Performance plots show, HDSS exhibits similar
performance as Concord, but the former is more energy eﬃcient because it tends to oﬄoad
147

Adaptive Partitioning for Irregular Applications on Heterogeneous Chips

Antonio Vilches et al.

a larger portion of the iteration space to the GPU, and therefore, the E CPU diminishes. In
any case, LogFit achieves the minimum energy (for 1 thread) and the maximum performance
(for 5 threads) as illustrated in the corresponding Energy-Performance plot. For PEPC and
any given number of threads, LogFit always delivers the best performance with the minimum
energy consumption.
For ﬁne grained applications, CFD and SpMV, the adaptive approaches implemented in
Concord and HDSS behave worse than LogFit. Let’s recall that in both approaches, the relative speed resulting at the end of the proﬁling/training phase is used during the rest of the
execution. However, in these benchmarks, this speed does not really ﬁt the actual speed of the
execution/completion phase. These two ﬁne grained benchmarks also present and additional
challenge because the GPU chunk size required to achieve a good enough GPU throughput is
comparable to the whole iteration space. HDSS performs poorly for CFD because this approach
enters the training phase for each of the 2000 time-steps of this application. During each run
of the training phase, HDSS feeds the GPU with sub-optimal chunk sizes. When ﬁnally HDSS
enters the completion phase there are not enough remaining iterations to assign to the GPU a
chunk size big enough to obtain the predicted GPU throughput (the estimated relative speed
can not be guaranteed). In addition, this leads to load imbalance with the CPU. By looking at
the energy graphs we can note that for Concord and HDSS, the GPU consumes more energy
(E GPU) than for LogFit. This is explained because Concord and HDSS always assign the
largest possible chunk size to the GPU in their completion phase. However, LogFit successfully
detects that, for the exhibited granularity, the number of available iterations is not enough
to assign to the GPU a chunk suﬃciently large to obtain the predicted GPU throughput, so
it’s not scheduled on the GPU. All in all, for CFD and 5 threads, LogFit is 20% and 30%
faster than Concord and HDSS, respectively. Also for CFD, Concord and HDSS consume 13%
and 23% more energy, respectively. The advantage of LogFit is even bigger for SpMV and 5
threads: 94%, 69% and 52% faster and 82%, 55% and 58% more energy eﬃcient than Concord,
HDSS and Static, respectively. The Energy-Performance plots show that again LogFit is the
approach that consumes the least energy (for 1 thread) and the fastest one (for 5 threads). On
the average, considering the four irregular benchmarks and 5 threads, LogFit runs faster than
Static, Concord and HDSS by 18%, 28% and 27% and consumes 18%, 23% and 19% less energy.

6

Related works

Approaches such as CUDA [16], OpenCL [18] and OpenACC [9] facilitate the programming of
heterogeneous systems of a multicore and an associated GPU by supporting code portability
across the two devices. However, they rely on the programmer to specify how the workload
should be distributed between CPU and GPU.
Previous works consider the problem of automatically scheduling on heterogeneous platforms
with a multicore CPU and an integrated or discrete GPU [14, 1, 3, 2, 11]. Among these works,
the ones closer to ours are HDDS [2] and Concord [11]. The main diﬀerence between these works
and ours is that they do not take into account the irregularity of the workload. Their main
focus is to determine the computational speed of each device and with this information to assign
the maximum chunk size to the GPU (and the multicore CPU) to avoid load imbalance. HDSS
is introduced in [2] where the authors compare its proposal with Qilin [14], ﬁnding that HDSS
always outperforms the later. In Section 5 we have compared LogFit against HDSS [2] and
also Concord [11]. None of the mentioned related approaches change the block size dynamically
based on the throughput of the application. Among them, Concord is the only one that evaluates
irregular applications. It is also the only one that, like us, focus on heterogeneous CPU-GPU
chips. All other approaches use the more powerful discrete GPUs.
148

Adaptive Partitioning for Irregular Applications on Heterogeneous Chips

Antonio Vilches et al.

StarPU [1] and OmpSs [3] have runtime systems that support heterogeneous executions, but
the granularity of the workload that is oﬄoaded to the GPU is determined by the programmer
or the compiler, and not automatically determined as LogFit does.

7

Conclusions and future works

In this paper, we address the problem of ﬁnding the appropriate chunk size for GPU and CPU
cores in the context of parallel loops in irregular applications running on heterogeneous CPUGPU chips. We propose LogFit, a novel adaptive partitioning strategy that dynamically ﬁnds
the chunk size that gets near optimal performance for the GPU at any point of the execution,
while balancing the workload among the GPU and the CPU cores. Using a regular and a
set of irregular benchmarks, we have assessed the performance and energy consumption of our
partitioner with respect to a Static approach and other adaptive state of the art partitioners.
For the studied irregular benchmarks on Haswell and 5 threads, we outperform the Oracle-like
Static approach by up to 52% (18% on average) and avoid the exhaustive oﬄine proﬁling. With
respect to the state-of-the-art Concord and HDSS approaches and for 5 threads, we obtain up
to 94% and 69% of speedup improvement (28% and 27% on average), respectively. Among
all the approaches, LogFit is almost always the solution that results in the minimum energy
consumption or the maximum performance. As future work, we will consider the parameter of
energy consumption as part of the scheduling decisions.

References
[1] C. Augonnet, J. Clet-Ortega, S. Thibault, and R. Namyst. Data-aware task scheduling on multiaccelerator based platforms. In Proc of ICPADS, 2010.
[2] M.E. Belviranli, L.N. Bhuyan, and R. Gupta. A dynamic self-scheduling scheme for heterogeneous
multiprocessor architectures. ACM Trans. Archit. Code Optim., 9(4), January 2013.
[3] J. Bueno, J. Planas, A. Duran, R.M. Badia, X. Martorell, E. Ayguade, and J. Labarta. Productive
programming of GPU clusters with OmpSs. In Proc. of IPDPS, 2012.
[4] M. Burtscher, R. Nasre, and K. Pingali. A quantitative study of irregular programs on GPUs. In
Proc. of IISWC, 2012.
[5] Shuai Che et al. A characterization of the Rodinia benchmark suite with comparison to contemporary CMP workloads. In IISWC, 2010.
[6] A. Danalis, G. Marin, C. McCurdy, J.S. Meredith, P.C. Roth, K. Spaﬀord, V. Tipparaju, and J.S.
Vetter. The scalable heterogeneous computing (SHOC) benchmark suite. In GPGPU, 2010.
[7] R. Dementiev, T. Willhalm, O. Bruggeman, P. Fay, P. Ungerer, A. Ott, P. Lu, J. Harris, P. Kerly,
and P. Konsor. Intel performance counter monitor. www.intel.com/software/pcm, 2014.
[8] Paul Gibbon, Wolfgang Frings, and Bernd Mohr. Performance analysis and visualization of the
n-body tree code pepc on massively parallel computers. In PARCO, pages 367–374. Citeseer, 2005.
[9] Alistair Hart. The OpenACC programming model. Technical report, Cray Exascale Research,
2012.
[10] Intel. Intel OpenCL N-Body Sample, 2014.
[11] Rashid Kaleem et al. Adaptive heterogeneous scheduling for integrated GPUs. In Intl. Conf. on
Parallel Architectures and Compilation, PACT ’14, pages 151–162, 2014.
[12] M. Kulkarni, M. Burtscher, C. Cascaval, and K. Pingali. Lonestar: A suite of parallel irregular
programs. In ISPASS, Apr 2009.
[13] J.V.F. Lima, T. Gautier, N. Maillard, and V. Danjean. Exploiting concurrent GPU operations for
eﬃcient work stealing on multi-GPUs. In SBAC-PAD’12, pages 75–82, 2012.
[14] Chi-Keung Luk, Sunpyo Hong, and Hyesoon Kim. Qilin: Exploiting parallelism on heterogeneous
multiprocessors with adaptive mapping. In Proc. Micro, pages 45–55, 2009.
[15] A. Navarro, A. Vilches, F. Corbera, and R. Asenjo. Strategies for maximizing utilization on
multi-CPU and multi-GPU heterogeneous architectures. The Journal of Supercomputing, 2014.
[16] NVidia. CUDA Toolkit 5.0 Performance Report, Jan. 2013.
[17] James Reinders. Intel TBB: Multi-core parallelism for C++ programming. O’Reilly, 2007.
[18] S.A. Russel. Levering GPGPU and OpenCL technologies for natural user interaces. Technical
report, You i Labs inc., 2012.

149

