Available online at www.sciencedirect.com

ScienceDirect
Science
(2017) 2473–2477
This Procedia
space isComputer
reserved
for 108C
the Procedia
header, do not use it

This space is reserved for the Procedia header, do not use it

International Conference on Computational Science, ICCS 2017, 12-14 June 2017,
Zurich, Switzerland

Parallel Post-Processing of
Post-Processing
of
the Parallel
Earth Climate
Model Output
the Earth Climate Model Output

Gijs van den Oord and Rena Bakhshi
Gijs van den Oord and Rena Bakhshi
Netherlands eScience Center, Amsterdam, The Netherlands
{g.vandenoord,r.bakhshi}@esciencecenter.nl
Netherlands eScience Center, Amsterdam, The Netherlands
{g.vandenoord,r.bakhshi}@esciencecenter.nl

Abstract
The
increasing resolution of climate and weather models has resulted in a fast growth of their
Abstract
data
production.
This callsoffor
a modern
and efficient
approach
to the in
post-processing
The increasing
resolution
climate
and weather
models
has resulted
a fast growth of
of these
their
data.
To
this
end,
we
have
developed
a
new
software
package
in
Python
that
exploits
the
data production. This calls for a modern and efficient approach to the post-processing of these
parallel
nature
of
the
post-processing
workload
to
process
the
output
of
EC-Earth,
a
coupled
data. To this end, we have developed a new software package in Python that exploits the
atmosphere-ocean
model.
We describeworkload
the design
our post-processing
and
present
parallel nature of the
post-processing
to of
process
the output of package,
EC-Earth,
a coupled
benchmark
results
showing
the
achieved
speedup
with
increasing
the
number
of
parallel
threads.
atmosphere-ocean model. We describe the design of our post-processing package, and present
benchmark
resultsPublished
showingby
the
achieved
©
2017 The Authors.
Elsevier
B.V.speedup with increasing the number of parallel threads.
Peer-review
under
responsibility
of
the
scientific
committee
of the International Conference on Computational Science
Keywords: climate models, CMOR, parallel
post-processing
Keywords: climate models, CMOR, parallel post-processing

1 Introduction
1
Introduction
The Earth System Model (ESM) EC-Earth [7] is widely used in the European climate science
research.
consists
of several
(atmosphere,
ocean,
land,
etc.) coupled
The EarthItSystem
Model
(ESM)components
EC-Earth [7]
is widely used
in the
European
climatethrough
science
exchange
of
heat,
moisture
and
momentum
fields.
Over
the
last
years,
ESMs
increased
in
research. It consists of several components (atmosphere, ocean, land, etc.) coupled
through
complexity,
in
terms
of
spatial
resolution,
number
of
components,
and
numerical
precision.
The
exchange of heat, moisture and momentum fields. Over the last years, ESMs increased in
parallelized
numerical
that form
the backbone
of the climate
models precision.
can nowadays
complexity, in
terms ofalgorithms
spatial resolution,
number
of components,
and numerical
The
be
run
at
unprecedented
scales
thanks
to
the
growing
capability
of
supercomputers.
parallelized numerical algorithms that form the backbone of the climate models canOptimized
nowadays
fluid-dynamical
and atmospheric
simulation
that
benefit from
the continuous increase
of
be run at unprecedented
scales thanks
to the codes
growing
capability
of supercomputers.
Optimized
processor
cache
size,
instruction
vector
length
and
number
of
cores
per
die
enable
us
to
explore
fluid-dynamical and atmospheric simulation codes that benefit from the continuous increase of
dynamical
systems
higher spatial
andlength
temporal
whileper
avoiding
a penalty
in time
processor cache
size,atinstruction
vector
and resolutions,
number of cores
die enable
us to explore
and
energy
consumption.
For
example,
it
is
feasible
to
let
coupled
atmospheric-ocean
models
dynamical systems at higher spatial and temporal resolutions, while avoiding a penalty in
time
simulate
100
years
of
climate
evolution
at
a
resolution
of
about
10
km,
as
is
currently
carried
and energy consumption. For example, it is feasible to let coupled atmospheric-ocean models
out
within
theyears
PRIMAVERA
joint effortat[2]a by
various of
climate
groups,
amongcarried
which
simulate
100
of climate evolution
resolution
aboutmodeling
10 km, as
is currently
the
EC-Earth
consortium.
out within the PRIMAVERA joint effort [2] by various climate modeling groups, among which
increase in model resolution goes along with increasing data production rate,
the Naturally,
EC-Earth an
consortium.
butNaturally,
the improvements
in mass
storage
technologies
keepincreasing
up with this
development.
This
an increase
in model
resolution
goes cannot
along with
data
production rate,
impacts
the
I/O
performance
of
the
climate
programs
[3],
in
particular
off-line
post-processing
but the improvements in mass storage technologies cannot keep up with this development. This
algorithms
typically exhibit
computational
intensity
and frequent
access, for
impacts thewhich
I/O performance
of thelow
climate
programs [3],
in particular
off-line disk
post-processing
algorithms which typically exhibit low computational intensity and frequent disk access, for
1

1877-0509 © 2017 The Authors. Published by Elsevier B.V.
Peer-review under responsibility of the scientific committee of the International Conference on Computational Science
10.1016/j.procs.2017.05.146

1

2474	

Parallel Post-Processing of the
Model
Output
van2473–2477
den Oord and Bakhshi
GijsEarth
van denClimate
Oord et al.
/ Procedia
Computer Science 108C (2017)

example when computing time averages of meteorological gridpoint fields or interpolating such
data onto a new grid. Nonetheless it often remains preferred to have a separate post-processing
step to keep the high-frequency model output on the original grid accessible for further studies.
(This could be also achieved by a library aggregating the data in memory during the model run,
but this functionality is absent in the atmosphere model in EC-Earth.) Indeed, many climate
models have their observational and analysis data output post-processed into so-called CMIP
standardized format such that it can be used for comparison with climate models in the same
way and independent of the platform on which the models were executed [5].
Meanwhile, the number of requested post-processed fields has also increased significantly for
the next generation of model intercomparison projects (MIPs) in the climate science community [6]. It is therefore important to minimize the computation time by exploiting the parallel
nature of the problem of processing many variables at once. This is the problem we address in
this paper. We describe the design of our post-processing package ece2cmor3, implemented in
Python, and we present benchmarking results showing the achieved speedup by increasing the
number of parallel threads.

2

Design

In this section we describe the tools and components that are part of our parallel post-processing
tool. Figure 1 summarizes the data flow in ece2cmor3. The EC-Earth model output for
the PRIMAVERA runs consist of GRIB-files produced by the atmosphere component, the
Integrated Forecasting System (IFS) [4], and netCDF files produced by the coupled ocean model
NEMO [8]. The latter component contains an online post-processing capability, provided by the
XIOS library. The atmospheric output is post-processed with existing technology, the widely
used Climate Data Operators (CDO) [1] to produce netCDF files for all requested variables.
After this step, all variables are extracted from netCDF and sent to the CMOR3 library [9]
which handles the reformatting to CMIP conventions. By using the Python language and
wrappers around these components, we have avoided complex shell scripts and established a
maintainable library with a straightforward API. Throughout the code we abundantly exploit
standard Python packages, e.g. for date and time arithmetic and multithreading.
The CMOR library rewrites input data according to a set of rules that are encoded in a
set of JSON-files, referred to as CMOR-tables. These tables contain information about variable

Figure 1: Graph of the data flow in the ece2cmor3 program. The parallelized processing is
highlighted in the violet box.

2

	

Parallel Post-Processing of the
Model
Output
van2473–2477
den Oord and Bakhshi
GijsEarth
van denClimate
Oord et al.
/ Procedia
Computer Science 108C (2017)

names and descriptions, but also the type of data reduction that has been applied to highfrequency instantaneous fields. We identify the workload as a set of independent tasks that can
be executed in parallel. Each task consists of a target, a variable in one of the CMOR-tables,
and a source, a variable inside one of the model output files. The tasks, which are all considered
pairwise independent, are automatically constructed from a user-defined set of desired targets
and a fixed lookup table containing pairs of source variables and possible targets. Hence the
user input list of targets completely determines the workload.
The ocean tasks are assumed to be post-processed already and are sent to the CMOR library
right away. The atmosphere tasks are grouped in a thread-safe queue, and a thread pool is used
to perform the post-processing of these tasks concurrently. The results of the post-processing
worker threads are stored as temporary netCDF files (keeping them in memory is currently not
supported by CDO), which serve as input for the output rewriting step in CMOR. The software
supports chunking the tasks to limit the amount of disk space used by these temporary files.
We use Python’s standard threading package for the parallel execution of the worker threads.
Hard-coded assumptions in the code about the nature of the resulting variables are kept to a
minimum. This is important because all information is contained in the input tables, which tend
to be volatile and will differ for various MIPs studies. Finally we note that a significant speedup
has been achieved by optimizing the ordering of the CDO operators for data reduction, to the
extent where it leaves the result unchanged. A dynamical field will typically be processed using
(i) selection criteria for extracting data from the model output, (ii) time aggregation operators
mapping to lower time resolution and (iii) a grid projection operator mapping the data to a
regular Gaussian grid1 . The latter step is costly for high-resolution 3D fields, but it is a linear
operation (matrix multiplication) which commutes with linear time aggregation operators such
as averaging. Hence it can often be applied as the last step on the reduced data.

3

Benchmark results

The result of our effort is the open source Python package ece2cmor3, which can be found
at https://github.com/goord/ece2cmor3. This code is also included in the PRIMAVERA
branch of EC-Earth 3.2 and will soon be merged to the EC-Earth trunk as a successor of the
existing ece2cmor code. For benchmarking we focus upon the post-processing of 37 six-hourly
three-dimensional fields (see, e.g., humidity field in Fig. 2), which constitute the bulk of the
output data processing time within the PRIMAVERA project 2 . We have tested this workload
on output from the standard resolution model, denoted T255 containing 65792 spectral modes,
as well as the high resolution version T511 which has 262656 spectral gridpoints.
A single month of high-resolution six-hourly data takes 80GB of disk space, whereas the
post-processed temporary netCDF files together occupy 68GB. We have run this test on the
ECMWF Cray XC40, which consists of Intel Xeon nodes of the Broadwell generation with 18
physical cores per node and 128GB of DDR4 memory. As temporary storage we use the scratch
drive, managed by the parallel LUSTRE file system.
The single-threaded version takes respectively 1 and 2.5 hours to finish processing a month
of respectively standard and high resolution model output, which is slower than the simulation
itself. By launching 16 worker threads we can achieve a speedup of almost a factor of eight
(see Fig. 3). This speedup by multithreading shows that the process is not simply limited by
1 IFS

output is usually defined on a reduced Gaussian grid or in a spectral representation
atmospheric fields are geopotential, temperature, north- and eastward wind speed, vertical velocity, relative humidity, specific humidity and cloud coverage and cloud liquid water and ice content. Various projections
upon pressure levels/model levels and various averaging operations make it to 37 post-processing tasks.
2 The

3

2475

2476	

Parallel Post-Processing of the
Climate
Output
van2473–2477
den Oord and Bakhshi
GijsEarth
van den
Oord et Model
al. / Procedia
Computer Science 108C (2017)

Figure 2: An example of 3D field calculated
from CMIP6 models: relative humidity at
the bottom model layer monthly average
over January 1990.

Figure 3: Speedup of the post-processing
workload with multithreading for standard
(T255) and high (T511) resolution sixhourly EC-Earth atmosphere output.

the bandwidth to the storage system, but also by the computationally intensive transformation
from variables in the spectral representation to a regular Gaussian grid. The less than ideal
scaling behavior can be explained by the finite bandwidth to temporary storage and DRAM,
and the fact that CDO itself has been configured to run four threads per processing task. The
input data can be trivially split into chunks of variables, and therefore a hybrid approach with
multiple processes on different compute nodes can be used if better scaling is needed.

4

Conclusion and Outlook

We have written a package for processing the output of EC-Earth that is flexible, maintainable and achieves a good performance by parallelizing the most time-consuming part of the
workload. Our benchmark results show that ece2cmor3 can perform its duty in EC-Earth for
the upcoming high-resolution CMIP6 model runs. Further speedup will be achieved by having
the program automatically split up the input variable list into chunks which are processed by
different subprocesses. In the longer term, one can link the atmosphere model to an on-line
postprocessing library such as XIOS or some library version of CDO. This is a necessary step
to increase space and time resolutions further in future CMIP-compliant climate simulations
with EC-Earth. Moreover, it will be useful to make all atmospheric spectral variables available
on the spatial grid, removing the need for expensive spectral transformation altogether.
Acknowledgements This work has been funded by the European Union’s Horizon 2020
research programme under Grant Agreement 641727.

References
[1] CDO: Climate Data Operators. Available at https://code.zmaw.de/projects/cdo.
[2] EU-H2020 PRIMAVERA project. https://www.primavera-h2020.eu.
[3] M. Asif, A. Cencerrado, O. Mula-Valls, D. Manubens, F. Doblas-Reyes, and A. Cortés. Impact
of I/O and data management in ensemble large scale climate forecasting using EC-Earth3. In

4

	

Parallel Post-Processing of the
Climate
Output
van2473–2477
den Oord and Bakhshi
GijsEarth
van den
Oord et Model
al. / Procedia
Computer Science 108C (2017)

[4]
[5]

[6]

[7]

[8]
[9]

Proc. Conf. on Computational Science (ICCS), volume 29 of Procedia Computer Science, pages
2370–2379. Elsevier, 2014.
ECMWF. IFS documentation. CY36R4. Available at http://www.ecmwf.int/en/forecasts/
documentation-and-support/changes-ecmwf-model/ifs-documentation.
V. Eyring, S. Bony, G. A. Meehl, C. A. Senior, B. Stevens, R. J. Stouffer, and K. E. Taylor.
Overview of the Coupled Model Intercomparison Project Phase 6 (CMIP6) experimental design
and organization. Geoscientific Model Development, 9(5):1937–1958, 2016.
V. Eyring, P. J. Gleckler, C. Heinze, R. J. Stouffer, K. E. Taylor, V. Balaji, E. Guilyardi, S. Joussaume, S. Kindermann, B. N. Lawrence, G. A. Meehl, M. Righi, and D. N. Williams. Towards
improved and more routine Earth system model evaluation in CMIP. Earth System Dynamics,
7(4):813–830, 2016.
W. Hazeleger, X. Wang, C. Severijns, S. Ştefănescu, R. Bintanja, A. Sterl, K. Wyser, T. Semmler,
S. Yang, B. van den Hurk, T. van Noije, E. van der Linden, and K. van der Wiel. EC-Earth V2.2:
description and validation of a new seamless earth system prediction model. Climate Dynamics,
39(11):2611–2629, 2012.
G. Madec. NEMO ocean engine. Note du Pole de modelisation, Institut Pierre-Simon Laplace
(IPSL), France, (27).
D. Nadeau, C. Doutriaux, T. Bradshaw, J. Kettleborough, T. Weigel, E. Hogan, and P. J. Durack.
PCMDI/cmor: CMOR version 3.2.2, March 2017. Available at http://cmor.llnl.gov/,.

5

2477

