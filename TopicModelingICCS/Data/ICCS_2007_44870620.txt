Parallelization of Generic Libraries Based on
Type Properties
Prabhanjan Kambadur, Douglas Gregor, and Andrew Lumsdaine
Open Systems Laboratory
Indiana University
Bloomington, IN 47405
{pkambadu, dgregor, lums}@osl.iu.edu

Abstract. This paper describes a new approach for parallelizing generic
software libraries. Generic algorithms are expressed in terms of type properties, which allows them to work with entire families of types rather than
speciﬁc types. Despite this generality, generic algorithms can be made as
eﬃcient as their hand-coded variants through the use of specialization,
which provides algorithm variants tuned for types with certain properties. Our approach leverages the specialization mechanism of generic
programming to eﬀect parallelism. We illustrate the process of specializing generic algorithms for parallelism using common algorithms in the
C++ Standard Template Library. When the resulting algorithms are invoked with types that have the required properties for parallelization,
the parallel variants of these algorithms are executed. We illustrate that
our parallelization strategy is simple, practical, and eﬃcient, using tools
and techniques available in most commercial compilers.
Keywords: Parallel Algorithms, Specialization, Type Properties.

1

Introduction

The advent of many-core and multi-core processors will signiﬁcantly broaden the
applicability of parallel computing. While this new era of ubiquitous parallelism
will surely bring new languages and tools built from the ground up for parallelism, a vast majority of tomorrow’s potential parallel applications are today’s
sequential programs. The primary challenge to ubiquitous parallelism is the need
to ease the transition from sequential to parallel, allowing existing applications
to be parallelized without requiring them to be rewritten or refactored.
The ultimate goal for enabling parallelization of sequential applications is the
fully automatic parallelizing compiler. Although there has been research on such
compilers for many years, practical solutions have remained elusive. An alternate solution to parallelization is to provide user-level libraries to aid in writing
explicitly parallel applications. Various user-level libraries such as pthreads and
MPI have provided services that enable parallelization of applications. However,
the programming interface that these libraries oﬀer is very low-level. Thus, they
are both diﬃcult to use and often lack support for the abstractions of relatively
high-level languages like C++.
Y. Shi et al. (Eds.): ICCS 2007, Part I, LNCS 4487, pp. 620–627, 2007.
c Springer-Verlag Berlin Heidelberg 2007

Parallelization of Generic Libraries Based on Type Properties

621

A ﬁrst step towards parallelizing existing sequential applications is to parallelize those components (libraries) that are most commonly used by these
applications. When parallel versions of commonly used libraries become available, previously sequential applications are “automatically” parallelized. This
approach has been successfully followed in many scientiﬁc libraries (for example,
BLAS [1] [2]). In this paper, we describe the methodology by which we are able
to accurately parallelize applications that use generic libraries. For example, we
are able to parallelize the following code segment:
std::vector<int> vec(100);
// Initialize vec...
std::accumulate (vec.begin(), vec.end(), 1, std::multiplies<int>());

This code segment calls accumulate(), a generic algorithm implemented in the
C++ Standard Template Library (STL) [3]. The STL is an excellent example of
Generic Programming, an important programming paradigm that is used for the
development of highly eﬃcient and maximally reusable software [4]. Moreover,
the STL is a widely-used portion of the C++ Standard Library, so parallelizing
it can have an immediate impact on many existing C++ applications.
In this paper, we illustrate the parallelization of the generic algorithms in
the STL through the use of type properties, and demonstrate how our approach
automatically parallelizes calls to STL algorithms. Our parallelization is based
on OpenMP, a set of compiler directives and library routines that enable users
to express shared-memory parallelism. We choose OpenMP because it is simple,
intuitive to use and is available in most modern C++ compilers. This last point
is particularly important: OpenMP, C++, and the STL are all existing, widelydeployed and widely-used technologies. By building on these technologies, our
OpenMP-parallel implementation of the STL can be used to parallelize existing
applications without requiring developers or end-users to install new tools or runtime systems; merely recompiling with our OpenMP-parallel STL is suﬃcient.

2

Related Work

ROSE [5] is a C++ infrastructure for source-to-source translation that provides an
interface for writing customized translators that optimize user-deﬁned high-level
abstractions. One such translator automatically inserts OpenMP statements into
user code. In principle, ROSE is akin to what we intend to achieve. However, the
methodologies are vastly diﬀerent. ROSE is an external tool that all developers
must install whereas we advocate a minimal approach based only on C++ and
OpenMP, both of which are widely available.
STAPL [6] [7] is designed in much the same spirit as the STL with containers,
iterators and algorithms. STAPL also allows users to specify their own scheduling
policies, data composition and data dependence environment. STAPL is designed
to operate on both shared and distributed memory systems and, therefore, it
includes an advanced runtime system. Existing applications need to be rewritten
to beneﬁt from STAPL, and end-users must install the STAPL runtime system
to use these applications.

622

P. Kambadur, D. Gregor, and A. Lumsdaine

HPC++ [8] is a C++ class and template library that portably supports sharedmemory and distributed-memory parallel applications. HPC++ supports both
task and data parallel applications. Unlike our approach, HPC++ delivers parallelism through a Java-like class library and consequently suﬀers from many of
the same drawbacks as STAPL. HPC++ is no longer in active development.
Threading Building Blocks (TBB) [?] is a library-based solution to sharedmemory parallelism that is tailored to the high-level of C++ abstractions. TBB
oﬀers a wide variety of customizable parallel algorithms and concurrent datastructures that facilitate development of parallel applications. However, parallelizing applications with TBB encumbers some amount of rewriting.

3

Generic Programming

Most modern software libraries consist of collections of related functions and
data types, often as archives of object ﬁles in conjunction with header ﬁles that
deﬁne interfaces to components in the library. Although the goal of these libraries
is to promote reuse, the requirements for invoking functions in these libraries are
often over-speciﬁed. This leads to a less than optimal reuse of the components.
With the explicit goal of improving the reuse of software libraries, generic programming has emerged as an important paradigm for the development of highly
eﬃcient and maximally reusable software [4]. Generic programming involves the
implementation of generic (abstract) algorithms that operate correctly and eﬃciently on a wide range of data types—including those not known at algorithm
design time—so long as the data types meet certain requirements expressed
as type properties. Related type properties are grouped together into concepts,
which identify domain-speciﬁc abstractions such as a Matrix, Vector, Graph, or
Iterator on which generic algorithms will operate.
C++ does not provide an inherent way to specify concepts and, therefore, there
exists no direct means of checking if a type models a particular concept. However,
we can approximate this behavior through the use of traits classes. Traits are a
C++ template technique that allow one to provide additional information about
a set of types without modifying the types themselves [9]. A key advantage of
using traits classes is that type properties can be attached retroactively. This
enables attachment of properties to types whose deﬁnitions are not available.
In many cases, we can improve the performance of a generic algorithm by
exploiting additional type properties. For example, consider a generic algorithm
that computes the circumference of an arbitrary polygon: it will require linear
time to sum the edge lengths. However, if we restrict the algorithm to equilateral polygons, we can compute the circumference in constant time. A generic
library typically provides the most generic algorithm (e.g., for arbitrary polygons) along with several specializations of the algorithm (e.g., for equilateral
polygons). When a user invokes the generic algorithm, the library will automatically select the most specialized algorithm at compile time, ensuring that the
library provides the best possible performance. In practice “selecting” the right

Parallelization of Generic Libraries Based on Type Properties

623

// Specialized Version
template <typename InIter, typename Func>
typename enable if<is parallel safe<Func>::value
&& is random access iterator<InIter>::value, Func>::type
for each(InIter ﬁrst, InIter last, Func f) {
typedef typename std::iterator traits<InIter>::diﬀerence type diﬀerence type;
const register diﬀerence type range = last−ﬁrst;
#pragma omp parallel for schedule(static)
for (register diﬀerence type i=0; i<range; i++) { f(ﬁrst[i]);}
return f;
}
// Fallback version
typename disable if<is parallel safe<Func>::value
&& is random access iterator<InIter>::value, Func>::type
for each(InIter ﬁrst, InIter last, Func f) {
for (; ﬁrst!=last; ++ﬁrst) f(∗ﬁrst);
return f;
}

Fig. 1. Parallel implementation of the STL for each() algorithm

algorithm often involves accessing traits classes to ensure that the input types
to the specialized algorithm meet all the type requirements.

4

Parallelizing Libraries

Specializations based on type properties can be used to extract optimal performance out of generic algorithms. Type properties serve as gate keepers that
ensure that input types meet the requirements stipulated by specialized versions
of algorithms. In this section, we elaborate through examples, the process of
parallelizing generic algorithms based on type properties.
First, consider the STL algorithm for each(), which applies a unary function
to all the values in the range [ﬁrst, last).
template <typename InIter, typename Func>
Func for each (InIter ﬁrst, InIter last, Func f);

This algorithm stipulates that InIter model the InputIterator concept, Func model
the UnaryFunction concept (i.e., have operator ( ) deﬁned that accepts one argument and returns a result) and that InIter’s value type be convertible to the
argument type of Func. Although these type requirements are necessary and sufﬁcient for sequential correctness, to parallelize for each() using OpenMP, input
types must possess additional properties. As OpenMP can only parallelize for
loops that are expressed in a simple single induction variable form, InIter must
model the RandomAccessIterator concept. Similarly, Func has to be reentrant and
give the same results regardless of the order in which the values are supplied to

624

P. Kambadur, D. Gregor, and A. Lumsdaine

it, i.e., Func must model the ParallelSafe concept. This particular requirement
ensures that it is safe to execute Func in parallel.
To check for these two additional type properties (i.e., meeting the ParallelSafe
and RandomAccessIterator concept requirements), we make use of the metafunctions is parallel safe and is random access iterator. is parallel safe is a simple templated structure that by default speciﬁes that no function object is safe to be
executed in parallel. To specify that the type plugged into Func models the
ParallelSafe concept, users need to fully specialize the is parallel safe structure.
This is nothing but a means of specifying traits for function objects. For convenience, is parallel safe has already been specialized for all the STL function
objects. Similarly, is random access iterator is used to determine whether a particular iterator models the RandomAccessIterator concept. Like is parallel safe,
is random access iterator is pre-deﬁned for all the STL iterators. The metafunctions enable if and disable if [10] are used to ensure that only one of the versions of
for each() is “turned on” when instantiated with concrete types. The code fragment that implements this specialization is given in Figure 1. To demonstrate
when the parallel for each is invoked, consider the following example.
struct square f { void operator()(int& value) {value ∗= value;}};
template <> struct is parallel safe <square f> {static const bool value = true; };
std::vector<int> vec(100);
std::list<int> lst(100);
for each(vec.begin(), vec.end(), square f());// PARALLEL
for each(lst.begin(), lst.end(), square f());// SEQUENTIAL

The ﬁrst call to for each() is supplied iterators to std::vector and a function
object of type square f. Since std::vector iterators support random access and
square f is safely invokable in parallel (as speciﬁed through the specialization of
the is parallel safe structure), this call is parallelized. The second call to for each()
uses the same function object as the ﬁrst, but is instantiated with iterators to
std::list, which do not support random access. Consequently, this call is not
parallelized and is executed sequentially instead.
Consider accumulate(), an STL algorithm that reduces based on some reduction operator, all the values in the iterator range [ﬁrst,last). For example, the
code fragment given below computes the sum of all the elements in the array.
int array[] = {1,2,3,4};
std::cout << accumulate (a, a+4, 10, std::plus<int>());

This fragment outputs 20, which is the sum of all the elements of the array
plus the initial value of 10. The prototype of accumulate() is given below.
template <typename InIter, typename T, typename Func>
T accumulate (InIter ﬁrst, InIter last, T init, Func binary op);

The type requirements for accumulate() stipulate that InIter model the InputIterator
concept, T be Assignable, T be convertible to value type of InIter and Func model the
BinaryFunction concept (i.e, have operator () deﬁned that accepts two arguments
and returns a result). As in the case of for each(), to parallelize this algorithm using

Parallelization of Generic Libraries Based on Type Properties

InIter

T

625

Func

accumulate()

No

Parallelizable?

Yes

OpenMP
Reduction?
No

Sequential

Parallel
+
Hand reduced

Yes

Parallel
+
OpenMP reduced

Fig. 2. Decision Logic followed for accumulate()

OpenMP, InIter is required to model the RandomAccessIterator concept and Func
to model the ParallelSafe concept. However, if we also want to use OpenMP’s reduction clause, we require T to be a fundamental type such as an int and Func to
be one of {+,−,∗ &, |, ˆ &&, ||} operators.
Therefore, we can parallelize accumulate() at two levels. First, we decide on
whether the input types meet the basic criteria for parallelization. If they do
meet these requirements, we then decide if we can use OpenMP’s reduction
clause. If it cannot be used, we manually reduce the results from the various
threads at the very end. Thus, we provide two diﬀerent parallel specializations
of the accumulate() algorithm, the most suitable of which will be chosen at
compile time. Had we provided only one of these, we would have either limited
the parallelism (e.g., by only providing the more limited OpenMP reduction)
or limited the potential for better performance (e.g., by using a manually-coded
reduction where an architecture-optimized OpenMP reduction would be better).
The decision process for accumulate() is illustrated by means of a ﬂowchart
in Figure 2. To illustrate the decision logic, we give some of code examples and
explain which ﬁnal version of accumulate() is used. First, consider the following
code that uses some STL iterators and STL function objects.
std::vector<int> array(100); ...
int sum = accumulate (array.begin(), array.end(), 1, std::multiplies<int>());

Here, iterator to std::vector is plugged in for InIter, int for T and std::multiplies<>
for Func. As iterators to std::vector model RandomAccessIterator concept, int is
a fundamental type and std::multiplies<int> can be mapped to the primitive

626

P. Kambadur, D. Gregor, and A. Lumsdaine
25

Speedup of accumulate (baseline std::accumulate)
Speedup of for_each (baseline std::for_each)
Speedup of sort (baseline std::sort)

Parallel Speedup

20

15

10

5

0

0

5

10
15
Number of Threads

20

25

Fig. 3. Parallel speedup achieved for several STL algorithms on a 32-way SMP

∗ operator, the parallel version with OpenMP reduction is used. Consider the
following example that uses fundamental types and user-deﬁned function objects.
std::vector<int> array(100); ...
int sum = accumulate (array.begin(), array.end(), 0, my bin op());

Although fundamental types are used, the iterators support random access, and
calls to my bin op can be parallelized (assuming is parallel safe is specialized),
OpenMP reductions cannot be applied because there is no primitive operator
to which we can map my bin op. Instead, we parallelize this invocation using a
hand-coded reduction. The same logic applies if T were a non-fundamental type
and a “known” function object such as std::plus<> were used. Furthermore, if, in
this example, array were to be a std::list instead of a std::vector, no parallelization
would be possible since iterators to std::list do not support random access.

5

Performance Evaluation

We have developed parallel specializations for several algorithms in the STL. In
this section, we give performance results for three of the algorithms: for each(),
sort() and accumulate(). Tests were run on a 32-way IBM AIX p690 SMP using 1.3
GHz Power4 processors with 192 GB of memory. IBM’s LoadLeveler was used for
batch queuing. XLC compiler version 7.0 was used with the −qsmp and −O3 ﬂags.
The performance results are given in Figure 3. All results are for 1 million randomly
generated ﬂoating point numbers that were stored in a std::vector. sort() showed a
maximum speedup of around 4x at 20 threads, accumulate() showed a speedup of
11x for 20 threads. for each(), which is an embarassingly parallel algorithm, scales
linearly with the number of threads for this particular problem size.

Parallelization of Generic Libraries Based on Type Properties

6

627

Conclusions

In this paper, we have presented a new methodology for parallelizing generic libraries based on type properties. Using modern C++ idioms such as specialization
and traits, we can isolate candidate invocations of generic algorithms for automatic
parallelization at the library level. Unlike most approaches to automatic parallelization or library-level parallelization, which require new compilers, source-tosource translation tools, or new run-time systems, our solution works with existing,
widely-deployed technologies (OpenMP and C++).
Preliminary results show that this approach gives excellent parallel speedup
for commonly used algorithms such as accumulate(), sort() and for each() on large
shared-memory machines; we have seen similar results on commodity desktops
and servers as well.
Acknowledgements. This work was supported by NSF grants EIA-0202048,
EIA-0131354, a grant by the Lilly Endowment and in part by Shared University
Research grants from IBM, Inc. to Indiana University.

References
1. Alpatov, P., Baker, G., Edwards, C., Gunnels, J., Morrow, G., Overfelt, J., van de
Geijn, R.: PLAPACK: Parallel linear algebra package. In: SIAM Parallel Processing
Conference. (1997)
2. Blackford, L.S., Choi, J., Cleary, A., D’Azevedo, E., Demmel, J., Dhillon, I., Dongarra, J., Hammarling, S., Henry, G., Petitet, A., Stanley, K., Walker, D., Whaley,
R.C.: ScaLAPACK: a linear algebra library for message-passing computers. In:
Proceedings of the Eighth SIAM Conference on Parallel Processing for Scientiﬁc
Computing (Minneapolis, MN, 1997). (1997)
3. Stepanov, A.A., Lee, M.: The Standard Template Library. Technical Report
X3J16/94-0095, WG21/N0482, ISO Programming Language C++ Project (1994)
4. Musser, D.R., Stepanov, A.A.: Generic programming. In Gianni, P.P., ed.: Symbolic and algebraic computation: ISSAC ’88, Rome, Italy, July 4–8, 1988: Proceedings. Volume 358 of Lecture Notes in Computer Science., Berlin, Springer Verlag
(1989) 13–25
5. Quinlan, D.: ROSE: Compiler support for object-oriented frameworks. Parallel
Processing Letters 10(2,3) (2000) 215–226
6. An, P., Jula, A., Rus, S., Saunders, S., Smith, T., Tanase, G., Thomas, N., Amato,
N., Rauchwerger, L.: STAPL: A standard template adaptive parallel C++ library.
In: Int. Wkshp on Adv. Compiler Technology for High Perf. and Embedded Processors. (2001) 10
7. An, P., Jula, A., Rus, S., Saunders, S., Smith, T., Tanase, G., Thomas, N., Amato,
N., Rauchwerger, L.: STAPL: An adaptive, generic parallel programming library
for C++. In: Wkshp. on Lang. and Comp. for Par. Comp. (LCPC). (2001) 193–208
8. Gannon, D., Beckman, P., Johnson, E., Green, T., Levine, M.: HPC++ and the
HPC++Lib Toolkit. (The High Performance C++ consortium)
9. Myers, N.C.: Traits: a new and useful template technique. C++ Report (1995)
10. J¨
arvi, J., Willcock, J., Hinnant, H., Lumsdaine, A.: Function overloading based on
arbitrary properties of types. C/C++ Users Journal 21(6) (2003) 25–32

