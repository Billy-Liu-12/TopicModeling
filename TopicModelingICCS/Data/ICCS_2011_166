Available online at www.sciencedirect.com

Procedia Computer Science 4 (2011) 768–777
Procedia Computer Science 00 (2009) 000–000

Procedia
Computer
Science
www.elsevier.com/locate/procedia

International Conference on Computational Science, ICCS 2011
CARMEN: Code analysis, Repository and Modeling for e-Neuroscience

Jim Austina, Tom Jacksona, Martyn Fletchera, Mark Jessopa, Bojian Lianga, Mike
Weeksa, Leslie Smithb, Colin Ingramc, Paul Watsonc.
a
The University of York, York, YO10 5DD, UK
The University of Newcastle, Newcastle NE17RU
c
The University of Stirling, Stirling FK9 4LA, UK

b

Abstract
The CARMEN (Code, Analysis, Repository and Modelling for e-Neuroscience) system [1] provides a web based portal platform
through which users can share and collaboratively exploit data, analysis code and expertise in neuroscience. The system has been
beendeveloped in the UK and currently supports 200 hundred neuroscientists working in a Virtual Environment with an initial
focus on electrophysiology data. The proposal here is that the CARMEN system provides an excellent base from which to
develop an ‘executable paper’ system. CARMEN has been built by York and Newcastle Universities and is based on over 10
years experience in the construction of eScience based distributed technology. CARMEN started four years ago involving 20
scientific investigators (neuroscientists and computer scientists) at 11 UK Universities (www.CARMEN.org.uk). The project is
supported for another 4 years at York and Newcastle, along with a sister project to take the underlying technology and pilot it as
a UK platform for supporting the sharing of research outputs in a generic way. An entirely natural extension to the CARMEN
system would be its alignment with a publications repository. The CARMEN system is operational on the domain
https://portal.CARMEN.org.uk, where it is possible to request a login to try out the system.
"Keywords: E-Science, Neuroscience, virtual collbaoration, software as a service, workflow"

1. Overview: The CARMEN System.
The CARMEN (Code, Analysis, Repository and Modelling for e-Neuroscience) system [1] provides a web based
portal platform through which users can share and collaboratively exploit data, analysis code and expertise in
neuroscience. The system has been beendeveloped in the UK and currently supports 200 hundred neuroscientists
working in a Virtual Environment with an initial focus on electrophysiology data. For clarity we provide a brief
overview of the main features of the system, before describing how it can support the concept of an executable paper
repository.
CARMEN provides researchers with a virtual research environment which facilitates collaboration and
interaction with four mains types of assets: data, metadata, services and workflows. Data and metadata are currently
structured for neurophysiology, but the mechanisms for data and metadata management are generic and hence the

1877–0509 © 2011 Published by Elsevier Ltd. Open access under CC BY-NC-ND license.
Selection and/or peer-review under responsibility of Prof. Mitsuhisa Sato and Prof. Satoshi Matsuoka
doi:10.1016/j.procs.2011.04.081

Jim Austin et al. / Procedia Computer Science 4 (2011) 768–777
Austin, J/ Procedia Computer Science 00 (2011) 000–000

769

platform is applicable for any science domain. Services, in CARMEN terms, are stand alone software applications
that are hosted on the platform and which can be executed on the underpinning compute resource using any
appropriate data assets on the system. Services are exposed by a simple execution GUI inside the portal, which
prompts the user to input the relevant operating parameters and input files/variables. Workflows provide the means
to orchestrate services into an execution pipeline; this is of particular relevance for the issue of an Executable Paper,
as will be described in later sections.
Data and metadata are submitted to the system via an upload wizard (see Figure 1.), which guides a user through
the metadata entry and the file selection process.

Fig 1. The metadata Upload Wizard.
Metadata for neurophysiology data is extensive, but an agreed format and schema has been agreed for the
CARMEN system [2]. In order to simplify the metadata entry process, users can create pre-populated templates,
which will auto-populate the wizard data entry forms, and allow the user to edit just the fields that change on a
regular basis for a particular experimental domain. Auto-population of the metadata schema from external XML or
other document structures is possible but is not currently implemented. The template feature is highlighted in Figure
2.

770 	

Jim Austin et al. / Procedia Computer Science 4 (2011) 768–777
Author name / Procedia Computer Science 00 (2011) 000–000

Fig 2. Metadata forms can be auto-populated from user-defined templates.
Data upload is managed through a JAVA application within the browser which can handle multiple, parallel
streaming across many HTTP ports, to accommodate large file sizes, and which also manages error checking and
other server-client transaction issues.
Access Control is critical to the management of the data upload process, and before a user can complete a data
upload process, they are are required to specify the access control lists for both the data and metadata for the file.
These are easily and quickly configured, remain under the control of the user completing the upload, and can be
modified at any future point in time to add additional users or groups, or to widen or reduce access.

Jim Austin et al. / Procedia Computer Science 4 (2011) 768–777
Austin, J/ Procedia Computer Science 00 (2011) 000–000

771

Fig 3. Controlling access to data and metadata via user controlled Access Lists
Services are also exposed to the user through a similar interaction wizard, Figure 3,which provides a simple,
intuitive interface for users to run applications whilst abstracting away the detail of the underlying compute
infrastructure (which is typically of little relevance to most users).

Fig 4. The Interface for executing services.

772 	

Jim Austin et al. / Procedia Computer Science 4 (2011) 768–777
Author name / Procedia Computer Science 00 (2011) 000–000

Services operate across local assets on the CARMEN system, but will only operate on data to which a user has
access rights. A wizard GUI steps the user through the input and output parameters necessary to run the application
(supplying default values where appropriate). Once submitted, the service and the chosen data set are staged to a
compute resource and the job is executed. A job log is maintained for each user so that they can monitor the
progress of their task, and so that they can interact with any outputs from the service.
Workflow is supported within the system by means of a graphical workflow editing tool, Figure 5. This allows
users to build a pipeline of individual services and execute them as an orchestrated set of tasks via the workflow
execution engine.

Figure 5: The workflow tool.
These workflows, similar to any asset on the system, can be stored, annotated and shared with other users. A
stored workflow, with versioning of the underlying services, means that any series of processing steps can be
repeated on demand, suitable for the requirements of an executable paper, as described in later sections.
1.1. The Architecture
The CARMEN system operates as Cloud and Software as a Service Model. It is underpinned by compute and
data resources at the University of York, and which form part of the White Rose Grid computing infrastructure [3].
As such the architecture is scalable and amenable to a fully accounted charge model, although services and data
storage are currently provided freely within the current project funding.
The architecture of the system is presented at a high level in Figure 6. There are four main components of the
system; the front end Portal, the execution engine, databases and a file virtualization system, Storage Request
Broker (SRB) [4]. A key aspect of the system is the ease by which software and tools can be made available in an
interactive Software as a Service model for end users. CARMEN services are based upon java wrapper code that is
written to handle command-line applications (written as python, matlab, perl, R scripts, or any compiled binary
executable). This java wrapper, along with the application, is wrapped up into a web service (as a WAR file) or into

Jim Austin et al. / Procedia Computer Science 4 (2011) 768–777
Austin, J/ Procedia Computer Science 00 (2011) 000–000

773

a loadable Java class in a JAR file. The WAR/JAR files can then be deployed onto the CARMEN servers as a
service.
There are currently two versions of CARMEN service; heavyweight web services and lightweight java loadable
classes. The heavyweight CARMEN services are based upon JAX-WS (Java API for XML Web Services) technology
and are bundled up as WAR files for deployment onto a tomcat container on a remote CARMEN server. The newer
(default) lightweight CARMEN services are based upon a simple dynamically-loadable Java wrapper class. They
are called lightweight services as they have a small JAR wrapping overhead (a few kByte) as opposed to approx
20MB of a JAX-WS WAR file. They are not lightweight in terms of functionality, as they actually have more
features. Being leaner, the lightweight CARMEN services are quicker to transfer over a network, unwrap, deploy
and to execute.
In order to hide the technology of web services from CARMEN users, web service creation and invocation is
performed within the CARMEN Portal using a browser (Note - Web Service creation is not integrated yet into the
CARMEN portal but exists in a demonstration portal). This is only possible due to the close integration of a service
metadata system. The service metadata is automatically generated when the service is created. The service metadata
system is used at service invocation time by the portal, and is totally hidden from the user. The metadata is used by
the portal to:






provide information to the CARMEN user on the service's functionality and I/O parameters;
generate a generic web service client on the fly;
deploy web services dynamically across the CARMEN servers;
provide a measure of input data validation for the user;
inform the portal how to generate portal interfaces; to present service input forms to the user, and to handle
and display output data.

1.2. Wrapping
Currently service wrapping is performed by the system administrators, although ultimately this will be achieved
from within the portal for users with the necessary skills.
Service wrapping is achieved using simple form-based web pages on the CARMEN Porta and takes 2-3 seconds
once the forms have been populated. As well as generating the CARMEN service WAR/JAR file, the portal stores
the WAR/JAR file in the CARMEN Service Repository (in SRB), and registers the service with the service metadata
database.
The CARMEN wrapping service can currently take Matlab (as a compiled executable), R scripts, Python scripts,
Perl Scripts, and (to date) any executable. These must be written as command line applications, and follow a simple
rule set.

774 	

Jim Austin et al. / Procedia Computer Science 4 (2011) 768–777
Author name / Procedia Computer Science 00 (2011) 000–000

Figure 6: The Underlying CARMEN architecture
2. Addressing the Executable Paper Challenge
For simplicity, in the following sections, we address each of the issues highlighted in the call, in turn, and then
discuss other issues experienced during the development of the CARMEN project.
2.1. Validation

How do we validate data and code, and decrease the reviewer’s workload? How can validation
of this information be made easy for the reviewer?

CARMEN provides an active repository, whereby both data and the software that produced the data can be
hosted in a manner that permits the code to be re-executed. This allows the reviewer to re-run the software and
dynamically generate the same results for validation purposes. One constraint is that the source code needs to be
made visible to the reviewer so that they can check that the code is the same as that available on the CARMEN
system. We expect users to place the source code in a standard source code repository which will be referred to and
linked from the meta-data attached to the service. One security option of the system allows a user to only share
assets with a set of known individuals. This is a suitable mechanism to submit the code and data for review. An
integrated system, that links to a journal publication system, would facilitate reviewing more efficiently. For
example, when the paper is uploaded for review, the system could prompt the user for code and data to be uploaded.
The CARMEN system would then use its existing upload mechanism to place the code and data in the system
(owned by the editor of the journal). The editor can then share this code and data with a reviewer. This is a relatively
simple modification to CARMEN.
2.2. Copyright/licensing

Data of this nature should always be freely available to researchers: how can this principle be
encouraged, while maintaining author’s patent and intellectual property protection?

Jim Austin et al. / Procedia Computer Science 4 (2011) 768–777
Austin, J/ Procedia Computer Science 00 (2011) 000–000

775

The CARMEN system solves the IP protection problem by only allowing a researcher to run a contributor’s
software and not download it. The code is kept private and can thus be protected. Data IP protection is more
difficult, as a user will want to look at any data before running it or typically will want to download the data to
experiment with it. In CARMEN all assets (software & data) are stored along with the meta-data about the assets.
Via a user controlled access list the user is able to allow researchers search and view just the meta-data, or the metadata and the assets. By restricting the view to just meta-data we are able to enforce a policy whereby an external
researcher must obtain permission to use the data or software from the submitter before they gain access to it
directly.
2.3. Systems

How do we convey work done on large-scale computers, which are possibly only available to a
small portion of the author/reader community?

CARMEN solves this by providing federated access to large compute clusters to run the users code. The current
system is hosted on the White Rose Grid, a multi-cluster system shared between the Universities of York, Leeds and
Sheffield as well as an associated node for CARMEN only at Newcastle University. The system is designed to map
jobs onto geographically remote machines which could be the White Rose Grid or other major supercomputer
resources. Access rights could be managed via CARMEN’s security infrastructure. The provision of a charging
system is also possible. The CARMEN system is being deployed as a UK pilot for sharing code and data from
research under a HEFCE funded (UK funding council for Universities) initiative which includes the possibility of a
charging model. Data sharing across platforms within CARMEN is dealt with through the Storage Request Broker
(SRB http://www.sdsc.edu/srb/index.php/Main_Page) technology, which provides a virtualised data storage system
operating across diverse and distributed computer systems.
2.4. Size

How do we manage very large file sizes?

Because CARMEN hosts both a user’s code and data, it allows a user to experiment with large data files without
the expense of downloading the files for local analysis. All analysis can be run on the code in situ on the remote
portal machines that hold the data. This is possibly the only practical solution to the use of such large data files. To
deal with large data processing on the computer cluster, data files are staged to a compute node then kept local to
that node for future runs (while capacity is available). To allow users to upload/download large data we have
developed a robust file transfer application. The final issue is backing up large data. In systems with 100’s of Tb,
backups can be expensive. One model is to assume that the user holds a copy (the backup) and the system maintains
the operational copy. This is our current policy.
2.5. Provenance

How to support registering and tracking of actions taken on the ‘executable paper?’
CARMEN currently records all actions taken within the system allowing us to mine and present information to a
user about how his code or data has been used within the system; information such as who viewed it, who ran which
service and when, what the results were. The main application of this will be to record how a researcher ran a
particular service and on what data so that it can be easily repeated. The system supports this with extensive metadata captured when a researcher uploads data or software to the system. The software is converted to a service along
with a meta-data framework to record how and when the service was last run.
Much more than this can be done with the provenance information. A major concern is how researchers assign
trust to the system, its services and the data. We have a new project examining this. Our plan is to use York
University’s extensive knowledge of mining large data sets to convert this data to useful knowledge, which will

776 	

Jim Austin et al. / Procedia Computer Science 4 (2011) 768–777
Author name / Procedia Computer Science 00 (2011) 000–000

focus on how to develop trust. One example is to show how users have used the data and services before, on
examples similar to the user’s request. If this has happened repeatedly, then trust in the system will be higher.
2.6. Other issues

How do we tackle risks and liabilities, including viruses and code contamination, plagiarism,
or other problems?

Security issues in such a system are very important because using CARMEN it is easy to upload data, and to
implement a service. This leads to the possibility of deploying malicious code. We are investigating testing
frameworks that may overcome this, which will be run before the code is made available, in addition we aim to sand
box untested code in its own virtual machine to reduce the possibility of cross infection.

The process of securely sharing data is being handled through three levels of sharing on the current system and
through the ability to share only the meta-data about the research asset or the meta-data and the asset. This allows a
user (owner) to inform researchers that the software or data is present, but it is this user decides whether to allow
others access. To support this strategy the user is able to grant only named users access to their data and/or services.
The system allows a group to be created and share research assets within the group or at the top level to share the
assets with all those with a login to the system.
Within the functioning framework that CARMEN provides there are many possible extensions to support the
executable paper. For example, all assets stored in CARMEN have a unique URL linked to them. This allows a user
embed these within their paper. Thus when the paper is read, a user may click on the link next to the result to be
directly sent to the service and data on CARMEN.
Although CARMEN is focused on neuroscience we are now porting it to new domains to work on generic
data/software. To allow this the meta-data schema is editable, allowing users to create templates particular to their
datasets.
CARMEN also tackles the problem of data interoperability within the electrophysiology domain. The problem
with many software packages is that they use non-standard data formats. We have developed a common data format
within CARMEN called NDF (Neurophysiology Data translation Format) that has a C++ and Matlab package for
allowing data in 6 commonly found electrophysiology formats to be converted, stored and used by users software.
This provides a model for how other data standards may be supported. NDF allows the system to fully support
workflows running on neuroscience data, as it allows services to easily write and read data from each other. NDF
may be extended to other domains if sufficient demand was present.
It is unlikely that any user will use a single service in their research, thus the composition of workflows using
multiple services is fundamental. CARMEN is looking to support these through a simple interactive portal based
graphical workflow editor, which interfaces to a workflow enactment system.
We have developed a simple code wrapping system for expert users to allow them to place a software component
on the system. At the moment this is made available to expert users, as the issues surrounding security and testing
are still being investigated.
To allow users to find the software and data relevant to a publication will require the integration of CARMEN
with a publications search engine, for example, allowing the use of DOI’s within CARMEN against an asset linked
to a publication. CARMEN already supports unique tagging of all stored assets and mechanisms to allow users
obtain direct access to these assets via external references that can be easily extended. CARMEN has a simple free
text search system at the moment that matches against the meta-data stored in the system, allowing users to find data
and software of interest. This could be integrated with any existing search engine used by the publication.

Jim Austin et al. / Procedia Computer Science 4 (2011) 768–777
Austin, J/ Procedia Computer Science 00 (2011) 000–000

777

Different journals are presented differently to users, thus some form of personalisation on login of the CARMEN
portal would be beneficial. We are in the process of allowing groups to be personalised for a given group, and assets
on the system made personal to that group. The portal could then present logos, colours and styles particular to a
journal as users registered to that journal log in.
3. Conclusions
The CARMEN Portal has been described; it provides a dynamic Software as a Service environment for
researchers to share assets relating to their research. These assets, data, workflows, algorithms, can be shared
publicly with any registered users of the system, or with a controlled group of users, via an access control list. The
system is more than a repository for the experimental assets, as the code and algorithms associated with their
research can be wrapped and made available to other users to run dynamically, either with their own data or with
related data stored on the system. Tools and services can thus be captured, shared amongst collaborators and
redeployed on demand via the Portal. Processes and experimental procedures can also be captured via a workflow
system, which can orchestrate a pipeline of processing activity. As such, we have argued that the CARMEN
platform would provide a highly relevant, scalable and generic platform to address the issues relating to the concept
of Executable Papers.
Acknowledgements
The CARMEN project is funded by the UK Engineering and Physical Sciences Research Council (EPSRC) as an
e-Science Pilot project under EPSRC grant reference EP/E002331/1. The CARMEN academic and industrial
collaborators are the Universities of Cambridge, Leicester, Manchester, Newcastle, Plymouth, St Andrews, Stirling,
Sheffield, Warwick, York; Imperial College, Cybula Ltd. and Lectus Therapeutics Ltd.
References
[1] Martyn Fletcher, Bojian Liang, Leslie Smith, Alastair Knowles, Tom Jackson, Mark Jessop, Jim Austin, "Neural Network Based Pattern
Matching and Spike Detection Tools and Services - in the CARMEN Neuroinformatics Project", Special Issue of Neural Networks on
"Neuroinformatics" Journal, 9th June, 2008, published by Elsevier.
[2] Gibson, F, et al, Minimum Information about a Neuroscience Investigation (MINI): Electrophysiology , Nature Precedings,
April 2009. http://precedings.nature.com/documents/1720/version/2
[3] Dew, JG Schmidt, M Thompson The white rose grid: practice and experience, Proceeding sof the UK eScience-All Hands,
Nottingham 2003
[4] Wan M, Chaitanya Baru, Moore R, Arcot Rajasekar , The SDSC storage resource broker, CASCON '98 Proceedings of the 1998
conference of the Centre for Advanced Studies on Collaborative Research.

