A Logic-Based Approach to Mining Inductive
Databases
Hong-Cheu Liu1 , Jeﬀrey Xu Yu2 , John Zeleznikow3 , and Ying Guan4
1

2

Department of Computer Science and Information Engineering,
Diwan University, Taiwan
hcliu@dwu.edu.tw
Department of Systems Engineering and Engineering Management,
The Chinese University of Hong Kong, China
yu@se.cuhk.edu.hk
3
School of Information Systems, Victoria University, Australia
John.Zeleznikow@vu.edu.au
4
School of Information Technology and Computer Science,
University of Wollongong, Australia
yguan@uow.edu.au

Abstract. In this paper, we discuss the main problems of inductive
query languages and optimisation issues. We present a logic-based inductive query language and illustrate the use of aggregates and exploit
a new join operator to model speciﬁc data mining tasks. We show how
a ﬁxpoint operator works for association rule mining and a clustering
method. A preliminary experimental result shows that ﬁxpoint operator
outperforms SQL and Apriori methods. The results of our framework
could be useful for inductive query language design in the development
of inductive database systems.

1

Introduction

Knowledge discovery from large databases has gained popularity and its importance is well recognised. Ever since the start of research in data mining, it
has been realised that the knowledge discovery process should be supported by
database technology. However, most eﬀorts on data mining and knowledge discovery have focused on developing novel algorithms and data structures and
these researches concentrated on examining the eﬃcient implementation issues.
While Data Base Management Systems (DBMS) and their enabling technology
have evolved successfully to deal with most of the data-intensive application
areas including decision support systems with OLAP queries, these techniques
are still insuﬃcient in a knowledge discovery environment. Therefore, databases
today are still using primarily a cache-mining approach, where the data is ﬁrst
extracted from the database to a memory cache, which is then processed using
procedural mining methods.
Research on inductive databases focuses on the integration of databases with
data mining. Such integration has been formalised in the concept of inductive
databases. The key ideas are that data and patterns (or models) are ﬁrst class
Y. Shi et al. (Eds.): ICCS 2007, Part I, LNCS 4487, pp. 270–277, 2007.
c Springer-Verlag Berlin Heidelberg 2007

A Logic-Based Approach to Mining Inductive Databases

271

citizen, i.e., they are handled in the same way. The emergence of Inductive
DBMS research aims to improve the current state of cache mining approach and
make it easy to mine databases by their query languages [1,2]. The one of crucial
criteria for the promising success of inductive databases is reasoning about query
evaluation and optimisation. In this paper, we will focus on inductive query
languages and optimisation issues.
Several specialised inductive query languages have been proposed and implemented, such as MSQL [3], DMQL [4] and MINE RULE [5]. These projects have
made a number of contributions including exploring and demonstrating some
of the key features required in an Inductive DBMS. Currently, these researches
have not led to signiﬁcant commercial deployments due to performance concern
and practical limitations.
The data mining query languages proposed in [3,4,5] require users to only provide high-level declarative queries specifying their mining goals. The underlying
inductive database systems need sophisticated optimiser with aim to selecting
suitable algorithms and query execution strategies in order to perform mining
tasks. Another tight-coupling approach using SQL implementations gives unrealistic heavy-burden on the users to write complicated SQL queries [6]. So it is
reasonable to explore alternative methods that make inductive databases realisable with current technology. Logic-based database languages provide a ﬂexible
model of representing, maintaining and utilising high-level knowledge. This motivates us to study a logic-based framework and develop relational operators
(e.g., ﬁxpoint) for intelligent data analysis.
In this paper, we consider the logic paradigm for association rule mining that
use the idea of least ﬁxpoint computation. We also demonstrate a logic query
language for data mining tasks and discuss optimisation issues. Some preliminary
experimental results show that our ﬁxpoint algorithm outperforms SQL-based
and the Apriori methods.

2

Inductive Query Languages

A desired feature of inductive database systems is the ability to support ad hoc
and interactive data mining in order to facilitate ﬂexible and eﬀective knowledge
discovery. Data mining query languages can be designed to support such a feature. In particular, declarative query language support acts an important role
in the next generation of Web database systems with data mining functionality.
Query systems should provide mechanism of obtaining, maintaining, representing and utilising high level knowledge in an uniﬁed framework. A knowledge
discovery support environment should be an integrated mining and querying
system capable of representing domain knowledge, extracting useful knowledge
and organising in ontology [7].
Designing a comprehensive data mining language is challenging because data
mining covers a wide spectrum of tasks and each task has diﬀerent requirements.
In this paper we provide some theoretical foundations for a logic-based data
mining query language.

272

3

H.-C. Liu et al.

Relational Computation for Association Rules

In this section, we investigate relational computation methods and demonstrate
a logic-based query paradigm for frequent itemset discovery that use the idea of
least ﬁxpoint computation.
3.1

Calculus+Fixpoint

We provide a non-inﬂationary extension of the complex value calculus with recursion and aggregate operation. We deﬁne a ﬁxpoint operator which allows the
iteration of calculus formulas up to a ﬁxpoint. In eﬀect, this allows us to deﬁne
frequent itemsets inductively using calculus formulas.
Deﬁnition 1. Let S k (V ) denote the set of all degree-k subset of V . For any two
sets S and s, s is said to be a degree-k subset of S if s ∈ P(S) and |s| = k. P(S)
denotes the powerset of S.
The non-inﬂationary version of the ﬁxpoint operator is presented as follows.
Consider association rule mining from object-relational data. Suppose that raw
data is ﬁrst preprocessed to transform to an object-relational database. Let D =
(x, y) be a nested table in the mapped object-relational database. For example,
x = items, y = count. Items is a set valued attribute. Let Sxk (D) = {t | ∃u ∈
D, v = S k (u[x]), t = (v, u[y])}. We develop a ﬁxpoint operator for computing
the frequent itemsets as follows. The relation Jn holding the frequent itemsets
with support value greater than a threshold δ can be deﬁned inductively using
the following formulas
ϕ(T, k) = σy≥δ ( x Gsum(y) Sxk (D)(x, y)) −→ T (x, y), if k = 1
ϕ(T, k) = T (x, y) ∨ σy≥δ (x Gsum(y) (∃u, v{T (u, v)
∧(Sxk (D)(x, y)) ∧ u ⊂ x −→ T (x, y)})), if k > 1
as follows: J0 = ∅; Jn = ϕ(Jn−1 , n), n > 0. Where G is the aggregation operator.
Here ϕ(Jn−1 , n) denotes the result of evaluating ϕ(T, k) when the value of T is
Jn−1 and the value of k is n. Note that, for each input database D, and the support threshold δ, the sequence {Jn }n≥0 converges. That is, there exists some k for
which Jk = Jj for every j > k. Clearly, Jk holds the set of frequent itemsets of D.
Thus the frequent itemsets can be deﬁned as the limit of the forgoing sequence.
Note that Jk = ϕ(Jk , k + 1), so Jk is also a ﬁxpoint of ϕ(T, k). The relation
Jk thereby obtained is denoted by μT (ϕ(T, k)). By deﬁnition, μT is an operator
that produces a new nested relation (the ﬁxpoint Jk ) when applied to ϕ(T, k).
3.2

Fixpoint Algorithm

We develop an algorithm for computing frequent itemset by using the above
deﬁned ﬁxpoint operator. We deﬁne a new join operator called sub-join.
Deﬁnition 2. Let us consider two relations with the same schemes {Item,
Count}. The sub-join, r ✶sub,k s = {t | ∃u ∈ r, v ∈ s such that u[Item] ⊆
v[Item] ∧∃t such that (u[Item] ⊆ t ⊆ v[Item] ∧ |t | = k), t =< t , v[Count] >}

A Logic-Based Approach to Mining Inductive Databases

273

Here, we treat the result of r ✶sub,k s as multiset meaning, as it may produce
two tuples of t with the same support value.
Example 1. Given two relations r and s, the result of r ✶sub,2 s is shown in
Figure 1.
r
Items Support
{a}
0
{b, f }
0
{d, f }
0

s
Items Support
{a, b, c}
3
{b, c, f }
4
{d, e}
2

r ✶sub,2 s
Items Support
{a, b}
3
{a, c}
3
{b, f }
4

Fig. 1. An example of sub-join

Given a database D = (Item, Support) and support threshold δ, the following
ﬁxpoint algorithm computes frequent itemset of D.
Algorithm. ﬁxpoint
Input: An object-relational database D and support threshold δ.
Output: L, the frequent itemsets in D.
Method:
begin
k
(D)))
L1 := σSupport≥δ ( Item Gsum(Support) SItem
for (k = 2; T = ∅; k + +) {
S := sub join(Lk−1 , D)
T := σSupport≥δ ( Item Gsum(Support) (S)
Lk := Lk−1 ∪ T
}
return Lk ;

end
procedure sub join
(T: frequent k-itemset; D: database)
for each itemset l1 ∈ T ,
for each itemset l2 ∈ D,
c = l1 ✶sub,k l2
if has inf requent subset (c, T)
then delete c else add c to S;
return S;

4

procedure has inf requent subset
(c: candidate k-itemset,
T: frequent (k − 1)-itemsets);
for each k − 1-subset s of c
if s not ∈ T then return TRUE;
return FALSE;

A Logic Query Language

In this section, we present a logic query language to model the association rule
mining, naive Bayesian classiﬁcation and partition-based clustering.

274

H.-C. Liu et al.

Association rule mining
We present an operational semantics for association rule mining queries expressed in Datalogcv,¬ program from ﬁxpoint theory.
We present a Datalog program as shown below which can compute the frequent itemsets.
← f req(I, C), J ⊂ I, |J| = 1
← cand(J, C), C > δ
← large(J, C1 ), f req(I, C2 ),
x ⊂ I, J ⊂ x, |x| = max(|J|) + 1,
4. T (genid(), x, C)
← T (x, C), ¬has inf requent subset(x)
5. cand(x, sum < C >) ← T (id, x, C)
6. large(x, y)
← cand(x, y), y > δ

1. cand(J, C)
2. large(J, C)
3. T (x, C2 )

The rule 1 generates the set of 1-itemset from the input frequency table. The
rule 2 selects the frequent 1-itemset whose support is greater than the threshold. The program performs two kinds of actions, namely, join and prune. In the
join component, the rule 3 performs the sub-join operation on the table large
generated in the rule 2 and the input frequency table. The prune component
(rule 4) employs the Apriori property to remove candidates that have a subset that is not frequent. The test for infrequent subsets is shown in procedure
has inf requent subset(x).
Datalog system is of set semantics. In the above program, we treat T facts as
multisets, i.e., bag semantics, by using system generated id to simulate multiset
operation. The rule 5 counts the sum total of all supports corresponding to each
candidate item set generated in table T so far. Finally, rule 6 computes the
frequent itemsets by selecting the itemsets in the candidate set whose support
is greater than the threshold.
We now show the program that deﬁnes has inf requent subset(x).
has inf requent subset(x) ← s ⊂ x, |s| = |x| − 1, ∀y[large(y, C), y = s]
Once the frequent itemset table has been generated, we can easily produce all
association rules.
Naive Bayesian Classiﬁcation
Let us consider a relation r with attributes A1 , ..., An and a class label C. The
Naive Bayesian classiﬁer assigns an unknown sample X to the class Ci if and
only if P (Ci |X) > P (Cj |X), for 1 ≤ j ≤ m, j = i.
We present a datalog program demonstrating that Naive Bayesian classiﬁcation task can be performed in deductive environment. The program ﬁrst evaluates the frequencies of the extension of r, each class and each pair of attribute
Ai and class.
f req r(A1 , ..., An , C, count(∗)) ← r(A1 , ..., An , C)
f req class(C, count(∗))
← r(A1 , ..., An , C)
f req Ai class(Ai , C, count(∗)) ← r(A1 , ..., An , C)

A Logic-Based Approach to Mining Inductive Databases

275

Then we obtain the probabilities of P (Ai | C), as follows.
P r class(C, p)
← f req r(A1 , ..., An , C, nr ), f req class(C, nc ), p = nc /nr
P r A class(A, C, p) ← f req A class(A, C, nA,C ), f req class(C, nc ), p = nA,C /nc
Finally, we get the answer predicate Classif ier(x1 , ..., xk , class).
← r(x1 , ..., xk ), P r A class(A, class, p),
∃ti ∈ P r A class, 1 ≤ i ≤ k,
x1 = t1 .A,...,xk = tk .A,
t1 .class = ... = tk .class, p = ti .p
← P r(x1 , ..., xk , class, p1 ),
P (x1 , ..., xk , class, p)
P r class(class, p2), p = p1 × p2
Classif ier(x1 , ..., xk , class) ← P (x1 , ..., xk , class, p), p = max{P.p}
P r(x1 , ..., xk , class, p)

Example 2. We wish to predict the class label of an unknown sample using
naive Bayesian classiﬁcation, given a training data. The data samples are described by the attributes age, income, student, and credit-rating. The class label attribute, buys-computer, has two distinct values, namely, YES, NO. The
unknown sample we wish to classify is X = (age = ” <= 30”, income =
”medium”, student = ”yes”, credit − rating = ”f air”).
The evaluation of the query Classif ier(x1 , ..., xk , class) returns the answer predicate Classif ier(age = ” <= 30”, income = ”medium”, student = ”yes”,
credit − rating = ”f air”, ”Y ES”).
Cluster analysis: partitioning method
Given a database of n objects and k, the number of clusters to form, a partitioning algorithm organises the objects into k partitions, where each partition represents a cluster. We present a deductive program performing the partitioningbased clustering task, as follows. P (Y, Ci ) ← r(X), 1 ≤ i ≤ k, Yi = X;
Cluster(Y, Ci , mi ) ← P (Y, Ci ), mi = mean{Y } Where mean is a function used
to calculate the cluster mean value; distance is a similarity function. First, it
randomly select k of the n objects, each of which initially represents a cluster
mean. For each of the remaining objects, an object is assigned to the cluster
to which it is the most similar, based on the distance between the object and
the cluster mean. It then computes the new mean for each cluster. This process
iterates until some criterion function converges, i.e., eventually, no redistribution
of the objects in any cluster occurs and so the process terminates.
The following two rules show the clustering process. An operational semantics
for the following datalog program P is ﬁxpoint semantics. The immediate consequence operator, TP is the mapping from instances of schema of P to instances
of schema of P .
new cluster(X, C) ← r(X), Cluster(Y, C, m), Cluster(Y, C , m ),
c = c , distance(X, m) < distance(X, m ),
Cluster(X, C, m) ← new cluster(X, C), m = mean{new cluster.X}

276

5

H.-C. Liu et al.

Performance and Optimisation Issues

Most performance experiments have shown that SQL-based data mining algorithms are inferior to cache-mining approach. The main-memory algorithms used
in today’s data mining application typically employ sophisticated data structures
and try to scan the dataset fewer times compared to SQL-based algorithms which
normally require many complex join operations between input tables. However,
database support knowledge discovery is important when data mining applications need to analyse current data which is so large that the in-memory data
structures grow beyond the size of main-memory.
An SQL3 expression mapping to a least ﬁxpoint operator has been presented
in [8]. The ﬁxpoint operator of that article has signiﬁcant strengths in term of
iterative relational computation. But it also requires substantial optimisation
and prune component to remove candidates that have a subset that is not frequent. The prune component can be implemented by a k-way join. The SQL3
implementation of a ﬁxpoint operator discussed in [8] does not claim to have
achieved performance level that is comparable to those SQL-based approaches,
nor does it claim to have identiﬁed a query-optimisation strategy.
We argue that if SQL would allow expressing our sub-join, ✶sub,k , in an intuitive manner and algorithms implementing this operator were available in a
DBMS, this would greatly facilitate the processing of ﬁxpoint queries for frequent
itemset discovery.
A Datalog expression mapping to our ﬁxpoint operator has more intuitive
than SQL expressions. In our opinion, a ﬁxpoint operator is more appropriate
exploited in the deductive paradigm which is a promising approach for inductive
database systems. The main disadvantage of the deductive paradigm for inductive query languages is the concern of its performance. However, optimisation
techniques from deductive databases can be utilised and the most computationally intensive operations can be modularised.
There exist some opportunities for optimisation in the expressions and algorithms expressed in the deductive paradigm. Like in the algebraic paradigm,
we may improve performance by exploiting relational optimisation techniques,
for example, optimizing subset queries [9], index support, algebraic equivalences
for nested relational operators [10]. Optimisation for data mining queries also
requires novel techniques, not just extensions of object-relational optimisation
technology.
We conducted several experiments on a PC with Pentium(R)4, 3.0G processor
running Windows XP with 1GB memory, 1.2GB virtual memory and 40GB hard
drive. These implementations were developed in C++. The preliminary experimental result shows that Calculus-Fixpoint alogrithm outperforms Apriori algorithm. We also chose SQL-92 implementation to compare with our algorithm.
In SQL-92 implementation we used the sub-query approach. The experimental result shows that Calculus-Fixpoint program outperforms SQL sub-query
approach.

A Logic-Based Approach to Mining Inductive Databases

6

277

Conclusion

Relational computation for association rule mining that uses the idea of least
ﬁxpoint computation has been demonstrated in this paper. We also present a
logic-based inductive query language and illustrate the use of aggregates and
exploit a new join operator to model speciﬁc data mining tasks. The results
provide theoretical foundations for inductive database research and could be
useful for data mining query language design in inductive database systems.

Acknowledgments
The work reported in this paper was supported by a grant (CUHK418205) from
the Research Grants Council of the Hong Kong Special Administrative Region,
China and by a Faculty of Commerce research grant from University of Wollongong. Part of the work performed while the ﬁrst author was at University of
Wollongong.

References
1. Raedt, L.D.: A perspective on inductive databases. SIGKDD Explorations 4 (2002)
69–77
2. Zaniolo, C.: Mining databases and data streams with query languages and rules.
In: Proceedings of 4th Workshop on Knowledge Discovery in Inductive Databases,
LNCS3933. (2005) 24–37
3. Imielinski, T., Virmani, A.: Msql: A query language for database mining. Data
Mining and Knowledge Discovery 2 (1999) 373–408
4. Han, J., Fu, Y., Koperski, K., Wang, W., Zaiane, O.: Dmql: A data mining query
language for relational databases. In: Proceedings of ACM SIGMOD Workshop on
Research Issues on Data Mining and Knowledge Discovery. (1996)
5. Meo, R., Psaila, G., Ceri, S.: An extension to sql for mining association rules. Data
mining and knowledge discovery 2 (1998) 195–224
6. Sarawagi, S., Thomas, S., Agrawal, R.: Integrating association rule mining with
relational database systems: alternatives and implications. Data mining and knowledge discovery 4 (2000) 89–125
7. Giannotti, F., Manco, G., Turini, F.: Towards a logic query language for data
mining. Database Support for Data Mining Applications, LNAI 2682 (2004)
76–94
8. Jamil, H.M.: Bottom-up association rule mining in relational databases. Journal
of Intelligent Information Systems (2002) 1–17
9. Masson, C., Robardet, C., Boulicaut, J.F.: Optimizing subset queries: a step towards sql-based inductive databases for itemsets. In: Proceedings of the 2004 ACM
symposium on applied computing. (2004) 535–539
10. Liu, H.C., Yu, J.: Algebraic equivalences of nested relational operators. Information
Systems 30 (2005) 167–204

