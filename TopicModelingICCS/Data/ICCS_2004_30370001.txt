Hierarchical Matrix-Matrix Multiplication
Based on Multiprocessor Tasks
Sascha Hunold1 , Thomas Rauber1 , and Gudula R¨
unger2
1

Fakult¨
at f¨
ur Mathematik, Physik und Informatik, Universit¨
at Bayreuth, Germany
2
Fakult¨
at f¨
ur Informatik, Technische Universit¨
at Chemnitz, Germany

Abstract. We consider the realization of matrix-matrix multiplication
and propose a hierarchical algorithm implemented in a task-parallel way
using multiprocessor tasks on distributed memory. The algorithm has
been designed to minimize the communication overhead while showing
large locality of memory references. The task-parallel realization makes
the algorithm especially suited for cluster of SMPs since tasks can then
be mapped to the diﬀerent cluster nodes in order to eﬃciently exploit
the cluster architecture. Experiments on current cluster machines show
that the resulting execution times are competitive with state-of-the-art
methods like PDGEMM .

1

Introduction

Matrix multiplication is one of the core computations in many algorithms of
scientiﬁc computing and numerical analysis. Many diﬀerent implementations
have been realized over the years, including parallel ones. On a single processor
ATLAS [7] or PHiPAC [1] create eﬃcient implementations by exploiting the
speciﬁc memory hierarchy and its properties. Parallel approaches are often based
on decomposition, like Cannon’s algorithm or the algorithm of Fox. Eﬃcient
implementation variants of the latter are SUMMA or PUMMA, see also [3] for
more references. Matrix-matrix multiplication by Strassen or Strassen-Winograd
beneﬁts from a reduced number of operations but require a special schedule for
a parallel implementation. Several parallel implementations have been proposed
in [2,5,4].
Most clusters use two or more processors per node so that the data transfer
between the local processors of a node is much faster than the data transfer between processors of diﬀerent nodes. It is therefore often beneﬁcial to exploit this
property when designing parallel algorithms. A task parallel realization based
on multiprocessor tasks (M-tasks) is often suited, as the M-tasks can be mapped
to the nodes of the system such that the intra-task communication is performed
within the single nodes. This can lead to a signiﬁcant reduction of the communication overhead and can also lead to an eﬃcient use of the local memory
hierarchy. Based on this observation, we propose an algorithm for matrix multiplication which is hierarchically organized and implemented with multiprocessor
tasks. At each hierarchy level recursive calls are responsible for the computation
of diﬀerent blocks with hierarchically increasing size of the result matrix. The
M. Bubak et al. (Eds.): ICCS 2004, LNCS 3037, pp. 1–8, 2004.
c Springer-Verlag Berlin Heidelberg 2004

2

S. Hunold, T. Rauber, and G. R¨
unger

processors are split into subgroups according to the hierarchical organization
which leads to a minimization of data transfer required. Moreover, only parts
of one input matrix are moved to other processors during the execution of the
algorithm, i.e., the local parts of the other matrix can be kept permanently in
the local cache of the processors.
We have performed experiments on three diﬀerent platforms, an IBM Regatta p690, a dual Xeon cluster with an SCI interconnection network, and a
Pentium III cluster with a fast Ethernet interconnect. For up to 16 processors,
the algorithm is competitive with the PDGEMM method from ScaLAPACK and
outperforms this method in many situations. Thus the algorithm is well-suited
to be used as a building block for other task parallel algorithms.
The rest of the paper is organized as follows. Section 2 describes the hierarchical algorithm. The implementation of the algorithm is presented in Section 3.
Section 4 presents experimental results and Section 5 concludes the paper.

2

Hierarchical Matrix Multiplication

The hierarchical matrix multiplication performs a matrix multiplication A · B =
C of an m×n matrix A and an n×k matrix B in a recursively blockwise manner
on p processors. We assume that p = 2i , i ∈ N, and that p divides m and k
without remainder. During the entire algorithm the input matrix A is distributed
in a row blockwise manner, i.e. processor q stores the rows with indices ((q−1)·s+
1, . . . , q·s), s = m/p, q = 1, . . . , p. Input matrix B is distributed columnwise with
varying mappings in the computation phases. Initially the distribution is column
blockwise, i.e. processor q stores the columns with indices ((q−1)·s +1, . . . , q·s ).
The columns are exchanged in later steps, see Figure 1.
The hierarchical matrix multiplication computes the result matrix C in
log p + 1 steps and processor q is responsible for the computation of the s rows
with indices ((q − 1) · s + 1, . . . , q · s) of C. The computation is organized so that
disjoint processor groups compute the diagonal blocks Clk in parallel, which contain the (s · 2l−1 )2 entries cij with 2l−1 · (k − 1) · s + 1 ≤ i, j ≤ 2l−1 · k · s. The
coarse computational structure is the following:
Hierarchical Matrix Multiplication(n,p) =
for (l = 1 to log p + 1)
for k = 1, . . . , p/2l−1 compute in parallel
compute block (Clk );
Figure 1, bottom row, illustrates the computation of blocks Clk . A diagonal block
Clk is computed by calling compute block(Clk ) which is performed in parallel by
all processors of a group. If in one group only a single processor q performs
compute block, i.e. l = 1, this processor computes one initial diagonal block Clk
by using its local entries of A and B. Otherwise, the computation of the two
diagonal sub-blocks Cl−1,2k−1 and Cl−1,2k of Clk have already been completed
in the preceding step by two other processor groups and the computation of Clk
is completed by computing the remaining sub-blocks Cl−1,2k−1 and Cl−1,2k in
the following way:

Hierarchical Matrix-Matrix Multiplication Based on Multiprocessor Tasks

3

The initial column blocks of B are virtually grouped into larger column
blocks according to the hierarchical binary clustering: for 1 ≤ l ≤ log p + 1 and
1 ≤ k ≤ p/2l−1 , column block Blk contains s · 2l−1 columns of B; these columns
have the indices (2l−1 (k − 1) · s + 1, . . . , 2l−1 k · s ). The ﬁrst index l of Blk
determines the size of the column block, the second index k numbers the column
blocks of the same size.
The function compute block() ﬁrst exchanges the column blocks Bl−1,2k−1
and Bl−1,2k of matrix B that are needed for the computation of Cl−1,2k−1 and
Cl−1,2k , respectively, between the processors of the corresponding groups. This
can be done in parallel since the processors of the group can be grouped into
pairs which exchange their data. After the transfer operations the sub-blocks
Cl−1,2k−1 and Cl−1,2k , respectively, are computed in parallel by recursive calls.
At any point in time, each local memory needs to store at most s rows of A
and s columns of B and only columns of B are exchanged between the local
memories.

3

Task Parallel Implementation

The realization of the task parallel matrix multiplication (tpMM ) is based on a
hierarchy of multiprocessor groups. The resulting implementation uses the runtime library Tlib which supports the programming with hierarchically structured
M-tasks and provides a tool to handle multiprocessor groups built on top of MPI
communicators [6].
The program realizes the recursive structure of the algorithm and uses a
description of the block of the result matrix C that is computed in the current
recursion step. This description contains the start column and the extent of the
sub-block. The implementation exploits that the algorithm ﬁlls the basic blocks
of C by alternating between basic blocks in the diagonal and the anti-diagonal
position, see Figure 1. More precisely, the recursion in each phase subdivides the
current block of C into sub-blocks containing 2 × 2 basic blocks, which are then
ﬁlled in the diagonal and anti-diagonal direction. The program of tpMM uses
the functions below. The variables A, B, C, m, n, k, mA = m/p and kB = k/p are
declared and deﬁned globally.
compute block(comm, lcc, cc, type) is the recursive function for computing
C = A · B. comm is the current communicator of the recursion step. lcc
denotes the leftmost column of C and cc speciﬁes the number of columns
of C for the next recursion step. type ∈ {DIAGONAL, ANTIDIAGONAL}
indicates if compute block updates a diagonal or anti-diagonal block of C.
multiply(cc, lcc) performs the actual work of multiplying two sub-matrices
and computes one basic block of C. The function is performed on a single
processor and is realized by using fast one-processor implementations such
as BLAS or ATLAS.
exchange columns(comm) performs the data exchange between pairs of processors in the current communicator. For each call of the function, each processor participates in exactly one data exchange. The function ensures that

4

S. Hunold, T. Rauber, and G. R¨
unger

P0 P1 P2 P3 P4 P5 P6 P7

P1 P0 P3 P2 P5 P4 P7 P6

P2 P3 P0 P1 P6 P7 P4 P5

P3 P2 P1 P0 P7 P6 P5 P4

P0

0 P0
P1 0

0 1 P0
P1
1 0
P2
0 1
P3 1 0

0
1
2
P3

P1
P2

0 P2
P3 0

P3
P4

0 P4
P5 0

P5
P6
P7

0 P6
P7

0 1 P4
P5
1 0
P6
0 1
P7 1 0

1 2 P0
0 P1 2
P2 0 1
2

1

0
0 1 2
1 0 P5
2 P6 0
P7 2 1

P4
2
1
0

Fig. 1. Data distribution of matrix B (top row) and computation order of the result
matrix (bottom row) for processors P0 , . . . , P7 for the ﬁrst half of the steps. Each block
is labeled with the owning processor. The numbers 0,1,2 denote the phase in which the
blocks of C are computed.

processor Pi sends/receives a block of B to/from Pj , j = (i+sizeof(comm)/2)
mod sizeof(comm).
The pseudo-code of compute block is given below. To perform a multiplication the programmer just needs to call compute block and pass the corresponding parameters. The computation phases of tpMM reuse the communicators
several times according to the recursive structure of the algorithm. Figure 2
illustrates the recursive splitting and the communicator reuse for p = 8 processors.
function compute block(comm, lcc, cc, type)
if ( Comm size(comm) == 1 ) { multiply(cc, lcc) }
else {
subcommi = split(comm); /* splitting into subcommunicator i={0, 1} */
if ( type == DIAGONAL )
{ lcc0 = lcc; lcc1 = lcc+cc/2 }
else
{ lcc0 = lcc+cc/2; lcc1 = lcc }
compute block(subcommi , lcci , cc/2, DIAGONAL); /* task parallel */
exchange columns(comm);
if ( type == DIAGONAL )
{ lcc0 = lcc+cc/2; lcc1 = lcc }
else
{ lcc0 = lcc; lcc1 = lcc+cc/2 }
compute block(subcommi , lcci , cc/2, ANTIDIAGONAL); /* task parallel */
}

Hierarchical Matrix-Matrix Multiplication Based on Multiprocessor Tasks

recursice
splitting

processor group
level

5

computation phase
ex

1
ex

2
ex

ex

ex

ex

mm mm

mm mm

mm mm

mm mm

3
4

ex

50

3000

40

2500

2000

30

tpmm_lam_buf 16 procs
tpmm_mpich 16 procs
tpmm_mpich_buf 16 procs

20

1500

1000

10

0

16

32

48

64

80

96

112

128 144
block size

160

176

192

208

224

240

256

Fig. 3. tpMM overlapping tests on CLiC
for matrix dimension n = 4096 and 16
processors.

4

dgemm/atlas without tiling
dgemm/atlas with tiling

MFLOPS

runtime (in sec)

Fig. 2. Usage of processor groups during the computation of tpMM for three recursive
splittings into sub-groups and four hierarchical levels. The matrices to be multiplied are
decomposed into eight blocks of rows and columns, respectively. mm denotes the matrix
multiplication for a single block, ex denotes the exchange of data at the corresponding
communicator level.

500
2000

2500

3000

3500

4000
4500
Matrix dimension

5000

5500

6000

6500

Fig. 4. comparison of DGEMM from
ATLAS with and without tiling enabled
(on dual Beowulf cluster; m, k=2048; n is
varying).

Experimental Results

The runtime tests of tpMM were performed on a IBM Regatta p690 (AIX,
6 x 32 processors, Power4+ 1.7GHz, Gigabit Ethernet) operated by the Research
Centre J¨
ulich, on a Linux dual Beowulf cluster (16 x 2 procs., Xeon 2.0 GHz, SCI
network) and the CLiC (Chemnitzer Linux Cluster, 528 procs., P3 800 MHz,
Fast-Ethernet) at TU Chemnitz.
Minimizing communication costs. The communication overhead of many applications can be reduced by overlapping communication with computation. To
apply overlapping to tpMM , the block of B that each processor holds is not
transfered entirely in one block. The blocks are rather send simultaneously in
multiple smaller sub-blocks while performing local updates of matrix C. This
requires non-blocking send and recv operations. Figure 3 shows runtime tests
on CLiC using mpich and lam. The suﬃx “buf” refers to MPI Ibsend, the

6

S. Hunold, T. Rauber, and G. R¨
unger
450

pdgemm 4 procs
tpmm 4 procs

500
450
400

350

350

300

mflops (per processor)

mflops (per processor)

pdgemm 8 procs
tpmm 8 procs

400

300
250
200

250

200

150
150
100
100
50

50
0

1024

2048

3072

4096
5120
matrix size

6144

7168

0

8192

1024

3500

pdgemm 4 procs
tpmm 4 procs

2048

3072

4096

5120
matrix size

6144

7168

8192

9216

pdgemm 8 procs
tpmm 8 procs

3500
3000
3000

mflops (per processor)

mflops (per processor)

2500
2500

2000

1500

2000

1500

1000
1000

500

500

0

1024

2048

3072

4096
5120
matrix size

6144

7168

0

8192

1024

pdgemm 4 procs
tpmm 4 procs

3000

2048

3072

4096

5120
matrix size

6144

7168

8192

9216

3072

4096

5120
matrix size

6144

7168

8192

9216

pdgemm 8 procs
tpmm 8 procs
2500

2500

mflops (per processor)

mflops (per processor)

2000
2000

1500

1500

1000
1000

500

500

0

1024

2048

3072

4096
5120
matrix size

6144

7168

8192

0

1024

2048

Fig. 5. MFLOPS per node reached by PDGEMM and tpMM on CLiC, IBM Regatta
p690 and dual Beowulf cluster (top to bottom).

buﬀered version of MPI Isend. For these tests matrices A, B, and C of dimension 4096 × 4096 and 16 processors are used, so that each processor holds 256
columns of B. In the experiments local updates with block sizes (matrix B) of
4 ≤ blocksize ≤ 256 are performed. For the full block size of 256, no overlapping
is achieved and this result can be used for comparison. The experiments show
that neither non-blocking nor non-blocking buﬀered communication leads to a
signiﬁcant and predictable improvement.

Hierarchical Matrix-Matrix Multiplication Based on Multiprocessor Tasks

7

Fig. 6. JUMPSHOT-4 proﬁles of PDGEMM (upper) and tpMM (below) recorded
on dual Beowulf cluster using 4 processors. Darker boxes represent Send-operations
and segments in light grey denote either calls to MPI Recv or MPI Wait in case of
non-blocking communication.

Underlying libraries. Low level matrix-matrix multiplications on one processor
(BLAS level 3) are performed by ATLAS [7] which optimizes itself at compile
time to gain maximum performance for a given architecture. Runtime experiments of tpMM on dual Beowulf cluster with more than 8 processors show a dramatic drop of the MFLOPS rate per node when using larger matrices (> 4096).
According to a detailed proﬁling analysis the performance loss is caused by an
internal call to DGEMM . Tests with a series of DGEMM matrix-matrix multiplications with ﬁxed dimensions of mA and kB and variable n are presented in
Figure 4. It turned out that when there are more than twice as many rows of B
as columns ATLAS internally calls a diﬀerent function which results in poor performance. This situation is likely to happen when executing tpMM with large
input matrices. One possible work-around is a tiling approach of the original
multiplication by dividing the problem into multiple sub-problems. The tiling
of the local matrices A and B must ensure that each tile is as big as possible
and two tiles must fulﬁll the requirements to perform a matrix-matrix multipliB
cation (columns of tile tA
i = rows of tj ). With tiling the local matrix-matrix
multiplication achieves a similar MFLOPS-rate for all inputs (see Figure 4).
Overall performance evaluation of tpMM. Figures 5 shows the MFLOPS reached
by DGEMM and tpMM on the three test systems considered. Since both
methods perform the same number of operations (in diﬀerent order), a larger
MFLOPS rate corresponds to a smaller execution time. The ﬁgures show that for
4 processors, tpMM leads to larger MFLOPS rates on all three machines for most
matrix sizes. For 8 processors, tpMM is usually slightly faster than DGEMM .
For 16 processors, tpMM is faster only for the IBM Regatta system. The most
signiﬁcant advantages of tpMM can be seen for the IBM Regatta system. For
32 and more processors, DGEMM outperforms tpMM in most cases.

8

S. Hunold, T. Rauber, and G. R¨
unger

Figure 6 presents trace proﬁles of PDGEMM and tpMM . The proﬁle of
PDGEMM contains a huge number of communications even though only 4 processors were involved. In contrast, the pattern of tpMM shows only a small
number of required communication calls. PDGEMM is superior if there are
many processors involved and the matrix is suﬃciently large. In these cases
overlapping of computation with communication can be achieved and the block
size remains suitable to avoid cache eﬀects and communication overhead. On
the other hand, tpMM decreases the communication overhead (e.g. numerous
startup times) what makes it faster for a smaller group of nodes. Thus, tpMM
is a good choice for parallel systems of up to 16 processors. For larger parallel
systems, tpMM can be used as a building block in parallel algorithms with a
task parallel structure of coarser granularity.

5

Conclusions

We have proposed a hierarchical algorithm for matrix multiplication which shows
good performance for smaller numbers of processors. Our implementation outperforms PDGEMM for up to 16 processors on recent machines. Due to the good
locality behavior, tpMM is well suited as building block in hierarchical matrix
multiplication algorithms in which tpMM is called on smaller sub-clusters. Experiments have shown that tpMM can be combined with one-processor implementations which have been designed carefully to achieve a good overall performance.

References
1. J. Bilmes, K. Asanovic, C.-W. Chin, and J. Demmel. Optimizing matrix multiply
using PHiPAC: A portable, high-performance, ANSI c coding methodology. In
International Conference on Supercomputing, pages 340–347, 1997.
2. Fr´ed´eric Desprez and Fr´ed´eric Suter. Impact of Mixed-Parallelism on Parallel Implementations of Strassen and Winograd Matrix Multiplication Algorithms. Technical
Report RR2002-24, Laboratoire de l’Informatique du Parall´elisme (LIP), June 2002.
Also INRIA Research Report RR-4482.
3. R. A. Van De Geijn and J. Watts. SUMMA: scalable universal matrix multiplication
algorithm. Concurrency: Practice and Experience, 9(4):255–274, 1997.
4. Brian Grayson, Ajay Shah, and Robert van de Geijn. A High Performance Parallel
Strassen Implementation. Technical Report CS-TR-95-24, Department of Computer
Sciences, The Unversity of Texas, 1, 1995.
5. Qingshan Luo and John B. Drake. A Scalable Parallel Strassen’s Matrix Multiplication Algorithm for Distributed-Memory Computers. In Proceedings of the 1995
ACM symposium on Applied computing, pages 221–226. ACM Press, 1995.
6. T. Rauber and G. R¨
unger. Library Support for Hierarchical Multi-Processor Tasks.
In Proc. of the Supercomputing 2002, Baltimore, USA, 2002.
7. R. Clint Whaley and Jack J. Dongarra. Automatically Tuned Linear Algebra Software. Technical Report UT-CS-97-366, University of Tennessee, 1997.

