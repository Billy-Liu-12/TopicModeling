Performance Evaluation of Fast Ethernet, Giganet and
Myrinet on a Cluster
Marcelo Lobosco, Vítor Santos Costa e Claudio L. de Amorim
Programa de Engenharia de Sistemas e Computação, COPPE, UFRJ
Centro de Tecnologia, Cidade Universitária, Rio de Janeiro, Brazil
{lobosco, vitor, amorim}@cos.ufrj.br

Abstract. This paper evaluates the performance of three popular technologies
used to interconnect machines on clusters: Fast Ethernet, Myrinet and Giganet.
To achieve this purpose, we used the NAS Parallel Benchmarks (NPB).
Surprisingly, for one application, LU, the performance of Fast Ethernet was
better than Myrinet. We also evaluate the performance gains provided by VIA,
a user lever communication protocol, when compared with TCP/IP, a
traditional, stacked-based communication protocol. The impacts caused by the
use of Remote DMA Write are also evaluated. The results show that Fast
Ethernet, when combined with a high performance communication protocol,
such as VIA, has a good cost-benefit ratio, and can be a good choice to connect
machines on a small cluster environment where bandwidth is not crucial for
applications.

1 Introduction
In the last few years, we have seen a continuous improvement in the performance of
networks, both in reduced latency and in increased bandwidth. These improvements
have motivated interest in the development of applications that can take advantage of
parallelism in clusters of standard workstations.
Fast Ethernet, Giganet[1] and Myrinet[2] are three popular interconnection
technologies used to build cluster systems. Fast Ethernet is a cheap LAN technology
that has as a primary goal to deliver 100Mbps bandwidth, while maintaining the
Original Ethernet’s transmission protocol, “Carrier Sense Multiple Access Collision
Detection” (CSMA/CD). Giganet and Myrinet are more expensive technologies that
provide low latency, high bandwidth, end-to-end communication between two
processes in the cluster. While Giganet is a connection-oriented interconnect based on
a hardware implementation of Virtual Interface Architecture (VIA)[3], Myrinet is a
connectionless interconnect.
This paper evaluates the impacts of these three popular cluster interconnection
technologies over the performance of applications. To achieve this purpose, the NAS
Parallel Benchmark (NPB)[4] was used to measure the performance of the cluster
when using each of the interconnection networks presented previously.
The main contributions of our work are a) the comparative study of three popular
interconnections technologies for clusters of workstations; b) the evaluation of the
performance gains provided by VIA, a user lever communication protocol, when

compared with TCP/IP, a traditional, stacked-based communication protocol; c) the
evaluation of the impacts caused by the use of Remote DMA Write on VIA and d) an
explanation for the poor performance of the LU benchmark on Myrinet The results
show that Fast Ethernet, when combined with a high performance communication
protocol, such as VIA, has a good cost-benefit ratio, and can be a good choice when
connecting machines on a small cluster environment where bandwidth is not crucial
for applications.
This paper is organized as follows. Section 2 gives an overview of the
communication protocols used in our experiments, namely TCP/IP, VIA and GM,
while section 3 presents the interconnects, Fast Ethernet, Giganet and Myrinet.
Section 4 presents the applications used in the study and their results. Section 5
concludes the work.

2. Communication Protocols

2.1 TCP/IP
TCP/IP was developed to connect a number of different networks designed by
different vendors into a network of networks. TCP/IP was designed to be robust and
automatically recover from any node or network failure. However, the reliability
provided by TCP/IP has a price, paid as overhead to the communication; to ensure
reliable data transfer, protocol stack implementations like TCP/IP usually require data
to be copied several times among layers and that communicating nodes exchange
numerous protocol-related messages during the course of data transmission and
reception. The number of protocol layers that are traversed, data copies, context
switches and timers contributes directly to the software overhead. Also, the multiple
copies of the data that must be maintained by the sender node and intermediate nodes
until receipt of the data-packet is acknowledged, contributes to reduce memory
resources and further slows down transmission.
2.2 VIA
The Virtual Interface Architecture (VIA)[3] is a user-level memory-mapped
communication architecture developed by the industry that aims to achieve low
latency and high bandwidth communication within clusters of servers and
workstations. The main idea is to remove the critical path of communication from the
operating system kernel. The operating system is called just to control and setup the
communication. Data is transferred directly to the network by the sending process and
is read directly from the network by the receiving process. Even though VIA allows
applications to bypass the operating system for message passing, VIA works with the
operating system to protect memory so that applications use only the memory
allocated to them. VIA supports two types of data transfers: Send-Receive, that is
similar to the traditional message-passing model, and Remote Direct Memory Access
(RDMA), where the source and destination buffers are specified by the sender, and no

receiver is required. VIA defines two RDMA operations, RDMA Write and RDMA
Read, to respectively write and read remote data.
2.3 GM
The GM communication system provides reliable, ordered delivery between
communication endpoints, called ports. This model is connectionless in that there is
no need for client software to establish a connection with a remote port in order to
communicate with it. GM also provides memory protected network access. Message
order is preserved only for messages of the same priority, from the same sending port,
and directed to the same receiving port. Messages with differing priority never block
each other.

3 Interconnection Technologies

3.1 Fast Ethernet
Fast Ethernet improves over the original 10-MBps Ethernet specification, by using
100-Mbps. This results in a factor of ten reduction in the bit-time, which is the
amount of time it takes to transmit a bit on the Ethernet channel, hence producing a
tenfold increase in the speed of the packets over the media system. However, the
other important aspects of the Ethernet system including the frame format, the amount
of data a frame may carry, and the media access control mechanism (CSMA/CD), are
all unchanged.
3.2 Giganet
Giganet[1] provides low latency, high bandwidth, end-to-end communication between
two processes in the cluster. Giganet provides both a switch and a host interface. The
internal switch is a 30 node thin-tree, with a channel operating at 1.25 Gbs. The
switch is based on a proprietary implementation of ATM. Unlike Fast Ethernet,
Giganet is based on connections set up in advance before data are transmitted between
two machines. These connections are called Virtual Channels (VC). Giganet’s host
interface is based on a hardware implementation of VIA.
3.3 Myrinet
Myrinet[2] is the most popular high-speed interconnect used to build clusters. Myrinet
also provides both a switch and a host interface. The internal switch is a 16-port, cutthrough, full-crossbar implementation with each channel operating at 1.28 Gbs.
Myrinet packets may be of any length, and thus can encapsulate other types of
packets, including IP packets, without an adaptation layer. Each packet is identified

by type, so that Myrinet, like Fast-Ethernet, can carry packets of many types or
protocols concurrently. Thus, Myrinet supports several software interfaces. When a
packet enters a switch, the leading byte of the header determines the outgoing port
before being stripped off the packet (source-based routing). When a packet enters a
host interface, the leading byte identifies the type of packet.

4. Performance Evaluation

4.1 Experimental Platform
Our experiments were performed on a cluster of 16 SMP PCs. Each PC contains two
650 MHZ Pentium III processors. For the results presented in this paper, we used just
one processor on each node. Each processor has a 256 Kb L2 cache and each node has
512 Mb of main memory. All nodes run Linux 2.2.14-5.0.
Each node has three network cards: (a) Intel EtherExpress Pro 10/100; (b) Giganet
cLAN NIC; and (c) Myrinet M2L-PCI64B. The EtherExpress Pro 10/100 card is
connected to a Micronet SP624C 10/100Mbps Dual-Speed switch. The Giganet cLAN
card is connected to a Giganet cLAN 5300 switch. The Myrinet card is connected to a
M2L-SW16 switch. To run VIA on the Intel EtherExpress Pro 10/100 card, we use
the NESRC’s M-VIA version 1.0 [5], a software implementation of VIA for Linux.
Table I shows s summary of the interconnect specifications used in the performance
evaluation.
Table 1. Summary of interconnect specification

Interconnection
Technologies
Switch
Network Card
Link Speed
Topology
Protocol
Middleware

Fast Ethernet

Giganet

Myrinet

Micronet SP624C
Intel EtherExpress
Pro 10/100
100Mbps
Single Switch
TCP/IP and VIA
MPICH1.2.0 and
MVICH1-alpha5

cLAN 5300
cLAN NIC

M2L-SW16
M2L-PCI64B

1.25Gbps
Thin Tree
VIA
MVICH1-alpha5

1.28Gbps
Full-Crossbar
GM
MPICH1.2..8

Figure 1 shows the latency and bandwidth of TCP/IP and M-VIA on Intel
EtherExpress Pro 10/100 card, VIA on Giganet and GM on Myrinet. The figures
show that the latency of M-VIA is 70% of the TCP/IP. Giganet’s and Myrinet’s
latency is an order of magnitude smaller than on Fast Ethernet. The latency to send 1
byte on Fast Ethernet with TCP/IP is 57 us, Fast Ethernet with M-VIA is 40 us,
Giganet is 7.92 us and Myrinet is 9.33 us. Giganet’s latency is smaller until 28 bytes;
after this Myrinet’s latency is smaller. The bandwidth to send 31487 bytes is 10.5
Mbytes/s on Fast Ethernet with TCP/IP, 11.2 Mbytes/s on Fast Ethernet with M-VIA,
98.28 Mbytes/s on Giganet and 108.44 Mbytes/s on Myrinet.

Table 2. Sequential times of applications

Program

Program Size

Sequential Time (s)

IS

33,554,432
Iterations = 10
256x256x256
Iterations = 20
75,000
Iterations = 75
102x102x102
Iterations = 400
2,147,483,648
Iterations = 10
102x102x102,
Iterations = 250

69.03

Standard
Deviation (%)
0.71

342.58

0.52

2,346.07

0.01

6,783.32

0.46

1,537.39

0.08

7,553.21

0.30

MG
CG
SP
EP
LU

Network Latency
3500

Network Bandwidth
120

Ethernet TCP/IP

Ethernet TCP/IP
Ethernet M-VIA

Ethernet M-VIA

Giganet VIA

Giganet VIA
3000

Myrinet GM

Myrinet GM

100

Bandwidth (MBytes/s)

Latency (us)

2500

2000

1500

80

60

40
1000

20

500

0

0
1

10

100

1000

10000

Message Syze (bytes)

100000

1

10

100

1000

10000

Message Size (bytes)

Fig. 1. Network Latency and Bandwidth

The NAS Parallel Benchmarks (NPB)[4] are widely used to evaluate the
performance of parallel machines. The set consists of five kernels – EP, FT, MG, CG
and IS – and three applications – BT, LU and SP – derived from computational fluid
dynamics (CFD) codes. There are five classes of each benchmark: S (Sample), W
(Workstation), A, B and C. Class S has the smallest problem size, and classes A, B
and C are progressively larger. In our experiments, we used the 2.3 Class B.
We did not run FT and BT in our experiments. The sequential version of BT did
not run, which hindered us to evaluate its performance. We cannot run FT in our
experiments because we did not have access to a Fortran 90 compiler (we used GNU
g77 version 2.91.66 to compile the source codes).
All NAS benchmarks communicate via the Message Passing Interface (MPI) [6]
standard. We used MPICH 1.2.0 in our experiments for TCP/IP, MPICH 1.2.8 for
GM and MVICH 1-alpha 5[7] for VIA. The implementation of MVICH is at the early
stages, so it is neither completely stable nor optimized for performance. We expect
MVICH results presented here to become better in future optimized versions. We run
the applications with and without RDMA for the tests with VIA (both MPIC/M-VIA
and MVICH). This option is enabled at compilation.

100000

The sequential execution times for the applications are presented in Table II. Each
application was run 5 times; the times presented are an average of these values. We
also present the standard deviation of the times. All execution times are non-trivial,
with LU and SP being the longest running-time applications. IS and MG have much
shorter running-times, but still take more than a minute.
Table 3. Messages and data at 16 processors

Program
IS
MG
CG
SP
EP
LU

Number of
messages
5,420
42,776
220,800
153,600
90
1,212,060

Total transfers
(MB)
1,281.19
667.25
13,390.54
14,504.17
0.003
3,525.70

Medium message
size (KB/m)
242.05
15.97
62.1
96.69
0.03
2.97

BW per CPU
(MB/s)
16.18
1.74
5.03
1.82
0.00
0.45

Table III shows the total amount of data and messages sent by the applications, as
well as medium message size (in kilobytes per message) and average bandwidth per
processor, when running on 16 nodes. We can observe that the benchmarks have very
different characteristics. EP sends the smallest number of messages, while LU sends
the larger amount of messages. Although it takes the least time to run, IS sends the
largest messages, hence requiring very high bandwidth, above the maximum provided
by Fast Ethernet. SP is a long running-time application and also sends the largest
volume of data, so it has moderate average bandwidth. The least bandwidth is used by
EP, which only sends 90 messages in more than a thousand seconds.
Table 4. Message Size

Size
0-9
10-99
100-999
1,000-9,999
10,000-99,999
5
> 10

IS
2,560
0
0
300
0
2,560

MG
760
8,960
9,920
11,520
9,664
1,952

CG
124,800
2,400
0
0
0
93,600

SP
0
0
0
0
57,600
96,000

EP
60
30
0
0
0
0

LU
0
60
5
3x10
5
9x10
0
12,000

Table IV presents a more detailed panorama of the message sizes sent by each
application. SP and EP are opposites in that SP sends a lot of large message and EP
few small messages. IS sends the same number of small and large messages. MG has
a relative uniform distribution. Last, LU mostly sends medium-sized messages, and
also some larger messages.

Speedup - IS
18

Speedup - MG
18

MPICH-Eth

MPICH-Eth

MPICH-GM-Myr
16

MPICH-GM-Myr

MVICH-Eth

16

MVICH-Eth

MVICH-Eth RDMA

MVICH-Eth RDMA

MVICH-Gig

14

MVICH-Gig

14

MVICH-Gig RDMA

MVICH-Gig RDMA
Linear

12

10

10

Speedup

Speedup

Linear
12

8

8

6

6

4

4

2

2

0

0
0

2

4

6

8

10

12

14

16

18

0

2

4

6

Number of Processo rs

8

(a)
18

16

18

12

14

16

18

12

14

16

18

MPICH-Eth

MPICH-GM-Myr

MPICH-GM-Myr

MVICH-Eth

16

MVICH-Eth

MVICH-Eth RDMA

MVICH-Gig

MVICH-Gig

14

14

Speedup - SP

MPICH-Eth

16

12

(b)

Speedup - CG
18

10

Number of Processo rs

MVICH-Gig RDMA

14

MVICH-Gig RDMA

Linear

12

12

10

10

Speedup

Speedup

Linear

8

8

6

6

4

4

2

2

0

0
0

2

4

6

8

10

12

14

16

18

0

2

4

6

Number of Processo rs

8

(c)

(d)

Speedup - EP
18

Speedup - LU
18

MPICH-Eth

MPICH-Eth

MPICH-GM-Myr
16

MPICH-GM-Myr

MVICH-Eth

16

MVICH-Eth

MVICH-Eth RDMA

MVICH-Eth RDMA

MVICH-Gig

14

MVICH-Gig

14

MVICH-Gig RDMA

MVICH-Gig RDMA

Linear

Linear

12

12

10

10

Speedup

Speedup

10

Number of Processo rs

8

8

6

6

4

4

2

2

0

0
0

2

4

6

8

10

Number of Processo rs

12

14

16

18

0

2

4

6

8

10

Number of Processo rs

(e)

(f)

Fig. 2. NPB speedups: (a) IS, (b) MG, (c) CG, (d) SP, (e) EP, (f) LU

4.2 Results
Table V shows the application’s speedups for 16 processors. Speedup curves are
presented in Figure 2.
To understand the performance of the benchmarks, we used both the log and trace
options available at the Multiprocessing Environment library. Figure 3 presents the
execution time breakdown of the benchmarks. We include only four communications

calls (MPI_Send, MPI_Recv, MPI_Isend and MPI_Irecv) and two synchronizations
calls (MPI_Wait and MPI_Waitall). All other MPI calls (for example, MPI_Reduce,
MPI_Bcast and MPI_Barrier) are constructed through the combination of these six
primitives calls. We show breakdowns for the Myrinet and MVICH configurations.
The breakdowns are quite similar, except for LU. Note that computation time
dominates the breakdown. IS has very significant Send and WaitAll times, and LU
has very significant RECV time. The dominance of computation time indicates we
can expect good speedups, as it is indeed the case.
Table 5. Speedups – 16 processors

Application
IS
MG
CG
SP
EP
LU

MPICH
Ethernet Myrinet
1.43
11.85
10.95
14.28
7.70
14.11
NA
13.57
15.92
16.01
12.76
7.22

Ethernet
NA
12.36
9.72
11.93
16.00
15.16

MVICH
Eth RDMA Giganet
NA
NA
NA
13.92
NA
13.36
NA
13.4
16.00
16.02
NA
15.48

Gig RDMA
13.95
14.03
13.59
13.66
16.02
15.66

4.2.1 IS
Integer Sort (IS) kernel uses bucket sort to rank an unsorted sequence of keys. The
algorithm divides up the keys among processors and each processor counts its keys,
writing the result in a private array of buckets. After counting their keys, the
processors form a chain, in which processor n send its local array of buckets to
processor n+1, which adds the values received in its local array of buckets and
forwards the result to the next processor. The last processor calculates the final result
and broadcasts it.
IS sends a total of 5,420 messages; 2,560 messages are smaller than 10 bytes, 300
are between 100 and 999 bytes and 2,560 are bigger than 100,000 bytes. IS requires a
total of 16.18 Mbytes/s of bandwidth per CPU. IS running on Myrinet spends 55% of
the time on communication, while the version running on Giganet with RDMA
spends 48% of time on communication. So, this is a communication bound
application and just the bandwidth required per CPU by IS explains the poor
performance of Fast Ethernet.
IS sends 300 blocking messages (sends) and 5,120 non-blocking messages
(isend). There are also 320 calls to the waitall primitive. We found that the difference
between Giganet and Myrinet stems from the time spent in the recv and waitall
primitives. For 16 nodes, Giganet spends 0.78s and 1.55s, respectively, in recv and
waitall, while Myrinet spends 0.90s and 2.26s (Figure 3a). This seems to suggest
better performance from the Giganet switch.
VIA is effective in improving performance of Fast Ethernet. For eight nodes, the
speedup of the M-VIA version is 3.66, against 1.02 of the TCP/IP version (figure 2a).
We believe this result to be quite good, considering the average bandwidth required
by the application. Unfortunately, we could not make M-VIA run with 16 nodes, due
to an excessive number of messages. M-VIA with RDMA version only runs with 2
nodes, so we cannot evaluate its effectiveness in improving performance.

Execution Time Breakdown - IS

Execution Time Breakdown - MG

100%

100%

90%

80%

80%

70%

60%

WailAll

Wait
Send
Recv
Irecv
Computation

60%

Send

50%

Recv
Isend

40%

40%

Irecv
30%

Computation

20%

20%

10%

0%

0%
Myrinet+GM

Giganet+VIA RDMA

Myrinet+GM

(a)

Giganet+VIA RDMA

(b)

Execution Time Breakdown - CG

Execution Time Breakdown - SP

100%

100%

90%

80%

80%

70%

60%

Wait
Send
Irecv
Computation

40%

60%

WailAll
Isend
Irecv
Computation

50%

40%

30%

20%

20%

10%

0%

0%
Myrinet+GM

Giganet+VIA RDMA

Myrinet+GM

(c)

Giganet+VIA RDMA

(d)
Execution Time Breakdown - LU

Execution Time Breakdown - EP
100%

100%

90%

90%

80%

80%

70%

70%

Wait
Send
Recv
Irecv
Computation

60%

60%

Send
Recv
Computation

50%

50%

40%

40%

30%

30%

20%

20%

10%

10%

0%

0%
Myrinet+GM

Giganet+VIA RDMA

Myrinet+GM

Giganet+VIA RDMA

(e)
(f)
Fig. 3. NPB execution time breakdown – 16 nodes : (a) IS, (b) MG, (c) CG, (d) SP,
(e) EP, (f) LU
4.2.2 MG
MG uses a multigrid method to compute the solution of the three-dimensional scalar
Poisson equation. MG tests both short and long distance highly structured
communication.
MG sends a total of 42,776 messages. Despite of sending more messages than IS,
the medium message size of MG is smaller and the benchmark runs for longer. This is
reflected in the average bandwidth required of only 1.74 Mbytes/s, smaller than the
bandwidth required by IS. As shown in Figure 3, MG is a computation bound

application, spending less than 10% of the time doing communication on both
Myrinet and Giganet.
Fast Ethernet also presented a better performance. The speedup of TCP-IP is 10.95
for 16 nodes. VIA was again effective in improving performance: the speedup is
12.36. The speedups of Myrinet and Giganet are quite similar: Myrinet achieved a
speedup of 14.28, against 14.03 of Giganet with the RDMA support. Without RDMA,
the speedup of Giganet is 13.93 (Figure 2b). So RDMA support does not offer, for
this application, a significant improvement in performance. Indeed, for 8 nodes, the
performance of the version with RDMA support on Fast Ethernet was slightly worst:
7.10 against 7.15 for the version without RDMA. For 16 nodes on Fast Ethernet, the
version with RDMA support has broken.
The reason why Myrinet has a slightly better performance than Giganet is the time
spent in the send primitive: Myrinet spends 1.4s, while Giganet spends 1.55s (Figure
3b). We believe MG may benefit from Myrinet’s larger bandwidth.
4.2.3 CG
In the CG kernel, a conjugate gradient method is used to find an estimate of the
largest eigenvalue of a large, symmetric positive definite sparse matrix with a random
pattern of nonzeros.
CG sends a total of 220,800 messages; 124,800 of which are smaller than 9 bytes
and 93,600 bigger than 100,000. In spite of the large number of messages sent, CG is
a computation bound application: 16% of the time is spend on communication on
Giganet and 14% on Myrinet. The bandwidth required by this benchmark is 5.03
Mbytes/s.
Myrinet achieves the better speedup: 14.11 on 16 nodes. Giganet achieves a
speedup of 13.59 with the RDMA support and 13.36 without RDMA. TCP/IP over
Fast Ethernet achieved a speedup of 7.7. Substituting TCP/IP by M-VIA makes the
speedup grow up to a very reasonable 9.72 (Figure 2c). Again, M-VIA with RDMA
was broken for 16 nodes. For 8 nodes, the version with RDMA has worst
performance (6.42) than the version without RDMA (7.00).
This time, the time spent in the MPI_wait primitive by Myrinet, 3.28s, is smaller
than the time Giganet spends on it, 7.5s. Giganet also spends more time in the
MPI_send primitive (21.05s) than Myrinet (19.7s), which contributed to the worst
performance of Giganet on this benchmark (Figure 3c). Again, we believe Myrinet
may benefit from larger bandwidth.
4.2.4 SP
SP solves three uncoupled systems of non-diagonally dominant, scalar, pentadiagonal
equations using the multi-partition algorithm. In the multi-partition algorithm, each
processor is responsible for several disjoint sub-blocks of points of the grid, which are
called cells. The cells are arranged such that each processor performs useful work
without being forced to wait for the partial solution from another processor before
beginning work. The information from a cell is not sent to the next processor until all
linear equations of the cell have been solved, which keeps the granularity of
communications large (57,600 messages sent by SP are between 10,000 and 99,999
bytes, and 96,000 are bigger than 100,000 bytes). As CG and MG, SP is also a
computation bound application, spending less than 8% on communication. The

bandwidth required by SP is 1.82 Mbytes/s. SP requires a square number of
processors.
Myrinet and Giganet have similar performance. Myrinet achieved a speedup of
13.57 on 16 nodes, and Giganet achieved a speedup of 13.66 when running with
RDMA support and 13.4 without RDMA. We were surprised to find that TCP/IP over
Fast Ethernet was broken for 16 nodes. M-VIA over Fast Ethernet achieved a speedup
of 11.93, reasonably close to the faster networks (Figure 2d).
4.2.5 EP
In the Embarrassingly Parallel (EP) kernel, each processor independently generates
pseudorandom numbers in batches and uses these to compute pairs of normally
distributed numbers. This kernel requires virtually no inter-process communication;
the communication is just needed when the tallies of all processors are combined at
the end of computation. All systems achieved linear speedups in this application (only
TCP/IP was slightly slower – see Figure 2e).
4.2.6 LU
The Lower-Upper (LU) diagonal kernel uses a symmetric successive over-relaxation
(SSOR) procedure to solve a regular sparse, block (5x5) lower and upper triangular
system of equations in three dimensions. A partitioning of the grid onto processors
occurs by halving the grid repeatedly in the first two dimensions until all processors
are assigned. The SSOR procedure proceeds on diagonals, which progressively sweep
from one corner on the third dimension to the opposite corner of the same dimension.
Communication of partition boundary data occurs after completion of computation on
all diagonals that contact an adjacent partition. This results in a very large number
(1,200,000) of very small messages (about 2 Kb). The bandwidth required by LU is a
little: 0.45 Mbytes/s.
Surprisingly, the performance of Myrinet is terrible in this benchmark. It achieves
a speedup of 7.22 on 16 nodes, while TCP/IP over Fast Ethernet achieves a speedup
of 12.76. VIA is effective in improving the performance of LU on Fast Ethernet,
achieving a speedup of 15.16. Giganet has the better performance for LU: 15.66 for
the version with RDMA, and 15.48 for the version without RDMA (Figure 2f).
While Giganet spends 13.5% of the time sending messages, Myrinet spends almost
40%. The MPI_Send, MPI_Recv and MPI_Wait primitives are responsible for the
poor performance of LU on Myrinet. While Myrinet spends 522s, 39s and 67s on
MPI_Send, MPI_Recv and MPI_Wait, respectively, Giganet spends 41s, 11s and
11s (Figure 3f). Previous work [8] also pointed this poor performance if LU on
Myrinet. They also used MPICH-GM on Myrinet, but used MPI/Pro on Giganet. The
work attributed the poor performance of LU on Myrinet to the pooling-based
approach adopted by MPICH-GM (MPI/Pro adopts a interrupt-based approach). But
MVICH also adopts the pooling-based approach, and its performance is good, which
indicates that the theory presented in [8] is not correct. Instead, we suggest that the
problem may stem from the way GM allocates memory for small blocks. GM has a
cache for small blocks. We believe that cache management for the very many small
blocks created by the application may be significantly hurting performance.

4.2.7 Analysis
The results show that M-VIA was effective in improving the performance of all
benchmarks on Fast Ethernet, when compared with the version that uses TCP/IP. The
performance on Ethernet with VIA is 3.6 times better than the version with TCP/IP
for IS; 1.12 times better for MG; 1.26 times better for CG; 1.25 times better for SP;
and 1.18 times better for LU. The biggest difference between VIA and TCP/IP
appeared in the application that has the biggest bandwidth requirement, IS, which
suggests that the gains provided by VIA are related to the bandwidth required by the
application: the bigger the bandwidth required, the bigger the performance difference
between VIA and TCP/IP.
The RDMA support provided by VIA on Giganet was effective in improving the
performance of IS: it runs 1.14 times faster than the version without RDMA. For all
other applications, the RDMA support contributes for a small improvement in
performance (less than 2%). On Ethernet, the version with RDMA support was quite
instable due to sequence mismatch messages. This occurs because the NIC has
dropped packets under heavy load: the interrupt service routine on the receiving node
cannot process the packets as fast as the NIC receives them, causing the NIC to run
out of buffer space. Because M-VIA supports only the “Unreliable Delivery” mode of
VIA, these events do not cause a disconnect and the MVICH library gets different
data than what it is expecting. The behavior of MVICH in this case is to abort. It is
also interesting to note that on Ethernet, the benchmarks with RDMA support
performed 8% worst on CG. In IS, for 2 nodes, the performance of the version with
RDMA was 6% better than the version without RDMA. Recall that for this
benchmark, the configurations with more than 4 nodes have broken. In all other
benchmarks, the performance was equal. The results of both Giganet and Fast
Ethernet indicate that the RDMA support is only effective when the message size is
huge.
As could be expected, Ethernet, even with VIA support, performed worst than the
best execution time, either Giganet’s or Myrinet’s, for all benchmarks. The only
exception was EP, where Ethernet performed as well as Myrinet and Giganet. This
happened because EP’s almost does not require communication. For IS, Ethernet had
its worst performance (124% slower than Giganet) due to the high bandwidth required
by this application. The better performance, after EP, is LU: only 3% slower than
Giganet. Not surprisingly, LU is the application with the smaller bandwidth
requirement after EP. Other performances are 15% worst for MG and SP and 45% for
CG. The performance on LU and MG suggests that Fast Ethernet is a good choice to
connect machines on a small cluster environment where bandwidth is not crucial for
applications, since it has a good cost-benefit ratio. Fast Ethernet is 10 times cheaper
than Giganet and Myrinet, and its performance is 2 times slower for the worst
application, IS. If performance is most important, a good rule of thumb would be to
choose Fast Ethernet with TCP/IP if your application requires very small bandwidth,
and to use a faster protocol such as M-VIA on the same hardware if your application
requires up to 5 MBs.
Giganet performed better than Myrinet for IS (18%) and LU (116%); had a similar
performance for EP and SP; and performed slightly worst for MG (2%) and CG (4%).
These results show the influence of the MPI implementation over performance. The
raw communication numbers presented in Figure 1 could suggest that Myrinet would

have the best performance, indicating that the implementation of MPICH-GM does
not take full advantage of the performance of Myrinet.

5. Conclusions
This paper evaluated the impacts of three popular cluster interconnection
technologies, namely Fast Ethernet, Giganet and Myrinet, over the performance of the
NAS Parallel Benchmarks (NPB). The results show that Fast Ethernet, when
combined with a high performance communication protocol, such as VIA, has a good
cost-benefit ratio, and can be a good choice to connect machines on a small cluster
environment where bandwidth is not crucial for applications, such as MG and LU. We
also evaluated the performance gains provided by VIA, when compared with TCP/IP,
and found that VIA is quite effective in improving the performance of applications on
Fast Ethernet. The RDMA support provided by VIA was evaluated, and we conclude
that it is only effective when the messages exchanged by the applications are huge.
Last, our results showed GigaNet performing better than Myrinet on the NPB. We
found the main difference in LU, where Myrinet performance is poor due to the MPI
implementation for this interconnect technology, and not due to the pooling-based
approach adopted by the MPI implementation, as a previous work has pointed out.

6. References
1. Giganet, Inc. http://www.emulex.com/
2. Myricom, Inc. http://www.myri.com/
3. Compaq, Intel, and Microsoft. Virtual Interface Architecture Specification, Version 1.0.
Available at http://www.viarch.org.
4. BAILEY, D.; BARTON, J.; LASINSKI, T.; SIMON, H. The NAS Parallel Benchmarks.
Technical Report 103863, NASA, July 1993.
5. National Energy Research Scientific Computing Center. M-VIA: A High Performance
Modular VIA for Linux. http://www.nersc.gov/research/FTG/via/.
6. Message Passing Interface Forum. http://www.mpi-forum.org/.
7. National Energy Research Scientific Computing Center. MVICH: MPI for Virtual Interface
Architecture. http://www.nersc.gov/research/FTG/mvich.
8. HSIEH, J.; LENG, T.; MASHAYEKHI, V; ROOHOLAMINI, R. Architectural and
Performance Evaluation of GigaNet and Myrinet Interconnects on Clusters of Small-Scale
SMP Servers. Proceedings of the ACM/IEEE SuperComputing 2000 Conference, Dallas,
November, 2000.

