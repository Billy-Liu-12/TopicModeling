An Algorithm for Generating Classification Rules Based
on Extended Function Dependency
Xiaoping Zhang, Fengzhan Tian, and Houkuan Huang
School of Computer & Information Technology,
Beijing Jiaotong University, Beijing 100044, P.R. China
xiao_ping_zhang@126.com,{fzhtian,hkhuang}@bjtu.edu.cn

Abstract. Classification is an important task in the fields of data mining and
pattern recognition. Now there have been many algorithms for this task, while
most of them do not focus on the application in databases. In this paper we
extend the definition of function dependency, prove the properties of the
extended function dependency, and on this basis propose an algorithm for
classification. According to the two theorems in the paper, our algorithm is
complete that means it can find all the classification rules from the database. At
last, we demonstrate our algorithm by an example that shows the validity of our
algorithm.
Keywords: Data mining, Classification, Function Dependency, Scope.

1 Introduction
Classification is an important task in the fields of data mining and pattern
[1,2]
[3]
recognition and there have been developed many algorithms used for this task up
to now. At present, the most used classification methods include decision tree method
[4]
[5]
[6]
(such as C4.5 , SLI algorithm and SPRIN algorithm ), Bayesian classifiers (such as
[7]
[8,9]
[10]
etc.), artificial neural network , classification algorithm
Naive Bayes , TAN
[11]
[12]
based on association rules , rough set and so on. Although these algorithms are
used successfully in many problems, they all use the strategy of handling the dataset
in main memory that is hard to integrate with database system. When the dataset is
too large to be stored in memory, the above methods can’t be applied. So it is very
important to study classification techniques oriented to database system.
At present, classification algorithms oriented to database system mainly include
[13]
[14]
MIND algorithm and GAC-RDB algorithm . MIND builds classifiers using
decision tree method, compute the class distribution information of the datasets of
non-leaf nodes using UDF method and partition the dataset. MIND algorithm is apt to
integrate with database system, but the UDF is achieved by advanced program
language that can’t utilize the query optimization mechanism of the database system.
So it is very complex to program and maintain UDF. In addition, the function
achieved by SQL statements in MIND algorithm is very simple, but the way of using
SQL statements is very complex. GAC-RDB is a kind of classification algorithm
using SQL statements. Its advantage is to use the standard group aggregation
Y. Shi et al. (Eds.): ICCS 2007, Part III, LNCS 4489, pp. 645–652, 2007.
© Springer-Verlag Berlin Heidelberg 2007

646

X. Zhang, F. Tian, and H. Huang

statements which can take advantage of the query processing function of database
system so as to improve its efficiency and expandability. Also, the algorithm does not
need to transform the dataset to the format of transaction database and to scan the
dataset repeatedly. Furthermore the algorithm is apt to be programmed and integrated
with database system. But the algorithm still needs to be improved in some aspects,
such as how to determine automatically the parameters in the algorithm and how to
improve the feature selection method etc.
[15]
In this paper, we use the idea of discernible matrix in rough set and Granule
[16]
calculating for reference and extend the definition of function dependency. And
then, we prove the properties of scope of the extended function dependency and on
this basis propose a classification algorithm. Our algorithm has the following
advantages. It can obtain the Boolean matrixes representing the function dependency
relations by scanning database only once. Using logical operation ‘Or’ on Boolean
matrixes, we can get all the classification rules. The algorithm is efficient because of
the use of Boolean operations. In addition, the algorithm can generate all the
classification rules without requiring manual input parameters. The two theorems in
the paper guarantee the completeness of our algorithm. Finally, the generated
classification rules are represented as the form of function dependency and easy to
understand.

2 Function Dependency and Its Extension
2.1 The Definition of Function Dependency
Definition 1. Let R is a relational schema, X , Y ⊆ R , X → Y represents a function
dependency, r is a relation of R . r fits function dependency X → Y if and only if
u1 [ X ] = u 2 [ X ] implies u1 [Y ] = u 2 [Y ] for ∀u1 , u 2 ∈ r . An equivalent description is as

follow: r fits function dependency X → Y if and only if for ∀(u1 , u 2 ) ∈ r × r
(u 1 [ X ] = u 2 [ X ]) ⇒ (u1 [Y ] = u 2 [Y ]) is true.

，

There are two conditions of (u 1 [ X ] = u 2 [ X ]) ⇒ (u1 [Y ] = u 2 [Y ]) being true. One is
u1 [ X ] ≠ u 2 [ X ] ; the other is u1[ X ] = u2 [ X ] and u1 [Y ] = u 2 [Y ] . Definition 1 shows that
in classical relation theory, relation r fitting function dependency X → Y indicates
that
each
pair
(u1 , u 2 )
in
r×r
satisfies
implication
(u 1 [ X ] = u 2 [ X ]) ⇒ (u1 [Y ] = u 2 [Y ]) . Next we extend the above definition and allow
part of pairs (u1 , u 2 ) in r × r to satisfy (u 1 [ X ] = u 2 [ X ]) ⇒ (u1 [Y ] = u 2 [Y ]) . We name
the set of this part of pairs (u1 , u 2 ) as the scope of function dependency X → Y .
2.2 The Definition of Extended Function Dependency
Definition 2. Let R is a relational schema, r = {u1 ,...u n } is a relation of R ,
X , Y ⊆ R , the scope of function dependency X → Y on relation r is denoted with
FX ,Y , which is a subset of r × r :

An Algorithm for Generating Classification Rules Based

647

FX ,Y = {(u i , u j ) | u i , u j ∈ r ∧ ((u i [ X ] = u j [ X ] ) ⇒ (u i [Y ] = u j [Y ] ))}
According to classical relation theory, if r fits X → Y
r does not fits X → Y

，then F

⊂ r×r .

X ,Y

，then F

X ,Y

= r × r , and if

Definition 3. Let R is a relational schema, r = {u1 ,...u n } is a relation of R , X , Y ⊆ R ,
the Boolean matrix T of function dependency X → Y on relation r is:

⎧1 (ui , u j ) ∈ FX ,Y
ti j = ⎨
⎩0 else

，

(i, j = 1,..., n)

In Boolean matrix T if all the values of a whole line are 1, we call this line as ‘all 1
line’. And then we define the set of elements corresponding to the ‘all 1 line’ as scope
of function dependency X → Y .
For the analysis purpose, if we regard the table 1 as original database to generate
classification rules and D as class attribute which are taken from the records in breast[17]
cancer-wisconsin_data of UCI and then the reduplicate record is removed , then we
can regard {u1,u2,u3} as class one, { u4,u5,u6} as class two. We can extend function
dependency to applications of classification.
Table 1. An example of a relation r

r

B (Uniformity of
Cell Size )
1
1
1
1
10
7

A (Clump thickness)

u1
u2
u3
u4
u5
u6

2
3
2
10
2
2

C (Mitoses)

D (Class)

1
1
1
1
4
1

2
2
2
4
4
4

2.3 The Properties of the Scope of the Extended Function Dependency

， r is a relation of

Theorem 1. Let R is a relational schema
F X ∪ X ,Y = F X ,Y ∪ F X , Y .
1

2

1

2

R, X 1 , X 2 , Y ⊆ R

，then

：

Proof. We only need to prove the following two propositions

（1） For ∀(u , u ) ∈ r × r ，if (u , u ) ∈ F
1

2

i

(u i , u j ) ∈ F X

2 ,Y

(u i , u j ) ∉ FX

2 ,Y

j

, then (u i , u j ) ∈ FX ,Y or
1

,whose equivalent proposition is that if (u i , u j ) ∉ FX ,Y and

，then (u , u ) ∉ F

1

（2） For ∀(u , u ) ∈ r × r , if (u , u ) ∈ F
1

X 1 ∪ X 2 ,Y

i

2

j

i

then (u i , u j ) ∈ FX ∪ X
1

2 ,Y

.

j

X 1 ∪ X 2 ,Y
X 1 ,Y

.

or (u i , u j ) ∈ FX

2 ,Y

，

648

X. Zhang, F. Tian, and H. Huang

The proof of the first proposition is as follows. According to (u i , u j ) ∉ FX

1 ,Y

and

the definition 2, we get u i [ X 1 ] = u j [ X 1 ] and u i [Y ] ≠ u j [Y ] . According to
(u i , u j ) ∉ FX

infer

2 ,Y

， we know u [ X ] = u [ X ] and u [Y ] ≠ u [Y ] . To sum up, we can
i

2

j

2

ui [ X 1 ∪ X 2 ] = u j [ X 1 ∪ X 2 ]

that

(u i , u j ) ∉ FX

1 ∪ X 2 ,Y

i

and

j

u i [Y ] ≠ u j [Y ]

,

that

means

. So the first proposition is proved.

The proof of the second proposition is given as following. When there is
(u i , u j ) ∈ FX ,Y according to the definition 2, there are two following cases. One case
1

，

is u i [ X 1 ] ≠ u j [ X 1 ] . Then there is u i [ X 1 ∪ X 2 ] ≠ u j [ X 1 ∪ X 2 ] and then we know
(u i , u j ) ∈ FX

according to the definition 2. The other is u i [ X 1 ] = u j [ X 1 ] and

1 ∪ X 2 ,Y

u i [Y ] = u j [Y ] ,
(u i , u j ) ∈ FX

When

1 ∪ X 2 ,Y

there

(u i , u j ) ∈ FX

1 ∪ X 2 ,Y

u i [ X 1 ∪ X 2 ] = u j [ X 1 ∪ X 2 ] or

Whether
.

is (u i , u j ) ∈ FX

2 ,Y

， in

the

same

not,

way,

we
we

can have
can

. So we conclude that if (u i , u j ) ∈ FX ,Y or (u i , u j ) ∈ FX
1

2 ,Y

prove

, there is

(u i , u j ) ∈ FX ∪ X ,Y . The second proposition is proved. Therefore, the original
proposition is proved.
1

2

Theorem 2. Let R = { A1 , , An , D} is a relational schema, where A1 , , An are
conditional attributes and D is a class attribute, r is relation of R, u ∈ r , X ⊆ R . If
for ∀u i ∈ r , there is (u , u i ) ∈ FX , D and for each X 1 ⊂ X , there exist ui ∈ r which

does not satisfy (u , u i ) ∈ FX , D , then ( X = u[ X ]) ⇒ ( D = u[ D ]) is a classification
rule.
1

Proof. For each u i ∈ r , according to (u , u i ) ∈ FX , D

， we

know u[ X ] ≠ u i [ X ] or

u[ X ] = u i [ X ] and u[ D ] = u i [ D ] . That means for each element v whose value is
u[ X ] on X, there is v[ D ] = u[ D ] , namely ( X = u[ X ]) ⇒ ( D = u[ D ]) . And because

for each X 1 ⊂ X , there is ( X 1 = u[ X 1 ]) ⇒
/ ( D = u[ D ]) ,
is
a
classification
rule.
( X = u[ X ]) ⇒ ( D = u[ D ])

so

we

know

3 An Algorithm for Generating Classification Rules Based on
Scope of Extended Function Dependency
On the basis of Theorem 1 and Theorem 2, we propose an algorithm for generating
classification rules based on scope of extended function dependency, whose
procedure is as follows.
Input: Relational schema R = { A1 , , An , D} and relation r of R, where A1 ,
conditional attributes and D is a class attribute
Output: A set of all classification rules S
Suppose the reduplicate records are removed before following step.

, An are

An Algorithm for Generating Classification Rules Based

649

Step 1. For conditional attributes A1 , , An , calculate the Boolean matrixes of
FA , D , … , FA in turn. For each matrix, each ‘all 1 line’ u can generate a
1

n,D

classification rule, ( Ai = u[ Ai ]) ⇒ ( D = u[ D ]) . Add it to set S.
Step 2. Let i = 2 , T = { FA , D } .
1

Step 3. Let T ′ = T ⊕ FAi , D , where
T ⊕ FA , D = {FX ∪{ A }, D | FX ∪{ A }, D = FX , D ∪ FA , D and FX , D ∈ T }
i

i

i

Step 4. Check the ‘all 1 lines’ of each FX ′, D

i

in T ′ (here X ′ = X ∪ Ai ). If the line is

included neither in T nor in FA , D , then we call the line as a new ‘all 1 line’. It can
i

generate a classification rule, ( X ′ = u[ X ′]) ⇒ ( D = u[ D]) . Add the rule to set S.
Step 5. Let T = T ∪ T ′ ∪ { FA ,D } .
i

Step 6. Let i = i + 1 . If i ≤ n , then go step 3; otherwise exit.
Suppose that the numbers of records and attributes in the database are m and n
respectively. The Step 1 in the above procedure need to scan the database only once
and then get FA , D , … , FA , whose time complexity is O( m 2*n ). The time
1

n,D

2

complexity is also O( m *n ) to find the ‘all 1 lines’ in FA , D . In step 3, the time
i

complexity of circling calculation of T ′ = T ⊕ FA , D is O( 2 n * m 2 ). In step 4, the time
i

complexity is O( 2 n * m 2 ) at worst case to check new ‘all 1 lines’ in each FX ′, D and
then generate classification rules. So the time complexity of the whole procedure is
O( m 2*n )+O( 2 n * m 2 ).

4 A Computation Example
Suppose R = { A, B, C , D} and relation r of R is shown in Table 1. Assume D is a
class attribute. According to the above algorithm, the steps to generate classification
rules are as follows.
Step 1. Firstly generate the Boolean matrixes of function dependency A → D ,
B → D and C → D , which are denoted as table 2, table 3 and table 4 respectively.
According to ‘all 1 lines’ in table 2, we know that FA, D contains u2 , u4 . So we have
classification rules ( A = u2 [ A]) ⇒ ( D = u2 [ D ]) and ( A = u4 [ A]) ⇒ ( D = u4 [ D]) ,
which means ( A = 3) ⇒ ( D = 2) and ( A = 10) ⇒ ( D = 4) . In the same way, we can
get classification rule ( B = 10) ⇒ ( D = 4) and ( B = 7) ⇒ ( D = 4) . And also
we can get classification rule
according to the three ‘all 1 lines’ u5 in table 4•we
(C = 4) ⇒ ( D = 4) . So,

S = { ( A = 3) ⇒ ( D = 2) , ( A = 10) ⇒ ( D = 4) , ( B = 10)

⇒ ( D = 4) , ( B = 7) ⇒ ( D = 4) , (C = 4) ⇒ ( D = 4) }.
Step 2. Let i = 2 , T = { FA,D } .
Step 3. Let T ′ = T ⊕ FB , D = FAB , D , which is denoted as table 5, where all the new
‘all 1 lines’ are denoted by bold font.

650

X. Zhang, F. Tian, and H. Huang

Step 4. The new ‘all 1 lines’ in table 5 are u1, u3. So we have the following rules
( AB = 〈 2 , 1 〉 ) ⇒ ( D = 2) . Therefore, S = S ∪ {( AB = 〈 2 , 1 〉 ) ⇒ ( D = 2)}.
Step 5. Let T = { FA , D , FAB , D , FB , D } .
Step 6. i = i + 1 = 3 , go back to Step 3 of the algorithm and recalculate T and S ,
whose scope of function dependency are FAC , D , FBC , D and FABC , D respectively. The
computation process is as same as the above, so we do not give the details here for the
sake of space. At last, we get the following set of all the classification rules:
S ={

( A = 3) ⇒ ( D = 2) , ( A = 10) ⇒ ( D = 4) , ( B = 10) ⇒ ( D = 4) , ( B = 7)

⇒ ( D = 4) , (C = 4) ⇒ ( D = 4) , ( AB = 〈 2 , 1 〉 ) ⇒ ( D = 2) }.
Table 2. Boolean matrix of function dependency A→D

FA,D
u1
u2
u3
u4
u5
u6

u1
1
1
1
1
0
0

u2
1
1
1
1
1
1

u3
1
1
1
1
0
0

u4
1
1
1
1
1
1

u5
0
1
0
1
1
1

u6
0
1
0
1
1
1

Table 3. Boolean matrix of function dependency B→D

FB,D

u1

u2

u3

u4

u5

u6

u1
u2
u3
u4
u5
u6

1
1
1
0
1
1

1
1
1
0
1
1

1
1
1
0
1
1

0
0
0
1
1
1

1
1
1
1
1
1

1
1
1
1
1
1

Table 4. Boolean matrix of function dependency C→D

FB,D

u1

u2

u3

u4

u5

u6

u1
u2
u3
u4
u5
u6

1
1
1
0
1
0

1
1
1
0
1
0

1
1
1
0
1
0

0
0
0
1
1
1

1
1
1
1
1
1

0
0
0
1
1
1

An Algorithm for Generating Classification Rules Based

651

Table 5. Boolean matrix of function dependency AB→D

FAB,D

u1

u2

u3

u4

u5

u6

u1

1

1

1

1

1

1

u2

1

1

1

1

1

1

u3

1

1

1

1

1

1

u4

1

1

1

1

1

1

u5

1

1

1

1

1

1

u6

1

1

1

1

1

1

5 Conclusion
In this paper, we extend the definition of function dependency, describe the concept
of scope of function dependency and prove its properties. Based on the above
properties, we put forward an algorithm for generating all the classification rules. In
our algorithm, the dataset, Boolean matrixes and classification rules are all stored in
database, so our algorithm can be applied to classification tasks in case of large-scale
datasets. The algorithm is efficient because most of calculate is Boolean operation on
Boolean matrixes. Our algorithm generates all the classification rules, but some
classification rules are complex and are not typical. Next, we will do research on how
to evaluate, merge and tailor the rule set to generate simple and effective rules. Our
algorithm provides a new idea for research of classification method.

Acknowledgments
This work is supported by NSF of China under grant NO. 60503017, Beijing Nova
Programme under grant NO. 2006A17 as well as Science Foundation of Beijing
Jiaotong University under Grant No. 2005SM012.
Thank anonymous readers for helpful comments.

References
1. Han J. W., Fu Y. J., Wang W. et al.: DBMiner: A System for Mining Knowledge in Large
Relational Database. In: Proceedings of the 2nd International Conference on Knowledge
Discovery and Data Mining. CA, USA(1996) 250-255
2. Agrawal R., Mehta M., Shafer J. et al.: The Quest Data Mining System. In: Proceedings of
the 2nd International Conference on Data Mining and Knowledge Discovery, Portland,
Oregon (1996) 244-249
3. Liu H. Y.: Research and Implementation of a Fast and Scalable Classification System.
Beijing: Tsinghua University(2000)
4. Quinlan J. R.: C4.5: Programs for Machine Learning. San Mateo. California: Morgan
Kaufmann (1993)

652

X. Zhang, F. Tian, and H. Huang

5. Mehta M., Agrawal R., Rissanen J.: SLIQ: A fast scalable classifier for data mining.
Lecture Notes in computer Science, Proc of the 5th International Conference on Extending
Database Tech. Avignon. France(1996) l8–33
6. Shafer J. C., Agrawal R, Mehta M.: SPRINT: A scalable parallel classifier for data
mining. Proc of the 22nd Int Conf on Very Large Databases. Mumbai Bombay.India(1996)
7. Elkan C.: Boosting and naïve bayesian learning. In Technical Report CS97-557, Dept. of
Computer Science and Engineering, Univ. Calif. At San Diego(1997)
8. Friedman N. ,Geiger D. and Goldszmidt M.: Bayesian network classifier. Machine
Learning. Vol. 29. (1997) l31–163
9. Meretakis D. and Wuthrich B.: Extending Naive Bayes classifiers using long itemsets.
Chaudhuri S.Proceedings of 5th International Conference on Knowledge Discovery and
Data Mining. USA: AAAI Press(l999)295-301
10. Weiss S. M. and Kulikowski C. A.: Computer Systems That Learn: Classification and
Prediction Methods from Statistics. Neural Nets, Machine Learning, and Expert Systems.
San Mateo, CA: Morgan Kaufmann(1991)
11. Liu B. Hsu W. Ma Y. : Integrating classification and association rule mining. Agrawal R
Proc of the 4th Int Conf on Knowledge Discovery and Data Mining. NY.USA: AAAI
Press(1998)80-86
12. Hu X. H.,Cercone N.: Learing in relational database: a Rough Set approach.
Computational Intelligence Vol 11. (1995) 323-338
13. WANG M., Iyer B., Vitter J. S.: Scalable mining for classification rules in relational
databases. Eaglestone B. Desai B C, SHAO Jianhua.Proc of the 1998 Int Database Eng and
Appl Syrup Cardiff,Wales.UK: IEEE Computer Society(1998) 58-67
14. Lu H. J. Liu H. Y.: Decision tables: scalable calssification exploring RDBMS capabilities.
Proc 26th Int Conf on Very Large Databases. Cairo.Egypt (2000)373-384
15. Skowron A. ,Rauszer C.: The discernibility matrices and functions in information system.
In : Slowinsk; Red. Intelligent Decision Support—Handbook of Applictions and Advances
of the Rough Set Theory, Kluwer Academic Publishers(1992) 331-362
16. Liu Q. and Jiang S. L.: Reasoning about Information Granulaes Based on the Rough
Logic. LNAI 2375, Springer(2002) 139-143
17. Bennett K. P., Mangasarian O. L.: Robust linear programming discrimination of two
linearly inseparable sets, Optimization Methods and Software 1, Gordon & Breach Science
Publishers(1992) 23-34

