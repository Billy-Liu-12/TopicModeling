Neural Network For Modeling Nonlinear Time Series:
A New Approach
Chokri Slim1
2
Abdelwahed Trabelsi
1

University of Tunis, Bestmod Laboratory, 41 rue de la liberté
2000 Le Bardo, Tunisia
chokri.slim@iscae.rnu.tn
2
University of Tunis, Bestmod Laboratory, 41rue de la liberté
2000 Le Bardo, Tunisia
abdel.trabelsi@isg.rnu.tn

Abstract. Nonlinear modeling with neural networks offers a promising
approach for studying the prediction of a chaotic time series. In this paper, we
propose a neural net based on Extended Kalman Filter to examine the nonlinear
dynamic proprieties of some financial time series in order to differentiate
between low-dimensional chaos and stochastic behavior. Kalman filtering,
because it can deal with varying unobservable states, provides an efficient
framework to model these non-stationary exposures. A controlled simulation
experiment is used to introduce the issues involved and to present the proposed
approach. Measures of forecast accuracy are developed. The pertinence of this
model is discussed from the Tunisian Stock Exchange database.

1 Introduction
Within the past decades, there has been a growing interest in applying nonlinear
models to predict chaotic time series [1]. The major problem in these researches is the
difficulty of distinguishing between deterministic chaos and purely random processes.
An appreciable amount of recent literature deals with this subject [2] and [3].
By new neural network architecture we allow additional information flows
between the different outputs and as a sequence we get a better representation of the
underlying dynamic system. The net is based on multilayer feed forward architecture
with random connections. The model can then convert to its equivalent state space
representation. Using this state space form, a Bayesian inferential algorithm based on
non linear Kalman Filter is derived to estimate the state. We analyze the net of a
chaotic time series (logistic map) by using the dynamic invariant that characterizes
the attractor, the largest Lyaponov exponent. This approach permits us to claim that
the net capture ''chaos'' if it learns the dynamic invariant of a chaotic time series. A
detailed step by step description of the methodology is presented to facilitate the use
of this new method. Some issues related to the practical use of the proposed model in
the context of the Tunisian Financial market are also considered.
P.M.A. Sloot et al. (Eds.): ICCS 2003, LNCS 2659, pp. 159−168, 2003.
 Springer-Verlag Berlin Heidelberg 2003

160

C. Slim and A. Trabelsi

2 Neural Net Architecture
A general form of nonlinear time series with functional coefficients can be given by:
Yt=ƒ (Xt,ϕ(Xt),γ(Xt)) + ξt

(1)

Where ƒ (.) : is unknown non linear function. Yt : the time series.
Xt =(Yt-1, Yt-2, …, Yt-p, Zt-d-1, Zt-d-2 ,…, Zt-d-m) : is a vector of p-past values time series Yt and
m-past values input Zt,. This vector reflect some changing environmental conditions
that causes the system’s parameters to vary; p, m, d : represent the orders and time
delay of the mode; Zt: the measured input; ϕ(Xt) and γ(Xt) : are vectors of unknown
parameters which are functions of Xt ; ξt : is a sequence of iid random variables with
mean 0 and variance Rt.
The main difficulty in using the proposed model in (1) is specifying the functional
coefficients. We consider in this section, a special class of neural networks called
Tangent Hyperbolic neural Networks THNN to identify the model in (1). The most
important feature of the THNN is the smooth output which is due to the shape of the
tangent hyperbolic functions. It has been proved that this class of networks can
approximate continuous functions at any arbitrary accuracy. A complete description
of a Neural Networks theory and the application of neural networks to the problem of
nonlinear system identification and prediction can be found in [5], [6]. The non linear
coefficients ϕi (Xt) and γj (Xt) in (1) can be approximated by a sub-THNN, then:

ϕˆ i ( X t)

q

ϕ

= ∑ w k i hk ( Xt ) .
k =1

(2)

q

= ∑ w kγ j h k ( X t ) .

γ j ( X t)

k =1

(3)

Representing each parameter of (2) and (3) as a neural network in figure1, the
overall estimate model (1) can be represented in figure1, which is a two layered
THNN structure. The first layer is composed of sub networks and the second layer is
the output given by:

Yt = h( Xt , ϕˆ ( Xt ) , γˆ ( Xt ) ) + ξ .

(4)

We call this new neural network a stochastic neural net (SNN). We propose, now,
a Gradient Back propagation algorithm to train the free parameters which are the
weights of the sub networks in the first layer of the SNN, while the ''weights'' of the
second layer are viewed as fixed in every iteration of this algorithm. Denote:
ϕ

[
= [w

ϕ

ϕ

]
] ∈IR , j = 1,...,m .

β t i = w i ,wϕ2 i ,...,w qi ∈IRq , i = 1,...,m .
1

β

γ
t

j

γ

1

j

γ

γ

,w 2j ,...,w q j

q

(5)

(6)

Neural Network for Modeling Nonlinear Time Series: A New Approach

[

ϕ

ϕ
ϕ
γ
γ
γ
B t −1 = β t 1 β t 2 ...β t p β t 1 β t 2 ...β t m

[

] ∈ IR
T

h t −1 = h1 ( X t ) , h 2 ( X t ) ,..., h q ( X t )

[

]

T

Ft −1 = Yt −1 , Yt − 2 ,..., Yt − p , Z t − d −1 ,..., Z t − d − m

[

qx(p + m)

∈ IR

]

T

q

(7)

.

.

∈ IR

]

161

(8)
(9)

.

O t −1 = ϕ 1 ( X t ),..., ϕ p ( X t ), γ 1 ( X t ),..., γ m ( X t ) ∈ IR

p+ m

.

(10)

Yt
h

Y t- 1

Y t- 2

Z t- d - m
•

β

ϕ1

•

•

γ m

ϕ 2

ϕ 1

Σ

β
…

ϕ2

Σ

•

•

•

β

γm

.. .

Σ
…

Xt

Fig. 1. A stochastic neural network for non linear time series

A time t, the input vector Ft-1 is presented to the neural network shown in figure1.
In the forward pass the networks calculates the output according to (2) and (3). The
output of the first layer and final output of the SNN are given by:
O t −1 = h t −1 Bt −1 .
T
Yˆt = h(O t −1 Ft −1 ) = h (h t −1 B t-1 Ft −1 ).

The error function to be minimized is:

(11)

(12)

162

C. Slim and A. Trabelsi

J=

1

n

(13)

2
∑ (Yt − Yˆt )

2 t −1

The algorithm computes the instantaneous gradient of the network's error function
(J):
∂J
.
∇ˆ t −1 =
∂B t- 1

(14)

And the updates the weights according to the steepest descent equation:
ˆ
B t = B t −1 − µ∇
t −1

(15)

Where µ is the learning rate. Let:

ε t = Yt − Yˆt .

(16)

h(.) : is the tangent hyperbolic function.
We can now perform the steepest descent procedure to each weight in the second
layer:
ϕi
t

β
β

γ
t

j

2
T
ϕ
= β t −i1 + µε t Yt −i (1 − Yˆt )h t −1

γ

2

T

= β t −j1 + µε t Z t −d − j (1 − Yˆt )h t −1

for i = 1,2,..., p

(17)

for j = 1,2,..., m.

(18)

This completes the description of the gradient back propagation algorithm and the
next section discusses the corresponding non linear state space method.

3 Nonlinear State Space Representation
Having identified the system model using the above neural network algorithm, it is
useful to recast the SNN in the state-space form to perform state estimation. This
requires the coefficients ϕi (Xt) and γj (Xt) as function of time. In this case the SNN
can be in the following form:
p

m

i =1

j =1

Yt = h ( ∑ ϕ i,t Yt-i + ∑ γ

j,t

Z t-d − j ) .

(19)

The non linear state space of SNN is given by:

Θ t +1 = H(Θ t , ϕ t , Z t ) + ω t .

(20)

Yt = h (Θ t , Z t ) + ξ t .

(21)

Neural Network for Modeling Nonlinear Time Series: A New Approach




Θt = 




θ 1, t
θ 2 ,t
...
θ p −1 , t
θ p ,t




;




Y t − p +1 = θ 2 , t


Y t − p + 2 = θ 3 ,t

...
H (Θ t , ϕ t , Z t ) = 

Y t −1 = θ p , t
 Y = h (ϕ θ
1, t
p , t + ... + ϕ p , t θ 1 , t
 t

163









m

And : Z t = ∑ γ j,t Z t − d − j
j=1

Equations (20) and (21) represent respectively the transition equation and the
observation equation, with Θt is the state vector of the system at time t and Yt is the
observation vector at time t. The ωt is of innovations, with zero mean and variance Qt,
ξt is an additive measurement noise with zero mean and variance Rt. We assume that

 Qt
 0

the noise vectors are uncorrelated with covariance: P ωξ = 

0
Rt

 .



4 Bayesian Inferential Algorithm Based on Extended Kalman Filter
The Kalman filer is a set of mathematical equations that provides an efficient
computational (recursive) solution of the least-squares method. The filter is very
powerful in several aspects: it supports estimations of past, present, and even future
states, and it can do so even when the precise nature of modeled system is unknown.
A complete discussion to the idea of the Kalman filter can be found in [7].
With the above non linear state space model of (20), the application of the Kalman
filter is not straightforward because the model is not linear. We are established, a
recursive algorithm for the measurement and the state update for the dynamic system
represented by the SNN. This algorithm can be summarized by the update recursion
equations.

[

]

ˆ
ˆ
Θ
t +1/t = H ( Θ t/t , Z t ) .
Θ

Θ

(22)

T

Pt +1 / t = A t Pt / t A t + Q t .
K t +1 =

Θ
Θ
T
T
Pt +1 / t G t +1 (G t +1 Pt +1 / t G t +1

(23)
-1

+ R t +1 ) .

ˆ
ˆ
ˆ
Θ
t +1 / t +1 = Θ t +1 / t + K t +1 (Yt +1 - h ( Θ t +1 / t , Z t +1 )) .
Θ

Θ

(24)
(25)

Pt +1 / t +1 = Pt +1 / t (I − K t +1 G t +1 ).

(26)

ˆ
Yˆt +1 / t = h(Θ
t +1 / t , Z t +1 )).

(27)

164

C. Slim and A. Trabelsi
Θ

Where Pt +1 / t : the prior estimate covariance of the state;
ˆ ,Z )
ˆ
∂H (Θ
∂h ( Θ
t
t/t
t +1/t , Z t +1 )
.
; G t +1 =
K t : the gain filter ; A t =
ˆ )
ˆ
∂ (Θ
∂ (Θ
t/t
t +1/t )

[

]

Equations (22) and (23) are the time update equations, these equations are
responsible for projecting forward (in time) the current state and error covariance
estimates to obtain the prior estimates for the next time step. Equations (24) to (27)
are the measurement update equations, the first task during the measurement update is
to compute the Kalman gain, Kt, the next step is to actually measure the process to
obtain Yt (27), and then to generate an a posterior state estimate by incorporating the
measurement as in (25), the final step is to obtain an a posterior error covariance
estimate via (26). After each time and measurement update pair, the process is
repeated with the previous a posterior estimate used to project or predicts the new a
priori estimates.

5 Simulation Studies
The largest Lyaponov exponent contains information on how far in the future
predictions are possible. The Lyaponov spectrum has proven to be one of the most
useful dynamic invariants that characterize chaotic dynamic systems. The Lyaponov
exponents provide us with a measure of the averaged exponential rates of divergence
or convergence of neighbor orbits in phase space [8]. If at least one positive
Lyaponov exponent exists, the dynamic system is said to be chaotic, and the initial
small differences between two trajectories will diverge exponentially.
We analyze the chaotic logistic map which has been hypothesized in a number of
empirical studies of financial time series (see [9]). SNN is used with the coupled
learning algorithm described in section2 and the BNKF. After training, the networks
are used in an iterative mode in order to generate a series of estimate outputs, and the
dynamic invariants measured for the original series. The data set is displayed in
figure2. In this study the chosen model has parameters d=2 and p=2, q= 15 (number
of hidden units) respectively, that is:
2

Yt = h ( ∑ ϕ i ( X t )Y t − i ) + ξ t .

(28)

i =1

Where Xt is a function of {Yt-1, Yt-2} which are the neural network’s inputs. The data
set from 1 to 500 is used to train the neural network and from 500 to 1000 to test the
neural network performance (figure2).
The network, during the training, learns the correct value of the Largest Lyaponov
exponent that is 0.52. Our analysis permits us to claim that the SNN capture ''chaos''
because it learns the dynamic invariant of a chaotic dynamic system. The simulated
SNN and BNKF output are compared with the measured output in figure3. As can be
seen, the agreements are very good.

Neural Network for Modeling Nonlinear Time Series: A New Approach

165

training and test data
1
L
O
G 0.8
I
S 0.6
T
I 0.4
c
0.2
0

0

100

200

300

0

10

20

30

400

500
600
time
Portion of the data set

700

800

900

1000

70

80

90

100

1
L
O
0.8
G
I
S 0.6
T
I 0.4
c
0.2
0

40

50
time

60

Fig. 2. . Data set from the Logistic map.

The measured output (-) and the neural net work output (- -)
1
0.8
0.6
0.4
0.2
0

0

10

20

30

40

50

60

70

80

90

100

70

80

90

100

The measured output (-) and the filtered output (--)

1
0.8
0.6
0.4
0.2
0

0

10

20

30

40

50

60

Fig. 3. Comparison of measured output and neural network model and filtered output.

6 Analysis of (BVMT)
To illustrate the application potential of the new SNN architecture on real data set, the
case study, in this section, involved the modeling of the Tunisian Stock price index
BVMT. The data set were daily closed values from Sept. 30, 1990 to Jul. 07, 1998.
Figure4 shows a temporal plot of the Tunisian Stock price index

166

C. Slim and A. Trabelsi

700

600

B
V
M
T

500

400

300

200

100

0

200

400

600

800

1000
time

1200

1400

1600

1800

2000

Fig. 4. Tunisian Stock Price Index (BVMT).
As we can see, the movements of the index not follow a random walk, and the
system dynamics are not smooth, then the linear approximation of this dynamics is
not appropriate. Identification of this non-linear structure by the SNN can clearly
enable formulation of such system.
The variables of interest in this study are:
1. BVMT: Tunisian stock Price Index.
2. PE : Price/Earning ratio for Tunisian Stock Price.
These particular set of explanatory variables were selected because they are the
types of variables used heavily by professional investors. The first Lag BVMT
(BVMTt-1) was also used as input for the SNN. The BVMTt, BVMTt-1 and the PEt
were transformed by the first difference operator (D). The time series that can be
modeled by the SNN is:
BVMTt = f(BVMTt-1, PEt, ϕ(BVMTt-1, PEt), γ(BVMTt-1, PEt))+ξt
The network architecture of the SNN was determined in part by the domain
variables. Since tree variables were selected as input stream to provide a univariate
forecast. A common difficulty in applying neural networks lies in over fitting the data.
A rule of thumb in the field of statistical modeling specifies that, for a case base of N
observations, the degrees of freedom in the model should not exceed N0.5. Since our
training set is fixed at 1000 observations, an upper bound would be about 32 weights,
each corresponding to a degree of freedom in the SNN to be trained. Initially, the
configuration 3*8*1= (corresponding to 3*8+8=32 weights) was evaluated. However,
the 3*7*1 architecture yielded slightly better performance in term of prediction error.
In this paper, the value of 7 hidden units was chosen for the SNN first layer. The SNN
was trained and validate by the gradient back propagation and the BNKF described in
section 3 and 4 (for µ=0.1).
The performance among the predictive model is presented in figure5. The SNN
output is closed to the desired output. Figure6 shows the evolution of the estimate
Lyaponov exponent as function of the number of iterations via the SNN. It is
important to remark that no overtraining is observed, the largest Lyaponov exponent
is 1, 48. We conclude that the BVMT index is chaotic and stochastic.

Neural Network for Modeling Nonlinear Time Series: A New Approach

167

The mesured output(--) and the SNN output(-)

15
10
5
0
-5
-10
-15
-20
-25

0

50

100

150

200

250
time

300

350

400

450

500

Fig. 5. Comparison of measured output and SNN output
L y a p o n o v E x p o n e n ts
3

2

1

0

-1

-2

-3

0

2 00

4 00

6 00

8 00

1 00 0
ite r a t io n

1 20 0

1 40 0

1 60 0

1 80 0

Fig. 6. Lyaponov exponents as a function of the number of iteration for the SNN trained by the
BNKF of the BVMT.

7 Conclusion
This paper proposed a new approach based on Extended Kalman filter to help
diagnose the dynamic structure of non linear time series.
For a set of input and output, a SNN can be used to identify the nonlinear time
series of (1). A non linear state space form for this net is presented and a Bayesian
algorithm for this model is derived to estimate the state.
Our analysis permits us to claim that the SNN capture ''chaos'' because it learns the
dynamic invariants of a chaotic dynamic system (Logistic map).

168

C. Slim and A. Trabelsi

The proposed model was applied to the Tunisian stock index (BVMT) index). Our
findings indicate that the BVMT index is chaotic and stochastic; the observed largest
Lyaponov exponent is 1.48682.
We believe that the encouraging results obtained herein with respect to the neural
modeling of chaotic systems, in combination with existing linear and nonlinear
dynamic techniques [10], [11] and [12], has a great potential for the modeling of
financial, economic and other time series generated by complex market driven
systems.

References
1. Blank, S.C.: Chaos in future Markets? a Nonlinear Dynamical Analysis. The Journal of
Future Markets. 11.6 (1991) 711-728
2. Albano, A., Passamante, A., Hediger, T., Farell, M. E.: Using Neural Nets to look for chaos.
Physica D. 58 (1992) 1
3. Nychka, D., Ellner, S., Gallant, A.R., Maccaffrey, D.: Findings chaos in noisy systems.
Journal of the Royal Statistical Society, Series B. 54 (1992) 399-426
4. Hertz, J., Krogh, A., Palmer, R.: Introduction to the Theory of Neural Computing, Santa Fe
Institute Studies in Science of Complexity, Amsterdam: Addison-Wesly (1991)
5. Weigend, .A.S., Huberman, B.A., Rumelhart, D.E.: Predicting the Future: A Connectionnist
Approach. International Journal of Neural Systems, Vol 1.3 (1991) 193-209
6. Hornik, K., Stinchcombe, M., White: Multilayer feed forward networks are universal
approximators. Neural Networks, vol.2 (1989) 359-366.
7. Aoki, M.: State-Space Modeling of Times Series, Berlin: Springer-Verlag (1987)
8. Wolf, A., Swift, J.B., Swinney, H.L., Vastano, J.A.: Determining Lyaponov Exponents from
a time series. Physica D. 16 (1985) 285-317
9. Hsieh, D.A.: Chaos and Nonlinear Dynamics:Application to Financial Markets. The Journal
of Finance. 6.5 (1991) 1839-1877
10. Hillmer,S.C, Trabelsi, A.: Benchmarking of Economic Time Series. Journal of the
American Statistical Association: Theory and Methods. 82 (1987) 1064-1071
11. Trabelsi, A., Hillmer, S.C.: A Benchmarking Approach to Forecast Combination. Journal of
Business and Economic Statistics. (1989) 353-362

12. Trabelsi, A., Hillmer, S.C.: Benchmarking Time Series with Reliable
Benchmarks. Journal of the Royal Statistical Society, Applied Statistics. 39:3
(1990) 367-379.

