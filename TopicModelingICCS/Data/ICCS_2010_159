Procedia
Computer
Science
1 (2012)
2407–2415
Procedia
Computer
Science
00 (2009)
000–000

Procedia
Computer
Science

www.elsevier.com/locate/procedia
www.elsevier.com/locate/procedia

International Conference on Computational Science, ICCS 2010

Kernel-based multiple criteria linear programming classifier
Zhan Zhanga, Dongling Zhangb, Yingjie Tian,c,*
a

China citic bank, Beijing 100027, China
Beijing municipal science and technology commission, Beijing 100035, China
c
Research center on fictitious economy and data sciences, Chinese academy of sciences, Beijing 100190, China
b

Abstract
This paper proposed a novel classification model which introduced the kernel function into the original Multiple Criteria Linear
Programming (MCLP) model. MCLP model is used as a classification method which can only solve linear separable problems in
data mining. However, the proposed kernel-based MCLP model can deal with non-linear cases. Meanwhile, unlike some other
complicated models, this model is effective and easy to understand. A couple of experimental tests were conducted to evaluate
the performance of the proposed kernel-based MCLP model compared with the existing methods: original MCLP and SVM. The
results show that kernel-based MCLP model is a competitive method in dealing with nonlinear separable classification problems.
c 2012 Published by Elsevier Ltd. Open access under CC BY-NC-ND license.
⃝
Keywords: MCLP, SVM, classification, kernel function;

1. Introduction
Classification is one of the most important data mining tasks, which is to find a set of models that describe and
distinguish data classes. Classification algorithms use the sample data from a given classified data set to learn
functions or models that map each item of the selected data into one of the predefined classes. The decision function
or model is then used to predict the class label of other unclassified data. This classification procedure usually
includes three steps: the first step is to build a classification model with one certain classification method based on
training data with predetermined data classes, the second step is to test the model on a given test set which can
validate the effectiveness of the model, and the third is to use the model on new data for future classification tasks.
This process is clearly described in Fig. 1.
Classification can be applied in many areas, such as customer classification for targeted marketing, credit
evaluation for safety loan, investment risk analysis and medical clinic decision, etc. As an important problem in
research and practice, classification has attracted interests from many fields. As a result, various of methods have
been presented to solve this problem. Traditional statistical methods, including linear discriminate function, the
quadratic discriminate function and the logistic discriminate function, have been generally accepted by industries.

* Corresponding author. Tel.: 0086-10-82680697; fax: 0086-10-82680698.
E-mail address: zhangdl365@sina.com, tianyingjie1213@163.com.

c 2012 Published by Elsevier Ltd. Open access under CC BY-NC-ND license.
1877-0509 ⃝
doi:10.1016/j.procs.2010.04.271

2408

Z. Zhang
et al.
/ Procedia
Computer
Science
(2012)000–000
2407–2415
Author
name
/ Procedia
Computer
Science
001(2010)

Recently in data mining and machine learning field, decision trees, fuzzy logic, neural networks, Bayesian and
genetic algorithm are the widely used methods for classification.
class label

Ĺ
New data

classification
model

classification
method

model testing
ĸ

Training data

0
1
0

……

ķ

1

Test data

Sample Data

Fig. 1. Three classification steps

In recent years, optimization based techniques have been widely used in many classification problems. In [1] and
[2] multi-objective linear programming model and quadratic programming model are respectively used for credit
card holders’ behavior analysis. [3] utilizes multiple criteria and multiple constraint levels linear programming
(MC2LP) model with changeable parameters in order to find dynamic best solutions for capital investment to make
"taking loss at the ordering time and making profit at the time of delivery" feasible. Support vector machine, which
is also based on optimization method, is often used in face detection [4, 5] and text mining application [6] in recent
researches. Of all these optimization-based data mining methods, support vector machine (SVM) and multiple
criteria linear programming (MCLP), have attracted data mining researchers’ interests recently. The SVM algorithm,
which is based on mathematical programming, is originally established by Vapnik [7]. He used a technique known
as the "kernel trick" to apply linear classification techniques to non-linear classification problems. By now, it has
become a popular classification tool for high-dimensional data. MCLP is also an optimization based classification
method [8]. The main feature of this method is that it has multiple objectives in one model. Two commonly used
criteria in linear programming for classification are the minimizing of overlapping degree with respect to the
discriminated boundary and the maximizing of the sum of the distance of all points to the discriminate boundary.
But because these two criteria are contradictory to each other, they can not be optimized simultaneously. In order to
combine the two criteria in one model, Shi et al. [9] initiated the compromise solution approach, which is later
widely used in many practical problems [1, 10 and 11]. Due to its linear feature, MCLP has the extraordinary
advantages in the computational speed. But, when facing the non-linear classification problems, it will not do as
well as the linear ones. In this paper, based on the kernel theory and SVM method, we introduced the kernel
function into the original MCLP classification model and proposed a kernel based MCLP model to solve non-linear
problems.
This paper is organized as follows. We will start from giving a brief review of two-class MCLP classification
model in section 2. Then section 3 introduces kernel method and the proposed kernel-based MCLP method which is
the main part of the paper. Section 4 tests the new model by a small sample with 90 cases. Section 5 shows a series
of experimental results of the comparison with the existing methods: MCLP and SVM. The last section gives the
conclusions.

2409

Z. Zhang
al. / Procedia
Computer
Science
1 (2012)
2407–2415
Authoretname
/ Procedia
Computer
Science
00 (2010)
000–000

2. Two-Class Multiple Criteria Linear Program Classifier
The formulation of classification problem can be described as follows:
Suppose the training set of the classification problem is XnX r, which has n observations in it. Of each observation,
there are r attributes (or variables) which can be any real value and a two-value class label G (Good) or B (Bad). Of
the training set, the ith observation can be described by Xi = (Xi1, . . . , Xir), where i can be any number from 1 to n.
The objective of the classification problem is to learn knowledge from the training set that can classify these two
classes, so that when given an unclassified sample z = (z1, . . . , zr ), we can predict its class label with the
knowledge.
Multiple criteria linear programming (MCLP) can be used as a classification method. The framework of MCLP is
based on the linear discriminate analysis models. In linear discriminate analysis, the purpose is to determine the
optimal coefficients (or weights) for the attributes, denoted by W = (w1, . . ., wr) and a boundary value (scalar) b to
separate two predetermined classes: G (Good) and B (Bad); that is

X i 1w1
and X i 1w1

X ir wr d b, X i  B (Bad )
(1)

X ir wr t b, X i  G (Good )

To formulate the criteria and constraints for data separation, some variables need to be introduced. Fig. 2
demonstrates the basic theory of MCLP. In the classification problem, X i w X i 1w1
X ir wr is the score
for the ith observation. If all records are linear separable and a sample Xi is correctly classified, then let E i be the
distance from Xi to b, and consider the linear system, X i w b Ei , X i  G and X i w b Ei , X i  B .
However, if we consider the case where the two groups are not linear separable because of mislabeled records, a
“soft margin” and slack distance variable D i need to be introduced. D i is defined to be the overlapping of the twoclass boundary for mislabeled case Xi . Previous equations now can be transformed to X i w b Di Ei ,
X i  G and X i w b Di Ei , X i  B . To complete the definitions of E i and D i , let Ei 0 for all
misclassified samples and Di 0 for all correctly classified samples.
X iW ÿ = b
a

ßi

ai

a

ßi

ai

G ood

B ad

X i W ÿ = b-a

X iW ÿ = b +a

Fig. 2. Overlapping of two-class Linear Discriminate Analysis

A key idea in linear discriminate classification is that the misclassification of data can be reduced by using two
objectives in a linear system. One is to maximize the sum of the distance of all points to the discriminate boundary
( Maximize b1 + + b n ) and another is to minimize the overlapping degree ( Minimize a 1 + + a n ). The two
criteria are contradictory to each other, so normally there are two different linear discriminate models MMD and
MSD for classification [12].

2410

Z. Zhang
et al.
/ Procedia
Computer
Science
(2012)000–000
2407–2415
Author
name
/ Procedia
Computer
Science
001(2010)

MCLP combines these two criteria into one single model. It aims at minimizing the sum of D i and maximizing
the sum of Ei simultaneously, which forms the two-class MCLP model.
Two-Class MCLP model: [12]

Minimize D1

Dn

Maximize E1

and

En

Subject t o :
X 11w1

X 1r wr

b

X n 1w1

X nr wr

b

D1 ,

, D n t 0,

E1 ,

D1

E1 , for X 1  B

Dn

En , for X n  G

(2)

, En t 0

To facilitate the computation, the compromise solution approach has been employed to modify the above model
and is finally applied to many practical problems. The compromise solution approach of two-class MCLP model[12]
is:

Minimize d α

dα

dβ

dβ

Subject to:
n

D

¦D

i

dα

dα

dβ

dβ

i 1

(3)

n

E

¦E

i

i 1

X 11w1

X 1r wr

b D1

X n1w1

X nr wr

b Dn

D1 ,

, D n t 0, E1 ,

E1 ,

for X 1  B

E n , for X n  G

, E n t 0, d α ,d α ,d β ,d β t 0

Here D*, E* and XnX r are given, dD+, dD -, dE+, dE -, D i , E i , w and b are unrestricted. Solving this linear
programming problem, we can get the best solution of w and b, denoted by (w*, b*). And the boundary plane of the
two classes is:

xw

b

(4)

where x has r attributes, w*=(w1, . . . , wr)T represents the weight for the attributes and b* is a real number.
3. Kernel-Based MCLP Classifier
MCLP is a linear programming model and is only applicable for linear separable problem. When faced with
nonlinear separable data set, it may have bad performance. However, as is commonly known, kernel function is a
powerful tool to deal with nonlinear separable data set. By projecting the data into a high dimensional feature space,
the data set will become more likely linear separable. So, if we can employ kernel method in MCLP classifier, it
might become a nonlinear classifier.
Here, we will briefly introduce the kernel method at first. Any detail about it can be found in [7].
Suppose the training set of the nonlinear classification problem is XnX r, which has n observations and r attributes
in it. Each observation belongs to one of the class, labeled G (Good) or B (Bad). Because this data set is nonlinear
separable, we need to map it into a higher dimensional space with function Ф so that it becomes linear separable in

Z. Zhang
al. / Procedia
Computer
Science
1 (2012)
2407–2415
Authoretname
/ Procedia
Computer
Science
00 (2010)
000–000

2411

the new space. Then with some linear classification model, i.e. MCLP, we can get the separation function to classify
the data in the new space.
But, as is commonly known, the mapping function Ф is always implicit. If the input data have many attributes, it
is hard to perform such mapping operations. Kernel function offers an alternative way to this problem. It is defined
to be such a direct computation method.
Definition of kernel function: A kernel is a function K, such that for all x1, x2ęX, K(x1, x2) = Ф(x1gФ(x2), where
Ф is a mapping from X to a new feature space. x1, x2 are two samples from X.
Although Ф is always implicit, K is explicit. We can calculate the kernel function directly in some algorithm
instead of compute function Ф of each sample separately. Some commonly used kernel functions are polynomial
kernels, Gaussian RBF kernels and Mercer kernels. Here, RBF kernel is used in the experiments:
2

k ( x1 , x2 ) = exp(- g || x1 - x2 || ).
In data mining field, SVM is the best known and the first to use kernel to construct a nonlinear estimation
algorithm. In addition, kernel algorithm is very useful in a number of approaches and fields.
In MCLP model (3), there is no way to use kernel function directly because the training samples are not included
in the inner product formulation. Some changes need to be done on this model. Actually, in linear programming
problem, given input data set X, the solution of w must be driven from these X, which makes w to be the linear
combination of xi. Assume that the solution of MCLP model can be described in the following form
n

¦O y x ,

w

i

(5)

i i

i 1

and put the

w into model (3), we can get the following variation of MCLP model:
d α+ + d α- + d β+ + d β-

Minimize
Subject to:

n
*

a +

å

-

+

a i = dα - dα

i= 1
n

å

*

b -

-

(6)

+

bi = dβ - dβ

i= 1

ǂl 1 y1 ( x1 ×x1 ) + l 2 y2 ( x2 ×x1 ) +

+ l n yn ( xn ×x1 ) = b + a 1 - b1ˈǂfor x1 Î B ,

ǂǂǂǂǂǂǂˊˊˊˊˊˊ
ǂǂ l 1 y1 ( x1 ×xn ) + l 2 y2 ( x2 ×xn ) +
ǂ ǂ a i ³ 0, ǂǂǂǂi = 1, ..., n;
ǂ ǂ b i ³ 0, ǂǂǂǂi = 1, ..., n;
+

-

+

-

d α ,d α ,d β ,d β ³ 0

+ l n yn ( xn ×xn ) = b - a n + b nˈǂfor xn Î G , ǂǂ

2412

Z. Zhang
et al.
/ Procedia
Computer
Science
(2012)000–000
2407–2415
Author
name
/ Procedia
Computer
Science
001(2010)

Consider the same nonlinear classification problem with training set XnX r, when each sample xi is mapped into a
higher dimensional space with function Ф, the new training set will be Xnew = (Ф(x1), . . . , Ф(xn))T, where Ф(xi)
represents each new training sample in the new space. This data set will probably be linear separable. Input this Xnew
into model (6), we will get the separation function of the two classes in the new space. The model (6) for Xnew is:

d α+ + d α- + d β+ + d β-

Minimize
Subject to:

n

å

*

a +

-

+

a i = dα - dα

i= 1
n

å

*

b -

-

+

bi = dβ - dβ

(7)

i= 1

ǂl 1 y1F ( x1 ) ×F ( x1 ) + l 2 y2F ( x2 ) ×F ( x1 ) +

+ l n ynF ( xn ) ×F ( x1 ) = b + a 1 - b1ˈǂfor x1 Î B,

ǂǂǂǂǂǂǂˊˊˊˊˊˊ
ǂǂ l 1 y1F ( x1 ) ×F ( xn ) + l 2 y2F ( x2 ) ×F ( xn ) +

+ l n ynF ( xn ) ×F ( xn ) = b - a n + b nˈǂfor xn Î G , ǂǂ

ǂ ǂ a i ³ 0, ǂǂǂǂi = 1, ..., n;
ǂ ǂ b i ³ 0, ǂǂǂǂi = 1, ..., n;
+

-

+

-

d α ,d α ,d β ,d β ³ 0

Replace the (Ф(xi)·Ф(xj)) with K(xi, xj) in the above model, we have this kernel-based MCLP model:
d α+ + d α- + d β+ + d β-

Minimize
Subject to:

n
*

a +

å

-

+

a i = dα - dα

i= 1
n

å

*

b -

-

+

bi = dβ - dβ

(8)

i= 1

ǂl 1 y1 K ( x1 , x1 ) + l 2 y2 K ( x2 , x1 ) +

+ l n yn K ( xn , x1 ) = b + a 1 - b1ˈǂfor x1 Î B ,

ǂǂǂǂǂǂǂˊˊˊˊˊˊ
ǂǂ l 1 y1 K ( x1 , xn ) + l 2 y2 K ( x2 , xn ) +

+ l n yn K ( xn , xn ) = b - a n + b nˈǂfor xn Î G , ǂǂ

ǂ ǂ a i ³ 0, ǂǂǂǂi = 1, ..., n;
ǂ ǂ b i ³ 0, ǂǂǂǂi = 1, ..., n;
+

-

+

-

d α ,d α ,d β ,d β ³ 0

The kernel function in the above model can be any acceptable formulation. Actually, model (6) is the formulation
of linear kernel which only applicable for linear separable problem. Here in this paper, we use RBF kernel to solve
nonlinear problem. By this way, we incorporate kernel function into a linear programming model, which make it
applicable in nonlinear separable problem. In this kernel-based MCLP model, D*, E* and xi, yi are given, dD+, dD -,
dE+, dE -, D i , E i , λ and b are unrestricted. Solving this linear programming problem, we can get the best solution of λ
and b, denoted by (λ*, b*). And the boundary plane of the two classes is:
n

¦O

i

i 1

yi K ( xi , x) b

(9)

Z. Zhang
al. / Procedia
Computer
Science
1 (2012)
2407–2415
Authoretname
/ Procedia
Computer
Science
00 (2010)
000–000

2413

where xi represents each input sample. And the corresponding decision function is:
n

y = f(x) = sgn( (

¦O

i

yi K ( xi , x)) b )

(10)

i 1

4. Sample Testing
In order to test the validity of the proposed kernel-based MCLP model (8), we conducted an experiment on a
small data set with 90 records which is a nonlinear separable set. In the following figures, the cross points and
diamond points belong to two different classes respectively.

Fig. 3. (a) Original data set; (b) Result with original MCLP

Fig. 4. (a) Result with linear kernel MCLP; (b) Result with RBF kernel MCLP.

Fig. 3(a) shows the original data set, in which two classes appear to be easily separated from each other. Fig. 3(b)
shows the classification result of MCLP model (3). Because MCLP is a linear model, it can only generate a straight
line to classify the two classes which leads to the result in Fig. 3(b) that the data at the bottom-left belong to one
class and the upper-right to the other. Fig. 4(a) describes the classification result by linear kernel MCLP, which is
equivalent to model (6). We can see Fig. 4(a) has similar result with Fig. 3(b), which also proves that MCLP model
(3) and linear kernel model (6) is identical. It is obvious from the results that MCLP and linear kernel MCLP model
can not properly solve this nonlinear problem. But if we use RBF kernel in kernel-based MCLP model (8), the
classification result will be perfect, and the precision will be 100% in this problem, see Fig. 4(b).
This test also shows that with an appropriate kernel function, the proposed kernel-based MCLP model (8) can
effectively solve the nonlinear separable problem.

2414

Z. Zhang
et al.
/ Procedia
Computer
Science
(2012)000–000
2407–2415
Author
name
/ Procedia
Computer
Science
001(2010)

5. Experimental Results
In this section, we will use two public classification data sets Splice and Breast Cancer data sets in UCI
repository [13] to compare the performance of kernel-based MCLP with the other two methods: SVM and original
MCLP. Here we only use the linear kernel and RBF kernel on SVM and MCLP.
Before doing the classification, we perform some data preprocessing: scale each feature to [0, 1] and split each
data set into 10 groups. We will use 10 cross-validation to estimate the accuracy of prediction. Every time we
choose 9 data sets together to be the training set, and the one leftover is the test set. For every data set we can get 10
accuracies and at last the average accuracy is the whole accuracy, shown in table 1.
In every training process, parameters in every algorithm are chosen in some discrete set in order to get the best
accuracy. We select the parameter J from [0.001 0.01 0.1 1 16 128]. The best J of certain algorithm on a given
training set is chosen according to the best accuracy, which is also the same value used for prediction on the test set.
Table 1. Accuracy with different algorithm

linear

RBF

kernel

kernel

SVM

SVM

splice

72.70%

88.50%

Breast Cancer

93%

93%

Accuracy

linear

RBF

kernel

kernel

MCLP

MCLP

79.60%

80.90%

98%

88.50%

90.50%

96.50%

MCLP

For each data set, we draw a broken line chart to show the results of the five different algorithms as follows:
Splice





OLQHDU
NHUQHO
690

5%)
NHUQHO
690

0&/3

OLQHDU
NHUQHO
0&/3

5%)
NHUQHO
0&/3

Fig. 5. Accuracy on data set Splice.

Breast Cancer




OLQHDU
NHUQHO
690

Fig. 6. Accuracy on data set Breast cancer

5%)
NHUQHO
690

0&/3

OLQHDU
NHUQHO
0&/3

5%)
NHUQHO
0&/3

Z. Zhang
al. / Procedia
Computer
Science
1 (2012)
2407–2415
Authoretname
/ Procedia
Computer
Science
00 (2010)
000–000

2415

From the above figures, we can see that for each data set, RBF kernel MCLP plays much better than the other
algorithms especially original MCLP and linear kernel SVM. This also shows that kernel-based MCLP is a
competitive method in classification.
6. Conclusions
In this paper, we introduced the kernel function into Multiple Criteria Linear Program (MCLP), which allows the
model to deal with not only linear separable problems, but also nonlinear ones. Our model is simple in its
formulation and effective in terms of performance in comparison with other algorithm such as SVM. And the
experimental results on some benchmark classification data sets show that the kernel-based MCLP model performs
even better than SVM classifier.

Acknowledgements
This research has been partially supported by grants from National Natural Science Foundation of China
(#70921061, #10601064, #70531040, #70621001).

References
1. G. Kou, Y. Peng, Y. Shi, M. Wise and W. Xu, Discovering Credit Cardholders’ Behavior by Multiple Criteria Linear Programming.
Annals of Operations Research, Vol. 135, No.1 (2005), 261-274.
2. K.-J. Tseng, Y.-H. Liu and J. –F. HO, An Efficient Algorithm for Solving a Auadratic Programming Model with Application in Credit
Card Holders’ Behavior, International Journal of Information Technology & Decision Making, Vol.7, No. 3 (2008), 421-430.
3. C. Y. Chianglin, T. C. Lai and P. L. Yu, Linear Programming Models with Changeable Parameters — Theoretical Analysis on “Taking
Loss at The Ordering Time and Making Profit at the Delivery Time”, International Journal of Information Technology & Decision Making, Vol.6,
No. 4 (2007), 577-598.
4. C.A. Waring and X. Liu, Face Detection Using Spectral Histograms and SVMs, IEEE transactions on systems, man, and cybernetics. Part
B, Cybernetics, Vol.35, No.3 (2005), 467-476.
5. B. Heiseley, T. Serre, M. Pontil and T. Poggio, Component-based Face Detection, IEEE Computer Society Conference on Computer
Vision and Pattern Recognition (CVPR'01), Vol. 1(2001), 657-662
6. C.-H. Lee and H.-C. Yang, A Classifier-based Text Mining Approach for Evaluating Semantic Relatedness Using Support Vector
Machines, Proceedings of the International Conference on Information Technology: Coding and Computing (ITCC'05), Vol. 1 (2005), 128-133
7. V. N. Vapnik, The Nature of Statistical Learning Theory, Springer, New York, 2000.
8. Y. Shi, Multiple Criteria and Multiple Constraint Level Linear Programming: Concepts, Techniques and Applications, World Scientific
Publishing Co, 2001.
9. Y. Shi, M. Wise, M. Luo, and Y. Lin, Data Mining in Credit Card Portfolio Management: A Multiple Criteria Decision Making Approach,
Advance in Multiple Criteria Decision Making in the New Millennium, Springer, Berlin. 2001.
10. G. Kou, Y. Peng, N. Yan, Y. Shi, Z. Chen, Q. Zhu, J. Huff and S. McCartney, Network Intrusion Detection by Using Multiple-Criteria
Linear Programming. 2004 International Conference on Service Systems and Service Management, Beijing, 2004.
11. G. Kou, Y. Peng, Y. Shi and Z. Chen, A New Multi-Criteria Convex Quadratic Programming Model for Credit Analysis. Alexandrov, V.
N. et al. (eds.): ICCS 2006, LNCS 3994, Springer, Berlin (2006), 476-484.
12. D. Olson and Y. Shi, Introduction to Business Data Mining. McGraw-Hill. Irwin, 2007.
13. UCI Machine Learning Repository, http://www.ics.uci.edu/~mlearn/MLRepository.html

