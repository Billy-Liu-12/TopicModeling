DeWiz - Modular Debugging for
Supercomputers and Computational Grids
Dieter Kranzlmüller
GUP Linz, Johannes Kepler University Linz,
Altenbergerstr. 69, A-4040 Linz, Austria/Europe,
kranzlmueller@gup.uni-linz.ac.at,
http://www.gup.uni-linz.ac.at/

Abstract. Debugging is accepted as one of the diﬃcult tasks of high
performance software development, which can be attributed to the high
complexity of parallel and distributed applications. Especially users of
massively parallel supercomputers or distributed metacomputer systems
experience huge obstacles, that are diﬃcult if not impossible to overcome with existing error detection approaches. The prototype tool DeWiz
presents an eﬀort to improve this situation by applying the abstract event
graph model as a representation of parallel program behavior. Besides
its usability for diﬀerent programming paradigms it permits analysis of
data with various debugging activities like automatic error detection and
sophisticated abstraction. In addition, DeWiz is implemented as a set of
loosely connected modules, that can be assembled according to the user’s
needs and given priorities. Yet, it is not intended as a complete replacement but as a plug-in for well-established, existing tools, which may
utilize it to increase their debugging functionality.

1

Introduction

The quest for ever increased computational performance seems to be a neverending story mainly driven by so-called ”grand challenge” problems of science
and engineering, like simulations of complex systems such as weather and climate, ﬂuid dynamics, and biological, chemical, and nuclear reactions. Since existing computing systems allow only insuﬃcient calculations and restricted solutions in terms of processing speed, required memory size, and achieved numerical precision, new architectures and approaches are being developed to shift
the performance barrier. The upper limit of this development is represented
by supercomputer systems, which may be further coupled to metacomputers.
Recent examples of realization projects are the US Accelerated Strategic Computing Initiative (ASCI) [6] and the multi-institutional Globus project [4]. While
ASCI seeks to enable Teraﬂop computing systems far beyond the current level
of provided performance, the Globus project tries to enable computational grids
that provide pervasive, dependable, and consistent access to distributed highperformance computational resources.
Such systems achieve their level of performance due to their high degree of
powerful parallel and distributed computing components. Yet, this complicates
V.N. Alexandrov et al. (Eds.): ICCS 2001, LNCS 2074, pp. 811–820, 2001.
c Springer-Verlag Berlin Heidelberg 2001
�

812

D. Kranzlmüller

the software development task due to the required coordination of multiple,
concurrently executing and communicating processes. As a consequence, big obstacles are experienced during all phases of the software lifecycle, which initiated
many research eﬀorts to improve the parallel programmers situation with manifold strategies and development tools.
One area of investigation is testing and debugging, which shares a great part
in determining the reliability of the application and thus the quality of the software. The goal of debugging is to detect faulty behavior and incorrect results
occurring during program execution, which is attempted by analyzing a program
run and investigating process states and state changes. Obviously, the complexity of the program and the number of interesting state changes determines the
amount of work needed to analyze the program’s behavior. This means, that
bigger systems are probably more diﬃcult to debug than smaller systems. Yet,
there are only a limited number of parallel debuggers, that are suitable for error
detection of applications running on massively parallel and distributed systems.
The biggest problem is that current debugging tools suﬀer from managing the
amount of presented data, which stems from mainly two characteristics: Firstly,
most parallel debugging tools are composed from combing several sequential
tools and integrating them under a common user interface. These tools often
lack support for detecting errors derived from parallelism. Secondly, many tools
are based on textual representations, which may be inappropriate in many cases
to display and manage the inherent complexity of parallel programs [13].
The work described in this paper diﬀers from existing approaches due to the
fact, that debugging activities are based on the event graph model instead of the
underlying source code. It describes a parallel program’s execution by occurring
state changes and their interactions on concurrently executing processes, which
allows to cope equally with programs based on message-passing and the shared
memory paradigm. Furthermore, it can be applied for automatic error detection
and to perform higher-level program abstraction.
These ideas are implemented in the tool prototype DeWiz, the Debugging
Wizard. In contrast to other tools, DeWiz does not contain a user interface for
the analysis task, but instead oﬀers its results to other existing debugging tools.
By providing an adaptable input interface, traces from post-mortem debuggers
and event-streams from on-line debuggers can be processed. Similarly, an output interface allows to use DeWiz as a plug-in for the user’s preferred analysis
tool. Another feature is its steering module, that oﬀers a way for the user to
describe the desired analysis more precisely. This covers the idea, that the user
has knowledge about the program’s expected behavior and may thus be able to
identify diﬀerent priorities for more or less critical errors and analysis tasks.
This paper is organized as follows. The next section discusses the target
systems for our debugging model and the requirements imposed onto parallel
debugging tools. Afterwards, the event graph model is introduced and some
possibilities for analyzing a program’s behavior are presented. This leads to the

DeWiz - Modular Debugging for Supercomputers and Computational Grids

813

actual implementation of DeWiz in Section 4, which is described by its main
features and its mode of operation.

2

Requirements to a Parallel Debugger

The main criterion for any tool developer is a deﬁnition of target systems, which
in our case are high performance supercomputers. Since there exist diﬀerent
architectures and possibilities of programming them, a wide variety of strategies and tools have already been proposed. In the case of parallel debuggers
many promising approaches exist as academic or research prototypes, for example Mantis [11], P2D2 [5], PDBG [2], and PDT [1], or even as a commercial
tool like Totalview [3]. A characteristic of these systems is, that every approach
applies several instances of an existing sequential debugger in order to perform
the debugging task on the participating processes. Although this may be useful
in most cases, it introduces some obstacles especially on large scale computing
systems like massively parallel machines or heterogenous clusters of supercomputers.
The problems experienced during debugging of supercomputer applications
are mostly connected to the amount of data that has to be analyzed. Firstly,
these programs tend to be long-lasting, from some hours to several days or even
more. As a consequence, many state changes occur that have to be observed and
processed by the debugging tool. In the worst case, debugging a supercomputer
may require another supercomputer to perform the analysis task. Secondly, the
execution time and the large number of participating processes leads to enormous
interprocess relations, which cannot be comprehended by the user. Thirdly, a
great amount of debugging activities has to be performed equally for diﬀerent
processes and repeated iterations of the target application.
Previous solutions were always based on down-scaling, which means that
the program’s size is reduced in terms of participating processes and executed
numbers of iterations. While this may be successful in many cases, it also contains
potential for critical errors, which may be experienced only in full-scale real-world
applications. As a consequence, there may be some cases where the program has
to be tested under regular conditions in its intended environment. In order to
comply to this requirement, we have identiﬁed the following characteristics for
a debugger of the above mentioned target systems:
– Heterogeneity: supporting diﬀerent programming models and architectures.
– Usability: managing huge amounts of data and improving program understanding.
– Abstraction: reducing the amount of debugging data presented to the user.
– Automatization: performing repeated debugging activities without user interaction.
A strategy or tool supporting these characteristics may then be applicable to
full-scale applications, and may allow to perform debugging activities impossible
with existing solutions. In addition, it may also improve the error detection task
on smaller-scale application sizes, especially if it can be combined with other
tools in this area.

814

3

D. Kranzlmüller

The Event Graph Model for Debugging

Our solution to fulﬁl the characteristics described above are based on the event
graph model. An event graph is a directed graph G = (E, →), where E is the nonempty set of vertices e of G , while → is a relation connecting vertices, such that
x → y means that there is an edge from vertex x to vertex y in G . The vertices
e are the events occurring during program execution, which change the state of
the corresponding process [14]. The relation establishes Lamport’s ”happened
before” ordering [10], which consists of the sequential order on one particular
process and the order between disjunct processes whenever communication or
synchronization takes place.
In principle, every change of process state is caused by an event, and there
are huge numbers of events being generated during program execution. However,
usually only a small subset of events is required during debugging, which allows
to ﬁlter only interesting events for error detection and to reduce the number of
state changes for investigation. The remaining events are then collected as the
vertices of the event graph. One diﬃculty is to deﬁne, which events to collect and
which state changes to ignore. In order to allow a large degree of ﬂexibility, the
events collected in the event graph are user-deﬁned. For example, a user looking
for communication errors may deﬁne point-to-point communication events as
established by send and receive function calls to be the target of investigation.
During program analysis, it is not only important to know about the occurrence of an event, but also about its properties. These properties are called
event attributes and represent everything that may be interesting for the investigator. Similar to the events, the set of attributes may be appointed by the
user. For the above mentioned communication points, a user may identify the
communication statement’s parameters to be event attributes for the analysis.
Another kind of attributes are source code pointers, which consist of ﬁlename
and line number corresponding to the original function call or statement. These
attributes are needed in order to establish a connection between the graph and
the faulty source code.
With the event graph model deﬁned as above it is possible to describe erroneous behavior. In principle, every error theory deﬁnes two groups of bugs in
programs, failures and computational errors. While the former is clearly recognizable, e.g. through program break-downs or exceptions being taken, the latter
always depends on the semantic contents of the results and requires a veriﬁcation step. Thus, computational errors can only be detected by comparison of
expected results with actually obtained results.
Integrating failure detection in the event graph model is relatively easy, since
their occurrence usually leads to an end of the corresponding process. Thus, a
failure is always the last event on a particular process, which is characterized by
having only one approaching edge but no leaving edge. Therefore, a debugging
tool can easily direct the programmer’s attention to such places by analyzing
the ﬁnal event on each participating process.
On the other hand, computational errors may occur at both, edges and vertices of the event graph. Since the edges describe a set of state changes, and these

DeWiz - Modular Debugging for Supercomputers and Computational Grids

815

isolated receive event
isolated send event
different message length at sender and receiver

Fig. 1. Basic event graph analysis (e.g. errors in point-to-point communication)

state changes determine the results of the program, analyzing the edges may be
required in order to detect incorrect behavior. Errors at vertices can be identiﬁed by describing the expected event attributes. If the event attributes obtained
during execution diﬀer from the expected data, incorrect or illegal operations
have been detected. Please note, that incorrect operations may not necessarily
result in errors, e.g. when the program is prepared to handle such unexpected
events.
For instance, comparing expected and actual attributes of the communication
events may expose isolated events or events with diﬀerent message length. Isolated events are send events without corresponding receive events or vice versa.
Events with diﬀerent message length are revealed, if the size of the message data
diﬀers at sender and receiver. However, even if events with these characteristics
are detected, they need not necessarily result in malign behavior. For example,
isolated send events may have no eﬀect, while isolated receive events may block
the processes’ execution forever.
An example of these basic debugging features is visible in Figure 1. It shows
the execution of a program on 8 selected nodes from a possibly much larger
execution. Some of the edges in the graph are highlighted to emphasize errors in
the communication parameters of corresponding send and receive function calls.
Besides the basic analysis capabilities of checking event attributes, a more
sophisticated way of analysis considers the shape of the graph itself. Often, a
set of corresponding send and receive events resembles more or less complex

816

D. Kranzlmüller

Fig. 2. Advanced event graph analysis (e.g. anomalous event graph pattern)

communication patterns, like broadcast, scatter, gather, and butterﬂy. Other
possibilities are iterations in the observed algorithm, repeated function calls to
selected communication statements, and grouping of processes (see [7] for an
overview of some examples). These characteristics can be detected with simple
pattern matching algorithms. As a result, this analysis allows to detect complete
patterns, nearly complete patterns, and the absence of expected patterns in the
event graph.
An example for communication pattern analysis is visible in Figure 2. This
shows a ﬁnite element solver that was executed on 16 nodes. There have been 200
iterations to perform the computation, and only point-to-point communication
events have been traced. In total, the trace contained around 20.000 events.
Therefore, it is possible that the strange behavior during one of the operations
could have remained unnoticed, especially if a smaller scale than seconds would
have been selected. With pattern matching, this strange behavior would have
been detected immediately. Please note, that this is a real example and we
detected this error only by accident, before we developed this strategy.
The next step after detecting anomalous behavior is to direct the users attention to such places in the event graph. This is called automatic abstraction
and means, that only a limited surrounding of the erroneous events is extracted.
Therefore, instead of presenting all the data to the user, only critical sections of
the event graph are displayed. A simpliﬁed operation of automatic abstraction

DeWiz - Modular Debugging for Supercomputers and Computational Grids

817

is to evaluate the history of events that ﬁnally resulted in the detected bug.
Therefore, it may only be necessary to display the communication partners of
the corresponding process, and this only for a certain interval of time. As a consequence, in contrast to displaying huge numbers of events and several hundred
processes, only a small subset is presented to the user, which still contains the
interesting places for the debugging task.

4

DeWiz, the Debugging Wizard Prototype Tool

The event graph model as described above together with a preliminary set of
error detection functions has been implemented in a tool prototype called DeWiz.
Besides this functionality, several aspects have been applied during the design
of DeWiz, which are as follows:
– Modularity: the debugging functionality must be adaptable to the users
needs and the applications characteristics.
– Independence: the features of DeWiz must be applicable without a deﬁned
graphical user interface in mind, but instead as a plug-in to available debugging tools.
– Eﬃciency: due to the prospected amount of data, the tool must be implemented in order to facilitate all available performance by executing modules
in parallel and applying parallelized algorithms during the analysis (e.g. pattern matching).
In addition, DeWiz contains a steering module, that allows to integrate the
users knowledge into the debugging task. Within the proposed strategy it is able
to allow the users to deﬁne
– events interesting for monitoring,
– expected behavior in terms of communication patterns, and
– priority between diﬀerent kinds of errors
These aspects are available in a ﬁrst tool prototype, whose operation during
program analysis is displayed in Figure 3. The starting point is a target system,
that is observed by an available monitoring tool. This monitoring tool is connected to DeWiz either on-line via event streams or post-mortem via traceﬁles.
In order to combine the tool with an available monitoring utility, the input interface has been adapted to process the given event data. This input interface
forwards the data to the modules of DeWiz, which perform the desired analysis.
At this point, the users knowledge interferes with the systems operation. A
dedicated steering module allows to decide about the expected behavior and
the priority of diﬀerent errors. The user with the knowledge about the target
system, called the debugging expert, enters this data via a conﬁguration ﬁle.
In the future, we will change this propriety form of steering with some kind
of graphical user interface. The conﬁguration given by the user determines the

818

D. Kranzlmüller

Monitor

On−line

Input
Interface

Post−mortem
Target System

Tracefiles
Filters

Debugging
Expert

Steering
Module

Abstractors
Detectors
Validators

Analysis
Tool A
Debugging
Users

Analysis
Tool B

Output
Interface

Fig. 3. Flowdiagram of DeWiz during opertation

arrangement of the working modules. Clearly, modules evaluating higher priority
errors are executed earlier during the analysis task. In addition, the distribution
of the modules to available processing elements can be deﬁned in order to improve
the analysis speed.
After the analysis has be completed, or critical errors have been detected,
the results are forwarded to the output interface. Again, at this end of the tool,
some customizations have to be carried out to provide the results of DeWiz to
other existing analysis tools. Of course, since the results are delivered as event
streams, any possible connection as well as concurrent connections to diﬀerent
tools are imaginable.
At present, the current prototype of DeWiz has been connected to the MAD
environment [8], which has been developed at our department. Besides others,
MAD contains an event monitoring utility that performs the program observation of MPI programs [11], and several analysis tools that visualize the evaluated
analysis data. One of these analysis tools is ATEMPT, which visualizes a statetime diagram of the programs execution. The results of DeWiz are therefore
mapped onto the display of ATEMPT, and allow to remove all analysis functionality from the graphical user interface. In addition, it is much easier to extend

DeWiz - Modular Debugging for Supercomputers and Computational Grids

819

the modular approach of DeWiz, and as a side eﬀect provide these extensions to
other available debugging environments.

5

Conclusions and Future Work

The work described in this paper tries to provide a solution for the diﬃculties
encountered during debugging of massively parallel supercomputers as well as
distributed metacomputers. With the event graph model it is relatively easy to
distinguish correct and incorrect behavior, to provide some means of abstraction
and to perform tedious analysis tasks automatically.
The presented ideas have already been implemented in the DeWiz tool prototype, which has been successfully connected to an existing debugging environment. In addition, the modular composition of DeWiz allows to easily extend
and improve its functionality, which is one of the current goals in this project.
By providing more analysis features, the capabilities of DeWiz can be increased
with beneﬁts for the debugging user. It is even imaginable to develop a component library of parallel programming debugging modules, that can arbitrarily
arranged by the users.
Another aspect experienced during our research is, that many analysis tasks
are really time-consuming, especially with the large amounts of data to be processed. Thus, it seems necessary to further optimize and parallelize some of the
existing modules in order to speed up the execution.
A near future goal of this project is the extension of the input and output
interfaces. So far, we have only processed post-mortem traceﬁles, and one of
the next goals is to integrate an interface based on the OMIS monitoring standard [16]. This will allow us more ﬂexibility in choosing the input connection
and processing diﬀerent event streams, and will probably deliver useful feedback
for additional analysis modules.
Acknowledgements. This work represents an extension of my PhD thesis [9] and
covers aspects that have not been described before. Consequently, many of the
ideas presented here evolved from the many discussions with my PhD supervisor,
Prof. Dr. Jens Volkert.

References
1. C. Clemencon, J. Fritscher, R. Rühl, “Visualization, Execution Control and Replay
of Massively Parallel Programs within Annai’s Debugging Tool”, Proc. High Performance Computing Symposium, HPCS’95, Montreal, Canada, pp. 393-404 (July
1995).
2. J. C. Cunha, J. Lourenço, J. Vieira, B. Moscão, and D. Pereira. , “A framework to
support parallel and distributed debugging”, Proc. of HPCN’98, High Performance
Computing and Networking Europe, Amsterdam, Netherlands, (1998).
3. Etnus (Dolphin Interconnect Solutions Inc.): TotalView 4.1.0, Documentation:
http://www.etnus.com/pub/totalview/tv4.1.0/totalview-4.1.0-doc-pdf.tar,
Framingham, Massachusetts, USA, 2000.
4. I. Foster, C. Kesselman, “The Globus Project: A Status Report”, Proc.
IPPS/SPDP’98 Heterogenous Computing Workshop, pp. 4-18 (1998).

820

D. Kranzlmüller

5. R. Hood, “The p2d2 Project: Building a Portable Distributed Debugger”, Proc.
SPDT’96, ACM SIGMETRICS Symposium on Parallel and Distributed Tools,
Philadelphia, USA, pp. 127-136 (May 1996).
6. F. Hossfeld, “Teraﬂops Computing: A Challenge to Parallel Numerics?” Proc. 4th
Intl. ACPC Conference, Springer, LNCS, Vol. 1557, Salzburg, Austria, pp. 1-12
(Feb. 1999).
7. D. Kranzlmüller, S. Grabner, J. Volkert, “Event Graph Visualization for Debugging
Large Applications”, Proc. SPDT’96, ACM SIGMETRICS Symposium on Parallel
and Distributed Tools, Philadelphia, PA, pp. 108-117 (May 1996).
8. Kranzlmüller, D., Grabner, S., Volkert, J., Debugging with the MAD Environment,
Parallel Computing, Vol. 23, No. 1–2, pp. 199–217 (Apr. 1997).
9. Kranzlmüller, D., Event Graph Analysis for Debugging Massively Parallel Programs, PhD Thesis, GUP Linz, Joh. Kepler University Linz,
http://www.gup.uni-linz.ac.at/ dk/thesis, (Sept. 2000).
10. Lamport, L., Time, Clocks, and the Ordering of Events in a Distributed System,
Communications of the ACM, pp. 558 - 565 (July 1978).
11. S.S. Lumetta, D.E. Culler, ”The Mantis Parallel Debugger”, Proc. of SPDT’96:
SIGMETRICS Symposium on Parallel and Distributed Tools, Philadelphia, PA,
pp. 118-126 (May 1996).
12. Message Passing Interface Forum, “MPI: A Message-Passing Interface Standard Version 1.1”, http://www.mcs.anl.gov/mpi/ (June 1995).
13. C.M. Pancake, “Visualization Techniques for Parallel Debugging and PerformanceTuning Tools”, in: A.Y.Zomaya, “Parallel Computing: Paradigms and Applications”, Intl. Thomson Computer Press, pp. 376-393 (1996).
14. M. van Rick, B. Tourancheau, “The Design of the General Parallel Monitoring System”, Programming Environments for Parallel Computing, IFIP, North Holland,
pp. 127-137 (1992).
15. M. Stitt, “Debugging: Creative Techniques and Tools for Software Repair”, John
Wiley & Sons, Inc., NY (1992).
16. R. Wismüller, “On-Line Monitoring Support in PVM and MPI”, Proc. EuroPVM/MPI’98, LNCS, Springer, Vol. 1497, Liverpool, UK, pp. 312-319, (Sept
1998).

