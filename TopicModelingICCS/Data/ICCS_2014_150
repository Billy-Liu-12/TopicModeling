Procedia Computer Science
Volume 29, 2014, Pages 2370–2379
ICCS 2014. 14th International Conference on Computational Science

Impact of I/O and Data Management in Ensemble Large
Scale Climate Forecasting Using EC-Earth3∗
Muhammad Asif1 , Andr´es Cencerrado2 , Oriol Mula-Valls1 , Domingo
Manubens1 , Francisco Doblas-Reyes1,3 , and Ana Cort´es2
1

2

Institut Catal`
a de Ci`encies del Clima (IC3), Barcelona, Spain.
{muhammad.asif,oriol.mula-valls,domingo.manubens,francisco.doblas-reyes}@ic3.cat
Computer Architecture and Operating Systems Department, Universitat Aut`
onoma de Barcelona,
08193 Bellaterra, Barcelona, Spain.
{andres.cencerrado,ana.cortes}@uab.es
3
Instituci´
o Catalana de Recerca i Estudis Avan¸cats (ICREA), Barcelona, Spain

Abstract
The EC-Earth climate model is a seamless Earth System Model (ESM) used to carry out climate
research in 24 academic institutions and meteorological services from 11 countries in Europe.
This model couples several components and it is continuously under development.
In this work we present a study regarding the impact of the I/O and data management
when using EC-Earth in well-known supercomputing environments.
Most large-scale and long-term climate simulators have been developed bearing in mind
the paramount importance of its scalability. However, the computational capabilities of the
High Performance Computing (HPC) environments increase at so great speed that it is almost
impossible to re-implement the whole models so that they are able to exploit eﬃciently the new
features. Therefore, it is necessary to design diﬀerent strategies to take advantage of them.
In this work we present an operational framework to run ensemble simulations in HPC platforms. A set of experiments are presented in order to validate the suitability of this technique.
Moreover, the derived impact regarding the I/O and data management aspects is analyzed.
Keywords: High Performance Computing, Climate Simulation, Supercomputing Environments, ECEarth3

∗ This study was supported by the European Unions FP7-funded SPECS (308378) and IS-ENES2 (312979)
projects, the MICINN-Spain under contract TIN2011-28689-C02-01 and the Catalan Government. It used
resources of the Oak Ridge Leadership Computing Facility at the Oak Ridge National Laboratory through the
grant CLI055, which is supported by the Oﬃce of Science of the U.S. Department of Energy under Contract
No. DE-AC05-00OR22725.

2370

Selection and peer-review under responsibility of the Scientiﬁc Programme Committee of ICCS 2014
c The Authors. Published by Elsevier B.V.
doi:10.1016/j.procs.2014.05.221

Impact of I/O in Large Scale Climate Forecasting Using EC-Earth3

1

M. Asif et al.

Introduction

Large-scale climate modeling and simulation is undoubtedly a scientiﬁc area in which computational science plays an important role. Diﬀerent organizations and consortiums have devoted
important eﬀorts towards the implementation and validation of diﬀerent models [7, 8]. This
kind of models are characterized by being very computationally demanding. Since the advent
of parallel-computing paradigms, the scientiﬁc community has been striving to take advantage
of parallel HPC platforms [2, 6, 11]. However, because of their intrinsic features, it turns out
very hard to obtain good scaling on climate models when working with thousands of processors.
This work describes a method to relieve this drawback in the case of the EC-Earth climate model by essentially wrapping together many independent simulations as a big single
job. This way, we are able to exploit more eﬃciently the computational time and amount of
processors provided by supercomputers. The eﬀectiveness of this method is demonstrated by
experimental work based on three important supercomputers: Lindgren, MareNostrum and
Jaguar. Nevertheless, since the EC-Earth model consists of diﬀerent components that need to
be periodically coupled (which involves signiﬁcant amounts of communication) and it produces
important amounts of data, it is necessary to focus on the impact of the use of this strategy on
the I/O and data storage. The experiments carried out indicate that the I/O and data storage
do not represent a bottleneck when applying this technique at present, but there are some signs
that suggest that it could be in the near future. A report on this study is provided, and the
results analyzed.
This paper is organized as follows. In the next section, an overview of the EC-Earth model
is given. Section 3 emphasizes the need of HPC platforms to run large-scale climate simulations.
In Section 4, we present the developed operational framework to run ensemble simulations and
the experimental studies are reported. Finally, the main conclusions are included in Section 5.

2

EC-Earth, the European Community Earth System
Model

EC-Earth [4, 9] is a seamless Earth System Model (ESM) selected by the Catalan Institute of
Climate Sciences (IC 3 ) to perform climate research as end users and also as part of the ECEarth consortium [3], which consists of 24 academic institutions and meteorological services
from 11 countries in Europe. It is based on the seasonal prediction system of the European
Centre for Medium-Range Weather Forecast (ECMWF). Its development has been based on
the ECMWF’s Integrated Forecast System (IFS) as a well tested and validated atmospheric
module, with diﬀerent components being added over the time. EC-Earth also counts on an
ocean model (NEMO), which includes sea-ice and biogeochemistry modules. The NEMO and
IFS components are coupled through the OASIS3 coupler. EC-Earth version 3 works with
resolutions ranging from approximately 128 to 25 kilometers.
A typical climate forecast experiment in EC-Earth is a run of the climate model having a
variable range of forecast length from a few months to a few years. An experiment may have
one or more start dates and every start date may comprise many diﬀerent initial conditions.
For each start date, several simulations can be run diﬀering slightly in their initial conditions.
Typically, each of these simulations is called a member. The full length of forecasting period
for the experiment can be divided into a certain number of chunks of ﬁxed length of forecast.
Furthermore, in the context of computing operations, every chunk presents two big sections: a
parallel section corresponding to the simulation itself and serial sections for carrying out other
2371

Impact of I/O in Large Scale Climate Forecasting Using EC-Earth3

M. Asif et al.

Figure 1: Example of Sea Surface Temperature forecast for one month (mean values, Kelvin
scale) using the standard resolution (1 degree) of the ocean component NEMO.
necessary operations like post-processing of the model output, archiving the model output and
cleaning the disk space.
In order to quantitatively assess the performance of EC-Earth3, the model performance
index deﬁned in [13] is used. This performance index is based on the Murphy’s Climate Prediction Index [12], and allows us to objectively determine the quality of EC-Earth3 in comparison
with other models.

2.1

EC-Earth3 Components

As stated above, the EC-Earth3 coupled climate model comprises three main computational
components: the atmospheric model IFS, the ocean model NEMO, and the OASIS3 coupler.
• IFS: The Integrated Forecasting System (IFS) component is used for weather prediction
at the European Center for Medium-range Weather Forecasts (ECMWF), and is also used
as the atmosphere component in EC-Earth3. Besides, it is used throughout Europe by
many institutes and universities for weather and climate forecasts and research. The IFS
component discretizes the 3D Navier-Stokes equations and it uses a spectral method to
compute the dynamics of the atmosphere. Atmospheric physics, e.g., precipitation, is
calculated on each grid-point, for which the domain is decomposed in two dimensions.
Therefore, the solution needs to be converted from spectral space in order to calculate
the dynamics and grid-point space so that it is possible to calculate the physics at each
grid-point.
• NEMO: NEMO (Nucleus for European Modelling of the Ocean) is an ocean model that
includes several components besides the ocean circulation, including sea-ice and biogeochemistry. The ocean general circulation model is based on a primitive equation model
numerically solved on a global ocean curvilinear grid known as ORCA [10]. It discretizes
the 3D Navier-Stokes equations and it uses MPI with a 2D domain decomposition approach as parallelization strategy. The NEMO ocean component is used in several climate
2372

Impact of I/O in Large Scale Climate Forecasting Using EC-Earth3

M. Asif et al.

models that are used for the Intergovernmental Panel on Climate Change (IPCC) [1] reports about climate change. Furthermore, it is also used for standalone simulations.
• OASIS3: The OASIS3 coupler is a software that allows synchronized exchanges of coupling information between the IFS and NEMO components. OASIS3 is currently used
by approximately 30 climate modeling and operational weather forecasting groups in Europe, USA, Canada, Australia, India and China. The OASIS3 coupler uses separate MPI
processes that send and receive messages with coupling ﬁelds between the diﬀerent components. Although OASIS3 is serial for a single coupling ﬁeld, it can use diﬀerent processes
to couple multiple ﬁelds in parallel (this is called pseudo-parallelism). The maximum
parallelism in the coupling process is therefore limited by the number of coupling ﬁelds.
All components are mainly written in Fortran (with some units implemented in C) and
parallelized for distributed memory architectures using MPI. The diﬀerent components use
NetCDF formats as output.
Parameter
IFS time step

T159-ORCA1
1 hour

T255-ORCA1
1 hour

T799-ORCA025
12 minutes

IFS output frequency

6 hours

6 hours

6 hours

NEMO time step

1 hour

1 hour

20 minutes

NEMO output frequency

varying:
daily,
5-daily,
monthly...

varying:
daily,
5-daily,
monthly...

varying:
daily,
5-daily,
monthly...

OASIS3 coupling frequency

3 hours

3 hours

3 hours

Output size (one-month forecast)

0.9 GB approx.

1.6 GB approx.

21.4 GB approx.

Restart size

1.4 GB approx.

2.3 GB approx.

25 GB approx.

Table 1: Parameters of diﬀerent EC-Earth3 conﬁgurations

2.2

Scientiﬁc cases

EC-Earth3 works with diﬀerent conﬁgurations as regards the resolutions of both the IFS and
NEMO components. The three conﬁgurations that have been chosen for the experimental
analyses of this work are described next:
a) T159-ORCA1: the atmosphere component IFS is run with resolution T159, which works
with a grid-spacing of approximately 128km. The ocean component NEMO is run at a 1
degree, which translates to about 110km. The resolution of the atmosphere and the ocean
components match quite closely. However, since the IFS component incorporates more
physical processes, it requires several times more computing resources than the NEMO
component. The input data for the IFS component are about 29MB in size. The input
data for the NEMO component are about 0.8GB in size. The time step is one hour, both
in the atmosphere and in the ocean components.
2373

Impact of I/O in Large Scale Climate Forecasting Using EC-Earth3

M. Asif et al.

b) T255-ORCA1: Resolution T255 in the IFS component converts to a grid-spacing of
approximately 80km, but otherwise it is the same as the above component. The input
data for the IFS component are about 95MB in size, and for NEMO it does not vary with
respect to the previous case, since its resolution is the same.
c) T799-ORCA025: high-resolution version of EC-Earth3, with a resolution of about 25km
and a similar resolution in the ocean. The input data for the IFS component are about
680MB in size. The input data for the NEMO component are about 11GB in size. The
time step for IFS is 12 minutes. The time step for NEMO is 20 minutes.
Depending on each case, the amount of data produced by each simulation may vary (in
one-month forecasts, for instance) from approximately 1 Gigabyte to more than 20 Gigabytes.
It is worth emphasizing the fact that EC-Earth3 allows simulation restarts, i.e., a set of special
output is periodically produced to allow restarting simulations that may have been interrupted
because of a failure or a stop committed by the user after completing some reasonable length of
simulation. To enable this feature, we rely on the aforementioned concept of chunk. The length
of each chunk is variable and conﬁgurable, and usually is counted in months of simulation. The
data produced when each chunk is completed may vary from 1.5 Gigabytes to 25 Gigabytes,
approximately. Table 1 summarizes this information as well as the most important parameter
values for each conﬁguration.

3

Large Scale Climate Simulations in High Performance
Computing environments

A well-known goal among the climate research community is to be able to simulate 10 years
per wall clock day, which means to simulate approximately 1 hour per second. In the context of
scientiﬁc cases chosen in Section 2.2 a scaling exercise was performed using the Lindgren supercomputer (described in the subsequent section). As can be seen in Figure 2, with T159-ORCA1
and T255-ORCA1 conﬁgurations the model could complete approximately 4.8 simulated years
in a single wall clock day. With T799-ORCA025 conﬁguration, however, for the completion of
10 simulated years the machine required to work continuously for approximately 17 wall clock
days with 1,104 cores. Therefore, it is clear that we need to rely on HPC environments as well
as to be able to exploit them to reach the current community ambitions.

3.1

Analyzing scalability limits

Each conﬁguration detailed in Section 2.2 has been used in diﬀerent HPC platforms. Subsequently, their main features are presented, where a) corresponds to the platform used with
T159-ORCA1, b) the one used with T255-ORCA1, and c) the one used with T799-ORCA025:
a) MareNostrum2 (Barcelona Supercomputing Center, Spain): IBM BladeCenter
JS21 with IBM Power PC 970MP processors (2.3GHz) and Myrinet interconnect technology.
b) Lindgren (PDC Center for High-Performance Computing, Sweden): Cray XE6
system, based on AMD 12-core Magny-Cours Opteron (2.1 GHz) processors and Cray
Gemini interconnect technology.
c) Jaguar (Oak Ridge National Laboratory, USA): Cray XT5 system, based on AMD
6-core Istanbul Opteron (2.6 GHz) processors and InﬁniBand network.
2374

Impact of I/O in Large Scale Climate Forecasting Using EC-Earth3

M. Asif et al.

Figure 2: EC-Earth simulation features for each conﬁguration at Lindgren, PDC.
In order to test the scalability of EC-Earth3, a set of experiments were carried out, checking
diﬀerent distributions of the available processors between the IFS and the NEMO components.
For the OASIS3 coupler, 10 cores were always allocated. This experimentation was carried out
using the Jaguar platform.
Figure 3(a) shows the elapsed times of 5-day simulations using the T799-ORCA025 conﬁguration as a function of the number of computing cores allocated. Each colored line represents the
times obtained using diﬀerent core distributions among the diﬀerent components. Concretely,
10 cores were always allocated to the OASIS3 coupler and the diﬀerent lines correspond to the
diﬀerent amounts of cores allocated to NEMO in this experiment: 384, 576 and 768. Therefore,
the number of processors allocated to IFS are, in each case, the remainder of subtracting the
number of processors for NEMO and the number of processors for OASIS3 (always 10) from
the value at the horizontal axis. Figure 3(b) shows the corresponding speedup values relating
to the times obtained with the reference test consisting of 384 cores allocated to IFS, 384 cores
allocated to NEMO, and 10 cores allocated to OASIS3 (778 cores in total).
Figure 3(b) shows that the optimum performance can be obtained with 2,122 CPUs, where
1,536 are used for IFS, 576 for NEMO and 10 for OASIS3.
These results demonstrate a good scalability of EC-Earth3 up to approximately two thousand processors. However, they also illustrate a typical problem among the current climate
models that is the diﬃculty to scale beyond few thousand processors, being unable to exploit
the massive amount of available computational cores and, therefore, declining their eﬃciency.
In the next section, we describe the developed method to relieve this drawback by wrapping
many simulations in a big single job.

4

Operational framework to run ensemble simulations

The limits the scalability showed in the previous section become a great disadvantage when
working with HPC environments that provide us with thousands of computational cores. For
this reason, an ensemble methodology has been adopted, in order to take advantage of the
number of processors available in computing platforms such as the ones presented in Section
3.1. Thus, a number of independent climate simulations can be wrapped to be run as a big single
job. This technique can be carried out because climate predictions (ensembles of simulations
with multiple start dates) are independent of each other. The parallel simulations that are
2375

Impact of I/O in Large Scale Climate Forecasting Using EC-Earth3

(a) Elapsed times

M. Asif et al.

(b) SpeedUp

Figure 3: (a) Elapsed times of the T799-ORCA025 conﬁguration as a function of the total number of processors for 5-day simulations, running in Jaguar. (b) Scaling of the T799-ORCA025
conﬁguration as a function of the total number of processors for 5-day simulations. Blue, orange
and yellow curves correspond to results for tests with NEMO using 384, 576 and 768 CPUs.
Speedup values are relative to the reference test (778 processors).

Figure 4: Sample experiment setup.

run simultaneously can be diﬀerent members of an ensemble of climate predictions or diﬀerent
climate predictions started at diﬀerent times in the past.
Figure 4 shows a sample experiment which consists of 10 start dates from 1960 to 2005 where
every start date is independent of each other, starting every 5 years and each one comprising
5 members. Every member is also independent and has been divided into 10 chunks which
are dependent of each other. Let us suppose that the forecast length for each chunk is one
year and every chunk comprises three types of jobs; a simulation, a post-processing and an
archiving and cleaning job. With this typical example experiment, just one member of each
start date comprises 30 jobs. Eventually, 1500 jobs will be run in total for the completion
of the experiment. To fulﬁll the need to automate such type of experiments and optimize
the use of resources, these tasks are controlled by Autosubmit [5]. Autosubmit is a tool that
2376

Impact of I/O in Large Scale Climate Forecasting Using EC-Earth3

M. Asif et al.

helps create, run and monitor climate experiments remotely by using the computing resources
available at supercomputing environments. The current version of the tool has been tested in
several supercomputing centers worldwide such as the Barcelona Supercomputing Center, the
Edinburgh Parallel Computing Centre, the ECMWF and the Oak Ridge Leadership Computing
Facility. Autosubmit manages the submission of jobs to the scheduler queue until there is no
job left to be run. Additionally, it also provides features to suspend, resume, restart and extend
similar experiment at later stage.

Figure 5: Average simulation times as a function of the number of jobs wrapped using conﬁgurations T159-ORCA1 (run in MareNostrum2) and T255-ORCA1 (run in Lindgren). Green
portions show communication times.
In Figure 5 and Figure 6 (top) one can see that this technique does not produce overhead
in terms of simulation time, since the time needed to perform the simulations did not vary
signiﬁcantly as function of the number of jobs wrapped. This experiment (for which 547,294
CPU-hours were consumed) demonstrates the viability of our developed wrapping technique.

4.1

I/O Performance Analysis

Nevertheless, given the important amount of communications in the parallel simulations between components and the OASIS3 coupler, and also the amount of data to be stored that
is produced by each long-term simulation, it is worth analyzing the impact of the use of the
wrapping technique in the I/O operations, since they could become a new important bottleneck
to deal with.
Figures 5 and Figure 6 (top) show that the proportion of time consumed by the I/O communications remains almost constant, independently of the number of jobs wrapped. The fact
that the absolute values are low (below 22% in all cases) indicates that currently the I/O communication is not a bottleneck of EC-Earth3, but it also shows an important caveat: given that
the proportion of time dedicated to I/O communications is signiﬁcant in all cases (especially in
T159-ORCA1 and T255-ORCA1 conﬁgurations), it may become a bottleneck if the scalability
of the computational part is increased and/or the frequency and amount of output change in
further versions.
Furthermore, in the case of T799-ORCA025 we also studied the total elapsed time for
the jobs completion. In Figure 6 (bottom) the diﬀerent proportions, including the ﬁlesystem
access times, are depicted. Thus, the 100% indicates the total elapsed times, and red portions
indicate the percentage of this time that is dedicated to store and move the great amount of
data produced by and required to set up the simulations.
2377

Impact of I/O in Large Scale Climate Forecasting Using EC-Earth3

M. Asif et al.

Figure 6: Top: average simulation times as a function of the number of jobs wrapped using
conﬁgurations T799-ORCA025 (run in Jaguar). Green portions show communication times.
Bottom: percentage of execution times, including ﬁlesystem access times.
As it can be seen, these values are very important, since the preparation of the environment
by reading the initial conditions and other essential auxiliary ﬁles from the common ﬁlesystem
leads us to consume almost 47% the elapsed time when wrapping 30 jobs together.

5

Conclusions and Future Work

In this work, we present an operational framework to run ensemble simulations of EC-Earth3
in HPC platforms. Given that this model (as it is usual among the current state-of-art climate
models) does not present a good scalability beyond few thousand processors, a strategy to
exploit the great amount of computational cores available is presented. This technique is
essentially based on wrapping together many independent simulations as big single jobs.
This wrapping method allows us to exploit more eﬃciently the computational time and
amount of available processors. A set of experiments are presented in order to validate the
suitability of this technique.
The impact of the use of this technique as regards the I/O and data management aspects
is also analyzed. The obtained results indicate that the I/O operations are not a current
bottleneck, but they are expected to represent a serious problem in the near future when
the application can be scaled to higher core counts, or if the frequency of I/O time steps
increases. Moreover, the I/O performance depends a lot on the internal structure of the gen2378

Impact of I/O in Large Scale Climate Forecasting Using EC-Earth3

M. Asif et al.

erated NetCDF. It is therefore recommended to design the NetCDF-output ﬁles and test their
performance.
The great amount of data produced, however, seems to be an important problem at present,
since the simultaneous access to the ﬁlesystem of thousands of simulations may represent almost
47% of the elapsed time. So, it is necessary to design suitable solutions to deal with this issue.

References
[1] Intergovernmental panel on climate change homepage (accessed december 2013). [online].
http://ipcc.ch.
[2] Thomas Bettge, Anthony Craig, Rodney James, Warren G. Strand Jr., and Vincent Wayland.
Performance of the parallel climate model on the sgi origin 2000 and the cray t3e. In Proceedings
of the 41st Cray User Group Conference, 1999.
[3] EC Earth Consortium. Ec earth consortium homepage (accessed december 2013). [online].
http://ecearth.knmi.nl.
[4] EC Earth Consortium. Ec earth homepage (accessed december 2013). [online].
http://www.ec-earth.org.
[5] Institut Catal`
a de Ci`encies del Clima (IC3). Autosubmit tool (accessed december 2013). [online].
http://ic3cfu.wikispot.org/Autosubmit.
[6] John M. Dennis, Mariana Vertenstein, Patrick H. Worley, Arthur A. Mirin, Anthony P. Craig,
Robert L. Jacob, and Sheri A. Mickelson. Computational performance of ultra-high-resolution
capability in the community earth system model. International Journal of High Performance
Computing Applications, 26(1):5–16, 2012.
[7] Peter R. Gent, Gokhan Danabasoglu, Leo J. Donner, Marika M. Holland, Elizabeth C. Hunke,
Steve R. Jayne, David M. Lawrence, Richard B. Neale, Philip J. Rasch, Mariana Vertenstein,
Patrick H. Worley, Zong-Liang Yang, and Minghua Zhang. The Community Climate System
Model version 4. Journal of Climate, 24(19):4973–4991, 2011.
[8] James J. Hack, James M. Rosinski, David L. Williamson, Byron A. Boville, and John E. Truesdale.
Computational design of the NCAR community climate model. Parallel Computing, 21(10):1545–
1569, 1995.
[9] Wilco Hazeleger, Camiel Severijns, Tido Semmler, Simona Stefanescu, Shuting Yang, Xueli Wang,
Klaus Wyser, Emanuel Dutra, Jos M. Baldasano, Richard Bintanja, Philippe Bougeault, Rodrigo
Caballero, Annica M. L. Ekman, Jens H. Christensen, Bart van den Hurk, Pedro Jimenez, Colin
Jones, Per Kallberg, Torben Koenigk, Ray McGrath, Pedro Miranda, Twan Van Noije, Tim
Palmer, Jos A. Parodi, Torben Schmith, Frank Selten, Trude Storelvmo, Andreas Sterl, Honor´e
Tapamo, Martin Vancoppenolle, Pedro Viterbo, and Ulrika Will´en. Ec-earth: A seamless earthsystem prediction approach in action. Bull. Amer. Meteor. Soc., 91:1357–1363, 2010.
[10] Gurvan Madec and Maurice Imbard. A global ocean mesh to overcome the north pole singularity.
Climate Dynamics, 12(6):381–388, 1996.
[11] Arthur A. Mirin and Patrick H. Worley. Improving the performance scalability of the community
atmosphere model. International Journal of High Performance Computing Applications, 26(1):17–
30, 2012.
[12] James M. Murphy, David M. H. Sexton, David N. Barnett, Gareth S. Jones, Mark J. Webb,
Matthew Collins, and David A. Stainforth. Quantiﬁcation of modelling uncertainties in a large
ensemble of climate change simulations. Nature, 430:768–772, 2004.
[13] Thomas Reichler and Junsu Kim. How well do coupled models simulate todays climate? Bull.
Amer. Meteor. Soc., 89:303–311, 2008.

2379

