Available online at www.sciencedirect.com

Procedia Computer Science 4 (2011) 1062–1071

Biomedical and Bioinformatics Challenges to Computer Science

Exact Closest String as a Constraint Satisfaction Problem
Tom Kelsey1 and Lars Kotthoﬀ
University of St Andrews

Abstract
We report the ﬁrst evaluation of Constraint Satisfaction as a computational framework for solving closest string
problems. We show that careful consideration of symbol occurrences can provide search heuristics that provide several
orders of magnitude speedup at and above the optimal distance. We also report the ﬁrst analysis and evaluation – using
any technique – of the computational diﬃculties involved in the identiﬁcation of all closest strings for a given input set.
We describe algorithms for web-scale distributed solution of closest string problems, both purely based on AI backtrack
search and also hybrid numeric-AI methods.
Keywords: closest string, constraint satisfaction, search heuristics

1. Introduction
The closest string problem (CS) takes as input a set of strings of equal length over a ﬁxed alphabet. A solution is a
string with the smallest possible maximum Hamming distance from any input string. (Strictly speaking, distance with
respect to any suitable metric can be minimised) CS has applications in coding and information theories, but when
the input strings consist of nucleotide sequences over the letters A, C, G and T, the CS has important applications in
computational biology. Examples include the identiﬁcation of consensus patterns in a set of unaligned DNA sequences
known to bind a common protein [1] and DNA motif representation with nucleotide dependency [2]. Our aim is to
provide theoretical and practical results – together with empirical supporting evidence – that lead to improved CS
solution for biological problems, so in this paper the base alphabet Σ will always consist of four symbols.
A Constraint Satisfaction Problems (CSP) consists of a set of constraints involving variables taking discrete values.
A solution to a CSP is an assignment of values to variables such that no constraint is violated. CSP solvers are used for
many important classes of problems for which solutions must take discrete values, but, to our knowledge, the closest
string problem has not been modelled and solved as a CSP. The research question under consideration, therefore, is “Is
CSP a useful framework for solving CS instances?”
In this paper we investigate approaches to such models. We demonstrate that a careful choice of search heuristic
can give several orders of magnitude speedup in general. We show that CSP modelling and solution are eﬀective tools
for the related problem of obtaining all closest strings. We consider the distribution of closest string problems across
a cloud (or grid) of computing nodes, and identify two super-linear speedups. Finally we identify the strengths and
weaknesses of existing numeric approaches, and suggest hybrid discrete and numeric methods that combine the best
features of CSP and numeric search.
1 Corresponding

author tom@cs.st-andrews.ac.uk. TK is supported by EPSRC grant EP/H004092/1. LK is supported by a SICSA studentship.

1877–0509 © 2011 Published by Elsevier Ltd. Open access under CC BY-NC-ND license.
Selection and/or peer review under responsibility of Prof. Mitsuhisa Sato and Prof. Satoshi Matsuoka doi:10.1016/j.procs.2011.04.113

Tom Kelsey et al. / Procedia Computer Science 4 (2011) 1062–1071

1063

In the rest of this introduction we discuss existing methods for the CS with respect to theoretical complexity results,
give brief overviews of Constraint Satisfaction theory and the Minion CSP solver, and formally deﬁne the theoretical
concepts upon which the research is based. In Section 2 we model closest string problems as CSPs, compare search
heuristics, and provide results for the all closest string problem. We describe distributed algorithms in Section 3, both
for pure CSP models and heuristics, and for hybrid CSP-numeric methods. In Section 4 we discuss the relative strengths
and limitations of CSP as a framework for closest string identiﬁcation.
1.1. Computational Complexity and Existing Methods
CS has been shown to be NP-complete for binary strings [3] and for alphabets of arbitrary size [4]. Intuitively there
are |Σ| choices for each of the L positions in any candidate closest string where Σ is the alphabet, and for any algorithm
that fails to check each of this exponential number of cases one could devise a CS for which the algorithm returns an
incorrect result.
Approximate solutions to within (4/3 + ) of the minimal d can be obtained in polynomial time [4, 5], with several
practically useful implementations available, notably those based on genetic algorithms [6]. However, in this paper
we are concerned with ﬁrst ﬁnding exact solutions, and then (given that we know the minimal distance d) ﬁnding all
closest strings that are within d of S . Clearly, an approximate method will not, in general, identify the minimal d, and
therefore can not be used as a basis for ﬁnding all solutions.
Excellent exact results – provided that close bounds have already been identiﬁed – have been obtained by modelling
and solving the CS as an Integer Programming Problem [7]. This form of search diﬀers from the backtrack search used
by CSP solvers by having a much less organised search pattern. This is often advantageous, but can be a hindrance
when searching for all solutions: IP branch and bound is optimised for optimisation, as it were, rather than exhaustive
search for all candidates for a constant objective function. If the IP formulation suggested in [7] is used, then the
feasible region deliberately excludes optimal solutions in order to reduce the numbers of variables, in which case no
search for all solutions can be made.
A linear time algorithm exists for solutions to the CS for ﬁxed distance d [8]. The exponential complexity is now in
the coeﬃcient, as the method is O(NL + Nddd ) where the problem has N strings of length L.

1.2. Constraint Satisfaction Problems
Deﬁnition. A Constraint Satisfaction Problem Υ is a set of constraints C acting on a ﬁnite set of variables Δ :=
{A1 , A2 , . . . , An }, each of which has a ﬁnite domain of possible values Di := D(Ai ) ⊆ Λ. A solution to Υ is an
instantiation of all of the variables in Δ such that no constraint in C is violated.
The Handbook of Constraint Programming [9] provides full details of CSP theory and techniques. A key observation
is that diﬀerent models (i.e. choices of variables, values and constraints) for the same problem (or class of problems)
will often give markedly diﬀerent results when the instances are solved, but, as with numeric Linear, Mixed-Integer and
Quadratic Programming, there is no general way to decide in advance which candidate models and heuristics will lead
to faster search.
A typical solver operates by building a search tree in which the nodes are assignments of values to variables, and the
edges lead to assignment choices for the next variable. If a constraint is violated at any node, then search backtracks. If
a leaf is reached, then all constraints are satisﬁed, and the full set of assignments provides a solution. These search trees
are obviously exponential, and in the worst-case scenario every node may have to constructed. However, large-scale
pruning of the search tree can occur by judicious use of consistency methods. The idea is to do a small amount of extra
work that (hopefully) identiﬁes variable-value assignments that are already logically ruled out by the current choice of
assignment, meaning that those branches of the search tree need not be explored. While there are no guarantees that
this extra work is anything other than an overhead, in practice enough search is pruned to give eﬃcient solutions for
otherwise intractable problems.
Heuristics exist for choices of variable-value pair for the next node, and as before these may have no eﬀect on the
number of nodes visited. Again, in general, variable and value orderings designed for speciﬁc problem classes can
lead to several orders of magnitude reduction in the number of nodes needed to ﬁnd a solution. Standard choices for
variable orderings include random, smallest domain, largest domain, most-constrained, least-constrained, etc. Results
will vary with the problem class and model under consideration.

1064

Tom Kelsey et al. / Procedia Computer Science 4 (2011) 1062–1071

1.2.1. The Minion CSP solver
The constraint solver Minion [10] uses the memory architecture of modern computers to speed up the backtrack
process compared to other solvers. Minion has an extensive set of constraints, together with eﬃcient propagators that
enforce consistency levels very rapidly. Minion has been used to solve open problems in combinatoric algebra [11],
ﬁnding billions of solutions in a search space of size 10100 in a matter of hours. Minion is used for this investigation as
it oﬀers both fast and scalable constraint solving, which are important factors when solving closest string problems.
1.3. Formal Deﬁnitions and Results
Deﬁnition. Let S 1 and S 2 be strings of length L over an alphabet Σ. Let D be the binary string of length L such that
D(i) =

1 S 1 (i) S 2 (i)
0 otherwise

The Hamming Distance hd(S 1 , S 2 ) is deﬁned as the sum from i = 1 to L of the D(i).
Deﬁnition. Let S = {S 1 , S 2 , . . . , S N } be a set of strings of length L over an alphabet Σ. A Closest String to S is
deﬁned as any string CS of length L over Σ such that hd(CS , S i ) ≤ d ∀i ∈ {1, 2, . . . , N} with d being the minimal
such distance for S . The Hamming diameter HD of S is deﬁned as HD(S ) = max(hd(S i , S j )) ∀i, j ∈ {1, 2, . . . , N}.
A solution to a closest string problem involving the strings in S is therefore a string CS and a minimal distance d
such that each member of S is within d of CS . The Hamming distance is an edit distance that quantiﬁes the number
of substitutions from Σ required to turn one string into another. It is easy to show that Hamming distance is a metric,
satisfying the triangle inequality. It is clear that the Hamming Diameter is an upper bound for the distance of a closest
string: a candidate closest string at a greater distance can be replaced by any member of S , reducing the maximal
distance to HD(S ). We can obtain a lower bound for the distance of a closest string by observing that the distance can
not be less than half the Hamming Diameter:
Lemma 1. Let S = {S 1 , S 2 , . . . , S N } be a set of strings of length L over an alphabet Σ. A closest string CS to S must
be within HD(S )/2 of S .
Proof. Let S i and S j be two strings from S for which the Hamming Diameter is achieved, and let S k be any other
string of length L over Σ. By the triangle inequality H(S ) = hd(S i , S j ) ≤ hd(S i , S k ) + hd(S j , S k ). If (without loss of
generality) hd(S i , S k ) < HD(S )/2 then hd(S j , S k ) ≥ HD(S )/2 . Hence any distance less than HD(S )/2 can not
be a maximal distance from S k to any string in S .
Search space reduction can be achieved by noting that any value not appearing in position j of any of the strings in
S need not appear in a closest string solution. It should be noted that this only applies when searching for the ﬁrst
optimal solution. When searching for all solutions, any symbol from Σ can, in principle, appear at any position in CS .
Lemma 2. Let S = {S 1 , . . . , S N } be deﬁned as in Lemma 1. Let Σ j for j ∈ 1, 2, . . . , L denote the subset of Σ obtained
by selecting every symbol that appears in position j of a string in S . Then any symbol in position j of a closest string
to S must also be in Σ j .
Proof. Suppose s in Σ \ Σ j appears in position j of a solution CS . Let CS be the string consisting of CS with s replaced
by a symbol from Σ j at position j. Then CS is strictly closer to those strings in S with that symbol at that position, and
distance to all other strings is unchanged. Hence if the current d is optimal for CS , it remains optimal for CS .
The ﬁnal deﬁnition needed for this investigation encapsulates frequencies of symbol appearances per string position,
and will be used in Section 2.1 to direct backtrack search for closest strings.
Deﬁnition. Let S = {S 1 , . . . , S N } be a set of strings of length L over an alphabet Σ. A Position Weight Matrix (PWM)
for S is an |Σ| × L matrix with entries PMS S (i, j) deﬁned as the frequency of symbol i appearing at position j in S .
An example Position Weight Matrix is given in Figure 1.

Tom Kelsey et al. / Procedia Computer Science 4 (2011) 1062–1071

A
C
G
T

a
C
a
a
C
3
2
0
0

G
c
c
c
c
0
4
1
0

g
A
g
g
g
1
0
4
0

t
t
t
t
t
0
0
0
5

a
a
T
C
a
3
1
0
1

c
c
A
c
c
1
4
0
0

T
g
g
A
g
1
0
3
1

1065

t
t
t
t
G
0
0
1
4

Table 1: Five strings of length 8 are shown above, with their PWM shown below.

2. Closest String as a Constraint Satisfaction Problem
Using the terminology from Sections 1.2 and 1.3, we now construct a CSP instance from a given closest string
problem. For the purposes of this paper, the alphabet Σ consists of the numbers 1, 2 ,3 and 4, representing A, C, G and
T respectively. Clearly this artiﬁcial restriction can easily be relaxed in order to model arbitrary alphabets.
Given S , a set of N strings of length L over alphabet Σ, we ﬁrst compute the Hamming Diameter HD(S ) and use
this to provide a lower bound, dmin , for the optimal distance d, as shown in Lemma 1. Υ(S , dmin , HD(S )) denotes the
CSP instance in which the set of variables is Δ := Δ1 ∪ Δ2 ∪ Δ3 ∪ Δ4 , where
1. Δ1 is the array [CS 1 , CS 2 , . . . , CS L ] of variables representing the closest string, each such variable having
domain 1 through 4
2. Δ2 is an N × L array of binary variables used to calculate Hamming Distances from Δ1 to the input strings S
3. Δ3 is the array [D1 , D2 , . . . , DN ] of variables representing the distance of each string in S to the current CS
candidate, each such variable having domain dmin through HD(S )
4. Δ4 is the single distance variable d with domain dmin through HD(S ).
The constraints are:
1.
2.
3.
4.

Δ2 (i, j) = 0 iﬀ S i ( j) = Δ1 ( j)
Δ3 (k) is the sum of row k of Δ2
Δ4 is the maximum value appearing in Δ3
Δ4 is minimised: if a solution is found with Δ4 = d, search for another solution with Δ4 = d − 1 (unless d = dmin ).

Δ1 are the search variables: nodes of the search tree consist of values assigned to these variables. Δ4 is the objective
function (or cost function). A returned solution is Δ1 ∪ Δ4 , a closest string together with the optimal distance. Solving
Υ(S , dmin , HD(S )) is guaranteed to return a solution, although it is not impossible that all 4L nodes are visited for every
current minimal d. Restricting the domains of Δ3 will save computational eﬀort when a solution is found with d = dmin
and will have no eﬀect otherwise. Restricting the domains of the Δ1 variables in line with lemma 2 also reduces the
search space, although the restrictions can not apply when searching for all solutions.
To ﬁnd all closest strings Υ(S , dmin , HD(S )) is solved to obtain CS and dopt . By restricting the domains of Δ3 to
dmin through dopt and removing the optimisation constraint we obtain a new CSP Υ∗ (S , dmin , d) which can be solved for
all solutions. The search undertaken to ﬁnd the ﬁrst solution CS need not be repeated: constraints can be added that
rule out those parts of the search tree already processed.
2.1. Position Weight Matrix Variable and Value Ordering
We now use results from computational biology to devise a bespoke variable and value ordering schema for
Υ(S , dmin ). By precalculating a Position Weight Matrix for S as deﬁned in Section 1.3 we can order the search variables
by maximum frequency. For each variable, we order the values assigned during search by decreasing relative frequency.
Tie breaks are either random or by least index. By Lemma 2, when seeking a single solution it is safe to exclude values
that don’t appear at a given position from their respective variable domains before search. The idea behind this heuristic
is that search starts close to (in the sense of maximum likelihood) an optimal solution. Only if no such solution is found
does search progress to less likely (but not impossible) parts of the search tree .

Tom Kelsey et al. / Procedia Computer Science 4 (2011) 1062–1071

1066
speedup

speedup

200.0

length = 10
0.000001 s

length = 10
0.0279955 s

100.0

length = 15
0.055491 s

length = 20
1.132327 s

length = 25
54.410228 s

length = 15
0.0039995 s

length = 20
0.1934705 s

length = 25
17.266875 s

length = 30
325.193563 s

length = 30
1346.360321 s
100000.0000

50.0

20.0
100.0000

10.0
5.0

0.1000

2.0
1.0
0.5

0.0001

3

4

5

6

3

4

5

6

3

4

5

6

3

number of strings

4

5

6

3

4

5

6

3

4

5

6

3

4

5

6

3

4

5

6

3

4

5

6

3

4

5

6

number of strings

Figure 1: Empirical data from 200 instances of 3, 4, 5 and 6 strings of lengths 10, 15, 20, 25 and 30. For each combination of parameters, 10 random
instances were generated with results summarised in the boxes which show median values (thick line), 25th–75th percentiles (boxed) and 0th–100th
percentiles (dashed lines). In the left panel we compare the exact optimal solution times. In the right panel we show the times taken to obtain
an optimal result, omitting the time needed to certiﬁcate that result. In both ﬁgures the y axis shows the speedup of Position Weight Matrix over
Smallest Domain First ordering on a logarithmic scale, and the times given below the string lengths are the median CPU time taken over all strings of
that length. The experiments were conducted on a dual quad-core 2.66 GHz Intel X-5430 processor with 16 GB of RAM.

2.2. Comparison of Search Heuristics
In this Section we test the hypothesis that PMS-based search heuristics reduce the search needed for solutions to
Υ(S , dmin ) CSP instances when compared to a standard heuristic. Figure 1 illustrates the results from 200 closest string
problems. Each problem was run ﬁrst with smallest domain variable ordering and ascending value ordering (Minion
defaults), and then with PWM-based variable and value ordering as described in Section 2.1.
For exact solutions – upper panel of Figure 1 – we observe an improvement of PWM over SDF in almost all cases.
The speedup is as high as several orders of magnitude in some cases. The diﬀerence is statistically signiﬁcant at the
0.001 level. These results are as expected: the PWM reﬂects the maximum likelihood of a closest string, so a search
that respects these likelihoods will nearly always be highly eﬃcient, but will visit very many non-essential nodes on the
few occasions that the maximum likelihood does not lead to a closest string. A key observation is that the magnitude of
speedup increases with increasing string length, which is highly encouraging since the complexity of closest sting is
exponential in string length.
If we consider only the search eﬀort needed to ﬁnd an optimal solution (not taking into account the work needed to
provide a certiﬁcate of optimality by ruling out closer strings at lower distance) then the speedup of PWM over smallest
domain is at the level of orders of magnitude in the general case – Figure 1, lower panel. This indicates that heuristics
are less important when searching exhaustively at a lower than optimal distance: most of the practical complexity of
closest string search is associated with providing certiﬁcates of optimality, rather than identifying close strings which
turn out to be optimal.
Figure 2 shows that we achieve a good approximation very quickly, in line with existing results that guarantee
approximation to four thirds of optimality in polynomial time [4, 5]. This motivates the hybrid symbolic-numeric
methods detailed in Section 3.4: practitioners can use CSP to obtain good bounds quickly, then use either numeric
methods or AI search methods – or indeed both using a distributed architecture – to explore the remaining search space
for an exact solution plus certiﬁcate of optimality.
Taken together the results indicate that: (i) CSP search with PWM variable-value ordering will (in general)
eﬃciently ﬁnd candidate solutions to closest string problems with decreasing maximum distance d; (ii) CSP search
with a standard search ordering can be used to exhaustively rule out the distance below the optimal d; (iii) our empirical
evidence is in line with previously reported results: an approximate solution to closest string can be computed in
polynomial time, but computation of the necessary certiﬁcates of optimality remains intractable in the general case; and
(iv) sequential, single-processor CSP search for problems having more strings of greater length (and possibly a larger

Tom Kelsey et al. / Procedia Computer Science 4 (2011) 1062–1071

1067

times optimal distance

1.8

1.6

1.4

1.2

1.0
0.0001

0.0100

1.0000

100.0000

10000.0000

time [s]

Figure 2: Convergence towards optimal Hamming distance. The upper line is the maximum relative distance, the lower line the minimum and the
middle line the mean of the 200 experiments performed. The y axis denotes multiples of the optimal distance, the x axis denotes CPU time for the
PWM heuristic on a logarithmic scale. The experiments were conducted on a dual quad-core 2.66 GHz Intel X-5430 processor with 8 GB of RAM.

alphabet) will become intractable due to the inherent NP-completeness of closest string.

Figure 3: Empirical data from 6 instances of 5 randomly generated strings of length 25. SDF and PWM indicate smallest domain and position weight
heuristics respectively. All timings were calculated using a dual quad-core 2.66 GHz Intel X-5430 processor with 16 GB of RAM.

2.3. All Closest Strings
To our knowledge, no study has investigated the problem of ﬁnding all closest strings for a given set S . We
investigate the eﬀect that modelling has on the set of all solutions. In our CSP model described in Section 2 we reduce
the search space for a ﬁrst solution by forbidding any variable to take a value that is not present at that position in one
of the input strings.
In Figure 3 we show the results of sample calculations. We see that in all cases (columns headed PWM Restricted
Domains) it is relatively easy using Minion to identify all closest strings if we restrict search to those alphabet symbols
that have non-zero values in the position weight matrix for the instance. We also ﬁnd that search using PMW ordering
heuristics performs marginally better than straightforward smallest domain heuristics.
When the variable-value assignments that have been ruled out because the value does not appear at the variable’s
position in any element of S are added back to the domains of the search variables, we can perform full search for
all closest strings (Figure 3, columns headed by Unrestricted Domains). The percentage of new closest strings found
ranges from 0% to 22%, but increase in search is typically two orders of magnitude. It should be noted that: (i) Minion
is searching far fewer than the 425 possible search nodes for each instance, the majority being pruned by eﬃcient

1068

Tom Kelsey et al. / Procedia Computer Science 4 (2011) 1062–1071

propagation of the logical consequences of the variable-value assignments implicit at each node, and (ii) Minion is
searching 250,000 – 300,000 nodes per second in addition to the work involved in identifying search sub-trees that
need not be explored.
3. Distributed and Hybrid Computing Strategies
Input : A CSP Υ, a cutoﬀ period T max and a branching factor K
Output : Either the ﬁrst solution, or a guarantee that there are no solutions
while not Solved?(Υ) do
Send Υ to a node
Run solver with input Υ for 0 ≤ t ≤ T max
if Solved?(Υ) then
Return solution
else
Υ ← Υ with new constraints ruling out search already performed
Split Υ into K subproblems Υ1 , Υ2 , . . . , ΥK
do in parallel
for 1 ≤ k ≤ K do
Solve(Υk , T max ,K)
end
end
end
end
Algorithm 1: A recursive distributed algorithm to solve any CSP

3.1. Distributed CSP
Given the inherently exponential increase in search eﬀort involved in providing a certiﬁcate for an optimal closest
string distance by ruling out any closest strings with with lower distance, the exact solution of large-scale problems is
not expected to be tractable using purely sequential search. In this Section we describe algorithms that distribute search
across multiple compute nodes. These algorithms will solve closest string problems either on a cluster (a local group of
homogeneous nodes), a grid (a more loosely coupled, heterogeneous and geographically dispersed set of nodes), or a
cloud (a set of an unknown number of nodes in unknown locations, each having unknown architecture and resource).
For our purposes we do not require any communication across nodes, and can therefore treat the three distributed
paradigms as a single approach. Empirical evaluation on clouds is hindered because clocks of virtual machines can be
slowed down or sped up by the VM management software. We therefore prototype on a cluster, and, when satisﬁed that
it is eﬃcient, deploy using a cloud to take advantage of the very large number of nodes available.
Algorithm 1 gives the basic structure of our distributed search. The predicate Solved? returns true whenever the
input CSP ﬁnds the ﬁrst solution or ﬁnishes searching the entire tree without ﬁnding a solution. It returns false if either
the computation has timed out, or the node has stopped work for some reason. If all solutions to the CSP are required,
then we modify Algorithm 1 so that all solutions found so far are returned whenever the Solved? predicate fails.
3.1.1. Distributed CSPs Using Minion
In common with Integer Programming problems, CSPs distribute naturally across multiple compute nodes [12].
Signiﬁcant research has been invested in the distribution of CSPs across multiple computers [13, 14, 15]. In particular
the area of balancing the load among the nodes is an area of active research [16].
We choose a technique that does not impose any constraints on the problem to be solved and is targeted towards
very large problems. Our algorithm closely follows Algorithm 1 – we run Minion with a time out and when this time
out is reached, we split the remaining search space into two parts. The subproblems are inserted into a FIFO queue and
processed by the computational nodes, splitting them again if necessary.

Tom Kelsey et al. / Procedia Computer Science 4 (2011) 1062–1071

1069

The main advantage of our way of distributing problems is that we explicitly keep the split subproblems in ﬁles.
This means that at any point we can stop, suspend, resume, move or cancel the computation and lose a maximum of
timeout × n seconds of work, much less in practice. Apart from contributing to the robustness of the overall system,
we can also easily move subproblems that cannot be solved using the available computational resources, for example
because of memory limitations, to nodes with a higher speciﬁcation that are not always available to us.
In the absence of global symmetry breaking constraints that can aﬀect diﬀerent parts of the search tree, it is easy to
subdivide a typical CSP into several non-overlapping sub-problems. Although there is an inherent latency in sending
problem instances to, and receiving solutions from, either a grid or a cloud, for large enough problems a speedup linear
in the number of compute nodes is achieved. Preliminary results using a computational grid indicate that a super-linear
speedup can be achieved using Minion, whenever a root node consistency check reduces the search tree.
Cloud computing is becoming an important computational paradigm, and the Minion developers have produced
robust, fault-tolerant, methods for distributing Minion instances across diﬀerent underlying architectures, including
clouds. By leveraging existing technologies, in particular the Condor distributed computing framework [17], we
can distribute problems across hundreds of CPUs and combine cluster, grid and cloud architectures for web-scale
computing. This enables us to tackle problems which have previously been thought to be unsolvable because of the
amount of computation required to ﬁnd a solution.
3.2. Distributed Closest String
Input : Υ(S , dmin , HD(S )), T max and K
Output : A closest string to S with its maximum Hamming distance to S
for 0 ≤ t ≤ T max do
RunΥ(S , dmin , HD(S )) in Minion
if Solved?(Υ(S , dmin , HD(S ))) then
Return CS and d, and halt all computation
else dhigh ← the best d found so far
Υ(S , dmin , HD(S )) ← Υ(S , dmin , dhigh ) plus constraints ruling out search already performed
end
end
do in parallel
for dmin ≤ dlow < dhigh do
DistSolve(Υ∗ (S , dmin , dlow ), T max , K)
if Solved?(Υ∗ (S , dmin , dlow )) then
Return CS and d = dlow , and halt all computation
else Update all (sub-)instances with new lower bound dlow + 1
end
end
for 2 ≤ k ≤ K + 1 do
DistSolve(Υk (S , dmin , dlow ), T max , K)
if Solved?(Υk (S , dmin , dhigh )) with dnew < dhigh then
if dnew = dhigh − 1 then Return CS and d = dnew , and halt all computation
else Update all (sub-)instances with upper bound dhigh = dnew
end
end
end
if not Solved?( Υk (S , dmin , dhigh ) ∀ k) ∧ not Solved?(any ﬁxed dlow instance) then
Return current dhigh as d, and the string found that achieved distance dhigh as CS
end
Algorithm 2: Solve the CS Υ(S , dmin , HD(S )) by distributing search for high and low distances
Algorithm 2 describes our approach to the distributed solution of closest string problems formalised as Constraint
Satisfaction problems. We ﬁrst run Minion on the original problem with the PWM ordering heuristic as a single process.

Tom Kelsey et al. / Procedia Computer Science 4 (2011) 1062–1071

1070

Our empirical evaluation in Section 2.2 indicates that nearly always this process will highly eﬃciently lower the upper
bound for the problem. Once we have a reasonable upper bound, we start searching for the optimal distance both above
and below. From above, we carry on optimising as before, but we use the recursive DistSolve algorithm to distribute.
From below we create instances each having a ﬁxed distance, the idea being to exhaustively rule out any closest
strings at these distances. These instances are run on the computational nodes at the same time as the optimisation
sub-problems. If at any stage we obtain a candidate closest string at a distance for which all lower distances have been
ruled out, then this is our solution. This can happen both from above and below.
As mentioned in Section 3.1.1, we expect a super-linear speedup by performing a root node consistency check for
each sub-instance. By keeping track of the best distance obtained so far during search from above, and of any lower
distances for which no solution has been found, we expect to obtain a further super-linear speedup in the majority of
instances. A large part of the search tree is pruned by updating all instances (either waiting for input to a node, or
currently being processed by a node) with improved distance bounds as they become available.
3.3. Preliminary Evaluation of Distributed Closest String
For a ﬁrst evaluation, we ran the algorithm on 6 random strings of lengths 25, 26, 27, 28, 29 and 30. We chose a
time limit of 1 hour to reduce communication overheads. The problems with strings of length 25, 26 and 27 were solved
to completion within this limit. The remaining three instances were split after one hour and distributed across multiple
machines. As suggested by Figure 2, the solutions converged towards the optimal distance extremely quickly. For only
one of the instances was a better Hamming distance found in one of the sub-instances. The remaining sub-instances
proved the optimality of the previously found solution.
These tests demonstrate the practical applicability of our distributed approach. Our experience with the distributed
solution of other classes of CSP strongly suggests that our system will scale seamlessly to grids or clouds: there is no
communication across nodes, a node failure can be recovered from with no extra search needed (the search tree already
explored is reported whenever search is interrupted for any reason), and the order in which sub-instances are solved can
be tuned.
3.4. Hybrid Methods
Input : Υ0 (S , dmin , HD(S ))
T OL, a limit for the gap between the highest and lowest computed distances
Output : A closest string to S with its maximum Hamming distance to S
Seek closer distance bounds for Υ0 (S , dmin , HD(S )) using CSP alone;
while |dhigh − dlow | < T OL do
Run Algorithm 2 on Υ0 (S , dmin , HD(S ))
Output dlow and dhigh when updated
end
Once bounds are close enough, send to numeric IP or linear time search;
if T OL > 1 ∧ |dhigh − dlow | ≤ T OL then
Formulate the remaining problem as an Integer Programming problem
Search for solution using numeric branch and bound
end
if |dhigh − dlow | = T OL = 1 then
Formulate the remaining problem as a ﬁxed d instance
Search for solution using linear time methods
end
Algorithm 3: Solve the CS Υ(S , dmin , HD(S )) using hybrid CSP and numerical methods
The empirical results obtained so far suggest that CSP formulation with PWM ordering is an eﬀective approach for
ruling out high distances: Minion will often ﬁnd a ﬁrst solution very quickly, given the search space involved. However,
at least for the approach suggested in this paper, CSP formulation requires much more time to provide a certiﬁcate
that an optimal solution is indeed optimal. As discussed in Section 1.1, eﬃcient methods have been reported in the

Tom Kelsey et al. / Procedia Computer Science 4 (2011) 1062–1071

1071

literature for when the upper and lower distance bounds are close [7], and for problems where the distance is ﬁxed [8].
In this Section we propose a hybrid approach that aims to take advantage of the best methods available. Algorithm 3
takes a closest string instance and partially solves it using Algorithm 1. If the upper and lower bounds come to within a
pre-deﬁned tolerance, then numeric branch and bound methods are used to solve an Integer Programming formulation
of the problem not yet solved by Minion. If the distance under consideration ever becomes ﬁxed, then the linear time
methods set out in [8] can be applied.
It should be stressed that these three methods (CSP, IP branch and bound, and linear time) need not be exclusive:
once tolerance achieving bounds are found by Minion, the distributed Minion search can continue, and the CSP and
numeric methods are then competing to ﬁnd the ﬁrst solution. This of course pre-supposes that computational resource
is not a problem, but that is why we are using web-scale facilities in the ﬁrst place.
4. Discussion
We have performed the ﬁrst evaluation of Constraint Satisfaction as a computational framework for solving closest
string problems. We have shown that careful consideration of symbol occurrences can provide search heuristics that
give, in general, several orders of magnitude speedup when computing approximate solutions. We have also shown that
CSP is less eﬀective when searching for certiﬁcates of distance optimality. This result motivated our detailed description
of algorithms for web-scale distributed CSP computation, and also our design of hybrid distributed algorithms that can
take advantage of the strengths of both numeric and CSP computational techniques.
We have also performed the ﬁrst analysis of the computational diﬃculties involved in the identiﬁcation of all
closest strings for a given input set, irrespective of the computational framework used. Our results for all closest strings
motivate the question of which deﬁnition of self-similarity is suitable for the computational biology setting. In terms
of information theory the all closest strings problem can not exclude alphabet symbols and still be correct. However,
when seeking to quantify the self-similarity of DNA sequences it may be perfectly justiﬁable to exclude closest strings
that can have no symbol in common with the sequences in question at a given point. If this were to be the case, then the
computational eﬃciency of the search for all closest strings would be greatly increased (Figure 3).
We have designed, implemented and deployed a computational methodology for distributed search for closest string
solutions. This contribution provides a practically useful means of attacking the NP-complete instances by division
into smaller sub-problems. Our system is guaranteed never to perform the same search twice, will recover seamlessly
from any unforseen loss of compute nodes, and is extendable to web-scale clouds.
References
[1] G. Z. Hertz, G. W. Hartzell, G. D. Stormo, Identiﬁcation of consensus patterns in unaligned DNA sequences known to be functionally related.,
Comput Appl Biosci 6 (2) (1990) 81–92.
[2] F. Chin, H. C. Leung, DNA motif representation with nucleotide dependency., IEEE/ACM transactions on computational biology and
bioinformatics / IEEE, ACM 5 (1) (2008) 110–119.
[3] M. Frances, A. Litman, On covering problems of codes, Theory Comput. Syst. 30 (2) (1997) 113–119.
[4] J. K. Lanctot, Some string problems in computational biology, Ph.D. thesis, University of Waterloo (2004).
[5] M. Li, B. Ma, L. Wang, On the closest string and substring problems, J. ACM 49 (2) (2002) 157–171.
[6] B. A. Julstrom, A data-based coding of candidate strings in the closest string problem, in: GECCO, ACM, 2009, pp. 2053–2058.
[7] C. N. Meneses, Z. Lu, C. A. S. Oliveira, P. M. Pardalos, Optimal solutions for the closest-string problem via integer programming, INFORMS
J. on Computing 16 (4) (2004) 419–429.
[8] J. Gramm, R. Niedermeier, P. Rossmanith, Exact solutions for closest string and related problems, in: Proc. ISAAC, 2001, pp. 441–453.
[9] F. Rossi, P. van Beek, T. Walsh (Eds.), The Handbook of Constraint Programming, Elsevier, 2006.
[10] I. P. Gent, C. Jeﬀerson, I. Miguel, Minion: A fast scalable constraint solver, in: Proc. ECAI, 2006, pp. 98–102.
[11] A. Distler, T. Kelsey, The monoids of orders eight, nine & ten, Ann. Math. Artif. Intell. 56 (1) (2009) 3–21.
[12] M. Yokoo, Distributed constraint satisfaction: foundations of cooperation in multi-agent systems, Springer-Verlag, London, UK, 2001.
[13] Z. Collin, R. Dechter, S. Katz, On the feasibility of distributed constraint satisfaction, in: Proc. IJCAI, 1991, pp. 318–324.
[14] M. Yokoo, E. H. Durfee, T. Ishida, K. Kuwabara, The distributed constraint satisfaction problem: Formalization and algorithms, IEEE Trans.
on Knowl. and Data Eng. 10 (5) (1998) 673–685.
[15] L. Michel, A. See, P. V. Hentenryck, Parallelizing constraint programs transparently, in: CP, 2007, pp. 514–528.
[16] C. C. Rolf, K. Kuchcinski, Load-balancing methods for parallel and distributed constraint solving, in: CLUSTER, 2008, pp. 304–309.
[17] T. Tannenbaum, D. Wright, K. Miller, M. Livny, Condor – a distributed job scheduler, in: T. Sterling (Ed.), Beowulf Cluster Computing with
Linux, MIT Press, 2001.

