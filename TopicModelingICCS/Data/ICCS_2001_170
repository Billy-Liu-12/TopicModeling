Eﬃcient Random Process Generation for
Reliable Simulation of Complex Systems
Alexey S. Rodionov1 , Hyunseung Choo2 , Hee Yong Youn2 , Tai M. Chung2 , and
Kiheon Park2
1

Novosibirsk Institute of Computational Mathematics and Mathematical Geophysics
Siberian Branch of the Russian Academy of Sceiences, Novosibirsk, Russia
alrod@rav.sscc.ru
2
School of Electrical and Computer Engineering
Sungkyunkwan University, Suwon, Korea
{choo, youn,tmchung,khpark}@ece.skku.ac.kr
Abstract. Generating pseudo random object is one of the key issues in
computer simulation of complex systems. Most earlier systems employ
independent and identically distributed random variables, while those
of real processes often show nontrivial autocorrelation. In this paper
a new approach is presented that generates random process for given
marginal distribution and autocorrelation function. The proposed approach achieves signiﬁcant speed-up and accuracy by using truncated
distribution instead of order statistics. An experiment displays more than
ten times speed-up for reasonable size system. Moreover, the proposed
generator is simple and requires less memory.

1

Introduction

In developing a stochastic model to describe a real system, a model realistically
replicating the actual operation should be chosen. Even though mathematical
analysis is a useful tool for predicting the performance of a system wherever it
is tractable, computer simulation is gaining more attention due to the relatively
recent advent of fast and inexpensive computational power. It allows faithful
reproduction of the behavior of real processes and comprehensive analysis based
on the data collected.
For computer simulation of complex systems pseudo random objects of various nature need to be generated [1,2]. In most general purpose discrete simulation
independent and identically distributed random variables are usually used. However, the random numbers of real processes often show nontrivial autocorrelation.
For example, observe that the two processes of a same exponential marginal distribution (λ = 1) in Figure 1 look quite diﬀerent due to diﬀerent autocorrelation
functions. For an M/M/1 queue with the service rate of 1.5 and inter-arrival rate
of 1, the average waiting time is 7.94. For the inter-arrival distribution of Figure
1(a) and (b), it is 12.14 and 7.62, respectively. The diﬀerence is signiﬁcant based
on the t-criteria with a signiﬁcance level of 95%. This exempliﬁes the importance
of simulation with dependent random numbers.
V.N. Alexandrov et al. (Eds.): ICCS 2001, LNCS 2074, pp. 912–921, 2001.
c Springer-Verlag Berlin Heidelberg 2001
�

Eﬃcient Random Process Generation for Reliable Simulation

913

A number of researches related to this problem have been done, while they are
for generating mostly correlated vectors and ﬁelds [3 − 6]. The method commonly
employed for generating dependent random vectors [3,7], however, is not suitable
for long one-dimensional sequence due to the requirement on large size memory.
As a result, other methods were developed to generate stable one-dimensional
processes with some given properties and arbitrary length [8 − 10]. Based on the
theoretical foundation on generating random processes with required properties
[10,12], we propose a new method for generating pseudo random processes with a
given marginal distribution and autocorrelation function (ACF). Such processes
often allow us to faithfully replicate the actual operation of the simulated system.
For a 16-state randomized Markov chain about 14 times acceleration is achieved
in comparison with the previous algorithm [12], while the storage requirement
is much smaller. Moreover, the proposed generator is simple.

Fig. 1. Two diﬀerent processes with the same marginal distribution.

2

Basic Concepts and Notation

Let ξ1 , ξ2 , . . . , ξN be a set of independent random variables with identical continuous distribution F (x), and let ξ(1) , ξ(2) , . . . , ξ(N ) be the same set of random
variables ranked in non-decreasing order. It is assumed that the ordering rule
does not change by time as
P (ξ(i) (t) < x) = P (ξ(i) < x) = Fi (x).
We obviously have
N
�
i=1

Fi (x) = N × F (x).

914

A.S. Rodionov et al.

Let Ψ be a homogeneous Markov chain independent of ξi (t) with a set of states
i = 1, 2, . . . , n, a vector of initial probabilities, π0 , and two-stochastic matrix of
transition probabilities:
(P (Ψt+1 = j | Ψt = i)) = (pij ) = P
N
N
�
�
pij = 1;
pij = 1; i, j = 1, 2, . . . , N.
i=1

Let the process

(1)

j=1

ξt = ξΨt (t), t = 0, 1, 2, . . .

be a randomized Markov chain (RMC).
If the initial probabilities of Markov chain are π0 = (1/N, . . . , 1/N ), then the
ﬁnite-dimensional distribution of the process is:
P (ξt1 < x1 , . . . , ξtm < xm ) = 1/N ×

N
�
j1 ,...,jm =1

[Fj1 (x1 ) × . . . × Fjm (xm )×

�
pj1 j2 (t2 − t1 ) × . . . × pjm−1 jm (tm − tm−1 ) .
The marginal distribution and ACF of randomized Markov chain above are:
Fξ (x) = 1/N ×
r� (τ ) =

N
�

Fi (x) = F (x),

(2)

i=1

�
�
N
1 �
1
,
Mi Mj p�ij (τ ) −
N Dξ i,j=1
N

τ = 1, 2, . . .

(3)

where Dξ is variance of the marginal distribution and Mi is the expectation of
order statistics ξ(i) as:
�∞
x dFi (x).
Mi =
−∞

To obtain dFi (x) of the expression above we use the well-known expression for
probability elements of order statistics [11]:
Γ (n + 1)
N −i
F i−1 (x) [1 − F (x)]
f (x) dx
Γ (i)Γ (N − i + 1)
N!
N −i
F i−1 (x) [1 − F (x)]
=
f (x) dx
(i − 1)!(N − i)!
�
�
N
N −i
f (x) dx
= (N − i + 1)
F i−1 (x) [1 − F (x)]
i−1

dFi (x) =

(4)

Based on this we obtain an algorithm generating pseudo random process
of a given distribution. It consists of two parts: obtaining a new state of the
Markov chain and generating a random number according to the corresponding
distribution.

Eﬃcient Random Process Generation for Reliable Simulation

915

The algorithm for simulating a Markov chain with a given matrix of transition probabilities is well-known. The following is the algorithm proposed for
generating a Markov chain [9].
Algorithm G1.
Input: N is the number of states of the Markov chain; n is the required length
of the process realized.
Output: an array X(1 × n) of a process realization. The algorithm consists of the
following steps:
Step 1 (Preliminary). Put a sample of a non-correlated sequence with the
given marginal distribution F (x) into an urn with reset. The counter k is set
to one. The initial state of Markov chain is gained uniformly from 1, 2, . . . , N .
Step 2. From an urn, ξi (i = 1, . . . , N ) are extracted by a random fashion
and then ranked in non-decreasing order. Let denote resulting variables as
ξ(i) (i = 1, . . . , N ).
Step 3. By the given two-stochastic matrix of transition probabilities the next
state (say j) of the Markov chain is generated.
Step 4. ξj is copied to output as Xk , and k is increased by one.
Step 5. If k > n, the generation comes to an end. Otherwise, go to Step 2.
Since Algorithm G1 has several shortcomings [13], we propose an improved
algorithm.
Algorithm G2.
Input and output are same as for Algorithm G1.
Step 1 (Preliminary). The initial state of Markov chain is gained uniformly
from 1, 2, . . . , N . The counter k is set to one.
Step 2. By the given two-stochastic matrix of transition probabilities the next
state (say j) of the Markov chain is generated.
Step 3. Generation of pseudo random number, ξ, by a truncated distribution
Fj (x) on j-th inter-fractile interval. ξ is copied to output as X k , and k is
increased by one.
Step 4. If k > n, the generation comes to an end. Otherwise, go to Step 2.
The cumulative distribution function Fi (x) whose distribution is truncated
on interval (xi−1 , xi ) is

for x < xi−1

 0,
F (x)−F (xi−1 )
Fi (x) = F (x )−F (x ) , for xi−1 ≤ x < xi .
i
i−1


1,
for x ≥ xi ,
If xi is i-th fractile of the marginal distribution, we obtain a simpler expression


for x < xi−1
 0,
),
for
xi−1 ≤ x < xi .
(5)
Fi (x) = N (F (x) − i−1
N

 1,
for x ≥ xi ,

916

A.S. Rodionov et al.

The Expressions (2) and (3) are still true, but now Mi ought to be obtained
using the distribution (5), so:
�xi

�xi

x�i

�
x dFi (x) = x Fi (x) � −

Mi =

xi−1

xi−1

xi−1

�xi
Fi (x) dx = xi −

Fi (x) dx

(6)

xi−1

For obtaining the transition probability matrix we need to solve the system
(3) of nonlinear equations of degree τ . The solutions were found only for some
special kinds of two-stochastic matrix [10], whereas it is necessary to have a
general solution for real problems that could be done only numerically. We next
present how to determine transition probability matrix.

3

Determination of Transition Probability Matrix

In determining transition probability matrix, we need to minimize the diﬀerence
between real ACF and ACF of simulated process by the elements of the two
stochastic matrices. If we use the sum of squares of the diﬀerences as a distance,
then we have to solve the following optimization task:
�
Φ(τ ) = (r(τ ) − r � (τ ))2 → min
τ

r� (τ ) =
N
�

1
N Dξ

Pij� = 1;

i=1

N
�
i,j=1

N
�

Mi Mj (Pij� (τ ) −

1
N)

Pij� = 1; i, j = 1, 2, . . . , N

(7)

(8)

j=1

∀i, j 0 ≤ Pij� ≤ 1,
where Pij� (τ ) is the element of the i-th row and j-th column of the derivable
matrix in power τ . r � (τ ) is the ACF of a randomized Markov chain corresponding
to the matrix and given marginal distribution.
Molchan had stated [9] that the gradient optimization method is not suitable
for solving the optimization problem of Expressions (7) and (8). Instead, he
oﬀered a heuristic method of stochastic optimization and a modiﬁcation of it.
Our experiments with various optimization methods revealed that the simple
gradient method of “Quickest descent” with the use of penalty functions is the
best, and thus it is adopted here.
First of all, let us free the task from the restrictions of two-stochasticity,
Expression (8), by using the following substitutions:
�
=1
PiN

PN� N =

−

N
−1
�

Pij� ,

PN� i

j=1

N
−1
�

i,j=1

Pij� − N + 2.

=1−

N
−1
�
j=1

�
Pji
,

i = 1, . . . , N − 1,

(9)

(10)

Eﬃcient Random Process Generation for Reliable Simulation

917

Then we can formally transform our optimization problem to a problem of unconstrained optimization by adding a penalty function:
(k)

(l)

(m)

(k)

Q(P � ij , ρij , δij ) = Φ(P � ij ) +

N
−1
�
i,j=1

�

�
(k)
(k)
(l) (k)
(l) (k)
ρij Uij (P � ij )2 + δij Vij (P � ij − 1)2 ,

i, j = 1, . . . , N − 1,

where

1 if P � ij < 0, (index operator),
� 0 otherwise
�
(k)
Vij = 1 if P ij > 1, (index operator),
0 otherwise
(l)
ρij =10l (weight coeﬃcients), where l is the amount of iterations from the mo(k)

Uij

=

�

(k)

ment of P � ij < 0, l ≤ 100;

(m)

δij =10l (weight coeﬃcients), where m is the amount of iterations from the
(k)

moment of P � ij > 1, l ≤ 100.

Derivation dQ/dpkl (1 ≤ k, l ≤ N ) is deﬁned by the following expressions
(hereafter we will omit (k) and apostrophe at P for better readability of expressions):
N
−1 �
�
�
dΦ
dQ
(l)
(l)
ρij Uij Pij − δij Vij (Pij − 1)
=
+2
dpkl
dpkl
i,j=1


�
�
N
N
�
dPij (τ )
1 � 1 �
1
dΦ
=
Mi Mj Pij (τ ) −
Mi Mj
− r(τ )
dpkl
N Dξ τ
N Dξ i,j=1
N
dpkl
i,j=1

In [13], we show how the computation steps can be minimized. Experiments
display very fast access to the minimum area in about 3 to 5 steps and abrupt
deceleration of convergence. The deceleration rate greatly depends on the length
of ACF; it is almost imperceptible for n < 10, while advancement to optimum
slows down very much for n > 30. This might be due to the cancellations in
the calculation of multiple sums and products of the numbers of very small
magnitudes. The number of operations grows exponentially with τ . Use of special
numerical methods and high-precision calculations could improve the situation.
In case when high accuracy is needed, we propose the following optimization
scheme.
Step 1. Input the initial two-stochastic matrix, say unitary matrix or matrix
with equal elements.
Step 2. Find a matrix for ACF with a length restricted to a small number.
Step 3. Using the result of the previous step, ﬁnd a matrix for ACF with the
needed length.
Step 4. If the accuracy is not suﬃcient, use random optimization for more
precise result.

918

A.S. Rodionov et al.

For our experiments we choose one of the worst cases in the point of view of
the diﬀerence from the examples given in [10]. Here the marginal distribution is
exponential with λ = 1, N = 10, and ACF r(τ ) = e−0.1τ cos(0.4τ ),
The diﬀerence displayed by the algorithm above with 2500 random steps was
about 10%. Higher accuracy can be achieved using the proposed optimization
scheme. First, by sequential execution of the gradient optimization for an ACF
length from 5 to 40 with step 5, we obtain a diﬀerence of 9.6% in not more than 10
steps for each length. Consequent execution of 500 steps of random search with
one-dimensional optimization gives us the diﬀerence of 6.1%. The computation
time was also signiﬁcantly reduced as about 3 minutes on the 600MHz Pentium
III PC. If the number of states is increased to 16, then a diﬀerence 2.4% is
archived for the ACF, but the computation takes more than 20 minutes. The
diﬀerence with gradient minimization is 14.6%. We next compare Algorithm G1
and G2 for generating random processes with exponential marginal distribution.

4

Case Studies

Algorithm G1 According to (4) the distribution density of ξi in our case is
�
�
�i−1 −λ(N −i+1)x
N �
fi (x) = λ(N − i + 1)
1 − e−λ x
e
,
i−1

and, consequently, the expectation for ξi is
�
�
� ∞
� ∞
�i−1 −λ(N −i+1)x
N �
Mi =
xfi (x) dx =
xλ(N − i + 1)
1 − e−λ x
e
dx =
i−1
0
0
�
�� ∞
�
�i−1 −(N −i+1)t
1
N
(N − i + 1)
t 1 − e−t
e
dt =
i−1 0
λ
�
� i−1
�
�� ∞
1
N � (−1)i−j−1 i − 1
(N − j)te−(N −j)t dt.
(N − i + 1)
i−1
j
λ
N
−
j
0
j=0
The last integral is expectation of the exponential distribution with parameter
N − j. Hence the ﬁnal result for Mi is:
�
� i−1
�
�
Ki
N −i+1
N � (−1)i−j−1 i − 1
,
=
Mi =
2
i−1
j
λ
(N − j)
λ
j=0
where Ki is a coeﬃcient that depends only on the values of i and N . N
is decided as some not very large constant independent on the distribution
parameter λ. When N is ﬁxed, all Ki ’s can be easily calculated.
Algorithm G2 The cumulative distribution function Fi (x) is determined by Expression (5) and expectations Mi by Expression (6). For the exponential marginal
distribution considered, the i-th fractile of exponential distribution is equal to
�
�
N −i
1
, 1 ≤ i ≤ N − 1,
(11)
xi = − ln
λ
N

Eﬃcient Random Process Generation for Reliable Simulation

919

where N is the number of intervals (for the uniformity of expressions, we also add
the leftmost and rightmost values: x0 = 0 and xN = ∞). We obtain an expression
for the cumulative distribution function of truncated exponential distribution on
the interval between the fractiles from Expressions (5) and (11).
Fi (x) = N (1 − e−λ x ) − i + 1.

(12)

The corresponding expectations are
�
�
(N −i)N −i
Mi = λ1 1 + ln (NN−i+1)
=
N −i+1
1
λ

(1 + ln N + (N − i) ln(N − i) − (N − i + 1) ln(N − i + 1)) =

Ki∗
λ ,

i = 1, . . . , N − 1
MN =

1
λ

(1 + ln N ) =

∗
KN
λ

As for Algorithm G1, the expectation Mi has inverse negative relationship
with the distribution parameter λ. Note that calculating Ki∗ ’s in Algorithm G2
is much simpler than calculating Ki ’s in Algorithm G1, i.e., it does not require
to calculate the sums and binomial coeﬃcients.
For the generation process we need to produce the samples from the total population with exponential distribution for Algorithm G1, while from the
truncated interval [xi − 1, xi ) for Algorithm G2. For detail, refer to [13]. As the
Algorithm G2 does not need the sorting step, it is much faster than G1. For
generating 100,000 numbers for a 16-state randomized Markov chain, 1.86 sec
was taken with Algorithm G2. It was 25.04 sec with Algorithm G1, which is
more than 13 times slower than the proposed approach. The accuracy of the
proposed ACF approximation scheme is illustrated in Figure 2. Observe that the
diﬀerence with the proposed algorithm is smaller than that of Algorithm G1. In
Figure 3, we illustrate an example of ACF
�
1 − 0.05τ if 0 < τ ≤ 20,
r(τ ) =
0
otherwise
which shows the signiﬁcance of proper choice of maximal ACF length, n. If we
have n = 20, the ACF is well approximated up to τ = 20 but the diﬀerence
becomes large beyond that point. For n = 25, we achieve better result, and
n = 30 allows a really good approximation. Yet on the interval [1, . . . , 20] , all
approximations are almost equally good. In this example, the number of states,
N , is 16.

5

Conclusion

In this paper we have proposed an algorithm for generating randomized Markov
chains with given marginal distribution and ACF. The proposed approach
achieves signiﬁcant speed-up and accuracy by using truncated distribution
instead of order statistics. Randomized Markov chains can approximate diﬀerent

920

A.S. Rodionov et al.

Fig. 2. Comparison of given and obtained ACFs.

Fig. 3. Inﬂuence of an ACF length truncation on the approximation accuracy (based
on algorithm G2).

Eﬃcient Random Process Generation for Reliable Simulation

921

stationary random processes when only the marginal distribution and ACF are
taken into account. The accuracy of an ACF approximation is good enough if the
numbers of the states of the Markov chain is not large. Usually, not more than
20 states provide a suﬃciently small diﬀerence between the approximated ACF
and that of the randomized Markov chain. On the other hand, with the number
of states more than 20, it is almost impossible to complete the optimization
task for the transition probability matrix on modern PC in reasonable time.
We have also presented an optimization approach using penalty functions. In
the future researches we will try to improve the quality of optimization. A
signiﬁcant improvement will also be achieved by using parallel algorithms for
matrix computation.
Acknowledgement. This work was supported in part by Brain Korea 21
project and grant No. 2000-2-30300-004-3 from the Basic Research Program
of Korea Science and Engineering Foundation.

References
1. Al-Shaer, E., Abdel-Wahab, H., Maly, K.: HiFi: A New Monitoring Architecture
for Distributed Systems Management. The 19th IEEE Int’l Conf. on Distributed
Computing Systems (1999) 171-178
2. Gribaudo, M., Sereno, M.: Simulation of Fluid Stochastic Petri Nets. Proceeding
of the 8th MASCOTS (2000) 231-239
3. Dagpunar, J.: Principle of Random Variate Generation. Clarendon Press, Oxford
(1988)
4. Blacknell, D.: New Method for the Simulation of Correlated K-distributed Clutter.
IEE Proc. Rad., Son. Nav., Vol. 141 (1994) 53-58
5. Ronning, G.: A Simple Scheme for Generating Multivariate Gamma Distributions
with Non Negetive Covariance Matrix. Technometrics, Vol. 19 (1977) 179-183
6. Bustos, O.H., Flesia, A.G., Frery, A.C.: Simulation of Correlated Intensity SAR
Images. Proc. XII Brazilian Symp. on Comp. Graph. and Image Proc. (1998) 10
7. Hammersley, J., Handscomb, D.: Monte-Carlo Methods. Methuen&Co, (1964)
8. Letch, K., Matzner, R.: On the Constraction of a Random Process with Given
Power Spectrum and Probability Density function. Proceedings of the 1998 Midwest Symposium on Systems and Circuits, Vol. 41 (1999) 14-17
9. Molchan, S.I., Prelovskaja, A.A., Rodionov, A.S.: Program Complex for Generation
of Random Processes with Given Properties. System modeling, Trans. of Computing Center of Siberian Branch of Academy of Sciences of USSR, Novosibirsk, ISSN
0134-630X, No. 13 (1998) 70-81
10. Molchan, S.I.: About One Approach to Generation of Random Processes with
Given Properties. Simulation of Computing Systems and Processes, Perm’s State
Univercity, USSR (1986) 59-72
11. Wilks, S.: A Mathematical Statistics. Second edition, John Wiley&Sons (1962)
12. Feller, W.: An Introduction to Probability Theory and its Applications. John Wiley&Sons (1971)
13. Rodionov, A.S., Choo, H., Youn, H.Y., Chung, T.M., Park, K.: On Generating
Random Variates for Given Autocorrelation Function. TR-2001-1, School of ECE,
Sungkyunkwan University (2001)

