Procedia Computer Science
Volume 51, 2015, Pages 1791–1800
ICCS 2015 International Conference On Computational Science

Total Least Squares and Chebyshev Norm
ˇ y2
Milan Hlad´ık1 and Michal Cern´
1

Charles University, Department of Applied Mathematics, Prague, Czech Republic
milan.hladik@matfyz.cz
2
University of Economics, Department of Econometrics, Prague, Czech Republic
cernym@vse.cz

Abstract
We investigate the total least square problem (TLS) with Chebyshev norm instead of the
traditionally used Frobenius norm. The use of Chebyshev norm is motivated by the need for
robust solutions. In order to solve the problem, we introduce interval computation and use
many of the results obtained there. We show that the problem we are tackling is NP-hard in
general, but it becomes polynomial in the case of a ﬁxed number of regressors. This is the most
important practical result since usually we work with regression models with a low number
of regression parameters (compared to the number of observations). We present not only a
precise algorithm for the problem, but also a computationally eﬃcient heuristic. We illustrate
the behavior of our method in a particular probabilistic setup by a simulation study.
Keywords: Total least squares, Chebyshev norm, interval computation, computational complexity

1

Introduction and motivation

Notation.
i |vi |, v

For a vector v ∈ Rn and a matrix M ∈ Rm×n , we use vector Lp norms v 1 =
2
2 =
i vi , v ∞ = maxi |vi |, and matrix norms: Frobenius norm M F =

2 and Chebyshev (max) norm M
Mij
max = maxi,j |Mij |. Next, |M | denotes the entrywise absolute value and Mi∗ the i-th row of a M . Eventually, In stands for the identity
matrix of size n, and E and e for the matrix and the vector of ones (with suitable dimensions),
respectively.
i,j

Motivation. We often need to work with data suﬀering from imprecision, instability, uncertainty or various kinds of errors. For example, in econometrics we often work with estimated
future values suﬀering from prediction errors; in numerical analysis, data are aﬀected by rounding errors; in operations research, data often represent planned values (such as ﬂight times or
processing times) which diﬀer from the true ones. Many further examples of erroneous data
can be found in other areas as well.
This paper is a contribution to a particular problem in regression analysis where erroneous
data can appear both in the output variable and the input variables.
Selection and peer-review under responsibility of the Scientiﬁc Programme Committee of ICCS 2015
c The Authors. Published by Elsevier B.V.

doi:10.1016/j.procs.2015.05.393

1791

Total Least Squares and Chebyshev Norm

ˇ
Hlad´ık and Cern´
y

OLS, TLS and problem formulation. In linear regression and data ﬁtting, we try to ﬁnd
x that “suits best” a given overdetermined system of equations Ax = b. In case of ordinary
least squares (OLS) it is assumed that errors are in b and we are to ﬁnd the closest b such that
Ax = b is consistent, where the distance between b and b is measured by the L2 -norm. It is
well-known that under some circumstances, other norms are also useful; e.g. in robust statistics
one often minimizes b − b 1 or b − b ∞ . Another example of usage of a norm diﬀerent from
L2 is the case when errors follow the Laplace distribution; then the minimization of b − b 1
leads to maximum likelihood estimation.
Total least-squares (TLS) is a natural generalization of OLS admitting errors in both A
and b; see [5, 12, 22]. The usual problem formulation is: ﬁnd (A | b ) such that A x = b is
consistent and (A | b) − (A | b ) F is minimal. The above mentioned ideas from statistics,
leading to the replacement of L2 -norm by another norm, can be reformulated to the TLS case
in a straightforward way. In general, we are interested in solving the TLS problem when the
Frobenius norm is replaced by another norm. (This problem is much less studied than TLS,
see [10, 13, 17, 22, 23, 24].) Said otherwise, the general question is:
given an overdetermined system Ax = b and a matrix norm · , ﬁnd (A | b )
such that A x = b is solvable and (A | b) − (A | b ) is minimal.

(1)

Motivation also arises from the fact that the classical TLS solutions are sometimes illconditioned, even more than OLS solutions [4, 12]. Thus, searching for an appropriate norm in
(1) yielding more robust solutions is of high importance.
Our contribution. This paper is a contribution to the general question (1) for the case of
Chebyshev norm.
Remark 1. Observe that the problem has also applications in numerical analysis. Say that
we know that a system A x = b has a solution, but the exact data A , b of the system are not
available; what is available is an imprecisely computed matrix A and an imprecisely computed
vector b. Let each element of (A | b) diﬀer from the corresponding element in (A | b ) by
at most δ ≥ 0, where δ is assumed to be as small as possible. Then, solving (1) with the
Chebyshev norm can be seen as a method of reconstruction of (A | b ) from (A | b), overcoming
the problem of the imprecise computation.
Interval computation. The main tool of our analysis is theory of computation with interval
vectors and matrices. By an interval matrix we mean a family of matrices
A := [A, A] = {A ∈ Rm×n ; A ≤ A ≤ A},
where A ≤ A are given and inequalities between matrices/vectors are understood entrywise.
The set of all m × n interval matrices is denoted by IRm×n . Interval vectors are deﬁned and
denoted accordingly.
Let A ∈ IRm×n and b ∈ IRm . Then the solution of the interval system Ax = b is any
x ∈ Rn such that Ax = b for some A ∈ A and b ∈ b. Thus, the solution set to Ax = b is
deﬁned as
{x ∈ Rn ; ∃A ∈ A, ∃b ∈ b : Ax = b}.
Notice that we use this deﬁnition not only for square, but also for overdetermined systems.
1792

ˇ
Hlad´ık and Cern´
y

Total Least Squares and Chebyshev Norm

An interval matrix A ∈ IRn×n is regular if every matrix A ∈ A is nonsingular; otherwise A
is singular. Similarly, A ∈ IRm×n has full column rank if every A ∈ A has full column rank.
An enclosure of a set S ⊂ Rn is any interval vector v ∈ IRn such that S ⊆ v.
One of major problems of interval analysis is to ﬁnd a tight enclosure to the solution set
of a given system of interval-valued linear equations. For some classical methods and recent
developments see, e.g., [2, 6, 15, 18]. Overdetermined systems in particular were discussed in
[8, 9].
Interval-theoretic formulation of (1).
optimization problem
min (ΔA | Δb)

max

Let A ∈ Rm×n and b ∈ Rm . We are to solve the

subject to (A + ΔA)x = b + Δb is solvable.

(2)

We can formulate (2) in terms of interval computation as
min δ subject to [A − δE, A + δE]x = [b − δe, b + δe] is solvable.

(3)

In this paper, we will investigate a more general problem
min δ subject to [A − δAΔ , A + δAΔ ]x = [b − δbΔ , b + δbΔ ] is solvable,

(4)

where AΔ ∈ Rm×n and bΔ ∈ Rm are nonnegative.
Remark 2. The fact that we admit a general matrix AΔ ≥ 0 and a general vector bΔ ≥ 0
can give us more ﬂexibility (but the reader could always think of AΔ = E and bΔ = e). We
can, for example, set AΔ := |A| and bΔ := |b|, which results in computation of minimal relative
perturbations of given data such that the system is solvable.
Remark 3. So far, interval computation was used in linear regression to deal with interval input data [1, 7]. Formulation (4) creates a link between linear regression and interval
computation, and opens the door to a possible application of interval techniques in TLS.
Optimal solutions. In the following sections, δ opt denotes the optimal value and xopt denotes
an optimal solution of (4).

2

Determining δ opt and xopt

Consider the interval system Aδ x = bδ , where
Aδ := [A − δAΔ , A + δAΔ ],
bδ := [b − δbΔ , b + δbΔ ].
By the Oettli–Prager theorem [2, 16], the solution set S of this interval system is characterized
as
S = {x ∈ Rn : |Ax − b| ≤ δAΔ |x| + δbΔ }.

(5)
1793

ˇ
Hlad´ık and Cern´
y

Total Least Squares and Chebyshev Norm

This is a nonlinear system, but it can be reformulated by means of 2n linear systems. Actually,
x ∈ Rn is a solution iﬀ there is s ∈ {±1}n such that the linear system
(A − δAΔ Ds )x ≤ b + δbΔ ,
Δ

Δ

(−A − δA Ds )x ≤ −b + δb ,
Ds x ≥ 0

(6a)
(6b)
(6c)

is solvable, where Ds = diag(s). Thus, δ opt , xopt can be determined as
δ opt =

min min{δ ≥ 0; (6) is solvable}.

s∈{±1}n

(7)

The right-hand side optimization problem of (7) can be written as
min

minn

s∈{±1}n x∈R

(−1)1−j Ai∗ x + (−1)j bi
subject to AΔ Ds x + bΔ ≥ 0, Ds x ≥ 0, .
Δ
i∈{1,...,m}
AΔ
D
x
+
b
s
i∗
i
max

j∈{0,1}

( )

The inner optimization problem ( ) is a generalized linear fractional programming problem
(GLFP) solvable in polynomial time using e.g. an interior point method [3, 14]. (The constraint
AΔ Ds x + bΔ ≥ 0 is obviously redundant, but we have stated it explicitly in order it be clear
that ( ) is indeed a GLFP.) We have proved the ﬁrst important result:
Theorem 1. Problem (1) can be solved in time O(2n · p(bitsize(A, b))), where p is a polynomial
and bitsize denotes the length of binary representations of rational numbers in A, b.
The following corollary captures the case which is important for practice: the point is that
we usually have regression models with a low number of regression parameters compared to
the number of observations. (Said otherwise, computation time exponential in the number of
regression parameters is “good news” — an algorithm exponential in the number of observations
would be indeed bad.)
Corollary 1. When the number of regression parameters is ﬁxed, that is
n = O(1),

(8)

then problem (1) can be solved in polynomial time.
Unfortunately, relaxation of the assumption (8) leads to a hardness result, which will be
proved in the next section.

2.1

Complexity

We show that computation of δ opt is NP-hard even for the case with AΔ = E, bΔ = e. NPhardness is proved for a natural decision version of the optimization problem. The hardness
result shows that the computation bound of Theorem 1 is in some sense best possible and that
we are indeed “lucky” that there exists an algorithm which is exponential in n but not in m.
Theorem 2. Let α ≥ 0. Then it is NP-hard to decide whether δ opt ≤ α on a sub-class of
problems with AΔ = E, bΔ = e, and m = n + 1.
1794

ˇ
Hlad´ık and Cern´
y

Total Least Squares and Chebyshev Norm

Proof. First notice that the condition δ opt ≤ α is equivalent to solvability of [A−αE, A+αE]x =
[b − αe, b + αe].
Now, let M ∈ Qn×n and α ∈ Q, α ≥ 0. Checking whether the interval matrix [M −αE, M +
αE] is singular is an NP-hard problem [11]. The interval matrix [M − αE, M + αE] is singular
iﬀ there is i ∈ {1, . . . , n} and x∗ ∈ Rn , x∗i = 1 such that x∗ solves [M − αE, M + αE]x = 0. In
other words, the interval system
[M ¬i − αE, M ¬i + αE]x = [M∗i − αe, M∗i + αe]

(9)

has a solution, where M∗i denotes the ith column of M and M ¬i denotes the matrix M with
the ith column deleted. Thus, if we can decide on solvability of (9) in polynomial time, we could
check for singularity of interval matrices in polynomial time as well. Therefore the problem in
question is NP-hard.

2.2

Finding a minimizer

We know how to compute xopt and δ opt . When we are to ﬁnd a minimizer (A | b ) of (1), it
suﬃces to solve the system
A xopt = b , A − δ opt AΔ ≤ A ≤ A + δ opt AΔ , b − δ opt bΔ ≤ b ≤ b + δ opt bΔ

(10)

with data δ opt , xopt , A, AΔ , b, bΔ and variables A , b . This is a linear feasibility problem which
can be solved in polynomial time by linear programming techniques. Moreover, (10) describes
exactly the set of all minimizers. On the other hand, if we are interested in only one minimizer,
we can use the explicit formula from [2]: Deﬁne z := sgn(x) and deﬁne the vector y ∈ [−1, 1]m
as
⎧
(Axopt − b)i
⎨
if δ opt (AΔ |xopt | + bΔ )i > 0,
opt
yi = δ (AΔ |xopt | + bΔ )i
⎩
1
otherwise.
Now, we can put
A := A − δ opt · diag(y)AΔ diag(z),

2.3

b := b + δ opt · diag(y)bΔ .

Properties

Existence.
consider

As in the classical TLS, the optimal solution needn’t exist. As an example,
⎛
⎞
⎛ ⎞
⎛
⎞
⎛ ⎞
1 0
1
0 1
0
A = ⎝0 1⎠ , b = ⎝0⎠ , AΔ = ⎝0 0⎠ , bΔ = ⎝0⎠ .
0 1
1
0 0
0

Here, the optimal value of (4) is unbounded. Typically, however, the optimal value is bounded.
This is the case, for instance, for the most natural choices AΔ = E, bΔ = e or AΔ = |A|,
bΔ = |b| since δ opt ≤ maxi,j {|aij |, |bi |} holds in the former and δ opt ≤ 1 holds in the latter.
Conversely, even when the optimal value is bounded, the optimal value still needn’t be
attained. For example, when
A=

1
1

1
, b=
1

0
, AΔ =
1

0
0

1
, bΔ =
0

0
.
0
1795

ˇ
Hlad´ık and Cern´
y

Total Least Squares and Chebyshev Norm

Here, any δ > 0 is feasible, but δ = 0 is not. Such problems are called non-generic in the
classical TLS.
Denote
δ inf := inf δ subject to [A − δAΔ , A + δAΔ ]x = [b − δbΔ , b + δbΔ ] is solvable.
As long as δ is bounded from above, we have δ inf < ∞. If Aδ x = bδ is solvable for δ = δ inf ,
then δ opt is attained and δ opt = δ inf . The following observation gives another condition under
which (4) has an optimal solution.
Proposition 1. Suppose that Aδ has full column rank for δ = δ inf . Then δ opt = δ inf .
Proof. For an interval matrix M ∈ IRm×n , denote its smallest singular value as
σmin (M ) := min{σmin (M ); M ∈ M },

(11)

where σmin (M ) is the smallest singular value of a real-valued matrix M . Due to continuity of
σmin (·) for real matrices and compactness of M , the minimum in (11) always exists.
Under the assumption of the proposition, Aδ x = bδ is solvable iﬀ the interval matrix
(Aδ | bδ ) has not full column rank. Equivalently, σmin (Aδ | bδ ) = 0. Since σmin (Aδ | bδ ) = 0
for every δ > δ inf , it vanishes also for δ = δ inf and therefore δ opt is attained.
Uniqueness. Similarly as for L∞ -regression (or: Chebyshev approximation), we uniqueness
of the optimal solution is not guaranteed in general. As an example, consider
A=

1
, b=
1

1
, AΔ =
−1

1
1

1
, bΔ =
1

1
.
1

for which δ opt = 1 and xopt is any real value.
Another formulation.

In view of (5), problem (3) can be expressed as

min δ subject to |Ax − b| ≤ δE|x| + δe is solvable,
which is equivalent to
min
x

Ax − b ∞
.
x 1+1

This gives a nice link to the classical TLS, which can be formulated as optimization problem
min
x

3

Ax − b 22
.
x 22 + 1

A heuristic method

The negative result of Theorem 2 shows that we might meet instances for which the computation
of δ opt is intractable, even in spite of the good news of Corollary 1. Then we must turn to
heuristics. Here we present two useful ideas, which can be used as a basis for more sophisticated
methods or combined with general metaheuristic approaches. (We are convinced that design of
heuristics for (1) is a tempting topic deserving a separate research.).
1796

ˇ
Hlad´ık and Cern´
y

Total Least Squares and Chebyshev Norm

3.1

Lower bound on δ opt

The system Ax = b is infeasible if the matrix (A | b) has full column rank. Thus, the interval
system Aδ x = bδ has no solution if the interval matrix (Aδ | bδ ) has full column rank, that is,
it contains only full column rank matrices.
Full column rank of interval matrices were investigated in several papers; see, e.g., [19, 20,
21]. Among the known methods, the following performs well. An interval matrix M has full
column rank if ρ(|(M c )† |M Δ ) < 1, where ρ(·) denotes the spectral radius and (·)† the Moore–
Penrose pseudoinverse. Applying this suﬃcient condition to our case with M := (Aδ | bδ ), we
get the condition
ρ |(A | b)† | · δ · (AΔ | bΔ ) = δ · ρ |(A | b)† | · (AΔ | bΔ ) < 1.
In other words, if
δ < ρ |(A | b)† | · (AΔ | bΔ )

−1

,

then (Aδ | bδ ) has full column rank and Aδ x = bδ is infeasible. Hence,
δ opt ≥ ρ |(A | b)† | · (AΔ | bΔ )

3.2

−1

.

Upper bounds on δ opt

We propose two upper bounds on δ opt .
First, let x∗ be the traditional least-squares solution of Ax = b, or any other heuristic
solution. Denote s := sgn(x∗ ). Solve the GLFP ( ) associated with this sign vector s and
denote by δ ∗ its optimal value. Then
δ opt ≤ δ ∗ .
Second, let δ F be the optimal value of the classical TLS problem
min (ΔA | Δb)

F

subject to (A + ΔA)x = b + Δb is solvable,

which can be found easily with SVD decomposition.
Since · max ≤ · F , we have
δ opt ≤ δ F .
The drawback of this approach is that it does not give us explicitely the corresponding perturbations ΔA and Δb.

3.3

The resulting heuristic

Now, we present a heuristic for determining δ opt .
Calculate an upper bound δ U ≥ δ opt by any method mentioned in Section 3.2. Let x ∈ IRn
be an enclosure of the solution set of the overdetermined interval system Aδ x = bδ .
If x crosses only a small number of orthants, than we can eﬀectively calculate δ opt , xopt by
(7), where s is subject to sign vectors of all crossing orthants instead of {±1}n .
In particular, if δ ∗ is the upper bound from Section 3.2, and if x lies in one orthant only,
then δ opt = δ ∗ and xopt is the corresponding optimal solution of the GLFP ( ).
1797

ˇ
Hlad´ık and Cern´
y

Total Least Squares and Chebyshev Norm

xopt
0.8

δ opt

0.7

0.6
γ = 0.6

0.5

∞

γ = 0.6

0.6
0.5

0.4
γ = 0.4

0.4

0.3

γ = 0.2

0.3
0.2
0.1
0

γ = 0.4

0.2

γ = 0.2

0.1

0

20

60 k

40

0

0

20

Figure 1: Average simulated values of δ opt (left plot) and xopt
k ∈ {2, 8, 16, 24, 40, 60} with γ ∈ {0.2, 0.4, 0.6}.

4

∞

40

60

k

(right plot) as a function of

Example: A simulation study

The aim of this section is to illustrate how the exact method of Section 2 works in a particular
probabilistic error-generating setup.
For a given k ≥ 1, we consider the simple choice
A∗ = (In | In | · · · | In )T ,

b∗ = 0kn×1 .

k times

Clearly, the correct solution of A∗ x∗ = b∗ is x∗ = 0. Now we introduce errors: we run our
method with AΔ = E, bΔ = e and
A = A∗ + U,

b = b∗ + u,

where U is a random (nk × n)-matrix with independent entries sampled from Unif(−γ, γ) and
u is an (nk × 1)-vector with independent entries sampled again from Unif(−γ, γ).
Figure 1 shows:
• average simulated values of δ opt as a function of k for three choices γ ∈ {0.2, 0.4, 0.6};
• average simulated values of xopt

∞

as a function of k for the same choices of γ.

Figure 1 suggests that the following (not surprising) claims could be true, at least in the
particular setup of this simulation study:
• δ ∗ underestimates γ, but asymptotically δ ∗ estimates γ consistently;
• asymptotically, the method consistently estimates x∗ .
1798

Total Least Squares and Chebyshev Norm

ˇ
Hlad´ık and Cern´
y

Remark 4. The plotted Figure is for n = 2. Additional simulations (not presented here)
showed that for other values of n the graphs would conﬁrm similar trends. Of course, it is not
surprising that the speed of convergence depends on all involved parameters n, k, γ.
Experiments with other distributions with support [−γ, γ] conﬁrm analogous behavior, too.

5

Conclusion

We considered the TLS problem with Chebyshev norm and designed an algorithm which solves
2n generalized linear fractional programs. We proved that the problem is NP-hard and thus it
cannot be expected that our method could be signiﬁcantly improved. But the complexity of
our method can be understood as good news, since it is exponential in the number of regression
parameters but not in the number of observations. (In practice we have usually regression
models with a low number of regression parameters compared to the number of observations;
only rarely we meet regression model with more that 20 regressors, say.) Then, we designed
a heuristic method utilizing eﬃciently computable lower and upper bounds on the optimal
residual value. Finally, we illustrated the behavior of our method by a simulation study.

Acknowledgments
ˇ
M. Hlad´ık was supported by the Czech Science Foundation Grant P402/13-10660S. M. Cern´
y
was supported by the Czech Science Foundation Grant P402/12/G097.

References
ˇ
[1] Michal Cern´
y, Jarom´ır Antoch, and Milan Hlad´ık. On the possibilistic approach to linear regression
models involving uncertain, indeterminate or interval data. Inf. Sci., 244:26–47, 2013.
[2] M. Fiedler, J. Nedoma, J. Ram´ık, J. Rohn, and K. Zimmermann. Linear Optimization Problems
with Inexact Data. Springer, New York, 2006.
[3] R. W. Freund and F. Jarre. An interior-point method for multifractional programs with convex
constraints. J. Optim. Theory Appl., 85(1):125–161, 1995.
[4] Laurent El Ghaoui and Herv´e Lebret. Robust solutions to least-squares problems with uncertain
data. SIAM J. Matrix Anal. Appl., 18(4):1035–1064, 1997.
[5] Gene H. Golub and Charles F. Van Loan. An analysis of the total least squares problem. SIAM
J. Numer. Anal., 17:883–893, 1980.
[6] Milan Hlad´ık. New operator and method for solving real preconditioned interval linear equations.
SIAM J. Numer. Anal., 52(1):194–206, 2014.
ˇ
[7] Milan Hlad´ık and Michal Cern´
y. Interval data, linear regression and minimum norm estimators:
Computational issues, 2014. submitted.
[8] Jaroslav Hor´
aˇcek and Milan Hlad´ık. Computing enclosures of overdetermined interval linear systems. Reliab. Comput., 19(2):142–155, 2013.
[9] Jaroslav Hor´
aˇcek and Milan Hlad´ık. Subsquares approach – a simple scheme for solving overdetermined interval linear systems. In R. Wyrzykowski, J. Dongarra, K. Karczewski, and J. Wa´sniewski,
editors, Parallel Processing and Applied Mathematics, volume 8385 of LNCS, pages 613–622.
Springer, 2014.
[10] D. Juki´c, T. Maroˇsevi´c, and R. Scitovski. Discrete total lp -norm approximation problem for the
exponential function. Appl. Math. Comput., 94(2-3):137–143, 1998.
[11] Vladik Kreinovich, Anatoly Lakeyev, Jiˇr´ı Rohn, and Patrick Kahl. Computational Complexity and
Feasibility of Data Processing and Interval Computations. Kluwer, 1998.

1799

Total Least Squares and Chebyshev Norm

ˇ
Hlad´ık and Cern´
y

[12] Ivan Markovsky and Sabine Van Huﬀel. Overview of total least-squares methods. Signal Process.,
87(10):2283–2302, 2007.
[13] Tomislav Maroˇsevi´c. A choice of norm in discrete approximation. Math. Commun., 1(2):147–152,
1996.
[14] Yu. E. Nesterov and A. S. Nemirovskij. An interior-point method for generalized linear-fractional
programming. Math. Program., 69(1B):177–204, 1995.
[15] Arnold Neumaier. Interval Methods for Systems of Equations. Cambridge University Press, Cambridge, 1990.
[16] W. Oettli and W. Prager. Compatibility of approximate solution of linear equations with given
error bounds for coeﬃcients and right-hand sides. Numer. Math., 6:405–409, 1964.
[17] M. R. Osborne and G. A. Watson. An analysis of the total approximation problem in separable
norms, and an algorithm for the total l1 problem. SIAM J. Sci. Stat. Comput., 6(2):410–424, 1985.
[18] Jiˇr´ı Rohn. Systems of linear interval equations. Linear Algebra Appl., 126(C):39–78, 1989.
[19] Jiˇr´ı Rohn. Enclosing solutions of overdetermined systems of linear interval equations. Reliab.
Comput., 2(2):167–171, 1996.
[20] Jiˇr´ı Rohn. A manual of results on interval linear problems. Technical Report 1164, Institute of
Computer Science, Academy of Sciences of the Czech Republic, Prague, 2012.
[21] Siegfried M. Rump. Veriﬁcation methods for dense and sparse systems of equations. In J¨
urgen
Herzberger, editor, Topics in Validated Computations, Studies in Computational Mathematics,
pages 63–136, Amsterdam, 1994. Elsevier.
[22] S. Van Huﬀel and J. Vandewalle. The Total Least Squares Problem. SIAM, 1991.
[23] G. A. Watson. Choice of norms for data ﬁtting and function approximation. Acta Numer., 7:337–
377, 1998.
[24] G. A. Watson. Data ﬁtting problems with bounded uncertainties in the data. SIAM J. Matrix
Anal. Appl., 22(4):1274–1293, 2001.

1800

