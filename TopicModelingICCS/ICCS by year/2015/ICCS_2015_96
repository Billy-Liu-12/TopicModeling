Procedia Computer Science
Volume 51, 2015, Pages 231–238
ICCS 2015 International Conference On Computational Science

Second-Order Tangent Solvers for Systems of
Parameterized Nonlinear Equations
Niloofar Saﬁran, Johannes Lotz, and Uwe Naumann
LuFG Informatik 12: Software and Tools for Computational Engineering,
RWTH Aachen University, Germany.
[safiran|lotz|naumann]@stce.rwth-aachen.de

Abstract
Forward mode algorithmic diﬀerentiation transforms implementations of multivariate vector
functions as computer programs into ﬁrst directional derivative (also: ﬁrst-order tangent) code.
Its reapplication yields higher directional derivative (higher-order tangent) code. Second derivatives play an important role in nonlinear programming. For example, second-order (Newtontype) nonlinear optimization methods promise faster convergence in the neighborhood of the
minimum through taking into account second derivative information. Part of the objective
function may be given implicitly as the solution of a system of n parameterized nonlinear equations. If the system parameters depend on the free variables of the objective, then second
derivatives of the nonlinear system’s solution with respect to those parameters are required.
The local computational overhead for the computation of second-order tangents of the solution
vector with respect to the parameters by Algorithmic Diﬀerentiation depends on the number
of iterations performed by the nonlinear solver. This dependence can be eliminated by taking
a second-order symbolic approach to diﬀerentiation of the nonlinear system.
Keywords: Nonlinear System, Second-Order Tangent, Hessian, Algorithmic Diﬀerentiation, Symbolic
Diﬀerentiation

1

Introduction and Summary of the Results

In this paper we consider two alternative approaches to evaluating second derivatives of numerical simulation programs with an embedded iterative solution procedure (the nonlinear solver)
for a system of n parameterized nonlinear equations. Algorithmic diﬀerentiation (AD) [10, 11]
yields a computational cost (operations count) for second-order tangents of the solution vector
with respect to the parameters which depends on the number of iterations ν performed by the
nonlinear solver. A similar computational cost is exhibited by the approximation of secondorder tangents using second-order central ﬁnite diﬀerence quotients (FD). It is well known that
the quality of this approximation is likely to become highly questionable in standard ﬁnite precision ﬂoating-point arithmetic. Symbolic diﬀerentiation (SD) of the nonlinear system assumes
Selection and peer-review under responsibility of the Scientiﬁc Programme Committee of ICCS 2015
c The Authors. Published by Elsevier B.V.

doi:10.1016/j.procs.2015.05.232

231

Second-Order Tangent Nonlinear Solvers

Saﬁran, Lotz and Naumann

availability of the exact solution due to full convergence of the nonlinear solver. It reduces the
computational cost by a factor of ν.
Table 1 summarizes these results. The nonlinear solver is assumed to perform ν iterations in
order to satisfy a given convergence criterion staring from a suitable initial guess for the solution.
In each iteration the whole Jacobian of the residual is factorized as part of a direct approach
to the solution of the embedded linear system. Newton’s algorithm is a famous representative
of such a method.

MEM
OPS

SD
O(n2 )
O(n3 )

AD
O(n2 )
ν · O(n3 )

Table 1: Computational cost (OPS) and memory requirement (MEM) for the second-order
tangent algorithmic (AD) and symbolic (SD) diﬀerentiation of a nonlinear system.
We assume that single second-order tangents of the residual can be obtained at a computational cost which is substantially lower than O(n3 ). This assumption turns out to be realistic
since this cost is equal to O(1) · OP S(F ) and the cost of a single evaluation of the residual
rarely exceeds the cost of a complete factorization of its Jacobian.

2

Foundations

In this section we recall some aspects from [13] which this work is based on. Further related
work by others includes [3, 4, 6, 9].
We consider the computation of second-order tangents for solvers of systems of n nonlinear
equations depending on m parameters and described by the residual
r = F (x(λ), λ) : IRn × IRm → IRn .
(1)
m
For a given λ ∈ IR , a primal solution vector x ∈ IRn is sought such that r = 0.
Without loss of generality, the nonlinear solver is assumed to be embedded into the unconstrained convex nonlinear programming problem (NLP)
minq f (z)
z∈IR

for a given objective function f : IRq → IR. Second-order derivative-based NLP solvers use
the gradient and the Hessian of the objective y = f (z) ∈ IR with respect to the free variables
z ∈ IRq . Hence, both ﬁrst and second derivatives of the nonlinear solver are required. As in
[13], f is decomposed as
(2)
y = f (z) = p(S(x0 , P (z))) ,
q
m
where P : IR → IR denotes the part of the computation that precedes the nonlinear solver
S : IRn × IRm → IRn and where p : IRn → IR maps the result x onto the scalar objective y. The
case study in Section 5 is based on the following algorithmic description of Equation (2):
˜ = S(x0 , λ), y = p(˜
λ = P (z), x
x) .
(3)
m
The parameters λ ∈ IR are computed as functions of z by the given implementation of the
“preprocessor” P. They enter the nonlinear solver S as arguments in addition to the given initial
˜ of the solution x is
estimate x0 ∈ IRn of the solution x ∈ IRn . The computed approximation x
reduced to a scalar objective value y ∈ IR by the given implementation of the “postprocessor”
p.
232

Second-Order Tangent Nonlinear Solvers

3

Saﬁran, Lotz and Naumann

Algorithmic Diﬀerentiation

We recall some signiﬁcant elements of AD described in further detail in [10, 11]. Without
loss of generality, the following discussion is based on Equation (1). Let u = (x λ)T ∈ IRh
and h = n + m. AD yields semantical transformations of the given implementation of F as a
computer program into ﬁrst and potentially also higher (k-th) derivative code. For this purpose
F is assumed to be k times continuously diﬀerentiable for k = 1, 2, . . . . Extensions of AD to
nondiﬀerentiable functions have been the subject of ongoing research for several years; see, for
example, [8]. The focus of this paper is on forward / tangent mode AD. Analogous results were
derived for reverse / adjoint mode AD [14].

3.1

First-Order Tangent AD

The Jacobian

∂F (x, λ)
∂(x λ)
of r = F (u) induces a linear mapping D → I, h = n + m, from the domain D of F into its
image I deﬁned by
u(1) →< ∇F, u(1) >≡ ∇F · u(1) .
In this paper we assume D = IRh and I = IRn . A ﬁrst-order tangent projection of ∇F in
direction u(1) ∈ IRh is deﬁned as the usual matrix-vector product ∇F · u(1) . Alternatively,
we use the inner product notation < ∇F, u(1) > as introduced in [11]. The function F (1) :
IRh × IRh → IRn , deﬁned as
(4)
r(1) = F (1) (u, u(1) ) =< ∇F, u(1) >
is referred to as the tangent model of F .
∇F = ∇F (u) ≡

Hessians of twice continuously diﬀerentiable multivariate vector functions are 3-tensors with
symmetric matrices forming the two domain dimensions. In Section 3.2 we require projections
of such 3-tensors in their domain dimensions for the deﬁnition of second-order tangents. Hence,
we choose to introduce the inner product notation in further detail.
Let [T ]i0 ,...,ik−1 denote an entry of a k-tensor T. We use wildcards ∗ to denote entire index
ranges along selected dimensions. For
k=0,...,n−1
∂[r]k
∂r
k=0,...,n−1
∇F = ([∇F ]k,j )j=0,...,h−1 ≡
=
)
∈ IRn×h
∂u
∂[u]j j=0,...,h−1
Equation (4) yields
[r(1) ]k =< [∇F ]k,∗ , u(1) >≡

h−1

[∇F ]k,j · [u(1) ]j

(5)

j=0

for k = 0, ..., n − 1. The k-th row of ∇F is denoted by [∇F ]k,∗ , that is, the expression
< [∇F ]k,∗ , u(1) > denotes the usual scalar product of two vectors in IRh .
Tangents r(1) of the residual can be regarded as partial derivatives of r with respect to an
auxiliary scalar variable s, where initially u(1) ≡ ∂u
∂s . The chain rule yields
∂r ∂u
∂r
(1)
=
·
=< ∇F, u(1) > .
r ≡
∂s
∂u ∂s
The whole Jacobian can thus be accumulated with machine accuracy at a computational cost
of O(h) · OP S(F ), where OP S(F ) denotes the computational cost of a single evaluation of the
residual.
233

Second-Order Tangent Nonlinear Solvers

3.2

Saﬁran, Lotz and Naumann

Second-Order Tangent Model

The Hessian

∂ 2 F (x, λ)
∂(x λ)2
of r = F (u) induces a bilinear mapping IRh × IRh → IRn deﬁned by
(u(1) , u(2) ) →< ∇2 F, u(1) , u(2) >=<< ∇2 F, u(1) >, u(2) > .
A second-order tangent projection < ∇2 F, u(1) , u(2) > of
k=0,...,n−1
∂ 2 [r]k
∂2r
k=0,...,n−1
=
∈ IRn×h×h ,
∇2 F = [∇2 F ]k,i,j i,j=0,...,h−1 ≡
∂u2
∂[u]i ∂[u]j i,j=0,...,h−1
where [∇2 F ]k,i,j = [∇2 F ]k,j,i , in directions u(1) , u(2) ∈ IRh is a ﬁrst-order tangent projection
in direction u(2) of the ﬁrst-order tangent projection of ∇2 F in direction u(1) (deﬁned below).
The function F (1,2) : IRh × IRh × IRh → IRn , deﬁned as
r(1,2) = F (1,2) (u, u(1) , u(2) ) ≡< ∇2 F, u(1) , u(2) > ,
is referred to as the second-order tangent model of F. The Hessian is projected along its two
domain dimensions (of size h) in directions u(1) and u(2) . Its ﬁrst-order tangent projection in
direction u(1) is deﬁned as
2
(1)
B = ([B]k,j )k=0,...,n−1
>∈ IRn×h ,
j=0,...,h−1 ≡< ∇ F, u
where
∇2 F = ∇2 F (u) ≡

h−1

[B]k,j =

[∇2 F ]k,j,i · [u(1) ]i

.

i=0

Consequently,

r(1,2) =< B, u(2) >=< ∇2 F, u(1) , u(2) >∈ IRn

.

Equation (5) yields
[r(1,2) ]k =

h−1
j=0

[B]k,j · [u(2) ]j =

h−1 h−1

[∇2 F ]k,j,i · [u(1) ]i · [u(2) ]j

j=0 i=0

for k = 0, ..., n − 1. The whole Hessian can be accumulated with machine accuracy at a computational cost of O(h2 ) · OP S(F ). Second-order ﬁnite diﬀerences exhibit the same computational
cost while suﬀering from potentially dramatic numerical errors due to truncation in ﬁnite precision ﬂoating-point arithmetic.
Application of tangent mode to the ﬁrst-order tangent model (Equation (4)) yields
(6)
r(1,2) =< ∇2 F, u(1) , u(2) > + < ∇F, u(1,2) > ,
∂u
∂u(1)
(2)
(1,2)
(1,2)
≡ ∂s . Setting u
= 0 yields the second-order tangent model.
where u ≡ ∂s and u
The application of tangent mode to the second-order tangent model yields third derivative
information and so forth. The derivative code generated by AD can compute projections of
derivative tensors of arbitrary order. For more information refer to [10, 11].

4

Second-Order Tangent Nonlinear Solver

We consider two approaches to the generation of second-order tangent solvers for systems of
parameterized nonlinear equations. A second-order algorithmic tangent version of the solver
computes second-order directional derivatives of the approximation of the solution, which is
actually computed by the algorithm. Second-order AD is applied to the individual statements of
the given implementation yielding an increase of roughly four in memory requirement as well as
in operations count. A second-order symbolic tangent version of the solver computes the second
directional derivatives of the solution under the assumption that the exact primal solution
has been reached. The nonlinear system F (x, λ) = 0 can be diﬀerentiated symbolically in this
234

Second-Order Tangent Nonlinear Solvers

Saﬁran, Lotz and Naumann

case. In symbolic tangent mode the computation of both ﬁrst- [13] and second-order directional
derivatives amounts to the solution of a linear system, respectively. The discrepancies in the
numerical results computed by second-order algorithmic and symbolic tangent nonlinear solvers
depend on the accuracy of the approximation of the primal solution.

4.1

Algorithmic Diﬀerentiation

Newton’s method meets the requirements formulated in Section 1. It uses the Jacobian of the
residual to determine the next Newton step by solving a linear system during each of the ν
iterations. A ﬁrst-order tangent version of Newton’s algorithm requires second-order tangents
of the residual. Consequently, the second-order tangent version of Newton’s algorithm requires
third directional derivatives of the given implementation of F . The required memory is four
times the memory required by the nonlinear solver (O(n2 )) and the number of operations is
four times the operations performed by the nonlinear solver (ν · O(n3 )).

4.2

Symbolic Diﬀerentiation

We start this section with two auxiliary observations to be used in the proof of our main result
(Theorem 4.3).
Lemma 4.1. Let A = A(s) ∈ IRn×h , b = b(s) ∈ IRh , and s ∈ IR. Then
∂A
∂ < A, b >
∂(A · b)
∂b
≡
=
·b+A·
=< A(1) , b > + < A, b(1) >
∂s
∂s
∂s
∂s

.

See, for example, [7] for a proof.
Lemma 4.2. Let T ∈ IRn×(n+m)×(n+m) , x(1) , x(2) ∈ IRn and λ(1) , λ(2) ∈ IRm . Then
0n
x(1)
x(2)
x(1)
x(2)
x(1)
< T,
,
> =< T,
,
> + < T,
,
>
(1)
(2)
0m
0m
0m
λ(2)
λ
λ
0n
0n
0n
x(2)
> + < T,
,
(1) ,
0m
λ
λ(1)
λ(2)
denote the zero vectors in IRn and IRm , respectively.
+ < T,

where 0n and 0m

>

,

The proof is rather obvious but technical and hence omitted.
Theorem 4.3. Second-order tangents x(1,2) ∈ IRn of solutions x ∈ IRn of the system of
parameterized nonlinear equations F (x(λ), λ) = 0 deﬁned in Section 2 with respect to the
parameter vector λ ∈ IRm are equal to the solution of the linear system
∂F (1,2)
∂F
∂2F
x(2)
x(1)
,
>−<
· x(1,2) = − <
,λ
,
> .
(7)
(1)
λ(2)
∂x
∂(x λ)2 λ
∂λ
Proof. First-order tangents x(1) of the solution x of F (x(λ), λ) = 0 with respect to λ are equal
to the solution of the linear system
∂F
∂F
· x(1) = −
· λ(1) ; see [13].
∂x
∂λ
Switching to inner product notation yields
∂F (1)
∂F (1)
<
, x >=< −
,λ > .
(8)
∂x
∂λ
(2)
The directional derivative (tangent) of Equation (8) with respect to λ in direction λ becomes
∂F (1)
∂F (1)
d
d
<
, x >, λ(2) >=<
<−
, λ >, λ(2) > .
(9)
<
dλ
∂x
dλ
∂λ
235

Second-Order Tangent Nonlinear Solvers

Saﬁran, Lotz and Naumann

Lemma 4.1 and the chain rule make the left-hand side of Equation (9) equal to
∂x (2)
∂2F
∂F (1,2)
∂ 2 F (1)
,
λ
, x(1) >, λ(2) > + <
,x
,
x
>,
<
>
>
+
<<
>
<<
∂x2
∂λ
∂x∂λ
∂x
=x(2)

yielding

∂F (1,2)
∂2F
0n
∂2F
x(1)
x(1)
x(2)
,x
,
,
> .
,
>+<
,
>+<
2
2
0m
0m
0m
λ(2)
∂(x λ)
∂(x λ)
∂x
Similarly, the right-hand-side of Equation (9) is equal to
∂2F
∂F (1,2)
∂x (2)
∂ 2 F (1)
− <<
, λ(1) >, <
, λ >> − <<
,λ
>, λ(2) > − <
>
2 ,λ
∂x∂λ
∂λ
∂λ
∂λ
yielding
∂2F
∂2F
∂F (1,2)
0n
λ(1)
λ(1)
λ(2)
−<
,λ
,
,
> .
,
>−<
,
>−<
(2)
2
2
x
0m
0m
0m
∂(x λ)
∂(x λ)
∂λ
Rearranging the terms yields
∂F (1,2)
∂2F
∂2F
0n
x(1)
x(1)
x(2)
,x
>=− <
,
,
>−<
,
,
>
<
2
2
0m
0m
0m
λ(2)
∂x
∂(x λ)
∂(x λ)
<

∂2F
∂2F
0n
λ(1)
λ(1)
,
,
,
>−<
,
(2)
2
2
x
0m
0m
∂(x λ)
∂(x λ)
∂F (1,2)
,λ
−<
> ,
∂λ
which by Lemma 4.2 implies
∂F (1,2)
∂F (1,2)
∂2F
x(2)
x(1)
,
>−<
,x
,λ
>= − <
,
<
(1)
λ(2)
∂x
∂(x λ)2 λ
∂λ
and hence
∂F (1,2)
∂F
∂2F
x(2)
x(1)
>−<
· x(1,2) = − <
,λ
,
>
(1) ,
2
λ
λ(2)
∂x
∂(x λ)
∂λ
−<

λ(2)
0m

>

>
.

According to Equation (6) the right hand side of Equation (7) can be evaluated by secondorder tangent AD at a computational cost of O(1) · OP S(F ). A previously computed factorization of the Jacobian of the residual can be reused. No additional memory is required to compute
symbolic second-order tangents of solutions of systems of parameterized nonlinear equations.
The overall computational complexity remains equal to O(n3 ).

5

Case Study

As a case study we consider a parameter estimation problem based on the one dimensional
nonlinear diﬀerential equation
on Ω = (0, 1)
∇2 (z · u∗ ) + u∗ · ∇(z · u∗ ) = 0
∗
u = 10 and z = 1
for x = 0
∗
u = 20 and z = 1
for x = 1
with parameters z(x). With an equidistant central ﬁnite diﬀerence discretization we get for a
given u (discretized and, hence, vector-valued variables are written as bold letters) the residual
function
1
1
· ([z]i+1 [u]i+1 − [z]i−1 [u]i−1 )
[r]i = 2 · ([z]i−1 [u]i−1 − 2[z]i [u]i + [z]i+1 [u]i+1 ) + [u]i ·
h
2h
with h = 1/n. The n discretization points yield a system of n nonlinear equations
r(u, z) = 0 , u ∈ IRn , z ∈ IRn .
236

Second-Order Tangent Nonlinear Solvers

Saﬁran, Lotz and Naumann

For given measurements um (x) we state the following parameter ﬁtting problem for z
z ∗ = arg min J(z) ⇒ J(z) = u(x, z) − um (x) 22 .
z∈IR

The measurements um (x) are generated from a given set of parameters (“twin experiment”).
We apply Newton’s method to the ﬁrst-order optimality condition ∇J(z) = 0 yielding
−1
zk+1 = zk − α · ∇2 J(zk ) · ∇J(zk ) ,
where α > 0 denotes the local line search parameter. The computation of gradient and Hessian
of J at the current iterate zk comprises the diﬀerentiation of the nonlinear solver.
Referring to Equation (3), the preprocessor λ = P (z) is the identity and the postprocessor
p(u) evaluates the cost function J(z). Our overloading AD tool dco/c++ [12] is used for
diﬀerentiation. The entire code including a trial license for dco/c++ is available on request.

10
20
30
40
50
80
100

SD
0.1
0.99
4.59
14.58
36.99
277.55
813.47

AD
0.47
4.25
21.92
73.23
186.87
1475.36
-

FD
0.3
2.54
16.18
41.17
-

run time in seconds

103

102

101
SD
AD
FD

100

10−1
101

101.2

101.4
101.6
101.8
problem dimension n

102

Figure 1: Run time (in seconds) for reference parameter estimation problem solved using different approaches to diﬀerentiation. Missing values indicate failure to converge within 2000
seconds.
In Figure 1 we observe the expected behavior for the computational cost induced by the
various diﬀerentiation methods. Symbolic diﬀerentiation (SD) of the embedded nonlinear solver
results in a signiﬁcantly lower total run time than Algorithmic Diﬀerentiation (AD) or if Finite
Diﬀerences (FD) is used for the approximation of both the gradient and the Hessian of the
objective. FD turns out to be slightly faster than AD on our reference computer.1 However, it
fails to converge for problem sizes larger than n = 40. No parallelization was performed.

6

Conclusion

Algorithmic Diﬀerentiation is based on the knowledge of partial derivatives of a set of elemental
functions [10] used to build up an evaluation procedure for a given target function. Traditionally,
the built-in functions and arithmetic operators of programming languages have been serving
as elementals. Deeper insight into the mathematical structure of numerical simulations yields
higher-level elemental functions including linear and nonlinear solvers, optimizers, integrators
for ordinary and partial diﬀerential equations. First- and higher-order numerical methods
generate the need for ﬁrst and higher derivatives of these new elemental functions. They
ﬁnd application in a wide range of modern methods in Computational Science, Engineering,
and Finance ranging from sensitivity and error analysis to adjoint approaches to the solution of
inverse problems. A rich collection of related articles can be found in the proceedings of the last
1

2x Intel(R) Xeon(R) CPU E5-2630 0 @ 2.30GHz (2x 6 Cores (12 Threads)), 128 GB RAM

237

Second-Order Tangent Nonlinear Solvers

Saﬁran, Lotz and Naumann

three international conferences on AD [1, 2, 5]. Software tools for Algorithmic Diﬀerentiation2
need to exhibit a high level of ﬂexibility and extensibility in order to facilitate the integration of
new elemental functions. Diﬀerentiated versions of numerical libraries will soon have to provide
appropriate naming and calling conventions for tangents and adjoints of numerical methods.
For example, the Numerical Algorithms Group Ltd.3 is in the process of expanding the NAG
Library with both tangent and adjoint versions for a suitable subset of the more than 1500
numerical routines. Our AD tool dco/c++ keeps adopting them as new elementals.

References
[1] C. Bischof, M. B¨
ucker, P. Hovland, U. Naumann, and J. Utke, editors. Advances in Automatic
Diﬀerentiation, volume 64 of Lecture Notes in Computational Science and Engineering. Springer,
Berlin, 2008.
[2] M. B¨
ucker, G. Corliss, P. Hovland, U. Naumann, and B. Norris, editors. Automatic Diﬀerentiation:
Applications, Theory, and Implementations, volume 50 of Lecture Notes in Computational Science
and Engineering. Springer, New York, NY, 2005.
[3] B. Christianson. Reverse accumulation and attractive ﬁxed points. Optimization Methods and
Software, 3(4):311–326, 1994.
[4] J. Davies, B. Christianson, L. Dixon, R. Roy, and P. van der Zee. Reverse diﬀerentiation and the
inverse diﬀusion problem. Adv. Eng. Softw., 28(4):217–221, 1997.
[5] S. Forth, P. Hovland, E. Phipps, J. Utke, and A. Walther, editors. Recent Advances in Algorithmic
Diﬀerentiation, volume 87 of Lecture Notes in Computational Science and Engineering. Springer,
Berlin, 2012.
[6] J. Gilbert. Automatic diﬀerentiation and iterative processes. Optimization Methods and Software,
1:13–21, 1992.
[7] M. Giles. Collected matrix derivative results for forward and reverse mode algorithmic diﬀerentiation. In [1], pages 35–44.
[8] A. Griewank. On stable piecewise linearization and generalized algorithmic diﬀerentiation. Optimization Methods and Software, 28(6):1139–1178, 2013.
[9] A. Griewank and C. Faure. Reduced functions, gradients and Hessians from ﬁxed-point iterations
for state equations. Numerical Algorithms, 30(2):113–139, 2002.
[10] A. Griewank and A. Walther. Evaluating Derivatives: Principles and Techniques of Algorithmic
Diﬀerentiation. Number 105 in Other Titles in Applied Mathematics. SIAM, Philadelphia, PA,
2nd edition, 2008.
[11] U. Naumann. The Art of Diﬀerentiating Computer Programs. An Introduction to Algorithmic
diﬀerentiation. Number 24 in Software, Environments, and Tools. SIAM, Philadelphia, PA, 2012.
[12] U. Naumann, K. Leppkes, and J. Lotz. dco/c++ user guide. Technical Report AIB-2014-03, RWTH
Aachen University, 2014.
[13] U. Naumann, J. Lotz, K. Leppkes, and M. Towara. Algorithmic diﬀerentiation of numerical
methods: Tangent and adjoint solvers for parameterized systems of nonlinear equations. ACM
Transactions on Mathematical Software. To appear.
[14] N. Saﬁran, J. Lotz, and U. Naumann. Algorithmic Diﬀerentiation of Numerical Methods: SecondOrder Tangent and Adjoint Solvers for Systems of Parametrized Nonlinear Equations. Technical
Report AIB-2014-07, RWTH Aachen University, 2014.

2 Refer

to the AD community’s web portal www.autodiff.org for further information.

3 www.nag.co.uk

238

