Procedia Computer Science
Volume 51, 2015, Pages 266–275
ICCS 2015 International Conference On Computational Science

A novel Factorized Sparse Approximate Inverse
preconditioner with supernodes
Massimiliano Ferronato1 , Carlo Janna1 , and Giuseppe Gambolati1
Department ICEA, University of Padova, Padova, Italy
massimiliano.ferronato@unipd.it

Abstract
Krylov methods preconditioned by Factorized Sparse Approximate Inverses (FSAI) are an eﬃcient approach for the solution of symmetric positive deﬁnite linear systems on massively parallel
computers. However, FSAI often suﬀers from a high set-up cost, especially in ill-conditioned
problems. In this communication we propose a novel algorithm for the FSAI computation
that makes use of the concept of supernode borrowed from sparse LU factorizations and direct
methods.
Keywords: linear systems, preconditioned iterative methods, approximate inverse, parallel computing

1

Introduction

Current computer simulations, especially related to earth models, may easily require the computation of several millions or even billions of unknowns, and the eﬃcient solution to the sequences
of sparse linear systems of equations
Ax = b
(1)
may represent one of the most, and often the most, expensive tasks. Roughly speaking, two
main classes of algorithms are available, namely direct [3] and iterative methods [14]. The
former are based on the sparse Gaussian elimination procedure and obtain the solution x
in (1) with a number of operations that can be predicted a priori. The latter start from a
tentative guess solution and improve it through a problem- and algorithm-dependent number
of iterations. The ﬂop count of direct methods is typically much higher than that of eﬃcient
iterative methods, especially in sparse three dimensional problems. However, direct methods are
still competitive because of the use of the so-called supernodes, i.e., clusters of unknowns that are
grouped and processed together with cache eﬃcient dense linear algebra kernels. By distinction,
iterative methods typically use an indirect memory indexing that prevents an intensive use of
the processor cache. Recently, the supernodes have already been successfully used with iterative
methods in the ﬁeld of incomplete LU preconditioning [6, 15].
In recent years the growing diﬀusion of parallel computers has fostered the development
of algorithms able to take advantage of the potential oﬀered by multiple processors. Iterative
266

Selection and peer-review under responsibility of the Scientiﬁc Programme Committee of ICCS 2015
c The Authors. Published by Elsevier B.V.
doi:10.1016/j.procs.2015.05.238

FSAI preconditioner with supernodes

Ferronato, Janna and Gambolati

methods are in principle much more attractive than direct methods because of their intrinsic
better parallelism. The bottleneck, however, is generally the preconditioner, that can be expensive in terms of both computation and application to a vector. Approximate inverses can
address the demand for eﬃcient parallel preconditioner [1, 5, 10, 8]. They can be computed in
several ways, e.g., through a bi-orthogonalization process or a Frobenius norm minimization,
and are applied to a vector with a matrix-by-vector multiplication. Even though approximate
inverses can be eﬀective in a large number of problems, with ill-conditioned matrices their computation can be very time consuming and may represent a barrier to their diﬀusion in industrial
applications.
In the present communication, we address such a drawback by introducing the concept of
supernodes to accelerate the set-up stage of the Factorized Sparse Approximate Inverse (FSAI)
preconditioner [13] for symmetric positive deﬁnite (SPD) problems. Identifying a supernodal
structure in the system matrix A may help reduce signiﬁcantly both the cost for computing FSAI
and the iteration count to converge, with an improvement of the overall FSAI performance.

2

The FSAI preconditioner

The classical FSAI preconditioner M −1 for an SPD matrix A reads:
M −1 = GT G

A−1

(2)

where G is computed minimizing the Frobenius norm
I − GL

F

(3)

over the set WS of matrices having a prescribed lower triangular non-zero pattern S. The
matrix L in (3) is the exact lower triangular factor of A and actually is not required to get G.
In fact, diﬀerentiating (3) with respect to the G entries gij and setting to zero give:
[GA]ij = [LT ]ij

∀ (i, j) ∈ S

(4)

Since LT is upper triangular and S is a lower triangular pattern, the matrix equation (4) can
be rewritten as:
0
i = j, (i, j) ∈ S
[GA]ij =
(5)
i=j
lii
where [·]ij is the entry in row i and column j of the matrix between square brackets, and lii
is the i-th diagonal element of L. L is unknown, so lii in (5) is replaced by 1. The matrix G
computed by solving:
[GA]ij = δij
(6)
with δij the Kronecker delta, is scaled as:
G = DG,

D = [diag(G)]−1/2

(7)

thus obtaining the matrix G used in the FSAI deﬁnition (2). The scaling (7) ensures that the
diagonal entries of the preconditioned matrix GAGT are unitary, and its Kaporin condition
number is minimum over all matrices G ∈ WS [11, 12]. The Kaporin number of an SPD
matrix is deﬁned as the ratio between the arithmetic and geometric mean of its eigenvalues and
gives a measure of the number of iterations required by the Preconditioned Conjugate Gradient
(PCG) method to converge. The FSAI preconditioner is very robust as it can be computed
267

FSAI preconditioner with supernodes

Ferronato, Janna and Gambolati

for any choice of the non-zero pattern S and the resulting preconditioned matrix is SPD by
construction.
The main computational cost in the FSAI set-up is the solution of the sequence of n small
dense linear systems, n being the size of A, arising from the component-wise equation (5). In
particular, it depends on the non-zero pattern S that can be selected either statically, i.e.,
prescribed a priori, or dynamically, i.e., generated during the computation of G. In this work
we introduce the use of supernodes in the static FSAI computation.

2.1

Supernodes

The eﬀective a priori selection of S usually is not trivial. Inspection of the Neumann power
series of A−1 suggests the use of non-zero patterns of small powers of A, recalling that for FSAI
only the lower triangular pattern is needed. This idea may be eﬀectively combined with both
preﬁltration [2] and recursion [7].
Recalling equation (5), it can be observed that the i-th row of G is computed by solving
a dense mi × mi system, with mi the number of non-zeroes assigned to row i. As a major
consequence, the cost for computing G is asimptotically proportional to ni=1 m3i and grows
very quickly with mi . Supernodes aim at reducing such a cost by aggregating the solution of
the systems required by diﬀerent rows in one dense system only. In the sequel, we will often
use the term “node” to denote a row of A, as it is typically done in graph theory.
Suppose that S is given and denote by Pi the set of column indices belonging to the i-th
row of S:
Pi = {j : (i, j) ∈ S}
(8)
A[Pi , Pi ] is the submatrix of A consisting of the elements with row and column indices in Pi .
Denote by g i and g i the dense vectors collecting the non-zero entries prescribed in the i-th row
of G and G, respectively. With this notation the component-wise equation (6) is equivalent to:
A[Pi , Pi ]g i = emi

i = 1, . . . , n

(9)

where emi = [0, 0, . . . , 1]T is the mi -th vector of the canonical basis of Rmi . The vector g i is
g i scaled by the square root of its last entry:
gi =

gi
gi,mi

(10)

For any row i, the most expensive operation is the solution of the system (9). If two rows,
say i1 and i2 , have a similar pattern, solving an enlarged system obtained from merging Pi1 and
Pi2 might be less expensive than solving two smaller systems. Let us clarify this idea using the
example of Figure 1. Assume i1 < i2 and P = Pi1 Pi2 , with mi1 , mi2 and m the cardinalities
of |Pi1 |, |Pi2 | and |P|, respectively. The vectors g i1 and g i2 can be found simultaneously by
solving the multiple right-hand side system:
A[P, P] g i1 g i2 = {b1 b2 }

(11)

with b1 , b2 ∈ Rm . Finding the explicit expression for b1 and b2 is easy. Deﬁne the two sets:
P1 = {j : j ∈ P and j ≤ i1 }
268

P2 = P \ P1

(12)

FSAI preconditioner with supernodes

Ferronato, Janna and Gambolati

A[Pi 1,Pi 1]

A[Pi 2,Pi 2]

i1

A[P,P]

i2

Figure 1: Schematic representation of the merging process of two rows, i1 and i2 . The entries
of A corresponding to Pi1 and Pi2 are denoted by circles and crosses, respectively. Squares
denote the A entries that do not belong to Pi1 or Pi2 but are collected in A[P, P].
with cardinalities m1 and m2 , respectively, and write (11) in the block form:
A[P1 , P1 ]
A[P2 , P1 ]

g i1 ,1
g i1 ,2

A[P1 , P2 ]
A[P2 , P2 ]

g i2 ,1
g i2 ,2

=

b1,1
b1,2

b2,1
b2,2

(13)

Then, we can set:
b1,1 = em1
b1,2 = H21 L−1
11 em1

b2,1 = 0
b2,2 = em2

(14)

where em1 and em2 are the m1 -th and the m2 -th vectors of the canonical basis of Rm1 and
Rm2 , respectively, and H21 and L−1
11 are part of the block factorization of A[P, P]:
A[P1 , P1 ]
A[P2 , P1 ]

A[P1 , P2 ]
A[P2 , P2 ]

=

L11
H21

0
L22

LT11
0

T
H21
T
L22

(15)

Note that the computation of b1,2 is relatively inexpensive since em1 has only one non-zero
component. This procedure can be generalized to merge an arbitrary number of rows.
It is now necessary to deﬁne a rule for deciding when it is convenient to aggregate two or
more rows into a supernode. The computational burden to gather and solve from scratch a
linear system of size m with l right-hand sides is:
3

c(m, l) =
i=0

a i mi + l

2

b i mi

(16)

i=0

where the coeﬃcients ai and bi depend on the hardware. Their values can be found by performing a least square regression on an ensemble of test runs. A supernode i merging i1 and i2 is
formed if c(m, 2) < c(m1 , 1) + c(m2 , 1), otherwise i1 and i2 are computed separately. A similar
argument is used if we want to merge the node ik to the supernode i that already aggregates
l nodes. Let mk and m denote the cardinality of Pik and P, respectively, and h the number
269

FSAI preconditioner with supernodes

Ferronato, Janna and Gambolati

of mismatches, that is the number of column indices lying in Pik but not in P. The node ik is
included into the supernode i if:
c(m + h, l + 1) < c(m, l) + c(mk , 1)

(17)

Condition (17) reduces the FSAI setup time and generally accelerates the PCG convergence.
In fact, the use of supernodes enlarges the non-zero pattern S, i.e., S ⊆ S, and the new
preconditioner minimizes the Kaporin condition number over a wider set. As a drawback, the
cost per iteration may grow because of the larger density. Hence, condition (17) can be relaxed
by introducing the parameter α:
c(m + h, l + 1) < α c(m, l) + c(mk , 1)

(18)

Finally, we deﬁne the score function:
sα (m, l, mk , h) = α c(m, l) + c(mk , 1) − c(m + h, l + 1)

(19)

A supernode is generated if sα > 0. The larger sα , the more cost eﬀective the merging.
To ﬁnd the supernodal structure a systematic comparison of all the nodes in the adjacency
graph is necessary. For this task an eﬃcient gready strategy based on level set traversals can
be employed. Level sets of a graph are deﬁned recursively:
• Basis: level 0 is a simple set of nodes;
• Recursion: level k + 1 includes all the neighbouring nodes of level k (adj(k)) not belonging
to level k − 1.
Our procedure works as follows. Let S and Q = ∅ be the initial static pattern and list of
supernodes, respectively. Level 0 is represented by node n only. Visit all the nodes in the
adjacency graph following the level set hierarchy and inspecting the elements within a level
from the last to the ﬁrst with respect to the original ordering. Level 0 is trivially processed by
adding node n to the list of supernodes Q. Level k is processed by exploring its nodes in reverse
order. Each node i is compared to the supernodes listed in Q to determine the maximum sα
value. If the maximum sα > 0 the current node i is merged, otherwise i is pushed into the head
of Q as a new supernode. The traversal is carried out in reverse order because it is most likely
that lower rows include upper rows as S is lower triangular. A drawback of this procedure can
occur when the list of supernodes Q becomes too long and comparing a new node with all the
supernodes may be too expensive. It can be observed that a node in a level generally shares
more column indices with the nodes from adjacent levels than those from far levels. Hence, it
is likely for a node to achieve the best score with a newly generated supernode. As a major
consequence, we can consider in our comparison the last lmax supernodes only. A reasonable
value for lmax is not overly diﬃcult to set. With lmax = 30, the cost of the above procedure
is negligible with respect to the FSAI set-up time, while larger lmax values slow down the
procedure without producing less supernodes.

3

Numerical results

The FSAI computational performance using supernodes is investigated in a set of large size
test problems arising from diﬀerent applications. All the test matrices are publicly available
270

FSAI preconditioner with supernodes

Name
Sebe
Emilia 923
Audikw 1
Serena
Bump 2911
Queen 4147

Size
742,793
923,136
943,695
1,391,349
2,911,419
4,147,110

Ferronato, Janna and Gambolati

# of non-zeroes
37,138,461
40,373,538
77,651,847
64,531,701
130,378,257
329,499,288

Problem description
3D pressure-temperature in porous media
3D geomechanical reservoir simulation
3D automotive structural problem
3D structural mechanics
3D geomechanical reservoir simulation
3D structural problem

Table 1: Size, number of non-zeroes and brief description of the test matrices.
a0
0.527655 · 10−5
b0
0.153699 · 10−5

a1
0.132448 · 10−5
b1
0.618331 · 10−7

a2
0.131749 · 10−7
b2
0.317156 · 10−8

a3
0.230335 · 10−9

Table 2: Cost model parameters, according to equation (16), for the IBM-BG/Q FERMI supercomputer.
from the University of Florida Sparse Matrix Collection [4] and are summarized in Table 1. In
all test cases the Preconditioned Conjugate Gradient (PCG) solver is used with the right-hand
side computed so that the solution is the unitary vector. The iterations are completed when
the relative residual is smaller than 10−10 . The computational performance is evaluated in
terms of the number of iterations niter , the wall clock time in seconds Tp and Ts needed for
the preconditioner computation and the PCG convergence, respectively, with the total time
Tt = Tp + Ts . The algorithm is coded in Fortran90 with OpenMP directives to exploit shared
memory architectures. All tests are performed on the IBM-BG/Q FERMI at the CINECA
Centre for High Performance Computing, equipped with IBM PowerA2 processors at 1.6 GHz
with 10,240 nodes, 163,840 computing cores, and 1 Gbyte/core of RAM. For each test case a
parallel run using 16 cores is performed. The cost model parameters ai and bi (equation (16))
for the IBM-BG/Q FERMI supercomputer are given in Table 2.
Let us consider the SPD matrix EMILIA 923. Table 3 shows the outcome of a set of simulations performed with this test case. The user-speciﬁed parameters κ, τ1 , and τ2 required to
set the static pattern have the following meaning:
• κ is the power of the lower triangular part of A used as reference pattern;
• τ1 is the pre-ﬁltration tolerance used to sparsify A before computing the pattern of Aκ ;
• τ2 is the post-ﬁltration tolerance used to sparsify G after its computation.
For more details on the meaning and the suggested range of the user-speciﬁed parameters, see
Janna et al. [9].
As expected, the use of supernodes decreases the cost for computing the preconditioner, but
the most signiﬁcant impact is on the PCG acceleration due to the increase of the preconditioner
density μ deﬁned as the ratio between the number of non-zeroes of G and A:
μ=

nnz(G)
nnz(A)

(20)

Table 3 provides also the average size dim˜i of the supernodes, denoting the average number of
rows collapsed to the same sparsity pattern. In the computation of the score sα we have set
271

FSAI preconditioner with supernodes

κ
1
2
3
4
5

Parameters
τ1
τ2
10−3 0.01
10−3 0.03
10−2 0.05
10−2 0.05
10−1 0.03

niter
4758
3143
2315
1749
1899

FSAI
Tp
Ts
5.1 200.7
22.7 146.3
11.2
97.8
34.9
77.5
4.6
76.0

Ferronato, Janna and Gambolati

Tt
205.8
169.0
109.0
112.4
80.6

dim˜i
10.3
6.4
5.6
6.1
3.2

FSAI with supernodes
niter
Tp
Ts
3189
4.1 154.1
2164 16.0 103.7
1720
9.9
75.9
1372 29.4
62.9
1616
2.8
66.9

Tt
158.2
119.7
85.8
92.3
69.7

Table 3: EMILIA 923 test case: FSAI computational performance with the standard pattern
(on the left) and the supernode approach (on the right) in a set of simulations. dim˜i is the
average size of the supernodes.
Test case
Sebe
Emilia 923
Audikw 1
Serena
Bump 2911
Queen 4147

S
62.5%
93.7%
57.8%
72.0%
80.0%
79.4%

max Rt
1.85
2.70
1.56
1.82
1.75
1.92

min Rt
0.83
0.98
0.83
0.95
0.88
0.92

Ri
1.06
1.27
1.01
1.03
1.41
1.23

Rp
1.19
1.49
1.12
1.20
1.19
1.12

Rs
0.97
1.20
0.95
1.01
1.25
1.14

Rt
1.09
1.32
1.03
1.08
1.23
1.12

Rμ
0.81
0.78
0.70
0.85
0.63
0.58

Table 4: Summary of the performance obtained with and without using supernodes.
α = 1. Usually, the larger dim˜i , the smaller the iteration count. In the previous example the
overall gain in terms of total wall clock time can be up to 25%.
The supernode eﬃciency is problem-dependent. An extensive numerical experimentation has
been performed on the set of matrices of Table 1 varying for each test case the user-speciﬁed
parameters κ, τ1 and τ2 . The total wall clock time obtained using FSAI with and without
supernodes is shown in Figure 2. In each diagram the experiments are reported according to
the ascending order of Tt . The numerical results show that supernodes are generally helpful.
In particular, the supernodes help reduce the variability range of the PCG performance with
κ, τ1 and τ2 , thus decreasing the solver sensitivity to the user-deﬁned parameters.
The global outcome of the numerical experiments for the diﬀerent test cases is summarized
in Table 4. The meaning of the symbols is as follows:
• S: percentage of experiments where the supernodes prove eﬀective in increasing the computational eﬃciency;
• max Rt : maximal Tt ratio without and with supernodes, i.e., a measure of the best computational gain obtained from the tests;
• min Rt : minimal Tt ratio without and with supernodes, i.e., the most unfavourable case;
• Ri : average niter ratio without and with supernodes;
• Rx , with x = p, s, t: average Tx ratio without and with supernodes;
• Rμ : average μ ratio without and with supernodes.
Table 4 shows that supernodes yield a performance improvement in the majority of experiments. Emilia 923 is the most favourable test case with the supernodes allowing for a best
computational gain of almost 3, and with most of the other matrices the best gain is not far
272

FSAI preconditioner with supernodes

Ferronato, Janna and Gambolati

800
static FSAI
static FSAI with supernodes

static FSAI
static FSAI with supernodes

800

600
Tt [s]

Tt [s]

600

1000

400

400

200
200

Sebe
Emilia_923

0

10

20

30
experiment id

40

0

50

20

30
40
experiment id

50

60

200

600
500

10

static FSAI
static FSAI with supernodes

150

static FSAI
static FSAI with supernodes

Tt [s]

Tt [s]

400
300

100

200
50
100
0

Audikw_1

10

20
30
experiment id

Serena
0

40

700
600

10

20
experiment id

30

40

3000

static FSAI
static FSAI with supernodes

2500

static FSAI
static FSAI with supernodes

500
Tt [s]

Tt [s]

2000

400
300

1000

200

500

Bump_2911

100
0

1500

10

20
experiment id

30

40

0

Queen_4147

10

20
experiment id

30

Figure 2: Static FSAI: total wall clock time [s] with and without supernodes. For each experiment id diﬀerent user-speciﬁed parameters have been used within a plausible range.

from 2. Where supernodes are not convenient, the eﬃciency loss is limited to a small percentage, say 10-15% in the worst cases. Also note that on the average the iteration count and the
time for computing the preconditioner is generally reduced with supernodes, while the time for
iterating may increase because of a denser FSAI preconditioner.
273

FSAI preconditioner with supernodes

4

Ferronato, Janna and Gambolati

Conclusions

The technique of supernodes is widely and successfully used in the parallel solution of sparse
linear systems by direct methods. The concept is here developed for the FSAI preconditioning
of SPD systems. In the present work, we have introduced the supernodes in static FSAI and
performed a numerical experimentation of their performance in a set of test problems.
The supernodal structure is generated evaluating the cost for computing each row of G.
A set of rows are merged into a supernode if the cost for solving a larger multiple right-hand
side system is lower than that required for several smaller systems. This procedure is helpful
for reducing the FSAI set-up time, improving the preconditioner quality and decreasing the
iteration count for the PCG convergence. The use of supernodes is cost eﬀective in 75% of the
runs carried out, allowing for a PCG scheme that may be up to about three times faster than
the standard FSAI. In cases where supernodes are not convenient, the total wall-clock time
increase is limited to 10-15% only. Moreover, the use of supernodes reduce to some extent the
solver sensitivity to the user-speciﬁed parameters, thus improving the overall robustness of the
ﬁnal PCG scheme.
Currently, the supernodal concept is going to be extended to two FSAI variants: (i) for
non-symmetric and/or symmetric indeﬁnite matrices, and (ii) using a dynamic computation
of the FSAI non-zero pattern, that is generally more eﬃcient than static FSAI especially in
ill-conditioned problems.

References
References
[1] M. Benzi. Preconditioning techniques for large linear systems: a survey. J. Comp. Phys., 182
(2002), pp. 418–477.
[2] E. Chow. A priori sparsity patterns for parallel sparse approximate inverse preconditioners. SIAM
J. Sci. Comput., 21 (2000), pp. 1804–1822.
[3] T. A. Davis. Direct Methods for Sparse Linear Systems. Philadelphia (PA), SIAM, 2006.
[4] T. A. Davis and Y. Hu. The University of Florida sparse matrix collection. ACM Trans. Math.
Softw., 38 (2011), pp. 1–25.
[5] M. Ferronato. Preconditioning of sparse linear systems at the dawn of the 21st century: history,
current developments, and future perspectives. ISRN Appl. Math., 2012 (2012), article ID 127647,
doi: 10.5402/2012/127647.
[6] A. Gupta and T. George. Adaptive techniques for improving the performance of incomplete factorization preconditioning. SIAM J. Sci. Comput., 32 (2010), pp. 84–110.
[7] T. Huckle. Approximate sparsity patterns for the inverse of a matrix and preconditioning. Appl.
Numer. Math., 30 (1999), pp. 291–303.
[8] C. Janna, M. Ferronato and G. Gambolati. Enhanced block FSAI preconditioning using domain
decomposition techniques. SIAM J. Sci. Comput., 35 (2013), pp. S229–S249.
[9] C. Janna, M. Ferronato, F. Sartoretto and G. Gambolati. FSAIPACK: a software package for high
performance FSAI preconditioning. ACM Trans. Math. Softw., 41 (2015), pages to be deﬁned.
[10] Z. Jia and B. Zhu. A power sparse approximate inverse preconditioning procedure for large sparse
linear systems. Numer. Lin. Alg. Appl., 16 (2009), pp. 259–299.
[11] I. E. Kaporin. A preconditioned conjugate gradient method for solving discrete analogs of diﬀerential problems. Diﬀ. Equat., 26 (1990), pp. 897–906.

274

FSAI preconditioner with supernodes

Ferronato, Janna and Gambolati

[12] I. E. Kaporin. New convergence results and preconditioning strategies for the conjugate gradient
method. Numer. Lin. Alg. Appl., 1 (1994), pp. 179–210.
[13] L. Y. Kolotilina and A. Y. Yeremin. Factorized sparse approximate inverse preconditioning. I.
Theory. SIAM J. Mat. Anal. Appl., 14 (1993), pp. 45–58.
[14] Y. Saad. Iterative Methods for Sparse Linear Systems. Philadelphia (PA), SIAM, 2003.
[15] N. Vannieuwenhoven and K. Meerbergen. IMF: An incomplete multifrontal LU-factorization for
element-structured sparse linear systems. SIAM J. Sci. Comput., 35 (2013), pp. A270–A293.

275

