Procedia Computer Science
Volume 51, 2015, Pages 276–285
ICCS 2015 International Conference On Computational Science

Nonsymmetric preconditioning for conjugate gradient and
steepest descent methods∗
Henricus Bouwmeester1 , Andrew Dougherty1 , and Andrew V. Knyazev2
1

University of Colorado Denver, USA
Henricus.Bouwmeester@ucdenver.edu and Andrew.Dougherty@ucdenver.edu
2
Mitsubishi Electric Research Laboratories, Cambridge, MA, USA
Andrew.Knyazev@merl.com, http://www.merl.com/people/knyazev

Abstract
We analyze a possibility of turning oﬀ post-smoothing (relaxation) in geometric multigrid when
used as a preconditioner in preconditioned conjugate gradient (PCG) linear and eigenvalue
solvers for the 3D Laplacian. The geometric Semicoarsening Multigrid (SMG) method is provided by the hypre parallel software package. We solve linear systems using two variants (standard and ﬂexible) of PCG and preconditioned steepest descent (PSD) methods. The eigenvalue problems are solved using the locally optimal block preconditioned conjugate gradient
(LOBPCG) method available in hypre through BLOPEX software. We observe that turning
oﬀ the post-smoothing in SMG dramatically slows down the standard PCG-SMG. For ﬂexible
PCG and LOBPCG, our numerical tests show that removing the post-smoothing results in
overall 40–50 percent acceleration, due to the high costs of smoothing and relatively insigniﬁcant decrease in convergence speed. We demonstrate that PSD-SMG and ﬂexible PCG-SMG
converge similarly if SMG post-smoothing is oﬀ. A theoretical justiﬁcation is provided.
Keywords: linear equations, eigenvalue, iterative, multigrid, smoothing, preconditioning, convergence,
nonsymmetric, conjugate gradient, steepest descent, parallel, hypre, BLOPEX, LOBPCG

1

Introduction

Smoothing (relaxation) and coarse-grid correction are the two cornerstones of multigrid techniques. In algebraic multigrid, where only the system matrix is (possibly implicitly) available,
smoothing is more fundamental since it is often used to construct the coarse grid problem.
In geometric multigrid, the coarse grid is generated by taking into account the geometry of the
ﬁne grid, in addition to the chosen smoothing procedure. If full multigrid is used as a standalone solver, proper smoothing is absolutely necessary for convergence. If multigrid is used as
a preconditioner in an iterative method, one is tempted to check what happens if smoothing is
turned partially oﬀ, although we have actually ﬁrst experienced it by accident in a class project.
∗A

276

preliminary version posted at http://arxiv.org/abs/1212.6680
Selection and peer-review under responsibility of the Scientiﬁc Programme Committee of ICCS 2015
c The Authors. Published by Elsevier B.V.

doi:10.1016/j.procs.2015.05.241

Nonsymmetric preconditioning for CG and SD

Bouwmeester, Dougherty, and Knyazev

For symmetric positive deﬁnite (SPD) linear systems, the preconditioner is typically required
to be also a ﬁxed linear SPD operator, to preserve the symmetry of the preconditioned system;
for exceptions, see, e.g. [4, Section 12.3], [5, 15], and [17, Section 10.2]. In the multigrid
context, the preconditioner symmetry is achieved by using balanced pre- and post-smoothing,
and by properly choosing the restriction and prolongation pair. In order to get a ﬁxed linear
preconditioner, all multigrid components, such as smoothing, restriction, prolongation, and
coarse solves, must be linear. The positive deﬁniteness is obtained by performing enough, e.g.,
in practice, even one may be enough, pre- and post-smoothing steps, where the number of the
pre-smoothing steps and the number of the post-smoothing steps must be equal; see, e.g., [6].
If smoothing is unbalanced, e.g., there is one step of pre-smoothing, but no post-smoothing,
the multigrid preconditioner becomes nonsymmetric. Traditional assumptions of the standard
convergence theory of iterative solvers are no longer valid, and convergence behavior may be unpredictable. The main goals of this paper are describing our numerical experience experimenting
with the inﬂuence of unbalanced smoothing in practical geometric multigrid preconditioning,
speciﬁcally, the Semicoarsening Multigrid (SMG) method, see [16], provided by the parallel
software package hypre [1] and explaining theoretically the observed interesting behavior.
We numerically analyze the possibility of turning oﬀ the post-smoothing in the geometric
multigrid used as a preconditioner in iterative linear and eigenvalue solvers for the 3D Laplacian
in hypre. The linear systems are solved using two variants (standard and ﬂexible, e.g., [8]) of the
preconditioned conjugate gradient (PCG) and preconditioned steepest descent (PSD) methods.
The eigenvalue problems are solved using the locally optimal block preconditioned conjugate
gradient (LOBPCG) method, see [9], readily available in hypre through BLOPEX [3].
We observe that turning oﬀ the post-smoothing in SMG dramatically slows down the standard PCG-SMG. However, for the ﬂexible PCG and LOBPCG, our numerical tests show that
post-smoothing can be dropped. Moreover, turning oﬀ the post-smoothing in SMG results in
overall acceleration, due to the high costs of smoothing and relatively insigniﬁcant decrease in
convergence speed. Our observations are also expected to be generally applicable for algebraic
multigrid preconditioning, e.g., for graph Laplacians used in spectral image segmentation, as
originally tested in [10] and appearing, e.g., in computational photography problems [14].
Our numerical experiments are executed in both a strictly shared memory environment
and in a distributed memory environment, demonstrating that the eﬀect of the acceleration
does not depend on the memory interconnection speed. All our numerical tests use multigrid
preconditioning, see also [7]; however, our theory predicts that nonsymmetric preconditioning
for SPD linear systems and eigenvalue problems could be eﬃcient in general, e.g., for domain
decomposition and inexact factorization preconditioning.
A diﬀerent case of non-standard preconditioning, speciﬁcally, variable preconditioning,
in PCG is considered in our earlier work [11]. There, we also ﬁnd a dramatic diﬀerence in
convergence speed between the standard and ﬂexible versions of PCG. The better convergence
behavior of the ﬂexible PCG is explained in [11] by its local optimality, which guarantees its
convergence with at least the speed of PSD. Our numerical tests in [11] show that, in fact, the
convergence of PSD is practically similar to the convergence of the ﬂexible PCG. We perform
the same comparison in this work, and obtain analogous results. We demonstrate for SPD linear systems that PSD-SMG converges almost as fast as the ﬂexible PCG-SMG method, while
the standard PCG-SMG method stalls, if the SMG post-smoothing is oﬀ
The rest of the paper is organized as follows. We formally describe the PSD and PCG
methods used for testing in this work, and explain their diﬀerences in Section 2. In Section 3,
we brieﬂy discuss the SMG preconditioning in hypre and present our numerical results for linear
systems. Section 4 deals with eigenvalue problems. Section 5 contains some relevant theory.
277

Nonsymmetric preconditioning for CG and SD

2

Bouwmeester, Dougherty, and Knyazev

PSD and PCG methods for linear systems

For a general exposition of PSD and PCG, let SPD matrices A and T , and vectors b and x0 be
given, and denote rk = b − Axk . Algorithm 1 is described in [11]
Algorithm 1: PSD and PCG methods
1
2
3
4
5
6
7
8
9
10
11

for k = 0, 1, . . . do
sk = T rk
if k = 0 then
p0 = s 0
else
pk = sk + βk pk−1 (where βk is either (1) or (2) for all iterations)
end
(sk , rk )
αk =
(pk , Apk )
xk+1 = xk + αk pk
rk+1 = rk − αk Apk
end
Various methods are obtained by using diﬀerent formulas for the scalar βk . We set
(sk , rk )
(sk−1 , rk−1 )

(1)

(sk , rk − rk−1 )
(sk−1 , rk−1 )

(2)

βk =
for the standard PCG, or
βk =

for the ﬂexible PCG, or βk = 0 for PSD,
We note that in using (2), we are merely subtracting one term, (sk , rk−1 ), in the numerator
of (1), which appears in the standard CG algorithm. If T is a ﬁxed SPD matrix, this term in
fact vanishes; see, e.g., [11]. By using (2) in a computer code, it is required that an extra vector
be allocated to either calculate rk −rk−1 or store −αk Apk , compared to (1). The associated cost
increase may be noticeable for large problems solved on parallel computers. To measure the
actual increase, we numerically evaluate the standard and ﬂexible PCG with no preconditioning
for a variety of problem sizes.
Our model problem used for all calculations in the present paper is for the three-dimensional
negative Laplacian in a brick with homogeneous Dirichlet boundary conditions approximated
by the standard ﬁnite diﬀerence scheme using the 7-point stencil with the grid size one in all
three directions. The number of the grid points varies in our tests, resulting in the size of the
matrix A in the range from sixteen thousand to over two billion; see Appendix A for details.
For the CG method without preconditioning, we observe a 20-25% cost overhead per iteration incurred due to the extra storage and calculation for the ﬂexible variant, (2), relative to the
standard variant, (1) over the whole range of the matrix sizes and in both shared and distributed
memory tests. For PCG, the cost overhead of using formula (2) vs. (1) per iteration is smaller,
because of the additional costs of the application of the preconditioner. Speciﬁcally for SMG
preconditioning, described in the next section, the cost overhead per iteration is negligible.
278

Nonsymmetric preconditioning for CG and SD

3

Bouwmeester, Dougherty, and Knyazev

Preconditioning linear systems with SMG

The CG method can be accelerated by preconditioning. We use the SMG solver as a preconditioner, provided by hypre. The SMG solver/preconditioner uses plane-relaxation as a smoother
at each level in the V-cycle; see [1]. The SMG preconditioner is ﬁxed and linear, according
to [16], and uses pre- and post-relaxation smoothing steps. In our tests, we either use the “balanced relaxation,” i.e. one step of pre-relaxation and one step of post-relaxation, resulting in
an SPD preconditioner, as required by PCG, or “no post-relaxation,” still with one step of prerelaxation, but turning oﬀ the post-relaxation smoothing, thus making the SMG preconditioner
nonsymmetric, violating the standard PCG assumptions.
We present results of numerical experiments, preconditioning linear systems for the threedimensional negative Laplacian using SMG. In all timing and iteration count ﬁgures below,
the horizontal axis represents the number n of grid points in each of the three directions per
processor. The size of the matrix is thus np × n × n × n, where np denotes the number of
processors (cores), e.g. n = 180 gives the problem size 16 × 180 × 180 × 180 = 93, 312, 000 for
np = 16 on one node and 384 × 180 × 180 × 180 = 2, 239, 488, 000 for np = 384 = 16 × 24 on 24
nodes with 16 processors (cores) on each node, using distributed memory.
100

18
16

# iterations

||r||C /||b||C

14
10−5

10−10

12
10
8
6

PCG-SMG with (1) and no post-relaxation
PSD-SMG and no post-relaxation
PCG-SMG with (2) and no post-relaxation

10−15
2

4

6
8
10
iteration number

12

14

(a) Convergence on 16 processors on 1 node, n = 80

PSD-SMG with no post-relaxation
PCG-SMG with (2) and no post-relaxation
PCG-SMG with (1) and balanced relaxation
PCG-SMG with (2) and balanced relaxation

4
2
0

20

40

60

80

100
n

120

140

160

180

(b) Iteration count on 16 processors on 1 node

Figure 1: Convergence and iteration count for PCG-SMG using (1) or (2) and PSD-SMG
Figure 1 displays the convergence history (left panel, Figure 1a, where n = 80) and the
iteration count (right panel, Figure 1b) for PSD-SMG and PCG-SMG using (1) or (2) with
and without balanced SMG relaxation on 16 processors on 1 node. If no post-relaxation is
performed within the SMG preconditioner, the convergence of the standard PCG method, i.e.
with (1), is dramatically slowed down, compared to PCG-SMG using (2) and even to PSDSMG, as demonstrated in Figure 1a. We thus drop the standard PCG method with (1) with
no SMG post-relaxation from further consideration, as evidently non-competitive, and plot
the iteration count reaching the default tolerance for other methods in Figure 1b. Speciﬁcally, Figure 1b compares the iteration count of PSD-SMG and PCG-SMG using (2), both
with no post-relaxation, with the ground truth, PCG-SMG with the same number of pre- and
post-relaxation steps, one, in SMG that creates an SPD preconditioner, and makes formulas
(1) and (2) mathematically equivalent generating the identical iteration count in Figure 1b.
We observe in Figure 1b a 50 (60) percent slowdown of PCG-SMG using (2) (PSD-SMG) with
no post-relaxation, compared to PCG-SMG with (1) or (2) and the balanced relaxation.
279

Nonsymmetric preconditioning for CG and SD

Bouwmeester, Dougherty, and Knyazev

The surprising convergence behavior in Figure 1 is similar to that observed in [11], where
a variable SPD preconditioner makes the standard PCG, i.e. using (1), almost stall, while the
convergence rates of PSD and ﬂexible PCG, i.e. with (2), are good and close to each other.
However, the SMG preconditioner is ﬁxed and linear, according to its description in [16] and our
numerical veriﬁcation, in contrast to the variable preconditioner in [11]. Moreover, turning oﬀ
the post-relaxation smoothing in the multigrid preconditioner makes it nonsymmetric—the case
not covered in [11], where the assumption is made that the preconditioner is SPD. We revisit
the theoretical arguments of [11] in Section 5, and ﬁnd that the SPD preconditioner assumption
in [11] is actually never signiﬁcantly used and can be dropped, as in [17, Section 10.2], providing
theoretical justiﬁcations for the unexpected numerical results displayed in Figure 1.
Timing results on 16 processors on 1 node

Timing results on 16 processors each on 24 nodes

350

350
PCG-SMG with (1) and balanced relaxation
PCG-SMG with (2) and balanced relaxation
PCG-SMG with (2) and no post-relaxation

250
200
150
100
50
0

PCG-SMG with (1) and balanced relaxation
PCG-SMG with (2) and balanced relaxation
PCG-SMG with (2) and no post-relaxation

300
cpu clock time (sec)

cpu clock time (sec)

300

250
200
150
100
50

20

40

60

80

100
n

120

(a) Shared memory

140

160

180

0

20

40

60

80

100
n

120

140

160

180

(b) Distributed memory

Figure 2: Cost comparison of relaxation in PCG-SMG using (1) or (2)
Our next numerical results are even more amazing. Figure 2 compares the CPU timing
of the ﬂexible PCG-SMG using (2) with no post-relaxation with the ground truth, which is
PCG-SMG with the same number of pre- and post-relaxation steps, one, in SMG that creates
an SPD preconditioner. PCG-SMG with (1) and PCG-SMG with (2), both with the balanced relaxation, generate nearly the same iterative approximations, as expected, since they
are mathematically equivalent (up to round-oﬀ errors) due to the fact that the only diﬀerent
term (sk , rk−1 ) between (1) and (2) is known to vanish if the preconditioner in SPD. Although
using (2) introduces some extra timing overhead, in the case of the SMG preconditioning the
overhead is negligible, as can be seen in Figure 2, where the timing curves CG-SMG with (1)
and with (2) are nearly indistinguishable, since the SMG preconditioning, ﬁrst, is relatively
expensive computationally and, second, its high quality makes the PCG method converge fast.
The results displayed in Figure 2 are easy to explain, knowing that the relaxation step in
SMG is the main contributor to the computational costs of applying the SMG preconditioner on
every iteration. Turning oﬀ the post-relaxation and keeping one step of pre-relaxation, on the
one hand, cuts almost in half the time of application of the SMG preconditioner per iteration.
On the other hand, as seen in Figure 1b, it results in 50 percent slowdown of ﬂexible PCG-SMG
using (2), not enough to outweigh the double cost reduction of not using the post-relaxation
in the SMG preconditioner. The overall time saving is thus expected to be approximately
75 percent, corresponding to the improvement of approximately 40 percent, actually observed
in Figure 2 for all problem sizes tested both for shared and distributed memory.
280

Nonsymmetric preconditioning for CG and SD

4

Bouwmeester, Dougherty, and Knyazev

Preconditioning of eigenvalue problems with SMG

The LOBPCG method [9] computes the m smallest eigenvalues of a linear operator and is
implemented within hypre via BLOPEX; see [3]. We conclude our numerical experiments with
a comparison of the use of balanced vs. unbalanced relaxation in the SMG preconditioner for the
LOBPCG method with m = 1, keeping the same setup as in the previous section. Figure 3 top
panels display the numbers of LOBPCG iterations for the non-balanced SMG preconditioner
being only 0–40% more than that for the balanced SMG preconditioner. Thus, the cost savings
of not using the post-relaxation in the SMG preconditioner may lead to over 50% acceleration,
compared to the conventional balanced relaxation, as Figure 3 bottom panels indeed show.

Iteration count on 16 processors each on 24 nodes
10

LOBPCG-SMG with balanced relaxation
LOBPCG-SMG with no post-relaxation

LOBPCG-SMG with balanced relaxation
LOBPCG-SMG with no post-relaxation

9
8
7
# iterations

# iterations

Iteration count on 16 Processors on 1 node
15
14
13
12
11
10
9
8
7
6
5
4
3
2
1
0

6
5
4
3
2
1

20

40

60

80

100

0

120

20

40

60

n
(a) Shared memory

Timing on 16 Processors on 1 node

Timing on 16 processors each on 24 nodes

LOBPCG-SMG with balanced relaxation
LOBPCG-SMG with no post-relaxation

60
cpu clock time (sec)

cpu clock time (sec)

120

70

50
40
30
20
10
0

100

(b) Distributed memory

70
60

80
n

LOBPCG-SMG with balanced relaxation
LOBPCG-SMG with no post-relaxation

50
40
30
20
10

20

40

60

80
n

(c) Shared memory

100

120

0

20

40

60

80

100

120

n
(d) Distributed memory

Figure 3: Iteration comparison of relaxation levels for LOBPCG-SMG
LOBPCG is locally optimal, requiring no changes in the code to handle nonsymmetric
preconditioning, even though the existing LOBPCG convergence theory in [9, 13] assumes an
SPD preconditioner T . We explain the convergence for a nonsymmetric T in the next section.
281

Nonsymmetric preconditioning for CG and SD

5

Bouwmeester, Dougherty, and Knyazev

Nonsymmetric preconditioning theoretical justiﬁcation

For linear systems with SPD coeﬃcient matrices, the use of nonsymmetric and variable preconditioning in PSD and PCG-like methods has been justiﬁed, e.g., in [4, Section 12.3], [5, 15], and
[17, Section 10.2]. Many key properties of Krylov-type methods are lost, such as orthogonality
and global optimality. However, it is shown in [17, Section 10.2] that the ﬂexible PCG, i.e.
using (2), is locally optimal, i.e. on every step it converges not slower than PSD, no matter
whether the preconditioner is SPD or not. The convergence rate bound for PSD, where βk = 0,
which thus also holds for ﬂexible PCG, with βk given by (2), is established in [17, Section 10.2],
rk+1

A−1

≤ δ rk

A−1 ,

(3)

under the assumption that the preconditioner T , which is non necessarily SPD, satisﬁes
I − AT

A−1

≤ δ < 1,

(4)
√

where · A−1 denotes the operator norm induced by the corresponding vector norm x A−1 x.
The key identity for PSD that can help deriving (3) is presented in the following theorem.
Theorem 1. If βk = 0 in Algorithm 1, then the identity holds,
rk+1

A−1 /

rk

A−1

= sin (∠A−1 {rk , AT rk }) ,

(5)

where the angle in the right-hand side is deﬁned via
cos (∠A−1 {rk , AT rk }) =

rk

|(rk ) T rk |
AT rk

A−1

A−1

.

Proof. Identity (5) is already actually proved, although not explicitly formulated, in the proof
of [17, Theorem 10.2]. Alternatively, substituting rk = Aek , identity (5) is equivalent to
ek+1

A/

ek

A

= sin (∠A {ek , T Aek }) ,

which is the statement of [11, Lemma 4.1]. Reference [11] assumes that the preconditioner T is
SPD, but this assumption is never, in fact, used in the proof of [11, Lemma 4.1].
Assumption (4) is simple, but has one signiﬁcant drawback—it does not allow arbitrary
scaling of the preconditioner T , while the PCG and PSD methods are invariant with respect to
scaling of T. The way around it is to scale the preconditioner T before assumption (4) is veriﬁed.
We now illustrate such a scaling under an additional assumption that T is SPD, following [11],
ﬁrst connecting assumption (4) with its equivalent and arguably more traditional form.
Theorem 2. Let the preconditioner T be SPD. Then assumption (4) is equivalent to
I − TA

T −1

≤ δ < 1.

(6)

Proof. Since T is SPD, on the one hand, the matrix product AT is also SPD, but with respect
to the A−1 scalar product. This implies that assumption (4) is equivalent to the statement
that Λ(AT ) ∈ [1 − δ, 1 + δ] with δ < 1, where Λ(·) denotes the matrix spectrum. On the other
hand, the matrix product T A is SPD as well, with respect to the T −1 scalar product. Thus,
assumption (6) is equivalent to the statement that Λ(T A) ∈ [1 − δ, 1 + δ]. This means the
equivalence of assumptions (4) and (6), since Λ(AT ) = Λ(T A).
282

Nonsymmetric preconditioning for CG and SD

Bouwmeester, Dougherty, and Knyazev

Let us now, without loss of generality, as in [12, p. 96] and [11, pp. 1268–1269], always
scale the SPD preconditioner T in such a way that max{Λ(T A)} + min{Λ(T A)} = 2. Then
we have δ = (κ(T A) − 1)/(κ(T A) + 1) and, vice versa, κ(T A) = (1 + δ)/(1 − δ), where κ(·)
denotes the matrix spectral condition number. The convergence rate bound (3) for the PSD with
nonsymmetric preconditioning in this case turns into the standard PSD convergence rate bound
for the case of SPD preconditioner T ; see. e.g., [11, Bound (1.3)]. Moreover, [11, Theorem 5.1]
shows that this convergence rate bound is sharp for PSD, and cannot be improved for ﬂexible
PCG, i.e. using (2), if the SPD preconditioner T changes on every iteration. The latter result
naturally extends to the case of nonsymmetric preconditioning of [17, Section 10.2].
Compared to linear systems, eigenvalue problems are signiﬁcantly more complex. Sharp convergence rate bounds for symmetric eigenvalue problems have been obtained in the last decade,
and only for the simplest preconditioned method; see [12, 13] and references therein. A possibility of using nonsymmetric preconditioning for symmetric eigenvalue problems has not been
considered before, to our knowledge. However, our check of arguments of [12] and preceding
works, where a PSD convergence rate bound is proved assuming (4) and SPD preconditioning,
surprisingly reveals that the latter assumption, SPD, is actually never signiﬁcantly used, and
can be dropped without aﬀecting the bound; see our idea implemented in details in [2].
The arguments above lead us to a surprising determination that whether or not the preconditioner is SPD is of no importance for PSD convergence, given the same quality of preconditioning, measured by (4) after preconditioner prescaling. If the preconditioner is ﬁxed SPD then
the standard PCG is the method of choice. The cases, where the preconditioner is variable or
nonsymmetric, are similar to each other—the standard non-ﬂexible PCG, using (1), stalls, while
the ﬂexible PCG, with (2), converges, due to its local optimality, but may not be much faster
compared to PSD. This explains the numerical results using nonsymmetric preconditioning
reported in this work, as related to results of [11] for variable SPD preconditioning.

Conclusions
Preconditioning for linear systems and eigenvalue problems with symmetric positive deﬁnite
(SPD) matrices may require extra design and computational eﬀorts to set up and apply preconditioners that are ﬁxed and also SPD, to ﬁt the standard theory of the preconditioned
steepest descent (PSD) and preconditioned conjugate gradient (PCG) methods. In contrast to
the variable preconditioning, the nonsymmetric preconditioning for SPD linear systems and
eigenvalue problems has attracted less attention. We test nonsymmetric multigrid preconditioning in hypre software. The geometric multigrid without post-relaxation is demonstrated to
be surprisingly eﬃcient as a preconditioner for locally optimal iterative methods, such as the
ﬂexible PCG for linear systems and LOBPCG for eigenvalue problems, leading to a 40–50%
acceleration, compared to the standard SPD preconditioning using the balanced relaxation.
Numerical and theoretical explanations are given, by comparing ﬂexible PCG to PSD and by
showing that the PSD convergence is robust even if the preconditioner is nonsymmetric.

Acknowledgment
The authors would like to thank Rob Falgout, Van Henson, Panayot Vassilevski, and other
members of the hypre team for their attention to our numerical results reported here and the
Center for Computational Mathematics University of Colorado Denver for the use of the cluster.
This work has partially been supported by NSF awards CNS 0958354 and DMS 1115734.
283

Nonsymmetric preconditioning for CG and SD

Bouwmeester, Dougherty, and Knyazev

References
[1] HYPRE User’s Manual. Center for Applied Scientiﬁc Computing, Lawrence Livermore National Lab, 2012. URL: https://computation.llnl.gov/casc/hypre/software.html.
[2] Merico E Argentati, Andrew V Knyazev, Klaus Neymeyr, Evgueni E Ovtchinnikov, and
Ming Zhou. Convergence theory for preconditioned eigenvalue solvers in a nutshell. arXiv
preprint arXiv:1412.5005, 2014. URL: http://arxiv.org/abs/1412.5005.
[3] Merico E. Argentati, Ilya Lashuk, Andrew V. Knyazev, and Evgenii E. Ovtchinnikov. Block
locally optimal preconditioned eigenvalue Xolvers (BLOPEX) in HYPRE and PETSC.
SIAM J. Sci. Comput., 29(4):2224–2239, 2007. doi:10.1137/060661624.
[4] Owe Axelsson. Iterative solution methods. Cambridge University Press, Cambridge, 1994.
[5] Radim Blaheta. GPCG-generalized preconditioned CG method and its use with non-linear
and non-symmetric displacement decomposition preconditioners. Numer. Linear Algebra
Appl., 9(6-7):527–550, 2002. doi:10.1002/nla.295.
[6] James H. Bramble and Xuejun Zhang. The analysis of multigrid methods. In P.G. Ciarlet
and J.L. Lions, editors, Solution of Equation in Rn (Part 3), Techniques of Scientiﬁc Computing, volume 7 of Handbook of Numerical Analysis, pages 173–415. Elsevier, 2000. URL:
http://www.sciencedirect.com/science/article/pii/S1570865900070034, doi:10.
1016/S1570-8659(00)07003-4.
[7] Laurent Debreu, Emilie Neveu, Ehouarn Simon, Fran¸cois-Xavier Le Dimet, and Arthur
Vidard. Multigrid solvers and multigrid preconditioners for the solution of variational
data assimilation problems. October 2013. URL: https://hal.inria.fr/hal-00874643.
[8] Gene H. Golub and Qiang Ye. Inexact preconditioned conjugate gradient method with
inner-outer iteration. SIAM J. Sci. Comput., 21(4):1305–1320, 1999. URL: http://link.
aip.org/link/?SCE/21/1305/1, doi:10.1137/S1064827597323415.
[9] Andrew V. Knyazev. Toward the optimal preconditioned eigensolver: Locally optimal block
preconditioned conjugate gradient method. SIAM J. Sci. Comput., 23(2):517–541, 2001.
URL: http://link.aip.org/link/?SCE/23/517/1, doi:10.1137/S1064827500366124.
[10] Andrew V. Knyazev. Modern preconditioned eigensolvers for spectral image segmentation
and graph bisection. In Workshop on Clustering Large Data Sets. Third IEEE International Conference on Data Mining (ICDM 2003), 2003. URL: http://math.ucdenver.
edu/~aknyazev/research/conf/ICDM03.pdf.
[11] Andrew V. Knyazev and Ilya Lashuk. Steepest descent and conjugate gradient methods
with variable preconditioning. SIAM J. Matrix Anal. Appl., 29(4):1267–1280, 2007. URL:
http://link.aip.org/link/?SML/29/1267/1, doi:10.1137/060675290.
[12] Andrew V. Knyazev and Klaus Neymeyr. A geometric theory for preconditioned inverse
iteration. III. A short and sharp convergence estimate for generalized eigenvalue problems.
Linear Algebra Appl., 358:95–114, 2003. doi:10.1016/S0024-3795(01)00461-X.
[13] Andrew V. Knyazev and Klaus Neymeyr. Gradient ﬂow approach to geometric convergence
analysis of preconditioned eigensolvers. SIAM J. Matrix Anal. Appl., 31(2):621–628, 2009.
doi:10.1137/080727567.
284

Nonsymmetric preconditioning for CG and SD

Bouwmeester, Dougherty, and Knyazev

[14] Dilip Krishnan, Raanan Fattal, and Richard Szeliski. Eﬃcient Preconditioning of Laplacian
Matrices for Computer Graphics. In ACM Transactions on Graphics (Proc. SIGGRAPH
2013), volume 32(4). ACM SIGRAPH, July 2013. doi:10.1145/2461912.2461992.
[15] Yvan Notay. Flexible conjugate gradients. SIAM J. Sci. Comput., 22(4):1444–1460, 2000.
doi:10.1137/S1064827599362314.
[16] Steve Schaﬀer. A semicoarsening multigrid method for elliptic partial diﬀerential equations
with highly discontinuous and anisotropic coeﬃcients. SIAM J. Sci. Comput., 20(1):228–
242, 1998. doi:10.1137/S1064827595281587.
[17] Panayot S. Vassilevski. Multilevel block factorization preconditioners. Springer, New York,
2008. doi:10.1007/978-0-387-71564-3_10.

A

Hypre drivers used for testing

We use the code called struct with the -solver 10 option provided in hypre [1] to test SMG,
with diﬀerent command-line options, executing the following command,
mpiexec −np 16 . / s t r u c t −n $n $n $n −s o l v e r 10
for the shared memory experiments and
mpiexec −np 384 . / s t r u c t −n $n $n $n −s o l v e r 10
for the distributed memory experiments, where $n runs from 10 to 180, and represents the
number of grid points in each of the three directions per processor. The size of the brick here
is $np-times-$n-by-$n-by-$n, i.e. the brick gets longer in the ﬁrst direction with the increase in
the number of cores. For example, using the largest value $n=180, the maximum problem size
we solve for $np=16 is 16x180-by-180-by-180=93,312,000 unknowns and for $np=384 we get
384x180-by-180-by-180=2,239,488,000 unknowns. The option -solver 10 tells the driver struct
to use the SMG preconditioning. The MPI option -np 16 means that we run on 16 cores and
we restrict to using only one node for the shared memory whereas for distributed memory we
use 16 cores on 24 nodes with the MPI option -np 384. In fact, all our tests in this paper are
performed on either 16 cores on one node or 16 cores on each of the 24 nodes, so in the rest of
the appendix we omit the “mpiexec -np 16(384)” part of the execution command for brevity.
The standard PCG and LOBPCG methods are already coded in hypre. We have written the
codes of ﬂexible PCG and PSD by modifying the hypre standard PCG function.
The number of pre- and post-relaxation smoothing steps in SMG is controlled by a command
line parameter speciﬁed by the -v ﬂag in the struct test driver, e.g., one step each of pre- and
post-relaxation, and one step each of pre-relaxation and no post-relaxation, correspondingly,
. / s t r u c t −n $n $n $n −s o l v e r 10 −v 1 1
. / s t r u c t −n $n $n $n −s o l v e r 10 −v 1 0
We generate the data for Figure 3 on eigenvalue problems with the following calls,
. / s t r u c t −n $n $n $n −s o l v e r 10 −l o b p c g −v 1 0
. / s t r u c t −n $n $n $n −s o l v e r 10 −l o b p c g −v 1 1
where $n runs from 10 to 120. We also use the -P option in struct, not shown above, to make
the brick more even-sided for the eigenvalue problems as np varies. In the shared memory
experiments we use -P 4 2 2, which creates a 4n-by-2n-by-2n brick, and in the distrubuted case
we take -P 8 8 6, which creates an 8n-by-8n-by-6n brick.
285

