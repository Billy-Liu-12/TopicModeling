Procedia Computer Science
Volume 51, 2015, Pages 483–492
ICCS 2015 International Conference On Computational Science

Strip Partitioning for Ant Colony Parallel and Distributed
Discrete-Event Simulation
Francisco Borges1 , Albert Gutierrez-Milla1 , Remo Suppi1 , and Emilio Luque1
Department of Computer Architecture & Operating Systems
Universitat Autonoma de Barcelona, Bellaterra,08193, Barcelona, Spain
francisco.borges@caos.uab.cat, albert.gutierrez@caos.uab.cat, remo.suppi@uab.cat,
emilio.luque@uab.cat

Abstract
Data partitioning is one of the main problems in parallel and distributed simulation. Distribution of data over the architecture directly inﬂuences the eﬃciency of the simulation. The
partitioning strategy becomes a complex problem because it depends on several factors. In an
Individual-oriented Model, for example, the partitioning is related to interactions between the
individual and the environment. Therefore, parallel and distributed simulation should dynamically enable the interchange of the partitioning strategy in order to choose the most appropriate
partitioning strategy for a speciﬁc context. In this paper, we propose a strip partitioning strategy to a spatially dependent problem in Individual-oriented Model applications. This strategy
avoids sharing resources, and, as a result, it decreases communication volume among the processes. In addition, we develop an objective function that calculates the best partitioning for a
speciﬁc conﬁguration and gives the computing cost of each partition, allowing for a computing
balance through a mapping policy. The results obtained are supported by statistical analysis
and experimentation with an Ant Colony application. As a main contribution, we developed
a solution where the partitioning strategy can be chosen dynamically and always returns the
lowest total execution time.
Keywords: Parallel and distributed simulation, Parallel discrete-event simulation, High performance
distributed simulation, Strip Partitioning, Individual-oriented Model

1

Introduction

The partitioning of the data in parallel and distributed simulation is a problem that aﬀects
the eﬃciency of these applications. This occurs because the chosen strategy might require an
increase in the barriers and communication volume among the processes in order to maintain
the coherence of the simulation. The partitioning techniques try to distribute the workload in a
homogeneous way among the distributed processes in order to ﬁnalize all of the tasks together
and decrease the total execution time. However, this is a hard task, because the communication
Selection and peer-review under responsibility of the Scientiﬁc Programme Committee of ICCS 2015
c The Authors. Published by Elsevier B.V.

doi:10.1016/j.procs.2015.05.272

483

Strip Partitioning for Ant Colony Parallel and Distributed Discrete-Event Simulation

Borges et al.

volume, workload, shared resources, synchronisation and other aspects should be considered in
the partitioning strategies. In general, the solutions stabilised a trade-oﬀ of these factors.
In this direction, we have been researching partitioning strategies with the Individualoriented Model (IoM) by using parallel and distributed simulation. IoM considers that all
individuals have simple roles, and a collective behaviour emerges of individual interactions.
In addition, the individuals interact with the environment. Therefore, these two important
issues (individuals and environment) must be considered in order to partition IoM parallel
and distributed simulations, because the partitioning strategy in IoM depends on how the iteration between the individuals and the environment is modelled. As a result, this iteration
can represent an exchange of information among the processes and, consequently, increase the
communication volume. Therefore, the relationship between the individuals, the relationship
between individuals and the environment, and how to distribute the environment in the architecture [13] are all issues that inﬂuence the partitioning strategy. As an example, we can
reference two diﬀerent IoM scenarios where diﬀerent partitioning approaches can be applied:
ﬁsh schooling [12] and ant colonies[14]. In Solar [12], the ﬁsh simply avoids a collision with
the environment. Changes do not occur due to interaction with the individual; therefore, the
environment could be shared among the processes without having an impact on the communication volume. The Replicate Environment approach, deﬁned by [13], is very well applied
in this context, because the environment is static. Therefore, the environment is replicated
in all Logical Processes (LP). However, the environment in the ant colony is very dynamic.
The quantity of food in the environment decreases when the ants ﬁnd food. In this situation,
sharing the food of the environment with all of the processes would be a bad strategy, because
all of the processes would have to be notiﬁed by broadcast whenever that occurred. A possible
partitioning approach for this case is to divide the environment among the processes. Each
process must have a consistent view of the objects and individuals that reside and interact in
it[13, 6]. One interesting way of dividing the environment is to create strip partitions. According to Saule [10], strip shapes implicitly minimize communications. The communication in this
type of partitioning is restricted to neighboring partitions.
Much research on rectilinear and strip partitioning has been done. In summary, these works
try to partition workloads on processors, minimizing execution time. Nicol[7] and Saule [10]
represent the workload as a matrix that is divided into rectangular ﬁxed-size regions. These
techniques are applied to solve numerical problems; however, many concepts and ideas can
be applied and can be useful in an IoM context. Nicol [7] studies three diﬀerent partitioning
methods: iterative reﬁnement, binary dissection and jagged rectilinear. Jagged rectilinear partitioning divides the domain into N strips, and each strip is divided into M rectangles. The strips
and rectangles have approximately the same load. This allows for the calculation of the overall
cost of a partition to a processor. Rao [9] reports degradation of performance when individuals span two or more partitions. He presents an agent proxy approach in order to handle the
spatially-explicit problem. Saule [10] agrees with Rao and mentions that spatially partitioning
localized computation is the key to obtaining a low communication volume in the distributed
processes. The author also suggests that each processor should be assigned a connected and
compact computational work space. This work space is divided into rectangular regions. Thus,
each process communicates just with its neighboring processes. Peschlow et al. [8] present
a dynamic partitioning of LP for optimistic distributed simulation. The authors identify the
unbalance and move the LP to another host. Bahulkar et al. [1] propose a partitioning solution
taking dynamic model behavior into account. Deelman [4] execute a strip partitioning where
strips migrate to other LP and the strips are resized during the load balance.
Many application areas, such as biology and physics, have spatially dependent problems
484

Strip Partitioning for Ant Colony Parallel and Distributed Discrete-Event Simulation

Borges et al.

modelled in the Euclidean Space (ES) that require IoM parallel and distributed solutions.
This paper focuses on the distribution of the environment represented by ES. We take into
account that the environment has resources (objects) that are consumed and used by all of the
individuals. In order to avoid communication among the processors, we will use the approach
where the environment is divided among the processes and just the local environment is kept
updated. This strategy avoids excessive communication among the processes, and the individual
iteration occurs only with objects of its partition space. In addition, we propose an objective
function which can be used dynamically in order to choose the best solution for a speciﬁc
environment conﬁguration. In order to develop this study, we have adapted the ant colony
model described in [14] for a parallel and distributed approach. We present a short description
of this model in Section 2. In our approach, the environment is distributed by architecture,
but the resources consumed by the ants are not shared among the processors. This avoids an
increase in communication and barriers. In addition, we propose some heuristics and strategies
for mapping for this type of problem. More details about our partitioning algorithm are shown
in Section 3. We perform a statistical analysis of the partitioning with the view of supporting
our experiments (Section 4) and conclusions (Section 5).

2

Ant Colony Model

As a case study, we are using the ant colony model described by [14]. In this model, the ants’
main rules are: ﬁnding food and bringing the food to the ant nest. In the search for food,
the ant drops a chemical when it ﬁnds some and moves back to the nest carrying the food.
When other ants sense the chemical, they follow it to the food. The chemical trail intensiﬁes
as more ants follow this trail, allowing other ants to ﬁnd the piece of food. We veriﬁed that
our parallel computational model follows the functional features described by [14]: it explores
the food source in order, beginning with the amount of food closest to the nest and ending
with the amount of food farthest away from the nest. This behaviour emerges in ant colonies
because it is easier to create a stable trail to the nearest food than to food that is farther
away. The chemical only lasts a short time before it evaporates; consequently, the trail must
be strengthened by other ants. Therefore, more distant food requires a larger number of ants
to form a stable pheromone trail.

3
3.1

Strip Partitioning Algorithm
General Deﬁnitions

Our partitioning algorithm aims to decrease the communication volume that divides the environment among the logical processes, avoiding shared objects in diﬀerent processes. Shared
objects mean that messages must be sent and received by processes in order to keep the objects’
state updated. This problem grows when the environment is dynamic, and communication volume increases as a consequence. Therefore, we have developed a strip partitioning algorithm
where each object of the environment is restrained in just one partition (Fig.1(c)). The algorithm prevents objects from being located in two partitions, as exempliﬁed in Figure 1(b). The
Euclidean Space is ﬁrst divided into N strips of approximately the same size, where N is the
number of cores. Following this, each strip is divided again until the partition limit restriction
heuristic is reached. We deﬁne this heuristic as the size that the partitions can take on. It has
three proposals: a) to avoid shared objects between partitions. An object cannot be shared
between strips. This restriction avoids an increase in communication between strips. These
485

Strip Partitioning for Ant Colony Parallel and Distributed Discrete-Event Simulation

Borges et al.

communications would be necessary in order to keep the object state updated; b) to avoid
excessive communication between neighboring strips. If the strip is too small, then the individuals will exceed the limits of the strip with greater constancy; and c) to avoid a migration
process to non-consecutive partitions when the partition are too small. The migration process
is the movement of individuals of one partition to another. In this way, the communication
only occurs between neighboring strips.

(a) Ant colony environment (b) Environment partitioned(c) Environment partitioned
with sharing objects
with no sharing objects

Figure 1: Object distributions in the environment.

Fig.2 represents the partitioning scheme proposed in this work. The strips may have two
directions: horizontal (H) or vertical (V). Each partition can be created in a ﬁxed (FI) or a
variable (VA) way. FI partitioning creates N partitions where N is equal to the number of
cores. For the ﬁxed partitioning, just one mapping approach can be applied. One partition is
allocated to one core (M0). We can apply two heuristics for a variable partitioning creation,
because the number of partitions is higher than the number of cores: in the ﬁrst one, the
partitions are created until the creation of other partitions is no longer possible. We call that
heuristic partition last restrictions (PLR). These restrictions are mentioned above. In the second
heuristic, the partitioning algorithm takes into account the cost of computing (PMC). Firstly,
this heuristic executes the PLR heuristic, then it calculates the computing cost for each strip.
The partitioning algorithm ﬁnishes when it ﬁnds the smallest standard deviation between the
strips. The heuristics PLR and PMC might be used with mapping M1 and M2. M1 mapping
divides the total number of strips among the cores. M2 considers the cost of each strip.
	

















Figure 2: Partitioning scheme

486

Strip Partitioning for Ant Colony Parallel and Distributed Discrete-Event Simulation

3.2

Borges et al.

Objective Function

We deﬁne the partitioning problem as follows. Our problem is partitioning a Euclidean Space
(ES) in a set of cores. ES has a ﬁxed size limit [x, −x] and [y, −y]. An ES may be divided into
many partitions. A partition is deﬁned as a set of strips: s = (x1 , x2 , y1 , y2 ) where (x1 , x2 ) ⊂
[x, −x] and (y1 , y2 ) ⊂ [y, −y]. The ES is composed of objects which cannot be shared between
strips. Each object o has a position (x, y) and size, and ∀o ∈ sx , if ox ∈ sx then ox (S − sx ),
where S is the total number of strips.
The execution load on a core (c) is the sum of all partitions allocated in this core. The total
computing cost of the partition strategy is the sum of the computing cost of each of the strips.
We consider that, if the area of strip s1 is bigger than the area of strip s2 , then the migration
cost of strip s1 is less than strip s2 . All objects have a computing cost. The quantity of objects
in the strip has an impact on computing and communication between strips. Therefore, our
objective function tries to ﬁnd the computing cost diﬀerence between the strips. This function
uses as terms: A) total area of the strip; B) computing cost of the strip objects; C) computing
cost objects at neighboring strips; and D) quantity of neighboring strips.
F (S) = σ(cost stripSs=1 (As + Bs + Cs + Ds ))

(1)

This function (1) calculates the cost of each strip (s), then returns the standard deviation(σ),
considering all strips (S). The lower the σ, the better the result, because it means that the
diﬀerence in the cost of computing in the strips is lower. Term A speciﬁes the size cost of each
strip. It is deﬁned by Equation 2.
strip areas
(2)
total area
The quantity of individuals is proportional to the strip area; therefore, the computing cost
of the individuals increases as the size of the area also increases, because the chances of the
strip receiving more individuals are higher.
As = strip size costs =

n

Bs =

n

comp cost objo /

comp cost objo

(3)

o=1

j:oj ∈si

In the ant algorithm, the individuals try to ﬁnd the objects; therefore, the strips which have
more objects tend to receive more individuals and have more migratory processes. A strip can
have zero or many objects, and each object has a computing cost. We calculate the proportional
cost of objects for each strip with Equation 3, where o is an object and n is the total number
of objects in the environment.

Cs =

n
j:oj ∈(si−1 ,si+1 )

comp cost objo

total comp cost obj

∗ strip size costi ∗ number obj(Si − 1, Si + 1)

(4)

The strip is subject to more migratory processes if its neighbors contain objects, because the algorithm converges in this direction. Therefore, we consider the size of the strip
(strip size costi ) and the number of objects in the neighboring strips (number obj(Si − 1,
Si + 1)). This implicates an individual’s migration to another strip. The term deﬁned by 6
represents the inﬂuence of the strip on the total volume of migration.
number neighbors strip =

1
2

if (si = 0 or si = S)
if (si <> 0 or si <> S)

(5)
487

Strip Partitioning for Ant Colony Parallel and Distributed Discrete-Event Simulation

Ds = number neighboring strips/(S − 2) ∗ 2 + 2

3.3

Borges et al.

(6)

Mapping

Strip partitioning returns a naive partitioning, where the load balancing focuses on area and
not on the load[10]. Therefore, the load balancing must be covered in the mapping process. We
use three mapping approaches: a) one strip per core; b) many strips per core with distribution
by division; and c) many strips per core with distribution by computing evaluation. In the
ﬁrst approach, the number of strips is the same as the number of cores. Therefore, each strip
is allocated to one core. When the partitioning algorithm generates many partitions, we can
distribute the strips by doing a simple division: the number of strips by the number of cores.
Neither of these mapping approaches (a or b) are eﬀective, because they do not take into
account the computing requirement of the strips. Therefore, some type of load imbalance can
be generated. However, the last approach (c) distributes the strips considering the workload
of each core. The partitioning algorithm saves the load computing of each partition when the
objective function is calculated. Then, we can use the computing load of partitions in the
mapping process. This approach tries to set the equivalent computing load for each core.

4

Experimental Results

A statistical analysis of the partitioning is required, with the aim of analysing which strategies
have statistically diﬀerent results. This enables us to eﬀectively identify which strategies have
diﬀerent behaviours. We follow the output analysis deﬁned by Chung[3] for simulations of
type terminating. A brief summary of the statistical steps followed: a) calculate the mean and
standard deviation of the ten replication means; b) calculate the standard error; c) calculate the
relative precision; d) apply ANOVA in order to ﬁnd out if the solutions are statistically diﬀerent;
and e) apply the Duncan test in order to ﬁnd out which solutions are statistically diﬀerent. This
statistical study was executed at 64 cores using the same environment conﬁguration in all tests.
Fig 3 presents the average of the total execution time obtained from the ten replications of
the statistical analysis. The histogram shows the horizontal and vertical approaches and the
results of all of the strategies combined with the mappings. In addition, the graph has the
standard deviation returned by the objective function of each partitioning strategy. We can
see that all the results of the vertical approach are better than the horizontal approach; and
we observe that, for the same strategy and mapping, lower standard deviation results in better
total execution time. In addition, as shown in the graph, M2 mapping presents much better
results than M1, mainly when the partition is in a vertical direction.
The statistical analysis says that the strategies are statistically diﬀerent. This indicates
that the partitioning solutions have diﬀerent eﬀects. However, we need to know which ones are
diﬀerent. Therefore, we have to perform the Duncan test [3]. This test shows that the pairs
of strategies: (H-PRL-M1, H-PMC-M1), (V-PRL-M1, V-PMC-M1) and (V-PRL-M2, V-PMCM2) are not statistically diﬀerent. In other words, these pair strategies have no signiﬁcant
result diﬀerences. This test also shows, that there are two outside values of H-PRL-M2 that
make this strategy statistically diﬀerent from H-PMC-M2.
We are interested in analysing the behaviour of our proposal, so we will use the TAU tool[11]
in order to analyse the overall communication volume and its pattern in the partitioning algorithm for the worst (H-FI-M0) and best (V-PMC-M2) total execution times that we obtained
in the previous experiment. The TAU provides users with the Communication Matrix graph
488

Strip Partitioning for Ant Colony Parallel and Distributed Discrete-Event Simulation

Borges et al.

, !"#$#%$-)*!#"$!#$



	





.#/$
0#!
)*!#"$!#$


0.0

664

	
	

27
0.0

636



09








0.0



490



43
0.0

0.0
463








31

490

43
0.0

0.0
463



490

31

40
0.0

0.0
463



27

490

40
0.0


463

27



)*!#"$!#$

 !"#$#%&'(






##$#$'!+%

Figure 3: Total execution time average and objective function of the partitioning strategies
(Fig.4), which gives information about the communication volume, such as Total Volume Bytes.
The information is shown through heat maps. In Fig.4 (a1) and (b1), the y and x axes depict
the sender and receiver processes, respectively. Fig.4 (a1) and (a2) represents the same information. However, (a2) shows the data in 3D in order to make the visualization of the results
easier. The same process is applied for (b1) and (b2). The main diagonal and all black points
have no volume byte information, because there is no process communication in these cores.

	






	






Figure 4: Comparison of total volume bytes with heat maps
Fig.4 presents the exchange of total volume bytes among the processes by using the worst
(a1 and a2) and best (b1 and b2) strategies. The two colorful diagonals (a1 and b1) represent
the data sent by a speciﬁc core to its neighbors. The top blue line represents the initial data
sent by rank zero for all of the cores at the start of the simulation. The V-PMC-M2 strategy
has a better partitioning of data in the cores. The heat maps show that more cores receive
more data in the V-PMC-M2 strategy than in the H-FI-M0 strategy. In addition, the H-FI-M0
strategy has more total volume data(4.11e6); therefore, the migration processes of individuals
from one partition to another are higher. The volume of data of the H-FI-M0 strategy is
concentrated in a few cores. As our simulation is of a conservative synchronisation type [5],
489

Strip Partitioning for Ant Colony Parallel and Distributed Discrete-Event Simulation

Borges et al.

this results in a synchronization point among the processes, and a bottleneck is created when a
few cores receive a much higher load than the other cores because these cores will take longer
to ﬁnish their work [2]. In both strategies, we can observe that the volume of communication is
concentrated in the central cores. The volume of communication is too low (6.47e4 of H-FI-M0
and 4.27e4 of V-PMC-M2) in the processes that have an MPI rank at the start or end. This
pattern occurs as a consequence of the observed environment layout (Fig 1(a)): the strips with
more objects tend to have more individuals. As the mapping starts to allocate the strips from
left to right (vertical approach) and from bottom to top( horizontal approach), the objects tend
to get allocated to the processes in the middle of the range. This information reveals that an
additional load balance strategy might be applied in order to prevent this behaviour.
Communication and computing time, presented in Fig 5, also were obtained from the TAU.
In spite of the overhead created by the dynamic instrumentation, we can use this data communication and computing to compare the strategies, since the overhead for both measures
is constant. We consider this communication and computation with the TAU measures to be
more reliable. The communication in the H-FI-M0 strategy is higher than the V-PMC-M2
strategy communication. We observe high communication volume with relation to computing,
even in the best approach. We checked that the MPI reduce messages were the cause of this
communication overhead after analysing the TAU proﬁle. This occurs as a consequence of the
conservative protocol used in our discrete event simulation.
 !"#$
















	








Figure 5: Communication and computing time of the worst and best strategies
The scalability study of the best strategy (V-PMC-M2) was the next experiment conducted.
In this experiment, shown in Fig.6, the simulation was executed 1000 steps, diﬀerent from the
previous experiments where we executed the simulation until the individuals consumed more
than 90% of the food at the colony. Now we are interested in checking the scalability of the
partitioning algorithm. Therefore, we used a diﬀerent and bigger environment conﬁguration in
order to create enough workload. In addition, we distributed the individuals (25000, 37500,
50000 and 100000) uniformly over the colony. Three characteristics of the ant colony algorithm
are important to remember in order to understand the scalability results: the ﬁrst is that the
ant colony is a spatially and locally dependent problem, meaning that the ant must know its
position in the area; the ant must always go back to the nest when it ﬁnds food; and the nest
of the colony is located at the center of the environment. The creation of a pheromone is less
for a smaller number of individuals. Therefore, the cores execute the rule: ﬁnding food by
more time. This rule is cheaper than creating and following the pheromone. That way, we
can see a gain in performance when we increase the number of cores to 25000. For 37500 and
50000 ants, there is no signiﬁcant diﬀerence between 128 and 256 cores. This occurs because
490

Strip Partitioning for Ant Colony Parallel and Distributed Discrete-Event Simulation

Borges et al.

of the third characteristic mentioned previously. Therefore, the communication and computing
of cores tend to be higher in the middle cores than at the cores located in the extremities. So,
comparing 128 to 256 cores, the extra cores have no inﬂuence over the total execution, because
their strips have less migration processes and trail creations. However, this changes when we
use a more ants (100000). In this case, all the strips are full of individuals; therefore, their
computing increases and these cores have a greater inﬂuence on the total execution time.


 







93%



54.



91%



!

49.














	

06%

	

08%



23%



67%



42.

04%

	



41.

3%



25.

8.9



41.

40.







	





Figure 6: Partitioning algorithm scalability. The percentage value represents the gain in eﬃciency of 128 and 256 cores in relation to 64.

5

Conclusions

The partitioning of data is the key to improving the total execution time for parallel and
distributed simulations. In this paper, we have proposed a strip partitioning strategy to a
spatially dependent problem in Individual-oriented Model applications. The partitioning algorithm avoids sharing objects among the strips. As a result, we have observed a decrease in the
communication volume and synchronization among the processes. Another important feature
of this strategy is that we can ﬁnd the best partitioning for a speciﬁc conﬁguration by using
the objective function proposed. In addition, this function returns the computing cost of each
partition and allows for a computing balance through a mapping policy. These results were
supported by statistical analysis and experimentation with an Ant Colony IoM application. As
our main contribution, we developed a solution where the partitioning strategy can be chosen
dynamically, always returning the lowest total execution time in multi-core architecture. Some
technical aspects must be considered for future work: a) we have to develop strategies to decrease the synchronization time. The ant colony must know the total volume of food. This kind
of operation consumes a lot of communication time and becomes a bottleneck in discrete event
simulation; b) the mapping process must consider inter-communication processes at the moment
of strip allocation. Strips with many objects and their neighboring strips must be allocated in
the same core. Since the partitioning algorithm converges in the direction of the strips which
have the most objects, the probability of an increase in the volume of communication at the
migration process in these situations is too high; c) the individuals have a dynamic and random
movement; therefore, adjusting the size of the strip [4] would be an interesting approach, as
this would avoid strips with overload and, consequently, would decrease the synchronization
491

Strip Partitioning for Ant Colony Parallel and Distributed Discrete-Event Simulation

Borges et al.

time; and ﬁnally d) analysing this partition algorithm with other IoM applications which are
not spatially dependent.

6

Acknowledgments

This research has been supported by the MINECO (MICINN) Spain under contract TIN201124384.

References
[1] Ketan Bahulkar, Jingjing Wang, Nael Abu-Ghazaleh, and Dmitry Ponomarev. Partitioning on dynamic behavior for parallel discrete event simulation. In Proceedings of the 2012 ACM/IEEE/SCS
26th Workshop on Principles of Advanced and Distributed Simulation, PADS ’12, pages 221–230,
Washington, DC, USA, 2012. IEEE Computer Society.
[2] Quentin Bragard, Anthony Ventresque, and Liam Murphy. Synchronisation for dynamic load
balancing of decentralised conservative distributed simulation. In Proceedings of the 2Nd ACM
SIGSIM/PADS Conference on Principles of Advanced Discrete Simulation, SIGSIM-PADS ’14,
pages 117–126, New York, NY, USA, 2014. ACM.
[3] Christopher A Chung. Simulation modeling handbook: a practical approach. CRC press, 2003.
[4] Ewa Deelman and Boleslaw K. Szymanski. Dynamic load balancing in parallel discrete event
simulation for spatially explicit problems. SIGSIM Simul. Dig., 28:46–53, July 1998.
[5] Richard M. Fujimoto. Parallel and Distribution Simulation Systems. John Wiley & Sons, Inc.,
New York, NY, USA, 1st edition, 1999.
[6] Fehmina Merchant, Lubomir F. Bic, and Michael B. Dillencourt. Load balancing in individualbased spatial applications. In Proceedings of the International Conference on Parallel Architectures
and Compilation Techniques (PACT’98), pages 350–357, 1998.
[7] D.M. Nicol. Rectilinear partitioning of irregular data parallel computations. Journal of Parallel
and Distributed Computing, 23(2):119 – 134, 1994.
[8] Patrick Peschlow, Tobias Honecker, and Peter Martini. A ﬂexible dynamic partitioning algorithm
for optimistic distributed simulation. In Proceedings of the 21st International Workshop on Principles of Advanced and Distributed Simulation, PADS ’07, pages 219–228, Washington, DC, USA,
2007. IEEE Computer Society.
[9] Dhananjai M. Rao. Accelerating parallel agent-based epidemiological simulations. In Proceedings of
the 2Nd ACM SIGSIM/PADS Conference on Principles of Advanced Discrete Simulation, SIGSIMPADS ’14, pages 127–138, New York, NY, USA, 2014. ACM.
[10] E. Saule, E.O. Bas, and U.V. Catalyurek. Partitioning spatially located computations using
rectangles. In Parallel Distributed Processing Symposium (IPDPS), 2011 IEEE International,
pages 709–720, May 2011.
[11] Sameer S. Shende and Allen D. Malony. The tau parallel performance system. Int. J. High
Perform. Comput. Appl., 20(2):287–311, May 2006.
[12] Roberto Solar, Remo Suppi, and Emilio Luque. High performance distributed cluster-based
individual-oriented ﬁsh school simulation. Procedia CS, 4:76–85, 2011.
[13] Yongwei Wang, M. Lees, Wentong Cai, Suiping Zhou, and M.Y.H. Low. Cluster based partitioning
for agent-based crowd simulations. In Winter Simulation Conference (WSC), Proceedings of the
2009, pages 1047 –1058, 2009.
[14] U. Wilensky. Netlogo ants model. Center for Connected Learning and Computer-Based Modeling,
Northwestern University, Evanston, IL., 1997. Visited on 30-11-2014.

492

