Procedia Computer Science
Volume 51, 2015, Pages 384–393
ICCS 2015 International Conference On Computational Science

On the Eﬀectiveness of Crowd Sourcing Avian Nesting
Video Analysis at Wildlife@Home
Travis Desell1 , Kyle Goehner1 , Alicia Andes2 , Rebecca Eckroad2 , and Susan
Ellis-Felege2
1

Department of Computer Science, University of North Dakota
Grand Forks, North Dakota, U.S.A.
tdesell@cs.und.edu,kyle.goehner.2@my.und.edu
2
Department of Biology, University of North Dakota
Grand Forks, North Dakota, U.S.A.
alicia.andes@my.und.edu,rebecca.eckroad@my.und.edu,susan.felege@email.und.edu

Abstract
Wildlife@Home is citizen science project developed to provide wildlife biologists a way to swiftly
analyze the massive quantities of data that they can amass during video surveillance studies.
The project has been active for two years, with over 200 volunteers who have participated in
providing observations through a web interface where they can stream video and report the
occurrences of various events within that video. Wildlife@Home is currently analyzing avian
nesting video from three species: Sharptailed-Grouse (Tympanuchus phasianellus) an indicator
species which plays a role in determining the eﬀect of North Dakota’s oil development on the
local wildlife, Interior Least Tern (Sternula antillarum) a federally listed endangered species,
and Piping Plover (Charadrius Melodus) a federally listed threatened species. Video comes
from 105 grouse, 61 plover and 37 tern nests from multiple nesting seasons, and consists of over
85,000 hours (13 terabytes) of 24/7 uncontrolled outdoor surveillance video. This work describes
the infrastructure supporting this citizen science project, and examines the eﬀectiveness of two
diﬀerent interfaces for crowd sourcing: a simpler interface where users watch short clips of
video and report if an event occurred within that video, and a more involved interface where
volunteers can watch entire videos and provide detailed event information including beginning
and ending times for events. User observations are compared against expert observations made
by wildlife biology research assistants, and are shown to be quite eﬀective given strategies used
in the project to promote accuracy and correctness.
Keywords: Citizen Science, Crowd Sourcing, Video Analysis, Wildlife Ecology, Big Data

1

Introduction

Cameras have become popular tools in the ﬁeld of avian ecology as they can dramatically reduce
researcher impacts on behavior and monitor animals in remote locations [3, 5]. However, many
384

Selection and peer-review under responsibility of the Scientiﬁc Programme Committee of ICCS 2015
c The Authors. Published by Elsevier B.V.

doi:10.1016/j.procs.2015.05.258

Crowd Sourcing Avian Nesting Video

Desell, Goehner, Andes, Eckroad and Ellis-Felege

Figure 1: A sharp-tailed grouse in day, dusk and night conditions (top), and a piping plover in
varying light conditions (bottom). Birds are circled in red. Given the cryptic coloration of the
bird and lighting conditions, it can be very diﬃcult to distinguish the bird from a rock, grass
or other objects.
of these studies have been hampered by small sample sizes, where few have studied more than
100 nests [5], limiting the biological inferences that could be made due to these limited sample
sizes. Part of this limitation is due to the lack of tools to swiftly analyze large amounts of video
footage. In the summers of 2012 and 2013, numerous cameras were set up across western North
Dakota, gathering over 35,000 hours of video footage of sharp-tailed grouse (Tympanuchus
phasianellus), approximately 6 terabytes of data. In addition, 20,000 hours of video has been
gathered for interior least tern (Sternula antillarum), a federally endangered species, and 30,000
hours for piping plover (Charadrius melodus), a federally threatened species. There are plans
to monitor these birds in future nesting seasons, which should result in another 30,000 hours of
video. The sharp-tailed grouse is considered an indicator species, meaning that the success of
the species is closely tied to the health of the wildlife in the area. An analysis of this video will
not only result in a wealth of biological knowledge about these species, but can also be used to
examine the impacts of oil development in western North Dakota.
There are signiﬁcant challenges detecting the presence of birds and events of interest within
this wildlife video (see Figure 1). The species being studied, along with many of their predators,
have evolved with cryptic coloration, or camouﬂage, which makes it diﬃcult to distinguish
them from their surroundings. Further, the video is taken from uncontrolled outdoor settings,
with vegetation moving in the wind and changing weather conditions. Footage is recorded
continuously with daytime video captured in color. Infrared light emitting diodes (LEDs) are
used in low light and night conditions and recordings during this time are in black and white.
This results in a wide variety of video quality and color.
Wildlife@Home has been developed as a citizen science project to recruit the help of public
volunteers in this analysis, to detect events of biological interest within the video. The end
goal is not to simply have the public aid in watching this video, but rather to take these
observations and create a robust human-annotated data set which can be used to develop and
evaluate computer vision methods for automated analysis of this video – which stands to greatly
beneﬁt the biological community. Wildlife@Home has been active for two years, with over 200
385

Crowd Sourcing Avian Nesting Video

Desell, Goehner, Andes, Eckroad and Ellis-Felege

volunteers having participated in watching avian nesting video. This work presents recent work
updating the crowd sourcing interface which has resulted in signiﬁcant improvements in the
accuracy of user observations, while at the same time dramatically reducing storage needs.

2

Related Work

Crowd sourcing has been successfully used by citizen science projects to tackle problems requiring human feedback. GalaxyZoo [8, 7] has had great success in using volunteers to classify
galaxies in images from the Sloan Digital Sky Survey [1]; and PlanetHunters [6] has been used
to identify planet candidates in the NASA Kepler public release data. More recently, Snapshot
Serengeti [9] has been created to classify images from camera traps in the Serengeti National
Park. However, these projects focus on volunteers doing identiﬁcation and classiﬁcation of
images, not video.
In avian ecology, Cornell’s NestCams project [2] has provided an outstanding resource for
environmental education and gained popularity through the use of nest cameras to attract
the public’s interest in environmental science. NestCams primarily focuses on public outreach
where video is collected opportunistically from cameras installed in bird houses, capturing a
variety of cavity-nesting species. The CamClickr project has sparked applications of nest video
archives for education in collegiate-level animal behavior courses [10]. More recently, eBird [11]
is a citizen science project which allows users to upload observations of birds through handheld
devices, providing spatio-temporal information about the bird distribution and abundance.
To our knowledge, Wildlife@Home is the only citizen science project which deals with classifying large volumes of video data, and is unique in that the volunteers are actively involved
in providing scientiﬁc results from their observations.

3
3.1

Wildlife@Home Infrastructure and Interfaces
Data Collection

The nest cameras used to gather this avian nesting video store video in 300 MB AVI ﬁles, which
are encoded with the H264 codec. This means while the ﬁles are of ﬁxed sizes (apart from the
last taken from a camera when videos are gathered), they are of varying times depending on
compressed the ﬁles were. On average videos are around 53 minutes long, but can reach times of
up to 11 hours when there is little to no movement or the video is very dark at night. Every 4-5
days per nest, the SIM cards are swapped out (as the camera sites are too remote for wireless
transmission, and wireless transmission also requires too much power) and then this video is
uploaded to the Wildlife@Home video server.

3.2

Hardware and Software Infrastructure

The video data for Wildlife@Home is stored on a shared Dell NSS-HA storage appliance with
144 terabytes (TB) in RAID 6 conﬁguration for 110TB usable storage utilizing the Red Hat
Enterprise Linux XFS ﬁle system. This storage system is mounted on the Wildlife@Home
video server which handles the conversion of the archival video to formats required for HTML5
streaming (OGV and MP4), as well as serving the video to crowd sourcing webpages and clients
for volunteer computing. The video server also hosts the database which keeps track of video
information and user observations. A third server handles all webpages for the Wildlife@Home
project and the database user information and volunteer computing. This infrasructure is
presented in detail in Desell et al. [4].
386

Crowd Sourcing Avian Nesting Video

Desell, Goehner, Andes, Eckroad and Ellis-Felege

Figure 2: An example of the original interface used for crowd sourcing. The page has been
developed using HTML5, jQuery and Bootstrap, allowing for easy use across diﬀerent devices
and web browsers. Users are shown 3, 5, 10 or 20 minute clips, and can select yes, no, or unsure
for if any of a set of events occur within the video.
Wildlife@Home has gone through two iterations of crowd sourcing interfaces, which are
described in Sections 3.3 and 3.4. The change to the new interface was motivated by a number of reasons, including user feed back, quality of information being generated (examined in
Section 4), and simpler infrastructure requirements.

3.3

Original Interﬁace

Figure 2 shows the original interface, where users were asked to mark yes, no or unsure to eight
diﬀerent categories about bird behavior: 1) a bird leaves the nest during the video, 2) a bird
returns to the nest during the video, 3) a bird is present in any frame of video, 4) a bird is absent
in any frame of the video, 5) a predator is in the video, 6) a bird defends the nest in the video,
7) chicks hatch in the video, and 8) chicks are present in the video. A user could also report
if a video was corrupt, too dark, or interesting. Initially, video segments of 3 minute durations
where shown to the users, and later 5, 10 and 20 minute durations were shown in response to
users having diﬃculty determining sharptailed grouse presence in the shorter videos.

3.4

New Interface

While the original interface was relatively simple and easy to use, it added additional data
requirements. The archival AVI data was already required to be stored, along with converted
full length videos in OGV and MP4 format for streaming to the expert video observation
interface. Those full length videos then also needed to be split into segments in both OGV and
MP4 formats for viewing on this crowd sourcing interface. This meant that for each video, 4
additional copies of that video, two for full length and two for each split length duration video
segment were required. With a total of 10.7 TB of archival video already gathered, and even
387

Crowd Sourcing Avian Nesting Video

Desell, Goehner, Andes, Eckroad and Ellis-Felege

Figure 3: An example of the new interface used for crowd sourcing. Users are shown entire
videos instead of short clips, and can specify the start and end time for a large number of events,
and provide tags and comments for additional detail. Users can also specify how diﬃcult it was
to determine events for the video.
more expected from future ﬁeld seasons, the additional space requirements for the split video
segments became excessive.
When the expert interface was completed, due to the volunteer’s requests for longer videos
along with poor performance given what users could determine from the short the video segments (see Section 4.1) and space issues, the more advanced expert interface was used to replace
the original interface, shown in Figure 3. In this interface, users have a signiﬁcantly wider range
of events to classify within videos. 25 diﬀerent events could be classiﬁed by users, from the
following 7 categories:
• Parent Behavior: Not in the video, on the nest, oﬀ the nest, and for piping plover and
least terns only (given that they are bi-parental nesting birds), foraging for food, adult
feeding adult, and parent feeding chicks.
• Parent Care: Brooding chicks, removing eggshells, and for piping plover and least terns
only exchanging nesting duties.
• Chick Behavior: Eggs hatching, chicks in video, and for piping plover and least terns
only chicks foraging and chicks acting submissive.
• Territorial: Human in video, non-predator animal in video, predator in video, and nest
defense.
• Camera Interaction: Sometimes the birds inspect or interact with the nest cameras.
These can be either a non-physical observation, physical interaction, or agressive attack.
• Miscellaneous: Users have the option to enter additional unspeciﬁed events.
• Error: Given the large amount of data collected and the uncontrolled outdoor settings,
various errors can occur such as camera issues wher the camera gets knocked over, video
errors where the video ﬁle is corrupted, and the video being too dark in some night time
conditions.
388

Crowd Sourcing Avian Nesting Video

Duration (s)
< 180
181 . . . 300
301 . . . 600
601 . . . 1200
Total

Completed
89,645
8,942
6,446
3,785
108,818

Observations
220,320
18,715
14,022
8,396
261,453

Desell, Goehner, Andes, Eckroad and Ellis-Felege

Valid
206,193
17,930
12,899
7,569
244,591

Invalid
13,129
649
1,033
744
15,555

Inconvclusive
618
75
50
55
798

Valid (%)
93.58
95.80
91.99
90.15
93.55

Table 1: Performance of volunteers based on varying video durations for the original interface.
Duration ranges are in seconds.
This interface allows user to enter any number of events, specify the start and end time of
the event along with comments and tags for further detail. By clicking the discuss button to the
right of an event, a forum post will be generated for the user to allow them to discuss the section
of a video speciﬁed by that event in the Wildlife@Home forums with other users and project
experts. Users can also specify how diﬃcult it was to provide events for that video. When a
user is ﬁnished, the interface will provide options for the user to either view the next video from
that nest, or to randomly select a new video. In addition to reducing space requirements, this
new interface also makes direct comparison of volunteer results to those made by the project’s
experts.

4
4.1

Results
Original Interface

Results for the original interface were gathered over a period of 9 months, from August 2013
to April 2014. 206 users provided 261,453 observations for 108,818 video segments, meaning on
average it took approximately 2.4 views to reach a quorum for a video segment. These 261,453
observations total over 7,411.2 hours of video watched by volunteers. Of these observations,
only 798 were marked inconclusive, and 15,555 marked invalid. In the later months of the
original interface, video segments were also generated with durations greater than 3 minutes,
due to feedback from the users and an interest in seeing how well volunteers would perform on
longer video segments. Additional video segments were generated with 5, 10 and 20 minute
durations, and as the original videos did not divide evenly, some segments were of less duration.
Table 1 provides a breakdown of how many segments were watched of each duration, as well
as how many were ﬂagged as valid, invalid or inconclusive. Observations were marked valid
if they were part of the quorum of observations, i.e., if 3 users speciﬁed the bird was on the
nest, and 2 did not, the 3 on nest observations were valid and the 2 oﬀ nest observations were
invalid. In general, it seems that video segments between three and ﬁve minutes provided the
most consensus from users, and longer video segments reduced user consensus.
Of the 108,818 video segments marked by volunteers, 25,549 corresponded to videos that
were marked by the projects experts. Table 2 compare the volunteer’s results to the experts
observations, which were obtained using the new interface. True positives (TP) were when a
quorum of volunteers marked an event as occuring a video segment, and the times of the video
segment overlapped with the time of a similar expert event; false positives (FP) were when the
marked event did not overlap with the time of a similar expert event; true negatives (TN) were
when the event was not marked and an expert did not mark the event during that time; and
false negatives (FN) were when the event was not marked and an expert did mark an event
389

Crowd Sourcing Avian Nesting Video

Event Type
Bird Leave/Return
Bird Presence
Bird Absence
Predator Presence
Nest Defense
Chick Presence

Total
12501
21230
9540
414
33
708

Desell, Goehner, Andes, Eckroad and Ellis-Felege

TP
154
9407
1092
4
0
12

TN
8504
1338
4680
393
33
418

FP
287
9270
2173
11
0
252

FN
3556
1215
1595
6
0
26

Accuracy (%)
69
51
61
96
100
61

Table 2:
Volunteer event quorums compared to expert events. True positive (TP), true
+T N
) percentages are
negative (TN), false positive (FP), false negative (FN), and accuracy ( T Ptotal
given.
during that time. Bird leave and bird return events were uniﬁed, as the expert interface had a
single event for a bird being in the video but not on the nest which is what these would match
to. There were not enough nest success events to provide meaningful results.
Using this interface the volunteers provided good results for obvious events such as predator
presence and nest defense (at 96% and 100% accuracy), and decent results for birds leaving
and returning (69%), results for bird presence and absence were poor (51% and 59%), due to
the diﬃculty of determining the presence of a bird during the short video clips.

4.2

New Interface

Results for the new interface have been gathered over the subsequence period of 9 months,
from April 2014 to January 2015. 150 users provided 25,427 observations for 8,338 full length
videos, with the average video duration being 53 minutes (durations ranged from 1 second
to 11 hours). In total, this was over 49,457.5 days of video watched by volunteers. Of these
observations, 137,895 were marked valid (by being marked by a quorum of volunteers, given a
5 second buﬀer for start and end times), 15739 were marked invalid, and 132 were inconclusive
(either no quorum, or no other matching events).
Of the 8,338 full length videos observed by volunteers, 1,824 had observations from both
a volunteer and an expert. Table 3 displays how well user observations matched to expert
observations for a 5 second buﬀer, with Table 4 shows the same data for a 10 second buﬀer, for
all observations that had more than 10 volunteer entries with corresponding expert observations.
A 5 second buﬀer means that two events would match if they were of the same type and their
start and end times were within 5 seconds of each other, and so on.
The misses column shows how many observations of a particular type could not be matched
to an expert observation with similar start and end times. The type mismatch column shows
how many observations matched an expert observation with similar start and end times, but
a diﬀerent event type. The match column shows how many observations fully matched an
expert observation. The improvement in user observations is signiﬁcant. With even a 5 second
buﬀer, users correctly marking on nest and not in video increased to 85% and 74%, oﬀ nest,
which meant that the bird is in the video but not on the nest, was similar at 68%. With a 10
second buﬀer, these increase to 87%, 79% and 73%, respectively. These represent signiﬁcant
improvements from the old interface for on nest and not in video, without losing accuracy on
oﬀ nest, which would correspond to bird leave/bird return from the old interface.
Given these results, the camera interaction events are the most problematic, with many
completely mismarked, and attack and physical inspection events showing signiﬁcant type mismatches. The video error and camera issue events have high type mismatches, and these results
390

Crowd Sourcing Avian Nesting Video
Event
Parent Behavior - Not In Video
Chick Behavior - In Video
Territorial - Predator
Territorial - Non-Predator Animal
Camera Interaction - Attack
Camera Interaction - Physical Inspection
Camera Interaction - Observation
Error - Video Error
Error - Camera Issue
Parent Behavior - On Nest
Parent Behavior - Oﬀ Nest

Desell, Goehner, Andes, Eckroad and Ellis-Felege

221
13
8
14
12
22
9
12
12
484
315

Misses
(0.23)
(0.93)
(0.53)
(0.93)
(0.57)
(0.55)
(0.64)
(0.09)
(0.09)
(0.11)
(0.31)

Type Mismatch
23 (0.02)
0 (0.00)
1 (0.07)
0 (0.00)
9 (0.43)
7 (0.18)
3 (0.21)
7 (0.05)
47 (0.34)
152 (0.04)
16 (0.02)

Matches
708 (0.74)
1 (0.07)
6 (0.40)
1 (0.07)
0 (0.00)
11 (0.28)
2 (0.14)
120 (0.86)
78 (0.57)
3686 (0.85)
701 (0.68)

Table 3: With a 5 second buﬀer for matching, how many full misses, type mismatches and full
matches were found for observations with more than 10 volunteer entries that had matching expert entries. Type mismatches were when a user had matching start and end times, but marked
a diﬀerent type of event. Percentages of total events of that type are shown in parenthesis.

Event
Parent Behavior - Not In Video
Chick Behavior - In Video
Territorial - Predator
Territorial - Non-Predator Animal
Camera Interaction - Attack
Camera Interaction - Physical Inspection
Camera Interaction - Observation
Error - Video Error
Error - Camera Issue
Parent Behavior - On Nest
Parent Behavior - Oﬀ Nest

177
13
8
13
10
12
7
12
12
409
253

Misses
(0.19)
(0.93)
(0.53)
(0.87)
(0.48)
(0.30)
(0.50)
(0.09)
(0.09)
(0.09)
(0.25)

Type Mismatch
26 (0.03)
0 (0.00)
1 (0.07)
1 (0.07)
11 (0.52)
14 (0.35)
4 (0.29)
7 (0.05)
47 (0.34)
168 (0.04)
29 (0.03)

Matches
749 (0.79)
1 (0.07)
6 (0.40)
1 (0.07)
0 (0.00)
14 (0.35)
3 (0.21)
120 (0.86)
78 (0.57)
3745 (0.87)
750 (0.73)

Table 4: With a 10 second buﬀer for matching, how many full misses, type mismatches
and full matches were found for observations with more than 10 volunteer entries that had
matching expert entries. Type mismatches were when a user had matching start and end
times, but marked a diﬀerent type of event. Percentages of total events of that type are shown
in parenthesis.

show that the two events should probably be merged as they are similar enough to not matter.
The issues with territorial events need to be addressed by providing more information to the
volunteers and a more in depth examination on a per video basis of why they were mismarked.
There are a few hypothetical reasons for this. First, in a recent survey taken of
Wildlife@Home users, only 38% considered themselves ﬂuent in English. It is possible that
while there are extensive instructions on how to properly mark events, there are not translations of these, making it challenging for some vounteers to understand some of the nuances
between these event types, e.g., the diﬀerence between a bird observing, physically inspecting,
or attacking a camera. Second, these events happen infrequently compared to on nest, oﬀ nest,
and not in video events. Either the limited number of samples is not portraying an accurate
representation of how the users are classifying these events, or users haven’t had enough experience with them being validated correctly or incorrectly to appropriately learn how to mark
these events.
391

Crowd Sourcing Avian Nesting Video

Misses
Type Mismatch
Matches

Desell, Goehner, Andes, Eckroad and Ellis-Felege
Easy
2529 (0.15)
1056 (0.06)
13774 (0.79)

Medium
145 (0.14)
57 (0.05)
863 (0.81)

Hard
90 (0.20)
24 (0.05)
330 (0.74)

Table 5: How many misses, type mismatches and matches were made by users depending on
how hard they marked the diﬃculty of determining the observations.

4.3

Reported Diﬃculty vs. Correctness

Table 5 shows how accurate the volunteers were depending on how diﬃcult they marked the
video. Interestingly, videos with medium diﬃculty had the highest accuracy at 81%. Videos
marked as hard had the most misses percentage wise, which is to be expected. However, apart
from easy and hard, there was not much diﬀerence in user accuracy depending on how hard they
marked the video. Type mismatches did not seem to have any correlation with user reported
diﬃculty, which can sense as type mismatches are because of users misunderstanding how to
mark events.

5

Conclusions and Future Work

This paper describes signiﬁcant improvements to the crowd sourcing interface of Wildlife@Home.
The original interface provided a simple method for users to mark yes, no or unsure for various
events within short clips of video (see Figure 2); while the new interface allows users to watch
full length videos and enter any number of events with speciﬁc beginning and ending times, tags
and comments (see Figure 3). This new interface provided a dramatic reduction in the amount
of storage resources required to host the over 85,000 hours of avian nesting video gathered
for the project, as the original interface required the archival video to be converted into short
segments which needed to be in multiple formats for HTML5 video streaming.
Using the original interface, users had signiﬁcant trouble determining the presence or absence of a bird in the short video segments, which contained varying weather conditions and
cryptically colored (camouﬂagued) birds The original interface had an approximately 51% accuracy rate compared to expert observations, which was barely better than guessing. With
the new interface, users ability to determine bird presence at the nest increased from 51% to
87%, bird absence from 61% to 74% and bird presence oﬀ the nest from 69% to 73%. While
being able to get signiﬁcantly better information on many events from the users, this interface
also allowed for a direct comparison of user observations to expert observations and uncovered
potential improvements to be made, especially in the cases of camera interaction events and
video/camera error events. These can potentially be improved by further user education and
the addition of translations as many of our volunteers are not native english speakers.
These results show that it is possible to get accurate results from the public for classifying
challenging video for scientiﬁc purposes, with proper education and instruction. While this is
signﬁciant on its own, and Wildlife@Home’s users are providing valuable information about
avian nesting behavior, this is not the ﬁnal goal for the project. For future work, we will be
codifying these observations that have also been validated by project scientists and developing
a data set for computer vision researchers. The end goal is to use this information to develop
computer vision algorithms which will be able to automate the arduous task of classifying events
within these videos, or at the very least ﬁlter out video where nothing is happening. Lastly,
Wildlife@Home is open source1 , and has been developed with the ability easily add additional
1 https://github.com/travisdesell/wildlife

392

at home

Crowd Sourcing Avian Nesting Video

Desell, Goehner, Andes, Eckroad and Ellis-Felege

projects and data sets, which will prove valuable for other wildlife ecologists who require the
analysis of large scale data sets.

6

Acknowledgements

We would like to thank all the dedicated Wildlife@Home volunteers who have spent so much
time watching video and providing helpful feedback. This work has been partially supported
by the National Science Foundation under Grant Number 1319700. Any opinions, ﬁndings, and
conclusions or recommendations expressed in this material are those of the author(s) and do
not necessarily reﬂect the views of the National Science Foundation.

References
[1] J. et al Adelman-McCarthy.
The 6th Sloan Digital Sky Survey Data Release,
http://www.sdss.org/dr6/, July 2007. ApJS, in press, arXiv/0707.3413.
[2] R. Bonney, C. B. Cooper, J. Dickinson, S. Kelling, T. Phillips, K. V. Rosenberg, and J. Shirk. A
developing tool for expanding science knowledge and scientiﬁc literacy. BioScience, 59:977–984,
2009.
[3] W. A. Cox, M. S. Pruett, T. J. Benson, J. C. Scott, and F. R. Thompson. Development of camera
technology for monitoring nests. Video surveillance of nesting birds. Studies in Avian Biology,
43:185–210, 2012.
[4] Travis Desell, Robert Bergman, Kyle Goehner, Ronald Marsh, Rebecca VanderClute, and Susan
Ellis-Felege. Wildlife@ home: Combining crowd sourcing and volunteer computing to analyze
avian nesting video. In eScience (eScience), 2013 IEEE 9th International Conference on, pages
107–115. IEEE, 2013.
[5] S. N. Ellis-Felege and J. P. Carroll. Gamebirds and nest cameras: present and future. Video
surveillance of nesting birds. Studies in Avian Biology, 43:35–44, 2012.
[6] Debra A. Fischer, Megan E. Schwamb, Kevin Schawinski, Chris Lintott, John Brewer, Matt
Giguere, Stuart Lynn, Michael Parrish, Thibault Sartori, Robert Simpson, Arfon Smith, Julien
Spronck, Natalie Batalha, Jason Rowe, Jon Jenkins, Steve Bryson, Andrej Prsa, Peter Tenenbaum,
Justin Crepp, Tim Morton, Andrew Howard, Michele Beleu, Zachary Kaplan, Nick vanNispen,
Charlie Sharzer, Justin DeFouw, Agnieszka Hajduk, Joe P. Neal, Adam Nemec, Nadine Schuepbach, and Valerij Zimmermann. Planet hunters: the ﬁrst two planet candidates identiﬁed by the
public using the kepler public archive data. Monthly Notices of the Royal Astronomical Society,
419(4):2900–2911, 2012.
[7] Chris Lintott, Kevin Schawinski, Steven Bamford, Ane Slosar, Kate Land, Daniel Thomas, Edd
Edmondson, Karen Masters, Robert C. Nichol, M. Jordan Raddick, Alex Szalay, Dan Andreescu,
Phil Murray, and Jan Vandenberg. Galaxy zoo 1: data release of morphological classiﬁcations for
nearly 900,000 galaxies. Monthly Notices of the Royal Astronomical Society, 410(1):166–178, 2011.
[8] Chris J. Lintott, Kevin Schawinski, Ane Slosar, Kate Land, Steven Bamford, Daniel Thomas,
M. Jordan Raddick, Robert C. Nichol, Alex Szalay, Dan Andreescu, Phil Murray, and Jan Vandenberg. Galaxy zoo: morphologies derived from visual inspection of galaxies from the sloan
digital sky survey. Monthly Notices of the Royal Astronomical Society, 389(3):1179–1189, 2008.
[9] Lion Research Center,
University of Minnesota.
[Accessed Online,
2012]
http://www.snapshotserengeti.org/.
[10] M. A. Voss and C. B. Cooper. Using a free online citizen-science project to teach observation and
quantiﬁcation of animal behavior. American Biology Teacher, 72:437–443, 2012.
[11] Chris Wood, Brian Sullivan, Marshall Iliﬀ, Daniel Fink, and Steve Kelling. ebird: engaging birders
in science and conservation. PLoS biology, 9(12):e1001220, 2011.

393

