Procedia Computer Science
Volume 51, 2015, Pages 1818–1827
ICCS 2015 International Conference On Computational Science

Eﬃcient Algorithm for Computing the Ergodic Projector of
Markov Multi-Chains
Joost Berkhout1 and Bernd F. Heidergott2
1
2

VU University Amsterdam, Amsterdam, Noord-Holland, the Netherlands
j2.berkhout@vu.nl
VU University Amsterdam, Amsterdam, Noord-Holland, the Netherlands
b.f.heidergott@vu.nl

Abstract
This paper extends the Markov uni-chain series expansion theory to Markov multi-chains, i.e.,
to Markov chains having multiple ergodic classes and possible transient states. The introduced
series expansion approximation (SEA) provides a controllable approximation for Markov multichain ergodic projectors which may be a useful tool in large-scale network analysis. As we will
illustrate by means of numerical examples, the new algorithm is faster than the power algorithm
for large networks.
Keywords: Series Expansion Approximation, Markov Multi-Chains, Large-Scale Networks, Google
PageRank, Social Networks

1

Introduction

Nowadays large-scale complex networks play an important role in modern society, which stimulated active research on large-scale networks. Think, for example, of social network analysis
which focuses on relationships among social agents, see [6]. As a second example, think of the
world wide web in which web-pages are connected through hyperlinks. Search engines, like
Google, analyze this network by means of a page ranking algorithm in order to provide some
order in the chaos of all web-pages out there [5]. Other applications of complex networks include
telecommunication networks, cognitive and semantic networks and biological networks.
This paper considers networks which can be modeled as a Markov chain. The objective is
to ﬁnd the long-term behavior of the Markov chain, i.e., ﬁnding its ergodic projector (a formal
deﬁnition will be presented shortly). The ergodic projector of the Markov chain allows for a
detailed network analysis as it uncovers the underlying structure of the network.
In the uni-chain case, where the Markov chain consists of only one closed irreducible set of
states (also known as an ergodic class) and possible transient states, the equilibrium distribution
is unique (i.e., the rows in the ergodic projector are the same). But it is clear that, in general,
real life networks have multiple ergodic classes and transient states. These, in Markov theory
1818

Selection and peer-review under responsibility of the Scientiﬁc Programme Committee of ICCS 2015
c The Authors. Published by Elsevier B.V.

doi:10.1016/j.procs.2015.05.403

Eﬃcient Algorithm for the Markov Multi-Chain Ergodic Projector

Berkhout and Heidergott

terminology, so-called Markov multi-chains (or short: multi-chains) pose diﬃculties in terms of
Markov chain analysis since the equilibrium distribution is not unique.
Google‘s PageRank algorithm circumvents the multi-chain problem using a ”bored random
server” model, [3, 5]. This essentially turns the multi-chain uni-chain via a mixing, which allows
every transition with a relative small probability. Its clear that this way important information
about the network structure is excluded that may provide valuable information.
Due to the size of real life networks, identifying the network structure is a time-consuming
task and even doesn’t give the long term behavior of the Markov chain. Once the network
structure has been identiﬁed, uni-chain Markov chain theory can be applied to ﬁnd the equilibrium distributions inside each ergodic class and the long-term behavior of the transient states,
see also the discussion in [2]. Another option for identifying the multi-chain ergodic projector
is by iteratively multiplying the transition matrix with itself, the so-called power method (PM).
By the deﬁnition of the ergodic projector, it follows that in the limit this method yields the
ergodic projector. Based on this principle, the hope is that after a ﬁnite number of iterations
the power method already provides a good approximation for the ergodic projector. It is also
one of the main mechanisms behind Google‘s original PageRank algorithm to approximate the
ergodic projector of the uni-chain made network, see for example [3].
This paper proposes an alternative for PM for ﬁnding an approximate for the ergodic projector. This so-called series expansion approximation technique (SEA) is based on series expansions
for Markov chains from [4]. While the series expansion from [4] doesn’t apply to multi-chains,
we extend SEA in this paper to multi-chains. To the best of authors’ knowledge, it is in the
inﬁnity case the ﬁrst inverse expression that gives an accurate approximation for a multi-chain
ergodic projector. This last statement will be illustrated in numerical experiments.
The outline of the rest of this paper is as follows. Section 2 introduces some terminology
and tools for Markov chains that will be used throughout the paper. Section 3 introduces
SEA, analyzes its convergence and discusses practical use. Section 4 presents some numerical
experiments where, for some randomly generated multi-chains of diﬀerent sizes, the performance
of SEA is compared to that of PM. Lastly, some conclusions can be found in Section 5.

2

Preliminaries on Finite Markov Multi-Chains

Consider a Markov multi-chain (in short: multi-chain) P which has a ﬁnite number of, say S,
states. Multi-chain means that there are multiple closed irreducible set of states (also called
ergodic classes) and a (possibly empty) set of transient states. On the contrary, a Markov
uni-chain contains only one ergodic class and a (possibly empty) set of transient states. The
ergodic projector is given by ΠP and is deﬁned as ΠP = limn→∞ P n . The (i, j)-th element of
the ergodic projector, i.e., ΠP (i, j), thus contains the probability of ending in state j in the
long-run when starting in state i. Without losing on generality, after appropriate relabeling of
the states we can write ΠP as
⎞
⎛
Π1 0 . . .
0
⎜ 0 Π2 0 . . . 0 ⎟
⎟
⎜
⎜
.. ⎟ ,
..
ΠP = ⎜ ...
(1)
⎟
.
.
⎟
⎜
⎝ 0
ΠI 0 ⎠
RI 0
R1 . . .
in case that there are I ergodic classes. Rk (i, j) gives the equilibrium probability of ending in
ergodic state j (which is part of the k-th ergodic class) when starting in transient state i. In case
1819

Eﬃcient Algorithm for the Markov Multi-Chain Ergodic Projector

Berkhout and Heidergott

there are multiple ergodic classes there doesn’t exist an unique steady-state distribution. When
an initial distribution μ is considered there exists only one unique steady-state distribution, i.e.,
μΠP , that satisﬁes (μΠP )P = (μΠP ).
The main tool for our analysis is the weighted supremum norm, also called the v-norm,
which is denoted with · v and for a matrix A ∈ RS×S deﬁned as
A

v

=

sup

S
j=1

|A(i, j)v(j)|
v(i)

i=1,...,S

,

(2)

where A(i, j) denotes the (i, j)-element of A, | · | represent the absolute value, and v(i) ≥ 1 for
all i ∈ S. v-norm convergence to zero implies element wise convergence to zero, for more details
see [4] and the references therein.
The following lemma shows that any ﬁnite-state aperiodic multi-chain P is v-geometric
ergodic, i.e., ∃c < ∞, β < 1, k < ∞ : P n − ΠP ≤ cβ n , for n ≥ k.
Lemma 2.1. [[4], v-geometric ergodicity] For ﬁnite-state, aperiodic and possible multi-chain
P , there exists a ﬁnite number k such that
P n − ΠP

v

≤ cβ n

(3)

for all n ≥ k, where c := supl=0,1,...,k−1 P l − ΠP < ∞ and β < 1.
Proof. For a proof see [4] which also applies to a multi-chain P .
Throughout this article we take v = e, where e is a vector of ones of appropriate size. Since
we consider ﬁnite-state Markov chains, the exact choice of v doesn’t eﬀect the convergence
results found in this paper. In particular, the choice of v = e results in the norm A e =
S
supi=1,...,S j=1 |A(i, j)|, also known as the maximum absolute row sum norm. In the rest
of the article we will use · instead of · e for easiness. Lastly, the deviation matrix
for a Markov chain P is denoted with DP and in mathematical terms it can be written as
∞
DP = n=0 (P n − ΠP ).

3

Series Expansion Approximation (SEA)

We are interested in the long-term behavior of a system that can be modeled through a Markov
multi-chain. In other words, we want to ﬁnd the ergodic projector of the multi-chain.
Consider two Markov kernels P0 and P1 , both having a common ﬁnite state space, which
represent the transition probabilities of two Markov chains. We assume throughout this article that P0 is uni-chain, P1 is multi-chain and both are aperiodic. For ease of reference, we
introduce:
Assumption 3.1. P0 is uni-chain, P1 is multi-chain and both are aperiodic and ﬁnite1 .
The goal of Subsection 3.1 is to provide a series expansion approximation (SEA) for ergodic
projector ΠP1 via a series expansion in P0 , i.e., the so-called root. In Subsection 3.2 we will
analyze the convergence of SEA. This section concludes in Subsection 3.3 with a discussion on
how to apply SEA in practice.
1 Note that aperiodicity is not a restrictive assumption and the authors conjecture that the results can be
extended to inﬁnite cases.

1820

Eﬃcient Algorithm for the Markov Multi-Chain Ergodic Projector

3.1

Berkhout and Heidergott

SEA for ΠP1

The approach to be presented relies on series expansion techniques [4] but a twist is needed in
order to handle multi-chains. The following update formula is given in [4]
ΠP1 = ΠP0 + ΠP1 (P1 − P0 )DP0 ,

(4)

which follows from the Poisson equation in matrix format and the uni-chain assumption for
P0 (see Assumption 3.1). N -times repeatedly iterating the expression on the right hand side
(RHS) of (4) into ΠP1 on the same RHS leads to the series expansion (SE)
N

((P1 − P0 )DP0 )n + ΠP1 ((P1 − P0 )DP0 )

ΠP1 = ΠP0

N +1

.

(5)

n=0

In the uni-chain setting it can be shown under mild conditions that the approximation
N
ΠP0 n=0 ((P1 − P0 )DP0 )n converges to ΠP1 for large N . In our case where P1 is of a multichain type this argument cannot be used. To see this, note that, due to the pre-multiplication
N
with ΠP0 , the SE ΠP0 n=0 ((P1 − P0 )DP0 )n will always lead to a stochastic matrix which has
the same rows which means that a multi-chain structure (1) can not be achieved.
In order to obtain a SE that also applies to multi-chains we will consider a slightly diﬀerent
update formula as in (4), i.e.,
ΠP1 = ΠP0 + ΠP1 (P1 − ΠP0 ),

(6)

which follows directly from (4) by noticing that
ΠP1 (P1 − P0 )DP0 = ΠP1 (I − P0 )DP0 = ΠP1 (I − ΠP0 ) = ΠP1 (P1 − ΠP0 ).

(7)

Multiplying both sides of (6) with (1 − α), for some α ∈ (0, 1), gives
(1 − α)ΠP1 = (1 − α)ΠP0 + (1 − α)ΠP1 (P1 − ΠP0 ),

(8)

some rewriting and adding αP1 to the RHS leads to,
ΠP1 = (αP1 + (1 − α)ΠP0 ) + (1 − α)ΠP1 (P1 − ΠP0 ) + α (ΠP1 − P1 ) ,

(9)

now iterate this last result N times in the ﬁrst ΠP1 term of the RHS to obtain
N

n

((1 − α)(P1 − ΠP0 )) + ΠP1 ((1 − α)(P1 − ΠP0 ))

ΠP1 = (αP1 + (1 − α)ΠP0 )
n=0

SEA

SEA error

+ α (ΠP1 − P1 )

E1 (N,α)

A(N,α)

N

N +1

(10)
n

((1 − α)(P1 − ΠP0 )) ,
n=0

Extra SEA error

E2 (N,α)

where we deﬁned the diﬀerent parts as A(N, α), E1 (N, α) and E2 (N, α), respectively. Note
that when α = 0 we are in the series expansion situation of [4].
Deﬁnition 3.1. [Series expansion approximation (SEA)] SEA with α ∈ (0, 1) and N ∈ N for
ΠP1 with P0 as root is deﬁned as
N

A(N, α) = (αP1 + (1 − α)ΠP0 )

n

((1 − α)(P1 − ΠP0 )) .

(11)

n=0

The next subsection analyzes the convergence of A(N, α).
1821

Eﬃcient Algorithm for the Markov Multi-Chain Ergodic Projector

3.2

Berkhout and Heidergott

Convergence of SEA

In this subsection we will analyze the error when using SEA as an approximation for the ergodic
projector of P1 , i.e., ΠP1 − A(N, α). In Theorem 3.1 we will prove that the norm of this error
(abbreviated as norm error) can be bounded by a function that goes to zero for large N and
small α ∈ (0, 1). Recall that convergence of a norm to zero implies element wise convergence
to zero.
Theorem 3.1. The norm error for SEA in case N ∈ N and α ∈ (0, 1) can be bounded as
ΠP1 − A(N, α) ≤ z1 (1 − α)N +1 + z2 α,

(12)

where z1 and z2 are deﬁned as
z1 :=

α(P1 )2 Γ(α) − ΠP0 (P1 )N

z2 := (k − 1)κ +

(13)

k

β c
,
1−β

(14)

∞

where Γ(α) := n=0 ((1 − α)P1 )n , κ := supl=1,...,k−1 (P1 )l − ΠP1 , c and k are ﬁnite constants
and β ∈ (0, 1) (c, k and β all follow from Lemma 2.1).
Proof. From (10) it follows for Q ∈ N (we will use N later on)
ΠP1 = A(Q, α) + E1 (Q, α) + E2 (Q, α),

(15)

so it also holds when Q → ∞ that
ΠP1 = lim A(Q, α) + lim E1 (Q, α) + lim E2 (Q, α).
Q→∞

Q→∞

Q→∞

(16)

Using this last expression and subtracting A(N, α), for N ∈ N, on both sides gives
ΠP1 − A(N, α) = lim A(Q, α) − A(N, α) + lim E2 (Q, α).
Q→∞

Q→∞

(17)

where we also used that limQ→∞ E1 (Q, α) = 0, where 0 is a zero matrix of appropriate size.
So ﬁlling in (17), where we remove ΠP0 from limQ→∞ E2 (Q, α), which is allowed since (ΠP1 −
P1 )ΠP0 = 0, gives us
∞

ΠP1 − A(N, α) = (1 − α)(αP1 + (1 − α)ΠP0 )(P1 − ΠP0 )

((1 − α)P1 )n−1

n=N +1
∞

+ α(ΠP1 − P1 )

(18)

n

((1 − α)P1 ) .
n=0

Using deﬁnition Γ(α) :=

∞
n=0 ((1

− α)P1 )n and taking norms, (18) can be bounded as2
:=(i)

ΠP1 − A(N, α) ≤ (1 − α)N +1 (αP1 + (1 − α)ΠP0 )(P1 − ΠP0 (P1 )N )Γ(α)
+ α(ΠP1 − P1 )Γ(α) ,

(19)

:=(ii)

where we deﬁned (i) and (ii), which we will further elaborate in the following, respectively:
2 Note

1822

that Γ(α) is a convergent series since (1 − α)P1 < 1 for α ∈ (0, 1).

Eﬃcient Algorithm for the Markov Multi-Chain Ergodic Projector

Berkhout and Heidergott

(i): the norm of the ﬁrst term, i.e., (i) , can be straightforwardly rewritten as
(i) = α(1 − α)N +1 ((P1 )N +2 − ΠP0 (P1 )N )Γ(α) + (1 − α)N +2 ΠP0 ((P1 )N +1 − (P1 )N )Γ(α)
= (1 − α)N +1

α(P1 )2 Γ(α) − ΠP0 (P1 )N .

(20)
As a side remark, (i) can be bounded by a ﬁnite function for α ∈ (0, 1) by observing that
αΓ(α) ≤ 1 which gives
(i) ≤ (1 − α)N +1 [ α(P1 )N +2 Γ(α) + ΠP0 (P1 )N ] ≤ 2(1 − α)N +1 .

(21)

(ii): the norm of the second term term, i.e., (ii) , can be written as
(1 − α)n (ΠP1 − (P1 )n+1 ) ,

(ii) = α

(22)

n≥0

splitting the sum at the ﬁnite number k − 1, where k follows from Lemma 2.1, and making use
of the bound given in the same lemma enables us to bound (ii) as
k−2

(ii) ≤ α

n

(1 − α) (ΠP1 − (P1 )

n+1

∞

) + α

n=0

(1 − α)n cβ n+1

n=k−1

(23)

α(1 − α)k−1 β k c
,
≤ (1 − (1 − α)k−1 )κ +
1 − (1 − α)β

where κ := supl=1,...,k−1 (P1 )l − ΠP1 , c a ﬁnite constant and β ∈ (0, 1) (c and β follow from
Lemma 2.1). Note that from the deﬁnitions it follows that κ ≤ c. Using the bound3
1 − (1 − α)k−1 ≤ α(k − 1), for α ∈ (0, 1) and k = 0, 1, . . . ,

(24)

together with (1 − α)k−1 < 1 and (1 − (1 − α)β)−1 < (1 − β)−1 gives
(ii) ≤ α (k − 1)κ +

βkc
.
1−β

(25)

Inserting the results for (i) and (ii) (see (20) and (25), respectively) into (19) leads to
ΠP1 − A(N, α) ≤ (i) + (ii)
≤

α(P1 )2 Γ(α) − ΠP0 (P1 )N (1 − α)N +1 + (k − 1)κ +
:=z1

βkc
α,
1−β

(26)

:=z2

where we deﬁned ﬁnite constants z1 and z2 .
Remark 3.1. It follows from (12) that limN →∞ ΠP1 − A(N, α) ≤ αz2 . Because z2 is ﬁnite,
this shows that the error can be controlled via the choice for α ∈ (0, 1). In theory one can choose
α arbitrarily small to reduce the norm error4 , but as we will discuss in Subsection 3.3 there is
a numerical boundary that prevents us choosing such small α.
3 Which is a good bound for α close to 0 but for α towards 1 it becomes worse for larger k. But our aim is
small α so that justiﬁes the use of the bound.
4 It can be shown that lim
α→0 limN →∞ A(N, α) = ΠP0 + ΠP0 (P1 − ΠP0 )DP1 + ΠP1 − ΠP0 ΠP1 , i.e., an
expression which follows from well known operations.

1823

Eﬃcient Algorithm for the Markov Multi-Chain Ergodic Projector

Berkhout and Heidergott

Remark 3.2. One might be interested in the α ∈ (0, 1) that minimizes the norm error upper
bound from (12) in case of ﬁnite N . Deﬁne the polynomial function f (α) as
f (α) := z1 (1 − α)N +1 + z2 α,

(27)

so that we can solve α (N ) : f (α (N )) = 0. It turns out that
α (N ) = 1 −

z2
z1 (N + 1)

1
N

,

(28)

and from the second order conditions it follows that it is a minimum which lies in (0, 1) when
z2 < z1 (N + 1), which is reasonable to assume for large enough N . α (N ) moves from 1 to 0
for increasing N , in particular limN →∞ α (N ) = 0. The value for f (α (N )) can be shown to
be
1
N
z2
1
.
(29)
f (α (N )) = z2 1 −
1 + N1 z1 (N + 1)
Some ﬁrst experiments for f (α (N )) showed that ensuring a small z1 by choosing a root ΠP0
which is close to ΠP1 result in the smaller norm error bound after enough N (this also follows
from (27)).

3.3

Application of SEA

When using limN →∞ A(N, α) as an approximation for ΠP1 in practice, according to Theorem 3.1
one should choose α very close to zero in order to achieve a small error. But experiments in
MATLAB showed that choosing α too small leads to numerical issues which (slowly) increase the
norm error as α decreases. It turned out that approximately α ≥ 10−8 leads to the theoretical
results that would be expected from Theorem 3.1. Choosing α < 10−8 leads to larger norm
error. A natural question is whether numerical ingenuity exist that enables inserting smaller α.
This is topic of further research.

4

Numerical Experiments

In this section we are going to test the practical usefulness of A(N, α), see also (11), in case
N → ∞, indicated with A(∞, α). The performance of SEA is compared with that of the
power method (PM) for approximating the ergodic projector ΠP1 . PM is one of the algorithms
used to update Google‘s uni-chain matrix, see, e.g., [1, 5], and it also works in the multichain setting. PM of order k approximates the ergodic projector through (P1 )k and once it
is known for a speciﬁc k it is clear that one extra matrix multiplication results in (P1 )2k .
From Lemma 2.1 it directly follows that the PM approximation will converge to ΠP1 . Two
performance measures are considered, namely the norm of the approximation error (norm error)
and the mean computing (comp.) time.
Diﬀerent instances are used for the numerical experiments which can be found in Table 1. An
instance is indicated with Ii, with i some number, and can be found in the ﬁrst column therin.
The second column gives the number of states for the particular instance. The third column
in Table 1 gives a vector which represents the multi-chain structure. The size of the vector
represents the number of ergodic classes and the entries give the sizes of the ergodic classes,
respectively. E.g., I1 consists of 20 states and 3 ergodic classes of which the ﬁrst ergodic class
has 5 states, the second and third ergodic classes 2 and 3 states, respectively. Consequently,
1824

Eﬃcient Algorithm for the Markov Multi-Chain Ergodic Projector

Instance name
I1
I2
I3
I4
I5
I6
I7

# states
20
20
100
100
500
1000
5000

Multi-chain structure
[5 2 3]
[1 1]
[30 15 3]
[3 1 1]
[30 15 20]
[100 35 20 2 3]
[500 100 100 50 10]

Berkhout and Heidergott

# repetitions
5000
5000
5000
5000
1000
100
5

Table 1: Chosen instances for numerical experiments (Ii stands for instance i).
the remaining 10 states are transient. The last column in Table 1 indicates the number of
repetitions used for the approximations in order to get a relative stable mean computing time.
For each instance a random multi-chain transition matrix is generated. In particular, random
numbers are sampled from (0, 1) which represent transition probabilities while respecting the
multi-chain structure. Afterwards, the rows are normalized such that they sum up to one. Once
a random transition matrix P1 is generated the corresponding exact ergodic projector ΠP1 is
calculated using the known multi-chain structure. ΠP1 is then used for calculating the norm
errors, namely ΠP1 − A(∞, α) and ΠP1 − (P1 )k , respectively. Lastly, A(∞, α) is calculated
for diﬀerent values for α from the set A = {10−6 , 10−7 , 10−8 } to verify the convergence bound
proven in Subsection 3.2 and to observe whether α has inﬂuence on the computation time. It
turns out that the computation time doesn’t depend on α, also separate experiments, in which
we calculated A(∞, α) for α = 10−i with i = 3, 4, . . . , 8, showed that the mean computation
time remained approximately constant.
The mean computing time of SEA is used as a basis for the numerical experiments. Once
this mean has been identiﬁed, the PM is allocated a same average amount of computing time
to achieve a fair comparison. The numerical experiments thus basically answer the question:
what can PM do in at least the same amount of time used for SEA? When the number (#) of
repetitions is T , the numerical experiment for an instance Ii can be described as follows5 :
Numerical experiment sketch for instance Ii;
for α ∈ A do
calculate T times A(∞, α) and save mean comp. time t¯1 and ΠP1 − A(∞, α) ;
init: set k = 1 and t¯2 = 0;
while t¯2 < t¯1 do
k = 2k, i.e., double k for eﬃcient PM;
calculate T times (P1 )k and save mean comp. time t¯2 and ΠP1 − (P1 )k ;
end
end

The numerical results for the instances from Table 1 can be found in Table 2. It reports
the performance measures of SEA and PM, respectively, under the headings ”norm error” and
”Mean comp. time”. Furthermore the speciﬁc α is given that was used for SEA and the value
of k that was used for PM, which resulted in at least the same mean computation time needed
as for SEA. The column ”Quotient” gives the fraction of the norm error for A(∞, α) and (P1 )k .
The last column ”% diﬀ.” gives the percentage of extra computing time used by PM with
respect to SEA (note that this cannot be negative).
For small matrices (small number of states) it seems that in the same time of solving a linear
5 For the computations of A(∞, α) and (P )k we rely on the MATLAB implementations of solving system
1
of linear equations and matrix multiplication, respectively.

1825

Eﬃcient Algorithm for the Markov Multi-Chain Ergodic Projector

Berkhout and Heidergott

Results I1
α
1.00e-006
1.00e-007
1.00e-008

k
1024
1024
1024

Norm
A(∞, α)
2.96e-006
2.96e-007
3.05e-008

error
(P1 )k
5.07e-014
5.07e-014
5.07e-014

Quotient
5.83e+007
5.83e+006
6.02e+005

Mean comp. time
A(∞, α)
(P1 )k
2.60e-005 4.27e-005
2.63e-005 4.38e-005
2.63e-005 4.32e-005

% diﬀ.
6.39e+001
6.63e+001
6.44e+001

Results I2
α
1.00e-006
1.00e-007
1.00e-008

k
2048
2048
2048

Norm
A(∞, α)
2.12e-005
2.12e-006
2.12e-007

error
(P1 )k
1.33e-015
1.33e-015
1.33e-015

Quotient
1.59e+010
1.59e+009
1.59e+008

Mean comp. time
A(∞, α)
(P1 )k
2.75e-005 2.81e-005
2.71e-005 2.81e-005
2.64e-005 2.77e-005

% diﬀ.
2.20e+000
3.73e+000
4.73e+000

Results I3
α
1.00e-006
1.00e-007
1.00e-008

k
64
64
128

Norm
A(∞, α)
2.58e-006
2.58e-007
2.38e-008

error
(P1 )k
2.31e-015
2.31e-015
4.27e-015

Quotient
1.12e+009
1.11e+008
5.58e+006

Mean comp. time
A(∞, α)
(P1 )k
3.67e-004 3.67e-004
3.45e-004 3.70e-004
3.82e-004 4.38e-004

% diﬀ.
7.60e-003
7.29e+000
1.45e+001

Results I4
α
1.00e-006
1.00e-007
1.00e-008

k
128
64
16

Norm
A(∞, α)
3.82e-005
3.82e-006
3.82e-007

error
(P1 )k
2.42e-003
7.07e-002
8.89e-001

Quotient
1.58e-002
5.40e-005
4.30e-007

Mean comp. time
A(∞, α)
(P1 )k
4.48e-004 4.90e-004
3.96e-004 4.76e-004
3.86e-004 4.42e-004

% diﬀ.
9.40e+000
2.02e+001
1.47e+001

Results I5
α
1.00e-006
1.00e-007
1.00e-008

k
04
04
04

Norm
A(∞, α)
1.38e-005
1.38e-006
1.37e-007

error
(P1 )k
1.18e+000
1.18e+000
1.18e+000

Quotient
1.17e-005
1.17e-006
1.16e-007

Mean comp. time
A(∞, α)
(P1 )k
1.68e-002 1.81e-002
1.63e-002 1.82e-002
1.66e-002 1.99e-002

% diﬀ.
7.70e+000
1.19e+001
1.99e+001

Results I6
α
1.00e-006
1.00e-007
1.00e-008

k
04
02
02

Norm
A(∞, α)
1.08e-005
1.08e-006
1.11e-007

error
(P1 )k
1.02e+000
1.45e+000
1.45e+000

Quotient
1.05e-005
7.44e-007
7.67e-008

Mean comp. time
A(∞, α)
(P1 )k
8.22e-002 1.17e-001
7.96e-002 8.05e-002
7.88e-002 7.96e-002

% diﬀ.
4.29e+001
1.19e+000
1.00e+000

Results I7
α
1.00e-006
1.00e-007
1.00e-008

k
02
02
02

Norm
A(∞, α)
1.13e-005
1.13e-006
1.16e-007

error
(P1 )k
1.46e+000
1.46e+000
1.46e+000

Quotient
7.76e-006
7.76e-007
7.95e-008

Mean comp. time
A(∞, α)
(P1 )k
5.69e+000 6.88e+000
6.18e+000 6.69e+000
5.83e+000 6.80e+000

% diﬀ.
2.08e+001
8.22e+000
1.67e+001

Table 2: Numerical results for I1, I2, ..., I6 and I7, respectively.

system a lot of matrix multiplications can be performed in MATLAB (e.g., in I1 log2 (1024) = 10
matrix multiplications could be performed in the same time of calculating SEA). This means,
as can be seen from the numerical results, that PM outperforms SEA in case of small matrices
(e.g., in I1 and I2), although it should also be mentioned that especially in I1 PM has much
more computation time compared to SEA.
When the matrices become larger solving a linear system becomes relatively more eﬃcient
compared to matrix multiplications according to our MATLAB results. In the numerical results
this is reﬂected by the smaller k that can be used for PM when the matrices become larger.
1826

Eﬃcient Algorithm for the Markov Multi-Chain Ergodic Projector

Berkhout and Heidergott

The eﬀect is that SEA outperforms PM in case of large matrices, both in terms of norm error
and mean computing time (e.g., in I3 and I4).
The presented numerical experiments suggest that SEA can provide an eﬃcient alternative
for PM in case of systems with a large number of states. Furthermore, the experiments also
verify the norm error bound from Theorem 3.1. I.e., when decreasing α with a factor of 10, the
norm error also approximately decreases with factor 10 (while ensuring that α ≥ 10−8 , see also
Subsection 3.3). Lastly, as stated before, the choice for α doesn’t seem to aﬀect the average
computing time for A(∞, α).

5

Conclusion

Convergence analysis and numerical experiments show that the newly introduced SEA provides
a viable approximation technique for Markov multi-chain ergodic projectors. In particular,
numerical experiments in MATLAB suggest that it outperforms PM for large systems, which
is the case in many real-life networks. The obtained Markov multi-chain ergodic projector can
be used to relative easily reveal the underlying structure of the Markov chain. Further research
points include: 1) SEA version based on the original update formula from [4], 2) analysis and
enhancements of ﬁnite term SEA (i.e., N < ∞) 3) numerical stretching so that smaller α can
be inserted in SEA, resulting in increased accuracy.

References
[1] Pavel Berkhin. A Survey on PageRank Computing. Internet Mathematics, 2(1):73–120, January
2005.
[2] Joost Berkhout and Bernd Heidergott. A Series Expansion Approach to Risk Analysis of an Inventory System with Sourcing. In Proc. 12th International Workshop on Discrete Event Systems
(WODES 2014), volume 12, pages 510–515, May 2014.
[3] Massimo Franceschet. PageRank: Standing on the shoulders of giants. Communications of the
ACM, 2011.
[4] Bernd Heidergott, Arie Hordijk, and Miranda van Uitert. Series Expansions for Finite-State Markov
Chains. Probability in the Engineering and Informational Sciences, 21(03):381, August 2007.
[5] Amy N. Langville and Carl D. Meyer. Google’s PageRank and Beyond: The Science of Search
Engine Rankings. Princeton University Press, 2011.
[6] Mark Newman. Networks: An Introduction. Oxford University Press, 2010.

1827

