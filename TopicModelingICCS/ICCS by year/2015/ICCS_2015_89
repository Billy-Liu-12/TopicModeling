Procedia Computer Science
Volume 51, 2015, Pages 1613–1622
ICCS 2015 International Conference On Computational Science

Forecasting Volcanic Plume Hazards With Fast UQ
E Ramona Stefanescu1 , Abani K Patra1 , Marcus Bursik2 , E Bruce Pitman3 , P.
Webley4 , and M. D. Jones
1

Department of Mechanical & Aerospace Engineering, University at Buﬀalo, U.S.A
ers32@buffalo.edu abani@buffalo.edu
2
Department of Geology, University at Buﬀalo, U.S.A
mib@buffalo
3
Department of Mathematics, University at Buﬀalo, U.S.A
pitman@buffalo.edu
4
University of Alaska, Fairbanks, AK
pwebley@gi.alaska.edu

Abstract
This paper introduces a numerically-stable multiscale scheme to eﬃciently generate probabilistic
hazard maps for volcanic ash transport using models of transport, dispersion and wind. The
scheme relies on graph-based algorithms and low-rank approximations of the adjacency matrix
of the graph. This procedure involves representing both the parameter space and physical
space by a weighted graph. A combination of clustering and low rank approximation is then
used to create a good approximation of the original graph. By performing a multiscale data
sampling, a well-conditioned basis of a low rank Gaussian kernel matrix, is identiﬁed and used
for out-of-sample extensions used in generating the hazard maps.
Keywords: Volcanic Ash Transport, Emulators, Multiscale Data Representation, Hazard Maps

1

Introduction

Volcanic eruptions can produce a wide range of hazards. Although phenomena such as pyroclastic ﬂows and surges, sector collapses, lahars and ballistic blocks are the most destructive
and dangerous, volcanic ash is by far the most widely distributed eruption product. Over the
past 25 years, the threat to aviation from airborne volcanic ash has been widely recognized and
documented.Eruption columns rise quickly from their source vents at velocities of 5 to >180
m/s and can be energetic enough to reach cruise altitudes of jet aircraft and beyond, to 45 km.
During volcanic eruptions, volcanic ash transport and dispersion models (VATDs) are used to
forecast the location and movement of ash clouds over hours to days in order to deﬁne hazards
to aircraft and to communities downwind. However, the uncertainty associated with the outcomes of these models and the lack of timely mechanisms for reﬁning forecasts with observation
in a timely manner are current challenges. The primary mechanism for communicating this
Selection and peer-review under responsibility of the Scientiﬁc Programme Committee of ICCS 2015
c The Authors. Published by Elsevier B.V.

doi:10.1016/j.procs.2015.05.293

1613

Forecasting Volcanic Plume Hazards With Fast UQ

Stefanescu, Patra, Bursik, Pitman

uncertainty is a hazard map. A hazard map is a predictive map for a region which provides
a probabilistic measure of a hazard (e.g. ash cloud reaching certain concentration or height
that can be considered hazardous/critical). There are numerous ways to create a volcanic
hazard map based on ash transport and dispersion modeling. Several approximate techniques
are commonly used to approximate the state pdf evolution [2], the most popular being Monte
Carlo (MC) methods [18], Gaussian closure [23], and Stochastic Averaging [26, 25]. In addition, a Gaussian Process approach to solve nonlinear stochastic diﬀerential equations has been
proposed in [3]. All of these algorithms except MC methods are suitable only for linear or
moderately nonlinear systems and the eﬀect of higher order terms can lead to signiﬁcant errors.
Uncertainty quantiﬁcation using a Monte Carlo approach for generating such hazard maps will
require at least O(103 − 106 ) such simulations. The computational diﬃculties include managing
and accessing select entities from the large data (from simulations) and of processing it using
compute intensive operations.
In this paper, we introduce a numerically-stable multiscale scheme to generate probabilistic
hazard maps. The scheme relies on graph-based algorithms and low-rank approximations of the
entire adjacency matrix of the graph. This procedure involves representing both the parameter
space and physical space by a weighted graph. A combination of clustering and low rank
approximation provides a good approximation of the original graph. By performing a multiscale
data sampling, we identify a well-conditioned basis of a low rank Gaussian kernel matrix, which
is used for out-of-sample extension. The procedure is computationally inexpensive compared to
MC, and is accurate when out-of-sample extension is performed at re-sample points at higher
resolution.
Implicitly we have addressed the problem of emulator (surrogate of expensive computer
simulation) construction by developing novel strategies for dealing with large scale data from
computationally expensive simulations, using a combination of eﬃcient sparse representations
of such simulation “data” using multilevel methodology. This framework works best when: a).
we deal with large datasets (frequently data at high temporal and spatial resolution), b) the
datasets contains many redundancies that add little important information beyond that of the
primary data, and, c) the input is too large to ﬁt in core memory - the cost of transferring
the data to and from slow memory typically dominates the cost of performing the analysis.
Such an analysis is much needed in complex environments, and provides Operational Decision
Support using a Dynamic Data Driven Application System (DDDAS) paradigm [31, 38]. To
capture the possibility of a wide range of uncertainty in numerical model response, a large set of
geostatistical model realizations needs to be processed. We couple three numerical tools for this
analysis. The ﬁrst is the Weather Research and Forecasting (WRF) model used to forecast wind
speed. The second tool is a volcanic eruption column model, bent, employed to incorporate
volcano observations and then provide initial conditions for a volcanic ash cloud transport and
dispersal (VATD) model. The third tool is a VATD model, PUFF, used to propagate ash parcels
in the wind ﬁeld. PUFF output is a record of ash concentrations in space and time, and we
have used this information to estimate the probability of the presence of ash in the atmosphere
above the Earth’s surface. Because of the separate spatial scales and physics, eruption column
(volcanic plume) models have generally been developed separately from VATD models. One
such eruption column model, bent solves a cross-sectionally averaged system of equations for
continuity, momentum and energy balance. It takes a size distribution of pyroclasts, then
outputs the height distribution of the clasts in the atmosphere [8]. In producing its eruption
outputs, bent accounts for atmospheric (wind, temperature, pressure, etc.) conditions as given
by atmospheric sounding or NWP data. It has been tested against plume rise height data, and
against dispersal data [10]. Details of the volcanic source parameters along with assumptions
1614

Forecasting Volcanic Plume Hazards With Fast UQ

Stefanescu, Patra, Bursik, Pitman

and probability distributions used are presented in [9, 28]. bent simulations output a volume
into which particles are placed in the atmosphere around the volcano. These are released into
the WRF gridded wind ﬁeld, and their movement is calculated via the VATD model PUFF.

2

Multiscale Hazard Map

Any simulator/ numerical model will naturally require values for input parameters. If the
input values for a future event of interest were known exactly, then a hazard map could be
generated from a single simulation evaluated at those inputs. Since we lack perfect knowledge
of the future, it is necessary to examine ﬂow behavior over a range of inputs. While there
are some sampling methods, such as the ones described above, there are far too expensive
(resulting in a large number of samples) or to complex to use to create a feasible hazard
map. The solution we came up to this problem can be summarized as: STEP 1) Running a
relatively small number of simulations (a few hundred to a few thousand) followed by; STEP 2)
Representing both the sample points and ash concentration at a certain hight over a speciﬁed
parcel as a weighted graph; STEP 3) Identifying representative points in both parameter and
physical space by performing a multiscale sampling (using a randomized projection to obtain
a low-rank approximation of the weighted graph adjacency matrix); STEP 4) Performing outof-sample extension at a relatively larger number of resample points (a few hundred to a few
thousand) in the parameter space which acts as a fast surrogate for the expensive simulator
STEP 5) Creating realizations (interpolation in the physical space) are created at the same or
ﬁner resolution STEP 6) Generating the hazard map from the fast surrogate.
Although this paper employs the PUFF model, any other numerical model can be used with
the appropriately adapted graph representation. The output of the model provides us information of the ash cloud, such as absolute airborne concentration, absolute fallout concentration
etc at every 6h forecast and every 2000 m pressure levels. In this study we apply our framework for the Eyjafjallaj¨
okull eruption from 14–18 April, 2010, being interested in generating a
hazard map which will provide information regarding the probability of having ash (absolute
airborne concentration > 0) on April 16 12 UTC (36 h forecast) at 2000 m. The framework
introduced here provides an approach for developing maps for many hazard scenarios, at a low
computational cost.
Schemes that rapidly extract information from large datasets often make use of low-rank
approximations of large, sparse data structures such as matrices or tensors. These decompositions usually involve the computation of eigenvectors or singular vectors. Fast, scalable
approximation to such vectors is important for the underlying scheme to be practical. Multilevel - multiscale hierarchies formed on coarse versions of the original graphs often allow rapid
calculation of low-rank approximations. We propose therefore to combine the ideas of multilevel
models to rapidly extract low rank approximations.
We build this approach using graph-based algorithms and low-rank approximations of the
adjacency matrix to obtain representative data points in the space of interest. Dimensionality reduction of this type occupies a central position in many ﬁelds, such as information
theory, where it is related to compression and coding, statistics, as well as machine learning
and sampling theory [15, 7, 21]. The goal is to change the representation of data sets into a lowdimensional description using only a small number of free parameters. The new representation
needs to describe the data in a faithful manner, preserving some quantities of interest. Analogous to the problem of dimensionality reduction is that of ﬁnding meaningful structures in data
sets. The goal here is to extract relevant features from the data in order to gain insight and understanding of the phenomena that generated the data. In order to achieve either of these two
1615

Forecasting Volcanic Plume Hazards With Fast UQ

Stefanescu, Patra, Bursik, Pitman

goals, numerous data mining and machine learning techniques rely on graph-based algorithms
[12, 21, 20, 34, 30]. In terms of data structures, graphs oﬀer an advantageous compromise
between their simplicity and interpretability while maintaining their ability to represent complex relationships between data points. A general approach for solving these large-scale graph
problems is through multilevel algorithms. This approach generally involves coarsening the
problem, extracting from it a sequence of progressively coarser levels (smaller, hence simpler,
related problems). Weighted graphs are usually employed to represent a notion of geometry
based on the local similarity or interactions between data points. In many situations, each data
sample is represented by a collection of numerical attributes. The condition for two nodes to
be connected is based on the proximity of the corresponding data point in the feature space
[33, 1].
Assembling data points according to local similarities to obtain reliable scale-dependent
global properties implies using algorithms for ﬁnding salient, coherent regions that display
similar features [11, 22, 27]. Computationally, it is useful to think of segmentation within the
framework of cuts in graph theory [17, 24]. Minimizing a normalized cut measure as proposed
by [36] is computationally prohibitive, with a cost that increases exponentially with graph size
[19]. Approximations to the optimal cut can be obtained using spectral methods, with the
most eﬃcient approximation having a computational cost proportional to n3/2 , where n is the
number of data points [35]
Multilevel algorithms have become a popular approach for graph coarsening [24, 35]This
class of algorithms recursively reduces the size of the graph (coarsens the graph) by collapsing
vertices and edges, partitions the smallest one and then reﬁnes it to construct partitions of all
subsequent larger graphs until a partition for the original graph is obtained. The coarsening
process is based on matching nodes according to heuristics (e.g., heavy edge matching). The
matched nodes are merged to create a new multinode with a volume equal to their sum. Similarly, the edges connecting multinodes are the sum of the edges incident on the corresponding
matched nodes. The coarsest level (smallest graph) is then partitioned by various heuristics.
During the uncoarsening (interpolation) process, the matched nodes are simply separated and
each gets the partition of its multinode.
Coarsening a graph into a hierarchy with optimal complexity that is still rich enough to
approximate desired features, within the provided tolerance of the original graph, is diﬃcult
[16]. For solving this problem we make use of the graph Laplacian [37, 13, 4], which gives rise
to symmetric semi-deﬁnite M -matrices and multilevel methods as Algebraic Multigrid (AMG)
are then natural candidates for the coarsening. The aim is thus to construct a coarse version
of the graph Laplacian operator using a multilevel hierarchy of graphs. In many situations,
having a coarse (i.e., reduced dimension) model that maintains some inherent features of the
original large-scale graph and respective graph Laplacian oﬀers potential to develop eﬃcient
algorithms to analyze the underlying network modeled by this large-scale graph [32, 39]. If many
realizations are processed through the same numerical model, a probability distribution of the
response can be constructed and serve as a model of uncertainty. Even though this Monte Carlo
simulation framework appears general and straightforward, several challenges make it diﬃcult
to apply. First, the set of realizations may be generated by varying several key parameters
impacting spatial variation. Applying a single geostatistical algorithm with a ﬁxed parameter
often does not cover a wide enough space of uncertainty. However, by jointly varying the
generating algorithm itself, one may need to create several hundred or thousand of realizations
to capture the possible space of uncertainty adequately. Second, the numerical model might
be computationally expensive to evaluate. Many functions are either ﬁnite diﬀerence or ﬁnite
element codes that may require some form of iterative optimization, which may take several
1616

Forecasting Volcanic Plume Hazards With Fast UQ

Stefanescu, Patra, Bursik, Pitman

CPU hours if the grid underlying the realizations contains a large (105 ,106 ) number of cells. It is
therefore impractical, in most cases, to process several hundred or thousand realizations. Thus,
we need a cheaper surrogate that builds oﬀ the limited realizations of the full model. The coarse
representation, if combined with a suitable interpolatory process to approximate the value of
the expensive model, provides a suitable process. We propose a coarsening process coupled
to an out of sample extension methodology developed in [15, 7, 29, 6]. This combination, as
a surrogate of the expensive model, is a major outcome of the work reported here. In ﬁnal
construction of an easy to evaluate model that represents the complex underlying model is
related to the ideas of Wiener Chaos and newer polynomial chaos [40] and Gaussian process
models [5].
In this work we introduce a multi-resolution scheme for an emulator construction on a
high-dimensional parameter space. The proposed scheme overcomes some limitations of the
parameter selection in the classic Bayesian emulator[5], which always involves repeated inversion of error “correlation matrix”, R. The requirement of matrix inversion restricts emulators
to small amounts of data mostly because for “large” N : R is poorly conditioned and cost of
inverting matrix is O(N 3 ) operations. Our scheme is based on mutual distances between data
points and on a continuous extension of Gaussian functions. It uses a coarse-to-ﬁne hierarchy
of the multi-resolution decomposition of a Gaussian kernel. It generates a sequence of approximations at the given function on the data, as well as their extensions to any newly-arrived
data point. The subsampling is done by interpolative decomposition of the associated Gaussian
kernel matrix in each scale in the hierarchical procedure. In this way a well-conditioned basis
is identiﬁed and used in the extension/ extrapolation process.
Typical usage involves evaluating the fast surrogate at hundreds of thousands or millions
of re-sample input points. The number of samples needed to generate the fast surrogate are
exponential in the number of dimensions. This eﬀectively limits its application to when there are
three or fewer uncertain dimensions. This implies that we need to change the representation of
the 2048 data sets, into a low-dimensional that describe the data in a faithful manner. In order to
achieve this goal, we use a technique that relies on graph-based algorithms. Weighted graphs are
employed to represent the “geometry” based on the local similarity or interaction between the
data points. Since each of the data sample is represented by a collection of numerical attributes,
the condition of two nodes to be connected is based on the proximity of the corresponding data
points in the feature space.
Many dimensionality reduction methods involve a spectral decomposition of large matrices,
whose dimensions are proportional to the size of the data, has high computational cost. The
n observations f1 , f2 , . . . , f3 are considered the data points. When the covariance of the data
points is unknown, an artiﬁcial function has to be chose. A Gaussian covariance is a popular
choice:
g (x, x ) = exp(−

x−x

2

/ )

(1)

where · · · constitutes a metric on the space (Euclidean distance in our case). The corresponding covariance (aﬃnities) is
(G ) = g (xi , xj ), i, j = 1, 2, . . . , n.

(2)

The combination of clustering and low rank approximation gives a better approximation of
the original graph. A standard low rank computation is likely to only extract information
from the largest or a few dominant clusters. The N ystr¨
om method, vastly used for out-ofsample extension has three signiﬁcant disadvantages: (a) Diagonalization of G costs O(n3 )
operations; (b) G may be ill-conditioned due to fast decay of its spectrum, and (c) it is unclear
1617

Forecasting Volcanic Plume Hazards With Fast UQ

Stefanescu, Patra, Bursik, Pitman

how to choose the length parameter since the output is sensitive to the choice of . To
overcome these limitations a multiscale approach is used: a sequence of Gaussian kernel matrices
Gs , s = 0, 1, . . . , whose entries are (G ) = g (xi , xj ), where s is a positive monotonic decreasing
function of s, which tends to zero as the scale parameter s tends to inﬁnity (i.e. s = 2−s ,
s = 0, 1, . . . .).
By the application of a randomized interpolative decomposition(ID) to Gs , a wellconditioned basis is identiﬁed for it numerical range. In each scale f is decomposed into a
sum of its projections on this basis and it is extended as f¯∗ = G∗ G−1 f . In addition, selection
of the proper columns in Gs is equivalent to data sampling of the associated data points.
The method requires no grid. It automatically generates a sequence of adaptive grids according to the data distribution. It is based on the mutual distances between the data points
and on a continuous extension of Gaussian functions. In addition, most of the costly computations are done just once during the process, independently of the number of the extended data
points since they depend only in the data and on the given function.

3

Application and Results

Our goal is to produce a map showing the probability of absolute airborne concentration
(mg/m3 ) > 0 on April 16 1200 UTC, at 2000 m at each location in the parcel, from a collection of simulator runs whose parameters are drawn from a distribution that represents a
volcanologist’s best guess of the range of possible scenarios. We consider the study case for Eyjafjallaj¨
okull eruption on 14–18 April, 2010, for which ranges of the vent radius and vent velocity
are available. For a good match of the ash volume range, the vent radius values were uniformly
distributed between 65 and 150 m, while the vent velocity followed the same distribution with
values between 45 and 124 m/s [9].
The 2-dimensional input parameter space is sampled using a simple space ﬁlling design like
Latin Hypercube Sampling (LHS) to obtain 128 sets of input. Simulations are performed at
each sample point using the PUFF model to generate a map of absolute airborne concentration,
absair (x) as function of position. In our approach, we ﬁrst identify in the physical space the
representative points (location in the parcel) for which we need to do out-of-sample extension
of the absolute airborne concentration. Using the weighted graph approach, we generate an
adjacency matrix as deﬁned in where xi and xj represent the absolute airborne concentration
at location i and j, respectively. By applying algorithm listed above we identify representative
data points at diﬀerent scales: s = 0, 1 and 1.5. Out-of-sample extension will be performed
at the same resolution (1◦ latitude by 1◦ longitude grid) as the original output. This allows
us to evaluate the performance of the out-of-sample extension (interpolation in this case) by
calculating the Root Mean Square Error (RMSE) (scaled). Based on the results presented in
Figure 1, we’ve decided that 55 points out of 274 (corresponding to s = 1) to be the chosen
data points in the physical space for which we will perform evaluation in the parameter space.
In Figure 2 is shown the absolute airborne concentration at two diﬀerent location on the
parcel as a function of the two parameters (the red dots are the LHS sample). It can be observed
that the absolute airborne concentration has not only very small values, but the majority of
the LHS sample give an absolute airborne concentration equal to 0. In many situations these
ﬁndings would create a lot of computational challenges in order to obtain an out-of-sample
extension at re-sample points in the parameter space. Using a similar approach as for the
physical space, we deﬁne the adjacency matrix whose entries are deﬁned by: f (xi , xj ) =
exp(− xi − xj 2 / s ). Here, xi and xj ∈ [65 150] × [45 124], as given by the range of vent radius
and vent velocity. This will results in a 39 representative points out of 128 LHS samples which
1618

Forecasting Volcanic Plume Hazards With Fast UQ

Stefanescu, Patra, Bursik, Pitman

(a)

(b)

(c)

(d)

Figure 1: Representative points of absolute airborne concentration (mg/m3 ) out of 274 data
points in the physical space at April 16 1200 UTC level 2000 m. The adjacency matrix of
physical space graph was deﬁned using s = 2(−s) a) For s = 1 there are 55 representative
points b) Interpolation RM SE = 0.027 c) For s = 1 there are 109 representative points d)
Interpolation RM SE = 0.013
are used in the out-of sample extension at a larger number of re-sample points ( red dots in
Fig. 2).
−13

x 10

−14

x 10

4.5

2
4
3.5

1.5

ash

ash

3
2.5

1

2

0.5

1.5

0
50

0.5

1

0
150
100

100

vent radius

150

40

60

80

100

120

140

50
vent velocity

vent velocity

(a) Absolute airborne concentration (mg/m3 ) at location (12, 55) lat, lon - 200 resample points

0

60

80

100

120

140

160

vent radius

(b) Absolute airborne concentration (mg/m3 ) at location (22, 48) lat, lon - 1000
re-sample points

Figure 2: Absolute airborne concentration (mg/m3 ) at a given location as a function of vent
velocity and vent radius. Blue dots are LHS samples and red dots are re-samples data points
Now for each re-sample point in parameter space we can generate a map showing absolute
airborne concentration and for each grid point ﬁnd the fraction of re-sample points with absolute
1619

Forecasting Volcanic Plume Hazards With Fast UQ

Stefanescu, Patra, Bursik, Pitman

(a) MC 3500 sample runs

(b) PCQ 255 sample runs

(c) Multiscale 128 sample runs and 50
re-samples

(d) Multiscale 128 sample runs and 200
re-samples

Figure 3: Probability of having absolute airborne concentration (mg/m3 ) > 0
airborne concentration > 0. The probability map resulted is shown in Figure 3 (c, d). Our
scheme is compared to MC when 3500 realizations of the parameter space are used and PCQ
with 255 samples in the parameter space. Both methods are in detailed explained by Dalbey et
al. [14]. Performing a qualitative analysis of the methods used, it can be seen that our approach
gives comparable results with MC and PCQ with much smaller number of evaluations of both
simulator and emulator.

4

Conclusions

We introduce a multiscale scheme to generate eﬃciently probabilistic hazard maps. Typical
usage involves evaluating a fast surrogate at hundreds of thousands or millions of re-sample
input points. Our scheme relies on graph-based algorithms. Weighted graphs are employed
to represent the “geometry” based on the local similarity or interaction between the data
points. Since each of the data sample is represented by a collection of numerical attributes, the
condition of two nodes to be connected is based on the proximity of the corresponding data
points in the feature space. We also introduced a stochastic algorithm for computing a low
rank approximation of the entire adjacency matrix of the graph. By performing a multiscale
data sampling we identify a well-conditioned basis of a low rank Gaussian kernel matrix, which
is used for out-of-sample extension.

References
[1] S. Alpert, M. Galun, R. Basri, and A. Brandt. Image segmentation by probabilistic bottom-up
aggregation and cue integration. In Computer Vision and Pattern Recognition, 2007. CVPR’07.

1620

Forecasting Volcanic Plume Hazards With Fast UQ

Stefanescu, Patra, Bursik, Pitman

IEEE Conference on, pages 1–8. IEEE, 2007.
[2] A. Apte, M. Hairer, A. Stuart, and J. Voss. Sampling the posterior: An approach to non-Gaussian
data simulation. Physica D, 230:50–64, 2007.
[3] C. Archambeau, D. Cornford, M. Opper, and J. Shawe-Taylor. Gaussian process approximations of
stochastic diﬀerential equations. Journal of Machine Learning Research Workshop and Conference
Proceedings, 1:1–16, 2007.
[4] M. Belkin and P. Niyogi. Laplacian eigenmaps and spectral techniques for embedding and clustering. In NIPS, volume 14, pages 585–591, 2001.
[5] J. Berger and R. Wolpert. Estimating the mean function of a gaussian process and the stein eﬀect.
Journal of multivariate analysis, 13(3):401–424, 1983.
[6] A. Bermanis, A. Averbuch, and R.R. Coifman. Multiscale data sampling and function extension.
Applied and Computational Harmonic Analysis, 34(1):15–29, 2013.
[7] A. Blum. Random projection, margins, kernels, and feature-selection. In Subspace, Latent Structure and Feature Selection, pages 52–68. Springer, 2006.
[8] M. Bursik. Eﬀect of wind on the rise height of volcanic plumes. Geophys. Res. Lett., 18:3621–3624,
2001.
[9] M.I. Bursik, M. Jones, C. Carn, K. Dean, A. Patra, M. Pavolonis, E. Pitman, T. Singh, P. Singla,
P. Webley, H. Bjornsson, and M. Ripepe. Estimation and propagation of volcanic source parameter
uncertainty in an ash transport and dispersal model: application to the Eyjafjallajokull plume of
14 - 16 April 2010. Bull Volcanoll, 2012.
[10] M.I Bursik, S. Kobs, A. Burns, O. Braitseva, L. Bazanova, I. Melekestsev, D. Kurbatov, and D.C
Pieri. Volcanic plumes and the wind: jetstream interaction examples and implications for air
traﬃc. J. of Volcanology and Geothermal Research, 186:60–67, 2009.
[11] A.P. Carleer, O. Debeir, and E. Wolﬀ. Assessment of very high spatial resolution satellite image
segmentations. Photogrammetric Engineering and Remote Sensing, pages 1285–1294, 2005.
[12] R.R. Coifman and S. Lafon. Diﬀusion maps. Applied and computational harmonic analysis,
21(1):5–30, 2006.
[13] R.R. Coifman, Y. Shkolnisky, F.J. Sigworth, and A. Singer. Graph laplacian tomography from
unknown random projections. Image Processing, IEEE Transactions on, 17(10):1891–1899, 2008.
[14] K. Dalbey, A.K. Patra, E.B. Pitman, M.I. Bursik, and M.F. Sheridan. Input uncertainty propagation methods and hazard mapping of geophysical mass ﬂow. Journal of Geophysical Research,
113:5203–5219, 2008. doi:10.1029/2006JB004471.
[15] S. Dasgupta and A. Gupta. An elementary proof of a theorem of Johnson and Lindenstrauss.
Random Structures & Algorithms, 22(1):60–65, 2003.
[16] H. De Sterck, T.A. Manteuﬀel, S.F. McCormick, K. Miller, J. Pearson, J. Ruge, and G. Sanders.
Smoothed aggregation multigrid for markov chains. SIAM Journal on Scientiﬁc Computing,
32(1):40–61, 2010.
[17] I.S. Dhillon, P. Guan, and B. Kulis. Weighted graph cuts without eigenvectors: A multilevel
approach. IEEE Transactions on Pattern Analysis and Machine Intelligence, 29 (11):1944–1957,
2007.
[18] A. Doucet, N. de Freitas, and N. Gordon. Sequential Monte-Carlo Methods in Practice. SpringerVerlag, April 2001, 6-14.
[19] C. Fowlkes, F. Belongie, F. Chung, and J. Malik. Spectral grouping using the nystr¨
om method.
IEEE Transactions on Pattern Analysis and Machine Intelligence, 26 (2):214–225, 2004.
[20] N. Halko, P.-G. Martinsson, and J. A. Tropp. Finding structure with randomness: Probabilistic
algorithms for constructing approximate matrix decompositions. SIAM review, 53(2):217–288,
2011.
[21] C. Hegde, M. Wakin, and R. Baraniuk. Random projections for manifold learning. In Advances
in neural information processing systems, pages 641–648, 2008.

1621

Forecasting Volcanic Plume Hazards With Fast UQ

Stefanescu, Patra, Bursik, Pitman

[22] T. Inglis, H. Sterck, G. Sanders, H. Djambazian, R. Sladek, S. Sundararajan, and T.J. Hudson. Multilevel space-time aggregation for bright ﬁeld cell microscopy segmentation and tracking.
Journal of Biomedical Imaging, 2010:8, 2010.
[23] R. N. Iyengar and P. K. Dash. Study of the random vibration of nonlinear systems by the Gaussian
closure technique. Journal of Applied Mechanics, 45:393–399, 1978.
[24] G. Karypis and V. Kumar. A fast and high quality multilevel scheme for partitioning irregular
graphs. SIAM Journal on scientiﬁc Computing, 20(1):359–392, 1998.
[25] T. Lefebvre, H. Bruyninckx, and J. De Schutter. Comment on a new method for the nonlinear transformations of means and covariances in ﬁlters and estimators. IEEE Transactions on
Automatic Control, 47(8), 2002.
[26] T. Lefebvre, H. Bruyninckx, and J. De Schutter. Kalman ﬁlters of non-linear systems: A comparison of performance. International journal of Control, 77(7):639–653, 2004.
[27] O.E. Livne and A. Brandt. Lean algebraic multigrid (lamg): Fast graph laplacian linear solver.
SIAM Journal on Scientiﬁc Computing, 34(4):B499–B522, 2012.
[28] R. Madankan, S. Pouget, P. Singla, M. Bursik, J. Dehn, M. Jones, A. Patra, M. Pavolonis, E.B.
Pitman, and T. Singh. Computation of probabilistic hazard maps and source parameter estimation
for volcanic ash transport and dispersion. Journal of Computational Physics, 2013.
[29] M.W. Mahoney. Randomized algorithms for matrices and data. Foundations and Trends R in
Machine Learning, 3(2):123–224, 2011.
[30] P.-G. Martinsson, V. Rokhlin, and M. Tygert. A randomized algorithm for the decomposition of
matrices. Applied and Computational Harmonic Analysis, 30(1):47–68, 2011.
[31] A.K. Patra, M. Bursik, J. Dehn, M. Jones, R. Madankan, D. Morton, M. Pavolonis, E.B. Pitman,
S. Pouget, T. Singh, P. Singla, E.R. Stefanescu, and P. Webley. Challenges in developing DDDAS
based methodology for volcanic ash hazard analysis eﬀect of numerical weather prediction variability and parameter estimation. Procedia Computer Science, 18(0):1871 – 1880, 2013. 2013
International Conference on Computational Science.
[32] D. Ron, S. Wishko-Stern, and A. Brandt. An algebraic multigrid based algorithm for bisectioning general graphs. Rapport technique MCS05-01, Department of Computer Science and Applied
Mathematics, the Weizmann Institute of Science, 2005.
[33] I. Safro, D. Ron, and A. Brandt. Graph minimum linear arrangement by multilevel weighted edge
contractions. Journal of Algorithms, 60(1):24–41, 2006.
[34] B. Savas and I.S. Dhillon. Clustered low rank approximation of graphs in information science
applications. In SDM, pages 164–175. SIAM, 2011.
[35] E. Sharon, A. Brandt, and R. Basri. Fast multiscale image segmentation. In Computer Vision
and Pattern Recognition, 2000. Proceedings. IEEE Conference on, volume 1, pages 70–77. IEEE,
2000.
[36] J. Shi and J. Malik. Normalized cuts and image segmentation. IEEE Transactions on Pattern
Analysis and Machine Intelligence, 22 (8):888–905, 2000.
[37] A. Singer. From graph to manifold laplacian: The convergence rate. Applied and Computational
Harmonic Analysis, 21(1):128–134, 2006.
[38] E.R. Stefanescu, A.K. Patra, M. Bursik, M. Jones, R. Madankan, E.B. Pitman, S. Pouget,
T. Singh, P. Singla, P. Webley, and D. Morton. Fast construction of surrogates for {UQ} central
to dddas application to volcanic ash transport. Procedia Computer Science, 29(0):1227 – 1235,
2014. 2014 International Conference on Computational Science.
[39] P.S. Vassilevski and L.T. Zikatanov. Commuting projections on graphs. Numerical Linear Algebra
with Applications, 21(3):297–315, 2014.
[40] D. Xiu and G. E. Karniadakis. The Wiener-Askey polynimial chaos for stochastic diﬀerential
equations. SIAM Journal on Scientiﬁc Computing, 24:619–644, 2002.

1622

