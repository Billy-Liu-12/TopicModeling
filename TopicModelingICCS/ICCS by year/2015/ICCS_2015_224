Procedia Computer Science
Volume 51, 2015, Pages 1483–1493
ICCS 2015 International Conference On Computational Science

Scalable multicase urban earthquake simulation method for
stochastic earthquake disaster estimation
Kohei Fujita1,2 , Tsuyoshi Ichimura3 , Muneo Hori3,1 ,
Lalith Maddegedara3 , and Seizo Tanaka3
1

RIKEN Advanced Institute for Computational Science, Hyogo, Japan
Research Fellow of the Japan Society for the Promotion of Science (PD)
Earthquake Research Inst. & Dept. of Civil Engineering, The University of Tokyo, Tokyo, Japan
kohei.fujita@riken.jp, {ichimura,hori,lalith,stanaka}@eri.u-tokyo.ac.jp
2

3

Abstract
High-resolution urban earthquake simulations are expected to be useful for improving the reliability of the estimates of damage due to future earthquakes. However, current high-resolution
simulation models involve uncertainties in their inputs. An alternative is to apply stochastic
analyses using multicase simulations with varying inputs. In this study, we develop a method
for simulating the responses of ground and buildings to many earthquakes. By a suitable mapping of computations among computation cores, the developed program attains 97.4% size-up
scalability using 320,000 processes (40,000 nodes) on the K computer. This enables the computation of more than 1,000 earthquake scenarios for 0.25 million structures in central Tokyo.
Keywords: Urban earthquake simulation, multicase simulation, stochastic simulation, scalability, ﬁle
I/O

1

Introduction

Earthquakes have the potential to impose severe damage over vast areas, so for eﬀective mitigation, means are needed for estimating earthquake damage. Current estimates of earthquake
disasters typically rely on empirical or statistical methods based on past earthquake data. However, such methods cannot account for the combination of the earthquake source, propagation
paths, and building characteristics, thus the reliability of those estimates is not necessarily
high. To account for the high-resolution physical processes that occur in earthquake disasters, we have developed an integrated earthquake simulator (IES) [1]. As shown in Fig. 1, the
IES can provide high-resolution and high-precision simulations of earthquakes by combining
physics-based numerical methods that simulate each phase of an earthquake disaster (i.e., wave
propagation through crust, surface soil ampliﬁcation, and structural responses). In past studies,
surface soil ampliﬁcation and structural response analyses were combined for quick estimates
Selection and peer-review under responsibility of the Scientiﬁc Programme Committee of ICCS 2015
c The Authors. Published by Elsevier B.V.

doi:10.1016/j.procs.2015.05.338

1483

Scalable multicase urban earthquake simulation method

ii)

K. Fujita et al.
Data

Analysis

Source parameters

i)
City

(c) Structural response

Surface soil

(a) Wave propagation
analysis (WPA)
Wave at bedrock
(b) Soil amplification
analysis (SAA)

Bedrock
(b) Soil
amplification

Crust
Fault

(a) Wave
propagation

Wave at surface
(c) Structural response
analysis (SRA)
Structural response
Target problem

Figure 1: (i) Earthquake disaster process and (ii) the corresponding ﬂow in the integrated
earthquake simulator (IES). In this study, we target soil ampliﬁcation phase and structural
response phases.

of damage distributions after earthquakes [2], while crust, surface soil, and structure analyses
were combined to perform fault-to-city simulations [3].
With improvements in high-performance computing methods, the resolution and precision
of earthquake simulations continue to improve (e.g., [4, 5]). Thus, the accuracy of an integrated
approach that combines these methods should further improve simulation results. However, it
is diﬃcult to obtain high-resolution data over wide areas of interest; thus, use of low-resolution
inputs to simulations introduces uncertainties in the calculations. Multicase analysis methods
with varying inputs are candidates for evaluating stochastic responses of systems with uncertain
inputs. In such computations, the challenge is handling a large amount of data that must be
passed among components; for example, the scalability of seismic tomography is limited by
large ﬁle I/O that increases linearly with respect to the number of cases [6].
In past studies, we developed a multicase analysis method for analyzing the stochastic
responses of structures (i.e., SRA in Fig. 1) [3]. In this study, we target multicase analyses
of ground and structures (i.e., SAA and SRA in Fig. 1). This problem requires more data
transfers; therefore, careful data handling is needed to attain scalability on parallel computers.
The rest of the paper is organized as follows. Section 2 describes the current IES and
its performance when applied to multicase analyses. Section 3 describes the methodology
for scalable multicase urban earthquake simulations, and its performance when run on the K
computer. As an example application, we show results from a 1,000-case simulation of an area
of Tokyo. Section 4 summarizes the paper.

2
2.1

Overview of IES and its performance
Overview of IES

IES combines multiple datasets and analysis programs for fault-to-city simulations. Several
types of analysis methods are used for each phase of an earthquake disaster, thus the ﬂexible
integration of multiple types of programs is required. To attain such ﬂexibility, IES separates
the functionality that controls the work ﬂow (hereafter called the kernel ) from the functionality that analyzes particular phenomena (called modules). The kernel manages the input and
1484

Scalable multicase urban earthquake simulation method

K. Fujita et al.

(c) Structural Response
Analysis (SRA)
Wave at surface

Structures
Surface
Interface between
soil layers
Bedrock

(b) Soil Amplification
Analysis (SAA)
Use same input wave at
bedrock for all points

Figure 2: Details of the target problem. Soil ampliﬁcation is analyzed under each structure, and
results are inputted to the structural response analysis. SAA and SRA of each point/structure
are independent of each other.

output of data, distribution of load to computation cores, and data conversion between modules. Predeveloped analysis programs are plugged into IES as modules using object-oriented
features such as polymorphism and template techniques. Interconnects of parallel computers
are used to exchange data inside each phase of a disaster, and ﬁle systems are used to exchange
data between distinct disaster phases (i.e., arrows in Fig. 1(ii) indicate data transfer using ﬁle
systems).
Other studies (e.g., [7, 8]) also use physics-based simulations to estimate damage from
earthquake disasters. The diﬀerences between IES and those studies are the automation of
model construction from digital data in the geographic information system (GIS), and the
ﬂexibility in the program structure. This enables IES to be applied to large domains with
many structures and allows the integration of many types of analysis modules, depending on
the aim of simulation.
In this study, we target a one-dimensional (1D) soil ampliﬁcation analysis and a structuralresponse analysis (Fig. 2). Surface ground motion was computed under each structure using
the same wave at bedrock, and the result was fed to the structural response analysis. The 1D
soil ampliﬁcation analysis was based on the assumptions that surface soil geometry is uniform
compared with wavelength and that 2D and 3D eﬀects can be neglected. The same wave was
used at bedrock under the assumption that the source is in the far ﬁeld. Computations for
each evaluation point and structure were independent of each other (soil-structure interactions
are not considered). Using these problem properties, IES executes serial programs that analyze
each point/structure concurrently in many computation cores of distributed-memory parallel
computers. Communications are not required in the main computation part, thus high scalability is expected. However, the ﬁle system is shared among all cases; therefore, I/O can hinder
scalability when running many simulations in parallel. Thus, we used simpler methods with
higher I/O demands per computation cost for measuring scalability. We expect higher scalability when using more sophisticated methods with larger computation costs (i.e., lower I/O
demands per computation cost) when compared with the methods used in this study.
A nonlinear 1D soil ampliﬁcation method was used for the soil ampliﬁcation analysis (SAA in
Fig. 2). The inputs were soil properties and acceleration waveforms at bedrock, and the outputs
were acceleration waveforms at the surface. An all-worker model with static load balancing was
used to distribute points to MPI processes, and each process called a serial SAA library. The
runtime of each evaluation point was estimated to be proportional to the number of soil layers.
1485

Scalable multicase urban earthquake simulation method
i)

ii)

Proc. #N-1

Input wave #M-1

SRA

Input wave #1

SRA

SAA

Input wave #0

SRA

SAA

SAA

Proc. #N-1

...

Same input
wave

K. Fujita et al.

SRA

SAA

Proc. #0
Time

Proc. #0

Time

Figure 3: Comparison of previous IES and IES targeted in this study. (i) In the previous
IES, all processes compute the response of the city to the same input wave. (ii) In this study,
processes are divided into (M ) groups, and each group computes the response of the city for
each input wave.

Input data for the target problem were divided into subregions with disjoint GIS tiles. Thus,
soil ampliﬁcation of points was computed per GIS tile and outputs were written to a single ﬁle
per GIS tile. Here, a collective write function of MPI-IO (MPI File write at all()) was used
for outputting data from distributed processes.
A multiple degree-of-freedom method was used for structural response analysis (SRA in
Fig. 2). The inputs were structural parameters and acceleration waveforms at the surface, and
the outputs were displacement waveforms of the nodes of structures (Fig. 7(i)). Here, an allworker model with static load balancing was used in the ﬂat MPI program [9]. The runtime of
each structure was estimated to be proportional to the recorded runtime of past computations.
Process #0 read input data (input parameters, structural conﬁguration) and broadcasts to all
processes. Each process read the input wave for assigned structures using the collective read
function MPI File read all(), and outputs of all structures in a GIS tile were written to a single
ﬁle using the collective write function MPI File write all().

2.2

Performance of previous IES when applied to multicase simulations

We measured the performance of IES when applied to multicase simulations. Here, we added
functionality to the previous IES to run multicase simulations by dividing processes into groups
that analyze diﬀerent input waves (Fig. 3). We measured performance on the K computer at
RIKEN. The K computer consists of 82,944 compute nodes, each with a single SPARC64TM
VIIIfx CPU. Tofu, a six dimensional interconnection network, is used for communication between nodes. A two-level ﬁle system consisting of global and local ﬁle systems is used. Both
are Lustre-based parallel ﬁle systems (Fujitsu Exabyte File System, FEFS) with stage in/out
between the two ﬁle systems before and after running simulations. Shared directories visible
from all processes were used for I/O in the measurements. Here, ﬁles were striped over twelve
object storage targets of the local ﬁle system.
The upper half of Table 1 shows the size-up scalability (sw =T1case /TN cases , where T is
elapsed time) of the SAA module. We used 160 processes (20 nodes) per analysis case and
increased the number of cases. Each case was a problem of 253,405 evaluation points with
three soil layers and 8,192 time steps. The table shows that the runtime increased with the
number of cases, with sw = 77.2% for 40 cases. Fig. 4(i) shows the breakdown of computation
time. Soil ampliﬁcation computations and ﬁle output were performed per GIS tile repeatedly
1486

Scalable multicase urban earthquake simulation method

K. Fujita et al.

Table 1: Size-up scalability of SAA module and SRA modules. Measured on the problem of
253,405 structures with N = 8,192 time steps.
#cases
1
5
10
40
1
2
4

SAA
module

SRA module

#procs. (#nodes)
160 (20)
800 (100)
1.6K (200)
6.4K (800)
320 (40)
640 (80)
1,280 (160)

#procs/case
160
160
160
160
320
320
320

Size-up scalability (sw )
(1)
0.975
0.884
0.772
(1)
0.763
0.445

1case

300

100

Proc.

50

4cases

0
0
preparation

2000

execution
4000

Proc.

Proc.

40cases

Proc.

1case

150

Runtime (s)
868.0
890.2
981.9
1,124.1
444.2
582.0
998.3

file output wave at surface

6000
0

200

400

600

800

200
100
0
0

preparation

400

file input wave at surface
execution

800

file output structural response

1200

1000

0

200

400

600

800

1000

Time s

Time s

ii) SRA module

i) SAA module

Figure 4: Time lines of (i) SAA and (ii) SRA modules.

until all tiles were ﬁnished. Although the ﬁle output throughput increased from 0.31 GiB/s
(one case) to 3.01 GiB/s (40 cases), the throughput per case decreased by about one quarter.
The lower half of Table 1 shows the size-up scalability of the SRA module. We solved a
problem of 253,405 structures with N = 8,192 time steps using 320 processes (40 nodes) per
case. Now, sw dropped with increases in the number of cases, and became 44.5% when four
cases were computed simultaneously. Fig. 4(ii) shows that, when four cases were computed
simultaneously, almost all runtime (96%) was devoted to ﬁle I/O.
These measurements show that ﬁle I/O is the major bottleneck for scalability. The scalability is worse for problems with large ratios of ﬁle I/O time to total runtime. Table 2 shows the
output size for one case of SAA and SRA. We can see that waveform data in surface and structural response data are dominant. Reduction in ﬁle sizes and speed-up of ﬁle I/O throughputs
are needed.

Table 2: File I/O size of SAA and SRA modules for one case of 253,405-structure, 8,192-timestep problem
Analysis module
SAA
SRA

Data type
Wave at bedrock
Wave at surface
Wave at surface
Structural response

I/O
Input
Output
Input
Output

Data size
0.219MiB
25.2GiB
25.2GiB
664.8GiB

1487

Scalable multicase urban earthquake simulation method

K. Fujita et al.
Processes

[1]
SAA

SAA

SAA

SRA

SRA

SRA

SAA

SAA

SAA

SRA

SRA

SRA

SAA

SAA

SAA

SRA

SRA

SRA

[115,000]
[3,035,000]
i) Problem

[ratio of data size]

File system

ii) Mapping of problem to
processes in original IES

iii) Mapping of problem to
processes in improved IES

Figure 5: Mapping of problem to processes. Thicknesses of arrows indicate the amount of data
passed between components.
Nodes
(x, y, z coordinates)

Displacement time
history of all nodes

Story nodes (z coords.)

Displacement time
history of each floor

Floor nodes (x, y coords.)
Connectivity of
walls, floors

Connectivity of floor

Properties:
• Structural type
• Analysis method
• Number of stories

ii) New format
i) Original format

Figure 6: Comparison of output data formats in the SRA module.

3
3.1

Scalable IES for multicase simulations
Methodology

First, we change the mapping of problem to processes so that it suits the change in problem
setting from one-case simulations (Fig. 3(i)) to multicase simulations (Fig. 3(ii)). Problem
settings are illustrated in Fig. 5(i), wherein arrows indicate ﬂow of data and the thickness of
each arrow indicates the amount of data. In the original IES, the problem was divided into SAA
and SRA phases and mapped to processes with individual load balancers (Fig. 5(ii)). Now, we
remap the problem to processes so that load balancing is achieved by making SAA and SRA
of each structure into a set, (Fig. 5(iii)). This allows SAA to be executed in the same process
that executes SRA, thus the output and input of surface waveforms can be eliminated.
We also reduce ﬁle operations by extracting and outputting only necessary information of
structural-response results. By using the property that the deformation of each node on the
same ﬂoor has the same value, we can change the output data format to reduce output data
size. Here, we output the deformation of one node per ﬂoor rather than all nodes of a structure
(Fig. 6(ii)). Since engineers or decision makers typically do not need all time-history results,
we can further reduce output ﬁle size by outputting only selected time steps. Table 3 shows the
reductions in ﬁle output. The amount of data was reduced by one-ﬁfth by changing the data
1488

Scalable multicase urban earthquake simulation method

K. Fujita et al.

Table 3: Output data size of structural response (253,405 structures, 8,192 time steps, one
case). Max. val. indicates the maximum response that corresponds to a data size of one time
step.
Data format
Original SRA module
Improved IES

Data size
664.8GiB
139.3GiB
31.6MiB

Time steps
8,192 (all)
8,192 (all)
1 (max. val.)

Output data of each structure
i)

Case#0

Case#1
Memory
Process
File
system

Shared directory
ii)

Case#0

Case#1
Memory
Process

Rank directories

File
system

Figure 7: Change in ﬁle output scheme from shared directory output (collective write) (top) to
rank directory output (noncollective write) (bottom).

format; it can be further decreased by reducing the number of time steps in the output.
In the original IES, collective output to a shared directory was used (Fig. 7(i)). Because all
ranks in the program access the same shared directory, ﬁle access contention can occur among
diﬀerent cases. The K computer implements a rank directory scheme that enables ﬁle I/O to
directories independent of processes. We can expect higher ﬁle output throughput by using the
rank directory scheme because it reduces dependency among processes. As shown in Fig. 7(ii),
output data from each case were gathered to the root process of each case and outputted to
rank directories using serial MPI-IO (MPI File write()).

3.2

Performance of improved IES

First, we measured the speed-up scalability (ss = T1case /(N TN cases ), where T is elapsed time),
of the improved IES using the same problem described in Section 2. Here, we input diﬀerent
waves per case and the maximum response was outputted to the local ﬁle system using the
rank directory scheme. All input waves had 32,768 time steps with dt = 0.01 s. The upper
half of Table 4 shows the elapsed time and speed-up scalability. We used ten cases of the
target problem and increased the number of processes. The table shows that the program
scales well up to 1,280 processes (160 nodes) per case, with ss = 90.5%. Scalability ss gradually
decreased with the increase in number of processes, and ss became 83.2% at 2,560 processes (320
nodes) per case. Fig. 8(i) shows the breakdown of runtime for 160 processes per case and 1,280
1489

Scalable multicase urban earthquake simulation method

K. Fujita et al.

Table 4: Scalability of improved IES measured on a problem of 253,405 structures with N =
32,768 time steps.
Runtime (s)

10
10
10
1
1,000
2,000

1.6K (0.2K)
12.8K (1.6K)
25.6K (3.2K)
160 (20)
160K (20K)
320K (40K)

160
1,280
2,560
160
160
160

3,411.3
471.1
256.2
3,402.3
3,446.2
3,491.8

preparation
1000

150

50

SAA execution
SRA execution
file output

500

103 cases

0
0
4000
8000
12 000
0

500

1000

1500

2000

2500

3000

Proc.

Proc.
1,280procs. case

1500

1case

#procs./case

Proc.

#procs. (#nodes)

Proc.
160procs. case

#cases

Speed-up
scalability (ss )
(1)
0.905
0.832
-

100

0
0
preparation

50 000

SAA execution
SRA execution
file output

100 000
150 000
0

500

1000

Time s

i) Speed-up scalability using 10 case-problem

Size-up
scalability (sw )
(1)
0.987
0.974

1500

2000

2500

3000

3500

Time s

ii) Size-up scalability using 160 procs./case

Figure 8: Time lines of improved IES.

processes per case. The execution times for SAA and SRA decreased with the increase in the
number of processes while the preparation time and ﬁle output time remained almost constant.
The execution time decreased almost linearly, which means that load was well balanced. High
scalability was attained at 160 processes per case; next, we measured size-up scalability using
this number of processes per case.
The lower half of Table 4 summarizes the size-up scalability of the program using 160
processes (20 nodes) per case. The program scales very well up to 2,000 cases (320,000 processes)
with sw = 97.4%. Fig. 8(ii) shows the breakdown of runtime for one case and for 1,000 cases.
The ﬁgure shows that the computation time was nearly constant, while the preparation time
and ﬁle output time increased from 14.3 s to 27.3 s and from 4.77 s to 28.5 s, respectively. This
increase in elapsed time is small compared with the computation time; thus, high scalability
was attained for up to half of the K computer system (40,000 compute nodes).

3.3

Example application

We model an area of central Tokyo that measured 8.0 × 7.5 km and consisted of 253,405
structures. We used the National Digital Soil Map provided by the Japanese Geotechnical
Society for modeling the three-layer soil structure. Material properties and conﬁgurations
of soil layers are shown in Table 5 and Fig. 9, respectively. One thousand waves, which were
observed in KiK-net observation stations in Japan, were inputted to bedrock, and the responses
of ground and structures were computed for each wave. All input waves had N = 32,768 time
steps with dt = 0.01 s. Using 160,000 processes (20,000 nodes) of the K computer, the time
needed to solve the problem was 3,446 s.
1490

Scalable multicase urban earthquake simulation method

K. Fujita et al.

Table 5: Material properties of soil layers.
Layer#
1 (clay)
2 (sand)
3 (bedrock)

Vs m/s
115
260
500

Gmax tf/ms2
26,640
135,000
500,000

ρ tf/m3
2.0
2.0
2.0

hmax
0.15
0.15
0.05

γr
0.20
0.20
0.20

Structures

Surface topology
-20

0

20

Clay/sand layer interface

Bedrock

40 Elevation [m]

Figure 9: Target area of 8.0 × 7.5 km in central Tokyo was modeled with three soil layers and
253,405 structures.

Fig. 10 shows the response of the city to the 1,000 input waves. Fig. 10(i) shows the
maximum response of the 1,000 cases, while Fig. 10(ii) shows the mean response of the 1,000
cases. As shown in Fig. 10(iii), all structures in the city were computed for the 1,000 input
waves, and for each structure, local soil information and properties were considered. While
the overviews in Figs. 10(i) and 10(ii) appear to be similar, we can see that the distributions
of maximum and mean responses diﬀer when compared per structure. This means that some
structures are vulnerable to most types of earthquakes, while other structures are vulnerable
to only certain types of large but rare earthquakes. Such information can only be obtained
by combining and executing many cases of ground and structural response simulations using
diﬀerent inputs. In the same way, we can use this method to run Monte Carlo simulations or
sensitivity analyses for evaluating uncertainties in earthquake disaster estimates.

4

Closing remarks

In this study, we developed a scalable method for running many cases of urban earthquake
simulations. By examining a previous implementation, we found that large ﬁle I/O used for
transferring data between simulation components was the bottleneck in multicase simulations.
We rearranged the mapping of the problem to processes so that data can be passed between
components without using a ﬁle system. The developed program attained 97.8% size-up scalability for up to 320,000 processes (40,000 compute nodes) on the K computer.
As an application, we ran 1,000 cases of ground and structure simulations targeting 0.25
million structures in central Tokyo. The results show a complex distribution of damage; such
distributions can only be obtained by combining ground and structural analyses computed for
many input waves. The developed method can be used in Monte Carlo simulations to evaluate the probability distributions of structure damage. For example, the developed simulation
1491

Scalable multicase urban earthquake simulation method

i) Max. [rad.]
0.02

K. Fujita et al.

iii) Maximum displacement

0.01
0.0

ii) Mean [rad.]
0.001
0.0005
0.0

Figure 10: Maximum and mean responses of structures in the multicase simulation of central
Tokyo. Responses of 253,405 structures were calculated using soil and structural properties for
each of 1,000 input waves at bedrock. θ indicates the maximum interstory drift angle.

method was used for the Monte Carlo structural response simulation reported in SC’14 [5];
it can also be used to improve the reliability of single-case high-resolution and high-precision
earthquake simulations. Although large computational costs are involved at present, a combination of high-resolution simulations and multicase simulations can be expected to improve the
future estimates of earthquake disasters.
Acknowledgments Results are obtained using the K computer at the RIKEN Advanced
Institute for Computational Science (Proposal number hp120308, hp130015, hp140223). We
used the National Digital Soil Map provided by the Japanese Geotechnical Society. The ﬁrst
author acknowledges support from Japan Society for the Promotion of Science (24-8183).

References
[1] Muneo Hori and Tsuyoshi Ichimura. Current state of integrated earthquake simulation for earthquake hazard and disaster. Journal of Seismology, 12(2):307–321, 2008.
[2] Kohei Fujita, Tsuyoshi Ichimura, Muneo Hori, M.L.L. Wijerathne, and Seizo Tanaka. A quick
earthquake disaster estimation system with fast urban earthquake simulation and interactive visualization. Procedia Computer Science, 29(0):866 – 876, 2014. 2014 International Conference on
Computational Science.

1492

Scalable multicase urban earthquake simulation method

K. Fujita et al.

[3] Shunsuke Homma, Kohei Fujita, Tsuyoshi Ichimura, Muneo Hori, Seckin Citak, and Takane Hori.
A physics-based monte carlo earthquake disaster simulation accounting for uncertainty in building structure parameters. Procedia Computer Science, 29(0):855 – 865, 2014. 2014 International
Conference on Computational Science.
[4] Yifeng Cui, Kim B. Olsen, Thomas H. Jordan, Kwangyoon Lee, Jun Zhou, Patrick Small, Daniel
Roten, Geoﬀrey Ely, Dhabaleswar K. Panda, Amit Chourasia, John Levesque, Steven M. Day, and
Philip Maechling. Scalable earthquake simulation on petascale supercomputers. In Proceedings
of the 2010 ACM/IEEE International Conference for High Performance Computing, Networking,
Storage and Analysis, SC ’10, pages 1–20, Washington, DC, USA, 2010. IEEE Computer Society.
[5] Tsuyoshi Ichimura, Kohei Fujita, Seizo Tanaka, Muneo Hori, Maddegedara Lalith, Yoshihisa
Shizawa, and Hiroshi Kobayashi. Physics-based urban earthquake simulation enhanced by 10.7
blndof x 30 k time-step unstructured fe non-linear seismic wave simulation. In Proceedings of the
International Conference for High Performance Computing, Networking, Storage and Analysis, SC
’14, pages 15–26. IEEE Press, 2014.
[6] Max Rietmann, Peter Messmer, Tarje Nissen-Meyer, Daniel Peter, Piero Basini, Dimitri Komatitsch, Olaf Schenk, Jeroen Tromp, Lapo Boschi, and Domenico Giardini. Forward and adjoint
simulations of seismic wave propagation on emerging large-scale gpu architectures. In Proceedings of
the International Conference on High Performance Computing, Networking, Storage and Analysis,
SC ’12, pages 38:1–38:11, Los Alamitos, CA, USA, 2012. IEEE Computer Society Press.
[7] Ricardo Taborda and Jacobo Bielak. Large-scale earthquake simulation: Computational seismology
and complex engineering systems. Computing in Science and Engineering, 13(4):14–27, 2011.
[8] S. Krishnan, M. Muto, R. Mourhatch, A. B. Bjornsson, and H. Siriki. Rupture-to-rafters simulations: Unifying science and engineering for earthquake hazard mitigation. Computing in Science
and Engineering, 13(4):28–43, 2011.
[9] M. L. L. Wijerathne, M. Hori, T. Kabeyazawa, and T. Ichimura. Strengthening of parallel computation performance of integrated earthquake simulation. Journal of Computing in Civil Engineering,
2012.

1493

