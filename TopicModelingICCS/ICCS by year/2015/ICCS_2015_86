Procedia Computer Science
Volume 51, 2015, Pages 2445–2452
ICCS 2015 International Conference On Computational Science

Fast Kernel Matrix Computation for Big Data Clustering
Nikolaos Tsapanos1 , Anastasios Tefas1 , Nikolaos Nikolaidis1 , Alexandros
Iosiﬁdis1 , and Ioannis Pitas1
Aristotle University of Thessaloniki Thessaloniki, Greece
{niktsap, tefas, nikolaid, aiosif, pitas}@aiia.csd.auth.gr

Abstract
Kernel k-Means is a basis for many state of the art global clustering approaches. When the
number of samples grows too big, however, it is extremely time-consuming to compute the
entire kernel matrix and it is impossible to store it in the memory of a single computer. The
algorithm of Approximate Kernel k-Means has been proposed, which works using only a small
part of the kernel matrix. The computation of the kernel matrix, even a part of it, remains a
signiﬁcant bottleneck of the process. Some types of kernel, however, can be computed using
matrix multiplication. Modern CPU architectures and computational optimization methods
allow for very fast matrix multiplication, thus those types of kernel matrices can be computed
much faster than others.
Keywords: Kernel Matrix, Approximate, Kernel k-Means, Big Data, Clustering

1

Introduction

The objective of data clustering is to divide a given group of unlabeled data samples in subgroups
(clusters), so that data samples belonging to the same cluster are similar to each and dissimilar
to data samples belonging to any other clusters. Clustering has found many applications in
diﬀerent scientiﬁc ﬁelds. Despite the fact that there has been an extremely rich bibliography
on this subject for years now [7], it is still an active research ﬁeld.
One of the earliest clustering methods is the k-Means algorithm [13]. It is a basic textbook
approach. Yet it is still popular, despite its age. It involves an iterative process, in which each
data sample is assigned to the closest of the k cluster centers and then each cluster center is
updated to the mean of all data samples assigned to this cluster. The initial cluster assignment
can be random. The process continues, until there are no changes, or until a maximum number
of iterations has been reached. The main drawback of this approach is the fact that the surfaces
separating the clusters can only be hyperplanes. Thus, if the clusters are not linearly separable,
the standard k-Means algorithm will not be able to give very good results.
In order to overcome this limitation, the classical algorithm has been extended into the
Kernel k-Means [15]. The basic idea behind kernel approaches is to project the data into a
Selection and peer-review under responsibility of the Scientiﬁc Programme Committee of ICCS 2015
c The Authors. Published by Elsevier B.V.

doi:10.1016/j.procs.2015.05.352

2445

Fast Kernel Matrix Computation

Tsapanos, Tefas, Nikolaidis, Iosiﬁdis and Pitas

higher, or even inﬁnite dimensional space. It is possible for a linear separator in that space
to have a non-linear projection back in the original space, thus solving the non-linear separability issue. The kernel trick [1] allows us to circumvent the actual projection to the higher
dimensional space. The trick involves using a kernel function to implicitly calculate the dot
products of vectors in the kernel space using the feature space vectors. Let ai , i = 1, . . . , n
be the data sample set to be clustered and xi ∈ Rd , i = 1, . . . , n their d-dimensional feature
vectors. If φ(xi ), φ(xi ) are the projections of the feature vectors xi and xj on the kernel space,
then κ(xi , xj ) = φ(xi )T φ(xj ) is a kernel function. Diﬀerent kernel functions correspond to different projections. Finally, Euclidean distances in the kernel space can be measured using dot
products. Kernel k-Means provides a popular starting point for many state of the art clustering
schemes [18, 19, 8, 5]. A recent survey on kernel clustering methods can be found in [6], while
[9] presents a comparative study which supports the superiority of kernel clustering methods,
over more conventional clustering approaches.
With the expansion of the Internet and the proliferation of social media, multimedia data
are being generated at unprecedented rates. This resulted in the rise of the so-called Big
Data issue. Datasets are growing larger and it is becoming increasingly harder to handle them
with traditional algorithms. With respect to clustering, a way to work around this problem is
provided by the Approximate Kernel k-Means algorithm [2]. Instead of using the full kernel
matrix, only a smaller, user deﬁned set of matrix rows is calculated and used to measure
distances and perform the clustering task. However, even the partial kernel matrix is extremely
large and time consuming to compute. In this paper, we show that, for certain types of kernel,
it is possible to calculate the kernel matrix using matrix multiplication, which in turn can
be carried out very fast on modern CPU architectures and using specialized software, namely
BLAS implementations. This demonstrates that even basic matrix operations have applications
in the analysis of modern data.
This paper is organized as follows: Sections 2 and 3 brieﬂy describe Kernel k-Means and
Approximate Kernel k-Means, respectively. Section 4 discusses the factors that constitute
the extremely fast computation of matrix multiplication possible. Section 5 Describes how
some kernels can be expressed in matrix multiplication form and can, therefore, be computed
extremely quickly. Experiments evaluating the speed and performance that the combination of
fast kernel matrix computation and Approximate Kernel k-Means yields on Big Data can be
found in Section 6, while Section 7 concludes the paper.

2

Kernel k-Means

The Kernel k-Means algorithm [3] is an extension of the classic k-Means clustering algorithm.
Taking advantage of the kernel trick, it implicitly projects the data onto a higher dimensional
space and measures Euclidean distances between data samples in that space. This circumvents
the limitation of linear separability imposed by k-Means. Let there be k clusters Cδ , δ = 1, . . . , k
and data samples ai , i = 1, . . . , n. Each cluster Cδ has a center mδ in the higher dimensional
space Rd (d << d ), where Φ : Rd → Rd is the mapping function. Assuming that there is
an assignment of every data sample to a cluster, then the center of cluster Cδ is computed as
follows:
mδ =
2446

aj ∈Cδ

φ(xj )

|Cδ |

,

(1)

Fast Kernel Matrix Computation

Tsapanos, Tefas, Nikolaidis, Iosiﬁdis and Pitas

where |Cδ | is the cardinality of cluster Cδ . The squared distance D(xi , mδ ) = ||φ(xi ) − mδ ||2
between the vectors xi and mδ can be written as:
D(xi , mδ ) = φ(xi )T φ(xi ) − 2φ(xi )T mδ + mTδ mδ .

(2)

By substituting mδ from (1) into (2), we get:
D(xi , mδ ) = φ(xi )T φ(xi ) − 2

φ(xi )T φ(xj )

|Cδ |
T
φ(x
)
φ(x
j
l)
al ∈Cδ
=
|Cδ |2

aj ∈Cδ

+

aj ∈Cδ

+

we can calculate the dot products using the kernel function:
D(xi , mδ ) = κ(xi , xi ) − 2
aj ∈Cδ

+

aj ∈Cδ

κ(xi , xj )

|Cδ |
al ∈Cδ κ(xj , xl )
=
|Cδ |2

+

Since the kernel function results are stored in the kernel matrix, κ(xi , xj ) = Kij , we ﬁnally
obtain
aj ∈Cδ Kij
aj ∈Cδ
al ∈Cδ Kjl
D(xi , mδ ) = Kii − 2
+
(3)
|Cδ |
|Cδ |2
After measuring the distance of data sample xi to each of the k clusters centers, the data
sample is reassigned to the cluster Cδ with the minimum distance D(xi , mδ ). This is an iterative
process, in which the distances are measured and the cluster assignments are updated, until
there are no more changes in the cluster entry assignments, or a maximum number of iterations
has been reached. The initial cluster entry assignments can either be manual, or completely
random.

3

Approximate Kernel k-Means

In order to derive the Approximate Kernel k-Means algorithm [2], it is useful to express the goal
of Kernel k-Means in matrix form. According to [4], the goal of Kernel k-Means is equivalent
to the following optimization problem:
C

min

n

Uki |ck (·) − κ(xi , ·)|2Hκ ,

max

U∈P {ck ∈Hκ }C
k=1

(4)

k=1 i=1

where U is a matrix containing the cluster assignment vectors, ck are the cluster centers, P is
the domain of the assignment matrices and Hκ is the kernel Hilbert space. This optimization
can be relaxed into
˜ U
˜ T )),
min (tr(K) − tr(UK
(5)
U

2447

Fast Kernel Matrix Computation

Tsapanos, Tefas, Nikolaidis, Iosiﬁdis and Pitas

˜ = [diag(√n1 . . . √nC )]−1 U and tr(·) denotes the trace of the
where K is the kernel matrix, U
respective matrix, i.e., the sum of its diagonal elements. The cluster centers for this relaxed
optimization problem are given by
n

ˆki κ(xi , ·),
U

ck (·) =

(6)

i=1

ˆ = [diag(n1 . . . nC )]−1 U.
where U
In order to subsample the kernel matrix, Approximate Kernel k-Means replaces the full
ˆ −1 KT , where KB is the n × m partial kernel matrix between
kernel matrix K in (5) with KB K
B
ˆ is the m×m kernel matrix between
the m sampled data points and the n total data points and K
the m sampled data points themselves. This is essentially the Nystrom low rank kernel matrix
approximation. Some theoretical guarantees regarding the bound of the performance loss that
results from the use of this approximation, instead of the full kernel matrix can be found in [2].
When m
n, this approximation makes clustering huge datasets viable.

4

Fast Matrix Multiplication

Basic Linear Algebra Subprograms (BLAS ) [11] is a collection of high performance functions
that provide basic tools for Linear Algebra related tasks, notably used in the Linear Algebra
Package (LAPACK ) library and MATLAB. There are several diﬀerent implementations of
BLAS, which usually specialize in diﬀerent processors, CPU architectures, or operating systems.
The function relevant to this paper is general matrix multiplication (GEMM ).
GEMM implements the operation C ← αAB + βC. In order to obtain the product AB,
one can simply set α = 1 and β = 0, but this general form of the operation can also prove
useful.
The speed of BLAS is a result of a combination of factors. It recursively splits the multiplication into smaller subtasks. It also takes advantage of the memory locality of data in
consecutive memory positions, in order to better utilize the CPU’s much faster cache memory.
It employs loop unrolling, to reduce the computational time taken up by the program’s control
ﬂow. The Single Input Multiple Data (SIMD) multiprogramming capabilities of the CPU and,
potentially, its Advanced Vector Extensions (AVX ) are exploited, in order to perform the same
computational operation on multiple matrix elements simultaneously. Finally, multiple threads
are used, so that every core of the CPU can contribute to the calculations.
There already exist matrix multiplication algorithms that improve the asymptotic computational complexity O(n3 ) of naive matrix multiplication, such as the Strassen algorithm [16]
with O(nlog2 7 ) ≈ O(n2.8 ), or other recent advancements [12] with about O(n2.373 ). In practice,
however, they are not used in BLAS implementations, as they face numerical issues, take up a
lot more memory, or require unrealistically large matrices, in order to provide an improvement.

5

Appropriate Kernels

In this section, we will see how it is possible to use the fast matrix multiplication tools provided
by BLAS to quickly compute the subsampled kernel matrix KB of very large datasets required
ˆ −1 are already contained in KB and need
by Approximate Kernel k-Means. The elements of K
no additional calculations.
2448

Fast Kernel Matrix Computation
⎡
⎢
⎢
⎢
⎢
⎢
⎢
⎢
⎢
⎣
⎡
⎢
⎢
⎢
⎢
−2 ⎢
⎢
⎢
⎢
⎣

..

..

⎢
⎢
⎢
⎢
=⎢
⎢
⎢
⎣

.

x2(i−1)l

d
l=1

d
2
l=1 xil
d
2
l=1 x(i+1)l

d
2
l=1 xil
d
2
l=1 x(i+1)l

d
2
l=1 xil
d
2
l=1 x(i+1)l

..
..
.
..

.

..
.
..

⎤
⎥
⎥
⎥
⎥
⎥
⎥
⎥
⎥
⎦

.
..

x(i−1)l x
ˆ(j−1)l

...
d
ˆjl
l=1 x(i−1)l x

d
l=1

d
ˆ(j−1)l
l=1 xil x
d
x
ˆ(j−1)l
l=1 (i+1)l x

d
ˆjl
l=1 xil x
d
x
ˆjl
l=1 (i+1)l x

d
ˆ(j+1)l
l=1 xil x
d
x
ˆ(j+1)l
l=1 (i+1)l x

⎡

.

x2(i−1)l

...

d
l=1

.

.

..

...
d
2
l=1 x(i−1)l

d
l=1

..
.

⎢
⎢
⎢
⎢
⎢
+⎢
⎢
⎢
⎢
⎣

⎡

.

.

..
.
..

..

Tsapanos, Tefas, Nikolaidis, Iosiﬁdis and Pitas

x(i−1)l x
ˆ(j+1)l

...
..

.

..
.
..

.

...

..

d
l=1

x
ˆ2(j−1)l

d
l=1

x
ˆ2jl

d
l=1

x
ˆ2(j+1)l

d
l=1
d
l=1

x
ˆ2(j−1)l

d
l=1
d
l=1

x
ˆ2jl

d
l=1
d
l=1

x
ˆ2(j+1)l

x
ˆ2(j−1)l

x
ˆ2jl

...

x
ˆ2(j+1)l

.

..
.
..

.

..
.
..

⎤

⎤
⎥
⎥
⎥
⎥
⎥
⎥
⎥
⎥
⎦

.

⎥
⎥
⎥
⎥
⎥
⎥
⎥
⎥
⎥
⎦

.

ˆ (j−1) ||2
||x(i−1) − x

...
ˆ j ||2
||x(i−1) − x

ˆ (j−1) ||2
||x(i−1) − x

ˆ (j−1) ||2
||xi − x
ˆ (j−1) ||2
||x(i+1) − x

ˆ j ||2
||xi − x
ˆ j ||2
||x(i+1) − x

ˆ (j+1) ||2
||xi − x
ˆ (j+1) ||2
||x(i+1) − x

...

..

.

..
.
..

⎤
⎥
⎥
⎥
⎥
⎥
⎥
⎥
⎦

.

Figure 1: The computation of the Euclidean distance matrix.
Let us consider the Radial Basis Function (RBF ) kernel. This kernel is deﬁned as
2

κ(xi , xj ) = e−γ||xi −xj || ,

(7)

where || · || denotes the Euclidean distance. Suppose that n is the total number of data points,
m is the number of subsampled data points for Approximate Kernel k-Means and d is the
dimensionality of the data. All the data points are stored in n × d matrix X, while the subˆ Normally, one would subtract vector xj
sampled data points are stored in m × d matrix X.
from xi , then square the results and, ﬁnally sum. However, taking advantage of the identity
(α − β)2 = α2 − 2αβ + β 2 , we can reformulate the Euclidean distance using matrices as:
ˆ T + ((X
ˆT ◦X
ˆ T 1T ) ⊗ 1n ),
Y = ((X ◦ X)1d ) ⊗ 1Tm − 2XX
d

(8)

where each element yij of matrix Y will store the Euclidean distance between data points xi
and xj , ◦ denotes the Hadamard matrix product, ⊗ denotes the Kronecker product and 1 are
column vectors with every element equal to 1 and their size is determined by their index. This
computation is illustrated in Figure 1.
Matrix ((X◦X)1d )⊗1Tm is essentially a column vector containing the sum of squares of every
ˆ T 1T ) ⊗ 1n ) is
ˆT ◦X
data point x repeated m times to form a n × m matrix. Respectively, ((X
d
2449

Fast Kernel Matrix Computation

Tsapanos, Tefas, Nikolaidis, Iosiﬁdis and Pitas

ˆ repeated
essentially a row vector containing the sum of squares of every subsampled data point x
n times to form another n × m matrix. Both of these matrices are more trivial to compute
ˆ T , which
than their matrix forms might indicate. The bottleneck of this computation is −2XX
is a traditional matrix multiplication that can be computed very fast, as described in Section
4. Finally, in order to obtain the RBF kernel matrix, every element of Y is multiplied by −γ
and used as an exponent for e to obtain the ﬁnal kernel matrix.
Every kernel that involves the Euclidean distance can therefore be computed in the
above manner. Kernels that involve the dot product of two data points are more straightˆ T provides the dot product of every data point comforward to compute, as matrix XX
bination directly. However, kernels that do not include either still have to be computed in the traditional way. Kernels that involve the Euclidean distance besides the
||xi −xj ||2
RBF kernel include the Rational Quadratic Kernel κ(xi , xj ) = 1 − ||xi −x
, the Multi2
j || +c
quadratic Kernel κ(xi , xj ) =
the Cauchy kernel

κ(xi , xj ) =

||xi − xj ||2 + c2 , its Inverse κ(xi , xj ) = √
1

1+

1
||xi −xj ||2 +c2

and

. Kernels that involve the dot product include

||xi −xj ||2
σ2

the Linear kernel κ(xi , xj ) = xTi xj + c , the Polynomial kernel κ(xi , xj ) = (axTi xj + c)b
and the Hyperbolic Tangent kernel κ(xi , xj ) = tanh(xTi xj + c) . Kernels that involve
any combination of data points except the Euclidean distance and the dot product in(x −x )2
d
clude the χ2 kernel κ(xi , xj ) = 1 − l=1 1 (xil −xjl ) and the Histogram Intersection kernel
2

κ(xi , xj ) =

6

d
l=1

il

jl

min (xil , xjl ) .

Experiments

In order to evaluate the speed with which datasets of Big Data magnitude can be clustered
and the performance of that clustering, we used the Youtube Faces dataset [17]. It contains
feature vectors for 621126 faces of politicians, actors, athletes and other celebrities, extracted
from videos on Youtube. The feature vectors are Local Binary Patterns (LBP s) [14]. Since
the features are essentially histograms, a kernel function more suited to such features would
be preferable, like Histogram Intersection of χ2 , however, we are limited to kernels we can
compute using matrix multiplication. We used 3 of the most commonly used kernels, RBF,
linear and polynomial. We calculated a partial 1000 × 621126 kernel matrix. For the RBF
kernel parameter γ, it was experimentally determined that a value of 0.05 provided the best
results. For the linear kernel, c was set to 1. The polynomial kernel was of the 2nd degree. We
run the Approximate Kernel k-Means (AKKM) algorithm for each kernel 10 times, measuring
the time for the kernel calculation, the time for AKKM and the Normalized Mutual Information
(NMI) [10] performance of the resulting clustering. An Intel Xeon E5-2680 at 2.8GHz CPU was
used for all the experiments. The results of these experiments are presented in Table 1, in the
format of mean (standard deviation).
Table 1: The experimental results for all the types of kernel used.
Kernel
RBF
Linear
Polynomial
Matrix calculation time (seconds) 61.5336 (14.1589) 8.5485 (1.0354) 8.5827 (1.0095)
AKKM time (seconds)
2887 (498.8669)
2775 (452.6060) 2.664 (49.1836)
NMI performance
0.8232 (0.0022)
0.8363 (0.0014) 0.8311 (0.0019)
2450

Fast Kernel Matrix Computation

Tsapanos, Tefas, Nikolaidis, Iosiﬁdis and Pitas

Overviewing the results, we notice that the kernel matrix computation is extremely fast,
mere seconds, in fact. In fact, it is faster than performing other operations on a per element
basis and additions, as evidenced by the fact that the best time was achieved by the linear
kernel, with a signiﬁcant advantage over the RBF kernel, which also needs to add the squared
values of the data vector elements. We notice that the time to perform AKKM was order of
magnitude longer than the kernel matrix computation, though this can be attributed to the
fact that the AKKM implementation was not as exhaustively optimized as the BLAS matrix
multiplication. In terms of performance, all kernels performed similarly, with the linear kernel
being the best, slightly above the polynomial kernel.

7

Conclusions

In this paper, we demonstrated that matrix multiplication can signiﬁcantly improve the computation speed of the kernel matrix for appropriate kernels. When using a kernel function, in
which the correlation of the data is done through the Euclidean distance or the dot product,
it is possible to express the computation in terms of matrix multiplication. This matrix operation can be performed extremely fast with modern hardware and the BLAS library, a highly
specialized and optimized piece of software, thus allowing for the extremely fast computation
of a kernel matrix. We used this approach in conjunction with Approximate Kernel k-Means,
in order to perform clustering on the Youtube Faces dataset that, which includes 621126 data
samples.

Acknowledgement
The research leading to these results has received funding from the European Union Seventh
Framework Programme (FP7/2007-2013) under grant agreement number 316564 (IMPART).
This publication reﬂects only the authors’ views. The European Union is not liable for any use
that may be made of the information contained therein.

References
[1] A. Aizerman, E. M. Braverman, and L. I. Rozoner. Theoretical foundations of the potential
function method in pattern recognition learning. Automation and Remote Control, 25:821–837,
1964.
[2] Radha Chitta, Rong Jin, Timothy C. Havens, and Anil K. Jain. Approximate kernel k-means:
solution to large scale kernel clustering. In Chid Apte’, Joydeep Ghosh, and Padhraic Smyth,
editors, KDD, pages 895–903. ACM, 2011.
[3] Inderjit S. Dhillon, Yuqiang Guan, and Brian Kulis. Weighted graph cuts without eigenvectors a
multilevel approach. IEEE Trans. Pattern Anal. Mach. Intell., 29(11):1944–1957, November 2007.
[4] Chris HQ Ding, Xiaofeng He, and Horst D Simon. On the equivalence of nonnegative matrix
factorization and spectral clustering. In SDM, volume 5, pages 606–610. SIAM, 2005.
[5] Marcelo R.P. Ferreira and Francisco de A.T. de Carvalho. Kernel-based hard clustering methods
in the feature space with automatic variable weighting. Pattern Recognition, (0):–, 2014.
[6] Maurizio Filippone, Francesco Camastra, Francesco Masulli, and Stefano Rovetta. A survey of
kernel and spectral methods for clustering. Pattern Recognition, 41(1):176 – 190, 2008.
[7] A. K. Jain, M. N. Murty, and P. J. Flynn. Data clustering: a review. ACM Comput. Surv.,
31(3):264–323, September 1999.

2451

Fast Kernel Matrix Computation

Tsapanos, Tefas, Nikolaidis, Iosiﬁdis and Pitas

[8] Hong Jia, Yiu ming Cheung, and Jiming Liu. Cooperative and penalized competitive learning
with application to kernel-based clustering. Pattern Recognition, (0):–, 2014.
[9] Dae-Won Kim, Ki Young Lee, Doheon Lee, and Kwang H. Lee. Evaluation of the performance of
clustering algorithms in kernel-induced feature space. Pattern Recognition, 38(4):607 – 611, 2005.
[10] Tarald O. Kv˚
alseth. Entropy and correlation: Some comments. IEEE Transactions on Systems,
Man, and Cybernetics, 17(3):517–519, 1987.
[11] C. L. Lawson, R. J. Hanson, D. R. Kincaid, and F. T. Krogh. Basic linear algebra subprograms
for fortran usage. ACM Trans. Math. Softw., 5(3):308–323, September 1979.
[12] Fran¸cois Le Gall. Powers of tensors and fast matrix multiplication. In Proceedings of the 39th
International Symposium on Symbolic and Algebraic Computation, ISSAC ’14, pages 296–303, New
York, NY, USA, 2014. ACM.
[13] J. B. MacQueen. Some methods for classiﬁcation and analysis of multivariate observations. In
Proceedings of 5th Berkeley Symposium on Mathematical Statistics and Probability, pages 281–297,
1967.
[14] Timo Ojala, Matti Pietik¨
ainen, and David Harwood. A comparative study of texture measures
with classiﬁcation based on featured distributions. Pattern Recognition, 29(1):51–59, January
1996.
[15] Bernhard Sch¨
olkopf, Alexander Smola, and Klaus-Robert M¨
uller. Nonlinear component analysis
as a kernel eigenvalue problem. Neural Comput., 10(5):1299–1319, July 1998.
[16] S.S. Skiena. The Algorithm Design Manual. Springer, 2009.
[17] Lior Wolf, Tal Hassner, and Itay Maoz. Face recognition in unconstrained videos with matched
background similarity. In in Proc. IEEE Conf. Comput. Vision Pattern Recognition, 2011.
[18] Shi Yu, Leon-Charles Tranchevent, Xinhai Liu, Wolfgang Glanzel, Johan A.K. Suykens, Bart De
Moor, and Yves Moreau. Optimized data fusion for kernel k-means clustering. IEEE Transactions
on Pattern Analysis and Machine Intelligence, 34(5):1031–1039, 2012.
[19] Feng Zhou, Fernando De la Torre Frade, and Jessica K Hodgins. Hierarchical aligned cluster
analysis for temporal clustering of human motion. IEEE Transactions on Pattern Analysis and
Machine Intelligence (PAMI), 35(3):582–596, March 2013.

2452

