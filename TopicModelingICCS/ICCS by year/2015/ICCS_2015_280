Procedia Computer Science
Volume 51, 2015, Pages 2357–2366
ICCS 2015 International Conference On Computational Science

Evolutionary replicative data reorganization with
prioritization for efficient workload processing
Anton Spivak , Andrew Razumovskiy , Anton Myagkov
and Denis Nasonov
1

1

1

1

ITMO University.
anton.spivak@gmail.com, xrew@yandex.ru, myagkov-a@ya.ru, denis.nasonov@gmail.com
1

Abstract
Nowadays the importance of data collection, processing, and analyzing is growing tremendously. Big
Data technologies are in high demand in different areas, including bio-informatics, hydrometeorology,
high energy physics, etc. One of the most popular computation paradigms that is used in large data
processing frameworks is the MapReduce programming model. Today integrated optimization
mechanisms that take into account only load balance and execution fast simplicity are not enough for
advanced computations and more efficient complex approaches are needed. In this paper, we suggest
an improved algorithm based on categorization for data reorganization in MapReduce frameworks
using replication and network aspects. Moreover, for urgent computations that require a specific
approach, the prioritization customization is introduced.
Keywords: MapReduce, optimization, prioritization, data reorganization, metaheuristic, genetic algorithm

1 Introduction
Nowadays the importance of data collection, processing and analyzing is growing tremendously. It
is not only needed in traditional enterprise and science, but in areas that have never used data collection
apart from financial reporting and basic analytics, such as medicine or retail business (Meng, 2011).
According to advanced research reports, the entire size of data generated all over the world is
exponentially growing and will exceed 7.9 Zb by the year 2015 (Gantz, 2012). Currently, a lot of wellknown solutions provide functionality to store and to process data (Sagynov, 2012), (Manyika, 2011).
However, one of the most popular paradigms used by large data processing frameworks is the
MapReduce programming model that was introduced by the Google research group in 2004 (Dean,
2008). In general, the MapReduce-based frameworks (such as Hadoop) distribute the data randomly
only taking into account load balance and simplicity, but query’s workload optimization problem
becomes a more and more important issue due to increasing overall data analysis utilization. It is
especially critical for data processing when user queries enforce the system to aggregate relevant data

Selection and peer-review under responsibility of the Scientiﬁc Programme Committee of ICCS 2015
c The Authors. Published by Elsevier B.V.

doi:10.1016/j.procs.2015.05.405

2357

Evolutionary replicative data reorganization with prioritization...

Denis Nasonov et al.

from the lowest worker nodes in a multi-level tree structure to the higher level nodes, or to wait until
occupied nodes will be accessed for execution. This leads to overstated cost due to increased data
transfer time and even more importantly, waiting in a queue during the computational stage. Such
conditions in certain requirements can be inacceptable especially for urgent-based computations. In this
paper we introduce a new evolutionary method based on our previous investigations (Razumovskiy,
2014)
Commonly the MapReduce technology investigations are directed towards reducing consumption
of storage space, as well as directed decreasing process time and network bandwidth consideration.
However, most of the investigations concentrate on one of the aspects. Data replica is also often used
only to increase the reliability of storage systems, but what is more important, at the same time, replica
can be used for task execution performance without negative consequence for primary goal.
There is a huge amount of relevant work on the Hadoop-based approaches optimization problems.
One of them is the DRAW algorithm (P. Shang, 2012), which consists of three main components: the
data access history graph (HDAG) used to scrutinize data access history patterns, a data grouping
matrix (DGM) derived from HDAG to group related data, and an optimal data placement algorithm
(ODPA) for generating final data layout. The basic ideas of our approach are similar, however we do
not dedicate much attention to history technical processing, and use the genetic algorithm to optimize
data placement, as well as include additional optimization criteria, like network capacity and
prioritization.
In (Shivarkar, 2014), the authors propose a method of data placement based on locality sensitive
hashing. The idea behind locality sensitive hashing (clustering algorithm) is that related files will have
similar signature. As a result, this approach has similar optimization goals; however, it does not use
historical data, and is based on the hypothesis that related files have similar structure.
Wang L. proposes a platform G-Hadoop that aims to enable large-scale distributed computing
across multiple clusters (Wang, 2013) . G-Hadoop uses GFarm file system that is a distributed file
system designed to share vast amounts of data between globally distributed clusters connected via a
wide-area network. By replicating the data of popular files to multiple nodes, GFarm is able to boost the
performance of MapReduce applications, whereas our approach use data reorganization of related files.
Jin in (Jin, 2012 ) proposes an Availability-aware DAta PlacemenT (ADAPT) strategy to improve
application performance without extra storage cost. The objective of the data placement algorithm is to
find an optimized mapping from data blocks to the nodes, such that, in general, all nodes complete their
assigned blocks processing at the same time, and in comparison with our approach, this strategy
optimizes the initialize stage of MapReduce while we can make optimization plan at any moment.
In (Poonthottam, 2013) the authors propose a new Data Placement scheme for HDFS using access
patterns. With this strategy, the data will be placed closer to the required users' queries, thereby
achieving optimization in access time and bandwidth. We also have optimization with network
assumption but it is only one of the criteria.
Kumar et al. (Kumar, 2013) propose a model and algorithms to analyze workload of nodes in
distributed environments as a hypergraph. The nodes represent file chunks that are related with each
other. The final decision on a file's replica replacement is made according to query task distribution
through the network of nodes during the execution stage, as well as its resource consumption. A
similar approach is suggested in (Çatalyürek, 2011). The hypergraph-partitioning-based formulation
helps to generate data placement and the task assignment approach to reduce the cost of the network
transfers between nodes with executed tasks.
In (Xu, 2014) a network-aware task assignment policy is introduced. It assigns tasks to cluster
nodes according to used network characteristics. It minimizes the time of "map" and "reduce" phases
execution for a task. As opposed to our proposed genetic algorithm, two types of greedy heuristics are
used here to obtain the needed assignment strategy.

2358

Evolutionary replicative data reorganization with prioritization...

Denis Nasonov et al.

An optimization approach that is closer to our research is presented in the work of (Shorfuzzaman,
2010). The authors suggest a dynamic replica placement algorithm based on a dynamic programming
approach to organize replica files placement more efficiently in the Grid environment.
The paper (Er-Dun, 2012 ) discusses the problem of balancing the nodes workload and
minimization of data transferring among the data centers. The authors similar to us use idea of genetic
heuristic algorithm to data placement strategy but with own aspects.
As it was shown, our research has significant differences from previously discussed works. As data
placement is NP-complete problem and there is no possibility to find optimal solution we tried to
remove main drawbacks as well as to aggregate basic advantages of the previously proposed approach
in (Razumovskiy, 2014) and to develop a more efficient and fast algorithm for data placement
optimization based on the categories.

2 Problem Statement
The main issue of the data optimization placement is its variability and number of dimensions. Let F
be a set of all stored files ݂௜ ‫ܨ א‬ǡ ݅ ൌ ͳǡ ݅‫ ݔܽܯ‬and set ‫ܨ‬௝ ൌ ൛݂௜௝ ൟ – files that are used in task j, ݆ ൌ
ͳǡ ݆‫ݔܽܯ‬,‫ ܥ‬ൌ ሼܿ௝ ሽ is set of tasks' importance (significance), collected from historical analysis. ܰ ൌ ሼ݊௟ ሽ
– set of nodes,݈ ൌ ͳǡ ݈‫ ݔܽܯ‬, ݈ܲ ൌ ሼ൏ ݂௜ ǡ ݊௟ ൐ሽ – set of tuples that represent current distribution of files.
Since we need to somehow calculate file distribution efficiency, we considered a function that tells how
may nodes are used to process a query task, ߮൫ܶ௝ ǡ ݈ܲ൯ ൌ ܿ‫ݐ݊ݑ݋‬൛݊௟௝ ൟǡ where ݊௟௝ ǣ‫׌‬൏ ݂௜ ǡ ݊௟௝ ൐‫א‬
݈ܲƬ݂௜ ‫ܶ  א‬௝ . Finally we have to find such ݈ܲ௢௣௧ that ‫݈ܲ׊‬Ʈǣ
௝ெ௔௫
σ௝ெ௔௫
(1)
௝ୀ଴ ܿ௝ ή ߮൫ܶ௝ ǡ ݈ܲ௢௣௧ ൯ ൑ σ௝ୀ଴ ܿ௝ ή ߮൫ܶ௝ ǡ ݈ܲƮ൯
The formula (1) represents the main part of fitness function that we used during genetic algorithm
execution. In order to improve the implemented algorithms, the categories should be introduced.
௝
௝ெ௔௫
௞
ൌ ሼ‫ܨ‬௝భ ‫ܨځ‬௝మ ‫ځ‬Ǥ Ǥ ‫ܨځ‬௝ೖ ሽȀሼ‫ܥ‬௠ǡೖశభ ǡ Ǥ Ǥ ǡ ‫ܥ‬௠ ሽ ് ‫׎‬, ݉ ൌ ͳǡ ݉‫ݔܽܯ‬that leads
Categories set is define as ‫ܥ‬௠
to the following conclusion ‫݂׊‬௜ ‫ܥ א‬௠ Ƭ‫݉׊‬Ʈ ് ݉ ‫݂  ׷‬௜ ‫ܥ  ב‬௠Ʈ  .Without loss of generality the
considerable benefit of categories usage in the role of atomic element instead of pure files can be easily
shown.

3 Heuristic approach
In the case when exact polynomial solution could not be found for some reason a heuristics
approach is mostly used in the optimization issue due to its advantages such as fast execution time. As
it was defined in chapter 2, initially there is a set of tasks that use different files in different amounts
during their executions. Tasks may intersect among used files. Each task has calculated significance,
based on the statistic of the task execution .
In this paper several types of heuristics methods are adapted and implemented with final result
comparison.

3.1 CRUSH
The CRUSH algorithm (Weil, 2006) is a pseudo-random data distribution algorithm that was
developed to efficiently place replicated data across a structured storage. The CRUSH operates in the
field of hierarchical cluster map consisted of storage devices with associated weight values. Taking
into consideration these values, the algorithm allocates replicas according to uniform distribution and

2359

Evolutionary replicative data reorganization with prioritization...

Denis Nasonov et al.

extra conditions that aim to increase reliability in the case of different domain failures. However the
CRUSH algorithm was modified to fit our goals and environment conditions, and as a result two
versions were implemented. The first implementation distributes all files across nodes, while the
second one works only with replica.

3.2 Greedy Algorithm
The greedy algorithm implementation has special aspects and can also manipulate using all files
and replica only. The basic operating element in the algorithm is file importance (or gravity) that
comes from executed tasks. It has two main stages. Firstly, all tasks are sorted by its summarized files'
gravity. Secondly, all files are sorted inside each task. At the start, the Greedy algorithm gets files one
by one according to file-gravity rank from the task with the highest accumulated gravity, and places
them on the storage agents until the free space ends, then this operation is repeated until no files
remain. The original and replica files are distributed to agents in parallel, that leads to two identically
filled by files sets of agents.
Listing 1: Pseudo-code of Greedy implementations.
1: PROCEDURE: Greedy Full Distribution
2: Input: Nodes A, TaskList T
3: begin
4: for f in Tasklist do
5:
Calculate G(f)
6: for task in Tasklist do
7:
Calculate Gt(task)
8: Sort TaskList by Gt(task)
9: for task in Tasklist do
10: Sort f in task by G(f)
11: for all task in Tasklist do
12: for all f in task end
13: if acur<Amax then
14:
Add f to acur
15: else
16:
Set a next empty
17: end
-----------------------------------------------------------------1: PROCEDURE: Greedy Replica Only Distribution
2: Input: Nodes A, TaskList T, Categories C
3: begin

4: for f in Tasklist do
5:
Calculate G(f)
6: for task in Tasklist do
7:
Calculate Gt(task)
8: Sort TaskList by Gt(task)
9: for task in Tasklist do
10: Sort f in task by G(f)
11: for task in Tasklist do
12: for f in task do
13:
Find all A with f in c
14: if acur<Amax and A in f then
15:
Add f to acur
16: else
17:
Set next a
18:
Select Randomly a in A
19: if acur<Amax then
20:
Add f to acur
21: else
22:
Set next a
23: end

The second algorithm's implementation takes into consideration the original files distribution. It
leads to modifications in the agents filling step. Before insert operation is executed the Greedy
algorithm finds all agents with files of the same category and sorts them according to its free space. It
chooses the first agent from the obtained list, which does not include the original file of the current
category. Pseudo-code of the algorithms is presented in Listing 1.

3.3 Comparison Results
The results of the implemented algorithms comparison is shown on table 1. The scenario used in
these experiments is described in section 5. Both CRUSH implementations have gained sizable
improvement in comparison with random configuration up to 61% for "all files" experiment that
confirms the effectiveness of the cluster map approach (searching position by increment) in applying
to optimization reconfiguration issue. This can be explained partly with algorithm orientation to locate

2360

Evolutionary replicative data reorganization with prioritization...

Denis Nasonov et al.

the files with closed numbers in neighbor positions. Therefore, files of the same category (task) are
filled to nodes together.
The Greedy implementations show better results in comparison with all other types up to 120%. It
can be easily explained by using two stage consideration of the files and tasks as well as as well as the
categories of importance.

Algorithm

Random all files

Fitness Results
Performance
increase (Random)

0,34
-

CRUSH all files

CRUSH replica

Greedy all files

Greedy replica

0,55

0,43

0,75

0,51

61%

26%

120%

50%

Table 1: Results of algorithm comparison

4 Metaheuristic approach
There are lot of metaheuristic approaches, including particle swarm optimization (PSO), simulated
annealing (SA), tabu search (TS), and genetic algorithm (GA), which can be used for the data
replacement optimization problem. However, in our investigations we use GA for its simplicity, which
is more important it is more suitable for combinatorial space, while PSO and SA as well as TS work
well with continual and discrete space only (Hassan, 2005).

4.1 GA and CGA
The first GA implementation of the developed scheme was presented in (Razumovskiy, 2014). It
basically considers task clustering, task significance calculation, and GA plan generation based on
provided information on tasks, files, nodes, and file distribution between nodes and tasks. The main
goals were to minimize the number of nodes that are used in task execution and to optimize file
distribution between nodes to execute the highest number of tasks simultaneously.
It has several crucial drawbacks: computational time, and loss of efficiency with growing of
number of files, as well as simplified environment representation. However, we tried to manage all
these issues by introducing several features: (a) categories, (b) network configuration, (c) bandwidth,
(d) cost of data transmission between nodes, and (e) different storage node options of file
management.

Figure 1. On the left - main schema of CGA, on the right - categories identification example.
On left part of figure 1 the main schema of the categorical genetic algorithm (CGA) is presented.
Categories play a valuable role in CGA. Previously, all files were handled separately, and in the case
of Big Data, this could lower the performance at once when file count was increasing. If we consider 3
tasks with the intersection illustrated on the right part of figure 2. It is clear that there are 7

2361

Evolutionary replicative data reorganization with prioritization...

Denis Nasonov et al.

intersections that form categories without dependency on file count in each category. Since files in
each category are equal in terms of optimization criteria (apart from some inconspicuous cases) and
may only influence the whole solution together as a full category it allows main functions like
crossover or mutation to operate directly with categories instead of files without loss of generality.
The next improvement is introducing file types. The first GA algorithm can operate with both
original files and replica. For some MapReduce solutions it can be unacceptable to move original files
that are are placed once, and no moving across storage nodes is admitted. CGA can operate only with
file replicas. It accepts other working files as well, that are not used in tasks. Another algorithm's
feature is network configuration and bandwidth awareness as we assume that files can be stored in
different data-centers with several network-separated parts. We also integrated a prioritization
mechanism to provide a possibility for urgent computing.

4.1.1. Crossover
The chromosome in our algorithm consists of nodes that contain files and information about the
categories. Duplicated files on the nodes with original files or other replicas are not allowed. This
limitation is strictly fulfilled during all operations. The main purpose of the crossover operation is to
mix parts of genes of different chromosomes in order to accelerate the optimal solution search and to
escape local extremum wells. In the core of crossover implementation gravitation principles are used
at all steps of process. Firstly, two chromosomes are chosen probabilistically according to their gravity
in the population. At the second step, the nodes of two chosen chromosomes are analyzed by final
stochastic selection of those that have the highest gravity. At the next step type of crossover
operations “work - free”, “work - idle”, and “work - work” are picked randomly. Here “work” means
replicas of task files, held by agents. “Idle” means files that are not used in the tasks, but also stored on
the nodes, and "free" means free space. Taking into account categories, the replacement stage
performes selected crossover operations. After file replacement a chromosome normalizing process
(removing duplicate, adding missing files) is executed. The crossover pseudo-code is shown in Listing
2.
Listing 2: Pseudo-code of CGA crossover implementation.
1: PROCEDURE: Crossover
2: Input: Population P, NumberOfCrossovers Num
3: begin
4: while Num is not satisfied do
5:
bestChromosome1← Randomly select from P using GC
6:
bestChromosome2← Randomly select from P using GC
7:
node1 ← Randomly select from bestChromosome1 using GN
8:
node2 ← Randomly select from bestChromosome2 using GN
9:
chosing crossover type
10
Swap work files between node1 and node2 according to type and categories
11: Normalize nodes
13: Create next population using Tournament selector
14: end

4.1.2. Mutation
Mutation in genetic algorithm is a minor exchange of two chromosome by their elements (usually
gens). A combination of category and storage node is considered to be an element in our schema. The
selection of chromosome elements depends on their gravity values that form a choosing probability
during the mutation process. An higher probability of being selected corresponds to lower gravity
value. After the first element is chosen, the second element is selected in three steps. During the first
step, the targeted storage node is found according to the summarized gravity value also. The storage
node with higher gravity has higher probability of being selected. At the second step is the decision of

2362

Evolutionary replicative data reorganization with prioritization...

Denis Nasonov et al.

which type of files on mutated storage node will be performed for exchange. The probability of this
choice is configurable. During the third step, categories optimization is performed.
Listing 3: Pseudo-code of CGA mutation implementation.
1: PROCEDURE: Mutation Categories
2: Input: Chromosome F, Categories C, Tasklist T, Nodes A,
Category per Agent CA, FileType Tp
3: beg in
4: for ca in CA do
5:
Calculate G(ca)
7: Reverse normalize all g in Gca
8: Random choice ca from CA with Gca probability
9: Get tasks in Tasklist where ca in t
10: for a in A do
11: for ca in a do
12: if ca in tasks then
13: Append Gca value Ga
14: Normalize all g in Ga
15: Random choice a in A with Ga probability

16:
17:
18:
19:
20:
21:
22:
23:
24:
25:
26:
27:
28:
29:

Random choice tp in Tp with Tp probability
if ca< free space then
Exchange ca and free space
if ca< waste space then
Exchange ca and waste space
for aca in a do
if ca<=aca then
Exchange ca and aca
for aca in a do
if ca<=set(aca) then
Exchange ca and set(aca)
else
set(asa)+=aca
end

Categories with equal or larger size are searched. If it fails, two and more categories are searched
to fulfill the condition. A pseudo-code of mutation is presented in listing 3.

4.1.3. Fitness
The fitness function is the core of the evolution mechanism in genetic algorithm. The GA selector
uses the fitness function to make a decision about the efficiency of the solution. The implemented
fitness function has two parts, which build the common rank of solution: (a) file distribution
effectiveness according to task statistics and (b) reconfiguration cost (network structure consideration).
File distribution. The conditions which are taken into account for part (a) in the rank calculation
are the following:
x the better solution is the fewer nodes with original files for each task it uses;
x the better solution is fewer nodes with replica files for each task it uses;
x the better solution is less overhead of data transferring between nodes per task execution
performed.
Files fitness function is divides into two stages. The first stage operates with original files while
the second one operates with replica files. In the beginning, the fitness function finds out the minimum
amount of nodes that store files needed for the particular task. It recursively scans all nodes
combinations that satisfy the requirement of full task file coverage. The variant with a minimum
amount of used nodes is selected from the formed set of nodes combination. The numeric fitness value
is calculated as the significance coefficient of the task divided by the amount of nodes chosen
previously.
The second stage tries to find another combination of nodes that differ from previously the selected
one for replica file task processing. It uses the same method with ignorance of the previously found
node for the same files. The special delta value defines the level in the node processing hierarchy,
which shows how much the processing task on replica files has to wait until it starts its execution, if
another same task is started simultaneously on the original files. The better solution minimizes delta
value to get no intersections between nodes with original and replica files.
The first fitness part is calculated according to the formula (2):

‫ܨ‬௜ ൌ ሺே

்೔೗೚ೌ೏

೔ೌ ୪୭୥మ ே೔ೌ ሻା஽೔

,

(2)

2363

Evolutionary replicative data reorganization with prioritization...

Denis Nasonov et al.

where ‫ܨ‬௜ is a value of the fitness function of second stage for i task, ܶ௜௟௢௔ௗ is the significance of i
task. ܰ௜௔ is amount of minimum nodes needed to processing i task and ‫ܦ‬௜ defines delta. A Pseudocode of fitness part is presented in listing 5.
Network consideration. The second part of the fitness function is based on network connection
configuration between nodes. It enables making rank by the cost of data transfer between nodes in the
case of changing current file distribution to the new generated one. The network fitness part operates
with a number of files which should be transferred between nodes, and its cost depending on network
configuration. The cost of transfer between all nodes is calculated by Dijkstra algorithm once, and it is
used in fitness on all GA iterations. The difference between each ranking solution and initial solution
is presented as a matrix of transfers between nodes. Then the cost of this solution is calculated using
the following formula:
்௥ೕ ‫כ‬஼ೕ

‫ܨ‬௡௘௧ǡ௜ ൌ ܰ‫ ܧ‬ቀͳ െ σ௡௝ୀ଴ ி௟

ೌ ‫כ‬஼೘ೌೣ

ቁ

ሺ͵ሻ

where ‫݈ܨ‬௔ is the amount of all files of all tasks, ‫ܥ‬௠௔௫ is the cost of path in network which has
maximum value, ܶ‫ݎ‬௝ is number of transfer files for j node of all transfers and ‫ܥ‬௝ - its cost, n is the
number of all countable transfers between nodes. ܰ‫ ܧ‬is the coefficient of the impact of network
configuration and data transfer cost on overall efficiency of the solution.
The final fitness function is calculated by accounting for both parts as ‫ܨ‬௡௘௧ǡ௜ as ‫ܨ‬௜ .
Prioritization In urgent computing there is a strong need for performance maximization of the
particular task. In our algorithm, this can be achieved by forced valuable increase of the significant
task coefficient, which automatically makes other task inconspicuous and provides a high guarantee
that the selected task will be served firstly within the best conditions.

5 Experimental study
For the experimental studies we have taken a third scenario from the work of (Razumovskiy, 2014)
as the most complex and representative case. It is built on information gathered from existing
hydrometeorological applications studies.
In figure 2, the results for the case when original files can be moved and not between nodes are
shown. It can be noted that GA gets approximately 0.82 fitness value while CGA at the same time
gained nearly 0.96 on 250 iterations when original files cannot be moved. In other words CGA
overcomes GA by 17%. For the situation with moving right CGA gets 1.5 points that provides twice
improvement. The sinuosity behavior (lines go up and down) of the lines on the all figures is
explained by different iterations count on each run used in aggregated results.
.

Original files fixed

Original files moved

Figure 2. CGA and GA average results (aggregated from 100 runs for each configuration) with
allowed (on the right) and not allowed (on the left) original file moving.

2364

Evolutionary replicative data reorganization with prioritization...

Denis Nasonov et al.

On the left plot of figure 4 results for different CGA modifications wCGA and wfCGA are
presented. in general, as it is shown, there is no significant difference between them in randomly used
configurations.

StarNet
wCGA, wfCGA
Figure 3. CGA and GA results for configuration with allowed only replica file moving.
In order to understand network configurations’ influence on solutions, we made a simulation of
fully-meshed, star and ring network structures. Also we consider a bandwidth between nodes. On the
right part of figure 3 the star structure was investigated for changing influence in the range [ 0.1; 1].
The whole fitness (red line) is changing almost linearly from 0,87 to 2.67 while ‫ܨ‬௡௘௧ǡ௜ influence
increasing(blue line * impact), a curious situation happens with ‫ܨ‬௜ (green line) that almost has no
effect from ‫ܨ‬௡௘௧ǡ௜ rising, it is changing from minimum value 0,78 to 0.88 maximum value.

Figure 4. CGA with urgent optimization: at the left single run, at the right average run evaluated
according to statistical analysis .
On figure 4 results for urgent cases are presented. For experimental study, we increased the
significance of the I task from 0.17 to 1.7 points. The urgent plan estimation is represented by a dark
green line, for non-urgent case a green line is used, while a red line shows how file distribution is
changing in an urgent case without increased significance of the I task. As it was expected, the file of I
task was organized in the most optimal way on two separated storage nodes (the second used by
replica), and the green line goes higher than the red one, whichclearly shows how urgency influences
global optimization of file distribution.

6 Conclusion
As it can be observed from the experimental results of the proposed algorithms, there is a clear
understanding that metaheuristics overcome heuristic approaches, and that categorical GA

2365

Evolutionary replicative data reorganization with prioritization...

Denis Nasonov et al.

significantly improves original GA. So CGA grows up to 1.5 points while the best Greedy
modification stops at 0.75. Also it was shown, CGA modifications have no crucial difference in their
generated results, as well as network structure can change global solution within 10% barrier. On the
other hand, computations with prioritization for urgent task were also shown. However, there are great
opportunities for future investigations in this research field, especially in feature detailed analysis and
dynamic optimization problem.
This paper is financially supported by Ministry of Education and Science of the Russian
Federation, agreement #14.578.21.0077 (24.11.2014).

References
Back, T. (1996). Evolutionary algorithms in theory and practice. Oxford: Oxford Univ. Press.
Çatalyürek, Ü. V. (2011). Integrated data placement and task assignment for scientific workflows
in clouds. . In Proceedings of the fourth international workshop on Data-intensive distributed
computing. ACM , 45-54.
Dean, J. &. (2008). MapReduce: simplified data processing on large clusters. Communications of
the ACM , 107-113.
Er-Dun, Z. X.-X. (2012 ). A data placement strategy based on genetic algorithm for scientific
workflows. Eighth International Conference on Computational Intelligence and Security (CIS) (pp.
146-149). IEEE.
Gantz, J. &. (2012). The digital universe in 2020: Big data, bigger digital shadows, and biggest
growth in the far east. IDC iView: IDC Analyze the Future , 1-16.
Jin, H. Y. (2012 ). Adapt: Availability-aware mapreduce data placement for non-dedicated
distributed computing. In Distributed Computing Systems (ICDCS), IEEE 32nd International
Conference on , 516-525.
Kumar, K. A. (2013). Data placement and replica selection for improving co-location in distributed
environments. Energy , 1500.
Manyika, J. C. (2011). Big data: The next frontier for innovation, competition, and productivity.
Meng, B. P. (2011). Ultrafast and scalable cone-beam CT reconstruction using MapReduce in a
cloud computing environment. Medical physics , pp. 6603-6609.
P. Shang, Q. X. (2012). DRAW: A New DatagRouping-AWare Data Placement Scheme for Data
Intensive Applications with Interest Locality. Digest APMRC , 1-8.
Poonthottam, V. P. (2013). A Dynamic Data Placement Scheme for Hadoop Using Real-time
Access Patterns. In Advances in Computing, Communications and Informatics (ICACCI) (pp. 225229). IEEE.
Razumovskiy, A. N. (2014). Evolutionary Data Reorganization For Efficient Workload
Processing. IEEE. 8th International Conference on Application of Information and Communication
Technologies (pp. 234- 239). Astana: IEEE.
Sagynov, E. (2012, 04 10). Commercial and Open Source Big Data Platforms Comparison.
Retrieved from DZone: http://architects.dzone.com/articles/commercial-and-open-source-big
Shivarkar, S. A. (2014, June). Speed-up Extension to Hadoop System. International Journal of
Engineering Trends and Technology (IJETT) .
Shorfuzzaman, M. G. (2010). Distributed popularity based replica placement in data grid
environments. Parallel and Distributed Computing, Applications and Technologies (PDCAT) (pp. 6677). IEEE.
Wang, L. T. (2013). G-Hadoop: MapReduce across distributed data centers for data-intensive
computing. Future Generation Computer Systems , 739-750.
Xu, F. L. ( 2014). Boosting MapReduce with Network-Aware Task Assignment. Cloud
Computing. Springer International Publishing , 79-89.

2366

