Procedia Computer Science
Volume 51, 2015, Pages 1363–1372
ICCS 2015 International Conference On Computational Science

Glprof: A Gprof inspired, Callgraph-oriented Per-Object
Disseminating Memory Access Multi-Cache Proﬁler∗
Tomislav Janjusic and Christos Kartsaklis
Oak Ridge National Laboratory, Oak Ridge, TN, U.S.A
{janjusict, kartsaklisc} @ ornl.gov

Abstract
Application analysis is facilitated through a number of program proﬁling tools. The tools vary
in their complexity, ease of deployment, design, and proﬁling detail. Speciﬁcally, understanding, analyzing, and optimizing is of particular importance for scientiﬁc applications where minor
changes in code paths and data-structure layout can have profound eﬀects. Understanding how
intricate data-structures are accessed and how a given memory system responds is a complex
task. In this paper we describe a trace proﬁling tool, Glprof, speciﬁcally aimed to lessen the
burden of the programmer to pin-point heavily involved data-structures during an application’s
run-time, and understand data-structure run-time usage. Moreover, we showcase the tool’s
modularity using additional cache simulation components. We elaborate on the tool’s design,
and features. Finally we demonstrate the application of our tool in the context of Spec benchmarks using the Glprof proﬁler and two concurrently running cache simulators, PPC440 and
AMD Interlagos.
Keywords: Application tracing, Benchmarking, Memory proﬁling, Data-structure analysis

1

Introduction

Application developers often rely on software analysis tools to help them understand, explore,
debug, or otherwise improve the performance of software applications. Tools overlap in their
scope and capabilities and sometimes supplement each other utilizing existing information to
present new and improved proﬁles. Scientiﬁc applications are known to be immensily complex
and take a signiﬁcant eﬀort to understand and analyze [1]. The complexity of application datastructure design typically follows out of accepted software-design principles to keep code in a
∗ This manuscript has been authored by UT-Battelle, LLC under Contract No. DE-AC05-00OR22725 with the
U.S. Department of Energy. The United States Government retains and the publisher, by accepting the article
for publication, acknowledges that the United States Government retains a non-exclusive, paid-up, irrevocable,
world-wide license to publish or reproduce the published form of this manuscript, or allow others to do so, for
United States Government purposes. The Department of Energy will provide public access to these results of
federally sponsored research in accordance with the DOE Public Access Plan (http://energy.gov/downloads/doepublicaccessplan).

Selection and peer-review under responsibility of the Scientiﬁc Programme Committee of ICCS 2015
c The Authors. Published by Elsevier B.V.

doi:10.1016/j.procs.2015.05.324

1363

Glprof: A Gprof inspired, Per-Object Memory Access Multi-Cache Proﬁler

Janjusic and Kartsaklis

structured, logical, and easy to follow way. However, scientiﬁc applications are usually many
years in production, and with the ever increasing number of modules, and, by implication,
data-structure additions, novice as well as veteran developers spend a signiﬁcant time eﬀort
understanding application’s internal design. This process is particularly burdensome when one
must exploit hardware design features to improve overall eﬃciency, increase memory localities,
and reduce the memory footprint.
Several recent eﬀorts revolved particularly around exploring new frameworks and environments to facilitate code exploration and analysis tools. In [2] Wang et al. discuss their practice
and experience in order to understand complex codes features of a well developed climate code
base.
In this paper we describe a trace analysis tool, Glprof, speciﬁcally aimed to lessen the
programmer’s burden to pin-point heavily used data-structures during an application’s runtime. Glprof was inspired by the well-known Gprof tool [3]. Gprof attributes a program’s
execution proﬁle in an eﬃcient and logical way easily read by the user. The simplicity and
easy of use makes Gprof particularly interesting because, since its debut in 1982, Gprof is still
very useful today. Similarly, with Glprof we aime to bring forth the same simplicity focusing
on memory trace analysis. For trace generation we use our in-house built tool Gleipnir [4].
Gleipnir is a memory tracing tool built around the Valgrind [5] instrumentation framework.
Trace analyis is external to the tool and typically supplemented through cache simulators
and various trace parsers. Though Gleipnir can provide some basic information about the
application’s runtime behavior the user must further analyze Gleipnir’s traces to get a more
meaningful picture. For example, during instrumentation the tool can gather various instruction
and process related information such as: the total number of instructions executed, the total
number of data reads, writes, or modiﬁes, the total number of function entries, average stack
size, total number of dynamically allocated blocks ,average memory usage, etc. It is important
to note that Gleipnir’s traces are unique in that each memory transaction is mapped to the
application’s internal data-structures. This information allows 3rd party analyzers to break
down memory related transactions per program object, a key component of Glprof. This paper
makes the following contributions:
1. We elaborate on the importance of tools and environment for application code optimization exploration.
2. We introduce a new memory-centric memory trace based tool, Glprof
3. We show through complex trace examples the importance of Glprof and its ability to
breakdown complex data-structure behavior into simple and intuitive summaries.
The rest of the paper is organized as follows. In Section 2 we describe some related work
in the area of program instrumentation and program proﬁling. In Section 3 we elaborate on
Glprof and discuss the implementation. We also describe the tracing tool and highlight a few
key capabilities which set it apart from other tracing tools. In Section 4 we show some usage
examples by tracing several larger Spec applications. In Section 5 we talk about the future
work and oﬀer our conclusions.

2

Related Work

There are several uses for application proﬁling, and each proﬁle may require diﬀerent information from the proﬁled application. Depending on implementation and scope, tools can be
1364

Glprof: A Gprof inspired, Per-Object Memory Access Multi-Cache Proﬁler

Janjusic and Kartsaklis

categorized into several broad categories relative to their information gathering methodology.
There are instrumentation based tools, simulators, and hardware performance counter based
tools.
Instrumentation based tools range from compiler driven static analysis [6] to fully dynamic
instrumentation tools (eg. PinTool [7]). Many tools which perform some type of analysis
will likely be based on an instrumentation framework. There are several frameworks available
and the ones most often cited are: PinTool [7], DynInst [8], and Valgrind [5]. The Gleipnir
tracing tool [4], as well as the work in [9], is based on the Valgrind framework which comes
with a set of proﬁlers and correctness tools, Cachegrind, Callgrind, and Memcheck to name a
few. Cachegrind and Callgrind are similar to Glprof in that they provide overall cache-metrics;
however, their representation is a code-centric view, i.e. they break down cache metrics per
function and function code-line.
Simulators, speciﬁcally hardware simulators, are a set of software tools typically used to
test new architectural feature, but similarly they can be used to test system and application
behavior under various run-time scenarios [10] [11]. Modern CPUs come with a number of
hardware performance counters available to a set of tools which invoke them to gather hardware
related events. The events are thus collected at interesting code sections. These may be used to
sandbox a code section, such as loops or frequently invoked routines etc. Hardware performance
counters enable tools and users to proﬁle application behavior in a near native run-time setting.
Performance counters are an attractive alternative for proﬁling tools due to the low overhead
but they usually lack the proﬁling detail typically available through binary instrumentation.
Examples of tools which rely on hardware performance counters are TAU[12], HPCToolkit[13],
OpenSpeedShop[14], etc. A common tool used for proﬁling HPC type MPI applications is
VampirTrace [15], similarly there are other tracing tools which speciﬁcaly aim to proﬁle focused
application metrics, such as its memory usage [16].
While instrumentation tools, particularly binary instrumentation tools, incur a heavy runtime overhead they compensate in terms of accuracy. More recent binary instrumentation tools
speciﬁcally aim to reduce the heavy overhead [17]. There are several frameworks that enable the
building of tracing tools similar to Gleipnir. Their runtime overhead, reported in [7, 5], for basic
block counting tools is 8× for Valgrind, 2.5× for Pin, and 5.1× for DynamoRio; however, in our
experience Valgrind’s overhead is lower and oﬀset by the run-once-multiple-measure method.
This means that using Gleipnir we can obtain an application’s trace and use analysis tools for
performance analysis and portability planning for future systems.

3

Glprof

Inspired by the well known Gprof call-graph proﬁling tool, Glprof, aims to be the memory and
data-structure trace proﬁling counterpart. The availability of detailed instrumentation based
memory traces are a crucial component of Glprof. The traces are derived using Gleipnir, our
in-house built tracing tool developed as a Valgrind plug-in [4] [18]. Gleipnir uses Valgrind’s
framework to collect a stream of memory transactions mapped to application’s data-structures
or memory regions. A memory region may fall within several categories: stack, memory mapped
(mmap()), dynamic, or global. The tracing granularity of collected memory traces can vary
from selectively choosing speciﬁc sections to trace, or tracking memory events related to speciﬁc
data-structures. Gleipnir records memory regions by intercepting memory allocation or memory
map calls, such as malloc(), calloc(), mmap(), using user speciﬁed annotations, or using a
debug-parser to retrieve the debug-symbol table. Note that a user may arbitrarily record a
custom memory region and intercept its memory transactions, all the user must provide is a
1365

Glprof: A Gprof inspired, Per-Object Memory Access Multi-Cache Proﬁler

Janjusic and Kartsaklis

starting address and the size in bytes. Thereafter, every memory access which falls within a
speciﬁed region will be intercepted and annotated. Collecting detailed memory traces, i.e. the
ability to map memory events to internal data-structures, is crucial if we aim to evaluate an
application’s memory related events. Gleipnir’s ability to relate internal memory transactions
enables Glprof’s data-structure centric proﬁles.

3.1

Proﬁle Metrics

Our initial set of metrics consists of memory related instructions grouped per function and its
data-structures. We aim to make Glprof customizible to suit individuals users. Our aim with
Glprof is to not merely bring these metrics to the user’s attention, but to present them in an
intuitive way which immediately captures a descriptive picture of an application’s data-structure
usage. Virtually all memory related optimizations revolve around designing an application
memory layout such that it exploits the target architecture memory design. For this purpose
we have also included two cache simulator components. The goal of cache components is not to
compete with tools that utilize hardware performance counters, but to present application datastructures and their layout behavior in a descriptive and intuitive way to the user. Currently we
have two cache components: a PowerPC440 L1 cache, and an AMD Interlagos L1 cache. Our
current implementation’s metrics are outlined in Table 1. Glprof adopts a design that makes it
particularly extensible. Memory proﬁle tool developers can add additional statistics-gathering
placeholders and implement their own functions to handle new sources of performance events.
Glprof employs a stack-trace structure that is appropriate for disseminating events to what
comprises the ﬁnal proﬁle.
Metric
LS COUNT
LS PRC
ACCESS{T,M,H}
PRC{T,M,H}

Metric description
Number of Load / Store / Modify instructions.
Number of Load / Store / Modify instructions as percent of total.
Number of total references, hits, and misses.
Percent of total hits and misses per data-structure.

Table 1: Glprof’s currently supported metrics.

3.2

Data Gathering Methodology

Glprof has two main components: the parser and the proﬁler. The parser reads records from
the trace-ﬁle. A record stores a single memory transaction, which includes an instruction access
type, virtual address, access size, thread id, and segment scope. A segment scope indicates the
type of the accessed memory region (eg. (S)tack, (H)eap, (M)map, (G)lobal, or (C)ode). Where
appropriate, a record will hold debug information such as a function name, data-structure instance, data-structure’s name, data-structure’s access oﬀset. Unless a memory transaction concerns a tracked memory region, only the function name will be present. For proﬁling purposes
the most important ﬁelds are the access type, function name, and data-structure’s name. The
proﬁler’s main function is to collect record’s access types and map data-structures to the accessing functions. It also keeps the parent-child relations (caller-callee pairs) which are compiled
during proﬁling based on the trace. We will explain the algorithm in the subsequent section.
In addition to establishing a caller-callee relationship, the proﬁler collects a list of visited datastructures for each function. Tracked memory regions can be grouped into a logical region. A
logical region is a collection of allocated data-structures treated as a unit, for instance a tree,
1366

Glprof: A Gprof inspired, Per-Object Memory Access Multi-Cache Proﬁler

Janjusic and Kartsaklis

linked-list, etc. Conceptually it strongly resembles a memory pool. An excessive number of
memory allocations is challenging to the design of a proﬁler. Our solution is thus, to utilize
the logical grouping as an information reduction mechanism. Note that annotating a logical
unit can be left to the user during tracing by simply invoking a Gleipnir interface call. This,
we believe, is intuitive for users as it allows them to personalize proﬁles by utilizing structure
names unique to their codes.

3.3

Callgraph

Glprof can be instrumented by the user to dump the stack-trace at run-time (i.e. to the
console). This assists debugging of Glprof and ultra ﬁne break-down of memory transactions.
Because generated traces often consume large amounts of memory, easily reaching into tens
of gigabytes, these dumps are disabled by default. The callgraph generation is crucial when
constructing a parent-child function relationship. Gleipnir traces memory transaction only, as
such, instructions that concern function calls (eg. CALL and RET) are absent. Therefore
Glprof has to derive calls by observing stack pointer changes. If the current stack top function
does not match the current record this implies that either the function returned or a new
function was called. We can easily determine the case by comparing the stack’s address. A
lower address implies a function call, a higher address implies a function return. This makes
Glprof architecture independent because it does not rely on architecture speciﬁc CALL and
RET instructions, simply observing the current stack top address as well as the associated
function name will suﬃce. Moreover, this mechanisms allows the tool to naturally handle
recursive calls because only a change in the stack top address and associated function name
indicates a jump outside the function. The challenge in determining the appropriate stack-trace
using only memory traces stems from Gleipnir’s feature to arbitrarily stop and resume tracing.
Thus we may have to restart our proﬁling and ﬂush an outstanding stack every time we start
or stop tracing. This is handled using Gleipnir’s key-word instructions. They are annotated
as an X and mostly omitted by the proﬁler. If the trace ends before the stack is empty the
remaining stack instructions are accumulated and ﬂushed.

3.4

Proﬁle

Glprof’s proﬁle is customizible, but the default statistics show the various memory load, store,
and modify transactions grouped per function and its data-structures. We believe that a ﬂatproﬁle, similar to Gprof, is an intuitive way to represent overall function related behavior.
Figure 1 shows an example Glprof ﬂat-proﬁle.1 Notice the grandparent(gp) − parent (p) −
child (c) relationship. Every function is broken down into its parent-child relationship but we
also list every parent’s grand-parent. When reporting total the instruction metrics are inclusive,
meaning, that the number of memory transaction in the children are propagated to the parent.
When reporting percentages the instruction metrics are exclusive, signifying the parents self
contribution to the total. This makes it easy to observe if any data-structures touched in
function X, originating from accesses lower in the call-chain, i.e. its children.
In Figure 1 columns that cite inclusive ﬁgures are the Total Rd , Total Wr, and Total Mod.
On the other hand columns that cite exclusive or self-ﬁgures are Rd %, Wr %, and Mod %. In
our example main is the originating function that called three other functions, foo, boo, and
goo, The breakdown is shown at the bottom of the proﬁle. On the right hand side we can
observe the function−child relationship. Symbol (p) signiﬁes a parent, trailing symbols (s) are
1 Cache

statistics have been omitted from this example.

1367

Glprof: A Gprof inspired, Per-Object Memory Access Multi-Cache Proﬁler

Janjusic and Kartsaklis

its objects. The parent’s children are annotated with the symbol (c). We can observe a total of
62 load, 28 store, and 9 modify instructions originating from main; however, it is intuitive to
see that main does not contribute to the majority of these accesses. This is expected due to the
nature of the program, however, in larger examples and programs, this information is not easy
to observe. According to Figure 1 in column (Rd %) we see that main is responsible for only
30% loads, in column (Wr %) 54% stores, and in column (Mod %) 33% modify instructions.
The inclusive/exclusive break-down helps us picture the overall behavior quickly and narrow
down functions and data-structures with most accesses. Note that these are small examples,
we will show larger examples in the following section.
Trace profile :
Tot Rd
( Rd %)
-------------

Tot Wr
-------

( Wr %)
-------

Tot Mod
-------

( Mod %)
-------

24
9
13

1.00
1.00
1.00

6
4
1

1.00
1.00
1.00

3
0
3

1.00
0.00
1.00

FNNAMES
------( gp ) main ,
( p ) boo
s ) array
s) i

18
6
0
10

1.00
1.00
0.00
1.00

6
1
3
1

1.00
1.00
1.00
1.00

3
0
0
3

1.00
0.00
0.00
1.00

( gp ) main ,
( p ) foo
s ) array
s ) array_f
s) i

2

1.00

1

1.00

0

0.00

( gp ) main ,
( p ) goo

62
0
0
20
0
33
24
9
13
18
6
0
10
2

0.29
0.00
0.00
0.25
0.00
0.30
1.00
1.00
1.00
1.00
1.00
0.00
1.00
1.00

28
6
1
5
3
3
6
4
1
6
1
3
1
1

0.54
1.00
1.00
0.00
0.00
0.33
1.00
1.00
1.00
1.00
1.00
1.00
1.00
1.00

9
0
0
0
0
9
3
0
3
3
0
0
3
0

0.33
0.00
0.00
0.00
0.00
0.33
1.00
0.00
1.00
1.00
0.00
0.00
1.00
0.00

( gp )
( p ) main
s ) _zzq_args
s ) _zzq_result
s ) array
s ) array_f
s) i
( c ) boo
s ) array
s) i
( c ) foo
s ) array
s ) array_f
s) i
( c ) goo

Figure 1: Example 1, proﬁle (t1.c)

4

Proﬁling Examples

Our examples in Section 3 are for illustration purposes. In order to show Glprof’s detailed
ﬂat-proﬁle we have traced applications from the SPEC2006 [19] benchmark suite. Notice that
chosen applications represent a sizable amount of information and tracing and proﬁling them
can result in large traces even for their smallest, test, input sizes. In subsequent Figures we
will demonstrate output snippets from GCC, HMMER, and Gromacs applications. The GCC
application is the integer benchmark inplemented in C based on GCC ver. 3.2., HMMER is a
gene sequencing benchmark which uses a proﬁle hidden markov model to search for patterns in
DNA sequences, Gromacs, performs molecular dynamics computations. We have selected these
benchrmark because we wanted to stress-test Glprof by using Gleipnir traces as examples of
complex data-structures and multiple function call-chains. The generated traces resulted large
1368

Glprof: A Gprof inspired, Per-Object Memory Access Multi-Cache Proﬁler

Janjusic and Kartsaklis

(> 2GB) trace ﬁles with deep (10-50) function call-chains which represents a sound base for
testing Glprof.

4.1

Flat-proﬁles

Figure 2 and 3 show a ﬂat-proﬁle snippet from the GCC and Gromacs benchmark, respectively.
GCC resulted in 1,405 individual functions. The largest function yyparse 1 was responsible for
nearly 2 × 108 memory transactions, distributed across 100+ structures. Gromacs had fewer
function calls and most memory transactions were attributed to a small function subset. The
beneﬁt of Glprof’s ﬂat-proﬁle output is that it makes observing a function’s proﬁle straightforward, moreover it makes it very easy to pin-point the function’s major reference contributors
(eg. var c 41 in Figure 2). Note the change in the naming convention for data-structure names
(eg. var c 41, hashtable c 92, etc.). When tracing large applications it becomes cumbersome
to annotate every allocated data-structure individually. Because these are dynamically allocated data they do not carry any debug information other than the ﬁle-name and line-number
where they originated from. However, due to software engineering practices we can observe an
allocation pattern. Meaning that data-structures which belong to a logical unit or are used
during a particular algorithm are allocated from the same ﬁle. Glprof can take advantage and
group such allocations into logical or abstract data-types. In Figure 3 we can observe that
Gromacs had a particular tendency to allocate data structures from a few ﬁles whose memory
blocks were prone to receiving accesses from the same subroutines. This implies with higher
probability that data-structures allocated from the same ﬁles are likely to constitute the same
logical group.
Trace profile :
Tot Rd
( Rd %)
-------------

Tot Wr
-------

( Wr %)
-------

Tot Mod
-------

( Mod %)
-------

142066972
4136619
4765
0
61
8
290415
142166
142057638
4136385
4765
142166

0.00
0.00
0.00
0.00
0.00
0.00
0.00
0.00
0.01
0.00
0.00
0.00

49489845
1943275
2920
972
0
8
1651798
16007
49485704
1942967
2920
16007

0.00
0.00
0.00
0.00
0.00
0.00
0.00
0.00
0.00
0.00
0.00
0.00

7211202
0
0
0
0
0
0
0
7210648
0
0
0

0.00
0.00
0.00
0.00
0.00
0.00
0.00
0.00
0.00
0.00
0.00
0.00

FNNAMES
------( gp ) compile_file ,
( p ) yyparse
s ) _sysres_
s ) alias_c_2468
s ) alias_c_2471
s ) varasm_c_3465
s ) varasm_c_390
s ) varray_c_41
s ) varray_c_65
( c ) yyparse_1
s ) _sysres_
s ) alias_c_2468
s ) varray_c_65

142057638
4136385
48
2730
0
245

0.01
0.00
0.00
0.03
0.00
0.00

49485704
1942967
0
1575
70
105

0.00
0.00
0.00
0.04
0.00
0.00

7210648
0
0
0
0
0

0.00
0.00
0.00
0.00
0.00
0.00

( gp ) yyparse ,
( p ) yyparse_1
s ) _sysres_
s ) ggc - page_c_462
(c) build_break_stmt
s ) ggc - page_c_655
s ) ggc - page_c_713

Figure 2: GCC ﬂat proﬁle (cropped).

4.2

Cache proﬁles

While Figures 2 and 3 only showed memory transaction related information we can also output
memory transaction related information with accompanying cache statistics. Provided that we
1369

Glprof: A Gprof inspired, Per-Object Memory Access Multi-Cache Proﬁler

Trace profile :
Tot Rd
( Rd %)
------------81953081
614824
2377563
21996
21996
43145
2377563
64
605925
10911
11459647
275480
418842
5622
5622
1885
418842
139614
3748
137740
1874
1874
1874
1874
14906799
15964664
39217796

0.00
0.00
0.00
0.00
0.00
0.82
0.00
1.00
0.00
0.00
1.00
1.00
1.00
1.00
1.00
1.00
1.00
1.00
1.00
1.00
1.00
1.00
1.00
1.00
1.00
1.00
1.00

Tot Wr
------29518889
0
2377563
21996
0
0
0
0
0
10911
3490438
0
418842
5622
0
0
0
0
0
0
0
0
0
1874
5101269
4972920
15638964

( Wr %)
------0.01
0.00
0.00
0.00
0.00
0.00
0.00
0.00
0.00
0.00
1.00
0.00
1.00
1.00
0.00
0.00
0.00
0.00
0.00
0.00
0.00
0.00
0.00
1.00
1.00
1.00
1.00

Tot Mod
------630742
0
0
0
0
0
0
0
0
0
139614
0
0
0
0
0
0
0
0
0
0
0
0
0
212470
171541
89632

( Mod %)
------0.03
0.00
0.00
0.00
0.00
0.00
0.00
0.00
0.00
0.00
1.00
0.00
0.00
0.00
0.00
0.00
0.00
0.00
0.00
0.00
0.00
0.00
0.00
0.00
1.00
1.00
1.00

Janjusic and Kartsaklis

FNNAMES
------( gp ) do_longrange ,
( p ) do_fnbf
s ) force_c_112
s ) force_c_635
s ) force_c_636
s ) force_c_655
s ) force_c_68
s ) init_c_113
s ) mdatom_c_50
s ) ns_c_366
s ) tgroup_c_102
( c ) inl0100_
s ) force_c_112
s ) force_c_635
s ) force_c_636
s ) force_c_655
s ) force_c_68
s ) init_c_113
s ) mdatom_c_60
s ) ns_c_100
s ) ns_c_366
s ) ns_c_97
s ) ns_c_98
s ) ns_c_99
s ) tgroup_c_102
( c ) inl1000_
( c ) inl1100_
( c ) inl1130_

Figure 3: GROMACS ﬂat proﬁle (cropped)

enable multiple cache conﬁgurations Glprof will output all statistics side by side for comparison.
The additional information includes the total number of access references, total number of hits
and total number misses. Similarly to previous ﬁgures the information is displayed in an
inclusive/exclusive format. Our aim in using a multi-cache ﬂat-proﬁle output is to two-fold.
First we want to break down an application’s cache behavior in an easy to understand and
intuitive way. Second, by enabling multiple side by side cache proﬁles gives the user an insight
into the potential hazards when porting codes to diﬀerent architectures. The example in Figure
4 shows the HMMER benchmark’s cache ﬂat-proﬁle. HMMER also resulted in the smallest
number of function invocations, only 15, thus making it a good candidate to demonstrate Glprof.
A ﬂat-proﬁle is ideal to observe which functions attribute the most accesses, in this case they
are P7Viterbi responsible for 48 × 106 accesses (Reads, Writes, and Modify instructions). The
next closest function is FChoose with ≈ 3 × 106 accesses. Due to space constraints we had to
rotate our tables; however, we felt that showing a side-by-side cache comparison is necessary
to depict the advantage of Glprof. In Figure 4 we show our two cache implementations as they
appear in the ﬁnal output. We can immediately observe the overall cache outcomes for a given
trace, for instance comparing raw data for functions P7Viterbi we see that Interlagos produces
almost half the miss count of PPC440, but then this observation is completely turned around
in the case of function FChoose where Interlagos produces almost 10 times more misses.
1370

Glprof: A Gprof inspired, Per-Object Memory Access Multi-Cache Proﬁler

Janjusic and Kartsaklis

Trace profile :
PPC440
Interlagos
----------------------------------------------------- ----------------------------------------------------------------Tot Ref ( Ref %) Tot Hit ( Hit %) Tot Mis ( Mis %) Tot Ref ( Ref %)
Tot Hit ( Hit %) Tot Mis ( Mis %)
FNNAMES
------- ------- ------- ------- ------- ------- ------- - ------------ ------- ------- ------------( gp ) main_loop_serial ,
665940
1.00
665751
0.50
189
0.99
665940
1.00
665774
0.50
166 1.00 ( p ) D i g i t i z e S e q u e n c e
300
1.00
119
1.00
181
1.00
300
1.00
134
1.00
166 1.00
s ) alp habet_c_215
33052
1.00
33051
1.00
1
1.00
33052
1.00
33052
1.00
0 0.00
s) sre_string_c_199
330520
1.00
330518
1.00
2
1.00
330520
1.00
330520
1.00
0 0.00 ( c ) SymbolIndex
2800
1.00
2800
0.39
0
0.00
2800
1.00
2800
0.39
0 0.00 ( c ) sre_malloc

5415610
1782684

1.00
1.00

5414190
1781509

0.67
1.00

1420
1175

0.17
1.00

5415610
1782684

1.00
1.00

5402564
1773879

0.67
1.00

13046
8805

10061
9219

1.00
1.00

9609
8770

0.09
1.00

452
449

0.01
1.00

10061
9219

1.00
1.00

9553
8711

0.09
1.00

508
508

1.00 47808210
1.00
65401
1.00
660168
1.00
96823
1.00
63767
1.00
63791
1.00
643985
1.00 3641564
1.00 2226138
1.00 1832465
1.00
20838
1.00
457
1.00
31837
1.00
31847
1.00 2245419
1.00
325071
1.00
292463
1.00
330079
1.00
330086
1.00
695645
1.00
98401
1.00
4440
1.00
4440
1.00
4467
1.00
4457
1.00
20838
1.00
520

1.00
1.00
0.99
0.95
0.93
0.93
1.00
1.00
1.00
1.00
0.00
1.00
1.00
1.00
1.00
1.00
1.00
1.00
1.00
1.00
0.99
1.00
1.00
1.00
1.00
1.00
1.00

216438
703
8552
8511
8515
8491
19857
47504
47538
47553
166
243
1215
1205
2117
5449
5005
441
434
847
2861
638
638
611
621
166
80

0.99 48024648
1.00
66104
0.93
668720
0.93
105334
0.93
72282
0.93
72282
1.00
663842
1.00 3689068
1.00 2273676
1.00 1880018
0.00
21004
1.00
700
1.00
33052
1.00
33052
1.00 2247536
1.00
330520
1.00
297468
1.00
330520
1.00
330520
1.00
696492
0.99
101262
1.00
5078
1.00
5078
1.00
5078
1.00
5078
1.00
21004
1.00
600

1.00
1.00
1.00
1.00
1.00
1.00
1.00
1.00
1.00
1.00
1.00
1.00
1.00
1.00
1.00
1.00
1.00
1.00
1.00
1.00
1.00
1.00
1.00
1.00
1.00
1.00
1.00

47902570
65437
664037
100428
67504
67645
653056
3661151
2244397
1850693
20908
520
32913
32598
2246865
329558
296027
330370
330373
696151
99745
4757
4749
4752
4747
20908
511

1.00
1.00
0.99
0.95
0.93
0.93
1.00
1.00
1.00
1.00
0.00
1.00
1.00
1.00
1.00
1.00
1.00
1.00
1.00
1.00
0.99
1.00
1.00
1.00
1.00
1.00
1.00

122078
667
4683
4906
4778
4637
10786
27917
29279
29325
96
180
139
454
671
962
1441
150
147
341
1517
321
329
326
331
96
89

48024648
66104
668720
105334
72282
72282
663842
3689068
2273676
1880018
21004
700
33052
33052
2247536
330520
297468
330520
330520
696492
101262
5078
5078
5078
5078
21004
600

0.33
1.00

( gp ) RandomSequence ,
( p ) FChoose
( c ) sre_random

( gp ) main_loop_serial ,
0.00 ( p ) Gaussrandom
1.00 ( c ) sre_random

0.99
1.00
0.93
0.93
0.93
0.93
1.00
1.00
1.00
1.00
0.00
1.00
1.00
1.00
1.00
1.00
1.00
1.00
1.00
1.00
0.99
1.00
1.00
1.00
1.00
1.00
1.00

( gp ) main_loop_serial ,
( p ) P7Viterbi
s ) alphabet_c_215
s ) co re _a l go _c _ 13 3
s ) co r e_ al go _ c_ 13 4
s ) co r e_ al go _ c_ 13 5
s ) co r e_ al go _ c_ 13 6
s ) c or e_ al g o_ c_ 1 44
s ) c or e _a lg o _c _1 45
s ) c or e _a lg o _c _1 46
s ) c or e _a lg o _c _1 47
s ) cor e_algo_c_66
s ) plan7_c_123
s ) plan7_c_124
s ) plan7_c_125
s ) plan7_c_126
s ) plan7_c_127
s ) plan7_c_128
s ) plan7_c_160
s ) plan7_c_161
s ) plan7_c_54
(c) ResizePlan7Matrix
s ) co re _a l go _c _ 13 3
s ) co re _a l go _c _ 13 4
s ) co re _a l go _c _ 13 5
s ) co re _a l go _c _ 13 6
s ) cor e_algo_c_66
( c ) Scorify

Figure 4: HMMER, comparing two cache simulator proﬁlers.

5

Conclusions and Future Work

In this paper we presented a new proﬁling tool, called Glprof, that targets the ﬁne-grained
memory proﬁling domain. The purpose of the tool is to help users peek into their codes and
inspect how exactly data structures and functions contribute to the memory traﬃc and how
diﬀerent caches interpret this activity, performance-wise. The tool, reports information in a
Gprof-manner, adopting a self/inclusive summarization of memory and cache statistics in a
callgraph-oriented reporting style. Importantly, Glprof simulates multiple cache architectures
simultaneously in order to report, side-by-side, and with similar distributions as for the traﬃc,
the caches’ response to the accesses. This in turn allows the user to easily examine the suitability
of additional and often unseen cache systems. As we prepare to release Glprof we are looking
into incorporating additional cache models. Considering Glprof’s components are serialized, we
next want to explore the feasability of parallelizing Glprof’s invocation of individual simulators
since they operate independently. Finally, we want to explore the feasaiblity of using diﬀerent
proﬁling heuristics to speed up overall proﬁling performance.
1371

Glprof: A Gprof inspired, Per-Object Memory Access Multi-Cache Proﬁler

Janjusic and Kartsaklis

References
[1] Y. Zhang, J. Sun, Z. Tang, X. Chi, Memory Complexity in High Performance Computing, in:
Proceedings of the Third International Conference on High Performance Computing in Asia-Paciﬁc
Region, Singapore, 1998, pp. 142–151.
[2] D. Wang, J. Schuchart, T. Janjusic, F. Winkler, Y. Xu, C. Kartsaklis, Toward better understanding
of the community land model within the earth system modeling framework, Procedia Computer
Science 29 (0) (2014) 1515 – 1524, 2014 International Conference on Computational Science.
[3] S. L. Graham, P. B. Kessler, M. K. Mckusick, Gprof: A call graph execution proﬁler, in: Proceedings of the 1982 SIGPLAN Symposium on Compiler Construction, SIGPLAN ’82, ACM, New
York, NY, USA, 1982, pp. 120–126.
[4] T. Janjusic, K. M. Kavi, B. Potter, International conference on computational science, iccs 2011
gleipnir: A memory analysis tool, Procedia CS 4 (2011) 2058–2067.
[5] N. Nethercote, J. Seward, Valgrind: a framework for heavyweight dynamic binary instrumentation,
Vol. 42, ACM, New York, NY, USA, 2007, pp. 89–100.
[6] G. Chakrabarti, F. Chow, P. Llc, Structure layout optimizations in the open64 compiler: Design,
implementation and measurements, 2008.
[7] C.-K. Luk, R. Cohn, R. Muth, H. Patil, A. Klauser, G. Lowney, S. Wallace, V. J. Reddi, K. Hazelwood, Pin: Building customized program analysis tools with dynamic instrumentation, in: Proceedings of the 2005 ACM SIGPLAN Conference on Programming Language Design and Implementation, PLDI ’05, ACM, New York, NY, USA, 2005, pp. 190–200.
[8] B. Buck, J. K. Hollingsworth, An api for runtime code patching, Int. J. High Perform. Comput.
Appl. 14 (2000) 317–329.
[9] A. J. Pena, P. Balaji, A Framework for Tracking Memory Accesses in Scientiﬁc Applications, 43nd
International Conference on Parallel Processing Workshops.
[10] E. A. Le´
on, R. Riesen, A. B. Maccabe, P. G. Bridges, Instruction-level simulation of a cluster at
scale, in: Proceedings of the Conference on High Performance Computing Networking, Storage
and Analysis, SC ’09, ACM, New York, NY, USA, 2009, pp. 3:1–3:12.
[11] A. F. Rodrigues, R. C. Murphy, P. Kogge, K. D. Underwood, The structural simulation toolkit:
Exploring novel architectures, in: Proceedings of the 2006 ACM/IEEE Conference on Supercomputing, SC ’06, ACM, New York, NY, USA, 2006.
[12] S. S. Shende, A. D. Malony, The Tau Parallel Performance System, Int. J. High Perform. Comput.
Appl. 20 (2006) 287–311.
[13] N. Tallent, J. Mellor-Crummey, L. Adhianto, M. Fagan, M. Krentel, HPCToolkit: performance
tools for scientiﬁc computing, Journal of Physics: Conference Series 125 (1) (2008) 012088.
[14] M. Schulz, J. Galarowicz, W. Hachfeld, OpenSpeedShop: open source performance analysis for
Linux clusters, in: Proceedings of the 2006 ACM/IEEE conference on Supercomputing, SC ’06,
ACM, New York, NY, USA, 2006.
[15] M. S. Muller, A. Knupfer, M. Jurenz, M. Lieber, H. Brunst, H. Mix, W. E. Nagel, Developing
scalable applications with vampir, vampirserver and vampirtrace., in: PARCO, Vol. 15 of Advances
in Parallel Computing, IOS Press, 2007, pp. 637–644.
[16] O. Perks, D. Beckingsale, S. Hammond, I. Miller, J. Herdman, A. Vadgama, A. Bhalerao, L. He,
S. Jarvis, Towards automated memory model generation via event tracing, The Computer Journal.
[17] M. Payer, E. Kravina, T. R. Gross, Lightweight memory tracing, in: Presented as part of the
2013 USENIX Annual Technical Conference (USENIX ATC 13), USENIX, San Jose, CA, 2013,
pp. 115–126.
[18] T. Janjusic, C. Kartsaklis, D. Wang, Scalability analaysis of gleipnir, a memory tracing tool, on
titan, Cray User Group.
[19] J. L. Henning, Spec cpu2006 benchmark descriptions, SIGARCH Comput. Archit. News 34 (4)
(2006) 1–17.

1372

