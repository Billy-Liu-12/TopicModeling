Available online at www.sciencedirect.com

Procedia Computer Science 9 (2012) 897 – 906

International Conference on Computational Science, ICCS 2012

Climate classiﬁcations: the value of unsupervised clustering
Jakob Zscheischlera,b,∗, Miguel D. Mahechaa , Stefan Harmelingb
a Max
b Max

Planck Institute for Biogeochemistry, PO Box 100164, 07701 Jena, Germany
Planck Institute for Intelligent Systems, Spemannstr. 38, 72076 T¨ubingen, Germany

Abstract
Classifying the land surface according to diﬀerent climate zones is often a prerequisite for global diagnostic or
predictive modelling studies. Classical classiﬁcations such as the prominent K¨oppen–Geiger (KG) approach rely on
heuristic decision rules. Although these heuristics may transport some process understanding, such a discretization
may appear “arbitrary” from a data oriented perspective. In this contribution we compare the precision of a KG
classiﬁcation to an unsupervised classiﬁcation (k-means clustering). Generally speaking, we revisit the problem of
“climate classiﬁcation” by investigating the inherent patterns in multiple data streams in a purely data driven way. One
question is whether we can reproduce the KG boundaries by exploring diﬀerent combinations of climate and remotely
sensed vegetation variables. In this context we also investigate whether climate and vegetation variables build similar
clusters. In terms of statistical performances, k-means clearly outperforms classical climate classiﬁcations. However,
a subsequent stability analysis only reveals a meaningful number of clusters if both climate and vegetation data are
considered in the analysis. This is a setback for the hope to explain vegetation by means of climate alone. Clearly,
classiﬁcation schemes like K¨oppen-Geiger will play an important role in the future. However, future developments in
this area need to be assessed based on data driven approaches.
Keywords: clustering, k-means, multivariate statistics, K¨oppen-Geiger climate classiﬁcation, PCA

1. Introduction
Discretization techniques are important to get an intuitive understanding of complex geo-spatial data sets. A
typical example are global climate classiﬁcations like the K¨oppen-Geiger approach [1, 2, 3] and its recent updates
[4]. Classiﬁcations of this kind provide an intuitive way to discretize the Earth land surface properties, at the cost of a
loss of information. While the K¨oppen-Geiger (KG) classiﬁcation certainly reﬂects several decades of environmental
and geographic research, today we can ask the question how well KG performs when being compared to modern
clustering techniques. These techniques aim to identify points in the data cloud which could be used as ”predictors”
for other samples in a class. Another critical issue is that the original objective of K¨oppen and Geiger was to classify
vegetation zones based on temperature and precipitation patterns only. Later on, Thornthwaite criticized the lack
of rational justiﬁcations for most of the boundaries separating KG classes [5]. He claimed that the boundaries in a
climate classiﬁcation should relate to truly active climatic factors.1 . Along these lines one may ask if it is eﬀectively
∗ Corresponding

author, jzsch@bgc-jena.mpg.de
a side remark it is worth noting that he questioned the uncritical reception of the KG classiﬁcation and pointed out that people tend to
evaluate climatic classiﬁcation according to the ease with which they can be applied [5].
1 As

1877-0509 © 2012 Published by Elsevier Ltd. Open access under CC BY-NC-ND license.
doi:10.1016/j.procs.2012.04.096

898

Jakob Zscheischler et al. / Procedia Computer Science 9 (2012) 897 – 906

possible to delineate vegetation zones based on temperature and precipitation only. This ecophysiological question
becomes important nowadays, where additional variables such as radiation patterns are becoming available and could
be used in novel classiﬁcation approaches.
Clearly, the critique sensu [5] focuses on the lack of process understanding in the available classiﬁcations. However, adopting a data oriented perspective the question is rather if the heuristics in KG could be directly retrieved from
the observations. Along these lines, Cannon [6] recently showed that modern data driven approaches may equally
allow to derive relevant discretization rules. The author provides another climate classiﬁcation based on a multivariate regression tree and argues that he can yield a higher performance as measured by a quantity called expected
variance (EV).
The main problem of climate classiﬁcations, however, remains unaddressed by [6]: climate classiﬁcations seek
to classify vegetation exclusively based on climate conditions. It is implicitly assumed that vegetation is a function
of climate only; the idea is to identify “climate thresholds” that divide the vegetation space into discrete classes. But
how can we ﬁnd these jumps in an unsupervised fashion directly in the data? Ecological arguments against this idea
can be found in the fact that vegetation is also inﬂuenced by history preventing e.g. seeds establishment or leading
to extraordinary rates of mortality. Similarly, rapid changes in climate conditions may not allow vegetation to adapt.
Anthropogenic inﬂuence additionally causes shifts in vegetation classes. Finally, there may exist certain climate
conditions which favor two or more diﬀerent vegetation types - a phenomenon known as bistable system [7, 8, 9].
In this paper we investigate a multivariate data cube, consisting of diﬀerent combinations of climate variables and
remote sensing vegetation indices to examine whether climate and vegetation classes coincide. We apply unsupervised
clustering techniques and look into the diﬀerences between clustering of climate variables versus vegetation variables.
In order to compare diﬀerent clustering results we use a distance measure introduced in [10]. We also discuss the
problem of ﬁnding the right number of clusters. Our aim here is to discover discrete climate classes in an unsupervised,
purely data driven way and to compare the performance skills to the KG approach.
2. Data
We use three climate variables, namely the updated CRU2 and GPCC3 data sets which were used for the updated
KG classiﬁcation in [4]. We additionally use short wave radiation from ERA-Interim [11]. Furthermore, we use two
vegetation variables. Firstly, we include the Enhanced Vegetation Index (EVI) which is known to be responsive to
structural variations in the canopy, including leaf area index, canopy type, plant physiognomy, and canopy architecture [12, 13] (EVI data is taken from MODIS4 ). Finally we use the Fraction of Absorbed Photosynthetically Active
Radiation (FAPAR) which is directly related to primary productivity (the used data is described in [14]).
All data sets are used only over land on a spatial resolution of 0.5 degrees with averaged monthly values. We work
with an average year of the biggest intersection of years where all data sets are available, 2001-2007. The techniques
presented in this work can easily be applied on larger data sets with both varying spatial and temporal resolution.
Both EVI and FAPAR have diﬃculties with snow. Taking the minimum intersection of full data coverage we end
up with 54580 pixels, excluding some parts of northern Siberia and Greenland. We introduce the following shortcuts:
• 1 Air Temperature T,
• 2 Precipitation P,
• 3 Downward Shortwave Radiation SW,
• 4 Enhanced Vegetation Index EVI,
• 5 Fraction of Absorbed Photosynthetically Active Radiation FAPAR
We denote the data cube by X. With X v , v = 1, . . . , 5, we denote the diﬀerent variables. X vp,. denotes a time series
v
on pixel p (p = 1, . . . , 54580) and X.,t
a time step at month t (t = 1, . . . , 12) of variable v.
3. Tools
The data sets will be analyzed by several tools, which are listed in Table 1. In the following subsection we brieﬂy
introduce all tools used in this paper.
2 available

at http://www.cru.uea.ac.uk/cru/data/hrg/
at ftp://ftp-anon.dwd.de/pub/data/gpcc/html/fulldata_download.html
4 available at http://modis.gsfc.nasa.gov/
3 available

Jakob Zscheischler et al. / Procedia Computer Science 9 (2012) 897 – 906

tool
k-means
EV
VI
Instab

task
how does the data cluster?
is a clustering good?
are two clusterings diﬀerent?
is a clustering for ﬁxed k stable?

result
assignments
[0, 1]
[0, log(k)]
[0, log(k)]

899

interpretation
1 is perfect
0 implies identical
0 is stable

Table 1: Tools to cluster and analyze clustering for this paper.
3.1. Preprocessing
We will apply k-means to several subsets of the variables. This will involve calculating distances between vectors,
where each coordinate has a diﬀerent unit (e.g. for temperature and precipitation). To alleviate the eﬀects of diﬀerent
scales, we normalize each variable by its standard deviation, more precisely, we divide every X v by the standard
deviation of X v taken over all pixels and months,
Xv =

Xv
.
std(X v )

(1)

Since the distance calculations involve only diﬀerences between variables of the same type, we do not have to remove
the mean.
To investigate the diﬀerences between climate and vegetation variables we will perform k-means clustering both
on the whole data set (including all variables) and on subsets (including only some variables). Thus we combine
variables in the following way: e.g. considering temperature and precipitation, we use [X 1 , X 2 ] and perform k-means
on the resulting data set of size 12s × 54580 where s is the number of variables included (s = 2 in the example).
3.2. k-means
Given a set of n data points x1 , . . . , xn ∈ Rd and a ﬁxed number k of clusters to construct, k-means minimizes the
clustering objective function:
n
1
Q(c1 , . . . , ck ) =
min xi − ck 2
(2)
n i=1 k=1,...,k
where c1 , . . . , ck denote the centers of the k clusters. In our case, n = 54580 and d = 12s. We use the implementation
of Gehler (2007) [15] based on [16]. A similar fast implementation of k-means which exploits the triangle inequality
was introduced in [17, 18] and used in [19]. Note that k-means does not determine the “best” number of clusters k
automatically. Instead we compute k-means for the range k = 3, . . . , 40 and study Q dependent on k. Additionally, we
analyze the stability by considering diﬀerent subsets of X for varying k (see Section 3.5).
3.3. Explained predictand variance (EV) — a quality measure for clusterings
[6] introduced the explained predictand variance EV which can be interpreted as a measure of performance for
clusterings and classiﬁcations. Let WCSSk be the within-cluster sum of squares of a clustering with k clusters,
k

WCSSk =

x j − μi

2

,

(3)

i=1 x j ∈S i

where the S i are disjoint sets containing the data points assigned to the i-th cluster with mean μi . Then we can measure
the quality of a clustering by EV which is deﬁned by
EVk = 1 −

WCSSk
.
WCSS1

(4)

If EVk is equal to one, it means that WCSSk = 0. This can only be achieved for a data set with exactly k data points.
So on larger data sets we expect EV to be between zero and one.

900

Jakob Zscheischler et al. / Procedia Computer Science 9 (2012) 897 – 906

3.4. Variation of information (VI) — a distance measure for clusterings
[10] introduced the variation of information (VI), which is an information theoretic index that deﬁnes a metric on
the space of clusterings. Even for clusterings with diﬀerent a number of clusters VI can provide a distance.
A clustering C is a partition of a set of points, or data set D into mutually disjoint subsets C1 , C2 , . . . C K . Formally
K

C = {C1 , C2 , . . . , C K }

such that Ck ∩ Cl = ∅

Ck = D .

and

(5)

k=1

Let n and nk be the number of data points in D and in Ck , respectively. Then
K

nk .

n=

(6)

k=1

We assume nk > 0, i.e. empty clusters are ignored.
Consider a second clustering C = {C1 , C2 , . . . , C K } with cluster sizes nk . How does VI measure the distance
between C and C ? Criteria for comparing clusterings are usually based on the so-called confusion matrix (also called
association matrix or contingency table) of C and C . The confusion matrix is a K × K matrix, whose (k, k )-th
element is the number of points in the intersection of clusters Ck of C and Ck of C ,
nkk = |Ck ∩ Ck | .

(7)

VI is now deﬁned via random variables that are determined by the diﬀerent clusterings. We start with some wellknown information theoretic quantities which we translate into the setting of clusterings. If one picks a point of D at
random and each point has an equal probability of being picked, the probability that the chosen point is in cluster Ck
is
nk
P(k) =
.
(8)
n
This deﬁnes a discrete random variable taking K values associated to the clustering C. The uncertainty in the picking
process is given by the entropy of the random variable,
K

P(k) log P(k) .

H(C) = −

(9)

k=1

Furthermore, we can deﬁne the joint probability P(k, k ) which values how likely it is that a point belongs to Ck in
clustering C and to Ck in clustering C
P(k, k ) =

|Ck ∩ Ck | nkk
=
.
n
n

(10)

The mutual information I(C, C ) between two clusterings is deﬁned to be the mutual information between the associated random variables
K

K

I(C, C ) =

P(k, k ) log
k=1 k =1

P(k, k )
.
P(k)P(k )

(11)

Then we can deﬁne VI to be the
VI(C, C ) = H(C) + H(C ) − 2I(C, C )
= H(C, C ) − I(C, C ).

(12)
(13)

which is the diﬀerence between the joint entropy and the mutual information. [10] has shown that VI is a metric on the
space of clusterings. Furthermore, VI is n-invariant (independent
of the number of samples) and bounded by 2 log K
√
if both C and C have at most K clusters with K ≤ n. More properties and their proofs can be found in [10]. The
most important property of VI is that it can measure distances between clusterings with diﬀerent number of clusters.

Jakob Zscheischler et al. / Procedia Computer Science 9 (2012) 897 – 906

901

3.5. Instability of a clustering (Instab)
Another quality measure for clustering is stability. Given two samples from the same distribution we expect kmeans to cluster the data similarly, if k was chosen appropriately. For wrong k we could get diﬀerent clusterings for
diﬀerent samples, see [20]. We deﬁne instability of a clustering by the expected diﬀerence between two clusterings
Ck (S n ), Ck (S n ) on diﬀerent data sets S n , S n of the same size n, i.e.
Instab(k, n) = E d(Ck (S n ), Ck (S n )) ,

(14)

where d(·, ·) is some distance measures between clusterings. The expectation is taken with respect to the drawing of
the two samples. There exist various methods to compute stability scores. We will use the procedure described in
Algorithm 1 following [20]:
Algorithm 1 Compute instability scores
1:
2:
3:
4:
5:
6:
7:
8:
9:
10:

Given: a set of data points X, a clustering algorithm A that takes the number of clusters k as input
for k = 2, . . . , kmax do
Generate subsamples Xb of the original data set
for b = 1, . . . , bmax do
cluster the set Xb with algorithm A into k clusters to obtain clustering Cb
end for
for b, b = 1, . . . , bmax do
compute pairwise distance d(Cb , Cb ) between these clusterings
end for
compute instability scores as the mean distance between clusterings Cb
Instab(k) =

11:

1
b2max

bmax

d(Cb , Cb )

(15)

b,b =1

end for

As distance measure we use variation of information (VI) which we described in Section 3.4. We need, however,
a protocol to compare clusterings on diﬀerent data sets Xb . We overcome this by comparing the clusterings on the
extended data sets Xb ∪ Xb , i.e. we estimate two clusterings one for Xb and one for Xb and then evaluate them on the
union. For a given clustering characterized by k means, we assign new data points to the cluster with the closest mean
[20]. The instability score is bounded from above by the maximal distance between clusterings. An instability score
of 0 describes a stable clustering which is desired.
3.6. Principal Component Analysis (PCA)
PCA is widely used in multivariate statistics. We want to use it as a method for dimensionality reduction and as a
measure for the complexity or nonlinearity of the various data streams. We brieﬂy explain PCA here. Let X be a data
matrix with m rows representing diﬀerent features (e.g. months in our case) and n columns representing the samples.
The singular value decomposition (SVD) of X is given by X = WΣV T , with an orthogonal m × m-matrix W containing
the eigenvectors of XX T and a rectangular diagonal matrix Σ containing the singular values of X sorted by decreasing
magnitude. V contains the eigenvectors of X T X. The principal components are given by
Y = W T X = W T WΣV T = ΣV T

(16)

where now the ﬁrst row contains the data projected into the direction of maximum variance, etc. We can obtain a
reduced-dimensionality representation by projecting X onto the ﬁrst l singular vectors, Wl .
Yl = WlT X = Σl V T

(17)

Which corresponds to ignoring the lower rows of Y and only keeping the ﬁrst l rows. To measure the complexity of
the data we analyze the spectrum of the covariance matrix of the data, i.e. its eigenvalues. A scaled version of their
eigenvalues are contained in ΣΣT . We rescale and sort them such that λ1 ≥, . . . , ≥ λm , m
i=1 λi = 1. To project the data
onto the ﬁrst two components we ﬁrst have to reorder the columns of V according to the order of the eigenvalues.

902

Jakob Zscheischler et al. / Procedia Computer Science 9 (2012) 897 – 906

4. Results
4.1. Comparing the performance of k-means clustering via EV
First, we compare EV (see Section 3.3) of clusterings of k-means with the values of EV reported in [6]. Note
that we do not adjust for the seasons between northern and southern hemisphere which is usually done in KG, cf.
also [6]. In our case, northern hemispheric summers coincides with southern hemispheric winters and vice versa.
Consequently, regions with similar behavior may end up in diﬀerent clusters depending on their hemispheric location.
An adjustment of this issue, however, would lead to discontinuous clusters along the equator which we want to avoid
here.
We choose the same number of clusters used in [6], namely 5,13, and 30 to report values of EV for diﬀerent
subsets of variables. The analysis is then performed based on diﬀerent subsets, i.e. {P}, {T}, {P, T}, {EVI, FAPAR}
and {P, T, SW, EVI, FAPAR} and summarized in Figure 1 where Z denotes the results of our analysis. In terms of
statistical prediction, k-means yields better results than both K¨oppen-Geiger and Cannon’s multivariate regression
tree classiﬁcation. This results is not very surprising given that k-means is precisely optimizing this quantity. On
the contrary, it is noteworthy that both other classiﬁcations perform relatively well too. Nonetheless, if one is purely
interested in the statistical performance, the k-means approach is to be preferred.

Figure 1: Expected variance (EV) for diﬀerent scenarios. KG=K¨oppen-Geiger, C=Cannon, Z=our analysis.
4.2. Climate versus vegetation
Second, we investigate whether the clusterings obtained using only the climate variables are very diﬀerent from the
clustering results obtained using only vegetation variables. Both approaches are compared to the clustering containing
all variables (Figure 2).

Figure 2: Pairwise comparison of distances (VI) between climate, vegetation and all variables.

Jakob Zscheischler et al. / Procedia Computer Science 9 (2012) 897 – 906

903

In this experiment the distance between the clusterings of the climate and vegetation variables is quite large for
all k. We conclude that a good partitioning of climate and vegetation space, representing both climate and vegetation
properties, cannot be achieved using climate variables alone.
4.3. Finding the right k
A well-known problem of the k-means algorithm is identifying the “right” number of clusters without any additional information. Our ﬁrst approach is to evaluate Q(k) (see Eq. (2)) for k = 1, . . . , kmax and search for the ﬁrst
kink in the monotonically decreasing curve. Figure 3(left) shows Q(k) for diﬀerent subsets of the data with kmax = 40
illustrating that it is not easy to ﬁnd a clear kink in these curves. Inspecting the diﬀerence between two consecutive
values of Q may reveal more insights as shown in Figure 3(right). One can recognize a ﬁrst strong decrease of the
diﬀerence Q(k) − Q(k − 1) in the curve of all variables at k = 13 pointing on k = 12 as the right number of clusters
for that set of variables. Such a clear decrease is not there for the other variables. We will investigate further whether
we’re on the right way.

Figure 3: Left: Q for diﬀerent number of clusters k and diﬀerent variable subsets. Right: diﬀerence between Q(k) and
Q(k − 1).
A more robust method of detecting the right number of clusters is calculating the cluster stability. As described in
Section 3.5, we ﬁrst need to compute clusterings on subsets of the data. Instability scores are then calculated following
Algorithm 1 where we use VI (see Section 3.4) as distance measure. We built 20 subsets of 10000 pixels each and
compute pairwise distances. Instability scores are shown in Figure 4(left) for diﬀerent sets of variables (for the sake
of clarity we don’t show the results for the set T, P, SW; they don’t diﬀer much from T, P).
Although it is not possible to determine whether a chosen number k of clusters is too small, it seems feasible to
identify cases where k is too large [20]. Surprisingly, however, using either exclusively climate (P and T) or vegetation
variables (EVI and FAPAR) does not allow to identify a region of too many clusters. Only if we combine all variables
in the analysis we ﬁnd a big jump between k =12 and k =13, suggesting again that the “right” number of clusters for
this conﬁguration is 125 .
This ﬁnding is supported by an analysis over the standard deviation of distances between the diﬀerent subset
clusterings,
std(k) =

var d(Cbk , Cbk )b,b =1,...,bmax .

Standard deviations for the same subsets of variables as in Figure 4(left) are shown in Figure 4(right). For k = 12 and
using all variables, the standard deviation is very small compared to larger values of k, making the above result even
more robust. The resulting map of the k-means clustering with k = 12 for all variables is shown in Figure 5.

5 Note that small stability scores are desirable and that one has to look from the right to ﬁnd the ﬁrst signiﬁcant jump towards zero [20]. It cannot
be decided whether the jump between 10 and 11 renders 10 to be stable not (cf. Conjecture 3.8 in [20]).

904

Jakob Zscheischler et al. / Procedia Computer Science 9 (2012) 897 – 906

Figure 4: Left: Instability scores for diﬀerent number of clusters k and diﬀerent variable subsets. Right: Standard
deviation of pairwise cluster distances over subsets of X for diﬀerent number of clusters k and diﬀerent variable
subsets.

o
90 N

o
60 N

30oN

0o

o

0

0o

60 o
E

12

0

o

E

W

18

oW

60

o

W

0

o

0

12

18

60oS

W

o
30 S

Figure 5: Map of k-means clustering with k = 12 for the variables P, T, SW, EVI, FAPAR.
4.4. Comparing k-means clusterings with the K¨oppen-Geiger classiﬁcation
We investigate whether a k-means clustering with k = 12 sticks out against other values of k in terms of cluster
distance to the KG classiﬁcation. To this end we ﬁrst classiﬁed the used temperature and precipitation data according
to KG (we use the decision rules of [4]). We compared the obtained classiﬁcation with several of our clusterings with
k ranging from 3 to 40. As distance measure we again used VI (Sec. 3.4). Figure 6 shows VI as a function of k for
diﬀerent subsets of the investigated variables. Although clusterings of data containing the climate variables are a bit
closer to the KG classiﬁcation in general, a clear change of behaviour at k = 12 is not determinable.
4.5. How well does K¨oppen-Geiger match the the properties of the data?
If we reduce the dimensionality, e.g. by projecting the data onto the ﬁrst two principal components, k-means
quite nicely partitions the data (not shown here). This suggests that the data streams, although 12-dimensional, have
low intrinsic dimensionality. Rapidly decreasing eigenvalue spectra of the covariance matrices support that (also not
shown here).

Jakob Zscheischler et al. / Procedia Computer Science 9 (2012) 897 – 906

905

Figure 6: Values of VI between the K¨oppen-Geiger classiﬁcation of the years 2001-2007 and several clusterings
obtained with k-means.
To visualize KG on a low-dimensional projection of the data, we colour every sample with the label obtained
from the K¨oppen-Geiger classiﬁcation. To show the plots clearly, we only use the main climates A to E (see [4])
for the colouring. Interestingly, it seems that K¨oppen-Geiger classiﬁes the northern hemisphere quite well even if we
apply PCA on all variables (see Figure 7 left). The southern hemisphere, on the other hand, is matched rather poorly
(Figure 7 right). We conclude that in the northern hemisphere the boundaries for the ﬁve main climates coincide
well for both climate and vegetation. The diﬀerence between north and south maybe explained by the fact that the
developers of the classiﬁcation had much more experience with the climate and vegetation zones in the northern
hemisphere, especially in Europe.

Figure 7: Projection of the data from the northern (left) and southern (right) hemisphere onto the ﬁrst two principal
components. Each point constitutes one pixel and is coloured according to the ﬁve main climates of the K¨oppenGeiger classiﬁcation
Figure 7 and other produced plots of this kind also suggests that the data rather represents a continuous manifold
than discrete classes.
5. Discussion
With the currently observed constantly increasing amount of data unsupervised learning methods are gaining more
and more importance. Hypotheses accepted for a long time can be approved or questioned by data driven analysis and
possibly need to be adjusted.

906

Jakob Zscheischler et al. / Procedia Computer Science 9 (2012) 897 – 906

Climate classiﬁcations like KG aim for predicting vegetation zones using only climate variables. We have shown
that KG may not be optimal regarding this task and we tried to tackle it with an approach using unsupervised clustering. However, the clustering of climate and vegetation variables separately leads to diﬀerent results in cluster space.
Furthermore, we were not able to determine a stable (and thus naturally emerging) number of classes in both cases.
Interestingly, using both climate and vegetation variables together, we could identify a stable number of clusters,
namely 12. The close entanglement between climate and vegetation which cannot be described by a one-directional
function may explain this diﬀerent behaviour.
With our analysis, we challenge the assumption whether a prediction from climate variables to vegetation zones
is possible at all. Consequently, the obtained clustering including climate and vegetation has rather diagnostic than
predictive character. With the proposed tools a number of questions could be addressed. Future research should, e.g.,
analyse the interannual variability of the obtained clustering and whether the found 12 classes remain stable over time.
With longer time series an important objective might be to ﬁnd regions of changing classes over time. This would
give information about changing climate-vegetation interaction.
Acknowledgements. JZ is part of the International Max Planck Research School for Global Biogeochemical Cycles.
The authors thank Nuno Carvalhais for providing a script computing the KG classiﬁcation and Bj¨orn Reu for useful
comments.
References
[1] W. K¨oppen, Versuch einer Klassiﬁkation der Klimate, vorzugsweise nach ihren Beziehungen zur Pﬂanzenwelt, Geogr. Zeitschr. 6 (1900)
593–611, 657–679.
[2] R. Geiger, Landolt-B¨ornstein – Zahlenwerte und Funktionen aus Physik, Chemie, Astronomie, Geophysik und Technik, Springer, Berlin,
1954, Ch. Klassiﬁkation der Klimate nach W. K¨oppen, pp. 603–607., Band III, alte Serie.
¨
[3] R. Geiger, Uberarbeitete
Neuausgabe von Geiger, R.: K¨oppen-Geiger / Klima der Erde. (Wandkarte 1:16 Mill.), Klett-Perthes, Gotha, 1961.
[4] M. Kottek, J. Grieser, C. Beck, B. Rudolf, F. Rubel, World Map of the K¨oppen-Geiger climate classiﬁcation updated, Meteorologische
Zeitschrift 15 (3) (2006) 259–263.
[5] C. Thornthwaite, Problems in the classiﬁcation of climates, Geographical Review 33 (2) (1943) 233–255.
[6] A. J. Cannon, K¨oppen versus the computer: an objective comparison between the K¨oppen-Geiger climate classiﬁcation and a multivariate
regression tree, Hydrology and Earth System Sciences Discussions 8 (2) (2011) 2345–2372.
[7] B. Walker, D. Ludwig, C. Holling, R. Peterman, Stability of semi-arid savanna grazing systems, The Journal of Ecology 69 (2) (1981)
473–498.
[8] M. Scheﬀer, S. Carpenter, J. Foley, C. Folke, B. Walker, Others, Catastrophic shifts in ecosystems, Nature 413 (6856) (2001) 591–596.
[9] P. D’Odorico, F. Laio, L. Ridolﬁ, Noise-induced stability in dryland plant ecosystems, Proceedings of the National Academy of Sciences of
the United States of America (PNAS) 102 (31) (2005) 10819–10822.
[10] M. Meilˇa, Comparing clusterings – an information based distance, Journal of Multivariate Analysis 98 (5) (2007) 873–895.
[11] D. Dee, S. Uppala, A. Simmons, P. Berrisford, P. Poli, S. Kobayashi, U. Andrae, M. Balmaseda, G. Balsamo, P. Bauer, Others, The ERAInterim reanalysis: Conﬁguration and performance of the data assimilation system, Quarterly Journal of the Royal Meteorological Society
137 (656) (2011) 553–597.
[12] A. Huete, K. Didan, T. Miura, E. Rodriguez, X. Gao, L. Ferreira, Overview of the radiometric and biophysical performance of the MODIS
vegetation indices, Remote Sensing of Environment 83 (1-2) (2002) 195–213.
[13] X. Gao, A. Huete, W. Ni, T. Miura, Optical-biophysical relationships of vegetation spectra without background contamination, Remote
Sensing of Environment 74 (3) (2000) 609–620.
[14] M. Jung, M. Reichstein, H. Margolis, A. Cescatti, A. Richardson, M. Arain, A. Arneth, C. Bernhofer, D. Bonal, J. Chen, Others, Global
patterns of land-atmosphere ﬂuxes of carbon dioxide, latent heat, and sensible heat derived from eddy covariance, satellite, and meteorological
observations, J. Geophys. Res 116 (2011) G00J07.
[15] P. Gehler, Mpikmeans, http://mloss.org/software/view/48/ (2007).
[16] C. Elkan, Using the triangle inequality to accelerate k-means, in: Proceedings of the Twentieth International Conference on Machine Learning
(ICML 2003), 2003, pp. 147–153.
[17] S. Phillips, Reducing the computation time of the Isodata and K-means unsupervised classiﬁcation algorithms, Geoscience and Remote
Sensing Symposium (IGARS 2002) 3 (2002) 1627–1629.
[18] S. Phillips, Acceleration of k-means and related clustering algorithms, Revised Papers from the 4th International Workshop on Algorithm
Engineering and Experiments (2002) 166–177.
[19] J. Kumar, R. T. Mills, F. M. Hoﬀman, W. W. Hargrove, Parallel k-Means Clustering for Quantitative Ecoregion Delineation Using Large Data
Sets, Procedia Computer Science 4 (2011) 1602–1611.
[20] U. von Luxburg, Clustering stability: An overview, Found. Trends Mach. Learn. 2 (3) (2010) 235–274.

