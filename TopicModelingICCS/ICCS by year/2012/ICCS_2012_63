Available online at www.sciencedirect.com

Procedia Computer Science 9 (2012) 216 – 225

International Conference on Computational Science, ICCS 2012

High Performance Dense Linear System Solver with Resilience to
Multiple Soft Errors✩
Peng Du, Piotr Luszczek1 , Jack Dongarra
Department of Electrical Engineering and Computer Science, University of Tennessee, Knoxville

Abstract
In the multi-peta-ﬂop era for supercomputers, the number of computing cores is growing exponentially. However,
as integrated circuit technology scales below 65 nm, the critical charge required to ﬂip a gate or a memory cell
has been reduced and thus causing higher soft error rate from cosmic-radiations. Soft errors aﬀect computers by
producing silently data corruption which is hard to detect and correct. Current research of soft errors resilience for
dense linear solver oﬀers limited capability when facing large scale computing systems, and suﬀers from both soft
error and round-oﬀ error due to ﬂoating point arithmetic. This work proposes a fault tolerant algorithm that recovers
the solution of a dense linear system Ax = b from multiple spatial and temporal soft errors. Experimental results
on Cray XT5 supercomputer conﬁrm scalable performance of the proposed resilience functionality and negligible
overhead in solution recovery.
Keywords: soft error, fault tolerance, multiple errors, dense linear system solver

1. Introduction
Soft errors, normally in the form of bit ﬂips, are events in microelectronic circuit that result in transient modiﬁcation without permanently damaging the device. They corrupt computed data and produce erroneous results without
leaving a trace. High-end computer systems are especially susceptible to such errors due to the ever increasing chip
density and shrinking assembly scale. Between 2003 and 2004, the 2048-node ASC Q supercomputer for scientiﬁc
computing in Los Alamos National Laboratorys experienced failures from extensive soft errors [1]. By comparing the
error logs with a radiation experiment conducted in a lab, the cause was soon identiﬁed to be the cosmic ray striking
the parity-protected cache tag array. A similar incident has also appeared in a commercial computing system from
Sun Microsystems that caused outages for many of its customers due to cosmic ray soft errors [2]. These incidents
signify that soft error is a real issue that both hardware and software developers must face. Soft error rate (SER) in
6
memory is usually quantiﬁed using FIT (failure in time) per MB, 1 FIT is 1 failure per 109 operation hours per 10
✩ This research used resources of the Oak Ridge Leadership Facility at the Oak Ridge National Laboratory, which is supported by the Oﬃce of
Science of the U.S. Department of Energy under Contract No. DE-AC05-00OR22725.
Email addresses: du@eecs.utk.edu (Peng Du), luszczek@eecs.utk.edu (Piotr Luszczek), dongarra@eecs.utk.edu (Jack Dongarra)
1 Corresponding author

1877-0509 © 2012 Published by Elsevier Ltd. Open access under CC BY-NC-ND license.
doi:10.1016/j.procs.2012.04.023

Peng Du et al. / Procedia Computer Science 9 (2012) 216 – 225

217

bits. Google has reported between 778 and 25,000 FIT from errors in the DRAMs of their server ﬂeet, an order of
magnitude higher than previously expected [3].
Among HPC applications that could beneﬁt from resilience capability to soft errors, dense linear algebra applications such as the HPL benchmark for the TOP500 competition [4] and the AORSA fusion energy simulation
program [5], are representative examples. These applications normally involve solving a dense system of equations of
the form Ax = b on large scale HPC systems with matrix sizes of A being as large as 500,000. Soft errors that occur
during such long running applications produce incorrect solution with no apparent trace. This lowers productivity by
wasting valuable time and power in error tracing with little chance of ever locating the error.
To mitigate the impact of soft errors, hardware and software methods have been developed. For example, ECC
(error correcting code) [6] is a commonly used hardware technique which is although limited to a small number of soft
errors due to the overhead of encoding/decoding. In software, until now most of the soft error resilience techniques
for dense linear solvers are limited to small scale computing installations, such as on systolic arrays, assuming that
encoding/decoding can be carried out with exact arithmetic [7, 8]. Unfortunately, this assumption does not hold for
today’s Pﬂop/s supercomputer systems. In previous work [9], we have demonstrated the ﬁrst attempt to take on the
challenge of recovering the solution from a dense linear system solver of Ax = b with a single error occurrence in both
L and U of the LU factorization. This work further extends that eﬀort into multiple soft errors resilience as a more
performance friendly alternative to the complex hardware ECC. The proposed algorithms consider both the temporal
and spatial distribution of multiple errors. Temporal soft errors occur at diﬀerent time, whereas spatial soft errors
manifest as simultaneous multiple bit ﬂips in disparate locations. The proposed method may also be extended to other
one-sided factorizations for the recovery of linear system solution and factorization matrices.
The rest of the paper is organized as follows. Section 2 introduces an LU based dense linear solver. The impact of
soft error on the linear solver is then analyzed and the general workﬂow of the proposed soft error resilience algorithm
is shown in Section 3. Sections 4 and 5 develop the protection method for both the left factor L and right factor U.
Section 6 proposes a segmented encoding method to reduce the computational complexity of the error detection and
locating. Finally, the recovery algorithm is discussed in Section 7 and the experimental results are shown in Section 8.
Related work is described in Section 9 while Section 10 concludes the paper.
2. High Performance Linear System Solver
For dense matrix A, the LU factorization produces PA = LU (or P = ALU), where P is a pivoting matrix, and
L and U are unit lower and upper triangular matrix respectively. LU factorization is popular for solving system of
linear equations. With L and U, the linear system Ax = b is solved by Ly = b and then U x = y. ScaLAPACK[10]
implements the right-looking version of LU with partial pivoting based on a block algorithm and 2D block cyclic data
distribution. Without loss of generality, this block algorithm is described with an N × N matrix (or submatrix) A.
Split A into 2 × 2 blocks with block size NB. A11 has size NB × NB, A12 is NB × (N − NB), A21 is (N − NB) × NB,
and A22 is (N − NB) × (N − NB), which is also known as the “trailing matrix”. Decompose A as
A11
A21

A12
L
= 11
L21
A22

0 U11
L22 0

U12
L22

and therefore
⎧
⎪
A11
L
⎪
⎪
⎪
= 11 U11
⎪
⎪
A21
L21
⎨
⎪
⎪
⎪
A12 = L11 U12
⎪
⎪
⎪
⎩L22 U22 = A22 − L21 U12

→ PDGET F2
→ PDT RS M
→ PDGEMM

(1)

This poses as one iteration (step) of the factorization, and pivoting is applied to the left and right side of the current
panel. The routines names in the ScaLAPACK LU are listed after “→”. For description, we use U¯ to represent the
area of U12 modiﬁed by PDTRSM, and U˜ for A22 after PDGEMM. Note that the size of U¯ and U˜ changes as the LU
algorithm proceeds. Block algorithms oﬀer good granularity to beneﬁt from high performance BLAS routines, while
2D block-cyclic distribution ensures scalability with load balancing.

218

Peng Du et al. / Procedia Computer Science 9 (2012) 216 – 225

3. Soft Error Resilience Framework
Since soft errors occur at times and locations unknown to the host algorithm, diﬀerent methodologies are required
to provide resilience to diﬀerent part of the matrix. In this section, the error propagation in LU factorization is
discussed and a general workﬂow of error detection and recovery is given.
3.1. Error Pattern in the Block LU Algorithm
During LU factorization, the left factor L and right factor U have
diﬀerent “dynamics” with regard to the frequency of data change. For
L, once a panel is factorized, the resulted data stored under the diagonal comes to the ﬁnal form without undergoing any further changes
except pivoting. Therefore Soft errors occurred in this area do not
propagate. This oﬀers an opportunity to use traditional diskless checkpointing method to protect these data. Algorithm based fault tolerance (ABFT) cannot be applied to the panel factorization since otherwise checksum rows for the panel could be moved into data by pivoting
which causes erroneous result. In LU, partial pivoting that swaps rows
of both L and U is adopted for better stability, but this pivoting operation could break the static feature of the L data as explained in [9],
and therefore in this work the pivoting to the factorized L is delayed to
the end of factorization. Since soft errors could strike at any moment,
checkpointing frequency as high as once per panel factorization is necessary, but this also potentially leads to high performance overhead and
Figure 1: Example of error propagation in the U
therefore checkpointing should be used to the minimum. For example,
result of a 30 × 30 matrix
even though the factorized U¯ (result of PDTRSM) also stays static
once produced, it can be protected by ABFT checksum with much less
overhead.
U˜ diﬀers from L and U¯ in that it undergoes changes constantly from trailing matrix update. If soft errors alter
˜ and the erroneous data are carried along with computation to update the U,
˜ even a single-bit soft error
data within U,
˜ let alone multiple errors at diﬀerent time of the factorization.
could propagate into large area of U,
Figure 1 shows an example of error propagation in U in a small matrix. Gaussian elimination is applied to a 30×30
matrix. To simplify the illustration, no pivoting nor block algorithm is used. Each step of the Gaussian elimination
zeros out elements below the diagonal in one column. The color in the ﬁgure is related to the diﬀerence between the
correct and incorrect upper triangular results. Higher brightness means larger absolute diﬀerence in value and black
means no diﬀerence. During the factorization, Two soft errors were injected at step 1 and 3 at location (6,13) and (12,
18) by adding random values. Since both errors occurred below the row 3, these errors fell in the U˜ area of steps 1 to
3. The two white dots at (6,13) and (12, 18) are the initial injection locations. Starting from step 4, the trailing matrix
update, which is a GEMM (matrix-matrix multiplication), picked up the erroneous data for computation. As the
iteration continued, the errors grew downward into the trailing matrix (in yellow). When they reached the diagonal,
the erroneous data started to participate in the vertical scaling of zeroing out values below diagonals, and as a result
the entire trailing matrix was contaminated immediately, shown in red dots. Both of the two errors followed the same
propagation pattern. This example shows that soft error propagation could result in large area of erroneous data.
3.2. General Workﬂow
We proposed an ABFT based method to protect the LU factorization based linear solver. This method can tolerate
multiple occurrences of soft error in the whole area of factorization result and recover the correct solution x to the
linear system of equations Ax = b. The general workﬂow of error detection and recovery is in Algorithm 1. Checksum
that is generated before solving the system is used to check and locate the soft errors and eventually recover the
solution. The veriﬁcation process is performed with no “online checking” interruption.

Peng Du et al. / Procedia Computer Science 9 (2012) 216 – 225

219

Algorithm 1 Fault Tolerant System Workﬂow
Require: Ax = b; Generator matrix G; Check matrix H
Step 1: Checkpoint A by Ac = A A × G
Step 2: Perform LU factorization Lc Uc = P × Ac in block algorithm of block size NB with partial pivoting; Pivoting
to L is delayed; Panel factorization result in each step is checkpointed immediately once produced; Errors in L are
correct after factorization and then pivoting in L is applied
Step 3: Detect error occurrence by checking δ = Uc × H
if Found error(s) by δ >> 0 then
Step 3.1: Locate initial error(s) in U using δ
ˆ
Step 3.2: Calculate xˆ by xˆ = U(\L\(P
× b)), and
Step 3.3: Adjust xˆ to the correct solution x = xˆ + Δ
else
Step 4: Reach the correct solution x = U\(L\(P × b))
end if
4. Encoding for Multiple Errors in L
The ﬁrst step of the workﬂow in Algorithm 1 is to checkpoint the input matrix A with a generator matrix G. For
1 ··· 1
the single error case, it has been demonstrated in [9] that generator matrix G1 =
and check matrix
w 1 · · · wn
1 · · · 1 −1
work for the entire area of factorization result. In this section, we apply this idea to multiple
H1 =
w1 · · · wn −1
errors in L, and the next section further extends it to protecting U. Only the encoding issue is discussed. For a scalable
implementation, we continues to use the local checkpointing method in [9] where each process checkpoints its local
participating blocks in the current panel area.
For any column of the factorized panel in L, [l1 , l2 , · · · , lk ]T , the vertical checkpointing produces the following
three checksums c1 to c3 :
⎧
⎪
l1 + l2 + · · · + lk = c1
⎪
⎪
⎨
w
l
+ w2 l2 + · · · + wk lk = c2
(2)
⎪
1
1
⎪
⎪
⎩ u l + u l + ··· + u l = c
1 1

2 2

k k

3

Since all computation is carried out in ﬂoating point number that has a ﬁxed number of digits for exponent and
fraction, the selection of wi and ui should avoid causing large contrast between operands that encourages the accumulation of round-oﬀ errors. As an opposite example, in [8], the use of Vandermonde matrix where wi = j and ui = j2
incurs fast increase of value magnitude in checksum and causes notable precision loss from round-oﬀ errors.
To work with round-oﬀ errors, we propose to choose wi and ui from random numbers between 0 and 1. Suppose
soft errors change li and l j to lˆi and lˆj respectively, i < j. During the error locating step (step 3.1) in Algorithm 1,
re-generating the checksum gives:
⎧
⎪
⎪
l1 + · · · + lˆi + · · · + lˆj + · · · + lk = cˆ1
⎪
⎪
⎨
(3)
w1 l1 + · · · + wi lˆi + · · · + w j lˆj + · · · + wk lk = cˆ2
⎪
⎪
⎪
⎪
⎩ u1 l1 + · · · + ui lˆi + · · · + u j lˆj + · · · + uk lk = cˆ3
Subtract (3) from (2), we have
⎧
⎪
⎪
cˆ1 − c1 = lˆi − li + lˆj − l j
⎪
⎪
⎨
c
ˆ
−
c2 = wi (lˆi − li ) + w j (lˆj − l j )
⎪
2
⎪
⎪
⎪
⎩ cˆ3 − c3 = ui (lˆi − li ) + u j (lˆj − l j )

(4)

Deﬁne the system of equations in (4) as the “symptom equations”. The symptom equations establish the relationship
between soft errors and checksum, however it cannot be solved “as is” since the six unknowns lˆi , lˆj , wi , w j and ui , u j
outnumber the available three equations.

220

Peng Du et al. / Procedia Computer Science 9 (2012) 216 – 225

To reduce the number of unknowns, let ui = w2i , i = 1, · · · , k. Combine the ﬁrst and second equation in (4):
lˆj − l j =

1
((cˆ2 − c2 ) − wi (cˆ1 − c1 ))
w j − wi

(5)

And similarly combine the ﬁrst and third equation:
lˆj − l j =

(cˆ3 − c3 ) − w2i (cˆ1 − c1 )

(6)

w2j − w2i

Eliminate lˆj − l j from (5) and (6) by connecting the right hand sides, (4) can be eventually reduced to
(cˆ3 − c3 ) − (wi + w j )(cˆ2 − c2 ) + wi w j (cˆ1 − c1 ) = 0

(7)

Deﬁne this equation as the “check equation”. wi , w j can be determined by iterating through all possibilities in w with
O(N 2 ) complexity because i < j, and for each i, N − i pairs of wi w j are tested in (7).
The error detection and recovery algorithm can be extended to t errors with complexity O(N t ) to determine the
locations of up to t errors. For example, when t = 3, symptom equation 4 is
⎧
⎪
cˆ1 − c1 = lˆi − li + lˆj − l j + lˆk − lk
⎪
⎪
⎪
⎪
⎪
⎨cˆ2 − c2 = wi (lˆi − li ) + w j (lˆj − l j ) + wk (lˆk − lk )
⎪
⎪
⎪
cˆ3 − c3 = ui (lˆi − li ) + u j (lˆj − l j ) + uk (lˆk − lk )
⎪
⎪
⎪
⎩ cˆ − c = h (lˆ − l ) + h (lˆ − l ) + h (lˆ − l )
4

4

i i

i

j

j

j

k k

(8)

k

Here i, j and k correspond to the three errors’ locations. Similar to the double-error case, the check equation can be
derived as
C4 (wi − w j ) + w3i (w jC1 − C2 ) − w3j (wiC1 − C2 )
(wi − w j )w3k − (wi − wk )w3j + (w j − wk )w3i

=

C3 (wi − w j ) + w2i (w jC1 − C2 ) − w2j (wiC1 − C2 )
(wi − w j )w2k − (wi − wk )w2j + (w j − wk )w2i

Similarly, by iterating through all possible pairs of wi , w j and wk using the check equation, the three error locations
can be determined and the error value can be found accordingly.
5. Encoding for Multiple Errors in U¯ and U˜
Soft errors in U¯ and U˜ diﬀer from those in L because they participate in the computation and cause error propagation. The t = 2 case is discussed in detail and is then extended to t > 2 errors.
5.1. Soft Errors Modeling
For temporal multiple soft errors, Luk et al. has proposed to cast soft error to a diﬀernt initial matrix to avoid
the diﬃculty of knowing when soft error occurs [7]. The eﬀect of soft error during factorization is treated as rankone perturbation to the original matrix. Fitzpatrick et al. applied this method to double error modeling for Gaussian
elimination [8]. This section extends the encoding method in section 4 to provide protection to U.
LU factorization can be viewed as multiplying a set of triangularization matrices from the left to the input matrix
A to get the ﬁnal triangular form. Let A0 = A, and At = Lt−1 Pt−1 At−1 . Pt−1 is the partial pivoting matrix at step t − 1.
At the end of the factorization, PA0 = LU, where U is an upper triangular matrix.
Suppose two soft errors occur in the U¯ or U˜ area at locations (i1 , j1 ) and (i2 , j2 ) in step s1 and s2 separately. In the
most general case, s1 s2 , i1 i2 and j1 j2 . Without loss of generality, let s1 < s2 .
At step s2 , express the soft error as a perturbation to the matrix at location (i2 , j2 ): Aˆ s2 = A s2 − δei2 eTj2 . A s2 is the
state of the matrix at step s2 before soft error occurs, and Aˆ s2 is outcome of A s2 modiﬁed by a soft error of magnitude
δ at location (i2 , j2 ). ei2 and e j2 are column vectors with all 0s except a 1 at rows i2 and j2 respectively.

221

Peng Du et al. / Procedia Computer Science 9 (2012) 216 – 225

The error at step s2 is cast back as a perturbation to the matrix at step s1 ,

∴

Aˆ s2 = A s2 − δei2 eTj2 = L s2 −1 P s2 −1 L s2 −2 P s2 −2 · · · L s1 P s1 Aˆ s1 − δei2 eTj2
(L s2 −1 P s2 −1 L s2 −2 P s2 −2 · · · L s1 P s1 )−1 Aˆ s2 = Aˆ s1 − (L s2 −1 P s2 −1 L s2 −2 P s2 −2 · · · L s1 P s1 )−1 δei2 eTi

2

Let f = (L s2 −1 P s2 −1 L s2 −2 P s2 −2 · · · L s1 P s1 )−1 δei2 , and (L s2 −1 P s2 −1 L s2 −2 P s2 −2 · · · L s1 P s1 )−1 Aˆ s2 = Aˆ s¯2 , we have:
Aˆ s¯2 = Aˆ s1 − f eTj2

(9)

Continue casting (9) to the soft error at step s1 , eventually we have
Aˆ s¯1 = A0 − deTj1 − geTj2

(10)

Here g = (L s1 −1 P s1 −1 L s1 −2 P s1 −2 · · · L0 P0 )−1 × f = (L s2 −1 P s2 −1 L s2 −2 P s2 −2 · · · L0 P0 )−1 δei2
Through this modeling process, the two soft errors are cast back to the input matrix A0 as perturbation to column
j1 and j2 . For more than 2 errors, the same process can be repeated and the general model for t errors is
t

Aˆ 0 = A0 −

d ji eTji

(11)

j=1

5.2. Errors Detection and Locating
With the model for soft errors, errors’ locations can be determined. This model is for the case where soft errors
occur only in matrix A. In fact checksum and the right hand sides b of Ax = b are equally susceptible to soft errors.
These cases can be protected by duplication and cross check, and the protection method for L in section 4 can be
directly applied to protect right hand sides.
In [8], four columns of checksum are used to locate two soft errors. Instead, we show that for N errors, N + 1
columns are suﬃcient for error detection and data recovery.
For the input matrix A ∈ R N×N , checksum is generated before the factorization using generator matrix
⎤
⎡ T ⎤ ⎡
⎢⎢⎢ e ⎥⎥⎥ ⎢⎢⎢ 1 · · ·
1 ⎥⎥⎥
⎥ ⎢
⎥
⎢
(12)
G = ⎢⎢⎢⎢ wT ⎥⎥⎥⎥ = ⎢⎢⎢⎢w1 · · · wN ⎥⎥⎥⎥
⎣ 2 T⎦ ⎣ 2
2⎦
(w )
w1 · · · wN
and A is encoded as [A, A × GT ] = [A, Ae, Aw, Aw2 ]. The squaring operation is elementwise.
LU factorization is applied with the three additional checksum columns on the right as
P[A, Ae, Aw, Aw2 ] = L[U, c, v, s]
c, v, s ∈ R N×1 are checksum after factorization.
Due to soft errors, A becomes erroneous. As shown in the error model, the LU factorization infected with soft
errors is equal to an soft-error-free LU factorization of a diﬀerent initial (erroneous) matrix Aˆ 0 . Using A to represent
the original correct initial matrix and Aˆ for the erroneous initial matrix, (5.2) becomes:
ˆ U,
ˆ cˆ , vˆ , sˆ]
ˆ A,
ˆ Ae, Aw, Aw2 ] = L[
P[
And using relationship between cˆ and Ae:
cˆ =

ˆ Tj + Pge
ˆ Tj )e = Ue
ˆ + Lˆ −1 Pd
ˆ = Lˆ −1 P(
ˆ Aˆ + deTj + geTj )e = Lˆ −1 (Lˆ Uˆ + Pde
ˆ + Lˆ −1 Pg
ˆ
Lˆ −1 PAe
1
2
1
2

ˆ + Lˆ −1 Pg.
ˆ
ˆ = Lˆ −1 Pd
Therefore, cˆ − Ue
ˆ + w j2 Lˆ −1 Pg,
ˆ and sˆ − Uw
ˆ 2 = w2 Lˆ −1 Pd
ˆ + w2 Lˆ −1 Pg.
ˆ
ˆ = w j1 Lˆ −1 Pd
By the same token, vˆ − Uw
j1
j2
⎧
⎪
ˆ
cˆ − Ue = x + y
⎪
⎪
⎪
ˆ = w j1 x + w j2 y
ˆ ∈ R N×1 , and y = Lˆ −1 Pg
ˆ ∈ R N×1 . We have ⎨
v
ˆ
−
Uw
Let x = Lˆ −1 Pd
⎪
⎪
⎪
⎪
ˆ 2 = w2 x + w2 y
⎩ sˆ − Uw
j1
j2

222

Peng Du et al. / Procedia Computer Science 9 (2012) 216 – 225

This system of equations is the vector form of (4), and similarly can be reduced to the check equation:
ˆ + w j1 w j2 (ˆc − Ue)
ˆ =0
ˆ 2 ) − (w j1 + w j2 )(ˆv − Uw)
( sˆ − Uw

(13)

w j1 and w j2 can be determined by iterating through all possible N × (N − 1) combinations in w for a pair that makes
(13) hold. As a result, the error columns j1 and j2 are determined. Later, using the error columns, solution of Ax = b
can be recovered.
For t soft errors, with the error model in (11), the check equation is:
⎧
ˆ 0
⎪
= w0j1 x1 + · · · + w0jt xt
c0 − Uw
⎪
⎪
⎪
⎪
⎪
ˆ 1
⎪
= w1j1 x1 + · · · + w1jt xt
⎪
⎨ c1 − Uw
(14)
⎪
..
⎪
⎪
⎪
.
⎪
⎪
⎪
⎪
⎩ct−1 − Uw
ˆ t−1 = wt−1 x1 + · · · + wt−1 xt
j1
jt
All the powers in (14) are elementwise. This general case of check equation in vector form for t soft errors exhibits
the same structure as in the scalar form. For t = 3 it has been shown that check equation (9) can be used to determine
ˆ i.
error locations except the scalar residues Ci is replaced with vector residues ci − Uw
3
For two errors, the complexity of locating w j1 and w j2 is O(N ) because for each pair of w j1 and w j2 a vector norm
is calculated to test for zero vector in (13) which takes O(N) operations. For t > 2, the complexity to determine the
error columns exceeds the complexity of LU factorization, rendering this method computationally impractical for real
use. The same problem exists for L protection too when t > 3. The next section provides solution to this issue.
Since errors in U¯ and U˜ propagate, the solution to (14) alone is insuﬃcient for recovering the right factor U
as only the columns of the initial errors can be determined. However for a system of linear equations, by using
Sherman-Morrison-Woodbury formula, the solution can be recovered.
6. Complexity Reduction
As the number of tolerable errors t increases, the complexity of locating initial error columns grows exponentially.
To allow practical use, a complexity reduction method is proposed in this section.
6.1. Reduction for L
In the complexity O(N t ) for locating t errors, N is the factor that determines the range of search. By breaking
the search range into smaller segments, the complexity can be decreased to an aﬀordable level. There exist many
ways of segmenting N but since each segment requires separate storage for checksum, the segmenting method should
minimize the overall storage requirement. Use Nk to represent the segment size, the kth root of the vector length is
chosen in this work as the segment size where k is integer and , k ≥ 1.
1
1
Split N into equally sized segments of length Nk = N k . Apply the encoding method in (2) to each of the N 1− k
segments. For each vector to tolerate t errors, t + 1 checksum items are required. Therefore for a vector of length N,
the total amount of space required to store checksums is
1

N 1− k × (t + 1)

(15)

And the storage overhead over that for the data vector has the trend
1

lim (N 1− k × (t + 1) ×

N→∞

t+1
1
) = lim √k
=0
N→∞
N
N

(16)

Since the expensive error locating procedure is now carried out within a smaller range, the complexity of error
√k
1
detection is largely reduced. The total overhead of locating t soft errors includes N 1− K vector norms
⎧ of length N,
⎪
√
⎪
if t ≤ k
1
1
1
t
⎨O(N)
and iterating in k N for the correct pair of wi and w j O(N k × N 1− k ) + O((N k )t ) = O(N) + O(N k ) = ⎪
⎪
⎩O(N kt ) if t > k
Note that the number of tolerable soft error t is for each segment. Therefore to tolerate the same amount of soft
errors, each segment could opt for a smaller t than in the k = 1 case. For a ﬁxed t, increasing k has the same eﬀect by
reducing the range of search, but comes at the cost of more extra storage according to (15).

223

Peng Du et al. / Procedia Computer Science 9 (2012) 216 – 225

6.2. Reduction for U
˜ without any complexity management, locating t soft errors requires O(N t+1 ) operations, one order
For U¯ and U,
higher than the original complexity for L protection. To reduce the complexity to an aﬀordable level, the segmenting
method in section 6.1 is extended to the block LU algorithm for U¯ and U˜ protection.
6.2.1. Block Encoding of Matrix
In block LU algorithm, the panel factorization itself is an LU factorization of a tall and skinny block, therefore the
encoding technique in (5) can be used to protect a panel or several panels too if the encoding is performed accordingly.
For the whole matrix, the segmented encoding can be applied based on the following theorem (proof is omitted due
to space constrain):
Theorem 6.1. Block Encoding protects the trailing matrix at the end of each iteration of LU factorization
For illustration, consider the following example: take a matrix A of 2 × 2 blocks encoded using the generator in (12)
Ac =

A11
A21

A12
A22

A11G
A21G

A12G
A22G

0 U11 U12 C11 C12
L11
L21 L22 0 U22 C21 C22
C11 = U11G, C12 = U12G
. This shows that after LU factorization, the added
And C1 to C4 are calculated as:
= ∅, C22
= U22G
C21
four checksum blocks oﬀer protection to the three data blocks U11 , U12 and U22 independently. Since G in (12) oﬀers
t errors protection capability, the three data blocks in U each can tolerate up to t soft errors.
In ScaLAPACK,
matrix A is split into blocks of size NB × NB, therefore when√k = 2,
√
√ the encoding block size
Nk is N × N rounded to multiple of NB. Error detection is performed on each N × N blocks. For U11 , ﬁrst
U11 × G(:, 1) − C11√(:, 1) √
is checked and if the norm is suﬃciently large, the error detection procedure in 5.2 is then
performed for this N × N block.
The complexity of performing blocked error detection and locating includes the error check that is either a full
1
or upper triangular matrix-vector multiplication and the error locating operation within the block. Suppose Nk = N k
rounded to a multiple of NB, and the generator matrix G has size Nk × t for t error resilience capability. Since error
1
checking is only carried out in the upper triangular blocks of A, there are in total 1 + 2 + · · · + N 1− k number of blocks.
Carry out LU factorization to Ac and we have: Ac =

1− 1
k

1− 1
k

Therefore the error checking complexity is (1 + 2 + · · · + N 1− k ) × O((N k )2 ) = N (N2 +1) × O(N k ).
t
1
And the error locating√complexity
is O(N k × N k ). For instance, when k = 2 and t = 2, the total overhead of error
√
3
detection and locating is N( 2 N+1) × O(N) + O(N 2 ) = O(N 2 ) < O(N 3 ). Therefore the overhead is aﬀordable for the
solver based on LU factorization.
1
The total amount of extra storage for storing checksum columns is N × N 1− k × (t + 1). And the storage overhead
1
√k = 0.
over that for the data vector has the trend limN→∞ (N × N 1− k × (t + 1) × N12 ) = limN→∞ t+1
1

1

2

N

7. Recovery Algorithm
After soft errors are detected and located by their columns, the correct solution to the system of equations Ax = b
can be recovered using the Sherman-Morrison-Woodbury formula as suggested by [8]. The computing complexity of
recovery the solution x is O(N 2 ).
8. Performance Evaluation
Soft errors in the left factor are static, and the detection and recovery for this area has been validated in [11, 9]
showing little performance impact to the host algorithm. The algorithms for multiple soft errors in the right factor,
¯ on the other hand, have higher complexity and are most eﬀectively aﬀected by the proposed encoding
namely U˜ and U,
and complexity reduction method. Therefore only the validation for this part is shown in this section.

224

Peng Du et al. / Procedia Computer Science 9 (2012) 216 – 225


"



!





	







	






















         

		



	

"!$$
 

!""

!

	



 $#

!%!

!!%!%

##!




 !!

!$!

!!#

#!%"%

!#$




 $"

! %

!

#!"

!!%"%

$

!" " 

$$
!%!

Figure 2: Experimental results on Cray XT5 (Kraken): a run on 16,384 (128 × 128) cores (left) and weak scaling (right).

The experiments are performed on a large scale distributed memory system: the Kraken supercomputer by Cray
Inc. at the Oak Ridge National Lab. Kraken has 9,408 compute nodes. Each node has two Istanbul 2.6 GHz six-core
AMD Opteron processors and 16 GB of memory. All the nodes are Connected by the Cray SeaStar2+ interconnect.
In the experiments, two soft errors are injected into location (336, 361) and (347, 359) at the beginning of the 2nd and
3rd panel factorization, respectively. Data values are incremented with random magnitudes
to simulate the results of
√
bit ﬂips in the memory slots that hold these data. The block size for encoding is N.
Figure 2 shows the eﬀectiveness of the complexity reduction method for U with a 16 × 16 process grid on Kraken,
non−FT −FLOPS FT
%. When Nk = N, block encoding for soft
t = 2. The overhead is in Gﬂop/s and calculated as FLOPSFLOPS
non−FT
errors in U is not in eﬀect. The whole matrix is encoded with a generator matrix of size N × 3. In this case the
overhead is close to 100% (the blue line), which means the error detection and recovery combined take as much time
as solving the linear system of equations. This is consistent
with the theoretical complexity of O(N t+1 ) = O(N 3 ). The
√
red line, on the other hand, is the result when Nk = N. The overhead drops quickly from a little less than 40% to
2%, which veriﬁes that block encoding largely reduces the error detection overhead. The cost of this improvement is
the extra space for storing checksum which is roughly 1% of the input matrix of size 50,000.
Figure 2 also shows the weak scalability experiment result where matrix size and grid dimension are doubled in
proportion. Throughout all the testing sizes from 64 to 16,384 cores, FT-PDGESV declares around 1% overhead with
and without soft error recovery. The solution to the linear system was successfully recovered.
The experiment result conﬁrms that the complexity of recovering the solution to Ax = b from two soft errors in
the right factor has been eﬀectively managed by the complexity reduction method, and soft errors can be precisely
detected and located with the presence of round-oﬀ error. The fault tolerance functionalities can recover the solution
of the dense linear system with trivial performance impact.
9. Related Work
In the ﬁeld of fault tolerance for HPC systems, checkpoint-restart (C/R) is the most commonly used method [12].
The running state of the application is written to reliable storage at certain intervals automatically by the message
passing middleware or at the request of the user application. C/R requires the least user intervention but suﬀers from
high overhead from checkpointing through disk I/O. To reduce the overhead, diskless checkpointing [13] turns to
system memory for checksum storage rather than disks. Applications have seen better fault tolerance performance
than C/R [14] for dense linear algebra problem. Both C/R and diskless checkpointing need the error information
for recovery, which is not available with soft error. Algorithm based fault tolerance (ABFT) eliminates the need for
periodical checkpointing. This signiﬁcantly reduced checkpointing overhead during computing, and the checksum by
ABFT reﬂects the most current status of the data and therefore oﬀers clues for soft error detection and recovery. ABFT
was originally introduced to deal with silent error in systolic arrays [15, 16]. Data is encoded before the computation
begins. Matrix algorithms are designed to work on the encoded checksum along with matrix data, and the correctness
is checked after the matrix operation completes.
Using ABFT to mitigate single soft errors in dense matrix factorization has been explored in [7, 17]. Later, this
was extended to multiple errors [18, 8, 19] by adopting methodology from ﬁnite-ﬁeld based error correcting code (e.g.

Peng Du et al. / Procedia Computer Science 9 (2012) 216 – 225

225

Reed-Solomon [20]) where only the right factor of factorization result is protected and computation is assumed to take
place with exact arithmetics. These make the ECC based error location determination method loose eﬀectiveness.
Recently, iterative solvers were evaluated for soft error vulnerability [21, 22, 23], signifying the continued awareness of soft error for solving large scale problems. For dense matrices, the eﬀect of soft errors on linear algebra
packages like BLAS and LAPACK has also been studied [24], which showed that their reliability can be improved
by checking the output of the routine, and the error patterns do not depend on the problem size. Also, the possibility of predicting the fault propagation is explored. For dense matrix factorization based solver, method to mitigate
single soft error has been discussed in [9] and the recovery of matrix factorization was shown in [11] using QR for
demonstration.
10. Conclusion
Soft error resilient algorithm for LU factorization based dense linear system solver is proposed in this work. Both
spatial and temporal multiple soft errors in the whole matrix can be addressed with the existence of round-oﬀ errors
from ﬂoating point operation. Once errors are detected, the solution of Ax = b can be recovered with low overhead
using the complexity reduction technique. Experimental results on the Kraken supercomputer conﬁrm both the soft
error mitigation capability and the negligible performance overhead. In future work, the proposed method will be
extended to the protection of dense matrix factorizations like LU and QR.
References
1. S. Michalak, K. Harris, N. Hengartner, B. Takala, S. Wender, Predicting the number of fatal soft errors in los alamos national laboratory’s asc
q supercomputer, Device and Materials Reliability, IEEE Transactions on 5 (3) (2005) 329–335.
2. D. Lyons, Sun screen, available at http://www.forbes.com/forbes/2000/1113/6613068a.html (November 13 2000).
3. B. Schroeder, E. Pinheiro, W. Weber, DRAM errors in the wild: a large-scale ﬁeld study, in: Proceedings of the eleventh international joint
conference on Measurement and modeling of computer systems, ACM, 2009, pp. 193–204.
4. H. W. Meuer, E. Strohmaier, J. J. Dongarra, H. D. Simon, TOP500 Supercomputer Sites, 36th Edition, (The report can be downloaded from
http://www.netlib.org/benchmark/top500.html) (November 2010).
5. R. Barrett, T. Chan, E. D’Azevedo, E. Jaeger, K. Wong, R. Wong, Complex version of high performance computing linpack benchmark (hpl),
Concurrency and Computation: Practice and Experience 22 (5) (2010) 573–587.
6. D. Abts, J. Thompson, G. Schwoerer, Architectural support for mitigating dram soft errors in large-scale supercomputers.
7. F. Luk, H. Park, An analysis of algorithm-based fault tolerance techniques* 1, JPDC 5 (2) (1988) 172–184.
8. P. Fitzpatrick, C. Murphy, Fault tolerant matrix triangularization and solution of linear systems of equations, in: Application Speciﬁc Array
Processors, 1992. Proceedings of the International Conference on, IEEE, 1992, pp. 469–480.
9. P. Du, P. Luszczek, J. Dongarra, High performance dense linear system solver with soft error resilience, in: Proceedings of the IEEE Cluster
2011, IEEE Computer Society Press, 2011.
10. J. Dongarra, L. Blackford, J. Choi, A. Cleary, E. D’Azevedo, J. Demmel, I. Dhillon, S. Hammarling, G. Henry, A. Petitet, et al., ScaLAPACK
user’s guide, Society for Industrial and Applied Mathematics, Philadelphia, PA.
11. P. Du, P. Luszczek, S. Tomov, J. Dongarra, Soft error resilient QR factorization for hybrid system., Tech. Rep. 252, LAPACK Working Note,
http://www.netlib.org/lapack/lawnspdf/lawn252.pdf (Jul. 2011).
12. Fault tolerance for extreme-scale computing workshop report (2009).
URL http://www.teragridforum.org/mediawiki/images/8/8c/FT_workshop_report.pdf
13. J. Plank, K. Li, M. Puening, Diskless checkpointing, Parallel and Distributed Systems, IEEE Transactions on 9 (10) (1998) 972–986.
14. J. Plank, Y. Kim, J. Dongarra, Algorithm-based diskless checkpointing for fault-tolerant matrix operations, in: ftcs, Published by the IEEE
Computer Society, 1995, p. 0351.
15. K. Huang, J. Abraham, Algorithm-based fault tolerance for matrix operations, Computers, IEEE Transactions on 100 (6) (1984) 518–528.
16. J. Abraham, Fault tolerance techniques for highly parallel signal processing architectures, Highly parallel signal processing architectures
(1986) 49–65.
17. F. Luk, H. Park, Fault-tolerant matrix triangularizations on systolic arrays, Computers, IEEE Transactions on 37 (11) (1988) 1434–1438.
18. H. Park, On multiple error detection in matrix triangularizations using checksum methods, JPDC 14 (1) (1992) 90–97.
19. C. Anﬁnson, F. Luk, A linear algebraic model of algorithm-based fault tolerance, Computers, IEEE Transactions on 37 (12) (1988) 1599–1604.
20. I. Reed, G. Solomon, Polynomial codes over certain ﬁnite ﬁelds, Journal of the SIAM 8 (2) (1960) 300–304.
21. G. Bronevetsky, B. de Supinski, Soft error vulnerability of iterative linear algebra methods, in: Proceedings of the 22nd annual international
conference on Supercomputing, ACM, 2008, pp. 155–164.
22. K. Malkowski, P. Raghavan, M. Kandemir, Analyzing the soft error resilience of linear solvers on multicore multiprocessors, in: Parallel &
Distributed Processing (IPDPS), 2010 IEEE International Symposium on, IEEE, pp. 1–12.
23. V. Heuveline, D. Lukarski, F. Oboril, M. Tahoori, J. Weiss, Numerical defect correction as an algorithm-based fault tolerance technique for
iterative solvers.
24. G. Bronevetsky, B. de Supinski, M. Schulz, A Foundation for the Accurate Prediction of the Soft Error Vulnerability of Scientiﬁc Applications,
Tech. rep., Lawrence Livermore National Laboratory (LLNL), Livermore, CA (2009).

