Available online at www.sciencedirect.com

Procedia Computer Science 9 (2012) 1064 – 1072

International Conference on Computational Science, ICCS 2012

An Eﬃcient Implementation of the Ensemble Kalman Filter
Based on Iterative Sherman Morrison Formula
Elias D. Ni˜noa , Adrian Sandua , Jeﬀrey L. Andersonb
a Computational

Science Laboratory
Department of Computer Science
Virginia Polytechnic Institute and State University
2201 Knowledgeworks II, 2202 Kraft Drive, Blacksburg, VA 24060, USA
Tel: 540-231-2193, Fax: 540-231-9218, E-mail: enino@vt.edu, sandu@cs.vt.edu
b Data Assimilation Research Section
Institute for Mathematics Applied to Geosciences
National Center for Atmospheric Research
1850 Table Mesa Drive, Boulder, CO 80307-3000
Tel: 303-497-8991, Fax: 303-497-1298 , E-mail: jla@ucar.edu

Abstract
This paper proposes an eﬃcient implementation of the ensemble Kalman ﬁlter (EnKF) for the solution of largescale data assimilation problems. The implementation exploits the special structure of the covariance matrix and
solves the analysis step by iteratively applying the Sherman Morrison formula. The iterative implementation leads to
savings in both memory and run time. The number of operations for the iterative method is O n2ens · nobs , while for
the standard implementation the cost is O n3obs . The new implementation of the EnKF is tested using the Lorenz 96
model and shows a better performance than EnKF with the direct computation of the inverse.
Keywords:
Ensemble Kalman Filter, Sherman Morrison formula, Lorenz formula
2008 MSC: 62H99, 62-XX

1. Introduction
The Kalman ﬁlter is a well established computational method to estimate the state of a linear process through
the minimization of its mean squared error. The ﬁlter has been applied in many diﬀerent ﬁelds, from noisy voltage
signals on dendritic trees [1], to biomechanical signal processing [2] and dynamical systems with multiple sensors
[3]. For nonlinear applications the extended Kalman ﬁlter uses a linearized evolution operator in order to compute
the evolution of the uncertainties (background error) in the state. For large scale problems such as those arising in
atmospheric [4] and ocean science [5], oil reservoir simulation [6], and hydrological modeling, the ensemble Kalman
ﬁlter (EnKF) [7] has become popular. Realistic atmospheric and ocean models typically have n ∼ 107 − 109 variables.
EnKF approximates the error statistics using an ensemble of model runs. The EnKF analysis step involves computing
the solution of linear systems with full, large matrices. The focus of this paper is the development of an eﬃcient
method to compute these solutions.
1877-0509 © 2012 Published by Elsevier Ltd. Open access under CC BY-NC-ND license.
doi:10.1016/j.procs.2012.04.115

1065

Elias D. NiÒo et al. / Procedia Computer Science 9 (2012) 1064 – 1072

This paper is structured as follows. Section 2 provides a general overview of EnKF and establishes the notation
used in the paper. An equivalent deﬁnition of the EnKF is given in section 3, and an iterative Sherman Morrison
formula is used in order to solve the full, large system required by the EnKF analysis step. In section 4 experimental
results are shown using the Lorenz 96 Model. Section 5 discusses the conclusions about the proposed method.
2. The Ensemble Kalman Filter
The EnKF [7] represents the state error distribution using an ensemble of nens model states
nstate ×nens

Xt = xt [1] , xt [2] , . . . , xt [nens ] ∈

,

(1)

where nstate is the number of state variables (the dimension of the discrete system), and t is the discrete time index. The
forecast ensemble Xtf is obtained by propagating the ensemble of states using the model, and the analysis ensemble
Xta is obtained by the ﬁlter formulation as follows:
Xtf

=

a
Mt−1→t Xt−1
,

f
Xt

=

(nens )−1 Xtf · 1nens ×nstate ,

(2a)

f
Xt

Ptf

=

(nens − 1)−1 Xtf −

Xta

=

Xtf + Kt Yt − Ht Xtf

Pat

=

Ptf − Kt Ht Ptf ,

Kt

=

Pat HtT

(2b)
· Xtf −

f T
Xt

+ Qt ,

(2c)

,

(2d)
(2e)

Ht Pat HtT

+ Rt

−1

.

(2f)

(∗)

Here M is the model solution operator, Qt is the covariance matrix of model errors, Ptf is the (localized) forecast
error covariance matrix, Pat is the analysis error covariance matrix, Kt is the Kalman gain matrix, and Ht = Ht is the
linearized observation operator (with the linearization performed about Xtf ). The forecast represents the best estimate
of the true state prior to any measurement being available. Also, observations of the state yt = Ht (Xt )+ηt are available
at each time. These observations are corrupted by measurement and representative errors (ηt ), which are assumed to
have a normal distribution with mean equal to 0 and covariance matrix Rt . For large scale systems the inverse (∗) in
(2f) requires a high computational eﬀort. The next section proposes an eﬃcient method to compute the solutions of
the linear systems
(3)
Ht Pat HtT + Rt Zt = Yt − Ht Xtf
in the analysis step (2d).
3. An Ensemble Kalman Filter based on an Iterative Sherman Morrison Formula
For the implementation, let xtf
from the mean (2b) by:

[k]

S tf = (nens − 1)−1/2 xtf

be the kth ensemble member at time t, and denote the matrix of member deviations
[1]

f

−1/2
− X t + μ[1]
xtf
t , . . . , (nens − 1)

[nens ]

f

− X t + μt[nens ] ∈

nobs ×nens

,

(4)

where μ[k]
t ∈ N(0, Qt ) are samples drawn from the model noise distribution. An equivalent deﬁnition of the ensemble
covariance matrix (2c) is:
T
(5)
Pat = S tf · S tf ∈ nobs ×nens .
With the matrix V as
V = Ht · S tf ∈

nobs ×nens

(6)

1066

Elias D. NiÒo et al. / Procedia Computer Science 9 (2012) 1064 – 1072

we have
T

Ht Ptf HtT = Ht S tf S tf Ht

nens

= V · VT =

nobs ×nens

vi vTi ∈

.

(7)

i=1

Hence, letting Dt = Yt − Ht Xtf , equation (2f) can be rewritten as follows:
Xta

=

Xtf

+

Pat HtT

⎛ nens
⎞−1
⎜⎜⎜
⎟⎟
T
⎜⎜⎝ vi vi + Rt ⎟⎟⎟⎠ Dt .

(8)

i=1

Consider the sequence of matrices deﬁned by:
W0

= Rt

W1

= Rt +

W 2 = Rt +
..
.
. = ..
Wnens

(9)
v1 vT1
v1 vT1

= W0 +

v1 vT1

+ v2 vT2 = W1 + v2 vT2

nens

= Rt +

vi vTi = Wnens −1 + vnens vTnens ∈

nobs ×nobs

i=1

Thus, replacing the summation in (8) with the result of (9), we obtain:
Xta = Xtf + Pat HtT Wnens −1 + vnens vTnens
Thus, in order to compute Wnens −1 + vnens vTnens

−1

−1

Dt

(10)

we focus on the solution of the linear system:
−1

Wnens −1 + vnens vTnens Zt = Dt ⇔ Zt = Wnens −1 + vnens vTnens

Dt

(11)

Recall the Sherman Morrison Formula:
(A + UCV)−1 = A−1 − A−1 U C −1 + V A−1 U

−1

V A−1 .

(12)

To solve (11) we make use of the Sherman Morrison formula (12) with A = Wnens −1 , C = 1, U = vnens and V = vTnens :
Zt

=

I + vTnens Wn−1
Wn−1
Dt − Wn−1
v
v
ens −1
ens −1 nens
ens −1 nens

−1 T
vnens Wn−1
Dt
ens −1

.

(13)

With the solutions of the linear systems
Wnens −1 fnens = Dt
Wnens −1 gnens = vnens

⇔

fnens = Wn−1
Dt ,
ens −1

⇔

Wn−1
v
ens −1 nens

gnens =

(14)
,

(15)

equation (13) can be written as:
Zt = fnens − gnens I + vTnens gnens

−1 T
vnens fnens

.

(16)

Note that vnens ∈ nobs ×1 and fnens ∈ nobs ×nens . Therefore, in order to standardize matrix computations, consider
Dt = dt[1] , dt[2] , . . . , dt[nens ] , dt[i] ∈ nobs ×1 , and
d[i] ∈
fn[i]ens = Wn−1
ens −1 t

nobs ×1

.

(17)

1067

Elias D. NiÒo et al. / Procedia Computer Science 9 (2012) 1064 – 1072

Using (16) for each dt[i] , Zt is built as:
z[i]
t

=

fn[i]ens −

gnens vTnens fn[i]ens
1 + vTnens gnens

nobs ×nens

[nens ]
[2]
∈
Zt = z[1]
t , zt , . . . , zt

,

.

(18)

Equation (18) can be seen as a function φ with parameters fn[i]ens and gnens , which imply the computation of inverse
matrices, as follows:
z[i]
t

=

φ fn[i]ens , gnens = φ Wn−1
d[i] , Wn−1
v
ens −1 t
ens −1 nens

(19)

Thus, similarly to (10), the Sherman Morrison formula can be recursively applied to Wn−1
d[i] and Wn−1
vnens
ens −1 t
ens −1
−1
in order to approximate the solution of each system, which implies the computation of Wnens −1 . Note that for any
y ∈ nobs ×1 we have:
Wn−1
y
ens −1

=

Wnens −2 + vnens −1 vTnens −1

..
.
. = ..
Wk−1 y = Wk−1 + vk vTk
=
=

W1−1 y

=

−1

g

g

f

..
.

⎛
⎞−1
⎜⎜⎜
⎟⎟⎟
⎜
⎟
⎜⎜⎜
−1
−1
T
−1 ⎟
W0 y − W0 v1 ⎜⎜1 + v1 W0 v1 ⎟⎟⎟⎟ vT1 W0−1 y
⎜⎝
⎟⎠
f

W0−1 y

y

y
⎛
⎞−1
⎜⎜⎜
⎟⎟⎟
⎜⎜
⎟⎟
−1
−1
−1
−1
y − Wk−1
vk ⎜⎜⎜⎜1 + vTk Wk−1
vk ⎟⎟⎟⎟ vTk Wk−1
y
Wk−1
⎜⎝
⎟⎠
f

..
.

−1

−1

g

= R y (for any y =

g

dt[i]

f

∈ Dt or y = vi ∈ V)

As can be seen above, the recursion involves the inverse of matrix R. However, R is a diagonal matrix which
should be compute only once. Formally, the solution to (13) is given by:
• Step 1. Compute the inverse R of the observation error covariance. For example, for diagonal observation error
covariance matrix R = diag{rii } ∈ nobs ×nobs , we have R = R−1 = diag{rii−1 }.
[nens ]
[2]
[i]
• Step 2. For i = 1, 2, . . . , nens compute Zt = {z[1]
} where z[i]
t , zt , . . . , zt
t = φ(dt , nens ) and φ is deﬁned as
follows:
⎧
⎪
⎪
R˜ · y
if i = 0
⎪
⎪
⎪
⎪
⎪
⎪
⎨ f = φ (y, i − 1)
(20)
φ (y, i) = ⎪
⎪
⎪
in all other cases.
g = φ (vi , i − 1)
⎪
⎪
⎪
⎪
⎪
⎩z = f − g (1 + vi g)−1 vT f
i

This recursive function can be represented as a binary tree, in which, for any node, its left and right branches represent
the computations of f and g respectively. For instance, consider a problem with nens = 3 and nobs = 3. By using
(20), ﬁgure 1 shows the solution of linear system W3−1 d, for d = dt[i] ∈ nobs ×1 and dt[i] ∈ Dt . However, this process
involves identical multiplications several times . Thus, (20) is redeﬁned in order to avoid repeated computations. A
better approximation can be done through an iterative method.
Consider the last level of the tree in ﬁgure 1 as an array:
˜ 2 , Rv
˜ 1 , Rv
˜ 3 , Rv
˜ 1 , Rv
˜ 2 , Rv
˜ 1 .
˜ Rv
˜ 1 , Rv
L3 = Rd,

(21)

1068

Elias D. NiÒo et al. / Procedia Computer Science 9 (2012) 1064 – 1072

Figure 1: Tree representation of the recursive Sherman Morrison formula for the solution of W3−1 d. Note that for any node, its left branch represents
the solution of the linear system f , and its right branch represents the solution of the linear system g. Dashed nodes represent redundant matrix
computations.

All the even positions of the array represent identical computations. After removing repeated computations (21) can
be rewritten as follows:
˜ 1 , Rv
˜ 2 , Rv
˜ 3 .
˜ t[i] , Rv
L3 = Rd

(22)

˜ i and Rd
˜ t[i] can be performed by independentely. Thus, for vectors dt[i] ∈ Dt and vi ∈ V, the
The computations of Rv
matrices Zt and Ψ are built:
Ψ =
Zt

=

˜ 2 , . . . , Rv
˜ nens = ψ1 , ψ2 , . . . , ψnens = RV
˜ ∈
˜ 1 , Rv
W0−1 v1 , W0−1 v2 , . . . , W0−1 vnens = Rv
W0−1 dt[1] , W0−1 dt[2] , . . . , W0−1 dt[nens ]

=

˜ t[2] , . . . , Rd
˜ t[nens ]
˜ t[1] , Rd
Rd

˜ t∈
= RD

nobs ×nens

nobs ×nens

(23)
(24)

After avoiding redundant matrix computations, the level 2 of the algorithm can be written as follows:
L2 = W1−1 dt[i] , W1−1 v2 , W1−1 v3 .

(25)

The computation of W1−1 dt[i] can be done by applying the Sherman Morrison formula
W1−1 dt[i]

W0−1 dt[i] − W0−1 v1 1 + vT1 W0−1 v1

=

R−1 dt[i] − R−1 v1 1 + vT1 R−1 v1

=

˜ t[i] − Rv
˜ 1 1 + vT1 Rv
˜ 1 vT1 Rd
˜ t[i]
Rd
⎛
⎞−1
⎜⎜⎜
⎟⎟⎟
⎜
⎟
[i]
⎜
T ˜
˜
˜
˜ t[i]
⎜
Rdt − Rv1 ⎜⎜⎜1 + v1 Rv1 ⎟⎟⎟⎟⎟ vT1 Rd
⎝
⎠

=
=
−1

−1 T −1 [i]
v1 R dt

−1

z[i]
t

Deﬁning h1 = ψ1 1 + vT1 ψ1

−1 T −1 [i]
v1 W0 dt

=

ψ1

ψ1

T
z[i]
t − ψ 1 1 + v 1 ψ1

z[i]
t

−1 T [i]
v1 zt

, the matrices Zt and Ψ are updated at level 2 as follows:
Zt

=

Zt − h1 vT1 Zt

ψ1

=

W1−1 v2 = ψ2 − h1 vT1 ψ2

=

W1−1 v3

ψ2

= ψ3 −

(26)

h2 vT1 ψ3

(27)

Elias D. NiÒo et al. / Procedia Computer Science 9 (2012) 1064 – 1072

1069

Note that h1 is computed only once. Also, the matrix Ψ is reduced by one column since ψ3 is no longer useful;
therefore Ψ ∈ 3×2 . Similarly, for level 1, Zt and Ψ are updated as follows:
Zt

= Zt − h2 vT2 Zt ,

(28)

ψ1

= W1−1 v2 = ψ2 − h2 vT2 ψ2 ,

(29)

−1

where h2 = ψ1 1 + vT2 ψ1 . Again, Ψ is one column smaller than at the previous level (ψ2 is not required anymore)
therefore it turns into a vector Ψ ∈ 3×1 .
Finally, at the root level (L0 = W3−1 dt[i] ), Zt is updated once again through the Sherman Morrison formula, to
yield the solution of linear system W3 Zt = Dt
Zt = Dt − h3 vT3 Zt = W3−1 Dt ,

h3 = Ψ 1 + vT3 Ψ

−1

.

(30)

In general, for nens ensemble members, the iterative procedure is:
⎫
R˜ = R−1 ⎪
⎪
⎪
˜ ⎬
Ψ = RV
level nens
⎪
⎪
⎭
˜ ⎪
Z = RZ
t

t

h2 = ψ1 1 + vT2 ψ1
Zt = Zt − h2 vT2 Zt
ψ j = ψ j+1 − h2 vT2 ψ j+1

⎫
⎪
⎪
⎪
⎪
⎪
⎬
level nens − 1
⎪
⎪
⎪
⎪
⎪
⎭
1 ≤ j ≤ nens − 1
⎫
⎪
⎪
⎪
⎪
⎪
⎬
level nens − 2
⎪
⎪
⎪
⎪
⎪
⎭
1 ≤ j ≤ nens − 2

..
.
hk = ψ1 1 + vTk ψ1
Zt = Zt − hk vTk Zt
ψ j = ψ j+1 − hk vTk ψ j+1

⎫
⎪
⎪
⎪
⎪
⎪
⎬
level nens − k
⎪
⎪
⎪
⎪
⎭
1 ≤ j ≤ nens − k ⎪

h1 = ψ1 1 + vT1 ψ1
Zt = Zt − h1 vT1 Zt
ψ j = ψ j+1 − h1 vT1 ψ j+1

..
.
hnens −1 = ψ1 1 + vTnens −1 ψ1
Zt = Zt − hnens −1 vTnens −1 Zt
ψ1 = ψ2 − hnens −1 vTnens −1 ψ2

⎫
⎪
⎪
⎪
⎪
⎪
⎬
level 1
⎪
⎪
⎪
⎪
⎪
⎭
⎫
⎪
⎪
hnens = Ψ 1 + vTnens Ψ
⎬
level 0 (root)
⎪
T
−1
⎭
Zt = Zt − hnens vnens Zt = Wnens Dt ⎪

Formally, the iterative Sherman Morrison formula for the solution of (13) based on (20) is deﬁned as follows:
Step 1. Build R˜ = R−1 ∈

nobs ×nobs

.

Step 2. For i = 1 to nens compute:
hi

=

ψ1 1 + vTi ψ1

Zt

=

Zt − hi vTi Zt

ψj

=

ψ j+1 −

−1

hi vTi ψ j+1

(31)
1 ≤ j ≤ nens − i .

1070

Elias D. NiÒo et al. / Procedia Computer Science 9 (2012) 1064 – 1072

3.1. Operations Count for the Iterative Sherman Morrison-Based EnKF
For counting the operations performed by the proposed method only multiplications and divisions are taken into
account. Recall that R ∈ Rnobs ×nobs , V ∈ nobs ×nens , vi ∈ nobs ×1 , Dt ∈ nobs ×nens , and ψ j ∈ nobs ×1 .
˜ and Dt = RD
˜ t are computed. Since
• Step 1 is executed only once. At this point, the matrices R˜ = R−1 , Ψ = RV
˜ and RD
˜ t can be performed with
R is a diagonal matrix, R˜ = R−1 requires nobs operations. On the other hand, RV
nobs × nens multiplications. Thus, at root level, the total number of operations is nobs + 2nobs · nens .
• Step 2 computes hi , Zt , and ψ j . For the computation of hi ∈

hi

=

⎛
⎜⎜⎜
⎜⎜
ψ1 ⎜⎜⎜⎜1 +
⎝⎜

nobs ×1

we have:

vTi ψ1
nobs multiplications

⎞−1
⎟⎟⎟
⎟⎟⎟
⎟⎟⎟
⎟⎠

(32)

nobs divisions

Thus, the number of computations performed is 2nobs .
• The calculations done to update Zt are:
Zt

=

⎛
⎜⎜⎜
⎜⎜
Zt − hi ⎜⎜⎜⎜
⎝⎜

vTi Zt

nobs ·nens multiplications

⎞
⎟⎟⎟
⎟⎟⎟
⎟⎟⎟
⎟⎠

(33)

nens ·nobs multiplications

Hence, the number of multiplications involved in the computation of Zt is 2nens · nobs . Notice, Zt ∈

nobs ×nobs

.

• The computations performed by ψ j are:
ψj

⎛
⎜⎜⎜
⎜⎜
= ψ j+1 − hi ⎜⎜⎜⎜
⎜⎝

vTi ψ j+1

nobs multiplications

⎞
⎟⎟⎟
⎟⎟⎟
⎟⎟⎟
⎟⎠

(34)

nobs divisions

The sequence in (34) is repeated nens − i times, therefore the number of matrix multiplications in order to
compute ψ j becomes 2 (nens − i) nobs .
The calculations (32), (33), and (34) are executed nens times owing to they are used in step 2 of the method. The
total number of operations is:
nens

f (nens , nobs )

= nobs + 2nobs · nens +
step 1

(2nobs + 2nobs · nens + 2 (nens − i) nobs )
i=1
step 2

nens (nens + 1)
· nobs
= nobs + 4nobs nens + 4nobs · n2ens −
2
7 2
7
=
nens · nobs + nens · nobs + nobs .
2
2

(35)

Asymptotically we have f (nens , nobs ) ∈ O n2ens nobs . This should be compared against the O n3obs cost of a Cholesky
factorization [8] of Wnens for solving the system (3). In the typical situation nens
nobs and the cost of the new
algorithm is signiﬁcantly lower. Iterative methods can also be used in order to solve (3), but one needs to be concerned
with the convergence speed (the number of iterations necessary to achieve a desirable tolerance).

1071

Elias D. NiÒo et al. / Procedia Computer Science 9 (2012) 1064 – 1072

4. Numerical results
4.1. Computational Setting
The proposed EnKF based on iterative Sherman Morrison formula was tested using the Lorenz 96 model and its
results were compared the traditional formulation of EnKF [9]. The Lorenz 96 model is described by the following
ordinary diﬀerential equations:
dXk
(36)
= −Xk−1 (Xk−2 − Xk+1 ) − Xk + F , i = 1, 2, . . . , nstate ,
dt
where nstate = 200 is the number of variables in the model and F = 8. We use a version of the system (36) that is
larger than the one originally proposed by Lorenz, which has nstate = 40. Periodic boundary conditions are assumed.
The Lorenz 96 model [10] is a relevant test problem owing to its chaotic behavior. The following setting is used for
the numerical experiment:
1. The initial covariance matrix is diagonal, and characterized by a standard deviation of 5% of the mean value.
2. The observation error covariance is diagonal, and the standard deviations are 1% of the mean observation value.
3. The simulation time interval is t ∈ [0, 1] [time units], and observations are taken every Δt = 0.1 [time units].
There are nsteps = 10 observation times.
4. Observations of the ﬁrst nobs states are taken, with 100 ≤ nobs ≤ 180.
5. The number of ensemble members was in the range 100 ≤ nens ≤ 160.
The implementation uses MATLAB. For each nobs and nens values, the time average of the errors was obtained by
comparing analyses and the reference (“true”) solution:
Λ (nens , nobs ) =

1
nsteps

nsteps

a

Xttrue − X t
t=1

2

.

(37)

4.2. Experimental Results
The values of error metric (37) applied to results from each EnKF simulation are shown in table 1. The bold
numbers indicate the best approximation proposed to the real solution. Most of the times the solutions obtained with
the proposed method are more accurate. In particular, when the number of ensemble members is incremented, the
proposed EnKF implementation is systematically more accurate than the direct implementation. This is likely due to
a slower propagation of the roundoﬀ errors in the new algorithm.
Table 1: EnKF results for the solution of linear system Zt = Wn−1
Dt . Each cell contains the error Λ (nens , nobs ).
ens

Method
EnKF - direct solution

EnKF - iterative Sherman Morrison solution

nens /nobs
100
120
140
160
100
120
140
160

100
4.7799
4.0382
2.5076
2.1575
3.8859
3.2391
8.0945
1.5234

120
4.0449
2.0956
2.1082
1.4397
4.0358
3.7270
2.0660
1.7152

140
2.9589
2.5776
1.7180
1.5972
3.4119
1.8786
1.6949
1.5979

160
3.6913
2.1489
1.7322
1.2224
3.6381
1.5609
1.4757
1.1593

180
2.4720
1.7529
0.9811
0.9377
3.5689
1.5591
1.1600
0.8820

5. Conclusions
The EnKF is an eﬃcient approach for data assimilation with large scale models arising in atmospheric and oceanic
sciences. The EnKF analysis step involves expensive matrix computations. This paper develops an equivalent formulation of the EnKF that exploits the structure of the forecast matrix to avoiding the direct solution of linear systems.
The algorithm applies iteratively the Sherman Morrison formula and it takes into account the special form of the
matrix, which involves a symmetric nens -rank update. The number of operations performed is O(n2ens · nobs ), and is
nobs .
signiﬁcantly smaller than the O(n3obs ) cost of a direct implementation when nens

1072

Elias D. NiÒo et al. / Procedia Computer Science 9 (2012) 1064 – 1072

Acknowledgements
This work has been supported in part by NSF through awards NSF OCI-8670904397, NSF CCF-0916493, NSF
DMS-0915047, NSF CMMI-1130667, and by the Computational Science Laboratory at Virginia Tech.
References
[1] L. Paninski, Fast Kalman ﬁltering on quasilinear dendritic trees, Journal of Computational Neuroscience 28 (2010) 211–228, 10.1007/s10827009-0200-4.
[2] A. Sabatini, Real-time kalman ﬁlter applied to biomechanical data for state estimation and numerical diﬀerentiation, Medical and Biological
Engineering and Computing 41 (2003) 2–10, 10.1007/BF02343532.
[3] J. Feng, M. Zeng, Optimal distributed Kalman ﬁltering fusion for a linear dynamic system with cross-correlated noises, International Journal
of Systems Science 43 (2) (2012) 385–398. doi:10.1080/00207721.2010.502601.
[4] A. Sandu, E. Constantinescu, G. Carmichael, T. Chai, D. Daescu, J. Seinfeld, Ensemble methods for dynamic data assimilation of chemical
observations in atmospheric models, Journal of Algorithms and Computational Technology 5 (4) (2011) 667–692.
[5] B. L. Simon, E., Gaussian anamorphosis extension of the denkf for combined state parameter estimation: Application to a 1d ocean ecosystem
model, Journal of Marine Systems 89 (1) (2012) 1–18.
[6] S. Jung, J. Choe, Stochastic estimation of oil production by history matching with ensemble Kalman ﬁlter, Energy Sources, Part A: Recovery,
Utilization, and Environmental Eﬀects 32 (10) (2010) 952–961. arXiv:http://www.tandfonline.com/doi/pdf/10.1080/15567030802463000,
doi:10.1080/15567030802463000.
[7] G. Evensen, The ensemble Kalman ﬁlter for combined state and parameter estimation, Control Systems, IEEE 29 (3) (2009) 83 –104.
doi:10.1109/MCS.2009.932223.
[8] C.-P. Jeannerod, G. Villard, Essentially optimal computation of the inverse of generic polynomial matrices, Journal of Complexity 21 (1)
(2005) 72 – 86, foundations of Computational Mathematics Conference 2002. doi:10.1016/j.jco.2004.03.005.
URL http://www.sciencedirect.com/science/article/pii/S0885064X04000275
[9] G. Evensen, The ensemble Kalman ﬁlter: theoretical formulation and practical implementation (2003).
[10] E. Lorenz, Atmospheric predictability experiments with a large numerical model, Tellus A 34 (6).
URL http://tellusa.net/index.php/tellusa/article/view/10836

