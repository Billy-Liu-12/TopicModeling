Available online at www.sciencedirect.com

Procedia Computer Science 9 (2012) 852 – 856

International Conference on Computational Science, ICCS2012

Computational optimization, modelling and simulation:
Smart algorithms and better models
Xin-She Yanga,*, Slawomir Kozielb, and Leifur Leifssonb
a

Mathematics and Scientific Computing, National Physical Laboratory, Teddington, Middlesex TW11 0LW, UK
Engineering Optimization and Modeling Center, School of Science and Engineering, Reykjavik University, 101 Reykjavik, Iceland.

b

Abstract
Computational optimization is becoming a standard tool that is widely used in engineering design and industrial applications.
Products and services are often concerned with the maximization of profits and reduction of cost, but also aim at being more
energy-efficient, environment-friendly and safety-ensured; at the same time they are limited by resources, time and money.
Despite of increasing computer power and availability of better simulation packages, there are a number of challenges remaining
when applying numerical optimization methods for real-world engineering problems. Also, new challenges emerge when
attempting to attack problems whose solution by means of simulation-based optimization was not even possible in the past. This
third workshop on Computational Optimization, Modelling and Simulation (COMS 2012) at ICCS 2012 will further summarize
the latest developments of optimization and modelling and their applications in science, engineering and industry.

Keywords: algorithm; black-box modelling; computational optimization; derivative-free method; optimization algorithm; modelling; nonlinear
optimization; surragate-based optimization; simulation;

1. Introduction
Computational optimization and modeling has become an important paradigm in contemporary engineering and
science. In almost all applications in engineering and industry, we are always trying to optimize something
whether to minimize the cost and energy consumption, or to maximize the profit, output, performance and
efficiency. Due to limitations of resources and time constraints optimization becomes critically important [1,2,3].
The optimal use of available resources of any sort requires a paradigm shift in scientific thinking. This is because
most real-world applications have complicated factors and parameters affecting the system behavior; subsequently,
it is not always possible to find the optimal solutions. In many cases, we have to settle for suboptimal solutions or
even feasible solutions which are good enough and practically achievable in a reasonable time scale.
There are a number of challenges while applying numerical optimization to solve real-world problems in
engineering, science and industry. The challenges may be due to high nonlinearity of the objective function and/or
____________
* Corresponding author, email: xy227@cam.ac.uk

1877-0509 © 2012 Published by Elsevier Ltd. Open access under CC BY-NC-ND license.
doi:10.1016/j.procs.2012.04.091

Xin-She Yang et al. / Procedia Computer Science 9 (2012) 852 – 856

853

constraints, the scale of the problem itself such as a large number of design variables, exponential growth of the
search space size with the number of designable parameters, the presence of multiple local optima, non-smoothness
of the objectives/constraints, the presence of uncertainties, to name just a few. On the other hand, contemporary
engineering design is heavily based on computer simulations which tend to be more and more time consuming
despite of the increase in available computing power. Increasing simulation cost is a source of additional difficulties
in optimization and creates the need for the development of better (or smarter) algorithms that would be able to yield
satisfactory designs within a reasonable timeframe. For this purpose, it is often necessary to use auxiliary models
that embed certain knowledge about the system under consideration while being computationally cheap at the same
time.
The third workshop on Computational Optimization, Modelling and Simulations (COMS 2012) at the ICCS 2012
is intended to provide an opportunity to foster discussion on the latest developments in optimization and computer
modeling with a focus on applications in science, engineering and industry. In the remaining sections of this
summary paper, we briefly review the latest trends and optimization techniques, as well as introduce the topics and
papers in this workshop.
2. Simulation and Optimization
The most important component of a design and design optimization process is an appropriate representation of
the system under consideration. In many cases, analytical models given by explicit formulas may be sufficient;
however, explicit formulas are not possible in most cases. Consequently, numerical simulation of the underlying
physical processes is becoming the most important modeling tool in a growing number of engineering disciplines.
Today, numerical simulation has sufficiently matured and sophisticated tools are available in many areas that
when set up properly may evaluate the system performance with great accuracies, which are nearly as good (but
much cheaper than) as physical measurements of the system.
In order to carry out proper optimization, the performance requirements imposed on the system have to be reformulated in mathematical terms to define an objective function (or functions) so that the formal relationship
between the values of the designable parameters and the system performance is established. In some cases, this
relationship can be represented in a form of a scalar function that can be minimized; in many other cases, a set of
competing objectives can be only formulated so that the (multi-objective) optimization process has to be followed
(or interleaved) by the decision making process to select the best combination out of a feasible set of, usually noncommensurable, objective sets.
There is a variety of optimization algorithms available for engineers and scientists that differ in how the initial
design is being modified in order to find an optimum or near-optimum one. Some of these methods are more
suitable to perform local search on analytically tractable (smooth, or at least continuous) objectives functions, others
are more suitable for global search, dealing with noisy data, or computationally expensive problems. There is a large
group of conventional techniques, mostly gradient-based or derivative-free ones, that find a new approximation of
the optimum design by iteratively modifying a single solution using, e.g., sensitivity information or a comparison of
several neighboring solutions. These methods are typically well established with good theoretical foundations, and
their application area is mostly local search or tractable functions. There is a group of metaheuristic approaches
(such as genetic and evolutionary algorithms, particle swarm optimizers, and firefly algorithms) that are based on
processing populations of candidate solutions (so-called individuals) using partly-stochastic updating rules.
Metaheuristics are suitable to tackle problems that are difficult for conventional methods (multiple local optima,
discontinuity of the objective function, etc.). Finally, there is a still-growing group of surrogate-based optimization
(SBO) methods [4,5,6], where the direct optimization of a problematic (noisy, expensive) function is replaced by
iterative updating and re-optimization of a tractable (smooth, cheap) surrogate model. These methods are
particularly suitable for problems where the evaluation of the objective function is computationally expensive (e.g.,
3D finite-element analysis of complex structures) and the optimization cost is a critical issue. In addition, a good
combination is to use both surrogate-based simulation tool with an efficient metaheuristic optimization algorithm,
and such hybridization could be even more powerful with suitable modifications using problem-specific knowledge.

854

Xin-She Yang et al. / Procedia Computer Science 9 (2012) 852 – 856

3. Metaheuristics
For some optimization problems, especially the linear problems and/or problems with a few design variables,
many classical algorithms such as Nelder-Mead down hill simplex method and trust-region methods work very well
[3,7]. For slightly more complicated problems with nonlinear constraints, algorithms such as interior-point and
active-set methods work are sufficient. But for large-scale, nonlinear global optimization problems, there is no
agreed algorithm. In fact, such type of global optimization does not have any efficient algorithm. However, a very
promising trend is to use metaheuristic algorithms.
Metaheuristic algorithms are mostly nature-inspired or biologically inspired algorithms, mimicking certain
successful characteristics of natural systems. There are a wide range of metaheuristic algorithms, including ant
colony optimization, bee algorithms, cuckoo search, artificial immune algorithms, genetic algorithms, differential
evolution, harmony search, particle swarm optimization, firefly algorithm, simulated annealing, Tabu search, and
monkey search. Most of these algorithms are population-based algorithms using multiple agents, and the only
exception is the simulated annealing which uses a trajectory-based approach. However, parallel simulated annealing
is also population-based.
Two important characteristics of metaheuristics are intensification and diversification, or exploitation and
exploration. Intensification focuses on the local search which exploits the information from local landscape, while
diversification often uses certain randomization techniques to explore the search space more aggressively and
extensively. Different algorithms use slightly different approaches for constructing these two components, but they
all tend to achieve some tradeoff between local intensive search and global exploratory search [3]. Recent advances
and developments have focused on the development of new algorithms and improvement of existing algorithms by
adding new features. Test and validation of various algorithms are also another important part of new developments.
4. Surrogate-Based Modeling and Optimization
Today, computer simulations are ubiquitous in numerous fields of engineering and science. Because of the
complexity of the physical systems considered by the engineers and scientists, as well as numerous interactions
between devices and their environments, it is no longer possible to reliably use simple, often analytical models to
fficient accuracy. High-fidelity computer
simulation (such as electromagnetic, finite element or computational fluid dynamics) is often the only way to model
the reality to the extent that is sufficient for carrying out the design process in a reliable way. Unfortunately,
accurate simulation may be computationally expensive and simulation-based objective function may be analytically
intractable (discontinuous, noisy), and sensitivity information is often unavailable. As a result, straightforward
optimization of simulation-based models is problematic and often prohibitive. Because of these issues, it is
convenient, in many situations, to replace the original model by an appropriately constructed surrogate. There has
been substantial research effort observed in the development of surrogate modeling methodologies that would allow
to create models that are globally (or quasi-globally) accurate, smooth, and computationally cheap. While there are
many different techniques available, both approximation-based (kriging [4], support-vector regression [8], neural
networks [9]) and physics-based (space mapping [10]), many problems remain open, e.g., reducing the amount of
data necessary to create the model, either by a smart sampling or by exploiting knowledge embedded in auxiliary,
low-fidelity models.
Surrogate-based optimization (SBO) [4,10] can accelerate the design process when dealing with time-consuming
computer simulations. In SBO, the direct optimization of a computationally expensive model is replaced by an
iterative process that involves the creation, optimization and updating of a fast and analytically tractable surrogate
model. The surrogate should be a reasonably accurate representation of the high-fidelity model, at least locally. The
design obtained through optimizing the surrogate model is verified by evaluating the high-fidelity model.
Subsequently, the surrogate model is updated using the high-fidelity model data. This predictor-corrector fashion is
continued iteratively until a termination criterion is met. SBO reduces the overall optimization cost, compared to the

Xin-She Yang et al. / Procedia Computer Science 9 (2012) 852 – 856

855

direct optimization of the high-fidelity model, because the computational load has been shifted to the cheap
surrogate model.
Typically, surrogate models are constructed using a sampling plan (i.e., design of experiments [4]) and nonlinear
regression using, e.g., low-order polynomials [4], kriging [13], or support vector regression [8]. These
approximation models are called functional surrogates and they are widely used in both academia and the industry
[13]. Variable-fidelity SBO (also referred to as multi-fidelity SBO) uses low-fidelity models and suitable correction
techniques to construct the surrogates [11]. The low-fidelity models can be simplified physics models [Robinson et
al.], computational models with varying level of accuracy [15], or a combination of the aforementioned models [16].
Numerous correction techniques are available, including simple response correction methods (both additive and
multiplicative, see, e.g., [14]), space mapping (SM) [10], and shape-preserving response prediction (SPRP) [16].
Compared to SBO with functional surrogates, variable-fidelity optimization techniques can yield significant savings
in computational cost, as the number of calls to the expensive simulation model is reduced, see, e.g., [17].
5. Recent Advances
Applications of optimization in engineering and industry are diverse and this is reflected in the papers
submitted to this workshop. The responses and interests to our call for papers are overwhelming; however, due to
limited space and time slots for presentations, many high-quality papers cannot be included in the workshop.
The accepted papers of this workshop COMS2012 at ICCS 2012 have spanned a wide range of applications and
reflect the state-of-the-art developments in computational optimization, modeling and simulation. Thaher and
Takaoka present an efficient algorithm of computing the k-overlapping maximum convex-sum problems. Koziel et
al. present techniques for knowledge-based response correction and adaptive design specification for microwave
design optimization, and scaling properties of multi-fidelity models. Leifsson et al. present the numerical
optimization and experimental validation of a low speed wind tunnel contract, and also discuss the low-fidelity
model mesh density and its relationship with the performance of algorithms. Liu et al. present a spatially explicit
agent-based simulator of beef cattle movements. Gajurel and Bielefeld discuss a simple NOVCA approach that
produces a minimum or near optimal vetrex cover for any known undirected graph. Saif et al. provide an optimized
A-MSDU frame aggregation with subframe retransmission in wireless networks. Abdildin and Abbas use an
algorithm for excluding redundant assessments in a multi-attribute utility problem.
On the modeling and simulation developments, Yang and Luan study modeling of a pulsating heat pipe and
startup asymptotics. Schmidt provides a nansocale analysis of the sheet metal structures in terms of plasticity and
fracture. Rechistov et al. present a simulation and performance study of large scale computer cluster configuration.
Furthermore, Gao et al. discuss the modeling and simulation of a decentralized mission control algorithm for a fleet
of collaborative UAVs.
As we can see, the contributions to this workshop involve a wide range of applications. Despite these active
advances, some questions remain unanswered, and this is special true in the area metaheuristics. For example, the
convergence analysis of most metaheuristic algorithms lacks behind, and only limited results exist about a few
metaheuristics. In addition, the No-Free-Lunch (NFL) theorems were proved for finite domains for single objective
optimization [18], and it remains unsolved for multiobjective optimization. A recent significant development is that
for continuous optimization or optimization with infinite domains, NFL does not hold [19]. This means there is
some free lunch for continuous optimization, which needs more extensive study. In the area of surrogate-based
optimization, several issues are the subjects of on-going research. These include the development of more efficient
surrogate modeling methodologies, improving the robustness of the surrogate-based optimization algorithms, as well
as the proper selection of low-fidelity models for variable-fidelity and variable-resolution optimization techniques.
From the above studies, we can see that the applications of computational optimization and modeling are diverse
and wide-ranging. There is no doubt that more and more applications will appear in the near future. This workshop
provides a timely platform for further discussions and development in optimization and modeling.

856

Xin-She Yang et al. / Procedia Computer Science 9 (2012) 852 – 856

References
1.
2.
3.
4.
5.
6.
7.
8.
9.

10.
11.
12.
13.
14.
15.

16.
17.
18.
19.

K. Deb, Optimization for Engineering Design: Algorithms and Examples, Prentice-Hall, New Delhi, 1995.
E. G. Talbi, Metaheuristics: From Design to Implementation, John Wiley & Sons, 2009.
X. S. Yang, Engineering Optimization: An Introduction with Metaheuristic Applications, John Wiley & Sons, (2010).
Queipo, N.V., Ha
Progress in Aerospace Sciences, Vol. 41, No. 1, 2005, pp. 1-28.
S. Koziel, J. W. Bandler and K. Madsen, Quality assessment of coarse models and surrogates for space mapping optimization,
Optimization and Engineering, 9(4), 375-391 (2008).
L. Leifsson and S. Koziel, Multi-fidelity design optimization of transonic airfoils using physics-based surrogate modelling and
shape-preserving response prediction, J. Comp. Science, 1(2), 98-106 (2010).
X. S. Yang, Introduction to Computational Mathematics, World Scientific Publishing, Singapore, (2008).
Statistics and Computing, vol. 14, no. 3, pp. 199-222, Aug.
2004.
V.K. Devabhaktuni, B. Chattaraj, M.C.E. Yagoub, and Q.Advanced microwave modeling framework exploiting
automatic model generation, knowledge neural networks, and space mapping IEEE Trans. Microwave Theory Tech., vol. 51, no. 7,
pp. 1822-1833, July 2003.
J.W. Bandler, Q.S. Cheng, S.A. Dakroury, A.S. Mohamed, M.H. Bakr, K. Madsen, and J. Sondergaard, Space mapping: the state of
the art, IEEE Trans. Microwave Theory Tech., 52(1), 337-361, 2004.
S. Koziel and X. S. Yang, Computational Optimization and Applications in Engineering and Industry, Springer, (2011).
Koziel, S., EcheverríaKoziel and X.S. Yang (Eds.) Computational
Optimization, Methods and Algorithms, Series: Studies in Computational Intelligence, Springer-Verlag, pp. 33-60, 2011.
A.I.J. Forrester, A.J. Keane, A.J., Recent advances in surrogate-based optimization, Prog. in Aerospace Sciences, 45 (1-3), 50-79,
2009.
Alexandrov, N.M., Lewis, R.M.,
-Fidelity Models
38th Aerospace Sciences Meeting & Exhibit, Reno, NV, AIAA Paper 2000-0841, Jan. 2000.
ptimization Using Surrogate Models and Partially Converged Computationally Fluid
Proceedings of the Royal Society A: Mathematical, Physical and Engineering Sciences, Vol. 462, No. 2071, 2006,
pp. 2177-2204.
Leifsson, L., and Koziel,
-fidelity design optimization of transonic airfoils using physics-based surrogate modeling and
shapeJournal of Computational Science, Vol. 1, No. 2, 2010, pp.98-106.
Toal, D.J.J., and Keane, A.J., Efficient multipoint aerodynamic design optimization via cokriging Journal of Aircraft, vol. 48,
no.5, 2011, pp. 1685-1695.
D. H. Wolpert and W. G. Macready, No free lunch theorems for optimization, IEEE Trans. Evol. Computation, 1, 67-82 (1997)
A. Auger and O. Teytaud, Continuous lunches are free plus the design of optimal optimization algorithms, Algorithmica, 57(1), 121146 (2010).

