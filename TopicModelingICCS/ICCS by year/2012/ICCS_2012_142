Available online at www.sciencedirect.com

Procedia Computer Science 9 (2012) 577 – 586

International Conference on Computational Science, ICCS 2012

Similarity Graph Neighborhoods for Enhanced Supervised
Classiﬁcation
Anirban Chatterjee, Padma Raghavan
Department of Computer Science and Engineering, The Pennsylvania State University

Abstract
We consider transformations to enhance classiﬁcation accuracy that are applied during the training phase of a
supervised classiﬁer with prelabeled data. We consider similarity graph neighborhoods (SGN) in the feature subspace
of the training data to obtain a transformed dataset by determining displacements for each entity. Our SGN classiﬁer is
a supervised learning scheme that is trained on these transformed data; the separating boundary obtained is thereafter
used for future classiﬁcation. We discuss improvements in classiﬁcation accuracy for an artiﬁcial dataset and present
empirical results for Linear Discriminant (LD) classiﬁer, Support Vector Machine (SVM), SGN-LD, and SGN-SVM
for 6 well-known datasets from the University of California at Irvine (UCI) repository [1]. Our results indicate that
on average SGN-LD improves accuracy by 5.0% and SGN-SVM improves it by 4.52%.
Keywords: graph neighborhoods, feature subspace, support vector machine, improved classiﬁcation

1. Introduction
Supervised classiﬁcation using Linear Discriminant classiﬁers (LD) [2, 3, 4] or Support Vector Machines (SVM) [5,
6, 7, 8] are eﬀective machine learning techniques for data classiﬁcation. Supervised learning algorithms, such as LD
or SVM, utilize similarity in the data as a criterion to build eﬀective classiﬁcation rules [9]. Such schemes typically
use a measure of distance in the high-dimensional space to quantify this similarity [10]. However, when the number of
features are large, then similarity measures can be ambiguous and represent false relationships between entities [11].
In such scenarios, we need to understand the strength of the similarity to be able to classify entities correctly. Usually,
the neighborhood of an entity plays a critical role in deciding the strength of a similarity [12]. The similarity between
entities is typically represented by the similarity graph whose vertices represent entities and the edge weights indicate
the strength of the similarity between entities. In forming the similarity graph, we are faced with a two-fold problem,
(i) the similarity graph is dense [13], and (ii) computing the similarity graph is quadratic in the number of observations
(N), which can be expensive when N is large. Therefore, our objective is to determine a sparse neighborhood of an
entity that represents the similarity relationships most relevant for achieving high classiﬁcation accuracy.
In this paper, we propose a data transformation scheme to enhance accuracy of two popular supervised classiﬁcation schemes, LD and SVM. We determine a sparse similarity graph G from the labeled training data A to reveal
Email addresses: achatter@cse.psu.edu (Anirban Chatterjee), raghavan@cse.psu.edu (Padma Raghavan)

1877-0509 © 2012 Published by Elsevier Ltd. Open access under CC BY-NC-ND license.
doi:10.1016/j.procs.2012.04.062

578

Anirban Chatterjee and Padma Raghavan / Procedia Computer Science 9 (2012) 577 – 586

a connectivity in the data based on their features. Subsequently, we utilize local neighborhoods of an entity in this
sparse similarity neighborhood graph (SGN) of the training data to transform the high-dimensional feature space to
˜ The position of an entity is transformed by a resultant displacement computation using the local neighborhood.
A.
We train a supervised classiﬁer, namely LD or SVM, on the transformed data to obtain the classiﬁcation boundary,
which is subsequently used for classiﬁcation of unlabeled data in the testing phase. We refer to this combination of
SGN data transformation with the LD or SVM classiﬁer as SGN-LD or SGN-SVM, respectively. We evaluate the
accuracy of our schemes SGN-LD and SGN-SVM on a suite of 6 well-known UCI [1] datasets and compare it with
a popular implementation of LD [14] and SVM [7]. Furthermore, we empirically characterize the sensitivity of classiﬁcation accuracy to choices of parameter values in SGN-SVM and present measures related to the sparsity of the
similarity graph neighborhoods. These measures indicate that the computational costs of our SGN transformations
remain proportional to the size of the original data set.
The remainder of this paper is organised as follows. We present background and give an overview of LD and SVM
in Section 2. In Section 3 we describe the main steps of our SGN transformation and its application to LD or SVM.
We present and discuss our empirical results in Section 4. Section 5 presents related research and Section 6 concludes
with a summary of our ﬁndings.
2. Preliminaries
In this section, we introduce our notation and give a brief overview of the support vector machine including its
primal and dual formulation.
2.1. Notation.
We represent matrices using bold upper-case alphabets, e.g., A. A lower-case alphabet with an arrow denotes a
vector, e.g., x. The notation xi j refers to the j-th component of the i-th vector in a set of n vectors {xi }n . We use || · ||2
to represent the two norm of a vector. R denotes the set of real numbers.
2.2. Supervised classiﬁcation.
Supervised classiﬁcation techniques use prelabeled data to learn decision models that can subsequently be applied
to classify unlabeled data. In this section, we provide a brief overview of two popular supervised learning schemes:
(i) Linear Discriminant (LD) classiﬁer, and (ii) Support Vector Machine (SVM). LD and SVM represent two distinct
types of classiﬁers; the former relies on constructing a covariance based discriminant function and the latter learns
coeﬃcients of a separating plane in high-dimensions to classify future data.
2.2.1. Linear Discriminant Classiﬁcation.
The goal of a Linear Discriminant (LD) classiﬁer is to express a predictor variable as a linear combination of
diﬀerent features in the data. Consider a set of n samples xi ı = 1, . . . , n and their corresponding class vector y, where
yi ∈ {0, 1} for binary classiﬁcation. LD is based on the assumption that the class conditional probability distributions
are normally distributed with mean and covariance (μ0 , Σ0 ) and (μ1 , Σ1 ). Additionally, LD imposes the condition that
all classes of data in the training set have the same covariance parameter, that is, Σ0 is equal to Σ1 . Classiﬁcation for
LD is obtained by computing the log-likelihood ratios for the two classes, and assigning samples to the second class
if this ratio is below a threshold τ. Equation 1 represents the LD classiﬁcation criteria, where Σ0 = Σ1 .
T −1
(x − μ0 )T Σ−1
0 (x − μ0 ) − (x − μ1 ) Σ1 (x − μ1 ) < τ

(1)

2.2.2. Support Vector Machine (SVM).
Supervised learning using support vector machines (SVM) was ﬁrst proposed by Vladimir Vapnik as a support
vector regression [15, 16] method. Subsequently, SVM gained popularity as a classiﬁer that requires only a subset of
the training points to build maximally separated hyperplanes to classify the data.
Consider a training set of data (x1 , y1 ), (x2 , y2 ),. . .,(xn , yn ) with xi ∈ Rn and yi ∈ {−1, +1}. Deﬁne ξi as the training
error associated with the ith data point and C as a constant deﬁning a bound on the training error. Formulate and solve
an optimization problem that seeks to maximize the separation between separating hyperplanes. The goal is to ﬁnd w,

Anirban Chatterjee and Padma Raghavan / Procedia Computer Science 9 (2012) 577 – 586

579

which is the vector of weights that deﬁnes the separating plane wT x = 0. In a binary SVM classiﬁcation problem, a
simple classiﬁcation rule h(w) = sign(wT x + b) is used to classify the given test data, where b can be easily modeled.
Equation 2 represents the primal form of the problem.
1 T
C
w w+
2
n

min

w,ξi ≥0

n

ξi

(2)

i=1
T

s.t.∀i ∈ {1, · · · , n} : yi (w xi )

≥

1 − ξi

Alternatively, recent implementations of SVM [7] solve equivalent dual formulation of the primal problem (stated in
Equation 2). Equation 3 presents the dual formulation of the primal SVM optimization problem. The variables αi ’s
indicates the i-th lagrangian multiplier and ci ∈ {0, 1} indicates whether the constraint is active (ci = 1) or inactive
(ci = 0).
max
α≥0

||ci ||1 αi −

1
n
i

1
2

αi α j xiT x j
i

j

αi ≤ C

s.t.

(3)

i

There are multiple implementations of the SVM classiﬁer available [5, 7, 17, 8, 6]. For our research, we use
Joachim’s SVM-Perf [7] package to solve the SVM dual optimization problem.
3. Exploiting Similarity Graph Neighborhoods for Enhancing SVM Accuracy
In this section, we develop our main contribution, namely the SGN-LD and SGN-SVM methods. Key steps in
SGN-LD or SGN-SVM concern: (i) determining a parameterizable γ entity neighborhood in the high dimensional
feature space of training dataset A through speciﬁcation of a sparse graph G(B, A), which is our similarity graph, (ii)
˜ from the original A, through displacements to entities using γ entity
a training data transformation scheme to obtain A
neighborhoods in the similarity graph G(B, A), (iii) training an LD or SVM classiﬁer on the transformed data matrix
˜ and (iv) classifying the test data. These steps are presented in detail in the remainder of this section.
A,
3.1. Determining γ-Neighborhoods in Similarity Graph G(B, A).
Consider a training dataset A ∈ Rn×r with n entities and r features represented by an n × r sparse matrix, whose
entities have a preassigned label ti ∈ {−1, 1} ∀ i = 1, · · · , n, where -1 and 1 each represent a class. The i-th row
ai , of the matrix A, represents the feature vector of the i-th entity in the dataset. Thus, we can view entity i as being
embedded in the high dimensional feature space (of dimension ≤ r) with coordinates given by the feature vector ai .
We consider the training data A and form matrix B ∈ Rn×n whose nonzero entries express similarity strength
between two entities ai and a j in A. Each element bi j , in B, is a similarity strength between pairs ai and a j computed
as shown in Equation 4. Function F (ai , a j ) represents a similarity function of ai and a j . For example F (ai , a j ) could
be replaced by a simple dot product similarity measure given by ai · a j .
bi j

=

F (ai , a j )

(4)

We view the similarity graph G(B, A) as the adjacency graph representation of B obtained from dataset A. Computing
the similarity function for all (ai , a j ) pairs results in a dense graph, hence, we focus on maintaining the sparsity of B
by preserving as nonzero only those elements bi j that satisfy the following two conditions. First, the |bi j | is greater
than φi , where φi is a threshold related to the mean over all elements of row i of B. Second, the entities associated
with bi j , i.e., ai and a j have q or more features in common. With appropriate choices for φ and q the matrix B will
be relatively sparse. It can then be stored in the compressed sparse row format (CSR [18] which stores only non-zero
edges), such that neighborhood information can be retrieved in constant, O(1), time.
In practice, F (ai , a j ) can be replaced by appropriate domain speciﬁc similarity measures that model similarity in
the training data better. For example in text mining, document similarity is typically represented as an inner product

580

Anirban Chatterjee and Padma Raghavan / Procedia Computer Science 9 (2012) 577 – 586

distance [9]. Therefore, we use the generic term “similarity” in our description of the method, as our SGN transformation is not limited by any particular similarity measure. Figure 1 illustrates the process of constructing the
neighborhood graph using a simple example.
Next, we deﬁne an entity neighborhood for ai in the similarity graph G(B, A) as the set of p entities {a j } p , such
that, ai and a j share an edge in G(B, A). We further generalize the notion of neighborhood by deﬁning a reach set,
γ-neighbor(ai ), for an entity ai as follows.
γ-neighbor(ai ){a j : reach(ai , a j ) ≤ γ ∀ j = 1, . . . , n.}

(5)

In the equation above, reach(ai , a j ) indicates a path from ai to a j in G(B, A) whose length is no more than γ. Now,
increasing reach with higher γ will typically result in larger neighborhoods; γ is viewed as a tunable parameter in our
method.

Figure 1: Forming the sparse γ-neighborhood similarity graph G(B, A) from A. F(A) represents the transformation described in Section 3.1. G(B, A)
is a weighted graph representation of matrix B.

3.2. Transforming Training Data through Entity Displacement Vectors.
˜ using a vector measure obtained from γ-neighborhoods
In this step, we consider transforming training data A to A
in G(B, A). Each entity ai ∈ Rr is viewed as embedded in an r-dimensional feature space, where r ≥ 2. The geometric
information for ai is obtained from A where aik represents the coordinate of the i-th entity in the k-th dimension
(k ≤ r). Using this information we ﬁrst construct the displacement vector Δi j between ai and a j ∀a j ∈ γ-neighbor(ai ).
Equation 6 represents the displacement vector for entity pair (ai , a j ).
Δi j

=

a j − ai

(6)

We multiply the displacement vector by the product ti t j , where ti , t j ∈ {−1, 1} indicate the class of ai and a j , respectively. Equation 7 represents the modiﬁed displacement vector.
Δi j

=

ti t j Δ i j

(7)

When ai and a j belong to the same class then the product ti t j equals 1, while ti t j equals -1 when ai and a j belong
to diﬀerent classes. Therefore, entities belonging to the same class are displaced towards each other and entities
belonging to the diﬀerent classes are displaced away from each other.
We compute the resultant displacement vector for entity ai as the weighted sum of all displacement vectors Δi j ,
j ∈ γ-neighbor(ai ). Thus the resultant displacement ρi is,
ρi

bi j Δi j .

=

(8)

j∈neighbor(ai )

For a γ > 1, we would also include, in the resultant calculation, other entities that are within a path length of less than
or equal to γ from ai .
Subsequently, we seek to update the position of ai in the direction governed by the unit vector ρˆ i = ||ρρii||2 . The
resultant direction for ai is typically directed towards the entities with stronger similarity with ai , therefore, we claim

Anirban Chatterjee and Padma Raghavan / Procedia Computer Science 9 (2012) 577 – 586

581

that this direction √is the direction in which similarity between related entities increases. We also introduce a normalizing factor K = N to scale ρi such that the step size for updating ai does not increase arbitrarily. We compute the
step size βi as shown in Equation 9.
βi

||ρi ||2
K

=

(9)

Therefore, β is essentially the magnitude of the displacement vector scaled by a constant K to limit the displacement
from growing arbitrarily large.
We update the position of entity ai in the direction of increasing similarity ρˆ i with a step size of βi . Equation 10
represents the updated position entity ai .
ai

=

ai + βi ρˆ i

(10)

In our formulation, the rate at which similar entities are moved closer to each other is high, therefore, we seek to
observe an enhanced separation amongst unrelated entities. Furthermore, similar entities move close to each other
while maintaining their positions relative to each other.
Figure 2(a) shows the result of our SGN data transformation scheme when applied to a simple graph. Figure 2(b)
˜ from the positions in the transformed high-dimensional feature space.
illustrates obtaining a transformed dataset A

˜ (b) We obtain new entity coordinates A
˜ from the graph
Figure 2: (a) Transforming the training data A using G(B, A) to a new graph G(B, A).
˜
G(B, A).

3.3. Training an LD or SVM on Transformed Data.
We consider the process of training two diﬀerent supervised classiﬁers, LD and SVM, on the transformed data.
LD training. We obtain the class mean and covariance parameter estimates from the transformed data. These estimates are required to construct the discriminant function for the LD classiﬁer.
˜ to produce separating hyperplanes characSVM training. We train an SVM [7] (as shown in Equations 2 and 3) on A
terized by a set of support vectors S . The set S is a subset of the transformed training data A˜ and marks the separating
boundary between the two-classes. Typically, the size of the set S is small relative to n, which is the size of the training
set A.
Using matrix notation, we rewrite Equation 3 as shown in Equation 11, where Q ∈ Rn×n is typically referred to as
the kernel matrix [9], c = {ci : ci ∈ (0, 1) ∀i = 1 . . . n}, and α is the vector of lagranges multipliers [19].
max
α≥0

1 T
nc α

− 12 αT Qα

(11)

In Equation 11, often the (i, j)-th entry qi j of Q is represented as the inner product of entities ai and a j (qi j = ai a j ).
However, qi j can be formed using other functions of ai and a j , such as the radial basis kernel and many others.

582

Anirban Chatterjee and Padma Raghavan / Procedia Computer Science 9 (2012) 577 – 586

˜ to obtain separating hyperplanes (shown in white).
Figure 3: Training an SVM on the transformed data matrix A

This technique, referred to as kernel trick [9], is useful in many situations where we have prior knowledge about the
application that generates the data.
Our selection of the similarity function F (ai , a j ) (or equivalently bi j ) is dependent on the choice of SVM kernel
function. This ensures that the transformation happens using the same similarity measure (kernel) as used by the
SVM optimization problem to avoid any form of ambiguity. Therefore, our transformation seeks to transform the
entity space such that it directly impacts the kernel matrix Q in the SVM formulation.
3.3.1. An example of SGN transformation using the radial basis function.
In the context of SVM learning, our SGN data transform modiﬁes Q by changing the position of entities in the
high-dimensional feature space. Our SGN transformation brings related entities closer, thereby decreasing the quantity
||ai − a j ||2 . For the purpose of explaining the impact of SGN on the Q, we consider that the radial basis function (RBF)
based kernel is used to form Q. Equation 12 represents a form of the radial basis function.
qi j

=

e−||ai −a j ||2
2

(12)

We observe that as (||ai − a j ||2 ) → 0 the quantity qi j → 1, and as (||ai − a j ||2 ) → ∞ the quantity qi j → 0. Modifying
Q using SGN, therefore, impacts the properties of Q and the convex optimization problem deﬁned in Equation 11.
Figure 3 illustrated the SVM training process using our simple example.
3.4. Classifying Test Data.
In the case of LD, the discriminant function that determines the separating boundary is used to classify test data.
In a two class problem, a test data sample is assigned to the second class if the log-likelihood ratio (as shown in
Equation 1) is below a certain threshold τ.
ˆ are directly mapped to the
In case of SVM, the support vectors obtained in the transformed training data A
corresponding entities in the original training data A. These mapped entities form the new support vector for the
training data A. For the testing phase, consider a test set X ∈ Rnx ×r . The similarity F (xi , s j ) of test sample xi is
computed with the set of modiﬁed k support vectors S = {s j } j = 1 . . . k. Since the set of support vectors is small and
sparse, computing similarity is constant time. We determine the support vector sk that has maximum similarity with
xi . We assign xi the same class as sk .
4. Evaluation and Discussion
In this section, we ﬁrst describe our experimental setup followed by evaluation of SGN-LD and SGN-SVM ﬁrst
on an artiﬁcial dataset then using a set of benchmark datasets. We also report on sensitivity of our method to the input
parameter γ (number of levels in G(B, A) used to decide neighborhood), the sparsity of B and the dimensions used to
form the dataset.
4.1. Experimental Setup and Metrics for Evaluation.
We implemented our SGN scheme using Matlab [14] and a sample implementation is available for academic
purposes. For our experiments, we use radial basis similarity for building the neighborhood similarity graph. The
δi value is set for each entity ai as the mean of all neighbor similarities and any radial basis similarity less than this
value are considered zero. We use the LD implementation as provided in the Matlab Statistics Toolbox [14] and SVMPerf [7] as a state-of-art two-class support vector machine for obtaining the SVM hyperplane. The base case, that is,

583

Anirban Chatterjee and Padma Raghavan / Procedia Computer Science 9 (2012) 577 – 586

training LD and SVM-Perf on the original training data and recording the accuracy of classiﬁcation is referred to as
LD and SVM, respectively. The training error bound parameter is set to 20. Finally, we use classiﬁcation accuracy and
F1-Score as metrics for comparing the potential beneﬁts of SGN-SVM over SVM for 6 benchmarks datasets described
in Table 1. The classiﬁcation accuracy is computed as the percentage of correctly classiﬁed test data samples out of
the total testing data. The F1-Score is computed as the harmonic mean of the precision and recall.
4.2. Evaluation using Synthetic Data.
We now discuss the impact of SGN-LD and SGN-SVM using an example of the fourclass dataset [20]. This
dataset, created by Kleinberg et al. [20], represents artiﬁcially created 862 data points in two dimensions in which
each attribute is linearly scaled to [−1, 1]. This dataset, that had four classes originally, was transformed to two
classes that are not easily separable as they have irregular spread over the space. We observe that SGN-LD obtains a
higher accuracy of 77.91% compared to LD that obtains 73.20%. Similarly, SGN-SVM obtains a higher accuracy of
94.47% compared to SVM that obtains 75.87%.
4.3. Empirical Results on Benchmark Datasets.
We use a suite of 6 benchmark datasets chosen from the UCI [1] repository for our experiments. Table 1 presents
the datasets with the number of observations and number of features in each dataset. Each dataset is randomly split to
form data for training (60%) and data for testing (40%).
Dataset
australian
heart
liver
sonar
splice
german

Observations
690
270
345
208
1,000
1,000

Features
14
13
6
60
60
24

Table 1: Description of UCI datasets.

Dataset
australian
heart
liver
sonar
splice
german

LD
86.23
84.63
64.93
68.67
78.25
71.75

Accuracy
SGN-LD SVM
89.86
85.87
87.96
77.78
67.25
57.25
74.70
79.52
80.50
73.50
75.50
68.75

SGN-SVM
82.25
79.63
63.62
86.27
73.75
69.50

LD
86.74
84.27
64.84
70.58
78.33
70.67

F1-Score
SGN-LD SVM
89.91
85.88
88.13
77.51
66.40
50.26
74.80
81.75
80.50
69.54
71.30
61.51

SGN-SVM
82.15
78.94
62.82
86.63
74.62
63.86

Table 2: Classiﬁcation accuracy (in percentage) and F1-Score of LD, SGN-LD, SVM, and SGN-SVM for benchmark datasets.

Tables 2 present classiﬁcation accuracy and F1-Score results for our 6 benchmark datasets. Figures 4(a) and
4(b) present percentage improvements in accuracy obtained using SGN-LD and SGN-SVM on the synthetic and
benchmark datasets. SGN-LD improves LD classiﬁcation accuracy on average by 5% and F1-Score by 3.44%. SGNSVM performs at par or better than SVM in 5 out of 6 datasets with an average improvement in accuracy of 4.52%
and an improvement in F1-Score of 8.09%. In case of the australian dataset SVM outperforms our SGN-SVM. This
dataset has a heavily unequal distribution of labeled data and any one class is more heavily represented than the other.
This adversely aﬀects the support vectors that SGN-SVM learns as it is biased towards a particular class of data.

584

Anirban Chatterjee and Padma Raghavan / Procedia Computer Science 9 (2012) 577 – 586

(a)

(b)

Figure 4: Percentage improvements in accuracy and F1-score with (a) SGN-LD over LD, and (b) SGN-SVM over SVM.

4.4. Why does classiﬁcation improve with our SGN transformation?
Based on our empirical evaluation of SGN-LD and SGN-SVM in earlier sections, we observe that our SGN
transformation improves classiﬁcation accuracy. We intuitively believe that our SGN transformation could potentially
help separate clusters that may appear very close in a high dimensional feature space. In this regard, the question we
pose is that, does increasing separation between the class centers improve classiﬁcation accuracy?
Earlier Hopcroft √
et al. [21] showed that two points generated by the same Gaussian process in d-dimensions
2d apart and distance between two points belonging to diﬀerent Gaussian processes would be
would
be
a
distance
√
2d + δ2 , where δ is the separation between the two centers. We conjecture that our SGN transformation increases
ˆ
the separation between the means of the diﬀerent classes, thus increasing the initial separation δ to a larger δ.

Figure 5: Illustration of two 4-dimensional Gaussian processes (a) before transformation and (b) after SGN transformation, that have been projected
to 2 PCA dimensions for the purpose of visualization. In both cases, the boundary is obtained using LD. Along side are the correlation matrices
before and after the transformation. The dashed boxes indicate the feature pairs that showed a signiﬁcant change in the correlation value after the
SGN transformation.

In case of SGN-SVM, the separation in the two classes of data aﬀects the similarity value between diﬀerent pairs
of samples, consequently, changing the decision boundary. Alternatively, LD relies on the sample covariance and
mean of the training data. To explore this in greater detail, we construct training and testing sets from two Gaussian
processes in 4-dimensions. Figure 5 visualizes the data in 2 principal component directions. Additionally, we show
the heatmap of the correlation matrix of the 4 features. We observe that for certain pairs of features (indicated by
dashed boxes) the correlation between features change during the SGN transformation indicating that some features
become more inﬂuential than others, consequently explaining the change in decision boundary.
4.5. Sensitivity of SGN-SVM Classiﬁcation Accuracy.
We study the eﬀect of varying the value of SGN input parameters on the classiﬁcation accuracy of SVM.
4.5.1. Eﬀect of growing or shrinking the neighborhood on classiﬁcation accuracy.
We study the eﬀect of increasing the neighborhood levels γ on the classiﬁcation accuracy. Figure 6(a) presents,
for 3 datasets, the accuracy of classiﬁcation as the γ parameter increases. We observe that accuracy improvements are

585

Anirban Chatterjee and Padma Raghavan / Procedia Computer Science 9 (2012) 577 – 586

(a)

(b)

(c)

Figure 6: (a) Eﬀect of increasing the entity neighborhood parameter γ in G(B, A) (considering larger neighborhoods) on classiﬁcation accuracy.
When the sparse neighborhood is a good approximation to indicate similarity, adding elements to the neighborhood does not alter classiﬁcation
accuracy. Eﬀect of varying number of common features q to create the similarity graph B on (b) sparsity of B relative to A and (c) impact on overall
classiﬁcation accuracy.

obtained with a relatively small number of levels γ in the range 1 to 3 in G(B, A). This observation indicates that the
local structure in a dataset can lead to improved training and thus improved classiﬁcation.
4.5.2. Eﬀect of changing sparsity of G(B, A) on classiﬁcation accuracy.
We next study the eﬀect of changing the sparsity of B and the eﬀects a sparser B has on the classiﬁcation accuracy
for 3 datasets from our benchmark suite.
In our similarity graph G(B, A) an edge exists between entities ai and a j only if they share q or more common
features. As expected, in Figure 6(b) we observe that the number of nonzeros in B relative to those in A decreases as
q increases, i.e., B becomes sparser for larger q. Plots in Figure 6(c) indicate that even with higher values of q, and
thus a sparser B, the accuracy of SGN-SVM is not adversely aﬀected.
These observations indicate that our SGN-SVM is not overly sensitive to relatively small parameter changes.
5. Related Research
In this section, we present brieﬂy an overview of related research that use the inﬂuence of node neighborhood in
graphs to improve classiﬁcation.
S. Chakrabarti et al. [22, 23] propose a greedy graph labeling algorithm that iteratively corrects the labeling by
considering the neighborhood around nodes. In an initial phase, it computes class probabilities for a node using
a Naive Bayes classiﬁer and then re-evalutes the probabilities after the probabilities of the neighboring nodes are
known. Angelova et al. [24] extend and generalize this method using the theory of Markov Random Fields to propose
a relaxation based graph labeling technique. This technique uses diﬀerent levels of trust in diﬀerent neighbors by
assigning node weights based on the similarity between neighbors.
Oh et al. [25] propose a method that assigns labels to each node of a graph by considering the popularity of the
node among all it’s immediate neighbors.
In a regression model proposed by Lu and Getoor [26], the text features of a document are used in combination
with the labels of immediate neighbors to improve classiﬁcation. The feature vector for a node is modiﬁed using
feature information from either all neighboring nodes or the most popular node amongst the neighbors.
Castillo et al. [27] address the problem of web spam detection in which they improve classiﬁcation accuracy using
dependencies among labels of neighboring hosts in the Web graph.
Our SGN data transformation derives motivation from the above earlier work and proposes a novel entity feature update technique based on resultant-displacement using neighborhood connectivity. Additionally, we unify our
approach with the learning phase of an LD or SVM classiﬁer to improve classiﬁcation.

586

Anirban Chatterjee and Padma Raghavan / Procedia Computer Science 9 (2012) 577 – 586

6. Conclusions
In this paper, our main contribution is a γ-neighborhood based training data transformation scheme (SGN) using
similarity graphs to displace entities in high-dimensional feature space. We unify our SGN data transformation with
the learning phase of either linear discriminant analysis classiﬁer or a support vector machine to form SGN-LD or
SGN-SVM, respectively. We show, that on average our SGN-LD obtains 5.00% better accuracy than traditional LD
and SGN-SVM obtains 4.52% better accuracy than traditional SVM on a set of 7 datasets. Additionally, we present
analysis on two aspects of our algorithm: (i) the eﬀects of growing or shrinking the neighborhood size γ, and (ii) the
eﬀects of sparsity of the γ-neighborhoods on classiﬁcation accuracy. Our analysis shows that our SGN-SVM improves
accuracy even with relatively sparse similarity graphs and small γ-neighborhoods.
7. Acknowledgement
This research was supported in part by the National Science Foundation through grants CCF-0830679, OCI0821527, and CNS-0720749.
References
[1] A. Asuncion, D. Newman, UCI machine learning repository (2007).
URL http://www.ics.uci.edu/∼mlearn/MLRepository.html
[2] R. A. Fisher, The use of multiple measurements in taxonomic problems, Annals Eugen. 7 (1936) 179–188.
[3] J. H. Friedman, Regularized discriminant analysis, Journal of the American Statistical Association 84 (1989) 165–175.
[4] S. Mika, G. Ratsch, J. Weston, B. Scholkopf, K. R. Mullers, Fisher discriminant analysis with kernels, IEEE, 1999, pp. 41–48.
[5] C. C. Chang, C. J. Lin, LIBSVM: a library for support vector machines (2001).
[6] E. Y. Chang, K. Zhu, H. Wang, H. Bai, J. Li, Z. Qiu, H. Cui, Psvm: Parallelizing support vector machines on distributed computers, in: NIPS,
2007.
[7] T. Joachims, Training linear svms in linear time, in: KDD ’06: Proceedings of the 12th ACM SIGKDD international conference on Knowledge
discovery and data mining, ACM, New York, NY, USA, 2006, pp. 217–226. doi:http://doi.acm.org/10.1145/1150402.1150429.
[8] O. L. Mangasarian, D. R. Musicant, Lagrangian support vector machines, J. Mach. Learn. Res. 1 (2001) 161–177.
doi:http://dx.doi.org/10.1162/15324430152748218.
[9] C. M. Bishop, Pattern Recognition and Machine Learning (Information Science and Statistics), Springer, 2006.
[10] M. Aizerman, E. Braverman, , L. Rozonoer, Theoretical foundations of the potential function method in pattern recognition learning, Automation and Remote Control (1964) 821837.
[11] T. Oommen, D. Misra, N. K. C. Twarakavi, A. Prakash, B. Sahoo, S. Bandopadhyay, An objective analysis of support vector machine based
classiﬁcation for remote sensing, Mathematical Geosciences 40(4) (2008) 409.
[12] J. Sun, H. Qu, D. Chakrabarti, C. Faloutsos, Neighborhood formation and anomaly detection in bipartite graphs, in: In ICDM, 2005, pp.
418–425.
[13] J. C. Platt, Fast embedding of sparse music similarity graphs, in: In Advances in Neural Info. Proc. Systems, MIT Press, 2004, p. 2004.
[14] MathWorks, Matlab and simulink for technical computing (2007).
[15] V. N. Vapnik, The Nature of Statistical Learning Theory (Information Science and Statistics), Springer, 1999.
[16] C. J. Burges, A tutorial on support vector machines for pattern recognition, Data Mining and Knowledge Discovery 2 (1998) 121–167.
[17] M. C. Ferris, T. S. Munson, Interior point methods for massive support vector machines, SIAM Journal on Optimization 13 (2003) 783–804.
[18] A. George, J. Liu, Computer Solution of Large Sparse Positive Deﬁnite Systems, Prentice Hall, 1981.
[19] J. Nocedal, S. J. Wright, Numerical Optimization, Springer, 1999.
[20] T. K. Ho, E. Kleinberg, Building projectable classiﬁers of arbitrary complexity, International Conference on Pattern Recognition 2 (1996)
880.
[21] J. E. Hopcroft, S. Soundarajan, L. Wang, The future of computer science, (Submitted for publication).
[22] S. Chakrabarti, Mining the Web: Discovering Knowledge from HyperText Data, Sci. & Tech. Books, 2002.
[23] S. Chakrabarti, B. Dom, P. Indyk, Enhanced hypertext categorization using hyperlinks, in: SIGMOD ’98: Proceedings of
the 1998 ACM SIGMOD international conference on Management of data, ACM, New York, NY, USA, 1998, pp. 307–318.
doi:http://doi.acm.org/10.1145/276304.276332.
[24] R. Angelova, G. Weikum, Graph-based text classiﬁcation: learn from your neighbors, in: SIGIR ’06: Proceedings of the 29th annual
international ACM SIGIR conference on Research and development in information retrieval, ACM, New York, NY, USA, 2006, pp. 485–492.
[25] H.-J. Oh, S. H. Myaeng, M.-H. Lee, A practical hypertext catergorization method using links and incrementally available class information,
in: SIGIR ’00: Proceedings of the 23rd annual international ACM SIGIR conference on Research and development in information retrieval,
ACM, New York, NY, USA, 2000, pp. 264–271.
[26] L. Getoor, Link mining: a new data mining challenge, SIGKDD Explor. Newsl. 5 (1) (2003) 84–89.
[27] C. Castillo, D. Donato, A. Gionis, V. Murdock, F. Silvestri, Know your neighbors: web spam detection using the web topology, in: SIGIR
’07: Proceedings of the 30th annual international ACM SIGIR conference on Research and development in information retrieval, ACM, New
York, NY, USA, 2007, pp. 423–430.

