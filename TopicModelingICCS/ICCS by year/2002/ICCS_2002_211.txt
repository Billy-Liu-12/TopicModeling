Using a passage retrieval system to support question answering process1
Fernando Llopis, Antonio Ferrández and José Luis Vicedo

Departamento de Lenguajes y Sistemas Informáticos
University of Alicante
Alicante, Spain
{llopis, antonio, vicedo}@dlsi.ua.es

Abstract. Previous works in Information Retrieval show that using pieces of
text obtain better results than using the whole document as the basic unit to
compare with the user’s query. This kind of IR systems is usually called Passage Retrieval (PR). This paper discusses the use of our PR system in the question answering process (QA). Our main objective is to examine if a PR system
provide a better support for QA than Information Retrieval systems based in the
whole document..

1 Introduction
Given a user’s query, Information Retrieval (IR) systems return a set of documents,
which is sorted by the probability to contain the required information. Since IR systems return whole documents, there is an additional work for the user, who has to read
the whole documents to search for the required information. However, when searching
for specific information, this last user’s task can be carried out by Question Answering
systems (QA), which are tools that usually work on the output of an IR system, and try
to return the precise answer to the user’s query.
IR systems work is based on measuring the similarity between each document and the
query by means of several formulas that typically use the frequency of query terms in
the documents. This way of measuring causes that bigger documents could have more
chances to be considered relevant, because of its higher number of terms that could
coincide with those of the query.
In order to solve this problem, some IR systems measure the similarity in accordance
with the relevance of the pieces of adjoining text that form the documents, where these
1

This paper has been partially supported by the Spanish Government (CICYT) project number
TIC2000-0664-C02-02

pieces of text are called passages. This kind of IR systems, which are usually called
Passage Retrieval (PR), allows that the similarity measure is not affected by the size
of the document. PR systems return the precise piece of text where it is supposed to
find the answer to the query, a fact that is especially important when big documents
are returned. PR systems are more complex than IR, since the number of textual units
to compare is higher (each document is formed by several passages) and the number
of tasks they accomplish is higher too (above all when passage splitting is performed
after processing each query as it is proposed in [7]). Nevertheless, the complexity
added to the system is rewarded with a significant increase in the performance of the
system. For example, in [1] the improvement reaches a 20%, and in [7] it does a 50%.
As well as obtaining better precision than document retrieval systems, PR return the
most relevant paragraphs to a query, information that could be used as input by a QA
system. This fact allows reducing QA systems computational complexity (they work
on small fragments of text instead of documents) and improves QA’s precision, since
the passage list retrieved is more relevant to the query than the results of a document
retrieval system.
The PR system presented in this paper is called IR-n. It defines a novel passage selection model, which forms the passages from sentences in the document. IR-n has been
used in the last Cross-Language Evaluation Forum (CLEF-2001) and in the last Text
REtrieval Conference (TREC-2001) for the Question Answering track. Part of the
better performance obtained in TREC-2001 with reference to TREC-2000, is due to
IR-n.
The following section presents PR and QA backgrounds. Section 3 describes the architecture of our proposed PR system. In section 4, we give a detailed account of the test,
experiments and results obtained. Finally, we present the conclusions of this work.

2 Backgrounds in Question Answering and Passage Retrieval
Computational Linguistic community has shown a recent interest on QA, and it comes
after developing Information Extraction systems, which have been evaluated in Message Understanding Conferences (MUC). Specifically, the interest was shown when in
TREC-8, appears a new track on QA that tries to benefit from large-scale evaluation,
that was previously carried out on IR systems, in previous TREC conferences.
If a QA system wants to successfully obtain a user’s request, it needs to understand
both texts and questions to a minimum level. That is to say, it has to carry on many of
the typical steps on natural language analysis: lexical, syntactical and semantic. This
analysis takes much more time than the only statistical analysis that is carried out in
IR. Besides, as QA has to work with as much text as IR, and the user needs the answer
in a limited interval of time, it is usual that an IR system processes the query and after,
the QA system will continue with its output. In this way, the time of analysis is highly
decreased.

Some of the best present QA systems are the following: [2],[3],[10],[5] After studying
these systems, it seems agreeable the following general architecture, that is formed by
four modules, where document retrieval module is accomplished by using IR technology:
• Question Analysis.
• Document Retrieval.
• Passage Selection.
• Answer Extraction.
Different IR models are used for Document Retrieval stage in QA systems however,
the best results are obtained by systems that apply PR techniques for this task.
The main differences between different PR systems are the way that they select the
passages, that is to say, what they consider as a passage and its size. According to the
taxonomy proposed in [1], the following PR systems can be found: discourse-based
model, semantic model and window model. The first one uses the structural properties
of the documents, such as sentences or paragraphs (e.g. the one proposed in [9], [12])
in order to define the passages. The second one divides each document in semantic
pieces, according to the different topics in the document (e.g. those in [4]). The last
one uses windows of a fixed size (usually a number of terms) to form the passages [1],
[6].
It looks coherent that discourse-based models are more effective since they are using
the structure of the document itself. However, the greater problem of them is that the
results could depend on the writing style of the document author. On the other hand,
window models have the main advantage that they are simpler to accomplish, since
the passages have a previously known size, whereas the remaining models have to
bear in mind the variable size of each passage. Nevertheless, discourse-based and
semantic models have the main advantage that they return logic and coherent fragments of the document, which is quite important if these IR systems are used for other
applications such as Question Answering.
The passage extraction model that we are proposing allows us to benefit from the
advantages from discourse-based models since logic information units of the text, such
as sentences, form the passages. Moreover, another novel proposal in our PR system is
the relevance measure, which unlike other discourse-based models, is not calculated
from the number of passage terms, but the fixed number of passage sentences. This
fact, allows a simpler calculation of this measure unlike other discourse-based or semantic models. Although we are using a fixed number of sentences for each passage,
we consider that our proposal differs from the window models since our passages does
not have a fixed size (i.e. a fixed number of words) because we are using sentences
with a variable size.

3 System overview
In this section, we describe the architecture of the proposed PR system, namely IR-n,
focusing on its two main modules: the indexation and the document extraction modules.
3.1 Indexation module
The main aim of this module is to generate the dictionaries that contain all the required information for the document-extraction module. It requires the following information for each term:
• The number of documents that contain the term.
• For each document:
− The number of times that the term appears in the document.
− The position of each term in the document: the number of sentence and position in the sentence.
Where we consider as terms, the stems produced by the Porter stemmer on those
words that do not appear in a list of stop-words, list that is similar to those used in IR
systems. For the query, the terms are also extracted in the same way, that is to say,
their stems and positions in the query for each query word that does not appear in the
list of stop-words.
3.2 Document extraction module
This module extracts the documents according to its similarity with the user’s query.
The scheme in this process is the following:
1. Query terms are sorted according to the number of documents in which
they appear, where the terms that appear in fewer documents are processed
firstly.
2. The documents that contain some query term are extracted.
3. The following similarity measure is calculated for each passage p with the
query q:
Similarity_measure(p, q) =

∑



∈ ∧


Wp,t * Wq,t

Where:
Wp,t = loge(fp,t + 1).
fp,t is the number of times that the term t appears in the passage p.
Wq,t= loge(fq,t + 1) * idf.
fq,t is the number of times that the term t appears in the query q.
idf = loge(N / ft + 1).
N is the number of documents in the collection.

ft is the number of documents that contain the term t.
4. Each document is assigned the highest similarity measure from its passages.
5. The documents are sorted by their similarity measure.
6. The documents are presented according to their similarity measure.
As it is noticed, the similarity measure is similar to cosine measure presented in [11].
The only difference is that the size of each passage (the number of terms) is not used
to normalise the results. This difference makes the calculation simpler than other discourse-based PR systems or IR systems, since the normalization is accomplished according to a fixed number of sentences per passage. Another important detail to notice
is that we are using N as the number of documents in the collection, instead of the
number of passages. That is because in [7] it is not considered relevant for the final
results.
The optimum number of sentences to consider per passage is experimentally obtained.
It can depend on the genre of the documents, or even on the type of the query as it is
suggested in [6]. We have experimentally considered a fixed number of 20 sentences
for the collection of documents in which we worked [8]. Table 1 presents the experiment where the 20 sentences per passage obtained the best results.

Precision IR-n
Recall 5 Sent.
10 Sent.
15 Sent.
20 Sent.
0.7343
0.00
0.6378
0.6508
0.6950
0.5516
0.10
0.5253
0.5490
0.5441
0.4891
0.20
0.4204
0.4583
0.4696
0.3964
0.30
0.3372
0.3694
0.3848
0.2970
0.40
0.2751
0.3017
0.2992
0.2633
0.50
0.2564
0.2837
0.2678
0.1880
0.60
0.1836
0.1934
0.1809
0.1498
0.70
0.1496
0.1597
0.1517
0.1254
0.80
0.1213
0.1201
0.1218
0.0880
0.90
0.0844
0.0878
0.0909
0.0755
1.00
0.0728
0.0722
0.0785
Table 1. Precision results obtained on Los Angeles Times
number of sentences per passage.

25 Sent.
30 Sent.
0.6759
0.6823
0.5287
0.5269
0.4566
0.4431
0.3522
0.3591
0.2766
0.2827
0.2466
0.2515
0.1949
0.1882
0.1517
0.1517
0.1229
0.1279
0.0874
0.0904
0.0721
0.0711
collection with different

IR-n with IR-n with IR-n 10
1 overlap. 5 overlap. overlap.
0.7729
0.7211
0.7244
0.00
0.7299
0.10
0.6707
0.6541
0.6770
0.20
0.6072
0.6143
0.5835
0.30
0.5173
0.5225
0.4832
0.4144
0.4215
0.40
0.4284
0.3704
0.3758
0.50
0.3115
0.60
0.2743
0.2759
0.2546
0.70
0.2252
0.2240
0.2176
0.80
0.1914
0.1918
0.1748
0.1504
0.1485
0.90
0.1046
1.00
0.0890
0.0886
0.4150
Medium
0.3635
0.3648
Table 2. Experiments with a different number of overlapping sentences
Recall

As it is commented, the proposed PR system can be classified into discourse-based
models since it is using variable-sized passages that are based on a fixed number of
sentences (but different number of terms per passage). The passages overlap each
other, that is to say, let us suppose that the size of the passage is N sentences, then the
first passage will be formed by the sentences from 1 to N, the second one from 2 to
N+1, and so on. We have decided to overlap just one sentence based on the following
experiment, where several numbers of overlapping sentences have been tested. In this
experiment, Table 2, can be observed that only one overlapping sentence obtain the
best results.

4 Evaluation
This section presents the experiment proposed for evaluating our approach and the
results obtained. The experiment has been run on the TREC-9 QA Track question set
and document collection.
4.1 Data collection
TREC-9 question test set is made up by 682 questions with answers included in the
document collection. The document set consists of 978,952 documents from the
TIPSTER and TREC following collections: AP Newswire, Wall Street Journal, San
Jose Mercury News, Financial Times, Los Angeles Times, Foreign Broadcast Information Service.

4.2 Experiment
In order to evaluate our proposal we decided to compare the quality of the information
retrieved by our system with the ranked list retrieved by the ATT information retrieval
system.
First, ATT IR system was used for retrieving the first 1000 relevant documents for
each question. Second, our approach was applied over these 1000 retrieved documents
in order to re-rank the list obtained by ATT system for each question. This process
was repeated for different passage lengths. These lengths were of 5, 10, 15, 20, 25 and
30 sentences.
The final step was to compare the list retrieved by ATT system and the four lists obtained by our system. Table 3 shows the precision obtained in each test. The precision
measure is defined as the number of answers included at 5, 10, 20, 30, 40, 50, 100 and
200 first documents or paragraphs retrieved in each list.
4.3 Results obtained
Table 3 shows the number of correct answers included into the top N documents of
each list for the 681 test questions. First column indicates the number N of first
ranked documents selected for measuring the precision of each list. The second column shows the results obtained by ATT list. The remaining columns show the precision obtained by our system for passages of 5, 10, 15, 20, 25 an 30 sentences.
It can be observed that our proposal obtains better results than the original model,
being 20 sentences the optimum passage length for this task.
IR-n system
Answer
ATT
25 Sent
5 Sent. 10 Sent. 15 Sent. 20 Sent.
included
system
488
At 5 docs
442
444
464
508
532
549
At 10 docs
479
505
531
561
570
584
At 20 docs
517
551
573
595
599
600
At 30 docs
539
575
596
612
617
623
At 50 docs
570
599
611
624
637
640
At 100 docs
595
614
631
644
650
648
At 200 docs
613
634
643
654
655
Table 3. Obtained results on TREC collection and 681 TREC queries.

30 Sent.
531
572
599
618
637
646
655

5 Conclusions and future works
In this paper, a passage retrieval system for QA purposes has been presented. This
model can be included in the discourse-based models since it is using the sentences as
the logical unit to divide the document into passages. The passages are formed by a
fixed number of sentences. As results show, this approach performs better than a good
IR system for QA tasks. Results seem better if we take into account that our passage
approach returns only a small piece of the whole document. This fact enhances our
results because the amount of text that a QA system has to deal with is much more
small than using a whole document. This way, our approach achieves to reduce QA
processing time without affecting system performance.
Several areas of future work have appeared while analysing results. First, we have to
investigate the application of question expansion techniques that allow increasing
system precision. Second, we have to measure the performance of our system by applying directly our paragraph retrieval method over the whole document collection
instead of using it for reordering a given list. An third, it would be very useful for QA
systems to develop a passage validation measure to estimate the minimum number of
passages that a QA system has to process to be nearly sure that contains the answer to
the question. All these strategies need to be investigated and tested.

References
[1]

[2]

[3]

[4]

[5]
[6]
[7]
[8]
[9]

Callan, J. Passage-Level Evidence in Document Retrieval. In Proceedings of the 17 th
Annual ACM SIGIR Conference on Research and Development in Information Retrieval, Dublin, Ireland, 1994, pp. 302-310.
Clarke, C.; Derek, C.; Kisman, I. and Lynam, T. R. Question Answering by Passage
Selection (MultiText Experiments for TREC9). In Nineth Text REtrieval Conference,
2000.
Harabagiu, S.; Moldovan, D.; Pasca, M.; Mihalcea, R.; Surdeanu, M.; Bunescu, R.;
Gîrju, R.; Rus, V. and Morarescu, P. FALCON: Boosting Knowledge for Answer Engines. In Nineth Text REtrieval Conference, 2000.
Hearst, M. and Plaunt, C. Subtopic structuring for full-length document access. Proceedings of the Sixteenth Annual International ACM SIGIR Conference on Research and
Development in Information Retrieval, June 1993, Pittsburgh, PA, pp 59-68
Ittycheriah, A.; Franz, M.; Zu, W. and Ratnaparkhi, A. IBM's Statistical Question Answering System. In Nineth Text REtrieval Conference, 2000.
Kaskiel, M. and Zobel, J. Passage Retrieval Revisited SIGIR ’97: Proceedings of the
20th Annual International ACM July, 1997, Philadelphia, PA, USA, pp 27-31
KaszKiel, M. and Zobel, J. Effective Ranking with Arbitrary Passages. Journal of the
American Society for Information Science, Vol 52, No. 4, February 2001, pp 344-364.
Llopis, F. and Vicedo, J. Ir-n system, a passage retrieval system at CLEF 2001
Working Notes for the Clef 2001 Darmstdt, Germany , pp 115-120
Namba, I Fujitsu Laboratories TREC9 Report. Proceedings of the Tenth Text REtrieval
Conference, TREC-10. Gaithersburg,USA. November 2001, pp 203-208

[10] Prager, J.; Brown, E.; Radev, D. and Czuba, K. One Search Engine or Two for QuestionAnswering. In Nineth Text REtrieval Conference, 2000.
[11] Salton G. Automatic Text Processing: The Transformation, Analysis, and Retrieval of
Information by Computer, Addison Wesley Publishing, New York. 1989
[12] Salton, G.; Allan, J. Buckley Approaches to passage retrieval in full text information
systems. In R Korfhage, E Rasmussen & P Willet (Eds.) Prodeedings of the 16 th annual
international ACM-SIGIR conference on research and development in information retrieval. Pittsburgh PA, pp 49-58

