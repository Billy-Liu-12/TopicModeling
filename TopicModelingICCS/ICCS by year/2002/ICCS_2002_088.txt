An Efficient Approach to Deal with the Curse of
Dimensionality in Sensitivity Analysis Computations
Ratto M., Saltelli A.
Institute for the Protection and Security of the Citizen (IPSC)
JRC, European Commission, TP 361, 21020 ISPRA (VA) - ITALY
marco.ratto@jrc.it

Abstract. This paper deals with computations of sensitivity indices in global
sensitivity analysis. Given a model y=f(x1,...,xk), where the k input factors xi’s
are uncorrelated with one another, one can see y as the realisation of a
stochastic process obtained by sampling each of the xi’s from its marginal
distribution. The sensitivity indices are related to the decomposition of the
variance of y into terms either due to each xi taken singularly, as well as into
terms due to the cooperative effects of more than one. When the complete
decomposition is considered, the number of sensitivity indices to compute is
(2k-1), making the computational cost grow exponentially with k. This has been
referred to as the curse of dimensionality and makes the complete
decomposition unfeasible in most practical applications. In this paper we show
that the information contained in the samples used to compute suitably defined
subsets A of the (2k-1) indices can be used to compute the complementary
subsets A* of indices, at no additional cost. This property allows reducing
significantly the growth of the computational costs as k increases.

1 Introduction
Global sensitivity analysis aims to quantify the relative importance of input variables
or factors in determining the value of an assigned output variable y, defined as

y = f ( x1 ,..., x k )

(1)

A recent review of applications of this discipline can be found in [1-2]. Global
sensitivity analysis is based on the variance decomposition scheme proposed by
Sobol’ [3], whereby the total unconditional variance of y can be decomposed as:

V ( y ) = ∑ Vi + ∑∑ Vij + ... + V12...k

(2)

Vi = V (E (Y xi ))

(3)

i

i

j >i

where

((

))

Vij = V E Y xi , x j − Vi − V j

(4)

and so on. The development in (2) contains k terms of the first order Vi, k(k-1)/2 terms
of the second order Vij and so on, till the last term of order k, for a total of (2k-1)
terms. The Vij terms are the second order (or two-way) terms, analogous to the second
order effects described in experimental design textbooks (see e.g. [4]). The Vij terms
capture that part of the effect of xi and xj that is not described by the first order terms.
If one divides the terms in equation (2) by the unconditional variance V, one obtains
the sensitivity indices of increasing order Si, Sij and so on. The so defined sensitivity
indices are nicely scaled in [0, 1]. Formula (2) has a long history, and various authors
have proposed different versions of it. A discussion can be found in [5-7]. Sobol’s
version of formula (2) is based on a decomposition of the function f itself into terms
of increasing dimensionality, i.e.:

f ( x1 , x 2 ,..., x k ) = f 0 + ∑ f i + ∑∑ f ij + ... + f12...k
i

i

j >i

(5)

where each term is function only of the factors in its index, i.e. fi=fi(xi), fij=fij(xi,xj) and
so on. Decompositions (2, 5) are unique provided that the input factors are
independent and that each individual term (5) is square integrable and the integral
over any of its own variables is zero [3].
Considering a generic term of cardinality c in the development (2), the set
u = ( xi1 , x i2 , ..., xic ) of the factors in the generic term and the remaining set

v = ( x l1 , xl2 , ..., x lk − c ) can be defined. The evaluation of the term requires the
computation a multidimensional integral of the following form

~ ) p (u
~ ) du
~ − E 2 ( y)
V (E ( y u )) = ∫ E 2 ( y u = u

(6)

where p(u) denotes the joint probability density function of factors in the set u, which,
upon orthogonality, can be expressed as
ic

( ( ) )

(7)

p (u)du = ∏ p j x j dx j
j =i1

Equation (6) is computationally impractical. In a Monte Carlo frame, it implies a
double loop: the inner one to compute the conditional expectation of y conditioned to
the subset u (integral over the complementary v subspace) and the outer to compute
the integral over the u subspace. The integral in (6) has been rewritten by Ishigami
and Homma [8], Sobol'[3], as:

∫ E (y u = u ) p(u) du = ∫ {∫ f (u, v ) p(v )dv} p(u) du =
~
~
~ ~
∫ {∫ f (u, v ) f (u, v’) p(v )dv p(v’)dv} p(u) du =
∫ f (u, v ) f (u, v’) p(v )dv p(v’)dv’ p(u )du
2

~

~

~

~

2

~

~

(8)

The expedient of using the additional integration variable primed, allows us to
realise that the integral in (8) is the expectation value of the function F of a set of (2kc) factors:

F (u, v, v ’) = f (u, v ) f (u, v ’) =
f ( x i1 , x i2 ,..., x ic , x l1 , x l2 , ..., x lk − c ) f ( x i1 , x i2 ,..., x ic , x l′1 , x l′2 , ..., x l′k − c ) =

(9)

f ( x1 , x 2 ,...x k ) f ( x i1 , x i2 ,..., x ic , x l′1 , x l′2 , ..., x l′k − c )
The integral (8) can be hence computed using a single Monte Carlo loop. The
Monte Carlo procedure that follows was proposed by Saltelli et al. [9].
Two input sample matrices M1 and M2 are generated:

M1 =

x11

x12

...

x 21

x 22

... x 2 k

...
x n1

xn2

x1k
,

M2 =

... x nk

′
x11
x ′21

′
x12
x ′22

...

x1′k
... x 2′ k

...
x ′n1

x n′ 2

′
... x nk

(10)

where n is the sample size used for the Monte Carlo estimate. In order to estimate the
sensitivity measure for a generic subset u, i.e.

S u = V (E ( y u )) V ( y )

(11)

V (E ( y u )) = U u − E 2 ( y )

(12)

Uu = ∫ ∫

∫ f (u, v) f (u, v’)dudvdv’

(13)

2

we need an estimate for both E (y) and Uu. The former can be either obtained from
values of y computed on the sample in M1 or M2. Uu can be obtained from values of
y computed on the matrix Nu, whose columns are taken from matrix M1 for factors in
subset u and from matrix M2 for factors in subset v (i.e. v’):

N u [1,..., n; i1 ,..., i c ] =

x1i1

...

x 2i1

... x 2ic

...
x ni1

...

x1ic
...

... x nic

N u [1,..., n; l1 ,..., l k −c ] =

x1′l1
x 2′ l

...

...
′
x nl

...

1

1

x1′lk −c
... x ′2lk − c
...

(14)

′
... x nl
k −c

i.e. by:
Uˆ u =

1
n −1

∑ f (x r1 , x r 2 ,..., x rk ) f (x ri1 , x ri2 ,..., x ric , x rl′ 1 , x rl′ 2 ,..., x rl′ k −c )
n

(15)

r =1

If one thinks of matrix M1 as the “sample“ matrix, and of M2 as the “re-sample”
matrix, then Ûu is obtained from products of values of f computed from the sample
matrix times values of f computed from Nu, i.e. a matrix where all factors v are re-

sampled. This procedure allows estimating all terms but the last one, quantifying the k
order interaction effect1. The latter can be estimated by difference, subtracting from
the unconditional variance the remaining (2k-2) terms. Error estimates for Ûu’s are
discussed in the original reference of Sobol’. A bootstrap based alternative is
discussed in [5].
So, a complete analysis of all effects of increasing order for the k input factors can
be obtained by computing all terms in (2), but these are as many as (2k-1). This
problem has been referred to by Rabitz et al. [7] as “the curse of dimensionality“. The
computational cost of estimating all effects in (2) is in fact as high as n(2k-1), where
again n is the sample size used to estimate one individual effect2.

2 A More Efficient Approach
Consider to have the matrices M1 and M2, and to have run the model for the
parameter values of matrix M1 and for a subset m of matrices (Nu1, …, Num) of the
whole set of (2k-1) Nui matrices to be considered. According to the standard
procedure described in the previous section, m indices can be computed by applying
the estimator (15), by multiplying one at a time the vectors of n model runs for
matrices (Nu1, …, Num), that we define [f(Nu1), …, f(Num)], by the vector of n model
runs for parameter values M1, that we define f(M1). However, nothing impedes to
apply the same estimator (15) to all pair-wise combinations of the set of m vectors of
model evaluations [f(Nu1), …, f(Num)]. By doing so, another set of m(m-1)/2 of Ûu
estimates, with repetitions, can be obtained. Such an additional set has no additional
computational cost, and so can be used for making the computation of the whole set
of (2k-1) indices more efficient. This idea has been firstly used by Saltelli [10], who
studied the amount of additional information that can be obtained at no additional
cost, once first order effects and total effects have been computed3.
Some formalism is now needed, in order to allow a correct problem formulation for
the efficient approach. The matrices M1, M2 and Nui can be represented by kdimensional Boolean arrays as follows:

a (1,...,k ) = [0,0,...,0] for M 1 ; a 0 = [1,1,...,1] for M 2
0 if j ∈ [i1 ,..., i ci ]
a (i1 ,...,ic ) [ j ] = 
for N ui
i
1 if j ∈ [l1 ,..., l k −ci ]

1

(16)

It is easy to see that for the k order term, the matrix Nu equals the sample matrix and the
estimator (15) would simply give the unconditional variance.
2 n(2k-2) is needed to compute all effects but the k order, and n more to compute Ê(y), V(y).
3 Total indices are defined as S =1-S , where ~i means all factors except i, and quantifies the
Ti
~i
effect of i alone (main effect) plus the interaction terms with all other factors [1,3,10].

where ci is the cardinality of the element ui of the decomposition (2) and (l1,…,lk-ci)
is., as usual, the array of indices of the complementary set of factors v.
For example, for k=3, the whole set of arrays is defined by:

a123 = (0,0,0)
a 0 = (1,1,1)

a1 = (0,1,1)

a12 = (0,0,1)

a 2 = (1,0,1)

a13 = (0,1,0)

a 3 = (1,1,0)

a 23 = (1,0,0)

(17)

and the estimates Ûu and their Boolean representations si can be expressed by:

Uˆ 1 = 1 /(n − 1) f (M 1 ) ⋅ f (N1 ); s1 = (1,0,0) = (0,0,0) ⊗ (0,1,1)
Uˆ 2 = 1 /( n − 1) f (M 1 ) ⋅ f (N 2 ); s1 = (0,1,0) = (0,0,0) ⊗ (1,0,1)
Uˆ = 1 /( n − 1) f (M ) ⋅ f (N ); s = (0,0,1) = (0,0,0) ⊗ (1,1,0)
3

1

3

1

Uˆ 12 = 1 /( n − 1) f (M 1 ) f (N12 ); s12 = (1,1,0) = (0,0,0) ⊗ (0,0,1)
Uˆ 13 = 1 /(n − 1) f (M1 ) f (N13 ); s13 = (1,0,1) = (0,0,0) ⊗ (0,1,0)
Uˆ 23 = 1 /(n − 1) f (M1 ) f (N 23 ); s 23 = (0,1,1) = (0,0,0) ⊗ (1,0,0)

(18)

where ⊗ is a bit-wise Boolean operator which gives 1 if the elements in the array are
equal and 0 if they are different. The ⊗ operator allows to know which estimate Ûu
can be obtained by taking pair-wise combinations of the arrays of Monte Carlo runs
[f(Nu1), …, f(Num)]. For example:

a13 ⊗ a3 = (0,1,0) ⊗ (1,1,0) = (0,1,1) = s 23 ⇒ 1 /( n − 1) f (N13 ) ⋅ f (N 3 ) = Uˆ 23

(19)

With this formalism, the various array elements ai can also be interpreted as coordinates of the vertices of a k-dimensional unit hypercube, while the sensitivity
indices can be represented by edges and hyper-diagonals joining couples of vertices
ai. With this interpretation the standard procedure consists of drawing all edges and
hyper-diagonals starting from the origin a123.
Another interpretation of arrays ai is that they can be seen as the k-bit binary
representations of the sequence of natural numbers [0, 1, …, 2k-1]∈ℵ. This can be
useful for a synthetic representation of arrays ai trough integer numbers.
Theorem 1
Let m* be the minimum dimension of the subset A of opportunely chosen elements
[au1,…,aum*], for which, combining pair-wise the elements of A through the ⊗
operator, the whole set of si elements can be obtained, then



m MIN = int + 1 + 1 + 8(2 k − 2)  2 ≤ m* < 2 k − 2




where int+(z) is the smallest integer larger the real number z.

(20)

Proof
First let us demonstrate the right-hand side inequality. The cost of the standard
procedure is as high as n(2k-1)2. However, for efficiency purposes, Ê(y) and V(y) can
be estimated also from any of the block matrices used for the (2k-2) Ûu estimates,
making the maximum cost to be as high as n(2k-2). Moreover, the minimum number
m* is surely smaller than (2k-2), because by combining an arbitrary pair of arrays
[f(Nu1), …, f(Num)], m>1, an additional estimate Ûu(m+1) can be computed at zero
cost, reducing the total cost at least by n.
The left-hand side can be demonstrated by introducing the constraint that the
number of pair-wise combinations of the m* elements must be at least as high as (2k2), i.e.:

m * (m * −1) / 2 ≥ 2 k − 2

(21)

which gives the minimum constraint for m*.
Remarks
1. Inequality (21) would become and equation if each combination gives a different
index, i.e. no repeated estimates are obtained in the optimal ai subset.
2. Theorem 1 shows that the order of the growth rate of the computational cost of the
full decomposition can be halved, since the cost is reduced from o(2k) to
o(2(k+1)/2). This makes the complete decomposition feasible at least for ’not too
high’ values of k (e.g. k ≤11, see next sections). On the other hand the exponential
form is unaltered. This implies that for large values of k, the curse of
dimensionality problem remains impracticable.
3. Theorem 1 does not tell us anything about the choice of the m* elements allowing
the efficient approach. This will be the solution of an optimisation problem, which
we have not yet solved for the general case, but only for some values of k. In
section 3 we will show a geometrical solution of the optimisation problem for k=3,
4. In section 4 we will show some experimental results obtained from a simple (but
not optimal!) algorithm for the selection of suitable elements, which allows
appreciating the degree of simplification offered by the proposed approach for
larger k values.

2.1 Geometrical Solution of the Optimisation Problem for k=3, 4
For the case of three factors, formula (20) gives that mMIN=4. The complete
decomposition in this case reads:

V ( y ) = V1 + V2 + V3 + V12 + V13 + V 23 + V123

(22)

Let us consider the 3D unit-cube representing our optimisation problem. The three
main effects can be represented by choosing three face diagonals taken on three
distinct non-parallel faces. The three second order effects are represented by choosing

three non-parallel edges. Our optimisation problem can be seen as identifying the
minimum set of vertices, which allow drawing three non-parallel edges and three nonparallel faces. It is easy to verify that it is sufficient to consider the 4 vertices shown
in Fig. 1 in order to draw all required edges and diagonals. Since in this case
m*=mMIN, the optimisation problem for k=3 is solved.

a =[0,0,1]
12

S

1

S2

a =[0,1,0]
13

S
S

S
13

3

12

a =[1,0,0]
S

a

=[0,0,0]

23

23

123

Fig. 1. Graphical interpretation of the efficient approach for k=3. Solid lines are second order
effects [edges]; dashed lines are first order effects [face diagonals].

Table 1. Results of the application of the operator ⊗ to all pair-wise combinations of optimal
subset of elements ai for k=4.

Bits ↔ ℵ

a1234

a123

a124

a134

a234

[0,0,0,0] ↔ 0

a1234

Vˆ ( y )

[0,0,0,1] ↔ 1

a123

Û 123

Vˆ ( y )

[0,0,1,0] ↔ 2

a124

Û 124

Û 12

Vˆ ( y )

[0,1,0,0] ↔ 4

a134

Û 134

Û 13

Û 14

Vˆ ( y )

[1,0,0,0] ↔ 8

a234

Û 234

Û 23

Û 24

Û 34

Vˆ ( y )

[1,1,1,1] ↔ 15

a0

Eˆ 2 ( y )

Û 4

Û 3

Û 2

Û 1

a0

Vˆ ( y )

In the case of k=4, formula (20) gives that mMIN=6. By extending the geometrical
solution found for k=3, we took as initial trial subset of vertices the origin a1234 and
its 4 adjacent vertices: a234, a134, a124, a123. Finally by adding the vertex opposite to
the origin a0, it can be easily verified from Table 1 that the whole set of 15 indices

can be computed from these 6 elements. Again, since our estimated m* equals mMIN,
the solution find is the optimal solution for the efficient approach. In the first column
of Table 1, the 4-bit binary representation of ai and the natural numbers
corresponding to such a binary array are also shown. A final remark regards the crosscombination of two complementary elements (see a1234 and a0 in Table 1). In such a
case, the result of the operator ⊗ gives the squared expected value, since we are
making the scalar product of two independent random vectors. On the other hand,
applying the operator ⊗ to repeated elements (diagonal elements of the matrix), we
get estimates of the unconditional variance of y.

2.2 Some Experimental Results for k>4
For k>4, we were not able to find similar geometrical considerations to find the
optimal subset of vertices in the k dimensional hyper-cube. The problem should be
solved with an optimisation algorithm. Such an optimisation problem can be a
formidable task. Consider the easiest case of k=5. The total number of elements
among which the optimal subset has to be found is as large as 25 = 32, while mMIN=9.
If no optimisation algorithm is applied, but a complete search of combinations is
performed, the combinatory study requires the evaluation of 32 elements grouped by
9, i.e. 28048800 possible combinations. Further examples of costs for the full search
procedure are shown in Table 2. It can be seen that, for k>5, a procedure based on a
full search of combinations is clearly unaffordable, at least on a workstation or PC.
Table 2. Cost of a plain search of combinations for some values of k.

k
5
6
7

mMIN
9
12
17

Elements
(2k)
32
64
128

Combinations
for m*=mMIN
28048800
3.3 1012
6.1 1020

Combinations
for m*=mMIN+1
64512240
1.3 1013
3.8 1021

Combinations
for m*=mMIN+2
4.8 1013
2.2 1022

So, to find possible suitable combinations fork>4, we applied a simple and
relatively quick algorithm, which is not an optimisation algorithm but gives
sufficiently small values of m* to appreciate the advantage of the new approach. The
algorithm takes an initial trial subset of k+1 elements, given by the origin a1,…,k and
the k adjacent vertices, as we did for k=3, 4. Then it adds one element at a time in
such a way that the additional number of indices computable introducing the new
element is maximum. The algorithm stops when the elements ai selected are sufficient
to compute the whole set of (2k-1) indices. In the case of k=5, the algorithm gave a
number of elements m*=10. In such a case, we verified with a complete search of
combinations that none of the 28048800 combinations of 9 elements allows the
computation of the whole set of indices, so that also for k=5 the optimisation problem
was solved.
The results of this procedure for k=3, …, 11 are shown in Table 3. The advantages
of the new approach are clear. If the standard procedure begins to be hard to

implement for k=7, with the new approach, we can afford a complete analysis at least
for k=11. Moreover, even if the ’true’ values of m* were smaller than the estimates
m̂ * shown in Table 3, the subsets of ai elements that we found were sufficiently
small to bridge most of the gap between the cost of the standard procedure and the
limit m(MIN). In fact, given the latter inferior limit, the further improvement, which
may be given by the true optimisation, will not be as large as the one presented here.
In Table 4 we show the subsets of ai elements represented by arrays of natural
numbers in the interval [0, 2k-1]. By taking the k-bit binary representation of such
arrays, we get the ai elements allowing the estimation of all terms in the
decomposition (2).
Table 3. Comparison of the computational cost of the standard and the new procedure for some
values of k.

k
3
4
5
6
7
8
9
10
11

Number
of indices
(2k-1)
7
15
31
63
127
255
511
1023
2047

Cost of the
standard
procedure
7n
15 n
31 n
63 n
127 n
255 n
511 n
1023 n
2047n

Estimated cost of
the new procedure
m̂ *
4 n4
6 n4
10 n4
14 n
20 n
29 n
42 n
67 n
98 n

Minimum limit
for cost
[formula (20)]
4n
6n
9n
12 n
17 n
24 n
33 n
46 n
65 n

Table 4. Arrays of natural numbers representing the optimal ai conbinations detected by the
simplified algorithm for some values of k.

k
3
4
5
6
7

Optimal combinations for the new approach
[0 1 2 4]
[0 1 2 4 8 15]
[0 1 2 4 8 15 16 17 18 19]
[0 1 2 3 4 5 6 8 16 24 32 40 48 63]
[0 1 2 4 8 15 16 22 28 32 44 51 57 64 77 85 94 106 107 112]

3 Conclusions
In this paper we presented some efficient procedures for numerical experiments aimed
at sensitivity analysis of model output. We have focused here on the computation of
sensitivity indices that are based on decomposing the variance of the target function in
a quantitative fashion. The approach presented aimed to identify to what extent it is
4

In this case the optimisation problem has been solved, so the estimate is correct.

possible to fight the so-called “curse of dimensionality”, that hinders the use of
quantitative sensitivity analysis for computationally expensive models.
The approach allows reducing by a factor 2 the growth rate of computational effort
required for the complete decomposition of the unconditional variance of a model
output. It consists of using the information contained in the Monte Carlo samples
applied for the computation of a given subset of terms, to compute the remaining set
of terms, at no additional cost. The complete solution of the optimisation problem is
still in progress and we managed to treat it in a simplified way only for k<12.
On the base of this incomplete results, we can state that even assuming for n the
value of 1000, the new procedure can make the full decomposition affordable for
models whose cost per run is in the range from milliseconds or lower to some
seconds. If k does not exceed 6, the model cost per run can increase to some minutes.
For models whose execution is in the tenths of minutes to a day range, quantitative
methods are not applicable and efficient qualitative methods such as that of Morris
[11] should be used (see [12] for a review).

References
1. Saltelli, A., Tarantola, S., Campolongo, F.: Sensitivity analysis as an ingredient of modelling.
Statistical Science 15 (2000) 377-395.
2. Saltelli, A., Chan, K., Scott, M. (eds.): Sensitivity Analysis. Wiley Series in Probability and
Statistics. John Wiley and Sons, Chichester (2000).
3. Sobol’, I. M.: Sensitivity estimates for nonlinear mathematical models. Matematicheskoe
Modelirovanie 2 (1990) 112-118, translated as: Sobol’, I. M.: Sensitivity analysis for
nonlinear mathematical models. Mathematical Modelling & Computational Experiment 1
(1993) 407-414.
4. Box, G. E. P., Hunter, W. G., Hunter, J. S.: Statistics for experimenters. John Wiley and
Sons, New York (1978).
5. Archer, G., Saltelli, A., Sobol’, I. M.: Sensitivity measures, ANOVA like techniques and the
use of bootstrap. Journal of Statistical Computation and Simulation 58 (1997) 99-120.
6. Rabitz, H., Ali•, Ö. F., Shorter, J., Shim, K.: Efficient input-output representations.
Computer Physics Communications 117 (1999) 11-20.
7. Rabitz, H., Ali•, Ö. F.: Managing the Tyranny of Parameters in Mathematical Modelling of
Physical Systems. In: Saltelli, A. Chan K., Scott M. (eds.): Sensitivity Analysis. Wiley
Series in Probability and Statistics. John Wiley and Sons, Chichester (2000) 199-223
8. Ishigami, T., Homma, T.: An importance quantification technique in uncertainty analysis for
computer models. Proceedings of the ISUMA '
90, First International Symposium on
Uncertainty Modelling and Analysis, University of Maryland, December 3-6 (1990).
9. Saltelli A., Andres, T. H., Homma, T.: Some new techniques in sensitivity analysis of model
output. Computational Statistics and Data Analysis 15 (1993) 211-238
10. Saltelli, A.: Making best use of model evaluations to compute sensitivity indices. Submitted
to Computer Physics Communications (2001).
11. Morris, M. D.: Factorial Sampling Plans for Preliminary Computational Experiments.
Technometrics 33 (1991) 161-174
12. Campolongo, F., Saltelli, A.: Design of experiment. In: Saltelli, A. Chan K., Scott M.
(eds.): Sensitivity Analysis. Wiley Series in Probability and Statistics. John Wiley and Sons,
Chichester (2000) 51-63.

