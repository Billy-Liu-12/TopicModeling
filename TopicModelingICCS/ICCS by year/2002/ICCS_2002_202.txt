Mining Association Rules for the Future Circumstance
Sung Sam Yuan, Li Zhao

Qi Xiao Yang

School of Computing
National University of Singapore
3 Science Drive 2, Singapore 117543
{ssung,lizhao}@comp.nus.edu.sg
tel: (65)8746148

Institute of High Performance of Computing
89B Science Park Drive#01-05/08 the Rutherford
Singapore 118261
qixy@ihpc.nus.edu.sg
tel: (65)7709265

Abstract
An important issue in data mining research area is forecasting
association rules----derive a set of association rules corresponding
to the future circumstance from the current dataset. For a new
situation, the correct set of rules could be quite different from
those derived from the dataset corresponding to the current
situation. In this paper, a model is proposed for understanding the
differences between situations based on the concept of
Foundation Groups. With this model, a simple technique called
Combination Dataset is provided to derive the set of rules for a
new situation---forecasting association rules for the future
circumstance. Intensive research is focused on the core of this
technique---acquisition of Background Coefficients by
information gain measure and preliminary experiment is
specifically conducted to prove the accuracy of the approach of
acquisition of Background Coefficients which is the first step of
the whole forecasting approach. The technique of Combination
Dataset works independent of the core mining process, so can be
easily implemented with all variations of rule mining techniques.
Through real-life and synthetic datasets test, we show the
effectiveness of the technique of finding the correct set of rules
for future situation.
Keywords: Foundation Group,Combination Dataset, data mining,
association rule, Background Coefficient.

1. Introduction
Data mining techniques are well accepted for discovering
previously unknown, potentially useful and interesting
knowledge from the past datasets. Association rule mining
is one of the most important data mining techniques. Our
past research work has been concentrated on many aspects
of the technique to improve the performance of the rule
generation [1, 2], so less attention is paid to the prediction
of association rules [3] for the future circumstance with the
current dataset. For a future circumstance, because of the
unavailability of the data source, the only rules available to
make the decision are those mined from the earlier dataset.
For example, when a supermarket manager plans to set up
a new store in a new location, decisions have to be made
including what kinds of goods are likely to be in greater
demand and therefore need to be stored more beforehand.
Unfortunately, the only information available for the
manager to make the decision is the past dataset from the
past store that he manages and is operational. But can the
past rules be applied to the future circumstance? Is there
any method to obtain a set of association rules for the new
store before it starts to run?

Confidence threshold = 20%,
STORE 1
Identified rule: soap ⇒ electric shaver (confidence = 21%)
STORE 2
Identified rule: soap ⇒ lipstick (confidence = 23%)
However, The rule: soap ⇒ electric shaver is not true for store 2
because the fact is that 100 transactions show soap, but only 14 of
them are electric shaver. Thus, the confidence level is below the
threshold.
Figure 1: An example of showing variation of rules in two stores

This paper addresses the above issues. Consider the above
supermarket example. Now setting up a new store, because
the new data is still not obtainable, the manager has no
other choice but to use the discovered association rules
from the past dataset to make the decision. But, if the
customer profiles of the two stores are quite different,
many of the rules are likely to be inapplicable to the new
store. Figure 1 shows the association rules discovered from
two stores under one manager with the same supermarket
environment. The rule soap ⇒ electric shaver is valid for
the first store but not for the second, while soap ⇒ lipstick
is valid only for the second store. The supermarket
environment such as all kinds of resources and managerial
methodology for the store 1 and 2 is the same. Further
study shows that 85% of the customers for store 1 are men
while 75% of the customers for store 2 are women.
Generally speaking, men are more likely to be the
customers of electric shavers while women are of lipsticks.
Hence, the above rules begin to become understandable.
From the above example, it is seen that the gender of the
customer plays an important role in mining the two rules,
even though it does not directly appear in the rules as an
item. Such kind of background attribute (does not appear in
the rules) is called Background Coefficient that can
influence the generation of the rules that indicate the
relations among the foreground attributes (those items
appear in the rules). A set of Background Coefficients
with the associated values/value-ranges identifies a
Foundation Group that has two natures no matter which
circumstance the Foundation Group resides in.

2.the Concept of the Background Coefficient,
Foundation Group and Construction Dataset
1

1) The members in the Foundation Group have the same
characteristics that result in their same behavior.
In the above example, for instance, Background Coefficient
"gender" with the associated value "male" identifies a male group
whose members are more likely to be the customers of the electric
shaver whichever store they go to.

2) Every Foundation Group corresponds to a set of
association rules.
Still in the above example, the rule that corresponds to the
male group is : its members, in general, will buy the electric
shaver when they buy the soap.

With N Background Coefficients, N-dimensional space can
be obtained for specifying the Foundation Groups. For
example, if gender and degree are two Background
Coefficients, a two-dimensional space can be obtained.
Also, the Foundation Groups derive from the combination
of the values of these N Background Coefficients. If the
values for the attribute "gender" are {male, female} and for
the attribute "age" {<25, 25--45,>45 }, then six Foundation
Groups need to be formed to address all of the possibilities.
{1: Male <25}{2: Male 25-45}{3: Male >45}{4: Female
<25}{5: Female 25-45}{6: Female >45}
Generally, if there are N Background Coefficients for a Figure 2 a dataset example for the construction of the
circumstance and there are n i possible values for N i Combination Dataset
Background Coefficient, for i=1.....N. To cover all of the
possibilities for a circumstance, the total number of For Figure 2 example, if the Background Coefficient is
“Gender”, it has two values “Male” and “Female”. So the
Foundation Groups is:
whole dataset consists of “Male” group (15 tuples) and
Nc=n 1 *n 2 *n 3 ,....,n N
“Female” group (10 tuples). If the number of the sampled
Foundation Group for the “Male” group is 5 (the following
These whole Nc Foundation Groups constitute a set called table 1 is an example of the sampled Foundation Group for
Complete Group Set. Each circumstance consists of the the “Male” group) and for the “Female” group is also 5,
same Complete Group Set, but every individual and let “Male” Foundation Group and “Female”
Foundation Group in the Complete Group Set is with Foundation Group be F and F respectively, then the
1
2
different proportion. For the above example, both future
values in the formula 1 for this example will be:
store customer group and current store customer group
consist of those six types of customers. If the current store
has more women with age less than 25, the proportion of
Total Number of Tuples for Dataset
the Foundation Group 4 in the example should be larger. If N td = 25
for the future circumstance, there is no men with age F =5 number of tuples of the Foundation Group 1
1
greater than 45, the proportion for the Foundation Group 3
P
the proportion of the Foundation Group 1
in the example is simply zero. Thus, the dataset for each
1 =3
circumstance is the sum of all of the Foundation Groups in F =5 number of tuples of the Foundation Group 2
2
the same Complete Group Set but with different proportion
the proportion of the Foundation Group 2
for each individual Foundation Group. Let the number of P 2 =2
Foundation Groups in the Complete Group Set for a
circumstance be N, the number of tuples of the Foundation
Group i be Fi and the proportion of the Foundation Group i
be Pi, for i=1,.....,N. The number of tuples of the dataset for
this circumstance is:
N

N td =

∑F *P
i

i

(1)

i =1

Table 1 An example of the sampled Foundation Group
for the “Male” group

2

Since the dataset for each circumstance is the sum of all of
the Foundation Groups in the same Complete Group Set
but with different proportion for every individual
Foundation Group, this also holds for the future
circumstance. Thus if all of the Foundation Groups and
corresponding proportions for them, that is, values for the
variables on the right side of the formula 1 can be obtained,
the dataset for the future circumstance called Combination
Dataset can be constructed and the association rules for the
circumstance can be mined. Therefore, to predict
association rules for the future circumstance from the past
dataset, the steps are:
(i)

(ii)

make the classes complete, they are all of the combinations
of items in the association rules. Therefore, the attributes
that are relevant enough to distinguish these classes are
Background Coefficients. Take the rule for division 1 from
figure 1 as an example, from the rule:
soap ⇒ electric shaver
four classes can be obtained: (1) buy soap, buy electric
shaver (2) not buy soap, buy electric shaver (3) buy soap,
not buy electric shaver (4) not buy soap, not buy electric
shaver. After the classes are constructed, attribute
relevance analysis can be carried out. The general idea is to
compute information gain that is used to quantify the
relevance of an attribute to a given class.

find the Background Coefficients based on the set of
association rules obtained from the past dataset. (to
be discussed in the section 3)
get the sample of every individual Foundation Group
(discussed in this section) for the Complete Group
Set ( F i in formula 1) based on the Background

Let S be a dataset that has s tuples, if there are m classes, S
contains si tuples of class Ci, for i=1,….,m. An arbitrary
tuple belongs to class Ci with probability si/s. the expected
information needed to classify a given tuple is
m

I(s 1 ,s 2 ,…..,s m )= -

Coefficients, (the Complete Group Set is the same for
the future circumstance, see the beginning of this
section).
(iii) survey needs to be done to get the proportions of the
values for the Background Coefficients for the future
circumstance (P i in formula 1). For the example of

si

∑ s log
i =1

2

si
s

(2 )

An attribute A with values {a1,a2,…..,av} can be used to
partition S into the subsets{S1,S2,…..,Sv}, where Sj
contains those tuples that have value aj of A. Let Sj contain
Sij tuples of class Ci. The expected information based on
this partitioning by A is known as the entropy of A.

the supermarket, the proportions of men and women
v s + ⋅⋅⋅⋅⋅⋅⋅ + s
for the new store can be obtained by survey.
1j
mj
I (s1 j ,⋅ ⋅ ⋅ ⋅ ⋅ ⋅ ⋅, smj ) (3)
E(A)=
(iv) the Combination Dataset for the future circumstance
s
1
j
=
can be constructed according to the formula 1, since
all of the values of F i , P i (for i=1,……,N )have The information gain obtained by this partitioning on A is
defined by
been found.
(4)
(v) association rule mining can be performed on this Gain(A)= I(s 1 ,s 2 ,…..,s m )-E(A)

∑

To use this approach, firstly m classes need to be obtained
based on the combinations of items in the association rules,
then every attribute except the items in the association
rules needs to be tried as attribute A to compute the
information gain. By doing so, a ranking of attributes can
be obtained. All of the attributes above the relevance
threshold are considered as Background Coefficients.

Combination Dataset.
Analysis: Some questions are raised at this point. "Why
can not you simply get the rule: [soap, man=>electric
shaver]?" The answer is that we can if we want. But the
reason we go a long way to avoid doing that and separate
the dataset into foreground attribute part and background
attribute part is because only in this way can we use our
forecasting approach.

Figure 3 table presents a set of data from the supermarket
dataset. Let the attribute "SportsShoe” be the class label
and class C 1 correspond to "buy" class and C 2 correspond
to "not-buy" class. (For the sake of simplicity, it has only
two classes.) Class "buy" has 9 tuples and class "not-buy"
has 5 tuples. Firstly, equation (2) is used to compute the
expected information to classify a given tuple:

3 Acquisition of Background Coefficients

This section addresses how to find the Background
Coefficients. According to the section 2, since the
definition of the Background Coefficient is that it is a
background attribute that can influence the generation of
the rules that indicate the relations among the foreground
9
attributes (those items appear in the rules), it shows that I(s 1 ,s 2 )=I(9,5)= log 2
14
this background attribute is highly relevant to these
foreground attributes that constitute the association rules.
Intuitively, an attribute is highly relevant to a given class if
the values of the attribute can be used to distinguish the
class from others. For example, it is unlikely that the color
of a computer can be used to distinguish expensive from
cheap computers, but the brand, the speed of hard-disk and
memory are likely to be more relevant attributes. Thus, the
first step to find the Background Coefficients is to
construct the classes (that will be distinguished) with those
foreground attributes appearing in the association rules. To
3

9 5
5
log 2
=0.939
14 14
14

Figure 3

table for the calculation of information gain

theory of acquisition of Background Coefficients by the
calculation of the information gain is correct, the set of
Next, in order to obtain a ranking of attributes, the entropy attributes derived from the calculation of the information
of each attribute except "SportsShoe" should be computed. gain can well match the target Background Coefficients.
Take the attribute "Age" for example, the distribution of
"buy" tuples as s1 and "not-buy" tuples as s2 for each value The ordinary itemset (without target attribute items) is
of Age needs to be calculated. The expected information generated by selecting the size of the itemset from a
for each of these distributions is computed as following:
Poisson distribution with mean that is equal to I ( see the
For Age="<25":
following table) and the weight distribution for the itemset
is derived from an exponential distribution with unit mean.
2
2 3
3
s 11 =2 s 21 =3 I(s 11 , s 21 )=- log 2
- log 2 =0.969 About the generation of the transaction, it is created by
5
5 5
5
adding itemsets and the size for each transaction is chosen
for Age="25-45":
from a Poisson distribution with mean T. If the size of the
s 12 =4 s 22 =0 I(s 12 , s 22 ) =0
itemset exceeds the size of that transaction, the itemset is
added to the transaction with a probability of 0.5, that
for Age=">45":
means only half of the chance is that the itemset is added
3
3 2
2
s 13 =3 s 23 =2 I(s 13 , s 23 )=- log 2 - log 2
=0.969 unchanged. The dropping of some of items in a large
5
5 5
5
itemset is modeled as follows. An item is dropped from an
with the equation 3, if the tuples are partitioned based on itemset if a uniformly distributed random variable between
the attribute "Age", the expected information to classify a 0 and 1 is less than a metric c which is the fixed corruption
given tuple is:
level associated with each itemset, obtained from a normal
5
4
5
distribution with mean 0.5 and variance 0.1. Table 2 gives
E(Age)=
I(s 11 ,s 21 )+
I(s 12 ,s 22 )+
I(s 13 ,s 23 )=0.692
the various input parameters of the dataset generator
14
14
14
So the information gain for this partitioning based on the program and the values used in the experiments.
attribute "Age" would be:
Gain (Age)=I (s 1 , s 2 )-E (Age)= 0.247
By the similar way, the following results can be obtained:
gain (Major)=0.0285, and gain (Degree)=0.152. If the
information gain threshold is 0.2, then only the attribute
"Age" can be chosen as the Background Coefficient.

I
T

N

4. Data Generation
The transaction dataset used in section 5 experiment is
generated automatically by our database generator [4] that
can provide the controlled experiment----generating
transactions containing associations among items. For
example, in figure 3, if the target Background Coefficient
(target Background Coefficient means that we hope the
final experimental result will show that this attribute is
Background Coefficient) is designed to be "Age" and the
rule be "Soap=>SportsShoe", an association among the
attributes "Age", "Soap" and "SportsShoe" needs to be
reflected in the generation of the itemsets. To model the
phenomenon that the probability of the simultaneous
occurrence of these three attributes is higher, an
exponentially distributed random variable with the mean
that is equal to the high "association level" is employed to
determine the addition of itemset in the transaction
database. All of the non-target attribute items in the itemset
are picked according to the item-weight corresponding to
the items. The item-weight corresponds to the probability
for choosing the item when creating the itemset and it is
derived from an exponential distribution with unit mean. It
is easily seen that the above way of generating the items in
the itemsets determines that while the target Background
Coefficients are closely associated with the attributes in the
association rules, the occurrence of the non-target attribute
items has no relation to that of the attributes in the rules.
(In the Figure 3 example, the attribute of "Major" has no
relation to the attribute of "SportsShoe" which appears in
the association rule "Soap=>SportsShoe") Therefore, if the

c

Average size of an itemset
(Poisson distribution)
Average size of a
transaction (Poisson
distribution)
Total number of
transactions in a
dataset
Corruption level (normal
distribution)

4
10

100000

mean = 0.5

Variance = 0.1
Table 2. Parameters for data generation program

5 Experimental Results
This section presents the experimental results to prove the
accuracy of the forecasting theory. The steps for testing are
as shown in Figure 4:

Figure 4 the process of testing result for forecasting theory

a. From the original dataset, two datasets are generated by
sampling. One is called source dataset for the current
circumstance and the other is called target dataset for the
supposed future circumstance. (The goal is to compare to
what extent the Combination Dataset constructed from the
4

formula 1 can simulate the target dataset. In other words, one another. The best effect can be achieved by the
compare the rules mined from the Combination Dataset common composition of items.
with the rules mined from the target dataset)
Figure 6 shows the relative performance of itemised large
b. To get the Combination Dataset:
itemsets. The notation Lk stands for k-item large itemsets,
1) Find the Background Coefficients based on the set C for Combination Dataset and S for source dataset. It can
of association rules obtained from the source dataset. (see be seen that Combination Dataset approach is better than
the section 3)
the direct approach in all cases. As the number of items in
2) Get the sample of the set of Foundation Groups the large itemsets increases, support for directly using
based on the Background Coefficients (F i in formula 1) source dataset case is dramatically dropping while for the
Combination Dataset approach, the support is quite stable.
(discussed in section 2).
3) Get the proportions of the values for the
Background Coefficients from the target dataset, these
proportions are just the ones for the Combination Dataset.
(P i in formula 1).
Since the target dataset serves as the future dataset, the
proportions can directly be obtained from the target
dataset.
4) The Combination Dataset can be built up
according to the formula 1, since all of the values of F i ,
P i (for i=1,……,N )have been found.
c.

Make the comparison between the association rules
mined from the Combination Dataset and the target
dataset.

5.1 Performance
To detect any variation in the efficiency of the
Combination Dataset technique, three conditions are
studied:
1) Figure 5(a): all Foundation Groups involve the same
set of 100 items,
2) Figure 5(b): 50 distinct items per Foundation Group
with the other 50 items being common to all
3) Figure 5(c): all Foundation Groups have their own
distinct set of 100 items.
The y-axis represents the number of large itemsets. The bar
marked “T” represents the number of large itemsets of the
target dataset, “S” represents source dataset and "C"
represents Combination Dataset. For the last two cases, the
lower component of the corresponding bar indicates the Figure 5. Distribution of large itemsets
number of correct large itemsets predicted for that case and
the higher component indicates the number of incorrect
itemsets predicted by that technique.
The percentage of correct large itemsets using the
Combination Dataset approach varies from 97.5% to 85%
and the incorrect itemsets from 6.25% to 12.5% while
directly using the large itemsets of source results in the
percentage of correct itemsets roughly at 65%, and the
percentage of incorrect itemsets roughly at 37.5%.
Figure 6. Relative performance of itemised large itemsets

The results indicate that, under the different itemcomposition of the Foundation Groups for different
situations, using Combination Dataset improves
dramatically the accuracy of estimating association rules.
As can be expected, the percentage of the correct large
itemsets decreases slightly and that of the error large
itemsets increases as the item-composition of the
Foundation Groups becomes more and more distinct from

Conclusions
It can be concluded that the Combination Dataset approach
provides a simple, but powerful means for forecasting
association rules corresponding to the future circumstance
from the current dataset.
5

Reference
[1] S. Y. Sung and V. Shaw, "Mining Association Rules
using Minimal Covers", Proc. of the 1995 Inter. Joint Conf.
on Information Sciences, NC, USA, September 28-30,
1995.
[2] S. Y. Sung, K Wang and B L Chua, "Data mining in a
large database environment". In proceedings of 1996 IEEE
International Conference on Systems, Man and
Cybernetics (SMC'96), Beijing, China, pp. 988-993.Oct.
1996.
[3] K. Rajamani, S. Y. Sung and A Cox, "Extending the
Applicability of Association Rules", In Pacific Asia
Knowledge Discovery and Data Mining (PAKDD’99),
Beijing, April, 1999, pp.64-73.
[4] S. Y. Sung, H. Lu and Y.Lu, "On Generating Synthetic
Database for Classification", 1996 IEEE International
Conference on Systems, Man, and Cybernetics, Oct. 14-17,
Beijing, China.

6

