VIGILANT: Surveillance Video Content and Event-Based Indexing and
Retrieval
Kamel Zerzour, Farhi Marir, and Karim Ouazzane
Knowledge Management Group (KMG), School of Informatics and Multimedia Technology, University of North
London, Holloway Road, London N7 8DB, UK
{k.zerzour, f.marir, k.ouazzane}@unl.ac.uk

Abstract. This paper addresses the need for a semantic video-object approach for efficient storage and manipulation of
video data to respond to the needs of several classes of potential applications when efficient management and deductions
over voluminous data are involved. We present the VIGILANT model for content and event-based retrieval of video images
and clips using automatic annotation and indexing of contents and events representing the extracted features and recognised
objects in the images captured by a video camera in a car park environment.
The underlying video-object model combines Object-Oriented modeling (OO) techniques and Description Logics (DLs)
Knowledge representation. The OO technique models the static aspects of video clips and instances and their indexes will be
stored in an Object-Oriented Database. The DLs model will extend the OO model to cater for the inherent dynamic content
descriptions of the video, as events tend to spread over a sequence of frames.

1. Introduction and Related Work
Recently, video surveillance industry, particularly in the UK, has experienced growth rates of over 10 %
year. CCTV camera systems have been installed on company premises, stores of city and around councils. The
total annual UK market is believed to be valued at £2billions. The bulk of this expenditure is spent on hardware
i.e. cameras, recording equipment and control centres. Currently, the overwhelming bulk of video systems
involve multiplexing CCTV channels onto single 24-hour videotapes. The extensive range of commercial and
public safety Content-Based Image/Video Retrieval applications includes law enforcement agencies, intruder
alarms, collation of shopping behaviours, automatic event logging for surveillance and car park administration.
It also include art galleries and museum management, architectural and engineering design, interior design,
remote sensing and management of earth resources, geographic information systems, medical imaging, scientific
database management systems, weather forecasting, retailing, fabric and fashion design, trademark and
copyright database management.
The problem of providing suitable models for indexing and effectively retrieving videos based on their
contents and events has taken three main directions. Database research concentrated on providing models to
mostly handle static and aspects of data video (with little or no support for the dynamic aspects of video), and
classification is process indices are usually performed offline. From the Image Processing point of view, video
and image features are created automatically during image (frame) capture. These features usually include
motion vectors and, in most cases an object-recognition module is employed to depict object attributes such as
identity, colour and texture. The need for a synergy between database and computer vision approaches to handle
video data is inevitable. This collaborative approach involves database modeling, Vision/Image Processing,
Artificial Intelligence and Knowledge Base Systems and other research areas [1].
The Database indexing realm includes the works carried by [4] who developed an EER video model that
captures video data structure (sequence, scene, and shot) and supports thematic indexing based on manually
inserted annotations (Person, Location, Event). They also provided a set of operations to model video data. [6]
investigated the appropriateness of the existing object-oriented modeling for Multimedia data modeling. He
recognised that there is a need for more modeling tool to handle video data as conventional object modeling
suffer from three main limitations when dealing with video images and scenes. The video data is raw and its use
is independent from why it is created, the video description is incremental and dynamic, and finally the
overlapping of meaningful video scenes. [15] proposed a content-based video indexing and retrieval architecture
in which a database management system (DBMS) is extended with capabilities to parse, index and retrieve or
browse video clips. Semantic indexing was attained through a frame-based knowledge base (similar to a record
in traditional databases) in the shape of a restricted tree form. The database also includes textual descriptions of
all frames present in the knowledge base.
With regards to Image Processing, objects may also be classified on their motion by extracting trajectories,
which are used with other primitives (colour, shape) as keys to index moving objects [2]. Description Logic is

also used to build Terminology Servers to handle video data content in a bottom up approach [3]. DLs proved
beneficial in terms of dynamic, incremental classification, knowledge extensibility and evolution, precise and
imprecise querying abilities. Recently, standard stochastic context-free grammars (SCFG) have been employed
to contextually label events in an outdoor car park monitoring [5]. Objects and events are incrementally detected
using a confidence factor with events being detected on the basis of Spatial Locations when objects tend to enter
or leave the scene. But no storage issues were addressed. [10] modeled object interaction by Behaviour and
Situation agents using Bayesian Networks. The system applies textual descriptions for dynamic activities in the
3D world by exploiting two levels of description: Object levels where each detected object is assigned a
behaviour agent, and Interaction level using situation agents. [9] developed a system to detect abandoned objects
in unattended railway stations. The system is able to reveal to the operator the presence of abandoned objects in
the waiting room. It also has the capability to index video sequences based on an event detection module, which
classifies objects into abandoned, person, and structural change (e.g. chair changes position) or lighting effect,
with the aid of a Multilevel Perception Neural Network.

2. The VIGILANT Video Model
VIGILANT is an end-to-end intelligent semantic video object model for efficient storage, indexing and
content / event-based retrieval of (real-time) surveillance video in a car park environment. The proposed system
diagnoses the problems of content extraction, (semantic) labeling and efficient retrieval of video content. The
system is intended to automatically extract moving objects, identify their properties and label the events they
participate in (e.g. a car entering the car park).
• Capture: A suitable means for capturing surveillance video footage with associated semantic labeling of
events happening in the car park had to be implemented. An unattended camera was set up to record events
(e.g. a car entering the car park) and a frame grabber is needed to process this information and make it
available for the next stage: indexing and content extraction.
• Segmentation and Content Extraction: The most desirable segmentation of surveillance video material is
clearly separation into individual meaningful events (a person getting off the car) with participating objects (a
car and a person). Semantic video segmentation is a highly challenging task and usually segmentation based
on simpler breaks of the video (e.g. a camera break) are opted for [14]. This involves examining the structure
of the video and using statistical measures (e.g. colour histograms) [15]; cuts are determined from scene to
scene. The VIGILANT projects attempts to exploit the frame-to-frame object tracking as a means of
providing shot cuts, whereby each new object appearing in the scene gives rise to a potential event which
ends when the tracking of that particular object finishes (e.g. object leaves the camera view). At each frame
various statistical features (colour, position, time, speed) are extracted from object at each frame and the
content (especially identities of objects and their activities) is incrementally built as objects are tracked from
frame to frame using DLs knowledge base (events) and Neural Networks (objects).
• Content / Event Indexing: Different indexing schemes have been researched [1] and these are classified into
various abstraction levels depending on the types of features being extracted. Conventional database indexing
tools have proved inadequate for video labeling due to the dynamic and incremental nature of video content
[6]. The VIGILANT system will couple Object Oriented Database (OODB) tools with Description Logics
(DLs). The former will be employed to model the static aspects of video (contextually independent content
such as object identity and maybe colour) using concepts of classes and relationships, while the latter will
deal with dynamic contents that spread over a sequence of frames (object activities) by means of concepts
and roles. Various issues will need to be addressed at this stage primarily concept tractability, DLs reasoning
tools and the ability of the DLs knowledge base to access secondary storage.
• Retrieval: The way video clips are retrieved is strongly tied to the way in which they are indexed. Current
retrieval practice relies heavily on syntactic primitive features such as colour, texture, shape and spatial
arrangement, and little work has been done into indexing (and hence retrieval) based on semantic contextual
features (events) [1]. To respond to this need, this work proposes to research and implement queries for the
second and third level of abstraction. We proposed to develop a real-time Video-Object Query Language
(Video-QQL) which will uses the semantic content, indexes and annotations generated by the Index Module.
The Video-OQL module will not be limited to the first level of abstraction i.e. (primitive features) but
respond to complex queries such as "display video clips showing red cars entering the car park between 11am
and 2pm".

3. A UML View of the Model
The first step in using common models for computer-based applications integration is to establish a common
data model or modeling language-that is, a standard paradigm for representing and communicating information
in general. The VIGILANT project is committed to a fully object-oriented approach, in which an emphasis is
placed on inter-working between software objects rather than on sharing of data using common formats. The
current version of the VIGILANT model is being built using UML (the Unified Modeling Language). UML is
rapidly being established as a de facto standard for object-oriented modeling, and is well supported by the
Rational Rose CASE Tool.
Similar to [12], a Video Shot specification consists of three main parts: a Description to represent content
information (such as start and end time), a Composition part to identify the different constituents of a Video
Object (these are objects each with its own constituents and states) and finally an Event to capture relationships
between a Video-Object’s components.
Physically, a Video Shot is made up of number of frames, each of which is composed of a background, and a
number of objects. Initially the background is either the empty parking lot or a car park with a number of objects
that happen to be static when recording starts. Objects depicted in each frame are assigned two states. Objects
with moving status are instances of an object that is still being tracked (an object that either starts moving in the
current frame or one whose tracking has started in previous frames).
The frame background implemented here is dynamic, so if a moving object stops for a period of time (e.g. 5
seconds) it becomes part of the background. From the event point of view, the proposed model views video
shots at two levels. If no events are detected a video shot is the same as a video tape and is hence composed
solely of frames where only background is present. In general surveillance videotape consists of a number of
video shots (in general more than one camera is used and they are allowed to move to have a wider view). The
model also allows for the possibility of video shot overlapping and shots containing other shots. Events are
triggered, interpreted and tagged dynamically with an aid of an object recognition module which determines
object identities.
An event can be viewed as a signature (trail) of a particular object spanning over a number of frames, where
the same objects tends to change position (and may be appearance depending on location with respect to the
camera), speed but not (logical) identities (an object retains its logical identity throughout the lifetime of the
event it is involved in). A video object model is presented in figure 1 using Unified Modeling Language (UML).
Various design decision have been made, the discussion of which is left to the conclusion.
Video
RTape
ecording_Ti
me
D ate
1
is com posed
of

1

0..*
1..*
Video
Shot
Starting_Ti
me
End_Tim
1
e

contains
0..*

1

1

O bject
Sequence

is m ade up
of

involves
1..*

1..*
Fram
e
F rame_Ti
F rame_Positi
me
on
Nu m_O bjec 1
ts

Event
Type

contains
0..*

1..*

1
O bject
X_Positio
n
Y _Positio
X _Siz
n
e
Y _Size 1..*
Start_Tim
e

1
Backgroun
Perso
dX_Siz
n State
Y _Size
e
Set_of_Pixel
s

Bicycle

4_W eeled_Vehic
le
1

Lorry

Car

Van

Car_Com
p State
1..*

D oor

W indow

Fig. 1. The VIGILANT Video Object Model Represented in UML

4. VIGILANT’s Architecture
The complete architecture of VGILANT is shown in Figure 2 with the parts to be addressed in this paper in
bold. The Video-Indexing Module parses the VIGILANT file (figure 3), which encodes the extracted features
and recognised objects, to either populate the video-object database (if the object and events are well
recognised) or to populate the description logic knowledge base (if not yet known). Then indices are generated
in each video-object to reference a frame or a sequence of frames (clips). The Schema Manager Module will
implement a mechanism and channels of communication between the video-object and description logic (DL)
schemas to reflect the dynamic changes of the video content. The Real-time Video-OQL Module will provide
visual facilities, through the VIGILANT user interface, for the user to access and browse retrieved images or
video clips of a particular scene using the indices and content of the video already stored in the video-object
database and DL knowledge base.

VIGILANT’s Retrieval

VIGILANT’s Image Processing

VIGILANT’s User Interface

Surveillance Camera

Video-OQL Module
Image Processing Module

VIGILANT’s Content/Event Indexing Module

Video Object Database
& Knowledge Base

Fig. 2. VIGILANT’s Proposed Architecture

5. VIGILANT content and Event Indexing
Video Content Indexing can be classified into two categories. Indexing of individual objects depicted in a
video scene, which include information such as identity, colour, shape and spatial relationships. Assigning tags
(keys) to video shots is referred to as Event-Based indexing (e.g. a car entering a car park). Before a shot and its
objects can be indexed, its boundaries must be determined. Various methods for detecting shot boundaries have
been employed, but the most widely used technique is pixel-intensity histogram comparison [15] which detects
changes from one shot to the next involving sudden changes in screen image content and camera angle. The next
step is to select a single representative frame (key frame or r-frame) from each shot and apply the same still
image techniques (colour, texture, shape, etc.) including object motion. Key-frame techniques are not suitable
for applications such as Car Park Surveillance as the distortion of image content varies considerably even from
one frame to another. In cases as such the problem of key-frame selection becomes a tracking problem where
moving objects are analysed in all the frames they appear in, in order to build a coherent content description
including motion, colour and shape. The problem of vehicle and human tracking has recently become a hot
research topic involving a multidisciplinary collaboration between Database, Image Processing and Artificial
Intelligence communities.
The Digital image Research Centre (DIRC) at Kingston University has implemented an event-detection and
tracking algorithm that detects and encodes visual events captured by a security camera [7, 8]. The object and
the event streams are parsed automatically to recover the visual bounds and pixels representing moving objects
(cars, persons, etc.) in the park. Figure 3 shows a sample of the text file used by the video frame parser.
The output of this process will be used by the Video-Index Module to either populate the video-object
database (if the object or the events are well recognised) or to enrich the DL knowledge base (if the objects or
the events are not yet known). As soon as an instance of a video object or event is created and stored in the

video-object database or the DL knowledge base, the Video-Index will generate automatically a logical pointer
to the digital file containing the video shot (an image or a video clip). To eliminate redundant video indices in
the database, the index-generating process will only be triggered when the video-Index Module detects a change
of content or events in the car park.

Frame xsize

Frame ysize

Object

Number of objects

Frame time

Object xsize
Object time

Object size

Statistical Data

Number of Children

Object ysize

Fig. 3. A sample of the text file used by the Video Frame Parser

Figure 4 summarises the work of the collaborating three schemas (video-object, description logins and the
schema manage).

DLs Schema
Object/Attribute
Concept

Automatic Concept Creation

Event Concept

Seed/ontology

S
C
H
E
M
A
M
A
N
A
G
E
R

Video Object Schema

Event

Object

Object Sequence

Digital Video
Repository

Fig. 4. The VIGILANT’s Proposed Content/Event Indexing Module shows the collaborative work between both the DLs
schema and the Video Object schemas through the Schema manager.

5.1 Video-Object Schema

Video content is difficult to model within a structured data model such as relational or object oriented mainly
due to the dynamic changes of this content. The proposed solution is to develop a dynamic descriptive video
object model reflecting the evolution of the content as camera shots is taken. It uses object-oriented (OO)
modeling to present the hierarchical structure (static) of the video content enhanced with Description Logics
(DLs) to model the incremental aspect of that video content (dynamic).
The video-object model proposed here extends the EMIR system developed by [12] and models the video
shots in terms of three main parts: The description part which holds attributes such as start and end times of the
main video-object. The Composition part is a hierarchical structure which represents all the component objects
of the main video-object "car", their sub-components (a car door) and their states (a door is open). The event
part describes and labels the significance of the event depicted from a video scene. In this part, the significance
of the event is used as a relationship relating two or many objects of the main video-object. For example the
event "person gets in car" relates "person" and "car" through the "gets in" relationship.
Existing Object-Oriented modeling tools have been described in [6] as to allow representation and
management of complex objects, handle object identities, provide encapsulation and inheritance mechanisms. A
video shot is a sequence of consecutive frames (images) which appear to have been continuously filmed and
exhibit some contextual consistency e.g. "a car entering a car park". Shots are the fundamental unit of
manipulation, having their own state and behaviour. The most appropriate tool to model the static properties of a
Video Shot is the object-oriented data modeling, thanks to its powerful aggregation and inheritance concepts
and its abstraction power to handle complexity. Yet, they are still limited in dealing with video data due to the
following restrictions
1. Video data is raw and its use is independent from the reason it is created,
2. Video content description is both changing and incremental, and
3. The provided inheritance is unable to accurately model metadata sharing for overlapping video shots.
The VIGILANT Video Object data model is also be able to interpret and label events depicted in surveillance
video. DL knowledge base is proposed to enhance the obtained video-object model.
5.2 The Description Logics Schema

The DLkb domain knowledge represents concepts held by objects in the class, and is defined by the
Equation
DLkb(C) = <F[fn,method], F[rn,rolebvr],CM[qc,val]>

(1)

where C is a concept in the knowledge base representing an object of a given class (moving objects or events).
Fn is an object feature (e.g. colour, type,…) which is extracted using a procedure method. R is a relationship
connecting a feature to a concept (e.g. has_colour) and role is the role’s semantic behaviour (e.g. has_colour).
CM is a condition mapper that converts the condition value(s) of the query into a certain data value value whose
data type is the same as that of the data values of the F (afternoon gets converted into Time > 12:00 am).
Two basic concepts are needed for the VIGILANT’s project, these reflect the two directions of the
undertaken research (content and event based retrieval). Content indexing is represented by moving object
concepts (car, person) and events by event concepts.
Object features include object identity, colour, speed, position within a particular frame, speed and size.
These attributes are available from the image-processing module, except object types which require utilising the
Semantic Neural Net. When an object appears for the first time a hypothesis is made about its identity, then
using other syntactic object features (colour, height, width, and area) the hypothesis is strengthened until at the
end of the tracking process the hypothesis becomes a thesis. Whenever an object is processed a new version of
that object concept is created in the DLs domain knowledge.
Event concepts are created in a similar manner. The first version of the object concept calls for an event
concept to be created with the following features:
1. A temporary event type which is then strengthened as the tracking continues over time.
2. The event start time is the first object’s time.
3. The end time is the last object version’s time.
4. An object sequence containing all the versions of one object (or many) involved in that particular event.
Figure 5 shows a formal definition of a moving-object and an event concept, while in figure 6 is a sample of the
definition of the concept Event in both OO and DLs environments

<MovingObject>

<Event>

Feature{[Type, Ext_Type],[Colour, Ext_Colour], …}

Feature{[Type, Ext_Type],[StartTime, Ext_STime], …}

Role{[Type, Has_Type], [Colour, Has_Colour],…}

Role{[Type, Has_Type], [STime, Has_Has_STime],…}

Condition_Mapper{[Red,[120,24,167], [fast, 0km/h],…}

Condition_Mapper{[Type,[Car_Entering,Person_Leaving,…],}

Fig. 5. Domain Knowledge Definitions for Moving Objects and Event Concepts

Class: Video-Shot
{
// description part - basic primitives
Starting-time;
Ending time;
Background colour;
Number of objects;
etc...

(define-concept Video-Shot (AND
(define-concept Color (aset red green yellow blue grey))
; Description
(ALL HasBackgroundColour Color) (ALL StartsAt Num)
(ALL EndsAt Num) (ALL HasNumberOfObjects Num)
; Composition Abstract level
(ALL ContainsComp VideoShotComp)
(define-concept Stat (aset CLOSED OPEN))
(define-primitive-concept Door CarComp)
// Composition part - object whose part can have different state
(define-primitive-concept Window CarComp)
Car ("door open", window open", etc...)
etc...
Person ("walking", "sitting")
;Event Abstract level
(ALL ContainsEvent Event)))
(define-concept EvType (aset GET_IN ENTER …))
// Event - objects and relationship between these objects
Prson, Car ( "gets in", "gets out", etc...)
(define-concept GetIn (AND Event (ALL HasEventType
Car, Car Park ("enters", "leaves", etc...)
(aset GET_IN))
(ALL HasFirstInteractor Person)
} // End of Video-Shot Class
(ALL HasSecondInteractor Car)))
ect…

Fig. 6. Examples of typical Class and DLs concept Definition of an Event

5.3 The Schema Manager

The Schema Manager Module aims at providing communication channel between the DLs inference engine
and OO schemas to reflect the dynamic nature of video content. Another equally important role is dealing with
the new developments that could occur to an unknown object. As soon as an unknown object or scene in the DL
knowledge base is fully recognised, the Schema Manger transfers the recognised object’s instance into the
video-object database. However, sometime the unknown object remains unrecognised. In this case the schema
manager will use the description or structure provided by the DL inference engine to either automatically update
the Video-object schema and include the new object using the schema evolution process, or leave unrecognised
objects residing in the DLs knowledge base.
This challenging task will be achieved through the design of a mapping mechanism between the DLs
inference engine and OO Video-Object Schema. Since the interpretation domain for the DL consists of atomic
objects, while each instance of an object-oriented model is assigned a structured value, the mapping mechanism
needs to implement new concepts and functions to allow translation from one schema to the other. This way,
DLs inability to handle large number of concepts and instances is addressed through:
• enhancing the DL inference engine with facilities to access secondary storage (where both the concepts that
compose the DL schema and the instances derived from the schema might be stored) and also
• Improving the performance problems with the reclassification and recognition of instances whenever
conceptual schemas or instances are changed.
The design of the mapping mechanism between DLs inference engine and video database will extend the
work done by [13] where he extended first-order logic (PROLOG-like) with relational data management
facilities to access external storage. This was achieved through the development of an algorithm that transforms
the first order logic into equivalent relational algebra expressions which could be evaluated during query
processing using algebraic operations such as selection, union, Cartesian products and join operations and SQL.
Figure 7 illustrates the three schemas collaborating in dealing with recognised and unrecognised
objects/events.

(define-primitive-concept Obj (AND (ALL Has_Type type)))
Seed
Concepts

(define-primitive-concept MovingObj (AND Obj (ALL Has_ID num) (ALL Has_ID_Conf num)
(ALL Has_Xsixe Num)
(ALL Has_Ysize Num)
(ALL Has_Colour Color)
(ALL Has_Time Num)
(ALL Has_Xpos Num)
(ALL Has_Ypos Num)
(ALL Has_EventID event)))
(define-primitive-concept Car (AND MovingObj (ALL Has_ID (aset car)))

Object Sequence

(define concept RedCar (AND Car
(ALL Has_ID (= 1))
(ALL Has_ID_Conf (= 90))
(ALL Has_Xsize (= 80.00))
(ALL Has_Ysize (= 20.00))
(ALL Has_Colour (aset Red))
(ALL Has_Time (= 10.00))
(ALL Has_Xpos (=2.00))
(ALL Has_Ypos (=3.00))
(ALL Has_Event_ID (aset UNKNOWN

ID = 1
Type = Car
Confidence 60%
Colour = Red
Xposition = 3.00

Frame n

Yposition = 2.00

S

Hight = 18.00

C
H
E
M
A

Width= 75.00
StartTime 10.00

Retrieve Object with ID = 1

(define concept RedCar (AND Car
(ALL Has_ID (= 1))
(ALL Has_ID_Conf (= 90))
(ALL Has_Xsize (= 80.00))
(ALL Has_Ysize (= 20.00))
(ALL Has_Colour (aset Red))
(ALL Has_Time (= 10.01))
(ALL Has_Xpos (=2.50))
(ALL Has_Ypos (=3.50))
(ALL Has_Event_ID (aset ENTERING

M
ID = 1
A
Type = Car
N
Confidence 90% A
G
Colour = Red
E
Xposition = 2.50 R
))))

Yposition = 3.50
Frame n+1

Hight = 20.00
Width = 80.00
Start Time 10.01

M
O
D
U
L
E

Domain knowledge

Final Object & Event Description

.
.
Last Frame of clip
.

Description Logics Knowledge Base

Fig. 7. A walk through example of how OODB and DLs schemas collaborate via the schema manager

6.

VIGILANT’s Content and Event Based Retrieval

The description, composition and event parts proposed for the design of the video-object model reflect these
levels of abstraction. Current Content-Based Image Retrieval (CBIR) techniques are used to break up long
videos into individual shots, extract still keyframes summarising the content of each shot, and search for video
clips containing specified types of movement. The effectiveness of all current CBIR systems is inherently
limited by the fact that they can operate only at the primitive feature level. None of them can search effectively

for "a car entering a park" though some semantic queries can be handled by specifying them in terms of
primitives. While Content-based image retrieval (CBIR) systems currently operate effectively at the lowest of
these levels, a variety of domains, including crime prevention, medicine, architecture, fashion and particularly
video surveillance demand higher levels of retrieval.
To respond to this urgent need, this work proposes to research and implement queries for the second and third
level of abstraction. For this, it is proposed to develop a real-time Video-Object Query Language (Video-QQL)
which will uses the semantic content, indexes and annotation generated by the Real-time Index Module. The
Real-time Video-OQL module will not be limited to the first level of abstraction i.e. (primitive features) but
respond to complex queries such as "display all the video clips showing red cars entering the car park between
11am and 2pm". This could be expressed in OQL-Video as:
SELECT VideoObject = VideoClip
FROM VideoClip.Cars AND VideoClip.CarPArk
WHERE VideoClip.Event.EventType = “Car Entering Car Park” AND
VideoClip.Time = IN(11am, 3pm) AND
Cars.Colour = “Red”

To achieve such complex queries, the Video-OQL will combine the facilities provided by the object database
Object Query Language (OQL) and the description logic (DL) descriptive language. This combination of Object
database and DL queries will also widen the power of Video-OQL to deal with exact queries as in database
query languages as well as with incomplete and imprecise queries.
In the VIGILANT project, similar to [11], a query Q is modelled as:
Q=dm(rf[ac,c])

(2)

where dm is the domain or the set of classes to be retrieved as a result of the query Q , rf is the retrieval
function, ac is the attribute(s) for which the condition(s) is/are specified, and c is the condition value(s) of the
attribute(s).an example query is “find video clips of cars”. This is expressed as:
ObjectSequence(find_equal[VideoTape.VideoShot.Frame.Object.Type,Car])

This is regarded as a primitive query and more sophisticated queries can be built by combining primitive
queries. Consider the following query statement: “find video clips of red cars entering the car park between
11:00 am and 12:00 am”, this can be expressed in our model as:

ObjectSequence(equal_type[videoTape.VideoShot.Frame.Object.Type,Car]&&
equal_color[videoTape.VideoShot.Frame.Object.Color,Red] &&
equal_event[videoTape.VideoShot.Event.Type,Entering] &&
time[videoTape.VideoShot.Frame.Object.StartTime,11am&12am]

Conclusions and Future Works
The VIGILANT’s proposed model couples Description Logics Domain Knowledge schemas which deal with
the static content with Object-Oriented schemas which handle the evolution of video content. Challenges
associated with this scheme include DLs inability to handle large volumes of data. This will be addressed
through (1) enhancing the DL inference engine with facilities to access secondary storage (where both the
concepts that compose the DL schema and the instances derived from the schema might be stored) and also. (2)
Improving the performance problems with the reclassification and recognition of instances whenever conceptual
schemas or instances are changed. Another equally important challenge will be the mapping of both OO and
DLs schemas.
The VIGILANT is still a mental model and has concentrated on the design of a Video Object Model. The
model is committed to a fully object-oriented approach, in which an emphasis is placed on inter-working

between software objects rather than on sharing of data using common formats. The current version of the
VIGILANT model is being built using UML (the Unified Modeling Language). UML is rapidly being
established as a de facto standard for object-oriented modeling, and is well supported by the Rational Rose
CASE Tool. To prove the concept of incremental video event indexing a prototype of the VIGILANT model has
been implemented using VERSANT Object Oriented Databases and Microsoft Visual C++ Programming Tool.
Although the OO and DL models seem to complement each other, this work will be facing two main
challenges. The first is mapping between DL and OO models or schemas. Tight coupling is preferred in this
system to allow on-demand traffic of instances between the video object database and the DLs knowledge base
on demand. Fully recognised objects and events are transferred to the video object database permanently
(persistent objects and events). Unknown objects (and events) reside in the knowledge base until further
information about their content is acquired. If still unrecognised, a decision has to be made whether to transfer
them to the video object database or leave them in DLs knowledge base. The second challenge is addressing the
limitation of DL in accessing external storage when the number of DL concepts (class) individuals (objects)
grows and requires external storage.

References
1. J.P. Eakins, M.E. Graham, “Content-Based Image Retrieval, Report to JISC Technology Applications
Program”, 1999.
2. E. Sahouria, “Video Indexing Based on Object Motion. Image and Video Retrieval for National Security
Applications: An Approach Based on Multiple Content Codebooks”, The MITRE Corporation, Mclean,
Virginia, 1997, USA.
3. C. A. Goble, C. Haul and S. Bechhofer, “Describing and Classifying Multimedia using the Description Logic
GRAIL”, SPIE Conference on Storage and Retrieval of Still Image and Video IV, San Jose, CA, 1996.
4. R. Hjelsvold, R.Midtstraum and O. Sandsta, “A Temporal Foundation of Video Databases”, Proceedings of
the International Workshop on Temporal Databases, 1995.
5. Y. Ivanov, C. Stauffer, A. Bobick and W. E. L. Grimsom, “Video Surveillance Interactions”, Second IEEE
International Workshop on Visual Surveillance, 1999.
6. E. Oomoto, and K. Tanaka, “OVID: Design and Implementation of a Video-Object Database System”. IEEE
Transaction on Knowledge and Data Engineering, Vol. 4, No 4, 1994.
7. J. Orwell, P. Massey, P. Romagnino, D. Greenhill, and G. A. Jones, “A Multi-Agent Framework for Visual
Surveillance”, Proceedings of the 10th International conference on Image Analysis and Processing, 1996.
8. J. Orwell, P. Romagnino, and G. A. Jones, “Multi- Camera Colour Tracking”, Proceedings of the second
IEEE International Workshop on Visual Surveillance, 1999.
9. C. S. Regazzoni, and E.Stringa, “Content-Based Retrieval and Real-Time Detection from Video Sequences
Acquired by Surveillance Systems”, IEEE International conference on image Processing ICIP98, 3(3), 1999.
10.P. Romagnino, T. Tan, and K. Baker, “Agent Oriented Annotation in Model Based Visual Surveillance”, In
ICCV, pp 857-862, 1998
11.A. Yoshitaka, S. Kishida, M. Hirakawa and T. Ichikawa, “Knowledge Assisted Content-Based Retrieval for
Multimedia Databases”, IEEE ’94.
12.Y. Lahlou, “Modeling Complex Objects in Content-Based Image Retrieval”, In Proceeding of Storage and
Retrieval for Image and Video Database III, Vol. 2420, pp 104-115, 1995.
13.F. Marir, “A Compiled Inference Approach to the Integration of Logic and Relational Database systems”,
Ph.D. thesis, University of Salford, 1993.
14.P. Aigrain, H. Zhang , and D. Petkovic, “Content-based Representation and Retrieval of Visual Media: A
state-of-the-art Review”, Multimedia Tools and Applications, 3(3), pp 79-202, 1996.
15.S.W. Smoliar and H. Zhang, “Content-Based Video Indexing and Retrieval”, IEEE Multimedia 1(2), pp 6272, 1994.

