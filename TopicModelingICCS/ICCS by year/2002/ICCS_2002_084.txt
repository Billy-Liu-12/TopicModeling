A Parallel SOR Algorithm on a Workstation
Cluster
Brian Mulhern1, Sean P. Mulligan2
1

Galway-Mayo Institute of Technology, Castlebar, Co. Mayo, Ireland
Computing Department, Dublin Institute of Technology, Bolton St., Dublin1, Ireland

2

briamulh@castle.gmit.ie,

shane.mulligan@dit.ie

Abstract. This paper investigates a method of parallelising the iterative SOR
algorithm, for problems arising from the discretisation of the Laplace operator
in 2D. Much work in this area has dealt with the parallelising of iterative
algorithms on high performance vector processors in a shared memory
environment. Here, we examine Red-Black and Multi-Colour orderings, in a
distributed memory environment, thus allowing SOR updates to be performed
in parallel. We compare the convergence of these orderings to the natural
ordering, and show that the advantage gained in parallelisation is not lost
through slower convergence. Load balancing and communication overhead are
examined through the allocation of work segments between the cooperating
processors. We use an NT cluster, running Parallel Virtual Machine (PVM),
and a Parallel Toolbox (DP-TB), for Matlab, as our experimental parallel
computing platform. Finally, we propose a master-slave algorithm to efficiently
implement the SOR method in a parallel cluster computing environment.

1.

Introduction

Many problems in the fields of Science and Engineering, such as weather prediction,
heat transfer, and high energy physics can be solved by reduction to a set of Linear
Equations. These equations are derived from the discretisation of the underlying
problem domain into a set of mesh points. In matrix notation, the problem then
reduces to solving an equation of the form:,

Ax = b

(1)

where A is an n x n coefficient matrix, x, is a vector of the unknowns, and b is also a
vector incorporating boundary condition information. In order to achieve a reasonable
degree of accuracy, however, a large number of mesh points, (i.e. a fine grid), is
required for the problem domain. This increases the number of linear equations that
need to be solved, and hence the solution time.
Until recently, the scale of these problems required the use of mainframes or
supercomputers, (parallel and vector computers), to provide the solutions to a
reasonable accuracy and within a reasonable time. In recent years, microcomputer

technology has advanced to such an extent, that the workstations of today match the
supercomputing power of ten to twelve years ago. Zomaya [8] has published results
indicating, that in the LAPACK 1000 benchmark tests, the DEC Alpha RISC
machine, costing in the region of 50,000 dollars, can achieve one-sixth the
performance of the multi-million dollar Cray C90. High speed LANs have opened up
the possibility of connecting a cluster of high performance workstations together to
cooperate on a single problem. These advances allow for much increased solution
times for the problem domains described. However, two major problems exist in
using this power , namely:
• How to expose the parallelism of the problem: Parallelism is inherently problem
specific and difficult to automate.
• How to avoid performance degradation in a distributed computing environment:
The communications overhead in a distributed computing environment can
severely degrade any advantages in performance gained by parallelising the
problem.
This paper examines the SOR iterative method of solution as it would be applied in
a distributed processing environment such as the Parallel Virtual Machine (PVM)
cluster.
1.1

A Sample Problem

The problem chosen to illustrate our proposed solution is the steady state Heat
Diffusion problem in a 2D rectangular domain. The problem is described by the
Laplace second order elliptic Partial Differential Equation (PDE):

∂ 2u ∂ 2u
+
=0
∂x 2 ∂y 2

(2)

Discretising equation (2) using a central difference formula yields the 5-point
stencil of equation (3) , with the discretisation interval equal to h.

− u i −1, j − u i +1, j + 4u i , j − u i , j −1 − u i , j +1 = 0

(3)

Applying (3) to the internal mesh points ui,j, gives a system of n linear simultaneous
equations with n unknowns, where n = N x N mesh points. In matrix form, this
system of equations reduces to equation (1).
For large N, the coefficient matrix will be sparse, (especially for a low order
difference operator), and iterative techniques are often the preferred methods of
solution.
1.2

The Successive Over-Relaxation Method

The Successive Over-Relaxation (SOR) method is an extension to the Gauss-Seidel
(GS) method, and uses a relaxation factor, •, to accelerate convergence. The

procedure starts with an initial approximation to the solution and updates each
unknown according to the relationship in equation (4):

u i( k +1) = u i( k ) +

i −1
n
ω
(b i − ∑ aij u (jk +1) − ∑ aij u (jk ) )
aii
j =1
j =i

(4)

Dongarra et al [2] suggests that • is calculated as the weighted average between the
previous iterate and the computed GS iterate, successively for each unknown. The
effect of • is to reduce the spectral radius of the iteration matrix, and in practice, it is
difficult to predict an optimum value, •opt. However for the model problem on a
uniform mesh, a formula for the theoretical •opt is well known, see [2].
1.3

Parallel Processing

Hockney and Jesshope [6] describe data parallelism as blocks of data which are
independent of other data, and which can be manipulated and changed without
affecting other parts of the problem at that time. Task (functional) parallelism
describes processes within the overall solution process, which can be carried out in
parallel. Because there is no single parallel computational model, the problem
decomposition must be mindful of the underlying parallel platform on which it will be
solved.
In the relationship in equation (4), the unknowns are coupled by the fact that the
current unknown being updated depends on the most current value of its immediate
neighbours, thus preventing the unknowns from being updated in parallel.
Parallelism can be exposed, however, by re-ordering (colouring) the unknowns,
allowing updates to carried out such that nearest neighbour values are obtained from
the previous iteration cycle, rather than the current one. Here we examine Red-Black,
and Multi-colour ordering strategies of decoupling the unknowns. In a distributed
processing environment, all nodes of the same colour can therefore be updated in
parallel, i.e. by different processors, and only the results of these updates need to be
communicated between processors.
2.

Experimental Infrastructure

The experiments entailed, firstly examining the convergence of Red-Black and MultiColour ordering strategies on a sequential machine and comparing the convergence to
that of the Natural ordering method. Secondly, we propose an algorithm for parallel
implementation on a machine cluster.

2.1

Hardware Architecture

The experimental platform consisted of a number of 450Mhz Pentium 2’s, with 64
Mbytes of RAM, running Windows NT 4.0 Workstation.

The configuration would allow approximately 46 Mbytes of available RAM for
matrix storage at any time. This would allow for 5.75e6 array elements stored in
floating point format, using 8 bytes, or a 2400 x 2400 dense matrix. The above figures
are derived from full matrix storage. Sparse matrix storage, where mostly the nonzero elements are stored, allows for much larger mesh sizes, without resorting to
virtual memory.
In the configuration, workstation A was configured as a master, while B and C
were both slaves, connected via a standard 100 BaseT Ethernet Hub. IP addresses
were supplied by the DIT Primary Domain Controller (PDC). Communication times
between the workstations were influenced by ordinary DIT network traffic.
2.2

Software Architecture.

Table 1 shows the four distinct layers of the experimental software architecture.
Interfacing directly with the hardware, is the NT Operating System. The next layer is
a Windows Remote Scheduler, (WRSH), which is required on NT clusters, to allow a
process initiated by one workstation, to execute on another workstation. This runs as a
service under NT4.0, cf. Dongarra and Fischer [3]. The PVM layer interacts with this
layer to control the Virtual machine. Finally, the DP Toolbox uses the services of
PVM to send messages to its counterparts on the other workstations in the cluster. The
main Matlab installation interfaces directly with NT.
Table 1. Software Architecture for the Parallel Machine

DP Toolbox 1.4
Matlab 5.3

PVM 3.5
WRSHD 3.5
NT 4.0

2.2.1 PVM Configuration
The machine on which the PVM console is initially started, is designated the Master,
by the PVM software. From this machine, then, other workstations are added to create
the physical parallel machine.
Pawletta et. al. [7], describe the DP Toolbox as a communications module enabling
parallel working in Matlab. Instances of Matlab running on different processors cooperate in solving a single problem, i.e., the Matlab instances are coupled via the DP
Toolbox. The toolbox itself does not implement the primitives for process
communication, but uses the underlying message passing system, which in our case is
PVM.

3.

Numerical Experiments

The first part of the experiment compares the sample problem using Natural, RedBlack and Multi-Colour ordering schemes, on a single sequential machine. In the
analysis, we examine how the relaxation factor, ω, affects the convergence of all three
ordering patterns.
3.1

Red-Black Ordering

In a Red-Black ordering scheme, the mesh points are numbered row-wise alternate,
starting at the bottom left corner. The resulting coefficient matrix is given by (5)

D
A =  11
 A21

A12 

D22 

(5)

where Dii are diagonal blocks representing each node colour, and Aij are sparse banded
matrices which represent the relationship to nodes of the other colour. The band
distance from the diagonal is a function of the mesh size, and is usually taken as
(N2/2) for a square N x N mesh. This permutation of the coefficient matrix from a
naturally ordered system, should permute the iteration matrix for the SOR method,
changing its eigenvalues, and consequently the convergence rate.
Table 2. A comparison of Natural and Red-Black Ordering schemes

Ordering

ω_opt

No. Nodes

Iterations

Predicted(P)

Experiment(E)

P

E

Natural

361

1.7295

1.7550

59

36

Red-Black

361

1.7295

1.7602

55

42

Natural

841

1.8107

1.8325

88

64

Red-Black

841

1.8107

1.8405

82

56

Natural

2401

1.8818

1.9050

148

137

Red-Black

2401

1.8818

1.9152

130

122

Natural

4761

1.9141

1.9325

207

195

Red-Black

4761

1.9149

1.9285

191

179

However, Harrar [4] proves that the permutation obtained by row wise red-black
ordering preserves the symmetry, and the consistently ordered (CO) property of the
coefficient matrix. This maintains the eigenvalue relation with the Jacobi matrix, and
hence should have a minimal effect on the convergence rate. The experimental
results, summarised in Table 2 bear this out, showing only a slight difference in
convergence rates between the Red-Black and naturally ordered systems. The results
also show an average of 5% discrepancy between the predicted and experimental
values for •opt. This leads to about a 10% change in convergence rate.
3.2

Multi-Colour Ordering

For Multi-color ordering, we use a three color row-wise ordering , which again results
in a consistently ordered symmetric coefficient matrix. The coefficient matrix is
constructed by moving a five-point stencil through each red point in turn, then
through each black point, and finally through the blue points.
ij + North (black)
ij (red)
ij +West (blue)

ij + East (black)
ij+South (blue)

Fig. 1. Five-point stencil for 3-Colour Ordering.

The stencil of Fig. 1 indicates that red points depend only on the black and blue
points for updates. Similarly, each black point is updated with reference to only red
and blue points, and finally, the blue points are updated with reference to the other
two colours. Convergence rates for this Multi-Colour scenario are compared to the
natural, and Red-Black schemes in Table 3.
Table 3. Comparison of Convergence Rates (iterations)

Mesh Points

Nat. Ord

R-B Ord

Multi-Color

961

85

70

79

2401

137

122

140

4489

220

207

228

The results show very little difference between the various ordering schemes. RedBlack ordering shows on average a 10 to 12 per cent improvement over the other two
schemes over all the ranges of points examined. Adams et al. [1], suggests that for

certain schemes of red-black and multi-colour ordering, the convergence rate should
be the same as for the natural ordering method. The results agree with this.
3.3

Scaling Factors

For mesh sizes greater than 4000, memory paging is required, which slows down the
solution process considerably. For data sizes that can be totally contained within
physical memory, the CPU usage is close to 100%, and the solution speed is approx
34 points/sec., (i.e. 140 sec’s. for 4761 points). However, for data sizes that require
virtual memory, much of the processor time is spent in memory paging, and CPU
usage drops to 20 percent. The solution times for various numbers of Red-Black
ordered mesh points are plotted in Fig. 2.
4000

100

CPU Usage
Solution
time (secs)

80

60
2000
40

Solution Time
0

0

1000

2000

3000

4000

5000

CPU
Usage
(%)

20

6000

7000

8000

9000

0
10000

No. of Mesh points
Fig. 2.

Solution Times v.’s CPU usage for Red-Black Ordering.

For grids greater than 6000 points, the latency time in disk accesses cause the CPU
to be put in a wait state, thus slowing down the solution process. Substantial speed
improvements can be achieved by updating using only the non-zero entries of the
coefficient matrix A, and not the full row of A.
On workstation clusters, combined memory paging and communication times may
cancel out much of the speedup gained in the original parallelisation of the problem.
4.

Parallel Algorithm Implementation

The RB and MC SOR algorithms are implemented using the standard master/slave
model. To enable parallelism the problem is partitioned among the available
processors. The steps in the solution process using the RB-SOR, with a 2-processor
configuration are as follows, with the algorithm outlined below.
1. Formulate the problem Ax = b on Processor 1 and initialise uold.

2. Distribute the rows of the A matrix corresponding to all the red phase (halfiteration), and the complete b vector to between both processors.
3. Concurrently update the red phase on both processors.
4. Synchronise results by processor 1 sending its unew node values to processor 2, and
processor 2 sending its unew node values to processor 1.
5. Concurrently update the black phase on both processors, and synchronise results.
6. Check for convergence on processor 1. Goto (c) if not converged.
7. Solution is unew on processor 1.
Begin
Formulate problem Ax=b
red_A = A(1:(NN+1)/2,;) /* upper rows of A red nodes */
black_A = A((NN+1)/2:NN,;) /*lower rows black nodes */
initialize x0,k;
IF (not dpis()) THEN /* if link to PVM layer is open*/
dpopen();
/*open and initiate a dp on the master*/
END IF
dpids = dpspawn(wkstnB); /*turn the matlab instance on
wkstnB into a dp instance, and return the PVM id */
dpsend(dpids,x0); /*send x0 to wkstnB with dpid slave*/
WHILE norm((xk) – (xk-1)) • 0.00005
dpsend(dpids,(phase1 reds n/2:n)
k = k+1;
dpsend(dpids,k);
FOR i = 1 to (nred)/2 /* update half the reds
xik = xik-1 + omega*(b-Ax)i/ai,i ;
END FOR
dprecv(dpids,xk(slave-red)) /* receive slave
red updates vector /*
dpsend(dpids, xk(master-red));/*send master
red updates , to slave */
/* phase 2–compute black updates from red node values*/
FOR ik = 1 k-1
to (nblack)/2 /* update black nodes*/
xi = xi + omega*(b-Ax)i/ai,i ;
END FOR
recv(dpids, xik(slave-black));
/* reconstruct unknowns vector */
xk = [xk(master-red),xk(slave-red),xk(masterk
black), x (slave-black)]T
END WHILE
x = xk
/* the solution vector */
dpclose(); /* kill the dp/pvm session on wkstnB*/
END

4.1

Analysis

The algorithm defines a work segment grain Wi, as being an update of a portion of the
total number of nodes of one colour, or more formally in (6) as:

Wi =

nn
pnc

(6)

where nn is the total number of nodes in the mesh, nc is the number of colours, and p is
the number of processors in the cluster. The work segment does not include the
communication overhead at the end of each update phase. The total computation W,
on a sequential machine, is then given in (7) as :

W = ∑i >=1Wi

(7)

For a cluster of p processors, the parallel efficiency will be of the order of W/P.
Inter-processor communications increase at the rate of 2P as the number of processors
increase, however, the size of each work segment decreases as 1/p. For our sample 2
processor, 2 colour ordering, there are 4 work segments giving a parallel efficiency of
2 (approx), with 4 communications (neglecting initial startup). Clearly, keeping the
size of Wi large, and using a small number of cluster processors gives an efficient
parallel implementation for this algorithm.

5.

Conclusion

Numerical methods of solution for sequential architectures do not readily adapt to a
parallel environment, and algorithms specific to the architecture, are required. Here,
we have examined a sequential iterative method of solution for a system of linear
equations, and proposed an implementation in a parallel cluster environment.
Red-Black and Multi-Colour ordering strategies expose data parallelism,
decoupling the equations of Ax = b, allowing Jacobian type updates to occur in
parallel. Convergence rates show no significant difference between the coloured and
the naturally ordered schemes. In fact R-B ordering shows a slight improvement on
the other schemes. The results show the equal sensitivity of natural, red-black, and
multi-colour orderings to ωopt., but a divergence of up to 5% between the predicted and
experimentally obtained values. For higher orders of mesh points, a sensitivity of the
order of 10-3 in the choice of ωopt results in a 15% - 20% fluctuation in the
convergence rate.
The proposed algorithm optimises the computational load on each processor, while
reducing communications overhead between processors. Communication between
processors cannot take place in parallel, when the processors are all connected via the
same backbone network, however PVM’s non-blocking message passing optimises
inter-process communications. Increasing the number of colours, increases the
number of communications between processors, and has a negative impact on parallel
efficiency. Finally, increasing the number of processors in the cluster, in keeping with
Amdahl’s Law, leads to more communication overhead, and reduced efficiency.

References.
1.

Adams, L., Jordan, H., (1986) Is SOR Color-blind? SIAM Journal on Scientific and
Statistical Computing, 7, No. 2, April 1986, pp. 490-506

2.

Dongarra, J., Van der Vorst, H. J., Barrett, R., Berry, M., Chan, T. F., Donato, J., Demmel,
J., Eijkhout, V., (1995) Templates for the solution of Linear Systems. SIAM, Philadelphia.

3.

Dongarra, J., Fischer, M, Experiences with Windows 95/NT as a Cluster Computing
Platform for Parallel Computing, Parallel and Distributed Computing Practices, February
1999 (Vol. 2, No. 2), Nova Science Publishers, USA.

4.

Harrar, D. L., (1993) Orderings, Multicoloring, and Consistently Ordered Matrices,
SIAM Journal of Matrix Analysis and Applications, Vol 14, No. 1, pp259-278.

5.

Heath, Prof. M. T., (1999) Parallel Algorithm Design, Dept. of Computer Science,
University of Illinois at Urbana-Champaign.

6.

Hockney, R. W., Jesshope, C. R., (1988) Parallel Computers 2, IOP Publishing Limited,
Bristol, England.

7.

Pawletta, S., Westphal, A., Pawletta, T., Drewelow, W., (1999) Distributed and Parallel
Applications Toolbox, DP Toolbox, Users Guide. Institute of Automatic Control,
University of Rostock, Germany.

8.

Zomaya, A. Y., (1996) Parallel and Distributed Computing Handbook, McGraw Hill, New
York, pp762-767.

