Available online at www.sciencedirect.com

Procedia Computer Science 4 (2011) 2307–2316

International Conference on Computational Science, ICCS 2011

A Multilevel Cholesky Conjugate Gradients Hybrid Solver for
Linear Systems with Multiple Right-hand Sides
Joshua Dennis Booth, Anirban Chatterjee, Padma Raghavan, Michael Frasca
Department of Computer Science and Engineering, The Pennsylvania State University

Abstract
We consider the problem of repeatedly solving a large sparse linear system Ax = b for the same coeﬃcient matrix
A with a sequence of diﬀerent right-hand side vectors b. We seek to develop a new tree-structured hybrid solver
as an eﬃcient alternative to Preconditioned Conjugate Gradients (PCG) based methods for this problem when A is
symmetric positive deﬁnite and sparse.
In this paper, our key contribution is a tree-based multilevel hybrid of direct and iterative methods using sparse
Cholesky and PCG, respectively. Cholesky is applied to the leaf submatrices and PCG is applied to higher level subsystems in the tree. Partial solutions obtained thereafter are aggregated and corrected to provide the ﬁnal solution. Our
tree-based hybrid solution strategy seeks to couple the accuracy of a sparse direct solver with the memory eﬃciency
of an iterative method to potentially improve overall performance for repeated solutions.
We provide an empirical evaluation on a suite of 12 benchmark matrices from the University of Florida collection. Our results indicate that our tree-based hybrid solver is 1.87 times faster than most variants of PCG, among
several, using diﬀerent combinations of ordering, level-of-ﬁll or threshold methods with an Incomplete Cholesky
preconditioner.
Keywords: Multilevel decomposition, preconditioned conjugate gradient, incomplete cholesky factorization

1. Introduction
The computational simulation of partial diﬀerential equation based models using ﬁnite-diﬀerence or ﬁnite-element
methods involves the solution of large sparse linear systems [1]. The cost of the linear system solution often dominates
overall execution time of such applications and in many instances a linear system with the same coeﬃcient matrix is
solved for a sequence of right-hand side vectors. In this paper, we speciﬁcally consider the multiple right-hand side
case and seek eﬃcient alternatives to Preconditioned Conjugate Gradient (PCG) based solutions when the coeﬃcient
matrix is symmetric positive deﬁnite and sparse.
Consider a linear system Ax = b, where A is the coeﬃcient matrix. Sparse solvers for such systems can be grouped
into two broad categories, namely, direct [2, 3] using sparse Cholesky and iterative [4, 5] using Conjugate Gradients

Email addresses: jdb5132@cse.psu.edu (Joshua Dennis Booth), achatter@cse.psu.edu (Anirban Chatterjee),
raghavan@cse.psu.edu (Padma Raghavan), mrf218@cse.psu.edu (Michael Frasca)

1877–0509 © 2011 Published by Elsevier Ltd. Open access under CC BY-NC-ND license.
Selection and/or peer-review under responsibility of Prof. Mitsuhisa Sato and Prof. Satoshi Matsuoka
doi:10.1016/j.procs.2011.04.251

2308

Joshua Dennis Booth et al. / Procedia Computer Science 4 (2011) 2307–2316

(CG) and its preconditioned variants. Sparse direct solvers are designed to be robust but are mainly limited by large
arithmetic and memory costs that grow superlinearly [6]. Conversely, a sparse iterative solver, such as Conjugate
Gradients [7] and its preconditioned forms, require only enough memory to store just the coeﬃcient matrix and a few
additional vectors but lacks robustness.
We propose a multilevel hybrid of Preconditioned Conjugate Gradients (PCG) [7] and Cholesky, where Cholesky
is applied on leaf submatrices and PCG is applied to subsystems corresponding to higher levels in the tree with
partial solutions aggregated and corrected to provide the overall solution. We expect the setup costs to be higher than
traditional PCG, however, these higher setup costs can be amortized over faster solutions for multiple right-hand sides.
In particular, we seek a hybrid that can provide faster solutions than PCG for multiple right-hand sides. We propose a
tree-based substructuring of A, in which, leaf nodes of the tree represent submatrices of A. Nodes at higher levels of
the tree represent the recursive coupling between these submatrices.
The remainder of this paper is organised as follows. Section 2 provides the relevant background and related
work. Section 3 develops our key contribution, our main solution approach. Section 4 presents our experiments and
empirical evaluation with a brief conclusion in Section 5.
2. Background and Related Work
In this section, we provide a brief background on sparse direct solvers, the Preconditioned Conjugate Gradient
method, and the Incomplete Cholesky factorization. We represent matrices using uppercase letters, for example, A is
a matrix, and vectors using lowercase letters, such as, x is a vector.
2.1. Sparse Direct Solvers
Sparse direct solvers [8, 9, 10, 11, 6] compute robust linear systems solutions through sparse matrix factorizations.
Often during factorization, ﬁllin occurs when zeros in the coeﬃcient matrix become nonzeros. The eﬃciency of a
sparse direct solver is primarily decided by its ability to control and manage ﬁllin. Typically, matrix reorderings are
applied during factorization to preserve sparsity of the resulting factor [12, 13, 14]. However, the overall memory
and computational costs can grow superlinearly with the dimension of the matrix making it unsuitable for threedimensional models [6].
2.2. Preconditioned Conjugate Gradient
The method of Conjugate Gradients(CG) [7] is a popular iterative method used to solve linear systems where the
coeﬁcient matrix A is symmetric positive deﬁnite. In practice, CG works more eﬀectively when used with a preconditioning scheme to improve convergence. In practice, the convergence of CG is governed by the conditioning [2, 15, 16]
of matrix A, that is, in spite of theoretical guarantees on convergence [17], CG could fail if A is ill-conditioned. Preconditioning [4, 18, 2, 19, 5] schemes can improve overall convergence by representing the original linear system
Ax = b as MAx = Mb, where M is the preconditioner matrix. The quality of a preconditioner depends mainly on ease
of its construction, its application and its ability to accelerate CG.
2.3. Incomplete Cholesky Preconditioning
Incomplete Cholesky(IC) [20, 5] preconditioner Lˆ is computed as an approximation to the sparse Cholesky factor
L where A = LLT . The Cholesky factor L is substantially dense as it incurs ﬁll-in, i.e., zeroes in the original matrix
A that become nonzeros during factorization [6]. The incomplete factor Lˆ is obtained by eliminating ﬁllin from
L using two popular method, (i) Incomplete Cholesky with level-of-ﬁll (IC(k)), and (ii) Incomplete Cholesky with
drop-threshold (ICT). An IC(k) level-of-ﬁll preconditioner [5] retains those nonzeros in the factor L that are within
a path length of k + 1 from any node in the adjacency graph representation of A. An ICT [5] preconditioner uses
a drop-threshold δ to decide which elements to drop in the factor L, that is, for δ = 10−2 all nonzero elements in
ˆ The ICT preconditioner provides greater ﬂexibility in creating an incomplete
L less than δ are dropped to form L.
Cholesky preconditioner. The IC preconditioner is applied to CG using two successive triangular solutions in every
CG iteration.

Joshua Dennis Booth et al. / Procedia Computer Science 4 (2011) 2307–2316

2309

3. A New Multilevel Sparse Cholesky-PCG Hybrid Solver
In this section, we develop our new tree-based sparse hybrid solver. We present the basic idea by ﬁrst developing
a one-level hybrid solver. Subsequently, we generalize the one-level to a multilevel hybrid solver with multiple levels
of nested solves. In particular, we elaborate on the solver design for solving a symmetric positive deﬁnite sparse linear
system.
Consider a sparse linear system Au = b, where A is an n × n sparse symmetric positive deﬁnite matrix (A ∈ Rn×n ),
b ∈ Rn is an n × 1 column vector, and u ∈ Rn is the n × 1 solution vector. Direct and indirect schemes can both
be used to solve this sparse linear system, however, the choice depends on the tradeoﬀs between memory demands
and robustness. Our goal is to develop a tree-structured hybrid solution framework that beneﬁts from both direct
and indirect schemes to obtain an eﬃcient solution with low memory overheads for diﬀerent multiple right-hand
side vectors b. Additionally, a tree-based multilevel method extends easily to parallel execution environments. In
developing this framework, we referred to earlier work on domain decomposition schemes [21] for sparse direct [22]
and sparse iterative solvers [23, 24].
3.1. A One-level Hybrid
The zero-nonzero structure of the sparse matrix A enables splitting A into a set of two disjoint subdomains Ω1
and Ω2 that are separated by another smaller subdomain Γ. Each subdomain is a supernode comprising multiple
nodes and this partitioning of A can be interpreted as a supernodal tree, where Ω1 and Ω2 represent leaf nodes rooted
at Γ. The decomposition of A into a blocked system and its supernodal tree representation forms the core of our
tree-based hybrid solver. Section 3.1.1 presents an in-depth description of our one-level supernodal tree construction.
Furthermore, in our tree-based representation, smaller subsystems at the leaf nodes could potentially be solved using
a sparse direct solver to obtain a part of the solution vector. Consequently, the partial solutions could be coupled at
the root node of the tree using an iterative scheme to obtain the complete solution. We provide a detailed description
of this scheme for a one-level tree in Section 3.1.2.
3.1.1. Obtaining a tree-structured aggregate of the coeﬃcient matrix A
We split A into two disjoint subdomains connected by a separating subdomain. We apply the nested dissection [25,
26, 27, 28] algorithm to obtain this split, however, a split could be obtained using any eﬃcient geometric [29] or
combinatorial graph partitioning scheme [27]. Earlier such kind of partitioning have been used for developing hybrid
preconditioners [30] for sparse linear systems. We obtain a reordering of A, such that, (a) rows and columns belonging
to each subdomain are numbered contiguously, and (b) those comprising the separating subdomain are numbered
higher than the disjoint subdomains.
Figures 1(a) to (c) represent a one-level nested dissection ordering of a sample matrix A and the corresponding
supernodal tree. In Figure 1(c) Σ(0) and Σ(1) represent the disjoint subdomains. Σ(0 : 1) indicates the subdomain
block that separates Σ(0) through Σ(1). Nodes in Σ(0 : 1) are numbered higher than nodes in both Σ(0) and Σ(1).
Consider a permutation matrix P that represents this tree-structured reordering of A. An n × n symmetric sparse
matrix B is obtained by permuting A using P. Equation 1 represents the permuted matrix B.
B = PAPT

(1)

3.1.2. Constructing a hybrid solution scheme using the tree-structure
In an earlier research on domain decomposition solvers, Mansﬁeld [31] showed that for a sparse linear system
Au = b, if matrix A is reordered to an equivalent blocked representation (as shown in Equation 2) using domain
decomposition then it can be solved eﬃciently by solving a set of three linear systems.
B=

B11
B21

BT21
B22

,

(2)

Consider the equivalent linear system Bx = f block-wise, where x and f are permutations of u and b corresponding
to the permutations for deriving B from A. It is possible to eﬃciently solve this system blockwise by making a simple

2310

Joshua Dennis Booth et al. / Procedia Computer Science 4 (2011) 2307–2316

(a)

(b)

(c)

Figure 1: (a) Matrix bcsstk11 with natural ordering (b) A one-level nested dissection ordering of bcsstk11 (c) A supernodal tree represention of the
one-level ordering.

assumption. The assumption is that x1 , i.e., the ﬁrst block component of x, can be expressed as the sum of two parts.
The block equation under these assumptions can be written as shown in Equation 3.
B11
B21

x1 = x1S + x1D
x2

BT21
B22

=

f1
f2

.

(3)

This system can be solved for x by ﬁrst solving for x1D as shown in Equation 5. Equation 4 represents the coeﬃcient
matrix S for the subsystem corresponding to x2 .
T
S = B22 − B21 B−1
11 B21

(4)

Subsequently, using x1D we solve for x2 and x1S as shown in Equations 6 and 7, respectively.
B11 x1D
S x2
B11 x1S

=

f1 ,

(5)
B21 x1D ,

=

f2 −

=

−BT21 x2 .

(6)
(7)

We derive our motivation from this approach and propose a scheme that is based on our tree-based restructuring
of A (discussed in Section 3.1.1). Consider a one-level tree with one separator as the root and two disjoint subdomains
as leaf nodes of the tree. In matrix notation we represent this tree structured linear system using Equation 8, with the
assumptions that B11 x1S = −BT31 x3 and B22 x2S = −BT32 x3 .
⎡
⎤⎡
⎤ ⎡
⎤
⎢⎢⎢ B11 0 BT31 ⎥⎥⎥ ⎢⎢⎢ x1 = x1S + x1D ⎥⎥⎥ ⎢⎢⎢ f1 ⎥⎥⎥
⎢⎢⎢
⎥
⎢
⎥
⎢
⎥
S
T
D
(8)
⎢⎢⎣ 0 B22 B32 ⎥⎥⎥⎥⎦ ⎢⎢⎢⎢⎣ x2 = x2 + x2 ⎥⎥⎥⎥⎦ = ⎢⎢⎢⎢⎣ f2 ⎥⎥⎥⎥⎦
B31 B32 B33
x3
f3
Since B is a symmetric positive deﬁnite matrix, a Cholesky decomposition of B can be represented as B = LLT ,
where L is the sparse lower triangular factor. Equation 9 indicates the block-wise Cholesky factorization of B.
⎡
⎤ ⎡
⎤⎡ T
T ⎤
⎢⎢⎢ B11 0 BT31 ⎥⎥⎥ ⎢⎢⎢ L11 0
⎥⎥⎥
0 ⎥⎥⎥ ⎢⎢⎢ L11
0 L31
⎢⎢⎢
⎥
⎢
⎥
⎢
T
T
T ⎥
(9)
⎢⎢⎣ 0 B22 B32 ⎥⎥⎥⎥⎦ = ⎢⎢⎢⎢⎣ 0 L22 0 ⎥⎥⎥⎥⎦ ⎢⎢⎢⎢⎣ 0 L22 L32 ⎥⎥⎥⎥⎦
T
B31 B32 B33
0
0 L33
L31 L32 L33
The Cholesky factor L can be computed blockwise as shown in Equations 10a to 10e.
B11

=

T
L11 L11
,

(10a)

B31

=

T
L31 L11
,

(10b)

B22

=
=

T
L22 L22
,
T
L32 L22 ,

(10c)
(10d)

=

T
T
T
L31 L31
+ L32 L32
+ L33 L33
.

(10e)

B32
B33

Joshua Dennis Booth et al. / Procedia Computer Science 4 (2011) 2307–2316

(a)

(b)

2311

(c)

Figure 2: (a) Matrix bcsstk11 with natural ordering (b) A two-level nested dissection ordering of bcsstk11 (c) A supernodal tree represention of the
two-level ordering.

In our supernodal representation, L11 and L22 are computed at the leaf nodes and reused to compute L31 and L32
as a sequence of linear system solutions as shown in Equations 11 and 12.
T
L11 L31

=

BT31

(11)

T
L22 L32

=

BT32

(12)

The ﬁnal solution x is obtained using a series of intermediate steps. We ﬁrst compute x1P and x2P as shown in
Equations 13 and 14.
T D
L11 L11
x1

=

f1

(13)

T D
L22 L22
x2

=

f2

(14)

Equation 15a computes the coeﬃcient matrix S for the separator linear system. Rewriting this expression using the
Cholesky factors, yields a simpler system as shown in Equation 15b.
S

=

T
−1 T
B33 − B31 B−1
11 B31 − B32 B22 B32

(15a)

=

T
L33 L33

(15b)

We then solve Equation 16 to compute x3 using the PCG scheme with an incomplete cholesky preconditioner.
S x3
Subsequently, we compute

x1S

and

x2S

=

f3 − B31 x1D − B32 x2D

(16)

using the Equations 17 and 18.
T S
L11 L11
x1

=

−BT31 x3

(17)

T S
L22 L22
x2

=

−BT32 x3

(18)

The ﬁnal solution x is computed by aggregating x1D , x1S , x2D , x2S , and x3 .
3.2. A Multilevel Tree-based Hybrid Solver
We apply our tree-structured reordering procedure recursively to each disjoint subdomain until the desired number
of levels η are reached. The above recursion can be represented in the form of a supernodal tree where each level in the
tree corresponds to one step in the recursion. Therefore, the disjoint submatrices at the lowest level of the recursion,
which form the leaf nodes of the supernodal tree, are connected by a structured hierarchy of supernodal separators.
In a supernodal tree with multiple levels, we take a bottom-up approach and evaluate the solution (as discussed
above in Section 3.1.2) at each subtree. We aggregate the solution at each subtree and pass it on to one level higher
as the tree folds during the solution process. Figures 2(a) to (c) represent a two-level nested dissection ordering of the
same sample matrix A as discussed in Figure 1 and the corresponding supernodal tree. In Figure 2(c) Σ(0) through
Σ(3) represent the disjoint subdomains. Σ(r : s) indicates the subdomain block that separates Σ(r) through Σ(s) for
r, s ∈ 0, 1, 2, 3. Nodes in Σ(r : s) are numbered higher than nodes in both Σ(r) and Σ(s). Therefore, Σ(0 : 3) contains
the nodes with the highest numbering.

2312

Joshua Dennis Booth et al. / Procedia Computer Science 4 (2011) 2307–2316

3.3. Computational costs of our hybrid solver
Consider a sparse linear system Bx = f , where B is an n × n sparse matrix. We deﬁne μ(B) as the number of
nonzeros in matrix A. The computational costs of our hybrid solver include (a) the cost to setup the Cholesky factors
of each subdomain, and (b) the cost to solve each subdomain.
Setup cost (Φsetup ). The setup cost is computed as the sum of the square of number of nonzeros in each column
of the Cholesky factor L of A. L∗, j represents the j-th column of L. Equation 19 provides an estimate of the setup cost.
n

Φsetup =

μ2 (L∗, j )

(19)

j=1

Solution cost (Φsolve ). Consider a supernodal tree representation of A with k levels, where level zero is at the root
of the tree and level k indicates the leafnodes. We deﬁne nd d = 1, . . . , 2k as the number of nodes in d-th subdomain
and n s s = 1, . . . , (2k − 1) as the number of nodes in the s-th separator in our supernodal tree. We solve the d-th
subdomain at level k using a sparse direct solver in time τd . For sparse matrices, τd is O(n1.5
d ) when the application
domain is two-dimensional and O(n2d ) in three-dimensions. We solve the s-th separator block Bs using PCG with
Incomplete Cholesky (IC) preconditioning in O(μBs ) + 2 × μ(Lˆ s ) time, where Lˆ s is the IC factor of Bs . Equation 20
presents the cost for the tree-structured solution of B.
2k −1

2k

τd +

Φsolve =
d=1

O(μ(Bs )) + 2μ(Lˆ s )

(20)

s=1

Therefore, the total cost Φhybrid is the summation of the setup cost and the total solution cost for m right-hand
side vectors.
Φhybrid = Φsetup + mΦsolve
4. Experiments and Evaluation
In this section, we present our experimental setup, an empirical evaluation of our solver framework using a suite
of benchmark datasets, and a discussion on our results.
4.1. Experimental Setup
We implemented our hybrid framework in Matlab [32] using Metis [27] to obtain the nested dissection ordering.
We used the sparse Cholesky direct solver to compute the direct solves and the PCG solver with Incomplete Cholesky
preconditioner for iterative solves. We report results on a suite of benchmark matrices from the University of Florida
Sparse Matrix Collection [33]. We report statistics for a total of 10 diﬀerent random repeated right-hand side vectors b.
Metrics for evaluation. We evaluate solver performance using the number of ﬂoating point operations and the
solution accuracy using relative error. Additionally, we perform analysis on the sensitivity of our hybrid solver peformance to number of levels in the supernodal tree and the number of right-hand sides.
Benchmark Matrices. We evalute our solver on a suite of 12 benchmark matrices from the University of Florida
sparse matrix collection [33]. Table 1 lists the details of these matrices. These matrices are typically obtained from
discretization of partial diﬀerential equations using ﬁnite element or ﬁnite diﬀerence methods.
4.2. Evaluation and Discussion
In Table 2 we report the operations count (in millions) required by PCG for our benchmark matrices and 10
repeated right-hand side vectors b. A lower count indicates higher performance and the absense of a value indicates
that the solution did not converge to the desired tolerance of 10−8 . We report our results for 4 matrix orderings
(Natural, RCM, Nested Dissection, and MMD) and 2 preconditioners (Incomplete Cholesky (IC) with zero level-ofﬁll (IC0) and drop-threshold (ICT) of 10−2 ). In Table 2, our hybrid direct-iterative solver (Hybrid-DI) has the lowest

2313

Joshua Dennis Booth et al. / Procedia Computer Science 4 (2011) 2307–2316

Table 1: Benchmark matrices from the University of Florida Sparse Matrix Collection [33].

Matrix
nos7
bcsstk09
bcsstk10
bcsstk11
bcsstk27
bcsstk14
bcsstk18
bcsstk16
cystem01
cystem02
s1rmt3m1
s1rmq4m1

N
729
1,083
1,086
1,473
1,224
1,806
11,948
4,884
4,875
13,965
5,489
5,489

Nonzeros
4,617
18,437
22,070
34,241
56,126
63,454
149,090
290,378
105,339
322,905
217,651
262,411

Description
Poisson”s Equation in Unit Cube
Stiﬀness Matrix - Square Plate Clamped
Stiﬀness Matrix - Buking of Hot Washer
Stiﬀness Matrix - Ore Car (Lumped Masses)
Stiﬀness Matrix - Buckling problem (Andy Mera)
Stiﬀness Matrix - Roof of Omni Coliseum, Atlanta
Stiﬀness Matrix - R.E. Ginna Nuclear Power Station
Stiﬀness Matrix - Corp. of Engineers Dam
FEM Crystal free vibration mass matrix.
FEM Crystal free vibration mass matrix.
Matrix from a static analysis of a cylindrical shell.
Matrix from a static analysis of a cylindrical shell.

operations count in 8 out of 12 matrices. On average our method performs 1.87 times faster than the best PCG and
ordering combination. For the bcsstk10 matrix, the gain in performance is as high as 7.36 times compared to the
best PCG. However, for some matrices, such as crystm02, we observe a degradation in performance. For this class
of matrices, it is often diﬃcult to ﬁnd a compact node separator, consequently, this leads to very small blocks (leaf
nodes) that are solved using Cholesky and very large separators that are solved using PCG. Therefore, the coupling
between these is not eﬀective.
Table 2: Operation counts (in millions) for 10 repeated right-hand side vectors and PCG with IC Level-of-ﬁll (IC0) and drop-theshold (ICT)
preconditioner using (a) natural ordering (NAT), (b) RCM ordering, and (c) nested dissection (ND) ordering, and (d) minimum degree (MMD)
ordering. Hybrid-DI represents our tree-based hybrid solver framework. Values in bold represent the best performance.

Matrix
nos7
bcsstk09
bcsstk10
bcsstk11
bcsstk27
bcsstk14
bcsstk18
bcsstk16
crystm01
crystm02
s1rmt3m1
s1rmq4m1

NAT
IC0
ICT
7.73
1.97
11.60
35.54
11.51
55.21
985.15
986.89 144.01
30.51
5.27
93.42
10.82
432.19
507.84

RCM
IC0
ICT
7.72
12.21
583.60
579.48
1,180.21 1,257.05
30.51
24.13
93.43
73.46
5,795.08
2,079.73

IC0
-

ND
ICT
32.37
687.51
2,128.73
58.63
193.60
-

IC0
-

MMD
ICT
32.56
2,005.01
69.66
190.17
-

Hybrid-DI
2.95
10.42
4.83
16.08
10.42
24.15
247.75
275.54
104.10
911.31
228.93
311.53

Table 3 presents the relative error in the ﬁnal solution. We compute relative error as ||x∗ − x||/||x∗ ||, where x∗ is the
true solution for the linear system solution. We observe that PCG does not converge for a majority of the orderings
even with a preconditioner. However, our tree-based hybrid method is able to produce a solution within an accuracy
of 10−4 or higher.
For a sparse matrix A, we deﬁne OpsHybridDI (A) as the operation count for our Hybrid-DI method, OpsBestPCG (A)
as the operation count for the best PCG performance, and OpsAvgPCG (A) as the average PCG performance for diﬀerent variants of PCG used in Table 2. We compute Speedupbest (A) as the ratio of OpsBestPCG (A) and OpsHybridDI (A).
Additionally, we compute Speedupavg (A) as the ratio of OpsAvgPCG (A) and OpsHybridDI (A).
Figure 3(a) presents Speedupbest (A), the speedup obtained by using our hybrid solver compared to the best PCG

2314

Joshua Dennis Booth et al. / Procedia Computer Science 4 (2011) 2307–2316

Table 3: Relative error for 10 repeated right-hand side vectors and PCG with IC Level-of-ﬁll (IC0) and drop-theshold (ICT) preconditioner using
(a) natural ordering (NAT), (b) RCM ordering, and (c) nested dissection (ND) ordering, and (d) minimum degree (MMD) ordering. Hybrid-DI
represents our tree-based hybrid solver framework.

Matrix
nos7
bcsstk09
bcsstk10
bcsstk11
bcsstk27
bcsstk14
bcsstk16
bcsstk18
crystm01
crystm02
s1rmt3m1
s1rmq4m1

NAT
IC0
ICT
4.8E-06
1.3E-06
0.0E+00
9.5E-06
0.0E+00
6.1E-06
0.0E+00 0.0E+00
4.2E-05
0.0E+00 0.0E+00
0.0E+00 0.0E+00
4.1E-06
4.1E-06

RCM
IC0
ICT
5.2E-06
2.0E-06
0.0E+00
0.0E+00
0.0E+00 0.0E+00
0.0E+00 0.0E+00
0.0E+00 0.0E+00
6.2E-04
5.0E-06

IC0
-

ND
ICT
7.0E-06
0.0E+00
0.0E+00
0.0E+00
-

IC0
-

MMD
ICT
1.6E-06
0.0E+00
0.0E+00
0.0E+00
-

Hybrid
1.7E-04
0.0E+00
0.0E+00
2.6E-05
0.0E+00
1.9E-06
0.0E+00
3.1E-05
0.0E+00
0.0E+00
2.8E-06
2.4E-06

performance. Figure 3(b) presents Speedupavg (A), the speedup using Hybrid-DI compared to an average PCG performance across diﬀerent variants. We observe that certain matrices, such as bcsstk10, can obtain over 7 times
performance gain by using our tree-based hybrid scheme. However, there are also matrices, such as crystm01 and
crystm02, that are not a good candidate for our hybrid scheme and may not beneﬁt from this method. In particular,
this suggests that the success of our methodology is related to the underlying structure and properties of matrix A,
which is an attribute of the application. We anticipate that setup cost will decrease with multiple levels, however, con-

(a)

(b)

Figure 3: Speedup obtained by our method over (a) best PCG and ordering combination, and (b) average PCG performance across diﬀerent variants
for 10 right-hand sides. A speedup greater than 1 indicates improvement.

vergence will be slower for a given right-hand side vector. Consequently, the overall operation count may increase.
However, for a speciﬁc number of right-hand sides, this tradeoﬀ could be exploited to obtain the right balance. For
a problem with few right-hand sides it may be important to consider a hybrid with relatively small number of levels
that also incurs low memory overheads. Figure 4 illustrates this using the example of the bcsstk10 matrix. For fewer
levels in the tree, the memory overheads due to sparse factorization could increase setup costs.
Eﬀect of increasing tree-levels on solver performance. We perform sensitivity study on 6 of our best performing

Joshua Dennis Booth et al. / Procedia Computer Science 4 (2011) 2307–2316

2315

Figure 4: Impact on performance with increase in tree-levels and number of right-hand sides for the bcsstk10 matrix.

matrices from our benchmark suite to understand solver performance as we increase tree-levels. Figure 5(a) indicates
the solver performance for 10 right-hand sides and levels increasing from 1 to 5. We observe that the ﬂoating point
operations increase asymtotically with the number of levels. Therefore, it is desired to set the number of levels, such
that the memory demands of the largest subdomain block are satisﬁed while maintaining lower setup costs.

(a)

(b)

Figure 5: Eﬀect of increasing (a) tree levels, and (b) right-hand sides on operations count (in millions) of our Hybrid solver compared to the best
PCG performance for 10 right-hand sides.

Eﬀect of increasing right-hand sides on solver performance. Figure 5(b) presents sensitivity of increasing
number of right-hand sides on the hybrid solver performance and compares it to the best PCG result for our 6 best
performing matrices. We observe that as the number of right-hand sides increases, the solution cost dominates the
performance. Additionally, our method sustains higher performance compared to PCG and speedup increases with
higher number of right-hand sides.
5. Conclusions and Future Work
In this paper, we developed a multilevel tree-based Cholesky-PCG hybrid solver that solves a linear system Ax =
b with the same coeﬃcient matrix but multiple right-hand sides. For our test matrices, we show that our hybrid
approach for obtaining a sparse linear system solution is 1.87 times faster than the best performing PCG solution for
multiple right-hand sides. We believe that such a hybrid solution framework could maintain the accuracy of a sparse

2316

Joshua Dennis Booth et al. / Procedia Computer Science 4 (2011) 2307–2316

direct solution while reducing memory demands. Additionally, a tree-structured approach enables us to consider the
development of a parallel implementation of our hybrid approach. We seek to explore this aspect of our method along
with tests on a wider range of problems as part of our future work.
6. Acknowledgement
This research was supported in part by the National Science Foundation through grants CCF-0830679, OCI0821527, and CNS-0720749.
References
[1] M. A. Heroux, P. Raghavan, H. D. Simon, Parallel Processing for Scientiﬁc Computing (Software, Environments and Tools), SIAM, Philadelphia, PA, USA, 2006.
[2] J. W. Demmel, Applied Numerical Linear Algebra, SIAM, 1997.
[3] G. Golub, C. V. Loan, Matrix Computations, 3rd Edition, Johns Hopkins University Press, Baltimore, MD, 1996.
[4] O. Axelsson, A survey of preconditioned iterative methods for linear systems of equations, BIT 25 (1987) 166–187.
[5] Y. Saad, Iterative Methods for Sparse Linears Systems, PWS Publishing Co., Boston, MA, 1996.
[6] J. A. George, J. W.-H. Liu, Computer Solution of Large Sparse Positive Deﬁnite Systems, Prentice-Hall Inc., Englewood Cliﬀs, NJ, 1981.
[7] M. R. Hestenes, E. Stiefel, Methods of conjugate gradients for solving linear systems, J. Res. Nat. Bur. Standards. 49 (1952) 409–436.
[8] J. Demmel, S. C. Eisenstat, J. R. Gilbert, X. S. Li, J. W. H. Liu, A supernodal approach to sparse partial pivoting, Tech. Rep. CSL–94–14,
Xerox Palo Alto Research Center (1995).
[9] A. Gupta, F. Gustavson, M. Joshi, G. Karypis, V. Kumar, PSPASES: An eﬃcient and scalable parallel sparse direct solver, see
http://www-users.cs.umn.edu/ mjoshi/pspases (1999).
[10] A. Gupta, V. Kumar, A scalable parallel algorithm for sparse matrix factorization, Tech. Rep. 94-19, Department of Computer Science,
University of Minnesota, Minneapolis, MN, a short version submitted for Supercomputing ’94 (1994).
[11] I. S. Duﬀ, A. M. Erisman, J. K. Reid, Direct Methods for Sparse Matrices, Clarendon Press, Oxford, 1986.
[12] P. Amestoy, T. A. Davis, I. S. Duﬀ, An approximate minimum degree ordering algorithm, SIAM J. Matrix Anal. Appl. 17 (1996) 886–905.
[13] B. Hendrickson, E. Rothberg, Improving the runtime and quality of nested dissection ordering, Tech. rep., Sandia National Laboratories,
Albuquerque, NM 87185 (1996).
[14] I. Lee, P. Raghavan, E. G. Ng, Ordering schemes for preconditioning with sparse incomplete factors, in: Proceedings of the 2003 International
Conference On Preconditioning Techniques For Large Sparse Matrix Problems In Scientiﬁc And Industrial Applications, 2003, longer report
is under preparation.
[15] A. Greenbaum, Z. Strakos, Predicting the behavior of ﬁnite precision lanczos and conjugate gradient computations, SIAM J. Matrix Anal.
Appl. 13 (1992) 121–137.
[16] A. V. D. Sluis, H. A. V. D. Vorst, The rate of convergence of conjugate gradients, Numer. Math. 48 (1986) 543–560.
[17] D. Luenberger, Introduction to Linear and Nonlinear programming, 2nd Edition, Addison Wesley, 1984.
[18] P. Concus, G. Golub, D. O’Leary, A generalized conjugate gradient method for the numerical solution of elliptic partial diﬀerential equations,
in: J. R. Bunch, D. J. Rose (Eds.), Sparse Matrix Computations, Academic Press, 1976, pp. 309–332.
[19] G. Golub, C. V. Loan, Matrix Computations, The Johns Hopkins University Press, 1989.
[20] T. A. Manteuﬀel, An incomplete factorization technique for positive deﬁnite linear systems, Math. Comput. 34 (1980) 473–497.
[21] T. F. Chan, B. Smith, Domain decomposition and multigrid algorithms for elliptic problems on unstructured meshes, Tech. Rep. CAM 93–42,
University of California at Los Angeles (1993).
[22] D. Rixen, C. Farhat, R. Tezaur, J. Mandel, Theoretical Comparison of the FETI and Algebraically Partitioned FETI Methods, and Performance
Comparisons with a Direct Sparse Solver, Int. J. Numer. Meth. Engrg. 46 (1999) 501–534.
[23] C. Farhat, J. Li, An iterative domain decomposition method for the solution of a class of indeﬁnite problems in computational structural
dynamics, Appl. Numer. Math. 54 (2) (2005) 150–166. doi:http://dx.doi.org/10.1016/j.apnum.2004.09.021.
[24] J. Sun, P. Michaleris, A. Gupta, P. Raghavan, A fast implementation of the feti-dp method: Feti-dp-rbs-lna and applications on large scale
problems with localized nonlinearities, International Journal for Numerical Methods in Engineering 60 (4) (2005) 833–858.
[25] M. T. Heath, P. Raghavan, A Cartesian nested dissection algorithm, SIAM J. Matrix Anal. Appl. 16 (1) (1995) 235–253.
[26] A. George, J. W.-H. Liu, An automatic nested dissection algorithm for irregular ﬁnite element problems, SIAM J. Numer. Anal. 15 (1978)
1053–1069.
[27] G. Karypis, V. Kumar, METIS: Unstructured graph partitioning and sparse matrix ordering system, Tech. rep., Department of Computer
Science, University of Minnesota, Minneapolis, MN (1995).
[28] R. J. Lipton, D. J. Rose, R. E. Tarjan, Generalized nested dissection, SIAM J. Numer. Anal. 16 (1979) 346–358.
[29] J. R. Gilbert, G. L. Miller, S.-H. Teng, Geometric mesh partitioning: Implementation and experiments, Tech. Rep. CSL–94–13, Xerox Palo
Alto Research Center, a short version appears in the proceedings of IPPS 1995 (1994).
[30] P. Raghavan, K. Teranishi, Parallel hybrid preconditioning: Incomplete factorization with selective sparse approximate inversion, SIAM J.
Sci. Comput. 32 (2010) 1323–1345.
[31] L. Mansﬁeld, On the conjugate gradient solution of the schur complement system obtained from domain decomposition, SIAM J. Numer.
Anal. 27 (1990) 1612–1620.
[32] J. R. Gilbert, C. Moler, R. Schreiber, Sparse matrices in matlab: Design and implementation, SIAM J. Matrix Anal. Appl. 13 (1992) 333–356.
[33] T. Davis, University of Florida Sparse Matrix Collection, uRL: http://www.cise.ufl.edu/research/sparse/matrices/.

