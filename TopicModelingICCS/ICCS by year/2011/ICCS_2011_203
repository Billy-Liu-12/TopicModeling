Available online at www.sciencedirect.com

Procedia Computer Science 4 (2011) 778–780

International Conference on Computational Science, ICCS 2011

Open Review in computer science
Elsevier grand challenge on executable papers
Y.-A. Le Borgnea,b , A. Campoc
a Computational

Modeling Laboratory and Electronics and Informatics Dpt., Vrije Universiteit Brussel, Pleinlaan, 2, 1050 - Brussels - Belgium
Learning Group, Computer Science Department, Universit´e Libre de Bruxelles, Bd Triomphe, 1050 - Brussels - Belgium
c IRIDIA, Artiﬁcial Intelligence Laboratory, Universit´
e Libre de Bruxelles, 50, Av. F. Roosevelt, CP 194/6, 1050 - Brussels - Belgium
b Machine

Abstract
We present Open Review, a web-based platform aimed at stimulating executable papers by means of post-publication
peer-review. Its goal is to bring computer science researchers to collaboratively build their work upon previous research results, in such a way that transparency, reproducibility and sustainability of research results are greatly improved. The main design goals of the platform are clarity, conciseness, and reproducibility. Its main features are
to: (i) provide incentives for making research communities to participate, (ii) make papers executable by means of
boards’ annotations, without necessarily involving the authors of an article, and (iii) give snapshots of the current
research state on any given article.
Keywords: Executable papers, reproducible research, peer-review, collaborative research, Elsevier Grand Challenge.

1. Towards a post-publication model for executable papers
The design of executable papers is becoming more and more necessary in computer science. Most research articles
published nowadays in computer science present experimental results which are diﬃcult or impossible to reproduce.
Such research articles are strongly detrimental to the whole research community, since they not only fail to provide
reusable blocks of research, but also make the bibliographical tasks of researchers much harder.
The Elsevier Executable Paper Grand Challenge aims at enhancing “how scientiﬁc information is used and communicated in computer- and computational sciences”. In our opinion, the key challenge in making papers executable
consists in stimulating authors to publish their data and code. Indeed, from the perspective of authors willing to have
papers published, sharing data and code before publication creates higher workload and risks of rejection [1]. Therefore, this abstract carefully addresses the question of incentives for researchers to make their research reproducible.
Our proposal in the direction of making executable papers, called Open Review, is original in that it aims at
providing a way to make all articles executables, whether already published or not. Moreover, it does not necessarily
involve authors (although it stimulates them to also participate). Therefore, we do not address technical solutions
which forces authors to comply with coding/data formats that can be embedded within an article prior to publication.
The rationale behind the Open Review platform is to make executable papers by relying on the ongoing reviewing
Email addresses: yleborgn@vub.ac.be (Y.-A. Le Borgne), acampo@ulb.ac.be (A. Campo)

1877–0509 © 2011 Published by Elsevier Ltd. Open access under CC BY-NC-ND license.
Selection and/or peer-review under responsibility of Prof. Mitsuhisa Sato and Prof. Satoshi Matsuoka
doi:10.1016/j.procs.2011.04.082

Y.-A. Le Borgne et al. / Procedia Computer Science 4 (2011) 778–780

779

eﬀorts which almost always happen after an article is published: those of all researchers trying to reproduce the
results of published paper. This abstract summarizes what the platform proposes, why it should work, and how it
brings answers for making executable papers a reality.
2. Open Review : A post-publication web-based platform for reproducible research
2.1. Platform overview
Open Review is a web-based platform, aimed at providing a concise view of the current research state on an article,
together with reproducible results. For each published article, a web page will serve as a reference for following
advances in the research related to the article. Each user will be given a board per article, where they can brieﬂy
sum up their position. These individual boards will be limited in size, e.g., 1000 characters, to enforce clarity and
conciseness of the researcher’s main contributions. Supplementary material will be available on each board, allowing
to bring detailed opinions, results, comparisons, and code. The main innovation with the boards will be to allow
readers interested in a paper to get a quick overview of the positions of the research community on a paper, together
with executable code.
Since researchers’ code, ﬁndings and opinions evolve over time, users will be free to update their board and/or
supplementary material whenever they wish. In order to follow these evolutions, a timeline will be provided on each
article page. When ﬁrst loaded, an article page will present the latest boards from all contributors. The timeline will
allow to trace back, in an intuitive way, the contributions and opinions of each user over time. Internally, the timeline
feature will be implemented using a versioning system.
2.2. Incentives for using the platform
Rather than a sharp change in the reviewing and research practices, the proposed Open Review platform will
address the challenges raised by executable papers in a smooth and realistic way. In a ﬁrst stage, we expect the
research community to bring code and additional experiments to existing papers. The incentive for researchers will be
to show their involvement in diﬀerent research topics, and to ﬁnd rewards in providing the research community with
results which will strengthen or mitigate previously claimed results.
As a concrete scenario of the use of the Open Review platform, let us consider the following example. A researcher has developed an algorithm, which has just been accepted for publication. His experimental results show
improvements upon two previously proposed approaches, published in articles A and B. In order to stimulate interest
of other researchers for his work, he creates a board on A and B articles, and attaches the code allowing to reproduce
his results (e.g., a Matlab script, and a text ﬁle that describes how to use the code). Authors of A and B may then
use their own boards to provide additional results, which show for example that their approach is still competitive using a slightly diﬀerent experimental setup. The beneﬁts are clear: researchers gain visibility, research is accelerated,
and the code added to boards makes paper executable. Moreover, as a side-eﬀect of these interactions, executable
contributions will be more objective, in that they will highlight diﬀerent facets of a published paper.
We also plan to promote identiﬁcation and participation by using reputation metrics that will quantify the contributions of users on the platform (for example, karma points, successfully used on questions and answers web platforms).
Regarding identiﬁcation, we believe that contributors should be free to choose whether they are anonymous or identiﬁed. On most social web sites, users usually choose to identify themselves in order to promote their contributions.
The freedom of anonymity will be left for those who think that their opinion should not be related to their proﬁle.
We however believe that the proportion of such individuals is low enough to allow anonymity on the review platform.
These features, allowing researchers to provide insights on published papers and promote their work, will be the main
incentives for researchers to use the Open Review platform.
3. Answers brought to the executable papers challenge
We discuss in this section the additional challenges identiﬁed by Elsevier in making paper executables. For the
sake of clarity, we grouped these challenges along the four following categories, and summarize our ideas/answers for
each of them:

780

Y.-A. Le Borgne et al. / Procedia Computer Science 4 (2011) 778–780

(i) Executability/ Validation: Make articles truly interactive, allowing readers to manipulate the result space by
interactively tuning data and parameters. Our ﬁrm belief is that the choice of software tools and data formats should
be left to the users. The main incentive for making the code truly reproducible will be that contributors beneﬁt from :
increased visibility and credit for derivative work. As a side-eﬀect, we expect that contributors will choose whenever
possible open source software and data formats. Ideally, “the results should be easily reproduced by an independent
researcher with at most 15 min of user eﬀort, requiring only standard, freely available tools” [2]. Without requiring
such a nice entry to reproducibility, any contributions on how to reproduce results should be welcome [3].
(ii) Compatibility/ Size: Adapt to heterogeneous user operating systems, and deal with experiments involving
large-scale computers and very large ﬁle sizes. By being web-based, Open Review provides an OS-agnostic way to
store results and discuss their content. The reproducibility of results will however require the user to be able to run
the software and load the data formats provided by contributors. Our point is similar to the executability challenge
discussed above: the users should be free to choose a data format and a speciﬁc implementation in order to leave the
platform open to the largest possible range of research practices. Users will however be encouraged to use, whenever
possible, open source and OS-independent software to make their result as reproducible as possible. Finally, the
question of experimental results which involve high computing resources is diﬃcult to answer, and we do not have
convincing answers for this challenge. We simply mention that Open Review could help in giving links to platforms
where data intensive experiments may be reproduced.
(iii) Copyright/ Plagiarism: Maintain authors intellectual property. By keeping track of each user’s contributions
over time, the proposed open review system will allow to better identify a posteriori those who proposed results
which help in advancing knowledge. We therefore believe that Open Review would help in bringing credit for any
stakeholder in a research process, from the data generation to the resulting scientiﬁc outcomes. Regarding plagiarism,
we believe that the platform will potentially serve as a place to detect such practices. More generally, intellectual
property is an important asset in scientiﬁc research. If data or implementations must be kept conﬁdential, authors will
not be able to provide them on the platform (but they can still mention this on their boards). However, an interesting
possibility will be for other users to provide alternative data sets and implementations which will still bring some
executability to the paper. The license under which contributions are provided will be left to the user’s choice [4].
(iv) Provenance/ Long-term compatibility: Track actions taken on executable papers, and make it compatible
to future systems. The tracking of boards over time is a planned feature of Open Review (see Section 2.1). The
compatibility issue for the data formats and code will not be handled directly, but dealt with as a side-eﬀect of the
users’ care in having their contributions reproducible and sustainable. Regarding the evolution of the Open Review
platform itself, we plan to make it open source, so that the code will be reusable and adaptable to diﬀerent contexts.
4. Prototype development and perspectives
A prototype is currently being developed at the Vrije Universiteit of Brussels/Universit´e Libre de Bruxelles
(VUB/ULB), Belgium, which will be made open source during the ﬁrst semester of 2011. The prototype will ﬁrst
be made available to a restrained research community of the VUB/ULB in order to thoroughly test the validity of the
executable paper model presented here. The platform will then be made open to a wider range of universities, with
possible enforcement of users’ identiﬁcation in a ﬁrst stage.
The proposed prototype answers the executable papers challenge in an original way, since the executability is
not embedded in the paper, but made available by means of a post-publication collaborative research platform. We
believe that our approach forms part of the what the next generation of executable papers will be. We stress that careful
attention will be brought to make it interoperable with institutional repositories, publishing houses, and other digital
libraries, using as a primary tool the Open Archives Initiative [5]. If successful, the main interest of the platform will
be to bring executability to the widest possible range of articles, whether published or not.
5. Bibliography
[1]
[2]
[3]
[4]
[5]

J. Mesirov, Accessible reproducible research, Science 327 (5964) (2010) 415.
P. Vandewalle, J. Kovacevic, M. Vetterli, Reproducible research in signal processing, Signal Processing Magazine, IEEE 26 (3) (2009) 37–47.
N. Barnes, Publish your computer code: it is good enough, Nature 467 (7317) (2010) 753.
V. Stodden, The legal framework for reproducible scientiﬁc research: Licensing and copyright, Computing in Science & Eng. (2009) 35–40.
Open Archives Initiative, http://www.openarchives.org.

