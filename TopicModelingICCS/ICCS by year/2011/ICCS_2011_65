Procedia
Computer
Science

Available online at www.sciencedirect.com

Procedia
Computer
Science
00 222–230
(2009) 000–000
Procedia
Computer
Science
4 (2011)

www.elsevier.com/locate/procedia

International Conference on Computational Science, ICCS 2011

Improving Scalability Using Hybrid Asynchronous Methods For
Non-Hermitian Eigenproblems
Jérôme Duboisa,b*, Christophe Calvina, Serge Petitonb
a

CEA/DEN/DANS/DM2S/SERMA/LLPR, CEA-Saclay, 91191 Gif-sur-Yvette Cedex, France
b
Université de Lille 1, Cité scientifique, Villeneuve d'Ascq Cedex 59655, France

Abstract
We propose an optimized implementation of the MERAM method and preliminary experiments to solve non-Hermitian
eigenproblems faster using this asynchronous hybrid method. We focus on improving the communication pattern by providing an
entity called the collector as well as an optimized communication scheme using MPI-2 one-sided communications. The
scalability of the parallelization is discussed and experiments are done to show how well our implementation scales to a large
number of nodes. The one 8 cores node computing time of 3800 seconds is reduced to 4 seconds using 1200 Nehalem cores, and
we achieve linear to superlinear speed-ups thanks to our efficient communication pattern and the coarse-grained parallel nature of
MERAM. Our approach achieved an optimal performance on more than a thousand cores for the first time with MERAM. We
conclude that hybrid asynchronous methods like MERAM with a good communication patter offer tremendous possibilities for
high performance computing.

Keywords: HPC, Asynchronous Hybrid Methods, Restarted Arnoldi Process

1. Introduction
Achieving Exascale performance with current algorithms is not straight-forward for every applications. The
current hardware trends are to increase the number of compute nodes as well as their density, or add dedicated
hardware such as accelerators to speed-up computations. The frequency of the computing elements remains
relatively stable, and the increase in performance is mainly due to the ever increasing number of cores per processor
and the efficiency of the architecture. Consequently, if an application is not able to take advantage of a large number
of computing elements, then its increase in performance will depends solely on the per-core performance increase.
Also, processors with dozens of core will be common in the short term, and any computational scientist will have a
high level of parallelism at his or her disposal. One solution to exploit this parallelism is to change or adapt the main
algorithm of the application. If this solution is not applicable or viable, then one other way is to exploit coarse grain
parallelism with asynchronous hybrid methods. We define hybrid methods by several methods that would run
collaboratively to solve the same problem. These may be identical methods with different parameters, or completely
different algorithms. Such a method is the multiple restarted Arnoldi method [1].
This method has been introduced in 1994 and 1996 [13, 14]. The first experiments were conducted on networks
of computers, and later on grid environments in 2005 [1]. Results were a decrease in the number of iterations, hence
1877–0509 © 2011 Published by Elsevier Ltd. Open access under CC BY-NC-ND license.
Selection and/or peer-review under responsibility of Prof. Mitsuhisa Sato and Prof. Satoshi Matsuoka
doi:10.1016/j.procs.2011.04.024

Jérôme Dubois et al. / Procedia Computer Science 4 (2011) 222–230
Jérôme Dubois/ Procedia Computer Science 00 (2011) 000–000

223

very often a decrease in the execution time. Scalability was not discussed in the previous papers. In the previous
environments, the network was very slow compared to the computational power at disposal, and using asynchronous
MERAM helped get good performance in such a frame. The experiments involved a few dozen computing elements
which were one core processors or Connection Machines. We now have relatively fast communications within and
out of a node, and access to large computational resources. Indeed, we can get the same computational power of the
machines involved in [13, 14, 1] in a multicore multi-socket, say 4 sockets of twelve cores each: 48 computing cores
in one rack. Thanks to a faster network, we can address hundreds and even thousands of cores with the same
numerical concepts introduced 15 to 20 years ago, but applying some modern communication APIs and software
sophistications.
In this paper, we show an optimized implementation of the MERAM method, which can take advantage of a
large number of cores, possibly thousands and even more. A process dedicated to data collection is created as well
as a hierarchy of processes to handle intra-ERAM communications and ERAM to Collector ones. We present
several experiments conducted on a supercomputer, to show the good capabilities of our communication pattern and
implementation.
The paper is organized as follows. Section 2 will present the numerical methods involved for our study: the
Arnoldi process, ERAM and MERAM. Section 3 describes the parallelization of a single ERAM process. We then
introduce our optimizations and new execution scheme for MERAM in section 4. Section 5 will illustrate the
performance obtained using these asynchronous hybrid methods. We then discuss possible improvements in section
6, and conclude in section 7 on the utility of this kind of method for Exascale computing.
2. Numerical Methods
We introduce in this section the numerical methods that we use as a basis for the hybrid method. First we
describe the “single method” components: the Arnoldi process and the Explicitly Restarted Arnoldi Method [3].
Then we show how the hybrid method classically uses the “single methods” components to get a better convergence
of the application and to add a higher level of parallelism.
2.1. Single method
The purpose of the Arnoldi process is to solve the non-Hermitian eigenproblem: Au = λu with λ ∈ C and u ∈ Cn,
and find (λi, ui), a few eigenpairs of A. To do so, the matrix A is projected onto a Krylov subspace using the Arnoldi
Reduction. The algorithm of the Arnoldi Reduction follows the description in table 1.
Table 1. Arnoldi Reduction algorithm. We propose here to use classical Gram-Schmidt orthogonalisation[ 2]. Other orthogonalizations may be
used.

Input : Anxn,v0 ; Output: Vnx(m+1),Hmx(m+1)
For i = 1 to m
do
1. Vi+1 = Avi
2. H1:i,i = V1:i* vi+1
3. Vi+1 = vi+1 – V1:i.vi+1
4. Vi+1 = vi+1 / ||vi+1||2
done
Once the reduction is done, then one may solve the sub-eigenproblem in the subspace, and project the
eigenvectors onto the original matrix space: the Ritz vectors. This whole process is the Arnoldi process, described in
table 2.
After one execution of the Arnoldi process, the solution may not be accurate enough. One way to correct this is
either to choose a larger subspace, or restart the method with a better initial guess made out of the linear
combination of the computed eigenvectors of A, as proposed in [3]. Then the process is iterated until convergence is
reached. This method is called the Explicitly Restarted Arnoldi Method (ERAM).

224 	

Jérôme Dubois et al. / Procedia Computer Science 4 (2011) 222–230
Jérôme Dubois/ Procedia Computer Science 00 (2011) 000–000

Table 2. Arnoldi Process algorithm

Input : Anxn,v0 ; Output: some A eigenpairs (λi, ui),residuals
1. Compute Arnoldi Reduction from A,v0
2. Solve H eigenproblem and keep desired eigenpairs (λi, yi)
3. Compute Ritz vectors : ui = V.yi
4. Compute residuals
2.2. Multiple methods
The key point when starting (or restarting) ERAM is the initial guess. The closer it is to the actual solution, the
better the accuracy. So, one idea presented in [1,4] is to execute several ERAM methods in parallel, and make them
communicate their results to each other before they restart with a new more accurate initial guess. Due to the
different convergence behavior of each method, one method may take advantage of the new solution of another one,
and get better results than any of the other ERAM methods at the next iteration. Furthermore, this kind of technique
allows using asynchronous communications so that each method will not be slowed down by communications if
there is no need of actual data exchange. The algorithm proposed in [1] is summarized in table 3.
Table 3. MERAM algorithm

Input : Anxn,v0 ; Output: some A eigenpairs (λi, ui),residuals
1. Start. Choose a starting matrix VL and a L set of subspaces sizes M = (m1,...,mL)
2. Iterate. Execute L ERAMs in parallel asynchronously until convergence:
a. Execute one step of ERAM
b. Communicate eigen-info to other processes
c. Take overall better result, and restart
3. Parallelization of the ERAM process
The parallelization of ERAM consists of the parallelization of the Arnoldi reduction. It implies parallelizing the
matrix vector operations involving the matrix A as well as the vector operations involving vectors of the matrices H
and V.
We use a block-rowwise distribution of the matrix among MPI processes, and all the vector operations are
distributed between the processes. The linear algebra libraries for basic computations are MKL for BLAS 1 and 2
computations [5], and multithread support is also available.
The limiting factor to the scalability of this parallelization is the reduction or all-to-all communications. For a p
subspace Arnoldi Reduction using Gram-Schmidt orthogonalization and looking at table 1, we need one reduction
after line 2, and another for the norm2 computation after line 4 for each p iteration. One all-to-all operation is done
after line 4, so that every process has an up-to-date version of what has been computed of the V matrix among all
MPI processes. Overall for a p subspace, 3p global communications among the n processes are necessary.
To sum things up, we support hybrid parallelism through MPI and OpenMP for multicore and multinode
computing. We also have a GPU version of the code using CUDA [6], CUBLAS [7] and CUSP, the CUDA sparse
library. In this paper we mainly focus on the multicore and multinode aspect, but results are similar with GPU
acceleration.
4. Optimizing the MERAM communication design
In this section, we will describe the two novel improvements we added to the existing execution scheme of
MERAM. First one is the collector, which will centralize the data and allow more flexible asynchronism. Then to

Jérôme
Dubois
et al.
/ Procedia
Computer
Science
4 (2011)
222–230
Jérôme
Dubois/
Procedia
Computer
Science
00 (2011)
000–000

225

exploit the new asynchronous possibilities of the collector, we chose to take advantage of MPI-2 one-sided
communications.
4.1. General distribution of the communications
When we run several ERAM methods concurrently, each of them can be parallelized and thus we have 2 levels
of MPI parallelism. Consequently, we organize the MPI processes in independent groups, one group per ERAM
method. In each group only one of the computing processes, called elected process, communicates with the other
groups. For each data exchange between two ERAMs, there would be a process-to-handle the exchange (ERAM to
ERAM), and then a distribution by the elected process to its co-workers. This kind of hierarchy avoids involving allto-all like communications between every MPI processes of two or more ERAM methods. To structure the
processes, we use MPI communicators: one general for all active processes and several sub-communicators: one for
each ERAM. There is also a communicator including the Collector and each elected process. See figure 1 for a
graphical view of the optimized communication scheme.
Table 4 Collector execution

while not every ERAM has finished
do
1. try to pick best result;
2. if there is a result
then put best result in shared place
done
4.2. Introducing the collector
When a large number of ERAM processes are involved, we do not want each ERAM to handle the data sending
to every other ERAMs. Consequently, we introduce a dedicated task, whose role will be to centralize the results of
every ERAM, and pick up at any time the best result. We call this process the collector. For the time being, it is one
MPI process, but it could also be a more complex system like a tree of processes where each leaf of the tree would
handle one or more ERAMs. That would help gain speed, flexibility and scalability in the communications, although
the need of such structure for the collector entity seems unlikely. As the communication volume is very small: only
one scalar and vector of size the matrix order are sent at most to and from each ERAM. We discuss this further in
subsection 4.4.
MERAM (4-7-9)
ERAM (4)

ERAM (7)

Elected
process

Collector

Elected
process

ERAM (9)
Elected
process

Figure 1. Optimized MERAM communications scheme. It involves the collector, a dedicated process, and ERAMs. Each ERAM has an
elected process, which is allowed to communicate with every processes.

226 	

Jérôme
Dubois
et al. / Procedia
Science
(2011) 222–230
Jérôme
Dubois/
Procedia
ComputerComputer
Science 00
(2011)4000–000

This collector follows the algorithm described in table 4. The idea is fairly simple: the collector acts as a daemon,
picking up the best result at any moment, and putting it in a place that will be remotely accessible to every ERAMs.
Also, if the convergence or the maximum number of iterations were to be reached, then the collector must stop after
every ERAM has automatically stopped.
Next subsection will explain how the collector knows the results from every ERAM, and also how it may allows a
remote access to every ERAM to the best result.
4.3. Using asynchronous one-sided MPI-2 communication
MERAM naturally exposes a coarse grain parallelism and asynchronism. We want to exploit these
characteristics, by using a specific MPI-2 feature: one sided communications. Basically, the needed operations are
put or get ones. These would involve actively one of the two processes communicating: the collector or the root of
an ERAM process. MPI-2 provides such feature: put and get operations using what is called a window. This window
offers MPI processes to remotely access data located in one MPI process through the MPI-2 put or get operations
and locks.
The collector acts as a passive bank of information, and each active ERAM process will get the best result from it
at each restart, if its own result is not already the best one. Loops where the processes would take the same sequence
of restarting vectors may sometimes happen. To avoid this, we forbid one ERAM to take the best result from the
same other ERAM repeatedly if the restarting vector is the same. With one sided communications, we actively
imply only one MPI process in the communications, and so we try to optimize the computing time of every ERAM
and the collector by implying them in the communications only when necessary.
4.4. Discussing the limitations of the Collector approach
Thanks to our communication design, for N solvers there are only N communication packets sent to the Collector
for each iteration. At most one packet is sent from the collector back to each solver in case a better result is posted
by one solver. Hence there would be 2N-1 data packets exchanged foreach iteration, if they were made synchronous.
In the unlikely case the Collector becomes a bottleneck for a large number of solvers, then one solution is to create
several collectors. Each of them would be attached to one or several ERAMs, and exchange its data with other
collectors. Actually, it would be like having multiple MERAMs or islands of MERAM. We would keep the
reactivity and scalability within each island, but we would possibly introduce a delay in the communications
between islands and hence a few more iterations for each of them. So the achieved performance would possibly be
less good, but it is to weight with the time that would be lost due to the bottleneck limitation of one Collector or
island.
5. Experiments
Hardware. The experiments were conducted on the Titane machine, a national supercomputer in France
(CCRT).This machine consists of 11,520 cores provided by 1,440 bi-quadcores nodes holding 24GB of memory.
The processors serving as a basis is a Xeon X5500 Intel Nehalem-EX clocked at 2,93 GHz. The overall peak power
of Titane is 130 TFlops.
Numerical case. We chose a DingDong matrix whose eigenvalues are clustered around: + or – Pi/2. This kind of
problem where eigenvalues clusters are very tough to solve, and generally require more mathematical involvement
as well as more computing time due to a slower convergence. We fixed the matrix order at 11,760, and the matrices
are dense. Hence, the total number of nonzeros is 138 millions. The parameters that we change are the subspace size
of the Arnoldi reduction for each ERAM, and also the number of ERAMs involved. The restarting strategy is the
same for every ERAM: take the best eigenvector from all ERAMs to restart.
Actual experiments. The logic we applied is the following: we have an application -say one ERAM process- that
we want to run on our workstation or on a supercomputer. Our purpose is to solve the problem as fast as possible.
Consequently, we study the scalability of ERAM depending on the number of nodes. Indeed, ERAM depends on
Arnoldi, which mainly relies on BLAS 1&2 operations. These operations are memory-bandwidth bound unless the
data fits in the processor caches. Depending on the scalability we get, we may try the MERAM approach.

227

Jérôme Dubois et al. / Procedia Computer Science 4 (2011) 222–230
Jérôme Dubois/ Procedia Computer Science 00 (2011) 000–000

Performance note. The ERAM process relies on BLAS 1&2 operations. Hence it is completely memory
bandwidth bound. So, except if the data fits into the processor caches, the two limiting factors are the memory
bandwidth on one node, and then the network performance between nodes: latency and bandwidth. Consequently,
the kind of behavior we expect from the ERAM performance in parallel is the following. First, we exploit to the
maximum extent the performance of one node by multithreading or using MPI processes. Second, we can aggregate
memory bandwidth to ERAM by adding nodes. Thus, for n nodes, we will have n times the memory bandwidth of
one node. From these facts, we deduce that the speed-up is completely bound to the number of nodes, and that for n
nodes we should expect at best a speed-up of n. Consequently, we compute the speed-up using the one full 8 cores
node computation as a reference: speed-up = Tmulti-nodes divided by Tsingle node.
ERAM scalability. We fixed a subspace size of 9, and took as an initial guess v0 = 1 / sqrt(n), n being the matrix
order. We chose a precision on the result of epsilon = 10 -11 for our experiment. We executed ERAM from one node
(8 cores) to 148 nodes (1,184 cores) to test ERAM scalability on the Titane machine. Results are presented in table
5. Number of iterations was stable for every test: 10,996 iterations.
Table 5. ERAM and MERAM experimentations. ERAM is executed with a subspace size of 9, and MERAM with three ERAMs using subspace
sizes of 4, 7 and 9. Hence we note these two instances ERAM (9) and MERAM (4-7-9). ERAM (9) needs 10,996 to reach convergence, whatever
the number of processing elements. The speed-up is the ratio between N nodes computation time over 1 full bi-socket node, implying 8 cores.
ERAM (9)
Cores

Nodes

8

MERAM (4-7-9)

Exec. Time

Speed-up(Theo)

Time / iter.

Exec. Time

Speed-up (Theo.)

Iterations

Time / iter.

1

3735

1 (1)

0.34

284,8

1 (1)

374

0.66

16

2

1659

2.3 (2)

0.15

123,33

2.3 (2)

105

1.17

32

4

826

4.5 (4)

0.075

111,7

2.5 (4)

471

0.23

64

8

416

9 (8)

0.038

23.9

11.9 (8)

119

0.2

120

15

256

14.6 (15)

0.023

8.75

32.5 (15)

150

0.058

216

27

173

21.6 (27)

0.016

5.5

51.8 (27)

77

0.071

400

50

158

23.63 (50)

0.014

5.95

47.9 (50)

265

0.022

1184

148

292.15

12.78 (148)

0.022

4.4

64.7 (148)

222

0.02

As we can see on this table, ERAM scales pretty well according to the number of nodes up to 15 nodes. Then the
computation time becomes smaller than the communications, and ERAM scales less well starting at 27 nodes. For
this problem, the execution time is acceptable, within 300 seconds with a large number of nodes. We identified a
limit for the scalability of ERAM at 15 nodes or 120 cores, for the tested machine, network, and our implementation
of this method. We are able to divide the time by 20-25 by using more than 15 nodes, and we will not easily reach a
hundred of nodes with good scalability. Now we will try to use the asynchronous hybrid MERAM method with our
optimized communication pattern, to gain more scalability.

228 	

Jérôme Dubois et al. / Procedia Computer Science 4 (2011) 222–230
Jérôme Dubois/ Procedia Computer Science 00 (2011) 000–000

Execution time (seconds)

10000

ERAM (9)

MERAM (4-7-9)

1000

100

10

1
1 (8)

2 (16)

4(32)

8 (64)

15 (120) 31 (216) 50 (400)

Nodes (cores) used

148
(1184)

Figure 2. Computation time of one ERAM with a subspace size of 9 compared to one MERAM consisting of 3 ERAMS of subspace size
4, 7 and 9. We use from one node of 8 cores to 148 nodes of the Titane supercomputer.

MERAM performance. To experiment the MERAM scalability, we solve the same eigenproblem we used to
test ERAM scalability, but this time with several concurrent and collaborative ERAM methods. We tested MERAM
with three ERAM processes. The subspaces we chose were of size 9, 7 and 4. Results are presented in table 5, with a
chosen stop criterion epsilon = 10 -11. The number of iterations indicated in the last column for MERAM is the
number of iterations of the largest ERAM, with subspace size equal to 9. Iterations of ERAMs with subspace size 4
and 7 are directly proportional to the subspace size: the smaller the subspace, the greater the number of iterations,
due to fewer computations. For instance with MERAM(4-7-9) on one node, 374 iterations are necessary for
ERAM(9). For ERAM(7) it is 480 and for ERAM(4) it is 841. Consequently, we can compare one MERAM (4-7-9)
to one ERAM (9) by looking at the number of iterations of the sub-ERAM(9) of MERAM(4-7-9). In the case of one
node / 8 cores computations, MERAM needs 374 iterations and ERAM(9) 10,996.
MERAM significantly speeds-up the convergence of the method numerically. When for one ERAM 10,996
iterations were needed, less than 500 are now necessary, and even less than a hundred sometimes. This variance in
the number of iterations is mainly due to the asynchronous aspect of the method, and the non regular order of
execution of each computations and communications for each ERAM. For different number of processors or
subspaces subset, we are guaranteed to run equal or faster than one ERAM process.
When we analyze the performance of MERAM, several facts are to be noticed. First, performance tends to
increase with the number of nodes, but not always regularly. This is due to the variance in the number of iterations,
as stated in the previous paragraph. We can also compare the per-iteration execution time of MERAM with ERAM.
Generally, MERAM takes more time per iteration for the same number of processors/nodes. The reason behind this
is fairly simple: we are executing three ERAMs with the same number of processors/nodes than for one.
Consequently, each of them will have on average the third of what one sole ERAM has. So, we should expect a periteration MERAM(4-7-9) execution time three times the one of ERAM(9). We see it is the case on average.
Optimized communications. Also for the performance side, MERAM is able to keep speeding-up the
computations, even for 148 nodes or 1,184 cores, where a sole ERAM could not. We even have superlinear
acceleration for some cases, due to the numerical acceleration combined with MPI parallelization. We are able to
solve a problem that would take an hour with ERAM(9) on one node in almost 4 seconds with MERAM(4-7-9) on
148 nodes or 1,184 cores. Also, when comparing ERAM and MERAM face to face we get figure 2. MERAM is
always faster than ERAM and that is reassuring, as it is what was expected. Also, MERAM keeps on scaling when
we go just over a thousand cores. This is very encouraging, and shows the interest of the hybrid asynchronous
methods for large machines, as well as the pertinence of the collector design. MPI-2 one sided operations do also
participate in the communication overlapping.

Jérôme Dubois et al. / Procedia Computer Science 4 (2011) 222–230
Jérôme Dubois/ Procedia Computer Science 00 (2011) 000–000

229

MERAM resilience. Another interesting quality of this method is its resilience. It is very tolerant to hardware
faults: if one ERAM were to stop for some reason, then the whole MERAM may continue to execute without
penalties. The only penalty is the loss of new information that the “deceased” ERAM could have brought to the
whole MERAM. Also, when the network is slow compared to the computational power, asynchronous methods have
a better behavior than synchronous ones, as shown in [8], where MERAM is used in a grid environment.
Lastly, we could compare the resilience performance of one MERAM versus one ERAM with checkpointing. If
several nodes were to fail, then MERAM could keep on running with the remaining ERAMs, with a possibility of
restarting the dead ERAMs without penalties. There would be no actual penalty for the remaining ERAMs apart a
maybe slower convergence of the whole MERAM. In the case of a sole large ERAM that would fail due to faults,
then the penalty of restarting this whole ERAM would be far superior to a full MERAM running with some failed
ERAMs. As a side note, restarted Arnoldi can be used from the PARPACK library, which provides parallelized
restarted Arnoldi methods off-the-shell [12]. If there is a fault during a PARPACK restarted Arnoldi, the previous
discussion about the penalty to restart checkpointed Arnoldi still applies: with multiple methods there would be no
penalty. Also, using PARPACK inside a MERAM is a viable option which would combine the attractivity of a fast
off-the-shell parallelized restarted Arnoldi with the good properties of asynchronous hybrid methods.
6. Future works
In the near future, we want to experiment MERAM on tougher and larger matrices for scalability testing onto
several thousands of cores or maybe more and reach the petascale. We also would like to test how the GPU
architecture influences the performance in a supercomputer environment, which is not a trivial matter as seen in
[11]. For the improvements, we will optimize further the MPI-2 calls and the collector to get a smaller variance in
the results when we vary the number of cores. As we are interested in dominant eigenvalue problems also, we may
make one power method collaborate with MERAM to improve the convergence of the power method. Other
eigensolvers might be used, such as those described in [10]. Also, work will be done to use MERAM as an
accelerator for a GMRES solver[9]. Finally, we made only the subspace vary, but it is also possible to vary the
restarting strategy, the orthogonalization used in the Arnoldi Reduction, and also the precision (SP or DP), matrix
format, and so on... to allow an auto-tuning of each ERAM to gain more speed per process.
7. Conclusion
We presented an optimized version of the MERAM communication pattern. We used MPI-2 one sided
communications and created a dedicated process called the collector as well as an elected process in each ERAM
The collector’s role is to keep track of the best results and make them available to each ERAM process. This way,
each ERAM focuses on its own computing, and get possibly better restarting vector for the next iteration. As a
whole, it improves the convergence numerically, with a factor 20 to 100 fold, and also improves asynchronism and
the scalability. This implies a dramatic decrease of the computing time from one hour with a single node ERAM to
almost 4 seconds with a 1,200 cores / 148 nodes MERAM.
Asynchronous hybrid methods show very good properties for the coming Petascale and also Exascale
supercomputers. Fault tolerance, asynchronism, scalability and modularity are necessary features to take advantage
of hundreds of thousands to millions of cores where the average time between interruptions might be critical, or the
network bandwidth limited. Applications that might take advantage of the hybrid method paradigm are multiple, and
we illustrated the non-Hermitian eigenproblem class with MERAM.
Acknowledgements
We thank the CCRT supercomputing center for giving us an access to the Titane machine.
References

230 	

Jérôme Dubois et al. / Procedia Computer Science 4 (2011) 222–230
Jérôme Dubois/ Procedia Computer Science 00 (2011) 000–000
1.
2.
3.
4.
5.
6.
7.
8.
9.
10.
11.
12.
13.

14.

Nahid Emad, Serge Petiton, and Guy Edjlali. Multiple explicitly restarted arnoldi method for solving large eigenproblems. SIAM J.
Sci. Comput., 2005
B. Noble, Applied Linear Algebra, Englewood Cliffs, NJ: Prentice-Hall, 1969.
Y. Saad. Numerical Methods for Large Eigenvalue Problems. Halstead Press, New York, 1992.
N. Emad, S.-A. Shahzadeh-Fazeli, and J. Dongarra.. An asynchronous algorithm on the NetSolve global computing system. Future
Gener. Comput. Syst,, February 2006.
An updated set of basic linear algebra subprograms (blas). ACM Trans. Math. Softw, 2002
NVidia. NVidia Compute Uniﬁed Device Architecture (CUDA) Toolkit, Version 3.1. Nvidia, 2010
NVidia Corporation. Nvidia : Cublas library. Technical report. Whitepaper. Part of CUDA Toolkit, 2009
N. Emad, O. Delannoy, M. Dandouna, "A design approach for numerical libraries in large scale distributed systems," ACS/IEEE
International Conference on Computer Systems and Applications - AICCSA 2010, 2010
Haiwu He, G. Bergere, and S. Petiton.. A hybrid GMRES/LS-arnoldi method to accelerate the parallel solution of linear systems.
Comput. Math. Appl., June 2006
H Gene, H.. Golub, A. van der Vorst, Eigenvalue computation in the 20th century, Journal of Computational and Applied
Mathematics, November 2000
C. Ali, A. Nukada, S. Matsuoka. High Performance Conjugate Gradient Solver on Multi-GPU Clusters Using Hypergraph
Partitioning. Computer Science – Research and Development. Springer, June. 2010.
R. B. Lehoucq and J. A. Scott. An Evaluation of Software for Computing Eigenvalues of Sparse Nonsymmetric Matrices, Preprint
MCS-P547-1195, Argonne National Laboratory, 1996.
G. Edjlali, N. Emad, and S. Petiton. Hybrid Methods on Network of Heterogeneous Parallel Computers. In Proceedings of the 14th
IMACS International Symposium on Iterative Methods in Linear Algebra World Congress, IMACS World Congress, Atlanta, USA,
July 1994
G. Edjlali, S. Petiton, and N. Emad. Interleaved Parallel Hybrid Arnoldi Method for a Parallel Machine and a Network of
Workstations. In Proceedings of the Conference on Information, Systems, Analysis and Synthesis (ISAS'96), Orlando, USA, July
1996.

