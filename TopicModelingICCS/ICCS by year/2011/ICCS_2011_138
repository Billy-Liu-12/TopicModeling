Available online at www.sciencedirect.com

Procedia Computer Science 4 (2011) 538–547

International Conference on Computational Science, ICCS 2011

Shift-invariant similarities circumvent distance concentration in
stochastic neighbor embedding and variants
John A. Leea,1 , Michel Verleysenb
a Department

of Molecular Imaging, Radiotherapy, and Oncology,
Universit´e catholique de Louvain, Brussels (Belgium)
b Machine Learning Group, ICTEAM institute
Universit´e catholique de Louvain, Louvain-la-Neuve (Belgium)

Abstract
Dimensionality reduction aims at representing high-dimensional data in low-dimensional spaces, mainly for visualization and exploratory purposes. As an alternative to projections on linear subspaces, nonlinear dimensionality
reduction, also known as manifold learning, can provide data representations that preserve structural properties such
as pairwise distances or local neighborhoods. Very recently, similarity preservation emerged as a new paradigm for
dimensionality reduction, with methods such as stochastic neighbor embedding and its variants. Experimentally, these
methods signiﬁcantly outperform the more classical methods based on distance or transformed distance preservation.
This paper explains both theoretically and experimentally the reasons for these performances. In particular, it details
(i) why the phenonomenon of distance concentration is an impediment towards eﬃcient dimensionality reduction and
(ii) how SNE and its variants circumvent this diﬃculty by using similarities that are invariant to shifts with respect
to squared distances. The paper also proposes a generalized deﬁnition of shift-invariant similarities that extend the
applicability of SNE to noisy data.
Keywords: Dimensionality reduction, data visualization, norm concentration, similarity preservation, stochastic
neighbor embedding

1. Introduction
The interpretation of high-dimensional data remains a diﬃcult task, mainly because human vision cannot deal
with spaces having more than three dimensions. Part of this diﬃculty stems from the curse of dimensionality, a convenient expression that encompasses all weird and unexpected properties of high-dimensional spaces. Dimensionality
reduction (DR) aims at constructing a low-dimensional representation of data, in order to improve readability and
interpretability. Of course, this low-dimensional representation must be meaningful and faithful to the genuine data.
In practice, the representation must preserve important structural properties of the data set, such as relative proximities, similarities or dissimilarities. The general idea is that dissimilar data items should be represented far from each
Email addresses: John.Lee@uclouvain.be (John A. Lee), Michel.Verleysen@uclouvain.be (Michel Verleysen)
author

1 Corresponding

1877–0509 © 2011 Published by Elsevier Ltd. Open access under CC BY-NC-ND license.
doi:10.1016/j.procs.2011.04.056

John A. Lee and Michel Verleysen / Procedia Computer Science 4 (2011) 538–547

539

other, whereas similar ones should appear close to each other. Dimensionality reduction applies to other purposes than
just data visualization. For instance, DR can be used in data compression and denoising. Dimensionality reduction
can also preprocess data, with the hope that a simpliﬁed representation can accelerate any subsequent processing or
improve its outcome.
Linear DR is well known, with techniques such as principal component analysis [1] and classical metric multidimensional scaling [2, 3]. The former tries to preserve the covariances in the low-dimensional space, whereas the latter
attempts to reproduce the Gram matrix of pairwise dot products. Nonlinear dimensionality reduction [4] (NLDR)
emerged later, with nonlinear variants of multidimensional scaling [5, 6, 7], such as Sammon’s nonlinear mapping [8]
(NLM), and curvilinear component analysis [9, 10] (CCA). Most of these methods are based on the preservation of
pairwise distances. The eighties and early nineties saw the advent of methods inspired by artiﬁcial neural networks
and soft-computing. Auto-encoders with multilayer perceptrons [11] and Kohonen’s self-organizing maps [12] are
the most prominent examples in this trend. After the seminal paper describing kernel PCA [13], spectral embedding
has met a growing interest. Isomap [14], locally linear embedding [15], maximum variance unfolding [16], and other
manifold learners based e.g. on diﬀusion in graphs [17] are only a few examples of spectral methods [18, 19]. Spectral methods provide the guarantee of ﬁnding the global optimum of their cost function. In contrast, methods based
on other optimization techniques generally do not oﬀer this advantage. However, they usually compensate for this
drawback by their capability of handling a broader range of cost functions, which are potentially more relevant for
NLDR. Recent and successful methods following this approach are stochastic neighbor embedding (SNE) [20] and
its variants, t-distributed SNE (t-SNE) [21] and NeRV (standing for neighborhood retrieval and visualization) [22].
All these methods attempt to match so-called similarities, which are basically decreasing functions of the pairwise
distances. Such functions were already used in the cost functions of methods like Sammon’s NLM [8] and CCA [10].
In terms of results, t-SNE and NeRV signiﬁcantly outperform most of the older methods, as shown in [21, 22, 23, 24]
for instance. However, the reasons as to why these methods behave so well still remain obscure.
This paper aims at investigating them pragmatically. It is indeed of utmost importance to explain why features
that seem at ﬁrst sight to be details of design, change fundamentally the behavior of NLDR algorithms, in order
to exploit these changes in future developments in the ﬁeld. For this purpose, we show that SNE and its variants
rely on a speciﬁc formulation of the similarities, which allows these methods to ﬁght the curse of dimensionality.
More speciﬁcally, the similarities involve a softmax ratio that has an important property: the normalization of the
exponential function makes it invariant to shifts applied to its argument. Such a shift invariance facilitates the NLDR
process by circumventing the fact that the phenomenon of norm concentration [25] manifests itself diﬀerently in the
high-dimensional data space and the low-dimensional visualization space. This diﬀerence also explains why NLDR
with a naive distance preservation principle is an ill-posed problem.
The remainder of this paper is organized as follows. Section 2 introduces similarity preservation as a way to
achieve NLDR and also brieﬂy describes SNE and its variants. Section 3 deals with the phenomenon of norm and
distance concentration. Section 4 investigates the property of shift-invariance and proposes a generalized and consistent deﬁnition of similarities. Section 5 gathers the experimental results. Finally, Section 6 draws the conclusions and
sketches some perspectives for the near future.
2. Stochastic neighbor embedding and its variants
In order for a low-dimensional visualization to be faithful to the data it has to represent, some structural properties
must be preserved or reproduced. These properties can be pairwise distances, more general dissimilarities, local
neighborhoods, or similarities. The basic principle that drives SNE, t-SNE, and NeRV is similarity preservation. For
two points in some space, a similarity is deﬁned as a decreasing function of their distance.
Let Ξ = [ξ i ]1≤i≤N denote a set of N points in some M-dimensional space. Similarly, let X = [xi ]1≤i≤N be its
representation in a P-dimensional space, with P ≤ M. The distances between the ith and jth points are given by
δi j = ξi − ξ j 2 and di j = xi − x j 2 in the high- and low-dimensional spaces respectively. The corresponding
similarities in SNE are deﬁned for i j by
σi j =

exp(−δ2i j /(2λ2i ))
k,k i

exp(−δ2ik /(2λ2i ))

and

si j =

exp(−di2j /2)
k,k i

exp(−dik2 /2)

,

(1)

540

John A. Lee and Michel Verleysen / Procedia Computer Science 4 (2011) 538–547

where λi is a bandwidth parameter. If i = j, then σi j = si j = 0 by convention. Similarity preservation could be implemented in various ways, such as e.g. a sum of squared similarity diﬀerences. However, SNE takes advantages of the
fact that the similarities are normalized (they sum to 1). Instead of using cost functions based on squared diﬀerences
between σi j and si j , SNE deﬁnes for each point i a Kullback-Leibler divergence Ei (X; Ξ, λi ) = j σi j log(σi j /si j ).
The resulting cost function can be written as
Ei (X; Ξ, λi ) =

E(X; Ξ, Λ) =
i

σi j log(σi j /si j ) ,

(2)

i, j

and can be minimized with respect to X by means of a gradient descent. This requires the parameters λi in Λ to be
ﬁxed. For this purpose, let us notice that each Ei (X; Ξ, λi ) = j σi j log(σi j /si j ) consists of a constant part that is
the entropy of σi = [σi j ]1≤ j≤N and a variable part that is the cross-entropy between σi and si = [si j ]1≤ j≤N . In SNE,
the scaling parameters λi are adjusted in order to equalize all entropies, namely, H = j σi j log(σi j ) for all i. The
user speciﬁes a perplexity value exp(H) that is trivially converted into the targeted entropy value. The equalization
actually ensures that each data point is given the same weight in the cost function. In the computation of its gradient,
the combination of logarithms in the divergences and the exponential functions in the similarities yields a very simple
update formula: xi ← xi + α j (σi j − si j + σ ji − s ji )(xi − x j ), where α is the step size or learning rate.
In NeRV, the cost function also involves the dual KL divergence, which leads to
(1 − β)σi j log(σi j /si j ) + βsi j log(si j /σi j ) ,

E(X; Ξ, Λ) =

(3)

i, j

where β is a balancing factor. The bandwidth parameters in the similarities are adjusted in the same way as in SNE.
The resulting update for the gradient descent is as simple as the one of regular SNE.
In t-SNE, the modiﬁcations concern mainly the similarities in the low-dimensional space, which are deﬁned as
si j =

(1 + di2j )−1
k,l,k l (1

+ dkl2 )−1

.

(4)

The name of the method stems from the replacement of the Gaussian shape in (1) with an expression that is closely
related to the probability density function of a Student t distribution with a single degree of freedom. Another noticible
change is the normalization, which runs over both indices instead of one only. The update for the gradient descent
becomes xi ← xi + α j (σi j − si j + σ ji − s ji )(xi − x j )/(1 + di2j ) The discrepancy between the Gaussian similarities in the
high-dimensional space and the heavy-tailed ones in the low-dimensional space amounts to applying an exponential
transformation to δi j to obtain di j [26]. In practice, this transformation stretches the distances and allows t-SNE
to yield less cluttered data representations than regular SNE; separations between clusters are also reinforced. In
the paper describing t-SNE, this transformation accounts for the superior results of t-SNE, as compared to those of
regular SNE. Stretched distances are assumed to circumvent a so-called ‘crowding problem’ [21], which intuitively
refers to fact NLDR requires data distributed in vast (hyper-)volumes to be ‘packed’ and displayed on a limited surface
in the low-dimensional space. This assumption is however questioned in [22], where NeRV is shown to perform as
well as t-SNE without using heavy-tailed similarities, and thus without exponential transformation of the distances.
These contradictory results motivate a thorough investigation in order to clearly elucidate why similarity preservation
can be so successful in SNE, t-SNE and NeRV, and to exploit the successful features of these methods in further
developments of the ﬁeld.
3. The phenomenon of norm and distance concentration
High-dimensional spaces have unexpected and counter-intuitive properties. The diﬃculty to cope with them has
been coined the curse of dimensionality [27, 28]. Among many other aspects, the dimensionality of the space aﬀects
the statistical distribution of norms and distances [25]. Let us consider a hypothetical case where vector ξ ∈ R M
has a Gaussian distribution, namely, ξ ∼ G(0, νI). In this case, we have ξ 22 /ν2 = ξT ξ/ν2 ∼ χ2M and ξ 2 /ν ∼ χ M .
Figure 1 shows the probability density function of several χ2M distributions for various values of M. For an increasing
dimensionality M, the mode situated at max(0, M−2) drifts to the right without suﬃcient thickening, since the standard

541

John A. Lee and Michel Verleysen / Procedia Computer Science 4 (2011) 538–547

χ 2M PDF for M ∈ {2, 5, 10, 15, 20, 25}

0.2

4

3.5

x 10

Squared pairwise distances in the MNIST data set

0.18
3
0.16
2.5
Frequency

2
2
2 /ν )

0.14

p( ξ

0.12
0.1

0.08
0.06

2
1.5
1

0.04
0.5
0.02
0
0

5

10

ξ

15
2
2
2 /ν

20

25

30

0
0

50

100

ξi − ξj

150

2
2

200

250

Figure 1: (left) The probability density function of the χ2M distribution for M ∈ {2, 5, 10, 15, 20, 25} (curves from left to right). (right) Histogram of
squared pairwise Euclidean distances for 1000 points in the MNIST data set of handwritten digit images. Notice the small secondary mode at zero,
caused by reﬂexive distances.

√
deviation divided by the mean is equal to 2M/M and tends to 0. This shows that the squared Euclidean √norm
concentrates [25]. For two vectors ξ1 and ξ2 drawn independently from G(μ, νI), we know that (ξ1 − ξ2 ) ∼ G(0, 2νI)
and thus ξ1 − ξ2 22 /(2ν2 ) ∼ χ2M . The case of pairwise distances computed within a ﬁnite set of vectors is very similar
up to a subtle diﬀerence. Let Ξ = [ξi ]1≤i≤N be a ﬁnite set of vectors, with ξi ∈ R M and ξi ∼ G(μ, νI). The statement
δ2i j /(2ν) ∼ χ2M holds only for nonreﬂexive distances, that is, for j i. Reﬂexive distances have a diﬀerent and trivial
distribution, namely the delta distribution, which can be written as δ2ii ∼ G(0, 0I). The probability density function
(PDF) of squared pairwise Euclidean distances δ2i j can thus be written as
p (u; M, N) =

N − 1 u M/2−1 exp(−u/2)
1
δ(0) +
,
N
N
2 M/2−1 Γ(M/2)

(5)

where δ(u) denotes Dirac’s delta function.
The concentration phenomenon applies to many distributions of points [25]. The right side of Fig. 1 shows the
histogram of pairwise distances in a real data set [29]. Concentration phenomenon is clearly visible, as well as the
small spike near zero that accounts for the reﬂexive distances δii .
The fact that distance concentration varies with the dimensionality explains one of the most prominent diﬃculties
of DR, when it relies on distance preservation. A necessary (but still insuﬃcient) requirement is that the shapes of
the distance distributions in both spaces approximately match each other. Figure 1 shows that this is not the case: the
shapes of a χ22 and e.g. a χ220 are irremediably diﬀerent. This also means that DR with a cost function that involves
diﬀerences of (squared) distances [8, 10] makes little sense. In other words, distance preservation should be replaced
with another paradigm that takes into account the dimensionalities of both spaces. An eﬀective idea is to transform the
distances with a function that cancels the concentration, regardless of the dimensionality. The simplest transformation
is linear, with a shift and a rescaling. Visually, this would allow the modes of all distributions in Fig. 1 to shift and to
maximize their overlap. This idea is further supported by Fig. 2 that shows the loci {(u, v)} ⊂ R2 with u and v such that
the cumulative distribution function (CDF) of a χ2M equals that of a χ22 . Distance shifting and scaling can diminish the
discrepancy between the distance distributions in the high- and low-dimensional spaces. However, the shift cannot
be applied to the reﬂexive distances, which would otherwise become negative. If the reﬂexive distances are not
transformed, we can rely on the fact that in a ﬁnite data set the probability to actually observe nonreﬂexive distances
within the ﬁrst percentiles of the χ M distribution is very low. The maximal shift amplitude then corresponds to the
minimal nonreﬂexive distance, i.e. min j, j i δ2i j , and hopefully suﬃces to bring the curve elbow near zero in Fig. 2.
In practice, however, excluding the diagonal of the matrix of pairwise distances might not suﬃce for two reasons.
First, the probability to observe a distance within the ﬁrst percentiles is very low but not exactly null. Second, real

542

John A. Lee and Michel Verleysen / Procedia Computer Science 4 (2011) 538–547
(u, v) s.t. CDFχ22 (p) = CDFχ2M (p) for M ∈ {2, 5, 10, 20, 25}

10

2

v = CDF−1
(p)
χ2

15

5

0
0

10

20
30
u = CDF−1
(p)
χ2

40

50

M

Figure 2: Loci for which a χ22 CDF equals a χ2M CDF, showing that DR driven by naive distance preservation proves to be a bad idea. Distance
preservation would be possible up to a scaling if the curves were straight lines passing through the origin. The percentiles {10, . . . , 90} indicate
that the ﬁrst ﬂat segment of each curve is sparsely populated and likely to be empty in a ﬁnite sample. If this segment is neglected, a linear
approximation of each curve suggests that the most basic transformation of the squared distances consists of a shift to the left and a scaling.

data could contain duplicates and/or near-duplicates. Typically, one can imagine that a document collection contains
several almost identical copies (the original and scanned versions, for instance). These two arguments suggest that
small, unexpected distances should be discarded like reﬂexive ones, in order to allow for a maximal shift.
So far, the provided theoretical framework overlooks the density diﬀerences at each data point. If the density of
points around ξi is lower than that around ξ j , then mink,k i δ2ik is likely to be much larger than mink,k j δ2jk . Hence,
distances δik from the ith vector and δ jk from the jth one, with i j and 1 ≤ k ≤ N, should be shifted diﬀerently. In
other words, a smart way to determine the best shift and scaling for each data point must be found.
4. Shift-invariant similarities
In similarities deﬁned as softmax ratios, shift invariance results from the equality
σi j =

exp(−δ2i j /(2λ2i ))
k,k i

exp(−δ2ik /(2λ2i ))

= σi j

exp(S i2 )
exp(S i2 )

=

exp(S i2 − δ2i j /(2λ2i ))
k,k i

exp(S i2 − δ2ik /(2λ2i ))

,

(6)

where S i is a shift to the left and λi a scaling factor. Working with squared distances δ2i j or with shifted quantities
δ2i j − 2S i2 λ2i has thus no impact on the ﬁnal similarity values. The similarities in the high- and low-dimensional
spaces can therefore match without knowing the value of S i . As a matter of fact, this matching is possible for two
reasons already mentioned in the previous section. First, reﬂexive distances must remain equal to zero and are thus not
considered in the similarity deﬁnition (the case k = i is excluded in the normalization factor). Second, the main mode
of a distance distribution has a left tail that is very thin. This is visible in Fig. 1 (right), where several of the ﬁrst bins
between the peak of reﬂexive distances and the main mode are empty. These empty bins are of the utmost importance
since positivity of the shifted distances limits the shift amplitude to S i ≤ mink,k i 2−1/2 δik /λi . The last inequality also
conﬁrms that the null reﬂexive distance must be excluded in the normalization of the similarity, as done in (1) and (4)
for SNE and t-SNE respectively. Allowing the case k = i would lead trivially to mink δik = 0 and thus S i ≤ 0. Any
shift to the left would therefore be impossible.
In practice, one might conjecture that the constraint k i in the normalization factor could be insuﬃcient in some
speciﬁc cases. For instance, with a very large data set, the probability to observe small distances in the left tail is
increased. Such a small distance could limit the shift amplitude and therefore would almost annihilate the invariance
to shift. Another case where unexpected small distances occur is the presence of duplicated data items, resulting in

John A. Lee and Michel Verleysen / Procedia Computer Science 4 (2011) 538–547

543

δi j = 0 for i j. If duplicates are easy to eliminate, near duplicates are more of an issue. They can be associated with
particular noise patterns that thicken the left tail of the distance distribution and annihilates shift invariance.
A typical example would be to consider text and image documents. Databases are likely to include duplicates of the
same document. In the case of photocopies, scanned documents, watermarked copies, or even character recognition
typos, the copies slightly diﬀer from the genuine document. In this example, the distribution of pairwise distances
consists of three components: reﬂexive distances, spurious small distances caused by noise, and eventually large
distances related to the video content. Only the third component is of interest and a good visualization requires the
shift to be large enough in order to overlook the spurious distances.
From a completely diﬀerent viewpoint, one might point out a lack of consistency in the similarity deﬁnition as it
is proposed in SNE and its variants. The constraint k i in the normalization is equivalent to imposing σii = 0 for
δii = 0, which is not really compatible with the requirement that similarities should be positive decreasing functions
of the distances. On the other hand, a small but nonzero distance leads to a similarity that almost attains its maximal
value according to the deﬁnition in (1). This jump from zero to a large similarity value is also diﬃcult to reconcile
with our claim that small distances are likely to be spurious and should therefore be ignored just as reﬂexive distances.
In order to address this consistency issue, we propose to modify and rewrite the deﬁnitions of the similarities in the
high- and low-dimensional spaces as
σi j =

exp(− max(δi j , τi )2 /(2λ2i ))
k

exp(− max(δik , τi )2 /(2λ2i ))

and

si j =

exp(− max(di j , ti )2 /2)
,
2
k exp(− max(dik , ti ) /2)

(7)

where τi and ti are thresholds. The exclusion of reﬂexive distances previously embodied by the constraint k i in
the normalization can be replaced here with τi = mink,k i δik . This guarantees shift invariance in the high-dimensional
space. As shift-invariance in the low-dimensional space is not necessary, ti can be null. In the new deﬁnition, similarities are indeed decreasing functions of the distances. Moreover, the new formulation is more ﬂexible than the
previous one, as threshold τi can take on any value. With τi > 0, all zero distances are discarded (reﬂexive ones and
those related to duplicates). Instead of setting τi to the shortest non-reﬂexive distance, one can also equate it with the
distance to the T th nearest neighbor of ξi , for any T ≥ 1. Spurious distances can be ignored in this way with a well
chosen value of T . At the same time, this T NN-like strategy also ensures that the value of τi is driven by data and
adjusted in an adaptive way.
Eventually, we can also compare graphically the shapes of the various similarity deﬁnitions and relate them with
the distribution of distances. In Fig. 3, the non-squared non-reﬂexive pairwise Euclidean distances follow a χ M
distribution, depicted by the dotted curve. The second curve (solid black line) corresponds to the complementary
CDF (CCDF) of this distribution, which is arguably the optimal similarity in terms of contrast for the considered
distance distribution [30, 25]. The third curve (dashed gray line) is drawn according the similarity deﬁnition used
in SNE and its variants; the discontuinity at zero is represented. The fourth and last curve (dashed black line) is the
truncated similarity that we propose: it is constant on the left side of threshold τi and Gaussian on the right side. As a
matter of fact, the truncated similarity has mostly the same shape as the CCDF of the actual distribution. In contrast,
the left part of the similarity as used in SNE totally diﬀers but it is unlikely to be exploited as the probability to observe
a distance in this region is very low in practice. Eventually, one might wonder why we should not prefer the CCDF to
the truncated similarity as it is theoretically optimal. The reason is that the CCDF depends on several parameters, such
as the number of degrees of freedom and the scaling, whose value is not known and diﬃcult to estimate. Compared to
the CCDF, the truncated similarity might look as a poor approximation. However, it is computationally much simpler
and its parameterization is not as intricate as in the CCDF. As indicated above, there are straightforward and adaptive
ways to adjust parameters τi and λi .
5. Experiments
This section aims to verify experimentally that shift invariant similarities are the key ingredient that allows SNE
and its variants to outperform other NLDR approaches. For this purpose, we show in various cases that the embedding
quality depends on an eﬀective treatment of both reﬂexive and spurious distances. Any limitation of the shift caused
by these distances degrades the embedding quality. In practice, we compare the similarities as deﬁned in (1) with those
that we propose in (7). The former deﬁnition discards only reﬂexive distances. The latter can show what happens

544

John A. Lee and Michel Verleysen / Procedia Computer Science 4 (2011) 538–547

Similarity functions
Distance distribution
CCDF similarity
(t−)SNE similarity (rescaled)
Truncated similarity (rescaled)

2
1.8
1.6

Similarity

1.4
1.2
1
0.8
0.6
0.4
0.2
0
0

1

2

3
Distance

4

5

6

Figure 3: The shape of several similarity functions. The dotted curve is the PDF of non-reﬂexive Euclidean distances drawn from a 10-dimensional
Gaussian distribution (thus a χ10 PDF). The solid black curve corresponds to the CCDF of the distance distribution, i.e. the most discriminant
similarity for that particular distribution. The thick gray dashed curve shows the shift-invariant pseudo-similarity used in (t-)SNE and NeRV. The
dashed black curve illustrates the truncated shift-invariant softmax similarity that we propose. The last two similarity functions are rescaled to
match the CCDF similarity; the actual maximum value depends on the normalization denominator.

if all distances are considered (τi = 0), if only reﬂexive distances are ignored (τi equal to the distance to the nearest
neighbor), and if other spurious distances are eliminated (τi equal to the distance to the T th nearest neighbor).
Three data sets are considered. The ﬁrst one is an academic example: 1000 points are drawn from a 30-dimensional
Gaussian distribution and we wish to obtain the best 2D visualization of the sample. Unlike the full Gaussian distribution, a ﬁnite sample can be expected to have some random structure that DR methods could reveal. The second
data set contains real data; it is a randomly drawn subset of the MNIST data base of handwritten digits [29]. This
data set comprises 1000 28-by-28 gray-level images whose pixels are rearranged in 784-dimensional vectors. A few
examples are shown on the left of Fig. 4. A 2D visualization is sought here too. The third data set is closely derived
from the second one. Half of the vectorized images are duplicated and speckle noise is added to all pairs of copies.
If p denotes a pixel intensity in the noisefree image, then we know that 0 ≤ p ≤ 1 and its noisy value can be written
as min(1, p + ind(u < 0.01)v/2), where ind is an indicator function and u and v are drawn from a uniform distribution
between 0 and 1. The third data set is intended to investigate the eﬀect of spurious distances. All visualizations are
computed with the same cost function as in t-SNE (perplexity set to 50, 300 iterations). To provide comparison points,
the results of Torgerson’s classical metric MDS [3] (CMMDS) and Demartines’ curvilinear component analysis [10]
(CCA) are also shown. The former is linear and equivalent to PCA, while the latter is an advanced nonlinear method.
In order to assess the quality of dimensionality reduction, we use one of the performance indices decribed in
[31]. It measures the average preservation of K-ary neighborhoods around all data points. The formulation of this
performance index requires ranks of sorted pairwise distances to be deﬁned. The rank of ξ j with respect to ξi in the
high-dimensional space is written as ρi j = |{k : δik < δi j or (δik = δi j and 1 ≤ k < j ≤ N)}|, where |A| denotes
the cardinality of set A. Similarly, the rank of x j with respect to xi in the low-dimensional space is ri j = |{k :
dik < di j or (dik = di j and 1 ≤ k < j ≤ N)}|. Hence, reﬂexive ranks are set to zero (ρii = rii = 0) and ranks
ρik for k
j, even if δi j = δik . This means that nonreﬂexive
are unique, i.e. there are no ex aequo ranks: ρi j
ranks belong to {1, . . . , N − 1}. The nonreﬂexive K-ary neighborhoods of ξi and xi are the sets deﬁned by νiK =
{ j : 1 ≤ ρi j ≤ K} and niK = { j : 1 ≤ ri j ≤ K}, respectively. Eventually, the performance index can be written
N
|νiK ∩ niK |/(KN). The index measures the average normalized agreement between corresponding
as QNX (K) = i=1
K-ary neighborhoods in the high- and low-dimensional spaces. It varies between 0 and 1; for a random embedding,
QNX (K) ≈ K/(N − 1) [31].

545

John A. Lee and Michel Verleysen / Procedia Computer Science 4 (2011) 538–547

4

4

Pairwise distances

x 10

4.5

4

4

3.5

3.5

3

3
Frequency

Frequency

4.5

2.5
2

2.5
2

1.5

1.5

1

1

0.5

0.5

0
0

2

4

6

8
10
δ ij = ξ i − ξ j 2

12

14

16

Pairwise distances

x 10

0
0

2

4

6

8
10
δ ij = ξ i − ξ j 2

12

14

16

Figure 4: Typical images randomly drawn from the MNIST data base of handwritten digits. Noisefree images are on the left, whereas images with
speckle noise are on the right. Histograms of all pairwise Euclidean distances are shown in the bottom row. Notice the appearance of a tiny tertiary
mode in the histogram on the right, close to the peak of reﬂexive distances and caused by near-duplicates in the data set.

The quality assessment curves of the 2D visualization of the 30-dimensional sample is shown in Fig. 5. Quite
obviously, linear DR yields the poorest results: the curve for MDS is the lowest one. Distance preservation with
CCA performs hardly better. As expected, the behavior of t-SNE signiﬁcantly depends on the way similarities are
implemented. A similarity normalization that excludes the terms associated with reﬂexive distances, such as originally proposed in SNE and its variants performs well in this case. However, we see that our alternative deﬁnition with
threshold τi can improve quality. In this example, τi is adaptively made equal to the distance to the T th nearest neighbor. With T = 0, τi = 0 and then no shift is possible without generating negative distances; this leads to a relatively
bad performance. With T = 1, reﬂexive distances are discarded in SNE’s pseudo-similarities, but performance are
slightly improved, especially for small neighborhood sizes (K ≤ 10). With T = 2, the similarity value is equal for the
ﬁrst and second neighbors and rank distinction between them is lost. Therefore, the quality index is much lower for
K = 1 than for K = 2 (the quality index is insensitive to random permutations within the K-ary neighborhoods). As
there are no spurious distances in this example, T = 2 leads to too large a value for τi . Nevertheless, the visualization
quality remains good.
Figure 6 shows the quality assessment curves for the MNIST data. For the noisefree data (on the left), the conclusions remain mostly the same as in the previous academic example, namely, shift invariance is the key to good
results and our similarity deﬁnition performs equally well or sometimes even better than SNE’s pseudo-similarity. For
the images with pairs of noisy duplicated images, the analysis is a bit more complicated, as t-SNE must cope with
spurious distances. All quality assessment curves start very high: as each image is duplicated, its closest neighbor is

546

John A. Lee and Michel Verleysen / Procedia Computer Science 4 (2011) 538–547

0.5
0.45
0.4
0.35

QNX(K)

0.3
0.25
0.2
0.15

CMMDS = PCA
CCA
t−SNE (j≠i)
t−SNE (T=0)
t−SNE (T=1)
t−SNE (T=2)

0.1
0.05
0
0

20

40

K

60

80

100

Figure 5: Quality assessment curves for the Gaussian data set. Each curve indicates the average normalized agreement between corresponding
K-ary neighborhoods in the high- and low-dimensional spaces. The higher the curve, the better the performance (the thin dotted ascending line
indicates the performance level of a random embedding).

pretty obvious, provided noise does not induce too large a distance between them. For K larger than 1, quality falls.
All similarities that discard merely reﬂexive distances lead to rather poor performance. Only our deﬁnition with T = 2
is able to maintain a good quality as the neighborhood size grows.
6. Conclusions
Nonlinear dimensionality reduction driven by the principle of similarity preservation has recently yielded impressive experimental results in the literature. However, the reasons of this breakthrough have remained mostly unidentiﬁed. This paper has shown both theoretically and experimentally that a proper deﬁnition of the similarities brings
useful properties, such as the invariance to shifts. This property allows the similarities to ﬁght the curse of dimensionality and in particular to circumvent the phenomenon of distance concentration. Therefore, the comparison and
matching of shift-invariant similarities computed in spaces of diﬀerent dimensionalities make sense. On the contrary,
naive distance preservation without shift is likely to fail.
This paper has also proposed a modiﬁed and more consistent similarity deﬁnition that generalizes previous ones.
In particular, it introduces a parameter that controls the maximal shift amplitude and widens the applicability of
similarity-based NLDR. For instance, it can cope with speciﬁc noise models and maintain a good level of performance,
in contrast to the regular similarity deﬁnition.
The identiﬁcation of shift-invariance as a key property in the success of similarity-based NLDR will help us to
design better visualization methods, with more eﬀective cost functions and improved robustness.
References
[1]
[2]
[3]
[4]
[5]
[6]
[7]
[8]
[9]

I. Jolliﬀe, Principal Component Analysis, Springer-Verlag, New York, NY, 1986.
G. Young, A. Householder, Discussion of a set of points in terms of their mutual distances, Psychometrika 3 (1938) 19–22.
W. Torgerson, Multidimensional scaling, I: Theory and method, Psychometrika 17 (1952) 401–419.
J. Lee, M. Verleysen, Nonlinear dimensionality reduction, Springer, 2007.
R. Shepard, The analysis of proximities: Multidimensional scaling with an unknown distance function (parts 1 and 2), Psychometrika 27
(1962) 125–140, 219–249.
J. Kruskal, Multidimensional scaling by optimizing goodness of ﬁt to a nonmetric hypothesis, Psychometrika 29 (1964) 1–28.
Y. Takane, F. Young, J. de Leeuw, Nonmetric individual diﬀerences multidimensional scaling: an alternating least squares method with
optimal scaling features, Psychometrika 42 (1977) 7–67.
J. Sammon, A nonlinear mapping algorithm for data structure analysis, IEEE Transactions on Computers CC-18 (5) (1969) 401–409.
P. Demartines, J. H´erault, Vector quantization and projection neural network, Vol. 686 of Lecture Notes in Computer Science, Springer-Verlag,
New York, 1993, pp. 328–333.

547

John A. Lee and Michel Verleysen / Procedia Computer Science 4 (2011) 538–547

0.6

1
0.9

0.5

0.8
0.7

0.4
(K)

NX

0.3

0.5

Q

QNX(K)

0.6

0.4
0.2
CMMDS = PCA
CCA
t−SNE (j≠i)
t−SNE (T=0)
t−SNE (T=1)
t−SNE (T=2)

0.1

0
0

20

40

K

60

80

100

0.3

CMMDS = PCA
CCA
t−SNE (j≠i)
t−SNE (T=0)
t−SNE (T=1)
t−SNE (T=2)

0.2
0.1
0
0

20

40

K

60

80

100

Figure 6: Quality assessment curves for MNIST data. Left: noisefree images. Right: images with speckle noise.

[10] P. Demartines, J. H´erault, Curvilinear component analysis: A self-organizing neural network for nonlinear mapping of data sets, IEEE
Transactions on Neural Networks 8 (1) (1997) 148–154.
[11] M. Kramer, Nonlinear principal component analysis using autoassociative neural networks, AIChE Journal 37 (2) (1991) 233–243.
[12] T. Kohonen, Self-organization of topologically correct feature maps, Biological Cybernetics 43 (1982) 59–69.
[13] B. Sch¨olkopf, A. Smola, K.-R. M¨uller, Nonlinear component analysis as a kernel eigenvalue problem, Neural Computation 10 (1998) 1299–
1319, also available as technical report 44 at the Max Planck Institute for Biological Cybernetics, T¨ubingen, Germany, December 1996.
[14] J. Tenenbaum, V. de Silva, J. Langford, A global geometric framework for nonlinear dimensionality reduction, Science 290 (5500) (2000)
2319–2323.
[15] S. Roweis, L. Saul, Nonlinear dimensionality reduction by locally linear embedding, Science 290 (5500) (2000) 2323–2326.
[16] K. Weinberger, L. Saul, Unsupervised learning of image manifolds by semideﬁnite programming, International Journal of Computer Vision
70 (1) (2006) 77–90.
[17] B. Nadler, S. Lafon, R. Coifman, I. Kevrekidis, Diﬀusion maps, spectral clustering and eigenfunction of Fokker-Planck operators, in: Y. Weiss,
B. Sch¨olkopf, J. Platt (Eds.), Advances in Neural Information Processing Systems (NIPS 2005), Vol. 18, MIT Press, Cambridge, MA, 2006.
[18] M. Brand, K. Huang, A unifying theorem for spectral embedding and clustering, in: C. Bishop, B. Frey (Eds.), Proceedings of International
Workshop on Artiﬁcial Intelligence and Statistics (AISTATS’03), 2003.
[19] L. Xiao, J. Sun, S. Boyd, A duality view of spectral methods for dimensionality reduction, in: Proceedings of the 23rd International Conference on Machine Learning, Pittsburg, PA, 2006, pp. 1041–1048.
[20] G. Hinton, S. Roweis, Stochastic neighbor embedding, in: S. Becker, S. Thrun, K. Obermayer (Eds.), Advances in Neural Information
Processing Systems (NIPS 2002), Vol. 15, MIT Press, 2003, pp. 833–840.
[21] L. van der Maaten, G. Hinton, Visualizing data using t-SNE, Journal of Machine Learning Research 9 (2008) 2579–2605.
[22] J. Venna, J. Peltonen, K. Nybo, H. Aidos, S. Kaski, Information retrieval perspective to nonlinear dimensionality reduction for data visualization, Journal of Machine Learning Research 11 (2010) 451–490.
[23] D. Erhan, P.-A. Manzagol, Y. Bengio, S. Bengio, V. P., The diﬃculty of training deep architectures and the eﬀect of unsupervised pre-training,
in: Journal of Machine Learning Research, Workshop and Conference Proceedings, Vol. 5, 2009, pp. 153–160.
[24] Z. Yang, I. King, Z. Xu, E. Oja, Heavy-tailed symmetric stochastic neighbor embedding, in: Y. Bengio, D. Schuurmans, J. Laﬀerty, C. K. I.
Williams, A. Culotta (Eds.), Advances in Neural Information Processing Systems 22, 2009, pp. 2169–2177.
[25] D. Franc¸ois, V. Wertz, M. Verleysen, The concentration of fractional distances, IEEE Transactions on Knowledge and Data Engineering
19 (7) (2007) 873–886.
[26] J. Lee, M. Verleysen, On the role and impact of the metaparameters in t-distributed stochastic neighbor embedding, in: Y. Lechevallier,
G. Saporta (Eds.), Proc. 19th COMPSTAT, Paris (France), 2010, pp. 337–348.
[27] R. Bellman, Adaptative Control Processes: A Guided Tour, Princeton University Press, Princeton, NJ, 1961.
[28] D. Donoho, High-Dimensional Data Analysis: The Curse and Blessings of Dimensionality, aide-m´emoire for a lecture for the American
Math. Society “Math. Challenges of the 21st Century” (2000).
[29] Y. LeCun, L. Bottou, Y. Bengio, P. Haﬀner, Gradient-based learning applied to document recognition, Proceedings of the IEEE 86 (11) (1998)
2278–2324.
[30] J. Lee, M. Verleysen, Simbed: similarity-based embedding, in: Proc. ICANN 2009, Limassol, Cyprus, 2009.
[31] J. Lee, M. Verleysen, Quality assessment of dimensionality reduction: Rank-based criteria, Neurocomputing 72 (7–9) (2009) 1431–1443.

