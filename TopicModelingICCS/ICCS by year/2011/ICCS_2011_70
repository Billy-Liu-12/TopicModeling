Available online at www.sciencedirect.com

Procedia Computer Science 4 (2011) 176–185

International Conference on Computational Science, ICCS 2011

A Scheduler based on Resource Competition for Parameter Sweep
Workflow
Sucha Smanchat *, Maria Indrawan, Sea Ling, Colin Enticott, David Abramson
Faculty of Information Technology, Monash University, 900 Dandenong Road, Caulfield East, Melbourne, 3145, Australia

Abstract
Grid workflow scheduling has been a prevalent field of research in order to allocate scientific workflow tasks to grid resources.
To actuate these grid workflow scheduling algorithms, schedulers need to be developed for grid workflow management systems.
A scheduler is a component that gathers information, such as estimated execution times and lists of available grid resources, as
inputs for scheduling algorithms. Once a grid schedule is generated, the scheduler uses it to allocate grid resources to the tasks in
the workflow. This is even more complicated for parameter sweep workflow scheduling. As parameter sweep workflows are
repeatedly executed a number of times with different inputs, to schedule them in parallel, the scheduler must be able to handle
multiple workflow instances and multiple scheduling iterations. In this paper, we present a scheduling algorithm for parameter
sweep workflows and suggest an implementation of a scheduler for parameter sweep workflows based on the algorithms. We
highlight the implementation issues encountered in our experience of scheduler development.

Keywords: grid workflow; workflow scheduler; workflow scheduling algorithm; parameter sweep; resource competition

1. Introduction
Grid workflows have become a popular tool for supporting scientists in conducting experiments in high
performance computing environments. Many scientific workflow management systems have been developed in
order to execute workflows on the Grid [1-3]. Essential elements of a scientific workflow management system are
scheduling algorithms which are used to decide on which grid resource each task in a workflow should be executed
so that the entire workflow execution can satisfy certain objectives, such as minimising overall execution time
and/or meeting a deadline. To generate a grid execution schedule, an algorithm needs various types of information
such as a list of available grid resources, estimated execution times of tasks, and estimated file transfer times
between grid resources as inputs. This information can be provided by a workflow scheduler, whose function is to
map tasks to grid resources based on a selected algorithm [4-6].
There are many existing grid workflow scheduling algorithms that aim to solve particular scheduling problems.
For scheduling a parameter sweep workflow, which is used to find optimal solutions over a large number of

* Corresponding author. Tel.: +61 4 0093 0921.
E-mail address: sucha.smanchat@monash.edu .

1877–0509 © 2011 Published by Elsevier Ltd. Open access under CC BY-NC-ND license.
Selection and/or peer-review under responsibility of Prof. Mitsuhisa Sato and Prof. Satoshi Matsuoka
doi:10.1016/j.procs.2011.04.019

Sucha Smanchat et al. / Procedia Computer Science 4 (2011) 176–185

177

parameter combinations, a scheduling algorithm that addresses resource competition is required. Since multiple
instances of a parameter sweep workflow can be executed in parallel [7], the scheduling algorithm needs to be aware
of the resource contention caused by a large number of tasks sharing the same set of resources. In this paper, we
present an improved version of our algorithm proposed in [8] to achieve better performance of parameter sweep
workflow execution. We also propose an implementation of a scheduler and its interaction with the scheduling
algorithm, and highlight the issues encountered during our scheduler development.
The rest of this paper is organised as follows. Section 2 describes the existing work in grid workflow scheduling.
Section 3 describes our scheduling objective and the definitions that are used in our algorithm are described in
Section 4. Section 5 explains an implementation of a workflow scheduler for parameter sweep workflows and the
implementation issues encountered in our work. Section 6 and 7 describes the simulation setup and the result. The
paper is concluded in Section 8 along with our future work.
2. Background
Among many existing grid workflow scheduling algorithms, we focus on the algorithms that try to minimise the
‘makespan’, since this is also the objective of our work.
Three well-known batch mode scheduling algorithms, Min-Min, Max-Min and Sufferage, were proposed by
Maheswaran et al. [9]. They first create a list of tasks that are ready to be executed; this is called the “task
prioritising” phase [4]. In the second phase, the tasks in the list are scheduled to resources based on a heuristic; this
is called the “resource selection” phase [4].
The task prioritising phase in the Min-Min algorithm involves creating a list of tasks that are ready to be
executed. The algorithm then computes the Estimated Completion Time (ECT) of each task for each suitable
resource. A resource that can execute a task with Minimum ECT (MCT) is then chosen as the most suitable resource
for that task. The task and resource are then paired. In the resource selection phase, the resource-task pair with
lowest value of MCT is scheduled first. The process is repeated until every task is scheduled [4, 9]. The Max-Min
algorithm is similar to the Min-Min algorithm except that it schedules a resource-task pair with the highest value of
MCT first.
The Sufferage algorithm uses the Sufferage value, which is the difference between the lowest and second lowest
MCT. The pair of task and resource with the maximum Sufferage value is scheduled first [9]. In practice, the
algorithm gives priority to a task that would suffer the most if it is not executed first [10]. The drawback of these
three algorithms is that they do not consider the time required to transfer the required input files to the scheduled
resource. As an extension to Sufferage, Casanova et al. [10] proposed the XSufferage algorithm for parameter sweep
applications. The Sufferage value in this algorithm is computed taking into account the time required to transfer data
file.
Shi and Dongarra [11] proposed the “Scheduling algorithm for heterogeneous processors with Different
Capabilities” (SDC) based on the HEFT algorithm [12] to deal with tasks that can only be executed by certain
resources. Tasks with “scarce capable resources” - the tasks that can be executed by few resources - are scheduled
earlier so that they will obtain the required resources before other tasks that can be executed by many other
resources [11]. However, since the ranking in HEFT based algorithms is based on the dependency of tasks [4], the
tasks with scarce capable resources further down the workflow may still be blocked [11, 12]. For example, at the
beginning of a workflow, a scarce resource R might be allocated to an overlong task T. This can block the tasks
further down the workflow that are independent of T but dependent on R. In addition, the definition of scarce
resources in this work (“scarce capable resources” mentioned above) does not include the resources that are needed
by several tasks. This justifies the need to include the notion of resource competition into the scheduling algorithm.
To efficiently schedule a parameter sweep workflow for parallel execution, it is necessary to consider the
resource competition issue, and at the same time, support multiple workflow instances. Our previous work in [8, 13]
addressed these issues but did not consider the data transfer time. In this paper, we explain our improved algorithm
and describe a conceptual approach to implement a scheduling algorithm for a scheduler.

178

Sucha Smanchat et al. / Procedia Computer Science 4 (2011) 176–185

3. Terms and definitions
This section describes the refinement of our previous work in [8]. Specifically, the notion of Estimated Transfer
Time (ETT) has been included in the proposed algorithm along with the adjusted ranking function.
3.1. Parameter sweep workflow
A parameter sweep workflow is one that is executed several times to explore combinations of parameters in an
experiment. A parameter sweep workflow instance is instantiated for a unique parameter combination. These
instances are semantically independent of each other and thus can be executed in parallel. Although parameter
sweeps can occur in sub-workflows and can contain loop structures, in this paper we assume the entire workflow is
involved and exclude loop structures from the current version of the algorithm.
A parameter sweep workflow is modelled as a Directed Acyclic Graph (DAG) and is defined as PS = (V,E,R,P)
where V is a set of tasks in the workflow; E is a set of edges representing the precedence dependencies between the
tasks or nodes in the workflow; R is a set of resources for executing the tasks in the workflow; and P is a set of
parameter combinations which can be used to determine the total number of workflow instances to be executed.
Since we are dealing with multiple parameter sweep workflow instances, it is important to note that a task
instance in one workflow instance is different from another task instance in another workflow instance even though
they are derived from the same task in the workflow definition.
3.2. Scheduler
A scheduler encompasses a scheduling algorithm and provides it with information required to generate a grid
workflow schedule. We separate a scheduler into two components: a schedule generator and a schedule executor.
The former is an implementation of a grid workflow scheduling algorithm, which is used to generate an execution
plan. The latter provides scheduling inputs for the generator and uses the execution plan to execute the workflow
through the mechanism of a workflow management system. The scheduler may contain many generators and the
executor can choose from different generators which implement different scheduling algorithms.
3.3. Grid resource
In order to execute the tasks in the workflow, each task must be assigned to a grid resource. Similar to [11], we
assume that each task instance can only be executed by certain resources. During scheduling, a set of current
resources that can execute task instance t or Capable Resources for t is denoted as CR(t) ⊆ R.
In the grid environment, a resource may govern multiple execution nodes. It is therefore necessary to define a
function to retrieve the number of available nodes for scheduling in a resource r as
nodes(r) = number of available nodes in r

(1)

where r ∈ R. In addition, we assume that each node can only execute one task at a time.
There are three commonly known resource performance metrics used in this paper [4]. They are Estimated
Execution Time (EET), Estimated Wait Time (EWT), and Estimated Transfer Time (ETT).
3.4. Resource competition and resource scarcity
The notion of “Resource Scarcity” (RS) is adopted from [11]. To calculate RS, we denote the number of capable
resource nodes for task instance t as CRN(t) which is the sum of the number of nodes in capable resources in CR(t).
CR ( t )

CRN (t ) =

¦ nodes(r )
k

k =1

(2)

179

Sucha Smanchat et al. / Procedia Computer Science 4 (2011) 176–185

where rk ∈ CR(t). The resource scarcity for a task instance t is then defined as

RS (t ) =

CRN (t )

(3)

R

¦ nodes( r )
j

j =1

where rj ∈ R. The resource scarcity is the ratio between the number of capable resource nodes for t and the number
of all resource nodes. A lower resource scarcity value means there are fewer resource nodes that can execute the
task. This value is used to prioritise tasks in our algorithm.
In addition to our adaptation of RS from [11], we prioritise resources based on resource competition so we can
avoid allocating the resources that have a high degree of competition i.e. the resources that are required by many
tasks [8]. The resource competition (RC) of resource r is defined as
RT ( r )

RC ( r ) =

¦
i =1

1
CRN (ti )
ttotal

(4)

where ti ∈ RT(r) and ttotal is the total number of task instances that are to be scheduled. RT(r) ⊆ T is the set of the
requiring unscheduled task instances that can be executed by r. The resource competition is the ratio between the
summation of the inverse ratio of the number of capable resource nodes for each requiring unscheduled task instance
and the number of all unscheduled task instances across all workflow instances. A higher resource competition value
means there are more task instances on average that require that particular resource for execution.
3.5. Ranking function
In order to select a resource for a task, we use a ranking function based on the resource competition value and the
Estimated Completion Time (ECT) of each resource. The ECT of a task instance t when executed by a resource r is
calculated using the following formula [4, 9].
ECT( r , t ) = EET( r , t ) + max( EWT( r ), ETT( r ) )

(5)

The resource competition value alone cannot be used as the ranking value as there might be a resource with low
resource competition value that takes considerably longer to execute a task. Therefore, the rank of a resource r
executing a task instance t is calculated using the following formula.
Rank( r ) = ECT( r , t ) Ɣ exp( K Ɣ RC( r ) )

(6)

Note that a lower rank value is better, since it means the resource has lower competition and can also execute the
task faster. The K constant is used to control the effect of RC on the rank [14]. Increasing the value of K will
increase the effect of RC on the resource selection. It is possible to later add more metrics such as resource
utilisation level into the ranking function and use additional K constants to control their effects as appropriate [14].
4. The proposed algorithm
This section describes the proposed grid workflow scheduling algorithm with the objective of minimising the
overall makespan of the entire parameter sweep workflow execution. To support multiple workflow instances, we
design a batch mode scheduler to gather and schedule ready tasks across multiple workflow instances. We separate

180

Sucha Smanchat et al. / Procedia Computer Science 4 (2011) 176–185

the algorithm into three phases namely, the Instance Generation, Task Prioritising, and Resource Selection, as
shown in Fig. 1.
In the Instance Generation Phase, as a parameter sweep workflow can involved numerous workflow instances, a
control is required so that there are not too many instances of parameter sweep workflow in comparison to the
number of available resources at any given time. The number of input task instances also affects the performance of
the scheduling algorithms. Increasing the number of input task instances in a scheduling round can reduce the
overall makespan as the algorithms such as Min-Min can achieve better resource utilisation and load balancing.
However, too many task instances will cause overpopulated resource queues which may lead to inaccurate wait time
estimation. Currently, if the number of unscheduled task instances is less than double the number of free resource
nodes, a new workflow instance is created and is assigned the next parameter combination (lines 2-5).
After the workflow instances are generated and the resources are updated, a list of capable resources CR(ti) for
each task instance is created (line 8). The resource scarcity (RS) of each unscheduled task is calculated using (3).
After that, the task instances whose predecessor task instances have completed execution are determined and added
to the Ready Batch (RB) (line 11). RB is a set of task instances that are ready to execute based on precedence
dependencies in the workflow and used as an input for the scheduling algorithm. Once all the ready task instances
are obtained, the task instances in RB are sorted in non-decreasing order based on RS. If there are two or more task
instances with the same RS value, the tie is broken by the average EET in non-decreasing order.
In the Resource Selection Phase, the first task instance t1 in the batch is iteratively scheduled. This task is the task
with the lowest RS in the batch i.e. the task that lowest number of resources can execute. The resource competition
(RC) of each capable resource is then calculated using (4). The ranking function (6) is applied to calculate the ranks
of all capable resources of t1 (line 15-19). Finally t1 will be removed from the batch and will be allocated to the
resource with the lowest rank value and then the EWT of the allocated resource is updated. This process repeats until
the batch is empty, and then the scheduling process stops and waits until the next scheduling round.
The next scheduling round is triggered by one of the following events: a new resource joins the grid, which might
consequently trigger the generation of a new workflow instance; a task instance finishes execution which might
enable its succeeding task instances; or a certain period of time elapses.
Phase
Iteration
Control
Instance
Generation

Task
Prioritising

Resource
Selection

Iteration
Control

Input: Parameter sweep workflow, total number of instances, list of grid resources
Output: Schedule for executing parallel instances of parameter sweep workflow
1 while there are more instances to schedule or set of unscheduled tasks T is not empty do
2
while number of unscheduled tasks in set T < (2 * number of free resource nodes)
3
Generate one workflow instance
4
Add task instances from the new workflow instance to set T
5
end while
6
Update available resources in set R
7
for each task instance ti (i ∈ N) in set T do
8
Find capable resources for ti and add them to set CR(ti)
9
Calculate RS(ti) using (3)
10
end for
11
Determine the task instances in set T that are ready to execute and add them to set RB
12
Sort tasks in set RB in non-decreasing order based on RS
13
while set of ready task instances RB is not empty do
14
Get the first task instance t1 from RB
15
for each capable resource rj in CR(t1) do
16
Calculate RC(rj) using (4)
17
Calculate ECT using (5)
18
Calculate rank using (6)
19
end for
20
Assign t1 to the resource rmin with the lowest rank
21
Update EWT of rmin
22
Remove t1 from RB
23
end while
24
Wait for next scheduling round
25 end while

Fig. 1. The proposed algorithm based on resource competition

Sucha Smanchat et al. / Procedia Computer Science 4 (2011) 176–185

181

Resource competition is calculated based on the unscheduled task instances across all instantiated workflow
instances. Therefore, the ranking function is affected by the unscheduled tasks further down the workflow so the
algorithm can avoid allocating the resource with the expected high competition. In addition, a much faster resource
with a slightly higher RC value can be allocated first as it may become available again before the tasks down the
workflow are ready to execute.
5. Implementing the scheduler
In this section, we describe an approach to implement a grid workflow scheduler. As the proposed algorithm is a
batch mode algorithm, the scheduler described in this section is designed with the intention to handle batch mode
scheduling algorithms only. Based on our prototype development in Nimrod/K [7], we highlight some
implementation issues which should be considered when implementing a scheduler.
5.1. Overview of scheduler for parameter sweep
We separate the scheduler into executor and generator components, which allows the generator to switch between
different scheduling algorithms for situations that have different scheduling characteristics.
The executor performs the functions of the Iteration Control Phase and the Instance Generation Phase. These
two phases will be the same regardless of the choice of scheduling algorithm. On the other hand, the core of the
algorithm consisting of the Task Prioritising Phase and the Resource Selection Phase is implemented separately
from the executor. It is implemented as a generator since they will be different in different algorithms. The executor
only needs to interact with the generator when providing inputs, requesting the schedule to be generated, and
receiving the generated schedule. Hence the coupling between the executor and the generator is minimised.
5.2. Scheduler termination
As batch mode algorithms may involve several rounds, the scheduler needs to know when to stop or when the
entire workflow execution actually completes. As the parameter sweep workflow is represented by a directed acyclic
graph, the termination of the scheduler can be determined by three basic conditions: the last task instance completes;
there are no more tasks unscheduled; and there are no “tokens” left at any tasks [15]. A token here refers to the
token that is used to control the execution of task instances in a workflow according to the precedence dependencies.
When there are enough tokens in every input, a task instance may execute. As the task instance starts execution, a
token is consumed from each of its input. Once the task completes, a token is produced to each of its output.
Therefore, once the entire workflow finishes execution, there must be no token left at any task instances [15]. To
meet this requirement, all workflows scheduled based on our proposed algorithm need to be well-structured and
bounded following the definitions in [15].
5.3. Estimation of transfer time
The transfer time between resources can be determined in various ways. For example, the scheduler can record
the transfer history and use the average value. The transfer time can also be determined by middleware such as the
Network Weather Service [16]. However, if there are multiple copies of the same file on the Grid i.e. multiple file
servers, the server that the file is to be transferred from must be determined. A rational approach is to transfer the
file from the source with minimum transfer time. However, it is possible that this particular resource is also a source
for other files, and transferring from this resource might actually be slower than estimated.
Another issue in estimating transfer time is where a task needs multiple files. Once the file source is decided, the
ETT can be determined. However, if there are multiple files required, to calculate ECT, a single final ETT needs to
be determined from the ETTs of all the files. In the best case, the final ETT can be the maximum of all ETTs
assuming all files are transferred in parallel. Alternatively, by considering the worst case that all the files are
transferred from the same source in sequential order, the final ETT could be the sum of all ETTs of the files. The
combination of both approaches could be more accurate where the ETTs of the same source are summed together

182

Sucha Smanchat et al. / Procedia Computer Science 4 (2011) 176–185

first and then the final ETT is assigned the maximum of the summed ETTs. However, it is also possible that some
resources transfer files in parallel while others transfer files sequentially.
In our current simulation, files are transferred from the sources with the minimum ETT. In the case of multiple
file transfers, since our simulation environment models the file transfers in parallel, the final ETT is determined by
using the maximum of all the ETTs.
5.4. Resource queues and firing
A scheduler may maintain the queues for the resources so that it can keep track of the estimated wait time (EWT)
on each resource. These queues are within the scheduler's executor, and are therefore separated from the queues
managed by the resources themselves. At the end of each scheduling round, the resource queues will reflect the
schedule generated by the scheduling algorithms.
To execute the scheduled task instances, as there could be task instances from the previous scheduling round
waiting to be executed, the scheduler goes through each resource queue and tries to start the task instances within.
The scheduler may choose to follow the generated schedule strictly, and only tries to fire the first task in the
resource queue. If the first task in the queue cannot be started because either the files required are not ready or there
is no free node on the scheduled resource, the scheduler stops trying the current resource queue and proceeds to the
next queue. This approach has the advantage that the order of task execution is kept according to the schedule
generated by the scheduling algorithm. However, when there are many free resource nodes, the scheduler might
miss an opportunity to improve the overall makespan by allocating the task instances whose files are ready.
The scheduler may also choose to violate the schedule by skipping the task instance that cannot start in the queue
and start the ones that are ready first. This approach has the advantage that the free resource nodes are used
immediately (higher resource utilisation). However, since this approach violates the schedule, the skipped task
instance might have to wait even longer than anticipated since the scheduler already allocated the free resource
nodes to other task instances. This delayed task instance could also propagate the delay through its succeeding task
instances and eventually increase the overall makespan. Note that the number of free resource nodes must be known
to the scheduler in this approach.
Our implementation combines the two approaches to try to make use of free resource nodes while maintaining
the schedule. If the scheduler encounters a task instance that cannot start, the number of nodes required by that task
instance is kept and accumulated. Once the scheduler finds a task instance that can start, it will start that task
instance only if the number of free resource nodes on that resource is greater than or equal to the number of
accumulated required nodes plus the number of nodes required by that particular task instance; otherwise the
scheduler will stop trying and proceed to the next resource queue.
6. Simulation setup
This section presents the simulation result of the proposed algorithm. A prototype scheduler has been developed
for Nimrod/K [7], which provides workflow schedulers with access to the underlying features of grid middleware
tools as well as Nimrod’s parameter exploration tools. It gives a scheduler access to all the file transfers, compute
resource scheduling and file streaming that need to be scheduled [7]. Nimrod/K also has the APIs for a scheduler to
support multiple grid middleware.
6.1. Scenarios
We use three different workflow scenarios, each with three settings to depict the behaviour of the proposed
algorithm. The simulation is set to execute 1, 5, 10, 20, 50 and 100 parameter sweep workflow instances. The PS
task is used to generate workflow instances during run-time and does not require grid execution. The sequential
scenario (S scenario) in Fig. 2a is composed of 5 tasks in sequential structure. In this scenario, it is assumed that the
instances of t1 use the shared file “InputT1” as denoted at the input arc along with the size of the file in the form of
(file_name, file_size). These shared input files, if already existed at the scheduled resource, does not need to be
transferred again. The output files of the task instances, on the contrary, are assumed to be specific to each workflow
instance and need to be transferred to the resources scheduled for the succeeding task instances that use them as

183

Sucha Smanchat et al. / Procedia Computer Science 4 (2011) 176–185

inputs. This is to simulate the real parameter sweep workflow since the files produced by tasks in each workflow
instances could be different.

Fig. 2. Simulation scenarios

The parallel scenario (P scenario) in Fig. 2b is composed of 4 tasks in a parallel structure which joins back to the
fifth task t5. In this scenario, it is assumed that all instances of t1, t2, t3, and t4 use the same shared files: “InputT1”,
“InputT2”, “InputT3”, and “InputT4” respectively. The output files become inputs for instances of t5. The mixed
scenario (M scenario) in Fig. 2c is composed of 2 sequential branches in parallel structure which joins back to the
fifth task. In this scenario, it is assumed that all instances of t1 and t3 use the same shared file “InputT1” and
“InputT3” respectively.
The initial location of all the shared input files in all the scenarios is the resource r1. The transfer rates in
kilobytes per second between resources are currently assumed to be static and are shown in Table 1 with the transfer
sources specified in the table columns. Each scenario is simulated in 3 settings. These settings feature the difference
degrees of resource competition induced by the tasks t2, t3, and t4. The first setting is shown in Table 2a. It features a
single resource r2 which is the only resource that can execute all 3 tasks. The second setting in Table 2b reduces the
resource competition on r2 but increase the resource competition on r1. In the third setting in Table 2c, each of the
resources r1, r2, and r3 execute different tasks.
The simulation is performed using these 9 scenario-setting combinations (S1, S2, S3, P1, P2, P3, M1, M2 and
M3) on both the proposed algorithm and the Min-Min algorithm modified for multiple instances scheduling by
extending it with the Instance Generation Phase and the Iteration Control described in the section 4 [8].
Table 1. Estimated transfer rates between resources
r1

r2

r3

r4

r5

r1

-

r2

110

60

70

100

90

-

100

60

70

r3

100

r4

50

90

-

130

80

80

120

-

90

r5

90

70

110

120

-

Table 2. EETs settings

r1

t1

t2

t3

t4

t5

5

-

-

-

4

r1

t1

t2

t3

t4

t5

5

-

4

-

4

r1

t1

t2

t3

t4

t5

5

-

4

-

4

r2

4

5

4

6

3

r2

4

5

-

6

3

r2

4

5

-

-

3

r3

6

-

-

-

4

r3

6

-

-

-

4

r3

6

-

-

6

4

r4

8

-

-

-

6

r4

8

-

-

-

6

r4

8

-

-

-

6

r5

7

-

-

-

6

r5

7

-

-

-

6

r5

7

-

-

-

6

a)

b)

c)

184

Sucha Smanchat et al. / Procedia Computer Science 4 (2011) 176–185

Fig. 3. Improvement of the proposed algorithm over Min-Min algorithm

7. Result and discussion
The result of the simulation shows an improvement in performance of the proposed algorithm over the modified
Min-Min algorithm. In the sequential scenarios in Fig. 3a, the proposed algorithm performs better than the Min-Min
algorithm overall. In the first (S1) and second (S2) settings, minor improvement can be obtained since the resource
r2 becomes a bottleneck. This creates a resource critical path formed by the populated queue in r2. As the workflow
structure is sequential, the performance can only be gained at the start of the execution before this resource critical
path is formed. The third setting (S3), however, shows increasing improvement as more instances are executed. This
is because the resource critical path is formed by the 3 resources r1, r2, and r3. When executing multiple instances,
albeit being in a sequential structure, the resources r1, r2, and r3 can execute their specialised tasks from different
instances in parallel. The proposed algorithm improves the performance by avoiding allocating these resources to t1
and t5.
The result of the parallel scenarios in Fig. 3b does not show any significant improvement. In the first (P1) and
second (P2) settings, similar to the sequential scenario S1 and S2, the resource critical path is formed by the
resource r2 and hence the performance cannot be gained from the critical path. The third setting (P3) also does not
show any significant improvement as in S3 because the tasks t2, t3, and t4 are executed by different resources in
parallel. This makes the behaviour of the proposed algorithm almost similar to the Min-Min algorithm. There are
also few cases where the Min-Min algorithm performed better than the proposed algorithm such as in P1 with one
workflow instance. These are caused by transferring files back and forth at the same resource since the proposed
algorithm does not look ahead to include the future transfer times of the output files.
In the mixed scenarios in Fig. 3c, the first (M1) and second (M2) settings does not show significant improvement,
similar to the sequential and parallel scenarios. However, in the third setting (M3), significant improvement is
observed at 50 and 100 instances. This is similar to the third setting of the sequential scenario (S3). Further
simulation in this setting was conducted and the result showed that the performance gain started to increase when
executing 28 instances.
Since the third setting of the mixed scenario (M3) is more complex than the other scenarios, it takes longer for
the resource queues to populate and form resource critical path. On the contrary, the resource critical path in other
scenarios forms quickly by the resource r2. From our analysis, the proposed algorithm tends to increase its
performance after the resource critical path is formed, and the more resources that execute specialised tasks such as
r1, r2, and r3 in the resource critical path, the better it performs. This is also reflected in S3 since the critical path is
formed early because of its sequential structure and the resources r1, r2, and r3 are all in the resource critical path.
8. Conclusion and future work
A new grid workflow scheduling algorithm for parameter sweep workflows to handle resource competition and
multiple workflow instance scheduling is presented in this paper. The algorithm, which includes file transfer times
in the schedule calculation, performs better than the Min-Min algorithm overall. The performance of the proposed
algorithm significantly improves when there are more resources with high competition or resources that execute
specialised tasks in the resource critical path. Along with the algorithm, we propose its scheduler implementation

Sucha Smanchat et al. / Procedia Computer Science 4 (2011) 176–185

185

and highlight a number of implementation issues which need to be considered when implementing a scheduler. By
separating the scheduling algorithms from the scheduler, other grid workflow scheduling algorithms can potentially
be implemented and easily added to the scheduler.
We plan to implement other batch mode scheduling algorithms such as Max-Min [9] and XSufferage [10] to
compare with the proposed algorithm in the real grid environment. Further investigation is also needed to determine
the optimal proportion between the number of input task instances in each scheduling round and the number of
resource nodes which is expected to be dependent on the structure of the parameter sweep workflow. As many
parameter sweep workflows involve loop, the proposed algorithm will be improved to handle loop structure as well.
The estimation of file transfer time will also be refined given that the required information regarding the file
transfers is available to the scheduler. As the current simulation assumes static data transfer rate, in the next stage
we plan to test the algorithm using variable data transfer rate as well as using real parameter sweep workflows in the
actual Grid environment.
Each grid workflow scheduling algorithm tends to perform better in a specific situation it is designed for.
Inspired by the selective algorithm proposed in [17], we have been exploring the possibility of implementing a
selective scheduler to switch between several scheduling algorithms during run-time to utilise their advantages in
different situations.

References
1.
2.
3.
4.
5.
6.

7.
8.
9.
10.
11.
12.
13.
14.
15.
16.
17.

Ludäscher, B., et al., Scientific workflow management and the Kepler system. Concurr. Comput. : Pract. Exper., 2006. 18(10): p. 1039-1065.
Deelman, E., et al., Pegasus: A framework for mapping complex scientific workflows onto distributed systems. Sci. Program., 2005. 13(3):
p. 219-237.
Oinn, T., et al., Taverna: a tool for the composition and enactment of bioinformatics workflows. Bioinformatics, 2004. 20(17): p. 3045-3054.
Yu, J., R. Buyya, and K. Ramamohanarao, Workflow Scheduling Algorithms for Grid Computing, in Metaheuristics for Scheduling in
Distributed Computing Environments. 2008. p. 173-214.
Wieczorek, M., A. Hoheisel, and R. Prodan, Taxonomies of the Multi-Criteria Grid Workflow Scheduling Problem, in Grid Middleware and
Services. 2008. p. 237-264.
Plankensteiner, K., R. Prodan, and T. Fahringer, A new fault tolerance heuristic for scientific workflows in highly distributed environments
based on resubmission impact, in Proceedings of the 5th IEEE International Conference on e-Science (e-Science '09). 2009: Oxford, UK. p.
313-320.
Abramson, D., C. Enticott, and I. Altintas, Nimrod/K: towards massively parallel dynamic grid workflows, in Proceedings of the 2008
ACM/IEEE conference on Supercomputing. 2008, IEEE Press: Austin, Texas.
Smanchat, S., et al., Scheduling Multiple Parameter Sweep Workflow Instances on the Grid, in Proceedings of the 5th IEEE International
Conference on e-Science (e-Science '09). 2009: Oxford, UK. p. 300-306.
Maheswaran, M., et al., Dynamic matching and scheduling of a class of independent tasks onto heterogeneous computing systems, in
Proceedings of the 8th Heterogeneous Computing Workshop (HCW '99). 1999. p. 30-44.
Casanova, H., et al., Heuristics for scheduling parameter sweep applications in grid environments, in Proceedings of the 9th Heterogeneous
Computing Workshop (HCW 2000) 2000. p. 349-363.
Shi, Z. and J.J. Dongarra, Scheduling workflow applications on processors with different capabilities. Future Gener. Comput. Syst., 2006.
22(6): p. 665-675.
Topcuoglu, H., S. Hariri, and W. Min-You, Performance-effective and low-complexity task scheduling for heterogeneous computing. IEEE
Transactions on Parallel and Distributed Systems, 2002. 13(3): p. 260-274.
Smanchat, S., S. Ling, and M. Indrawan, Toward grid workflow scheduling based on resource competition, in Proceedings of the 13th
Enterprise Distributed Object Computing Conference Workshops (EDOCW 2009). 2009. p. 126-130.
Pinedo, M.L., Scheduling: Theory, Algorithms, and Systems. Third ed. 2008: Springer.
Aalst, W.M.P.v.d., The Application of Petri Nets to Workflow Management. The Journal of Circuits, Systems and Computers, 1998. 8(1): p.
21-66.
Network Weather Service. [cited August, 2010; Available from: http://nws.cs.ucsb.edu.
Etminani, K. and M. Naghibzadeh, A Min-Min Max-Min selective algorihtm for grid task scheduling, in Proceedings of the 3rd IEEE/IFIP
International Conference in Central Asia on Internet (ICI 2007). 2007. p. 1-7.

