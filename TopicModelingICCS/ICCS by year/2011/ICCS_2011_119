Available online at www.sciencedirect.com

Procedia Computer Science 4 (2011) 126–135

Procedia Computer
Science

Procedia Computer Science 00 (2011) 1–10

International Conference on Computational Science, ICCS 2011

Eﬃcient Solution of Evolution Models for Virus Populations
Gerhard Niederbruckera , Wilfried N. Gansterera,∗
a University

of Vienna, Research Lab Computational Technologies and Applications (Austria)

Abstract
The computation of the quasispecies in Eigen’s quasispecies model requires the solution of a very large scale
eigenvalue problem. Since the problem dimension is of an exponential growing nature the well known methods for
dealing with such a problem run out of resources already far away from practically relevant cases. We propose the
use of an implicit matrix vector product using the special problem structure as building block for eigenvalue solvers
to partially overcome the exponential growth, which let us reach unexpected large problem sizes. As we will show
our implicit matrix vector product is a prime example for an algorithm perfectly matching the requirements of GPU
computing since it has low space and high parallel computation requirements. Therefore we will also present an GPU
implementation delivering a speedup factor of about 100 compared to a standard implementation.
Keywords: evolution models for virus populations, quasispecies, eigenvector computation, structured eigenvalue
problem

1. Introduction
We present a GPU-based approach for numerically computing the largest eigenvector of a large specially structured eigenproblem arising in modelling the evolution of virus populations. By exploiting the specific problem properties, our new approach is much more eﬃcient than standard methods and is an important step towards handling the
exponential growth of the underlying eigenproblem.
Under some assumptions on the environment the evolution of RNA molecules can be modeled by a system of
ODEs [1]. In this model, RNA molecules are represented as strings over the binary alphabet with a fixed length ν which
we call the chain length. The other important model parameter is the error rate p which represents the possibility of
single point mutations in an RNA sequence. Given a fixed ν we will always have to consider all 2ν possible sequences
since any RNA molecule can mutate into any other even if the possibility might be very low. Our overall goal is to
numerically compute the so called quasispecies [2] which describes an ordered stationary distribution in the model.
In practice, cases of interest start with chain lengths of 40, and for realistic viruses, chain lengths are 100 and above.
Thus, an important objective of our work is to raise the bar of problem instances which can be solved in reasonable
time as high as possible. For some kinds of problems with special structure eﬃcient methods are known which reduce
∗

Corresponding author
Email addresses: gerhard.niederbrucker@univie.ac.at (Gerhard Niederbrucker), wilfried.gansterer@univie.ac.at (Wilfried
N. Gansterer)

1877–0509 © 2011 Published by Elsevier Ltd. Open access under CC BY-NC-ND license.
Selection and/or peer-review under responsibility of Prof. Mitsuhisa Sato and Prof. Satoshi Matsuoka
doi:10.1016/j.procs.2011.04.014

Gerhard Niederbrucker and Wilfried N. Gansterer / Procedia Computer Science 4 (2011) 126–135
G. Niederbrucker and W. N. Gansterer / Procedia Computer Science 00 (2011) 1–10

127
2

the computational complexity by an order of magnitude [3, 4]. Our methods are designed to deal with a more general
and therefore more realistic setting were no eﬃcient computations were performed yet [5].
ν
ν
The ODE system leads to a non-negative matrix W ∈ R2 ×2 which basically describes the constitution and the
ν
possibilities for mutations of the 2 RNA molecules with chain length ν. Our problem is to compute the eigenvector
corresponding to the largest eigenvalue of W up to a defined degree of precision. This vector contains the information
about the concentration of the quasispecies. The hardness of this problem comes clearly from the exponential growth
of the dimension of W with the chain length ν. So we need significant improvements in our methods to see an
improvement in the maximal tractable chain length ν. In this paper, we consider applying the power iteration as a
numerical method for computing the eigenvector we are looking for.
Given this eigenvalue problem and the knowledge that the matrix dimension grows exponentially with the problem
size we develop algorithms to partially tackle this exponential growth. The first idea how to deal with such huge matrices is to use thresholding together with sparse computations. From sparse algorithms we expect that their complexity
depends only on the number of non-zero matrix elements and not on the dimension of the matrix. Together with a
low percentage of non-zero elements this strategy leads to a first reduction of time and space consumption. As an
improvement to this straightforward approach we will show a method for creating the sparse matrix implicitly without
touching the whole matrix which will give us already a significant improvement. Due to the exponential growth of
the dimension even the sparse computations lead to serious problems in the space consumption. To solve this issue
we will present an implicit matrix vector product for our matrix W where we do not need to store the elements of
W in the memory. Since we do not lose the basic shape of the usual matrix vector product we are able to massively
parallelize our computations. Together with the low memory consumption this will lead us to OpenCL and especially
GPU computing where programs should fulfill exactly those requirements to get a great benefit from the usage of
GPU devices.
The rest of the paper is structured as follows: In Section 2 we precisely define the eigenvalue problem we are
interested in and show its properties and applications. In Section 3 we will show basic strategies to tackle the problem
and afterwards we present our special purpose implicit matrix vector product for the power iteration. Section 4 is
devoted to concrete implementations and experimental results showing how our matrix vector product benefits from
the usage of GPUs as computing devices. Throughout this paper we will use zero-based indexing when dealing with
vectors and matrices to get a consistent and more readable notation.
2. Problem Statement
2.1. Biochemical Background
We briefly introduce the model which leads to the eigenvalue problem we are dealing with. For a more detailed
introduction and further references see [5, 6]. As already mentioned, we use a binary alphabet for our RNA molecules.
When talking about a molecule Xi with 0 ≤ i < 2ν we mean the sequence Xi := (b0 , . . . , bν−1 ) where the vector
(b0 , . . . , bν−1 ) is the binary encoding of i with length ν. As a measure for the diﬀerence between a species Xi and a
species X j we will use the well known Hamming distance dH (Xi , X j ) which delivers the minimal number of changes
(mutations) one has to apply to get X j out of Xi .
Due to Eigen [1] the following system of ODEs models the evolution of RNA molecules:
dxi
dt

=

Φ(t) =

N

j=0

N

j=0

f j · Qi, j · x j (t) − xi (t) · Φ(t),
f j · x j (t),

N


i = 0, . . . , N = 2ν − 1

x j (t) = 1

j=0

where xi is the relative concentration of the molecular species Xi . The fitness values f j describe the constitution of the
molecular species X j in terms of a single number and the (i, j) entry of the mutation matrix Q describes the possibility
for a mutation from sequence Xi to sequence X j and is defined as
Qi, j = pdH (Xi ,X j ) · (1 − p)ν−dH (Xi ,X j )

(1)

128 	

Gerhard
Niederbrucker
and
N. /Gansterer
/ Procedia Science
Computer
4 (2011) 126–135
G. Niederbrucker
and W.
N.Wilfried
Gansterer
Procedia Computer
00 Science
(2011) 1–10

3

By definition the values Qi, j depend only on dH (Xi , X j ) and therefore the whole matrix Q consists of only ν+1 diﬀerent
values. This also illustrates that p has to be understood as an average error rate over all possible mutations since there
is no dependence on the positions and the overall number of the mutations. Equation 1 also shows that an index i (row
or column) corresponds to the sequence Xi . It would also be possible to apply some permutation π such that an index
i corresponds to sequence Xπ(i) 1 . When using a permutation one has to apply the permutation to Q as well as to F to
get the same results.
Since the xi represent a relative concentration we are only interested in solutions where all entries of the computed
eigenvector are non-negative. Other solutions where one or more entries are negative are not interesting for us since
there is no physically meaningful interpretation of a negative concentration.
Since our ODE system is a Bernoulli system we get by the proper change of variables a linear system with
constant coeﬃcients z˙ = W · z with W = Q · F where Q is defined by Equation (1) and F is the diagonal matrix with
the fitness values f j on the diagonal. The search of the quasispecies now reduces to the computation of the eigenvector
corresponding to the dominating eigenvalue of W [7]. Moreover, the Perron-Frobenius theorem [8] guarantees that
the eigenvector we are searching for only has non-negative entries since W clearly satisfies the conditions for applying
the theorem.
As mentioned before, the entries in xi represent the relative concentration of the species Xi in the stationary
distribution. Out of the computed eigenvector containing the relative concentrations one can compute the cummulative
concentrations of so called error classes. An error class Γk,i is defined via some fixed sequence i and a fixed Hamming
distance k
(2)
Γk,i := { j | 0 ≤ j < 2ν ∧ dH (Xi , X j ) = k}

In our model we call X0 the master sequence because at time t = 0 this is the only species which exists. Since we
are especially interested in the error classes with respect to the master
sequence we call them Γk := Γk,0 . By our
ν
binary interpretation we can easily determine that Γk consists of k elements. A change of variables from treating
single molecules to variables for the classes Γk is also the idea behind the computations for large ν in [3, 4] since this
transforms the 2ν × 2ν problem to a ν × ν problem. But as already mentioned this approach is only valid for very
special structured fitness landscapes.

By computing the cumulative concentrations |Γk | = j∈Γk x j of the error classes Γk in the stationary distribution for
diﬀerent p one can create graphs as the ones shown in Figure 1 where we vary values of p on the x−axis and show the
relative concentrations |Γk | on the y−axis. This application of our computations is used to get a better understanding
on how the chosen fitness landscape influences the evolution. Such computations are even more interesting on the
granularity of single molecules but up to now they are very rare in the literature due to the limited chain lengths people
are able to deal with. Depending on the concrete fitness values the error threshold phenomenon may occur or not.
When this phenomenon occurs there is an ordered stationary distribution up to a critical value pmax for p. For error
rates above pmax the structure of the population changes immediately into the uniform distribution which represents an
entire random replication. Typical values for pmax on certain fitness landscapes are around 0.01 − 0.1 [5, 7] depending
on the concrete fitness values and the chain length. Those small values for pmax are quite surprising since we get the
random replication as exact solution of the ODE system only for p = 0.5 [7].
This sudden change from an ordered distribution to random replication may also serve as building block for new
antiviral strategies [9] because the error rates of RNA viruses are usually close to this critical value [10] and an increase
of the error rates is possible by the use of pharmaceutical drugs.
In the style of the definition of the error classes we will us QΓk for the ν + 1 diﬀerent values of Q, which means that
QΓk = pk · (1 − p)ν−k for 0 ≤ k ≤ ν. In this paper (in contrast to [3, 4]) the only assumption on F is that it is a diagonal
matrix. Later on we will use randomly generated landscapes when presenting our results to highlight this assertion.
We should also point out the case where all values in F are equal. In this case the problem reduces to the computation
of the dominating eigenvector of a bistochastic matrix2 which is trivial and leads to an eigenvector where all entries
are equal. From the practical point of view this is not at all surprising since for equally fit sequences we clearly expect
the uniform distribution as result.
1 For

example using the Gray code as permutation would deliver a matrix Q where the first diagonal above and below the main diagonal are
constant. This comes from the basic definition of the Gray code which states that dH (Xi , Xi+1 ) = 1for
 all i.
ν
2 Q is symmetric and we have by the binomial theorem for all 0 ≤ i ≤ N that N Q = ν
k
ν−k = (p + 1 − p)ν = 1
j=0 i, j
k=0 k · p · (1 − p)

129
4

1

Γ0, Γ20
Γ1, Γ19
Γ ,Γ
2 18
Γ ,Γ
3 17
Γ4, Γ16
Γ ,Γ

0.8

0.6

5

Γ6, Γ14
Γ7, Γ13
Γ ,Γ
8 12
Γ ,Γ
9 11
Γ10

15

0.4

0.2

0
0

0.02

0.04

Error rate p

0.06

0.08

Relative concentration in error class Γk

Relative concentration in error class Γk

Gerhard
N. Gansterer
/ Procedia
Computer
Science
4 (2011)
G.Niederbrucker
Niederbruckerand
andWilfried
W. N. Gansterer
/ Procedia
Computer
Science
00 (2011)
1–10126–135
1

Γ0, Γ20
Γ1, Γ19
Γ ,Γ
2 18
Γ ,Γ
3 17
Γ4, Γ16
Γ ,Γ

0.8

0.6

5

Γ6, Γ14
Γ7, Γ13
Γ ,Γ
8 12
Γ ,Γ
9 11
Γ10

15

0.4

0.2

0
0

0.02

0.04

Error rate p

0.06

0.08

Figure 1: On the left we see an example for the visualization of the error threshold phenomenon. We see an ordered stationary distribution up to
pmax ≈ 0.035 and the sudden change to the uniform distribution for p > pmax . Error classes with the same number of elements (Γk , Γν−k ) have the
same color and therefore there curves meet when the uniform distribution is reached above the threshold. We used ν = 20, F0,0 = 2 and Fi,i = 1 for
all 1 ≤ i ≤ N. This simple fitness landscape is known as the single peak fitness landscape in the literature. On the right we used again ν = 20 and
the so called linear landscape defined as Fi,i = f0 − ( f0 − fν ) · dH (i, 0)/ν for all 0 ≤ i ≤ N with f0 = 2 and fν = 1. For this landscape we observe a
smooth transition into the uniform distribution and no error threshold.

2.2. Solution Strategies
Before we consider concrete methods for finding the desired eigenvector we study the structure of the matrices
occurring in the model. Remember that Q, F and W have dimension 2ν × 2ν since this is the number of diﬀerent RNA
molecules with length ν over a binary alphabet. The exponential growth of the problem size N with the chain length ν
is the central challenge in this context. Therefore we are looking for approaches which deliver speedups by orders of
magnitude because, as already stated, our overall goal is to raise the number of tractable chain lengths. The central
idea is to exploit the special structure of the matrices Q and F.
As already mentioned the whole matrix Q consists of just ν + 1 diﬀerent values which is a first hint that we
may find an improved matrix product which explicitly uses this property resulting in a drastic reduction of the space
consumption. Based on this observation, we will consider the well known power iteration for the task of finding the
eigenvector corresponding to the dominating eigenvalue of W, because we can use our understanding of the problem
structure best by using the power iteration. Moreover we expect a more accurate result compared to other existing
methods for approximating dominating eigenvectors [11, 12]. We can also expect by the special structure of Q and
F that we will be able to find highly eﬃcient algorithms for realizing exactly the required matrix vector product. As
a measure for the accuracy of the approximated
dominating
eigenvector x˜ and as termination condition for the power


˜ x˜) = W x˜ − λ˜ x˜ where λ˜ is the approximation of the corresponding eigenvalue
iteration we will use the residual R(λ,
2
calculated via the Rayleigh quotient.
In the following, we first study the behavior of some basic strategies like sparsification based on thresholding and
afterwards we present an eﬃcient method for implicitly computing matrix vector products with the matrix W.
3. Algorithmic Strategies
With increasing chain length ν we quickly run out of memory due to the exponentially growing dimension of W.
In the following we develop several strategies for coping with this problem.
3.1. Sparsification
A first approach is based on using thresholding which sparsifies the problem. Since Q is defined via powers of p
and the interesting values of p are usually not significantly larger than 10−2 [3] we immediately see that there are
many elements below machine precision which we can eliminate already for small ν.
As already mentioned, we may even eliminate further elements, since less than full precision in the desired eigenvector is usually suﬃcient. When thinking of a bound for the elements we want to consider we should have in mind
the uniform distribution as possible solution where we have xi = 2−ν for all i. Therefore we should not take a limit
above this value and try to take a significant smaller value as long as we do not run out of time. Actually, we do not

130 	

Gerhard Niederbrucker and Wilfried N. Gansterer / Procedia Computer Science 4 (2011) 126–135
G. Niederbrucker and W. N. Gansterer / Procedia Computer Science 00 (2011) 1–10
0

0

−4

Species Xj

40

−6

60

−8

80

−10
−12

100

−14

120
20

40

60

80

Species Xi

100

120

−16

10

Percentage of non−zero elements

−2

20

5

−1

10

−2

10

max

dH =9
−3

10

max

dH =7
max

dH =5
−4

10

10

max
=3
H

d

12

14

16

18

Chain length ν

20

22

24

Figure 2: On the left, we see a graphical representation of the matrix Q for ν = 7 and p = 0.001 where the elements are colored according to their
order of magnitude. The color scheme ranges from 100 to 10−16 . On the right we show the percentage of non-zero elements (x-axis) we have to
max has a stronger influence on the
consider for diﬀerent chain lengths (y-axis) and Hamming distance limits. This graph shows that a decrease of dH
number of non-zero elements as an increase of the chain length. This is an important property for us which will help to treat large chain lengths.
The experimental results in Table 1 also show this property in terms of concrete run times.

use a concrete numerical bound to decide which elements we use and which we remove from W. Instead, we use
a bound dHmax for the maximal Hamming distance between elements to be considered. This delivers also a natural
˜ x˜) ≈ QΓ max . Theory tells us just the accuracy
stopping criterion for the power iteration, namely when we reach R(λ,
d
+1
H
˜ x˜). How the numerical error translates to the eigenvector is
of the approximated eigenvalue which is related to R(λ,
discussed in Section 3.4.
This restriction can be easily applied to the matrix Q since the values of Q are defined via the Hamming distance
between the sequences corresponding to the row and column index. By multiplying this sparsified version of Q with
F we get again a sparse version of W because F is diagonal.
In the model, this approach corresponds to restricting the possible mutants of an RNA molecule to near relatives.
A positive side eﬀect of this approach is that the Perron-Frobenius theorem stays valid since it holds for primitive
matrices [8]. The sparsified W is indeed primitive even for dHmax = 1 because in this minimal case the non-zero values
of Q correspond to the non-zero values of the adjacency matrix of the ν-dimensional hypercube. Therefore, there
exists a mutation path between any two sequences Xi and X j which guarantees the condition for primitivity, namely
that there exists a value of m such that W m contains only positive elements. The border case dHmax = 0 is useless since
in this case no mutations are possible and therefore we have to compute the dominating eigenvector of a diagonal
matrix which is trivial and gives us the expected result that only the master sequence (more precisely, the sequence
with the dominating fitness value) appears in the stationary distribution.
The fact that the entries of F are not considered in the thresholding is not a problem since they usually do not
vary by the same magnitude as it is the case for the entries of Q. Thus, our simple strategy of restricting the maximal
Hamming distance considered will deliver almost the same results as specifying some fixed numerical lower bound
on the entries of W.
Obviously, the thresholding technique implies a certain loss of accuracy in the eigenvector computed. We know
that the we can expect an accuracy of the order of the largest deleted entry of W [13]. This exactly explains why we
˜ x˜) ≈ QΓ max as stopping criterion. As already told this delivers just the accuracy of the eigenvalue but as
choose R(λ,
d
+1
H
we discuss in Section 3.4 we also expect a similar behavior for the entries in the eigenvector.
3.2. A Memory-Eﬃcient Hierarchical Algorithm for Generating W
In the sparsified version of W the percentage of non-zero values after restricting the maximal Hamming distance
easily goes down to 10−2 and below (especially for small p) as illustrated in Figure 2. Thus, in order to achieve
reductions in execution time it is extremely important not to explicitly compute every single entry of W before deciding
whether it can be eliminated or not.
We will split up generating W in two steps. First, we use a smart method for generating Q and afterwards we
perform the multiplication with the values of F as needed (since F is diagonal, this can be done separately for each
element). For generating Q, we use an algorithm which visits the elements of Q according to their Hamming distance.

Gerhard Niederbrucker and Wilfried N. Gansterer / Procedia Computer Science 4 (2011) 126–135
G. Niederbrucker and W. N. Gansterer / Procedia Computer Science 00 (2011) 1–10

131
6

Note that for 0 < p ≤ 0.5 Equation (1) shows that the elements of Q decrease with increasing Hamming distance.
Consequently, the algorithm only computes the elements which are needed.
First remember that the dimension of the matrix Q is a power of two which implies that we can evenly divide Q
into four blocks


ν
ν
ν−1
ν−1
Q00 Q01
Q00 , Q01 , Q10 , Q11 ∈ R2 ×2 .
,
Q ∈ R2 ×2 ,
Q=
Q10 Q11

The dimension of each of these blocks is again a power of two and therefore we can split them up again and again
until we reach 1 × 1 blocks. The second important observation for our eﬃcient algorithm is that the matrix Q has a
blockwise symmetry for each power of two as block size. This is an immediate consequence of our ordering of the
columns and of the interpretation of indices as binary sequences. Due to this blockwise symmetry we have Q11 = Q00
and Q10 = Q01 . Putting together these observations leads to the recursive Algorithm 1 which visits the elements of Q
during the generation of Q ordered by their size and therefore only touches the elements we really want to consider,
thus leading to a very eﬃcient method for generating the matrices Q
respectively.
By using Algorithm 1 we

 andW,
 
max  
dH
ν
ν
2·ν
reduced the runtime complexity for setting up W from O 2
to O 2 · k=0 k . The right plot in Figure 2 exactly
shows (the inverse of) the speedup using Algorithm 1 compared to the trivial approach. This reduction also implies
that now the creation and the the matrix vector product have the same runtime complexity.
Algorithm 1 Sparse creation of Q
1: function CreateQ(Q, dH )
2:
if Q ∈ R1×1 then
3:
Q = pdH · (1 − p)ν−dH
4:
else
5:
CreateQ(Q00 , dH )
6:
if dH < dHmax then
7:
CreateQ(Q10 , dH + 1)
8:
end if
9:
Q11 ← Q00
10:
Q01 ← Q10
11:
end if
12: end function

3.3. An Eﬃcient Implicit Matrix Vector Product
The principal idea to gain further improvements is to give up saving the matrix W explicitly in memory. Instead,
we design an implicit matrix vector product where the elements of W are computed while building the product. We
will then discuss the successful implementation of our algorithm with OpenCL on GPUs.
Remember that the size of the elements in Q decreases with the Hamming distance corresponding to the sequence
for the row and the column, respectively. Also remember that in Section 3.1 we did not specify a specific numerical
bound for the elements but instead we used a bound dHmax for the maximal Hamming distance between elements we
want to take into account. The only adaption we have to make in the standard matrix vector product is the order how
the elements in a row are traversed during the multiplication. Usually we run through a row from left to right. Instead,
we will run through the elements sorted by their Hamming distance with respect to the row index. In the notation
of Section 2 this means that we run through the diﬀerent error classes (see Equation (2)) of the row index. More
specifically, for computing y = W · x we get


N
ν 
ν
 





Qi, j · F j, j · x j =
Qi, j · F j, j · x j =
QΓk · 
F j, j · x j  .
yi =
j=0

k=0 j∈Γk,i

k=0

j∈Γk,i

We get now again a ”sparse” matrix vector product by replacing ν as upper bound for the sum by some limit dHmax
for the Hamming distance we want to consider. For sure the sorting according dH has to happen implicitly during the
multiplication since otherwise we would get a significant increase of the time complexity.

132 	

Gerhard Niederbrucker and Wilfried N. Gansterer / Procedia Computer Science 4 (2011) 126–135
G. Niederbrucker and W. N. Gansterer / Procedia Computer Science 00 (2011) 1–10

7

So the first problem which arises is to compute the sequence of indices sorted according dH to a fixed element Xi .
For simplicity we will first use the element X0 = (0, . . . , 0). Based on this element computing the Hamming distance
just means counting bits. We also know in advance the number of elements with a certain Hamming distance so
we can use a bucket sort like procedure to create the sorted sequence we are interested in in linear time without any
additional memory needed.
Up to now we just have a properly sorted sequence for one base element (0, . . . , 0) but we need such a sequence for
every possible base element. Computing these sequences separately is not possible when we want to get an eﬃcient
method. Instead of that we will use the single sequence with base (0, . . . , 0) and transform this sequence into the one
we actually need during the multiplication. For this purpose let us remember the truth table of the binary XOR ⊕:
⊕
0
1

0
0
1

1
1
0

An informal interpretation of A ⊕ B would be that the bit A gets flipped if and only if the bit B is true. As we know
the extension of ⊕ : {0, 1} → {0, 1} for bit vectors to a bitwise XOR ⊕ : {0, 1}ν → {0, 1}ν is one of the commonly
available operations for basic data types like integers on todays digital computers. By our informal interpretation of
⊕ we can conclude that for two binary sequences Xi and X j it holds that
dH (Xi , Xi ⊕ X j ) = dH (X j , X0 )

(3)

And now we have all what we need for an eﬃcient (implicit) matrix vector product for W. We can compute the sorted
sequence with base element (0, . . . , 0) eﬃciently once as a preprocessing step and the ⊕ operator lets us transform this
sequence to the sorted sequence for an arbitrary base element during the multiplication. Using Equation (3) we get




ν
ν
 
 








yi =
(4)
QΓk · 
F j, j · x j  =
QΓk · 
Fi⊕ j,i⊕ j · xi⊕ j 
k=0

j∈Γk,i

k=0

j∈Γk

as new formula for computing the elements of the result vector y. Putting everything together leads to Algorithm 2 as
an eﬃcient procedure for multiplying any vector with W using a specified accuracy dHmax . Obviously Algorithm 2 is
Algorithm 2 Sparse matrix vector multiplication for W · v = Q · F · v
1:
2:
3:
4:
5:
6:
7:
8:
9:
10:
11:
12:

for all i ∈ {0, . . . , N} do
vout [i] ← 0
j←0
for k ← 0 to dHmax do
sum ← 0  
for l ← 0 to νk − 1 do
sum ← sum + F[i ⊕ S [ j]] · vin [i ⊕ S [ j]]
j← j+1
end for
vout [i] ← vout [i] + QΓ [k] · sum
end for
end for

 vin is the input vector, vout the output vector

 S contains sorted sequences according dH to (0, . . . , 0)

just an algorithmic encoding of Equation (4). The occurring fixed values S [k],
once before starting the power iteration.

ν 
k

and QΓ [k] should be precomputed

3.4. Relative Error in the Computed Concentrations
In this section we want to briefly address the question how the thresholding translates into an error in the computed
˜ x˜) we mean the exact and approximated dominating eigenpair of W. First one can easily
eigenvector. By (λ, x) and (λ,

133
8

Gerhard Niederbrucker and Wilfried N. Gansterer / Procedia Computer Science 4 (2011) 126–135
G. Niederbrucker and W. N. Gansterer / Procedia Computer Science 00 (2011) 1–10
max

verify that the largest element we delete from W when using the threshold dHmax is bounded by  := pdH +1 · Fmax where
Fmax = maxi Fi,i since the largest deleted element is QΓdmax +1 · Fmax . By estimating the error in our single concentrations
H
we run into the common problem that there are quite a lot of bounds for eigenvalue approximations but there are only
a few results on the estimation of errors in eigenvectors. What we observed in practice for small examples was that
˜ x˜) which is a very tight and good bound which we can not yet confirm by some formal arguments.
x − x˜2 ≈ R(λ,
Instead of that we want to present some formal bounds for the eigenvalue and the condition number which are not at
all tight but they show how the parameter ν,dHmax , p and the landscape F influence the error. And this precise formal
connections correspond to observations we made for small examples.
˜ ≤ τ = 2ν ·. To see more clearly how the parameter
Following [13] we get as upper bound for the eigenvalue |λ− λ|
max
ν+log2 (p)·(dH
+1)
· Fmax . Now we see that the influence of p and dHmax is much bigger
eﬀect the error we may write τ = 2
than the influence of ν and F since a change of p is multiplied by dHmax + 1 and vice versa.
To get a feeling for the concrete value of τ we use the example ν = 20, p = 0.01, Fmax = 10, dHmax = 5 from
show
that the row
Section 4 and observe a value of τ ≈ 10−5 which should be a satisfying bound. Since one can easily


−1
.
and column sums of Q−1 are also one we can estimate the condition by cond1 (W) = W1 · W −1 1 ≤ Fmax · Fmin
−1
By looking at the common landscapes in the literature we can expect that neither Fmax nor Fmin will be big. So in
principal we can expect that our problem is not at all ill conditioned.
4. Eﬃcient GPU Implementation
4.1. An OpenCL Implementation
Up to now using Algorithm 2 provokes a great reduction of the space complexity, but also a slight increase of the
time complexity due to the transformations we apply during the multiplication. Observe that the iterations of the outer
i-loop are entirely independent, like in the case of the standard matrix vector product. Thus, the only natural limit of
the number of concurrent processes is the matrix dimension. Algorithm 2 perfectly fits to the GPU philosophy having
low space and high (parallel) computation requirements since we just need to store the four vectors.
The pseudo code shown in Algorithm 2 just shows a single threaded implementation of our eﬃcient matrix vector
product. Nevertheless, the algorithm can be adapted straightforwardly to an OpenCL kernel function. This is done
by removing the outer loop and instead of that the current value of i is computed out of the id3 of the calling thread.
Since the computations of the single rows are independent we can invoke our kernel with one thread for each row. For
the number of local threads we also use the maximum number supported by the hardware the kernel is running on.
Like the algorithm itself also this implementation is just a basic one which should show the principal properties of
the problem and our solutions. Several other refinements can be applied to gain further improvements. Especially the
computations required between two successive iterations are in our implementation only parallelized in a rather simple
way. But our implementation still reaches a level of eﬃciency such that the matrix vector product is the dominating
part of the overall runtime, which suﬃces for the moment to illustrate the benefits of our approach.
4.2. Experimental Results
As mentioned earlier, we do not want to make assumptions on the concrete values of F, and therefore we use the
following definitions to create landscapes:
F0,0 = c,

Fi,i = σ · (ηrnd (i) + 0.5)

with c > 0, 0 < σ < c/2

(5)

where ηrnd (i) is as in [5] a call to some uniform random number generator on the interval [0, 1].
The reason for this definition is that we want to ensure that the fitness of the master sequence is somewhat separated
from the second largest fitness value. This is due to the fact that the separation of the dominating fitness values
correspond to the separation of the dominating eigenvalues of W which is the important factor for the convergence
speed of the power iteration. The reason for this is that F scales the well understandable spectrum of Q by the fitness
values. That the master sequence has a somewhat dominating fitness value is a common feature of the landscapes
discussed in the literature. Therefore, our restriction is not at all an artificial condition. For the power iteration we
always use (1, . . . , 1)T (the dominating eigenvector of Q) as start vector. In our experiments we compare three methods
for numerically computing the dominating eigenvector of W:
3 In

OpenCL threads are numbered from 0, 1, . . . , T − 1 when T is the number of threads which should be started.

134 	

Gerhard Niederbrucker and Wilfried N. Gansterer / Procedia Computer Science 4 (2011) 126–135
G. Niederbrucker and W. N. Gansterer / Procedia Computer Science 00 (2011) 1–10

9

1. An OpenCL implementation of Algorithm 2 running on a Nvidia Tesla C2050 using Nvidia Cuda
2. An OpenCL implementation of Algorithm 2 running on a Intel I5 750 @ 2.67Ghz core using Ati Stream
3. A standard single threaded CPU implementation of Algorithm 2 running on a Intel I5 750 @ 2.67Ghz.
We miss here a comparison with well known eigenvalue solvers for the simple reason that the matrices are to huge for
the reported chain lengths to put them into those methods. In Figure 3 we present the running times4 of the diﬀerent
4

4

10

3

2

10

1

10

0

10

−1

14

16
18
Chain Length ν

20

22

10

24

10

12

14

16
18
Chain Length ν

20

22

24

16
18
Chain Length ν

20

22

24

4

10

10

I5 C++
I5 OpenCL
Tesla C2050

3

2

10

1

10

0

10

−1

2

10

1

10

0

10

−1

10

10

−2

10

I5 C++
I5 OpenCL
Tesla C2050

3

10
Execution Time in seconds

10
Execution Time in seconds

0

10

−2

12

4

10

1

10

10

−2

10

2

10

−1

10

10

I5 C++
I5 OpenCL
Tesla C2050

3

10
Execution Time in seconds

10
Execution Time in seconds

10

I5 C++
I5 OpenCL
Tesla C2050

−2

12

14

16
18
Chain Length ν

20

22

24

10

10

12

14

Figure 3: The execution times for finding the dominating eigenvector up to the expected accuracy (≈ Q(Γdmax +1 ) ) are shown. On the x-axis we
H

increase the input chain length ν from ν = 10 to ν = 24 and on the (logarithmic) y-axis we show the runtime needed in seconds for the diﬀerent
max = 3 (left), d max = 5 (right). In the lower row we used c = 3, σ = 1 and again
implementations. In the upper row we used c = 10, σ = 1 and dH
H
max = 3 (left), d max = 5 (right). We can observe that the decrease in the domination of the fitness of the master sequence from c = 10 to c = 3
dH
H
max also causes an increase of the
results, as expected, in an increase of the runtime due to the need of more iterations. Obviously the increase of dH
runtime. The change of the used implementation does of course not change the principal problem complexity but nevertheless one should notice
the speedup factor of more than 100 by using the GPU instead of the standard implementation. So the important observation should be that our
algorithm perfectly scales with the underlying hardware.

implementations for two example landscapes and increasing chain lengths. Additionally we show in Table 1 some
example test cases together with their resulting space complexity (assuming that all floating point vectors and matrices
are of type double precision requiring 8 bytes per element) and time complexity. The space and time requirements in
Table 1 clearly show the importance and the benefit of our implicit matrix vector product which gives us the ability to
deal with unexpected large (compared with the expectations given in [5]) chain lengths up to5 ν ≈ 25, depending on
the values of p and dHmax . Obviously our algorithm also oﬀers the ability of a distributed implementation which would
deliver a further significant raise of the chain length on large scale hardware.
In Figure 3 we see that our algorithm also already delivers good results from the point of runtime and we see a
great benefit compared to standard methods.
5. Conclusion
In this paper we studied a very large scale eigenvalue problem occurring in a model for the evolution of RNA
sequences. We got a non-negative matrix W with dimension 2ν × 2ν as an input and our aim was to compute the
4 All
5 On

times have to be understood as overall execution times, so the GPU times also include the data transfer to and from the device.
a modern GPU device like the Nvidia Tesla C2050 we used here.

Gerhard Niederbrucker and Wilfried N. Gansterer / Procedia Computer Science 4 (2011) 126–135
G. Niederbrucker and W. N. Gansterer / Procedia Computer Science 00 (2011) 1–10

ν
19
20
21
22
23
24

p
0.01
0.01
0.01
0.01
0.01
0.01

dHmax
5
5
4
4
3
3

Full W
2 Tb
8 Tb
32 Tb
128 Tb
512 Tb
2048 Tb

Sparse W
65.1 Gb
169.5 Gb
117.9 Gb
284.7 Gb
128.0 Gb
290.6 Gb

Algorithm 2
14 Mb
28 Mb
56 Mb
112 Mb
224 Mb
448 Mb

Time GPU [s]
6.4
17.5
13.7
35.9
24.2
71.1

135
10

Time CPU [s]
260
746
506
1438
726
1844

Table 1: Model parameters and resulting space and time requirements for some concrete test cases. We have to specify the chain length ν, the error
max to fully define a test case. Additionally the space requirements for handling
rate p and the maximal Hamming distance between to elements dH
the full matrix W, for handling the sparse matrix W (as in Section 3) and for using Algorithm 2 are reported. For the sparse matrix we only report
the space required by the non-zero elements without the overhead of a specific sparse storage format. In the last two columns we show the runtimes
for computing the dominating eigenvector with our OpenCL implementations on a Nvidia Tesla C2050 and on a Intel I5 750 @ 2.67Ghz for the
landscape defined by c = 10, σ = 1. It is clearly visible that Algorithm 2 has the typical properties of a sparse matrix vector product since the
execution times are proportional to the size (number of non-zero elements) of the sparsified W. The stopping criterion in all reported cases was that
˜ x˜) reaches Qdmax +1 .
R(λ,
H

dominating eigenvector of W. Since the dimension of W grows exponentially with the chain length ν we had to find
methods which perform by magnitudes better than the standard methods such that we are able see improvements also
in terms of (significantly) larger tractable chain lengths. Using the fact that often the results are not needed in full
precision, we started with some basic thresholding and sparsification techniques together with the power iteration
as numerical method. Our main result was the development of an implicit matrix vector product which exploits the
special problem structure and thus reduced the memory requirements drastically. Nevertheless, we did not lose the
parallelization potential as it is available in the standard matrix vector product. We also showed the great benefit one
receives when mapping our special purpose matrix vector product onto OpenCL and GPUs as computing devices.
As already stated, the presented methods have to be understood as a first approach to solving the presented problem eﬃciently. There is a wide range of possible future improvements. We will consider ideas like an adaptive
power iteration, smart choices for the start vector depending on the input parameters, computing the right out of the
(somewhat easier computable) left eigenvector, methods for improving the convergence speed, etc.
Acknowledgments. This work was partly supported by the Austrian Science Fund (FWF) under contract S10608
(NFN SISE). We would like to thank Peter Schuster for many fruitful discussions.
References
[1]
[2]
[3]
[4]
[5]
[6]
[7]
[8]
[9]
[10]
[11]
[12]
[13]

M. Eigen, Selforganization of matter and the evolution of biological macromolecules, Naturwissenschaften 58 (1971) 465–523.
M. Eigen, P. Schuster, A principle of natural self-organization, Naturwissenschaften 64 (1977) 541–565.
J. Swetina, P. Schuster, Self-replication with errors : A model for polvnucleotide replication, Biophysical Chemistry 16 (4) (1982) 329 – 345.
M. Nowak, P. Schuster, Error thresholds of replication in finite populations mutation frequencies and the onset of muller’s ratchet, Journal of
Theoretical Biology 137 (4) (1989) 375 – 395.
P. Schuster, Mathematical modeling of evolution. solved and open problems, Theory in Biosciences 130 (2011) 71–89.
P. Schuster, Prediction of RNA secondary structures: from theory to models and real molecules, Reports on Progress in Physics 69 (2006)
1419–1477.
P. Schuster, The mathematics of Darwinian systems, 2008, appendix for the book: Manfred Eigen, ’From Strange Simplicity to Complex
Familiarity, Vol.I’.
E. Seneta, Non-negative Matrices and Markov Chains (Springer Series in Statistics), Springer, 2006.
M. Eigen, Error catastrophe and antiviral strategy, Proceedings of the National Academy of Sciences of the United States of America 99 (21)
(2002) 13374–13376.
J. Drake, Rates of spontaneous mutation among rna viruses., Proc Natl Acad Sci U S A 90 (9) (1993) 4171–5.
E. Drinea, P. Drineas, P. Huggins, A randomized singular value decomposition algorithm for image processing applications, in: Image
Processing, Panhellenic Conference on Informatics, PCI 2001, 2001, p. 279.
M. Mascagni, A. Karaivanova, A parallel quasi-monte carlo method for computing extremal eigenvalues, in: Lecture Notes in Statistics,
Springer, 2000.
Y. Bai, W. N. Gansterer, R. C. Ward, Block tridiagonalization of ”eﬀectively” sparse symmetric matrices, ACM Trans. Math. Softw. 30 (2004)
326–352.

