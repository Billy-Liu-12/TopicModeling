Available online at www.sciencedirect.com

Procedia Computer Science 4 (2011) 860–868

International Conference on Computational Science, ICCS 2011

Lattice gauge theory on a multi-core processor, Cell/B.E.
Shinji Motokia , Atsushi Nakamurab
b Research

a BioSphere Science, Hiroshima University
Institute for Information Science and Education (RIISE), Hiroshima University

Abstract
We report our implementation experience of a lattice gauge theory code on the Cell Broadband Engine, which
is a new heterogeneous multi-core processor. As a typical operation, we take a SU(3) matrix multiplication which
is one of the most important parts of lattice gauge theories. Employing full advantage of the Cell/B.E. including
SIMD operations and many registers, which enable the full use of the arithmetic units through the loop-unrolling, we
obtain about 200 GFLOPS with 16 SPE, which corresponds around 80 % of the theoretical peak. To our knowledge,
this is the fastest value of this operation obtained on the Cell/B.E. so far. However, when we measure the whole time
including the data supply, the speed drops down to about 13 GFLOPS.We found that the bandwidth of the data transfer
between the main memory and EIB, 25 GB/s, is a bottleneck. In other words, it is possible to run the arithmetic units
on the Cell/B.E. with 200 GFLOPS speed, but the current socket structure of Cell/B.E. prevents it. We discuss several
techniques to improve the problem partially by reducing the transferred data.
Keywords: Cell/B.E., Multi-core, Lattice gauge theory, lattice QCD

1. Introduction
Multi-core processors will be a standard computational engine in the high performance computing ﬁeld in near
future, because of their low price and energy-eﬃciency. Yet, there have been a few investigations how to extract their
computational power [1], [2], [3], [4], [5], [6], [7], [8], [9], [10] and [11].
In this report, we take a typical operation of lattice gauge theories and study how we can write its high performance
code on a multi-core processor, Cell Broadband Engine (Cell/B.E.). The lattice gauge theory was formulated by
Wilson[12], and has become very powerful tool to study the non-perturbative aspects of the theory together with
numerical simulations. Lattice QCD (Quantum Chromodynamics) is a version of the lattice gauge theories, and is a
basic framework for the hadron physics and nuclear physics. In these thirty years, the computational power has been
increasing very rapidly, and together with this progress, the lattice QCD has become a more reliable tool. In the era
of multi-core processors, the lattice QCD is expected to have strong predictive power especially for hadron physics at
ﬁnite temperature and density, which includes the astrophysics and high energy heavy ion experiments.
We consider the complex 3 × 3 matrix (SU(3) matrix) multiplication,
C = A×B
⎛
⎜⎜⎜ c11
⎜⎜⎜
⎜⎜⎜ c21
⎜⎝
c31

c12
c22

c13
c23

c32

c33

⎞
⎟⎟⎟
⎟⎟⎟
⎟⎟⎟
⎟⎠

=

⎛
⎜⎜⎜ a11
⎜⎜⎜
⎜⎜⎜ a21
⎜⎝
a31

a12
a22

a13
a23

a32

a33

(1)
⎞ ⎛
⎟⎟⎟ ⎜⎜⎜ b11
⎟⎟⎟ ⎜⎜⎜
⎟⎟⎟ × ⎜⎜⎜ b21
⎠⎟ ⎜⎝
b31

b12
b22

b13
b23

b32

b33

1877–0509 © 2011 Published by Elsevier Ltd. Open access under CC BY-NC-ND license.
Selection and/or peer-review under responsibility of Prof. Mitsuhisa Sato and Prof. Satoshi Matsuoka
doi:10.1016/j.procs.2011.04.091

⎞
⎟⎟⎟
⎟⎟⎟
⎟⎟⎟ .
⎠⎟

Shinji Motoki et al. / Procedia Computer Science 4 (2011) 860–868

861

This is most fundamental operation in the lattice QCD. Most technique which we will investigate in this paper can be
applied also to the following operation,
B= A×X
(2)
⎛
⎜⎜⎜ b11
⎜⎜⎜
⎜⎜⎜ b21
⎜⎝
b31

b12
b22
b32

⎞ ⎛
b13 ⎟⎟ ⎜⎜ a11
⎟⎟⎟ ⎜⎜⎜
b23 ⎟⎟⎟⎟ = ⎜⎜⎜⎜ a21
⎟⎠ ⎜⎝
b33
a31

a12
a22
a32

⎞⎛
⎞
a13 ⎟⎟ ⎜⎜ x1 ⎟⎟
⎟⎟⎟ ⎜⎜⎜
⎟⎟⎟
a23 ⎟⎟⎟⎟ ⎜⎜⎜⎜ x2 ⎟⎟⎟⎟ .
⎟⎠ ⎜⎝
⎟⎠
a33
x3

(3)

This 3 × 3 complex matrix times vector calculation appears in the large sparse matrix solver, that is the most time
consuming part in the lattice QCD.
We consider a case in which the above matrix×matrix operations are executed 573, 440 = 112 × 5120 times in the
single precision. This corresponds to a lattice size of N x × Ny × Nz × Nt = 16 × 16 × 16 × 32.
The Cell/B.E. is a novel multi-core architecture computational processor. Originally it was developed for a game
machine, PS3, and later used for a super-computer, Load-Runner, in Los Alamos National Laboratory, US. IBM also
provides a blade server, QS20 for scientiﬁc calculations. It was upgraded to QS22. Boards with Cell/B.E. are also
available. In addition, QS20 or 22 has two Cell/B.E. in a Board(see Fig.1). In this paper, we report an implementation
of a code for SU(3) matrix multiplication on QS20.
The Cell/B.E. has eight operation system processor cores called Synergistic Processor Element (SPE) and one
system order processor core called PowerPC Processor Element (PPE). It is a multi-core CPU that deﬁnes a family of
heterogeneous processors. The PPE is PowerPC architecture compliant processor. It is intended to be used as management processor, on which the operating system, memory-management and SPE run-time management application
are executed. On the other hand, the SPE is specialized to calculation and has the new architecture with the ability
of the 128bit Single Instruction Multiple Data (SIMD) operation that instructs only data within the Local Store (LS)
256kB. Each processor is connected by a high-speed bus called Element Interconnect Bus (EIB). In addition, the EIB
is connected to main memory and I/O device, and each processor core performs data access via the EIB.
It is necessary to forward a program and data to LS from main memory so that program execution in the SPE. The
SPE uses Direct Memory Access (DMA) transfer for data transmission.
The DMA is used to forward data directly between main memory and LS (or, main memory and I/O device). The
DMA data transfer instructions are performed concurrently with the SPE program execution because each SPE has
Memory Flow Controller (MFC). See Fig.1.
Theoretical peak speed of our calculation, Eq.(1), on Cell/B.E. is 268.769518291 GFLOPS, (Hardware peak speed
is 460GFLOPS.)
2. Data structure for communication between PPE and SPE through DMA
We pack the matrices of 16KByte in a structure shown in Listing 1. In order to ﬁt the algorithm done on SPE, we
separate the real and imaginary parts of a complex matrix, and pack 112 matrices.
Listing 1: Data Structure for DMA Send

/ / DMA Send Data S t r u c t . ( 1 6 KB Packed )
typedef struct s g p r o d s e n d t
{
f l o a t a r [ 3 ] [ 3 ] [ 112 ] ;
f l o a t a i [ 3 ] [ 3 ] [ 112 ] ;
f l o a t b r [ 3 ] [ 3 ] [ 112 ] ;
f l o a t b i [ 3 ] [ 3 ] [ 112 ] ;
u n s i g n e d char pad [ 256 ] ;
} s gprod send t ;
Our simple starting code to calculate Eq.(1) is given in Algorithm 1. The speed of this scalar operation is shown
in Fig.2 as a function of the number of SPE.

862

Shinji Motoki et al. / Procedia Computer Science 4 (2011) 860–868

Figure 1: Cell/B.E. Architecture

Shinji Motoki et al. / Procedia Computer Science 4 (2011) 860–868

863

Algorithm 1 A simple code to calculate C = A × B. Suﬃxes, R and I stand for the real and imaginary component.
for n = 0 to NV-1 do
for i = 0 to 2 do
for j = 0 to 2 do
DR ← A(n)
(i, 0) · B(n)
(0, j)
R
R
(n)
(n)
DI ← AR (i, 0) · BI (0, j)
DR ← DR − A(n)
(i, 0) · B(n)
(0, j)
I
I
(n)
(n)
DI ← DI + AI (i, 0) · BR (0, j)
DR ← DR + A(n)
(i, 1) · B(n)
(1, j)
R
R
(n)
(n)
DI ← DI + AR (i, 1) · BI (1, j)
DR ← DR − A(n)
(i, 1) · B(n)
(1, j)
I
I
(n)
(n)
DI ← DI + AI (i, 1) · BR (1, j)
DR ← DR + A(n)
(i, 2) · B(n)
(2, j)
R
R
(n)
(n)
DI ← DI + AR (i, 2) · BI (2, j)
CR(n) (i, j) ← DR − A(n)
(i, 2) · B(n)
(2, j)
I
I
(n)
CI(n) (i, j) ← DI + A(n)
(i,
2)
·
B
(2,
j)
R
I
end for
end for
end for

3. Optimization
3.1. SIMD Calculation
The SIMDization is the operation technique that can process plural data by one instruction(see Fig.3). On the
Cell/B.E., the SIMD numerical operations handle 128 bits data, i.e., four real 32 bit data, two double 64 bit data,
(n)
simultaneously. We combine four sequential data A(n)
i, j with n = k, k + 1, k + 2, k + 3 as a vector Ai, j . Thus the loop of
n in Algorithm 1 is changed to 2. We show a calculation result of Scalar Operation and SIMD Operation in Figure 4.
When we use only PPE, it takes 365.080039(msec) , while it is 40.55844 (msec) when we use SIMD(1SPE). When
we use 16SPE, it reaches to 10.13485(msec).
3.2. Loop Unrolling and register optimal usage
The SPE has a large number of registers. Therefore, loop unrolling results in an eﬃcient code. Present compiler
is not strong enough to use this feature.
We must optimize code by hand. We develop a loop by manual operation, and it widens the code range so that a
compiler can optimize many registers.
In addition, the dependency due to the register competition decreases, and one can conceal a stall. Furthermore
the total road/store number in the loop decreases, and can conceal the access latency to LS. These are important
advantages. We should avoid the resister competition. For this purpose we must keep it in mind that the load from
LS to register needs six cycles. Then, it produce a high performance code in which no variable is called which is just
used, or to rearrange the instruction of operations. We show a calculation result of Loop Unrolling in Figure 5.

864

Shinji Motoki et al. / Procedia Computer Science 4 (2011) 860–868

15

10

5

0

Figure 2: Result of using scalar operation

Figure 3: Schematic of Scalar and SIMD operation

Shinji Motoki et al. / Procedia Computer Science 4 (2011) 860–868

25
20
15
10
5
0

Figure 4: Result of using SIMD operation

250
200
150
100
50
0

Figure 5: Result of using Loop Unrolling

865

866

Shinji Motoki et al. / Procedia Computer Science 4 (2011) 860–868

Algorithm 2 n is now running with a stride four in the SIMD version.
1: for i = 0 to 2 do
2:
for j = 0 to 2 do
3:
for n = 0 to NV/4-1 step 4 do
4:
5:
6:
7:
8:
9:

DR ← A(n)
(i, 0) · B(n)
(0, j)
R
R
(n)
(n)
DI ← AR (i, 0) · BI (0, j)
······
end for
end for
end for

4. Real bottleneck
So far, we obtained very satisfactory result, i.e., we can use 80% of the SPU hardware power. Needless to say, we
use whole Cell system to perform a calculation, i.e., we should provide data to SPU through Main Memory.
We measure the time including the data transfer process, and ﬁnd that the speed drops down to 12 GFLOPS. This
means that we failed to provide number of data which SPU process. In order to improve the situation by hiding the
time for communication, we construct a double buﬀering programming. The essential idea of this method is to overlap
the data transfer and ﬂoating calculations.
Algorithm 3 Single Buﬀering
repeat
SPE gets Buf1 that includes A and B from MM.
SPE calculates C = A × B
SPE puts Buf2 that includes C to MM
until no data is left in MM.

Algorithm 4 Double Buﬀering
parallel do
SPE gets Buf1-black : SPE calculates data in Buf1-red
end parallel do
repeat
parallel do
SPE puts Buf2-red : SPE calculates data in Buf1-black
SPE puts get Buf1-red : SPE calculates data in Buf1-black
SPE puts Buf2-black : SPE calculates data in Buf1-red
SPE puts get Buf1-black : SPE calculates data in Buf1-red
end parallel do
until no data is left in MM.
During buﬀer transfer between MM(Main Memory) and SPE, SPE’s calculation units are idle. Then we can
prepare two buﬀers, say Buf-red and Buf-black and when one buﬀer is sent, the SPE can deal data in other buﬀer(see
Algorithm 3 and 4). However, we ﬁnd that the speed in this case is 13 GFLOPS.
One can go further, i.e., one may construct triple-buﬀering. But, we see that already for the double buﬀering case,
we gain essentially little. Indeed, the EIB and the bandwidth between the EIB and each SPE is enough wide. However,
all data are supplied from the main memory, and 16 SPE’s crunch huge amount of data. In our highly eﬃcient code,
the SPE’s process matrices A and B with a rate of 145 GBYTE per second. Moreover, we have to return the matrix C.
In total, we need a data transfer band width of about 220 GB/s.

Shinji Motoki et al. / Procedia Computer Science 4 (2011) 860–868

867

However, the band width to the main memory through the MIC is only 25.6 GB/s, one order of magnitude less.
Therefore even we use many buﬀering method, there is no way to improve the performance.
Now the essential point is to reduce data which are required for SPE calculations. For SU(3) matrix, not all
elements of 3 × 3 matrices A,B and C are independent. The third row of each matrix can be constructed from the ﬁrst
and second rows.
⎞
⎛
⎞
⎛
⎜⎜⎜ v1 w1 ⎟⎟⎟
⎜⎜⎜ v1 w1 z1 ⎟⎟⎟
⎟⎟⎟
⎜⎜⎜
⎟
⎜⎜⎜
⎜⎜⎜ v2 w2 z2 ⎟⎟⎟ ⇒ ⎜⎜⎜ v2 w2 ⎟⎟⎟⎟⎟ , z = (v × w)† .
(4)
⎠⎟
⎝⎜
⎠⎟
⎝⎜
v3 w3 z3
v3 w3
Using this “ reconstruction” technique, we can reduce data, and the speed is now 22 GFLOPS. At PPE, we simply
drop the third column of the matrices A and B. We reconstruct A and B as 3 × 3 matrix at SPE, the computational time
of the reconstruction is negligible. We return the matrix C to the PPE of 3 × 3 form.
The real degrees of freedom of a SU(3) matrix is eight, i.e., U = eiA with A = 8i=1 λ2c Ac . However, in this case, it
takes time to calculate A in the PPE. Therefore, it is reasonable to use the above reduction, (4).
5. Concluding remarks
In this paper, we reported our high-performance code redeveloping experience for SU(3) matrix multiplication on
Cell/B.E.. This calculation is a central part of lattice gauge theories. After tuning the code, the performance with 16
SPU’s is 200 GFLOPS, in single precision. This tuning technique can be applied also for SU(3) matrix times a vector,
and we can expect a good result also for the fermion matrix times vector calculation, which appears in a solver for
Y = DX.

(5)

where D is a huge sparse matrix and contains SU(3) matrices.
Taking the data transfer into account we reach a speed of 22 GFLOPS, i.e. 9% of the theoretical peak speed. This
is sustained value, and from the cost/performance consideration this is still not bad value. Cell/B.E. can be a handy
supercomputing resource even at a small laboratory.
It should be noticed that in real simulations most SU(3) matrix multiplications appear as
A1 × A2 × · · · × An .

(6)

When n is large, the ratio of the communication to the calculation decreases. Therefore the band width problem
becomes weaker.
We stress that the arithmetic calculation speed on the Cell/B.E. is excellent, and the real bottle neck is the band
width to the main memory through the MIC, that is only 25.6 GB/s, whereas the bandwidth between the EIB and each
SPE is wide enough. Therefore, it is highly desirable that in the next generation of Cell/B.E., this band-width should
be widened.
Acknowledgment
The authors would like to thank K. Hashimoto, K. Mizumaru, T. Ishikawa and H. Matsufuru for very helpful
discussions.
References
[1] S. Motoki and A. Nakamura “Development of QCD-code on a CELL Machine”, PoS (LATTICE2007) 040.
[2] S. Motoki et al., Journal of Convergence Information Technology(JCIT), Vol. 5 Num. 4 pp.187-194, 2010.
[3] S. Motoki and A Nakamura, “Problem Solving Environment of Lattice QCD”, Proceeding of 5th International Conference on Computer
Sciences and Convergence Information Technology, pp249-252, ed. by F. Ko, S. Kawata, S. Fong and Y. Na, AICIT, IEEE Conference
Record number #17832.
[4] S. Motoki, Y. Nakagawa, K. Nagata, K. Hashimoto, K. Mizumaru and A. Nakamura, “Development of lattice QCD Tool Kit on cell broadband
engine processor”, PoS(LAT2009)039, 2009.

868

Shinji Motoki et al. / Procedia Computer Science 4 (2011) 860–868

[5] S. Motoki, Y. Nakagawa, K. Nagata, and A. Nakamura, “Implementation and performance optimization of Lattice QCD Tool Kit on the
Cell/B.E., PoS(LAT2010)032, 2010.
[6] F. Belletti et al., PoS (LATTICE 2007) 039.
[7] G. Shi, V. Kindratenko and S. Gottlieb, PoS (LATTICE2008) 026; Int. J. Parallel Prog. 37, (2009), pp488-507. “The Bottom-Up Implementation of One MILC Lattice QCD Application on the Cell Blad”.
[8] G. Shi et al., Scientiﬁc Programming, Vol17 pp.135-151, 2009.
[9] G. Shi et al., “Application Acceleration with the Cell Broadband Engine” Computing in Science & Engineering, Vol12, Issue:1, pp76-81,
2010.
[10] H. Baier et al., PoS (LAT2009) 001.
[11] K. Z. Ibrahim and F. Bodin, “SIMDization and data management of the Lattice QCD computation on the Cell Broadband Engine”, Scientiﬁc
Programming, Vol.17, pp.153-172, 2009; Proc. the 22. annual int. conf. on Supercomputing, pp.4-14
[12] K. Wilson, Phys. Rev. D10,2445 (1974).

