Available online at www.sciencedirect.com

Procedia Computer Science 4 (2011) 412–421

Procedia Computer
Science

Procedia Computer Science 00 (2011) 1–10

International Conference on Computational Science, ICCS 2011

A method for reliably managing files with RNS in multi Data Grids
Yutaka Kawaia , Adil Hasanb , Go Iwaia , Takashi Sasakia , Yoshiyuki Watasea
a Computing

Research Center, High Energy Accelerator Research Organization (KEK), Tsukuba, Ibaraki Japan
b School of English, University of Liverpool, Liverpool UK

Abstract
This paper describes a method for reliably managing files distributed in different kinds of Data Grids with RNS
(Resource Namespace Service). RNS provides hierarchical namespace management for name-to-resource mapping
as a key technology when using Grid resources for different kinds of middleware. We define attribute expressions in
XML for the RNS entries and give algorithms to access distributed files stored within different kinds of Data Grids.
The volume of digital data and the size of an individual file are increasing due to the introduction of high-resolution
images, high-definition audiovisual files, etc. The reliable storage of such large files is becoming problematic with
whole file replication as a failure in the integrity of the file is difficult to localise. Our method involves managing large
files in Data Grids by splitting them into smaller units in a traceable manner and then managing the smaller units. The
RNS catalog service contains EPR (Endpoint Reference) and metadata that describe the original locations as well as
the checksum values. The example in this paper shows how our Grid application can retrieve the actual file locations
and the checksum values from the RNS service via SAGA (A Simple API for Grid Applications). An application can
access the distributed files as though they were files in the local file-system without worrying about the underlying
Data Grids.
This approach can be used with various Data Grid systems to enhance file reliability.
Keywords: RNS, Data Grid, SAGA, checksum

1. Introduction
The volume of digital data and the sizes of the individual files are increasing due to the introduction of highresolution images, high-definition audiovisual files, etc. The reliable storage of such large files is becoming problematic and replication failures anywhere in such a file are difficult to localize.
In this note we describe a method of managing large files in different kinds of Data Grids by splitting them into
smaller units in a traceable manner and managing the smaller units with the RNS (Resource Namespace Service)[1].
RNS catalog server contains the metadata that describes the original locations of the divided pieces and their MD5
checksum values. We also describe the tools developed to demonstrate the method that allows a file to be split before
ingestion into Data Grids and assembled after extraction from the Data Grids. We can store metadata information in
RNS and RNS allows the large file to be discovered in the Grid systems.
Email addresses: yutaka.kawai@kek.jp (Yutaka Kawai), adilhasan2@gmail.com (Adil Hasan), go.iwai@kek.jp (Go Iwai),
takashi.sasaki@kek.jp (Takashi Sasaki), yoshiyuki.watase@kek.jp (Yoshiyuki Watase)

1877–0509 © 2011 Published by Elsevier Ltd. Open access under CC BY-NC-ND license.
Selection and/or peer-review under responsibility of Prof. Mitsuhisa Sato and Prof. Satoshi Matsuoka
doi:10.1016/j.procs.2011.04.043

Yutaka
(2011)
412–421
YutakaKawai
Kawaietetal.
al./ /Procedia
ProcediaComputer
ComputerScience
Science400
(2011)
1–10

413
2

/grid
ogf

jp

data

file1

gfs

fil 1
file1

fil 2
file2

fil 3
file3

file2
fil 4
file4

VirtualDirectory
EPR2
EPR1

Junction

Figure 1: RNS: Hierarchical namespace management.

This paper describes the use of metadata in RNS to manage files distributed in different kinds of Data Grids.
We use two kinds of Data Grids: iRODS (The Integrated Rule-Oriented Data System)[2, 3] and Gfarm (Grid Data
Farm)[4]. The example of this paper involves sharing large files using an application based on SAGA (A Simple API
for Grid Applications)[5, 6] to span the environment with iRODS and Gfarm. We already showed how to manage
distributed files with RNS in such heterogeneous Data Grids[7]. That paper presented an easier way to share the
metadata about each file between the sites of different research organizations for ongoing use. This paper uses the
information for the physical file locations and the MD5 checksum values to enhance the reliability of the files.
This paper gives an overview of RNS in Section 2. Section 3 describes how to access distributed files by using
RNS. An overview of the SAGA is presented in Section 4. In Section 5 we describe the current reliability to manage
a large file and in Section 6 we describe how to enhance the reliability. Section 7 summarizes some of the tests we
used to determine the effectiveness of our approach and we then conclude with a summary.
2. RNS Overview
RNS, which was introduced in GFD101, offers a simple standard way of mapping names to endpoints within a
Grid or distributed network[1]. As shown in Figure 1[8, 9], RNS provides hierarchical namespace management with
name-to-resource mapping[8]. RNS has two fundamental types of entries: virtual directories and junctions. RNS
virtual directories represent non-leaf nodes in a hierarchical RNS namespace tree. RNS junctions link references to
the existing resources into the hierarchical RNS namespace. All compliant RNS implementations must have a valid
WSAddressing[10] EPR (Endpoint Reference).
The prototype implementation of RNS is available from Osaka University[9] and the University of Tsukuba[11].
We use the API of the RNS prototype in our implementation. The RNS servers and clients of the application are
communicating with XML messages using SOAP (Simple Object Access Protocol)[12] as defined in GFD101, and
also each RNS directory or junction entry can contain its own XML messages as metadata.
3. Access to Distributed Files With RNS
As our example of distributed file access, we assume that the large file dataA consists of several file pieces stored
in different kinds of Data Grids. To manipulate dataA, application users need to retrieve all of the distributed pieces
by using a Grid application.
A Grid application needs the physical locations of the existing resources in the Data Grids. The required additional
information can be attached to each RNS entry because RNS entries can include XML metadata. We have defined
XML attribute expressions for each RNS entry to handle all of the physical locations. An RNS virtual directory
contains RNS junctions that have the physical location and the checksum value for each piece of the file. The EPR

414 	

Yutaka Kawai et al. / Procedia Computer Science 4 (2011) 412–421
Yutaka Kawai et al. / Procedia Computer Science 00 (2011) 1–10

3

<ns1:EndpointReferenceType
xsi:type="ns1:EndpointReferenceType"
xmlns:ns1="http://www.w3.org/2005/08/addressing"
1
//
3
/200 /08/
i
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance">
<ns1:Address xsi:type="ns1:AttributedURI">
gfarm://sg07.cc.kek.jp/kawai/images/1GB_100
f
// 07
k k j /k
i/i
/1GB 100
</ns1:Address>
</ns1:EndpointReferenceType>
Gfarm Path Name

Figure 2: EPR example indicating Gfarm resource

<?xml version="1.0" encoding="UTF-8"?>
<file xmlns="http://kek.jp/rns/test">
<rnskv key="checksum" xmlns="">f1c9645dbc14efddc7d8a322685f26eb</rnskv>
</file>
Figure 3: Metadata example of the RNS junction

contains the physical file location and the metadata of the RNS junction contains its checksum value. The algorithms
to retrieve the data from the physical location and the checksum value for the distributed files will be covered in
Subsection 3.3.
3.1. File Piece Information
This is how we express the file piece information in RNS. If the dataA consists of dataA0 , dataA1 , dataA2 , . . . ,
dataAn , the RNS path will be defined in this way:
/. . . /dataA
dataA0
dataA1
dataA2
..
.
dataAn
The dataAi RNS junctions contain the file piece information. These RNS junctions can be listed by pointing
at dataA in the RNS virtual directory. RNS virtual directories directly returns the number of RNS junctions under
themselves.
3.2. Physical Location & Checksum Value
Each RNS junction should be registered to refer to the physical file locations of the dataA pieces. The EPR of each
junction is by itself sufficient to address an existing resource. Any RNS path-name can be registered logically, but it is
better to register the pieces directly in the dataA virtual directory. In particular, we must avoid specifying an absolute
RNS path in the metadata for any RNS entry because an absolute RNS path can be different for each client[1]. The
MD5 checksum value should be contained in each RNS junction as metadata. Figure 2 shows the EPR example for
an RNS junction. Figure 3 shows metadata for an RNS junction in XML.
3.3. Algorithm
The physical location list (urlList) of the existing resources can be obtained by using Algorithm1. In this algorithm, L is the set containing the RNS junctions within the RNS virtual directory. The set R has all of the RNS
junctions in the RNS namespace. The function U maps each RNS junction to an existing physical URL.
The checksum list (chkList) of the file piece can be obtained by using Algorithm2. The function C maps each
RNS junction to a checksum string of an existing file. Algorithm1 and Algorithm2 are algorithms so it is possible to

Yutaka Kawai et al. / Procedia Computer Science 4 (2011) 412–421
Yutaka Kawai et al. / Procedia Computer Science 00 (2011) 1–10

415
4

Algorithm 1 Get the location list of existing resources
1: for each rnsJunction li in L do
2:
if li ∈ R then
3:
add URL ui = U(li ) to urlList
4:
else
5:
add NULL to urlList
6:
end if
7: end for
8: return urlList

// Open RNS directory
saga::url u (argv[1]);
saga::replica::logical_directory ld (u, saga::replica::Create | saga::replica::ReadWrite);
// get RNS junctions
std::vector<saga::url> rns_jncs = ld.list();
// get physical locations and md5 checksum values
std::vector<saga::url> img_urls;
std::vector<std::string> img_md5s;
for(unsigned int i=0; i<rns_jncs.size(); i++){
saga::url f_url = u.get_string() + "/" + rns_jncs[i].get_string();
saga::replica::logical_file lf (f_url, saga::replica::Create | saga::replica::ReadWrite);
std::vector<saga::url> epr_list = lf.list_locations();
img_urls.push_back(epr_list[0]);
img_md5s.push_back(lf.get_attribute("checksum"));
}
Figure 4: A part of SAGA C++ source example

combine them in actual implementations. Part of the SAGA C++ source code that gets the physical location list and
the checksum list is shown in Figure 4. Additional information about SAGA appears in the next section.
Algorithm 2 Get the checksum list of existing resources
1: for each rnsJunction li in L do
2:
if li ∈ R then
3:
add MD5CHECKSUM ci = C(li ) to chkList
4:
else
5:
add NULL to chkList
6:
end if
7: end for
8: return chkList

4. SAGA Overview
Using Data Grids involves using their specialized commands and rules to access their files. The differences between these commands and rules can cause problems for application developers. SAGA provides a unified interface
that conceals the differences among the different middleware infrastructures. The abstraction layer of SAGA is positioned as a bridge among the various kinds of Data Grids. Once a SAGA adaptor for each kind of middleware has
been prepared, the application developers only need the functional API without worrying about the specific features of

416 	

Name
SNA
STA
SPA
SIA
SGFA
SRA

Yutaka Kawai et al. / Procedia Computer Science 4 (2011) 412–421
Yutaka Kawai et al. / Procedia Computer Science 00 (2011) 1–10

5

Table 1: SAGA Adaptors under development in KEK.

Full Name
SAGA NAREGI Adaptor
SAGA Torque Adaptor
SAGA PBSPro Adaptor
SAGA iRODS Adaptor
SAGA Gfarm Adaptor
SAGA RNS Adaptor

Middleware
NAREGI (National Research Grid Initiative)[14, 15]
Torque
PBSPro (Portable Batch System Professional Edition)
iRODS
Gfarm
RNS

SAGAAdaptorHost

Login
SRA

Get
Location

User

SGFA

Put
Location

RNS
Server

Get
Data

SIA

DefaultFILE

iRODS

Local
Filesystem

Put
Data
Gfarm

Figure 5: Workflow diagram in the user environment based on SAGA.

the middleware. We are developing the SAGA adaptors shown Table 1, in compliance with the SAGA specification,
Version 1.0[6], as standardized[5] within the OGF[13].
Our prototype works in a SAGA environment with iRODS, Gfarm, a local file-system and RNS. Therefore, we
used SIA, SGFA and SRA. A SAGA library with SIA, SGFA and SRA is installed on a host server that is called the
“SAGA Adapter Host”. The client applications for iRODS and Gfarm should also be installed on the SAGA Adapter
Host. Figure 5 shows the workflow for our experiments.
The client application should get the information about the physical file locations and their checksum values from
an RNS server via SRA before accessing the Data Grids. When the application stores files on the Data Grids, the
application should send the information about the file piece locations and the checksum values into the RNS server
after storing the file pieces.
We can easily use any type of Data Grid as one resource if the SAGA adaptors are available. The SAGA community has released several SAGA core implementations[16]. The current SAGA C++ implementation supports Curl,
Globus GridFTP, SSH File, and Local File system, in addition to our developments work for iRODS and Gfarm.
5. Current Approach to Checksum
The traditional way to checksum a large file (around 50GB) is by preparing the checksum value for the whole
file and then re-evaluating the checksum after a certain period of time[17]. Such an approach has two main issues:
re-computing the checksum for the entire file is time-consuming, and it is not possible to identify the exact location of
a discrepancy when the checksum comparison fails. If we encounter checksum errors, we need time-consuming work
to redo the entire operation, or to find a copy of the file that has retained its integrity.
Preserving audiovisual data is required by TV and other companies today. Movie files are very large and in some
cases compressed. Using the traditional approach to checksum a file, it is not easy to figure out where the problem is
or how to fix it if the downloaded data has incorrect or missing data.

417
6

Yutaka Kawai et al. / Procedia Computer Science 4 (2011) 412–421
Yutaka Kawai et al. / Procedia Computer Science 00 (2011) 1–10

Data Grid
Piece 0
Source File

split

Piece 1
Piece n

+
+

Checksum
Checksum

+

Checksum

Figure 6: Splitting a file with checksum

Data Grid
Piece 0
Piece 1
Piece n

+
+
+

combine
Checksum

Checksum OK

Checksum

Checksum NG

Checksum

Checksum OK

Piece 0
X

Retry to
download

Piece n
Source File

Figure 7: Combining pieces with comparing checksum

6. Split & Checksum Approach
Our approach is to split a large file into smaller pieces, then checksum each piece and assemble the file again when
it is accessed. This approach reduces the risks of incorrect data or missing data. Figure 6 shows how to split a file and
Figure 7 shows how to combine the pieces. If we encounter checksum discrepancies, we can identify the problematic
piece and retry the download of that piece to resolve the discrepancy. This approach is much faster than the traditional
method of erasing and downloading all of the data. To implement this approach, we developed a command that can
split a large file into smaller pieces and store the pieces into Data Grids with their metadata. The metadata information
can also be stored in RNS with XML expressions.
SAGA supports storing the pieces in different kinds of Data Grids. Figure 8 shows an example. If we divide a file
into 100 pieces, we can store the first pieces from #0 to #33 in iRODS. The next pieces from #34 to #66 can be stored
in Gfarm and the remaining pieces will be stored in the local file system. Each physical location and checksum value
will be recorded by the RNS catalog service via SAGA. We do not need to worry about the different interfaces of the
Data Grids.
7. Performance Evaluation
In this section we describe some tests carried out to assess the overhead of checking the MD5 checksum values of
the divided pieces. We used a 1GB file and divided it into different numbers of pieces from 10 to 100. We evaluated
the elapsed time to download the pieces for each collection.

418 	

Yutaka Kawai et al. / Procedia Computer Science 4 (2011) 412–421
Yutaka Kawai et al. / Procedia Computer Science 00 (2011) 1–10

7

Data Grids
Piece 00
Piece 33

User

SIA
iRODS

Piece 34
Piece 66

SGFA
Gfarm

Piece 0

SAGA

Piece 67
Pi
Piece
99

Local
File
System

Piece 99

Default

SRA
Catalog Service
#00 - #33 Æ iRODS
#33 - #66 Æ Gfarm
#67 - #99 Æ Local

+

Checksum
Checksum
Checksum

RNS

Figure 8: Access to distributed pieces in different Data Grids

Also, we evaluate the overhead of SAGA and RNS. The first test was run without SAGA. The next test assessed
the overhead of SAGA with RNS.
7.1. Test Environment
The test environment consists of the iRODS server, the Gfarm server, the RNS server, and the SAGA Adaptor
host. Those machines run on CentOS 5.5 as VMware guests on the same physical machine. The physical machine
has an Intel CPU i7-920@2.67GHz with 12GB of RAM. Each guest OS runs with 1GB of RAM and one dedicated
core. All of the machines are located on the same subnet. The test environment can have a non-trivial and noticeable
effect on the results of such tests.
7.2. Test Cases
We evaluated our implementations for the transfer of several different numbers of pieces: 1, 10, and 10 to 100.
For the evaluation, we stored all of the pieces on the three different Data Grids: iRODS, Gfarm and the local filesystem. Then we downloaded the pieces and compared each checksum value. For example, in the case of iRODS
with 20 pieces, we divided the 1GB source file into 50MB pieces and stored them in the iRODS. In the evaluation,
we then downloaded all of the 50MB pieces and compared the checksum of each piece, measuring the elapsed time
to download the pieces and compare the checksum values. For the average values, each test program ran three times.
We compared six test configurations:
• Case 1: iRODS without SAGA
• Case 2: Gfarm without SAGA
• Case 3: iRODS with SAGA and RNS
• Case 4: Gfarm with SAGA and RNS

Yutaka Kawai et al. / Procedia Computer Science 4 (2011) 412–421
Yutaka Kawai et al. / Procedia Computer Science 00 (2011) 1–10

419
8

• Case 5: Local File System with SAGA and RNS
• Case 6: Mixture of iRODS, Gfarm and Local File System with SAGA and RNS
Here are detailed descriptions of the test cases:
Case 1, 2: Cases without SAGA
The normal cases are traditional situations. The specific commands for the Data Grids are used. Each checksum
value is contained in each Data Grid as its own metadata. This case is the most basic because the list of checksum
values cannot be shared and the commands to access the Data Grids are specified in the source code.
Case 3, 4, 5: Cases with SAGA and RNS
Using SAGA with RNS. We can use standardized SAGA interfaces and the information about the file locations
and checksum values are contained in the RNS catalogue. Therefore, the file locations and the checksum values can
be shared.
Case 6: Mixture of different Data Grids with SAGA and RNS
The mechanism of this case is same as for Cases 3, 4, and 5. Additionally, in this case, we stored a third of the
pieces on each of the three different Data Grids: iRODS, Gfarm and the local file-system. For example, in the case of
the 10 pieces, we divide the 1GB source file into 100MB pieces and stored them in the three different Data Grids: 3
pieces in iRODS, 3 pieces in Gfarm, and 4 pieces in the local file system.
7.3. Test Results
Figure 9 shows the results of the tests without SAGA. The overhead to compare the checksum values in Case 1
grows linearly. The 10 piece version of Case 1 is about 10.9% slower than the non-divided cases, which is not so
different. In contrast, the 100 piece version of Case 1 is about 54.4% slower on average than the non-divided case
which is unacceptably slow.
The latency involves accessing each piece to compare the checksum. As the number of pieces increases, the
elapsed time increases. However, in the versions from 10 to 40 pieces for Case 1, the elapsed times are almost
the same as for the non-divided case. In these cases the checksum values can be compared efficiently because the
checksum overhead is absorbed in its potential access latency, which is around 25 seconds. Therefore, we can use
this approach without worrying about the loss of speed when we are dividing a large file into less than 40 pieces for
Case 1. The same conclusion applies to the 10 to 90 piece versions of Case 2. The checksum overhead of Case 2 is
absorbed in its potential access latency, which is around 50 seconds.
Figure 10 shows the results of the tests with SAGA and RNS. The overhead to use the SAGA abstraction layer
in all of the cases grows linearly. The versions with 100 pieces are about 50.7% (Case 3) and 58.8% (Case 4) slower
than the non-divided cases.
SAGA allows us to use different kinds of Data Grids. The speed degradation can be mitigated by SAGA by using
faster storage resources in a mixture. The local file system is optimal for speed. The 100 piece version of Case 6 is
about 34.5% slower than the non-divided version. The latency is decreased by using the SAGA and RNS solution.
8. Related Work
Various projects are underway to reliably manage files and to aggregate different heterogeneous Data Grids or
file-systems for efficient data sharing. Here is a summary of work related to various aspects of our study.
Luis E G. Sarmenta presented sabotage-tolerance mechanisms that work without depending on checksums or
cryptographic techniques[18]. A new mechanism using voting and spot-checking together with credibility-based
fault-tolerance reduced the error rates. However, such a complex mechanism requires understanding the approach and
implementing it within existing systems. Also, they assumed that the mechanism would be applied to only one type
of storage system. It would be difficult to apply it to different kinds of Data Grids simultaneously.
Jerzy Kaczmarek et al presented the concept and architecture of the ICAR System (Integrity Checking And Restoring System)[19]. The ICAR System not only covers the functions of integrity checkers but also automatically restores

420 	

Yutaka Kawai et al. / Procedia Computer Science 4 (2011) 412–421
Yutaka Kawai et al. / Procedia Computer Science 00 (2011) 1–10

sec

9

Elapsed
p
Time to download p
pieces
(shorter is better)

60
50
40
30
20

iRODS without SAGA
Gfarm without SAGA

10
0
1

10

20

30

40

50

60

# of pieces

70

80

90

100

*Size of Source File is 1GB

Figure 9: Speed Performance Test Results

sec

Elapsed Time to download pieces
(shorter is better)

90
80
70
60
50
40

iRODS with SAGA

30

Gfarm with SAGA

20

Local with SAGA

10

Mix with SAGA

0
1

10

20

30

40

50

# of pieces

60

70

80

90

100

*Size of Source File is 1GB
**Mix case starts from 10pcs

Figure 10: Speed Performance Test Results with SAGA

files. However, ICAR is designed as a kernel module for the operating system, so it is mainly used for local file
systems. This is not suitable for use with Data Grids.
Hamid-Reza Mizani et al. proposed VOFS (Virtual Organization File System) to hide the heterogeneity of aggregated file-systems[20]. VOFS also has a metadata management service so that VOFS can handle traditional information (file size, last modification time, creation time, files in a directory, etc.) and VOFS-specific metadata. However,
VOFS requires each registered file-system to install a VOFS server. In our RNS solution, the file-systems do not need
any customization.
Horst E Wedde et al.[21], Dan Feng et al.[22], and Yinjin Fu et al.[23] all discuss efficient metadata management.
However, they are using only one or a few limited kinds of file-systems. RNS can handle the resources of any kind of
file-system.

Yutaka Kawai et al. / Procedia Computer Science 4 (2011) 412–421
Yutaka Kawai et al. / Procedia Computer Science 00 (2011) 1–10

421
10

9. Conclusion
We have described a method for reliably managing large files in different kinds of Data Grids using SAGA and
RNS. The example showed how to split a large file and contain its MD5 checksum value as its metadata in RNS
catalog service. We also showed how the application can compare all of the checksum values and then combine the
pieces that were distributed among different Data Grids. Our tests showed that not only is there no need to change
an application to access files in the multiple Data Grids, but also that the physical file locations and MD5 checksum
values associated with each file can be shared with RNS. Our tests also showed that the speed degradation can be
mitigated by SAGA by using faster storage resources in a mixture. We believe this approach results in more reliability
for large-file replication as different sub-file units can be stored on different storage systems, reducing the risks due to
hardware failures.
10. Acknowledgments
The authors would like to thank the RNS implementation development team lead by Hideo Matsuda of Osaka
University and Osamu Tatebe of the University of Tsukuba.
References
[1] M. Pereira, O. Tatebe, et al., Resource namespace service specification (GFD-R-P.101), Tech. rep., GFS-WG, http://www.ggf.org/
documents/GFD.101.pdf (2007).
[2] iRODS – the Integrated Rule-Oriented Data System, Online, http://www.irods.org.
[3] A. Rajasekar, M. Wan, R. Moore, W. Schroeder, A Prototype Rule-based Distributed Data Management System, in: Proc. HPDC workshop
on ”Next Generation Distributed Data Management”, Paris, France, 2006.
[4] Gfarm – Grid Data Farm, Online, http://datafarm.apgrid.org/index.en.html.
[5] S. Jha, H. Kaiser, Y. El Khamra, O. Weidner, Design and Implementation of Network Performance Aware Applications Using SAGA and
Cactus, in: Proc. The 3rd IEEE Conference on eScience2007 and Grid Computing., Bangalore, India, 2007, pp. 143–150.
[6] Goodale, Tom, et al., SAGA - v1.0 Specification (GFD-R-P.90), Tech. rep., SAGA-CORE-WG, http://www.ggf.org/documents/
GFD.90.pdf (2008).
[7] Y. Kawai, G. Iwai, T. Sasaki, Y. Watase, Managing distributed files with RNS in heterogeneous Data Grids, in: Proc. the 11th IEEE/ACM
International Symposium on Cluster, Cloud, and Grid Computing, CCGrid 2011, California, US, 2011.
[8] O. Tatebe, Discussion of File Catalog Standardization, Tech. rep., GFS-WG, http://www.ogf.org/OGF24/materials/1403/
intro.pdf (2008).
[9] H. Matsuda, File Catalog Development in Japan e-Science Project, Tech. rep., GFS-WG, http://www.ogf.org/OGF24/materials/
1403/OGF24-GFS-matsuda.pdf (2008).
[10] Web Services Addressing 1.0 – Core, Online, http://www.w3.org/TR/ws-addr-core/.
[11] N. Masahiro, T. Osamu, Implementation of Resource Namespace Service [in Japanese], Information Processing Society of Japan (IPSJ) 5
(2008) 145 – 146.
[12] Simple Object Access Protocol (SOAP) 1.1, Online, http://www.w3.org/TR/2000/NOTE-SOAP-20000508.
[13] OGF – Open Grid Forum, Online, http://www.ogf.org/.
[14] S. Matsuoka, S. Shimojo, M. Aoyagi, S. Sekiguchi, H. Usami, K. Miura, Japanese Computational Grid Research Project: NAREGI, Proceedings of the IEEE 93 (3) (2005) 522–533.
[15] NAREGI – NAtional REsearch Grid Initiative, Online, http://www.naregi.org/index e.html.
[16] SAGA: A Simple API for Grid Applications, Online, http://saga.cct.lsu.edu/.
[17] Z. Yong-Xia, Z. Ge, MD5 Research 2 (2010) 271 – 273.
[18] L. E. G. Sarmenta, Sabotage-tolerance mechanisms for volunteer computing systems, in: Proc. Cluster Computing and the Grid, Brisbane,
Australia, 2001.
[19] J. Kaczmarek, M. Wrobel, Modern approaches to file system integrity checking, in: Proc. The 1st International Conference on Information
Technology, Gdansk, Poland, 2008.
[20] H.-R. Mizani, L. Zheng, V. Vlassov, K. Popov, Design and Implementation of a Virtual Organization File System for Dynamic VOs, in: Proc.
The 11th IEEE International Conference on Computational Science and Engineering (CSE), Sao Paulo, Brazil, 2008, pp. 77 – 82.
[21] H. E. Wedde, J.-O. P. Siepmann, A Universal Framework for Managing Metadata in the Distributed Dragon Slayer System 2 (2000) 96 – 101.
[22] D. Feng, J. Wang, F. Wang, P. Xia, DOIDFH: an Effective Distributed Metadata Management Scheme, in: Proc. Computational Science and
its Applications (ICCSA), Kuala Lumpur, Malaysia, 2007, pp. 245 – 252.
[23] Y. Fu, N. Xiao, E. Zhou, A Novel Dynamic Metadata Management Scheme for Large Distributed Storage Systems, in: Proc. The 10th IEEE
International Conference on High Performance Computing and Communications (HPCC), Dalian, China, 2008, pp. 987 – 992.

