Available online at www.sciencedirect.com

Procedia Computer Science 4 (2011) 96–105

International Conference on Computational Science, ICCS 2011

The UrbanFlood Common Information Space for Early Warning
Systems
Bartosz Balisa,∗, Marek Kasztelnikb , Marian Bubaka,b , Tomasz Bartynskib , Tomasz Gubałab , Piotr Nowakowskib ,
Jeroen Broekhuijsenc
a Department

of Computer Science, AGH University of Science and Technology, Krakow, Poland
b ACC CYFRONET AGH, Krakow, Poland
c TNO, Groningen, The Netherlands

Abstract
Early Warning Systems (EWS) can play a crucial role in mitigating the eﬀects of natural disasters. Modern
EWSs leverage wireless sensors for real-time monitoring of natural phenomena and computing-intensive scientiﬁc
applications for scenario-based prediction and analysis of sensor data. This paper presents the UrbanFlood Common
Information Space (CIS), a framework facilitating the creation, deployment and reliable operation of early warning
systems. CIS proposes a reference architecture for EWS and provides services to address problems common to all
EWSs as complex software systems: integration of legacy scientiﬁc applications, workﬂow orchestration, allocation
of computational resources and robust operation. We demonstrate a ﬂood early warning system created using the CIS
technology and discuss the beneﬁts of our approach which include shorter EWS development time, exposing EWS as
a set of reusable services, platform independence and extensibility.
Keywords: Early Warning System, Common Information Space, software framework, Service Oriented
Architecture, legacy applications

1. Introduction & Motivation
Early Warning Systems are found in many places. In spite of variations between monitored domains, their main
goal is similar: to reduce economic losses and mitigate the number of deaths from disasters by delivering information
which allows people and organizations to prepare for emerging disasters [1]. This paper deals with systems which
rely on wireless sensors and provide early warnings against environmental threats (ﬁres, ﬂoods, earthquakes). Such
EWSs typically contain parts responsible for gathering data from sensors, a number of scientiﬁc applications for
performing various analyses of this data (such as simulation-based predictions) and user interfaces where analysis
results are presented. A scientiﬁc challenge is to ﬁnd the best methods and tools for ﬂexible and eﬃcient integration
and orchestration of these compute- and data intensive applications in a way that fulﬁlls the requirements speciﬁc for
the EWSs as a class of systems.
∗ Corresponding

author. E-mail address: balis@agh.edu.pl

1877–0509 © 2011 Published by Elsevier Ltd. Open access under CC BY-NC-ND license.
Selection and/or peer-review under responsibility of Prof. Mitsuhisa Sato and Prof. Satoshi Matsuoka
doi:10.1016/j.procs.2011.04.011

Bartosz Balis et al. / Procedia Computer Science 4 (2011) 96–105

97

Emerging initiatives such as the Inspire directive [2], the GEOSS initiative [3] or SEIS [4] recognize the importance of common standards and infrastructures for organizing geospatial information, including the requirements of
early warning systems.
The goal of the UrbanFlood project [5] is to create an open platform for Early Warning Systems. An Early Warning
System is understood, very broadly, as any system which follows a four-step cycle: (1) Monitoring: data is collected
in real time from sensors and fed to EWS; (2) Analysis: one or more applications are invoked to perform analysis of
sensor data streams, such as anomaly detection or simulations based on expert models; (3) Judgement: the outcome of
this analysis is judged according to EWS-speciﬁc rules in order to estimate the risk level or determine the necessity of
taking action: (4) Advice/act: if mandated by the value judgment, further in-depth analysis may be invoked in order
to recommend or automatically perform actions to inﬂuence the system.
The heart of the ICT infrastructure of the UrbanFlood platform is the UrbanFlood Common Information Space.
While the concept of a common information space is central to the above-mentioned emerging initiatives, it usually
forms a small part of the whole system. In contrast, the UrbanFlood CIS is meant as a blueprint, or a reference
model, for an entire EWS. While the UrbanFlood platform is generic, the project validates it on the basis of ﬂood
early warning scenarios [6, 7].
The key goal of CIS is to help reduce eﬀort related to development of a new EWS by providing a reference
model and a methodology for building early warning systems, and a framework for running them. CIS addresses all
stages of the EWS lifecycle: (1) creation of a blueprint for a speciﬁc early warning system, e.g. for ﬂood monitoring;
(2) deployment of the blueprint in a concrete setting (for a speciﬁc dike managed by a speciﬁc institution); (3) reliable
operation of the Early Warning System. CIS, being a software framework, aims at solving problems common to all
early warning systems at all these stages, such as integration of legacy scientiﬁc applications, workﬂow orchestration,
allocation of computational resources and robust operation.
The paper is organized as follows. In section 2 related work is discussed. Section 3 describes concepts and
technical details of the Common Information Space. Section 4 presents the Flood Early Warning System implemented
using the CIS technology. Section 5 concludes the paper.
2. Related Work
Interest in Early Warning Systems has picked up over the past years; however the applied approaches have usually
been very speciﬁc, and the created systems rather monolithic. In a typical solution sensor data is acquired, processed
by a model (scientiﬁc application) and then visualized using a tool. Based on the tool or on the outcome of the model,
a warning is generated. Usually these systems do not oﬀer any interaction, the models are not extensible and there is
no support for arbitrary types of sensors.
Some use cases warrant a diﬀerent, more global approach. These include Tsunami Early Warning Systems,
Earthquake Early Warning Systems and Fire Early Warning Systems. The Indian Tsunami Early Warning [8, 9]
combines sensors for earthquake detection, ocean-based sensors to detect tsunamis, satellites to correlate weather
information together with maps of the area to be able to correctly predict where the tsunami will strike and possibly
cause casualties. The US Geological Survey Home [10, 11] oﬀers access to a global network of seismic sensors and
performs processing of the data to generate warnings to concerned stakeholders. The Fire Warning System [12, 13, 14]
created by the University of Freiburg has a similar goal – to deliver a map which plots all forest ﬁres in the shortest
time possible.
Another group of systems that should be mentioned comprises scientiﬁc workﬂows environments. Some of them
have made the eﬀort to support data stream processing within workﬂows. The Kepler workﬂow management system
has recently been adapted to support access to environmental sensor data [15]. The work is driven by two diﬀerent
use cases. In the ﬁrst one, a terrestrial ecology domain application, sensors are used to measure soil properties and
meteorological conditions in order to experiment and verify hypotheses related to grass growth. The second use case
is from the oceanography domain and concerns sea surface temperature (SST) sensors. Datasets collected from such
sensors are very large and are typically preprocessed to obtain smaller, workable datasets. For this purpose, Kepler
has been used to compare heterogeneous SST datasets from diﬀerent providers. A new component (actor in Kepler
terms) has been added, capable of feeding a workﬂow with data from the DataTurbine streaming middleware to which
sensor data is published. The two use cases are rather diﬀerent from those in UrbanFlood. They do not possess

98

Bartosz Balis et al. / Procedia Computer Science 4 (2011) 96–105

the characteristics of an early warning system and, consequently, impose diﬀerent requirements on the workﬂow
management system.
In [16], an attempt to adapt a WS-BPEL runtime Apache ODE to scientiﬁc workﬂows is presented. The use cases
include “stream mining” which denotes the capability to feed a workﬂow with a data stream. The overall goal is
to integrate the WS-BPEL runtime with the Linked Environment for Atmospheric Discovery (LEAD) project whose
aim is to provide weather forecasting capabilities for severe conditions such as tornadoes. The LEAD workﬂow
environment tackles several problems important for scientiﬁc workﬂows. It provides a graphical editor to construct
workﬂows; moreover, it supports dynamic creation and binding of service instances, dynamic workﬂow deployment,
monitoring and asynchronous invocation. Data stream support is provided by introducing two BPEL workﬂows: the
ﬁrst one receives messages in a loop and, for each message, invokes the actual scientiﬁc workﬂow. Overall, this
solution has a few restrictions. It is strictly oriented towards web services and the SOAP protocol, which makes it
unsuitable for dealing with sensor data streams due to performance issues. Furthermore, its focus on web services
limits the possibilities for integration with legacy applications.
Inspire [2] is an initiative of the Europe Union which deﬁnes a set of APIs for connecting systems developed by
diﬀerent organization into one working system. The directive is speciﬁcally aimed at metadata, spatial data sets and
services. It also lists regulations for network services, service sharing, monitoring and reporting. The Sensor Web
Enablement (SWE) initiative [17] is a project advanced by the Open Geospatial Consortium whose goal is to build
a framework of open standards for exploiting web-connected sensors and sensor systems. Both initiatives stress the
need for leveraging open standards and reliance on Service-Oriented Architectures.
Overall, none of the described approaches attempts to propose a complete framework for creating new early warning systems. Some generic solutions target important but speciﬁc problems. Nevertheless, it is clear that a framework
for early warning systems must take into account existing initiatives and emerging standards for environmental information.
3. The Common Information Space for Early Warning Systems
3.1. CIS Functions & Architecture
CIS provides universal services and building blocks for EWSs in order to address basic problems common to most
early warning systems which rely on sensor-based monitoring: (1) Integration of various EWS-speciﬁc components
with CIS (sensor data sources and legacy applications for processing sensor data, simulations, etc.); (2) Orchestration
of data and control ﬂow between the EWS components in order to execute more complex early warning scenarios;
(3) Allocation of computing resources (organized as clouds) to one or more Early Warning Systems; (4) Publishing
and browsing various metadata (regarding existing early warning systems, sensor data semantics, available resources,
provenance, etc.); (5) Deployment and conﬁguration of a new EWS instance; (6) Robust operation of the EWS.
The requirement analysis for the Early Warning System framework has led us to adopt the Service Oriented
Architecture, partially in view of compliance with the emerging initiatives for geospatial data which are oriented
towards leveraging open standards and SOA. The functional architecture of CIS is depicted in Fig. 1. Key components
of CIS are as follows:
• Integration platform (PlatIn): CIS core technologies for component integration, data exchange and workﬂow
orchestration.
• Metadata registry (UFoReg): a generic service for hosting and querying metadata.
• Dynamic resource allocation service (DyReAlla): a service for dynamic allocation of resources to running
Early Warning Systems.
The technical core of the framework is based on modern paradigms and mature open-source technologies: Enterprise Service Bus (OpenESB), BPEL, Enterprise Integration Patterns (Apache Camel), message-oriented communication middleware (ActiveMQ), robust document-oriented database (MongoDB). The following sections discuss main
concepts as well as some technical details behind the key CIS components.

Bartosz Balis et al. / Procedia Computer Science 4 (2011) 96–105

99

Figure 1: Functional Architecture of the Common Information Space.

3.2. Integration & Flow Orchestration
An Early Warning System typically works according to a scenario which involves monitoring and analysis of
sensor signals, computation of alert levels, invocation of additional analysis triggered automatically or by human
interaction, visualization, etc. Unlike a business workﬂow, this scenario as a whole is not as ﬁxed. Indeed, it may
often proceed in an ad-hoc manner. On the other hand, it is possible to identify certain sub-workﬂows which have
a high repeatability rate. These characteristics have led us to adopt the following basic software model for a CIS-based
Early Warning System.
First, the basis for EWS functionality is provided by appliances, i.e. applications wrapped into virtual images
and exposed as services, which means they can be invoked over the network. The interface of an appliance should be
deﬁned in WSDL and XML (message exchange format). CIS provides a Service Bus which abstracts communication
protocols. It is not restricted to SOAP – other possible protocols include REST, JMS or FTP.
Second, Composite services (composites) can be created to orchestrate data and control ﬂow involving one or more
appliances (or other composites). Composites operate according to the following principles: (1) Composites work
independently: they can be started and stopped independently and should not require other composites to perform
their task; (2) They consume input data from and publish results to the message bus. Composites are also referred to
as EWS Parts as each is responsible for a speciﬁc part of the high-level services oﬀered by the Early Warning System.
Several beneﬁts of the adopted model can be identiﬁed here:
• Support for both ad-hoc and ﬁxed scenarios. Whenever a ﬁxed workﬂow is identiﬁed, it can be implemented
as a Part. Ad-hoc orchestration is supported by the dataﬂow mechanism between parts: one part can trigger the
execution of another indirectly by publishing a message to the message bus.
• Extensibility. EWS can be extended simply by adding a new part, which, for example, consumes data already
published to the bus and processes it in a new way. This does not aﬀect existing parts in any way and, in fact,
can even be done at runtime.
• Technology independence. Since communication between parts proceeds solely through well-deﬁned message
exchanges, composites can be created in various technologies. Indeed, our current implementation already
supports two integration technologies: BPEL (OpenESB) and EIP (Enterprise Integration Patterns based on
Apache Camel).
• EWS-as-a-Service. Composites are in fact independent services which may leverage legacy scientiﬁc applications (or any other software) in order to perform well-deﬁned tasks. These services can easily be reused as
building blocks for new EWSs or even entirely diﬀerent systems. We provide part templates which can be
customized in order to create EWS-speciﬁc services.

100

Bartosz Balis et al. / Procedia Computer Science 4 (2011) 96–105

Apart from providing high-level services, composites also fulﬁll other important functions: asynchronous receipt
and correlation of messages, data transformations, computation of alert levels, logging and self-monitoring. Moreover,
because orchestration among parts is done purely on a dataﬂow basis, all control ﬂow (except for human interaction)
needs to be encapsulated in parts.
3.3. Metadata Management
The UrbanFlood Metadata Registry (UFoReg) is one of the main “memory” components of a CIS setup. Its
main purpose is to persistently store and provide, on an on-demand basis, various pieces of data or metadata. The
information managed by UFoReg may come from diﬀerent sources and be consumed by various actors, whether
artiﬁcial (other CIS components) or human (who use dedicated tools to browse the data). Therefore, the core of
the Registry involves a general-purpose data persistence solution, speciﬁcally targeted at managing a large number
of small data objects (mini-documents) of considerable diversity (ranging from geographical coordinates, through
simulation parameter templates up to the conﬁguration descriptions for entire environments). In order to properly deal
with the vast domain of the stored metadata in a structured manner, the Semantic Integration methodology [18] has
been applied. This required us to divide all the stored metadata into two main parts: the domain metadata (associated
with the UrbanFlood hydrology applications, required by the end users and simulations) and the integration metadata
(all other data required by CIS components, the applied technologies, protocols etc.)
UFoReg is meant to be interfaced by many diﬀerent peers. These may be roughly divided into two main groups:
• Tools which are used by humans (directly, through GUIs or indirectly, invoked by other GUIs) – these are
usually responsible for manual data registration or retrieval (browsing, searching). Users might also browse
historical data which is automatically generated by engines,
• Engines which work in the lower layers of CIS and perform the duties related to EWS workﬂow management
– these entities automatically (according to their conﬁguration) log (and later reuse) historical and provenance
data about the performed operations. In addition, they may consume custom metadata provided, for example,
by EWS administrators using tools.
Stored metadata may be divived into three main subsets:
• Sensor metadata: UFoReg stores and eﬃciently publishes sensor descriptions and important information about
what measurements are taken and where they belong in the monitored infrastructure (for instance, the sensors
installed inside dikes). Sensor data may be registered by human operators or fed directly by means of a dedicated
service. In order to retrieve sensor data, the user can provide partial semantic descriptions or just ask for sensors
which most closely match the supplied characteristics. UFoReg will respond to such queries by presenting the
identiﬁers of the target sensors. Using the IDs, clients may subsequently refer to the Measurement service,
requesting observations from a certain interval.
• EWS topology: UfoReg also works as an “introspection” element of EWS, allowing it to store information about
itself, its topology, computing infrastructure and the metadata regarding the monitoring of that infrastructure
(hosts, appliances, composites). The information is obtained from real-time host monitoring sensors and stored
in UFoReg where any management or self-healing service can obtain it on demand.
• Provenance: UFoReg stores information about data provenance, which is the recording of the the process
which produced such data. In UrbanFlood, the main provenance-based use case is the repetition of a previous
simulation workﬂow in order to, for example, calibrate a simulation model. To support this, UFoReg should
store a workﬂow trace (what activities have been executed in the workﬂow, in what sequence, what was their
input and output data) and information about invoked applications (what components were invoked from the
workﬂow activities, what was their conﬁguration, where were they deployed, what data was passed to and
received from these components).
The current implementation of the UFoReg metadata registry in the UrbanFlood project supports the main use
cases mentioned above and handles most of the metadata domain. The sensors domain is fully supported, while EWS
topology and provenance domains are supported in large part and will be further extended in the course of CIS design.
The persistence solution used is the NoSQL MongoDB database while the entire solution is written in Ruby. The
Web/REST interface is served on top of the Phusion Passenger/Sinatra technology stack.

Bartosz Balis et al. / Procedia Computer Science 4 (2011) 96–105

101

3.4. Resource Orchestration
Operating Early Warning Systems generates highly variable demands for computational power which depend on
external and unpredictable factors, for instance weather conditions or sensor data measurements as well as human
actions. The latter may include decisions to alter the monitored area or run simulations. Furthermore, Early Warning
Systems must process data within a certain time frame, especially in emergencies such as dike breaches or ﬂoods.
As a consequence, the Common Information Space should employ a dynamic and powerful infrastructure that can be
adapted to its current requirements. These features are supported by cloud systems combined with a manager who
can adjust the environment to current needs (examples of such systems include [19], [20], [21]). CIS has therefore
been deployed on a private cloud and includes a Dynamic Resource Allocation (DyReAlla) component. DyReAlla is
responsible for:
• provisioning appliances (EWS components) on demand by starting and conﬁguring virtual machines where the
required applications are installed. These, in turn, communicate through Message and Service Buses;
• dynamic allocation of optimal shares of resources to appliances (based on monitoring of appliance load and
response times);
• providing urgent computing capabilities [22] that may involve preempting of resources (stopping or pausing
VMS) and/or acquiring computational power from external providers).
In order to provide the requested functionality DyReAlla needs to cooperate with both internal and external CIS
components. UFoReg is used as a persistent store of metadata describing appliances while PlatIn is a client that
requests the availability of certain appliances. DyReAlla also exposes a REST interface which enables monitoring
its operational status. It employs a cloud management interface, which is considered external to CIS, to retrieve
information about infrastructure (hosts, available appliances, machine load etc.) as well as to apply optimal allocation
by managing virtual machines.
3.5. Self-Monitoring & Robustness
A system is as robust as its weakest link. Given what is at stake, robust operation of the software infrastructure –
CIS – is crucial for early warning system. In view of this, we consider self-monitoring – and ultimately self-healing
– mechanisms an inherent part of the Common Information Space. The system should monitor itself, warn about
possibe problems, and, ideally, take counter measures to prevent technical failures that would make the entire early
warning system inoperable. Self-monitoring must be applied virtually to every component of the EWS software:
sensor connections, appliances, virtual machines hosting appliances, EWS parts, CIS services, application servers
hosting those services, etc. Furthermore, we have recognized several aspects of robustness that require monitoring,
namely availability, health, correctness and performance. Their function and examples are presented in Table 1.
Type of monitoring
Availability monitoring

Purpose
Is the service available?

Health monitoring

Is the service in a healthy condition?
Is the service responding properly?

Examples
Is the VM pingable? Is the WSDL available? Is the
connectable? Heartbeats.
Exception condition. Resource overload (low mem
disk). Health trends: e.g. frequency of exceptions.
Invoke service with test input and check output.

How eﬃcient is the system?

Service throughput. Queue length. Response time.

Functional monitoring
(correctness)
Performance monitoring

Table 1: Types of self-monitoring.

Further analysis of requirements has led us to a conclusion that the self-monitoring system is in fact an early
warning system according to the deﬁnition given in Section 1. Indeed, the four steps can clearly be distinguished:
(1) Monitoring. The Flood EWS is the monitored entity; software sensors deployed in it perform measurements
according to monitoring types summarized in Table 1. (2) Analysis. Examples of analysis are: (i) Calculation of health

102

Bartosz Balis et al. / Procedia Computer Science 4 (2011) 96–105

indicators (e.g. frequency of failure, mean availability); (ii) Performance analysis (e.g. measuring load balance);
(iii) Machine learning (e.g. ﬁnding correlations between system conﬁguration and failures, prediction of runtime
based on input parameters). (3) Judgement. In this step, a decision is made whether there is a health problem in
the system, a load imbalance, or whether a component is behaving unexpectedly. (4) Advice / act. Health alerts are
generated and visualized in a ICT monitoring control center; failed components are automatically restarted; a new
instance is requested in order to balance the load, etc.

Figure 2: Self-monitoring as a supervising Early Warning System.

Consequently, self-monitoring is organized as a Supervising Early Warning System leveraging the same CIS technology as the ﬂood EWS, and taking advantage of all the beneﬁts that come with it. The relationships between the
Flood EWS and the Supervising EWS are depicted in Fig. 2. Note that the DyReAlla component is deployed in
the supervising EWS. Indeed, DyReAlla relies on monitoring of running EWSs, performs performance analysis, and
takes actions regarding resource allocation. DyReAlla also serves requests for resource allocation from new EWSs.
The inevitable question is “what if the supervising EWS fails?”. In order to avoid this vicious circle, we employ
fault-tolerance and self-healing mechanisms in the supervising EWS. First of all, we have designed a robust software
sensor network, capable of detecting failures and self-healing. For this we employ the Erlang technology, known
for its succesful use in highly fault-tolerant systems. Because of space limitations, we cannot provide more details
about the design of the robust sensor network. Ensuring robustness of sensors is crucial: even if high-level analysis
components should fail, we still can obtain meaningful warnings.
4. Case Study: Flood Early Warning System
4.1. The Flood EWS
This section presents an example EWS implemented in the CIS technology: a ﬂood Early Warning System which
monitors selected sections of dikes through sensor networks, detects anomalous dike conditions, and, if found, invokes
simulations, e.g. for prediction and damage assessment due to inundation.
Fig. 3 (left-hand side) presents a basic scenario for this EWS. It also shows the basic applications and external
components which need to be integrated:
1. Artiﬁcial Intelligence (AI); an anomaly detector which obtains raw sensor data from sensors in the dike and
provides an indication of whether the sensors are behaving normally or abnormally.
2. The AnySense Sensor data aggregator, which obtains raw sensor data from sensors in the dike and translates it
into generic XML messages. It also stores the raw data for backup and oﬀers a web-service interface to request
historical data based on structured storage of processed sensor data.

103

Bartosz Balis et al. / Procedia Computer Science 4 (2011) 96–105

3. DSS (Decision Support System), a graphical frontend responsible for visualization. It plots the dike region on
a map and indicates its current status. It also allows users to request historical or current sensor data overviews
and change model parameters or initiate ﬂood impact simulations.
4. HRW Reliable, an expert model which calculates the failure probability for a dike given a set of failure modes
and model parameters. Given the height of the water in front of the dike it will return the probability of breach.
5. HRW Hydrograph, an expert model which, given a prediction of water level distribution over time, computes
the volume of water ﬂowing through a breach in a dike.
6. HRW Dynamic RFSM, an expert model which calculates the ﬂooded area. Given a set of points where a dike
breach has occurred and the amount of water which will ﬂow through each of these points, this model calculates
the depth of inundation over time, after the ﬂood has started.
HRW
Hydrograph

Dike
Sensors

Multi-Touch
Table

Sensor
Cabinet

Simulation
results

AnySense
Simulation
commands

AnySense
Monitoring

AI Anomaly
Detection
Control flow
AI

Abnormal?

DSS
(visualization)

Anomaly
probability

HRW
Hydrograph

HRW
DRFSM

High impact?

UFoReg(CIS
metadata
registry)

Legend:

Action

EWSPart
CIS
Technology

Attention
level
change

Reliable
MonitoringPart

Reliable

Appliance

Flood
Simulation
Part

HRW
DRFSM

Simulation
commands
Attention
level set

Simulation
results

CISMessage Bus

Filtered
sensor data

High risk?

Attention
level
change

Sensor
data
Sensor
data

Data flow

HRW
Reliable

Attention Level
Manager Part

Dike failure
probability

Attention
Filtered
level
sensor data
change

Sensor
data /
Anomaly
probability

Attention
level set /
Dike failure
prob.

Archiver Part

AI-based
MonitoringPart
Storage
(AnySense)

External
component

Figure 3: Flood Early Warning System. Left: basic workﬂow. Right: parts, appliances, and external components of the EWS and their message
exchanges through the CIS message bus.

The workﬂow proceeds as follows. First, sensor data is transferred from the physical dike to the Anysense component. Next, sensor data is passed over to the AI component which indicates the probability of an anomaly. If the
probability exceeds a threshold, further analysis is performed by the HRW Reliable component which computes risk
of a breach in the dike. If the risk is high, two components are invoked to perform simulation of the inundation in
case of the actual breach: HRW Hydrograph and HRW DRFSM. The output from the DRFSM component is passed
to the Decision Support System, a graphical control center deployed on a multi-touch table. The user can observe the
visualization of the ﬂooded areas on the multi-touch table and decide whether further actions are necessary.
4.2. Implementation in CIS
The rightmost part of Fig. 3 presents the Flood EWS from the perspective of its implementation in CIS, focusing
on the EWS parts and their message exchanges through the message bus. Upon startup, each part is passed a unique
correlation ID assigned to this particular instance of the EWS. This allows multiple instances of the same EWS
(possibly assigned to diﬀerent dikes) to run and use the same message bus. There are ﬁve parts in this EWS:
1. AI-based Monitoring. Consumes anomaly probability published by the AI appliance and decides whether the
alert level should be set to 1. If so, it performs two actions: (i) Filters sensor data: subscribes to sensor data
published by AnySense, selects only the part which concerns the dike section where the anomaly was detected,
adds the correlation ID to this data, and publishes it again to the bus; (ii) publishes an ‘Alert level change:1’
message to the bus.1
1 This is a generic mechanism: there is no interpretation of the meaning of a particular alert level (which is done at the user interface level, e.g.
via color-based representation). The only decision which has to be made is which parts produce lower and higher alert levels.

104

Bartosz Balis et al. / Procedia Computer Science 4 (2011) 96–105

2. Reliable Monitoring. Triggered by occurrence of ﬁltered sensor data. It consumes such data and computes the
breach risk by invoking the Reliable appliance. Subsequently it decides whether the alert level should be raised
to 2. If so, it publishes the appropriate message to the bus.
3. Flood Simulation. Serves as a service for running inundation simulation; it accepts simulation commands
which contain water level distribution over time. When such requests arrive with a proper correlation ID, the
appliances are invoked and the simulation results are published as a series of messages to the bus. Typically this
service is invoked on demand by the user interacting with the DSS on the multi-touch table.
4. Attention Level Manager. This part consumes the ‘Alert level change’ messages published by other parts
(AI-based Monitoring and Reliable Monitoring in this case). It decides whether the alert level should actually
be changed and, if so, publishes the ‘Alert level set’ message to the bus. This changes the state of the whole
EWS and may be used by other parts. In the DSS, this message results in an alert message and changing the
color of the appropriate dike section.
5. Archiver. This part consumes certain messages published by other parts for which persistence is required, and
sends them over to a permanent storage service (AnySense serves as one).
4.3. Discussion
The design of the Flood EWS presented in the previous section illustrates several beneﬁts aﬀorded by the looselycoupled-parts model adopted in CIS.
1. Clean modularization of functionality: parts are responsible for single tasks and can therefore be updated independently without aﬀecting other parts. Good examples are parts which encapsulate business rules of the whole
system, for example the Alert Level Manager.2
2. Extensibility: the need for persistent storage came up during the development of the system. Instead of changing
each aﬀected part, a new Archiver part was developed.
3. Technology independence: parts have been implemented both in Apache Camel and OpenESB/BPEL technologies.
4. Enabling software-as-a-service: tools for the inundation simulation are examples of legacy scientiﬁc applications which previously could only have been used on a desktop computer, on one platform, under the control of
a spreadsheet to enter input data. Once wrapped as appliances and exposed as parts, they become independent
software-as-a-service assets which can easily be reused in other systems.
5. Conclusion & Future Work
We have presented the UrbanFlood Common Information Space, a software framework for Early Warning Systems. CIS deﬁnes a reference model and a methodology for building new early warning systems. As a software
framework, it is meant to solve problems common to all early warning systems so that creation of a new EWS is
reduced – as much as possible – to plugging in domain-speciﬁc components and deﬁning early warning scenarios.
CIS has recently had its ﬁrst release and it has been demonstrated in action for a ﬂood early warning system.
We believe that the proposed methods and solutions are applicable not only in the early warning system domain,
but can also contribute to the ﬁeld of environments for system-level science [23]. Let us name a few assets of CIS
that justify this. First, the adoption of SOA and loose coupling enables integration of multiple scientiﬁc applications,
possibly from various domains of science, owned by diﬀerent research teams, and controlled by diﬀerent environments for scientiﬁc applications (e.g. scientiﬁc workﬂow engines). Second, support for sensor data streams can be
generalized into support for data exchange between computer models (implemented by scientiﬁc applications) and
physical models (constructed in laboratories). Third, the integration with a cloud infrastructure is in line with the
increasingly important trend of leveraging clouds for large-scale scientiﬁc computing.
Future work concerns virtually every aspect of the framework, notably dynamic resource allocation (performance
analysis, resource allocation algorithms and the ﬁrst release of DyReAlla), self-monitoring (robust sensor network,
2 In

the future, the use of a business rule engine is planned for encapsulation of thresholds and more complex rules.

Bartosz Balis et al. / Procedia Computer Science 4 (2011) 96–105

105

analysis components), metadata (provenance model). Auxiliary tools for automation of tedious and error-prone tasks
are also very important. These include wrapping of legacy scientiﬁc applications as services, customization of part
templates, data transformations, designing data ﬂows between parts, etc.
Acknowledgments. This research was partially funded by the EU FP7 Project UrbanFlood, grant N 248767. The
authors are grateful to our colleagues – Robert Meijer, Nico Pals, and the whole UrbanFlood team – for many valuable
discussions and suggestions.
References
[1] F. Grasso, Veronica, Early Warning Systems: State-of-Art Analysis and Future Directions, United Nations Environment Programme,
http://na.unep.net/geas/docs/Early Warning System Report.pdf.
[2] Inspire Directive, http://inspire.jrc.ec.europa.eu/.
[3] C. Lautenbacher, The global earth observation system of systems: Science serving society, Space Policy 22 (1) (2006) 8–11.
[4] Shared environmental information system, http://ec.europa.eu/environment/seis/.
[5] Urbanﬂood project home page, http://www.urbanflood.eu.
[6] V. Krzhizhanovskaya, G. Shirshov, N. Melnikova, R. Belleman, F. Rusadi, B. Broekhuijsen, B. Gouldby, J. L’Homme, B. Balis, M. Bubak,
A. Pyayt, I. Mokhov, A. Ozhigin, B. Lang, R. Meijer, Flood early warning system: design, implementation and computational models, in:
Proceedings of the International Conference on Computational Science, ICCS 2011. Procedia Computer Science, Elsevier, 2011, accepted.
[7] N.B.Melnikova, V.V.Krzhizhanovskaya, Virtual Dike: ﬁrst steps towards a multiscale simulation lab for dike stability analysis, in: Proceedings of the International Conference on Computational Science, ICCS 2011. Procedia Computer Science, Elsevier, 2011, submitted to ICCS
2011.
[8] T. Kumar, Srinivasa, Indian tsunami early warning system, Indian National Centre for Ocean Information Services, Hyderabad:
http://csi-sigegov.org/ppts2/IndianTsunamiEarlyWarningSystem.pdf (2009).
[9] T. Kumar, Srinivasa, Implementation of the indian national tsunami early warning system, in: P. Gupta, R. k. Bagga, A. Sridevi (Eds.),
Fostering e-Governance: Selected Compendium of Indian Initiatives, Pune, India, 2009, pp. 380–391.
[10] United states geological surveys earthquake hazards program, http://earthquake.usgs.gov/.
[11] ANSS Technical Integration Committee, Technical Guidelines for the Implementation of The Advanced National Seismic System, U.S.
Department of the Interior, U.S. Geological Survey, 2002.
[12] Global Fire Monitoring Center, http://www.fire.uni-freiburg.de/.
[13] Global Fire Warning Early Warning System, http://www.fire.uni-freiburg.de/gwfews/overview.html.
[14] Modis realtime ﬁre map, http://rapidfire.sci.gsfc.nasa.gov/firemaps.
[15] D. Barseghian, I. Altintas, M. Jones, D. Crawl, N. Potter, J. Gallagher, P. Cornillon, M. Schildhauer, E. Borer, E. Seabloom, Workﬂows and
extensions to the Kepler scientiﬁc workﬂow system to support environmental sensor data access and analysis, Ecological Informatics 5 (1)
(2010) 42–50.
[16] T. Gunarathne, C. Herath, E. Chinthaka, S. Marru, Experience with adapting a WS-BPEL runtime for eScience workﬂows, in: Proceedings
of the 5th Grid Computing Environments Workshop, ACM, 2009, pp. 1–10.
[17] M. Botts, G. Percivall, C. Reed, J. Davidson, OGC r sensor web enablement: Overview and high level architecture, GeoSensor Networks
(2008) 175–190.
[18] T. Gubala, M. Bubak, P. Sloot, Semantic Integration of Collaborative Research Environments, Information Science Reference IGI Global,
2009, Ch. XXVI, pp. 514–530.
[19] M. N. Bennani, D. Menasc´e, Resource Allocation for Autonomic Data Centers using Analytic Performance Models, in: ICAC ’05: Proceedings of the Second International Conference on Automatic Computing, IEEE Computer Society, Washington, DC, USA, 2005, pp. 229–240.
[20] X. Wang, Z. Du, Y. Chen, S. Li, D. Lan, G. Wang, Y. Chen, An autonomic provisioning framework for outsourcing data center based on
virtual appliances, Cluster Computing 11 (3) (2008) 229–245. doi:10.1007/s10586-008-0053-z.
[21] B. Sotomayor, R. S. Montero, I. M. Llorente, I. Foster, Virtual Infrastructure Management in Private and Hybrid Clouds, IEEE Internet
Computing 13 (5) (2009) 14–22.
[22] A. Cencerrado, M. Senar, A. Cort´es, Support for Urgent Computing Based on Resource Virtualization, in: G. Allen, J. Nabrzyski, E. Seidel,
G. D. Albada, J. Dongarra, P. M. A. Sloot (Eds.), Computational Science – ICCS 2009, Vol. 5544, Springer Berlin Heidelberg, Berlin,
Heidelberg, 2009, pp. 227–236.
[23] I. Foster, C. Kesselman, Scaling System-Level Science: Scientiﬁc Exploration and IT Implications, Computer 39 (11) (2006) 31–39.

