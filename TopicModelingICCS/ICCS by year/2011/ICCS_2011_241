Procedia
Computer
Science

Available online at www.sciencedirect.com

Procedia
Computer
(2009) 000–000
Procedia
Computer
ScienceScience
4 (2011)002166–2175

www.elsevier.com/locate/procedia

International Conference on Computational Science, ICCS 2011

I/O-Performance Prediction Method
for Mission-critical Grid-batch Processing
Toshihiko Kashiyamaa, Tomohiro Hanaia, Yoshio Suzukia, and Ken Naonoa *
a

Hitachi, Ltd., Central Research Laboratory, 292, Yoshidacho, Totsuka, Yokohama, Kanagawa, 244-0817, Japan

Abstract
Aiming to solve the performance-degradation problem when multiple computing nodes are in use in mission-critical batch
systems (so-called "grid-batch" systems), a new performance-prediction method that focuses on metadata management for file
input/output (I/O) control and performance degradation in case of concurrent I/O streams is proposed. To enhance the accuracy
of the prediction, this I/O-performance prediction method models metadata management time as a function of number of files and
models performance degradation as a probabilistic function of sequential I/O throughput and random I/O throughput. According
to an evaluation of the proposed method, the difference between actual and estimated execution time is 14.0%. In contrast, as for
the storage/network-based conventional method, the difference is 36.5%. These results demonstrate that the target prediction
error, namely, within 20%, was accomplished with the proposed method, which can therefore be considered effective in
predicting the performance of grid-batch systems.
Keywords: grid-batch; I/O performance prediction; metadata management; concurrent I/O streams; distributed file system

1. Introduction
A lot of batch processing is executed on mission-critical systems [1] such as core-banking systems or productionmanagement systems. In recent years, both MapReduce [2] and GFS (Google File System) [3] which are used for
document analysis and business intelligence have been widely used for distributed batch processing. It is, however,
difficult to apply these technologies to conventional mission-critical systems because modifying source codes of
applications requires a lot of testing. Our research group has developed a decentralized and parallel-execution
control technology – called "grid-batch [4]" – for batch processing in mission-critical systems. Grid-batch
technology has high affinity with conventional batch applications.
So far, it has been necessary to predict the performance of a mission-critical system when it is designed so that
the system can complete the required job processing in the given time. In the case of conventional prediction
methods, the total execution time is calculated by summing up the CPU time required for processing each
input/output (I/O) record when the system bottleneck is the CPU. On the other hand, when the bottleneck is the (I/O),
* Corresponding author. Tel.: +81-45-860-2138; fax: +81-45-860-2113.
E-mail address: {toshihiko.kashiyama.ez, tomohiro.hanai.sk, yoshio.suzuki.rf, ken.naono.aw}@hitachi.com.

1877–0509 © 2011 Published by Elsevier Ltd. Open access under CC BY-NC-ND license.
Selection and/or peer-review under responsibility of Prof. Mitsuhisa Sato and Prof. Satoshi Matsuoka
doi:10.1016/j.procs.2011.04.237

540.indd 2166

5/3/11 1:53:49 PM

2167

Toshihiko
Kashiyama
al. / Procedia
Computer
Science
4 (2011)
2166–2175
Author
name /etProcedia
Computer
Science
00 (2011)
000–000

the execution time is determined by combining the time calculated from sequential read/write throughput and the
time calculated from the bandwidth of the networks (NWs) or of the fibre channels (FCs), because the processing is
mainly composed of sequential I/O streams with one record after another. The bottleneck of conventional systems is
often the CPU, while that of recent systems has shifted from the CPU to the I/O, because today’s multi-cores or
many-cores CPUs technologies increase the number of processes per node, because grid-batch technologies, which
are increasingly used for real business systems, increase the number of nodes per system, and because the
comparative number of I/O loads against CPU loads has become enormous. It is therefore necessary to predict I/O
performance with much more accurately.
When many nodes process jobs on a grid-batch system, the new overheads incurred by managing the control of
simultaneous file I/O accesses (hereafter, metadata management) should be evaluated, because the system is
required to keep consistency among simultaneous accesses from multiple nodes, and the processing overhead is thus
high. Especially when a file in a grid-batch system is split into processes on multiple nodes, the number of files
increases, leading to higher overheads for access-consistency management. Moreover, it is depicted in the literature
[5, 6] that highly concurrent sequential I/O decreases the sequential read/write throughput. It is therefore difficult to
predict the total performance of the grid-batch system only by using information concerning the sequential
read/write throughputs of the disk array or information concerning the bandwidth of NWs and FCs.
In light of the above-described difficulty, the authors have developed a highly accurate method for predicting the
I/O performance (hereafter, I/O-performance-prediction method). This method focuses on the overheads incurred by
the metadata management of inter-node file consistency and on the performance degradations in the highly
concurrent I/O streams. The target accuracy of the prediction is set so that the prediction error is within 20%, which
is also used in the prediction criterion in the case of a conventional single-batch system.
The rest of this paper is organized as follows. Section 2 presents the problems and the challenges concerning
prediction of grid-batch I/O performance. Section 3 discusses a new performance-prediction method that specifies
the metadata management for file I/O control and the performance degradation in the case of concurrent I/O streams.
In Section 4, to confirm the efficiency of the proposed method, comparative evaluations of the predictions of both
the conventional method and the proposed method against the actual performances are made by composing three
application programs for issuing file I/O requests. Section 5 concludes the paper.
2. Problems and challenges concerning prediction of grid-batch I/O performance
2.1. Outline of grid-batch systems
Figure 1 describes an outline of a conventional mission-critical batch system, a distributed batch system and a
grid-batch system. In the case of a conventional batch system, batch jobs are managed by a job scheduler, and an
application (AP), which issues file-I/O requests to a disk array, is executed by a batch-job execution-control module.
In the case of distributed batch system, such as MapReduce [2] and GFS [3], applications are executed on multiple
Conventional batch system

Distributed batch system
Scheduler
node

Mainframe
Job scheduler

File system

Input file

File system

Output file

Job scheduler
Grid-job scheduler

Computing
nodes

AP
Job execution control

HDD

Disk array

Job scheduler

Computing
nodes

AP
Job execution control

Grid-batch system
Scheduler
node

AP
Job execution control

File system
HDD

AP

New I/F

(map/reduce
Job execution control
function)

File system
HDD

AP

AP
Job execution control

Job execution control

File system

File system

Storage
nodes

File System

Disk
array

Split
Input file

AP

Conventional

Job execution control I/F

File system

File System

Merge
Output file

Fig. 1. Outline of conventional batch system, distributed batch system, and the grid-batch system

540.indd 2167

5/3/11 1:53:50 PM

2168 	

Toshihiko Kashiyama et al. / Procedia Computer Science 4 (2011) 2166–2175
Author name / Procedia Computer Science 00 (2011) 000–000

computing nodes. Scheduling methods that focus on data locality in server clusters have been proposed [7, 8]. In the
commercial enterprises field, job schedulers and file systems for distributed batch processing are provided [9, 10,
11]. It is, however, difficult to apply these technologies to conventional mission-critical systems because modifying
source codes of applications to fit new interfaces requires a lot of testing.
On the other hand, applications of the grid-batch system use a conventional interface, so the amount of
application modification is small [4]. The grid job scheduler splits a batch job into fine-grained jobs, which it
allocates to multiple computing nodes. On the computing nodes, applications are executed by batch-job-execution
control modules on multiple processes. File I/O requests are issued from applications to the storage nodes. The
storage nodes then access the disk array. The input files are also split and allocated in each process on the computing
node, and the output files executed in each process are merged.
2.2. Conventional method for predicting I/O performance
With conventional prediction methods, when the bottleneck is the I/O, the total execution time is determined by
combining the time calculated from sequential read/write throughput and the time calculated from bandwidth of the
NWs or the FCs. The total execution time is determined according to the throughput stated in storage guidelines
such as those given in the literature [12]. The sequential read/write throughput varies with the number of disks and
the cache size in the disk array. If the sequential read/write throughput exceeds the bandwidth of an I/O path such as
Ethernet cable (for example, 1 Gbps = 125 MB/s) or FC cable (for example, 4 Gbps = 500 MB/s), the actual
throughput cannot exceed the bandwidth of the I/O path. Under this limitation, input file size is given as FI, output
file size is given as FO, sequential read throughput is given as TSR, sequential write throughput is given as TSW, and
bandwidth of I/O path is given as BP. I/O execution time in the conventional prediction method, tC, is thus given as
tC = FI / min(TSR , BP) + FO / min(TSW , BP).

(1)

In the case of the grid-batch system, as shown in Fig. 1, the input file is split and the output files are merged. In
this case, input file size per process is given as FIP, output file size per process is given as FOP, number of computing
nodes is given as NN, number of processes per node is given as NP, and number of input/output files per process is
given as NFP. Total file size, FI + FO, and total number of files, NF, are thus given as follows.
FI + FO = (FIP + FOP) NN NP
NF = NFP NN NP

(2)
(3)

In the case of grid-batch systems, to maintain consistency among simultaneous accesses to the same file from
multiple computing nodes, file-access tokens are transmitted between computing nodes and storage nodes. For
example, in the case of simultaneous writes from multiple computing nodes, a file must not be damaged, and only
updated files are read at any time. Metadata management for mission critical systems should therefore be stricter
than that for other systems. Consequently, according to equation (2), if NN increases, or if NP increases, FIP or FOP
decreases, and according to equation (3), NF increases. As a result, the proportion of metadata-management time to
total execution time increases because of the stricter metadata management.
Moreover, when highly concurrent sequential I/O streams access the disk array, the total throughput of the disk
array is reduced because of the disk-head-seek overhead [5, 6]. The conventional I/O-performance prediction
method based on equation (1) therefore has problems with inaccuracy because of the metadata management and of
the performance degradation in the case of concurrent I/O streams.
In recent years, a lot of benchmarks for the performance design and prediction in storage systems and file
systems have been proposed [13]. In regard to network-attached storage (NAS), a benchmark called SPEC SFS is
defined in [14]. SPEC SFS outputs the benchmark index calculated from the time for metadata management such
file-creation time and read/write time. Moreover, business-application benchmarks like TPC-C and TPC-H for
database systems have been proposed [15]. These benchmarks are used in the performance design for business
application systems. Storage guidelines such as those cited in [12] provide only sequential read/write throughput

540.indd 2168

5/3/11 1:53:50 PM

Toshihiko Kashiyama et al. / Procedia Computer Science 4 (2011) 2166–2175
Author name / Procedia Computer Science 00 (2011) 000–000

2169

(MB/s), random read/write performance (IOPS: I/O per second). In addition to providing the above-mentioned
throughput and performance, NAS-performance guidelines provide only the benchmark index SPEC SFS. However,
these benchmarks cannot solve the above-described problems concerning grid-batch systems. A benchmark for
designing mission-critical grid-batch systems has not yet been proposed.
2.3. Challenges of I/O performance prediction for grid-batch system
To solve the previously mentioned problems concerning the I/O prediction method used in the grid-batch system,
the two challenges regarding improved prediction accuracy listed in Table 1 were addressed. The first was to
propose a file-I/O performance-prediction method that focuses on metadata management time. The second was to
propose a file-I/O performance-prediction method that focuses on performance degradation in the case of highly
concurrent I/O streams.
Table 1. Problems and challenges concerning I/O performance prediction for the grid-batch systems
Problems

Challenges

Metadata management time increases in the case of gridbatch systems.

Propose a file-I/O performance prediction method that focuses on
metadata management time.

When highly concurrent sequential I/O streams access disk
array, total throughput of the disk array is reduced.

Propose a file-I/O performance prediction method that focuses on
performance degradation in the case of highly concurrent I/O streams.

3. Method for predicting I/O performance
3.1. Approaches to problems
Table 2 lists the approaches taken to solve the problems listed in Table 1. First, the metadata management time is
modeled as a linear function of the number of files. Second, the performance degradation is modeled as a
probabilistic function of sequential I/O throughput and random I/O throughput. This function is determined by the
number of nodes and the number of processes per node.
Table 2. Challenges and approaches concerning I/O-performance prediction for the grid-batch systems
Challenges

Approaches

Propose a file-I/O performance prediction method that focuses on
metadata management time.

Model metadata management time as a linear function of
number of files.

Propose a file-I/O performance prediction method that focuses on
performance degradation in the case of highly concurrent I/O streams.

Model performance degradation as a probabilistic function
of sequential I/O throughput and random I/O throughput.

3.2. Proposed I/O performance prediction method that focuses on metadata management time
As for the first approach in Table 2, the processing patterns under which metadata management time increases
are extracted from all the processing patterns of batch applications. The batch processing basically repeatedly reads
records from input files and writes records to output files. In the case of mission-critical systems, applications are
often written in COBOL, and such COBOL-written batch applications are classified into 24 processing patterns
determined by the number of I/O files and categories [16]. They include record extraction, record aggregation, file
split, file merge, independent check, related check, duplication check, sequential file update, random file update, and
report creation. If some of the above-mentioned processing patterns include multiple I/O functional elements (such
as "file-create" and "file-open") that require metadata management, the ratio of the metadata management time to
the total execution time rises. As a result, the file-split pattern includes multiple file-create executions, and the filemerge pattern includes multiple file-open executions. When application patterns of a mission-critical batch jobs
were investigated, the record-extraction pattern including sort processing accounts for the greatest proportion of all

540.indd 2169

5/3/11 1:53:51 PM

2170 	

Toshihiko Kashiyama et al. / Procedia Computer Science 4 (2011) 2166–2175
Author name / Procedia Computer Science 00 (2011) 000–000

patterns. Especially in the case of the grid-batch systems, the number of the file-split-pattern jobs and file-mergepattern jobs increases because the input file is split and the output files are merged. As a result, it is assumed that in
addition to the record-extraction pattern as a basic pattern (called a "simple pattern" hereafter), the target processing
patterns are the split pattern and the merge pattern.
Table 3 lists the details of the I/O requests for the three target processing patterns. The split pattern requests one
sequential read and N sequential writes and has two parameters, namely, number of split files and split-algorithm
type (round-robin split and sequential split). Similarly, the merge pattern requests N sequential reads and one
sequential write, and it has two corresponding parameters.
Table 3. Details of I/O requests for three target processing patterns
Processing
pattern

Details of I/O requests in three processing patterns

Input stream
(I/O elements)

Output stream
(I/O elements)

Other
parameter

Simple

Read one record from an input file and write one
record to an output file. Output file size is specified by
the ratio of input to output file sizes.

1 Sequential stream
(1 file-open
execution)

1 Sequential stream
(1 file-create
execution)

Ratio of input
to output file
sizes

Split

One input file is split to N output files. The roundrobin split switches the output file every record; is the
sequential split switches the output file after [the total
number of records / N] records.

1 Sequential stream
(1 file-open
execution)

N Sequential streams
(N file-create
executions)

Number of
split files

Merge

N input files are merged into one output file. The
round-robin merge algorithm switches the input file
every record; the sequential-merge algorithm switches
the input file after [total number of records/N] records.

N Sequential streams
(N file-open
executions)

1 Sequential stream
(1 file-create
execution)

Number of
merge files

A new I/O performance prediction method for the split pattern and the merge pattern is proposed in the following
section. The split pattern includes N file-create executions, which account for the majority of the metadatamanagement time, which can therefore be approximated by the product of a so-called "file-create metadatamanagement constant" and number of split files. Under this approximation, this file-create metadata management
constant is given as CC, and number of split files is given as NSF. Split I/O time tS is therefore given as
tS = FI / min(TSR , BP) + FO / min(TSW , BP) + CC NSF.

(4)

In regard to the merge pattern, the merge pattern includes N file-open executions, which account for the majority
of metadata-management time. In a similar manner to that explained above, the file-open metadata-management
constant is given as CO, and number of merged files is given as NMF. Merge I/O time tM is thus given as
tM = FI / min(TSR , BP) + FO / min(TSW , BP) + CO NMF.

(5)

Throughput for the split pattern or the merge pattern is calculated by the equations (FI + FO) / tS or (FI + FO) / tM.
3.3. Proposed I/O performance prediction method that focuses on performance degradation in case of highly
concurrent I/O streams
The processing performance is expected to improve proportionally with the number of processes per node up to a
certain number of CPU cores if the system bottleneck is the CPU. On the other hand, the performance is degraded in
the case of highly concurrent I/O streams if the system bottleneck is the disk array or I/O path (NW, FC, etc.).
Especially in the case of a disk array bottleneck, the total I/O throughput of multiple processes approaches the
random read/write throughput (where the I/O size of random read/write equals application buffer size) because the
probability that the following request accesses consecutive HDD blocks is lowered by concurrent I/O streams. The
performance degradation is therefore modeled as a probabilistic function of sequential I/O throughput and random

540.indd 2170

5/3/11 1:53:51 PM

2171

Toshihiko Kashiyama et al. / Procedia Computer Science 4 (2011) 2166–2175
Author name / Procedia Computer Science 00 (2011) 000–000

I/O throughput. This function is determined by NP. In this case, random read throughput is given as TRR, and random
write throughput is given as TRW. Random I/O throughput of one process for the simple pattern, TRP, and total
throughput of multiple processes for the simple pattern, TMP, are thus given as follows.
TRP = (FI + FO ) / {FI / min(TRR , BP) + FO / min(TRW , BP)}
TMP = (sequential I/O throughput) (1 / NP) + (random I/O throughput) (1 - 1 / NP )
= [ (FI + FO ) / {FI / min(TSR , BP) + FO / min(TSW , BP) } ] (1 / NP)
+ [(FI + FO ) / {FI / min(TRR , BP) + FO / min(TRW , BP) } ] (1 - 1 / NP )

(6)
(7)

When the number of I/O paths in multiple nodes increases, the bottleneck of the I/O path in the computing nodes
tends to be canceled; therefore the disk array or the I/O path of the storage nodes tends to be the bottleneck.
Accordingly, bandwidth of the I/O path per computing node is given as BCP, bandwidth of the I/O path per storage
node is given as BSP, number of computing nodes is given as NCN, and number of the storage nodes is given as NCN.
As a result, function min (TSR, BP) is replaced by function min (TSR, BCP NCN, BSP NSN) in equations (1), (4), and (5).
TSW, TRR, and TRW are replaced similarly. Total throughput of multiple nodes for the simple pattern, TMN, is therefore
calculated as
TMN = (sequential I/O throughput) (1 / NCN NP) + (random I/O throughput) (1 - 1 / NCN NP )
= [ (FI + FO ) / {FI / min(TSR , BCP NCN , BSP NSN )
+ FO / min(TSW , BCP NCN , BSP NSN ) } ] (1 / NCN NP)
+ [ (FI + FO ) / {FI / min(TRR , BCP NCN , BSP NSN )
+ FO / min(TRW , BCP NCN , BSP NSN ) } ] (1 – 1 / NCN NP ).

(8)

From equation (4) and (8), total throughput of multiple nodes for the split pattern, TMNS, is calculated as
TMNS = [ (FI + FO ) / {FI / min(TSR , BCP NCN , BSP NSN )
+ FO / min(TSW , BCP NCN , BSP NSN ) + CC NSF } ] (1 / NCN NP)
+ [ (FI + FO ) / {FI / min(TRR , BCP NCN , BSP NSN )
+ FO / min(TRW , BCP NCN , BSP NSN ) } + CC NSF } ] (1 - 1 / NCN NP ).

(9)

From equation (5) and (8), total throughput of multiple nodes for the merge pattern is calculated in a similar
manner to that for the split pattern.
4. Experimental evaluation
4.1. Evaluation method and evaluation environment
The evaluation benchmark consists of three processing patterns, shown in Table 5, and the order of read/write in
the three processing patterns is described in Fig. 2. The split algorithm is a sequential-split type, and the merge
algorithm is a sequential-merge type. The input/output interface is implemented by using the C standard library
(fread, fwrite, fseek, setvbuf, etc.).
The accuracy of I/O performance prediction is measured as follows. First, the conventional sequential read
throughput and sequential write throughput are calculated from the measured execution time for the simple pattern.
Second, CC is obtained from measured execution time for the split pattern. Similarly, CO is obtained from the
corresponding merge-pattern measurements. Finally, the difference between actual and estimated performance is
calculated. Similarly, the difference is also calculated in the case of concurrent I/O streams.
The I/O performance was measured under following conditions:
 Input file size is 2 GB, record size is 1 KB, and application buffer size is 2 MB.
 To evaluate the processing patterns, the execution time for one node and one process only is measured.

540.indd 2171

5/3/11 1:53:51 PM

2172 	

Toshihiko Kashiyama et al. / Procedia Computer Science 4 (2011) 2166–2175
Author name / Procedia Computer Science 00 (2011) 000–000
Merge

Split

Simple
Input File
(1)Read R1

Output file
(2)Write R1

(3)Read R2

(4)Write R2

:

:

:

:

:

:

Input file

Output file 1
(2)Write R1

Input file 1
(1)Read R1

(4)Write R2

(3)Read R2

:

:

(1)Read R1
(3)Read R2

:

:

:

Output file 2
(12)Write R6

Input file 2
(11)Read R6

(14)Write R7

(13)Read R7

:

:

:

:

(11)Read R6
(13)Read R7

Output file
(2)Write R1
(4)Write R2
:

(12)Write R6
(14)Write R7

Fig. 2. Order of read/write in three processing patterns

 In the evaluation of multiple processes and multiple nodes, file size is 2GB per a node, file size of each process is
2 GB/NP, ratio of input to output file sizes is one, number of split files is 500, and number of merge files is 500.
 Between the I/O-performance measurements, umount and mount commands are executed to clear the OS file
cache and terminate and start the file system (in the case of the distributed file systems).
 For each of the parameters above, I/O performance is measured three times, and the average time is calculated.
Four computing nodes, one storage node, and one disk array were used for the above-described measurements.
Each server mounts two CPUs (Intel® Xeon® E5405 2.00GHz, four cores) and an 8GB ECC DDR2 677 FB-DIMM,
and use Red Hat Enterprise Linux 5.4. The disk array is composed of a 1-GB storage cache and six 15,000-rpm FC
disks with 300-GB capacity. The connection between the servers and the NW switch is one 1-Gb Ethernet cable
(MTU = 9000), the connection between the servers and the FC switch is one 4-Gbps FC cable, and the connection
between the disk array and the FC switch is two 2-Gbps FC cables. To eliminate influence of I/O performance on
the file system, three file systems were used: ext3, NFS (Network File System), and our distributed file system,
Hitachi Striping File System (HSFS), which is designed for mission-critical systems. NFS block size is 32KB (max
value on Linux). Three LUs (logical units) were created for each file system in a 5D + 1P RAID (redundant arrays
of inexpensive disks) group.
4.2. Results of processing pattern measurement
Figure 3 shows the measured execution times for the three processing patterns. Figure 3(a) plots the ratio of input
to output file sizes versus execution time in seconds for the simple pattern. The execution time for each file system
is approximated as a linear function of the ratio. Every file system uses the same RAID group, so the same execution
time is supposed to be achieved in theory. However, the execution time of NFS is larger than of the other two file
systems. The resource utilization of the NFS server for CPU, NW, and I/O acquired by the Linux sar command
exhibits a long I/O wait. It is supposed that the prefetch by the NFS server is not effective, so the NFS cannot obtain
the maximum throughput of the disk array in the case of one process (as discussed again in section 4.3).
(a) Simple pattern

(b) Split pattern

160

140

120

120

80

60
40
20

ext3

0
0.00

0.50

1.00

NFS
1.50

Ratio of input and output file size

HSFS
2.00

Execution time [s]

140

120
100

100
80
60
40
20

ext3

0
0

200

400

NFS
600

Number of files

(c) Merge pattern

160

140

Execution time [s]

Execution time [s]

160

HSFS
800

1000

100
80
60
40
20

ext3

0

0

200

400

600

Number of files

NFS
800

HSFS
1000

Fig. 3. Execution time for three processing patterns in the case of the grid-batch application: (a) ratio of input to output file sizes versus execution
time in the case of simple pattern; (b) and (c) number of files versus execution time in the case of split and merge patterns

540.indd 2172

5/3/11 1:53:51 PM

Toshihiko Kashiyama et al. / Procedia Computer Science 4 (2011) 2166–2175
Author name / Procedia Computer Science 00 (2011) 000–000

2173

The execution time of the proposed method is estimated as follows. First, the sequential read throughput for the
simple pattern is calculated from the execution time in the case that the ratio of input to output file sizes is zero. The
sequential write throughput is also obtained from the difference between the execution time when the ratio is zero to
that when the ratio is one. Table 4 lists the sequential read throughput and the sequential write throughput of each
file system (1 GB is transformed to 1024 MB).
Table 4. Sequential read throughput and random write throughput calculated by the conventional I/O performance prediction method
File system

Sequential read throughput

Sequential write throughput

Ext3

2,048MB / 17.9s = 114.4MB/s

2,048MB / (56.2s – 17.9s) = 53.5MB/s

NFS

2,048MB / 77.0s = 26.6MB/s

2,048MB / (108.9s – 77.0s) = 64.2MB/s

HSFS

2,048MB / 21.5s = 95.3MB/s

2,048MB / (56.8s – 21.5s) = 58.0MB/s

Second, the I/O throughputs for the split pattern and the merge pattern were determined. Figures 3(b) and (c)
show the number of files versus execution time in seconds. The execution time for ext3 stays constant with
increasing number of files because each node does not exchange metadata with other nodes. On the other hand, the
execution time for NFS and HSFS increases as the number of files increases. The increase rate for HSFS is larger
than that for NFS because the metadata management of HSFS is stricter than that of NFS. CC is calculated as 0.044
by a linear approximation from the split-pattern execution times for HSFS. Similarly, CO is obtained as 0.069.
Finally, Fig. 4 shows estimated execution times for HSFS by the conventional and proposed methods in the case
of the split pattern and merge pattern when the number of files is 500. As for the proposed method, the difference
between actual and estimated execution time is 3.8% in the split-pattern case and 8.0% in the merge-pattern case. In
contrast, as for the conventional method, the respective differences are 25.1% and 42.7%. These results demonstrate
that the target prediction error, namely, within 20% was accomplished with the proposed method.
120
-42.7%

Execution time [s]

100
80

-25.1%

3.8%

60

Actual value
Conventional method

40

Proposed method

20
0

-8.0%

Split
Merge
Processing pattern

Fig. 4. Difference between actual and estimated execution time of HSFS (number of split and merge files is 500)

4.3. Results of concurrent I/O stream measurement
Figure 5 shows the number of concurrent I/O streams versus the measured throughput (MB/s) in the case of the
three processing patterns. The combination "(4,8)" for example, means that the number of nodes is four and the
number of processes is eight. Generally, the throughput decreases as the number of processes increases or as the
number of nodes increases, except in the case of the merge pattern for HSFS. Especially, the performance
degradation of NFS is remarkable.
Note that the throughput of NFS in the case of the simple pattern is improved when the combination is (1,2)
compared to when it is (1,1). As mentioned in Section 4.2, it was supposed that the prefetch of the NFS server is not
effective and the NFS server cannot achieve maximum throughput of the disk array when the combination is (1,1). If
the polynomial approximation of the throughput (excluding the throughput for combination (1,1)) has a degree of –1,

540.indd 2173

5/3/11 1:53:52 PM

Toshihiko Kashiyama et al. / Procedia Computer Science 4 (2011) 2166–2175
Author name / Procedia Computer Science 00 (2011) 000–000

(a) Simple pattern

90
80
70
60
50
40
30
20
10
0

(b) Split pattern

90
80

ext3
(1,1)

(1,2)

(1,4)

HSFS
(1,8)

NFS

(2,8)

(4,8)

70

Throughput [MB/s]

Throughput [MB/s]

Throughput [MB/s]

2174 	

60
50
40
30
20
10

ext3

0

(1,1)

number of I/O streams [node, process]

(1,2)

(1,4)

(1,8)

HSFS
(2,8)

NFS
(4,8)

number of I/O streams [node, process]

(c) Merge pattern

90
80
70
60
50
40
30
20
10
0

ext3
(1,1)

(1,2)

(1,4)

HSFS
(1,8)

(2,8)

NFS
(4,8)

Number of I/O streams [node, process]

Fig. 5. Number of concurrent streams versus throughput for the three processing patterns of the grid-batch application

y = 38.6/x + 36.6 is obtained. If x=1 is substituted into this approximation, the throughput becomes 75.2 MB/s and
approaches that of ext3 or HSFS. In the case of the simple pattern, according to equation (8), sequential throughput
is 75.2 MB/s, and random throughput is 36.6 MB/s.
The execution time of the proposed method was estimated in the split-pattern case for HSFS and NFS with the
(4,8) combination. First, the execution time of HSFS in the split-pattern case is calculated as 78.8 seconds from
Table 4 and equation (4). Second, the sequential throughput is calculated as (2,048 + 2,048)/78.8 = 52.0 MB/s. Third,
if a polynomial approximation with degree of –1 is calculated, the coefficient of –1 degree is 12.1. Fourth, equation
y = 12.1/x + 39.9 is acquired because y = 52.0 when x = 1. Fifth, the throughput is calculated as 12.1/(4*8) + 39.9 =
40.2 MB/s. Finally, the execution time is calculated as (2,048 + 2,048)*4/39.1 = 407.6 seconds.
As for NFS, the throughput of 75.2 MB/s acquired in the simple-pattern case is regarded as the sequential I/O
throughput, and Cc is calculated as 0.024. The throughput in the split-pattern case is calculated as 61.6 MB/s from
equation (4). If a polynomial approximation with degree of –1 is calculated from the measured throughputs (except
that for combination (1,1)), the coefficient of –1 degree is 38.3. Equation y = 38.3/x + 23.3 is thereby obtained.
Finally, the execution time is calculated as 668.7 seconds.
Figure 6 shows estimated execution times for the conventional method and the proposed method. As for the
proposed method, the difference between actual and estimated execution time is 14.0% in HSFS case and 14.0% in
NFS case. In contrast, as for the conventional method, the respective differences are 36.5% and 40.8%. These results
demonstrate that the target prediction error, namely, within 20%, was accomplished with the proposed method in the
split-pattern case for HSFS and NFS.
On the other hand, as shown in Fig. 5 (c), the throughput of HSFS increases as number of nodes increases or as
number of processes increases. As shown in Section 3.2, the execution time is increased by the metadata
management in the case of the split pattern and merge pattern. In the case of multiple nodes, one node can request
I/O simultaneously when another node executes the metadata management. Total throughput thus improves as a
whole. I/O performance prediction method based on above study is a future work.
900

-40.8%

Execution time [s]

800

-14.0%

700
600

500
400

-36.5%

14.0%

300

Actual value
Conventional method

200

Proposed method

100
0

HSFS

File system

NFS

Fig. 6. Difference between actual and estimated execution time in the split pattern case (32 concurrent streams, and 500 split files)

540.indd 2174

5/3/11 1:53:52 PM

Toshihiko Kashiyama et al. / Procedia Computer Science 4 (2011) 2166–2175
Author name / Procedia Computer Science 00 (2011) 000–000

2175

5. Concluding remarks
Aiming to solve the performance-degradation problem when multiple computing nodes are in use in missioncritical batch systems (so-called "grid-batch" systems), a new performance-prediction method that focuses on
metadata management for file I/O control and performance degradation in case of concurrent I/O streams is
proposed. To enhance the accuracy of the prediction, this I/O-performance prediction method models metadata
management time as a function of number of files and models performance degradation as a probabilistic function of
sequential I/O throughput and random I/O throughput. The latter function is determined by number of computing
nodes and number of processes in the nodes. According to an evaluation of the proposed method, the difference
between actual and estimated execution time is 14.0%. In contrast, as for the storage/network-based conventional
method, the difference is 36.5%. These results demonstrate that the target prediction error, namely, within 20%, was
accomplished with the proposed method, which can therefore be considered effective in predicting the performance
of grid-batch systems.
As for future works, first, the I/O performance prediction method will be adapted to I/O control. For example, for
automatic tuning of the optimal number of execution processes, this parameter will be determined from the number
of computing nodes and the I/O performance of the disk array. Second, as discussed in Section 4.3, more accurate
I/O performance prediction method will be required. Finally, in applying the proposed method, it is necessary to
analyze processing patterns such as split or merge and the number of input/output files; therefore, a technology that
automatically extracts the processing pattern and the number of input/output files from source codes such as JCL
(which is a job-control language on a mainframe) will be developed.
References
1. K. Fowler, Mission-Critical and Safety Critical Development, IEEE Instrumentation & Measurement Magazine, vol. 7, no. 4, pp. 52-59,
2004.
2. J. Dean, et al., MapReduce: Simplified Data Processing on Large Clusters, Commun. ACM Vol. 51, No. 1, pp. 107-113, 2008.
3. S. Ghemawat, et al., The Google file system, Proc. SOSP '03, pp. 29-43, 2003.
4. M. Hosouchi, et al., Job Processing Method, Computer-readable Recording Medium Having Stored Job Processing Program and Job
Processing System, US2010/0251248, 2010.
5. G. Panagiotakis, et al., Reducing Disk I/O Performance Sensitivity for Large Numbers of Sequential Streams, ICDCS2009, pp. 22-31, 2009.
6. C. Li, et al., Competitive Prefetching for Concurrent Sequential I/O, EuroSys07, pp. 189-202, 2007.
7. M. Isard, et al., Quincy: Fair Scheduling for Distributed Computing Clusters, SOSP 2009, 2009.
8. M. Zaharia, et al., Delay Scheduling: a Simple Technique for Achieving Locality and Fairness in Cluster Scheduling, EuroSys10, pp. 265278, 2010.
9. S. Antani, Batch Processing with WebSphere Compute Grid: Delivering Business Value to the Enterprise, IBM Redbooks, 2010,
http://www.redbooks.ibm.com/redpapers/pdfs/redp4566.pdf.
10. F. Schmuck, et al., GPFS: A Shared-Disk File System for Large Computing Clusters, Proc. FAST 2002, 2002.
11. S. Lang, et al., I/O Performance Challenges at Leadership Scale, Proc. SC09, 2009.
12. S. Racherla, et al., IBM Midrange System Storage Implementation and Best Practices Guide, IBM Redbooks, 2010,
http://www.redbooks.ibm.com/redbooks/pdfs/sg246363.pdf.
13. A. Trayger, et al., A Nine Year Study of File System and Storage Benchmarking, ACM Transactions on Storage, Vol. 4, No. 2, pp. 1-56,
2008.
14. SPEC SFS, http://www.spec.org/osg/sfs/.
15. TPC: Transaction Processing Performance Council, http://www.tpc.org/.
16. O. Ohno, et al., Automated Software Development Based on Composition of Categorized Reusable Components : Construction and
Sufficiency of Skeletons for Batch Programs, IEICE Trans. D Vol. J83-D-I, No. 10, pp. 1055-1069, 2000 (in Japanese).

540.indd 2175

5/3/11 1:53:52 PM

