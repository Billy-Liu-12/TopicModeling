Procedia
Computer
Science

Available online at www.sciencedirect.com

ProcediaComputer
Computer Science
Science 00
(2009) 000‚Äì000
Procedia
1 (2012)
1387‚Äì1396

www.elsevier.com/locate/procedia
www.elsevier.com/locate/procedia

International Conference on Computational Science, ICCS 2010

SWOOP: An adjustable global optimization technique
H.I. Sayeda*, A.M. Fadelb, S.A. Mouradc
a,b

Housing & Building National Research Center, 87 Tahrir St., Dokki, Giza 11511, Egypt
c
Faculty of Engineering, Cairo University, Giza 12613, Egypt

Abstract
Optimization is the process of obtaining the best result under given circumstances and in its broadest sense; optimization can be
applied to solve any engineering problem. In many practical applications, the objective function contains, besides the global
minimum, several local minima. Implementing local optimization algorithms to functions that contain multiple local minima
within the search space results in problem instability. Unfortunately, the distinction between the concepts of local and global
optimization is frequently overlooked by researchers although a huge amount of work has been done in this field due to the
cumbersome of implementing a global optimization technique. A recent technique for global optimization was developed under
the name of Coupled Local Minimizers (CLM) in which the local minimizers exchange information with each other resulting into
a cooperative search mechanism. This technique was comprehensively discussed in the realm of finite element model updating
and many challenges were illustrated that hinder the practical implementation of the technique. In this paper, the authors present
a modified version of the CLM technique under the name of SWOOP in some of the drawbacks the CLM are mitigated, and then
added a perusal part to the technique by plotting a synthesized response surface of the function to guide the optimization process.
The proposed SWOOP technique is tested for various function types and its superior performance was validated.
c 2012 Published by Elsevier Ltd. Open access under CC BY-NC-ND license.
‚Éù
Keywords: Global Optimization; Cooperative search;Response surface synthesis; minimizer validation

1. Introduction
Optimization is the process of obtaining the best result under given circumstances and in its broadest sense;
optimization can be applied to solve any engineering problem [1]. The general optimization problem is stated as:

minn

x¬è ¬É

f ( x)

subject to the constraint s

¬≠ h j ( x ) 0,
¬Æ h ( x ) d 0,
¬Ø j

where x is the vector of design variables ( x ¬è
vector function containing the constraints (h(x) :

j¬èH

(1)

j¬è¬Ç

n

n

), f(x) is the objective function (f(x) : ¬É o ¬É ) and h(x) is a
H
n
l
¬É
o ¬É ). There may be equality (index set ) and inequality

¬É

*
Corresponding author. Tel.: +2-02-3761-7057; fax: +2-02-3335-1564.
E-mail address: hh_consult@yahoo.com.

c 2012 Published by Elsevier Ltd. Open access under CC BY-NC-ND license.
1877-0509 ‚Éù
doi:10.1016/j.procs.2010.04.154

1388

H.I. Sayed et al. / Procedia Computer Science 1 (2012) 1387‚Äì1396
Author name / Procedia Computer Science 00 (2010) 000‚Äì000

(index set ¬Ç ) constraints. The global minimizer x* of f is defined as the point where the function attains its least
n
value, i.e. where f(x*) ¬î f(x) for all x ¬è ¬É (or at least in the domain of interest).
In many practical applications, especially when dealing with minimization of an ‚Äúerror‚Äù function in system
identification problems, the global minimum is preferable and may be essential to get, but it is usually difficult to
identify. Local minima always reveal scrambled values for the design parameters that are the main goal of solving
this kind of problems. Adding to this, it may be difficult to know in advance if the objective function contains
besides the global minimum also local minima, since this requires the knowledge of the overall shape of f. It is
therefore not certain whether the obtained minimum is the desired global one.
Unfortunately, the distinction between the concepts of local and global optimization is frequently overlooked by
researchers although a huge amount of work has been done in this field due to the cumbersome of implementing a
global optimization technique.
Implementing local optimization algorithms to functions that contain multiple local minima within the search
space results in problem instability, which when encountered, can thus be considered as an indicator of the need for
a global optimization technique.
2. Instability of the local optimization problem:
The optimization problem can be visualized as a system that has an input ‚Äúthe initial design point‚Äù and output
‚Äúthe optimizer‚Äù. For any system, or any mathematical model, the condition of stability requires that a small change
in the input results in only a small change in output, i.e., small perturbation of the input should not result in a huge
change in the output, [2]. In case of a function with multiple valleys within the search (design) domain, which is
often the case in engineering problems, the problem loses its stability and the need of global optimization arose.
To illustrate the problem of local-optimization instability, Figure (1) plots a two-dimensional objective function
with three local minima and one global minimum, located at x* = (√≠4.454, √≠4.454). Also there is a local maximum
at (-0.32, -0.32) encompassed by these four valleys. The specifications of the minima are given in Table (1). The
function is defined by [2] as follows:
f ( x)

4
2
0.01 ¬¶ ( xi  0.5)  30 xi  20 xi ,
i 1, 2

with

 6 d xi d 6.

(2)

Conventional gradient-based local optimization methods have a satisfactory convergence rate, but they may get
stuck into a local minimum depending on the starting point, i.e., on the ‚Äúarbitrarily assumed‚Äù input. To illustrate this
tendency, individual optimization runs are performed assuming small perturbations in the initial search point in the
vicinity of the local maximum of the function at (-0.32, -0.32).

Fig. 1. Two-Dimensional objective function; (a) surface plot; (b) contour plot

1389

H.I. Sayed et al. / Procedia Computer Science 1 (2012) 1387‚Äì1396
Author name / Procedia Computer Science 00 (2010) 000‚Äì000
Table 1. Specifications and function values of the global and local minima of function (2).
Label

x1

x2

f

x*

- 4.454

- 4.454

- 5.233

LM1

- 4.454

3.287

- 4.458

LM2

3.287

3.287

- 3.684

LM3

3.287

- 4.454

- 4.458

LMax

- 0.320

- 0.320

0.0666

The perturbation is limited to 0.33% of the search space defined by the lower and upper limits of the 2 design
variables, i.e., ¬± 0.02. Nine initial points are assumed for the perturbation in each direction either one-at-a-time or
concurrently to investigate how the local optimization algorithm will perform with objective functions that have
multiple local minima. These several initial search points are shown in Fig. (2) where corner points are named after
the local minimum they are closest to, while the mid-side points are named after the 2 local minima they are located
in between. The ninth point is located just at the local maximum. The optimization process is implemented by using
the Matlab¬Æ fmincon function available from the Matlab¬Æ optimization toolbox [3].
All results are summarized in Fig. (3) where the first four runs show that the corner-points runs converge to their
closest local minimum as they are already located within the descending valley of that minimum, while the
minimizers of the rest of the runs are indicated by arrows. The results of two out of the nine runs (Fig. 4) are shown
by plotting the search path starting from the initial point ending at the estimated minimizer of each run.
In summary, when initial points are located mid-distance between two local minima, it moves to the direction of
greater gradient vector and Hessian matrix, and therefore converges to the local minimum located in that direction,
even if it is only a local one, and then it is trapped there.
This shows the severe dependence of getting the global minimum of a function within a specified design space on
the initial start point, or more precisely, on the gradient vector and Hessian matrix at that point and whether they
direct the search path in the first few iterations towards or away from the global minimum. This clearly shows the
instability of multiple local-minima problems if they are dealt with using local optimization algorithms. To
overcome this problem, global optimization techniques are needed.
-0.38

-0.36

-0.34

-0.32

-0.3

LM1

LM1-LM2

LM2

G-LM1

Lmax

LM2-LM3

-0.28

-0.26
-0.26

LM1

-0.28

LM1-LM2

-0.38

-0.36

-0.34

-0.32

-0.3

LM1

LM1-LM2

LM2

G-LM1

Lmax

LM2-LM3

LM2-LM3
2

x2

-0.32
LM3-G

LM3

LM3-G

x

LM2-LM3

G

LM3-G

LM3

LM3-G
G
-0.36

G-LM1

G-LM1
Lmax

Lmax

x1

LM3

-0.34

G

-0.38

LM1-LM2
LM2

-0.32

-0.34

-0.36

LM1

-0.3

-0.3

G

-0.26
-0.26
-0.28

LM2

LM3

-0.28

-0.38

x1

Fig. 2. Nine initial points for nine independent optimization runs Fig. 3. Dependence of the solution on the ‚Äúarbitrarily assumed‚Äù initial point.

3. Global optimization
Several techniques are available for global optimization. Some of these are based on using multi-start local
optimization, in which a number of local optimization processes are carried out independently, each starting from a
different point in the search space and the best result from all trials is selected. The global search methods such as
genetic algorithms (GA) and simulated annealing (SA), however, are in general more robust, i.e. the choice of the
starting position has little influence on the final result, and they can find the global solution with a high probability.

1390

H.I. Sayed et al. / Procedia Computer Science 1 (2012) 1387‚Äì1396
Author name / Procedia Computer Science 00 (2010) 000‚Äì000

But both algorithms share the disadvantage of requiring a large number of function evaluations since they are based
on probabilistic searching without the use of any gradient information.

Fig. 4. Local optimization of Function (2) with various initial points: a) Initial Point (-0.34,-0.30), leading to LM1; b) Initial Point (-0.30,-0.30),
leading to LM2

4. Method of coupled local minimizers
The method of coupled local minimizers (CLM) is proposed by [4], [5] and introduced to the structural
engineering community by [6]. The method is applicable to global optimization of functions with multiple local
minima. In CLM, a cooperative search mechanism is set up by performing a number of local optimization runs
simultaneously, that are coupled by information exchange instead of running independently. The global search
process is directed by the gradients in each search point, which is favorable for the convergence speed. In this way,
CLM combines the advantage of the local gradient-based algorithms (relative fast convergence) with the global
approach of genetic algorithms (parallel strategy and information exchange).
4.1. Principal idea
In CLM a population of search points is used at which the function and its derivatives are calculated. It is the aim
to combine the results at the search points during the process, in order to let the population generate a minimum that
is better than the best result obtained from all individual local search processes. Hence a cooperative search
mechanism is set up that is realized by minimizing the average function value, i.e. the function value averaged over
all the search points of the population, and during the minimization process the search points are coupled in pairs by
synchronization constraints, so that they are able to interact and exchange information. Thanks to the coupling
mechanism a better performance is obtained than with multi-start local optimization.
4.2. Algorithm
For the minimization of the objective function f(x) with x* ¬è ÃìR n, Suykens et al., [4], [5] used a population
consisting of t search points, whose average objective function value is defined as:
f

1

t
¬¶

t

j 1

f (x

( j)

)

(3)

where x(j) denotes the jth search point (design vector) in the population. Pair-wise synchronization constraints are
imposed on the t search points, resulting in an equality-constrained minimization problem:

min f

x ( j ) ¬è ¬Én

such that x ( j )  x ( j 1)

0

(4)

for j = 1, 2, . . . , t, and with boundary condition x(t+1)= x(1) to allow for the pair-wise synchronization constraint
between the first and last points.

1391

H.I. Sayed et al. / Procedia Computer Science 1 (2012) 1387‚Äì1396
Author name / Procedia Computer Science 00 (2010) 000‚Äì000

Applying the Augmented Lagrange Method Which combines the quadratic penalty method and the Lagrange
multiplier method:

LA ( x , J )

K
t

t

( j)

t

> @>x

¬¶ f (x )  ¬¶ v

j 1

j 1

( j)

( j)

x

( j 1)

@  J2 ¬¶t x

( j)

x

( j 1)

2

(5)

j 1

where K“èweighting factor of the average value of the objective function,
and J Penalty parameter.

[X

( j )

]

: Vector of Lagrange multipliers,

4.3. Reported drawbacks of CLM
x The performance of the CLM method depends on the choice of the following parameters: the population size (t),
the penalty parameter (»ñ) and the weighting factor (»ò). Also initial Lagrange multiplier estimates {v0} and initial
values for the design variables {x0} should be chosen appropriately.
x The number of search points, (t), needed to achieve a good performance depends on the complex shape of the
surface or typically on the number of local minima per volume in the search space.
x The search process is also influenced by the tuning parameters »ñ and »ò.
x In general, updating »ñ does not improve unconditionally the performance of the CLM method.
x Since the parameters t, »ñ, »ò are problem dependent, it is difficult to determine them a priori or in a general way.
x Solution depends on the randomly chosen initial points
The difficulty of indeterminacy of the parameters is typical for global search methods, but, at the same time, they
provide the capability of finding the global minimum. This is in contrast to the local deterministic methods, which
are fully determined but can only locate local minima. Thus, the sought method should ideally be fully determined
and provides the capability of finding the global minimum.
5. Spider-Web Optimization by Ongoing Perusal (SWOOP):
5.1. From CLM to SWOOP
Some modifications are implemented in formulating the objective function to enhance its performance in
exploring the region of the global minimum, and ultimately getting it. These modifications are as follows.
First, as what matters is the relative values of the weighting factor (»ò), Lagrange multipliers {vj}, and the penalty
parameter (»ñ), the weighting factor can be set to ‚Äú1‚Äù and the others can be adjusted accordingly.
Second, the Lagrange and the quadratic penalty terms are replaced by one term that represents the summation of
Euclidean 2-norm of the differences of all design vectors. This can be interpreted graphically for 2D design space,
( j)
i.e., when x
is a 2x1 vector, as the summation of chords lengths of a spider-web connecting all the search points
as illustrated in Fig. (5), hence the name of the proposed method. This modification gets rid of all Lagrange
multipliers {vj} that need to be iteratively updated in CLM which resulted in computationally expensive algorithm.
Third, the search points will not be randomly chosen as implemented in CLM method, because this may lead to a
different solution in each run if all other factors are kept constant depending upon the starting search points‚Äô location
and their distance to the global minimum and the topography between their initial location and the valley of the
global minimum. As a substitute of this, they can be assumed uniformly distributed all over the search domain by
forming a square equally-spaced grid of search points within the boundaries assumed for the design space, as
illustrated in Fig. (5) for a 3x3 grid of search points. The order of the search points will be represented by the size of
either of the dimensions of the matrix of search points, n, where number of search points, t, will be equal to n2.
Fourth, search points are assumed to have no differential preference between each other. Thus, weighting factors
of the penalty term will be reduced to a scalar value rather than a vector. A further simplification is done by
assuming that scalar factor as static all over the search history, i.e., keeping the relative weight between the original
objective function and the penalty term as constant. Applying these simplifications and modifications will lead to
Eq. (6):

1392

H.I. Sayed et al. / Procedia Computer Science 1 (2012) 1387‚Äì1396
Author name / Procedia Computer Science 00 (2010) 000‚Äì000

t t
1 t
( j)
¬¶ f (x )  J ¬¶ ¬¶
j 1 i j 1
t j 1

LA ( x, J )

[x

( j)

x

(i )

]

(6)

2

where JPenalty parameter,
t

t

¬¶ ¬¶

j 1 i j1

( j)

[x

(i)

x ]

2

: Summation of all chords constituting the web connecting all search points.

A further enhancement can be done by dividing the ‚Äúspider-web‚Äù term using a normalization approach by
dividing it by total number of lines ‚Äúchords‚Äù within the web which equals to [t(t-1)/2] so that it reflects the average
length of the spider-web chords. Doing this, the term will be independent of the number of search points. Thus the
objective will take the form stated in Eq. (7)
L A ( x, J )

t
t
1 t
2J
( j)
¬¶ f (x ) 
¬¶ ¬¶
t j 1
t (t  1) j 1 i j 1

[x

( j)

x

(i )

]

2

(7)

This final form of the augmented objective function stated in Eq.(7) depends only on 2 parameters t and J“è
(alternatively, n and J“ó“† ).

Fig. 5. SWOOP method ‚Äì a) spider-web connecting the 3x3 grid of search points at iteration (1), b) spider-web at iteration (5), c) individual
function values for all search points up to iteration (5), d) Search points at iteration 5.

5.2. Perusal Part: Synthesized response surface and contour lines
The history of the function evaluations can be used to plot a ‚Äúsynthesized response surface‚Äù in the design domain
using ‚ÄúDelaunay triangulation‚Äù, which can be further utilized to get a ‚Äúsynthesized contour lines‚Äù of the objective
function. Both of the synthesized response surface and the synthesized contour lines are implemented in Matlab¬Æ
using the Delaunay and griddata functions where a cubic method of interpolation is implemented to avoid getting
discontinuities in higher derivatives.
5.3. Performance of SWOOP
The test function listed in Eq. (2) is used by [6] to evaluate the performance of CLM. Teughels [6] implemented a
comprehensive parametric study upon the CLM method parameters and showed that the final solution may not lead
to the global minimum due to increased value of penalty parameter J relative to the objective function weight K, the
‚Äúrandomly‚Äù chosen search points, initial values of Lagrange multipliers, or even the number of search points.
Although these tuning parameters enable the search process to find the global minimum among multiple local
minima, they are problem-dependent and therefore cannot be determined in a general way.

1393

H.I. Sayed et al. / Procedia Computer Science 1 (2012) 1387‚Äì1396
Author name / Procedia Computer Science 00 (2010) 000‚Äì000

To investigate the performance of the new proposed SWOOP method, the same test function is used in a
parametric study. The SWOOP method, as stated in Eq. (7), has only two parameters, t and Jor alternatively, n and
JThe parameters matrix implemented is a 5x4 matrix consisting of 5 values of n (2 to 6), i.e., search points as (4, 9,
16, 25, and 36) spread uniformly along the 2D search domain and 4 values of (J0.1, 1.0, 10, and 100.
The results are summarized in Table (2) for the 20 runs showing whether the search points population converge
to one value at the end of iterations or not, to what points in the design space they end up with, and whether they
manage to ‚Äútouch‚Äù or ‚Äúget close to‚Äù the global minimum. Success is considered whenever the history, not only the
final values, of individual function value of each search point shows that the overall minimum returned is within the
global minimum valley.
Table 2. Results for various SWOOP runs on test Function grouped by penalty factor

Run No.

n

1
5
9
13
17
2
6
10
14
18
3
7
11
15
19
4
8
12
16
20

2
3
4
5
6
2
3
4
5
6
2
3
4
5
6
2
3
4
5
6

J

0.1

1

10

100

No. of
No. of Fun.
Converged**
iterations Evaluations*
6
17
13
19
20
28
57
9
58
59
27
62
49
81
105
401
69
105
140
159

54
325
429
970
1,460
309
1,159
297
2,984
4,369
279
1,321
1,831
4,528
8,281
26,136
1,528
3,834
7,747
12,432

N
N
N
N
N
Y
Y
N
Y
Y
Y
Y
Y
Y
Y
Y
Y
Y
Y
Y

Solution

LM1

LM2

Locals
Locals
Locals
Locals
Locals
Global
Global
Indefinite¬§
Global
Global
Global
LM2
Local Max
Local Max
Local Max
Local Max
LM2
LM2
LM2
LM2

1
2
4
6
9
-

1
4
4
9
9
9
9
16
25
36

Include
LM3 GLOBAL Global in
final
1
2
4
6
9
-

1
1
4
4
9
4
9
25
36
4
-

Y
Y
Y
Y
Y
Y
Y
Y
Y
Y

Include
Global
during
#
history

Success

!

Y
Y
Y
Y
Y
Y
Y
2.1%
Y
Y
Y
1.1%
-4.9%
-7.7%
7.1%
N
N
N
N
7.1%

* This is compared to 151x151=22,801 function evaluations to get the "exact" response surface as plooted in Figures
** "Converged" mean all search point converged to the same point within tolerance, i.e., f=fn
# "Y" indicates that the global minimum is encountered through the history of the search points with tolerance < 1%. IF not, the ratio between the distance between the
encountered "global minimum" and the "exact" one to the whole interval throughout the history
! Success is assumed whenever the history gives enough information about the global minmum
¬§ Transition between the local maximum and the local minima

5.3.1. Discussion of Algorithm performance:
For All runs with low value of penalty factor (J runs 1, 5, 9, 13, and 17, all search points do not converge
to one value, (Fig. 6.a), but they are attracted, in groups, to local minima in the design domain because the constraint
to enforce the search points to converge to one value is relaxed. Although they do not converge to one value at the
final iteration, all these are considered as successful runs because the global minimum was ‚Äútouched‚Äù during the
search points‚Äô navigation history and is returned with extra information about other local minima, which is a further
check that the global one is obtained.
As the constraint to enforce the search points to converge to one value (penalty parameter) is strengthened by
increasing (Jto 1.0, runs 2, 6, 14, 18 converge to the global minimum (Fig. 6.b), while it does not converge for run
10 (n=4) where a spectrum of function values at the final step is encountered which represents a frozen solution
between ‚Äúgrouped‚Äù convergence at local minima (J  and a global convergence of all points at a local
maximum/minimum (J  as will be presented later All these runs are also considered successful because
the global minimum was ‚Äútouched‚Äù during the search points‚Äô navigation history. Even for the case (n=4), the global
minimizer was nearly ‚Äútouched‚Äù as the history of the search points gets close to it to 2.1% of the whole search

1394

H.I. Sayed et al. / Procedia Computer Science 1 (2012) 1387‚Äì1396
Author name / Procedia Computer Science 00 (2010) 000‚Äì000

domain, which is considered as a successful ‚Äúexploration‚Äù of global minimum valley, that can be easily exactly
determined by further narrowing the search domain to this valley.

Fig. 6: SWOOP method ‚Äì Search history for t =3; (a) J =0.1, (b) J =1.0

Further strengthening the penalty constraint by increasing (Jto 10 and then to 100 leads to enforcing the search
points to converge to one value in all cases, but the convergence is dependent on the number of search points. For
extremely high value of (J i.e. 100, all search point are attracted to each other very rapidly, as the sequential search
motion of each point goes to the current center of gravity of search points population, then all search points move
together to the valley that has the steepest gradient at that position, which may not be the global one, in this case it
was the local minimum (2) as was previously noticed while implementing the local optimization in section 2. In
addition to this, when using very few search points (n=2), the attraction to the center of gravity of the search point
population formed a ‚ÄúBlack-Hole‚Äù effect that does not permit any of the search points to escape this ‚Äútiny region‚Äù to
go to even any of the minima and the process cannot converge to any value and it is terminated only by reaching the
previously determined maximum number of iteration, 400. Using a large number of search point (n=6) only
manages to get closer to the global minimum, or explore where it is located, because of the fine grid formed at the
initial step but not during the optimization process. Examining the search points‚Äô history reveals that, for all orders
of search points‚Äô number, the search points are forced to bridge over all valleys, including the global one, to
converge to one value causing the optimization process to fail to lend back any extra information other than the
information inferred from the initial points population. This shows that a ‚Äútoo high‚Äù value of (Jshould be always
avoided.
For a somehow relaxed value of (J “è the search points are also enforced to converge to the same value. The
sequential search motion of each point goes towards the current center of gravity of search points population but this
time with a ‚Äúrelaxed‚Äù speed, i.e., the point has more ability to choose which is optimum for this iteration; go to the
population centroid against the function value, or staying somehow away from the centroid for a gain in minimizing
the function value. In other words, this relaxation of the penalty constraint enables the points to ‚Äúswim against the
current‚Äù of the gravitational force of the ‚Äúblack hole‚Äù created by the increased penalty parameter. By comparing the
results for (J and (J  for each corresponding number of search points, for (n=2), when (J  the points
can slip to the global minimum valley rather than frozen at the local maximum for (J For (n=4, 5, and 6),
rather than bridging over all valleys as described before for (J setting (J  the search points are enabled to
take shorter steps at each iteration providing more exploration of the search domain which results in ‚Äúshooting
close‚Äù the global minimum by 4.9-7.7% of its value, which is not the case for (J . Also setting (J  may
send an erroneous message when examining the history of function values, as the obtained minimum may be
‚Äúthought‚Äù as the global one. On the contrary, setting (J all these histories of function values, reveal that the
converged value is NOT the global one and also return enough information about its location.
In summary, the already ‚Äúnormalized‚Äù ‚Äúweighted‚Äù augmented objective function can be tried first with unity
penalty parameter and according to the symptoms realized of its performance, it can be adjusted accordingly. If the
search points converged to one value, the penalty parameter can be decreased. If the search points converged to
multiple ‚Äúgrouped‚Äù values, the penalty parameter can be increased.

1395

H.I. Sayed et al. / Procedia Computer Science 1 (2012) 1387‚Äì1396
Author name / Procedia Computer Science 00 (2010) 000‚Äì000

5.3.2. Discussion of response surface synthesis:
The response surface of the objective function, not the augmented one, can be ‚Äúsynthesized‚Äù using the Delaunay
triangulation to test which combination of the parameters matrix will lend back more information about other local
minima. Furthermore, the ‚Äúsynthesized‚Äù response surface is used to ‚Äúsynthesize‚Äù grid data to plot a ‚Äúsynthesized‚Äù
contour lines of the function.
The contour lines are compared to the ‚Äúexact‚Äù ones using the matrix comparison methods [7], and the values are
listed in Fig. (7). The Computational Cost is found to be quadratically-proportional to (n), i.e., directly-proportional
to the number of search points and logarithmically-proportional to the penalty parameter. So, for a reasonable
computational cost, the penalty parameter should be kept as low as possible (0.1‚Äì1.0) whereas the number of search
points can be chosen of moderate values (n= 3-5).

Fig. 7: SWOOP method ‚Äì Response contour synthesis for t=3; (a) J =0.1, (b) J=1.0

The most successful runs to synthesize a response surface closest to the ‚Äúexact‚Äù one are the ones with surface
shape parameter and surface magnitude parameter closest to unity [7]. The most successful runs are of the same
characteristics as the ‚Äúcomputationally-efficient‚Äù runs, i.e., of low penalty parameter (0.1‚Äì1.0) and a moderate
number of search points (n= 3-5). There is no significant improvement in the results when increasing number of
search points compared to the corresponding rapid (quadratic) increase in computation cost. A too high value of the
penalty parameter and a too low number of search points should be always avoided.
Following the aforementioned arguments, it‚Äôs advised to start with a ‚Äúreasonably‚Äù low number of search points
(n= 3 or 4) with a low value of penalty parameter (J= 0.1), this combination represents the most computationallyefficient reasonable solution. According to the observed performance, these parameters can be tuned to confirm/get
closer to the global minimizer; a too low value of J leads to ‚Äúgrouped‚Äù convergence, a too high value leads to a
local optimizer, and a moderate value leads to the sought global minimizer. According to the output from one trial,
penalty parameter can be tuned to confirm/get closer to the global minimizer.
Some other test functions have been tested, such as the one listed in Eq. (8), that has numerous multiple valleys
and hills spread uniformly within the search space with so-close values of objective function at those valleys [6].
One of the successful runs is plotted in Fig. (8) with almost-perfect synthesized contour lines. Other test functions
can be found in [7].
f (x)

8n 

0 .01 n

n
n
2
¬¶ x i  4 n ¬ñ cos( 0 .2 x i )  4 n ¬ñ cos( x i )
i 1
i 1

2n i 1

(8)

1396

H.I. Sayed et al. / Procedia Computer Science 1 (2012) 1387‚Äì1396
Author name / Procedia Computer Science 00 (2010) 000‚Äì000

Fig. 8: SWOOP method ‚Äì Response contour synthesis for t=4, J=1.0; (a) Search history in 3D, (b) Contour line synthesis.

6. Conclusions
x The SWOOP method is able to determine the global minimum of functions of various characteristics; multiple
valleys with a local maximum, numerous multiple valleys and hills spread uniformly within the search space with
so-close values of objective function at those valleys, and multiple valleys and hills non-uniformly distributed
over the search space with widely varying function values at those valleys.
x The SWOOP method returns extra information to further check/validate whether the obtained minimum is the
global one or not and information about the overall behavior of the function over the search domain. This
information may be valuable to better understand the function behavior and to upgrade the heuristics.
x As a general strategy, upon implementing the SWOOP method, one should start with a low value of the penalty
parameter along with a reasonable number of search points, e.g., n=3 or 4 and according to the observed behavior
of the optimization process, these values can be tuned to confirm/get closer to the global minimizer.
x The synthesized response surface and synthesized contour lines got from the SWOOP method are of high
precision at the valleys, as the method uses larger steps at hills and smaller steps at valleys due to the fact it uses
a minimization function. In order to get a high precision at the hills comparable to those at the valleys, the
method can be used twice with a minimization of the original function for the valleys and the minimization of the
negative version of the objective function for the hills. This can be used when the full behaviour of some specific
function is needed to be ‚Äúscanned‚Äù over a specific domain with a minimal computational cost.
In-progress improvements to SWOOP method
Currently, some enhancements are being done to the technique, which are as follows:
x Updating the penalty parameter over the search history to reflect the relative weight between the original
objective function and the penalty term.
x Integrating the concept of design of experiment in setting the initial points to make use of fractional factorial
design in case of large number of design variables.
x Replacing the initial grid point with more efficient distribution of the points resembling Gaussian points of
numerical integration.
References
1. S. S. Rao, Engineering Optimization - Theory and Practice, John Wiley & Sons, New York, 3rd edition (1996).
2. K. J. Bathe, Finite Element Procedures, Prentice Hall, New Jersey, (1996).
3. MATLAB, Matlab Optimization Toolbox User‚Äôs Guide, http://www.mathworks.com/ products/optimization/ , Version 2.1 (Release 12.1).
The Mathworks, (2000).
4. J. A. K. Suykens, J. Vandewalle, and B. De Moor, Intelligence and Cooperative Search By Coupled Local Minimizers, International
Journal of Bifurcation and Chaos, vol. 11, No. 8 (2001) 2133‚Äì44.
5. J. A. K. Suykens and J. Vandewalle, Coupled Local Minimizers: Alternative formulations and extensions, 2002 World Congress on
Computational Intelligence - International Joint Conference on Neural Networks IJCNN 2002, Honolulu, USA, (2002) 2039‚Äì43.
6. A.Teughels, Inverse Modelling of Civil Engineering Structures Based on Operational Modal Data, PhD thesis, Department of Civil
Engineering, K.U.Leuven, Belgium (2003).
7. H.I. Sayed, Application of Dynamic Field Testing on Structural Assessement and Damage Identification Using Finite Element Model
Updating, PhD thesis, Department of Structural Engineering, Cairo Univerisiy, Egypt (2009).

