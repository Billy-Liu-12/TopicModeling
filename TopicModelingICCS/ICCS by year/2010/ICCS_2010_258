Procedia Computer
Science
Procedia Computer
001(2010)
1–9
Procedia
ComputerScience
Science
(2012)
175–183

www.elsevier.com/locate/procedia

International Conference on Computational Science, ICCS 2010
Mixed-Precision AMG as Linear Equation Solver for Deﬁnite Systems
Maximilian Emansa,∗ , Albert van der Meera
a AVL

List GmbH, Hans-List-Platz 1, 8020 Graz, Austria

Abstract
The performance of algebraic multigrid (AMG) algorithms, implemented in 4-byte ﬂoating point arithmetic, is
investigated on modern cluster architecture with multi-core CPUs. The algorithmic considerations comprise the eﬀect
of preconditioning in 4-byte ﬂoating point arithmetic on Krylov solvers using standard 8-byte ﬂoating point arithmetic.
The data of basic linear algebra benchmarks are used to interpret the performance of AMG algorithms employed as
linear solvers in computational ﬂuid dynamics simulation tools.
c 2012 Published by Elsevier Ltd. Open access under CC BY-NC-ND license.
⃝

Key words: algebraic multigrid, mixed-precision

1. Introduction
Until recently, the eﬀective run times of an unchanged program performing some task in scientiﬁc computing
could be reduced by simply switching to a more modern computer since the frequency of the operations on the CPU
was increased permanently. Now, it is no longer possible for the software developer to rely on this accelerating eﬀect
since the clock cycle frequency of the processing cores can no longer be increased signiﬁcantly. But even on older
hardware, the clock cycle frequency says little about the eﬀective time required to solve a certain problem since by far
not every clock cycle can be used eﬀectively to perform a ﬂoating point operation. In many cases the reason for this
is that the data cannot be supplied fast enough to the processing unit. This leaves space for algorithmic improvement
for both, serial and parallel scientiﬁc computations.
Using 4-byte precision instead of the standard 8-byte precision in deliberately chosen parts of the algorithm is a
convenient, but eﬀective measure to accelerate programs. The idea to exploit mixed-precision ﬂoating point hardware
for the solution of linear system is not new. It has been discussed e.g. recently by Baboulin et al. [1] who apply it in
the context of an FGMRES solver. Here we will focus on AMG algorithms that diﬀer from Krylov-subspace methods
in the important aspect that systems of very diﬀerent size have to be treated eﬃciently. As we will demonstrate,
the beneﬁts of the mixed-precision approach here lie in the reduction of data that can be stored in the cache; this
accelerates serial tasks, but for shared memory parallel computations it has also the positive side eﬀect that less data
needs to be transferred from the main memory to the individual cores.
We will analyse the performance of linear multigrid solvers on a modern cluster that is equipped with multi-core
CPUs where a complex interplay between inter-node and intra-node data exchange takes place in addition to the
data transfer processes in the memory hierarchy of a single core. In this contribution we consider particularly the
advantages of 4-byte precision algebraic multigrid algorithms.
∗ Corresponding

Author
Email addresses: maximilian.emans@gmx.at (Maximilian Emans), albert.vandermeer@avl.com (Albert van der Meer)

c 2012 Published by Elsevier Ltd. Open access under CC BY-NC-ND license.
1877-0509 ⃝
doi:10.1016/j.procs.2010.04.020

176

M.
A. van
derder
Meer
/ Procedia
1 (2012)
175–183
M. Emans,
Emans and
A. van
Meer
/ ProcediaComputer
Computer Science
Science 00
(2010) 1–9

2

2. AMG algorithms
For the iterative computation of an approximate solution x ∈ Rn of a linear system
Ax = b

(1)

with A ∈ Rn×n , b ∈ Rn we apply four diﬀerent algorithms that are described brieﬂy in the subsequent section. The
fundamental AMG algorithm is not repeated here; for this we refer the reader to the literature, e.g. to the appendix
of Trottenberg et al. [9]. AMG methods diﬀer in one or more elements of multigrid, i.e. in the coarse-grid selection
scheme, the smoother or the cycle type. For the sake of brevity we mention the relevant features of the algorithms
used here. For more details we will refer explicitly to the literature.
The AMG algorithms build the coarse-grid operators with the well known Galerkin approach. For the parallelisation of the solver the total number of unknowns is divided into disjoint subsets that are distributed to the available
processors. The parallel versions of the AMG methods we use have in common that the interpolation and restriction is
a strictly local process (with respect to the domains assigned to each process) such that the computation of the coarsegrid operator is simpliﬁed, see Emans [4]; moreover, a hybrid Gauß-Seidel smoother is applied, see van Emden and
Meier-Yang [5].
2.1. AMG implementations
The algorithm ams1cg is a Smoothed Aggregation AMG that has been described by Vanˇek et al. [10]. It is
characterised by a rapid coarsening and a rather expensive computation of the coarse-grid operators. For details
about the basic algorithm we refer to Vanˇek et al. [10], for its parallel implementation to Emans [4]. This algorithm
is employed as a preconditioner; in the v-cycle scheme two pre- and two post-smoothing sweeps are applied. The
coarse-grids are merged to appropriate neighbours as soon as the number of nodes drops below a certain threshold,
i.e. an agglomeration strategy is applied, see Trottenberg et al. [9].
The algorithm amf1cg has a pairwise aggregation algorithm and uses constant interpolation. Consequently, many
grids are constructed (compared to ams1cg), but the computation of the coarse-grid operator is very cheap. In order to
limit the number of grids, the Smoothed Aggregation method is applied on very coarse grids. The cycling scheme is
an F-cycle, see Trottenberg et al. [9]; only two hybrid Gauß-Seidel sweeps are performed after the return to the ﬁner
grid, i.e. there is no pre-smoothing. The coarse-grid treatment is again an agglomeration scheme. This algorithm is
employed as a preconditioner of a conjugate gradient algorithm.
In algorithm amggs3 the same AMG algorithm as in amf1cg is used as a “stand-alone” solver.
The algorithm ichpcg is the same conjugate gradient algorithm that is used by ams1cg and amf1cg, however, it is
preconditioned by an incomplete (parallel) Cholesky factorisation.
For the communication in the parallel case the hp-MPI library, version 2.3, is used. The data of the inter-domain
boundaries is always exchanged asynchronously. Further details of the implementation are described in detail in
Emans [3].
2.2. 4-byte substitution
Since the solver is embedded into an 8-byte algorithm, the required accuracy of the solution is checked in 8-byte
precision. Moreover, if the conjugate gradient method is applied (and AMG is preconditioner), then the conjugate
gradient algorithm is implemented in 8-byte precision, too, since the robustness of the method might be impaired by a
less accurate computation of the orthogonal search directions. If the preconditioning operation is done in less accurate
arithmetic, then care must be taken that the search directions are still orthogonal with respect to the accuracy of the
8-byte precision; This is not self-evident: e.g. Tadano and Sakurai [8] report that left preconditioning of a certain
BiCGStab implementation leads to the loss of the 8-byte accuracy of the orthogonalisation.
We use the preconditioned conjugate gradient algorithm, see Saad [7], in the form of algorithm 1. The input is an
initial guess x0 (in our cases x0 = 0), the right-hand side vector b, the system matrix A, and the solver tolerance ε. The
output is the solution vector xi . M is the preconditioner that is provided as application of the inverse to some vector.
Algorithm 1 is known to be suﬃciently robust for a large variety of problems characterised by positive deﬁnite A
if the search directions ri are computed in 8-byte accuracy, but now we want to use a 4-byte representation of M and

M. Emans,
A. van
dervan
Meer
Procedia
Computer
Science
1 (2012)
175–183
M. Emans
and A.
der /Meer
/ Procedia
Computer
Science
00 (2010)
1–9

177
3

Algorithm 1 Preconditioned conjugate gradient algorithm
1:
2:
3:
4:
5:
6:
7:
8:
9:
10:
11:
12:
13:
14:
15:
16:

r0 = b − Ax0
ρ = ρ0 = r0T · r0
i=1
while ρ ≥ ε · ρ0 do
zi = M −1 ri−1
T
γi = ri−1
· zi
if i = 1 then
βi = 0, pi = zi
else
βi = γγi−1i , pi = zi + βi · pi−1
end if
qi = Api
i
αi = p Tγ·q
i
i
xi = xi−1 + αi · pi , ri = ri−1 − αi · qi
ρ = riT · ri , i = i + 1
end while

its inverse. Let us assume that all the vectors and scalars up to iteration i − 1 have been computed in 8-byte ﬂoating
point arithmetic. In iteration i we apply the inverse of M in 4-byte accuracy. The vectors and scalars diﬀer then from
those that would have been obtained if we continued in 8-byte accuracy. We express this diﬀerence in placing a tilde
(˜) on top of the symbols in the following. Thus, instead of zi in line 5 of above algorithm we obtain z˜i . All vectors
and scalars that are computed from this vector are aﬀected by the perturbation through the 4-byte preconditioning.
Consequently, in line 14 we have the vector updates
x˜i = xi−1 + α˜ i · p˜ i

and

r˜i = ri−1 − α˜ i · q˜ i

(2)

Since, analogously to statement 1 of algorithm 1, ri−1 = b − Axi−1 we can write for any iteration i
r˜i = b − A(xi−1 + α˜ i · p˜ i ) = b − A x˜i .

(3)

Thus, using the preconditioner with 4-byte ﬂoating point arithmetic does no harm to the precision of orthogonalisation
of the conjugate gradients algorithm, i.e. the search directions are still orthogonal with respect to the accuracy of the
8-byte ﬂoating point arithmetic.
For algorithm amggs3 a representation of the solution in 8-byte precision is required. In this case the solution in
4-byte precision is simply cast to a 8-byte vector since the conjugate gradient framework is lacking. The quality of
the solution is checked in 8-byte ﬂoating point arithmetic, too.
3. Performance of basic linear algebra operations
In order to estimate what we can expect from switching from 8-byte precision to 4-byte precision, we examine the
performance of model matrix-vector multiplication and dot product on the hardware that is used for all computations
presented in this contribution. With respect to the AMG algorithms it is important to note that the matrix-vector
product occurs in the computation of the residuals and that it is essentially as expensive as one lexicographic GaußSeidel sweep. This way for instance approximately 95% of the computational work of amggs3 (and the corresponding
part of amf1cg) is represented by the model of a matrix-vector multiplication.
Since all matrices (apart from the inverted ones on the coarsest level) are sparse and the zero elements are not explicitly stored it is necessary for these performance tests to provide model matrices. For this study we setup two types
of square model matrices with a random structure. The ﬁrst matrix type models the ﬁne-grid system that represents the
discretisation of a partial diﬀerential equation on an unstructured combined tet/hex mesh where average the number of

178

M.
A. van
derder
Meer
/ Procedia
1 (2012)
175–183
M. Emans,
Emans and
A. van
Meer
/ ProcediaComputer
Computer Science
Science 00
(2010) 1–9

4

elements per row is around six (m6); the second type models the coarse-grid systems where the connectivity is much
higher such that the number of elements per row is around thirty (m30).
In ﬁgure 1 the average computing times of a suﬃciently large number of matrix-vector multiplications (“saxpy”)
with both types of matrices m6 and saxpy m30 and dot products are plotted as a function of the problem size, i.e. the
length of the vector. The computer that is referred to as com1 is a single node of a Linux cluster with two Intel Xeon
5365. Each of these chips provides four processing cores with a clock frequency of 3.0 GHz. An L1-cache of 32 kB
is assigned to each core; two cores share an L2-cache of 4 MB. The machine distributes automatically up to four tasks
in such a way onto the eight available cores that for each task a complete L2-cache is available.
The computations were performed with 4-byte and 8-byte accuracy. Additionally the computing times of two and
four parallel identical tasks that are started synchronously have been measured. The data can be read from ﬁgure 1.
Let us ﬁrst consider the single-task computations in 8-byte precision. The depletion of the cache is marked by the
deviation of the curve of the computing time from the straight line that it forms in the double logarithmic diagrams
in ﬁgure 1 for low problem sizes. Let us denote the computing time of a single task by tˆ1 , and that of i parallel, but
identical, tasks on the same node by tˆi . As long as the problem ﬁts into the cache, tˆ1 ≈ tˆ2 ≈ tˆ4 which can be expected
since for each task a complete L2-cache is available. As soon as the problem size (matrix elements plus two vectors)
exceeds the size of the L2-cache such that the cache has to be reloaded during the calculation, this no longer holds
true: The more tasks are performed synchronously, the longer the computation will take. This shows that the data
supply to the four individual caches on one node is not independent of each other; more precisely, the tasks compete
for the resources performing the data transfer from the main memory.
The diagrams in ﬁgure 1 show that, as expected, for 4-byte computations larger problems (in terms of the vector
lengths) ﬁt into the cache. This can be seen most clearly for the dot-product task. Moreover, for problem sizes
requiring to reload the cache, the computing times for 4-byte matrix-vector products for more than one task lie closer
to the computing times of a single task (compared to 8-byte computations); evidently the work load of the resources
performing the data transfer from the main memory is lower since the amount of data is reduced by a factor of two.
saxpy m6

saxpy m30

10

10

1

1

0.1

0.1

dot product

0.1

0.01

0.01

0.001

0.001

0.0001

0.0001

1e-05
1000

10000 100000 1e+06 1e+07
problem size

computing time / s

computing time / s

computing time / s

0.01

1e-05
1000

0.001

0.0001

1e-05

10000 100000 1e+06 1e+07
problem size

1e-06
1000

10000 100000 1e+06 1e+07
problem size

Figure 1: Computing time vs. problem size (size of the vectors) on machine com1 for model problems for 8-byte and 4-byte precision and
synchronised computation on a single node : 1 task, : 2 tasks, : 4 tasks; empty symbols: 8-byte, ﬁlled symbols: 4-byte

Due to the logarithmic y-axis the gain from using 4-byte precision instead of 8-byte precision is diﬃcult to read
from ﬁgure 1. Therefore in ﬁgure 2 the diﬀerence between the computing time for 4-byte precision (or/and parallel

179
5

M. Emans,
A. van
dervan
Meer
Procedia
Computer
Science
1 (2012)
175–183
M. Emans
and A.
der /Meer
/ Procedia
Computer
Science
00 (2010)
1–9

tasks) and the computing time for 8-byte precision, compared to the computing time for 8-byte precision, is shown.
In order to demonstrate that the discussed matter is not unique to the employed machine com1 we added the same
data measured on a second machine com2 to these diagrams. It is equipped with two Intel Xeon 3.20 GHz, each
disposing of a single core with an L1-cache of 16 kB and an L2-cache of 2 MB. From the ﬁgures can be read that
for the matrix-vector product throughout the range of problem sizes the gain in computing time through using 4-byte
precision instead of 8-byte precision is at least 20%, but it becomes larger for parallel tasks of sizes between 104
and 106 , but again smaller for even larger problem sizes. Due to simple data transport requirement we see no gain
in the dot product case until the cache is depleted. For larger problems the situation is comparable to that of the
matrix-vector product.
penalty saxpy m30

penalty dot product
0.8

0.6

0.6

0.6

0.4

0.4

0.4

0.2
0

penalty / %

0.8

penalty / %

penalty / %

penalty saxpy m6
0.8

0.2
0

0.2
0

-0.2

-0.2

-0.2

-0.4

-0.4

-0.4

-0.6
1000

10000

100000 1e+06
problem size

1e+07

-0.6
1000

10000

100000 1e+06
problem size

1e+07

-0.6
1000

10000

100000 1e+06
problem size

1e+07

Figure 2: Penalty (positive sign) and gain (negative sign) of using 4-byte precision and more than one core of one node (zero is 8-byte computation
using a single core): : 1 task, : 2 tasks, : 4 tasks; empty symbols: 8-byte, ﬁlled symbols: 4-byte; solid line: machine com1, dotted line:
machine com2

The situation becomes more complicated if we use the model problems in order to ﬁnd out what a parallel eﬃciency
t1
(4)
E p :=
tp · p
(p number of parallel tasks, t p time of computation with p tasks) of shared memory computations can be expected. For
this purpose we interpret the data shown in ﬁgure 1 in another way: For two and four parallel tasks a model parallel
eﬃciency Eˆ p for p parallel processes that depends on the problem size n is deﬁned as
tˆ1 (p · n)
.
Eˆ p (n) :=
tˆp (n) · p

(5)

E p expresses the change in computing time if a certain task is distributed to p processors and each processor does a
certain portion of the work; it eventually includes the parallel overhead (cost of tasks that are speciﬁc to parallelisation
as well as idling). Eˆ p (n), on the other hand, shows how much a computation is retarded if identical model problems
with the size of the distributed tasks are carried out synchronously without parallel overhead. The model parallel
eﬃciency is plotted in ﬁgure 3 for those model problems that are relevant for AMG. For saxpy m6 problems with sizes
between approximately 10000 and 500000 the 8-byte computations can be expected to be signiﬁcantly accelerated if
they are distributed to two or four cores on a single node. For larger problem sizes the model parallel eﬃciency drops
below 100%. The curves of the 4-byte computations are shifted to about double the problem size. For saxpy m30
problems there is a second peak that corresponds to the second bend in the corresponding curves in ﬁgure 1; it is due
to a slower memory access for very large amounts of data. For the purpose of this study, however, the saxpy m30 is
only a model for problem sizes lower than about 50000 since the matrices associated with the ﬁne multigrid levels
have a much lower connectivity.

180

M.
A. van
derder
Meer
/ Procedia
1 (2012)
175–183
M. Emans,
Emans and
A. van
Meer
/ ProcediaComputer
Computer Science
Science 00
(2010) 1–9
penalty saxpy m30

200

200

180

180
model parallel efficiency / %

model parallel efficiency / %

penalty saxpy m6

160
140
120
100
80
60
1000

6

160
140
120
100
80
60

10000

100000

1e+06

1e+07

problem size

Figure 3: Model parallel eﬃciency Eˆ p (n) for matrix-vector multiplications:

1000

10000

100000

1e+06

1e+07

problem size

: 2 tasks, : 4 tasks; empty symbols: 8-byte, ﬁlled symbols: 4-byte;

4. Benchmarks
An important application of linear solvers in science and industry are computational ﬂuid dynamics (CFD) simulations. Our two benchmark cases are two short periods taken from an unsteady, three-dimensional CFD simulation
of a full cycle of a four cylinder gasoline engine. In these cases the Navier-Stokes equations are solved in terms of the
velocity and pressure ﬁeld. The SIMPLE algorithm, see Patankar [6], based on a ﬁnite-volume discretisation on an
unstructured grid, is applied in order to deal with the non-linearity and the coupling of the system. The SIMPLE algorithm iteratively improves the solution by solving a discretised linear formulation of the momentum equations and the
pressure-correction equation in a subsequent manner. For compressible subsonic ﬂow the solution of the symmetric
positive deﬁnite pressure-correction equation is computationally most expensive.
Both benchmark cases consist of a few time-steps for which a few SIMPLE iterations are performed until convergence is reached. For details about this simulation we refer to Emans [3]; for the purpose of the current study it
is suﬃcient to mention that the ﬁrst benchmark (0000) is a part of the phase when the cylinder is loaded by cold air
(1.4 mio cells) and the second one (0972) is a part of the phase when the combustion takes place (240000 cells). The
reported computing times refer to the time spent on the solution of the pressure-correction equations.
For the measurements we used up to four nodes with 2 quad-cores (i.e. 8 cores per node in total) of a Linux
cluster (nodes equipped as machine com1) connected by a Gigabit Ethernet network with an eﬀective bandwidth of
around 117 Mbit/s and a latency of around 60 μs. The relevant part of the program is compiled by the Intel-FORTRAN
compiler, version 10.1; the communication is performed through calls to hp-MPI subroutines (version 2.3, C-binding).
The benchmarks were run within the environment of the software AVL FIRE(R) 2009 with 1, 2, 4, 8, and 16 parallel
processes, where the domain decomposition was performed once for each case by the standard algorithm provided
with this software; the sizes of the decomposed problems were essentially equal. Computations with 1, 2, and 4
processes were done on a single node since in the practical applications the possible gain in performance usually does
not justify the occupation of the additional nodes. For 8 and 16 processes we used 2 and 4 nodes respectively such
that each process had full access to 4 MB L2-cache.
The data transfer for computations with 1, 2, and 4 processes is done using a shared memory model that is invoked
automatically by the MPI implementation. The eﬀective bandwidth is around 1400 Mbit/s for this kind of data transfer,
the latency is below 0.5 μs. For runs on more cores the intra-node communication is also done using the same shared
memory model. Only the inter-node communication is performed through the network.
The solvers are used to get an approximate solution of the systems. The iteration is terminated as soon as the
1-norm of the residual is smaller than 5% of the norm of the initial guess (zero vector), i.e. ε = 0.05 in algorithm 1.
The computing times, the parallel eﬃciency, and the number of iterations are displayed in ﬁgure 4.

181
7

M. Emans,
A. van
dervan
Meer
Procedia
Computer
Science
1 (2012)
175–183
M. Emans
and A.
der /Meer
/ Procedia
Computer
Science
00 (2010)
1–9
Problem 012-0000 - 1.4 mio cells, load
120

600

1000
100

100

number of iterations

parallel efficiency / %

computing time / s

550

80

60

500

450
40

10

20
1

2

4
8
processors

16

400
1

2

4
8
processors

16

1

2

4
8
processors

16

4
8
processors

16

Problem 012-0972 - 240000 cells, combustion
120

1400
1350

100

100

number of iterations

parallel efficiency / %

computing time / s

1300

80

60

1250
1200
1150
1100

40
10

1050
20
1

2

4
8
processors

16

1000
1

2

4
8
processors

16

1

2

Figure 4: Computing time of solution phase (left), parallel eﬃciency of solution phase (middle), and cumulative iteration count (right):
: amf1cg, : amggs3, ♦: ichpcg (number of iterations divided by 5); empty symbols: 8-byte, ﬁlled symbols: 4-byte

: ams1cg,

4.1. Performance of 4-byte computations
Since for all solvers the iteration count for the 8-byte and 4-byte versions is essentially the same, one can expect
that the performance gains observed at the model problems in principal carry over to these application problems. The
parallel overhead, of course, will reduce the performance. The speed-up (4-byte compared to 8-byte) lies in a range
between 1.1 and 1.5; it is comparable to that reported by Baboulin et al. [1].
4.2. Shared memory computations
In the following we interpret the measured values using the data of the model problems on a qualitative level.
We believe that a quantitative interpretation that would allow a prediction of the performance, like e.g. done in a
ˇ
less complex situation by Ciegis
et al. [2], is not possible in the framework of a single scientiﬁc publication. The

182

M.
A. van
derder
Meer
/ Procedia
1 (2012)
175–183
M. Emans,
Emans and
A. van
Meer
/ ProcediaComputer
Computer Science
Science 00
(2010) 1–9

8

main obstacle seems to be the prediction of the performance of the asynchronous point-to-point communication; this
process requires also access to the shared memory which is a limiting factor and it depends strongly on details of the
MPI-implementation as well as the chip control strategy. Therefore it interferes to a large extent with the rest of the
computation.
The computational work of a properly working multigrid algorithm is dominated by the work on the ﬁne grids;
for the algorithm amggs3 about 75% of the computing time of the AMG part is spent on the ﬁnest three grids. In the
case of benchmark 0000 the ﬁnest grid comprises 1.4 mio unknowns. The parallel eﬃciency of the basic operations
of the 8-byte computations is in an order of magnitude where no signiﬁcant acceleration or even a retardation can be
expected (see curves in diagram for saxpy m6 in ﬁgure 3 for n = 1.4 · 106 /p, p = 2,4). For the 4-byte computation the
situation is more favourable: Here an acceleration of up to 20% can be expected and the parallel eﬃciency is in fact
signiﬁcantly higher than that of the 8-byte computations.
For benchmark 0972 the ﬁnest grid has 240000 unknowns. The sizes of the basic tasks are in a range where an
accelerated parallel computation can be expected. With regard to the shift towards larger problem sizes of the 4-byte
curves in the diagrams in ﬁgure 3 one can assume that the 4-byte computations are more accelerated than the 8-byte
computations. The comparison of the curves of the parallel eﬃciency of benchmarks 0000 and of benchmark 0972
shows that the smaller benchmark has a higher parallel eﬃciency; it seems evident that in this case the accelerated
computation of the basic operations can almost compensate the parallel overhead; in the larger benchmark 0000 the
parallel eﬃciency is not only reduced by parallel overhead but also by the retardation of the basic operations. The
data conﬁrms that for the shared memory computations both, the data transfer between the processes and to a large
extend also the competition among the processes for memory bandwidth are relevant.
4.3. Combined shared and distributed memory computations
In addition to the intra-node data transfer, these calculations comprise a certain amount of comparatively slow
inter-node data transfer; the limiting factor is, according to our experience, not so much the bandwidth, but the
latency. The reason is that the AMG algorithms require the exchange of a large number of small data packages since,
due to the cycling scheme, the coarse grids are visited frequently.
The parallel eﬃciency of the calculations of the large benchmark 0000 is greater than that of shared memory
calculations. The model parallel eﬃciency of the problems with the size of the distributed problems is now greater
than unity and a large part of the computational work can be expected to be accelerated. Since this eﬀect is more
signiﬁcant for the 4-byte cases, a more favourable parallel eﬃciency is observed for these calculations.
For the smaller benchmark 0972, a large part of the calculations with amggs3 and amf1cg on 16, but also already
on 8, cores consists of basic problems that are of a size where the problem ﬁts entirely into the cache such that no
acceleration can be expected (n < 30000). For ams1cg (due to the more rapid reﬁnement) and ichpcg (no coarse-grids
at all) a greater part of the computation consists of work on the ﬁner grids that can be expected to be accelerated such
that for these solvers the parallel eﬃciency of the 16 processes calculation is signiﬁcantly larger compared to that of the
4 processes calculation. For all solvers and a number of processes of 16 it can be observed that the parallel eﬃciency
of the 4-byte computations is less than that of the 8-byte computation. This is due to the fact that the computing time
of the basic problems of the corresponding size starts to lie in the range of the communication latency; note again the
shift of the 4-byte curves towards larger problem sizes in ﬁgure 3.
5. Conclusions
We have shown that the preconditioning operations in a standard implementation of a conjugate gradient algorithm can be performed in 4-byte instead of in 8-byte ﬂoating point arithmetic. Benchmarking basic model linear
algebra operations has revealed that, if the problems do not ﬁt into the cache, the data transfer from the main memory
to the cache and to the processing unit tends to be a limiting factor on the used standard shared memory architecture. The observed eﬀects are exploited by switching from standard 8-byte to 4-byte ﬂoating-point variables for the
AMG-preconditioning of a conjugate gradient algorithm. Whereas the retardation of the algorithm through inter-node
communication is only marginally alleviated through the reduced amount of data that is exchanged between the nodes,
the computations are accelerated since the data ﬂow between memory and cache is smaller when 4-byte variables are
used. Diﬀerences in the convergence due to the reduced accuracy of the preconditioning have not been observed. In

M. Emans,
A. van
dervan
Meer
Procedia
Computer
Science
1 (2012)
175–183
M. Emans
and A.
der /Meer
/ Procedia
Computer
Science
00 (2010)
1–9

183
9

the AMG part of linear solvers the proposed preconditioning using 4-bytes ﬂoating point arithmetic accelerates the
solution of linear systems of equations, e.g. in CFD simulations, signiﬁcantly.
References
[1] M. Babouli, A. Buttari, J. Dongarra, J. Kurzak, J. Lanlou, J. Lanlou, P. Luszczek, and S. Tomov: Accelerating Scientiﬁc Computations with
Mixed Precision Algorithms, Computer Physics Communications, vol. 180, pp. 2526-2533, 2009
ˇ
[2] R. Ciegis,
O. Iliev, Z. Lakdawala: On Parallel Numerical Algorithms for Simulating Industrial Filtration Problems, Computational Methods
in Applied Mathematics, vol. 7, pp. 118–134, 2007
[3] M. Emans: AMG for linear systems in engine ﬂow simulations, Proceedings of PPMA09, Lecture Notes in Computer Science, Springer
Verlag, 2010, to appear
[4] M. Emans: Performance of Parallel AMG-preconditioners in CFD-codes for weakly compressible ﬂows, Parallel Computing, 2010, to appear
[5] H. van Emden and U. Meier-Yang: BoomerAMG: a Parallel Algebraic Multigrid Solver and Preconditioner, Applied Numerical Mathematics,
vol. 41, pp. 155-177, 2001 2002 Part III, Lecture Notes on Computer Science, vol. 2331, Springer Verlag, pp. 632–641, 2002
[6] S.V. Patankar: Numerical Heat Transfer and Fluid Flow, Hemisphere Publishing, 1980
[7] Y. Saad: Iterative Methods for Sparse Linear Systems, 2nd edition, SIAM, 2003
[8] H. Tadano and T. Sakurai: On Single Precision Preconditioners for Krylov Subspace Iterative Methods, in: I. Lirkov, S. Margenov, and J.
Wa´sniewski, eds.:Large-Scale Scientiﬁc Computing: 6th International Conference, LSSC 2007, Lecture Notes on Computer Science, vol.
4818, pp. 708–715, Springer Verlag Berlin Heidelberg, 2008
[9] U. Trottenberg, C. Oosterlee, and A. Sch¨uller: MULTIGRID, Elsevier Academic Press, London, 2001
[10] P. Vanˇek, M. Brezina, and J. Mandel: Algebraic Multigrid by Smoothed Aggregation for Second and Fourth Order Elliptic Problems, Computing, vol. 56, pp. 179–196, 1996

