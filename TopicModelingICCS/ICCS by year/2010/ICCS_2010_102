Procedia Computer
Science
Procedia
Computer Science
Science 001(2010)
1–91561–1569
Procedia
Computer
(2012)

www.elsevier.com/locate/procedia

International Conference on Computational Science, ICCS 2010

Numerical solution of level dependent quasi-birth-and-death
processes
Hendrik Baumann, Werner Sandmann
Clausthal University of Technology, Department of Mathematics, Erzstr. 1, D–38678 Clausthal-Zellerfeld, Germany

Abstract
We consider the numerical computation of stationary distributions for level dependent quasi-birth-and-death processes. An algorithm based on matrix continued fractions is presented and compared to standard solution techniques.
Its computational eﬃciency and numerical stability is demonstrated by numerical examples.
c 2012 Published by Elsevier Ltd. Open access under CC BY-NC-ND license.
⃝
Keywords: Continuous-Time Markov Chains, Block-Tridiagonal Generator Matrices, Level Dependent
Quasi-Birth-and-Death Processes, Numerical Solution, Matrix Continued Fractions

1. Introduction
In many applications of multi-dimensional Markov chains, an appropriate numbering of the states yields a chain
with block-tridiagonally structured generator or transition probability matrix, that is a quasi-birth-and-death process
(QBD). QBDs are well suited for modeling various population processes as the abstract term of a population is
interpreted in a wide sense. They apply to a diversity of biological, social, economic and technical processes such as
cell growth, biochemical reaction kinetics, epidemics, demographic trends, or queueing systems, amongst others. For
complex systems where no explicit expressions can be obtained, there is a great interest in numerical analysis.
If all but boundary blocks are identical, then the QBD is said to be level independent and matrix-geometric
techniques [1, 2, 3] provide a powerful machinery. However, level dependent QBDs are often more realistic and
while eﬃcient and stable numerical solution techniques are available for level independent QBDs, there are only a
few approaches that try to exploit the block structure in the level dependent case [4, 5]. Since their numerical stability
is not in general guaranteed one often resorts to numerical solution approaches for general Markov chains [6, 7]
without taking advantage of the speciﬁc QBD structure.
We consider an algorithm for computing stationary distributions of level dependent QBDs based on matrix continued fractions (MCFs). Our description is focused on the continuous-time case but the algorithm applies analogously
to the discrete-time case. Essentially, the algorithm relies on an appropriate generalization of continued fractions to
matrices, the convergence of such MCFs if a certain subdominant solution of a second-order vector-matrix recursion
exists, and a transformation of the system of equations for the stationary distribution of CTMCs into a ﬁrst-order recurrence scheme. The relation of MCFs to ﬁrst-order linear recurrence systems is provided by [8] and the connection
Email addresses: hendrik.baumann@tu-clausthal.de (Hendrik Baumann), werner.sandmann@tu-clausthal.de (Werner
Sandmann)

c 2012 Published by Elsevier Ltd. Open access under CC BY-NC-ND license.
1877-0509 ⃝
doi:10.1016/j.procs.2010.04.175

1562

H. Baumann, W. Sandmann / Procedia Computer Science 1 (2012) 1561–1569

H. Baumann and W. Sandmann / Procedia Computer Science 00 (2010) 1–9

2

to Markov chains as well as its usefulness for analyzing queueing models is exploited in [9, 10]. For the general
theory of MCFs, see [11, 12, 13].
In the remainder of this paper, we describe the MCF algorithm in Section 2 and provide numerical examples and
comparisons to general (not requiring structured generator matrices) numerical solution approaches for Markov chains
in Section 3. Finally, Section 4 concludes the paper and outlines further research directions.
2. Matrix continued fraction algorithm
Let (Xt )t≥0 be an ergodic level dependent QBD, that is an ergodic CTMC with block-tridiagonally structured
generator matrix
⎛
⎞
⎜⎜⎜ Q00 Q01
⎟⎟⎟
⎜⎜⎜ Q
⎟⎟⎟
⎜⎜⎜ 10 Q11 Q12
⎟⎟⎟
⎜⎜⎜
⎟⎟⎟
Q
Q
Q
21
22
23
⎜⎜⎜
⎟⎟⎟
Q = ⎜⎜
(1)
⎟⎟⎟
..
..
..
⎜⎜⎜
.
.
.
⎟⎟⎟
⎜⎜⎜
⎟
⎜⎜⎜
QN−1,N−2 QN−1,N−1 QN−1,N ⎟⎟⎟⎟
⎝
⎠
QN,N−1
QNN

where Qn,n+1 ∈ Rdn ×dn+1 , Qnn ∈ Rdn ×dn , and Qn,n−1 ∈ Rdn ×dn−1 .
Note that in the literature QBDs are often deﬁned as two-dimensional Markov chains with block-tridiagonal generator matrices. Though we shall focus on examples with two-dimensional state spaces when presenting numerical
results in Section 3, we emphasize that in general we do not need any restrictions with regard to the dimensionality of
the state space but only require a block-tridiagonal generator matrix. That is, we deﬁne a (continuous-time) QBD as a
CTMC (Xt )t≥0 = (Nt , Ct ))t≥0 with state space S = N × S , S ⊆ Nd , d ∈ N where Nt is called the level component, Ct
is the control component and direct state transitions are only possible between states whose level components diﬀer
by less than one. Then an appropriate numbering of the states yields the desired block-tridiagonal generator matrix.
The speciﬁc structure of Q shall be used to construct a fast and numerically stable algorithm for computing the
solution x of xQ = 0 which is unique up to a constant factor. By normalizing, the stationary distribution π is obtained.
The system of equations xQ = 0 is equivalent to the system of second-order vector-matrix diﬀerence equations

with row vectors xn ∈ R
n = 0, . . . , N, then

dn

x0 Q00 + x1 Q10
xn−1 Qn−1,n + xn Qnn + xn+1 Qn+1,n

=

0,

=

0,

xN−1 QN−1,N + xN QNN

=

0

and x = (x0 , x1 , . . . , xn ). If
x1

xn+1

Q−1
n,n−1

(2)
n = 1, . . . , N − 1,

(3)
(4)

exists for all n = 1, . . . , N, in particular dn = d for all

= −x0 Q00 Q−1
10 ,

(5)

−1
= −xn−1 Qn−1,n Q−1
n+1,n − xn Qnn Qn+1,n ,

n = 1, . . . , N − 1,

(6)

from which we can obtain x1 = x0 F1 , . . . , xN = x0 F N with matrices F1 , . . . , F N . Substituting in the last equation
yields x0 (F N−1 QN−1,N + F N QNN ) = 0 and we can compute x0 .
However, apart from the fact that Q−1
n,n−1 does not exist in many applications, this algorithm of directly forward computing is numerically cumbersome and extremely unstable, because the solution of the diﬀerence equation
xn−1 Qn−1,n + xn Qnn + xn+1 Qn+1,n = 0, which is compatible with the boundary conditions, is dominated by some other
solutions. This holds as well for the similar backward computing algorithm. The MCF algorithm on the other hand
transforms the second-order vector-matrix diﬀerence equation into a ﬁrst-order recurrence scheme, that is xn+1 = xn Rn
for n = 0, . . . , N − 1. Substituting this recursion into the diﬀerence equations yields
xN−1 QN−1,N + RN−1 QNN
xn−1 Qn−1,n + Rn−1 Qnn + Rn−1 Rn Qn+1,n
x0 (Q00 + R0 Q10 )

= 0,
= 0,
= 0.

(7)
n = 1, . . . , N − 1,

(8)
(9)

1563

H. Baumann, W. Sandmann / Procedia Computer Science 1 (2012) 1561–1569

H. Baumann and W. Sandmann / Procedia Computer Science 00 (2010) 1–9

3

The ﬁrst equations can be used to compute recursively
RN−1

=

Rn−1

=

−QN−1,N Q−1
NN ,

−Qn−1,n Qnn + Rn Qn+1,n

(10)
−1

,

n = 1, . . . , N − 1.

(11)

Then x0 can be estimated by solving the last equation. By iterating xn+1 = xn Rn for n = 0, . . . , N − 1 we obtain the
solution x and normalizing yields the stationary distribution.
The ﬁrst similar algorithm for ﬁnite QBDs due to Gaver et. al. [4] is based on the backward recursion xn = xn+1 Rn
leading to a forward recursion for the Rn . It has the disadvantage that it is not capable of solving inﬁnite systems with,
e.g., a state space truncation method or any other straightforward extension. Bright and Taylor [5] have developed
an algorithm applicable to inﬁnite level dependent QBDs and give a ’physical interpretation’ of the matrices Rn by
identifying the matrix coeﬃcients as conditional expected state sojourn times. From that, it can be shown that Q−1
NN
and Qnn + Rn Qn+1,n −1 exist as well as a nontrivial root x0 of x0 (Q00 + R0 Q10 ) = 0, which is unique up to a constant
factor. Gaver et. al. [4] as well as Bright and Taylor [5] point out that for their methods there is a risk of overﬂow or
underﬂow errors in the recursive computations of the matrices Rn such that numerical stability cannot be guaranteed.
An interpretation of the Rn as MCFs is due to Hanschke [10]. The main diﬀerence between a corresponding algorithm for the analysis of a QBD-type queueing model provided by Hanschke [10] and the method of Bright and
Taylor [5] is the initial value for the recursive computation of the Rn if Q is inﬁnite. While Bright and Taylor [5]
attempt to approximate RN before invoking the main algorithm, Hanschke [10] chooses RN = 0 for a large N, which
we adapt here for general level dependent QBDs. Altogether we get
Algorithm
• Compute RN−1 = −QN−1,N Q−1
NN ;
• Iterate Rn−1 = −Qn−1,n Qnn + Rn Qn+1,n
• Estimate a solution π0

−1

for n = N − 1, N − 2, . . . , 1;

0 of π0 (Q00 + R0 Q10 ) = 0;

• Iterate πn+1 = πn Rn for n = 0, . . . , N − 1;
• Renormalize π.
In practice, for the estimation of π0 one column of Q00 + R0 Q10 is simply ignored and one component of π0 is ﬁxed,
for example π01 = 1. Note that the algorithm can be easily applied to QBDs with inﬁnite state spaces. The algorithm
˜ An augmentation of the row
can also be used if Q is the northwest-corner-truncation of an inﬁnite generator matrix Q.
sums to zero is not necessary if N is chosen large enough.
2.1. Memory-eﬃcient implementation
For all examples we considered in a preliminary study the MCF algorithm is much faster than the Gaussian
elimination and standard iterative methods (power method, Jacobi, Gauss-Seidel), but it turns out that with a too
crude ’direct’ implementation a disadvantage compared to the iterative methods is the larger memory requirement.
The matrix Q is often very sparse, that means in each row and in each column there are only m nonzero components
with m about 3 to 6. This holds for the iteration matrix, too, and it preserve sparsity such that it can be stored using
sparse matrix storage techniques, where the positions (4 bytes each for row index and column index) and the value of
the matrix coeﬃcient (8 bytes) is stored. In this manner each nonzero value requires 16 bytes. In our examples it is
d0 = d1 = d2 = . . . = d, the number of blocks is N + 1. Altogether the storage of the iteration matrix requires about
16 · m · (N + 1) · d bytes.
On the other hand for the MCF algorithm we have to store the matrices R0 , R1 , . . . , RN−1 , which are usually not
sparse and have to be stored in the standard way. Doing so and storing each value with 8 bytes we require 8 · N · d2
bytes for the MCF algorithm. Additionally, in both cases we require 8 · (N + 1) · d bytes for the storage of the
current approximation π(n) and the solution π respectively. Because this is the same for all algorithms we ignore this
requirement in further comparison.

1564

H. Baumann, W. Sandmann / Procedia Computer Science 1 (2012) 1561–1569

H. Baumann and W. Sandmann / Procedia Computer Science 00 (2010) 1–9

4

A simple idea for reducing the memory requirement of the MCF algorithm is not to store all the Rn but only
R0 , RK , R2K , . . .. R0 is used to compute π0 and π1 . Then starting with RK we again compute RK−1 , RK−2 , . . . , R1 and
π2 , . . . , πK+1 subsequently. Afterwards, starting with R2K we compute R2K−1 , R2K−2 , . . . , RK+1 and πK+2 , . . . , π2K+1 . In
this way we only need memory for storing K + NK matrices at the same time. The number K + NK of course is small for
√
√ 2
√
K ≈ N and
√ this choice leads us to a memory requirement for 2 N matrices of size d × d, that means about 16 Nd
bytes. If m N > d the MCF algorithm using this storage scheme requires less memory than the iterative methods.
Of course, the method described above implies that the computing time grows. The worst case behavior would be a
doubling of the computing time, but we will see in the examples that for N >> d not that much additional time is
required. Even with this storage scheme the MCF algorithm is much quicker than the iterative methods.
3. Numerical Examples
For the purpose of comparing the MCF algorithm with standard solution algorithms for Markov chains we have
considered Gaussian elimination, the power method on the uniformized DTMC which has the same stationary distribution as the CTMC, the Jacobi iteration method, and the Gauss-Seidel iteration method, see, e.g., [6, 7] for details.
The methods were implemented in Matlab using the sparse matrix operations for all methods. The iterative
methods were terminated as soon as the total diﬀerence π(n+1) − π(n) of two successive iterates was smaller than
10−16 . We compared the computing time and the memory requirement dependent on the number of levels N and
dependent on the block size d = d0 = d1 = . . . = dN .
3.1. Erlang/M/∞ Queue
Our ﬁrst model is a speciﬁc PH/M/∞ queue, that is a queueing system with phase-type distributed interarrival
times, exponential service times, and inﬁnitely many servers. It can be modeled as a two-dimensional Markov chain
with states (n, k) where n is the number of customers and k is the exponential phase of the interarrival time distribution
for the next arriving customer. The phase-type distribution is speciﬁed by an initial distribution α and the generator
matrix
T t
0 0
of an absorbing Markov chain. Using these notations we get Qnn = T − nμI, Qn,n+1 = tα and Qn,n−1 = nμI. As a
simple example we chose an Erlang distribution, deﬁning α = (1, 0, . . . , 0),
⎛
⎜⎜⎜ −λ
⎜⎜⎜
⎜⎜⎜
⎜
T = ⎜⎜⎜⎜
⎜⎜⎜
⎜⎜⎜
⎝

λ
−λ

λ
..
.

⎞
⎟⎟⎟
⎟⎟⎟
⎟⎟⎟
⎟⎟⎟
..
⎟⎟⎟
.
⎟
−λ λ ⎟⎟⎟⎟⎠
−λ

and

⎛
⎜⎜⎜
⎜⎜⎜
⎜⎜⎜
⎜
t = ⎜⎜⎜⎜
⎜⎜⎜
⎜⎜⎜
⎝

⎞
⎟⎟⎟
⎟⎟⎟
⎟⎟⎟
⎟⎟⎟
⎟⎟⎟
⎟
0 ⎟⎟⎟⎟⎠
λ
0
0
..
.

with λ = 8 and μ = 3. For the dependence on N we additionally ﬁxed d = 10 and chose truncation levels N from 100
(Q has the size 1010 × 1010) to 1000 (Q has the size 10010 × 10010). For the dependence on d we ﬁxed N = 100 and
chose d = 10, 20, 30, 40.
Numerical results for diﬀerent truncation levels and diﬀerent numbers of phases are depicted in Figures 1–4. In
particular, we note that the Jacobi iteration method did not converge at all. Terminating the iteration after 60 seconds
for N = 100 produced an error of ||πQ|| ≈ 0.03. Even direct forward computing produces a smaller error of about
10−7 . Direct backward computing is not possible for Q−1
n,n+1 does not exist.
3.2. A Telecommunications model with impatient customers
Our next model is a telecommunications model with impatient customers that was also discussed, e.g., in [6]. It
deals with the inﬂuence of impatience on telephone customers during a computerized telephone exchange. Customers
arrive at the telecommunication server forming a Poisson process with rate a. At the server, customers are processed
according to the processor sharing discipline with maximum capacity N. Customers arriving at a full server are lost.
If there are n > 0 customers at the server, one of them may leave the server either after being successfully served (rate

1565

H. Baumann, W. Sandmann / Procedia Computer Science 1 (2012) 1561–1569

5

H. Baumann and W. Sandmann / Procedia Computer Science 00 (2010) 1–9

3

10

Gauss
Power method
Jacobi
Gauss−Seidel
MCF
MCF (memory reduction)

2

Computing time (seconds)

10

1

10

0

10

−1

10

−2

10

0

100

200

300

400

500
600
N (truncation level)

700

800

900

1000

Figure 1: Computing times versus truncation level for the Erlang/M/∞ model

2

10

Gauss
Power method
Gauss−Seidel
MCF
MCF (memory reduction)
1

Memory requirement (MB)

10

0

10

−1

10

−2

10

0

100

200

300
400
500
600
700
N (number of blocks in each row and in each column)

800

900

Figure 2: Memory requirements versus truncation level for the Erlang/M/∞ model

1000

H. Baumann, W. Sandmann / Procedia Computer Science 1 (2012) 1561–1569

6

H. Baumann and W. Sandmann / Procedia Computer Science 00 (2010) 1–9

3

10

Gauss
Power method
Jacobi
Gauss−Seidel
MCF
MCF (memory reduction)

2

10

1

Computing time (seconds)

10

0

10

−1

10

−2

10

−3

10

0

5

10

15

20
Number of phases

25

30

35

40

Figure 3: Computing times versus number of phases for the Erlang/M/∞ model

2

10

Gauss
Power method
Gauss−Seidel
MCF
MCF (memory reduction)

1

10

Memory requirement (MB)

1566

0

10

−1

10

−2

10

10

15

20

25
c (number of phases)

30

35

Figure 4: Memory requirements versus number of phases for the Erlang/M/∞ model

40

1567

H. Baumann, W. Sandmann / Procedia Computer Science 1 (2012) 1561–1569

7

H. Baumann and W. Sandmann / Procedia Computer Science 00 (2010) 1–9

μ) or due to impatience (rate n · τ). An impatient customer may quit completely (with a probability 1 − h), or, after an
exponentially distributed ’thinking time’ (rate λ), rejoin the telecommunication server.
If the thinking time is interpreted as a •/M/∞-node, the system is transformed into a two-node open queuing
network. The states of the system are described by pairs (n, k) ∈ {0, . . . , N} × N0 , where n is the number of customers
being served and k is the number of customers ’thinking’. In order to obtain a ﬁnite Markov chain we additionally
assume k ≤ s, where s has to be chosen large enough. The rates of the corresponding Markov chain are as follows.
• new arrival: q(n,k),(n+1,k) = a, n ≤ N,

• successfully served or leaving the network due to impatience: q(n,k),(n−1,k) = μ + nτ(1 − h), n ≥ 1,
• leaving the server due to impatience and ’begin thinking’: q(n,k),(n−1,k+1) = nτh, n ≥ 1, k ≤ s − 1,
• rejoining the server after ’thinking’: q(n,k),(n+1,k−1) = kλ, n ≤ N, k ≥ 1.

The diagonal entries of Q, Q(n,k),(n,k) guarantee that the row sums of Q disappear, all other neglected values are zero.
The desired structure of Q is achieved by
Qn,n−1
Qn,n

=

Qn,n+1

=

=

s
) k, =1
s
q(n,k),(n, ) k, =1 ,
q(n,k),(n+1, ) k,s =1

q(n,k),(n−1,

,

(12)

.

(14)

(13)

We chose a = 0.6, μ = 1, τ = 0.05, h = 0.85 and λ = 5 and s = 10 (dependence on N) respectively N = 550
(dependence on s). Numerical results for diﬀerent numbers of blocks and diﬀerent block sizes are depicted in
4

10

Gauss
Power method
Gauss−Seidel
MCF
MCF (memory reduction)

3

10

Computing time (seconds)

2

10

1

10

0

10

−1

10

−2

10

0

0.5

1

1.5
2
2.5
3
3.5
N (number of blocks in each row and in each column)

4

4.5

5
4

x 10

Figure 5: Computing times versus number of blocks for the telecommunications model

Figures 5–8. The iterative methods converged very slowly or did not converge at all, e.g., for s = 10 and N =
110, 220, 330, 440, 550 we terminated the Jacobi method after 180 seconds and the error was still large (between 10−3
and 10−2 ). The other iterative methods converged for small values of N in reasonable time and the errors ||πQ|| were
about 10−13 for the power method and smaller than 10−15 for the Gauss-Seidel method, the Gaussian elimination and
the MCF algorithm. For larger N we only compared the MCF with the Gaussian elimination. For the largest model in
this example (N = 50000 and s = 10) Q is a 550011 × 550011-matrix. The direct solution methods for matrix-vector
diﬀerence equations in this example for N = 110 and s = 10 produced errors ||πQ|| of about 51 (forward computing)
and 1.3 (backward computing).

1568

H. Baumann, W. Sandmann / Procedia Computer Science 1 (2012) 1561–1569

8

H. Baumann and W. Sandmann / Procedia Computer Science 00 (2010) 1–9

3

10

2

Memory requirement (MB)

10

1

10

0

10

Gauss
Power method
Jacobi
Gauss−Seidel
MCF
MCF (memory reduction)

−1

10

−2

10

0

0.5

1

1.5
2
2.5
3
3.5
N (number of blocks in each row and in each column)

4

4.5

5
4

x 10

Figure 6: Memory requirements versus number of blocks for the telecommunications model

4

10

3

10

2

Computing time (seonds)

10

Gauss
Power method
Jacobi
Gauss−Seidel
MCF
MCF (memory reduction)

1

10

0

10

−1

10

−2

10

0

10

20

30
s (block size −1)

40

50

60

Figure 7: Computing times versus block size for the telecommunications model

4. Conclusion
We have addressed the numerical computation of stationary distributions for level dependent QBDs. A matrix
continued fraction (MCF) algorithm has been proposed and compared with standard solution techniques for Markov
chains. It turns out that the MCF algorithm is much faster than the standard approaches and numerically stable.
Further research includes more extensive empirical comparisons with other algorithms such as aggregation tech-

1569

H. Baumann, W. Sandmann / Procedia Computer Science 1 (2012) 1561–1569

9

H. Baumann and W. Sandmann / Procedia Computer Science 00 (2010) 1–9

2

10

1

Memory requirement (MB)

10

0

10

Gauss
Power method
Jacobi
Gauss−Seidel
MCF
MCF (memory reduction)

−1

10

−2

10

0

10

20

30

40

50

60

70

s (block size −1)

Figure 8: Memory requirements versus block size for the telecommunications model

niques and the Bright-Taylor (BT) algorithm. Also the major similarities and diﬀerences between the MCF algorithm
and the BT algorithm should be worked out. Furthermore, more complex models including state spaces of dimension
higher than two are under consideration. This ﬁts the framework as we only require the desired block-tridiagonal
structure of the generator or transition probability matrix and do not require restrictions on the dimensionality. In
particular, the application to queueing networks and epidemiological models is currently under investigation.
Another topic of further research is the extension of the algorithm beyond QBDs in order to be able to eﬃciently
analyze Markov chains with more general transition structure such as QBDs with catastrophes or Markov chains
skip-free to the right having block lower Hessenberg generator or transition probability matrices.
References
[1] M. F. Neuts, Matrix-Geometric Solutions in Stochastic Models, The John Hopkins University Press, 1981.
[2] G. Latouche, V. Ramaswami, A logarithmic reduction algorithm for quasi birth and death processes, Journal of Applied Probability 30 (1993)
650–674.
[3] G. Latouche, V. Ramaswami, Introduction to Matrix Analytic Methods in Stochastic Modeling, SIAM, 1999.
[4] D. P. Gaver, P. A. Jacobs, G. Latouche, Finite birth-and-death models in randomly changing environments, Advances in Applied Probability
16 (4) (1984) 715–731.
[5] L. Bright, P. G. Taylor, Calculating the equilibrium distribution in level dependent quasi-birth-and-death processes, Stochastic Models 11 (3)
(1995) 497–525.
[6] B. Philippe, Y. Saad, W. J. Stewart, Numerical methods in Markov chain modelling, Operations Research 40 (6) (1992) 1156–1179.
[7] W. J. Stewart, Introduction to the Numerical Solution of Markov Chains, Princeton University Press, 1994.
[8] P. Levrie, A. Bultheel, Matrix continued fractions related to ﬁrst-order linear recurrence systems, Electronical Transactions on Numerical
Analysis 4 (1996) 46–63.
[9] T. Hanschke, Markov chains and generalized continued fractions, Journal of Applied Probability 29 (4) (1992) 838–849.
[10] T. Hanschke, A matrix continued fraction algorithm for the multiserver repeated order queue, Mathematical and Computer Modelling 30
(1999) 159–170.
[11] M. Raissouli, A. Kacha, Convergence of matrix continued fractions, Linear Algebra and its Applications 320 (1–3) (2000) 115–129.
[12] V. N. Sorokin, J. Van Iseghem, Matrix continued fractions, Journal of Approximation Theory 96 (2) (1999) 237–257.
[13] H. Zhao, G. Zhu, Matrix-valued continued fractions, Journal of Approximation Theory 120 (1) (2003) 136–152.

