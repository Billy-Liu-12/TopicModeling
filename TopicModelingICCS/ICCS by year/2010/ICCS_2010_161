Procedia Computer
Science
Procedia Computer
Computer Science
Procedia
Science001(2010)
(2012)1–10
1301–1310

www.elsevier.com/locate/procedia

International Conference on Computational Science, ICCS 2010

Application of Derivative-Free Methodologies to
Generally Constrained Oil Production Optimization Problems
D. Echeverr´ıa Ciaurri∗, O.J. Isebor, L.J. Durlofsky
Department of Energy Resources Engineering, Stanford University, Stanford, CA 94305-2220, USA

Abstract
Oil production optimization involves the determination of optimum well controls (e.g., well pressures, injection
rates) to maximize an objective function such as cumulative oil production or net present value. In practice, this problem additionally requires the satisfaction of physical and economic constraints. Thus, the overall problem represents
a challenging nonlinearly constrained optimization. The cost function and constraints involve calls to a reservoir simulator. Therefore, in many situations gradient information cannot be obtained eﬃciently. This fact motivates the use
of derivative-free (non-invasive, black-box) optimization methods. This work entails a comparative study of a number
of these methods applied to the solution of generally constrained production optimization problems. The derivativefree techniques considered include two pattern search methods (generalized pattern search and Hooke-Jeeves direct
search) and a genetic algorithm. A gradient-based algorithm, in which derivatives are estimated numerically, is also
considered. The performance of the derivative-free algorithms is shown to be quite satisfactory and can be improved
signiﬁcantly when implemented within a distributed computing environment. In order to address the solution of
the generally constrained production optimization problem, diﬀerent constraint handling techniques are investigated.
Penalty functions can be used successfully for this purpose, but they typically involve a tuning/iterative process that is
not exempt, in theory, from potential pitfalls. The results indicate that the ﬁlter method combined with pattern search
suitably addresses this issue while keeping the scheme eﬃcient. We have also explored a parameterless penalty
method for genetic algorithms that appears promising when hybridized with pattern search techniques. In total, the
results of this study demonstrate the applicability of derivative-free methods for challenging reservoir management
problems.
Keywords: nonlinear programming, derivative-free optimization, oil production optimization, reservoir simulation,
closed-loop reservoir modeling
c 201247.55.-t,
⃝
Published89.30.aj,
by Elsevier
Ltd. Open88.10.gc,
PACS:
93.85.Tf,
91.65.My,
47.56.+r
access under
CC BY-NC-ND
license.
2010 MSC: 90C30, 49M99

1. Introduction and problem statement
As a result of economic and population growth, the world total energy demand in 2030 is expected to be approximately 35 percent higher than in 2005, even after accounting for gains in eﬃciency [1]. Oil and natural gas together
∗ Corresponding

author.
Email address: echeverr@stanford.edu (D. Echeverr´ıa Ciaurri)

c 2012 Published by Elsevier Ltd. Open access under CC BY-NC-ND license.
1877-0509 ⃝
doi:10.1016/j.procs.2010.04.145

1302

D.ıaEcheverr´
ıa Ciaurri
et al.L.J.
/ Procedia
1 (2012)
D. Echeverr´
Ciaurri, O.J.
Isebor and
DurlofskyComputer
/ ProcediaScience
Computer
Science 1301–1310
00 (2010) 1–10

2

provided almost 60 percent of global energy in 2005, and forecasts indicate that these fossil fuels will continue to
be signiﬁcant contributors to global energy for decades to come. For this reason, signiﬁcant eﬀort in the oil and gas
industry is being expended to optimize the performance of existing reservoirs. This has led to a great deal of interest
in the idea of eﬃcient closed-loop reservoir management [2], of which production optimization is a key component.
In the context of reservoir management, production optimization entails maximizing a particular objective function, such as cumulative oil produced or net present value (NPV), or minimizing an objective function such as total
water injected, by ﬁnding the optimal set of control variables. In this case the control variables correspond to the sequence (in time) of the well rates or well bottom-hole pressures (BHPs). Since the relationship between the reservoir
dynamics and the control variables is in general nonlinear, searching for the optimal set of controls is a very challenging task. In addition, the problem must usually be solved subject to operational constraints, such as maximum and
minimum BHPs, maximum ﬁeld water injection rates, maximum water cut (fraction of water in the produced ﬂuid),
etc. Thus, the problem is typically a generally constrained optimization.
The production optimization problem can be formally stated as:
min J (u)

u∈Ω⊂Rn

subject to

c (u) ≤ 0,

(1)
n

where J (u) is the objective function (e.g., −NPV or cumulative water produced), u ∈ R is the vector of control
variables (e.g., sequence of BHPs for each well), and c : Rn → Rm represents the nonlinear constraints in the
problem. Bound and linear constraints are included in the set Ω ⊂ Rn . The objective function and constraint variables
are usually computed using the output from a reservoir simulator, which renders function evaluations expensive (as
discussed below).
In all the production optimization cases presented, the problem involves the maximization of (undiscounted) net
present value by adjusting the BHPs of water injection and production wells. Speciﬁcally, we seek to minimize
J (u) = −NPV (u) = −ro qo (u) + cwp qwp (u) + cwi qwi (u) ,

(2)

where ro is the price of oil $/STB , cwp and cwi are the cost of handling produced water and the cost of water injection
$/STB , and qo , qwp and qwi are the cumulative oil production, water production and water injection (STB) obtained
from the simulator. Here ‘STB’ refers to stock tank barrel (1 STB = 0.1590 m3 ). The reservoir simulator used
in all cases is Stanford’s general purpose research simulator (GPRS) [3, 4]. In this simulator, the partial diﬀerential
equations describing the ﬂow of oil, gas and water are discretized using a ﬁnite volume procedure. The system evolves
in time, so time discretization also enters. In practical applications, simulation models may contain O(105 ∼ 106 ) grid
blocks, though the systems considered here are somewhat smaller. The discrete system of equations is nonlinear and
is solved using a Newton-Rhapson procedure. See, e.g., [5] for a general description of the governing equations and
numerical treatments.
This production optimization problem can be addressed using various gradient-based optimization approaches,
though this is not our emphasis here. Derivative information can be estimated numerically in a straightforward manner, but this computation can be expensive and may lack robustness (e.g., selection of the perturbation size in ﬁnite
diﬀerencing is often problematic). The use of eﬃcient adjoint-based techniques for computing the required gradients
greatly reduces the computational eﬀort [6, 7, 8]. However, these procedures require extraction of information from
the reservoir simulator during the course of the computations, and therefore are only feasible with full access to, and
detailed knowledge of, the simulator source code. Even when such access exists, the eﬀort associated with the development and maintenance of the adjoint code is signiﬁcant. We note ﬁnally that most gradient-based strategies cannot
avoid being trapped in local optima.
The goal of this study is to assess a number of derivative-free optimization algorithms. As will be shown, these
methods are easy to implement and most of them parallelize very naturally. Many existing derivative-free implementations can readily handle problems with only bound and linear constraints. As stated earlier, the production
optimization problem is a generally constrained problem, and it is common to have nonlinear constraints. We investigate here various techniques for dealing with these challenging optimizations.
The paper is structured as follows. In Section 2 all the optimizers considered will be brieﬂy described for unconstrained optimization problems. The extension to nonlinearly constrained cases will be presented in Section 3. The
schemes are applied in Section 4 to production optimization cases of reasonable complexity. We conclude the paper
with some conclusions and recommendations.

D. Echeverr´
ıa Ciaurri
et al.and
/ Procedia
Computer
Science
1 (2012)
1301–1310
D. Echeverr´
ıa Ciaurri,
O.J. Isebor
L.J. Durlofsky
/ Procedia
Computer
Science
00 (2010) 1–10

1303
3

2. Derivative-free optimization
Derivative-free methods do not require the explicit calculation of gradients and use just the values obtained from
function evaluations. These methods can be divided into deterministic (e.g., generalized pattern search) and stochastic
methods (e.g., a genetic algorithm). The stochastic component is usually included as a means for dealing with multiple local solutions. Therefore, almost exclusively, deterministic algorithms aim at local optimization. The general
performance of derivative-free methods depends strongly on the number of optimization variables considered, and
these methods have been used successfully in situations when this number is less than a few hundred.
In the last decade, gradient-free optimization methods have been applied in a number of areas. These include
molecular geometry [9], aircraft design [10, 11], hydrodynamics [12, 13] and medicine [14, 15]. Within the oil
industry, most of the derivative-free schemes used are of the stochastic type [16, 17, 18], sometimes hybridized with
a deterministic search [19]. Examples of purely deterministic strategies can be found in [20, 21]. It should be noted
that none of these studies addressed nonlinear constraint handling.
The derivative-free methods considered in this study are two pattern search schemes, generalized pattern search
and Hooke-Jeeves direct search, and a genetic algorithm. In the next section, brief descriptions of these methods for
unconstrained optimization are provided. These schemes can be extended without diﬃculty to deal with bound and
linear constraints.
2.1. Pattern search methods
Pattern search methods are optimization procedures that evaluate the cost function in a stencil-based fashion. This
stencil is sequentially modiﬁed as iterations proceed. The recent popularity of these schemes is due in part to the
development of mathematically sound convergence theory [22, 23].
2.1.1. Generalized pattern search
The generalized pattern search (GPS) [24, 25] optimization method is a subset of pattern search algorithms. In
essence, GPS relies on polling, and it works as follows. At any particular iteration a stencil is centered at the current
solution. The stencil comprises a set of directions such that at least one is a descent direction (this is called a generating
set [22]). If some of the points in the stencil represent an improvement in the cost function, the stencil is moved to
one of these new solutions. Otherwise, the stencil size is decreased. The optimization progresses until some stopping
criterion is satisﬁed (typically, a minimum stencil size). In GPS the polling stencil remains the same along iterations,
and typically induces a coordinate or compass search. Generalized pattern search can be further generalized by polling
in an asymptotically dense set of directions (this set is variable along the iterations). The resulting algorithm is the
mesh adaptive direct search (MADS) [26].
The GPS method parallelizes naturally since, at a particular iteration, the objective function evaluations at the
polling points can be accomplished in a distributed fashion. In the absence of multiple processors, the method typically
requires around n function evaluations per iteration (where n is the number of optimization variables).
2.1.2. Hooke-Jeeves direct search
The Hooke-Jeeves direct search (HJDS) [27] is another pattern search method, that is based on two types of moves:
exploratory and pattern. These moves are illustrated in Figure 1 for some optimization iterations in R2 .
The iteration starts with a base point u0 and a given step size. During the exploratory move, the objective function
is evaluated at successive perturbations of the base point in the search (often coordinate) directions. All the directions
are polled sequentially and in an opportunistic way. This means that if d1 ∈ Rn is the ﬁrst search direction, the ﬁrst
function evaluation is at u0 + d1 . If this represents an improvement in the cost function, the next point polled will
be, assuming n > 1, u0 + d1 + d2 , where d2 is the second search direction. Otherwise the point u0 − d1 is polled.
Upon success at this last point, the search proceeds with u0 − d1 + d2 , and alternatively with u0 + d2 . The exploration
continues until all search directions have been considered. If after the exploratory step no improvement in the cost
function is found, the step size is reduced. Otherwise, a new point u1 is obtained, but instead of centering another
exploratory move in u1 , the algorithm performs the pattern move, which is an aggressive step that moves further in
the underlying successful direction. After the pattern move, the next polling center u2 is set at u0 + 2(u1 − u0 ). If the
exploratory move at u2 fails to improve upon u1 , a new polling is performed around u1 . If this again yields no cost
function decrease, the step size is reduced, keeping the polling center at u1 .

1304

D.ıaEcheverr´
ıa Ciaurri
et al.L.J.
/ Procedia
1 (2012)
D. Echeverr´
Ciaurri, O.J.
Isebor and
DurlofskyComputer
/ ProcediaScience
Computer
Science 1301–1310
00 (2010) 1–10

4

Figure 1: Illustration of exploratory and pattern moves in Hooke-Jeeves direct search. The star represents the optimum.

Notice the clear serial nature of the algorithm. This makes HJDS a reasonable pattern search option when distributed computing resources are not available. Because of the pattern move, HJDS may also be beneﬁcial in situations
where an optimum is far away from the initial guess.
The ﬁrst stages of pattern search schemes use a relatively large stencil size. This feature can be interpreted as a
(rough) global search, and it may provide the algorithm with some robustness against noisy cost functions. Pattern
search methods (and genetic algorithms as well) can be accelerated through the use of inexpensive surrogates, which
can be highly useful given the large number of objective function evaluations that are typically required.
2.2. Genetic algorithms
Genetic algorithms (GAs) are stochastic search techniques that are based on the theory of natural selection. These
algorithms perform a global search by ﬁrst generating a set of possible solutions (a population) and then evaluating
the ﬁtness (i.e., objective function) of all the individuals in this population. Individuals are then ranked, after which
certain operators (typically selection, crossover and mutation) are applied to generate a new population. The selection
operator chooses as parents the individuals with the best objective function values. After selection, the crossover
operator combines the parents to produce children (next population of individuals). During mutation, a speciﬁc part
(e.g., bit or element) of an individual is probabilistically modiﬁed. Refer to [28] for a more detailed description of a
genetic algorithm.
One of the most important parameters in a GA is the population size. With a proper population size, GAs can
explore complex nonsmooth search spaces with multiple local optima and may as a result identify promising regions
in the search space. This often requires a large population size and thus a correspondingly high number of function evaluations. However, as in GPS, the simulations corresponding to a population can be readily performed in a
distributed manner.
3. Generally constrained derivative-free optimization
We now describe nonlinear constraint handling techniques that can be combined with the optimization methods
presented in Section 2.
3.1. Penalty functions
The penalty function method (cf. [29]) for general optimization constraints involves modifying the objective function with a penalty term that depends on the constraint violation h : Rn → R. The original optimization problem
in (1) is thus modiﬁed as follows:
min J (u) + ρ h (u) ,

u∈Ω⊂Rn

(3)

D. Echeverr´
ıa Ciaurri
et al.and
/ Procedia
Computer
Science
1 (2012)
1301–1310
D. Echeverr´
ıa Ciaurri,
O.J. Isebor
L.J. Durlofsky
/ Procedia
Computer
Science
00 (2010) 1–10

1305
5

Figure 2: A (pattern search) ﬁlter at iteration k.

where ρ > 0 is a penalty parameter. It should be noted that the modiﬁed optimization problem still has constraints,
but they are straightforward to handle.
In this work h (u) = ||c+ (u)||22 , with c+ : Rn → Rm deﬁned as c+i (u) = max{0, ci (u)}. Normalizing the constraints
can be beneﬁcial since they are all weighted equally in the penalty term. In [30] penalties other than the quadratic
one have been suggested for pattern search techniques. The optimizations presented in that work are, however, much
simpler than those considered here, so the recommendations given might not be applicable to our problems. In future
research, it will be useful to explore the performance of diﬀerent penalty functions for practical optimization problems.
If the penalty parameter is iteratively increased (tending to inﬁnity), the solution of (3) converges to that of the
original problem in (1). However, in certain cases, a ﬁnite (and ﬁxed) value of the penalty parameter ρ also yields the
correct solution (this is the so-called exact penalty [29]). For exact penalties, the modiﬁed cost function is not smooth
around the solution [29], and thus the corresponding optimization problem can be signiﬁcantly more involved than
that in (3) when ρ is not too large. Our approach here is based on sequentially increasing the penalty parameter, as
described in [29]. This avoids a potentially expensive tuning process associated with the search for the exact penalty.
However, the eventual high value of the penalty parameter can lead to numerical complications.
3.2. Filter method
The penalty function approach is easier to implement than other more sophisticated constraint handling techniques
(e.g., augmented Lagrangian methods [29]) and can be used successfully for many applications. However, as discussed
above, the sequential increase of the penalty parameter can be problematic and/or time consuming. Filter methods
[31, 29] provide a somewhat cleaner means of handling general constraints. Using ﬁlters, the original problem (1)
is viewed as a bi-objective optimization: besides minimizing the cost function J (u), one also seeks to reduce the
constraint violation h (u). The concept of dominance, crucial in multi-objective optimization, is deﬁned as follows:
the point u1 ∈ Rn dominates u2 ∈ Rn if and only if J (u1 ) ≤ J (u2 ) and h (u1 ) ≤ h (u2 ). A ﬁlter is a set of pairs
(h (u) , J (u)), such that no pair dominates another pair. In practice, a maximum allowable constraint violation hmax is
speciﬁed. This is accomplished by introducing the pair (hmax , −∞) in the ﬁlter. An idealized ﬁlter (at iteration k) is
shown in Figure 2.
A ﬁlter can be understood as essentially an add-on for an optimization procedure. The intermediate solutions
proposed by the optimization algorithm at a given iteration are accepted if they are not dominated by any point in the
ﬁlter. The ﬁlter is updated with all the points evaluated by the optimizer at that iteration. It should be noticed that the
optimization search is enriched by considering infeasible points. Filters have been often observed to perform faster
than methods that rely only on feasible iterates.
Pattern search optimization techniques have been combined with ﬁlters [32]. In the pattern search ﬁlter method
the polling center at iteration k can be either the feasible point with lowest cost function, or the infeasible point with

1306

D.ıaEcheverr´
ıa Ciaurri
et al.L.J.
/ Procedia
1 (2012)
D. Echeverr´
Ciaurri, O.J.
Isebor and
DurlofskyComputer
/ ProcediaScience
Computer
Science 1301–1310
00 (2010) 1–10

5

5

10

10

15

15

20

20

25

25

30

30

35

35

40

6

40
5

10

15

20

25

30

35

40

5

10

(a)

15

20

25

30

35

40

(b)

Figure 3: Well conﬁgurations and geological model considered in Section 4.1 (left) and Section 4.2 (right). Injection and production wells are
represented as blue and red circles, respectively. Grid blocks are colored to indicate value of permeability; channelized geology is evident.
Table 1: Optimization parameters for the cases in Section 4

ro
cwp
cwi
Injector BHP range
Producer BHP range
Max. ﬁeld water injection rate
Min. ﬁeld oil production rate
Max. ﬁeld liquid production rate
Max. water cut in any production well

Section 4.1
$80/STB
$36/STB
$18/STB
6000 - 9000 psi
2500 - 4500 psi
0000–
0000–
0000–
0000–

Section 4.2
$50/STB
$10/STB
$05/STB
6000 - 10000 psi
0500 - 04500 psi
1000 STB/day
0450 STB/day
2000 STB/day
0000.5

lowest constraint violation. These two points, 0, JkF and hIk , JkI , respectively, are shown in Figure 2. When the ﬁlter
is not updated in a particular iteration (and thus the best feasible point is not improved), the pattern size is decreased.
Refer to [32, 33] for more details on pattern search ﬁlter methods.
3.3. Parameterless penalty function for genetic algorithms
Within the context of genetic algorithms, there are alternatives to penalty functions that handle general constraints
through use of approaches borrowed from multi-objective optimization (which is also the case with ﬁlters). We have
tested here the parameterless penalty method described in [34]. Other approaches, such as that of [35], could also
be considered. The parameterless penalty method for GAs is based on the tournament selection operator. Within
tournament selection, two individuals are compared in terms of their cost function, and the one with lowest value is
kept for the next generation. The parameterless penalty method modiﬁes the cost function for infeasible individuals
within a given population to Jmax + h (u), where Jmax is the cost function value corresponding to the worst feasible
individual in the population. The constraint violation considered here is again h (u) = ||c+ (u)||22 . The cost function
associated to feasible individuals remains as J (u). Thus, when two feasible (infeasible) solutions are compared, the
one the with lowest cost function (constraint violation) is selected. A feasible individual always prevails over an
infeasible one. In this algorithm, some individuals are optimized based on J (u) and others with respect to h (u), while
keeping a trend that favors feasibility. In contrast to ﬁlter methods, a slightly infeasible individual with a low cost
function can in some cases be discarded (e.g., in a tournament against any feasible solution).

D. Echeverr´
ıa Ciaurri
et al.and
/ Procedia
Computer
Science
1 (2012)
1301–1310
D. Echeverr´
ıa Ciaurri,
O.J. Isebor
L.J. Durlofsky
/ Procedia
Computer
Science
00 (2010) 1–10

1307
7

40

NPV [$ MM]

38
36
34
SQP
GPS
HJDS
GA

32
30
0

5000

10000
15000
Number of simulations

20000

Figure 4: Performance comparison of the optimization algorithms for the case in Section 4.1.
Table 2: Performance summary for the case in Section 4.1

Optimization algorithm
SQP
GPS
HJDS
GA

Number of simulations
09450
12570
00619
17400

Max. NPV $ MM
39.15
39.20
39.16
38.50

4. Oil production optimization cases
The methods described in the preceding sections will be now applied to two realistic (though synthetic) production optimization problems. The ﬁrst case has 80 optimization variables and only bound constraints, while the
second example is a generally constrained production optimization involving 20 optimization variables and 5 general
constraints.
4.1. Production optimization with bound constraints
The reservoir in this case is represented on a two-dimensional 40 × 40 grid. Eight wells (four injectors and four
producers) drive the ﬂow. The wells are arranged in a line drive pattern as shown in Figure 3(a). This model involves
two-phase oil-water ﬂow. The production time frame is 3000 days. The bottom-hole pressure (BHP) of each well
is updated every 300 days (10 control intervals). During control intervals, BHP is held constant. The total number
of optimization variables in this problem is therefore 80. The main problem parameters are shown in Table 1. The
optimization bounds are indicated by means of the injector and producer BHP range. More details can be found
in [36].
The performance of the various optimization algorithms is compared in Figure 4, which presents the evolution of
the net present value (NPV) as a function of the number of simulations. In the absence of distributed computing the
number of simulations is roughly proportional to computing time. A sequential quadratic algorithm (SQP) [29] is also
tested, with gradients estimated numerically by second-order ﬁnite diﬀerencing (with a perturbation size of 0.01 psi).
The initial guess for SQP, GPS and HJDS is the center of the optimization domain (a BHP value of 7500 psi for all
injectors, and 3500 psi for all producers). The initial stencil size for GPS and HJDS is 1000. The GA has a population
size of 300. The associated NPV for this starting point is $31.29 million. (It should be noticed that GA, being a
stochastic population-based search scheme, does not require an initial guess.) The results summarized in Table 2 are
obtained using the same, practically sensible, stopping criterion (increase in NPV below $0.01 million). It should

D.ıaEcheverr´
ıa Ciaurri
et al.L.J.
/ Procedia
1 (2012)
D. Echeverr´
Ciaurri, O.J.
Isebor and
DurlofskyComputer
/ ProcediaScience
Computer
Science 1301–1310
00 (2010) 1–10
0.8

Cumulative Injection [MM STB]

Cumulative Production [MM STB]

1308

Initial guess
HJDS
0.6

0.4

0.2

0
0

500

1000

1500
2000
Time [days]

2500

3000

8

1
0.8

Initial guess
HJDS

0.6
0.4
0.2
0
0

500

1000

1500
2000
Time [days]

2500

3000

Figure 5: Comparison for the case in Section 4.1 of cumulative production and injection proﬁles for the base case and solution found with HJDS.
Left: Cumulative oil (red) and water (blue) production. Right: Cumulative water injection.

be noticed that since the methods search distinctly, it is diﬃcult to set common (and meaningful) stopping criteria.
Nevertheless, the results in Table 2, together with those in Figure 4, oﬀer a broad indication of the potential of the
algorithms studied. As a reference, the SQP algorithm with gradients computed through solution of adjoint equations
(this implementation is described in [8]) yields an optimal NPV of $39.14 million in 142 function evaluations. We
reiterate, however, that simulator-invasive procedures such as this are not the focus of our study.
It is interesting to note that, without a distributing computing environment, HJDS is signiﬁcantly more eﬃcient
than SQP with numerical derivatives. The solutions obtained (BHP controls) in all algorithms, except the GA, are
very similar. In Figure 5 we compare the cumulative production and injection proﬁles of the base case and the HJDS
optimized case. It is evident that the increase in NPV over the base case provided by the optimizer is due to an increase
in cumulative oil produced and a decrease in cumulative water produced. A slightly higher amount of water is injected
in the optimized solution.
We have also tested APPSPACK [37], a distributed computing implementation of GPS. For example, using 36
nodes, a solution with NPV comparable to that of HJDS in Table 2 was obtained in around 250 equivalent simulations1 . We note ﬁnally that additional acceleration can be expected in all the methodologies presented by introducing
surrogates.
4.2. Production optimization with general constraints
The reservoir model used in this case is the same as in Section 4.1, but with diﬀerent cell dimensions and only four
wells (two injectors and two producers; see Figure 3(b)). The reservoir production time frame is 3650 days, divided
into ﬁve control periods of 730 days each. Therefore, the total number of optimization variables in this problem is 20.
The ﬁve derivative-free constraints are maximum total water injection and ﬂuid production rate, minimum total oil
production rate, and maximum water cut at each of the two production wells. The production optimization parameters
for this case, including the bounds for the general constraints, are given in Table 1.
In Table 3 we summarize the performance of the methods for this problem. All of the solutions reported in the
table are feasible. The SQP implementation considered deals with nonlinear constraints via an active set method [29]
and approximates the required derivatives using second-order ﬁnite diﬀerencing (here we use a perturbation size of
0.01 psi). The initial stencil size for GPS and HJDS is 1000. The GA has a population size of 200. In all cases, the
penalty functions were used as in [29], with a penalty parameter ρ iteratively increased by one order of magnitude
from 105 until until 109 for GPS, and until 108 for HJDS and GA. The initial guess for all methods, except the GA,
is again the center of the optimization domain (i.e., constant BHP of 8000 psi for all injectors, and 2500 psi for all
producers). This base case has an associated NPV of $72.90 million. Larger diﬀerences between the NPVs obtained
1 In this context, the equivalent number of simulations is the total number of simulations divided by the speed-up factor obtained with distributed
computing.

D. Echeverr´
ıa Ciaurri
et al.and
/ Procedia
Computer
Science
1 (2012)
1301–1310
D. Echeverr´
ıa Ciaurri,
O.J. Isebor
L.J. Durlofsky
/ Procedia
Computer
Science
00 (2010) 1–10

1309
9

Table 3: Performance summary for the case in Section 4.2

Optimization algorithm
SQP
GPS
GPS
HJDS
HJDS
GA
GA

Constraint handling
active set
penalty function
ﬁlter
penalty function
ﬁlter
penalty function
parameterless penalty

Number of simulations
08059
07378
01903
03808
00134
16800
05400

Max. NPV $ MM
92.88
93.40
90.40
91.64
90.48
93.02
87.23

by the various methods are noticed in Table 3 than were observed in Table 2. This is likely due to the presence
of nonlinear constraints in the optimization, which render the results more sensitive to the choice of algorithm and
associated stopping criterion (increase in NPV below $0.01 million).
The best solution, in terms of maximum NPV for a feasible solution, was found by GPS with penalty function
(NPV of $93.40 million) followed by GA with penalty function, and the SQP method. The ﬁlter combined with
GPS/HJDS is quite promising since it yields a high NPV in much less function evaluations, and it does not require
any penalty parameter. The parameterless penalty function for GA can be noticeably improved if hybridized with a
local optimizer. When combined with GPS and a penalty function, an NPV of $92.79 million is obtained after 6417
function evaluations. This hybrid approach relies, however, on some heuristics (e.g., when to switch from GA to
GPS). Further investigation of hybrid procedures will be very useful for delineating practical approaches. We note
ﬁnally that, within a distributed computing environment, the results in Table 3 can be accelerated according to the
number of nodes available.
In this example, the improvement in NPV over the base case is mainly explained by increased oil production. The
speciﬁcation of maximum ﬁeld water injection and maximum water cut leads to solutions with more eﬃcient use of
water than in the bound-constrained scenario.
5. Conclusions
In this work we have applied non-invasive optimization methodologies, which can handle general constraints, to
two oil production optimization problems of practical relevance. The algorithms considered are attractive in complex
simulation-based optimization scenarios when derivative information is not directly available. We have identiﬁed
generalized pattern search and Hooke-Jeeves direct search as suitable procedures for these optimization problems.
Though a sequential quadratic programming algorithm, with derivatives estimated by ﬁnite diﬀerencing, performs
well in this study, this scheme lacks robustness since the selection of the perturbation size in the gradient approximation is often problematic. Based on our ﬁndings, Hooke-Jeeves direct search is the recommended approach when
distributed computing resources are not available. When general constraints are present, the penalty function method
can provide acceptable results, but a potentially problematic iterative process is required. An alternative approach is to
use the ﬁlter method, which is an add-on for most optimization algorithms and does not require a penalty parameter.
The ﬁlter method combined with pattern search was found to provide promising results in our tests. A parameterless penalty method for genetic algorithms was also studied, and when hybridized with generalized pattern search,
appears to provide a robust optimization strategy. Further research that formalizes this hybridization is needed. All
of the techniques discussed can be signiﬁcantly improved in terms of eﬃciency if surrogates for the cost function and
constraints are available.
Acknowledgements
We are grateful to the industry sponsors of the Stanford Smart Fields Consortium for partial funding of this work.
We also thank the Stanford Center for Computational Earth and Environmental Science for providing distributed
computing resources.

1310

D.
ıa Ciaurri
et al.
/ Procedia
Science 1 Science
(2012) 00
1301–1310
D. Echeverr´
ıa Echeverr´
Ciaurri, O.J.
Isebor and
L.J.
DurlofskyComputer
/ Procedia Computer
(2010) 1–10

10

References
[1]
[2]
[3]
[4]
[5]
[6]
[7]
[8]
[9]
[10]
[11]
[12]
[13]
[14]
[15]
[16]
[17]
[18]
[19]
[20]
[21]
[22]
[23]
[24]
[25]
[26]
[27]
[28]
[29]
[30]
[31]
[32]
[33]
[34]
[35]
[36]
[37]

ExxonMobil, The outlook for energy: A view to 2030, Tech. rep., ExxonMobil Corporation (2008).
J. D. Jansen, D. R. Brouwer, G. Naevdal, C. P. J. W. van Kruijsdiik, Closed-loop reservoir management, First Break 23 (2005) 43–48.
H. Cao, Development of techniques for general purpose simulators, Ph.D. thesis, Dept. of Petroleum Engineering, Stanford University (2002).
Y. Jiang, Techniques for modeling complex reservoirs and advanced wells, Ph.D. thesis, Dept. of Energy Resources Engineering, Stanford
University (2007).
K. Aziz, A. Settari, Petroleum Reservoir Simulation, Applied Science Publishers, 1979.
O. Pironneau, On optimum design in ﬂuid mechanics, J. Fluid Mech. 64 (1974) 97–110.
D. R. Brouwer, J. D. Jansen, Dynamic optimization of waterﬂooding with smart wells using optimal control theory, SPE Journal 9 (4) (2004)
391–402.
P. Sarma, L. J. Durlofsky, K. Aziz, W. H. Chen, Eﬃcient real-time reservoir management using adjoint-based optimal control and model
updating, Computational Geosciences 10 (2006) 3 – 36.
J. C. Meza, M. L. Martinez, On the use of direct search methods for the molecular conformation problem, J. Comput. Chem. 15 (1994)
627–632.
A. J. Booker, J. E. Dennis, Jr., P. D. Frank, D. W. Moore, D. B. Seraﬁni, Optimization using surrogate objectives on a helicopter test example,
Birkh¨auser, Boston, 1998, pp. 49–58.
A. L. Marsden, M. Wang, J. E. Dennis Jr., P. Moin, Trailing-edge noise reduction using derivative-free optimization and large-eddy simulation,
J. Fluid Mech. 572 (2007) 13–36.
R. Duvigneau, M. Visonneau, Hydrodynamic design using a derivative-free method, Struct. Multidisp. Optim. 28 (2004) 195–205.
K. Fowler, J. Reese, C. Kees, J. D. Jr., C. Kelley, C. Miller, C. Audet, A. Booker, G. Couture, R. Darwin, M. Farthing, D. Finkel, J. Gablonsky,
G. Gray, T. Kolda, Comparison of derivative-free optimization methods for groundwater supply and hydraulic capture community problems,
Advances in Water Resources 31 (5) (2008) 743 – 757.
R. Oeuvray, M. Bierlaire, A new derivative-free algorithm for the medical image registration problem, Int. J. Model. Simul. 27 (2007)
115–124.
A. L. Marsden, J. A. Feinstein, C. A. Taylor, A computational framework for derivative-free optimization of cardiovascular geometries,
Computational Methods in Applied Mechanics and Engineering 197 (2008) 1890–1905.
T. J. Harding, N. J. Radcliﬀe, P. R. King, Optimization of production strategies using stochastic search methods, SPE paper 35518 presented
at the 1996 European 3-D Reservoir Modeling Conference, Stavanger, Norway, 16-17 April.
A. S. Cullick, D. Heath, K. Narayanan, J. April, J. Kelly, Optimizing multiple-ﬁeld scheduling and production strategy with reduced risk,
SPE paper 84239 presented at the 2009 SPE Annual Technical Conference and Exhibition, Denver, Colorado, 5-8 October.
V. Artus, L. J. Durlofsky, J. Onwunalu, K. Aziz, Optimization of nonconventional wells under uncertainty using statistical proxies, Computational Geosciences 10 (2006) 389 – 404.
A. Bittencourt, Optimizing hydrocarbon ﬁeld development using a genetic algorithm based approach, Ph.D. thesis, Dept. of Petroleum
Engineering, Stanford University (1997).
J. A. Carroll III, Multivariate production systems optimization, Master’s thesis, Dept. of Petroleum Engineering, Stanford University (1990).
D. Echeverria, T. Mukerji, A robust scheme for spatio-temporal inverse modeling of oil reservoirs, in: R. S. Anderssen, R. D. Braddock,
L. T. H. Newham (Eds.), Proceedings of the 18th World IMACS Congress and MODSIM09 International Congress on Modelling and Simulation, 2009, pp. 4206–4212.
T. G. Kolda, R. M. Lewis, V. Torczon, Optimization by direct search: New perspectives on some classical and modern methods, SIAM
Review 45 (3) (2003) 385–482.
A. R. Conn, K. Scheinberg, L. N. Vicente, Introduction to Derivative-Free Optimization, MPS-SIAM Series on Optimization, MPS-SIAM,
2009.
V. Torczon, On the convergence of pattern search algorithms, SIAM Journal on Optimization 7 (1) (1997) 1–25.
C. Audet, J. E. Dennis Jr., Analysis of generalized pattern searches, SIAM Journal on Optimization 13 (3) (2002) 889–903.
C. Audet, J. E. Dennis Jr., Mesh adaptive direct search algorithms for constrained optimization, SIAM Journal on Optimization 17 (1) (2006)
188–217.
R. Hooke, T. A. Jeeves, Direct search solution of numerical and statistical problems, Journal of the ACM 8 (2) (1961) 212–229.
D. E. Goldberg, Genetic Algorithms in Search, Optimization and Machine Learning, Addison-Wesley Professional, 1989.
J. Nocedal, S. J. Wright, Numerical Optimization, 2nd Edition, Springer, 2006.
J. D. Griﬃn, T. G. Kolda, Nonlinearly-constrained optimization using asynchronous parallel generating set search, Tech. Rep. SAND20073257, Sandia National Laboratories (2007).
R. Fletcher, S. Leyﬀer, P. Toint, A brief history of ﬁlter methods, Tech. Rep. ANL/MCS/JA-58300, Argonne National Laboratory (2006).
C. Audet, J. E. Dennis Jr., A pattern search ﬁlter method for nonlinear programming without derivatives, SIAM Journal on Optimization
14 (4) (2004) 980–1010.
M. A. Abramson, NOMADm version 4.6 User’s Guide, Dept. of Mathematics and Statistics, Air Force Institute of Technology (2007).
K. Deb, An eﬃcient constraint handling method for genetic algorithms, Computer Methods in Applied Mechanics and Engineering 186 (2-4)
(2000) 311 – 338.
P. D. Surry, N. J. Radcliﬀe, I. D. Boyd, A multi-objective approach to constrained optimisation of gas supply networks: The COMOGA
method, no. 993 in Lecture Notes in Computer Science, Springer-Verlag, Berlin, 1995, pp. 166–180.
O. J. Isebor, Constrained production optimization with an emphasis on derivative-free methods, Master’s thesis, Dept. of Energy Resources
Engineering, Stanford University (2009).
J. D. Griﬃn, T. G. Kolda, R. M. Lewis, Asynchronous parallel generating set search for linearly-constrained optimization, SIAM Journal on
Scientiﬁc Computing 30 (4) (2008) 1892–1924.

