Procedia
Computer
Science

Available online at www.sciencedirect.com

Procedia
Computer
Science
00 (2009)
000–000
Procedia
Computer
Science
1 (2012)
475–484

www.elsevier.com/locate/procedia
www.elsevier.com/locate/procedia

International Conference on Computational Science, ICCS 2010

Mobile iris recognition systems: An emerging biometric technology
Jin-Suk Kang a*
a

Jangwee Research Institute for National Defence, Ajou University 443-749, South Korea

Abstract
Wireless communications has matured from a curiosity to a serious business tool. Personal digital assistants have totally replaced
daytimers and notepads. But security has been lacking or weak, making such automation untrustworthy for critical applications.
By adding strong security and authentication, these tools will facilitate trustworthy electronic methods for commerce, financial
transactions, medical data, even prescriptions. In this paper, considering the limited computing power of mobile and portable
devices, a simple but efficient pre-processing method is introduced for iris localization for such iris images. An iris database
(http:// chungbuk.ac.kr/Iris/index.html) with such consideration is created for this paper. The proposed iris pre-processing method
implements the following steps: (a) Automatic segmentation for pupil region, (b) Helper data extraction and pupil detection, etc
(c) Eyelids detection and feature matching. Experiment results show that the proposed iris pre-processing method is performing
well and stable across different iris databases.
c 2012 Published by Elsevier Ltd. Open access under CC BY-NC-ND license.
⃝
Keywords: Mobile Iris; Cellular phones; Iris database; Helper data extraction; Mobile recognition; Personal digital assistants

1. Introductions
Iris recognition uses the unique patterns of the human iris to recognize a given individual [1, 2, 3]. For optimal
performance, it is essential to acquire high-quality iris images. Although not the only factor, optical defocusing is a
major source of iris image quality degradation [4]. Image acquisition is a very important process as iris image with
bad quality will affect the entire iris recognition process. Thus, it is critical to be implemented through good
hardware design. Equally important is the iris image preprocessing step for mobile applications as the iris images
taken by the users are less controllable as in the laboratory environment. Clearly, the failure of iris image
preprocessing can also influence the subsequent processes like feature extraction and enrollment/recognition [5].
This is an exciting time for the cellular phone, wireless communication and PDA communities! Cellular phones
and PDAs are merging and melding in a variety of ways. Pagers have evolved from simple "beepers" to receiving
numeric messages, to text messages, to bidirectional messaging. Palm's Treo is a PDA evolved into a cellular phone.
RIM (Research In Motion) makes the Blackberry device: originally a bidirectional alphanumeric pager, it evolved
into a PDA and now a cellphone. This system has released middleware that can add iris recognition to cellphones
and PDAs. The “Iris Recognition Technology for Mobile Terminals” software reportedly uses existing cameras and
currently target handheld devices running windows Mobile, Windows XP, or Symbian OS. For several years, The
* Corresponding author. Tel.: +82-31-219-2991; fax: +82-31-219-2993.
E-mail address: jskang01@ajou.ac.kr.

c 2012 Published by Elsevier Ltd. Open access under CC BY-NC-ND license.
1877-0509 ⃝
doi:10.1016/j.procs.2010.04.051

476

J.-S. Kang
ComputerScience
Science
(2012)000–000
475–484
Author
name // Procedia
Procedia Computer
001(2010)

system has been providing iris-recognition equipment, but these devices required dedicated infrared cameras. In this
paper its new middleware can now use any camera offering more than 1 megapixels. This permits developers to add
biometric security to easily-stolen mobile devices, without a fingerprint reader or other additional hardware.
In this paper, we propose a new pupil & iris segmentation method apt for the mobile environment. We find the
pupil & iris at the same time, using both information of the pupil and iris. And we also use characteristic of the eye
image. Experimental result shows that our algorithm has good performance in various images, which include motion
or optical blurring, ghost, specular refection and etc., from various environments for Mobile iris recognition system.
2. Overview and Theoretical Comparison
2.1. Fixed focal length
In order to fit the optics into a limited size phone, a few manufacturers incorporate focal mechanisms that
can adjust the distance between lens and optical sensor, thus limiting the depth of field. For a convex lens, the
distance of object ‫݌‬, the distance of image ‫ ݍ‬and the focal length ݂ satisfy the thin lens formula:

Fig. 1. Thin lens equation

ͳΤ‫ ݍ‬൅ ͳΤ‫ ݌‬ൌ  ͳΤ݂ . When objects are far away relative to the focal length ݂ (which is often less than 1cm)
ͳΤ‫  ݌‬ൎ Ͳǡ and ‫ ݍ‬ൎ ݂ǡ the CMOS sensor is therefore usually mounted on the focal plane to capture typical scenes
that are on the order of meters from the device. However, when objects are closer to the camera, ͳΤ‫ ݌‬cannot be
approximated by zero and ͳΤ‫ ݍ‬൏ ͳΤ݂ȭ‫ ݍ‬൐ ݂. From Figure 1 we can see that when an object moves closer to the
lens, its image is pushed further back, as the CMOS sensor stays in front of the image, and captures a blurry (defocused) image. One solution to this problem is to attach a close-up lenes Figure 2 over the phone camera lens. We
found existing close-up lens Figure 2(a) are usually bulky compared to the size of the phone and inconvenient to
carry, so we designed one Figure 2(b) using two magnetic rings to hold the lens. Our close-up lens can be easily
attached to, or removed from, the steel ring mounted on the phone. The attached close-up lens decreases the
effective focal length of the phone camera, compensates for the increase of ͳΤ‫ ݌‬and ‫ ݍ‬remains unchanged.
An alternative view of a close-up lens is that it converts the rays of light from its focal plane into parallel rays of
light. When the parallel light rays go through the original phone camera, they concentrate on the phone camera’s
focal plane, where the CMOS sensor is mounted. With a close-up lens, the camera takes the best picture of object
placed on the focal plane of the close-up lens. For applications which require images taken at a distance D, one
should design a close-up lens with focal length D.

J.-S.
Kangname
/ Procedia
Computer
Science
1 00
(2012)
475–484
Author
/ Procedia
Computer
Science
(2010)
000–000

477

Fig. 2. Close-up lenses : (a) existing lenses (b) our design

2.2. Iris database
In this section, demonstrates in detail experimental results on images of the Iris database (download at (http://
chungbuk.ac.kr/Iris/index.html)), showing high accuracy in object recognition, even on low-quality, low-resolution
image (320Ý240 pixel). We tested the trained object classifier using a reference database of Iris Image: The
database includes images from 20 objects. The iris images are collected using Panasonic BM ET100US
Authenticam using visible light and its operating range in even farther with a distance of 47-53 cm away from the
user. The captured iris images have reflected spot at the pupil and noises like pupil, eyelashes, eyelids and eyebrows.
Figure 3 shows the examples of iris database.

(a)

(b)
Fig. 3. The structure of human iris (a) and iris database (b) (http:// chungbuk.ac.kr/Iris/index.html)

478

J.-S. Kang
ComputerScience
Science
(2012)000–000
475–484
Author
name // Procedia
Procedia Computer
001(2010)

3. Proposed Iris Image Restoration Method
In order to minimize the search space and processing time, an automatic segmentation is performed until the
pupil region is found. Initially, two stages are implemented before the automatic segmentation starts. At first, a 3 x 3
mask Gaussian smoothing with sigma 1.0 is performed throughout the image to filter out the high frequency noise.
Secondly, the image operations are carried out in the predefined area as shown in Figure 4. A technical rejection of
iris image occurs when the estimated location of pupil falls outside the predefined area. It may mean that there are
high chances that the iris portion is excluded totally or partially from the image during iris image acquisition. In
general, the pupil looks dark because it absorbs the light that enters into our eye. It consists of a group of the darkest
pixels in the iris image. Therefore, the proposed automatic segmentation process begins to search the darkest point
in the predefined area. After that, a threshold value of intensity (v) is calculated based on the intensity of the darkest
point (s) with some tolerance and set as a cutoff intensity to decide whether a given pixel is dark as in the case of the
pixels in the pupil region.

H e ig h t / 1 0

W id th / 1 0

P re d e fin e d
a re a

W id th / 1 0

H e ig h t / 1 0

Fig. 4. Predefined area for the image operation

3.1. Helper data extraction
The schematic diagram of helper data extraction scheme in the proposed iris cryptosystem is shown in Figure 5
(a). The salting transform consists of two operations, namely, BCH encoding [6] and an exclusive-or operation. Let
H be a (‫ܯ‬ூ ൏ ‫ܯ‬௄ ) BCH encoding function, which takes a message K of length ‫ܯ‬௄ (‫ܯ‬௄ ൏ ‫ܯ‬ூ ) and appends
(‫ܯ‬ூ െ ‫ܯ‬௄ ) error correcting symbols to it in order to generate a codeword ‫ ܫ‬ൌ ‫ܪ‬ሺ‫ܭ‬ሻ of length ‫ܯ‬ூ . In particular, we
employ a primitive binary BCH encoding scheme, where ‫ܯ‬ூ is chosen to be (ʹ௠ െ ͳ) and ݉ is an integer greater
than or equal to 3. The values of ‫ܯ‬ூ and ‫ܯ‬௄ determine the number of errors that can be corrected by the BCH
coding scheme.

(a)

J.-S.
Kangname
/ Procedia
Computer
Science
1 00
(2012)
475–484
Author
/ Procedia
Computer
Science
(2010)
000–000

479

(b)
Fig. 5. Schematic diagram of the iris cryptosystem based on iris-code features. (a) Enrollment or helper data extraction and (b) authentication or
key recovery

Let ‫ ்ܫ‬be a iriscode template of length ܰூ bits that is to be secured using the fuzzy vault framework. First, we
௝
partition the template ‫ ்ܫ‬into ‫ ݎ‬non-overlapping components [ ‫்ܫ‬ଵ ǡ ‫்ܫ‬ଶ ǡ ‫ ڮ‬ǡ ‫்ܫ‬௥ ] such that each component ‫ ்ܫ‬ሺ݆ ൌ
ͳǡʹǡ ‫ ڮ‬ǡ ‫ݎ‬ሻ contains exactly ‫ܯ‬ூ bits. Here, ‫ ݎ‬is selected such that ‫ܯݎ‬ூ ൒ ܰூ . When ܰூ  ൏ ‫ܯݎ‬ூ , appropriate number
(i.e., ሺ‫ܯݎ‬ூ െ ܰூ ሻሻ of zero bits are appended to the iriscode template ‫ ்ܫ‬to obtain the components [‫்ܫ‬ଵ ǡ ‫்ܫ‬ଶ ǡ ‫ ڮ‬ǡ ‫்ܫ‬௥ ]. Next,
we randomly generate ‫ ݎ‬binary vectors ‫ ܭ‬ଵ ǡ ‫ ܭ‬ଶ ǡ ‫ ڮ‬ǡ ‫ ܭ‬௥ each of length ‫ܯ‬௄ bits. These ‫ ݎ‬random binary vectors
together constitute the transformation key ‫ ܭ‬ଵ of length ‫ܯݎ‬௄ bits, i.e., ‫ܭ‬ଵ ൌ ሾ‫ ܭ‬ଵ ǡ ‫ ܭ‬ଶ ǡ ‫ ڮ‬ǡ ‫ ܭ‬௥ ሿ. BCH encoder H is
applied individually to the binary vectors ‫ ܭ‬ଵ ǡ ‫ ܭ‬ଶ ǡ ‫ ڮ‬ǡ ‫ ܭ‬௥ to obtain the codewords ‫ܪ‬ሺ‫ ܭ‬ଵ ሻǡ ‫ܪ‬ሺ‫ ܭ‬ଶ ሻǡ ‫ ڮ‬ǡ ‫ܪ‬ሺ‫ ܭ‬௥ ሻ. Note
that ‫ܪ‬ሺ‫ ܭ‬ଵ ሻǡ ݆ ൌ ͳǡ ʹǡ ‫ ڮ‬ǡ ‫ ݎ‬is a binary vector of length ‫ܯ‬ூ . Finally, an exclusive-or operation is performed between
the ‫ ݎ‬codewords generated by the BCH encoder and the corresponding components of the iriscode template to
obtain the components of the transformed iriscode. The transformed iriscode template ‫ כܫ‬can be represented as
௝
௝
௝
[‫כܫ‬ଵ ǡ ‫כܫ‬ଶ ǡ ‫ ڮ‬ǡ ‫כܫ‬௥ ], where the ݆ ௧௛ component ‫ כܫ‬is given by ‫ כܫ‬ൌ  ‫ܪ ْ  ்ܫ‬ሺ‫ܭ‬௝ ሻǡْ denotes the exclusive-or operation and
݆ ൌ ͳǡ ʹǡ ‫ ڮ‬ǡ ‫ݎ‬. Hence, the complete salting transformation can be represented as a function ‫ܨ‬ଵ that takes iriscode
template ‫ ்ܫ‬and the transformation key ‫ܭ‬ଵ as inputs and generates the transformed iriscode ‫ כܫ‬such that ‫ כܫ‬ൌ
‫ܨ‬ଵ ሺ‫ ்ܫ‬ǡ ‫ܭ‬ଵ ሻ.
The transformation key ‫ܭ‬ଵ is secured using the fuzzy vault construct as follows. Since the value of ‫ܯ‬௄ is set to 16
in our implementation, we can directly represent the ‫ ݎ‬component of the transformation key as elements in the
Galois field ‫ܨܩ‬ሺʹଵ଺ ሻ. Our authentication (or key recovery) scheme has been designed in such a way that it does not
require the relative order information between the components of key ‫ܭ‬ଵ . Hence, the components of the
୰
transformation key ‫ܭ‬ଵ can be directly represented as an unordered set ܺ ൌ ൛୨ ൟ , where ୨ is the representation of
୨ୀଵ 

the component ‫ܭ‬௝ in ‫ܨܩ‬ሺʹଵ଺ ሻ . Let ܻ ൌ ሼ୩ ሽୱ୩ୀଵ  be the set of chaff points such that ‫ݕ‬௞ ‫ܨܩ א‬ሺʹଵ଺ ሻ , ‫ݕ‬௞ ് ‫ݔ‬௝ ,
‫ ݆׊‬ൌ ͳǡʹǡ ‫ ڮ‬ǡ ‫ ݎ‬and ݇ ൌ ͳǡʹǡ ‫ ڮ‬ǡ ‫ݏ‬. Based on these two sets X and Y and a different key ‫ܭ‬ଶ (referred to as the vault
key with size 16n bits), we can construct a fuzzy vault ܸ ൌ ሼሺܽ௜ ܾ௜ ሻሽ்௜ୀଵ , ‫ ݐ‬ൌ ‫ ݎ‬൅ ‫ ݏ‬by following steps 5 through 8 in
the vault encoding algorithm presented in Figure. 6. As pointed out earlier, the transformed iriscode ‫ כܫ‬and the vault
ܸ together constitute the helper data in the iris cryptosystem.

480

J.-S. Kang
ComputerScience
Science
(2012)000–000
475–484
Author
name // Procedia
Procedia Computer
001(2010)

Fig. 6. Propose implementation of vault encoding

3.2. Feature matching
For matching, we assign a weight to each feature vector and also apply a weighted similarity measure procedure
in our work. In order to achieve a higher accuracy of recognition rate, we assign a weight to each characteristic
value. The main idea is to assign a higher value of weights to a feature which has more significance in the iris part.
We observed that upper and lower part of the iris is occluded by the eyelids and eyelashes in maximum cases. From
our study we also observed that the most discriminating iris features are present in the iris region near to the pupil
boundary. Hence, we apply a weighted matching procedure to compare the two feature vectors. Figure 7() is the
eight sub divided regions in normalized image corresponding to iris structure which is shown in Figure 7(b). The
maximum weight is given to that sub images which are more visible and the least weight is given to that sub images
which are maximally occluded by eyelids and eyelashes. The weight assignment procedure is shown in Figure 7.
In Figure 7, we see that the regions ‫ܫ‬ଵ and ‫ܫ‬ଷ are given maximum weight (1.0) because these regions are closer to
the pupil boundary and are not obstructed by eyelashes or eyelids in majority of the cases. The ‫ܫ‬ହ and ‫ ଻ܫ‬regions are
far away from the pupil boundary but the chance of blocking by eyelashes or eyelids is comparatively low. So, we
assign the second maximum weight (0.75) to the regions ‫ܫ‬ହ and ‫ ଻ܫ‬. The maximum chance of eyelashes and eyelids
occlusion is ‫ ଺ܫ‬and ‫ ଼ܫ‬regions and these regions are also far away from the pupil boundary. Hence, we assign the
least weight (0.25) to these regions. The ‫ܫ‬ଶ and ‫ܫ‬ସ regions are nearer to the pupil boundary but chance of occlusion
by eyelashes and eyelids are high, so we assign 0.50 weight to these regions. We decide the above mentioned weight
assignment empirically after a several run on the iris image databases used in our work.

Fig. 7. Partitioning of a normalized iris image (a) and weight assignment in eight subdivided iris blocks (b)

J.-S.
Kangname
/ Procedia
Computer
Science
1 00
(2012)
475–484
Author
/ Procedia
Computer
Science
(2010)
000–000

481

The similarity measure is calculated by taking the weighted inner product we denote it as ܵ of the weighted
feature vector under test and each of the seven values of features corresponding to seven angles of rotation. We
consider the minimum value among these seven product value as a similarity value. Equation (1) is used for
calculating the inner product of two characteristic vectors.
ܵ ൌ ݉݅݊ఏ ൜
೙

ଵ
ଵହଶ

೙

σ଼௡ୀଵ ‫ܣ‬ூ ௡ σଵଽ
௜ୀଵ

೙

೅
ఏ௙ᇱೃ
೔ ௙ᇱ೔

ଵ଺

ൠ (1)

೙

Where, ߠ݂Ԣோ௜ and ݂Ԣ்௜ are the ݅-th feature of the ݊-th sub image in ߠ rotation corresponding to the feature vector
of the user that has been already registered and the feature vector of the user that is generated from the iris image of
the eye of the user, respectively. ‫ܣ‬ூ ௡ is the weight corresponding to the sub image shown in Figure 7. ‫ܣ‬ூ ௡ is defined
in Equation (2).
ͳǤͲͲ݂݅݊ ൌ ͳ‫͵ݎ݋‬
ͲǤ͹ͷ݂݅݊ ൌ ͷ‫ݎ݋‬͹
(2)
‫ܣ‬ூ ௡ ൌ  ൞
ͲǤͷͲ݂݅݊ ൌ ʹ‫ݎ݋‬Ͷ
ͲǤʹͷ݂݅݊ ൌ ͸‫ݎ݋‬ͺ
The ߠ݂Ԣ௜௫ is defined in Equation (3).

௡
൅Ͷ݂݅ߠ݂௡௘௪
೟
‫ۓ‬
௡
ۖ൅ͳ݂݅ߠ݂௡௘௪
೟
௡
ߠ݂Ԣ௜ ൌ 
௡
‫۔‬െͳ݂݅ߠ݂௡௘௪೟
ۖ
௡
‫ە‬െͶ݂݅ߠ݂௡௘௪೟
where ݅ ൌ ͳǡʹǡ ‫ ڮ‬ǡͳͻܽ݊݀݊ ൌ ͳǡʹǡ ‫ ڮ‬ǡͺ

ൌ ͳͳ
ൌ ͳͲ
(3)
ൌ Ͳͳ
ൌ ͲͲ

3.3. Eyelids detection
Previously, some researchers [7, 8] had fixed the predefined area to select the iris portion. However, every person
has different type of eyelids occlusion on iris portion as shown in Figure 8. The problem occurs if the system fixes
the predefined region but it is occluded by eyelid. Definitely, the eyelids detection [3, 5] method provides a good
solution to detect the eyelids. However, a faster way can be done by simply detecting the upper and lower eyelids to
check if they exist within the iris region. It is possible to use the contrast between the iris portion and the eyelids to
identify the iris portion which is not occluded by the eyelids. By calculating the average intensities of pixels in the
iris portion in the radial direction for every direction from the center of pupil, it is noticed from Figure 8 that the iris
portion not occluded by the eyelids can be detected. Figure 8(a) shows an example of upper eyelid occlusion with
high average intensities at upper portion of iris. On the other hand, Figure 8(b) shows an example of lower eyelid
occlusion with high average intensities at lower portion of iris. As a result, the noise free iris region can be chosen
based on the angle with low average intensities.

Fig. 8. Different type of eyelids occlusion in the iris portion : (a) upper eyelid occlusion and (b) lower eyelid occlusion

482

J.-S. Kang
ComputerScience
Science
(2012)000–000
475–484
Author
name // Procedia
Procedia Computer
001(2010)

3.4. Eyelashes detection
A modified unsharp masking is proposed to detect the eyelashes within the iris portion. This method does not
require any threshold or edge detection. Moreover, it is fast by reusing the Gaussian smoothing results already done
during the iris localization step. The modified unsharp masking has two sub-steps. First, the difference of original
image and Gaussian smoothed image is calculated. After this sub-step, only the high frequency components in the
iris image are retained. Next, all of the high frequency components are digital enhanced to show the strong edge
points. Figure 9 shows some examples of proposed modified unsharp masking. The edge points that fall within the
inner and the outer boundaries of iris are considered as eyelashes. In comparison, the proposed method can be done
automatically as it does not involve edge detection method like canny edge detector which need user to provide high
and low threshold values.

Fig. 9. Examples of modified unsharp masking results to detect the eyelashes. The two circles are the outer and inner boundaries of iris,
respectively.

4. Experimental Results and Discussion
For experiment setup, a prototype system was developed in Microsoft Visual Embedded C 6.0, and installed on a
4GHz Intel machine with 512MB of RAM. The proposed iris preprocessing method is tested on iris databases with a
total number of 20 iris images and the method proves its ability to detect inner and outer boundaries of the iris. The
mobile iris system architecture has been primarily designed to match the functional specifications required in the
P2P operation mode. [Figure. 10].

“Database”
Fig. 10. Mobile iris system architecture and service models

J.-S.
Kangname
/ Procedia
Computer
Science
1 00
(2012)
475–484
Author
/ Procedia
Compputer
Science
(2010)
(
000–000

483

Fig. 111. Some screenshhots and a picturee of the mobile irris recognition sysstem

F
Figure 11 shoows the mobille phone that we used. It was
w a Samsung
g mobile phonne SPH-2300 [11], which has
h a
3.2 m
mega-pixels camera
c
modulle. In the first test, we measured the accu
uracy (hit ratiio) of our algoorithm. Tests were
perfo
formed on 4000 face imagess captured from 100 personns. These facee images weree not used forr unsharp massking
algoorithm [9, 10]. The test im
mages consisteed of the folloowing four caategories: imaages without glasses or co
ontact
lensees (100 imagges) in indoorr environmentts; images wiith glasses an
nd contact lennses (100 imaages); and im
mages
withhout glasses orr contact lensees (100 images) in outdoor environmentss.
E
Experimental results
r
showedd that the pupiil detection raate was 99.5%
% (for images without
w
glassees or contact leenses
in inndoor and outtdoor environnments) and 99%
9
(for imagges with glassses or contacct lenses in inndoor and outtdoor
enviironments). Thhe iris detectioon rate was 999.5% (for images without glasses or conttact lenses in iindoor and outtdoor
enviironments) andd 98.9% (for images
i
with glasses
g
or conttact lenses in indoor
i
and ouutdoor environnments).
W
When we measured perform
mance only using the unshaarp masking algorithm, the detection ratee was almost 98%.
But there were also many falsse alarms (e.gg., when noneeye regions such
s
as eyebrrows or framees of glasses were
detected as correcct eye regionss). Experimenntal results witth 400 face im
mages showedd that the falsse alarm rate using
u
onlyy the unsharp masking eye detector was almost 53%. So, to solve th
hese problems, we used booth the inform
mation
of thhe corneal speecular reflectionns and the unnsharp maskinng eye detecto
or. These resuults showed thhat the correcct eye
detection rate wass more than 999% (as mentiooned above) annd the false allarm rate was less than 0.2%
%.
5. C
Conclusions
Inn this paper, thhe proposed iris
i preprocesssing method shows
s
an enco
ouraging resullt in large num
mber of iris im
mages
acrooss different irris databases. Therefore, a many-to-onee platform forr the iris recoognition serveer can be setu
up to
receive iris imagees from differrent type of caameras. Besiddes, it extractss the noises liike pupil, eyeelids and eyelaashes
withhin iris region automaticallyy after the iriss localization process. Therefore, it is very useful to select a noisee free
iris rregion for furrther processiing. On the other
o
hand, real-time iris im
mages were collected
c
for iiris database(http://
h
chunngbuk.ac.kr/Iris//index.html) ussing portable web
w camera (PDA Cameraa, PocketPC Camera,
C
etc). A
Also, we propose a
pupiil and iris loccalization meethod in varioous environments and iris preprocessingg method with noise deteection
analyysis has provvided good fouundation towaards developiing an accuratte portable iriis recognitionn system. In future
f
workk, we plan to test
t our algoriithm over a wide
w range of camera
c
platforrms and applyy it to auto-foccusing iris cam
meras.
Ack
knowledgemen
nts
This reseaarch was suppported by the MKE (The M
Ministry of Kn
nowledge Ecoonomy), Koreaa, under the ITRC
I
(Infoormation Tecchnology Ressearch Centerr) support program supervised by thhe NIPA(Natiional IT Ind
dustry
Prom
motion Agency (NIPA-20100-(C1090-10221-0011))

484

J.-S. Kang
ComputerScience
Science
(2012)000–000
475–484
Author
name // Procedia
Procedia Computer
001(2010)

References
[1] J. G. Daugman., “High confidence visual recognition of persons by a test of statistical independence,” IEEE Trans. Pattern Anal.Mach. Intell.,
vol. 15, no. 11, pp. 1148–1160, Nov. 1993.
[2] J. G. Daugman., “Demodulation by complex-valued wavelets for stochastic pattern recognition,” Int. J. Wavelets, Multi-Resolution Inf.
Process., vol. 1, no. 1, pp. 1–17, 2003.
[3] J. G. Daugman., “How iris recognition works,” IEEE Trans. Circuits Syst. Video Technol., vol. 14, no. 1, pp. 21–30, Jan. 2004.
[4] B. J. Kang and K. R. Park., “A study on iris image restoration,” in Proc. AVBPA, vol. 3546, pp. 31–40, Jul. 2005.
[5] J. Cui, Y. Wang, T. Tan, L. Ma, and Z. Sun., “A Fast and Robust Iris Localization Method Based on Texture Segmentation,” SPIE Defense
and Security Symposium, 2004.
[6] S. Lin and D. J. Costello., “Error Control coding: Fundamentals and Applications,” Prentice Hall, Englewood cliffs, USA, 1983.
[7]
W. Yang, L. Yu, G. Lu and K. Wang., “Iris Recognition Based on Location of Key Points,” in Proc. ICBA, LNCS 3072, pp. 484-490,
2004.
[8] D.H. Cho, K.R. Park and D.W. Rhee., “Real Time Iris Localization for Iris Recognition in Cellular Phone,” in Proc SNPD/SAWN’05, pp.
254-259, May.2005.
[9] Z.Ou, X. Tang, T. Su, and P. Zhao., “Cascade AdaBoost classifiers with stage optimization for face detection,” in Proceedings of
International Conference on Biometrics (ICB ’06), vol. 3832 of LectureNotes in Computer Science, pp. 121–128, January 2006.
[10] Y. Freund and R. E. Schapire., “A decision-theoretic generalization of on-line learning and an application to boosting,” Journal of Computer
and System Sciences, vol. 55, no. 1, pp. 119–139, 1997.
[11] http://www.anycall.com (accessed on 2009. 11. 6.)
[12] T. Yamane., “Statistics,” in An Introductory Analysis, 2nd ed. New York: Harper & Row, 1967.
[13] S. Kim, J. Shin, and J. Paik., “Real-time iterative framework of regularized image restoration and its application to video enhancement,”
Real-Time Imaging, vol. 9, no. 1, pp. 61–72, Feb. 2003.
[14] R. P. Wildes., “Iris recognition,” in Biometric Systems. London, U.K.: Springer-Verlag, 2005.
[15] A. Mansfield and J. Wayman., “Best Practices in Testing and Reporting Performance of Biometric Devices,” UK Government Biometrics
Working Group, 2002.
[16] J. Wayman., “Technical testing and evaluation of biometric identification devices,” in Biometrics: Personal Identification in Networked
Society A. Jain et al., Eds. Norwell, MA: Kluwer, 1999.
[17] R. Jain, R. Kasturi, and B. G. Schunck., “Machine Vision,” New York: McGraw-Hill, 1995.
[18] B. J. Kang and K. R. Park., “A study on iris image restoration based on focus value of iris image,” J. IEEK, vol. 43, no. 2, pp. 30–39, Mar.
2006.
[19] International Biometrics Group, Independent Testing of Iris Recognition Technology Final Report, 2005. (accessed on 2007. 04. 26).
[20] R. P. Wildes., “Automated iris recognition: An emerging biometric technology,” Proc. IEEE, vol. 85, no. 9, pp. 1348–1363, 1997.
[21] R. P. Wildes, J. C. Asmuth, G. L. Green, S. C. Hsu, R. J. Kolczynski, J. R. Matey, and S. E.McBride., “A system for automated iris
recognition,” in Proc. IEEE Workshop Mach. Vis. Appl., pp. 121–128, 1994.
[22] N. Macmillan and C. Creelman, Detection Theory: A Users Guide. New York: Cambridge Univ. Press, 1991.
[23] A. B. J. Teoh, K.-A. Toh and D. C. L. Ngo., “Random Multispace Quantization as an Analytic Mechanism for BioHashing of Biometric and
Random Identity Inputs,” IEEE Transaction on Pettern Analysis and Machine Intelligence, Vol. 28, No. 12, pp. 1892-1901, Dec 2006.

