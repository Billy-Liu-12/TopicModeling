Procedia Computer Science 1 (2012) 37–46
Available online at www.sciencedirect.com

Procedia Computer Science 00 (2009) 000–000

Procedia
Computer
Science

www.elsevier.com/locate/procedia

www.elsevier.com/locate/procedia

International Conference on Computational Science, ICCS 2010

Data Mining and Integration for Predicting Significant
Meteorological Phenomena
Juraj Bartoka, Ondrej Habalab, Peter Bednarc*, Martin Gazaka, Ladislav Hluchýb
a
MicroStep-MIS spol. s r.o., Čavojského 1, 84108 Bratislava, Slovenská republika
Ústav informatiky SAV, Dúbravská cesta 9, 84507 Bratislava, Slovenská republika
c
Fakulta elektrotechniky a informatiky TUKE, Letná 9 04200 Košice, Slovenská republika
b

Abstract
This paper describes the planned contribution of the project Data Mining Meteo (DMM) to the research of parametrized models
and methods for detection and prediction of significant meteorological phenomena, especially fog and low cloud cover. The
project is expected to cover methods for integration of distributed meteorological data necessary for running the prediction
models, training models and then mining the data in order to be able to efficiently and quickly predict even randomly occurring
phenomena. We present the methods and technologies we will use for integration of the input data, distributed on different
vendors’ servers. The meteorological detection and prediction methods are based on statistical and climatological methods
combined with knowledge discovery – data mining of meteorological data (SYNOP, METAR messages, weather radar imagery,
“raw” meteorological data from stations, satellite imagery and results of common meteorological prediction models).

c 2012 Published by Elsevier Ltd. Open access under CC BY-NC-ND license.
⃝
Meteorological prediction; fog; data mining; data integration; cloudiness

1. Significant and Hazardous Meteorological Phenomena
Recently the interest in short-term weather warnings with higher localization accuracy has been heightened,
especially in connection with the influence of significant and hazardous meteorological events on our society
(traffic, agriculture, tourism and public safety).
Fog has significant impact on human activities (let us just mention air and road traffic and shipping) and an
improvement of fog prediction methods is of importance to the human society as a whole.
Low cloud cover over a significant portion of the sky has impact on air traffic management (according to ICAO
rules), and can result even in a complete stop of takeoffs and landings.
Precipitation, resp. information about its occurrence is important as an input to various hydrological models of
river flows and flooded areas, for meteorologists, agriculture, or as an input necessary for any local flood warning

* Corresponding author: Ondrej Habala. Tel.: +421 2 59411176; fax: +421 2 54771004.
E-mail address: ondrej.habala@savba.sk

c 2012 Published by Elsevier Ltd. Open access under CC BY-NC-ND license.
1877-0509 ⃝
doi:10.1016/j.procs.2010.04.006

38

J. Bartok et al. / Procedia Computer Science 1 (2012) 37–46

Juraj Bartok et al./ Procedia Computer Science 00 (2010) 000–000

system.
Data mining is emerging as a suitable method for extracting patterns from extensive sets of heterogeneous data
related to prediction of meteorological phenomena [5],[6].
This paper describes certain significant meteorological phenomena, and the possibilities of enhancing their shortterm prediction using data mining. To this end we have created a consortium of business (MicroStep-MIS spol. s r.o.
) and scientific organizations (Institute of Informatics of the Slovak Academy of Sciences (II SAS), Faculty of
Electrical Engineering and Informatics of the Technical University of Košice (FEI TUKE)).
The data mining process is currently in the phase of data preparation and understanding.
1.1. Fog
We start with an example of currently used approach to the prediction of visibility-reducing fog (see [7]). It starts
with a common 3D meteorological model executed for a limited region; its outputs are converted using empirical
formulae into visibility [4]. This approach by itself cannot achieve results of satisfactory quality and common
meteorological models often fail to handle inversion weather conditions, which commonly produce fog. Therefore
there are several experimental models in development worldwide, which further process the results of common
meteorological model: 1D physical fog modeling methods, statistical post-processing of model outputs [2],[3]. The
result is then interpreted by a meteorologist, who takes into account further factors – mainly his/her experience with
meteorological situations and local conditions, satellite imagery, real-time data from meteorological stations
suggesting that fog has started to form, or conditions are favorable for the occurrence of one, conditions of the soil
in the target locations, snow cover, recent fog occurrences, etc.
1.2. Low Clouds

Ceilometers are sensors routinely deployed at airports for measurement of cloud base heights above the points of
their installation. The extent of cloud cover (cloud amount) is assessed by a meteorological observer and reported
periodically and in the case of significant change. Sometimes the ceilometers data is used to determine also the
cloud amount using the FAA method, where the result is a simple combination of laser reflection counts in different
height categories. The method uses data from only one ceilometer. We will use data mining to obtain more
comprehensive 2D information on both cloud base heights and cloud amount. We will mine data from several
deployed ceilometers, making the cloud information continuous and available in fully automated operation also.

1.3. Intense Precipitation
Our motivation for mining of weather radar data is the possibility of combining less accurate spatial information
(weather radar data) and more accurate point information (rain gauge) into a more precise spatial precipitation
information. This is currently the best option for determining the amount of rainfall in a watershed. The amount
necessary for a flood state can be obtained by hydrological modeling. The quality of the hydrological model output
is in direct relation to the quality of its inputs.
2. Project Data Mining Meteo
The project consortium consists of business partner with extensive experience in meteorology (both commercial
and research projects [15]) and two scientific partners with experience in data integration and data mining
[10][16][17][18]. The partners share also common project experience form the past [14]. In more detail, MicroStepMIS spol. s r.o. develops, deploys and markets monitoring and information systems in the fields of meteorology,
seismology, radiation and emission monitoring and crisis information systems. The product range covers
measurement, real-time data collection, data processing, archiving and tools for analysis and decision support.
Among the products connected to this project are meteorological modeling and warning systems with requirements
for fog prediction capability, precipitation station networks and radars. Among the most frequently installed

J. Bartok et al. / Procedia Computer Science 1 (2012) 37–46

Juraj Bartok et al./ Procedia Computer Science 00 (2010) 000–000

products is the Airport Weather System, where fog forecast and ceilometer cloud data analysis is of great interest to
airport users. Institute of Informatics of the Slovak Academy of Sciences (II SAS) is one of the leading Grid
Computing research institutions in Slovakia, and has experience in (among others) parallel and distributed
computing, grid computing, as well as application of these technologies in the Earth Sciences domain. This
experience may be greatly utilized in Data Mining Meteo, and II SAS will contribute mainly in the data integration
domain, and by providing experimental data for the project’s application. II SAS is also a skilled participant of many
national and international research projects. Faculty of Electrical Engineering and Informatics of the Technical
University of Košice (FEI TUKE) has long time experience with data-mining in various domains including the
mining on unstructured textual data. Additionally, FEI TUKE has experience with the modification of the datamining algorithms for the distributed GRID environments. The main contribution of FEI TUKE will be design of
data preprocessing tasks and selection and application of the data mining methods.
3. Methods and Technologies for Integration of Meteorological Data
The meteorological datasets used in the project are dynamically changing, as new data is added to them, and
sometimes already existing data is changed to be more accurate. In data mining it is usual to first create a static,
preprocessed image of the data to be mined [13]. In our application, this technique is not applicable, since it would
require additional time to prepare this image, and the results of our data mining process would lose their relevance,
as they are real-time short-term meteorological predictions. Therefore, it is our aim to create a software framework
for real-time distributed environmental data integration, which would be able to read, modify, filter, stream, and
integrate the input data of the data mining process in real time, so that the data mining process can be started
virtually without any delay.
In order to achieve this ambitious goal, we will turn to other scientific projects, which are trying to solve a similar
problem. One of the members of DMM consortium, the Institute of Informatics of the Slovak Academy of Sciences,
takes part in the project ADMIRE [10], which does research specifically in the field of distributed data mining and
integration, and applies the newly developed technologies to two different applications – one of them being also a
set of environmental scenarios.
3.1. Methods of Distributed Data Mining and Integration in the Project ADMIRE
The project Advanced Data Mining and Integration Research for Europe (ADMIRE) has among its goals the
design and development of an architecture, a framework, and a high-level language for integration and mining of
distributed data, usable by scientists from different fields, who are not data mining experts. The architecture will
allow integration of different data providers, data consumers, and users, will be highly modular, and will support an
extensive toolkit, using a rich benchwork-style user interface, and also a web-based portal as a second, not complete,
but still usable user interface.
The toolkit which will realize the architecture is dominated by a so-called Gateway, used as a single point of
entry from the high-level domain-oriented world into the low-level data integration and data mining machine. More
than one Gateway is to be deployed, each of them allowing access to local resources (databases, file storage servers,
and specialized data processing tools), and the Gateways communicate with each other and stream the processed
data. A Gateway is operated by means of a document written in a new high-level data mining language, based on the
concept of connecting streams of data through data Processing Elements (PEs). The language is named DataIntensive Systems Process Engineering Language (DISPEL), and as its name says it describes the construction of a
process for data manipulation.
Apart from the gateway, the toolkit contains also a comfortable GUI-based tool for DISPEL document
construction, using UML as the graphical modeling language. The model can be exported to DISPEL and
subsequently used for data processing process instantiation. The Registry and Repository modules are used to store
documents and data, and to semantically describe them.
The whole ADMIRE framework is intended to operate on large data sets, streaming them through PEs acting as
filters. The PEs instantited in a process are called activities, since they are OGSA-DAI [11] activities. Some of the
PEs are generic (e.g. an SQL reader), and some must be developed specifically for the application process where
they figure.

39

40

J. Bartok et al. / Procedia Computer Science 1 (2012) 37–46

Juraj Bartok et al./ Procedia Computer Science 00 (2010) 000–000

3.2. Example of Application of ADMIRE Technology to Environmental Data Processing Scenario
While the project DMM is just beginning, there are already existing examples of application of the (still highly
experimental) ADMIRE framework to an environmental application. One of the pilot applications of ADMIRE is
using environmental data and data mining to create experimental methods of predicting environmental processes in
meteorology and hydrology, which currently are not being predicted by standard (usually physical model-based)
methods.
Figure 1 shows an example of a distributed data integration and mining (DMI) process from the domain of
environmental applications. The process is divided into several steps, which we have identified as being generic in
most of DMI processing operating on spatio-temporal data. The data have first to be retrieved from their storage,
using several PEs for GRIB (Gridded binary file format, see [12]) file reading and database access. In order to be
able to select proper GRIB files, a spatial transformation of the input parameters of the process is necessary.
After the initial data sets are queried, the data is streamed through a series of filters (realized by PEs), which
perform some of the steps of CRISP-DM. Thus by preparing a DISPEL document describing this process in
advance, we are able to eliminate the repeated data preparation step of CRISP-DM, and the data is processed very
quickly, nearly in real time. This method may not be usable for one-time scientific experiments, but as a way of
using advanced data mining techniques in day-to-day operations of the DMM project it is very suitable. The stream
of data is processed – missing data is filled in, the different temporal and then spatial resolutions of the data are
unified, and an integrated stream ready for data mining is prepared.

Figure 1.Data integration and mining process using distributed environmental data

J. Bartok et al. / Procedia Computer Science 1 (2012) 37–46

Juraj Bartok et al./ Procedia Computer Science 00 (2010) 000–000

3.3. Application of DMI technology in DMM
The project DMM has a different goal than ADMIRE – we do not aim to create a new technology for data
integration and data mining, but rather use the state-of-the-art technology which is result of other IT projects in
creating an operations platform for short-range prediction of certain meteorological phenomena. In order to do so,
we will adopt the ADMIRE framework to our needs, in the process selecting the components which are suitable for
DMM (for example the Gateway and DISPEL), and maybe omitting some other components. We will develop a set
of PEs for accessing and filtering the data sets used in DMM, and also another PEs for visualizing and delivering the
output of the data mining process into the operational infrastructure which will use DMM results for issuing
predictions and warnings.
4. Mining of Meteorological Data
This chapter we will describe the process of knowledge discovery in databases (KDD) [8]. The goal of this process
is to gain new and potentially useful knowledge from extensive databases. The knowledge is in the form of a set of
generalized rules allowing us to better understand domain-specific relations, and to better predict future cases. This
process is generally iterative as well as interactive (i.e. it cannot be fully automated).
The process of knowledge discovery in databases is detailed in the CRISP-DM (Cross Industry Standard Process for
Data Mining) methodology, which is the de-facto industry standard for KDD. The methodology defines several
successive steps, described below.
4.1. Goal understanding
This starting phase of CRISP-DM targets the understanding of business or other goals and tries to transform them
into the definition of a data mining task. In our case the goal is to improve prediction of selected meteorological
phenomena, leading to cost savings or increased public safety. From the point of view of KDD it is a time series
prediction task, where based on the historical data from time ..., tn-2, tn-1, t we try to come up with a predictor for data
in time tn+1, ..., tn+K. In this particular case it is limited to future three hours. We can also specify our task as
• classification – the predicted value is from a finite set of values (the simplest case is a binary classification – e.g.
there will be fog, or there won’t be fog), or as
• regression – the predicted value is of a continuous nature (e.g. a real number determining the amount of rainfall
in a watershed).
Next we have identified the fact that apart from the predicted value itself it is useful to have as one the predictor’s
outputs also a “prediction confidence” (e.g. saying “the estimation of the probability of a fog in two hours is 0.8”).
Even if our primary goal is to get a prediction of the best possible quality, even the interpretation of the rules used
for predicting can be interesting – a secondary task is thus a descriptive data mining – the ability to comprehensibly
describe the processes leading to occurrence of fog.
4.2. Data understanding
This phase starts with the acquisition of data relevant to our task. An overview of data identified as relevant to
our case is part of the next chapter. After making the identified data available, an initial data examination is
performed, leading to verification of the quality of the data. Part of the examination is also computation of basic
statistics of key attributes of the data and their correlations.

41

42

J. Bartok et al. / Procedia Computer Science 1 (2012) 37–46

Juraj Bartok et al./ Procedia Computer Science 00 (2010) 000–000

4.3. Data preparation
The third phase of CRISP-DM is data pre-processing; this is usually the most complex and also most time
consuming phase of KDD (usually taking 60 to 70 percent of the overall time of the KDD process). We have
identified the following data preparation tasks most characteristic for our goal:
• Data extraction from the meteorological messages – the goal is to extract data encoded in the text messages
broadcasted from the meteorological stations. The format of the messages is fixed with standard codes
denoting the parts of the messages and data values. The output of this task is the relational database with
extracted data.
• Data extraction from the satellite/radar images – the goal is to extract indicator variables, which encode
fog and low cloud cover in the given area. The output is the relational database with the extracted data.
• Data integration – Meteorological data from all sources (i.e. data extracted from messages, satellite
images, meteorological stations and physical model predictions) are integrated into one relational
database. Each record in the integrated database has assigned valid from/to time interval and 3D
coordinates of measured area (i.e. ranges for longitude, latitude and altitude).
• Data interpolation – It is possible to have more measures/predictions of the meteorological data for the
given time and area with different data precision and/or granularity. The goal of this task is to
interpolate measured values and compute additional data for the requested area and time with the
specified data granularity.
• Data reduction – From a large data set we will select a representative sample, which is used in modeling.
Reduction is usually necessary because of technical restrictions inherent to some methods, but it can
also lead to simplification of the task at hand by removing irrelevant attributes and records, thus even
increasing the quality of the results. The scheme of the integrated database should support effective data
selection according to the time intervals and area coordinates. Specification of the area of interest and
time intervals will be the base method for reduction of the number of database records for mining.
Additionally, various statistical scoring functions such Information Gain will be used in order to select
set of attributes most correlated with the target variables (i.e. the most important for the prediction).
We are now in the process of finding alternative sources for missing METAR meteorological observations and
replacing missing physical model forecasts by running the historical situations on our distributed computing
infrastructure. Concerning data quality, we rely on both manual and automatic quality checks. Manual checks
consists of examining raw data time series by professional meteorologists and finding unusual or nonsensical data
peaks, nonstandard daily course of variables and relationships between time series of individual variables. In another
manual check we examine visually 2D data fields to find spatial inconsistencies. Automated quality checks comprise
a global range check (reasonable value ranges for each variable), a monthly range check (comparing of values to
monthly climatological values), rate of change checks (for each meteorological variable a reasonable change within
10minutes, 1 hour is defined) and internal consistency checks (relationships between different variables must obey
physical principles of atmosphere).
4.4. Modeling
This phase will be the core of the data mining process, the application of a selected data mining method to the
available (already understood and prepared) data; the parameters of the method must be tuned in order to obtain
optimal results. The tuning is usually done by dividing the input data set into a training and validation data set; the
validation part is used for assessing the quality of the model. Each method has its specific requirements for the input
data, meaning that a change of method can lead to the necessity of repeating the data preparation step. There is a
whole range of prediction methods – from statistical methods to artificial intelligence methods, like linear or

J. Bartok et al. / Procedia Computer Science 1 (2012) 37–46

Juraj Bartok et al./ Procedia Computer Science 00 (2010) 000–000

logistical regression models, Support Vector Machine, neural nets, probabilistic models (for example the Bayesian
networks), decision/regression trees and lists, etc.
4.5. Evaluation
In this phase we evaluate the results of the KDD process from the point of view of the original criteria of our task
set in the first step (goal understanding); we evaluate whether the process has been successful, or whether we have
to take more steps in order to succeed. The evaluation is based on quantitative indicators of the quality of created
models, for example the estimation of the classification error, mean quadratic error of a regression model, etc. In
some cases it is possible to state, based on the model quality, the quantitative indicators for our final goal (for
example the actual cost savings). In our case a suitable evaluation criteria is a comparison with existing
meteorological models; here we need to consider also the “character of the error” (e.g. different criticality of false
positives and false negatives for fog prediction).
4.6. Deployment
In this phase we develop a practical deployment plan for our model. We also determine the strategy or its
monitoring and maintenance in order to reduce the costs of its deployment and the possibility of its incorrect
application. One of the results of this phase should be also an overall evaluation of our project and the definition of
recommendations for future projects of similar character – best practices.
5. Data Mining Scenarios in DMM
We use the above described DMI framework and CRISP-DM methods for solving several scenarios from domain
of meteorological warning systems. Two scenarios have direct application in air and road transport and one in flood
mitigation, therefore creation of final working products is expected and also strongly required.
5.1. Data Mining Applied to Fog Prediction
The business partner of this DMM consortium runs in his projects a fog prediction system composed of in site
and remote monitoring parts and physical prediction models. (Under physical prediction models we mean numerical
weather prediction models based on numerical solving of physical equations describing atmospheric processes.) The
output of models, as well as real-time measured data, are left for operator’s interpretation. The operator has
advantage to fully automatic system, because there can be found typical meteorological situations in which model
overpredicts/underpredicts, and the experienced operator can consider this experience in his decision making. The
DMM project will utilize all the measured and physically predicted information to extract fog prediction by data
mining model. The input data are grouped by source:
• Meteorological messages with measured data. This data are available from standard network of meteorological
stations (text codes SYNOP, METAR). They are available in hourly intervals. Messages immediately preceding
the forecasting interval are of importance, as well as recent history (last 72 hours).
• Upper air measured data. This data are measured by special techniques and the output is profile of basic
meteorological variables above certain station. Data are available once daily or once hourly, with varying
accuracy.
• Data from specialized automated weather stations measuring elements significant for fog occurrence. Such
stations comprise standard meteorological sensors plus soil humidity and temperature in different depths,
visibility sensors, and additional air temperature sensors. The data are available each 2 minutes.
• Satellite imagery and its sequences. Satellites are the most complicated to integrate into data mining, but very
valuable, because they represent spatial information, in contrast to the point stations. The satellite scans the Earth
on several channels, and combining the channels into so called RGB composites results in a picture emphasizing
feature of interest – in our case fog or low cloud cover. Simple or more complicated picture are available. By
combination with station measurement we gain a spatial estimate of current fog extent and density.

43

44

J. Bartok et al. / Procedia Computer Science 1 (2012) 37–46

Juraj Bartok et al./ Procedia Computer Science 00 (2010) 000–000

• 3D physical model weather prediction. Predictions for 48 hours are available each 6 hours, with 1.8 km
horizontal resolution and 42 vertical levels.
• 1D physical model weather prediction. Predictions for 12 hours are available each hour above selected station
sites, with finer vertical resolution.
Expected predictors among meteorological elements are temperature, relative humidity, visibility, wind speed,
wind direction, precipitation, cloud amount, soil temperature and moisture, fog on satellite image in nearby areas.
We have available the elements are both measured and forecasted by physical models. Temperature drop is
necessary to start fog, especially in radiation fog cases. Relative humidity is one of measures of water content in
atmosphere and shows how near the air is to saturation and condensation of fog droplets. Visibility is connected to
fog presence and density, although visibility can be lowered also by other means (rainfall, snow, dust). Wind is
important selector of radiation fogs – low wind speeds favor forming of radiation fog, while both zero and strong
wind speeds do not. Wind direction is important in sea shore areas – wind can “bring” humidity form the sea.
Nearby fog on satellite image tells us that the conditions suitable for fog formation were fulfilled there. Soil
moisture informs if there is a sufficient amount of source humidity in the soil. Soil temperature in several depths (a
profile) influences how the soil will cool the air to create the necessary temperature drop.
The outputs of the data mining model will be:
• Visibility prediction for one to three hours ahead (the usual short-term warning system range);
• The prediction will be categorical – visibility below 200 m (important road threshold), 600 m (important aviation
threshold), 1000 m (threshold in meteorological definition of fog), 3000 m.

Figure 2. Left: RGB composite of satellite images of fog. Fog is in orange colour. Blue are higher level clouds, which are causing missing
information when covering area of interest. © 2009 EUMETSAT. Right: Example of image with artificially recognized fog by simple
thresholding algorithm.

J. Bartok et al. / Procedia Computer Science 1 (2012) 37–46

Juraj Bartok et al./ Procedia Computer Science 00 (2010) 000–000

5.2. Data Mining in Cloud Data Measured by Ceilometers
It is important to have either no-cloud or cloud-with-base-height-h information above ceilometer installation
point for at least 10-20 minutes with sampling frequency below one minute. Then it is possible from clouds
“flowing” above ceilometer estimate portion of nearby sky covered by clouds. We would like to combine data from
all ceilometers at the airport and other relevant data. The input data for this scenario are:
• Measured data from at least three ceilometers. This data contain instant heights of cloud bases above a
ceilometer, and are available each 30 seconds. Typically an airport has in addition to ceilometer data also wind
data from surface and upper air levels, which can help.
• Observations of the airport observer. Observations contain fractional coverage of sky by individual cloud layers
and are available each half an hour (text codes METAR).
The expected output of data mining model is:
• Cloud amount expressed in 1/8 fractions of the sky (possible states are 0/8 to 8/8 and no visible clouds) updated
each 30 seconds.
5.3. 2.3 Mining of Weather Radar Data
Inputs (predictors) of data mining of weather radar data targeting spatial assessment of precipitation and very
short term predictions of the movement of cloud water and precipitation area are:
• Radar imagery and its sequences. Available in 15 minutes intervals.
• Precipitation measured by precipitation stations deployed in the target area. Precipitation are hourly
accumulated.
Another potentially useful input data are:
• Measured mean hourly temperature in several atmosphere layers (the actual set of layers depends on the desired
output quality, and is part of the experiment),
• Modeled wind speed and direction in a grid covering the target area.
The result is the current precipitation estimate in the target area and possible precipitation in the very near future.
5.4. Evaluation and Deployment of Models
Model evaluation results show if our data mining efforts were successful.
Within evaluation we will compare the model results with independent data set. In all cases the particular model
will be trained on part of the available data and verified against the rest of available data. Additionally, we will
compare data mining models results with results of classical methods: For now we have e.g. statistical results of fog
forecasting by 3D physical model and by combination of 3D physical model – 1D physical model initialized with
data form specialized automatic weather stations. We expect data mining models to be more successful in
comparison.
Quantitative verification results in form of standard scores (POD, FAR, ROC curves [3]) can be linked to the
economic cost/loss ratios of the end user (road authority, airport authority) [9].
6. Summary
The aim of the project DMM is to improve specialized short term forecasts and meteorological warnings. The
main beneficiary areas are transportation (visibility, low clouds) and hydrology (radar). We plan to reach the goal
via research of prediction and detection methods of significant meteorological phenomena based on data mining of
meteorological data (measured data, forecast data by traditional methods – physical models). Data mining is
supported by system of integration of distributed data sources. We believe that data mining results will be less
sensitive to quality of input data, and that the quality of results will degrade more gracefully with the degradation of
inputs. This is important in scenarios, where input data is incomplete or of varying quality, as it is often the case in
specialized meteorological predictions without long history of use. Even so, our priority is to prepare the most
accurate and complete input data as possible, using manual as well as automated data quality checking.

45

46

J. Bartok et al. / Procedia Computer Science 1 (2012) 37–46

Juraj Bartok et al./ Procedia Computer Science 00 (2010) 000–000

In this paper we presented consortium abilities and summarized the scope of work, methods and domain
knowledge.
The very first results achieved are in data analysis and cleaning. The crucial future work consists of training of
the models and their evaluation. The detection models of low clouds and precise spatial precipitation are profiling
themselves as a good tool for airports and hydrological services. The predictive fog model will serve for
meteorological institutes, traffic services and public and in case of positive evaluation all models will be
transformed to usable products.

Acknowledgements
This work is supported by projects DMM VMSP-P-0048-09, SEMCO-WS APVV-0391-06, VEGA No.
2/0211/09 and SMART (ITMS: 26240120005).

References
1. Bergot et al., Improved Site-Specific Numerical Prediction of Fog and Low Clouds: A Feasibility Study. Weather and Forecasting, 20, 627646, 2005.
2. Bott, A., and Trautmann, T.: PAFOG - a new efficient forecast model of radiation fog and low-level stratiform clouds. Atmos. Research, 64,
191-203, 2002.
3. COST 722 - Short range forecasting methods of fog, visibility and low clouds. Final Report, COST Office, Brussels, Belgium, 2007.
4. Gultepe, I., Müller, M. D., Boybeyi, Z.. A new visibility parameterization for warm fog applications in numerical weather prediction models.
In J. Appl. Meteor. 45, 2006, p. 1469-1480.
5. Hluchý, L., Habala, O., Tran, D. V., Ciglan, M.: Hydro-meteorological scenarios using advanced data mining and integration. In The Sixth
International Conference on Fuzzy Systems and Knowledge Discovery : proceedings: FSKD 2009. Editor Y. Chen, Hepu Deng, Degan
Zhang, Yingyuan Xiao. Los Alamitos : IEEE Computer Society, 2009. ISBN 978-0-7695-3735-1. Vol. 7, p. 260-264. 2009.
6. Habala, O., Ciglan, M., Tran, D. V., Hluchý, L.: Advanced data mining and integration for environmental scenarios. In Informačné technológie
- Aplikácie a teória : zborník príspevkov prezentovaných na pracovnom seminári ITAT. Praha : Matematicko-fyzikální fakulta UK, ISBN
978-80-970179-1-0, p. 98-99. 2009.
7. Kolektív autorov Meteorologický slovník výkladový terminologický. Academia Ministerstvo životního prostředí ČR Praha,1993
8. Paralič, J.: Objavovaie znalostí v databázach. Elfa, Košice, 2003, 80s, ISBN 80-89066-60-7.
9. Jolliffe, I.T. and Stephenson, D.B.: Forecast Verification: A Practitioner's Guide in Atmospheric Science. John Wiley and Sons, Chichester,
2003, 240p, ISBN 0-471-49759-2
10. Atkinson, M., Brezany, P., Corcho, O., Han, L., van Hemert, J., Hluchy, L., Hume, A., Janciak, I., Krause, A., Snelling, D.: ADMIRE White
Paper: Motivation, Strategy, Overview and Impact. Technical Report v0.9, the ADMIRE Project, January 2009.
11. Chue Hong, N.P., Antonioletti, M., Karasavvas, K.A. and Atkinson, M.: Accessing Data in Grids Using OGSA-DAI. In: Talia, D., Bilas, A.,
Dikaiakos, M.D. (Eds.): Knowledge and Data Management in GRIDs, p3-18. 2007. ISBN: 978-0-387-37830-5.
12. World Meteorological Organization: Guide to the WMO Table Driven Code Form Used for the Representation and Exchange of Regularly
Spaced Data In Binary Form: FM 92 GRIB Edition 2. Geneva, January 1, 2003.
13. Han, J., Kamber, M.: Data Preprocessing. In: Data Mining: Concepts and Techniques. Second Edition. Pp.47-104. Morgan Kaufmann, 2006.
ISBN 978-1-55860-901-3.
14. Habala, O., Paralič, M., Rozinajová, V., Bartaloš, P.: Data-aware Composition of Workflows of Web and Grid Services. In: Bubak, M.,
Turala, M., Wiatr, K. (eds.): Proceedings of the Cracow Grid Workshop '08. Academic Computer Centre CYFRONET AGH, Poland, 2009,
ISBN 978-93-61433-00-2, pp.120-128.
15. http://www.microstep-mis.com/index.php?lang=en&site=src/references;
http://www.microstep-mis.com/index.php?lang=en&site=src/research
16. INCO - COPERNICUS GOAL - Geographic Information On-Line Analysis (GIS - Data Warehouse Integration), P.No. 977091. October
1998 - September 2001
17. HPSE-2001-00065 PRODGAP - The Determinants of the Productivity Gap. February – March 2004
18. IST-FP6-26476 SEAMLESS - Small Enterprises Accessing the Electronic Market of the Enlarged Europe by a Smart Service Infrastructure.
January 2006 – June 2008

