Procedia Computer
Science

Procedia Computer Science 1 (2012) 745–752
Procedia Computer Science 00 (2010) 1–8

www.elsevier.com/locate/procedia

International Conference on Computational Science, ICCS 2010

Time parallel kinetic-molecular interaction algorithm for
CPU/GPU computers
Sorin Mitran1
Department of Mathematics, University of North Carolina, Chapel Hill, NC, 27599-3250

Abstract
A parallel-in-time, multiscale interaction procedure is introduced for systems described at molecular scales by
time-dependent random variables that obey Langevin dynamics. At larger, kinetic scales, the system is described by a
probability distribution function that obeys an associated Fokker-Planck equation. It is assumed that the main quantity
of interest is the kinetic-scale probability distribution function, and that molecular time scales are much smaller than
those of interest at the kinetic scale. The overall multiscale interaction procedure is an iterative reﬁnement predictorcorrector algorithm similar to the parareal method, but with diﬀerent physical models used in each stage. In the
predictor stage, a partial diﬀerential equation solver is used to obtain estimates of the probability distribution function
on a subdivision of a time interval. At the starting time of each subinterval, the predicted probability distribution
function is used to initialize an ensemble of random variables that are subsequently evolved forward in time by the
Langevin dynamics. A probability distribution function estimate is constructed from the Langevin time evolution at
the molecular scale and compared to that from the kinetic scale Fokker-Planck equation to determine whether further
iterative reﬁnement is needed. The molecular-scale computations are executed on highly parallel graphics processor
units, while the kinetic-scale simulation is running on central processing units. The performance of the algorithm is
tested on the simple dumbbell model of polymer ﬂow.
c 2012 Published by Elsevier Ltd. Open access under CC BY-NC-ND license.
⃝

Keywords: Multiscale algorithm, multiphysics algorithm, Kinetic equation

1. Introduction
One of the challenges in multiscale computation is to develop algorithms that take into account the diﬀerent
physical descriptions of phenomena at disparate scales. One such class of systems are continua with complex, perhaps
time-varying, microstructure. Polymer ﬂow, rupture propagation in solids, and mechanics of the cytoskeleton are
some of the challenging examples for which a multiscale, multiphysics approach is required. This paper presents
an algorithm suited for simultaneous computation of a kinetic and a molecular-level description. A notable feature
of the algorithm introduced here is time domain decomposition to allow parallel-in-time computation similar to the
parareal method [1], but adapted to the peculiarity of a multiphysics description. A further novelty is the use of highperformance graphical processing units for the molecular simulation. Indeed, it is the possibility of using this type of
1 E-mail:

mitran@unc.edu, web: http://mitran.amath.unc.edu/tPKM

c 2012 Published by Elsevier Ltd. Open access under CC BY-NC-ND license.
1877-0509 ⃝
doi:10.1016/j.procs.2010.04.080

Mitran/ /Procedia
Procedia Computer
Computer Science
S.S.Mitran
Science001(2010)
(2012)1–8
745–752

746

1

r2

2 r
3

r1

3
m+1
r m+1

rm

0

m−1
d

2

m
n

rn

n−1

Figure 1: Rouse bead chain model of a polymer.

hardware which makes the approach feasible. A simple example of the potential of the method is presented for the
dumbbell model of polymer ﬂow [2]
2. Physical models
2.1. Molecular description
Langevin dynamics describes phenomena at molecular scales by stochastic diﬀerential equations that combine
deterministic evolution of resolved phenomena with random forcing from unresolved scales. Consider a molecularscale system with 3n internal, resolved degrees of freedom that correspond to n constituent particles. The state of the
system at time time t is speciﬁed by the vector q(t) ∈ 3n . An example would be the Rouse bead-chain model of the
internal structure of a polymer [2] with n + 1 monomer units (Fig. 1). Monomer m is at position vector rm ∈ 3 with
respect to monomer m − 1 for m = 1, 2, . . . , n, and
q(t) = (r1 (t), r2 (t), . . . , rn (t)).

(1)

Due to the eﬀect of unresolved degrees of freedom (e.g., action of solvent molecules), the system trajectory q(t) is
considered to be a time-dependent random variable vector whose components satisfy the Langevin equations
γdrm = − f (rm (t))dt + σdwm (t), m = 1, 2, . . . , n,

(2)

written using It¯o calculus [3]. The overall system of stochastic diﬀerential equations (SDEs) is
dq = p(q(t))dt + σdg(t),

(3)

with p(q(t)) = ( f (r1 (t))/γ, . . . , f (rn (t))/γ) ∈ 3n , g(t) = (w1 (t), . . . , wn (t)) ∈ 3n . In (2), the system constituent
particles are acted on by the internal, resolved force f (r j ), and a random force due to the unresolved degrees of
freedom of magnitude σ = 4γkB T (notation: γ - friction coeﬃcient, kB - Boltzmann constant, T - temperature) with
wm (t) a Wiener process, i.e. a Gaussian process that satisﬁes
wm (t) = 0, wm (t)wm (t ) = min(t, t )1.

(4)

A simple forward Euler discretization of (2) is
rm (t + δt) = rm (t) + f (rm (t))/γδt +

4kB T/γδwm (t), m = 1, . . . , n,

(5)

with δw(t) independent Gaussian variables with zero mean and variance δt.
2.2. Kinetic description
The statistical properties of the molecular-scale behavior will be established by introducing an ensemble of N
systems with trajectories E = {q1 (t), . . . , qN (t)}. Introduce the probability distribution function (pdf) ψ(q, t), with

S. Mitran
/ Procedia
Computer
Science
00 (2010)
1–8
S. Mitran
/ Procedia
Computer
Science
1 (2012)
745–752

3
747

ψ(q, t)dq denoting the probability at time t of ﬁnding the n particles in a volume of measure dq centered at point
q = {r1 , . . . , rn } within the phase space Q of the system, dim Q = 3n. The pdf satisﬁes a Fokker-Planck equation
p
∂ψ
∂
2kB T ∂ ∂ψ
+
· ψ =
·
.
∂t ∂q γ
γ ∂q ∂q

(6)

We shall say that the Fokker-Planck equation describes phenomena at a kinetic scale by analogy to the Boltzmann
equation of gas kinetics.
Average properties of the system are determined by the weighted phase space average u (t), which is numerically
approximated by the ensemble average u¯ (t)
u (t) =

u(q)ψ(q, t)dq

u¯ (t) =

Q

1
N

N

u(qi (t)),

(7)

i=1

with u some molecular-scale property of interest, and qi (t) the solution of the SDEs
dqi = p(qi (t))dt + σdg(t),

(8)

for each of the i = 1, . . . , N, members of the ensemble E.
3. Multiscale, multiphysics computational method
3.1. Motivation
Both the molecular description (2) and the kinetic description (6) present considerable challenges for numerical
simulation. Though the discretization (5) is simple, accurate statistics are only obtained for a large number of particles,
with the estimation error ε converging at a slow, sublinear rate ε ∼ O(N −1/2 ) with increase in number of particles.
The most often-encountered diﬃculty in solving the Fokker-Planck equation (6) is the high, 3n, dimension of the
conﬁguration space Q which renders most grid-based methods impractical. On the other hand, both formulations also
exhibit features attractive for numerical simulation. The molecular description is trivially parallelizable. Well tested,
meshless numerical discretization techniques can be applied to the Fokker-Planck equation. It is of interest to study
the possibility of combining the attractive features of numerical simulation of the molecular and kinetic descriptions in
a kinetic-molecular interaction model that could alleviate the diﬃculties encountered in each formulation. Here, such
a kinetic-molecular interaction algorithm is introduced starting from a parallel in time approach to the Fokker-Planck
equation.
3.2. Synopsis of parareal algorithm
Parallel in time, or parareal, algorithms have been introduced in [1], [4] as a procedure to carry out domain
decomposition in time. The basic framework is to consider a system of ordinary diﬀerential equations
q (t) = f (t, q(t)),

(9)

which we wish to advance forward in time over the interval [0, tP ] from the initial condition q0 = q(0). Subintervals
[t j−1 , t j ], j = 1, 2, . . . , P, t j = jΔt, Δt = tP /P, are introduced with P the number of available processors. Let Qkj be
the numerical approximation of the exact solution q(t j ) at stage k of an iterative reﬁnement algorithm. The standard
parareal approach is to consider two diﬀerent algorithms to advance the numerical solution forward in time:
1. M(t j+1 , t j , Qkj ) is an accurate, but computationally expensive algorithm;
2. K(t j+1 , t j , Qkj ) is a less accurate, but low computational cost algorithm.
The cheap algorithm K is applied in serial processing mode to predict required initial conditions. The costly algorithm
M is then applied in parallel over all time subintervals to improve the approximation. The iterative reﬁnement is given
k+1
k
k
by Qk+1
j+1 = K(t j+1 , t j , Q j ) + M(t j+1 , t j , Q j ) − Q j+1 ,for j = k + 1, k + 2, . . . , P (Fig. 2). Iterative reﬁnement is continued
k
until a desired error criterion is met at the end of the time interval, Qk+1
P − QP < ε, which one hopes to occur for k
signiﬁcantly less than P since at k = P the more accurate algorithm would have been applied over all time intervals
and nothing would have been gained by the predictor step. Analysis and numerical experimentation on the parareal
algorithm indicates good performance when the eigenvalues of A = fq (local linearization of (9)) do not have large
imaginary parts [9]

4

Mitran/ /Procedia
Procedia Computer
Computer Science
S.S.Mitran
Science001(2010)
(2012)1–8
745–752

748

Time t j (serial)

P =4
Q00

Q01

K

Q02

K

M

M

k
(parallel)
Reﬁnement

Q04

K

M

M

Q12

K
M

t1

Q13

K
M

Q14

K

K

M
t3

t2

k=0
Compare.
Further reﬁne?

Yes. Reﬁne further.
Q11

t0

Q03

K

M
t4

k=1
t5

Figure 2: Parareal algorithm workﬂow.

3.3. Multiphysics parallel-in-time approach
The parareal approximation has been introduced as a method to speed up computation of the solution to a single
set of diﬀerential equations using two diﬀerent numerical approximations. Here the tPKM (time-Parallel KineticMolecular) algorithm is introduced using parallel-in-time domain decomposition and two diﬀerent physical models.
Kinetic stage. Assume that the quantity of intrinsic interest is the pdf ψ(q, t) over the time interval [t0 = 0, tP =
PΔt], typically because the moments of ψ(q, tP ) are needed to formulate a closure at an even larger, continuum scale.
One way of approximating ψ(q, tP ) is to numerically solve the Fokker-Planck equation (6) at the kinetic level on the
time discretization {t0 , t1 , . . . , tP }, ψ¯ 0j = ψ(q, t0 ),
ψ(q, t j )

ψ¯ 0j (q) = K(t j , t j−1 , ψ¯ 0j−1 (q)), j = 1, . . . , P.

(10)

The superscript in ψ¯ kj denotes the stage k in an iterative reﬁnement scheme, and in (10) we have the starting approximation k = 0.
Molecular stage. Another method to ﬁnd ψ(q, tP ) would be to instantiate an ensemble E(t0 ) = {Qi (t0 ), i =
1, . . . , N} from the initial pdf ψ(q, t0 ), evolve the system over the time interval Δt by solving (8) with time step δt,
˜ tP ). This direct numerical simulation approach at
and then accumulate statistics to determine an approximation ψ(q,
the molecular level is however extremely costly if δt Δt and if precision requirements lead to large values of N.
The cost of the molecular computation can however be reduced if the approximations of the pdf at various times
t j are used to predict the states of the molecular system by carrying out the following steps.
1. Use ψ¯ 0j (q) to instantiate P ensembles E j (t j−1 ) = {Qi (t j−1 ), i = 1, . . . , N}, j = 1, . . . , P;
2. Advance the stochastic diﬀerential equations (2) n M times using time step δt to ﬁnd the state of the ensembles
E j (t j−1 + n M δt) = {Qi (t j−1 + n M δt), i = 1, . . . , N};
3. Use a pdf estimation procedure

to ﬁnd ψ˜ 0j−1,nM (q)

ψ˜ 0j−1,nM (q) =

ψ(q, t j−1 + n M δt) from E j (t j−1 + n M δt),
[E j (t j−1 + n M δt)].

Again the superscript k in ψ˜ kj−1,nM (q) denotes the stage in an iterative reﬁnement process, with k = 0 in this initial
approximation.
Two scenarios can arise in the molecular-scale computation.
A. Fast approach to statistical steady state over time interval Δt. Assume that the physics inherent in the molecular
1. This
system imply that ψ˜ 0j−1,nM (q) rapidly approaches a steady state in n M steps and κn M = (Δt/δt), with κ

5
749

S. Mitran
/ Procedia
Computer
Science
00 (2010)
1–8
S. Mitran
/ Procedia
Computer
Science
1 (2012)
745–752

would be the case if the molecular system is rapidly responding to slow changes in ambient conditions set by the
Fokker-Planck equation2 . In this scenario, a kinetic-molecular interaction algorithm has the potential of reducing
the overall computational time by the factor κ.
B. No statistical steady state over time interval Δt. If the molecular system responds slowly to changes at kinetic
scales, the SDEs would have to be advanced over n M = Δt/δt 1 time steps. It would seem that nothing is gained
from a kinetic-molecular approach since at least the same amount of computational eﬀort as in the case of direct
numerical simulation is required, and perhaps more if the pdf estimates ψ¯ 0j−1 (q) are inaccurate. This is generally
true when carrying out both the kinetic and molecular computations on processors of the same speed. However, if
the easily parallelized molecular computations can be carried out on much faster processors, some overall gain in
wall-clock simulation time is possible in this scenario also.
In either scenario, an approximation of the pdf at the end of each time interval [t j−1 , t j ] is obtained from the pdf
ψ˜ 0j−1,nM (q) = [E j (t j−1 + n M δt)], and in scenario B,
estimation procedure . For scenario A we would set ψ˜ 0j (q)
0
ψ˜ j (q) = [E j (t j )]. The molecular and kinetic approximations can be compared and the error between the predictions
made by the computations on the two scales is
ε0KM = ψ¯ 0 (q) − ψ˜ 0 (q) ,

(11)

with ψ¯ 0 (q) = (ψ¯ 01 (q), . . . , ψ¯ 0P (q)), ψ˜ 0 (q) = (ψ˜ 01 (q), . . . , ψ˜ 0P (q)). If the error ε0KM is within an acceptable bound no
further reﬁnement is necessary. If not, the kinetic-molecular interaction procedure is restarted (cf. Fig. 3). Note
that the E1 (t) ensemble was initialized from the exact initial condition ψ(q, 0). It is assumed that the molecular-scale
description is the most accurate model of the system. Hence the pdf estimate ψ˜ 01 constructed from the ensemble
E1 (t1 )= {Qi (t1 ), i = 1, . . . , N} is more accurate than the estimate from advancing the Fokker-Planck equation ψ¯ 01 . In
starting a new reﬁnement stage of the kinetic-molecular interaction algorithm we therefore set ψ11 = ψ˜ 01 . The ensemble
E1 (t1 ) obtained by evolving the SDEs forward in time is kept, since reinstantiating by use of ψ11 would introduce
additional errors due to pdf estimation procedure and pseudo-random number generation. At the end of P reﬁnement
stages the ensemble from the initial condition ψ(q, 0) would have been advanced over the time interval [0, tP ]. A direct
numerical simulation of the molecular level system would have been carried out, albeit a very expensive one since an
additional P(P − 1)/2 computations would have been run over intervals Δt for an increase in overall work by a factor
of (P − 1)/2. Clearly, the kinetic-molecular interaction algorithm will only be eﬀective if the number of reﬁnement
stages is much less than P.
3.4. Communication between scales
An important aspect of the tPKM algorithm is information exchange between the computational scales. The
molecular ensemble at time t j , E(t j ) is initialized by statistical sampling of the pdf ψ(q, t j ) through an instantiation
operator ,
E(t j ) = [ψ(q, t j )].

(12)
3

In this paper, we assume that the n molecular system components are statistically independent ,
n

ψ(q, t j ) =

ζ(rm , t).

(13)

m=1

The instantiation operator is obtained by passing uniform random numbers ρm ∈ [0, 1]3 , m = 1, . . . , n, through the
inverse of the cummulative distribution functions (cdf) Z(rm , t j ),
Z(rm , t j ) =

rm1
−∞

rm2
−∞

rm3
−∞

ζ(sm , t j )dsm1 dsm2 dsm3 .

(14)

2 Though not the case for the simple polymer system considered here, this scenario arises frequently in the more realistic example of the polymer
being entrained in the solvent ﬂow u(x). In this case the deterministic part of the SDEs is f j (∇u, r j ) and modulation with spatial coordinates x of
u(x) typically occurs at a much larger spatial scale than that of the molecular descriptions.
3 For Rouse polymer chains this corresponds to neglecting the Zimm correction that captures hydrodynamic interaction eﬀects [2].

6

Mitran/ /Procedia
Procedia Computer
Computer Science
S.S.Mitran
Science001(2010)
(2012)1–8
745–752

750

P =4
ψ00

Time tj (serial)
ψ10

K

I
Q00

K

ψ20
I
Q02

I
Q01
M

M

Q11

Q11

M

t1

Q˜21

M

Q˜31

k=0
Compare.
Reﬁne?

E
ψ21 K

E
ψ31 K

E
ψ41 K

Q12

Q13

Q14
M

M

M

Yes. Reﬁne

ψ40

K

I
Q03

Q˜21

Reﬁnement E
k
ψ11 K
(parallel)

t0

ψ30

K

t3

t2

k=1
M

t4

t5

Figure 3: Time-parallel kinetic-molecular interaction algorithm workﬂow

In the opposite direction, from molecular to kinetic, a pdf estimation procedure
from an ensemble E(t j )
˜ t j ) = [E(t j )].
ψ(q,

˜ t j)
is applied to approximate ψ(q,

In this paper a multivariate kernel density estimator procedure is applied,
˜ t) =
ψ(q,

1
Lh3n

n

L

cl
l=1

K
m=1

r − rl,m
,
h

(15)

with a radially symmetric Gaussian kernel,
K

r − rl,m
r2
1
exp
−
=
.
h
2
(2π)−3/2

3.5. Modeling errors
A number of numerical errors arise in the procedure and must all be veriﬁed to lie within acceptable bounds. Let
ψ(q, t j ) be the exact pdf of the system at time t j , and let ψex (q) denote a vector of exact pdf’s at time subintervals,
ex
ψex (q) = (ψ(q, t1 ), . . . , ψ(q, tP )). Let Qex = (Qex (t1 ), . . . , Qex (tP )) with Qex (t j ) = (Qex
1 (t j ), . . . , QN (t j )), the solution at
t j of the SDE system that had initial conditions Qex (0) generated from the exact initial pdf ψ(q, 0) for an ensemble of
size N.
1. Numerical error of the Fokker-Planck solver,
εkK = ψ¯ k (q) − ψex (q) ;
2. Strong numerical error of the SDE solver,
εkM =

[ Qk − Qex ];

3. Numerical error of the ﬁnite ensemble size and pdf estimation procedure,
εkIE = ψ(q) −

[E] .

One plausible measure of the error of the overall algorithm at reﬁnement stage k is εkKM = εkK + εkM + εkIE .

S. Mitran
/ Procedia
Computer
Science
00 (2010)
1–8
S. Mitran
/ Procedia
Computer
Science
1 (2012)
745–752

7
751

3.6. Graphics processing units
Recent progress in computer architecture has led to the development of processing units specialized for graphics
operations. Such operations are carried out simultaneously over the pixels of a raster image at high speed to provide
rich, real-time visual feedback. The graphics processing units (GPUs) thus developed are also useful in scientiﬁc
computations and, indeed, have started to be delivered for general-purpose parallel computing without any graphical
output4 . For suﬃciently simple parallelization tasks GPUs can outperform central processing units (CPUs) by factors
of 50 or more in terms of ﬂoating point operations per second. The compromise made in hardware design is that high
performance is only achieved for relatively simple operations on large data sets with minimal communication between
threads that execute in parallel. GPUs are similar to vector processors found in shared memory supercomputers, but
with a reduced instruction set.
The availability of such computer hardware is of great utility for the tPKM algorithm presented here. The molecular stage of the computation is easily parallelized by simultaneous computation of the SDEs for each member of an
ensemble. Since the main consideration in achieving full throughput on GPUs is exposing suﬃcient data for parallelization, the additional ensemble states provided by the kinetic-level prediction of the pdf ψ(q, t j ) aid in keeping any
available GPUs fully occupied. Indeed, reductions of wall-clock computational time are possible even in Scenario B
above if corrections to the kinetic computation pdf-estimates ψ¯ j (q) can be incorporated in an a posteriori, Bayesian
framework, an extension to be treated elsewhere.
One diﬃculty that arises in the GPU implementation of the tPKM algorithm is that a pseudo-random number
generator (PRNG) must be developed. Since random numbers are required at each time step of the numerical solution
of the SDEs, the PRNG must execute eﬃciently on the GPU hardware. In this work the KISS algorithm [5] is used.
4. Sample computations
Here we shall consider only Scenario A, and the rather simple case dim q = 3. For polymer ﬂow this corresponds to the dumbbell microscopic model. Furthermore, it is assumed that the three components of the dumbbell
vector are statistically independent due to isotropy. This is intended as a proof-of-concept of the tPKM method and
assessment of performance on mixed CPU/GPU architecture. More extensive testing will be presented in forthcoming
papers. The tPKM algorithm was implemented using the PyCUDA5 Python interface to the Nvidia CUDA application
programming interface (API). An initial bimodal pdf is chosen
3

ψ(r) =
i=1

⎡
⎡
⎤
⎤
⎢⎢ (ri − ri1 )2 ⎥⎥⎥
⎢⎢⎢ (ri − ri2 )2 ⎥⎥⎥
⎢
⎥
⎥⎦ ,
ψ(ri ), ψ(ri ) = a1 exp ⎢⎢⎣−
exp
−
+
a
⎣
⎦
2
σ21
σ22

and relative error bounds of 10−3 are imposed for each of the model errors εK , ε M , εIE , using L2 norms. The relaxation
of the pdf to equilibrium is governed by the ratio of elasticity to viscosity H/γ ( f = Hr). The background thermal
forcing kB T is set by the ﬂuctuation-dissipation theorem.
Initial tests were carried out on a workstation using P = 4 CPUs and a NVidia Quadro FX3700M GPU with 1 GB
of memory and 128 CUDA cores, capable of a theoretical maximum throughput of 500 GFlops. The wall clock time
speed-up factor,
S = (DNS-SDE execution time)/(tPKM execution time)
of the tPKM algorithm with respect to that of direct numerical simulation at the molecular level (DNS-SDE) is shown
in Table 1, along with k, the number of iterative reﬁnements carried out. All computations on the GPU were carried
out in single precision (32 bit ﬂoating point numbers). The time step ratio δt/Δt = 10−4 was used. Ensemble sizes
of N = 106 were used. The maximum possible speed-up in this case is 4, which is only approached for the highly
damped system H/γ = 10−2 . The small number of processors gives a coarse discretization of the kinetic scale time
interval.
4 For example, in 2009 NVidia introduced the Tesla GPU workstation add-on card capable of peak throughput of 1015 ﬂoating point operations
per second (1 TeraFlop).
5 Documentation for PyCUDA is available at http://documen.tician.de/pycuda/

Mitran/ /Procedia
Procedia Computer
Computer Science
S.S.Mitran
Science001(2010)
(2012)1–8
745–752

752

8

Additional tests were carried out using P = 64 CPUs and GPUs on the UNC BASS supercomputer6 . With
additional parallel capacity, order-of-magnitude decreases in computational time were observed. Again, more speedup is observed for cases with higher dampening, i.e. lower values of H/γ. This is to be expected since the inﬂuence
of errors made in instantiating ensembles at time subintervals are reduced by larger physical dampening.
P
lg(H/γ)
k
S
% of possible

4
-2
1
3.1
78

4
-1
1
2.7
68

4
0
1
2.2
55

4
1
3
1.3
33

4
2
4
0.4
10

64
-2
1
52
82

64
-1
1
47
74

64
0
1
43
67

64
1
3
35
55

64
2
3
20
31

Table 1: Speedup (wall clock time) of tPKM algorithm by comparison to direct numerical simulation of the stochastic diﬀerential equations at the
molecular scale.

5. Discussion
The tPKM algorithm introduced here has been shown to lead to order-of-magnitude reduction in computational
wall clock time due to a combination of deferred correction, time parallelization and use of the speciﬁc capabilities of
graphics processing units. The proof-of-concept computation presented in this paper treats a very simple system with
a low-dimensional phase space for the Fokker-Planck equation. Meshless, particle methods are required for more realistic examples and work is underway in treating problems in which the phase space dimension is signiﬁcantly higher
(∼ 60). One of the important aspects in overall eﬃciency is the use of the information made available by the molecular simulation started from predicted initial states. Two research directions are being actively pursued in this regard.
(1) Bayesian a posteriori corrections to populate ensembles on an ongoing basis as corrected probability distribution
functions from future times become available. (2) Optimal transport [6] approximations of the transformation of the
probability distribution functions between time subintervals.
It is of interest to compare the tPKM algorithm introduced here to related work. Bal investigates the application
of the parareal algorithm to SDEs [7], and concludes that system trajectories can be computed faster when using
parareal, but recovering the probability distribution functions is more eﬃciently done using the natural parallelization
inherent in advancing in time the members of an ensemble. The possibility of simultaneously advancing the associated
Fokker-Planck equation to use as a predictor is not considered though, especially in conjunction with dividing the work
between central processing units (Fokker-Planck) and graphics processing units (SDEs). Recently, Frantziskonis et
al. [8] also consider a multiphysics framework starting from the parareal algorithm in which the statistical sampling
and instantiation procedures considered here are replaced by compound wavelet transformations. They report similar
order-of-magnitude speedups for test cases involving ODEs that model chemical reactions.
Acknowledgement. This work was supported in part by DOE grant A10-0486-001.
References
[1] Jacques-Louis Lions, Yvon Maday, Gabriel Turinici, R´esolution d‘EDP par un sch´ema en temps parar´eel, C. R. Acad. Sci. Paris Ser. I
332:661-668, 2001.
[2] R.B. Bird, R.C. Armstrong, O. Hassager, Dynamics of Polymeric Liquids, John Wiley, 1987.
[3] B. Oksendal, Stochastic Diﬀerential Equations, Springer, 2000.
[4] Guillaume Bal, Yves Maday, A parareal time discretization for non-linear PDEs with application to the pricing of an american put, Lect.
Notes Comput. Sci. Eng., 189-202, Springer, 2002.
[5] G. Marsaglia, Random Number Generators for C: Some suggestions. Postings in newsgroups sci.math, sci.math.numer-analysis, sci.stat.math,
Jan 1999. Full thread with responses available via deja.com.
[6] C. Villani, Optimal Transport, Springer, 2009.
[7] Guillaume Bal, Parallelization in time of (stochastic) ordinary diﬀerential equations, (unpublished).
[8] G. Frantziskonis, K. Muralidharan, P. Deymier, S. Simunovic, P. Nukala, S. Pannala, Time-parallel multiscale/multiphysics framework, J.
Comp. Phys. 228 (2009) 8085–8092.
[9] G.A. Staﬀ, The Parareal Algorithm: A survey of present work,2003 NOTUR Report.
6 http://wwwx.cs.unc.edu/Research/bass/index.php/Bass

Wiki

