Available online at www.sciencedirect.com

Procedia Computer Science 00 (2009) 000–000

Procedia Computer Science 1 (2012) 407–416

Procedia
Computer
Science
www.elsevier.com/locate/procedia

www.elsevier.com/locate/procedia

International Conference on Computational Science, ICCS 2010

Iterative learning control of dynamic memory caching to enhance
processing performance on java platform
Mutlu Ercana,b, Tankut Acarmanb*
a

AvivaSA Retirement and Life Insurance Comp., Technology Management Dept., Ümraniye, Istanbul and 34768, Turkey
b
Galatasaray University, Computer Engineering Dept., Çıra÷an Cad., 36, Ortakoy, Istanbul and 34357, Turkey

Abstract

In this study, computing system performance enhancement by dynamic memory scheduling has been presented.
CPU processing time of the computing systems has been enhanced by classifying the arriving computing jobs on an
“intelligent manner” and dynamically caching them in a limited memory space. In some special computing areas,
like insurance risk investigation, calculations of income and premium need heavy and repetitive actuarial
calculations. To improve the computation response time, computing schemes may be improved. In this study, a
fairly special computing process has been elaborated, the computing jobs are always created by choosing the inputs
among a finite set data such as age, income level, education, which describes the profile of the insurance policy
holder or applicant who is inquiring the risk of his/her policy. Caching the repetitive jobs’ results and enhancing the
CPU benefit has been presented and an intelligent regulation scheme has been introduced to software control. For
experimental validation, Aspect Oriented Programming (AOP) methodology on Java platform is used. AOP allows
identify computational jobs and their parameters based on the marked annotations, and the repeating jobs and the
same results may be accessible for caching.
c 2012 Published by Elsevier Ltd. Open access under CC BY-NC-ND license.
⃝
Keywords: Software control; dynamic resource allocation; scheduling; iterative learning; modelling

1. Main text
Information processing requests and distributed applications are increasing along the web-based server-client
interaction development cycle, and the systems with much more powerful processing capabilities are needed.
Despite the increase in hardware’s processing and transmission busses’ speed, performance enhancement of the
overall computing system may require simultaneous resource allocation depending on the computing task. To
enhance the overall processing performance, research on software control and dynamic resource allocation of the

* Corresponding author. Tankut Acarman Tel.: +90-212-227-4480 ext 112; fax: +90-212-260-4672.
E-mail address: tacarman@gsu.edu.tr.

c 2012 Published by Elsevier Ltd. Open access under CC BY-NC-ND license.
1877-0509 ⃝
doi:10.1016/j.procs.2010.04.044

408

M. Ercan,
Acarman
/ Procedia
Computer
1 000–000
(2012) 407–416
AuthorT.name
/ Procedia
Computer
Science Science
00 (2010)

computing and storage units have been underway by almost all the leading computer and computing nodes’
manufacturers. In order to improve system performance, many studies has been developed on different subjects
including scheduling strategies, architectures and device improvements. Many static and dynamic scheduling
algorithms have been developed on a real-time basis systems operations. Static scheduling algorithms have low
costs, but incapable to adapt scheduling rules to enhance efficiency of the limited resources. Dynamic scheduling
algorithms may adapt to the changing system parameters, [1]. The dynamic scheduling algorithms mainly based on
“selection criteria driven” approach such as Earliest Deadline First, [2], Highest Value First,[3], Highest Value
Density First, [4]. On the other hand, static scheduling algorithms have been improved by adding them dynamic
approach i.e., an improved round-robin algorithm with two processors which may be assigned for different
computing tasks, [5]. Data access operations may degrade computing system performance. One of the design
patterns in software development architecture is to improve system performance by caching. Unnecessary data
reading may be avoided by caching the same and frequently occurring computational jobs, [6]. Caching is one of
the common performance improvement patterns which may be applied in different layers of the systems, for
example web browser caches static web content, database caches blocks of data from disk fro performance, the SAN
controller will cache read/write data from disk and only complete when efficient to do so, (see instance different
caching approaches in [7]).
Iterative Learning Control (ILC) is a methodology, which can be applied to repetitive systems, tries to improve
system performance based on the knowledge of the previous experiences. System learns from previous repetitions
to improve next repetitions to minimize the tracking error, [8]. In factories, industrial manipulators performs same
cycles repetitively, by using ILC some improvements in energy, time minimization has been gained, [9]. ILC-based
automatic train operation systems have led to some improvements in energy savings, trip time, and safety [10]. ILC
has been used in antilock braking systems in electric and hybrid electric where autonomously operating anti-lock
brake systems learn the maximum tire-road friction, which may be unknown a priori due to the varying weather
conditions in [11].
In some areas, like insurance and retirement enterprises area, the calculations of retention and premium need
heavy actuarial calculations. Like different types of mortality tables, disability, assurance, commission calculation
tables, funds grow expectations and many other ones are used to perform these calculations. The tables are not
changed frequently. The tables that are used for life insurance calculations, are updated when death proportions or
born rates are changed (i.e. a disaster or an epidemic disease occurs). Individual retirement tables are changed
when enterprise’s strategy is updated which may occur on a yearly basis, [12]. It can be assumed that for a life
insurance policy with same input values always same premium is calculated.
In this study, a memory scheduling methodology is explored. Idle memory has been used for caching repetitive
jobs and ILC decides to cache the individual jobs to avoid some unnecessary CPU processing load. In Section 2,
computing process modeling is presented. The desired reference and dynamic resource allocation is elaborated in
Section 3. Experimental study to illustrate the effectiveness of the proposed approach is given in Section 4. Finally,
some conclusions are given.
2.

Computing Process Modeling

From the view point of our study, the cost of processing a job in CPU instead of caching its previously obtained
result is considerably higher. The memory is used to store the repeated job’s results. Accessing to the memory from
the CPU is fast enough. For example, the average time of getting an output from memory is approximately 12 10-7
milliseconds. A priori, cost of a simple request from the memory is smaller than complex memory usages. The
CPU is the core element where all jobs are processed. CPU uses the memory to perform its jobs. Sometimes, for
simple jobs, CPU does not need to use the memory.
Jobs are defined as the expressions in the programming languages. The analytical operation such as “3+5” is a
job. The jobs have the inputs and the outputs which can be used as an input of another job or can be assigned to a
variable. The assignment operations can not be defined as a job, for example “x=8” is not a job. There are two
kinds of jobs: I/O jobs and computational jobs. The computational jobs are the jobs which are deterministic, give
always same output with respect to same input (In this study random or time dependant jobs are not used as
computational jobs, they exists in the I/O jobs). “3+x” is a computational job, because when a value is set for “x”
the output is independent from time. The other jobs, like writing to a file system are I/O jobs. An I/O job such as

M. Ercan,
T. Acarman
/ Procedia
Computer
Science
1 (2012)
407–416
Author
name / Procedia
Computer
Science
00 (2010)
000–000

409

data writing to a disk is not a deterministic job from this point of view. In some systems, the computational jobs are
called frequently. If the frequencies of these jobs are so overly, the effect of caching them in the memory would
achieve a better system performance than running them repetitively.
The time cost of a job which runs in CPU can be called as tr and the time cost of getting its output from memory
is denoted by tc . In this study, to minimize the time cost of processing the repeating jobs by a single CPU, some
amount of the previously processed job’s results are cached in the memory. Obtaining the results from memory is a
faster operation compared with running a complex job in CPU which may use other storage devices such as memory
and disk etc. Some partition of the memory is used to cache the job’s results and the partition of this dynamic
memory space versus the total memory is denoted by D ,
m
D
(1)
mT
where m describes the total memory footprints of the outputs of the jobs, and also mT is the total memory of the
system. Each job produces an output object that occupies some footprint in the memory
The tradeoff of regulating the partition of the memory to cache the large number of jobs’ results versus the
request of large memory space to process the jobs whose outputs have not been cached in memory is illustrated by
the following cost diagram in Fig 1. In this figure, a set of I/O and computational jobs has been generated to
investigate the computing performance of the system. The jobs are mainly constituted by I/O and computational
tasks. The characteristics of the jobs, footprints of outputs, service rates, and arrival rates are chosen randomly. The
service rates are chosen between 1 and 10000, the footprints are between 1 and 1000 and all the jobs are distributed
uniformly. Time costs with respect to the classified job types are plotted.
5

x 10
3.5

Time cost of the jobs (ms)

3

2.5

2

1.5

1

0.5
100
90
80

90

70

80

60

70
50

60
50

40

40

30

30

20

20

10

(I/O jobs / (computational jobs + I/O jobs))

0

10
0

The proportion of the memory to cache the jobs

Fig 1 Cost diagram of varying memory proportion to cache the jobs’ results

When the small partition of the memory is reserved for caching purposes, the possibility of finding the previously
processed result is low and the job needs to be processed by the CPU which costs some time. If most of the jobs are
I/O jobs, there is a limited gain in regulating the caching space and caching the computational jobs’ results does not
affect the CPU processing time. When the number of computational jobs is increasing versus the overall jobs,
caching may reduce CPU’s processing load by using the previously processed identical job result. On the other
hand, to increase the memory caching partition may cause slower computing system response due to low memory
space left for processing the new jobs or whose results are not cached, i.e., when the system becomes unable to find
enough memory to process the jobs, time costs of the jobs increases. The slope of the cost function presents a valley
form and its slope in the left side shows this effect. To regulate caching space on the memory dynamically, ILCbased dynamically caching scheme system is introduced to select the most suitable repeating jobs and to increase the
overall processing service time and performance.
The time cost of a computational job may be described by,

410

M. Ercan,
Acarman
/ Procedia
Computer
1 000–000
(2012) 407–416
AuthorT.name
/ Procedia
Computer
Science Science
00 (2010)

tCPU (D )

tc if the job exists in the memory
r if the job is operated by the CPU

{t

(2)

where tc and tr denotes the cache and runtime time, respectively. The jobs are assumed to be in the processing queue
and the total size of the jobs is restricted by the window dimension. Initially, the last n jobs can be supposed in the
sliding window. When one job is processed, a new job enters in the sliding window and the first job is left out of the
window. The total operating time of the kth window is shown as:
k
'tCPU
(D )

m

ki
ki CPU

¦b t

(D )

b

 ki  {0, 1}

(3)

i 1

bki denotes the existence of the ith job in the kth window. The total time cost of the I/O jobs in the same window is,

't Ik/ O (D )

akj

q

¦a

kj
kj I / O

t

(D )

a

 kj  {0, 1}

(4)

j 1

k
k
denotes the existence of the jth job in the kth window. 'tCPU and 't I / O are dependent on the regulated window
size D to cache the previous job results, (for example when D is increased, the memory proportion to process is
decreased),
k
Tk (D ) 'tCPU
(D )  'tIk/ O (D )
(5)

Tk is the total time cost of the kth window, q defines the number of I/O jobs and m defines the number of

computational jobs. 't Ik/ O denotes the time cost of the jth I/O jobs that is being operated in the kth window.

Fig 2. Sliding Window and window regulation process.

The proposed intelligent caching scheme is illustrated in Fig 2. ILC is running on the CPU and it is selecting and
caching some processed results in some partition of the memory. The sliding window methodology is used to select
the jobs and decide to cache some of their results. The window size is adjusted in an optimal way to enable a
performance criterion between forgetting the jobs with high selective criteria versus their recall or arrival frequency.

M. Ercan,
T. Acarman
/ Procedia
Computer
Science
1 (2012)
407–416
Author
name / Procedia
Computer
Science
00 (2010)
000–000

411

In Fig 2, selection example is illustrated where the window size is equal to 4, but the memory contains only 2 jobs
which are selected by selection parameter. The results of the (i-1)th and (i-3)th job are selected and their results are
cached in the memory to be used in their next recall. The selection parameter of the (i-2)th job is lower compared to
the cached ones. The selection parameter of the (i-4)th job is going to be calculated in the next sampling period while
the selection window is sliding to perform caching selection among the individual jobs including the new arrivals.
The selection parameter, denoted by pisc , is calculated for each individual job and it is ranked among the jobs in
the sliding window to cache the most suitable job results in the memory. The selection criteria is derived by,
sc
i

p (W )
where *

:

1

¦P
i 1

and

e

(

ami Pi *W
)
Oi C

(6)

mi is the size of the output of the ith job, Pi is the service rate of the ith job performed by the

i

: is the number of the jobs in the computing system, W is the window dimension (also the number of jobs in
the window), Oi is the Poisson arrival rate of the ith job, C is the capacity of the memory and a is the scaling factor
CPU,

to assure that the selective criteria takes values between 0 and 1. All of the jobs’ parameters are assumed to be
determined a priori.
A simplesc analysis may show that when the individual size of the job output, mi , increases, the selection
parameter p i decreases. This leads to cache
as many as job results instead of caching few larger results. When the
sc
individual service rate, Pi , increases, p i decreases leading to the choice of caching larger service time requiring
jobs results instead of caching the jobs that can be processed faster. The selection criteria may increase as the
individual arrival rate Oi increases leading to cache the jobs with higher occurrence probability to avoid frequent
computation efforts. When the sliding scwindow size, W is increased, the number of the jobs is also increased and the
individual caching selection criteria p i may be decreased due to fact that the number of the overall jobs increase.
Decision about caching the individual job result depends on both of the selection criteria and the probability on
the occurrence of the individual job in the selection window. The probability of existence in the memory is given by,

pi (W )

~

pi (W ) pisc (W )

(7)

The probability of selecting a job from the system depends on the ratio of the occurrence number of the
individual job versus the total number of the individual jobs,
1 (i )
(8)
P(i ) N
6 1( j)
j 1

where N(i) denotes number of occurrences of the ith job, and P(i) denotes the probability of selecting the ith job by
the ILC scheme. The probability of selecting different job is,
*
1 Total  1 (i )
1 (i )
(9)
P(i ) 1  P (i ) 1  N
1 Total
6 1( j)
N

j 1

*

NTotal= 6 1 ( j ) is the total number of the individual jobs and P (i ) is the probability of absence of the ith job in
j 1
the sliding window. The probability of absence of the individual job in two consecutive selections may be given,
*
1 Total  1 (i ) 1 Total  1 (i )  1
(10)
P12 (i )
1 Total
1 Total  1
The probability of absence in a window for the individual job,
W 1
*
 1 (i )  k  1
(11)
PW (i )  Total
1 Total  k  1
k 1
~
where pi is the probability of the existence of the ith job in the window ,
W 1
~
 1 (i )  k  1
(12)
pi (W ) 1   Total
1 Total  k  1
k 1

412

M. Ercan,
Acarman
/ Procedia
Computer
1 000–000
(2012) 407–416
AuthorT.name
/ Procedia
Computer
Science Science
00 (2010)

N(i) is the total number of the ith job occurrence which is described in terms of the arrival rate versus the sum of the
arrival rate of the individual jobs in the window,
Oi
(13)
1 (i )

1 Total

:

¦O

j

j 1

The jobs for which CPU does not use the memory to process, the service rate is assumed to tend to the infinite
utilization rate, and the individual selective criterion is neglected. These types of jobs would not be kept in the
memory unless there is no other computational job which may be unrealistic scenario for real-time computing
systems.
Gain of a job is defined as the time cost of CPU to process the output result. Time cost of getting from memory
is ignored, because it is small and invariant (almost same time cost value for all jobs). The relative gain is important
for the system, the proportion of the absolute gain and the total of absolute gains of all jobs. The relative gain of the
ith job in the window, which is denoted by < i , is given by,
<

where *

:

1

¦P
i 1

1
i

P*

(14)

i

. The expectation of a random variable X, is the mean of the distribution of X, denoted by E(X),

i

E( X )

¦ xP( X

all x

x)

(15)
which is the average of all possible values of X, weighted by their probabilities, [13]. For the total expected gain of
the window, the possibilities of existence in the memory, weighted by the relative gains are obtained by,

E (< )

¦ < i pi (W )

i in W

(16)

The cumulative number of the individual distinct jobs’ outputs which are cached in the memory is derived by,

E (< )
3.

§
§
W 1
·  ( amOi PCi *W ) · ·
1 ¨ ¨§
Total  1 (i )  k  1 ¸
¨
i
¸¸
e
1 
¦
¸¸
1 Total  k  1 ¸
i 1 ¨ Pi * ¨ ¨
k 1
¹
©©
¹¹
©
:

(17)

Dynamic Memory Allocation Based On Iterative Learning

The service time of the CPU is minimized by preventing reprocessing the same jobs and results through the
learning scheme, for example the time cost of obtaining the outputs of the jobs is minimized. Thus, by the first time
processing of the individual jobs, they have to be existed in the memory. If an unlimited memory is available, all
jobs’ results may be cached in the memory. Due to the limited system resources, caching size is limited and a
reference value for the optimal caching procedure may be derived. 1Total is called as the number of jobs processed
and it is bigger than : number of the jobs in the sliding window. TTotal presents the total service time elapsed for
all k jobs,
1Total

¦ T (D )
k

k 1

(18)
:
it is desired to be minimized, and by their first call all computational jobs are cached in the memory, and in the next
sequence, they are replaced by the most selective ones,
TTotal |

m

vd

¦t

i
r

(19)

i 1

A constraint on the dynamical usage of memory is enforced to assure some free space to accept and process some
individual jobs which are appeared first their time or have not been cached due to their low arrival rate. If the

M. Ercan,
T. Acarman
/ Procedia
Computer
Science
1 (2012)
407–416
Author
name / Procedia
Computer
Science
00 (2010)
000–000

413

running time of one consequent job is bigger than the average of the time cost of running the other jobs, jobs’
cached results are organized such that some with lower selection criteria are deleted in the memory. In other way,
desired value, which can be described as in (16), is equal to the expected value.
The error is derived in terms of the sum of the cached results and the attainable desired value,
max( E (< )) 
ek

W N

k

< i pi

i k W 1

W

¦

max( E (< ))

k

¦

< i pi

W

1  i k W 1
max( E (< ))

W N

(20)

W N

where N denotes the set of the previously arrived computing jobs whose parameters such as occurrence frequencies,
size, arrival rates are known. The expected gain, which is the first term in the error function given by (20), is
constituted by summing all the individual expected gain of the jobs existing in the memory (see also (17) for its
derived expression). The second term is the averaged sum of the individual gain existing in the sliding window.
ILC methodology is based on the previously processed results and on the job parameters such as their size in the
memory, arrival and service rate, etc., and they will be used for the next memory scheduling process. The goal is to
minimize the error and to regulate the window size to ensure caching the most frequent repeating jobs’ results in the
memory to avoid unnecessary computing efforts. To achieve this goal, the window dimension may be adjusted after
each iteration. A typical ILC control formation is given by, [14],
uk 1 (t )

uk (t )  J

d
ek (t )
dt

(21)

where ek(t) is defined as the error term expressed as a difference between the desired expected gain of the overall
jobs and the window gain. Towards software control, the control term to regulate the selection window size to
perform caching the most suitable jobs’ results in the memory may be given by,
e e
W k 1 W k  sign( k k 1 )
(22)
W k  W k 1
where sign function is described,
1
if x t 0
sign( x) {
(23)
if x<0
1
4.

Experimental Study

The proposed dynamic memory scheduling scheme is applied to the insurance policy server operating on a realtime basis. An insurance company server executes more than 10.000 remote policy inquiries on a daily basis. To
benefit from the individual retirement product, a customer must be more than 18 years old and 56 years old, and
must be contributing to the insurance financial system more than 10 years. Most of the remote policy inquiring
customers are between 18 and 46. About 350 customers are at the same age. At each age interval, half of them are
male and female and they have to select their incomes and professions etc., in a limited set of the given query data.
Many calculations are executed for each request coming from different individual customer having the similar input
data.
In this study, Aspect Oriented Programming (AOP) methodology is used to separate business logic and other
programming requirements such as logging, transactions etc. AOP works on join points like method calls, object
instantiations with advice types: before, after, around etc. And also AOP is used to identify the individual jobs and
their similar results. The developer marks computational jobs by annotations or regular expressions as join points.
Each join point has its own signature, when a method is called by same parameters the signature does not change. If
the output of the previous job is cached, it is accessible by the signature of the job, (see for instance [15] or more
information on AOP methodology). AOP has been designed to be implemented in many ways, including sourceweaving or bytecode-weaving and directly in the virtual machine. Each method has its own signature including
package and class name. In this study, a map is used to cache methods, key objects of the map are string

414

M. Ercan,
Acarman
/ Procedia
Computer
1 000–000
(2012) 407–416
AuthorT.name
/ Procedia
Computer
Science Science
00 (2010)

implementation of the signature of the method concatenated with string implementation of the parameters (supposed
that if and only if the parameters that have the same string implementation produce the same output).

Jobi(parami(1),parami(2)..)

Statistics of
the jobs

Hashmap<String,Object> cache
jobj1_paramj11_paramj12_...=outputj1
jobj2_paramj21_paramj22_...=outputj2
jobj3_paramj3_paramj3_...=outputj3
jobj4_paramj4_paramj4_...=outputj4
.
.
.

Dynamic Caching
Implementation

Outputi

Fig 3. Implementation of process flow

1

1
ILC APPLIED SYSTEM
ILC NOT APPLIED SYSTEM
DEVELOPER

0.8

0.7

0.6

0.5

0.4

0.3

0.2

0.1
0

ILC APPLIED SYSTEM
ILC NOT APPLIED SYSTEM
DEVELOPER

0.9

THE USAGE OF THE CPU

THE USAGE OF THE CPU

0.9

0.8

0.7

0.6

0.5

0.4

0.3

0.2

0.2

0.4

0.6

0.8

1

1.2

1.4

1.6

1.8

NUMBER OF JOBS PROCESSED

2

0.1
0

4

x 10

0.2

0.4

a) First day processing performance

1

1.2

1.4

1.6

1.8

2
4

x 10

b) Fifth day processing performance
1
ILC APPLIED SYSTEM
ILC NOT APPLIED SYSTEM
DEVELOPER

0.9

ILC APPLIED SYSTEM
ILC NOT APPLIED SYSTEM
DEVELOPER

0.9

THE USAGE OF THE CPU

0.8

THE USAGE OF THE CPU

0.8

NUMBER OF JOBS PROCESSED

1

0.7

0.6

0.5

0.4

0.3

0.2

0.8

0.7

0.6

0.5

0.4

0.3

0.2

0.1

0
0

0.6

0.2

0.4

0.6

0.8

1

1.2

1.4

1.6

NUMBER OF JOBS PROCESSED

c) Tenth day processing performance

1.8

2
4

x 10

0.1
0

0.2

0.4

0.6

0.8

1

1.2

1.4

1.6

NUMBER OF JOBS PROCESSED

1.8

2
4

x 10

d) Fifteenth day processing performance

Fig 4. Process time of CPU to execute the incoming jobs on a real-time basis: a) The First day b) The Fifth day c) The Tenth day
d) The Fifteenth day processing performance

M. Ercan,
T. Acarman
/ Procedia
Computer
Science
1 (2012)
407–416
Author
name / Procedia
Computer
Science
00 (2010)
000–000

415

In Fig 3, the implementation of the process is shown. The ith job has some parameters concerning the inquiry. The
process searches whether the cache map has an object with string representation of the job (String representation
consists of the signature of the method and the parameters separated by underscores, respectively. Suppose that
these string representations do not contain underscores and all parameters have individual and distinct string
representations). If the job with similar result is found, the output of the job is read from the cache and the overall
job statistics are updated, otherwise the job is computed, and the statistics of the jobs are updated by counting the
new processed result and the properties of the job. For the realization of the process, around aspect advice is used,
and cacheable (deterministic) methods are marked with an annotation (@CanCache) by the developer.
In the previous experiences, all of the jobs characteristics are supposed to be known. But in real-life, it may not
be possible to know all the characteristics in advance. To overcome this situation, the system is designed as the
averages of the data characteristics cumulated in the previous days. When the new individual job arrives, its
statistical values are used to derive the next day’s scheduling average values in order to determine the reference
value, the window size etc. In Fig 4., process time of CPU is plotted for different days. Each day CPU usage has
been based on the previous day arrival jobs. The performance of the proposed dynamic caching scheme is plotted
for every five days. For each day, when CPU is intended to process each individual job without considering even if
their results may be the same, CPU is loaded by its %80 of the time. When ILC is applied, CPU is loaded by almost
%20 percent of the time leading almost %80 of the runtime free for the other jobs. In the case where the developer
has known all the computing jobs a priori, CPU performs around %50 duty in comparison with its full operational
cycle.

a) Process time of CPU on a daily-basis

b) Number of jobs processed on a daily-basis

Fig 5. Process time of CPU to execute the incoming jobs and the number of jobs processed on a real-time basis.

Processing times of CPU to execute the incoming jobs on an hourly basis per each day are plotted on the left side
of Fig 5. The dark gray bars represent processing time of ILC applied system. On first day, applying ILC assures
jobs to be processed by using 25% of the computing performance in comparison with the conventional processing
and caching. The light gray shows the conventional computing systems, when ILC is not applied, and requires
almost 80% of CPU time to perform same computing jobs. Even if the developer has known all computing jobs
statistics, the computational performance gain would be around 45% as plotted with the gray bars.
The number of jobs that may be processed per hour on each day is plotted on the right side of Fig.5. The dark
gray bars present the number of jobs which can be processed on the conventional processing system. The gray bars
shows the number of jobs which are processed on an ILC applied system and the dark gray bars shows the number
of jobs processed without learning scheme for the corresponding hour in each day. The number of computational
jobs is significantly higher when the proposed learning and sliding window based caching algorithm is applied.

416

5.

M. Ercan,
Acarman
/ Procedia
Computer
1 000–000
(2012) 407–416
AuthorT.name
/ Procedia
Computer
Science Science
00 (2010)

Conclusion

The experimental study demonstrates that an efficient dynamic memory allocation may lead to remarkable
improvement on the system performance. In this study, idle memory has been scheduled to keep the outputs of the
repeating jobs having the same inputs and processing the same outputs. To present the potential benefits of the
proposed methodology, a real-time dynamic scheduling is performed.
In computer systems, the jobs run by the CPU with using the memory and the other system resources. In the
enterprise applications, the outputs of the repetitive jobs are kept in the memory by the system developer. Thus the
outputs of these jobs are brought from the memory without overloading the CPU. Caching some relevant jobs may
be efficient in terms of CPU processing time and performance improvement. And to select the jobs among a
predetermined set to separate the most suitable ones from the ones which may degrade or not affect positively the
overall computing performance. The success of analyzes are controversial.
In our study, the dynamic scheduling system defines its own requirements and keeps the outputs of the suitable
jobs in the memory on a basis of the previous learning experiences. The error and reference values are dynamically
updated to maximize the CPU and memory usage. Through our proposal, the developers will not have to decide
which jobs should be kept in the memory. The improvements on the system performance and the response time are
achieved. These dynamic interactions may improve the inherited systems to be more responsive to the coming
individual request and performs more computational jobs and more fast replies to the end users.
Acknowledgements

The authors gratefully acknowledge support of the Turkish National Research Council TÜBøTAK under grant
no: 106E121 and Galatasaray University through its research funding.
References
1. W. Ding and R. Guo, Design and Evaluation of Sectional Real-Time Scheduling Algorithms Based on System Load, Young Computer
Scientists, 9, 14 - 18, (2008).
2. C.L. Liu and J.W. Layland, Scheduling Algorithms for Multiprogramming in a Hard-Real-Time Environment, Journal of Association of
Computing Machinery, 20 (1), 46 - 61, (1973).
3. E.D. Jensen and C.D. Locke and H. Takuda, A Time-Driven Scheduling Model for Real-Time Operating Systems, Computer Science
Department, Carnegie-Mellon University, Pittsburgh, (1985).
4. H. Chen and J. Xia, A Real-Time Task Scheduling Algorithm Based on Dynamic Priority, Embedded Software and Systems, 431 - 436,
(2009).
5. S. Kantabutra and P. Kornpitak and C. Naramittakapong, Dynamic Clustering-Based Round-Robin Scheduling Algorithm, International
Symposium on Communications and Information Technology, 3, (2003).
6. C. Nock, Data Access Patterns: Database Interactions in Object-Oriented Applications, Addison Wesley, Boston, (2003).
7. C. Ford and I., Gileadi and S. Purba and M. Moerman, Patterns for Performance and Operability – Building and Testing Enterprise Software,
Auerbach Publications, (2008).
8. M. Butcher and A. Karimi and R. Longchamp, A Statistical Analysis of Certain Iterative Learning Control Algorithms, International Journal of
Control, 81, 156 - 166, (2008).
9. J. Moon and T. Doh and M.J. Chung, An Iterative Learning Control Scheme for Manipulators, Intelligent Robots and Systems, 2, 759 - 765,
(1997).
10. W. Yi and H. Zhongsheng and L. Xingyi, A Novel Automatic Train Operation Algorithm Based on Iterative Learning Control Theory,
Service Operations and Logistics, and Informatics, 2, 1766 - 1770, (2008) .
11. C. Mi and H. Lin and Y. Zhang, Iterative Learning Control of Antilock Breaking of Electric and Hybrid Vehicles, Vehicular Technology, 54
(2), 486 - 494, (2005).
12. P. Booth and R. Chadburn and S. Haberman and D. James and Z. Khorasanee and R.H. Plumb and B. Rickayzen, Modern Actuarial Theory
and Practice, Chapman & Hall/CRC, Florida, (2005).
13. J. Pitman, Probability, Springer, Pittsburg, (1999).
14. H. Ahn and K. L. Moore and Y. Chen, Iterative Learning Control: Robustness and Monotonic Convergence for Interval Systems, Springer,
(2007).
15. S. Clacke and E. Baniassad, Aspect-Oriented Analysis and Design: The Theme Approach, Pearson Education, (2005).

