Procedia Computer
Science
Procedia Computer
001(2010)
1–9
Procedia
ComputerScience
Science
(2012)
297–305

www.elsevier.com/locate/procedia

International Conference on Computational Science, ICCS 2010

Discrete ﬁrst- and second-order adjoints and automatic
diﬀerentiation for the sensitivity analysis of dynamic models✩
Ralf Hannemanna,c,1 , Wolfgang Marquardta , Uwe Naumannb , Boris Gendlerb
a Aachener

b LuFG

Verfahrenstechnik – Process Systems Engineering, RWTH Aachen University, 52056 Aachen, Germany
Informatik 12: Software Tools for Computationial Engineering, RWTH Aachen University, 52056 Aachen, Germany
c German Research School for Simulation Sciences GmbH, 52425 J¨
ulich, Germany

Abstract
We describe the use of ﬁrst- and second-order tangent-linear and adjoint models of the residual of linear-implicit
autonomous diﬀerential algebraic systems in the context of an extrapolated Euler scheme. The derivative code compiler dcc is applied to a C-implementation of the residual to get ﬁrst derivative code. Second-(and higher-)order
derivative models are obtained by reapplication of dcc to its own output. The resulting solver serves as a ﬁrst proof of
concept of a new platform for source-level manipulation of mathematical models that is currently under development
at RWTH Aachen University.
c 2012 Published by Elsevier Ltd. Open access under CC BY-NC-ND license.
⃝

Keywords: ﬁrst-order adjoints, second-order adjoints, discrete adjoints, one step extrapolation, parametric
diﬀerential-algebraic equations

1. Introduction
The general interest in mathematical modeling and numerical simulation has been growing with the ability to
handle increasingly complex scientiﬁc problems over the past decades. Nowadays, simulations are able to replace real
life experiments in many situations where these are either not possible (e.g. operator training for extreme situations in
chemical or power plant operation) or too expensive (simulation of crash behavior of cars). Thus, the ability to obtain
reliable information based on simulations is a key factor for success in industry as well as academia. Very high-level
modeling languages such as Modelica2 , VHDL3 , or gPROMS4 have been developed to support the user in formulating
the mathematical model. These tools allow to formulate models in an intuitive, equation-based representation. The
facilities of these descriptive modeling languages also allow for hierarchical, highly structured object-orientated modeling. All the aforementioned issues are related to model development and maintenance. While model simulation has
✩ This work was supported by the German Research Foundation under grant MA 1188/28-1 and by the ERS program from RWTH Aachen
University.
Email addresses: ralf.hannemann@avt.rwth-aachen.de (Ralf Hannemann), wolfgang.marquardt@avt.rwth-aachen.de
(Wolfgang Marquardt), bgendler@stce.rwth-aachen.de (Uwe Naumann), naumann@stce.rwth-aachen.de (Boris Gendler)
1 Corresponding author
2 http://www.modelica.org/
3 http://www.eda.org/vhdl-200x/
4 http://www.psenterprise.com/gproms/

c 2012 Published by Elsevier Ltd. Open access under CC BY-NC-ND license.
1877-0509 ⃝
doi:10.1016/j.procs.2010.04.033

298

R.Marquardt,
Hannemann
et al. / Procedia
Computer
ScienceComputer
1 (2012)Science
297–305
R. Hannemann, W.
B. Gendler
and U. Naumann
/ Procedia
00 (2010) 1–9

2

been the primary focus for the interpretation of the model equations, their use in optimization problems for diﬀerent
kinds of applications is becoming increasingly important in scientiﬁc as well as industrial applications. These optimization problem formulations include for instance (i) model parameter estimation, (ii) optimal design of experiments
for parameter estimation or model structure discrimination, (iii) design optimization, (iv) optimization of operations,
(v) soft sensing and process monitoring, and (vi) model-predictive control and real-time optimization.
The solution of each of these simulation and optimization problems typically requires the evaluation of symbolic
expressions involving diﬀerent type and order of derivatives. Most of the solution engines linked to a modeling
tool focus on one or few speciﬁc tasks - often just simulation - and do not provide derivative information required
by other tasks. Classical numerical approximation using ﬁnite diﬀerence approximations is often computationally
expensive and may be even infeasible for a large number of variables. Moreover, this approach yields a number of
disturbing numerical eﬀects making the quality of the obtained approximation highly questionable. A semantic code
transformation technique known as Automatic Diﬀerentiation (AD) [1] allows for derivatives of arbitrary order to be
computed eﬃciently and with machine accuracy. AD has been applied successfully to a large number of problems
in science and engineering [2, 3, 4]. Adjoint versions of the original simulation F (with F : Rnin → Rnout being
the operator mapping the data specifying the simulation experiment into the desired simulation results) are used to
compute gradients (nout = 1) of size nin at a small constant multiple of the computational cost Cost(F) of the simulation
itself.5
The aim of the AC-SAMMM project6 is to provide a tool that allows the calculation of speciﬁed symbolic expressions involving any kind of derivative based on AD using code transformation for a structured model represented in
any equation-oriented modeling language. This paper describes a ﬁrst successful integration of a software tool for the
solution of parametric diﬀerential-algebraic systems into the AC-SAMMM infrastructure.
2. Parametric diﬀerential-algebraic systems
Parametric diﬀerential-algebraic systems arise in many engineering applications. They describe kinetics of chemical reactions [5], complete chemical manufacturing processes [6], mechanical multibody systems [7], electrical circuits [8] or even biological systems [9]. In general, these systems cannot be solved analytically such that numerical
solution methods are required. The mathematical problem deﬁnition is as follows: Let f : Rn × Rm → Rn and
x0 : Rm → Rn be suﬃciently smooth, M ∈ Rn×n be a possibly singular matrix. Let p ∈ Rm be a constant parameter
vector and I = [0, T ] be a time interval. We consider the linear-implicit autonomous diﬀerential-algebraic equation
M x˙ = f (x, p),

(1)

where we assume the index of eq. (1) to be less or equal to one. Further, we assume that x0 provides consistent initial
conditions for eq. (1). The aim is to ﬁnd a continuously diﬀerentiable function x : I × Rm → Rn that solves the initial
value problem (IVP)
x(0, p) = x0 (p),
M x˙(t, p) = f (x(t, p), p)

∀ t ∈ I.

(2)
(3)

Under weak assumptions on f and x0 the solution x(t, p) is multiple continuously diﬀerentiable with respect to p,
which we assume in the following. For details on the existence theorems and mathematical properties of diﬀerentialalgebraic systems we refer to [8]. For a couple of computational tasks like parameter estimation [5] or dynamic
optimization [10] not only the solution of eq. (1) is required, but also the evaluation of a so-called objective function
φ : Rn → Rk to be evaluated at x(T, p) and thus resulting in an objective function Φ : Rm → Rk which is only
dependent from p ∈ Rm :
(4)
Φ(p) = φ(x(T, p)) ∈ Rk .
5 ...

as opposed to a computational complexity of O(nin ) · Cost(F) for ﬁnite diﬀerences or the tangent-linear model.
also http://wiki.stce.rwth-aachen.de/bin/view/Projects/ERS/WebHome.

6 See

R. W.
Hannemann
/ Procedia
Computer
1 (2012)
297–305
R. Hannemann,
Marquardt,etB.al.
Gendler
and U.
NaumannScience
/ Procedia
Computer
Science 00 (2010) 1–9

299
3

E. g. Φ could be sum-of-squares objective of a parameter estimation problem that shall be minimized. If the minimization is carried out by second-order gradient-based method the optimizer also requires the ﬁrst and second derivatives
of Φ with respect to p, namely
Φ p (p) = D p φ(x(T, p)),

and

Φ pp (p) = D pp φ(x(T, p)),

(5)

where we use subscripts to denote the partial derivative of a function and the D p notation for the total derivative
operator with respect to p. If Φ represents such an objective function, we have k = 1 which, for notational convenience,
we assume in the following. The generalization to k > 1 is straightforward. Here, the operator F : Rnin → Rnout
corresponds to φ, where nin = m and nout = k.
3. NIXE – a solver for diﬀerential-algebraic systems
NIXE is a C++ template class for the solution of eqs. (2) and (3) as well as for the eﬃcient computation of the
gradients and Hessians in eq. (5). The solver relies on the extrapolation of the linearly-implicit Euler discretization
[11, 12] which gives reason to its name: NIXE, an acronym for NIXE Is eXtrapolated Euler. The concepts of the
algorithm are brieﬂy sketched in the following.
The algorithm creates a series of basic step sizes H1 , . . . , HN and approximations for the states x0 , x1 , . . . , xN at
time instances t0 , t1 , . . . , tN , such that
i

ti =

Hi ,

tN = T,

l=1

xi ≈ x(ti , p),

i = 0, 1, . . . , N,

(6)

where by abuse of notation the right superscripts of x are indices, not powers. We sketch the step from xi−1 to xi . Let
jmax (i) be the maximal extrapolation order in this step. For j = 1, 2, . . . , jmax (i) and an increasing sequence of integers
n1 < n2 < n jmax (i) we set h j := Hi /n j , x j,0 := xi−1 , p j,k := p and apply the scheme
x j,k+1 = x j,k + (M − h A0 )−1 h j f (x j,k , p j,k ),

j = 1, . . . jmax (i), k = 0, . . . n j − 1,

(7)

where

∂ f i−1
(x , p).
(8)
∂x
This discretization scheme is of order 1 and provides us with a family of ﬁrst-order approximations x j,n j for xi . One
applies extrapolation to achieve higher orders [11]. The basic step sizes Hi and the extrapolation order jmax (i) are
determined on-line by a combined step size and order control. The reader is referred to [11] or [12, pp. 131–141] for
a more detailed presentation.
Since the linearly-implicit Euler discretization is a so-called W-method [12, p. 114] the Jacobian A0 could also be
replaced by an approximation [13]. This fact is exploited in the numerical computation of the ﬁrst-order sensitivities
A0 =

si (t, p) :=

∂x
(t, p), i = 1, . . . , m,
∂p

which satisfy the initial value problem
∂f
∂f
(x(t, p), p) si (t, p) +
(x(t, p), p),
∂x
∂pi
∂x0
(p).
si (0, p) =
∂pi

M s˙i (t, p) =

We set
⎛ ⎞
⎛
f (x, p)
⎜⎜⎜ x ⎟⎟⎟
⎜⎜⎜
⎜⎜⎜ s ⎟⎟⎟
⎜⎜⎜ f (x, p) s + f (x, p)
1
x
1
p1
⎜ ⎟
⎜
M := diag(M, . . . , M), y := ⎜⎜⎜⎜⎜ . ⎟⎟⎟⎟⎟ , F(y, p) := ⎜⎜⎜⎜⎜
..
.
⎜⎜⎜ . ⎟⎟⎟
⎜⎜⎜
.
⎝ ⎠
⎝
sm
f x (x, p) sm + f pm (x, p)

⎛
⎞
⎞
⎜⎜⎜ x0 (p) ⎟⎟⎟
⎟⎟⎟
⎜⎜⎜ x (p) ⎟⎟⎟
⎟⎟⎟
⎜ 0 p1 ⎟
⎟⎟⎟
⎟⎟⎟ , y0 (p) := ⎜⎜⎜⎜⎜ .. ⎟⎟⎟⎟⎟ .
⎜⎜⎜ . ⎟⎟⎟
⎟⎟⎟
⎝
⎠
⎠
x0 pm (p)

(9)
(10)

300

R.Marquardt,
Hannemann
et al. / Procedia
Computer
ScienceComputer
1 (2012)Science
297–305
R. Hannemann, W.
B. Gendler
and U. Naumann
/ Procedia
00 (2010) 1–9

4

The initial value problems in eqs. (2)-(3) and eqs. (9)-(10) can be solved as one system:
M y˙ (t, p) = F(y(t, p), p),

(11)

y(0, p) = y0 (p).

(12)

If the linear-implicit Euler discretization is used to solve eqs. (11) and (12) it turns out that the resulting numerical
computations are identical if the tangent-linear mode of automatic diﬀerentiation is applied to eq. (7). Hence, in this
case we have the equivalence
ﬁrst diﬀerentiate, then discretize ⇔ ﬁrst discretize, then diﬀerentiate.
The right hand side of eq. (11) includes already ﬁrst-order derivatives of f (x(t, p), p). Hence, The Jacobian of
F(y(t, p), p) involves second derivatives. However, Schlegel et al. [14] exploit the already mentioned W-method
property to show that these second-order derivative can be ignored while still preserving the convergence to the sensitivities. NIXE also implements this reduced method.
If k in eq. (4) is small, e.g. k = 1 in our assumed case of an objective function in parameter estimation, it may be
more eﬃcient to apply adjoint sensitivity analysis. NIXE implements a modiﬁed discrete-adjoint approach. It applies
the reverse mode of automatic diﬀerentiation to eq. (7) but ignores appearing second-order derivatives. Denoting
λ j,k ≈ x¯ j,k and ν j,k ≈ p¯ j,k the modiﬁed discrete adjoint vectors we get
λ j,k = λ j,k+1 + h j λTj,k+1 (M − h A0 )−1 f x (x j,k , p),
ν j,k = ν j,k+1 +

h j νTj,k+1 (M

−1

− h A0 ) f p (x j,k , p),

j = 1, . . . jmax (i), k = n j − 1, . . . , 0,
j = 1, . . . jmax (i), k = n j − 1, . . . , 0.

(13)
(14)

The discrete adjoints evolve backwards in time and indices. If xN is the numerically computed value for x(T, p), the
ﬁnal values of the modiﬁed discrete adjoints are given by λN = φ x (xN ) and νN = φ p (xN ) = 0. This modiﬁed discrete
approach yields derivatives which are, up to rounding errors, identical to the derivatives computed by Schlegel et
al. [14]. As in the case of ﬁrst-order adjoints, second derivatives are computed by a modiﬁed discrete second-order
adjoint scheme with similar eﬃciency-increasing reductions. The second summands in eqs. (13) and (14) involve
vector-Jacobian products. This motivates us to introduce the Hamiltonian
H : Rn × Rn × Rm → R,

H(x, λ, p) := λT f (x, p),

(15)

such that the vector-Jacobian products in eqs. (13) and (14) can be realized by evaluating ﬁrst-order partial derivatives
of H with respect to x and p.
The overall computation of Φ, Φ p and Φ pp consists of two steps.
1. One forward sweep which solves for x(T, p), the sensitivities si (T, p), i = 1, . . . , m, and the objective function
value Φ(p). At a couple of checkpoints the states and sensitivities are store for the subsequent backward sweep.
2. One backward sweep which implements the modiﬁed discrete second-order adjoint sensitivity analysis to compute Φ p (p) and Φ pp (p).
Aside from computational routines which are called only once to evaluate the initial and ﬁnal values for the
states, sensitivities, adjoints and second-order adjoints, a couple of derivative and derivative projections of f (x, p) and
H(x, λ, p) are evaluated several times during the numerical computations. During the forward sweep, these are the
following derivatives:
f : Rn × R m → R n ,

f x : Rn × Rm → Rn×n ,

f p : Rn × Rm → Rn×m .

(16)

During be backward sweep, the derivatives in (16) are required to locally reconstruct the forward solution. Additionally, the following derivatives are evaluated:
Hx :
D p Hx :
Dp Hp :

Rn × Rn × R m → R n ,
n

n

m

R ×R ×R ×R

n×m

Hp :
×R

n×m

Rn × Rn × Rm → R m ,

→R

n×m

(17)

.

(18)

Rn × Rn × Rm × Rn×m × Rn×m → Rm×m ,

(19)

R. W.
Hannemann
/ Procedia
Computer
1 (2012)
297–305
R. Hannemann,
Marquardt,etB.al.
Gendler
and U.
NaumannScience
/ Procedia
Computer
Science 00 (2010) 1–9

301
5

Table 1: Forward sweep

Set x0 := x0 (p) and s0 := (D p x0 )(p).
for i = 0, . . . , N − 1 do
Compute the Jacobian A0 = f x (xi , p)
for j = 0, . . . , jmax (i) do
Set h j := Hi /n j
Compute the LU-decomposition LU = M − h j A0
Set x j,0 := xi
for k = 0, . . . , j − 1 do
Compute f (x j,k , p), f x (x j,k , p) and f p (x j,k , p), i.e. derivatives of eq. (16)
Set x j,k+1 := x j,k + h j (LU)−1 f (x j,k , p)
Set s j,k+1 := s j,k + h j (LU)−1 f x (x j,k , p)s j,k + f p (x j,k , p)
end for
end for
Compute xi+1 and si+1 by means of extrapolation
end for
Set Φ(p) := φ(xN )
where λ = λ(p) is the adjoint vector. Employing NIXE, the derivatives marked by the preceding total derivative
operator D p have to be provided as projections by the user. All other derivatives have to be provided as whole
matrices, where the interfaces for f x and f p support dense and sparse storage schemes.
The detailed schedule of the forward sweep is given in Table 1. Note, that this presentation of the algorithm
exhibits redundant function evaluations which are avoided in practice.
The backward sweep is sketched in detail in Table 2. Here we denote with μ = D p λ and ξ = D p ν the matrices of
the second-order adjoints. Note, that the backward sweep requires information of the forward sweep which is restored
from a couple of checkpoints.
4. The Derivative Code Compiler dcc
The derivative code compiler dcc is a tool for automatic diﬀerentiation of C- code7 by semantic source code
transformation. For a given implementation of f (x, p) as f = g(x, p) in C- dcc generates code for the computation of
derivatives of arbitrary order. Higher derivatives are generated by reapplication of dcc to its own output. In particular,
dcc generates tangent-linear, ﬁrst- and second-order adjoint models.
The tangent-linear model f (1) = g(1) x, x(1) , p, p(1) ∈ Rn where
g(1) ≡ f (x, p) ·

x(1)
,
p(1)

(20)

is used to compute the Jacobian-vector product f x (x, p) x(1) + f x (x, p) p(1) . Note that we use parenthesized upper
indices (i) to denote tangent-linear projections and parenthesized lower indices (i) to mark adjoint projections. The
number i indicates that the variable is automatically created by the ith application of dcc.
x
The adjoint model (1) = g(1) x, x(1) , p, p(1) , f(1) ∈ Rn+m where
p(1)
g(1) ≡

x(1)
+ f (x, p)
p(1)

T

· f(1) ,

(21)

7 Our focus is on a subset of C/C++, from hereon referred to as C-, that is rich enough to cover the fundamental elements of numerical simulation
codes.

302

R.Marquardt,
Hannemann
et al. / Procedia
Computer
ScienceComputer
1 (2012)Science
297–305
R. Hannemann, W.
B. Gendler
and U. Naumann
/ Procedia
00 (2010) 1–9

6

Table 2: Reverse sweep

Set λN := φ x (xN ), νN := 0, μN := (D p φ x )(xN , sN , p)and ξ N := 0
for i = N − 1, . . . , 0 do
Compute the Jacobian A0 = f x (xi , p)
for j = 0, . . . , jmax (i) do
Set h j := Hi /n j
Compute the LU-decomposition LU = M − h j A0
Set λ j,n j := λi , ν j,n j := νi μ j,n j := μi and ξ j,n j := ξi
for k = j − 1, . . . , 0 do
Compute λˆ = (LU)−1 λ j,k+1 and μˆ = (LU)−1 μ j,k+1
ˆ p, s j,k , μ)
Compute the derivatives of eqs. (17) – (19) evaluated at (x j,k , λ,
ˆ
Do some work to compute λ j,k , ν j,k , μ j,k and ξ j,k
end for
end for
Compute λi , νi , μi and ξi by means of extrapolation
end for
Set Φ p (p) := (λ0 )T (x0 ) p (p) + ν0 and Φ pp (p) := (λ0 )T (x0 ) pp (p) + (μ0 )T (x0 ) p (p) + ξ0
computes ( xˆT , pˆ T ) + λT1 f x (x, p), f p (x, p) , which is stored in the variables x(1) and p(1) at the exit of the routine. Here,
λ1 = f(1) is the seeding vector and xˆT and pˆ take the initial values of x(1) and p(1) at the entry of the routine.
The second-order adjoint model
⎛ (2) ⎞
⎜⎜⎜ x ⎟⎟⎟
⎟⎟⎠ = g(2) x, x(2) , x(2) , p, p(2) , p(2) , f(1) , f (2) ∈ Rn+m
⎜⎜⎝ (1)
(1)
(1)
(1)
(1)
p(2)
(1)
is deﬁned by

g(2)
(1)

⎛ (2) ⎞
⎜⎜ x ⎟⎟⎟
⎟⎟⎠ + f (x, p)
≡ ⎜⎜⎜⎝ (1)
p(2)
(1)

T

(2)
· f(1)
+ < f(1) , f (x, p),

x(2)
>,
p(2)

(22)

where the second term is the projection of the (n × (n + m) × (n + m))-Hessian tensor f (x, p) in directions f(1) ∈ Rn
x(2)
and (2) ∈ Rn+m . Note that these projections are uniquely deﬁned due to the symmetry of f (x, p) in the (n + m)p
dimensions. Please also note that (20)-(22) have to be interpreted as assignments, especially (x(1) , p(1) )T in (21) and
(x(2) , p(2) )T in (22) serve both as input and output variables, which are usually initialized to zero by the user.
The tangent-linear model is used to compute both f x (x, p) ∈ Rn×n and f p (x, p) ∈ Rn×m . Let f (x, p) be implemented
by a C-routine with the following signature
v o i d g ( i n t n , i n t m, d o u b l e ∗ x , d o u b l e ∗ p , d o u b l e ∗ f )

where the inputs x and p are n- and m-vectors, respectively, and where the n outputs are returned in the vector f. dcc
generates a tangent-linear version of g with the following signature:
v o i d d 1 g ( i n t n , i n t m, d o u b l e ∗ x , d o u b l e ∗ d1 x ,
d o u b l e ∗ p , d o u b l e ∗ d1 p , d o u b l e ∗ f , d o u b l e ∗ d 1 f )

The superscript in v(1) is represented by a corresponding preﬁx d1 v. The actual source for d1 g must be omitted
due to space restriction. Refer to the AC-SAMMM website8 for examples of ﬁrst- and higher-order derivative codes
generated automatically by dcc.
f x (x, p) is accumulated by letting d1 x range over the Cartesian basis vectors in Rn . The vector d1 p is set to zero.
d1 f contains the product of the Jacobian of f with respect to both x and p with x˙ =d1 x and p˙ = 0. Sparsity in f x (x, p)
8 www.stce.rwth-aachen.de

R. W.
Hannemann
/ Procedia
Computer
1 (2012)
297–305
R. Hannemann,
Marquardt,etB.al.
Gendler
and U.
NaumannScience
/ Procedia
Computer
Science 00 (2010) 1–9

303
7

should be exploited, for example, through direct Jacobian compression techniques [15]. Total derivatives in directions
corresponding to sums of Cartesian basis vectors representing structurally orthogonal columns9 in f x are computed
in this case. Similarly, f p (x, p) is accumulated by letting d1 p range over (sums of) the Cartesian basis vectors in Rm .
The vector d1 x must be set to zero. Consequently, d1 f contains the product of the Jacobian of f with respect to both
x and p with x˙ = 0 and p˙ =d1 p.
A total of n + m runs of d1 g is required to compute f x and f p in tangent-linear mode. Depending on the actual
values for n and m it might be computationally less expensive to perform the at most n runs of the adjoint model
to accumulate f x and f p as products of the transposed Jacobian of f with respect to both x and p with (sums of)
Cartesian basis vectors in Rn . Typically the adjoint code generated by dcc yields a constant overhead of at least two
when compared to the tangent-linear code. The product of the transposed Jacobian with an n-vector takes roughly
twice as long as the product of the Jacobian with an n − m-vector due to the data ﬂow reversal mechanism that is
inherent to any adjoint code. Refer to [16] for details. For most of our current target applications we observe that
n ≈ m. Hence, the computational costs of accumulating f x and f p in tangent-linear or adjoint mode are roughly
identical.
The adjoint model of f is used to compute both H x (x, p, λ) = λT f x (x, p) ∈ Rn and H p (x, p, λ) = λT f p (x, p) ∈ Rm .
dcc generates an adjoint version of g with the following signature:
v o i d b 1 g ( i n t n , i n t m, d o u b l e ∗ x , d o u b l e ∗ b1 x ,
d o u b l e ∗ p , d o u b l e ∗ b1 p , d o u b l e ∗ f , d o u b l e ∗ b 1 f )

The subscript in v(1) is represented by a corresponding preﬁx b1 v. For use with NIXE we set b1 f=λ and b1 x=b1 p=0.
dcc supports interprocedural data ﬂow reversal based on subroutine argument checkpointing also known as joint
call tree reversal [1]. Arbitrary checkpointing schemes can be implemented by the user through appropriate restructuring of the original source code. Code for the storage and recovery of the subroutine arguments needs to be provided
by the user as its automatic generation at compile time constitutes an in general undecidable problem due to missing
array descriptors in C/C++. Fortran allows for a higher level of automation. Moreover, the user has to provide upper
bounds on the sizes of the global data and control ﬂow reversal stacks that are allocated statically by dcc. Other
derivative code compilers, such as, for example, OpenAD [17] or Tapenade [18] implement dynamically growing
stacks. While this approach is more convenient for the user it complicates the generation of higher-order adjoint
codes by reapplication of the derivative code compiler to its own output. Algorithmically the two approaches are
similar. A more detailed discussion of the generated adjoint code is beyond the scope of this paper.
The second-order adjoint model of f is used to implement both D p H x = H xx x(2) + H xλ λ(2) + H xp and D p H p =
H px x(2) + H pλ λ(2) + H pp , where x(2) and λ(2) can be interpreted as the partial derivatives (∂/∂p)x(t, p) respectively
(∂/∂p)λ(t, p) evaluated at the current time t10 . Reapplication of dcc in tangent-linear mode to the previously generated
adjoint code yields a second-order adjoint version of g with the following signature:
void d2 b1
double ∗
double ∗
double ∗

g ( i n t n , i n t m,
x , d o u b l e ∗ d2 x , d o u b l e ∗ b1 x , d o u b l e ∗ d 2 b 1 x ,
p , d o u b l e ∗ d2 p , d o u b l e ∗ b1 p , d o u b l e ∗ d 2 b 1 p ,
f , double ∗ d2 f , double ∗ b1 f , double ∗ d 2 b 1 f )

The sub- and superscripts in v(2)
(1) are represented by a corresponding concatenation of preﬁxes d2 b1 v. To compute
(2)
=< λ, f xx (x, p), x(2) >
H xx (x, p, λ)x(2) = x(1)

and

(2)
H xp (x, p, λ)p(2) = p(2)
>
(1) =< λ, f xp (x, p), p

we set b1 f=λ and d2 b1 x=d2 b1 p=d2 b1 f=0 in addition to x, d2 x, p, and d2 p. H xλ (x, p)λ(2) = f x (x, p)λ(2) is
computed eﬃciently using the tangent-linear model of f.
9 Two

10 The

columns are structurally orthogonal if they do not have nonzero entries in the same row.
terms H xp and H pp are derived from the projections H xp p(2) and H pp p(2) where p(2) = ∂p/∂p = I is the identity matrix.

304

R.Marquardt,
Hannemann
et al. / Procedia
Computer
ScienceComputer
1 (2012)Science
297–305
R. Hannemann, W.
B. Gendler
and U. Naumann
/ Procedia
00 (2010) 1–9

8

5. Case studies
Our algorithm is tested using two test problems: one batch reactor model from [19] and a reduced model of
a domestic heating system with stratiﬁed storage tank which is taken from [20, 21]. The batch reactor example
originally stems from the Dow Chemical Company [5]. The reactor is modeled by a diﬀerential-algebraic initial value
problem of type (2)-(3) with n = 10 states, m = 8 parameters and a scalar objective function (k = 1) . The state
vector x represents the time-dependent concentrations, the parameter vector p represents the time-invariant kinetic
coeﬃcients.
The domestic heating system is modeled by a set of parametric partial diﬀerential-algebraic equations. The partial
diﬀerential equations are discretized using the method of lines (MOL) [22] resulting in a set of parametric ordinary
diﬀerential-algebraic equations. The number of state variables strongly depends on the ﬁneness of the MOL discretization. Since we ran the problem with the dense linear algebra options, we used a coarse discretization to totally
result in n = 116 states and m = 33 parameters. We chose an arbitrary scalar objective function (k = 1), just to
evaluate the speed of the gradient and Hessian computations.
The user provided derivative functions for the forward sweep in eq. (16), and the reverse sweep in eqs. (17) – (19)
were implemented using two modes: operator overloading by standard techniques and source code transformation
by means of the derivative code compiler dcc. In our setting, all functions of the forward sweep are constructed by
tangent-linear drivers while all functions of the backward sweep use ﬁrst- or second-order adjoint drivers.
All computation were performed on notebook running Windows 7 on a 2.53 MHz CPU11 using the Visual Intel
C++ Compiler (version 11.1) using processor speciﬁc optimizations. The results for the batch reactor problem are
shown in Table 3, the ones for the domestic heating system are presented in Table 4.
For each mode (operator overloading or dcc) we measured four values: the time only for the state integration
(zeroth-order), for the state and forward sensitivity equations (zeroth- and ﬁrst-order), for the ﬁrst-order adjoint backward sweep (ﬁrst-order) and for the second-order adjoint backward sweep (ﬁrst- and second-order). The times for
the backward sweep do not include the preceding forward sweep, such that e.g. in Table 1 the total time of the batch
reactor problem Hessian computation employing dcc-generated residual derivatives comprises the 0th- & 1st-order
forward and the 1st- & 2nd-order backward sweep to result in a total computational time of 38 ms.
Table 3: Gradient and Hessian computation for batch reactor problem (in milliseconds)

Mode
Operator overloading
dcc

Forward sweep
only 0th-order 0th- & 1st-order
7 ms
26 ms
4 ms
13 ms

Backward sweep
only 1st-order 1st & 2nd-order
15 ms
127 ms
7 ms
25 ms

Table 4: Gradient and Hessian computation for domestic heating system problem (in seconds)

Mode
Operator overloading
dcc

Forward sweep
only 0th-order 0th- & 1st-order
0.25 s
2.40 s
0.26 s
3.50 s

Backward sweep
only 1st-order 1st & 2nd-order
0.36 ms
10.2 s
0.36 s
4.30 s

We see that for the ﬁrst case study, NIXE runs signiﬁcantly faster with the dcc-generated derivatives. In the
second case study, overloading outperforms dcc for the residual derivatives of the combined zeroth- and ﬁrst-order
forward sweep. For the combined ﬁrst- and second-order backward sweep, dcc is signiﬁcantly faster.
Moreover, the modiﬁed discrete adjoint approach of NIXE seems to be a promising techniques: using dccgenerated derivatives for the case studies, the backward sweep costs maximal two times the forward sweep.
11 Intel(R)

Core(TM)2 Duo P9600

R. W.
Hannemann
/ Procedia
Computer
1 (2012)
297–305
R. Hannemann,
Marquardt,etB.al.
Gendler
and U.
NaumannScience
/ Procedia
Computer
Science 00 (2010) 1–9

305
9

6. Summary and outlook
We presented two core computational routines of the AC-SAMMM project: NIXE, a solver for (second-order)
adjoint sensitivity analysis of parametric diﬀerential-algebraic equations, and dcc, the derivative code compiler which
provides arbitrary high derivatives of C-code by semantic code transformation. In a case study, dcc generated code
was interfaced to NIXE and compared with derivative generation by operator overloading. The modiﬁed discrete
adjoint approach of NIXE seems to be a fast implementation for adjoint sensitivity analysis, since for the two case
studies, the backward sweep costs maximal two times the forward sweep.
Future work will focus on algorithmic improvements of NIXE and dcc, main extensions regarding the problem
scope as well as on the use of Modelica to specify the problem formulation in a high-level language.
References
[1] A. Griewank, A. Walther, Evaluating Derivatives. Principles and Techniques of Algorithmic Diﬀerentiation, SIAM, Philadelphia, 2008.
[2] C. Bischof, M. B¨ucker, P. Hovland, U. Naumann, J. Utke (Eds.), Advances in Automatic Diﬀerentiation, Lecture Notes in Computational
Science and Engineering, Springer, Berlin, 2008, to appear.
[3] M. B¨ucker, G. Corliss, P. Hovland, U. Naumann, B. Norris (Eds.), Automatic Diﬀerentiation: Applications, Theory, and Tools, no. 50 in
Lecture Notes in Computational Science and Engineering, Springer, Berlin, 2006.
[4] G. Corliss, C. Faure, A. Griewank, L. Hascoet, U. Naumann (Eds.), Automatic Diﬀerentiation of Algorithms – From Simulation to Optimization, Springer, New York, 2002.
[5] G. Blau, L. Kirkby, M. Marks, An industrial kinetics problem for testing nonlinear parameter estimation algorithms, Process Math Modeling
Department, The Dow Chemical Company (1981).
[6] G. D¨unnebier, D. v. Hessem, J. Kadam, K.-U. Klatt, M. Schlegel, Optimization and control of polymerization processes, Chemical Engineering Technology 28 (5) (2005) 575–580.
[7] P. Betsch, P. Steinmann, A dae approach to ﬂexible multibody dynamics, Multibody System Dynamics 8 (3) (2002) 365–389.
URL http://dx.doi.org/10.1023/A:1020934000786
[8] K. E. Brenan, S. L. Campbell, L. R. Petzold, Numerical Solution of Initial-Value Problems in Diﬀerential-Algebraic Equations, SIAM, 1996.
[9] Q. Zhang, C. Liu, Dynamical behavior in a diﬀerential-algebraic algal blooms model, in: Control Conference, 2008. CCC 2008. 27th Chinese,
2008, pp. 757–761. doi:10.1109/CHICC.2008.4605893.
[10] M. Schlegel, K. Stockmann, T. Binder, W. Marquardt, Dynamic optimization using adaptive control vector parameterization, Computers &
Chemical Engineering 29 (8).
[11] P. Deufhard, E. Hairer, J. Zugck, One-step and extrapolation methods for diﬀerential- algebraic systems, Numer. Math. 51 (5) (1987) 501–
516.
URL http://dx.doi.org/http://dx.doi.org/10.1007/BF01400352
[12] E. Hairer, G. Wanner, Solving Ordinary Diﬀerential Equations II - Stiﬀ and Diﬀerential-Algebraic Problems, Springer, Berlin, 1996.
[13] U. Nowak, Dynamic sparsing in stiﬀ extrapolation methods, IMPACT Comput. Sci. Eng. 5 (1) (1993) 53–74.
doi:http://dx.doi.org/10.1006/icse.1993.1003.
[14] M. Schlegel, W. Marquardt, R. Ehrig, U. Nowak, Sensitivity analysis of linearly-implicit diﬀerential-algebraic systems by one-step extrapolation, Appl. Numer. Math. 48 (1) (2004) 83–102. doi:http://dx.doi.org/10.1016/j.apnum.2003.07.001.
[15] A. Curtis, M. Powell, J. Reid, On the estimation of sparse Jacobian matrices, Journal of the Institute of Mathematics and Applications 13
(1974) 117–119.
[16] U. Naumann, DAG reversal is NP-complete, Journal of Discrete Algorithms (7) (2009) 402–410.
[17] J. Utke, U. Naumann, C. Wunsch, C. Hill, P. Heimbach, M. Fagan, N. Tallent, M. Strout, OpenAD/F: A modular, open-source tool for
automatic diﬀerentiation of Fortran codes, ACM Transactions on Mathematical Software 34 (4).
[18] L. Hasco¨et, V. Pascual, TAPENADE 2.1 user’s guide, Rapport technique 300, INRIA, Sophia Antipolis (2004).
URL http://www.inria.fr/rrrt/rt-0300.html
[19] M. Caracotsios, W. Stewart, Sensitivity analysis of initial value problems with mixed odes and algebraic equations, Comp. Chem. Eng. 9 (4)
(1985) 359–365.
[20] T. Kreuzinger, Control of a domestic heating system with stratiﬁed storage tank, Ph.D. thesis, RWTH Aachen University (2009).
[21] T. Kreuzinger, M. Bitzer, W. Marquardt, Mathematical modelling of a domestic heating system with stratiﬁed storage tank, Mathematical and
Computer Modelling of Dynamical Systems 14 (3) (2008) 231–248.
[22] W. Schiesser, The numerical method of lines: integration of partial diﬀerential equations, Academic Press San Diego, 1991.

