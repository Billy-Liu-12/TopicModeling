Procedia
Computer
Science

Procedia
Computer
Science
1 (2012)
2549–2557
Procedia
Computer
Science
00 (2009)
000±000

www.elsevier.com/locate/procedia
www.elsevier.com/locate/procedia

International Conference on Computational Science, ICCS 2010

Hybrid Blob and Particle Filter Tracking Approach for Robust Object Tracking
Sze Ling Tanga, Zulaikha Kadima, Kim Meng Lianga, Mei Kuan Lima
a

MIMOS Berhad,Technology Park Malaysia,Kuala Lumpur and 57000, Malaysia

Abstract
Analysing and characterising human behaviour is now receiving much attention from the visual surveillance research
community. Generally, human behaviour recognition requires human to be detected and tracked so that the trajectory patterns of
the human can be captured and analysed for further interpretation. Therefore, it is crucial for tracking algorithms to be fast and
robust to partial and short-life occlusion. In addition, the detection of object-of-interest to be tracked should be automated,
without the need for manual intervention. This paper thus proposes a tracking system targeted for real time surveillance
applications that integrate blob and simplified particle filter tracking approaches so as to exploit the advantages of both
approaches while minimizing their respective disadvantages. The blob approach acts as the main tracking and will invoke the
simplified particle filter tracking in the event of blob merging or occlusion. In this paper, the proposed tracking method is tested
using PETS 2009 sequences to illustrate the capability of solving occlusion and obstruction in the scene. The results show that
the proposed system successfully tracks objects during and after occlusion with other objects or after obstructed by the
background.
c 2012 Published by Elsevier Ltd. Open access under CC BY-NC-ND license.
⃝
Keywords: Blob tracking; Particle filter; Motion detection; Hybrid tracking

1. Introduction
Typical surveillance systems require human intervention to monitor the behaviors of subjects in the usually
complex scenes. Manual observation by human however is not appropriate, as it requires attentive and careful
concentration over a long period of time. Therefore there is a dire need for an automated vision-based surveillance
system to assist human in monitoring and detecting abnormal behaviours in the scene. Generally human behavior
recognition in an automated surveillance system requires human to be detected and tracked first before analysing
them for further interpretation. Thus, object tracking plays an important role in any automated surveillance system.
Object detection and tracking is important in many applications such as in visual surveillance, video
communication and human computer interaction. Video object detection and tracking can be defined as the process
of locating a moving object of interest in the scene and assigning a consistent object label to the same object
throughout the video image sequences. This process is not a trivial task due to the following difficulties: a) fast
moving object, b) scene illumination changes, c) image appearance changes as the image viewpoint changed, d)
partial or full occlusion of object-to-object and object-to-scene, e) non-rigid object shape as it moves and f) real time
processing requirement.
In this paper, a hybrid approach to track object in crowded scene is proposed and the idea has been patented as in
[17], targeted for real-time surveillance application. For real time application, the deployed tracking system must
c 2012 Published by Elsevier Ltd. Open access under CC BY-NC-ND license.
1877-0509 ⃝
doi:10.1016/j.procs.2010.04.288

2550

S.L. Tang et al. / Procedia Computer Science 1 (2012) 2549–2557

Sze Ling T. et al. / Procedia Computer Science 00 (2010) 000±000

fulfill four common criteria which are; i) providing fast tracking, ii) minimizing user intervention by automatically
detecting object of interest for tracking, iii) able to track multiple objects and iv) robust to full or partial occlusion.
Thus, an object tracking system which utilises the integration of blob tracking and simplified particle filter approach
is proposed so as to fully exploit the advantages offered by both approaches and minimize their individual
weaknesses in order to achieve the requirements for real time application. This paper is organized as follows: section
2 reviews the conventional object tracking methods. In section 3, we describe the proposed method in detail. Then,
we demonstrate the experimental results and followed by the discussion in section 4. Finally, we conclude the
proposed hybrid tracking approach in section 5.
2. Related Works
All In this section, we present the summary of our survey on the state-of-the art methods for object tracking.
Object tracking is defined as the problem to estimate the trajectory of the object of interest moving in the scene. In
general, object tracking is a process that involves: i) detecting object of interest to be tracked and use that
information to initialize the tracking model, ii) compute the correspondence of objects in current frame with that of
the previous frame (if any) to ensure that that a consistent label is assigned to the same object throughout the image
sequence and iii) tracking algorithm will estimate the location of the object in current image frame and use that
information to update the tracking model.
In bottom-up approaches, object detection is applied in every image frame. Then, the correspondence between
the detected objects in consecutive frames is established to enable tracking of the object in the subsequent frames.
To detect an object, image is normally segmented into blobs or regions using some common segmentation
techniques such as background subtraction mean shift clustering and graph cuts. Segmented regions are then
grouped together to represent an object based on some deterministic rules such as proximity (pixel connectivity) and
homogeneous motion vector (optical flow) [1].
In the early generation, Pfinder [2] is a well-known real time system which tracks the single entire human body
in the scene without occlusion. It uses a unimodal background model to locate the moving object via background
subtraction. In W4 system proposed by I. Haritouglu et al [3], dynamic appearance models is used to track people
during occlusion. Single people and groups are distinguished using projection histograms and each person in a group
is tracked by tracking their head. In [5], ARJ Francois et al proposed a real time blob tracking algorithm by
establishing temporal relationships between blobs that can be analyzed at a higher semantic level. Real time is
achieved by employing multi resolution following coarse to fine hypothesis generation, propagation and refinement
process. The algorithm can reliably track not only large, slow blobs but also small fast blobs. But, the algorithm is
not addressing higher level issues such as blob splitting and merging. In [6], an adaptive block-based approach is
proposed that avoids segmentation in every frame except for initialization of the object during the initial frame.
Tracking is achieved by estimating motion between frames to detect occlusion and dis-occlusion. Uncovered region
is classified into actual dis-occlusion or false alarm by observing the motion characteristics of uncovered regions.
Object can be tracked accurately for a long time without requiring re-initialization by modifying the object mask to
take care of occlusion and dis-occlusion. The method can be extended to track multiple objects. Unfortunately, the
methods perform poorly for objects that are relatively small. In [7], Fuentes et al proposed tracking of multiple
objects in complex environments. Foreground pixels are detected using luminance contrast and grouped into blobs.
Blobs from two consecutive frames are matched creating the matching matrices. Tracking is performed using direct
and inverse matching matrices. The proposed method successfully solved blob merging and splitting.
Silhouette or contour based tracking is more effective than the blob tracking especially tracking objects under
disturbance or partial occlusion. However, this tracking method is highly sensitive to the initialization of the
tracking, making it difficult to start tracking automatically. For instance, Mikel and Mubarak [8], investigated in
methods that enable efficient use of silhouette-based cues in the presence of heavy occlusion of the kind present in
most urban environments. Davis et al [9] have also endeavor to utilize silhouette-based cues by comparing edges to
series of learned models. Wu et al [10] have presented learning human silhouette models and representing via
Boltzmann distribution in a Markov Field.
On the other hand, the top-down approach mostly associated with high computational complexity but more
robust to object occlusion. This tracking approach basically generates object hypothesis and tries to verify them
using image. The object to be tracked is detected only in the first frame the object appears. The detected object
features are then used to initialize the tracker. Initialization stage is critical for this approach and it will affect the
tracking performance. For this reason, most of state-of-the art methods employing manual tracker initialization

S.L. Tang et al. / Procedia Computer Science 1 (2012) 2549–2557

Sze Ling T. et al./ Procedia Computer Science 00 (2010) 000±000

2551

whereby user intervention is required or automatic initialization can be done by employing some intelligent and
complex object detection method. Apart from expensive processing speed, top down tracking approaches promise
desirable result for tracking through partial object occlusion.
Feature based as described in [11] is part of top down tracking approach whereby the algorithms perform
recognition and tracking of objects by extracting elements, clustering them into higher level features and matching
the features between images. Object is tracked by computing the motion of the features in consecutive frames. This
motion is usually in the form of a parametric transformation such as translation, rotation and affine.
T. A. J. Lipton et al. [12], using shape and colour information to detect and track multiple people and vehicles in
a cluttered scene and monitor activities over a large area and extended periods of time. However, these methods
required complicated calculation or expensive computational power.
3. System Overview
The main objective of the tracking is to be able to locate the same foreground object in subsequence image
frames. To indicate the same object throughout the image sequence, each object tracker with a unique object
identifier is assigned to each object. In the hybrid framework, blob tracking is employed as the main tracking system
that will invoke simplified particle filter when objects are merging or when occlusion happens. Then the output from
simplified particle filter will be used to update object tracker properties kept by the blob tracking.
3.1. Blob tracking approach
In blob tracking, firstly, object is extracted from current frame based on background subtraction method. In this
stage, object detection can be achieved by building a representation of the background model of the scene and
finding the deviations from the model for each incoming frame. The background model is updated in every frame in
order to capture any global gradual illumination changes. The background is selectively updated whereby only
pixels belonging to background in any frame will be considered to update the model. This is to prevent background
model to be polluted by pixels logically not belonging to the background scene. The rate at which the background
model is updated can be controlled by the user. At a higher rate, static foreground such as human standing or sitting
still for a long period of time will be updated as background. Background subtraction is chosen due to its simplicity
in implementation yet providing acceptable accuracy in an indoor and static camera environment. For outdoor
surveillance environment such as in the parking lot, currently the problem of dynamic background such as waving
leaves is handled by marking specific region of interest which excludes these background regions.
Foreground object is then extracted from current frame based on the subtraction of the current frame from the
learned background model. Any significant change in an image region from the background model signifies a
moving object. The resulting extracted foregrounds which indicate only the significant motion pixels, will then
preprocessed using connected component algorithm followed by standard morphological filtering such as opening
and closing to remove noise and smooth the boundary of motion blob. Size filter is then applied to remove other
noise which motion blob size is smaller than certain presumed human size (e.g. 1.5 ratio of height to width). The
resulting map after preprocess denoted as the motion map and each of connected pixels is denoted as the motion
blob. Next, current motion map will be compared with its equivalent map from previous frame to obtain previouscurrent motion blobs relationship. Each of motion blobs in current motion map will be compared to each of motion
blobs from previous motion map to determine any one of four events; new, existing object, blob splitting or blobs
merging. In this case, overlapping region analysis is used as a comparison method. The rule of determining these
events based on the overlapping previous-current blobs test is illustrated in Figure 1. Referring to Figure 1, each of
the detected events will invoke different processes of updating the tracker information and assigning the object
identifier. The processes are described as follows:
i.

If one previous blob, Oj,i-1, where j denotes the blob number of previous blob and i denotes the frame number,
overlapped with only one current blob, OM¶L, where M¶ denotes the blob number of current blob then the Oj,i is
corresponding to an existing object tracker and it will be continued tracked and labeled as the same object label
assigned to its overlapped Oj,i-1.
Oj,i-1

OM¶L

same label

(1)

2552

ii.

S.L. Tang et al. / Procedia Computer Science 1 (2012) 2549–2557

Sze Ling T. et al. / Procedia Computer Science 00 (2010) 000±000

If OM¶L does not overlap with any Oj,i-1, that blob will be taken up as a new object to be tracked and process new
object sub-routine will be invoked to assign an object tracker for this newly identified object.
Oj,i-1

OM¶L

new object

(2)

iii. If one Oj,i-1 overlaps with more than one OM¶L, splitting event is detected.
{O0,i, O1,i, O2,i «2M¶L}

Oj,i-1

splitting

(3)

iv. If more than one Oj,i-1, relates to only one blob, merging event is taking place and top-down tracking approach
will be invoked to estimate the location of each object in the merged motion blobs.
OM¶L {O0,i-1 ,O1,i-1, O2,i-1 « Oj+1,i-1}

merging

(4)

Fig. 1. Blob event detection based on previous-current blob relationship.

3.1.1. Existing tracked object
When there is one current blob, relates to only one previous blob, then it is assumed that current blob contains
the previously tracked objects. If related previous blob is belong to single object identifier, then the current blob
tracker will be assigned the same object identifier and tracker location will be updated based on the location of
current blob. Whereas if previous blob belongs to more than one object identifiers, it is assumed that previous blob
was tracked by using top down tracking approach. All object identifiers from previous blob will be assigned to
current blob, and then the object locations within current blob will be estimated by previously initialized particle
filter trackers.
3.1.2. Process new object
In the process new object sub-routine, initially, ratio test will be performed on the motion blob identified as new
object to be tracked. This ratio test is to determine whether the blob contains a single or multiple objects. If the blob
contains multiple objects, a human detection algorithm using profile of silhouette which similar with human

2553

S.L. Tang et al. / Procedia Computer Science 1 (2012) 2549–2557

Sze Ling T. et al./ Procedia Computer Science 00 (2010) 000±000

detection in W4 [13] is applied on the blob to separate multiple objects into each single object. Only an absolute
single object (i.e. entire object with estimated human ratio) within the multiple objects blob is worth to be tracked at
this stage. Then for each of single object blob, its object properties e.g. colour will be extracted together with its
location information (e.g. oEMHFW¶V FHQWURLGSubsequently, a tracker will be initialized for each of absolute single
objects within the blob. Each tracker will hold the properties and location information of each object. In order to
assign object label identifier, first of all the object features will be matched with all objects in the memory. Objects
in the memory are the objects have been tracked before and already leave the scene for not more than a certain
number of frames. If current object is found similar to any object in the memory, then current object is assigned as
the same identifier as the object in the memory and object in memory will be removed. Otherwise, a new object
identifier will be assigned to current object.
3.1.3. Blob splitting
In the blob splitting event, if previous blob belongs to single object identifier, then trackers with new identifiers
are assigned to the new splitted blobs and each tracker locations are initialized to current blob locations. Whereas if
previous blob belongs to multiple object identifiers, then it is assumed that previous blob was tracked using previous
initialized particle filter trackers, then object locations with current blobs will be continued tracked using these
trackers. In this case, particles propagations are concentrated within all splitted current blobs. During blob splitting,
trackers properties will be updated only if the splitted blob belongs to one identifier to ensure that object is clearly
appear in the scene.
3.1.4. Blob merging
Blob merging indicating the first occurrence of objects occlusion. Correspondingly, top down tracking approach
will be invoked and its details are explained in section 3.2. During blob merging, tracker properties or models will
not be updated to avoid any error due to lost of object properties because of occlusion.
3.2. Simplified particle filter tracking approach
In our proposed tracking approach, a simplified particle filter will be invoked only when there are at least two
motion blobs in previous frame merged as a single blob in current frame. From the blob tracking, we can know
which motion blobs, what are their object identifiers and their locations in previous frame before they merged as one
blob. We use that information to initialize the target object. In our case, kernel-based color distribution feature as in
[14] is associated with the simplified particle filter. Color distribution of the each target objects are obtained from
previous frame (the frame just before the objects merge). This step will overcome the requirement of manual tracker
initialization, limitation of model- based tracker initialization and the requirement of difficult object segmentation to
first detect the object for initialization.
After tracker initialization, then the sample set of the tracker is propagated randomly but concentrated to be
within the location of the merged blob in current motion map. This is done based on the prior knowledge that the
merged object location in current frame must be within the area of merged motion blobs detected in current motion
map. In general, particle filter is a Bayesian sequential importance sampling technique, which recursively
approximates the posterior distribution using a finite set of weighted samples. In particle filtering the conditional
state density p(Xt|Zt) at time t is represented by a sets of samples, S={(s(n), (n)}|Q «1 where each sample is
represented by sample state, s(n) and its weight or sampling probability, (n), where
. The weights define the
( n)
1

n

importance of a sample, that is, its observation frequency [15]. In our approach, each sample is represented by a
sample set consists of enclosing bounding box of the target object, s(n)={x,y,W,H} where (x,y) is the object centroid,
(W,H) is the objects width and height. After the propagation, hypotheses distribution will be computed at each of the
samples locations. These hypotheses are then compared to the target object distribution using Bhattacharyya
distance and weight of their corresponding samples is updated accordingly based on Eq. (5) [16].

2554

S.L. Tang et al. / Procedia Computer Science 1 (2012) 2549–2557

Sze Ling T. et al. / Procedia Computer Science 00 (2010) 000±000
d (n)

1

( n)

2

2pi

e2

2

(5)

Where d corresponds to the Bhattacharyya distance between target color distribution, q and the nth sample
hypothesis color distribution, p(n) as in Eq. (6).
d (n) 1

[ p (n) , q]

(6)

In Eq. (6), [p,q] is the measure between 2 distribution, p and q, and called as Bhattacharyya coefficient given in
Eq. (7).

[ p ( n ) , q]

m
u 1

p (nu) qu

(7)

Where m is the number histogram bin, q is the target color distribution and p(n) is the sample hypothesis color
distribution.
Finally, sample set of target object tracker will be updated based on the mean of the sample set as in Eq. (8).
Calculated mean represents the estimated object location.
E(S)

(n) (n)
n

s

(8)

4. Experimental results and discussion
In this section we show the experimental results using standard datasets, PETS 2009 [18] for performance
evaluation purposes. 2 sequences of image from Performance Evaluation in Tracking for Surveillance (PETS)
Workshop 2009 are used for performance evaluation. The dataset S2.L1 collection with scenario - walking with
elements sparse crowd is investigated. We choose the sequences from view 001 and view 008 for our evaluation due
to the wide angle view in order to reduce the possibility of capturing object partially. Dataset view 001 consist more
than 8 persons with similar color properties walking from various directions. While dataset view 008 consists the
same condition but the scene with tree obstruction. We illustrate the experimental results by graphical presentation
(refer to Figure 2 and 3). We verify the ability of the tracking system with fulfill following requirement: i) Occlusion
- Able to identify the individual objects during occlusion and ii) Obstruction - Able to identify objects when
distracted by static objects or background.
Occlusion ± Figure 2 shows that the performance evaluation of the image sequences which focused in tracking 1
significant color object and more than 3 similar color objects. The occlusion happened at frame 485 and 495 where
the significant color object is labeled as 6. Frame 475 to frame 545 demonstrates the tracking system successfully
tracked the significant color object (in label 6) before and after occlusion happened (frame 485 and frame 495). The
experimental results also showed a promising result for the tracking of similar color objects from frame 515 to frame
545.
Obstruction ± Figure 3 illustrates performance evaluation of the image sequences with tree obstruction in the
scene. The images show the tracking results of the object with label 20 appeared and re-appeared with same label in
the scene after obstructed by the tree. We show the ground truth images of the object highlighted in red circle for
better comparison.
Figure 2 and 3 demonstrate the experimental results PETS dataset. The average time performance for the 2 dataset
is approximately 40 milliseconds.
5. Conclusion

S.L. Tang et al. / Procedia Computer Science 1 (2012) 2549–2557

Sze Ling T. et al./ Procedia Computer Science 00 (2010) 000±000

2555

From the experimental result, it is shown that the proposed tracking approach is suitable for real time
surveillance application due to its fast tracking process, and ability to track partially occluded objects with good
accuracy.
Fast object tracking is achieved by integrating blob tracking (bottom-up approach) and simplified particle
filtering tracking approach (top-down approach). Main tracking routine is controlled by the blob tracking which
provide fast tracking system with good accuracy for tracking multiple non occluded objects. And then based on
simple blob overlapping test, new object event and blob merging or occlusion event can be detected automatically,
subsequently invoking top down tracking approach which is more robust to partial occlusion in case of blob merging
or occlusion event. With the information obtained from blob WUDFNLQJLQLWLDOL]DWLRQRISDUWLFOHILOWHU¶VREMHFWWUDFNHU
can be done easily and automatically by extracting object information from previous frame just before the objects
merged. Besides that, particle hypotheses also can be effectively distributed within the area where the blobs merged
in current motion map.
Apart from the advantages, this proposed tracking approach also suffers from some limitations due to its
assumption that the occluded objects to be tracked by employing top down approach must be appeared as separate
objects before merged. If multiple objects enter the video scene as a merged group, then the proposed tracking
approach will assume that there is single object within the group and track the group as single object until the
objects splitted. Error will be produced if the group merged with other blob before splitted as single objects.
In future, we would like to investigate multi-cameras object tracking in order to overcome the limitations of
current object tracking system.
Acknowledgements
This work was supported by MIMOS Berhad, Malaysia.

Fig. 2. The experimental results of our proposed tracking method. Highlighted region indicates the object of interest, in which the
objects are in similar properties, yet our proposed method able to track the objects. The experiment shows promising results
especially from frame 515 to frame 545.

2556

S.L. Tang et al. / Procedia Computer Science 1 (2012) 2549–2557

Sze Ling T. et al. / Procedia Computer Science 00 (2010) 000±000

Fig. 3. The side by side graphical performance evaluation of the image sequences with tree obstruction.

References
1.
2.
3.
4.
5.
6.
7.
8.
9.
10.
11.
12.
13.
14.
15.
16.

Y. A. Wang and E. H. Adelson. Representing moving images with layers. IEEE Trans. Image Process., vol. 3, no. 5, pp. 625±638, Sep.
1994.
CR Wren, Pfinder: Real-time tracking of the human body,IEEE Trans. On Pattern Analysis and Machine Intelligence, 19, pp. 780-785,
1997.
I Haritaoglu, D Harwood, L S Davis, W 4 : Who? When? Where? What? A real time system for detecting and tracking people, Third
IEEE International Conference on Automatic Face and Gesture Recognition, pp. 222-227, 1998.
I Haritaoglu, D Harwood, L S Davis, W4: Real-time surveillance of people and their activities, IEEE Transactions on Pattern Analysis
and Machine Intelligence, pp. 809-830, 2000.
A. R. J. Francois. Real time multi-resolution blob tracking. IRIS Technical Report, 2004.
K. Hariharakrishnan, D. Schonfeld. Fast object tracking using adaptive block matching. IEEE Trans. On Multimedia 7(5), pp. 853859, 2005.
L. M. Fuentes and S. A. Velastin. People tracking in surveillance application, Image and Vision Computing, vol. 24, no. 11, pp. 11651171, 2006.
M. R. Sullivan and M. Shah. Detecting and Segmenting Humans in Crowded Scenes. In 0XOWLPHGLD ¶ SURFHGGLQJV RI WKH  th
international conference on Multimedia, pp. 353-356, 2007.
L. Zhao, L. S. Davis. Closely Coupled Object Detection and Segmentation. In Tenth IEEE International Conference on Computer
Vision (ICCV'05), Volume 1, pp.454-461, 2005.
Y. Wu and T. Yu. A Field Model for Human Detection and Tracking. IEEE Transactions on Pattern Analysis and Machine Intelligence,
CVPR, vol. 28, no. 5, pp. 753-765, 2006.
W. Hu, T. Tan, L. Wang, S. Maybank. A survey on visual surveillance of object motion and behaviors. IEEE Trans. On Systems, Man,
and Cybernetics, vol. 34, no.3, pp. 334-352, 2004.
A. Lipton, H. Fujiyoshi and R. Patil. Moving target classification and tracking from real-time video. In DARPA Image Understanding
Workshop, pp. 129-136, November 1998.
I. Haritaoglu, D. Harwood, L. S. Davis. W4: Real-Time Surveillance of People and Their Activities. IEEE Transactions on Pattern
Analysis and Machine Intelligence, 22(8), pp. 809-830, Aug. 2000.
D. Comaniciu, V Ramesh, P. Meer, Real time tracking of non rigid objects using mean shift, IEEE Conf. on Computer Vision and
Pattern Recognition, vol. 2, pp. 142-149, 2000.
M. Isard and A. Blake. Condensation ± conditional density propagation for visual tracking. ,QW¶O-RXUQDORI&RPSXWHU9LVLRQ, vol. 29,
no. 1, pp. 5±28, Aug. 1998
K. Nummiaro, K. M. Esther and V. C. Luc, An adaptive color based particle filter, Image and vision computing, vol. 21, no. 1, pp. 99110, 2003.

S.L. Tang et al. / Procedia Computer Science 1 (2012) 2549–2557

Sze Ling T. et al./ Procedia Computer Science 00 (2010) 000±000

2557

17. Object tracking system based on the integration of bottom-up and top-down approaches. Malaysia patent office. Pending for PI:
20093323.
18. PETS 2009 Denchmark Data http://www.cvg.rdg.ac.uk/WINTERPETS09/data.htm

