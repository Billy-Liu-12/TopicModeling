Procedia Computer
Science
Procedia Computer Science 00 (2010) 1–7
Procedia
Computer Science 1 (2012) 2417–2423

www.elsevier.com/locate/procedia

International Conference on Computational Science, ICCS 2010

L p-norm proximal support vector machine and its applications
Wen Jing Chen1 , Ying Jie Tian∗
Research Center on Fictitious Economy and Data Science, CAS, Beijing, China

Abstract
In this paper, we propose an eﬃcient l p -norm (0 < p < 1)Proximal Support Vector Machine by combining
proximal support vector machine (PSVM) and feature selection strategy together. Following two lower bounds for the
absolute value of nonzero entries in every local optimal solution of the model, we investigate the relationship between
sparsity of the solution and the choice of the regularization parameter and p-norm. After smoothing the problem in
l p -norm PSVM, we solved it by smoothing conjugate gradient method (SCG) method, and preliminary numerical
experiments show the eﬀectiveness of our model for identifying nonzero entries in numerical solutions and feature
selection, and we also apply it to a real-life credit dataset to prove its eﬃciency.

c 2012 Published by Elsevier Ltd. Open access under CC BY-NC-ND license.
⃝

Keywords: proximal support vector machine, feature selection, smoothing conjugate gradient, l p norm

1. Introduction
Support vector machine (SVM) [1, 2] is a promising tool in machine learning, but it is unable to get the importance feature. To identifying a subset of features which contribute most to classiﬁcation is also an important task in
classiﬁcation. The beneﬁt of feature selection is twofold. It leads to parsimonious models that are often preferred in
many scientiﬁc problems, and it is also crucial for achieving good classiﬁcation accuracy in the presence of redundant
features[3, 4, 16, 18]. We can combine SVM with various feature selection strategies, Some of them are ”ﬁlters”:
general feature selection methods independent of SVM. That is, these methods select important features ﬁrst and then
SVM is applied for classiﬁcation. On the other hand, some are wrapper-type methods: modiﬁcations of SVM which
choose important features as well as conduct training/testing. In the machine learning literature, there are several
proposals for feature selection to accomplish the goal of automatic feature selection in the SVM[4]-[9], in some of
which they applied the l0 -norm, l1 -norm or l∞ -norm SVM and got competitive performance.
Naturally, we expect that using the l p -norm (0 < p < 1) in SVM can ﬁnd more sparse solution than using l1 -norm.
[10] considered a minimization model where the objective function is the sum of a data ﬁtting term in l2 norm and a
regularization term in l p norm (0 < p < 1), and gave out several interesting theoretical results. Because the problem
they solved is formulated as
minn
x∈R

∗

Ax − b

2
2

+ λ x pp ,

Email address: tianyingjie1213@163.com,Corresponding author (Ying Jie Tian )
work has been partially supported by grants from National Natural Science Foundation of China(NO.10601064, NO.70621001)

1 This

c 2012 Published by Elsevier Ltd. Open access under CC BY-NC-ND license.
1877-0509 ⃝
doi:10.1016/j.procs.2010.04.272

(1)

2418

W.J. Chen, Y.J. Tian / Procedia Computer Science 1 (2012) 2417–2423
2

W.J. Chen and Y.J. Tian / Procedia Computer Science 00 (2010) 1–7

, where A ∈ Rm×n , b ∈ Rm , λ ≥ 0, 0 < p < 1, therefore in this paper we naturally want to combine proximal support
vector machine (PSVM) [12] and feature selection strategy by introducing the l p -norm (0 < p < 1). Because the
primal problem in PSVM can be directly transformed to formulation (1), so that we can apply the theoretical results in
[10] to PSVM and study its corresponding properties. This paper is organized as follows. In Section 2 we propose the
l p -norm proximal support vector machine for classiﬁcation, and based on two lower bounds in [10], we present the
corresponding lower bounds for the absolute value of nonzero entries in every local optimal solution of our new model.
In Section 3, we smooth the problem in l p -norm PSVM and introducing the conjugate gradient method (SCG) to solve
it. Section 4 investigates the performance of l p -norm PSVM, and compare it with other feature selection methods by
numerical experimental on UCI datasets, and also apply it to a real-life credit dataset to prove its eﬃciency. Finally,
we have discussion and conclusions in Section 4.
2. Lower bounds for nonzero entries in solutions of l p-norm Proximal SVM
For a classiﬁcation problem, the training set is given by
T = {(x1 , y1 ), ..., (xl , yl )} ∈ (Rn × {−1, 1})l ,

(2)

where xi = ([xi ]1 , · · · , [xi ]n )T ∈ Rn and yi ∈ {−1, 1}, i = 1, · · · , l. Proximal support vector machine aims to build a
decision function by solving the following primal problem:
min
w,b,η

s.t.

1
( w
2

2

+ b2 ) +

l

C
2

η2i ,

(3)

i=1

yi ((w · xi ) + b) = 1 − ηi , i = 1, · · · , l,

2

(4)
2

in which w is the l2 -norm of w. Figure 1 describe its geometric explanation in R , the planes (w · x) + b = ±1
around which points of the points ”◦” and points ”+” cluster and which are pushed apart by the optimization problem
(3)∼(4). PSVM leads to an extremely fast and simple algorithm for generating a linear or nonlinear classiﬁer that
merely requires the solution of a single system of linear equations, and has been eﬃciently applied to many ﬁelds.

ﬁgure1
In order to get more sparse solutions, we substitute the l2 -norm by l p -norm(0 < 1 < p) in problem (3)∼(4) and it
turns to be
min
w,b,η

s.t.

λ( w

p
p

l

+ |b| p ) +

η2i ,

yi ((w · xi ) + b) = 1 − ηi , i = 1, · · · , l,

in which
w

p
p

(5)

i=1

(6)

n

=
i=1

|wi | p .

(7)

W.J. Chen, Y.J. Tian / Procedia Computer Science 1 (2012) 2417–2423
W.J. Chen and Y.J. Tian / Procedia Computer Science 00 (2010) 1–7

2419
3

Obviously, problem (5)∼(6) is equivalent to the following unconstrained minimization problem
min Az − e

z∈Rn+1

2
2

+ λ z pp ,

(8)

where
(wT , b)T ∈ Rn+1 ,
⎛
⎞
⎜⎜⎜ y1 x1T , y1 ⎟⎟⎟
⎜⎜⎜
⎟⎟⎟
..
⎟⎟⎟ ∈ Rl×(n+1) ,
A = ⎜⎜⎜
.
⎜⎝
⎟⎠
T
yl xl , yl
z =

e =

(1, · · · , 1)T ∈ Rl .

(9)
(10)
(11)

we call problem (8) l p -norm PSVM problem. Paper [10] established two lower bounds for the absolute value of
nonzero entries in every local optimal solution of the general model (8), which can be used to eliminate zero entries
precisely in any numerical solution. Therefor, we apply them to describe the performance for feature selection of our
l p -norm PSVM for classiﬁcation.
Let Z∗p denote the set of local solutions of problem , then for any z∗ ∈ Z∗p , we have the corresponding versions of
theorem 2.1 and theorem 2.2 in [10] for model (8):
λp 1
Theorem 2.1(ﬁrst bound) Let L = ( ) 1−p , where β = A e , then for any z∗ ∈ Z∗p , we have
2β
z∗i ∈ (−L, L) ⇒ z∗i = 0, i = 1, · · · , n + 1.
Theorem 2.2(second bound) Let Li = (
(10), then for any z∗ ∈ Z∗p , we have

(12)

1
λp(1 − p) 2−p
) , i = 1, · · · , n + 1, where ai is the ith column of the matrix A
2 ai 2

z∗i ∈ (−Li , Li ) ⇒ z∗i = 0, i = 1, · · · , n + 1.

(13)

Just as pointed out by [10], the above two theorems clearly shows the relationship between the sparsity of the
solution and the choice of the regularization parameter λ and norm · p . The lower bounds is not only useful for
identiﬁcation of zero entries in all local optimal solutions from approximation ones, but also for selection of the
regularization parameter λ and norm p.
3. Smoothing l p-norm PSVM problem
However, solving the nonconvex, non-Lipschitz continuous minimization problem (8) is very dicult. Most optimization algorithms are only eﬃcient for smooth and convex problems, and they can only ﬁnd local optimal solutions.
In [10], they ﬁrst smooth problem (8) by choosing appropriate smoothing function and then apply the smoothing conjugate gradient method (SCG)[11] to solve it, which guarantees that any accumulation point of a sequence generated
by this method is a Clarke stationary point of the nonsmooth and nonconvex optimization problem (8). Here we
introduce the smoothing function and then smooth problem (8). Let sμ (·) be a smoothing function of |t|, which takes
the formulations as
sμ (t) =

|t|,
t2
μ

+

μ
4,

min Az − e

2
2

+λ

if |t| > μ2 ,
if |t| ≤ μ2 ,

(14)

so the smoothed problem (8) is
n+1
z∈Rn+1

(sμ (zi )) p ,

(15)

i=1

Therefore, based on solving (15), and determining the nonzero elements of solutions by two bounds, we can establish
the l p -norm PSVM algorithm for both feature selection and classiﬁcation problem:
Algorithm 1. (l p -PSVM)

2420

W.J. Chen, Y.J. Tian / Procedia Computer Science 1 (2012) 2417–2423
4

W.J. Chen and Y.J. Tian / Procedia Computer Science 00 (2010) 1–7

(1)
(2)
(3)
(4)
(5)
(6)

Given a training set T = {(x1 , y1 ), ..., (xl , yl )} ∈ (Rn × {−1, 1})l ;
Select appropriate parameters λ and p;
Solve problem (15) using SCG method and get the solution z∗ = (w∗T , b∗ )T ;
Set the variables w∗i to zero if it satisﬁes either of the two bounds, get the sparse solution w∗ ;
Select the features corresponding to nonzero elements of w∗ ;
Construct the decision function as
f (x) = sgn((w∗ · x) + b∗ ).

(16)

4. Numerical experiments and application
In this section, based on several UCI datasets, we ﬁrst apply model (8) to investigate the performance of feature
selection by the choice of λ and norm · p . And because [3][9] has compared the l1 -norm SVM, l2 -norm SVM and
l∞ -norm, we then only conduct the issue of numerically comparing svm with l1 -norm with our l p -method. Before
doing experiments, data sets are scaled with each feature to [0, 1]. At last we apply algorithm 1 to a real-life credit
dataset. The computational results are conducted on a Dell PC (1.80 GHz, 1.80 GHz, 512MB of RAM) with using
Matlab 7.4.
4.1. Relationship between the sparsity of solutions with λ and p
For every dataset, we choose parameter λ ∈ {20 , 21 , · · · , 27 } and p ∈ [0.1, 0.9] with step 0.1. The following tables
describe the sparsity of solutions of problem (8) under corresponding (λ, p), and ( 1 , 2 ) in each table means the
number of zero variables in solutions w∗ determined by bound (12) and bound (13) separately. Each Bold number
denotes the maximum number for a given λ and varying p.
Table 1: Experiments On Heart Disease Dataset (l=270, n=13).

0.1

0.2

0.3

0.4

0.5

0.6

0.7

0.8

0.9

20

(0, 0)

(0, 0)

(0, 1)

(0, 0)

(0, 0)

(0, 0)

(0, 0)

(0, 0)

(0, 0)

21

(0, 0)

(0, 1)

(0, 0)

(0, 0)

(0, 1)

(0, 1)

(0, 0)

(0, 0)

(0, 0)

22

(0, 0)

(0, 1)

(0, 1)

(0, 1)

(0, 1)

(0, 1)

(0, 1)

(0, 0)

(0, 0)

23

(0, 1)

(0, 1)

(0, 1)

(0, 1)

(0, 1)

(0, 1)

(0, 1)

(0, 1)

(0, 1)

24

(0, 1)

(0, 1)

(0, 1)

(0, 1)

(0, 1)

(0, 1)

(0, 1)

(0, 1)

(0, 1)

5

(0, 2)

(0, 3)

(0, 3)

(0, 3)

(0, 3)

(0, 3)

(0, 3)

(0, 2)

(0, 1)

26

(0, 7)

(0, 7)

(0, 8)

(0, 8)

(0, 8)

(0, 6)

(0, 4)

(0, 4)

(0, 3)

27

(0, 9)

(0, 10)

(0, 11)

(0, 11)

(0, 11)

(0, 11)

(0, 8)

(0, 7)

(0, 5)

λ\ p

2

From all the four tables we can see that: bound (13) gives out more sparsity than bound (12); for bound (13), the
sparsity value takes its maximum mainly at p ∈ (0.2, 0.6) for any given λ, which also can be roughly estimated by
p∗ (λ) = arg max (λp(1 − p))1/(2−p)
0<p<1

(17)

for λ ∈ (0, 27 ) if we scale ai such that ai = 1; the number of nonzero entries in any local minimizer of (8) reduces
when λ becomes larger.

2421

W.J. Chen, Y.J. Tian / Procedia Computer Science 1 (2012) 2417–2423

5

W.J. Chen and Y.J. Tian / Procedia Computer Science 00 (2010) 1–7
Table 2: Experiments On German Credit Dataset (l=1000, n=24)

0.1

0.2

0.3

0.4

0.5

0.6

0.7

0.8

0.9

20

(0, 1)

(0, 2)

(0, 1)

(0, 1)

(0, 2)

(0, 1)

(0, 1)

(0, 1)

(0, 1)

21

(0, 2)

(0, 2)

(0, 2)

(0, 1)

(0, 1)

(0, 1)

(0, 1)

(0, 0)

(0, 0)

22

(0, 2)

(0, 3)

(0, 2)

(0, 2)

(0, 1)

(0, 2)

(0, 1)

(0, 1)

(0, 1)

3

(0, 3)

(0, 5)

(0, 4)

(0, 2)

(0, 2)

(0, 2)

(0, 1)

(0, 1)

(0, 0)

4

2

(0, 5)

(0, 5)

(0, 5)

(0, 5)

(0, 4)

(0, 3)

(0, 2)

(0, 1)

(0, 1)

25

(0, 7)

(0, 7)

(0, 6)

(0, 7)

(0, 7)

(0, 6)

(0, 4)

(0, 1)

(0, 1)

λ\ p

2

6

2

(1, 7)

(0, 10)

(0, 10)

(0, 9)

(0, 9)

(0, 7)

(0, 7)

(0, 3)

(0, 1)

27

(1, 12)

(1, 13)

(0, 13)

(0, 13)

(0, 12)

(0, 10)

(0, 9)

(0, 6)

(0, 2)

Table 3: Experiments On Australian credit Dataset (l=690, n=14)

0.1

0.2

0.3

0.4

0.5

0.6

0.7

0

2

(0, 1)

(0, 1)

(0, 1)

(0, 1)

(0, 1)

(0, 1)

21

(0, 1)

(0, 1)

(0, 1)

(0, 1)

(0, 1)

(0, 0)

2

2

(0, 1)

(0, 1)

(0, 2)

(0, 1)

(0, 1)

23

(0, 1)

(0, 1)

(0, 2)

(0, 1)

(0, 2)

4

2

(0, 3)

(0, 4)

(0, 5))

(0, 4)

(0, 2)

(0, 1)

(0, 1)

(0, 1)

(0, 1)

25

(1, 5)

(0, 6)

(1, 6)

(0, 6)

(0, 6)

(0, 5)

(0, 2)

(0, 2)

(0, 1)

26

(0, 6)

(0, 6)

(0, 6)

(0, 6)

(0, 6)

(0, 6)

(0, 6)

(0, 3)

(0, 2)

27

(0, 7)

(1, 9)

(1, 9)

(0, 10)

(0, 10)

(0, 8)

(0, 7)

(0, 6)

(0, 3)

λ\ p

0.8

0.9

(0, 0)

(0, 1)

(0, 0)

(0, 0)

(0, 0)

(0, 0)

(0, 1)

(0, 1)

(0, 1)

(0, 0)

(0, 1)

(0, 1)

(0, 1)

(0, 0)

Table 4: Experiments On Sonar Dataset (l=208, n=60)

λ\ p
20

0.1

0.2

0.3

0.4

0.5

0.6

0.7

0.8

0.9

(0, 1)

(0, 5)

(0, 2)

(0, 1)

(0, 3)

(0, 1)

(0, 0)

(0, 0)

(0, 0)

21

(0, 2)

(0, 4)

(0, 3)

(0, 2)

(0, 2)

(0, 2)

(0, 0)

(0, 4)

(0, 0)

22

(0, 11)

(0, 9)

(0, 7)

(0, 7)

(0, 7)

(0, 3)

(0, 3)

(0, 0)

(0, 1)

23

(0, 12)

(0, 17)

(0, 13)

(0, 13)

(0, 11)

(0, 8)

(0, 4)

(0, 2)

(0, 1)

24

(0, 19)

(0, 23)

(0, 26)

(0, 23)

(0, 22)

(0, 19)

(0, 14)

(0, 8)

(0, 1)

5

2

(0, 29)

(0, 42)

(0, 44)

(1, 44)

(0, 44)

(0, 40)

(0, 32)

(0, 16)

(0, 5)

26

(1, 46)

(1, 58)

(0, 60)

(1, 60)

(0, 60)

(0, 59)

(0, 55)

(0, 44)

(0, 22)

27

(1, 60)

(2, 60)

(1, 60)

(1, 60)

(0, 60)

(0, 60)

(1, 60)

(0, 60)

(0, 52)

4.2. Comparison with l1 -PSVM
We compare l p -PSVM with algorithm l1 -PSVM in this part, and if p = 1 the problem (8) turns to be a convex
problem
minz∈Rn+1 Az − e 22 + λ z 1 .
(18)

2422

W.J. Chen, Y.J. Tian / Procedia Computer Science 1 (2012) 2417–2423
W.J. Chen and Y.J. Tian / Procedia Computer Science 00 (2010) 1–7

6

¯ p)
For every dataset, we use 5-fold cross-validation error to choose the appropriate parameters (λ,
¯ for l p -PSVM and λ˜
for algorhtm l1 -PSVM, the following table gives out the numerical results, where ¯ mean the number of zero variables
in w¯ of algorithm l p -PSVM determined by bound (13), ˜ means the number of zero variables in w˜ of algorithm
l1 -PSVM.
Table 5: Numerical results

dataset \ algorithm
heart

Australian
S onar
German

79.63%(λ¯ = 26 , p¯ = 0.3, ¯ = 8)
85.8%(λ¯ = 25 , p¯ = 0.3, ¯ = 6)

l p -PSVM

l1 -PSVM
79.63%(λ˜ = 1, ˜ = 3)
85.94%(λ˜ = 128, ˜ = 2)

77.51%(λ¯ = 26 , p¯ = 0.2, ¯ = 58)
75.7%(λ¯ = 27 , p¯ = 0.3, ¯ = 13)

75.62%(λ˜ = 16, ˜ = 36)
76.1%(λ˜ = 4, ˜ = 13)

From Table 5 we ﬁnd that l p -PSVM successes in ﬁnding more sparse solution with higher accuracy than or almost
the same with l1 -PSVM.
4.3. Credit Card Dataset
Now we test the performance of l p -PSVM on credit card dataset. The 6000 credit card records used in this
paper were selected from 25,000 real-life credit card records of a major US bank. Each record has 113 columns or
variables to describe the cardholders behaviors, including balance, purchases, payment cash advance and so on. With
the accumulated experience functions, we eventually get 65 variables from the original 113 variables to describe the
cardholders’ behaviors[15, 17, 19].
In this paper we chose the holdout method on credit card dataset to separate data into training set and testing set:
ﬁrst, the bankruptcy dataset (960 records) is divided into 10 intervals (each interval has approximately 100 records).
Within each interval, 25 records are randomly selected. Thus the total of 250 bankruptcy records is obtained after
repeating 10 times. Then, as the same way, we get 250 current records from the current dataset. Finally, the total of
250 bankruptcy records and 250 current records are combined to form a single training dataset, with the remaining
710 lost records and 4790 current records merge into a testing dataset. This process is performed for ﬁve times, and
for each time we apply l p -PSVM to training and test, and in each training, we apply 5-fold cross-validation to choose
appropriate parameters in l p -PSVM for testing. Here, we apply three scores to evaluate two algorithms: sensitivity
(S n), speciﬁcity(S p) and G-Mean(g)on
Sn =
Sp =
g =

TP
,
T P + FN
TN
,
T N + FP
S n × S p,

(19)
(20)
(21)

where T P is true positive, T N is true negative, FP is false positive and FN is false negative. At last we recorded
the corresponding average scores for each time in Table 6, where p∗ , λ∗ are the optimal parameters corresponding
to higher average g of 5-fold cross-validation; ¯ means the number of zero variables in w¯ of algorithm l p -PSVM
determined by bound (13). We can see that l p -PSVM actually ﬁnds sparse features and gets high performance scores.
In this experiment, we also prove that: for a given p, with the increase of λ, solution turns to be more sparse;
for a given λ, more sparse solution appears in the interval p ∈ [0.2, 0.6], which give us practical guide in choosing
appropriate parameters.
5. Conclusions
We proposed an eﬃcient model which combines proximal support vector machine (PSVM) and feature selection
strategy by introducing the l p -norm (0 < p < 1) in its primal problem. Based on the theoretical results of [10], two

2423

W.J. Chen, Y.J. Tian / Procedia Computer Science 1 (2012) 2417–2423

7

W.J. Chen and Y.J. Tian / Procedia Computer Science 00 (2010) 1–7
Table 6: Experiments on Credit Card Dataset

Training Set
DS 1
DS 2
DS 3
DS 4
DS 5

Sp
74.26%
67.85%
71.88%
71.07%
70.66%

Sn
83.57%
83.16%
80.73%
82.33%
80.32%

g
78.22%
74.74%
75.69%
76.33%
74.85%

Testing Set
∗

p
0.3
0.4
0.2
0.3
0.1

∗

λ
26
26
26
26
27

¯
56
55
56
58
56

Sn
74.15%
68.97%
73.15%
73.19%
73.92%

Sp
78.98%
85.9%
83.07%
80.68%
80.39%

g
76.53%
76.97%
77.95%
76.84%
77.09%

lower bounds for the absolute value of nonzero entries in every local optimal solution of l p -PSVM is also developed.
After smoothing the problem in l p -norm PSVM and solving it by smoothing conjugate gradient method (SCG) method,
preliminary numerical experiments show the eﬀectiveness of algorithm l p -PSVM. Further development of SVMs with
l p norm may be in two ways: Introducing kernel functions to solve nonlinear classiﬁcation problem by l p -PSVM;
Develop other lower bounds for the absolute value of nonzero entries in every local optimal solution of l p - norm
standard SVMs, such as C-SVM, ν-SVM and etc.
Acknowledgments
This work has been partially supported by grants from National Natural Science Foundation of China(NO.10601064,
NO.70621001).
References
[1] B. Boser, I. Guyon and V. Vapnik, A training algorithm for optimal margin classiﬁers, Computational Learing Theory, 144-152, 1992.
[2] V. Vapnik, Statistical Learning Theory. New York, NY:Wiley, 1998.
[3] J. Friedman and T. Hastie(eds.), Discussion of ”Consistency in boosting” by W. Jiang, G. Lugosi, N. Vayatis and T. Zhang, Ann. Statist.
32,102-107, 2004.
[4] J. Zhu, S. Rosset, T. Hastie and R. Tibshirani , 1-norm Advances in Neural Information Processing Systems 16, 2004.
[5] I. Guyon , J. Weston , S. Barnhill and V. Vapnik, Gene selection for cancer classiﬁcation using support vector machines, Machine Learning
46, 389-422, 2002.
[6] J. Weston, S. Mukherjee and V. Vapnik(eds.), Feature selection for svms. Advances in Neural Information Processing Systems 13, 2001.
[7] P. Bradley and O. Mangasarian, Feature selection via concave minimization and support vector machines, International Conference on Machine Learning, Morgan Kaufmann, 1998.
[8] M. Song, C. Breneman, J. Bi , N. Sukumar, K. Bennett, S. Cramer and Tugcu N(2002). Prediction of protein retention times in anion-exchange
chromatography systems using support vector regression. J. Chemical Information and Computer Sciences.
[9] H. Zou and M. Yuan, The f∞ norm support vector machine. Statistica Sinica, 18, 379-398, 2008.
[10] Chen X, Xu F and Ye Y. Lower Bound Theory of Nonzero Entries in Solutions of l2 -l p Minimization. Technical report, Department of Applied
Mathematics, The Hong Kong Polytechnic University.
[11] X. Chen and W. Zhou, Smoothing nonlinear conjugate gradient method for image restoration using nonsmooth nonconvex minimization,
Preprint, Department of Ap- plied Mathematics, The Hong Kong Polytechnic University, 2008.
[12] G. Fung and O. Mangasarian, Proximal support vector machine classiﬁers, Proceedings of International Conference of Knowledge Discovery
and Data Mining, 77-86, 2001.
[13] Y. Chen and C. Lin, Combining SVMs with various feature selection strategies. Feature Extraction, Foundations and Applications. New York,
Springer, 319-328, 2005.
[14] V. Vapnik , O. Chapelle, Bounds on error expectation for SVM. In Advances in Large-Margin Classiﬁers (Neural Information Processing),
chapter 14, MIT press, 261-280, 2000.
[15] Y. Shi, Y. Peng, W. X. Xu and X. W. Tang, Data mining via multiple criteria linear programming: applications in credit card portfolio
management. International Journal of Information Technology and Decision Making, 131-151, 2002
[16] L.G. Zhou, K.K.Lai and J. Yen, credit scoring models with AUC maxinization based on weighted SVM, International Journal of Information
Technology and Decision Making, 677-696, 2009.
[17] J. He, X.T. Liu and Y. Shi(eds.), Classiﬁcations of cr edit cardholder behavior by using fuzzy linear programming, International Journal of
Information Technology and Decision Making, 633-650, 2004.
[18] L. Yu, S. Y. Wang and J. Cao, A modiﬁed least squares support vector machine classiﬁer with application, International Journal of Information
Technology and Decision Making, 697-710, 2009.
[19] Y. Shi, Current research trend: information technology and decision making in 2008, International Journal of Information Technology and
Decision Making, 1-5, 2009.

