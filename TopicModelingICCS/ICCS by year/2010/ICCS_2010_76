Procedia Computer Science 1 (2012) 1093–1100
Procedia Computer Science 00 (2010) 1–8

Procedia Computer
Science
www.elsevier.com/locate/procedia

International Conference on Computational Science, ICCS 2010

3D ﬁnite element numerical integration on GPUs
Paweł Macioła , Przemysław Płaszewskib , Krzysztof Bana´sb,a
a Institute of Computer Modelling,
Cracow University of Technology, Warszawska 24, 31-155 Krak´ow, Poland
b Department of Applied Computer Science and Modelling,
AGH University of Science and Technology, Mickiewicza 30, 30-059 Krak´ow, Poland

Abstract
The algorithmic and computational aspects of 3D ﬁnite element numerical integration on GPUs are investigated in
the paper. The special stress is put on selecting the proper parallelization strategies depending upon the properties of
FEM problems solved and approximations used. The close interplay between the available computational resources
of GPUs and the possible implementation strategies and obtained results is observed.
c 2012 Published by Elsevier Ltd. Open access under CC BY-NC-ND license.
⃝
Keywords: GPGPU, numerical integration, ﬁnite elements, parallel programming, higher order approximation

1. Finite element numerical integration
Numerical integration is the process that leads from a weak, ﬁnite element statement of the problem solved to
the system of linear equations, solution of which produces a ﬁnite element approximation. The standard procedure
consist of a loop over ﬁnite elements into which the computational domain is discretized. For each element a, so
called, element stiﬀness matrix is produced and than assembled into the global matrix – the matrix of a solved system
of linear equations, called also the global stiﬀness matrix.
The level of complexity of creation of the element stiﬀness matrix depends on the problem solved and the approximation used. In theory, each entry of the element stiﬀness matrix is obtained by evaluation of integrals comprising the
weak, ﬁnite element statement of the problem. In practice, for many simple elements and approximations, integrals
can be precomputed analytically and creation of stiﬀness matrix entries changes into algebraic operations, with closed
formulae into which element parameters, like e.g. vertices coordinates, are substituted. In such cases creation of the
global stiﬀness matrix consist mainly of assembling of quickly calculated element stiﬀness matrices [1].
There are, however, situations, e.g. for nonlinear problems (with PDE coeﬃcients depending on the current
solution) or for complex elements with curvilinear shapes, where integrals cannot be precomputed. In these cases
numerical integration has to be performed and can form a substantial, in terms of required resources, part of the
ﬁnite element solution process, especially for higher order approximation. The current article investigates such cases,
focusing on integration procedure, without considering the further assembly step.
Email address: kbanas@pk.edu.pl (Krzysztof Bana´s)

c 2012 Published by Elsevier Ltd. Open access under CC BY-NC-ND license.
1877-0509 ⃝
doi:10.1016/j.procs.2010.04.121

1094

P. Macioł et al. / Procedia Computer Science 1 (2012) 1093–1100

2

/ Procedia Computer Science 00 (2010) 1–8

Number of:
shape functions
Ae entries
Gaussian points

1
6
36
6

Degree of approximation p
3
4
5
6
40
75
126
196
1600 5625 15876 38416
48
80
150
231

2
18
324
18

7
288
82944
336

Table 1: Parameters determining the computational characteristics of ﬁnite element numerical integration – the number of shape functions, the
number of element stiﬀness matrix entries and the number of Gaussian integration points – as functions of the degree of approximating polynomials

2. The integration algorithms
As a model problem we consider simple Laplace’s equation in a 3D domain. In computing element stiﬀness matrix
entries, the most important, from the performance point of view, are calculations of the domain integral:

Ωe

∂φˆ r ∂φˆ s
dΩ
∂xi ∂xi

(1)

where Ωe is the domain of an element, φˆ r are element shape functions (forming a basis for approximation) and the
summation convention for repeated indices is used. Each integral (1) corresponds to a pair of shape functions φˆ r and
φˆ s , r, s = 1, .., Nsh , and contributes to a single entry Aer,s of the element stiﬀness matrix Ae . Hence in each element we
compute many integrals, each of which corresponds to a single entry in Aer,s , i.e. a pair of shape functions.
The number of entries calculated in each element depends on the number of shape functions, Nsh . The number of
shape functions depend on the order of approximation. For higher orders of approximation we use rich ﬁnite element
spaces with many shape functions (some of them being polynomials of higher degree). For low order approximations
(e.g. linear) we use basic ﬁnite element spaces, most often with linear or multi-linear shape functions. For our model
problem we use prismatic elements and shape functions being products of 2D shape functions for triangles and 1D
shape functions in vertical direction. Table 1 gives the number of shape functions for this combination, as a function
of the degree of approximating polynomial, p.
Numerical integration in ﬁnite element codes is usually done in two steps. First, the change of the domain of
integration, from a real element (with x coordinates) to a reference element (with ξ coordinates) is performed. Then
standard numerical integration is applied. There are only several reference elements used in ﬁnite element codes
and for each of them there are prescribed quadratures allowing for suﬃciently accurate integration of ﬁnite elements
integrals. The most common are Gauss-Legendre quadratures that we also employ in our investigations.
Finite element Gaussian integration transforms integral (1) into a sum over integration points ξ I with weights wI ,
I = 1, .., NI :
NI
∂φˆ r ∂ξ ∂φˆ s ∂ξ
(2)
det J T e wI
∂ξ ∂x
∂ξ ∂x
I=1

i

i

All values in (2) are computed at integration points ξ I and J T e denotes the Jacobian matrix of transformation T e from
ˆ e to the real element Ωe . ∂ξ forms a column of the Jacobian matrix J −1 , J −1 = ∂ξ , that
the reference element Ω
∂xi
∂x
T
T
e

e

corresponds to the inverse transformation T −1
e .
The number of Gaussian points, NI , is determined by the requirements of convergence of the ﬁnite element solution
[2]. It also depends on the order of approximation and usually (with some departures for special kinds of problems) is
chosen so as to integrate exactly ﬁnite element integrals in a reference element. Table 1 shows the number of Gaussian
points used in our study.
3. Computational aspects of numerical integration
2
integrals (entries) have to be computed, each as a sum of contributions
To create an element stiﬀness matrix, Nsh
from diﬀerent Gaussian points. Hence, in the numerical integration algorithm there are three loops: two over shape
functions and one over integration points.

1095

P. Macioł et al. / Procedia Computer Science 1 (2012) 1093–1100

3

/ Procedia Computer Science 00 (2010) 1–8

The size of:
(in kB)
ˆ
φˆ i , ∂∂φxi
e
A entries
ξ I , wI
ˆ
φˆ i , ∂∂φxi at all ξ I

1
0.09
0.14
0.09
0.56

2
0.28
1.27
0.28
5.06

Degree of approximation p
3
4
5
6
0.63
1.17
1.97
3.06
6.25 21.97
62.02 150.06
0.75
1.25
2.34
3.61
30.00 93.75 295.31 707.44

7
4.50
324.00
5.25
1512.00

Table 2: The size (in kilobytes) of data used in numerical integration algorithm: values of shape functions and their derivatives, element stiﬀness
matrix entries, Gaussian integration points and weights

It is possible to perform integration naively: for each pair of shape functions compute the corresponding integral
in a loop over integration points. At each point, values of shape functions are calculated and used in summations. For
the next pair of shape functions the procedure repeats. This however neglects the fact that computing the values of
diﬀerent shape functions at a speciﬁed point is strongly related. Instead of repeating calculations of values for each
particular pair of shape functions the procedure is reversed.
The loop over integration points is the outer loop. At each integration point the values of all shape functions are
calculated and contributions from the given integration point to all element stiﬀness matrix entries are added in a
double loop over shape functions.
To design parallelization strategies for diﬀerent hardware and software environments it is necessary to realize what
are the resources required for performing necessary calculations.
There are data that need to be retrieved from ﬁnite element data structures. These include geometrical data for
elements (usually in the form of vertices coordinates but for curvilinear elements more data may be necessary) and
coeﬃcients for the solved PDEs (or some parameters needed for evaluating coeﬃcients). In the case of our model
problem we assume that only coordinates of six element vertices are passed as arguments to the integration procedure.
The next important data are the coordinates of integration points and weights associated with them. For each
reference element of a given type and given degree of approximation these data are the same. Hence it is advantageous
to put these data in some fast memory, possibly read only, available to all working threads. Moreover, the loop over
elements, the outermost loop of the global stiﬀness matrix creation, should be organized in such a way, that elements
are grouped into sets of elements with the same type and degree of approximation p.
Hence in our study we investigate the case in which integration is done for a sequence of elements of the same
type and degree of approximation. Performance data are obtained for such a case and does not take into account the
situation in which elements of diﬀerent type or approximation order are sent to CPU or GPU for processing.
When looking for the part of memory where integration data can be placed it is important to realize its required
size. In terms of the number of scalars it can be quickly calculated using Table 1. In terms of kilobytes it is given in
Table 2 (because we consider older types of GPUs we assume ﬂoating point data with single precision only).
The last group of data used in ﬁnal calculations of approximate integrals are the values of shape functions and
their derivatives. Once again these values are the same for all elements of a given type and degree of approximation.
Hence they can be precomputed and stored in some quickly accessible memory. However, in such a case the memory
ˆ
requirements grow substantially. Table 2 presents in its ﬁrst line only the size of φˆ i and ∂∂φxi for a single integration
point. The last row of Table 2 shows the required size of data structures storing the values of shape functions and their
derivatives for all integration points.
In a competitive strategy of computing the values of shape functions separately for each element, the values of φˆ i
ˆ

and ∂∂φxi are computed within the loop over integration points. At each point all values are computed but only at this
point. The eﬃciency of the procedure depends on the kind of shape functions (whether they are standard Lagrangian,
hierarchical, obtained as tensor products, etc.).
For lower orders of approximation the option of precomputing the values of shape functions and their derivatives
may be attractive. The necessary storage remains small. When deciding which strategy to choose one has to take
into account the characteristics of computing platforms – the time to perform necessary calculations and the time to
retrieve precomputed values. The larger the size of data structures, the higher probability that they will not ﬁt into a
fast memory. For really high orders of approximations the required memory resources may not even be available.

1096

P. Macioł et al. / Procedia Computer Science 1 (2012) 1093–1100

4

/ Procedia Computer Science 00 (2010) 1–8

The number of
operations ×10−3
outside loops
inside loops

1
0.5
0.2

2
0.8
2.2

3
1.5
11.2

Degree of approximation p
4
5
6
7
2.6
4.2
6.3
9.2
39.3 111.1 268.9 580.6

8
12.8
1148.1

9
17.3
2117.5

Table 3: The number of operations (in thousands) performed at each integration point for the model problem, split into operations performed
outside loops over shape functions and inside loops

Because of greater ﬂexibility of this strategy and the known fact of limited resources available for GPU computations, we consider in our investigations only the case of computing shape functions separately for each element. Then,
the required memory resources are moderate and do not vary signiﬁcantly with p.
There is a diﬀerent situation with the resulting element stiﬀness matrix. Its size changes (see Table 2) from less
than one kilobyte to several hundreds kilobytes. For execution on large CPU cores with large caches this does not
pose substantial problems, for other hardware architecture the chosen strategy for parallelization may be inﬂuenced
by the requirements of properly storing the element stiﬀness matrix.
Another factor that has to be taken into account when designing parallelization strategies for numerical integration
is the number of operations performed within subsequent loops of the algorithm. The general orders of magnitude
results from the number of integration points and shape functions, however exact quantities depend on the problem
solved, approximation used and the geometry of ﬁnite elements.
Table 3 presents these numbers for our model problem, split into two groups (only ﬂoating point operations are
counted): operations performed outside loops over shape functions and operations performed within these loops.
The values in the table correspond to a single integration point and both groups are calculated assuming that shape
functions are computed separately for each element and each integration point. In this particular case, the number of
operations for a single pair of shape functions at a given integration point is equal to 7 (3 multiplications of derivatives
with respect to three space dimensions, 3 summations and 1 additional multiplication by det J T e · wI – precomputed
earlier). For other types of problems (especially nonlinear) this number may be much higher. However, usually this
would mean that the number of operations per memory access will be greater and obtained parallel speed-ups should
also increase.
The reason for the splitting presented in Table 3 is the fact that only operations within loops over shape functions
are easily parallelizable. There are no dependencies between calculations for two separate entries of the element stiﬀness matrix. To the contrary, in calculations outside loops over shape functions there are many strong dependencies
making parallelization diﬃcult or even impossible. Both, computing the values of shape functions and their derivatives, as well as computing parameters depending upon the geometry of an element (Jacobian matrices, determinant,
derivatives with respect to physical coordinates), especially for the most popular geometrically linear and multilinear
elements, have small potential for parallelization.
4. Mapping of numerical integration algorithm to GPU architecture
The most important characteristics of GPU architectures are: massive parallelism and rich memory hierarchy with
complex optimal access patterns. The potential for concurrency in ﬁnite element numerical integration is related
mainly to two factors: the number of elements for which integration is performed and the number of entries for a
single element stiﬀness matrix.
For standard CPU cores the ﬁrst option is easier to exploit. Following the standard approach of domain decomposition the loop over all elements can be split and executed on as many cores as there are elements. One thread
calculates stiﬀness matrices for several subsequent elements, making use of large caches to improve performance.
Calculations for diﬀerent elements are practically independent (diﬀerent elements can use the same geometrical data
for common parts but this will not have any impact on performance). The only dependencies appear in the assembly
phase and they have to be properly resolved. Hence, ﬁnite element numerical integration is perfectly parallel for CPU
cores – the time will decrease proportionally to the number of cores and the speed-up should approach the perfect
line.

1097

P. Macioł et al. / Procedia Computer Science 1 (2012) 1093–1100

/ Procedia Computer Science 00 (2010) 1–8

5

For GPU architectures there appear a problem of small resources available for a single thread. For a single element
we should put the data structures from the ﬁrst three lines and a particular column in Table 2 in fast memory available
to the thread. Assuming, for example, that we want to use concurrently 32 threads (minimal number for CUDA
programming model), we have to provide a fast memory with the size 32 times larger than the values in the table.
While for p = 1 this means only approximately 10 kBytes, for p greater than 4 it reaches more than 1 MByte.
Therefore we consider another approach where the decomposition concerns the resulting element stiﬀness matrix.
All entries are divided among threads and each thread computes the values of several entries. When adopting this parallelization strategy the eﬃciency would result from the proper placement of data structures in the available memories
and the proper organization of calculations, that would ensure optimal access patterns to data structures.
We consider the implementation of numerical integration algorithm for CUDA execution environment [3]. From
the point of view of organization of threads the environment is characterized by:
• grouping of threads into warps, sets of 32 threads simultaneously executed by hardware
• grouping of warps into threadblocks, sets of threads accessing the same shared memory and synchronized by
fast operations
• grouping of threadblocks into a grid, a set of threads executing the same kernel
The available memory consist of:
• limited number of registers allocated by the compiler
• 16 kB of fast (shared) memory
• so called constant memory, ﬁlled by the code running on the host CPU and quickly accessible thanks to caching
• large device memory, relatively slow – both when used for transfers from and to the main memory of the host
computer and when accessed by threads
Because of the reasons described previously the main assumption of the implementation is that a single ﬁnite
element corresponds to a single threadblock and that individual threads calculate sets of element stiﬀness matrix
entries. The whole grid of threadblocks correspond to the whole ﬁnite element mesh (or possibly a submesh for large
problems or parallel execution in distributed environments).
The size of threadblocks is one of the key characteristics of the parallelization strategy. It is always a multiple
of 32. From all the threads in the threadblock, only those for which there are entries in the stiﬀness matrix to be
computed are active. The rest remains inactive – do not perform calculations.
The number of computed element stiﬀness matrix entries cannot be smaller than the number of active threads in
a threadblock, however may be larger. In such a case there are several entries per thread. For large element stiﬀness
matrices one can select diﬀerent numbers of active threads (diﬀerent sizes of threadblocks) with a diﬀerent number of
element stiﬀness matrix entries per thread.
The actual decision on the size of threadblocks depends on the amount of required memory resources. We try to
optimize memory accesses by threads and, on the other hand, try to minimize the amount of memory used. On one
hand we want the number of threads in a threadblock to be large (as is recommended for CUDA architecture, since it
helps to eﬃciently schedule threads), but on the other hand we want to allow for more than one threadblock to perform
computations on each multiprocessor (since this also increase the eﬃciency of computations).
To allow for optimal access by the threads, the data structures, when necessary, are padded, making their size a
multiple of 32, according to the requirements of CUDA architecture. This concerns the input array with element data,
as well as the output array with stiﬀness matrix entries.
The resulting element stiﬀness matrix for orders of approximation greater than 3 is too big to ﬁt into the fast
shared memory. For these cases computations are split, the array is divided into horizontal stripes of several rows, and
subsequent stripes are calculated one after another, with threads always operating on shared memory. The penalty for
this splitting is that for each stripe of the element stiﬀness matrix the values of shape functions and their derivatives
are calculated again.
The algorithm proceeds in the following steps:

1098

P. Macioł et al. / Procedia Computer Science 1 (2012) 1093–1100

/ Procedia Computer Science 00 (2010) 1–8

6

• element data (vertex coordinates, PDE coeﬃcients, etc.) for all elements related to a given grid of threadblocks
are placed in special, padded arrays and copied from the host computer memory to the CUDA device memory
in one call
• Gaussian integration data are copied to constant memory
• a proper integration kernel function is invoked to be executed by concurrent threads
• output data (in the form of element stiﬀness matrices) are copied from the device memory to the host memory
A single kernel function performs integration for a set of element stiﬀness matrix entries in the following order:
• based on its ID the thread realizes which element and which stiﬀness matrix entries are assigned to it
• element data are copied from device to shared memory by ﬁrst 32 threads in the block (the array with element
data is padded so there are no bank conﬂicts when accessing memory)
• because of the possible large size of the element stiﬀness matrix the outermost loop is the loop over stripes of
the matrix
• the next loop is the standard loop over integration points
• for each integration point (and each stripe!) the values of all shape functions and their derivatives are calculated
by a single thread
• geometrical quantities (Jacobian matrices etc.) are computed by a single thread
• derivatives of shape functions with respect to physical coordinates are calculated in parallel by the respective
number of threads
• in a double loop over the element stiﬀness matrix entries corresponding to a given thread the calculations of
entries are performed
• the entries for a given stripe of the element stiﬀness matrix are copied from the shared to the device memory
(with coalesced memory accesses)
5. Numerical experiments
The developed algorithm has been tested using an NVIDIA GeForce 8800GTX graphics card (with CUDA SDK
2.3 and Microsoft Visual Studio 2008 C/C++ compiler and environment). The performance results have been compared with those of a single core of AMD X2 processor (with 2.6GHz clock and 1MB L2 cache and the same C/C++
compiler and environment as GPU). Compilers were used with simple -O2 optimization ﬂag.
Both, the GPU and the CPU were performing single precision computations. The use of single precision was
dictated by the capabilities of the GPU utilized. For newer GPUs double precision calculations are also available.
The GFlops performance in this case is however worse. This can also concern CPU cores – vector instructions for
single precision numbers can be twice as fast as those for the double precision. Whether to use single or double
precision becomes a computational as well as numerical problem. Properly optimized single precision calculations
can be faster, but the accuracy of the ﬁnal ﬁnite element solution may require the use of double precision numbers
(or special mixed precision reﬁnement [4]). The ﬁnal decision which type of computations to apply will eventually
depend on the ﬁnite element problem solved and such factors as e.g. stability of the FEM algorithm or the condition
number of the obtained system of linear equations [5, 6].
Table 4 presents execution parameters that characterize the utilization of GPU resources during numerical integration. It can be seen how a proper number of stiﬀness matrix (SM) entries has to be chosen to achieve a balance
between the degree of concurrency within a single threadblock (more SM entries per thread means less redundant
shape functions calculations) and the concurrency among diﬀerent threadblocks (less SM entries per thread means
less GPU resources per threadblock and more active blocks per GPU multiprocessors).

1099

P. Macioł et al. / Procedia Computer Science 1 (2012) 1093–1100

7

/ Procedia Computer Science 00 (2010) 1–8

The number of threads in a block
The number of working threads
The number of SM entries per thread
The number of SM rows proceeded at once
The size of padded stiﬀness matrix
The number of registers per thread
Shared memory per block (in kB)
The number of active blocks

p=1
64
36
1
6 (all)
64
16
0.84
8

p=2
128
108
3
18 (all)
384
18
5.51
2

p=3
128
128
12.5
40 (all)
1600
22
6.90
2

p=4
384
375
15
25
5625
20
8.84
1

p=5
128
126
126
6
15876
21
5.04
2

p=6
256
196
196
7
38416
21
9.96
1

p=7
288
288
288
6
82944
24
10.91
1

Table 4: The characteristics of the execution of the numerical integration algorithm by NVIDIA GeForce 8800GTX graphics card for the model
problem and diﬀerent orders of approximation p

CPU core time per element [s]
GPU multiprocessor time per element [s]
Speed-up (1 core, 1 multiprocessor)
Speed-up (2 cores, 16 multiprocessors)

p=1
0.006
0.006
1.07
8.56

p=2
0.103
0.046
2.25
18.00

p=3
1.164
0.466
2.50
19.98

p=4
6.543
6.596
1.00
7.97

p=5
33.834
46.271
0.73
5.85

p=6
125.039
257.341
0.49
3.89

p=7
390.167
889.719
0.44
3.51

Table 5: Execution times in milliseconds and speed-up GPU versus CPU for the numerical integration algorithm applied to the model problem with
diﬀerent orders of approximation p for GeForce 8800GTX GPU and AMD X2 CPU

Table 5 shows the results of experiments. First, execution times are presented for a single CPU core and a single
GPU multiprocessor. The numbers are obtained by dividing the execution times for the whole CPU and GPU by the
number of cores or multiprocessors. The procedure is justiﬁed by the fact that the numerical integration calculations
are fully scalable - for large enough numbers of ﬁnite elements (the only interesting case) execution time for p cores
or multiprocessors will be p-times smaller than for one core or multiprocessor. Hence for any real CPU and GPU
architectures, with the same cores or multiprocessors, execution times can be computed from results in table 5. Moreover, the results can be used to compute GPU versus CPU speed-ups. The speed-up results in table 5 for the reference
processors: NVIDIA GeForce 8800GTX graphics card and AMD X2 CPU were obtained by dividing CPU times by
two and GPU times by 16 and then computing speed-up in the standard way. The same procedure can be repeated for
diﬀerent CPUs and GPUs, since in current designs the performance of single cores does not change that much, while
the number of utilized cores changes in time and from one model to another.
6. Conclusions
The most important conclusion is that still the most limiting factor for GPU computations of the kind presented in
the paper are the small resources available to individual threads. Given possible massive parallelism in the algorithm
considered in the paper, it was not fully realized in practice. Because of insuﬃcient memory resources we could not
choose the optimal strategy for parallelization and had to, instead, perform additional splittings of data structures as
well as additional calculations.
We succeeded however in another diﬃcult task, designing data structures in such a way so to allow for proper (coalesced) accesses of threads to arrays. As a ﬁnal result, the performance obtained during computational experiments
is encouraging and it seems that ﬁnite element codes, also those utilizing higher order of approximation, should proﬁt
from porting to GPU accelerators another important algorithm – numerical integration.
Acknowledgment
The support of this work by the Polish Ministry of Science and Higher Education under the grant N N501 120836
is gratefully acknowledged

1100

P. Macioł et al. / Procedia Computer Science 1 (2012) 1093–1100

/ Procedia Computer Science 00 (2010) 1–8

8

References
[1] D. G¨oddeke, H. Wobker, R. Strzodka, J. Mohd-Yusof, P. McCormick, S. Turek, Co-processor acceleration of an unmodiﬁed parallel solid
mechanics code with FEASTGPU, International Journal of Computational Science and Engineering (IJCSE) (2009) to appear.
[2] P. Ciarlet, The Finite Element Method for Elliptic Problems, North-Holland, Amsterdam, 1978.
[3] NVIDIA CUDA Programming Guide 2.2.1, NVIDIA, 2009.
[4] M. Baboulin, A. Buttari, J. Dongarra, J. Kurzak, J. Langou, J. Langou, P. Luszczek, S. Tomov, Accelerating scientiﬁc computations with mixed
precision algorithms, preprint.
[5] D. G¨oddeke, R. Strzodka, S. Turek, Performance and accuracy of hardware-oriented native-, emulated- and mixed-precision solvers in FEM
simulations, International Journal of Parallel, Emergent and Distributed Systems 22 (4) (2007) 221–256.
[6] D. Komatitsch, D. Micha, G. Erlebacher, Porting a high-order ﬁnite-element earthquake modeling application to NVIDIA graphics cards using
CUDA, Journal of Parallel and Distributed Computing 69 (5) (2009) 451–460.

