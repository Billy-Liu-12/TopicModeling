Procedia Computer
Science
Procedia
Computer Science
Science 001(2010)
1–81579–1586
Procedia
Computer
(2012)

www.elsevier.com/locate/procedia

International Conference on Computational Science, ICCS 2010

An optimal Finite State Projection Method
Vikram Sunkaraa , Markus Heglanda
a Centre

of Mathematics and its Applications, Mathematical Sciences Institute,
The Australian National University, Australia

Abstract
It is well known that many realistic mathematical models of biological and chemical systems, such as enzyme cascades
and gene regulatory networks, need to include stochasticity. These systems can be described as Markov processes and
are modelled using the Chemical Master Equation (CME). The CME is a diﬀerential-diﬀerence equation (continuous
in time and discrete in the state space) for the probability of certain state at a given time. The state space is the
population count of species in the system. A successful method for computing the CME is the Finite State Projection
Method (FSP). In this paper we will give the mathematical background of the FSP and propose a new addition to the
FSP which guaranties our approximation to have optimal order.
c 2012 Published by Elsevier Ltd. Open access under CC BY-NC-ND license.
⃝
Keywords: Chemical Master Equation, Finite State Projection, State Space Growth.

1. Introduction
The ﬁnite state projection method (FSP) [1] by Munsky and Khammash has been very successful in solving the
chemical master equation even for relatively large systems. It achieves this by an adaptive approach where only
probabilities are stored if they are suﬃciently diﬀerent from zero. While there is some evidence from computational
experiments and some theoretical observations which support the claim of high eﬃciency [1, 2, 3], what is missing in
the FSP literature is a rigorous mathematical analysis of the performance. In this paper we will provide bounds based
on results from nonlinear approximation theory.
The bounds use estimates of weak r spaces (instances of Lorentz spaces) which have been used successfully in
nonlinear approximation theory, in particular, in the discussion of N-term approximation [4].
The weak r spaces and the theory of N-term approximation have been instrumental to the establishment of a new
paradigm of solving partial diﬀerential equations and integral equations with wavelets [5, 6, 7]. In particular, this
approach provides a solid foundation for its adaptive solution methods. This is remarkable as the theory of adaptive
✩ Research

funded by the ARC Centre of Excellence in Bioinformatics and The Australian National University.
Email addresses: Vikram.Sunkara@anu.edu.au (Vikram Sunkara), Markus.Hegland@anu.edu.au (Markus Hegland)

c 2012 Published by Elsevier Ltd. Open access under CC BY-NC-ND license.
1877-0509 ⃝
doi:10.1016/j.procs.2010.04.177

1580

V. Sunkara, M. Hegland / Procedia Computer Science 1 (2012) 1579–1586

Sunkara et al. / Procedia Computer Science 00 (2010) 1–8

2

solution of partial diﬀerential equations has still many open questions and does not have such a solid foundation. The
ﬁnite state method may be viewed as a simple variation of the adaptive methods used in the wavelet techniques. It is
thus not surprising that the same theoretical background can be used here.
The outline of the remaining sections is as follows: In section 2 we provide a review of the FSP algorithm and some
currently known results. Section 3 then gives an additional step to the FSP which will guaranty optimality in the
approximation. In section 4 we show how to obtain bounds on the weak r quasi-norms for some speciﬁc cases. In
particular, we have some results for unimodal probability distributions.
2. The ﬁnite state projection method (FSP)
The ﬁnite state projection method approximates the solution of the chemical master equation
dp
= Ap,
dt
Ω
J
with initial condition p(0) = p0 , by determining a solution in a ﬁnite dimensional subspace V J = RΩ
+ ⊂ R+ obtained
by truncating the full space Ω. The algorithm is able to determine the V J adaptively such that the approximate solution
p J which satisﬁes
d pJ
= AJ pJ
dt
has a guaranteed error bound p J (t) − p(t) 1 ≤ and moreover, one has p J (t) ≤ p(t) component wise. Here A J =
A[Ω J , Ω J ] denotes the restriction of the matrix onto the index set Ω J and p J (0) = p0 (Ω J ) is the restriction of the initial
condition to the set Ω J . The algorithm was suggested by Munsky and Khammash in [1]. The authors prove two
theorems which guarantee that the L1 error of the algorithm is at most . The size of Ω J is determined using heuristics
which control that Ω J is not too large. This has been further improved in [3]. The FSP for a single time step proceeds
as follows:

Repeat
check if error less than epsilon
if not
grow Omega_J
solve again to get p_J
else stop
While the method works well in practice there is no theoretical guarantee on the size of Ω J . Indeed, the results so
far cannot prove optimality. Using results from approximation theory, we will describe how Ω J may grow using the
results from [1], we will see that this growth is suboptimal. A suggestion on how to improve the algorithm to achieve
provable optimal performance is provided in the next section.
3. Optimal FSP
We still start by giving some background needed to describe the new Optimal FSP method. The results used here have
been developed in nonlinear approximation theory, in particular in the theory of n-term approximations. A review of
this work can be found in the paper by DeVore [4]. Consider any mapping f : Ω → R for a countable set Ω where
f ∈ L1 (Ω), i.e.,
| f (x)| < ∞.
f 1=
x∈Ω

There then exists an enumeration σ : N → Ω (of Ω) such that the sequence f ∗ deﬁned by
f ∗ (k) = | f (σ(k))|

1581

V. Sunkara, M. Hegland / Procedia Computer Science 1 (2012) 1579–1586

Sunkara et al. / Procedia Computer Science 00 (2010) 1–8

is non-decreasing, i.e.,

∗
,
f ∗ (k) ≥ fk+1

3

k ∈ N.

By deﬁnition, the enumeration is a bijection and we call f ∗ the nondecreasing rearrangement of f . The weak r-norm
of f is deﬁned as
f r,∞ = sup k1/r f ∗ (k)
k∈N

and for r ∈ (0, 1) let the set Lr,∞ (Ω) be
Lr,∞ (Ω) = { f ∈ L1 (Ω) | f

r,∞

< ∞}.

Note that · r,∞ is not a norm, only a quasinorm as the triangle inequality does not hold. The following equivalent
characterisations of Lr,∞ can be found in [4]:
Proposition 1. Let r ∈ (0, 1) and p ∈ L1 (Ω). Then the following three statements are equivalent:
1. p ∈ Lr,∞
2. there exists M > 0 such that the nondecreasing rearrangement p∗ of p satisﬁes p∗ (k) ≤ Mk−1/r
3. there exists an M > 0 such that
|{x | | f (x)| > θ, x ∈ Ω}| ≤ M r θ−r
where |S | denotes the cardinality of any set S .

The least M > 0 for which the bounds above are satisﬁed is M = f

r,∞ .

Approximation theory provides characterisations of approximation spaces which in our case are of the form
A s = { f ∈ L1 (Ω) | f − fN

1

≤ CN −s }

where fN is the best N-term approximation of f which by deﬁnition satisﬁes | supp f | = N and
f − fN

1

for all g with | supp g| = N.

≤ f − g 1,

The N-term approximation is thus the function which has at most N nonzero values which minimises the error f −
fN 1 .
A major result of approximation theory is that
where

As =

(1)

r,∞

1
= s+1
r
and furthermore, the error of the N-term approximation in this case is
f − fN ≤ C N −s f

r,∞ .

(2)

(3)

These results can be found in the review paper [4] by DeVore.
The thresholding approximation which occurred in proposition 1 is deﬁned by
⎧
⎪
⎪
for x ∈ Ω and p(x) ≥ θ
⎨ p(x)
pθ (x) := ⎪
⎪
⎩0
for x ∈ Ω and p(x) < θ.

(4)

By construction, the thresholding approximation reproduces the N-term approximation with N = | supp pθ |. Inserting
N = p rr,∞ θ−r from proposition 1 in the optimal error bound (3) gives
p − pθ

1

≤ θ1−r p

r
r,∞ .

1582

V. Sunkara, M. Hegland / Procedia Computer Science 1 (2012) 1579–1586

4

Sunkara et al. / Procedia Computer Science 00 (2010) 1–8

These results from approximation theory tell us that the weak
order N −s , where N is the support of the best approximation.

r

spaces are exactly the spaces where the errors are of

Our aim is to have an approximation which gives the error to the same order as the error of the n-term approximation.
We will show below that the FSP as is, doesn’t not achieve the same order in the error as the n-term approximation.
Hence we add in an extra step to make it optimal. This will be shown here in:
In our context an approximation f of f is said to have optimal order if | supp f | = O( −1/s ), where s = 1/r − 1
and f − f 1 ≤ . This deﬁnition is a generalisation of the statement given in [5]. This property ensures an optimal
memory size for the approximate solution [5].
In the following we will say that pFSP is a (generalised) FSP approximation of the probability distribution p if it is
nonnegative and
• has ﬁnite support
• pFSP − p ≤

• 0 ≤ pFSP (x) ≤ p(x),

for all x ∈ Ω.

The FSP method described in [1] does have these properties. While the support can be small, the results given so far
in the literature do not indicate that the method is optimal. The bounds
pFSP (x) ≤ p(x) ≤ pFSP (x) +
given in [1] do imply that any element which is or smaller could be truncated but there is no guarantee that just
the largest elements will be truncated. Indeed, most of the error could be incurred by errors in the nonzero elements
and not in truncation. We suggest to add an extra N-term approximation step which ensures that the support of the
approximation is small and the approximation error is order optimal. Thus our approach has the following two steps:
(a) determine pFSP
/2 using any FSP method

(5)

(b) determine a best N term approximation p of pFSP
/2 with minimal N and
p − pFSP
/2

1

≤ /2.

This then follows:
Theorem 1. Let p be determined as in (5). Then p is an order optimal approximation for p ∈
r ∈ (0, 1). In particular, one has p − p 1 ≤ and support bounded by
| supp p | ≤ C p
for some C > 0 and

1
r

r,∞

and some

1/s −1/s
r,∞

= s + 1.

Proof. By the triangle inequality one has
p− p
For any p ∈

r,∞

one has pFSP
/2 ∈

r,∞

1

≤ p − pFSP
/2

1

+ pFSP
/2 − p

1

≤ .

as
0 ≤ pFSP
/2 (x) ≤ p(x), for all x in the domain.

Then by Lemma 1 it follows that

pFSP
/2

r,∞

≤ p

r,∞

FSP
As the N-term approximation p is an optimal approximation of pFSP
/2 , one has for the support | supp p | ≤ γ p /2
where
1
= s + 1.
r

1/s
r,∞

( /2)−1/s

1583

V. Sunkara, M. Hegland / Procedia Computer Science 1 (2012) 1579–1586

Sunkara et al. / Procedia Computer Science 00 (2010) 1–8

Combining the last two inequalities:

| supp p | ≤ 21/s γ p

5

1/s −1/s
.
r,∞

We have proved that adding the step given in (5) guaranties optimailty. The general literature on the FSP have shown
that the error is of O( ), however in the theorem above we were able to show that the error is of O( −1/s ), (s > 0), for
the new method.
In the next section we will give a framework for the estimation of the r quasinorm. As we have observed through this
section, knowing the r quassinorm can help us predict and describe how the state space grows over time, this would
have uses in improving approximation methods of the CME.
4. Estimating the weak

r

quasi-norms

From Theorem 1, we can deduce that if we knew the asymptotic of p(t) r,∞ , for a jump process p(t), then we can
infer asymptotic of the growth of the state space. In this section we will derive the asymptotic of p(t) r,∞ , where p(t)
is a Poisson process.
The analysis requires the determination of weak

r

p

quasi-norms for r ∈ (0, 1), which were deﬁned as
r,∞

= sup p∗ (k)k1/r ,
k∈N

where p∗k denotes the non-decreasing rearrangement of the values p(x) for the probability distribution p : Ω → R+
with countable Ω. While the determination of p∗ (k) for ﬁnite vectors requires a simple sort operation, working with
p∗ (k) for inﬁnite vectors turns out to be more diﬃcult.
In the simplest case with Ω = N and a monotone distribution, i.e. p(k + 1) ≤ p(k), one has p∗ (k) = p(k). In this case
the quasinorm is
p r,∞ = sup p(k)(k + 1)1/r .
k∈N

Unfortunately, monotone probability distributions are rare. Even in the case of the Poisson processes where p(k) =
e−t tk /k!, the condition p(k + 1) ≤ p(k) only holds for all k ∈ N if t ∈ [0, 1].

For t > 1 the Poisson distribution is unimodal and this is a very common situation for probability distributions.
Unimodality means that the distribution only has one local maximum. In the case of Ω = N there is thus a kmax such
that
p(k) ≤ p(kmax ), for all k ∈ N.
We now derive a lower bound for the quasinorms. They are based on two simple monotonicity results.
Lemma 1. Let p, q ∈ RN
+ be two probability distributions such that q ≥ p, i.e.,
for all k ∈ N.

q(k) ≥ p(k),
Then
and furthermore,

q∗ (k) ≥ p∗ (k),
q

σ,∞

for all k ∈ N

≥ p

σ,∞ .

A direct consequence is the next result for quasinorms relating the quasi norm of a subsequence q to that of p. We
will call a subsequence a sequence where some elements have been replaced by zeros.
Lemma 2. For any subsequence q of a probability distribution p ∈ RN
+ one has
p

r,∞

≥ q

r,∞ .

1584

V. Sunkara, M. Hegland / Procedia Computer Science 1 (2012) 1579–1586

Sunkara et al. / Procedia Computer Science 00 (2010) 1–8

6

Proof. This is a simple consequence of lemma 1 as in this case p ≥ q.
Using these results we can now get the following lower bound for the quasinorm of a unimodal probability distribution.
Proposition 2. Let p ∈ RN
+ be a unimodal probability distribution with maximum p(kmax ). Then the quasinorm is
bounded from below by
p r,∞ ≥ sup p(k) |k − kmax + 1|1/r .
k∈N

Proof. A unimodal distribution p has the following two monotone subsequences q1 and q2 deﬁned by
⎧
⎪
⎪
k ≥ kmax
⎨ p(k)
q1 (k) = ⎪
⎪
⎩0
k < kmax
⎧
⎪
⎪
⎨ p(k)
q2 (k) = ⎪
⎪0
⎩

k ≤ kmax
k > kmax .

One can see that the corresponding (qi )∗ are obtained by moving the nonzero elements to the front and for i = 2
reverting the order. One thus has
q1 r,∞ = sup p(kmax + k)(k + 1)1/r
k∈N

and

q2

r,∞

=

max p(kmax − k)(k + 1)1/r .

k=0,...,kmax

The bound then follows by applying lemma 1 to get
p

r,∞

≥ max{ q1

r,∞ ,

q2

r,∞ }.

The lower bound is very important in order to establish optimality of algorithms. The upper bound has been omitted
in this paper, however we conjecture that an upper bound can be obtained which is proportional to the lower bound.
4.1. The

r -quasinorm

of the Poisson distribution

The Poisson distribution is

tk
.
k!
is given by

p(k) = e−t
For the Poisson distribution the maximal point kmax

p(kmax − k) ≤ p(kmax + k) ≤ p(kmax ),

k = 0, . . . , kmax ,

and kmax ∈ [t − 1, t]. One sees that for non-integer t the maximum is unique at kmax ∈ t and for integer t one has two
maximal points at kmax = t and kmax = t − 1. In the previous section we have seen that the r-quasinorm for unimodal
distributions are proportional to
e−t tkmax +k (k + 1)1/r
.
(6)
sup
(kmax + k)!
k∈N
To understand the asymptotic of the r quasinorm, we let kmax + k be the maximal point in (6), then the maximal point
would satisfy
e−t tkmax +k−1 k1/r e−t tkmax +k (k + 1)1/r e−t tkmax +k+1 (k + 2)1/r
≤
≥
.
(kmax + k − 1)!
(kmax + k)!
(kmax + k + 1)!

1585

V. Sunkara, M. Hegland / Procedia Computer Science 1 (2012) 1579–1586

7

Sunkara et al. / Procedia Computer Science 00 (2010) 1–8

This gives the two inequalities
(kmax + k) k1/r ≤ t (k + 1)1/r ,

(kmax + k + 1) (k + 1)1/r ≥ t (k + 2)1/r .
Now lets consider the case of integer times t. In this case we can set kmax = t − 1 and one gets the two bounds,
(k − 1)k1/r
and
(k + 1)1/r − k1/r
k(k + 1)1/r
t≤
.
(k + 2)1/r − (k + 1)1/r

t≥

Now let

φ(k) =
then the above inequality is

(k − 1)k1/r
,
(k + 1)1/r − k1/r

(7)

φ(k) ≤ t ≤ φ(k + 1).

(8)

By applying mean value theorem to φ(m) := (k + m)1/r , m ∈ [0, 1], one has
(k + 1)1/r − k1/r =

1
(k + κ)1/r−1 ,
r

Applying 1/r > 1 one has
(k + 1)1/r − k1/r ≥

for some κ ∈ (0, 1).,
1 1/r−1
k
.
r

Applying thus lower bound to (7) gives us the asymptotic
φ(k) = rk2 (1 + o(k)).
According to (8) we have t ≈ φ(k)(1 + o(k)), hence
k≈

We will now estimate the term in (6), then substitute (9) to get an asymptotic for p
Stirling’s approximation to the denominator:

We now estimate

(9)

t/r.
r,∞ .

We start by applying the

e−t tt−1+k (k + 1)1/r
e−t tt−1+k (k + 1)1/r
≈
√
(t − 1 + k)!
((t − 1 + k)/e)t−1+k 2π(t − 1 + k)

by taking a log transformation, which reduces to

e−t tt−1+k
,
((t − 1 + k)/e)t−1+k

−t + (t − 1 + k) log(t) − (t − 1 + k) log(t − 1 + k) + (t − 1 + k) = −(t − 1 + k)(log(t − 1 + k) − log(t)) + k − 1.

Applying log(t − 1 + k) − log(t) ≈ (k − 1)/(t − 1 + k) one sees that approximately everything cancels out. Hence we get
p
Substituting (9) gives us

r,∞

=

(k + 1)1/r
e−t tt−1+k (k + 1)1/r
≈ √
.
(t − 1 + k)!
2π(t − 1 + k)
p

r,∞

≈

√
( t/r + 1)1/r
.
√
2π(t − 1 + (t/r))

1586

V. Sunkara, M. Hegland / Procedia Computer Science 1 (2012) 1579–1586

Sunkara et al. / Procedia Computer Science 00 (2010) 1–8

8

5. Conclusions
In this paper we have shown that by making a small alteration to the FSP we guarantee our approximation to have optimal order. We have also provided the foundation to estimating the weak r norm, which helps us compute estimates
for the growth of the state space under the FSP method. Furthermore we have given asymptotic of the Poisson distribution under the weak r norm. Further research involves extending these results to help implement more accurate
and faster solvers for the Chemical Master Equation.
Reference
[1] M. Khammash, B. Munsky, The ﬁnite state projection algorithm for the solution of the chemical master equation, The Journal of Chemical
Physics 124 (044104).
[2] S. MacNamara, A. M. Bersani, K. Burrage, R. B. Sidje, Stochastic chemical kinetics and the total quasi-steady-state assumption: application
to the stochastic simulation algorithm and chemical master equation, Journal of Chemical Physics 129.
[3] K. Burrage, M. Hegland, S. MacNamara, R. Sidje, A krylov-based ﬁnite state projection algorithm for solving the chemical master equation
arising in the discrete modeling of biological systems., In: Langville, A.N., Stewart, W.J. (Eds.), Proceedings of the 150th Markov Anniversary
Meeting, Boson Books. (2006) pp. 21–38.
[4] R. A. DeVore, Nonlinear approximation, Acta Numerica 7 (-1) (1998) 51–150. doi:10.1017/S0962492900002816.
[5] A. Cohen, W. Dahmen, R. DeVore, Adaptive wavelet methods for elliptic operator equations: convergence rates, Math. Comp. 70 (233) (2001)
27–75 (electronic).
[6] A. Cohen, Numerical analysis of wavelet methods, Vol. 32 of Studies in Mathematics and its Applications, North-Holland Publishing Co.,
Amsterdam, 2003.
[7] W. Dahmen, Multiscale and wavelet methods for operator equations, in: Multiscale problems and methods in numerical simulations, Vol. 1825
of Lecture Notes in Math., Springer, Berlin, 2003, pp. 31–96.

