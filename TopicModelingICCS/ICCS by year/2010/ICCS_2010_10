Journal Logo

Procedia Computer Science 1 (2012) 1805–1813
00 (2010) 1–9

www.elsevier.com/locate/procedia

International Conference on Computational Science, ICCS2010

Towards mechanical derivation of Krylov solver libraries
Victor Eijkhouta,∗, Paolo Bientinesi1 , Robert van de Geijnc
a Texas

Advanced Computing Center, The University of Texas at Austin
b RWTH-Aachen, Germany
c Department of Computer Science, The University of Texas at Austin

1. Introduction
We show how the FLAME methodology [1, 2] can be used to derive the Conjugate Gradients (CG) algorithm [3].
We use this example to make the case that a FLAME-based environment for deriving, and reasoning about, new
iterative methods and implementations is a distinct, and attractive, possibility.
CG and related methods for solving a linear system Ax = b are iterative processes, computing a new approximation
xi to the solution in each i-th iteration. This process involves complicated computations of (scalar) quantities that
follow from conditions, such as orthogonality of the residuals ri = Axi − b. In this paper we will show how a very high
level description of the algorithm leads by a mechanical process, ﬁrst to a predicate to be satisﬁed by the essential
loop body of the algorithm, then, driven by the correctness proof of this loop body, to the actual instructions.
Our paper proceeds as follows.
In Section 2, we present a block formalism for iterative methods, ﬁrst proposed by Householder [4]. This casts
iterative methods as a set of governing conditions on the matrices that express the vector sequences, which makes it
amenable to the FLAME approach for deriving algorithms. For this, in Section 2 we extend the FLAME notation
slightly.
Traditional expositions of the CG method, and other Krylov methods, posit the basic form of relations between
matrices and vectors, and compute the scalar coeﬃcients in them by ‘lengthy induction arguments’ [5]. In Section 3
we show how the FLAME methodology dispenses with these induction proofs, and how it can be applied to a representative computation yielding a nonsymmetric CG algorithm.
Finally, in Section 4 we present a systematic framework for determining how variables must be deﬁned as part of
the computation. Our derivation is driven by the construction of Hoare triples that together constitute a correctness
proof of the algorithm derived.
Together these insights support the vision of systematic and, ultimately, mechanical derivation of Krylov subspace
methods libraries. In this paper we derive some existing algorithms, leading to the prospect of automatically generated,
proved correct, library software1 . Our methodology holds the further prospect of easy experimentation with new
methods and derivation of computationally advantageous variants of old ones.
∗ Corresponding

author
Email address: eijkhout@tacc.utexas.edu (Victor Eijkhout)
1 This prospect is already largely realized through FLAME for dense direct matrix algorithms [6].

c 2012 Published by Elsevier Ltd. Open access under CC BY-NC-ND license.
1877-0509 ⃝
doi:10.1016/j.procs.2010.04.202

1806

V. Eijkhout et al. / Procedia Computer Science 1 (2012) 1805–1813
Eijkhout et al. / 00 (2010) 1–9

2

For readers familiar with the FLAME concept, we remark that this research marks several new ideas. First of all,
our basic partitioning scheme is 3-way, rather than 2-way. Next, our Partitioned Matrix Expression (PME) is not only
more elaborate than in previous research, we introduce the device of augmenting it with new equations that are not
part of the original problem deﬁnition. Finally, the update step derived gives only implicit deﬁnition of the quantities
to be computed; the actual computation requires its own correctness proof.
2. Theory and Notation
We start by presenting, in this section, the basic form of iterative methods such as the CG method in a block
formalism [7, 4]. Then we phrase this block formalism in terms of the FLAME framework. Our formulation will be
quite general, giving a basic form that holds for all polynomial iterative methods. The speciﬁc computations in the
algorithm will follow in a later section from orthogonality requirements.
2.1. Block formalism
This section serves to familiarize the reader with the block formalism, and to establish the basic equations, as well
as the question of their essential degrees of freedom. These dof s will then be derived in subsequent sections.
We need the basic concept of a ‘Krylov sequence’, which, given a square matrix A and initial vector k0 , is deﬁned
as the matrix with n columns K A, k0 ≡ k0 Ak0 A2 k0 · · · An−1 k0 . In other words, the jth column of
K A, k0 , k j , is deﬁned by the recurrence
kj =

k0
Ak j−1

if j = 0,
otherwise,

which we can write as AK = K J where the underline means that the last column is omitted, and J is deﬁned as the
n × (n − 1) matrix with entries
1 if i = j + 1,
Ji j =
0 otherwise.
We now state the coupled recurrences form of polynomial iterative methods. In block form they are
⎧
⎧
⎪
⎪
⎪
⎪
⎨ri+1 = ri − Api δii ,
⎨APD = R(I − J)
and in scalar form ⎪
⎪
⎪
⎪
⎩ pi+1 = ri+1 − j≤i p j υ ji .
⎩P(I + U) = R

(1)

where R are residuals, P search directions, D a diagonal matrix, and U strictly upper triangular. This form can be
derived from ‘ﬁrst principles’ (for full details, see [8]), but anyone familiar with iterative methods will recognize
that they mostly conform to this scheme, testifying to its generality. The diﬀerent iterative methods that exist (CG,
MinRes, BiCGstab) all follow from imposing certain conditions on R, or equivalently on the coeﬃcients of D and U.
For instance, stationary iteration and steepest descent correspond to U ≡ 0; the CG method corresponds to U being
single upper diagonal (upper bidiagonal), with values deriving from the orthogonality of R.
In the remainder of this paper, we will take the block form of (1) as our starting point, and show how FLAME can
be used to derive the vector recurrences as well as the coeﬃcients in D and U.
2.2. FLAME Notation for Representing Krylov Subspace Methods
In this section, we use the CG iteration to motivate “index-free” notation that in Section 3 will allow us to systematically derive the algorithm.
Let us consider the nonsymmetric CG iteration2 given in Figure 1. In that ﬁgure, for the moment ignore the
column marked by “Step” and the gray-shaded boxes. To understand the notation used to present the algorithm, it
suﬃces to note that RL r M RR represents matrix R partitioned into three key components: the “old” residual
vectors (RL ), the most recently computed residual vector (r M ), and the residual vectors yet to be computed (RR ). At
2 See

[8] for a similar derivation of the (symmetric) Hestenes-Stiefel CG method.

1807

V. Eijkhout et al. / Procedia Computer Science 1 (2012) 1805–1813

3

Eijkhout et al. / 00 (2010) 1–9

Step

1a
4
2
3
2,3

5a

6

8

7

5b

2
2,3
1b

Annotated Algorithm: Compute R, P, D, U as given in Eqn. (2)
{precondition holds}
R → RL r M RR , P → PL p M PR ,
⎞
⎞
⎞
⎛
⎛
⎛
0
0 ⎟⎟
⎜⎜⎜ JT L
⎜⎜⎜ UT L uT M UT R ⎟⎟⎟
⎜⎜⎜ dT ⎟⎟⎟
⎟⎟⎟
⎟
⎟
⎜⎜⎜ t
⎜
⎜
⎟
⎜
⎜
T
0
0 ⎟⎟⎟ , U → ⎜⎜⎜ 0
0
u MR ⎟⎟⎟ , d → ⎜⎜⎜ δ M ⎟⎟⎟⎟
J → ⎜⎜⎜ er
⎟⎠
⎟⎠
⎟⎠
⎜⎝
⎜⎝
⎝
0
e0 JBR
0
0
U BR
dB
{loop-invariant holds}
while n(RR ) > 0 do
{ (loop-invariant holds) ∧ ( n(RR ) > 0 ) }
RL r M RR → R0 r1 r2 R3 , PL p M PR → P0
⎞
⎛
⎛
⎞
0 ⎟⎟
⎜⎜⎜⎜ J00 0 0
0
0 ⎟⎟
⎜⎜⎜ JT L
⎟⎟
⎜⎜⎜ et
⎟⎟
⎜⎜⎜ t
0 0
0 ⎟⎟⎟⎟
⎟⎟ ,
0
0 ⎟⎟⎟⎟ → ⎜⎜⎜⎜ r
⎜⎜⎜⎝ j ML
⎟⎠
⎜⎜⎜ 0
1 0
0 ⎟⎟⎟⎟
⎠
⎝
0
j MR JBR
0
0 e0 J33
⎞
⎞
⎛
⎛
⎞
⎞
⎛
⎜⎜⎜ U00 u01 u02 U03 ⎟⎟⎟ ⎛
⎜⎜⎜ d0 ⎟⎟⎟
⎜⎜⎜ UT L uT M UT R ⎟⎟⎟
⎟⎟⎟ ⎜⎜⎜ dT ⎟⎟⎟
⎟
⎜⎜⎜
⎜⎜⎜
T
⎟
⎟
⎜⎜⎜
0
υ12 u13 ⎟⎟ ⎜⎜⎜
δ ⎟⎟
⎜ 0
⎟⎟⎟ , ⎜⎜ δ M ⎟⎟⎟⎟ → ⎜⎜⎜⎜⎜ 1 ⎟⎟⎟⎟⎟
0
uTMR ⎟⎟⎟⎟ → ⎜⎜⎜⎜
⎜⎜⎜ 0
T
⎟
⎟
⎜
⎜⎜⎜ 0
⎜⎜⎜ δ2 ⎟⎟⎟
⎠
⎠
⎝
0
0
u23 ⎟⎟⎟ ⎝
⎠
⎠
⎝
⎝
0
0
U BR
dB
0
0
0
U33
d3
{before predicate holds}
S 0 : d1 := r1t r1 /r1t Ap1
S 1 : r2 := r1 − Ap1 d1
S 2 : u02 := (Pt0 AP0 )−1 Pt0 Ar2
S 3 : υ12 := (pt1 Ap1 )−1 (pt1 Ar2 − pt1 AP0 u02 )
S 4 : p2 := r2 − P0 u02 + p1 υ12
{after predicate holds}
RL r M RR ← R0 r1 r2 R3 , PL p M PR ← P0
⎞
⎛
⎛
⎞
0 ⎟⎟
⎜⎜⎜ J00 0 0
⎟
0
0 ⎟⎟
⎜⎜⎜ JT L
⎜⎜⎜ et
⎟
0 ⎟⎟⎟⎟⎟
⎜⎜⎜ t
⎟
⎜ r 0 0
⎟,
0
0 ⎟⎟⎟⎟ ← ⎜⎜⎜⎜
⎜⎜⎜ j ML
⎟⎠
⎜⎜⎜ 0
1 0
0 ⎟⎟⎟⎟⎟
⎝
⎠
⎝
0
j MR JBR
0
0 e0 J33
⎞
⎞
⎛
⎛
⎞
⎛
⎜⎜⎜⎜ U00 u01 u02 U03 ⎟⎟⎟⎟ ⎛⎜ dT ⎞⎟
⎜⎜⎜⎜ d0 ⎟⎟⎟⎟
⎜⎜⎜ UT L uT M UT R ⎟⎟⎟
T
⎟
⎜
⎟
⎜
⎜
⎟
⎜
⎟
⎜
⎜
0
0
υ
u
δ
⎟
⎟
⎜⎜⎜
12
⎟⎟ ⎜⎜⎜
⎜
⎜ 1 ⎟⎟⎟⎟⎟
13 ⎟
0
uTMR ⎟⎟⎟⎟ ← ⎜⎜⎜⎜
, ⎜ δ ⎟⎟⎟ ← ⎜⎜⎜⎜
⎜⎜⎜ 0
⎟⎠
⎜⎜⎜ 0
⎜⎜⎜ δ2 ⎟⎟⎟⎟⎟
0
0
uT23 ⎟⎟⎟⎟⎟ ⎜⎜⎝ M ⎟⎟⎠
⎝
⎠
⎠
⎝
⎝
0
0
U BR
dB
0
0
0
U33
d3
{loop-invariant holds}
endwhile
{ (loop-invariant holds) ∧ ¬ ( n(RR ) > 0 ) }
{postcondition holds}

p1

p2

P3

,

p1

p2

P3

,

Figure 1: Algorithm the nonsymmetric Conjugate Gradients method presented as part of a “worksheet”.

the top of the loop, the next residual vector to be computed is exposed as r2 , and at the bottom of the loop that newly
computed vector becomes r M . Thus, thick and thin lines are used to indicate movement through the matrices. Vector
d represents the diagonal of diagonal matrix D. Vectors e0 and er correspond to the zero vector with a 1 in the ﬁrst
and last entry, respectively.
Careful examination reveals that the so presented algorithm exposes the relationship between the computations of
the CG algorithm and the matrices in Eqn. (2).

1808

V. Eijkhout et al. / Procedia Computer Science 1 (2012) 1805–1813
Eijkhout et al. / 00 (2010) 1–9

4

3. Applying the FLAME Methodology to the CG Algorithm
We now show how the FLAME framework can be used to derive iterative methods such as the CG algorithm. This
section shows how the basic block form of the CG algorithm leads – through a systematic process – to the predicate
that needs to be satisﬁed by the loop body of the algorithm. This predicate is translated into concrete instructions in
the next section.
The reader should imagine the “worksheet” in Fig. 1 as being initially empty. We ﬁll it out in the order indicated
in the column marked “Step”.
Step 1: Precondition and postcondition. The precondition and postcondition indicate the states of the variables before
and after the algorithm is executed, respectively.
The deﬁning equations for iterates, residuals and search directions, under orthogonality of the residuals are given
by
(2)
X(I − J) = PD, APD = R(I − J), P(I + U) = R, Rt R = Ω (diagonal)

We will ignore the ﬁrst equation, since X can be computed almost trivially from the search direction sequence P, and
other quantities do not depend on it. From the above equations we can derive other relations, in particular
Pt AP = (I + U)−t Rt R(I − J)D−1 = (I + U)−t Ω(I − J)D−1 ,

(3)

which implies that Pt AP is upper triangular (and diagonal if A is symmetric). Now, the precondition is {Re = Ax0 − b}
where x0 is an initial guess for the solution, and the postcondition is formed by combining the precondition and
equations (2) and (3):
{APD = R(I − J) ∧ P(I + U) = R ∧ Rt R = Ω ∧ Pt AP = lower triangular ∧ Re = Ax0 − b.}
This information is entered in the worksheet.
Determining the Partitioned Matrix Expression (PME). We are interested in expressing the postcondition in terms of
partitioned matrices. This yields
⎞
⎛
⎧
⎪
⎟⎟⎟
⎜⎜⎜ IT L − JT L 0
0
⎪
⎪
⎪
⎟⎟⎟
⎜
⎪
⎜
⎪
⎜
t
⎪
⎟⎟⎟ ,
⎪
1
0
APL DL Ap M d M APR DR = RL r M RR ⎜⎜⎜⎜ −er
⎪
⎪
⎟⎟⎟
⎪
⎜⎜⎝
⎪
⎪
⎠
⎪
⎪
0
e
I
−
J
⎪
0
BR
BR
⎪
⎪
⎞
⎛
⎪
⎪
⎪
uT R
⎪
⎟⎟⎟⎟
⎜⎜⎜⎜ IT L + UT L uT M
⎪
⎪
⎪
⎟⎟⎟
⎜⎜⎜
⎨
⎟⎟⎟ = RL r M RR ,
(4)
0
1
u MR
PL p M PR ⎜⎜⎜
⎪
⎪
⎪
⎟⎟⎠
⎜
⎪
⎜⎝
⎪
⎪
⎪
0
0
I
+
U
⎪
⎪
⎪
⎞BR⎛ t BR
⎞
⎞ ⎛
⎛
⎛ t ⎞
⎪
⎪
⎪
⎟⎟⎟ ⎜⎜⎜ PL APL PtL Ap M PtL APR ⎟⎟⎟ ⎜⎜⎜
⎜⎜⎜
⎜⎜⎜ RL ⎟⎟⎟
0
0
0 0 ⎟⎟⎟
⎪
⎪
⎪
⎟
⎟⎟⎟
⎟
⎟
⎜
⎜
⎜
⎜
⎪
⎟⎟⎟ ⎜⎜⎜ t
⎟ ⎜
⎜
⎜⎜⎜ t ⎟⎟⎟
⎪
⎪
⎟⎟⎟ ,
⎪
⎟⎟⎟ , ⎜⎜⎜ p M APL ptM Ap M ptM APR ⎟⎟⎟⎟⎟ = ⎜⎜⎜⎜⎜
⎜⎜⎜ r M ⎟⎟⎟ RL r M RR = ⎜⎜⎜⎜⎜ 0
0
0
⎪
⎪
⎟⎟⎟
⎪
⎟⎟⎠ ⎜⎜⎝
⎟
⎟⎟⎠ ⎜⎜⎝
⎜⎜⎝
⎜
⎪
⎪
⎠
⎩⎜⎝ Rt ⎟⎠
t
t
t
0
0
P
AP
P
Ap
P
AP
L
M
R
R
R
R
R

where indicates that the exact value is not of interest. Note our convention that upper case letters (R, P) denote
matrices, lower case vectors (e, r, p), and lowercase Greek letters scalars (δ, ω).
Step 2: Loop-invariant. The loop-invariant is a predicate on the state of the variables that is satisﬁed before and after
each iteration of the loop. The Fundamental Theorem of Invariance establishes that if the loop completes, then the
loop-invariant and the negation of the loop-guard hold true after the loop. This is all captured in Fig. 1.
One of the key concepts of the FLAME methodology is that of selecting a loop-invariant a priori, and then
constructing an algorithm around it. In terms of program correctness this means that we set up a proof of correctness
ﬁrst, and then build an algorithm that satisﬁes such a proof.
To derive a loop-invariant, it is observed that while the loop executes, not all results in the PME have yet been
achieved. Thus, the loop-invariant consists of subresults that are part of the PME. For space considerations we will

1809

V. Eijkhout et al. / Procedia Computer Science 1 (2012) 1805–1813

5

Eijkhout et al. / 00 (2010) 1–9

not go into further detail here. The point is that there is a systematic way of choosing loop-invariants from the PME,
and that choice is often non-unique (which then leads to diﬀerent algoritms). We choose the loop-invariant
APL DL

= RL (IT L − JT L ) − r M etr ∧ (IT L + UT L )PL PL uT M + p M =
0
RtL RL RtL r M
PtL APL PtL Ap M
=
=
∧
rtM RL rtM r M
0
ptM APL ptM Ap M

RL
0

rM
.

∧

(5)

Steps 3 and 4: The loop-guard and initialization. The loop-guard is the condition under which control remains in
the loop. If the loop-invariant is maintained, then it will be true after the loop completes and the loop-guard will be
false. Together these predicates must imply that the post-condition has been computed. Thus, the loop-invariant and
the postcondition dictate the choice of the loop-guard. This loop-guard is given in Fig. 1.
Similarly, the loop-invariant must be true before the loop commences. Thus, an initialization, given in Step 4 of
Fig. 1, is dictated by the precondition and the loop-invariant. (We note that the initial partitionings of the operands are
merely indexing operations.)
Step 5: Traversing the operands. The computation must make progress through the operands, so that the loop-guard
will eventually be false and the loop terminates. This dictates the updates of partitionings in Steps 5a and 5b. In step
5a we split oﬀ one column from the ‘R’ block, going from a 3-way to a 4-way partitioning, for instance
RL

rM

RR

→

R0

←

R0

r1

r2

R3

;

after the intervening computations, in step 5b we compact the ﬁrst two partitions into the new ‘L’-block, for instance
RL

rM

RR

r1

r2

R3

.

Step 6: The ’before predicate’. The repartitioning of the operands in Step 5a is purely an indexing step; no computations are implied. Thus, at Step 6 (before the computation in Step 8) the contents of the diﬀerent submatrices are
still prescribed by the loop-invariant. These contents can be derived by applying the loop invariant (5) to the 4-way
partitionin derived in step 5a.
This process yields what we will call the ’before predicate’:
⎞
⎞
⎛
⎛
⎧
⎪
⎜⎜⎜ J00 ⎟⎟⎟
⎜⎜⎜ I + U00 u01 ⎟⎟⎟
⎪
⎪
⎪
⎟⎟⎠ = R0 r1 ,
⎟
⎜
⎜
AP0 D0 = R0 r1 ⎜⎝ t ⎟⎠ ,
⎪
P0 p1 ⎜⎝
⎪
⎪
⎪
j10
0
1
⎨
⎛ t ⎞
⎞ ⎛
⎞
⎞ ⎛
⎛
Pbefore : ⎪
(6)
⎪
⎪
0 ⎟⎟⎟ ⎜⎜⎜ Pt0 AP0 Pt0 Ap1 ⎟⎟⎟ ⎜⎜⎜
0 ⎟⎟⎟
⎪
⎜⎜⎜⎜ R0 ⎟⎟⎟⎟
⎜⎜⎜⎜
⎪
⎪
⎟
⎟
⎟
⎜
⎜
⎪
⎟⎠ , ⎜⎝ t
⎟⎠ .
⎟⎠ = ⎜⎝
⎪
⎩⎜⎝ rt ⎟⎠ R0 r1 = ⎜⎝ 0
p AP
pt Ap
1

1

0

1

1

Step 7: The ’after predicate’. At the bottom of the loop the loop-invariant must again be true. This means that the
update in Step 8 must place the submatrices in a state were the loop-invariant is again true after the redeﬁnition of the
partitioned operands (Step 5b). The state that the submatrices must be in can be derived by substituting equivalent
submatrices (as deﬁned by Step 5b) into the loop-invariant after which algebraic manipulation yields the desired ’after
predicate’ in Step 7:
⎧
⎞
⎞
⎛
⎛
⎪
⎜⎜⎜ D0 0 ⎟⎟⎟
⎜⎜⎜ J00 0 ⎟⎟⎟
⎪
⎪
⎪
⎟⎟⎠ + r2 0 −1 ,
⎟
⎜
⎜
⎪
A P0 p1 ⎜⎝
⎟⎠ = R0 r1 ⎜⎝ t
⎪
⎪
⎪
0
δ
j
1
⎪
1
⎪
10
⎪
⎞
⎛
⎪
⎪
⎪
⎜⎜⎜ I + U00 u01 u02 ⎟⎟⎟
⎪
⎪
⎪
⎜
⎪
⎟⎟⎟
⎜
⎪
⎜
⎪
⎪
⎪
P0 p1 p2 ⎜⎜⎜⎜
0
1
υ12 ⎟⎟⎟⎟ = R0 r1 r2 ,
⎪
⎪
⎟⎟⎠
⎜⎜⎝
⎪
⎪
⎪
⎪
⎪
0
0
1
⎪
⎪
⎞
⎛
⎪
⎨⎛ t ⎞
⎜⎜⎜
⎜⎜⎜ R0 ⎟⎟⎟
0 0 ⎟⎟⎟
Pafter : ⎪
(7)
⎪
⎟⎟⎟
⎜
⎪
⎜⎜⎜
⎪
⎜⎜⎜⎜ t ⎟⎟⎟⎟
⎪
⎟
⎪
⎪
0 ⎟⎟⎟ ,
⎜⎜⎜ r1 ⎟⎟⎟ R0 r1 r2 = ⎜⎜⎜ 0
⎪
⎪
⎟⎟⎠
⎜⎜⎝
⎪
⎜⎝ t ⎟⎠
⎪
⎪
⎪
r
⎪
0
0
⎪
2
⎪
⎞
⎪
⎞ ⎛
⎛ t
⎪
⎪
⎪
⎜⎜⎜ P0 AP0 Pt0 Ap1 PT0 Ap2 ⎟⎟⎟ ⎜⎜⎜
0 0 ⎟⎟⎟
⎪
⎪
⎟⎟⎟
⎜
⎪
⎟
⎜
⎪
⎟ ⎜⎜
⎜⎜⎜ t
⎪
⎪
⎪
0 ⎟⎟⎟⎟ .
⎜⎜⎜ p1 AP0 pt1 Ap1 pT1 Ap2 ⎟⎟⎟⎟⎟ = ⎜⎜⎜⎜
⎪
⎪
⎪
⎟ ⎜
⎜
⎟⎟⎠
⎪
⎪
⎩⎝ pt AP0 pt Ap1 pT Ap2 ⎠ ⎜⎝
2
2
2

1810

V. Eijkhout et al. / Procedia Computer Science 1 (2012) 1805–1813
Eijkhout et al. / 00 (2010) 1–9

6

Step 8: The update. Comparing the before and after predicates yields
Pafter = Pbefore ∧(δ1 Ap1 = r1 − r2 ) ∧ (P0 u02 + υ12 p1 + p2 = r2 )
∧(Rt0 r2 = 0) ∧ (r1t r2 = 0)) ∧ (Pt0 Ap2 = 0) ∧ (pT1 Ap2 = 0).
Step 8 of Fig. 1 has to update and compute variables in such a way that, given the ‘before’ predicate, the ’after’
predicate will hold. In the next section we show how the actual computation again follows by a mechanical process
from these predicates.
Final algorithm. The described process constructs the algorithm by systematically deriving predicates that indicate
the state that the variables must be in, which in turn dictates the actual computational statements. Eliminating the
predicates leaves the ﬁnal algorithm.
4. Deriving the update
There are two critical steps in the above derivation that are less than straightforward for a more complex algorithm
like the CG algorithm: choosing the loop-invariant, and identifying a set of updates of the operands that transform the
’before’ predicate into the ’after’ predicate. In this section, we focus on how the update can be systematically derived.
This derivation is much more systematic than in previous papers of ours that focus on dense matrix computations, for
the reason that in those cases the update step was relatively obvious.
4.1. Deriving assignment statements
To understand the approach one must ﬁrst understand some fundamental results from computer science related to
the derivation of algorithms. Consider the triple {Q}S {R} where Q and R are predicates indicating the state of variables
and S is a command in, or segment of, the algorithm. This is known as a Hoare triple and is itself a predicate that
evaluates to true if the command S , when initiated with variables in a state where Q is true, completes in a state where
R is true. In this triple Q is the precondition and R is the postcondition. In our discussion in Section 3 and Fig. 1 we
have seen many examples of Hoare triples and how they can be used to reason about the correctness of an algorithm.
A Hoare triple can be used to assert a code segment correct. For example, {χ = η}χ := χ + 1{χ = η + 1} takes on the
value true.
The next question becomes “Under what circumstances is the Hoare triple {Q}x := exp{R} true, where exp
is an expression. To answer to this question the operator wp(S , R) is introduced: this returns the weakest precondition (least restrictive predicate) that describes the state of variables such that if the statement S is executed,
then this command completes in a state where R is true. Now, {Q}S {R} if and only if Q implies wp(S , R). If
we wish to ﬁnd a sequence of statements S 0 ; S 1 ; . . . ; S k−1 such that {Q}S 0 ; S 1 ; . . . ; S k−1 {R} then Q must imply
wp(S 0 ; S 1 ; . . . ; S k−1 , R) = wp(S 0 , wp(S 1 , . . . , wp(S k−1 , R) . . .)). We can summarize this by noting that the following must be true:
{Q ⇒ wp(S 0 , Q1 )}S 0 {Q1 = wp(S 1 , Q2 )}S 1 {Q2 = wp(S 2 , Q3 )} . . . {Qk−1 = wp(S k−1 , R)}S k−1 {R}
Finally, we recall that wp(”x := exp”, R) equals the predicate R with all instances of x replaced by the expression
exp. For example, wp(”x := x + 1”, x = y + 4) = {(x + 1) = y + 4} = {x = y + 3}.

V. Eijkhout et al. / Procedia Computer Science 1 (2012) 1805–1813
Eijkhout et al. / 00 (2010) 1–9

1811
7

4.2. Application to the CG algorithm
The above theory can be used to derive the update in Fig. 1. The idea is that we wish to determine expressions
exp0 , . . . , exp4 such that3
{Pbefore }
{Q0 = wp(”δ1 := exp0 ”, Q1 )}
S 0 : δ1 = exp0
{Q1 = wp(”r2 := exp1 ”, Q2 )}
S 1 : r2 := exp1
{Q2 = wp(”u02 := exp2 ”, Q3 )}
S 2 : u02 := exp2
{Q3 = wp(”υ12 := exp3 ”, Q4 )}
S 3 : υ12 := exp3
{Q4 = wp(”p2 := exp4 ”, Pafter )}
S 4 : p2 := exp4
Pafter = Pbefore ∧(δ1 Ap1 = r1 − r2 ) ∧ (P0 u02 + υ12 p1 + p2 = r2 )
∧(Rt0 r2 = 0) ∧ (r1t r2 = 0) ∧ (r2t r2 = ω2 ) ∧ (Pt0 Ap2 = 0) ∧ (pt1 Ap2 = 0)

Now,

Q4

= wp(”p2 := exp4 ”, Pafter )
= {Pbefore ∧(δ1 Ap1 = r1 − r2 ) ∧ (P0 u02 + υ12 p1 + exp4 = r2 )
∧(Rt0 r2 = 0) ∧ (r1t r2 = 0) ∧ (Pt0 Aexp4 = 0) ∧ (pt1 Aexp4 = 0)}

from which we deduce that exp4 = r2 − P0 u02 − υ12 p1 and
Q4

= wp(”p2 := r2 − P0 u02 − υ12 p1 ”, Pafter )
= {Pbefore ∧(δ1 Ap1 = r1 − r2 ) ∧ T ∧ (Rt0 r2 = 0) ∧ (r1t r2 = 0))
∧(Pt0 A(r2 − P0 u02 − υ12 p1 ) = 0) ∧ (pt1 A(r2 − P0 u02 − υ12 p1 ) = 0)}

=

{Pbefore

∧(δ1 Ap1 = r1 − r2 ) ∧ (Rt0 r2 = 0) ∧ (r1t r2 = 0))
∧(Pt0 Ar2 − Pt0 AP0 u02 = 0) ∧ (pt1 Ar2 − pt1 AP0 u02 − υ12 pt1 Ap1 = 0)}

Similarly, we can determine υ12 := exp3 = (pt1 Ar2 − pt1 AP0 u02 )/pt1 Ap1 and
Q3

= wp(”υ12 := (pt1 Ar2 − pt1 AP0 u02 )/pt1 Ap1 ”, Q4 )
= {Pbefore ∧(δ1 Ap1 = r1 − r2 ) ∧ (Rt0 r2 = 0) ∧ (r1t r2 = 0)) ∧ (Pt0 Ar2 − Pt0 AP0 u02 = 0) ∧ T }

Next we can determine u02 := exp2 = (Pt0 AP0 )−1 Pt0 Ar2 and

= wp(”u02 := (Pt0 AP0 )−1 Pt0 Ar2 ”, Q2 )
= {Pbefore ∧ (δ1 Ap1 = r1 − r2 ) ∧ (Rt0 r2 = 0) ∧ (r1t r2 = 0)) ∧ T }
followed by r2 := exp1 = r1 − δ1 Ap1 and
Q2

Q1

=

wp(”r2 := r1 − δ1 Ap1 ”, Q2 )

= {Pbefore
= {Pbefore

∧T ∧ (Rt0 (r1 − δ1 Ap1 ) = 0) ∧ (r1t (r1 − δ1 Ap1 ) = 0) ∧ T }

∧T ∧ (r1t r1 − δ1 r1t Ap1 = 0)}

(where Rt0 r1 = 0 is part of the ‘before’ equations and Rt0 Ap1 = 0 can be derived from them) and ﬁnally δ1 := exp0 =
r1t r1 /r1t Ap1 and
Q0

= wp(”δ1 := r1t r1 /r1t Ap1 ”, Q1 ) = {Pbefore

so that Pbefore implies Q0 , as required.
The updates of the variables can then be entered as Step 8 in Figure 1.
3 In

∧T }

section 5 we will address the fact that we need not lay out explicitly the sequence in which quantities are to be computed.

1812

V. Eijkhout et al. / Procedia Computer Science 1 (2012) 1805–1813
Eijkhout et al. / 00 (2010) 1–9

8

5. Discussion and Conclusion
At ﬁrst glance, the reader may conclude that the presented extension of the FLAME framework merely provides a
‘mental discipline’ for deriving known Krylov subspace based algorithms. While this may become a major contribution of the project, we believe it shows a lot more promise than just that.
The reader may have already noticed that there are a number of decisions that were made that led to the derived
algorithm. Let us itemize some of these decisions and discuss how diﬀerent choices will lead to a rich family of
algorithms, both diﬀering in mathematical respects and in performance aspects.
• The governing equation. In Section 2, we started with the governing equation
⎧
⎪
⎪
⎨APD = R(I − J)
⎪
⎪
⎩P(I + U) = R

The additional equation Rt R = Ω (diagonal) represents one choice of constraints that can be enforced on the
residual vectors. As mentioned, diﬀerent choices lead to diﬀerent known methods, such as Steepest Descent or
GMRES.
We believe the presented methodology will be able to clarify how all these methods are related, but drawing
up the constraints is work still to be undertaken. Our framework will make it far easier for a human expert
to derive new algorithms, since only the basic notion (orthogonality of the residuals in the CG case) needs to
be speciﬁed on top of the basic equations: the derivation of the actual constants is done through a systematic,
indeed automatable, process.

• PME manipulation Even within the context of a single method such as CG, manipulation of the PME can
be interesting. We already saw this mechanism in action when equation (3), which is not strictly part of the
deﬁnition of the CG method, was added. In [8], this mechanism was used to argue that our approach can
discover variant algorithms that combine inner products [9, 10].
• Choices of invariants. The governing equation leads to a PME, which is as a recursive deﬁnition of the operation. The methodology systematically transforms this recursive deﬁnition into a loop-based algorithm. But for
each PME there are multiple possible loop-invariants. Some of these may lead to uncomputable formulations;
other may lead to distinct algorithm that may or may not have desirable properties for a given situation (see [8]
for an example).
• Choices for updates of individual variables. There are often diﬀerent choices for computing variables all of
which lead to correct algorithms, although possibly with diﬀerent computational or numerical properties. For
example, manipulation of the governing equation in our example show that rit pi = rit ri , which leads to a diﬀerent
algorithmic version for the same loop-invariant.
A related source of variant algorithms derives from the observation that in Section 4 we started by ﬁxing the
order in which the variables were to be computed. In practice, there may be multiple orders that lead to further
versions for a given loop-invariant. One possibility is to examine the ‘before’ and ‘after’ predicates for inherent
dependencies that determine the order. Another would be to try all possible orderings.
• How to choose. Given that we expect a large family of algorithms to result from the ultimate approach, we
need to develop a way of determining which algorithm is most appropriate for a given situation. Measures
of “goodness” could include computational cost, numerical stability, rate of convergence, or ability to reduce
communication cost on, for example, a distributed memory parallel architecture. There is a distinct possibility
of reasoning about such factors in our framework, which we are undertaking in separate research.
We conclude that our framework supports a vision for exploration of Krylov subspace methods as a coherent
family of algorithms, as well as the derivation of proved correct library software. The discussion above shows that
the space to be explored is large, which is where mechanization becomes an important part of the solution. How to
achieve mechanization of derivation for dense matrix computations was the subject of the dissertation of one of the
authors [11]. His system will need to be expanded to achieve what we propose. In other words, there is a lot of
interesting research ahead of us.

V. Eijkhout et al. / Procedia Computer Science 1 (2012) 1805–1813
Eijkhout et al. / 00 (2010) 1–9

1813
9

Acknowledgments
This work was sponsored by NSF through awards CCF 0917096 and OCI-0850750, and by grant GSC 111 of
the Deutsche Forschungsgemeinschaft (German Research Association). Any opinions, ﬁndings and conclusions or
recommendations expressed in this material are those of the authors and do not necessarily reﬂect the views of the
National Science Foundation (NSF).
References
[1] J. A. Gunnels, F. G. Gustavson, G. M. Henry, R. A. van de Geijn, Flame: Formal linear algebra methods environment, ACM Transactions on
Mathematical Software 27 (4) (2001) 422–455.
URL http://doi.acm.org/10.1145/504210.504213
[2] R. A. van de Geijn, E. S. Quintana-Ort´ı, The Science of Programming Matrix Computations, www.lulu.com, 2008.
[3] M. Hestenes, E. Stiefel, Methods of conjugate gradients for solving linear systems, Nat. Bur. Stand. J. Res. 49 (1952) 409–436.
[4] A. S. Householder, The theory of matrices in numerical analysis, Blaisdell Publishing Company, New York, 1964, republished by Dover
Publications, New York, 1975.
[5] J. G. Lewis, R. G. Rehm, The numerical solution of a nonseperable elliptic partial diﬀerential equation by preconditioned conjugate gradients,
J. Res. Nat. Bureau of Standards 85 (1980) 367–390.
[6] P. Bientinesi, E. S. Quintana-Ort´ı, R. A. van de Geijn, Representing linear algebra algorithms in code: The FLAME application programming
interfaces, ACM Transactions on Mathematical Software 31 (1) (2005) 27–59.
URL http://doi.acm.org/10.1145/1055531.1055533
[7] S. F. Ashby, T. A. Manteuﬀel, P. E. Saylor, A taxonomy for conjugate gradient methods, SIAM J. Numer. Anal. 27 (1990) 1542–1568.
[8] V. Eijkhout, P. Bientinesi, R. van de Geijn, Formal derivation of Krylov methods, Tech. Rep. TR-08-03, Texas Advanced Computing Center,
The University of Texas at Austin (2008).
[9] A. Chronopoulos, C. Gear, s-step iterative methods for symmetric linear systems, Journal of Computational and Applied Mathematics 25
(1989) 153–168.
[10] E. D’Azevedo, V. Eijkhout, C. Romine, A matrix framework for conjugate gradient methods and some variants of cg with less synchronization
overhead, in: Proceedings of the Sixth SIAM Conference on Parallel Procesing for Scientiﬁc Computing, SIAM, Philadelphia, 1993, pp. 644–
646.
[11] P. Bientinesi, Mechanical derivation and systematic analysis of correct linear algebra algorithms, Ph.D. thesis, The University of Texas at
Austin, Department of Computer Sciences (2006).

