Procedia Computer
Science
Procedia
Computer Science
1–10
Procedia
Computer
Science001(2010)
(2012)
1727–1736

www.elsevier.com/locate/procedia

International Conference on Computational Science, ICCS 2010

Data Transport between Visualization Web Services for
Medical Image Analysis
Spiros Koulouzis1,∗, Elena Zudilova-Seinstra∗, Adam Belloum∗
Section Computational Science, Informatics Institute, University of Amsterdam, the Netherlands

Abstract
With the rapid development of IT data driven experiments and simulations have become very advanced and complicated. The amount and complexity of scientiﬁc data increase exponentially. Nowadays the challenge is to eﬃciently support scientists who generate more data than they can possibly look at and understand. This requires not
only high-performance visualization and feature extraction techniques but also eﬃcient and intuitive management of
available data and underlying computational resources. Consequently, Service Oriented Architectures and workﬂow
management systems are becoming popular solutions for the deployment of e-Science Infrastructures aimed to assist in exploration and analysis of large scientiﬁc data. In this paper, we compare two transport models of workﬂow
execution for data-intensive medical visualizations that rely on web services. The image-based analysis of vascular
disorders served as a case study for this project. We applied a service oriented approach to construct distributed
visualization pipelines, which allow visualization experts to develop visualizations to view and interact with large
medical data sets. Moreover, end-users (i.e., medical specialists) can explore these visualizations irrespective of their
geographical location and available computing resources. The paper reports on the current implementation status and
presents our main ﬁndings.
c 2012 Published by Elsevier Ltd. Open access under CC BY-NC-ND license.
⃝
Keywords: Service-oriented visualization, Web services, Data Transfers, Image-based medical analysis
1. Introduction
With the fast development of information technologies, the amount of data at one’s disposal is enormous. Data
is produced at diﬀerent scales, arrives from various sources (sensors, instruments, simulations, databases, direct manipulations, etc.) and has diﬀerent structures. The data explosion has led to very large detailed data sets. Sheer
sizes, large-scale multi-regional and multi-institution collaborations have resulted in the distribution of data around
the world. The greatest challenge today is to eﬃciently support the community of researchers who generate more data
than they can possibly look at and understand [1].
Clinical data-driven experiments and simulations have become very advanced and complicated. For instance,
manual tracing of structures such as the vascular system, which may appear in more than 1000 images in a single
∗

Email address: S.Koulouzis@uva.nl (Adam Belloum)
author

1 Corresponding

c 2012 Published by Elsevier Ltd. Open access under CC BY-NC-ND license.
1877-0509 ⃝
doi:10.1016/j.procs.2010.04.194

1728

S. Koulouzis et al. / Procedia Computer Science 1 (2012) 1727–1736
S. Koulouzis, E.V. Zudilova-Seinstra, A.S.Z. Belloum / Procedia Computer Science 00 (2010) 1–10

2

Magnetic Resonance Angiography (MRA) or Computed Tomography (CT) imaging study, requires both considerable
expertise and a great deal of time. Hence, processing, visualization and integration of large medical data sets are the
major cornerstones of the modern Healthcare [2]. e-Science can assist in coping with the challenges presented by the
creation, management and analysis of these large data sets. Thus, e-Science has been introduced as a paradigm that
promotes collaborative and interdisciplinary research [1, 3]. With the use of Service Oriented Architectures (SOA),
concepts of distributed processing, and data access, have scaled to new levels.
Adopting SOA provides new challenges and opportunities for data delivery and information management. Connectivity between distant locations, interoperability between diﬀerent kinds of systems and resources and high levels
of computational performance are some of the most promising characteristics of this architecture. Taking advantage
of these beneﬁts, we have developed a visualization framework based on the service oriented model introduced in [4].
This model suggests that any intermediate visualization sub-process is potentially transformable into an independent
web service. The granularity oﬀered by this model, enables users to compose and execute distributed visualization
pipelines, including those running on heterogeneous distributed resources. The image-based analysis of vascular
disorders served as the case study for this project. We experimented with scanned and simulated medical data.
2. Background
Vascular diseases are among the leading causes of death and disability all over the world, especially in developed
countries. In general, vascular diseases fall into two main categories [5]: aneurisms and stenosis. An aneurysmal
disease is a balloon like swelling of the artery. Stenosis is a narrowing of the artery. To redirect the blood ﬂow or
to repair the weakened artery, a vascular reconstruction procedure can be applied. A criterion for the success of a
vascular procedure is the normalization of the blood ﬂow in the aﬀected area. However the best treatment is not
always obvious because of the complexity of the human vascular system and because of other diseases that a patient
may have.
Numerical simulation of the blood ﬂow in human vascular structures helps to obtain an extensive knowledge about
its behavior and to develop solutions for the treatment of vascular disorders [6]. Sometimes simulation data sets are
so big and complex that they cannot be easily interpreted. Instead, visualizations of such data can reveal relevant
information without loosing the overall perspective. However, the location of these data sets is often distributed. This
with the combination of limited local resources (hardware, software, etc.) creates diﬃculties to researchers for easy
development, sharing and execution of their visualizations.
e-Science focuses on creating a collaborative research environment where resources can be shared at a global scale.
This enables research to cross and combine many disciplines, in order to address and solve more complex problems.
Grid computing as the (e)investiture of e-Science provides excellent opportunities in this respect [7, 8], especially
in recent years when this paradigm has shifted towards SOA [9]. This has motivated several attempts to build Grid
enabled visualization frameworks. For instance, the RealityGrid [10] project used visualization as part of a bespoke
application to visualize the output of simulations running on the Grid. In the gViz project [11], an extension to the
NAG IRIS Explorer [12] was developed. Thanks to this extension, individual IRIS Explorer visualization modules
can be prepared and executed using remote Grid resources. The Grid Visualization Kernel (GVK) developed as part
of the CrossGrid project is a middleware extension built on top of the Globus Toolkit. GVK allows remote interactive
visualization of both original medical data and acquired simulation results [13, 14].
Unfortunately, existing frameworks have drawbacks caused by the visualization models they support. None of
the traditional visualization models (e.g., visualization cycle model [15], visualization pipeline model [16], data-pull
models [17], reference visualization models [18, 19], etc.) fully comply with the needs and requirements of distributed
computing as they are based on the major linkage of visualization sub-processes. In the Grid environment, this linkage often results in underutilization of the used computing resources, which are often requested for a period of time
which is longer than the actual usage [20, 21]. Also, to allow interactive user steering over the intermediate visualization stages, external monitoring tools need to be developed. When frequently invoked, these tools can signiﬁcantly
slowdown the visualization process [11, 22].

1729

S. Koulouzis et al. / Procedia Computer Science 1 (2012) 1727–1736
S. Koulouzis, E.V. Zudilova-Seinstra, A.S.Z. Belloum / Procedia Computer Science 00 (2010) 1–10

3

Figure 1: From a visualization pipeline to a service-oriented model.

3. A Service-Oriented Approach to Data Visualization
To better utilize the available distributed resources and provide users with an easier and more ﬂexible way of
visualizing medical data sets, we have opted for a SOA approach. SOA refers to a style of building reliable distributed
systems that deliver functionality as services, with the additional emphasis on loose coupling between interacting
services [23]. This approach provides some beneﬁts, namely: i) Scalability: Services can be added and removed in
order to meet the demands of the application. ii) Reliability: Provided that services maintain a standard interface new
or updated implementations can be introduced, without disrupting the functionality of any workﬂow or application.
iii) Fault tolerance: If a service becomes unavailable for any reason, clients can still query for alternate services that
oﬀer the same functionality. iv) Reusability: A service can be part of more than one workﬂow or application, as it can
interact with multiple clients. Moreover, because web services provide a descriptive interface –called Web Service
Description Language (WSDL)–, this makes them ideal for usage in workﬂow management systems. Thanks to these
workﬂow management systems, end-users can easily create visualizations that use distributed data sets and resources.
Considering these beneﬁts, we applied a service-oriented visualization model presented in [4]. This model originates from the traditional visualization pipeline model of Haber and McNabb [16] and is based on the principle that
any visualization sub-process (i.e., reading, ﬁltering, mapping and rendering) can be transformed into an independent
service [24]. In the architecture shown in Figure 1, each visualization service performs a speciﬁc role and can be
executed on a diﬀerent logical machine at a diﬀerent geographical location. Therefore, thanks to the granularity of
SOA, services can be assigned to diﬀerent resources based on their complexity and computational demands. For instance, the rendering service2 can be provided with a more signiﬁcant computational power than the mapping service
responsible only for the conversion of the ﬁltered data into geometrical primitives.
Interactive execution of workﬂows plays an important role in service oriented visualization. This can be achieved
by means of workﬂow orchestration, where control over each visualization sub-process is centralized in the workﬂow
engine3 to allow a better conﬁguration and control over the visualization pipeline, as well as error handling. In
addition, the amount of data that needs to be analyzed by visualization services, and thus to be transferred from one
service to another, might cause signiﬁcant time loss with respect to the overall workﬂow execution.
4. A Visualization Case Study
In this project, we focused on the visualization of two diﬀerent types of medical data: experimental and simulated.
The experimental data about the patient’s vascular condition was be obtained via Magnetic Resonance Imaging (MRI),
and then converted to the Visualization Toolkit (VTK) format. To simulate the microscopic properties of the blood
ﬂow, while exploring its macroscopic properties, the lattice-Boltzmann method (LBM) was applied. The main idea
behind LBM is mapping the average motion of the ﬂuid/blood particles on the lattice [5].
In this project, we chose the iso-surface extraction4 technique to visualize data of the patient’s vascular condition.
To represent the simulated blood ﬂow in a selected region of interest, we used streamline-based ﬂow visualization
(Figure 2-c-d). Streamlines represent a set of paths traced out by massless particles as they move within a ﬂow [25, 26].
2 It

takes the geometry generated by the mapping stage of the visualization pipeline and renders it into the ﬁnal image.
set of higher level clients, able to coordinate the invocation of web services
extraction allows the representation of human vascular structures as surfaces, which are generally constructed using polygons as
primitives (see Figure. 2-a)
3A

4 Iso-surface

1730

S. Koulouzis et al. / Procedia Computer Science 1 (2012) 1727–1736
S. Koulouzis, E.V. Zudilova-Seinstra, A.S.Z. Belloum / Procedia Computer Science 00 (2010) 1–10

4

Figure 2: Iso-surface extraction (a, b); streamline-based ﬂow visualization (c, d); combined visualization (e, f)

(a)

(b)

Figure 3: Service enabled pipelines: iso-surface extraction (a); streamline-based ﬂow visualization (b)

Using streamlines, the user (e.g., medical specialist) can investigate how the ﬂow curves and whether it diverges or
converges at speciﬁc points. To make it easier for users to analyze the behavior of the blood ﬂow, streamlines can be
displayed in combination with the patient’s data where streamlines are positioned inside the arterial structures as it is
shown in Figure 2-e-f.
5. Visualization Services
Based on our previous work [4] we developed a set of web services using the VTK. These web services allow the
construction of three visualization pipelines suitable for our case study described in Section 4. The ﬁrst pipeline is
the iso-surface extraction pipeline shown in Figure 3(a). As can be seen, the iso-surface extraction pipeline starts with
the data reading sub-process handled by the vtkStructuredPointsReader class. Further on, the pipeline contains
the vtkPolyDataMapper class that maps polygonal data (passed by vtkContourFilter) to graphics primitives,
and ﬁnally the vtkRenderer class responsible for rendering graphics primitives into the ﬁnal image (3D arterial
structures). The second pipeline is a streamline-based ﬂow visualization pipeline (Figure 3(a)). It also starts with
the reading sub-process handled by the vtkStructuredPointsReader class. The derived data object is then passed
to vtkStreamLine ﬁlter to generate streamlines. The position of each streamline has to be properly deﬁned via the
vtkPointSource class which generates random points in a sphere space with a speciﬁed center, radius and number
of seed points. The vtkTubeFilter is a ﬁlter that can be applied additionally to generate tubes wrapped around
streamlines. The vtkLookupTable class is used to apply the color palette to streamtubes. Finally, the pipeline
contains vtkPolyDataMapper and vtkRenderer, which serve the same purpose as explained for the iso-surface
extraction pipeline. In this project, we experimented with the blood ﬂow velocity values.
In total, three visualization services have been developed in this project: (i) the ReadVTK service responsible for
reading and ﬁltering sub-processes in the iso-surface extraction pipeline as well as in the streamline-based ﬂow visualization pipeline; (ii) The StreamVTK service responsible for the ﬁltering sub-process in the streamline-based ﬂow

1731

S. Koulouzis et al. / Procedia Computer Science 1 (2012) 1727–1736
S. Koulouzis, E.V. Zudilova-Seinstra, A.S.Z. Belloum / Procedia Computer Science 00 (2010) 1–10

(a)

5

(b)

(c)
Figure 4: Service-oriented visualization pipelines: (a) iso-surface extraction pipeline; (b) streamline-based visualization pipeline; (c) combined
visualization pipeline

(a)
(b)
Figure 5: Data transport models: (a) centralized — web services exchange data through a central repository; (b) distributed — web services
exchange directly with each other.

visualization pipeline; (iii) The RenderVTK service responsible for mapping and rendering sub-processes (applicable
to both pipelines).
In addition to the iso-surface extraction pipeline (Figure 4(a)) and the streamline-based ﬂow visualization pipeline
(Figure 4(b)) explained earlier, the aforementioned visualization web services can also be used to construct the combined visualization pipeline (Figure 4(c)).
6. Data Transport of Medical Data
As mentioned earlier, distributed visualization pipelines can potentially analyze and generate large amounts of
data. It is therefore required to eﬃciently transport data between visualization services. In this project, in order
to increase the eﬃciency of distributed visualization pipelines, we identiﬁed two models for delivering data to each
service. Each model uses SOAP to transfer references, instead of the actual data, because SOAP is ineﬃcient for
transporting large data sets [27]. Additionally, using references in an orchestration workﬂow allows the workﬂow
engine to maintain full control over the data ﬂow, while controlling the application logic. This approach enables users
to employ conditional elements in their workﬂow execution. In this way, visualization parameters can be passed to
web services allowing a ﬂexible conﬁguration of a visualization pipeline. For the ﬁrst data transport model shown in
Figure 5(a), we assumed that medical data are located in a central data repository, and all services transfer their results
to this location. This model should be reliable and able to support large medical data sets. The second model shown
in Figure 5(b), adopts a distributed approach. Intermediate temporary data are delivered directly from the producing
to the consuming services. This approach can speed up data driven workﬂows, while at the same time it relieves data
resources from unnecessary storage requirements.
Both models were implemented the use of the ProxyWS. The ProxyWS is a data-aware web service able to
coordinate the transportation of large data volumes from a variety of remote locations (GridFTP [28], LFC [29], SRM
[30], etc.), connect web services via direct streaming and also provide support for large data transfers to existing or
“legacy” web services.

1732

S. Koulouzis et al. / Procedia Computer Science 1 (2012) 1727–1736
S. Koulouzis, E.V. Zudilova-Seinstra, A.S.Z. Belloum / Procedia Computer Science 00 (2010) 1–10

6

(a)
(b)
Figure 6: ProxyWS Architecture. The ProxyWS used as for proxy calls (a) and the ProxyWS is used as an API (b).

7. The ProxyWS
The ProxyWS is designed to be a non-intrusive, portable web service that can transport large data sets between
web services, while taking care that results produced are not returned via SOAP. The main components that enable
the design to facilitate large data transfers are, the Virtual Resource System (VRS), the WSDL interface and the
VRSServer.
More speciﬁcally, VRS is a Java API used by the VBrowser [31] that enables homogeneous access to multiple
resources (services, data streams, ﬁle systems, etc.). It adopts a layered architecture, where at the top level the VRS
deﬁnes a resource abstraction seen as a handler for referencing resources like ﬁles and services. The next levels, the
API deﬁnes resources such as ﬁle systems (local ﬁle systems, GridFTP, LFC, SRM, etc.), HTTP(S) streams, OGSA
services [8], and web services. Thus this layered implementation adopted by the VRS enables a uniform access
to multiple data resources just by providing their URI. These features enable the ProxyWS to access diﬀerent data
resources in a uniform fashion, thereby oﬀering web services (legacy or new) access to those resources.
Next, the WSDL used by the ProxyWS exposes a set of methods that support data transports for legacy web
services (Figure 6(a)). The most important are: i) callService: This method invokes a web services located in the
same container 5 as the ProxyWS, and returns to the caller a URI referring to the data produced. ii) getUloadURI: To
be able to bring data to the consuming web services, the ProxyWS can provide multiple URIs that are used to transfer
data (i.e. HTTP or GridFTP). This gives to a client the chance to decide which is more suitable. iii) getResourceURI:
This method can reference any resource, local or remote, in any of the available protocols (i.e., a local ﬁle or a data
stream to an HTTP URL, or a remote GridFTP ﬁle to an HTTP URL).
Besides using the ProxyWS as a service for making proxy calls, the ProxyWS can be used as an API for developing
web services capable of exchanging large data sets. The eﬀort required for such a task is minimal as it only involves
the implementation of a method to obtain URI for data transfer. Figure 6(b) shows the architecture of such a web
service.
Finally the VRSServer is a simple interface that enables web services to stream data to other resources independently from a speciﬁc infrastructure (GridFTP, etc.). An implementation of that is the HTTPTransport component,
which from the web service’s point of view only provides secure6 I/O streams to data resources, or other web services. More speciﬁcally the HTTPTransport is able to directly stream data produced by the ProxyWS or any ﬁle,
irrespective of whether it is local or remote.
5A

6 In

web service container is a SOAP engine i.e. Axis
this case this is achieved by HTTPS, which provides reasonable protection from eavesdroppers and man-in-the-middle attacks.

1733

S. Koulouzis et al. / Procedia Computer Science 1 (2012) 1727–1736
S. Koulouzis, E.V. Zudilova-Seinstra, A.S.Z. Belloum / Procedia Computer Science 00 (2010) 1–10

7

(a)
(b)
Figure 7: Implementation two visualization workﬂows. The workﬂow shown in (a) adopts a centralized data ﬂow, while (b) a distributed.

8. Experimental Setup
In order to compare advantages and disadvantages of the data transport models described in Section 6, we implemented two workﬂows. The ﬁrst workﬂow shown in Figure 7(a) adopts the centralized data ﬂow model, while the
second shown in Figure 7(b) uses the distributed model. More speciﬁcally the centralized data workﬂow supports the
following scenario:
1) The workﬂow engine invokes the ﬁrst ReadVTK service to read the artery data set from a remote location (a
GridFTP server). 2) The ﬁrst ReadVTK service downloads and reads the artery data-set, the location (memory or local
ﬁle system) of which is returned back to the workﬂow engine. 3) The workﬂow engine provides the ﬁrst ReadVTK
service with the remote URI (the GridFTP server) to upload the results. 4) The same procedure takes place with the
second ReadVTK service, but with the blood ﬂow ﬁeld data instead. 5) The workﬂow engine calls the StreamVTK web
service to calculate streamlines for the ﬂow ﬁeld data, by providing the URI of the remote server, where the simulated
data are located. 6) The result of the previous step is uploaded to the remote server. 7) The workﬂow engine calls the
rendering service by providing the URIs of the artery and ﬂow ﬁeld (that now is mapped to streamlines) polydata. As
soon as these polydata have been rendered, the resulting image is uploaded again to the remote server. 8) Finally, the
user can inspect the generated image from the remote server.
Alternatively, the distributed data workﬂow shown in Figure 7(b), is executed as follows: 1) The workﬂow engine
invokes the ﬁrst ReadVTK service to read the experimental data set from a remote location (a GridFTP server), which
returns a URI (local ﬁle system) where the result is saved. 2) The same procedure is followed for the second ReadVTK
service. 3) In order to calculate streamlines for the ﬂow ﬁeld data set, the workﬂow engine invokes the StreamVTK
service to obtain the data from the second ReadVTK service. 5) As soon as both data sets have been read, ﬁltered,
and mapped, the workﬂow engine calls the RenderVTK service to render the ﬁnal image. 6) After the RenderVTK has
generated the ﬁnal image, the user can inspect it directly from the location of the RenderVTK service.
These two workﬂows where compared in terms of total execution time, using diﬀerent conﬁgurations and data
sets. Thus for each workﬂow three diﬀerent data sets were considered, i.e. 1.9 MB, 30.1 MB and 66.7 MB. For each
of these data sets, several ratio values were deﬁned to generate diﬀerent number of seed points for the streamlines to
be rendered: 300, 150, 110, 50 and 30. So for both workﬂows, it was expected that the longest execution time would
be for the 66.7 MB data set with the ratio of 30 (i.e., every 30th point in a data set will be used as a seed point).
Firstly, because larger ﬁles take longer to process (read, calculate streamlines and render) and secondly because the
number of streamline increases with the low ratio value resulting in the bigger output ﬁle generated by the StreamVTK
service. The last aﬀects the execution time of the RenderVTK service, since it will have to render a much more detailed
representation of the blood ﬂow ﬁeld.
Additionally, all services used in both workﬂows were hosted on diﬀerent machines. These machines have an Intel
Dual-Core E2180 CPU at 2GHz and 2 GB of RAM, while the GridFTP server has an Intel Quad-Core E5450 CPU at
3GHz and 8GB of RAM. Furthermore, all the machines are part of the same network. This however did not aﬀect the
nature of measurements, as both workﬂows were compared under the same circumstances, and the main of focus was

1734

S. Koulouzis et al. / Procedia Computer Science 1 (2012) 1727–1736
S. Koulouzis, E.V. Zudilova-Seinstra, A.S.Z. Belloum / Procedia Computer Science 00 (2010) 1–10

8

Figure 8: Rendered artery and ﬂow ﬁeld data. Starting from the top left, the streamlines ratios used are 300, 150, 110, 50, and 30, while the last
image (bottom right), shows the artery with a ratio of 2.

Figure 9: Workﬂow execution times for the GridFTP and ProxyWS workﬂows.

the diﬀerences in workﬂow implementation, as well as the data ﬂow models used.
9. Results and Discussion
Figure 8 shows the diﬀerent rendered images obtained for ratios speciﬁed for the 66.7 MB ﬂow ﬁeld data set. These
results show that as the ratio value decreases the number of visualized streamlines increases, which also implies the
increase in computation and storage demands.
Figure 9 shows the results obtained for the centralized and distributed data workﬂows when executed with the data
sets and conﬁguration mentioned in Section 8. From these results, we conclude that by transferring data directly to the
consuming service execution time can be shortened. This is especially true, as data and computations become more
intensive. So in the case when the ﬂow ﬁeld data set is 66.7 MB and the streamlines ratio is 30, the time diﬀerence
between the two methods (GridFTP and ProxyWS) is about 35.14 sec.
Providing a more detailed look at the previous results, Figure 10 shows a breakdown of the total execution time
for both workﬂows for the 66.7 MB data set (the left part represents the time break down for the ProxyWS workﬂow,
while the right part for the GridFTP workﬂow). To obtain this graph, we measured the individual steps of the two
workﬂows. The total execution time is broken down as follows: i) Read time, representing the time each workﬂow
needs to access the data repository and to read the raw data (experimental and/or simulated). ii) Process time, which

1735

S. Koulouzis et al. / Procedia Computer Science 1 (2012) 1727–1736
S. Koulouzis, E.V. Zudilova-Seinstra, A.S.Z. Belloum / Procedia Computer Science 00 (2010) 1–10

9

Figure 10: Breakdown of execution time in read, process, render and postage time, for both workﬂows while visualizing the 66.7 MB data set. The
left part of the graph shows the time breakdown for the ProxyWS workﬂow while the right part shows the GridFTP

is the time it takes to get the data (ﬂow ﬁeld) from the previous step and calculate streamlines for it. iii) Render time,
representing the time it takes for each data set to be converted into an image, and ﬁnally iv) Postage time, which is the
time it takes for the image to be transferred to the user.
Apart from the read time, which is almost the same for both workﬂows, the two other metrics (process and postage
times), show a clear advantage when considering the ProxyWS workﬂow. This is because for the GridFTP workﬂow
all services need to upload their results back to the GridFTP server, before the workﬂow can proceed to the next call.
Another interesting ﬁnding shown in Figure 10 is the tendency of rendering time to increase non-linearly depending
on processing intensity (i.e., more streamlines). This observation reveals potential scaling problems, but the nature of
SOA should make it easy to cope with this problem. More speciﬁcally, we can assign more resources to rendering
services allow the architecture to scale better. So, for example, the rendering service may be the front-end of a
multiprocessor system. This shift will not disrupt the workﬂow itself, since SOA separates the implementation of a
service from its access interface (WSDL).
10. Conclusions and Future Work
Traditional visualization models do not fully exploit features and beneﬁts of distributed computing oriented mostly
towards SOA. Having this in mind, we took advantage of the beneﬁts that the SOA model provides to construct
distributed visualization pipelines.
The image-based analysis of vascular disorders served as a case study for our research. We experimented with
two diﬀerent types of medical data: the experimental data of the patient’s vascular condition and the simulated blood
ﬂow data, which can be used to predict the outcome of a vascular reconstruction procedure.
We used service orchestration for the ﬂow control, which allows interactive user steering over distributed visualization services via workﬂow management systems. We compared two data ﬂow models: a centralized, where data
are located in a centralized data repository and a distributed where data are delivered directly from producing to consuming services. For distributed data, the ﬂow model provided by the ProxyWS was able to speed up the workﬂow
execution, while visualization services could remain independent from a speciﬁc data infrastructure, which increases
their portability. In the case when data had to be accessed and/or archived from remote data repositories, the ProxyWS
provides a uniform and simple way to read data from such locations via the VRS.

1736

S. Koulouzis et al. / Procedia Computer Science 1 (2012) 1727–1736
S. Koulouzis, E.V. Zudilova-Seinstra, A.S.Z. Belloum / Procedia Computer Science 00 (2010) 1–10

10

To be able to even further speed up the execution of visualization pipelines, it would be desirable to be able to directly connect the visualization services with data streams. This could be achieved with the use of the vtkSocketCom
municator. This approach however might be diﬃcult to implement due to ﬁrewall restrictions. Additionally having
identiﬁed a potential scaling problem for rendering, it would be desirable to parallelize this process. The last will not
aﬀect the current workﬂow implementation, as the service interface (WSDL) will remain unchanged.
References
[1] J. Gray, Jim Gray on eScience: A Transformed Scientiﬁc Method (2007).
URL http://research.microsoft.com/en-us/collaboration/fourthparadigm/4th paradigm book jim gray transcript.pdf
[2] P. Sloot, G. van Albada, E. Zudilova, P. Heinzlreiter, D. Kranzlm¨uller, H. Rosmanith, J. Volkert, Grid-based interactive visualisation of
medical images, in: S. Nrager (Ed.), Proceedings of the First European HealthGrid Conference, January, 16th-17th, 2003, Commission of the
European Communities, Information Society Directorate-General, Brussels, Belgium, 2003, pp. 57 – 66.
[3] J. Zhang, I. Altintas, J. Tao, X. Liu, D. D. Pennington, W. K. Michener, Integrating data grid and web services for e-science applications: A
case study of exploring species distributions, e-science 0 (2006) 31.
[4] E. V. Zudilova-Seinstra, N. Yang, L. Axner, A. Wibisono, D. Vasunin, Service-oriented visualization applied to medical data analysis, Service
Oriented Computing and Applications 2 (4) (2008) 187–201.
[5] L. Axner, A. Hoekstra, P. Sloot, Simulating time harmonic ﬂows with the lattice boltzmann method, Phys. Rev. E (2007) 036709.
[6] E. Zudilova, P. Sloot, Bringing combined interaction to a problem solving environment for vascular reconstruction, Int. J. Future Generation
Computer Systems 21 (7) (2005) 1167–1176.
[7] C. Kesselman, I. Foster, The Grid: Blueprint for a New Computing Infrastructure, Morgan Kaufmann Publishers.
[8] I. Foster, C. Kesselman, J. Nick, S. Tuecke, The physiology of the grid: An open grid services architecture for distributed systems integration.
[9] M. D. Stefano, Distributed Data Management for Grid Computing, Wiley-Interscience, 2005.
[10] J. Stanton, S. Newhouse, J. Darlington, Implementing a scientiﬁc visualisation capability within a grid enabled component framework (research note), in: Euro-Par ’02: Proceedings of the 8th International Euro-Par Conference on Parallel Processing, Springer-Verlag, London,
UK, 2002, pp. 885–888.
[11] K. Brodlie, D. Duce, M. Sagar, J. Wood, gviz: Visualization and computational steering on the grid, in: Proceedings of the UK e-Science All
Hands Conference 2004, Engineering and Physical Sciences Research Council, London, UK, 2004, pp. 54–61.
[12] IRIS Explorer, http://www.nag.co.uk/Welcome IEC.html.
[13] P. Heinzlreiter, A. Wasserbauer, H.-P. Baumgartner, D. Kranzlmller, G. Kurka, J. Volkert, Interactive virtual reality volume visualization on
the grid, in: Proceedings of DAPSYS 2002, ELinz, Austria, 2002, pp. 90–97.
[14] A. Tirado-Ramos, J. Ragas, D. Shamonin, H. Rosmanith, D. Kranzlm¨uller, Integration of blood ﬂow visualization on the grid: the ﬂowﬁsh/gvk
approach, in: M. Dikaiakos (Ed.), Grid Computing: Proceedings of the 2nd European AcrossGrids Conference, Nicosia, Cyprus, Vol. 3165
of Lecture Notes in Computer Science, Springer Verlag, Nicosia, Cyprus, 2004, pp. 77–79.
[15] C. Upson, T. A. Faulhaber, D. Kamins, D. Laidlaw, D. Schlegel, J. Vroom, R. Gurwitz, A. van Dam, The application visualization system: a computational environment for scientiﬁc visualization, Computer Graphics and Applications, IEEE 9 (4) (1989) 30–42.
doi:10.1109/38.31462.
URL http://dx.doi.org/10.1109/38.31462
[16] R. Haber, D. A. McNabb, Visualization idioms: A conceptual model for scientiﬁc visualization systems, in: Visualization in Scientiﬁc
Computing, 1990.
[17] M. Tory, T. Moller, Rethinking visualization: A high-level taxonomy, pp. 151–158.
[18] E. H. Chi, A taxonomy of visualization techniques using the data state reference model, Information Visualization, IEEE Symposium on 0
(2000) 69.
[19] D. A. Keim, Visual exploration of large data sets, Commun. ACM 44 (8) (2001) 38–44.
[20] S. M. Charters, N. S. Holliman, M. Munro, Visualisation in e-demand: A grid service architecture for stereoscopic visualisation, in: Proceedings of UK e-Science Second All Hands Meeting, 2003.
[21] E. N. Houstis, J. R. Rice, E. Gallopoulos, R. Bramley, Enabling Technologies for Computational Science, Kluwer, Boston, 2000.
[22] J. Montagnat, F. Bellet, H. Benoit-cattin, V. Breton, L. Brunie, Y. Legr, I. E. Magnin, L. Maigne, S. Miguet, J. m. Pierson, L. Seitz, T. Tweed,
Medical images simulation, storage, and processing on the european datagrid testbed.
[23] Hao He, What is service-oriented architecture., http://webservices.xml.com/pub/a/ws/2003/09/30/soa.html (September 2003).
[24] C. Stuart M, H. Nicolas S, M. Munro, Visualisation on the grid: A web service approach, in: Proceedings of the UK e-Science All Hands
Conference 2004, Engineering and Physical Sciences Research Council, 2004, pp. 202 – 210.
[25] J. Raskin, The Humane Interface: New Directions for Designing Interactive Systems, Addison-Wesley Professional.
[26] U. Fayyad, G. Grinstein, A. Wierse, Information Visualisation in Data Mining and Knowledge Discovery, Morgan Kaufmann, San Francisco,
2002.
[27] Three web services recommendations., Tech. rep., W3C, http://www.w3.org/2005/01/xmlp-pressrelease.html (2009).
[28] J. Bresnahan, M. Link, G. Khanna, Z. Imani, R. Kettimuthu, I. Foster, Globus GridFTP: what’s new in 2007, in: GridNets ’07, 2007.
[29] J.-P. Baud, J. Casey, S. Lemaitre, C. Nicholson, Performance analysis of a ﬁle catalog for the lhc computing grid, High-Performance Distributed Computing, International Symposium on 0 (2005) 91–99.
[30] A. Shoshani, A. Sim, J. Gu, Storage resource managers: Middleware components for grid storage.
[31] VBrowser web site., http://www.vl-e.nl/vbrowser.

