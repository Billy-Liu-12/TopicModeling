Procedia Computer
Science
Procedia Computer Science 00 (2010) 1–6
Procedia
Computer Science 1 (2012) 2391–2396

www.elsevier.com/locate/procedia

International Conference on Computational Science, ICCS 2010

A new training method for sequence data
Lingfeng Niua,b , Yong Shia,b,c,∗
a Research

Center on Fictitious Economy and Data Science, CAS, Beijing 100190, China
University of Chinese Academy of Sciences, Beijing 100190, China
c College of Information Science and Technology, University of Nebraska at Omaha, Omaha, NE 68182, USA
b Graduate

Abstract
Under the framework of max margin method, this work proposes a model for training sequence data, which can
be solved as a binary classiﬁcation. However, there are too many samples in the auxiliary classiﬁcation problem
to make the model eﬃcient enough for median to large scale data sets in practice. Therefore, under the additive
assumption for the feature mapping and loss function, a simpliﬁed model is introduced in order to speed up training.
The major advantage of our method is that the new model does not share slack variable for a sequence. This provides
the ability to utilize the discriminate information within the sequence and select the discriminative patterns more
precisely. Experiment on the task of named entity recognition validates the eﬀectiveness of the new method.

c 2012 Published by Elsevier Ltd. Open access under CC BY-NC-ND license.
⃝

Keywords: max margin method, sequence data, structure learning

1. Introduction
Data mining techniques are widely used in many ﬁelds of quantitative management such as ﬁnancial risk analysis
[27, 24], business intelligence [23], etc. Most of the solutions and algorithms for the real world data mining problems
rely heavily on the development of optimization, particularly the linear, quadratic and multiple criteria programming
techniques [19, 16, 32, 34, 35, 36]. A survey was conducted to understand the most challenge problems in data mining
[33]. Sequence data mining is shown to be the third of 10 most challenge problems in data mining, which therefore
draws much attention of researchers in the data mining community.
Supervised learning for sequence data plays important roles in the wide applications of part-of-speech tagging,
online optical character recognition, named entity recognition, etc. A straightforward way for labeling sequence data
is to predict the label of each node in the sequence independently and then concatenate them together to form the
whole label of the sequence. However, this method does not take the advantages of the dependency of the labels
hidden in the sequence which are valuable for the accuracy improvement of sequence classiﬁcation [1, 15]. Recent
years, several models are proposed to address this defection [1, 2, 15, 22, 25].
Hidden Markov Model(HMM) is the widely used probabilistic model to capture the label transition information
in the sequence [1]. It assigns a joint probability to the paired input and label sequence. Given sequence data and the
label, the process of training HMMs is to maximize the joint likelihood of the training samples. Strict local dependency
∗ Corresponding

author
Email addresses: niulf@lsec.cc.ac.cn (Lingfeng Niu), yshi@gucas.ac.cn (Yong Shi)

c 2012 Published by Elsevier Ltd. Open access under CC BY-NC-ND license.
1877-0509 ⃝
doi:10.1016/j.procs.2010.04.269

2392

L. Niua, Y. Shi / Procedia Computer Science 1 (2012) 2391–2396
Lingfeng Niu, Yong Shi / Procedia Computer Science 00 (2010) 1–6

2

is assumed for input feature and label sequence to make the training and inference trackable. Predicting for new input
is to seek the consistent label sequence that has maximum joint probability. As a generative model, because local
dependency of input sequence is assumed, the correlation of the input features that can be used is limited. For
classiﬁcation task, it is believed that discriminative models perform better than generative models because the long
range correlation of the input features can be utilized [15, 22].
Conditional Random Field(CRF) proposed in [15] learns a discriminative model by focusing on the conditional
probability of label sequence given the input. CRFs are trained by maximizing likelihood or a posteriori of the
training data with exponential family distribution. Compared with HMMs, CRFs only assume the local dependency
constraints for the distribution of label sequence described by graphical model [5]. Experiment results in [15] show
that CRFs have better accuracy for the classiﬁcation task than HMMs. However, CRFs do not show the same level of
generalization as the max-margin methods.
The most successful model utilizing the max-margin scheme is Support Vector Machine(SVM) [6, 9, 29, 30].
Originally, SVM is designed for binary classiﬁcation and then extended to other problems such as multi-class classiﬁcation and regression [10, 20, 8]. Nowadays, it becomes one of the state-of-the-art classiﬁers and several eﬃcient
training algorithms and solvers have been developed [17, 18, 13, 7, 11]. Recently, several research works try to extend
SVMs to the sequence and, more generally, structural learning problem [2, 28, 25, 4]. In [2, 28], with a joint deﬁnition
of the feature mapping of input and label sequence, the generalization information in input space as well as the structural label space are utilized. In [25], the author proposed the Maximum Margin Markov Networks(M3 N) method,
which uniﬁes the probabilistic framework and the max-margin method with Markov Networks graphical model. The
Conditional Graphical Model proposed in [4] decomposes the training of sequence model to several multi-class SVMs
by reformulating the loss function.
We notice that for the models in [2, 28, 25], each sequence has only one slack variable. This modeling way
ignores part of the discriminate information within the sequence. The similar phenomenon is observed in [4] as well.
In order to utilize the local information provided by the elements of the sequence, we propose a new model under the
framework of max margin method. In this model, instead of all the elements in one sequence share the same slack
variable, each constraint has its own slack variable. The model proposed in [4] is also under the non-shared slack
variable conﬁguration. However, it only can handle the data sets in which each sequence has the same length. For the
more general data sets, special technique, which increases the complexity of the training algorithm, must be taken to
make the model can be used. The new model we proposed in this work does not suﬀer this problem.
The structure of the rest of this paper is organized as follows: section 2 introduces the new sequence model and
proposes the training algorithm at the same time. Implementation details and experiment results are listed in the third
section. At last, we give a short conclusion and point out some possible future research directions.
2. Sequence model and the new training method
D = {(xi , yi )|i = 1, · · · , n} is a set of data, where xi ∈ X is input and yi ∈ Y is the corresponding label. For
the sequence data, the i-th sample consists of Li parts in sequence, i.e. xi = (xi,1 , · · · , xi,Li ) and yi = (yi,1 , · · · , yi,Li ).
Usually, diﬀerent sample has diﬀerent length. In this work, for the simplicity of the representation, we assume that
the set of possible labels for all the samples in diﬀerent position is the same and denote it as Σ. Hence, the label space
for the i-th sequence is ΣLi , which grows exponentially with the length Li .
To explore the dependence in the label space, the feature vector is jointly deﬁned over input and output as Φ(x, y)
in [28]. The linear discriminant function over the feature representation can be stated as:
F(x, y; w) = wT Φ(x, y),

(1)

which can be viewed as a criteria to measure how consistent it is to assign x with label y. Usually the value of
F(x, y; w) is called the conﬁdence score of assigning input x to label sequence y. Then when a new sample x comes,
the predicted label of x is the one with the maximum consistency, i.e.
y¯ = argmax F(x, y; w).
y∈Y

(2)

L. Niua, Y. Shi / Procedia Computer Science 1 (2012) 2391–2396
Lingfeng Niu, Yong Shi / Procedia Computer Science 00 (2010) 1–6

2393
3

We want to select the w for which the sum of diﬀerences between the score of the correct label yi and other labels
y is maximal. For this aim, the following quadratic programming(QP) model can be constructed:
1
2

min
w,ξ

w

2

n
i=1

+C

(3a)

y∈Y ξi,y

s.t. wT (Φ(xi , yi ) − Φ(xi , y)) ≥ L(yi , y) − ξi,y , ∀i = 1, · · · , n, ∀y ∈ Y,
ξi,y ≥ 0,
∀i = 1, · · · , n, ∀y ∈ Y\yi ,

(3b)
(3c)

where L(yi , y) is the loss function which measures the loss of predicting yi as y, ξ is the slack variable vector. We can
see that diﬀerent constraint has diﬀerent slack variable.
Model (3) can be trained as a binary classiﬁcation. In details, ∀i ∈ {1, · · · , n}, ∀y ∈ Y/yi , we construct either
a positive sample Φ(xi , yi ) − Φ(xi , y) or negative sample Φ(xi , y) − Φ(xi , yi ) randomly with equal probability. Then
problem (3) is just the SVM model for binary classiﬁcation with these ni=1 (|Σ|Li − 1) auxiliary samples. One thing we
want to stress is that this approach of training models is not new. Similar idea is also used in [14]. Since the number
of samples grows exponentially with the length of sequence, when the sequence is long, there are too many samples
to be manipulated in practice. Therefore, we will discuss how to reduce problem (3) into a simpliﬁed model to save
the training time in the rest of this section.
For all i = 1, · · · , n, we represents the sequential graph for sample (x, y) as G = (V, E), where V = {yt |t = 1, · · · , L}
is the set of vertices, E = {(t, t + 1)|t = 1, · · · , L − 1} is the set of edges. Assume that all edges in the graph denote the
same type of interaction, the joint feature mapping Φ(·, ·) and loss function L(·, ·) can be decomposed into a sum over
the edges, i.e.
Φ(xi , y) =
e∈Ei

L(yi , y) =
e∈Ei

φe (xi , y), ∀y ∈ Y,

(4a)

le (yi , y), ∀y ∈ Y,

(4b)

where e = (t, t + 1), φe (·, ·) and le (·, ·) are the local joint feature mapping and loss at edge e, respectively. When only
the dependency between the adjacent vertices is considered, the following QP model can be constructed
min
w,ξ

s.t.

1
2

w

2

n
i=1

+C

y∈Y

e∈Ei

(5a)

ξi,y,e

wT (φe (xi , yi ) − φe (xi , y)) ≥ le (yi , y) − ξi,y,e , ∀i = 1, · · · , n; y ∈ Y; e ∈ Ei ,
ξi,y,e ≥ 0,
∀i = 1, · · · , n, y ∈ Y\yi , e ∈ Ei .

(5b)
(5c)

Similar to the discussion following model (3), the above problem can also be solved as a binary classiﬁcation. In this
case, the number of auxiliary samples becomes ni=1 (Li − 1)|Σ|2 , which just increases linearly with the length of the
sequence. Therefore, the speed of training model (5) is much faster than the training speed for model (3).
Now we consider the relationship between the optimal solution of problem (3) and problem (5). Suppose (w∗ , ξ∗ )
is the optimal solution for (5), deﬁne
w¯

w∗ ,

ξ¯i,y
e∈Ei

(6)
∗
ξi,y,e
, ∀i = 1, · · · , n, y ∈ Y.

Based on the additive properties in (4), we have
w¯ T (Φ(xi , yi ) − Φ(xi , y))

=
e∈Ei

≥
=

e∈Ei

w∗T (φe (xi , yi ) − φe (xi , y))
∗
(le (yi , y) − ξi,y,e
)

L(yi , y) − ξ¯i,y .

(7)

2394

L. Niua, Y. Shi / Procedia Computer Science 1 (2012) 2391–2396
Lingfeng Niu, Yong Shi / Procedia Computer Science 00 (2010) 1–6

4

¯ is a feasible solution for (3). Although, (w,
¯ is not an optima for (3), we still think using w¯ in the
Obviously, (w,
¯ ξ)
¯ ξ)
discriminate function makes sense for lots of applications. This is because the edge level discriminative information
in the sequence, which may be the most prominent structure, is utilized. In fact, from the experiment result in the next
section, we can see that when predicting with function
¯ = F(x, y; w∗ ) = w∗T Φ(x, y),
F(x, y; w)
satisﬁable classiﬁcation accuracy can be obtained for the named entity recognition task we test.
3. Experiments
The eﬀectiveness of the new method is tested in this section. The baseline is the CRF model, which is the most
widely used model for sequence labeling currently. The implementation we used is the Stanford CRF package [12].
The source code can be obtained at http://nlp.stanford.edu/software/CRF-NER.shtml. Thanks very much for the
authors making the package public available.
We choose a data set called WSJ [31] for the experiment, which contains 48972 sentences from the news articles of
Wall Street Journal. The label for each sample indicates whether the words or phrases in the sentence is a named entity
or not. The ﬁrst ten sections of data set WSJ are used in the following experiment. The three ﬁrst level categories
PERSON, LOCATION and ORGANIZATION in the label set are selected and all the other categories are denoted as
MISC. Therefore, there are four possible classes for each node in the sequence data. In all, the selected part contains
19810 sentences and the average length is 24 words. The number of words labeled as PERSON, LOCATION and
ORGANIZATION are 5978, 423 and 12215, respectively.
Deﬁnition and selection of features are critical for the performance of named entity recognition [12]. In the
following experiment, a commonly used set of features for word is calculated by the Stanford CRF package [12] and
260580 features are generated. The same set of features is used in both the CRF model and our new method. The
features for two adjacent words in sample (x, y) can be constructed as
φe (x, y) = (ψ(xt ) ⊗ Λ(yt ) + ψ(xt+1 ) ⊗ Λ(yt+1 ))

β(Λ(yt ) ⊗ Λ(yt+1 )),

(8)

where e = (t, t + 1), ψ(xt ) is the feature vector for word at position t, ⊗ is the Kroneck product, Λ(yt ) is the canonical(binary) representation of label yt in the label set Σ, denotes the concatenation of two vectors, β is a parameter to
balance the trade-oﬀ between two types of features, we use β = 1 in the following experiment. Then Φ(x, y) can be
calculated by (4a). We use Hamming distance as the Loss function. For this case, (4b) is satisﬁed naturally.
The binary classiﬁcation problem constructed from (5) is solved by the famous linear SVM solver LibLinear1
[11]. From the additive assumption in (4a), we have
w∗T Φ(x, y) =

w∗T φe (x, y),
e∈E x

∗

where w is the optimal w for (5).Furthermore, the label y∗ with maximum value, which is the label predicted for input
x, can be computed by the Viterbi decoding method [1, 15].
For the CRF model, the default regularization arguments in the Stanford CRF package(a Gaussian prior with
variance 1 and mean 0) are used. For our model, we set the regularization argument C = 5. Because the arguments for
the two models are not comparable, other possibilities of the choice are not explored in this section. We use precision
P, recall R and F-score F-1 = 2PR/(P + R) to evaluate the performance of diﬀerent models. These measurements are
widely used in Information Retrieval area [3].
Because the new model can utilize the discriminative information in the sequence precisely, we believe that reliable
sequence classiﬁer can be learned even with small training data set. In this experiment, we use one section of the
data set for training model and the remaining nine sections for testing in each round. An entity is considered to be
recognized correctly if and only if both the boundary and label predicted match with the labeled data. Table 1 shows
1 LibLinear

is available at http://www.csie.ntu.edu.tw/∼cjlin/liblinear/

L. Niua, Y. Shi / Procedia Computer Science 1 (2012) 2391–2396
Lingfeng Niu, Yong Shi / Procedia Computer Science 00 (2010) 1–6

2395
5

Table 1: Performance comparison on Named Entity Recognition
Entity
Person
Org.
Loc.

P
0.9405
±0.0003
0.8939
±0.0003
0.9601
±0.0045

CRF
R
0.7841
±0.0009
0.6906
±0.0022
0.1405
±0.0063

F-1
0.8547
±0.0002
0.7780
±0.0007
0.2358
±0.0152

P
0.9217
±0.0003
0.9160
±0.0003
0.9001
±0.0056

New Method
R
F-1
0.8406
0.8789
±0.0005 ±0.0001
0.7449
0.8205
±0.0024 ±0.0007
0.3278
0.4737
±0.0067 ±0.0073

the average performance of the ten sections. Column “New Method” and “CRF” list the results for our new method
and CRF model, respectively. The best results are marked in bold font. The variance of the ten sections is also given
at the same time. From these results we can see that our method performs better than CRF on all the three types of
entities considering F-1. F-1 is the harmonic average of precision and recall, it thus provides us a more reliable and
consistent criteria for the model performance. Particularly, on the recognition of LOCATION, the gain of our solution
is much more obvious by improving the F-1 measure from 23.6% to 47.4%. This is because there are only 423
LOCATION entities, which is much less than the number of entity PERSON or ORGANIZATION. This phenomena
shows that our new method can explore the discriminative information in training better than CRF model, especially
for the case that the size of training data is small.
4. Conclusion and future works
We proposed an eﬀective method for training sequence data in this paper. The new model can be solved by a
binary classiﬁcation with auxiliary samples constructed from the training data. When the additive assumption for the
joint feature mapping and loss function holds, a reduced model which results in much less auxiliary training samples
than the original model is introduced. The major advantage of our method is that the new model does not share slack
variable for a sequence. This provides the ability to select the discriminative patterns more precisely. Experiment on
the task of names entity recognition shows that the proposed method performs better than CRF model.
The idea in this paper can be extended to second order Markov dependency model by deﬁning the local feature
mapping considering three continuous nodes in the sequence. In this case, a binary classiﬁcation with ni=1 (Li − 1)|Σ|3
training samples need to be solved. The prediction of new sequence can be computed by the corresponding improved
Viterbi decoding in this case. We can also solve the maximization problem for predicting approximately by the general
Loopy Belief Propagation(LBP) [5]. Furthermore, if proper local feature mapping for more nodes can be deﬁned, we
can also extend the new method to higher order Markov dependency models. LBP again can be used for predicting.
However, how to balance the trade-oﬀ between using high order models and the diﬃculty for inference in prediction
need to be considered carefully.
Another interesting topic is to explore the extension of our method to the tree structured prediction task [28, 21] or,
more general, the graphical structured prediction problem [26]. If the additive assumption in (4) also holds [26, 21],
the model can still be solved as a binary classiﬁcation problem. The challenge is to ﬁgure out a proper way to reduce
the model and make the number of samples trackable for training.
Acknowledgement
This work was partially supported by the National Natural Science Foundation of China (Grant No.70621001,
70531040, 70840010, 70921061), and the BHP Billiton Cooperation of Australia.
References
[1] E. Alpaydin. Introduction to Machine Learning. The MIT Press, 2004.

2396

L. Niua, Y. Shi / Procedia Computer Science 1 (2012) 2391–2396
Lingfeng Niu, Yong Shi / Procedia Computer Science 00 (2010) 1–6

6

[2] Y. Altun, I. Tsochantaridis, and T. Hofmann. Hidden markov support vector machines. In T. Fawcett, N. Mishra, T. Fawcett, and N. Mishra,
editors, ICML, pages 3–10. AAAI Press, 2003.
[3] R. A. Baeza-Yates and B. A. Ribeiro-Neto. Modern Information Retrieval. ACM Press / Addison-Wesley, 1999.
[4] G. H. Bakir, T. Hofmann, B. Sch¨olkopf, A. J. Smola, B. Taskar, and S. V. N. Vishwanathan. Predicting Structured Data (Neural Information
Processing). The MIT Press, 2007.
[5] C. M. Bishop. Pattern Recognition and Machine Learning. Springer, 2006.
[6] B. E. Boser, I. M. Guyon, and V. N. Vapnik. A training algorithm for optimal margin classiﬁers. In Proceedings of the 5th Annual ACM
Workshop on Computational Learning Theory, pages 144–152. ACM Press, 1992.
[7] C.-C. Chang and C.-J. Lin. LIBSVM: a library for support vector machines. http://www.csie.ntu.edu.tw/∼cjlin/libsvm/, 2001.
[8] R. Collobert and S. Bengio and C. Williamson. SVMTorch: Support vector machines for large-scale regression problems. Journal of Machine
Learning Research, 1:143–160, 2001.
[9] C. Cortes and V. Vapnik. Support-vector networks. Machine Learning, 20(3):273–297, 1995.
[10] K. Crammer and Y. Singer. On the algorithmic implementation of multiclass kernel-based vector machines. Journal of Machine Learning
Research, 2:265–292, 2002.
[11] R.-E. Fan, K.-W. Chang, C.-J. Hsieh, X.-R. Wang, and C.-J. Lin. LIBLINEAR: A library for large linear classiﬁcation. Journal of Machine
Learning Research, 9:1871–1874, 2008.
[12] J. R. Finkel, T. Grenager, and C. Manning. Incorporating non-local information into information extraction systems by gibbs sampling. In
ACL ’05: Proceedings of the 43rd Annual Meeting on Association for Computational Linguistics, pages 363–370, Morristown, NJ, USA,
2005. Association for Computational Linguistics.
[13] T. Joachims. Making large-scale support vector machine learning practical. In A. S. B. Sch¨olkopf, C. Burges, editor, Advances in Kernel
Methods: Support Vector Machines. 1998.
[14] T. Joachims. Optimizing search engines using clickthrough data. In KDD’02: : Proceeding of the 8th ACM SIGKDD international conference
on Knowledge discovery and data mining, pages 133–142, New York, NY, USA, 2002. ACM.
[15] J. Laﬀerty, A. McCallum, and F. Pereira. Conditional random ﬁelds: Probabilistic models for segmenting and labeling sequence data, 2001.
[16] Jianping Li, Zhenyu Chen, Liwei Wei, Weixuan Xu and Gang Kou. Feature selection via least squares support feature machine. International
Journal of Information Technology & Decision Making, 6:671–686, 2007.
[17] E. Osuna, R. Freund, and F. Girosi. Training support vector machines:an application to face detection, 1997.
[18] J. Platt. Sequential minimal optimization: A fast algorithm for training support vector machines, 1998.
[19] Kristin P. Bennett and Emilio Parrado-Hern´andez. The Interplay of Optimization and Machine Learning Research. Journal of Machine
Learning Research, 7:1265–1281, 2006.
[20] R. Rifkin and A. Klautau. In defense of one-vs-all classiﬁcation. Journal of Machine Learning Research, 5:101–141, January 2004.
[21] J. Rousu, C. Saunders, S. Szedmak, and J. Shawe-Taylor. Kernel-based learning of hierarchical multilabel classiﬁcation models. Journal of
Machine Learning Research, 7:1601–1626, 2006.
[22] F. Sha and F. Pereira. Shallow parsing with conditional random ﬁelds. In NAACL ’03, pages 134–141, Morristown, NJ, USA, 2003.
Association for Computational Linguistics.
[23] Yong Shi, Gang Kou and Zhengxin Chen. Classifying credit card accounts for business intelligence and decision making: a multiple-criteria
quadratic programming approach. International Journal of Information Technology & Decision Making, 4:581–599, 2005.
[24] Chien-Wen Shen. A bayesian networks approach to modeling ﬁnancial risks of e-logistics investments. International Journal of Information
Technology & Decision Making, 8:711–726, 2009.
[25] B. Taskar, C. Guestrin, and D. Koller. Max-margin markov networks. In Advances in Neural Information Processing Systems (NIPS),
Vancouver, Canada, December 2003.
[26] B. Taskar, S. Lacoste-Julien, and M. I. Jordan. Structured prediction, dual extragradient and bregman projections. Journal of Machine
Learning Research, 7:1627–1653, 2006.
[27] K.-J. Tseng, Y.-H. Liu and Jow-Fei Ho. An eﬃcient algorithm for solving a quadratic programming model with application in credit card
holders’ behavior. International Journal of Information Technology & Decision Making, 7:421–430, 2008.
[28] I. Tsochantaridis, T. Hofmann, T. Joachims, and Y. Altun. Support vector machine learning for interdependent and structured output spaces.
[29] V. N. Vapnik. The nature of statistical learning theory. Springer-Verlag New York, Inc., New York, NY, USA, 1995.
[30] V. N. Vapnik. Statistical Learning Theory. Wiley-Interscience, 1998.
[31] R. Weischedel and A. Brunstein. BBN Pronoun Coreference and Entity Type Corpus. Linguistic Data Consortium, Philadelphia, 2005.
[32] Fred W. Glover and Gary Kochenberger. New optimization models for data mining. International Journal of Information Technology &
Decision Making, 4:605–609, 2006.
[33] Qiang Yang and Xindong Wu. 10 challenging problems in data mining research. International Journal of Information Technology & Decision
Making, 5:597–604, 2006.
[34] P. Zhang, X. Zhu, Z. Zhang, and Y. Shi. Multiple Criteria Programming for VIP Email Behavior Analysis. Web Intelligence & Agent Systems,
8:No.1, 2008.
[35] P. Zhang, Y. Tian, Z. Zhang, X. Li, and Y. Shi. Supportive instances for Regularized multiple criteria linear programming. International
Journal of Operations & Quantitative Management, 4:249–263, 2008.
[36] P. Zhang, Z. Zhang, A. Li, and Y. Shi. Global and Local Bagging Approach for Classifying Noisy Dataset. International Journal of Software
and Informatics, 2:181-197, 2008.

