Procedia Computer Science
Volume 29, 2014, Pages 8–19
ICCS 2014. 14th International Conference on Computational Science

SparseHC: a memory-eﬃcient
online hierarchical clustering algorithm
Thuy-Diem Nguyen1 , Bertil Schmidt2 , and Chee-Keong Kwoh3
1

3

School of Computer Engineering, Nanyang Technological University, Singapore
thuy1@e.ntu.edu.sg
2
Institut f¨
ur Informatik, Johannes Gutenberg University, Mainz, Germany
bertil.schmidt@uni-mainz.de
School of Computer Engineering, Nanyang Technological University, Singapore
asckkwoh@ntu.edu.sg

Abstract
Computing a hierarchical clustering of objects from a pairwise distance matrix is an important algorithmic kernel in computational science. Since the storage of this matrix requires
quadratic space with respect to the number of objects, the design of memory-eﬃcient approaches
is of high importance to this research area. In this paper, we address this problem by presenting
a memory-eﬃcient online hierarchical clustering algorithm called SparseHC. SparseHC scans a
sorted and possibly sparse distance matrix chunk-by-chunk. Meanwhile, a dendrogram is built
by merging cluster pairs as and when the distance between them is determined to be the smallest among all remaining cluster pairs. The key insight used is that for ﬁnding the cluster pair
with the smallest distance, it is unnecessary to complete the computation of all cluster pairwise
distances. Partial information can be utilized to calculate a lower bound on cluster pairwise
distances that are subsequently used for cluster distance comparison. Our experimental results show that SparseHC achieves a linear empirical memory complexity, which is a signiﬁcant
improvement compared to existing algorithms.
Keywords: hierarchical clustering, memory-eﬃcient clustering, sparse matrix, online algorithms

1

Introduction

Clustering is an important unsupervised machine learning technique to group similar objects in
order to uncover the inherent structure of a given dataset. Depending on the output, clustering
algorithms are broadly divided into two main categories: hierarchical clustering and partitional
(or ﬂat) clustering [2, 12, 26]. The structured output produced by hierarchical clustering algorithms is often more informative than the unstructured set of clusters returned by partitional
clustering algorithms [16, 24]. Thus, hierarchical clustering is a crucial data analysis tool in
many ﬁelds including computational biology and social sciences [9]. Nonetheless, the quadratic
time and especially the quadratic memory complexity have limited the use of hierarchical clustering software to rather small datasets [24]. Since many areas of computational science face a
8

Selection and peer-review under responsibility of the Scientiﬁc Programme Committee of ICCS 2014
c The Authors. Published by Elsevier B.V.
doi:10.1016/j.procs.2014.05.001

SparseHC: an online hierarchical clustering algorithm

Nguyen, Schmidt and Kwoh

Table 1: The parameters of the Lance-Williams recurrence formula for 7 popular linkage schemes
Linkage
Single

α1
0.5

α2
0.5

β
0

γ
-0.5

Alternative formula
dij =
min dxy

Complete

0.5

0.5

0

0.5

dij =

|Ci |
|Ci |+|Cj |

|Cj |
|Ci |+|Cj |

Average
Weighted
Centroid
Median
Ward

0.5

0.5

|Ci |
|Ci |+|Cj |

|Cj |
|Ci |+|Cj |

0.5

0.5

|Ci |+|Cm |
|Ci |+|Cj |+|Cm |

|Cj |+|Cm |
|Ci |+|Cj |+|Cm |

0
0
|Ci ||Cj |
− (|C |+|C
2
i
j |)
-0.25
|Cm |
− |Ci |+|C
j |+|Cm |

0

x∈Ci ,y∈Cj

dij

max

x∈Ci ,y∈Cj

1
=
|Ci ||Cj |

dxy
dxy

x∈Ci ,y∈Cj

0
0
0
0

data explosion, addressing the problem of computing a hierarchical clustering from a large and
possibly sparse pairwise distance matrix in a memory-eﬃcient way is becoming increasingly
important. In this paper, we tackle this problem by presenting a new general-purpose online
hierarchical clustering algorithm called SparseHC.
Hierarchical clustering can be divided into two categories: the agglomerative “bottom-up”
approach and the divisive “top-down” approach [24]. We focus on the former category: agglomerative hierarchical clustering (AHC). AHC algorithms can be characterized as sequential,
agglomerative, hierarchical, and non-overlapping [7, 23]. In AHC algorithms, objects or data
points are ﬁrst treated as singletons and subsequently merged one pair of clusters at a time
until there is only one cluster left. There are seven commonly used linkage schemes: single,
complete, average (UPGMA), weighted (WPGMA), centroid (UPGMC), median (WPGMC)
and Ward’s method. The properties of each scheme are discussed in [8]. The merging criteria
used by all these schemes can be neatly represented with the recurrence formula by Lance and
Williams [14]. Given that two clusters Ci and Cj have previously been merged into cluster Ck ,
the distance between cluster Ck and any unmerged cluster Cm is deﬁned as:
dkm = d(Ci ∪ Cj , Cm ) = α1 dim + α2 djm + βdij + γ|dim − djm |
The speciﬁc parameters for each scheme are deﬁned in Table 1.
Depending on the input data, AHC algorithms can be divided into the “stored data approach” and the “stored matrix approach” [1, 18]. The stored data approach requires the
recalculation of pairwise distance values for each merging step. Since only data points are
stored in the main memory, algorithms in this approach can achieve O(N ) space complexity
often at the expense of O(N 3 ) time complexity [19], where N is the number of input data
points. One notable algorithm in the stored data approach is the nearest- neighbor chain algorithm, which achieves O(N ) space complexity and O(N 2 ) time complexity for the Ward’s
method linkage scheme. However, this algorithm is not applicable to the centroid and median linkage schemes because these schemes do not fulﬁll the required reducibility criterion i.e.
d(Ci ∪ Cj , Cm ) ≥ min(d(Ci , Cm ), d(Cj , Cm )) [18, 19]. For the single-, complete- and averagelinkage schemes, this algorithm requires O(N 2 ) space and time complexity [10]. On the contrary, in the stored matrix approach an all-against-all pairwise distance matrix of size N 2 is
ﬁrst computed and then used for clustering. As a result, this approach requires O(N 2 ) time
and memory complexity [26].
To overcome the low memory eﬃciency of classical AHC algorithms, new techniques perform
either data reduction by random sampling (e.g. data sampling and partitioning in CURE [11])
9

SparseHC: an online hierarchical clustering algorithm

Nguyen, Schmidt and Kwoh

or data summarization by using a new data structure to represent the original data (e.g. the
CF tree in BIRCH [27]). Although these algorithms have linear memory complexity [26],
the dendrograms produced by these algorithms are indeterministic and are dissimilar those
produced by standard AHC tools because of the random procedures being used.
In this paper, we focus on reducing the primary memory consumption of the AHC stored
matrix approach. We introduce SparseHC, a general-purpose memory-eﬃcient AHC algorithm
for single-, complete- and average-linkage schemes. SparseHC is an online algorithm. Borodin
and El-Yaniv [5] deﬁned online algorithms as algorithms that focus on scenarios where “the
input is given one piece at a time and upon receiving an input, the algorithm must take an
irreversible action without the knowledge of future inputs”. Because online algorithms only
require partial input in the main memory for processing, they are often used to target problems
with high space complexity. To our knowledge, there are only a few existing online hierarchical
clustering algorithms for the stored matrix approach including MCUPGMA [15] for the average
scheme and ESPRIT hcluster [25] for single and complete schemes.
SparseHC employs a similar strategy as in MCUPGMA and hcluster where the input distance matrix is ﬁrst sorted and then processed in a chunk-by-chunk manner. SparseHC incorporates two new techniques in order to achieve signiﬁcantly better performance:
1. Compression of the information in the currently loaded chunk of the input matrix into
the most compact form.
2. Usage of an eﬃcient graph representation to store unmerged cluster connections, which
allows constant access to these connections for faster speed.

2

Background and Concepts

SparseHC and other online AHC algorithms work based on the observation that once the values
of an input distance matrix are sorted in ascending order and loaded chunk-by-chunk from the
top, the merge order and the dendrogram distances can be accurately determined using only
the loaded part i.e. without any knowledge about the unseen portion.

(a) The dendrogram

(b) The complete binary tree

Figure 1: Illustration of the dendrogram and the corresponding complete binary tree produced
by applying average-linkage clustering to a full distance matrix computed from 10 data points.
SparseHC takes a sorted distance matrix D as input and iteratively builds a dendrogram
10

SparseHC: an online hierarchical clustering algorithm

Nguyen, Schmidt and Kwoh

from reading only a part of D in each iteration step as shown in Figure 1. Depending on the
available main memory, a sequence of values 0 = λ0 < λ1 < . . . < λT = θ is built on-the-ﬂy.
In each iteration step 1 ≤ t ≤ T , all distances dxy with λt−1 ≤ dxy < λt are read from D.
Starting from the a tree consisting of only N leaves where a leaf node i (1 ≤ i ≤ N ) represents
the singleton cluster Ci = {i}, a binary tree (which is the dendrogram) is built from bottom
up. Since only two clusters are merged at a time, the full binary tree has a height of N − 1 and
consists of 2N − 1 nodes (see Figure 1).
In oﬄine AHC algorithms, D has to be a full pairwise distance matrix. However, in online
AHC algorithms such as SparseHC, D can be either full or sparse. A sparse distance matrix
Dθ uses a predeﬁned distance cutoﬀ θ (0 ≤ θ < 1) and stores only distance values up to θ
(0 ≤ dxy ≤ θ, ∀dxy ∈ D). For sparse matrix clustering, the output dendrogram has a height in
the range of [1, N − 1] and a size in the range of [N, 2N − 1] as shown in Figure 2.

(a) The dendrogram

(b) The incomplete binary tree

Figure 2: Illustration of the dendrogram and the corresponding incomplete binary tree produced
by applying average-linkage clustering to a sparse distance matrix computed from 10 data points
with a distance cutoﬀ θ = 0.4.
The input to SparseHC is a sorted full or sparse distance matrix stored in a list of tuples (i, j, dij ) format (similar to the MATLAB sparse matrix external format: http://www.
mathworks.com/help/matlab/ref/spconvert.html). The maximum element of a full matrix
is 1.0 while that of a partial matrix is a pre-deﬁned distance cutoﬀ θ < 1.0. The ability to
process sparse distance matrices is particularly useful in applications like taxonomic studies
in bioinformatics [4, 25] where only the lower part of the ﬁnal dendrogram is of interest. In
these situations, runtime and memory usage are further reduced depending on the sparsity of
the input matrix. The memory eﬃciency and ability of SparseHC to process sparse matrices
come at the cost of pre-sorting the input matrices. Nonetheless, the memory performance of
SparseHC is not aﬀected if an external merge sort algorithm [13] is used for the sorting stage.
Similar to oﬄine AHC algorithms, during the clustering process, SparseHC needs to store
all the connections amongst unmerged clusters to ﬁgure out which cluster pair will be merged
next. However, same as other online AHC algorithms, SparseHC only stores the connections
amongst active clusters. A cluster pair is called active in iteration step t when (1) both clusters
do not have a parent and (2) at least one distance value between the member data points has
been read from the input ﬁle during the ﬁrst t iteration steps. We observe that active clusters
contribute to only a small subset of unmerged clusters. The memory eﬃciency of online AHC
algorithms is determined by their ability to store active connections in a compact way.
11

SparseHC: an online hierarchical clustering algorithm

Nguyen, Schmidt and Kwoh

Table 2: The time and memory complexity of diﬀerent graph representations. We derive the
adjacency map from the adjacency list to facilitate edge operations required by SparseHC.
Representation
Storage
Add edge Remove Edge Query edge
Incidence matrix
O(|V ||E|)
O(|V ||E|)
O(|V ||E|)
O(E)
O(1)
O(1)
O(1)
Adjacency matrix
O(|V |2 )
Incidence list
O(|V | + |E|)
O(1)
O(E)
O(E)
Adjacency list
O(|V | + |E|)
O(1)
O(E)
O(V )
Adjacency map
O(|V | + |E|)
O(1)
O(1)
O(1)
Table 3: Distance dij between cluster Ci and Cj for clustering sparse matrices
Linkage

Edge deﬁnition

Single
Complete

(t)
eij = ()
(t)
(t)
eij = (nij )

Average

eij = (sij , nij )

(t)

(t)

(t)

Cluster distance
Incomplete edge
Complete edge
(t)
(t)
dij = 1.0
dij = dxy
(t)
(t)
dij = 1.0
dij = dxy
(t)

dij =

(t)

(t)

sij +λ(t) (|Ci ||Cj |−nij )
|Ci ||Cj |

(t)

dij =

(t)

sij

|Ci ||Cj |

Complete condition
(t)

nij = 1
= |Ci ||Cj |

(t)
nij
(t)

nij = |Ci ||Cj |

In SparseHC, we use an undirected weighted graph to model the connections amongst active
cluster pairs. This graph consists of a set of vertices V and a set of edges E. The vertices are
the nodes of the binary tree i.e. V = {C1 , C2 , . . . , C2N −1 }. SparseHC uses a ﬁxed size array
to store all possible vertices, hence allowing O(1) vertex query and update. The undirected
weighted edges are the active connections amongst the clusters.
Graphs are typically implemented using an adjacency matrix, an adjacency list, an incidence
matrix or an incidence list [21]. The time and space complexity of each representation are shown
in Table 2. To facilitate its cluster merging process, SparseHC prefers a graph representation
that requires minimum storage for the graph and allows constant time to perform edge insertion,
edge deletion, and edge update. Therefore, we have modiﬁed the standard adjacency list to
assist these operations. We call this graph representation the adjacency (hash) map. The
adjacency map is a collection of unordered hash maps, one for each vertex of the graph. Each
hash map records the set of neighbors of its vertex using the neighbor vertex identiﬁcation
number as the key. Because of this adjacency map representation, SparseHC can use O(|V |+|E|)
space to store all the clusters and their active connections. More importantly, these connections
can be accessed and updated in O(1) time.

3
3.1

SparseHC
Algorithm
(t)

The deﬁnition of the edge eij between two active clusters Ci and Cj in iteration step t is deﬁned
(t)

in Table 3 depending on the clustering scheme. dij is the minimum possible distance between
(t)

(t)

Ci and Cj and is computed according to Table 3. sij (nij ) is the sum (number) of distance
values between any member of Ci to any member of Cj that has been read from the input ﬁle
so far. λt is the maximum distance value loaded from the input matrix so far.
In each iteration step t, active edges are partitioned into two sets: a set of complete edges K (t)
and a set of incomplete edges I (t) (both sets are stored in the adjacency map). A complete edge
is a connection between two active clusters that are ready to be merged. An incomplete edge is
12

SparseHC: an online hierarchical clustering algorithm

Nguyen, Schmidt and Kwoh

a connection between two active clusters that are yet to be merged. For complete- and average(t)
(t)
linkage schemes, an edge is complete when nij = |Ci ||Cj |. Otherwise, when nij < |Ci ||Cj |, the
(t)

edge is considered incomplete. For single-linkage scheme, an edge is complete when nij = 1
i.e. the connection between two clusters is complete as soon as the ﬁrst distance value between
any member reads has been read from the input.
Let min(I (t) ) (min(K (t) )) denote the smallest distance value in I (t) (K (t) ). The high-level
description of the SparseHC algorithm in each iteration t (1 ≤ t ≤ T ) consists of three steps:
1. Read the distance values dxy from matrix D in ascending order until the adjacency map
is full and determine the value λ(t) .
2. Update/create the edges for all active cluster pairs with the new distances and partition
them into I (t) and K (t) .
(t)

(t)

3. Retrieve the edge eij for which dij = min(K (t) ) ≤ min(I (t) ). Merge the cluster pair Ci
(t)

and Cj into cluster Ck . Delete eij from K (t) and combine existing edges to either cluster
Ci or Cj into new edges to cluster Ck . Repeat until min(K (t) ) > min(I (t) ).
Algorithm 1 shows the details of SparseHC.

3.2

Correctness

To show the dendrogram produced by SparseHC is correct, we need to prove that up to the
distance cutoﬀ θ both the merge distance values and the merge order are preserved.
(t)
Merge distances: Let dij be the merge distance between two clusters Ci and Cj assuming
that they are being merged by SparseHC in an iteration t. Let dij be the merge distance
(t)
between Ci and Cj produced by a traditional AHC algorithm. We need to show that dij = dij .
(t)

Indeed, when Ci and Cj are merged by SparseHC, the edge eij is complete. By deﬁnitions of
(t)

(t)

(t)

dij in Table 1 and in dij when eij is complete in Table 3, it holds that dij = dij . Therefore,
the merge distance values are preserved.
Merge order : To prove that SparseHC preserves the merge order, we show that if Ci and Cj
are merged before Ck and Cm , then dij ≤ dkm . At the time when Ci and Cj are being merged
(t)
in an iteration t, we have dij = dij = min(K (t) ). In step t, after Ci and Cj are merged, the
(t)

status of the edge ekm is one of the followings:
(t)

(t)

(t)

1. ekm is active and complete ⇒ ekm ∈ K (t) . For a complete edge, it holds that dkm = dkm .
(t)
(t)
Besides, ekm ∈ K (t) ⇒ min(K (t) ) ≤ dkm . Therefore, dij ≤ dkm
(t)
(t)
(t)
2. ekm is active and incomplete ⇒ ekm ∈ I (t) . For an incomplete edge, it holds that dkm ≤
(t)
(t)
(t)
dkm . In SparseHC, it always holds that min(K ) ≤ min(I ) ⇒ dij ≤ min(I ).
(t)
(t)
Besides, ekm ∈ I (t) ⇒ min(I (t) ) ≤ dkm . Therefore, dij ≤ dkm
(t)
(t)
(t)
3. ekm is inactive ⇒ ekm ∈
/ {K (t) ∪I (t) }. For an inactive edge, it holds that λt < dkm ≤ dkm .
Since Ci and Cj have been merged in iteration t, dij ≤ λt . Therefore, dij < dkm
(t)

For all cases, we have dij = dij and dij ≤ dkm i.e. both the merge distances and the merge
order are preserved in SparseHC.

3.3

Memory eﬃciency

While standard oﬄine AHC algorithms store all the connections amongst unmerged clusters in
memory (i.e. |Ci | × |Cj | values for a cluster pair Ci , Cj ), SparseHC uses at most two values
13

SparseHC: an online hierarchical clustering algorithm

Nguyen, Schmidt and Kwoh

Algorithm 1 SparseHC algorithm for a sorted input matrix D from N data points stored as
a list of tuples (x, y, dxy ).
Ci ← {i}
∀i = 1, . . . , N
E.max size ← N {E is the adjacency map E = K ∪ I}
k ← N ; t ← 0; λ0 ← 0 {initialize cluster id k, iteration t, distance threshold λ}
while D = ∅ do
t←t+1
while D = ∅ and E.size ≤ E.max size do
dxy ← D.get next(); D = D \ {dxy }
Ci ← Cx .get ancestor(); Cj ← Cy .get ancestor()
(t)
(t)
eij .update(dxy ) {create eij if it does not exist}
(t)

compute dij {use the cluster distance formula in Table 3}
(t)

if eij is complete then

(t)

Ci .minK ← min(Ci .minK, dij ); Ci .merge candidate ← Cj
else
(t)
Ci .minI ← min(Ci .minI, dij )
end if
end while
λt ← dxy {λt is the largest distance in an iteration}
while dij = min(K (t) ) ≤ min(I (t) ) and k ≤ 2N − 1 do
k ← k + 1; Ck ← Ci ∪ Cj {merge clusters Ci and Cj into cluster Ck }
(t)
(t)
for all Cm such that eim ∈ E ∨ ejm ∈ E do
(t)

(t)

(t)

(t)

(t)

(t)

ekm ← merge(eim , ejm ) {skm ← sim + sjm ;
(t)

(t)

(t)

(t)

(t)

(t)

(t)

nkm ← nim + njm }

E = E ∪ {ekm } \ {eim , ejm , eij }
(t)

compute dkm {use the cluster distance formula in Table 3}
(t)
if ekm is complete then
(t)
Ck .minK ← min(Ck .minK, dkm ); Ck .merge candidate ← Cm
else
(t)
Ck .minI ← min(Ci .minI, dkm )
end if
end for
end while
if E.size ≥ E.max size then
E.max size ← 2 × E.max size {dynamically increase the adjacency map size}
if E.size ≥ RAM.size then
return partial result {when the memory limit is reached}
end if
end if
end while
return full result

14

SparseHC: an online hierarchical clustering algorithm

Nguyen, Schmidt and Kwoh

(t)

(t)

per cluster pair: the number of connections nij and the sum of distances sij (see Table 3).
(t)

Speciﬁcally, SparseHC maintains only one value per cluster pair (nij ) for complete-linkage
(t)

(t)

clustering, two values per pair (nij , sij ) for average-linkage clustering and none for singlelinkage clustering.
Compared to oﬄine AHC tools, SparseHC uses less primary memory because of two reasons:
(1) SparseHC stores only the information from the currently loaded chunks and (2) It stores a
compact version of the seen information: at most two values per active cluster pair.
Compared to existing online AHC tools such as hcluster and MCUPGMA, SparseHC is
better because of three reasons. Firstly, SparseHC uses an array of hash maps to store the
compact cluster connections. This eﬃcient data structure allows O(1) query, insert and delete,
which contributes to the compute eﬃciency of SparseHC. Secondly, for average-linkage clustering, SparseHC uses two values instead of four values per cluster connection as in MCUPGMA.
More importantly, SparseHC dynamically allocates the amount of memory needed and returns
partial results if all the available memory is consumed. MCUPGMA and hcluster require the
user to specify the amount of memory beforehand and return error if the allocated amount is
insuﬃcient. Thirdly, SparseHC supports three linkage types while ESPRIT hcluster supports
only single- and complete-linkage clustering and MCUPGMA supports only average-linkage.
ESPRIT has another sub-module called aveclust which performs fast average-linkage clustering. However, aveclust is not memory-eﬃcient and still requires quadratic memory complexity.
Finally, SparseHC stops after performing N − 1 merges. This termination condition is particularly useful for single-linkage clustering where the clustering process converges early.

4
4.1

Empirical Results
Experiment setup

We compare the performance of SparseHC against two oﬄine AHC implementations: MATLAB linkage, fastcluster [17] and two online AHC implementations: EPSRIT hcluster and
MCUPGMA. These tools are chosen for their compute and/or memory eﬃciency as well as the
availability of executable source codes.
The experiments in this section are conducted on a 64-bit Linux operating system using a
Dell T3500 PC with a quad-core Intel Xeon W3540 2.93 GHz processor and 8GB of RAM. The
runtime is measured using the Linux time command and the peak memory usage is measured
with the Valgrind Massif proﬁler [20].

4.2

Empirical complexity

Since online AHC algorithms have a heuristic nature, their theoretical complexity is often hard
to estimate. As a result, we use the regression model of space and running time [6] to calculate
the empirical complexity [22] instead of the theoretical values to compare the algorithms of
interest. Assuming the runtime and memory usage follow the power rule i.e. f (n) ≈ Cnk where
n is the input size, the constant factor C and the order k can be estimated using regression on
the log-transformed model where is the error term:
log f (n) = k log n + log C +
Table 4 reports the average empirical runtime and memory growth of the tested AHC clustering implementations of interest. We use full pre-sorted pairwise Euclidean distance matrices as
inputs in this experiment. These matrices are computed from 1000 - 20000 randomly-generated
data points. Although the values of C and k in Table 4 are only representative of the performance of these algorithms on the tested random datasets, our results on larger datasets in Table
15

SparseHC: an online hierarchical clustering algorithm

Nguyen, Schmidt and Kwoh

Table 4: The empirical runtime and memory growth (f (n) = Cnk ) of SparseHC versus popular
oﬄine and online AHC implementations. This experiment uses 20 matrices computed from 1000
to 20000 data points. Runtime fr (n) is measured in seconds using the Linux time command.
Memory usage fs (n) is measured in megabytes using the Valgrind Massif proﬁler. The input
size n is measured in thousand data points.
The empirical runtime growth fr (n)
AHC tool
Single-linkage Complete-linkage Average-linkage
SparseHC
0.003 × n1.855
0.190 × n2.047
0.216 × n2.040
2.015
2.000
hcluster/aveclust
0.340 × n
0.378 × n
0.216 × n2.047
1.996
1.996
MATLAB linkage
0.352 × n
0.344 × n
0.336 × n2.003
2.085
1.955
fastcluster
0.221 × n
0.306 × n
0.236 × n2.073
MCUPGMA
not available
not available
1.313 × n2.120
The empirical memory growth fs (n)
AHC tool
Single-linkage Complete-linkage Average-linkage
SparseHC
0.886 × n0.456
1.272 × n0.848
1.155 × n0.962
0.482
hcluster/aveclust
0.242 × n
user-deﬁned
1.007 × n1.982
1.998
1.998
MATLAB linkage
7.674 × n
7.673 × n
7.674 × n1.998
1.995
2.001
fastcluster
79.166 × n
78.343 × n
78.336 × n2.001
MCUPGMA
not available
not available
user-deﬁned

6 and on biological sequence datasets in Table 5 further conﬁrm and strengthen the validity of
the regression model for evaluating empirical complexity and the estimated values in Table 4.
The upper subtable of Table 4 shows that all algorithms have quadratic runtime with k ≈ 2
as expected. Nevertheless, if we plot these functions in the domain [0, 106 ] data points, we
observe that SparseHC is the fastest amongst them. Especially for single-linkage clustering,
the constant factor C of SparseHC is two orders of magnitude smaller than other tools. For
the complete- and average-linkage schemes, the main reason for the fast runtime of SparseHC
is the the eﬃciency of edge operations of the adjacency map data structure. For the singlelinkage scheme, the signiﬁcant improvement in speed is due to the edge completion condition
(t)
(nij = 1). This condition allows two clusters to be merged as soon as the connection between
them becomes active, making it unnecessary for SparseHC to store and query active connections
of unmerged clusters. Moreover, because of this condition, the merging process for the singlelinkage scheme often completes before all values of the input ﬁle are loaded, eﬀectively reducing
the amount of runtime spent for ﬁle input.
The lower subtable of Table 4 shows that oﬄine algorithms have quadratic memory complexity with k ≈ 2 as anticipated. Python clustering modules such as fastcluster or SciPy cluster
function are less memory-eﬃcient than MATLAB linkage since they require additional intermediate data besides the input matrix. On the contrary, the memory usage of SparseHC grows
sublinearly/linearly with the input size. SparseHC mainly uses memory to store the adjacency
map of unmerged cluster connections. For the “user-deﬁned“ cases in Table 4, our experiments
show that SparseHC uses less memory than hcluster and MCUPGMA. For example, to cluster a 4GB matrix, SparseHC consumes 16MB while hcluster uses up 192MB of main memory.
Similarly, to cluster a 2.2GB matrix, SparseHC consumes 21MB while MCUPGMA uses up
312MB of main memory. Therefore, SparseHC is the most space-eﬃcient for complete- and
average-linkage clustering. For single-linkage, SparseHC and hcluster achieve similarly good
memory performance. The reasons behind SparseHC memory eﬃciency are discussed in details
in Section 3.3.
16

SparseHC: an online hierarchical clustering algorithm

Nguyen, Schmidt and Kwoh

Table 5: Using SparseHC, aveclust and MCUPGMA for clustering sparse matrices computed
from DNA datasets with sparsity = 50%
Number of
sequences
10000
20000
30000
40000

Sparse matrix
size (in MB)
483
2035
4706
8415

Runtime (in seconds)
SparseHC aveclust MCUPGMA
13.3
15.0
169.2
54.2
67.3
651.2
126.0
174.8
1477.9
229.8
321.1
2815.9

Memory usage (in MB)
SparseHC aveclust MCUPGMA
8.4
96.4
311.2
14.7
383.3
311.9
24.3
860.9
312.7
30.9
1529.6
313.8

matrix size
Table 6: The memory eﬃciency of SparseHC, presented by the ratio memory
usage
Number of Matrix size
Memory usage (in MB)
Memory eﬃciency of SparseHC
data points
(in GB)
Single Complete Average Single Complete
Average
50000
14
7.0
30.5
44.9
2055
469
318
100000
56
12.2
60.0
90.2
4673
954
635
150000
126
17.7
89.6
143.4
7272
1437
897
200000
224
22.9
119.4
198.9
10013
1917
1151

4.3

SparseHC for clustering DNA datasets

To demonstrate the usage of SparseHC for bioinformatics applications, we use SparseHC for
average-linkage clustering of sparse matrices computed from DNA sequence datasets. This
experiment uses four sparse matrices computed from DNA sequence datasets of size 10000 40000 sequences. The sparsity of these matrices is about 50%.
The matrices are computed using the sequence embedding approach as used in the popular
Clustal-Omega multiple sequence alignment tool [3]. Each DNA sequence is converted into
a vector of real coordinates by computing the k-mer distances between that sequence and a
set of seeds (seeds are representative sequences chosen from the same datasets). The pairwise
distances amongst these DNA sequences are then computed by the Euclidean distances of their
corresponding embedding vectors. Subsequently, the pairwise distance matrix is sorted and
its lower half is written to disk for clustering. We report the runtime and memory usage of
SparseHC and demonstrate its eﬃciency against other sparse clustering tools in Table 5.

4.4

SparseHC for clustering large matrices

matrix size
To highlight the memory eﬃciency of SparseHC, we report the memory
usage ratio for four
representative large datasets in Table 6. These datasets are 2 - 28 times bigger than the
amount of RAM on the test platform. This table shows that SparseHC can process distance
matrices three to four orders of magnitude larger than the memory capacity.

5

Conclusion

Producing dendograms by performing a hierarchical clustering of objects is a crucial data analysis tool in computational science. In this paper we have addressed the problem of ﬁnding
a memory-eﬃcient and fast approach (SparseHC) to compute such dendograms, which is of
high importance to research since many scientiﬁc areas are facing a data explosion. SparseHC
is a new online AHC tool which can perform accurate single-, complete- and average-linkage
hierarchical clustering with linear empirical memory complexity, making it particularly useful
to cluster large datasets using computers with limited memory resources.
17

SparseHC: an online hierarchical clustering algorithm

Nguyen, Schmidt and Kwoh

SparseHC is available at https://bitbucket.com/ngthuydiem/sparsehc. The Euclidean
distance matrix simulator is available at https://bitbucket.com/ngthuydiem/simmat.

References
[1] Michael R. Anderberg. Cluster analysis for applications. Academic Press, 1973.
[2] Pavel Berkhin. A survey of clustering data mining techniques. In Jacob Kogan, Charles Nicholas,
and Marc Teboulle, editors, Grouping multidimensional data, volume 10, pages 25–71. Springer,
2006.
[3] Gordon Blackshields, Fabian Sievers, Weifeng Shi, Andreas Wilm, and Desmond G Higgins. Sequence embedding for fast construction of guide trees for multiple sequence alignment. AMB,
5(1):21, January 2010.
[4] Marc J Bonder, Sanne Abeln, Egija Zaura, and Bernd W Brandt. Comparing clustering and
pre-processing in taxonomy analysis. Bioinformatics, 28(22):2891–2897, September 2012.
[5] Allan Borodin and Ran El-Yaniv. Online computation and competitive analysis. Cambridge University Press, May 1998.
[6] Marie Coﬃn and Matthew J. Saltzman. Statistical Analysis of Computational Tests of Algorithms
and Heuristics. INFORMS Journal on Computing, 12(1):24–44, January 2000.
[7] William H. E. Day and Herbert Edelsbrunner. Eﬃcient algorithms for agglomerative hierarchical
clustering methods. Journal of Classiﬁcation, 1(1):7–24, December 1984.
[8] Brian S Everitt, Sabine Landau, Morven Leese, Daniel Stahl, Walter A Shewhart, and Samuel S
Wilks. Cluster Analysis, 5th Edition. Wiley Series in Probability and Statistics. 2011.
[9] M Girvan and M E J Newman. Community structure in social and biological networks. Proceedings
of the National Academy of Sciences of the United States of America, 99(12):7821–6, June 2002.
[10] I Gronau and S Moran. Optimal implementations of UPGMA and other common clustering
algorithms. Information Processing Letters, 104(6):205–210, 2007.
[11] Sudipto Guha, Rajeev Rastogi, and Kyuseok Shim. CURE: an eﬃcient clustering algorithm for
large databases. ACM SIGMOD Record, 26(1):73–84, March 1998.
[12] A K Jain, M N Murty, and P J Flynn. Data clustering: a review. ACM Computing Surveys,
31(3):264–323, 1999.
[13] Donald Ervin Knuth. The art of computer programming: Sorting and searching. Addison-Wesley,
1998.
[14] G. N. Lance and W. T. Williams. A General Theory of Classiﬁcatory Sorting Strategies: 1.
Hierarchical Systems. The Computer Journal, 9(4):373–380, February 1967.
[15] Yaniv Loewenstein, Elon Portugaly, Menachem Fromer, and Michal Linial. Eﬃcient algorithms
for accurate hierarchical clustering of huge datasets. Bioinformatics, 24(13):i41–9, July 2008.
[16] Christopher D. Manning, Prabhakar Raghavan, and Hinrich Sch¨
utze. Introduction to Information
Retrieval. Cambridge University Press, July 2008.
[17] Daniel M¨
ullner. fastcluster: Fast Hierarchical, Agglomerative Clustering Routines for R and
Python. Journal of Statistical Software, 53(9):1–18, September 2011.
[18] Daniel M¨
ullner. Modern hierarchical, agglomerative clustering algorithms. September 2011.
[19] Fionn Murtagh and Pedro Contreras. Methods of Hierarchical Clustering. April 2011.
[20] Nicholas Nethercote and Julian Seward. Valgrind. ACM SIGPLAN Notices, 42(6):89, June 2007.
[21] Robert Sedgewick. Algorithms in C++: Graph Algorithms. Addison-Wesley, 2002.
[22] Robert Sedgewick and Philippe Flajolet. Analysis of Algorithms. Addison-Wesley, 2013.
[23] P H A Sneath and R R Sokal. Numerical Taxonomy: The Principles and Practice of Numerical
Classiﬁcation. A Series of books in biology. W.H. Freeman, 1973.
[24] M Steinbach, G Karypis, and V Kumar. A Comparison of Document Clustering Techniques. KDD
Workshop on Text Mining, 400(X):1–2, 2000.

18

SparseHC: an online hierarchical clustering algorithm

Nguyen, Schmidt and Kwoh

[25] Yijun Sun, Yunpeng Cai, Li Liu, Fahong Yu, Michael L Farrell, William McKendree, and William
Farmerie. ESPRIT: estimating species richness using large collections of 16S rRNA pyrosequences.
Nucleic Acids Research, 37(10):e76, June 2009.
[26] Rui Xu and Donald Wunsch. Survey of clustering algorithms. IEEE Transactions on Neural
Networks, 16(3):645–678, May 2005.
[27] Tian Zhang, Raghu Ramakrishnan, and Miron Livny. BIRCH: A New Data Clustering Algorithm
and Its Applications. Data Mining and Knowledge Discovery, 1(2):141–182, June 1997.

19

