Procedia Computer Science
Volume 29, 2014, Pages 2152–2161
ICCS 2014. 14th International Conference on Computational Science

The Design and Implementation of a GPUenabled Multi-Objective Tabu-Search intended
for Real World and High-Dimensional
Applications
Christos Tsotskas 1*, Timoleon Kipouros 1, and Anthony Mark Savill1
1

Department of Power and Propulsion Cranfield University, Bedfordshire, U.K .
c.tsotskas@cranfield.ac.uk, t.kipouros@cranfield.ac.uk,
mark.savill@cranfield.ac.uk

Abstract
Metaheuristics is a class of approximate methods based on heuristics that can effectively handle real
world (usually NP-hard) problems of high-dimensionality with mu ltiple objectives. An existing mu ltiobjective Tabu-Search (MOTS2) has been re-designed by and ported onto Co mpute Unified Dev ice
Architecture (CUDA) so as to effect ively deal with a scalable mu lti-objective problem with a range of
decision variables. The h igh co mputational cost due to the problem co mplexity is addressed by
emp loying Graphics Processing Units (GPUs), wh ich alleviate the computational intensity . The main
challenges of the re-implementation are the effect ive co mmunication with the GPU and the
transparent integration with the optimizat ion procedures. Finally, future work is proposed towards
heterogeneous applications, where improved features are accelerated by the GPUs.
Keywords: M OTS2, GPU, CUDA, metaheuristics, ZDT, multi-objective optimization, real world applications

1 Introduction
Real world applications are usually NP-hard problems and are governed by a large number of
decision variables that frequently involve multip le-object ives. These objectives are conflict ing in
nature and have to be optimized at the same time . In principle, optimization algorith ms, frequently
called optimizers, are engaged to deal with these problems , where a solution is expected within
reasonable time. Because the objectives are usually hard to resolve analytically, appro ximat ion
methods are required to resort them (Talb i, 2009). These are relatively co mputationally intensive and
*

Corresponding Author

2152

Selection and peer-review under responsibility of the Scientiﬁc Programme Committee of ICCS 2014
c The Authors. Published by Elsevier B.V.
doi:10.1016/j.procs.2014.05.200

GPU-enabled Multi-Objective Tabu-Search for Real World Applications

Christos Tsotskas et al.

ought to be kept to a min imu m. When employing mu lti-objective optimizers, the result is a set of nondominated points in the objective function space, which are called Pareto Optimal points and when
projected they form the Pareto Front. Consequently, the goal of a mult i-objective optimizer is to find
the global Pareto Front of the problem. In addition, a solution is required within relatively short time
intervals. Simply, the problem size is large, the co mputational load is increased and there are
supplementary real-t ime constraints to deliver the solution. Therefore, it is sensible to emp loy
metaheuristic optimization algorith ms on such complex problems when the aforementioned conditions
are met and there is a limited computational budget (Glover & Kochenberger, 2003) , (Talbi, 2009).
Metaheuristics have been effectively adapted on a variety of problems with continuous variables ,
non-linear and noisy objectives , and highly co mp licated interactions between decision variables and
objectives. These have demonstrated good performance on a variety of engineering problems and can
yield satisfactory solutions within acceptable time frames. They are a special class of optimizers that
among others include Tabu-Search, evolutionary algorith ms (EAs) and particle swarm optimizat ion
(PSO). The first is a local-search based optimizer and belongs to single-solution based metaheuristics,
whereas the latter are global-search based optimizers and belong to population-solution-based
metaheuristic. Nevertheless, metaheuristics’ ab ility to resolve these problems fast due to their
computational complexity is a critical factor in their overall performance (Talbi, 2009).
Over the last years, due to the increasing demand for high co mputational efficiency alternative
computational architectures have been engaged. Since the conventional Central Processing Units
(CPUs) have reached their evolution limit, there is an increased focus on an alternative infrastructure
with many cores to carry out computationally expensive tasks. Compute Unified Device Arch itecture
(CUDA) is a platfo rm for heterogeneous computing that started by a commercial vendor in 2007 so as
to address complex co mputational problems more efficiently th an a single Central Processor Unit
(CPU). Since then, it has been a leader in general purpose GPU programming (Halfhill, 2008). In
practice, the potential of the CPU is combined with the GPU, which acts as a co -processor that
executes many threads in parallel. No wadays, CUDA offers an ecosystem with hardware
specifications, a rich p rogramming model and a mature develop ment enviro n ment. A detailed
introduction to CUDA can be found in (NVIDIA Corporation, 2012a) and fu rther develop ment
directions are included in references (NVIDIA Corporation, 2012b) (NVIDIA Corporation, 2012c).
Graphics Processing Units (GPUs) are often considered due to their perfo rmance improvements and
programming accessibility, which result in g reat processing power, storage capability and very h igh
cost effectiveness (Brodtkorb, Hagen, & Sætra, 2013) (Luong, Melab, & Talbi, GPU co mputing for
parallel local search metaheuristic algorithms, 2013). In order to harness the computing cycles and to
get maximu m potential out of the hardware, a lgorith ms ought to be re-designed and re-imp lemented
bottom-up, starting from the chip.
Here, the main contribution is to illustrate an approach that can handle multi-objective continuous
problems with an increased number of decision variables, as they frequently occur in real world
applications. To the best of the authors’ knowledge, it is the first time that a mult i-object ive TabuSearch optimizer is imp lemented by using CUDA. Th is new version is an in-house development and
is based on the Multi-Object ive Tabu-Search 2, which has been used in two other real world
applications (Razzaq, Tsotskas, Kipouros, Savill, & Hron, 2013) (Tsotskas, Kipouros, & Savill,
2013). Important issues related to high-dimensional data parallelism on GPU arch itectures for multiobjective optimizers are addressed, which are also associated with the selected implementation.
The rest of the paper is structured as follows. Section 2 demonstrates the computational model of
GPUs. A review of related work on high-d imensional problems is presented in the fo llo wing Section.
Section 3 describes the porting of M OTS2 on GPU architecture, where design decisions and
implementation issues are explained. Consequently, preliminary results of the aforementioned
application are demonstrated in the next Sect ion. A summary o f the work and future extensions are
given in the last section.

2153

GPU-enabled Multi-Objective Tabu-Search for Real World Applications

Christos Tsotskas et al.

2 Scalable Multi-Objective Optimization Problems
The performance of mult i-objective optimizers is usually assessed by using benchmark problems.
These are mathematical functions expressed in an analytic formu la and can be calculated quickly.
Their characteristics are well-known and they can demonstrate a number of features that replicate
situations met in real world cases , such as discontinuous Pareto Front and mu lti-modality. The most
typical are test instances in (Zhang, et al., 2009), ZDT (Zit zler, Deb, & Th iele, 2000), DTLZ (Deb,
Thiele, Lau manns, & Zit zler, 2002) and WFG (Huband, Barone, While, & Hingston, 2005) (Huband,
Hingston, Barone, & While, 2006). All these benchmark suites are scalable in the nu mber of decision
variables, whereas the last two are also scalable in the number of objective functions. Usually, the
optimizers are given a limited co mputational and t ime budget, when they are expected to discover the
optimal trade-off o f the problem. Significant contributions with industrial impact that also serve as
benchmarks have been addressed in (Woolard & Fieldsend, 2013) (Luong, Melab, & Talbi, GPUBased multi-start local search algorithms, 2011) (Schulz, 2013).
The scalability of the problem in terms of the number of decision variables and the number of
objective functions is a very active field with high interest. Although the aforementioned problem
instances have a relatively small number o f variab les , usually less than 30, in real world applications
the number of decision variables can gro w fro m hundreds to thousands. The capability of
metaheuristic optimizers to scale in the multi-objective domain has been studied in (Durillo, et al.,
2010), (Ho, Shu, & Chen, 2004), (Zhang & Li, 2007), (Luna, Gon zález-Álvarez, Chicano, & VegaRodríguez, 2011).
When the problem scales-up and more variab les are involved, due to the curse of dimensionality,
the standard methods soon straggle to deliver any acceptable solution. Capturing the target Pareto
Front becomes increasingly mo re difficult. Validating the optimizers becomes very challenging and,
frequently, the computational resources do not suffice. Therefore, alternative ways are required.

3 Running Asynchronously Parallel Multi-Objective TabuSearch on GPUs
A variation o f metaheuristics was developed by the lead author as an in-house development and is
called Multi-Ob jective Tabu-Search 2 (MOTS2), which is based on the original Tabu-Search (Glover
& Laguna, 1997) and its mu lti-objective variant (Jaeggi, Parks, Kipouros, & Clarkson, 2008). M OTS2
has been tested on real world cases with a s mall nu mber o f decision variab les (Tsotskas, Kipouros, &
Savill, 2013) (Razzaq, Tsotskas, Kipouros, Savill, & Hron, 2013). Consequently, it is interesting to
investigate its performance when the problem scales -up in terms of number of variables . Ho wever,
MOTS2, by imp lementation, is a local-search based optimizer and it is believed that it would be very
beneficial to run on GPUs. The ideas and concerns discussed below were strategically considered to
deliver a sustainable design, appropriate for engineering applications . The following two subsections
present the general and specific parts of the development of the new optimizer, respectively.
Starting fro m the software perspective, it is important to understand the features of the selected
optimizer (M OTS2) by following the classification of metaheuristics (Talb i, 2009). First of all,
MOTS2 is nature based in the sense that it attempts to mimic hu man intellectuality when searching
through the design space. It uses a variety of memories, each for slightly different purposes. At the top
level, the majority of the steps are deterministic, but some minor parts are stochastic, so it is mixed in
that sense. It performs a single-solution-based search. Because every search starts from a single
solution - one of the principles of design optimization - that is transformed in an iterative manner, it is
an iterative optimizer. Simply, this list characterizes the performance of MOTS2 and attempts to
describe an appropriate environment to be adapted on CUDA.

2154

GPU-enabled Multi-Objective Tabu-Search for Real World Applications

Christos Tsotskas et al.

Fro m the hardware perspective, the characteristics of the selected hardware (GPU) will indicate
what features are more preferable to be imp lemented, simp ly because only the software behavior can
change. By production, GPUs contain a large nu mber of cores that can execute the same instruction in
parallel, but the algorith mic logic must follow the princip les of Single-Instruction Multip le-Threads
(SIMT) in order to be effective. This makes GPUs ideal for applications where d ifferent data are
similarly processed in large batches, where each batch represents a set of solutions of the optimizat ion
problem. In addition, the storage capability of GPUs consists of hierarchical memo ries with variable
size and access speed; the larger the memo ry, the lower the access speed is and vice versa. In practice,
this means that the right amount of data of interest should be at the right memo ry level and at the right
mo ment. Moreover, since groups of cores can access the same memory location, in order to avoid
latencies, this has to be implemented in a prescheduled manner. So, the local memo ries should be
man ipulated in a way that the decomposed data could be used independently, but very precisely
planned, as if they were part of a factory pipeline that processes batches of products. In addition,
transferring data between CPU and GPU has to be scheduled in a way to hide data latencies; one part
of the card will carry out calculations and the other will send/receive data, preparing the
outbound/inbound batches. The major point is to keep everything local, with separate concerns at the
very fine level of data decomposition. Consequently, this will simp lify the design, will minimize the
execution branching and will suppress any other delays. These features ought to be used so as to fully
exploit the underlying infrastructure, as suggested in (NVIDIA Corporation, 2012c).

3.1 Design Decisions and Concerns
The key requirement of an application, when deployed on GPU arch itectures, is to maximize the
utilizat ion of the available resources. This is possible by balancing the rat io of processing, storage and
data transfers. MOTS2 was main ly re-designed to match the capabilit ies of the hardware. Nowadays,
computational infrastructure is very heterogeneous and this is a considerable challenge since the
overall performance depends on individual specificat ions of the modules and how they co mmunicate.
In addition, the design should transparently scale by adding a mo re powerfu l hardware, in terms of
specifications, and this is an absolute requirement since the number of required object ive function
evaluations will exponentially increase as the problem size gro ws . In any case, it is not possible to
come up with a design that would be equally good on every case.
In order to be sustainable, the functionality of the optimizer is split such that each part of the
optimizer should be carried out at the computational environ ment that can perform the best. Following
the CUDA co mputational model, the execution starts from the host and occasionally the device is
called to support the optimizat ion process. It is not possible to fully deploy an imp lementation on
GPU and dis miss the host. Also, a princip le fro m the abstract field of parallel co mputing d ictates that
a fract ion of the optimizer is not worth parallelizing, but this has to be kept to a minimu m so as to
increase the overall computational efficiency.
When designing a metaheuristic optimizer, the challenge is to combine explorat ion
(diversification) and exp lo itation (intensification) features at the right analogy. However, the nature of
real-world applications makes it difficult to precisely decide the analogy. The effect ive operation of
the optimizer lies within a margin of confidence and user’s experience. Since MOTS2 is local-search
/single-solution based, the intensification features will be used more frequently and this will have a
great impact at the runtime.
There are two common conceptual points between MOTS2 and a GPU’s operation.
Fundamentally, M OTS2 is based on and manipulates memories to guide the optimization search.
Partly, the great performance of GPUs depends on memo ries. In addit ion, MOTS2 is a local-search
based optimizer and GPUs access data by following local patterns. Contrary, recently investigated
solutions are not considered by MOTS2. Since they might be tabu, whereas an ideal GPU applicat ion
should reuse as much as possible data located in the shared memories. The new design ought to take

2155

GPU-enabled Multi-Objective Tabu-Search for Real World Applications

Christos Tsotskas et al.

advantage of the concepts of locality and memories, which will be a co mpetitive advantage over other
(global-search based) optimizers and strategies. Therefore, the objective function evaluation in
different objectives will take place on the device.
Currently, the high level (conceptual) parts of MOTS2, which emp loy practices from the fields of
Artificial Intelligence, run at the host, whereas the device only evaluates the batches (see Figure 1).
Within the device either all the cores together are used to evaluate the objective function (similar ly to
data decomposition) or each core is responsible for a single objective function evaluation (or even part
of it – like function decomposition). Certainly, there is not a clear answer here, as it heavily depends
on the intrinsic features of the prob lem; mo re importantly, the specificat ions of a single core are
relatively weak and it is more p referab le to manage the cores in groups so as to make use of the data
locality of hierarchical memo ries. In any case, the host would transparently delegate the
computationally intensive evaluation part to the device, only, as was also suggested in (Melab,
Boufaras, Talbi, & others, 2013) (Nashed, Ugolotti, Mesejo, & Cagnoni, 2012).

Figure 1 Parallel Evaluation S cheme on GPU

2156

GPU-enabled Multi-Objective Tabu-Search for Real World Applications

Quadro 1000M
CUDA Driver Version / Runtime Version
CUDA Capability
Total Amount of Global Memory (MB)
CUDA Cores
Stream Processors Rate (MHz)
Memory Clock Rate (MHz)
Power Consumption (Watt)

Christos Tsotskas et al.

5.5 / 5.5
2.1
2048
96
1400
900
45

Table 1: GPU Hardware Specifications

3.2 Implementation
The idea is to gradually reform parts of the optimizer (both algorith mic and data structures) in a
way to fit the SIMT philosophy, which also aligns with a more ob ject-oriented approach, where the
concerns have to be separated and ought to be as modular as possible. More importantly, within the
device, the optimizer should execute without branching and should exp loit the local features of localsearch (such as the generation of permitted neighborhood† of the design space) so as to match the
locality of data that GPUs operate on.
Currently, there are two thoughts; first, the appropriate parts should be moved directly onto the
device code and should be modified accord ingly. For instance, the local neighborhood could be
generated within the device, wh ile the host will not be aware of this and will have no access to.
Second, the logic o f the optimizer should be altered in a more SIM T-friendly way. For examp le,
although the Pattern Move is an enhancement to the logic of CPU ’s execution, it performs a single
evaluation, which leaves idle all the rest of the GPU cores whenever it occurs.
As already explained above the high level and co mplex parts of the code will be performed by the
host, whereas the evaluations will be carried out in batches by the device. At a certain stag e, the
optimizer will generate the candidate solutions for evaluation and will aggregate them in lists. Only
these will be sent to the device, where each solution will be asynchronously evaluated in the most
efficient way, managed by the CUDA scheduler. Thereafter, only the results will be returned back to
the host, where they will be matched with the initial solutions. This scheme will be repeated until the
stopping criteria are met and is illustrated in Figure 1. It is important to note that due to the weaklyordered parallelis m, see (NVIDIA Corporation, 2012b), ext ra care is required to secure that the reads
and writes do not interfere and they have to be synchronized , within the GPU.
The number of unique solutions that have never been evaluated before, also called the size of the
solution list, will determine the amount of parallelis m. More specifically, the CUDA kernel
configuration parameters are automatically adjusted fro m the size; first, the number of CUDA threads
is decided by the number of solutions , which in turn defines the number of CUDA blocks. For
simp licity a single d imension grid is employed in CUDA. This approach guarantees that the right
number of threads will be spawn in order to achieve h igh perfo rmance throughput and maximu m
utilization of the other resources .
In terms of access patterns, the standard practices were follo wed (NVIDIA Corporation, 2012c).
At the host side, the generated candidate solutions are checked, samp led, ordered, filtered for existing
solutions and sent to the device. Most likely, the solutions in adjacent rows will have some degree of
similarity, especially when the nu mber o f variables would be large enough. The similarity of the
candidate solutions could be combined along with a different representation of solutions within the
GPU so as to gain more performance out of the data locality, as suggested in (Luong, Melab, & Talb i,
†

A set of local points around a base point, as defined in MOTS2 functionality.

2157

GPU-enabled Multi-Objective Tabu-Search for Real World Applications

Christos Tsotskas et al.

GPU co mputing for parallel local search metaheuristic algorith ms, 2013). Even if the number of
samples remains the same, moving an additional part into the GPU, will speed -up the overall
optimization search. The global thread identification nu mber, wh ich uniquely d istinguishes a thread
within the overall grid do main, was used to load the received solutions. Finally, the results are sent
back to the host in a coalesced manner.

4 Discussion and Results
In this section the preliminary results of a feasibility study are demonstrated. The developed
optimizer had already been tested against the ZDT functions (see Section 2) at the standard variables’
size and delivered the expected trade-off. This time, the same process is applied on the GPU variant of
MOT2 with the test function ZDT2 on problem instances with 30, 120, 270, 480 and 750 variables.
The numbers were selected so as to progressively increase the size of the problem. Following the
complet ion of the optimization search, the wall clock time has been measured. In addition, the
performance of the GPU, whose specifications are listed in Tab le 1, is co mpared against the host’s
CPU, wh ich is a regular Intel i7-2720QM at 2.2 GHz. The range for each decision variab le is between
0.0 and 1. Moreover, the configuration settings for MOTS2 are listed in Table 2 and most of them
have been preserved from the verification phase. However, some of the values we re chosen as a
function of the number of decision variab les , for convenience. The optimizat ion search starts fro m the
Parameter Description
Call diversification move after # non-improvements
Call intensification move after # non-improvements
Reduce the search step size after # non-improvements
Initial step sizes (as % of variable range)
Step sizes are multiplied by this factor at restart
Number of points randomly sampled
# of variables
# of objectives
# of objective function evaluations
Divide search space into # regions
Size of Tabu Memory

Value
20
10
35
0.05
0.5
# of variables / 5
30, 120, 270, 480, 750
2
# of variables * 3000 / 5
4
# of variables * 2 / 3

Table 2: M OTS2 Configuration Settings

middle of the design space (i.e. 0.5) for each variable, a princip le fro m design optimization, where the
process starts from a well-known design. The intention is to assess what is the performance of the
optimizer and how it behaves when the problem size increases , where the problem co mplexity
increases dramatically. The scale factor and the type of test function were chosen arbitrarily, but the
scope is to replicate real world conditions. Here, it is assumed, without loss of generality, that there is
a class of real-world problems with a concave and continuous Pareto Front. Currently, the focus is to
locate any weakness and potential improve ments. Furthermore, the intention is to check that the
optimizer works well and can manage the number of design parameters.
It is obvious from the results that there are three cases of performance, based on the achieved
speed-up, as shown in Figure 2, which in the end prove why CUDA is a v iable alternative
architecture. At the end of the execution, each and every instance have accurately captured the target
trade-off and the elapsed time is presented in Table 2. In the first case, the CPU version executes
faster than the CUDA version. Th is is expected, since the computational load is little and an additional
amount of time is required in order to communicate with the GPU. The situation is similar when the

2158

GPU-enabled Multi-Objective Tabu-Search for Real World Applications

Christos Tsotskas et al.

number o f variables beco mes 4 times larger than the in itial problem size (120 variables), but this time
the performance gap is closer. Again, the data exchange between the host and the device and the work
load are not sufficiently big to justify the use of GPUs. In the second case, when 270 variab les are
used, the elapsed time between the two variat ions is almost identical, which reveals that the
performance has been matched.

CPU/CUDA

Speed-Up
1.2
1.15
1.1
1.05
1
0.95
0.9
0.85
0.8
0.75
0.7

Nu mber of
variables
30
120
270
480
750

Average Time (s)
CPU
GPU
18.4
24.4
59.7
61.5
143.5
143.1
458.0
452.4
1055.9
935.2

Table 3: Elapsed Time

0

200

400

600

800

Number of Variables
Figure 2 Scalability

For problem instances above 270 variables, the CUDA variant starts to gradually outperform the
CPU version. Init ially, for 480 variables, there is only 1% speed -up, which reveals a relatively flat
region of performance gain between 270 and 480 variables. In this situation, additional informat ion
will be required to determine which infrastructure behaved better. The power consumption can be
such as a metric, wh ich give the advantage to the CPU version because the CUDA variant requires
both CPU and GPU. Ho wever, above 480 variables, there is a clear performance imp rovement for the
CUDA version, which is 12% faster. Exp loring what is the performance gain for larger problem
instances will be part of the future work.
This behavior is highly related to the inherent operation of CUDA and MOTS2 configurat ion
settings. Even, at a low specification machine, such the one used, all the available cores have to be
utilized in order to save in elapsed time. Since, the GPU has 96 cores, it is expected not to observe any
gain below that number of parallel evaluations. Moreover, fro m MOTS2 configuration, on every
iteration, up to 1/ 5 of the nu mber of variables will need to be evaluated in parallel. So, the
performance illustrated above makes perfect sense. In the current arrangement an actual performance
improvement would occur when 96 (cores) * 5 (candidate points per iteration) = 480 evaluations.
Simp ly, when the complete nu mber of GPU cores is utilized, the CUDA enabled version of MOTS2
becomes effective. It is important to note again, that this is a transparent feature and the performance
of the optimizat ion search will change if the GPU is replaced by a more powerful counterpart. In the
end, it is up to the user to fully appreciate and realize the p otential of the applicat ion and use the
appropriate settings for the provided tool(s).

2159

GPU-enabled Multi-Objective Tabu-Search for Real World Applications

Christos Tsotskas et al.

5 Conclusions and Future Work
This paper presented a flexible and sustainable design of MOTS2 for the GPU architecture. The
new variant allo ws the user to optimize mu lti-objective high-dimensional problems within acceptable
time frames and cost-efficiently, a very desirable requirement when dealing with real-world
applications. Although the development is at a preliminary stage, promising results were delivered.
Emp loying GPUs reduced the elapsed time, wh ich comp lements the operation of MOTS2,
especially at high dimensionality. In the proposed imp lementation, the GPU acts as a co-processor
that supports the CPU log ic by evaluating big batches of solutions at higher rates. The evaluation
procedures of MOTS2 were mod ified so as to transparently couple with any GPU. The main
challenges of porting were the synchronization of data transfers and the mapping of solutions to
CUDA threads by using the hierarchical memories. The ult imate goal is to use the heterogeneous
computational infrastructure by comb ining low level SIMT and high level CPU approaches for higher
efficiency.
The performance o f h igh-dimensional mu lti-objective optimization by using ZDT2 was
demonstrated on two different architectures and 5 different problem instances (30, 120, 270, 480 and
750 decision variab les). The performance comparison of M OTS2 on CPU and CUDA show that by
the mo ment all the availab le GPU cores are utilized, the performance of the optimizat ion search
increases and the elapsed time decreases. The correct co mb ination of hardware (GPU cores),
configuration settings and problem instance dictate which version of MOTS2 is more appropriate and
where should the user expect a performance gain. It was found that initially CPU behaves better for
small problem instances, whereas the CUDA version should be preferred when the number of parallel
evaluations is equal or greater than the number of the available CUDA cores.
Future extensions will investigate more use cases and will acco mmodate mo re enhancements.
First, more models will be included fro m the ZDT suite, and other benchmark suites, such as (Zhang,
et al., 2009). Second, a range of numbers of decision variables will be tested, such as (Durillo, et al.,
2010). Th ird, co mparative studies will be conducted against sequential optimization instances and
against other optimizers. Fourth, larger problem instances will be tested so as to d iscover the upper
limit of performance gain. Finally, more capabilities and functionality will be added in the next
versions.

References
Brodtkorb, A. R., Hagen, T. R., & Sætra, M. L. (2013). Graphics processing unit (GPU) programming
strategies and trends in GPU co mputing. Journal of Parallel and Distributed Computing,
73(1), 4-13.
Deb, K., Thiele, L., Lau manns, M., & Zitzler, E. (2002). Scalab le Multi-Ob jective Optimizat ion Test
Problems. Proceedings of the 2002 Congress on Evolutionary Comp utation (pp. 825-830).
IEEE.
Durillo, J. J., Nebro, A. J., Coello, C. A., García -Nieto, J., Luna, F., & Alba, E. (2010). A study of
mu ltiobjective metaheuristics when solving parameter scalable problems. Transactions on
Evolutionary Computation. 14, pp. 618-635. IEEE.
Glover, F., & Kochenberger, G. A. (2003). Handbook in Metaheuristics. Springer.
Glover, F., & Laguna, M. (1997). Tabu Search. Kluwer Academic Publishers.
Halfhill, T. R. (2008). Parallel Processing with CUDA. Microprocessor Report.
Ho, S.-Y., Shu, L.-S., & Chen, J.-H. (2004). Intelligent evolutionary algorith ms for large parameter
optimization problems. Transactions on Evolutionary Computation. 8, pp. 522-541. IEEE.
Huband, S., Barone, L., While, L., & Hingston, P. (2005). A scalable mu lti-objective test problem
toolkit. Evolutionary multi-criterion optimization (pp. 280-295). Springer.

2160

GPU-enabled Multi-Objective Tabu-Search for Real World Applications

Christos Tsotskas et al.

Huband, S., Hingston, P., Barone, L., & While, L. (2006). A review of mu ltiobjective test problems
and a scalable test problem toolkit. Transactions on Evolutionary Computation. 10, pp. 477506. IEEE.
Jaeggi, D. M., Parks, G. T., Kipouros, T., & Clarkson, P. J. (2008). The develop ment of a mu lti objective Tabu Search algorith m for continuous optimisation problems. European Journal of
Operational Research, 185(3), 1192-1212.
Luna, F., Gon zález-Álvarez, D. L., Chicano, F., & Vega-Rodríguez, M. A. (2011). On the scalability
of mu lti-objective metaheuristics for the software scheduling problem. 11th International
Conference on Intelligent Systems Design and Applications (ISDA), (pp. 1110-1115). IEEE.
Luong, T. V., Melab, N., & Talb i, E. -G. (2011). GPU-Based mult i-start local search algorithms.
Learning and Intelligent Optimization. 6683, pp. 321-335. Springer.
Luong, T. V., Melab, N., & Talbi, E.-G. (2013). GPU co mputing for parallel local search
metaheuristic algorithms. Transactions on Computers. 62, pp. 173-185. IEEE.
Melab, N., Boufaras, K., Talbi, E.-G., & others. (2013). Paradis EO-MO-GPU: a framework for
parallel GPU-based local search metaheuristics. Proceeding of the fifteenth annual
conference on Genetic and evolutionary computation conference (pp. 1189-1196). ACM.
Nashed, Y. S., Ugolotti, R., Mesejo, P., & Cagnoni, S. (2012). lib CudaOptimize: an open source
lib rary of GPU-based metaheuristics. Proceedings of the fourteenth international conference
on Genetic and evolutionary computation conference companion (pp. 117-124). ACM.
NVIDIA Corporation. (2012a). What is CUDA. NVIDIA.
NVIDIA Corporation. (2012b). CUDA C Programming Guide (5.0 ed.). NVIDIA.
NVIDIA Corporation. (2012c). CUDA C Best Practices Guide (5.0 ed.). NVIDIA.
Razzaq, M., Tsotskas, C., Kipouros, T., Savill, M., & Hron, J. (2013). Mult i-Object ive Optimizat ion
of a Flu id St ructure Interaction Benchmarking. CMES : Computer Modeling in Engineering
& Sciences, 90(4), 303-337.
Schulz, C. (2013). Efficient local search on the GPU—investigations on the vehicle routing problem.
Journal of Parallel and Distributed Computing, 73(1), 14-31.
Talbi, E.-G. (2009). Metaheuristics: From Design to Implementation. Wiley.
Tomov, S., Dongarra, M., & Baboulin, M. (2010). To wards dense linear algebra for hybrid GPU
accelerated manycore systems. Parallel Computing, 36(5-6), 232-240.
Tsotskas, C., Kipouros, T., & Savill, M. (2013). Biobject ive Optimisation of Preliminary Aircraft
Trajectories. Evolutionary Multi-Criterion Optimization. 7811, pp. 741-755. Springer.
Woolard, M. M., & Fieldsend, J. E. (2013). On the Effect of Selection and Archiving Operators in
Many-Objective Particle Swarm Optimisation. Proceeding of the fifteenth annual conference
on Genetic and evolutionary computation conference (pp. 129-136). ACM.
Zhang, Q., & Li, H. (2007). MOEA/D: A mu ltiobjective evolutionary algorith m based on
decomposition. Transactions on Evolutionary Computation. 11, pp. 712-731. IEEE .
Zhang, Q., Zhou, A., Zhao, S., Suganthan, N. P., Liu, W., & Tiwari, S. (2009). Multiobjective
optimization test instances for the CEC 2009 special session and competition. University of
Essex, The School of Co mputer Science and Electronic Engineering. Essex: University of
Essex.
Zitzler, E., Deb, K., & Th iele, L. (2000). Co mparison of Multiobjective Evo lutionary Algorith ms:
Empirical Results. Evolutionary Computation, 8(2), 173-195.

2161

