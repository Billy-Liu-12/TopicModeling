Procedia Computer Science
Volume 29, 2014, Pages 102–112
ICCS 2014. 14th International Conference on Computational Science

Blood Flow Arterial Network Simulation with the Implicit
Parallelism Library SkelGIS
H´el`ene Coullon1 ,3 , Jose-Maria Fullana2 , Pierre-Yves Lagr´ee2 , S´ebastien Limet1 ,
and Xiaofei Wang2
1

Univ. Orl´eans, INSA Centre Val de Loire, LIFO EA 4022, F-45067 Orl´eans, France
helene.coullon@univ-orleans.fr, sebastien.limet@univ-orleans.fr
2
CNRS & UPMC Univ Paris 06, UMR 7190,
Institut Jean Le Rond d’Alembert, Boˆıte 162, F-75005 Paris, France
fullana@ida.upmc.fr, pierre-yves.lagree@upmc.fr, xfwang0219@gmail.com
3
Antea G´eo-Hyd, 101 rue Jacques Charles, 45160 Olivet, France

Abstract
Implicit parallelism computing is an active research domain of computer science. Most implicit
parallelism solutions to solve partial diﬀerential equations, and scientiﬁc simulations, are based
on the speciﬁcity of numerical methods, where the user has to call speciﬁc functions which embed
parallelism. This paper presents the implicit parallel library SkelGIS which allows the user to
freely write its numerical method in a sequential programming style in C++. This library relies
on four concepts which are applied, in this paper, to the speciﬁc case of network simulations.
SkelGIS is evaluated on a blood ﬂow simulation in arterial networks. Benchmarks are ﬁrst
performed to compare the performance and the coding diﬃculty of two implementations of the
simulation, one using SkelGIS, and one using OpenMP. Finally, the scalability of the SkelGIS
implementation, on a cluster, is studied up to 1024 cores.
Keywords: Implicit parallelism, network simulations, PDEs, Blood ﬂow

1

Introduction

A scientiﬁc simulation program is an imitation of a process over time on a computer. Most of the
time, the real phenomena is modeled by a system of partial diﬀerential equations which are timeand space-dependent (PDEs). In almost all cases, it is not possible to directly solve analytically
these equations. Hence, approximated solutions of the system are computed using numerical
methods [14, 15] which discretize time and space. In a program, the discrete representation of
space, called a mesh, is implemented by a data structure which models the connectivity between
space elements. Time discretization, on the other hand, is translated to a main loop for time
iterations. Three numerical methods with space discretization are more popular. The ﬁrst one,
based on the derivation from Taylors polynomial, is called ﬁnite diﬀerence method. In this
102

Selection and peer-review under responsibility of the Scientiﬁc Programme Committee of ICCS 2014
c The Authors. Published by Elsevier B.V.
doi:10.1016/j.procs.2014.05.010

Blood ﬂow with SkelGIS

Coullon, Fullana, Lagr´ee, Limet and Wang

method the spatial domain is discretized by a regular mesh, typically a Cartesian grid. In the
second method, called ﬁnite volume method, the domain can be discretized by a general mesh
where elements are called volume controls. This method computes the averaged value of the
exact solution on each mesh element by integrating the given system on volume controls. The
third resolution method is called ﬁnite element method. The exact solution is approximated
by a continuous and piecewise polynomial function and the domain is often discretized by a
triangulation. Each time iteration applies the scheme, obtained by the resolution method, to
the mesh. The type of scientiﬁc simulations we are interested in can be represented by the
explicit scheme
{Ut−1 (x), Ut−1 (y); y ∈ N (x)} −→ Ut (x),
(1)
where x represents an element of the mesh (i.e. discretized space domain), Ut (x) is the set
of quantities to compute for element x at the time iteration t, N (x) is the neighborhood of x
required to compute Ut (x) in the mesh. The precision of a simulation is deﬁned by its order
which gives information on needed neighborhood. In computer-science, such a computation is
called a stencil, and this domain is currently arousing a great deal of interest. The last important
point concerning scientiﬁc simulations is the concept of physical border of the domain. In a
real-world process, the domain is not limited, however, in a program, the discretized mesh has
to be ﬁnite. Thus, speciﬁc elements have to be created, to deﬁne the behavior of the physical
border of the mesh.
Complexity of scientiﬁc models and precision of data to compute in simulations are growing.
Moreover, access to super-computers is becoming easier with a growing number of clusters over
the world. As a result, it becomes signiﬁcant for scientists of all domains to write parallel
programs. For this reason, and because of lack of time and resources to get eﬃcient parallel
programs, implicit parallelism research is an active domain of computer-science. To be used,
an implicit parallelism solution has to ﬁnd a good abstraction level for the user. Indeed, if the
solution is too generic, it works for most scientiﬁc problems, but it could be diﬃcult to use. On
the other hand, if a solution is too speciﬁc, it could be too limited for most users. In this paper
is presented the implicit parallelism library SkelGIS [6, 7]. This library is speciﬁc to scientiﬁc
simulations which can be represented by the scheme (1). However, this simulation class is very
large, as most scientiﬁc simulations are solved with explicit numerical schemes.
The implicit parallelism library odeint [1] solves ordinary diﬀerential equations and proposes
implicit parallelism on GPUs with CUDA. PETSc [2] solves partial diﬀerential equations and
proposes implicit parallelism for GPUs, with CUDA, CPUs, with MPI and pthreads programming, and hybrid systems. The main diﬀerence with SkelGIS is the view of the user on the
simulation, and his programming freedom. SkelGIS does not provide predeﬁned tools to solve
diﬀerential equations, as for example interpolate or jacobi functions etc. The abstraction level
proposed by SkelGIS is closer to the OP2 framework [16] and the domain speciﬁc language
Liszt [9]. OP2 and Liszt both are implicit parallelism solutions to solve partial diﬀerential
equations based on unstructured-meshes solutions. In SkelGIS, the user has the total control
on the produced code. However, all complex data structures, optimizations and parallelizations
are hidden from the user through four macroscopic concepts: distributed data structures, data
mapping, applier and programming interface. Finally, SkelGIS can be compared to generic
libraries as for example STAPL [5] which is a parallel version of the C++ standard library
(STL). However, SkelGIS is speciﬁc to scientiﬁc simulations, while STAPL is as generic as the
STL.
The four concepts of SkelGIS have already been applied to the case of two-dimensional
regular meshes. Both heat equation and shallow-water equations have been solved using SkelGIS [6, 7]. Eﬃciency of SkelGIS programs were convincing compared to the equivalent MPI
103

Blood ﬂow with SkelGIS

Coullon, Fullana, Lagr´ee, Limet and Wang

codes, and with a smaller coding eﬀort, according to Halstead metrics [11]. SkelGIS aims to
apply its four concepts to diﬀerent kinds of simulations and to diﬀerent kinds of meshes. This
paper deals with the case of network simulations which highlights diﬃculties which do not occur in a simulation on a two-dimensional regular mesh. In a network simulation, the domain
is divided in two diﬀerent kinds of elements with two diﬀerent behaviors (diﬀerent schemes):
nodes and edges. A network also represents the connectivity between diﬀerent elements of the
domain, as does a mesh. A network is typically used for arterial or vein simulations, road or
rail traﬃc simulations, water-ﬂow or pollutant transfer simulations etc. A network can be represented as a graph and in most cases as a directed acyclic graph (DAG). The physical border
of a network is simulated by the speciﬁc behaviors of roots and leaves of the DAG. As far as we
know, no speciﬁc implicit parallelism solutions exist to solve network simulations. Libraries like
PETSc, which implement sparse matrices, could be used since a network can be represented
by a sparse matrix. However, two diﬀerent schemes have to be computed, then multiple kinds
of sparse matrices, for nodes and edges, have to be managed by the user. On the other hand,
using OP2 and Liszt, an unstructured mesh has to be created by the user, however a network
is not an unstructured mesh and is not composed of faces and cells. Thus, it seems impossible
to deﬁne a network with those solutions. The work presented in this paper oﬀers an intuitive
way to write network simulations and to obtain eﬃcient parallel programs. The paper studies implementation of a complex network simulation using SkelGIS and is organized as follow.
Concepts of the SkelGIS library as well as details on the speciﬁc case of networks are explained
in the next section. Then, the blood-ﬂow arterial simulation and its numerical resolution are
detailed. Performance results are compared to an OpenMP version of the same simulation, and
eﬃciency on a big network is evaluated on a cluster. Finally, conclusion and perspectives on
this work are given.

2

SkelGIS implicit parallelism library

The SkelGIS library is an implicit parallelism solution for scientiﬁc simulations. It aims to oﬀer a
transparent access to parallel computing. Niklaus Wirth’s aphorism [18] “Program = Algorithm
+ Data Structure” (1) can be transposed to SPMD parallel programs (Single Program Multiple
Data) on distributed memory architectures: “Parallel program = Distributed data + Algorithm
+ communications” (2). SkelGIS generates MPI parallel programs of type (2), however, it aims
at providing a sequential programming style of type (1). Figure 1 illustrates the SkelGIS
principles that relies on four concepts named DDS, DPMap, AP and PI. DDS is a distributed
data structure which manages the mesh and its connectivity and distributes automatically the
mesh among processors. Most of the eﬃciency of SkelGIS relies on the DDS. DPMap is a data
mapping. Each of its instantiation represents data of the simulation (quantities it uses) and its
mapping on the DDS. AP is an applier. It is used to apply a sequential user function, called an
operation, to a set of DPMaps. An applier also transparently proceeds MPI communications
between processors. Finally, PI represents the programming interface of SkelGIS. PI is used
by the programmer to navigate through the data, read and update them. This interface is
based on iterators and speciﬁc functions to access the neighborhood of mesh’s elements (see
Equation (1)).
As illustrated in Figure 1, SkelGIS users write an operation OP using the programming
interface PI. Then, the operation OP is called through an applier AP. SkelGIS supports transparently data distribution and communications. This way, a sequential view of the program is
provided to the user (Niklaus Wirth’s aphorism), while a parallel program is actually written.
SkelGIS implements several DDSs and their related DPMaps, AP and I. In this paper,
104

Blood ﬂow with SkelGIS

Coullon, Fullana, Lagr´ee, Limet and Wang

Proc1
AP

OP

comm

PI

DDS

PI

OP

DS

User's view

Proc2
comm

DDS

PI

OP

DPMap

DPMap

Real view

Figure 1: SkelGIS user’s view and its actual parallel execution

parallelization of an arterial blood-ﬂow simulation using SkelGIS is studied, thus the rest of
this section describes SkelGIS components used to represent networks.
DDS. The DDS which represents a network is called distributed DAG and is denoted DDAG.
The implementation of the DDAG object is not detailed in this paper, but it uses a modiﬁed
and parallel version of the “Compressed Sparse Row” storage (CSR) [3]. CSR is a light data
structure to store sparse matrices. SkelGIS uses a modiﬁed CSR method optimized to store
distributed DAGs. This representation is also speciﬁcally improved for scientiﬁc simulations to
optimize accesses to physical border elements and neighborhood elements. the DDAG object
manages eﬃcient communications between processors including an overlap of computations with
communications. Since a DDAG instantiation is an irregular data structure, its distribution is
very important to obtain a good load balancing between processors. This data distribution can
be represented as a graph partitioning problem, and is solved in the current version of SkelGIS
using a sibling-edges heuristic, adapted to scientiﬁc simulations. This data distribution produces
sensible results but will be improved in future work using hypergraph partitioning [12] and the
partitioner Mondriaan [17].
DPMap. An eﬃcient access to data in an irregular structure, as a network, leads to a complex
data structure which is heavy to store. Thus, to minimize the memory footprint of the whole
data storage, SkelGIS clearly separates the DDAG data structure from data mapped on it. This
way, the DDAG is created once and data are stored in lighter objects: DPMaps. Two kinds
of DPMaps are available for the DDAG DDS. DPMap Nodes and DPMap Edges to map data
respectively on nodes and edges of the network.
AP. A SkelGIS applier performs a communication phase, to exchange ghost data between
processors, overlapped by a computation phase where each processor applies the operation
deﬁned by the user. The prototype of the applier is the following
apply list : {DP M ap Edges} × {DP M ap N odes} × OP

(2)

where {DP M ap Edges} and {DP M ap N odes} denote two sets of DP M ap Edges and
DP M ap N odes instantiations used by the operation OP .
105

Blood ﬂow with SkelGIS

Coullon, Fullana, Lagr´ee, Limet and Wang

PI. An operation OP is a sequential function written by the user. It manipulates
DPMap Edges and DPMap Nodes instantiations through some programming interfaces. First,
three kinds of iterators allow the user to navigate through DPmaps. According to Equation (1),
there is no computational dependency between elements of the mesh in the same time iteration. Thus, the ﬁrst kind of iterators moves through nodes or edges and guarantees that the
whole DPMap is parsed in an unknown order. Two other iterators are used to parse physical
border elements of the network. As already explained, physical border elements of a network
are roots and leaves of the DAG. The bracket operator [] is used with an iterator it, [it], to
access and update values in DPMaps. Finally, SkelGIS provides some methods to access the
neighborhood of an element in the DDAG. Figure 2(a) shows the neighborhood respectively for
nodes and edges of the DDAG. This ﬁgure illustrates that computations on a node may depend
on incoming nodes and edges, and outgoing nodes and edges. Computations on an edge, on
the other hand, could be only impacted by its source and destination nodes. As a consequence,
DPMap Edges have two methods which return an iterator to access, respectively, the source
and the destination node. In the case of DPMap Nodes, two methods return a list of iterators
to access the incoming and outgoing nodes of the current node, and two methods return the
equivalent incoming and outgoing lists for edges. In a distributed simulation, the neighborhood implies some MPI communications between processors. To automatically proceed those
communications, before applying an operation, the applier uses the information given by the
user when deﬁning the DDS and its DPMaps. In many network scientiﬁc simulations, edges
represent a part of the space (a section of a river or an artery for example) which is discretized
by a one-dimensional mesh as illustrated in Figure 2(b). In such a case, the order of the onedimensional scheme has to be speciﬁed by the user to automatically optimize communications.
Thus, for example, in Figure 2(b), only information on two values at the beginning and at the
end of the mesh are needed by other processors.









EDGE ORDER=2




	


	
	


(a) Neighborhood of nodes and edges in a DDAG.

(b) 2nd order scheme on edges.

Figure 2: Neighborhood and communications for networks

Thanks to the four SkelGIS concepts dedicated to network simulations, it is possible to
totally hide parallelization of codes from the user. Moreover, programming freedom is preserved
through a sequential programming style. In the next section, the complex simulation of arterial
blood-ﬂow is explained, and the parallelization of this simulation, using SkelGIS, is described.
106

Blood ﬂow with SkelGIS

3
3.1

Coullon, Fullana, Lagr´ee, Limet and Wang

The 1D model of arterial blood-ﬂow
1D mathematical model

The 1D model of blood-ﬂow in large arteries is usually derived with two main assumptions:
axisymmetric velocity proﬁle and large wave length compared with the radius of the vessel, see
literature such as [10, 13]. By integrating the Navier Stokes equations across the cross section
of the artery, one obtains two PDEs, which link the cross sectional area A, the volumetric ﬂow
rate Q and the internal pressure P :
A ∂P
Q
∂A ∂Q
∂Q
∂ Q2
+
= 0, and
+
( )+
= −8πν ,
∂t
∂x
∂t
∂x A
ρ ∂x
A

(3)

where x is the longitudinal axis of the artery, t is time and −8πν is a coeﬃcient of the friction.
The ﬁrst PDE is the conservation law of mass, and the second is the balance of the momentum.
The blood density ρ is assumed a constant, ν is the kinematic viscosity (the modeling of shear
stress and inertia is discussed in [13]). To close the system we assume that the arterial wall is
thin, isotropic, homogeneous, incompressible, and moreover that it deforms axisymmetrically
with each circular cross-section independently of the others and it behaves like a Kelvin-Voigt
model. We denote the undeformed cross-sectional area by A0 and the external pressure of the
vessel by Pext . Then, the relation linking A and P is:
√
P = Pext + β( A −

A0 ) + ν s

∂A
,
∂t

(4)

with the stiﬀness coeﬃcient β, and the viscosity (of the arterial wall) coeﬃcient νs .

3.2

Numerical resolution

A conservation law can be written in the general form
∂F
∂U
+
= S,
∂t
∂x
where U is the conservation variable, F the ﬂux and S the source term. For ﬁnite volume
method, the domain is decomposed into ﬁnite volumes or cells with vertex xi as the center of
cell [xi−1/2 , xi+1/2 ]. In every cell, the conservation law must hold (see [8]),
xi+1/2
xi−1/2

∂U
dx +
∂t

xi+1/2
xi−1/2

∂F
dx =
∂x

xi+1/2

Sdx.
xi−1/2

Gauss’s theorem is used on the second term and variables are approximated by the averaged
xi+1/2
xi+1/2
1
1
U (x)dx, Si = Δx
S(x)dx. After the discretization
values in each cell: Ui = Δx
xi−1/2
xi−1/2
in space, we have the semi-discrete form,
∗
∗
− Fi−1/2
)
(Fi+1/2
dUi
= Φ(Ui−2 , ...Ui+2 ) were Φ(Ui−2 , ...Ui+2 ) = −
+ Si .
dt
Δx
∗
∗
The numerical ﬂuxes Fi+1/2
and Fi−1/2
are given by Rusanov ﬂux (Depending on the approximate approaches on solving the Riemann problem, diﬀerent numerical ﬂuxes are possible, with
slightly diﬀerent numerical diﬀusivity. This one is widely used because it is simple and robust, according to [4]). For second order accuracy, a MUSCL (monotonic upwind scheme for

107

Blood ﬂow with SkelGIS

Coullon, Fullana, Lagr´ee, Limet and Wang

conservation law) linear reconstruction technique is used. This is a scheme with ﬁve stencils.
The values at x1 and xN +1 are determined by the characteristic method. For the temporal
integration, we may apply a 2-step second order Adams-Bashforth (A-B) scheme,
Un+1 = Un + Δt

3
1
Φ(Un ) − Φ(Un−1 ) .
2
2

This scheme can be initiated by a forward Euler method.
Arteries are joined at conjunctions: the nodes of the network. We take a simple branching
with two daughter arteries as an example. At the node, there are then six boundary conditions,
n+1
n+1
and Qn+1
for the outlet of the parent artery and An+1
and Qn+1
for the
An+1
p
p
d1 , Qd1 ,Ad2
d2
inlets of the two daughter arteries. The conservation law of mass and momentum ﬂuxes reads
n+1
= 0,
− Qn+1
Qn+1
p
d1 − Qd2
n+1
1 Qn+1
1 Qdi 2
p
ρ( n+1 )2 + Ppn+1 − ρ( n+1
) − Pdn+1
=0
i
2 Ap
2 Ad i

(5)
i = 1, 2.

(6)

shall be expressed in cross-sectional area A by the constituThe pressures Ppn+1 and Pdn+1
i
tive relation (4). Moreover, the unknowns should match the three outgoing characteristics of
the joined arteries [10]. Thus we obtain a nonlinear algebraic system of 6 equations with 6
unknowns, which can be readily solved by Newton-Raphson iterative method with U n as the
initial guess.

3.3

Parallelization of the simulation

This simulation has ﬁrst been implemented sequentially using C++. Then, two parallel programs have been derived from the sequential code. The ﬁrst one is an OpenMP version of the
sequential code, which applies a coarse-grain parallelization model. This produces a SPMD
parallel program (Single Program, Multiple Data) where each processor is in charge of a subpart of the network. Since this program is a shared memory solution, no communications are
needed between processors which can directly access values managed by other processors. The
advantage of this kind of parallelization is that the sequential code is almost not modiﬁed.
However, coarse-grain parallelization is not completely implicit as the user has to manage the
distribution of the network among processors and specify whether a variable is shared or local.
The second parallel version of the simulation has been implemented with SkelGIS. The main
function pseudo-code of the simulation is shown in Algorithm 1. This code is very close to the
sequential code, but it uses SkelGIS objects and tools instead of homemade data structures.
The blood-ﬂow operation pseudo-code is described in Algorithm 2. The operation is organized
exactly as the sequential code. Thus, it ﬁrst deals with roots and leaves of the network to
simulate the physical border behavior (lines 1 to 13). Then, as in the sequential code, conjunction nodes of the arterial network (nodes of the DAG) are solved. Finally arteries of the
network (edges of the DAG) are solved. For each of these steps, iterators are used to move
through elements of the network. Most of the sequential code can be kept because an object
obtained from a SkelGIS iterator is a plain C++ variable as in the sequential code. Thus, as
the SkelGIS program keeps the sequential code structure and most of the sequential code itself,
the eﬀort to code the blood-ﬂow simulation with SkelGIS is quite light. In addition to this, the
use of SkelGIS data structures avoids the user to implement its own containers. As a result,
the SkelGIS code is even simpler than the sequential code.
108

Blood ﬂow with SkelGIS

Coullon, Fullana, Lagr´ee, Limet and Wang

Algorithm 1: Main function of the blood-ﬂow simulation with SkelGIS
1
2
3
4
5
6

Instantiation of the distributed network DDAG
Instantiation of DP M aps using DDAG
Initializations
while not end of time iteration do
applier({DP M ap},bloodﬂow)
end

Algorithm 2: Sequential SkelGIS bloodﬂow operation

1
2
3
4
5
6
7
8
9
10
11
12
13

4

Data: {DPMap}
Result: Modiﬁcation of {DPMap}
ItR := beginning iterator on roots
endItR := end iterator of on roots
while ItR≤endItR do
Use of neighborhood of {DP M ap}
Use of [] on {DP M ap}
ItR++
end
ItL := beginning iterator on leaves
endItL := end iterator of on leaves
while ItL≤endItL do
Use of neighborhood of {DP M ap}
Use of [] on {DP M ap}
ItL++

14
15
16
17
18
19
20
21
22
23
24
25
26
27
28

end
ItC= beginning iterator on conjunctions
endItC := end iterator on conjunctions
while ItC≤endItC do
Use of neighborhood of {DP M ap}
Use of [] on {DP M ap}
ItC++
end
ItA= beginning iterator on arteries (edges)
endItA := end iterator on arteries
while ItA≤endItA do
Use of neighborhood of {DP M ap}
Use of [] on {DP M ap}
ItA++
end

Experiments

This section ﬁrst presents performance comparisons between OpenMP and SkelGIS parallelizations of the 1D arterial blood-ﬂow simulation. Then, both versions are compared in terms
of programming eﬀorts. Finally, as the SkelGIS version is written in MPI and made for distributed memory architectures, it is evaluated on a bigger network with more processors of a
cluster. Experiments have been computed on thin/standard nodes of the TGCC-Curie cluster
(20th cluster in the top500 list of November 2013). Each of those nodes is equipped with two
eight-cores CPU Sandy Bridge clocked at 2.7GHz, 64Go of RAM DDR3 and a local SSD disk.
Each experiment has been compiled with the ﬂag -O3, and has been computed 4 times. The
average execution time is used in the evaluations of this section.
As explained in the previous section, the OpenMP version uses a coarse-grain parallelization
which, most of the time, produces better performance than a ﬁne-grain parallelization but does
not totally hide parallelization of codes. The SkelGIS version, on the other hand, produces an
MPI program, where parallelization of codes is totally hidden from the user. As the OpenMP
version is a shared memory solution, it runs on a single machine. Thus, a single standard
node of TGCC-Curie, with 16 cores, has been used for evaluations. An artery network of 4.000
arteries and conjunction nodes has been used for this experiment. Speedups of both versions
are presented in Figure 3(a), while Figure 3(b) represents execution times with a logarithmic
scale.
First, execution times and speedup of the SkelGIS version are better than the OpenMP
109

Blood ﬂow with SkelGIS

(a) Speedups of OpenMP and SkelGIS versions

Coullon, Fullana, Lagr´ee, Limet and Wang

(b) Execution time with logarithmic scale

Figure 3: Experimental comparisons between OpenMP and SkelGIS implementations

version. However, one can note that the diﬀerence is exaggerated by the knee at two cores in
the scaling graph of the OpenMP version. However, even if this weakness is not taken into
account, speedup of the OpenMP version is less steep than the SkelGIS one.
Thus the SkelGIS version of the arterial blood-ﬂow network has interesting performance
results compared to an OpenMP equivalent version even if this OpenMP version could probably
be improved. Moreover, the SkelGIS version totally hides parallelization of codes while the
coarse-grain parallelization of the OpenMP version does not. Thus, it is interesting to evaluate,
with metrics, the eﬀort needed to code both parallel simulations. Halstead metrics [11] have
been used to estimate the diﬃculty and the eﬀort to write both source codes. It results that
the OpenMP code is 20% more diﬃcult to write than the SkelGIS code, and it requires a
programming eﬀort 80% greater than the SkelGIS version. As explained previously, using
SkelGIS, the data structure, which represents the network and its connectivity, does not have
to be implemented by the user. The user code is thus drastically simpliﬁed. As a result, the
length and the diﬃculty of the SkelGIS implementation is more interesting than the OpenMP
implementation.
SkelGIS, is a parallel library to create parallel scientiﬁc simulations for distributed memory
architectures. SkelGIS programs are designed to run on clusters. Thus, the SkelGIS 1D arterial
blood-ﬂow simulation has been evaluated on the TGCC-Curie cluster. To show the scalability
of the SkelGIS program, a bigger network than the one used in the previous experiment, has
been used. It contains 15.000 arteries and 15.000 conjunction nodes. The speedup presented on
Figure 4(a) illustrates the speedup from 1 to 128 cores and the speedup presented on Figure 4(b)
represents evaluations from 1 to 1024 cores. From 1 to 128 cores, the speedup is almost ideal and
linear, which is a very good result for network simulations, especially for an implicit parallelism
solution where no parallel code has been written by the user. With more cores, the speedup
begins to fall. This is probably due to a weakness in the data distribution implemented. This
implementation should be improved to avoid load imbalance and to decrease the communication
cost. Two improved data distributions are under progress.
110

Blood ﬂow with SkelGIS

(a) SkelGIS speedup from 8 to 128 cores

Coullon, Fullana, Lagr´ee, Limet and Wang

(b) SkelGIS speedup from 8 to 1024 cores

Figure 4: Speedups of SkelGIS implementation on a network of 15000 arteries

5

Conclusion

In this paper has been presented the implicit parallelism library SkelGIS for the speciﬁc case
of network simulations. Indeed, SkelGIS can be used for networks which can be represented
as directed acyclic graphs, and the four concepts of SkelGIS have been explained for this case.
A real case-study: the blood-ﬂow simulation in an arterial network, has been described, and
numerical schemes of both conjunction nodes and arteries have been explained. Three experiments illustrate that, ﬁrst, the obtained SkelGIS code is eﬃcient on a single multi-core machine
compared to an equivalent OpenMP program. Second, the eﬀort needed to implement the simulation is greater with OpenMP than with SkelGIS. Finally, the SkelGIS simulation has a very
good scalability on clusters. As explained previously, SkelGIS has already been implemented for
two-dimensional regular meshes. A work in progress is to propose a SkelGIS implementation
for general networks, thus the work presented in this paper on DAGs will be generalized to
graphs. After this, SkelGIS will be implemented for unstructured meshes and later for adaptive
meshes to be able to solve most scientiﬁc simulations in parallel.

References
[1] Karsten Ahnert and Mario Mulansky. Odeint - solving ordinary diﬀerential equations in C++.
CoRR, abs/1110.3397, 2011.
[2] S. Balay, W. D. Gropp, L. Curfman McInnes, and B. F. Smith. Eﬃcient management of parallelism
in object oriented numerical software libraries. In Modern Software Tools in Scientiﬁc Computing,
pages 163–202. Birkh¨
auser Press, 1997.
[3] R. Barrett, M. Berry, T. F. Chan, J. Demmel, J. Donato, J. Dongarra, V. Eijkhout, R. Pozo,
C. Romine, and H. Van der Vorst. Templates for the Solution of Linear Systems: Building Blocks
for Iterative Methods, 2nd Edition. SIAM, Philadelphia, PA, 1994.
[4] F. Bouchut. Nonlinear stability of ﬁnite volume methods for hyperbolic conservation laws and
well-balanced schemes for sources. Birkh¨
auser, 2004.
[5] A. Buss, Harshvardhan, I. Papadopoulos, O. Pearce, T. Smith, G. Tanase, N. Thomas, X. Xu,
M. Bianco, N. M. Amato, and L. Rauchwerger. Stapl: standard template adaptive parallel library.

111

Blood ﬂow with SkelGIS

[6]

[7]
[8]
[9]

[10]
[11]
[12]
[13]
[14]
[15]
[16]

[17]
[18]

112

Coullon, Fullana, Lagr´ee, Limet and Wang

In Proceedings of the 3rd Annual Haifa Experimental Systems Conference, SYSTOR ’10, pages
14:1–14:10. ACM, 2010.
H´el`ene Coullon, Minh-Hoang Le, and S´ebastien Limet. Parallelization of shallow-water equations
with the algorithmic skeleton library skelgis. In ICCS, volume 18 of Procedia Computer Science,
pages 591–600. Elsevier, 2013.
H´el`ene Coullon and S´ebastien Limet. Algorithmic skeleton library for scientiﬁc simulations: Skelgis. In HPCS, pages 429–436. IEEE, 2013.
O. Delestre and P.-Y. Lagr´ee. A well balanced ﬁnite volume scheme for blood ﬂow simulation.
International Journal for Numerical Methods in Fluids, page doi: 10.1002/ﬂd.3736, 2012.
Z. DeVito, N. Joubert, F. Palacios, S. Oakley, M. Medina, M. Barrientos, E. Elsen, F. Ham,
A. Aiken, K. Duraisamy, E. Darve, J. Alonso, and P. Hanrahan. Liszt: A domain speciﬁc language
for building portable mesh-based pde solvers. In Proceedings of 2011 International Conference for
High Performance Computing, Networking, Storage and Analysis, SC ’11, pages 1–12. ACM, 2011.
L. Formaggia, A. Quarteroni, and A. Veneziani. Cardiovascular Mathematics: Modeling and
simulation of the circulatory system, volume 1. Springer, 2009.
Maurice H. Halstead. Elements of Software Science (Operating and programming systems series).
Elsevier Science, New York, USA, 1977.
Bruce Hendrickson and Tamara G. Kolda. Graph partitioning models for parallel computing.
Parallel Comput., 26(12):1519–1534, November 2000.
P.-Y. Lagr´ee. An inverse technique to deduce the elasticity of a large artery. EPJ Applied Physics,
9(2):153–164, 2000.
Brigitte Lucquin and Olivier Pironneau. Introduction to Scientiﬁc Computing. John Wiley & Sons,
Inc., New York, NY, USA, 1997.
W.E. Milne. Numerical Solution of Diﬀerential Equations. Applied Mathematics Series. John
Wiley and Sons, 1953.
GR Mudalige, MB Giles, I. Reguly, C. Bertolli, and PHJ Kelly. Op2: An active library framework
for solving unstructured mesh-based applications on multi-core and many-core architectures. In
Innovative Parallel Computing (InPar), pages 1–12. IEEE, 2012.
Brendan Vastenhouw and Rob H. Bisseling. A two-dimensional data distribution method for
parallel sparse matrix-vector multiplication. SIAM Review, 47(1):67–95, 2005.
Niklaus Wirth. Algorithms + Data Structures = Programs. Prentice Hall PTR, Upper Saddle
River, NJ, USA, 1978.

