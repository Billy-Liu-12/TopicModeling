Procedia Computer Science
Volume 29, 2014, Pages 113–123
ICCS 2014. 14th International Conference on Computational Science

Triplet Finder: On the Way to Triggerless Online
Reconstruction with GPUs for the PANDA Experiment
Andrew Adinetz1 Andreas Herten2 Jiri Kraus3
Marius C. Mertens4 Dirk Pleiter1 Tobias Stockmanns2 Peter Wintz2
1

J¨
ulich Supercomputing Centre, Forschungszentrum J¨
ulich, 52425 J¨
ulich, Germany
a.adinets@fz-juelich.de,d.pleiter@fz-juelich.de
2
IKP, Forschungszentrum J¨
ulich, 52425 J¨
ulich, Germany
a.herten@fz-juelich.de,t.stockmanns@fz-juelich.de,p.wintz@fz-juelich.de
3
NVIDIA GmbH, Germany
jkraus@nvidia.com
4
ZIM, Universit¨
at Duisburg-Essen, 45127 Essen, Germany
marius.mertens@uni-due.de

Abstract
PANDA is a state-of-the-art hadron physics experiment currently under construction at FAIR,
Darmstadt. In order to select events for oﬄine analysis, PANDA will use a software-based
triggerless online reconstruction, performed with a data rate of 200 GB/s.
To process the raw data rate of the detector in realtime, we design and implement a GPU
version of the Triplet Finder, a fast and robust ﬁrst-stage tracking algorithm able to reconstruct
tracks with good quality, specially designed for the Straw Tube Tracker sub-detector of PANDA.
We reduce the algorithmic complexity of processing many hits together by splitting them into
bunches, which can be processed independently. We evaluate diﬀerent ways of processing
bunches, GPU dynamic parallelism being one of them. We also propose an optimized technique
for associating hits with reconstructed track candidates.
The evaluation of our GPU implementation demonstrates that the Triplet Finder can process
almost 6 Mhits/s on a single K20X GPU, making it a promising algorithm for the online event
ﬁltering scheme of PANDA.
Keywords:

Contents
1 Introduction

114

2 PANDA Experiment

116

3 Triplet Finder Algorithm

117

Selection and peer-review under responsibility of the Scientiﬁc Programme Committee of ICCS 2014
c The Authors. Published by Elsevier B.V.
doi:10.1016/j.procs.2014.05.011

113

PANDA GPU Triplet Finder

A. Adinetz et al.

4 GPU Implementation

118

5 Performance Evaluation
120
5.1 Reconstruction Quality Evaluation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 120
5.2 GPU Performance Evaluation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 120
6 Summary and Outlook

1

122

Introduction

The PANDA experiment (Fig. 1) is a state-of-the-art hadron physics experiment currently
being built for the Facility for Antiproton and Ion Research (FAIR) in Darmstadt, Germany.
When PANDA (Antiproton Annihilation in Darmstadt) starts operating in 2018, it will deliver
unprecedented insight into open charm physics, reﬁning current physical measurements and
searching for rare, potential exotic events.
In the investigated energy region, signal and background events have similar signatures and can
not be easily distinguished using conventional hardware-based triggering mechanisms. Instead,
in PANDA a sophisticated realtime multistage event ﬁlter is substituting this element. Only
with this process it is possible to reduce the raw data rate of 200 GB/s (based on an event
rate of 2 × 107 events/s) by three orders of magnitude. For further in-depth oﬄine analysis,
the remaining data is written to a storage facility which can store about 10 PB per year for the
whole experiment.
Every sub-detector of PANDA employs speciﬁc reconstruction algorithms to eventually perform a global, detector-wide software trigger selection. One important step in the chain of
online reconstruction algorithms is tracking: The discrete detector responses to particle transitions (hits) are converted to continuous particle trajectories. From these, particle properties like
momentum, charge, or the interaction point, which are crucial for ﬁnal signal selection, can be
extracted. The problem thus may be considered as a high-performance data analysis challenge

Figure 1: The PANDA experiment in side view. Antiprotons enter the experiment through the
beam pipe from the left.
114

PANDA GPU Triplet Finder

A. Adinetz et al.

where high-performance computing resources are needed to extract relevant information from
a large stream of data.
Track reconstruction algorithms are established and well-researched parts of any particle
physics experiment’s computing framework. The algorithms can be accelerated extensively by
introducing many-core parallel systems. The future Compressed Baryonic Matter (CBM) experimen at FAIR, for example, investigates the performance of parallel tracking algorithms on
many-core processors for their oﬄine event reconstruction [10, 9]. The OPERA experiment in
Gran Sasso (Italy) uses GPUs for recognizing particle trajectories in photo emulsion plates – a
gain of 60× in comparison to a CPU algorithm could be achieved [5]. The recent advances in affordable computing power make it possible to move tracking algorithms to ever earlier stages in
experiments’ event reconstruction [4]. The former CDF experiment at Fermilab featured a Silicon Vertex Fitter (SVT), comprised of a number of computing boards, tracking particles as part
of the experiment’s second level triggering mechanism – the Level 2 trigger [2]. A single new,
FPGA-based board replaced the twelve SVT computing boards in 2010: The introduction of
the Gigaﬁtter lead to a gain in performance and number of extractable features. The experience
obtained during the implementation of the Gigaﬁtter is used to design another experiment’s
future tracking trigger [3]: ATLAS, one of the big multi-purpose experiment at CERN’s Large
Hadron Collider (LHC), will use GPUs in its trigger chain as part of the High Level Trigger
(HLT). Located in the event reconstruction directly after the hardware-implemented Level 1
trigger, the GPU-enabled Level 2 tracking exceeds its CPU counterpart immensely [6]. The
second multi-purpose experiment at the LHC, CMS, is speeding up its Level 1 trigger with
the introduction of a Track Trigger based on CMOS chips [14]. Also investigated are Hough
Transforms for particle tracking on GPUs as part of CMS’s HLT [8]. Not as an upgrade, but
since the start of its experimental run, another LHC experiment, ALICE, uses GPUs in their
HLT [7]. A Cellular Automaton and a Kalman ﬁlter are combined. A speed-up of 3.3× could be
achieved in relation to a comparable CPU implementation. PANDA wants to use GPU-based
online track reconstruction at the highest point of the experiment’s trigger chain: At Level 1
position, as it is referred to by the above experiments.
In this paper, we present a new algorithm specially designed for the Straw Tube Tracker
(STT) of the PANDA experiment. The STT is a central tracking detector consisting of 4 636
light-weight drift tubes. The algorithm, the Triplet Finder, is designed as a fast and robust
ﬁrst-stage tracking algorithm, which does not require knowledge of the time t0 when an event
occurred. t0 is not known a-priori, as the experiment runs without a triggering mechanism.
The Triplet Finder enables t0 to be determined, which can then be used for other, more precise
tracking algorithms later in the online reconstruction chain.
More research on the Triplet Finder and testing this new algorithm at scale is necessary
to better understand its parameters and tuning options. The main focus of this paper is,
however, not research on the reconstruction quality but rather an investigation of the compute
performance. We present an implementation of the algorithm on state-of-the-art GPUs and
analyze its performance properties. The contributions of this paper are as follows:
• We propose improvements to the Triplet Finder, an online track reconstruction algorithm
for the PANDA experiment.
• We present an GPU implementation of the algorithm and describe platform speciﬁc optimizations.
• We present performance analyses for our implementation and demonstrate that processing
times of 162 ns/hit can be achieved on a single GPU.
115

PANDA GPU Triplet Finder

A. Adinetz et al.

This paper is organized as follows. In Section 2 we describe the PANDA experiment, its
requirements and detectors. We give an overview of the Triplet Finder algorithm and proposed
improvements in Section 3, and present the GPU implementation in Section 4. The evaluation
of the proposed algorithm, both in terms of reconstruction quality and GPU performance, is
given in Section 5. Section 6 concludes this paper with a summary.

2

PANDA Experiment

FAIR is a new international accelerator facility currently under construction in Darmstadt,
Germany. Phase space-cooled antiprotons in a quasi-continuous beam will be accelerated to
momenta between 1.5 and 15 GeV/c and interact with hydrogen and heavier nuclei in ﬁxedtarget kinematics. The interaction region is enclosed by the PANDA experiment, which is being
built by over 500 researchers from all over the world.
While quantum chromodynamics, the theory of strongly interacting quarks and gluons,
is well understood for high energies, this is typically not the case for the non-perturbative
regime of mid (and low) energies. To tackle related and open scientiﬁc questions, PANDA
will allow to perform high-precision measurements and to probe for new physics [13]. The
selection of an antiproton beam colliding with protons sets PANDA distinctively aside from
all previous experiments. It’s excellent detector resolution and the precise antiproton beam
will, e.g., allow establishing new limits to existing measurements. PANDA furthermore enables
searches for exotic states ranging from gluonic hadrons, e.g. glueballs, to possible four-quark
states, i.e. tetraquarks or quark molecules.
PANDA consists of two parts: the Target Spectrometer, surrounding the interaction region in
onion-like layers, and the Forward Spectrometer, detecting the boosted particles in the forward
direction. Both spectrometers comprise diﬀerent sub-detectors, each specialized on measuring
diﬀerent kind of particle properties and event topologies. Overall, PANDA is 13 m long and
has a diameter of up to 5 m.
The innermost sub-detector of PANDA is the Micro Vertex Detector (MVD). Made out of
silicon sensor it enables open charm tagging. Located surrounding the MVD is the STT [12].
It consists of 4 636 straw tubes, each of 10 mm diameter and 150 cm length, closely packed
into 27 layers. Each tube is a small, cylindrical drift chamber. Most layers of straw tubes are
aligned parallel to the beam axis, except 8 layers, which are skewed by ±3◦ with respect to the
axis, giving access to the longitudinal dimension z in particle reconstruction. An average event
creates about 80 hits in the STT. Taking into account PANDA’s event rate of 2 × 107 events/s,
the STT deals with an average hit rate of 1600 Mhits/s.
To detect rare events at high rate, PANDA chooses a novel scheme for event recording.
Usually, speciﬁc sub-detectors in a particle physics experiment are designed to detect a certain
triggering particle. Only when this trigger ﬁres, all other sub-detectors are read out. For
PANDA, such a dedicated trigger would not be suitable. Instead, an online event ﬁlter evaluates
on the ﬂy continuously the whole data stream of all sub-detectors and searches for certain
interesting event candidates. When one of many signal channels is detected, the event is stored
for further, more detailed oﬄine analysis. An important part of the real-time event ﬁlter is
online tracking. Only with proper knowledge of the particles’ trajectories it is possible to
determine the kind of the event.
Important parameters of particle tracking algorithms are the resolution, with which an
algorithm reconstructs the original track, the eﬃciency (the fraction of found track from all
ﬁndable tracks), and the purity (the ratio of correctly found tracks and all found tracks). Also,
the performance (number of reconstructions per second) is speciﬁcally important for this work.
116

PANDA GPU Triplet Finder

(a) Illustration of Triplet generation. A hit (gray)
in a pivot straw (red outline) is combined with two
surrounding (dashed outline) hits to a center-ofmass cluster (black) - a Triplet. Combining this
Triplet with another Triplet and the interaction
point, a circle (green) is computed analytically.
Subsequently, hits are associated to the track.

A. Adinetz et al.

(b) Projection of the STT to the x y plane with
a snapshot at 450 ns running time. Shown are
all detector hits of that time (small circles for
the STT, points in the center for the MVD) and
the found tracks calculated by the Triplet Finder
(blue curves). Thick red lines indicate the location of the pivot layers for one sector. The pivot
layers of the other sectors are distributed in the
same way but not shown here.

Figure 2: Illustrations of the Triplet Finder.

3

Triplet Finder Algorithm

The Triplet Finder matches the requirements of the PANDA experiment very well. It is expected to provide suﬃcient reconstruction quality and speed at the same time. To achieve the
latter, parallelization at diﬀerent levels is foreseen, thus enabling the use of a large number of
computational cores or devices. The quasi-continuous beam delivered to PANDA consists of
2000 ns bursts separated by 400 ns gaps. Thus, groups of on average 40 overlapping events can
be easily separated. The Triplet Finder algorithm allows to constraint the time t0 at which
each individual event took place.
The STT is located inside a solenoidal magnetic ﬁeld, forcing charged particles onto helical
trajectories. The concept of the Triplet Finder centers around the fast calculation of a circle
as a two-dimensional representation of a helix, using only a small number of hit combinations.
The algorithm has three main stages: Triplet ﬁnding, circle calculation, and hit association.
See Fig. 2a for a graphical illustration.
Triplet ﬁnding As soon as a hit is detected in a selected, so-called pivot straw, a virtual hit
is calculated out of this pivot hit and two adjacent hits, using the center of mass of the
three points. The virtual hit is called Triplet. Pivot straws are part of pivot layers and
located as shown in Fig. 2b for one sector of the STT. Their location is chosen such that
they provide a large lever arm to the track reconstruction without using all STT layers.
Circle calculation Taking into account another Triplet as well as the interaction point located
at origin in (0, 0), a circle is computed.
117

PANDA GPU Triplet Finder

A. Adinetz et al.

Hit association Remaining STT hits, lying on the trajectory established by the circle of the
previous step, are associated to the track.
The algorithm was ﬁrst implemented as a serial CPU-only version into PANDA’s computing
framework PandaRoot. It has been extensively tested with realistic event scenarios. A timebased simulation using realistic occurrences of hits is used to model the mixing of diﬀerent
events.
The main algorithmic drawback of the Triplet Finder is its computational complexity of
O(N 2 ), where N is the number of hits processed at once. While not a problem for CPU
implementation, which processes a small number of hits, it becomes relevant for GPUs, where
O(104 ) hits are required just to load the GPU fully. However, computational complexity can
be reduced by splitting the whole set of hits into subsets that can be processed independently.
In this case, the quadratic complexity will apply only to the number of hits in each subset. The
complexity of processing many subsets is linear with the number of subsets, and thus linear
with the total number of hits.
Such a subdivision is implemented by splitting the entire simulation time into segments of
duration TC , core time, and attempting to reconstruct tracks with t0 inside that segment for
each subset. A time segment of duration TD , equivalent to the straw tube’s maximum drift
time, is added to each subset. The subsets thus obtained are called bunches, and the bunching
scheme is illustrated in Fig. 3. If NC and ND are the average number of hits in core time and
+ND )2
drift time of a bunch, respectively, the total processing cost can be expressed as O(N (NCN
),
C
which is linear in the total number of hits N . As bunches overlap, duplicate tracks are produced
at the bunch boundaries. However, they can be easily removed by a later duplicate elimination
stage, and are a small price to pay for a large reduction in complexity achieved. Note that due
to gaps in particle stream inserted after each 2 μs, overlap can be avoided for suﬃciently large
bunches.

bunch 1
[TC, 2TC + TD)

1TC

3TC

0
bunch 0
[0, TC+TD)

...

2TC
TD

t
bunch 2
[2TC, 3TC + TD)

Figure 3: Bunching scheme used for the GPU implementation of the Triplet Finder

4

GPU Implementation

In all stages of the Triplet Finder, various elements, such as triplets or track candidates are
processed independently. Also, the working data set of the algorithm is not large and easily ﬁts
into GPU memory. While a single bunch is not large enough to fully utilize a GPU, processing
multiple bunches together provides enough parallelism to utilize a GPU fully. The Triplet
Finder is thus a good candidate for GPU parallelization. A simple GPU implementation can
be achieved by replacing each stage of the algorithm with a GPU kernel.
Bunch size, speciﬁcally, its core time TC , is a parameter to be chosen. Too small or too large
a value will lead to increased computational costs due to large overlap or quadratic complexity
118

PANDA GPU Triplet Finder

A. Adinetz et al.

within each bunch, respectively. In practice, we found a core time of 1–2 μs to work well with
our data. Another design issue is organizing the processing of multiple bunches on a single
GPU. The following approaches were tested:
Dynamic parallelism The host launches one kernel with one thread per bunch, which then
launches kernels for individual Triplet Finder stages from the GPU.
Joined kernel All Triplet Finder stages for each bunch are joined into a single GPU kernel,
which is then launched with one thread block per bunch.
Host streams Individual kernels are used for all Triplet Finder stages, but are launched from
the host, with one CUDA stream per bunch.
It turns out that testing track candidates against the straws in the hit association stage is the
most time-consuming part. This is because each track is tested against all STT straws, while
it only crosses a fraction of them, about 50. A better approach would be to limit the tested
hits using geometrical information. Straws belonging to a speciﬁc layer in a speciﬁc sector are
called a sector-row. The centers of all straws of a sector-row lie on a single line and the tubes
are tightly packed (see Fig. 2). It is therefore possible to quickly test whether a track intersects
a sector-layer, and if so, what are the ranges of tubes to be tested. To reduce computational
complexity, tracks are “thickened” into rings, and straws of sector-rows are shrunk into points
lying on line segments, as in Fig. 4a; the rings are then tested against line segments. As there
are only 108 non-skewed sector-rows, and only a small number of straws is intersected within a
sector-row, the amount of computations for tube testing is reduced.

Track

Track

Sector-Row

Sector-Row

(a) Intersecting a track (ring) with a sector-row
(segment of line with points) for tube-testing with
sector-rows.

(b) Deviation of the reconstructed transverse momentum of charged tracks from the Monte Carlo
value.

Figure 4: Aspects of Triplet Finder algorithm.
Other GPU-speciﬁc optimizations have also been applied:
• Storing sector-row data in constant memory and tube and skewlet data in shared memory
on the GPU.
• Testing for point of closest approach in a separate kernel to reduce branch divergence.
• Avoiding trigonometric functions and square roots for testing hits against tracks to reduce
computational complexity.
119

PANDA GPU Triplet Finder

5
5.1

A. Adinetz et al.

Performance Evaluation
Reconstruction Quality Evaluation

The track reconstruction done by the Triplet Finder is benchmarked with the goal to optimize
both eﬃciency and purity. Using an input test sample of 9 228 reconstructible tracks generated
by PandaRoot’s background-type Monte Carlo generator (DPM ), the Triplet Finder outputs
38 445 tracks. The higher number originates from tracks being formed from diﬀerent subsets of
hits of the original track and hence counted multiple times. Also, wrongly found tracks (ghost
tracks) are counted in that number. 21 137 of the output tracks can be considered well-matched
to the originally simulated tracks. A track merging algorithm still has to be implemented.
A variable of interest in the evaluation of the reconstruction quality of the algorithm is the
deviation of the reconstructed track momentum from the original track momentum. Fig. 4b
shows the relative deviations. The distribution is located around 0, suggesting a good reconstruction. A more detailed algorithmic evaluation of the Triplet Finder can be found in [11].

5.2

GPU Performance Evaluation

For the performance evaluation of the Triplet Finder we generated 148 556 STT hits using the
DPM generator in PandaRoot, with the hits sorted in order of increasing hit detection time.
To get a smaller number of hits, only the ﬁrst N hits from the set are taken. The algorithm
has been tested on a machine with a K20X Kepler GPU with 2 688 streaming processors (SPs)
and 6 GiB RAM, and a K40 Kepler GPU with 2 880 SPs and 12 GiB RAM, both with ECC
enabled. We focus in this section on K20X, as K40 has become available only very recently. If
not mentioned otherwise, the K20X running at SM frequency 732 MHz and memory frequency
2 600 MHz was used. The system was operating with OpenSUSE 12.3, using CUDA 5.5 and
NVIDIA driver version 331.22 installed.
Execution times of both the bunched and non-bunched version of the algorithm are shown
in Fig. 5a. The performance of the bunched approach saturates and stays constant with the
number of hits, while performance of the non-bunched version decreases, which completely
agrees with expectations. For 148 556 hits, the bunched version is already 20 times faster than
the non-bunched one, which shows that bunching is a method worth using. Also, while implemented for the Triplet Finder, bunching does not depend on the algorithm’s particular details,
and is thus usable with other reconstruction algorithms.
The maximum performance is achieved when about 105 hits are processed simultaneously.
Measuring performance of diﬀerent components reveals that the bottleneck has shifted from
tube testing to operations involving skewed straws. Combining triplets in skewed layers
(skewlets) and associating hits with skewlets taking 64 % execution time together.
Fig. 5b shows the eﬀect of sector-row testing versus testing against all tubes. Sector-row
testing is 5 times faster than all-tubes testing, and reduces the algorithm’s overall execution
time by about 40%.
Finally, we compared three approaches to processing multiple bunches: dynamic parallelism,
joined kernel and host streams. Performance for various approaches are shown in Fig. 5d. Joined
kernels approach is the slowest, as large number of registers used leads to low occupancy. It is
also interesting to notice that dynamic parallelism and host streams approaches achieve best
performance with diﬀerent bunch sizes, 1 μs and 2 μs, respectively, as can be seen in Fig. 5c.
For a small number of processed hits, the host streams approach is faster; however, starting from about 45 000 hits, the dynamic parallelism approach overtakes it. The plot shows
that while the performance for the host streams approach almost stagnates at that point, the
120

PANDA GPU Triplet Finder

A. Adinetz et al.

(a) Performance comparison of bunched and nonbunched version.

(b) Tube testing: times for sector-row testing and
testing of all tubes.

(c) Performance of host stream and dynamic parallelism approach for diﬀerent bunch sizes.

(d) Performance of diﬀerent bunching approaches
with varying number of hits.

Figure 5: Various aspects of GPU Triplet behavior; results obtained on K20X.
performance of the dynamic parallelism approach continues to grow. Due to the high number
and short duration of required kernel calls, kernel launch latency and memory copy latency is
a performance-limiting factor. This aﬀects the host streams approach more than the dynamic
parallelism approach and thus explains why the latter is faster. The reasons why it is less
aﬀected by the latencies are:
1. It avoids PCI-E transfers which are necessary in the host streams approach to get the
correct launch conﬁguration of subsequent kernels.
2. It achieves higher launch throughput and thus better hides kernel launch latencies.
3. It avoids false dependencies between kernel calls introduced by the host streams approach.
There are two reasons for false dependencies in the host stream approach. The ﬁrst is that
a single CPU thread handles all CUDA streams and bunches. While multiple host threads can
be used, according to our measurements, synchronization overhead far outweighs the beneﬁts
of removing false dependencies. The second reason is that the grid scheduling is done in
hardware by the Grid Management Unit for the host stream approach while it is done in
software for the dynamic parallelism approach. With hardware scheduling, false dependencies
arise if more streams are used than the number of device connections available; the latter is
up to 32 as of compute capability 3.5 [1]. Fig. 6a shows that the performance of the host
121

PANDA GPU Triplet Finder

A. Adinetz et al.

stream approach increases when more device connections are available. We expect that this
performance advantage of dynamic parallelism can apply to all algorithms which process their
workload in small independent bunches.

(a) Host streams approach with diﬀerent number
of device connections on Kepler K20X.

(b) Dynamic parallelism approach with diﬀerent
application clocks on Kepler K20X and K40.

Figure 6: Performance of GPU Triplet Finder with varying number of device connections and
application clock frequencies.
Fig. 6b shows the performance of our application on K20X and K40 GPUs at diﬀerent application clocks. Setting the clock frequency is supported in Kepler GPUs using the GPUBoost
feature. This allows applications to get higher performances if they do not fully utilize the GPU
thermal budget at default clock frequencies.
With the bunched version of the algorithm, the whole set of hits is processed in 22 ms on
K20X. Grouping of hits and assigning them to tubes is omitted as it is currently done on the
host. With preprocessing included, the total processing time is 24 ms using a single GPU. This
is equivalent to 162 ns/hit, or a rate of 6.19 Mhits/s. Running with highest luminosity, PANDA
produces roughly 1 600 Mhits/s in the STT. With 6.19 Mhits/s, our current GPU implementation of the Triplet Finder is one of the fastest online tracking algorithm in PANDA. As multiple
bunches can be processed independently, the Triplet Finder can be easily parallelized across
multiple computing nodes with GPUs. Together with our implementation this makes online
tracking a feasible technique for PANDA.

6

Summary and Outlook

In this paper, we discussed using a GPU implementation of the Triplet Finder algorithm for
online triggerless reconstruction in the PANDA experiment. We presented the idea of the algorithm and its GPU implementation. We discussed algorithmic aspects, such as using bunching and sector-rows, for better performance. We presented diﬀerent bunching strategies and
discussed GPU-speciﬁc optimizations. Performance evaluation shows that with dynamic parallelism, the GPU Triplet Finder can achieve processing rates of nearly 6 Mhits/s on a single
GPU. As it can be parallelized across multiple GPU compute nodes, our implementation makes
online tracking a feasible technique for PANDA.
Further work will continue in several directions. First, while the GPU Triplet Finder enables
fast processing rates, there’s still room for improvement. This includes using structure-of-arrays
(SoA) data layouts instead of the currently used AoS layout, which will help with memory
coalescing and reduce register pressure. This also includes introducing an approach similar
122

PANDA GPU Triplet Finder

A. Adinetz et al.

to sector-rows for testing against skewlets. Second, the Triplet Finder’s excellent performance
makes it possible to consider some modiﬁcations, which evaluate a larger number of tracks and
provide better reconstruction quality. Such modiﬁcations can, e.g., take into account displaced
tracks not passing through the (0, 0) point. Finally, there are other algorithms being developed
for PANDA online tracking, and porting them to GPU will certainly increase their performance.
As of today, with the achieved processing rate for a single GPU, roughly 260 K20X GPUs
would be required to run the Triplet Finder for PANDA’s STT sub-detector, which produces
1600 Mhits/s. If the performance of GPUs continues to double every 2 years, less than 100
GPUs would be required when the experiment starts in 2018.

References
[1] CUDA C Programming Guide, 2013.
[2] S. Amerio, A. Annovi, M. Bettini, M. Bucciantonio, P. Catastini, F. Crescioli, M. Dell’Orso,
B. Di Ruzza, P. Giannetti, D. Lucchesi, M. Nicoletto, and M. Piendibene. The Gigaﬁtter: An
online track ﬁtting processor for CDF experiment and beyond. In Nuclear Science Symposium
Conference Record (NSS/MIC), 2010 IEEE, pages 636–639, Oct 2010.
[3] S. Amerio et al. Online Track Reconstruction at Hadron Colliders. In Proceedings of the 35th
International Conference of High Energy Physics (ICHEP 2010). July 22-28, 2010. Paris, France.,
volume 1, page 481, 2010.
[4] R. Ammendola, A. Biagioni, L. Deri, M. Fiorini, O. Frezza, G. Lamanna, F. L. Cicero, A. Lonardo,
A. Messina, et al. GPUs for real-time processing in HEP trigger systems. Journal of Physics:
Conference Series, 2013.
[5] A. Ariga and T. Ariga. Fast 4π track reconstruction in nuclear emulsion detectors based on GPU
technology. ArXiv e-prints, Nov. 2013.
[6] D. Emeliyanov and J. Howard. GPU-Based Tracking Algorithms for the ATLAS High-Level
Trigger. Journal of Physics: Conference Series, 396(1):012018, 2012.
[7] S. Gorbunov, D. Rohr, K. Aamodt, T. Alt, et al. ALICE HLT High Speed Tracking on GPU.
Nuclear Science, IEEE Transactions on, 58(4):1845–1851, Aug 2011.
[8] V. Halyo, P. LeGresley, P. Lujan, V. Karpusenko, and A. Vladimirov. First Evaluation of the CPU,
GPGPU and MIC Architectures for Real Time Particle Tracking based on Hough Transform at
the LHC. arXiv preprint, Oct. 2013.
[9] I. Kisel, S. Gorbunov, U. Kebschull, V. Lindenstruth, and W. M¨
uller. Fast SIMDized Kalman
ﬁlter based track ﬁt. Computer Physics Communications, 178(5):374 – 383, 2008.
[10] A. Lebedev, C. H¨
ohne, I. Kisel, G. Ososkov, and the CBM collaboration. Track reconstruction algorithms for the CBM experiment at FAIR. Journal of Physics: Conference Series, 219(3):032048,
2010.
[11] M. C. Mertens for the PANDA collaboration. Triplet based online track ﬁnding in the PANDASTT. Journal of Physics: Conference Series (accepted for publication), 2014.
[12] PANDA Collaboration, W. Erni, I. Keshelashvili, B. Krusche, et al. Technical Design Report for
the: PANDA Straw Tube Tracker. The European Physical Journal A, 49(arXiv:1205.5441):1–104,
May 2012.
[13] PANDA Collaboration, W. Erni, I. Keshelashvili, B. Krusche, M. Steinacher, Y. Heng, Z. Liu,
H. Liu, X. Shen, O. Wang, H. Xu, et al. Physics Performance Report for PANDA: Strong Interaction Studies with Antiprotons. arXiv preprint, Mar. 2009.
[14] N. Pozzobon. Development of a Level 1 Track Trigger for the CMS experiment at the highluminosity LHC. Nuclear Instruments and Methods in Physics Research Section A: Accelerators,
Spectrometers, Detectors and Associated Equipment, 732(0):151 – 155, 2013. Vienna Conference
on Instrumentation 2013.

123

