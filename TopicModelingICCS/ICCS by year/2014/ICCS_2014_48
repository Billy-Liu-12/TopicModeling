Procedia Computer Science
Volume 29, 2014, Pages 145‚Äì158
ICCS 2014. 14th International Conference on Computational Science

Handling Data-skew EÔ¨Äects in Join
Operations using MapReduce
M. Al Hajj Hassan1 , M. Bamha2 , and F. Loulergue2
1

2

Lebanese International University, Beirut, Lebanon
mohamad.hajjhassan01@liu.edu.lb
Universit¬¥e Orl¬¥eans, INSA Centre Val de Loire, LIFO EA 4022, France
{mostafa.bamha,frederic.loulergue}@univ-orleans.fr

Abstract
For over a decade, MapReduce has become a prominent programming model to handle vast
amounts of raw data in large scale systems. This model ensures scalability, reliability and
availability aspects with reasonable query processing time. However these large scale systems
still face some challenges: data skew, task imbalance, high disk I/O and redistribution costs
can have disastrous eÔ¨Äects on performance.
In this paper, we introduce MRFA-Join algorithm: a new frequency adaptive algorithm
based on MapReduce programming model and a randomised key redistribution approach for
join processing of large-scale datasets. A cost analysis of this algorithm shows that our approach
is insensitive to data skew and ensures perfect balancing properties during all stages of join
computation. These performances have been conÔ¨Årmed by a series of experimentations.
Keywords: Join operations, Data skew, MapReduce model, Hadoop framework

1

Introduction

Today with the rapid development of network technologies, internet search engines, data mining
applications and data intensive scientiÔ¨Åc computing applications, the need to manage and query
a huge amount of datasets every day becomes essential. Parallel processing of such queries
on hundreds or thousands of nodes is obligatory to obtain a reasonable processing time [6].
However, building parallel programs on parallel and distributed systems is complicated because
programmers must treat several issues such as load balancing and fault tolerance. Hadoop [14]
and Google‚Äôs MapReduce model [8] are examples of such systems. These systems are built from
thousands of commodity machines and assure scalability, reliability and availability aspects [9].
To reduce disk I/O, each Ô¨Åle in such storage systems is divided into chunks or blocks of data
and each block is replicated on several nodes for fault tolerance. Parallel programs are easily
written on such systems following the MapReduce paradigm where a program is composed of
a workÔ¨Çow of user deÔ¨Åned map and reduce functions.
Selection and peer-review under responsibility of the ScientiÔ¨Åc Programme Committee of ICCS 2014
c The Authors. Published by Elsevier B.V.
doi:10.1016/j.procs.2014.05.014

145

Handling Data-skew EÔ¨Äects in Join Operations using MapReduce

M. Bamha, F. Loulergue et al.

Join operation is one of the most widely used operations in relational database systems,
but it is also a heavily time consuming operation. For this reason it was a prime target for
parallelization. The join of two relations R and S on attribute A of R and attribute B of S (A
and B of the same domain) is the relation, written R ‚ú∂ S , obtained by concatenating pairs of
tuples from R and S for which R.A = S.B.
Parallel join usually proceeds in two phases: a redistribution phase (generally based on
join attribute hashing and therefore called hashing algorithms) and then a sequential join of
local fragments. Many parallel join algorithms have been proposed. The principal ones are:
Sort-merge join, Simple-hash join, Grace-hash join and Hybrid-hash join [12]. All of them are
based on hashing functions which redistribute relations such that all the tuples having the
same join attribute value are forwarded to the same node. Local joins are then computed and
their union is the output relation. Research has shown that join is parallelizable with nearlinear speed-up on distributed architectures but only under ideal balancing conditions: data
skew may have disastrous eÔ¨Äects on the performance [13, 10]. To this end, several parallel
algorithms were presented to handle data skew while treating join queries on parallel database
systems [2, 3, 1, 13, 7, 10].
The aim of join operations is to combine information from two or more data sources. Unfortunately, MapReduce framework is somewhat ineÔ¨Écient to perform such operations since
data from one source must be maintained in memory for comparison to other source of data.
Consequently, adapting well-known join algorithms to MapReduce is not as straightforward as
one might hope, and MapReduce programmers often use simple but ineÔ¨Écient algorithms to
perform join operations especially in the presence of skewed data [11, 4, 9].
In [15], three well known algorithms for join evaluation were implemented using an extended
MapReduce model. These algorithms are Sort-Merge-Join, Hash-Join and Block Nested-Loop
Join. Combining this model with a distributed Ô¨Åle system facilitates the task of programmers
because they don‚Äôt need to take care of fault tolerance and load balancing issues. However,
load balancing in the case of join operations is not straightforward in the presence of data-skew.
In [4] Blanas et al. have presented an improved versions of MapReduce sort-merge joins and
semi-join algorithms for log processing to Ô¨Åx the problem of buÔ¨Äering all records from both
inner and outer relations. For the same reasons as in parallel database management systems
(PDBMS), even in the presence of integrated functionality for load balancing and fault tolerance
in MapReduce, these algorithms still suÔ¨Äer from the eÔ¨Äect of data skew. Indeed all the tuples
having the same values in map phase are sent to the same reducer which limits the scalability
of the presented algorithms [9].
In this paper we are interested in the evaluation of join operations on large scale systems
using MapReduce. To avoid the eÔ¨Äect of data skew, we introduce the MapReduce Frequency
Adaptive Join algorithm (MRFA-Join) based on distributed histograms and a randomised key
redistribution approach. This algorithm, inspired from our previous research on join and semijoin operations in PDBMS, is well adapted to manage huge amount of data on large scale
systems even for highly skewed data. The remaining of the paper is organised as follows. In
section 2 we brieÔ¨Çy present the MapReduce programming model. Section 3 is devoted to the
MRFA-Join algorithm and its complexity analysis. Experiments presented in section 4 conÔ¨Årm
the eÔ¨Éciency of our approach. We conclude and give further research directions in section 5.

2

The MapReduce Programming Model

MapReduce [6] is a simple yet powerful framework for implementing distributed applications
without having extensive prior knowledge of issues related to data redistribution, task allocation
146

Handling Data-skew EÔ¨Äects in Join Operations using MapReduce

M. Bamha, F. Loulergue et al.

or fault tolerance in large scale distributed systems.
Google‚Äôs MapReduce programming model presented in [6] is based on two functions: map
and reduce, that the programmer is supposed to provide to the framework. These two functions
should have the following signatures:
map:
reduce:

(k1 , v1 ) ‚àí‚Üí list(k2 , v2 ),
(k2 , list(v2 )) ‚àí‚Üí list(v3 ).

The user must write the map function that has two input parameters, a key k1 and an associated
value v1 . Its output is a list of intermediate key/value pairs (k2 , v2 ). This list is partitioned
by the MapReduce framework depending on the values of k2 , where all pairs having the same
value of k2 belong to the same group.
The reduce function, that must also be written by the user, has two parameters as input:
an intermediate key k2 and a list of intermediate values list(v2 ) associated with k2 . It applies
the user deÔ¨Åned merge logic on list(v2 ) and outputs a list of values list(v3 ).

	



Mapper





split
split
split
split
split
split
split
split
split

	

bucket
bucket
bucket

	
	


Reducer
Mapper

bucket
bucket
bucket

Mapper

bucket
bucket
bucket

Reducer

split
split
split
split
split
split
split

Reducer
Mapper

bucket
bucket
bucket

	


	


	

	

Figure 1: Map-reduce framework.

In this paper, we used an open source version of MapReduce called Hadoop developed by
‚ÄùThe Apache Software Foundation‚Äù. Hadoop framework includes a distributed Ô¨Åle system called
HDFS1 designed to store very large Ô¨Åles with streaming data access patterns.
For eÔ¨Éciency reasons, in Hadoop MapReduce framework, users may also specify a ‚ÄúCombine
function‚Äù, to reduce the amount of data transmitted from Mappers to Reducers during shuÔ¨Ñe
phase (see Ô¨Åg 1). The ‚ÄúCombine function‚Äù is like a local reduce applied (at map worker) before
storing or sending intermediate results to the reducers. The signature of combine function is:
combine:

(k2 , list(v2 )) ‚àí‚Üí (k2 , list(v3 )).

To cover a large range of applications needs in term of computation and data redistribution,
in Hadoop framework, the user can optionally implement two additional functions : init() and
close() called before and after each map or reduce task. The user can also specify a ‚Äúpartition
function‚Äù to send each key k2 generated in map phase to a speciÔ¨Åc reducer destination. The
reducer destination may be computed using only a part of the input key k2 . The signature of
the partition function is:
partition:
1 HDFS:

k2 ‚àí‚Üí Integer,

Hadoop Distributed File System.

147

Handling Data-skew EÔ¨Äects in Join Operations using MapReduce

M. Bamha, F. Loulergue et al.

where the output of partition should be a positive number strictly smaller than the number of
reducers. Hadoop‚Äôs default partition function is based on ‚Äúhashing‚Äù the whole input key k2 .

3

A MapReduce Skew Insensitive Join Algorithm

As stated in the introduction section, MapReduce hash based join algorithms presented in
[4, 15] may be ineÔ¨Écient in the presence of highly skewed data[11] due to the fact that in Map
function in these algorithms, all the key-value pairs (k1 , v1 ) representing the same entry for the
join attribute are sent to the same reducer (In Map phase, emitted key-value pairs (k2 , v2 ), key
k2 is generated by only using join attribute values in the manner that all records with the same
join attribute value will be forwarded to the same reducer).
To avoid the eÔ¨Äect of repeated keys, Map user-deÔ¨Åned function should generate distinct
output keys k2 even for records having the same join attribute value. This is made possible by
using a user deÔ¨Åned partitioning function in Hadoop : the reducer destination for a key k2 can be
computed from diÔ¨Äerent parts of key k2 and not by a simple hashing of all input key k2 . To this
end, we introduce, in this section, a join algorithm called MRFA-Join (MapReduce Frequency
Adaptive Join) based on distributed histograms and a random redistribution of repeated join
attribute values combined with an eÔ¨Écient technique of redistribution where only relevant data
is redistributed across the network during the shuÔ¨Ñe phase of reduce step. A cost analysis for
MRFA-Join is also presented to give for each computation step, an upper bound of execution
time in order to prove the strength of our approach.
In this section, we describe the implementation of MRFA-Join using Hadoop MapReduce
framework as it is, without any modiÔ¨Åcation. Therefore, the support for fault tolerance and load
balancing in MapReduce and Distributed File System are preserved if possible: the inherent
load imbalance due to repeated values must be handled eÔ¨Éciently by the join algorithm and
not by the MapReduce framework.
To compute the join, R ‚ú∂ S, of two relations (or datasets) R and S, we assume that input
relations R and S are divided into blocks (splits) of data. These splits are stored in Hadoop
Distributed File System (HDFS). These splits are also replicated on several nodes for reliability
issues. Throughout this paper, for a relation T ‚àà {R, S}, we use the following notations:
‚Ä¢ |T |: number of pages (or blocks of data) forming T ,
‚Ä¢ ||T ||: number of tuples (or records) in relation T ,
‚Ä¢ T : the restriction (a fragment) of relation T which contains tuples which appear in the
join result. ||T || is, in general, very small compared to ||T ||,
‚Ä¢ Timap : the split(s) of relation T aÔ¨Äected to mapper i,
‚Ä¢ Tired : the split(s) of relation T aÔ¨Äected to reducer i,
‚Ä¢ Ti : the split(s) of relation T aÔ¨Äected to mapper i,
‚Ä¢ ||Ti ||: number of tuples in split Ti ,
‚Ä¢ Histmap (Timap ): Mapper‚Äôs local histogram of Timap , i.e. the list of pairs (v, nv ) where v is
a join attribute value and nv its corresponding frequency in relation Timap on mapper i,
‚Ä¢ Histred
i (T ) : the fragment of global histogram of relation T on reducer i,
‚Ä¢ Histred
i (T )(v) is the global frequency nv of value v in relation T ,
‚Ä¢ HistIndex(R ‚ú∂ S): join attribute values that appear in both R and S and their corresponding three parameters: Frequency index, Nb buckets1 and Nb buckets2 used in communication templates,
‚Ä¢ cr/w : read/write cost of a page of data from/to distributed Ô¨Åle system (DFS),
‚Ä¢ ccomm : communication cost per page of data,
148

Handling Data-skew EÔ¨Äects in Join Operations using MapReduce

‚Ä¢
‚Ä¢
‚Ä¢
‚Ä¢

M. Bamha, F. Loulergue et al.

tis : time to perform a simple search in a Hashtable on node i,
tih : time to add an entry to a Hashtable on node i,
N B mappers: number of job mapper nodes,
N B reducers: number of job reducer nodes.

We will describe MRFA-Join algorithm while giving a cost analysis for each computation
phase. Join computation in MRFA-Join proceeds in two MapReduce jobs:
a. the Ô¨Årst map-reduce job is performed to compute distributed histograms and to create
randomized communication templates to redistribute only relevant data while avoiding
the eÔ¨Äect of data skew,
b. the second one, is used to generate join output result by using communication templates
carried out in the previous step.
In the following, we will describe MRFA-Join steps while giving an upper bound on the execution
time of each MapReduce step. The O(. . .) notation only hides small constant factors: they only
depend on program‚Äôs implementation but neither on data nor on machine parameters. Data
redistribution in MRFA-Join algorithm is the basis for eÔ¨Écient and scalable join processing while
avoiding the eÔ¨Äect of data skew in all the stages of join computation. MRFA-Join algorithm
(see Algorithm 1) proceeds in 4 steps:
Algorithm 1 MRFA-join algorithm workÔ¨Çow /* See Appendix for detailed implementation */
Map phase: /* To generate a tagged ‚ÄúLocal histogram‚Äù for input relations */
Each mapper i reads its assigned data splits (blocks) of relation Rimap and Simap from the DFS
Extract the join key value from input relation‚Äôs record.
Get a tag to identify source input relation.
Emit a couple ((join key,tag),1) /* a tagged join key with a frequency 1 */
Combine phase: To compute local frequencies for join key values in relations Rimap and Simap
‚úÑ Each combiner, for each pair (join key,tag) computes the sum of generated local frequencies
associated to the join key value in each tagged join key generated in Map phase.
Partition phase:
‚úÑ for each emitted tagged join key, compute reducer destination according to only join key value.
a.2 Reduce phase: /* To combine ShuÔ¨Ñe‚Äôs records and to create Global Join histogram index */
‚úÑ Compute the global frequencies for only join key values present in both relations R and S.
‚úÑ Emit, for each join key, a couple (join key,(frequency index,Nb buckets1, Nb buckets2)).
b.1 Map phase:
‚úÑ Each mapper reads join result global histogram index from DFS, and creates a local Hashtable.
‚úÑ Each mapper, i, reads its assigned data splits of input relations from DFS and generates
randomized communication templates for records in Rimap and Simap according to join key value
and its corresponding frequency index in HashTable. In communication templates, only relevant
records from Rimap and Simap are emitted using hash or a randomized partition/replicate schema.
‚úÑ Emit relevant randomised tagged records from relations Rimap and Simap .
Partition phase:
‚úÑ For each emitted tagged join key, compute reducer destination according to values of join key,
and random reducer destination generated in Map phase;
b.2 Reduce phase: to combine ShuÔ¨Ñe‚Äôs output records and to generate join result.
a.1

‚úÑ
‚úÑ
‚úÑ
‚úÑ

a.1: Map phase to generate a tagged ‚Äúlocal histogram‚Äù for input relations:
In this step, each mapper i reads its assigned data splits (blocks) of relation R and S from
distributed Ô¨Åle system (DFS) and emits a couple (<K,tag>,1) for each record in Rimap (resp.
Simap ) where K is join key value and tag represents input relation tag. The cost of this step is :
T ime(a.1.1) = O

N B mappers

max
i=1

cr/w ‚àó (|Rimap | + |Simap |) +

N B mappers

max
i=1

(||Rimap || + ||Simap ||) .

149

Handling Data-skew EÔ¨Äects in Join Operations using MapReduce

M. Bamha, F. Loulergue et al.

Emitted couples (<K,tag>,1) are then combined and partitioned using a user deÔ¨Åned partitioning function by hashing only key part K and not the whole mapper tagged key <K,tag>. The
result of combine phase is then sent to reducers destination in the shuÔ¨Ñe phase of the following
reduce step. The cost of this step is at most : T ime(a.1.2) =
O

N B mappers

max
i=1

||Histmap (Rimap )|| ‚àó log ||Histmap (Rimap )|| + ||Histmap (Simap )||‚àó
log ||Histmap (Simap )||) + ccomm ‚àó (|Histmap (Rimap )| + |Histmap (Simap )|

.

And the global cost of this step is: T imestepa.1 = T ime(a.1.1) + T ime(a.1.2).
We recall that, in this step, only local histograms Histmap (Rimap ) and Histmap (Simap ) are sorted
and transmitted across the network and the sizes of these histograms are very small compared
to the size of input relations Rimap and Simap owing to the fact that, for a relation T , Histmap (T )
contains only distinct entries of the form (v, nv ) where v is a join attribute value and nv the
corresponding frequency.
a.2: Reduce phase to create join result global histogram index and randomized
communication templates for relevant data:
red
At the end of shuÔ¨Ñe phase, each reducer i will receive a fragment of Histred
i (R) (resp. Histi (S))
map
map
map
map
obtained through hashing of distinct values of Hist (Rj ) (resp. Hist (Sj )) of each
red
mapper j. Received Histred
i (R) and Histi (S) are then merged to compute global histogram
HistIndexi (R ‚ú∂ S) on each reducer i. HistIndex(R ‚ú∂ S) is used to compute randomized communication templates for only records associated to relevant join attribute values (i.e. values
which will eÔ¨Äectively be present in the join result).
In this step, each reducer i, computes the global frequencies for join attribute values which
are present in both left and right relations and emits, for each join attribute K, an entry of the
form : (K,<Frequency index(K),Nb buckets1(K),Nb buckets2(K)>) where:
‚Ä¢ F requency index(K) ‚àà {0, 1, 2} will allow us to decide if, for a given relevant join attribute
value K, the frequencies of tuples of relations R and S having the value K are greater
(resp. smaller) than a deÔ¨Åned threshold frequency f0 . It also permits us to choose
dynamically the probe and the build relation for each value K of the join attribute. This
choice reduces the global redistribution cost to a minimum.
For a given join attribute value K ‚àà HistIndexi (R ‚ú∂ S),
‚éß
red
Frequency index(K)=0 If Histred
‚é™
i (R)(K) < f0 and Histi (S)(K) < f0
‚é™
‚é™
‚é™
(i.e. values associated to low frequencies in both relations),
‚é™
‚é™
‚é®
red
red
Frequency index(K)=1 If Histred
i (R)(K) ‚â• f0 and Histi (R)(K) ‚â• Histi (S)(K)
(i.e. Frequency in relation R is higher than those of S),
‚é™
‚é™
‚é™
red
red
‚é™
‚é™
Frequency index(K)=2 If Histred
i (S)(K) ‚â• f0 and Histi (S)(K) > Histi (R)(K)
‚é™
‚é©
(i.e. Frequency in relation S is higher than those of R).

‚Ä¢ Nb buckets1(K): is the number of buckets used to partition records of relation associated
to the highest frequency for join attribute value K,
‚Ä¢ Nb buckets2(K): is the number of buckets used to partition records of relation associated
to the lowest frequency for join attribute value K.
For a join attribute value K, the number of buckets N b buckets1(K) and N b buckets2(K) are
generated in a manner that each bucket will Ô¨Åt in reducer‚Äôs memory. This makes the algorithm
insensitive to the eÔ¨Äect of data skew even for highly skewed input relations.
Figure 2 gives an example of communication templates used to partition data for HistIndex
entry (K,<Frequency index(K),Nb buckets1(K),Nb buckets2(K)>) corresponding to a join attribute
150

Handling Data-skew EÔ¨Äects in Join Operations using MapReduce

M. Bamha, F. Loulergue et al.

(K,Tag1)
(K,Tag2)
(K,i3,1,Tag1)

(K,i0,1,Tag1)

(K,i1,1,Tag1)

(K,i0 ,2,Tag2,0)

(K,i1,2,Tag2,0)

(K,i2,2,Tag2,0)

(K,i3,2,Tag2,0)

(K,i0,2,Tag2,1)

(K,i1,2,Tag2,1)

(K,i2,2,Tag2,1)

(K,i3,2,Tag2,1)

(K,i4,2,Tag2,1)

(K,i0,2,Tag2,2)

(K,i1,2,Tag2,2)

(K,i2,2,Tag2,2)

(K,i3,2,Tag2,2)

(K,i4,2,Tag2,1)

	


		


		


		


		


(K,i2,1,Tag1)

(K,i4,1,Tag1)
(K,i4,2,Tag2,0)

Figure 2: Generated buckets associated to a join key K corresponding to a high frequency where records
from relation associated to T ag1 (i.e relation having the highest frequency) are partitioned into Ô¨Åve
buckets and those of relation associated to T ag2 are partitioned into three buckets.

K associated to a high frequency, into small buckets. In this example, data associated to relation corresponding to T ag1 is partitioned into 5 buckets (i.e. N b buckets1(K) = 5) where as
those of relation corresponding to T ag2 is partitioned into 3 buckets (i.e. N b buckets2(K) = 3).
For these buckets, appropriate map keys are generated so that all records in each bucket of relation associated to T ag1 are forwarded to the same reducer holding all the buckets of relation
associated to T ag2 . This partitioning guarantees that join tasks, are generated in a manner
that the input data for each join task will Ô¨Åt in the memory of processing node and never exceed
a user deÔ¨Åned size, even for highly skewed data.
Using HistIndex information, each reducer i, has local knowledge of how relevant records
of input relations will be redistributed in the next map phase. The global cost of this step is
B reducers
red
(||Histred
at most: T imestepa.2 = O maxN
i=1
i (R)|| + ||Histi (S)||) .
red
red
Note that, HistIndex(R ‚ú∂ S) ‚â° ‚à™i (Histi (R)‚à©Histi (S)) and ||HistIndex(R ‚ú∂ S)|| is very small
compared to ||Histred (R)|| and ||Histred (S)||.
To guarantee a perfect balancing of the load among processing nodes, communication templates are carried out jointly by all reducers (and not by a coordinator node) for only join
attribute values which are present in join result : Each reducer deals with the redistribution of
the data associated to a subset of relevant join attribute values.
b.1: Map phase to create a local hash table and to redistribute relevant data using
randomized communication templates:
In this step, each mapper i reads join result global histogram index, HistIndex, to create a local
B mappers i
hash table in time: T ime(b.1.1) = O(maxN
th ‚àó ||HistIndex(R ‚ú∂ S)||).
i=1
Once local hash table is created on each mapper, input relations are then read from DFS, and
each record is either discarded (if record‚Äôs join key is not present in the local hash table) or
routed to a designated random reducer destination using communication templates computed
in step a.2 (Map phase details are described in Algorithm 6). The cost of this step is :
T ime(b.1.2) = O

N B mappers

max
i=1

map

||Ri

(cr/w ‚àó (|Rimap | + |Simap |) + tis ‚àó (||Rimap || + ||Simap ||)+
map

|| ‚àó log ||Ri

map

|| + ||S i

map

|| ‚àó log ||S i

map

|| + ccomm ‚àó (|Ri

map

| + |S i

|)) .

The term cr/w ‚àó (|Rimap | + |Simap |) is time to read input relations from DFS on each mapper
151

Handling Data-skew EÔ¨Äects in Join Operations using MapReduce

M. Bamha, F. Loulergue et al.

i, the term tis ‚àó (||Rimap || + ||Simap ||) is the time to perform a hash table search for each input
map
map
map
map
record, ||Ri || ‚àó log ||Ri || + ||S i || ‚àó log ||S i || is time to sort relevant data on mapper i,
map
map
where as the term ccomm ‚àó (|Ri | + |S i |)) is time to communicate relevant data from mappers
to reducers, using our communication templates described in step a.2. Hence the global cost of
this step is: T imestepb.1 = T ime(b.1.1) + T ime(b.1.2).
We recall that, in this step, only relevant data is emitted by mappers (which reduces communication cost in the shuÔ¨Ñe step to a minimum) and records associated to high frequencies
(those having a large eÔ¨Äect on data skew) are redistributed according to an eÔ¨Écient dynamic
partition/replicate schema to balance load among reducers and avoid the eÔ¨Äect of data skew.
However records associated to low frequencies (these records have no eÔ¨Äect on data skew) are
redistributed using hashing functions.
b.2: Reduce phase to compute join result:
red
red
At the end of step b.1, each reducer i receives a fragment Ri (resp. S i ) obtained through
map
map
randomized hashing of Rj (resp. S j ) of each mapper j and performs a local join of received
data. This reduce phase is described in detail in Algorithm 8. The cost of this step is:
T imestepb.2 = O(

N B reducers

max
i=1

red

red

red

(||Ri || + ||S i || + cr/w ‚àó |Ri

red

‚ú∂ S i |).

The global cost of MRFA-Join is therefore the sum of the above four steps :
T imeM RF A‚àíJoin = T imestepa.1 + T imestepa.2 + T imestepb.1 + T imestepb.2

Using hashing technique, the join computation of R ‚ú∂ S requires at least the following lower
bound : boundinf =
Œ©

N B mappers

max
i=1

(cr/w + ccomm ) ‚àó (|Rimap | + |Simap |) + ||Rimap || ‚àó log ||Rimap || + ||Simap || ‚àó log ||Simap ||
+

N B reducers

max
i=1

||Rired || + ||Sired || + cr/w ‚àó |Rired ‚ú∂ Sired |

,

where cr/w ‚àó (|Rimap | + |Simap |) is the cost of reading input relations from DFS on node i. The
term ||Rimap ||‚àólog ||Rimap ||+||Simap ||‚àólog ||Simap || represents the cost to sort input relations records
on map phase. The term ccomm ‚àó (|Rimap | + |Simap |) represents the cost to communicate data from
mappers to reducers, the term ||Rired || + ||Sired || is time to scan input relations on reducer i and
cr/w ‚àó |Rired ‚ú∂ Sired | represents the cost to store reducer‚Äôs i join result on the DFS.
MRFA-Join algorithm has asymptotic optimal complexity when: ||HistIndex(R ‚ú∂ S)||
‚â§ max

N B mappers

max
i=1

(||Rimap || ‚àó log ||Rimap ||, |Simap || ‚àó log ||Simap ||),

N B reducers

max
i=1

||Rired ‚ú∂ Sired ||) , (1)

this is due to the fact that, all other terms in T imeM RF A‚àíJoin are bounded by those of
boundinf . Inequality 1 holds, in general, since HistIndex(R ‚ú∂ S) contains only distinct values
that appear in both relations R and S.
Remark: In practice, data imbalance related to the use of hashing functions can be due to:
‚Ä¢ a bad choice of used hash function. This imbalance can be avoided by using the hashing
techniques presented in the literature making it possible to distribute evenly the values
of the join attribute with a very high probability [5],
‚Ä¢ an intrinsic data imbalance which appears when some values of the join attribute appear more frequently than others. By deÔ¨Ånition a hash function maps tuples having
the same join attribute values to the same processor. There is no way for a clever
152

Handling Data-skew EÔ¨Äects in Join Operations using MapReduce

M. Bamha, F. Loulergue et al.

hash function to avoid load imbalance that results from these repeated values [7]. But
this case cannot arise here owing to the fact that histograms contain only distinct values
of the join attribute and the hashing functions we use are always applied to histograms
or applied to randomized keys.

4

Experiments

To evaluate the performance of MRFA-Join algorithm presented in this paper, we compared our
algorithm to the best known solutions called respectively Improved Repartition Join and
Standard Repartition Join. Improved Repartition Join was introduced by Blanas et al.
in [4], where as Standard Repartition Join is the join algorithm provided in Hadoop framework‚Äôs contributions. We ran a large series of experiments where 60 Virtual Machines (VMs)
were randomly selected from our university cluster using OpenNubula software for VMs administration. Each Virtual Machine has the following characteristics : 1 Intel(R) Xeon@2.53GHz
CPU, 4 Cores, 2GB of Memory and 100GB of Disk. Setting up a Hadoop cluster consisted of
deploying each centralised entity (namenode and jobtracker) on a dedicated Virtual Machine
and co-deploying datanodes and tasktrackers on the rest of VMs . The data replication parameter was Ô¨Åxed to three in the HDFS conÔ¨Åguration Ô¨Åle.
To study the eÔ¨Äect of data skew on performance, join attribute values in the generated data
have been chosen to follow a Zipf distribution [16] as it is the case in most database tests: Zipf
factor was varied from 0 (for a uniform data distribution) to 1.0 (for a highly skewed data).
Input relations size was Ô¨Åxed to 400M records for the right relation (‚àº40GB of data) and 10M
of records for the left relation ‚àº1GB of data) and the join result varying from approximately
35M to 1700M records (corresponding respectively to about 7GB and 340GB of output data).
We noticed in all the tests and also those presented in Figure 3, that our MRFA-Join algorithm outperforms both Improved Repartition Join and Standard Repartition Join algorithms even for low or moderated skew. We recall that our algorithm requires the scan of input
data twice. The Ô¨Årst scan is performed for histogram processing and the second one for join processing. The cost analysis and tests performed showed that the overhead related to histogram
processing is compensated by the gain in join processing since only relevant data (that appears
in the join result) is emitted by mappers in the map phase which reduce considerably the amount
of data transmitted over the network in shuÔ¨Ñe phase (see Figure 4). Moreover, for skew factors
varying from 0.6 to 1.0, both Improved Repartition Join and Standard Repartition Join
jobs fail due to lack of memory. This is due to the fact that, in the reduce phase, all the records
emitted by the mappers having the same join key are sent and processed by the same reducer
which makes both Improved Repartition Join and Standard Repartition Join algorithms
very sensitive to data skew and limits their scalability. This cannot occur in MRFA-Join owing
to the fact that attribute values associated to high frequencies are forwarded to distinct reducers
using randomised join attribute keys and not by a simple hashing of record‚Äôs join key.

5

Conclusion and Future Work

In this paper, we have introduced the Ô¨Årst skew-insensitive join algorithm, called MRFA-Join,
using MapReduce, based on distributed histograms and randomised keys redistribution approach for highly skewed data. The detailed information provided by these histograms, allows
us to reduce communication costs to only relevant data while guaranteeing perfect balancing
processing due to the fact that all the generated join tasks and buÔ¨Äered data never exceed a user
153

Handling Data-skew EÔ¨Äects in Join Operations using MapReduce

M. Bamha, F. Loulergue et al.


!**+,&+
-,.++&





' *$/'0
' *$/'0



' *$/'0
' *$/'0



' *$/'0
' *$/'0



' *$/'0
' *$/'0

-,.+


' *$/'0
' *$/'0



(&)*+,&+



  	  
     
 !"#$%&'&

('&)*+&!),+-),
!,!+&!),+-),
./+-),+0&),1



-)!$2)'')
-)!$2)'')

-)!$2)'')
-)!$2)'')

-)!$2)'')
-)!$2)'')

./+-),
-)!$2)'')
-)!$2)'')












-)!$2)'')
-)!$2)'')



Figure 3: Data skew eÔ¨Äect on Hadoop join processing time

     	 
   
 !"#$%&&!!'

Figure 4: Data skew eÔ¨Äect on the amount of data moved across the network during shuÔ¨Ñe phase

deÔ¨Åned size using threshold frequencies. This makes the algorithm scalable and outperforming
existing MapReduce join algorithms which fail to handle skewed data whenever a join task
cannot Ô¨Åt in the available node‚Äôs memory. It is to be noted that MRFA-Join can also beneÔ¨Åt
from MapReduce underlying load balancing framework in a heterogeneous or a multi-user environment since MRFA-Join is implemented without any change in the MapReduce framework.
Our experience with join operations shows that the overhead related to distributed histograms
processing remains very small compared to the gain in performance and communication costs
since only relevant data is processed or redistributed across the network.
We expect a higher gain related to histograms preprocessing in complex queries computation
due to the fact that histograms can be used to reduce drastically the costs of communication
and disk I/O of intermediate data by generating only relevant data for each sub-query. We will
explore these aspects in the context of more complex and pipelined join queries.

References
[1] M. Bamha and G. Hains. Frequency-adaptive join for Shared Nothing machines. Parallel and
Distributed Computing Practices, 2(3):333‚Äì345, 1999.
[2] Mostafa Bamha. An optimal and skew-insensitive join and multi-join algorithm for distributed
architectures. In DEXA, volume 3588 of LNCS, pages 616‚Äì625. Springer, 2005.

154

Handling Data-skew EÔ¨Äects in Join Operations using MapReduce

M. Bamha, F. Loulergue et al.

[3] Mostafa Bamha and Ga¬¥etan Hains. A skew-insensitive algorithm for join and multi-join operation
on Shared Nothing machines. In DEXA, volume 1873 of LNCS, pages 644‚Äì653. Springer, 2000.
[4] Spyros Blanas, Jignesh M. Patel, Vuk Ercegovac, Jun Rao, Eugene J. Shekita, and Yuanyuan Tian.
A comparison of join algorithms for log processing in MapReduce. In SIGMOD, pages 975‚Äì986.
ACM, 2010.
[5] J. Lawrence Carter and Mark N. Wegman. Universal Classes of Hash Functions. Journal of
Computer and System Sciences, 18(2):143‚Äì154, 1979.
[6] JeÔ¨Ärey Dean and Sanjay Ghemawat. MapReduce: SimpliÔ¨Åed Data Processing on Large Clusters.
In OSDI, pages 137‚Äì150. USENIX Association, 2004.
[7] D. J. DeWitt, J. F. Naughton, D. A. Schneider, and S. Seshadri. Practical Skew Handling in
Parallel Joins. In VLDB, pages 27‚Äì40, 1992.
[8] Ralf L¬®
ammel. Google‚Äôs MapReduce programming model ‚Äì Revisited. Science of Computer Programming, 70(1):1‚Äì30, 2008.
[9] Kyong-Ha Lee, Yoon-Joon Lee, Hyunsik Choi, Yon Dohn Chung, and Bongki Moon. Parallel Data
Processing with MapReduce: A Survey. ACM SIGMOD Record, 40(4):11‚Äì20, 2011.
[10] A. N. Mourad, R. J. T. Morris, A. Swami, and H. C. Young. Limits of parallelism in hash join
algorithms. Performance Evaluation, 20(1/3):301‚Äì316, 1994.
[11] Andrew Pavlo, Erik Paulson, Alexander Rasin, Daniel J. Abadi, David J. Dewitt, Samuel Madden,
and Michael Stonebraker. A comparison of approaches to large-scale data analysis. In SIGMOD,
pages 165‚Äì178. ACM, 2009.
[12] D. Schneider and D. DeWitt. A Performance Evaluation of Four Parallel Join Algorithms in a
Shared-Nothing Multiprocessor Environment. In SIGMOD. ACM, 1989.
[13] M. Seetha and P. S. Yu. EÔ¨Äectiveness of parallel joins. IEEE, Transactions on Knowledge and
Data Enginneerings, 2(4):410‚Äì424, 1990.
[14] Tom White. Hadoop ‚Äì The DeÔ¨Ånitive Guide. O‚ÄôReilly, second edition, 2010.
[15] Hung-Chih Yang, Ali Dasdan, Ruey-Lung Hsiao, and D. Stott Parker. Map-reduce-merge: simpliÔ¨Åed relational data processing on large clusters. In SIGMOD, pages 1029‚Äì1040. ACM, 2007.
[16] G. K. Zipf. Human Behavior and the Principle of Least EÔ¨Äort: An Introduction to Human Ecology.
Adisson-Wesley, 1949.

A

Appendix: Implementation of MRFA-Join functions

Algorithm 2 Map function /* To generate local histograms values and tag input relation records */
map(K: null, V : a record from a split of either relation R or S) {
‚úÑ relation tag ‚Üê get relation tag from current relation split;
‚úÑ join key ‚Üê extract the join column from record V of relation R;
‚úÑ Emit ((join key,relation tag), 1);
}

Algorithm 3 Combine function: /* To compute local histogram‚Äôs frequencies for join key */
combine(Key K,List List V ) {

}

/* List V is the list of values ‚Äú1‚Äù corresponding to the unique
frequencies in relation Ri or Si emitted by Mappers */
‚úÑ frequency ‚Üê sum of frequencies in List V ;
‚úÑ Emit (K,frequency);

155

Handling Data-skew EÔ¨Äects in Join Operations using MapReduce

M. Bamha, F. Loulergue et al.

Algorithm 4 Partitioning function /* Returns for, each composite key K=(join key,relation tag)
emitted in Map phase, an integer corresponding to destination reducer for the input key K. */
int partition(K: input key ){
‚úÑ join key ‚Üê K.join key;
/* extracts join key part from input key K */
‚úÑ Return (HashCode(join key) % NB reducers);
}

Algorithm 5 Reduce function /* To compute HistIndex(R ‚ú∂ S) Global histogram index */
void reduce init(){
hash index ‚Üê 0;
/* a Ô¨Çag to identify low frequencies records to redistribute using hashing */
partition index ‚Üê 1; /* a Ô¨Çag to identify relation‚Äôs records to partition */
replicate index ‚Üê 2 ; /* a Ô¨Çag to identify relation‚Äôs records to replicate */
last inner key ‚Üê ‚Äù‚Äù ; /* to store the last processed key in inner relation */
last inner frequency=0; /* to store the frequency of the last processed key in inner relation */
/* THRESHOLD FREQ: a user deÔ¨Åned threshold frequency used for communication templates */
}
reduce(Key K,List List V ) {/* List V :list of local frequencies of join key in either Rimap or Simap */
‚úÑ join key ‚Üê K.join key;
/* extracts join key part from input key K */
‚úÑ relation tag ‚Üê K.relation tag; /* extracts relation tag part from input key K */
If (relation tag corresponds to inner relation ) Then
‚úÑ last inner key ‚Üê join key;
‚úÑ last inner frequency ‚Üê sum of frequencies in List V ;
Else If (join key = last inner key) Then
‚úÑ frequency ‚Üê sum of frequencies in List V ;
If ((last inner frequency<THRESHOLD FREQ) and (frequency<THRESHOLD FREQ) Then
‚úÑ Emit (join key, (hash index,1,1));
ElseIf (last inner frequency ‚â• frequency)
‚úÑ Nb buckets1 ‚Üê last inner frequency / THRESHOLD FREQ ;
‚úÑ Nb buckets2 ‚Üê frequency / THRESHOLD FREQ ;
‚úÑ Emit (join key, (partition index,Nb buckets1,Nb buckets2));
Else
‚úÑ Nb buckets1 ‚Üê frequency / THRESHOLD FREQ ;
‚úÑ Nb buckets2 ‚Üê last inner frequency / THRESHOLD FREQ ;
‚úÑ Emit (join key, (replicate index,Nb buckets1,Nb buckets2));
End If;
End If;
End If;
}

156

Handling Data-skew EÔ¨Äects in Join Operations using MapReduce

M. Bamha, F. Loulergue et al.

Algorithm 6 Map function: /* To generate relevant randomized tagged records for input relations
using HistIndex communication templates.*/
void map init(){
inner tag ‚Üê 1 ;
/* a tag to identify relation R records */
outer tag ‚Üê 2 ;
/* a tag to identify relation S records */
hash index ‚Üê 0;
/* a Ô¨Çag to identify hash based records */
partition index ‚Üê 1;
/* a Ô¨Çag to identify records to partition */
replicate index ‚Üê 2 ;
/* a Ô¨Çag to identify records to replicate */
Read HistIndex(R ‚ú∂ S): histogram index from DFS;
Create a HashTable using join key value, frequency‚Äôs index and Nb buckets of HistIndex(R ‚ú∂ S);
}
map(K: null, V : a record from a split of either relation R or S) {
‚úÑ relation tag ‚Üê get relation tag from current relation split;
‚úÑ join key ‚Üê extract the join column from record V of current input relation;
If (join key ‚àà HashTable) Then /* To redistribute only relevant records */
‚úÑ frequency index ‚Üê HashTable(join key).frequency index;
‚úÑ Nb buckets1 ‚Üê HashTable(join key).Nb buckets1;
‚úÑ Nb buckets2 ‚Üê HashTable(join key).Nb buckets2;
‚úÑ random integer ‚Üê Generate Random Integer(join key);
If (frequency index = hash index) Then
‚úÑ Emit ((join key,-1,relation tag), V ); /* for records, with low frequencies, to be hashed */
ElseIf ((frequency index = partition index) and (relation tag = inner tag))
or ((frequency index = replicate index) and (relation tag=outer tag))
‚úÑ random dest ‚Üê (random integer+SRAND(Nb buckets1)) % Nb buckets1;
/* A random integer between 0 and Nb buckets1 */
‚úÑ Ô¨Çag index ‚Üê partition index ;
‚úÑ Emit ((join key,random dest,(Ô¨Çag index,relation tag)), V );
Else
For (int i=0; i<Nb buckets1; i++) Do
‚úÑ random dest ‚Üê (random integer+i) % Nb buckets1;
‚úÑ Ô¨Çag index ‚Üê replication index ;
‚úÑ bucket dest ‚Üê i % Nb buckets2; /* A random integer between 0 and Nb buckets2 */
‚úÑ Emit ((join key,random dest,(Ô¨Çag index,relation tag,bucket dest)), V );
End For;
End If;
End If;
}

Algorithm
7
Partitioning function /* Returns for each composite input key
K =(join key,random integer,DataTags) emitted in Map phase, an integer corresponding to
destination reducer for key K. */
int partition(K: input key ){
join key ‚Üê K.join key;
/* extracts join key part from input key K */
relation tag ‚Üê K.relation tag; /* extracts relation tag part from input key K */
reducer dest ‚Üê K.random dest; /* extracts reducer destination number from input key K */
If (reducer dest = -1) Then
Return (reducer dest % NB reducers);
Else
Return (HashCode(join key) % NB reducers);
End If ;
}

157

Handling Data-skew EÔ¨Äects in Join Operations using MapReduce

M. Bamha, F. Loulergue et al.

Algorithm 8 Reduce function: /* To generate join result. */
void reduce init(){
last key ‚Üê ‚Äù‚Äù ;
/* to store the last processed key */
inner relation tag ‚Üê 1 ;
/* a tag to identify Inner relation records */
outer relation tag ‚Üê 2 ;
/* a tag to identify Outer relation records */
Array buÔ¨Äer ‚Üê NULL ;
/* an array list used to buÔ¨Äer records from one relation */
}
reduce(Key K,List List V ) { /* List List V : the list of records from either relation R or S */
‚úÑ join key ‚Üê K.join key;
/* extracts the join key part from input key K */
‚úÑ relation tag ‚Üê K.relation tag;
/* extracts relation tag part from input key K */
‚úÑ Ô¨Çag index ‚Üê K.Ô¨Çag index;
/* extracts Ô¨Çag index part from input key K */
If ((join key = last key) and (relation tag = Ô¨Çag index)) Then
For each record (x ‚àà List V ) Do
For each record (y ‚àà Array buÔ¨Äer) Do
If (relation tag = outer relation tag) Then
‚úÑ Emit (NULL, x ‚äï y);
Else
‚úÑ Emit (NULL, y ‚äï x);
End If ;
End For ;
End For ;
Else
‚úÑ Array buÔ¨Äer.Clear();
For each record (x ‚àà List V ) Do
‚úÑ Array buÔ¨Äer.Add(x);
End For ;
‚úÑ last key ‚Üê K.join key;
End if
}

158

