Scientific Workflow Infrastructure for Computational
Chemistry on the Grid
Wibke Sudholt1, Ilkay Altintas2, and Kim Baldridge1,2
1

Institute of Organic Chemistry, University of Zurich, Winterthurerstrasse 190,
CH-8057 Zurich, Switzerland
{wibke, kimb}@oci.unizh.ch
2 San Diego Supercomputer Center (SDSC), UC San Diego, 9500 Gilman Drive,
La Jolla, CA 92093-0505, USA
altintas@sdsc.edu

Abstract. We present ongoing research in the Resurgence (RESearch sURGe
ENabled by CyberinfrastructurE) project. This infrastructure shall enable the
flexible combination of computational chemistry tools from a unified interface,
with the focus on automated high-throughput processing. The implementation is
based on the idea that the time-consuming parts of the calculations can be distributed onto computational Grids using the Kepler scientific workflow system
and the Nimrod toolkit for distributed parametric modeling. We describe an example workflow that allows preparing, running, and displaying jobs on different
molecules, employing the GAMESS quantum chemical program package.

1 Introduction
1.1 Scientific Background
Catalyzed by life and material sciences, studying vast amounts of molecules becomes
more and more important in applied chemical research. In the drug discovery pipelines of pharmaceutical companies, many thousand compounds are screened for activity in a high-throughput manner, often using computational chemistry approaches
such as QSAR (Quantitative Structure Activity Relationships) and docking. Fundamental chemistry research, on the other hand, has traditionally been sparsely
automated. However, established computational chemistry tools such as quantum
chemistry, molecular dynamics, and continuum electrostatics nowadays allow the
accurate prediction of molecular properties. Advances in technology continuously
speed up these calculations. Together with the opportunities of data mining, one
hopes to learn new chemical concepts from analyzing many molecular computations.
Moreover, driven by the interest in large and complex systems such as biomolecules, solutions, and materials, it is increasingly necessary to couple different
computational chemistry packages. Algorithms from various scientific domains are
combined, and resolutions range over several time and length scales, an important
example being the QM/MM (Quantum Mechanics/Molecular Mechanics) techniques.
However, these studies usually require the direct integration of application codes. The
V.N. Alexandrov et al. (Eds.): ICCS 2006, Part III, LNCS 3993, pp. 69 – 76, 2006.
© Springer-Verlag Berlin Heidelberg 2006

70

W. Sudholt, I. Altintas, and K. Baldridge

purpose of the Resurgence project is thus the flexible and reusable coupling of computational chemistry packages. This will allow easier exchange of data between applications to realize new scientific ideas, and building of pipelines for automation of
tasks. Our focus is on calculations for different compounds or conformers.
1.2 Distributed Scientific Computing
The underlying physical equations often make computational chemistry calculations
very compute- and data-intensive. Complexity of the parallelized codes often results
in highly communication-dependent algorithms. However, technology moves to
loosely-coupled Grid infrastructures. Computational Grids are defined as “a hardware
and software infrastructure that provides dependable, consistent, pervasive, and inexpensive access to high-end computational capabilities” [1]. Virtual organizations
manage the contribution and distribution of resources on a Grid. Established Grid
projects have created new opportunities for computational research.
Unfortunately, apart from a few examples (e.g., [2, 3, 4]), building and using Cyberinfrastructure for computational chemistry has been lagging behind the hype. Not
many applications have been ported to a Grid infrastructure, exceptions being for
example the OpenMolGRID [5] and Gemstone [6]. Due to the long runtimes and high
resource demands of the calculations, service-oriented architectures are seldom applied. Computational chemistry programs often result from years of collaboration. To
gain speed, they are mainly written in procedural programming languages such as
Fortran or C, using formatted input and output file formats specific to each code.
Many programs are commercial, and only available as binaries or in-house. This
makes it difficult to adapt computational chemistry packages to the requirements of
Grid computing. But also Grid computing itself has partly stayed behind its promises.
Most Grid middleware has long been in development state and exposes APIs complex
to work with for non-specialists. Many management and policy issues have to be
solved before a Grid infrastructure can be set up, which is often only accessible for
selected researchers. Finally, network latency over long distances, hardware and software heterogeneity, and missing control on resource availability in Grids make it less
efficient to port the highly-coupled parallel computational chemistry programs.
Nevertheless, Grid computing has a lot to offer for computational chemistry.
Neighboring disciplines such as bioinformatics and high-energy physics with similar
problems, though stronger orientation towards web services and data Grids, respectively, have demonstrated the benefits of using such infrastructures. Embarrassingly
parallel calculations such as parameter sweeps and optimizations are perfectly suited
for running on Grids. Corresponding tools such as Nimrod [7] and APST [8] provide
wrappers around existing codes to distribute them over a number of machines, and
have been successfully applied in scientific studies. What these tools are missing
though are simple ways to seamlessly couple different applications to form pipelines,
where the results of one stepare used as basis for the next. The purpose of the Resurgence project is thus to provide a workflow tool that allows to flexibly combine computational chemistry codes within one common interface. This should be easy to use
for beginners, hiding the complexity of the middleware, but details should be accessible for advanced users. Compute-intensive steps should be distributed onto the Grid,
while short and interactive steps should stay on the local machine.

Scientific Workflow Infrastructure for Computational Chemistry on the Grid

71

1.3 Scientific Workflow Systems
As the scientific process moves into an era of distributed data and computation, the
needs of scientific studies have also changed. Now, the solution to a scientific problem often involves reaching out to Grids, discovering datasets, running pipelined and
repeated analysis tasks in parameterized calculations, storing the outcomes into databases, registering the workflows and results into online portals, and visually displaying them. Today’s scientists are expected to use all these technologies.
Scientific workflows emerged as a glue to compose different technologies by providing interfaces to run a coordinated set of data and computation components in a
systematic and automated way. They can be applied to combine data integration,
analysis, and visualization steps into larger ‘knowledge discovery pipelines’ and ‘Grid
workflows’ [9]. Scientific workflow research and system development have recently
been the focus of several big efforts. However, the lack of generic graphical interfaces
that abstract the technical details from users and let them focus on the science, has
been a major obstacle for using traditional workflow systems rather than customized
applications. Scientific workflow user environments, for example Kepler [10], Taverna [11], and Triana [12], aim at improving this situation by ‘wrapping’ Grid tools
and making them available in a user-friendly visual programming environment.
Kepler is particularly suited for computational chemistry workflows, as it does not
only provide web service, Grid, and XML-based features, but also command-line
wrapping, file system interaction, and string processing capabilities. The rest of this
paper describes our case study in using Kepler for a computational chemistry pipeline.
We first explain the requirements of the scientific problem and how different tools
were used together to solve it. Then we describe how we utilized Kepler’s components to assemble the workflow. Finally, we discuss the results and unresolved issues.

2 Tools
2.1 Computational Chemistry Programs
In the first Resurgence [13] implementation, we concentrate on the preparation, execution, and analysis of quantum chemical calculations on a number of small molecules. Our main principles are the restriction to open source codes to retain the accessibility for all users, and the use of downloadable versions of all application programs
without modifications. The core tool employed is the GAMESS ab initio quantum
chemistry package [14]. It is a well-established open source code to determine chemical properties based on the Schroedinger equation. It is mainly written in Fortran,
highly parallelized, and can run very efficiently on different computer architectures.
As example of an analysis tool for the calculation results we selected the molecular
graphics program QMView [15]. It allows displaying and modifying the threedimensional structures of chemical compounds and provides a large variety of options
to examine their molecular properties. An important issue in computational chemistry
studies is related to file formats and their transformation. Despite emerging standards
such CML (Chemical Markup Language) [16], the vast majority of applications still
use their own legacy input and output file formats. This is one of the main limiting

72

W. Sudholt, I. Altintas, and K. Baldridge

factors of program interoperability. Therefore, we included two key file format conversion tools, Open Babel [17] and its predecessor Babel [18].
2.2 Nimrod Distributed Parametric Modeling Toolkit
As Grid computing interface for the parameterized calculation jobs, Nimrod/G [7] is
used. It is part of the Nimrod toolkit for distributed parametric modeling and was
already successfully employed previously [2]. Nimrod can perform embarrassingly
parallel parameter sweeps (Nimrod/G) and optimizations (Nimrod/O) distributed on a
Computational Grid. The user specifies a list of available computer resources, and for
each computational ‘experiment’ (i.e., collection of parallel jobs), selects the particular resources to use. A short, declarative ‘plan file’ describes the parameters to consider and the tasks to be performed for each single job. Nimrod then allocates, executes, controls, and collects the individual jobs via ‘agents’ submitted automatically to
the involved resources. Nimrod only has to be installed once on the central root node,
and then communicates with the Grid nodes via the corresponding middleware,
mainly the Globus toolkit [19]. At the backend of Nimrod is a PostgreSQL database
and Python code. It has a command-line interface, which we employ here, a web
portal (Nimrod Portal), and an Excel interface (Active Sheets).
2.3 Kepler Workflow Approach
We have selected Kepler [10] as the environment to utilize the above-mentioned tools
in a workflow. Kepler is a collaboration between several domain research projects, to
develop an extensible and customizable open-source scientific workflow system. The
Kepler system allows scientists from different disciplines (bioinformatics, cheminformatics, ecoinformatics, geoinformatics, oceanography, astrophysics, etc.) to design
and execute scientific workflows. It builds on top of the mature Ptolemy II software
developed at UC Berkley [20]. Ptolemy II is a Java-based system along with a set of
APIs for heterogeneous hierarchical modeling. The focus of Ptolemy II is to build
models based on the composition of existing components, which are called ‘actors’.
Actors are encapsulations of parameterized actions performed on input data to produce output data. Inputs and outputs are communicated through ports within the actors. The interaction between the actors is defined by Models of Computation (MoC).
A MoC specifies the communication semantics among ports and the flow of control
and data among actors. Directors are responsible for implementing particular MoCs,
and thus define the ‘orchestration semantics’ for workflows. By changing the director,
one can change the scheduling and execution semantics of a workflow, without
changing any of the components or the network topology. An example for a director
is the ‘Synchronous Data Flow (SDF)’ domain, in which a sequential execution order
of actors can be statically determined prior to execution. Here we need to combine it
with the more dynamic ‘process networks (PN)’ director, which also demonstrates the
usage of multiple execution domains in one workflow. The descriptions of Kepler
workflows are saved in an XML-based format called Modeling Markup Language
(MoML). MoML is the primary persistent file format for Ptolemy II models and Kepler workflows. It also represents the primary mechanism for constructing workflows
whose definition and execution is distributed on the network.

Scientific Workflow Infrastructure for Computational Chemistry on the Grid

73

4 Implementation and Results
Our initial case study tests the capabilities of Kepler, and the applicability of Kepler
workflow development methodologies to computational chemistry calculations and
the spreading of tasks onto distributed nodes. The conceptual workflow underlying
our example implementation is simple (see Figure 1). Several files containing the
atomic coordinates of different molecules are provided. From these, input files for the
quantum chemical program package GAMESS are generated. Then the corresponding
calculations are executed. Finally, the results of the computations are visualized using
the QMView software. A pipeline like this is part of the daily work of computational
chemists. The largest variant is usually that different molecular systems, application
codes, and keyword and parameter settings are used for the preparation, calculation
and visualization steps. If more than a few compounds have to be processed, the typical practice is to use shell scripts. The Resurgence implementation into Kepler provides an easier, more flexible, and reusable manner to accomplish the same tasks.

Fig. 1. Conceptual workflow

However, it is clear that the workflow in Figure 1 cannot be directly realized, as
different file formats are involved. In our example, we start with CML files, which are
then transformed into GAMESS input files, processed into GAMESS output files, and
finally converted into PDB (Protein Data Bank) [21] files, which can be displayed.
Other format transformations would be possible. This means that at steps number 1
and 3, additional format conversion programs have to be included, in our case Open
Babel and Babel. Moreover, the GAMESS calculations are very resource-intensive
and time-consuming, and thus the most interesting step to target for Grid distribution.
On the other hand, the graphical display and the data-intensive, but fast file format
transformations are currently better performed on the local machine. However, if
management and storage requirements for these steps increase in future applications,
database or data Grid capabilities already available in Kepler might be employed.
The major Kepler actors used for the construction of the executable workflow for
the mentioned conceptual workflow are the CommandLine and ExternalExecution
actors as well as generic textual display and various constant, array, string function,
and file system manipulation actors. The CommandLine and ExternalExecution actors
are wrappers for executing commands that run legacy code in a workflow. Their purpose here is to call the diverse employed external programs or corresponding automatically generated temporary shell-script wrappers. To conceptually organize the
workflow and to be as modular as possible, we have extensively used composite actors and MoML actor classes. Composite actors are combinations of other actors that
are implemented as a block diagram and can be saved into the actor library for reusing
and updating purposes. Actor classes allow applying these composite entities in workflows in a similar way as one can use native atomic actor Java classes.

74

W. Sudholt, I. Altintas, and K. Baldridge

Fig. 2. GAMESS/Nimrod workflow in Kepler

Figure 2 illustrates the Kepler view for the workflow that implements the conceptual workflow in Figure 1. The constructs that make up the workflow were abstracted
to six main steps: ‘Molecule Selector’ to choose the input files for a set of target
molecules, ‘Open Babel’ to transform these files into the GAMESS input format,
‘GAMESS Input Generator’ to annotate the inputs with the right set of calculation
keywords, ‘GAMESS Nimrod Run’ for the Grid execution, ‘Babel’ for transformation
of GAMESS outputs to PDB format, and finally, ‘QMView Display’ to visualize the
outputs for each molecule. The only data sent between these actors are sequences of
file handles belonging to the individual molecules. The corresponding file name bases
also serve as the ‘parameter’ in the auto-generated intermediate Nimrod/G plan file.
On this highest-level view of the workflow, the most important variables of the computations can be specified. However, all described steps are composite actor classes
which have several layers of sub-workflows hidden inside. By opening and modifying
them, the user can fine-tune the calculation pipeline to various degrees. We have
tested the workflow for geometry optimizations of a number of different small molecules on a Mac laptop for local execution. For distributed Nimrod execution, a small
Globus-based Grid consisting of two seven and eight-node Xeon Linux clusters can
be used. Results from ongoing tests will be made available online.

5 Discussion and Conclusions
The presented example workflow allows to easily setup a pipeline to prepare, run, and
display quantum chemical calculations on different compounds. The compute-intensive
part may be distributed onto a computational Grid with the help of the
Nimrod toolkit, reducing the time for this embarrassingly parallel step. Due to the
graphical visualization of the results at the end, the number of molecules that can be
treated concurrently is limited. The most important parameters, such as file types, directory names, external binary locations and arguments, important GAMESS keywords,

Scientific Workflow Infrastructure for Computational Chemistry on the Grid

75

as well as the experiment name, can be specified on the highest workflow level. Tuning of further details of the calculations, for example seldom used GAMESS keywords and Grid node set up can be modified using sub-components of composite
actor classes. Due to its high modularity, each component can be reused in other
workflows. The GAMESS Nimrod execution actor can substituted by a GAMESS
local execution actor without changing the rest of the pipeline. Overall, our implementation is expected to enhance the design and execution of computational chemistry pipelines. By reducing complexity and increasing manageability, adaptability, and
reusability as well as by transparent access to Grid resources, preparation and calculation time and effort decreases and more advanced workflows are possible. However,
success will finally be measured by its usage scientific studies.
Despite this progress, we did not reach all our goals yet. The integration of the external application codes GAMESS, QMView, Open Babel, and Babel is already far
developed. However, the implementation of Nimrod/G currently only covers a part of
its functionality. In addition to Nimrod and local execution, implementing SSH,
Globus, and web service capabilities for GAMESS is planned. Certainly, the Resurgence interface should be expanded to further domain-specific computation codes.
However, also some fundamental aspects of our solution need to be looked at. Its robustness in real high-throughput settings remains to be tested. The necessary file format transformations could benefit from a systematic and rigorous typing system. And
finally, the logging, monitoring, and control of the long-running computational experiments have to be improved. The Resurgence module of the Kepler scientific workflow system can be downloaded free of charge from the Kepler website and CVS.

Acknowledgements
We thank Celine Amoreira and Yohann Potier from the University of Zurich, Adam
Birnbaum and Jerry Greenberg from SDSC, Yang Zhao from UC Berkeley, and
David Abramson and his group from Monash University, Australia, for collaboration.
W.S. acknowledges support from the CoLab at ETH Zurich, K.B. from the NSF-NMI
(ANI-0223043), NSF-PRAGMA, and NIH-NCRR (NBCR-RR08605).

References
[1]

[2]

[3]

[4]

Foster, I., Kesselman, C.: Computational Grids. In I. Foster and C. Kesselman, eds.,
The Grid: Blueprint for a New Computing Infrastructure, Morgan Kaufman(1999), pp.
15-51.
Sudholt, W., Baldridge, K.K., Abramson, D., Enticott, C., Garic, S., Kondric, C.,
Nguyen, D.: Application of grid computing to parameter sweeps and optimizations in
molecular modeling. Future Generation Computer Systems, 21 (2005), pp. 27-35.
Baldridge, K.K., Greenberg, J.P., Sudholt, W., Mock, S., Altintas, I., Amoreira, C.,
Potier, Y., Birnbaum, A., Bhatia, K., Taufer, M.,: The Computational Chemistry Prototyping Environment, Proceedings of the IEEE, 93 (2005), pp. 510-521;
Baldridge, K.K., Sudholt, W., Greenberg, J.P., Amoreira, C., Potier, Y., Altintas, I.,
Birnbaum, A., Abramson, D., Enticott, C., Garic, S.: Cluster and Grid Infrastructure for
Computational Chemistry and Biochemistry. In A.Y. Zomaya, ed., Parallel Computing
for Bioinformatics and Computational Biology, Wiley (2005), pp. 533-552.

76
[5]
[6]
[7]

[8]
[9]

[10]

[11]
[12]
[13]
[14]

[15]

[16]
[17]
[18]
[19]
[20]
[21]

W. Sudholt, I. Altintas, and K. Baldridge
http://www.openmolgrid.org/.
http://www.baldridge.unizh.ch/gemstone/.
Abramson, D., Giddy, J., Kotler, L.: High Performance Parametric Modeling with Nimrod/G: Killer Application for the Global Grid?. International Parallel and Distributed
Processing Symposium (IPDPS), Cancun, Mexico (2000), pp. 520-528;
http://www.csse.monash.edu.au/~davida/nimrod/.
http://grail.sdsc.edu/projects/apst/.
Altintas, I., Birnbaum, A., Baldridge, K.K., Sudholt, W., Miller, M., Amoreira, C.,
Potier, Y., Ludaescher, B.: A Framework for the Design and Reuse of Grid Workflows.
Scientific Applications of Grid Computing: First International Workshop, SAG 2004,
Beijing, China, LNCS, 3458 (2004), pp. 119-132.
Ludaescher, B., Altintas, I., Berkley, C., Higgins, D., Jaeger-Frank, E., Jones, M., Lee,
E., Tao, J., Zhao, Y.: Scientific Workflow Management and the Kepler System. Concurrency and Computation: Practice & Experience, Special Issue on Scientific Workflows (2005), to appear; http://www.kepler-project.org/.
http://taverna.sourceforge.net/.
http://www.trianacode.org/.
http://www.baldridge.unizh.ch/resurgence/.
Schmidt, M.W., Baldridge, K.K., Boatz, J.A., Elbert, S.T., Gordon, M.S., Jensen, J.H.,
Koeski, S., Matsunaga, N., Nguyen, K.A., Su, S.J., Windus, T.L., Dupuis, M., Montgomery, J.A.: The General Atomic and Molecular Electronic Structure System. J. Comput. Chem., 14 (1993), pp. 1347-1363; http://www.msg.ameslab.gov/GAMESS/.
Baldridge, K.K., Greenberg, J.P.: QMView: A computational chemistry threedimensional visualization tool at the interface between molecules and mankind. J. Mol.
Graphics, 13 (1995), pp. 63-66; http://www.nbcr.net/software/QMView/.
http://www.xml-cml.org/; http://cml.sourceforge.net/.
http://openbabel.sourceforge.net/.
http://www.ccl.net/cca/software/UNIX/babel/.
http://www.globus.org/.
http://ptolemy.eecs.berkeley.edu/ptolemyII/.
http://www.rcsb.org/pdb/.

