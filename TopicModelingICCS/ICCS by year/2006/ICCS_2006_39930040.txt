Per-pixel Rendering of Terrain Data
Taek Sang Jeong and JungHyun Han
Department of Computer Science and Engineering, Korea University, Korea

Abstract. This paper presents a novel approach to terrain rendering,
which mostly relies on GPU/shader rather than CPU. The most popular
representation for terrain data is uniformly sampled height ﬁeld. As the
height ﬁeld is stored as a texture map, it is directly accessible by a
pixel shader. The pixel shader uses a ray casting algorithm, and the
CPU and the vertex shader provide ray information to be passed to
the pixel shader. Then, the pixel shader samples the ray, computes the
intersection of the ray and the terrain surface, and ﬁnally determines the
pixel color. The experimental results show the feasibility of the shaderintensive approach to real-time terrain rendering.

1

Introduction

Terrain rendering is essential for many applications such as 3D games, geographic
information systems, virtual reality, ﬂight simulation, etc. Applications usually
require walk-through or ﬂy-through navigation of the terrain in an interactive or
real-time mode. The most popular representation for terrain data is uniformly
sampled height ﬁeld, often called digital elevation model[1]. The height ﬁeld is
a set of height or elevation data sampled in a uniform grid, and its simplicity
has made it very popular in terrain visualization. Various approaches have been
proposed in terrain rendering, and a recent survey can be found in [2]. In the wide
spectrum of the proposed approaches, a common factor is that they all require
a tremendous amount of CPU computation, for example, to do LOD(levels of
detail) control, occlusion culling, etc.
The role of computer graphics hardware is performing the rendering pipeline,
which is largely partitioned into vertex processing and fragment processing. The
early graphics hardware handled only the fragment processing, but soon evolved
to be able to handle vertex processing which includes transformation and lighting (T&L). Recently, we have witnessed the graphical processing unit (GPU)
revolution where the hardware capability has been exponentially growing and
further the hardware has become programmable. The programmable pipeline is
named shader. The shader program is a set of GPU instructions, and is partitioned into vertex shader (or vertex program) and pixel shader (or fragment
program).
This paper proposes shader-based terrain rendering algorithms, which maximally utilize the powerful computing capabilities of the shader. Taking the
Corresponding author.
V.N. Alexandrov et al. (Eds.): ICCS 2006, Part III, LNCS 3993, pp. 40–47, 2006.
c Springer-Verlag Berlin Heidelberg 2006

Per-pixel Rendering of Terrain Data

41

rendering load oﬀ the CPU, the CPU can do otherwise impossible missions,
such as AI control in game applications.

2

Why Shader for Terrain Rendering

The original terrain data are often perceived too large to display at interactive frame rates. Therefore, various methods have been adopted that reduce the
complexity of the terrain data but retain the image quality. The LOD(levels
of detail) method has been popular as a suitable tool that selects only a subset of the height ﬁeld points and produces a coarser mesh from the subset. For
example, the near/important parts are represented by a large number of small
polygons, and the far/unimportant parts by a small number of large polygons.
Good examples of the LOD-based algorithms are found in [3][4][5].
The graphics hardware revolution makes people call the above-mentioned
CPU-intensive approaches into question, i.e. the bottleneck in the terrain rendering process often lies in the CPU, which performs terrain simpliﬁcation and
LOD generation per each frame. In contrast, the increased computing capability of the GPU allows the GPU to be loaded with more work. Reﬂecting the
changed computing environment, for example, Losasso and Hoppe[6] proposed
so-called geo-clipmap approach, which adopts a fairly simple LOD scheme such
that a ﬁner resolution is generated for a region closer to the viewpoint, and a
coarser resolution for a farther region. Then, the simple LOD terrain is passed
to GPU, which handles the discontinuities between the diﬀerent LOD levels of
the terrain data.
Along the emerging trend, this paper proposes a shader-intensive terrain rendering approach, where the CPU does little or only a small amount of work,
and almost all rendering jobs are assigned to the GPU, more precisely to the
pixel shader. The proposed shader-intensive approach presents the terrain data
represented as a texture map to the pixel shader. Then, the pixel shader renders
the terrain data using ray casting. The traditional discrete ray casting algorithm
is used, where a ray from the viewpoint to a screen pixel position is generated,
and then the intersection between the ray and the terrain surface is computed.

3

Rendering Process

The overview of the rendering algorithm can be stated as follows. First of all,
terrain envelope is computed oﬀ-line, which is deﬁned as a coarse superset of
the real terrain geometry. The bounding box of the terrain data is a kind of
envelope, but is too loose. A tighter envelope example is shown in Fig. 1-(a).
In the vertex shader, the object-space coordinates of the envelope mesh vertices are output as texture values (using TEXCOORD0), and therefore the pixel
shader obtains the interpolated object-space coordinates for each pixel position.
Those coordinates are taken as the starting points for ray casting, as illustrated
in Fig. 1-(b). Each ray is then sampled by uniformly-spaced points. Each sampled point is processed in order towards the terrain surface, as illustrated in

42

T.S. Jeong and J. Han

(a) terrain envelope

(c) ray sampling

(b) starting points of ray casting

(d) no intersection
ray sampling

with

Fig. 1. Terrain envelope and ray casting

Fig. 1-(c). Each sampled point is tested if it lies below the terrain surface. If
a point is found to be below the surface, the current and previous points are
interpolated to determine the actual intersection position between the ray and
the terrain surface.
For the intersection point, lighting is computed, i.e. Phong shading is implemented. For Phong shading, normals at the intersection points are obtained
from the normal map, which is pre-computed from the terrain height map and
passed to the pixel shader. The intersection position’s (x,z)-coordinates are used
to reference the texture map (real image texture!) of the terrain surface.

4

Rendering Optimization

As demonstrated in Fig. 1-(c), the required numbers of sampled points vary,
depending on both the eye position and the ray direction. The major problem of
the discrete ray casting is that it is hard to ﬁnd the upper limit on the number of
the sampled points. See Fig. 1-(d), where the ray is sampled for a ﬁnite number
of iterations, but no intersection is found yet. Do we have to move further? Until
when? There is no ‘reasonable’ upper limit. With a ﬁnite number of iterations,
the intersection point between the ray and the target object is not guaranteed

Per-pixel Rendering of Terrain Data

43

to be found even when it really exists. Only the inﬁnite number of iterations
guarantees it, but obviously we cannot do so.
Recently, shader-based ray casting has been explored in the domain of height
ﬁeld, and tackled the sampling problem. For example, Policarpo [7] proposed
to use a combination of linear search and binary search, and Donnelly [8] proposed to use a so-called distance map in order to non-uniformly sample the ray.
However, the intersection point is not guaranteed to be found in the proposed
approaches. The rendering algorithm presented in this paper linearly searches
the uniformly sampled points, but the intersection point is always found, if any.

Fig. 2. Ray sampling resolution

Note that the height ﬁeld data are organized in a uniform grid. In the proposed
approach, the ray is uniformly sampled exactly at the resolution of the height
ﬁeld data. Then, it is guaranteed that we do not miss an intersection between
the ray and the terrain surface. In contrast, it is not guaranteed as long as the
ray samples are spaced farther than a single height texel, as shown in Fig. 2.
However, correctly setting the sampling resolution is not enough. We have
to ﬁnd a way to avoid inﬁnite sampling. The solution is found by processing a
bounded region at a time. For example, the entire height ﬁeld can be partitioned
into 9x9-sized blocks. Then, we need just 9 sampled points per a ray for processing the block. (Recall that the sampling resolution is equal to the height texel
resolution.) If an intersection is found during the 9 iterations, color is computed
for the intersection point. Otherwise, the ray segment can be safely discarded as
the ray will be re-traced in the adjacent block(s). In other words, the undetermined pixel position will be processed again when an adjacent block is picked up.
The envelope is pre-computed, and the CPU’s role is simply rendering the precomputed envelope. The tighter envelopes we have, the less pixels are processed
in the pixel shader. For this purpose, we have adopted the simpliﬁcation envelope
approach1 by Cohen et al. [9] Fig. 3-(a) and -(b) show a part of the height
ﬁeld data, and Fig. 3-(c) is its outer mesh, which is generated by adaptively
oﬀsetting the vertices (all along +y direction) of the original mesh. The original
mesh is taken as the inner mesh, and then mesh simpliﬁcation is done only
1

The original mesh’s vertices are oﬀset by a user-speciﬁed distance along its normal and opposite directions to produce outer and inner meshes, respectively. Then,
the original mesh is simpliﬁed between the inner and outer meshes. Therefore, the
simpliﬁed mesh is guaranteed to be within from the original mesh.

44

T.S. Jeong and J. Han

(a) original mesh

(b) outer mesh

(c) simpliﬁed mesh

Fig. 3. Envelope construction

(a) height elevation

(b) outer mesh

Fig. 4. Outer mesh construction

between the inner and outer meshes. For the sake of simplicity, the outer mesh
itself is simpliﬁed in the current implementation, and the resulting simpliﬁed
mesh, shown in Fig. 3-(d), lies between the original mesh (inner mesh) and
the ‘ﬁne-resolution’ outer mesh. Vertex decimation [10] is currently used as a
simpliﬁcation method.
Let us discuss the outer mesh construction process. See Fig. 4-(a) for an
example in the cross section of the height ﬁeld. Each vertex v of the original
height ﬁeld has to be vertically (along +y direction) oﬀset to generate the outer
mesh. To determine v’s oﬀset distance, consider a square (denoted by a brace
in Fig. 4-(a)) centered at v, and take the height ﬁeld vertex w which is inside
the square and has the highest elevation. The elevation of v in the outer mesh is
set to that of w. Such elevated or vertically oﬀset vertices constitute the outer
mesh, as shown in Fig. 4-(b).
Recall that we need bounded regions. For the current implementation, the size of
a bounded region is 9x9. The outer mesh construction process is done per a bounded
region. Furthermore, note that the boundary vertices of each region are not oﬀset,
as shown in Fig. 4-(b). By not oﬀsetting the boundary vertices, the re-traced ray
(discussed above) can be assigned the starting position for discrete ray casting.

5

Experiment Results

The terrain data set used for experiment is 1025x1025-sized Puget Sound[11].
Fig. 5 shows the height ﬁeld data and its texture map. The height is represented
by 1 byte, and therefore in the range of [0,255]. Fig. 5-(a) is the gray-scale

Per-pixel Rendering of Terrain Data

45

visualization of the height ﬁeld data, where black(0) represents the lowest elevation and white(255) represents the highest elevation.
The experiment is implemented on Pentium 4 (3.0 GHz) and NVidia GeForce
6800 Ultra with DirectX 9.0c and pixel shade ps 3 0. Unlike the previous proﬁles,
ps 3 0 supports static and dynamic ﬂow control such as iteration and branch,
and therefore sampling of the ray becomes possible. Fig. 6 shows the snapshots
of the rendered terrain from diﬀerent viewpoints. The average frame rate is 78
fps (frames per second).
Fig. 7 compares two images: one is rendering with the polygon mesh obtained
from the height ﬁeld data, and the other is rendering with ray casting. Both of
them have the comparable image quality.

(a) height map

(b) texture
Fig. 5. Test data

Fig. 6. Rendered images

46

T.S. Jeong and J. Han

(a) mesh rendering

(b) ray casting

Fig. 7. Rendering result comparison

6

Conclusion

This paper presents a novel shader-intensive approach to terrain rendering. The
height ﬁeld stored as a texture map is directly accessible by a pixel shader. The
pixel shader uses a ray casting algorithm, and the CPU and the vertex shader
provide ray information to be passed to the pixel shader. The experimental
results show the feasibility of the shader-intensive approach to real-time terrain
rendering.
The earnest work of shader-based terrain rendering has been reported in [12]
and [13]. The work by [12] proposes a simple application of shader. In the work
by [13], however, the rendering process mostly relies on GPU/shader rather than
CPU. Our approach follows such a shader-intensive approach as [13].
In the current implementation, the CPU does little work for the shader. However, the authors envision that the CPU could perform terrain data simpliﬁcation
and LOD generation unless they cause a bottleneck in the rendering process.
Then, the simpliﬁed portion could be accommodated by a mipmap structure.
Such a small CPU program can be plugged in the proposed framework 2 where
the main body of rendering is performed by the shader.

Acknowledgements
This research was supported by the Ministry of Information and Communication, Korea under the Information Technology Research Center support program
supervised by the Institute of Information Technology Assessment, IITA-2005(C1090-0501-0019). This work was also sponsored and funded by Korea Game
Development & Promotion Institute as Korean government project (Ministry of
Culture and Tourism).
2

The traditional LOD construction algorithms cannot be moved to shader because
the neighbouring vertex information which is essential for an LOD construction is
not available to shaders.

Per-pixel Rendering of Terrain Data

47

References
1. U.S. Geological Survey. http://www.usgs.gov/
2. R. Pajarola: Overview of Quadtree-based Terrain Triangulation and Visualization.
UCI-ICS Technical Report No. 02-01, Department of Information & Computer
Science, University of California, Irvine, 2002.
3. P. Lindstrom, V. Pascucci: Terrain Simpliﬁcation Simpliﬁed: A General Framework
for View-Dependent Out-of-Core Visualization. IEEE Transactions on Visualization and Computer Graphics, Vol. 8, No. 3, July-September, pp. 239-254, 2002.
4. M. Duchaineau, M. Wolinsky, D. Sigeti, M. Miller, C. Aldrich, M. MineevWeinstein: ROAMing Terrain: Real-time Optimally Adapting Meshes. IEEE Visualization 1997, pp. 81-88, 1997.
5. S. R¨
ottger, W. Heidrich, P. Slusallek, H.-P. Seidel: Real-time Generation of Continuous Levels of Detail for Height Fields. Central Europe Conf. on Computer
Graphics and Vis., pp. 315-322, 1998.
6. F. Losasso, H. Hoppe: Geometry Clipmaps: Terrain Rendering using Nested Regular Grids. ACM SIGGRAPH, pp. 769-776, 2004.
7. F. Policarpo: Relief Mapping in a Pixel Shader using Binary Search.
http://www.paralelo.com.br/arquivos/ReliefMapping.pdf.
8. W. Donnelly: Per-pixel Displacement Mapping with Distance Functions. GPU
Gems 2, Addison-Wesley, pp. 123-136, 2005.
9. J. Cohen, A. Varshney, D. Manocha, G. Turk, H. Weber: Simpliﬁcation Envelopes.
ACM SIGGRAPH, pp. 119-128, 1996.
10. W. Schroeder, J. Zarge, W. Lorensen: Decimation of Triangle Meshes. ACM SIGGRAPH, pp. 65-70, 1992.
11. Puget Sound. http://www.cc.gatech.edu/projects/large models/ps.html
12. Aaron Burton: Voxel Rendering with PS 3 0. ShaderX2 Shader Programming Tips
& Tricks with DirectX9, Wordware publishing, pp.161-171, 2004.
13. A. Asirvatham, H. Hoppe: Terrain Rendering using GPU-based Geometry
Clipmaps. GPU Gems 2, Addison-Wesley, pp. 27-45, 2005.

