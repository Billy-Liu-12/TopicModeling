Support Vector Machine Regression Algorithm Based on
Chunking Incremental Learning
Jiang Jingqing1, 2, Song Chuyi2, Wu Chunguo1, 3, Marchese Maurizio4,
and Liang Yangchun1,4,*
1
College of Computer Science and Technology, Jilin University, Key Laboratory of Symbol
Computation and Knowledge Engineering of Ministry of Education, Changchun 130012, China
2
College of Mathematics and Computer Science,
Inner Mongolia University for Nationalities, Tongliao 028043, China
3
The Key Laboratory of Information Science & Engineering of
Railway Ministry/The Key Laboratory of Advanced Information Science and Network
Technology of Beijing, Beijing Jiaotong University, Beijing 100044, China
4
Department of Information and Communication Technology,
University of Trento, Via Sommarive 14, 38050, Povo (TN) Italy

Abstract. On the basis of least squares support vector machine regression
(LSSVR), an adaptive and iterative support vector machine regression algorithm
based on chunking incremental learning (CISVR) is presented in this paper.
CISVR is an iterative algorithm and the samples are added to the working set in
batches. The inverse of the matrix of coefficients from previous iteration is used
to calculate the regression parameters. Therefore, the proposed approach permits
to avoid the calculation of the inverse of a large-scale matrix and improves the
learning speed of the algorithm. Support vectors are selected adaptively in the iteration to maintain the sparseness. Experimental results show that the learning
speed of CISVR is improved greatly compared with LSSVR for the similar training accuracy. At the same time the number of the support vectors obtained by the
presented algorithm is less than that obtained by LSSVR greatly.

1 Introduction
The support vector machine (SVM) is a novel learning method that is constructed
based on statistical learning theory. The support vector machine has been studied
widely since it was presented in 1995. It has been applied to pattern recognition
broadly and its excellent performance has been shown in function regression problems. Training a standard support vector machine requires the solution of a large-scale
quadratic programming problem. This is a difficult problem when the number of the
samples exceeds a few thousands. Many algorithms for training the SVM have been
studied. Osuua [1] proposed a decomposition algorithm and the quadratic programming problem for standard SVM is divided into a serial small-scale quadratic programming sub-problem. Focusing on the problem of the working set selection,
Joachims [2] presented a SVMLight algorithm to implement the decomposition
algorithm in [1] efficiently. A sequential minimal optimization algorithm (SMO) was
proposed by Patt [3]. It transformed the quadratic programming problem for standard
*

Corresponding author.

V.N. Alexandrov et al. (Eds.): ICCS 2006, Part I, LNCS 3991, pp. 547 – 554, 2006.
© Springer-Verlag Berlin Heidelberg 2006

548

J. Jingqing et al.

SVM to the minimization quadratic programming problem that could be solved analytically. Suykens [4] suggested a least squares support vector machine (LSSVM) in
which the inequality constrains were replaced by equality constrains. By this way,
solving a quadratic programming was converted into solving linear equations. The
efficiency of training SVM is improved greatly and the difficulty of training SVM is
cut down. Suykens [5] studied the LSSVM for function regression further. Hao [6]
proposed a chunking incremental learning algorithm for LSSVM to deal with classification problem. In this paper, an adaptive and iterative support vector machine regression algorithm based on chunking incremental learning (CISVR) is presented. The
support vectors are selected adaptively in the iteration to maintain the sparseness and
the samples are added to working set in batches.

2 Least Squares Support Vector Machine for Regression (LSSVR)
According to [5], let us consider a given training set of l samples { x i , y i }li =1 with the
ith input datum x i ∈ R n and the ith output datum y i ∈ R . The aim of support vector
machine model is to construct the decision function that takes the form:
f ( x, w) = wT ϕ ( x ) + b

(1)

where the nonlinear mapping ϕ (⋅) maps the input data into a higher dimensional
feature space. In least squares support machine for function regression the following
optimization problem is formulated
min J ( w , e ) =
w ,e

1 T
w w +γ
2

l

∑e
i =1

2
i

(2)

subject to the equality constraints
y i = w T ϕ ( x i ) + b + ei ,

i = 1,..., l

(3)

This corresponds to a form of ridge regression. The Lagrangian is given by
L ( w , b , e ,α ) = J ( w , e ) −

l

∑α
i =1

i

{ w T ϕ ( xi ) + b + ei − y i}

(4)

with Lagrange multipliers α k . The conditions for the optimality are
l
⎧ ∂L
0
α iϕ ( x i )
=
→
=
w
∑
⎪ ∂W
i =1
⎪
l
⎪ ∂L = 0 →
αi = 0
∑
⎪ ∂b
i =1
⎨ ∂L
⎪
= 0 → α i = γ ei
⎪ ∂ ei
⎪ ∂L
= 0 → w T ϕ ( xi ) + b + ei = 0
⎪
∂
α
i
⎩

(5)

Support Vector Machine Regression Algorithm Based on Chunking Incremental Learning 549

for i = 1,..., l . After eliminating ei and w , we could have the solution by the following linear equations

⎡0
⎢
⎣1

1T
Ω+γ

−1

⎤ ⎡b ⎤ ⎡ 0 ⎤
⎥⎢ ⎥ = ⎢ ⎥
I ⎦ ⎣a ⎦ ⎣ y ⎦

(6)

where y = [ y1 ,..., y l ]T ,1 = [1,...,1]T ,α = [α 1 ,..., α l ]T and the Mercer condition

Ωkj = ϕ ( xk )T ϕ ( x j ) = ψ ( xk , x j )

k , j = 1,...,l

(7)

is applied. Set A = Ω + γ − 1 I . If A is a symmetric and positive-definite matrix, A-1
exists. Solving the linear equations (6) we obtain the solution
α = A −1 ( y − b1)

b =

1 T A −1 y
1 T A −1 1

(8)

Substituting w in Eq. (1) with the first equation of Eqs. (5) and using Eq. (7) we have
f ( x, w) = y( x) =

l

∑ α ψ ( x, x
i =1

i

i

)+b

(9)

where α i and b are the solution to Eqs. (6). The kernel function ψ (⋅) can be chosen
as linear function ψ ( x , x i ) = x iT x , polynomial function ψ ( x , xi ) = ( xiT x + 1) d or

radial basis function ψ ( x , xi ) = exp{ − x − xi

2
2

/ σ 2} .

3 Adaptive and Iterative Least Squares Support Vector Machine
Regression Algorithm Based on Chunking Incremental
Learning
3.1 Chunking Increment Procedure

According to Eq. (6), set

AN = Ω + γ

−1

I

αN =α

yN = y

(10)

where N is the number of samples in current working set. Eq. (8) can be rewritten as

α N = AN−1 ( y N − b 1 )

b =

1 T A N− 1 y N
1 T A N− 1 1

(11)

1 = (1,...,1)T .When K new coming samples (xN+1, yN+1 ),(xN+2 , yN+2 ),...,(xN+K , yN+K ) are added
to the current working set, we could calculate the parameters according to Eq. (12)

α N + K = A N−1+ K ( y N + K − b 1 )

b =

1 T A N− 1+ K y N + K
1 T A N− 1+ K 1

(12)

550

J. Jingqing et al.

where 1 = (1,...,1)T , α N +K = (α N ,α N +1,...,α N +K ) , y N + K = ( y N , y N +1 ,..., y N + K ) ,
⎡A
AN + K = ⎢ NT
⎣Q

⎡ Ω1, N +1
⎢
Q = ⎢ ...
⎢Ω N , N +1
⎣

Q⎤
S ⎥⎦

⎡ Ω N +1, N +1
⎢
S=⎢
...
⎢⎣ Ω N + K , N +1

Ω N +1, N + 2

...

...

...

Ω N + K ,N +2

...

Ω 1, N + 2
...
Ω N ,N +2

... Ω1, N + K ⎤
⎥
...
... ⎥
... Ω N , N + K ⎥⎦

Ω N +1, N + K ⎤
⎥
−1
...
⎥ +γ I
Ω N + K , N + K ⎥⎦

According to the algorithm in [6], the matrix AN−1+ K in Eq. (12) could be calculated
from matrix AN−1 and the inverse of a small K×K matrix, that is
⎡ A −1
A N−1+ K = ⎢ N
⎣ 0

0 ⎤ ⎡− A N− 1 Q ⎤
−1
T
⎥+⎢
⎥ S − Q AN Q
0⎦ ⎣ I
⎦

[

]

−1

⎡− Q T A N− 1 ⎤
⎢
⎥
I
⎣
⎦

(13)

where 0 is a matrix whose elements are all zero. I is a unit matrix with K rows and K
columns. In this way the calculation for the inverse of a large-scale matrix could be
avoided.
3.2 Decrement Procedure

The number of support vectors will increase with the chunking increment procedure.
To maintain the sparseness of support vectors, a decrement procedure is implemented
after the chunking increment procedure. A support vector is omitted in this procedure.
Meanwhile, a trained sample in the working set corresponding to the discarded support vector is also omitted. Form [7], Al−−11 = (aˆ ij ) can be calculated from Al−1 = a~ij ,
Al = (a ij ) and A l − 1 = (a ij

)

i, j≠ k

( )

i , j ≠k

in the decrement procedure, that is

1 ~ ~ ,
aˆ ij = a~ ij −
a ik a kj i , j ≠ k
a kk

(14)

where Al −1 is a matrix obtained from Al by omitting the kth row and the kth column.
3.3 Steps of CISVR Algorithm

{

Set the training sample set T = si | si = ( xi , yi ), xi ∈ R n , yi ∈ R , i = 1,2,
form of the regression function is

f ( x, α , b) = ∑ α iψ ( x, xi ) + b ≡ f ( x) |W~
i∈W

}

, l . The

(15)

where α and b are the regression parameters, W is named working set whose elements are the training samples selected to calculate the regression parameters, and W~

Support Vector Machine Regression Algorithm Based on Chunking Incremental Learning 551

is the regression parameters set which is decided by working set W . Set θ is the
precision in training and testing, the precision in stop criterion is ε .
Steps of CISVR algorithm are as follows:
Initialization: set W = {( x1 , y1 ),..., ( x N , y N )} and calculate A −1 analytically. Calculate
~
W and f ( x ) |W~ from Eqs. (8) and (9). Set k=0.
for i = N + 1,..., l do
adaptive learning
1. read a sample si = ( xi , yi )

2.
3.
4.
5.

if f (xi ) |W~ −yi >θ and si ∉ W then

W = W ∪ {si } , k=k+1
end if
if k=K then

~

6.
7.

calculate W by chunking increment procedure
find the minimization support vector α = min { α i }

8.

Wˆ = W \{si* } // Wˆ is temporary working set

9.

i*

s i ∈W

~
calculate (Wˆ ) and temporary regression function f ( x) |Wˆ
~
// (Wˆ ) is the temporary regression parameters set corresponding to the

// temporary working set Wˆ
10.

read a sample si +1

11.

if f (xi+1 ) |(Wˆ ~) −yi+1 ≤ θ

then

~
~
12.
W = Wˆ
W = (Wˆ )
13.
end if
k=0
14.
15. end if
end for
while the stop criterion is false do
for i = 1,..., l do
adaptive learning
end for
end while

The stop criterion is related to the objective value. The formulation of the objective
function is J ( w , e ) |W =
ei =

1

γ

1
w
2

T
w ∈W

+

γ

2

∑

s i ∈W

e i2 , where w =

∑α

s i ∈W

i

ϕ ( xi ) ,

α i . The meaning of the defined stop criterion is that the procedure ends

552

J. Jingqing et al.

when the relative error of objective values in the two adjacent iterations is smaller
than a given precision ε . In the decrement procedure, the minimization support vector
is omitted because it has least effect on the performance of the regression function.
The matrix A −1 in the current iteration is obtained from that in the previous iteration
in both chunking increment and decrement procedure. In this way, it is possible on
one hand to avoid calculating the inverse for a large-scale matrix and on the other
hand to improve the learning speed of the procedure.

4 Numerical Experiments
In order to examine the efficiency of CISVR algorithm and compare CISVR with
LSSVR algorithm, numerical experiments are performed using two kinds of data sets.
One kind of data set is composed of the simply elementary functions which include
f ( x) = sin( x ) and f ( x) = x 2 . These functions are used to test the regression ability for
the known function. The other kind of data set is composed of Mackey-Glass (MG)
system and simple function f ( x ) = sin c( x) . The MG system is a blood cell regulation
model established in 1977 by Mackey and Glass. It is a chaos system
dx
a ⋅ x(t − τ )
=
− b ⋅ x(t ) described in [8], where τ = 17 a = 0.2 b = 0.1 Δt = 1
dt 1 + x10 (t − τ )
t ∈ (0,400) . The embedded dimensions are n = 4,6,8 respectively. The sample function
2
1 x=0
is f ( x) = ⎧⎪ sin(
.An RBF kernel function ψ ( xi , x j ) = exp(− xi − x j /(2σ 2 )) is
x)
⎨

⎪⎩ x

x≠0

employed in these two algorithms. The parameters γ and σ are showed in Tab.1. The
other parameters are as follows: θ = 0.01, ε = 0.01 . The comparison between LSSVR

Table 1. Parameters used in algorithm

γ
σ

LSSVR
CISVR

sin

square

sinc

50000
1.0
1.0

30000
1.0
1.0

5000
2.0
1.0

MG
system4
50000
2.0
1.0

MG
system6
50000
2.0
1.0

MG
system8
50000
2.0
1.0

and CISVR are showed in Tab.2, where the third column is the number of support
vectors, the forth column is the seconds for training, and the fifth and seventh columns are the regression accuracy for training and testing, respectively. The regression
accuracy is a ratio that is the number of samples whose relative error is smaller than
θ to the number of samples in the working set (testing set). The sixth and eighth
columns are the mean square error for training and testing, respectively. It can be seen
from Tab.2 that the learning speed of CISVR is much faster than LSSVR. Moreover,

Support Vector Machine Regression Algorithm Based on Chunking Incremental Learning 553
Table 2. Comparison between CISVR algorithm and standard LSSVR
Dataset

Algorithm
name

# of SVs

Train time
(CPU s)

sin
3000 × 1

LSSVR

3000

1465.05

99.93

3.58e-009

99.87

4.19e-009

CISVR

56

4.44

99.96

9.31e-007

99.70

1.04e-006

square
3000 × 1

LSSVR

3000

1463.44

99.97

2.62e-005

99.97

2.49e-005

CISVR

132

10.65

97.76

3.29e-003

97.73

2.85e-003

sinc
3000 × 1

LSSVR

3000

1448.28

99.93

8.23e-011

99.80

1.14e-010

CISVR

53

4.984

99.83

2.87e-008

99.56

3.18e-008

MG system4
6000 × 4

LSSVR

6000

11048.69

100

1.44e-008

100

1.49e-008

CISVR

16

52.35

100

6.65e-007

100

6.91e-007

MG system6
6000 × 6

LSSVR

6000

12954.34

100

8.06e-009

100

8.48e-009

CISVR

14

55.64

100

9.67e-007

100

1.02e-006

MG system8
6000 × 8

LSSVR

6000

13104.99

100

3.30e-009

100

3.28e-009

CISVR

10

54.92

100

1.08e-006

100

1.13e-006

l×n

Accuracy
(train%)

MSE
(train)

Accuracy
(test%)

MSE
(test)

the number of support vectors is less than that obtained by LSSVR for the similar
regression accuracy.

5 Discussion and Conclusion
In this paper we propose an adaptive and iterative support vector machine regression
algorithm based on the chunking incremental learning and the least square support
vector machine regression algorithm. The samples are added to the working set in
batches. The support vectors are selected adaptively in the iteration and the sparseness
of support vectors is maintained. Meanwhile, the inverse of matrix A in the previous
iteration is used to calculate the regression parameters. Therefore, the proposed approach can avoid calculating the inverse of a large-scale matrix, and at the same time,
substantially improve the learning speed compared to that of LSSVR for the similar
regression accuracy.

Acknowledgment
The authors are grateful to the support of the National Natural Science Foundation of
China (60433020), the science-technology development project for international collaboration of Jilin Province of China (20050705-2), the doctoral funds of the National
Education Ministry of China (20030183060), Graduate Innovation Lab of Jilin University (503043), and “985” Project of Jilin University. The last two authors would
like to thank the support of the EuMI School and the Erasmus Mundus programme of
the European Commission.

554

J. Jingqing et al.

References
1. Osuna E., Freund R., Girosi F.: An Improved Training Algorithm for Support Vector Machines. IEEE Workshop on Neural Networks and Signal Processing, Amelia Island, (1997)
276-285
2. Joachims T.: Making Large-scale Support Vector Machine Practical. In Advances in Kernel
Methods-Support Vector Learning, Cambridge, Massachusetts: The MIT Press, (1999)
169-184.
3. Platt J.C.: Fast Training of Support Vector Machines Using Sequential Minimal Optimization. In Advances in Kernel Methods-Support Vector Learning, Cambridge, Massachusetts:
The MIT Press, (1999) 185-208.
4. Suykens J.A.K., Vandewalle J.: Least Squares Support Vector Machine Classifiers. Neural
Processing Letters, Vol.9 (1999) 293–300.
5. Suykens J.A.K., Lukas L., Wandewalle J.: Sparse Approximation Using Least Squares Support Vector Machines. In Proc. of the IEEE International Symposium on Circuits and Systems (ISCAS 2000), Geneva, Switzerland, (2000) 757-760.
6. Hao Z.F., Yu S., Yang X.W., Hu R., Zhao F., Liang Y.C.: Online LS-SVM Learning for
Classification Problems Based on Incremental Chunk. Lecture Notes in Computer Science,
Vol.3173 (2004) 558-564.
7. Cauwenberghs G., Poggio T.: Incremental and Decremental Support Vector Machine
Learning. In Advances in Neural Information Processing Systems, Cambridge, MA: MIT
Press, Vol.13 (2001) 426-433.
8. Flake G.W., Lawrence S.: Efficient SVM Regression Training with SMO. Machine Learning, Vol.46 (2002) 271–290.

