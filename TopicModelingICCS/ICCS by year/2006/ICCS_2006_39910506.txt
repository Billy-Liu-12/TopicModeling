Parallel Exact and Approximate Arrow-Type
Inverses on Symmetric Multiprocessor Systems
George A. Gravvanis and Konstantinos M. Giannoutakis
Department of Electrical and Computer Engineering, School of Engineering,
Democritus University of Thrace, 12, Vas. Soﬁas street, GR 671 00 Xanthi, Greece
{ggravvan, kgiannou}@ee.duth.gr

Abstract. In this paper we present new parallel inverse arrow-type matrix algorithms based on the concept of sparse factorization procedures,
for computing explicitly exact and approximate inverses, on symmetric
multiprocessor systems. The parallel implementation of the new inversion algorithms is discussed and numerical results are presented, using
the simulation tool of Multi-Pascal.

1

Introduction

Sparse matrix computations, which have inherent parallelism, are of central importance, because of the applicability to real-life problems and are the most
time-consuming part in computational science and engineering computations.
Hence research eﬀorts were focused on the production of eﬃcient parallel computational methods and related software suitable for multiprocessor systems,
[1, 2, 5, 11, 12, 14].
Until recently, direct methods have been eﬀectively used, but the increase of
size, even with the use of modern computer systems, has become a barrier to
such methods, [2, 3]. Additionally, the solution of sparse linear systems, because
of its applicability to real-life problems, is obtained by iterative methods, which
are in competitive demand after the emergence of Krylov subspace methods,
[5, 11, 14].
An important achievement over the last decades is the appearance and use
of Explicit Preconditioned Methods, [4], for solving sparse linear systems, and
the preconditioned form of a linear system Au = s is M Au = M s, where M is
preconditioner, [1, 2, 5, 11, 14]. The preconditioner M has therefore to satisfy
the following conditions: (i) M A should have a ”clustered” spectrum, (ii) M can
be eﬃciently computed in parallel and (iii) ﬁnally ”M × vector” should be fast
to compute in parallel, [2, 5, 11, 12, 14].
Hence, the derivation of parallel methods was the main objective for which several families of parallel inverses of an arrow-type matrix, are proposed. The main
motive for the derivation of the parallel exact and approximate inverse matrix
algorithms is that they result in parallel direct methods and in parallel iterative
methods in conjunction with parallel preconditioned conjugate gradient - type
schemes respectively, for solving arrow-type linear systems on multiprocessor
systems.
V.N. Alexandrov et al. (Eds.): ICCS 2006, Part I, LNCS 3991, pp. 506–513, 2006.
c Springer-Verlag Berlin Heidelberg 2006

Parallel Exact and Approximate Arrow-Type Inverses

2

507

Parallel Exact and Approximate Inverses

Let us consider the arrow - type linear system, i.e.,
Au = s,
where A is a sparse arrow - type (n × n) matrix of the following form:
⎤
⎡
b1 c1
p1
⎥
⎢ a2 ❜❜❜
⎥
⎢ ❜❜ ❜
⎥
⎢
❜ ❜
0
❜
⎢
⎥
❜
❜ ❜❜
⎢
⎥
❜
❜
⎥
⎢
❜
❜ ❜❜
⎥
⎢
❜
❜
⎥.
⎢
❜
A≡⎢
❜ ❜❜
⎥
❜
❜
⎥
⎢
❜
❜ ❜❜
⎥
⎢
0
❜
❜
⎥
⎢
❜
❜ ❜❜
⎥
⎢
p
❜
❜
n−2 ⎥
⎢
❜
❜ ❜❜
⎣
❜ ❜ cn−1 ⎦
❜
v1
vn−2 an❜ bn

(1)

(2)

Arrow-type matrices occur in practice for example, in the course of the Lanczos method for solving the eigenvalue problem for large sparse matrices, in the
eigenstructure problems of arrowhead matrices which arise from applications in
molecular physics and in the application of the ﬁnite element or ﬁnite diﬀerence
method over a region by removing part of the region. This class of arrow-type
linear systems captures the class of linear systems, which can be considered as
a special tridiagonal linear systems, which occur when solving certain constantcoeﬃcient elliptic partial diﬀerential equations by the Fourier method, or using ﬁnite diﬀerence methods to solve linear constant-coeﬃcient boundary value
problems, [6, 7, 8].
Let us now assume the factorization of the coeﬃcient matrix A, i.e.
A = LU,

(3)

where L and U are sparse strictly lower and upper triangular matrices of the
same proﬁle as the matrix A, [6, 7, 8]. The elements of the L and U decomposition factors were computed by the so-called Arrow - type Approximate
LU-type Factorization algorithm (henceforth ATALUFA algorithm), [6]. The
memory requirements of the ATALUFA algorithm are O(5n) words and the
computational work is O(6n) multiplicative operations, [6].
Let M = (μi,j ), i, j ∈ [1, n], be the exact inverse of the coeﬃcient matrix A.
The elements of M = (μi,j ) can be computed by solving recursively the following
systems, [6]:
(4)
M L = U −1 and U M = L−1 .
The Exact Inverse Arrow - type Matrix algorithm (henceforth called the
EIATM algorithm) for computing the elements of the exact inverse, has been
presented in [6]. Similarly, by retaining δl elements next to the main diagonal, the elements of the approximate inverse M δl = (μi,j ), i ∈ [1, n], j ∈
[max (1, i − δl + 1) , min (n, i + δl − 1)], were computed by the Banded

508

G.A. Gravvanis and K.M. Giannoutakis

Approximate Inverse Arrow - type Matrix algorithm (henceforth called the
BAIATM algorithm), [6].
For the parallelization of the EIATM and the BAIATM algorithms on symmetric multiprocessor systems, the ATALUFA algorithm was used as a “frontend”computational procedure. The elements of the matrix M were computed in
a sequence, as shown in Eq. 5 (for n = 8 and δl = 3 without loss of generality),
because of the dependence of the elements of the inverse during the construction.
(k)
The values of the parentheses at the superscript of elements (e.g. μi,j ), indicate
that the element μi,j was computed at the k-th step of the algorithm, while the
elements with the same superscript (i.e. (k)) were computed concurently. The
pattern for the sequence of computations was considered as an anti-diagonal
motion, starting from the element μn,n down to μ1,1 , where the elements on an
anti-diagonal were computed in parallel.
⎡
⎢
⎢
⎢
⎢
⎢
⎢
⎢
⎢
⎢
⎢
M =⎢
⎢
⎢
⎢
⎢
⎢
⎢
⎢
⎢
⎣

✛

δl

✲

✧ (11)
❜ ✧(12) ✧
(15)
(14) ✧ (13)
(10)
(9)
(8) ⎤
μ1,1 ✧✧μ1,2✧✧
μ1,3 ✧❜
μ1,4 ✧ μ1,5✧✧✧μ1,6 ✧✧μ1,7✧✧μ1,8✧
✧

✧
✧
✧
✧
✧ ⎥
✧
✧(14)
❜ ✧✧
✧ (9) ✧✧
✧(11)
(13)
(12)
(10)
(8)
(7)
μ2,1 ✧✧
μ2,2✧✧μ2,3 ✧ μ2,4 ✧❜
μ2,5✧✧ μ2,6✧ μ2,7✧✧
μ2,8✧⎥
⎥
✧
✧
✧
✧
✧ ⎥
✧
✧
❜
✧ (8) ✧(7) ✧(6)
✧(13) ✧(12) ✧✧
✧(10) ✧✧
(11)
(9)
✧ μ3,6 ✧ μ3,7 ✧ μ3,8 ⎥
❜
μ3,1 ✧ μ3,2✧ μ3,3 ✧ μ3,4 ✧ μ3,5
✧⎥
✧ ❜
✧
✧
❜✧
✧
⎥
✧
✧
✧
✧
✧
✧
✧
✧
✧(12)❜ ✧(11)
✧
(10)
(9) ✧ (8) ✧ (7)❜✧ (6) ✧ (5) ⎥
μ4,1 ✧❜
μ4,2✧✧μ4,3 ✧ μ4,4✧ μ4,5✧ μ4,6✧ μ4,7✧ μ4,8✧⎥
⎥
❜✧
✧
✧
✧
✧
✧ ⎥.
❜ ✧(9) ✧✧
✧(11) ✧✧
(10)
(8)
(7)
(6) ✧ (5)
(4) ⎥
✧
✧
❜
✧
μ5,1 ✧ μ5,2✧✧
μ5,8✧⎥
✧ μ5,4 ✧ μ5,5 ✧ μ5,6 ✧ μ5,7✧ ❜
❜ μ5,3
✧
✧
✧
✧
✧
✧⎥
✧
❜
✧
✧
✧
✧
✧
✧
✧(3)❜ ⎥
✧(10)
(9)
(8)
(6)
(5)
(4)
✧ μ(7)
✧
✧
✧
✧
μ6,1 ✧✧μ6,2✧✧ μ6,3 ❜
μ
μ
μ
μ
6,5
6,6
6,7
6,8
✧⎥
✧ ❜6,4✧
✧
✧
✧
✧
⎥
✧
✧
✧
✧
✧
✧
✧
✧
✧(9) ✧ (8) ✧ (7) ✧ (6)❜ ✧ (5) ✧ (4) ✧ (3) ✧ (2) ⎥
μ7,1 ✧ μ7,2 ✧ μ7,3✧ μ7,4✧ μ7,5✧ μ7,6✧ μ7,7✧ μ7,8✧⎥
⎦
✧
✧
✧
✧
✧
✧ ❜✧
✧
✧(8)
(7)
(6)
(5)
(4)
(3) ✧ (2)
(1)
✧
✧
✧
✧
❜
✧
✧
μ8,1 ✧ μ8,2 ✧ μ8,3 ✧ μ8,4 ✧ μ8,5 ✧❜μ8,6✧ μ8,7 ✧ μ8,8

(5)

If we consider that the command forall is responsible for process creation and
execution, then the algorithm for the Parallel implementation of the EIATM
algorithm (henceforth called the PEIATM algorithm) is:
For k = 1 to n
forall l = 1 to k
call inverse(n − l + 1,n − k + l)
for k = n − 1 downto 1
forall l = 1 to k
call inverse(l,k − l + 1)
while the algorithm for the Parallel implementation of the BAIATM algorithm
(henceforth called the PBAIATM algorithm) is:
For k = 1 to δl
forall l = 1 to k
call inverse(n − l + 1,n − k + l)
m=2

Parallel Exact and Approximate Arrow-Type Inverses

509

for k = (δl + 1) to n
forall l = m to (k − m + 1)
call inverse(n − l + 1,n − k + l)
if (k − δl) mod 2 = 0 then
m=m+1
m=m−1
for k = (n − 1) downto (δl + 1)
forall l = m to k − m + 1
call inverse(l,k − l + 1)
if (k − δl) mod 2 = 1 then
m=m−1
for k = δl downto 1
forall l = 1 to k
call inverse(l,k − l + 1)
where the function inverse(i,j) computes the element μi,j of the exact inverse,
and can be described as follows, [6]:
function inverse(i,j)
if (i ≥ j) then
if (i = n) then
if (j = n) then
μn,n = 1/wn
else
if (j = n − 1) then
μn,n−1 = (−dj · μn,n ) /wj
else
μn,j = (−dj · μn,j+1 − ej · μn,n ) /wj
else
if (i = j) then
if (j = n − 1) then
μn−1,n−1 = (1 − dn−1 · μi,n ) /wj
else
μi,j = (1 − dj · μi,j+1 − ej · μi,n ) /wj
else
μi,j = (−dj · μi,j+1 − ej · μi,n ) /wj
else
if (j = n) then
if (i = n − 1) then
μn−1,n = −gn−1 · μn,n
else
μi,n = −gi · μi+1,n − hi · μn,n
else
μi,j = −gi · μi+1,j − hi · μn,j
It should be mentioned that the parallel construction of exact inverses on
distributed systems has been studied and implemented in [9], and is under further
investigation.

510

3

G.A. Gravvanis and K.M. Giannoutakis

Numerical Results

For the parallel implementation of the PEIATM and PBAIATM algorithm
described above, the simulation tool of Multi-Pascal has been utilized, where a
time unit of the simulated time is approximately equivalent to one microsecond
of the real execution time on a general purpose multiprocessor, [13]. The architecture platform is that of a shared memory system consisted of 512 processors.
The relative speedups and eﬃciency, for several values of the order n and
number of processors for the PEIATM algorithm are presented in Table 1.
In Fig. 1 and 2 the relative speedups versus the order n for several number of
processors and the relative speedups versus the number of processors for several
values of the order n, are presented respectively for the PEIATM algorithm.
Table 1. Speedups and eﬃciency for the PEIATM algorithm, for various values of
the order n and number of processors
n
15
20
40
60
80

no proc=2
no proc=4
no proc=8
no proc=13
Speedup Eﬃ/ncy Speedup Eﬃ/ncy Speedup Eﬃ/ncy Speedup Eﬃ/ncy
1.825
0.913
2.867
0.717
3.505
0.438
3.645
0.280
1.856
0.928
2.975
0.744
3.898
0.487
4.155
0.320
1.894
0.947
3.083
0.771
4.931
0.616
5.361
0.412
1.902
0.951
3.102
0.776
5.494
0.687
5.957
0.458
1.905
0.953
3.106
0.777
5.870
0.734
6.314
0.486

Fig. 1. Speedups versus the order n for the PEIATM algorithm for several number
of processors

The relative speedups and eﬃciency for several values of the order n, the
retention parameter δl and number of processors for the PBAIATM algorithm
are presented in Table 2. In Fig. 3 and 4 the relative speedups versus the retention
parameter δl for several number of processors with n = 100, and the relative
speedups versus the order n for several number of processors with δl = n/2, are
presented respectively for the PBAIATM algorithm.
It should be noted that the restrictions imposed by the developing environment used, did not allow us to fully explore cases for n > 80 for the exact inverse

Parallel Exact and Approximate Arrow-Type Inverses

511

Fig. 2. Speedups versus the number of processors for the PEIATM algorithm for
several values of the order n
Table 2. Speedups and eﬃciency for the PBAIATM algorithm, for various values of
the order n, δl and number of processors
n

δl

20

2
4
8
n/2
2
4
8
n/2
2
4
8
n/2
2
4
8
n/2
2
4
8
n/2

40

60

80

100

no proc=2
no proc=4
no proc=5
Speedup Eﬃciency Speedup Eﬃciency Speedup Eﬃciency
1.104
0.552
1.477
0.739
1.760
0.440
1.760
0.352
1.637
0.819
2.201
0.550
2.307
0.461
1.669
0.835
2.376
0.594
2.421
0.484
1.107
0.554
1.484
0.742
1.784
0.446
1.784
0.357
1.650
0.825
2.258
0.565
2.370
0.474
1.793
0.897
2.809
0.702
2.943
0.589
1.108
0.554
1.486
0.743
1.792
0.448
1.792
0.358
1.654
0.827
2.275
0.569
2.389
0.478
1.783
0.892
2.953
0.738
3.151
0.630
1.108
0.554
1.487
0.744
1.796
0.449
1.795
0.359
1.656
0.828
2.283
0.571
2.398
0.480
1.768
0.884
3.118
0.780
3.262
0.652
1.108
0.554
1.487
0.744
1.798
0.450
1.798
0.360
1.657
0.829
2.288
0.572
2.403
0.481
1.762
0.881
3.218
0.805
3.351
0.670

algorithm and for n > 100 for the approximate inverse algorithm. Additionally,
the number of processors no proc = 13 for the exact inverse and no proc = 5
for the approximate inverse, was the maximum number of processors allowed by
the simulation environment in each case (for δl = 2 the maximum number was
no proc = 2).

512

G.A. Gravvanis and K.M. Giannoutakis

Fig. 3. Speedups versus the retention parameter δl for the PBAIATM algorithm for
several number of processors with n = 100

Fig. 4. Speedups versus the order n for the PBAIATM algorithm for several number
of processors with δl = n/2

We observe that the relative speedups and eﬃciency are increasing as the order
n increases for various values of the number of processors. Although the relative
speedups increase when adding more processors on the system, the eﬃciency is
decreasing, but remains high for large values of n.
Numerical results for such arrow-type linear systems have been presented in
[6, 7, 8] for solving boundary value problems and in [10] for solving problems
considered in reliability engineering and in particular performability evaluation
of multitasking and multiprocessor systems.

4

Conclusion

The design of parallel explicit exact and approximate inverses results in eﬃcient
parallel direct and iterative methods for solving arrow-type linear systems on
multiprocessor systems. Despite the restrictions of the simulated environment
used as the experimental platform, the experimental results obtained tend to
the upper theoretical bounds.

Parallel Exact and Approximate Arrow-Type Inverses

513

Finally, further parallel algorithmic techniques will be investigated in order to
improve the speedups and eﬃciency of the parallel construction of explicit exact
and approximate inverses, on symmetric multiprocessor systems.

References
[1] Akl, S.G.: Parallel Computation: Models and Methods. Prentice Hall (1997)
[2] Dongarra, J.J., Duﬀ, I., Sorensen, D., van der Vorst, H.A.: Numerical Linear
Algebra for High-Performance Computers. SIAM (1998)
[3] Duﬀ, I.: The impact of high performance computing in the solution of linear
systems: trends and problems. J. Comp. Applied Math. 123 (2000) 515-530
[4] Evans, D.J.: Preconditioning Methods: Theory and Applications. Gordon and
Breach Science Publishers (1983)
[5] Gravvanis, G.A.: Explicit Approximate Inverse Preconditioning Techniques.
Archives of Computational Methods in Engineering 9(4) (2002) 371-402
[6] Gravvanis, G.A.: Explicit isomorphic iterative methods for solving arrow-type
linear systems. Inter. J. Comp. Math. 74(2) (2000) 195-206
[7] Gravvanis, G.A.: Solving parabolic and nonlinear 1D problems with periodic
boundary conditions. CD-ROM Proceedings of the European Congress on Computational Methods in Applied Sciences and Engineering 2000
[8] Gravvanis, G.A.: Parallel preconditioned algorithms for solving special tridiagonal
systems. Proceedings of Dynamical Systems and Applications III, (1999) 241-248
Dynamic Publishers
[9] Gravvanis, G.A.: Parallel matrix techniques. In: K. Papailiou, D. Tsahalis, J.
Periaux, C. Hirsch, M. Pandolﬁ eds. Computational Fluid Dynamics I (1998)
472-477 Wiley
[10] Gravvanis, G.A., Platis, A.N., Giannoutakis, K.M., Violentis, J.B., Lipitakis,
E.A.: Performability evaluation of multitasking and multiprocessor systems by
explicit approximate inverses. Proceedings of the International Conference on Parallel and Distributed Processing Techniques and Applications, H.R. Arabnia and
Youngsong Mun eds. III (2003) 1324-1331 CSREA Press
[11] Grote, M.J., Huckle, T.: Parallel preconditioning with sparse approximate inverses. SIAM J. Sci. Comput. 18 (1977) 838-853
[12] Grote, M.J., Simon, H.D.: Parallel preconditioning and approximate inverses on
the connection machine. In: R.F. Sincovec, D.E. Keyes, L.R. Petzold and D.A.
Reed, eds. Parallel Processing for Scientiﬁc Computing 2 (1993) 519-523 SIAM
[13] Lester, B.P.: The Art of Parallel Programming. Prentice-Hall Int. Inc (1993)
[14] Saad, Y., van der Vorst, H.A.: Iterative solution of linear systems in the 20th
century. J. Comp. Applied Math. 123 (2000) 1-33

