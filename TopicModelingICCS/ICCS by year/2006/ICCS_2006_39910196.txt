Performance Improvement of Sparse Matrix
Vector Product on Vector Machines
Sunil R. Tiyyagura1, Uwe K¨
uster1 , and Stefan Borowski2
1

2

High Performance Computing Center Stuttgart,
Allmandring 30, 70550 Stuttgart, Germany
{sunil, kuester}@hlrs.de
NEC HPCE GmbH, Heßbr¨
uhlstr. 21B, 70565 Stuttgart, Germany
sborowski@hpce.nec.com

Abstract. Many applications based on ﬁnite element and ﬁnite diﬀerence methods include the solution of large sparse linear systems using
preconditioned iterative methods. Matrix vector multiplication is one of
the key operations that has a signiﬁcant impact on the performance of
any iterative solver. In this paper, recent developments in sparse storage
formats on vector machines are reviewed. Then, several improvements to
memory access in the sparse matrix vector product are suggested. Particularly, algorithms based on dense blocks are discussed and reasons for
their superior performance are explained. Finally, the performance gain
by the presented modiﬁcations is demonstrated.
Keywords: Matrix vector product, Jagged diagonal storage, Vector
processors.

1

Introduction

The main challenge facing computational scientists and engineers today is the
rapidly increasing gap between sustained and peak performance of the high
performance computing architectures. Even after spending considerable time on
tuning applications to a particular architecture, this gap is an ever existing problem. Vector architecture provides a reasonable solution (at least up to certain
extent) to bridge this gap [1]. The success of the Earth Simulator project [2] also
emphasizes the need to look towards vector computing as a future alternative.
Sparse iterative solvers play an important role in computational science to
solve linear systems arising from discretizing partial diﬀerential equations in ﬁnite element and ﬁnite diﬀerence methods. The major time consuming portions
of a sparse iterative solver are the matrix vector product (MVP) and preconditioners based on domain decomposition like ICC, ILU, BILU, etc. The MVP
algorithm depends on the storage format used to store the matrix non-zeros.
Storage formats can be broadly classiﬁed as row, column or pseudo diagonal
oriented formats. Commonly used examples for each of the formats are compressed row storage (CRS), compressed column storage (CCS) and jagged diagonal storage (JAD). A detailed discussion of diﬀerent storage formats along with
V.N. Alexandrov et al. (Eds.): ICCS 2006, Part I, LNCS 3991, pp. 196–203, 2006.
c Springer-Verlag Berlin Heidelberg 2006

Performance Improvement of Sparse MVP on Vector Machines

197

corresponding conversions can be found in [3]. As row/column formats store the
matrix non-zero entries of each row/column, the MVP algorithm in these cases
is restricted to work on a single row/column in the innermost loop. This hinders
the performance on vector machines if the number of non-zeros per row/column
is less than the length of hardware vector pipeline. In case of NEC SX-8, a
classical vector machine, the pipelines are 256 words long. Therefore, at least
256 non-zero entries per row/column are needed to have optimal performance
on this architecture. If start-up time (pipeline depth and starting latencies) is
additionally considered, the required number of entries for optimal performance
is even larger.
Pseudo diagonal formats like the JAD storage are commonly used on vector
machines as they ﬁll up the vector pipelines and result in long average vector
length. The length for the ﬁrst few pseudo diagonals is equal to the size of the
matrix and then decreases for the latter depending on the kind of problem. This
helps in ﬁlling up the vector pipelines which results in superior performance over
other formats for MVP. However, pseudo diagonal formats are not as natural
as the row/column formats which makes their initial setup diﬃcult. In the paper on hand, we focus on performance issues of the sparse MVP algorithm on
vector machines. Diﬀerent approaches to optimize the memory access pattern
in this algorithm are addressed. The reduction of load/store operations for the
result vector is regarded by modifying the algorithm and using vector registers.
The more challenging problem of reducing the indirect memory access for the
multiplied vector is considered by introducing a block based algorithm.
In Section 2 of the paper, we take a closer look at some recently proposed
improvements for sparse storage formats. In Section 3, the changes decreasing
the memory access in the result vector are explained. Then, the block based
MVP algorithm is discussed. In Section 4, performance results are presented for
the proposed changes. Section 5 summarizes the outcome of this paper.

2

Recent studies

There have been recent studies on optimizing sparse storage formats. It is worth
understanding the implications of this work in the context of typical vector
processing.
2.1

Optimizing the Storage

Transposed jagged diagonal format (TJAD) optimizes the amount of storage
needed for a sparse matrix by eliminating the need for storing the permutation
vector [4]. However, this forces a shift of indirect addressing from the multiplied
vector to the result vector. As the result vector has to be both loaded and stored,
this eﬀectively doubles the amount of indirect addressing needed for MVP. This
is a matter of concern on conventional vector architecture, but not much on
cache based machines. Since this format is principally used on vector machines,
this cannot be the desired alternative.

198

S.R. Tiyyagura, U. K¨
uster, and S. Borowski

2.2

Improving Average Diagonal Length

Bi-jagged diagonal storage (Bi-JDS) combines both common JAD and TJAD to
further increase the average length of diagonals [5]. The main idea is to store
all the full length diagonals in JAD format and the remaining matrix data in
TJAD format. TJAD has the disadvantage of increasing the indirect addressing
as explained in the previous section. Setting up such a format within the scope
of the whole iterative solver would cause changes to all algorithms that use the
matrix data such as preconditioning. Furthermore, a lot of problems yield matrices with satisfactorily long average vector lengths in JAD format like matrices
from structured ﬁnite element, ﬁnite diﬀerence and ﬁnite volume analysis. For
problems involving surface integrals, it is common that the number of non-zeros
in only some of the rows are extremely high. In this case, it may be advantageous
to use such a scheme. This has to be extensively tested to measure its pros (long
diagonals) against its cons (set-up cost and doubled indirect addressing).
2.3

Row Format on Vector Machines

Compressed row storage with permutation (CRSP) was introduced in [6]. Results
for this format on Cray X1 show a performance of the MVP algorithm that is
an order of magnitude higher than for common CRS format. The permutation
introduced in the CRSP format adds a level of indirection to both the matrix and
the multiplied vector, i.e. eﬀectively two additional indirectly addressed memory
loads per loop iteration. This is the overhead incurred because of permuting the
rows in ascending order of their length. Although this results in tremendous
performance improvement on Cray X1 due to caching, such an algorithm would
perform poorly on conventional vector machines because of the heavy cost of
indirect addressing involved (in absence of cache memory). The overhead of
indirection in memory access is elaborated in the next section.

3

Improvements to the Algorithm

Here, several changes to the JAD MVP algorithm are proposed, which improve
the performance on vector machines. In order to reduce memory access for the
result vector, an algorithmic approach is to operate on more than one diagonal in the innermost loop. A technical alternative is the use of vector registers
oﬀered by vector machines. Finally, the more critical issue of reducing indirect
memory access for the multiplied vector is addressed by considering block based
algorithms. Before introducing the changes, a brief introduction to features of a
vector architecture is provided.
3.1

Vector Processor

Vector processors like NEC SX-8 use a very diﬀerent architectural approach than
scalar processors. Vectorization exploits regularities in the computational structure to accelerate uniform operations on independent data sets. Vector arithmetic

Performance Improvement of Sparse MVP on Vector Machines

199

instructions are composed of identical operations on elements of vector operands
located in vector registers.
For non-vectorizable instructions the NEC SX-8 processor also contains a
cache-based superscalar unit. Since the vector unit is by far more powerful than
the scalar unit, it is crucial to achieve high vector operation ratios, either via
compiler discovery or explicitly through code and data (re-)organization. The
vector unit has a clock frequency of 2 GHz and provides a peak vector performance of 16 GFlop/s (4 add and 4 multiply pipes working at 2 GHz). The total
peak performance of the processor is 22 GFlop/s (including divide/sqrt unit and
scalar unit). Table 1 gives an overview of the diﬀerent processor units.
Table 1. NEC SX-8 processor units
Unit

No. of results Peak (GFlop/s)
per cycle
Add
4
8
Multiply
4
8
Divide/sqrt
2
4
Scalar
2
Total = 22

3.2

Original JAD MVP Algorithm

The original JAD MVP algorithm is listed in Fig. 1. For simplicity, the permutation of the result vector is not shown. The performance limitations of this
algorithm can be better understood in terms of an operation ﬂow diagram shown
in Fig. 2, which explains the execution of the vector instructions. It should be
noted that there is only one load/store unit in the NEC SX-8 processor. In each
clock cycle, two ﬂoating point (FP) operations per pipeline are possible : 1 add
and 1 multiply. The main bottleneck of this algorithm is the indirect load of the
multiplied vector (vec) which takes roughly 5 times longer than a directly addressed one on NEC SX-8. On superscalar architectures, this factor is in general
even greater and more complicated to predict.
In 9 cycles (5 for indirectly addressed vector), the possible number of FP
operations is 18 (add and multiply). But the eﬀective FP operations in the
innermost loop of the algorithm are only 2 (1 add and 1 multiply). Hence, the
expected performance of this operation would be 2/18th of the vector peak.
for i=0, number_of_diagonals
offset = jad_ptr[i]
diag_length = jad_ptr[i+1] - offset
for j=0, diag_length
res[j] += mat[offset+j] * vec[index[offset+j]]
end for
end for
Fig. 1. Original JAD MVP algorithm

200

S.R. Tiyyagura, U. K¨
uster, and S. Borowski

Load/Store unit

Load
index

Ind. Load
vec

Load
mat

Load
res

Store
res

Multiply

Multiply unit

Add

Add unit

Cycles

Fig. 2. Operation ﬂow diagram for original JAD MVP algorithm

3.3

Working on Groups of Diagonals

Matrices originating from structured grids have groups of pseudo diagonals with
equal length (stored in JAD format). One way to improve the performance is
to operate on groups of equal length diagonals in the innermost loop instead of
a single diagonal. This considerably saves load/store operations for the result
vector (res). The accordingly modiﬁed algorithm is listed in Fig. 3. For simplicity, it works on utmost 2 diagonals of equal length. Extending this procedure to
more diagonals improves the performance notably.
for i=0, number_of_diagonals
offset = jad_ptr[i]
dia_length = jad_ptr[i+1] - offset
if( ((i+1)<number_of_diagonals) &&
(dia_length==(jad_ptr[i+2]-jad_ptr[i+1])) )
offset1 = jad_ptr[i+1]
for j=0, diag_length
res[j] +=
mat[offset+j] * vec[index[offset+j]]
+ mat[offset1+j] * vec[index[offset1+j]]
end for
i = i+1
else
for j=0, diag_length
res[j] += mat[offset+j] * vec[index[offset+j]]
end for
end if
end for
Fig. 3. JAD MVP algorithm grouping at most 2 diagonals

3.4

Use of Vector Registers

Most vector machines provide a programmer interface to vector registers in
order to temporarily store data, like the result vector (res). Using vector registers would need the user to strip mine the innermost loop. The resulting algorithm is listed in Fig. 4. This procedure does not depend on the grid

Performance Improvement of Sparse MVP on Vector Machines

201

//Size of the hardware vector register
strip = 256
for j0=0, number_of_rows, strip
//Initialising the vector register
for j=0, strip-1
vregister[j]=0.0
end for
//Performing the multiplication
for i=0, number_of_diagonals
offset = jad_ptr[i]
diag_length = jad_ptr[i+1] - offset
for j=j0, min(diag_length, j0+strip-1)
vregister(j-j0) += mat[offset+j] * vec[index[offset+j]]
end for
end for
//Write results to memory
for j=j0, min(number_of_rows,j0+strip-1)
res[j] = vregister[j-j0]
end for
end for
Fig. 4. JAD MVP algorithm using vector registers

(structured/unstructured) and hence equally reduces the memory access for the
result vector.
3.5

Operating on Dense Blocks

In the point based algorithms discussed so far, the major hurdle to performance
of MVP is indirect memory addressing. To overcome this, block based computations exploit the fact that many problems have multiple physical variables per
node. Thus, small blocks can be formed by grouping the equations at each node.
Operating on such dense blocks considerably reduces the amount of indirect
addressing required for MVP. This improves the performance dramatically on
vector machines [7] and also remarkably on superscalar architectures [8]. The
block based algorithm for MVP is listed in Fig. 5 (block size 2 for simplicity).
The reduction in indirect addressing can be clearly seen in the corresponding
operation ﬂow diagram shown in Fig. 6. The multiplied vector is indirectly addressed only twice (vec1 and vec2) instead of four times as it would be the case
for the original algorithm. The needed index vector index is only loaded once
and then incremented. To generalize, indirect memory addressing is reduced by
a factor of block size (2 in this case).

4

Performance Results

The performance of the algorithm working on groups of diagonals is listed in
Table 2. The case tested was a structured ﬁnite element problem with 8500

202

S.R. Tiyyagura, U. K¨
uster, and S. Borowski

blksize = 2
for i = 0, number_of_diagonals
offset = jad_ptr[i]
diag_length = jad_ptr[i+1] - offset
for j = 0, diag_length
temp_mat = (offset+j)*blksize*blksize
temp_vec = (index[offset+j])*blksize
vec1 = vec(temp_vec)
vec2 = vec(temp_vec+1)
res(j*blksize)
+=
mat(temp_mat)
+ mat(temp_mat+1)
res(j*blksize+1) +=
mat(temp_mat+2)
+ mat(temp_mat+3)
end for
end for

*
*
*
*

vec1
vec2
vec1
vec2

Fig. 5. Block based JAD MVP algorithm (for 2 × 2 blocks)

Load/Store unit
Multiply unit
Add unit

Ind. Load
index/vec1

Load
mat0,2

Load
res0,1

Load
vec2

Load
mat1,3

Store
res0,1

Mul/Mul

Mul/Mul
Add/Add

Add/Add

Cycles

Fig. 6. Operation ﬂow diagram for block based JAD MVP algorithm

hexahedral elements, 10065 nodes and 40260 unknowns. Increasing the group
size improves the performance signiﬁcantly. Grouping 9 diagonals results in a
performance improvement of 78% over the original algorithm. Using vector registers, the performance of MVP was measured to be 2197 MFlop/s, about 23%
improvement compared to the original algorithm.
Table 2. Performance of JAD MVP algorithm grouping diagonals
Max. group size Performance (MFlop/s)
orig
1780
3
2511
5
2830
9
3181

Block based algorithms perform much better than the point based ones. For
small test cases the performance of MVP for block sizes 4 and 5 is listed in
Table 3. Operating on small dense blocks is a superior way to achieve a good

Performance Improvement of Sparse MVP on Vector Machines

203

percentage of the peak performance on vector machines (37% for block size 5).
The performance improvement over the original algorithm is 235%. The improvement compared to the best performing point based algorithm introduced
is 87%.
Table 3. Performance of block based JAD MVP algorithm
Block size Performance (MFlop/s)
orig
1780
4
5804
5
5969

5

Summary

Several improvements to the matrix vector product algorithm based on jagged
diagonal storage for vector machines have been suggested in the paper. Most
of the memory access problems in this key operation of sparse iterative solvers
have been addressed. Block based algorithms are necessary to achieve a good
portion of the peak performance on vector machines and moreover should also
beniﬁt superscalar architectures. Further work is still required to look for eﬃcient
preconditioning methods based on dense blocks.

References
1. Oliker, L., Canning, A., Carter, J., Shalf, J., Ethier, S.: Scientiﬁc computations on
modern parallel vector systems. In: Proceedings of the ACM/IEEE Supercomputing
Conference 2004, Pittsburgh, USA (2004)
2. http://www.es.jamstec.go.jp/esc/eng/.
3. Saad, Y.: SPARSKIT: A basic toolkit for sparse matrix computations. Technical
Report RIACS-90-20, NASA Ames Research Center, Moﬀet Field, CA (1994)
4. Montagne, E., Ekambaram, A.: An optimal storage format for sparse matrices.
Information Processing Letters 90 (2004) 87–92
5. Hossain, S.: On eﬃcient storage of sparse matrices. In: 2005 Istanbul Computational
Science and Engineering Conference (ICCSE ’05), ITU, Istanbul (2005)
6. D’Azevedo, E.F., Fahey, M.R., Mills, R.T.: Vectorized sparse matrix multiply for
compressed row storage format. In: Proceedings of the 5th International Conference
on Computational Science, Atlanta, USA, Springer-Verlag (2005)
7. Nakajima, K.: Parallel iterative solvers of geofem with selective blocking preconditioning for nonlinear contact problems on the earth simulator. GeoFEM 2003-005,
RIST/Tokyo (2003)
8. Tuminaro, R.S., Shadid, J.N., Hutchinson, S.A.: Parallel sparse matrix vector multiply software for matrices with data locality. Concurrency: Practice and Experience
(3)10 (1998) 229–247

