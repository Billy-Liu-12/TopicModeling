Comparison of the Computational Cost of a
Monte Carlo and Deterministic Algorithm for
Computing Bilinear Forms of Matrix Powers
Christian Weihrauch, Ivan Dimov, Simon Branford, and Vassil Alexandrov
Centre for Advanced Computing and Emerging Technologies
School of System Engineering, The University of Reading
Whiteknights, PO Box 225, Reading, RG6 6AY, UK
{c.weihrauch, i.t.dimov, s.j.branford, v.n.alexandrov}@reading.ac.uk
Institute for Parallel Processing, Bulgarian Academy of Sciences
Acad. G. Bonchev 25 A, 1113 Sofia, Bulgaria
ivdimov@bas.bg

Abstract. In this paper we consider bilinear forms of matrix polynomials and show that these polynomials can be used to construct solutions
for the problems of solving systems of linear algebraic equations, matrix
inversion and finding extremal eigenvalues. An almost Optimal Monte
Carlo (MAO) algorithm for computing bilinear forms of matrix polynomials is presented.
Results for the computational costs of a balanced algorithm for computing the bilinear form of a matrix power is presented, i.e., an algorithm
for which probability and systematic errors are of the same order, and
this is compared with the computational cost for a corresponding deterministic method.
Keywords: Monte Carlo algorithms, matrix computations, performance
analysis, computational cost, iterative process.

1

Introduction

Many scientiﬁc and engineering applications are based on the problems of ﬁnding
extremal eigenvalues, solving a system of linear algebraic equations (SLAE), or
inverting a real n×n matrix (MI). The computation time for very large problems,
or for ﬁnding solutions in real-time, can be prohibitive and this prevents the use
of many established algorithms. Monte Carlo methods give statistical estimates
of the required solution, by performing random sampling of a random variable,
whose mathematical expectation is the desired solution [10, 11].
Several authors have presented work on the estimation of computational complexity of linear algebra problems [4, 5, 6, 12, 13, 14]. In this paper we consider
bilinear forms of matrix powers, which can be used to formulate solutions for all
three problems. Considering the set, A, of algorithms, A, for calculating bilinear
forms of matrix powers
A = {A : P r{rn,N ≤ ε} ≥ c} ,
V.N. Alexandrov et al. (Eds.): ICCS 2006, Part III, LNCS 3993, pp. 640–647, 2006.
c Springer-Verlag Berlin Heidelberg 2006

Computational Cost of a Monte Carlo and Deterministic Algorithm

641

with a probability error less than a given constant ε, there is the practical question of which algorithm in the set has the smallest computational cost. In this
paper we compare the computational cost of two such method - a Monte Carlo
algorithm and a deterministic algorithm.
The formulation of MI, SLAE and ﬁnding extreme eigenvalues in terms of bilinear forms of matrix powers is presented in Section 2; in Section 3 we present a
Monte Carlo algorithm for ﬁnding the bilinear form of a matrix power; the computational cost of the Monte Carlo algorithm and of the deterministic method
are presented in Section 4; and we conclude the work in Section 5.

2

Formulation of the Problems

In this paper we are interested in the evaluation of forms
(v, p(A)h) ,

(1)

where p(A) is a matrix polynomial and v, h ∈ IRn are arbitrary vectors.
2.1

Bilinear Form of Matrix Powers

In a special case of p(A) = Ak then (1) becomes
(v, Ak h).
2.2

Eigenvalues of Matrices

The well-known Power method [9] gives an estimate for the dominant eigenvalue
λ1 , of a matrix A. This estimate is called Rayleigh quotient :
(v, Ak h)
,
k→∞ (v, Ak−1 h)

λ1 = lim

where v, h ∈ IRn are arbitrary vectors. The Rayleigh quotient is used to obtain,
for an arbitrary large natural number k, an approximation:
λ1 ≈

(v, Ak h)
.
(v, Ak−1 h)

(2)

To construct an algorithm for evaluating the minimal by modulo eigenvalue,
λn , one has to consider the following matrix polynomial:
∞

p(A) =

k
q k Cm+k−1
Ak ,

(3)

k=0
k
where Cm+k−1
are binomial coeﬃcients, and the characteristic parameter, q, is
used as acceleration parameter of the algorithm [4, 7, 8].
If |qA| < 1, then the polynomial (3) becomes the resolvent matrix [6, 7]:
∞

p(A) =
k=0

k
q k Cm+k−1
Ak = [I − qA]−m = Rqm ,

642

C. Weihrauch et al.

where Rq = [I − qA]−1 is the resolvent matrix of the equation
x = qAx + h.

(4)

Values q1 , q2 , . . . (|q1 | ≤ |q2 | ≤ . . .) for which equation (4) is fulﬁlled are called
characteristic values of the equation. The resolvent operator Rq = [I − qA]−1 =
A + qA2 + . . . exists if the sequence converges.
Let us consider the ratio:
λ=
If q < 0, then

(v, ARqm h)
(v, Ap(A)h)
=
.
(v, p(A)h)
(v, Rqm h)

(v, ARqm h)
1
≈
(v, Rqm h)
q

1−

1
μ(k)

≈ λn ,

(5)

where λn = λmin is the minimal by modulo eigenvalue, and μ(k) is the approximation to the dominant eigenvalue of Rq .
If |q| > 0, then
(v, ARqm h)
≈ λ1 ,
(6)
(v, Rqm h)
where λ1 = λmax is the dominant eigenvalue.
The approximate equations (2), (5) and (6) can be used to formulate eﬃcient
Monte Carlo algorithms for evaluating both the dominant and the minimal by
modulo eigenvalue of real symmetric matrices.
2.3

Bilinear Forms of Solution of LAE Systems

Consider the bilinear form:
(v, x),

(7)

where x is the solution of the system:
Bx = b.

(8)

For a non-singular matrix B one can use the presentation of Jacobi Overrelaxation Iterative Method:
x = Ax + h.
(9)
Assume that matrix A satisﬁes:
n

|aij | < 1, i = 1, . . . , n.

(10)

j=1

Now, we can consider the first-order stationary linear iterative process for the
system (9):
x(k) = Ax(k−1) + h, k = 1, 2, . . . .
(11)

Computational Cost of a Monte Carlo and Deterministic Algorithm

643

In fact, the presentation (11) deﬁnes a Neumann series
x(k) = h + Ah + . . . + Ak−1 h + Ak x(0) .

(12)

It is a well-known fact that the property (10) is a suﬃcient condition for
convergence of the Neumann series, i.e.,
x = lim x(k) .
k→∞

It is clear, that every iterative algorithm (including those based on MC) uses a
ﬁnite number of iterations k. If a MC algorithm is applied, then the k th iteration
can be computed with an additional statistical error. In practice the truncating parameter k is not a priori given parameter. Normally it is obtained from
the condition that the diﬀerence between the stochastic approximation of two
successive approximations is smaller than a given suﬃciently small parameter ε.
Thus, we approximate the bilinear form (7) by
k

(v, x) ≈

Ai h .

v,

(13)

i=0

One can see now, that for this problem, the matrix polynomial is of special type,
k
i.e., p(A) = i=0 Ai .
If the Neumann series (12) does not converge the technique of mapping can
be applied. This gives us a resolvent method, with the bilinear form (7):
k

(v, x) ≈

gi Ai h .
(k)

v,

(14)

i=0

This procedure leads to matrix polynomial of type: p(A) =
2.4

(k) i
k
i=1 gi A .

Matrix Inversion

Assume B ∈ IRn×n is a non-singular matrix. The problem of ﬁnding the inverse
matrix
C = B −1
of B is equivalent to solve n times the problem (8) written in the following form:
Bcj = e(j) , j = 1, . . . , n,
where e(j) ≡ (0, . . . , 0, 1 , 0, . . . , 0)T and cj ≡ (c1j , . . . , cnj )T is the j th column
j

of the inverse matrix C = B −1 . This leads to the following bilinear form:
k

cij = (e(i) , cj ) ≈

Ai dj e(j)

e(i) ,

.

(15)

i=0

It is easy to see, that the latter form (15) is the same as (13) for a special choice
of vectors v and h: v = e(i) and h = dj e(j) .

644

3

C. Weihrauch et al.

Formulation of the MC Algorithm

We shall use the so-called MAO algorithm, studied in [1, 2, 3, 5, 6], where
pi =

3.1

|vi |
,
v

n

v

|vi |

=

and

pij =

i=1

n

|aij |
,
ai

|aij |.

ai =

(16)

j=1

MC Algorithm for Computing Bilinear Forms of Matrix Powers
(v, Ak h)

From the pair of density distributions (16) we obtain a ﬁnite chain, which induces
k
k
the matrix/vector powers Akv = vα0 s=1 aαs−1 αs and Akv = v × s=1
sign{Ak }

v
aαs−1 . With such densities we have that E{hαk } =
v, Ak h .
Ak
v
If we consider N realizations of the Markov chain Tk = α0 → α1 → . . . → αk ,
then

N

N

(k)

θ¯(k) =

θi

= sign{Akv }

Akv

{hαk }i

i=1

(17)

i=1

is an MC approximation of the bilinear matrix power (v, Ak h). The probability
error of this approximation is
1
(k)
RN = (v, Ak h) − θ¯(k) = cp σ{θ(k) }N − 2 .

(18)

From (17), together with the sampling rules using (16), leads us to a MC
(k)
algorithm for estimating (v, Ak h) with a probability error RN . The quality of
the MC algorithm depends on the behaviour of the standard deviation σ{θ(k) }.

4

Performance Analysis

In this section we formulate the computational cost of a Monte Carlo algorithm
and a deterministic algorithm for computing the bilinear forms of matrix powers
(v, Ak h). To do this we use the following notation: α is a cost of an addition or
subtraction; β is a cost of a multiplication; and δ is a cost of a logical operation.
4.1

Computational Cost of MC Algorithm for Computing Bilinear
Forms of Matrix Powers (v, Ak h)

To estimate the computational cost one needs to consider the following expression:
k

sign{vα0 }

|vα |
α

sign{aαi−1 αi }
i=1

|aαi−1 αi | hαk .

(19)

αi

The computational cost of (19) is thus:
δ + β + (n − 1)α + β + (k − 1)β + kδ + k(β + (n − 1)α) + β
= (k + 1)(n − 1)α + (2k + 2)β + (k + 1)δ.

(20)

Computational Cost of a Monte Carlo and Deterministic Algorithm

645

For the generation of the required random numbers and for the selection of
the elements of v and A the following operations are required:
m + δ log n + k (m + δ log n) ,

(21)

where m is the number of operations required to generate a random number.
The δ log n term is from the binary search method [15] used to select the next
element in the Markov Chain, as used in [1].
To compute the MC approximation to (v, Ak h) one needs to perform N realizations of the Markov chain, so that the computational cost of the algorithm
is
N [(k + 1)(n − 1)α + 2(k + 1)β
+(k + 1)δ + m + δ log n + k (m + δ log n)] .
4.2

(22)

Computational Cost of a Deterministic Method for Computing
Bilinear Forms of Matrix Powers (v, Ak h)
n

For a matrix-vector multiplication: j=1 aij hj , ∀i ∈ 1, . . . , n, the computational cost is:
n ((n − 1)α + nβ) = n(n − 1)α + n2 β.
For a vector-vector multiplication:

n
j=1 vj hj

the computational cost is:

(n − 1)α + nβ.
To compute the bilinear form of a matrix power, (v, Ak h), one needs to perform k matrix-vector multiplications and one vector-vector multiplication. Thus,
the computational cost of the deterministic method is:
k[n(n − 1)α + n2 β] + (n − 1)α + nβ
= (kn2 − kn + n − 1)α + (kn2 + n)β.
4.3

(23)

Comparison of the Monte Carlo and Deterministic Method

For a rough estimation of the cost we may assume that α = β = δ = 1. Then,
from (22), the computational cost for the Monte Carlo algorithm becomes:
N (kn + n + k log n + log n + km + m + 2k + 2) = N kn + O(N n)

(24)

and the computational cost for the deterministic method, from (23), is:
2kn2 − kn + 2n − 1 = 2kn2 + O(kn).

(25)

A comparison of costs (24) and (25) shows that, for a suﬃciently large matrix, the
Monte Carlo algorithm will be quicker than the deterministic method, provided

646

C. Weihrauch et al.

that one can keep the number of Markov chains required to reach a suitably
accurate solution low enough. As a rough criteria for choosing the proposed
Monte Carlo algorithm we can use the following inequality:
1N
≤ 1.
2n
If the required accuracy to compute (v, Ak h) is ε, then (according to (18))
the following inequality
2ε2 n ≥ c2p σ 2 (Θ(k) )
should be fulﬁlled. Let us note that for many real-life applications the matrix
size n is 107 − 108 and the typical number of Markov chains N is 104 − 105 . In
such cases the Monte Carlo algorithm is deﬁnitely preferable. However, it should
be mentioned that if the matrix size n is close to N , then more accurate analysis
of the computational cost taking into account weights α, β and δ of diﬀerent
operations has to be done.

5

Conclusion

In this paper we introduced the matrix polynomial form (1). Further (2), (5) and
(6) show how this form can be used to ﬁnd extreme eigenvalues; (13) and (14)
show how this form can be used to solve SLAE; and (15) show how to extend
the solution of SLAE to MI.
From there we concentrated on the bilinear form of matrix polynomials and
constructed a Monte Carlo algorithm for solving this problem. This allowed us
to estimate the computational cost of the Monte Carlo algorithm, as well as the
cost of the deterministic approach. These results on computational cost show
that the Monte Carlo algorithm will out perform the deterministic method on
suﬃciently large problems, which are common to many areas of mathematical
modelling. In cases when the matrix size n is close to the required number of
Markov chains N a careful error analysis of the MC algorithm is needed.

References
1. V. Alexandrov, E. Atanassov, I. Dimov, Parallel Quasi-Monte Carlo Methods for
Linear Algebra Problems, Monte Carlo Methods and Applications, Vol. 10, No. 3-4
(2004), pp. 213-219.
2. I. Dimov, Minimization of the Probable Error for Some Monte Carlo methods.
Proc. Int. Conf. on Mathematical Modeling and Scientific Computation, Albena,
Bulgaria, Sofia, Publ. House of the Bulgarian Academy of Sciences, 1991, pp. 159170.
3. I. Dimov, Monte Carlo Algorithms for Linear Problems, Pliska (Studia Mathematica Bulgarica), Vol. 13 (2000), pp. 57-77.
4. I. Dimov, V. Alexandrov, A. Karaivanova, Parallel resolvent Monte Carlo algorithms for linear algebra problems, J. Mathematics and Computers in Simulation,
Vol. 55 (2001), pp. 25-35.

Computational Cost of a Monte Carlo and Deterministic Algorithm

647

5. I. Dimov, T. Dimov, T. Gurov, A New Iterative Monte Carlo Approach for Inverse Matrix Problem, Journal of Computational and Applied Mathematics, Vol.
92 (1997), pp. 15-35.
6. I.T. Dimov, V. Alexandrov, A New Highly Convergent Monte Carlo Method for
Matrix Computations, Mathematics and Computers in Simulation, Vol. 47 (1998),
pp. 165-181.
7. I. Dimov, A. Karaivanova, Parallel computations of eigenvalues based on a Monte
Carlo approach, Journal of Monte Carlo Method and Applications, Vol. 4, Nu. 1,
(1998), pp. 33-52.
8. I. Dimov, A. Karaivanova, A Power Method with Monte Carlo Iterations, Recent Advances in Numerical Methods and Applications, (O. Iliev, M. Kaschiev,
Bl. Sendov, P. Vassilevski, Eds., World Scientific, 1999, Singapore), pp. 239-247.
9. G.V. Golub, C.F. Van Loon, Matrix computations (3rd ed.), Johns Hopkins Univ.
Press, Baltimore, 1996.
10. I.M. Sobol Monte Carlo numerical methods, Nauka, Moscow, 1973.
11. J.R. Westlake, A Handbook of Numerical matrix Inversion and Solution of Linear
Equations, John Wiley & Sons, inc., New York, London, Sydney, 1968.
12. I. Dimov, O.Tonev, Monte Carlo Algorithms: Performance Analysis for Some Computer Architectures, Journal of Computational and Applied Mathematics, Vol. 48
(1993), pp. 253-277.
13. I. Dimov, Monte Carlo Algorithms for Linear Problems, Pliska (Studia Mathematica Bulgarica), Vol. 13 (2000), Proceedings of the 9th International Summer School
on Probability Theory and Mathematical Statistics, Sozopol, 1997, pp. 57-77.
14. I. Dimov, A. Karaivanova and P. Yordanova, Monte Carlo Algorithms for calculating eigenvalues, Second International Conference on Monte Carlo and Quasi-Monte
Carlo methods in scientific computing, University of Salzburg, 8-12 July, 1996, (in:
Proceedings of MC & QMC 96, Springer Notes in Statistics (H. Niederreiter, P.
Hellekalek, G. Larcher and P. Zinterhof, Eds)), 1997, pp. 205-220.
15. D. Knuth, The Art of Computer Programming, Volume 3: Sorting and Searching,
Third Edition. Addison-Wesley, 1997.

