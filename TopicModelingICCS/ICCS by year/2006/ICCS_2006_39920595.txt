Creation and Control of Interactive Virtual
Environments
Adrian Haﬀegee, Priscilla Ramsamy, Ronan Jamieson, and Vassil Alexandrov
Advanced Computing and Emerging Technologies Centre,
The School of Systems Engineering, University of Reading,
Reading, RG6 6AY, United Kingdom
sir04amh@reading.ac.uk

Abstract. Within the conﬁnes of a Virtual Environment (VE) almost
anything is possible. It is easy to establish the beneﬁts such an application could provide throughout the many walks of life, and yet current VE
development remains within the domain of Virtual Reality application
programmers. We describe methods that enhance VE development, ﬁrst
by providing scene creation for non-programmers, and second through
a scene management entity that controls interaction within the environment. We explore methods for interacting through the scene to enable
multiuser collaboration, and detail sample applications making use of
this approach.

1

Introduction

The ﬁeld of Virtual Reality (VR) concerns itself with the study of how the senses
and perception can by fooled into believing that virtual objects exist, or that
a non existing experience has occurred. This ability to alter what each of the
senses is telling the brain, combined with the way the brain is translating them,
provides vast scope in exploring the diﬀerent aspects of reality. The uses of a
perfect VR system are only limited by the imagination provided in development
of its applications. Those depicting objects could have commercial uses in previewing or analyzing products. Alternatively medical based objects could provide
surgeons a preview of what they will be facing without the prior need of investigative surgery. Indeed a virtual representation of any object could be created
and then limitlessly studied and modiﬁed, thereby negating a large dependence
on real world models or prototypes.
Rather than just depicting individual objects, an entire Virtual Environment
(VE) could also be constructed. Representations of real environments could be
used to experience products or locations, such as a new house or a holiday
destination, while viewing from a diﬀerent location.
Adding user interaction creates a more dynamic environment that could react
to certain user provided stimulii. Examples here would include training exercises
where users could be repeatedly placed inside simulations of otherwise diﬃcult
to reproduce situations, thereby learning as though they were in the real environment. However VEs need not just be constrained to environments that
V.N. Alexandrov et al. (Eds.): ICCS 2006, Part II, LNCS 3992, pp. 595–602, 2006.
c Springer-Verlag Berlin Heidelberg 2006

596

A. Haﬀegee et al.

humans would naturally ﬁnd themselves in. For instance animal behavior biologists may beneﬁt from experiencing an environment as though they were seeing
through the eyes of a diﬀerent creature, or for entertainment purposes a user
could become a hero in some ﬁctional fantasy world.
The natural extension to having one person using a VE is to allow multiple
users to share the environment. Ideally this should be possible regardless of the
users’ physical location, allowing geographically dispersed users to meet together
in a Networked Virtual Environment (NVE)[1].
Once multiple users are connected together within a VE it is possible to use
such an environment to replace current forms of remote user to user communication and interaction. These current methods make use of limited resources such
as telephones, videoconferencing systems through to simple pens and paper. All
of this functionality can brought into the almost limitless domain of the VE,
making it a perfect medium supporting inter-person communications. However
such technology extends beyond the boundaries imposed through the conventional resources; rather than just conveying their views to distant participants,
users can interact with each other within their shared virtual world, dynamically
manipulating it to convey their thoughts.
One issue eﬀecting the usage of VEs is the eﬀort taken in their development.
While tools for their creation do exist [2, 3] they require programming experience along with a detailed knowledge of the area. This coupled with limited
accessibility to VR hardware has resulted in restricted VE development. Current
environments have little interaction between their users, with them often only
sharing head and hand positioning and simple action events. Although some work
has been undertaken in bringing realistically mixed audio into the environment
[4, 5], generally external audio applications [6, 7] are used by VE participants.
Recently, the AGJuggler project [8] has attempted to bring the AccessGrid [9]
conferencing technology into the virtual world. This is still a work in progress,
and beyond the inclusion of remote video streams into the VE, currently has
limited user interaction. However it does demonstrate areas where real world
communications technology is beginning to merge into the VE.
Assuming the complexities of VE implementation can be ignored there also
exists the problem of designing the environment itself. Whilst almost anyone
could populate a blank environment with walls, ﬂoors, windows and doors, it
would still take someone with architectural knowledge to be able to construct a
practical and realistic virtual building. Similarly a psychologist would be needed
to develop environments for psychological testing, or a doctor for simulating
medical procedures. It is clear that the development of these advanced environments require skills that are not usually available for VR programmers.
This paper describes a method for creating and controlling interactive virtual
environments that can be used by most people regardless of their computer programming background. Section 2 details the tools that achieve this functionality
before section 3 describes how they can be used for diﬀerent types of interaction.
Section 4 describes sample applications built around this methodology and then
the paper concludes with section 5.

Creation and Control of Interactive Virtual Environments

2

597

Controlling the Virtual World

This work builds on top of a lower level VR application development toolset [10],
that described a library of infrastructure tools providing functionality such as
display management and user representation/control. A key development from
this work was the creation of a common architectural platform for writing VR
applications. Since they share the same source code, these applications could be
used for cross system development from desktop PCs through to CAVEs [11].
The new additions presented here consist of a conﬁguration tool that provides
non-programmers with a method of constructing a VE from various virtual objects, and the scene controller which is a management entity that administers
dynamic aspects of a scene.
2.1

Scene Components

Generally speaking immersive VR can be used to represent 2 diﬀerent types
of data; visualization and realistic, lifelike scenes. Visualization concerns itself
with the visual representation and manipulation of data sets. It does not try to
persuade the user that the data is anything more than what it is, and immersion
is only used to provide improved representation. Lifelike scenes however are used
to persuade the user that they really are in an alternate place. The greater the
user feels attached to the scene and has a presence within it, the more successful
the VE and the greater its likelihood of achieving its desired results. This split
does not mean visualization cannot be used in lifelike scenes; on the contrary
data sets are often used in such scenes, however then the visualization object is
just a part of the full scene.
Similarly to the number of objects that surround us in real life, a virtual
scene consists of a heterogeneous array of diﬀerent virtual objects. Creating a
believable lifelike scene depends on incorporating the right objects for the VE
being built.
Objects created from primitive geometries have basic shapes such as spheres,
boxes and cylinders. They are easy to deﬁne but lack the complexity required for
many scenes. However they often have uses representing basic man-made objects
such as balls, walls, tubes and so on.
More complex complex objects can be created through CAD or modeling
packages. Assuming they are available in, or can be converted to an acceptable
format, they can be brought into the VE as realistic representations. For example, an architect could create a CAD model of a house he is designing and add
it life size to an environment. He could then enter the environment and examine
or demonstrate his design before any materials have even been ordered, allowing
potential modiﬁcations if required.
Textures can be added to objects to improve realism, and range from simple
patterns to high quality images. Such textures can greatly enhance the virtual
objects, for instance a basic box can be covered in a texture of a previously
photographed item to create a realistic virtual representation (see ﬁgure 1).
Being image based, textures can be enhanced for providing additional eﬀects

598

A. Haﬀegee et al.

Fig. 1. CAVE applications demonstrating virtual shopping (left), multimedia conferencing (middle) and a virtual control room (right)

such as areas of transparency, lighting artifacts (reﬂections/shadows), aging or
fogging. Textures do not need to remain static, and useful eﬀects such as the
illusion of passing time can be created from a dynamic sequences of diﬀerent
images.
While static objects are suﬃcient in a number of cases, it is often desirable
for some degree of animation. In its simplest form objects can be made to move,
either independently or resulting from user interaction. More advanced transformations could include scaling changes or morphing between objects.
2.2

Scene Conﬁguration

The scope for potential VE development areas is vast. Its successful implementation would open huge avenues in many diﬀerent ﬁelds. For instance it would
become the perfect blank canvas for psychologists, artists and architects to name
but a few. Using it they could create their environments tailored to their own
speciﬁcations. However, these people generally do not possess the necessary VR
programming skills to directly access the scene creation libraries. Simpliﬁed VR
scripting languages (e.g VRML [12]) do exist, although being originally developed for creating/browsing 3D content on the Internet, have limited functionality in an immersive environment and lack widespread support for the various
VR platforms. In an attempt to address the ease of creation issue, the Scene
Conﬁgurator tool has been developed.
The Scene Conﬁgurator is a utility that creates a VE following some simple
commands from a human readable conﬁguration ﬁle. These commands deﬁne
the objects that exist within the scene (geometries, models and any textural
information), along with their location, animation and interaction characteristics. Using this approach non-programmers can create and edit the conﬁguration
ﬁles that manage their scenes. Currently proprietary plain text is used for the
simpliﬁcation of the commands, however it is anticipated that an additional text
formating such as XML may be considered.
Having a common format for scene conﬁguration enables further development
of scene generation tools, enabling applications to store the state of their VE
for backup or scene editing purposes. It also allows for automation in generating

Creation and Control of Interactive Virtual Environments

599

the scenes from from external data sources such as architectural design plans,
shop-ﬂoor layouts and so on.
2.3

Scene control

An optional scene controller module has been added to the library of tools to
free the application developer from some of the maintenance tasks within an
environment. When used it creates and manages a registered hierarchy of virtual
objects within the scene and can be used to provide animation and interaction.
In addition to their geometries the objects have speciﬁc personalities eﬀecting
their behavior and enabling them to interact. Predeﬁned personalities exist for
these objects such as those that are manipulable (can attach to and be moved
by a user or another object), mobile (they have their own rules determining
how they move locally or around the scene) or are container objects (which
can be used to hold/carry other objects). This base level of scene control can be
provided to objects deﬁned through the scene conﬁgurator. Additionally, further
objects with extended characteristics and behavior can be derived from these
base personalities.
Due to the underlying nature of the OpenSG scenegraph [13] that this work
is built on, the object hierarchy has good support over distributed or multiprocessor architectures. This coupled with state storage in persistent or shared
memory allows for more complex objects, or those that have additional processing requirements to exist without hampering rendering performance in the scene.
An simple example of this could involve the dynamic motion and interaction of
objects exposed to multiple forces. The scene controller may only need positional
information from the objects, and this could be retrieved with minimal overhead.
However additional threads (and processors) could be constantly calculating the
physics behind the objects’ motion to determine the resultant positions.

3

User Interactions Within the Environment

An extension of a single user interactive environment would involve multiple
users, using aspects of the environment as a communication medium. Current
applications sometimes use avatars [14, 15] as virtual world representations of
remote users as they share a VE. Here tracking devices follow user actions and
map these onto their avatar counterpart. Using this method remote users can be
seen to gesture or otherwise interact with the virtual world. However the expense
of current tracking technology tends to result in a maximum of 2-3 tracked points
for each person, thereby limiting its eﬀectiveness.
A more acceptable method for interaction, and one that would help drive the
uptake of VEs would be one that uses readily available technologies alongside
the virtual world. Currently, most media formats revert to a 2D image such as
those seen in books, televisions or displayed on PCs. While in the future it would
be desirable for all users to be able to share and interact with 3D objects and
models, right now these existing 2D formats can be incorporated into the 3D

600

A. Haﬀegee et al.

world. An example of this could be a virtual television or screen upon which
could be displayed a traditional 2D image.
To facilitate this a particular type of virtual object has been developed with
changeable textures each of which can represent a single frame/page of 2D data.
This texture is read as needed from a shared memory segment attached to the
object. The shared memory is written to by an external process based around
the Multicast Application Sharing Tool (MAST) [16], which receives and decodes
streams of application display data generated during group collaborations. More
recently this tool has also been used for streaming face video. The resulting
virtual object is therefore able to display a ﬂat plane within the environment
upon which can displayed live streams of video or application data. Since MAST
supports multiple channels such an object could easily be used to represent a
virtual monitor that could switch its display from one of many sources. Several
of these could then be combined in the VE to form the basis of a multiple user
multimedia conferencing system.

4

Applications

Several applications have already been developed from the described technologies, and demonstrate just some of the possible uses of these utilities. Figure 1
shows three of these applications running in a CAVE like environment.
The ﬁrst of these is a virtual supermarket which allow users to pick up and
examine diﬀerent products. The whole scene has been developed with the scene
conﬁgurator, so could easily have been created by a non-programmer. The store,
the shelves and all the items on them as well as how they can be manipulated
have been deﬁned within the conﬁguration ﬁle. The products and signs are simple geometries upon which textures created from photographs of the real objects
have been added. Similar environments could just as easily be created to demonstrate alternative scenes, maybe museums containing artifacts that users could
examine, or architectural designs to prototype new houses.
The second diagram shows a potential use of multimedia conferencing, in
this case a virtual art class. MAST has been used to distribute application
and face video from number of users. The teacher within the VE can view the
students progress as they develop their drawing regardless of the application
that each of them is using. The video and audio streams are provided to enhance
communication between participants. This type of interaction could be used in
many types of virtual meetings where audio, video and application data may
wish to be shared amongst distributed users.
The ﬁnal application shows a virtual environment being used as a security centred virtual control room. Multiple video streams from security cameras would
be fed into the VE, and remote sensors could also be used to trigger alarm notiﬁcation. The user from within the VE could be made aware of any intrusion,
and enhance the respective video stream for further examination. Should it be
necessary the user could notify the authorities while also having access to virtual models of the real environment to assist with any remedial action. Since it is

Creation and Control of Interactive Virtual Environments

601

unlikely that a user will be permanently required to monitor the same location,
a virtual control centre could be established where a user could monitor many
sites, each with their own virtual representation. If any of these sites require
servicing then that VE could be brought up for the user’s attention.

5

Conclusion

This work describes extensions to the VE development process that ﬁrst open up
the ﬁeld to non-VR experts, and second provide a control entity that help in the
management of dynamic virtual worlds. It discusses how virtual objects can be
derived alongside these extensions to populate the environment with interactive
items, and how these items can be used for multiuser collaboration and interaction. Finally it demonstrates applications that make use of this technology and
suggests how they could be further enhanced.
Future work should extend the range of behaviors of the virtual objects and
provide support for these within the scene conﬁgurator. Additional scene generation tools could provide automated or assisted development, potentially allowing
extensive scene editing from within the VE. Improved collaborative interaction
could also be developed through closer integration with MAST, potentially allowing bidirectional messaging for remote application communication and control.

References
1. Macedonia, M.R., Zyda, M.J.: A taxonomy for networked virtual environments.
IEEE MultiMedia 4 (1997) 48–56
2. Rohlf, J., Helman, J.: IRIS performer: A high performance multiprocessing toolkit
for real-time 3d graphics. In: SIGGRAPH, ACM Press (1994) 381–394
3. Bierbaum, A.D.: VR Juggler: A virtual platform for virtual reality application
development. Master’s thesis, Iowa State University, Ames, Iowa (2000)
4. Radenkovic, M., Greenhalgh, C., Benford, S.: A scaleable audio service for CVEs.
In: Proc of the sixth conference of the UK VRSIG. (1999)
5. Neumann, T., F¨
unfzig, C., Fellner, D.W.: TRIPS - a scalable spatial sound library
for OpenSG. Technical Report TUBSCG-2003-02, Institute of ComputerGraphics
(2003)
6. Robust Audio Tool (RAT) website. Available on: http://www-mice.cs.ucl.ac.
uk/multimedia/software/rat
7. Teamspeak website. Available on: http://www.goteamspeak.com
8. Gonzalez, D.: AGJuggler: An architecture for virtual reality within a collaboration
environment. Master’s thesis, Purdue University (2005)
9. Childers, L., Disz, T., Olson, R., Papka, M., Stevens, R., Udeshi, T.: Access grid:
Immersive group-to-group collaborative visualization. In: 4th International Immersive Projection Technology Workshop. (2000)
10. Haﬀegee, A., Jamieson, R., Anthes, C., Alexandrov, V.: Tools for collaborative VR
application development. In: International Conference on Computational Science,
Springer Verlag (2005) 350–358
11. Cruz-Neira, C., Sandin, D.J., Defanti, T.A., Kenyon, R.V., Hart, J.C.: The CAVE:
Audio visual experience automatic virtual environment. Communications of the
ACM 35 (1992) 64–72

602

A. Haﬀegee et al.

12. Carey, R., Bell, G.: The VRML 2.0 annotated reference manual. Addison-Wesley,
Reading, MA, USA (1997)
13. Reiners, D.: OpenSG: A Scene Graph System for Flexible and Eﬃcient Realtime
Rendering for Virtual and Augmented Reality Applications. PhD thesis, Technische Universit¨
at Darmstadt (2002)
14. Badler, N.I., Phillips, C.B., Webber, B.L.: Simulating Humans: Computer Graphics
Animation and Control. Oxford University Press, New York, NY, USA (1992)
15. Park, K., Cho, Y., Krishnaprasad, N., Scharver, C., Lewis, M., Leigh, J., Johnson,
A.: CAVERNsoft G2: A toolkit for high performance tele-immersive collaboration.
In: VRST, Seoul, Korea, ACM Press (2000) 8–15
16. Lewis, G.J., Hassan, S.M., Alexandrov, V.N., Dove, M.T., Calleja, M.: Multicast
application sharing tool - facilitating the eminerals virtual organization. In: International Conference on Computational Science, Springer Verlag (2005) 359–366

