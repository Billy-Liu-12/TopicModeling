Space and Time Eﬃcient Algorithms for Planted
Motif Search
Jaime Davila, Sudha Balla, and Sanguthevar Rajasekaran
CSE Department at University of Connecticut, Storrs
{jdavila, ballasudha, rajasek}@engr.uconn.edu

Abstract. We consider the (l, d) Planted Motif Search Problem, a problem that arises from the need to ﬁnd transcription factor-binding sites
in genomic information. We propose the algorithms PMSi and PMSP
which are based on ideas considered in PMS1 [10]. These algorithms are
exact, make use of less space than the known exact algorithms such as
PMS and are able to tackle instances with large values of d. In particular
algorithm PMSP is able to solve the challenge instance (17, 6), which has
not reported solved before in the literature.

1

Introduction

The Planted Motif Search Problem arises from the need to ﬁnd transcription
factor-binding sites in genomic information and has been studied extensively in
the biocomputing literature –see [11] for a literature survey.
The problem can be deﬁned in the following formal way.
Deﬁnition 1. Given a string s with |s| = m and a string x with |x| = l with
l < m. We say x l s if x is a subsequence of s. Equivalently we say that x is an
l-mer of s.
Deﬁnition 2. Given a set of strings {si }ni=1 over an alphabet Σ , with |si | = m
and l, d with 0 ≤ d < l < m we deﬁne the (l, d) motif search problem as that of
ﬁnding a string x with |x| = l such that si has an l-mer xi with
dH (x, xi ) = d for i = 1, . . . , n.
We will call x a motif.
This problem is known to be NP-complete [6] and a PTAS exists for variants of
the problem known as the Common Approximate Substring and Common Approximate String [7], [1]. However the high degree in the polynomial complexity
of the PTAS makes it of little practical use.
Numerous algorithms have been implemented in order to solve instances of
this problem. Among them we have Random Projection [2], MITRA [4], Winnower [8], Pattern Branching [9], Hybrid Sample and Pattern Driven Approaches
[12], PMS1 [10], CENSUS [5] and Voting [3].
This research was supported in part by the NSF Grant ITR-0326155.
V.N. Alexandrov et al. (Eds.): ICCS 2006, Part II, LNCS 3992, pp. 822–829, 2006.
c Springer-Verlag Berlin Heidelberg 2006

Space and Time Eﬃcient Algorithms for Planted Motif Search

823

Out of these algorithms, CENSUS, MITRA, PMS1 and Voting are in the
exact category. The last two algorithms are able to work in reasonable time for
practical instances where m = 600 and n = 20. In [2] the notion of a challenging
instance was deﬁned, as one where if the strings are selected at random, the
expected number of (l, d) motifs is greater than 1. We have that (9, 2), (11, 3),
(13, 4), (15, 5) and (17, 6) are challenging instances.
PMS1 is able to solve (9, 2),(11, 3) and (13, 4) in less than a couple of minutes,
and Voting is able to solve these instances and (15, 5) –the last one in 22 min–.
However as d becomes larger the higher memory requirements get.
In this paper we propose the algorithms PMSi and PMSP that build upon
ideas similar to the ones in PMS1, but are able to achieve better time results and
less use of memory for practical as well as challenging instances. In particular
we notice that PMSP is able to handle challenging instances like (l, d) = (15, 5)
and (17, 6). No other exact algorithm has been reported to work on (17, 6).
In section 2 we describe the algorithm PMS1 and our improvements PMSi
and PMSP and prove some complexity bounds on them.
In section 3 we describe some of the experimental results we have obtained,
and compare the results with the already existing exact algorithms.

2

Improved Algorithms Based on PMS1

PMS1 is a simple exact algorithm introduced in [10] which works as follows.
It considers each l-mer in each input sequence and for each such l-mer q it
constructs a list of neighbors (i.e., l-mers) that are at a distance of d from q.
Neighbor lists of the input sequences are then intersected using radix sort to
identify the planted motif. This algorithm works well in practice for values of
d ≤ 3 however as d increases the memory requirement tends to be large.
We propose two algorithms PMSi and PMSP that improve on the memory
requirements of PMS1 at the cost of possibly more computation time. However
in practical instances we show how they perform better than PMS1 and are able
to tackle instances which could not be solved by PMS1.
2.1

PMS1

Before we describe PMS1 [10] we consider the following deﬁnitions.
Deﬁnition 3. For any string x, with |x| = l:
1. Let B(x, d) := {y : |y| = l and dH (y, x) = d}.
2. Let N (l, d) := |B(x, d)|.
Deﬁnition 4. Given s, with |s| = m and 0 ≤ d < l ≤ m let Ls :=

B(x, d).
x

PMS1 works by doing the following simple steps.
1. Build Li for i = 1, . . . , n.

n

2. The set of (l, d) motifs will be M :=

Li .
i=1

ls

824

J. Davila, S. Balla, and S. Rajasekaran

In order to do the intersections of step 2 in an eﬃcient way, we keep the sets
Li sorted in lexicographical order.
Theorem 1. PMS1 can be implemented in O(mn wl N (l, d)) time and
O(m wl N (l, d)) space, where w is the word length of the computer.
2.2

PMSi

PMSi works by generating Ls2i−1 ∩ Ls2i for i = 1, . . . , n2 and then taking the
intersection of these lists. The advantage is that Ls2i−1 ∩ Ls2i ⊂ Ls2i , so it uses
less memory than PMS1. In practice, the size of these sets can be substantially
smaller than the size of Ls2i .
In a more formal way we have the following.
Deﬁnition 5. Let 1 ≤ i ≤

¯ i := Ls2i−1 ∩ Ls2i .
we deﬁne L

n
2

Algorithm. PMSi
¯ 1.
1. Let M := L
2. Sort M by lexicographical order using radix sort.
3. For i = 2, . . . , n2
¯ i.
(a) Construct L
¯
(b) Sort Li by lexicographical order using radix sort.
¯ i by merging L
¯ i with M and considering the l-mers
(c) Construct M ∩ L
which appear twice.
4. Output M .
¯ i we need to introduce the following deﬁnitions and
In order to generate L
results.
Deﬁnition 6. Given x, with |x| = l, s with |s| = m and 0 ≤ h ≤ l, we deﬁne
Li,d (x) := {y l si : dH (x, y) ≤ d}.
B(x, d) ∩ B(y, d) .

¯i =
Lemma 1. L
x

l s2i−1

y

l s2i

Proof. It follows by using De Morgan’s theorem.
B(x, d) ∩ B(y, d) .

¯i =
Lemma 2. L
x

l s2i−1

y∈L2i,2d (x)

Proof. It follows from fact that if dH (x, y) > 2d then B(x, d) ∩ B(y, d) = ∅ and
Lemma 1.
Taking into account Lemma 2 we deﬁne the procedure GenInter(i) which
¯ i.
outputs L

Space and Time Eﬃcient Algorithms for Planted Motif Search

825

Procedure. GenInter(i)
1. Let M = ∅.
2. For each l-mer x of s2i−1 :
(a) Let N := L2i,2d (x).
(b) For each l-mer y ∈ B(x, d)
i. For each x ∈ N , if dH (y, x ) = d add y to M .
3. Output M .
It is useful when generating L2i,2d (x), to order the elements of it in ascending
order of distance towards x, in this way we calculate the distance against the lmers which are closer, which have greater probability of being in the intersection.
It should also be noted that we represent every l-mers as a sequence of integers,
as in [10]. In step 2(b)i the distance is calculated by using an array in which the
distances of the sequences represented by one integers are cached.
Taking these into account, the following two results are straightforward.
Theorem 2. PMSi can be implemented in O(nm2 + S wl N (l, d)) time and in
¯ i |) space, where S =
O( max n |L
i=1,..., 2

n
2

|L2i,2d (x)|.

i=1 x

l s2i−1

Corollary 1. PMSi can be implemented in O(nm2 wl N (l, d)) time using
O(m wl N (l, d)) space.
2.3

PMSP

PMSP follows the following simple idea. For every l-mer x in s1 it generates the
set of neighbors of x and tries to guess if an l-mer y in that neighborhood is
a motif by checking whether there are l-mers in si for i = 2, . . . , n that are at
distance d from it. This algorithm has been given in [10]. But we modify this
algorithm in a critical way. The key observation is to notice that we do not need
to evaluate the distance in all l-mers of si , but rather in the l-mers of si which
are at distance ≤ 2d from x.
This can be said in a formal way in the straighforward lemmas 3 and 4.
Lemma 3. The set of motifs M can be written as
n

B(x, d) ∩

M=
x s1

Proof. Follows by deﬁnition of Li and M .

Li .
i=2

826

J. Davila, S. Balla, and S. Rajasekaran

Lemma 4. Let x

l

s1 and x ∈ B(x, d). We have that x ∈

n

Li iﬀ
i=2

∀i = 2, . . . , n : ∃yi ∈ N (x, si , 2d) : dH (x , yi ) = d.
Proof. Follows by lemma 3 and the use of lemma 2 iteratively.
Taking these considerations into account, we can write PMSP in the following
way.
Algorithm. PMSP
1. Let M = ∅.
2. For each x l s1 :
(a) Construct N (i) = Li,2d (x) for i = 2, . . . , n.
(b) For each x ∈ B(x, d):
i. Check if for every i (2 ≤ i ≤ n) there exists a yi ∈ N (i) such that
dH (x , yi ) = d.
ii. In the aﬃrmative add x to M .
3. Output M .
In PMSP we are using O(m2 n) space due to the fact that we need to calculate
the distance from all l-mers in s1 to all l-mers in si for i = 2, . . . , n. This can
be done in O(m2 n) time and space by using the strategy described in [8]. Hence
the following results hold.
Theorem 3. PMSP can be implemented in O(nm2 +
O(nm2 ) space, where S =

n
2

l
w N (l, d)S)

time and in

|L2i,2d (x)|.

i=1 x l s2i−1

Corollary 2. PMSP can be implemented in O(nm2 wl N (l, d)) time and in
O(nm2 ) space.
2.4

Complexity Bounds in the Expected Case

According to corollaries 1 and 2, we have that the newly designed algorithms
are slower than PMS1 by a factor of m in the worst case. In this section we will
argue than in the expected case the behavior is better.
If we assume that the set of strings si is constructed by picking each character
from every sequence with equal probability we can prove sharper estimates in the
expected case. In doing so we follow the approach used in [2], which basically
assumes that the probabilty of occurence of two diﬀerent l-mers at diﬀerent
positions is independent from each other –which is not the case when the l-mers
are in close proximity–. Furthermore we assume Σ = {A, C, G, T }.

Space and Time Eﬃcient Algorithms for Planted Motif Search

827

The following results can be found in [2].
Lemma 5. For a ﬁxed 0 ≤ d ≤ l we can estimate the probability of two random
l-mers being at a Hamming distance of ≤ d as
d

pd :=
i=0

l
i

3
4

i

1
4

l−i

.

Lemma 6. The expected number of (l, d) motifs for a set of strings si with
i = 1, . . . , n, |si | = m and such that si is picked at random using the previous
model can be estimated by
E(l, d, n) = 4l 1 − (1 − pd )m−l+1

n

.

By applying the previous lemma we can estimate in a better way the time and
space complexity in the expected case.
Lemma 7. The expected value of S can be estimated as p2d m2 n2 .
Proof. It follows from the deﬁnition of Li,d (x) and S, linearity of expectation
and Lemma 5.
¯ i | for i = 1, . . . , n can be estimated as
Lemma 8. The expected value of |L
2
l
m−l+1 2
.
E(l, d, 2) = 4 1 − (1 − pd )
Proof. It follows from Lemma 6.
Theorem 4. The expected time complexity of PMSi is O(p2d wl nm2 N (l, d)) and
the expected space complexity is O( wl E(l, d, 2)). The expected time complexity of
PMSP is O(p2d wl nm2 N (l, d)).
One consequence of Theorem 4 is that the proposed algorithms will be better if
1
. Also notice that they imply that if we ﬁx d, in the expected case we
p2d < m
are going to get better results in PMSi and PMSP if we increase the value of l.
Notice that it also implies the worst time and space complexities are attained
for values of l and d such that E(l, d, n) is greater than 1.

3

Experimental Results

As described before we follow the experimental setting described in [8] and [2].
That is, we ﬁx n = 20, m = 600 and consider diﬀerent values of l and d. In this
model the strings are generated uniformly at random, and an l-mer is planted
in random positions of these strings, mutating it in exactly d places.
We implemented versions of PMS1, PMSi and PMSP in C, and run our programs on a Pentium4 2.40 GHz machine with a core memory size of 1GB.
Our version of PMS1 is coded in C as well –the original version of PMS1 is
implemented in Java–. Our version of PMS1 is slower than the original one but
it is a good framework to test our results.

828

J. Davila, S. Balla, and S. Rajasekaran
Table 1. Comparison of PMS1 and PMSi for d = 3
Algorithm
PMSP
PMSi
PMS1

l = 13
l = 14
l = 15
Time (sec) Mem (%) Time(sec) Mem (%) Time(sec) Mem (%)
3
2.8%
2
2.8 %
1.5
2.8%
28
1.2%
15
<1%
10
< 1%
70
20.0%
79
21.0%
94
22.0%

Table 2. Comparison of PMS2, PMSi, PMSP for d = 4
Algorithm
PMSP
PMSi
PMS2

l = 13
l = 14
l = 15
Time (min) Mem (%) Time(min) Mem (%) Time(min) Mem (%)
2:12
2.8%
1:36
2.8%
1:34
2.8%
28:30
35%
17:54
20%
10:23
12%
3:48
95%
3:46
95%
3:37
95%

Table 3. Time Comparison In Challenging Problems Instances
Algorithm
PMSP
Voting
PMSi
PMS1

(9,2) (11,3) (13,4) (15,5) (17,6)
0.6s 6.9s 152s 35m 12h
0.4s 8.6s 108s 22m
–
4.6s 111.0s 18m
–
–
3.0s 44.9s
–
–
–

Table 1 shows the running time in seconds as well as the memory usage in
percentage for our implementations of PMS1, PMSi and PMSP for d = 3 and
l = 13, 14, 15. When we compare PMSi with PMS1 we get a speedup between 5
and 8 times and between 10 and 20 times less memory. In the case of PMSP we
get a speedup between 20 and 50 times and the amount of memory used is very
low. Let us also notice that the speedup in time as well as in memory use gets
higher for higher values of l which is consistent with Theorem 4.
Table 2 shows the running time and memory usage of PMSi and PMSP for
instances that PMS1 could not solve because of large memory requirements. We
have to notice that already existing extensions of PMS1 such as PMS2 are able
to handle the case of d = 4 as reported in [10]. In this table we see that PMSi
takes more time than PMS2 and for bigger values of l it gets faster and uses less
memory. PMSP outperforms the two previous algorithms, using signiﬁcantly less
memory and the time it expends decreases as the value of l increases.
In Table 3 we get to see the behavior of PMSP, PMSi, PMS1 and Voting
Algorithm in diﬀerent challenging instances. A ’–’ in the table means that the
algorithm uses too much memory in that instance or its time is not reported.
PMSi takes 3 times as much time as PMS1 in some cases, but it uses less memory,
which allows it to solve the (13, 4) instance which cannot be solved by PMS1.
PMSP clearly outperforms PMSi and PMS1 and uses less memory which allows

Space and Time Eﬃcient Algorithms for Planted Motif Search

829

it to solve bigger challenging instances. PMSP behaves well when compared with
Voting, sometimes performing better and sometimes worse by no more than twice
in time. However the main advantage of PMSP is its low use of memory which
allows it to solve the (17, 6) instance, which was not reported solved before.

4

Conclusions

We presented algorithms PMSi and PMSP that are based upon ideas used in
PMS1. These algorithms are more space eﬃcient than PMS1 and improve the
running time of PMS1 in several cases. PMSP clearly outperforms PMS1 and
PMSi in challenging instances and compares well with already existing exact
algorithms while making use of signiﬁcantly less memory. This property allows
it to solve the challenging instance (17, 6) which was not reported solved before.

References
1. Balla, S., Davila, J., Rajasekaran, S.: Approximation Algorithms for the Primer
Selection, Planted Motif Search and Related Problems. In: Approximation Algorithms and Metaheuristics. (To Appear)
2. Buhler, J., Tompa, M.: Finding motifs using random projections. Journal of
Computational Biology 9(2) (2002) 225–242
3. Chin, F.Y., Leung, H.C.: Voting algorithms for discovering long motifs. In: APBC.
(2005)
4. Eskin, E., Pevzner, P.A.: Finding composite regulatory patterns in dna sequences.
In: ISMB. (2002) 354–363
5. Evans, P., Smith, A.: Toward optimal motif enumeration. In: WADS. (2003)
6. Evans, P., Smith, A., Wareham, H.T.: On the complexity of ﬁnding common
approximate substrings. TCS: Theoretical Computer Science 306 (2003)
7. Li, M., Ma, B., Wang, L.: On the closest string and substring problems. Journal
of the ACM 49(2) (2002) 157–171
8. Pevzner, P.A., Sze, S.H.: Combinatorial approaches to ﬁnding subtle signals in
DNA sequences. In: ISMB. (2000) 269–278
9. Price, A.L., Ramabhadran, S., Pevzner, P.A.: Finding subtle motifs by branching
from sample strings. In: ECCB. (2003) 149–155
10. Rajasekaran, S.: Exact algorithms for planted motif problems. Journal of Computational Biology 12(8) (2005) 1117–1128
11. Rajasekaran, S.: Motif Search Algorithms. In: Handbook of Computational Molecular Biology. CRC Press (2005)
12. Sze, S.H., Lu, S., Chen, J.: Integrating sample-driven and pattern-driven approaches in motif ﬁnding. In: WABI. (2004)

