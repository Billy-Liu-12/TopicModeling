A New Multi-criteria Convex Quadratic Programming
Model for Credit Analysis*
Gang Kou1, Yi Peng1, **, Yong Shi1, 2, and Zhengxin Chen1
1
College of Information Science & Technology,
University of Nebraska at Omaha, Omaha, NE 68182, USA
2
Chinese Academy of Sciences Research Center on Data Technology
& Knowledge Economy, Graduate University of the Chinese Academy of Sciences,
Beijing 100080, China
{gkou, ypeng, yshi, zchen}@mail.unomaha.edu

Abstract. Mathematical programming based methods have been applied to
credit risk analysis and have proven to be powerful tools. One challenging issue
in mathematical programming is the computation complexity in finding optimal
solutions. To overcome this difficulty, this paper proposes a Multi-criteria
Convex Quadratic Programming model (MCCQP). Instead of looking for the
global optimal solution, the proposed model only needs to solve a set of linear
equations. We test the model using three credit risk analysis datasets and
compare MCCQP results with four well-known classification methods: LDA,
Decision Tree, SVMLight, and LibSVM. The experimental results indicate that
the proposed MCCQP model achieves as good as or even better classification
accuracies than other methods.
Keywords: Credit Analysis, Classification, Mathematical Programming, Multicriteria Decision Making.

1 Introduction
This paper explores solving classification problem, one of the major sub-fields of data
mining, through the use of mathematical programming based methods (Bradley et al
1999, Vapnik 1964 and 2000). Such methods have proven to be powerful in solving a
variety of machine learning problems (Chang and Lin 2001, Fung 2003, Joachims
1999, Kou et al 2005, Mangasarian 1999, 2000 and 2005, Mitchell 1997, Zheng et al
2004). However, it is difficult to find the optimal solution of a mathematical
programming problem. To overcome this difficulty, a new Multi-criteria Convex
Quadratic Programming model (MCCQP) is proposed. In the proposed model, we
only need to solve a set of linear equations in order to find the global optimal solution.
This paper is organized as follows: section 2 presents the MCCQP model, section 3
illustrates the numerical implementation and comparison study with several well*

This work was supported in part by Key Project #70531040, #70472074, National Natural
Science Foundation of China; 973 Project #2004CB720103, Ministry of Science and
Technology, China and BHP Billion Co., Australia.
**
Corresponding author.
V.N. Alexandrov et al. (Eds.): ICCS 2006, Part IV, LNCS 3994, pp. 476 – 484, 2006.
© Springer-Verlag Berlin Heidelberg 2006

A New Multi-criteria Convex Quadratic Programming Model

477

established data mining software and reports the results, and section 4 summarizes the
paper and discusses future research directions.

2 Multi-criteria Convex Quadratic Programming Model
This section introduces a new MCQP model. This model classifies observations into
distinct groups via a hyperplane and based on multiple criteria. The following models
represent this concept mathematically (Kou et al 2006):
Each row of a n × r matrix A= (A1,…,An)T is an r-dimensional attribute vector

= ( Ai1 ,..., Air ) ∈ℜr which corresponds to one of the records in the training
dataset of a binary classification problem, i = 1,..., n ; n is the total number of records
in the dataset. Two groups, G1 and G 2 , are predefined while G1 ∩ G2 = Φ and Ai
∈ {G1 ∪ G2 } . A boundary scalar b can be selected to separate G1 and G2 . Let X =
Ai

( x1 ,..., xr )T ∈ℜr be a vector of real number to be determined. Thus, we can
establish the following linear inequations (Fisher 1936, Shi et al. 2001):
Ai X < b,

∀ Ai ∈

G1 ;

(1)

Ai X ≥ b,

∀ Ai ∈

G2 ;

(2)

In the classification problem, Ai X is the score for the ith data record. If all
records are linear separable and an element Ai is correctly classified, then let
the distance from A

i

βi

to b, and obviously in linear system, AiX = b -

βi

be

, ∀ Ai ∈

G1 and AiX = b + β i , ∀ Ai ∈ G2 . However, if we consider the case where the
two groups are not completely linear separable, there exist some misclassified
records. When an element Ai is misclassified, let
AiX = b +

αi

definitions of

βi

, ∀ Ai ∈
and

αi

αi

be the distance from Ai to b,

G1 and AiX = b - α i , ∀ Ai ∈ G2 . To complete the
, let

βi

=0 for all misclassified elements and

αi

zero for all correctly classified elements. Incorporating the definitions of

αi

equals to

βi

and

, (1) and (2) can be reformulated as the following model:
AiX = b -

is a given scalar. b -

δ

δ

+

αi

-

βi

, ∀ Ai ∈

G1

αi

+

βi

and b +

δ

are two adjusted hyper planes for the

as

α i / δ , βi

AiX = b +

δ

δ

-

, ∀ Ai ∈

G2

(3)

model.
Redefine X as X / δ , b as b / δ ,

αi

as

βi / δ ,

and Define a

n × n diagonal matrix Y which only contains “+1” or “-1” indicates the class

478

G. Kou et al.

membership. A “-1” in row i of matrix Y indicates the corresponding record Ai
∈ G1 and a “+1” in row i of matrix Y indicates the corresponding record Ai

∈ G2 . The model can be rewritten as:
Y ( <A⋅X> - eb )= 1 +
where e=(1,1,…,1)T,

α = (α 1 , K , α n ) T

α

-

β,

(4)

β = ( β1 , K , β n ) T .

and

The proposed multi-criteria optimization problem contains three objective
functions. The first mathematical function
norm of

β i ,1 ≤ q ≤ ∞ )

f (α ) = α

β i ,1 ≤ q ≤ ∞ )

p

n

= ∑ (α i ) p
i =1

(

lp —

describes the summation of total overlapping distance of

misclassified records to b. The second function
norm of

p

g (β ) = β

n

= ∑ (βi )q ( l q —

q
q

i =1

represents the aggregation of total distance of correctly

separated records to b. In order to maximize the distance (

2
) between the two
|| X || ss

|| X || ss
should be
adjusted bounding hyper planes, the third function h( X ) =
2
minimized. Furthermore, to transform the generalized Multi-Criteria classification
model into a single- criterion problem, weights Wα > 0 and Wβ > 0 are introduced for
f (α ) and g ( β ) , respectively. A single-criterion mathematical programming model
can be set up:

1
|| X ||ss +Wα α
2

（ Model 1）

Minimize

Subject to:

Y (<A⋅X> - eb) = e -

α

+

p
p

− Wβ β

q
q

β

α i , β i ≥ 0 ,1≤ i ≤n
where Y is a given

n × n diagonal matrix, e=(1,1,…,1)T, α = (α 1 , K , α n ) T ,

β = ( β1 ,K , β n ) T , X and b are unrestricted.
Please note that the introduction of βi is one of the major differences between the
proposed model and other existing Support Vectors approaches (Vapnik 1964 and
2000). It is much easier to find optimal solutions for convex quadratic programming
form than other forms of nonlinear programming. To make Model 1 a convex
quadratic programming form, let s = 2 , q = 1 and p = 2. The constraints remain
the same and the objective function becomes:

A New Multi-criteria Convex Quadratic Programming Model

479

n
n
1
|| X || 22 +Wα ∑ α i2 − Wβ ∑ β i
2
i =1
i =1

（ Model 2）

Minimize

Subject to:

Y (<A⋅X> - eb) = e -

α

+

β

Let ηi =αi − βi . According to the definition, ηi = αi for all misclassified records
and ηi = − βi for all correctly separated records. The definition of ηi is one of the
major differences between the proposed model and other existing approaches (Fung
2003, Gonzalez-Castano and Meyer 2000).
Add

Wb 2
b to Model 2’s objective function and the weight Wb is an arbitrary
2

positive number.

（ Model 3）

Minimize

W
1
|| X || 22 + α
2
2

n

n

i =1

i =1

∑η i2 + Wβ ∑η i +

Subject to: Y (<A⋅X> - eb) = e - η
where Y is a given

Wb 2
b
2

n × n diagonal matrix, e=(1,1,…,1)T, η = (η1 , K ,η n ) T , η , X

and b are unrestricted,1≤ i ≤n .
The Lagrange function corresponding to Model 5 is
n
W n
W
1
L( X , b,η,θ ) = || X ||22 + α ∑ηi2 + Wβ ∑ηi + b b2 −θ T (Y ( <A ⋅ X> - eb )-e+η)
2
2 i=1
2
i=1

where

θ = (θ1 , K , θ n ) T , η = (η1 , K ,η n ) T , θi ,ηi ∈ℜ.

According

to

Wolfe

Dual

Theorem, ∇ X L( X , b,η,θ ) = X − A Yθ = 0 ,
T

∇b L( X , b,η, θ ) = Wb b + eT Yθ = 0 , ∇η L( X , b,η,θ ) = Wαη +Wβ e −θ = 0 .
Introduce the above 3 equations to the constraints of Model 5, we can get:

1 T
1
e(e Yθ )) + (θ −Wβ e) = e
Wb
Wα
Wβ
(1 + )e
Wα
⇒θ =
I
1
+ Y (( A⋅ AT ) + eeT )Y
Wα
Wb

Y (( A⋅ AT )Yθ +

(1+
Proposition 1. For some Wα > 0 , θ =

Wβ
Wα

(5)

)e

I
1
+ Y (( A⋅ AT ) + eeT )Y
Wα
Wb

exists.

480

G. Kou et al.
1

1
Proof. Let H=Y[A – ( ) 2 e], we get:
Wb
W I
θ = (1+ β )( + HHT )−1 e (5’)
Wα Wα
∀H , ∃Wα > 0 , when Wα is small enough, the inversion of (

(1+
So

θ=

Wβ
Wα

I
+ HHT ) exists.
Wα

)e
exists.

1
I
+ Y (( A⋅ AT ) + eeT )Y
Wα
Wb

Algorithm 1
Input:
a n × r matrix A as the training dataset, a n × n diagonal matrix Y
labels the class of each record.
Output: classification accuracies for each group in the training dataset, score for

> 0, ⇒ Ai ∈G1

every record, decision function sgn((X ⋅ Ai ) − b ){
*

*

≤ 0, ⇒ Ai ∈G2

Step 1 compute

θ = (θ1 , K , θ n )
*

T

by one of (5) or (6). Wβ , Wα and

Wb are

chosen by standard 10-fold cross-validation.
Step 2 compute X = A Yθ , b =
*

Step

3

T

*

classify

a

*

−1 T *
e Yθ .
Wb

incoming

> 0, ⇒ Ai ∈G1

function sgn((X ⋅ Ai ) − b ){
*

*

≤ 0, ⇒ Ai ∈G2

Ai

by

using

decision

.

END

3 Numerical Experiments in Credit Risk Analysis
The model proposed can be used in many fields, such as general bioinformatics,
antibody and antigen, credit fraud detection, network security, text mining, etc. We
conducted three numerical experiments to evaluate the proposed MCCQP model. All
experiments are concerned about credit risk analysis. Each record in these three sets
has a class label to indicate its’ financial status: either Normal or Bad. Bad indicates a
bankrupt credit or firm account and Normal indicates a current status account. The
result of MCCQP is compared with the results of 4 widely accepted classification
methods: Linear Discriminant Analysis (LDA) (SPSS 2004), Decision Tree based
See5 (Quinlan 2003), SVM light (Joachims 1999) and LibSVM (Chang and
Lin 2001).

A New Multi-criteria Convex Quadratic Programming Model

481

The first benchmark set is a German credit card application dataset from UCI
Machine Learning databases (UCI 2005). The German set contains 1000 records (700
Normal and 300 Bad) and 24 variables. The second set is an Australian credit
approval dataset from See5 (Quinlan 2003). The Australian set has 383 negative cases
(Normal) and 307 positive cases (Bad) with 15 attributes. The last set is a Japanese
firm bankruptcy set (Kwak et al 2005). The Japanese set includes Japanese bankrupt
(Bad) sample firms (37) and non-bankrupt (Normal) sample firms (111) between
1989 and 1999. Each record has 13 variables.
Credit Classification Process
Input:

The Credit Card dataset A = { A 1 , A 2 , A 3 , L , A n }, a

n × n diagonal matrix Y

Output: Average classification accuracies for Bad and Normal of the test set in
10-fold cross-validation; scores for all records; decision function.
Step 1 Apply several classification methods: LDA, Decision Tree, SVM, MCCQP,
to A using 10-fold cross-validation. The outputs are a set of decision functions, one
for each classification method.
Step 2 Compute the classification accuracies using the decision functions.
END
The following tables (Table 1, 2, and 3) summarize the averages of 10-fold crossvalidation test-sets accuracies of Linear Discriminant Analysis (LDA) (SPSS 2004),
Decision Tree base See5 (Quinlan 2003), SVM light (Joachims 1999), LibSVM
(Chang and Lin 2001), and MCCQP for each dataset. “Type I Error” is defined as the
percentage of predicted Normal records that are actually Bad records and “Type II
Error” is defined as the percentage of predicted Bad records that are actually Normal
records. Since Type I error indicates the potential charge-off lost of credit issuers, it is
considered more costly than Type II error. In addition, a popular measurement, KS
score, in credit risk analysis is calculated. The higher the KS score, the better the
classification methods. The KS (Kolmogorov-Smirnov) value is defined as:
KS value = Max |Cumulative distribution of Bad - Cumulative distribution of
Normal|.
Table 1 reports the results of the five classification methods for German set.
Among the five methods, LibSVM achieves the best results for all the measurements
and MCCQP achieves the second best results.
Table 1. 10-fold cross-validation result of German set

Method
LDA
See5
SVMlight
LibSVM
MCCQP

Classification Accuracy
Overall
72.20%
72.20%
66.50%
94.00%
73.50%

Normal
72.57%
84.00%
77.00%
100.00%
74.38%

Bad
71.33%
44.67%
42.00%
80.00%
72.00%

Error Rate
Type I
28.32%
39.71%
42.96%
16.67%
27.35%

Type II
27.77%
26.37%
35.38%
0.00%
26.24%

KS-Score
43.90
28.67
19.00
80.00
46.38

482

G. Kou et al.

Table 2 summarizes the results for the Australian set. Among the five methods,
MCCQP achieves the best overall accuracy, Normal accuracy and Type II error while
LDA achieves the lowest Type I error rate and highest Bad classification accuracy
and KS-score.
Table 2. 10-fold cross-validation result of Australian set

Method
LDA
See5
SVMlight
LibSVM
MCCQP

Classification Accuracy
Overall Normal Bad
85.80%
80.68% 92.18%
86.52%
87.99% 84.69%
44.83%
18.03% 90.65%
44.83%
86.89% 27.10%
86.38%
87.00% 85.52%

Error Rate
Type I Type II
8.83%
17.33%
14.82% 12.42%
34.14% 47.48%
45.62% 32.61%
14.27% 13.20%

KS-Score
72.86
72.68
8.69
13.99
72.52

Table 3 summarizes the result for Japanese set. Among the five methods, MCCQP
achieves the highest classification accuracies for overall, Normal, and Bad. In
addition, MCCQP has the highest KS-score and lowest Type I and II error rates.
Although See5 got the highest classification accuracy for Normal class, it’s
classification accuracy for Bad is only 35.14%.
Table 3. 10-fold cross-validation result of Japanese set

Method
LDA
See5
SVMlight
LibSVM
MCCQP

Classification Accuracy
Overall
68.92%
72.30%
48.15%
50.46%
72.30%

Normal
68.47%
84.68%
47.25%
49.45%
72.30%

Bad
70.27%
35.14%
52.94%
55.88%
72.47%

Error Rate
Type I
30.28%
43.37%
49.90%
47.15%
27.58%

Type II
30.97%
30.36%
49.91%
47.49%
27.65%

KS-Score
38.74
19.82
0.19
5.33
44.77

4 Conclusion
In this paper, a new MCCQP model for classification problem has been presented. In
order to validate the model, we apply the model to three credit risk analysis datasets
and compare MCCQP results with four well-known classification methods: LDA,
Decision Tree, SVMLight, and LibSVM. The experimental results indicate that the
proposed MCCQP model achieves as good as or even better classification accuracies
than other methods.
There are still many aspects that need further investigation in this research.
Theoretically, MCQP is highly efficient method in both computation time and space

A New Multi-criteria Convex Quadratic Programming Model

483

on large-scale problems. Since all 4 datasets used are relatively small, it will be a
nature extension to apply MCQP in massive dataset. ( Ai ⋅ A j ) in Model 3 and 4 is
inner product in the vector space and it can be substituted by a kernel K( Ai , A j ) ,
which will extend the applicability of the proposed model to linear
inseparable datasets. Future studies may be done on establishing a theoretical
guideline for selection of kernel that is optimal in achieving a satisfactory credit
analysis result.

References
Bradley, P.S., Fayyad, U.M., Mangasarian, O.L., Mathematical programming for data mining:
Formulations and challenges. INFORMS Journal on Computing, 11, 217-238, 1999.

Chang, Chih-Chung and Lin, Chih-Jen, LIBSVM : a library for support vector
machines, 2001. Software available at http://www.csie.ntu.edu.tw/~cjlin/libsvm
Fung , G. “Machine learning and data mining via mathematical programming-based support
vector machines”, Ph.D thesis, The University of Wisconsin - Madison. 2003
Fung, G. and Mangasarian, O. L. Multicategory Proximal Support Vector Machine Classifiers,
Machine Learning 59, 2005, 77-97.
Gonzalez-Castano, F. and Meyer, R. Projection support vector machines. Technical Report 0005, Computer Sciences Department, The University of Wisconsin – Madison, 2000
Li, J.P, Liu, J.L, Xu, W.X., Shi, Y. Support Vector Machines Approach to Credit Assessment.
In Bubak, M., Albada, et al (Eds.), LNCS 3039, Springer-Verlag, Berlin, 892-899, 2004.
LINDO
Systems
Inc.,
An
overview
of
LINGO
8.0,
http://www.lindo.com/cgi/frameset.cgi?leftlingo.html;lingof.html.

Joachims, T. Making large-Scale SVM Learning Practical. Advances in Kernel
Methods - Support Vector Learning, B. Schöl lkopf and C. Burges and A. Smola
(ed.), MIT-Press, 1999.
Joachims,
T.
(2004)
SVM-light:
Support Vector
Machine,
available
at:
http://svmlight.joachims.org/.
Kou, G., X. Liu, Y. Peng, Y. Shi, M. Wise and W. Xu, “Multiple Criteria Linear Programming
to Data Mining: Models, Algorithm Designs and Software Developments” Optimization
Methods and Software 18 (4): 453-473, Part 2 AUG 2003
Kou, G., Y. Peng, Y. Shi, M. Wise and W. Xu, “Discovering Credit Cardholders’ Behavior by
Multiple Criteria Linear Programming” Annals of Operations Research 135 (1): 261-274,
JAN 2005
MATLAB. User's Guide. The MathWorks, Inc., Natick, MA 01760, 1994-2005.
Mitchell, T. M. Machine Learning. McGraw-Hill, Boston, 1997.
Murphy, P. M. and Aha, D. W. UCI repository of machine learning databases, 1992.
www.ics.uci.edu/_mlearn/MLRepository.html.
Mangasarian, O. L. and Musicant, D. R. Successive overrelaxation for support vector
machines. IEEE Transactions on Neural Networks, 10:1032-1037, 1999.
Mangasarian, O. L. Generalized support vector machines. In A. Smola, P. Bartlett, B.
Scholkopf, and D. Schuurmans, editors, Advances in Large Margin Classifiers, pages 135146, Cambridge, MA, 2000. MIT Press.
Mangasarian, O. L. Support Vector Machine Classification via Parameterless Robust Linear
Programming, Optimization Methods and Software 20, 2005, 115-125.

Quinlan, J. See5.0. (2004) [available at:http://www.rulequest.com/see5-info.html].

484

G. Kou et al.

Shi, Y., Peng, Y., Kou, G. and Chen, Z “Classifying Credit Card Accounts for Business
Intelligence and Decision Making: A Multiple-Criteria Quadratic Programming Approach”
International Journal of Information Technology and Decision Making, Vol. 4, No. 4 (2005)
1-19.
Vapnik, V. N. and Chervonenkis (1964), On one class of perceptrons, Autom. And Remote
Contr. 25(1).
Vapnik, V. N. The Nature of Statistical Learning Theory. Springer, New York, second edition,
2000.
Zheng, J., Zhuang, W., Yan, N., Kou, G., Peng, H., McNally, C., Erichsen, D., Cheloha, A.,
Herek, S., Shi, C. and Shi, Y., “Classification of HIV-1 Mediated Neuronal Dendritic and
Synaptic Damage Using Multiple Criteria Linear Programming” Neuroinformatics 2 (3):
303-326 Fall 2004.

