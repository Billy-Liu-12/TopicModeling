A Hybrid Scheme for Object Allocation in a Distributed
Object-Storage System∗
Fang Wang**, Shunda Zhang, Dan Feng, Hong Jiang, Lingfang Zeng,
and Song Lv
Key Laboratory of Data Storage System, Ministry of Education,
School of Computer, Huazhong University of Science and Technology, Wuhan, China
wangfang@mail.hust.edu.cn, zhangshunda@163.com

Abstract. The object-based storage system, in which files are mapped onto one
or more data objects stored on Object-Based Storage Devices (OSDs), has
distributed storage system architecture. In such a system, the policy for object
allocation is a critical aspect affecting the overall systems performance.
Hashing and fragment-strip are two common techniques used for managing
objects, but both have their disadvantages, and advantages, e.g. hashing
achieves good workload balance and provide rather high effectiveness in
allocating data, but it can not provide readily available parallelism for large file;
fragment-strip takes full advantage device parallelism, simplifies the clients’
operations, but this policy is not fit for small file. In this paper, we present an
efficient algorithm that combines the advantages of these two approaches while
avoiding their shortcomings. The key factors which can impact the performance
of the whole system in the objects allocation are also be discussed.

1 Introduction
Object-based storage systems represent files as sets of objects stored on self-managed
Object-Based Storage Devices (OSDs). By distributing the objects across many
devices, these systems have the potential to provide high throughput, reliability,
availability and scalability [1]. Much research has gone into hierarchy management,
scalability, and availability of distributed file systems in projects such as AFS [3],
Lustre [8], GFS [11], Coda [12] and GPFS [13], but relatively little research has been
aimed at improving the efficiency of objects allocation in large scale object-based
storage systems. The algorithm used for object allocation determines the performance
of the system at the beginning of the communication process. It affects the workload
among the devices, and it also influences the OSD-level parallelism of the objectbased storage system.
The object-based storage model is emerging as architecture for distributed
storage systems. Traditionally, metadata and data are managed by the same file
system, on the same machine, and stored on the same device [3]. For efficiency,
∗

This work was supported by the National Basic Research Program of China (973 Program)
under Grant No. 2004CB318201, the National Science Foundation of China under Grant
No.60303032.
**
Corresponding author.
V.N. Alexandrov et al. (Eds.): ICCS 2006, Part IV, LNCS 3994, pp. 396 – 403, 2006.
© Springer-Verlag Berlin Heidelberg 2006

A Hybrid Scheme for Object Allocation in a Distributed Object-Storage System

397

metadata is often stored physically close to the data it describes [4]. In some
modern distributed file systems, data is stored on devices that can be directly
accessed through the network, while metadata is managed separately by one or
more specialized metadata servers [5].
Currently, most approaches to object allocation employ one of two techniques. The
first one, which we call hashing [2], allocates a file to one device by using hashing
functions that map file IDs to OSD IDs. This approach converts a file to one object
and sends it to only one device. The second object allocation technique, which we call
fragment-strip or fragment-mapping [2], uses equal-sized fragments of each file to
widely distribute the file among the OSDs.
Our object allocation scheme combines the best aspects of hashing and fragmentstrip. In the algorithm, when the file is small, it is converted to a single object and
directly mapped to an OSD by hashing. If the file is large, it is converted to multiple
objects and each object will be distributed to an OSD.

2 Related Works
To improve the scalability of hashing, a self-adaptive hashing scheme is presented in
[6]. To reduce the cost of adaptation and continue to exploit the high effectiveness of
hash functions, the self-adapt hashing policy is designed to improve scalability.
In OBFS (a file system for object-based storage devices) [1], the boundary of small
objects and large objects is set at 512KB. The workload characteristics of a highperformance distributed file system from Lawrence Livermore National Laboratory
(LLNL) [7] were analyzed as an example of large-scale distributed file systems [1].
OBFS provides most of the files are larger than 4 KB and the majority of all files are
distributed between 32 KB and 8MB. Those files that are smaller than 4 KB (a typical
block size for a general-purpose file system) only account for a very small portion of
the total files.
In the Panasas storage cluster [9], if a file is smaller than 64KB, it will be mirrored
on the first two component objects (RAID 1). If the file is larger than 64KB, it will
use additional component objects, up to full stripe worth (RAID 5). That means that
64KB is the boundary distinguishing small files from large files, and that objects are
not larger than 64KB. The Lustre cluster file system [8] logical object volume
management (LOVM) manages the objects as RAID.

3 Algorithm Design
3.1 The Boundary of Small and Large File
According to OBFS [1], this boundary of small and large files should be 512KB. The
OBFS's conclusion was based on the analysis of LLNL [7] workload characteristics.
In the LLNL [7] workload, we estimate that about 85% of all objects will be of size
512 KB and 15% of all objects will be of size smaller than 512 KB. We will refer to
files that are smaller than 512KB as small objects and the rest as large objects. Small
objects and large objects are treated differently by the object allocation algorithm.

398

F. Wang et al.

3.2 The Optimal Number of Objects Mapped from One File
The relationship between the number of OSDs and parallelism is quite complex. More
devices provide more transfer channels and data can be transferred in parallel.
However, increasing connections can also bring down the performance, because
establishing connections takes time, especially when connections are numerous. Thus,
while increasing the number of devices mapping to the same file improves
parallelism, it consumes extra resources of the system.
We can describe the relationship described above with the following formula:
Tp
T

=

n×a +

1
× b + δ (t )
n
a+b

(1)

Tp: The time of transferring file with multiple objects in parallel.
T: The time of transferring file sequentially.
n: Number of objects mapped to a large file.
a: The sum of sender overhead, receiver overhead and the time of flight of
transferring a file.
b: The time for transferring a whole file to a single device.
δ (t ) : Other delays of objects transmission, it is an amendment factor of the
formula.
Numerator of the formula is made up of three terms, which are n*a, b/n and δ (t ) .
The term n*a means connecting to n devices costs n times of the overhead connecting
to a single device. And the b/n indicates that n devices' parallel working can make
bandwidth n times wider than the single-devices situation.
Let's review the performance parameters of interconnection networks.
Depending on whether it is an SAN, LAN or WAN, the relative lengths of the time
of flight and transmission may be quite different from those shown here, based on a
presentation by Greg Papadopoulos of Sun Microsystems. [10]
Total latency = Sender overhead + Time of flight + (Message size / Bandwidth) +
Receiver overhead
(2)
Notice that the time of flight for SANs is so short relative to overhead that it can be
ignored, yet in WANs, time of flight is so long that sender and receiver overheads can
be ignored. Thus, we can simplify the performance equation by combining sender
overhead, receiver overhead, and time of flight into a single term called Overhead:
Total latency § Overhead + (Message size / Bandwidth)

(3)

In our formula:
a = Overhead, b = (Message size / Bandwidth)

(4)

Although the δ (t ) has some relationship with n, their relationship is rather loose.
That is to say, we can simplify δ (t ) to a variable c that is irrelevant to n.
We can simplify our formula by replacing δ (t ) with c:

A Hybrid Scheme for Object Allocation in a Distributed Object-Storage System
Tp
T

=n×

a
b
c
1
+ ×
+
a+b n a+b a+b

399

(5)

The reciprocal of the above formula, T/Tp is the speedup in file transfer time as a
result of parallelism. To maximize the speedup is equivalent to minimizing the above
formula for Tp/T. Since we are interested in how to best map a file into objects,
namely, determining an optimal n, for data transfer, although a, b are not constants,
they are irrelevant to n. We now show that the sum of the two terms containing n has
a minimum when n= b / a , as follows.
dF(n) =0, we have n=
Let F(n) = n× a + 1 × b , and solve for
b / a . Since
dn
a + b n a +b
d2F(n) >0, F(n) has a global minimum at n= b / a . Therefore,
dn2
Tp
T

≥

2 ab
c
+
a+b a+b

(6)

The Overhead is the time for the processor to inject the message into the network,
including both hardware and software components. For pedagogic reasons, we
assume that Overhead is not dependent on message size. (Typically, only very large
messages have larger overhead.) So we can assume that a network with a bandwidth
of 1000 Mbits/second has an Overhead of 80 microseconds [10]. This situation is
very common in today's network, and we use this representative case to estimate the
parameter we need. As the factor c/(a+b) do not impact n, we can ignore it.

Fig. 1. The Relationship Curve of Tp/T and n

Figure 1 describes the relationship Tp/T and Number of Devices n according to
formula (6). Figure 1 shows that for 1MB and 2MB files, when n >= 10 the change in
Tp/T is not distinguishable. And for 4MB, 8MB and 16MB, when n >= 20 the change
in Tp/T is not distinguishable. So do files above 32MB when n >= 40. We can arrive at
a conclusion: n=10 (1MB-2MB), n=20 (4MB-16MB), n=40 (32MB-1GB).

400

F. Wang et al.

3.3 How to Select OSDs for Parallel Transmission
An object-based storage system typically has hundreds of OSDs. How to select OSDs
for parallel transmission of large files from numerous devices? Random choice is a good
idea, which is easy to implement and can always keep workload balanced. However, it
can not ensure that the fastest devices are fully utilized. We can sort the devices by
some parameters such as speed, free-capacity and so on, then select the first n(10, 20 or
40) devices. This algorithm makes sure that the best-conditioned devices are used first.
However, sorting hundreds of OSDs is a time consuming task for the system. Does it
affect the performance to some degree? We will carry out an asymptotic time
complexity analysis of the sorting algorithm to address such questions.
In a bubble sort algorithm, the time complexity of searching the first n items from a
total of N items is:
n

¦ ( N − i) =
i =1

2 N − (n + 1)
×n
2

(7)

The time complexity: T( n ) = O( n2 ) and T( N ) = O(N). So n impact the time
complexity more noticeably than N. Because n is small (10, 20 or 40) and MDS
always has high-capacity memory and high-performance CPUs, sorting algorithm will
not affect the performance much. We can use
simplify our module.

n = (10+20+40)/3 § 20 instead of n to

3.4 Objects Allocation Algorithm Details
The object allocation algorithm proposed here is based on sorting those devices by some
parameters, such as OSD types, busy status, free capacity, partitions in the device and IP
address. A pseudo-code of the algorithm is presented as Figure 2. The basic idea behind
the algorithm is to find those best OSDs according to the size of files.

Fig. 2. Objects Allocation Algorithm

A Hybrid Scheme for Object Allocation in a Distributed Object-Storage System

401

4 Simulation Results
4.1 Experimental Setup
All of the experiments were executed on a PC with a 2.4 GHZ Intel Celeron CPU and
512 MB of RAM, running Red Hat Linux, kernel version 2.4.20. We used Matlab as
the simulator. Matlab first generated an array of random numbers chosen from the
exponential distribution of the file sizes. Then our algorithm (Section 3.4) was applied
to estimate the response time of the system. We implemented the algorithm in
Matlab's M file. The parameter a (Overhead) was assumed to be 80 µs and network
bandwidth was assumed to be 1000 Mb/s (Section 3.2). Devices after being sorted
should reduce the total response time (Section 3.3). According to the number of
OSDs, the simulator considered the following situations: 16 OSDs, 32 OSDs, and
64 OSDs.
4.2 Results
Figures 3 and 4 show the simulation results with 16 OSDs, 32 OSDs and 64 OSDs. In
each virtual system, we measure response times and compare among results from the
hashing scheme, the fragment-strip policy and our algorithm.

Fig. 3. Response time of the 16-OSDs (a), 32-OSDs (b) and 64 OSDs (c) system

Fig. 4. Comparing our algorithm, hashing and fragment-strip in the 16-OSDs (a), 32-OSDs (b)
and 64-OSDs (c) system

402

F. Wang et al.

4.3 Analysis and Discussion of Experimental Results
During evaluation process, we confirmed that the file sizes generated by Matlab
conforms the description of file sizes from LLNL [7] (Section 2) fairly.
Tables 1, 2 and 3 show the mean value, standard deviation, and max and min
value of the three algorithms' response time in different object-based storage
systems. In the 16-OSDs system, the mean value of fragment-strip's response time
is the smallest of the three algorithms. However, when the number of devices
increases, the fragment-strip consumes more and more response time because of the
increasing the Overhead. In the 32-OSDs system, the fragment-strip's mean value of
response time is larger than our algorithm's. In the 64-OSDs system, the fragmentstrip's mean value of response time becomes larger than the other two. The response
time of hashing is between 4 and 4.6 seconds, our algorithm's response time is
between 2 and 2.3 seconds. They have not been changed much in the three different
cases.
Our algorithm is faster than hashing because it makes good use of parallelism. The
standard deviation of fragment-strip's response time decreases while the number of
devices increases. And the standard deviation of hashing and fragment-strip is
relatively steady. Our algorithm's standard deviation is smaller than hashing and
larger than fragment-strip. As a whole, our algorithm is steadier than the others. And
it performs best when the object-based storage system has many OSDs.
Table 1. The statistical data of our algorithm

OSDs
16
32
64

Mean value
(µs)
2.22947e+06
2.23067e+06
2.23583e+06

Standard deviation
(µs)
3.30344e+04
3.51447e+04
3.87005e+04

Max value
(µs)
2.30907e+06
2.31596e+06
2.31128e+06

Min value
(µs)
2.13224e+06
2.15275e+06
2.16086e+06

Table 2. The statistical data of hashing

OSDs
16
32
64

Mean value
(µs)
4.27464e+06
4.25589e+06
4.18744e+06

Standard deviation
(µs)
1.38243e+05
1.18006e+05
1.44812e+05

Max value
(µs)
4.64453e+06
4.51130e+06
4.51206e+06

Min value
(µs)
3.88316e+06
4.01033e+06
3.85776e+06

Table 3. The statistical data of fragment-strip

OSDs
16
32
64

Mean value
(µs)
1.57276e+06
2.75187e+06
5.30830e+06

Standard deviation
(µs)
8.64023e+03
3.68768e+03
2.26269e+03

Max value
(µs)
1.59588e+06
2.75985e+06
5.31338e+06

Min value
(µs)
1.54829e+06
2.74420e+06
5.30315e+06

A Hybrid Scheme for Object Allocation in a Distributed Object-Storage System

403

5 Conclusion and Future Work
In this paper, we present a hybrid algorithm of hashing and fragment-strip, which
combines the best aspects of these two algorithms while avoiding their disadvantages.
Fragment-strip's good scalability is retained reasonably well, while the high efficiency
of hashing makes its presence felt in our algorithm. We combine the two popular
objects-allocation approaches at 512KB, the boundary of small and large file. We
calculate two key factors in our algorithm, namely, the optimal number of objects
mapped from one file and the scope of selecting OSDs for parallel transmission.
Simulation results validate the correctness of the parameters we have calculated. Our
object-allocation algorithm consumes lest time when there are numerous OSDs and its
performance does not change much when the total number of devices increases.
As future work, we plan to finish the Object-Based Storage System. We will also
test the algorithm in the real system instead of the simulation environment created by
Matlab.

References
1. Feng Wang, Scott A. Brandt, and Ethan L. Miller, Darrell D. E. Long.: OBFS: A File
System for Object-based Storage Devices. In 21st IEEE / 12th NASA Goddard Conference
on Mass Storage Systems and Technologies (MSST2004), College Park, MD, April 2004.
2. Lan Xue, Yong Liu.: MDS Functionality Analysis. December 4, 2001.
3. J. H. Morris, M. Satyanarayanan, M. H. Conner, J. H. Howard, D. S. H. Rosenthal, and F.
D. Smith.: Andrew: A distributed personal computing environment. Communications of
the ACM, 29(3):184–201, Mar. 1986.
4. M. K. McKusick,W. N. Joy, S. J. Leffler, and R. S. Fabry.: A fast file system for UNIX.
ACM Transactions on Computer Systems, 2(3):181–197, Aug. 1984.
5. G. A. Gibson and R. V. Meter.: Network attached storage architecture. Communications
of the ACM, 43(11):37–45, 2000.
6. M. Spasojevic and M. Satyanarayanan.: An Empirical Study of a Wide-Area Distributed
File System. ACM Transactions on Computer Systems 14(2) May 1996.
7. D. Roselli, J. Lorch, and T. Anderson.: A comparison of file system workloads. In
Proceedings of the 2000 USENIX Annual Technical Conference, June 2000.
8. Peter J. Braam (with others): The Lustre Storage Architecture. Cluster File Systems, Inc.
http://www.clusterfs.com August 2003.
9. J.R. Moase.: Panasas Storage Cluster & Object Storage Overview. www.panasas.com
October 2004
10. John L. Hennessy, David A. Patterson.: Computer Architecture A Quantitative Approach
(Third Edition).
11. S. R. Soltis, T. M. Ruwart, and M. T. O'Keefe.: The Global File System. In Proceedings of
the 5th NASA Goddard Conference on Mass Storage Systems and Technologies, College
Park, MD, 1996.
12. M. Satyanarayanan, J. J. Kistler, P. Kumar, M. E. Okasaki, E. H. Siegel, and D. C. Steere.:
Coda: A highly available file system for a distributed workstation environment. IEEE
Transactions on Computers, 39(4):447–459, 1990.
13. F. Schmuck and R. Haskin.: GPFS: A shared-disk file-system for large computing clusters.
In Proceedings of the 2002 Conference on File and Storage Technologies (FAST),
USENIX, Jan. 2002.

