Using Haptics to Improve Immersion in Virtual
Environments
Priscilla Ramsamy, Adrian Haﬀegee, Ronan Jamieson, and Vassil Alexandrov
Centre for Advanced Computing and Emerging Technologies,
The University of Reading, Reading, RG6 6AY,
United Kingdom
sir04pr@reading.ac.uk

Abstract. Current immersive Virtual Reality (VR) system strategies
do not fully support dynamic Human Computer Interaction (HCI) and
since there is a growing need for better immersion, due consideration
should be given to integrate additional modalities for improved HCI.
While feedback in Virtual Environments (VE) is predominantly provided
to the user through the visual and auditory channels, additional modalities such as haptics can increase the sense of presence and eﬃciency in
VE simulations. Haptic interfaces can enhance the VE interaction by enabling users to “touch” and “feel” virtual objects that are simulated in
the environment. This paper examines the reasons behind its integration
based on the limitations of present immersive projection system.
Keywords: Virtual Reality, Haptics, Tactile feedback, Force feedback.

1

Introduction

There are three main categories of virtual reality systems whereby each implementation is ranked by the sense of immersion or the degree of presence it provides to the user. While moving from desktop systems (non-immersive) through
semi-immersive (power-walls) to ﬁnally, a fully immersive system (e.g. the CAVE
[20]) end users are provided with a much richer and engaging experience [2].
Virtual reality permits the user to ‘step-into’ a computer generated world or a
Virtual Environment by immersing the user in the synthetic world. Considering
that virtual worlds are completely conceived and created by the user, it is under
their complete control and is therefore not conﬁned to the laws of physics [3].
Human interaction capabilities are restricted if VEs deprive the users of sensorial
cues that they experience in the real world; not only do we rely on our visual
and auditory cues but we also depend on information that is conveyed through
touch and force. These haptic cues complement the usual visual (graphics) and
sound feedback modalities used in current VR simulations [1]. As such, providing
greater realism to users could be achieved by integrating these sensory cues
during the manipulation and interaction of virtual objects in VEs. A virtual
object is a representation of an object in the virtual environment.
V.N. Alexandrov et al. (Eds.): ICCS 2006, Part II, LNCS 3992, pp. 603–609, 2006.
c Springer-Verlag Berlin Heidelberg 2006

604

2

P. Ramsamy et al.

Haptics and Limitations of Our Current System

Haptic is a term derived from the Greek “haptesthai” which means “to come in
contact with” by providing the sense of touch with both tactile (cutaneos) and
kinaesthetic (proprioception) feedback [4]. Tactile, or touch feedback describes
the sensations felt by the skin. Tactile feedback permits users to feel things
such as the texture of surfaces, temperature and vibration. Kinaesthetic force
feedback on the other hand enables us to recognise a force that is applied to our
body by the help of sensoric cells located at the end of the tendons or between
the muscle strands. This permits us to measure the weight of grasped virtual
objects.
Haptics can be further broken down into three diﬀerent areas.
1. Human haptics: Understanding the roles played by the mechanical, sensory, motor and cognitive subsystem of the human haptic system.
2. Machine haptics: Constructing haptic hardware device interfaces to replace or enhance the human touch.
3. Computer haptics: Considers the techniques to generate and display touch
and feel sensations to users by using a force reﬂecting device. It considers
both the behaviour of virtual objects as well as the rendering algorithm for
the time displays.
Input devices such as computer keyboard, mice and wands can convey the
user’s commands to the computer but are unable to provide a natural sense of
touch and feel to the user. It is crucial to understand the nature of touch interaction, since we react to our surroundings based on what and how we perceive
things [5]. A study conducted by researchers at Texas University to identify the
eﬀects of the visual component of haptics proved to show, that by adding sensory cues end-users were able to identify hidden objects. The tests carried out
consisted of introducing a 3-dimensional object that could be made invisible [6].
By separating the haptic and the visual components the user had to rely on the
sense of touch to identify the object in question. The results obtained made it
possible to quantify the eﬀectiveness of using in this case, point-based haptic
devices and was considered to be beneﬁcial for applications for visually impaired
users. The user’s sensorial experience in a virtual environment is a fusion of
information received through his perception channels. Based on the above observations and on[1][7][8][9] there is great potential to improve VR simulations.
By adding haptic enabled interfaces in our current immersive projection system
at Reading University it is hoped that by providing tactile and force feedback
human performance and eﬃciency can be enhanced if the realism of the VE is
closer to the physical world. The additional information from haptic feedback
makes certain tasks such as manipulation much easier compared to that of the
traditional computer interface [5][11]. The next section provides an overview of
haptic rendering techniques.

Using Haptics to Improve Immersion in Virtual Environments

3

605

Haptic Rendering Algorithms

The haptic algorithm consists of 2 steps:
1. A collision detection step that determines if the user’s pointer is colliding
with a virtual object. Information about the extent of penetrations and where
collisions have occurred is determined during this step.
2. A collision response step that computes the interaction force between the
user’s pointer and virtual objects when a collision is detected. The return
values are normally force and torque vectors that are applied to the haptic
interface. If there is a collision, the contact information is sent to the collision
response algorithm which generates the relevant tactile feedback based on
the force model used.
There are several collision detection algorithms and each one of them possesses
certain advantages in diﬀerent situations. The choice of the algorithm will depend
on the speed of calculation to detect the collision and also on the haptic interface
used. For the purpose of our implementation we will consider using the friction
cone algorithm with the associated face transition algorithms discussed in [12].

Fig. 1. Interaction methods [5]

Haptic rendering with force display also depends on the techniques used to
model the probing object. Figure 1 provides models of the probing object.
– The simplest one is the point-based model Figure 1a where the probe is
modelled as a point. Here only the tip of the end-eﬀector is used to explore
and manipulate objects. Checks are made to verify whether the tip of the
end-eﬀector is inside the virtual object and if so, the penetration depth is
calculated based on the diﬀerence between the current position of the probe
and the surface contact point.
– Ray-based interaction Figure 1b is where the probe is represented as a line
segment. In this case the entire length of the line segment as well as its tip
is used to explore and manipulate objects.
– The probe can be modelled as a 3D-object and is made up of points, line
segments and polygons as shown in Figure 1c. Collisions between vertices
of its geometry and the objects of the scene are checked every time the
probe is moved. This is the most realistic technique but is also the most
computationally expensive one [5][14][15][16].

606

P. Ramsamy et al.

The type of interaction method used in simulations greatly depends on the
needs and complexity of the application. Moving from point based, ray-based to
3D-object we increase the accuracy but while so doing we would require intensive
computation to detect collisions [13][17]. We therefore have to consider the trade
oﬀs between realism of the VEs and computational costs. For our implementation
we will consider the ray-based technique. The next section covers some of current
haptic device operations that have been implemented.

4

Current Feedback Device Operations

The device operations are methods that are concerned with setting and getting
state. The device will require initialisation before use and will be initialised
by setting the values to zero. In our implementation we have a instrumented
tactile glove and a grounded haptic mobile platform both are currently being
developed at Reading University. The tactile glove can replace or be used in
conjuction with the traditional wand to provide a more realistic and intuitive
way of manipulating virtual objects. The haptic mobile platform will be replacing
the traditional joystick and will permit the user to navigate within the virtual
environment. It could mimic several devices ranging from a bicycle to a trolley
or hang glider.
4.1

The Instrumented Tactile Glove

The instrumented glove provides the following information:
– Position of the hand
– Finger angles - provides the information on whether the hand is clenched
or open. To be noted that in our implementation of the haptic glove only
three ﬁngers (thumb, index and the middle ﬁnger) are actively being used
and calibrated.
The device state and other information will be retrieved by using the haptic
glove functions. The functions return -1 on failure indicating a hardware or
network problem and 0 on success. The existing functions can retrieve the hand
orientation and ﬁnger angles. Based on these values and on a collision detection
algorithm we can render the appropriate feedbacks. If collision is detected with a
manipulatable object the person would be able to interact with and manipulate
the object. While the grasp aperture (the distance between thumb and ﬁngers)
is less than a critical distance (Gc) the object will be attached to the glove.
If the grasp aperture is greater than Gc then we apply object physics to the
object. Gc could either be set to a ﬁxed value or vary depending on the size of
the object being grasped. In order to produce the tactile cues on contact with
virtual objects we need to generate some kind of reaction to the user’s skin.
While the skin responds to several distributed physical quantities such as highfrequency vibrations, pressure distribution and thermal properties in our ﬁrst
attempt, we will incorporate a vibro-tactile actuator in the centre of the palm.

Using Haptics to Improve Immersion in Virtual Environments

607

The actuator will vibrate informing the user when he/she has made ﬁrst contact
with the object upon which the user can then manipulate the virtual object by
picking it up (clenching the hand).
4.2

Grounded Haptic Mobile Platform

The grounded haptic mobile platform provides information on the following:
– The forward and backward velocity,
– The platform turn rates
– The amount of inertia, damping and friction - this would be dynamic depending on the force applied by the end-user onto the mobile platform handle.
The haptic enabled mobile platform functions will have similar error checks
to that of the tactile glove. The class functions can retrieve the force applied,
the displacement forward or backwards and the angle of rotation and can set
the relevant constraints on the grounded mobile platform. If the mobile platform
comes in contact with a surface in order to imitate what is experienced in real
life, it should not be able to rotate or move forward. The programmer should
also be able to query the forces applied as well as being able to deﬁne and set
new forces to the object (in this case the mobile platform).

5

Future Work

We are currently in the early stages of the project and the main focus has been
on the development and integration of the haptic hardware interface. Our future
work will concentrate on the reﬁnement of the current haptic device operations.
We could enhance and provide more functionalities to the instrumented glove
by increasing the number of vibrotactile stimulators (incorporating them onto
each ﬁnger and not only in the palm) where, each stimulator can be individually
programmed to vary the strength of touch sensation. Complex tactile feedback
patterns can be produced by using a combination of stimulators to generate
simple sensations such as pulses or sustained vibration. This would bring an
enhanced realistic mode of interaction in immersive VEs whereby the user can
pick up and maniputate for instance objects in a the virtual museum, it can also
be used during the simulation of complex surgeries [18] by rendering accurate
tactile information and by providing a natural and more intuitive way of control
and manipulation.
Development of our own haptic library to provide collision detection or the
integration of existing haptic rendering software subsystems will also be considered. Future investigation is required as to the possibility of using open source
haptic software frameworks that are available for haptic programming and control of haptic force feedback devices. Other issues that can be considered is
the fact that haptic rendering complex objects in virtual environments is computationally intensive and therefore due consideration should be given to the
reduction of computational cost. We should have the ability to control the Level
of Detail (LoD) of objects in VEs by substituting ﬁne models with coarser ones
to reduce the computation cost [19].

608

6

P. Ramsamy et al.

Conclusion

While several architectures have been reviewed it has been found that fewer fully
immersive systems make use and integrate haptic feedback devices. By integrating sensorial cues in our immersive projection VE system, end-users would be
provided with a more intuitive and eﬃcient way of manipulating and interacting
with virtual objects. The interface has been successfully developed and permits
both the tactile and force feedback devices to integrate with our immersive system, and is being used in ongoing application development.

Acknowledgements
I would like to thank Dr William Harwin and those at ISRG for their help.

References
1. Burdea, G.C. Haptics issues in virtual environments. In Computer Graphics International,Proceedings. (2000)
2. P.J. Costello, Health and Safety Issues associated with Virtual Reality - A Review
of Current Literature. Advisory Group on Computer Graphics (AGOCG) Technical
Reports, July (1997)
3. K.P. Beier, Virtual Reality: A Short Introduction. http://www-vrl.umich.
www-vrl.umich.edu/intro/
4. M. Mokhtari, F. Bernier, F. Lemieux, H. Martel, J.M. Schwartz, D. Laurendeau
and A. Branzan-Albu. Virtual Environment and Sensori-Motor Activities: Haptic,
Audition and Olfaction. WSCG POSTERS proceedings Plzen, Czech Republic.,
Vol.12, No.1-3, February, (2004) 2-6
5. M.A. Srinivasan and C. Basdogan, “Haptics in Virtual Environments: Taxonomy,
Research Status, and Challenges,” Computers and Graphics, Special Issue on Haptic Displays in Virtual Environments, Vol. 21, No. 4, (1997)
6. Eric Acosta, Bharti Temkin, “Touch&Tell: A game-based tool for learning to use
the PHANToM,” Seventh PHANToM Users Group Workshop, October (2002)
26-29
7. F. P. Brooks, M. Ouh-Young, J. J. Batter, and P. J. Kilpatrick. Project GROPE
– Haptic Displays for Scientiﬁc Visualization. In Proc. ACM SIGGRAPH, Dallas,
TX, Aug (1990) 177–185
8. Michael Dinsmore, Noshir Langrana, Grigore Burdea, Jumoke Ladeji. “Virtual
Reality Training Simulation for Palpation of Subsurface Tumors,” vrais, Virtual
Reality Annual International Symposium, (1997) 54
9. Burdea, G., G. Patounakis, V. Popescu, & R. E. Weiss. Virtual Reality Training
for the Diagnosis of Prostate Cancer. In IEEE International Symposium on Virtual
Reality and Applications, Atlanta, Georgia, March (1998) 190-197
10. Burdea G. Haptic Interfaces for Virtual Reality. Proceedings of International Workshop on Virtual prototyping, Laval, France, May (1999) 87-96
11. Andrew G. Fischer, Judy M. Vance, “Implementing Haptic Feedback in a Projection Screen Virtual Environment,” Seventh PHANToM Users Group Workshop,
October (2002) 26-29

Using Haptics to Improve Immersion in Virtual Environments

609

12. Melder, N. and W.S. Harwin. Extending the friction cone algorithm for arbitrary
polygon based haptic objects. in Haptic Interfaces for Virtual Environment and
Teleoperator Systems, HAPTICS ’04. Proceedings. 12th International Symposium
(2004)
13. Gregory, A., Mascarenhas, A., Ehmann, S., Lin, M. and Manocha, D. “Six Degreeof-Freedom Haptic Display of Polygonal Models”, Proceedings of 2000 IEEE Visualization. (2000)
14. Ho, C., Basdogan, C. and Srinivasan, M. “An eﬃcient haptic rendering technique
for displaying 3D polyhedral objects and their surface details in virtual environments”, Presence: Teleoperators and Virtual Environments, (1999) 477-491
15. Chris Raymaekers, Joan De Boeck, Karin Coninx. “An Empirical Approach for the
Evaluation of Haptic Algorithms,” First Joint Eurohaptics Conference and Symposium on Haptic Interfaces for Virtual Environment and Teleoperator Systems
(WHC’05) (2005) 567-568
16. Cristian Luciano, Pat Banerjee, Thomas DeFanti, Sanjay Mehrotra. Realistic
Cross-Platform Haptic Applications Using Freely-Available Libraries. Haptic Interfaces for Virtual Environment and Teleoperator Systems, HAPTICS ’04. Proceedings. 12th International Symposium March (2004) 282 - 289
17. Kim, Y., Lin, M. and Manocha, D. “DEEP: Dual-space Expansion for Estimating
Penetration depth between convex polytopes”, IEEE International Conference on
Robotics and Automation, (2002)
18. A. Al-khalifah, D. Roberts, “Survey of modeling approaches for medical simulators”, International Conference Series on Disability, Virtual Reality and Associated
Technologies, Oxford, (2004) 321-329
19. Jian Zhang, Shahram Payandeh, John Dill. “Levels of Detail in Reducing Cost
of Haptic Rendering: A Preliminary User Study,” haptics, 11th Symposium
on Haptic Interfaces for Virtual Environment and Teleoperator Systems (HAPTICS’03),(2003) 205
20. Cruz-Neira, C.,Sandin, S.J.,DeFanti, T.A., Kenyon, R.V., and Hart, J.C. The
CAVE: Audio visual experience automatic virtual environment. Communications
of the ACM 35 (1992) 64-72

