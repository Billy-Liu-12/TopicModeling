Computing MAS Dynamics Considering the
Background Load
Maciej Smolka1, and Robert Schaefer2
1

Institute of Computer Science, Jagiellonian University, Krak´
ow, Poland
smolka@ii.uj.edu.pl
2
Department of Computer Science,
Stanislaw Staszic University of Science and Technology, Krak´
ow, Poland
schaefer@agh.edu.pl

Abstract. The paper extends the formal model of a computing Multi
Agent System introduced in our previous papers to the case in which
the background load coming from operating systems activities as well as
other applications is included. Results concerning the existence of the optimal scheduling strategy as well as the characterization of such strategies
have been obtained. The theorems partially verify scheduling heuristics
(diﬀusion rules) designed and tested for large scale CAE computations.

1

Introduction

Multi-Agent Systems (MAS) dedicated to the distributed computing have been
developed for the last several years (see. e.g. [1]). A formal model of such systems
was introduced in our earlier papers [2, 3, 4]. It enables us to formulate a new
deﬁnition of the optimal scheduling problem which is well suited to the diﬀusiontype distributed government of computing agents. We extend this model to the
case in which the background load coming from operating systems activities as
well as other applications is included. Results concerning the existence of the
optimal scheduling strategy as well as the characterization of such strategies
have been obtained. Such theorems partially verify scheduling heuristics (agent
diﬀusion rules) designed and tested for large scale CAE computations [5, 6].

2

Properties of a Computing MAS

The suggested architecture of the system is composed of: a computational environment (MAS platform) and a computing application being a collection of
mobile agents called Smart Solid Agents (SSA). The computational environment
is a triple (N, BH , perf ), where:
N = {P1 , . . . , PN } , where Pi is a Virtual Computation Node (VCN). Each VCN
can maintain more than one agent.
This author has been supported by the State Committee for Scientiﬁc Research of
the Republic of Poland under research grants 2 P03A 003 25 and 7 T07A 027 26.
V.N. Alexandrov et al. (Eds.): ICCS 2006, Part III, LNCS 3993, pp. 799–806, 2006.
c Springer-Verlag Berlin Heidelberg 2006

800

M. Smolka and R. Schaefer

BH is the connection topology BH = {N1 , . . . , NN }, Ni ⊂ N is an immediate
neighborhood of Pi (including Pi as well).
perf = {perf1 , . . . , perfN }, perfi : R+ → R+ is a family of functions, which describes relative performance of all VCN’s with respect to the total memory
i
i
of all allocated agents. If Mtotal
on Pi is small, perfi turns
request Mtotal
back the constant value, which depends only on the CPU architecture. If
i
Mtotal
is larger, perfi decreases due to the intensive swap utilization.
The MAS platform is responsible for maintaining the basic functionalities of the
computing agents.
– It delivers the information about the local load concentration Lj and Qj
(see (2) and (3) below),
– It performs the agent migration between the neighboring VCN’s,
– It executes the agent partitioning and destruction,
– It supports the transparent communication among agents.
We shall denote an SSA by Ai where index i stands for an unambiguous agent
identiﬁer. Each Ai contains its computational task and all data necessary for
its computations. Another component of an agent is a shell which is responsible
for the agent logic. An SSA can denominate the pair (Ei , Mi ) where Ei is the
remaining computation time measured in units common for all agents of an
application and Mi is the agent’s RAM requirement in bytes. An agent may
undertake autonomously the following actions:
– Continue executing its internal task,
– Migrate to a neighboring VCN if it founds better resources there (e.g. the
new VCN is signiﬁcantly less loaded),
– Decide to be partitioned, which results in creating two child agents {Aij =
(Tij , Sij )}, j = 1, 2. We assume that in the case of the agent partitioning the
following conditions holds: Ei > Eij , Mi > Mij , j = 1, 2. The parent SSA
dies after such a partition.
A computing application may be characterized by the triple (At , Gt , Scht ), t ∈
[0, +∞) where:
At is the set of application agents, At = {Aξj }ξj ∈It , It is the set of indices of
agents active at the time t.
Gt is the tree representing agents partitioning at the time t. All agents constitute the set of nodes ξ∈Θ Aξ , Θ = tj=0 Ij , while Gt edges show the
partitioning history. All information on how to rebuild Gt is spread among
all agents such that each of them knows only its neighbors in the tree.
{Scht }t∈[0,+∞) is the family of functions such that Scht : At → N is the current schedule of application agents among the MAS platform servers. The
function is represented by the sets ωj of agents’ indices allocated on each
Pj ∈ N. Each of ωj is locally stored and managed by Pj .

Computing MAS Dynamics Considering the Background Load

801

Each server Pj ∈ N periodically asks all local agents (allocated on Pj ) for their
requirements and computes the local load concentration
Lj =

j
Etotal
j
perfj (Mtotal
)

j
=
where Etotal

j
Ei and Mtotal
=
i∈ωj

Mi

(1)

i∈ωj

Then Pj communicates with neighboring servers and establishes
ζ
ζ
Lj = {(Lζ , Etotal
, Mtotal
, perfζ )} where ζ is such that Pζ ∈ Nj

(2)

as well as the set of node indices Qj such that
k ∈ Qj ⇐⇒ k = j, Pk ∈ Nj , Lj − Lk > 0

(3)

The detailed description of the architecture and the agent governing strategies
mentioned above as well as their current application may be found in our earlier
papers (see e.g. [2, 5, 6]). The agent migration and partitioning rules described
in these papers are related to the phenomenon of the molecular diﬀusion in
crystals. The agent Ai migrates along the gradient of its binding energy which
depends mainly on its remaining computation time Ei and the local load concentration [2].

3

The System State and the Background Load

In this section we recall our formal mathematic model for multi-agent computations. It was already presented in [2] and further developed in [3, 4], so we
shall use the notations introduced therein. Denote by A the set of all agents of
an application. Recall the notion of the vector weight of an agent which is the
mapping w : N × A −→ R2+ whose components are Ei and Mi as above. Assume
that the dependency of the total weight of child agents after partition upon their
parent’s weight before partition is well-known, componentwise and linear, i.e. we
know the constants c1 , c2 ≥ 0 such that in the case of partition A → {A1 , A2 }
we have
i
i
(A1 ) + wt+1
(A2 ) = ci wti (A)
wt+1
for i = 1, 2. Such an assumption seems realistic, in simple cases we can have
c1 = c2 = 1 but in general the constants may be either greater or less than 1.
In order to avoid considering a single agent’s dynamics we deﬁne a global
quantity describing our computing system as a whole. Namely recall the notion
of the total weight of all agents allocated on a virtual node P at any time t, i.e.
Wt (P ) =

Scht (A)=P

wt (A).

We put 0 if no agent is maintained by P .
In the sequel we shall assume that the number of virtual nodes N = N is
ﬁxed. Thus we can consider Wt as a nonnegative vector in R2N such that
Wtj = Wt1 (Pj ),

WtN +j = Wt2 (Pj )

802

M. Smolka and R. Schaefer

for j = 1, . . . , N . We shall treat Wt as a state of the computing MAS at a
time t. Considering its dynamics we assume ﬁrst of all that in an idealized
situation when migrations and partitions do not occur Wt evolves according to
the following recursive equation
Wt+1 = F (Wt , ξt )

(4)

where ξt is a given sequence of random variables (a stochastic process) representing the background load. (4) means that Wt is itself a discrete stochastic process.
Wt = 0 means that our computations are ﬁnished so we do not want the system
to leave this state. This leads to the assumption that for every t we have
F (0, ξt ) = 0

(5)

with probability 1.
As for ξt , we assume that it takes the ﬁnite number of values in the cube
[0, M ]N . Its i-th coordinate (ξt )i may be interpreted as an additional load concentration on the i-th VCN, which may slow down computations performed by
agents. We shall distinguish the following background load behaviors expressed
by the conditions imposed on the sequence (ξt )t=0,1,2,... .
ξ(1) The momentary background load changes rapidly many times during a
single time period of the model. It may be generated by the basic activities
of operating systems and network protocols. We assume that all ξt have the
same distribution for each time period and they are mutually independent.
This case has been considered in [4].
ξ(2) The variables ξt are also mutually independent but have diﬀerent, wellknown probability distributions. The distributions may change periodically
after each T time steps of the model. Such a background load may be caused
by small tasks which are executed in the period comparable with the time
step of the model.
ξ(3) The sequence (ξt )t=0,1,2,... constitutes a stationary Markov chain. This is
the case of the system additionally loaded by a set of similar tasks whose
execution time is comparable to the several steps of the model. The large
number of such tasks in a particular time step increases the probability of
the high background load in the next step.
ξ(4) The variables ξt constitute a nonstationary, periodical Markov chain with
period T . Such a situation may be related to the case of background tasks
with the long execution time (much longer then the time step of the model)
that appear according to a well-known trend. Such a situation is considered
e.g. in [7].

4

The MAS Dynamics

Now we shall formulate the full equations of evolution of Wt . To this end we
need to put into (4) terms representing migrations and partitions. A detailed

Computing MAS Dynamics Considering the Background Load

803

description of this process is presented in [4], here we show only the ﬁnal step.
Namely we have
j
i
1
˜ t , ξt ) + c1 u1 (Wt ) W i +
= F i (W
Wt+1
t
ii,t
j=i uji,t (Wt ) Wt
N +j
N +i
2
˜ t , ξt ) + c2 u2 (Wt ) W N +i +
Wt+1
= F N +i (W
t
ii,t
j=i uji,t (Wt ) Wt

(6)

for i = 1, . . . , N , where
˜ ti = 1 −
W

N
k=1

u1ik,t (Wt ) Wti ,

˜ tN +i = 1 −
W

N
k=1

u2ik,t (Wt ) WtN +i

with initial conditions
ˆ.
W0 = W

(7)

N
um
jj,t : R+ → [0, 1], m = 1, 2 are the proportions of the weight components of
splitting agents to the corresponding components of the total weight of all agents
at node j at time t and um
jk,t for j = k are the respective proportions of the weight
of agents migrating from j to k. Thus the ﬁrst term on the right-hand side of (6)
corresponds to the ’established’ evolution of agents which neither migrate nor
split, the second is the weight of partitioned agents and the third is the weight
gathered as a result of the immigration. It follows that our Wt is a controlled
stochastic process with a control strategy

π = (u1t , u2t )t∈N

(8)

2N
N ×N
such that um
t : R+ −→ U . The control set U contains matrices α ∈ [0, 1]
that satisfy at least the following conditions

αij · αji = 0 for i = j,

αi1 + · · · + αiN ≤ 1 for i = 1, . . . , N .

(9)

The ﬁrst equation in (9) can be interpreted in the following way: at a given
time migrations between two nodes may happen in only one direction. The second equality says that the number of agents leaving a node must not exceed the
number of agents present at the node just before the migration.
In practice it may be appropriate to impose some stronger conditions on a
control set (detailed considerations about that can be found in [4]). Denote by
G(W, u, ξ) the right hand side of (6) with u = (u1 , u2 ). Then we can rewrite our
equation of evolution in the following way
Wt+1 = G (Wt , ut (Wt ), ξt ) .

(10)

It is easy to see that thanks to (5) we have also that
G(0, u(0), ξt ) = 0

(11)

with probability 1 for any admissible u and any t ∈ N.
In the most general case one could take the whole R2N
+ as the state space S
of Wt . But in reality both the available RAM and the maximal estimated time

804

M. Smolka and R. Schaefer

of computations are bounded and quantized, so it is not unnatural to consider
the state space ﬁnite.
The character of the state dynamics changes for diﬀerent types of the background load. Detailed considerations on the case ξ(1) are contained in [4]. In
short, it can be shown that in this case Wt is a stationary controlled Markov
chain. The case ξ(2) is similar, but this time Wt is not stationary. In fact when
ξt is T -periodic, Wt is a T -periodic Markov chain (for every u). In both cases
(11) means that 0 is an absorbing state (cf. [8]) of Wt .
When ξt is itself a Markov chain, we can no longer say the same about Wt .
We can only show that the latter is a second-order Markov chain (i.e. a process
with two-step memory). Such a process can be transformed into a Markov chain
by means of a simple classical transformation of the state variable (see e.g. [8].
Namely we should put
Xt =

Wt−1
,
Wt

X0 =

ˆ
W
ˆ .
W

(12)

It can be shown that the state equations for Xt have the form
Xt+1 =

Xt2
2
G(Xt , ut (Xt2 ), ξt )

.

(13)

In the case ξ(3) such Xt is simply a stationary controlled Markov chain, and,
again, in the case ξ(4) it is T -periodic provided ξt is T -periodic. From the form
of (13) and from (11) it follows that also in these cases 0 is an absorbing state.
Note that when Wt (or Xt ) is T -periodic, it can be transformed into a stationary process by introducing a new state Yt = [WT t , . . . , WT (t+1)−1 ]. In the
sequel we shall consider only stationary Markov processes, in other words the
cases ξ(1) and ξ(3). But when we have a background process of type ξ(2) or ξ(4)
which is periodic, we can apply the above transformation and all the following
results shall remain true.

5

The Optimal Scheduling Problem

The general form of the cost functional for controlled Markov chains of our type
is (cf. [8])
ˆ ) = E [ ∞ k(Wt , ut (Wt ))]
V (π; W
(14)
t=0
ˆ is the initial state of Wt . Since 0 is an
where π is a control strategy (8) and W
absorbing state we shall always assume that k(0, ·) = 0, i.e. remaining at 0 has
no cost (it guarantees that the overall cost can be ﬁnite). In the case ξ(3) we
need to rewrite (14) in the following manner.
V¯ (π; X0 ) = E

∞
t=0

k(Xt2 , ut (Xt2 ))

(15)

Note that (15) inherits good properties (such as putting no cost on remaining
at 0 or satisfying assumption A(3) of Prop. 1) from (14), so in the sequel we can
safely substitute Xt for Wt and V¯ for V and all the results shall remain true.

Computing MAS Dynamics Considering the Background Load

805

We deﬁne the following set of admissible controls U = π : um
t (s) ∈ Us , m =
1, 2 . where Us is a closed subset of U (see (9)). Now we are ready to formulate
ˆ we
the optimal scheduling problem. Namely given an initial conﬁguration W
search for such control strategy π ∗ ∈ U that
ˆ ) = min V (π; W
ˆ ) : π ∈ U, Wt is a solution of (10), (7) .
V (π ∗ ; W

(16)

Consider some cost functionals which seem appropriate for multi-agent computations. The ﬁrst one is the expected total time of computations
ˆ ) = E inf{t ≥ 0 : Wt = 0} − 1 .
VT (π; W

(17)

We can rewrite it in the form (14) if we put k(s, α) = 1 for s = 0 and any α.
The second example takes into account the mean load balancing over time. It
has the following form
ˆ)=E
VL (π; W

∞
t=0

N
i
i=1 (Lt

− Lt )2

Wi

(18)
N

i
where Lit = perf (Wt N +i ) is the load concentration and Lt = N1
i=1 Lt is its
i
t
mean over all computing nodes. This time the form of k is straightforward.
Both the above examples do not contain an explicit dependency on the control.
Generalizing it a little allows us to penalize migrations. Namely take ϕ : S → R+ ,
m
a ≥ 0, m
ij ≥ 0 and μij : [0, 1] → R+ continuous nondecreasing and such that
m
μij (0) = 0, and put

ˆ)=E
VM (π; W

6

∞
t=0

ϕ(Wt ) + a

2
m=1

i=j

m m m
ij μij (uij,t (Wt ))

.

(19)

Results

Now let us revisit some theoretical results that were proven for our model in [4]
and extend them for considered types of the background load. First consider the
existence of optimal solutions for problem (16). Let us recall that in [4] we used
the notation R(u) for the ’probably not absorbing’ part of the transition matrix
for our system while applying control u and by Rn (u) the analogous part of
n-step transition matrix obtained by applying the stationary control ut = u (for
details on this notion see [4] or [8]). The following proposition is a straightforward
consequence of [8, Theorem 4.2].
Proposition 1. Assume that A(1) and (A(2) or A(3)) hold.
A(1) k(s, ·) is continuous on Us × Us
A(2) RK (u) is a contraction for every u such that u(s) ∈ Us × Us
A(3) Rn (u) is a contraction for some n ≥ 1 and u as above but additionally
k(s, α) ≥ ε > 0 for s = 0, α ∈ Us × Us .
Then there exists the unique optimal solution of (16).

806

M. Smolka and R. Schaefer

This proposition allows us to prove the existence of optimal solutions for the
cost functionals VT , VL and VM . In case of VT we need the assumption A(3), in
case of VL the assumption A(2) and in case of VM either A(3) or A(2) depending
on whether ϕ is separated from 0 or not (cf. [4]).
Now we shall recall some Bellman-type optimality conditions for (16). Let
us denote the elements of the state space S by si , i = 0, . . . , K. The following
proposition is another consequence of [8, Theorem 4.2] and its proof.
Proposition 2. Make the same assumptions as in Proposition 1. Then the optimal solution of (16) is a stationary strategy π ∗ = u∞ = (u, u, . . . ) and it is the
unique solution of the equation
V (π ∗ ; s) = minα∈(Us )2

K
j=1

P (G(si , α, ξ0 ) = sj )V (π ∗ ; sj ) + k(s, α) .

(20)

The solution of (20) exists and it is the optimal solution of (16).

7

Conclusions

The presented MAS technology as well as the diﬀusion scheduling of agents give
the advantageous environment for large-scale distributed computations (see [5]).
The presented formal model based on the discrete stochastic ﬁeld enables us to
formulate a new deﬁnition of the optimal scheduling in such an environment. It
have been shown (Propositions 1 and 2), that there exists an optimal diﬀusion
scheduling strategy in the class of stationary rules that are intensively utilized
during tests [5, 6]. This extends the earlier results [4] to the case in which the
background load caused by operating systems and diﬀerent kinds of other applications is taken into account.

References
1. Luque, E., Ripoll, A., Cort´es, A., Margalef, T.: A distributed diﬀusion method for
dynamic load balancing on parallel computers. In: Proceedings of EUROMICRO
Workshop on Parallel and Distributed Processing, San Remo, Italy, IEEE Computer
Society Press (1995) 43–50
2. Smolka, M., Grochowski, M., Uhruski, P., Schaefer, R.: The dynamics of computing
agent systems. Lecture Notes in Computer Science 3516 (2005) 727–734
3. Grochowski, M., Smolka, M., Schaefer, R.: Architectural principles and scheduling
strategies for computing agent systems. Fundamenta Informaticae (2006) accepted.
4. Smolka, M.: Optimal scheduling problem for computing agent systems. Inteligencia
Artiﬁcial (2006) accepted.
5. Grochowski, M., Schaefer, R., Uhruski, P.: Diﬀusion based scheduling in the agentoriented computing systems. Lecture Notes in Computer Science 3019 (2004) 97–
104
6. Momot, J., Kosacki, K., Grochowski, M., Uhruski, P., Schaefer, R.: Multi-agent
system for irregular parallel genetic computations. Lecture Notes in Computer
Science 3038 (2004) 623–630
7. Lepiarz, M., Onderka, Z.: Agent system for load monitoring of the heterogeneous
computer network. Lecture Notes in Computer Science 2328 (2002) 364–368
8. Kushner, H.: Introduction to Stochastic Control. Holt, Rinehart and Winston (1971)

