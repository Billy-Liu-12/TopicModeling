Schur Decomposition Methods for the
Computation of Rational Matrix Functions
T. Politi1 and M. Popolizio2
1

2

Dipartimento di Matematica, Politecnico di Bari,
Via Amendola 126/B, I-70126 Bari (Italy)
politi@poliba.it
Dipartimento di Matematica, Universit`
a degli Studi di Bari,
Via Orabona 4, I-70125 Bari (Italy)
popolizio@dm.uniba.it

Abstract. In this work we consider the problem to compute the vector
y = Φm,n (A)x where Φm,n (z) is a rational function, x is a vector and
A is a matrix of order N , usually nonsymmetric. The problem arises
when we need to compute the matrix function f (A), being f (z) a complex analytic function and Φm,n (z) a rational approximation of f . Hence
Φm,n (A) is a approximation for f (A) cheaper to compute. We consider
the problem to compute ﬁrst the Schur decomposition of A then the matrix rational function exploting the partial fractions expansion. In this
case it is necessary to solve a sequence of linear systems with the shifted
coeﬃcient matrix (A − zj I)y = b.

1

Introduction

Matrix functions arise in diﬀerent ﬁelds of Mathematical Sciences and Engineering, in particular in connection with the solution of ordinary diﬀerential systems, with applications in control theory (see [1]), nuclear magnetic resonance,
Lie group methods for geometric integration (see [8]), and in the numerical solution of stiﬀ diﬀerential equations (see [11], and references therein). Usually
the matrix function can be deﬁned via power series or as the solution of nonlinear systems. Several methods have been proposed in past to solve one of the
following problems:
– Given a complex matrix A ∈ Cn×n compute f (A), where f is an analytic
function on a region of the complex plane;
– Given a matrix A with a given geometric structure compute the matrix
function f (A), having a special structure too;
– Given a matrix A and a vector x compute f (A)x.
The second problem has a great interest in very important cases when f (z)
is the exponential function. In fact it is well known that the exponential map
is a function deﬁned from a Lie Algebra to its related Lie Group, i.e. if A is
skew-symmetric (i.e. AT = −A) then Q = exp(A) is orthogonal. In these cases
it is very important in the numerical approximation to preserve the property.
V.N. Alexandrov et al. (Eds.): ICCS 2006, Part IV, LNCS 3994, pp. 708–715, 2006.
c Springer-Verlag Berlin Heidelberg 2006

Schur Decomposition Methods

709

Recently this problem has been considered using approaches based on the Krylov
subspaces technique but also exploiting the Schur decomposition of Hamiltonian
skew-symmetric matrices ([5, 9]).
In past many algorithms have been proposed for the approximation of matrix
f (A) (see [7]) and most of these are based on the rational approximation of f ,
Φm,n (z) =

Rm (z)
,
Qn (z)

being Rm (z) and Qn (z) polynomials of degree m and n, respectively. Hence
f (A)

Φm,n (A) = Rm (A) [Qn (A)]

−1

.

A classical way to obtain the function Φm,n (z) is to use the Pad´e approximation
(usually with m = n) together with a pre-processing technique for matrix A
(usually the scaling and squaring technique, see [6]).
In this work we decribe brieﬂy some numerical methods based on the Schur
decomposition of the matrix, in order to compute the rational function approximation of the exponential matrix function, then we consider the use of the partial
fraction expansion. In Section 2 we describe the numerical methods based on the
Schur decomposition, while in Section 3 we describe the algorithms to compute
rational matrix functions based on the partial fractions expansion technique.
Finally in Section 4 some numerical issues are shown.

2

Numerical Methods Based on the Schur Decomposition

A general approach to compute f (A), with A ∈ Cn×n is to employ similarity
transformation
(1)
A = SDS −1
where f (D) is easily computable. In fact
f (A) = Sf (D)S −1 .

(2)

If A is diagonalizable then we could take D diagonal hence the computation of
f (D) is trivial since it is a diagonal matrix. For stability problems it is a good
choice to take well conditioned matrix S in (1). The better way to take a well
conditioned matrix is S to be orthogonal, so that the decomposition (1) becomes
the Schur decomposition of A:
A = QT Q∗ ,
where T is upper triangular or block upper triangular. The computation of the
Schur factors is achieved with perfect backward stability by the QR algorithm
(see [6]). Once the decomposition has been performed the problem is the computation of f (T ). If T is upper triangular also
F = f (T )

710

T. Politi and M. Popolizio

is upper triangular, so that Parlett (see [10]) proposed the following recurrence
relation, which comes equating the (i, j) elements (i < j) in the commutativity
relation F T = T F :
fij = tij

fii − fjj
+
tij − tji

j−1

k=i+1

fik tkj − tik fkj
tii − tjj

(3)

The problem with this approach is that the recurrence breaks in when tii = tjj for
some i = j, that is when T has repeated eigenvalues, and it can give inaccurate
results in ﬂoating point arithmetic when T has close eigenvalues. Parlett observed
that if T = (Tij ) is block upper triangular then F = (Fij ) has the same block
structure and, for i < j,
j−1

Tii Fij − Fij Tjj = Fii Tij − Tij Fjj +

(Fik Tkj − Tik Fkj ) .

(4)

k=i+1

The recurrence can be used to compute F a superdiagonal at a time, provided
we can evaluate the blocks Fii = f (Tii ) and solve the Sylvester equation (4)
for the Fij . For the equation (4) to be nonsingular we need that Tii and Tjj
have no common eigenvalue. For the Sylvester equations to be well conditioned
a necessary condition is that the eigenvalues of Tii and Tjj are well separated.
An alternative approach is ﬁrst to compute
A = XDX −1
where X is well conditioned and D is block diagonal. Then
f (A) = Xf (D)X −1
and the problem reduces to compute f (D). The usual way to compute a block
diagonalization is ﬁrst to compute the Schur form and then to eliminate oﬀdiagonal blocks by solving Sylvester equations (see [6]). In order to gurantee a
well conditioned matrix X a bound must be imposed on the condition of the
individual transformations, this bound will be a parameter of the algorithm
(see [4]).
Computing f (D) reduces to compute f (Dii ) for each diagonal block Dii .
The Dii are triangular but, unlike the Schur method, no particular eigenvalue
distribution is guaranteed, because of limitations on the condition of the transformations; therefore f (Dii ) is still a nontrivial calculation.

3

Partial Fractions Expansions

A diﬀerent approach in the computation of the matrix function f (A) is to use a
rational approximation, Φm,n (z):
Φm,n (z) =

Rm (z)
Qn (z)

(5)

Schur Decomposition Methods

711

where Rm (z) and Qn (z) are polynomials of degree m and n respectively. Usually function Φm,n (z) could be the (m, n) Pad´e approximation of the exponential
function or the Chebyshev approximation but also another approaches are possible. The problem becomes the computation of the vector Φm,n (A)y. Supposing
all the roots of polynomial Qn (z) to be distinct and n ≥ m, the rational function
Φm,n (z) can be represented using the partial fractions expansion:
n

αj
Rm (z)
= α0 +
Qn (z)
z − zj
j=1

(6)

where
Rm (z)
z→∞ Qn (z)

α0 = lim
and
αj =

Rm (z)
Qn (z)

(7)

where zj are the roots of Qn (z). Hence
−1

Φm,n (A) = Rm (A) [Qn (z)]
and, from (6):
n

αj (A − zj I)−1 x.

Φm,n (A)x = α0 x +
j=1

The vector y can be computed using the following algorithm:
1. Compute
α0 = lim

z→∞

Rm (z)
,
Qn (z)

αj =

Rm (z)
Qn (z)

for j = 1, . . . , n.
2. For j = 1, . . . , n, compute the vector xj solving
(A − zj I)xj = b
3. Set

n

y = α0 b +

αj xj .
j=1

The main problem is the solution of the linear systems at step 2. A good choice
for this solution is to use Krylov-subspace iterations, since they are invariant
under the shifts zj . Hence the work required to solve n linear systems is the same
to solve one system. Another advantage of this technique is that it can easily
implemented on a parallel architecture since the linear systems to be solved in
step 2 are independent (see [2]). Moreover there is a second level parallelism
since also each system could be solved using a parallel algorithm. As last remark

712

T. Politi and M. Popolizio

we consider that, when the rational function has poles and residuals as complex
conjugate pairs, it is possible to exploit the following property (see [12]):
αj (A − zj I)−1 xj + αj (A − z j I)−1 xj = 2

αj (A − zj I)−1 xj .

(8)

In [3] it is shown that the algorithm based on partial fractions representation
can be very sensitive to perturbations. In fact the coeﬃcients (7) are
Rm (zj )

αj =

.

n

(zj − zk )

a0
k=1,k=j

and it follows that the presence of closed poles zk may cause some coeﬃcient
αj to be very large. In this case the error on the computation of vector xj
is ampliﬁed. In ([3]) these diﬃculties are remedied using an incomplete partial
fraction representation of the rational function. The representation consists in
writing the function as
t
rl (z)
Rm (z)
=
(9)
Qn (z)
ql (z)
k=1

and using a partial fraction representation for each of the factors rl (z)/ql (z).
The functions rl (z) and ql (z) are polynomials such that the degree of ql (z) is
not greater than the degree of pl (z) for each l. The polynomials rl (z) and ql (z)
are chosen so that the partial fraction coeﬃcient αlj of the representation
n

l
rl (z)
αjl
= α0l +
ql (z)
z − zjl
j=1

where nl is the degree of ql (z), and j = 1, . . . , nl , and 1 ≤ l ≤ t, are not so
large and the number of factors t is small. The incomplete partial representation
algorithm can be described through the following steps:
1. Given the sets zjl and αjl with j = 1, . . . , nl , and l = 1, . . . , t;
2. Put x = b;
3. For l = 1, . . . , t:
3.1 Solve the systems
(A − zjl I)xj = x,

j = 1, . . . , nl

3.2 Compute the vector
nl

x = α0l x +

αjl xj .
j=1

The methods described previously could be used together computing ﬁrst the
Schur decomposition of matrix A:
A = QT Q∗

Schur Decomposition Methods

713

and then using the partial fractions expansions of the rational approximation
of function f (z) applied to matrix T . Hence given the vector x we have the
approximation
f (A)x QΦm,n (T )y
where y = Q∗ x.

4

Numerical Examples

In this section we show an example of the application of the algorithm described
in the previous section to the computation of the exponential matrix applied to
a real vector. Given the unsymmetric matrix A ∈ IRn×n and the vector x ∈ IRn
our aim is to approximate the vector eA x. We recall that the method is based
on the following steps:
1. Computation of the real Schur decomposition of the real matrix A = QT QT ;
2. Computation of the Chebyshev rational approximation for the exponential
function Cn,n (z);
3. Computation of the poles zj and the residuals αj of rational function Cn,n (z);
4. Solve the linear systems (T − zj I)xj = y, where y = QT x;
5. Compute the vector
n

w = α0 y +

αj xj ;
j=1

6. Compute the vector z = Qw.

1

10

0

10

−1

−2

10

−3

10

n

||Φ (A)x−expm(A)x||

2

10

−4

10

−5

10

−6

10

2

4

6

8
n

10

Fig. 1. Estimate of the error

12

14

714

T. Politi and M. Popolizio
5

10

4

10

3

j

max(abs(α ))

10

2

10

1

10

0

10

−1

10

0

5

10

15
n

20

25

30

Fig. 2. Maximum Residuals of the Partial Fractions Expansion for Chebyshev Approximations of degree n

4.5

4

3.5

||Φn(A)x||2

3

2.5

2

1.5

1

0.5

2

3

4

5

6

7
n

8

9

10

11

12

Fig. 3. Error respect to unitary vector solution

It is obvious that the steps 2 and 3 of the algorithm do not require any computation since the coeﬃcients of the rational function Cn,n (z), the poles and the
residuals are tabbed. For the solution of these systems here we have considered
just direct methods neglecting their shifted structure, in future we shall consider the application of iterative methods based on Krylov subspaces. A further

Schur Decomposition Methods

715

remark is related to the use of Chebyshev approximation in step 2: we recall that
using the property (8) the number of linear systems can be reduced. In Figure 3
we show the diﬀerence between the vector computed by MatLab function expm
times a random vector x taking a random matrix A ∈ IR20×20 and the vector
computed by the algorithm described in the present section. We observe that
taking the result computed by MatLab routine as correct the error decreases
when the degree of the Chebyshev approximation is growing until n = 10, then
the error increases. The reason of this behaviour is explained in Figure 3 where
we show that the maximum of the residuals |αj | increases with n, and as observed in the Section 3 the error in the solution of the linear systems is ampliﬁed.
Finally we have considered a random skew-symmetric matrix A and a unitary
2-norm vector x. In this case also the vector eA x has unitary 2-norm. We observe
that the behaviour of the error is exactly the same observed in the ﬁrst example.

References
˚strom K.J. , Wittenmark B.: Computer-Controlled Systems: Theory and Design.
1. A
Prentice-Hall, Englewoods Ciﬀs, NJ, (1997)
2. Baldwin C., Freund R.W., Gallopoulos E.: A Parallel Iterative Method for Exponential Propagation. Proceedings of the Seventh SIAM Conference on Parallel
Processing for Scientiﬁc Computing. D.H. Bailey et al. ed. SIAM (1995) 534–539
3. Calvetti D., Gallopoulos E., Reichel L.: Incomplete Partial Fractions for Parallel
Evaluation of Rational Matrix Functions. J. Comp. Appl. Math. 59 (1995) 349–380
4. Davies P.J., Higham N.J.: A Schur-Parlett Algorithm for Computing Matrix Functions. SIAM J. Matr. Anal. Appl. 25 (2) (2003) 464–485
5. Del Buono N., Lopez L., Politi T.: Computation of functions of Hamiltonian and
skew-symmetric matrices. Preprint (2006)
6. Golub G.H., Van Loan C.F.: Matrix Computation. The John Hopkins Univ. Press,
Baltimore, (1996)
7. Higham N.J.: Functions of Matrices. MIMS EPrint 2005.21 The University of
Manchester
8. Iserles A., Munthe-Kaas H., Nørsett S., Zanna A.: Lie-Group Methods. Acta Numerica 9 (2000) 215–365
9. Lopez L., Simoncini V.: Analysis of projection methods for rational function approximation to the matrix exponential. SIAM J. Numer. Anal. (to appear)
10. Parlett B.N.: A Recurrence among the Elements of Functions of Triangular Matrices. Lin. Alg. Appl. 14 (1976) 117–121
11. Saad Y.: Analysis of some Krylov subspace approximation to the matrix exponential operator, SIAM J. Numer. Anal. 29 (1) (1992) 209–228
12. Schmelzer T.: Rational approximations in scientiﬁc computing. Computing Laboratory, Oxford University, U.K. (2005)

