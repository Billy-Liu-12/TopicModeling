Quasi-Gaussian Particle Filtering*
Yuanxin Wu, Dewen Hu, Meiping Wu, and Xiaoping Hu
Department of Automatic Control, College of Mechatronics and Automation,
National University of Defense Technology, Changsha, Hunan, P.R. China, 410073
yuanx_wu@hotmail.com

Abstract. The recently-raised Gaussian particle filtering (GPF) introduced the
idea of Bayesian sampling into Gaussian filters. This note proposes to generalize
the GPF by further relaxing the Gaussian restriction on the prior probability.
Allowing the non-Gaussianity of the prior probability, the generalized GPF is
provably superior to the original one. Numerical results show that better
performance is obtained with considerably reduced computational burden.

1 Introduction
The Bayesian probabilistic inference provides an optimal solution framework for
dynamic state estimation problems [1, 2]. The Bayesian solution requires propagating
the full probability density function, so in general the optimal nonlinear filtering is
analytically intractable. Approximations are therefore necessary, e.g., Gaussian
approximation to the probability [3-9]. This class of filters is commonly called as the
Gaussian filters, in which the probability of interest, e.g. the prior and posterior
probabilities, are approximated by Gaussian distribution. An exception is the so-called
augmented unscented Kalman filter [10] where the prior probability is encoded by the
nonlinearly transformed deterministic sigma points instead of by the calculated mean
and covariance from them. By so doing, the odd-order moment information is captured
and propagated throughout the filtering recursion, which helps improve the estimation
accuracy. This note will show that similar idea can be applied to the recently-raised
Gaussian particle filtering (GPF) [7].
The GPF was developed using the idea of Bayesian sampling under the Gaussian
assumption [7]. It actually extends the conventionally analytical Gaussian filters via
Monte Carlo integration and the Bayesian update rule [11]. The Gaussian assumption
being valid, the GPF is asymptotically optimal in the number of random samples,
which means that equipped with the computational ability to handle a large number of
samples the GPF is supposed to outperform any analytical Gaussian filter. The GPF
also have a lower numerical complexity than particle filters [7].
This work generalizes the GPF by relaxing the assumption of the prior probability
being Gaussian. Since the prior probability is allowed to be non-Gaussian, the resulting
filter is named as the quasi-Gaussian particle filtering (qGPF) in the sequel. It turns out
*

Supported in part by National Natural Science Foundation of China (60374006, 60234030 and
30370416), Distinguished Young Scholars Fund of China (60225015), and Ministry of
Education of China (TRAPOYT Project).

V.N. Alexandrov et al. (Eds.): ICCS 2006, Part I, LNCS 3991, pp. 689 – 696, 2006.
© Springer-Verlag Berlin Heidelberg 2006

690

Y. Wu et al.

that the qGPF outperforms the GPF in both accuracy and computational burden. The
contents are organized as follows. Beginning with the general Bayesian inference,
Section II derives and outlines the qGPF algorithm. Section III examines two
representative examples and the conclusions are drawn in Section IV.

2 Quasi-Gaussian Particle Filtering
Consider a discrete-time nonlinear system written in the form of dynamic state space
model as
xk = f k −1 ( xk −1 , wk −1 )
(1)
yk = hk ( xk , vk )

where
hk :

the
n

×

s

process
→

m

function

fk :

n

×

r

→

n

and

observation

function

are some known functions. The process noise wk ∈

r

is

uncorrelated with the past and current system states; the measurement noise vk ∈
is
uncorrelated with the system state and the process noise at all time instants. The
probabilities of the process and measurement noises are both assumed to be known.
Denote by y1: k { y1 ,… , yk } the observations up to time instant k . The purpose of
s

filtering is to recursively estimate the posterior probability p ( xk | y1: k ) conditioned on

all currently available but noisy observations. The initial probability of the state is
assumed to be p ( x0 | y1:0 ) ≡ p ( x0 ) . The prior probability is obtained via the
Chapman-Kolmogorov equation

p ( xk | y1: k −1 ) = ∫ n p ( xk | xk −1 ) p ( xk −1 | y1: k −1 ) dxk −1 .

(2)

The transition probability density p ( xk | xk −1 ) is uniquely determined by the known
process function and the process noise probability. Using the Bayesian rule, the posterior
probability is given by

p ( xk | y1: k ) =

p ( yk | xk ) p ( xk | y1: k −1 )
p ( yk | y1: k −1 )

(3)

where the normalizing constant
p ( yk | y1: k −1 ) = ∫ n p ( yk | xk ) p ( xk | y1: k −1 ) dxk .

(4)

The likelihood probability density p ( yk | xk ) is uniquely determined by the known
observation function and the measurement noise probability. Equations (2)-(4) constitute
the foundation of the optimal Bayesian probabilistic inference. Unfortunately, the exact
analytic form only exists for a couple of special cases, e.g., when the system (1) is linear
and Gaussian. In order to make the filtering problem tractable, approximation must be
made.

Quasi-Gaussian Particle Filtering

691

Next, we start to derive the qGPF by assuming the posterior probability at time instant
k − 1 to be well approximated by a Gaussian distribution, i.e.,
p ( xk −1 | y1: k −1 ) ≈ N ( xk −1 ; mk −1 , Pk −1 )

(5)

Substituting (5) and using the Monte-Carlo integration [12], the prior probability in (2) is

p ( xk | y1: k −1 ) ≈ ∫ n p ( xk | xk −1 ) N ( xk −1 ; mk −1 , Pk −1 ) dxk −1
M1

= ∫ n p ( xk | xk −1 ) ∑
i =1

(6)
1
1 M1
δ ( xk −1 − xki −1 ) dxk −1 =
p ( xk | xki −1 )
∑
M1
M 1 i =1

where xki −1 are random samples from the assumed posterior probability at time instant

k − 1 , i.e., N ( xki −1 ; mk −1 , Pk −1 ) , i = 1,…, M 1 . The idea of the importance sampling [13,

14] is crucial to numerically implement the Bayesian rule, through which the prior
probability is updated to yield the posterior probability using the information provided by
the newcome observation. In view of the difficulty of drawing samples directly from the
posterior probability p ( xk | y1: k ) , the importance sampling proposes to sample from a

choice importance density q ( xk | y1: k ) instead, from which random samples can be

readily generated. With (6), the posterior probability in (3) is rewritten as
M1

p ( xk | y1: k ) ∝ p ( yk | xk ) p ( xk | y1: k −1 ) ∝

M2

∝∑

p ( yk | x

j =1

i =1

q ( xk | y1: k )

M1

)∑ p(x
q(x | y )
j
k

p ( yk | xk ) ∑ p ( xk | xki −1 )

i =1
j
k

j
k

i
k −1

|x

)

δ ( xk − xkj )

1: k

M2

q ( xk | y1: k )
(7)

∑ wˆ δ ( x
j =1

j
k

k

− xkj )

where xkj are random samples from the importance density q ( xk | y1: k ) and
M1

wˆ kj =

p ( yk | xkj ) ∑ p ( xkj | xki −1 )
i =1

,

q ( xkj | y1: k )

j = 1,… , M 2 .

(8)

Considering the normalization condition, the posterior probability at time instant k is
approximated by
M2

p ( xk | y1: k ) = ∑ wkj δ ( xk − xkj )

(9)

j =1

where
wkj = wˆ kj

M2

∑ wˆ
j =1

j
k

.

(10)

692

Y. Wu et al.
Table 1. Quasi-Gaussian Particle Filtering

1.

Draw samples from the posterior probability at time instant k − 1 , i.e.,

2.

Draw samples from the important density, that is, xkj ∼ q ( xk | y1: k ) ,

xki −1 ∼ p ( xk −1 | y1: k −1 ) ≈ N ( xk −1 ; mk −1 , Pk −1 ) , i = 1,… , M 1 ;
j = 1,… , M 2 ;

3.
4.

Assign each sample xkj a weight wkj according to (8) and (10);
Calculate the mean and covariance according to (11),
p ( xk | y1: k ) ≈ N ( xk ; mk , Pk ) .

then

Then approximate the posterior probability at time instant k by N ( xk ; mk , Pk ) in which
M2

M2

j =1

j =1

mk = ∑ wkj xkj , Pk = ∑ wkj ( xkj − mk )( xkj − mk ) .
T

(11)

This ends the derivation and the resulting algorithm is summarized and outlined in
Table I. It is clear from above that we only assume the posterior probability to be
Gaussian while do not impose any restriction on the prior probability, which is the major
difference from the GPF. Recall that the GPF approximates both the prior and posterior
probabilities by Gaussian densities. To be more specific, the GPF approximates the
discrete representation of the prior probability p ( xk | y1: k −1 ) by a Gaussian density, from

which random samples are regenerated to be weighted by the likelihood p ( yk | xk ) ,

while the qGPF directly employs the discrete representation of the prior probability. This
resembles the difference between the non-augmented UKF and augmented UKF [10]. By
a peer-to-peer comparison between the qGPF and the GPF ([7], Table I), we see that the
qGPF needs not to calculate the sample mean and covariance for the assumed Gaussian
prior probability and thus has lower numerical complexity.
The following theorem says that the mean and covariance in (11) converge almost
surely to the true values under the condition that the posterior probability at time instant
k − 1 is well approximated by a Gaussian distribution.
Theorem: If the posterior probability p ( xk −1 | y1: k −1 ) is a Gaussian distribution, then the

posterior probability expressed in (9) converges almost surely to the true posterior
probability p ( xk | y1: k ) .
Proof: it is a very straightforward extension of Theorem 1 in [7] and thus omitted here.
It follows as a natural corollary that the mean and covariance in (11) converge almost
surely to the true value. Therefore the qGPF is provably better than the GPF in accuracy
because the former takes the non-Gaussianity of the prior probability into consideration.
Note that the non-Gaussianity of the prior probability is not uncommon for
nonlinear/non-Gaussian systems.

Quasi-Gaussian Particle Filtering

693

In theory, we could assume the posterior probability to be any other distribution, as
long as the samples from the distribution were easily obtained, e.g. mixed Gaussian
[15-17]. The derivation procedure and theoretical proof would be analogical to the
above.

3 Numerical Results
This section examines the qGPF via the univariate nonstationary growth model and
bearing only tracking, which have been extensively investigated in the literature [2, 7, 18,
19]. We also carried out the GPF for comparison. The prior probability was selected as
the importance density for both filters, i.e., q ( xk | y1: k ) = p ( xk | y1: k −1 ) .
Univariate Nonstationary Growth Model
The model is formulated as
xk = f k −1 ( xk −1 , k ) + wk −1

yk = hk ( xk ) + vk , k = 1,… , N

where f k −1 ( xk −1 , k ) = 0.5 xk −1 + 25

(12)

xk −1
+ 8cos (1.2 ( k − 1) ) , hk ( xk ) = xk2 20 . The
1 + xk2−1

process noise wk −1 and measurement noise vk are zero-mean Gaussian with variances
Qk −1 and Rk , respectively. In our simulation, Qk −1 = 10 and Rk = 1 . This model has
significant nonlinearity and is bimodal in nature depending on the sign of observations.
The reference data were generated using x0 = 0.1 and N = 100 . The initial probability
p ( x0 ) ∼ N ( 0,1) .

The mean square error (MSE) averaged across all time instants defined as

MSE = ∑ ( xk − xk |k )
N

2

N is used to quantitatively evaluate each filter. We carried out

k =1

50 Monte Carlo runs for M 1 = M 2 = 20,50,100, 200, 400 , respectively. Figure 1 shows
MSEs as a function of the number of samples. The qGPF remarkably outperforms the
GPF. With the same number of samples, MSE of the qGPF is less than half of that of the
GPF; on the other hand, to achieve comparable performance the GPF needs at least as
twice samples as the qGPF does. The average running time of the qGPF is about 20
percent less than that of the GPF.
Bearing Only Tracking
The target moves within the s − t plane according to the standard second-order model

xk = Φxk −1 + Γwk −1 ,
where xk = [ s, s, t , t ]k , wk = [ ws , wt ]k ,
T

T

k = 1,… , N

(13)

694

Y. Wu et al.

⎡1
⎢0
Φ=⎢
⎢0
⎢
⎣0

1 0 0⎤
⎡0.5
⎥
⎢1
1 0 0⎥
and Γ = ⎢
⎢0
0 1 1⎥
⎥
⎢
0 0 1⎦
⎣0

0 ⎤
0 ⎥⎥
.
0.5⎥
⎥
1 ⎦

Here s and t denote Cartesian coordinates of the moving target. The system noise
wk ∼ N ( 0, QI 2 ) . A fixed observer at the origin of the plane takes noisy measurements of
the target bearing
110
GPF
qGPF

100
90

MSE

80
70
60
50
40
30
20

0

50

100

150
200
250
Number of samples

300

350

400

Fig. 1. MSE as a function of the number of samples for both filters
Mean of MSEs
0

-1

-2

-3

-4

-5
GPF
qGPF
-6

1

2

3

4

Fig. 2. Averaged MSE of all four coordinates (logarithmic in y axis)

Quasi-Gaussian Particle Filtering

yk = arctan ( tk sk ) + vk

695

(14)

where the measurement noise vk ∼ N ( 0, R ) . The reference data were generated using

Q = 0.0012 , R = 0.0052 and N = 24 . The initial true state of the system was
x0 = [−0.05, 0.001, 0.7, − 0.055]T and the initial estimate was x0|0 = x0 with covariance

(

P0|0 = diag ⎡⎣0.12 , 0.0052 , 0.12 , 0.012 ⎤⎦

T

).

We carried out 100 random runs for M 1 = M 2 = 1000 and the averaged MSEs of all
four coordinates are given in Fig. 2. We see that the qGPF is smaller in MSE, though
marginally, than the GPF. Similar observations were obtained for various number of
samples and is omitted here for brevity. In the simulation, eighteen percent of
computational time was spared by using the qGPF.

4 Conclusions
This note proposes the qGPF filter that generalizes the GPF by allowing the prior
probability to be non-Gaussian. It has provable superiority over the GPF. The
numerical results show that the qGPF achieves (sometimes remarkably) better
improvement in estimation accuracy with lower numerical complexity than the GPF.
Theoretically, the posterior probability could be assumed to be any other distribution as
long as it was readily sampled, e.g., mixed Gaussian. In such a case, it is promising for
the qGPF to be used to construct more superior filter than the GPF-based Gaussian sum
particle filter in [17].

References
[1]
[2]

[3]
[4]
[5]

[6]
[7]
[8]

Y. C. Ho and R. C. K. Lee, "A Bayesian approach to problems in stochastic estimation and
control," IEEE Transactions on Automatic Control, vol. AC-9, pp. 333-339, 1964.
M. S. Arulampalam, S. Maskell, N. Gordon, and T. Clapp, "A tutorial on particle filters for
online nonlinear/non-Gaussian Bayesian tracking," IEEE Transactions on Signal
Processing, vol. 50, no. 2, pp. 174-188, 2002.
K. Ito and K. Q. Xiong, "Gaussian filters for nonlinear filtering problems," IEEE
Transactions on Automatic Control, vol. 45, no. 5, pp. 910-927, 2000.
A. H. Jazwinski, Stochastic Processing and Filtering Theory. New York and London:
Academic Press, 1970.
S. J. Julier and J. K. Uhlmann, "A new extension of the Kalman filter to nonlinear
systems," in Signal Processing, Sensor Fusion, and Target Recognition VI, vol. 3068,
Proceedings of the Society of Photo-Optical Instrumentation Engineers (SPIE), 1997, pp.
182-193.
R. E. Kalman, "A new approach to linear filtering and prediction problems," Transactions
of the ASME, Journal of Basic Engineering, vol. 82, pp. 34-45, 1960.
J. H. Kotecha and P. A. Djuric, "Gaussian particle filtering," IEEE Transactions on Signal
Processing, vol. 51, no. 10, pp. 2592-2601, 2003.
M. Norgaard, N. K. Poulsen, and O. Ravn, "New developments in state estimation for
nonlinear systems," Automatica, vol. 36, no. 11, pp. 1627-1638, 2000.

696
[9]
[10]

[11]
[12]
[13]

[14]
[15]
[16]
[17]
[18]
[19]

Y. Wu et al.
Y. Wu, D. Hu, M. Wu, and X. Hu, "A Numerical-Integration Perspective on Gaussian
Filters," IEEE Transactions on Signal Processing, to appear, 2005.
Y. Wu, D. Hu, M. Wu, and X. Hu, "Unscented Kalman Filtering for Additive Noise Case:
Augmented versus Non-augmented," IEEE Signal Processing Letters, vol. 12, no. 5, pp.
357-360, 2005.
Y. Wu, X. Hu, D. Hu, and M. Wu, "Comments on "Gaussian particle filtering"," IEEE
Transactions on Signal Processing, vol. 53, no. 8, pp. 3350-3351, 2005.
P. J. Davis and P. Rabinowitz, Methods of Numerical Integration: New York, Academic
Press, 1975.
A. F. M. Smith and A. E. Gelfand, "Bayesian statistics without tears: a
sampling-resamping perspective," The American Statistician, vol. 46, no. 2, pp. 84-88,
1992.
R. Y. Rubinstein, Simulation and the Monte Carlo Method. New York: Wiley, 1981.
H. W. Sorenson and D. L. Alspach, "Recursive Bayesian estimation using Gaussian
sums," Automatica, vol. 7, pp. 465-479, 1971.
D. L. Alspach and H. W. Sorenso, "Nonlinear Bayesian estimation using Gaussian sum
approximation," IEEE Transactions on Automatic Control, vol. 17, pp. 439-448, 1972.
J. H. Kotecha and P. M. Djuric, "Gaussian sum particle filtering," IEEE Transactions on
Signal Processing, vol. 51, no. 10, pp. 2602-2612, 2003.
G. Kitagawa, "Non-Gaussian state-space modeling of nonstationary time-series," Journal
of the American Statistical Association, vol. 82, no. 400, pp. 1032-1063, 1987.
N. J. Gordon, D. J. Salmond, and A. F. Smith, "Novel approach to nonlinear/non-Gaussian
Bayesian state estimation," IEE Proceedings-F, vol. 140, no. 2, pp. 107-113, 1993.

