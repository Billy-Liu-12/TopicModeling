An Approach to Buﬀer Management in Java
HPC Messaging
Mark Baker, Bryan Carpenter, and Aamir Shaﬁ
Distributed Systems Group, University of Portsmouth

Abstract. One of the most challenging aspects to designing a Java messaging system for HPC is the intermediate buﬀering layer. The lower and
higher levels of the messaging software use this buﬀering layer to write
and read messages. The Java New I/O package adds the concept of direct
buﬀers, which—coupled with a memory management algorithm—opens
the possibility of eﬃciently implementing this buﬀering layer. In this
paper, we present our buﬀering strategy, which is developed to support
eﬃcient communications and derived datatypes in MPJ Express—our
implementation of the Java MPI API. We evaluate the performance of
our buﬀering layer and demonstrate the usefulness of direct byte buﬀers.

1

Introduction

The challenges of making parallel hardware usable have, over the years, stimulated the introduction of many novel languages, language extensions, and programming tools. Lately, though, practical parallel computing has mostly adopted
conventional (sequential) languages, with programs developed in relatively conventional programming environments usually supplemented by libraries like MPI
that support parallel programming. This is largely a matter of economics:
creating entirely novel development environments matching the standards programmers expect today is expensive, and contemporary parallel architectures
predominately use commodity microprocessors that can best be exploited by
oﬀ-the-shelf compilers.
This argues that if we want to “raise the level” of parallel programming,
one practical approach is to move towards advanced commodity languages.
Compared with C or Fortran, the advantages of the Java programming language include higher-level programming concepts, improved compile-time and
run-time checking, and as a result, faster problem detection and debugging.
Also, it supports multi-threading and provides simple primitives like wait()
and notify() that can be used to synchronize access to shared resources. Recent JDKs (Java Development Kits) provide greater functionality in this area, including semaphores and atomic variables. In addition, Java’s automatic garbage
collection, when exploited carefully, relieves the programmer of many of the
pitfalls of lower-level languages.
We have developed MPJ Express (MPJE) [6], a thread-safe implementation
of Java MPI API. A challenging aspect of implementing Java HPC messaging
V.N. Alexandrov et al. (Eds.): ICCS 2006, Part II, LNCS 3992, pp. 953–960, 2006.
c Springer-Verlag Berlin Heidelberg 2006

954

M. Baker, B. Carpenter, and A. Shaﬁ

software is providing an eﬃcient intermediate buﬀering layer. The low-level communication devices and higher levels of the messaging software use this buﬀering
layer to write and read messages. The heterogeneity of these low-level communication devices poses additional design challenges. To appreciate this fully, assume
that the user of a messaging library sends ten elements of an integer array. The
C programming language can retrieve the memory address of this array and
pass it to the underlying communication device. If the communication device is
based on TCP, it can then pass this address to the socket’s write method. For
proprietary networks, like Myrinet [7], this memory region can be registered for
Direct Memory Access (DMA) transfers, or copied to a DMA capable part of
memory and sent using low level Myrinet communication methods. Until quite
recently doing this kind of thing in Java was hard.
JDK 1.4 introduced Java New I/O (NIO) [8]. In NIO, read and write methods on ﬁles and sockets (for example) are mediated through a family of buﬀer
classes handled specially by the Java Virtual Machine (JVM). The underlying
ByteBuffer class essentially implements an array of bytes, but in such a way
that the storage can be outside the JVM heap (so called direct byte buﬀers).
So now if a user of a Java messaging system sends an array of ten integers, they can be copied to a ByteBuffer, which is used as an argument to the
SocketChannel write method. For proprietary networks like Myrinet, NIO provides a viable option because it is now possible to get memory addresses of direct
byte buﬀers, which can be used to register memory regions for DMA transfers.
Using direct buﬀers may eliminate the overhead [9] incurred by additional copying when using Java Native Interface (JNI) [4]. On the other hand, it may be
preferable to create a native buﬀer using the JNI. These native buﬀers can be
useful for a native MPI or a proprietary network device.
We are convinced that NIO provides essential ingredients [2] of an eﬃcient
messaging system via non-blocking I/O and direct buﬀers.
Based on these factors, we have designed an extensible buﬀering layer that
allows various implementations based on diﬀerent storage mediums like direct or
indirect ByteBuffers, byte arrays, or memory allocated in the native C code. The
higher levels of MPJE use the buﬀering layer through an interface. This implies
that functionality is not tightly coupled to the storage medium. The motivation
behind developing diﬀerent implementations of buﬀers is to achieve optimal performance for lower level communication devices. Our buﬀering strategy uses a
pooling mechanism to avoid creating a buﬀer instance for each communication
method. The creation time of these buﬀers can aﬀect overall communication
time, especially for large messages. Our current implementation is based on
Knuth’s buddy algorithm [5], but it is possible to use other pooling techniques.
The main contribution of this paper is the design and implementation of
our buﬀering layer for HPC supported by two diﬀerent pooling mechanisms. In
addition, we have evaluated the performance of these two pooling mechanisms.
We show that one of them is faster with a smaller memory footprint. Also, we
demonstrate the usefulness of direct byte buﬀers in Java messaging systems.

An Approach to Buﬀer Management in Java HPC Messaging

955

The remainder of this paper is organized as follows. Section 2 discusses related work. The strategy itself with an explanation of our memory management
algorithms is described in section 3. In section 4, we present performance evaluation of our buﬀering strategies. Section 5 concludes the paper outlining future
research work.

2

Related Work

The most popular Java messaging system is mpiJava [1], which uses a JNI wrapper to the underlying native C MPI library. Being a wrapper library, mpiJava
does not use a clearly distinguished buﬀering layer. After packing a message
onto a contiguous buﬀer, a reference to this buﬀer is passed to the native C
library. But in achieving this, additional copying may be required between the
JVM and the native C library. This overhead is especially noticeable for large
messages.
Javia [3] is a Java interface to the Virtual Interface Architecture (VIA). An
implementation of Javia exposes communication buﬀers used by the VI architecture to Java applications. These communication buﬀers are created outside
the Java heap and can be registered for DMA transfers. This buﬀering technique
makes it possible to achieve performance within 1% of the raw hardware.
An eﬀort similar to Javia is JAGUAR [9]. This uses compiled-code transformations to map certain Java bytecodes to short, in-lined machine code segments.
These two projects, JAGUAR and Javia were the motivating factors to introduce
the concept of direct buﬀers in the NIO package. The design of our buﬀering
layer is based on direct byte buﬀers. In essence, we are applying the experiences gained by JAGUAR and Javia to design a general and eﬃcient buﬀering
layer that can be used for pure Java and proprietary devices in Java messaging
systems alike.

3

The Buﬀering Layer in MPJE

In this section, we discuss our approach to designing and implementing an eﬃcient buﬀering layer supported by a pooling mechanism. The self-contained API
developed as a result is called the MPJ Buﬀering (mpjbuf) API. The functionality provided includes packing and unpacking of user data.
An mpjbuf buﬀer object contains two data storage structures. The ﬁrst is a
static buﬀer, in which the underlying storage primitive is an implementation of
the RawBuffer interface. The implementation of static buﬀer called NIOBuffer
uses direct or indirect ByteBuffers. The second is a dynamic buﬀer where a
byte array is the storage primitive. The size of the static buﬀer is predeﬁned,
and can contain only primitive datatypes. The dynamic buﬀer is used to store
serialized Java objects, where it is not possible to determine the length of the
serialized objects beforehand. The class structure of our package is shown in
Figure 1.

956

M. Baker, B. Carpenter, and A. Shaﬁ
NIOBuffer
RawBuffer
package
mpjbuf

NativeBuffer
Buffer

Fig. 1. Primary Buﬀering Classes in mpjbuf

3.1

Memory Management

We have implemented our own application level memory management mechanism based on a buddy allocation scheme [5]. The motivation is to avoid creating
an instance of a buﬀer (mpjbuf.Buffer) for every communication operations like
Send() or Recv(), which may dominate the total communication cost, especially
for large messages. We can make eﬃcient use of resources by pooling buﬀers for
future reuse instead of letting the garbage collector reclaim the buﬀers and create
them all over again. The functionality provided by the buﬀering API is exported
to the users through a BufferFactory.
In the MPJ buﬀering API it is possible to plug in diﬀerent implementations
of buﬀer pooling. A particular strategy can be speciﬁed during the initialisation of mpjbuf.BufferFactory. Each implementation can use diﬀerent data
structures like trees or doubly linked lists. In the current implementation, the
primary storage buﬀer for mpjbuf is an instance of mpjbuf.NIOBuffer. Each
mpjbuf.NIOBuffer has an instance of ByteBuffer associated with it. The pooling strategy boils down to reusing ByteBuffers encapsulated in NIOBuffer.
Our implementation strategies are able to create smaller thread-safe ByteBuffers from the initial ByteBuffer associated with the region. We achieve this
by using ByteBuffer.slice() for creating new byte buﬀer whose contents are
a shared sub sequence of original buﬀers contents. In the sub-sections to follow,
we discuss two implementations of memory management techniques.
In a buddy algorithm, the region of available storage is conceptually divided
into blocks of diﬀerent levels, hierarchically nested in a binary tree. A free block
at level n can be split into two blocks of level n − 1, half the size. These sibling
blocks are called buddies. To allocate a number of bytes s, a free block is found
and recursively divided into buddies until a block at level log2 (s) is produced.
When a block is freed, one checks to see if its buddy is free. If so, buddies are
merged (recursively) to consolidate free memory.
The First Pooling Strategy. Our ﬁrst implementation (called Buddy1 below)
is developed with the aim of keeping a small memory footprint of the application.
This is possible because a buﬀer only needs to know its oﬀset in order to ﬁnd
its buddy. This oﬀset can be stored at the start of the allocated memory chunk.
Figure 2 outlines the implementation details of our ﬁrst pooling strategy.
FreeList is a list of BufferLists, which contains buﬀers at diﬀerent levels. Here,
level refers to the diﬀerent sizes of buﬀer available. If a buﬀer is of size s, then
its corresponding level will be log2 (s) . Initially, there is no region associated
with FreeList. An initial chunk of memory of size M is allocated. At this point,

An Approach to Buﬀer Management in Java HPC Messaging

957

FreeList

B
u
f
f
e
r
L
i
s
t
s

0

1

2

N

R1

R2

R1

RN

R2

R1

RN

R2

R2

RN

R1

R2

Region
1

Region
2

Region
N

Fig. 2. The First Implementation of Buﬀer Pooling

BufferLists are created starting from 0 to log2 (M ). When buddies are merged,
a buﬀer is added to the BufferList at the higher level and the buﬀer itself
and its buddy are removed from the BufferList at the lower level. Conversely,
when a buﬀer is divided to form a pair of buddies, a newly created buﬀer and
its buddy is added to the BufferList at the lower level while removing a buﬀer
that is divided from the higher level BufferList. An interesting aspect of this
implementation is that FreeList and BufferLists are independent of a region
and these lists grow as new regions are created to match user requests.
The Second Pooling Strategy. Our second implementation (called Buddy2
below) stores higher-level buﬀer abstractions (NIOBuffer) in BufferLists. Unlike the ﬁrst strategy, each region has its own FreeList and has a pointer to
the next region as shown in Figure 3. While ﬁnding an appropriate buﬀer for a
user, this implementation starts sequentially starting from the ﬁrst region until
it ﬁnds the requested buﬀer or creates a new region. We expect some overhead
associated with this sequential search. Another downside for this implementation
is a bigger memory footprint.

B
u
f
f
e
r
L
i
s
t
s

0

Region 1

Region 2

FreeList

FreeList

1

2

N

B
u
f
f
e
r
L
i
s
t
s

0

1

2

Region N
FreeList

N

B
u
f
f
e
r
L
i
s
t
s

0

1

Fig. 3. The Second Implementation of Buﬀer Pooling

2

N

958

4

M. Baker, B. Carpenter, and A. Shaﬁ

Buﬀering Layers Performance Evaluation

In this section, we compare the performance of our two buﬀering strategies with
direct allocation of ByteBuffers. Also, we are interested in exploring the performance diﬀerence between using direct byte buﬀers and indirect byte buﬀers
in MPJE communication methods. There are six combinations of our buﬀering strategies that will be compared in our ﬁrst test—Buddy1, Buddy2, and
simple-minded allocation all using direct and indirect byte buﬀers.
4.1

Simple Allocation Time Comparison

In our ﬁrst test, we are interested in comparing isolated allocation times for
a buﬀer for our six allocation approaches. Only one buﬀer is allocated at one
time throughout the tests. This means that after measuring allocation time for
a buﬀer, it is de-allocated in the case of our buddy schemes (forcing buddies to
merge into original region chunk of 8 Mbytes before the next allocation occurs),
or the reference is freed in the case of straightforward ByteBuffer allocation.
Figure 4 shows a comparison of allocation times. The ﬁrst thing to note is, that
all the buddy-based schemes are dramatically better than relying on the JVMs
management of ByteBuffer. This essentially means that without a buﬀer pooling
mechanism, creation of intermediate buﬀers for sending or receiving messages in
a Java messaging system can have detrimental eﬀect on the performance. Results
are averaged over many repeats, and the overhead of garbage collection cycles
will be included in the results in an averaged sense; this is a fair representation
of what will happen in a real application. In a general way we attribute the
dramatic increase in average allocation time for large ByteBuffers as due to
forcing proportionately many garbage collection cycles. All the buddy variants
(by design) avoid this overhead. The allocation times for buddy based schemes
decrease for larger buﬀer sizes because less time is spent in traversing the data
structures to ﬁnd an appropriately sized buﬀer. The size of the initial region is
8 Mbytes—resulting in the least allocation time for this buﬀer size. The best
strategy in almost all cases is Buddy1 using direct buﬀers.
Qualitative measurements of memory footprint suggest the current implementation of Buddy2 also has about a 20% bigger footprint because of the extra
objects stored.
In its current state of development, Buddy2 is clearly outperformed by
Buddy1. But there are good reasons to believe that with further development,
a variant of Buddy2 could be faster than Buddy1. This is future work.
4.2

Incorporating Buﬀering Strategies into MPJE

In this test, we compare throughput measured by a simple ping-pong benchmark
using each of the diﬀerent buﬀering strategies. These tests were performed on
Fast Ethernet. The reason for performing this test is to see if there are any
performance beneﬁts for using direct ByteBuffers. From the source-code of the
NIO package, it appears that the JVM maintains a pool of direct ByteBuﬀers

An Approach to Buﬀer Management in Java HPC Messaging

959

Allocation Time Comparison
Buddy1 with direct ByteBuffer
Buddy1 with indirect ByteBuffer
Buddy2 with direct ByteBuffer
ByteBuffer.allocateDirect()
ByteBuffer.allocate()
Buddy2 with indirect ByteBuffer

Allocation Time (us)

104

103
500
300
200
125

20
10
5
4
3
2
1
128

256

512

1K

2K

4K

8K

16K 32K 64K 128K 256K 512K 1M
Buffer Size (Bytes)

2M

4M

8M

16M

Fig. 4. Allocation Time Comparison
Throughput on Fast Ethernet
Buddy1 using direct
Buddy1 using indirect
Buddy2 using direct
Buddy2 using indirect

80

ByteBuffer
ByteBuffer
ByteBuffer
ByteBuffer

70

Bandwidth ( Mbps)

60

50

40

30

20

10

1

2

4

8

16

32

64 128 256 512 1K 2K 4K 8K 16K 32K 64K128K
256K
512K 1M 2M 4M 8M 16M
Message Length (Bytes)

Fig. 5. Throughput Comparison

for internal purposes. These buﬀers are used for reading and writing messages
into the socket. A user provides an argument to SocketChannels write or read
method. If this buﬀer is direct, it is used for writing or reading messages. If this
buﬀer is indirect, a direct byte buﬀer is acquired from direct byte buﬀer pool
and the message is copied ﬁrst before writing or reading it into the socket. Thus,
we expect to see an overhead of this additional copying for indirect buﬀers.
Figure 5 shows that MPJE achieves maximum throughput when using direct
buﬀer in combination with either of the buddy implementations. We expect to
see this performance overhead related to indirect buﬀers to be more signiﬁcant

960

M. Baker, B. Carpenter, and A. Shaﬁ

for faster networks like Gigabit Ethernet and Myrinet. The drop in throughput
at 128Kbytes message size is because of the change in communication protocol
from eager send to rendezvous.

5

Conclusions and Future Work

In this paper, we have discussed the design and implementation of our buﬀering
layer, which uses our own implementation of buddy algorithm for buﬀer pooling. For a Java messaging system, it is useful to rely on an application level
memory management technique instead of relying on JVM’s garbage collector
because constant creation and destruction of buﬀers can be a costly operation.
We benchmarked our two pooling mechanisms against each other using combinations of direct and indirect byte buﬀers. We found that one of the pooling
strategies (Buddy1) is faster than the other with a smaller memory footprint.
Also, we demonstrated the performance gain of using direct byte buﬀers.
We released a beta version of our software in early September 2005. This
release contains our buﬀering API with the two implementations of buddy allocation scheme. This API is self-contained and can be used by other Java
applications for application level memory management. Currently, we are working to release additional messaging devices based on mpiJava and Myrinet
eXpress(MX).

References
1. Mark Baker, Bryan Carpenter, Geoﬀrey Fox, Sung Hoon Ko, and Sang Lim. An
object-oriented Java interface to MPI. In International Workshop on Java for Parallel and Distributed Computing, San Juan, Puerto Rico, April 1999.
2. Mark Baker, Hong Ong, and Aamir Shaﬁ. A status report: Early experiences with
the implementation of a message passing system using Java NIO. Technical Report
DSGTR06102004, DSG, October 2004. http://dsg.port.ac.uk/projects/mpj/docs/
res/DSGTR06102004.pdf.
3. Chi-Chao Chang and Thorsten von Eicken. Javia: A Java interface to the virtual
interface architecture. Concurrency - Practice and Experience, 12(7):573–593, 2000.
4. The Java Native Interface Speciﬁcations. http://java.sun.com/j2se/1.3/docs/
guide/jni.
5. Donald Knuth. The Art of Computer Programming: Fundamental Algorithms. Addison Wesley, Reading, Massachusetts, USA, 1973.
6. MPJ Express. http://dsg.port.ac.uk/projects/mpj.
7. Myricom, The MX (Myrinet eXpress) library. http://www.myri.com.
8. The Java New I/O Speciﬁcations. http://java.sun.com/j2se/1.4.2/docs/guide/nio.
9. Matt Welsh and David Culler. Jaguar: enabling eﬃcient communication and I/O
in Java. Concurrency: Practice and Experience, 12(7):519–538, 2000.

