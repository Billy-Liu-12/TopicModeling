Modified Adaptive Resonance Theory Network
for Mixed Data Based on Distance Hierarchy
Chung-Chian Hsu1, Yan-Ping Huang1,2, and Chieh-Ming Hsiao1
1

Department of Information Management, National Yunlin University of Science and
Technology, 123, Sec. 3, University Road, Douliu, Yunlin 640, Taiwan, R.O.C
hsucc@mis.yuntech.edu.tw, g9120817@yuntech.edu.tw
2
Department of Management Information System, Chin Min Institute of Technology,
110, Hsueh-Fu Road, Tou-Fen, Miao-Li 351, Taiwan, R.O.C

sunny@chinmin.edu.tw

Abstract. Clustering of data is a fundamental data analysis step that has been
widely studied across in data mining. Adaptive resonance theory network
(ART) is an important algorithm in Clustering. ART is also very popular in the
unsupervised neural network. Type I adaptive resonance theory network
(ART1) deals with the binary numerical data, whereas type II adaptive
resonance theory network (ART2) deals with the general numerical data.
Several information systems collect the mixing type attitudes, which included
numeric attributes and categorical attributes. However, ART1 and ART2 do not
deal with mixed data. If the categorical data attributes are transferred to the
binary data format, the binary data do not reflect the similar degree. It
influences the clustering quality. Therefore, this paper proposes a modified
adaptive resonance theory network (M-ART) and the conceptual hierarchy tree
to solve similar degrees of mixed data. This paper utilizes artificial simulation
materials and collects a piece of actual data about the family income to do
experiments. The results show that the M-ART algorithm can process the mixed
data and has a great effect on clustering.
Keywords: adaptive resonance theory network (ART), distance hier-archy,
clustering algorithm, data mining, software engineering.

1 Introduction
Clustering is the unsupervised classification of patterns into groups. It is an important
data analyzing technique, which organizes a collection of patterns into clusters based
on similarity [1-3]. Clustering is useful in several exploratory pattern-analysis,
grouping, decision-making, and machine-learning situations. This includes data
mining, document retrieval, image segmentation, and pattern classification. Clustering
methods have been successfully applied in many fields including image processing
[1], pattern recognition [4], biology, psychiatry, psychology, archaeology, geology,
geography, marketing and information retrieval [5,6], software engineering [7-10].
Intuitively, patterns with a valid cluster are more similar to each other than they are to
a pattern belonging to a different cluster.
V.N. Alexandrov et al. (Eds.): ICCS 2006, Part IV, LNCS 3994, pp. 757 – 764, 2006.
© Springer-Verlag Berlin Heidelberg 2006

758

C.-C. Hsu, Y.-P. Huang, and C.-M. Hsiao

The majority of the software clustering approaches presented in the literature
attempt to discover clusters by analyzing the dependencies between software artifacts,
such as functions or source files [7-13]. Software engineering principles, such as
information hiding or high-cohesion and low-coupling are commonly employed to
help determine the boundaries between clusters.
Most of clustering algorithms consider either categorical data or numeric data.
However, many mixed datasets including categorical and numeric values existed
nowadays. A common practice to clustering mixed dataset is to transform categorical
values into numeric values and then proceed to use a numeric clustering algorithm.
Another approach is to compare the categorical values directly, in which two distinct
values result in distance 1 while identical values result in distance 0. Nevertheless,
these two methods do not take into account the similarity information embedded
between categorical values. Consequently, the clustering results do not faithfully
reveal the similarity structure of the dataset. This article is based on distance
hierarchy [2-3] to propose a new incremental clustering algorithm for mixed datasets,
in which the similarity information embedded between categorical attribute is
considered during clustering. In the setting, each attribute of the data is associated
with a distance hierarchy, which is an extension of the concept hierarchy [21] with
link weights representing the distance between concepts. The distance between two
mixed data patterns is then calculated according to distance hierarchies.
The rest of this article is organized as follows. Section 2 reviews clustering
algorithms and discusses the shortcomings of the conventional approaches to
clustering mixed data. Section 3 presents distance hierarchy for categorical data and
proposes the incremental clustering algorithm based on distance hierarchies. In
Section 4, experimental results on synthetic and real datasets are presented.
Conclusions are given in Section 5.

2 Related Work
Adaptive resonance theory neural networks model real-time prediction, search,
learning, and recognition. ART networks function as models of human cognitive
information processing [15-19]. A central feature of all ART systems is a pattern
matching process that compares an external input with the internal memory of an
active code. ART1 deals with the binary numerical data and ART2 deals with the
general numerical data [18]. However, these two methods do not deal with mixed data
attributes.
About clustering mixed data attributes, there are two approaches for mixed data.
One is resorted to a pre-process, which transferred the data to the same type, either all
numeric or all categorical. For transferring continuous data to categorical data, some
metric function is employed. The function is based on simple matching in which two
distinct values result in distance 1, with identical values of distance 0 [20]. The other
is to use a metric function, which can handle mixed data [21]. Overlap metric is for
nominal attributes and normalized Euclidean distance is for continuous attributes.
Among problems with simple matching and binary encoding, a common approach
for handling categorical data is simple matching, in which comparing two identical
categorical values result in distance 0, while two distinct values result in distance 1

Modified Adaptive Resonance Theory Network

759

[21, 22]. In this case, the distance between patterns of Gary and John in the previous
example becomes d(Gary, John) =1, which is the same as d(John, Tom) = d(Gary,
Tom) = 1. Obviously, the simple matching approach disregards the similarity
information embedded in categorical values.
Another typical approach to handle categorical attributes is to employ binary
encoding that transforms each categorical attribute to a set of binary attributes and a
categorical value is then encoded to a set of binary values. As a result, the new
relation contains all numeric data, and the clustering is therefore conducted on the
new dataset. For example, as the domain of the categorical attribute: Favorite_Drink.
The set of it is {Coke, Pepsi, Mocca}. Favorite_Drink is transformed to three binary
attributes: Coke, Pepsi and Mocca in the new relation. The value Coke of
Favorite_Drink in a pattern is transformed to a set of three binary values in the new
relation, i.e. {Coke=1, Pepsi=0, Mocca=0}. The Manhattan distance of patterns Gary
and John is dM(Gary, John) =2, which is the same as dM(Gary, Tom) and dM(John,
Tom), according to the new relation. Traditional clustering algorithm transfers
Favorite_Drink categorical attributes into a binary numerical attribute type as shown
in figure 1.
ID
Gary
John
Tom

Favorite Drink
Coke
Pepsi
Coffee

Amt.
70
70
70

ID

Coke

Pepsi

Coffee

Amt.

Gary
John
Tom

1
0
0

0
1
0

0
0
1

70
70
70

Fig. 1. Traditional clustering algorithm transfers Favorite_Drink categorical attributes into
binary numerical attribute type

The ART network is a popular incremental clustering algorithm [1]. It has several
variants [23, 24], in which ART1 handles only the binary data and ART2 can handle
only the arbitrary continuous data. K-prototype [25] is a recent clustering algorithm
for mixed data. It transfers categorical data attributes to the binary data format,
however, the binary data do not reflect the similar degree. It influences the clustering
quality. Therefore, this paper proposes a modified adaptive resonance theory network
algorithm and the conceptual hierarchy tree to solve the similar degree of mixed data.

3 Clustering Hybrid Data Based on Distance Hierarchy
The distance hierarchy tree is a concept hierarchy structure. It is also a better
mechanism to facilitate the representation and computation of the distance between
categorical values. A concept hierarchy consists of two parts: a node set and a link set
[2, 3, 26, 27]. According to binary encoding approach, it does not reflect the similar
degree. However, it influences the clustering quality. Maintenance was difficult when
the domain of a categorical attribute changes, because the transformed relation
schema also needs to be changed. The transformed binary attributes cannot preserve
the semantics of the original attribute. Because of the drawbacks resulting from the
binary-encoding approach, this paper uses distance hierarchy to solve the similar
degree of mixed data.

760

C.-C. Hsu, Y.-P. Huang, and C.-M. Hsiao

This paper extends the distance hierarchy structure with link weights. Each link has
a weight representing a distance. Link weights are assigned by domain experts. There
are several assignment alternatives. The simplest way is to assign all links as a
uniform constant weight. Another alternative is to assign heavier weights to the links
closer to the root and lighter weights to the links away from the root. For simplicity,
unless stated explicitly, each link weight is set to 1 in this article. The distance of two
concepts at the leaf nodes is the total weight between those two nodes.
A point X in a distance hierarchy consists of two parts, an anchor and a positive
real-value offset, denoted as X(N, d), that is, anchor(X) =N and offset(X) =d. The
anchor is a leaf node and the offset represents the distance from the root of the
hierarchy to the point. A point X is an ancestor of Y if X is in the path from Y to the
root of the hierarchy. If neither one of the two points is an ancestor of the other point,
then the least common ancestor, denoted as LCA(X, Y), is the deepest node that is an
ancestor of X as well as Y.
A special distance hierarchy calls numeric distance hierarchy for a numeric
attribute, say xi, is a degenerate one, which consists of only two nodes, a root MIN
and a leaf MAX, and has the link weight w being the domain range of xi, i.e. w =
(maxi-mini). A point p in such a distance hierarchy has the value (MAX, dp) where the
anchor is always the MAX and the offset dp is the distance from the point to the root
MIN.
About measuring distance, the distance between two data points can be measured
as follows: Let x = [x1 x2 … xn] and y = [y1 y2 … yn]. The distance between a training
pattern x and an M-ART neuron y is measured as the square root of the sum of the
square differences between each-paired components of x and y. Specifically, x and y
represent a training data and a map neuron, respectively, with n-dimension, and C is a
set of n distance hierarchies, then the distance between x and y can be expressed as
d ( x, y ) = x − y = ( ∑ wi ( xi − y i ) 2 )1 / 2 = ( ∑ wi ( h( xi ) − h( yi )) 2 )1 / 2
i =1, n

(1)

i =1, n

Where h(xi) and h(yi) are the mapping of xi and yi to their associated distance
hierarchy hi and wi , the attribute weight, is a user specified parameter allowing the
domain expert to give different weights. For a numeric attribute I, h(xi) - h(yi) is equal
to xi-yi, since h(xi) - h(yi) = (MIN, dh(xi)) - (MIN, dh(yi)) = (MIN, xi-mini)- (MIN, yimini) = (xi-yi).
This paper proposes the distance hierarchy tree structure to overcome the
expression for similar degree. This distance hierarchy tree algorithm combines the
adaptive resonance theory network algorithm and it can be effective with mixed data
in data clustering. This section presents distance hierarchy for categorical data and it
proposes the incremental clustering algorithm based on distance hierarchies.
The categorical utility function [28] attempts to maximize the probability that the
two objects in the same cluster have attribute values in common and the probability
that the objects from different clusters have different attributes. The categorical utility
of a set of clusters can be calculated as
CU = ∑ (
k

Ck
D

∑ ∑ [P(A
i

i

= V ij | C k ) 2 - P(A i = V ij ) 2 ]

(2)

j

Here, P(Ai=Vij|Ck) is the conditional probability that the attribute i has the values Vij
given the cluster Ck, and P(Ai=Vij) is the overall probability of the attribute i having

Modified Adaptive Resonance Theory Network

761

the values Vij in the entire set. The function aims to measure if the clustering
improves the likelihood of similar values falling in the same cluster. Obviously, the
higher the CU values, the better the clustering result [29].
For numeric attributes, the standard deviation represents the dispersion of values.
Variance (σ2) can be used for evaluating the quality of clustering numeric data.
Several cluster validity indices, such as Davies-Bouldin (DB) Index and Calinski
Harabasz (CH) Index [27, 30], have been published; however, they are only suitable
for the numeric data. Hence, in order to evaluate the effectiveness of clustering mixed
data, this paper uses CV index [31], which combined the category utility (CU)
function with variance. The CV is defined as in Equation (3), where the CU and
variance are the validity index for categorical and numeric data, respectively. The
higher the CV values, the better the clustering result.
CV =

CU
1 + Variance (σ )
2

(3)

4 Experiments
This paper develops a prototype system with Borland C++ Builder 6. A series of
experiments have been performed in order to verify the method. A mixed synthetic
dataset and a UCI dataset have also been designed to show the capability of the MART in reasonably expressing and faithfully preserving the distance between the
categorical data. It also reports the experimental results of artificial and actual data.
These experiments use a real Adult dataset from the UCI repository with 48,842
records of 15 attributes, including eight categorical attributes, six numerical attributes,
and one class attribute.
This experiment uses 7 attributes, which include three categorical attributes, such
as Relationship, Marital_status, and Education; and four numeric attributes,
Capital_gain, Capital_loss, Age, and Hours_per_week. The concept hierarchies are
constructed in figure 2.

（ a） marital-status

(b） education

（ c） relationship

Fig. 2. Concept hierarchies for (a) Marital-status (b) Education and (c) Relationship attributes
of the Adult dataset

The M-ART parameters are established as follows: the initial warning value is 0.55
and it increases progressively 0.05 until 0.75. The initial learning rate is 0.9. The stop
condition t occurs when the momentum of the output layer is lower than 0.000015.
This paper collects the dataset with different methods. These methods divide adult
datasets into 5, 6, 7 and 8 groups. Concerning the CU values for categorical attributes,

762

C.-C. Hsu, Y.-P. Huang, and C.-M. Hsiao

the higher the CU values, the better the clustering result. The CU value of clustering
M-ART method is the highest, K-prototype method is second, and traditional ART2 is
the lowest. The symbol " ***" means that it does not find the suitable parameter to
divide into group with the datasets. The parameter of ART2 reaches 7, it is unable to
divide seven groups all the time. The problem occurs because there are too many
parameters in ART2.
This paper normalizes the variance between 0 and 1 for numeric results. The
normalized variance is useful in CV index. Table 1 shows the CV values of the
clustering results by M-ART, ART2 and k-prototypes on level 1 and the leaf level in
individual concept hierarchies with cluster numbers 5, 6, 7 and 8. The higher the
value of CV values, the better the clustering result. The CV value in M-ART method
is the highest, K-prototype method is second, and traditional ART2 is the lowest.
Table 1. The CV values for Adult dataset with 5, 6, 7, 8 clusters by M-ART, ART2 and KPrototypes
M-ART
Cluster
CU
No.
Leaf
5
1.069
6
1.113
7
1.115
8
1.177

Level 1
1.16
1.2
1.21
1.31

Variance
age gain
0.127 0.022
0.163 0.023
0.182 0.011
0.218 0.014

loss
0.038
0.044
0.044
0.052

hrs_per_week
0.071
0.085
0.100
0.115

CV
Leaf_Level
0.85
0.85
0.83
0.84

Level 1
0.59
0.60
0.61
0.65

K-Prototype
Cluster
CU
No.
Leaf
5
0.859
6
1.002
7
1.039
8
1.088

Level 1
0.834
0.977
0.919
1.087

Variance
age gain
0.114 0.033
0.424 0.72
0.515 0.893
0.610 1.068

loss
0.046
1.454
1.801
2.154

hrs_per_week
0.073
2.937
3.635
4.342

CV
Leaf_Level
0.68
0.15
0.13
0.12

Level 1
0.46
0.16
0.12
0.13

ART2
Cluster
CU
No.
Leaf
5
0.0010
6
0.0043
8
0.0073

Level 1
0.00081
0.00570
0.00757

Variance
age gain
0.176 0.027
0.211 0.033
0.281 0.044

loss
0.042
0.051
0.068

hrs_per_week
0.079
0.095
0.128

CV
Leaf_Level
0.00075
0.00309
0.00479

Level 1
0.00070
0.00482
0.00608

Increased
31.08%
29.05%
27.05%
23.00%

Increased
32.85%
-1.72%
7.01%
-5.56%

Increased
6.64%
-55.87%
-26.71%

5 Conclusions and Future Work
Most traditional clustering algorithms can only handle either categorical or numeric
value. Although some research results have been published for handling mixed data,
they still cannot reasonably express the similarities among categorical data.
This paper proposes the distance hierarchy tree structure to overcome the
expression for similar degree. This distance hierarchy tree algorithm combines the
adaptive resonance theory network algorithm and it can be effective with mixed data
in data clustering. It presents a MART algorithm, which can handle mixed dataset

Modified Adaptive Resonance Theory Network

763

directly. The experimental results on synthetic data sets show that the proposed
approach can better reveal the similarity structure among data, particularly when
categorical attributes are involved and have different degrees of similarity, in which
the traditional clustering approaches do not perform well. The experimental results on
the real dataset have better performances than other algorithms.
MART is a clustering algorithm for any field with mixed data. The future work
will try to use this method in finding out the pattern rules from software engineering
databases

References
1. Jain, A., and Dubes, R.: Algorithms for clustering Data. Prentice-Hall, Englewood Cliffs.
NJ. (1988).
2. Hsu, C. C.: Generalizing Self-Organizing Map for Categorical Data. IEEE Transactions on
Neural Networks. (2006, In press).
3. Hsu, C. C. and Wang, S. H.: An Integrated Framework for Visualized and Exploratory
Pattern Discovery in Mixed Data. IEEE Transactions on Knowledge and Data
Engineering. vol. 18, no. 2. (2006) 161-173.
4. Anderberg, M.: Cluster Analysis for Applications. Academic Press, New York. (1973).
5. Rasmussen, E.: Clustering Algorithms. Information Retrieval: Data Structures &
Algorithms. William B. Frakes and Ricardo Baeza-Yates (Eds.). Prentice Hall. (1992).
6. Salton, G. and Buckley, C.: Automatic Text Structuring and Retrieval-experiments in
Automatic Encyclopedia Searching. Proceedings of the Fourteenth International ACM
SIGIR Conference on Research and Development in Information Retrieval. (1991) 21-30.
7. Kadamuddi, D. and Tsai, J. P.: Clustering algorithm for parallelizing software systems in
multiprocessors environment. Software Engineering, IEEE Transactions, vol. 26, Issue 4,
(2000) 340-361.
8. Tian, J.: Better reliability assessment and prediction through data clustering. Software
Engineering, IEEE Transactions, vol. 28, Issue 10, (2002) 997-1007.
9. Chen K., Zhang W., Zhao H., Mei H.: An approach to constructing feature models based
on requirements clustering. Requirements Engineering, 2005. Proceedings. 13th IEEE
International Conference. (2005) 31-40.
10. Andritsos, P. and Tzerpos, V.: Information-theoretic software clustering. Software
Engineering, IEEE Transactions, vol. 31, Issue 2. (2005) 150-165.
11. Hutchens, D. H. and Basili, V.R.: System Structure Analysis: Clustering with Data
Bindings. Software Engineering, IEEE Transactions, vol. 11, no. 8. (1985) 749-757.
12. Schwanke, R. W.: An Intelligent Tool for Re-Engineering Software Modularity. Proc. 13th
Int’l Conf. Software Engineering. (1991) 83-92.
13. Mancoridis, S., Mitchell, B., Chen, Y. and Gansner, E.: Bunch: A Clustering Tool for the
Recovery and Maintenance of Software System Structures. Proc. Int’l Conf. Software
Maintenance. (1999).
14. Can, F.: Incremental clustering for dynamic information processing. ACM Transaction for
Information Systems, vol. 11. (1993) 143-164.
15. Carpenter, G. A.: Distributed learning, recognition, and prediction by ART and ARTMAP
neural networks. Neural Networks, vol. 10, no. 8. (1997) 1473–1494.
16. Carpenter, G. A., and Grossberg, S.: Normal and amnesic learning, recognition, and
memory by a neural model of cortico-hippocampal interactions. Trends in Neuroscience,
vol. 16, no. 4. (1993) 131–137.

764

C.-C. Hsu, Y.-P. Huang, and C.-M. Hsiao

17. Grossberg, S.: How does a brain build a cognitive code? Psychological Review, vol. 87.
(1980) 1–51.
18. Grossberg, S.: The link between brain, learning, attention, and consciousness.
Consciousness and Cognition, vol. 8. (1999) 1–44.
19. Grossberg, S.: How does the cerebral cortex work? Development, learning, attention, and
3D vision by laminar circuits of visual cortex. Behavioral and Cognitive Neuroscience
Reviews, vol. 2, no. 1. (2003) 47–76.
20. Guha, S., Rastogi, R. and Shim, K.: ROCK: A robust clustering algorithm for categorical
attributes. Proceedings of the IEEE Conference on Data Engineering. (1999) 512-521.
21. Wilson, D.R. and Martinez, T. R.: Improved heterogeneous distance functions. Journal of
Artificial Intelligence Research, vol. 6. (1997) 1-34.
22. Ester, M., Kriegel, H. P., Sander, J., Wimmer, M. and Xu, X.: Incremental clustering for
mining in a data warehousing environment. Proceedings of the 24th Intl. Conf. on Very
Large Data Bases (VLDB). (1998) 323-333.
23. Carpenter, G. A. and Grossberg, S.: ART 2 Self-organization of stable category
recognition codes for analog input patterns. Applied Optics : Special Issue on Neural
Networks, vol. 26. (1987) 4919-4930.
24. Carpenter, G., Grossberg, A., S. and Rosen, D. B.: Fuzzy ART: fast stable learning and
categorization of analog patterns by an adaptive resonance system. Neural Networks, vol.
4. (1991) 759-771.
25. Huang, Z.: Extensions to the k-means algorithm for clustering large data sets with
categorical values. Data Mining and Knowledge Discovery, vol. 2, no. 3. (1998) 283-304.
26. Dash, M. and Choi, K., Scheuermann, P. and Liu, H.: Feature selection for clustering - a
filter solution, IEEE International Conference on Data Mining. (2002) 115 – 122.
27. Maulik, U. and Bandyopadhyay, S.: Performance evaluation of some clustering algorithms
and validity indices. IEEE Transactions on Pattern Analysis and Machine Intelligence.
(2002) 1650-1654.
28. Gluck, M. A. and Corter, J. E.: Information, uncertainty, and the utility of categories.
Proceedings of the Seventh Annual Conference of the Cognitive Science Society. (1985).
29. Barbara, D., Couto, J. and Li, Y.: COOLCAT: an entropy-based algorithm for categorical
clustering. Proceedings of the eleventh international conference on Information and
knowledge management. (2002) 582-589.
30. Halkidi, M., Batistakis, Y. and Vazirgiannis, M.: On clustering validation techniques.
Journal of Intelligent Information Systems, vol. 17. (2001) 107-145.
31. Hsu, C. C. and Chen, Y. C.: Mining of Mixed Data with Application to Catalog
Marketing. Expert Systems with Applications. (2006, In press).

：

