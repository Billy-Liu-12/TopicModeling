An Algorithm for the Generalized k-Keyword
Proximity Problem and Finding Longest
Repetitive Substring in a Set of Strings
Inbok Lee1, and Sung-Ryul Kim2,
1

King’s College London, Department of Computer Science,
London WC2R 2LS, United Kingdom
inboklee@gmail.com
2
Division of Internet & Media and CAESIT,
Konkuk University, Seoul, Republic of Korea
kimsr@konkuk.ac.kr

Abstract. The data grid may consist of huge number of documents and
the number of documents which contain the keywords in the query may
be very large. Therefore, we need some method to measure the relevance
of the documents to the query. In this paper we propose algorithms for
computing k-keyword proximity score [3] in more realistic environments.
Furthermore, we show that they can be used to ﬁnd longest repetitive
substring with constraints in a set of strings.

1

Introduction

The data grid may consist of huge number of documents and the number of
documents which contain the keywords in the query may be very large. All these
documents may not be relevant to what the user wants: some may contain them
in diﬀerent contexts. Therefore, we need a method to measure the relevance of the
documents to the query. Here we focus on the proximity of the keywords which
means how close they appear together in the document. If they are appearing
close (good proximity), it is likely that they have stronger combined meaning.
The oﬀset of a word in a document is the distance (number of the words) from
the start of the document. A range [a, b] in a document represents the contiguous
part of the document from a-th word to b-th word in the document. The size of
the range [a, b] is b − a.
In the data grid, we assume that documents are stored in inverted ﬁle structure. Each keyword has a list of IDs of documents which contain the keyword
and a sorted list of oﬀsets in the document. Using inverted ﬁle structure, we can
easily obtain the set of documents which contain the keywords.
Kim et al. [3] deﬁned the generalized proximity score and proposed O(n log k)
time algorithm where k is the number of keywords and n is the number of
occurrences of the keywords in the document.
This work was supported by the Post-doctoral Fellowship Program of Korea Science
and Engineering Foundation (KOSEF).
Contact author.
V.N. Alexandrov et al. (Eds.): ICCS 2006, Part IV, LNCS 3994, pp. 289–292, 2006.
c Springer-Verlag Berlin Heidelberg 2006

290

I. Lee and S.-R. Kim

Deﬁnition 1. Given k keywords w1 , w2 ,. . ., wk , a set of lists K={K1 , K2 , . . . Kk
where each keyword wi has a sorted list of oﬀsets Ki = {oi1 , oi2 , . . . , oiji (1 ≤
i ≤ k), and k positive integers R1 , R2 , . . . , Rk , and another integer k (≤ k),
the generalized k-keyword proximity problem is ﬁnding the smallest range that
contains k distinct keywords where each wi of these keywords appears at least
Ri times.
We brieﬂy explain Kim et al.’s algorithm. A more detailed account can be found
in [3]. We need to deﬁne two terms.
Deﬁnition 2. A candidate range is a range which contains at least k distinct
keywords where each keyword wi appears at least Ri times in the range.
Deﬁnition 3. A critical range is a candidate range which does not contain another candidate range.
We can easily show that the solution is the smallest critical range. Hence we need
to ﬁnd critical ranges and report the smallest one. First, we merge the sorted
lists of oﬀsets of k keywords into one list. This step runs in O(n log k) time. For
simplicity we assume that each oﬀsets are mapped to a number in range [1..n].
We store the keyword ID into a list L[1..n].
The outline of the algorithm is that we ﬁrst ﬁnd a candidate range (expanding
sub-step) and ﬁnd a critical range from that candidate range (shrinking sub-step).
We maintain a range [a, b]. Initially a = 1 and b = 0. We also maintain k
counters c1 , c2 , . . . , ck . Initially c1 = c2 = · · · = ck = 0. And we maintain a
counter h which counts the number of ci ’s (1 ≤ i ≤ k) that are ≥ Ri . Initially
h = 0.
In the expanding sub-step, the range is expanded from [a, b] to [a, b + 1]. We
check L[b] and set cL[b] = c[L[b] + 1. We also check whether cL[b] = Ri . If so, we
update h = h + 1. We repeat this loop until h = k . Then [a, b] is a candidate
range and go to the shrinking sub-step.
In the shrinking sub-step, the range is reduced from [a, b] to [a + 1, b]. We also
set cL[a] = cL[a] − 1 and check whether cL[a] ≤ RL[a] . If so, h = h − 1. And if
h < k , we report a critical range [a− 1, b]. We go back to the expanding sub-step
with the range [a, b]. These steps run in O(n) time and the total time complexity
is O(n log k).

2

Our Improvements

Here we can consider the following variations. First, the original problem does
not specify the keyword which should be included. Some keywords may be more
important than others.
Problem 1. In k-keyword proximity problem, one special keyword wi in the query
should appear in the range.
Without loss of generality, assume the keyword that must appear in the critical
range is w1 . The original algorithm may report no critical range with w1 even
though the document contains w1 !

An Algorithm for the Generalized k-Keyword Proximity Problem

291

We ﬁrst ﬁnd a candidate for the problem and make the range narrow as much
as possible. In the expanding sub-step, we move to the shrinking sub-step only
after the current range [a, b] contains w1 at least R1 times. We guarantee that
the input to the shrinking sub-step meets the constraints of Problem 1. In the
shrinking sub-step, we add one more check. If, by shrinking from [a, b] to [a+1, b],
c1 becomes smaller than R1 , we report [a, b] as a critical range (without checking
the condition h < k ).
Now we consider another problem when keywords in the query must be in
some order (for example, “Paris Hilton” and “Hilton Paris”).
Problem 2. A keyword wi must appear before another keyword wj .
Without loss of generality, assume that w2 must follow w1 . It means that w2 can
appear only after w1 appears at least R1 times. In the expanding sub-step, we
move to the shrinking sub-step only after the current range [a, b] contains w1 at
least R1 times before w2 appears. We may encounter w2 before R1 w1 ’s. Then
we discard the current range [a, b]. We restart the expanding sub-step with the
range [b + 1, b + 1] and initialize all the variables. In the shrinking sub-step, we
do the same as we did in Problem 1.
Finally, we can consider the case where two keywords have a Boolean relation.
Problem 3. If a keyword wi appears in the document, then also another keyword
wj must/must not appear in the document (AND/XOR relation).
Without loss of generality, assume that w1 and w2 forms these relations. First
we consider the AND relation. In the expanding sub-step, we maintain a ﬂag
f . Initially f = 0. When we meet a w1 , we set f = 1. When we move to the
shrinking sub-step in the original algorithm, we check whether the ﬂag f is ON.
If so, we postpone until we meet an occurrence of w2 . In the shrinking sub-step,
we use the ﬂag again. If f = 0, there is no modiﬁcation at all. But if f = 1, each
time we shrink the range we check whether it removes the occurrence of w1 . If
so, we report the range [a, b] as the critical range.
The procedure is almost the same when we handle the XOR relation. We use
the ﬂag again. When we meet an occurrence of w1 , we set f = 1. If we meet an
occurrence of w2 and f = 1, then we discard the current range [a, b] and restart
with [b + 1, b + 1]. The shrinking sub-step is the same as the original algorithm.
All these modiﬁcation does not change the time complexity.
Theorem 1. All the problems in Section 2 can be solved in O(n log k) time.

3

Repetitive Longest Substring in a Set of Strings

Now we consider ﬁnding the longest substring in a set of strings.
Problem 4. Given a set of strings U = {T1 , T2 , . . . , Tk }, a set of positive integers
D = {d1 , d2 , . . . , dk }, and a positive integer k (≤ k), ﬁnd the longest sting w
which satisﬁes two conditions: (a) there is a subset U of U such that w appears
at least di times in each string Ti in U , and (b) |U | = k .

292

I. Lee and S.-R. Kim

We use the suﬃx array. The suﬃx array of a text T is a sorted array suf [1..|T |]
and lcp[1..|T |]. suf [k] = i if and only if T [i..|T |] is the k-th suﬃx of T . lcp[k] is the
length of the longest common preﬁx between each substring in the suﬃx array
and its predecessor and lcp(a, b) = mina≤i≤b lcp[i] with the following properties.
Fact 1. lcp(a, b) ≤ lcp(a , b ) if a ≤ a and b ≥ b .
Fact 2. The length of the longest common preﬁx of T [suf [a]..|T |], T [suf [a +
1]..|T |],. . ., T [suf [b]..|T |] is lcp(a + 1, b).
To build the suﬃx array for U = {T1 , T2 , . . . , Tk }, we create a new string
T = T1 %T2 % · · · Tk where % is a special symbol and is smaller than any other
character in Σ. sufand lcparrays can be computed in O(|T |) time by [2, 1] with
one modiﬁcation: % does not match itself. We use another array ids. ids[j] = i if
T [j]T [j + 1] · · · % was originally a suﬃx of Ti (we mean the ﬁrst % after T [j]).
This computation also takes O(|T |) time.
We brieﬂy explain the outline of [4]. Fact 1 tells that the smaller a range
becomes, the longer the common preﬁx is. Hence, we consider IDs of strings as
IDs of keywords as we did in Section 2. What we need is the smallest range that
yields the longest common preﬁx of the suﬃxes (the longest common substring).
We use the same algorithm in Section 2, without the merging step. Hence the
time complexity is O(n).
The problems in Section 2 can be transformed into these following problems
except Problem 2 because we do not consider order in U.
Problem 5. The same as Problem 4, but U must contain Ti .
Problem 6. The same as Problem 4, but if U contains a string Ti , it must/must
not contain another string Tj (AND/XOR relation).
All these problems can be solved in O(n) time with equivalent algorithm in
Section 2.

References
1. T. Kasai, G. Lee, H. Arimura, S. Arikawa, and K. Park. Linear-time longestcommon-preﬁx computation in suﬃx arrays and its applications. In CPM 2001,
pages 181–192, 2001.
2. D. K. Kim, J. S. Sim, H. Park, and K. Park. Linear-time construction of suﬃx
arrays. In CPM 2003, pages 186–199, 2003.
3. S.-R. Kim, I. Lee, and K. Park. A fast algorithm for the generalised k-keyword
proximity problem given keyword oﬀsets. Information Processing Letters, 91(3):115–
120, 2004.
4. I. Lee, and Y. J. Pinzon Ardil`
a. Linear time algorithm for the generalised longest
common repeat problem. In SPIRE 2005, pages 191–200, 2005.

