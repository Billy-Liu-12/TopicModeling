Learning Classifier System Approach to Natural
Language Grammar Induction
Olgierd Unold
Institute of Computer Engineering, Control and Robotics
Wroclaw University of Technology
Wyb. Wyspianskiego 27, 50-370 Wroclaw, Poland
olgierd.unold@pwr.wroc.pl
http://sprocket ict.pwr.wroc.pl/~unold

Abstract. This paper describes an evolutionary approach to the problem of
inferring non-stochastic context-free grammar (CFG) from natural language
(NL) corpora. The approach employs Grammar-based Classifier System (GCS).
GCS is a new version of Learning Classifier Systems in which classifiers are
represented by CFG in Chomsky Normal Form. GCS has been tested on the NL
corpora, and it provided comparable results to the pure genetic induction
approach, but in a significantly shorter time. The efficient implementation for
grammar induction is very important during analysis of large text corpora.

1 Introduction
Syntactic processing, one of the complex task on natural language processing (NLP),
has always been considered to be paramount to a wide range of applications, such as
machine translation, information retrieval, speech recognition and the like.
Historically, most computational systems for syntactic parsing, employ hand-written
grammars, consisting of a laboriously crafted set of grammar rules to apply syntactic
structure to a sentence. But in recent years, a lot of research efforts are trying to
automatically induce workable grammars from annotated corpora. The process in
which a system produces a grammar given a set of corpora is known as grammatical
inference or grammar induction [4]. In general, the natural language (NL) corpora
may contain both positive and negative examples from the language under study,
which is described most often by context-free grammar (CFG). There are very strong
negative results for the learnability of CFG. Effective algorithms exist only for regular
languages, thus construction of algorithms that learn context-free grammar is critical
and still open problem of grammar induction. Many researchers have attacked the
problem of grammar induction by using evolutionary methods to evolve (stochastic)
CFG or equivalent pushdown automata [8], but mostly for artificial languages like
brackets, and palindromes. For surveys of the non-evolutionary approaches for CFG
induction see [6].
In this paper we examine NL grammar induction using Grammar-based Classifier
System (GCS) [7] - a new model of Learning Classifier System (LCS). In spite of
intensive research into classifier systems in recent years [5] there is still slight number
Y. Shi et al. (Eds.): ICCS 2007, Part II, LNCS 4488, pp. 1210–1213, 2007.
© Springer-Verlag Berlin Heidelberg 2007

Learning Classifier System Approach to Natural Language Grammar Induction

1211

of attempts at evolving grammars using LCS. Bianchi in his work [2] revealed, on the
basis of experiments with bracket grammars, palindromes and toy-grammar, higher
efficiency of LCS in comparison with evolutionary approach. Cyre [3] inducted a
grammar for subset of natural language using LCS but comparison to his results is
hard since usage of corpora protected by trademarks. GCS tries to fill the gap
also bringing grammar induction issues up. As was shown in [7], GCS achieves better
results than Bianchi’s system with reference to artificial grammars. This paper
describes GCS approach to the problem of inferring non-stochastic CFG from
NL corpora.

2 Grammar-Based Classifier System
The GCS operates similar to the classic LCS but differs from them in (i)
representation of classifiers population, (ii) scheme of classifiers’ matching to the
environmental state, (iii) methods of exploring new classifiers. Population of
classifiers has a form of a context-free grammar rule set in a Chomsky Normal Form
(CNF). This is not a limitation actually because every CFG can be transformed into
equivalent CNF. CNF allows only production rules in the form of A→α or A→BC,
where A, B, C are the non-terminal symbols and a is a terminal symbol. The first rule
is an instance of terminal rewriting rule. These ones are not affected by the genetic
algorithm (GA), and are generated automatically as the system meets unknown (new)
terminal symbol. Left hand side of the rule plays a role of classifier’s action while the
right side a classifier’s condition. All classifiers (production rules) form a population
of evolving individuals. In each cycle a fitness calculating algorithm evaluates a value
(an adaptation) of each classifier and a discovery component operates only on a single
classifier. CFG learns using a training set that consists of sentences both syntactically
correct and incorrect. Grammar which accepts correct sentences and rejects incorrect
ones is able to classify unseen so far sentences from a test set. Cocke-YoungerKasami parser (CYK), which operates in Θ(n3) time, is used to parse sentences from
corpus. Environment of classifier system is substituted by an array of CYK parser.
Classifier system matches the rules according to the current environmental state (state
of parsing) and generates an action (or set of actions in GCS) pushing the parsing
process toward the complete derivation of the sentence analyzed. The discovery
component in GCS is extended in comparison with standard LCS. In some cases a
“covering” procedure may occur, adding some useful rules to the system. It adds
productions that allow continuing of parsing in the current state of the system. Apart
from the “covering” a GA also explores the space searching for new, better rules.
Classifiers used in parsing positive examples gain highest fitness values, unused
classifiers are placed in the middle while the classifiers that parse negative examples
gain lowest possible fitness values. GCS uses a mutation of GA that chooses two
parents in each cycle to produce two offspring. The selection step uses the roulette
wheel selection. After selection a classical crossover or mutation can occur. Offspring
that are created replace existing classifiers based on their similarity using crowding
technique, which preserves diversity in the population and extends preservation of the
dependencies between rules by replacing classifiers by the similar ones.

1212

O. Unold

3 The Experiments
Bianchi in [2] was not trying to use his system to induct a grammar for huge NL
corpora. However such an experiment was performed using pure genetic algorithm
and CFG by Aycinena at all [1]. Their system used grammar in CNF and a CYK
parser, and as a corpora extensive part of various children books and the Brown
linguistic data. The corpora were part-of-speech tagged using a Brill tagger. All
English words were then removed – leaving only the tags themselves, and number of
tags was reduced to 7 categories: nouns, pronouns; verbs, helping verbs; adjectives,
numeral, possessives; adverbs; prepositions, particles; conjunctions, determiners;
other (foreign words, symbols, and interjections).
The corpuses were divided into two parts, every third sentence was used for testing
evolved grammar, and the remaining part of the corpora for inducing the grammars.
The incorrect sentences were generated randomly from uniform distribution of length
from 2 to 15 tags. Some comparison set of experiments with GCS was performed on
the above NL corpora. Ten independent experiments were performed, evolution on
each training corpus ran for 1,000 generations. The main results of the NL grammar
induction with GCS are summarized in the table 1. In case of 5 corpuses the GCS
model induced a grammar of higher quality fitness, for the brown this value is only
slightly lower, and in the remaining 3 cases the estimator’s value is lower, but not
exceeding 5%. The values of the positive estimator are in 8 cases significantly higher
for the GCS model (the differences oscillate in the range of 4.2% and 16.2%), and for
the brown corpus the AKM approach got a result which is better by 0.5%.
Undoubtedly, the worst for the GCS model comes up the comparison of the negative
values – for each corpus the model got decidedly higher values of this estimator, and
the differences oscillate in the range 1% for wizard to 17.3% for tom corpus. It
indicates that during the grammar induction the GCS model created in a few cases
(for 5 bodies the differences do not exceed 7%) productions which are too universal
in comparison to the AKM approach, which also parse a part of negative sentences.
The last parameter which can be compared is the number of evolutionary steps
(evals), in which both approaches found their best solutions. In as many as 6 cases the
GCS model did not exceed 50 steps, in the next case did not exceed 100 steps, and
two longest inductions took only slightly above 500 steps (somewhat over an hour).
The AKM approach took, in the best case, 15,500 steps, and for as many as 5 corpora
– 200,000 steps, and, according to the authors, 60 hours of calculation! The GCS
model proved to be incomparably more effective, being able to find, in the majority of
cases, the grammars with higher values of fitness and positive estimators. The evolved
grammar learned for the corpus children indicates some interesting linguistic features.
There are quite obvious groups like adjective noun, as well as rule noun verb. The
model found in the corpus also often appearing in English bigrams, so as noun
adverb, noun conjunction, verb adverb, or verb conjunction. The sentence can start
from the article why adding the article for the beginning of sentence is also keeping
its correctness. The straight majority of context-free production rules are beginning
from the starting symbol what is suggesting the big generality of these rules. On one
hand it will knock for economical writing entire grammar on the other however such a
versatility is enabling parsing also of sentences not belonging to the language.

Learning Classifier System Approach to Natural Language Grammar Induction

1213

Table 1. Comparison of NL grammar induction using genetic approach (AKM) with GCS. The
corpora include a selection of children’s books (denoted children, 986 learning correct
sentences, and 986 learning incorrect sentences), The Wizard of Oz (wizard, 1540/1540), Alice
in Wonderland (alice, 1012/1012), Tom Sawyer (tom, 3601/3601), and five Brown corpora:
brown_a (2789/2789), brown_b (1780/1780), brown_c (1099/1099), brown_d (1062/1062), and
brown_e (2511/2511). For each learning corpus, the table shows the target language, and four
sets of results. The first is the best fitness gained by GCS within 10 experiments and compared
approach. The fitness describes the percentage of sentences (correct and incorrect) recognized
correctly. Next results of the GCS model refer to the experiment in which best fitness was
obtained. The second result, positive, shows the percentage of correct examples from the train
set classified correctly. The third sort of results, negative, is the percentage of negative
examples classified incorrectly, and the last one indicates the number of generations needed to
reach the best fitness (evals).

Corpus
children
wizard
alice
tom
brown_a
brown_b
brown_c
brown_d
brown_e

fitness
GCS AKM
93,2
93,1
94,6
90,2
89,5
92,1
86,3
92,1
93,8
94,0
94,6
94,0
92,5
87,9
91,6
91,3
89,5
94

positive
GCS AKM
98,8
91,8
99,3
89,5
96,8
92,5
98,4
92,7
98,3
94,1
99,3
94,7
96,7
80,5
97,1
88,2
93,4
93,9

negative
GCS AKM
12,5
5,7
10,2
9,2
17,9
8,4
25,9
8,6
11,6
6,1
10,2
6,7
11,7
4,7
13,8
5,6
14,5
5,9

GCS
9
32
81
3
45
506
592
18
38

evals
AKM
200,000
200,000
200,000
200,000
48,500
200,000
15,500
45,000
122,000

References
1. Aycinena, M., Kochenderfer, M.J., Mulford D.C.: An evolutionary approach to natural
language grammar induction. Final project for CS224N: Natural Language Processing.
Stanford University (2003)
2. Bianchi, D.: Learning Grammatical Rules from Examples Using a Credit Assignement
Algorithm. In: Proc. of The First Online Workshop on Soft Computing (WSC1), Nagoya
(1996) 113-118
3. Cyre, W.R.: Learning Grammars with a Modified Classifier System. In: Proc. 2002 World
Congress on Computational Intelligence, Honolulu Hawaii (2002) 1366-1371
4. Gold, E.: Language identification in the limit. Information Control 10 (1967) 447-474
5. Lanzi, P.L., Riolo, R.L.: A Roadmap to the Last Decade of Learning Classifier System
Research. LNAI 1813, Springer Verlag (2000) 33-62
6. Lee, L.: Learning of Context-Free Languages: A Survey of the Literature. Report TR-12-96.
Harvard University, Cambridge, Massachusetts (1996)
7. Unold, O.: Playing a toy-grammar with GCS. In Mira J., Álvarez J.R. (eds.) IWINAC 2005,
LNCS 3562 (2005) 300-309
8. Unold, O.: Context-free grammar induction with grammar-based classifier system. Archives
of Control Science, vol. 15 (LI) 4 (2005) 681-690

