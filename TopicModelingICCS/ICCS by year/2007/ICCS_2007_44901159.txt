A Genetic Algorithm for Solving a Special Class
of Nonlinear Bilevel Programming Problems
Hecheng Li1,2 and Yuping Wang1
1

2

School of Computer Science and Technology,
Xidian University, Xi’an, 710071, China
School of Science, Xidian University, Xi’an, 710071, China
lihecheng@qhnu.edu.cn, ywang@xidian.edu.cn

Abstract. A special nonlinear bilevel programming problem (BLPP),
whose follower-level problem is a convex programming with a linear objective function in y, is transformed into an equivalent single-level programming by using Karush-Kuhn-Tucker (K-K-T) conditions. To solve
the equivalent problem eﬀectively, a new genetic algorithm is proposed.
First, a linear programming (LP) is constructed to decrease the dimensions of the transformed problem. Then based on a constraint-handling
scheme, a second-phase evolving process is designed for some oﬀspring of
crossover and mutation, in which the linear property of follower’s function is used to generate high quality potential oﬀspring.
Keywords: Bilevel programming problems, genetic algorithm, linear
programming problem, constraint handling, optimal solutions.

1

Introduction

The bilevel programming problem (BLPP) is a mathematical model of the leaderfollower game. As an optimization problem with a hierarchical structure, BLPP
has a wide variety of applications[1]. However, owing to the complex structure,
the vast majority of research on BLPP is concentrated on the linear version
of the problem, and a few works on the nonlinear BLPP[2]. Moreover, most of
existing algorithms for nonlinear BLPP are usually based on the assumption
that all of functions are convex and twice diﬀerentiable[3]. In recent years, genetic algorithms (GAs) have been used for solving BLPP[2,4,6]. [2] proposed an
evolutionary algorithm for solving the BLPP in which the follower’s problems
are convex. In this paper, we further discuss the simpliﬁed model of the BLPP
given in [2], in which the follower’s objective function is linear in y. We construct an LP to avoid increasing the dimensions of the search space, and design
a second-phase evolving process after both crossover and mutation.
Since a convex programming can be transformed into another convex programming with a linear objective function, as a result, the proposed algorithm
can also be used for solving the BLPP given in [2].
This work is supported by the National Natural Science Foundation of China
(No. 60374063).
Y. Shi et al. (Eds.): ICCS 2007, Part IV, LNCS 4490, pp. 1159–1162, 2007.
c Springer-Verlag Berlin Heidelberg 2007

1160

2

H. Li and Y. Wang

Transformation of the Problem

We consider the following nonlinear bilevel programming problem (BLPP):
⎧
min F (x, y)
⎪
⎪
x∈X
⎪
⎪
⎪
⎨ s.t. G(x, y) ≤ 0
⎪
min f (x, y) = c(x)T y
⎪
⎪
y∈Y
⎪
⎪
⎩
s.t. g(x, y) ≤ 0

(1)

where F : Rn × Rm → R, G : Rn × Rm → Rp , g : Rn × Rm → Rq , and
c : Rn → Rm . For x ﬁxed, each component of g is convex and diﬀerentiable in
y. Let the search space Ω = {(x, y)|x ∈ X, y ∈ Y }, and the constraint region
S = {(x, y) ∈ Ω|G(x, y) ≤ 0, g(x, y) ≤ 0}. For other related deﬁnitions, refer to
[2,3].
We assume that intS = φ. Replace the follower’s programming problem by
K-K-T conditions, we can transform the nonlinear BLPP (1) as follows:
⎧
min F (x, y)
⎪
⎪
x,y,λ
⎨
(2)
s.t. G(x, y) ≤ 0, g(x, y) ≤ 0, λ ≥ 0
⎪
⎪
⎩
c(x) + ( y g(x, y))T λ = 0, λT g(x, y) = 0
where λ = (λ1 , λ2 , . . . , λq )T is Lagrangian multipliers.

3

Constraint-Handling and Decreasing the Dimensions of
the Transformed Problem

For any infeasible individual B, a new constraint-handling scheme is designed
to generate an approximate feasible individual D. Firstly, randomly choose an
individual A ∈ S. Let Sˆ = {(x, y) ∈ Ω|G(x, y) ≤ 1 , g(x, y) ≤ 2 }, and =
( T1 , T2 )T , where i are small positive vectors and tend to zero with the increasing
of the generations. Let D = rB + (1 − r)A, where r ∈ (0, 1) is random number,
ˆ then stop. Otherwise, let B = D, and re-compute D. The process is
if D ∈ S,
ˆ
repeated until D ∈ S.
For ﬁxed x
¯ and y¯, one can get λ by solving the following LP:
⎧
u¯(¯
x, y¯) = min(1, 1, . . . , 1)U
⎪
⎪
λ,U
⎨
(3)
s.t. h(¯
x, y¯, λ) + U = 0
⎪
⎪
⎩
λ ≥ 0, U ≥ 0
where h(¯
x, y¯, λ) = ((c(¯
x)+( y g(¯
x, y¯))T λ)T , λT g(¯
x, y¯))T , U is an artiﬁcial vector.
Thus we only need to evolve (¯
x, y¯) in the algorithm, this is equivalent to the
reduction of the problem dimensions.

A Special Class of Nonlinear Bilevel Programming Problems

4

1161

Proposed Algorithm(Algorithm 1)

Step 1 (Initialization). Randomly generate the initial population pop(0) of Np
points in Ω such that there is at least one point in S. Apply the constrainthandling scheme to the points which don’t belong to Sˆ such that these Np
ˆ Denote N = {(x, y) ∈ pop(0) ∩ S} and let k = 0.
points are in S.
F (x, y),
u¯(x, y) = 0;
Step 2. Evaluate the ﬁtness F¯ (x, y) =
where K is
K + μ¯
u(x, y), u¯(x, y) = 0.
an upper-bound of F (x, y) on the set {(x, y)|¯
u(x, y) = 0}, μ ≥ 0.
Step 3 (Crossover ). For each pair of randomly matched parents p1 and p2 , the
crossover generates oﬀspring: o1 = rp1 + (1 − r)p2 , o2 = (1 − r)p1 + rp2 , where
r ∈ [0, 1] is random number. Let O1 stands for the set of all these oﬀspring.
Sept 4 (Mutation). Gaussian mutation is executed, and the oﬀspring set is denoted by O2.
Step 5 (Constraint-handling). Let O = O1 ∪ O2. If any point in O is not in
S, then arbitrarily choose η ∈ N to replace a point in O. Apply the proposed
ˆ such
constraint-handling method to modify each point τ ∈ O which is not in S,
ˆ
that all points in O are in S. Let N = {(x, y) ∈ S ∩ O} and = θ , θ ∈ [0, 1].
Step 6 (Improving oﬀspring by the second-phase evolving). For each point (x, y) ∈
N ⊂ O, let d be a descent direction of f (x, y) in y for x ﬁxed. Take ρ > 0 such
that y¯ = y + ρd reaches the boundary of the feasible region of the follower’s
problem for x ﬁxed. Replace (x, y) by (x, y¯) in O.
Step 7 (Selection). Evaluate the ﬁtness values of all points in O. Select the best
n1 points from the set pop(k) ∪ O and randomly select Np − n1 points from the
remaining points of the set. All these selected points form the next population
pop(k + 1).
Step 8. If the termination condition is satisﬁed, then stop; Otherwise, let k =
k + 1, go to Step 3.

5

Simulation Results

In this section, 10 benchmark problems F 1−F 10 are selected from the references
[3,4,5,6,7,8] for simulation. In order to demonstrate the eﬀectiveness of the proposed algorithm on the BLPPs with nondiﬀerentiable leader level functions, we
construct two benchmark problems F 11 and F 12 only by replacing the leader’s
objective functions in F 8 and F 10 by F (x, y) = |sin(2x1 + 2x2 − 3y1 − 3y2 − 60)|
and minx F (x, y) = |sin((x1 −30)2 +(x2 −20)2 −20y1 +20y2 −225)|, respectively.
The parameters are chosen as follows: Np = 30, the crossover probability
pc = 0.8, the mutation probability pm = 0.3, n1 = 10, μ = 1, the initial =
(1, · · · , 1) ∈ Rp+q , θ = 0.7 for k ≤ kmax /2, while θ = 0 for k > kmax /2, where k
represents generation number, while kmax the maximum generation number. For
F 1 − F 3, F 5, F 8, F 9 and F 11, kmax = 50, while for other problems, kmax = 100
. We execute Algorithm 1 in 30 independent runs on each problem, and record
the following data: (1) leader’s(follower’s) objective values F (x∗ , y ∗ )(f (x∗ , y ∗ ))
at the best solution; (2) the leader’s objective function value F (¯
x, y¯) at the worst
point (¯
x, y¯); (3) mean value of F (x, y) in all 30 runs(denoted by Fmean in short).

1162

H. Li and Y. Wang

Table 1. Comparison of the results found by Algorithm 1 and the related algorithms

N o.
F 1[5]
F 2[3]
F 3[4]
F 4[4]
F 5[4]
F 6[4]
F 7[6]
F 8[7]
F 9[6]
F 10[8]
F 11
F 12

F (x∗ , y ∗ )
f (x∗ , y ∗ )
Algorithm 1 Ref. Algorithm 1 Ref.
−9
NA
−54
NA
2
2
12
12
1000
1000
1
1
−1.2098
3.57
7.617
2.4
100.003
100.58
0
0.001
81.3262
82.44
−0.3198 0.271
0
0
5
5
0
5
200
0
469.1429 469.1429
8.8571
8.8571
225
225
100
100
0
NA
200
NA
0
NA
100
NA

Fmean
−9
2
1000
−1.2096
100.012
81.3263
0
0
469.1429
225
0
0

F (¯
x, y¯)
−9
2
1000
−1.2090
100.039
81.3266
0
0
469.1429
225
0
0

All results are presented in Table 1, where NA means that the result is not
available for the algorithms and Ref. stands for the related algorithms in references. It can be seen from Table 1 that for F 4, F 5, F 6 and F 8, the best results
found by Algorithm 1 are better than those by the compared algorithms. For
F 11 and F 12, Algorithm1 found the optimal solutions. For other problems, the
best results found by Algorithm1 are almost as good as those by the compared
algorithms.

References
1. Colson, B., Marcotte, P., Savard, G: Bilevel programming: A survey. A Quarterly
Journal of Operations Research(4OR) 3(2005) 87–107
2. Wang, Yuping, Jiao, Yong-Chang, Li, Hong : An Evolutionary Algorithm for Solving
Nonlinear Bilevel Programming Based on a New Constraint-Handling Scheme. IEEE
Transactions on Systems, Man, and Cybernetics(C) 35(2)(2005) 221–232
3. Bard, J. F.: Practical Bilevel Optimization. Norwell, Kluwer, MA (1998)
4. Oduguwa, V., Roy, R.: Bi-level optimization using genetic algorithm. Proceeds of
the 2002 IEEE International Conference on Artiﬁcial Intelligent Systems (ICAIS02)
(2002) 123–128
5. Zheng, P.-E.: Hierarchical optimization algorithm-based solutions to a class of bilevel
programming problems. Systems Engineering and Electronics 27(4) (2005) 663–665
6. Li, H., Wang, Yuping: A hybrid genetic algorithm for nonlinear bilevel programmings. J. Xidian University 29(6) (2002) 840–843
7. Aiyoshi, E., Shimuzu, K.: A solution method for the static con-strained Stackelberg problem via penalty method. IEEE Trans. Autom. Control AC-29(12) (1984)
1112-1114
8. Shimizu, K., Aiyoshi, E.: A new computational method for Syackelberg and minmax problems by use of a penalty method. IEEE Trans. Autom. Control AC-26(2)
(1981) 460–466

