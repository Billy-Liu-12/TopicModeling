The Shannon Entropy-Based Node Placement
for Enrichment and Simpliﬁcation of Meshes
Vladimir Savchenko1, Maria Savchenko2, Olga Egorova3, and Ichiro Hagiwara3
1

Hosei University, Tokyo, Japan
vsavchen@k.hosei.ac.jp
2
InterLocus Inc.Tokyo, Japan
savchenko.m.aa@m.titech.ac.jp
3
Tokyo Institute of Technology, Tokyo, Japan
egorova.o.aa@m.titech.ac.jp, hagiwara@mech.titech.ac.jp

Abstract. In this paper, we present a novel simple method based on
the idea of exploiting the Shannon entropy as a measure of the interinﬂuence relationships between neighboring nodes of a mesh to optimize
node locations. The method can be used in a pre-processing stage for
subsequent studies such as ﬁnite element analysis by providing better
input parameters for these processes. Experimental results are included
to demonstrate the functionality of our method.
Keywords: Mesh enrichment, Shannon entropy, node placement.

1

Introduction

Construction of a geometric mesh from a given surface triangulation has been
discussed in many papers (see [1] and references therein). Known approaches
are guaranteed to pass through the original sample points that are important in
computer aided design (CAD). However, results of triangulations drastically depend on uniformity and density of the sampled points as it can be seen in Fig.1.
Surface remeshing has become very important today for CAD and computer
graphics (CG) applications. Complex and detailed models can be generated by
3D scanners, and such models have found a wide range of applications in CG
and CAD, particularly in reverse engineering. Surface remeshing is also very
important for technologies related to engineering applications such as ﬁnite element analysis (FEA). Various automatic mesh generation tools are widely used
for FEA. However, all of these tools may create distorted or ill-shaped elements,
which can lead to inaccurate and unstable approximation. Thus, improvement
of the mesh quality is an almost obligatory step for preprocessing of mesh data
in FEA. Recently, sampled point clouds have received much attention in the CG
community for visualization purposes (see [2], [3]) and CAD applications (see [4],
[5], [6]). A noise-resistant algorithm [6] for reconstructing a watertight surface
from point cloud data presented by Kolluri et al. ignores undersampled regions;
nevertheless, it seems to us that some examples show that undersampled areas
need an improvement by surface retouching or enrichment algorithms. In some
Y. Shi et al. (Eds.): ICCS 2007, Part II, LNCS 4488, pp. 65–72, 2007.
c Springer-Verlag Berlin Heidelberg 2007

66

V. Savchenko et al.

applications, it is useful to have various, for instance, simpler versions of original
complex models according to the requirements of the applications, especially, in
FEA. In addition to the deterioration in the accuracy of calculations, speed may
be sacriﬁced in some applications. Simpliﬁcation of a geometric mesh considered
here involves constructing a mesh element which is optimized to improve the
elements shape quality using an aspect ratio (AR) as a measure of the quality.

(a)

(b)

Fig. 1. Surface reconstruction of a ”technical data set”. (a) Cloud of points (4100
scattered points are used). (b)Triangulation produced by Delaunay-based method (N
triangular elements: 7991, N points: 4100).

In this paper, we present an attempt of enrichment of mesh vertices according
to AR-based entropy which is the analog of the Shannon entropy [7]. Further it
is called A-entropy. We progressively adapt the new coming points by performing elementary interpolation operations proposed by Shepard [8] (see also [9] for
more references) for generating the new point instances until an important function If (in our case, a scalar which speciﬁes the ideal sampling density) matches
some user-speciﬁed values. To optimize node coordinates during simpliﬁcation
process (edge collapsing), A-entropy is also implemented.
Recently, a wide scope of papers addressed the problem of remeshing of triangulated surface meshes, see, for instance, [10], [11] and references therein where
surface remeshing based on surface parameterization and subsequent lifting of
height data were applied. However, the main assumption used is that geometric
details are captured accurately in the given model. Nevertheless, as it can be
seen from the Darmstadt benchmark model (technical data set shown in Fig. 1),
a laser scanner often performs non-uniform samples that leads to under-sampling
or the mesh may have holes corresponding to deﬁciencies in point data. In theory, the problem of surface completion does not have a solution when the plane
of the desirable triangulation is not planar; and, especially, begins to go wrong
when the plane of triangulation is orthogonal to features in the hole boundary
(so called, crenellations features). See a good discussion of the problem in [12].
Let us emphasize that our approach is diﬀerent from methods related to reconstruction of surfaces from scattered data by interpolation methods based, for
instance, on minimum-energy properties (see, for example, [13]). In our case, an

The Shannon Entropy-Based Node Placement

67

approximation of the original surface (triangular mesh) is given. In some applications, it is important to preserve the initial mesh topology. Thus, our goal
is to insert new points in domains where the If function does not satisfy the
user-speciﬁed value. The main contribution of the paper is a novel algorithm of
a vertex placement which is discussed in details in Section 2.

2

Vertex Placement Algorithm

The approach is extremely simple and, in theory, is user dependent. In an analogy
to a hole ﬁlling, the user deﬁnes an area, where enrichment may be done, that
is, the user selects a processing area with no crenellations. In practice, all object
surface can be considered as the processing area (as it has been done in the
example shown in Fig. 1).

Fig. 2. Scheme of a new point generation. p1 , p2 , pi are points of the neighborhood.
The dashed line is a bisector of the ”empty” sector, g is the generated point.

After that the algorithm works as follows:
1. Deﬁne a radius of sampling Rs as an analog of the point density; for instance,
for the ”technical data set” the radius equal to 1. It can be done automatically
by calculating average point density.
2. For each point of the user-deﬁned domain, select K nearest points p that are
in Rs neighborhood. If K is less (or equal) than the user-predeﬁned number of
the neighborhood points (in our experiments - 6) and the maximum angle of the
”empty” area is larger (or equal) the user-predeﬁned angle (in our experiments
- 900 ), generate a new point g with the initial guess provided by a bisector of
the ”empty” sector as shown in Fig. 2.
3. Select a new neighborhood of the point g; it can be slightly diﬀerent from the
initial set of points. This is done in the tangent plane (projective plane) deﬁned
by neighborhood points.
4. Perform a local Delaunay triangulation.
5. Find points forming a set of triangles with the common node g (a star) as
shown in Fig. 3(a). Calculate the new placement of the center of the star g using
technique described below (Fig. 3(b)).

68

V. Savchenko et al.

6. And on the lifting stage, calculate the local z-coordinates of g by Shepard
interpolation. In our implementation, we use the compactly supported radial
basis function [14] as the weight function.

(a)

(b)

Fig. 3. (a) An initial star. (b) The ﬁnal star.

The key idea of the ﬁfth step is to progressively adapt the newly created points
throw a few iterations. That is, an area with low sampling density will be ﬁlled
in accordance with points generated on the previous steps. In order to obtain
a ”good” set of the new (approximated) points coordinates, we need a measure
of a ”goodness” of triangulations arising from randomly coming points. It is
natural to use a mesh quality parameter, AR of the elements of a star, for such
a measure. In the case of a triangular mesh, AR can be deﬁned as a ratio of the
maximum edge length to the minimum edge length of an element. Nevertheless,
according to our experiments it is much better to use an information Mi (Mi is
the AR of the i-th triangle of the star) associated with respect to a point g (Fig.
3) of the star in an analogy with the Shannon entropy [8], which deﬁnes the
uncertainty of a random variable, and can be a natural measure for the criterion
used in the enrichment algorithm. Shannon deﬁned the entropy of an ensemble
of messages: if there are N possible messages that can be sent in one package,
and message m is being transmmited with probability pm , then the entropy is
as follows
N

S=−

pm log (pm ) .

(1)

1

Intuitively, we can use AR-based entropy, with respect to the point g as follows
N

S=−

Mi /Mt log (Mi /Mt ) ,

(2)

i=0

where Mt is the summarized AR value of a star, N is the number of faces of
the star. From the statistical point of view, a strict deﬁnition of the Shannon
entropy for a mesh, which we denoted as A-entropy and used in our algorithm,
is provided as follows: consider discrete random variable ξ with distribution:

The Shannon Entropy-Based Node Placement

x1 x2 ... xn
p1 p2 ... pn

69

( )

where probabilities pi =P{ξ=xi }. Then divide an interval 0 ≤ x < 1 into such
intervals Δi that the length of Δi equals to pi . Random variable ξ is deﬁned as
ξ = xi , when γ ∈ Δi has distribution ( ). Suppose we have a set of empirically
received numbers γ1 = a1 ,...,γn = an written in its increasing order, where ai
is the AR of the i-th element of the neighborhood with the point g as a center.
Let these numbers deﬁne a division of an interval a1 ≤ x < an into Δi = ai
- ai−1 . In our case, the parameter ai has its minimal value equal to 1, which
is not necessarily achieved in given sampling data. Constructing the one-to-one
correspondence between 1 ≤ x < an and 0 ≤ x < 1 , the following probabilities
can be written:
p1 =

a1 − 1
a2 − a1
an − an−1
, p2 =
, pn =
...
an − 1
an − 1
an − 1

Thus, we can deﬁne the random variable with the distribution as follows
a1 a2 ... an
p1 p2 ... pn

.

Its probability values are used in formula (3) for A-entropy:
N

A=−

pi log (pi ) , pi =
1

ai − ai−1
, p0 = 1.
an − 1

(3)

The value A of A-entropy depends on the coordinates of the center of the star
(point g in Fig. 3). Thus, the problem of maximization of the value A is reduced
to the problem of ﬁnding the new coordinates of this center (Fig. 3(b)) and is
considered as the optimization problem. For solving this optimization problem,
we use the downhill simplex method of Nelder and Mead [15].

3

Experimental Results

In practice, as it can be seen in Fig. 4, implementation of the algorithm discussed
above leads to a reasonable surface reconstruction of areas with initially low sampling density (see Fig. 1). The number of scattered points in the initial model
is 4100, after enrichment the number of points was increased up to 12114. For
decreasing the number of points we simplify this model and the ﬁnal number of
points is 5261.
Our triangular mesh simpliﬁcation method uses predictor-corrector steps for
predicting candidates for edge collapsing according to a bending energy [16] with
the consequent correction of the point placement in simpliﬁed mesh. Let us notice that the main idea of our simpliﬁcation approach is to provide minimal local
surface deformation during an edge collapse operation.

70

V. Savchenko et al.

(a)

(b)

(c)

Fig. 4. The mechanical data set. (a) Mesh after enrichment. (b) Mesh after simpliﬁcation. (c) Shaded image of the ﬁnal surface.

At each iteration step:
- candidate points for an edge collapse are deﬁned according to a local decimation cost of points belonging to a shaped polygon.
- after all candidates have been selected, we produce a contraction of the edge
with choosing an optimal vertex position by using A-entropy according to the
ﬁfth step of the algorithm (see Section 2).
To detail the features of the proposed point placement scheme, Fig. 5 presents
results of applying well known or even classical surface simpliﬁcation algorithms
(tools can be found in [17]) and our method. We show fragment of a ”Horse”
model (the initial AR value is equal to 1.74; here and further, the average value
of the AR is used) after the mesh simpliﬁcation produced by the diﬀerent simpliﬁcation techniques.

(a)

(b)

(c)

(d)

Fig. 5. Mesh fragments of the ”Horse” model after simpliﬁcation (∼13% of original
elements) by using: (a) Progressive simpliﬁcation method; (b) Method based on a
global error bound; (c) Method based on a quadric error metric; (d) Our method

The volume diﬀerence between the initial model and simpliﬁed one by our
technique is 0.8%; the ﬁnal AR value is equal to 1.5. The global error bound
method demonstrates the worst results; the volume diﬀerence is 1.3%, the ﬁnal
AR value is equal to 2.25. At a glance of the model visualization and the volume preservation, the best method, without any doubt, is the method based on
the quadric error metric, see [18]. However, there is a tradeoﬀ between attaining a

The Shannon Entropy-Based Node Placement

71

high quality surface reconstruction and minimization of AR. As a result, the
ﬁnal AR value is equal to 2 and many elongated and skinny triangles can be
observed in the mesh.

4

Concluding Remarks

In this paper, we introduce the notion of AR-based entropy (A-entropy) which
is the analog of the Snannon entropy. We consider the enrichment technique and
the technique for improving the mesh quality which are based on this notion.
The mesh quality improvement in presented simpliﬁcation technique can be compared with smoothing methods based on the averaging of the coordinates, such
as Laplacian [19] or an angle based method of Zhou and Shimada [20]. These
methods have an intrinsic drawback such as a possibility of creating inverted triangles. In some non-convex domains, nodes can be pulled outside the boundary.
Implementation of the entropy-based placement in simpliﬁcation algorithm decreases a possibility that a predicted point does not create an inverted triangle,
but does not guarantee that such event does not occur at all. However, producing operations in the tangent plane allows suﬃciently easy avoiding creation of
inverted triangles. Interpolation based on the Shepard method produces excessive bumps. In fact, it is a well known feature of the original Shepard method.
More sophisticated local interpolation schemes such as [21] and others can be
implemented to control the quality of interpolation. Matters related to feature
preserving shape interpolation have to be considered in the future. We have
mentioned that it might be natural to use AR (the mesh quality parameter) of
the elements of a star as a measure for providing a reasonable vertex placement.
Nevertheless, we would like to emphasize that according to our experiments, in
many cases it does not lead at all to a well-founded estimate of a point g. It
might be a rational approach to use the Shannon entropy as a measure of the
inter-inﬂuence relationships between neighboring nodes of a star to calculate optimal positions of vertices. We can see in Fig. 5 that shapes of mesh elements
after implementation of our method diﬀer signiﬁcantly from results of applying
other simpliﬁcation methods. Meshes in Fig. 5(a, b, c) are more suitable for
visualization than for numerical calculations. Improvement of meshes of a very
low initial quality, for instance, the ”Horse” model simpliﬁed by the global error bound method, takes many iteration steps to attain AR value close to our
result and after implementation of Laplacian smoothing to the model shown in
Fig. 5(b) the shape of the model is strongly deformed. After implementation of
Laplacian smoothing (300 iteration steps) to the ”Horse” model, simpliﬁed by
the quadric error metric method, AR and volume diﬀerence between the original model and improved one become 1.6 and 5.2%, correspondingly. Examples
demonstrated above show that the mesh after applying our method is closer to
a computational mesh and can be used for FEA in any ﬁeld of study dealing
with isotropic meshes.

72

V. Savchenko et al.

References
1. Frey P. J.: About Surface Remeshing. Proc.of the 9th Int.Mesh Roundtable (2000)
123-136
2. Alexa, M., Behr, J.,Cohen-Or, D., Fleishman, S., Levin, D., Silvia, C. T.: Point
Set Surfaces. Proc. of IEEE Visualization 2001 (2002) 21-23
3. Pauly, M., Gross, M., Kobbelt, L.: Eﬃcient Simpliﬁcation of Point-Sampled Surfaces. Proc. of IEEE Visualization 2002(2002) 163-170
4. Hoppe, H., DeRose, T., Duchamp, T., McDonald, J.,Stuetzle, W.: Surface Reconstruction from Unorganized Points. Proceedings of SIGGRAPH 92 (1992) 71-78
5. Amenta, N., Choi, S., Kolluri, R.: The Powercrust. Proc. of the 6th ACM Symposium on Solid Modeling and Applications (1980) 609-633
6. Kolluri, R., Shewchuk, J.R., O’Brien, J.F.: Spectral Surface Reconstruction From
Noisy Point Clouds. Symposium on Geometry Processing (2004) 11-21
7. Blahut, R.E.: Principles and Practice of Information Theory. Addison-Wisley
(1987)
8. Shepard, D.: A Two-Dimensional Interpolation Function for Irregularly Spaced
Data. Proc. of the 23th Nat. Conf. of the ACM (1968) 517-523
9. Franke, R., Nielson, G.,: Smooth Interpolation of Large Sets of Scattered Data.
Journal of Numerical Methods in Engineering 15 (1980) 1691-1704
10. Alliez, P., de Verdiere, E.C., Devillers, O., Isenburg, M.: Isotropic Surface Remeshing. Proc.of Shape Modeling International (2003)49-58
11. Alliez, P., Cohen-Steiner, D., Devillers, O., Levy, B., Desburn, M.: Anisotropic
Polygonal Remeshing. Inria Preprint 4808 (2003)
12. Liepa, P.: Filling Holes in Meshes. Proc. of 2003 Eurographics/ACM SIGGRAPH
symp.on Geometry processing 43 200-205
13. Carr, J.C., Mitchell, T.J., Beatson, R.K., Cherrie, J.B., Fright, W.R., McCallumn,
B.C., Evans, T.R.: Filling Holes in Meshes. Proc.of SIGGRAPH01 (2001) 67-76
14. Wendland, H.: Piecewise Polynomial, Positive Deﬁned and Compactly Supported
Radial Functions of Minimal Degree. AICM 4 (1995) 389-396
15. Nelder, J.A., Mead, R.: A simplex Method for Function Minimization. Computer
J. 7 (1965) 308-313
16. Bookstein, F.L.: Morphometric Tools for Landmarks Data. Cambridge University
Press (1991) Computer J. 7 (1965) 308-313
17. Schroeder, W., Martin, K., Lorensen,B.: The Visualization Toolkit. Ed.2 Prentice
Hall Inc. (1998)
18. Garland, M.: A Multiresolution Modeling: Survey and Future Opportunities. Proc.
of EUROGRAPHICS, State of the Art Reports (1999)
19. Bossen, F.J., Heckbert, P.S.: A Pliant Method for Anisotropic Mesh Generation.
Proc. of the 5th International Meshing Roundtable (1996) 63-74
20. Zhou, T., Shimada, K.: An Angle-Based Approach to Two-dimensional Mesh
Smoothing. Proc.of the 9th International Meshing Roundtable (2000) 373-384
21. Krysl, P., Belytchko, T.: An Eﬃcient Linear-precision Partition of Unity Basis
for Unstructured Meshless Methods. Communications in Numerical Methods in
Engineering 16 (2000) 239-255

