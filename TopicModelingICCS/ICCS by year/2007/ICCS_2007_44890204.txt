Heterogeneous Workﬂows in Scientiﬁc Workﬂow
Systems
Vasa Curcin, Moustafa Ghanem, Patrick Wendel, and Yike Guo
Department of Computing, Imperial College London

Abstract. Workﬂow systems are used to model a range of scientiﬁc and
business applications, each requiring a diﬀerent set of capabilities. We
analyze how these heterogeneous approaches can be resolved, look at
how existing workﬂow systems address this and present the solution in
Discovery Net, which combines three levels of workﬂows, control, data
and grid, at diﬀerent levels of abstraction.

1

Introduction

A disctinction is often drawn between workﬂow systems based on their intended
aim. Typically, the two types isolated are dubbed business and scientiﬁc workﬂows, with the former concentrating on increasing eﬃciency within an organization, while the latter are concerned with the fostering of innovation in scientiﬁc
environments. While the two are not neccessarily in direct conﬂict and even
share some common areas of interest such as automation, provenance tracking
and collaboration, they do branch into diﬀerent ﬁelds. The business workﬂow
ﬁeld revolves around issues such as messaging protocols, process optimization,
and sophisticated patterns involved in component interaction. Scientiﬁc workﬂows, on the other hand include technologies which help bring the usability of
the workﬂow abstraction in scientiﬁc research up to the level of a research scientist, by investigating interactive analysis creation, process knowledge capture,
dissemination and reusability, and harnessing complex computational infrastructures within simple graphical metaphors. In addition to this separation, there are
several perspectives from which a workﬂow model can be observed [2]. Control
ﬂows describe the ﬂow of execution control between tasks, data ﬂows represent
a functional programming perspective, in which tasks are data transformations,
while some other perspectives, such as resource and operational are also possible.
Workﬂow technologies have also been used in the context of Grid computing,
mainly automating the submission and execution of user tasks on distributed
computing resources. The level of details captured in such workﬂows varies between diﬀerent systems. For example, within the Globus GT3 [3] project, GridAnt provides a workﬂow language with conditional, sequential, and parallel
constructs that are used to describe the detailed steps required for submission
and execution of user tasks on high performance resources. In contrast, within
the Pegasus [4] system, the VDL language is used to deﬁne workﬂows as dependency graphs between user tasks, and the Pegasus system then automatically
Y. Shi et al. (Eds.): ICCS 2007, Part III, LNCS 4489, pp. 204–211, 2007.
c Springer-Verlag Berlin Heidelberg 2007

Heterogeneous Workﬂows in Scientiﬁc Workﬂow Systems

205

maps the abstract workﬂows down to executable ones based on the available grid
resources.
The Discovery Net system [1] was designed primarily to support the analysis
of scientiﬁc data based on a distributed Web Service/Grid Service composition
methodology. Within this framework, computational services executing at remote locations can be treated as black boxes with known input and output
interfaces. Such services are connected together into acyclic graphs or workﬂows
deﬁned as sequences of operations where the outputs of one service act as inputs for other services. To initiate an execution, the user deﬁnes the termination
node in the workﬂow and data is pulled through the preceding nodes in a demand driven fashion. Data passed between the nodes can be either atomic or a
collection (e.g. a table of values), and in the latter case, each node will implicitly
iterate over the input contents. That original paradigm is best suited to capture
the semantics of a data ﬂow model of computation. Although the system allowed
the deﬁnition of parallel branches within a workﬂow to be formed (e.g. through
the availability of multiple output ports in a service), it supported no explicit
control ﬂow constructs (e.g. conditionals) or explicit iteration operations. Despite these restrictions, the system has been extremely successful in enabling
end users to create complex data analysis applications in various ﬁelds, including Life Sciences [6], Environmental Monitoring [5] and Geo-hazard Modelling
[7]. Some of the key features that enabled its wide uptake were rooted in its
support for a fully interactive model of workﬂow construction and deployment.
In this paper we describe the introduction of new extended features for supporting control ﬂow semantics within the Discovery Net system. The paper is
organized is follows. Section 2 reviews the main features of the scientiﬁc workﬂow systems that were described in the literature since the design of the original
Discovery Net system. In Section 3, we present and describe the notion of workﬂow embedding as a generic solution to heterogeneous workﬂow composition,
demonstrating it in Discovery Net through two new layers: a control ﬂow layer
and Grid control layer. Section 4 presents the conclusion and lays out the future
research directions.

2

Heterogeneity in Existing Workﬂow Systems

There are numerous criteria which we can use to characterize the workﬂow
execution semantics: atomic vs. streaming, push vs. pull, single vs. multiple
starting/ending points, implicit or explicit iterations over data sets etc. The
important thing to note is that all of these have their uses in some workﬂow
scenarios. However, we argue that the correct way of modelling systems with
multiple requirements is by implementing the heterogeneity in diﬀerent semantic levels, rather than providing all of the functionality as diﬀerent components
in the same environment. In this section, we will investigate how some popular
workﬂow systems address this issue.

206

2.1

V. Curcin et al.

Taverna

Taverna [8] is a graphical workﬂow authoring and execution environment, using
SCUFL as its workﬂow language, and Freeﬂuo as its enactment engine. SCUFL
(Simple Conceptual Uniﬁed Flow Language) was developed for the purposes of
the Taverna project, and can integrate any Java executable code as a component.
The basic unit of execution in SCUFL is a processor, which has associated with it
input and output ports. The processor may be regarded as a function from input
data to output data, with possible side eﬀects on the execution environment.
Data links between the components have a source processor and output port
name, a sink processor and an input port name. This type of link ensures the
basic consistency in pure dataﬂow execution. However, due to possible eﬀects on
the execution environment which are separate from inputs and outputs, explicit
ordering constraints are introduced, which are not based on the data dependency
between processors. These control constraints are useful when execution ordering
must be imposed between two processors but when there is no dataﬂow between
them. The execution of a SCUFL workﬂow will start from the nominated starting
points, sources, and ﬁnish when all end points, sinks, have either produced their
outputs or failed. Whether the workﬂow can execute partially (ie. if one sink
failed, should the others complete) is conﬁgurable by the user.
In addition to control constraints and data links SCUFL also has an indirect
conditional construct, which corresponds to if/else or case structure in procedural programming languages. This construct consists of a single input being
passed to multiple nodes, which are all connected downstream to the same component. The user has to ensure that only one of them will succeed and continue
the execution. If, however, for some reason multiple nodes execute and produce output, the downstream node will only process the ﬁrst input received.
The generic iteration construct is not supported, but one special case is the
implicit iteration. This form of execution happens when a single-item processor
receives a data collection, then the processor is executed once for each item in
the collection. This behaviour is equivalent to the map construct in functional
programming languages.
2.2

Triana

Triana [9] is a visual workﬂow-based problem solving environment, developed at
Cardiﬀ University. The functional component in Triana is called a unit. Units are
connected through cables to form workﬂows. The notion of hierarchical workﬂows is supported through grouping connected components into a higher-level
group unit. The created group unit implicitly has an associated control structure,
another unit, which can coordinate its executional behaviour.
In addition to numerous functional components, Triana provides control ﬂow
units that operate on the same level as the data ﬂow, and can be freely combined
with them. The looping is achieved through a dedicated Loop component, that
has two output ports. One typically connects to a functional unit, while the
other provides the ﬁnal output once the iteration is over. The Loop component

Heterogeneous Workﬂows in Scientiﬁc Workﬂow Systems

207

receives input data, evaluates its exit condition, and then passes the data either
to the exit port or to its functional unit. Also, Triana posseses a number of trigger
units, which can be used to send a signal on user action, at a ﬁxed time or after a
certain delay. Streaming execution is achieved using dedicated units for blocking,
merging, splitting, pausing, etc. There is no explicit distinction between data and
control components, placing them all on the same level. Semantics are left for the
user to design, using the large number of specialized units provided. This design
is opposed to languages such as YAWL and Kepler, which try to minimize the
number of control nodes, the former through formalizing them into a minimum
neccessary set, and the latter by separating control from the nodes altogether.
2.3

YAWL

YAWL (Yet Another Workﬂow Language) has its origins in theoretical work
on workﬂow patterns [2] comparing a number of (mostly business) workﬂow
systems. and was not developed for the purposes of an application project. Realizing that all the patterns described can be implemented in high-level Petri nets,
albeit with some diﬃculty in cases of patterns with multiple process instances,
advanced synchronization and cancellation, YAWL was based on high-level Petri
nets, in order to preserve their beneﬁts, but with extensions to help direct support of the special cases mentioned. YAWL’s formal semantics are in contrast
with other languages which either have no formal semantics (Triana, XPDL)
or they constructed post hoc (Taverna). While the control ﬂow perspective of
YAWL is the most investigated one, the language also supports the data and resource perspective by allowing input and output parameters in the components
to be connected to global variables.
The core component concept in YAWL is derived from the Petri Nets, with
the workﬂow being a set of tasks, conditions and ﬂows between them. Unlike
Petri Nets, tasks can be connected to each other directly, without a condition
inbetween (or an implicit condition that automatically succeeds). A task in a
workﬂow can be either atomic or composite, with composite tasks containing
another workﬂow (extended workﬂow net in YAWL terminology) within them.
This corresponds exactly to the grouping concept present in Discovery Net, Taverna, Kepler, Triana and some other systems. There are six explicit branching
constructs: three splits (AND, XOR and OR) and three joins (AND, XOR and
OR), which model every legal data routing through the workﬂow. Due to the
nature of splits the execution path through the workﬂow is determined dynamically at runtime, as opposed to being apriori statically determined. Both looping
and conditional constructs are achieved using explicit conditions which evaluate
the state of the workﬂow and direct the execution accordingly. So, there are
no if/else or while components but the structure of the graph can create such
behaviour.
The data ﬂow in YAWL is achieved via variables, and can be split into internal
and external data transfers. Internal transfers are always performed between the
tasks and their workﬂows, since all variables inside tasks are internal to that
task. So, in order to communicate some data between tasks A and B, task A

208

V. Curcin et al.

has to register its variable as the output parameter, and pass it to some global
workﬂow variable N, which task B will take as its input parameter. External
transfers occur between global variable and the user or an external component,
such as a web or Grid service.
2.4

Kepler

Kepler [11] is a scientiﬁc workﬂow construction, composition, and orchestration
engine, focusing on data analysis and modelling. This focus inﬂuenced the design in that it is suitable for modelling a wide variety of scientiﬁc domains, from
physics via ecosystems to bioinformatics web services. Instead of trying to provide a generic semantic for all possible types of processes encountered in these
domains, Kepler externalizes the execution engine from the workﬂow model, and
assigns one director to each model, that then coordinates the model execution.
The workﬂow components in Kepler are represented by actors with ports
that can be input, output, or mixed. Tokens are basic data containers, and they
are passed from the output port of one actor to another through the relation
connections. The number of tokens consumed and produced depends on the
node used. Directors are the key concept in Kepler. While actors and relations
together constitute a workﬂow model, the directors form the execution model,
or the model of computation. In this setup, actors’ intelligence stretches as far as
knowing its inputs, the operation to be performed on them and what outputs to
produce. The decision when to schedule the execution of each actor is left to the
director. Therefore, depending on the director used, the actors may have separate
threads of control, or they may have their executions triggered by the availability
of new input, in a more conventional dataﬂow manner. The architecture in which
components are agnostic to the manner in which they are executed is formalized
as behavioural polymorphism.
Kepler supports four director types. SDF - Synchronous Dataﬂow is characterized by ﬁxed token production and consumption rates per ﬁring. The actor
is invoked as soon as all inputs have data, which is possible to know since all
actors have to declare their token production before the execution. Therefore,
the order of execution is statically determined from the model, and components
cannot change the routing of tokens during execution. PN - Process Network
is a derestricted variant of SDF, in that the actor is invoked when the data arrives. However, there is no requirement that all data has to be present, which
results in a more dynamic environment, where actors are executing in parallel
and sending each other data when and if needed. The tokens are created on
output ports whenever input tokens for an actor are available and the outputs
can be calculated. The output tokens are then passed to connected actors where
they are held in a buﬀer until that next actor can ﬁre. The workﬂow is thus
driven by data availability. CT - Continuous Time introduces the notion of
time that is aﬃxed to tokens in order to perform system simulations. The system
is typically based on diﬀerential equations, and start conditions, which are then
used to predict the state at some speciﬁed future time. The data tokens that
are passing through the system then have a timestamp that the director is using

Heterogeneous Workﬂows in Scientiﬁc Workﬂow Systems

209

to determine the step and the stop condition. DE - Discrete Event director
is working with timestamps, however, they are not used to approximate functions and schedule executions, but to measure average wait times and occurrence
rates.

3

Discovery Net Workﬂow Embedding

The key new feature of Discovery Net 3.0 is the layered approach for deﬁnition,
embedding and execution of scientiﬁc workﬂows. The top, control ﬂow, layer is
introducing new control operators for the coordination of the traditional data
ﬂow operations. This layer enables the execution of distributed applications with
scheduling dependencies more advanced than the simple data availability criteria, and where the data passing between components need to be restricted due to
volume or costs associated with transfers. The middle layer corresponds to the
traditional Discovery Net data ﬂow layer enabling data integration, transformation and processing using distributed services. The bottom layer, Grid Control,
enables the access and control of Grid resources.

Fig. 1. Hierarchical workﬂow composition

3.1

Control Flow Layer

This layer includes a range of control ﬂow elements such as branching synchronisation, conditional branching and looping. The control ﬂow mechanisms use
a diﬀerent execution scheme to the standard workﬂow execution engine; hence
the control ﬂow components cannot be connected to data ﬂow components, but
rather they orchestrate data ﬂows, specifying which will be the next data ﬂow

210

V. Curcin et al.

to run based on the control decision. The key new control ﬂow operations introduced in Discovery Net include:
– Test Generic condition construct, where the user speciﬁes the condition, and
indicates which branch is to be executed, depending on the outcome.
– Wait for All Provides synchronisation allowing waiting for all arriving tasks
to be completed before a workﬂow can proceed.
– Wait for One Similar to Wait for All, except that it executes the node,
once the ﬁrst of the arriving tasks is complete.
– For Explicit looping construct, which loops through activities a speciﬁed
number of times - a syntactic shortcut for a generic loop implementable
using the former three components.
– While Loops through activities while a variable is true, also a syntactic
shortcut.
Firstly, a notion of token was introduced, in which each component may receive
a token, which will cause an execution of its instance. Multiple tokens may exist
in the workﬂow at any one time, and even in a single component. Secondly,
the execution switched from the pull-based model to a push-based one, with a
dynamic ﬂow of control – nodes making runtime decisions about the direction
which the execution will take. Furthermore, tokens have access to the component
that produced them, thereby giving a link to the result of a previous execution,
if needed.
Interestingly enough, even though the control ﬂow does not ﬁt into the
workﬂow-as-service model, so strongly upheld on the dataﬂow level, it utilizes it
in order to have a uniform execution paradigm for its components. Namely, the
execution node in the control ﬂow is the only component which can perform an
action outside of the scope of the ﬂow being executed. The way it operates is by
executing an internal dataﬂow, with a nominated output, which is used to produce and ﬁre a token once the execution is complete. The output is nominated
by declaring it as the output of the service created from the inner workﬂow. So,
the atomic unit of execution inside the control ﬂow is actually a dataﬂow-based
service.
3.2

Grid Control Layer

The role of the bottom layer is to enable access and control of remote Grid computing resources. Speciﬁcally, within this layer, sub-workﬂows are used to control
selection of resources as well as authentication, job submission, job invocation,
collection of results as well as session management. An example implementation
of these operations for the GRIA middleware is described in details in [10].

4

Summary

This paper discussed the need for heterogeneity in workﬂow systems, reviewed the
approaches diﬀerent scientiﬁc workﬂow systems take, and introduced the hierarchical approach to combining control and data structures in Discovery Net.

Heterogeneous Workﬂows in Scientiﬁc Workﬂow Systems

211

Discovery Net adopts a conservative approach to its dataﬂow semantic, insisting on a single-output static execution model, corresponding to a function call,
with no non-determinism in the execution ﬂow. All non-deterministic elements,
such as conditional and looping structures, are implemented in a separate level,
as well as strategies for GRID execution which involve scheduling and service
discovery. This is in sharp contrast to other systems analyzed, which are either enriching a dynamic control ﬂow structure with global variables (YAWL),
introducing dataﬂow-ﬂavoured control elements (Taverna), placing all the combinations of functionality/semantics into separate nodes (Triana). The Kepler
approach of letting the user determine the hierarchical composition of diﬀerent
semantics, is the closest to the one presented here, but it allows for some distinctly non-workﬂow systems to be built. In the future work, we plan to analyze
how multiple execution semantics interact in a hierarchical model, by looking at
error handling, user interaction and process provenance. The complete picture
of the Discovery Net model will then allow us to compare and contrast workﬂow
executions with other scientiﬁc workﬂow systems, producing formal notions of
equivalences between these systems.
Acknowledgement. The authors would like to thank Dr. Yong Zhang and Mr.
Nabeel Azam for the helpful discussions on the embedding implementation.

References
1. AlSairaﬁ et al: The Design of Discovery Net: Towards Open Grid Services for
Knowledge Discovery, High Performance Computing Applications, vol 17, no 3,
(2003) 297–315
2. Aalst van der et al: Advanced Workﬂow Patterns, 7th International Conference on
Cooperative Information Systems, vol 1901, Lecture Notes in Computer Science
(2000) 18–29
3. Ian T. Foster: Globus Toolkit Version 4: Software for Service-Oriented Systems,
NPC (2005) 2–13
4. Deelman, E. et al: Pegasus: Mapping Scientiﬁc Workﬂows onto the Grid, Lecture
Notes in Computer Science : Grid Computing, (2004) 11–20
5. Richards et al: Grid-based analysis of air pollution data, Ecological Modelling, vol
194, no 1–3 (2006) 274–286
6. Rowe et al: The discovery net system for high throughput bioinformatics, Bioinformatics, vol 19, no 90001, (2003) 225–231
7. Guo et al: Bridging the Macro and Micro: A Computing Intensive Earthquake
Study Using Discovery Net, Proceedings of SC2005 ACM/IEEE (2005)
8. Hull et al: Taverna: A tool for building and running workﬂows of services, Nucleic
Acids Research, Web Server Issue, vol 34, (2006) W729–W732
9. Taylor, Ian et al: Visual Grid Workﬂow in Triana Journal of Grid Computing, vol
3, no 3-4, (2005) 153–169,
10. Ghanem, M. et al.: Grid-enabled workﬂows for industrial product design, 2nd IEEE
International Conference on e-Science and Grid Computing (2006)
11. Ludaescher, B. et al.:Scientiﬁc Workﬂow Management and the Kepler System,
Concurr. Comput. : Pract. Exper., vol 18, no 10, (2006) 1039–1065

