epsilon-Support Vector and Large-Scale Data Mining
Problems*
Gang Kou1, 2, Yi Peng2, **, Yong Shi2, 3, and Zhengxin Chen2
1
Thomson Co., R&D, 610 Opperman Drive, Eagan, MN 55123, USA
College of Information Science & Technology, University of Nebraska at Omaha, Omaha, NE
68182, USA
{gkou, ypeng, yshi, zchen}@mail.unomaha.edu
3
Chinese Academy of Sciences Research Center on Data Technology & Knowledge Economy,
Graduate University of the Chinese Academy of Sciences, 100080, China
Tel.: ++1-402-4030269

2

Abstract. Data mining and knowledge discovery has made great progress
during the last fifteen years. As one of the major tasks of data mining,
classification has wide business and scientific applications. Among a variety of
proposed methods, mathematical programming based approaches have been
proven to be excellent in terms of classification accuracy, robustness, and
efficiency. However, there are several difficult issues. Two of these issues are
of particular interest of this research. The first issue is that it is challenging to
find optimal solution for large-scale dataset in mathematical programming
problems due to the computational complexity. The second issue is that many
mathematical programming problems require specialized codes or programs
such as CPLEX or LINGO. The objective of this study is to propose solutions
for these two problems. This paper proposed and applied mathematical
programming model to classification problems to address two aspects of data
mining algorithm: speed and scalability.
Keywords: Data mining, Classification, Mathematical Programming, Multiple
criteria decision making, Support Vector Machine.

1 Introduction
Over the years, optimization-based algorithms have shown their effectiveness in data
mining classification (e.g., Bugera, Konno, and Uryasev, 2002) and Mathematical
programming (MP) based algorithm is one of the optimization-based classification
algorithms (e.g., Kou et al 2006). Nevertheless, due to the limitation of computation
power and memory, it is difficult to apply the MP algorithm, or similar optimization
algorithms, to huge datasets which may contain millions of observations. As the size
*

This research has been partially supported by grants #70621001, #70531040,#70472074,
National Natural Science Foundation of China; 973 Project #2004CB720103, Ministry of
Science and Technology, China; and BHP Billiton Co., Australia.
**
The corresponding author.
Y. Shi et al. (Eds.): ICCS 2007, Part III, LNCS 4489, pp. 874–881, 2007.
© Springer-Verlag Berlin Heidelberg 2007

epsilon-Support Vector and Large-Scale Data Mining Problems

875

of today's databases is continuously increasing, it is highly important that data mining
algorithms are able to perform their functions regardless of the sizes of datasets.
Develop mining algorithms that scale to real-life massive databases is the first
research challenges proposed by Bradley, Fayyad, and Mangasarian in their overview
of applying mathematical programming for data mining. They also pointed out that
"approaches that assume that data can fit in main memory need to be revised or
redesigned (Bradley, Fayyad, and Mangasarian 1998)." MP based algorithm is such
an approach that requires the data to fit in main memory. This requirement comes
from the fact that constraint matrix must be loaded into main memory in order to
achieve an acceptable computation time and the size of constraint matrix is
determined by the size of the training dataset. Therefore, as the size of dataset
increases, the computation time increases and performance degraded.
The objectives of this research are: (1) to propose a new epsilon-support vector
approach, and (2) to apply this approach to large scale data mining problem and
analyze substantial datasets (size of O(109)). The results indicate that this new
approach has the potential to handle arbitrary-size of datasets.
This paper is organized in three parts. The next section describes the epsilonsupport vector for Multi-criteria Convex Quadratic programming (MCQP) two-group
classification model. Section 3 presents the experimental results. The last section
concludes the paper.

2 Epsilon-Support Vector (eSV) Approach
Each

row

of

a

n × r matrix

(A1,…,An)T

A=

is

an

vector

Ai

= (ai1 ,..., air ) ∈ ℜ which corresponds to one of the records in the training dataset
of a binary classification problem, i = 1,..., n ; n is the total number of records in the
dataset. Two groups, G1 and G 2 , are predefined while G1 ∩ G 2 = Φ and Ai
∈ {G1 ∪ G2 } . A boundary scalar b can be selected to separate G1 and G2 . Let X =
r

( x1 ,..., xr )T ∈ℜr be a vector of real number to be determined. In the classification
problem, Ai X is the score for the ith data record. If all records are linear separable and
an element Ai is correctly classified, then let
consider the linear system, AiX = b -

βi

βi

be the distance from A

, ∀ Ai ∈

i

to b, and

G1 and AiX = b + β i , ∀ Ai ∈

G2 . However, if we consider the case where the two groups are not linear separable
because of mislabeled records, a “Soft Margin” and slack distance variable
to be introduced. Previous equations now transforms to AiX = b +

αi

-

βi

αi

need

, ∀ Ai ∈

G1 and AiX = b - α i + β i , ∀ Ai ∈ G2 . To complete the definitions of β i and

αi

, let

βi

=0 for all misclassified elements and

αi

classified elements. Incorporating the definitions of
models could be set as:

equals to zero for all correctly

βi

and

αi

, out classification

876

G. Kou et al.

Minimize

n
n
1
W
|| X ||22 +Wα ∑ αi2 − Wβ ∑ β i + b b 2
2
2
i =1
i =1

Subject to: Y (<A⋅X> - eb) =
where Y is a given

δ'e- α

+

β

n × n diagonal matrix, e=(1,1,…,1)T, α = (α 1 , … , α n ) T ,

β = ( β1 ,… , β n ) T , X and b are unrestricted.
The Lagrange function corresponding to Model 3 is

W
1
L( X , b,η,θ ) = || X || 22 + α
2
2
where

θ = (θ1 ,… ,θ n )

According

to

T

n

n

i =1

i =1

∑ηi2 +Wβ ∑ηi +

,η

Wolfe

Wb 2 T
b −θ (Y ( < A⋅ X > - eb) - eδ ' +η)
2

= (η1 , … ,η n ) T , θi ,ηi ∈ℜ.
Theorem, ∇ X L( X , b,η,θ ) = X − A Yθ = 0 ,
T

Dual

∇b L( X , b,η, θ ) = Wb b + eT Yθ = 0 , ∇η L( X , b,η,θ ) = Wαη +Wβ e −θ = 0 .

(3)

Introduce the above 3 equations to the constraints of Model 3, we can get:

1
1
e(eT Yθ )) +
(θ −Wβ e) = δ ' e
Wb
Wα
Wβ
(δ ' + )e
Wα
⇒θ =
I
1
+ Y ((A⋅ AT ) + eeT )Y
Wα
Wb

Y ((A⋅ AT )Yθ +

(4)

In algorithm, we describe how to use this model in a classification problem.
Algorithm
Input:
a n × r matrix A as the training dataset, a n × n diagonal matrix Y
labels the class of each record.
Output: classification accuracies for each group in the training dataset, score for

> 0, ⇒ Ai ∈G1

every record, decision function (( X ⋅ Ai ) − b ){
*

*

≤ 0, ⇒ Ai ∈G2

Step 1. compute

θ = (θ1 , … , θ n )
*

validation.

T

by (5). Wβ , Wα , Wb are chosen by cross

−1 T *
e Yθ .
Wb
by
incoming Ai

Step 2. compute X = A Yθ , b =
*

Step

3.

classify

T

a

> 0, ⇒ Ai ∈G1
.
(( X * ⋅ Ai ) − b* ){
≤ 0, ⇒ Ai ∈G2
END

*

*

using

decision

function

epsilon-Support Vector and Large-Scale Data Mining Problems

In existing SVM approaches (Boser et al 1992), the sparsity of the optimal
of

αi* = 0 )

877

αi* (many

is the key in solving large scale SVM problem and the support vectors

( αi = 0 ) are the points at the hyperplane. Similarly, the support vector Ai of our
*

models can be defined as

αi* = βi* = 0 .

Due to the formulation of the model

objectives, almost none of the data points Ai lie on two adjusted bounding hyper
planes b ± 1 . Thus most of the time

αi* and βi* do not equal to zero simultaneously. A

Karush-Kuhn-Tucker (KKT) condition that has been used in previous support vector
approaches (Cristianini and Shawe-Taylor 2000, Schölkopf, and Smola 2002) can not
be held any more because of the introduction of distance variable

βi

in our model.

As a result, we need to establish another condition for those data points Ai whose
corresponding

αi* − βi* < ε

to define the epsilon-support vectors (eSVs) such that

the computation complexity can be reduced for large scale problems. ε is picked so
that only a small part (e.g. 1%) of the records belongs to epsilon-support vectors.
Generally, if n < 1000000 , Model 2 can use the whole dataset; otherwise, a sample
training set (e.g. 1% of the population) can be randomly generated from the original
data. Using the sample set, we can find an optimal solution and epsilon-support
vectors for the original dataset. All epsilon-support vectors are used to re-train the
model repeatedly until the classification results meet certain criteria/threshold. A
stratified random sampling approach is incorporated into our algorithm for large scale
data mining problem (>O(109)).
Stratified Random Sampling
Since MP based algorithm requires training datasets to fit in main memory, the size of
training dataset is limited by the capacity of main memory (O(106~8)). One possible
solution is to use only part of the training dataset when the dataset size is extremely
huge (>O(109)). However, this approach may loss valuable information that exists in
the unused part of the training dataset. In order to make the best use of the training
dataset, we employ a revised stratified random sampling.
Let’s briefly describe how standard stratified random sampling works. First, the
dataset is partitioned into groups of data called strata. Each data belongs to one and
only one stratum. Second, a sample is selected by some design within each stratum
(Thompson 1992). As a sampling technique, the goal of stratified random sampling is
to select a portion of a population that can be used as a “representation” of the
population as a whole.
Algorithm
The following procedure summarized the whole process:
Algorithm 1
Input:
The training dataset A as the population, n is the number of
observation in the population and is a huge number; m is the number of

878

G. Kou et al.

subpopulations which can be fitted into main memory; The testing dataset A’;
Algorithm Stop criteria.
Output:
Average classification accuracies; decision scores for all
observations; decision function.
Step 1: A is evenly partitioned into m subpopulations or strata by random
selection.
Step 2: One random sample is drawn from each of m subpopulations and
formulate the first training set Tr.
Step 3: Compute

θ * = (θ1 ,… ,θ n ) T

using Tr as input. Wβ , Wα , Wb are

chosen by cross validation.
Step 4: Compute X = A Yθ , b =
*

T

*

*

−1 T *
e Yθ . If the performance
Wb

measure meet pre-set criteria (accuracy, total computation time, etc) and
the iteration times > 10, stop; otherwise go to step 5.
Step 5: Compute

αi* − βi*

for all observations in A.

Step 6: Suppose we have n1 observations (epsilon-support vectors (eSVs))
corresponding

αi* − βi* < ε

support vectors) which

and n2 observations (Non epsilon-

αi* − βi* > ε . Formulate a new set with all n1

eSVs and some (n3, n3 ≤ n2 and n3 ≅ 10%*n1 ) non eSVs. If (n3 +n1)
≤ m, the new set is the new training set, otherwise, random sample m
observations from the new set to formulate a train set. Go back to step
3.
END
* Iteration times must be greater than 10 to minimize the influence of the selection
of initial training dataset.

3 Experimental Results
The purpose of this research is to test the applicability of our algorithm in large scale
data mining problem (>O(109)), which is too large to be loaded into a typical PC
memory (512MB-4Gb) but can be saved in harddisk (40Gb-1Tb). Because we can’t
find any real life data in such size, we use a Normally Distributed Clustered program
(NDC) (Musicant 1998) to generate datasets. Normally distributed cluster is a data
generator. NDC generates data points clustered to a series of random centers and all
points are multivariate normal distributed. All variables are integers for simplicity.
Then, NDC randomly generates a separating plane and assign a class for each random
center according to the random separating plane. NDC changes dataset separability by
changing the variances of the distributions. The percentage of separability is
calculated by counting how many points end up on the right and wrong side of the
separating plane. NDC will provide the percentage of linear separability.

epsilon-Support Vector and Large-Scale Data Mining Problems

879

The experimental study has two parts. The first one is to test the capability of the
proposed algorithm in massive problem with low noises. The second test is to
evaluate the performance of the proposed algorithm under high noises, large dataset.
Massive Dataset
The first experiment dataset consists of 100 million records and each record has 20
variables. It is balanced with 50% of the data in the first group and 50% of the data in
the second group. The dataset is 95% linear separable (Musicant 1998).
Algorithm 2
Input:
The input data set of 100 million records; Assume that at most 1
million records can be fitted into main memory.
Output:
10-fold cross validation average classification accuracies;
computing time.
Step 1:
The input data set is evenly partitioned into 10 subsets by stratified
random sampling selection.
Step 2:
A standard 10-fold cross-validation set of 10 sets of training dataset
and testing dataset is stored by using one of the 10 subsets as the testing
dataset and using the remaining 9 subsets as training dataset. Repeat the
following steps for each of the 10 sets of training dataset and testing dataset
Step 3:
n=100 million and m=1 million. Stop criteria: Prediction Accuracy >
94.8% or computing time > 1000 minutes. Apply algorithm 1.
END
Table 1. Massive dataset computing results
10-fold
fold 1
fold 2
fold 3
fold 4
fold 5
fold 6
fold 7
fold 8
fold 9
fold 10
average

training
Class 1
Class 2
94.93% 94.87%
94.96% 94.82%
94.85% 94.93%
94.88% 94.89%
94.84% 94.95%
94.95% 94.93%
94.93% 94.93%
94.98% 94.85%
94.87% 94.94%
94.83% 94.88%
94.90% 94.90%

time
(seconds)
631
933
378
785
655
1217
467
632
876
539
711.3

testing
Class 1
Class 2
94.91% 94.82%
94.93% 94.82%
94.87% 94.91%
94.87% 94.90%
94.82% 94.93%
94.89% 94.91%
94.91% 94.86%
94.92% 94.84%
94.85% 94.83%
94.84% 94.86%
94.88% 94.87%

Large Dataset with High Noise
The first experiment dataset consists of 2 million records and each record has 10
variables. It is balanced with 50% of the data in the first group and 50% of the data in
the second group. The dataset is 75% linear separable.

880

G. Kou et al.

Algorithm 3
Input:
The input data set of 2 million records; Assume that all records can
be fitted into main memory.
Output:
10-fold cross validation average classification accuracies;
computing time.
Step 1: The input data set is evenly partitioned into 10 subsets by stratified
random sampling selection.
Step 2: A standard 10-fold cross-validation set of 10 sets of training dataset
and testing dataset is stored by using one of the 10 subsets as the testing
dataset and using the remaining 9 subsets as training dataset. Repeat the
following steps for each of the 10 sets of training dataset and testing
dataset
Step 3: n=2 million and m=2 million. Stop criteria: Prediction Accuracy >
70% or computing time > 1000 minutes. Apply algorithm 1.
END
Table 2. High noise dataset computing results
10-fold
fold 1
fold 2
fold 3
fold 4
fold 5
fold 6
fold 7
fold 8
fold 9
fold 10
average

training
Class 1
Class 2
71.34% 74.33%
72.77% 74.13%
74.79% 72.44%
70.94% 73.58%
74.11% 72.35%
73.56% 74.33%
71.90% 70.38%
73.89% 72.48%
72.37% 73.38%
73.56% 72.84%
72.92% 73.02%

time
(seconds)
1279
849
1537
2574
742
1380
3546
836
711
1207
1466.1

testing
Class 1
Class 2
70.25% 71.12%
72.85% 72.47%
72.31% 71.23%
70.32% 72.09%
72.57% 71.32%
73.24% 72.67%
70.39% 70.12%
72.37% 72.23%
71.56% 71.73%
72.40% 72.19%
71.83% 71.72%

4 Conclusion
In this research, we proposed a new epsilon-support vector approach and conducted 2
experimental studies using computer generated large scale classification problem to
address two aspects of data mining algorithm: speed and scalability. The experimental
results indicate that the new approach is capable in solving large scale data mining
problems (>O(109)) and highly efficient (computing time complexity O(n1.5~2) ). This
approach also has good performance when the dataset is highly noised (noises are
larger than 25% of the whole data).

References
1. Boser, B. E., Guyon, I., Vapnik., V. N. : A training algorithm for optimal margin
classifiers. Proceedings of the Fifth Annual Workshop on Computational Learning Theory,
5:, (1992) 144--152

epsilon-Support Vector and Large-Scale Data Mining Problems

881

2. Bradley, P. : Mathematical Programming Approaches to Machine Learning and Data
Mining. PhD thesis, University of Wisconsin, Computer Sciences Department, Madison,
WI, USA, TR-98-11. (1998)
3. Bugera, V., Konno, H., Uryasev, S.: Credit Cards Scoring with Quadratic Utility Function.
Journal of Multi-Criteria Decision Analysis 11 (2002) 197-211.
4. Cristianini, N. and J. Shawe-Taylor. An Introduction to Support Vector Machines.
Cambridge University Press, Cambridge, UK, (2000)
5. Musicant,
D.
R.
NDC:
normally
distributed
clustered
datasets,.
www.cs.wisc.edu/_musicant/data/ndc/. (1998)
6. Kou, G., Peng, Y., Shi, Y., Wise, M., Xu, W.: Using Multi-objective Linear Programming
to Classify Credit Cardholder Behavior, Optimization Methods and Software, Vol. 18, ,
(2003) 453-473
7. Kou, G., Peng, Y., Shi, Y., Wise, M., Xu, W.: Discovering Credit Cardholders’ Behavior
by Multiple Criteria Linear Programming, Annals of Operations Research 135 (1):, JAN
(2005) 261-274
8. Schölkopf, B., Smola., A. J.: Learning with Kernels. MIT Press, (2002)
9. Smola, A., Bartlett, P., Schölkopf, B., Schuurmans, D. (eds).: Advances in Large Margin
Classifiers. MIT Press, Cambridge, MA, (2000)
10. Thompson, Sampling, S.K., :A Wiley-Interscience Publication, New York. (1992)
11. Vapnik, V. N.,: The Nature of Statistical Learning Theory, Springer, New York. (1995)

