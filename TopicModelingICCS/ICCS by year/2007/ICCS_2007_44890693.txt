Feature Selection for VIP E-Mail Accounts Analysis
Zhan Zhang, Yingjie Tian*, and Yong Shi
Research Center on Data Technology and Knowledge Economy of CAS,
Beijing 10080,
zhangzhan05@mails.gucas.ac.cn,
tianyingjie1213@163.com,
yshi@gucas.ac.cn,
www.dtke.ac.cn

Abstract. This paper introduces several feature selection methods on the VIP
E-Mail account analysis, and then uses SVM to classify the users in order to
maintain the ones who may leave in a short time. The websites hosting
companies can follow the strategies, which are investigated in this paper, to
reduce their labor cost and provide an attractive and simple registration for the
users. The result shows that we can also reach the feasible predict accuracy.
Keywords: Support Vector Machine, Feature selection, VIP E-Mail.

1 Introduction
With the development of internet, E-Mail is becoming more and more popular and
widely used. Because of its convenience and speediness, it is not only a tool for
communication but also has its own commercial characteristics. Many websites
hosting companies have developed the charged VIP E-Mail service to meet the
demand of the certain persons. According to the statistics, as Chinese network
advanced in the past few years, the total market of the Chinese VIP E-Mail service
has reached 6.4 hundred million RMB by 2005, which has caused drastic
competitions around the IT market. Meanwhile, the providers have to try their best to
collect more information about the users and buy more memory to save them. The
technical staffs have to face more and more data bases and log files. Therefore, it has
become the most important problems for the service providers that how to look for the
potential users, how to maintain the current users and how to analysis the behaviors of
the users throughout the feature as few as possible.
Data mining means the nontrivial extraction of implicit, previously unknown, and
potentially useful information from data. Feature selection is an important part of data
mining. The number of features affects the classifier's speed that including a large
number of features can result in long training and classification times.
The purpose of this paper is to apply several feature selection methods including
Support Vector Machine (SVM), F-score method, and one-norm SVM to reduce the
data scale and then use the SVM to deal with the classification on a famous Chinese
*

Corresponding author.

Y. Shi et al. (Eds.): ICCS 2007, Part III, LNCS 4489, pp. 693–700, 2007.
© Springer-Verlag Berlin Heidelberg 2007

694

Z. Zhang, Y. Tian, and Y. Shi

website’s VIP E-Mail data set. This paper is organized as follows. In Section 2 we
introduce the SVM classifier; Section 3 focuses on the different methods that work on
the feature selection; Section 4 introduces how to choose satisfying parameters with
10-fold cross validation; In Section 5, we show the experimental results during the
development period of the competition; and the last part is the conclusion.

2 Support Vector Classification
The algorithm about SVM is originally established by Vapnik (1998). Since 1990s
SVM has been a promising tool for data classification. Its basic idea is to map data
into a high dimensional space and find a separating hyper plane with the maximal
margin. SVM solves the following optimization problem:
m
1
min　 　 w ' w + C ∑ ξ k， 　 　 　 　 　 　
w,b ,ξ
2
k =1
subject to

yk ( wT iφ ( xk ) + b) ≥ 1 − ξ k ,
　 　 　 　 　 　 ξ k ≥ 0, k = 1,..., m,
Where training data are mapped to a higher dimensional space by the function φ ,
and C is a penalty parameter on the error. The decision function (predictor) is

f ( x) = sgn( wT φ ( xk ) + b).
The RBF kernel is used in our experiments: k ( x, x ') = exp( −γ || x − x ' || ). With
the RBF kernel, there are two parameters to be determined in the SVM model i.e. C
and γ .
2

3 Feature Selection Strategies
In this section, we will discuss three feature selection strategies respectively: SVM, Fscore and one-norm SVM.
3.1 No Feature Selection: Direct SVM Strategy
Firstly, we use the SVM strategy directly without feature selection and choose the
linear kernel function.
3.2 SVM Feature Selection Strategy
From 3.1, we can get the decision function (predictor):

f ( x) = sgn( wT xk + b). If

we scale the data by putting each feature to [0, 1], the coefficient |w| can be

Feature Selection for VIP E-Mail Accounts Analysis

695

considered as the weight of each feature. The larger the |w| is, the more discriminative
this feature is. Therefore, we use this score as a feature selection criterion.
3.3 F-Score Feature Selection Strategy[3]
F-score is a simple technique which measures the discrimination of two sets of real
numbers. Given training vectors xk , k = 1… m, if the numbers of positive and
negative instances are n+ and n− , respectively, then the F-score of the ith feature is
defined as:

( xi( + ) − xi ) 2 + ( xi( − ) − xi ) 2
F (i ) =
1 n+ ( + )
1 n− ( − )
(+) 2
(
)
x
−
x
+
∑ k ,i i
∑ ( xk ,i − xi( −) )2
n+ − 1 k =1
n− − 1 k =1
Where

xi , xi( + ) , xi( − ) are the average of the ith feature of the whole, positive, and

negative data sets, respectively;

xk( +,i) is the ith feature of the kth positive instance, and

xk( −,i) is the ith feature of the kth negative instance. The numerator indicates the
discrimination between the positive and negative sets, and the denominator indicates
the one within each of the two sets. The larger the F-score is, the more discriminative
this feature is. Therefore, we use this score as a feature selection criterion.
3.4 One-Norm SVM Feature Selection Strategy
According to SVM, we give the following linear program to calculate the coefficient
w:
Given training vectors
labels

xk ∈ R n , k = 1… m, in two classes and a vector of

y ∈ R m such that y k ∈ {1, − 1} , and we will solve the following linear

program:
k

m

i =1

j

min 　 　
∑ si + C ∑ ξ j ,

w,b ,ξ , s

subject 　 to
　 　 y j (( wi x j ) + b) ≥ 1 − ξ j , j = 1,..., m,
　 　 　− si ≤ wi ≤ si , i = 1,..., k ,
　 　 　 ξ j ≥ 0, j = 1,..., m.
Where C is a penalty parameter on the training errors.

696

Z. Zhang, Y. Tian, and Y. Shi

Actually, the larger the |w| is, the more discriminative this feature is. Therefore, we
use this score as a feature selection criterion.

4 K-Fold Cross Validation
When we have finished the feature selection, we use the SVM to do the classification.
The cross validation will help to identify good parameters so that the classifier can
accurately predict unknown data. In this paper, we use 10 fold cross validation to
choose the penalty parameter C and γ in the SVM. When we get the nice arguments,
we will use them to train model and do the final prediction.

5 Experimental Results
In this part, we use LIBSVM (Chang and Lin 2001) for SVM classification. Before
doing the classification, we perform some data preprocessing. We choose 5499
positive records and 5498 negative records from the data set and then delete the nonnumeric features, and scale each feature to [0,1].
Firstly, we use the SVM to do the classification without any feature selection.
According to the 10-fold cross validation, we get the high accuracy as follows:
Table 1. Cross Validation with SVM on the Training Set

10

Obviously, when C= 2 , we get the highest accuracy 89.25%.
Then we will focus on the feature selection strategies mentioned in Section 3.2—3.4.
For the SVM feature selection strategy, after the above cross validation, with C
10

equaling to 2 , we can get the model

f ( x) = sgn( wT xk + b). Then we put the |w|

in order, which stands for the weight of each feature. In fact, there are 226 features
with |w|>0. In the experiment, we use the half-cut method to choose the features,
which means every time we just maintain half features whose |w| are bigger than
others. Using the same cross validation method we can select the optimized C, and the
following table gives the top five accuracies with each half feature selected.
We find that only when there are still 113 features, the accuracy can be over 80%.
For the F-score feature selection strategy, as we have discussed in Section 3.3, first
we calculate the F-score of each feature, and then put them in order, at last use the

Feature Selection for VIP E-Mail Accounts Analysis

697

half-cut method to choose the features. Using the same cross validation method we
can select the optimized C, the following table gives the top five accuracies with half
features selected.
Table 2. Cross Validation with SVM after SVM Feature Selection on the Training Set

Table 3. Cross Validation with SVM after F-Score Feature Selection on the Training Set

From the table we find that when we use the first 28 features, we can get a
satisfying accuracy 84.99%.
For the one-norm SVM feature selection strategy, we can also get the |w| from the
model the same as the SVM feature selection. In this experiment, we figure out that
there are only 26 |w|>0, so with the same method, following table gives the top five
accuracies with each half feature selected.
Table 4. Cross Validation with SVM after One-Norm Feature Selection on the Training Set

All the above strategies are based on the hypothesis that the data set can be
classified by a linear decision function. The following experiments are based on the
hypothesis that the data set can not be classified by a linear function. As a result, we
introduce the RBF kernel to the SVM.
When we use the SVM directly to choose the parameters C and γ , we get the
graph as below:

698

Z. Zhang, Y. Tian, and Y. Shi

Fig. 1. Direct Use SVM with RBF Kernel on the Training Set

The following table gives the top five accuracies with all features.
Table 5. Cross Validation with RBF Kernel SVM on the Training Set

10

Compared with the other values of C, the value 2

can always get the better

accuracy; therefore, we just fix C at 2 to choose the parameter γ . Based on the
above three feature selection strategies, we still use the features selected respectively
to do the RBF kernel SVM, we can get the following three tables for each strategy.
10

Table 6. Cross Validation with RBF Kernel SVM after SVM Feature Selection on the Training Set

Feature Selection for VIP E-Mail Accounts Analysis

699

Table 7. Cross Validation with RBF Kernel SVM after F-Score Feature Selection on the Training
Set

Table 8. Cross Validation with RBF Kernel SVM after One-Norm SVM Feature Selection on
the Training Set

In table 1, it is obvious that the accuracy can be over 80% as long as the parameter
C is bigger than 1 with all the 230 features, and also, in table 5, when we introduce
the RBF kernel into SVM, the rate can be better than 90% with the proper parameters.
In table 2, the accuracy could not be higher than 80% unless we maintain 113 features
at least; with the same feature selection strategy, in table 6, we also have to leave no
less than 113 features in the model to get the 80% accuracy; In table 3, we just need to
keep only 28 features in order to get the accuracy better than 80% as well as 14
features in table 7; Meanwhile, in table 4 and table 8, we can get the 80% rate with 26
features.

6 Conclusion
In this paper, we experiment several feature selection strategies to work on the VIP EMail accounts data set. On the whole, the strategies with RBF kernel are better than
the ones without it; the F-Score strategy and norm-SVM strategy are better than the
SVM strategy. According to the result, the websites hosting companies can reduce the
labor cost for information collection. On one hand, the engineers can be released from
dealing with the massive log files; on the other hand, the users can get a quick and
attractive registration without providing so much personal information.
Acknowledgments. In the process of performing this paper, the authors get much
support from the follow members: Zhang Dongling, Liu Rong, Li Xingsen, Zhang Peng,
Li Aihua, without their help, we could not get the balanced samples from the VIP EMails accounts data bases. We express our esteem with the deepest appreciation. This

700

Z. Zhang, Y. Tian, and Y. Shi

research has been partially supported by a grant from National Natural Science
Foundation of China (#10601064,#70531040) and NSFB(#9073020).

References
1. Chang C.-C, Lin C.-J. LIBSVM: a library for support vector machines, (2001). Software
available at http://www.csie.ntu.edu.tw/~cjlin/libsvm.
2. iResearch, China E-mail Market Research Report 2005 (Online). Available: http://
www.iresearch.com.cn/html/email_service/detail_free_id_27497.html. (2005)
3. Chen Y.W., Lin C.J., Combining SVMs with various feature selection strategies. (2005)
4. Chapelle O., Vapnik V., Bousquet O., Mukherjee S.. Choosing kernel parameters for
support vector machines. Machine Learning, Submitted. (2000)
5. Cortes C., Vapnik V.: Support-vector networks. Machine Learning, 20(3) (1995):273–297.

