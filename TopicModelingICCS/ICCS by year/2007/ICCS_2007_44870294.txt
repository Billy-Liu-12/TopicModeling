A Modified Quantum-Behaved Particle Swarm
Optimization
Jun Sun1, C.-H. Lai2, Wenbo Xu1, Yanrui Ding1, and Zhilei Chai1
1
Center of Intelligent and High Performance Computing,
School of Information Technology, Southern Yangtze University,
No. 1800, Lihudadao Road, Wuxi,
214122 Jiangsu, China
{sunjun_wx, xwb_sytu}@hotmail.com, zlchai@gmail.com
2School of Computing and Mathematical Sciences,
University of Greenwich, Greenwich, London SE10 9LS, UK
C.H.Lai@gre.ac.uk

Abstract. Based on the previously introduced Quantum-behaved Particle
Swarm Optimization (QPSO), a revised QPSO with Gaussian disturbance on
the mean best position of the swarm is proposed. The reason for the
introduction of this novel method is that the disturbance can effectively prevent
the stagnation of the particles and therefore make them escape the local optima
and sub-optima more easily. Before proposing the Revised QPSO (RQPSO), we
introduce the origin and the development of the original PSO and QPSO. To
evaluate the performance of the new method, the Revised QPSO, along with
QPSO and Standard PSO, is tested on several well-known benchmark
functions. The experimental results show that the Revised QPSO has better
performance than QPSO and Standard PSO generally.
Keywords: Global Optimization, Swarm Intelligence, Particle Swarm,
Quantum-behaved Particle Swarm and Gaussian distribution.

1 Introduction
The Particle Swarm Optimization (PSO), originally invented by J. Kennedy and R.C.
Eberhart [7], is a member of a wider class of Swarm Intelligence methods used for
solving Global Optimization (GO) problems. It was proposed as a population-based
search technique simulating the knowledge evolvement of a social organism. In a
PSO system, individuals (particles) representing the candidate solutions to the
problem at hand, fly through a multidimensional search space to find out the optima
or sub-optima. The particle evaluates its position to a goal (fitness) at every iteration,
and particles in a local neighborhood share memories of their “best” positions. These
memories are used to adjust particle velocities and their subsequent positions.
In the original PSO with M individuals, each individual is treated as an
infinitesimal particle in D-dimensional space, with the position vector and velocity
Y. Shi et al. (Eds.): ICCS 2007, Part I, LNCS 4487, pp. 294–301, 2007.
© Springer-Verlag Berlin Heidelberg 2007

A Modified Quantum-Behaved Particle Swarm Optimization

vector of particle i, X i (t ) = ( X i1 (t ), X i 2 (t ), " , X iD (t )) and
particle moves according to the following equations:

Vi (t ) = (Vi1 (t ), Vi 2 (t ), " , ViD (t )) .

295

The

Vij (t + 1) = w ⋅ Vij (t) + c1 ⋅ r1 (Pij (t) − X ij (t)) + c2 ⋅ r2 ⋅ (Pgj − X ij (t))

(1)

X ij ( t + 1) = X ij ( t ) + V ij ( t + 1)

(2)

for i = 1,2,"M ; j = 1,2", D . The parameters c1 and c 2 are called the acceleration
coefficients. Vector Pi = (Pi1 , Pi 2 ,", PiD ) known as the personal best position, is the best
previous position (the position giving the best fitness value so far) of particle i; vector
Pg = (Pg1, Pg2,", PgD) is the position of the best particle among all the particles and is
known as the global best position. The parameter w is the inertia weight and the
parameters r1 and r 2 are two random numbers distributed uniformly in (0,1), that is
r1 , r2 ~ U ( 0 ,1) . Generally, the value of Vij is restricted in the interval [ − V max , V max ] .
Many revised versions of PSO algorithm are proposed to improve the performance
since its origin in 1995. For the detailed information about these variants of PSO, one
may refer to the literature such as [1], [3], [8], [9], etc.
In our previous work, a Quantum-behaved Particle Swarm Optimization (QPSO),
inspired by quantum mechanics was proposed in [10], [11] and [12]. In this paper, we
introduce a method of improving QPSO by exerting a Gaussian disturbance on the
mean best position of the swarm and therefore propose a revised version of QPSO.
The rest of the paper is structured as follows. In Section 2, the concept of QPSO is
presented. The revised QPSO is proposed in Section 3. Section 4 gives the numerical
results on some benchmark functions. Some concluding remarks and future work are
presented in the last section.

2 Quantum-Behaved Particle Swarm Optimization
Trajectory analyses in [4] demonstrated the fact that convergence of PSO algorithm
may be achieved if each particle converges to its local attractor p i = ( p i1 , p i 2 ," p iD )

with coordinates
pij (t ) = ( c1r1 Pij (t ) + c2 r2 Pgj (t )) (c1 r1 + c 2 r2 ), or pij (t ) = ϕ ⋅ Pij (t ) + (1 − ϕ ) ⋅ Pgj (t )

(3)

where ϕ = c1r1 (c1r1 + c2 r2 ) . It can be seen that the local attractor is a stochastic attractor
of particle i that lies in a hyper-rectangle with P i and Pg being two ends of its
diagonal. We introduce the concepts of QPSO as follows.
Assume that each individual particle move in the search space with a δ potential
on each dimension, of which the center is the point pij . For simplicity, we consider a
particle in one-dimensional space, with point p the center of potential. Solving
Schrödinger equation of one-dimensional δ potential well, we can get the probability
distribution function D.

D ( x) = e

−2 p − x L

(4)

296

J. Sun et al.

Using Monte Carlo method, we obtain
x = p±

L
ln( 1 u ) , u ~ U (0,1)
2

(5)

The above is the fundamental iterative equation of QPSO.
In [11], a global point called Mainstream Thought or Mean Best Position of the
population is introduced into PSO. The global point, denoted as C, is defined as the
mean of the personal best positions among all particles. That is
⎛ 1
C (t ) = (C1 (t ), C 2 (t ), " , C D (t )) = ⎜
⎝M

M

∑P
i =1

i1

(t ),

1
M

M

∑P
i =1

i2

(t ), " ,

1
M

M

∑P

iD

i =1

⎞
(t ) ⎟
⎠

(6)

where M is the population size and Pi is the personal best position of particle i. Then
the value of L is evaluated by L = 2α ⋅ C j ( t ) − X ij ( t ) and the position are updated by
X ij (t + 1) = p ij (t ) ± α ⋅ C j (t ) − X ij (t ) ⋅ ln(1 / u )

(7)

where parameter α is called Contraction-Expansion (CE) Coefficient, which can be
tuned to control the convergence speed of the algorithms. Generally, we always call
the PSO with equation (7) Quantum-behaved Particle Swarm Optimization (QPSO),
where parameter α must be set as α < 1.782 to guarantee convergence of the particle
[12]. In most cases, α can be controlled to decrease linearly from α 0 to α 1 ( α 0 < α1 ).

3 The Proposed Method
Although QPSO possesses better global convergence behavior than PSO, it may
encounter premature convergence, a major problem with PSO and other evolutionary
algorithms in multi-modal optimization, which results in great performance loss and
sub-optimal solutions. In a PSO system, with the fast information flow between
particles due to its collectiveness, diversity of the particle swarm declines rapidly,
leaving the PSO algorithm with great difficulties of escaping local optima. Therefore,
the collectiveness of particles leads to low diversity with fitness stagnation as an
overall result. In QPSO, although the search space of an individual particle at each
iteration is the whole feasible solution space of the problem, diversity loss of the
whole population is also inevitable due to the collectiveness.
From the update equations of PSO or QPSO, we can infer that all particles in PSO
or QPSO will converge to a common point, leaving the diversity of the population
extremely low and particles stagnated without further search before the iterations is
over. To overcome the problem, we introduce a Revised QPSO by exerting a
Gaussian disturbance on the mean best position when the swarm is evolving. That is,
m j (t ) = C j (t ) + ε ⋅ Rn ,

j = 1,2, " , D

(8)

where ε is a pre-specified parameter and Rn is random number with Gaussian
distribution with mean 0 and standard deviation 1. Thus the value of L is calculated as
L = 2α ⋅ m j ( t ) − X ij ( t ) and equation (6) becomes

A Modified Quantum-Behaved Particle Swarm Optimization
X ij ( t + 1) = p ij ( t ) ± α m j ( t ) − X ij ( t ) ⋅ ln( 1 / u )

297

(9)

The above iterative equation can effectively avoid the declination of the diversity and
consequently the premature convergence. It is because that when the swarm is
evolving, a consistent disturbance on mean best position can prevent the
L = 2α ⋅ m j ( t ) − X ij ( t ) decreasing to zero, maintaining the particle’s vigor, which is
particularly serviceable to make the particle escape local optima at the later stage of
evolution, and able to result in a better performance of the algorithm overall. The
revised QPSO is outlined as follows.
Revised QPSO Algorithm
Initialize particles with random position Xi=X[i][:];
Initialize personal best position by set Pi=Xi;
while the stop criterion is not met do
Compute the mean best position C[:] by equation (6);
Get the disturbed point m by equation (8)
for i = 1 to swarm size M
If f(Xi)<f(Pi) then Pi=Xi; Endif
Find the Pg=arg min f(P[g][:]);
for j=1 to D
=rand(0,1); u=rand(0,1);
p=*P[i][j]+(1- )*P[g][j];
if (rand(0,1)>0.5)
X[i][j]=p+α*abs(m[j]- X[i][j])*ln(1/u);
Else
X[i][j]=p-α*abs(m[j]- X[i][j])*ln(1/u);
Endif
Endfor
Endfor
Endwhile

4 Numerical Experiments
In this section five benchmark functions listed in Table 1 are tested for performance
comparisons of the Revised QPSO (RQPSO) with Standard PSO (SPSO) and QPSO
algorithms. These functions are all minimization problems with minimum objective
function values zeros. The initial range of the population is asymmetry, as used in
[13], [14]. Vmax for SPSO is set as the up-bound of the search domain.
Setting the fitness value as function value, we had 50 trial runs for every instance
and recorded mean best fitness and standard deviation. In order to investigate the
scalability of the algorithm, different population sizes M are used for each function
with different dimensions. The population sizes are 20, 40 and 80. The maximum
generation (iteration) is set as 1000, 1500 and 2000 corresponding to the dimensions
10, 20 and 30 for first four functions, respectively. The maximum generation for the
last function is 2000. For SPSO, the acceleration coefficients are set to be c1=c2=2
and the inertia weight is decreasing linearly from 0.9 to 0.4 as in [13], [14].

298

J. Sun et al.
Table 1. Expression of the five tested benchmark functions
Function Expression
Sphere

Rosenbrock
Rastrigrin
Greiwank
Shaffer’s

∑

f1 ( X ) =
f2 (X ) =

Search Domain

Initial Range

n

i =1

x

2
1

(-100, 100)

(50, 100)

(-100, 100)

(15, 30)

(-10, 10)

(2.56, 5.12)

n
⎛ x ⎞
− ∏ cos ⎜ i ⎟ + 1
i =1
⎝ i⎠

(-600, 600)

(300, 600)

(sin( x 12 + x 22 )) 2
(1 . 0 + 0 . 001 ( x 12 + x 22 )) 2

(-100, 100)

(30, 100)

n −1

∑ (100 ⋅ ( x
i =1

n

f3 ( X ) =

∑ (x

f4 (X ) =

1
4000

i =1

2
i

f 5 ( X ) = 0 .5 +

i +1

− x i2 ) 2 + ( x i − 1) 2 )

− 10 ⋅ cos( 2π x i ) − 10 )
n

∑x
i =1

2
i

In experiments for QPSO and RQPSO, the value of CE Coefficient α varies from 1.0
to 0.5 linearly over the running of the algorithm as in [11], [12]. For Revised QPSO
(RQPSO), the parameter ε is fixed at 0.001. The mean values and standard deviations
of best fitness values for 50 runs of each function are recorded in Table 2 to Table 5.
The numerical results show that both QPSO and RQPSO are superior to SPSO
except on most of the function. On Shpere Function the RQPSO has worse
performance than QPSO and SPSO because maintaining the diversity by consistent
disturbance on the mean best position leads to poor local convergence of the
algorithm. On Rosenbrock function, the RQPSO outperforms the QPSO when the
swarm size is 20, but it does not show the better performance than QPSO when the
swarm size is 40 and 80. On Rastrigrin function, it is shown that the RQPSO
generated better results than QPSO when the number of particles is 20, but its
advantages over the QPSO are not remarkable if the sampling errors are considered.
On Griewank function, RQPSO is superior to the QPSO in most cases, particularly
when the dimension of the problem is high. On Shaffer’s function, the RQPSO shows
no improvements for QPSO. Generally speaking, the Revised QPSO has better global
search ability than QPSO when the problem is complex.
The figure 1 shows the convergence process of the RQPSO and QPSO on the first
four benchmark functions with dimension 30 and swarm size 20. It is shown that the
RQPSO has comparable convergence speed with QPSO and both of them can
converge more rapidly than SPSO.
Table 2. Numerical results on Sphere function
M

20

40

80

Dim.
10
20
30
10
20
30
10
20
30

Gmax
1000
1500
2000
1000
1500
2000
1000
1500
2000

Mean Best
3.16E-20
5.29E-11
2.45E-06
3.12E-23
4.16E-14
2.26E-10
6.15E-28
2.68E-17
2.47E-12

SPSO
St. Dev.
6.23E-20
1.56E-10
7.72E-06
8.01E-23
9.73E-14
5.10E-10
2.63E-27
5.24E-17
7.16E-12

QPSO
Mean Best
St. Dev.
2.29E-41
1.49E-40
1.68E-20
7.99E-20
1.34E-13
3.32E-13
8.26E-72
5.83E-71
1.53E-41
7.48E-41
1.87E-28
6.73E-28
3.10E-100
2.10E-99
1.56E-67
9.24E-67
1.10E-48
2.67E-48

RQPSO
Mean Best
St. Dev.
1.7412E-007 4.594E-008
1.8517E-006 3.387E-007
6.0118E-006 1.086E-006
1.5035E-007 4.627E-008
1.3454E-006 2.572E-007
4.9908E-006 7.878E-007
1.0779E-007 3.592E-008
1.3454E-006 2.572E-007
4.1723E-006 6.179E-007

A Modified Quantum-Behaved Particle Swarm Optimization

299

Table 3. Numerical results on Rosenbrock function
M

20

40

80

Dim.
10
20
30
10
20
30
10
20
30

SPSO
St. Dev.
194.3648
293.4544
547.2635
174.1108
377.4305
478.6273
57.4734
137.2637
289.9728

Gmax
Mean Best
94.1276
204.337
313.734
71.0239
179.291
289.593
37.3747
83.6931
202.672

1000
1500
2000
1000
1500
2000
1000
1500
2000

QPSO
Mean Best
St. Dev.
59.4764
153.0842
110.664
149.5483
147.609
210.3262
16.2338
24.46731
46.5957
39.536
59.0291
63.494
8.63638
16.6746
35.8947
36.4702
51.5479
40.849

RQPSO
Mean Best
St. Dev.
47.6904
99.3668
70.7450
116.1039
103.7322
166.9141
20.1872
33.3523
46.8270
51.7003
78.5551
71.2005
13.4288
14.5301
29.8257
33.2984
54.1137
38.2911

Table 4. Numerical results on Rastrigrin function
M

20

40

80

Dim.
10
20
30
10
20
30
10
20
30

Gmax
Mean Best
5.5382
23.1544
47.4168
3.5778
16.4337
37.2796
2.5646
13.3826
28.6293

1000
1500
2000
1000
1500
2000
1000
1500
2000

SPSO
St. Dev.
3.0477
10.4739
17.1595
2.1384
5.4811
14.2838
1.5728
8.5137
10.3431

QPSO
Mean Best
St. Dev.
5.2543
2.8952
16.2673
5.9771
31.4576
7.6882
3.5685
2.0678
11.1351
3.6046
22.9594
7.2455
2.1245
2.2353
10.2759
6.6244
16.7768
4.4858

RQPSO
Mean Best
St. Dev.
4.4489
2.2451
15.9715
6.4180
27.6414
6.1376
3.2081
1.4512
10.5817
3.6035
20.9748
5.2488
2.0922
1.5245
8.4794
2.7922
16.4016
4.4312

Table 5. Numerical results on Griewank function
M

20

40

80

Dim.
10
20
30
10
20
30
10
20
30

Gmax
Mean Best
0.09217
0.03002
0.01811
0.08496
0.02719
0.01267
0.07484
0.02854
0.01258

1000
1500
2000
1000
1500
2000
1000
1500
2000

SPSO
St. Dev.
0.0833
0.03255
0.02477
0.0726
0.02517
0.01479
0.07107
0.0268
0.01396

QPSO
Mean Best
St. Dev.
0.08331
0.06805
0.02033
0.02257
0.01119
0.01462
0.06912
0.05093
0.01666
0.01755
0.01161
0.01246
0.03508
0.02086
0.01463
0.01279
0.01136
0.01139

RQPSO
Mean Best
St. Dev.
0.0694
0.0641
0.0201
0.0212
0.0091
0.0129
0.0531
0.0509
0.0180
0.0165
0.0107
0.0137
0.0342
0.0423
0.0141
0.0158
0.0066
0.0105

Table 6. Numerical results on Shaffer’s f6 function
M
20
40
80

Dim.
2
2
2

Gmax
2000
2000
2000

Mean Best
2.782E-04
4.744E-05
2.568E-10

SPSO
St. Dev.
0.001284
3.593E-05
3.134E-10

QPSO
Mean Best
St. Dev.
0.001361
0.003405
3.891E-04
0.001923
1.723E-09 3.303E-09

RQPSO
Mean Best
St. Dev.
0.001433
0.0036
2.1303E-008 6.726E-008
3.5979E-009 1.251E-008

300

J. Sun et al.
Sphere Function-30 Dimensions

Rosenbrock Function-30 Dimensions

10000

10000

9000

7000

8000
7000

6000

Fitness Value

Fitness Value

9000

PSO
QPSO
RQPSO

8000

5000
4000

6000
5000
4000

3000

3000

2000

2000

1000

1000

0

0

500

1000
Number of Iterations

1500

0

2000

PSO
QPSO
RQPSO

0

500

(a)

1500

2000

(b)

Rastrigrin Functions-20 Dimensions

Griewank Function-30 Dimensions

700

2
1.8

600

PSO
QPSO
RQPSO

1.6
1.4
Fitness Value

500
Fitness Value

1000
Number of Iterations

400

300

1.2
1
0.8
0.6

200

PSO
QPSO
RQPSO

0.4
100

0.2
0

0
0

500

1000
Number of Iterations

(c)

1500

2000

0

500

1000
Number of Iterations

1500

2000

(d)

Fig. 1. Convergence process of the RQPSO and QPSO on the first four benchmark functions
with dimension 30 and swarm size 20 averaged on 50 trail runs

5 Conclusions
In this paper, a Revised QPSO with the mean best position exerted by Gaussian
disturbance is proposed. The reason for introduction of the disturbance is that it can
avoid the declination of the diversity effectively, which is particularly serviceable for
the particle’s escaping local optima at later stage of the evolution. The iterative
equation derives from the hybrid of exponential and normal distributions. The
numerical results on benchmark functions show that the Revised QPSO enhance the
global search ability of QPSO efficiently.
Our future work will focus on find out more efficient methods of improving QPSO.
A promising method may be introducing another type of probability distribution into
QPSO to replace exponential distribution. Moreover, we will also be devoted to
applying the novel QPSO to many real world problems.

References
1. Angeline, P.J.: Using Selection to Improve Particle Swarm Optimization. Proc. 1998 IEEE
International Conference on Evolutionary Computation. Piscataway, NJ (1998) 84-89
2. Van den Bergh, F.: An Analysis of Particle Swarm Optimizers. PhD Thesis. University of
Pretoria, South Africa (2001)

A Modified Quantum-Behaved Particle Swarm Optimization

301

3. Clerc, M.: The Swarm and Queen: Towards a Deterministic and Adaptive Particle Swarm
Optimization. Proc. 1999 Congress on Evolutionary Computation. Piscataway, NJ (1999)
1951-1957
4. Clerc, M., Kennedy, J.: The Particle Swarm: Explosion, Stability, and Convergence in a
Multi-dimensional Complex Space. IEEE Transactions on Evolutionary Computation, Vol.
6, No. 1. Piscataway, NJ (2002) 58-73
5. Eberhart, R.C., Shi, Y.: Comparison between Genetic Algorithm and Particle Swarm
Optimization. Evolutionary Programming VII, Lecture Notes in Computer Science 1447,
Springer-Verlag, Heidelberg (1998) 611-616
6. Holland, J.H.: Adaptation in Natural and Artificial Systems. The University of Michigan
Press, Michigan (1975)
7. Kennedy, J., Eberhart, R.C.: Particle Swarm Optimization. Proc. IEEE 1995 International
Conference on Neural Networks, IV. Piscataway, NJ (1995) 1942-1948
8. Kennedy, J.: Small worlds and Mega-minds: Effects of Neighborhood Topology on
Particle Swarm Performance. Proc. 1999 Congress on Evolutionary Computation.
Piscataway, NJ (1999) 1931-1938
9. Suganthan, P.N.: Particle Swarm Optimizer with Neighborhood Operator. Proc. 1999
Congress on Evolutionary Computation, Piscataway, NJ (1999) 1958-1962
10. Sun, J., Feng, B., Xu, W.-B.: Particle Swarm Optimization with Particles Having Quantum
Behavior. Proc. 2004 Congress on Evolutionary Computation, Piscataway, NJ (2004)
325-331
11. Sun, J., Xu, W.-B., Feng, B.: A Global Search Strategy of Quantum-behaved Particle
Swarm Optimization. Proc. 2004 IEEE Conference on Cybernetics and Intelligent
Systems, Singapore (2004) 111-115
12. Sun, J., Xu, W.-B., Feng, B.: Adaptive Parameter Control for Quantum-behaved Particle
Swarm Optimization on Individual Level. Proc. 2005 IEEE International Conference on
Systems, Man and Cybernetics. Piscataway, NJ (2005) 3049-3054
13. Shi, Y., Eberhart, R.: Empirical Study of Particle Swarm Optimization. Proc. 1999
Congress on Evolutionary Computation. Piscataway, NJ (1999) 1945-1950
14. Shi, Y., Eberhart, R.C.: A Modified Particle Swarm. Proc. 1998 IEEE International
Conference on Evolutionary Computation. Piscataway, NJ (1998) 69-73

