Adaptive Sparse Grid Classiﬁcation
Using Grid Environments
Dirk Pﬂ¨
uger, Ioan Lucian Muntean, and Hans-Joachim Bungartz
Technische Universit¨
at M¨
unchen, Department of Informatics,
Boltzmannstr. 3, 85748 Garching, Germany
{pflueged, muntean, bungartz}@in.tum.de

Abstract. Common techniques tackling the task of classiﬁcation in data
mining employ ansatz functions associated to training data points to ﬁt
the data as well as possible. Instead, the feature space can be discretized
and ansatz functions centered on grid points can be used. This allows for
classiﬁcation algorithms scaling only linearly in the number of training
data points, enabling to learn from data sets with millions of data points.
As the curse of dimensionality prohibits the use of standard grids, sparse
grids have to be used.
Adaptive sparse grids allow to get a trade-oﬀ between both worlds by
reﬁning in rough regions of the target function rather than in smooth
ones. We present new results for some typical classiﬁcation tasks and
show ﬁrst observations of dimension adaptivity. As the study of the critical parameters during development involves many computations for different parameter values, we used a grid environment which we present.
Keywords: data mining, classiﬁcation, adaptive sparse grids, grid environment.

1

Introduction

Today, an ever increasing amount of data is available in various ﬁelds such as
medicine, e-commerce, or geology. Classiﬁcation is a common task making use
of previously known data and making predictions for new, yet unknown data.
Eﬃcient algorithms that can process vast datasets are sought for. The basics
of sparse grid classiﬁcation have already been described in [1,2], for example.
Therefore, in this section, we summarize the main ideas only very brieﬂy and
refer to the references cited above for further information.
We focus on binary classiﬁcation. Given is a preclassiﬁed set of M data points
for training, S = {(xi , yi ) ∈ [0, 1]d ×{−1, 1}}M
i=1 , normalized to the d-dimensional
unit hypercube. The aim is to compute a classiﬁer f : [0, 1]d → {−1, 1} to obtain
a prediction of the class −1 or +1 for previously unseen data points. To compute
f , we follow the Regularization Network approach and minimize the functional
H[f ] =

1
M

M

(yi − f (xi ))2 + λ ∇f

2
L2 ,

i=1

Y. Shi et al. (Eds.): ICCS 2007, Part I, LNCS 4487, pp. 708–715, 2007.
c Springer-Verlag Berlin Heidelberg 2007

Adaptive Sparse Grid Classiﬁcation Using Grid Environments

709

with the cost function (yi − f (xi ))2 ensuring good approximation of the training
data by f and the regularization operator ∇f 2L2 guaranteeing that f is somehow smooth, which is necessary as the classiﬁer should generalize from S. The
regularization parameter λ stirs the trade-oﬀ between accuracy and smoothness.
Rather than common algorithms which employ mostly global ansatz functions
associated to data points, scaling typically quadratically or worse in M , we follow
a somehow data-independent approach and discretize the feature space to obtain
a classiﬁcation algorithm that scales linearly in M : We restrict the problem to
a ﬁnite dimensional space VN spanned by N basis functions φj to obtain our
N
classiﬁer fN (x) =
j=1 αj φj (x), in our case the space of d-linear functions.
Minimization of H[f ] leads to a linear system with N unknowns,
λM C + B · B T α = By,

(1)

with Cij = (∇φi (x), ∇φj (x))L2 and Bij = φi (xj ).
To counter the curse of dimensionality and to avoid N d unknowns in d dimensions, we use sparse grids, described for example in [3]. Regular sparse grids
(1)
Vn up to level n in each direction base on a hierarchical formulation of basis
functions and an a priori selection of grid points – needing only O(N log(N )d−1 )
grid points with just slightly deteriorated accuracy. Sparse grids have been used
for classiﬁcation via the combination technique [1], where the sparse grid solution is approximated by a combination of solutions for multiple, but smaller
and regular grids. Sparse grids have the nice property that they are inherently
adaptive, which is what we will make use of in the following sections.
Using sparse grids, the system of linear equations can be solved iteratively,
each iteration scaling only linearly in the number of training data points and
grid points, respectively. The underlying so-called UpDown algorithm which was
shown in [2] bases on traversals of the tree of basis functions.

2

Grid-Based Development

For classiﬁcation using regular sparse grids there are two important parameters
determining the accuracy of the classiﬁer to be learned. First, we have n, the
maximum level of the grid. For low values of n there are usually not enough
degrees of freedom for the classiﬁer to be able to learn the underlying structure,
whereas large n lead to overﬁtting, as each noisy point in the training data set
can be learnt. Second and closely related, there is the regularization parameter
λ, steering the trade-oﬀ between smoothness and approximation accuracy. Given
enough basis functions, λ determines the degree of generalization.
To ﬁnd good values for these parameters, heuristics or experience from other
problems can be used. For a ﬁxed n the accuracy is maximized for a certain
value of λ and decreases for higher or lower values, oscillating only little, for
example. Especially during the development stage, heuristics are not suﬃcient;
an optimal combination of both parameters for a certain parameter resolution is
of interest. To make things even harder, systems under development are usually

710

D. Pﬂ¨
uger, I.L. Muntean, and H.-J. Bungartz

not optimized for eﬃciency, but rather designed to be able to be ﬂexible so that
diﬀerent approaches can be tested.
Such parameter studies typically demand a signiﬁcant computational eﬀort,
are not very time critical, but should be performed within a reasonable amount
of time. This requires that suﬃcient computational power and/or storage space
is available to the developer at the moment when needed. Grid environments
grant users access to such resources. Additionally, they provide an easy access
and therefore simplify the interaction of the users – in our setting the algorithm
developers – with various resources [4]. In grid middleware, for example the
Globus Toolkit [5], this is achieved through mechanisms such as single sign-on,
delegation, or by providing extensive support for job processing. Parameter studies are currently among the most widespreaded applications of grid computing.
For the current work, we used the grid research framework GridSFEA (Gridbased Simulation Framework for Engineering Applications) [6]. This research
environment aims at providing various engineering simulation tasks, such as
ﬂuid dynamics, in a grid environment. GridSFEA uses the Globus Toolkit V4. It
comprises both server-side components (grid tools and services) and client-side
tools (such as an OGCE2 grid portal, using the portlet technology as a way to
customize the access to the grid environment).
The grid portal was extended by a portlet adapted to the requirements of
sparse grid classiﬁcation. Thus, support was introduced for sampling the parameter space, for automatic generation and submission of corresponding jobs, and
for collecting and processing of results. Figure 1 shows a snapshot of our current
portlet, running a sparse grid classiﬁcation for various values of λ.

Fig. 1. Performing sparse grid parameter studies using a portlet of the GridSFEA portal

The usage of this grid portal enables the algorithm developer to focus on the
design of the algorithm and the interpretation of the results, and to leave the
management of the computations to the framework. Furthermore, the portal
of GridSFEA allows the user to choose the grid environment to be used. This
way, the computations – here parameter studies – can be performed at diﬀerent
computing sites, such as supercomputing centres.

Adaptive Sparse Grid Classiﬁcation Using Grid Environments

3

711

Adaptivity and Results

The need for a classiﬁcation algorithm that scales only linearly in the number
of training data points forces us in the ﬁrst place to trade the possibility to
make use of the structure of the training data for a somehow data-independent
discretization of the feature space. Therefore we have to deal with a higher
number of basis functions than common classiﬁcation algorithms which try to
ﬁt the data with as few basis functions as possible. Even though one iteration
of a sparse grid solver scales only linearly in the number of unknowns, this still
imposes restrictions on the maximum depth of the underlying grid.
The idea of adaptive sparse grid classiﬁcation is to obtain a trade-oﬀ between
common, data-dependent algorithms and the data independence of sparse grid
classiﬁcation. The aim is to reduce the order of magnitude of unknowns while
keeping the linear time training complexity: Especially those grid points should
be invested that contribute most to the classiﬁcation boundary, as the zerocrossing of the classiﬁer determines the class prediction on new data. For the
classiﬁcation task, the exact magnitude of the approximation error is not important in regions that are purely positive or negative. It is reasonable to spend
more eﬀort in rough regions of the target function than in smooth ones.
As we already showed in [2], special consideration has to be put on treating the
boundary of the feature space. Normally, in sparse grid applications, grid points
have to be invested on the boundary to allow for non-zero function values there.
Employing adaptivity in classiﬁcation allows us to neglect those grid points as
long as it is guaranteed that no data points are located exactly on the boundary.
This corresponds to the assumption that the data belonging to a certain class is
somehow clustered together – and therefore can be separated from other data –
which means that regions towards the border of the feature space usually belong
to the same class. In regions where the classiﬁcation boundary is close to the
border of the feature space, adaptivity will take care of this by reﬁning the grid
where necessary and by creating basis functions that take the place of common
boundary functions.
Thus we normalize our data to ﬁt in a domain which is slightly smaller than
the d-dimensional unit hypercube, the domain of fN . This way we can start
(1)
the adaptive classiﬁcation with the d-dimensional sparse grid for level 2, V2 ,
without the boundary and therefore only 2d + 1 grid points, rather than using
d
d d−j
(1 + 2j) =
grid points on the boundary, which would result in
j=0 j 2
d
d−1
d
∈ O(d · 3 ) unknowns.
3 + 2d · 3
For the remaining part of this section we proceed as follows: Starting with
(1)
V2 , we use a preconditioned Conjugated Gradient method for a few iterations
to train our classiﬁer. Out of all of the reﬁnement candidates we choose the
grid point with the highest surplus, add all the children to our current grid and
repeat this until satisﬁed. One has to take care not to violate one of the basic
properties of sparse grids: For each basis function all parents in the hierarchical
tree of basis functions have to exist. All missing ancestors have to be created,
which can be done recursively.

712

D. Pﬂ¨
uger, I.L. Muntean, and H.-J. Bungartz

3.1

Classiﬁcation Examples

The ﬁrst example, taken from Ripley [7], is a two-dimensional artiﬁcial dataset
which was constructed to contain 8% of noise. It consists of 250 data points
for training and 1000 points to test on. Being neither linear separable nor very
complicated, it shows typical characteristics of real world data sets. Looking
for a good value of λ we evaluated our classiﬁer after six reﬁnement steps for
λ = 10−i , i = 0 . . . 6, took the ones with the best and the two neighbouring
accuracies, namely 0.01, 0.001 and 0.0001, and looked once more for a better
value of lambda in between those by evaluating for λ = 0.01 − 0.001 ·i, i = 1 . . . 9
and λ = 0.001 − 0.0001 · i, i = 1 . . . 9. The best λ we got this way was 0.004.
Figure 2 shows the training data (left) as well as the classiﬁcation for λ = 0.004
and the underlying adaptive sparse grid after only 7 reﬁnement steps (right).
Even though there are only a few steps of reﬁnement, it can clearly be observed
that more grid points are spent in the region with the most noise and that
regions with training data belonging to the same class are neglected. Note that
one reﬁnement step in x1 -direction (along x2 = 0.5) causes the creation of two
children nodes in x2 -direction (at x2 = 0.25 and x2 = 0.75).
1

0.8

0.6

0.4

0.2

0

0.2

0.4

0.6

0.8

1

Fig. 2. Ripley dataset: training data and classiﬁcation for λ = 0.004

Table 1 shows some results comparing diﬀerent sparse grid techniques. For
the combination technique and sparse grids with and without grid points on the
boundary, we calculated the accuracy for level one to four and λ := 10−j − i ·
10−j−1 , i = 0 . . . 8, j = 0 . . . 5, each. Increasing the level further does not improve
the results as this quickly leads to overﬁtting. We show the number of unknowns
for each grid, the best value of λ, and the best accuracy achieved on the test data.
A general property of sparse grid classiﬁcation can be observed: The higher the
number of unknowns, the more important the smoothness functional gets and
therefore the value for the regularization parameter λ increases. Using a coarser
grid induces a higher degree of smoothness, which is a well-known phenomenon.

Adaptive Sparse Grid Classiﬁcation Using Grid Environments

713

Table 1. Ripley dataset: accuracies [%] obtained for the combination technique, regular
sparse grids with and without boundary functions, and the adaptive technique
comb. techn.
n |grids|
λ
1
9 6 · 10−5
2
39 0.0005
3
109 0.005
4
271 0.007

sg boundary

acc. |grid|
λ
90.3
9 6 · 10−5
90.8
21 0.0004
91.1
49 0.006
91.1
113 0.005

acc.
90.3
90.7
91.2
91.2

sg

adapt. sg
λ = 0.004
|grid|
λ
acc. |grid| acc.
1*
50.0
5 89.9
5 0.0005 90.3
9 90.2
17 0.005 91.2
13 89.8
49 0.007 91.2
19 91.1
21 91.2
24 91.2
28 91.3
32 91.4
35 91.5

Another observation is that the assumption that grid points on the boundary
can be neglected, which holds here even for regular grids. Of course, the sparse
grid on level 1 can do nothing but guess one class value for all data points and
ﬁve unknowns are not enough to achieve an accuracy of 90.7%, but already for
level 3 the same accuracy as for the grid with boundary values is reached.
For the adaptive sparse grid classiﬁcation we show the results only for λ =
0.004 for eight times of reﬁnement. With less grid points than the sparse grid
with boundary on level 3, we achieved an excellent accuracy of 91.5% – 0.3%
higher than what we were able to get using regular sparse grids. This is due to
the fact that increasing a full level results in a better classiﬁcation boundary
in some parts, but leads to overﬁtting in other parts at the same time. Here,
adaptivity helps to overcome this problem.
We used our grid environment to compute and gather the results for these 180
runs for each level, even though the problem sizes are quite low: Our implementation of the combination technique has just been done for comparing it with
the direct sparse grid technique, for example, and it is therefore far from being
eﬃcient. We neither use an eﬃcient solver as a multigrid technique for example,
nor care about the amount of memory we consume. We are just interested in
getting eventually some results, a problem suited for grid environments. As the
portlet mentioned in Sec. 2 allows us to easily specify a set of parameters, the
adaptation for the use in GridSFEA was just a matter of minutes.
A second, 6-dimensional dataset we used is a real-world dataset taken from
[8] which contains medical data of 345 patients, mainly blood test values. The
class value indicates whether a liver disease occurred or not. Because of the
limited data available we did a stratiﬁed 10-fold cross-validation. To ﬁnd a good
value for λ we calculated the accuracy for diﬀerent values of the regularization
parameter as we did above for the regular sparse grids. This time we calculated
the accuracy of the prediction on the 10-fold test data. Again we used GridSFEA
to compute and gather the results. Theoretically one could even submit a job
for each fold of the 10-fold cross-validation and gather the diﬀerent accuracies

714

D. Pﬂ¨
uger, I.L. Muntean, and H.-J. Bungartz

afterwards, but as the coeﬃcients computed for the ﬁrst fold can be used as a
very good starting vector for the PCG in the other 9 folds, this was neglected.
Table 2 shows some results, comparing the results of the adaptive technique
(zero boundary conditions) with the best ones taken from [9], the combination
technique (anisotropic grid, linear basis functions), and from [10], both linear and
non-linear support vector machines (SVM). The adaptive sparse grid technique
Table 2. Accuracies [%] (10-fold testing) for the Bupa liver dataset
adapt. sg
λ = 0.3
# reﬁnements |grid|
7
485
8
863
9
1302

acc.
70.71
75.64
76.25

comb. techn.
lin. anisotrop.
acc.
73.9

SVM
linear
acc.
70.0

SVM
non-linear
acc.
73.7

was able to outperform the other techniques. For a relatively large value of λ
and after only 9 reﬁnement steps we got a 10-fold testing accuracy which is more
than 2.3% higher than our reference results – with only 1302 unknowns. Again
it proved to be useful to reﬁne in the most critical regions while neglecting parts
of the domain where the target function is smooth.
3.2

Dimension Adaptivity

Similar observations can be made for a third dataset, the Pima diabetes dataset,
taken again from [8]. 769 women of Pima Indian heritage living near Phoenix
were tested for diabetes mellitus. 8 features describe the results of diﬀerent
examinations, such as blood pressure measurements. For reasons of shortness we
will focus only on some observations of the dimension adaptivity of the adaptive
sparse grid technique.
As assumed, the adaptive reﬁnement neglects dimensions containing no information but only noise quite well. Extending the diabetes dataset for example by
one additional feature with all values set to 0.5 leads to two more grid points
(1)
for V2 . As the two additional surpluses are close to zero, the eﬀects on BB T
and By are just minor, see (1). But extending the dimensionality modiﬁes the
smoothness functional. There are stronger impacts on C and therefore the tradeoﬀ between smoothness and approximation error changes. One could expect that
the same eﬀects could be produced by changing the value of λ suitably. And in
fact, for training on the ﬁrst 567 instances and testing on the remaining 192 data
points, almost identical accuracies (about 74.5%) can be achieved when changing λ to 0.0002 compared to extending the dimensionality by one for λ = 0.001
– at least for the ﬁrst few reﬁnements.
Further improvement could be achieved by omitting the “weakest” dimension,
the one with the lowest surpluses, during reﬁnement. When some attributes are
expected to be less important than others, this could lead to further improvements considering the number of unknowns needed.

Adaptive Sparse Grid Classiﬁcation Using Grid Environments

4

715

Summary

We showed that adaptive sparse grid classiﬁcation is not only possible, but useful.
It makes use of both worlds: the data-independent and linear runtime world and
the data-dependent, non-linear one that reduces the number of unknowns.
If increasing the level of a regular sparse grid leads to overﬁtting in some
regions but improves the quality of the classiﬁer in others, adaptivity can take
care of this by reﬁning just in rough regions of the target function. In comparison
to regular sparse grids, the adaptive ones need far less unknowns. This can reduce
the computational costs signiﬁcantly.
Considering dimension adaptivity we pointed out ﬁrst observations. Further
research has to be invested here.
Finally we demonstrated the use of GridFSEA, our grid framework, for parameter studies, especially during algorithm development stage.
Theoretically, even high dimensional classiﬁcation problems could be tackled
by adaptive sparse grid classiﬁcation as we showed that we can start with a
number of grid points which is linear in the dimensionality. Practically, our
current smoothness functional permits high dimensionalities by introducing a
factor of 2d in the number of operations. Therefore we intend to investigate the
use of alternative smoothness functionals in the near future to avoid this.

References
1. Garcke, J., Griebel, M., Thess, M.: Data mining with sparse grids. Computing
67(3) (2001) 225–253
2. Bungartz, H.J., Pﬂ¨
uger, D., Zimmer, S.: Adaptive Sparse Grid Techniques for
Data Mining. In: Modelling, Simulation and Optimization of Complex Processes,
Proc. Int. Conf. HPSC, Hanoi, Vietnam, Springer (2007) to appear.
3. Bungartz, H.J., Griebel, M.: Sparse grids. Acta Numerica 13 (2004) 147–269
4. Foster, I., Kesselman, C.: The Grid: Blueprint for a New Computing Infrastructure.
Morgan Kaufmann Publishers (2005)
5. Foster, I.: Globus toolkit version 4: Software for service-oriented systems. In:
IFIP International Conference on Network and Parallel Computing. Volume 3779
of LNCS., Springer-Verlag (2005) 2–13
6. Muntean, I.L., Mundani, R.P.: GridSFEA - Grid-based Simulation Framework for
Engineering Applications. http://www5.in.tum.de/forschung/grid/gridsfea
(2007)
7. Ripley, B.D., Hjort, N.L.: Pattern Recognition and Neural Networks. Cambridge
University Press, New York, NY, USA (1995)
8. Newman, D., Hettich, S., Blake, C., Merz, C.: UCI repository of machine learning
databases. http://www.ics.uci.edu/∼ mlearn/MLRepository.html (1998)
9. Garcke, J., Griebel, M.: Classiﬁcation with sparse grids using simplicial basis
functions. Intelligent Data Analysis 6(6) (2002) 483–502
10. Fung, G., Mangasarian, O.L.: Proximal support vector machine classiﬁers. In:
KDD ’01: Proceedings of the seventh ACM SIGKDD international conference on
Knowledge discovery and data mining, New York, NY, USA, ACM Press (2001)
77–86

