Generic Control Interface for Networked Haptic
Virtual Environments
Priscilla Ramsamy, Adrian Haﬀegee, and Vassil Alexandrov
Advanced Computing and Emerging Technologies Centre,
The School of Systems Engineering, University of Reading,
Reading, RG6 6AY, United Kingdom
p.ramsamy@reading.ac.uk

Abstract. As Virtual Reality pushes the boundaries of the human computer interface new ways of interaction are emerging. One such technology is the integration of haptic interfaces (force-feedback devices) into
virtual environments. This modality oﬀers an improved sense of immersion to that achieved when relying only on audio and visual modalities.
The paper introduces some of the technical obstacles such as latency and
network traﬃc that need to be overcome for maintaining a high degree
of immersion during haptic tasks. The paper describes the advantages
of integrating haptic feedback into systems, and presents some of the
technical issues inherent in a networked haptic virtual environment. A
generic control interface has been developed to seamlessly mesh with existing networked VR development libraries.
Keywords: Virtual Reality, Force Feedback.

1

Introduction

“The real power of VR technology is that it allows people to expand their perception of the real-world in ways that were previously impossible”[1]. Changes
to virtual objects and their interrelationships can be eﬀected with ease, which is
often not feasible with real objects. Through VR market researchers are now able
to generate precisely the same stimulus conditions for all participants and are
able to modify certain environment variables in real-time thus allowing them
to have more control over the experimental settings. Our study is centred on
exploring shoppers’ habits for market research with regards to how they move
about in their search for desired items within the shopping centre. Based on
these results market researches can provide the relevent foundation for developing eﬀective marketing strategy and advertising campaigns to boost sales. A
random sample of people categorised by age was selected and were interviewed
about their experience regarding the eﬃciency, ease of use and degree of realism
to the physical world that the application had. Analysis of the collected data
showed that respondents not only experienced diﬃculty to navigate in the 3D
environment using the wand but also in its use to actually get hold of certain
objects by pressing buttons. It was then decided to develop a more natural and
Y. Shi et al. (Eds.): ICCS 2007, Part II, LNCS 4488, pp. 768–775, 2007.
c Springer-Verlag Berlin Heidelberg 2007

Generic Control Interface for Networked Haptic Virtual Environments

769

intuitive mode of interaction and navigation to better suit the users in such an
environment.
Input devices such as computer keyboards, mice and joysticks can convey the
user’s commands to the computer but are unable to provide a natural sense of
touch and feel to the user. It is crucial to understand the nature of touch interaction, since we react to our surroundings based on what and how we perceive
things [2]. The user’s sensorial experience in a virtual environment is a fusion of
information received through his perception channels. The results obtained from
previous studies have made it possible to quantify the eﬀectiveness of using haptics to provide a greater sense of presence and immersion. Based on these ﬁndings
[3][4][5][6] there is great potential to improve VR applications. The additional information from haptic feedback makes certain tasks such as manipulation much
easier compared to that of the traditional computer interface [7]. In an eﬀort
to bring in more realistic and intuitive experiences in the virtual environment
created, a haptic trolley and an instrumented glove were incorporated with the
VE. The study consists of augmenting an immersive graphic display (CAVE like
system[8]) with a haptic interface.
The integrated system is multi-frequency since the haptic loop and the display/rendering loop require diﬀerent update rates. The haptic loop should be
maintained with an update rate of 1KHz to avoid force artifacts and because
humans can detect discrete events at less than this rate. The display loop should
be updated at 30Hz(minimum) to produce stable images and since the human
visual system has a ﬂicker frequency around 30-60Hz. An eﬀective integrated
system of haptics and graphics should ensure the eﬀectiveness of each modality. The users experience and sense of immersion would be enhanced and would
complement the visual display only if both the haptic and the visual feedback
are consistent with each other. Evidence show that latency is a limiting factor
in shared haptic environments [9]. Latency creates a lag from when collision is
detected and when a force is applied to the display device. The latency experienced for local systems is very small(instantaneous) however, for systems that
are distributed across a network the lag experienced could generate instability
during haptic rendering. A generic control interface has been developed at Reading University which forwards the relevant device information over the network
to both the haptic and rendering loop. The latency felt is minimal as we are
integrating the haptic display on a local system.
In section 2 of this paper, we give an overview of the software architecture
implemented as well as the networking protocol used and how its implementation
has been utilised in real-time. Section 3, presents the current feedback device
operations developed. Finally the future work is outlined in section 4.

2

Software Architecture

Haptic simulations involve a human operator, a force reﬂecting device and a virtual environment. Haptic is a term derived from the Greek “haptesthai” which
means “to come in contact with” by providing the sense of touch with both tactile

770

P. Ramsamy, A. Haﬀegee, and V. Alexandrov

(cutaneos) and kinaesthetic (proprioception) feedback [10]. Tactile, or touch
feedback describes the sensations felt by the skin. Tactile feedback permits users
to feel things such as the texture of surfaces, temperature and vibration. kinaesthetic force feedback on the other hand enables us to recognise a force that
is applied to our body by the help of sensoric cells located at the end of the
tendons or between the muscle strands. The human operator kinaesthetically
explores the virtual environment by interacting and grasping an active mechanical device. The device provides a new sensory display modality and presents
the necessary information by exerting controlled forces on the human operator.
Figure 3. shows the mobile platform developed. The device is equipped with an
actuator that produces the relevant forces and a sensor from which the velocity
can be calculated. The motion and position of the device is coupled to a virtual
object in the virtual environment. The virtual object should experience the same
forces and movement as experienced by the physical device.

Fig. 1. Software architecture for multi-processing

Several issues can contribute to limiting haptic device capability to render
and provide the relevant forces. To provide the desire results the system should
update in real-time however to have higher accuracy would require that we
sacriﬁce the update speed. Therefore due consideration should be given between
the trade-oﬀs of being highly eﬃcient or highly accurate.
The generic interface in Figure 1. currently forwards the relevant information
that is velocity, force and position over the network. These values are then used
by both the visual-rendering and haptic-rendering loop to produce the necessary
visual and haptic feedback. In our implementation the haptic and the graphic
loops have their own databases, which are not shared. This architecture enables
us to run the haptic process and visual process on either diﬀerent computer
systems or on the same system. For this multi-processing structure the critical
task would be to update both the graphic and the haptic database. The visual

Generic Control Interface for Networked Haptic Virtual Environments

771

rendering subsystem displays the necessary scenes onto the display system and
this is usually run at rates of up to 80Hz. For this work the VieGen framework
as described in [11] was used for control and rendering of the scene.
2.1

Transport Protocol Used

Since the transmission speed would have a direct bearing on the update rate of
the simulation engine the solution chosen was a trade-oﬀ to provide maximum
ﬂexibility and high throughput at the expense of the implementation being unreliable. Currently the User Datagram protocol (UDP) transport protocol is used
to route data packets over the network. However based on the fact that the
network currently being used is a close coupled controlled network, reliability
due to lost packets is not be an issue. Error recovery for lost packets has been
inbuilt in the system. The interface is written in C programming language and
uses the stream socket or connected datagram socket for sending and receiving
data across the network.
2.2

The Client/Server Architecture

The proposed design is a client/server architecture. The real-time implementation is done over a LAN and consists of 3 components namely the client, the
server and the generic control interface. The server component : is responsible
for receiving packets from the force reﬂecting/haptic device through the xPC
TargetBox. The server is then responsible for forwarding the relevant information over the network. A generic control interface deals with this information
by writing and storing it into shared memory on each system. The following
ﬂowchart Figure 2. depicts the current workings of the server application. This
application unpacks the packets received, which consists of data for both the
instrumented glove and the mobile platform. Once the data is unpacked the information is stored in the relevant structure in shared memory. Calls are made
to the underlying methods in the generic control interface to write to shared
memory. Error checks have been implemented to verify whether memory has
been initialised. If not, a memory segment is initialised and allocated to accommodate the information for both the instrumented glove and the mobile platform
data structures. Since the packets sent over the network are updated at a high
frequency writing to shared memory should be synchronised to be able to update the information at the same rate. The client component : is responsible for
retrieving the relevant information from memory and using it to render scenes
in the Virtual environment. The update/refresh rate can be set according to the
application requirement. The server component receives data from the haptic
interface via the network whilst the client interface sends the force values back
to the haptic interface. Both Client/Server components run on the same host.
As mentioned previously the motion and position of the haptic trolley is coupled to a virtual trolley object in the virtual environment. The virtual object
should experience the same forces and movement as experienced by the physical
device. Hence to provide a realistic experience to end-users the scene rendered

772

P. Ramsamy, A. Haﬀegee, and V. Alexandrov

Fig. 2. Server and Client Application Flowchart

should portray the changes in real-time and this is achieved by enabling a high
refresh/update rate. The interface was tested and produced successful results as
to providing a realistic enhanced experience to the end-user. Since the current
network implementation was based on the User Datagram protocol less overhead
and network lag was experienced. We are currently experiencing a round trip
time of 1ms for every packet being sent whilst the grounded trolley is being used
as a navigational tool. The ﬂowchart in Figure 2. depicts the current workings
of the client application.
2.3

Application Development

The shopping application was developed to allow users in the ReaCTor to shop
in the virtual environment by interacting with 3D models of diﬀerent products
and brands. A more intuitive mode of navigation was provided by integrating the
trolley during user trials. The application implemented made use of the VieGen
framework as described in [10]. VieGen provides several libraries of tools and
utilities that are useful in the development of Virtual Interactive Environments.
The elements making up the framework consists of the interface to the display
hardware, a networking subsystem, scene management, environment simulation
and accompanying utilities, all of which can be used or left out as depending on

Generic Control Interface for Networked Haptic Virtual Environments

773

the requirements. The CAVE Scene Manager component (CSM) oﬀers a generic
abstraction layer to the underlying VR hardware and this allows the same application to be compiled for a variety diﬀerent systems. At run-time a conﬁguration
ﬁle is then used to stipulate the required system set-up and mode of operation
and by using this approach, it was possible to use the VieGen framework to run
on diﬀerent display systems. Ranging from mono desktop systems through Powerwalls and through to immersive CAVE like systems. With such an application,
it not only implies application interoperability throughout the entire range of
hardware systems, but it also lends itself to application development and testing
without the excessive tying up of the essential immersive system resources.
The networking subsystem consists of two layers. While the low level layer provides a consistent, low latency infrastructure for buﬀering messages to and from
multiple remote locations, the higher level layer [9] builds on top of this to provide a hierarchical topology for Distributed Virtual Environments (DVEs). This
topology assumes that the environment is split into domains and controls the
individual users in these domains. One of the users is dynamically and transparently allocated (designated) as the Domain Server, and is consequently used by
the topology to handle connections between the remaining clients. Later these
connections are used to relay information such as position, action events or
user-deﬁned messages. The modules for simulation are run concurrently with
the networking subsystem. This operation forms the basis for establishing an
environment as a domain, and allows its population by a number of computer
generated users. It is then possible for these pseudo-users to perform actions
within the environment to collect statistical data, perform tests and statistical
analysis.

3

Device Operations

In our implementation we have a instrumented tactile glove and a grounded
haptic mobile platform both are currently being developed at Reading University. The tactile glove can replace or be used in conjunction with the traditional
wand to provide a more realistic and intuitive way of manipulating virtual objects. The generic control interface developed permits function calls to the device
state whereby the relevant information can be retrieved. The existing functions
can retrieve the hand orientation and ﬁnger angles. The appropriate feedback
can be rendered based on these values retrieved and the chosen collision detection algorithm. Once collision is detected with a manipulatable object the
person would be able to interact with and manipulate the object. The virtual
object will be attached to the glove only while the grasp aperture (the distance
between thumb and ﬁngers) is less than a critical distance. The critical distance
can be ﬁxed or vary depending on the size of the object being manipulated.
The following ﬁgure 3 shows the glove in question being used in the immersive
shopping environment to provide a more intuitive way to manipulate objects in
the environment. The haptic mobile platform is currently being used to replace

774

P. Ramsamy, A. Haﬀegee, and V. Alexandrov

Fig. 3. Glove and trolley in use

the traditional joystick and permits the user to navigate within the virtual environment. It could mimic several devices ranging from a bicycle to a trolley or
hang glider. Information on the device sate, the force applied, the displacement
forward or backwards and the angle of rotation can be retrieved. Figure 3 shows
the trolley being used as a navigational tool in the shopping environment.

4

Conclusion and Future Work

Our generic control interface has been successfully developed and permits both
the tactile and force feedback devices to integrate with our immersive system,
and is being used in ongoing application development. The current work in
progress involves making the generic interface a platform independent portable
interface. The generic control interface provides a ﬂexible method of developing
libraries that acts as an interface between diﬀerent input devices and existing
CVEs.
Based on the requirements for stability and ﬁdelity a very high update rate
has been used in our current implementation. However to minimise transmission
requirements updates need only be made when there is a change in state. Our
future work will concentrate in implementing such an architecture to enable an
improved local interaction and to ﬁnd the eﬀects on the degree of immersion
and haptic feedback of the system. The magnitude of inconsistencies between
states of the local copies of the VE will be evaluated to verify the network delay
and the amount of packet loss. Reﬁnement of current haptic device operations
and incorporating an increased number of vibrotactile stimulators onto each
ﬁnger of the glove would also be considered. This would enhance and provide
more functionality to the instrumented glove would enable a realistic mode of
interaction in VEs.
The current implementation considers the integration of haptic information to
augment visual displays on a local system. The latency experienced is minimal
in such a situation. However the work will be extended to provide a haptically

Generic Control Interface for Networked Haptic Virtual Environments

775

mediated collaborative virtual environment. This would be challenging considering that the users would be graphically distributed and various problems such
as network delay and latency would be introduced.

References
1. J.L. Mohler, “Desktop Virtual Reality for the Enhancement of Visualization Skills”.
Journal of Educational Multimedia and Hypermedia,(2000) 151-165.
2. M.A. Srinivasan and C. Basdogan, “Haptics in Virtual Environments: Taxonomy,
Research Status, and Challenges,” Computers and Graphics, Special Issue on Haptic Displays in Virtual Environments, Vol. 21, No. 4, (1997)
3. Burdea, G.C. Haptics issues in virtual environments. In Computer Graphics International,Proceedings. (2000)
4. F. P. Brooks, M. Ouh-Young, J. J. Batter, and P. J. Kilpatrick. Project GROPE
– Haptic Displays for Scientiﬁc Visualization. In Proc. ACM SIGGRAPH, Dallas,
TX, Aug (1990) 177–185
5. Michael Dinsmore, Noshir Langrana, Grigore Burdea, Jumoke Ladeji. “Virtual
Reality Training Simulation for Palpation of Subsurface Tumors,” vrais, Virtual
Reality Annual International Symposium, (1997) 54
6. Burdea, G., G. Patounakis, V. Popescu, & R. E. Weiss. Virtual Reality Training
for the Diagnosis of Prostate Cancer. In IEEE International Symposium on Virtual
Reality and Applications, Atlanta, Georgia, March (1998) 190-197
7. Andrew G. Fischer, Judy M. Vance, “Implementing Haptic Feedback in a Projection Screen Virtual Environment,” Seventh PHANToM Users Group Workshop,
October (2002) 26-29
8. Cruz-Neira, C.,Sandin, S.J.,DeFanti, T.A., Kenyon, R.V., and Hart, J.C. The
CAVE: Audio visual experience automatic virtual environment. Communications
of the ACM 35 (1992) 64-72
9. Ottensmeyer MP, Hu J, Thompson JM, Ren J, and Sheridan TB Investigations into
performance of minimally invasive telesurgery with feedback time delays. Presence
9 (4), (Aug 2000) 369-382.
10. M. Mokhtari, F. Bernier, F. Lemieux, H. Martel, J.M. Schwartz, D. Laurendeau
and A. Branzan-Albu. Virtual Environment and Sensori-Motor Activities: Haptic,
Audition and Olfaction. WSCG POSTERS proceedings Plzen, Czech Republic.,
Vol.12, No.1-3, February, (2004) 2-6
11. Haﬀegee, A., Jamieson, R., Anthes, C., Alexandrov, V. “ Tools for Collaborative
VR Application Development.” In: International Conference on Computational
Science, Springer Verlag (2005) 350358

