An Eﬃcient Algorithm and Its Parallelization
for Computing PageRank
Jonathan Qiao, Brittany Jones, and Stacy Thrall
Converse College
Spartanburg, SC 29302, USA
{Jonathan.Qiao,Brittany.Jones,Stacy.Thrall}@converse.edu

Abstract. In this paper, an eﬃcient algorithm and its parallelization to
compute PageRank are proposed. There are existing algorithms to perform such tasks. However, some algorithms exclude dangling nodes which
are an important part and carry important information of the web graph.
In this work, we consider dangling nodes as regular web pages without
changing the web graph structure and therefore fully preserve the information carried by them. This diﬀers from some other algorithms which
include dangling nodes but treat them diﬀerently from regular pages
for the purpose of eﬃciency. We then give an eﬃcient algorithm with
negligible overhead associated with dangling node treatment. Moreover,
the treatment poses little diﬃculty in the parallelization of the algorithm.
Keywords: PageRank, power method, dangling nodes, algorithm.

1

Introduction

A signiﬁcant amount of research eﬀort has been devoted to hyperlink analysis
of the web structure since Sergey Brin, Larry Page brought their innovative
work [1] to the information science community in 1998. Brin and Page launched
Google at the time when there were already a number of search engines. Google
has succeeded mainly because it has a better way of ranking web pages, which
is called PageRank by its founders.
The original PageRank model is solely based on the hyperlink structure. It
considers the hyperlink structure of the web as a digraph. For each dangling
node (a page without any out-link), edges are added so that it is connected to
all nodes. Based on this, an adjacency matrix can be obtained. It is then to ﬁnd
the eigenvector of the adjacency matrix, whose elements are the ranking values
of the corresponding web pages. Due to the size of the web graph, the dominant
way in ﬁnding the eigenvector is the classic power method [2], which is known
for its slow convergence. Because of this, a large amount of work has been done
to speed up the PageRank computation since 1998.
One factor contributing substantially to the computational cost is the existence of dangling nodes. A single dangling node adds a row full of ones to the
adjacency matrix. Including dangling nodes not only produces a signiﬁcantly
Y. Shi et al. (Eds.): ICCS 2007, Part I, LNCS 4487, pp. 237–244, 2007.
c Springer-Verlag Berlin Heidelberg 2007

238

J. Qiao, B. Jones, and S. Thrall

larger matrix, but also makes it no longer sparse. Some existing algorithms exclude dangling nodes from consideration in order to speed up the computation.
Those which consider them don’t treat them as regular pages for purpose of
eﬃciency. In either case, the original web structure is changed, and more importantly the information carried by dangling nodes is changed as well.
In this paper, we will aim at ﬁnding the PageRank vector using the power
method with dangling nodes fully included and treated as regular pages. We will
then propose an highly eﬃcient algorithm which minimizes the overhead of our
treatment of dangling nodes to be negligible in either centralized or distributed
approach.

2

PageRank Review

As discussed in Sect. 1, the web structure is treated as a digraph, which gives a
N ×N column-wise adjacency matrix, where N is the number of total web pages.
Let the matrix be A. Let D be a N × N diagonal matrix with each entry the
reciprocal of the sum of the corresponding column of A. Then, AD is a column
stochastic Markov matrix. Let v = ADe, where e is a vector of all ones. The
ith entry of v tells how many times the ith page gets recommended by all web
pages. Introducing the weight of a recommendation made by a page which is
inversely proportional to the total number of recommendations made by that
page gives the equation
v = ADv .
(1)
Equation (1) suggests that v is the ranking vector as well as an eigenvector of
AD with the eigenvalue 1.
By nature of the web structure, the Markov matrix AD is not irreducible [2].
This implies that there are more than one independent eigenvectors associated
with the eigenvalue 1. Google’s solution to this problem is to replace AD by a
convex combination of AD and another stochastic matrix eeT /N ,
v=

αAD +

1−α T
ee v ,
N

(2)

where 0 < α < 1 (Google uses α = 0.85). The underlying rational is, by probability 0.85, a random surfer chooses any out-link on a page arbitrarily if the
page has out-links; by probability 0.15, or if a page is a dead end, another page
is chosen at random from the entire web uniformly.
The convex combination in (2) makes the ranking vector v the unique eigenvector associated with the eigenvalue 1 up to scaling and the subdominant eigenvalue α, which is the rate of convergence when the power method is applied
[3,4,2]. By normalizing the initial and subsequent vectors, a new power method
formulation of (2) is
1−α
e ,
(3)
vi+1 = αADvi +
N
where v0 = (1/N )e because eT v = 1.

An Eﬃcient Algorithm and Its Parallelization for Computing PageRank

3

239

Related Work

Most algorithms so far focus on the cost reduction of each power method iteration, although there have been eﬀorts aiming at reducing the number of
iterations, such as [5]. Because the major obstacle of speeding up each iteration lies in the size of data, it is natural to compress the data such that it ﬁts
into the main memory, as by [6,7,8]. However, as pointed out by [6], even the best
compression scheme requires about .6 bytes per hyperlink, which still results in
an exceedingly large space requirement. Others are to design I/O-eﬃcient algorithms, such as [9,10], which can handle any size of data without any particular
memory requirement. Also aiming at reducing the cost of each iteration, many
other works, such as [11,12], combine some linear algebra techniques to reduce
the computational cost of each iteration. Nevertheless, the I/O-eﬃciency must
always be considered.
In [9], Haveliwala proposes two algorithms, Naive Algorithm and Blocking
Algorithm, and demonstrates high I/O eﬃciency of the latter when the ranking
vector does not ﬁt in the main memory. Applying the computational model in
(3), both algorithms use a binary link ﬁle and two vectors, the source vector
holding the ranking values for the iteration i and the destination vector holding
the ranking values for the iteration i + 1. Each diagonal entry of the matrix D
is stored in the corresponding row of the link ﬁle as the number of out-links of
each source page.
For purpose of the cost analysis, there are several more parameter families,
M , B(·), K and nnz(·).
1.
2.
3.
4.

The
The
The
The

total
total
total
total

number
number
number
number

of
of
of
of

available memory pages will be denoted M .
pages of disk/memory resident will be denoted B(·).
dangling nodes will be denoted K.
links of a web digraph will be denoted nnz(·).

Unless speciﬁed, the cost is for a single iteration since we focus on reducing the
cost of each iteration in this work.
3.1

Matrix-Vector Multiplication Treatment

Given the computation model in (3), computing a single entry of vi+1 requires
reading a row of matrix A and the source vector. Since the link ﬁle is sorted
by source node id’s and all elements of a row of matrix A are spread out in the
link ﬁle1 , a less careful implementation would result in one pass for calculating
a single value. Preprocessing the link ﬁle so that it is sorted by destination node
id’s not only needs tremendous eﬀort, but also adds N entries for each dangling
node to the link ﬁle and thus signiﬁcantly increases the storage requirement and
the I/O cost. To get around of this diﬃculty, we could use the column version
of the matrix-vector multiplication
Av = v0 A∗0 + v1 A∗1 + · · · + vn−1 A∗(n−1) ,
1

A column of A corresponds to a row in the adjacency list.

(4)

240

J. Qiao, B. Jones, and S. Thrall

where each A∗i is the ith column of A. The computation model in (4) requires
one pass of the link ﬁle and the source vector for ﬁnding the destination vector
when the destination vector can be a memory resident.
3.2

The Blocking Algorithm

Let v be the destination vector, v be the source vector and L be the adjacency
list of the link ﬁle. To handle the memory bottleneck, Blocking Algorithm in
[9] partitions v evenly into b blocks so that each vi ﬁts in (M − 2) pages. L
is vertically partitioned into b blocks. Note, each Li is then represented by a
horizontal block Ai . The partition gives the equation
v i = αAi Dv +

1−α
e ,
N

(5)

where i ∈ {0, 1, · · · , b − 1}.
Based on (5), computing each vi requires one pass of vi Li , v. Updating the
source vector at the end adds another pass of v. Therefore, the cost (referred to
as Cblock ) is
b−1

Cblock =

b−1

B(Li ) + (b + 1)B(v) +
i=0

B(vi ) = (b + 2)B(v) + (1 + )B(L) , (6)
i=0

where is a small positive number due to the partition overhead.
The cost in (6) does not scale up linearly when b is not a constant, which can
happen given the remarkably fast growth of the web repository.

4

Dangling Nodes Treatment

The cost model in (6) only counts disk I/O’s with an assumption that the inmemory computational cost is negligible compared to the I/O cost. The assumption is justiﬁable when A is sparse since the in-memory cost is approximately the
number of 1 s in A. This can be seen in [9], whose test data set which contains close
to 80% of dangling nodes originally is preprocessed to exclude all dangling nodes.
Many web pages are by nature dangling nodes, such as, a PDF document, an
image, a page of data, etc. In fact, dangling nodes are a increasingly large portion
of the web repositories. For some subsets of the web, they are about 80% [2].
Some dangling nodes are highly recommended by many other important pages,
simply throwing them away may result in a signiﬁcant loss of information. This
is why some existing works don’t exclude dangling nodes, such as [12], which
computes the ranking vector in two stages. In the ﬁrst stage, dangling nodes are
lumped into one; in the second stage, non-dangling nodes are combined into one.
The global ranking vector is formed by concatenating two vectors.
One of goals of this work is to make improvements over [9] with dangling
nodes included. Diﬀerent from [12] and some other algorithms which include
dangling nodes, our approach treats them as regular web pages. It can be easily

An Eﬃcient Algorithm and Its Parallelization for Computing PageRank

241

seen that including dangling nodes does not add any storage overhead since a
dangling node does not appear in the link ﬁle as a source id. Thus, our approach
does not change the I/O cost model in (6).
To minimize the in-memory computational overhead imposed by inclusion of
dangling nodes, we decompose A into Aˆ + eΔT , where Aˆ is an adjacency matrix
of the original web graph (a row full of zeros for a dangling node), Δ is a N × 1
vector with the ith entry 1 if the ith node is a dangling node and 0 otherwise.
Substituting the decomposition of A into (3), we have
ˆ i + ci + c ,
vi+1 = α(Aˆ + eΔT )Dvi + c = αADv

(7)

where c = e(1 − α)/N , a constant vector at all iterations, ci is a constant vector
at iteration i, whose constant entry is computed by adding all dangling node
ranking values at the beginning of each iteration, which is
(Dv)i .

(8)

out−degree(i)=0

This involves K (the number of dangling nodes) additions and one multiplication.
In the implementation, we extract out-degree’s from every Li and save them in
a separate ﬁle, which has N entries and can be used for computing (8). This
also has an advantage of reducing the storage overhead caused by partition since
each nonzero out-degree repeatedly appears in every Li .
A substantial gain of the in-memory cost can be achieved. Let G be an original
web digraph, then, the total number of ﬂoating number additions in (3) is
C1 = nnz(G) + KN + N = (r + K + 1)N ,

(9)

where r is the average out-links, which varies from 5 to 15 [10]. Based on (7),
this cost can be reduced to
C2 = nnz(G) + K = rN + K .

(10)

When K is large compared to N , which is often the case for the web data, C1 is
Θ(N 2 ) while C2 is Θ(N ).

5

The Parallelization of the Algorithm

The computation model in (7) can be readily parallelized without any extra
parallel overhead associated with inclusion of dangling nodes.
When applying Blocking Algorithm directly, we may vertically partition the
link ﬁle L into L0 , L1 , · · · , Lb−1 and distribute them over b nodes in a cluster.
Each node holds a partition of L, the source vector v and a partition of v . This
gives the computation model at each node
vi = αAi Dv + ci ,

(11)

where ci is a constant vector of the size N/b with (1 − α)/N for all entries.

242

J. Qiao, B. Jones, and S. Thrall

The above parallelization does not take the advantage given by the dangling
nodes treatment. Combining (5) and (7), the new parallel formulation can be
established as
ci + ci ,
(12)
v i = αAi Dv + ˆ
where i ∈ {0, 1, · · · , b − 1}, ˆ
ci is a constant vector at each iteration, and ci is
a constant vector at all iterations of the corresponding size as deﬁned in (7).
As discussed in Sect. 4, the vector representation of the matrix D is stored in a
separated ﬁle, which is read b times at each node, the same as a partition of the
link ﬁle.
Computing vi at the ith node is carried out in the same fashion as in the serial implementation. It needs to read Li , v and to write vi to update the source
vector. The I/O cost is B(Li ) + B(v) + B(vi ). One advantage of the distributed
algorithm can be seen in normalizing the destination vector. A serial implementation needs to read the whole destination vector. In the distributed case, each
partition of the destination vector is held in memory at its corresponding node
and the normalization can be done concurrently. Therefore, the I/O cost at each
node is
1
(13)
CI/O = B(Li ) + (1 + )B(v) ≈ B(L)/b + B(v) .
b
The in-memory computational cost does not have any parallel overhead and
is therefore
(14)
Cin−memory ≈ C2 /b .
The communication cost can be a bottleneck. To start a new iteration, every
node in the cluster needs to collect one block of the updated source vector from
every other node, which is 4N/b bytes. The total communication cost is
b−1

b−1

i=0

j=0,j=i

Ccomm =

4N/b

= 4(b − 1)N .

(15)

By making every pair of all nodes communicate concurrently, the communication
cost is reduced to O(N ) since
Ccomm = Ccomm ×

2
≈ 8N .
b

(16)

The cost models in (13), (14) and (16) show we could achieve a near linear
scale-up and a near linear speed-up provided that the data size, rN , is large
compared to N since the communication cost is independent of r.

6
6.1

Experimental Evaluation
Experimental Setup

Experiments for the algorithm handling dangling nodes were conducted on Linux
platform on a single dedicated machine with a 2.00 GHz Intel(R) Celeron(R)

An Eﬃcient Algorithm and Its Parallelization for Computing PageRank

243

Table 1. Data sets and the cost comparison
Name

Pages K/N

Links C1

r

C2 C1 /C2

California 9.7K 48.00% 1.67 16K 26.3s 0.77s 34.2s
Stanford 281K 0.06% 8.45 2.4M 58.8s 30.7s 1.9s

CPU. Experiments for the parallel algorithm were conducted on Windows platform on dedicated machines each with a 2.8 GHz Intel Pentium(R)-4 CPU. The
memory size in either case is ample in the sense that there is enough physical
memory for the OS. The page size of disk access for all experiments is 512 KB.
The implementations of both algorithms were in Java.
Table 1 shows the basic statistics for the two data sets. The ﬁrst data set,
California, is used solely to test the algorithm of handling dangling nodes.
It was obtained from http://www.cs.cornell.edu/Courses/cs685/2002fa/. The
second data set, stanford, is used for both algorithms. It was obtained from
http://www.stanford.edu/˜sdkamvar/.
6.2

Results for Handling Dangling Nodes

In Tab. 1, C1 represents the cost based on the computation model in (3), C2 represents the cost based on the computation model in (7), which handles dangling
nodes using the proposed algorithm.
The data set california needs only one disk access due to its small size. Its I/O
cost is then negligible. The dangling nodes in this data set are almost a half of
the total pages. The speedup obtained by the proposed algorithm, which is the
ratio of C2 and C1 , is 34.2. This veriﬁes the two cost models in (9) and (10). The
data set stanford is about 11.2M , which results in about 22 disk accesses. Even
though the dangling nodes in the data set are only about 0.06%, the speedup
obtained by the proposed algorithm is about 1.9.
6.3

Results for the Parallelization of the Algorithm

The parallel implementation uses the data set stanford only. The experiments
were conducted on a cluster of diﬀerent number of nodes. The experimental
results in Tab. 2 show we have achieved a near linear speed-up. One reason for
Table 2. Parallel running times and the corresponding speed-up’s
Number of Nodes
Elapsed Time
Speed-up

1

2

4

8

47.1s 23.7s 12.1s 6.2s
N/A 2.0

3.9 7.6

244

J. Qiao, B. Jones, and S. Thrall

the nice results is that the average out-degree of the experimental data set is 8.45,
which makes the data size much larger than the ranking vector size. Therefore,
the I/O cost and the in-memory computational cost weigh signiﬁcantly more
than the communication cost.

7

Conclusions and Future Work

In this paper, we have derived an eﬃcient algorithm and its parallelization for
computing PageRank with dangling nodes fully preserved. Both the analysis
and the experimental results demonstrate that our algorithm has little overhead associated with inclusion of dangling nodes. There are two areas for future
work: conducting experiments on the larger data sets and at a larger parallel
cluster; and exploring more fully dangling nodes’ impact and their treatment to
PagaRank computation.

References
1. Brin, S., Page, L., Motwami, R., Winograd, T.: The pagerank citation ranking:
bringing order to the web. Technical report, Computer Science Department, Stanford University (1999)
2. Langville, A.N., Meyer, C.D.: Deeper inside pagerank. Internet Math 1 (2004)
335–380
3. Haveliwala, T.H., Kamvar, S.D.: The second eigenvalue of the google matrix.
Technical report, Computer Science Department, Stanford University (2003)
4. Elden, L.: A note on the eigenvalues of the google matrix. Report LiTH-MAT-R04-01 (2003)
5. Kamvar, S., Haveliwala, T., Manning, C., Golub, G.: Extrapolation methods for
accelerating pagerank computations. Twelfth International World Wide Web Conference (2003)
6. Randall, K., Stata, R., Wickremesinghe, R., Wiener, J.: The link database: Fast
access to graphs of the web. In: the IEEE Data Compression Conference. (March
2002)
7. Adler, M., Mitzenmacher, M.: Towards compressing web graphs. In: the IEEE
Data Compression Conference. (March 2001)
8. Raghavan, S., Garcia-Molina, H.: Representing web graphs. In: the 19th IEEE
Conference on Data Engineering, Bangalore, India (March 2003)
9. Haveliwala, T.H.: Eﬃcient computation of pagerank. Technical report, Computer
Science Department, Stanford University (Oct. 1999)
10. Chen, Y., Gan, Q., Suel, T.: I/o-eﬃcient techniques for computing pagerank. In:
Proc. of the 11th International Conference on Information and Knowledge Management. (2002)
11. Kamvar, S., Haveliwala, T., Golub, G.: Adaptive methods for the computation of
pagerank. Technical report, Stanford University (2003)
12. Lee, C., Golub, G., Zenios, S.: A fast two-stage algorithm for computing pagerank
and its extension. Technical report, Stanford University (2003)

