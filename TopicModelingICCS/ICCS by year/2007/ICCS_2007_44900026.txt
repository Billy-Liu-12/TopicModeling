Discovering Latent Structures: Experience with
the CoIL Challenge 2000 Data Set
Nevin L. Zhang
Hong Kong University of Science and Technology, Hong Kong, China
lzhang@cse.ust.hk

Abstract. We present a case study to demonstrate the possibility of
discovering complex and interesting latent structures using hierarchical
latent class (HLC) models. A similar eﬀort was made earlier [6], but
that study involved only small applications with 4 or 5 observed variables. Due to recent progress in algorithm research, it is now possible to
learn HLC models with dozens of observed variables. We have successfully analyzed a version the CoIL Challenge 2000 data set that consists of
42 observed variable. The model obtained consists of 22 latent variables,
and its structure is intuitively appealing.
Keywords: Latent structure discovery, Bayesian networks, learning,
case study.

1

Introduction

Hierarchical latent class (HLC) models [7] are tree-structured Bayesian networks
where variables at leaf nodes are observed and are hence called manifest variables, while variables at internal nodes are hidden and hence are called latent
variables. All variables are assumed discrete. HLC models generalize latent class
(LC) models [3] and were ﬁrst identiﬁed as a potentially useful class of Bayesian
networks (BN) by Pearl [4].
HLC models can used for latent structure discovery. Often, observed variables
are correlated because they are inﬂuenced by some common hidden causes. HLC
models can be seen as hypotheses about how latent causes inﬂuence observed
variables and how they are correlated among themselves. Finding an HLC model
that ﬁts data amounts to ﬁnding a latent structure that can explain data well.
The CoIL Challenge 2000 data set [5] contains information on customers of a
Dutch insurance company. The data consists of 86 variables, around half of which
are about ownership of various insurance products. Diﬀerent product ownership
variables are correlated. One who pays a high premium on one type of insurance
is more likely, than those who do not, to also purchase other types of insurance.
Intuitively, such correlations are due to people’s (latent) attitudes toward risks.
The more risk-aversion one is toward one category of risks, the more likely one is
to purchase insurance products in that category. Therefore, the CoIL Challenge
2000 data set is a good testbed for latent structure discovery methods.
Y. Shi et al. (Eds.): ICCS 2007, Part IV, LNCS 4490, pp. 26–34, 2007.
c Springer-Verlag Berlin Heidelberg 2007

Discovering Latent Structures

X1

Y1

Y2

Y4
Y2

Y5

X1

X3

X2

Y1

27

Y3

Y5

Y6

Y7

Y3

X3

X2

Y4

Y6

Y7

Fig. 1. An example HLC model and the corresponding unrooted HLC model. The Xi ’s
are latent variables and the Yj ’s are manifest variables.

We have analyzed the CoIL Challenge 2000 data set using HLC models. The
structure of the model obtained is given in Section 4. There are 42 manifest
variables and 22 latent variables, and the structure is intuitively very appealing.
Latent structure discovery is very diﬃcult. It is hence exciting to know that we
are able to discover such a complex and meaningful structure.
HLC models can also be used simply for probabilistic modeling. They possess
two nice properties for this purpose. First, they have low inferential complexity due to their tree structures. Second, they can model complex dependencies
among the observed. In Section 5, the reader will see the implications of the
second property on prediction and classiﬁcation accuracy in the context of the
CoIL Challenge 2000 data.
We begin with a review of HLC models in Section 2 and a description of the
CoIL Challenge 2000 data set in Section 3.

2

Hirarchical Latent Class Models

Figure 1 shows an example HLC model (left diagram). A latent class (LC) model
is an HLC model where there is only one latent node. We usually write an HLC
model as a pair M = (m, θ), where θ is the collection of parameters. The ﬁrst
component m consists of the model structure and cardinalities of the variables.
We will sometimes refer to m also as an HLC model. When it is necessary to
distinguish between m and the pair (m, θ), we call m an uninstantiated HLC
model and the pair (m, θ) an instantiated HLC model.
Two instantiated HLC models M =(m, θ) and M =(m , θ ) are marginally equivalent if they share the same manifest variables Y1 , Y2 , . . . , Yn and
P (Y1 , . . . , Yn |m, θ) = P (Y1 , . . . , Yn |m , θ ).

(1)

An uninstantiated HLC models m includes another m if for any parameterization θ of m , there exists parameterization θ of m such that (m, θ) and (m , θ ) are
marginally equivalent, i.e. if m can represent any distributions over the manifest
variables that m can. If m includes m and vice versa, we say that m and m
are marginally equivalent. Marginally equivalent (instantiated or uninstantiated)
models are equivalent if they have the same number of independent parameters.
One cannot distinguish between equivalent models using penalized likelihood
scores.

28

N.L. Zhang

Let X1 be the root of an HLC model m. Suppose X2 is a child of X1 and it is
a latent node. Deﬁne another HLC model m by reversing the arrow X1 →X2 . In
m , X2 is the root. The operation is hence called root walking; the root has walked
from X1 to X2 . Root walking leads to equivalent models [7]. This implies that it is
impossible to determine edge orientation from data. We can learn only unrooted
HLC models, which are HLC models with all directions on the edges dropped.
Figure 1 also shows an example unrooted HLC model. An unrooted HLC model
represents a class of HLC models. Members of the class are obtained by rooting
the model at various nodes. From now on when we speak of HLC models we
always mean unrooted HLC models unless it is explicitly stated otherwise.
Assume that there is a collection D of i.i.d samples on a given set of manifest
variables that were generated by an unknown regular HLC model. The learning task is to reconstruct the unrooted HLC models that corresponds to the
generative model.
The ﬁrst principled algorithm for learning HLC models was developed by
Zhang [7]. The algorithm consists of two search routines, one optimizes model
structure while the other optimizes cardinalities of latent variables in a given
model structure. It is hence called double hill-climbing (DHC). It can deal with
data sets with about one dozen manifest variables. Zhang and Koˇcka [8] recently
proposed another algorithm called heuristic single hill-climbing (HSHC). HSHC
combines the two search routines of DHC into one and incorporates the idea of
structural EM [2] to reduce the time spent in parameter optimization. HSHC
can deal with data sets with dozens of manifest variables.
Results presented in this paper were obtained using the HSHC algorithm. The
algorithm hill-climbs in the space of all unrooted regular HLC models for the
given manifest variables. We assume that the BIC score is used to guide the
search. The BIC score of a model m is:
BIC(m|D) = logP (D|m, θ∗ ) −

d(m)
logN
2

where θ∗ is the ML estimate of model parameters, d(m) is the standard dimension
of m, i.e. the number of independent parameters, and N is the sample size.

3

The Coil Challenge 2000 Data Set

The training set of the COIL Challenge 2000 data consists of 5,822 customer
records. Each records consists of 86 attributes, containing sociodemographic information (Attributes 1-43) and insurance product ownerships (Attributes 4486). The sociodemographic data is derived from zip codes. In previous analyses,
these variables were found more or less useless. In our analysis, we include only
three of them, namely Attributes 43 (purchasing power class), 5 (customer main
type), and 4 (average age). All the product ownership attributes are included in
the analysis.
The data was preprocessed as follows: First, similar attribute values were
merged so that there are at least 30 cases for each value. Thereafter, the attributes have 2 to 9 values. In the resultant data set, there are fewer than 10

Discovering Latent Structures

29

cases where Attributes 50, 60, 71 and 81 take “nonzero” values. Those attributes
were therefore excluded from further analysis. This leaves us with 42 attributes.
We analyzed the data using a Java implementation HSHC algorithm. In each
step of search, HSHC runs EM on only one model to optimize all its parameters.
However, it may run local EM on several candidate models to optimize the
parameters that are aﬀected by search operators. The number of such candidate
models is denoted by K, and K is a parameter for the algorithm. We tried four
values for K, namely 1, 5, 10, and 20. The experiments were run on a Pentium 4
PC with a clock rate of 2.26 GHz. The running times and the BIC scores of the
resulting models are shown in the following table. The best model was found in
the case of K=10. We denote the model by M ∗ . The structure of the model is
shown in Figure 3.1
K
1
5
10
20
Time (hrs) 51
99
121
169
BIC
-52,522 -51,625 -51,465 -51,592

4

Latent Structure Discovery

Did HSHC discover interesting latent structures? The answer is positive. We
will explain this by examining diﬀerent aspects of Model M ∗ . First of all, the
data contains two variables for each type of insurance. For bicycle insurance,
for instance, there are “contribution to bicycle insurance policies (v62 )” and
“number of bicycle insurance policies (v83 )”. HSHC introduced a latent variable
for each such pair. The latent variable introduced for v62 and v83 is h11 , which
can be interpreted as “aversion to bicycle risks”. Similarly, h10 can be interpreted
as “aversion to motorcycle risks”, h9 as “aversion to moped risks”, and so on.
Consider the manifest variables below h12 . Besides “social security”, all the
other variables are related to heavy private vehicles. HSHC concluded that they
are inﬂuenced by one common latent variable. This is clearly reasonable and h12
can be interpreted as “aversion to heavy private vehicle risks”. Besides “social
security”, all the manifest variables below h8 are related to private vehicles.
HSHC concluded that they are inﬂuence by one common latent variable. This is
reasonable and h8 can be interpreted as “aversion to private vehicle risks”.
All the manifest variables below h15 , except “disability”, are agriculturerelated; while the manifest variables below h1 are ﬁrm-related. It is therefore
reasonable for HSHC to conclude that those two groups of variables are respectively inﬂuenced by two latent variables h1 and h15 , which can be interpreted as
“aversion to ﬁrm risks” and “aversion to agriculture risks” respectively.
It is interesting to note that, although delivery vans and tractors are vehicles,
HSHC did not conclude that they are inﬂuenced by h8 . HSHC reached the correct
1

Note that what HSHC obtains is an unrooted HLC model. The structure of the
model is visually shown as a rooted tree in Figure 3 partially for readability and
partially due to the discussions of the following section.

30

N.L. Zhang

conclusion that the decisions to buy insurance for tractors, for delivery vans, or
for other private vehicles are inﬂuenced by diﬀerent latent factors.
The manifest variables below h3 intuitively belong to the same category; those
below h6 are also closely related to each other. It is therefore reasonable for
HSHC to conclude that those two groups of variables are respectively inﬂuenced
by latent variables h3 and h6 .
The three sociodemographic variables (v04 , v05 , and v43 ) are connected to
latent variable h21 . Hence h21 can be viewed as a venue for summarizing information contained in those three variables. Latent variable h0 can interpreted
as “general attitude toward risks”. Under this interpretation, the links between
h0 and its neighbors are all intuitively reasonable: One’s general attitude toward risks should be related to one’s sociodemographic status (h21 ), and should
inﬂuence one’s attitudes toward speciﬁc risks (h8 , h1 , h15 , . . . , etc).
There are also aspects of Model M ∗ that do not match our intuition well. For
example, since there is a latent variable (h12 ) for heavy private vehicles under
h8 , we would naturally expect a latent variable for light private vehicles. But
there is no such variable. Below h3 , we would expect a latent variable speciﬁcally for life insurance. Again, there is no such variable. The placement of the
variables about social insurance and disability is also questionable. With an eye
on improvements, we have considered a number of alterations to M ∗ . However,
none resulted in models better than M ∗ in terms of BIC score.
Those mismatches are partially due to the limitations of HLC models. Disability is a concern in both agriculture and ﬁrms. We naturally would expect
h17 (aversion to disability risks) be connected to both h1 (aversion to ﬁrm risks)
and h15 (aversion to agriculture risks). But that would create a loop, which is
not allowed in HLC models. Hence, there is a need to study generalizations of
HLC models in the future. As mentioned in Section 2, it would also be interesting to study the impact of standard model dimensions versus eﬀective model
dimensions.

5

Probabilistic Modeling

We have so far mentioned two probabilistic models for the CoIL Challenge 2000
data, namely the HLC model M ∗ and the latent class model produced during
latent class analysis. In this section, we will denote M ∗ as MHLC and the latent
class model as MLC . For the sake of comparison, we have also used the greedy
equivalence search algorithm [1] to obtain a Bayesian network model that do not
contain latent variables. This model will be denoted as MGES . This structure of
MGES is shown in Figure 2. In general, we refer to Bayesian networks that do
not contain latent variables observed BN models.
The structure of MHLC is clearly more meaningful than that of MLC and
MGES . The structure of MLC is too simplistic to be informative. The relationships encoded in MGES are not as interpretable as those encoded in MHLC .
How do the models ﬁt the data? Before answering this question, we note that
HLC models and observed BN models both have their pros and cons when it

Discovering Latent Structures
v05

v59

v04

v45

v43

v58

v79

v62

v83

v66

v48

v69

v77

v56

v80

v72

v51

v67

v73

v52

v75

v70

v49

31

v46
v53

v74

v54
v47

v68

v44

v65

v86

v82

v63

v84

v55

v76

v85

v64

v57

v78

v61

Fig. 2. Bayesian network model without latent variables

comes to represent interactions among manifest variables. The advantage of HLC
models over observed BN models is that they can model high-order interactions.
In MHLC , latent variable h12 models some of the interactions among the heavy
private vehicle variables; h8 models some of the interactions among the private
vehicle variables; while h0 models some of the interactions among all manifest
variables. On the other hand, observed BN models are better than HLC models in
modeling details of variable interactions. In MGES , the conditional probability
distributions P (v59 |v44 ) and P (v67 |v59 , v44 ) contain all information about the
interactions among the three variables v44 , v59 , and v67 .
As can be seen from the table below, the logscore of MHLC on training data
is slightly higher than that of MGES . On the other hand, MGES is less complex
than MHLC , and its BIC score is higher than that of MHLC . In COIL Challenge
2000, there is a test set of 4,000 records. The logscore of MHLC on test data is
higher than that of MGES and the diﬀerence is larger than that on the training
data. In other words, MHLC is better than MGES when it comes to predicting
the test data.
Model Logscore Complexity BIC Logscore
(test data)
MLC -62328
739
-65532 -43248
MGES -49792
284
-51023 -34627
MHLC -49688
410
-51465 -34282

Because HLC models represent high-order variable interactions, MHLC should
perform better than MGES in classiﬁcation tasks. Out of the 4,000 customers
in the COIL Challenge 2000 test data, 238 own mobile home policies (v86 ). The

N.L. Zhang

v04: Avg age

v05: Customer main type
v52: Contr. tractor
v73: Num. tractor
v74: Num. agricultural machines
v53: Contr. agricultural machines
v51: Contr. trailer
v72: Num. trailer
v79: Num. disability
v58: Contr. disability

h22(2)

h17(2) h18(2) h19(2) h20(3)

v43: Purchasing power class

v67: Num. 3rd party (agriculture)

h2(2)

h1(2)

h15(2)

h21(9)

32

v69: Num. delivery van

v46: Contr. 3rd party (agriculture)

v48: Contr. delivery van
v66: Num. 3rd party (ﬁrm)

h13(2) h14(2)

h0(5)

h12(3)

v45: Contr. 3rd party (ﬁrm)
v82: Num. boat
v61: Contr. boat
v85: Num. social security
v64: Contr. social security
v86: Num. mobile home
v68: Num. car

h11(2)

v83: Num. bicycle
v49: Contr. motorcycle

h9(2)
h7(2)

v70: Num. motorcycle

v84: Num. property

h4(2)

h3(2)

v62: Contr. bicycle

v75: Num. moped

h5(2)

h6(2)

h10(2)

h8(4)

v47: Contr. car

v54: Contr. moped

v63: Contr. property
v65: Num. private 3rd party
v44: Contr. private 3rd party
v77: Num. private accident
v56: Contr. private accident
v78: Num. family accidents
v57: Contr. family accidents
v76: Num. life

v55: Contr. life
v80: Num. ﬁre
v59: Contr. ﬁre

Fig. 3. Structure of Model M ∗ . The number next to a latent variable is the cardinality
of that variable.

Discovering Latent Structures

33

classiﬁcation task is to identify a subset of 800 that contains as many mobile
home policy owners as possible. As can be seen from the following table, MHLC
does perform signiﬁcantly better than MGES .
Model/Method # of Mobile Home Pol- Hit Ratio
icy Holders Identiﬁed
Random
42
17.6%
GGES
83
34.9%
GLC
105
44.1%
GHLC
110
46.2%
CoIL 2000 Best 121
50.8%

The classiﬁcation performance of MHLC ranks at Number 5 among the 43
entries to the CoIL Challenge 2000 contest [5], and it is not far from the performance of the best entry. This is impressive considering that no attempt was
made to minimize classiﬁcation error when learning MHLC . In terms of model
interpretability, MHLC would rank Number 1 because all the 43 entries focus on
classiﬁcation accuracy rather than data modeling.

6

Conclusions

Through the analysis of the CoIL Challenge 2000 data set, we have demonstrated
that it is possible to infer complex and meaningful latent structures from data
using HLC models.

Acknowledgements
Research on this work was supported by Hong Kong Grants Council Grant
#622105. We thank Tao Chen, Yi Wang and Kin Man Poon for valuable
discussions.

References
1. Chickering, D. M. (2002). Learning equivalence classes of Bayesian-network structures. Journal of Machine Learning Research, 2: 445-498.
2. Friedman, N. (1997). Learning belief networks in the presence of missing values
and hidden variables. In Proc. of 14th Int. Conf. on Machine Learning (ICML-97),
125-133.
3. Lazarsfeld, P. F., and Henry, N.W. (1968). Latent structure analysis. Boston:
Houghton Miﬄin.
4. Pearl, J. (1988). Probabilistic Reasoning in Intelligent Systems: Networks of Plausible Inference Morgan Kaufmann Publishers, Palo Alto.
5. van der Putten, P. and van Someren, M. (2004). A Bias-Variance Analysis of a Real
World Learning Problem: The CoIL Challenge 2000. Machine Learning, Kluwer
Academic Publishers, 57, 177-195.

34

N.L. Zhang

6. Zhang, N. L. (2002). Hierarchical Latent Class models for Cluster Analysis, AAAI02.
7. Zhang, N. L. (2004). Hierarchical latent class models for cluster analysis. Journal of
Machine Learning Research, 5:697–723.
8. Zhang, N. L. and Kocka, T. K. (2004). Eﬃcient Learning of Hierarchical Latent
Class Models. In Proc. of the 16th IEEE International Conference on Tools with
Artiﬁcial Intelligence (ICTAI-2004).

