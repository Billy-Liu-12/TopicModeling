Hierarchical-Matrix Preconditioners for
Parabolic Optimal Control Problems
Suely Oliveira and Fang Yang
Department of Computer Science, The University of Iowa, Iowa City IA 52242, USA

Abstract. Hierarchical (H)-matrices approximate full or sparse matrices using a hierarchical data sparse format. The corresponding H-matrix
arithmetic reduces the time complexity of the approximate H-matrix operators to almost optimal while maintains certain accuracy. In this paper,
we represent a scheme to solve the saddle point system arising from the
control of parabolic partial diﬀerential equations by using H-matrix LUfactors as preconditioners in iterative methods. The experiment shows
that the H-matrix preconditioners are eﬀective and speed up the convergence of iterative methods.
Keywords: hierarchical matrices, multilevel methods, parabolic optimal control problems.

1

Introduction

Hierarchical-matrices (H-matrices) [6], since their introduction [1,2,3,6], have
been applied to various problems, such as integral equations and partial diﬀerential equations. The idea of H-matrices is to partition a matrix into a hierarchy of
rectangular subblocks and approximate the subblocks by low rank matrices (Rkmatrices). The H-matrix arithmetic [1,3,4] deﬁnes operators over the H-matrix
format. The ﬁxed-rank H-matrix arithmetic keeps the rank of a Rk-matrix block
below a ﬁxed value, whereas the adaptive-rank H-matrix arithmetic adjusts the
rank of a Rk-matrix block to maintain certain accuracy in approximation. The
operators deﬁned in the H-matrix arithmetic include H-matrix addition, Hmatrix multiplication, H-matrix inversion, H-matrix LU factorization, etc. The
computation complexity of these operators are almost optimal O(n loga n).
The H-matrix construction for matrices from discretization of partial diﬀerential equations depends on the geometric information underlying the problem
[3]. The admission conditions, used to determine whether a subblock is approximated by a Rk-matrix, are typically based on Euclidean distances between the
supports of the basis functions. For sparse matrices the algebraic approaches
[4,11] can be used, which use matrix graphs to convert a sparse matrix to an
H-matrix by representing the oﬀ-diagonal zero blocks as Rk-matrices of rank 0.
Since the H-matrix arithmetic provides cheap operators, it can be used with
H-matrix construction approaches to construct preconditioners for iterative methods, such as Generalized Minimal Residual Method (GMRES), to solve systems of
linear equations arising from ﬁnite element or meshfree discretizations of partial
diﬀerential equations [4,8,9,10,11].
Y. Shi et al. (Eds.): ICCS 2007, Part I, LNCS 4487, pp. 221–228, 2007.
c Springer-Verlag Berlin Heidelberg 2007

222

S. Oliveira and F. Yang

In this paper we consider the ﬁnite time linear-quadratic optimal control problems governed by parabolic partial diﬀerential equations. To solve these problems, in [12] the parabolic partial diﬀerential equations are discretized by ﬁnite
element methods in space and by θ-scheme in time; the cost function J to be
minimized is discretized using midpoint rule for the state variable and using
piecewise constant for the control variable in time; Lagrange multipliers are
used to enforce the constraints, which result a system of saddle point type; then
iterative methods with block preconditioners are used to solve the system.
We adapt the process in [12] and use H-matrix preconditioners in iterative
methods to solve the system. First we apply algebraic H-matrix construction
approaches to represent the system in the H-matrix format; then H-LU factorization in the H-matrix arithmetic is adapted to the block structure of the saddle
point system to compute the approximate H-LU factors; at last, these factors
are used as preconditioner in iterative methods to compute the approximate solutions. The numerical results show that the H-matrix preconditioned approach
is competitive and eﬀective to solve the above optimal control problem.
This paper is organized as follows. In Sect. 2 we introduce the optimal control
model problem and the discretization process; Section 3 is an introduction to
H-matrices; in Sect. 4, we review the algebraic approaches to H-matrix construction; in Sect. 5 we present the scheme to build the H-matrix preconditioners;
ﬁnally in Sect. 6 we present the numerical results.

2

The Optimal Control Problem

The model problem [12] is to minimize the following quadratic cost function:
J(z(u), u) :=

q
r
z(v) − z∗ 2L2 (t0 ,tf ;L2 (Ω)) + v
2
2
s
+ z(v)(tf , x) − z∗ (tf , x) 2L2 (Ω)
2

2
L2 (t0 ,tf ;Ω)

under the constraint of the state equation:
⎧
⎨ ∂t z + Az = Bv, t ∈ (t0 , tf )
z(t, ∂Ω) = 0
,
⎩
z(t0 , Ω) = 0

(1)

(2)

where the state variable z ∈ Y = H01 (Ω) and the control variable v ∈ U =
L2 (t0 , tf ; Ω). B is an operator in L(L2 (t0 , tf ; Ω), L2 (t0 , tf ; Y )) and A is an uniformly elliptic linear operator from L2 (t0 , tf ; Y ) to L2 (t0 , tf ; Y ). The state variable z is dependent on v. z∗ is a given target function.
2.1

Discretization in Space

The system is ﬁrst discretized in space by ﬁxing t. Considering the discrete
subspace Yh ∈ Y and Uh ∈ U , then the discretized weak form of (2) is given as:
(z˙h (t), ηh ) + (Azh (t), ηh ) = (Buh (t), ηh ),

∀ηh ∈ Yh and t ∈ (t0 , tf ) .

(3)

Hierarchical-Matrix Preconditioners for Parabolic Optimal Control Problems

223

Let{φ1 , φ2 , .., φn } be a basis of Yh and {ψ1 , ψ2 , .., ψm } be a basis of Uh , where
m ≤ n. Apply the ﬁnite element methods to (3), we obtain the following system
of ordinary diﬀerential equations:
M y˙ + Ay = Bu,

t ∈ (t0 , tf ) .

(4)

Here Ai,j = (Aφj , φi ) is a stiﬀness matrix, Mi,j = (φj , φi ) and Ri,j = (ψj , ψi )
are mass matrices, and Bi,j = (Bψi , φj ). The semi-discrete solution is zh (t, x) =
i yi (t)φi (x) with control function uh (t, x) =
i ui (t)ψi (x).
We can apply the analogous spatial discretization to the cost function (1),
and obtain:
t0

e(t)T Q(t)e(t) + u(t)T R(t)u(t) dt + e(tf )T C(t)e(tf ),

J(y, u) =

(5)

tf

where e(·) = y(·) − y∗ (·) is the diﬀerence between the state variable and the
given target function.
2.2

Discretization in Time

After spatial discretization, the original optimal problem is transferred into minimizing (5) under the constraint of n ordinary diﬀerential equations (4). θ-scheme
is used to discretize the above problem.
First the time scale is subdivided into l intervals of length τ = (tf − t0 )/l. Let
F0 = M + τ (1 − θ)A and F1 = M − τ θA. The discretization of equation (4) is
given by:
Ey + N u = f ,
(6)
where

⎤
⎡
−F1
⎥
⎢
.. ..
E=⎣
⎦,
. .
F0 −F1

⎡
⎢
N =τ⎣

⎤

B
..

⎥
⎦,

.
B

⎤
y(t1 )
⎥
⎢
y ≈ ⎣ ... ⎦ ,
⎡

etc .

(7)

y(tn )

Then discretize the cost function (5) by using piecewise linear functions to approximate the state variable and piecewise constant functions to approximate
the control variable and obtain the following discrete form of (5):
J(y, u) = uT Gu + eT Ke,

(8)

where e = y − y∗ and the target trajectory z∗ (t, x) ≈ z∗,h (t, x) = i (y∗ )i (t)φi (x).
A Lagrange multiplier vector p is introduced to enforce the constraint of (6), and
we have the Lagrangian
1 T
(u Gu + eT Ke) + pT (Ey + N u − f ) .
(9)
2
To ﬁnd y, u and p where ∇L(y, u, p) = 0 in (9), we need to solve the following
system:
⎤
⎤⎡ ⎤ ⎡
⎡
y
M y∗
K 0 ET
⎣ 0 G N T ⎦ ⎣u⎦ = ⎣ 0 ⎦ .
(10)
f
E N 0
p
L(y, u, p) =

224

3

S. Oliveira and F. Yang

Hierarchical-Matrices

The concept and properties of H-matrices are induced by the index cluster tree
TI and the block cluster tree TI×I [6] . In the rest of this paper, #A denotes the
number of elements of set A and S(i) denotes the children of node i.
3.1

Index Cluster Tree and Block Cluster Tree

An index cluster tree TI deﬁnes a hierarchical partition tree over an index set
I = (0, . . . , n − 1). Note that (0, 1, 2) = (0, 2, 1).TI has the following properties:
its root is I; any node i ∈ TI either is a leaf or has children S(i); the parent
node i = j∈S(i) j and its children are pairwise disjoint.
A block cluster tree TI×I is a hierarchical partition tree over the product index
set I × I. Given tree TI and an admissibility condition (see below), TI×I can be
constructed as follows: its root is I × I; if s × t in TI×I satisﬁes the admissibility
condition, it is an Rk-matrix leaf; else if #s < Ns or #t < Ns , it is a full-matrix
leaf; otherwise it is partitioned into subblocks on the next level and its children
(subblocks) are deﬁned as S(s × t) = { i × j | i, j ∈ TI and i ∈ S(s), j ∈ S(t) }.
A constant Ns ∈ [10, 100] is used to control the size of the smallest blocks.
An admissibility condition is used to determine whether a block to be approximated by an Rk-matrix. An example of an admissibility condition is:
s × t is admissible if & only if : min(diam(s), diam(t)) ≤ μ dist(s, t),

(11)

where diam(s) denotes the Euclidean diameter of the support set s, and dist(s, t)
denotes the Euclidean distance of the support set s and t. The papers [1,2,3] give
further details on adapting the admissibility condition to the underlying problem
or the cluster tree.
Now we can deﬁne an H-matrix H induced by TI×I as follows: H shares the
same tree structure with TI×I ; the data are stored in the leaves; for each leaf
s × t ∈ TI×I , its corresponding block Hs×t is a Rk-matrix, or a full matrix if
#s < Ns or #t < Ns .
Fig. 1 shows an example of TI , TI×I and the corresponding H-matrix.
0 1 2 3

{0 1 2 3}

{0 1} {2 3}

{0 1 2 3}X{0 1 2 3}
{0 1}X{0 1} {0 1}X{2 3}{2 3}X{0 1} {2 3}X{2 3}
RK

{0} {1} {2} {3}
(a)

RK

{0}X{0} {0}X{1} {1}X{0} {1}X{1}
(b)

0
1
2
3

(c)

Fig. 1. (a) is TI , (b) is TI×I and (c) is the corresponding H-matrix. The dark blocks
in (c) are Rk-matrix blocks and the white blocks are full matrix blocks.

An m × n matrix M is called an Rk-matrix if rank(M ) ≤ k and it is represented in the form of a matrix product M = AB T , where M is m × n, A is

Hierarchical-Matrix Preconditioners for Parabolic Optimal Control Problems

225

m × k and B is n × k. If M is not of rank k, then a rank k approximation can
be computed in O(k 2 (n + m) + k 3 ) time by using a truncated Singular Value
Decomposition (SVD) [1,3].
3.2

H-Matrix Arithmetic

The following is a short summary of the H-matrix arithmetic. A detailed introduction can be found in [1,2].
H-matrix operations perform recursively; therefore it is important to deﬁne
the corresponding operations on the leaf subblocks, which are either full or Rkmatrices. These operations are approximate as certain operations do not create
Rk-matrices (such as adding two Rk-matrices). In such case, a truncation is
performed using an SVD to compute a suitable Rk-matrix. For example, the
sum of two rank k matrices can be computed by means of a 2k × 2k SVD.
The computational complexity of the H-matrix arithmetic strongly depends
on the structure of TI×I . Under fairly general assumptions on the block cluster
tree TI×I the complexity of H-matrix operators is O(n logα n) [3,6].

4

Algebraic Approaches for Hierarchical-Matrix
Construction

Algebraic H-matrix construction approaches can be applied to sparse matrices.
These approaches take advantage that most entries of a sparse matrix are zeros.
They build H-matrix cluster tree by partitioning a matrix graph either bottomup or top-down. The multilevel clustering based algorithm [11] constructs the
cluster tree “bottom-up”, i.e., starts with the leaves and successively clusters
them together until the root is reached. Domain decomposition in [4] and bisection are “top-down” algebraic approaches, which start with the root and
successively subdivides a cluster into subsets.
4.1

Algebraic Approaches to Construct an Index Cluster Tree

In [11] we propose an H-matrix construction approach based on multilevel
clustering methods. To build clusters over the nodes in Gi = (V (Gi ), E(Gi )),
an algorithm based on Heavy Edge Matching (HEM) [7] is used. After building the clusters, a coarse graph Gi+1 is constructed: such that for each clus(i)
ter Ck ⊂ V (Gi ) there is a node k ∈ V (Gi+1 ); the edge weight wkt of edge
ekt ∈ E(Gi+1 ) is the sum of the weights of all the edges, which connect the nodes
(i)
(i)
in cluster Ck to the nodes in cluster Ct in graph Gi . Recursively applying the
above coarsening process gives a sequence of coarse graphs G1 , G2 , . . . , Gh . The
index cluster tree TI is constructed by making k ∈ V (Gi ) the parent of every
(i)
s ∈ Ck . The root of TI is the set V (Gh ), which is the parent to all nodes in Gh .
In [4], domain decomposition based clustering is used to build a cluster tree
TI . Starting from I, a cluster is divided into three sons, i.e. S(c) = {c1 , c2 , c3 }
and c = c1 ∪ c2 ∪ c3 , so that the domain-clusters c1 and c2 are disconnected

226

S. Oliveira and F. Yang

and the interface-cluster c3 is connected to both c1 and c2 . Then the domainclusters are successively divided into three subsets, and the interface-clusters are
successively divided into two interface-clusters until the size of a cluster is small
enough.
To build a cluster tree TI based on bisection is straight forward. Starting from
a root set I and a set is successive partitioned into two subsets with equal size.
This construction approach is suitable for the sparse matrices where the none
zero entries are around the diagonal blocks.
4.2

Block Cluster Tree Construction for Algebraic Approaches

The admissibility condition used to build TI×I for the algebraic approaches is
deﬁned as follows: a block s × t ∈ TI×I is admissible if and only if s and t are
not joined by an edge in the matrix graph; an admissible block corresponds to a
zero block and is represented as a Rk-matrix of rank zero; an inadmissible block
is partitioned further or represented by a full matrix.

5

Hierarchical-Matrix Preconditioners

The construction of H-matrix preconditioners for a system of saddle point type
is based on the block LU factorization.
To obtain a relative cheap yet good approximate LU factors, we replace the
ordinary matrix operators by the corresponding H-matrix operators[4,8,11].
First the matrix in (10) is converted to an H-matrix. Since the nonzero entries
of each subblock are centered around the diagonal blocks, we apply the bisection
approach to the submatrix K, G, E and N respectively. Then we obtain the
following H-matrix, which is on the left side of the equation (H indicates a block
in the H-matrix format):
⎡
⎤ ⎡
⎤⎡
⎤
T
KH 0 EH
L1H 0
U 1H 0 M 1TH
0
T ⎦
⎣ 0 GH NH
= ⎣ 0 L2H 0 ⎦ ⎣ 0 U 3H M 2TH ⎦ .
(12)
EH NH 0
0
0 U 3H
M 1H M 2H L3H
The block cluster tree TI×I of L1H , L2H , M 1H , and M 2H is same as the
block cluster tree structure of KH , GH , EH , and NH respectively. The block
cluster tree structure of L3H is based on the block tree structure of EH : the
block tree of L3H is symmetric; the tree structure of the lower-triangular of L3H
is same as the tree structure of the lower-triangular of EH ; the tree structure of
the upper-triangular of L3H is the transpose of the tree structure of the lower
triangular. L1H and L2H are obtained by apply H-Cholesky factorization to
KH and GH : KH = L1H ∗H U 1H and GH = L2H ∗H U 2H . Then use the Hmatrix upper triangular solve, we can get M 1H by solving M 1H U 1H = EH .
M 1H have the same block tree as EH . In the same way we can compute M 2H ,
which has the same block cluster tree structure as NH . At last we construct
the block cluster tree for L3H and then apply H-LU factorization to get L3H :
L3H U 3H = M 1H ∗H M 1TH +H M 2H ∗H M 2TH .

Hierarchical-Matrix Preconditioners for Parabolic Optimal Control Problems

6

227

Experimental Results

In this section, we present the numerical results of solving the optimal control
problem (1) constrained by the following equation:
⎧
⎨ ∂t z − ∂xx z = v, t ∈ (0, 1), x ∈ (0, 1)
z(t, 0) = 0, z(t, 1) = 0
(13)
⎩
z(0, x) = 0, x ∈ [0, 1]
with the target function z∗ (t, x) = x(1 − x)e−x . The parameters in the control
function J are q = 1, r = 0.0001 and s = 0.
Table 1. Time for computing H-LU factors and GMRES iterations
n(n1/n2)

L1H L2H M 1H M 2H L3H GMRES
number
iteration time of GMRES iterations
592(240/112)
0
0
0.01 0
0.01 0
1
2464(992/480)
0.01 0
0.01 0
0.04 0
1
10048(4032/1984)
0.03 0.01 0.13 0.02 0.39 0.04
1
40576(16256/8064) 0.21 0.06 0.84 0.26 4.13 0.32
3
163072(65280/32512) 1.09 0.42 4.23 1.74 25.12 2.66
6

GMRES iteration stops where the original residuals are reduced by the factor
of 10−12 . The convergence rate a ia deﬁned as the average decreasing speed of
residuals in each iteration. The ﬁxed-rank H-matrix arithmetic is used and we
set the rank of each Rk-matrix block to be ≤ 2. The tests are performed on a Dell
workstation with AMD64 X2 Dual Core Processors (2GHz) and 3GB memory.
Table 1 shows the time to compute the diﬀerent parts of the H-LU factors
and the time of GMRES iterations (in seconds). n is the size of the problem, n1
and n2 is the number of rows of K and G respectively. Based on Table 1, the
time to compute L3H contributes the biggest part of the total time to set up the
preconditioner.
The convergence rates

Times for solving the system

102

10−2

total time
setup time

10−4
Convergence rate

101

time (sec.)

gmres iteration time

10−6

100

10−8

10−1
10−10

10−12 3
10

10 4

Problem size

10 5

10 6

10−2 2
10

10 3

10 4
Problem size

10 5

10 6

Fig. 2. (a) The convergence rates of GMRES (b) Total times for solving the system

228

S. Oliveira and F. Yang

Fig. 2-(a) shows the convergence rate of the H-LU preconditioned GMRES
and Fig. 2-(b), plotted on a log-log scale, shows the time for building the preconditioners and the time for the GMRES iterations.
Based the results, we can see that H-LU speeds up the convergence of GMRES
iteration signiﬁcantly. The problem in our implementation is that the time to
compute L3 still consists a signiﬁcant part of the LU-factorization time. In the
future, more work needs to be done to reduce the complexity of computing L3
further. More discussion about H-matrix preconditioners and applications will
come in [13].

References
1. B¨
orm, S., Grasedyck, L., Hackbusch, W.: Introduction to hierarchical matrices with
applications. Engineering Analysis with Boundary Elements. 27 (2003) 405–422
2. B¨
orm, S., Grasedyck, L., Hackbush, W.: Hierarchical matrices. Lecture Notes No.
21. Max-Planck-Institute for Mathematics in the Sciences, Leipzig (2003)
3. Grasedyck, L., Hackbusch, W.: Construction and Arithmetics of H-matrices. Computing. 70 (2003) 295–334
4. Grasedyck, L. , Kriemann, R. , LeBorne, S.: Parallel Black Box Domain Decomposition Based H-LU Preconditioning. Mathematics of Computation, submitted.
(2005)
5. Gravvanis, G.: Explicit approximate inverse preconditioning techniques Archives
of Computational Methods in Engeneering. 9 (2002)
6. Hackbusch, W.: A sparse matrix arithmetic based on H-matrices. Part I: Introduction to H-matrices. Computing. 62 (1999) 89–108
7. Karypis, G., Kumar, V.: A fast and high quality multilevel scheme for partitioning
irregular graphs. SIAM J. Sci. Comput. 20 (1999) 359–392
8. LeBorne, S.: Hierarchical matrix preconditioners for the Oseen equations. Comput.
Vis. Sci. (2007)
9. LeBorne, S., Grasedyck, L.: H-matrix preconditioners in convection-dominated
problems. SIAM J. Matrix Anal. Appl. 27 (2006) 1172–1183
10. LeBorne, S., Grasedyck, L., Kriemann, R.: Domain-decomposition based H-LU
preconditioners. LNCSE. 55 (2006) 661–668
11. Oliveira, S., Yang, F.: An Algebraic Approach for H-matrix Preconditioners. Computing, submmitted. (2006)
12. Schaerer, C. and Mathew, T. and Sarkis, M.: Block Iterative Algorithms for the
Solution of Parabolic Optimal Control Problems. VECPAR. (2006)
13. Yang, F.: H-matrix preconditioners and applications. PhD thesis. the University
of Iowa

