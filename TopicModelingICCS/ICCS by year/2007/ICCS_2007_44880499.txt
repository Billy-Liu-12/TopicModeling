A New Multi-Criteria Quadratic-Programming Linear
Classification Model for VIP E-Mail Analysis
Peng Zhang1,2, Juliang Zhang1, and Yong Shi1
1

CAS Research Center on Data Technology and Knowledge Economy,
Beijing 100080, China
zhangpeng04@mails.gucas.ac.cn, yshi@gucas.ac.cn
2
School of Information Science and Engineering, Graduate University of
Chinese Academy of Sciences, Beijing 100080, China

Abstract. In the recent years, classification models based on mathematical
programming have been widely used in business intelligence. In this paper, a
new Multi-Criteria Quadratic-Programming Linear Classification (MQLC)
model is proposed and tested with VIP E-Mail dataset. This experimental study
uses a variance of K-fold Cross-Validation to demonstrate the accuracy and
stability of the model. Finally, we compare our model with the decision tree by
using commercial software C5.0. The result indicates that the proposed MQLC
model performs better than decision tree on small samples.
Keywords: VIP E-Mail Analysis, Data Mining, Multi-criteria Quadraticprogramming Linear Classification, Cross-Validation, Decision Tree.

1 Introduction
Data Mining is a discipline combining a wide range of subjects such as Statistics,
Machine Learning, Artificial Intelligence, Neural Network, Database Technology and
Pattern recognition [1]. Recently, classification models based on mathematical
programming approaches have been introduced in data mining. In 2001, Shi et al [2,
3] proposed a Multiple Criteria Linear Programming (MCLP) model which has been
successfully applied to a major US bank credit card portfolio management. In this
paper, a new Multi-criteria Quadratic-programming Linear Classification (MQLC)
model is proposed and used in the VIP E-Mail dataset provided by a major web
hosting company in China. As a new model, the performance and stability of MQLC
are focal issues. In order to respond to these challenges, this paper conducts a
variance of k-folders cross-validation experiment and compares the prediction
accuracy of MQLC with decision tree in software C5.0. Our findings indicate that
MQLC is highly stable and performs well in small samples.
This paper is organized as the following. Next section is an overview of MQLC
model formulation; the third section describes the characteristics of the VIP E-Mail
dataset; the fourth section talks about the process and results of cross validation; the
fifth section illustrates the comparison study with decision tree in commercial
software C5.0; and the last section concludes the paper with summary of the findings.
Y. Shi et al. (Eds.): ICCS 2007, Part II, LNCS 4488, pp. 499–502, 2007.
© Springer-Verlag Berlin Heidelberg 2007

500

P. Zhang, J. Zhang, and Y. Shi

2 Formulation of Multi-Criteria Quadratic-Programming Linear
Classification Model
Given a set of r attributes a = (a1 ,..., ar ) , let Ai = ( Ai1 ,..., Air ) ∈ R r be one of the

sample observations of these properties. Suppose we predefine two groups G1 and G2 ,
we can select a boundary b to separate these two groups. A vector
X=(x1 ,x 2 ,...,x n ) ∈ R r can be identified to establish the following linear inequality
A i x < b, ∀Ai ∈ G1 ;

A i x ≥ b, ∀Ai ∈ G2 ;

[2,

3,

4,

5].

We

define

external

measurement α i to be the overlapping distance of the two-group boundary for
record Ai . If Ai ∈ G1 but we misclassified it into G2 or vice versa, α i will equal
to | Ai x − b | . Then we define internal measurement βi to be the distance of
record Ai from its adjusted boundary b* . If Ai is correctly classified, distance βi will
equal to | Ai x − b* | , where b* = b + α i or b* = b − α i . Suppose f (α ) denotes the
relationship of all overlapping α i while g ( β ) denotes the aggregation of all
distances βi . The final absolute catch rates depend on simultaneously
minimizing f (α ) and maximizing g ( β ) , the most common way of representation is
by normal value. Given weights wα , wβ >0 and let f (α ) = || α + c1 || pp , g ( β ) = || β + c2 ||qq ,
the generalized model can be converted into single criterion mathematical
programming model as:
p
q
(Normal Model) Minimize wα || α + c1 || p − wβ || β + c2 ||q

(1)

Subject to:
Ai x − α i + β i − b = 0, ∀Ai ∈ G1
Ai x + α i − β i − b = 0, ∀Ai ∈ G2

α i , β i ≥ 0, i = 1,..., n

Based on the Normal Model, α , β , c1 , and c2 can be randomly chosen. When we
set p=2, q=1 and
n

n

i =1

i =1

c1 =1, c2 =0, we can formulate the objective function as

wα ∑ (α i + 1) 2 − wβ ∑ β i . After expanding it, we can write the model as follows:
n

n

n

i =1

i =1

i =1

(MQLC Model) Minimize wα ∑ α i 2 + 2w α ∑ α i − w β ∑ β i

(2)

Subject to:
Ai x − α i + β i − b = 0, ∀Ai ∈ G1
Ai x + α i − β i − b = 0, ∀Ai ∈ G2
α i , β i ≥ 0, i = 1,..., n

In MQLC model, the normal value of α is larger than β implies that the penalty
for misclassifying Ai is more severe than not enlarging the distance from the adjusted

A New MQLC Model for VIP E-Mail Analysis

501

boundary. Since it is possible that α i is less than 1, in such scenario the objective value
will decrease when we square α i . To make sure the penalty is aggravated when
misclassification occurs, MQLC add 1 to all α i before squaring them.

3 VIP E-Mail Dataset
Our partner company’s VIP E-Mail data are mainly stored in two kinds of repository
systems; one is databases manually recorded by employee, which was initially
produced to meet the needs of every kind of business service; the other is log files
recorded automatically by machine, which contains the information about customer
login, E-Mail transaction and so on. After integrating all these data with the keyword
SSN, we finally acquire the Large Table which has 185 features from log files and 65
features from database. Considering the customer records integrity, we eventually
extracted two groups of customer record, the current customers and the lost
customers, 4998 records in each group respectively. Combining these 9996 SSN with
the 230 features, we eventually acquired the Large Table for data mining.

4 Empirical Study of Cross-Validation
In this paper, a variance of k-fold cross-validation is used on VIP E-Mail dataset; each
calculation is consisting of training with one of the subsets and testing with the other
k-1 subsets. We have computed 10 group dataset. The accuracy of 10 group training
set is extremely high, with the average accuracy on the lost user of 88.48% and on the
Current user of 91.4%. When we concentrate on the 10 group testing set, the worst
and best classification catch rates are 78.72% and 82.17% for the lost customers and
84.59% and 89.04 % for the Current customers. That is to say, the absolute catch rates
of the lost class are above 78% and the absolute catch rates of the Current class are
above 84%. The result indicates that a good separation of the lost class and Current
class is observed with this method.

5 Comparison of MQLC and Decision Tree
The following experiment compares MQLC and Decision Tree in the commercial
software C5.0. From the results of Table II, we can see that when the percentage of
training set is increased from 2% to 25%, the accuracy of Decision Tree increases
from 71.9% to 83.2% while the accuracy of MQLC increases from 78.54% to
83.56%. The accuracy of MQLC is slightly better than Decision Tree. Moreover,
when training set is 10% of the whole dataset, the accuracy of MQLC peaked at
83.75%. That is to say, MQLC can perform well even on small samples. The reason
maybe due to the fact that MQLC solves a convex quadratic problem, which can
acquire the global optimal solution easily, on the other hand, Decision Tree merely
selects the better tree from a limited built tree, which might not be the best tree. In
addition, the pruning procedure of Decision Tree may further eliminate some better
branches. In conclusion, MQLC performs better than Decision Tree on small samples.

502

P. Zhang, J. Zhang, and Y. Shi
Table 1. Comparison of MQLC and Decision Tree

Percentage
of training

Decision Tree

MQLC

2%
5%

Training
92.7%
94.9%

Testing
71.9%
77.3%

Training
96.0%
92.8%

Testing
78.54%
82.95%

10%
25%

95.2%
95.9%

80.8%
83.2%

90.7%
88.95%

83.75%
83.56%

6 Conclusion
In this paper, a new Multi-criteria Quadratic-programming Linear Classification
(MQLC) Model has been proposed to the classification problems in data mining. 230
features are extracted from the original data source to depict all the VIP E-Mail users,
and 9996 records are chosen to test the performance of MQLC. Through the results of
cross-validation, we can see that the model is extremely stable for multiple groups of
randomly generated training set and testing set. The comparison of MQLC and
Decision Tree in C5.0 tells us that MQLC performs better than Decision Tree on
small samples. There have been experiments that show the accuracy of MQLC is not
affected by the parameters of the objective function, further research will include
mathematically proving this phenomenon.
Acknowledgments. This research has been partially supported by a grant from
National Natural Science Foundation of China (#70621001, #70531040, #70501030,
#70472074), 973 Project #2004CB720103, Ministry of Science and Technology,
China, NSFB(No.9073020) and BHP Billiton Co., Australia.

References
1. Han, J. W., M. Kamber: Data Mining: Concepts and Techniques. San Diego. CA: Academic
Press (2000) ch.1.
2. Shi, Y., Wise, M., Luo, M. and Lin, Y.: Data mining in credit card portfolio management: a
multiple criteria decision making approach. in Multiple Criteria Decision Making in the
New Millennium, M. Koksalan and S.Zionts, eds., Berlin: Springer (2001) 427-436
3. Shi, Y., Peng, Y., Xu, W., Tang, X.: Data Mining via Multiple Criteria Linear
Programming: Applications in Credit Card Portfolio Management. International Journal of
Information Technology and Decision Making, vol. 1(2002) 131-151.
4. Gang Kou, Yi Peng, Yong Shi, Weixuan Xu: A Set of Data Mining Models to Classify
Credit Cardholder Behavior. International Conference on Computational Science (2003)
54-63
5. Yi Peng, Gang kou, Zhengxin Chen, Yong Shi: Cross-Validation and Ensemble Analyses
on Multiple-Criteria Linear Programming Classification for Credit Cardholder Behavior.
International Conference on Computational Science (2004) 931-939

