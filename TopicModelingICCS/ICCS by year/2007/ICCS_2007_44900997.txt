A Steep Thermodynamical Selection Rule for
Evolutionary Algorithms
Weiqin Ying1 , Yuanxiang Li1,2 , Shujuan Peng2 , and Weiwu Wang1
1

State Key Lab. of Software Engineering, Wuhan University, Wuhan 430072, China
2
School of Computer Science, Wuhan University, Wuhan 430079, China
{weiqinying,yxli62}@yahoo.com.cn

Abstract. The genetic algorithm (GA) often suﬀers from the premature
convergence because of the loss of population diversity at an early stage
of searching. This paper proposes a steep thermodynamical evolutionary algorithm (STEA), which utilizes a steep thermodynamical selection
(STS) rule. STEA simulates the competitive mechanism between energy
and entropy in annealing to systematically resolve the conﬂicts between
selective pressure and population diversity in GA. This paper also proves
that the rule STS has the approximate steepest descent ability of the free
energy. Experimental results show that STEA is both far more eﬃcient
and much stabler than the thermodynamical genetic algorithm (TDGA).
Keywords: Evolutionary algorithms, thermodynamics, selection rule,
population diversity, free energy.

1

Introduction

The genetic algorithm (GA) is an optimization technique based on the mechanism of evolution by natural selection [1]. However, it has some disadvantages
yet for solving large-scale combinatorial optimization problems because the astronomical size of search spaces with local optima often lead GA to extremely
slow search and “premature convergence” [2]. The premature convergence also
causes the low stability of GA. Whitley [3] argues that population diversity and
selective pressure are the two primary factors in genetic search. Increasing selective pressure speeds up the search, while it also results in a faster loss of
population diversity. Maintaining population diversity can help the search to
escape local optima, while it oﬀsets the eﬀect of increasing selective pressure.
These two factors are inversely related.
Some techniques on controlling population diversity have been proposed, such
as scaling the ﬁtness [1], sharing the ﬁtness [1], and driving all individuals moving
[4] etc. But they are not yet suﬃciently systematical and eﬀective for large-scale
combinatorial problems. Kirkpatrick et al. [5] have proposed another general
optimization algorithm called the simulated annealing (SA). SA controls search
systematically by the cooling temperature and the Metropolis rule. Mori et al.
[6,7] have proposed a method of combining SA and GA, called the thermodynamical genetic algorithm (TDGA). They introduce a greedy thermodynamical
Y. Shi et al. (Eds.): ICCS 2007, Part IV, LNCS 4490, pp. 997–1004, 2007.
c Springer-Verlag Berlin Heidelberg 2007

998

W. Ying et al.

selection rule in TDGA, getting a hint from the principle of minimal free energy.
TDGA attempts to utilize the concepts of temperature and entropy in annealing
to control population diversity systematically. TDGA is eﬀective to some extent,
while its performance is still inadequately stable and its computational cost is
extremely high.
This paper proposes a steep thermodynamical evolutionary algorithm (STEA)
to improve the stability and the computational eﬃciency of TDGA. STEA simulates the competitive mechanism between energy and entropy in annealing to
systematically resolve the conﬂicts between selective pressure and population
diversity in GA. The measurement of population diversity and the minimization
process of free energy at each temperature are mended and a steep thermodynamical selection (STS) rule is proposed in STEA. The paper is organized as
follows. We brieﬂy review the thermodynamic background on STEA in Section 2,
describe the outline of STEA in Section 3, and prove the approximate steepest
descend ability of the rule STS in Section 4. Finally, the experimental results
are presented in Section 5 and the conclusion is given in Section 6.

2

Brief Thermodynamic Background on STEA

In thermodynamics and statistical mechanics, annealing can be viewed as an
adaptation process to optimize the stability of the ﬁnal crystalline solid. In an
annealing process, a metal, initially at high temperature and disordered, is slowly
cooled so that the system at any temperature approximately reaches thermodynamic equilibrium [5]. As cooling proceeds, the system becomes more ordered
and approaches a “frozen” ground state at the temperature T=0. There are a
few observations about annealing which are helpful for STEA:
1. If the initial temperature is too low or cooling is done insuﬃciently slowly, the
system may become quenched forming defects or trapped in a local minimum
energy state.
2. Any change from non-equilibrium to equilibrium of the system at each temperature follows the principle of minimum free energy. In other words, the
system will change spontaneously to achieve a lower total free energy and
the system reaches equilibrium when its free energy seeks a minimum. In
thermodynamics, the free energy F is deﬁned as F = E − HT , where E is
the energy of the system and H its entropy.
3. In thermodynamics, the entropy can quantiﬁcationally measure the energy
dispersal of the particles in the system.
4. Any change of the system can be viewed as a result of the competition
between its energy and its entropy. The temperature T determines their
relative weights in the competition.

3

Steep Thermodynamical Evolutionary Algorithm

There are some deep and useful similarities between annealing in solids and
convergence in GA. The population and the individuals in GA can be regarded

A Steep Thermodynamical Selection Rule for Evolutionary Algorithms

999

as the system and the particles. Then the mean of negative ﬁtness, the population
diversity and a controllable weight parameter can play the roles of the energy,
the entropy and the temperature respectively. Every population state exactly full
of global optima can be interpreted as the ground state. This analogy provides
an approach for STEA to simulate the competitive mechanism between energy
and entropy in annealing to systematically resolve the conﬂicts between selective
pressure and population diversity in GA.
3.1

Measurement of Population Diversity

It is a critical part how to measure population diversity when introducing the competitive mechanism into GA. TDGA uses the sum of the information entropy at
each locus, called a gene-based entropy in this paper. However, it has two disadvantages. Firstly, its repetitive calculations at all loci cause high computational
costs. Secondly, the thermodynamic entropy can measure the energy dispersal of
particles that is equivalent to the ﬁtness dispersal of individuals in the population,
while the gene-based entropy in TDGA can’t measure the ﬁtness dispersal. In this
section, we propose a level-based entropy by grading the ﬁtness.
Deﬁnition 1. Let S be the search space, f : S → IR be the objective function,
and Xr ∈S be one individual. Then the individual energy e(Xr ) = f (Xr ) for
minimum problems and e(Xr ) = −f (Xr ) for maximum problems. Assume eu
and el be respectively an upper bound and a lower bound of the individual energy.
Then π = {gi |0≤i≤K − 1} is called a level partition on [el , eu ] if
gi = (

2i−1 − 1
2i − 1
(e
(eu − el ) + el ]∩[el , eu ].
−
e
)
+
e
,
u
l
l
2K−1 − 1
2K−1 − 1

(1)

We shall say that Xr is at level gi if e(Xr )∈gi .
Deﬁnition 2. Let P = (X1 , X2 , . . . , XN )∈S N be one population of size N , π =
{gi |0≤i≤K − 1} be a level partition, and ni be the number of individuals in gi
of population P . Then H(π, P ) is called the level-based entropy of P for π where
K−1

H(π, P ) = −
i=0

ni
ni
logK , 0≤i≤K − 1.
N
N

(2)

The value of the level-based entropy varies from 0 to 1 depending on the level
distribution of P . It measures the ﬁtness dispersal with very low computational
costs.
3.2

Minimization of Free Energy at Each Temperature

Deﬁnition 3. For P ∈S N , E(P ) is called the population energy of P where
E(P ) =

1
N

N

e(Xr ), Xr ∈P.
r=1

(3)

1000

W. Ying et al.

F (π, T, P ) is called the free energy of P at temperature T for partition π where
F (π, T, P ) = E(P ) − H(π, P )T.

(4)

The free energy is the driving force toward equilibrium in annealing. Similarly,
we should force the population to minimize its free energy at each temperature
Tk during evolution. However, there is only one generation of competition at
each temperature Tk in TDGA. This insuﬃcient competition only lowers the free
energy very slightly, and the population can’t reach the minimum free energy (or
equilibrium) at Tk . In order to approach equilibrium, STEA holds Lk generations
of competitions at Tk . These competitions at Tk form a M arkov chain of length
Lk , where Lk should grow with the hardness of problems.
3.3

Steep Thermodynamical Selection Rule

In order to minimize the free energy rapidly at each temperature Tk , we should
design a thermodynamical selection rule to descend the free energy of the next
generation most steeply. Its mission is to select N individuals from N parent
individuals and M oﬀspring individuals as the next generation with the minimum
free energy. However, It is infeasible to exactly minimize the free energy for each
N
generation because of the extremely high complexity O((N + K)CN
+M ). Hence,
TDGA uses a greedy thermodynamical selection (GTS) rule with the complexity
O(N 2 K). But its reliability can’t be guaranteed. In this section, we proposes a
steep thermodynamical selection (STS) rule by assigning the free energy of the
population to its individuals.
Deﬁnition 4. Let P = (X1 , X2 , . . . , XN )∈S N be one population of size N and
π = {gi |0≤i≤K − 1} be a level partition. For an individual Xr ∈P at level gd ∈π,
its free energy component in P at temperature T for π is deﬁned as:
Fc (π, T, P, Xr ) = e(Xr ) + T logK (

nd
),
N

(5)

where nd is the number of individuals at gd of P .
The steep selection rule STS (π, T, Pt , Ot ) is described as follows:
1. Produce an interim population Pt of size N +M by appending M individuals
in the oﬀspring population Ot to the parent population Pt .
2. Calculate the free energy component Fc (π, T, Pt , Xr ) for each individual
Xr ∈Pt .
3. Pick the M individuals with the largest free energy components from Pt .
4. Form the next generation Pt+1 by removing these M individuals from Pt .
STS has the lower complexity O((N + M )M ). It’s proved in Section 4 that
STS has the approximate steepest descent ability of the free energy.
3.4

Outline of STEA

Figure 1 provides the general outline of the whole STEA described above.

A Steep Thermodynamical Selection Rule for Evolutionary Algorithms
01
02
03
04
05
06
07
08
09
10
11
12
13
14
15
16
17

1001

Create N individuals randomly as an initial population P0 and evaluate them;
Determine the energy bounds eu and el for a level partition π;
Conﬁgure the length Lk of the M arkov chain at each temperature Tk ;
T0 = 10(eu − el );
t = 0; k = 0;
while(Termination test(Pt )==False)
{
for (i = 0; i < Lk ; i++)
{
Generate M oﬀspring by uniform selection, crossover and mutation;
Organize these M oﬀspring as an oﬀspring population Ot and evaluate them;
Pt+1 = STS (π, Tk , Pt , Ot );
t = t + 1;
}
k = k + 1;
Tk = T0 /(1 + k);
}
Fig. 1. The outline of STEA

4

Approximate Steepest Descent Ability of STS

G
Lemma 1. Assume Pt+1
is the next generation population with the exact minimum free energy and Pt+1 is the next generation generated by STS. Then

1
N

Fc (π, Tk , Pt , Xr ) −
Xr ∈Pt+1

0≤F (π, Tk , Pt+1 ) −
G
0≤F (π, Tk , Pt+1
)−

1
N
1
N

1
N

Fc (π, Tk , Pt , Xr )≤0,

(6)

G
Xr ∈Pt+1

Fc (π, Tk , Pt , Xr )≤Tk logK ((N + M )/N ),

(7)

Fc (π, Tk , Pt , Xr )≤Tk logK ((N + M )/N ).

(8)

Xr ∈Pt+1

G
Xr ∈Pt+1

Here we omit the proof of Lemma 1 for paper length limitation.
G
Theorem 1. Let y = NM
+M and D(π, Tk , Pt , Ot ) = F (π, Tk , Pt+1 )−F (π, Tk , Pt+1 ).
Then
lim D(π, Tk , Pt , Ot ) = 0.
(9)
y→0

Proof. Subtracting (8) from (7), we obtain an inequation:
D(π, Tk , Pt , Ot )≤Tk logK ((N + M )/N )+
1
1
(
Fc (π, Tk , Pt , Xr ) −
N
N
Xr ∈Pt+1

Fc (π, Tk , Pt , Xr )). (10)
G
Xr ∈Pt+1

1002

W. Ying et al.

Then we substitute (6) into (10) to get:
D(π, Tk , Pt , Ot )≤Tk logK ((N + M )/N ).

(11)

G
has the exact minimum free energy, we also get:
Since Pt+1

D(π, Tk , Pt , Ot )≥0.

(12)

Furthermore, there exists such a limit:
lim (Tk logK ((N + M )/N )) = 0.

y→0

(13)

Applying the Squeeze Law of limits, we can obtain (9) from (11), (12) and (13).
Theorem 1 asserts that the selection rule STS has the approximate steepest
descent ability of the free energy if M N . According to Theorem 1, M and N
have been set to satisfy y<0.1 in all our experiments.

5

Experiments and Results

In this section, we present the experimental results on one instance of 0-1 knapsack problems, called KP2. KP2 is generated by the Pisinger’s algorithm [8] of
constructing test instances with these parameters: data rang R=1000, instance
size n=100, problem type t =weakly correlated, instances sum S =1000 and random seed i=750. Note that here the proﬁts pj , the weights wj and the capacity c
are positive integers. We apply the simple genetic algorithm (SGA) with elitism,
the steady state genetic algorithm (SSGA), TDGA and STEA to this instance.
They all utilize the uniform crossover with its probability Pc = 0.8, the uniform
mutation with its probability Pm = 0.05, and the same population size N = 80.
The termination condition is satisﬁed when 3.2×106 individuals are searched.
The oﬀspring population for SSGA, TDGA and STEA has the size M = 8. All
experiments are performed on a Pentium-4 3.0G computer.
Assume that the items are ordered according to their eﬃciency such that
pi /wi ≥pj /wj when i < j. Martello [9] has proved that the greedy value fu =
b−1

pj + xb pb is an upper bound of the objective function f where
j=1
b−1

b

pj ≤c,
j=1

b−1

pj >c, and xb = (c −
j=1

wj )/wb .

(14)

j=1

It’s also obvious that fl = 0 is an inﬁmum of f . Therefor we can get an upper
bound eu = −fl = 0 and a lower bound el = −fu = −38249 of energy for the
level partition π about KP2. The other parameters of STEA have the following
values: level number K = 15 N = 16 and chain length Lk = 10n = 1000.
There were respectively 40 trials on KP2 for each algorithm. Table 1 provides
the statistic results of 40 trials for each algorithm, including the rate of hitting

A Steep Thermodynamical Selection Rule for Evolutionary Algorithms

1003

Table 1. Statistic results for four algorithms
Algorithm

Hitting rate Worst

SGA
SSGA
TDGA
STEA
†

1/40
8/40
23/40
40/40

38145
38240
38240
38245†

Mean

Best

38215.700
38241.000
38242.875
38245.000†

Time First hitting time
†

38245
38245†
38245†
38245†

152.2
303.2
56930.4
191.8

>151.8
>254.0
>26275.7
=30.8

optimal solution
4

35

3.8245
SSGA
TDGA
STEA

30

3.824

20

Profit

Frequency

25

x 10

15
10

3.8235
SSGA
TDGA
STEA

3.823

5
0

3.8225
Excellent

Good
Weak
Performance

Bad

Fig. 2. Performance distributions

0

1

2
Generation

3

4
5

x 10

Fig. 3. Convergence curves

optimum successfully, the worst result, the average result, the best result, average
running time (seconds), and average time (seconds) of ﬁrst hitting optimum. We
score the performance during each trial for each algorithm according to the number s of searched individuals when ﬁrst hitting optimum as follows: (1) excellent
if s≤8×105 ; (2) good if 8×105 <s≤1.6×106 ; (3) weak if 1.6×106 <s≤3.2×106 ; (4)
bad if s>3.2×106 . Figure 2 shows the frequencies of four scores in 40 trials for
SSGA, TDGA and STEA. Figure 3 shows their average convergence curves in
40 trials of the best individual of each generation. There are a few interesting
observations which can be made on the basis of the experiments:
1. The results in Table 1 demonstrate clearly the stability of STEA. Its hitting
rate is much higher than that of the other three algorithms. Note that for
STEA the optimum was found in all 40 trials. Moreover, the quality of its
solutions is averagely superior to that of the others due to its stability.
2. The above results also illustrate the high computational eﬃciency of STEA.
STEA nearly spends the same running time as SGA. It contrasts clearly
with the extremely high computational cost of TDGA, which is about 300
times as much as the others.
3. The ﬁrst hitting time is a very signiﬁcant performance goal for GA. It’s very
exciting that STEA has the far more rapid ﬁrst hitting time than the others
because of its high stability and low computational cost.
4. TDGA as well as SSGA presents the serious polarization phenomenon in
Figure 2. It indicates that TDGA often get trapped in local optima.

1004

W. Ying et al.

However, STEA avoids this phenomenon due to keeping a systematical balance between selective pressure and population diversity successfully.
5. Figure 3 shows that the convergence speeds of SSGA and TDGA are faster
than STEA at the early stage. However, they are in a nearly stagnant state
after that stage and then the speed of STEA exceeds theirs. STEA gains the
more rapid global convergence at the very little cost of the early stage.

6

Conclusions

This paper proposes a steep thermodynamical evolutionary algorithm (STEA),
which utilizes a steep thermodynamical selection rule. STEA simulates the competitive mechanism between energy and entropy in annealing to systematically
resolve the conﬂicts between selective pressure and population diversity in GA.
The experimental results show that STEA not only speeds up the global convergence of TDGA remarkably at the very little cost of the early stage, but also
improves the stability and the computational eﬃciency of TDGA greatly.
Further research will concentrate on the analysis of the convergence traits
about STEA from the viewpoint of statistical mechanism.
Acknowledgement. This research is supported by the National Natural
Science Foundation of China under Grant No.60473014.

References
1. Goldberg, D.E.: Genetic algorithms in search, optimization, and machine learning.
Addison-Wesley. (1989)
2. Su, X.H., Yang, B., Wang, Y.D.: A genetic algorithm based on evolutionary stable
strategy. Journal of Software. 14(11) (2003) 1863-1868 (in Chinese)
3. Whitley, D.: The GENITOR algorithm and selection pressure: Why rank-based
allocation of reproductive trials is best. Proc. of Int. Conf. on Genetic Algorithms.
(1989) 116-123
4. Li, Y.X., Zou, X.F., Kang, L.S., Michalewicz, Z.: A new dynamical evolutionary
algorithm based on statistical mechanics. J. Computer Science and Technology. 18
(2003) 361-368
5. Kirkpatrick, S., Gelatt, C.D., Vecchi, M.P.: Optimization by simulated annealing.
Science. 220 (1983) 671-680
6. Mori, N., Yoshida, J., Kita, H., Nishikawa, Y.: A thermodynamical selection rule for
the genetic algorithm. IEEE Conf. on Evolutionary Computation. (1995) 188-192
7. Kita, H., Mori, N., Nishikawa, Y.: Maintenance of Diversity by means of Thermodynamical Selection Rules for Genetic Problem Solving. Proc. of Int. Symposium
on Artiﬁcial Life and Robotics. (1999) 330-333
8. Pisinger, D.: Core problems in knapsack algorithms. Operations Research. 47 (1999)
570-575
9. Martello, S., Toth, P.: Knapsack problems: algorithms and computer implementations. John Wiley & Sons. (1990)

