Unsupervised and Semi-supervised Lagrangian
Support Vector Machines
Kun Zhao1 , Ying-Jie Tian2 , and Nai-Yang Deng1,
1

College of Science
China Agricultural University
piaopiao-zk@163.com,dengnaiyang@vip.163.com
2
Chinese Academy of Sciences
Research Center on Data Technology and Knowledge Economy
tianyingjie1213@163.com

Abstract. Support Vector Machines have been a dominant learning
technique for almost ten years, moreover they have been applied to supervised learning problems. Recently two-class unsupervised and semisupervised classiﬁcation problems based on Bounded C-Support Vector
Machines and Bounded ν-Support Vector Machines are relaxed to semideﬁnite programming[4][11]. In this paper we will present another version
to unsupervised and semi-supervised classiﬁcation problems based on Lagrangian Support Vector Machines, which trained by convex relaxation
of the training criterion: ﬁnd a labelling that yield a maximum margin on
the training data. But the problems have diﬃculty to compute, we will
ﬁnd their semi-deﬁnite relaxations that can approximate them well. Experimental results show that our new unsupervised and semi-supervised
classiﬁcation algorithms often obtain almost the same accurate results
as the unsupervised and semi-supervised methods [4][11], while considerably faster than them.
Keywords: Lagrangian Support Vector Machines, Semi-deﬁnite Programming, unsupervised learning, semi-supervised learning, margin.

1

Introduction

As an important branch in unsupervised learning, clustering analysis aims at partitioning a collection of objects into groups or clusters so that members within
each cluster are more closely related to one another than objects assigned to different clusters[1]. Clustering algorithms provide automated tools to help identify
a structure from an unlabelled set, in a variety of areas including bio-informatics,
computer vision, information retrieval and data mining. There is a rich resource
of prior works on this subject. The works reviewed below are most related to
ours.
This work is supported by the National Natural Science Foundation of China (No.
10371131,10631070 and 10601064).
The corresponding author.
Y. Shi et al. (Eds.): ICCS 2007, Part III, LNCS 4489, pp. 882–889, 2007.
c Springer-Verlag Berlin Heidelberg 2007

Unsupervised and Semi-supervised Lagrangian Support Vector Machines

883

Eﬃcient convex optimization techniques have had a profound impact on the
ﬁeld of machine learning. Most of them have been used in applying quadratic programming techniques to Support Vector Machines (SVMs) and kernel machine
training[2]. Semi-deﬁnite Programming (SDP) extends the toolbox of optimization methods used in machine learning, beyond the current unconstrained, linear
and quadratic programming techniques.
Semi-deﬁnite Programming (SDP) has showed its utility in machine learning. Lanckreit et al show how the kernel matrix can be learned from data
via semi-deﬁnite programming techniques[3]. De Bie and Cristanini develop a
new method for two-class transduction problem based on semi-deﬁnite relaxation technique[5]. Xu et al based on[3][5] develop methods to two-class unsupervised and semi-supervised classiﬁcation problems in virtue of relaxation to
Semi-deﬁnite Programming[4]. Zhao et al present another version to unsupervised and semi-supervised classiﬁcation problems based on Bounded ν-Support
Vector Machines [11].
In this paper we provide a brief introduction to the application of Semideﬁnite Programming in machine learning[3][4][5][11] and construct other unsupervised and semi-supervised classiﬁcation algorithms. They are based on
Lagrangian Support Vector Machines (LSVMs), which obtain almost accurate
results as other unsupervised and semi-supervised methods [4][11], while considerably faster than them.
We brieﬂy outline the contents of the paper now. We review the Support
Vector Machines and Semi-deﬁnite Programming in Section 2 . Section 3 will
formulate new unsupervised and semi-supervised classiﬁcation algorithms which
are based on LSVMs. Experimental results will be showed in Section 4. In the
last Section we will have a conclusion.
A word about our notation. All vectors will be column vectors unless transposed to a row vector by ”T”. The scalar (inner) product of two vectors x and y
in the n-dimensional real space Rn will be denoted by xT y. For an l × d matrix
A, Ai will denote the ith row of A. The identity matrix in a real space of arbitrary dimension will be denoted by I, while a column vector of ones of arbitrary
dimension will be denoted by e.

2

Preliminaries

Considering the supervised classiﬁcation problem, we will assume the given labelled training examples (x1 , y1 ), . . . , (xn , yn ) where each example is assigned
a binary yi ∈ {−1, +1}. The goal of SVMs is to ﬁnd the linear discriminant
f (x) = wT φ(x) + b that maximizes the minimum misclassiﬁcation margin
min

w,b,ξ

1
( w
2

2

+ b2 ) +

C
2

n

ξi2
i=1

s.t. yi ((w · φ(xi )) − b) + ξi ≥ 1 , i = 1, 2, . . . , n

(1)

884

K. Zhao, Y.-J. Tian, and N.-Y. Deng

Let Φ = (φ(x1 ), . . . , φ(xn )), K = ΦT Φ, then Kij = φ(xi )T φ(xj ), dual problem
of (1) is
max −
α

1
2

n

n

yi yj αi αj Kij −
i=1 j=1

1
2

n

n

yi yj αi αj −
i=1 j=1

1
2C

n

n

α2i +
i=1

αi (2)
i=1

s.t. αi ≥ 0, i = 1, 2, . . . , n
The problem (1) and (2) are primal and dual problem of Lagrangian Support
Vector Machines (LSVMs) respectively [12].
Based on Bounded C-Support Vector Machines (BC-SVMs) [8] Xu et al get
the optimization problem [4] that can solve unsupervised classiﬁcation problem.
Zhao et al based on Bounded ν-Support Vector Machines (Bν-SVMs)[7] get the
optimization problem [11] that can solve unsupervised classiﬁcation problem
too. In this paper Lagrangian Support Vector Machines will be used to resolve
unsupervised and semi-supervised classiﬁcation problems.
Given H ∈ Mn , Ai ∈ Mn and b ∈ Rm , where Mn is the set of n × n
symmetric matrix. The standard Semi-deﬁnite Programming problem is to ﬁnd
a matrix X ∈ Mn for the optimization problem

(SDP)

min H • X
s.t. Ai • X = bi , i = 1, 2, . . . , m
X

0

where the • operation is the matrix inner product A • B = trAT B, the notation
X
0 means that X is a positive semi-deﬁnite matrix. The dual problem to
(SDP) can be written as:
max bT λ
m

(SDD)

s.t. H −

λi Ai

0

i=1

Here λ ∈ Rm . For Semi-deﬁnite Programming, interior point method has good
eﬀect, moreover there exists several softwares such as SeDuMi[10] and SDP3.

3

Unsupervised and Semi-supervised Classiﬁcation
Algorithms

A recent development of convex optimization theory is Semi-deﬁnite Programming, a branch of that ﬁelds aimed at optimizing over the cone of semi-positive
deﬁnite matrices. One of its main attraction is that it has been proven successful in construct tight convex relaxation of NP-hard problem. Semi-deﬁnite
Programming has showed its utility in machine learning too.
Lanckreit et al show how the kernel matrix can be learned from data via semideﬁnite programming techniques[3]. They presented new methods for learning a
kernel matrix from labelled data set and transductive data set. Both methods

Unsupervised and Semi-supervised Lagrangian Support Vector Machines

885

can relax the problem to Semi-deﬁnite Programming. For a transductive setting,
using the labelled data one can learn a good embedding (kernel matrix), which
can then be applied to the unlabelled part of the data. De Bie and Cristanini
relax two-class transduction problem to semi-deﬁnite programming based on
transductive Support Vector Machines[5].
Xu et al develop methods to two-class unsupervised and semi-supervised classiﬁcation problems based on Support Vector Machines in virtue of relaxation to
Semi-deﬁnite Programming[4]in the foundation of [5][3]. Its purpose is to ﬁnd a
labelling which has the maximum margin not to ﬁnd a large margin classiﬁer.
This leads to the method to cluster the data into two class, which subsequently
run a SVM, and will obtain the maximum margin with all possible labelling. We
should add constraint about class balance −ε ≤ ni=1 yi ≤ ε, otherwise we can
simply assign all the data to the same class and then get unbounded margin;
moreover this can avoid noisy data’s inﬂuence in some sense.
Using the method in [5][3], Xu et al based on BC-SVMs get the optimization problem[4]that can solve unsupervised classiﬁcation problem. Analogously
Zhao et al based on Bν-SVMs get the optimization problem[11]that can solve
unsupervised classiﬁcation problem too, which the parameter ν in Bν-SVMs
has quantitative meaning. However, the time consumed of both methods based
on BC-SVMs and Bν-SVMs is too long. So it seems necessary to ﬁnd a faster
method, which has almost accurate results as above at least. The reason that unsupervised classiﬁcation algorithms based on BC-SVMs and Bν-SVMs run slowly
is their semi-deﬁnite relaxations have so many variables, concretely n2 + 2n + 1
and n2 +2n+2 variables respectively. In order to fasten the speed of algorithm, it
seems better to ﬁnd a qualiﬁed SVM which has fewer constraints, for the number
of variables in semi-deﬁnite relaxation problem equals to sum of n2 + 1 and number of constraints in SVM. Primal problems of BC-SVMs and Bν-SVMs have 2n
and 2n + 1 constraints respectively, while primal problem of Lagrangian Support
Vector Machines has n constraints. Therefore it seems better to use Lagrangian
Support Vector Machines to resolve unsupervised classiﬁcation problem.
We use the same method in [5][3] to get the optimization problem based on
LSVMs
min

yi ∈{−1,+1}n

1
min ( w
w,b,ξ 2

2

+ b2 ) +

C
2

n

ξi2
i=1

s.t. yi ((w · φ(xi ) − b) + ξi ≥ 1

(3)

n

−ε ≤

yi ≤ ε
i=1

It is diﬃcult to solve Problem (3), so we will consider to get its approximate
solutions. Since Semi-deﬁnite Programming can provide eﬀective algorithms to
cope with diﬃcult computational problems and obtain high approximate solutions, it seems better to relax problem (3) to Semi-deﬁnite Programming.
Let y = (y1 , y2 , . . . , yn )T , M = yy T , Φ = (φ(x1 ), . . . , φ(xn )) and K = ΦT Φ,

886

K. Zhao, Y.-J. Tian, and N.-Y. Deng

moreover A ◦ B denotes componentwise matrix multiplication. Use the same
method in [3], we obtain the Unsupervised Classiﬁcation Algorithm.
Algorithm 3.1 (Unsupervised Classiﬁcation Algorithm)
1.Given data set D = {x1 , . . . , xn }, where xi ∈ X = Rd .
2.Select appropriate kernel K(x, x ), C and ε, then construct and solve the problem
1
δ
2
(K ◦ M + M +
s.t.
(u + e)T
−εe ≤ M e ≤ εe

min

M,δ,u

1
C I)

(u + e)
δ

0

(4)

M 0, diag(M ) = e
u≥0
Get the optimal solution M ∗ , δ ∗ and u∗ with SeDuMi.
3.Construct label y ∗ = sgn(t1 ), where t1 is eigenvector corresponding to the
maximal eigenvalue of M ∗ .
It is easy to extend the unsupervised classiﬁcation algorithm to semi-supervised
classiﬁcation algorithm. For semi-supervised SVMs training, we can assume
(x1 , y1 ), . . . , (xn , yn ) have labelled by experts and xn+1 , . . . , xn+N are not labelled.
Only adding the constraints Mij = yi yj , i, j = 1, 2, . . . , n to the problem (4)will
obtain the Semi-Supervised Classiﬁcation Algorithm.

4

Experimental Results

4.1

Results of Unsupervised Classiﬁcation Algorithm

In order to evaluate the performance of unsupervised classiﬁcation algorithm,
we will compared our unsupervised classiﬁcation algorithm ( L-SDP )with (νSDP)[11] and maximum margin clustering algorithm (C-SDP)[4]. Firstly we consider four synthetic data sets including data set AI, Gaussian, circles and joinedcircles, which every data set has sixty points. ε = 2, C = 100 and Gaussian kernel
with appropriate parameter σ = 1 are selected. Results are showed in Table 1.
The number is the misclassiﬁcation percent. From Table 1 we can ﬁnd that the
result of L-SDP is better than that of C-SDP and ν-SDP, moreover the time
consumed are showed in Table 2. The numbers are seconds of CPU. From Table 2
Table 1. Classiﬁcation results about three algorithms on four synthetic data sets
Algorithm
L-SDP
C-SDP
ν-SDP

AI
9.84
9.84
9.84

Gaussian
0
1.67
1.67

circles
0
11.67
1.67

joined-circles
8.19
28.33
11.48

Unsupervised and Semi-supervised Lagrangian Support Vector Machines

887

Table 2. Computation time about three algorithms on four synthetic data sets
Algorithm
L-SDP
C-SDP
ν-SDP

7

AI
1425
2408.9
2621.8

Gaussian
1328
1954.9
1891

3

circles
1087.6
2080.2
1837.1

joined-circles
1261.8
2284.8
2017.2

5

5

4

4

3

3

2

2

6
2.5

5

2

1

1

4
0

0

−1

−1

−2

−2

3
1.5

2

−3

1

−3

1
−4

0

1

2

3

4

5

6

7

8

9

10

11

0.5
0.8

1

1.2

1.4

1.6

1.8

2

2.2

2.4

2.6

−5
−5

−4

−4

−3

−2

−1

0

1

2

3

4

5

−5
−5

−4

−3

−2

−1

0

1

2

3

4

5

Fig. 1. Results by our unsupervised classiﬁcation algorithm based on LSVMs on the
four synthetic data sets including data set AI, Gaussian, circles and joined-circles

we can ﬁnd that the speed of L-SDP is faster than those of C-SDP and ν-SDP,
moreover almost half of the time consumed of others.
We also conduct our algorithm on the real data sets which can be obtained
from http://www.cs.toronto.edu/ roweis/data.html, including Face and Digits
data sets. As same to synthetic data sets, with thirty samples of every class of
data sets. To evaluate clustering performance, a labelled data set was taken and
the labels are removed , then run clustering algorithms, and labelled each of
the resulting clusters with the majority class according to the original training
labels, then measured the number of misclassiﬁcation.The results are showed
in Table 3 and the number is the misclassiﬁcation percent. From Table 3 we
Table 3. Results about three algorithms on Face and Digits data sets
Algorithm
L-SDP
C-SDP
ν-SDP

Digits32
0
0
0

Digits65
0
0
0

Digits71
0
0
0

Digits90
0
0
0

Face12
8.33
1.67
1.67

Face34
0
0
0

Face56
0
0
0

Face78
0
0
0

can ﬁnd that the result of L-SDP is almost same to C-SDP and ν-SDP except
data set face12, but the time consumed are showed in Table 4. The numbers are
seconds of CPU.
From Table 4 we can ﬁnd that the speed of L-SDP is much faster than those
of C-SDP and ν-SDP, moreover quarter of the time consumed of others at least.
4.2

Results of Semi-supervised Classiﬁcation Algorithm

We test our algorithm to semi-supervised learning on the real data sets as same
to section of unsupervised Classiﬁcation Algorithm. As same to unsupervised
classiﬁcation algorithm, in order to evaluate the performance of semi-supervised

888

K. Zhao, Y.-J. Tian, and N.-Y. Deng

Table 4. Computation time about three algorithms on Face and Digits data sets
Algorithm
L-SDP
C-SDP
ν-SDP

Digits32
445.1
1951.8
1721.6

Digits65
446.2
1950.7
1721.5

Digits71
446.4
1951.6
1722.2

Digits90
446.5
1953.4
1722.8

Face12
519.8
1954.5
1721.4

Face34
446.3
1952.1
1722.1

Face56
446.1
1950.3
1721

Face78
446
1951
1719.7

Fig. 2. Every row shows a random sampling of images from a data set, the ﬁrst ten
images are in one class, while the rest ten images are in another class by L-SDP.

classiﬁcation algorithm, we will compared our semi-supervised classiﬁcation algorithm ( Semi-L-SDP )with (Semi-ν-SDP)[11] and maximum margin clustering
algorithm (Semi-C-SDP)[4]. We separate the data into labelled and unlabelled
parts, and get rid of the labels of the unlabelled portion, then run semi-supervised
classiﬁcation algorithms to reclassify the unlabelled examples in use of the learning results, eventually measured the misclassiﬁcation error on the original labels.
Thirty samples of every class of data sets will be used. The results will be showed
in Table 5 and the number is the misclassiﬁcation percent. From Table 5 we
can ﬁnd that the result of Semi-L-SDP is almost same to Semi-ν-SDP and better than Semi-C-SDP except data set face12, but time consumed of CPU is
much less than those of others. Results are showed in Table 6. The numbers are
seconds of CPU. From Table 6 we can ﬁnd that the speed of Semi-L-SDP is much
Table 5. Results about three algorithms on Face and Digits data sets
Algorithm Digits32
Semi-L-SDP 5
Semi-C-SDP 25
Semi-ν-SDP 5

Digits65
5
28.3
5

Digits71
5
28.3
5

Digits90
5
28.3
5

Face12
11.67
16.67
3.3

Face34
5
28.3
5

Face56
5
28.3
5

Face78
5
28.3
5

Table 6. Computation time about three algorithms on Face and Digits data sets
Algorithm Digits32
Semi-L-SDP 606.8
Semi-C-SDP 1034
Semi-ν-SDP 734.8

Digits65
607.8
1036
735.5

Digits71
607.9
1035.6
735.7

Digits90
608.3
1035.8
735.9

Face12
653.9
1094.7
810.6

Face34
608.4
1033.1
734.4

Face56
608
1035.1
735.5

Face78
608.2
1035.8
736

Unsupervised and Semi-supervised Lagrangian Support Vector Machines

889

faster than those of Semi-C-SDP and Semi-ν-SDP, moreover almost half of the
time consumed of others.

5

Conclusion

We have proposed eﬃcient algorithms for unsupervised and semi-supervised classiﬁcation problems based on Semi-deﬁnite Programming. From Section of experimental results we can learn that unsupervised and semi-supervised classiﬁcation
algorithms based on Lagrangian Support Vector Machines is much faster than
other methods based on Bounded C-Support Vector Machines and Bounded
ν-Support Vector Machines, and classiﬁcation results are better than them.
In the future we will continue to estimate the approximation of SDP relaxation
and get an approximation ratio of the worst case.

References
1. J.A.Hartigan, Clustering Algorithms, John Wiley and Sons, 1975.
2. B.Schoelkopf and A.Smola, Learning with kernels: Support Vector Machines, Regularization, Optimization,and Beyond, MIT Press, 2002.
3. G.Lanckriet, N.Cristianini, P.Bartlett, L.Ghaoui and M.Jordan, Learning the kernel matrix with semideﬁnite programming,Journal of Machine learning research,
5, 2004.
4. L.Xu, J.Neufeld, B.Larson and D.Schuurmans, Maximum margin clustering, Advances in Neural Information Processing Systems 17(NIPS-04), 2004.
5. T.De Bie and N.Crisrianini, Convex methods for transduction, Advances in Neural
Information Processing Systems 16(NIPS-03), 2003.
6. N.Y.Deng and Y.J.Tian, A New Method of Data Mining: Support Vector Machines,
Science Press, 2004.
7. T.Friess, C.N.Christianini, C.Campbell, The kernel adatron algorithm: a fast and
simple learning procedure for support vector machines, Proceeding of 15th Intl.
Con Machine Learning, Morgan Kaufman Publishers, 1998.
8. O.L.Mangasarian and D.R.Musicant, Successive overrelaxation for support vector
machines, IEEE Trans. Neural Networks, 5,1999(10),1032-1037.
9. Nello Cristianini and John Shawe-Taylor, An Introduction to Support Vector Machines and Other Kernel-based Learning Methods, Cambridge University Press,
2000.
10. Jos F.Sturm, Using SeDuMi1.02, A Matlab Toolbox for Optimization over Symmetric Cones, Optimization Methods and Software, 11-12, 1999, 625-653.
11. Kun Zhao, Ying-jie Tian and Nai-yang Deng, Unsupervised and Semi-supervised
Two-class Support Vector Machines, to appear.
12. O.L.Mangasarian and David R.Musicant,Lagrangian Support Vector Machines,
Journal of Machine Learning Research,1,2001,161-177.

