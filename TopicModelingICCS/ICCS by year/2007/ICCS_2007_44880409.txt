Kimberlites Identiﬁcation by Classiﬁcation
Methods
Yaohui Chai1,2 , Aihua Li1,2 , Yong Shi1,2 , Jing He1,2 , and Keliang Zhang3
1

Chinese Academy of Sciences Research Center on Data Technology and Knowledge
Economy,
Chinese Academy of Sciences, Beijing 100080, China
2
School of Management, Graduate University of Chinese Academy of Sciences,
Chinese Academy of Sciences, Beijing 100039, China
3
College of earth science, Graduate University of Chinese Academy of Sciences,
Chinese Academy of Sciences, Beijing 100039, China
chaiyaohui05@mails.gucas.ac.cn, lah04b@mails.gucas.ac.cn,
yshi@gucas.ac.cn, hejing@gucas.ac.cn

Abstract. Kimberlites identiﬁcation is a very important task for diamond mining. In traditional way, geologists draw upon past experience
to do this work. Whether the bedrock should be drilled depends on their
analysis of rock samples. This method has two disadvantages. First, as
the database increasing, it becomes more diﬃcult to do this work by
manual inspection. Secondly, the accuracy is inﬂuenced by the expert’s
experience, and it reaches scarcely 80 percents averagely. So an analytical
method to kimberlites identiﬁcation over large geochemical datasets is
demanded. This article applies two methods (SVM and decision tree) to
a dataset provided by a mining company. Comparing the performances
of these two methods, our results demonstrate that SVM is an eﬀective
method for this work.
Keywords: Kimberlites Identiﬁcation, Classiﬁcation, Feature Selection,
SVM, Decision Tree.

1

Introduction

World natural diamond production for 2004 is estimated at 156 million carats
and it translated into 61.5 billion US dollars in worldwide jewellery sales [1]. Even
though, the current level of demand for diamonds with high color and quality is
still not being met by the world’s producing diamond mines. Numerous companies are carrying out various phases of diamond exploration in Botswana, which
is the world’s leading producer of gem quality diamonds. Due to the extensive
This research has been partially supported by a grant from National Natural Science
Foundation of China (#70621001, #70531040, #70501030, #70472074), #9073020
from NSFB, 973 Project #2004CB720103, Ministry of Science and Technology,
China, and BHP Billiton Co., Australia.
Y. Shi et al. (Eds.): ICCS 2007, Part II, LNCS 4488, pp. 409–414, 2007.
c Springer-Verlag Berlin Heidelberg 2007

410

Y. Chai et al.

Kalahari sand cover (and Karoo basalts underneath), sophisticated and innovative sampling and geophysical techniques are required to locate undiscovered
kimberlites [2].
The goal of this paper is to build an analytical model for kimberlites identiﬁcation. Two classiﬁcation methods are applied to a dataset containing information
about rock samples drilled in Botswana. This article is organized as follows. In
Section 2, we describe the dataset and its preprocessing steps. Section 3 gives
an overview of those two classiﬁcation methods of SVM and decision tree, and
their results are listed out in Section 4. Finally, we conclude this work with a
summary in Section 5.

2

Data Description and Data Preprocessing

The dataset contains rock samples data from one region of Botswana. Original
dataset has 5921 row of observations and 89 variables, and each observation
describes detailed information of one rock sample about its position, physical
and chemical attributes. These variables include numeric and character types.
After consulting the experts, we deleted some rows missing important variables
and exclude some variables, which are irrelevant, redundant, or correlated such
as sample-id and horizon. Then some types of variables are transformed from
character to binary to satisfy the requirements of models, such as color and
shape.
After data transformation, the dataset includes 4659 observations and 101
variables.

3

Methods

Data classiﬁcation is a two-step process [3]. In the ﬁrst step, a model is built
describing a predetermined set of data classed or concepts. This model is constructed by analyzing database tuples described by attributes. Each tuple is
assumed to belong to a predeﬁned class, as determined by one of the attributes,
called the class label attributes. The data used to build the model is called training set. And this step can be called supervised learning. In the second step, the
model is used to classiﬁcation. And the predictive accuracy of the model is estimated. The data set used to classiﬁcation in this step is called testing set. When
the constructed model is proved to be stable and robust, then this model can be
used to predict the new data.
The kimberlites identiﬁcation for this dataset can be regarded as a four-group
classiﬁcation problem based on the fact that there are four important kinds of
rock in this dataset. We will apply two standard classiﬁcation methods for this
work.

Kimberlites Identiﬁcation by Classiﬁcation Methods

3.1

411

SVM

The support vector machine[4] is a powerful machine learning method for classiﬁcation, and it is always described as an optimal problem as follows:
1
2

M in

w

2

(1)

Subject To:
yi ((ω • xi ) + b) ≥ 1, i = 1, 2, . . . , l

(2)

l

T = {(x1 , y1 ) , · · · (xl , yl )} ∈ (X × Y ) , xi ∈ X = Rn , yi ∈ Y = {1, −1}, i =
1, 2, · · ·l. Usually, as for the linearly inseparable sets, we should introduce the
kernel function and solve its dual problem to get the decision function:
Dual problem:
l

l

αi −

M ax w(α) =
i=1

1
αi αj yi yj xi , xj
2 i,j=1

(3)

Subject To:
l

αi ≥ 0,

αi yi = 0,

i = 1, 2, · · ·l

(4)

i=1

Here, use a kernel to compute xi , xj = Φ(xi ), Φ(xj ) = k (xi , xj ),and the
decision function is:
l

α∗i yi k(x, xi ) + b∗ )

f (x) = sgn(

(5)

i=1

3.2

Decision Tree

A decision tree[5] is a ﬂow-chart-like tree structure, where each internal node
denotes a test on an attribute, each branch represents an outcome of the test,
and leaf nodes represent classes or class distributions. The top node in a tree is
called the root node.
Algorithms to build a tree is listed as follows,
Function Tree =Decision Tree Create (T, A, Y) [3]
Input: training set, attributes and label.
Output: A decision tree.
1. Tree = Create Node (T);
2. If samples in T are the same, then label the leaf nodes with the same class,
return Tree;
3. If attributes are empty, then label the node with the most common class in
T,return Tree;

412

Y. Chai et al.

4. (X,Values) = Attribute Selection (T,A,Y); //select the best attribute X and
split points’ Values
5. for each V in Values do
6. Sub T =V’s sub-samples set satisﬁed with the testing condition
7. Node = Decision Tree Create (Sub T, A-X, Y);
8. Create Branch (Tree, Node);
9. end for;
10. return Tree;
3.3

Cross-Validation

There are several ways to estimate the classiﬁer accuracy, such as holdout
method, k-fold cross-validation method, and leaving-one-out and so on. In this
article we choose the 10-fold Cross-validation method. The initial data are randomly partitioned into 10 mutually exclusive subsets or folds, each of which has
the approximately equal size. Training and testing are performed 10 times.

4

Experimental Results

For this dataset are linearly inseparable, we use C-SVM with RBF kernel function [6] to classify the rock samples. (This model is called C − SV M 1 ) There
are two parameters (C for the objective function and GAMMA for the kernel
function) selected for this model. The most common and reliable approach is to
decide on parameter ranges, and to do an exhaustive grid search over the parameter space to ﬁnd the best setting [7]. Figure.1 shows its process. It contains
contour plots for training datasets. The diﬀerent colors of the projected contour
plots show the progression of the grid method’s best parameter estimate. The
ﬁnal optimal parameter settings are C=128 and GAMMA=0.0078125.

Fig. 1. Parameter selected by grid.py

Kimberlites Identiﬁcation by Classiﬁcation Methods

413

Decision tree is assembled to SPSS 12(SPSS Inc.), and it is easy to use the
GUI to import the data and export the tree model. The depth of the tree is 3,
and 36 nodes are created for this model. There are 12 rules to classify the rock
samples. The accuracy is also estimated by 10 folds cross validation method.
Table.1 shows the accuracy result of both these methods compared with another
method linear discriminant:
Table 1. The ten folds cross validation accuracy for those methods
Methods
C − SV M 1 Decision tree Linear discriminant C − SV M 2
Cross validation 95.66%
89.1%
80.1%
95.04%

The two main approaches take the comparable computation time with 2
minutes around, while the SVM has excellent accuracy compared with decision
tree and linear disciminant. Still we ﬁnd that the parameter selection for SVM
takes a couple of hours. For reducing the computation time and computational
complexity, a feature selection is needed. And this work can also help the geophysical experts to make right decision based on less rock sample attributes. In
this article, we use F-score [8] as a feature selection criterion base on the simple
rule that the large score is, the more likely this feature is more discriminative.
Table 2. The accuracy after feature selection
Feature selected
Cross validation

101
95.66%

88
95.15%

44
95.04%

Based on the F-score rank, we selected 44 features and then apply C-SVM
with RBF for training and prediction. Its accuracy is still above 95 percents.
This model (C − SV M 2 ) takes less time on parameter selection and the best
settings are C=64, GAMMA=0.00195. Through the ten-fold cross validation,
this compromise model is proved to be accurate and stable, so it can be applied
to a new geochemical dataset.

5

Conclusion

A sophisticated and innovative method for Diamond-bearing kimberlites identiﬁcation is needed for the diamond mining, especially in the area covered by
extensive Kalahari sand. When a model is proved to be robust and eﬀective
for this work, it will be greatly helpful to the experts on kimberlites discovery.
This article applies two methods to this new domain of application. Our results
demonstrate that, both of these two methods have much higher prediction accuracy than 80 percents (the experts suggested). The tree model is faster than
SVM while SVM provides a higher accuracy. Feature selection method is used to

414

Y. Chai et al.

build a compromise model between higher accuracy and less computation time,
and its classiﬁcation accuracies are acceptable.
This paper is not aimed at providing a method to take place of the expert’s
dicision. It is just the opposite that the process developed in this research should
be supervised by a veteran expert, and its results must be understandable.

Acknowledgement
The author received much help from following members: Dr.Yi Zeng(exploration
and mining technology BHP billiton centre, Principal Scientist), Dr.Dongping
Wei(College of earth science, Graduate University of Chinese Academy of Sciences, Professor), Zhan Zhang(School of Management, Graduate University of
Chinese Academy of Sciences, Masters).

References
1. Diamond Facts 2004/05 NWT Diamond Industry Report, available at
http://www.iti.gov.nt.ca/diamond/diamond facts2005.htm
2. C. Williams, B. Coller, T. Nowicki, J. Gurney: MEGA KALAHARI GEOLOGY:
CHALLENGES OF KIMBERLITE EXPLORATION IN THIS MEDIUM (2003)
3. Han, J. W. and Kamber, M.: Data Mining: Concepts and Techniques. Morgan Kaufmann Publication (2001)
4. Vapnik, V.: The Nature of Statistical Learning Theory. New York: Springer-Verlag
(1995)
5. C. Wu, D. Landgrebe, and P. Swain: The decision tree approach to classiﬁcation.
Technical Report TR-EE-75-17, Laboratory for Applications of Remote Sensing,
School of Engineering, Purdue University, West Lafayette, IN, May (1975)
6. Mackay D.: Introduction to Gaussian processes. In: Neural Networks and Machine
Learning (1999)
7. Chih-Chung Chang and Chih-Jen Lin, LIBSVM: a library for support vector machines. (2001) Software available at http://www.csie.ntu.edu.tw/ cjlin/libsvm
8. Yi-Wei Chen and Chih-Jen Lin: Combining SVMs with Various Feature Selection
Strategies. In: Feature extraction, foundations and applications (2005)

