Eye Tracking and Gaze Based Interaction within
Immersive Virtual Environments
Adrian Haﬀegee and Russell Barrow
Advanced Computing and Emerging Technologies Centre,
The School of Systems Engineering, University of Reading, UK
a.haffegee@reading.ac.uk

Abstract. Our eyes are input sensors which provide our brains with
streams of visual data. They have evolved to be extremely eﬃcient, and
they will constantly dart to-and-fro to rapidly build up a picture of the
salient entities in a viewed scene. These actions are almost subconscious.
However, they can provide telling signs of how the brain is decoding
the visuals, and can indicate emotional responses, prior to the viewer
becoming aware of them.
In this paper we discuss a method of tracking a user’s eye movements,
and use these to calculate their gaze within an immersive virtual environment. We investigate how these gaze patterns can be captured and
used to identify viewed virtual objects, and discuss how this can be used
as a natural method of interacting with the Virtual Environment. We describe a ﬂexible tool that has been developed to achieve this, and detail
initial validating applications that prove the concept.

1

Introduction

Psychological studies often use eye tracking to gather information relating to
how a user reacts to particular visuals. Typical uses would be for areas such
as marketing aesthetics, the eﬀectiveness of emergency signage, or measuring
attention[1,2]. Current methods record the gaze position on a 2D image or video
stream, with the captured data being stored for oﬄine analysis.
Virtual Environments (VE) are used to create alternate worlds that users can
enter and interact with. These worlds are conﬁgurable and controllable, and are
well suited for constructing scenes that would be diﬃcult or time consuming to
build in the real world e.g., those that are hazardous, dynamic, or too expensive.
By using eye tracking within the VE, it is possible to capture a user’s eye
movements and analyse how they are observing the scene. Because the composition of the VE is known, these actions can be directly mapped to entities within
the scene providing the possibility of automatic analysis of where a user is looking. This could then be used for interaction with the environment itself, with
previous work considering sight operated pointing and selecting[3,4]. Although
there has been a reasonable research with eye tracking within VEs, there has
been little within modern 3D immersive projection technologies. This combination with these systems that provide a more natural interactive experience has
G. Allen et al. (Eds.): ICCS 2009, Part II, LNCS 5545, pp. 729–736, 2009.
c Springer-Verlag Berlin Heidelberg 2009

730

A. Haﬀegee and R. Barrow

wide potential. In multiuser environments, research has been undertaken on the
importance of gaze to aid communication amongst remote participants[5,6].
In the next section we will discuss the system conﬁgurations that have been
used, and why they were chosen. Section 3 will then detail the methods and
algorithms used in the implementation. Section 4 will cover the validation and
a basic sample application, before Sect. 5 concludes the paper and describes
potential future extensions.

2

System Configuration

This research focuses on the problem of enabling a head mounted eyetracker to be
used inside an Immersive Virtual Environment (IVE). The intelligent coupling of
these two systems makes it possible to calculate where in the virtual scene a user
is looking. The main components are the immersive VR system, the eyetracker,
and software that binds it together.
Immersive Systems. Immersive projection based technologies such as the
CAVE [7] place the user in an environment surrounded by one or more projection screens. The user is free to move within the conﬁnes of the system. They are
position tracked and the perspective of their viewed images are adjusted according to their relative head position. While more expensive than other immersive
technologies such as Head Mounted Displays (HMDs), these systems are less
restrictive and provide a more natural view of the environment in addition to
the methods of interacting with it. These systems provide the user with a higher
degree of presence within the VE [8]. This is the degree that they feel that they
are a participant within the environment. The greater the feeling is, the more
likely they are to instinctively behave and react as though they were within a
real situation.
The Mobile Eye. The ASL Mobile Eye eye tracker [9], was used for this project.
It is lightweight and glasses mounted, and is well suited for use within IVEs where
the user will be free to move around. This is in contrast to freestanding trackers,
which require the user to remain relatively still, and at a ﬁxed position from the
device. Other head mounted devices could use the approach described, but may
need minor modiﬁcations depending on the format and structure of their data
output.
The Mobile Eye uses Dark Pupil Tracking to calculate gaze position. This
uses the pupil position and the reﬂection from the cornea to determine the eye’s
direction. Figure 1 shows how we combine the system with a standard head
tracker (here an Intersense IS-900), which obtains the user’s head position and
orientation within the environment. Attached to the glasses are two cameras; one
aimed in the direction of the user’s vision, and the second capturing an image
of their eye reﬂected by the combiner.
The system is calibrated to provide a gaze position for where the user is
looking at any time. This is indicated to the user by overlaying a gaze position
marker on top of the output video stream from the scene camera. A sample

Eye Tracking and Gaze Based Interaction

731

Fig. 1. The ASL Mobile Eye device, showing how it is combined with a head tracker
(left). Video output from the device, with a crosshair overlay for gaze direction (right).

output from this can be seen in the right hand image of Fig. 1. Here a crosshair
is being used to represent the user’s line of sight. The (x,y) coordinates of the
marker as displayed on the video image can optionally be streamed in an encoded
format from the analysis computer’s serial port. It is these Point of Gaze (PoG)
coordinates that we use for calculating our virtual world gaze tracking.
Virtual Environment Application Development. Diﬀerent methods and
tools are available for the creation of IVEs. This research used the VieGen
framework[10], which is a set of tools and utilities to aid application development. Entities within the virtual world are represented by members of an
extensible family of SceneNodes. These contain the conﬁgurable attributes and
behaviours of the objects, can be dynamically loaded at runtime, and provide a
harness for developers to extend the environment.

3

Calculating and Using the Gaze Vector

This project converts the PoG output from the Mobile Eye into a virtual world
gaze vector. This is a vector starting at the user’s eye position and heading oﬀ
in the direction of their line of sight. Within the VE, this vector can be used to
indicate potential areas of visual interest, or as advanced methods of controlling
the environment. Being glasses mounted, the Mobile Eye’s frame of reference
is that of the head tracker oﬀset by the distance from the tracker to the eye.
This relationship provides a method of converting from the (x,y) PoG coordinate
output into the 3D virtual world gaze vector.
Figure 2 shows a breakdown of the modules developed for this research. The
left image shows the high level components which have been wrapped into a
VieGen Dynamic SceneNode, allowing for rapid development and portability.
The Eye Tracking Control Module provides the core functionality from this
research, and could easily be extracted for incorporation into any other VR

732

A. Haﬀegee and R. Barrow

Fig. 2. Gaze Track SceneNode overview (left) & Eye Tracking Control Module (right)

application or framework. The right image shows the breakdown of this core
component, containing the RS-232 (serial) stream decoding, coordinate translation, conversion to gaze vector, and the state machine that binds it together. In
addition to the control module the SceneNode also contains conﬁguration data
and an interaction component responsible for how the node will react with the
rest of the VE.
3.1

Eye Tracking Control Module

Serial Capture, Alignment and Decoding. The Mobile Eye streams the
encoded tracking information as consecutive 10 byte blocks of serial data. This
component locks onto the stream to locate the start of each block, and then
decodes the data into a structure which contains the PoG coordinates in the
video stream. If the tracker fails to calculate the eye position, (e.g. due to the
user blinking or removing the glasses), a status byte within this structure is used
to indicate an error condition.
Mapping PoG Coordinates on to a Virtual World Plane. The PoG
coordinates can be considered as the (x,y) coordinates on a plane that is a
constant distance and perpendicular to the user’s head position. A similar plane
can be created in virtual space maintaining a ﬁxed position relative to the user’s
head tracked location. A relationship between the real and virtual gaze positions
can be obtained by having the user ﬁxate on a known point on the virtual
plane, while reading the PoG coordinates streamed from the Mobile Eye. The
software takes several readings for each of these ﬁxation points, and averages
the valid ones to minimise errors or inaccuracies. By sampling a number of these
relationships across diﬀerent positions on the gaze plane, a calibration mapping
of PoG (x,y) position to virtual plane location can be constructed.
This calibration data is stored within the module, and can be further analysed
to determine its nature. It was observed that for the Mobile Eye there was a
linear correlation between the PoG coordinates and the virtual plane positions.
However it should be noted that diﬀerent cameras/lenses could deviate from this

Eye Tracking and Gaze Based Interaction

733

and would require subsequent algorithm modiﬁcations. Ideally future versions
would be able to automatically self-check the calibration data and could prompt
for recalibration if required.
From the calibration mapping it is desirable to formulate a method of calculating the virtual gaze plane interception location from any PoG (x,y) position.
Assuming that the mapping is linear the gaze plane location can be calculated
by comparing the unknown position’s PoG value relative to two of the known
calibration points. Ideally the chosen points should be suﬃciently distant from
each other to reduce the eﬀect of errors in the calibration data. The reliability of
this calculation can then be further improved by combining the results obtained
relative to a number of these calibration point pairs. However small degrees of
non-linearity in the mapping would introduce errors the greater the distance to
these calibration points.
Obtaining the Gaze Vector. The virtual gaze plane is located at a ﬁxed
oﬀset from the user’s head, the location of which is known within the VE. By
applying the eye oﬀset to the head location the eye position can be found. The
gaze vector is a ray starting at the eye and heading through the gaze plane point
of interception, and oﬀ into the distance. This ray can be applied to the virtual
scene to determine the ﬁrst object that it intercepts. Assuming the object is
visible it will be the object being viewed.
3.2

Scene Interaction Control

The gaze tracking functionality is wrapped in a VieGen SceneNode, which provides useful infrastructure for interfacing with the virtual scene. It includes the
functionality for calculating the gaze vector, and also a state machine for controlling the calibration process, markers for indicating the direction or position
of gaze, and event message handling for informing other scene objects if they are
being viewed. Although this section is based on the VieGen infrastructure, the
methods and algorithms could be ported to other VE development frameworks.
Calibration. Although the PoG calibration mapping is handled in the eye
tracking control module, the process of conducting a calibration run is controlled
by the SceneNode. The user is required to sequentially ﬁxate on known points
distributed about the virtual gaze plane. A state machine manages this process,
displaying the gaze plane in the VE and using a sphere to indicate where the user
should be looking. A green sphere is used to indicate reception of valid readings
from the mobile eye, and this is changed to red should they switch to invalid.
During the sampling process for each sphere position, the gaze plane changes
red to inform the user that they need to remain ﬁxated. Once the background
returns to grey the sampling will have ﬁnished, and the sphere will either move
to the next location or will remain in place should it need to be repeated.
Upon completion of all calibration points the system stores the set of calibration data. A simple graphing object within the VE was used to represent the
mappings for each of the X and Y coordinates. The left image of Fig. 3 shows

734

A. Haﬀegee and R. Barrow

Fig. 3. In-scene visualization of calibration data to validate results (left), & live gaze
selection in the virtual gallery, with multiple indicators for the viewed object (right)

a typical output from these graphs for the Mobile Eye. As can be seen, these
should display a linear mapping. The provision of graphs that can be viewed
within the VE enables in-scene indication of the reliability of the calibration
data. This simpliﬁes the process of identifying inaccuracies in either individual
calibration points or the complete set.
Gaze Position Markers, and Message Events. A simple way of indicating
the gazed object is by adding a marker to the scene at the gaze vector intersection
point, however this may not be the best approach. If there is any error from the
calibration the marker may be slightly oﬀset from the viewed position. Naturally
the users eyes will be drawn to the marker, resulting in a new position for the
marker, again slightly oﬀset. This repeats, resulting in a marker that appears to
wander the scene. The involuntary eye action can wrongly lead to assumptions
about the system stability. This can be resolved by using diﬀerent markers. One
such approach is to use a bounding box around the object being viewed. This
provides a more stable indication, but can lose the accuracy as to which part of
the object was being viewed. If the scene objects have been named, their textual
information can be displayed within the scene, but again this does not convey
the exact hit location. An alternative method is not to display the marker in the
user’s view of the scene, but to store a log of its position for separate display to
interested third parties.
Some VEs allow virtual objects to create and consume message events. For
VieGen, an event has been deﬁned to indicate that an object is being viewed,
and this is forwarded to the ﬁrst object intersected by the gaze vector. While
current reactions to this event are limited, future responses could include moving,
animating, or other methods of interaction between the object and the viewer.
Configuration. For this project, the gaze tracking SceneNode can be conﬁgured at run time. It uses XML to deﬁne the com port to be used for the serial

Eye Tracking and Gaze Based Interaction

735

connection between the analysis computer and the VR system, the oﬀset from
the head tracker to the users eye, the position and size of the virtual gaze plane,
and the number and locations of calibration points used in the PoG mapping.

4

Validation and Virtual Gallery Application

The practical experiences during the development of this technology have clearly
shown that the immersive eye tracking developed has been successful. The basic
functionality of the gaze tracking SceneNode enables a gaze marker to be displayed which indicates the location in the environment that the user is viewing.
To further prove the features of the technology an eye gaze speciﬁc application
was developed based around a virtual gallery. This VE consists of a display case
containing various artifacts that the user can view. It has been enhanced with
in-scene menus to start calibration, add or remove diﬀerent types of indication
markers, and to start/stop the logging to disk of the gaze vector interception
positions. The latter of these can be used for subsequent analysis and replay of
the user’s viewing patterns. The photo on the right of Fig. 3 shows this application in use. The user is able to select the diﬀerent objects solely by using their
eyes, with movements of their head and body having no detrimental eﬀect on
the selection process. In this example, a sphere is used as a gazed object marker
along with a wireframe bounding box, and these can be seen on the selected
rabbit. During the tests, the gaze positions were recorded and these could be
replayed within the scene to show the users viewing patterns.

5

Summary and Future Work

While no formal evaluation tests have yet been conducted with this research,
the initial results clearly demonstrate its feasibility. Once calibrated the system
will reliably follow the users eye position regardless of how they move both in
the virtual world and within the conﬁnes of the VR system.
However there is still scope for optimization of the algorithms used, particularly in the ﬁeld of calibration. The relationship between the PoG and the gaze
plane should be further studied to determine the nature of this relationship and
to investigate if can be represented mathematically rather than as a comparison
between calibration points. It would also be useful if the diﬀerent calibration
points could be compared to assess their reliability and accuracy. A further extension could involve modiﬁcations to the Mobile Eye analysis software which
would allow it to receive scene coordinates from the VR system. It this case the
complete system could be calibrated inside the VE, and this would do away with
the need for the intermediate gaze plane currently being used.
Additional extensions could also be developed to aid analysis of the captured
user data. In addition to walk-throughs of the VE that replay dynamic user head
and eye positioning, these could also include hot-spots indicating areas of particular interest. Analysts could explore and navigate these VEs, with superimposed
markers demonstrating the viewing behaviour. Indeed, multiple user input ﬁles

736

A. Haﬀegee and R. Barrow

could be combined to allow more quantitative analysis. These could be ﬁltered
as desired, and overlaid on the scene to show key areas of interest.
There is vast potential in the use of this technology, and many researchers
may beneﬁt from studying eye behaviour from within a fully controllable environment. Commercially this could include market research or safety analysis,
where attracting a user’s attention visually is important. Research could include
how eye movements are used as communication extensions. By extending the
technology as a control or navigation interface, it could also provide a natural
method of interaction. This could be especially useful for those with disabilities
that preclude them from otherwise controlling or participating within the VE.

References
1. Duchowski, A.T.: Eye Tracking Methodology: Theory and Practice. Springer, New
York (2007)
2. Cox, A.L., Cairns, P., Berthouze, N., Jennett, C.: The use of eyetracking for measuring immersion. In: CogSci 2006 Workshop: What have eye movements told us
so far, and what is next?, Vancouver, Canada (July 2006)
3. Tanriverdi, V., Jacob, R.J.K.: Interacting with eye movements in virtual environments. In: CHI, pp. 265–272 (2000)
4. Asai, K., Osawa, N., Takahashi, H., Sugimoto, Y.Y., Yamazaki, S., Samejima,
M., Tanimae, T.: Eye mark pointer in immersive projection display. In: VR 2000:
Proceedings of the IEEE Virtual Reality 2000 Conference, Washington, DC, USA,
p. 125. IEEE Computer Society, Los Alamitos (2000)
5. Garau, M., Slater, M., Vinayagamoorthy, V., Brogni, A., Steed, A., Sasse, M.A.:
The impact of avatar realism and eye gaze control on perceived quality of communication in a shared immersive virtual environment. In: CHI 2003: Proceedings
of the SIGCHI conference on Human factors in computing systems, pp. 529–536.
ACM, New York (2003)
6. Murray, N., Roberts, D., Steed, A., Sharkey, P., Dickerson, P., Rae, J.: An assessment of eye-gaze potential within immersive virtual environments. ACM Trans.
Multimedia Comput. Commun. Appl. 3(4), 1–17 (2007)
7. Cruz-Neira, C., Sandin, D.J., Defanti, T.A., Kenyon, R.V., Hart, J.C.: The CAVE:
Audio visual experience automatic virtual environment. Communications of the
ACM 35(6), 64–72 (1992)
8. Slater, M., Steed, A., Chrysanthou, Y.: Computer Graphics and Virtual Environments. Addison Wesley, Reading (2002)
9. Applied Science Laboratories: Operation Manual Mobile Eye (January 2007)
10. Haﬀegee, A.: VieGen: An Accessible Toolset for the Conﬁguration and Control of
Virtual Environments. Ph.D thesis, University of Reading (March 2008)

