Incremental Clustering Algorithm for Earth
Science Data Mining
Ranga Raju Vatsavai
Computational Sciences and Engineering Division
Oak Ridge National Laboratory, Oak Ridge, TN 37831, USA
vatsavairr@ornl.gov

Abstract. Remote sensing data plays a key role in understanding the
complex geographic phenomena. Clustering is a useful tool in discovering interesting patterns and structures within the multivariate geospatial
data. One of the key issues in clustering is the speciﬁcation of appropriate number of clusters, which is not obvious in many practical situations.
In this paper we provide an extension of G-means algorithm which automatically learns the number of clusters present in the data and avoids
over estimation of the number of clusters. Experimental evaluation on
simulated and remotely sensed image data shows the eﬀectiveness of
our algorithm.
Keywords: Clustering, EM, GMM, Remote Sensing, K-means, G-means.

1

Introduction

Remote sensing, which provides inexpensive, synoptic-scale data with multitemporal coverage, has proven to be very useful in land cover mapping, environmental monitoring, forest and crop inventory, urban studies, natural and man
made object recognition, etc. Thematic information extracted from remote sensing imagery is also useful in a variety spatio-temporal applications. For example,
land management organizations and the public have a need for more current regional land cover information to manage resources and monitor land use changes.
Likewise, intelligence agencies, such as, National Geospatial Intelligence Agency
(NGA), and Department of Homeland Security (DHS), utilizes pattern recognition and data mining techniques to classify both natural and man made objects
from large volumes of high resolution imagery.
Clustering algorithms play a key role in earth science data mining. They are
often used to analyze complex and large volumes of multivariate geospatial data,
such as, remotely sensed images, sensor measurements, ﬁeld observations, etc.,
as a ﬁrst step in gaining insights into the structure or natural groupings. Clustering is also used to in compression, exploratory analysis, and summarization
of the data. Cluster analysis is used in many other spatial and spatiotemporal application domains. Cluster analysis is routinely used in epidemiology for
ﬁnding unusual groups of health-related events. Cluster analysis is also used in
detection of crime hot spots.
G. Allen et al. (Eds.): ICCS 2009, Part II, LNCS 5545, pp. 375–384, 2009.
c Springer-Verlag Berlin Heidelberg 2009

376

R.R. Vatsavai

One of the key challenges in clustering is the speciﬁcation of the number
of clusters. Determining an optimal number of clusters manually is not feasible
given the complexity and volume of geospatial data sets. In this paper we provide
a simple extension of the G-means [3] algorithm that automatically discovers
the number of clusters. Experimental evaluation shows that our algorithm can
avoid the common problem of ending up with a large number of spurious clusters
by the G-means algorithm.

2

Related Work and Our Contributions

Clustering is a fertile research area with applications cutting across many
domains. A large number of clustering algorithms can be found in the literature [4,5]. These algorithms can be broadly categorized into: hierarchical, partitional, density-based, and grid-based methods. Partitional clustering algorithms,
especially, K-Means algorithm is very popular in several application domains,
including earth sciences. One of the key inputs to K-Means algorithm is the
speciﬁcation of K, the number of clusters. However, determining an optimal
number of clusters manually is not feasible given the complexity and volume of
geospatial data sets.
Considerable research has gone into ﬁnding the optimum number of clusters
directly from the data itself [2,6,7,8,13]. In [1,3] authors proposed a G-means
algorithm that automatically discovers the number of clusters. Basic idea behind G-means is simple. Initial number of clusters (k) determined by k-means
are incremented by splitting each cluster that doesn’t pass a statistical test. The
clustering process is repeated until all the clusters have passed this statistical
test. In many practical situations there is a danger of over estimating the number of clusters, especially if the model is assumed to be a Gaussian Mixture
Model (GMM). We extended the G-means algorithm to overcome this practical
limitation. In many situations G-means clustering algorithm tends to ﬁnd more
clusters. In order to reduce the chance of ﬁnding more clusters, we devised a
new approach that prevents some splits and allows to reverse the splits. In a
nutshell we made two modiﬁcations to the G-means algorithm. First, instead of
univariate test statistic, we used a multivariate test statistic, known as, ShapiroWilk statistic. This modiﬁcation has following advantages. First we don’t have
to project multivariate data into 1-d. This is important for earth science data
mining as the geospatial data is often high-dimensional in nature. Finding a
good projection can be as diﬃcult as ﬁnding a good K. Second, AD test is good
for small samples, that is, number of samples ≤ 25. However, in earth science
data sets typically we have large number of samples (per cluster). Finally, the
multivariate Shapiro-Wilk test exhibits good power against alternatives [10].
We used KL Divergence measure after splitting the clusters to see if any pair
of clusters are too close to each other. If any two clusters are too close to each
other, then it is better to combine them, even though such combination may
violate signiﬁcance testing. In the following sections, we present our algorithm
and experimental results.

Incremental Clustering Algorithm for Earth Science Data Mining

3

377

Clustering Framework

Basic statistical framework for our clustering approach is Gaussian Mixture Models (GMMs). Typically model based clustering approaches are not applied on
entire data set given the computational and data complexity. Rather a subset of
data samples are collected from the full data set. Model parameters are estimated
using these samples. Once a model is constructed, all the samples (data points)
in the full data set can then be assigned to one of the clusters, based on some
distance (or decision) criteria. Our algorithm is based on the assumption that
the data samples are generated by a GMM. Then the objective is to learn the
GMM parameters from these samples. We now brieﬂy describe an expectation
maximization based algorithm to learn the GMM parameters.
3.1

Estimating GMM Parameters

Let us now assume that the sample data set D = {xi }ni=1 is generated by the
following mixture density.
K

p(xi |Θ) =

αj pj (xi |θj )

(1)

j=1

Here pj (xi |θj ) is the pdf corresponding to the mixture j and parameterized by
θj , and Θ = (α1 , . . . , θK , θ1 , . . . , θK ) denotes all unknown parameters associated
with the K-component mixture density. For a multivariate normal distribution
(eq. 2), θj consists of elements of the mean vectors μj and the distinct components of the covariance matrix Σj .
−1
t
−1
1
e 2 (x−μj ) |Σj | (x−μj )
−N
(2π) |Σj |

p(x|yj ) =

(2)

The log-likelihood function for this mixture density can be deﬁned as:
⎡
⎤
n

L(Θ) =

ln ⎣

i=1

M

αj pj (xi |θj )⎦ .

(3)

j=1

In general, Equation 3 is diﬃcult to optimize because it contains the ln of a sum
term. However, this equation greatly simpliﬁes in the presence of unobserved (or
incomplete) samples. Typically, we assume that the cluster labels as missing
(unobserved) data, and use expectation maximization technique to estimate parameters (Θ). The EM algorithm consists of two steps, called the E-step and
and M-step as given below.
E-Step. For multivariate normal distribution, the expectation E[.], which is
denoted by pij , is the probability that Gaussian mixture j generated the data
point i, and is given by:
pij =

ˆj
Σ
M
l=1

−1/2

ˆl
Σ

e{− 2 (xi −ˆμj )

−1/2

1

t

ˆ −1 (xi −ˆ
Σ
μj )}
j

1
t ˆ −1
e{− 2 (xi −ˆμl ) Σl (xi −ˆμl )}

(4)

378

R.R. Vatsavai

Table 1. Algorithm for Computing Parameter of Finite Gaussian Mixture Model Over
Unlabeled Training Data
Inputs: D, sample data set; K, the number of clusters.
Initial Estimates: Do clustering by K-Means, and estimate initial parameter
ˆ
using Maximum Likelihood Estimation (MLE) technique to ﬁnd θ.
Loop: While the complete data log-likelihood improves:
E-step: Use current classiﬁer to estimate the class membership of each
unlabeled sample, i.e., the probability that each Gaussian mixture component
generated the given sample point, pij (see Equation 4).
ˆ given the estimated GausM-step: Re-estimate the parameter, θ,
sian mixture component membership of each unlabeled sample (see Equations
5, 6, 7)
Output: Parameter vector Θ.

M-Step. The new estimates (at the k th iteration) of the model parameters in
terms of the old parameters are computed using the following update equations:
α
ˆ kj =
μ
ˆkj =
ˆk =
Σ
j

n
i=1

1
n

n

pij

(5)

i=1

n
i=1 xi pij
n
i=1 pij

(6)

pij (xi − μ
ˆkj )(xi − μ
ˆkj )t
n
i=1 pij

(7)

The EM algorithm iterates over these two steps until convergence is reached. We
can now put together these individual pieces into the following algorithm (Table 1)
which computes the parameters for each component in the ﬁnite Gaussian mixture
model that generated our sample data D (without any cluster labels).
3.2

Simulation Example 1

We now demonstrate GMM clustering algorithm ( 1) on a simulated data set.
We generated a GMM with three components. The parameters are given in
Table 2 and Table 3. We generated 150 bivariate Gaussian samples from each
Table 2. Simulation Parameters (Mean)

x
y
C1 55.00 25.00
C2 80.00 50.00
C3 50.00 40.00

Table
3.
(Covariance)

Simulation

Parameters

C1
C2
C3
x
y
x
y
x
y
x 30.00 25.00 60.00 40.00 60.00 50.00
y 25.00 40.00 40.00 90.00 50.00 70.00

Incremental Clustering Algorithm for Earth Science Data Mining

379

component density. We applied GMM clustering algorithm on this sample data
set by assuming diﬀerent K values and the results were summarized in Figure 1.
From the ﬁgure it can be seen that when K assumption is correct (that is, K=3)
we have very good estimates (compare subﬁgures (a) and (c)), however for other
K’s (subﬁgures (b) and (d)) the estimates are very diﬀerent than the original
distribution. This simulation emphasizes the need to estimate a good K value
(if possible automatically from the data).
Cluster Distributions

● ●
●
●●
● ●
●
●
● ●
●
●
●
●
●● ●● ●●
● ●●
●● ● ● ● ●
●● ●
●
●
●
● ● ●
●
●
●
●● ●
●
●●●●●
●
●● ● ●
●● ●
● ●
●
●
● ●● ● ●
● ● ●
●● ● ● ●
●
●●●●
● ●
●●
●
● ●
●
●
●● ●
●
●
●●
●●
●
●
● ●● ●●
●
●
●
●
● ● ●
●
●
●
●●
● ●●●●●
● ●
● ●
●
● ●●
● ●● ●● ● ● ● ● ●
●
● ●
●● ●●
●
●
●● ● ●
●● ●●
● ●●●
● ● ●●
●
●●●
●
●●
●●● ●●
●
●● ●●● ●●
●
● ●
●
●
●● ●●● ● ● ● ●
● ●●
●
● ●●
● ●
● ●
●
● ●●●
●
●●●● ●
●
● ●
●●
●
● ●●
● ●
●●
● ●● ●
●
● ● ●●
●
●
● ●
●
●●
● ●●●
●● ●
●
● ●●●
●●
● ●
●● ●
●●
●●●
●
●
●
●
●
●
●
●
●
●
● ●
●
●● ●
●●
●●
● ●
● ●
●
● ●●●
● ●
●
●
●
●
●●●●●●
● ●●
●
● ●
●
●●
● ●●
●● ● ● ●
●●●
●
●●●●●●
● ●
● ●●
●● ●
●●
●
●
●●●●●
●●●
●●●
●
●● ●●
●
●
● ●●● ●●
●●
● ●●
●
●●●● ●●●●●●●●
●●
●● ●
●
● ● ●●
●
●●
●●●
●● ●
●●
●
● ●
●● ● ● ● ●
●
● ●
●
●
●
●
●
●

60

60

2

100

80
40

4

2

20

40

60

80

100

X1
GMM Clustering

0

20

20
0

0

80

1

1

20

20
0

60
X1
Original Cluster Distributions

X2

X2

40

X2

1

1

40

3

2

3

40

60
40

3

●

60

●

2

X2

●
●

●

● ●
●
●●
● ●
●
●
● ●
●
●
●
●
●● ●● ●●
● ●●
●● ● ● ● ●
●● ●
●
●
●
● ● ●
●
●
●
●● ●
●
●●●●●
●
●● ● ●
●● ●
● ●
●
●
● ●● ● ●
● ● ●
●● ● ● ●
●
●●●●
● ●
●●
●
● ●
●
●
●● ●
●
●
●●
●●
●
●
● ●● ●●
●
●
●
●
● ● ●
●
●
●
●●
● ●●●●●
● ●
● ●
●
● ●●
● ●● ●● ● ● ● ● ●
●
● ●
●● ●●
●
●
●● ● ●
●● ●●
● ●●●
● ● ●●
●
●●●
●
●●
●●● ●●
●
●● ●●● ●●
●
● ●
●
●
●● ●●● ● ● ● ●
● ●●
●
● ●●
● ●
● ●
●
● ●●●
●
●●●● ●
●
● ●
●●
●
● ●●
● ●
●●
● ●● ●
●
● ● ●●
●
●
● ●
●
●●
● ●●●
●● ●
●
● ●●●
●●
● ●
●● ●
●●
●●●
●
●
●
●
●
●
●
●
●
●
● ●
●
●● ●
●●
●●
● ●
● ●
●
● ●●●
● ●
●
●
●
●
●●●●●●
● ●●
●
● ●
●
●●
● ●●
●● ● ● ●
●●●
●
●●●●●●
● ●
● ●●
●● ●
●●
●
●
●●●●●
●●●
●●●
●
●● ●●
●
●
● ●●● ●●
●●
● ●●
●
●●●● ●●●●●●●●
●●
●● ●
●
● ● ●●
●
●●
●●●
●● ●
●●
●
● ●
●● ● ● ● ●
●
● ●
●
●
●
●
●
●

20

Cluster Distributions

80

●
●
●

Cluster Distributions

80

80

Cluster Distributions

20

40

60

80

100

X1
Clusters Found by Model Selection (BIC)

20

40

60

80

100

X1
GMM-Clustering(K=4)

(a) Simulated (K=3) (b) Estimated (K=2) (c) Estimated (K=3) (d) Estimated (K=4)

Fig. 1. Simulated vs. Estimated (GMM-Clustering for diﬀerent K values)

4

Learning to Estimate K

In this section we address the problem of estimating K automatically from the
data. As with the estimation of the model parameters for ﬁnite Gaussian mixture
model, we assume that the training dataset D is generated by a ﬁnite Gaussian
mixture model, but we don’t know either the number of components or the labels
for any of the mixture component. In the previous section, we devised an algorithm to ﬁnd parameters by assuming a K-component ﬁnite Gaussian mixture
model. In general, we can estimate parameters for any arbitrary K-component
model, as long as there are suﬃcient number of samples available for each component and the covariance matrix does not become singular. Then the question
remains, which K-component model is better? This question is addressed in the
area of model selection, where the objective is to chose a model that maximizes
a cost function. There are several cost functions available in the literature, most
commonly used measures are Akaike’s information criterion (AIC), Bayesian information criteria (BIC), and minimum description length (MDL). The common
criteria behind these models is to penalize the models with additional parameters,
so BIC and AIC based model selection criteria follows the principal of parsimony.
In this study we considered BIC as a model selection criteria, which also takes the
same form as MDL. We also chose BIC, as it found to be very useful in model based
clustering [2], and also because it is deﬁned in terms of maximized log-likelihood
which any way we are computing in our parameter estimation procedure deﬁned
in the previous section. BIC can be deﬁned as
BIC = M DL = −2 log L(Θ) + m log(N )

(8)

where N is the number of samples and m is the number of parameters. We
now describe our BIC based model selection criteria to determine the number

380

R.R. Vatsavai

components in each aggregate class. First, we take the aggregate class and split
it into two Gaussians at a time using the Gaussian splitting criteria speciﬁed
in [11]. Then the parameters of this new mixture model are estimated using the
algorithm 1. This process is recursively applied for a ﬁxed number times or BIC
is minimized.
On the other hand, G-means [3] clustering is initialized by k-means clustering for suitable initial K value. Each cluster is then tested for normality using
univariate test: Anderson-Darling (AD) statistic. For a user given p-value, if AD
test fails, then the cluster is split into two clusters. K-means cluster is performed
again with new K, and the process is repeated until no more (new splits) clusters can be found. Multivariate data is projected on to 1-d to facilitate AD test.
In [3], authors argued that BIC has a tendency to ﬁnd more clusters. In our
experiments, we found that G-means clustering also tend to ﬁnd more clusters.
We demonstrate this through an example simulated data set. In order to reduce
the chance of ﬁnding more clusters, we devised a new approach that prevents
splits and allows to reverse the splits.
Our algorithm diﬀers from G-means in two ways. First, instead of univariate test statistic, we used a multivariate test statistic, known as, Shapiro-Wilk
statistic. More details on Shapiro-Wilk test can be found in [12]. This modiﬁcation has following advantages. First we don’t have to project multivariate
data into 1-d. This is important for earth science data mining as the geospatial data is often high-dimensional in nature. Finding a good projection can be
as diﬃcult as ﬁnding a good K. Second, AD test is good for small samples,
that is, number of samples ≤ 25. Finally, the multivariate Shapiro-Wilk test
exhibits good power against alternatives [10]. Finally, statistical tests are sensitive to noise. It is likely that splitting process (increasing K) continue beyond
optimal K as many times statistical signiﬁcance test fails (even though clusters are close to multivariate normal). As a check to prevent this happening,
we added additional criteria to check for the quality of splits. We used KL Divergence [9] measure after splitting to see if any pair of clusters are too close
to each other. If any two clusters are too close to each other, then it is better to combine them. The new algorithm (GMM-Adaptive-K) is summarized in
Table 4.
4.1

Simulation Example 2

We now demonstrate GMM-Adaptive-K clustering algorithm (4) on the simulated data set (Table 3). The results were summarized in Figure 2. First iteration
found two clusters (Figure 2(b)), red cluster passes Shapiro-Wilk test. As result,
only the 2nd cluster (black) is split into two clusters (c). In the next iteration,
red cluster failed Shapiro-Wilk test, as a result it was split into two clusters (d).
The G-means cluster algorithm would have resulted in a ﬁnal solution shown in
Figure 2(e). On the other-hand the additional step introduced in our algorithm,
ﬁnds that these two clusters are very close (KL-Divergence), thus decrements
number of clusters to 3. Final solutions is shown in Figure 2(f). Compare Figure 2(f) with original distribution in Figure 2(a).

Incremental Clustering Algorithm for Earth Science Data Mining

80

Cluster Distributions

80

Cluster Distributions
●
●
●

●

●
●●
● ●
●
●
● ●
●
●
●
●● ●
● ●●
● ●●
●● ● ● ● ●
●● ●
●
●
●
●
● ● ●
●
●
●
●
●
●
●●
●
●●
●
●● ● ●
●● ●
● ●
●
●
●
● ●● ●
● ● ● ●
●● ● ● ●
●
●
● ●
●●
●●
●
● ●
●
●● ●
●
●●
●
●●
●
● ● ● ●●
●●
●●
●
●
● ● ●
●
●●
●
●
● ●●
●
●● ● ●
● ●
● ●
●
● ●●●●● ● ● ● ● ●
●
●
●
●
●● ●●
●
●
●
●
●● ●
●● ●●
● ●
● ● ●●
●
●
●●●
●
●
●●● ●●
●
●● ●●● ●●
●
● ●
●
●
●● ●●● ● ● ●
●
● ●●
●
● ● ●
● ●
●
●
●
●
● ●●●
●
●●●
●
●
● ●
●●
●
● ●●
●
●●
● ●● ●
●
● ● ●●
●
●
●
● ●
●●
● ●●
●●
●
● ●●●
●●
●
● ●
●
●
●
●
●
●
●● ●
●●●●●
●
● ●
●
●
●● ●
●
●● ●
●●
●●
●
● ●
●
●
● ●
●
●
●●●●●●●●●
●
●
●
●
●
● ●
●
● ●●
●●
● ●●
● ●●
●● ●● ● ●
●●●
●
●
●
●
●
●
●
●
●● ●●
●
●●
●
●●
●●
●● ●
● ●●●
●● ●●
●
●
●
●●●
●
● ●●
●
●●
●●
● ● ●●
●●●● ●●●
●
●● ●
●●
● ● ●●
●
●●●
●● ●
●●
●
● ●
●● ● ● ● ●
●
● ●
●
●
●
●
●
●

●
●●
● ●
●
●
● ●
●
●
●
●● ●
● ●●
● ●●
●● ● ● ● ●
●● ●
●
●
●
●
● ● ●
●
●
●
●
●
●
●●
●
●●
●
●● ● ●
●● ●
● ●
●
●
●
● ●● ●
● ● ●
●
●
●● ● ● ●
●
●
●
● ●
●●
●
●●
● ●
●
●
●●
●
●●
●
●
● ● ● ●●
●●
●●
●
●
● ● ●
●
●●
●
●
● ●●
●
●● ● ●
● ●
● ●
●
● ●●●●● ● ● ● ● ●
●
●
●
●
●● ●●
●
●
●
●
●● ●
●● ●●
● ●
● ● ●●
●
●
●●●
●
●
●●● ●●
●
●● ●●● ●●
●
● ●
●
●
●● ●●● ●
● ●● ● ●● ●
●
● ● ●
● ●
●
●
● ●●●
●
●●●
●
●
● ●
●●
●
● ●●
●
●●
● ●● ●
●
● ● ●●
●
●
●
● ●
●●
● ●●
●●
●
● ●●●
●●
●
● ●
●
●
●
●
●
●
●
●●
●● ●
●
● ●
●
●
●● ●● ● ●●
●
●● ●
●
●●
●
● ●
●
●
● ●
●
●
●●●●●●●●●
●
●
●
●
●
● ●
●
● ●●
●●
● ●●
● ●●
●● ●● ● ●
●●●
●
●
●
●
●
●
●
●
●● ●●●
●
●●
●
●●
● ●● ● ● ●
●
●● ●●
●
●
●
●●●
●
● ●●
●
●●
●●
● ● ●●
●●●● ●●●
●
●● ●
●●
● ● ●●
●
●●●
●● ●
●●
●
● ●
●● ● ● ● ●
●
● ●
●
●
●
●
●
●

60

●

2

40

X2

40

X2

2

3

1

20
0

0

20

1

20

40

60

80

100

20

40

60

X1
Original Cluster Distributions

80

100

X1
Iteration 1, Split 1

(a) Original Distribution

(b) Iteration 1, Split 1

60

60

80

Cluster Distributions

80

Cluster Distributions

●
●
● ●
●
● ●● ●
● ●
●
●
●●
● ● ● ●●
●●
●
● ●●
●
●● ● ●
● ●●●●● ● ● ● ● ●
●
●
●● ●
●● ●●
●●● ●●
●●
● ●● ●●
●
●
●
●
●
●
●
●
●
●
●● ●●● ●
●
●
●
●
●
●
●
● ●
●
●
●
● ●●●
●●●● ●
●●
●
● ●●
●
● ●●
●
● ● ●●
●
●
●
● ●
●●
● ●●
●●
● ●●●
●
●
● ●
●
●
●
●
●
●
●
●
●● ●
●
●
●● ●● ● ●●
●
●● ●
●
●
●
●
●
● ●
●
●
●
●●●●●●●●●
●
●
●
●
●
● ●
●
● ●●
●●
●
● ●●
●● ● ● ●
●●●
●
●
●● ●
●
●● ●
●●
● ●●●●● ●
●
●●
●
●●
● ●● ● ● ●
●● ●●
●
●
●
●●●
●
● ●●
●
●●
●●
● ● ●●
●●●● ●●●
●●
●● ●
● ● ●●
● ●
●
●●●
●●
●
●
●
●
●
●
●
●
●
●
●
●
● ●
●
●
●
●
●
●

●
●
● ●
●
● ●● ●
● ●
●
●
●●
● ●● ●●
●●●●●● ●
●
● ●
●
●●
● ●●●●● ● ● ● ● ●
●
●● ●
●● ●●
●●● ●●
●●
● ●● ●●
●
●
●
●
●
●
●
●
●● ●●● ●
●
●
●
●
●
●
●
● ●
●
●
● ●●●
●●●● ●
●●
●
● ●●
●
●
● ● ●●
●
●●
● ●●
●●
●
●
●
●
●
●●
●● ●● ● ●
●
●
● ●
●
●
●●
●
●
●
●
●
●● ●
●
●
●

1

40

X2

40

2

2

1

0

0

20

1

20

1

●

●

X2

●

●

●

●

60

●
●

●

●

●

20

40

60

80

100

20

40

X1
Iteration 2 , Split 1

80

80

●
●

●

●

●
●●
● ●
●
●
● ●
●
●
●
●● ●
● ●●
● ●●
●● ● ● ● ●
●● ●
●
●
● ●●
● ● ●
●
●
●
●
●
●
●
●
● ●
●
●● ● ●
●● ●
● ●
●
●
●
● ●● ●
● ● ●
●
●
●● ● ● ●
●
●
●
● ●
●●
●
●●
● ●
●
●
●●
●
●●
●
●
● ●● ●●
●●
●
●●
●
● ● ●
●
●
●●●●●● ●
●
● ●
●
● ●
● ●
●
●●
● ●●●●● ● ● ● ● ●
●
●
●● ●●
●● ●
●
●
●● ●
●● ●●
●
●
●
●
●
●
●
●
●●●
●
●
●●● ●●
●
●● ●●● ●●
●
● ●
●
●
●● ●●● ●
● ●● ● ●● ●
●
● ● ●
● ●
●
● ●●●
●
●●●● ●
●
● ●
●●
●
● ●●
●
●●
● ●● ●
●
●
● ● ●●
●
●
●
● ●
●●
● ●●
●●
●
● ●●●
●●
●
● ●
●
●
●
●
●
●
●● ●
●●●●●
●
● ●
●
●
●● ●
●
●● ●
●●
●●
●
● ●
●
●
● ●●●
● ●
●
●
●
●
●
●
●
●
●
●
●
●
● ● ●●
●
●
●
●●
● ●●
●● ● ● ●
●●●
●
●
●● ●
● ●
●
●● ●
●
●
●● ●●
●
●●
●
●●
● ●● ● ● ●
●
●● ●●
●
●
●
●●●
●
● ●●
●
●●
●●
● ● ●●
●●●● ●●●
●
●● ●
●●
● ● ●●
●
●●●
●● ●
●●
●
● ●
●● ● ● ● ●
●
● ●
●
●
●
●
●
●

●

1

1

40

X2

3

2

0

20

2

0

●

●
●●
● ●
●
●
● ●
●
●
●
●● ●
● ●●
● ●●
●● ● ● ● ●
●● ●
●
●
● ●●
● ● ●
●
●
●
●
●
●
●
●
● ●
●
●● ● ●
●● ●
● ●
●
●
●
● ●● ●
● ● ●
●
●
●● ● ● ●
●
●
●
● ●
●●
●
●●
● ●
●
●
●●
●
●●
●
●
● ●● ●●
●●
●
●●
●
● ● ●
●
●●
●
●
● ●●
●
●● ● ●
● ●
● ●
●
● ●●●●● ● ● ● ● ●
●
●
●
●● ●●
●● ●
●
●
●● ●
●● ●●
●
●
●
●
●
●
●
●
●●●
●
●
●●● ●●
●
●● ●●● ●●
●
● ●
●
●
●● ●●● ●
●
●
● ●●
●
●
● ● ●
● ●
●
●
●
● ●●●
●
●●●● ●
●
● ●
●●
●
● ●●
●
●●
● ●● ●
●
● ● ●●
●
●
●
● ●
●●
● ●●
●●
●
● ●●●
●●
●
● ●
●
●
●
●
●
●
●● ●
●●●●●
●
● ●
●
●
●● ●
●
●● ●
●●
●●
●
● ●
●
●
● ●●●
● ●
●
●
●
●
●
●
●
●
●
●
●
●
● ● ●●
●
●
●
●●
● ●●
●● ● ● ●
●●●
●
●
●● ●
● ●
●
●● ●
●
●
●● ●●
●
●●
●
●●
● ●● ● ● ●
●
●● ●●
●
●
●
●●●
●
● ●●
●
●●
●●
● ● ●●
●●●● ●●●
●
●● ●
●●
● ● ●●
●
●●●
●● ●
●●
●
● ●
●● ● ● ● ●
●
● ●
●
●
●
●
●
●

3

4

●

●

60

60

●
●

●

●

40

100

Cluster Distributions
●

20

80

(d) Iteration 3, Split 1

Cluster Distributions

●

60
X1
Iteration 3 , Split 1

(c) Iteration 2, Split 1

X2

381

20

40

60

80

X1
Final Cluters

(e) Without Merge Step

100

20

40

60

80

X1
Final Cluters (After Merging)

(f) With Merge Step

Fig. 2. GMM-Apdaptive-K Algorithm Trace

100

382

R.R. Vatsavai
Table 4. GMM-Adaptive-K Algorithm

Inputs: D, sample data set; signiﬁcance (default p-value = 0.05), initial K
(default = 2), nClusters = K
Clustering: GMM-Clustering (see Algorithm 1)
Loop 1: WHILE (TRUE):
Loop 2: FOR 1:nClusters
Statistical test: Shapiro-Wilk test.
Check: IF a cluster fails statistical test, split that cluster into
two clusters using GMM-Clustering; increment nClusters and K; ELSE accept
cluster, decrement nClusters
Clustering: GMM-Clustering(failed-cluster-data-samples, new K)
Merge: Compute KL-Divergence, IF two-clusters are closer than threshold value, decrement K, continue (Loop 2)
Check: IF nClusters = 0 (break, Loop 1)
Output: Parameter vector Θ.

5

Experimental Results

We have applied our GMM-Adaptive-K algorithm on the real data set described
below.The Cloquet study site encompasses Carlton County, Minnesota, which is
approximately 20 miles southwest of Duluth, Minnesota. The region is predominantly forested, composed mostly of upland hardwoods and lowland conifers.
There is a scattering of agriculture throughout. The topography is relatively
ﬂat, with the exception of the eastern portion of the county containing the St.
Louis River. Wetlands, both forested and non-forested, are common throughout
the area. The largest city in the area is Cloquet, a town of about 10,000. We
used a spring Landsat 7 scene, taken May 31, 2000, and clipped to the study
region. The ﬁnal rectiﬁed and clipped image size is 1343 lines x 2019 columns x

0

50

b4

100

150

Cluster Distributions

20

40

60

80

100

b2
GMM-Adaptive-K Clustering, Final K=8

(a) Prob. Distributions of Clusters

(b) Clustered Image Subset

Fig. 3. GMM-Apdaptive-K Algorithm on Carleton Satellite Image Data

Incremental Clustering Algorithm for Earth Science Data Mining

383

6 bands. We selected 400 random plots. From each plot, we extracted 9 feature
vectors (6 dimensional) by placing a 3 x 3 window at the center of each plot.
That is, sample data set consisted of of 3600 feature vectors. We applied our
GMM-Adaptive-K algorithm on this sample data set and found 8 clusters. In a
supervised classiﬁcation experiment, remote sensing analysts have identiﬁed 10
classes for this study site. Supervised classiﬁcation image was visually compared
with our clustering algorithm. It appears a good correspondence between the
clusters and thematic classes identiﬁed by the analyst. However, further analysis and experimentation is needed to establish this correspondence between the
clusters and thematic (information) clusters. Figure 3(a) shows the cluster (bivariate density) distributions in feature space (bands 2 and 4), and Figure 3(b)
shows a small clip from the clustered image.

6

Conclusions

We developed an incremental clustering algorithm. The algorithm is based on
GMM distribution and expectation maximization (EM) parameter estimation.
The algorithm is also an extension of G-means algorithm, which splits clusters
failing statistical signiﬁcance tests, in a iterative manner to ﬁnd optimal number of clusters. However, our algorithm avoids an important limitation of over
estimation of K by employing KL divergence measure to ﬁnd highly overlapping
clusters and try to avoid them from further splitting. Experimental evaluation
on simulated data shows that our algorithm produces parameters which are very
close to the original distribution. Clustering on a real data set shows a good correspondence between the clusters and thematic (information) classes chosen by
the remote sensing analyst in a supervised classiﬁcation project. Further analysis
and experimentation is needed to understand the performance and utility of this
algorithm in earth science data mining applications.

Acknowledgments
We would like to thank our former collaborators Thomas E. Burk, Jamie Smedsmo,
Ryan Kirk and Tim Mack at the University of Minnesota for useful comments
and inputs into this research. The comments of Eddie Bright, Phil Coleman, and
Veeraraghavan Vijayraj, have greatly improved the technical accuracy and readability of this paper. Prepared by Oak Ridge National Laboratory, P.O. Box 2008,
Oak Ridge, Tennessee 37831-6285, managed by UT-Battelle, LLC for the U. S.
Department of Energy under contract no. DEAC05-00OR22725.

References
1. Feng, Y., Hamerly, G.: Pg-means: learning the number of clusters in data. In:
Advances in Neural Information Processing Systems 19, pp. 393–400. MIT Press,
Cambridge (2007)
2. Fraley, C., Raftery, A., Wehrens, R.: Incremental model-based clustering for large
datasets with small clusters. Journal of Computational and Graphical Statistics 14
(2005)

384

R.R. Vatsavai

3. Hamerly, G., Elkan, C.: Learning the k in k-means. In: Neural Information Processing Systems. MIT Press, Cambridge (2003)
4. Jain, A.K., Dubes, R.C.: Algorithms for clustering data. Prentice-Hall, Inc., Upper
Saddle River (1988)
5. Jain, A.K., Murty, M.N., Flynn, P.J.: Data clustering: a review. ACM Comput.
Surv. 31(3), 264–323 (1999)
6. Mclachlan, G.J., Peel, D.: On a resampling approach to choosing the number of
components in normal mixture models. In: Proceedings of Interface 96, 28th Symposium on the Interface, pp. 260–266 (1997)
7. Carreira-Perpi, M.A.: Mode-ﬁnding for mixtures of gaussian distributions. IEEE
Trans. Pattern Anal. Mach. Intell. 22(11), 1318–1323 (2000)
8. Pelleg, D., Moore, A.W.: X-means: Extending k-means with eﬃcient estimation of
the number of clusters. In: ICML 2000: Proceedings of the Seventeenth International Conference on Machine Learning, pp. 727–734. Morgan Kaufmann Publishers Inc, San Francisco (2000)
9. Penny, W.: Kullback-liebler divergences of normal, gamma, dirichlet and wishart
densities (2001)
10. Rivas, M.: An exposition on tests for multivariate normality (2007)
11. Sankar, A.: Experiments with a gaussian merging-splitting algorithm for hmm
training for speech recognition. In: Proceedings of the Broadcast News Transcription and Understanding Workshop, pp. 99–104 (1998)
12. Shapiro, S.S., Wilk, M.B.: An analysis of variance test for normality (complete
samples). Biometrika 3(52) (1965)
13. Tibshirani, R., Walther, G., Hastie, T.: Estimating the number of clusters in a
data set via the gap statistic. Journal of the Royal Statistical Society: Series B
(Statistical Methodology) 63(2), 411–423 (2001)

