Kernel Based Regularized Multiple Criteria Linear
Programming Model
Yuehua Zhang1,*, Peng Zhang1, and Yong Shi1,2
1

CAS Research Center on Fictitious Economy and Data Sciences,
Beijing 100080, China
2
College of Information Science & Technology,
University of Nebraska at Omaha,
Omaha, NE 68182, USA
{zhangyuehua07,zhangpeng04}@mails.gucas.ac.cn
yshi@omaha.edu

Abstract. Although Regularized Multiple Criteria Linear Programming
(RMCLP) model has shown its effectiveness in classification problems, its inherent drawback of linear formulation limits itself into only solving linear classification problems. To extend RMCLP into solving non-linear problems, in
this paper, we propose a kernel based RMCLP model by using a form
w =

N

∑

i =1

β iφ ( x i )

to replace the original weight w in RMCLP model. Empirical

studies on synthetic and real-life datasets demonstrate that our new model is capable to classify non-linear datasets. Moreover, comparisons to SVM and
MCQP also exhibit the fact that our new model is superior to other non-linear
models in classification problems.
Keywords: MCLP, RMCLP, classification, kernel function.

1 Introduction
Data mining is defined as "The nontrivial extraction of implicit, previously unknown,
and potentially useful information from data"[1]. Traditionally, data mining uses
machine learning, statistical and visualization techniques to discover and present
knowledge in a form which is easily comprehensible to humans. From the aspect of
methodology, data mining can be performed through association, classification, clustering, prediction, sequential patterns, and similar time sequences. [2].Classification is
one of the most important parts of data mining. Classification generally includes three
steps: the first step is to build a model by using a given data which is predetermined.
The next step is to test the model. The last step is to use the model to predict unlabeled data if the model accuracy is high enough.
Recent years have witnessed a large body of research work on mining useful
knowledge by multiple criteria mathematical programming method where various of
classification models have been proposed and received great success in business intelligence [1]. All these models are created mainly by adapting the objective functions of
the original multiple criteria linear programming (MCLP) model [4] to improve
*

Corresponding author.

G. Allen et al. (Eds.): ICCS 2009, Part II, LNCS 5545, pp. 625–632, 2009.
© Springer-Verlag Berlin Heidelberg 2009

626

Y. Zhang, P. Zhang, and Y. Shi

MCLP’s accuracy and stability, and lots of research works have exhibited their powerful ability to classify different kinds of real-life data. Among all these models, the
most recent Regularized Multiple Criteria Linear Programming (RMCLP) model [3]
which is created by adding two regularized objective functions into the original
MCLP model, has been theoretically demonstrated that it is stable in finding the
global optimal solution. The experiment results show that RMCLP model is an effective classification model.
However, RMCLP is inadequate to classify non-linear data sets. To overcome this
shortage, in this paper, we add kernel function into the original RMCLP model to
enable it to solve non-linear classification problems.
The remained of this paper is organized as follows. In section 2, we introduce the
kernel algorithm. In section 3, we give a short review of MCLP and RMCLP model.
In section 4, we formulate our new kernel based RMCLP model. In Section 5, we
perform experiments on a synthetic dataset. In Section 6, we use two real-life data
sets to compare our model with two other non-linear models: MCQP and SVM. In the
last section, we conclude our paper with discussions.

2 Kernel Algorithms
Linear classification problem is the most basic situation in the entire classification
problems. To classify the non-linear problems, we need to construct non-linear mapping functions. For example, SVM uses a non-linear map ϕ to map a input vector x
to a higher dimensional space Z ( which is also called as the feature space), then it
constructs Optimal Hyper plane in the feature space Z. Assuming the non-linear map
is ϕ : R

→ Z , we transform the vector x in the original space to a new vector
z = φ (x) in the feature space Z. Considering the input vector is x = ( x1 , x 2 ,..., x d )
with weights wi = α i yi , we can get the SVM discrimination function as follows:
d

N

y = sgn(∑ α i yi K ( xi , x) + b) .

(1)

i =1

The parameter

K ( xi , x) is the non-linear transformation of the i-th support vector.

Figure 1 shows the framework of SVM in the sense of neural networks:

Fig. 1. The framework of SVM

Kernel Based Regularized Multiple Criteria Linear Programming Model

627

In the discrimination function:
N

y = sgn(∑ α i y i K ( xi , x) + b)
i =1
N

= sgn(∑ β i ⋅ φ ( xi ) ⋅ φ ( x) + b)
i =1

(2)

= sgn( w ⋅ φ ( x) + b)
N

Then we set w=

∑ β φ ( x ) where φ denotes a higher dimensional feature map asi

i

i =1

sociated with the nonlinear kernel and β i denotes the variables. By the Representer
theorem (Scholkopf and Smola, 2002), we indeed know that the optimal solution has
the following form:[6]
K ( xi , x) = 〈φ ( xi ),φ ( x)〉
(3)
The most common kernel functions are listed as follows [8]:
(1) Radial Basis Kernel Function:

K ( xi , x) = exp(−c x − xi
where c is a constant,

σ

2

σ 2) ,

(4)

is variance;

(2) Polynomial Kernel Function:

K ( xi , x) = (( x ⋅ xi ) + c) d ,
where d is a positive real number, c ≥ 0 ;

(5)

(3) Sigmoid Kernel Function:

K ( xi , x) = tanh(b( x ⋅ x1 ) + c) ,

(6)

Where b>0, c<0.

3 MCLP and RMCLP Models
3.1 The Formulation of MCLP Model
Suppose each evaluated target x i is described by n attributes (or variables). Consider

l targets where data observation of the i-th target is xi = ( xi1 ,..., xin )T , for i = 1...l .
In linear discriminate analysis, the purpose is to determine the optimal coefficients (or
weights) for the attributes, denoted by w = ( w1 ,..., wn ) and a boundary value (scaT

lar) b to separate two predetermined classes: G (Good) and B (Bad); that is

628

Y. Zhang, P. Zhang, and Y. Shi

Fig. 2. Overlapping of two-class linear discriminate analysis

To measure the separation of Good and Bad,

αi

is defined to be the overlapping of

α is denoted as the max overlapping of
the two-class boundary for all cases xi (α i < α ) . βi is also defined to be the distance
of case xi from its boundary, while β is the minimum distance of all cases xi to the

the two-class boundary for case xi . Then,

boundary ( β i < β ) . To find the compromise solution of MMD (maximize the minimum distances) and MSD (minimizing the sum of the deviations) for data separation,
we want to minimize the sum of α i and maximize the sum of βi simultaneously, as
follows:
l

l

i =1

i =1

Minimize C ∑ α i − ∑ β i
s.t. :
x11 w1 + ... + x1n wn = b + α1 + β1 , for x1 ∈ B,
xl1w1 + ... + xln wn = b + α l + β l , for x1 ∈ G,
xl1w1 + ... + xln wn = b + α l + β l , for

α i ≥ 0,
β i ≥ 0,

(7)

x1 ∈ G,

i = 1,...l ,
i = 1,...l ,

wi ∈ R .
n

As we discussed above, the inherent drawback of linear formulation makes MCLP
can only solve linear classification problems.
3.2 RMCLP Model
Lots of empirical studies have shown that MCLP is a powerful tool for classification.
However, there is no theoretical work on whether MCLP always can find an optimal

Kernel Based Regularized Multiple Criteria Linear Programming Model

629

solution under different kinds of training samples. To go over this difficulty, recently,
Shi et.al [5] proposed a RMCLP model by adding two regularized items 1 x T Hx and
2
1 T
α Qα on MCLP as follows:
2

1
1
Minimize wT Hw + α T Qα + d T α − cT β
2
2
s.t. :
xi1 w1 + ... + xin wn = b + α i − β i , ∀ xi ∈ G1;

(8)

xi1 w1 + ... + xin wn = b − α i + β i , ∀xi ∈ G 2;

α i , β i ≥ 0.
where H ∈ R r *r , Q ∈ R n*n are symmetric positive definite matrices. d T , c T ∈ R n .The
RMCLP model is a convex quadratic program. Theoretically studies [5] have shown
that RMCLP can always find a global optimal solution.

4 Kernel Based RMCLP Model
In order to solve the non-linear classification problem, we add kernel function into the
N

RMLCP model. As discussed above, we use w = ∑ βiφ ( xi ) to replace the original
i =1

weight w in RMCLP model. By letting βi = λi yi , we get w = λ y φ ( x ) and the new
∑ i i i
N

i =1

RMCLP model can be formulated as follows:

Minimize

1 T
1
w Hw + α T Qα + d T α − cT β
2
2

s.t. :

λ1 y1 (φ ( x1 ) ⋅ φ ( xi )) + ... + λn yn (φ ( xn ) ⋅ φ ( xi )) = b + αi − βi , ∀ xi ∈ G1;
λ1 y1 (φ ( x1 ) ⋅ φ ( xi )) + ... + λn yn (φ ( xn ) ⋅ φ ( xi )) = b − αi = βi , ∀ xi ∈ G 2;
α i , βi ≥ 0.
C ≤ λi ≤ E;

(9)

Where H ∈ R r *r , Q ∈ R n*n are symmetric positive definite matrices. d T , cT ∈ R n .
Besides, we add the last constrain C ≤ λi ≤ E when we compare the model with

SVM. By letting K ( xi , x j ) = φ ( xi ) ⋅ φ ( x j ) , we get the kernel based RMCLP model in
Equ. (10).

630

Y. Zhang, P. Zhang, and Y. Shi

Minimize

1 T
1
w Hw + α T Qα + d T α − cT β
2
2

s.t. :

λ1 y1 K ( x1, xi ) + ... + λn yn K ( xn , xi ) = b + αi − βi , ∀ xi ∈ G1;
λ1 y1 K ( x1, xi ) + ... + λn yn K ( xn , xi ) = b − αi = βi , ∀ xi ∈ G 2;

(10)

αi , βi ≥ 0.
C ≤ λi ≤ E;

5 Testing on Synthetic Dataset
In order to investigate the performance of our new model in classifying non-linear
problems, we design a two-group non-linear dataset as shown in Fig. 3:

Fig. 3. Original dataset

Fig. 5.
RMCLP

Result with Polynomial kernel

Fig. 4. Result with original RMCLP

Fig. 6. Result with RBF kernel RMCLP

Our synthetic dataset contains 90 records, with 38 records in Group 1 (the blue
points) and 52 records in Group 2 (the red points). Since the number of instances in
the two groups is imbalanced, RMCLP model puts all records into Group 2 which is

Kernel Based Regularized Multiple Criteria Linear Programming Model

631

bigger than Group 1. It can reach the best accuracy among all linear classifiers. Fig. 5
and Fig. 6 show the results of the kernel based RMCLP (with Polynomial kernel and
RBF kernel respectively). The two groups are perfectly separated.

6 Experimental Results
In this section, we use two public UCI datasets to compare the performance of Kernel
based RMCLP with two other methods: SVM and MCQP (a quadratic formulation of
MCLP). Here we only use linear kernel to SVM.
Before building models, we scale each feature into the range [0, 1]. For the Australian dataset, we randomly divide it into two parts: training set with 200 records and
testing set with 490 records. For Heart dataset, we also randomly split it into two
parts: training set with 100 records and testing set with 170 records.
In every training process, parameters in Kernel based RMCLP are selected in some
discrete sets in order to get the best accuracy. We select the parameter E from the set
[0.1 1 16 128 1024 10000], C from the set [-∞ 0], and γ from [0.001 0.01 0.1 1 16
128 1024].[2] When getting the model for every C, E and γ , we test the performances
on the training sets. Then, we fix the C, E and γ set which achieves the highest accuracies to predict the test sets and record the accuracies in Table 1 and Table 2.
Table 1. Test On Australian Dataset

Classification
Algorithms

Training(200 records)

Testing (490 records)

Training Accuracy

Testing Accuracy

MCQP
SVM
RMCLP
K-RMCLP

83.5%
85%
85.5%
90.5%

84.49%
84.55%
81.84%
87.14%

Table 2. Test On Heart Dataset

Classification
Algorithms

Training(100 records)

Testing (170 records)

Training Accuracy

Testing Accuracy

MCQP
SVM
RMCLP
K-RMCLP

84%
84%
79%
90%

83.53%
82.35%
81.18%
85.88%

In the table above, we can observe that RBF Kernel based RMCLP performances
better than the other two algorithms. And the prediction accuracy improvement of our
new model has a strong relationship with the linear-separable degree of the training
sample. Through these experiments, we find out that the Kernel based RMCLP is a
competitive method in non-linear classification.

632

Y. Zhang, P. Zhang, and Y. Shi

7 Conclusions
In this paper, we add the kernel function into the Regularized Multiple Criteria Linear
Programming (RMCLP) model to deal with the non-linear classification problems. By
the Representer Theorem, we know that the optimal solution has the following form:

K(xi , x) = 〈φ(xi ),φ(x)〉 . After replacing w in RMCLP model with w = ∑ β iφ ( xi ) ,
N

i =1

we upgrade the original RMLCP model into tackling the non-linear classification
problems. Experimental results on both synthetic and real-life datasets show that our
model is effective in classifying non-linear datasets. In the future work, we will study
the parameters’ effect of the kernel functions in our new kernel based RMCLP model.

References
1. Olson, D., Shi, Y.: Introduction to Business Data Mining. McGraw-Hill/Irwin (2007)
2. Zhang, Z., Zhang, D., Tian, Y., Shi, Y.: Kernel Based Multiple Criteria Linear Program
3. Frawley, W., Piatetsky-Shapiro, G., Matheus, C.: Knowledge Discovery in Databases: An
Overview. AI Magazine, 213–228 (Fall 1992)
4. Shi, Y.: Multiple criteria and multiple constraint levels linear programming: concepts, techniques and applications. World Scientific Pub. Co Inc., New Jersey (2001)
5. Shi, Y., Tian, Y., Chen, X., Zhang, P.: A Regularized Multiple Criteria Linear Program for
Classification. In: ICDM Workshops 2007, pp. 253–258 (2007)
6. Chapelle, O., Sindhwani, V.: Optimization Techniques for Semi-Supervised Support Vector
Machines. Journal of Machine Learning Research 9, 203–233 (2008)
7. Zhang, P., Tian, Y., Zhang, Z., Li, X., Shi, Y.: Supportive instances for Regularized Multiple Criteria Linear Programming Classification
8. Deng, N., Tian, Y.: New Approach in Data Mining – Support Vector Machine. Science
Press, Beijing (2004)

