Instruction Hints for Super Eﬃcient Data
Caches
Jie Tao1 , Dominic Hillenbrand2 , and Holger Marten1
1

Steinbuch Center for Computing
Forschungszentrum Karlsruhe
Karlsruhe Institute of Technology, Germany
{jie.tao,holger.marten}@iwr.fzk.de
2
Computer Laboratory
University of Cambridge, United Kingdom
dh378@cam.ac.uk

Abstract. Data cache is a commodity in modern microprocessor systems. It is a fact that the size of data caches keeps growing up, however,
the increase in application size goes faster. As a result, it is usually not
possible to store the complete working set in the cache memory.
This paper proposes an approach that allows the data access of some
load/store instructions to bypass the cache memory. In this case, the
cache space can be reserved for storing more frequently reused data. We
implemented an analysis algorithm to identify the speciﬁc instructions
and a simulator to model the novel cache architecture. The approach was
veriﬁed using applications from MediaBench/MiBench benchmark suite
and for all except one application we achieved huge gains in performance.
Keywords: Cache optimization, simulation, architecture design.

1

Introduction

The memory system is a traditional optimization target for enhancing the overall performance of a computational architecture. With the microprocessor design
increasingly focusing on the power issue, memory also becomes the goal of optimization in terms of both performance and energy consumption.
To optimize the memory system, cache is the key. In the performance aspect,
an access to the cache is much more eﬃcient than a reference to the main memory because a cache access takes only several CPU cycles while an access to
the main memory needs more than 100 cycles on modern processors. Hence, a
lower cache miss rate can reduce the CPU time for executing an application.
This actually also decreases the power requirement because accessing the main
memory consumes more energy than accessing the cache.
The common approach for cache optimization focuses on improving the data
locality so that data stored in the cache can be reused before being moved back
to the main memory. This locality is typically achieved by restructuring the
program code based on aﬃnity analysis [8,10,12,13,14].
G. Allen et al. (Eds.): ICCS 2009, Part II, LNCS 5545, pp. 677–685, 2009.
c Springer-Verlag Berlin Heidelberg 2009

678

J. Tao, D. Hillenbrand, and H. Marten

We propose a diﬀerent approach that enhances the cache eﬃciency, in terms
of both performance and energy consumption, by storing only carefully selected
data in the cache. We achieve this goal with an analysis algorithm that goes
through the memory access trace of an application and thereby detects load/store
instructions which do not introduce any cache hits. We decide not to load the
data required by these instructions into the cache. In this case, cache space can
be maintained for other data.
We then implemented a cache simulator for verifying the proposed approach
and also for providing researchers a generic tool to study the impact of cache
bypassing on the performance of applications. The simulator models the basic
functions of a traditional cache with an additional functionality of bypassing the
data of speciﬁc instructions.
The MediaBench and MiBench benchmarks for embedded system were used to
examine the results of our cache optimization. As the cache is usually small on
embedded systems it is more valuable to use their cache space eﬃciently. For the
tested applications we achieved an average improvement of 2.7x in cache hit rate.
The remainder of the paper is organized as following. Section 2 ﬁrst gives an
overview of related work on cache optimization. This is followed by a detailed
description of the proposed concept in Section 3. Section 4 presents the initial
experimental results. The paper concludes in Section 5 with a short summary
and several future directions.

2

Related Work

In the area of cache optimization, a lot of research work has been performed and
diﬀerent approaches were proposed. They either only address the performance aspect or also have speciﬁc mechanisms for power saving. Here, we focus on the latter.
Esakkimuthu et al. [5] deployed both hardware and software techniques to
save the energy consumption of the memory system. The hardware approach
includes a block buﬀering scheme and a sub-banking strategy. The former uses
a buﬀer to store the previously accessed cache line. In this case, if the next data
request targets on the same cache line, data can be acquired from the buﬀer
without accessing the cache. In the sub-banking optimization, the data array of
the cache is partitioned into banks and by a data request only the corresponding
bank is accessed rather than the whole cache. The software approach applies the
conventional code optimization schemes like loop interchange, loop tiling, and
loop unrolling to improve the data reuse with a combined result of less cache
misses and energy consumption.
Yang et al. [16] proposed a cache architecture to reduce the power demand
but without performance degradation. This architecture deploys an L0, an instruction cache between CPU and the L1 instruction cache, to store frequently
accessed instructions. A steering mechanism is employed to direct an instruction
to the correct location. Simulation results show a 52% instruction cache energy
reduction on average for a set of multimedia applications.

Instruction Hints for Super Eﬃcient Data Caches

679

Benitez et al. [3] proposed a reconﬁgurable cache architecture that can be
adapted to the running programs. The adaptation scheme is based on two techniques: a learning process provides the best cache conﬁguration for each program
phase, and a recognition process detects program phase changes. In addition, a
low-overhead reconﬁguration mechanism was designed. Simulation results show
that this approach can achieve performance improvement and cache energy saving at the same time. Similarly, Cordon-Ross et al. [6] also address reconﬁgurable
cache architectures but use heuristic tuning method to determine the best cache
parameters. A speciﬁc contribution of this work is its mechanisms for tuning a
data and instruction uniﬁed second level cache. As such tuning covers a large
space, this approach has achieved a higher power saving than the other one.
Abella et al. [1] also focus on the second level cache and designed a hardware
technique to turn oﬀ those cache lines that are not expected to be reused. Alternatively, Ishihara et al. [9] use speciﬁc algorithm to place the code in the cache
in a way that not all cache lines in a set are used. Unused cache lines can be
disconnected for saving the power.
Overall, various schemes have been deployed in the last years for performance
and energy issues of the memory system. They can be roughly classiﬁed to two
groups: one focuses on improving the data locality with additional hardware
support and the other on adapting the cache architecture to applications. Our
approach can be classiﬁed to the ﬁrst group because it also addresses on the
locality issue. However, this approach is diﬀerent from other work in that it
allows some data accesses to bypass the cache memory so to save spaces for
more important data.

3

The Ihint Architectural Approach

Key to our performance improvements is a novel cache architecture capable of
bypassing the data of some loads/stores according to the instruction hints (Ihint
for short). The instructions are selected by an analysis algorithm and the cache
architecture is simulated.
3.1

The Ihint Cache

Figure 1 shows the proposed cache architecture which is a slightly modiﬁed
version of the traditional one. We mainly extend the ways of data transferred
between the processor and the oﬀ-chip memory. In comparison to a standard
cache design, we allow direct reads by the processor and provide a FIFO for
potentially non-blocking writes. In both case the cache is bypassed and its state
is untouched. Accesses to the oﬀ-chip memory are controlled by the arbiter.
The cache is accessed in case of a hit. In case of a miss, the actions depend on
whether a special-instruction has been encountered. A special-instruction always
bypasses the data cache and leads to a direct read from oﬀ-chip memory or to a
write into the FIFO.

680

J. Tao, D. Hillenbrand, and H. Marten

Fig. 1. Ihint-Cache Overview

3.2

“Special”-Instruction Selection Process

The hardware provides us with more choices in the memory paths. Now we need
to decide which load/store-instructions can be regarded as “special”. For this,
an analysis algorithm is designed and implemented.

Fig. 2. Instruction selection and cache simulation

The left diagram in Fig. 2 illustrates the workﬂow of the analysis process. As
depicted in the diagram, the analysis algorithm needs a memory access trace
and an instruction trace as input. We acquire both trace ﬁles with the ArchC
[2] Instruction Set Simulator. For recording the executed instruction and data
access addresses we modiﬁed this simulation tool.
The memory access and instruction traces allow us to determine which memory accesses go along with a distinct instruction. This is essential for the analysis
algorithm to look for special load/store instructions.
Selecting special instructions is actually to decide whether to move data from
oﬀ-chip memory to the cache at a read/write miss. The analysis is split into two
distinct steps. One for identifying the special loads and one for the stores. The
analysis in both steps is based on the diﬀerence between the amount of cache
hits and misses caused by an individual instruction.

Instruction Hints for Super Eﬃcient Data Caches

681

Table 1. Binary instruction break-down by application and architecture

PEGWIT

total instructions
loads/stores
special loads/stores
CJPEG
total instructions
loads/stores
special loads/stores
GSM
total instructions
loads/stores
special loads/stores
DIJKSTRA total instructions
loads/stores
special loads/stores

Instructions
MIPS
SPARC
23149
20652
3524 2367 2913 1717
41 149 25 80
36043
32008
7218 4832 6353 3548
80 766 67 417
21899
19888
3289 2397 2660 1760
18 46 12 38
13251
12100
1944 1500 1462 1008
11 95
9 101

PowerPC
22405
3664 2809
53 174
35388
7.171 5.165
93 821
21124
3508 2682
38
71
13753
2130 1813
36 131

For the load-instructions, we keep track which data they have loaded. Basically, this means every word in memory has a pointer to the instruction which
has loaded it. Upon a hit we decrease a per instruction miss-counter for the
instruction that has previously loaded the requested data at a given address.
Upon a read/write miss we increase the counter. Load-instructions with a misscounter value higher than zero, meaning that the instruction does not introduce
any cache hit, are considered as special instructions.
For store-instructions the miss-counter is computed diﬀerently. We keep track
which instructions have stored data to a particular address in memory. If upon
loading a miss is detected, we increase the miss-counter for every previous storeinstruction, deploying a decaying penalty for older entries. Currently, we start
with a penalty of 10 and halve it for every following older entry until we reach
a value of 1. Similar to the load instructions we use a threshold value of zero,
meaning that higher values lead to a designation as a special store instruction.
Table 1 shows sample analysis results with four diﬀerent applications on three
diﬀerent processor architectures. For each application, we counted the instructions in total, the number of load/stores, and the number of special load/store
instructions chosen by the analysis algorithm. In general more special store instructions are picked than load instructions and the number of special instructions is not high in contrast to the total number of loads/stores. However, even
with this small set of special instructions applications beneﬁt from the Ihint
approach. In the next section we show the experimental results.
3.3

Cache Simulation

In order to verify the proposed approach as well as to observe the inﬂuence
of this novel design on performance we developed a cache simulator. Similar to
any existing cache models, our tool simulates the basic functionality of the cache

682

J. Tao, D. Hillenbrand, and H. Marten

memory and provides statistics on cache hits and misses. The cache conﬁguration
can be speciﬁed by the user. An additional function of this cache simulator is
to process the special load/store instructions. Hence, it requires not only the
common memory access trace but also the instruction trace.
The right diagram in Fig. 2 illustrates the simulation process. As shown in
the diagram, the simulator takes the memory access trace, the instruction trace,
and the addresses of special instructions as input. For each memory access it
models the lookup procedure and thereby modiﬁes the status of the cache line.
If the memory access is related to a special instruction, the simulator does not
load the data into the cache for load operations or puts the data into the FIFO
buﬀer for write operations. During the simulation procedure, the number of
cache hits/misses and the amount of data transfers to the main memory are
calculated.
For accuracy we adopted the dinero [4] cache simulator to verify our own
cache simulation component. We compare some report of both simulators to be
sure that our component provides correct information.

4

Experimental Results

The Ihint concept was veriﬁed using four applications of MediaBench[11] and
MiBench[7]. These applications are PEGWIT, CJPEG, GSM, and DIJKSTRA.
Each benchmark was run on MIPS, SPARC and PowerPC. PEGWIT deals with
the cryptographic key generation; CJPEG handles the JPEG encoding; GSM
works with audio decoding, and DIJKSTRA computes the shortest path between
two nodes in a graph. We measured the amount of oﬀ-chip data traﬃc and the
CPU cycles for data accesses. The ﬁrst metric can be used to evaluate the energy
consumption [15], while the second metric represents the performance.
All simulated caches are 4x associative and have 32 byte cache line size. Oﬀchip memory accesses take 64 cycles for the ﬁrst access and 32 cycles in burst
mode for both reading and writing. Figure 3 depicts the experimental results,
where we compare the metric values of Ihint with systems without and with
caches.
We ﬁrst observe all diagrams on the left side of the ﬁgure for performance
issues. Overall, Ihint outperforms the normal cache in all benchmarks except
for GSM, where both kind of caches behave similarly. This result holds true for
all architectures; although they show diﬀerent values due to the concrete design
in instruction sets. Totally, CJPEG achieves an average performance improvement of 78% to the non-cache case and 43% to the normal cache on all three
architectures. DIJKSTRAT introduces the best performance, where the Ihint
cache works 3.3-folds better than the normal cache. With PEGWIT the performance gain is 52% and 72% individually. It is also surprised to see that with
this application the cache version requires more time to access the data than
the non-cache case. From the corresponding diagram on the right side it can be
seen that a large amount of data are transferred from the cache to the oﬀ-chip
memory. This means that for PEGWIT the additional traﬃc caused by the cache

Instruction Hints for Super Eﬃcient Data Caches

CJPEG (2kB Cache) - Access Time

CJPEG (2kB Cache) - Transferred Data

800.000.000

12.000.000

Cycles

600.000.000
500.000.000
400.000.000

10.000.000
8.000.000
Byte

MIPS
SPARC
POWERPC

700.000.000

6.000.000
4.000.000

300.000.000
200.000.000

2.000.000

100.000.000

0
Read (no Write (no
cache) cache)

0
Cycles without
Cache

Cycles with Cache

Cycles with IHINT

Write
IHINT

70.000.000

2.500.000.000
2.000.000.000

60.000.000
50.000.000
Bytes

MIPS
SPARC
POWERPC

3.000.000.000
Cycles

Read
IHINT

DIJKSTRA (2kB Cache) - Transferred Data

3.500.000.000

40.000.000
30.000.000

1.500.000.000

20.000.000

1.000.000.000

10.000.000

500.000.000

0
Read (no Write (no
cache) cache)

0
Cycles without
Cache

Cycles with Cache

Cycles with IHINT

Cache
Read

Cache
Write

Read
IHINT

Write
IHINT

MIPS SPARC POWERPC

GSM (2 kB Cache) - Transferred Data

GSM (2 kB Cache) - Access Time

12.000.000

1.400.000.000

1.000.000.000
800.000.000

10.000.000
Byte

MIPS
SPARC
POWERPC

1.200.000.000
Cycles

Cache
Write

MIPS SPARC POWERPC

DIJKSTRA (2kB Cache) - Access Time

8.000.000
6.000.000
4.000.000

600.000.000

2.000.000

400.000.000

0

200.000.000

Read Write Cache Cache Read Write
(no
(no
Read Write IHINT IHINT
cache) cache)

0
Cycles without
Cache

Cycles with Cache

Cycles with IHINT

MIPS

PEGWIT (2 kB Cache) - Access Time

SPARC

POWERPC

PEGWIT (2 kB Cache) - Transferred Data

800.000.000

25.000.000

700.000.000

MIPS

600.000.000

SPARC

500.000.000

POWERPC

400.000.000
300.000.000

20.000.000
Byte

Cycles

Cache
Read

15.000.000
10.000.000
5.000.000

200.000.000

0

100.000.000
0
Cycles without
Cache

Cycles with Cache

Cycles with IHINT

Read (no Write (no
cache) cache)

Cache
Read

Cache
Write

Read
IHINT

MIPS SPARC POWERPC

Fig. 3. Experimental results

Write
IHINT

683

684

J. Tao, D. Hillenbrand, and H. Marten

for implementing the cache policies exceeds the amount of data transfers in the
non-cache case. As a result, the introduction of a cache memory does not bring
beneﬁt for this application. The Ihint cache, however, can better use the cache
space, hence showing its advantages. With GSM Ihint fails to surpass the cache.
This is because the normal cache already performs fairly well with a very low
penalty for data access. In this case, the improvement space is limited.
Observing the diagrams on the right side of Fig. 3 similar results with the
data traﬃc can be seen as with the access time: all applications, except GSM,
show less data transfers with the Ihint approach. The decrease in data traﬃc
indicates a decrease in power requirement. Unfortunately, we do not have an
energy model to quantify the amount of power saving.

5

Conclusions

In this paper, we demonstrate an approach of selective bypassing of the cache
memory based on instruction hints. It can be seen that our cache extension is able
to exploit the new data paths for reading and writing to/from oﬀ-chip memory.
Our analysis method is successful in selecting suitable load/store instructions at
design time. As a result we achieve a much higher eﬃciency than a normal cache
design.
The proposed approach was evaluated on embedded systems with small caches.
In the next step, commodity caches with large size and realistic applications will
be investigated. It is expected that the same gain can be achieved. In addition, we are also thinking about a hardware approach of selecting the special
instructions.

References
1. Abella, J., Gonz´
alez, A., Vera, X., O’Boyle, M.: IATAC: a smart predictor to turnoﬀ L2 cache lines. ACM Transactions on Architecture and Code Optimization 2(1),
55–77 (2005)
2. ArchC. Archc - architecture description language (2007)
3. Benitez, D., Moure, J.C., Rexachs, D.I., Luque, E.: Evaluation of The Fieldprogrammable Cache: Performance and Energy Consumption. In: CF 2006: Proceedings of the 3rd conference on Computing frontiers, pp. 361–372 (2006)
4. Edler, J., Hill, M.D.: Dinero IV cache simulator
5. Esakkimuthu, G., Vijaykrishnan, N., Kandemir, M., Irwin, M.J.: Memory System
Energy: Inﬂuence of Hardware-software Optimizations. In: ISLPED 2000: Proceedings of the 2000 international symposium on Low power electronics and design, pp.
244–246 (2000)
6. Gordon-Ross, A., Vahid, F., Dutt, N.: Fast Conﬁgurable-cache Tuning with a Uniﬁed Second-level Cache. In: ISLPED 2005: Proceedings of the 2005 international
symposium on Low power electronics and design, pp. 323–326 (2005)
7. Guthaus, M.R., Ringenberg, J.S., Ernst, D., Austin, T.M., Mudge, T., Brown,
R.B., Mibench: A free, commercially representative embedded benchmark suite.
In: 2001 IEEE International Workshop on WWC-4. WWC 2001: Proceedings of
the Workload Characterization, pp. 3–14 (2001)

Instruction Hints for Super Eﬃcient Data Caches

685

8. Hu, J., Kandemir, M., Vijaykrishnan, N., Irwin, M.J.: Analyzing data reuse for
cache reconﬁguration. Transactions on Embedded Computing System 4(4), 851–
876 (2005)
9. Ishihara, T., Fallah, F.: A Non-uniform Cache Architecture for Low Power System
Design. In: ISLPED 2005: Proceedings of the 2005 international symposium on
Low power electronics and design, pp. 363–368 (2005)
10. Kandemir, M., Kadayif, I., Choudhary, A., Zambreno, J.A.: Optimizing Inter-nest
Data Locality. In: CASES 2002: Proceedings of the 2002 international conference on
Compilers, architecture, and synthesis for embedded systems, pp. 127–135 (2002)
11. Lee, C., Potkonjak, M., Mangione-Smith, W.H.: Mediabench: a tool for evaluating
and synthesizing multimedia and communicatons systems. In: MICRO 30: Proceedings of the 30th annual ACM/IEEE international symposium on Microarchitecture,
pp. 330–335 (1997)
12. Pingali, V., McKee, S., Hseih, W., Carter, J.: Computation Regrouping: Restructuring Programs for Temporal Data Cache Locality. In: ICS 2002: Proceedings of
the 16th international conference on Supercomputing, pp. 252–261 (2002)
13. Sermulins, J., Thies, W., Rabbah, R., Amarasinghe, S.: Cache Aware Optimization of Stream Programs. In: LCTES 2005: Proceedings of the 2005 ACM SIGPLAN/SIGBED conference on Languages, compilers, and tools for embedded systems, pp. 115–126 (2005)
14. Shen, X., Gao, Y., Ding, C., Archambault, R.: Lightweight Reference aﬃnity analysis. In: ICS 2005: Proceedings of the 19th annual international conference on
Supercomputing, pp. 131–140 (2005)
15. Sotiriadis, P.P., Chandrakasan, A.P.: A bus energy model for deep submicron technology. IEEE Trans. Very Large Scale Integr. Syst. 10(3), 341–350 (2002)
16. Yang, C., Lee, C.: HotSpot Cache: Joint Temporal and Spatial Locality Exploitation for I-cache Energy Reduction. In: ISLPED 2004: Proceedings of the 2004
international symposium on Low power electronics and design, pp. 114–119 (2004)

