Smoothing Newton Method for L1 Soft Margin
Data Classification Problem
Weibing Chen1 , Hongxia Yin2 , and Yingjie Tian1
1

2

Research Center on Fictitious Economy & Data Science,
Chinese Academy of Sciences, Beijing 100190, China
Department of Mathematics and Statistics, Minnesota State University Mankato,
273 Wissink Hall, Mankato, MN 56001, USA
chenweibing06@mails.gucas.ac.cn,
hongxia.yin@mnsu.edu,
tianyingjie1213@163.com

Abstract. A smoothing Newton method is given for solving the dual
of the l1 soft margin data classification problem. A new merit function
was given to handle the high-dimension variables caused by data mining
problems. Preliminary numerical tests show that the algorithm is very
promising.

1

Introduction

In the multiple kernel learning two-class nonlinear data classiﬁcation problem, we
suppose that n data {(xi , yi )} are given, where xi ∈ X for some input space X ⊂
Rp , and yi ∈ {−1, 1} indicating the class to which the point xi belongs and are
called lables. We assume that F is an embedding space (called feature space), ϕ is
a map from X to F . A kernel is a function k such that k(xi , xj ) = ϕ(xi ), ϕ(xj )
for any xi , xj ∈ X , where ·, · is the inner production. A kernel matrix is a square
matrix K ∈ Rn×n such that Kij = k(xi , xj ) for x1 , · · · , xn ∈ X . It is known that
every kernel matrix is symmetric positive semideﬁnite. Kernel based method for
two-class nonlinear classiﬁcation is to ﬁnd an aﬃne function in the feature space,
f (x) = w, ϕ(x) + b for some weight vector w ∈ F and b ∈ R to maximize the
margin (or distance) between the parallel hyperplanes w, ϕ(x) + b = 1 and
w, ϕ(x) + b = −1 in the high-dimensional feature space that are as far apart
as possible while still separating the data. The hard margin problem is to ﬁnd
(w∗ , b∗ ) that solves
min w, w
w,b

(1)

s.t. yi ( w, Φ(xi ) + b) ≥ 1, i = 1, · · · , n,
which realizes the maximal margin classiﬁer with geometric margin γ = w2∗ ,
assuming it exists. γ is actually the distance between the convex hulls of the two
classes of data.
G. Allen et al. (Eds.): ICCS 2009, Part II, LNCS 5545, pp. 543–551, 2009.
c Springer-Verlag Berlin Heidelberg 2009

544

W. Chen, H. Yin, and Y. Tian

Problem (1) was widely investigated by many authors, see [1], [9], [10], [11],
[13], and [14] for example. However, since the solution of (1) exists only when the
labeled sample is linearly separable in the feature space (f (x) = w, Φ(x) + b),
the following soft margin problem was deﬁned by introducing slack variable ξ =
(ξ1 , ξ2 , · · · , ξn )T with ξi ≥ 0 for all i = 1, · · · , n in order to relax the constraints
in (1) to
yi ( w, Φ(x) + b) ≥ 1 − ξi , i = 1, · · · , n.
There are variety of ways to deﬁne the soft margin problem [4], [9], [10], [11].
In 2004, Ferris and Munson [5] produced a semismooth support vector machine
(SVM) where the soft margin was deﬁned by the least-square of the error (it is
l2 norm). In the paper, We consider the l1 -norm soft margin problem
min

w,b,ξ

w, w + C

n
i=1 ξi

(2)

s.t. yi ( w, Φ(x) + b) ≥ 1 − ξi , i = 1, · · · , n,
ξi ≥ 0, i = 1, · · · , n,
where the parameter C > 0 determines the trade oﬀ between a large margin
γ = 2/ w and a small error penalty. The dual problem of (2) is
max 2αT e − αT G(K)α
s.t.
y T α = 0,

(3)

0 ≤ α ≤ C,
Here C is a constant vector with all components to be the same positive number
C, α ∈ Rn is the dual parameter with its components αi (i = 1, 2, · · · , n) taking
values in the interval [0, C]. For a ﬁxed kernel K, (3) gives an upper bound on
misclassiﬁcation probability (see [1] Chapter 4 for details), solving problem (3)
for a single kernel matrix is therefore a way to minimize the upper bound on
error probability.
Smoothing Newton method was veriﬁed to be an eﬃcient way to some nonsmooth equation system with global convergence and local superlinear convergence.
Especially, it is successfully used to solve variation inequality, complementarity
problems and KKT system of optimization problems. See [6], [3], and [8]. In the
paper, we introduce a smoothing Newton method for solving the dual of the
l1 soft margin SVM problem (3). Moreover, in order to overcome the diﬃculty
caused by the high-dimensional variables in data mining, we introduce a new
merit function based on the Huber function [7].
The paper is organized as follows. In Section 2, we produce the smoothing equations for the KKT system of problem (3) by projection technique,
then we introduce a new merit function and give a smoothing Newton method
for solving (3). Numerical tests for illustrating the eﬃciency of the algorithm are
given in Section 3. Conclusion and further remarks are in Section 4.

Smoothing Newton Method for L1 Soft Margin Data Classification Problem

2

545

Smoothing Newton Method

In this section, we introduce a smoothing Newton method for solving (3). For
this we rewrite the problem as the following minimization problem.
min 12 αT G(K)α − eT α
s.t.

(4)

T

y α = 0,
0 ≤ α ≤ C.

The Lagrange function for problem (4) is
L(α, λ) =

1 T
α G(K)α − eT α − λy T α
2

(5)

and its derivative Lα (α, λ) = G(K)α − e − λy, Lλ (α, λ) = −y T α. We denote
u = (α, λ) ∈ Rn × R and
F (u) = F (α, λ) =

G(K)α − λy − e
.
yT α

(6)

Then α∗ is a solution of (4) if and only if there exists λ∗ such that u∗ = (α∗ , λ∗ )
is a solution of the following variational inequality problem
(u − u∗ )T F (u∗ ) ≥ 0, ∀ u = (α, λ) ∈ Ω = [0, C]n × R.

(7)

It is well known from [6] that solving (7) is equivalent to ﬁnding a root of the
Robinson’s Normal equations:
F (ΠΩ (u)) + u − ΠΩ (u) = 0

(8)

where ΠΩ (u) is the projection of u onto Ω. From the deﬁnition of Ω we have
that
G(K)Π[0,C]n α − λy − e
+
y T Π[0,C]n α

α
−
λ

Π[0,C]n α
= 0,
λ

(9)

that is,
H(u) := H(α, λ) :=

G(K)Π[0,C]n α + α − Π[0,C]n α − λy − e
= 0.
y T Π[0,C]n α

(10)

Recall that for any three numbers c ∈ R ∪ {−∞}, d ∈ R ∪ {∞} with c ≤ d and
v ∈ R, the median function
⎧
⎨ c if v < c
mid(c, d, v) = Π[c,d](v) = v if c ≤ v ≤ d
⎩
d if d < v
and the Chen-Harker-Kanzow-Smale smoothing function for mid(c, d, v) is
φ(t, c, d, v) =

c+

d−
(c − v)2 + 4t2
+
2

(d − v)2 + 4t2
,
2

(11)

546

W. Chen, H. Yin, and Y. Tian

where (t, v) ∈ R++ × R. It can be seen that function φ(·) is continuously diﬀerentiable at any (t, v) ∈ R++ × R.
By deﬁning φ : R++ × Rn with its components
φi (t, α) = φ(t, 0, C, αi ) =

C−
(αi )2 + 4t2
+
2

(C − αi )2 + 4t2
,
2

(12)

where (t, αi ) ∈ R++ × R, i = 1, 2, · · · , n, we have the smoothing equations associated to equations (10) as follows
n
j=1
n
j=1

G(K)ij φj (t, α) + αi − φi (t, α) − λyi − 1 = 0, i = 1, · · · , n,
yj φj (t, α) = 0.

(13)

Then we have
G(K)φ(t, α) + α − φ(t, α) − λy − e
= 0,
y T φ(t, α)

Φ(t, α, λ) :=

(14)

where Φ : R++ × Rn × R → Rn+1 .
Let z = (t, α, λ) ∈ R × Rn × R and deﬁne Θ :∈ R × Rn × R → Rn+2 ,
Θ(z) = Θ(t, α, λ) =

t
.
Φ(z)

(15)

It can be seen that Θ is continuously diﬀerentiable at any z ∈ R × Rn × R.
Proposition 1. For any z = (t, α, λ) ∈ R × Rn × R with t > 0, the matrix Θ (z)
is nonsingular.
Proof. Since C > 0, there exist relative interior points for problem (4).
⎡
⎤
1
0
0
Θ (t, α, λ) = ⎣ (G(K) + I)φt (t, α) M (z) −y ⎦
y T φα (t, α) 0
y T φ t (t, α)

(16)

where
M (z) = G(K)φα (t, α) + I − φα (t, α),

(17)

and φt : Rn+1 → Rn is the gradient of φ with respect to t. φα (t, α) = diag{qi }
is a n × n diagonal matrix with its elements
qi (z) =

1
[(
2

αi
α2i

+

4t2

)+(

(C − αi )
(C − αi )2 + 4t2

)], i = 1, 2, · · · , n.

(18)

Let dz := (dt, dα, dλ)T ∈ R × Rn × R satisﬁes Θ (z)dz = 0. Then from the above
equations dt = 0 and
M (z)dα − ydλ = 0,

(19)

Smoothing Newton Method for L1 Soft Margin Data Classification Problem

y T φα (t, α)dα = 0.

547

(20)
T

Since φα (t, α) is positive deﬁnite for t > 0 then we have from (20) that y dα = 0.
Substitute it into (19) and left multiply (19) by (dα)T we have
(dα)T M (z)(dα) + (dα)T ydλ = 0,
which means that dα = 0. Since G(K) is positive semideﬁnite and yi takes value
at {−1, 1}, from (19) we have that dλ = 0. Therefore, Θ (z) is nonsingular for
any z with t > 0.
In smoothing Newton method, people usually take Θ(z) 2 as the merit function. However, since the data mining problems usually produce large-scale optimization problems, in order to overcome this diﬃculty in numerical computation,
we introduce the following merit function Υ (z):
n

Υ (z) =

ρhj (Θj (z))
j=1

where

ξ 2 /2,
hj |ξ| − h2j /2,

ρhj (ξ) =

if |ξ| ≤ hj ,
otherwise

is the well-known Huber function [7]. Since Huber function is linear when ξ is not
small enough, the computation on the merit function will be very simple when
the iteration is far from the optimal solution. However, we can not simply apply
Newton’s method on Υ (z) since Huber function is smooth but not second order
diﬀerentiable. Before we give the smoothing Newton method, we let γ ∈ (0, 1)
is a real number and deﬁne β : Rn+2 → R+ ,
β(z) := γ min{1, ||Θ(z)||},
and
Ω := {z = (t, α, λ) ∈ R × Rn × R| u ≥ β(z)t¯},
where t¯ is a given positive number. Now we are ready to give the smoothing
Newton method.
Algorithm 1
Step 0. Choose constants t¯ > 0, δ ∈ (0, 1) and σ ∈ (0, 1/2). Let z¯ = (t¯, 0, 0) ∈
R+ × Rn × R, t0 = t¯ and (α0 , λ0 ) ∈ Rn+1 be an arbitrary point. Set l := 0.
Step 1. If Θ(z l ) = 0 then stop. Otherwise, let βl := β(z l ).
Step 2. Compute Δz l := (Δtl , Δαl , Δλ) ∈ R × Rn × R by
Θ(z l ) + Θ (z l )Δz l = βl z¯.

(21)

Step 3. Let ml be the smallest nonnegative integer m satisfying
Υ (z l + δ m Δz l ) ≤ [1 − 2σ(1 − γ t¯ )δ m ]Υ (z l ).
l+1

l

ml

l

Deﬁne z
:= z + δ Δz .
Step 4. Set l := l + 1 and go to Step 1.

(22)

548

W. Chen, H. Yin, and Y. Tian

From Proposition 1 and by using nonsmooth analysis, we can prove the following convergence theorem of the above algorithm. We omit its proof here because
of the page limitation.
Theorem 1. Suppose that z ∗ is an accumulation point of the infinite sequence
{z k } generated by Algorithm 1. If all V ∈ ∂Θ(z ∗ ) are nonsingular, then for any
initial point z 0 = (t0 , α0 , λ0 ) with t0 > 0, the sequence {z k } converges to z ∗
quadratically. i.,e., for k large enough,
z k+1 − z ∗ = O( z k − z ∗ 2 )
and
tk+1 = O(tk )2 .

3

Numerical Tests

In this section we report our numerical results of smoothing Newton methods for
data classiﬁcation of the heart disease, ionosphere and breast-cancer-Wisconsin
data obtained from the UCI repository. Our numerical experiments were carried
out in personal computer with 1060GHz AMD Sempron (tm) Processor and
512MB memory and 80G hard disk. The program is written in MATLAB 7.0.1.
3.1

The Pre-process of the Data

In data mining, the raw data we got may include very big and/or very small
numbers. As we known, in numerical computation by computer, the big number
may ‘eat’ the small number. In order to overcome this diﬃculty and obtain good
computation results from Algorithm 1, we need to do the data pre-process ﬁrst,
which includes normalization and standardization for the data.
The data normalization and standardization we took are as follows,
v = v − min(v)

(23)

v = v /max(v )

(24)

where the vector v is a n × 1 vector, and it presents an attribute of the data, (23)
presents the standardization of the data and (24) presents the normalization of
the data. When there is an attribute which has the same number in every sample,
that is v is a vector of the same number, v will be zero vector, and v will be
not feasible. Thus we must delete the attributes which have the same numbers
in every sample, so that the revised data can be feasible.
3.2

Numerical Results

Now we present the numerical results of smoothing Newton method for the
soft margin data classiﬁcation problem on the heart disease, ionosphere, breastcancer-Wisconsin, and the data of Credit Cards. For the convenience of expression, we denote the linear kernel function k1 (x1 , x2 ) = x1 .x2 as K1 , polynomial

Smoothing Newton Method for L1 Soft Margin Data Classification Problem

549

kernel function k2 (x1 , x2 ) = (x1 .x2 + 1)d as K2 , and the Gaussian kernel function k3 (x1 , x2 ) = exp(−(x1 − x2 )T (x1 − x2 )/σ 2 ) as K3 .
We choose the values of all parameters in Algorithm 1 as
γ = 0.01, σ = 0.25, δ = 0.5,

= 0.0001,

C = 1.

For each data-set, we randomly take 60% of the data as the training set and
the left 40% as the test set. Numerical results on standard benchmark datasets
are summarized in Table 1, Table 2 and Table 4. In the process of computation,
numerical diﬀerentiation can be used to take the place of accurate derivative.
We take two-point diﬀerentiation formula in our numerical tests:
f (x) ≈

f (x + ) − f (x)

,

> 0.

(25)

We ﬁrst apply Algorithm 1 on the training set and obtain the value α, then we
use it to classify the test data to check the accuracy of our methods. In Table 1
below, we listed the processing time (in seconds) and the number of iteration
for working out the value of α. For example, in the heart disease data-set, the
total number of the data is 270, the computation time is 4.969 seconds, and the
number of Newton iterations in Algorithm 1 is 18 for using the linear kernel
function K1 .
Table 1.

Time and number of iterations in computation

total

K1

K2
K3
K3
d=2
σ 2 = 1 σ 2 = 0.2
heart disease 270 4.969 18 6.203 22 4.156 15 5.047 17
ionosphere
351 12.328 25 56.188 111 7.75 15 7.094 14
breast-cancer-W 699 100.64 47 59.000 29 39.734 19 41.719 19
After we obtained the value of α from Algorithm 1, we can easily check the accuracy of our algorithm by applying it on the test set, and it only takes little time.
Table 2 below listed the accuracy of our algorithm with diﬀerent kernel functions.
Table 2.

The accuracy of Algorithm 1

K1
heart disease
ionosphere
breast-cancer-W

86.36%
92.20%
98.57%

K2
K3
K3
d = 2 σ 2 = 1 σ 2 = 0.2
77.27% 77.27% 78.18%
96.45% 97.87% 78.01%
98.92% 98.57% 98.21%

In Table 3 below we listed the accuracy of data classiﬁcation by using semideﬁnite programming (SDP) on the l1 soft margin classiﬁcation problem with
C = 1 in [12]. It can be seen from Table 2 and Table 3 that Algorithm 1
can provide better classiﬁcation for the problems except the polynomial kernel
classiﬁcation K2 with d = 2 for heart disease data.

550

W. Chen, H. Yin, and Y. Tian
Table 3. Accuracy of SDP in [12]
K1
heart disease
ionosphere
breast-cancer-W

84.3%
83.1%
87.7%

K2
d=2
79.3%
94.5%
96.4%

K3
K3
σ 2 = 1 σ 2 = 0.2
59.5%
−
92.1%
−
89.0%
−

Table 4. Test results for credit cards data
Kernal Parametes Time (min) Iterations Accuracy
K1
5.4083
57
79.39%
K2
d=2
3.8000
31
83.26%
K3
σ2 = 2
1.5516
16
84.19%
K3
σ2 = 1
1.5576
16
84.15%

Finally, we apply Algorithm on the data of Credit Cards. There are 6000
samples in total, 5040 of which are good credit, 960 of which are bad credit.
Each sample involves 65 attributes, which were processed into numbers. Because
the number of the samples is large, the data was randomly partitioned into 10%
training and 90% test sets, that is the training data-set has 600 samples(500
good, 100 bad samples). The Table 4 below shows the detail of Algorithm 1 on
the problem.

4

Conclusions

In the paper, we provide a smoothing Newton method for support vector machine
model for l1 soft magian data classiﬁcation problem. The algorithm is global and
local quadratic convergent. Numerical tests on some well-know data classiﬁcation
problem shows that the method is fast and can provider better accuracy that the
method in literature. Further research on variety of kernel matrix classiﬁcation
and smoothing Newton methods for ν-support vector machines [2] and multiclass classiﬁcation [15] are under going.
Acknowledgments. This research has been partially supported by a grant
from National Natural Science Foundation of China (#10671203, #70621001,
#70531040, #70501030, #10601064, #70472074) and Faculty Research Grant
of Minnesota State University Mankato.

References
1. Cristianini, N., Shawe-Taylor, J.: An Introduction to Support Vector Machines.
Cambridge University Press, Cambridge (2000)
2. Chen, P., Lin, C., Sch¨
olkopf, B.: A tutorial on ν-support vector machines. Appl.
Stoch. Models Bus. Ind. 21, 111–136 (2005)

Smoothing Newton Method for L1 Soft Margin Data Classification Problem

551

3. Chen, X., Qi, L., Sun, D.: Global and superlinear convergence of smoothing Newton
method and its application to general box constrasined variational inequalities.
Math. Comp. 67, 519–540 (1998)
4. Cortes, C., Vapnik, V.: Support vector networks. Machine Learning 20, 1–25 (1995)
5. Ferris, M.C., Munson, T.S.: Semismooth support vector machines. Math. Program.
Ser.B 101, 185–204 (2004)
6. Harker, P.T., Pang, J.-S.: Finite-dimensional variational inequality and nonlinear
complementarity problem: A survey of theory, algorithm ans applications. Math.
Program. 48, 161–220 (1990)
7. Huber, P.J.: Robust regression: Asymptotics, conjectures, and Monte Carlo: Ann.
Statist. 1, 799–821 (1973)
8. Qi, L., Sun, D., Zhou, G.: A new look at smoothing Newton methods for nonlinear complementarity problems and box constrained variational inequalities. Math.
program. 87, 1–37 (2000)
9. Mangasarian, O.L.: Mathematical programming in data mining. Data Mining and
Knowledge Discovery 1, 183–201 (1997)
10. Mangasarian, O.L., Musicant, D.R.: Lagrangian support vector machines. Journal
of Machine Learning Research 1, 161–177 (2001)
11. Mangasarian, O.L.: A finite Newton method for classification. Optim. Methods
Softw. 17, 913–929 (2002)
12. Lanckriet, G.R.G., Cristianini, N., Ghaoui, L.E., Bartlett, P., Jordan, M.I.: Learning the kernel matrix with semidefinite programming. J. Machine Learning Research 5, 27–72 (2004)
13. Sch¨
olkopf, B., Smola, A.J.: Learning with Kernels– Support Vector Machines, Regularization, Optimization, and Beyond. The MIT Press, Cambridge (2002)
14. Vapnik, V.N.: The Nature of Statistical Learning Theory, 2nd edn. Springer, Heidelberg (2000)
15. Zhong, P., Fukushima, M.: Regularized nonsmooth Newton method for multi-class
support vector machines. Optim. Methods Softw. 22, 225–236 (2007)

