Parallel Simulated Annealing
for the Job Shop Scheduling Problem
Wojciech Bo˙zejko , Jaroslaw Pempera, and Czeslaw Smutnicki
Institute of Computer Engineering, Control and Robotics
Wroclaw University of Technology
Janiszewskiego 11-17, 50-372 Wroclaw, Poland
wojciech.bozejko@pwr.wroc.pl, jaroslaw.pempera@pwr.wroc.pl,
czeslaw.smutnicki@pwr.wroc.pl

Abstract. This paper describes two parallel simulated annealing algorithms for the job shop scheduling problem with the sum of job completion times criterion. Some properties of the problem associated with the
block theory have been presented and discussed. These properties allow us
to introduce the eﬀective neighborhood based on the adjacent swap type
moves. In this paper, an original method for parallel calculation of optimization criterion value for set of solutions, recommended for the use in
metaheuristics with single- and multiple- search trajectories is proposed.
Additionally, the vector calculation method, that uses multiple mathematical instructions MMX supported by suitable data organization, is
presented. Properties of parallel calculations are empirically veriﬁed on
the PC with Intel Core 2 Duo processor on Taillard’s benchmarks.
Keywords: parallel metaheuristics, scheduling, optimization, job shop,
simulated annealing.

1

Introduction

Job shop scheduling problems follow from many real cases, which means that
they own good practical applications as well as the industrial signiﬁcance. Because of NP-hardness of the problem, despite the criteria value form, heuristics
and metaheuristics are recommended as “the most reasonable” solution methods.
The majority of these methods refers to the makespan minimization. We mention
here, as an example, a few recent studies: Jain, Rangaswamy, and Meeran [7];
Pezella and Merelli [11]; Grabowski and Wodecki [4]; Nowicki and Smutnicki [10].
The job shop problem with general regular criteria is commonly regarded as
harder than the job shop problem with the makespan criterion, mainly because
of the lack of special properties that would reinforce the solution algorithm.
Moreover, till now, there have not been discovered for the problem any sequential accelerators (properties that can speed up computations throughout skillful
aggregation and decomposition of calculations). In this context, parallelization
Corresponding author.
G. Allen et al. (Eds.): ICCS 2009, Part I, LNCS 5544, pp. 631–640, 2009.
c Springer-Verlag Berlin Heidelberg 2009

632

W. Bo˙zejko, J. Pempera, and C. Smutnicki

techniques are the only methods allowing ones, practical instances in reasonable
time to be solved. Therefore they are especially desirable.
Some heuristics algorithms based on dispatching rules for the considered problem are presented in papers of Holthaus and Rajendran [6], Bushee and Svestka
[3]. For the other regular criteria such as the total tardiness there are proposed
metaheuristics based on various local search techniques: simulated annealing [5],
[14], tabu search [2] and genetic search [8].
In this paper we propose a genuine method of cost function computing in
parallel by using multi-processor system as well as a single-processor system with
multiple mathematical instructions MMX. The obtained results can be applied
directly to modern PCs equipped with a few processors, multi-core processors
or processors with multiple mathematical instructions.

2

The Problem

We consider a manufacturing system with any structure consisting of m machines of a unit capacity given by the set M = {1, . . . , m}. In the system, there
are processed n jobs given by set J = {1, 2, ..., n}. The job j-th requires the
sequence of nj operations indexed consecutively (lj−1 + 1, ..., lj−1 + nj ), where
lj = ji=1 ni , is the total number of operations of the ﬁrst j jobs, j = 1, 2, ..., n,
n
(l0 = 0), and o = i=1 oi . The operation x is to be processed on the machine
μx ∈ M during an uninterrupted processing time px > 0, x ∈ O = {1, 2, . . . , o}.
Our aim is to ﬁnd the schedule under the following constraints: (1) each machine
can process at most one product at a time, (2) each product can be processed
by at most one machine at a time, (3) operations cannot be preempted.
The set of operations O can be decomposed into subsets Ok = {x ∈ O :
μx = k}, each of them contains operations to be processed on the machine k,
k ∈ M . Let the permutation πk deﬁnes the processing order of operations from
the set Ok on machine k, and let Πk be the set of all permutations on Ok . The
processing order of all operations on machines is determined by the m-tuple
π = (π1 , π2 , ..., πm ), where π ∈ Π1 × Π2 × ... × Πm .
For any operation j ∈ O and processing order given by π we deﬁne the machine
predecessor/successor sj , sj as well as the technological predecessor/successor
tj , tj , according to the expressions below
sπi (j) =

⎧
⎨
⎩

0

j=1

sπi (j) =

⎧
⎨ πi (j + 1)

j = 1, . . . , nj − 1,

⎩
0
j = nj ,
πi (j − 1) j = 2, . . . , nj ,
⎧
⎧
⎨ 0 j = li−1 + 1, i ∈ J
⎨ 0 j = li , i ∈ J
tπi (j) =
tπi (j) =
⎩
⎩
j−1
otherwise,
j + 1 otherwise,

Note that if the index of predecessor of the operation j is 0, then j is the
ﬁrst operation of proper job or/and the ﬁrst operation processed on the proper
machine. Similarly, if the index of successor of the operation j is 0 then j is the
last operation of proper job or/and the last operation processed on the proper

Parallel Simulated Annealing for the Job Shop Scheduling Problem

633

machine. The machine predecessor/successor depends on π, but for simplicity in
a notation we will not express it explicitly.
The schedule for the ﬁxed processing order π is described by event vectors
S = (S1 , . . . , So ) and C = (C1 , . . . , Co ), where values Sj and Cj denote the
starting time of operation j and its completion time. The schedule has to satisfy
the following constraints
Ctj ≤ Sj

tj = 0, j ∈ O,

(1)

Csj ≤ Sj

sj = 0, j ∈ O,

(2)

Cj = Sj + pj

j ∈ O.

(3)

Because of equation (3), the schedule can be represented by a single event vector,
and we use C to this aim. The schedule C, for the ﬁxed π, is feasible if it satisﬁes
conditions (1)–(3). The constraint (1) follows from the technological processing
order of operations inside job, whereas (2) from the unit capacity of machines.
Our aim is to ﬁnd the feasible processing order π ∗ ∈ Π, so that
Csum (π ∗ ) = min Csum (π),
π∈Π

(4)

n

where Csum (π) = i∈OL Ci is the sum of jobs completion times and OL = {i :
i = li , i ∈ O} is the set of the last operations of jobs.
It is convenient to represent the processing order π and the schedule C by
using the following direct graph G(π) = (O, R ∪ E(π)) with a set of nodes O
and a set of arcs R ∪ E(π), where R = {(tj , j) : tj = 0, j ∈ O} E(π) = {(sj , j) :
sj = 0, j ∈ O}. The node j ∈ O represents the j-th operation of a certain job
and has the weight pj . Arcs from the set R represent the processing order of
operations in jobs and correspond to the constraint (1), whereas arcs from set
E(π) represent the processing order of operations on machines and correspond
to the constraint (2). All arcs from both subsets have the weight zero. Let us
start from some well-known facts.
Property 1. The processing order π is feasible if G(π) does not contain a cycle.
Consider the graph G(π) for a feasible π. Denote by Ui the longest path (i.e.
sequence of nodes) going to node i and by di the length (including pi ) of Ui ,
i ∈ O.
Property 2. For each feasible processing order π, there exists a feasible schedule
C of the problem, such that Ci = di and each Ci is as small as possible, i ∈ O.
The path Ui can be written as Ui = (ui (1), ui (2), . . . , ui (wi )), where ui (x) ∈ O,
1 ≤ x ≤ wi , and wi is the number of nodes in this path. Clearly, ui (wi ) = i. Each
path Ui , i ∈ O can naturally be decomposed into several speciﬁc subpaths, so
that each subpath contains nodes linked by the same type of arcs. We deﬁne block
as the maximal subsequence Bi∗ = (ui (gi∗ ), ..., ui (h∗i )) of Ui such that μui (gi∗ ) =
μui (gi∗ +1) = . . . = μui (h∗i ) and (ui (j), ui (j + 1)) ∈ E(π) for all i = gi∗ , . . . , h∗i − 1,

634

W. Bo˙zejko, J. Pempera, and C. Smutnicki

gi∗ < h∗i . A block corresponds to a sequence of operations (jobs) processed on the
same machine without inserted an idle time. In further considerations we will be
interested only in non-empty blocks, i.e. containing at least two operations.
Based on the properties of the job shop problem with the makespan criterion
[4] one can prove the following properties of the problem.
Theorem 1. Let Bi1 , Bi2 , . . . , Biri be decomposition of Ui , i ∈ OL for the acyclic
graph G(π). If the acyclic graph G(α) has been obtained from G(π) through the
modifications of π so that Csum (α) < Csum (π), then in G(α) at least one operation x ∈ Bik is executed on earlier position than an original one in permutation
πμx , for some k ∈ {1, 2, ..., ri } and some i ∈ OL .
Property 3. Let Bik = (ui (gik ), ..., ui (hki )), k ∈ {1, 2, ..., ri }, i ∈ OL for the
acyclic graph G(π). If the graph G(α) has been obtained from G(π) through the
interchanging two successive operations of Bik that G(α) is acyclic.
Property 4. All paths Ui , i ∈ O and their lengths can be found in the time O(o).

3

Simulated Annealing

Simulated annealing (SA) method applies an analogy to the thermodynamic
cooling process to avoid local minima and escape from them. The search trajectory is guided through the set of solution Π in a ”statistically suitable” direction.
The SA application to our problem is described brieﬂy as follows. In each iteration the new processing order π is selected randomly among those from the
neighborhood N (π) of current processing order π. This processing order (solution) can provide either Csum (π ) ≤ Csum (π) or Csum (π ) > Csum (π). In the
former case π is accepted immediately as the new solution for the next iteration, i.e. π = π . In the latter case π is accepted as the new solution with the
probability exp(Δ/T ), where Δ = Csum (π ) − Csum (π) - and T is a parameter
called a temperature at iteration. The temperature T is getting changed along
iterations by the use of a cooling scheme. A number of iterations, say m, is
performed at the ﬁxed temperature. Although the cooling should be carried out
very slowly, most of the authors consider the change of the temperature at every
iteration (m = 1). Two schemes of the temperature modiﬁcation are commonly
used: geometric Ti+1 = λi Ti and logarithmic Ti+1 = 1/(1 + λi Ti ), i = 1, . . . , N ,
where N is the total number of iterations, λi is a parameter, and T0 is an initial
temperature.
Aarts and van Laarhooven [1] have also made several suggestions concerning the choice of initial solution π 0 , initial temperature T0 , λi , m and the
stopping criterion. They have proposed to select π 0 at random, which helps
with randomizing the search and removing solution dependence on π 0 . The
initial temperature is set to be k = 10 times the maximum value Δmax =
max1≤i≤k (Csum (π i ) − Csum (π i−1 )) between any two successive perturbed solutions when both are accepted, where π i is the current solution in i-th iteration
of algorithm. The initial temperature is set T0 = −Δmax / ln(p), where p = 0.9,

Parallel Simulated Annealing for the Job Shop Scheduling Problem

635

the logarithmic cooling scheme parameter λi = ln(1 + δ)/3σi where δ is the
parameter of closeness to the equilibrium (0.1 - 10.0) and σi is the standard deviation of Csum (π) for all π generated at the temperature T . The value m equals
the number (or its fraction) of diﬀerent solutions that can be reached from the
given one by introducing a single perturbation.
3.1

Neighborhood

The neighborhood N (V, π) of a solution π is deﬁned as a set of new solutions
generated by the set of moves V (π). The move v ∈ V (π) transforms (perturbs)
solution π ∈ Π into another one π (v) ∈ Π. One of the well-known transition
operators for the job shop problem is the swap operator which takes two adjacent operation x and y and insert the operation y into the original position of
operation x and x into the original position of operation y. The swap move can
be unambiguously described by the pair of adjacent operation v = (x, y). The
set of moves V (π) consists of all such moves that can be applied to π. For each
machine i ∈ M there are nk − 1 possible swap moves. Thus, the size of this set
m
and neighborhood is o − m = k=1 (nk − 1). Unfortunately, N (V, π) contains a
huge number of unfeasible solutions as well as a quite large number of solutions
worse than π.
Based on the Theorem 1 and Property 3 we can reduce the set V (π) to the set
X(π) ⊂ V (π) which consists of only feasible and perspective moves. Formally,
the set X(π) is deﬁned by the following formula
X(π) = {(x, y) ∈ V (π) : x = ui (j), y = ui (j + 1),
j = gik , . . . , hki − 1, k = 1, . . . , ri , i ∈ OL }.

(5)

The size of X(π) strongly depends on the distribution of blocks in π.
3.2

The Representative Neighborhood

As already mentioned, in each iteration, the SA algorithm selects in N (X, π)
randomly a single solution neighboring to π. We consider also an alternative
method of selecting neighbors which refers to the idea of representatives, see
Nowicki and Smutnicki, [9], applied to the tabu search method for the permutation ﬂow shop problem with a makespan criterion. This method has been also
successively applied by Yamada and Reeves [12] for the permutation ﬂow shop
problem with the total completion time criterion.
In the representative neighborhood the large original neighborhood N (X, π) is
shared into small subsets (clusters). The representative of the cluster is the best
solution in this cluster. Selection is made among representatives. Notice that
selection of representatives requires signiﬁcantly greater computational eﬀort
than for the conventional SA method. Hence, this method is useful for problems
having eﬀective accelerators and/or eﬀective methods of parallel computing.

636

4

W. Bo˙zejko, J. Pempera, and C. Smutnicki

Parallel Computation of the Objective Function

In this section we propose the original method of parallel computation of Csum
criterion for the given set of neighbors N (π) of the current solution π. At the begin, we will analyze properties of π and π (v) , where v is a move. In the description
we refer to the well known method of computing Ci , i ∈ O values.
Fact 1. Values Ci , i ∈ O, can be found by using the recursive formula
Ci = max(Csi , Cti ) + pi , where C0 = 0.

(6)

The application of (6) is correct if nodes of the graph are revised in a suitable
order. Let Tπ = (t1 , . . . , to ) be a topological order of nodes in graph G(π). Note
that Tπ can be perceived as a permutation of elements from the set O.
Fact 2. The topological order Tπ , for the ﬁxed feasible π, can be found in the
O(o) time.
Fact 3. Values Ci , i ∈ O, for the ﬁxed feasible π, can be found by running (6)
for i = t1 , . . . , to . It requires the O(o) time.
Using Facts (1)–(3) one can propose the following Procedure C of calculating Ci ,
i ∈ O, for the ﬁxed π. The computational complexity of this procedure is O(o).
PROCEDURE C
Step 1. Find the topological order Tπ . If it does not exist return the unfeasible
solution.
Step 2. Calculate values Ci , i ∈ O, by using (6) for i = t1 , . . . , to , where
(t1 , . . . , to ) = Tπ .
Let us analyze the quick method of obtaining Tπ(v) and Ci , i ∈ O, after the
swap move v = (x, y) made from π. Let Tπ−1 be the inverse permutation to
Tπ . The element Tπ−1 (x) denotes the position of x in Tπ . Clearly, we have
Tπ−1 (x) < Tπ−1 (y) for move v = (x, y). It is easy to verify that Tπ(v) can be obtained by reordering in Tπ elements from position Tπ−1 (x) to position Tπ−1 (y). It
takes O(Tπ−1 (y)−Tπ−1 (x)+1) time. For frequently met case Tπ−1 (y) = Tπ−1 (x)+1
the computation complexity is O(1). Unfortunately, regarding to Ci , the change
of completion time of two swapped operations should be broadcasted to all successive operations; then updating of Ci from the position Tπ−1 (x) to position o,
in Tπ(v) obtained by reordering Tπ , requires O(o − Tπ−1 (x) + 1) time.
Now, we are ready to present our parallel computing method dedicated to
the fast calculation of objective function values for a set of neighbors. Among
the known parallel computation models we select the vector calculations which
are the most promising now and easy in hardware implementation. The selected
model is the special case of the single instruction multiple data (SIMD) model.
Assume that the vector processor operates on vectors consisting of s elements. Let Vs = {v1 , . . . , vs } be a subset of parallel computed neighbors and let

Parallel Simulated Annealing for the Job Shop Scheduling Problem
1

s

637

k

C i = (C i , . . . , C i ), where C i , k = 1, . . . , s, denote completion times of operations i ∈ O calculated for the k-th neighbor. In a similar way we deﬁne the vector
1
s
1
2
s
of processing times P i = (P i , . . . , P i ), obviously P i = P i =, . . . , = P i = pi .
For each v = (x, y) ∈ Vs we deﬁne three positions in Tπ : (i) fTπ (v) = Tπ−1 (x)
the ﬁrst position of updating Tπ , (ii) lTπ (v) = Tπ−1 (y) = Tπ−1 (sx ) the last posik
tion of updating Tπ , (iii) lC (v) the last position of updating C . The lC (v) =
−1
Tπ (y) if operation y is the last operation executed on the proper machines and
lC (v) = Tπ−1 (sy ) in the opposite case. Finally, we reorder moves from the set Vs
according to the non-decreasing value of lC (v). The proposed method is outlined
in the Procedure P.
PROCEDURE P
Step
Step
Step
Step
Step
Step

0: set C 0 = 0.
1: for i = 1 to o do
1.1: Calculate values C ti by using (6).
1.2: for each k such that lC (vk ) = i do
1.2.1: execute move vk in π
1.2.2: reorder Tπ from position fTπ (vk ) to lTπ (vk )
k

Step 1.2.3: calculate values C ti from position fTπ (vk ) to lC (vk ).
Step 1.2.4: restore π and Tπ
The initial Step 0 is clear. In the Step 1.1 all values of the vector C ti are
calculated in parallel. This step is performed for t1 , . . . , to and takes O(o) time.
It is easy to verify that computations in Step 1.1 are performed according to
order Tπ determined by π. Therefore, for each πv , v ∈ Vs it is necessary to
recalculate the completion times for all operations whose position have changed,
i.e. from the position fTπ (vk ) to lTπ (vk ) and additionally for the successor of the
operation y.
The computational complexity of the Step 1.2.1 is O(1), for Step 1.2.2 and
1.2.4 is O(lTπ (vk ) − lTπ (vk )). Step 1.2.3 is the most time consuming and takes
O(lC (vk )−lTπ (vk )) time. The total time required by Step 1.2 is O( v∈Vs (lC (v)−
fTπ (v))). In the optimistic case, namely lC (vk ) − lTπ (vk ) = 2 for all k = 1, ..., s,
the computational complexity of the algorithm is O(o + s).
4.1

Multiple Mathematical Instructions

Contemporary used PCs are equipped with processors having the extended instruction MMX set. These special instructions allow one to make vectoring computations. The single instruction operates in a single processor cycle on extended
registers (8 bytes). In the MMX set, it can be distinguished three groups of vector instructions operating on vector 8 × 1, 4 × 2, 2 × 4, where the former number
denotes the vector size and the latter number – the data size. Since for the
tested instances all performed values (Ci , pi ) can be coded on two bytes, we can
perform vectoring operation on the vector of a size equals 4.

638

W. Bo˙zejko, J. Pempera, and C. Smutnicki

The seven main steps of parallel computation of expression (6) using MMX
intrinsics are shown in Fig. 1. In the steps 1 and 2, the MMX registers mm0 and
mm1 are loaded by data following from the vectors C si and C ti respectively.
The calculation of value max(C si , C ti ) are decomposed into two steps: 3 and 4.
At ﬁrst, the value of max(0, C si − C ti ) are calculated by using subtracts with
saturation MMX intrinsic (step 3), the result is stored into the mm0 register.
Afterwards the content of the mm0 register is increased by C ti (step 4). The
values of vector P i are stored in the mm1 register (step 5) and added to the
contents of the mm0 register (step 6). The ﬁnal results (contents of the mm0
register) are stored into memory in the step 7.

Memory

i

Ct

Cs

Ct

Cs

i

4

Step

i

i

3
i

paddusw mmo,mm1

3

2

psubusw mmo,mm1

2

i

1

Pi

2

Pi

3

paddusw mmo,mm1

Cs

Ci

1

1

i

1

Pi

Ct

Cs

i

Ci

3

4

4

4

2

Ci

Pi

Ct

Ci

mm0

mm1

mm0

mm0

mm1

mm0

mm0

1

2

3

4

5

6

7

Fig. 1. Parallel computation of expression (6) for the given operation i

5

Computational Experiments

We have implemented three algorithms based on the simulated annealing method.
In the ﬁrst algorithm PSA-R we have used the representative-based neighborhood. From the neighborhood N (X, π) we can select randomly s = 4 neighbors
and we compute in parallel the value of the objective function. From the set
obtained now we select the best solution. In the second PSA algorithm, for each
solution s = 4 moves from X(π) are generated at random. Moves are applied
in turn to order π until a new solution is accepted. The objective function for
all the solutions is computed in parallel. It is easy to observe that PSA emulates a traditional one-thread simulated annealing (SA) algorithm. The classic
SA algorithm is the third from the implemented and tested algorithms.
Algorithms were coded in Visual C++ 2008 Express Edition, ran on a PC with
Intel Core 2 Duo 2.66 GHz processor and the Windows XP operating system, and
tested on 50 benchmark instances provided by Taillard [13]. The benchmark set

Parallel Simulated Annealing for the Job Shop Scheduling Problem

639

contains 5 groups of hard instances in diﬀerent sizes. For each size (group) n × m :
15 × 15, 20 × 15, 20 × 20, 30 × 15, 30 × 20 a set of 10 instances was provided. In our
tests all the values of tuning parameters for algorithms are found in the automatic
way described in the previous section. Each algorithm was terminated after performed 1000 iterations. At the ﬁxed temperature, the SA-R algorithm performs n
iteration, whereas SA and PSA algorithms 4n iteration, i.e. all algorithms calculated the objective function value for the same number of solutions.
For each test instance and for each run of algorithm we have collected the following values: π ref – reference solution – the best solution found in all runs of
algorithms, P RD = 100 · (Csum (π) − Csum (π ref ))/Csum (π ref ) – the value of
the percentage relative diﬀerence between Csum function values for the solution
π and reference solution π ref , CP U – total computations time (in seconds). For
each instance and for each algorithm based on 10 solutions generated during each
of 10 runs we have calculated the following values: M P RD – minimal PRD value,
AP RD – average PRD value, ACP U – average computation time (in seconds).
Table 1. Computational results of PSA-R, PSA and SA for PRD and CPU values
Group
15×15
20×15
20×20
30×15
30×20

PSA-R
MPRD APRD
1.27
1.38
0.51
0.60
0.20

2.72
3.10
1.72
3.02
2.23

PSA (SA)
MPRD APRD
0.11
0.11
0.23
0.42
1.11

1.07
1.21
1.43
1.45
1.97

ACPU time
PSAR PSA SA
3.9
6.7
11.3
14.8
26.1

10.3 18.7
19.3 34.2
33.6 61.6
46.9 81.2
83.9 147.2

Speedup ratio
PSAR PSA
4.7
5.1
5.4
5.5
5.6

1.8
1.8
1.8
1.7
1.8

Table 1 shows results of computational experiments. The main observation is
that the proposed method of SA algorithm parallelization signiﬁcantly reduces
the computation time. The speedup values are from 4.7 to 5.6 and increase
with the increasing number of machines. It is easy to notice that the speedup is
greater than the theoretical one (≤ 4) (we are obtaining superlinear speedup).
The additional speedup is obtained due to the MMX instruction utilization,
which eliminates branches in the executing code. The branches are essential for
implementation of max function in x86 set of instruction. Comparing computations time of PSA and SA one can observe that the speedup is signiﬁcantly
smaller (1.8). With respect to PRD values it has been pointed that PSA provides
better results than PSA-R for the majority of groups of instance. For the ﬁrst
ﬁve groups the MPRD values are from 0.5 to 1.4 for PSA-R, whereas from 0.1
to 0.4 for PSA (SA). In the last group of instances, conversely PSA-R provides
better results (MPRD=0.2) than PSA-R (MPRD=0.2). It can be notice that for
this group of instances we observe the highest speedup value.

6

Conclusions

To the best of our knowledge, this is the ﬁrst paper which deals with the smallgrain parallelization of algorithms for the job shop problem. A special attention

640

W. Bo˙zejko, J. Pempera, and C. Smutnicki

has been paid to the kind of parallelism which can be easily applied to the new
generation of processors installed in PCs, with multiple cores (dual, quad, etc.)
as well as with the extended set of instructions (such as MMX and SSE2). The
proposed methods have been applied successfully to various simulated annealing
metaheuristics for the job shop problem with Csum criterion.

References
1. Aarts, E.H.L., van Laarhoven, P.J.M.: Simulated annealing: a pedestrain review
of the theory and some aplications. In: Deviijver, P.A., Kittler, J. (eds.) Pattern
Recognition and Applications. Springer, Berlin (1987)
2. Armentano, V.A., Scrich, C.R.: Tabu search for minimizing total tardiness in a job
shop. International Journal of Production Economics 63(2), 131–140 (2000)
3. Bushee, D.C., Svestka, J.A.: A bi-directional scheduling approach for job shops.
International Journal of Production Research 37(16), 3823–3837 (1999)
4. Grabowski, J., Wodecki, M.: A very fast tabu search algorithm for job shop problem. In: Rego, C., Alidaee, B. (eds.) Metaheuristic optimization via memory and
evolution. Tabu search and scatter search, vol. 30, pp. 117–144. Kluwer Academic
Publ., Boston (2005)
5. He, Z., Yang, T., Tiger, A.: An exchange heuristic embedded with simulated annealing for due-dates job-shop scheduling. European Journal of Operational Research 91, 99–117 (1996)
6. Holthaus, O., Rajendran, C.: Eﬃcient jobshop dispatching rules: further developments. Production Planning and Control 11, 171–178 (2000)
7. Jain, A.S., Rangaswamy, B., Meeran, S.: New and stronger job-shop neighborhoods:
A focus on the method of Nowicki and Smutnicki (1996). Journal of Heuristics 6(4),
457–480 (2000)
8. Mattfeld, D.C., Bierwirth, C.: An eﬃcient genetic algorithm for job shop scheduling
with tardiness objectives. European Journal of Operational Research 155(3), 616–
630 (2004)
9. Nowicki, E., Smutnicki, C.: A fast tabu search algorithm for the permutation ﬂow
shop problem. European Journal of Operational Research 19(1), 160–175 (1996)
10. Nowicki, E., Smutnicki, C.: An advanced tabu search algorithm for the job shop
problem. Journal of Scheduling 8, 145–159 (2005)
11. Pezzella, F., Merelli, E.: A tabu search method guided by shifting bottleneck for
the job-shop scheduling problem. European Journal of Operational Research 120,
297–310 (2000)
12. Reeves, C.R., Yamada, T.: Solving the Csum Permutation Flowshop Scheduling
Problem by Genetic Local Search ICEC 1998. In: IEEE International Conference
on Evolutionary Computation, pp. 230–234 (1998)
13. Taillard, E.: Benchmarks for basic scheduling problems. European Journal of Operational Research 64, 278–285 (1993)
14. Wang, T.Y., Wu, K.B.: An eﬁcient conﬁguration generation mechanism to solve
job shop scheduling problems by the simulated annealing. International Journal of
Systems Science 30(5), 527–532 (1999)

