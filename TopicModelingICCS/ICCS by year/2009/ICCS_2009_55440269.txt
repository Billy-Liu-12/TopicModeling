Modular, Fine-Grained Adaptation
of Parallel Programs
Pilsung Kang1 , Naresh K. C. Selvarasu2 , Naren Ramakrishnan1 ,
Calvin J. Ribbens1 , Danesh K. Tafti2 , and Srinidhi Varadarajan1
1

2

Department of Computer Science, Virginia Tech, Blacksburg, VA 24061
Department of Mechanical Engineering, Virginia Tech, Blacksburg, VA 24061

Abstract. We present a modular approach to realizing ﬁne-grained
adaptation of program behavior in a parallel environment. Using a compositional framework based on function call interception and manipulation, the adaptive logic to monitor internal program states and control
the behavior of program modules is written and managed as a separate
code, thus supporting centralized design of complex adaptation strategies
for adapting to dynamic changes within an application. By ‘catching’ the
functions that execute in synchronization across the parallel environment
and inserting the adaptive logic operations at the intercepted control
points, the proposed method provides a convenient way of synchronous
adaptation without disturbing the parallel execution and communication structure already established in the original program. Applying our
method to a CFD (computational ﬂuid dynamics) simulation program to
implement example adaptation scenarios, we demonstrate how eﬀectively
applications can change their behavior through ﬁne-grained control.

1

Introduction

Implementing adaptive execution of an application in a distributed or parallel environment has been of much interest in recent years. The approaches to
support program adaptation include: languages and compilers for specifying
adaptation strategies [1, 2, 3] and runtime platforms or middleware for adaptive execution [4, 5, 6, 7]. These eﬀorts are primarily centered around resource
management to achieve eﬃcient utilization of the environment, such as adaptive
load-balancing and scheduling of application tasks, to match resource constraints
or dynamic operating conditions of the environment. Adaptation schemes are
‘coarse-grained’ in these approaches in that cooperating processes of a distributed application are each abstracted as a task and adaptation strategies
are designed to reassign the tasks onto the resources or to reorganize the execution ﬂow among them. The metrics to initiate adaptation are usually based on
measured history or estimates of the application execution time, which is a function of the environment’s operating conditions such as available resources (e.g.,
number of processors) or physical characteristics of the resources (e.g., network
bandwidth).
G. Allen et al. (Eds.): ICCS 2009, Part I, LNCS 5544, pp. 269–279, 2009.
c Springer-Verlag Berlin Heidelberg 2009

270

P. Kang et al.

Even with such support, however, adaptation schemes where functional behavior needs to be adjusted in response to internal changes to program state
can be hard to design. Key challenges include the need for specifying adaptive parallel control points and monitoring state changes within the program,
the need for executing parallel adaptation without disturbing the original execution ﬂow, and the lack of support for centralizing adaptive logic operations
in a separate module, thereby providing a compositional approach to dealing
with the increased complexity of parallel adaptive applications. On the whole,
coarse-grained approaches such as supported by runtime systems do not provide
mechanisms to access ﬁne-grained aspects of program state or to manipulate
ﬁne-grained behavior of the processes of a parallel application.
To address the issues, this paper presents a modular method for implementing ﬁne-grained adaptive behavior with parallel programs using a function call
interception (FCI) framework called Invoke [8]. Our work makes the following
contributions:
– Factoring out the adaptation logic: A new code that implements the intended
adaptive logic is written in a separate module and inserted by intercepting
the functions of interest in a running application. This enables adaptation
without code modiﬁcation. We speciﬁcally target MPI programs written in
the SPMD (Single Program, Multiple Data) style.
– Fine-grained control: Adaptation of program behavior such as simulation
parameter adjustment or algorithm switching can be initiated in response to
changes in internal computation states.
– Synchronous adaptation: By ‘catching’ global computation functions in the
original program and plugging in new codes at the intercepted places, adaptive operations can be safely carried out without disturbing the parallel
execution structure already established in the original program.

2

Compositional Approach for Parallel Adaptation

Invoke is a composition framework with a set of FCI APIs, through which every
call to a function of interest is intercepted and program control is diverted to an
associated handler, a piece of newly inserted code responsible for monitoring and
modifying the target function’s behavior. By specifying a target function to be
manipulated by Invoke, we essentially deﬁne an adaptive control point over the
original program, where newly developed modules can be introduced to maneuver the program toward the intended adaptive behavior. Thus, as Fig. 1 shows,
composition through Invoke enables one to separately reason about applicationspeciﬁc adaptive strategies, factor them out in a centralized code, and plug in
the adaptation code at control points to build an adaptive application.
2.1

Fine-Grained Program Adaptation

By deﬁning adaptive control points at the interfaces of subprogram modules, the
compositional approach conveniently achieves eﬀective, ﬁne-grained control over

Modular, Fine-Grained Adaptation of Parallel Programs

original program

adaptation code
Invoke

main() {

271

Symbol Table

global_f();

− global_f()
: global_f_hndl()

global_g();

− global_g()
: global_g_hndl()

global_f_hndl():
check & adjust
global_f()

....

check & adjust
global_g()

}
parallel environment

global_g_hndl():

intercept

dispatch

Fig. 1. Composition of an adaptive parallel application using Invoke

application behavior, where adaptation strategies can be designed to monitor
and react to changes in internal program states. Global state variables can be
accessed from the adaptivity code by declaring these variables as external. The
Invoke framework also provides function parameter control APIs, which enables
an extra level of ﬂexibility in ﬁne-grained adaptation. Function arguments are
usually not exposed as globals in a program but still can hold important runtime
program state for certain adaptation purposes. Through the parameter control
APIs, dynamic program states that are communicated between modules can not
only be accessed to check the computational progress, but also be manipulated to
adjust the program’s runtime behavior. We had previously presented such adaptation for sequential environments in [9] but here we focus on parallel execution
environments.
2.2

Synchronous Parallel Adaptation

Implementing parallel adaptive behavior through the existing Invoke compositional framework requires adaptive logic operations to take place synchronously
at clearly deﬁned program control points that are shared across all the participating processes. This is important for implementing ﬁne-grained adaptation strategies with SPMD programs where program behavior needs to change dynamically
in response to changes in program state, because asynchronous adaptation in a
parallel program can cause race conditions among the processes and make the entire computation invalid. For example, if one process changes a global simulation
parameter or algorithm, and continues the computation, before another process
makes the corresponding adaptation, the result may be inconsistent. Therefore,
a synchronous adaptation mechanism is essential for implementing ﬁne-grained
adaptation in a parallel environment, where program behavior (typically in response to changes in internal program states) needs to adapt dynamically.
Synchronous adaptation can degrade performance if the adaptivity code involves extra global communication and synchronization. To mitigate the potential performance slowdown caused by adaptive global operations, we plug in the
adaptivity code at global synchronization points that are already established

272

P. Kang et al.

in the original program, thus placing separate barriers (one from the original
code and the other from the new adaptivity code) close together and making the
combined overhead smaller. By having the adaptivity operations “piggyback”
onto the existing communications that are executed synchronously across the
parallel environment, monitoring and adjusting the program states can also be
performed synchronously without explicitly using extra global operations.

3

Adaptive CFD Simulations

In this section, we apply our framework to the GenIDLEST CFD simulation
code [10] to automatically adjust the simulation time step value and dynamically change the ﬂow model. Written in Fortran 90 with MPI to simulate CFD
problems, GenIDLEST solves the time-dependent incompressible Navier-Stokes
and energy or temperature equations.
The simulated problem is a pin ﬁn array geometry as shown in Fig. 2. Extended surfaces or ﬁns have been used extensively to augment the heat/mass
transfer from or to a surface primarily by increasing the transfer area and/or
increasing the heat/mass transfer coeﬃcient. Reducing the size and weight requirements of equipment necessitates the need for optimal designs of these systems, which in turn requires a detailed understanding of ﬂow and heat transfer
characteristics. The schematic and the geometric parameters of the pin ﬁn array
under consideration, along with the dimensions of interest, are listed in Fig. 2.
The slenderness ratio is set to 1. For the GenIDLEST simulation, we divided the
geometry into 16 block structures so that the maximum degree of parallelism is
16, where each block is assigned to one MPI process.
Program launch

Read input
parameters

Time integration

....
Reduce
global CFL
Record
program states

SD
2.0

SL
1.414214

ST
2.828427

H
1

D
1

Fig. 2. Schematic and Geometric Parameters of Pin Fin Array under Consideration

3.1

root process

MPI Environment
Terminate

Fig. 3. GenIDLEST Execution Flow

Automatic Adjustment of Simulation Time Step

The stability of the simulation depends on the time step size used. Based on observed Courant-Friedrich-Levi (CFL) numbers one could discern if the simulation

Modular, Fine-Grained Adaptation of Parallel Programs

273

is proceeding towards convergence or is becoming unstable. Current practice of
running GenIDLEST simulations records intermediate results at the end of a
preset number of iterations onto the disk, thereby allowing the user to stop the
execution and restart from the last known stable state when the user determines
the running simulation is diverging. By plugging in a simple adaptivity module,
the enhanced GenIDLEST simulation (requiring no modiﬁcations to the original GenIDLEST code) will incrementally adjust the time step value at runtime,
allowing the computation to proceed in a stable manner.
Implementation: Fig. 3 shows the execution ﬂow of the GenIDLEST simulation. At the end of every preset number of iterations, a local CFL number is
calculated by each MPI process, and then the global CFL value is computed using a reduction operation (mpi allreduce) across all the processes. This point is
a good candidate for adaptivity code insertion, since by catching and imposing
operations at this synchronization point, the newly inserted code can also be
executed in synchronization, thereby avoiding dangerous race conditions among
the processes. Furthermore, catching the global reduction call also makes it easy
to monitor the global CFL number because its value is passed as the second
parameter of the function. Invoke’s parameter accessing APIs can be utilized
to access this value. In the adaptive logic, we employed a simple multiplicative increase, multiplicative decrease algorithm with upper (CFL U THRESHOLD)
and lower (CFL L THRESHOLD) threshold values for the CFL number, such that
the time step is increased or decreased by a preset factor if the current CFL
number becomes out of the bounds deﬁned by the thresholds. Importantly, the
entire adaptive logic operations are performed synchronously at the call sites of
mpi allreduce without involving any extra global operations, thereby achieving
eﬃcient parallel program adaptation. The implementation aspects of this and
the following adaptation scenarios are summarized in Table 1.
Experimental Results: Fig. 4 shows the results of GenIDLEST enhanced with
the constructed adaptivity module for the pin ﬁn array simulation, with diﬀerent
initial values of time step ranging from 10−3 to 10−5 . CFL U THRESHOLD and
CFL L THRESHOLD were set to 0.5 and 0.25, respectively. The graphs show how
the CFL value changes as the time step parameter is controlled by the new
Table 1. Implementation Aspects of GenIDLEST Adaptation

Purpose
Type of Scheme
States to Monitor
Control Point
Adaptive Logic
Communication

Change of Time Step
improve stability
automatic adjustment
CFL number communicated by
mpi allreduce
mpi allreduce in calc cfl
adjust time step to conﬁne CFL
number within certain bounds
not necessary

Change of Flow Models
enhance accuracy
user’s dynamic decision
stream-wise velocity written to a log
calc cfl in time integration loop
switch ﬂow model (i les) and activate turbulent data structures
broadcast of user’s decision

274

P. Kang et al.
−3

1

10

10

−3

10
−4
10
−5
10

CFL Number

Time Step

0

−4

−4

1.70×10
1.68×10−4
1.58×10−4

10

−5

10

10

CFL_U_THRESHOLD=0.5

CFL_L_THRESHOLD=0.25
−1

10

−2

0

10
Iteration Number

100

400

10

(a) Time Step Change

0

10
Iteration Number

100

400

(b) CFL Number

Fig. 4. Automatic Adjustment of the Time Step Parameter

module, thereby maintaining the stability of the simulation. Interestingly, it also
shows that the time step in all cases converge to somewhere around 1.7 × 10−4 ,
which might be the optimal value for the model, regardless of diﬀerent starting
values. Therefore, an adaptive logic based on a sophisticated CFD theory might
be devised to ﬁnd the optimal time step for more generalized problems through
our composition method.
3.2

Runtime Change of Flow Models

The predicted heat transfer and ﬂow characteristics depend on the selection of
the appropriate ﬂow model. A fundamental distinction is between laminar and
turbulent ﬂow models, and simulations of interest often require a switch from
one to the other. This problem becomes acute when the Reynolds number is
in the transition region between laminar and turbulent ﬂows. Thus it becomes
important to change the ﬂow model from laminar to turbulent once instabilities
arise in the ﬂow ﬁeld, for a simulation that is started assuming the ﬂow is laminar.
Two Large Eddy Simulation (LES) turbulent models are considered in this
study – Smagorinsky model (SM) and dynamic Smagorinsky model (DSM) [11].
The most commonly used model is the Smagorinsky model, where the eddy viscosity of the subgrid scales is obtained by assuming that the energy production
and destruction are in equilibrium. The drawback of this model is that the model
coeﬃcient is kept constant, while in reality it should vary within the ﬂow ﬁeld
depending on the local state of turbulence. The dynamic Smagorinsky model
computes the model coeﬃcient dynamically, which overcomes the deﬁciencies of
the Smagorinsky model by locally calculating the eddy viscosity coeﬃcient to
reﬂect closely the state of the ﬂow [11]. The advantage of the DSM model is that
the need to specify the model coeﬃcient is eliminated, making the model more
self-contained, but with an additional computational expense of 10-15%.
Implementation: The simulated ﬂow model in GenIDLEST is set initially by
the user through an input speciﬁcation parameter, namely i les: 0 for laminar,

Modular, Fine-Grained Adaptation of Parallel Programs

275

1 for Smagorinsky, and 2 for Dynamic Smagorinsky model. Hence, the program
state needs to be accessed and changed at runtime via this variable. Importantly,
the change should be made synchronously across all processes to maintain the
consistency of the parallel computation. To this end, we plug in the adaptivity
module at the call site of the CFL reduction function as shown in Fig. 3, because
it is executed in synchronization across all the MPI processes, providing a safe
place for carrying out adaptation operations without modifying the original code
and disturbing the parallel execution ﬂow already established in the original
GenIDLEST. Speciﬁcally, the adaptivity code checks if the user wants to change
the ﬂow model, for which we make use of Unix signals (e.g., SIGUSR1) that can
handle immediate, unanticipated user decisions to switch the ﬂow model. These
user-sent signals set a ﬂag in the root process, which will pause accordingly
with a simple user interface in the next iteration to accept the user’s adaptation
decisions, which in turn are broadcast to the other processes.
Experimental Results: The variation of the velocity in the direction of ﬂow
(stream-wise) is plotted in Fig. 5, showing the points in time when the ﬂow models are switched from laminar to SM and then to DSM. The stream-wise velocity
initially decreases, as the simulation proceeds towards the solution, which occurs
till about 0.6 time units. After this simulation time, we see that the stream-wise
velocity tends to vary with time, indicating the development of ﬂow instabilities,
and implying that the initial assumption of laminar ﬂow is no longer valid. Thus
the model is switched to SM at time 1.0. The drawback with the SM model, as
mentioned earlier, is that the model coeﬃcient is set to a constant value, but in
reality the coeﬃcient varies with the local state of turbulence, thus it becomes
imperative to change the model from SM to DSM. This switch is done after a
few hundred iterations (at time 1.4) to make sure that the switch from laminar to turbulent model does not introduce instabilities in the computation. The
switch from laminar to turbulent ﬂow model has a signiﬁcant eﬀect on the heat
transfer. This is shown in Fig. 6(a) and 6(b), which show the variation of the
Nusselt number at the channel walls, which is a measure of heat transfer at that
location. The dotted line shows the region of interest, which is at the front of
the pin in the line of ﬂuid ﬂow. The laminar ﬂow model does not capture the
15

10

Velocity

Change from SM to DSM

5

0

Change from Laminar to SM
-5

0

0.5

1

1.5

2

Time

Fig. 5. Variation of Stream-Wise Velocity with Flow Model Change

P. Kang et al.
.6
28
.4

33.3

.3

28
.8

.6

28.6

.8

28

42

33.3

4
.3 6.
4373.5 6

42
38
38.1.1
.3
33

1442.4
.8

28.6

5.0

5363
5 ..93
426.9
38.8
.1

38.1

.42

33.3

28

(a) Laminar Model

33

2

61.7

.3

3
33.3
333.
3.
33.3

33

42.8

2

6
8.

33.3

33.3

33.3

3

.6

.1

28.6

38.1 33.3

149

38.1

33.3

38

33.3

.5
.5 7 5.
47 4

.3

4.4
128
.6

33.3

14.4

33

19.2

.3

42.8

5.0
5.

.6

33

.3

42.8
383.1
8.1

9.7

.9

.3

33.3
5255.2
225.2
5.2
66.9
.9
14.4
9.7

.3
33

14

.1
385
.872..2
4245.7
61

1928.6
.2

52.2 38.1
42
47.5
9.7 61 .8
.7

33.3
9.7

38.1

33.3

42.8

33.3

.6

..88
4422

33.3

33

28

42

.8

56

33

.9
23
33.3

42.8

.6

3

.6

33.3

.6

28

52.2

28

28.6
33.3
38.
38138
.1 .1

.5
47 .1
38

28

3
3.

.6

28.6
52.2

33.3

28

28.6

28.6
.4
14 28.6
28.6

5.0
28.6
33.3

33

276

(b) Turbulent DSM Model

Fig. 6. Dynamic Flow Model Change from Laminar to Turbulent in a CFD Simulation

heat transfer eﬀects at the front of the pin, predicting lower heat transfer rates
at the pin front than the turbulent model, thus justifying the model switch from
laminar to DSM. This switch shows the capabilities of the adaptive scheme, since
to eﬀect the switch without it would have meant stopping the current execution
and then restarting the simulation after eﬀecting the required change.

4

Adaptation Overhead

The runtime overhead of our adaptation method comes from catching the function calls at adaptive control points, which in itself does not involve any global
operations that cause communication overhead. The catching overhead is measured at 0.10µs per call on average on an AMD Opteron 240 1.4GHz dualprocessor machine with 1GB memory, which translates to 140 CPU cycles. Since
the catching cost is ﬁxed, the relative overhead depends on the number of interceptions and the entire execution proﬁle of an application. That is, the overhead
increases as the number of adaptive control points increases. Still, the catching cost is relatively insigniﬁcant if the application spends most of its time on
executing other parts of the computation than at control points.
4

4

4

x 10

7

original
Invoke

3
2.5
2
1.5

5
4
3
2

1
0.5

original
Invoke

6
Execution Time (sec)

Execution Time (sec)

3.5

x 10

4

8
Number of Processors

16

(a) Time Step Change (500 steps)

1

4

8
Number of Processors

16

(b) Flow Model Change (1000 steps)

Fig. 7. Invoke Overhead with GenIDLEST Simulations

Modular, Fine-Grained Adaptation of Parallel Programs

277

In the adaptive GenIDLEST simulations, control points are intercepted only
once at the end of every preset number of iterations of the time integration
loop, while most of the computing time is spent inside the loop. As a result, the
catching overhead is negligible compared to the whole simulation proﬁle. For
example, Fig. 7 shows execution time of the GenIDLEST simulations where the
Invoke framework is imposed at control points in the time step change and in
the ﬂow model change scenario, respectively, but with no adaptation operations.
Across the 3 conﬁgurations with varying number of processors, the costs for
catching 500 calls of mpi allreduce during 500 time steps in the time step
change example were measured to be less than 0.7% in all cases compared to
the original GenIDLEST simulations (Fig. 7(a)). Similarly, the overhead is less
than 0.95% for catching 1000 calls of calc cfl during 1000 time steps in the
ﬂow model change example (Fig. 7(b)).

5

Related Work

In the language and compiler approaches for implementing program adaptation,
our work is similar to Program Control Language (PCL) [2] in that centralized
design of adaptation strategies can be speciﬁed at a high level for distributed
programs. The expressive power of PCL comes from its underlying framework
which oﬀers a global representation of the distributed program as a graph of task
nodes, the static task graph (STG), connected by edges indicating precedence
relationships. Each adaptation primitive of PCL maps to a sequence of graphchanging operations on the STG of the target program. The Invoke framework
provides more ﬁne-grained control than PCL STGs by supporting monitoring
and manipulating of state variables internal to a program.
In Grid and cluster computing, there is a large body of research work on
runtime platforms for supporting program adaptation at the level of middleware or runtime platforms [4,5,6,7]. However, as their objective is to implement
middleware support for adaptation between the application and the underlying
execution layer, these eﬀorts focus on resource management towards eﬃcient
utilization of the environment, such as load-balancing and scheduling of application tasks, where coarse-grained strategies based on resource constraints or
external operating parameters are employed. In contrast, our work implements
a parallel adaptation framework that can adjust ﬁne-grained aspects of program state and behavior by monitoring dynamic progress of the computation
itself.
Dynamic binary instrumentation tools such as DynInst [12] oﬀer a modular,
language-independent way of code modiﬁcation, so that new code modules can be
transparently combined with existing software. Since the accompanying overhead
is signiﬁcant while they perform code instrumentation at program runtime, they
are usually developed for sophisticated programs analysis purposes [13] rather
than as a tool to realize program behavior adaptation.

278

6

P. Kang et al.

Conclusions

The proposed compositional framework oﬀers a modular way of implementing
ﬁne-grained program adaptation in a parallel environment. By deﬁning adaptive
control points at the functions that execute in synchronization across the parallel
environment, adaptive logic operations can safely be executed without interfering
with the parallel execution structure of the original program. In future work,
we intend to deﬁne ‘adaptivity schemas’ that abstract our recurring templates
of adaptivity and that can be ‘weaved’ over an unmodiﬁed program, akin to
aspect oriented programming. We also intend to explore more dynamic and less
synchronous scenarios of parallel program adaptation.

References
1. Voss, M.J., Eigemann, R.: High-level Adaptive Program Optimization with
ADAPT. In: PPoPP 2001: Proceedings of the 8th ACM SIGPLAN Symposium
on Principles and Practices of Parallel Programming, pp. 93–102. ACM, New York
(2001)
2. Ensink, B., Stanley, J., Adve, V.: Program Control Language: A Programming Language for Adaptive Distributed Applications. J. Parallel Distrib. Comput. 63(11),
1082–1104 (2003)
3. Du, W., Agrawal, G.: Language and Compiler Support for Adaptive Applications.
In: SC 2004: Proceedings of the 2004 ACM/IEEE Conference on Supercomputing,
Washington, DC, USA, pp. 29–29. IEEE Computer Society Press, Los Alamitos
(2004)
4. Kennedy, K., Mazina, M., Mellor-Crummey, J.M., Cooper, K.D., Torczon, L.,
Berman, F., Chien, A.A., Dail, H., Sievert, O., Angulo, D., Foster, I.T., Aydt, R.A.,
Reed, D.A., Gannon, D., Johnsson, S.L., Kesselman, C., Dongarra, J., Vadhiyar,
S.S., Wolski, R.: Toward a Framework for Preparing and Executing Adaptive Grid
Programs. In: IPDPS 2002: Proceedings of the 16th International Parallel and
Distributed Processing Symposium, Washington, DC, USA, pp. 171–175. IEEE
Computer Society Press, Los Alamitos (2002)
5. Buaklee, D., Tracy, G.F., Vernon, M.K., Wright, S.J.: Near-Optimal Adaptive Control of a Large Grid Application. In: Proceedings of the 16th International Conference on Supercomputing, pp. 315–326. ACM Press, New York (2002)
6. Berman, F., Wolski, R., Casanova, H., Cirne, W., Dail, H., Faerman, M., Figueira,
S., Hayes, J., Obertelli, G., Schopf, J., Shao, G., Smallen, S., Spring, N., Su, A.,
Zagorodnov, D.: Adaptive Computing on the Grid using AppLeS. IEEE Transactions on Parallel and Distributed Systems 14(4), 369–382 (2003)
7. Janjic, V., Hammond, K., Yang, Y.: Using Application Information to Drive Adaptive Grid Middleware Scheduling Decisions. In: Proceedings of the 2nd Workshop
on Middleware-Application Interaction, pp. 7–12. ACM, New York (2008)
8. Heﬀner, M.A.: A Runtime Framework for Adaptive Compositional Modeling. Master’s thesis, Blacksburg, VA, USA (2004)
9. Kang, P., Cao, Y., Ramakrishnan, N., Ribbens, C.J., Varadarajan, S.: Modular
Implementation of Adaptive Decisions in Stochastic Simulations. In: SAC 2009:
Proceedings of the 24th Annual ACM Symposium on Applied Computing, pp.
995–1001 (March 2009)

Modular, Fine-Grained Adaptation of Parallel Programs

279

10. Tafti, D.: GenIDLEST - A Scalable Parallel Computational Tool for Simulating
Complex Turbulent Flows. In: Proceedings of the ASME Fluids Engineering Division (FED), vol. 256, ASME-IMECE (November 2001)
11. Germano, M., Piomelli, U., Moin, P., Cabot, W.H.: A Dynamic Subgrid-Scale Eddy
Viscosity Model. Physics of Fluids A: Fluid Dynamics 3(7), 1760–1765 (1991)
12. Buck, B., Hollingsworth, J.K.: An API for Runtime Code Patching. Int. J. High
Perform. Comput. Appl. 14(4), 317–329 (2000)
13. Schulz, M., Ahn, D., Bernat, A., de Supinski, B.R., Ko, S.Y., Lee, G., Rountree, B.:
Scalable Dynamic Binary Instrumentation for Blue Gene/L. SIGARCH Comput.
Archit. News 33(5), 9–14 (2005)

