Nearest Neighbor Convex Hull Classification Method
for Face Recognition
Xiaofei Zhou1,* and Yong Shi1,2
1

Research Center on Fictitious Economy and Data Science, Chinese Academy of Sciences,
Beijing 100190, China
2
College of Information Science and Technology University of Nebraska at Omaha,
Omaha, NE 68182, USA
zhouxf@gucas.ac.cn,
yshi@gucas.ac.cn, yshi@unomaha.edu

Abstract. In this paper, nearest neighbor convex hull (NNCH) classification
approach is used for face recognition. In NNCH classifier, a convex hull of
training samples of a class is taken as the distribution estimation of the class, and
Euclidean distance from a test sample to the convex hull (the distance is called
convex hull distance) is taken as the similarity measure for classification. Experiments on face data show that the nearest neighbor convex hull approach can
lead to better results than those of 1-nearest neighbor (1-NN) classifier and SVM
classifiers.
Keywords: classification, SVM, convex, nearest neighbor convex hull, face
recognition.

1 Introduction
As one of most important biometrics technologies, face recognition has become an
active research in pattern recognition and data mining area. Similar to many pattern
recognition problems, solving classification problem is a key for face recognition.
During the past 20 years many classification methods have been successfully applied in
face recognition, such as 1-NN [1,2], Neural Networks [3], SVM [4], HMM [5] etc. As
a non-parametric pattern recognition approach, the single nearest neighbor (1-NN)
classifier is often used for classification. 1-NN is a most intuitive approach based on the
nearest neighbor rule, which decides a query to the class including the nearest prototype
to it. However, the performance of 1-NN is limited by the available prototypes in each
class, which depends on how prototypes are chosen to account for possible sample
variations and also how many prototypes are available. Practically no matter how
representative the prototypes may be, there are always un-prototyped viewings, because only a finite, often small, number of prototypes are available as compared to all
possibilities [6]. To adapt for more prototypes changes than the original prototypes,
many classification approaches have been presented by using the linear combinations
*

Corresponding Author.

G. Allen et al. (Eds.): ICCS 2009, Part II, LNCS 5545, pp. 570–577, 2009.
© Springer-Verlag Berlin Heidelberg 2009

Nearest Neighbor Convex Hull Classification Method for Face Recognition

571

of the prototypes to expand the representational capacity of them, such as NLC [6],
NFL [7], NFP and NFS [8], NNL and NNP [9] etc. Some classifiers also utilize convex
hull of prototypes to represent training set. In [10], k-local convex distance is used as
measure, and k-local convex hulls of each class are used to represent the class. Different from the above method, Jiang et al. [11] and Zhou et al. [12] present a new
classification idea called nearest neighbor convex hull (NNCH) classification, which
utilizes convex hull of all prototypes per class to represent each class. NCH method
[13] mentioned by Nalbantov et al. has similar ideas with NNCH. The approaches
mentioned above are all based on the nearest neighbor rule and provide an infinite
number of prototypical points to represent the query. In this paper, we apply the NNCH
method [11,12] for face recognition. Like 1-NN, NNCH is a non-parameters method,
which uses convex hull of prototypes to represent each class, and Euclidean distance
between the query and the convex hull is used as the measure for nearest neighbor
classification. The experiments on Yale face database and FERET face database show
good performance.
The rest of the paper is organized as follows: Section 2 introduces foundations of
NNCH, Section 3 describes NNCH method, Section 4 presents experimental results on
face recognition.

2 Foundations of NNCH
Definition 1 [Convex Set]. Let G ⊆ R d . Say that G is convex set if, for each x1 and
x 2 in G , the line segment between x1 and x 2 is contained in G : ∀x1 , x 2 ∈G and
λ ∈ [0,1] , λx1 + (1− λ )x 2 ∈G .

That is, G is convex if, for all x1 and x 2 in G and λ ∈[0,1] , the point λx1 + (1− λ )x 2 lies
in G .
Given a set S with n elements { x1 , x 2 ,…, x n } , where x i = ( xi1 ,…, xid )T is d dimensional
vector in a feature space. We use co( S ) to denote the convex hull of S , co( S ) is the
smallest convex set containing set S :
Definition 2 [Convex Hull]. The convex hull of a set S ⊂ R d is the smallest convex set
n

n

i =1

i =1

containing set S : co( S ) = {∑α i x i | x i ∈ S ,α i ≥ 0, ∑α i = 1, i = 1,2,"n} .
The convex hull of a set S is simply the set of all linear combinations of elements
of S in which the coefficients of elements of S are nonnegative and sum to 1. Such
constrained linear combinations are known as convex combinations.
In this paper the distance measure based on convex hull is used for classification,
i.e., the distance from a query to a convex hull of training samples of a class is taken as
the similarity measure. Virtually, the distance problem is a projection problem. Given a
nonempty set G ⊂ R d and a vector y , the projection problem is the problem of determining the point xˆ ∈ G that is the closest to y among all x∈G (with respect to the
Euclidean distance). Formally, the problem is given by

572

X. Zhou and Y. Shi

min y − x

2

x∈G

(1)

When the set G is a closed and convex set, the solution exists and it is unique, as seen
in the following theorem.
Theorem 1 [Projection onto Closed Convex Set]. Let H is a Hilbert space, and
G ⊂ H be a nonempty closed convex set and y∈ H .
• (Existence and uniqueness) There is a unique xˆ ∈G such that
xˆ = arg min x∈G y − x . xˆ is called the projection of y onto G and is denoted by p G ( y ) .

ˆ S is the projection p G ( y ) if < y − xˆ , x − xˆ >≤ 0 for
• (Characterization) A point x∈
all x∈ S .
The theorem guarantees the existence and uniqueness of the projection of a vector on a
closed convex set. That is, for a nonempty closed convex set G ⊂ H and arbitrary y∈ H , d ( y, G ) = min y − x , where d ( y, G ) can be computed, and there is a
x∈G

unique xˆ ∈ G such that y − xˆ = min y − x . The projection xˆ is the convex combination
x∈G
that is nearest to the query y . The distance between the query and nearest point in the
convex hull is used as the similarity of the query and the class set. The unique solution y to the projection problem is referred to as the projection of xˆ on co(S ) .

3 Nearest Neighbor Convex Hull Classifier
Inspired by both nearest neighbor rule and the geometric interpretation of SVM, Jiang
et al. [11] and Zhou et al. [12] presented a new classification method: nearest neighbor
convex hull (NNCH) method. They use convex hull to extend the representation of
prototypes and adopt nearest neighbor rule to realize classification.
The idea of NNCH is to expand representational capacity of prototypes of each class
by convex combinations. This virtually provides an infinite number of prototypical
points, and thus can account for more prototypical changes than the original prototypes.
In the calculation of distance between a query vector and a class, the query is projected
to the convex hull spanned by the prototypes of this class. The projection point is the
convex combination that is nearest to the query. The distance between the query and
convex hull is used as the basis for classification. Based on such distances, the conventional 1-NN classification, which compares each prototype individually, is extended to the nearest neighbor convex hull classification, which compares the convex
hulls of each class: the query is classified to the class of the nearest convex hull.
The idea using convex hull of all samples in a class to represent the class is also derived from the geometric interpretation of SVM. SVM is a robust methodology [14, 15]
for classification. Intuitively, given a set of points belonging to two classes, SVM finds
the optimal hyperplane that separates binary class data without errors, while maximizing
the distance from either class to the hyperplane. In geometry, this is equivalent to separating the convex hulls of all samples in each class with maximal margin [16, 17]. So the

Nearest Neighbor Convex Hull Classification Method for Face Recognition

573

convex hull of all samples in a class can represent the class space of samples well. In
NNCH, such convex hulls are considered to extend the representation of prototypes. But
unlike maximal margin rule of SVM, NNCH uses the nearest neighbor rule to classify. In
fact, NNCH can be considered as an approach separating the convex hull by nearest
neighbor rule. In reference [10], k-local convex distances are used for classification,
which emphasis the local linear construction of sample distribution and use local convex
hulls to represent the class. But in NNCH, general convex hull of a class is presented to
represent sample distribution, which is inspired by the convex hull in SVM.
3.1 Convex Hull Distance

In NNCH classifier, a convex hull of training samples of a class is taken as distribution
estimation of the class, and Euclidean distance from a test sample to the convex hull
(the distance is called convex hull distance) is taken as the similarity measure for
classification.
Given a set S ⊂ R d , S = { x1 , x2 ,…, x k } , the distance function between a query x and
the convex hull of S can be written in detail:

x −η
d 2 ( x, co( S )) = ηmin
∈co ( S )

2

2

k

= min x − ∑ α i x i
α

i =1

k

k

k

= min[( x ⋅ x ) − 2∑α i ( x ⋅ x i ) + ∑∑α i α j ( x i ⋅ x j )]
α

i =1

(2)

i =1 j =1

k

s.t.

∑α i = 1, α i ≥ 0, i = 1,2 ," ,k
i =1

Let X = ( x1 , x 2 ,", x k ) , e = (1,1,",1)1×k , 0 = (0,0,",0)1×k , η = Xα , e T α = 1,α ≥ 0 . Equation (2) can be written in matrix:
T

T

d 2 ( x, co( S )) = min x − η

2

y ∈co ( S )

= min
x − Xα
α

2

= min ( x x − 2 x Xα + α X Xα )
T

T

T

(3)

T

α

s.t.

e α = 1, α ≥ 0
T

As x T x is constant, which can be discard from the optimal problem. The optimal
problem we need to solve is
min 2 x T Xα + α T X T Xα
α

s.t.

e T α = 1 ，α ≥ 0

(4)

Equation (4) is a convex quadratic optimization. In this paper, we use the MATLAB
optimal tools to realize the solution.
T
Supposed α ∗ = (α1∗ ,α 2∗ ,",α k∗ ) is the optimum solution for (4), projection y is the
convex combination of elements in S, with the coefficient α 1∗ ,α 2∗ ,",α k∗ :

574

X. Zhou and Y. Shi
k

y = ∑ α i∗ xi

(5)

i =1

Then we can compute d 2 ( x,co( S )) by y:
k

d 2 ( x,co( S )) = x − y = x − ∑α i∗ xi
2

2

.

(6)

i =1

3.2 Nearest Neighbor Convex Hull Algorithm

We assume to have a multi-labeled training set S = {( x1 ,c1 ),...,( x n ,c n )} ,
where x i ∈ R d is a sample and ci ∈{1,...,l} is its corresponding class or label, i.e. there
are l categories training sets: S1 = { xi | ci =1} , S 2 ={ x i | ci = 2} , … , S l = { xi | ci = l} .
The l convex hulls from different category training sets are:
⎧
co( S1 ) = ⎨ ∑α i xi
⎩i ,ci =1
…,
⎧
co( S l ) = ⎨ ∑α i xi
⎩i ,ci =l

⎫

∑α i =1, α i ≥ 0⎬ ,
⎭

i ,ci =1

⎫

∑α i =1, α i ≥ 0⎬ .
⎭

i ,ci =l

For an arbitrary query ( x ,c) , x ⊂ R d , c is class label which is unknown, we need to
give the c value. We respectively compute the square distance between x and each class
convex hull:
d 2 ( x ,co( S1 )) = η min
x − η1 ,
∈co ( S )
2

1

1

…,
d 2 ( x , co( Sl )) = min x − ηl .
2

ηl ∈co ( Sl )

We take d 2 ( x,co( S j )) as the similarity of x and the jth class, and classify x to the
class of the nearest neighbor convex hull:
c = arg min d 2 ( x,co( S j )), j =1,2,",l .
j

(7)

Compared with 1-NN, NNCH presents infinite samples convex combined by training samples, whereas 1-NN has the limited training set. Compared with SVM, NNCH
can be directly used for multi-class problems. For SVM, it usually needs to decompose
multi-class problems into many two-class problems.

4 Experiments
We apply NNCH classifier on face recognition. The comparison experiments of NNCH
with 1-NN and SVM are conducted on two face databases, ORL and FERET face

Nearest Neighbor Convex Hull Classification Method for Face Recognition

575

databases. For SVM, three kernels, linear kernel k = ( x ⋅ y ) , quadratic ker-

‖ ‖

nel k = (1 + ( x ⋅ y )) 2 and gaussian kernel k=exp (-0.5( x-y /σ)2) are chosen, and a
bottom-up binary tree-structured approach [4] is used to extend SVM to deal
with multi-class problems. All of our experiments are carried out under Matlab 7.0
platform.
The first experiment is performed on ORL face database (http://www.cl.cam.ac.uk/
Research/DTG/attarchive/facedatabase.html). ORL database contains 40 distinct persons. Each person has ten different images labeled with the number 1,2, … ,10. The
original face images were all sized 92×112 pixels with 256-level gray scale. Fig.1
shows ten face images of one subject. In our experiments, all images are transformed to
JPEG format, and downsampled to 16×16 by bicubic interpolation. We use the first two
images of each individual and their mirrors as training set. The remaining 320 images
are as test set.

Fig. 1. Ten face images of one person in ORL face database

Table 1 gives the comparisons of different methods on face recognition accuracy.
The penalty parameter of SVMs adopt C=∞, and for gaussian kernel SVM, σ=3.
Table 1. Results of experiments on ORL face database
Classification methods
1-NN
SVM(Linear kernel)
SVM(quad kernel)
SVM(gaussian kernel)
NNCH

Recognition accuracy
86.56%
85.63%
83.00%
86.88 %
87.81%

The second experiment is performed on a subset of FERET face database
(http://www.itl.nist.go/iad/humanid/feret /feret_master.html). The subset contains 200
distinct persons. Each person has seven different images labeled with“ba, bd, be, bf, bg,
bj, bk”. The original face gray images are cropped to 80 × 80 pixels. Fig.2 shows the
cropped face images from one subject.

576

X. Zhou and Y. Shi

ba

bd

be

bf

bg

bj

bk

Fig. 2. Seven face images for one person in FERET face database

In our experiments, the images are also downsampled to 40×40 by bicubic interpolation, and the illumination normalization technique is done for compensating illumination variations of all faces. The first four images (labeled with“ba, bd, be, bf”) of
each person and their mirrors are used for training, and the rest 600 images are used for
testing.
Table 2 gives the comparisons of different methods on face recognition accuracy.
The penalty parameter for SVMs adopt C=∞, and for gaussian kernel SVM, σ=50.
Table 2. Results of experiments on FERET face database
Classification methods
1-NN
SVM(linear kernel)
SVM(quadratic kernel)
SVM(gaussian kernel)
NNCH

Recognition accuracy
75.00%
81.50%
83.00%
83.67%
86.50%

From the results of above experiments, it is clear that as a whole the face classification
performance of the NNCH is competitive with that of 1-NN and SVMs. In Table 1 and
Table 2, NNCH on face recognition all can obtain the highest recognition accuracy
among the methods. In Table 1, the recognition rate of NNCH (87.81%) is higher than
1-NN (86.56%) and three kernel SVMs (85.63%, 83.00% and 86.88 %); In Table2, the
recognition rate of NNCH is 86.50%, which is much higher 11.5% than 1-NN, also
higher 5% than linear SVM, and higher almost 3% than nonlinear SVMs.

5 Conclusions
This paper introduces a novel pattern classification method called nearest neighbor
convex hull (NNCH) approach for face recognition. In NNCH, a convex combination
of vectors belonging to a class is used to define a measure of distance from a query
vector to the class. The measure is defined as the Euclidean distance from the query to
the nearest convex combination. Experiments on face data show that NNCH leads to
better results than those of 1-nearest neighbor (1-NN) classifier and SVM classifiers.
Acknowledgments. This work was partially supported by the National Grand Fundamental Research 973 Program of China under Grant No.2004CB720103, by the National
Nature Science Foundation of China under Grant No.70531040, No.70621001,
No.10601064 and No.70501030 and by a research grant from BHP Billion Co., Australia.

Nearest Neighbor Convex Hull Classification Method for Face Recognition

577

References
1. Jin, Z., Yang, J.Y., Hu, Z.S., Lou, Z.: Face Recognition based on Uncorrelated Discriminant
Transformation. Pattern Recognition 34(7), 1405–1416 (2001)
2. Yang, J.Y., Zhang, David, Yang, J.Y., et al.: Two-Dimensional PCA: A New Approach to
Appearance-Based Face Representation and Recognition. IEEE PAMI 26, 1131–1137
(2004)
3. Ranganath, S., Arun, K.: Face Recognition Using Transform Features and Neural Networks.
Pattern Recognition 30(10), 1615–1622 (1997)
4. Guo, G., Li, S.Z., Chan, K.L.: Face Recognition by Support Vector Machines. In: Proc.
fourth IEEE International Conference on Automatic Face and Gesture Recognition, pp.
196–201 (2000)
5. Samaria, F., Young, S.: HMM based Architecture for Face Recognition. Image and Computer Vision 12, 537–543 (1994)
6. Li, S.Z.: Face Recognition based on Nearest Linear Combinations. In: IEEE Computer Society Conference on Computer Vision and Pattern Recognition, pp. 839–844 (1998)
7. Li, S.Z.: Performance Evaluation of the Nearest Feature Line Method in Image Classification and Retrieval. IEEE Trans. On PAMI 22(11), 1335–1339 (2000)
8. Chen, J.T., Wu, C.C.: Discriminant Waveletfaces and Nearest Feature Classifiers for Face
Recognition. IEEE Trans. On PAMI 24(12), 1644–1649 (2002)
9. Zheng, W.M., Zou, C., Zhao, L.: Face Recognition Using Two Novel Nearest Neighbor
Classifiers. In: Proceedings of IEEE International Conference on Acoustics, Speech and
Signal Processing, vol. 5, pp. v-725–728 (2004)
10. Vincent, P., Bengio, Y.: K-local Hyperplane and Convex Distance Nearest Neighbor Algorithms. In: Dietterich, T.G., BeckerS, G.Z. (eds.) Advances in Neural Information Processing Systems 14, pp. 985–992. MIT Press, Cambridge (2002)
11. Jiang, W.H., Zhou, X.F., Yang, J.Y.: P-norm Nearest Neighbor Convex Hull Classification
Algorithms (in Chinese). Journal of Harbin Institute of Technology 38, 982–984 (2006); In:
Proceedings of the 7th Chinese Symposium on Intelligent Robot(2006)
12. Zhou, X.F., Jiang, W.H., Yang, J.Y.: Kernel Nearest Neighbor Convex Hull Classification
Algorithm (in Chinese). Journal of Image and Graphics, 1209–1213 (2007)
13. Nalbantov, G.I., Groenen, P.J.F., Bioch, J.C.: Nearest Convex Hull Classification, Report
(2007), http://repub.eur.nl/publications/index/728919268/NCH10.pdf
14. Vapnik, V.N.: The Nature of Statistical Learning Theory. Springer, New York (1995)
15. Deng, N.Y., Tian, Y.J.: A New Approach in Data Mine-Support Vector Machine. Science
Press, Beijing (2004)
16. Bennett, K.P., Bredensteiner, E.J.: Duality and Geometry in SVM Classifiers. In: Langley,
P. (ed.) Proceedings of the 17th International Conference on Machine Learning, pp. 57–64.
Morgan Kaufmann, San Francisco (2000)
17. Keerthi, S.S., Shevade, S.K., Bhattacharyya, C., Murthy, K.R.K.: A Fast Iterative Nearest
Point Algorithm for Support Vector Machine Classifier Design. IEEE Transactions on
Neural Networks 11(1), 124–136 (2000)

