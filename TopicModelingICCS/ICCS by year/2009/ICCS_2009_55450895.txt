Agent Strategy Generation by Rule Induction
in Predator-Prey Problem
´ zy´
Bartlomiej Snie˙
nski
AGH University of Science and Technology,
Department of Computer Science Krak´
ow, Poland
Bartlomiej.Sniezynski@agh.edu.pl

Abstract. This paper contains a proposal of application of rule induction for generating agent strategy. This method of learning is tested on
a predator-prey domain, in which predator agents learn how to capture
preys. We assume that proposed learning mechanism will be beneﬁcial
in all domains, in which agents can determine direct results of their
actions. Experimental results show that the learning process is fast.
Multi-agent communication aspect is also taken into account. We can
show that in speciﬁc conditions transferring learned rules gives proﬁts to
the learning agents.
Keywords: multi-agent systems, rule induction, machine learning.

1

Introduction

Decentralized problem solving becomes more and more popular. Architecture,
which can be used for this purpose is multi-agent system. The problem is that in
complex environments it is very diﬃcult (or sometimes impossible) to specify and
implement all system details a priori. A solution is a learning method application,
which adopts the system to the environment. Learning can be also useful if the
environment is not stationary.
To apply learning in a multi-agent system, one should chose a method of
learning. There are many algorithms developed so far. However; in multi-agent
systems most applications use reinforcement learning.
In this paper we show that rule induction can be successfully applied to generate agent strategy in the predator-prey domain, in which reinforcement learning
and evolutionary computation methods were applied.
Rule induction is a supervised learning method, therefore, in the contrast to
the methods mentioned above, it needs labeled examples (e.g. percepts-action
pairs) to generate knowledge. We show, that in this domain an agent is able
to generate such examples. The reason is that results of actions performed by
learning agents are visible immediately. Any supervised learning method can be
applied in a similar way in all environments with such a property.
In the following sections, related research considering learning in multi-agent
systems is brieﬂy discussed, the developed system is described, and experimental
results are presented and analyzed.
G. Allen et al. (Eds.): ICCS 2009, Part II, LNCS 5545, pp. 895–903, 2009.
c Springer-Verlag Berlin Heidelberg 2009

896

2

´ zy´
B. Snie˙
nski

Learning in Multi-agent Systems

Good survey of learning in multi-agent systems can be found in [1] and [2].
As it was mentioned above, the most popular learning technique used is reinforcement learning that allows to learn agent strategy: what action should be
executed in a given situation. Other techniques can be also applied: neural networks, models coming from game theory as well as optimization techniques (like
the evolutionary approach, tabu search etc.). However, optimization techniques
improve performance of the system using many populations of agents instead of a
single agent.
Reinforcement learning allows to generate a strategy for an agent in a domain, in which the environment provides some feedback after the agent has
acted. Feedback takes the form of a real number representing reward, which
depends on the quality of the action executed by the agent in a given situation. The goal of the learning is to maximize estimated reward. This method
was successfully applied in the predator-prey domain [3]. In this work predator
agents use reinforcement learning to learn a strategy minimizing time to catch a
prey. Additionally, agents can cooperate by exchanging sensor data, strategies,
or episodes. Experimental results show that cooperation is beneﬁcial. Other researchers working on this domain successfully apply genetic programming [4]
and evolutionary computation [5].
There is only a few works known to the author on supervised learning in
multi-agent systems. Rule induction is used in a multi-agent solution for vehicle
routing problem [6]. However; in this work learning is done oﬀ-line. First, rules
are generated by AQ algorithm (the same as used in this work) from traﬃc data.
Next, agents use these rules to predict traﬃc. In [7], agents learn coordination
rules, which are used in coordination planning. If there is not enough information during learning, agents can communicate additional data during learning.
Airiau [8] adds learning capabilities into BDI model. Decision tree learning is
used to support plan applicability testing.
In [9] there is a comparison of supervised learning and reinforcement learning
methods in a Fish-Banks game. Agents run ﬁshing companies and learn how to
allocate ships. In this case, agents using rule induction perform slightly better
than ones using reinforcement learning.
Universal architecture for learning agent can be found in [10]. It ﬁts mainly
reinforcement learning. Sardinha et. al, propose a learning agent design pattern,
which can be used during system implementation [11]. More abstract architecture, which is used in this work, is presented in [12].

3
3.1

System Architecture
Predator-Prey Domain

Predator-prey domain is a simple simulation with two types of agents: predators,
and preys. The aim of a predator is to hunt for a prey. Environment is a grid
world with size n×n with ”glued” opposite edges (it is a torus). Time is discrete,

Agent Strategy Generation by Rule Induction in Predator-Prey Problem

897

its ﬂow is represented by turns. In every turn all agents receive percept data from
the environment and chose their actions, which are next executed.
Agents can move in the four directions (up, down, left, and right) or not move
at all. If predator occupies a ﬁeld next to the prey, the prey is captured.
Predators have limited range of sight. If prey is closer than a given threshold,
predator gets information about type of the prey, and its relative position. If
more then one prey is in range, only the closer one is visible.
Two types of the preys are deﬁned: bird and mouse. First one moves up, down
or does not move at all with equal probability. Similarly, mouse move left, or
right, or does not move.
Two types of predators are deﬁned: random and learning. The former moves
in the four directions or do not move with equal probability. The latter uses rule
induction to improve performance.
3.2

Learning Predator

Learning predator architecture is presented in Fig. 1. In this application percepts
received by the agent is nul if no prey is in the observation range, or is a triple
(t, dx, dy), where t is a type of a prey, and dx, dy are relative coordinates of the
prey along X and Y axis, respectively.
Initially, processing module selects actions from the set {none, up, down, left,
right} randomly. If, after actions execution, distance to the prey decreases, new
example is stored in the training data memory. Percept triple from the previous
round form attribute values of the example and action executed is its class. Here
important property of the environment is used. It is possible to generate training
data because action results are immediately visible.
Rules generated during learning have a form p1 , p2 , . . . , pn → a, where pi are
tests on the attributes representing current percepts, and a is an action, which
should be executed.
During a learning phase training data is sent to the learning module. In this
research AQ21 rule induction program is used [13]. It is the latest implementation

Fig. 1. Learning agent architecture

898

´ zy´
B. Snie˙
nski

of AQ algorithm [14]. Rules are generated using sequential covering: the best
rule (e.g. giving correct answer for the most examples) is constructed by a beam
search, examples covered by this rule are eliminated from the training set, and
the procedure repeats.
AQ21 program is executed in a theory formation mode. It means that rules generated are sound and complete. Ambiguity option is set to IncludeInMajority.
If there are more events in the training data with the same values of all attributes
and diﬀerent classes (actions), learning algorithm assigns to the event the majority class. All other options are set to their default values.
In the future various option settings can be tested. However; it is one of
advantages of rule induction application for agent strategy generation: tuning of
parameters is not necessary to get good results.
If knowledge is generated, processing module can use learning module to chose
an action in the current situation. Input to the learning module is a problem,
which consists of a percept triple, and output – answer is a conclusion of the
rule matching the percepts or nul if the rule is not found (in such a case agent
executes random action).
In this research two types of communication were considered for the predator
agent: exchange of generated rules and exchange of the training data. If no
matching rule is found, predator agent can ask another agent for a matching
rule and store it in its knowledge base. Exchange of the training data means
that during learning agent asks for examples stored in other agent’s memory
to generate better knowledge. During experiments only the former case was
tested. Results are initial, but we can show that in speciﬁc conditions transferring
learned rules gives proﬁts to the learning agents.
3.3

Implementation

The software used in experiments is written in Prolog, using Prologix compiler [15].
Prologix is an extension of BinProlog that has many powerful knowledge-based extensions (e.g. agent language LOT, Conceptual Graphs and KIF support). AQ21
program [13] is executed from Prolog and rules generated are added to the code
as additional horn clauses.

4
4.1

Experimental Results
Setup

Let us introduce several deﬁnitions. Game is deﬁned as a sequence of turns
beginning with the initial positions of agents and ending in a turn when all preys
are captured. Twelve consecutive games is called a sequence. It is assumed that
memory and knowledge base of learning predators is kept unchanged between
games in a sequence. However, it is cleared between sequences. Performance of
predators is measured by a number of turns in a game; the less, the better.

Agent Strategy Generation by Rule Induction in Predator-Prey Problem

899

160
140
120
100
80
60
40
20
Avg. Perf.
Std. Dev.
0
2

4

6

8

10

12

Fig. 2. Average performance and its standard deviation for random agents in consecutive games

160
140
120
100
80
60
40
20
Avg. Perf.
Std. Dev.
0
2

4

6

8

10

12

Fig. 3. Average performance and its standard deviation for learning agents (without
communication) in consecutive games

Grid world in experiments has dimensions 16×16. Initial positions of predators
are (0, 0) and (8, 8) to maximize distance between them. There are two prey
agents: bird and mouse. In all experiments 60 sequences of games are executed
and performance measures in every game are stored. Learning predators execute
learning algorithm at the end of every even game.
4.2

Random Predators

The aim of the ﬁrst experiment is to test performance of random predators.
Two of them take part in the games. Initial positions of preys are random.
The average performance of the predators is 140.01. Its standard deviation is

900

´ zy´
B. Snie˙
nski

very high: 120.50. Hence, as we can see in Fig. 2, average performance varies
from game to game in the sequence.
4.3

Learning Predators

Two learning predators take part in this experiment. They do not communicate. Initial positions of preys are random. Average performance measures in
consecutive games are presented in Fig. 3.
The learning process is fast. In this experiment after executing about 300
random actions in two games, and collecting examples, performance changes
from about 130 to 90.
As we can see, performance of learning agents increases rapidly at the beginning of the learning process, when generated rules are used instead of a
(a)
160
140
120
100
80
60
40
20
Comm. Perf.
Noncomm. Perf.
0
2

4

6

8

10

12

(b)
160
140
120
100
80
60
40
20
Comm. Perf.
Noncomm. Perf.
0
2

4

6

8

10

12

Fig. 4. Performance comparison or learning agents with and without communication
in experiment with random initial prey positions (a) and ﬁxed initial prey positions (b)

Agent Strategy Generation by Rule Induction in Predator-Prey Problem

901

random choice. Next it increases slowly, because new examples do not contain
any signiﬁcant knowledge. The performance stabilizes at the end of the process.
4.4

Learning Predators with Communication

This experiment tests inﬂuence of communication. In Fig. 4-(a) we can see comparison of performances of communicating and non-communicating predators
with random initial positions of preys. As we can see, communication not only
does not help learning agents, but even makes the performance worse. Unfortunately, the reason of this behavior is not clear yet.
We tested another initial setting. In these experiments bird-prey starts in
position (3, 0) with probability 0.8 and in (11, 8) with probability 0.2, and
mouse-prey vice versa. In Fig. 4-(b) we can see performance comparison for
this settings. Performance of communicating agents is better than ones without
communication.
This result suggests that communication is proﬁtable if a learning agent is
rarely in the situation, which is common for the other agent. In such conditions
the other agent can provide appropriate, useful knowledge to transmit and use.
4.5

Rule examples

Examples of rules learned by the predator-agent in a form of Prolog clauses
are presented in Fig. 5. They can be interpreted in the following way. Both rules
have action left in the conclusion. S is an identiﬁer of a state, for which decision
should be made. Rule (a) is applicable if the predator sees a bird which has the
same or smaller by one x coordinate and relative vertical position is between
-3 and 0. Premise of rule (b) checks if the predator sees a mouse, relative x
coordinate is -2, and y is between -3 and -2.
(a)
dir(S,left) :type(S,bird),
dx(S,X),
X >= -1,
X =< 0,
dy(S,Y),
Y >= -3,
Y =< 0.

(b)
dir(S,left) :type(S,mouse),
dx(S,-2),
dy(S,Y),
Y >= -3,
Y =< -2.

Fig. 5. Examples of rules (in the form of Prolog clauses) learned by the agent

5

Conclusion and Further Research

In this paper idea of the agents using rule induction for learning strategy is presented. Solution proposed is tested on a speciﬁc example – predator-prey domain.

902

´ zy´
B. Snie˙
nski

However, similar strategy generation works also for a Fish-Banks game [16]. The
only condition for the environment to use this method is that agent should be
able to observe direct results of its actions. In other words, results are not delayed
and there is no reward assignment problem. We assume that rule induction, and
more general, supervised learning can be applied in such circumstances in other
domains.
Results for the Fish-Banks game [9] and initial results for the predator-prey
problem [17] show that supervised learning give improvements faster than reinforcement learning.
Advantage of rule induction is clarity of rule-based knowledge representation.
It is possible to interpret the knowledge base or check its correctness manually.
It can be very important for some domains.
Additionally, because rule-based knowledge representation is modular, exchange of the knowledge is easy and has low cost. Only necessary rules can
be transmitted.
A problem discovered in this research is a high value of standard deviation of
performance measure in predator-prey domain. It makes this domain not very
attractive for comparison of various methods. In the future research this domain
should be modiﬁed and/or other domains will be explored.
Interesting issue for further research is developing agents using several learning
methods for diﬀerent aspects of their activity. Also exchange of training data as
an another form of communication during learning should be tested. And last
but not least, performance of rule learning agent should be compared to the
performance of reinforcement learning agent in the same conditions.
Acknowledgments. The author is grateful to Arun Majumdar, Vivomind Intelligence Inc. for providing Prologix system (used for implementation), and for
help with using it, Janusz Wojtusiak, MLI Laboratory, George Mason University
for AQ21 software and assistance, and last but not least Jens Pfau, Technische
Universit¨
at Darmstadt for suggesting predator-prey domain, and implementation of the ﬁrst, Java version of the system using decision-tree induction and
reinforcement learning.

References
1. Stone, P., Veloso, M.: Multiagent systems: A survey from a machine learning perspective. Autonomous Robots 8, 345–383 (2000)
2. Panait, L., Luke, S.: Cooperative multi-agent learning: The state of the art. Autonomous Agents and Multi-Agent Systems 11 (2005)
3. Tan, M.: Multi-agent reinforcement learning: Independent vs. cooperative agents.
In: Proceedings of the Tenth International Conference on Machine Learning, pp.
330–337. Morgan Kaufmann, San Francisco (1993)
4. Haynes, T., Sen, I.: Evolving behavioral strategies in predators and prey. In: Adaptation and Learning in Multiagent Systems, pp. 113–126. Springer, Heidelberg
(1996)
5. Giles, C.L., Jim, K.-C.: Learning communication for multi-agent systems. In:
WRAC, pp. 377–392 (2002)

Agent Strategy Generation by Rule Induction in Predator-Prey Problem

903

6. Gehrke, J.D., Wojtusiak, J.: Traﬃc prediction for agent route planning. In: Bubak,
M., van Albada, G.D., Dongarra, J., Sloot, P.M.A. (eds.) ICCS 2008, Part III.
LNCS, vol. 5103, pp. 692–701. Springer, Heidelberg (2008)
7. Sugawara, T., Lesser, V.: On-line learning of coordination plans. In: Proceedings
of the 12th International Workshop on Distributed Artiﬁcial Intelligence, pp. 335–
345, 371–377 (1993)
8. Airiau, S., Padham, L., Sardina, S., Sen, S.: Incorporating learning in bdi agents.
In: Proceedings of the ALAMAS+ALAg Workshop (May 2008)
´ zy´
9. Snie˙
nski, B.: Resource management in a multi-agent system by means of reinforcement learning and supervised rule learning. In: Shi, Y., van Albada, G.D., Dongarra, J., Sloot, P.M.A. (eds.) ICCS 2007. LNCS, vol. 4488, pp. 864–871. Springer,
Heidelberg (2007)
10. Russell, S., Norvig, P.: Artiﬁcial Intelligence – A Modern Approach. Prentice-Hall,
Englewood Cliﬀs (1995)
11. Sardinha, J., Garcia, A., Milidi, R., Lucena, C.: The agent learning pattern. In:
Fourth Latin American Conference on Pattern Languages of Programming, SugarLoafPLoP 2004, Brazil (2004)
´ zy´
12. Snie˙
nski, B.: An architecture for learning agents. In: Bubak, M., van Albada,
G.D., Dongarra, J., Sloot, P.M.A. (eds.) ICCS 2008, Part III. LNCS, vol. 5103, pp.
722–730. Springer, Heidelberg (2008)
13. Wojtusiak, J.: AQ21 User’s Guide. Reports of the Machine Learning and Inference
Laboratory, MLI 04-3. George Mason University, Fairfax, VA (2004)
14. Michalski, R.S., Larson, J.: Aqval/1 (aq7) user’s guide and program description.
Technical Report 731, Department of Computer Science, University of Illinois,
Urbana (June 1975)
15. Majumdar, A., Tarau, P., Sowa, J.: Prologix: Users guide. Technical report, VivoMind LLC (2004)
´ zy´
16. Snie˙
nski, B., Ko´zlak, J.: Learning in a multi-agent approach to a ﬁsh bank game.
In: Pˇechouˇcek, M., Petta, P., Varga, L.Z. (eds.) CEEMAS 2005. LNCS, vol. 3690,
pp. 568–571. Springer, Heidelberg (2005)
´ zy´
17. Pfau, J., Snie˙
nski, B.: Comparison of reinforcement and supervised learning in
the predator prey game (2008) (unpublished)

