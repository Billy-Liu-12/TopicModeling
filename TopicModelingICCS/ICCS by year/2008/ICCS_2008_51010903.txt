Accuracy of Baseline and Complex Methods
Applied to Morphosyntactic Tagging of Polish
Marcin Kuta, Michal Wrzeszcz, Pawel Chrzaszcz, and Jacek Kitowski
Institute of Computer Science, AGH-UST, al. Mickiewicza 30, Krak´
ow, Poland
{mkuta,kito}@agh.edu.pl

Abstract. The paper presents baseline and complex part-of-speech taggers applied to the modiﬁed corpus of Frequency Dictionary of Contemporary Polish. Accuracy of 5 baseline part-of-speech taggers is reported.
On the base of these results complex methods are worked out. Thematic
split and attribute split methods are proposed and evaluated. Tagging
accuracy of voting methods is evaluated ﬁnally. The most accurate baseline taggers are SVMTool (for a simple tagset) and fnTBL (for a complex
tagset). Voting method called Total Precision achieves the top accuracy
among all looked over methods.
Keywords: part-of-speech tagging, natural language processing.

1

Introduction

Part-of-speech (POS) tagging algorithms are intensively exploited in a wide
range of applications including syntactic and semantic parsing, speech recognition and generation, ontology construction, machine translation, text understanding, information retrieval and many others [1].
Unfortunately POS tagging of highly inﬂecting languages like Polish is much
more challenging than application to analytic languages (e.g. English or French),
as the former are annotated with large, complex tagsets, describing many morphological categories.
POS tagging algorithms are computationally time demanding, especially when
employed to inﬂecting languages. In the domain training time exceeding 24 h is
nothing extraordinary. Time requirements of the complex algorithms like split
models or voting methods are moreover one order of magnitude higher.
The paper examines accuracy of selected baseline algorithms applied to morphosyntactic tagging of Polish. Next more complex methods are investigated,
both originally proposed (split models) as well as already known but not evaluated on Polish (voting methods). We present also results for the simple tagset
(only the ﬁrst attribute of each tag considered) to provide point of reference to
English and other languages described by small tagsets.
The taggers are evaluated on the modiﬁed corpus of Frequency Dictionary of
Contemporary Polish (m-FDCP), authors’ improved version [2] of the standard
FDCP corpus available at [3] site. The m-FDCP corpus is annotated with the
complex tagset, containing over 1200 tags. Tags consist of a set of attributes,
M. Bubak et al. (Eds.): ICCS 2008, Part I, LNCS 5101, pp. 903–912, 2008.
c Springer-Verlag Berlin Heidelberg 2008

904

M. Kuta et al.

each attribute describing selected morphological category. A token is the entity
being subject of tagging. A word segment is a token containing at least one letter
or digit. By raw text we mean a sequence of tokens without their tags.

2

Baseline Tagging Algorithms

POS tagging algorithms are roughly divided into statistical and rule based methods. For a given token w all methods examine its context, being a window of
N tokens and tags centred on the token w. The rule based methods take into
account a wide context, which is desirable in the case of languages containing
long-distance syntax dependencies. Statistical algorithms map a sequence of tokens into a sequence of tags with a probability model, which describes occurrence
of the most probable sequence of tags for a given sequence of tokens.
2.1

Evaluated Algorithms

In the paper we ﬁrst investigate ﬁve baseline algorithms, providing new results,
compared to [2] [4]. The methods serve furthermore as components for construction of more sophisticated taggers.
Hidden Markov Model (HMM). The algorithm belongs to the statistical
methods. Given a sequence of tokens, w1 , . . . , wn , the HMM tagger assigns a
sequence of tags, T = (t1 , . . . , tn ), according to a formula
n

p(wi |ti ) · p(ti |ti−1 , . . . , ti−N ) ,

Tˆ = arg max
T

(1)

i

where p(wi |ti ) is the conditional probability of occurrence of word wi given
that tag ti occurred and p(ti |ti−1 , . . . , ti−N ) is the conditional probability of
occurrence of tag ti given that tag sequence ti−1 , . . . , ti−N previously occurred.
Maximum entropy. This statistical method [5] aims to maximize the entropy
function by selection of binary features, reﬂecting dependence in a training corpus. The model assumes a set of binary features, fj , is deﬁned on the combination of a tag ti and its context c. The probabilistic model is built from family of
models
f (t ,c)
p(ti , c) = πμ
αj j i
,
(2)
j

where p(ti , c) stands for joint distribution of tags and contexts and π, μ are
normalisation factors in order that p(·, ·) forms the probability function.
Memory-based learning. The algorithm exploits rule based approach. Tagger
[6] acquires examples from training corpora, which are later used in the tagging
process. During the learning process memory-based taggers store in memory a
set of examples (ti , ci ), where ti denotes the tag and ci its context. Given a token

Accuracy of Baseline and Complex Methods

905

w in context c, the memory-based tagger assigns it a tag tk , such that distance
between c and ck is minimal.
Transformation-based error-driven learning. This rule based method [7]
starts with assigning a trivial sequence of tags to a given tokenised text. The
target sequence of tags is determined by applying series of transformations. Each
transformation, F , is a rule in the form: ”Replace value of tag t with value y if
current context c fulﬁls condition φ.” The core of learning process is the algorithm
for ﬁnding suitable transformations.
Support vector machines. The SVM is a statistical algorithm, which maps
input data to a higher dimensional space and next constructs a linear separating
hyperplane with the maximal margin. The mapping is done with kernel functions,
of which linear, polynomial, radial basis (RBFs) and sigmoid functions are the
most used. The SVM approach achieves 97.16% accuracy for English [8].
As an implementation of the above algorithms the following taggers have been
chosen for evaluation:
Table 1. Taggers used in experiments
Algorithm

Tagger name

HMM
Maximum entropy
Transformation based
Memory based
SVM

TnT [9]
MXPost 1 [5]
fnTBL [10]
MBT [6]
SVMTool 2 [8]

Next the complex methods have been worked out: the split models have been
elaborated and the collective methods evaluated.

3

Split Models

3.1

Thematic Split

To make beneﬁt of the thematic split approach the corpus structure should
be nonuniform, i.e., it should consist of segments diversiﬁed in language style.
This is the case of the m-FDCP corpus, containing 5 segments (thematic parts)
diﬀering in vocabulary, style, etc., e.g. average sentence length varies from 10.42
tokens/sentence (artistic drama) to 23.27 tokens/sentence (popular science).
These diﬀerences cause that tagging rules utile in one segment may become
ineﬃcient for another one. Thus, instead of providing one overall language model,
created from the whole corpus, it is worth considering building a number of
separate models, each acquired with a baseline tagger from a diﬀerent thematic
part.
1
2

Referred further as MXP.
Referred further as SVM.

906

M. Kuta et al.

3.2

Attribute Split

Attribute split method is applicable only to corpora annotated with complex
tagsets. Assuming a tagset provides for presence of K morphological categories,
the entire corpus is replicated K times, each copy corresponding to one morphological category. The i-th copy (1 ≤ i ≤ K) contains the whole text (all tokens),
annotated with a small tagset Ti . The tagset Ti is obtained from the complex
tagset by removing from each tag all attributes except the attribute describing
i-th morphological category. If i-th morphological category is not applicable to a
current token, the token is annotated with a special tag none in a relevant copy.
Next, the training procedure and tagging of the raw test set is performed
separately on each copy with a given baseline algorithm. Partition to training
and test sets remains preserved as in the original corpus annotated with the
complex tagset. Finally, K output ﬁles, generated within tagging of the raw test
set, are merged into one ﬁle, where each token is back annotated with all the
relevant morphological categories. The merged ﬁle is evaluated against the test
set of the corpus annotated with the complex tagset.

4

Collective Methods

Collective methods and their performance on English and Dutch are shown in
the fundamental work [11]. The idea of collective methods is based on assumption, that diﬀerent baseline methods make errors in diﬀerent places, i.e., baseline
methods are to some extent complementary. The higher the complementarity of
taggers, the bigger chance the combined system compensates errors of its constituents and performs better than the components alone. A few independent
baseline taggers (components) propose a tag simultaneously [11] [12] [13] according to their algorithms described in Sect. 2.1. Proposed tags are compared
and the best tag is selected according to an arbitration mechanism. As an arbitration several voting strategies are available.
Assuming the reference test corpus contains n tokens w1 , . . . , wn , a token wi
is annotated in the corpus with a tag ti (1 ≤ i ≤ n), a tagger A guesses for a
token wi a tag tA
i , the accuracy of the tagger A is deﬁned as follows:
df

accuracy =

#correctly tagged tokens
=
#all tokens

n
i=1

δ(tA
i , ti )
,
n

(3)

where δ is the Kronecker delta function. The sentence accuracy is a ratio of
entirely correctly tagged sentences to the total number of sentences.
If a tagger B guesses a tag tB
i respectively, the complementarity of the tagger
B to the tagger A, comp(B|A), is determined as follows [14]:
df

comp(B|A) = 1 −

#common errors of taggers A and B
.
#errors of tagger A

(4)

Given a tag X, the precision and recall of the tagger A on the tag X are
given as:
df #tokens tagged and annotated with X
precX =
,
(5)
#tokens tagged with X

Accuracy of Baseline and Complex Methods

df

recallX =

#tokens tagged and annotated with X
.
#tokens annotated with X

907

(6)

Majority is a simple voting method, with exactly one vote assigned to each tagger. With weighted voting methods each tagger votes for its accuracy (Total
Precision method) or precX (Tag Precision or Precision-Recall method). Additionally, the tagger may be obliged to support tags other than suggested by
itself (Precision-Recall method, with weight 1 − recallY ). Ties are resolved by a
random selection amongst winning tags. Vote strength of particular methods is
summarised in Table 2.
Table 2. Vote strength of a component tagger in various voting methods, X stands
for a tag proposed by tagger, Y (Y = X) represents each tag proposed by opposition
Voting method
Tag X
Tag Y
Majority
1
0
Total Precision accuracy
0
Tag Precision
precX
0
Precision-Recall precX 1 − recallY

For each tagger parameters accuracy, precX and recallX (for each tag X)
are to be determined with help of a tuning set, disjoint from a test set, to avoid
artiﬁcial boosting of the above methods.

5

Evaluated Data and Experiments Setup

We used the modiﬁed corpus of Frequency Dictionary of Contemporary Polish
[15], annotated with the slightly abridged version of the IPI PAN tagset [16]. The
m-FDCP corpus is partially corrected and disambiguated version of the FDCP
corpus [3], both with manual checking and automatic procedures. The whole
process of corpus improving has been described in details in [2].
The corpus is balanced between ﬁve thematic parts: (A) popular science,
(B) news dispatches, (C) editorials and longer articles, (D) artistic prose and
(E) artistic drama, each standing for approximately 20% of the corpus and representing diﬀerent style of the language. The used tagset provides for 9 morphological categories: grammatical class (part of speech or POS), number, case,
gender, person, degree, aspect, negation, vocalicity. The main parameters of the
corpus are gathered in Table 3 (4th column).
The baseline algorithms have been evaluated with the split of the m-FDCP
corpus to a training and test set, standing for 90% and 10% of the corpus,
respectively. The balanced character of the training and test set was carefully
preserved within the split. Their characteristics are gathered in Table 3.
The setup for the thematic split approach was the following: for each of 5
thematic parts of the m-FDCP corpus the separate experiment with the baseline

908

M. Kuta et al.
Table 3. Main parameters of the m-FDCP corpus [2]

tokens
word segments
sentences
diﬀerent tokens

Training Test
Full
90%
10% 100%
592729 65927 658656
496907 55139 552046
36601 4211 40812
87097 19557 92872

tagset size
ambiguous tokens, %
mean token ambiguity

Simple tagset
30
30
30
26.15 26.19 26.16
1.44 1.43
1.44

tagset size
ambiguous tokens, %
mean token ambiguity

Complex tagset
1191
724
1243
47.76 47.65 47.74
3.12 3.12
3.12

taggers was performed. Each part was split to a training and test set in 90%/10%
ratio (18% and 2% of the whole corpus respectively).
In the attribute split approach we used the entire m-FDCP corpus, divided
to training and test sets as for baseline tagging. This corpus was replicated 9
times, each with an appropriate tagset, as there are 9 morphological categories.
The setup for voting methods was slightly more complex. We avoided partition
of the corpus to training, tuning and test sets in 80%/10%/10% ratio [12] but
instead the tuning set was created on the base of the 90% training set, used
already within baseline tagging experiments. The 90% training set was divided
into 9 equal parts among which 8 parts served for training and one part for
testing. The procedure was repeated 9 times, with a diﬀerent part serving for
testing each time. The 9 output ﬁles were merged into one ﬁle, standing for the
tuning set; cf. [11].
In this way two important aims have been achieved. We got bigger training
set (90% instead of 80% of the corpus) for training the baseline taggers and at
the same time 9 times bigger tuning set (90% instead of 10% of the corpus).
As baseline components we used the taggers from Table 1, prepared already
for the baseline experiments. But when applied to the complex tagset, voting
methods omitted the SVM tagger, as achieving much lower accuracy than the
rest of components.
All experiments were performed at the ACC Cyfronet AGH-UST site on the
SMP supercomputer, SGI Altix 3700, equipped with 128 1.5 GHz Intel Itanium 2
processors, 256 GB RAM and 4.75 TB disk storage. The taggers have been compiled with standard optimization level. Depending on baseline tagger chosen
training time varies considerably from 3 to 980 · 102 seconds and tagging speed
from 5 to 220 · 102 tokens/second [4].

Accuracy of Baseline and Complex Methods

6
6.1

909

Results
Results for Baseline Methods

The accuracy of 5 baseline tagging algorithms is presented in Table 4.
Table 4. Accuracy of baseline taggers trained on 90% of the m-FDCP corpus, [%]
TnT MXP fnTBL MBT SVM

6.2

All tokens
Known tokens
Unknown tokens
Ambiguous tokens
Word segments
Word segments with known tags
Word segments with unknown tags
Unknown word segments
Sentences

96.20
96.98
88.65
89.50
95.46
96.94
0.00
88.65
61.48

Simple tagset
96.30 96.51 95.74
97.01 97.51 97.10
89.43 86.89 82.60
91.09 91.36 89.94
95.57 95.83 94.91
96.79 97.55 97.08
28.21 3.85
0.00
89.43 86.90 82.60
62.15 63.71 58.54

96.74
97.52
89.14
91.36
96.10
97.60
0.00
89.15
65.16

All tokens
Known tokens
Unknown tokens
Ambiguous tokens
Word segments
Word segments with known tags
Word segments with unknown tags
Unknown word segments
Sentences

86.33
88.97
60.86
78.66
83.66
90.73
0.00
60.86
28.95

Complex tagset
85.00 86.79 82.31
87.53 89.76 85.75
60.55 58.09 49.06
78.71 80.34 72.48
82.07 84.21 78.85
87.44 90.27 86.58
29.84 30.50 0.66
60.55 58.09 49.05
26.88 29.87 22.51

73.54
81.12
0.23
63.44
68.36
80.69
0.00
0.23
12.04

Results for Split Models

The observed decline of accuracy with the thematic split model (Table 5, columns
2-7) is due to considerable reduction of training sets sizes, comparing to setup for
the baseline algorithms, what might mask the proﬁts of thematic split. To prevent the masking eﬀect we made another experiment. We examined the baseline
algorithms, with uniform (balanced) training and test set of identical sizes as for
the thematic split, i.e., standing for 18% and 2% of the entire corpus (Table 5,
column 8). The higher accuracy of the thematic split model over this additional
experiment is apparent (column 8 vs. column 7 of Table 5).
We remark also accuracy degradation of the attribute split model (Table 6)
comparing to the baseline taggers. Only the attribute split model for SVM experiences improvement, in comparison to the baseline tagger. This can be explained
by considerably low accuracy of the SVM tagger.

910

M. Kuta et al.

Table 5. Accuracy of taggers trained on thematic parts A–E, average accuracy
(arithmetic mean) of all thematic parts, accuracy of taggers trained on the uniform
set, [%]
Thematic parts
B
C
D

A

Simple
95.07
94.83
94.58
94.00
95.39

E

18%/2%
average uniform

tagset
95.24
94.85
95.40
94.69
95.83

TnT
MXP
fnTBL
MBT
SVM

95.24
94.63
94.98
94.14
95.44

96.49
95.98
95.63
95.61
96.62

95.39
94.66
95.20
94.40
95.49

TnT
MXP
fnTBL
MBT
SVM

82.61
79.05
80.74
77.91
66.69

83.35
80.34
80.80
77.80
66.72

Complex tagset
82.24 82.39 85.79
79.47 79.83 84.02
80.43 80.99 84.00
77.13 78.42 82.92
65.42 67.94 74.53

95.49
94.99
95.16
94.57
95.75

94.82
94.46
94.64
93.80
94.93

83.28
80.54
81.39
78.84
68.26

83.26
79.22
80.92
78.03
67.69

Table 6. Tagging accuracy of the attribute split model applied to the complex tagset,
in next rows accuracy of its individual components given, [%]

attribute split
POS
number
case
gender
person
degree
aspect
negation
vocalicity

6.3

TnT

MXP fnTBL MBT

SVM

81.73
96.20
97.08
91.00
92.46
99.31
98.13
97.73
98.82
100.00

82.55 81.73 81.00
96.30 96.44 95.74
97.10 96.74 96.77
92.81 92.08 91.03
91.95 91.13 91.86
99.45 99.54 99.38
98.44 98.59 98.11
97.91 98.03 97.54
98.91 98.91 98.70
100.00 100.00 100.00

84.06
96.74
97.42
92.81
93.01
99.53
98.58
98.19
98.92
100.00

Results for Voting Methods

The possibility of improving the baseline algorithms by voting methods is expressed by complementarity (Table 7). Complementarity value near 0% indicates
that a pair of taggers gives similar eﬀects and no accuracy increase with voting
methods. The closer the complementarity values to 100%, the higher the margin
for accuracy increase of voting methods.
The accuracy of the voting methods themselves is given in Table 8. The methods are ordered according to their growing complexity. The eﬀort connected with
gathering weights required by voting methods and creation of the tuning set paid
oﬀ. Voting methods achieve the highest accuracy among all presented algorithms.
The results of Total Precision methods are especially encouraging.

Accuracy of Baseline and Complex Methods

911

Table 7. Complementarity, comp(B|A), of baseline taggers trained on 90% of the
m-FDCP corpus, [%]
(a) simple tagset

❅A
B❅
❅

TnT
MXP
fnTBL
MBT
SVM

(b) complex tagset

TnT MXP fnTBL MBT SVM
–
37.32
36.84
31.80
36.16

35.72
–
40.35
38.55
38.55

31.29
36.72
–
28.03
30.47

39.21
46.58
41.03
–
41.60

25.74
30.30
25.65
23.79
–

❅A
B❅
❅

TnT
MXP
fnTBL
MBT
SVM

TnT MXP fnTBL MBT SVM
–
33.20
34.56
21.39
13.62

39.13 32.28 39.25 55.39
–
33.25 43.19 56.42
41.22
–
41.07 54.05
33.01 21.09
– 39.96
23.12 7.94 10.17 –

Table 8. Accuracy of voting methods, [%]
Voting method Simple tagset Complex tagset
Majority
96.93
87.14
Total Precision
96.95
88.03
Tag Precision
96.93
87.41
Precision Recall
96.93
87.21

7

Conclusions

According to our experiments, the following conclusions can be drawn.
The SVM tagger achieves the highest, state-of-the-art accuracy among the
baseline methods for the simple tagset, although is useless when applied to the
complex tagset. For the complex tagset fnTBL yields both the highest overall
accuracy and sentence accuracy among the baseline methods. If the amount of
unknown tokens is prevailing, the MXPost should be considered.
Attribute split models give lower results than the baseline taggers except the
SVM case, whose baseline accuracy is however signiﬁcantly low.
The thematic split model has not outperformed any baseline tagger. The additional experiment with uniform 20% set of the m-FDCP corpus proved however
potential usefulness of the model. The condition of its applicability is that a corpus consists of several thematic segments. The segments should be large enough,
so that the beneﬁt of separate thematic models dominates accuracy degradation
tied to reduction of a training set size.
Voting methods are the most complex methods, requiring preparation of several baseline taggers. Elaboration of a reliable tuning set is another computationally demanding issue. Each of these methods outperforms the most accurate
baseline method. The Total Precision method achieves the highest accuracy and
at the same time is simpler than the Tag Precision and Precision Recall methods
yielding the simplicity only to the Majority method.
Acknowledgments. This research is partially supported by the Polish Ministry
of Science and Higher Education, grant no. 11.11.120.777. ACC CYFRONET
AGH is acknowledged for the computing time.

912

M. Kuta et al.

References
1. Mauco, M., Leonardi, M.: A derivation strategy for formal speciﬁcations from natural language requirements models. Computing and Informatics 26(4), 421–445
(2007)
2. Kuta, M., Chrzaszcz, P., Kitowski, J.: Increasing quality of the Corpus of Frequency
Dictionary of Contemporary Polish for morphosyntactic tagging of the Polish language. Computing and Informatics (to appear)
3. Corpus of Frequency Dictionary of Contemporary Polish,
http://www.mimuw.edu.pl/polszczyzna
4. Kuta, M., Chrzaszcz, P., Kitowski, J.: A case study of algorithms for morphosyntactic tagging of Polish language. Computing and Informatics 26(6), 627–647 (2007)
5. Ratnaparkhi, A.: A maximum entropy model for part-of-speech tagging. In: Proc.
of the 1st Conf. on Empirical Methods in Natural Language Processing, Univ. of
Pennsylvania, USA, pp. 133–142 (1996)
6. Daelemans, W., Zavrel, J., Berck, P., Gillis, S.: MBT: A memory-based part of
speech tagger-generator. In: Proc. of the 4th Workshop on Very Large Corpora,
Copenhagen, Denmark, pp. 14–27 (1996)
7. Brill, E.: Transformation-based error-driven learning and natural language processing: A case study in part of speech tagging. Computational Linguistics 21(4),
543–565 (1995)
8. Gim´enez, J., M`
arquez, L.: SVMTool: A general POS tagger generator based on
Support Vector Machines. In: Proc. of the 4th Int. Conf. on Language Resources
and Evaluation, Lisbon, Portugal, pp. 43–46 (2004)
9. Brants, T.: TnT - a statistical part-of-speech tagger. In: Proc. of the 6th Applied
Natural Language Processing Conf., Seattle, USA, pp. 224–231 (2000)
10. Florian, R., Ngai, G.: Fast Transformation-Based Learning Toolkit manual. John
Hopkins Univ., USA (2001), http://nlp.cs.jhu.edu/~rflorian/fntbl
11. van Halteren, H., Zavrel, J., Daelemans, W.: Improving accuracy in word class
tagging through the combination of machine learning systems. Computational Linguistics 27(2), 199–229 (2001)
12. van Halteren, H., Zavrel, J., Daelemans, W.: Improving data driven wordclass tagging by system combination. In: Proc. of the 36th Annual Meeting on Association
for Computational Linguistics, Montr´eal, Canada, vol. 1, pp. 491–497 (1998)
13. Schr¨
oder, I.: A Case Study in Part-of-Speech Tagging Using the ICOPOST Toolkit,
Technical report FBI-HH-M-314/02, Univ. of Hamburg, Germany (2002)
14. Brill, E., Wu, J.: Classiﬁer combination for improved lexical disambiguation. In:
Proc. of the 7th Int. Conf. on Computational Linguistics, San Francisco, USA, pp.
191–195 (1998)
15. Modiﬁed corpus of Frequency Dictionary of Contemporary Polish,
http://nlp.icsr.agh.edu.pl
16. IPI PAN Corpus resources, http://korpus.pl

