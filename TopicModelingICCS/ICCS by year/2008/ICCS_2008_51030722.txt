An Architecture for Learning Agents
´ zy´
Bartlomiej Snie˙
nski
AGH University of Science and Technology, Institute of Computer Science
Krak´
ow, Poland
Bartlomiej.Sniezynski@agh.edu.pl

Abstract. This paper contains a proposal of an architecture for learning
agents. The architecture supports centralized learning. Learning may be
performed by several agents in the system, but it should be independent
(without communication or cooperation connected with the learning process). An agent may have several learning modules for diﬀerent aspects of
its activity. Each module can use diﬀerent learning strategy. Application
of the architecture is studied on example of Fish-Banks game simulator.
Keywords: multi-agent systems, machine learning, agent architecture.

1

Introduction

Multi-agent systems often work in complex environments. Therefore it is very diﬃcult (or sometimes impossible) to specify and implement all system details a priori.
Applying learning algorithms allows to overcome such problems. One can implement an agent that is not perfect, but improves its performance. This is why
machine learning term appears in a context of agent systems for several years.
A lot of multi-agent systems, which are able to learn, have been built so
far. But in these works authors use their own architectures for learning agents,
specialized for the considered application domains. The universal model of the
learning agent was missing. It should be general enough to use in every domain,
cover as many learning methods as possible, but also it should be speciﬁc enough
to help to develop learning multi-agent systems.
This paper contains a proposal of the architecture for learning agents. The
proposed architecture supports centralized learning only. It means that all the
learning process is performed by an agent itself. Learning may be performed by
several agents in the system, but it should be independent (without communication or cooperation with other agents regarding the learning process).
In the following sections learning in multi agent systems is brieﬂy discussed,
the architecture of the learning agent is described, and its use in developed
system is presented.

2

Learning in Multi-agent Systems

Machine learning focuses mostly on research on an isolated process performed
by only one module in the whole system. The multi-agent approach concerns the
M. Bubak et al. (Eds.): ICCS 2008, Part III, LNCS 5103, pp. 722–730, 2008.
c Springer-Verlag Berlin Heidelberg 2008

An Architecture for Learning Agents

723

systems composed of autonomous elements, called agents, whose actions lead to
the realization of given goals. In this context, learning is based on the observation
of the inﬂuences of activities, performed to achieve the goal by an agent itself
or by other agents. Learning may proceed in a traditional – centralized (one
learning agent) or decentralized manner. In the second case more than one agent
is engaged in one learning process.
In multi-agent systems the most common technique is reinforcement learning [1]. It allows to generate a strategy for an agent in a situation, when the
environment provides some feedback after the agent has acted. Feedback takes
the form of a real number representing reward, which depends on the quality of
the action executed by the agent in a given situation. The goal of the learning
is to maximize estimated reward.
Supervised learning is not so widely used in multi-agent systems. However
there are some works using such strategies (e.g. [2,3]). Supervised learning allows
to generate knowledge from examples. Using this method instead of reinforcement learning has several advantages, see [4].
Architecture for learning agent can be found in [5]. Unfortunately it ﬁts mainly
reinforcement learning.

3

The Learning Agent Architecture

In this paper we propose a learning agent architecture for centralized learning,
which allows to use several learning modules in an agent. The architecture is
presented in Fig. 1.

Fig. 1. Learning agent architecture

An agent gets percepts from an environment, and executes actions to interact
with the environment. The main unit in an agent is a processing module, which
is responsible for analyzing sensor input, and choosing appropriate action. It can
be realized in a simple way, e.g. using reactive agent architecture, or in a complex
way, e.g. using layered or BDI architectures. To improve the performance, agent

724

´ zy´
B. Snie˙
nski

can use learning modules. To learn, agent should provide a training data. After
learning, the module can be used to get an answer for a provided problem.
Therefore, the training data and the problem are inputs for the module, and the
answer is an output. Several learning modules, using various learning strategies,
can be used by one agent for diﬀerent aspects of its activity.
Characteristics of the training data, the problem and the answer depend on
the learning strategy used in the module. Therefore we can deﬁne a learning
module as a four-tuple: (Learning strategy, Training data, Problem, Answer ).
Details of the learning modules are domain-speciﬁc. Learning strategy, (knowledge representation, learning algorithm, and conditions for which learning is executed), structure and source of Training data, Problem, and Answer should be
carefully designed by the system architect. Additional research is necessary to
provide guidance in this aspect.
Two types of learning modules were developed and tested so far: reinforcement
learning module and inductive rule learning (see section 4). Although, other
learning methods can be also used. Below modules for three types of popular
learning strategies are characterized.
3.1

Reinforcement Learning

As it was mentioned earlier, the most popular learning method in multi-agent
systems is reinforcement learning. In this method, an agent gets description of
the current state and using its current strategy chooses an appropriate action
from a deﬁned set. Next, using reward from the environment and next state
description it updates its strategy. Several methods of choosing the action and
updating the strategy have been developed so far. E.g. in Q-learning developed
by Chris Watkins [6] action with the highest predicted value (Q) is chosen. Q is
a function that estimates value of the action in a given state:
Q: A×X → ,

(1)

where A is a set of actions, and X is a set of possible states. Q function is
updated after action execution:
Q(a, x) := Q(a, x) + βΔ

(2)

Δ = γQmax + r − Q(a, x)

(3)

Qmax = max Q(a, x )

(4)

a

where x, x ∈ X are subsequent states, a ∈ A is an action chosen, r is a reward
obtained from the environment, γ ∈ [0, 1] is a discount rate (importance of the
future rewards), and β ∈ (0, 1) is a learning rate. Various techniques are used to
prevent from getting into a local optimum. The idea is to explore the solution
space better by choosing not optimal actions (e.g. random or not performed in
a given state yet) from time to time.
Reinforcement learning module can be responsible for managing all the agent
activities or only a part of it (it can be activated in some type of states or can be

An Architecture for Learning Agents

725

responsible for selected actions only). The Problem deﬁnition that is provided
consists of the description of the current state. The Answer is an action chosen
using the current strategy (current Q function). Training data consists of the
next state description (after executing action returned by the module), and a
reward. The reward may be observed by the agent or may be calculated by the
processing module using some performance measures.
3.2

Supervised Learning

Supervised learning allows to generate an approximation of a function f : X → C
from labeled examples, which consist of pairs of arguments and function values.
This approximation is called a hypothesis h. Elements of X are described by
set of attributes A = (a1 , a2 , . . . , an ), where ai : X → Di . Therefore xA =
(a1 (x), a2 (x), . . . , an (x)) is used instead of x.
Supervised learning module gets a Training data, which is a set {(xA , f (x))},
and generates hypothesis h. Problem is a xA , and the Answer is h(xA ).
There are lots of supervised learning methods. They use various hypothesis
representation, and various methods of hypothesis construction. One of the most
popular algorithms is C4.5, inductive decision tree learning algorithm developed
by Ross Quinlan [7]. It can be used if the size of the set C is small. In such a
case we call C a set of classes, and hypothesis is called a classiﬁer. C4.5 uses
decision trees to represent h. The basic idea of learning is as follows. The tree
is learned from examples recursively. If (almost) all examples in the training
data belong to one class, the tree consisting of the leaf labeled by this class is
returned. In the other case, the best attribute for the test in the root is chosen
(using entropy measure), training examples are divided according to the selected
attribute values, and the procedure is called recursively for every attribute test
result with the rest of attributes and appropriate examples as parameters.
Another learning algorithm with broad range of abilities, which was used in the
implemented system (see section 4) is AQ. It was developed by Ryszard Michalski [8]. Its subsequent versions are still developed. This algorithm also generates
classiﬁer from the training data, but h is represented by a set of rules, which have
tests on attribute values in the premise part, and a class in a conclusion. Rules
are generated using sequential covering: the best rule (e.g. giving a good answer
for the most examples) is constructed by a beam search, examples covered by this
rule are eliminated from a training set, and the procedure repeats.
Other methods, using diﬀerent knowledge representation, such as support vector machines, Bayesian or instance-based models also ﬁt the above speciﬁcation.
Similarly, learning module using artiﬁcial neural networks for classiﬁcation or
function approximation have the same input and output.
What is important, in the case of supervised learning, the processing module
should provide in the training data a proper function value f (x) for examples.
If we are not able to provide this, inductive learning can not be used. However,
if we have at least some qualitative information about f (x) for given xA , as we
suggested in [9] we can build a classiﬁer. Details of this work-around can be
found in section 4.2.

726

3.3

´ zy´
B. Snie˙
nski

Unsupervised Learning

In unsupervised learning the task of the learning module is to organize examples
into groups called clusters, whose members are similar in a some way. Examples
of this strategy are Kohonen neural networks and clustering. Training data have
a form of example descriptions: {xA } (without any label). The Problem is an
example description xA , and the Answer is the example’s cluster identiﬁer.
This type of module was not tested yet. It is presented here to show that the
framework proposed is general enough to cover this type of learning.

4

Application of the Architecture

In this section we present application of the proposed learning agent architecture.
An multi agent system was built to simulate the Fish Banks game [10]. The game
is a dynamic environment providing resources, action execution procedures, and
time ﬂow represented by game rounds. Each round consists of the following
steps: ships and money update, ship auctions, trading session, ship orders, ship
allocation, ﬁshing, ﬁsh number update.
Agents represent players that manage ﬁshing companies. Each company aims
at collecting maximum assets expressed by the amount of money deposited at
a bank account and the number of ships. The company earns money by ﬁshing
at ﬁsh banks. The environment provides two ﬁshing areas: coastal and deep-sea.
Agents can also keep their ships at the port. The cost of deep-sea ﬁshing is the
highest. The cost of staying at the port is the lowest but such ship does not
catch ﬁsh. Initially, it is assumed that the number of ﬁsh in both banks is close
to the bank’s maximal capacity. Therefore, at the beginning of game deep-sea
ﬁshing is more proﬁtable.
Usually, exploration of the banks by ﬁshing is too high and after several rounds
the number of ﬁsh decreases to zero. It is a standard case of ”the tragedy of
commons” [11]. It is more reasonable to keep ships at the harbor then, therefore
companies should change theirs strategies.
Agents may observe the following aspects of the environment: arriving of new
ships bought from a shipyard, money earned in the last round, ships allocations
of all agents, and ﬁshing results for deep sea and inshore area. All types of agents
can execute the following two types of actions: order ships, allocate ships.
Three types of agents can play the game in the system: two types of learning
agents using reinforcement learning and rule inductive learning, and a random
agent.
Order ships action is currently very simple. It is implemented in all types of
agents in the same way. At the beginning of the game every agent has 10 ships.
Every round, if it has less than 15 ships, there is 50% chance that it orders two
new ships.
Ships allocation action is controlled by a learning module or is done randomly.
It is based on the method used in [12]. The allocation action is represented by
a triple (h, d, c), where h is the number of ships left in a harbor, d and c are
numbers of ships sent to a deep sea, and a coastal area respectively. Agents

An Architecture for Learning Agents

727

generate a list of allocation strategies for h = 0%, 25%, 50%, 75%, and 100% of
ships that belong to the agent. The rest of ships (r) is partitioned; for every h
the following candidates are generated:
1. All: (h, 0, r), (h, r, 0) – send all remaining ships to a deep sea or coastal area,
2. Check: (h, 1, r − 1), (h, r − 1, 1) – send one ship to a deep sea or coastal area
and the rest to the other,
3. Three random actions: (h, x, r − x), where 1 ≤ x < r is a random number –
allocate remaining ships in a random way,
4. Equal: (h, r/2, r/2) – send equal number of ships to both areas.
The random agent allocates ships using one of the candidates chosen by random.
Methods used by learning agents and their learning modules are described below.
4.1

Reinforcement Learning Agent

Reinforcement learning agent chooses action by random in the ﬁrst round. In
the following rounds, reinforcement learning module is used. In this module
Problem is a pair (dc, cc), where dc ∈ {1, 2, . . . 25} represent catch in a deep-sea
area, and cc ∈ {1, 2, . . . , 15}} represents catch in a coastal area in the previous
round. Answer is a triple representing ship allocation action (h, d, c), such that
h, d, c ∈ {0%, 25%, 50%, 75%100%}, d + c = 1. The Training data consists of a
pair (dc , cc ), which is a catch in the current round, and a reward that is equal
to the income (money earned by ﬁshing decreased by ship maintenance costs).
Learning strategy applied is the Q-Learning algorithm.
At the beginning Q is initialized as a constant function 0. To provide suﬃcient
exploration, in a game number g a random action is chosen with probability 1/g
(all actions have the same probability then). Therefore random or the best action
(according to Q function) is chosen and executed.
4.2

Rule learning agent

Because agent has no information what action is the best in the given situation, it
is not able to prepare a training data in the form of (state, f(state)). To overcome
this problem the following work-around is used. Thank to comparison of income
of all agents after action execution, the learning agent has information about
quality of actions executed in the current situation. Leaning module is used to
classify action in the given situation as good or bad. When it is learned, it may
be used to give ranks to action candidates.
The Problem is deﬁned as a ﬁve-tuple (dc, cc, h, d, c), it consists of catch in the
both areas during the previous round and a ship allocation action parameters.
The Answer is an integer, which represents the given allocation action rating.
The agent collects ratings for all generated allocation action candidates and
chooses the action with the highest rating.
Training examples are generated from agent observations. Every round the
learning agent stores ship allocations of all agents, and the ﬁsh catch in the
previous round. The action of an agent with the highest income is classiﬁed as

´ zy´
B. Snie˙
nski

728
35000

SLA
RLA
RA1
RA2
RA3

30000

25000

20000

15000

10000

5000

0
1

2

3

4

5

6

7

8

9

10

Fig. 2. The average performance of rule learning agent (SLA) reinforcement learning
agent (RLA), and agents using random strategy (RA1, RA2, RA3)

good, and the action of an agent with the lowest income is classiﬁed as bad. If
in some round all agents get the same income, none action is classiﬁed, and as
a consequence, none of them is used in learning. Training data consists of the
following pairs: ((dc, cc, h, d, c), q), where q is equal to good or bad. At the end
of each game the agent uses training examples, which were generated during all
games played so far, to learn a new classiﬁer, which is used in the next game.
Learning strategy used is AQ21 program, which is an implementation of the AQ
algorithm [13].
Rating r of the action a is calculated according to the formula:
v(a) = α good(a) − bad(a),

(5)

where good(a) and bad(a) are numbers of rules, which match the action and
current environment parameters, with consequence good and bad, respectively,
and α is a weight representing the importance of rules with consequence good.
4.3

Performance of the Agents

The average performance of agents presented above in 10 subsequent games is
presented in Fig. 2. In these experiments there were three random agents and
one reinforcement learning agent or one rule learning agent (α was equal to one).
Performance was measured as a balance at the end of every game. In the ﬁgure
the average performance from ten repetitions of the simulation is presented.

An Architecture for Learning Agents

729

In the experiments the average balance of both types of learning agents increases with time. Reinforcement learning agent was worse than a rule learning
agent, but tuning of its parameters and taking into account during learning
actions of other agents should increase its performance. On the other hand,
reinforcement learning works well even if the reward is delayed. More about
comparison of these two learning strategies can be found in [4].
4.4

Two Learning Modules in One Agent

Currently, we are working on the version of the system, in which agent will be
learning in two aspects: ship allocation and setting a catch limit. The former
aspect will be the same as described above. The latter will be used to develop
a strategy of limiting ﬁshing in the areas with small number of ﬁsh. Learning
strategy that is assumed is Q-Learning algorithm. Currently, Problem is deﬁned
as a ﬁsh catch in the previous round, Answer contain information if the limit
proposal (which is constant) should be accepted or not. Training data consists
of the ﬁsh catch information and a reward. The reward is equal to 0 in all rounds
except the last one, when it is a balance of the agent.

5

Conclusion and Further Research

In the paper an architecture of learning agent is proposed. It was used in the
description of learning agents in a Fish-Banks simulation system. Agents are
learning ship-allocation strategy using reinforcement learning and rule induction.
The architecture is general enough to represent diﬀerent approaches to learning.
Applying the proposed model in a description of the system makes the description
clearer. It also helps to develop learning agents and to add new learning modules
to existing agents. It may be considered as a tool for learning agents design.
Currently the architecture supports centralized learning only. In the future it
should be extended to cover distributed learning (cooperation and communication during learning). Also agents with more then one learning module should
be studied and the possibility of interaction between modules in the same agent
should be examined.
Acknowledgments. The author is grateful to Arun Majumdar, Vivomind Intelligence Inc. for providing Prologix system (used for implementation), and for
help with using it, Janusz Wojtusiak, MLI Laboratory for AQ21 software and
assistance, and last but not least Jaroslaw Kozlak, AGH University of Science
and Technology for help with the Fish Bank Game.

References
1. Sen, S., Weiss, G.: Learning in multiagent systems. In: Weiss, G. (ed.) A Modern
Approach to Distributed Artiﬁcial Intelligence. MIT Press, Cambridge (1999)
2. Sugawara, T., Lesser, V.: On-line learning of coordination plans. In: Proceedings
of the 12th International Workshop on Distributed Artiﬁcial Intelligence, pp. 335–
345, 371–377 (1993)

730

´ zy´
B. Snie˙
nski

´ zy´
3. Snie˙
nski, K.J.: Learning in a multi-agent approach to a ﬁsh bank game. In:
Pˇechouˇcek, M., Petta, P., Varga, L.Z. (eds.) CEEMAS 2005. LNCS (LNAI),
vol. 3690, pp. 568–571. Springer, Heidelberg (2005)
´ zy´
4. Snie˙
nski, B.: Resource Management in a Multi-agent System by Means of Reinforcement Learning and Supervised Rule Learning. In: Shi, Y., van Albada, G.D.,
Dongarra, J., Sloot, P.M.A. (eds.) ICCS 2007. LNCS, vol. 4488, pp. 864–871.
Springer, Heidelberg (2007)
5. Russell, S., Norvig, P.: Artiﬁcial Intelligence – A Modern Approach. Prentice-Hall,
Englewood Cliﬀs (1995)
6. Watkins, C.J.C.H.: Learning from Delayed Rewards. PhD thesis, King’s College,
Cambridge (1989)
7. Quinlan, J.: C4.5: Programs for Machine Learning. Morgan Kaufmann, San Francisco (1993)
8. Michalski, R.S., Larson, J.: Aqval/1 (aq7) user’s guide and program description.
Technical Report 731, Department of Computer Science, University of Illinois,
Urbana (June 1975)
9. Sniezynski, B.: Rule induction in a ﬁsh bank multiagent system. Technical Report 1,
AGH University of Science and Technology, Institute of Computer Science (2005)
10. Meadows, D., Iddman, T., Shannon, D.: Fish Banks, LTD: Game Administrator’s Manual. Laboratory of Interactive Learning, University of New Hampshire,
Durham, USA (1993)
11. Hardin, G.: The tragedy of commons. Science 162, 1243–1248 (1968)
12. Kozlak, J., Demazeau, Y., Bousquet, F.: Multi-agent system to model the ﬁshbanks
game process. In: The First International Workshop of Central and Eastern Europe
on Multi-agent Systems (CEEMAS 1999), St. Petersburg (1999)
13. Wojtusiak, J.: AQ21 User’s Guide. Reports of the Machine Learning and Inference
Laboratory, MLI 04-3. George Mason University, Fairfax, VA (2004)

