Automated Positioning of Overlapping Eye Fundus
Images
Povilas Treigys1, Gintautas Dzemyda1, and Valerijus Barzdziukas2
1

Institute of Mathematics and Informatics, Akademijos str. 4, LT-08663 Vilnius, Lithuania
{treigys,dzemyda}@ktl.mii.lt
2
Kaunas University of Medicine, Eiveniu str. 4, LT-3007 Kaunas, Lithuania
vb@tmc.kmu.lt

Abstract. Changes in eye fundus images can be associated with numerous vision threatening diseases such as glaucoma, optic neuropathy, swelling of the
optic nerve head, or related to some systemic disease. Tracking the progress of
a possible disease of the patient becomes very difficult from separated retinal
images. In this article we present a method which registers two retinal images
so that the fundus images overlaps each other in the best way. As a separate
case, this article shows that in order to solve the optic nerve disc registration
problem a linear transformation of retinal image is sufficient. A human identification possibility via retinal image registration will be disclosed as well.
Keywords: automated eye fundus registration, vasculature structure extraction,
automated shifting, optic nerve registration, identification, retinal image
transformation.

1 Introduction
At present ophthalmologists can collect and analyze the eye fundus from digital images. Whenever the image of the eye fundus becomes digital, the means of automatic
image processing comes in play. A high quality colour photograph of the eye fundus
is helpful in the accommodation and follow-up of the development of the eye disease.
Evaluation of the eye fundus images is complicated because of the variety of anatomical structure and possible fundus changes in eye diseases. The optic nerve disc
(OD) appears in the normal eye fundus image as a yellowish disc with whitish central
cupping (excavation) through which the central retinal artery and vein pass (Fig. 1 left
and centre images). Changes of the optic nerve disc can be associated with numerous
vision threatening diseases such as glaucoma, optic neuropathy, swelling of the optic
nerve head, or related to some systemic disease. Thus, one of the basic tasks in ophthalmology is to analyze the optic nerve disc. Also, the analysis of vasculature can be
helpful to indicate pathologic changes associated with diseases such as: a hypertension, diabetes, or atherosclerosis [7].
Vasculature extraction methods from retinal images can be classified into one of
the groups: kernel, classifier and tracing-based [19]. In the kernel-based methods, an
image is convolved with predefined kernel in most cases. Further, the Gaussian filter
M. Bubak et al. (Eds.): ICCS 2008, Part I, LNCS 5101, pp. 770–779, 2008.
© Springer-Verlag Berlin Heidelberg 2008

Automated Positioning of Overlapping Eye Fundus Images

771

is introduced in order to model cross-section of the vessels. Afterwards the vessel
identification filters [6] are applied. Such a class of vasculature structure extraction
algorithms is commonly modelled together with neural networks [8] and is a very
time-consuming task. Classification-based methods are composed of two steps. During the first step, segmentation of an image is performed. Segmentation [16, 14] is basically accomplished by the kernel-based methods. In the second step, a set of features
has to be provided for the algorithm. Such a set describes the vessels visible in the
image. These methods that belong to this class allow processing of the objects with
complex structures [1]. This enables algorithms to perform faster, however, these algorithms cannot be automatic in most cases. In the tracing-based class of algorithms
[2], the algorithm traces the structure of a vessel between predefined points. Basically
tracing ends at the provided reference points. It is common that these reference points
are provided interactively by the human.
We present here a method for automatic retinal image registration. Image
registration is the process of transforming the different sets of data into one coordinate
system. In this particular situation, the registration should be performed so that visible
structures in two images overlap each other in the resulting image (Fig. 1 right side
image). The resulting image comes with a quality measurement parameter that later
can be introduced into a decision support system for ophthalmologists. Besides, this
article shows that in order to solve the optic nerve disc registration problem, a linear
transformation is sufficient. It should be noted that the structure of vasculature is
commonly used for human identification purposes. Also, automated human identification within the patient’s database problem is presented also.

Fig. 1. Base retinal image is shown (the left side); committed for registration retinal image is
shown (in the centre); superimposed retinal images with structures have yet not been registered
is shown (the right side)

2 Image Pre-processing and Scaling
The eye fundus images were collected in the Department of Ophthalmology of the Institute for Biomedical Research of Kaunas University of Medicine, using the fundus
camera Canon CF-60UVi, at a 60° angle. 6,3 Mpixel images (image size 3072x2048
pixels) were taken. The magnification quotient was 0,0065248 mm/pixels and the
common magnification quotient for the system eye-fundus camera was
0,556782±0,000827 (mean±SD). The scale (mm/pixels) for the fundus camera was
0,01171875 mm/pixels.

772

P. Treigys, G. Dzemyda, and V. Barzdziukas

In order to register the position of OD of two retinal images, first of all we have to
pre-process images. The first step of image pre-processing is accomplished by scaling
down the retinal image to the size of 768x512 pixels. Scaling is performed in order to
decrease the computation time. Basically the morphological operations are the most
time consuming procedures, because each pixel in a spatial domain is probed with
some structuring element also known as a convolution kernel. This leads to a substantial acceleration of vessel structure extraction, which is very important at this stage.
It is well known that every pixel in a colour image can be described by three components, namely: red (R) channel, green (G) channel, and blue (B) channel intensity
values. Then, every image that consists of NxM pixels can be described by three separate matrices: {R(x,y);G(x,y);B(x,y)}, where x = 1,…,N; y = 1,…,M. Here each function returns the specific intensity value of the channel at the position of (x,y).
As usual, in order to calculate the monochrome luminance of colour image, we
need to apply coefficients related with the eye's sensitivity to each of the RGB channel. This is done according to the NTSC standard and can be expressed by:
I(x,y)=0.2989*R(x,y)+0.587*G(x,y)+0.1140*B(x,y) Here I is the intensity image with
integer values ranging from a minimum of zero, to a maximum of 255.

3 Mathematical Morphology and Point-Wise Operations
Typically the morphologic operation was developed to deal with binary images but it
can be easily applied to intensity images. In the case of intensity images, erosion and
dilation is understood as a nonlinear search for minimum or maximum by introducing
some filters as well as opening and closing is a combination of erosion and dilation.
However, the fundamental concepts of grey-level morphology operations cannot be
directly applied to colour images [5, 13]. Thus, we need to convert colour images to
intensity ones as described in the pre-processing section.
Morphological operations typically probe an image with a small shape or template
known as a structuring element. The four basic morphological operations are erosion,
dilation, opening, and closing [15]. The grey-scale erosion can be described as the
calculation of the minimum pixel value within the structuring element centred on the
current pixel Ai,j. Denoting an image by I and a structuring element by Z, the erosion
operation IΘZ at a particular pixel (x,y) is defined as:
IΘZ = min ( Ax+i , y + j ).
( i , j )∈Z

(1)

Here i and j are indices of the pixels of Z. The grey-scale dilation is considered in a
dual manner and thus can be written as:
I ⊕ Z = max ( Ax+i , y + j ).
( i , j )∈Z

(2)

The opening of an image is defined as erosion followed by dilation, while the image closing includes dilation followed by erosion. Thus, the morphological operation
as closing can be defined as follows:
I • Z = ( I ⊕ Z )ΘZ = min ( max ( Ax+i , y + j )).
( i , j )∈Z ( i , j )∈Z

(3)

Automated Positioning of Overlapping Eye Fundus Images

773

The closing operator usually smoothes away the small-scale dark structures from
colour retinal images. As closing only eliminates the image details smaller than the
structuring element used, it is convenient to set the structuring element big enough to
cover all possible vascular structures. Mendels et al. [9] applied the closing grey-level
morphology operation to smooth the vascular structures from the retinal images.
Let us assume that the above presented scheme is applied to two pre-processed images. Also, let us say that one image (P1) is the base image and the other image (P2) is
the image that has to be registered on the base image. In order to see the differences
of two spatial images, the technique of intensity values subtraction is frequently used.
This operation can be defined as follows: Q(x,y)=|P1(x,y)-P2(x,y)|. After this operation (for each x = 1,…,N; y = 1,…,M ) if at a particular time there are no changes in
the spatial domain, the subtracted intensity values acquire the value of 0, otherwise, if
there are some differences the intensity value does not become 0. In order to visualize
the subtracted image, we have to apply a intensity adjustment procedure. This is
needed because the vessels intensity values in colour images are very low according
to the surrounding background of the retinal image.
Basically the intensity adjustment procedure can be described in this way. Let us
assume that the distribution of intensity values of subtracted image Q(x,y) and the
transformation function f are continuous in the interval [0, 1] [4]. Moreover, assume
that the transfer function is single-valued and monotonically increasing. Then actual
intensity levels in shown interval will be recalculated using the function f to the desired intensity levels in a desired interval. In our investigation we used the desired interval with the minimum value of 0 and the maximum value of 255. Also, the Gamma
correction factor was set to 1 (this transfer function is nearly linear).
By thresholding the intensity adjusted image, next, we will be able to apply skeletonization operation in order to achieve the reference vasculature structures in both
images. Thus, for automated threshold level calculation we use Otsu’s method based
on the weighted histogram calculation [12].Otsu’s method maximizes the a posteriori
between-class variance σ B2 (t ) given by:
⎛ μ T (τ 1 ) − μ1 (τ 1 ) μ1 (τ 1 ) ⎞
⎟.
−
w0 (τ 1 ) ⎟⎠
⎝ 1 − w0 (τ 1 )

σ B2 (t ) = w0 (τ 1 )[1 − w0 (τ 1 )]⎜⎜
τ1

Here

w0 (τ 1 ) = ∑
i =0

(4)

τ1
L −1
ni
n
n
; w1 (τ 1 ) = 1 − w0 (τ 1 ); μ1 (τ 1 ) = ∑ i i ;μT (τ 1 ) = ∑ i i .
N
i =0 N
i =0 N

The optimal threshold τ1 is found by Otsu’s method through a sequential search for
the maximum of max σ B2 (τ 1 ) of τ 1 , where ni represents the number of pixels in the
0≤τ1 < L

grey-level i, L is the number of grey-levels, and N is the total number of pixels in the
image [18]. The Otsu thresholding method was applied because in the next morphological operation we have unambiguously distinguished what is foreground and what
is background in the retinal image. A foreground is assumed to be the vasculature of
the retinal image and the background – remaining part of the retinal image.
The next step is to extract the structure of vasculature from both images: from the
base image and the image to be registered. For this end we have used the medial axis

774

P. Treigys, G. Dzemyda, and V. Barzdziukas

transform (skeletonization) [10]. Basically, the skeletonization operation is calculated
by shifting the origin of the structuring element (Fig. 2) to each possible pixel position
in the image. Then, at each position it is compared with the underlying image pixels.
If the foreground and background pixels in the structuring element match exactly the
foreground and background pixels in the image, then the image pixel situated under
the origin of the structuring element is set to the background, otherwise, it is left unchanged. Here we denote that a foreground pixel is assumed to be 1 and a background
pixel is 0. An empty cell means that a particular pixel is of no interest, and it is not
taken into account for evaluation.

Fig. 2. Structuring elements used for skeletonization

In Fig. 2, both images are first skeletonized by the left-hand structuring element,
and afterwards by the right-hand one. Then the above presented process is performed
with the remaining six 90° rotations of those two elements during the same iteration.
The iteration process is stopped when there are no changes in the images for the last
two iterations.

4 Transformation to the Frequency Domain
In order to register two vasculature trees achieved by scheme proposed we have to incorporate some cross-correlation method. It is well known that for big images the
convolution methods designed for cross-correlation runs very slowly. This problem
can be solved by introducing a discrete Fourier transform (DFT) [3]. Usually DFT is
defined for the discrete function f(x,y) that is non-zero over the finite region
0 ≤ m ≤ M − 1 and 0 ≤ n ≤ N − 1 . In our case, this function represents a retinal image
in the spatial domain. Then, the two-dimensional discrete Fourier transformation of
the matrix M by N can be calculated as follows:

F ( p, q ) =

M −1 N −1

∑∑

f (m, n)e

⎛ 2π ⎞
⎛ 2π ⎞
−i ⎜
⎟ pm − i ⎜
⎟ qn
⎝M ⎠
⎝ N ⎠

e

(5)

.

m =0 n = 0

Where p=0,…,M-1 and q=0,…,N-1. The inverse DFT can be achieved by applying:
f (m, n) =

1
MN

M −1 N −1

⎛ 2π ⎞
⎛ 2π ⎞
i⎜
⎟ pm i ⎜
⎟ qn
⎠
⎝ N ⎠

∑ ∑ F ( p , q )e ⎝ M
p =0 q = 0

Here m=0,…,M-1 and n=0,…,N-1.

e

.

(6)

Automated Positioning of Overlapping Eye Fundus Images

775

The Fourier transform produces a complex number valued output image. This image can be displayed with two images, either with the real and imaginary part or with
magnitude and phase.
In our investigation, we apply Eq. 5 to the base retinal image. The retinal image
committed for the registration process is rotated by 1800, since the convolution operation itself reverses the provided pattern [17]. Then, the Eq. 5 is applied to the rotated
pattern as well. This results in four arrays, the real and imaginary parts of the two images being convolved. Multiplying the real and imaginary parts of the base image by
the real and imaginary parts of the image committed for registration generates a new
frequency image with the real and imaginary parts. Taking an inverse DFT of the
newly created frequency image, described by Eq. 6 completes the algorithm by producing the final convolved image. The value of each pixel in a convolved correlation
image is a measure of how well the target image matches the searched image at a particular point. The new correlation image calculated is composed of noise plus a single
high peak, indicating the best match of vasculature of the image to be registered in the
base retinal image vasculature. Simply by locating the highest peak in this image, it
would specify the detected coordinates of the best match.
The frequency transformation procedure described above is applied by taking the
structure of vasculature of the image which has to be registered on itself. This is done
because another coordinates are necessary that show where the best match of image
on itself is (Fig. 3).

Fig. 3. Peaks indicate a shift along the x axis (the left side), and peaks indicate shift along y axis
(the right side)

In Fig. 3 on both sides a smaller peak corresponds to the two different images convolved together. The biggest peak corresponds to the image convolved by itself. Then,
by introducing a simple linear transform to the retinal image, committed for registration, we shift pixels by the calculated distance along the x and y axes. The result of
closing, subtraction, histogram equalization, thresholding, skeletonization and shift
calculation is shown in Fig. 4.
Fig. 4 shows two structures of vasculature extracted by the method proposed
above. The stronger structure belongs to the base retinal image on which s a retinal
image intended for registration has to be put. The weaker structure of vasculature belongs to the retinal image intended for registration.

776

P. Treigys, G. Dzemyda, and V. Barzdziukas

Fig. 4. Shows a superimposed structure of the extracted vasculature of two retinal images (top
figure), (the bottom figure) shows the registered vasculature structure of two retinal images

5 Results
Eye fundus images were provided by the Department of Ophthalmology of the Institute for Biomedical Research of Kaunas University of Medicine (BRKU). The testing

Automated Positioning of Overlapping Eye Fundus Images

777

set consisted of 19 patients’ retinal images of both eyes. It should be noted that registration of images is possible only if those images are of the same patient and of the
same eye. This comes from the fact that the structure of eyes vasculature of each human is unique. In order to verify this fact and to obtain the factor of registration error,
the algorithm proposed was applied to the retinal images taken from different patients
of the same eye (Fig. 5).

Fig. 5. Shows two correlated images. (On the left side) the correlation between the base retinal
image and that committed for registration is shown, (on the right side), self-correlation of retinal images is shown.

Fig. 5 shows the magnitudes of convolved images along the x axis. In this particular case, where it is not the same person is taken for investigation, note, that magnitudes on the left image are dramatically lower than that in the image on the right.
Thus, to evaluate the quality of the registration, we computed the ratio of the peaksignal to noise (PSNR). 119 possible pairs of eye fundus images have been investigated. The conditions for those images to be of the same person and also of the same
eye have been satisfied. The results achieved are shown in Fig. 6.

Fig. 6. Histogram of the ratio between the peak-signal to noise

In Fig. 6, a histogram of the ratio peak-signal to noise is presented. According to
[11], acceptable PSNR values are between 20Db and 40Db. The higher the value of
decibels, the better registration is performed (Fig. 7).
Here we can draw the conclusion on automated human identification from retinal
images. If PSNR dramatically lower than 20Db one can made the decision that it is
not the same person. In case shown in Fig. 5 and 3 calculated PSNR value was 4.3Db.

778

P. Treigys, G. Dzemyda, and V. Barzdziukas

Fig. 7. (On the left), two overlapping images are illustrated, (on the right), the registered image
of the quality of 51Db

The comparative PSNR analysis can be made over the patients’ database in order to
automatically identify the person. This can be used for solving the problem of patient’s data protection because medic will be working only with the data about the
state of the patient without knowing who really the patient is.

6 Conclusions
In this article the authors presented an automated technique for retinal image registration where two images overlap each other in the best way. The task was accomplished
by introducing the intensity level morphology operations for vessel extraction. Then
the intensity adjustment procedure was performed to enhance the resulting image after
subtraction. This operation was followed by the image binarization, where the skeletonization operation was introduced. In the next step the spatial domain of the extracted vasculature structure was converted into the frequency domain, which resulted
in a fast convolution of two images. This fact enables us to calculate the image shift.
The analysis of provided retinal images showed that the registration quality parameter
basically occurs within the bounds of decibels accepted in literature. Also, we have
shoved that for fundus image registration problem a linear transformation is enough to
obtain satisfactory results. Disclosed problem on human identification revealed that
proposed algorithm is also suitable to solve the identification-related class problems.
However, more careful analysis in order to evaluate the identification results should
be made.
Acknowledgements. The research is partially supported by the Lithuanian State Science and Studies Foundation project “Information technology tools of clinical decision support and citizens wellness for e.Health system” No. B-07019.

References
1. Chanwimaluang, T., Guoliang, F., Fransen, S.R.: Hybrid retinal image registration. IEEE
Transactions on Technology in Biomedicine 10(1), 129–142 (2006)
2. Dongxiang, X., Jenq-Neng, H., Chun, Y.: Atherosclerotic blood vessel tracking and lumen
segmentation intopology changes situations of MR image sequences. In: Proceedings. International Conference on Image Processing, vol. 1, pp. 637–640 (2000)

Automated Positioning of Overlapping Eye Fundus Images

779

3. Edward, W.,, Kamen, B., Heck, S.: Fundamentals of Signals and Systems Using the Web
and Matlab (2000)
4. Gonzalez, R., Woods, R.: Digital Image Processing. Addison-Wesley Publishing Company, Reading (1992)
5. Goutsias, J., Heijmans, H., Sivakumar, K.: Morphological operators for image sequences.
Computer Vision and Image Understanding (62), 326–346 (1995)
6. Hoover, A., Goldbaum, M.: Locating the optic nerve in a retinal image using the fuzzy
convergence of the blood vessels. IEEE Transactions on Medical Imaging 22(8), 951–958
(2003)
7. Lowell, J., Hunter, A., Steel, D., Basu, A., Ryder, R., Kennedy, R.L.: Measurement of
Retinal Vessel Widths from Fundus Images Based on 2-D Modeling. MedImg 23(10),
1196–1204 (2004)
8. Matsopoulos, G.K., Asvestas, P.A., Mouravliansky, N.A., Delibasis, K.K.: Multimodal
registration of retinal images using self organizing maps. MedImg 23(12), 1557–1563
(2004)
9. Mendels, F., Heneghan, C., Thiran, J.: Identification of the optic disc boundary in retinal
images using active contours. In: The Proceedings of the Irish Machine Vision and Image
Processing Conference, pp. 103–115 (1999)
10. Mukherjee, J., Kumar, A.M., Das, P.P., Chatterji, B.N.: Use of medial axis transforms for
computing normals at boundary points. Pattern Recognition Letters 23(14), 1649–1656
(2002)
11. Netravali, A.N., Haskell, B.G.: Digital Pictures Representation, Compression, and Standards, 2nd edn. Plenum Press, New York (1995)
12. Otsu, N.: A threshold selection method from gray-level histograms. IEEE Trans. Syst. Man
Cybernet. SMC91(1), 62–66 (1979)
13. Peters, R.: Mathematical morphology for angle-valued images. Non-linear Image Processing. In: International Conference on Electronic Imaging, Society of Photo-optical Instrumentation Engineers, pp. 1–11 (1997)
14. Soares, J.V.B., Leandro, J.J.G., Cesar Jr., R.M., Jelinek, H.F., Cree, M.J.: Retinal vessel
segmentation using the 2-D Gabor wavelet and supervised classification. MedImg 25(9),
1214–1222 (2006)
15. Soille, P.: Morphological Image Analysis. Springer, Berlin (1999)
16. Staal, J., Abramoff, M.D., Niemeijer, M., Viergever, M.A., van Ginneken, B.: Ridge-based
vessel segmentation in color images of the retina. MedImg 23(4), 501–509 (2004)
17. Steven, W.S.: The Scientist & Engineer’s Guide to Digital Signal Processing. California
Technical Pub. (1997)
18. Tian, H., Lam, S.K., Srikanthan, T.: Implementing Otsu’s Thresholding Process Using
Area-Time Efficient Logarithmic Approximation Unit. In: IEEE International Symposium
on Circuits and Systems (ISCAS), vol. 4, pp. 21–24 (2003)
19. Vermer, K.A., Vos, F.M., Lemij, H.G., Vossepoel, A.M.: A model based method for retinal blood vessel detection. Computers in Biology and Medicine 34, 209–2019 (2004)

