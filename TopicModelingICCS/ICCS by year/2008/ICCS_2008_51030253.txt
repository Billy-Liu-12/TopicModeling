Using MPI Communication Patterns to Guide
Source Code Transformations
Robert Preissl1,2, Martin Schulz1 , Dieter Kranzlm¨
uller2,
1
Bronis R. de Supinski , and Daniel J. Quinlan1
1

2

CASC, Lawrence Livermore National Laboratory, USA
{preissl2,schulzm,bronis,dquinlan}@llnl.gov
GUP, Johannes Kepler University Linz, Austria/Europe
{rpreissl,dk}@gup.jku.at

Abstract. Optimizing the performance of HPC software requires a highlevel understanding of communication patterns as well as their relation to
source code structures. We describe an algorithm to detect communication
patterns in parallel traces and show how these patterns can guide static
code analysis. First, we detect patterns that identify potential bottlenecks
in MPI communication traces. Next, we associate the patterns with the
corresponding nodes in an abstract syntaxtree using the ROSE compiler
framework. Finally we perform static analysis on the annotated control
ﬂow and system dependence graphs to guide transformations such as code
motion or the automatic introduction of MPI collectives.

1

Introduction

With today’s increasingly complex and highly scalable parallel systems, we require approaches that combine static (compile time) and dynamic (run time)
information to capture suﬃcient information to optimize their applications. Similar techniques already support feedback guided compilation; however, these approaches limit the kind of data they collect and the optimizations that they
apply (often restricted to simple localized decisions like inlining or code layout).
In this paper, we apply this principle to the analysis and optimization of
global communication behavior, which is a primary source of ineﬃciency in parallel machines [2]. We use a suﬃx tree algorithm to detect repeating patterns in
communication traces and map the patterns to static data structures. For the
latter part we rely on ROSE, a comprehensive and ﬂexible toolkit for the generation of source-to-source translators. We present early experiences on static
source transformations exploiting the additional runtime information.
Sec. 2 gives an overview of the approach and shows the most important steps
of the process. In Sec. 3 we formally deﬁne the repeating structures in strings
and then compare the runtime behavior and memory usage of two diﬀerent
Part of this work was performed under the auspices of the U.S. Department of Energy
by Lawrence Livermore National Laboratory under Contract DE-AC52-07NA27344.
(LLNL-CONF-400356).
M. Bubak et al. (Eds.): ICCS 2008, Part III, LNCS 5103, pp. 253–260, 2008.
c Springer-Verlag Berlin Heidelberg 2008

254

R. Preissl et al.

Fig. 1. Flow diagram of the main approach

algorithms that use the structures to ﬁnd MPI communication patterns. Sec.
4 gives an overview of the ROSE compiler infrastructure that enables us to
perform static analysis of the associated regions in the parallel code and to
carry out source-to-source transformations. Sec. 5 provides a concrete example
of a successfully transformed parallel code that achieves better performance by
overlapping communication and computation. In the ﬁnal sections we discuss
aspects of future research and present our conclusions.

2

Combing Static and Dynamic Analysis

The analysis of MPI communication patterns has traditionally been used for
high-level understanding and error detection [3,4,5]. In this work we extend the
use of patterns to static analysis and source code transformations for performance optimization. We detect repetitive patterns of ineﬃcient communication
at runtime (e.g., poorly implemented broadcast operations) and use this information to optimize these heavily used structures in existing applications statically
by replacing them with more eﬃcient equivalent operations.
Fig. 1 gives an overview of our approach: we instrument the target MPI application, generate an MPI trace of the program executed under a given set of
parameters, and then use pattern matching to isolate recurring ineﬃcient communication structures. We also generate an abstract syntax tree (AST) of the
application and perform a static analyses to extract control and data ﬂow. We
then map the detected patterns onto this information and use these annotations
to guide potential source-to-source transformations.
This process comes with four major challenges:
1. Detecting MPI operations that would make recurring patterns interesting
2. Finding and ﬁltering the patterns that involve these operations

Using MPI Communication Patterns to Guide Source Code Transformations

255

3. Determining if the optimizations applicable to the patterns can be applied
safely to the AST (possibly through specialization)
4. Transforming the related source code fragments

3

Detecting Patterns in MPI Communication Traces

MPI communication patterns are repeating communication structures in an MPI
event graph [6]. First each MPI event is encoded into a 32 bit integer, such that
each task’s trace information is represented by an array of integer variables. For
each task, we compute all repetitive and maximal repetitive sequences [7].
Deﬁnition 1. A maximal pair of a string S of length n is a triple (p1, p2, l),
such that
S[p1 , p1 + l − 1] = S[p2 , p2 + l − 1], but
S[p1 − 1] = S[p2 − 1] and S[p1 + l] = S[p2 + l]

(1)

where p1 , p2 denote the starting positions of the two substrings and l gives their
lengths. A maximal repeat is a string represented by such a triple.
We use the maximal repeats, which are a subset of all repeated sequences, to
select a start sequence. Starting from a maximal repeat, we identify global communication patterns that they contain. Using maximal repeats is crucial for
ﬁnding repeating communication patterns eﬃciently. We have currently implemented two ways to detect repetitive structures in MPI traces: a naive algorithm
based on convolution and a more advanced technique using suﬃx trees.
Our naive convolution method aligns the left end of the pattern P with the
left end of the string S and then compares the characters of P and S left to
right until either two unequal characters are found or until P is exhausted. It
then shifts P to the right one place and repeats the comparisons process. If n
is the length of P and m is the length of S, this approach makes Θ(nm) [7]
comparisons in the worst case. Since there is possibly a huge number of MPI
events in the trace, this method take signiﬁcant runtime.
Our more advanced compressed suﬃx tree approach borrows ideas from computational biology for ﬁnding repeating gene structures in huge DNA sequences
[7]. The suﬃx tree for a string S with length m is a rooted directed tree with
exactly m leaves and whose labels correspond to substrings of S. For any leaf i,
the concatenation of the edge-labels on the path from the root to leaf i exactly
spells out the suﬃx of S starting at position i.
Deﬁnition 2. An internal node v of the suﬃx tree for string S is called left
diverse if at least two leaves with suﬃx position i and j in v s subtree have
diﬀerent left characters, that is, S[i − 1] = S[j − 1]
Theorem 1 (D. Gusﬁeld). The substring α labeling the path to an internal
node v of the suﬃx tree for string S is a maximal repeat iﬀ v is left diverse.

256

R. Preissl et al.

Fig. 2. Suﬃx for a string representing MPI events of a trace

D. Gusﬁeld describes algorithms (e.g., Ukkonen’s algorithm) for constructing
such trees with all its suﬃx links for a string S with length m in Θ(m) time [7].
However, these algorithms are not space eﬃcient, particularly with large alphabets, as with MPI traces. Thus, we use an alternative approach. The compressed
suﬃx tree [8] for a string S with length m of an alphabet Σ occupies Θ(m log|Σ|)
bits. The ﬁnal time requirement for creating the tree is Θ(m log|m| log|Σ|), being
reasonably close to the best current theoretical result [9].
Fig. 2 shows a suﬃx tree of the string S = ”17, 18, 23, 17, 18, 5, 17, 18, 5, 9”, in
which each integer represents an MPI event. We can easily see from the suﬃx
tree that 17, 18 with starting positions (1, 4 and 7) and 17, 18, 5 with starting
positions (4 and 7) are maximal repeats. In addition to these (maximal) repeats,
S has the repeat 18, 5 with starting positions (5 and 8).
We must choose a repeat on a speciﬁc task from which to start the pattern
matching process. Since each task typically has many maximal repeats, the time
required to compute a communication pattern for all maximal repeats is generally prohibitive. Hence, we ﬁrst ﬁlter start-repeats and then compute only
communication patterns that involve these ﬁltered events. This ﬁlter step is either guided by special seed events, e.g. events in the trace with a huge diﬀerence
between start- and end-time, or by focusing on selected MPI operations.
Fig. 3 shows an example MPI trace in which the events in bold highlight a
repeating set of MPI operations that our pattern-detection algorithm ﬁnds. This
pattern is potentially equivalent to a broadcast operation using a tree-based
communication structure. In this example, our static analysis must verify that
the pattern is a broadcast operation and, if so, that we can replace the associated
code segments with an equivalent, but probably more eﬃcient version in the form
of a native MPI collective. This is promising because it will enable programmers,
that do not have a broad knowledge of MPI to apply more eﬃcient functions
deﬁned by the MPI standard in without having to use them explicitly.

Using MPI Communication Patterns to Guide Source Code Transformations

257

Fig. 3. A detected pattern representing a broadcast operation in an MPI trace

4

Static Analysis with ROSE

A communication pattern extracted from the runtime trace only points to a
potential bottleneck. Further, the pattern is initially only valid for the particular input set used during the application run. We must verify the pattern
occurs across all control ﬂows and investigate pattern speciﬁc global data ﬂow
constraints (e.g., in the case of a suspected broadcasts that the same data is
communicated in all messages). The required information to achieve this goal
is encapsulated in the System-Dependence-Graph (SDG) [10] and the ControlFlow-Graph (CFG).

Fig. 4. Excerpt of the System-Dependence-Graph (SDG) of a parallel program

We use the ROSE open-source compiler infrastructure to generate both of
these graphs. ROSE is a tool kit to generate custom source-to-source translators. It provides mechanisms to translate input source code into an intermediate
representation (AST) [11], libraries to traverse and manipulate the information
stored in the AST, as well as mechanisms to transform the changed AST information back into valid source code. The representation within the AST as well as
the supporting data structures is powerful enough to readily exploit knowledge of
the architecture, parallel communication characteristics, and cache architecture
in the speciﬁcation of the transformations [12].
Fig. 4 illustrates a small excerpt of an SDG generated by ROSE providing both
data and control dependency information. The speciﬁc example shows the nodes

258

R. Preissl et al.

Fig. 5. The ROSE compiler infrastructure

and data-dependence egdes that represent ﬂow of data between statements or
expressions as well as control-dependence edges that represent control conditions
on which the execution of a statement or expression depends, in this case around
an MPI Send function in a parallel application.
The ﬂow diagram in Fig. 5 reﬂects the complete approach, transforming unoptimized C++ code based on user deﬁned abstractions into highly optimized code.
In our approach we have deﬁned speciﬁc transformations to MPI code structures
based on dynamic analysis results in the form of ineﬃcient MPI communication
patterns.

5

Examples and Early Results

The following simple example illustrates how we can use our techniques to replace
blocking with non-blocking communication to overlap communication and computation for better overall performance. Fig. 6 shows the structure of the corresponding “Late-Sender” pattern: the receiving task wastes useful time in waiting
for a message to be sent by another task. We detect this pattern from our communication trace and then apply a source code transformation using the ROSE

Fig. 6. Late-Sender pattern

Using MPI Communication Patterns to Guide Source Code Transformations

259

Fig. 7. Source code transformation: before (left), after (right)

compiler framework that introduces non-blocking communication, as shown in
the pseudo-code in Fig. 7. Our transformation replaces the blocking MPI Recv
with its non-blocking MPI Irecv version and adds the matching MPI Wait so as
to provide the largest possible communication/computation overlap allowed by
data dependencies on the receive buﬀer.

6

Conclusion and Future Work

This paper presents our work to automate the analysis and optimization of
parallel scientiﬁc applications by combining dynamic runtime information in the
form of communication patterns with static analysis and code transformation.
We use the dynamic information to identify the Send- and Receive Events that
are part of ineﬃcient communication patterns and are good targets for source
code optimizations.
In our ongoing research, we are extending the library of ineﬃcient MPI communication patterns that we optimize beyond the communication/computation
overlap transformation described here. In particular, we will introduce static
and dynamic analysis as well as the corresponding transformation engine for automatically adding collective operations. Further, we will provide several other
novel methods for selecting and ﬁltering MPI seed events that will help us ﬁnd
more complex patterns that lead to communication bottlenecks.

References
1. The List of Worlds 500 Fastest Supercomputers, www.top500.org
2. Bassetti, F., Davis, K., Quinlan, D.: Improving Scalability with Loop Transformations and Message Aggregation in Parallel Object-Oriented Frameworks for Scientiﬁc Computing. In: Computing, Information, and Communications Division, Los
Alamos, NM, USA (1998)

260

R. Preissl et al.

3. Kranzlm¨
uller, D., Grabner, S., Volkert, J.: Event Graph Visualization for Debugging Large Applications. In: Proc. of SIGMETRICS Symposium on Parallel and
Distributed Tools, SPDT Philadelphia, PA, USA, pp. 108–117 (1996)
4. Kranzlm¨
uller, D.: Communication Pattern Analysis in Parallel and Distributed
Programs. In: Proc. of the 20th IASTED International Multi-Conference Applied
Informatics (AI 2002), International Symposia on Software Engineering, Databases,
and Applications, International Association of Science and Technology for Development (IASTED). ACTA Press, Innsbruck (2002)
5. Kn¨
upfer, A., Kranzlm¨
uller, D., Nagel, W.E.: Detection of Collective MPI Operation
Patterns. In: Kranzlm¨
uller, D., Kacsuk, P., Dongarra, J. (eds.) EuroPVM/MPI
2004. LNCS, vol. 3241, pp. 259–267. Springer, Heidelberg (2004)
6. Kranzlm¨
uller, D.: Event Graph Analysis for Debugging Massively Parallel Programs. PhD thesis, GUP Linz, Johannes Kepler University Linz, Austria (2000)
7. Gusﬁeld, D.: Algorithms on Strings, Trees, and Sequences. Computer Science and
Computational Biology (1997)
8. Dixit, K., Gerlach, W., Maekinen, Vaelimaeki, N.: Engineering a Compressed Suﬃx
Tree Implementation. Department of Computer Science, Series of Publications C,
Report C-2006-37, University of Helsinki, Finland (2006)
9. Hon, W.-K., Sadakane, K., Sung, W.-K.: Breaking a time-and-space barrier in
constructing full-text indices. In: FOCS 2003: Proceedings of the 44th Annual
IEEE Symposium on Foundations of Computer science, Washington, DC, USA, p.
251 (2003)
10. Horwitz, S., Reps, T., Binkley, D.: Interprocedural slicing using dependence graphs.
ACM Transactions on Programming Languages and Systems, p. 26 (1990)
11. Schordan, M., Quinlan, D.: A source-To-Source Architecture for User-Deﬁned Optimizations. Lawrence Livermore National Laboratory, USA (2003)
12. Quinlan, D.: Compiler Support for Object-Oriented Frameworks. Lawrence Livermore National Laboratory, USA (1999)
13. Panas, T., Quinlan, D., Vuduc, R.: Tool Support for Inspecting the Code Quality
of HPC Architectures. Lawrence Livermore National Laboratory, USA (2007)

