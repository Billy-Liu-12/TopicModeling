Modeling of 3D Scene Based on Series of
Photographs Taken with Diﬀerent
Depth-of-Field
Marcin Denkowski1 , Michal Chlebiej2 , and Pawel Mikolajczak1
1

2

Faculty of Computer Science, Maria Curie-Sklodowska University,
pl. Marii Curie-Sklodowskiej 5, 20-031 Lublin, Poland
denmar@goblin.umcs.lublin.pl
Faculty of Mathematics and Computer Science, N. Copernicus University,
Chopina 12/18, 87-100 Toru´
n, Poland

Abstract. This paper presents a method for fusing multifocus images
into enhanced depth-of-ﬁeld composite image and creating a 3D model of
a photographed scene. A set of images of the same scene is taken from a
typical digital camera with macro lenses with diﬀerent depth-of-ﬁeld. The
method employs convolution and morphological ﬁlters to designate sharp
regions in this set of images and combine them together into an image
where all regions are properly focused. The presented method consists of
several phases including: image registration, height map creation, image
reconstruction and ﬁnal 3D scene reconstruction. In result a 3D model
of the photographed object is created.

1

Introduction

Macro photography is a type of close-up photography with magniﬁcation ratios
from about 1:1 to about 10:1. The most crucial parameter of macro photography
is the depth of ﬁeld (DOF) [1]. Because it is very diﬃcult to obtain high values
of DOF for extreme close-ups it is essential to focus on the most important part
of the subject. Any other elements that are even a millimeter farther or closer
may appear blurred in the acquired photo. The depth of ﬁeld can be deﬁned
as the distance in front of and behind the subject appearing in focus. Only a
very short range of the photographed subject will appear in exact focus. The
most important factor that determine whether the subject appears in focus is
how a single point is mapped onto the sensor area. If a given point is exactly
at the focus distance it will be imaged as one point on the sensor, in the other
case it will produce a disk whose border is known as a “circle of confusion”.
These circles can be used to deﬁne the measure of focus and blurriness as they
increase in diameter the further away they are from the focus point. For a speciﬁc
ﬁlm format, the depth of ﬁeld is described as a function parametrized by: the
focal length of the lens, the diameter of the lens opening (the aperture), and
the distance between the subject and the camera. Let D be the distance at
which the camera is focused, F the focal length (in millimeters) calculated for
M. Bubak et al. (Eds.): ICCS 2008, Part II, LNCS 5102, pp. 25–34, 2008.
c Springer-Verlag Berlin Heidelberg 2008

26

M. Denkowski, M. Chlebiej, and P. Mikolajczak

an aperture number f and k - the “circle of confusion” for a given ﬁlm format
(in millimeters), then depth of ﬁeld (DOF) [1] can be deﬁned as:
DOF1,2 =

D
1±

1000×D×k×f
F2

(1)

where DOF1 is distance from the camera to the far depth of ﬁeld limit, and
DOF2 is the distance from the camera to the near depth of ﬁeld limit. The
aperture controls the eﬀective diameter of the lens opening. Reduction of the
aperture size increases the depth of ﬁeld, however, it also reduces the amount
of light transmitted. Lenses with a short focal length have a greater depth-ofﬁeld than long lenses. Greater camera-to-subject distance results in a greater
depth-of-ﬁeld.
We used this optical phenomenon to achieve two aims. The ﬁrst one was
to obtain the deepest possible depth-of-ﬁeld using standard digital camera images and image processing algorithms. And the second goal was to create threedimensional model of photographed scene.
As an input we have created a series of macro photograph images of the
same subject with diﬀerent focus lengths. In the ﬁrst step of our method we
have to register them together to create a properly aligned stack of images.
The next step is to fuse them into a one composit image. For that purpose
we propose enhanced multiscale convolution and morphology method, which we
have introduced in [2]. Methods for image fusion using multiscale morphology
have been broadly discussed in [3,4,5]. As an eﬀect of fusing algorithm we obtain
a height map and the reconstructed focused image with a very deep depth-ofﬁeld. The height map is a label map which determines the height of each part
of the scene. From this map, we can construct a 3D model of the scene. In this
work we limit our method to macro photography only and we assume that images
were taken perpendicularly or almost perpendicularly to the scene. However, to
clearly present advantages and problems of our method, we also show some cases
with sets of images acquired in diﬀerent way.

2

Registration

In the ﬁrst step a set of photographs of the desire object is acquired. Unfortunately, during extreme close-up sessions small movements of the camera are
possible even when using tripods for stabilization. To make the reconstruction
method more robust we can make use of an image registration procedure. The
main idea behind image registration is to ﬁnd perfect geometric alignment between a set of overlapping images. The quality of match measure represents the
matching function parametrized by the geometric transformation. In our method
we use the rigid (translations and rotation) or the aﬃne transformation model
(rigid + scaling and shears). In most cases it is suﬃcient to use the simpliﬁed
rigid transformation (translations only). But when images are acquired without
stabilization devices the use of complete aﬃne transformation is a necessity. In

Modeling of 3D Scene Based on Series of Photographs

27

our approach we use the normalized mutual information [7] as the matching
function:
h(F I) + h(RI)
N M I(F I, RI) =
(2)
h(F I, RI)
where RI represents the reference image and F I represents the ﬂoating image.
h(F I) = −

pF I (x)log(pF I (x))

(3)

h(RI) = −

pRI (x)log(pRI (x))

(4)

h(F I, RI) = −

pF I,RI (x, y)log(pF I,RI (x, y))

(5)

where h(F I), h(RI) and h(F I, RI) are the single and joint entropies [2], pF I
and pRI are the probabilities of each intensity in the intersection volume of
both data sets and pF I,RI is a probability distribution of a joint histogram. For
the minimization of the selected similarity measure we use Powell’s algorithm
[8]. As a result of the registration procedures we obtain a set of geometrically
matched images that can be used in the next stages of our wide depth of ﬁeld
reconstruction algorithm.

3
3.1

Image Fusion
Overview

Image fusion is a process of combining a set of images of the same scene into
one composite image. The main objective of this technique is to obtain an image
that is more suitable for visual perception. This composite image has reduced
uncertainty and minimal redundancy while the essential information is maximized. In other words, image fusion integrates redundant and complementary
information from multiple images into a composite image but also decreases dimensionality. There are many methods discovered and discussed in literature
that focus on image fusion. They vary with the aim of application used, but
they can be mainly categorized due to algorithms used into pyramid techniques
[10,11], morphological methods [3,4,5], discrete wavelet transform [12,13,14] and
neural network fusion [15].
The diﬀerent classiﬁcation of image fusion involves pixel, feature and symbolic
levels [16]. Pixel-level algorithms are low level methods and work either in the
spatial or in transform domain. This kind of algorithms work as a local operation
despite of transform used and can generate undesirable artifacts. These methods
can be enhanced by using multiresolution analysis [10] or by complex wavelet
transform [14]. Feature-based methods use segmentation algorithms to divide
images into relevant patterns and then combine them to create output image by
using various properties [17]. High-level methods combine image descriptions,
typically, in the form of relational graphs [18].

28

3.2

M. Denkowski, M. Chlebiej, and P. Mikolajczak

Methodology

In our work we use multiscale convolution and morphology methods combined
with pyramid segmentation algorithm to distinguish homogeneous regions. Our
fusion method is also capable to work with color images. Color image fusion has
been discussed in [19]. At this stage we assume that images on the image stack
are aligned to each other. At this point the main objective is to create focused
image and the height map. The whole algorithm, shown in Fig. 1, can be divided
into 5 stages:
1. Creation of n-level multiresolution pyramid for every input image. In this
case we use median ﬁlter to downscale images.
2. Segmentation of every image on the stack by using pyramid segmentation.
For this process we convert images into HSL color model [9] to separate luminance (contrast) information contained in luminance channel from color
description in hue and saturation channels. Example results of the segmentation process are shown in Fig. 2 as segmentation maps.
3. Calculation of local standard deviation SD at local region R for every pixel
f (x, y) at each pyramid level L for every image on the stack (z):
(L)

SDR (x, y, z) =

1
NR

(f (x, y) − fR )2

(6)

(x,y)∈R,z

Color RGB components are coverted to its graylevel intensity according to
Gf = 0.299R + 0.587G + 0.114B.
4. Reconstruction rules.
(0)
Step-1. For the lowest level of pyramid, pixels with maximum SDmax (x, y, z)
are marked as focus and labeled in the height map HM (x, y) with z value. If
(0)
(0)
abs(SDmax (x, y) − SDmin (x, y)) < Ts , where Ts is a threshold value, pixel
is marked as unresolved because it usually belongs to smooth region. These
pixels are taken care of at subsequent steps.
Step-2. Every pixel is checked with the segmentation map. If it isn’t nearby
any edge and its SDR (x, y, z) value drastically diﬀer from SDR (x, y, z) average pixel value for its region R it is marked with SDR (x, y, z) value of the
median pixel. It prevents from marking false or noise pixels.
(i)
Step-3. For every i-th pyramid level, starting from i=1, if SDR (x, y, z) of
(i−1)
actual pixel is not equal to SDR (x, y, z) from previous pyramid level,
then:
(a) if the pixel is nearby some edge marked on the segmentation map, pixel
(i)
(i−1)
with max(SDR (x, y, z), SDR (x, y, z)) value is taken and labeled in
the height map HM (x, y) with (i) or (i − 1) value,
(b) else, the height map HM (x, y) is labeled as:
HM (x, y) = HM (i−1) (x, y) +

HM (i) (x, y) − HM (i−1) (x, y)
2

(7)

Modeling of 3D Scene Based on Series of Photographs

29

Fig. 1. Image Fusion scheme using pyramid decomposition and HSL Segmentation

Step-4. Labeling remaining pixels. If unresolved pixel belongs to region with
many other unresolved pixels it is marked as a background, else the median
value from region is taken.
5. Creation of fusing image. The value of fused image pixel f (x, y) is equal to
the pixel f (z) (x, y) from z − th input image on the stack, where z is a value
taken from created height map HM (x, y).
The main diﬃculty is to obtain the height map without spikes or noise, generally smooth but with sharp edges. It is not essential from the point of view
of the image fusion, but it may be crucial in three-dimensional reconstruction
of the scene. Most of such peaks are generated in smooth regions, where noise
in defocused region on one image from the stack often gives greater values of
SD than in the corresponding region on sharp image. This leads to undesired
deformations of reconstructed spatial surface. For that reason, it is necessary to
determine a background plane. For now, we assumed that the background plane
overlaps with the last image on the stack, but the plane equation may be also
given by hand.
Fusion process often creates halo eﬀects near the edges of objects. This phenomenon can be observed in Fig. 3. To resolve this problem we use segmentation
maps to determine edges. After that we are able to mark pixels near edges properly as shown in Step-2 and Step-3 of Reconstruction rules.

4

3D Scene Creation

Spatial scene is generated on the basis of information contained in the height
map, where each pixel value represents z coordinate of appropriate mesh vertex.
In 3D reconstruction process we have considered two methods e.i. marching cubes
algorithm (MS) [20,21] and simply by changing z coordinate in a 3D regular
mesh. Both methods have advantages as well as disadvantages. Marching-cube
gives more control over reconstruction process but is also more complicated and
sometimes produces too sharp blocky edges, while second method is very simple
and fast but always produces regular mesh. Generated mesh is decimated and
smoothed. Created surface is textured with a plane mapping by the fused image.

30

M. Denkowski, M. Chlebiej, and P. Mikolajczak

Fig. 2. Segmentation maps created by using pyramid segmentation (right column) for
multifocus images (left column)

Fig. 3. Example of halo eﬀect. Part of the original image (a), the segmentation map
(b), the height map created without using the segmentation map - visible halo eﬀect
(c) and edges in the height map with help of the segmentation map (d).

5

Experimental Results

The proposed method has been implemented on Linux platform in C++ language
using SemiVis framework [22] and Kitware VTK library for visualisation purposes.
For testing procedure we have prepared eight image stacks from macrophotography. Each stack contains six to twelve images taken with diﬀerent depth-of-ﬁeld,
and one control image taken with the largest possible depth-of-ﬁeld that we were
able to receive from our testing digital camera with macro lens.
In all cases the procedure is performed in the following order. At ﬁrst, the
registration process aligns multifocus images to each other to minimize misregistration. Then all images are segmented and the pyramid is created up to three
levels. Finally, the reconstruction process combine image stack into height map
and fused image.
Reconstruction time strongly depends on the size of the images used in the fusion and the number of images on the stack. The most computationally expensive
is the registration procedure, which consumes above ﬁfty percent of the overal reconstruction time. The fusion process takes about 35%, and generation of three
dimensional mesh takes remaining 15%. For a typical set of images, containing ten
images with resolution 512x512 the whole procedure lasts about 60 seconds.

Modeling of 3D Scene Based on Series of Photographs

31

Fig. 4. Sets of multifocus images (1,2,3abc), reconstructed focus image (1,2,3e), created
height map (1,2,3f), control image taken with the largest possible depth-of-ﬁeld (1,2,3g)

Examples of multifocus images with height map and reconstructed fused images are shown in Fig. 4. Each fused image is compared to its control image.
Mutual Information (MI) and Mean Square Diﬀerence (MSD) are useful tools
in such comparison. Table 1 contains calculated similarity values for every fused
image and corresponding reference image.
Table 1 also contains widely used metric QAB/F that measures quality of
image fusion. This measure was proposed by Xydeas and Petrovi´c in [23]. In this
case, a per-pixel measure of information preservation is obtained between each
input and the fused image which is aggregated into a single score QAB/F using a
simple local importance assignment. This metric is based on the assumption that
fusion algorithm that transfers input gradient information into result image more
accuretely performs better. QAB/F is in range [0, 1] where 0 means complete loss
of information and 1 means perfect fusion.
From the height map and fused image we can generate 3D model of the scene.
Additionally, the height map is ﬁltered with strong median and gaussian ﬁlter

32

M. Denkowski, M. Chlebiej, and P. Mikolajczak

Table 1. Similarity measures between reconstructed image and reference image with
large depth-of-ﬁeld - MI and MSD and the quality measure QAB/F
Stack

Similarity measures
MI
MSD

S-1
S-2
S-3
S-4
S-5
S-6
S-7
S-8

0.82
0.67
0.72
0.88
0.82
0.64
0.69
0.71

28.48
32.11
38.43
26.03
27.74
35.81
34.30
41.65

QAB/F
0.84
0.73
0.79
0.85
0.80
0.72
0.72
0.69

Fig. 5. Result fused images and 3D models

Modeling of 3D Scene Based on Series of Photographs

33

Fig. 6. Typical image that creates failed 3D model. This photograph presents a common child’s spinning top. Reconstruction algorithms failed because of many smooth
and uniform regions and a lack of background plane.

to smooth regions and after that the mesh is created. Fig. 5 shows qualitative
results of our method for eight tested image sets. The biggest problem in this 3d
reconstruction is to obtain a surface which is smooth enough in uniform regions
and simultaneously has sharp edges on the objects boundaries. The best results
are received when the photographs are taken perpendicularly to the background,
objects are within the scene, and they are rough without smooth regions.
Fig. 6 shows an example of a typical failure. Our method often fails when
there are large smooth regions which don’t belong to the background plane. The
main diﬃculty in such cases is to distinguish between background and an object
without any external spatial knowledge of the scene.

6

Conclusions

This paper presented an attempt to the problem of generating of 3d model from a
set of multifocus images. We proposed the whole pipeline from raw photographs
to the ﬁnal spatial model. Input multifocus images were registered together and
next, using typical image ﬁlters and gradient methods the height map was created by detecting focused regions in each of them. Based on the height map the
image with a greater depth-of-ﬁeld was composed. Finally, further algorithms
reconstructed the 3d model of the photographed scene.
The presented results of generation of 3D models are very promising, but
as for now, there are still many problems that need to be solved. Future work
could include improvements in segmentation and edge detection to help in automatic detection of the background plane. Second, there should be more complex
methods used to identify smooth regions of objects. We think that in both cases
pattern recognition algorithms should improve eﬀectiveness of our method. Also
Feature-based fusion methods such as [17] could generate more accurate height
maps.

References
1. Constant, A.: Close-up Photography. Butterworth-Heinemann (2000)
2. Denkowski, M., Chlebiej, M., Mikolajczak, P.: Depth of ﬁeld reconstruction method
using partially focused image sets. Polish Journal of Environmental Studies 16(4A),
62–65 (2007)

34

M. Denkowski, M. Chlebiej, and P. Mikolajczak

3. Ishita, D., Bhabatosh, C., Buddhajyoti, C.: Enhancing eﬀective depth-of-ﬁeld by
image fusion using mathematical morphology. Image and Vision Computing 24,
1278–1287 (2006)
4. Mukopadhyay, S., Chanda, B.: Fusion of 2d gray scale images using multiscale
morphology. Pattern Recognition 34, 1939–1949 (2001)
5. Matsopoulos, G.K., Marshall, S., Brunt, J.N.M.: Multiresolution morphological
fusion of mr and ct images of the human brain. IEEE Proceedings Vision, Image
and Signal Processing 141(3), 137–142 (1994)
6. Eltoukhy, H., Kavusi, S.: A computationally eﬃcient algorithm for multi-focus
image reconstruction. In: Proceedings of SPIE Electronic Imaging (June 2003)
7. Studholme, C., et al.: An overlap invariant entropy measure of 3D medical image
alignment. Pattern Recognition 32(1), 71–86 (1999)
8. Press, W.H., Flannery, B.P., Teukolsky, S.A., Vetterling, W.T.: Numerical Recipes
in C, 2nd edn. Cambridge University Press, Cambridge (1992)
9. Gonzalez, R.C., Woods, R.E.: Digital image processing. Addison-Wesley Publishing
Company, Inc, Reading (1992)
10. Burt, P.J.: The pyramid as a structure for eﬃcient computation. In: Multiresolution
Image Processing and Analysis, pp. 6–35. Springer, Berlin (1984)
11. Toet, A.: Image fusion by rati of low-pass pyramid. Pattern Recognition Letters 9(4), 245–253 (1989)
12. Li, H., Manjunath, H., Mitra, S.: Multisensor image fusion using the wavelet transform. Graphical Models and Image Processing 57(3), 235–245 (1995)
13. Chibani, Y., Houacine, A.: Redundant versus orthogonal wavelet decomposition
for multisensor image fusion. Pattern Recognition 36, 879–887 (2003)
14. Lewis, L.J., O’Callaghan, R., Nikolov, S.G., Bull, D.R., Canagarajah, N.: Pixeland region-based image fusion with complex wavelets. Information Fusion 8, 119–
130 (2007)
15. Ajjimarangsee, P., Huntsberger, T.L.: Neural network model for fusion of visible
and infrared sensor outputs, Sensor Fusion, Spatial Reasoning and Scene Interpretation. In: The International Society for Optical Engineering, SPIE, Bellingham,
USA, vol. 1003, pp. 152–160 (1988)
16. Goshtasby, A.A.: Guest editorial: Image fusion: Advances in the state of the art.
Information Fusion 8, 114–118 (2007)
17. Piella, G.: A general framework for multiresolution image fusion: from pixels to
regions. Information Fusion 4, 259–280 (2003)
18. Wiliams, M.L., Wilson, R.C., Hancock, E.R.: Deterministic search for relational
graph matching. Pattern Recognition 32, 1255–1516 (1999)
19. Bogoni, L., Hansen, M.: Pattern-selective color image fusion. Pattern Recognition 34, 1515–1526 (2001)
20. Lorensen, W.E., Cline, H.E.: Marching cubes: A high resolution 3D surface construction algorithm. Computer Graphics 21(4), 163–169 (1987)
21. Durst, M.J.: Additional reference to Marching Cubes. Computer Graphics 22(2),
72–73 (1988)
22. Denkowski, M., Chlebiej, M., Mikolajczak, P.: Development of the cross-platform
framework for the medical image processing. Annales UMCS, Sectio AI Informatica III, 159–167 (2005)
23. Xydeas, C., Petrovi´c, V.: Objective image fusion performance measure. Electronics
Letters 36(4), 308–309 (2000)

