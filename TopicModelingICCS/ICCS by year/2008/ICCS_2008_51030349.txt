Discovering Knowledge in a Large Organization
through Support Vector Machines
J.A. Gutiérrez de Mesa and L. Bengochea Martínez
Computer Science Department, The University of Alcalá
Ctra. Barcelona km. 33.6. 28871 Alcalá de Henares (Madrid), Spain
{jagutierrez,luis.bengochea}@uah.es

Abstract. Much of the information used by an organization is collected in the
form of manuals, regulations, news etc. These are grouped into controlled
documentary collections, which are normally digitized and accessible via a
content management system. However, obtaining new knowledge from
collected documents in an organization requires not only sound search and
retrieval of information tools, but also the techniques to establish relationships,
discover patterns and provide overall descriptions of the entire contents of the
collection. This article explores the nature of knowledge and the role that
occupy the documentary collections as a source of obtaining him knowledge. It
also describes the collection of documents will be used along the exposure of
this study and the techniques of processing information in order to obtain the
desired results. This paper describes the use of computational methods, support
vector machines in particular, in a large organisation for document
classification.
Keywords: Stemming, Indexation, Support Vector Machines, Documentation
and Knowledge management.

1 Introduction
Knowledge Management (KM) as discipline has acquired importance in recent years.
The number of scientific articles devoted to this discipline has increased in recent
years. One of the main features of knowledge management is its heavy reliance on
related disciplines such as information retrieval, data mining, databases and content
management systems (CMS). It can even become the standard technology for the
implementation of programs of knowledge management [1].

2 Document Collection
In this work, we used a collection of articles published by a Spanish newspaper
between July 1, 2004 and June 30, 2006 (two years). The collection consisted of a
total of 2,067 documents. The sum of all the words in all documents of the collection
was 883,425. The number different character was 104 and the total number of
characters used in all documents was 5,441,472. Most of the documents have a length
between 350 and 500 words (83%).
M. Bubak et al. (Eds.): ICCS 2008, Part III, LNCS 5103, pp. 349–357, 2008.
© Springer-Verlag Berlin Heidelberg 2008

350

J.A. Gutiérrez de Mesa and L. Bengochea Martínez

2.1 Modelling Documents
Regardless of who is elected one level or another in the choice of terms, in all cases
are going to get vectors with many dimensions. In our case, the dimensionality of
space vector is given by the number of different words that are used in each and every
one of the documents that are part of the collection. Of the 883,425 words contained
in the collection, 37,402 are different, which is the vocabulary V of the collection of
37,402 words.
n

W=

∑w

j

= 883,425;

(1)

i =1

One way to reduce the dimensionality is to delete words that do not add any
meaning to the text (empty words) and another way is to group words that have the
same root in a single lexical (Stemming) such that the total number of different words
is reduced. These two processes are described below.
2.2 Normalization Process

In order to be able to run the algorithms, the first step is to transform the documents
into plain text and extract the vectors that represent each of the documents. Then,
there is a standardization step to facilitate the extraction of measures, such as
frequencies and being the normalization the most common operation [2].
2.3 Selection of the Vocabulary

Since the documents will be classified according to their textual content, it is possible
to discard all those terms that do not provide relevant information for this purpose.
Human languages include many words that are only used to articulate phrases, but do
not add any meaning to the text. Also, those words have with very high frequencies.
Other words less frequent but more useful for the text to discriminate on the basis of
their content.
The set of words that can be regarded as irrelevant or empty for a given language,
in our case Spanish, is a priori comprised of the following categories: common
adjectives, articles, adverbs, prepositions, conjunctions. Interjections, pronouns,
auxiliary verbs (e.g., be, can, do, etc.) and modal verbs (e.g., power, hold, sing, etc.).
Once the list of empty words is built and after their removal, we have the following
values: total number of empty words is 503,198; total number no empty words:
380,227 and number of different not empty words: 36,352.
While the elimination of empty words considerably reduces the size of the text,
another technique is to remove those words does not reach a certain threshold to
reduce the dimensionality of space vector [3]. This technique is based on the idea that
when a term appears very infrequently in a collection of documents, their
discrimination capabilities is virtually zero, so it can be ruled out at the time of

Discovering Knowledge in a Large Organization through Support Vector Machines

351

building the model to represent the documents [4]. With this threshold, and after the
stemming the process described in the next section, the size of vocabulary will
increase from V=13,256 to be V = 6,768, i.e. get a reduction of the dimensionality of
nearly only 50% by removing words that appear only once, twice or three times in the
set of all documents that form the collection.

3 Stemming
The basis of a lemmatizer consists of a finite state machine that tries to represent
changes in a certain suffix stem. Each suffix involves a series of rules that express
how a suffix has been incorporated into the stemming. Since, there can be many
variations and exceptions for the same suffix, the PLC can sometimes be quite
complex. From these bases, are developing various algorithms stemming for years,
such as those based on the probability that a word belongs to the class defined for a
stem [5].
Almost all lemmatizers are built upon the foundation of the work by Lovin [6] in
1968 and variants such as those described by Dawson [7], Porter [8] and Chris D.
Dave [9]. We have also built a Lemmatizer to apply to documents from the collection
object of our study, based on the works of Porter and other more specific to the
Spanish language [10].
3.1 Vector Construction

With a very large number of elements that are zero, the following the nomenclature is
used to represent every element of the non-zero vector: {Position: Value}, where
Position is an ordinal representing the position it occupies in the lexeme vocabulary,
and Value is the measure of the contribution that lexeme in the full meaning of the
document, Di. Therefore, a document is represented by the vector:
Di = {w1 : f1 , w2 : f 2 , ....

wni : f ni ,}

(2)

The metrics to be used is TF x IDF and vectors will be standardized (|Di | = 1) so
that the values of fj will be given as:

TF ( w j , Di ) log(
fj =

∑
j

| Di |
)
DF ( w j )

⎡
| Di | ⎤
)⎥
⎢TF ( w j , Di ) log(
DF ( w j ) ⎥⎦
⎢⎣

2

(3)

Where TF(pj,Di) represents the frequency with which appears lexeme that took the
position pi in the document Di; DF(pj) is the frequency of that same lexeme in the
entire collection. Applying the formula to documents, get vectors as shown in Fig. 1
and who will be that we use from this point forward.

352

J.A. Gutiérrez de Mesa and L. Bengochea Martínez

4 Classification of Documents
In our study, we use the thesaurus Eurovoc [11] for selecting the categories to which
documents may be assigned. This choice was justified by the need to have a package
that covers all possible areas addressed in the documents. Moreover, it has been
developed by experts following strict criteria.
Prior to the construction and implementation of our own classifier based on the
technique known as Support Vector Machine (SVM) [12], which is the one that
obtained better results classifying documents [13] , we will give a brief description of
some of the most commonly used methods for grading.
In all cases it is building a model by automatic learning from a set of documents
previously tagged by an expert. The model thus constructed will be able to deduct the
class to which should be given every new document unknown to be present. This type
is called supervised learning, because it gives the system the list of categories to
which they belong all documents of a collection. A system of unsupervised learning,
which builds a model able to infer the existence of clusters of documents, and hence
"discover" a class structure that is not known in advance. The action taken by this
type of system will call the "grouping of documents", and will be treated in the
following point, to differentiate it from the classification of documents "or" text
categorization "study in this point.
4.1 Support Vector Machines

Recent studies [14], [15], [16], [17] show that Support Vector Machines (SMV) are
the preferred method for text classification. Unlike other methods, SVM can work
efficiently with thousand of dimensions whereas in other classifiers, when there is a
large number of attributes with little discrimination power, attributes need to be
discarded by some preprocessing filters affecting their performance [18]. However,
despite their high accuracy documented in numerous publications, and perhaps
because of their complexity, SVMs have failed to completely replace simpler methods
of automatic classification such as Naïve-Bayes [19].
SVM is based on the concept of minimizing risk structural which is found in the
vector space which is represented as vectors documents, hyperplane separating those
who belong to two different categories, and also do so with the greatest possible
margin of separation. The position in space, occupy any new document, the class to
determine who should be allocated. It is therefore a classifier binary and to build a
multi classifier must be calculated so as hyperplans classes there.
To carry out the classification of the collection, we are going to use the program
package SVM light [20] that allow us to employ algorithms SVM learning, with
different parameters and kernel functions, to suit the nature of our problem. However,
how to use through orders or commands in text mode has lifted us to develop a GUI
to implement the programs. In the preparation phase are formed vectors that are going
to represent the set of documents or evidence of learning, properly labelled according
to their membership in the class for which we construct the grid. Through panel
“svm_learn” (Fig. 1) allows the user to execute the learning module with the options
you choose. To carry out this operation is necessary and at least one file of learning.
The outcome of this panel will be a file with the model for classifying built.

Discovering Knowledge in a Large Organization through Support Vector Machines

353

Fig. 1. Panel WinSvm for obtaining model in the training phase

5 Experimental Results
To calculate the optimal values to apply when constructing models for qualifying, we
chose the three categories that belong to a larger number of documents on joint
training. These are: “04 Life policy” “08 International relations” and “28 Social
Affairs”. In order to have two sets of classified documents manually, one for the
training phase and the other to check the behaviour of the model built, the initial set of
104 documents have been divided into two sets of 52 papers each. It has been tested
with the four types of kernel function possible and the results are shown in Table 1.
Table 1. Classification with 52 papers training
Model construction
Category

04
04
04
04
08
08
08
08
28
28
28
28

Test over 52

Kernel

Iterant.

Kern Evl.

%Success

#Failures

Precision

Recall

Lineal
Polynomic
Sigmoid
RBF
Lineal
Polynomic
Sigmoid
RBF
Lineal
Polynomic
Sigmoid
RBF

17
19
9
20
19
23
16
26
19
26
17
22

1556
3044
2490
3099
1666
3264
2870
3429
1666
3429
2934
3209

69,23
71,15
46,15
73,08
71,15
71,15
67,31
67,31
71,15
73,08
71,15
71,15

16
15
28
14
15
15
17
17
15
14
15
15

72,22
73,68
46,15
75,00
60,00
75,00
50,00
100,00
-

54,17
58,33
100,00
62,50
35,29
17,65
5,88
6,67
-

354

J.A. Gutiérrez de Mesa and L. Bengochea Martínez

The kernel function used to construct the model of learning, in all cases is SVMlight,
a very efficient algorithm in relation to CPU time. Therefore, the only criterion we
should look to choose between the modes is the rate of accuracy, leaving aside other
considerations such as the number of iterations or the number of reviews of the kernel
function used. Table 1 shows that is the best-performing kernel functions are linear
functions and polynomic, with a slightly advantage for the latter. Therefore, for the
construction of the binder will use a polynomic kernel

r
r
( x1 * x

2

+ 1)

3

.

5 The Emergence the New Knowledge
There are several algorithms to identify relevant phrases in a document. The most
interesting are supervised learning algorithms, as C4.5, KEA or GenEx attempting to
document as a set of phrases that should be classified as relevant or irrelevant. To do
so, it must provide before a set of documents belonging to a body similar to those
discussed and whose relevant phrases are known in advance. From this set and
through a process of training builds a table of discretization of the characteristics
associated with the terms deemed relevant documents joint training. Once the system
is trained, the process of automatic identification of the terms will be considered the
metadata value of a new document. It consists of the following: after a period of
normalization of the text obtained a first relationship sintagmes candidates, discarding
those that do not meet a number of conditions (that is not its length between a
maximum and a minimum preset, which begin or end with empty words, which do
not reach a minimum frequency of occurrence, etc.). . It was also put to candidates for
a phase Stemming with the aim of considering only the roots of words and thus
increase the value of their frequencies. Then, a discreet rate of each term, based on the
following values:
•

•

•

Relative frequency of occurrence of S phrase in the text in relation to the
overall control (TF × IDF), as measured:
1 + df ( S )
frecuency( S , D )
× (− log 2
)
TF × IDF =
(4)
size( D)
N
where frequency (S, D) is the number of times the phrase appeared in the
paper S D size (D) is the number of words that has the document, df (S)
indicates the number of documents corpus overall contain the term S (adds 1
to avoid log 0) and N is the total number of documents in the overall corpus).
Distance, in the words from the beginning of the text until the first
appearance of the words. The result is a number between 0 and 1 which
represents the portion of the document that precedes the first appearance of a
word:
first time to see a sintagme
distance =
(5)
total words of the document
Frequency with which he has already been considered as relevant among the
objects of all control. This measure is known as frequency k and the

Discovering Knowledge in a Large Organization through Support Vector Machines

355

underlying idea is that a word candidate is more likely to be relevant if it has
been found as other relevant documents corpus training.
For the construction program, we started KEA system, developed in the "Digital
Libraries and Machine Learning Labs" [21] at the University of Waikato and is
distributed under the GNU Public License.
5.1 Clustering

The grouping a collection of papers has historically been perceived by the researchers
as a discovery tool and to help reduce redundancy and demand cognitive [22]. A
system of grouping should have the ability to assign each new document to the group
most appropriate and should therefore be able to solve three problems: how to create
groups, how to identify the relationships between the groups and how to keep the
group system.
The most interesting approach in the form of documents and added that in addition,
has the advantage of providing direct labels of the groups, is to extract the most
relevant phrases for each document in the collection and use as a criterion for
grouping. The relevant phrases are good descriptors of the topics covered in a
document and therefore help build subspaces small, but representative of space full of
documents. The method used to form aggregates is to sort the relevant phrases by the
number of documents that share, from highest to lowest. The first group of documents
on this list, will form the nucleus of the first added, and the term will be shared by the
label of this aggregate. Then he goes through the list of documents added and are
appended documents with which it shares other relevant phrases. When this process is
completed recursive, passed to the next term of the ordered list, and so complete.
Table 2. Aggregates from syntagms of long> = 1 in category 28
Social Issues
Descriptor Eurovoc

Aggregates

2811.- Movimientos migratorios
2816.- Demografía y población
2821.- Marco social

Ley de Extranjería
alto el fuego
política antiterrorista
matrimonio homosexual
Juan Pablo II
EE UU
Benedicto XVI
Bin Laden
Conferencia Episcopal
Ceuta y Melilla
accidentes de tráfico
Severo Ochoa
plan de choque

2826.- Vida social
2831.- Cultura y religión

2836.- Protección social
2841.- Sanidad
2846.- Urbanismo y construcción

356

J.A. Gutiérrez de Mesa and L. Bengochea Martínez

During the process of forming aggregates in each category, you get the average
length of the documents that form, expressed as the average number of days between
July 1, 2004 and the date of publication of document (column "Days (Dias)" on the
Table 2).

6 Conclusions
In this work, we presented the development of a bespoke computational science
application that is going to be used by a large organization to classify documents. To
do so, on the one hand, we presented the latest developments in the techniques of
automatic classification and clustering textual documents. On the other hand, we
showed how to build and validated models using a medium size collection of
documents text in Spanish to perform measurements and results that were not
previously available. Results show that it is possible to generate patterns of searching
documents from the collection, exclusively using automatic learning techniques based
on statistical methods, without having to implement other techniques of natural
language processing.

References
1. Pérez-Montoro, M.: Sistemas de gestión de contenidos en la gestión del conocimiento.
Textos universitaris de biblioteconomia i documentació. 14. Facultat de Biblioteconomia i
Documentació. Universitat de Barcelona (2005)
2. Baeza-Yates, R., Ribeiro-Neto, B.: Modern Information Retrieval. ACM Press, AddisonWesley, New York (1999)
3. Yang, Y., Pedersen, J.: Intelligent information retrieval. Intelligent Systems and Their
Applications, IEEE 14, 4 (1999)
4. Luhn, H.P.: The automatic creation of literature abstracts. IBM Journal of Research and
Development 2, 159–165 (1958)
5. Allan, J., Kumaran, G.: Details on Stemming in the Language Modeling Framework.
Center for Intelligent Information Retrieval. Department of Computer Science. University
of Massachusetts Amherst. Technical Report No. IR289 (2001)
6. Lovins, J.B.: Development of a Stemming Algorithm. Mechanical translation and
computational linguistics 11, 22–31 (1968)
7. Dawson, J.: Suffix removal and word conflation. Bulletin of the Association for Literary &
Linguistic Computing, 33–46 (1974)
8. Porter, M.F.: An algorithm for suffix stripping. Originally published in Program 14(3),
130–137 (1980)
9. Paice, C.D.: Another stemmer. ACM SIGIR Forum archive 24, 56–61 (1980)
10. Figuerola, C.G., Gómez, R., López, E.: Stemming and n-grams in Spanish: An evaluation
of their impact on IR. Journal of Information Science 26, 461–467
11. Eurovoc: Tesauro Eurovoc. Presentación alfabética permutada. Edición 4.2 - Lengua
española. Comunidades Europeas (2006) ISSN 1725-426
12. Joachims, T.: Leaning to classify text using SVM Methods Theory and Algorithms.
Kluwer Academic Publishers, Dordrecht (2001)
13. Sebastiani, F.: Classification of text, automatic. In: Brown, K. (ed.) The Encyclopedia of
Language and Linguistics, vol. 14. Elsevier Science Publishers, Amsterdam (2006)

Discovering Knowledge in a Large Organization through Support Vector Machines

357

14. Flach, P.A.: On the state of the art in machine learning: A personal review. Artificial
Intelligence 131, 199–222 (2001)
15. Yang, Y., Zhang, J., Kisiel, B.: A scalability analysis of classifiers in text categorization.
In: Proceedings of the 26th annual international ACM SIGIR conference on Research and
development in information retrieval. ACM Press, New York (2003)
16. Lewis, D.D., Yang, Y.R., Tony, G., Li, F.: RCV1: A New Benchmark Collection for Text
Categorization Research. The Journal of Machine Learning Research 5 (2004)
17. Lai, C.-C.: An empirical study of three machine learning methods for spam filtering.
Knowledge-Based Systems 20, 249–254 (2007)
18. Gabrilovich, E., Markovitch, S.: Text categorization with many redundant features: using
aggressive feature selection to make SVMs competitive with C4.5. In: Proceedings of the
twenty-first international conference on Machine learning, ICML 2004. ACM Press, New
York (2004)
19. Gayo Avello, D.: BlindLight - Una nueva técnica para procesamiento de texto no
estructurado mediante vectores de n-gramas de longitud variable con aplicación a diversas
tareas de tratamiento de lenguaje natural". Univ. Oviedo. Dpto. de Informática (2005)
20. Joachims, T.: Leaning to classify text using SVM Methods Theory and Algorithms.
Kluwer Academic Publishers, Dordrecht (2001)
21. Frenk, et al.: Domain-specific keyphrase extraction. In: Proc. Sixteenth Int. Joint Conference
on Artificial Intelligence, pp. 668–673. Morgan Kaufmann, San Francisco (1999)
22. Roussinov, D., Chen, H.: A Scalable Self-organizing Map Algorithm for Textual
Classification: Neural Network Approach to Thesaurus Generation. Communication and
Cognition 15(1-2), 81–112 (1998)

