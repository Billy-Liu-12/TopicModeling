Non-uniform Distributions of Quantum Particles
in Multi-swarm Optimization
for Dynamic Tasks
Krzysztof Trojanowski
Institute of Computer Science, Polish Academy of Sciences
Ordona 21, 01-237 Warsaw, Poland
trojanow@ipipan.waw.pl

Abstract. This paper presents research considering mixed multi-swarm
optimization approach applied to dynamic environments. One of the versions of this approach, called mQSO is a subject of our special interest.
The mQSO algorithm works with a set of particles divided into subswarms where every sub-swarm consists of two types of particles: classic
and quantum ones. The research is focused on studying properties of
the latter type. Two new distributions of new locations for the quantum
particles are proposed: static and adaptive one. Both of them are based
on an α-stable symmetric distribution. In opposite to already published
methods of distribution of new locations the proposed methods allow the
locations to be distributed over the entire search space. Obtained results
show high eﬃciency of the mQSO approach equipped with the proposed
two new methods.

1

Introduction

In the presented research a mixed multi-swarm optimization in dynamic environments is studied. Application of mixed multi-swarm approach to dynamic
optimization has already been tested and proved its eﬃciency. Approaches with
static and varying number of subs-warms [1], [2], [3] as well as approaches with
adaptive number of species in the swarm [4], [5], [6] have been researched. A
version with static number of subs-warms called mQSO, where two types of particles: quantum particles and classic ones are in use and especially their rules of
movement became a subject of our interest. Classic particles use velocity vectors
to evaluate their new positions. The rules for quantum particles are based on
the random distribution of possible new locations of a particle around its current location similarly to the distribution of the locations in the quantum cloud
of the atom. The rules for quantum particles proposed in [2], [3] are based on
the idea of uniform distribution over the space of a quantum cloud, which is a
hyper-sphere of a constant radius with the current location in the middle.
In this paper we examine strategies of the quantum particle movement being
alternative to the strategy mentioned above. Two methods of evaluation of new
locations of the quantum particle are proposed and the eﬃciency of the mQSO
equipped with these methods is experimentally veriﬁed.
M. Bubak et al. (Eds.): ICCS 2008, Part I, LNCS 5101, pp. 843–852, 2008.
c Springer-Verlag Berlin Heidelberg 2008

844

K. Trojanowski

As a dynamic test-bed a MPB [7] generator was selected. In MPB we optimize
in a real-valued 5-dimensional search space and the ﬁtness landscape is built of
a set of unimodal functions individually controlled by the parameters allowing
to create diﬀerent types of changes.
The paper is organized as follows. In Sec. 2 there is a brief presentation of
the optimization algorithm. Two new methods of generation of the quantum
particle’s new locations are described in Sec. 3. Section 5 shows a measure used
for evaluation of the results of experiments and the selected testing environment while Sec. 6 – the results of experiments performed with the environment.
Section 7 concludes the presented research.

2

Quantum Multi-swarm

A simple scheme of the particle swarm optimization algorithm is given in Fig. 1.
A PSO optimizer is equipped with a set of particles xi where i ∈ [1, . . . N ]. Each
of the particles represents a solution in an n-dimensional real valued search
space. For the search space a ﬁtness function f (·) is deﬁned which is used to
evaluate the quality of the solutions. A particle yi represents the best solution
found by the i-th particle (called particle attractor), and a particle y∗ – the best
solution found by the swarm (called swarm attractor). The scheme is made for
maximization problem.
Algorithm 1. The particle swarm optimization
1: Create and initialize the swarm
2: repeat
3:
for i = 1 to N do
4:
if f (xi ) > f (yi ) then
5:
yi = xi
6:
end if
7:
if f (yi ) > f (y∗ ) then
8:
y∗ = yi
9:
end if
10:
end for
11:
update location and velocity of all the particles
12: until stop condition is satisﬁed

Search properties of the PSO scheme in Algorithm 1 are represented by the
step ”update location and velocity”. In this step there are two main actions
performed: ﬁrst the velocity of each of the particles is updated and then all the
particles change their location in the search space according to the new values
of velocity vectors and the kinematic laws. Formally for every iteration t of the
search process every j-th coordinate of the vector of velocity v as well as the
coordinate of the location x undergo the following transformation [8]:
vjt+1 = χ(vjt + c1 r1t (yjt − xtj ) + c2 r2t (yj∗t − xtj )) ,
= xtj + vjt+1 ,
xt+1
j

(1)

Non-uniform Distributions of Quantum Particles

845

where r1t and r2t are random values uniformly generated in the range [0, 1], χ is
a constriction factor and χ < 1 and c1 and c2 control the attraction to the best
found personal and global solutions, respectively.
The basic idea presented in Algorithm 1 has been developed for non-stationary
optimization applications. One of the ﬁrst signiﬁcant changes in this scheme is
an introduction of multi-swarm. In the presented approach the number of subswarms is constant during the process of searching. Each of them is treated
as an independent self-governing population which is not inﬂuenced by any of
the neighbors. However, there are mechanisms which periodically perform some
actions based on the information about the state of search of the entire swarm [3].
To guarantee the appropriate distribution of the sub-swarms over the entire
search space the exclusion mechanism eliminates sub-swarms, which are located
too close to each other. When the sub-swarms are too close to each other, the
occupation of the same optimum is most likely to occur. In this case one of them
is selected to be eliminated and a new one is generated from scratch. Any two subswarms are considered as located too close to each other if for the best solutions
from the compared two sub-swarms the euclidean distance is closer than the
deﬁned threshold ρ. In [3] yet another mechanism of sub-swarms’ management
was proposed called anti-convergence, which protects against convergence of subswarms. However it was oﬀ in the presented experiments.
The last of the sub-swarms’ management mechanisms described in [3] is based
on mixing of types of particles in sub-swarms. In the presented research the mixed
sub-swarms consist of two types of particles governed by two diﬀerent rules
of movement. While the location of the particles of the ﬁrst type is evaluated
according to classic formulas as discussed above, the remaining ones are treated
as quantum particles and change their location according to the analogy with
quantum dynamics of the particles. All the particles in such a mixed sub-swarm
share the information about the current best position and the best position ever
found by the sub-swarm.
Idea of quantum particle proposed by Blackwell and Branke in [2] originates
from the quantum model of atom where the trajectories of electrons are described
as quantum clouds. Adaptation of this idea to the model of movement of the
particles rejects kinematics laws used in classic PSO for evaluation of a distance
traveled by the particle with a constant velocity in a period of time. Instead
of this a new position of the quantum particle is randomly generated inside
a cloud of the given range rcloud surrounding y∗ i.e. the current sub-swarm
attractor. In quantum model the particle’s speed becomes irrelevant, because
every location inside the cloud can be chosen as a new location with a non-zero
probability.
The model of quantum particles has been extended in this paper. Since the
model proposed in [2] assumes the uniform distribution of the set of possible
new locations of the particle over the cloud’s space, it was interesting to test
and verify another types of distributions.

846

3

K. Trojanowski

Movement of Quantum Particles

Two new types of distribution of new locations are considered in this paper. The
ﬁrst one is deﬁned with static rules while the second one – adaptive rules. Both
of them are based on two-phase mechanism. In the ﬁrst phase a direction θ is
selected. In the second phase a distance d from the original is calculated.
The direction θ can be obtained with use of a random variable from the
angularly uniform distribution on the surface of a hyper-sphere [9]. The distance
d is an α-stable random variate and is computed as follows:
d = SαS(0, σ), and

(2)

σ = rSαS · (Dw /2),

(3)

where SαS(·, ·) represents α-stable symmetric distribution variate and Dw is a
width of the feasible part of the domain, i.e. a distance between a lower and an
upper boundary of the search space. The new location is based on the found
direction θ and a distance d from the original. This is an isotropic distribution.
The α-stable distribution is controlled by four parameters: stability index α
(α ∈ 0 < α ≤ 2 ), skewness parameter β, scale parameter σ and location
parameter μ. The Chambers-Mallows-Stuck method of generation of the α-stable
symmetric random variables [10] can be used. The method for σ = 1 and μ = 0
with a correction for the case where α = 1 given by Weron in 1996 [11] is
presented in (4). To calculate the α-stable distributed random variate X two
another independent random variates are needed: a random variate U , which
is uniformly distributed on [−π/2, π/2] and an exponential random variate W
obtained with rate parameter λ = 1:
⎧
(1−α)/α
α(U+Bα,β )
cos(U−α(U+Bα,β ))
⎪
⎪ Sα,β · sin(cos
·
,
1
⎪
U) /α
W
⎪
⎪
⎪
⎨ iﬀ α = 1,
(4)
X=
⎪
⎪
2
π
W
cos
U
⎪
⎪
( + βU ) tan U − β ln π +βU ,
⎪
⎪
2
⎩π 2
iﬀ α = 1.
2 πα 1/(2α)
2
.
where Bα,β = α−1 arctan β tan πα
2 , and Sα,β = 1 + β tan 2
In the symmetric version of this distribution (called SαS, i.e. symmetric αstable distribution) β is set to 0. For α = 2 the SαS(μ, σ) distribution reduces to
the Gaussian N (μ, σ) and in the case of α = 1 the Cauchy C(μ, σ) is obtained.
In (3) rSαS is a scale parameter.
The diﬀerence between the static and the adaptive version of the distribution is in the way of calculation of the distance d. In the static version the
distance depends on merely the α-stable generator. In the adaptive version it
depends on the value returned by the generator and multiplied by normalized
ﬁtness of the particle. The latter version was inspired by a mutation operator
introduced in [12] where it was a component of the immune optimization algorithm called opt-aiNet and designed for multimodal function optimization. The

Non-uniform Distributions of Quantum Particles

847

mutation operator uses independent random variates for modiﬁcation of each
of the coordinates. This approach gives isotropic distribution for Gaussian random variables but unfortunately turns into non-isotropic for any other α-stable
distribution in multidimensional search space. We wanted to keep to isotropic
distributions, therefore the operator was not migrated as-is. In our adaptive approach the direction is evaluated in the same way like in the static one but the
distance is calculated respectively to the current ﬁtness values of the remaining
antibodies in P :
(5)
d = SαS(0, σ) · exp(−f (xi )),
where σ is calculated as in (3) and f (xi ) is the ﬁtness of the i-th solution xi
normalized in [0, 1] respectively to the ﬁtness values of all the solutions in P :
f (xi ) − fmin
,
(fmax − fmin )
= max f (xj ) and fmin =

(6)

f (xi ) =
fmax

j=1,...,N

min f (xj ).

j=1,...,N

Fig. 1. Distribution of the new points in 2-dimensional search space for α: 2, 1, 0.5,
and 0.1

Fig. 1 presents sample distributions of a set of points in the 2-dimensional
search space generated for the same original with the static method of generation.
There are four distributions for four diﬀerent values of α: 2, 1, 0.5, and 0.1.
It often happens that the domain of possible solutions is limited by a set of
some box constraints and only the solutions ﬁtted in the constraints are classiﬁed as feasible. Both types of distribution presented above allow to generate new
locations over the entire domain, so it is possible to generate feasible and unfeasible locations as well. From the theoretical point of view we can easily cope with
unfeasible locations simply by allowing them just to stay where they are because
the evaluation function formula is usually deﬁned for all points in Rn . However,
from the engineering point of view we cannot accept such a free treatment since
in the real world the constraints are based on the knowledge of the modeled
phenomenon and represent its features like e.g. temperature (which cannot be
less than -273 C or higher than some reasonable limit: +100 C for a water or a
smoke point for an oil). Therefore in the presented research it is assumed that
the domain of possible solutions is limited by a set of some box constraints and
only the solutions ﬁtted in the constraints are classiﬁed as feasible.

848

K. Trojanowski

Since the main focus in this paper is not about the constrained optimization,
we selected a very simple procedure of immediate repairing unfeasible particles.
Clearly the j-th coordinate of the solution x breaking its box constraints is
trimmed to the exceeded limit, i.e.:
if xj < loj then xj = loj ,
if xj > hij then xj = hij .
The procedure is applied in the same way to both types of the particles, the
classic and the quantum ones. In case of classic particles the velocity vector v
of the repaired particle stays unchanged even if it still leads the particle outside
the acceptable search space.

4

Settings of the Algorithm’s Parameters

The algorithm parameters’ settings applied to the experiments presented below
originate from [3]. In the cited publication authors present results of experiments
obtained for diﬀerent conﬁgurations of swarms tested with the MPB benchmark
where there are 10 moving peaks. Among many tested conﬁgurations the best
results for the optimization problem with 10 moving peaks are obtained where
there are 10 sub-swarms and each of them consists of ﬁve classic particles and ﬁve
quantum ones (see Table III in [3]). The total population of particles consists
of 100 solutions divided equally into 10 sub-swarms. The values of pure PSO
parameters are: c1,2 = 2.05 and χ = 0.7298. For QSO the range of exclusion
is set to 31.5 (for the best performance the value of ρ should be set close to
30. However, the precision of this parameter’s setting is not crucial. In [3] the
authors claim that the algorithm is not very sensitive to small changes of ρ).
In the presented algorithm there is no strategy of detecting the appearance of
change in the ﬁtness landscape. Since our main goal was studying the properties
of the diﬀerent distributions of the quantum particles, we assumed that it would
just introduce yet another unnecessary bias into the obtained values of oﬄine
error and make their analysis even more diﬃcult. Therefore a change is known
to the system instantly as it appears and there is not any additional computational eﬀort for its detection. When the change appears, all the solutions stored
in both classic and quantum particles are reevaluated and the swarm memory is
forgotten. Classic particle’s attractors are overwritten by current solutions represented by these particles and sub-swarms’ attractors are overwritten by the
current best solutions in the sub-swarms.

5

Applied Measure and the Benchmark

In the performed experiments the oﬄine error (brieﬂy oe) measure [7,13] of
obtained results was used. The oﬄine error represents the average deviation
from the optimum of the ﬁtness of the best individual evaluated since the last
change of the ﬁtness landscape. Every time the solution’s ﬁtness is evaluated,

Non-uniform Distributions of Quantum Particles

849

an auxiliary variable is increased by the value which is the deviation of the best
solution evaluated since the last change including the one just evaluated as well.
When the experiment is ﬁnished the sum in the variable is divided by the total
number of evaluations and returned as the oﬄine error. Formally:
1
oe =
Nc

Nc

1
(
N
e (j)
j=1

Ne (j)
∗
(fj∗ − fji
)) ,

(7)

i=1

where Nc is the total number of changes of the ﬁtness landscape in the experiment, Ne (j) is the number of evaluations of the solutions performed for the j-th
state of the landscape, fj∗ is the value of optimal solution for the j-th landscape
∗
(i.e. between the j-th and (j + 1)-th change in the landscape) and fji
is the
current best found ﬁtness value for the j-th landscape i.e. the best value found
among the ones belonging to the set from fj1 till fji where fji is the value of
the ﬁtness function returned for its i-th call performed for the j-th landscape.
During the process of search the oﬄine error can be calculated in two ways:
in one of them the error is evaluated from the beginning of the experiment while
in another one – the value of oﬄine error starts to be evaluated only after some
number of changes in the ﬁtness landscape. The latter way is advised as saddled
with the less measurement error caused by the initial phase of the search process
(please see e.g. [14] for a discussion about the possible inﬂuence of the initial
phase on the quality of the results obtained for MPB). Therefore, just this way
was applied in our tests.
For compatibility with experiments published by others the number of evaluations between subsequent changes of the ﬁtness landscape equals 5000. During a
single experiment the ﬁtness landscape changes 110 times (however for the ﬁrst
10 changes the error is not evaluated). Every experiment was repeated 50 times
and the means are presented.
The parameters of MPB were set exactly the same as speciﬁed in [3] in scenario
2. The ﬁtness landscape was deﬁned for the 5-dimensional search space with
boundaries for each of dimensions set to 0; 100 . For the search space there
exist a set of 10 moving peaks which vary their height randomly within the
interval 30; 70 , width within 1; 12 and position by a distance of 1.

6

Results of Experiments

We started our research from repeating some of the experiments presented in [3].
It was necessary to make the earlier results comparable to the current ones since
in our case a period of the ﬁrst 10 changes in the environment is excluded from
calculating the oﬄine error which makes the values of oe signiﬁcantly smaller
than those in [3]. We repeated experiments with uniform distribution of new
locations inside a quantum cloud (called further Mcloud ) for a series of values of
rcloud : from 0.05 to 4.5 with step 0.05. The three best values of oe were: 1.6264
(std.dev.: 0.4104), 1.6297 (std.dev.: 0.4062), 1.6298 (std.dev.: 0.5227) and they
were obtained for rcloud : 0.30, 0.35 and 0.25 respectively.

850

K. Trojanowski
1.65
1.6
1.55
1.5

1.65
1.6
1.55
1.5

4.5
4
3.5
3
2.5
2
1.5

4.5
4
3.5
3
2.5
2
1.5
0.5
1
0.1
rS α S

0.01

1.5

α

0.5
1
0.1

0.0012

rS α S

1.5

0.01

α

0.0012

Fig. 2. Oﬄine error for Mαstatic i.e. a static version (on the left) and for Mαadapt i.e.
an adaptive version (on the right): rSαS vs. α
Table 1. The three best values of oﬄine error obtained for the two tested methods of
the new location generation for the case with 10 moving peaks
method

oﬄine error

std. dev.

σ

α

Mαstatic [1]
Mαstatic [2]
Mαstatic [3]

1.4603
1.4665
1.5023

0.3066
0.4188
0.3518

0.25
0.25
0.35

1.35
1.80
1.00

Mαadapt [1]
Mαadapt [2]
Mαadapt [3]

1.4614
1.4722
1.5008

0.3255
0.3041
0.3496

0.60
0.85
0.60

1.70
1.70
1.75

In our research we wanted to get performance characteristics of the tested
distributions ﬁrst. This way we are able to compare not only the best results
possible to obtain for a given test-case but also an information about the robustness of the searching engines and its sensitivity to changes in their parameters
settings. Thus, two large groups of experiments were performed. We tested the
static version of α-stable symmetric distribution — Mαstatic , and adaptive version of α-stable symmetric distribution — Mαadapt . The sets of tests with the
two methods were based on variation of values of two method’s parameters: α
and rSαS . The former parameter varied from 0.5 to 2 with step 0.05 while the
latter – from 0.001 to 0.1 with step 0.001. It gave 3100 conﬁgurations for each
of the approaches. They were tested on the same class of dynamic environments
build by MPB benchmark with 10 moving peaks and with its parameters set to
values as deﬁned above. Obtained values of oﬄine error for the two groups of
experiments are presented in Fig. 2.
In Table 1 for each of the distributions the best three conﬁgurations and the
values of oﬄine error for each of the three are presented. The oﬄine error for
Mcloud is higher than for the two remaining methods. The signiﬁcance levels
obtained with Student’s t tests indicate diﬀerence between Mcloud and distributions in unlimited area i.e. for both tests between Mcloud and the two methods the

Non-uniform Distributions of Quantum Particles

851

1.6

1.9

1.58
1.8

1.56
1.54

1.7
1.52
1.5

1.6

1.48
1.5

Mα static
Mα adapt
50

100

150

200

250

300

350

Mα static
Mα adapt

1.46
400

450

2

4

6

8

10

12

14

16

18

20

Fig. 3. The ﬁrst 10% of values of oﬄine error sorted ascending for the two tested
distributions – left side graph, and a zoom to the best ﬁrst 20 values – right side graph

signiﬁcance level is lower than the commonly accepted level of 0.05 (for Mαstatic
– p = 0.025437, for Mαadapt – p = 0.029881).
Apart from Fig. 2 and Table 1 yet another quantitative comparison was performed based on the same set of results. The analysis is presented in Fig. 3.
To generate Fig. 3 for each of the methods, the values of oﬄine error obtained
for tested conﬁgurations of parameters were sorted ascending. This way we can
observe sensitivity of the methods for changes in the parameters values as well
as the small disturbances in the optimized ﬁtness landscape. We can compare
not only the best results but also the number of other conﬁgurations giving satisfying results i.e. the size of area of useful conﬁgurations of the distributions. A
graph with the ﬁrst 10% of sorted values of oﬄine error is in Fig. 3.
Both curves in Fig. 3 start from the level of oﬄine error which is presented
in Table 1 and which is almost the same for each of them. However they quickly
branch. It is clearly visible that the adaptive method outperforms the static
method. The advantage is that the adaptive method is much more tolerant for
lack of ﬁtting the methods parameters to the properties of the ﬁtness landscape.
In other words an appropriately tuned algorithm is doing very well for each of
the methods however if the algorithm is not perfectly tuned to the problem, the
loss of performance is much less for adaptive methods of distribution.

7

Conclusions

In this paper two methods of evaluation of the quantum particle’s new position
are experimentally veriﬁed: the static and the adaptive one. Both methods employ the α-stable symmetrically distributed random variable. The distribution
is controlled by the parameter α, which is responsible for the density of distribution of new locations around the quantum particle. Obtained results are
satisfactory: they are better than those for uniform distribution of new locations

852

K. Trojanowski

in the limited area around the quantum particle. Besides the results of series of
experiments visualized in Fig. 3 showed that the adaptive method of distribution
is less sensitive to small changes in its parameters than the static one.

References
1. Blackwell, T.: Particle Swarm Optimization in Dynamic Environments. In: Evolutionary Computation in Dynamic and Uncertain Environments. Studies in Computational Intelligence, vol. 51, pp. 29–49. Springer, Heidelberg (2007)
2. Blackwell, T., Branke, J.: Multi-swarm optimization in dynamic environments. In:
Raidl, G.R., Cagnoni, S., Branke, J., Corne, D.W., Drechsler, R., Jin, Y., Johnson,
C.G., Machado, P., Marchiori, E., Rothlauf, F., Smith, G.D., Squillero, G. (eds.)
EvoWorkshops 2004. LNCS, vol. 3005, pp. 489–500. Springer, Heidelberg (2004)
3. Blackwell, T., Branke, J.: Multiswarms, exclusion, and anti-convergence in dynamic
environments. IEEE Trans. Evol. Comput. 10(4), 459–472 (2006)
4. Li, X.: Adaptively choosing neighborhood bests in a particle swarm optimizer for
multimodal function optimization. In: Deb, K., et al. (eds.) GECCO 2004. LNCS,
vol. 3102, pp. 105–116. Springer, Heidelberg (2004)
5. Li, X., Branke, J., Blackwell, T.: Particle swarm with speciation and adaptation in a
dynamic environment. In: GECCO 2006: Proc. Conf. on Genetic and Evolutionary
Computation, pp. 51–58. ACM Press, New York (2006)
6. Parrot, D., Li, X.: Locating and tracking multiple dynamic optima by a particle
swarm model using speciation. IEEE Trans. Evol. Comput. 10(4), 440–458 (2006)
7. Branke, J.: Memory enhanced evolutionary algorithm for changing optimization
problems. In: Proc. of the Congress on Evolutionary Computation, pp. 1875–1882.
IEEE Press, Piscataway (1999)
8. Clerc, M., Kennedy, J.: The particle swarm-explosion, stability, and convergence in
a multi-dimensional complex space. IEEE Trans. Evol. Comput. 6(1), 58–73 (2002)
9. Marsaglia, G.: Choosing a point from the surface of a sphere. Ann. Math.
Statist. 43(2), 645–646 (1972)
10. Chambers, J.M., Mallows, C.L., Stuck, B.W.: A method for simulating stable random variables. J. Amer. Statist. Assoc. 71(354), 340–344 (1976)
11. Weron, R.: On the Chambers-Mallows-Stuck method for simulating skewed stable
random variables. Statist. Probab. Lett. 28, 165–171 (1996)
12. de Castro, L.N., Timmis, J.: An artiﬁcial immune network for multimodal function
optimization. In: Proc. of the IEEE Congress on Evolutionary Computation, vol. 1,
pp. 674–699. IEEE Press, Piscataway (2002)
13. Branke, J.: Evolutionary Optimization in Dynamic Environments. Kluwer Academic Publishers, Dordrecht (2002)
14. Trojanowski, K.: B-cell algorithm as a parallel approach to optimization of moving
peaks benchmark tasks. In: Sixth International Conf. on Computer Information
Systems and Industrial Management Applications (CISIM 2007), IEEE Computer
Society Conf. Publishing Services, pp. 143–148 (2007)

