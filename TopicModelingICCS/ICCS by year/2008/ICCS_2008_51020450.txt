An Optimization-Based Classification Approach with
the Non-additive Measure
Nian Yan1, Zhengxin Chen1, Rong Liu2, and Yong Shi1,2
P

P

P

P

P

P

1

College of Information Science and Technology,
University of Nebraska at Omaha, NE 68182, USA
{nyan, zchen, yshi}@mail.unomaha.edu
2
Chinese Academy of Sciences Research Center on Fictitious Economy and Data Science,
Graduate University of Chinese Academy of Sciences, Beijing 100080, China
{liu.rong@163.com, yshi@guas.ac.cn}

Abstract. Optimization-based classification approaches have well been used for
decision making problems, such as classification in data mining. It considers
that the contributions from all the attributes for the classification model equals
to the joint individual contribution from each attribute. However, the impact
from the interactions among attributes is ignored because of linearly or equally
aggregation of attributes. Thus, we introduce the generalized Choquet integral
with respect to the non-additive measure as the attributes aggregation tool to the
optimization-based approaches in classification problem. Also, the boundary for
classification is optimized in our proposed model compared with previous optimization-based models. The experimental result of two real life data sets
shows the significant improvement of using the non-additive measure in data
mining.
Keywords: Data Mining, Classification, Non-additive Measure, Optimization.

1 Introduction
In data mining, the classification is the task that aims to construct a model that could
most efficiently distinguish different groups in a dataset. The optimization-based
classification approaches formalize the data separation problem as the mathematical
programming problems. Since Fisher’s linear classification model [1], the groups are
described as AX ± b, where A, X, b are representing parameters to be learned, observations, and the constant boundary respectively. There are numerous optimizationbased classification models, from the classical linear classification to the popular
Support Vector Machine (SVM). The common feature of these methods is the use of
optimization techniques. Such technique as linear programming (LP) has already been
widely used in the early studies of classification problem, e.g. Freed and Glover
[2],[3] introduced two classification approaches based on the idea of reducing the
misclassification through minimizing the overlaps or maximizing the distance of two
objectives in a linear system, i.e. maximizing the minimum distances (MMD) of data
from the critical boundary and minimizing the sum of the distances (MSD) of the data
M. Bubak et al. (Eds.): ICCS 2008, Part II, LNCS 5102, pp. 450 – 458, 2008.
© Springer-Verlag Berlin Heidelberg 2008

An Optimization-Based Classification Approach with the Non-additive Measure

451

from the critical boundary. The linear SVM formulizes the bounding planes with soft
margins to separate the data [4],[5].
An important indicator for the quality of the classifier is classification accuracy, as
most existing researches on optimization-based classification approaches concern. Yet
there are a number of other important issues for classification, such as speed, robustness,
scalability, and interpretability [6]. Since interpretability refers to the level of understanding and insight that is provided by the classifier or predictor, it is related to the important
aspect of handling the data. Unfortunately, the classic linear optimization-based classification approaches are with weak interpretability because it only considers the contributions from the attributes towards the classification equal to the sum of contributions from
each individual attribute. Even with non-linear approach, the attributes are equally treated
in the modeling stage. When interactions exist among attributes, the classification model
does not correctly interpret the nature of the data. Thus, in this paper, we introduce an
optimization-based classification approach with respect to the non-additive measure
which is able to identify the interaction among the attributes and improve the classification performance in both accuracy and interpretability.

2 Non-additive Measures and Integrals
The concept of non-additive measures (also referred as fuzzy measure theory) and the
ways of aggregation, nonlinear integrals, were proposed in the 1970s and have been
well developed [7],[8],[9]. Non-additive measures have been successfully used as a
data aggregation tool for many applications such as information fusion, multiple regressions and classifications [9],[10],[11],[12].
2.1 Definitions
0B

Let finite set X = {x1 ,..., x n } denote the attributes in a multidimensional data set. The
non-additive measures are defined as following [8],[13]:
Definition 1. A non-additive measure μ defined on X is a set function μ :

P ( X ) → [0, ∞) satisfying
(1) μ (φ ) = 0
(2) μ ( E ) ≤ μ ( F ) if E ⊆ F
The property (2) is called monotonicity.

P ( X ) denotes the power set of X , which means the set of all subsets of X , μ i
denote the values of set function μ and i = 1,...,2 n − 1 .
Definition 2. A signed non-additive measure μ defined on X is a set function μ :

P ( X ) → (−∞, ∞) satisfying (1) in definition 1.
2.2 Nonlinear Integrals
1B

Nonlinear integrals are regarded as the methods of aggregating μ i in the set function μ .
The studies of non-additive measures and the corresponding nonlinear integrals could be

452

N. Yan et al.

found in the literatures [7],[8],[9],[13] from additive Lebesgue-like integral to the classic
nonlinear integrals, i.e. Sugeno integral and Choquet integral. The Lebesgue-like integral
is exactly the weighted sum of all the attributes and is widely used in forms of the linear
models. However, considering the nonlinear relationships particularly the interactions
among attributes, Sugeno integral and Choquet integral are the necessary data aggregation tools to be applied. Choquet integral is more appropriate to be chosen for data mining applications because it provides very important information in the interaction among
attributes in the database compared with Sugeno integral [9]. Thus, in this paper, we
choose the Choquet integral as the representation of the non-additive measure. Now let
the values of f , { f ( x1 ), f ( x 2 ),..., f ( x n )}, denote the values of each attribute in the data
set; let μ be a non-additive measure. The general definition of Choquet integral, with
function f : X → (−∞,+∞) , based on signed non-additive measure μ is defined by the
formula (1)
0

+∞

−∞

0

(c)∫ f du = ∫ [μ( Fα ) − μ( X )]dα + ∫ μ( Fα )dα

(1)

Where Fα = {x | f ( x ) ≥ α } for α ∈ [0, ∞) and is called α -cut set of f , for

α ∈ [0, ∞) , n is the number of attributes in the database.
When μ is additive, it coincides with the Lebesgue-like integral. Therefore, the
Lebesgue-like integral is a special case of Choquet integral. The linear relationship of
the attributes is still able to be identified when Choquet integral is used in data modeling. The general algorithm used to calculate the Choquet integral is shown below.
General algorithm for calculating Choquet integral
Step 1: Let f = { f ( x1 ), f ( x 2 ),..., f ( x n )} denote the weighted values of each attribU

ute for one given record. Then, we rearrange those values into a non-decreasing order:
f * = { f ( x1* ), f ( x 2* ),..., f ( x n* )},

where f ( x1* ) ≤ f ( x2* ) ≤ ... ≤ f ( x n* ) . The sequence of ( x1* , x2* ,..., x n* ) is one of the possibilities from the permutation of ( x1 , x2 ,..., x n ) .
Step 2: Create the variables of μ , where μ = { μ1 , μ 2 ,..., μ 2 }, and μ1 = μ (φ ) =0.
n

Each of them represents the interaction of several attributes, e.g. μ 2 = μ ({x2* , x3* }) .
Step 3: The value of the Choquet integral is calculated by:
n

(c) ∫ fdμ = ∑ [ f ( xi* ) − f ( xi*−1 )] × μ ({xi* , xi*+1 ,..., x n* }) , where f ( x0* ) = 0 .
i =1

The above 3-step algorithm is easy to understand but hard to implement with computer program. In this paper, we apply and modify the method proposed in [11] to
calculating the Choquet integral. The method is illustrated in formula (2) and (3):
2 n −1

(c)

∫ fdμ = ∑ z μ
j

j =1

j

(2)

An Optimization-Based Classification Approach with the Non-additive Measure

⎧ min( f ( xi )) − max( f ( xi ) )if > 0 or j = 2 n − 1
⎪i: frc( j / 2i )∈[0.5,1) i: frc( j / 2i )∈[ 0,0.5)
where z j = ⎨
⎪
0
otherwise
⎩

453

(3)

frc( j / 2 i ) is the fractional part of j / 2i and the maximum operation on the empty
set is zero. Also the criteria of judging i equals to the following, if we transform j into
binary form j n j n − 1 ... j 1

{i | frc( j / 2 ) ∈ [0.5,1)} = {i | j = 1} and
{i | frc( j / 2 ) ∈ [0,0.5)} = {i | j = 0}
i

i

i

i

Generally, the Choquet integral does not satisfy the additivity, which is defined as:
(c ) ∫ gdμ + (c) ∫ fdμ = (c) ∫ ( g + f )dμ , g and f are functions defined as
f , g : X → (−∞,+∞)
From the above two versions of algorithms to the Choquet integral according to the
general definition in formula (1), we observe that the pre-ordering of the values of
attributes are required in both versions. However, the order of the attributes is sometimes only with only one ordering situation because different attributes may have
different levels of scales. Thus, data normalization is needed to map the data into the
same scale before the calculation of the Choquet integral. Two different types of data
normalization are considered: the first is to perform the traditional data normalization
process such as min-max normalization, z-score normalization and etc. [6]. In this
way, each attribute is mapped into a certain range, e.g. [0, 1] for a typical min-max
normalization. The real world situation is that you never know which data normalization is better for a given dataset unless to test all of them. The reason is that those
different data normalization approaches treat every attributes equally. With this concern, the second type of data normalization is to set weights and bias on each attributes. In this way, we have to extend the definition of the Choquet integral, as follows:
(c)∫ (a + bf )du
where a = {a1 , a 2 ,..., a n } , b = {b1 , b2 ,..., bn } representing bias and weights to the attributes respectively.
However, with this scenario, a and b are not pre-determined and need to be
learned during the training process of data mining. A typical searching algorithm such
as genetic algorithm can be used to obtain those bias and weights according to the
performance of the data mining task [11].

3 Optimization-Based Classification Model with the Non-additive
Measure
In this section, we introduce the classical optimized-based classification models and
extend the model with the signed non-additive measure.

454

N. Yan et al.

3.1 Classical Optimization-Based Classification Models
2B

Freed and Glover [2],[3] introduced two classification approaches based on the idea
of reducing the misclassification through minimizing the overlaps or maximizing the
distance of two objectives in a linear system. The simple MSD classification model
for two group classification is described as follows [2]:
m

Minimize

∑β
i =1

i

Subject to:

y i ( AX − b) ≤ β i
βi ≥ 0
where y i = {1,−1} denotes the two different groups. The model is such a simple and
efficient method that separates the data by searching a linear cut AX and a suitable
boundary b.
Before we introduce the use of signed non-additive measure in classification, the
concept of separating data by hyperplanes is briefly described in section 3.2.
3.2 Concept of Linearly Separable and SVM
3B

“Linearly separable” means that data is able to be perfectly separated by linear classification model. Data sets are sometimes linearly separable, most of the time not. The
theorem of linearly separable is defined as: two groups of objects are linearly separable if and only if the optimal value of LP is zero. The proof of the theorem is from
Bosch and Smith [14]. A linearly inseparable data set can not be perfectly separated
by linear classification model due to the unreachable to the objective of LP.
The linear classification models contain such constrains as “AX” (or “ x T w ” in
SVM). The efforts have been made to achieve the better separation in classification
through constructing different objectives in optimization. For example, the very typical objective for SVM [4],[5] is
l
1
Minimize || w || 2 +C ∑ ξ i
2
i
Subject to:
y i ( K ( x, w) − b) ≥ 1 − ξ i
ξi ≥ 0
where xi denotes the parameters which need to be determined and are even possibly
mapped into a even higher dimensional space by the kernel function
K ( xi , x j ) = φ ( xi )T φ ( x j ) . φ : X → H is a map from low dimension to high dimension,
e.g. φ ( x1 , x 2 ) = ( x12 , 2 x1 x 2 , x 22 ) is a map from two dimensional to three dimensional
space. C is a positive constant and is regarded as the penalty parameter. Given a linear
kernel, the training procedure of SVM is to find linearly separating hyperplanes with

An Optimization-Based Classification Approach with the Non-additive Measure

455

the maximal margin in this higher dimensional space. The linear kernel function K,
e.g. K ( x, w) = x T w , makes the SVM classification coincide with the linear classification models.
3.3 Optimization-Based Non-additive Classification
4B

The optimization-based signed non-additive measure classification approach was
developed by minimizing the SVM like objective and creating “Choquet hyperplanes”
to separate the groups is as follows [15]:
Minimize

m
1
|| μ || 2 +C ∑ β i
2
i =1

Model 1

Subject to:
y i (( c ) ∫ fd μ − b )) ≤ β i

βi ≥ 0
From programming perspective, the boundary value b is hardly to be optimized because of the degeneracy issue of the programming. It is even much more difficult to
solve with the above non-linear programming. The solution is to predetermine the
value of b or implement the learning scheme in the iterations, such as updating b with
the average of the lowest and largest predicted scores [10],[12]. The famous Platt’s
SMO algorithm [16] for SVM classifier utilized the similar idea by choosing the average of the lower and upper multipliers in the dual problem corresponding to the
boundary parameter b in the primal problem. Keerthi et al. [17] proposed the modified
SMO with two boundary parameters to achieve a better and even faster solution.
The boundary b in MSD becomes b ± 1 in standard form of linear SVM, in which it
is called the soft margin. The idea is to construct the separation belt in stead of a single cutting line. This variation of soft margin actually made the achievement that: the
using of formation of b ± 1 in optimization constrains coincidently solved the degeneracy issue in mathematical programming. The value of b could be optimized by a
simple linear programming technique. Thus we simplify the model in [12] into a linear programming solvable problem with optimized b and non-additive measure with
respect to generalized definition of the Choquet integral (formula 2, 3):
m

Minimize C ∑ β i

Model 2

i =1

Subject to:
y i (( c ) ∫ ( a + b f ) d μ − b )) ≤ 1 + β i

βi ≥ 0

In this model, the generalized “Choquet hyperplanes” separates the data with more
flexibility and its geometric meaning could be found in [12]. We utilized the linear
programming technique for solving the non-additive measure μ . The parameters from
a and b are optimized by genetic algorithm as we mentioned earlier.

456

N. Yan et al.

4 Experimental Results
We conduct the proposed approach (Model 2) on two datasets with comparisons to
other popular approaches, i.e. Decision Tree (C4.5) and SVMs.
4.1 US Credit Card Dataset
5B

The credit card dataset is obtained from a major US bank. The data set consists of 65
attributes and 5000 records. There are two groups: current customer (4185 records)
and bankrupt customer (815 records). The task is to predict the risk of bankruptcy of
customers. We regard the current customers as good customers and bankruptcy as
bad. In order to reduce the curse of dimensionality, we use hierarchical Choquet integral [12],[18], that calculates the Choquet integral hierarchically, to Model 2 for decision makings on the new applicants. The way of hierarchical is determined by human
experts. We use randomly sub-sampling 10-fold cross-validation (90% for training
and 10% for testing). The results are summarized in Table 1.
Table 1. Classification accuracy (%) on US credit card dataset

Training
Testing
Training
Testing
Training
Testing
Training
Testing
Training
Testing

Sensitivity
Specificity
(Good)
(Bad)
Linear_SVM
69.8
87.2
69.1
84.6
Polynomial_SVM
69.9
88.1
68.7
85.5
RBF_SVM
71.5
88.4
69.1
85.6
See5.0 (C4.5)
74.8
88.4
69.5
81.8
Model 2
75.0
85.8
70.9
81.6

Accuracy
(Overall)
78.5
69.4
79.0
69.1
80.0
69.5
81.6
69.8
80.5
71.1

There is no such significant difference in terms of classification accuracy among those
different approaches. Our approach performs best on testing dataset while See 5.0 performs best on training. One advantage of our approach is reliability which refers to as the
classification model achieves similar accuracy on both training and testing dataset.
4.2 UCI Liver Disorder Dataset
6B

The Liver Disorder dataset is obtained from UCI Machine Learning Repository
(www.ics.uci.edu/~mlearn/MLRepository.html). This is a two-group classification
problem and data consists of 6 attributes and 345 samples. We perform model 2 with

An Optimization-Based Classification Approach with the Non-additive Measure

457

Table 2. Classification accuracy (%) on liver disorder dataset
Training

Testing

Linear_SVM

Classification Methods

63.9

55.6

Polynomial_SVM

65.1

60.0

RBF_SVM
See5.0 (C4.5)

100.0
88.9

75.4
69.1

Model 2

79.3

75.9

a 10-fold cross-validation (90% for training and 10% for testing) on the dataset. The
comparison results on average classification accuracy are summarized in Table 2.
We observe that the linear SVM performs worst that shows the linearly inseparable
property of the dataset. The RBF SVM model classifies the dataset with 100% for
training and 75.44% for testing. The proposed Model 2, which is based on the signed
non-additive measure, performs 79.34% and 75.86% on training and testing respectively. This result is comparable to the RBF kernel SVM.

5 Conclusions
In this paper, we proposed a new optimization-based approach for classification when
we believe the attributes have interactions. The new approach achieved higher accuracy of classification on two real life datasets compared with traditional approaches.
We introduced the concept of non-additive measures as the data integration approach
to optimization-based classification in data mining. The use of the Choquet integral
with respect to the signed non-additive measure identified more hidden interactions
among attributes and contributes to construct more reliable classification models. The
learned non-additive measure shows some impact from the joint effect from several
certain attributes towards classification and provides a potentially better interpretability of classification model. In the future, more experiments will be conducted for a
better illustration both in classification performance and model representation by
describing the relationship of learned μ and the interactions.
Acknowledgments. This research has been partially supported by a grant from National Natural Science Foundation of China (#70621001, #70531040, #70501030,
#70472074), National Natural Science Foundation of Beijing #9073020, 973 Project
#2004CB720103, National Technology Support Program #2006BAF01A02, Ministry
of Science and Technology, China, and BHP Billion Co., Australia.

References
1. Fisher, R.A.: The Use of Multiple Measurements in Taxonomic Problems. Annals of
Eugenics 7, 179–188 (1936)
2. Freed, N., Glover, F.: Simple but powerful goal programming models for discriminant
problems. European Journal of Operational Research 7, 44–60 (1981)

458

N. Yan et al.

3. Freed, N., Glover, F.: Evaluating alternative linear, programming models to solve the twogroup discriminant problem. Decision Science 17, 151–162 (1986)
4. Vapnik, V.: The Nature of Statistical Learning Theory. Springer, New York (1995)
5. Smola, A.J., Scholkopf, B.: A tutorial on support vector regression. Statistics and Computing 14, 199–222 (2004)
6. Han, J., Kamber, M.: Data Mining Concepts and Techniques, 2nd edn., pp. 286–289. Morgan Kaufmann Publishers, Inc., San Francisco (2002)
7. Choquet, G.: Theory of capacities. Annales de l’Institut Fourier 5, 131–295 (1954)
8. Wang, Z., Klir, G.J.: Fuzzy Measure Theory. Plenum, NewYork (1992)
9. Wang, Z., Leung, K.-S., Klir, G.J.: Applying fuzzy measures and nonlinear integrals in
data mining. Fuzzy Sets and Systems 156(3), 371–380 (2005)
10. Xu, K., Wang, Z., Heng, P., Leung, K.: Classification by Nonlinear Integral Projections.
IEEE Transactions on Fuzzy Systems 11(2), 187–201 (2003)
11. Wang, Z., Guo, H.: A New Genetic Algorithm for Nonlinear Multiregressions Based on
Generalized Choquet Integrals. In: The Proc. of Fuzz/IEEE, pp. 819–821 (2003)
12. Yan, N., Wang, Z., Shi, Y., Chen, Z.: Nonlinear Classification by Linear Programming
with Signed Fuzzy Measures. In: 2006 IEEE International Conference on Fuzzy Systems
(July 2006)
13. Grabisch, M.: A new algorithm for identifying fuzzy measures and its application to pattern recognition. In: Proceedings of 1995 IEEE International Conference on Fuzzy Systems (March 1995)
14. Bosch, R.A., Smith, J.A.: Separating Hyperplanes and the Authorship of the Disputed
Federalist Papers. American Mathematical Monthly 105(7), 601–608 (1998)
15. Yan, N., Wang, Z., Chen, Z.: Classification with Choquet Integral with Respect to Signed
Non-Additive Measure. In: 2007 IEEE International Conference on Data Mining, Omaha,
USA (October 2007)
16. Platt, J.C.: Fast Training of Support Vector Machines using Sequential Minimal Optimization, Microsoft Research (1998), http://research.microsoft.com/jplatt/
smo-book.pdf
17. Keerthi, S., Shevade, S., Bhattacharyya, C., Murthy, K.: Improvements to Platt’s SMO algorithm for SVM classifier design. Tech Report, Dept. of CSA, Banglore, India (1999)
18. Murofushi, T., Sugeno, M., Fujimoto, K.: Separated hierarchical decomposition of the
Choquet integral. International Journal of Uncertainty, Fuzziness and Knowledge-Based
Systems 5(5) (1997)

