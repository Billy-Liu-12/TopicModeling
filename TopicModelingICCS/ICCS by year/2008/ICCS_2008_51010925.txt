Parallel Approximate Finite Element Inverses on
Symmetric Multiprocessor Systems
Konstantinos M. Giannoutakis and George A. Gravvanis
Department of Electrical and Computer Engineering, School of Engineering,
Democritus University of Thrace, 12, Vas. Soﬁas street, GR 671 00 Xanthi, Greece
{kgiannou, ggravvan}@ee.duth.gr

Abstract. A new parallel normalized optimized approximate inverse algorithm for computing explicitly approximate inverses, is introduced for
symmetric multiprocessor (SMP) systems. The parallelization of the approximate inverse has been implemented by an antidiagonal motion, in
order to overcome the data dependencies. The parallel normalized explicit approximate inverses are used in conjuction with parallel normalized explicit preconditioned conjugate gradient schemes, for the eﬃcient
solution of ﬁnite element sparse linear systems. The parallel design and
implementation issues of the new algorithms are discussed and the parallel performance is presented, using OpenMP. The speedups tend to
the upper theoretical bounds for all cases making approximate inverse
preconditioning suitable for SMP systems.

1

Introduction

Sparse matrix computations, which have inherent parallelism, are of central importance in computational science and engineering computations and are the
most time-consuming part. Hence research eﬀorts were focused on the production of eﬃcient parallel computational methods and related software suitable for
multiprocessor systems, [1,2,3,10].
Until recently, direct methods have been eﬀectively used, but the increase of
size, even with the use of modern computer systems, has become a barrier to such
methods, [2,3,10]. Additionally, the solution of sparse linear systems, because of
its applicability to real-life problems, is obtained by iterative methods, which are
in competitive demand after the emergence of Krylov subspace methods, [4,8].
An important achievement over the last decades is the appearance and use
of Explicit Preconditioned Methods, [4], for solving sparse linear systems, and
the preconditioned form of a linear system Au = s is M Au = M s, where M
is preconditioner, [2,4,8,10]. The preconditioner M has therefore to satisfy the
following conditions: (i) M A should have a “clustered”spectrum, (ii) M can be
eﬃciently computed in parallel and (iii) ﬁnally “M × vector”should be fast to
compute in parallel, [2,4,8,9,10].
Hence, the derivation of parallel methods was the main objective for which
several families of parallel inverses are proposed. The main motive for the derivation of the parallel explicit approximate inverse matrix algorithms is that they
M. Bubak et al. (Eds.): ICCS 2008, Part I, LNCS 5101, pp. 925–934, 2008.
c Springer-Verlag Berlin Heidelberg 2008

926

K.M. Giannoutakis and G.A. Gravvanis

result in parallel iterative methods in conjunction with parallel preconditioned
conjugate gradient - type schemes respectively, for solving ﬁnite element linear
systems on SMP systems. The important feature of the proposed parallel approximate inverse preconditioning is that the approximate inverse is computed
explicitly and in parallel, eliminating the forward-backward substitution, which
does not parallelize easily, [10].
For the implementation of the parallel programs, the OpenMP application
programming interface has been used. OpenMP has emerged as a shared-memory
programming standard and it consists of compiler directives and functions for
supporting both data and functional parallelism. The parallel for pragma with
static scheduling has been used for the parallelization of loops on both the construction of the approximate inverse and the preconditioned conjugate gradient
scheme.

2

Parallel Explicit Approximate Inverses

Let us consider the arrow - type linear system, i.e.,
Au = s,
where A is a sparse arrow - type (n × n) matrix of the following form:
✛
✲✛
✲
m
⎡
⎤
a1 b 1
❜
❜
❜
⎢ a2 b 2
⎥
❜
❜
⎢
⎥
❜0 ⎥
❜
⎢
❜
❜
❜
⎢
❜ ❜
❜⎥
❜
⎢
⎥
❜ ❜
❜
⎢
⎥
❜ ❜
❜
⎢
⎥
❜
❜
0
❜
⎢
⎥)
❜❜ ❜
❜
V
≡
(v
❜
k,η
⎢
⎥
am−1 bm−1
❜
⎢
⎥
❜
⎢
⎥
am b m
A≡⎢
❜ ⎥
❜⎥
⎢
❩ ❩
⎢
⎥
⎢
⎥
❩ ❩
symmetric
⎢
⎥
❩ ❩
⎢
⎥
❩ ❩
⎢
⎥
❩ ❩
⎢
⎥
⎢
⎥
❩ ❩
⎣
❩ bn−1 ⎦
❩
an

(1)

(2)

Let us assume the normalized ﬁnite element approximate factorization of the
coeﬃcient matrix A, such that:
A ≈ Dr Trt Tr Dr ,

r ∈ [1, . . . , m − 1),

(3)

where r is the “ﬁll-in” parameter, i.e. the number of outermost oﬀ-diagonal
entries retained in semi-bandwidth m, Dr is a diagonal matrix and Tr is a sparse
upper (with unit diagonal elements) triangular matrix of the same proﬁle as the
coeﬃcient matrix A, [7].

Parallel Approximate Finite Element Inverses on SMP Systems

927

The elements of the Dr , Tr decomposition factors were computed by the
FEANOF algorithm, cf. [12]. The memory requirements of the FEANOF algorithm are ≈ (r + 2 + 2)n words, while the computational work for m << n
is ≈ 1/2(r + )(r + + 3)n mults + n square roots, cf. [7].
Let Mrδl = (μi,j ), i ∈ [1, n] j ∈ [max(1, i − δl + 1), min(n, i + δl − 1)], be the
normalized approximate inverse of the coeﬃcient matrix A, i.e.
Mrδl = Dr−1 Trt Tr

−1

Dr−1 = Dr−1 Mrδl Dr−1 , with Mrδl = Trt Tr

−1

.

(4)

The elements of the approximate inverse were determined by retaining a certain number of elements of the inverse, i.e. only δl elements in the lower part
and δl − 1 elements in the upper part of the inverse (by applying the so-called
“position-principle”), next to the main diagonal, by the Normalized Optimized
Banded Approximate Inverse Finite Element Matrix -2D algorithmic procedure (henceforth called the NOROBAIFEM-2D algorithm, without inverting
the decomposition factor Tr , [7].
The challenge of computing parallel approximate inverses is to overcome its
data dependencies, which create a critical path and an order of computations,
hence any parallel approximate inverse matrix algorithm must abide by those
dependencies in order to avoid any data loss.
✛
✲
⎤
⎡ (15) δl (14) ✧ (13) ❜
✧
✧
μ1,1
μ
μ1,3 ✧
✧ ❜
✧
1,2
✧
✧
⎥
⎢✧✧
✧
✧
✧(12)
(13)
(11)❜
⎥
⎢ μ(14) ✧✧
μ2,2 ✧ μ2,3✧✧μ2,4 ✧ ❜
⎥
⎢ 2,1
✧
✧
✧
⎥
⎢ ✧
✧
✧
✧
⎥
⎢✧ (13) ✧✧
(12) ✧(11)
(10) ✧✧
(9) ❜
⎥
⎢ μ3,1 ✧ μ3,2 ✧ μ3,3 ✧μ3,4✧ μ3,5 ✧
✧❜
✧ ✧
⎥
⎢❜ ✧
✧
✧
❜
✧
⎥
⎢ ❜
✧
✧
✧(11) ✧ (10) ✧ (9)
(8)
(7)
✧
⎥
⎢
✧
✧
❜
μ
μ
μ
μ
μ
✧
✧
✧
4,2
4,3
4,4
4,5
4,6
✧
✧
⎥
⎢
❜ ✧
✧ ✧✧
✧
❜
Mrδl = ⎢
✧
✧
⎥ . (5)
✧
❜ ✧✧(9) ✧✧
(8) ✧ (7)
(6) ✧ (5)
✧
⎢
✧❜ ⎥
μ5,3
μ5,4✧ μ5,5 ✧ μ5,6✧ μ5,7 ✧
⎢
❜
✧
❜⎥
✧
✧ ✧
⎥
⎢
✧
❜✧ ✧✧
✧(6) ✧✧
✧
⎥
⎢
(7)
(5)
(4)
(3)
✧
✧
✧
⎢
μ
μ
μ
μ
μ
❜✧ 6,4
6,5✧
6,6✧
6,7 ✧ 6,8 ⎥
✧
⎥
⎢
✧
✧
✧
✧
❜✧
⎢
✧(5) ✧✧
✧
(4)
(3)
(2) ⎥
✧
⎥
⎢
✧
✧
❜
μ
μ
μ
μ
7,5
7,6 ✧ 7,7✧
7,8 ✧
⎦
⎣
✧
❜ ✧✧
✧
✧
✧
(3)
(2)
(1)
✧
✧
❜✧μ
μ
μ
❜ 8,6✧ 8,7 ✧ 8,8
For the parallelization of the NOROBAIFEM-2D algorithm, an antidiagonal motion (wave-like pattern), starting from the element μ8,8 down to μ1,1 ,
has been used, because of the dependency of the elements of the inverse during
its construction. More speciﬁcally, any element within the banded approximate
inverse requires its corresponding right or lower element to be computed ﬁrst.
This sequence of computations, without any loss of generality and for simplicity
reasons, is shown for the normalized banded approximate inverse in equation (5)
(with n = 8 and δl = 3). The values of the parentheses at the superscript of each
(k)
element (e.g. μi,j ), indicate that the element μi,j was computed at the (k)-th
sequential step of the algorithm (k-th antidiagonal), while the elements with the
same superscript (i.e. (k)) were computed concurrently. It should be noted that

928

K.M. Giannoutakis and G.A. Gravvanis

due to the data dependencies, for δl = 1, 2 the parallel algorithm will execute
sequentially.
Let us consider that the command forall denotes the parallel for instruction
(forks/joins threads), for executing parallel loops. Then, the algorithm for the
implementation of the Parallel ANti Diagonal NOROBAIFEM-2D algorithm
(henceforth called the PAND-NOROBAIFEM-2D algorithm), on symmetric
multiprocessor systems, can be described as follows:
for k = 1 to δl
forall l = 1 to k
call inverse(n − l + 1,n − k + l)
m=2
for k = (δl + 1) to n
forall l = m to (k − m + 1)
call inverse(n − l + 1,n − k + l)
if (k − δl) mod 2 = 0 then
m=m+1
m=m−1
for k = (n − 1) downto (δl + 1)
forall l = m to k − m + 1
call inverse(l,k − l + 1)
if (k − δl) mod 2 = 1 then
m=m−1
for k = δl downto 1
forall l = 1 to k
call inverse(l,k − l + 1)
where the function inverse(i,j) computes the element μi,j of the normalized
optimized approximate inverse, and can be described as follows, [7]:
function(inverse)
Let r = r + , m = m + , mr = m − r, nmr = n − mr, nm = n − m .
if i >= j then
if j > nmr then
if i = j then
if i = n then
(6)
μ1,1 = 1
else
(7)
μn−i+1,1 = 1 − gj · μn−j,δl+1
else
(8)
μn−i+1,i−j+1 = −gj · μn−i+1,i−j
else
if j ≥ r and j ≤ nmr then
if i = j then
nmr−j

μn−i+1,1 = 1 − gj · μn−j,δl+1 −

hr

−1−k,j+1−r+k

k=0

call mw(n, δl, i, mr + j + k, x, y)

· μx,y

(9)

Parallel Approximate Finite Element Inverses on SMP Systems

929

else
nmr−j

μn−i+1,i−j+1 = −gj · μn−i+1,i−j −

hr

−1−k,j+1−r+k

· μx,y

(10)

k=0

call mw(n, δl, i, mr + j + k, x, y)
else
if j > nm + 1 and j ≤ r − 1 then
if i = j then
μn−i+1,1 = 1 − gj · μn−j,δl+1 −

hj,k · μx1 ,y1
k=j+1−r
k>0

nm

−
call mw(n, δl, i, m + k − 1, x1 , y1 )
else

hj−1−λ,

+1+λ

· μx2 ,y2

(11)

λ=0

call mw(n, δl, i, m + λ, x2 , y2 )

μn−i+1,i−j+1 = −gj · μn−i+1,i−j −

hj,k · μx1 ,y1
k=j+1−r
k>0

nm

−
call mw(n, δl, i, m + k − 1, x1 , y1 )
else
if j ≤ nm + 1 then
if i = j then
if i = 1 then

hj−1−λ,

+1+λ

· μx2 ,y2

(12)

λ=0

call mw(n, δl, i, m + λ, x2 , y2 )

μn,1 = 1 − g1 · μn−1,δl+1 −

h1,k · μx,y

(13)

k=1

call mw(n, δl, 1, m + k − 1, x, y)
else
j−1

μn−i+1,1 = 1 − gj · μn−j,δl+1 −

hj,k · μx1 ,y1 −
k=j+1−r
k>0

call mw(n, δl, i, m + k − , x1 , y1 )
else

hj−λ,

+λ

· μx2 ,y2

(14)

λ=1

call mw(n, δl, i, m + λ − 1, x2 , y2 )
j−1

μn−i+1,1 = −gj · μn−j,δl+1 −

hj,k · μx1 ,y1 −
k=j+1−r
k>0

call mw(n, δi, i, m + k − , x1 , y1 )
if i <> j then
μn−i+1,δl+i−j = μn−i+1,i−j+1

hj−λ,

+λ

· μx2 ,y2

(15)

λ=1

call mw(n, δl, i, m + λ − 1, x2 , y2 )
(16)

The procedure mw(n, δl, s, q, x, y), [5], reduces the memory requirements of
the approximate inverse to only n × (2δl − 1)-vector spaces. The computational
process is logically divided into 2n − 1 sequential steps representing the 2n −
1 antidiagonals, while synchronization between processes is needed after the

930

K.M. Giannoutakis and G.A. Gravvanis

computation of each antidiagonal, to ensure that the elements of the matrix are
correctly computed.

3

Parallel Normalized Preconditioned Conjugate
Gradient method

In this section we present a class of parallel Normalized Explicit Preconditioned
Conjugate Gradient (NEPCG) method, based on the derived parallel optimized approximate inverse, designed for symmetric multiprocessor systems.The
NEPCG method for solving linear systems has been presented in [7]. The computational complexity of the NEPCG method is O [(2δl + 2 + 11) nmults + 3n
adds] ν operations,where ν is the number of iterations required for the convergence to a certain level of accuracy, [7].
The Parallel Normalized Explicit Preconditioned Conjugate Gradient (PN
EPCG) algorithm for solving linear systems can then be described as follows:
forall j = 1 to n
(r0 )j = sj − A (u0 )j
if δl = 1 then
forall j = 1 to n
(r0∗ )j = (r0 )j / d2
else
forall j = 1 to n
(r0∗ )j =

(17)

(18)

j

j

μn+1−i,i+1−k
k=max(1,j−δl+1)
min(n,j+δl−1)
+
k=j+1

(r0 )k /dk

μn+1−k,δl+k−j (r0 )k /dk

/ (d)j

(19)

forall j = 1 to n
(20)
(σ0 )j = (r0∗ )j
forall j = 1 to n (reduction+p0 )
p0 = (r0 )j ∗ (r0∗ )j
(21)
Then, for i = 0, 1, . . ., (until convergence) compute in parallel the vectors ui+1 ,
ri+1 , σi+1 and the scalar quantities αi , βi+1 as follows:
forall j = 1 to n
(qi )j = A (σi )j
(22)
forall j = 1 to n (reduction +ti )
(23)
ti = (σi )j ∗ (qi )j
(24)
αi = pi /ti
forall j = 1 to n
(ui+1 )j = (ui )j + αi (σi )j
(25)
(ri+1 )j = (ri )j − αi (qi )j
(26)
if δl = 1 then
forall j = 1 to n
∗
= (ri+1 )j / d2 j
(27)
ri+1
j

Parallel Approximate Finite Element Inverses on SMP Systems

931

else
forall j = 1 to n
∗
ri+1

j
j

=

μn+1−i,i+1−k
k=max(1,j−δl+1)
min(n,j+δl−1)
+
k=j+1

(ri+1 )k /dk

μn+1−k,δl+k−j (ri+1 )k /dk

/ (d)j (28)

forall j = 1 to n (reduction+pi+1 )
∗
pi+1 = (ri+1 )j ∗ ri+1
j
βi+1 = pi+1 /pi
forall j = 1 to n
∗
+ βi+1 (σi )j
(σi+1 )j = ri+1
j

(29)
(30)
(31)

It should be noted that the parallelization of the coeﬃcient matrix A×vector
operation has been implemented by taking advantage of the sparsity of the coeﬃcient matrix A.

4

Numerical Results

In this section we examine the applicability and eﬀectiveness of the proposed
parallel schemes for solving sparse ﬁnite element linear systems.
Let us now consider a 2D-boundary value problem:
uxx + uyy + u = F,

(x, y) ∈ R,

with

u (x, y) = 0,

(x, y) ∈ ∂R,

(32)

where R is the unit square and ∂R denotes the boundary of R.
The domain is covered by a non-overlapping triangular network resulting in a
hexagonal mesh. The right hand side vector of the system (1) was computed as
the product of the matrix A by the solution vector, with its components equal
to unity. The “ﬁll-in”parameter was set to r = 2 and the width parameter was
set to = 3. The iterative process was terminated when ri ∞ < 10−5 . It should
be noted that further details about the convergence behavior and the impact of
the “retention”parameter on the solution can be found in [6].
The numerical results presented in this section were obtained on an SMP
machine consisting of 16 2.2 GHz Dual Core AMD Opteron processors, with 32
GB RAM running Debian GNU/Linux (National University Ireland Galway).
For the parallel implementation of the algorithms presented, the Intel C Compiler
v9.0 with OpenMP directives has been utilized with no optimization enabled at
the compilation level. It should be noted that due to administrative policies, we
were not able to explore the full processor resources (i.e. more than 8 threads).
In our implementation, the parallel for pragma has been used in order to generate code that forks/joins threads, in all cases. Additionally, static scheduling
has been used (schedule(static)), whereas the use of dynamic scheduling has not
produced improved results.
The speedups and eﬃciencies of the PAND-NOROBAIFEM-2D algorithm
for several values of the “retention”parameter δl with n = 10000 and m = 101,

932

K.M. Giannoutakis and G.A. Gravvanis

Table 1. Speedups for the PAND-NOROBAIFEM-2D algorithm for several values
of δl
Speedups for the PAND-NOROBAIFEM-2D algorithm
Retention parameter 2 processors 4 processors 8 processors
δl = m
1.8966
3.8458
6.8653
δl = 2m
1.9600
3.8505
7.4011
δl = 4m
1.9741
3.9260
7.5768
δl = 6m
1.9986
3.9501
7.8033

Table 2. Eﬃciencies for the PAND-NOROBAIFEM-2D algorithm for several values of δl
Eﬃciencies for the PAND-NOROBAIFEM-2D algorithm
Retention parameter 2 processors 4 processors 8 processors
δl = m
0.9483
0.9615
0.8582
δl = 2m
0.9800
0.9626
0.9251
δl = 4m
0.9870
0.9815
0.9471
δl = 6m
0.9993
0.9875
0.9754

are given in Table 1 and 2. In Fig. 1 the parallel speedups for several values of
the “retention”parameter δl are presented for the PAND-NOROBAIFEM2D method, for n = 10000 and m = 101.
The speedups and eﬃciencies of the PNEPCG algorithm for several values of the “retention”parameter δl with n = 10000 and m = 101, are given in
Table 3 and 4. In Fig. 2 the parallel speedups for several values of the “retention”parameter δl are presented for the PNEPCG method, for n = 10000 and
m = 101.
Table 3. Speedups for the PNEPCG algorithm for several values of δl
Speedups for the PNEPCG method
Retention parameter 2 processors 4 processors 8 processors
δl = 1
1.1909
1.5365
1.6097
δl = 2
1.5261
2.2497
2.7299
δl = m
1.8070
3.4351
6.3522
δl = 2m
1.8576
3.4824
6.3636
δl = 4m
1.9103
3.5453
6.4043
δl = 6m
1.9735
3.5951
6.6106

It should be mentioned that for large values of the “retention”parameter, i.e.
multiples of the semi-bandwidth m, the speedups and the eﬃciency tend to the
upper theoretical bound, for both the parallel construction of the approximate
inverse and the parallel normalized preconditioned conjugate gradient method,
since the coarse granularity amortizes the parallelization overheads. For small

Parallel Approximate Finite Element Inverses on SMP Systems

Fig. 1. Speedups versus the
NOROBAIFEM-2D algorithm

“retention”parameter

δl

for

the

933

PAND-

Table 4. Eﬃciencies for the PNEPCG algorithm for several values of δl
Eﬃciencies for the PNEPCG algorithm
Retention parameter 2 processors 4 processors 8 processors
δl = 1
0.5955
0.3841
0.2012
δl = 2
0.7631
0.5624
0.3412
δl = m
0.9035
0.8588
0.7940
δl = 2m
0.9288
0.8706
0.7954
δl = 4m
0.9551
0.8863
0.8005
δl = 6m
0.9867
0.8988
0.8263

Fig. 2. Speedups versus the “retention”parameter δl for the PNEPCG algorithm

values of the “retention”parameter, i.e. δl = 1, 2, the ﬁne granularity is responsible for the low parallel performance, since the parallel operations reduces to
simple ones, like inner products, and the utilization of the hardware platform is
decreasing.

934

5

K.M. Giannoutakis and G.A. Gravvanis

Conclusion

The design of parallel explicit approximate inverses results in eﬃcient parallel preconditioned conjugate gradient method for solving ﬁnite element linear
systems on multiprocessor systems.
Finally, further parallel algorithmic techniques will be investigated in order to
improve the parallel performance of the normalized explicit approximate inverse
preconditioning on symmetric multiprocessor systems, particularly by increasing
the computational work output per processor and eliminating process synchronization and any associated latencies.
Acknowledgments. The author would like to thank indeed Dr. John Morrison
of the Department of Computer Science, University College of Cork for the
provision of computational facilities and support through the WebCom-G project
funded by Science Foundation Ireland.

References
1. Akl, S.G.: Parallel Computation: Models and Methods. Prentice-Hall, Englewood
Cliﬀs (1997)
2. Dongarra, J.J., Duﬀ, I., Sorensen, D., van der Vorst, H.A.: Numerical Linear Algebra for High-Performance Computers. SIAM, Philadelphia (1998)
3. Duﬀ, I.: The impact of high performance computing in the solution of linear systems: trends and problems. J. Comp. Applied Math. 123, 515–530 (2000)
4. Gravvanis, G.A.: Explicit Approximate Inverse Preconditioning Techniques.
Archives of Computational Methods in Engineering 9(4), 371–402 (2002)
5. Gravvanis, G.A.: Parallel matrix techniques. In: Papailiou, K., Tsahalis, D., Periaux, J., Hirsch, C., Pandolﬁ, M. (eds.) Computational Fluid Dynamics I, pp.
472–477. Wiley, Chichester (1998)
6. Gravvanis, G.A., Giannoutakis, K.M.: Normalized Explicit Finite Element Approximate Inverses. I. J. Diﬀerential Equations and Applications 6(3), 253–267 (2003)
7. Gravvanis, G.A., Giannoutakis, K.M.: Normalized ﬁnite element approximate inverse preconditioning for solving non-linear boundary value problems. In: Bathe,
K.J. (ed.) Computational Fluid and Solid Mechanics 2003. Proceedings of the Second MIT Conference on Computational Fluid and Solid Mechanics, vol. 2, pp.
1958–1962. Elsevier, Amsterdam (2003)
8. Grote, M.J., Huckle, T.: Parallel preconditioning with sparse approximate inverses.
SIAM J. Sci. Comput. 18, 838–853 (1977)
9. Huckle, T.: Approximate sparsity patterns for the inverse of a matrix and preconditioning. Applied Numerical Mathematics 30, 291–303 (1999)
10. Saad, Y., van der Vorst, H.A.: Iterative solution of linear systems in the 20th
century. J. Comp. Applied Math. 123, 1–33 (2000)

