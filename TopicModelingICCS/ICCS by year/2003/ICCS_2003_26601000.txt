Object Oriented Parallel Programming Model on a
Network of Workstations
B. Suresh and R. Nadarajan
Department of Mathematics and Computer Applications, PSG College of Technology,
Peelamedu, Coimbatore-641 004, TamilNadu, India
{suresh_psgtech, nadarajan_psg}@yahoo.co.in

Abstract. Parallel computing in network of workstations is receiving lot of
attentions from the research community. However, still it is faced with many
practical difficulties. One of the major problems is the lack of high level
programming model. Many programming languages provide an efficient
system, but employ a relatively low level programming model and rather too
complex to be used in network environment. The object oriented system
paradigm in software engineering has wide acceptance and provides support
for the construction of modular and reusable program components and it is
attractive for the design of large and complex serial programs. To address the
previous said problem, in this paper we present a flexible, easy to use, and an
effective object oriented parallel programming model by amalgamating the
object orientation paradigm and parallelism. It is developed on top of PVM.
This model exhibits a very convenient and natural programming style and
provides functionality similar to C++.
Keywords. Object oriented programming, Parallel programming, Object oriented parallel programming.

1 Introduction
The object oriented paradigm provides support for the construction of modular and
easily reusable program components and has proven to be useful for the design of
large and complex system. In object oriented programming a system is described as a
collection of objects. An object is an integrated unit of data and procedures, called
methods acting on such data. The data in an object are stored in variables whose
contents can be changed by assignment statements, present in the methods. It means
that the variables in an object are not accessible from outside. This mechanism is a
clear protocol for the exchange of information between the different objects of a
programming application.
The parallel programming adopts a style, which facilitates the decomposition of a
problem in to several independent tasks or processes [15]. The above said aspects of

P.M.A. Sloot et al. (Eds.): ICCS 2003, LNCS 2660, pp. 1000−1010, 2003.
 Springer-Verlag Berlin Heidelberg 2003

Object Oriented Parallel Programming Model on a Network of Workstations 1001

object oriented paradigm are potentially suitable for the field of parallel
programming by treating an object as an independent task. An object and its private
data are good abstraction of a process and process’s local data. Public methods
activated by sending messages are similar to message passing mechanism in parallel
computing [13]. That is why, an object naturally becomes an autonomous
programming unit or unit of execution in parallel programming environment.
Many systems have been developed by researchers on object oriented parallel
programming by extending C++ (See Section 4: Related works) and defined many
new notations to include specific constructs to express the parallelism. These new
notations are burden to the programmers in perspective of learning its functionalities
and these systems normally fail to maintain conventional programming style.
In this paper, a new object oriented parallel programming model OP++ has been
proposed. We focus more on this model to give more casualty and natural
programming style as like C++ when compared to any existing system for parallel
programming in network of workstations.
This paper is organized as follows: In Section 2, description about our
programming model is given. In Section 3, the working principle of our model is
discussed. In Section 4, some existing systems that are closely related to our model
have been analyzed and in Section 5, an example application program for our model
is presented.

2 Programming Model
OP++ is an object oriented parallel programming language based on C++[17]. The
basic idea of OP++ is to view an object as an independent or individual process and
its member functions are decomposed computational part of the problem to provide
an efficient parallel execution.
2.1 The Process Class
The responsibilities assigned to the programmers are to write C++ class(es), which
are decomposed part(s) to warrant parallel execution. This is accomplished by
deriving the written C++ class(es) from the built-in class called “Process”. This
mechanism provides “is-a” relationship between the class Process (base class) and
the user written class (derived class). The class Process is a general class whose
behavior is inherited by the derived class. The instance of such a class is called
“process object”. The process object will become an autonomous execution unit in
parallel environment. The process object can be used like any other C++ class
instance.
When an object is instantiated to the class derived from the class Process, a new
process is created and placed at any processing node, which has been added to the
PVM [7, 5] network to implement the object’s autonomous activities. This scenario
will be held when the execution point reaches the constructor of the class Process at
the time of process object creation. The spawned process’s ID is maintained as a
member data in the class Process for further communication to the corresponding

1002 B. Suresh and R. Nadarajan

process while any member function is invoked at the derived class of the class
Process. The spawned process consists of actual definition of the class(es) that is
derived from the class Process. This is discussed in detail in Section 3. So, for every
process object there exists a worker process in the network. Since there is no public
methods in class Process, deriving the class from it may be public, private or
protected.
2.2 Asynchronous Method Calling
Parallelism is achieved by calling the member functions of process objects in an
asynchronous manner. The statement that invoked a member function can continue
its execution in parallel with the invoked member function. (i.e. the caller does not
wait for the method completion and continues its execution to the consecutive
statements). When the result is needed from the invoked member function, the
execution has to wait until the required results are available.
The member function can call asynchronously by calling it in par {} block. par
{} is a key word that we extended from compositional C++ (CC++) [2]. In our
system par {} block implies that the method invocation are executed in parallel.
1. # include <parallel.h>
2. class foo : Process
3.
{ public:
4.
int function1 () {...}
5.
int function2 () {...}
6.
};
7. void main () {
8.
foo ob1,ob2;// 2 processes are created
9.
int x,y;
10. par {
11.
x = ob1.function1 ();
12.
y = ob2.function2 ();
13.
}
14.
}
Fig 1.a. Example program

ob1

ob2

Fig 1.b. Degree of parallelism
in par { } block

The given program (Fig 1.a) works as follows: when object ob1 and ob2 are
instantiated, two processes will be created on processing node in network. When the
line 11 calls the member function function1( ) that belongs to the process object ob1,
it never waits for the return value. It continues the execution and calls another
member function of another process object ob2 in line 12. The value of these two
member functions is received at the end of the par {} block. The receiving point or
synchronous point is found out depending upon the data dependency that exists in
the given code inside the par {} block. This is discussed in detail in Section 2.3 with
an example.

Object Oriented Parallel Programming Model on a Network of Workstations 1003

The member function belongs to different process object can be invoked and
executed on different nodes in parallel. As shown in Fig 1.b, ob1.function1 ( ) and
ob2.function2 ( ) are executed in parallel. The only thing is no more than one
member function of the given process objects will be executed at the same time.
When more than one member function is called on same process object in par {}
block it is executed in a sequential manner. The statements that are not available in
par {} block will be executed sequentially.
2.3 Synchronization Point
Since the method in side the par {} block are invoked asynchronously,
synchronization between the statements is required. The synchronization mechanism
defined in our model can be described as follows. Methods invoked prior to a
synchronization point can be executed in parallel with the calling methods, but only
after the invoked methods have returned can the statements proceed from
synchronization point.
Consider the process objects ob1, ob2 and ob3. (i.e., the instance of the class
derived from the class Process). The dependency between the statements in par {}
block is observed at compile time and it produces suitable code to maximize the
degree of parallelism. It is assumed that the methods used in the given example
return some result, so that the dependency can be explained in more details.
par {
1. __________________________
1.
x = ob1.fun (10,10);
ob1
ob3
2. __________________________
2.
a = b + c;
ob2
3. __________________________
3.
y = ob2.fun (10,a);
4.

z = ob3.fun (15,10);

4. __________________________

5.

w = ob1.fun (1,2);

6.

c=y + z;

5. __________________________
ob1
6. __________________________

}
End of par {} block
Fig 2.a Example code

Fig 2.b Degree of parallelism

In Fig 2.a, the methods in line 1 and line 4 are invoked because both methods
were called on different process objects and there doesn’t exist any dependency
between these statements. The method in line 3 invoked after the execution of line 2
because the method uses the variable “a” that was defined by line 2.
The value is received from the method of ob1 before the line 5 since no more than
one member function will run in parallel. Now the line 5 is executed. The value is
received from the method of ob2 and ob3 before the line 6 and the value is received
from ob1 at end of the par {} block (Fig 2.b). The statements that are not associated
with method invocation will not run in parallel with each other (i.e line 2 & line 6).
And also if there is any statement like ,

1004 B. Suresh and R. Nadarajan

x = ob3.fun1 (ob1.fun (10, 10), ob2.fun (15, 25) )
in par {} block, the method of different process objects ob1 and ob2 in object ob3
are invoked and executed in parallel. After the execution of the method of ob1 and
ob2 the result is forwarded to the method of ob3.
2.4 Lifetime of a Process in a Network
The lifetime of a process that spawned at the time of creation of a process object is
based on lifetime of the process object. When an object is created the process is
created and placed in the processing node of the network. When the scope of the
process object goes out and its destructor is reached, it kills the corresponding
process from the network. The users need not to bother about maintaining the
running processes.
2.5 Nested Parallelism
The fundamental construct of our model is based on master/worker process model.
The process that runs main () function is said to be a master process. Other processes
spawned at the time of process object creation are said to be worker processes. The
worker process, which itself is capable of creating some other worker process(es) and
so on. This kind of parallel execution is referred as a nested parallelism [11]. The
nested parallelism is accomplished by containment [17]. The containment to achieve
the nested parallelism is the inclusion of instance of a class derived from the class
Process as a member data in its private area of another class derived from the class
Process. The given example shows the nested parallelism by means of containment.
class foo : Process
{ public:
int fun1 ( )
{ ....
Statements
....
}
int fun2 ( )
{ ....
Statements
....
}
}; // End of the class

class bar : Process
{
foo f1,f2; //creates 2 processes
int x,y;
public:
int function ()
{
par{
x = f1.fun1 ();
y = f2.fun2 ();
}
}
}; // End of the class

The bar class consists of two process objects that are instance of foo class. The
method in a bar class calls two methods of process objects f1 & f2 in par {} block to
run it in parallel. When an object (instance of bar class) invokes this method in some
other par block, this gives nested parallelism.

Object Oriented Parallel Programming Model on a Network of Workstations 1005

void main ( )
{
bar ob1,ob2; // creates 2 processes
foo ob3;

ob1

ob3

ob2

// creates 1 process

int x,y,z;
par {

f1

f2

f1

f2

x = ob1.function ();
y = ob3.fun1 ();
z = ob2.function ();

ob1

ob2

}
} // End of the main

par { } block in main () function

2.6 Parallel Arithmetic Expression
Many scientific problems contain many arithmetic expressions, which take much
time to evaluate. Normally most of the arithmetic expressions may have some sub
expression which doesn’t affect the result of the given expression if it is evaluated in
parallel. That is the result of some sub expression is not required at time of some
other expression evaluation. Such type of sub expressions can be observed and safely
executed in parallel. Our system provides this facility to evaluate the expression in
parallel, when the operands of the expression is a process object and the operator
followed by that process object in the expression are overloaded as a public method.
To illustrate the parallel expression consider the process objects a, b, c and d. The
operators succeeded to it are overloaded operators and the operands followed by the
operator are parameters to the operator function.
par { x = ( ( a + 2 ) * ( c / f ) ) + (d * 10 ) + (b / f) }
Step 1 : (a + 2) , (c / f), (d * 10), (b + f) are computed simultaneously.
Step 2 : Result of (a + 2) * Result of (c / f) is computed
Step 3 : Result of step 2 + Result of (d * 10) is computed
Step 4 : Result of step 3 + Result of (b / f) is computed
In the above expression when (a + 2) is evaluated the result of other sub expression
like (c / f), (d * 10) and (b / f) are not required. So these sub expressions are
evaluated in parallel. That is the methods +, /, *, / of the process objects a, b, c and d
respectively are invoked asynchronously.
The synchronization points of the invoked methods (operator function) in an
arithmetic expression strictly depend on the precedence of the operator at the

1006 B. Suresh and R. Nadarajan

evaluation. When the above expression is evaluated in serial, it requires seven steps,
but in parallel, it is evaluated in four steps. But this gives good computational
performance when large amount of computation (coarse grained) is performed in
operator overloaded method.
2.7 Inheritance and Virtual Functions
The concepts of inheritance of object orientation raised the reusability and
extendibility of existing code over other programming styles. Basically our system is
based on single inheritance to provide parallel processing by derived a class from the
class Process. The inheritance can continue to a class from the class derived by the
class Process. OP++ also supports virtual function to give the polymorphism to the
public method in conjunction with dynamic binding.
Our system supports both multi level inheritance and multiple inheritance. While
using multiple inheritance, the only rule to follow is the parents of the class that
emerge in multiple inheritance must derived virtually from the class Process. The
reason is when any object is created to the class derived from the class Process, a
unique process ID is created and maintained in a private data of the class Process.
To avoid the ambiguity of the process ID in class Process, when any class is derived
from more than a class that is derived from the class Process, we need to maintain a
single copy of a class Process by means of virtual base class.

3 Principle of OP++
A program written in OP++ is passed through the C++ preprocessor to extract the
definition about the classes that are derived from the class Process. From the
extracted information, the system creates a file whose name is filename_wrk.cpp by
removing the key word par {} and adding appropriate calls that are related to PVM
library functions. The worker file contains the actual code of the written class, which
is derived from the class Process. The native C++ compiler compiles this file to
generate an executable file (worker program) and it is spawned in the processing
nodes that added to the PVM network at the time of object creation. The file
filename.cpp is regenerated by including necessary library functions that provide an
asynchronous message passing between the worker programs and compiled to
produce executable file (master program). When process object is created, it spawns
the worker process and dispatches the message to it.
The message is packed of class index, function index and list of argument
values. The spawned process, receipt and unpack the message and performs the
desired operation, which is either the creation of a new object or a method invocation
based on the value of class index and function index. After the execution of the
method in the worker process the result is stored in the buffer. The master program
collects the result when it needs. The indices, which are generated by this system, are
unique values of every class and its public method(s) that are derived from the class
Process. The node that runs the master program is said to be master node and other
is worker node.

Object Oriented Parallel Programming Model on a Network of Workstations 1007

4 Related Works
Investigations in amalgamating the concept of object orientation and parallel
programming have already been made for several years. The classification of
merging these concepts is described in [14] i.e., Parallelism in object orientation and
Object orientation in parallelism. Based on these several languages are compared
with respect to what objects stand for. Object may consider as process, shared passive
abstract data type or encapsulation of multiple process and data.
In this section, we highlight the contribution made already in connection with our
research work. We stress the criteria of object orientation in parallelism, casual
programming style, flexibility and modularity. As we are interested in message
passing technique, we have not considered the systems such as pC++ [6], CC++ [2],
Distributed Processing in C++ (DPC++) [16] and DoPVM [10] which are based on
distributed shared memory (DSM), which employs some complex concepts like
mutex, semaphores, deadlock and critical sections. Even though the message passing
technique does not involve such complex concepts, it raises the programming
complexity to manage the synchronization points. So we focused on to provide an
object oriented programming model based on message passing technique.
Many object oriented parallel programming systems have been developed based
on DSM. But very few systems were developed using message passing. Such systems
can be categorized in two ways. The first one is the system which provides object
oriented library functions that supports message passing like PVM++[3], Para++[4],
CPPvm[8]. These systems do not support object orientation in perspective of
programming methodology and these systems are good at system and not at
application level. The second one is providing object orientation in programming
methodology like Dome [1] and Mentat [9, 12] that are closely relevant to our work.
Dome is based on C++, and supports only data parallelism. It is related to our
work as it is run on top of the PVM. Dome supports only SPMD (single program,
multiple data) programming model. It provides a library of classes to express
parallelism. When an object is instantiated to the tailor made classes the data inside
the object is partitioned among the processing node of the PVM network that
performs the same code on different data. There are several differences between our
approach and that of Dome. Our system supports both SPMD and MPMD (multiple
programs, multiple data) programming model often with in the same program. The
Dome is fully constructed of tailor made classes. And also, the problem having
functional parallelism cannot be implemented in Dome.
Mentat is a noteworthy portable C++ based language. The philosophy followed in
our programming model and Mentat looks related to each other. But, the principle
and the execution strategy are totally different from one another. One of the major
concepts is the mechanism of constructing dependency graph. In our system the
dependency is observed based on process object and not depends upon the method
invocation or the value of the parameters of the method of the process object. But in
Mentat the dependency graph is constructed based on the value passed to the method
and not by Mentat object. However, the object creation and method invocation, which
makes the cost of a call unclear to the programmer. That is, if Mentat object is

1008 B. Suresh and R. Nadarajan

instantiated, it is not the instance itself. The new object has been created for every
method invocation on that object. If N times the method is invoked in Mentat object
it will create N instance of that class which may execute in parallel.
In Mentat, definition of each Mentat class must be written in a separate file and
each file must be compiled separately. Even the Mentat support inheritance, the
compilation order is restricted. The base class must be compiled before the derived
class. This gives burden to the programmer to maintain the inheritance hierarchies at
the time of compilation. But in our system, programmer can write the program in a
single file that consists of any number of classes derived from the class Process. In
Mentat, many new notations are introduced for the data types that cast down the
causality of the programming style. One of the highlight of our system is, it allows to
overload the arithmetic operators like +, -, \, * for parallel arithmetic expressions,
function overloading, constructor overloading and virtual functions. Our system
doesn’t support friend functions, which access the private data of the object.

5 Example Program in OP++
In the given example code, the class MathProcess consists of two member functions
that sum up and find the maximum value of the given integer array and return the
results and the class is derived from the class Process. There are two arrays of
process objects ob1 and ob2 are instantiated. The idea behind this is to split the
problem array into 5 sub arrays and it is passed to each process.
#
#
#
#

include <iostream.h>
include <parallel.h>
define SIZE 5000
define NPROCS 5

class MathProcess : Process
{ public :
int summation ( int num, int *data)
{ int result=0;
for (int i=0; i < num; i++)
result+=data[i];
return result; }
int maximum ( int num, int *data)
{ int maxno=data[0];
for (int i=0; i < num; i++)
if (maxno < data[i]) maxno = data[i];
return maxno; }
}; //End of the class
void main () {
MathProcess ob1[NPROCS],ob2[NPROCS]; // 10 processes
int *data,temp1[4],temp2[4],result1=0,result2=0;
data = (int*) malloc (sizeof (int) * SIZE);

Object Oriented Parallel Programming Model on a Network of Workstations 1009

// fill the problem array ( test data)
for (int i=0; i<SIZE; i++) data[i] = i % 25;
par
{ for (i = 0; i < NPROCS; i++) {
temp1[i]=ob1[i].summation(1000,&data[i * 1000]);
temp2[i]=ob2[i].maximum (1000, &data[i * 1000]);}
} // End of par
result2 = temp2[0];
for (i=0; i < NPROCS; i++)
{ result1+ = temp1[i];
if (result2 < temp2[i]) result2 = temp2[i];}
cout << “Summation “ <<result1<<endl;
cout << “Maximum Number “ <<result2<<endl;
} // End of main ()
The process objects in the array ob1 performs addition operation and the array ob2
performs finding maximum number in an array passed to it. In par {} the method of
process objects are invoked asynchronously by passing the array as a parameter. In
method calling, the first parameter represents the number of items from the address
specified followed by it or in other words the value before the address parameter
specify the number of items from the given address. The array is passed by value.
The copy of specified items is moved to the process. The worker process performed
the operation and the result is received at the end of the par {} block and the
intermediate result is calculated and displayed. The worker process is killed from the
network when the destructor of the process object is reached.

6 Conclusion
In this article, we have described the object oriented parallel programming in OP++
based on C++ that integrates the concepts of object orientation and parallel
programming. We have shown the program written in OP++ that gives a casual
programming style. The major problematic point in object oriented parallel
programming is providing inheritance. Often many systems compromised in most
important area like single and multiple inheritance, operator overloading and
constructor overloading. But we designed our system to use all the major concepts of
object oriented programming in parallel programming and also we concentrate more
on minimizing the insertion of new notation that look odd. We have implemented
OP++ on network of workstations that runs on Microsoft’s Windows 95/98/NT and
UNIX and programmed varieties of application programs. Our experience in writing
programs in OP++ shows the benefits of object orientation that provides flexibility,
modularity and casual programming style.

1010 B. Suresh and R. Nadarajan

Acknowledgement.
We wish to thank V. Giraga Durai for his support during this research to led this
paper.

Reference
1.
2.
3.
4.
5.
6.
7.
8.
9.
10.
11.
12.
13.
14.
15.
16.

17.

Adam Begulian, Erik Seligam, Michael strankey. DOME : Distributed Object Migration
Environment. School of Computer Science, Carnegie Mellon University. Technical
Report– CMU–CS–94–153.
K. Mani Chandy, Carl Kesselman. CC++:A Declarative Concurrent Object Oriented
Programming Notation. A Technical Report. California Institute of Technology. March
12, 1993.
Daniel Cohen Laroque. PVM++. Institute of Computer Design & Fault Tolerance.
University of Karlsrue, September 30, 1994.
Oliver Coulaud, Eric Dillon. Para++ : C++ Bindings for Message Passing Libraries –
User Guide.March 1997. http:// www.loria.fr/para++.
Markus Fischer, Jack Dangarra. Experience with Windows 95/NT as a Cluster Computing
Platform for Parallel Computing. Journal of Parallel and Distributed Computing
Practice. 1999, Vol 2Nr2.
Dennis Gannon, Shelby X. Yang, Peter Beckman. User Guide for a Portable Parallel
C++. Programming System, pC++. Deptartment of Computer Science and CICA,
Indiana University, Bloominton, Indiana, USA, September 9, 1994.
G.A Geist, Adam Beguelin, Jack Dongarra, Weicheng Jiang, Robert Manchek, Vaidy
Sunderam.PVM : Parallel Virtual Machine. A User Guide and Tutorial for Networked
Parallel Computing. MIT Press, 1994.
Steffen Gorzig, CPPVM : C++ interface to PVM.
University of Stuttgart, Institute of Parallel and Distributed High Performance Systems.
A.S. Grimshaw. Introduction to Parallel Object Oriented Programming with Mentat.
Department of Computer Science. University of Virginia. Report No TR-91-07, April 4,
1991.
C.L. Hartley and V.S. Sunderam. Programming with Shared Objects in Networked
Environment, International Parallel Processing Symposium, April 1993.
Shih-Chen Huang. Supporting Flexible Programming Model on a Network of
Workstations. PhD dissertation. New York University. January 2000.
Mentat 3.0 User Manual. The Mentat research group. Department of Computer Science,
University of Virginia. http://www.cs.virginia.edu/~mentat
Janusz Niemiec. CC++, pC++, Charm++ and Orca: Languages for Parallel
Programming. CIS Department, Syracuse University, December 22, 1993.
A.A. Radenski. Object Oriented Programming & Parallelism. Information Science Vol.
93, Issue 1-2, North Holland. Department of Computer Science. Winston Salem
University. 1996.
Seyed H Roosta. Parallel Processing and Parallel Algorithms. Springer Publications.
1999.
Adre Silvetia, Rafael Avila, Marcos Barreto, Philippe Navausx. DPC++: Object
Oriented Programming Applied to Cluster Computing. Institute of Informatics, Federal
University of Rio Grande do Sul, Porto Algere, Brazil. First International Workshop of
Project SNOW, November 23, 2001.
B. Stroustrup. The C++ Programming Language, Addison-Wesley, 1991.

