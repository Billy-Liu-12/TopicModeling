A Set of Data Mining Models to Classify Credit
Cardholder Behavior*
1

1

1

2

Gang Kou , Yi Peng , Yong Shi , and Weixuan Xu
1

College of Information Science and Technology
University of Nebraska at Omaha
Omaha, NE 68182, USA
yshi@unomaha.edu
2

Institute of Policy and Management
Chinese Academy of Sciences
Beijing, 100080, China
wxu@mail.casipm.ac.cn

Abstract. In this paper, we present a set of classification models by using
multiple criteria linear programming (MCLP) to discover the various behaviors
of credit cardholders. In credit card portfolio management, predicting the
cardholder’s spending behavior is a key to reduce the risk of bankruptcy. Given
a set of predicting variables (attributes) that describes all possible aspects of
credit cardholders, we first present a set of general classification models that
can theoretically handle any size of multiple-group cardholders’ behavior
problems. Then, we implement the algorithm of the classification models by
using SAS and Linux platforms. Finally, we test the models on a special case
where the cardholders’ behaviors are predefined as five classes: (i) bankrupt
charge-off; (ii) non-bankrupt charge-off; (iii) delinquent; (iv) current and (v)
outstanding on a real-life credit card data warehouse. As a part of the
performance analysis, a data testing comparison between the MCLP and
induction decision tree approaches is demonstrated. These findings suggest that
the MCLP-data mining techniques have a great potential in discovering
knowledge patterns from a large-scale real-life database or data warehouse.
Keywords: Data Mining, Multi-criteria Linear Programming, Classification,
Algorithm, SAS and Linux platforms

1

Introduction

The history of credit card can be traced back to 1951 when the Diners’ Club issued
the first credit card in the US to 200 customers who could use it at 27 restaurants in
New York [1]. At the end of fiscal 1999, there are 1.3 billion payment cards in
circulation and Americans made $1.1 trillion credit purchases [2]. These statistics
show that credit card business becomes a major power to stimulate the US economy
growth in the last fifty years. However, the increasing credit card delinquencies and
*

This research has been partially supported by a grant under (DUE-9796243), the National
Science Foundation of USA, a National Excellent Youth Fund under (#70028101), National
Natural Science Foundation of China and a grant from the K.C. Wong Education
Foundation, Chinese Academy of Sciences.

P.M.A. Sloot et al. (Eds.): ICCS 2003, LNCS 2658, pp. 54–63, 2003.
© Springer-Verlag Berlin Heidelberg 2003

A Set of Data Mining Models to Classify Credit Cardholder Behavior

55

personal bankruptcy rates are causing plenty of headaches for banks and other credit
issuers. The increase in personal bankruptcy rates was substantial. From 1980 to
2000, the number of individual bankruptcy filings in the US increased approximately
500% [3]. How to predict bankruptcy in advance and avoid huge charge-off losses
becomes a critical issue of credit card issuers. Since a credit card database can
contain hundreds and thousands of credit transactions, it is impossible to discover or
predict the cardholders’ behaviors without using mathematical tools. In fact, the
practitioners have tried a number of quantitative techniques to conduct the credit
card portfolio management. Some examples of known approaches are (1) Behavior
Score developed by Fair Isaac Corporation (FICO) [4]; (2) Credit Bureau Score also
developed by FICO [4]; (3) First Data Resource (FDR)’s Proprietary Bankruptcy
Score [5]; (4) Multiple-criteria Score [6,7] and (5) Dual-model Score [8]. A basic
characteristic of these models is that they first consider the behaviors of the
cardholders as two predefined classes: bankrupt accounts and non-bankrupt accounts
according to their historical records. Then they use either statistical methods or
neural networks to compute the Kolmogorov-Smirnov (KS) value that measures the
largest separation of these two cumulative distributions of bankrupt accounts and
non-bankrupt accounts in a training set [9]. The resulting KS values from the
learning process are applied to the real-life credit data warehouse to predict the
percentage of bankrupt accounts in the future. Thus, these methods can be generally
regarded as two-group classification models in credit card portfolio management.
In order to discover more knowledge for advanced credit card portfolio
management, multi-group (group number is larger than two) data mining methods
are needed. Comparing with two-group classification, the multi-group classifier
enlarges the difference between bankrupt accounts and non-bankrupt accounts. This
enlargement increases not only the accuracy of separation, but also provides more
useful information for credit card issuers or banks. From theoretical point of view, a
general model of multi-group classifier is easy to construct. From practical point of
view, the best control parameters (such as class boundaries) have to be identified
through a learning process on a training data set. Therefore, finding the practical
technology with certain size of multi-group classifier is not trivial task. Peng et al
[10] and Shi et al [7] have explored a three-group classifier considering the number
of months where the account has been overlimit. This model produces the prediction
distribution for each behavior class and inner relationship between these classes so
that credit card issuers can establish their credit limit policies for various
cardholders.
The purpose of this paper is to build a set of data mining models to classify
credit cardholders’ behavior. Then these models are tested on the real-life data for
five groups of credit cardholders who are predefined as: Bankrupt charge-off
accounts, Non-bankrupt charge-off accounts, Delinquent accounts, Current accounts
and Outstanding accounts. Bankrupt charge-off accounts are accounts that have
been written off by credit card issuers because of cardholders’ bankrupt claims. Nonbankrupt charge-off accounts are accounts that have been written off by credit card
issuers due to reasons other than bankrupt claims. The charge-off policy may vary
among authorized institutions.

56

2

G. Kou et al.

Models of Multiple Criteria Linear Programming
Classification

A general problem of data classification by using multiple criteria linear
programming can be described as [11]:
Given a set of r variables or attributes in database a = (a1 , …, ar), let A i = (A i1 , …,
A ir ) ∈ R be the sample observations of data for the variables, where i = 1, . . ., n
and n is the sample size. If a given problem can be predefined as s different classes,
G1, …, Gs, then the boundary between the jth and j+1th classes can be bj, j = 1, . . .,
s-1. We want to determine the coefficients for an appropriate subset of the variables,
T
r
denoted by X = (x1, . . ., xr) ∈ R and scalars bj such that the separation of these
classes can be described as follows:
A i X ≤ b 1 , A i ∈ G1; b k −1 ≤ A i X ≤ b k , A i ∈ Gk, k = 2, . . ., s-1; A i X ≥ b s −1 ,
r

A i ∈ Gs; where A i ∈ Gj, j = 1, . . ., s, means that the data case A i belongs to the
class Gj.
The quality of classification is measured by minimizing the total overlapping of data
and maximizing the distances of every data to its class boundary simultaneously. Let
j
α i be the overlapping degree with respect of data case A i within Gj and Gj+1, and
j
β i be the distance from A i within Gj and Gj+1 to its adjusted boundaries. A
multiple criteria linear programming (MCLP) classification model can be defined as:
j
j
(M1)
Minimize Σ i Σjα i and Maximize Σ i Σjβ i
k
k −1
Subject to: A i X ≤ b 1 +α 1i , A i ∈ G1; b k −1 -α i
≤ A i X ≤ b k +α i , A i ∈ Gk,

k
k −1
s −1 , A ∈ G ; b
≤ b k - α i , k = 2, . . ., s-1,
i
s
k −1 + α i
j
j
where A i are given; X and bj are unrestricted; and α i , β i ≥ 0, for j = 1, . . ., s-1, i = 1, . . .,

k = 2, ..., s-1; A i X ≥ b s −1 - α i

n.

k
k −1
Note that the constraints b k −1 + α i
≤ b k - α i ensure the existence of the
boundaries. If minimizing the total overlapping of data, maximizing the distances of
every data to its class boundary, or a given combination of both criteria is considered
separately, model (M1) is reduced to linear programming (LP) classification (known
as linear discriminant analysis), which is initiated by Freed and Glover [12].
However, the single criterion LP could not determine the “best tradeoff” of two
misclassification measurements. Therefore, the model (M1) is potentially better than
LP classification in identifying the best tradeoff of the misclassifications for data
separation. To facilitate the computation on the real-life data, a compromise solution
approach [13,14,15] is employed to reform model (M1) for the “best tradeoff”
j
j
between Σ i Σjα i and Σ i Σjβ i . Let us assume the “ideal values” for s-1 classes
s −1

overlapping (-Σ i α 1i ,…, -Σ i α i

s −1

) be (α 1* ,…,α * ) >0, and the “ideal values” of

A Set of Data Mining Models to Classify Credit Cardholder Behavior
s −1

(Σ i β 1i , …, Σ i β i

s −1

) be (β 1* , …, β * ). When -Σ i α i

+

j

57

j

> α * , we define the regret

j

j

measure as -d αj = α * + Σ i α i ; otherwise, it is 0, where j = 1, . . ., s-1. When j

−

j

j

j

Σ i α i < α * , we define the regret measure as d αj = α * + Σ i α i ; otherwise, it is 0,
where j = 1, . . ., s-1. Thus, we have:
−

+

j

−

+

j

+

−

−

+

Theorem 1. α * + Σ i α i = d αj - d αj , |α * + Σ i α i | = d αj + d αj , and d αj , d αj ≥
j

j

j

0, j = 1, . . ., s-1.
Similarly, we can derive:
−

+

−

+

Corollary 1. β * - Σ i β i = d βj - d βj , |β * - Σ i β i | = d βj + d βj , and d βj , d βj ≥ 0,
j

j

j

j = 1, . . ., s-1.
Incorporating the above results into model (M1), it is reformulated as:
s −1

(M2)

Minimize

∑
j =1

Subject to:

j
α*

+

−

−

+

(d αj + d αj + d βj + d βj )
−

j

+

j

j

−

+

+ Σ i α i = d αj - d αj , j = 1, . . ., s-1; β * - Σ i β i = d βj - d βj , j = 1, . . .,

s-1;

k −1

A i X ≤ b 1 +α 1i , A i ∈ G1; b k −1 -α i

k

≤ A i X ≤ b k +α i , A i ∈ Gk, k = 2, . . ., s-1;

k
k −1
s −1 , A ∈ G ; b
≤ b k - α i , k = 2, . . ., s-1,
i
s
k −1 + α i

A i X ≥ b s −1 - α i
j

j

where Ai, α * , and β *
−

are given; X and b

j

j

j

−

+

are unrestricted; and α i , β i , d αj , d αj ,

+

d βj , d βj ≥ 0, for j = 1, . . ., s-1, i = 1, . . ., n.

We can call model (M1) or (M2) is a “weak separation formula” since it considers as
many overlapping data as possible. We can build a “medium separation formula” on
the absolute class boundaries in (M3) and a “strong separation formula” which
contains as few overlapping data as possible in (M4).
s −1

(M3)

Minimize

∑
j =1

Subject to:

j
α*

+

−

−

+

(d αj + d αj + d βj + d βj )
j

−

+

j

j

−

+

+ Σ i α i = d αj - d αj , j = 1, . . ., s-1; β * - Σ i β i = d βj - d βj , j = 1, . . .,

s-1;
A i X ≤ b 1 , A i ∈ G1; b k −1 ≤ A i X ≤ b k , A i ∈ Gk, k = 2, . . ., s-1;

k

A i X ≥ b s −1 , A i ∈ Gs; b k −1 + ε ≤ b k - α i , k = 2, . . ., s-1,

58

G. Kou et al.
j

j

where Ai, ε , α * , and β *
−

are given; X and b

j

j

−

+

are unrestricted; and α i , β i , d αj , d αj ,

j

+

d βj , d βj ≥ 0, for j = 1, . . ., s-1, i = 1, . . ., n.
s −1

(M4)

Minimize

∑
j =1

Subject to:

j
α*

+

−

−

+

(d αj + d αj + d βj + d βj )
−

j

+

j

j

+

−

+ Σ i α i = d αj - d αj , j = 1, . . ., s-1; β * - Σ i β i = d βj - d βj , j = 1, . . .,

s-1;

k −1

A i X ≤ b 1 - α 1i , A i ∈ G1; b k −1 + α i

k

≤ A i X ≤ b k - α i , A i ∈ Gk, k = 2, . . ., s-1;

k
k −1
s −1 , A ∈ G ; b
≤ b k - α i , k = 2, . . ., s-1,
i
s
k −1 + α i

A i X ≥ b s −1 + α i
j

j

where Ai, α * , and β *
−

are given; X and b

j

j

j

−

+

are unrestricted; and α i , β i , d αj , d αj ,

+

d βj , d βj ≥ 0, for j = 1, . . ., s-1, i = 1, . . ., n.

A loosing relationship of models (M2), (M3), and (M4) is given as:
Theorem 2. If a data case A i is classified in a given class Gj by model (M4), then it
may be in Gj by using models (M3) and (M2). If a data case A i is classified in a
given class Gj by model (M3), then it may be in Gj by using models (M2).
Example 1. As an illustration, we use a small training data set adapted from [16,17]
in Table 1 (Column 1-6) to show how the two-class model works.
Suppose whether or not a customer buys computer relates to the attribute set {Age,
Income, Student and Credit_rating}. We first define the variables Age, Income,
Student and Credit_rating by numeric numbers as follows:
For Age: “≤30” assigned to be “3”; “31…40” to be “2”; and “>40” to be “1”.
For Income: “high” assigned to be “3”; “medium” to be “2”; and “low” to be “1”.
For Student: “yes” assigned to be “2” and “no” to be “1”.
For Credit_rating: “excellent” assigned to be “2” and “fair” to be “1”.
G1 = {yes to buys_computer} and G2 = {no to buys_computer}
Then, let j = 1, 2 and i = 1, . . ., 14, model (M2) for this problem to classify the
customer’s status for {buys_computer} is formulated by
-

+

-

+

Minimize dα + dα + dβ + dβ
Subject to:
α* + Σiαi = dα - - dα + ,β* - Σiβi = dβ - - dβ+ ,
2x1 + 3x2 + x3 + x4 = b + α1 - β1, x1 + 2x2 + x3 + x4 = b + α2 - β2
x1 + x2 + 2x3 + x4 = b + α3 - β3, 2x1 + x2 + 2x3 + 2x4 = b + α4 - β4
3x1 + x2 + 2x3 + x4 = b + α5 - β5, x1 + 2x2 +2 x3 + x4 = b + α6 - β6
3x1 + 2x2 + 2x3 + 2x4 = b + α7 - β7, 2x1 + 2x2 + x3 + 2x4 = b + α8 - β8
2x1 + 3x2 + 2x3 + x4 = b + α9 - β9, 3x1 + 3x2 + x3 + x4 = b + α10 - β10

A Set of Data Mining Models to Classify Credit Cardholder Behavior

59

3x1 + 3x2 + x3 + 2x4 = b + α11 - β11, x1 + x2 + 2x3 + 2x4 = b + α12 - β12
3x1 + 2x2 + x3 + x4 = b + α13 - β13, x1 + 2x2 + x3 + 2x4 = b + α14 - β14
where α*, and β* are given, x1 , x2 , x3 , x4 and b are unrestricted, and αi , βi dα ,
+
+
dα , dβ , dβ ≥ 0, i = 1,…, 14.
Before solving the above problem for data classification, we have to choose
the values for the control parameters α*, β* and b. Suppose we use α* = 0.1, β* =
30000 and b = 1. Then, the optimal solution of this linear program for the classifier
is obtained as Column 7 of Table 1, where only cases A8 and A12 are misclassified. In
other words, cases {A1, A2, A3, A4, A5, A6, A7, A9 } are correctly classified in G1, while
cases {A10, A11 , A13, A14}are found in G2. Similarly, when we apply models (M3) and
(M4) with ε = 0, one of learning processes provides the same results where cases
{A1, A2, A3, A5, A8} are correctly classified in G1, while cases {A10, A11, A12, A14} are
correctly found in G2. Then, we see that cases {A1, A2, A3, A5} classified in G1 by
model (M4) are also in G1 by models (M3) and (M2), and cases {A10, A11, A14}
classified in G2 by model (M4) are in G2 by model (M3) and (M2). This is
consistent to Theorem 2.
Table 1. A two-class data set of customer status
Cases

A1
A2
A3
A4
A5
A6
A7
A8
A9
A10
A11
A12
A13
A14

Age

Income

Student

Credit
Rating

31… 40
>40
> 40
31… 40
≤30
>40
≤30
31… 40
31… 40
≤30
≤30
>40
≤30
>40

high
medium
low
low
low
medium
medium
medium
high
high
high
low
medium
medium

no
no
yes
yes
yes
yes
yes
no
yes
no
no
yes
no
no

fair
fair
fair
excellent
fair
fair
excellent
excellent
fair
fair
excellent
excellent
fair
excellent

Class:
buys_
computer
yes
yes
yes
yes
yes
yes
yes
yes
yes
no
no
no
no
no

Training
results
success
success
success
success
success
success
success
failure
success
success
success
failure
success
success

3 Algorithm Implementation and Software Development
A general algorithm to execute the MCLP classification method can be outlined as:
Algorithm 1.
Step 1 Build a data mart for task data mining project.
Step 2 Generate a set of relevant attributes or dimensions from a data mart,
transform the scales of the data mart into the same numerical measurement and
determine predefined classes, classification threshold, training set and verifying set.
Step 3 Use the MCLP model to learn and compute the best overall score (X*) of
the relevant attributes or dimensions over all observations.
Step 4 Discover the interested patterns that can best match the original classes
under the threshold by choosing the proper control parameters (α*, β* and b). If the
patterns are found, go to Step 5. Otherwise, go back to Step 3.
Step 5 Apply the final learned score (X**) to predict the unknown data cases.

60

G. Kou et al.

Two versions of actual software have been developed for the MCLP
classification method. The first version is based on the well-known commercial SAS
platform. In this software, we have applied SAS codes to execute Algorithm 1 in
which the MCLP models (M2)-(M4) utilize SAS linear programming procedure.
The second version of the software is written by C++ language running on Linux
platform. The reason for developing Linux version of the MCLP classification
software is that the majority of database vendors, such as IBM are aggressively
moving to Linux-based system development. Our Linux version goes along with the
trend of information technology. Because many large companies currently use SAS
system for data analysis, our SAS version is also useful to conduct data mining
analysis under SAS environment.

4 Experimental Results from a Real-Life Database
Given a set of attributes, such as monthly payment, balance, purchase, and cash
advance and the criteria about “bankruptcy”, the purpose of data mining in credit
card portfolio management is to find the better classifier through a training set and
use the classifier to predict all other customer’s spending behaviors [18]. The
frequently used data-mining model in the business is still two-class separation
technique. The key of two-class separation is to separate the “bankruptcy” accounts
from the “current” accounts and identify as many bankruptcy accounts as possible.
This is also known as the method of “making black list.” The examples of popular
methods are Behavior Score, Credit Bureau Score, FDC Bankruptcy Score, and Set
Enumeration Decision Tree Score [6]. These methods were developed by either
statistics or decision tree. Using a terabyte real credit database of the major US
Bank, the SAS version of two-class MCLP model (as in Example) has demonstrated
a better predication power (e.g., higher KS values) than these popular business
methods [6].
4.1 Testing on Five-Class MCLP Models
We demonstrate the experimental results of 5-class MCLP model in Linux version
on a real-life credit data warehouse from a major US bank. Since this database
contains 64 attributes with a lot of overlapping, we employed the weak separation
model (M2).
In the five-class MCLP model, we defined five classes as Bankrupt charge-off
accounts (the number of over-limits ≥ 13), Non-bankrupt charge-off accounts (7 ≤
the number of over-limits ≤ 12), Delinquent accounts (3 ≤ the number of over-limits
≤ 6), Current accounts (1 ≤ the number of over-limits ≤ 2), and Outstanding accounts
(no over limit). We selected 200 samples with 40 accounts in each class as the
training set. The same 5,000 samples with 53 in G1; 165 in G2; 379 in G3, 482 in G4
and 3921 in G5 were used as the verifying set. We found, in the training process,
that G1 has been correctly identified 47.5% (19/40), G2 55% (22/40), G3 47.5%
(19/40), G4 42.5% (17/40), and KS values 42.5 for G1 vs. G2, G2 vs. G3 and G3 vs.
G4, but 67.5 for G4 vs. G5 in Figure 1. When we used this as the better classifier, we
predicted the verifying set as G1 for 50.9% (27/53), G2 for 49.7% (82/165), G3 for
40% (150/379), G4 for 31.1% (150/482) and G5 for 54.6% (2139/3921). The
predicted KS values are 36.08 for G1 vs. G2, 23.3 for G2 vs. G3, 27.82 for G3 vs.

A Set of Data Mining Models to Classify Credit Cardholder Behavior

61

G4, and 42.17 for G4 vs. G5 in Figure 2. This indicates that the separation between
G4 and G5 is better than other situations. In other words, the classifier is favorable
to G4 vs. G5. We note that many real-life applications do not require more than five
classes separations. This claim is partially supported by psychological studies.
According to [19], Human attention span is “seven plus or minus two”. Therefore,
for the practical purpose of classification in data mining, classifying five interesting
classes in a terabyte database can be very meaningful.

Fig. 1. Five-Class Training Data Set

Fig. 2. Five-Class Verifying Data Set

4.2 Comparison of MCLP Method and Decision Tree Method
A commercial Decision Tree software C5.0 (the newly updated version of C4.5) was
used to test the classification accuracy of the multiple classes (from two to five)
against MCLP methods [17, 20]. Given a terabyte credit card database of the major
US Bank, the number samples for the training sets were 600 for two-class problem,
300 for three-class problem, 160 for four-class problem, and 200 for five-class
problem. The verifying sets were used the same 5,000 credit card records as in
Section 4.1. Table 2 summarizes these comparisons of two methods. As we see,
generally the decision tree method does a better job than the MCLP method on
training sets when the sample size is small. When applying the classifier from the

62

G. Kou et al.

training process on the larger verifying sets, the MCLP method outperforms the
decision tree method. Two issues may cause this evidence. One is that the MCLP
method as a linear model may miss some nonlinear nature of data, while the decision
tree is nonlinear model. This could be the reason why the latter is better than the
former in the training process. However, the robustness and stability of the MCLP is
better than the decision tree when the classifier is applied to predict the classification
of the verifying sets. This may be due to the fact that the MCLP method employs
optimization to find the optimal factors from all feasible factors for the scores while
the decision tree method just selects the better tree from a limited built trees, which
is not a best tree. In addition, when the decision tree gets big (i.e., the size of the
verifying sets increase), the pruning procedure may further eliminate some better
branches.
Table 2. Five-class Comparison of MCLP and Decision Tree
5
Classes

T-Set

V-Set

Multi Criteria Linear Programming
Class
1
2
3
4
5
Class
1
2
3
4
5

(a)
19
3
0
0
0
(a)
27
26
20
13
28

(b)
21
22
8
0
0
(b)
18
82
145
70
110

(c)
0
15
19
6
0
(c)
6
42
150
182
470

(d)
0
0
13
17
10
(d)
2
14
41
150
1074

(e)
0
0
0
17
30
(e)
0
1
13
32
2139

Decision Tree
Total
40
40
40
40
40
Total
53
165
379
482
3921

Class
1
2
3
4
5
Class
1
2
3
4
5

(a)
35
0
0
0
0
(a)
41
30
38
26
85

(b)
4
37
1
0
0
(b)
6
82
104
73
95

(c)
0
1
38
5
0
(c)
4
31
159
178
1418

(d)
1
2
0
29
2
(d)
2
16
63
162
428

(e)
0
0
1
6
38
(e)
0
6
14
44
1895

Total
40
40
40
40
40
Total
53
165
379
482
3921

Furthermore, a parallel experimental study on the MCLP classifications through
the developed SAS version can be referred to [18]. For the sake of space, we will not
elaborate on the results here.

5

Concluding Remarks

There are some research and experimental problems remaining to be explored. From
the structure of MCLP formulation, the detailed theoretical relationship of models
(M2), (M3), and (M4) need to be further investigated in terms of classification
separation accuracy and predictive power. In addition, in the proposed MCLP
models, the penalties to measure the “cost” of misclassifications (or coefficients of
j
j
Σ i Σjα i and Σ i Σjβ i ) were fixed as 1. If they are allowed to change, their influence
on the classification results can be studied, and a theoretical sensitivity analysis of
the misclassification in the MCLP models will be also conducted. From
mathematical structure point of view, a multiple criteria non-linear classification
p

model may be generalized if the hyper-plane X becomes non-linear cases, say X , p
>1. The possible connection of the MCLP classification with the known Support
Vector Machine (SVM) method in pattern recognition can be researched. In the
empirical tests, we have noticed that identifying the optimal solution for model (M2),
(M3), or (M3) in the training process may be time-consuming. Instead, we can apply
the concept of fuzzy multiple criteria linear programming to seek a satisfying

A Set of Data Mining Models to Classify Credit Cardholder Behavior

63

solution that may lead to a better data separation. Other well-known methods, such
as neural networks, rough set, and fuzzy set should be considered into part of the
extensive comparison study against the MCLP method so that the MCLP method can
be known in the data mining community for both researchers and practitioners. We
will report any significant results from these ongoing projects in the near future.

References
1
2
3
4
5
6

7

8
9
10
11

12
13
14
15
16
17
18
19
20
21

The First Credit Card Was Issued in 1951. http://www.didyouknow.cd/creditcards.htm.
Debt statistics. http://www.nodebt.org/debt.htm.
Stavins, J.: Credit Card Borrowing, Delinquency, And Personal Bankruptcy. New England Economic Review. July/August 2000,
http://www.bos.frb.org/economic/neer/neer2000/neer400b.pdf.
www.fairisaac.com
http://www.firstdatacorp.com
Shi, Y., Wise, M., Luo, M., Lin, Y.: Data Mining in Credit Card Portfolio Management:
A Multiple Criteria Decision Making Approach. In: Koksalan, M., Zionts, S. (eds.):
Multiple Criteria Decision Making in the New Millennium. Springer, Berlin (2001) 427436.
Shi, Y., Peng, Y., Xu, W., Tang, X.: Data Mining Via Multiple Criteria Linear
Programming: Applications in Credit Card Portfolio Management. International Journal
of Information Technology and Decision Making. 1 (2002) 131-151.
Lin, Y.: Improvement On Behavior Scores by Dual-Model Scoring System. International
Journal of Information Technology and Decision Making. 1 (2002) 153-164.
Conover, W.J.: Practical Nonparametric Statistics. Wiley (1999).
Peng, Y., Shi, Y., Xu, W: Classification for Three-Group Of Credit Cardholders’
Behavior Via A Multiple Criteria Approach. Advanced Modeling and Optimization. 4
(2002) 39-56.
Kou, G., Peng, Y., Shi, Y., Wise M., Xu, W.: Discovering Credit Cardholders’ Behavior
by Multiple Criteria Linear Programming. Working Paper, College of Information
Science and Technology, University of Nebraska at Omaha (2002).
Freed, N. , Glover, F.: Simple but Powerful Goal Programming Models for Discriminant
Problems. European Journal of Operational Research. 7 (1981) 44–60.
Shi, Y., Yu, P.L.: Goal Setting and Compromise Solutions. In Karpak, B., Zionts, S.
(eds.): Multiple Criteria Decision Making and Risk Analysis Using Microcomputers.
Springer-Verlag, Berlin (1989) 165–204.
Shi, Y.: Multiple Criteria Multiple Constraint-levels Linear Programming: Concepts,
Techniques and Applications. World Scientific Publishing, River Edge, New Jersey
(2001).
Yu, P.L.: Multiple Criteria Decision Making: Concepts, Techniques and Extensions.
Plenum, New York (1985).
Han, J., Kamber, M.: Data Mining: Concepts and Techniques. Morgan Kaufmann
Publishers, San Francisco, California (2001).
Quinlan, J.: Induction of Decision Trees. Machine Learning. 1 (1986) 81–106.
Peng, Y.: Data Mining in Credit Card Portfolio Management: Classifications for Card
Holder Behavior. Master Thesis, College of Information Science and Technology,
University of Nebraska at Omaha (2002).
Miller, G.A.: The Magical Number Seven, Plus or Minus Two: Some Limits on Our
Capacity for Processing Information. The Psychological Review. 63 (1956) 81–97.
http://www.rulequest.com/see5-info.html

