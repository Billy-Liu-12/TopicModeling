On the Optimality of Linear, Diﬀerential, and
Sequential Distinguishers
Pascal Junod
Security and Cryptography Laboratory
Swiss Federal Institute of Technology
CH-1015 Lausanne, Switzerland
pascal.junod@epfl.ch

Abstract. In this paper, we consider the statistical decision processes
behind a linear and a diﬀerential cryptanalysis. By applying techniques
and concepts of statistical hypothesis testing, we describe precisely the
shape of optimal linear and diﬀerential distinguishers and we improve
known results of Vaudenay concerning their asymptotic behaviour.
Furthermore, we formalize the concept of “sequential distinguisher” and
we illustrate potential applications of such tools in various statistical
attacks.
Keywords: Distinguishers, Statistical Hypothesis Testing, Linear
Cryptanalysis, Diﬀerential cryptanalysis

1

Introduction

Historically, statistical procedures are indissociable of cryptanalytic attacks against block ciphers. One of the ﬁrst attack exploiting statistical correlations in
the core of DES [24] is Davies and Murphy’s attack [9]. Biham and Shamir’s differential cryptanalysis [1,2,3], Matsui’s attack against DES [17,18], Vaudenay’s
statistical and χ2 cryptanalysis [29], Harpes and Massey’s partitioning cryptanalysis [13], and Gilbert-Minier stochastic cryptanalysis [21] are attacks using
statistical procedures in their core.
To the best of our knowledge, Murphy et al., in an unpublished report [22],
proposed for the ﬁrst time a general statistical framework for the analysis of
block ciphers using the technique of likelihood estimation. Other examples can
be found in the ﬁeld of cryptology: recently, Coppersmith, Halevi and Jutla [7]
have devised a general statistical framework for analysing stream ciphers; they
use the concept of statistical hypothesis testing for systematically distinguishing
a stream cipher from a random function. Other examples (this list being nonexhaustive) include Maurer’s analysis of Simmon’s authentication theory [19,20]
and Cachin’s theoretical treatment of steganography [4,5].
In a parallel way, some attempts of formalizing resistance of block ciphers
towards cryptanalytic attacks have been proposed: for instance, Pornin [25] proposes a general criterion of resistance against the Davies and Murphy attack;
for this purpose, he makes use of statistical hypothesis testing. Vaudenay, in a
E. Biham (Ed.): EUROCRYPT 2003, LNCS 2656, pp. 17–32, 2003.
c International Association for Cryptologic Research 2003

18

P. Junod

sequence of papers (e.g. [30,28]) proposes the decorrelation theory as a generic
technique for estimating the strength of block ciphers against various kinds of
attacks. In these papers, he notably derives bounds on the best advantage of any
linear and diﬀerential distinguishers, however without using statistical hypothesis testing concepts.
As pointed out by many authors, statistical hypothesis tests are convenient in
the analysis of statistical problems, since, in certain cases, well-known optimality
results (like the Neyman-Pearson lemma, for instance) can be applied.

Contributions of This Paper
In this paper, we consider the resistance of block ciphers against linear and
diﬀerential cryptanalysis as a statistical hypothesis testing problem, which allows
us to improve Vaudenay’s asymptotic bounds on the best advantage of any linear
and diﬀerential distinguishers and to give optimality results on the decision
processes involved during these attacks.
For this, we recall some well-known statistical concepts in Section §2. In
Section §3, we treat linear distinguishers and we derive a Chernoﬀ-like bound,
which gives the right asymptotic behaviour of the best advantage of such
distinguishers. In §4, we do the same for diﬀerential distinguishers. In §5, we
formalize the notion of sequential distinguisher ; this kind of statistical procedure
has been recognized quite early as potentially useful (in [22,9], for instance).
We restate this by showing, with help of a toy-example (a linear cryptanalysis
of 5-rounds DES), that sequential sampling procedures may divide the needed
number of plaintext-ciphertext pairs by a non-negligible factor in certain
statistical cryptanalysis. In §6, we discuss potential applications of statistical
hypothesis testing concepts in various attacks, and ﬁnally, we conclude in §7.
Notation. The following notation will be used throughout this paper. Random variables1 X, Y, . . . are denoted by capital letters, while realizations
x ∈ X , y ∈ Y, . . . of random variables are denoted by small letters; random
vectors X, Y, . . . and their realizations x, y, . . . are denoted in bold characters.
The fact for a random variable X to follow a distribution D is denoted X ← D,
while its probability function is denoted by PrX [x]. Finally, as usual, “iid”
means “independent and identically distributed”.

2

Statistical Hypothesis Testing

We recall some well-known facts about statistical hypothesis testing, both in the
classical and in the Bayesian approaches; details can be found in any good book
on statistics (e.g. see [26]).
1

In this paper, we are only dealing with discrete random variables.

On the Optimality of Linear, Diﬀerential, and Sequential Distinguishers

2.1

19

Classical Approach

Let D0 and D1 be two diﬀerent probability distributions deﬁned on the same ﬁnite set X . In a binary hypothesis testing problem, one is given an element x ∈ X
which was drawn according either to D0 or to D1 and one has to decide which
is the case. For this purpose, one deﬁnes a so-called decision rule, which is a
function δ : X → {0, 1} taking a sample of X as input and deﬁning what should
be the guess for each possible x ∈ X . Associated to this decision rule are two different types of error probabilities: α PrX0 [δ(x) = 1] and β PrX1 [δ(x) = 0].
The decision rule δ deﬁnes a partition of X in two subsets which we denote by A
and A, i.e. A ∪ A = X ; A is called the acceptance region of δ. We recall now the
Neyman-Pearson lemma, which derives the shape of the optimal statistical test
δ between two simple hypotheses, i.e. which gives the optimal decision region
A.
Lemma 1 (Neyman-Pearson). Let X be a random variable drawn according
to a probability distribution D and let be the decision problem corresponding to
hypotheses X ← D0 and X ← D1 . For τ ≥ 0, let A be deﬁned by
A

x∈X :

PrX0 [x]
≥τ
PrX1 [x]

(1)

Let α∗ PrX0 A and β ∗ PrX1 [A]. Let B be any other decision region with
associated probabilities of error α and β. If α ≤ α∗ , then β ≥ β ∗ .
Hence, the Neyman-Pearson lemma indicates that the optimum test (regarding
error probabilities) in case of a binary decision problem is the likelihood-ratio
test. All these considerations are summarized in Deﬁnition 1.
Deﬁnition 1 (Optimal Binary Hypothesis Test). To test X ← D0 against
X ← D1 , choose a constant τ > 0 depending on α and β and deﬁne the likelihood
ratio
PrX0 [x]
(2)
lr(x)
PrX1 [x]
The optimal decision function is then deﬁned by
δopt

0 (i.e accept X ← D0 ) if lr(x) ≥ τ
1 (i.e. accept X ← D1 ) if lr(x) < τ

(3)

We note that Lemma 1 does not consider any special hypothesis on the observed random variable X. In the following, we will assume that we are interested
in taking a decision about the distribution of a random vector X (X1 , . . . , Xn )
where X1 , . . . , Xn are iid random variables, i.e. X ← Dn is a random vector of
n independent samples of the random variable X. This is a typical situation
during a known-plaintext attack.
When dealing with error probabilities, one usually proceeds as follows in the
classical approach: one of the two possible error probabilities is ﬁxed, and one
minimizes the other error probability. In this case, Stein’s lemma (we refer to [8]
for more details) gives the best error probability expression. As this approach
lacks symmetry, we won’t describe it in more details.

20

P. Junod

2.2

Bayesian Approach

The other possibility is to follow a Bayesian approach and to assign prior probabilities π0 and π1 to both hypotheses, respectively, and costs κi,j ≥ 0 to the
possible decisions i ∈ {0, 1} and states of nature j ∈ {0, 1}. In this case, we
would like to minimize the expected cost. If we assign κ0,0 = κ1,1
0 and
κ0,1 = κ1,0 1, i.e. correct decisions are not penalized, while incorrect decisions
are penalized equally, then the optimal Bayesian decision rule is given by
δ(x)

0 if π0 PrX0n [x] ≥ π1 PrX1n [x]
1 if π0 PrX0n [x] < π1 PrX1n [x]

(4)

(n)

Clearly, the overall error probability Pe
π0 α(n) + π1 β (n) of such an optimal
Bayesian distinguisher must decrease towards zero as the number n of samples
increases. It turns out that the decrease asymptotically approaches an exponential in the number of samples drawn before the decision, the exponent being
given by the so-called Chernoﬀ bound (see Theorem 1; in the long version [14]
of this paper, we give some information-theoretic results justifying this bound,
and we refer to [8] for a detailed and complete treatment).
Theorem 1 (Chernoﬀ ). The best probability of error of the Bayesian decision
rule deﬁned in (4) satisﬁes
(n)

Pe
1
log −nν = 0
n→+∞ n
2
lim

(5)

where ν = C(D0 , D1 ) is the Chernoﬀ information2 between D0 and D1 .
Note that the Bayesian error exponent does not depend on the actual value of π0
and π1 , as long as they are non-zero: essentially, the eﬀect of the prior is washed
out for large sample sizes.

3

Linear Distinguishers

In this section, we consider the classical model of a linear distinguisher and we
present several new results derived using tools of statistical hypothesis testing.
3.1

Introduction

A linear distinguisher δlin is a (possibly computationally unbounded) Turing
machine which can play with an oracle Ω implementing a permutation C; δlin is
bounded in the number n of queries to the oracle Ω. Furthermore, it uses a linear
characteristic (a, b) which is a pair of boolean vectors. Algorithm 1 deﬁnes the
classical modelization of a linear distinguisher (see [30]).
2

The Chernoﬀ information between two discrete probability distributions D0 and D1
is
C(D0 , D1 )

Pr[x]λ Pr[x]1−λ

− min log
0≤λ≤1

x∈X

X0

X1

On the Optimality of Linear, Diﬀerential, and Sequential Distinguishers
1:
2:
3:
4:
5:
6:
7:
8:
9:
10:
11:
12:
13:
14:

21

Parameters: a complexity n, a characteristic (a, b), an acceptance region A(n)
Input: an oracle Ω which implements a permutation C
Initialize a counter u to 0.
for i = 1 . . . n do
Pick uniformly at random x and query C(x) to the oracle Ω.
if a · x = b · C(x) then
Increment u
end if
end for
if u ∈ A(n) then
Output 0
else
Output 1
end if
Algorithm 1: Modelization of a linear distinguisher δlin .

The statistical game is the following. One gives an oracle Ω to Algorithm
1, which is with probability π0 = 12 the permutation C or, with probability
π1 = 12 , a permutation C ∗ ∈U Cm drawn uniformly at random from the set Cm
of all permutations over inputs of size m (C ∗ is often refereed as the “Perfect
Cipher”). The goal of Algorithm 1 is to decide whether Ω implements C or C ∗ .
One measures the performance of a distinguisher δlin by the expression
Advnδlin (C, C ∗ )

Pr [δlin (x) = 1] − Pr∗ [δlin (x) = 1] = 2Pe(n) − 1
C

C

(6)

where x = (x1 , . . . , xn ) is the vector of the values queried to the oracle. The
distinguisher’s core is the acceptance region A(n) : it deﬁnes the set of values
(x1 , . . . , xn ) which lead to output 0 (i.e. it decides that the oracle implements
C) or 1 (i.e. it decides that the oracle implements C ∗ ).
As pointed out by Chabaud and Vaudenay in [6], linear cryptanalysis is
2
based on the quantity LPC (a, b) (2 · PrX [a · X = b · C(X)] − 1) . This value
depends of the (ﬁxed) permutation C and of the distribution of plaintext, which
is usually deﬁned to be uniform. Actually, most of the time, a cryptanalyst
does not possess any information about the permutation (i.e. about the key), so
one is more interested in the average LPC (a, b) over the permutation space Cm
(or, equivalently, over the key space K); this quantity is denoted ELP(a, b)
E LPC (a, b) , where the expectation is taken over the permutation distribution.
When studying linear distinguishers, one is interested in bounding the advantage of any linear distinguisher in terms of ELP(a, b). We review now a known
result of Vaudenay (see [28], for instance).
Theorem 2 (Vaudenay). For any distinguisher in the model of Algorithm 1
BestAdvnδlin (C, C ∗ ) ≤ 2.78 3 n · ELP(a, b) + 2.78 3
where m is the block size of the permutation.

n
2m − 1

(7)

22

P. Junod

In the case of a practical linear cryptanalysis of DES [18], we have ELP(a, b) ≈
2
4 · 1.19 · 2−21 ≈ 1.288 · 10−12 and m = 64, which means that (7) is useful as
long as n ≤ 235 . Thus, although of great theoretical interest, we note that (7)
is not tight for large n, or, in other words, does not capture the asymptotical
behavior of the advantage. In the next part, we reconsider this problem in the
statistical hypothesis testing framework and we derive an asymptotically tight
Chernoﬀ-like bound on the best advantage of any linear distinguisher.

3.2

New Asymptotic Bounds
(n)

First, we note that if δlin is optimal, then Pe ≤ 12 for all n > 0 (otherwise, we
could modify it such that it outputs the opposite decision as deﬁned in Algorithm
(n)
1 and get a smaller error probability). Thus, we have Advnδlin (C, C ∗ ) = 1−2Pe .
As outlined before, the crucial part of δlin is the acceptance region A(n) . The
following lemma, which is a direct application of Lemma 1, gives the optimal
(n)
Aopt , i.e. the region producing the smallest overall error probability. Without
with > 0
loss of generality, we assume that E [PrX [a · X = b · C(X)]] 12 +
where the expectation is taken over a uniformly distributed plaintext space X
and the key space K.
Lemma 2. The optimal acceptance region for δlin is
(n)

Aopt =

u ∈ {0, . . . , n} : u ≥ n ·

log2 (1 − 2 )
log2 (1 − 2 ) − log2 (1 + 2 )

(8)

where u is deﬁned in Algorithm 1.
See [14] for a detailed proof. Note that, for
with
(n)

Aopt ≈

small, one can approximate (8)

u ∈ {0, . . . , n} : u ≥ n ·

1
+
2 2

(9)

Using a precise version of Chernoﬀ’s theorem 1, we can bound the advantage of
the best linear distinguisher as follows (see [14] for a detailed proof):
Theorem 3. Let m be the block size of the involved permutations. For any distinguisher in the model of Algorithm 1
1−

(n + 1)
1
≤ BestAdvnδlin (C, C ∗ ) ≤ 1 −
2nν−1
(n + 1) · 2nν−1

(10)

where ν = C(D0 , D1 ) is the Chernoﬀ information between D0 , a binary distribution having a bias equal to max{ 2m1−1 , } such that ELPC (a, b) = 4 2 and the
uniform binary distribution D1 .

On the Optimality of Linear, Diﬀerential, and Sequential Distinguishers

23

Generally, the Chernoﬀ information cannot be expressed explicitely, because
one has to solve a transcendental equation. However, in the case which interests
us, ν(λ) 2−λ · (( 12 + )λ + ( 12 − )λ ) and
∗

∗

C(D0 , D1 ) = ν(λ ) for λ =

log − log(1−2
log(1+2

)
)

1+2
1−2

log

(11)

We give now a numerical illustration: for = 1.19 · 2−21 (which is the bias of the
best linear approximation of 14 rounds of DES), we obtain a useful lower bound
only for n ≥ 248.2 ; unfortunately, even if it captures the asymptotic exponential
shape of the best advantage curve, it is not practically useful for “interesting”
values of n; for which concerns the upper bound, it is useful for all n but it is
not tight: one may give a tighter lower bound using Bernstein’s inequality (see
Theorem 4 and [11] for a proof). In the following, we will assume that is small
and thus that one is using (9) as acceptance region.
Theorem 4 (Bernstein’s Inequality). Let Xi be iid discrete random variables following a Bernoulli law with parameter 0 ≤ p ≤ 1 and let Sn
i Xi .
Then
Pr [Sn ≥ n(p + )] ≤ e− 4 n
1

2

>0

for

(12)

This allows to derive in an easy way the following lower bound:
Theorem 5. Let m be the block size of the involved permutations. For any distinguisher in the model of Algorithm 1
BestAdvnδlin (C, C ∗ ) ≥ 1 − e−
where

4

n 2
16

(13)

max{ 2m1−1 , } such that ELPC (a, b) = 4 2 .

Diﬀerential Distinguishers

Similarly, one can study diﬀerential distinguishers with the same tools. A differential distinguisher δdiﬀ is a (possibly computationally unbounded) Turing
machine which is able to submit chosen pairs of plaintexts to an oracle Ω, implementing with probability π0 = 12 a ﬁxed permutation C or, with probability
π1 = 12 , a permutation drawn uniformly at random from the set Cm of all permutations on m-bit blocks. Although the cryptanalytic settings are quite diﬀerent
(δdiﬀ can submit chosen pairs of plaintext), in a statistical point of view, the
distinguishing process is very similar to linear distinguishers. In Algorithm 2,
the classical modelization of a diﬀerential distinguisher [30] is given.
If we look at Algorithm 2, we note that, although the complexity n is given in
advance as input and is (implicitly) ﬁxed, the eﬀective number of queries to the
oracle is merely a random variable. In other words, δdiﬀ does not make use of all

24

P. Junod

the information that it could exploit. In fact, we can see the class of distinguishers
submitting a random number of queries to the oracle as a generalization of
the class of distinguishers submitting a ﬁxed number of queries. We will call
this generalization sequential distinguishers; this new concept is formalized and
studied in Section 5.
1:
2:
3:
4:
5:
6:
7:
8:
9:

Parameters: a complexity n, a characteristic (a, b)
Input: an oracle Ω which implements a permutation C
for i = 1 . . . n do
Pick uniformly at random x and query C(x) and C(x + a) to the oracle Ω.
if C(x + a) = C(x) + b then
Output 0 and stop.
end if
end for
Output 1.
Algorithm 2: Classical modelization of a diﬀerential distinguisher δdiﬀ .

In order to better understand the statistical decision processes, we give in Algorithm 3 an “unorthodox” modelization, denoted δdiﬀ , which is very similar to
the linear one. As for linear distinguishers, it is well-known [23] that diﬀerential

1:
2:
3:
4:
5:
6:
7:
8:
9:
10:
11:
12:
13:
14:

Parameters: a complexity n, a characteristic (a, b), an acceptance region A(n)
Input: an oracle Ω which implements a permutation C
Initialize a counter u to 0.
for i = 1 . . . n do
Pick uniformly at random x and query C(x) and C(x + a) to the oracle Ω.
if C(x + a) = C(x) + b then
Increment u
end if
end for
if u ∈ A(n) then
Output 0
else
Output 1
end if
Algorithm 3: Unorthodox modelization of a diﬀerential distinguisher δdiﬀ .

cryptanalysis depends on the quantity DPC (a, b) PrX [C(X + a) = C(X) + b],
where the plaintext space X is uniformly distributed. As this value depends on
the choice of the cipher (i.e. on the key), one deﬁnes EDP(a, b) E DPC (a, b) ,
where the expectation is taken over the permutation space. We note that Algorithm 2 outputs 1 if and only if no diﬀerential event occurs. As for linear
distinguishers, and considering this time Algorithm 3, one can deﬁne the optimal acceptance region using Lemma 1 and which is given by Lemma 3. As

On the Optimality of Linear, Diﬀerential, and Sequential Distinguishers

25

EDP(a, b) = 2m1−1 (where m is the block size of the permutation), and, typi1+
cally, DPC (a, b)
≤ 2m − 2, we can note that the optimal
2m −1 with 0 <
acceptance region will make δdiﬀ output 0 if
n
u

1+
2m − 1

u

1−

1+
2m − 1

n−u

n
u

≥

1
2m − 1

u

2m − 2
2m − 1

n−u

which gives the following result.
Lemma 3. The optimal acceptance region for δdiﬀ is
(n)

Aopt =

u ∈ {0, . . . , n} : u ≥ n ·

log(2m − 2) − log(2m − 2 − )
log((2m − 2)(1 + )) − log(2m − 2 − )
(14)

where u is deﬁned in Algorithm 3.
For small , (14) may be approximated by
(n)

Aopt ≈

u ∈ {0, . . . , n} : u ≥ n ·

2m

1
2m−1 − 1
+ m
·
− 1 (2 − 2)(2m − 1)

(15)

Thus, we have
Corollary 1. δdiﬀ is an optimal diﬀerential distinguisher submitting n queries
to the oracle if and only if (14) is satisﬁed for all u ∈ N with 1 < u ≤ n and for
all 0 < ≤ 2m − 2.
It is not diﬃcult to build artiﬁcially a situation where Algorithm 2 is not optimal:
it is suﬃcient to take a characteristic (a, b) with DPC (a, b) having a very high
probability. In this case, it is not suﬃcient for δdiﬀ to wait for only one diﬀerential
event and to stop, since if it is unique during the n samplings, it would have been
better to output 1. However, if we have a look at (15), we can note that Algorithm
2 captures well real-world situations, where exploited diﬀerential probabilities are
only slightly greater than ideal ones.
A very similar proof of Theorem 3 leads to
Theorem 6. For any distinguisher in the model of δdiﬀ ,
1−

n+1
1
≤ BestAdvnδdiff (C, C ∗ ) ≤ 1 −
2nν−1
(n + 1) · 2nν−1

(16)

where ν = C(D0 , D1 ) is the Chernoﬀ information between D0 , a binary distribution with PrX0 [X0 = 0] = 1 − PrX0 [X0 = 1] = DPC (a, b), and D1 , a binary
distribution with PrX1 [X1 = 0] = 1 − PrX0 [X1 = 1] = 2m1−1 .
Usually, in the context of diﬀerential cryptanalysis, one encounters the concept
of signal-to-noise ratio, which was used by Biham and Shamir in the papers
deﬁning the diﬀerential cryptanalysis [1,2,3]; it is deﬁned as being the ratio
of probability of the right (sub-)key being suggested by a right pair and the

26

P. Junod

probability of a random (sub-)key being suggested by a random pair, given the
initial diﬀerence. By empirical evidence, they suggested that when this ratio is
around 1-2, about 20-40 right pairs are suﬃcient for a successful attack; clearly,
this is a (implicitly deﬁned) likelihood-ratio test which turns out to be optimal
in terms of error probabilities.

5

Sequential Distinguishers

In this section, we formalize the concepts of generic sequential non-adaptive
distinguisher (GSNAD) and of n-limited generic sequential non-adaptive distinguisher (n-limited GSNAD). These kinds of distinguishers use sequential sampling procedures as their statistical core. We note that this idea was used earlier
by Davies and Murphy (see Appendix of [9]) in an attempt to decrease the
complexity of their attack against DES.
In the Luby-Rackoﬀ model [16], a non-adaptive attacker (which may be modelized by an n-limited GNAD as described in Algorithm 4) is an inﬁnitely powerful Turing machine which has access to an oracle Ω. It aims at distinguishing
a cipher C from the “Perfect Cipher” C ∗ by querying Ω, and with a limited
number n of inputs. The attacker must ﬁnally take a decision; usually, one is
interested in measuring the ability (i.e. the advantage as deﬁned in (6)) to distinguish C from C ∗ for a given, ﬁxed amount n of queries. Clearly, in this model,
one is interested in maximizing the advantage given a ﬁxed number of queries.
In a more “real-life” situation, a cryptanalyst proceeds usually in an inverse
manner: given a ﬁxed success probability (i.e. a given advantage), she may look
for minimizing the amount of queries to Ω, since such queries are typically expensive. With this model in head, we can now deﬁne a n-limited generic sequential
non-adaptive distinguisher (see Algorithm 5), which turns out to be more eﬃcient in terms of the average number of oracle queries than Algorithm 4 given a
ﬁxed advantage. In fact, such a distinguisher is adaptive regarding the decision
process.
After having received the i-th response from the oracle, the distinguisher
compare the i responses it has at disposal towards an acceptance set Ai and a
rejection set Bi , which depend on the number of queries and on the (ﬁxed in
advance) advantage, and can then take three diﬀerent decisions: either it decides

1:
2:
3:
4:
5:
6:
7:
8:
9:

Parameters: a complexity n, an acceptance set A.
Input: an oracle Ω implementing a permutation C
Compute some messages x = (x1 , . . . , xn ).
Query y = (C(x1 ), . . . , C(xn )) to Ω.
if y ∈ A then
Output 0
else
Output 1
end if
Algorithm 4: A n-limited generic non-adaptive distinguisher (GNAD)

On the Optimality of Linear, Diﬀerential, and Sequential Distinguishers

27

to output “0” or “1” and to stop, or to query one more question to the oracle
and to repeat the decision process, until it has queried n questions. Note that
Ai ⊆ Y i and Bi ⊆ Y i are disjoint sets for all 1 ≤ i ≤ n and that An ∪ Bn = Y n
(i.e. the distinguisher must stop and take a decision at step n). In statistics, this
process is known as a sequential decision procedure.
We note that Algorithm 2 can be viewed as a sequential diﬀerential distinguisher which does not take explicitely into account a decision region, since it
always outputs 0 as soon as it observes a “diﬀerential event”.
1: Parameters: a complexity n, acceptance sets Ai , 1 ≤ i ≤ n and rejection
sets Bi , 1 ≤ i ≤ n.
2: Input: an oracle Ω implementing a permutation C
3: i ← 1
4: repeat
5:
Select non-adaptively a message xi and get yi = C(xi ).
6:
if (y1 , . . . , yi ) ∈ Ai then
7:
Output 0 and stop.
8:
else if (y1 , . . . , yi ) ∈ Bi then
9:
Output 1 and stop.
10:
end if
11:
i←i+1
12: until i = n
Algorithm 5: A n-limited sequential generic non-adaptive distinguisher

5.1

Sequential Statistical Inference

We describe now formally the sequential decision procedure behind Algorithm 5.
Let D be the set of possible decisions.
Deﬁnition 2 (Sequential decision procedure). Let X1 , X2 , . . . be random
variables observed sequentially. A sequential decision procedure consists in:
1. a stopping rule σn which speciﬁes whether a decision must be taken without
taking any further observation. If at least one observation is taken, this rule
speciﬁes for every set of observed values (x1 , . . . , xn ), with n ≥ 1, whether to
stop sampling and take a decision out of D or to take another observation
xn+1 .
2. a decision rule δn which speciﬁes the decision to be taken. If n ≥ 1 observations have been taken, then one takes an action δn (x1 , . . . , xn ) ∈ D. Once a
decision has been taken, the sampling process is stopped.
If we consider Algorithm 5 at the light of this formalism, D = {0, 1},
δn (x1 , . . . , xn ) =
and

0 if (x1 , . . . , xn ) ∈ An
1 if (x1 , . . . , xn ) ∈ Bn

(17)

28

P. Junod

σn (x1 , . . . , xn ) =
5.2

continue sampling if (x1 , . . . , xn ) ∈ An ∪ Bn
stop sampling if
(x1 , . . . , xn ) ∈ An ∪ Bn

(18)

Sequential Decision Procedures

We have seen that Lemma 1 deﬁnes the shape of the optimal acceptance region
for binary hypothesis testing. Theoretically, if one is able to compute the exact
joint probability distribution of the oracle’s responses when it implements both
ciphers, one is able to compute the optimal acceptance region A for a generic
n-limited distinguisher.
A sequential likelihood-ratio test uses exactly the same process to deﬁne
two types of acceptance regions, denoted A and B, respectively. So, it is always
possible to deﬁne a sequential test when one has a classical test at disposal. In
few words, a sequential test has three alternatives once it has received a response
from the oracle: either it can conclude for one of both hypotheses, or it can decide
to query more samples. In its simpler deﬁnition, a sequential ratio test has the
possibility to query as many samples as it is needed to take a decision, given
a ﬁxed error probability. The expected number of queries required to reach one
of the two possible decision turns out to be less than it would need in order to
make the same decision on the basis of a single ﬁxed-size sample set. Of course
it may happen that the sequential procedure will take more queries than the
ﬁxed-size one, but sequential sampling is a deﬁnitely economical procedure.
One may deﬁne Algorithm 5, as a truncated sequential test, i.e. one ﬁxes an
upper-bound n on the number of queries; it is still clear that such a sequential
procedure cannot be worse than a ﬁxed-size sampling procedure. In the following,
we state some deﬁnitions and results about sequential hypothesis tests.
Deﬁnition 3 (Sequential Likelihood-Ratio Test). To test X ← D0 against
X ← D1 , deﬁne two constants τup > τdown > 0 depending on α and β, and deﬁne
the likelihood ratio
lr(x)

fX1 (x)
fX0 (x)

The decision function at i-th step is

 1 (i.e accept X ← D1 ) if lr(x(i) ) ≥ τup
δopt
0 (i.e. accept X ← D0 ) if lr(x(i) ) ≤ τdown

∅ query another sample otherwise

(19)

When the observations are independent and identically distributed, then sequential likelihood-ratio tests have the following nice property (we refer to [27] as an
excellent treatment of sequential procedures and for the proof of the following
three theorems):
Theorem 7. For testing a simple hypothesis against a simple alternative with
independent, identically distributed observations, a sequential probability ratio
test is optimal in the sense of minimizing the expected sample size among all
tests having no larger error probabilities.

On the Optimality of Linear, Diﬀerential, and Sequential Distinguishers

29

The following results relate error probabilities α and β to τup and τdown , and
give an approximation of the expected number of samples.
Theorem 8. Let be a sequential likelihood-ratio test with stopping bounds τup
and τdown , with τup > τdown and error probabilities 0 < α < 1 and 0 < β < 1,
β
then τdown ≥ 1−α
and τup ≤ 1−β
α .
β
1−β
The approximation τdown
1−α and τup
α is known as “Wald’s approximation”. The following theorem gives some credit to this approximation.

Theorem 9. Let us assume we select for given α, β ∈]0, 1[, where α+β ≤ 1, the
β
1−β
stopping bounds τdown
1−α and τup
α . Then it holds that the sequential
likelihood-ratio test with stopping bounds τdown and τup has error probabilities α
β
α
and β where α ≤ 1−β
, β ≤ 1−α
and α + β ≤ α + β.
By taking into account Wald’s approximation, we can compute approximations
of the expected number of queries:
EX0 [N ] ≈

α log

1−β
α

+ (1 − α) log

β
1−α

EX1 [log(fX0 (x)) − log(fX0 (x))]

(20)

and
EX1 [N ] ≈

5.3

(1 − β) log

1−β
α

+ β log

β
1−α

EX1 [log(fX1 (x)) − log(fX0 (x))]

(21)

A Toy-Example on DES

In order to illustrate advantages of sequential linear distinguishers, we have
implemented a linear cryptanalysis of DES reduced to ﬁve rounds which uses a
sequential distinguisher for deciding the parity of the linear approximation, i.e.
the parity of the sum of involved key bits.
Using a static test, we needed 2800 known plaintext-ciphertext pairs in order
to get a success probability of 97 %. Using a sequential strategy and for the same
success probability, only 1218 samples were necessary on average. We give here
both the static and the sequential decision rules.
Let Sn denote the number of times that Matsui’s best linear characteristic
[17] on 5-rounds DES evaluates to 0, where n is the number of known plaintextciphertext pairs at disposal. This linear approximation holds with probability
1
2 + 0.01907. The static decision rule is given by
Output “key parity = 0” if Sn ≥
Output “key parity = 1” if Sn <

n
2
n
2

(22)

With 2800 known pairs at disposal, the static rule is successful in 97% of the
cases.

30

P. Junod

1
For α = β
0.025, Wald’s approximation gives τup = 48 and τdown = 48
.
The sequential rule is then deﬁned by

log τup
n

1+2

 Output “key parity = 1” if Sn ≤ 2 − 2 log( 1−2
)
log τdown
n
(23)
Output “key parity = 0” if Sn ≥ 2 + 2 log 1−2

( 1+2 )


Query another sample, otherwise.

where = 0.01907. We repeated this experiment 1’000’000 times for 5 diﬀerent
keys and got the following results:
Exp. 1 Exp. 2 Exp. 3 Exp. 4 Exp. 5
Pr[ static distinguisher successful ]
0.9689 0.9687 0.9684 0.9686 0.9688
Pr[ sequential distinguisher successful ] 0.9686 0.9684 0.9683 0.9682 0.9684
Average number of queries
1218.7 1218.7 1218.3 1219.1 1218.8

6

Links to Other Statistical Attacks

Potential applications in cryptanalysis of sequential distinguishers are numerous.
As soon as one is able to derive underlying probability distributions, it is possible
to deﬁne likelihood-ratios, and thus to use a sequential distinguisher. However,
deriving even approximations of probability distributions may not be a trivial
task in certain cases.
Furthermore, even if one has the probability distributions in hand, one should
not neglect the amount of computations necessary to get the information which
will be fed into the likelihood-ratio.
Under the light of the hypothesis testing paradigm, several known statistical
attacks can be summarized (for which concerns their decisional part), and thus
potentially analyzed in a simple way. The χ2 statistical test, proposed in [29] for
the ﬁrst time and then used in many cryptanalytic contributions (e.g. see [12,
15,10,21]), is closely related to generalized likelihood-ratio tests.
Indeed, as outlined in Section §2, likelihood ratio tests are optimal for testing
a simple versus a simple hypothesis. It is possible to develop a generalization of
this test for use in situations in which the hypotheses are not simple (e.g. one
tests a probability distribution depending of a parameter θ ∈ ω0 against θ ∈ ω1
where ω0 and ω1 are disjoint subsets of possible parameters). Such tests are not
generally optimal, but they are typically non-optimal in situations for which no
optimal test exists, and they usually perform reasonably well.
It is well-known (see for instance [26]) that Pearson’s χ2 statistic and a generalized likelihood-ratio test for a multinomial distribution are asymptotically
equivalent. Thus, the underlying statistical decision processes in linear, diﬀerential, statistical, χ2 - and stochastic cryptanalysis are all equivalent in a statistical
point of view: they try to distinguish two diﬀerent (families of) probability distributions with help of a generalized likelihood-ratio test.
Another interesting attack is Harpes and Massey’s partitioning cryptanalysis [13]. In such an attack, one deﬁnes the imbalance of a random variable as

On the Optimality of Linear, Diﬀerential, and Sequential Distinguishers

31

being a non-uniformity measure, i.e. as measure of distance between a uniform
distribution and the distribution obtained through the partitioning process. In
[13], two diﬀerent imbalance measures are considered, namely the peak imbalance
and the squared Euclidean imbalance: one could consider a χ2 -value or, equivalently, a generalized likelihood-ratio value as well (and maybe slightly improve
its performances). Thus, the statistical problem behind this attack remains the
same.

7

Conclusion

In this paper, we have used the power of some tools proposed by the theory of
statistical tests for considering various situations in well-known cryptanalytic
attacks, like linear and diﬀerential cryptanalysis; we improve known bounds
on the asymptotical behavior of the best advantage of distinguishers implementing these attacks. Furthermore, we formalize the concept of “sequential
distinguisher” and we illustrate its potential power in a toy-example. Finally,
we discuss the application of the statistical tools in a couple of known attacks;
this suggests that statistical hypothesis testing theory may be a mean to unify,
to characterize and to analyze most of the known attacks against block ciphers.
Acknowledgments. The author would like to thank Serge Vaudenay for
many interesting and enlightning discussions.

References
1. E. Biham and A. Shamir, Diﬀerential cryptanalysis of DES-like cryptosystems
(extended abstract), Advances in Cryptology – CRYPTO’90, LNCS, vol. 537,
Springer-Verlag, 1990, pp. 2–21.
2.
, Diﬀerential cryptanalysis of DES-like cryptosystems, Journal of Cryptology 4 (1991), no. 1, 3–72.
3.
, Diﬀerential cryptanalysis of the Data Encryption Standard, SpringerVerlag, 1993.
4. C. Cachin, An information-theoretic model for steganography, Information Hiding,
2nd International Workshop, LNCS, vol. 1525, Springer-Verlag, 1998, pp. 306–318.
5.
, An information-theoretic model for steganography, Available on
http://eprint.iacr.org/2000/028/, 2000.
6. F. Chabaud and S. Vaudenay, Links between diﬀerential and linear cryptanalysis,
Advances in Cryptology - EUROCRYPT’94, LNCS, vol. 950, Springer-Verlag,
1995, pp. 356–365.
7. D. Coppersmith, S. Halevi, and C. Jutla, Cryptanalysis of stream ciphers with
linear masking, Advances in Cryptology - CRYPTO’02, LNCS, vol. 2442, SpringerVerlag, 2002, pp. 515–532.
8. T. M. Cover and J. A. Thomas, Information theory, Wiley Series in Telecommunications, Wiley, 1991.
9. D. Davies and S. Murphy, Pairs and triples of DES S-boxes, Journal of Cryptology
8 (1995), no. 1, 1–25.

32

P. Junod

10. H. Gilbert, H. Handschuh, A. Joux, and S. Vaudenay, A statistical attack on RC6,
Fast Software Encryption FSE’00, LNCS, vol. 1978, Springer-Verlag, 2000, pp. 65–
74.
11. G.R. Grimmett and D.R. Stirzaker, Probability and random processes, Oxford University Press, 2001, 3rd edition.
12. H. Handschuh and H. Gilbert, χ2 cryptanalysis of the SEAL encryption algorithm,
Fast Software Encryption FSE’97, LNCS, vol. 1267, Springer-Verlag, 1997, pp. 1–
12.
13. C. Harpes and J. Massey, Partitioning cryptanalysis, Fast Software Encryption
FSE’97, LNCS, vol. 1267, Springer-Verlag, 1997, pp. 13–27.
14. P. Junod, On the optimality of linear, diﬀerential and sequential distinguishers (full
version), Available on http://eprint.iacr.org and on
http://crypto.junod.info, 2003.
15. L. Knudsen and W. Meier, Correlations in RC6 with a reduced number of rounds,
Fast Software Encryption FSE’00, LNCS, vol. 1978, Springer-Verlag, 2000, pp. 94–
108.
16. M. Luby and C. Rackoﬀ, How to construct pseudorandom permutations from pseudorandom functions, SIAM Journal on Computing 17 (1988), no. 2, 373–386.
17. M. Matsui, Linear cryptanalysis method for DES cipher, Advances in Cryptology
– EUROCRYPT’93, LNCS, vol. 765, Springer-Verlag, 1993, pp. 386–397.
18.
, The ﬁrst experimental cryptanalysis of the Data Encryption Standard,
Advances in Cryptology – CRYPTO’94, LNCS, vol. 839, Springer-Verlag, 1994,
pp. 1–11.
19. U. Maurer, A uniﬁed and generalized treatment of authentication theory, Proc. 13th
Symp. on Theoretical Aspects of Computer Science (STACS’96), LNCS, vol. 1046,
Springer-Verlag, 1996, pp. 387–398.
20.
, Authentication theory and hypothesis testing, IEEE Transactions on Information Theory 46 (2000), no. 4, 1350–1356.
21. M. Minier and H. Gilbert, Stochastic cryptanalysis of Crypton, Fast Software Encryption FSE’00, LNCS, vol. 1978, Springer-Verlag, 2000, pp. 121–133.
22. S. Murphy, F. Piper, M. Walker, and P. Wild, Likelihood estimation for block
cipher keys, Technical report, Information Security Group, University of London,
England, 1995.
23. K. Nyberg, Perfect nonlinear S-boxes, Advances in Cryptology – EUROCRYPT’91, LNCS, vol. 547, Springer-Verlag, 1991, pp. 378–386.
24. National Bureau of Standards, Data Encryption Standard, U.S. Department of
Commerce, 1977.
25. T. Pornin, Optimal resistance against the Davies and Murphy attack, Advances in
Cryptology – ASIACRYPT’98, LNCS, vol. 1514, Springer-Verlag, 2000, pp. 148–
159.
26. J. A. Rice, Mathematical statistics and data analysis, Duxbury Press, 1995.
27. D. Siegmund, Sequential analysis – tests and conﬁdence intervals, Springer-Verlag,
1985.
28. S. Vaudenay, Decorrelation: a theory for block cipher security, to appear in the
Journal of Cryptology, Available on http://lasecwww.epfl.ch.
29.
, An experiment on DES statistical cryptanalysis, 3rd ACM Conference on
Computer and Communications Security, ACM Press, 1996, pp. 139–147.
30.
, Resistance against general iterated attacks, Advances in Cryptology – EUROCRYPT’99, LNCS, vol. 1592, Springer-Verlag, 1999, pp. 255–271.

