A Method of Hidden Markov Model
Optimization for Use with Geophysical Data
Sets
Robert A. Granat
Jet Propulsion Laboratory Pasadena CA 91109, USA

Abstract. Geophysics research has been faced with a growing need for
automated techniques with which to process large quantities of data. A
successful tool must meet a number of requirements: it should be consistent, require minimal parameter tuning, and produce scientiﬁcally meaningful results in reasonable time. We introduce a hidden Markov model
(HMM)-based method for analysis of geophysical data sets that attempts
to address these issues. Our method improves on standard HMM methods and is based on the systematic analysis of structural local maxima
of the HMM objective function. Preliminary results of the method as
applied to geodetic and seismic records are presented.

1

Introduction

In recent years, geophysics research has been faced with a growing need for
automated techniques by which to process the ever-increasing quantities of geophysical data being collected. Global positioning system (GPS) networks for
measurement of surface displacement are expanding, seismic sensor sensitivity
is increasing, synthetic aperture radar missions are planned to measure surface
changes worldwide, and increasingly complex simulations are producing vast
amounts of data. Automated techniques are necessary to assist in coping with
the deluge of information. These techniques are useful in a number of ways: they
can analyze quantities of data that would overwhelm human analysts, they can
ﬁnd subtle changes in the data that might evade a human expert, and they assist
in objective decision making in cases where even experts disagree (for example,
identifying aftershock sequences, or modes in GPS time series). These techniques
are not expected to replace human analysis, but rather to be tools for human
experts to use as part of the research cycle.
The ﬁeld of geophysics poses particular challenges for automated analysis.
The data is often noisy or of poor quality, due to the nature of the sensor equipment; for similar reasons it is also often sparse or incomplete. Furthermore, the
underlying system is unobservable, highly complex, and still poorly understood
by theory. Automated analysis is a useful tool only if it can satisfy several criteria. The results produced must be consistent across experiments on the same or
similar data. Only minimal parameter tuning can be required, lest the results be
P.M.A. Sloot et al. (Eds.): ICCS 2003, LNCS 2659, pp. 892−901, 2003.
 Springer-Verlag Berlin Heidelberg 2003

A Method of Hidden Markov Model Optimization

893

considered arbitrary. As well, the method must be computationally tractable, so
results can be returned to the user in reasonable time.
In this work, we investigate the use of hidden Markov models (HMMs) [1–5]
as the basis for an automated tool for analysis of geophysical data. We begin
by giving a brief overview of hidden Markov models and introducing our notation. We then present the standard method for solving for the optimal HMM
parameters and discuss the inherent local maxima issue associated with the
HMM optimization problem. In answer to this we introduce our modiﬁed robust
HMM optimization method and present some preliminary results produced by
this method.

2

Hidden Markov Models

A hidden Markov model (HMM) is a statistical model for ordered data. The
observed data is assumed to have been generated by a unobservable statistical
process of a particular form. This process is such that each observation is coincident with the system being in a particular state. Furthermore it is a ﬁrst order
Markov process: the next state is dependent only the current state. The model
is completely described by the initial state probabilities, the ﬁrst order Markov
chain state-to-state transition probabilities, and the probability distributions of
observable outputs associated with each state.

✎☞

✎☞ ✎☞
✎☞
✲ q2
✲ q3
♣ ♣ ♣ ✲ qT
✍✌ ✍✌ ✍✌
✍✌
♣♣♣
❄
❄
❄
❄
✎☞
✎☞
✎☞
✎☞
♣♣♣
O1
O2
O3
OT
✍✌ ✍✌ ✍✌
✍✌
q1

Partially observed Markov chain.
Fig. 1. A representation of the hidden Markov model, with hidden nodes in underlying
system states q, and observable variables O.

Our notation is as follows: a hidden Markov model λ with N states is composed of initial state probabilities π = (π1 , . . . πN ), state-to-state transition
probabilities A = (a11 , . . . , aij , . . . , aN N ), and the observable output probability
distributions B = (b1 , . . . , bN ). The observable outputs can be either discrete or
continuous. In the discrete case, the output probability distributions are denoted
by bi (m), where m is one of M discrete output symbols. In the continuous case,
the output probability distributions are denoted by bi (y, θi1 , . . . , θij , . . . , θiM )
where y is the real-valued observable output (scalar or vector) and the θij s are
the parameters describing the output probability distribution. For the normal
distribution we have bi (y, µi , Σi ).

894

R.A. Granat

3

Optimization Procedure for the HMM

For the series of observations O = O1 O2 · · · OT , we consider the possible model
state sequences Q = q1 q2 · · · qT to which this series of observations could be
assigned. The probability of O given the model is obtained by summing the
joint probability over all possible state sequences Q:
P (O|λ) =

πq1 bq1 (O1 )aq1 q2 bq2 (O2 ) · · · aqT −1 qT bqT (OT ).

(1)

all Q=q1 q2 ···qT

Although other optimization criteria are possible, most commonly we wish to
optimize the model parameters so as to maximize this likelihood P (O|λ). We can
pose this as non-convex, non-linear optimization problem with constraints on π,
A, and B that reﬂect the fact that they are probabilities. Often this problem is
presented as the equivalent problem of maximizing the log likelihood, log P (O|λ).
The most common optimization technique employed to solve this problem is
the Expectation-Maximization (EM) algorithm [6]. Brieﬂy, using this method a
likelihood P (λ) with model parameters λ is optimized by repeated calculation
and optimization of a so-called Q-function, which is deﬁned as the expectation
of the log of a certain underlying positive real-valued function p(x, λ).
This method is inherently sensitive to the initial conditions and only guarantees eventual convergence to a local maxima of the objective function, not the
global maximum. Nevertheless, it is widely used in practice and often achieves
good results.
For the hidden Markov model, we employ the EM method in following manner. We have the Q-function Q(λ, λ(k) ) which we wish to maximize over λ
at each iteration. We can view Q as the sum of three separable components,
Q = Q1 + Q2 + Q3 :
Q1 (λ, λ(k) ) =

N

(k)

τi1 log πi ,

(2)

i=1

Q2 (λ, λ(k) ) =

N

N T −1

(k)

τijt log aij ,

(3)

i=1 j=1 t=1

Q3 (λ, λ(k) ) =

N

T

(k)

τit log bi (Ot ).

(4)

i=1 t=1

Maximization of each component may be pursued separately. We have
(k) (k)

πi =

πi bi (O1 )
N
j=1

(k) (k)

πj bj (O1 )

,

(5)

as the maximizing solution for Q1 and
aij =

T −1 (k)
t=1 τijt
,
T −1 (k)
τ
t=1 it

(6)

A Method of Hidden Markov Model Optimization

895

as the maximizing solution for Q2 . If the outputs of the model are discrete, the
maximizing solution for Q3 is
bi (m) =

(k)
T
t=1 τit δ(Ot −
(k)
T
t=1 τit

m)

(7)

where m is a possible output symbol. If the outputs of the model are continuous, then there is no general explicit formula for the maximum value of
the output distribution parameters. However, for certain special forms of the
output distribution, the maximizing values can be calculated analytically. For
example, in the case of multivariate Gaussian output distributions (bi (y) =
n(det(Σi ))−1/2 exp(−(y − µi )T Σi−1 (y − µi )/2), where n is a normalizing factor),
we have
µi =

(k)
T
t=1 τit Ot
,
(k)
T
t=1 τit

(8)

and
Σi =

(k)
T
t=1 τit (Ot

(k+1)

− µi

(k+1) T

)(Ot − µi

(k)
T
t=1 τit

)

.

(9)

What remains is to calculate the probabilities τit and τijt . To do so, it is
possible to make use of the lattice structure of the HMM to perform an iterative calculation, known as the forward-backward procedure. For a more detailed
explanation of this procedure, see [7].

4

Multimodality of the HMM Objective Function

As noted, the EM algorithm only guarantees convergence to a local maximum.
Since the algorithm is deterministic, the initial model parameter selection controls which local maxima is eventually reached. In many cases, the EM algorithm
functions well; this is one reason for its popularity. However, the likelihood function of an HMM potentially has an exponential number of local maxima; this
makes the optimization problem much more diﬃcult.
Suppose we consider a particular data set, one composed of S distinct segments s, each starting at t1 (s) and ending at tT (s). For each segment the outputs
Os = Ot1 (s) · · · OtT (s) are all a single unique value, ms . For this data set, the
local maxima are solutions in which the possible output values for each state are
unique, so that if bi (m) = 0, then bj (m) = 0 for all i = j, and are contiguous in
the time sequence. More speciﬁcally, let the Nsi segments si (1), . . . , si (Nsi ) be
associated with the state i; that is, let bi (msi (k) ) = 0, k = 1, . . . , Nsi . Furthermore, let Lsi (k) be the length of the segment si (k). Then a locally maximum

896

R.A. Granat

model λ∗ is such that
πi∗ =

a∗ij

=

bi (m)∗ =











1 if O1 = msi (k) for some k
,
0 otherwise
Ns
i
k=1 Lsi (k) −1
Ns
i
k=1 Lsi (k)

0

1
Ns
i L
k=1 si (k)
Lsi (k)

Ns
i
k=1

0

Lsi (k)

if i = j
if tT (si (k)) + 1 = t1 (sj (l)) for some k, l ,
otherwise

if m = msi (k) for some msi (k)
otherwise

.

(10)

We ﬁrst present a simple illustrative example. Consider the sequence O =
112233 of length T = 6, on which we train a model of size N = 2. Consider

 


1
1/5 

1
01
λ1 = π =
,A =
, b1 =  0  , b2 =  2/5  ,
0
01


0
2/5

 


1
0


1
1/2 1/2
λ2 = π =
,A =
, b1 =  0  , b2 =  1/2  ,
0
0 1


0
1/2





2/3
0


1
2/3 1/3
λ3 = π =
,A =
, b1 =  1/3  , b2 =  1/3  .
0
0 1


0
2/3
Then P (O|λ1 ) = 0.00512, P (O|λ2 ) = 0.015625, P (O|λ3 ) = 0.01, so λ2 is a local
maximum. A second local maximum exists for which q1 · · · q4 = 1, q5 q6 = 2; a
third maximum is one for which the entire sequence is in the same state.
Now we present the general case and demonstrate that λ∗ of the form described in (10) is in fact a local maximum. For ease of notation, we assume
without loss of generality that t1 (si+1 (1)) > tT (si (1)), that is, the segment laNsi
bels increase monotonically with t. We furthermore deﬁne Li =
k=1 Lsi (k) .
Then we have
∗

P (O|λ ) =

L1 −1

L1 − 1
L1
LN − 1
LN

1
L1

LN −1

Ns1

·
k=1

···
Ls1 (k)
L1

Ls1 (k)

NsN

···
k=1

LsN (k)
LN

LsN (k)

(11)

Now consider a model λ which is slightly perturbed from λ∗ so that
Ls1 (k)
1
, b1 (ms1 (k) ) =
, k = 1, . . . , Ns1
L1 + 1
L1 + 1
Ls (1) − 1
Ls2 (k)
, b2 (ms2 (k) ) =
, k = 2, . . . , Ns2 ,
b2 (ms2 (1) ) = 2
L2 − 1
L2 − 1
b1 (ms2 (1) ) =

(12)

A Method of Hidden Markov Model Optimization

897

and
L1
1
, a12 =
,
(13)
L1 + 1
L1 + 1
L2 − 2
1
a22 =
, a23 =
.
(14)
L2 − 1
L2 − 1
In other words, this model λ corresponds to a state sequence Q such that qt = 1
for t = 1, . . . , L1 + 1. We have
a11 =

Ls2 (1) −1

P (O|λ) =
n=0

L2 − 2
L2 − 1
Ns1
k=1
Ns2
k=2

L1
L1 + 1

(L2 −1−n)

Ls1 (k)
L1 + 1

Ls2 (k)
L2 − 1

(L1 −1+n)

Ls1 (k)

Ls2 (k) Ns3
k=1

1
L2 − 1
1
L1 + 1
Ls3 (k)
L3

1
L1 + 1
···
n

LN − 1
LN

Ls2 (1) − 1
L2 − 1

Ls3 (k)

NsN

···
k=1

(LN −1)

·
Ls2 (1) −n

LsN (k)
LN

·
LsN (k)

, (15)

from which we can see that P (O|λ∗ ) > P (O|λ). A similar analysis follows for
the model λ perturbed from λ∗ corresponding to the state sequence Q such that
qt = 1 for t = 1, . . . , L1 −1. We can extend this analysis to all such models λ such
that A and B are perturbed in a like manner around the segment boundaries
from A∗ and B ∗ , so that P (O|λ∗ ) > P (O|λ). From this we can conclude that λ∗
is in fact a local maximum.
S−1
∗
We note that for S unique segments there are N
−1 local maxima λ of this
form utilizing all N states, since we choose N −1 of the S −1 possible transitions
between segments as our state transition points. We further note that this same
analysis holds true for all models for which less than the full number of states
N
are utilized. So in total there are n=1 S−1
n−1 local maxima for this data set and

N −1
model size N . If S ≥ N , then n=1 S−1
, so the lower bound on the
n−1 ≥ 2
number of local maxima is exponential in the model size.
An additional problem arises for certain forms of the output distribution
B. For these forms there are values of the parameters θim such that the likelihood achieves an unfavorable global maximum. By unfavorable, we mean that
these globally maximum model parameters are less informative about the values
of the hidden variables than models with merely local maxima. For example,
in the case of Gaussian output probability distributions, the likelihood goes
approaches inﬁnity as the eigenvalues of the variances approach zero. We can
N
D
D
identify n=1 N
d=1 d such unfavorable global maxima, where D is the
n
dimension of the observations, since the likelihood will approach inﬁnity if even
one eigenvalue of the variance of a single state approaches zero. This implies
that the number of such global maxima is exponential in both the number of
states and in the dimension of the observable data.
N

898

5

R.A. Granat

Q-Function Penalty Terms

The analysis of the previous section indicates that many ﬁxed points of the EM
transformation and sub-optimal local maxima are located in the model parameter space at predictable points where bi = bj . It would therefore appear to be
advantageous to augment to the standard optimization procedure so as to avoid
these parts of the parameter space. One way to do this is to add penalty terms
to the Q-function.
No general penalty term exists to assist in avoiding the condition where
bi = bj . However, for particular forms of the output distribution penalty terms
can be devised. For example, for discrete output distributions, we can add a
penalty term based on the inner product:
N

T

M

N

(k)

N

M

τit δ(Ot − m) log bi (m) − ωQ3

Q3 =
i=1 t=1 m=1

bi (m)bj (m) (16)
i=1 j=1 m=1

where ωQ3 > 0 is a small weighting factor. As a second example, we consider
the case of Gaussian output distributions. We add a penalty term based on the
squared Euclidean distance:
N

T

(k)

τit

Q3 =
i=1 t=1

log n −

1
1
log det(Σi ) − (mi − µi )T Σi−1 (mi − µi )
2
2

1
ωQ3
− (Ot − mi )T Σi−1 (Ot − mi ) +
2
2

N

(µi − µj )T (µi − µj ) . (17)
j=1

In both these cases conditions on the weighting terms ωQ3 can be found such that
the function Q3 remains concave and thus has a single local maxima. Computing the solution to either maximization problem requires an iterative procedure
with a computational cost per iteration which is cubic in the dimension of the
observations. As the basic HMM optimization method requires inversion of the
covariance matrices at each EM iteration, the modiﬁed method merely introduces a constant factor for bounded iterations in the inner loop. In practice,
solutions to Q3 can be found in very small (< 10) numbers of iterations.
We note in that these penalty terms do not help to escape from local maxima
when the model parameters are already at a point where bi = bj . Although
random initialization of the model parameters makes this unlikely, alternate
initialization methods can make this more problematic. In such cases, one way
to escape from the local maximum is to perturb the distributions by some small
amount when the case bi = bj is detected.
In the case of Gaussian output distributions we can impose an additional
penalty term in order to deal with unfavorable global maxima located where the
covariance matrices become singular. Our penalty term is based on the trace of
the inverse of the covariance matrix, since
Tr Σi−1 =

D
d=1

1
λid

(18)

A Method of Hidden Markov Model Optimization

899

where D is the dimension of the observations and λi1 , . . . , λiD are the eigenvalues
of the ith covariance matrix. The modiﬁed Q-function is
T

N

(k)

τit

Q3 =
t=1 i=1

log n +

1
1
log det(Σi−1 ) − (mi − µi )T Σi−1 (mi − µi )−
2
2
1
ωΣ
Tr Σi−1 Si −
Tr Σi−1 , (19)
2
2

where ωΣ is a weighting factor. This leads us to an optimum solution in which
we add a diagonal matrix ωΣ I to each covariance matrix.
Incorporating all of the above, our modiﬁed EM algorithm is then:
1. Start with k = 0 and pick a starting λ(k) .
2. Calculate Q (λ(k) , λ) (expectation step).
3. Maximize Q (λ(k) , λ) over λ (maximization step). This gives us the transformation F.
4. Set λ(k+1) = F(λ(k) ). If Q (λ(k+1) , λ) − Q (λ(k) , λ) is below some threshold,
stop. Otherwise, go to step 2.
5. Check to see if bi = bj for any i = j. If so, then perturb the current model
so that θi = θi + θ , and go to step 2. Otherwise, stop.

6

Experimental Results

We applied our robust HMM method to GPS and seismicity data collected in the
southern California region. In our implementation we assume Gaussian output
probability distributions for both FMM and HMM for simplicity and ease of
computation. Presented here are some preliminary experimental results.
The GPS data consists of surface displacement signals collected from a number of sites scattered around the southern California region. The data was three
dimensional, consisting of east-west displacement, north-south displacement, and
vertical displacement measurements, collected daily. Figure 2 shows a representative example of the results of the method applied to GPS data collected in the
city of Claremont, California. The method determined that a ﬁve state model
was optimal for this data set. Using a ﬁve state model, the HMM was able to
separate the data into distinct classes that correspond to physical events. These
classes are indicated in the ﬁgure by diﬀerent shades and vertical lines. There
is one instance of class 2 in the midst of class 3, corresponding to sharp northsouth and vertical movements at that time sample, but otherwise the classes
are sequential. The states before and after the Hector Mine quake of October
1999 are clearly separated, and distinct in turn from a period in 1998 in which
well ground water drainage caused displacement in the vertical direction. Sharp
movements in the north-south direction (as yet unattributed) were also isolated
as a separate class.
The seismicity data was taken from the Southern California Earthquake Center (SCEC) catalog. For this experiment, the original data set was processed to

900

R.A. Granat
CLAR hmm plot
east (cm disp)

10

Class 1
Class 2
Class 3
Class 4
Class 5

5
0
−5
1998

1998.5

1999

1999.5

2000

2000.5

north (cm disp)

2

Class 1
Class 2
Class 3
Class 4
Class 5

0
−2
−4
1998

1998.5

1999

1999.5

2000

2000.5

up (cm disp)

2

Class 1
Class 2
Class 3
Class 4
Class 5

0
−2
−4
1998

1998.5

1999

1999.5

2000

2000.5

float year

Fig. 2. HMM analysis results of global positioning system (GPS) relative displacement
data collected from a receiver located in Claremont, California. Classes associated with
diﬀerent regimes are indicated by line coloration and vertical indicator lines.

produce six components for each observed seismic event between January 1st,
1960 and December 31st, 1999: latitude, longitude, depth, magnitude, time to
next event, and time to previous event. Events of less than magnitude four were
removed. The method determined that a model with 17 states would be optimal
for this data sets. The data was grouped into scientiﬁcally meaningful classes,
including clusters of aftershocks for the Hector Mine, Landers, and Northridge
earthquakes, Transverse Range events, and swarm events in the Salten Sea area.
Furthermore, relationships between the classes as indicated by the transition
probabilities reveal evidence of scientiﬁcally meaningful phenomenon such as
stress waves. Figure 3 show examples of the classiﬁcations produced by the
method. Circles indicate the location of earthquakes; circle size corresponds to
magnitude. Lines represent the major faults.

7

Conclusions and Future Work

We have presented a tool for geophysical data analysis that is based around the
use of hidden Markov models (HMMs). The tool employs a method for estimating the optimal HMM parameters that is based on the analytical analysis
of certain local maxima of the HMM objective function that originate in the
model structure itself rather than the data. This analysis is then used to modify
the standard optimization procedure through the application of penalty functions which enable the solution to avoid many local maxima. This improves both
the quality and consistency of results. Preliminary experiments employing this
method in the analysis of geodetic and seismic record data have yielded results
which can be veriﬁed with respect to a priori scientiﬁc knowledge.
As part of our continued work on this method we are performing systematic
analysis of the eﬀect of our modiﬁed method on the quality and stability of
the optimizated solution. In addition we are applying the method to a diverse
assortment of geophysical data sets.

A Method of Hidden Markov Model Optimization
38

38

37

37

36

36

35

35

34

34

33

33

32
−122

−121

−120

−119

−118

−117

−116

−115

32
−122

38

38

37

37

36

36

35

35

34

34

33

33

32
−122

−121

−120

−119

−118

−117

−116

−115

32
−122

−121

−120

−119

−118

−117

−116

−115

−121

−120

−119

−118

−117

−116

−115

901

Fig. 3. HMM analysis result for SCEC catalog seismicity data. Upper left: the class of
Transverse Range events; upper right: the class of Hector Mine and Landers earthquake
aftershocks; bottom left: the class of Salten Sea swarm events; bottom right: the class
of Northridge earthquake aftershocks.

References
1. L. E. Baum. An inequality and associated maximization technique in statistical
estimation for probabilistic functions of markov processes. Inequalities, 3:1–8, 1972.
2. L. E. Baum and J. A. Egon. An inequality with applications to statistical estimation
for probabilistic functions of a markov process and to a model for ecology. Bull Amer
Meteorol Soc, 73:360–363, 1967.
3. L. E. Baum and T. Petric. Statistical inference for probabilistic functions of ﬁnite
state markov chains. Ann Math Stat, 37:1554–1563, 1966.
4. L. E. Baum, T. Petrie, G. Soules, and H. Weiss. A maximization technique occuring
in the statistical analysis of probabilistic functions of markov chains. Ann Math Soc,
41(1):164–171, 1970.
5. L. E. Baum and G. R. Sell. Growth functions for transformations on manifolds.
Pac J Math, 27(2):211–227, 1968.
6. A. P. Dempster, N. M. Laird, and D. B. Rubin. Maximum likelihood from incomplete
data via the em algorith. J Roy Stat Soc, 39(1):1–38, 1977.
7. L. R. Rabiner. A tutorial on hidden markov models and selected applications in
speech recognition. P IEEE, 77(2):257–286, 1989.

