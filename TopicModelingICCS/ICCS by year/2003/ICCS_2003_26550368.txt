Normalized Restricted Random Testing
Kwok Ping Chan1, , Tsong Yueh Chen2 , and Dave Towey1
1

Department of Computer Science and Information Systems
The University of Hong Kong, Pokfulam Road, Hong Kong
{kpchan, dptowey}@csis.hku.hk
2
School of Information Technology
Swinburne University of Technology, Hawthorn 3122, Australia
tychen@it.swin.edu.au

Abstract. Restricted Random Testing (RRT) is a new method of testing software that improves upon traditional random testing (RT) techniques. This paper presents new data in support of the eﬃciency of RRT,
and presents a variation of the algorithm, Normalized Restricted Random
Testing (NRRT). NRRT permits the tester to have better information
about the target exclusion rate (R) of RRT, the main control parameter
of the method. We examine the performance of the NRRT and Original RRT (ORRT) methods using simulations and experiments, and oﬀer
some guidance for their use in practice.

1

Introduction

It is widely recognised that software testing plays an important role in software
development. Exhaustive testing (the checking of all possible input combinations) is seldom feasible, and testers are usually only able to use a small portion
of a program’s input domain.
A common method of testing software is to draw test cases at random from
the input domain. This method, Random Testing (RT ), has been the subject
of much debate in the testing community, and, as Gutjahr [10] points out, few
topics in software testing methodology are more contentious than the question
of whether or not it is eﬃcient. Starting in the 70’s with Myers’, “probably the
poorest [testing] methodology is random input testing,” opinions swung fullcircle in the 80’s, when Duran and Ntafos [9] concluded that Random Testing
might even be more cost eﬀective than Partition Testing, a testing methodology that intuitively, should outperform Random Testing. Surprising as this was,
further evidence from Hamlet and Taylor served to conﬁrm the observation [12].
While Random Testing may well be surrounded in some controversy, what is
undeniable is that it does oﬀer some distinctively attractive features: In addition
to alleviating the overheads associated with partitioning, eﬃcient algorithms
exist to generate test cases [11], and reliability estimates and statistical analyses
All correspondence should be addressed to: K.P. Chan, Department of Computer Science and Information Systems, the University of Hong Kong. Email:
kpchan@csis.hku.hk
J.-P. Rosen and A. Strohmeier (Eds.): Ada-Europe 2003, LNCS 2655, pp. 368–381, 2003.
c Springer-Verlag Berlin Heidelberg 2003

Normalized Restricted Random Testing

369

are also easily calculated [16]. In many situations, it is recommended to apply RT
in the early stages of software development, since it is then that the program
being tested is more error-prone, and Random Testing may be most eﬀective
[14].
Although Random Testing consists of no more than repeatedly drawing test
cases randomly from a speciﬁed input range, it has been found that by requiring
the testing candidates to be more evenly distributed, and far separated from
each other, the program failure can be more eﬀectively identiﬁed under some
situations. This forms the basis of the Adaptive Random Testing (ART ) method
proposed by Chen et al. [6] (see also [7]), which has been shown to outperform
ordinary RT by as much as 50%. Restricted Random Testing (RRT ) [5] is an
alternative methodology which, although motivated by the same goals underlying
ART of guaranteeing a good spread of the test cases within the input domain
while maintaining low overheads, ensures the distribution through the use of
exclusion zones around non-failure causing test cases.
In this paper, we present additional experimental data for the RRT method,
and introduce a variation, Normalized Restricted Random Testing (NRRT ).
Although sharing the same fundamental algorithm, NRRT attempts to improve
upon the shortcomings of Ordinary RRT (hereafter referred to as ORRT ).
An analysis of the ORRT data revealed that the best error detection was
obtained when the target exclusion (R) was at a maximum. Unfortunately, except in the simplest cases, prior knowledge of the maximum R is not available.
NRRT , on the other hand, permits the tester to know in advance what the
maximum R should be, and therefore enables a faster and more accurate testing
process. It was also found that the shape of the input domain, in addition to
determining the maximum possible exclusion, also inﬂuenced the eﬀectiveness of
the exclusion zones. NRRT , by homogenizing the input domain and exclusion
zones, improves the eﬀectiveness of the RRT algorithm.
We next introduce the background of this study, and describe some of the
notation used in this paper. In Sect. 3, we describe a simulation and some experiments that were performed to investigate the eﬀectiveness of the RRT methods.
The results of these studies are discussed in Sect. 4, and some conclusions and
suggestions for when to use the methods are given.

2
2.1

Preliminaries
F-Measure

Traditionally, the eﬀectiveness of testing strategies have been measured according to the probability of detecting at least one failure (the P-measure), or by the
expected number of failures successfully detected (the E-measure). These measures however, are less than ideal, and Chen et al. [6] propose instead to use the
expected number of test cases required to ﬁnd the ﬁrst failure (the F-measure) as
a gauge of how eﬀective the method is. In support of this they point out that, in
practice, testing usually stops when a failure is found. Therefore, the F-measure
is not only intuitively more appealing than either the E- or P- measures, but

370

K.P. Chan, T.Y. Chen, and D. Towey

also is more realistic from a practical viewpoint. As with [5] the F-measure is
used to evaluate the testing strategies presented in this paper.
2.2

Background

When conducting testing using a random selection algorithm, aside from avoiding repetition of test cases (points located in the input domain and used to test
the software for failures), no account is taken of previous, successful test cases.
Random test cases are repeatedly generated and tested until one causes a failure.
Such a test case is referred to as a failure-causing input [8].
Following the notation of Chen and Yu [8], for an input domain D, we let d
denote the domain size, and m the size of the failure-causing input regions. The
failure rate, θ, is then deﬁned as m/d. For Random Testing with replacement of
test cases [13], the expected value for F is equal to 1/θ, or equivalently, d/m.
So, when using Random Testing on an input domain with a failure rate (θ) of
0.1%, we expect the F-measure to be 1/(0.1%), 1000.
2.3

Failure Patterns

Previous research has identiﬁed three major categories of failure patterns (input
points which cause failures), and reported on how they inﬂuence the performance
of some partition testing strategies [3]. The categories are: point, characterised
by individual or small groups of failure-causing input patterns; strip, characterised by a narrow strip of failure-causing inputs; and block , characterised by
the failure-causing inputs being concentrated in either one or a few regions.
Figures 1a–1c give examples of each of the categories (the outer boundaries
represent the borders of the input domain, and the shaded areas represent the
failure-causing regions).
Since detecting failure regions for programs in which the failure-causing proportion of the input space (θ) is large, is a relatively trivial task, and should
be fairly easily completed by any reasonable selection strategy, it is those cases
where θ is small that require more attention. Chen et al. [7] suggest that for
non-point patterns, by slightly modifying the basic random testing technique,

(a) Point Pattern

(b) Strip Pattern

(c) Block Pattern

Fig. 1. Types of Failure Patterns

Normalized Restricted Random Testing

371

and requiring that the testing candidates be more evenly distributed, and far
separated from each other, the failure detection capability can be signiﬁcantly
improved. This suggestion motivates the RRT methods.
2.4

Method

When testing according to the RRT method, given a test case that has not
revealed failure, rather than simply generate another test case randomly, we restrict the areas of the input domain from which subsequent test cases may be
drawn. To ensure a minimum distance between all test cases, a circular area
around non failure-causing inputs is deﬁned, and subsequent test cases are restricted to coming from outside of these regions.
Although the exclusion zone around each test case is the same for all inputs,
the area of each zone decreases with successive attempts. The size of the exclusion
zones is related to the size of the entire input domain. For example, in two
dimensions, with a target exclusion area of A, if there are n points around which
we wish to generate exclusion zones, then each exclusion zone area will be A/n,
and each exclusion zone radius will be A/(2nπ).
Because of the eﬀects of overlapping zones and of portions of the zones lying
outside the input domain, the actual coverage within the input domain may be
signiﬁcantly less than the target coverage. In this paper we refer to the target
coverage when discussing our methods and results.

3

Empirical Study

To investigate the eﬀectiveness of the RRT method, we conducted some empirical
studies. In the ﬁrst case, we simulated a failure region and, by varying the
target exclusion area (R), calculated how eﬀective RRT was. We also applied the
algorithm to several previously studied error-seeded programs [5,6], and found
the results to be very favourable. Finally, motivated by some of the shortcomings
of the method, we introduce an adapted form of the method and apply this to
the programs, again obtaining encouraging results.
3.1

Simulation

For certain types of failure patterns, in particular the non-point pattern types,
it has been suggested that the failure detection capabilities for RT can be improved by ensuring that the test cases are more evenly spread over the input
domain [6,7]. To test this, we conducted a simple simulation [5] in which a certain
percentage of a square input domain was speciﬁed to be a failure zone (a region
from which an input drawn would cause the tested program to fail). The failure
zone was circular in shape, and its size was set at 0.001 of the entire input area
(θ = 0.1%). Having advance knowledge as to what portion of the input domain
is failure-causing (θ) enables the expected F-measure by Random Testing to be
calculated (1/θ).

372

K.P. Chan, T.Y. Chen, and D. Towey

Fig. 2. Expected F-measures for Random Testing (RT ) compared with the calculated
F-measure, averaged over 10,000 trials, for Restricted Random Testing (RRT ). The
failure region (θ) is 0.1% of the entire input domain. The target exclusion (R) varies
from 10% to 150%

The target exclusion zone (R) was varied between 10% and 150%. (even with
a target exclusion of 150%, the actual exclusion is often still less than 100%.)
The experiments were repeated 10,000 times and the mean F-measure for each
target exclusion percentage was calculated. The results are reproduced in Fig. 2.
As can be seen from the ﬁgure, the experimentally measured F-measure for
RRT shows considerable improvement over the expected values for Random
Testing. Additionally, the failure-ﬁnding eﬃciency appears to improve as the
target exclusion (R) increases. These results support our hypothesis that detection capabilities of Random Testing can be improved by ensuring that the test
cases are more evenly spread over the input space.
3.2

Adaptive Random Testing

An alternative method of testing, also motivated by the goal of improving the
failure detection capabilities of ordinary Random Testing by forcing a more even
spread of the input cases, was proposed by Chen et al. [6]. They named this
method the Adaptive Random Testing (ART ) method, and it works as follows:
two sets of test cases are maintained: the executed set, a set of those test cases
which have been executed but without causing failure; and the candidate set,
a set of cases selected randomly from the input domain. Each time a new test
case is required, the element in the candidate set with the maximum minimum
distances from all the executed set elements is selected. Distance is measured as
the Euclidean distance. In the Fixed Size Candidate Set (FSCS ) version of the
method, the candidate set is maintained at a constant size, and is reconstructed
each time a test case is selected.

Normalized Restricted Random Testing

3.3

373

Error-Seeded Programs

In our study, we tested the RRT method against seven error-seeded programs
previously studied by Chan et al. [4]. These are published programs [1,15], all
involving numerical calculations, which were written in C (converted to C++
for our experiments) and which varied in length from 30 to 200 statements.
Using Mutation Analysis [2], errors in the form of simple mutants were seeded
into the diﬀerent programs. Four types of mutant operators were used to create
the faulty programs: arithmetic operator replacement (AOR); relational operator replacement (ROR); scalar variable replacement (SVR) and constant replacement (CR). These mutant operators were chosen since they generate the
most commonly occurring errors in numerical programs [4]. For each program,
after seeding in the errors, the range of each input variable was then set such
that the overall failure rate would not be too large [4]. Table 1 summarizes the
details of the error-seeded programs.
Table 1. Program name, dimension (D), input domain, seeded error types, and total
number of errors for each of the error-seeded programs. The error types are: arithmetic
operator replacement (AOR); relational operator replacement (ROR); scalar variable
replacement (SVR) and constant replacement (CR)
Program
Name
bessj
bessj0
cel

D
2
1
4

erfcc
gammq
plgndr
sncndn

1
2
3
2

3.4

Input Domain
Error Type
From
To
AOR ROR SVR
(2.0, −1000.0)
(300.0, 15000.0)
2
1
(−300000.0)
(300000.0)
2
1
1
(0.001, 0.001,
(1.0, 300.0,
1
1
0.001, 0.001)
10000.0, 1000.0)
(−300000.0)
(300000.0)
1
1
1
(0.0, 0.0)
(1700.0, 40.0)
3
(10.0, 0.0, 0.0) (500.0, 11.0, 1.0) 1
2
(−5000.0, −5000.0) (5000.0, 5000.0)
4

Total
CR Errors
1
4
1
5
1
3
1
1
2
1

4
4
5
5

ART Applied to Seeded Programs

The results of Chen et al.’s experiments [6] using the Fixed Size Candidate Set
(FSCS ) version of Adaptive Random Testing (ART ) method on the error-seeded
programs are given below (Fig. 3). In their version, the candidate set has a ﬁxed
size of 10 elements. The results are expressed in terms of their improvement over
the RT F-measure. For most of the programs, ART oﬀers signiﬁcant improvement over the calculated F-measure for Random Testing (RT ).
We next present the results for the Restricted Random Testing (RRT ) methods when applied to the same error-seeded programs. First, we used the original,
Ordinary RRT (ORRT ) method directly on the programs. We then applied an
adapted version of the algorithm, the Normalized RRT (NRRT ) method, which

374

K.P. Chan, T.Y. Chen, and D. Towey

Fig. 3. Improvement in F-measures for Adaptive Random Testing (ART ) compared to
Random Testing (RT ), when applied to the error-seeded programs. (Taken from Chen
et al. [6])

attempts to reduce some of the inﬂuence that the input domain shape has on
the eﬀectiveness of RRT .
3.5

ORRT Applied to Seeded Programs

In an earlier study [5], we converted seven of the original error-seeded programs
to C++ and applied the Ordinary Restricted Random Testing (ORRT ) method.
We varied the target exclusion ratio (R) between 10% and 220%, and obtained
an F-measure for each ratio by calculating the average over 5,000 iterations.
We continued these experiments allowing the target exclusion (R) to increase
to its maximum. (Maximum target exclusion is deﬁned as the highest target exclusion ratio at which every iteration (5,000) of the method revealed a failure.)
The value of the maximum R was found to vary for each program, sometimes,
as was the case for the cel program, quite dramatically (Table 2). Further investigation revealed that the maximum R was related to the shape of the input
domain.
A summary of the results of the comparison between ORRT and Random
Testing (RT ) is given in Figs. 4 and 5. The ﬁgures show the percentage improvement of ORRT over RT .
For most of the programs, as was the case with the simulation, there appears
to be an increase in the improvement over RT corresponding to the increase in
target exclusion area. The gammq program, although not displaying the relationship between improvement and target exclusion increase as strongly as the
other programs, does show overall improvement over RT . Only sncndn, as was
found in the ART results [6], does not show any signiﬁcant improvement.
When we conducted the experiments using ORRT over the target exclusion
(R) range 10% to 220%, the cel program appeared to perform badly [5]. However, when this range was signiﬁcantly extended (to 32,000%), it was found that

Normalized Restricted Random Testing

375

Table 2. Maximum Target Exclusion rates (R) for the error-seeded programs, using
Ordinary Restricted Random Testing (ORRT ). The results are averaged over 5,000
iterations
Program Maximum Target
Name
Exclusion (R)
bessj
220%
bessj0
100%
cel
32,000%
erfcc
100%
gammq
200%
plgndr
460%
sncndn
150%

Fig. 4. Comparison of the F-measures for Ordinary Restricted Random Testing
(ORRT ) when applied to the error-seeded programs. The target exclusion (R) varies
from 10% to 460%. The ﬁgures show the improvement of ORRT over RT

the program actually did respond favourably, and did display the characteristic
improvement related to the increasing target exclusion (R) (Fig. 5).
In almost all cases, the best improvement is obtained when the maximum
target exclusion (R) is used; this information is summarized in Table 3.
By varying the size and shape of the input domains for the programs, it
was discovered that these parameters aﬀect the value of the maximum target
exclusion. Since the ORRT results indicate that the best improvements over RT
are obtained when the target exclusion (R) is at a maximum, prior knowledge
of the maximum R would be very useful to testers using the RRT method.

376

K.P. Chan, T.Y. Chen, and D. Towey

Fig. 5. Improvement in the F-measures for Ordinary Restricted Random Testing
(ORRT ) compared with RT , when applied to the cel program. The target exclusion
(R) varies from 1,000% to 32,000%
Table 3. Program name, Max R, Improvement over RT at Max R, R for best improvement, and best improvement over RT for the error-seeded programs, using ORRT
Program Max Target Improvement over R for Best
Best
Name Exclusion (R)
RT at Max R
Improvement Improvement
bessj
220%
56.74%
220%
56.74%
bessj0
100%
43.03%
100%
43.03%
cel
32,000%
64.31%
32,000%
64.31%
erfcc
100%
46.24%
100%
46.24%
gammq
200%
12.76%
170%
13.94%
plgndr
460%
46.84%
450%
46.94%
sncndn
150%
1.05%
120%
3.01%

Therefore, we adjusted ORRT to incorporate a scaling feature enabling the
exclusion zones to be normalized to the input domain dimensions. We then
applied this Normalized RRT (NRRT ) method to the seeded programs.
3.6

NRRT Applied to Seeded Programs

We adapted the RRT method so that, instead of a uniform exclusion zone (circle,
sphere, etc.) around each non failure-causing test case, we moulded the exclusion
zone to the input domain. We deﬁned the exclusion zone initially to be within
a unit square/cube/hypercube, and then mapped the points to the actual input
domain. We refer to this version of RRT as the Normalized Restricted Random
Testing (NRRT ) Method.
The same error-seeded programs as above were tested to calculate the Fmeasure, and, as before, we varied the target exclusion ratio (R) and obtained
an F-measure for each ratio by calculating the average over 5,000 iterations.

Normalized Restricted Random Testing

377

Fig. 6. Comparison of the F-measures Normalized Restricted Random Testing (NRRT )
when applied to the seeded programs. The target exclusion (R) varies from 10% to
430%. The ﬁgure shows the improvement of NRRT over RT

Because the initial input domain has been homogenized, programs with the
same dimension were now expected to have similar maximum target exclusion
rates (R), diﬀering now only according to the size of the failure regions.
A summary of the results of the comparison between NRRT and Random
Testing is shown in Fig. 6. The ﬁgure shows the percentage improvement of
NRRT over RT . As before, for most of the programs there appears to be an
increase in the improvement over RT corresponding to the increase in target exclusion area. In fact, it is only the sncndn program that, again, shows no significant improvement over the Random Testing result. As expected, the maximum
target exclusion rates for programs with the same dimensionality are similar,
and the best results are again obtained when this maximum exclusion ratio is
used. These results are shown in Table 4.
A comparison between the best RRT and ART results is given in Fig. 7,
which compares ORRT , NRRT , and ART according to their improvement over
the calculated F-measure for Random Testing (RT ).
From the ﬁgure, it is can be seen that the RRT methods compare favourably
with the ART method, even for the sncndn program. In almost all cases, the
NRRT results are very similar to, or better than, the ORRT results. Only for
the bessj program does the NRRT method perform less well than the other
methods.
In the next section, we discuss these results, and some other details regarding
the implementation of RRT .

378

K.P. Chan, T.Y. Chen, and D. Towey

Table 4. Program name, dimension (D), Max R, Improvement over RT at Max R, R
for best improvement, and best improvement over RT for the error-seeded programs,
using NRRT
Program
Name
bessj
bessj0
cel
erfcc
gammq
plgndr
sncndn

D Max Target Improvement over R for Best
Best
Exclusion (R)
RT at Max R
Improvement Improvement
2
150%
18.00%
130%
18.83%
1
100%
42.29%
100%
42.29%
4
430%
79.37%
430%
79.37%
1
100%
46.67%
100%
46.67%
2
150%
22.17%
150%
22.17%
3
270%
79.55%
270%
79.55%
2
150%
2.25%
120%
3.27%

Fig. 7. Comparison of best improvement of F-measures for Ordinary Restricted Random Testing (ORRT ), Normalized Restricted Random Testing (NRRT ), and Adaptive
Random Testing (ART ), when applied to the seeded programs. All ﬁgures refer to the
improvement over the calculated RT

4
4.1

Discussion
Experiment Results

In the previous section, the results from the various experiments showed that the
RRT methods outperform RT , and compare favourably to ART when the target
exclusion area is at a maximum. With the exception of the sncndn program, all
the error-seeded programs showed an increase in the improvement over RT as
the target exclusion ratio increased for the RRT methods.
The cel program, which at ﬁrst seemed to perform so badly under ORRT ,
and later with a maximum target exclusion (R) of 32,000% showed excellent
improvement over RT (64.31%), displayed one of the features that motivated
the modiﬁcations to ORRT , which resulted in the NRRT method. The reason

Normalized Restricted Random Testing

379

for the extreme Max R was traced to the extremely inhomogeneous shape of the
input domain: cel takes input from a 4D domain whose sides are in proportion
1:300:1,000:10,000. Because ORRT makes use of a ﬁxed shape exclusion region,
for the cel program much of the exclusion zone fell outside the input domain.
When we homogenized the input domain by using the NRRT method, the improvement over RT actually increased to nearly 80%, and the maximum R was
reduced to 430%!
As previously explained [5], the poor response of the sncndn program to the
RRT methods is related to the failure pattern, which is characterised as the
point pattern type. According to our hypothesis, failure detection capabilities
for programs with this failure pattern type are not expected to improve when
using RRT .
With the exception of the bessj program, the results for NRRT are similar
to, or improve upon, the results for ORRT . NRRT was motivated by a desire
to improve the spread amongst test cases in the input domain, especially for
inhomogeneous domains. For those programs with homogeneous input domains
(sncndn, and the 1-D programs: bessj0 and erfcc), the performance for ORRT
and NRRT is the same.
The bessj program, although showing excellent improvement over RT with
the ORRT method, had a poorer performance under NRRT . The reason for this
appears to be connected to its failure pattern, which is best described as a narrow
block type, or an incomplete strip type. This is currently being investigated
further.
4.2

Complexity and Overheads

The main overhead in the RRT method, common to both ORRT and NRRT , is
associated with the generation of a valid test candidate. It is necessary to ﬁrst
generate a random point, and then check if this point lies within any exclusion
zone. If it does lie in any of the zones, then it is discarded, and another random
point is generated. The process of generating and discarding random points
continues until a point lying outside the exclusion zones is produced. The number
of attempts to generate a valid test candidate goes up as the target exclusion
ratio (R) increases.
4.3

Summary and Conclusion

Previously, a new method of testing software, Restricted Random Testing
(RRT ), was introduced [5]. In this paper, additional data for this method, and
a new adaptation, Normalized Restricted Random Testing (NRRT ), were presented. The NRRT algorithm improves on the Ordinary Restricted Random
Testing (ORRT ) version by adapting the exclusion zones to better suit inhomogeneous domains, and thus more easily enabling a better spread of the test
candidates.
The RRT methods have been shown to compare favourably with both RT ,
and ART , another testing method previously introduced in the literature [6,

380

K.P. Chan, T.Y. Chen, and D. Towey

7]. Preliminary research has indicated a correlation between the type of failure
pattern and which RRT version (ORRT or NRRT ) performs best. The overheads
of the method are related to the generation of the test candidate. When the
execution overheads of testing a program exceed those of the test candidate
generation, then the RRT methods oﬀer considerable improvement over Basic
Random Testing (RT ).
Acknowledgement. We should like to gratefully acknowledge the support
given to this project by the Research Grants Council of the Hong Kong Special Administrative Region, China (Project No: HKU 7007/99E).

References
1. Association for Computer Machinery, Collected Algorithms from ACM, Vol. I, II,
III , Association for Computer Machinery, 1980.
2. T.A. Budd, “Mutation Analysis: Ideas, Examples, Problems and Prospects”, Computer Program Testing, B. Chandrasekaran and S. Radicci (eds.), North-Holland,
Amsterdam, pp. 129–148, 1981.
3. F.T. Chan, T.Y. Chen, I.K. Mak and Y.T. Yu, “Proportional Sampling Strategy:
Guidelines for Software Testing Practitioners”, Information and Software Technology, Vol. 28, No. 12, pp. 775–782, December 1996.
4. F.T. Chan, I.K. Mak, T.Y. Chen and S.M. Shen, “On the Eﬀectiveness of the Optimally Reﬁned Proportional Sampling Testing Strategy”, Proceedings of the 9th
International Workshop on Software Technology and Engineering Practice (STEP
’99), S. Tilley and J. Verner (eds.), IEEE Computer Society, Los Alamitos, California, pp. 95–104, 1999.
5. K.P. Chan, T.Y. Chen and D. Towey, “Restricted Random Testing”, Software
Quality - ECSQ 2002, 7th European Conference, Helsinki, Finland, June 9–13,
2002, Proceedings. LNCS 2349, Springer-Verlag, Berlin Heidelberg 2002
6. T.Y. Chen, H. Leung and I.K. Mak, “Adaptive Random Testing”, submitted for
publication.
7. T.Y. Chen, T.H. Tse, and Y.T. Yu, “Proportional Sampling Strategy: A Compendium and Some Insights”, The Journal of Systems and Software, 58, pp. 65–81,
2001.
8. T.Y. Chen and Y.T. Yu, “On the Relationship Between Partition and Random
Testing”, IEEE Transactions on Software Engineering, Vol. 20, No. 12, pp. 977–
980, December 1994.
9. J.W. Duran and S.C. Ntafos, “An Evaluation of Random Testing”, IEEE Transactions on Software Engineering, Vol. 10, No. 4, pp. 438–444, July 1984.
10. W.J. Gutjahr, “Partition Testing vs. Random Testing: The inﬂuence of Uncertainty”, IEEE Transactions on Software Engineering, Vol. 25, No. 5, pp. 661–674,
September/October 1999.
11. R. Hamlet, “Random Testing”, Encyclopedia of Software Engineering, edited by
J. Marciniak, Wiley, pp. 970–978, 1984.
12. R. Hamlet and R. Taylor, “Partition Testing Does Not Inspire Conﬁdence”, IEEE
Transactions on Software Engineering, Vol. 16, No. 12, pp. 1402–1411, December
1990.

Normalized Restricted Random Testing

381

13. H. Leung, T.H. Tse, F.T. Chan and T.Y. Chen, “Test Case Selection with and
without Replacement”, Information Sciences, 129 (1–4), pp. 81–103, 2000.
14. P.S. Loo and W.K. Tsai, “Random Testing Revisited”, Information and Software
Technology, Vol. 30, No. 9, pp. 402–417, September 1988.
15. W.H. Press, B.P. Flannery, S.A. Teulolsky and W.T. Vetterling, Numerical Recipes,
Cambridge University Press, 1986.
16. M.Z. Tsoukalas, J.W. Duran, and S.C. Ntafos, “On some Reliability Estimation
Problems in Random and Partition Testing”, IEEE Transactions on Software Engineering, Vol. 19, No. 7, pp. 687–697, July 1993.

