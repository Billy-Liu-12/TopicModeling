Comparative Analysis of Stochastic
Identiﬁcation Methods and Fault Diagnosis for
Multi-mode Discrete Systems
Olga Fatyanova and Alexey Kondratiev
Ulyanovsk State University, 42 Leo Tolstoy Str., 432970 Ulyanovsk, Russia
folga@nm.ru,mouse fin@mail.ru

Abstract. This paper oﬀers the comparative analysis of stochastic approximation methods applied for multi-mode discrete systems identiﬁcation from incomplete noisy observations. The main item discussed here
concerns the rate of convergence of these methods and several aspects
that are considered to aﬀect this property. To corroborate the theoretical
arguments, the experimental results are provided.

1

Introduction

A multi-mode system is understood as a system with changeable coeﬃcients that
compounds the vector parameter θ. The mode change means that parameter θ
switches, say, from value θ1 to value θ2 , taken from a certain compact set of
available parameters Θ. It is assumed that system is stable if and only if the
parameter θ ∈ Θ. The only data can be used is contained into incomplete noisy
observations. So, one can consider two problems to be solved: the problem of
mode switching diagnosis, or so-called fault diagnosis, and the problem of identiﬁcation of the new value θ2 of parameter θ. We insist on the joint performance
of these tasks, using the method of Adaptive Model (AM) and proposing Startand-Stop Algorithm (SSA), which we. Every time the fault is detected, the SSA
launches the identiﬁcation, which adjusts the AM in order to suit the certain
quality requirements. When the identiﬁcation reaches its goal and demanded
agreement with observation is provided, the SSA stops the process. In our research the requirements, mentioned above, are represented in the form of the
Auxiliary Functional of Quality (AFQ), compared with certain threshold value.
In this article only the identiﬁcation is discussed, while the description of the
SSA can be found in [7].
The outline of the paper is as follows: Section 2 describes the Monitored
System (MS) and Kalman ﬁlter. In Section 3 we build the AM and the AFQ.
Section 4 shows some identiﬁcation algorithms and certain experimental results
are contained in Section 5. Finally, Section 6 concludes the paper.
This work was supported in part by the Russian Ministry of Education (grant
No. T02-03.2-3427).
P.M.A. Sloot et al. (Eds.): ICCS 2003, LNCS 2658, pp. 446–455, 2003.
c Springer-Verlag Berlin Heidelberg 2003

Comparative Analysis of Stochastic Identiﬁcation Methods

2

447

Monitored System

In the frames of this paper we consider MS without control input with an uncertainty parameter θ, θ ∈ IRN . First of all, MS includes the object with state
equation
x(ti+1 ) = Φθ x(ti ) + Γ w(ti ),

i = 0, 1, . . .

(1)

with x ∈ IRn , Φθ ∈ IRn×n , Γ ∈ IRn×q and a noise w(ti ) where the random initial
x(t0 ) at some t0 has a ﬁnite mean x
¯0 and a ﬁnite covariance P0 ≥ 0.
The other part of MS is a sensor with the equation
z(ti ) = Hx(ti ) + v(ti ),

i = 1, 2, . . .

(2)

where H ∈ IRm×n , rank(H) < n (incomplete observations) and a noise v(ti ).
Both {w(ti )} and {v(ti )} are i.i.d. zero mean mutually independent wide-sense
stationary sequences whose covariances are E{w(ti )w(ti )T } = Qθ ≥ 0 and
E{v(ti )v(ti )T } = Rθ > 0 for all ti .
The derivation of unbiased estimate x
˜(ti ) for state x(ti ) is considered to be
the main problem of the project. Assume that the value θ0 of parameter θ is
known in advance and for certain period it remains permanent. Hence in this
situation we have ﬁxed (fault-free) system mode speciﬁed by a nominal value θ0
of θ and, correspondingly, the steady-state variant of estimation task. This fact
allows us to use the classical approach consisting in the use of Kalman ﬁlter to
tackle our main problem under these simpliﬁed conditions.
The standard Kalman ﬁlter equations are as follows.
Initial values:
P˜ (t+
0 ) = P0 ,

x
˜(t+
¯0
0)=x

(3)

Step 1: time propagation, i = 0, 1, . . .
x
˜(t−
˜(t+
i+1 ) = Φθ x
i );
−
T
T
P˜ (ti+1 ) = Φθ P˜ (t+
i )Φθ + Γ Qθ Γ .

(4)
(5)

Step 2: measurement update, i = 1, 2, . . .
−1
T
˜ − T
K(ti ) = P˜ (t−
;
i )H [H P (ti )H + Rθ ]
−
−
x
˜(t+
)
=
x
˜
(t
)
+
K(t
)[z(t
)
−
H
x
˜
(t
)];
i
i
i
i
i
˜ (t− ) − K(ti )H P˜ (t− ).
P˜ (t+
)
=
P
i
i
i

(6)
(7)
(8)

The MS (1), (2) is assumed to be asymptotically stable in all modes of operating referred to by the subscript θ, and so all the processes within the system
are wide-sense stationary at every ti as t0 → −∞ (the main assumption). The
system is designed to hold the main properties in all modes:
1/2

(Φθ , Γ Qθ Γ T ) is stabilizable and (Φθ , H) is completely observable.

448

O. Fatyanova and A. Kondratiev

These properties guarantee the existence of the optimal steady-state parameters
for a ﬁxed (fault-free) system mode
¯x(t+ ),
x
˜(t−
i+1 ) = Φ˜
i

¯
x
˜(t+
˜(t−
˜(t−
i )=x
i ) + K[z(ti ) − H x
i )]

¯
¯
with some ﬁxed x
˜(t+
0 ), constant transition matrix Φ and constant gain K designed to be optimal in a steady state (as t0 → −∞).
Unfortunately, because of uncertainty classical theory can not be used as it
is. In this work we consider two levels of uncertainty.
Case 1: unknown Qθ , Rθ . The estimate of Kalman gain K we denote as D.
Here vector θ consists of all components of vector K.
Case 2: unknown Qθ , Rθ , Φθ . As in the previous case, we consider D to be
the estimate of K, and let A to be the estimate of Φθ . Here vector θ consists of
all components of vector K and unknown components of matrix Φθ .

3

Adaptive Model

The problem concerning the derivation of unbias estimate for Φθ in the AM,
based on the Kalman ﬁlter principle, has a solution only for the case of complete
observations [3]. In this work we consider the possibility of evading the strict
limitations and accomplishing the task in the case of incomplete observations.
We build AM for MS and Kalman ﬁlter with adjustable parameter θˆ considered to be the estimate of the corresponding MS parameter θ. The property
of estimate unbiasedness can be provided only by the optimal choice of the AM
structure. In the case of noises with zero mean and gaussian distribution it is
the structure of Kalman ﬁlter [3]. Let us write the equations of AM:
x
ˆ(t−
x(t+
i ) = Aˆ
i−1 ),
zˆ(ti ) =

(9)

Hx
ˆ(t−
i ),

(10)

x
ˆ(t−
i )

(11)
(12)

r(ti ) = z(ti ) − zˆ(ti ) = z(ti ) − H x
ˆ(t−
i ),
x
ˆ(t+
i )

=

+ Dr(ti ).

where x
ˆ(t±
i ) is the estimate of x(ti ), r(ti ) is a diﬀerence (residual) between the
observation z(ti ) (immediately obtained from the sensor) and the observation
estimate zˆ(ti ) (obtained form the AM).
Hence, the AM allows us to derive the unbiased estimate x
ˆ(t±
i ) for x(ti ). Let
us consider the initial functional of quality to be built on the base of diﬀerence
ˆ(ti ). As to [4], error-squared functional of quality is the optimal
e(ti ) = x(ti ) − x
one for the system with noises having gaussian distribution and zero mean:
Je (ti ) =

1
E[eT (ti )e(ti )],
2

i = 0, 1, . . .

(13)

While constructing the AFQ, we must take into account the important condition
[3]:

Comparative Analysis of Stochastic Identiﬁcation Methods

449

the components of argument θˆ∗ of AFQ minimization must coincide with
corresponding actual values of the entire system parameter (unknown coefﬁcients of Φθ in the second case of uncertainty) and optimal value of steady¯
state ﬁlter K.
In practice the value of x(ti ) is unknown, and our goal is to be ﬁnd it, therefore
AFQ should be built up in another way. Usually the diﬀerence r(ti ) is used for
this purpose. So, we take the AFQ in the following form [2]:
Jε (ti ) =

1
E[εT (ti )ε(ti )],
2

i = 0, 1, . . .

(14)

where ε(ti ) is the history of residuals r(ti ), transformed in a special manner to
provide
Jε (ti ) = Je (ti ) + const,

i = 0, 1, . . .

(15)

Without coming into details of ε(ti ) construction, we ﬁnd it necessary to mention
here, that the truth of this condition is provided by the known theorem for the
general case [1]. We have ﬁxed the system dimension n = 2 and checked it for
this particular case. While proving the fact the constant mentioned above has
been determined: const = Rθ .
As one can see from (15), the argument of the minimization θ∗ of Je coincides
with θˆ∗ . Hence we will take Jε as the AFQ, because it is available for practical
utilization in the numerical algorithms of identiﬁcation. Under this point of view
the process of identiﬁcation is evident to turn out the process of optimization
or, in other words, of AFQ minimization. The AM is optimal iﬀ (if and only if)
∇θˆJε (ti ) = εT (ti )S(ti ) = 0

(16)

where ∇θˆJε (ti ) ∈ IR1×N , the matrix S(ti ) = ∇θˆε(ti ) is the sensibility matrix [7].
In order to write down ε in an explicit form the dimension n is to be ﬁxed,
and below we will operate on 2-order systems in standard observable form:
x(ti+1 ) =

0
f1

1
0
x(ti ) +
w(ti ),
f2
α

z(ti ) = 1

0 x(ti ) + w(ti ).

Since Qθ , Rθ and Φθ are unknown according to list of uncertainty cases, K and
Φθ are to be estimated as D and A correspondingly, and almost sure convergence
is to be demanded:
D=

k
d1
→K= 1 ,
d2
k2

A=

0
a1

1
0
→ Φθ =
f1
a2

Now let us write down the equation for ε for n = 2:
ε(ti ) = N (D)H =

r(ti−1 )
ε (t )
= 1 i .
d2 r(ti−1 ) + r(ti )
ε2 (ti )

N (D) =

1
d2

0
,
1

H=

r(ti−1 )
.
r(ti )

1
.
f2

450

O. Fatyanova and A. Kondratiev

To obtain the gradient of AFQ (for the ﬁrst case of uncertainty) the sensibility
matrix should be calculated.
SD (ti )T =

∂ε1 (ti )
∂d1
∂ε1 (ti )
∂d2

∂ε2 (ti )
∂d1
∂ε2 (ti )
∂d2

=

∂r(ti−1 )
∂d1
∂r(ti−1 )
∂d2

i−1 )
i)
d2 ∂r(t
+ ∂r(t
∂d1
∂d1
∂r(ti−1 )
r(ti−1 ) + d2 ∂d2 +

∂r(ti )
∂d2

For this purpose we diﬀerentiate componentwise the equations of AM by D,
taking in account(4), (5), (6), (7), (8).
∂x
ˆ(t+
∂x
ˆ(t−
∂x
ˆ(t−
∂r(ti )
i−1 )
i )
i )
=− 1 0
,
=A
,
∂d1
∂d1
∂d1
∂d1
∂r(ti )
∂x
ˆ(t−
∂x
ˆ(t+
1
i )
i )
+
r(ti ) + D
.
=
0
∂d1
∂d1
∂d1
∂x
ˆ(t+
∂r(ti )
∂x
ˆ(t−
∂x
ˆ(t−
i−1 )
i )
i )
=A
,
=− 1 0
,
∂d2
∂d2
∂d2
∂d2
∂x
ˆ(t−
∂r(ti )
∂x
ˆ(t+
0
i )
i )
=
+
r(ti ) + D
.
1
∂d2
∂d2
∂d2
∂x
ˆ(t− )

∂x
ˆ(t+ )

∂x
ˆ(t− )

∂x
ˆ(t+ )

∂x
ˆ(t+
0)
= 0,
∂d1

∂x
ˆ(t+
0)
= 0.
∂d2

∂r(ti )
i)
i
i
i
Hence, ∂d1i , ∂r(t
are calculated iteratively and
∂d1 , ∂d1 , ∂d2 , ∂d2 , ∂d2
then substituted into the SD (ti )T matrix. The initial values is necessary to set:

For second case of uncertainty the sensibility model is designed analogically.

4

Stochastic Approximation

In this paper the process of identiﬁcation is performed by discrete searchless
algorithms, which are also called the methods of stochastic approximation and
represented below in the general recurrent form [5]:
ˆ j−1 ] − Λ[tj ]S T [tj−1 ]ε[tj−1 ].
ˆ j ] = θ[t
θ[t

(17)

The choice of Λ[tj ] aﬀects the rate of convergence greatly. The suﬃcient conditions of algorithm convergence can be found in [5], [6]. The identiﬁcation method,
corresponding diagonal form of Λ[tj ] is discussed in 4.3. The case of equal diagonal components λ[tj ] see below in 4.1.
4.1

Algorithm 1. Robbins-Monro Algorithm

Let us take λ[tj ] = 1j . By doing so we get the algorithm representing the stochastic analogue of gradient method. This method is also called the Robbins-Monro
multidimensional procedure of stochastic approximation. [6], [7]:
ˆ j ] − λ[tj+1 ]S T [tj ]ε[tj ],
ˆ j+1 ] = θ[t
θ[t

λ[tj+1 ] =

1
,
j+1

j = 1, 2, . . .

(18)

Comparative Analysis of Stochastic Identiﬁcation Methods

4.2

451

Algorithm 2. Optimal Algorithm

The information about the class of density of distribution the noises belong is
of primary importance, because it helps to make an optimal choice of matrix
Λ. The algorithm, corresponding to this optimal choice provides the best rate
ˆ j ] to θˆ∗ among all methods [4]. So, for the case of noises
of convergence of θ[t
having gaussian distribution with zero mean and unknown covariances leastsquares method (LSM) is the optimal one. To derive LSM the general form of
identiﬁcation algorithm is to be written down:
ˆ j+1 ] = θ[t
ˆ j ] − Λ[tj+1 ]S T [tj ]ε[tj ], j = 1, 2, . . .
θ[t
(19)
where Λ[tj ] ∈ IRN ×N . In LSM Λ[tj ] is determined by the formula [4]:
Λ[tj+1 ] = Λ[tj ] − Λ−1 [tj ]S T [tj ](S[tj ]Λ[tj ]S T [tj ])−1 S[tj ]Λ[tj ].
The equation of approximation written down through the increment is
ˆ j+1 ] = θ[t
ˆ j ] + ∆θ[t
ˆ j ], j = 1, 2, . . .
θ[t
We denote P [tj ] = Λ
–
–
–
–

−1

(20)

(21)

[tj ] and from (19), (20), (21) obtain the LSM:

set the initial value P [t1 ] = I;
calculate P [tj+1 ] = P [tj ] + S[tj ]T S[tj ];
ˆ j ] from the system of linear equations P [tj+1 ]∆θ[t
ˆ j ] = −S T [tj ]ε[tj ]
ﬁnd ∆θ[t
ˆ
ˆ
ˆ
calculate new value of parameter θ[tj+1 ] = θ[tj ] + ∆θ[tj ]

The disadvantage of the LSM is that for each step the solution of the system
of linear equations is sought. This operation is quite laborious to perform, so
it is worth to modify the method in order to avoid it. Certainly the newly
obtained algorithm is not optimal in the strict sense, that is why we call it
suboptimal, nevertheless its employment let us to cut down computational costs
in comparison with the LSM. In the later section suboptimal method is discussed
in details.
4.3

Algorithm 3. Suboptimal Algorithm

So-called suboptimal algorithm is developed on the base of optimal one by transforming the S T [tj ]S[tj ] matrix to diagonal form. Only diagonal elements of matrix product S T [tj ]S[tj ] remain unchanged, the rest are substituted for zeros:

 ∂ε[t ] 2
j
···
0
∂θ1


..
..

(22)
S T [tj ]S[tj ] ≈ 
.
.

 ...
0

···

∂ε[tj ] 2
∂θN

Thus, we have got the suboptimal algorithm:
– set the initial value P [t1 ] = I;
∂ε[t ] 2
– calculate diagonal elements of matrix P , pii [tj+1 ] = pii [tj ] + ∂θij , for
i = 1, 2, . . . , N
S T [t ]ε[t ]
– calculate new value of parameter θˆi [tj+1 ] = θˆi [tj ] − pii [tjj+1 j] , i = 1, 2, . . . , N

452

4.4

O. Fatyanova and A. Kondratiev

Stability Requirements. Jury’s Criteria

From the condition of AM stability ρ[(I −DH)A] < 1 the characteristic equation
can be obtained [2]:
q(λ) = b0 λ2 + b1 λ + b2 = 0

(23)

where b0 = 1, b1 = d2 − a2, b2 = a1 (d1 − 1). The stability condition according
to Jury criteria may be written as [2]:
q(1) > 0,

(−1)N q(−1) > 0,

b0 > |bN |.

(24)

In particular case of 2-order system these conditions transform to the next ones:
1 + b1 + b2 > 0,

10 − b1 + b2 > 0,

1 > |b2 |.

(25)

Hence, we have got an opportunity to determine the stability of a system at
every step of approximation process and to work out the heuristic algorithm,
that would be capable of preventing the AM crash.
ˆ j+1 ) within the set
The main idea of this method is to keep the parameter θ(t
of stability Θ. Therefore we have designed the algorithm reasoning from the idea
that identiﬁcation is to be reinitiated, if the AM became unstable, and launched
with a new stable value.
ˆ j+1 );
– calculate θ(t
ˆ j+1 ) is stable, con– check Jury’s criteria (24); if the AM with parameter θ(t
ˆ
tinue identiﬁcation; else choose some θnew (tj+1 ) ∈ Θ.
Now the question is: how to ﬁnd Θ and choose θˆnew during the approximation?
Let us consider this problem for 2-order system and write down the conditions
of stability for d1 and d2 :
1
1
< d1 <
+ 1,
|a1 |
|a1 |
− (a1 (d1 − 1) + 1) + a2 < d2 < (a1 (d1 − 1) + 1) + a2 .
1−

(26)
(27)

From (26) it is reasonably to take d1 = 1 and d2 = a2 from (27). In equivalent
manner we take a1 = 0 and a2 = d2 . Thus, it is guaranteed that θˆnew (tj+1 ) ∈ Θ.

5

Experiments

The workability of the proposed ideas is to be approved by the experiments
for 2-order system. The subject of our interest is the rate of convergence. We
consider several aspects to aﬀect this characteristic of identiﬁcation: the method
used in adaptation process, the case of uncertainty, the level of noise both in
object and sensor (Qθ and Rθ ), and the stability of entire system (eigenvalues
of matrix Φθ from the Juri’s criteria point of view).

Comparative Analysis of Stochastic Identiﬁcation Methods

453

In real life the parameter θ is beyond our reach, while in the experimental
conditions our programm simulates both entire system and AM, and we know
ˆ Hence we have “a
θ (we, but not the AM!) and are able to compare it with θ.
task with known solution”.
Therefore the scheme of all experiments is as follows (unless otherwise arˆ then at i = 1000 we
ranged): during the ﬁrst thousand of iteration θ = θ,
ˆ
simulate a fault by changing θ and at i = 1000 the process of approximation is
launched. In this type of experiments the SSA is not used.
Let us consider the ﬁrst case of uncertainty and take MS with
Φθ =

0
0.30

1
.
0.67

which eigenvalues (approximately 0.307 and 0.977) are close to stability limits.
The table below contains the convergence time (in iterations) which is averaged
over 100 realization of approximation process. The 15-percent ratio error is taken
(see ﬁg. 1, ﬁg. 2 and ﬁg. 3).
Q
1
1

R
1
0.1

Alg 1.
d1
14479
17766

d2
4675
651

Alg. 2
d1
5967
4314

d2
599
493

Alg. 3
d1
6698
4716

d2
963
1138

Now the MS with eigenvalues which equal approximately -0.358 and 0.558 and,
hence, are far from stability limits:
Φθ =

0
0.20

1
.
0.20

The average convergence time is represented in the table:
Q
1
1

R
1
0.1

Alg 1.
d1
38376
16889

d2
39822
8440

Alg. 2
d1
13626
4431

d2
18380
8610

Alg. 3
d1
11928
5566

d2
21387
8000

In the second case of uncertainty the convergence of A is much slower than the
one of D. To explain this fact series of experiments were made. The parame¯ and ﬁxed. The AFQ was considered as a
ter D was taken coincided with K
function of a1 and a2 , and level lines, consisting of the points with coordinates
(a1 , a2 ) : Jε (a1 , a2 ) = const. These lines look like oblong ellipses. It is typical
for ravine surfaces, so one take f1 = 0.2, f2 = 0.7 and in the ﬁrst realization of
approximation process get a1 [tmax ] ≈ 0.7 and a2 [tmax ] ≈ 0.2, in the second –
a1 [tmax ] ≈ 0.4 and a2 [tmax ] ≈ 0.5, then – a1 [tmax ] ≈ 0.8 and a2 [tmax ] ≈ 0.1,
or even a1 [tmax ] ≈ −1.3 and a2 [tmax ] ≈ 0.4. The fact that the “correct answer”
is also available, is very important, because for all these realizations the error
e(tmax ) does not vary greatly, as well as Jε (a1 , a2 ), so x
ˆ is “good enough”. Taking
these features in account one may note that a1 (tmax ) + a2 (tmax ) ≈ 0.9 = const,
as if points (a1 [tmax ], a2 [tmax ]) belong to certain line or oblong ellipse.

454

O. Fatyanova and A. Kondratiev

Fig. 1. Tracking the parameters k1 and k2 by d1 (left) and d2 (right) for Algorithm 1

Fig. 2. Tracking the parameters k1 and k2 by d1 (left) and d2 (right) for Algorithm 2

Fig. 3. Tracking the parameters k1 and k2 by d1 (left) and d2 (right) for Algorithm 3

5.1

Experimental Conclusions

– The ravine character of AFQ (as a function of a1 and a2 ) degrades seriously
the rate of convergence, that does not allow to get correct estimates of Φθ for
appropriate time. The enlargement of modelling time to 500000 iterations
gives nothing, the millions of iterations are demanded. Nevertheless x(ti ) is
estimated good enough to say that the main task – to derive x
ˆ(ti ) – is solved.

Comparative Analysis of Stochastic Identiﬁcation Methods

455

– The series of experiments approves that LSM has got the best rate of convergence. Suboptimal method considered to be the successful improvement
of LSM, because this algorithm provides serious computational shortcut and
insigniﬁcantly yields to LSM in the rate of convergence.
– The larger R (in comparison with Q) the worse the rate of convergence.
– The rate of convergence is better for MS which eigenvalues are close to the
limit of stability. It is easily seen from (1): such MS possess more strongly
marked inner dynamics than another ones.

6

Conclusions and Future Work

Basing on the results of the experiments we can say that identiﬁcation methods
can be successfully applied for the solution of the main task, the derivation of
unbiased estimate of x. Unfortunately we face the challenge in obtaining correct
estimations in the second case of uncertainty, because of the ravine character of
AFQ. So one of the directions of the future works is to minimize the inﬂuence
of this factor to the rate of convergence. Another direction is to study new
identiﬁcation methods such as ones of Newton’s type and conjugated gradients
algorithm.

References
1. Semoushin, I.V., Tsyganova, J.V.: Indirect error control for adaptive ﬁltering. In:
Neittaanmaki, P., Tiihonen, T.,Tarvainen, P. (eds.): Proc. of the 3rd European Conference on Numerical Mathematics and Advanced Applications. World Scientiﬁc,
Singapore New Jersey London Hong Kong (2000) 333–340
2. Semoushin, I.V.: Adaptive Identiﬁcation and Fault Detection Methods in Random
Signal Processing. Saratov University Publishers, Saratov (1985) [in Russian]
3. Semoushin, I.V.: Identiﬁcation of linear stochastic objects on the base of incomplete
noisy observations. Automatics and telemechanics, (1985) 61–71 [in Russian]
4. Tsypkin, J.Z.: Optimal identiﬁcation of dynamic objects. Mensuration control automation, Vol. 3 (1983) 47–59 [in Russian]
5. Tsypkin, J.Z.: Adaptation and training in automatic systems. Moscow (1968) [in
Russian]
6. Vasan M.: Stochastic approximation. Moscow (1972) [in Russian]
7. Ponyrko S.A., Semoushin, I.V.: About the choice of start and stop algorithm for
minimisation of root-mean-square quality criterion. Autometria, Vol. 2 (1973) 68–
74 [in Russian]

