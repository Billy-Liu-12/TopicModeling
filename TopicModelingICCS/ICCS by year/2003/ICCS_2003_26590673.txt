V-Invariant Methods for Generalised Least Squares
Problems
M.R. Osborne
Mathematical Sciences Institute, Australian National University, ACT 0200, Australia

Abstract. An important consideration in solving generalised least squares problems is the dimension of the covariance matrix V . This has the dimension of the
data set and is large when the data set is large. In addition the problem can be
formulated to have a well determined solution in cases where V is illconditioned
or singular, a class of problems that includes the case of equality constrained least
squares. This paper considers a class of methods which factorize the design matrix
A while leaving V invariant, and which can be expected to be well behaved exactly when the original problem solution is well behaved. Implementation is most
satisfactory when V is diagonal. This can be achieved by a preprocessing step in
which V is replaced by the diagonal matrix D which results from the modified
Cholesky factorization P V P T → LDLT where L is unit lower triangular and
P is the permutation matrix associated with diagonal pivoting. Conditions under
which this replacement is satisfactory are investigated.

1

Introduction

The generalised least squares problem is
min rT V −1 r; r = Ax − b
x,r

(1)

where A ∈ Rp → Rn , rank (A) = p < n, x ∈ Rp , r ∈ Rn . In this form it requires V
to be positive definite and hence invertible. However, the necessary conditions for this
problem to have a solution can be cast in the form
V −A
−AT 0

λ
−b
=
x
0

(2)

where λ is the vector of Lagrange multipliers and is related to the residual vector by
r = V λ. This proves to have solutions under weaker conditions on V .
In (2) the solution of the generalised least squares problem requires the solution
of a linear system with symmetric, indefinite matrix. This poses problems in the data
analytic context because n may well need to be large as a consequence of the generic
n−1/2 rate of convergence in stochastic estimation problems, while p has to be small
in typical regression situations. Thus the size of V is potentially the main source of
difficulty in otherwise well behaved problems. This would also suggest that if structure
is present in V then advantage should be taken of this fact. One possible approach is
considered here based on the paper [2]. The generalisation embodied in the formulation
P.M.A. Sloot et al. (Eds.): ICCS 2003, LNCS 2659, pp. 673–682, 2003.
c Springer-Verlag Berlin Heidelberg 2003

674

M.R. Osborne

(2) has the interesting advantage that it permits least squares problems subject to linear
equality constraints to be considered in the same framework as the generalised least
squares problem. Let the constrained problem be written
min rT2 V −1 r2 ; r2 = Ax − b, 0 = Cx − d,
x,r2

where C ∈ Rp → Rm , m < p, is assumed to have full rank. This ensures that the
constraints are consistent for arbitrary problem data d. One possible approach involves
reformulating the problem using penalised least squares:
min rT2 V −1 r2 +
x,r

m
i=1

2

σ 2 Wi−2 (r1 )i

(3)

where r1 = Cx − d, and W = diag {Wi } is some bounded positive scaling matrix.
The idea is that as σ gets large the penalised term will dominate unless r1 → 0. It
is straightforward to show a convergence rate of O σ −2 . But (3) is just a generalised
least squares problem and can be reformulated as
min sT s;

σ −1 W

x,s

L

s=

C
d
x−
.
A
b

This problem has the limiting form as σ → ∞
min sT s;
x,s

0
L

s=

C
d
x−
.
A
b

The solution of this problem is well determined provided


0 0
C
−
0V
A 
M =
T
T
− C A
0
is nonsingular. The ordering of terms here proves important.
The development of this paper is as follows. The next two sections consider V invariance and its application to the factorization of the design matrix A with only
minor differences from [2]. Practical implementation consideration puts an emphasis
on diagonal V with elements ordered in increasing size, and it is suggested [4] that
the LDLT , L lower triangular with unit diagonal, modification of the basic Cholesky
factorization of V using diagonal pivoting [3] could prove useful in weakening this
requirement. The major contribution in this paper is made in the concluding section
where it is argued that the error that occurs when small elements in the computed D are
set to zero is, with some qualification, of the same size as these small elements. In fact
this argument extends to the case where D supports multiple scales switching between
these in discrete steps. Also, it is not required that all steps of the Cholesky factorization
be completed successfully in the case that V is (almost) semi-definite. It is shown that
one consequence of the ability to support multiple scales is a requirement for column
pivoting in the factorization of the design matrix A, especially when equality constraints
are present.

V-Invariant Methods for Generalised Least Squares Problems

2

675

V-Invariance

The motivation for introducing algorithms for the solution of generalised least squares
problems based on transformations with special invariance properties derives from the
special case of the methods for the linear least squares problem based on orthogonal
factorization of the design matrix A where an orthogonal matrix Q is constructed to
reduce A to upper triangular form while preserving column lengths corresponding to
V = I. The condition for V -invariance generalises this metric condition to
JV J T = V.
Now assume that a V -invariant factorization of A is given by
JA =

U
,
0

(4)

and that V has the form
Ok 0
0 V2

V =

(5)

where Ok is the k × k zero matrix and V2 has the form
V11 V12
V21 V22

V2 =

(6)

where V11 ∈ Rp−k → Rp−k . Then the transformation applied to equation (2) gives






0
0 0
U  J −T λ 1
(Jb)1
 0 V11
V
−
12

 −T



0 
 J λ 2 = − (Jb)2 .
 0 V21
V22
0
x
− UT 0
0
This system reduces to the equations
U T J −T λ
V22 J
0
V12

J −T λ

−T

2

λ

1
2

= 0,
= − (Jb)2 ,

− U x = − (Jb)1 .

Remark 1. If U in (4) is nonsingular, and V2 is ordered so that V22 is nonsingular, then
the generalised least squares problem has a well determined solution.
To construct factorizations it is convenient to develop elementary V -invariant transformations. Let
J = I − 2uvT .

676

M.R. Osborne

If vT V v >0 then the condition of V-invariance determines u :
u=

1
vT V

v

V v, J = I −

2
vT V

v

V vvT .

(7)

In this case uT v = 1 so that J is also an elementary reflector (J 2 = I). If v =
v1
, v1 ∈ Rk then vT V = 0 and the above derivation of J breaks down. A V 0
invariant transformation is given by
u1
u2

J =I −2

v1T 0 ,

(8)

and J 2 = I if v1T u1 = 1. There is a useful connection between the two forms of
transformations. Let
εW

Vε =

V2

where Vε ∈ Rk → Rk is a positive diagonal matrix. Then the Vε -invariant transformation
derived from (7) but written in terms of u is
Jε = I −
=I−
→I−

2
uuT Vε−1 ,
T
u Vε−1 u
2
uuT εVε−1 ,
T
u εVε−1 u
2
uT1 W −1 u1

u1
u2

uT1 W −1 0 ,

(9)

as ε → 0. Note that the action of this transformation can be considered to be on the scale
defined by ε.
Numerical stability of the elementary V -invariant transformations depends on the
size of the elements of J and hence on its norm. In this connection the following result
[2] is important.
Lemma 1. Let J be an elementary reflector, and set η = u
norm of J is given by
J =η+

√

v > 1. Then the spectral

η2 − 1 .

In the case of the V -invariant transformation (7)
η=

v Vv
.
vT V v

Thus the relative size of vT V v is important.

(10)

V-Invariant Methods for Generalised Least Squares Problems

3

677

V-Invariant Factorization

The basic idea is to build up a V -invariant transformation J taking A to upper triangular
form using elementary V -invariant transformations. Assume that a partial factorization
Ji−1 has been constructed:
Ji−1 A =

i
Ui−1 U12
.
Ai

The aim now is to construct v such that the elementary V -invariant transformation (7)
Ji = I −

2
V vvT
vT V v

has the action
J i (Ji−1 Aei ) = J i

ui
ui
=
ai
γe1

giving
Ji = J i Ji−1 , Ui =

Ui−1 ui
.
γ

Note that the scale of v is disposable. Thus the choice
Vv =

0
ai − γe1

(11)

is allowed. This would be suitable except that computation of v requires the solution of
a system of linear equations with matrix V , and this would seem to reintroduce exactly
the kind of complication that we seek to avoid. There are considerable simplifications if
V2 is diagonal. In this case transformation of the design matrix to upper triangular form
is made easier by some structural assumptions on V . These are that V has the reduced
form (5) where V2 ∈ Rn−k → Rn−k , and
V2 = diag {νk+1 , νk+2 , · · · , νn } , 0 < νk+1 ≤ νk+2 ≤ · · · ≤ νn .

(12)

If V is a positive semidefinite matrix in general form then this could be achieved by
1. Scaling V so that the diagonal elements of the transformed matrix are unity. Here
1/2
1/2
V → SV S where S = diag V11 , · · · , Vnn .
2. A diagonal pivoting (rank revealing) LDLT factorization [3] applied to V to construct a matrix of known factorization and rank which closely approximates V
P V P = LDLT .
Note that the ordering achieved by this factorization gives the elements of D in
decreasing order of magnitude which is the inverse of that assumed in (5).

678

M.R. Osborne

3. Transformation of the problem to one with covariance matrix D
r → L−1 P S −1 r ⇒ A → L−1 P S −1 A, b → L−1 P S −1 b.
The conditioning of the forward substitution is aided here by the use of diagonal
pivoting which ensures that |Lij | ≤ 1, j < i. Thus illconditioning in V tends to be
concentrated in D by the rank revealing factorization.
4. Permutation of the elements of the new covariance into increasing order with permutation matrix Q
D → QDQT , r → Qr.
To compute γ let
V2 =

V11

, V11 = diag {νk+1 , · · · , νi−1 } , V22 = diag {νi , · · · , νn } ,

V22

where the partitioning is chosen to correspond to the i’th factorization step. Then


Ik

J i =  I1
I2 − vT V222 v V22 vvT
is V -invariant where Ik is the k × k unit matrix and I1 and I2 are unit matrices conformable with V11 and V22 respectively. Then (11) gives
v=

0
.
−1
ai
V22

The properties of the elementary reflector to give
ui
ui
= Ji
,
ai
γe1
=

I−

2
V vvT
vT V v

ui
.
γe1

Taking the scalar product with v gives a quadratic equation for γ
T

−1
(ai − γe1 ) V22
(ai + γe1 ) = 0.

This has the solution
γ=θ

√

−1
νi aTi V22
ai .

where the standard argument which seeks to minimize cancellation suggests that θ =
− sgn ((ai )1 ) is appropriate. An interesting feature is the appearance of the term N i =
−1
νi V22
which means that γ is independent of the scale of V . [2] give the stability criterion
η=

−1
(ai − γe1 )
V22
T

(ai − γe1 )

−1
V22

ai − γe1
(ai − γe1 )

≥

ai
.
2 |γ|

V-Invariant Methods for Generalised Least Squares Problems

Remark 2. Column pivoting can be used to exchange any column

679

uj
, j = i, i +
aj

i
U12
into the pivotal position. Let ρi (A, j) = aTj N i aj . The above analysis
Ai
2
suggests that j be chosen to maximize ρi (A, j) / aj in order to make the lower bound
for η as small as possible. However, if V = I, lengths are not preserved by the V -invariant
transformations and a consequence is that denominators cannot be updated economically
in general. Thus the selection of the pivotal column would typically be made on the basis
of the size of the ρi (A, j) alone.

1, · · · , p of

If 1 ≤ l ≤ k then the key mapping
Jal = γel , l ≤ k,
needs to make use of the second family of V -invariant transformations (8). Let a1 =
a11
in conformity with the partitioning of V. Here the limiting argument leading to
a21
(9) provides insight into the reason why V -invariant transformations work well in the
multiscaled situations. If l = 1 the resulting transformation matrix J = I − 2cdT has
the generic form (8) with
√
2c = a1 + sgn a11 1 a11 2 e1 / a11 2 ,
√
a11 + sgn a11 1 a11 2 e1
2d =
/ a11 2 + a11 1 .
0
This transformation will have large elements if
a11

2

a1

2

.

(13)

This is the limiting case of (10) which characterizes illconditioning in the other class
of transformations. The extreme case corresponds to a11 2 = 0, and this can certainly
occur in the case k > 0 in (5), (12). This is illustrated in the following example which
provides a justification for the use of column pivoting in implementing the V -invariant
factorization.
Example 1. Column pivoting is necessary for V -invariant solution methods if the generalised least squares problem is subject to equality constraints. Here the pivotal column
n+m
at the i’th step is chosen as the one that maximizes ρi (A, j) = q=i Nqi A2qj , j =
i, i + 1, · · · , p, where m is the number of equality constraints and N i is the scaled
diagonal weighting matrix. To illustrate the requirement consider the design matrix
A2 ∈ Rp → Rn given by
(A2 )∗1 = Se,
(A2 )i(2j) = Si cos (2πj (i − 1) h) ,
(A2 )i(2j+1) = Si sin (2πj (i − 1) h) ,
i = 1, 2, · · · , n, j = 1, 2, · · · , k,

680

M.R. Osborne

√
√
where p = 2k + 1, S = diag 1/ 2, 1, · · · , 1, 1/ 2 , h = 1/ (n − 1). The columns
of A2 are orthogonal and similarly scaled so a least squares problem with A2 as design
is very well conditioned. Let the rows of the constraint matrix A1 ∈ Rp → R2 be given
by
(A1 )1∗ = eT , (A1 )2∗ = eT − peTk+1 .
Then the constrained least squares problem
min sT s;
s,x

0
I

s=

b
A1
x− 1 ,
A2
b2

with
b1
A1 e
=
, εi = sin (2π (k + 1) (i − 3) h) , i = 3, 4, · · · , n + 2,
A2 e + ε
b2
has the exact solution x = e. However, the leading 2 × 2 submatrix of A1 is singular
if k ≥ 2. This causes a breakdown of the V -invariant factorization at the second step
(13) as a consequence of the form of N 2 if column interchanges are not used. In the
particular case corresponding to n = 8, p = 5, the ordering resulting from column
pivoting is {3, 2, 1, 4, 5}. The first interchange is a consequence of the largest coefficient
in the second constraint, and it succeeds in forcing the leading 2 × 2 submatrix to be
nonsingular.

4

Does the Use of the LDLT Factorization of V Make Sense?

In this section it is assumed that V has full rank, but is illconditioned as a result of a
cluster of small eigenvalues. The results extend immediately to the case where V has a
known nullspace plus a cluster of small eigenvalues. Here the rank-revealing Cholesky
factorization of V , performed in exact arithmetic, has the form
P V P T → L diag {Dn , Dn−1 , · · · , D1 } LT
where P is the permutation matrix corresponding to the diagonal pivoting, and the
diagonal pivoting ensures that
Dn ≥ Dn−1 ≥ · · · ≥ D1 .
This order is the reverse of that required here, and it must be inverted in order to construct
the factorization of the design matrix based on V -invariant transformations. Conditions
which guarantee that n − k steps of the Cholesky factorization with diagonal pivoting
can be computed leaving a small remainder are given in [3]. One form compatible with
the above assumption is
∆1 = diag {D1 , D2 , · · · , Dk } > 0, small,
Dk

Dk+1 ,

∆2 = diag {Dk+1 , · · · , Dn } not small,
where k ≤ p.

V-Invariant Methods for Generalised Least Squares Problems

681

It would be expected that if the computed factorization does go to completion then
the computed values {D1 , D2 , · · · , Dk } could have high relative error as a result of
cancellation. Does this matter? The following argument suggests strongly that it does not.
It assumes that the factorization is stopped after n − k steps. This yields the incomplete
transformation
∆2

PV PT = L

Vk

LT ,

where use has been made of the result that the unit lower triangular matrix L has the
particular structure
Ln−k
,
(n−k)
L2
I

L=
and
L−1

0 0
0 0
L−T =
.
0Vk
0Vk

First note that the case D = diag {0, · · · , 0, Dk+1 , · · · , Dn } corresponds to the equality
constrained problem
min sT s;
x

0
1/2
∆2

s=

A1
b1
x−
.
A2
b2

Here the Cholesky L has been absorbed into the design to simplify notation (r → L−1 r),
and it is assumed that necessary permutations to order D have also been applied. This
is the limiting problem as λ → ∞ associated with the penalised objective
T −1
min rT2 ∆−1
r1 ; r =
2 r2 + λr1 E
x

b1
A1
x−
A2
b2

(14)

where λ−1 E = V k where V k ∈ Rk → Rk is small as the remainder after n − k
steps of the rank-revealing Cholesky factorization [3] p. 212, E is positive definite, and
λ = 1/ V k . The problem (14) has the alternative form
min sT s;

λ−1/2 E 1/2

1/2

∆2

x

s=

A1
b1
x−
.
A2
b2

Now penalty function theory can be used to show x (λ) − x = O (1/λ) , λ → ∞
provided
M=

T −T /2
AT2 ∆−1
2 A2 A 1 E
−1/2
E
A1
0

has full rank [1]. This is a weaker condition than both A1 and A2 having full rank. Here
the conditioning of the scaled perturbation matrix E could also be important. Assuming
that E is reasonably well conditioned then the equality constrained problem obtained

682

M.R. Osborne

by setting ∆1 = 0 has a well defined solution which differs from the exact solution of
that based on the computed LDLT factorization by O ( ∆1 ).
The above argument identifies a class of problems where V is illconditioned for
inversion, but where solution by V -invariant methods is satisfactory after preliminary
problem transformation based on the LDLT factorization of V with diagonal pivoting .
One relevant case is the fourth example in [4]. This is defined to have an illconditioned
L, and unit diagonal. However, if the LDLT factorization is recomputed using diagonal
pivoting then the illconditioning is moved to D and does not cause problems.

References
1. A.V. Fiacco and G.P. McCormick, Nonlinear programming: Sequential unconstrained minimization techniques, John Wiley and Sons, Inc., 1968.
2. M. Gulliksson and P. Wedin, Modifying the QR-decomposition to constrained and weighted
linear least squares, SIAM J. Matrix Anal. Appl. 13 (1992), no. 4, 1298-–1313.
3. N.J. Higham, Accuracy and stability of numerical algorithms, SIAM, 1996.
4. I. Søderkvist, On algorithms for generalized least squares problems with ill-conditioned covariance matrices, Computational Statistics 11 (1996), 303—313.

