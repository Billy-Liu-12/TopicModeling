Spectral Subtraction Using Spectral Harmonics for
Robust Speech Recognition in Car Environments
Jounghoon Beh and Hanseok Ko
Departments of Electronics and Computer Engineering, Korea University, 5 ka-1,
Anam-dong, Sungbuk-ku, Seoul, 136-701, Korea
jhbeh@ispl.kore.ac.kr

Abstract. This paper addresses a novel noise-compensation scheme to solve the
mismatch problem between training and testing condition for the automatic
speech recognition (ASR) system, specifically in car environment. The conventional spectral subtraction schemes rely on the signal-to-noise ratio (SNR) such
that attenuation is imposed on that part of the spectrum that appears to have low
SNR, and accentuation is made on that part of high SNR. However, since these
schemes are based on the postulation that the power spectrum of noise is in
general at the lower level in magnitude than that of speech. Therefore, while
such postulation is adequate for high SNR environment, it is grossly inadequate
for low SNR scenarios such as that of car environment. This paper proposes an
efficient spectral subtraction scheme focused specifically to low SNR noisy environment by representing harmonics distinctively in speech spectrum. Representative experiments confirm the superior performance of the proposed method
over conventional methods. The experiments are conducted using car noisecorrupted utterances of Aurora2 corpus.

1 Introduction
The mismatch between training and testing condition is a major problem in ASR systems. The techniques to solve this problem can be categorized into two principal approaches. First is the spectral subtractive-type of algorithm performing noise suppression using short-time spectral amplitude, such as spectral subtraction, nonlinear
spectral subtraction and Weiner filter. The other is the feature compensation algorithm such as cepstral mean normalization or vector Tayler series. In general, it is
well known that spectral subtractive-type algorithm is very simple and efficient especially in stationary noisy environments.
This paper is about a new spectral subtractive-type scheme based on the idea that
even though speech is heavily corrupted by noise, the shape of spectral harmonics of
speech is well preserved as when speech is not corrupted [1] [2].
The weakness of conventional spectral subtractive-type algorithm is identified and
presented in Section 2. The proposed remedial approach is described in Section 3. In
Section 4, we show the proposed method’s effectiveness over conventional methods
with representative experiments using Aurora 2. Concluding remarks are provided in
Section 5.

P.M.A. Sloot et al. (Eds.): ICCS 2003, LNCS 2660, pp. 1109–1116, 2003.
© Springer-Verlag Berlin Heidelberg 2003

1110

J. Beh and H. Ko

2 Spectral Subtractive-Type Algorithm
When speech signal x(n) is corrupted by background additive noise b(n), the corrupted speech can be expressed as follows:
y( n) = x(n) + b(n) .

(1)

If speech and noise are assumed to be uncorrelated, in frequency domain, it can be
represented as follows:
| Y (k ) |2 =| X ( k ) |2 + | B( k ) |2

(2)

.

where k is the index of frequency bin.
2.1 Spectral Subtraction
In the case of power spectral subtraction, the short-time power spectrum | Xˆ ( k ) 2 | of
enhanced speech signal can be obtained by subtracting its noise estimate | Bˆ ( k ) 2 | off
from the corrupted speech. Note that every procedure is carried out in frame-by-frame
basis [3]. Instead of power spectrum, its magnitude spectrum can be made available.
General power spectral subtraction algorithm is as follows.


| Bˆ (k ) |2 
2
,
| Y (k ) | 1 − α
| Y (k ) |2 



| Xˆ (k ) |2 = 
if | Y (k ) |2 −α | Bˆ (k ) |2 > β | Y (k ) |2

2
β | Y (k ) | ,

otherwise


(3)
.

With over-subtraction factor and floor factor , this algorithm concerns the tradeoff between noise reduction and residual noise. Note that the enhanced short-time
power spectral amplitude | Xˆ ( k ) 2 | depends on a posteriori SNR [4]:
SNR p =| Y (k ) |2 / | Bˆ ( k ) |2

(4)

.

2.2 Nonlinear Spectral Subtraction
Nonlinear spectral subtraction algorithm is as follows [5]:

Φ (k ) 
2 
,
| Y (k ) | 1 −
Y
|
(k ) |2 



| Xˆ (k ) |2 = 
if | Y (k ) |2 −Φ (k ) > β | Y (k ) |2

2
β | Y (k ) | ,

otherwise

(5)
.

Spectral Subtraction Using Spectral Harmonics for Robust Speech Recognition

1111

Fig. 1. Example spectrum of a speech frame (pronunciation /oh/ by female)

Function (k) can be chosen arbitrarily to implement the notion that relatively
greater subtraction is applied to the low SNR region of spectrum and less subtraction
to the high SNR region.
In general, the spectral amplitude of speech components (e.g. spectral harmonics)
is higher than that of noise or the side lobes among harmonics so that those algorithms
are rather suitable for reasonably high SNR (about 15~20dB) noisy speech cases.
However, it appears that in the case of low SNR environments (especially 0~10dB),
such that when the noise level is as close in amplitude as that of speech, there occurs
unnecessarily subtracted speech region or less subtracted noisy region in noisy speech
spectrum. Consequently, instead of the mundane use of SNR as a measure for subtraction, we need a new and better measure for activating the subtraction procedure.
In particular, the subtraction over spectrum requires a more accurate measure than
mere SNR in order to apply the subtraction rule, which is selective to speechdominant region vs. noise-dominant region of spectrum.

3 Proposed Algorithm
3.1 Speech Dominant Region vs. Noise Dominant Region
In speech, it is observed that the voiced speech segment has peaks positioned periodically in spectrum due to the vibration of vocal cords. These points are very critical in
speech sound perception. Figure 1 illustrates this phenomenon with a sample spectrum of speech frame capturing the pronunciation /oh/ in one utterance contained in
Aurora2 corpus. Note that at the peaks, their amplitudes are far greater than the amplitudes at the points between adjacent small peaks, or side lobes. Also, it is observed
that the degree of corruption by the noise in the peaks is not as much as the degree of
corruption at the points over the side lobes. From this observation, it can be deduced
that in speech spectrum, the speech-dominant regions exist over or near the peaks and
the noise-dominant regions exist over or near the side lobes.
The fundamental frequency can be obtained roughly by auto-correlation and then
we can find the peak points from the roughly obtained fundamental frequency. For the
frequency regions covering the peak points and their vicinity, we apply a small oversubtraction factor and large floor factor. In other regions, we apply a large over-

1112

J. Beh and H. Ko

subtraction factor and small floor factors. The detailed procedure is described in rest
of Section 3.
Note that all procedures in the proposed algorithm are done on a frame-by-frame
basis.
3.2 Peak Points Detection and Segmentation
It is known that the spectral harmonics are located at the points of multiples of fundamental frequency [6]. First, using autocorrelation function, the indices of local
maxima in autocorrelation values are found. Among those indices, we can find the
fundamental frequency f0 by taking the reciprocal of the index that represents the
maximum value. Secondly, by applying the nonlinear smoothing method [6], f0 is
modified to better reflect the true fundamental frequency. This algorithm works well
even in high degree noisy environment. Autocorrelation function is expressed as follows [7]:
φ (τ ) =

1
N

N −1

∑ x ( n) x ( n + τ )

(6)

.

n =0

We then establish the index as k0 for the frequency bin that corresponds to f0. Using k0, the peak points (harmonics) hl are determined as h1 = k0, h2=2k0, h3=3k0, ...,
hL=Lk0 where L is the index of the last component of harmonics. Using this procedure, we accomplish determining all harmonics in the spectrum of input frame, which
are assumed to be speech-dominant region.
For the purpose of implementing the proposed scheme (section 3.4), frequency axis
is divided into several non-overlapping bands in the following form.

[1
,


 h1

k0

 hL −1 − 2

h1 ],

k0 
,
2 

k0

 
h1 + 2 h2 ,  h2

 
k0  
k0

hL−1 + , hL −1 +
hL  ,
2 
2


h1 +

h2 +

hL

k0 
,
2 

LL

(7)

FFT order 
+ 1 .
2


3.3 Voice Activity Detection (VAD) and Noise Estimation
Note that in each input frame, whether it is the speech frame or not, the fundamental
frequency is calculated. As a result, the value of autocorrelation is also calculated at
each frame inevitably. Since this value is appeared to be a more effective measure to
distinguish speech region from non-speech region than mundane energy measure in
experiments, so we used this value instead of energy for VAD. Then, two separate
procedures are employed for robust detection of speech vs. non-speech region. First,
we take a logarithm to this value, and then a smoothing procedure is carried out. If
this value is greater than the mean value of autocorrelation obtained during the first 5
frames by 0.3, the relevant frame is decided as ‘speech frame’. Secondly, for frames
determined as ‘speech frame’, if the detected fundamental frequency is not in
50~400Hz, then it is concluded as ‘non-speech frame’ once again. It is well known

Spectral Subtraction Using Spectral Harmonics for Robust Speech Recognition

1113

that 50~400Hz is considered an appropriate fundamental frequency range for human
voice.
Input speech is coded into 32ms frames, with a frame-shift of 16ms. Then shorttime FFT is applied. We performed 256 points FFT analysis on the relevant input
frame.
For the noise estimation, we assume that any starting input speech is followed by a
silence or background noise segment corresponding to 5 frames (96 ms). In that duration, mean of short-time spectral amplitude at each frequency bin are calculated.
Then, if input frame is determined as ‘non-speech frame’, the estimated noise spectral
magnitude is updated as follows;
| Bi (k ) | 2 = λ | Bi −1 (k ) | 2 +(1 − λ ) | Bi (k ) | 2 , 0.65 ≤ λ ≤ 0.998 .

(8)

where i is the frame index and k is the index of FFT bins and we use =0.75.
This procedure is for the purpose of applying different subtraction rule to the
speech frame and the non-speech frame. It is because of the ‘non-speech frame’,
whose harmonics components cannot be determined. Consequently, the proposed
scheme is inappropriate to ‘non-speech frames’.
3.4 Spectral Subtraction
Based on the result of VAD procedure, we apply different subtraction rule.
Speech Frame. In order to implement the proposed scheme, we designed following
simple linear function.
− If k ∈ [1 h ] or k ∈ h + k 0 h  , l = 2,3,4,..., L , then
1
l
 l −1


γ (k ) =

2

α min − α MAX
( h − k ) + α MAX
k  l

hl −  hl −1 + 0 
2


δ (k ) =

− If k ∈ h
 l


hl +

k0 
2 

.



or

β MAX − β min
( h − k ) + β min
k  l

hl −  hl −1 + 0 
2


k ∈  hL


γ (k ) =

δ (k ) =

.

.

(9)

(10)

FFT order 
+ 1, l = 1,2,3,..., L − 1 , then
2


α MAX − α min
( hl − k ) + α min
k 

 hl + 0  − hl
2


β min − β MAX
( hl − k ) + β MAX
k 

 hl + 0  − hl
2


.

.

(11)

(12)

1114

J. Beh and H. Ko

Fig. 2. An illustration about implementing over-subtraction factor to FFT bins
(k) applies the minimum over-subtraction factor to each harmonic component and
the maximum over-subtraction factor to the middle point at each components. Then,
over-subtraction factors of points exist in the interval between those points are interpolated linearly. Figure.2. illustrates a shape of (k) which of fundamental frequency
is about 156Hz when sampling rate is 8kHz and 1,024 points FFT analysis is applied.
On the contrary, (k) applies the maximum floor factor to each harmonic component and the minimum floor factor to the middle point at each component. Then, other
floor factors between those points are generated by linear interpolation.
The proposed subtraction rule can be represented as follows:


| Bˆ (k ) |2 
2
,
| Y (k ) | 1 − γ (k )
| Y (k ) |2 



| Xˆ (k ) |2 = 
if | Y (k ) |2 −γ (k ) | Bˆ (k ) |2 > δ (k ) | Y (k ) |2 .

2
δ (k ) | Y (k ) | ,

otherwise


(13)

The values of parameters used are as follows:
MAX

= 5,

min

= 2,

MAX

= 0.2,

min

= 0.05 .

(14)

Non-speech Frame. We applied the subtraction rule as same as conventional method
using Eq. (1) with MAX, min in Eq. (14).

4 Experimental Results
4.1 Experimental Conditions
In these experiments, utterances corrupted by car noise among Aurora2 corpus are
used. For comparison, spectral subtraction algorithm [3] and nonlinear spectral subtraction algorithm [5] are evaluated. Throughout all experiments, the VAD procedure
and the noise estimation procedure are the same as the proposed method. Finally, for
each algorithm, the enhanced speech signals are recovered by overlap-and-add man-

Spectral Subtraction Using Spectral Harmonics for Robust Speech Recognition

1115

ner followed by inverse FFT on frame-by-frame basis. Then, all performances are
evaluated.
In Table 1, the meanings of the each abbreviation are as follows:
− ‘SS’: general power spectral subtraction.
− ‘SS_D’: general power spectral subtraction + different subtraction rule.
− ‘NSS’: nonlinear spectral subtraction.
− ‘NSS_D’: nonlinear spectral subtraction + different subtraction rule.
Experiments ‘SS_D’, ‘NSS_D’ are evaluated with different subtractions rule as it
is applied in the proposed method. That is, if the input frame is decided as ‘nonspeech frame’, we applied the rule in section 3.4.2 which is the conventional subtraction rule Eq. (1) with the over-subtraction factor MAX and floor factor min.
4.2 Experimental Results
‘Avg’ denotes to the mean value of word accuracy over 0dB~20dB. From the Table,
the proposed method is seen effective at low SNR cases. The average word accuracy
of the proposed method also shows that it is superior over other spectral subtraction
approaches.
Table 1. Word Accuracy (%)
SNR

Baseline

SS

SS_D

NSS

NSS_D

Proposed

clean

99.02

98.78

98.78

98.75

98.75

98.81

20dB

97.97

98.27

98.51

98.18

98.21

98.36

15dB

94.24

97.08

97.55

97.26

97.20

97.52

10dB

78.17

91.59

94.15

93.95

93.83

94.66

5dB

42.59

79.39

81.78

84.61

84.49

85.00

0dB

14.67

31.26

50.58

54.64

54.49

56.13

-5dB

9.25

10.47

17.78

17.95

17.86

20.52

Avg.

69.79

79.52

84.51

85.73

85.64

86.33

5 Conclusions
This paper proposes an efficient spectral subtraction scheme focused to specifically
low SNR noisy environments by distinguishing the speech-dominant segment from
the noise-dominant segment in speech spectrum. Consequently we let the shape of the
spectral harmonics be preserved more clearly in noisy environments, which are very
critical in speech sound perception. Representative experiments confirm the superior
performance of the proposed method to conventional methods. In particular, the proposed method is seen effective at low SNR cases (SNR < 10 dB). The average word
accuracy of the proposed method also shows that it is superior to other recently intro-

1116

J. Beh and H. Ko

duced approaches. The experiments are conducted using car noise-corrupted utterances of Aurora2 corpus.

References
1. Jensen, J., Hansen, J.: Speech Enhancement Using a Con-strained Iterative Sinusoidal
Model, IEEE Transactions on Speech and Audio Processing, Vol. 9, No. 7, Oct (2001)
731–740
2. Ealey, D., Kellher, H., Pearce, D.: Harmonic tunneling: track-ing non-stationary noises
during speech, Eurospeech 2001(2001) 437–440
3. Berouti, M., Schwartz, R., Makhoul, J.: Enhancement of speech corrupted by additive
noise, Proceedings of the IEEE Conference on Acoustics, Speech, and Signal Processing,
April (1979) 208–211
4. Virag, N.: Single Channel Speech Enhancement Based on Masking Properties of the Human Auditory System, IEEE Transactions on Speech and Audio Processing, Vol. 7, No. 2,
Mar (1999) 126–137
5. Lockwood, P., Boudy, J.: Experiments with a Nonlinear Spectral Subtractor(NSS), Hidden
Markov Models and the pro-jection, for robust speech recognition in cars, Speech Communication, Vol. 11 (1992) 215–228,
6. Hess, W.: Pitch Determination of Speech Signals, Springer-Verlag Berlin Heidelberg New
York Tokyo (1983)
7. Rabiner, L., Schafer, R.: Digital Processing of Speech Signals, Prentice-Hall (1978)
8. Boll, S.F.: Suppression of Acoustic Noise in Speech Using Spectral Subtraction, IEEE
Transaction on Acoustics, Speech and Signal Processing, Vol.27, No.2, April (1979) 113–
120

