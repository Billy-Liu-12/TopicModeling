A Parallel Loop Self-Scheduling on Extremely
Heterogeneous PC Clusters
Chao-Tung Yang and Shun-Chyi Chang
Department of Computer Science and Information Engineering
Tung-Hai University
Taichung, Taiwan 407, R.O.C.
ctyang@mail.thu.edu.tw

Abstract. Cluster system is viable and less expensive alternative to SMP.
However, the approaches to deal with scheduling and load balancing on
heterogeneous cluster computer system are not mature. Self-scheduling
schemes which are suitable for parallel loops with independent iterations on
heterogeneous cluster computer system have been designed in the past.
However, these schemes, such as FSS, GSS and TSS, can not achieve load
balancing in extremely heterogeneous environment. In this paper, we propose a
heuristic approach to solve parallel regular loop scheduling problem on
extremely heterogeneous PC cluster environment.

1 Introduction
Parallel computers are becoming increasingly widespread, nowadays many of these
parallel computers are no longer shared-memory multiprocessors, but rather follow
the distributed memory model for scalable. These systems may consist of
homogeneous workstations, where all the workstations have processors, memory and
caches with exactly the identical specifications. According to Moore’s law, CPU
clock will double in 18 months. As the law still works today, to build clusters
consisting of extremely different computer performance becomes demanding.
However, more and more systems are now composed of a number of heterogeneous
workstations clustered together, where each workstation may have different CPU
performance capabilities, different amounts of memory and caches, and even different
architectures and operating systems.
To exploit the potential computing power of cluster computers, an important issue
is how to assign tasks to computers so that the computer loads are well balanced. That
is how to assign the different parts of a parallel application to the computing resources
to minimize the overall computing time and to efficiently use the resource. Loops
often comprise a large portion of a program’s parallelism. An efficient approach to
extract potential parallelism is to concentrate on the parallelism available in the loops.
A loop is called a DOALL loop if there is no cross-iteration dependence in the loop;
i.e., all the iterations of the loop can be executed in parallel. If all the iterations of a
DOALL loop are distributed among different processors evenly, a high degree of
parallelism can be exploited. Parallel loop scheduling is a method that attempts to
evenly schedule a DOALL loop on multiprocessor systems.
P.M.A. Sloot et al. (Eds.): ICCS 2003, LNCS 2660, pp. 1079–1088, 2003.
© Springer-Verlag Berlin Heidelberg 2003

1080

C.-T. Yang and S.-C. Chang

In homogeneous environment, workload can be partitioned equally to each working
computer, but in heterogeneous environment, this method will not work. Some
researches were proposed to solve parallel loop scheduling problems on
heterogeneous cluster environments by using self-scheduling schemes. These selfscheduling schemes work well in moderate heterogeneous cluster environment but not
in extremely heterogeneous environment where the performance difference between
the fastest computer and the slowest computer is large.
In this paper, we will propose a loop scheduling based on self-scheduling scheme
to approach load balancing on extremely heterogeneous cluster. The experimental
results are conducted on a PC Cluster with six nodes and the fastest computer is 7.5
times faster than the slowest ones in CPU-clock. In our experiments, assigning 80%
workload corresponding to the CPU clock, and 20% workload using traditional selfscheduling will achieve good load balancing.
The rest of the paper is organized as follows. In section 2, a brief overview of selfscheduling is given. Section 3 states our approach and reports the experiments.
Finally, the conclusion remarks are given in section 4.

2 Background

Workload

Workload

The major source of parallelism in a program is loops. If the loop iterations can be
distributed to different processors as evenly as possible, the parallelism within loop
iterations can be exploited. Loops can be roughly divided into four kinds, as shown in
Figure 1: uniform workload, increasing workload, decreasing workload, and random
workload loops. They are the most common ones in programs, and should cover most
case. These four kinds can be classified two types: regular and irregular.

Time
2. Increasing workload

Workload

Workload

Time
1. Uniform workload

Time
3. Decreasing workload

Time
4. Random workload

Fig. 1. Four kinds of loops.

A Parallel Loop Self-Scheduling on Extremely Heterogeneous PC Clusters

1081

In a parallel process system, two kinds of parallel loop scheduling decisions can be
made either statically at compile-time or dynamically at run-time.
Static scheduling is usually applied to uniformly distributed iterations on
processors [6]. It has the drawback of creating load imbalances when the loop style is
not uniformly distributed; when the loop bounds cannot be known at compile-time;
when system is heterogeneous; or when locality management cannot be exercised. In
contrast, dynamic scheduling is more appropriate for load balancing; however, the
runtime overhead must be taken into consideration. In general, parallelizing compilers
distribute loop iterations by using only one kind of scheduling algorithm, either static
or dynamic.
2.1 Static Scheduling
Traditional static scheduling [6] is applied when each loop iteration takes roughly the
same amount of time, and the compiler knows how many iterations will be run and
how many processors are available for use at compile-time. It has the advantage of
incurring the minimum scheduling overhead, but load imbalances may occur. These
static scheduling schemes including Block Scheduling, Cyclic Scheduling, Block-D
Scheduling, Cyclic-D Scheduling… etc [6]. Nevertheless, these scheduling schemes
were unsuitable in heterogeneous environment.
Theoretically, workload can be partitioned according to their computer
performance. Unfortunately, in heterogeneous system, it is difficult to evaluate each
computer performance. Intuitively, CPU clock speed may be a good evaluation value.
But it seems not enough. Many factors affect computer performance, such as the
performance capability of the CPU, the amount of memory available, the cost of
memory access, the communication medium between processors… etc [5]. Bohn and
Lamont try to evaluate the performance of computer in compiler-time [4]. In their
experiment, HINT is a good benchmark. It evaluates processor and memory
performance for any data type and returns a single value, "QUIPS". Bohn and Lamont
declared "QUIPS" can present the computer performance. It has the advantage of all
computers being working computer - no control computer is needed. But, HINT
requires hours to execute. It means this way will not be scaling well. It takes a long
time to add one more computer and if we want to change the peripheral, for example
to replace RAM from PC100 to PC133, we might have to rerun HINT.
2.2 Dynamic Scheduling
Dynamic scheduling adjusts the schedule during execution and is especially suitable
whenever the number of iterations is uncertain or each iteration may take a different
amount of time. Although it is more suitable for load balancing between processors,
runtime overhead is the cost.
We use master/slave computation patterns to model problems, i.e., the master
coordinate data distribution to the slaves, which perform computations and transmit
the results back to the master. The master is not responsible for workload, the idle
slave requests to the master for new loop iterations, and no communication occurs
between slaves. How many iterations that a slave should be assigned is a critical
issue. Improper assignment will cause bad system performance.

1082

C.-T. Yang and S.-C. Chang

Self-scheduling is a large class of adaptive/dynamic centralized loop scheduling
schemes. In a common self-scheduling scheme, p denotes the number of processors, N
denotes the total iteration and f() is a function to produce the chunk-size at each step.
At the i-th scheduling step, the master computes the chunk-size Ci and the remaining
number of tasks Ri,
R0=N,
Ci=f(i,p),
Ri=Ri-1-Ci
where f() possibly has more parameters than just i and p, such as Ri-1. The master
assigns Ci tasks to an idle slave and the load imbalancing will depend on the
execution time gap between tj, for j=1, ,p [7].

Fig. 2. A master/slave model.

Different ways to compute Ci have given rise to different scheduling schemes. The
most notable examples are as following:
Pure Self-Scheduling (PSS). This is the easiest and most straightforward dynamic
loop scheduling algorithm [8]. Whenever a processor is idle, an iteration is assigned
to it. This algorithm achieves good load balancing but also induces excessive
overhead.
Chunk Self-Scheduling (CSS). Instead of assigning one iteration to an idle
processor as in self-scheduling, CSS assigns k iterations each time, where k, called the
chunk size, is fixed and must be specified by either the programmer or the compiler
[8]. When the chunk size is one, this scheme is pure self-scheduling, as discussed
above. If the chunk size is set to the bound of the parallel loop equally divided by the
number of processors, the scheme becomes static scheduling. A large chunk size will
cause load imbalancing while a small chunk is likely to produce too much scheduling
overhead. For different partitioning schemes, we adopted CSS(s), which is a modified
version of CSS, where s means the size of chunks.
Guided Self-Scheduling (GSS). This algorithm can dynamically change the
number of iterations assigned to each processor [2]. More specifically, the next chunk
size is determined by dividing the number of remaining iterations of a parallel loop by
the number of available processors. The property of decreasing chunk size implies an
effort is made to achieve load balancing and to reduce the scheduling overhead. By
assigning large chunks at the beginning of a parallel loop, one can reduce the
frequency of communication between master and slaves. The small chunks at the end
of a loop partition serve to balance the workload across all the working processors.

A Parallel Loop Self-Scheduling on Extremely Heterogeneous PC Clusters

1083

Factoring. In some cases, GSS might assign too much work to the first few
processors, so that the remaining iterations are not time-consuming enough to balance
the workload. This situation arises when the initial iterations in a loop are much more
time-consuming than the later iterations. The Factoring algorithm addresses this
problem [1]. The assignment of loop iterations to working processors proceeds in
phases. During each phase, only a subset of the remaining loop iterations (usually
half) is divided equally among the available processors. Because Factoring assigns a
subset of the remaining iterations in each phase, it balances loads better than GSS
does when the computation times of loop iterations vary substantially. In addition, the
synchronization overhead of Factoring is not significantly larger than that of GSS.
Trapezoid Self-Scheduling (TSS). This approach tries to reduce the need for
synchronization while still maintaining a reasonable load balance [3]. TSS(Ns, Nf)
assigns the first Ns iterations of a loop to the processor starting the loop and the last Nf
iterations to the processor performing the last fetch, where Ns and Nf are both specified
by either the programmer or the parallelizing compiler. This algorithm allocates large
chunks of iterations to the first few processors and successively smaller chunks to the
last few processors. Tzen and Ni proposed TSS(N/2P, 1) as a general selection. In this
2
case, the first chunk is of size N/2p, and consecutive chunks differ in size N/8p
iterations. The difference in the size of successive chunks is always a constant in TSS
whereas it is a decreasing function in GSS and in Factoring.
Table 1 shows the different chunk sizes for a problem with the number of iteration
N=1000 and the number of processor p=4.
Table 1. Sample partition sizes
Scheme
PSS
CSS(125)
FSS
GSS
TSS

Partition size
1,1,1,1,1,1,1…
125,125,125,125,125,125,125,125
125,125,125,125,63,63,63,63,31,…
250,188,141,106,79,59,45,33,25,…
125,117,109,101,93,85,77,69,61,…

3 Our Approach and Experiment
In extremely heterogeneous environment, each slave computer in a cluster system
owns extremely different performance. In this condition, an additional slave computer
may not lead to a better performance since these known self-scheduling schemes
partition size of loop iteration according to formula instead of computer performance.
In FSS, for example, every slave gets a size of N/2p workload. If the performance
difference between the fastest computer and the slowest computer is larger than N/2p,
then load imbalance happens. Furthermore, dynamic load balancing approach should
not be aware of the run-time behavior of the applications before execution. But in
GSS and TSS, to achieve good performance, computer performance of each computer
in the cluster has to be in order in extremely heterogeneous environment, which is not
very applicable.

1084

C.-T. Yang and S.-C. Chang

A combination of different machine types is used to test the behavior of these
approaches in a heterogeneous computing environment, and the matrix multiplication
is chosen as the test application to get a heuristic result due to its regular behavior.
This experiment included four computers. One of them is assigned as master using
some self-scheduling approach to partition size of loop iteration. The master is a PC
with 300 MHz CPU speed and 208MB physical memory. Three slaves are PCs,
respectively, with 1.5GHz CPU speed and 256MB physical memory, 233 MHz CPU
speed and 96MB physical memory, and 200MHz CPU speed and 64MB physical
memory. The slaves are added into the system sequentially in this order. We
respectively use TSS and FSS approach to test matrix multiplication with different
problem sizes from 512*512, 1024*1024, to 2048*2048 by floating point operations.
Table 2 shows our experiment result. Note that just one slave in Table 2 means that all
work is done by the fastest computer only. It shows the system having two slave
computers gets worse performance than having only one slave computer. An
additional PC does not help the performance.
Table 2. The result performance of number of slaves in extremely heterogeneous environment.
No. of
slaves

Execution time (TSS)
512*512

1024*1024

1

0’12’’066

1’44’’357

17’12’’483

2

0’17’’520

2’49’’652

3

0’13’’339

1’53’’202

Execution time (FSS)

2048*2048 512*512

1024*1024

2048*2048

0’12’’136

1’44’’688

17’11’’402

19’34’’016

0’18’’371

3’16’’561

23’48’’723

16’30’’651

0’14’’543

2’00’’491

16’36’’007

For the programs with regular loops, intuitively, we may want to partition problem
size according to their CPU clock in heterogeneous environment. However, the CPU
clock is not the only factor which affects computer performance. Many other factors
also have dramatic influences in this aspect, such as the amount of memory available,
the cost of memory accesses, and the communication medium between processors…
etc[5]. Using this intuitive approach, the result will be degraded if the performance
prediction is inaccurate. A computer with largest inaccurate prediction will be the last
one to finish the assigned job.
We propose to partition problem size in two stages. At first stage, partition the %
of workload according to their performance weighted by CPU clock. In the way, the
communication between master and slaves can be reduced efficiently. At second
stage, partition following (100- )% of workload according to known self-scheduling
scheme. In the way, load balancing can be archived. With this approach, we don t
need to know the real computer performance. The computer finishing its job early
gets another larger job. The parameter
should not be too small or too big. In former
case, the dominate computer will not finish its work. In the latter case, the dynamic
scheduling strategy is rigid. In both cases, good performance can not be attained. An
appropriate
value will lead to good performance and reduce communication times.
Many
values are applied to the experiments, and
=80 result in the best
performance.

A Parallel Loop Self-Scheduling on Extremely Heterogeneous PC Clusters

1085

Algorithms MASTER and SLAVE in pseudo code:
Module MASTER
/* performs task scheduling and load balancing */
Initialization
Gather CPU clock of all slave computers
r=0;
For (i=1; i<number_slave; i++) {
Partition
% of loop iteration corresponding
their CPU clock speed and sent data to slave
r++;
}
Partition (100- )% of loop iteration into task queue
using some known self-scheduling scheme
Probe if some data in
Do{
Distinguish source and receive data
If task queue not empty
Sent other data to this idle slave
r--;
Else
sent TAG=0 to this idle slave
}while (r>0);
Finalization
END MASTER
Module SLAVE /* worker */
Initialization
Sent my CPU clock to master
Probe if some data in
While (TAG>0){
Receive initial solution and size of subtask work and
compute to find solution
Send the result to master
Probe if some data in
}
Finalization
END SLAVE

3.1 The Experiments and Results
The approach is applied in an extremely heterogeneous environment which includes
six computers. One of them is assigned as the master. The master is a PC with 300
MHz CPU and 208MB physical memory. Two of the slaves are PCs with 200 MHz
CPU and 64 MB physical memory. The other three slaves are PCs, respectively, with
233 MHz CPU and 96MB physical memory, 533MHz CPU and 128MB physical
memory, and 1.5GHz CPU and 256MB physical memory. Those computers may own

1086

C.-T. Yang and S.-C. Chang

various NIC and cost of memory access, regarding as part of computer performance.
SWAP occurs in some computers. If SWAP does not occur often, this will not affect
the result.
Table 3 and Figure 3 show the result in =80. The column name "None" stands
for "none load-balancing" and workload be partitioned just by CPU clock. "FSS/80"
stand for " =80, and remainder use FSS to partition" and so on.
Table 3 shows using this approach in 2048*2048 matrix multiplication will get
25%, 30%, 21% performance improvement than FSS, GSS, TSS respectively. Note
that in extremely heterogeneous environment, known self-scheduling get worse
performance than schemes partitioning workload merely according to the CPU clock.
Table 3. The result of our approach in extremely heterogeneous environment (

512*512
1024*1024
2048*2048

None
8.1
74.9
598.6

FSS
9.8
98.7
678.1

FSS/80
7.0
56.6
509.3

GSS/80
GSS
10.0
7.5
115.5
59.4
732.6
509.0

TSS
9.1
71.6
666.1

Fig. 3. The result of our approach in extremely heterogeneous environment (

=80).
TSS/80
7.6
63.0
521.3

=80)

Our approach is also useful in moderate environment. Following experiment
includes five computers. One of them is assigned as the master. The master is a PC
with 900 MHz CPU and 256MB physical memory. Two slaves are PCs with 600GHz
CPU speed and 256MB of physical memory. The other two slaves are PCs,
respectively, with 975 MHz CPU and 512MB physical memory, 900MHz CPU and
256MB physical memory. Table 4 and Figure 4 present the result when the approach
is applied to matrix multiplication, as
= 80. It shows that our approach has equal,
even better, performance than the known self-scheduling.

A Parallel Loop Self-Scheduling on Extremely Heterogeneous PC Clusters
Table 4. The result of our approach in moderate heterogeneous environment (

1024*1024
2048*2048
3072*3072

None
64.6
520.8
1792.8

FSS
59.1
477.3
1720.9

FSS/80
59.1
474.4
1711.0

GSS
59.1
477.6
1715.8

GSS/80
59.0
474.3
1714.7

Fig. 4. The result of our approach in moderate heterogeneous environment (

1087

=80).
TSS
65.2
505.8
1792.1

=80).

4 Conclusion and Future Work
In extremely heterogeneous environment, known self-scheduling schemes cannot
achieve good load balancing. In this paper, we propose an approach to partition loop
iterations and achieve good performance in such an environment, that is that
partitioning the 80% of workload according to their performance weighted by CPU
clock and the rest 20% of workload according to known self-scheduling. In case of
matrix multiplication with 2048*2048 problem size, our approach can obtain as much
as 30% performance improvement than GSS. Therefore, our approach is suitable in all
applications with predictable loops.
Fann, Yang, Tseng and Tsai propose a knowledge-based approach to solving loopscheduling problems [9]. A rule-based system, called IPLS, is developed by
combining a repertory grid and an attribute ordering table to construct a knowledge
base. IPLS chooses an appropriate scheduling algorithm by inferring some features of
loops and assigning parallel loops to multiprocessors to achieve significant speedup.
However, this system is based on UMA architecture. In near future, we will migrate

1088

C.-T. Yang and S.-C. Chang

IPLS to cluster architecture. Also, we will solve parallel loop scheduling problems
with unpredictable loops on extremely heterogeneous PC clusters and integrate our
approach into the new IPLS.

References
1. S. F. Hummel, E. Schonberg, L. E. Flynn, "Factoring, a Scheme for Scheduling Parallel
Loops," Communications of the ACM, Vol 35, No 8, Aug. 1992.
2. C. D. Polychronopoulos and D. Kuck, "Guided Self-Scheduling: a Practical Scheduling
Scheme for Parallel Supercomputers," IEEE Trans. on Computers, Vol 36, Dec. 1987, pp
1425–1439.
3. T. H. Tzen and L.M. Ni, "Trapezoid Self-Scheduling: A Practical Scheduling Scheme for
Parallel Compilers," IEEE Trans. on Parallel and Distributed Systems, Vol 4, No 1, Jan.
1993, pp 87–98.
4. Christopher A. Bohn, Gary B. Lamont, "Load Balancing for Heterogeneous Clusters of
PCs," Future Generation Computer Systems 18 (2002) 389–400.
5. E. Post, H. A. Goosen, "Evaluating the Parallel Performance of a Heterogeneous System," in
the Proceedings of HPCAsia2001.
6. H. Li, S. Tandri, M. Stumm and K. C. Sevcik, "Locality and Loop Scheduling on NUMA
Multiprocessors," in Proceedings of the 1993 International Conference on Parallel
Processing, Vol. II, 1993, pp. 140–147.
7. A. T. Chronopoulos, R. Andonie, M. Benche and D.Grosu, "A Class of Loop SelfScheduling for Heterogeneous Clusters," in Proceedings of the 2001 IEEE International
Conference on Cluster Computing, pp. 282–291
8. P. Tang and P. C. Yew, "Processor self-scheduling for multiple-nested parallel loops," in
Proceedings of the 1986 International Conference on Parallel Processing , 1986, pp. 528–
535.
9. Yun-Woei Fann, Chao-Tung Yang, Shian-Shyong Tseng, and Chang-Jiun Tsai, "An
intelligent parallel loop scheduling for multiprocessor systems," Journal of Info. Science and
Engineering - Special Issue on Parallel and Distributed Computing, vol. 16, no. 2, pp. 169–
200, March 2000.

