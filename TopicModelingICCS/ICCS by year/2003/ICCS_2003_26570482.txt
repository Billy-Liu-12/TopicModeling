A New Reduced Rank Square Root Kalman Filter for
Data Assimilation in Mathematical Models
Dimitri Treebushny1 and Henrik Madsen2
1

Institute of Mathematical Machines and System Problems, NAS Ukraine,
Prospekt Glushkova, 42, Kiev, Ukraine
dima@env.com.ua
2
DHI Water and Environment,
Agern Allé 11, DK-2970 Hørsholm
hem@dhi.dk

Abstract. The reduced rank square root filter is a special formulation of the
Kalman filter for assimilation of data in large scale models that represent simple
linear or complex nonlinear systems. In this formulation, the covariance matrix
of the model state is expressed in a limited number of modes. In the classical
implementation [15] some sort of normalization of the square-root matrix is required when variables of different scales are considered in the model. A new
approach is formulated that avoids the normalization step. In addition, it provides a more cost-efficient scheme and includes a precision coefficient that can
be tuned for specific applications depending on the trade-off between precision
and computational load.

1 Introduction
The question of development, choice and implementation of suboptimal Kalman filtering [12] in engineering applications has been investigated since the first work of
Kalman [6]. Suboptimal procedures save computational burden compared to a
straightforward implementation of the classical Kalman filter, which may be completely infeasible in high-dimensional models. It has been shown that the Ensemble
Kalman filter [3] and the reduced rank square-root filter (RRSQRT KF) [15] can be
used as effective Data Assimilation procedures in modern modeling software products. In this paper the RRSRQT KF is considered.
According to the RRSQRT approach the error covariance matrix is expressed in a
small number of modes, stored in a lower rank square root matrix. The number of
modes is a measure of the storage and computation time required by the filter, and
should be kept as low as possible. The RRSQRT algorithm includes a reduction part
that reduces the number of modes if it becomes too large in order to ensure that the
filter problem is feasible. However, one cannot prevent the filter from losing some
covariance information during this reduction. Additionally, when considering variables of different values of magnitude as part of the state vector some sort of normalization before truncation is needed.

P.M.A. Sloot et al. (Eds.): ICCS 2003, LNCS 2657, pp. 482–491, 2003.
© Springer-Verlag Berlin Heidelberg 2003

A New Reduced Rank Square Root Kalman Filter

483

Here a new and more appropriate truncation procedure is presented. According to
this approach one completely avoids normalization problems, and, even more, the
new truncation step needs much less computational time than the original procedure
presented in [14].
The paper is organized as follows. First an overview of the classical RRSQRT KF
is given. Then an additional procedure known as the Lanczos algorithm [4] is presented, followed by a short discussion and a presentation of the new TRUE RRSQRT
algorithm. Finally, some tests are shown that compare the TRUE RRSQRT KF and
the classical RRSQRT KF for application in the 1D radionuclide transport model
RIVTOX [17].

2 Classical RRSQRT KF
The RRSQRT KF is based on a square-root factorization of the covariance matrix
P = SS T where S is a matrix that contains a small number (q) of leading eigenvectors
s i , i = 1,..., q . The algorithm can be summarized as follows:
1. Forecast of the state vector

x kf = Φ( x ka−1 , u k )

(1)

where xk is the state vector at time step k, uk is the forcing of the system, and Φ() is
the model operator. Indices f and a denote, respectively, forecast and analysis (update). Note that a general non-linear model operator is assumed.
2. Propagation of the leading eigenvectors

s if, k =

[

]

1
Φ ( x ka−1 + ω s ia,k −1 , u k ) − Φ ( x ka−1 , u k ) , i = 1,.., q
ω

(2)

where ω is a scale factor. Heemink et al. [5] argue that ω = 1 is a suitable choice.
Smaller values of ω will increase the danger of filter divergence, whereas larger
values will provide unrealistic inputs to the dynamical model.
3. Propagation of the new system noise

s qf+i ,k =

[

1
Φ ( x ka−1 , u k , ω ε i ,k ) − Φ ( x ka−1 , u k )
ω

]

, i = 1,..., l

(3)

where εi,k is the i’th column of the square-root of the covariance matrix of the system noise Qk, and l is the number of modes, that are used to approximate the noise
covariance matrix.
4. Reduction of covariance matrix. The propagation step in (1)-(3) increases the number of columns in the error covariance matrix from q to q + l. To reduce the number of columns, and hence keep the rank of the matrix constant throughout the
simulation, a lower rank approximation of S kf is applied by keeping only the q
leading eigenvectors of the error covariance matrix. The reduction is achieved by
f

T

f

an eigenvalue decomposition of the matrix ( S k ) S k . If the state vector contains

484

D. Treebushny and H. Madsen

variables with different scales of magnitude, S kf should be normalized prior to the
eigenvalue decomposition.
5. Measurement update. The state vector is updated using the Kalman filter update
scheme. By using a sequential updating algorithm it is not necessary to calculate
the full forecast error covariance matrix, and the updating can be performed using

S kf directly. Corresponding formulae can be found in [2].

3 Reduction Step. Problem Formulation and Discussion
Mathematically, the reduction step can be formulated in the following form: calculate
in the best possible way matrix S such that
T

S ⋅ S ≅ Pk f = S kf ⋅ S kf

T

(4)

S ~ N *q
S kf ~ N * (q + l )

where
N
the dimension of the state vector, N >> 1,
q
the rank of the approximate square root matrix, q << N,
l
the rank of the approximate square root matrix of the system noise, l << N.
An elegant and effective solution of the truncation problem was suggested by the
author of the RRSQRT KF Martin Verlaan in his PhD thesis [14]. It is based on the
well-known linear algebra fact [11]: two matrices (omitting indices) S ⋅ S T (large)
and S T ⋅ S (small) have the same nonzero eigenvalues, in other words the same
leading eigenvalues. The truncation problem (4) has then a quite straightforward solution:
T

S kf ⋅ S kf = L ⋅ D ⋅ LT ,

[

]

S = S kf ⋅ L i =1,q

(5)

The first operation is a SVD decomposition of a symmetric matrix with additional
non-increasing ordering of eigenvalues in a diagonal matrix

D = diag {λ1 , λ 2 ,..., λ q + l }, λi ≥ λi +1

The second operation

[ ]i =1,q

(6)

takes only the first q columns of the corresponding

matrix.
This solution is operational and really attractive for its computational time savings.
One needs only O N * (q + l )2 + (q + l )3 operations, which is much less then

( )
3

(

)

O N in case of calculation of the full covariance matrix and its SVD decomposition (q, l << N).

A New Reduced Rank Square Root Kalman Filter

485

The algorithm works fine in cases where the state vector components are the values
of the same physical variable. The situation becomes much worse when one has to include into the state vector two or more different physical variables of different order
of magnitude. Then procedure (5) will mix all the variables together, and optimality
of the truncation procedure can not be guaranteed. A simple example explains this
situation.
Example: In a 1-D water flow simulation the modeled variables are water levels
and discharges. They have strong interrelations so they have to be included in one
state vector. The corresponding low rank approximation of the covariance matrix has

[

]T

a blocked structure S = H Q where H and Q are low rank approximations of the
covariance matrix of, correspondingly, water levels and discharges.
Usually in the case of large rivers the errors associated with water levels are of or0
1
2
3 -1
der 10 (m), while those associated with discharges could be of order 10 -10 (m s ).
The structure of the two matrices P = SS T (large) and Pˆ = S T S (small) illustrates
the problem

 HH T HQ T 
P = SS T = 

T
QQ T 
QH
Pˆ = S T S = H T H + Q T Q
In the case of P all the errors of the different variables are separated into different
blocks, while for Pˆ the first item H T H can be even dropped because of the huge dif1
2
4
ference of orders when compared to Q T Q (10 and 10 -10 , respectively). So, the
problem of a proper weighting of the components of S appears.
In [15] an extension of (5) was suggested for the case of a mixed state vector. The
extended algorithm can be written in the following form:

Sˆ = N ⋅ S kf
Sˆ T ⋅ Sˆ = L ⋅ D ⋅ LT ,

[ ]

S = Sˆ ⋅ L i =1,q

(7)

S = N −1 ⋅ S
Matrix N in (7) is a normalization matrix and provides normalization of all variables to be of approximately equal order of magnitude. The question of choice of matrix N is discussed in [1], [14], [15] and others. An interesting approach of choosing N
independently of variable units is suggested in Segers et al. [9]. Nevertheless, it has to
be mentioned that there is no universal and correct procedure of choosing the normalization matrix.
Before proceeding let us remind once again the formulation of the truncation
problem.
Truncation problem: Calculate in the best possible way matrix S such that
T

S ⋅ S ≅ Pk f = S kf ⋅ S kf

T

S ~ N * q S kf ~ N * (q + l )

(8)

486

D. Treebushny and H. Madsen

In other words the problem can be formulated as:

Calculate the small number q of leading eigenpairs of the (large) matrix

Pk f = S kf ⋅ S kf

T

The following section contains a description of an algorithm that solves approximately the same problem.

4 Lanczos Procedure
The Lanczos algorithm first appeared in the work by Lanczos [7]. The main problem
to be solved is formulated as
Find several extreme eigenpairs of a (large) real symmetric sparse matrix A
For the definition of objects see [4]. The algorithm itself can be characterized as an
iterative procedure, and, omitting details, has the following pseudo code structure:

Start with (random) vector b, compute v 0 = b / b

2

For j=1,2,… until Convergence<ε

v j = Av j −1 ;
orthogonalize (if needed)

v j to all {v0 ,..., v j −1 };

form new (tridiagonal) matrix T knowing the information about “present” and “past”;
compute eigenvalues of T which are approximate eigenvalues of A;
compute Convergence;
Compute approximate eigenpairs of A.
It has to be noted that one needs only several matrix-vector multiplications and
some vectors operations such as scalar product during the whole algorithm. The most
essential part of the procedure can be formulated as:
For the calculation of several extreme eigenpairs of a real symmetric matrix A one needs to compute several members of the sequence

v 0 = b / b 2 ,

v j = Av j −1 , j ≥ 1
with some additional vector-vector calculations.

(9)

A New Reduced Rank Square Root Kalman Filter

487

In the case of a large sparse matrix A the operation Av can be performed in a fast
way using information about the sparsity. It has to be noted, however, that sparsity is
not an essential part of the algorithm but an additional useful feature.
Additionally, the criterion of convergence has a specific precision parameter ε, so
one can vary it to obtain optimized performance taking into account the trade-off between precision and computational time.

5 The TRUE RRSQRT KF
At this point we know how the truncation problem is formulated and, in addition, we
know an effective way of solving similar problems. Before proceeding to the formulation of the new algorithm some remarks should be noted.
Remark 1: In every time step in the RRSQRT KF algorithm we have to compute the
fixed (small) number q of leading eigenvalues and corresponding eigenvectors of the
symmetric covariance matrix P * = S * ⋅ S *T , where the matrix
propagation step of the RRSQRT KF filter.

S*

is formed in the

Remark 2: The Lanczos algorithm provides the opportunity to compute a small number of leading eigenvalues and corresponding eigenvectors of a symmetric matrix A
using several matrix-vector multiplications.
Remark 3: There is no restriction on the matrix to be sparse in order to use the
Lanczos algorithm. Sparsity is used only for fast matrix-vector multiplications.
Remark 4:

(

P * v = S * ⋅ S *T v = S * ⋅ S *T v

)

Thus we can formulate the new truncation step of the TRUE (TRUncation Enhanced) RRSQRT KF algorithm:

Start with (random) vector b, compute

v0 = b / b

2

For j=1,2,… until Convergence<ε

w j = S *T v j −1 ;
v j = S *w j ;
orthogonalize (if needed)

v j to all {v0 ,..., v j −1 };

488

D. Treebushny and H. Madsen

form new (tridiagonal) matrix T knowing the information about “present” and “past”;
compute eigenvalues of T which are approximate
eigenvalues of P * = S * ⋅ S *T ;
compute Convergence;
Compute approximate eigenpairs of

P* .

All other steps – propagation and update – are similar to those of the classical
RRSQRT KF.
Several important features of the new truncation step should be noted:
• Normalization problems are completely avoided.
• The operations S *T v and S * w need a rather small amount of computational
*

time because the matrix S has a small number of elements. The number of
iterations needed for the convergence is relatively small. Overall, the algorithm is very efficient.
• The algorithm has a specific coefficient ε that regulates the precision of the
algorithm. The classical RRSQRT KF is not tunable at all.

6 Performance Tests
Some tests for evaluating the performance of the new filter were made using the
RIVTOX model. The main model equation is of an Advection-Dispersion-Reaction
type. For more details see [10], [17], [18].
The purpose of the tests is to compare the precision of the approximation of the
“true” covariance matrix of the state vector. The “true” covariance matrix was derived
from the propagation of 1000 ensemble members using the Ensemble KF methodology [3].
Two measures of precision are defined:
Tr (Ptrue − Pappr )
1. trace =
⋅ 100%
Tr Ptrue
2. norm 2 =
where
Tr P
Ptrue

Ptrue − Pappr
Ptrue

2

⋅ 100%

2

Pappr

is the trace of matrix P – the sum of all diagonal elements,
is the referred “true” system covariance matrix, computed from the statistics
of 1000 ensemble members in the corresponding Ensemble KF run,
is the approximation of the covariance matrix, Pappr = S ⋅ S T ,

S
P

is the approximation of the square root of the system covariance matrix,
is the second norm of matrix P – the square root of the sum of squares of all

2

matrix elements,

A New Reduced Rank Square Root Kalman Filter

489

%

For both measures a perfect approximation corresponds to trace = norm2 = 0.
In the tests the main variable parameter was the number of modes q for approximation of the square root matrix, and it was set to values of 5, 10, 15, 20, 25, 30 and
35. The number of modes l for approximation of the system noise covariance matrix
was fixed and set equal to 10.
For the classical RRSQRT KF the normalization according to energy suggested by
Verlaan [14] was applied.

100
80
60
40
20
0
0

5

10

15

20

25

30

35

40

rank

Fig. 1. Relative second norm norm2 of the error of covariance matrix approximation of the
TRUE RRSQRT filter (solid line) and the classical RRSQRT filter (dotted line).

200
160
120
80
40
0
0

10

20

30

40

rank

Fig. 2. Computational time [sec] of the TRUE RRSQRT filter (solid line) and the classical
RRSQRT filter (dotted line).
The results of the tests are presented in Fig.1 and Fig.2. From the figures the following can be seen:

490

D. Treebushny and H. Madsen

• For the same rank the TRUE RRSQRT provides a better approximation of the covariance matrix than the classical RRSQRT. The gain in using the TRUE RRSQRT
is more pronounced for smaller ranks and in the case of a zero initial covariance
matrix.
• The TRUE RRSQRT is computationally more efficient than the classical
RRSQRT. The classical RRSQRT requires more than twice the computational time
of the TRUE RRSQRT to obtain the same level of precision of the covariance approximation. This becomes more pronounced for larger ranks since the computational time required by the classical RRSQRT has a cubic dependence on the rank
and the TRUE RRSQRT has an almost linear dependence.

7 Conclusions
A new data assimilation algorithm has been developed – the TRUE RRSQRT
(TRUncation Enhanced Reduced Rank SQuare-RooT) Kalman filter. Some tests were
carried out using the mathematical model of nuclide pollution transport via 1-D river
network RIVTOX to evaluate the performance of the TRUE RRSQRT filter and
compare with the classical implementation of the RRSQRT. The following conclusions were obtained:
• Compared to the classical RRSQRT approach the TRUE RRSQRT filter gives a
reduction of a factor 2 or more in computation time for the same level of precision
of the approximation of the covariance matrix.
• The TRUE RRSQRT avoids normalization of the square-root matrix before truncation, which is a major problem with the classical RRSQRT filter when working
with variables of different scales.
• The TRUE RRSQRT includes a precision coefficient that can be tuned for specific
applications depending on the trade-off between precision and computational load.
The classical RRSQRT filter cannot be tuned.

Acknowledgement. This work was supported by the European Commission under the
contract No. FIKR-CT-2001-50021 "Data Assimilation for Off-site Nuclear
Emergency Management (DAONEM-CTI)".

References
1.
2.
3.

Cañizares, R., 1999, On the application of data assimilation in regional coastal models,
PhD thesis, IHE Delft and TU Delft, The Netherlands.
Chui, C.K. and Chen, G., 1991, Kalman Filtering: With Real-Time Applications, 2nd edition. Springer-Verlag, New York
Evensen, G., 1994, Sequential data assimilation with a non-linear quasi-geostrophic model
using Monte Carlo methods to forecast error statistics, J. Geophys. Res., 99(C5), 10,143–
10,162.

A New Reduced Rank Square Root Kalman Filter
4.
5.
6.
7.
8.
9.
10.

11.
12.
13.
14.
15.
16.
17.

18.

491

Golub G. and Van Loan, C, 1989, Matrix Computations, John Hopkins U. P., Baltimore.
Heemink, A.W., Verlaan, M. and Segers, A.J., 2000, Variance reduced ensemble Kalman
filtering, Report 00-03, Department of Applied Mathematical Analysis, Delft University of
Technology, The Netherlands.
Kalman, R.E., 1960. A new approach to linear filter and prediction theory. Journal of Basic Engineering 82D, 35–45.
Lanczos, C., 1950, An iterative method for the solution of the eigenvalue problem of linear
differential and integral operators. J. Res. Nat. Bureau Standards, B, 45, 225–280.
Madsen, H. and Cañizares, R., 1999, Comparison of extended and ensemble Kalman filters
for data assimilation in coastal area modeling, Int. J. Numer. Meth. Fluids, 31, 961–981.
Segers A.J., Heemink A.W., Verlaan M., van Loon M., 2000, A modified rrsqrt-filter for
assimilating data in atmospheric chemistry models, Environmental Modeling & Software,
15, 663–671.
Slavik O., Zheleznyak M., Dzuba N., Marinets A., Lyashenko G., Papush L., Shepeleva T.,
Mihaly B. Implementation of the decision support system for the river-reservoir network
affected by releases from the Bohunice NPP, Slovakia – Radiation Protection Dosimetry,
1997, v.73, No.1–4, pp.171–175
Strang, G.: Linear Algebra and Its Applications. 3rd ed. International Thomson Publishing,
(1988)
Todling, R., Cohn, S.E., 1994, Suboptimal schemes for atmospheric data assimilation
based on the Kalman filter, Mon. Wea. Rev., 122, 2530–2557.
Van Leeuwen, J. (ed.): Computer Science Today. Recent Trends and Developments. Lecture Notes in Computer Science, Vol. 1000. Springer-Verlag, Berlin Heidelberg New York
(1995)
Verlaan, M., 1998, Efficient Kalman filtering algorithms for hydrodynamic models, TU
Delft, The Netherlands.
Verlaan, M., Heemink, A.W., 1995, Reduced rank square root filters for large scale data
assimilation problems, Second International Symposium on Assimilation of Observations
in Meteorology and Oceanography, World Meteorological Organization, 247–252.
Verlaan, M. and Heemink, A.W., 1997, Tidal flow forecasting using reduced rank square
root filters, Stochastic Hydrol. Hydraul., 11, 349–368.
Zheleznyak, M, Donchytz, G., Hygynyak, V., Marinetz, A., Lyashenko, G., Tkalich, P.,
1997, RIVTOX – one dimensional model for the simulation of the transport of radionuclides in a network of river channels, RODOS Report WG4-TN(97)05, Forschungszentrum Karlsruhe, 2000 , 48 p.
Zheleznyak M.J., Tkalich P.V., Lyashenko G.B., Marinets A.V. Radionuclide aquatic dispersion model-first approach to integration into the EC decision support system on a basis
of Post-Chernobyl experience. - Radiation Protection Dosimetry, N6, 1993, pp.37–43.

