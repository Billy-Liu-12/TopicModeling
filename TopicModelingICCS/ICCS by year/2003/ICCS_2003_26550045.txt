Software Fault Tolerance:
An Overview
Jörg Kienzle
School of Computer Science, McGill University
Montreal, QC H3A 2A7, Canada
Joerg.Kienzle@mcgill.ca

Abstract. This paper presents an overview of the techniques that can be used by
developers to produce software that can tolerate design faults and faults of the
surrounding environment. After reviewing the basic terms and concepts of fault
tolerance, the most well-known fault-tolerance techniques exploiting software-,
information- and time redundancy are presented, classified according to the kind
of concurrency they support.
Keywords: Software fault tolerance, failures, concurrency, exceptions.

1 Introduction
The scope, complexity, and pervasiveness of computer-based and controlled systems
continue to increase dramatically, and hence the consequences of such systems failing
can be considerable. Ideally, the processes by which the software controlling such systems is created, analyzed, designed, implemented and tested would have come to the
point where software could be developed without errors [1].
Indeed, advances in fault avoidance (or fault prevention) techniques, such as rigorous specification of system requirements, structured design and good programming
discipline, formal methods, and software reuse, have considerably improved the potential for high quality software development. Also, fault removal techniques, such as verification and validation methods, rigorous testing, formal inspection and formal proofs,
can be used effectively to reduce the amount of faults that remain in a final software
product when it is shipped. However, even if the best people, practices, and tools are
used, it would be very risky to assume that the developed software is error-free.
Fault forecasting techniques concentrate on estimating the presence of faults in
software based on software metrics and failure data obtained during system testing or
system operation. Fault forecasting techniques can, for instance, justify the need for
additional testing, especially in safety- and mission-critical applications, which must
often guarantee a very low probability of failure.
Ultimately, preventing system failure in spite of faults remaining in the software
can be achieved by applying software fault tolerance techniques. Some of these techniques have been available for close to 30 years now and are well understood. A survey
of the most important techniques is presented in this paper. In general, software fault
tolerance techniques are categorized into design diverse and data diverse techniques.
J.-P. Rosen and A. Strohmeier (Eds.): Ada-Europe 2003, LNCS 2655, pp. 45−67, 2003.
 Springer-Verlag Berlin Heidelberg 2003

46

J. Kienzle

This overview, however, presents software fault tolerance models based on the different forms of concurrency they support. The paper is structured as follows:
Section 2 presents the most important concepts and definitions in the field of fault
tolerance. Section 3 looks at the base techniques that can be used in a sequential environment (recovery blocks, retry blocks). Section 4 presents how these techniques can
be extended to loosely coupled concurrent systems (N-version programming, N-copy
programming). Section 5 investigates techniques for addressing fault tolerance in competitive concurrent systems (transactions). Section 6 concentrates on techniques suited
for handling fault tolerance in cooperative systems (atomic actions). Finally, Section 7
presents techniques that combine features of the previous ones, and therefore support
more than one form of concurrency.

2

Fundamental Concepts

To discuss fault tolerance meaningfully, a definition of correct behavior of a program is
needed — otherwise, how could one know that something went wrong? For the purposes of fault–tolerant computing, the specification of the program is considered to be
the definition of correct program behavior: as long as the program meets its specification, it is considered correct.
A failure is the observation of an erroneous system state: an observable deviation
from the specification is considered a failure. An error is that part of the system state
that leads to a failure of the system. An error itself is caused by some defect in the system; those defects that cause observable errors are called faults. There may be defects
in the system that remain undetected; only those that manifest themselves as errors are
considered faults. Likewise, an error does not necessarily lead to a failure: it may be a
latent error [2]. Only when the error in the system state causes the system to behave in
a way that is contradictory to its specification, a failure occurs. This relationship is
illustrated in Fig. 1.

Fault

Error

Failure

Fig. 1. Fault tolerance terminology

If we look at the occurrence of faults, errors and failures over time we can define the
notions of latency and inertia, illustrated in Fig. 2.
Latency is the meantime between the fault occurrence and its initial activation as an
error. In software systems, faults are generally introduced during the development process, but they are sometimes activated only by, for example, certain input values or situations. In this case, latency may vary according to the frequency of use of the module
that contains the fault.
Inertia is the meantime of the duration between the occurrence of a failure, i.e. an
observable deviation from the specification, and the beginning of external (irrecoverable) consequences to the environment.

Software Fault Tolerance: An Overview

Internal

Fault

Error

47

External

Failure

Latency

Consequences
Inertia

Time

Fault Tolerance
Fig. 2. Latency and inertia

The goal of fault tolerance is to avoid system failure in the presence of faults. Therefore, as soon as an error has been detected, it must be corrected to avoid a later potential failure: corrective actions have to be taken to restore a correct system state.
2.1 Fault Classification
Faults can be characterized in various ways. One can consider the temporal characteristics of a fault. A transient fault has a limited duration, e.g. a temporary malfunction
of the system, or a fault due to external interference. If a transient fault occurs repeatedly, it is called an intermittent fault. In contrast, permanent faults persist, i.e. the
faulty component of the system will not work correctly again unless it is replaced.
Another way to classify faults is to consider the software life-cycle phase in which
they occur. Here, one can distinguish design faults (in particular software design
faults) from operational faults occurring during the use of the system.
2.2 Failure Semantics
Failures, i.e. deviations from a program’s specification, can manifest themselves in
various ways [3]:
•

•
•

•

A crash failure occurs when the system stops responding completely. One generally distinguishes fail–silent and fail–stop behavior: with the latter, the clients of
the system have a means to detect that it has failed.
Omission failures occur when the system does not respond to a request when it is
expected to do so.
Timing failures can occur in real–time systems if the system fails to respond within
the specified time slice. Both early and late responses are considered timing failures; late timing failures are sometimes called performance failures.
A system is said to exhibit byzantine failure semantics, if upon failure it behaves
arbitrarily [4].

These failure semantics can be organized in a hierarchy: byzantine failures are the
most general model, and subsume all others as shown in Fig. 3.
The algorithms used for achieving any kind of fault tolerance depend on the computational model, i.e. on what failure semantics we assume for the components in our
system.

48

J. Kienzle

Byzantine
Timing
Omission
Crash
Fail–Stop

Fig. 3. Failure semantics hierarchy

2.3

Redundancy

The key supporting concept for fault tolerance is redundancy. In software, redundancy
can take several forms: functional redundancy, data redundancy and temporal redundancy.
Functional redundancy aims at tolerating design faults. To the contrary of hardware fault tolerance, software design and implementation faults can not be detected
simply by replication of identical software units, since the same fault will exist and
manifest itself in all copies (provided that they all run with the same input). The idea is
to introduce diversity into the software replicas, creating different versions, variants or
alternates. These versions are functionally equivalent, i.e. based on the same specification, but internally use different designs, algorithms and implementation techniques.
Information or data redundancy includes the use of additional information that
allows to check for integrity of important data, for example error-detecting or errorcorrecting codes. Diverse data, i.e. identical data represented in different formats, also
fall into this category.
Finally, temporal redundancy involves the use of additional time to achieve fault
tolerance. Temporal redundancy is an effective way of tolerating transient faults. Provided that the temporary circumstances causing the fault are absent at a later time, simple re-execution of the failed operation will result in success. In general, most software
fault tolerance techniques add execution overhead to an application, and therefore use
additional time compared to a non-fault-tolerant application.
2.4 Error Processing
Once an error has been detected in the system state, it should be corrected to avoid a
potential system failure later on. Of course, the fault(s) causing the error also should be
treated, which means that the reason for the error must be identified and then the defect
be corrected in order to avoid that the fault causes more errors. As stated in the introduction, fault diagnosis and removal are very important steps when producing depend-

Software Fault Tolerance: An Overview

49

able software. In fault tolerance, however, we want to continue to provide system
functionality according to the specification in spite of the presence of faults.
Once an error is detected, the damage to the system state must be assessed. Measures must be taken that keep the error from propagating to other parts of the system,
preventing further damage. Once the error is “under control”, error recovery is applied,
i.e. the erroneous system state is substituted with an error-free one.
There are two base cases:
•
•

Forward error recovery attempts to construct a coherent, error–free system state by
applying corrective actions to the current, erroneous state.
Backward error recovery replaces the erroneous system state with some previous,
correct state.

Forward error recovery requires that a more or less accurate damage assessment be
made. The error must be identified in order to apply corrective actions in a way that
makes sense. This diagnosis for forward error recovery depends on the particular system. Exceptions are provided in programming languages to signal and at the same time
identify the nature of an error. Forward error recovery can be achieved through exception handling.
Backward error recovery requires that a previous correct state exists: such systems
periodically store a copy of a coherent state (sometimes called recovery point, check
point, savepoint or recovery line, depending on the recovery technique), to which they
can roll back in case of an error. Backward error recovery is a general method: because
it re–installs a previous, hopefully correct system state, it does not depend on the
nature of the error nor on the application’s semantics. Its main drawback is that it
incurs an overhead even in failure–free executions, because recovery points have to be
established from time to time.
2.5

System Structuring for Fault Tolerance

Software systems and systems in general are not monolithic; they usually consist of
several components or subsystems, and fault tolerance approaches must account for
that. Different approaches may be applied to different components. The composite
nature of systems also means that the classification of fault, error, and failure is not
absolute: a given component may perceive the failure of a sub–component as a fault
and have its own fault tolerance techniques in place to handle it.
This hierarchic model of a system gives rise to the notion of error confinement: the
system is structured in regions beyond which the effects of a fault should not propagate
undetected. This implies that a given component be accessible to other components
only through a well–defined (and preferably narrow [5]) interface. Different error confinement regions may employ different means to achieve fault tolerance. The chosen
technique depends upon the failure semantics the system component should adhere to
according to its specification, as well as on the failure semantics of its sub–components.
[6, 7] advocate structuring system execution based on idealized fault-tolerant components (see Fig. 4). The component offers services that may return replies to the component that made a service request. If a request is malformed, the component signals

50

J. Kienzle

this by raising an interface exception, otherwise it executes the request and produces a
reply. If an internal exception or local exception signaling an error occurs, error processing is activated in an attempt to handle the error. If it can be dealt with, normal processing in the component resumes; if not, the component itself signals its failure by a
failure exception. It is immaterial whether exceptions are true exceptions in the sense
of exceptions provided by programming languages or are indicated using exceptional
replies to requests. It is even possible that some entity external to the system component observes its failure and initiates appropriate error processing in the users of the
component.
Service
Request

Interface
Exception

Reply

Failure
Exception

Local Exception
Normal Processing

Error Processing

Return to normal

Service
Request

Reply

Interface
Exception

Failure
Exception

Fig. 4. Idealized fault-tolerant component

2.6

Classification of Concurrent Systems

The subsequent parts of the paper present and review the most commonly used software fault tolerance techniques, classified based on the different forms of concurrency
they support: sequential techniques, independent or loosely-coupled concurrent techniques, competitive- and collaborative concurrent techniques, and hybrid techniques.

3

Sequential Techniques

3.1

Robust Software

Robust software is often not considered part of software fault tolerance, since it does
not use any for of redundancy. However, it represents the base for achieving any form
of dependability.
Robustness is defined as “the extent to which software can continue to operate correctly despite the introduction of invalid inputs” [8]. Invalid inputs must be defined in
the specification. They include out of range inputs, inputs of the wrong type or format,
corrupted inputs, wrong sequencing of input, and violations of pre-conditions.
Upon detection of such invalid input, several optional courses of action may be
taken: requesting new input from the input source, using the last acceptable value, or
using a pre-defined default value.

Software Fault Tolerance: An Overview

3.2

51

Recovery Blocks

Recovery Blocks [9, 10] have been introduced as a software structuring mechanism
providing software fault tolerance based on the concept of design diversity and backward error recovery. A recovery block consists of one or more algorithms called alternatives that implement the same functionality, coupled with an acceptance test that
determines whether an alternative has functioned correctly or not. Under normal conditions, only the first alternative is executed. Only if an error is detected by the acceptance test, other alternatives are tried successively. The typical syntax for expressing
recovery blocks is as follows:
ensure Acceptance Test
by Primary Alternate
else by Second Alternate
...
else by N’th Alternate
else signal Failure

It is important to note that prior to execution of the primary alternate, a snapshot of
the current state of the component is taken. When an alternative fails to pass the acceptance test, this state is restored, and the next alternative is tried. This is repeated until
either an alternative succeeds, or there are no more alternatives available. In that case,
the execution of the component is considered a failure, and the failure is signaled to the
outside.
The overhead introduced by the recovery block scheme in fail-free mode is fairly
low. It comprises establishing a checkpoint and running the acceptance test. However,
every failure of an alternate requires restoring the checkpoint, executing another alternate and running the acceptance test again. Although unlikely, the potential overhead is
huge. Also, “infinite loop” type of errors can not be detected easily.
In order to make it possible to use recovery blocks in real-time applications, the
base scheme has been extended to include a watchdog timer that interrupts the recovery block execution after a specific time. The execution of the watchdog version of
recovery blocks is illustrated in form of a UML state diagram in Fig. 5.

Establish
Checkpoint /N=1;
Reset
Watchdog

Execute
Alternate N

Evaluate
AT
[AT fail; N<max]

[AT succ]

Discard
Checkpoint

Restore
/N:=N+1 Checkpoint
[AT fail; N=max]

[Watchdog expires]

Discard
Checkpoint /signal failure
Fig. 5. Recovery block execution

52

J. Kienzle

3.3 Retry Blocks
Retry Blocks are the data diverse complement of recovery blocks [11]. They are based
on the assumption that although an algorithm might fail on some specific input, it
might succeed on related input, i.e. slightly different input that still produces acceptable results. Instead of relying on a secondary algorithm in case the primary one fails,
retry blocks use a data re-expression algorithm (DRA) to slightly modify the input,
and run the primary algorithm again. Apart from this, the retry blocks follow the same
idea as recovery blocks, i.e. they rely on an acceptance test for determining validity of
results, and on backward error recovery to restore a consistent application state in case
of failure.
The syntax for expressing retry blocks is shown below:
ensure Acceptance Test
by Primary Algorithm (Original Input)
else by Primary Algorithm (Re-expressed Input)
...
... [Deadline expires]
else by Backup Algorithm (Original Input)
else signal Failure

The execution is shown in the UML state diagram of Fig. 6.

Establish
Execute Primary
Evaluate
Checkpoint /N=1;
Algorithm
AT
Reset
[AT fail]
/N:=N+1
Watchdog
Re-express
Restore
Input
[N<max] Checkpoint
[Watchdog expires]
Restore
Checkpoint

[AT succ]

Discard
Checkpoint

[N=max]
Execute Backup
Algorithm

Evaluate
AT

[AT succ]

[AT fail]
Discard
Checkpoint /signal failure
Fig. 6. Retry block execution

3.4

Acceptance Test

Central to both techniques, recovery blocks and retry blocks, is the acceptance test. It
examines the system state in order to verify that the system’s behavior is acceptable.
Acceptance tests have to be:
•
•

Simple, in order to keep run-time overhead reasonable,
Effective, to ensure that anticipated faults are detected, and to ensure that nonfaulty behavior is not incorrectly rejected,

Software Fault Tolerance: An Overview

•

53

Highly reliable, to reduce the chance of introducing additional design faults.

Acceptance tests are difficult to develop. Comprehensive tests have a better chance of
detecting errors, but create more run-time overhead and, in general, are more complex
and hence more likely to introduce new faults into the system. Also, it is not always
clear if an acceptance test should test for what a program should do or for what a program should not do.
Testing for satisfaction of the requirements stated in the specification often requires
a computation as complex as the one performed by the algorithm to be tested. Moreover, if the acceptance test uses a similar algorithm than the one that is to be tested,
chances of common-mode failures are increased. However, in certain situations, satisfaction of requirement test can be very efficient. For example, some mathematical
operations, such as calculating the square root, can be verified by applying the inverse
operation.
Often, testing for what a program should not do is simpler and provides a higher
degree of independence between the acceptance test and the algorithm to be tested.
Accounting tests, e.g. checksums, are well suited for data-manipulating applications
with simple mathematical operations. Reasonableness tests or testing for violations of
safety conditions are particularly suitable for process control systems. The tests can be
based on physical constraints or rate change of values. Finally, run-time tests, i.e. testing for divide-by-zero, over- or underflow, end-of-file, and similar conditions, present
the most cursory class of acceptance test.

4

Independent Concurrent Systems

If several processing nodes are available, executing multiple versions of an algorithm
does not require additional time anymore, since the alternates can execute in parallel.
Also, since each version has it’s own copy of the state, there is no need to perform
backward error recovery. The N-version programming and N-copy programming techniques exploit these facts.
4.1

N-Version Programming

The N-Version Programming scheme [12, 13] is a design diverse scheme similar to the
recovery block scheme. The main difference is that the different versions execute concurrently (often in separate processes on separate processors). Once all versions have
completed their execution, the results are compared, and the final result is determined
using a decision mechanism.
The general syntax is given below:
run Version 1, Version 2, ... , Version n in parallel
if Decision Mechanism (Result 1, Result 2, .., Result n)
then
return Determined Result
else signal Failure

The execution of the n-version programming scheme is illustrated in the UML state
diagram shown in Fig. 7.

54

J. Kienzle

Execute
Version 1
Execute
Version 2

Distribute
Input

[succ]/return result
Adjudicate
Results

...
[fail]/signal failure
Execute
Version N
Fig. 7. N-version program execution

4.2

N-Copy Programming

N-Copy Programming [14] is the data diverse complement of n-version programming.
In the n-copy scheme, the n processes all run the same algorithm, but each one using
slightly different input.
The general syntax is given below:
Algorithm (DRA 1(original input)),
Algorithm (DRA 2(original input)), ... ,
Algorithm (DRA n(original input)) in parallel
if Decision Mechanism (Result 1, Result 2, .., Result n)
then
return Determined Result
else signal Failure
run

The original input is distributed to the n processes, which each start by executing a different data re-expression algorithm to transform the input. Then, they all run the same
algorithm concurrently. The results of each version are provided to the decision making mechanism, which determines if a correct result can be adjudicated. If yes, then the
result is returned, if not, a failure is signaled to the outside.
4.3

Decision Mechanism

In both techniques, N-version programming and N-copy programming, multiple
results are calculated concurrently. The task of the decision mechanism is to determine
if one of the results can be considered correct. This is actually not that easy.
First of all, there might be multiple correct results (MCR). For instance, if the problem is calculating a root of an n-th order equation, then there are n different correct
answers. Then there is the problem of limited floating point arithmetic precision
(LFPA). Diverse algorithms for solving a given floating point problem might calculate
results that vary slightly, but are within a given tolerance. This issue, combined with
different execution paths taken by different variants results in a more fundamental
problem called the consistent comparison problem (CCP). The difficulty is that, if n

Software Fault Tolerance: An Overview

55

versions operate independently, then whenever the specification requires that they perform a comparison, it is not possible to guarantee that the versions will all make the
same decision, i.e. make comparisons that are consistent [15]. It has been shown that,
without communication among the variants, there is no solution for the consistent
comparison problem.
Different decision algorithms have been developed, all having their strengths and
weaknesses. A straightforward voter is the exact majority voter [16], which considers a
result correct if a majority of versions agree. It is best used in situations where the
results are discrete values. Unfortunately, it is very vulnerable to MCR and LFPA, and
cannot be used in data diverse schemes that use approximate data re-expression algorithms. Other similar voters include the mean voter and the consensus voter [17].
Other voters have been developed, such as the median voter, which selects the
median of the results provided by the versions. This can, or course, only be done with
ordered values.
Finally, tolerance values can be added to voters in order to address LFPA. In this
case, results that are within a pre-defined tolerance value ε are considered identical.

5

Competitive Concurrent Systems

Competitive concurrency exists when two or more processes are designed separately,
are not aware of each other, but share some resources. Programmers of such processes
would like to live in an artificial world in which they do not have to care about other
concurrent activities. They want to access objects as if they had them at their exclusive
disposal.
Moreover, a fault of one process should not affect other processes running in the
system. The ability to isolate the processes from each other is crucial for achieving
reliability in such systems. Since the processes have been designed separately, they
obviously can not anticipate any faults that might originate from others. Systems in
which erroneous state from one process can propagate to other concurrent running processes might lead to disastrous results.
5.1 Transactions
Transactions [18] are the base technique for structuring the execution of competitive
concurrent systems. A transaction groups an arbitrary number of operations on one or
more data objects together, making the whole appear indivisible with respect to other
concurrent transactions. Transactions guarantee the so-called ACID properties: Atomicity, Consistency, Isolation and Durability [18].
Transactions rely on backward error recovery to provide fault tolerance. The
boundaries of a transaction are marked with two of the three standard operations:
begin, commit and abort. After beginning a new transaction, all update operations on
data objects are made on behalf of that transaction. At any time during the execution of
the transaction it can abort, which means that the state of the system is restored to the
state at the beginning of the transaction. An abort can be explicitly requested by the
application, or automatically triggered due to some failure in the system (e.g. a remote
object on some other machine can not be contacted due to a network failure). By committing a transaction, the application states that the updates to the system state are

56

J. Kienzle

completed and the resulting state is error-free. From that point on, the state changes
made on behalf of the transaction become permanent and are made visible to the outside.
Flat transactions, illustrated in Fig. 8, represent the simplest type of transaction.
They are called flat, because there is only one layer of control available to the application programmer. Every statement inside the transaction is at the same level; that is, the
transaction will either survive together with all modifications made to data objects on
behalf of the transaction, or it will be rolled back, which means that all changes made
to transactional objects will be undone.
The example in Fig. 8 represents a transaction that performs a money transfer from
bank account A to bank account B. Both bank accounts are shared data objects that
might be concurrently accessed by other processes running in the system. After starting the transaction, the amount of money to be transferred is first withdrawn from
account A, then the amount is deposited on account B. If no problems are encountered,
the transaction commits.
Account B
Deposit (Amount)
Thread

Transaction Begin

Withdraw (Amount)

Transaction Commit

Account A

Fig. 8. A flat transaction

Banking systems extensively use transactions, and their importance is nicely illustrated
by the transfer example. Without an enclosing transaction, a failure occurring after the
withdraw operation has been completed on account A, but before the deposit operation
has begun on account B, may result in the loss of the amount of money being transferred. Such a situation is not acceptable.
5.2 Transaction Model Extensions
Flat transactions have been extended in many ways, e.g. in order to provide more finegrained rollback control. Extended models include flat transactions with savepoints,
chained transactions, split and joint transactions [19], recoverable communicating
actions [20], and Sagas [21].
One of the most important extension from the point of view of fault tolerance is the
nested transaction model [22]. In the nested transaction model, a transaction is allowed
to start subtransactions, thereby creating a hierarchy of transactions in form of a tree.
The transaction at the root of the tree is called the top-level transaction. The transactions at the leaf level are flat transactions.

Software Fault Tolerance: An Overview

57

It is important to note that leaf-level subtransactions are not fully equivalent to classical flat transactions. The key point is that the properties of such transactions are valid
only within the confines of the surrounding parent transaction. Leaf-level subtransactions are atomic from the perspective of the parent transaction, they preserve consistency with respect to the local function they implement; they are isolated from all other
activities inside and outside the parent transaction. An immediate consequence of the
commit rules is that subtransactions are not durable, since their changes are only made
persistent when the top-level transaction commits.
5.3 Transactions and Software Fault Tolerance
The notion of transaction has first been introduced in database systems in order to correctly handle concurrent updates of data and to provide fault tolerance with respect to
hardware failures [18]. However, transactions can also be applied in the context of
software fault tolerance.
To begin with, transactions provide backward error recovery thanks to the atomicity property and the possibility of aborting voluntarily. Next, the isolation property
hides state changes made on behalf of a transaction from other processes until the
transaction commits. Therefore, potential errors are also confined within the transaction boundaries, and cannot propagate to the outside and spread to other processes.
Finally, the durability property ensures that committed results remain available in the
future in spite of any subsequent failures.
According to the rules, a transaction must be written to preserve consistency. Each
transaction expects a consistent state when it starts, and recreates that consistency after
making its modifications, provided it runs to completion. In a sense, the transaction is
responsible for making sure that there are no errors in the state of the application that it
has modified before committing the transaction. This can be achieved by using an
acceptance test (see Section 3.4) at the end of every transaction. If an error is detected,
the application can either choose to perform manual forward error recovery, or it can
recover the initial consistent state by aborting the transaction.
Transactions combined with structured exception handling can be even more powerful. Exceptions signal abnormal situations during an application execution, i.e.
potential erroneous state that must be addressed in order to guarantee consistency. The
idea is to make transactions exception handling contexts. As long as an exception can
be dealt with inside a transaction there is no danger. However, if an exception crosses a
transaction boundary unhandled, then potential erroneous state might exist. To be on
the safe side, such unhandled exceptions should be treated as an abort vote, i.e. a command to the transaction support to rollback all changes and re-establish the consistent
state as it was at the beginning of the transaction.
Integrating transactions and exceptions also makes it possible to design and implement so-called self-checking transactional objects [23]. For such objects, methods are
decorated with pre- and post-conditions. When an invariant, a pre- or a post-condition
is violated by the execution of a method, an exception is propagated to the caller. The
caller must then handle this exception in order to address a potential inconsistency. If
handling fails, i.e. the exception propagates outside of the transaction context, the

58

J. Kienzle

transaction is aborted, and all the changes made to transactional objects on behalf of
the transaction are undone.
To make transactions a general technique for achieving software fault tolerance, the
nested transaction model should be used. Nested transactions combined with exception
handling make it possible to recursively structure the execution of an application into
error confinement regions, following the idealized fault tolerant component approach
described in Section 2.5.

6

Cooperative Concurrent Systems

Cooperative concurrency exists when several processes cooperate, i.e. do some job
together and are aware of this. They can communicate by resource sharing or explicitly. They have been designed together. They cooperate to achieve their joint goal and
use each other’s help and results.
6.1 Conversations
The concept of a Conversation has been introduced by [24] to structure the execution
of a set of collaborating processes. A fixed number of processes enter a conversation
asynchronously; a recovery point is established in each of them. They freely exchange
information within the conversation but cannot communicate with any outside process
(violations of this rule are called information smuggling). When all processes participating in the conversation have come to the end of the conversation, their acceptance
tests are to be checked. If all tests have been satisfied, the processes leave the conversation together. Otherwise, they restore their states from the recovery points and may try
and execute a different alternate. By providing different alternates based on different
algorithms that produce the same result, conversations can tolerate software design
faults just as recovery blocks.
The occurrence of an error in a process inside a conversation requires the rollback
of all (and only) the processes in the conversation to the checkpoint established upon
entering the conversation. Conversations may be nested freely, meaning that any subset
of the processes involved in a conversation at nesting level i may enter a conversation
at nesting level i + 1 [25].
6.2 Atomic Actions
Later on, the conversation scheme has been augmented with additional support for forward error recovery and exception resolution. The resulting model is named Atomic
Actions [26, 6].
The structure of an atomic action is represented in Fig. 9. A fixed number of participants (threads, processes or active objects) enter an action and cooperate inside it to
achieve joint goals. They share work and explicitly exchange information in order to
complete the action successfully.
Atomic actions structure dynamic system behavior. To guarantee action atomicity,
no information is allowed to cross the action border. Actions can be nested, meaning
that a subset of the participants of the containing action can enter a nested action. The
number of participants of an atomic action is fixed in advance, and hence no dynamic

Software Fault Tolerance: An Overview

Asynchronous Entry

Exception Y raised

Exception Y raised in
All Participants

59

Acceptance Test
Synchronous Exit

Process A
Y

Process B
Process C

Y

Exception
Resolution Y
Direct Communication
X

Cooperative Handling
Of Exception Y

Y

Process D
Y

Exception X raised

Fig. 9. An atomic action with coordinated exception handling

creation of threads is allowed. Participants leave the action together when all of them
have completed their job.
Atomic actions have very detailed exception handling rules. A set of internal and
external exceptions is associated with each atomic action, and these exceptions are
clearly separated. The model is recursive, and all external exceptions of an action are
viewed as internal ones of the containing action. Each participant of the action has a
set of handlers for all internal exceptions. In this approach, action participants cooperate not only when they execute program functions (i.e. during normal activity) but also
when they handle abnormal events. This is mainly due to the fact that when an atomic
action is executed, an error can spread to all participants, and the system can be
returned into a consistent state only if all participants are involved in handling. This is
why, when an exception is raised in any participant, appropriate handlers are initiated
in all of them. An action can be completed either normally (without raising any internal exceptions or after a successful cooperative handling of such exceptions) or by signaling an external exception to the context of the containing action. Concurrent
internal exceptions are resolved using a resolution graph, and handlers for the resulting
exception are called in all participants as illustrated in Fig. 9.

7

Hybrid Systems

This section covers some of the most important techniques that can handle different
forms of concurrency. The next two subsections present coordinated atomic actions
and open multithreaded transactions, two models that support competitive and cooperative concurrency. The remaining sections cover other advanced models, mainly extensions of recovery blocks or n-version programming, that combine the ideaa of
sequential and loosely coupled concurrent models.
7.1 Coordinated Atomic Actions
The developers of the Coordinated Atomic Action (CA action) concept [27] have
defined a model that fully integrates cooperative and competitive concurrency. They
have extended the atomic action concept by allowing participants of an atomic action
to access external objects. Atomic actions are used to control cooperative concurrency
and to implement coordinated error recovery, whilst external objects are accessed

60

J. Kienzle

using transactions in order to maintain the consistency of shared resources in the presence of failures and competitive concurrency.
Each CA action is designed as a stylized multi-entry procedure with roles which
are activated by action participants cooperating within the CA action. Logically, the
action starts when all roles have been activated and finishes when all of them reach the
action end. CA actions can be nested. The state of the CA action is represented by a set
of local and external objects. External objects can be used concurrently by several CA
actions in such a way that information cannot be smuggled among them. Any sequence
of operations on these objects bracketed by the start and completion of the CA action
has the ACID properties with respect to other sequences. The execution of a CA action
looks like a transaction for the outside world. Action participants explicitly cooperate
(interact and coordinate their executions) through local objects.
All participants are involved in recovery if an error is detected inside a CA action.
Conceptually it makes no difference which of them detects the error. The whole CA
action represents the recovery region. Exception handling in CA actions is very similar
to the one found in atomic actions: all action participants are involved in cooperative
handling of any internal exception, internal exceptions raised concurrently are resolved
and external exceptions are explicitly propagated by action participants. Exception
handling in CA actions explicitly deals with local and transactional objects [28].
The CA action interface can contain one or more abort exceptions, a predefined
failure exception and a number of exceptions corresponding to partial (committed and
consistent) results which the action can provide. In the latter case, it uses external
exceptions to inform the containing action of the fact that it has not been able to produce a complete required result and, indirectly, of the state in which objects have been
left and of the available partial results. If one of the participants signals an abort interface exception, the CA action is aborted; all modifications made to transactional
objects are undone and all local objects destroyed. Note that to improve performance,
local objects can simply be re-initialized if software diversity or retry are used for
recovery. A failure interface exception is signalled by the support when some serious
problems are encountered. This might happen if, for example, the support cannot abort
or commit the states of transactional objects. When an external exception corresponding to a partial result is signalled to the outside of an action, the state of all transactional objects is committed before raising this exception in the containing context. In
all these cases signaling an interface exception means that the responsibility for dealing with the abnormal event is passed to a higher level in the system structure. At this
level, detailed information about the current system state and the reasons for the exception occurrence can be determined based on the identity of the exception, optional output parameters and the post-conditions associated with the exception.
7.2 Open Multithreaded Transactions
The Open Multithreaded Transaction model [29] is a transaction model that provides
features for controlling and structuring not only accesses to objects, as usual in transaction systems, but also threads taking part in transactions. The model allows several
threads or processes, here called participants, to enter the same transaction in order to
perform a joint activity. It provides a flexible way of manipulating threads executing

Software Fault Tolerance: An Overview

61

inside a transaction by allowing them to be forked and terminated, but it restricts their
behavior in order to guarantee correctness of transaction nesting and isolation among
transactions.
Within an open multithreaded transaction, threads can access a set of transactional
objects. Although individual threads evolve independently inside an open multithreaded transaction, they are allowed to collaborate with other threads of the transaction by accessing the same transactional objects. The transactional objects therefore
must preserve data consistency despite concurrent accesses from within a transaction,
and at the same time provide isolation among concurrent accesses from different transactions.
The open multithreaded transaction model incorporates disciplined exception handling adapted to nested transactions. It allows individual threads to perform forward
error recovery by handling an abnormal situation locally. If local handling fails, the
transaction support applies backward error recovery and reverses the system to its “initial” state.
The model distinguishes internal and external exceptions. The set of internal
exceptions for each participant consists of all exceptions that might occur during its
execution. There are three sources of exceptions inside an open multithreaded transaction:
•
•
•

An internal exception can be raised explicitly by a participant;
An external exception raised inside a nested transaction is raised as an internal
exception in the parent transaction;
Transactional objects accessed by a participant of a transaction can raise an exception to signal a situation that violates the consistency of the state of the transactional object (see self-checking transactional objects in Section 5.3).

All these situations give rise to a possibly inconsistent application state. If a participant
does not handle such a situation, the application’s correct behavior can not be guaranteed.
A participant must therefore provide handlers for all internal exceptions. If such a
handler is not able to deal with the situation, it can signal an external exception. If a
participant “forgets” to handle an internal exception, the external exception
Transaction_Abort is signaled, and the application consistency is restored by
aborting the transaction.
If any participant of a transaction signals an external exception, the transaction is
aborted, the exception is propagated to the containing context, and the exception
Transaction_Abort is signalled to all other participants. If several participants signal an external exception, each of them propagates its own exception to its own context.
7.3 N-Version Programming Variants
Acceptance Voting [30] is an example of a minor extension of n-version programming.
In this scheme, only results that pass an initial acceptance test are voted upon.
The N-Version Programming with Tie Breaker and Acceptance Test [31] technique
strives for better performance by just comparing the results of the two fastest versions.

62

J. Kienzle

If they match, then the result is immediately output. However, if they do not agree,
then all the results are voted upon. The final result of the voter is tested by means of an
acceptance test.
7.4 Distributed Recovery Blocks
Distributed Recovery Blocks [32, 33] provide hardware and software fault tolerance
specifically targeted to real-time applications. A distributed recovery block executes
simultaneously on two processing nodes that are structured as a primary-shadow pair.
The general syntax is given below:
ensure Acceptance Test on Node 1 or Node 2
by Primary on Node 1 or Alternate on Node 2
else by Alternate on Node 1 or Primary on Node 2
else signal Failure

First the input data is distributed to both nodes. Then one node starts executing the
primary algorithm, whereas the other node starts with the alternate algorithm. If the
primary result fails the acceptance test, then the alternate result is tested (forward error
recovery). Only if both fail, then backward error recovery rolls back the state, and the
roles are inverted, e.g. the first node executes the alternate, while the second one now
tries the primary. Each node also contains two watchdogs: one that monitors the execution of the local algorithm, and one that monitors the execution of the alternate algorithm on the other node.
7.5 Consensus Recovery Blocks
The Consensus Recovery Block technique [34] combines recovery blocks and n-version programming ideas. It uses n variants that are ranked in order of their service and
reliability. The n variants are first run in n-version programming fashion, and the
results are checked by a voter. If, however, the voter can not determine a correct result,
then the result of the highest ranked variant is submitted to the acceptance test. If this
fails, then the next highest ranked variant’s result is tested, and so on.
It is claimed that the consensus recovery block reduces the importance of the
acceptance test used in recovery blocks and is able to handle cases where n-version
programming would not be appropriate because of multiple correct results (see
Section 4.3).
7.6 Two-Pass Adjudicators
The Two-Pass Adjudicators technique [35] combines data and design diverse software
fault tolerance. The first pass uses n-version programming, meaning that the different
alternates are executed concurrently and their results are passed to the voter. If, however, the voter can not determine a result, then the input data is re-expressed and the
algorithms are executed again with the new input. The two-pass adjudicator technique
can, in some situations, handle multiple correct results.

Software Fault Tolerance: An Overview

7.7

63

Self-Configuring Optimal Programming

Self-Configuring Optimal Programming [36] attempts to reduce the cost of fault-tolerant software in terms of space and time redundancy by providing a flexible architecture. The trade-off between dependability and efficiency can be dynamically adjusted
at run-time.
In short, self-configuring optimal programming selects a set of variants to be run in
the first phase according to how many results are needed to make a decision, and
according to how much processing power is available. If no result can be determined
during the first phase, then additional new variants are executed. This goes one until
either a satisfying result has been determined, or until all versions have been unsuccessfully tried.

8

Conclusion and Discussion

This paper presented an overview of the most important techniques that can be used by
developers to produce software that can tolerate design faults and faults of the surrounding environment. The different techniques have been classified with respect to
the kind of concurrency they support.
Although some of the techniques presented here have been available for close to 30
years, software fault tolerance is still not used extensively in current software development, except in mission- and safety-critical systems. There are several reasons for this.
As we have seen thoughout the examples in this paper, software fault tolerance
requires additional resources, such as processing power, memory, time, etc. Design
diversity requires additional design and implementation efforts. In short, addressing
fault tolerance in software increases the development cost.
This goes together with the fact that fault tolerance is often considered a non-functional requirement during the development of an application. As a result, it is not
addressed properly during the analysis and design phases. This leads to poor integration of fault tolerance, since it is too late to make use of structured fault tolerance models once the implementation phase has begun. There still is a lot research and
educational work to be done in order to make fault tolerance an integral part of software development processes.
Finally, modern programming languages do not provide any direct software fault
tolerance support. Programmers have to manually implement the techniques, using the
basic building blocks offered by programming languages, such as exceptions, threads,
synchronization means, object-orientation, information hiding, controlled copying,
and serialization. The interaction among these features, however, can be very subtle,
which makes the development of software fault-tolerant applications complicated and
error-prone.
One way to avoid this problem is to develop middleware that provides fault tolerance support. This can be done successfully in certain situations, as demonstrated by
transactional middleware such as the CORBA Object Transaction Service [37] or
Enterprise Java Beans [38]. However, if tight interaction between the application and
the fault tolerance infrastructure is required, such a separation is artificial, results in
complex interfaces, and might result in poor performance.

64

J. Kienzle

Ada [39] is a language that has been used extensively in safety- and mission-critical systems. It has a reputation of being highly reliable, and it is therefore not surprising that there have been many Ada-based implementations of software fault tolerance
schemes, such as checkpointing and recovery cache support [40], shared recoverable
objects [41], atomic actions [42, 43], coordinated atomic actions [44], and open multithreaded transactions [45]. All schemes follow a somehow similar approach. They provide a library or framework of reusable components with well-defined interfaces to the
application programmer, and define rules and programming conventions of how to use
them in a safe way. The elegance of the interfaces, however, depends heavily on the
features offered by the programming language. Unfortunately, some characteristics of
Ada, such as rules for limited types, access discriminants, and controlled types, lead to
rather complicated interfaces.
Programming languages that provide powerful features such as reflection [46]
make it possible to build nicer interfaces, as shown for instance in [47]. Also,
approaches using the upcoming aspect-oriented programming paradigm [48] show
promising results [49].

References
[1]
[2]

[3]
[4]
[5]
[6]
[7]
[8]
[9]

[10]
[11]

[12]

Pullum, L.L.: Software Fault Tolerance - Techniques and Implementation, Artech House,
Boston, 2001.
Laprie, J.-C.: “Dependable Computing and Fault Tolerance : Concepts and Terminology”, in Proceedings of the 15th International Symposium on Fault–Tolerant Computing
Systems (FTCS–15), pp. 2–11, Ann Arbour, MI, USA, June 1985.
Cristian, F.: “Understanding Fault–Tolerant Distributed Systems”, Communications of
the ACM 34(2), February 1991, pp. 56–78.
Lamport, L.; Shostak, R.; Pease, M.: “The Byzantine Generals Problem”, ACM Transactions on Programming Languages and Systems 4(3), pp. 382–401, 1982.
Kopetz, H.: Real–Time Systems – Design Principles for Distributed Embedded Applications, Kluwer Academic Publishers, 1997.
Lee, P.A.; Anderson, T.: “Fault Tolerance – Principles and Practice”, in Dependable
Computing and Fault-Tolerant Systems, Springer Verlag, 2nd ed., 1990.
Randell, B.; Xu, J.: The Evolution of the Recovery Block Concept, chapter 1, pp. 1–21, in
Lyu, M.R. (Ed.): Software Fault Tolerance, John Wiley & Sons, 1995.
IEEE Standard 729-1982: “IEEE Glossary of Software Engineering Terminology”,
1982.
Horning, J.J, et al.: “A Program Strucure for Error Detection and Recovery”, in E.
Gelenbe and C. Kaiser (eds.), Lecture Notes in Computer Science 16, pp. 171–187,
Springer, 1974.
Randell, B.: “System Structure for Software Fault Tolerance”, IEEE Transactions on
Software Engineering SE-1(2), pp. 220–232, 1975.
Ammann, P.E.; Knight, J.C.: “Data Diversity: An Approach to Software Fault Tolerance”, Proceedings of the 17th International Symposium on Fault–Tolerant Computing
Systems (FTCS–17), Pittsburgh, PA, pp. 122–126, 1987.
Elmendorf, W.R.: “Fault Tolerant Programming”, Proceedings of the 2nd International
Symposium on Fault–Tolerant Computing Systems (FTCS–2), Newton, MA, pp. 79–83,
1972.

Software Fault Tolerance: An Overview
[13]

[14]
[15]

[16]
[17]

[18]
[19]

[20]

[21]
[22]
[23]

[24]
[25]

[26]
[27]

[28]

[29]

65

Chen, L. and Avizienis, A.: “N-Version Programming: A Fault Tolerance Approach to
Reliability of Software Operation”, Proceedings of the 8th International Symposium on
Fault–Tolerant Computing Systems (FTCS–8), Toulouse, France, pp. 3–9, 1978.
Ammann, P.E.; Knight, J.C.: “Data Diversity: An Approach to Software Fault Tolerance”, IEEE Transactions on Computers 37(4), pp. 418–425, 1988.
Brilliant, S.S.; Knight, J.C.; Leveson, N.G.: “The Consistent Comparison Problem in NVersion Software”, IEEE Transactions on Software Engineering 15(11), pp. 1481–1485,
1989.
Avizienis, A.: “The N-Version Approach to Fault-Tolerant Software”, IEEE Transactions on Software Engineering SE-11(12), pp. 1491–1501, 1985.
Vouk, M.A. et al.: “An Empirical Evaluation of Consensus Voting and Consensus Recovery Block Reliability in the Presence of Failure Correlation”, Journal of Computer and
Software Engineering 1(4), pp. 367–388, 1993.
Gray, J.; Reuter, A.: Transaction Processing: Concepts and Techniques. Morgan Kaufmann Publishers, San Mateo, California, 1993.
Pu, C.; Kaiser, G.E.; Hutchinson, N.C.: “Split-Transactions for Open-Ended Activities”,
in 14th International Conference on Very Large Data Bases, pp. 26–37, Los Angeles,
California, Morgan Kaufmann, 1988.
Vinter, S.; Ramamritham, K.; Stemple, D.: “Recoverable Actions in Gutenberg”, in Proceedings of the 6th International Conference on Distributed Computing Systems,
pp. 242–249, Los Angeles, Ca., USA, IEEE Computer Society Press, 1986.
Garcia-Molina, H.; Salem, K.: “SAGAS”, in Proceedings of the SIGMod 1987 Annual
Conference, pp. 249–259, San Francisco, CA, ACM Press, May 1987.
Moss, J. E. B.: Nested Transactions, An Approach to Reliable Computing. PhD Thesis,
MIT, Cambridge, April 1981.
Kienzle, J.; Strohmeier, A.; Romanovsky, A.: “Auction System Design Using Open Multithreaded Transactions”. In Proceedings of the 7th IEEE International Worshop on
Object-Oriented Real-Time Dependable Systems (WORDS’02), San Diego, CA, USA,
January 7th–9th, 2002, pp. 95–104, IEEE Computer Society Press, Los Alamitos, California, USA, 2002.
Randell, B.: “System Structure for Software Fault Tolerance”, IEEE Transactions on
Software Engineering 1(2), pp. 220–232, 1975.
Strigini, L.; Giandomenico, F.D.; Romanovsky, A.: “Coordinated Backward Recovery
between Client Processes and Data Servers”, IEEE Proceedings – Software Engineering
144(2), pp. 134–146, April 1997.
Campbell, R.H.; Randell, B.: “Error Recovery in Asynchronous Systems”, IEEE Transactions on Software Engineering SE-12(8), pp. 811–826, August 1986.
Xu, J.; Randell, B.; Romanovsky, A.; Rubira, C.M.F.; Stroud, R.J.; Wu, Z.: “Fault Tolerance in Concurrent Object-Oriented Software through Coordinated Error Recovery”, in
Proceedings of the 25th International Symposium on Fault–Tolerant Computing Systems
(FTCS–25), pp. 499–509, Pasadena, California, 1995.
Xu, J.; Romanovsky, A.; Randell, B.: “Concurrent Exception Handling and Resolution
in Distributed Object Systems”, IEEE Transactions on Parallel and Distributed Systems
11(11), pp. 1019–1032, November 2000.
Kienzle, J.; Romanovsky, A.; Strohmeier, A.: “Open Multithreaded Transactions: Keeping Threads and Exceptions under Control”. In Proceedings of the 6th International
Worshop on Object-Oriented Real-Time Dependable Systems, Universita di Roma La
Sapienza, Roma, Italy, January 8th–10th, 2001, pp. 197–205, IEEE Computer Society
Press, Los Alamitos, California, USA, 2001.

66
[30]
[31]
[32]

[33]
[34]
[35]
[36]

[37]
[38]

[39]
[40]

[41]

[42]

[43]
[44]

[45]

[46]
[47]

[48]

J. Kienzle
Athavale, A.: “Performance Evaluation of Hybrid Voting Schemes”, M.S. thesis, North
Carolina State University, Department of Computer Science, 1989.
Tai, A.T.; Meyer, J.F.; Aviziensis, A.: “Performability Enhancement of Fault-Tolerant
Software”, IEEE Transactions on Reliability 42(2), pp. 227–237, 1993.
Kim, K.H.: “Distributed Execution of Recovery Blocks: An Approach to Uniform Treatment of Hardware and Software Faults”, Proceedings of the Fourth International Conference on Distributed Computing Systems, pp. 526–532, 1984.
Kim, K.H.: “The Distributed Recovery Block Scheme”, in M.R. Lyu (ed.), Software
Fault Tolerance, New York, John Wiley & Sons, pp. 189–209, 1995.
Scott, R.K.; Gault, J.W.; Mc Allister, D.F.: “The Consensus Recovery Block”, Proceedings of the Total Systems Reliability Symposium, Gaithersburg, MD, pp. 95–104, 1983.
Pullum, L.L.: ”Fault-Tolerant Software Decision-Making Under the Occurrence of Multiple Correct Results”, Ph.D. thesis, Southeastern Institute of Technology, 1992.
Bondavelli, A.; Di Giandomenico, F.; Xu, J.: “Cost-Effective and Flexible Scheme for
Software Fault Tolerance”, Journal of Computer System Science & Engineering 8(4), pp.
234–244, 1993.
Object Management Group, Inc.: Object Transaction Service, Version 1.1, May 2000.
Shannon, B.; Hapner, M.; Matena, V.; Davidson, J.; Pelegri-Llopart, E.; Cable, L.: Java
2 Platform Enterprise Edition: Platform and Component Specification. The Java Series,
Addison Wesley, Reading, MA, USA, 2000.
ISO: International Standard ISO/IEC 8652:1995(E): Ada Reference Manual, Lecture
Notes in Computer Science 1246, Springer Verlag, 1997; ISO, 1995.
Rodgers, P.; Wellings, A.J.: “An Incremental Recovery Cache Supporting Software Fault
Tolerance”, in Reliable Software Technologies - Ada-Europe’99, Santander, Spain, June
7–11, 1999, Lecture Notes in Computer Science 1622, pp. 385–396, 1999.
Kienzle, J.; Strohmeier, A.: “Shared Recoverable Objects”, in Reliable Software Technologies - Ada-Europe’99, Santander, Spain, June 7–11, 1999, Lecture Notes in Computer Science 1622, pp. 397–411, 1999.
Romanovsky, A.; Mitchell, S.E.; Wellings, A.J.: “On Programming Atomic Actions in
Ada 95”, Ada Europe'97, London, Lecture Notes in Computer Science 1251, pp. 254–
265, 1997.
Mitchell, S.E.; Wellings, A.J.; Romanovsky, A.: “Distributed Atomic Actions in Ada
95”, The Computer Journal 41(7), pp. 486–502, 1998.
Romanovsky, A.; Randell, B.; Stroud, R.; Xu, J.; Zorzo, A.: “Implementation of Blocking Coordinated Atomic Actions Based on Forward Error Recovery”, Journal of System
Architecture (Special Issue on Dependable Parallel Computing Systems) 43(10), pp.
687–699, September, 1997.
Kienzle, J.; Jiménez-Peris, R.; Romanovsky, A.; Patiño-Martinez, M.: “Transaction Support for Ada”. In Reliable Software Technologies – Ada-Europe’2001, Leuven, Belgium,
May 14–18, 2001, pp. 290 – 304, Lecture Notes in Computer Science 2043, Springer
Verlag, 2001.
Maes, P.: “Concepts and Experiments in Computational Reflection”, ACM SIGPLAN
Notices 22(12), December 1987, pp. 147–155.
Xu, J.; Randell, B.; Zorzo, A. F.: “Implementing Software-Fault Tolerance in C++ and
Open C++: An Object-Oriented and Reflective Approach”, Proc. Int. Workshop on Computer-Aided Design, Test, and Evaluation for Dependability (CADTED96), Beijing,
China, pp. 224–229, Int. Academic Publ., 1996.
Elrad, T.; Aksit, M.; Kiczales, G.; Lieberherr, K.; Ossher, H.: “Discussing Aspects of
AOP”. Communications of the ACM 44(10), pp. 33–38, October 2001.

Software Fault Tolerance: An Overview
[49]

67

Kienzle, J.; Guerraoui, R.: “AOP – Does it make sense? The case of concurrency and
failures”. In Proceedings of the 16th European Conference on Object-Oriented Programming (ECOOP 2002), pp. 37–54, Malaga, Spain, June 2002, Lecture Notes in Computer Science 2374, Springer Verlag, 2002.

