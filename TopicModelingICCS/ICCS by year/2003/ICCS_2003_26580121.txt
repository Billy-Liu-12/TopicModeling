A Discrete Approach for the Inverse Singular
Value Problem in Some Quadratic Group
T. Politi
Dipartimento Interuniversitario di Matematica, Politecnico di Bari,
Via Orabona 4, I-70125 Bari (Italy). pptt@dm.uniba.it

Abstract. In this paper the solution of an inverse singular value problem is considered. First the decomposition of a real square matrix
A = U ΣV is introduced, where U and V are real square matrices orthogonal with respect to a particular inner product deﬁned through a real
diagonal matrix G of order n having all the elements equal to ±1, and Σ is
a real diagonal matrix with nonnegative elements, called G−singular values. When G is the identity matrix this decomposition is the usual SVD
and Σ is the diagonal matrix of singular values. Given a set {σ1 , . . . , σn }
of n real positive numbers we consider the problem to ﬁnd a real matrix A having them as G−singular values. Neglecting theoretical aspects
of the problem, we discuss only an algorithmic issue, trying to apply a
Newton type algorithm already considered for the usual inverse singular
value problem.

1

Introduction

Inverse problems are an important topic in applied mathematics (statistics, data
analysis, applied linear algebra), since some times it is important to recovery
some general structures (for instance matrices) starting from known (e.g. experimentally) data (eigenvalues, singular values, some prescribed entries). In this
work we consider a special inverse problem related to the already studied inverse
singular value problem (see [2]). In particular, ﬁrstly we consider the decomposition of a real square matrix A of order n
A = U ΣV

(1)

where U and V are G−orthogonal (or hypernormal) matrices and Σ is a nonnegative diagonal matrix. When G is the Minkowski matrix, (1) has some interesting
applications in the study of polarized light (see [9,10]). Hence, we consider the
inverse singular value problem for the decomposition (1). At the present, we do
not know if this problem has a practical interest, but we observe that in the last
years there has been a growing interest towards other kinds of SVD (for instance
the Hyperbolic Singular Value Decomposition having some applications in signal
processing and other ﬁelds of the engineering [1,7,11]). The paper is organized
as follows: in the Section 2 we describe some algebraic properties of a product
between vectors of IRn and introduce the singular value decomposition (1), in
P.M.A. Sloot et al. (Eds.): ICCS 2003, LNCS 2658, pp. 121–130, 2003.
c Springer-Verlag Berlin Heidelberg 2003

122

T. Politi

Section 3 we present a discrete approach for the inverse singular value problem
in the groups deﬁned in Section 2 and ﬁnally in Section 4 we show a numerical
example.

2

Singular Value Decomposition in Quadratic Groups

Let us denote by Dp the set of real diagonal matrices of dimension n, with
elements equal to ±1 and with p elements equal to +1. Let G ∈ Dp , 1 ≤ p < n,
and let us deﬁne the following inner product:
x, y ∈ IRn :

(x, y) = xT Gy.

(2)

Given a vector x ∈ IRn , the number
xT Gx =

n

gii x2i

i=1

is called Hypernormal Norm even if it does not deﬁne a norm since it could be
nonpositive. If G is the identity matrix of order n, then (2) deﬁnes the usual Euclidean scalar product. Using the hypernormal norm, the following classiﬁcation
among the vectors of IRn can be introduced.
Deﬁnition 1. A vector x ∈ IRn is called Timelike (Strictly Timelike) if (x, x) ≥
0 (if (x, x) > 0).
Deﬁnition 2. A vector x ∈ IRn is called Spacelike (Strictly Spacelike) if
(x, x) ≤ 0, (if (x, x) < 0).
Deﬁnition 3. The set {x ∈ IRn | (x, x) = 0} is called Light Cone.
Deﬁnition 4. The vectors x, y ∈ IRn are G−orthogonal with respect to (2) if
(x, y) = xT Gy = 0.
We observe that the G−orthogonality does not imply the linear independence.
In fact, if x is in the light cone, it is G−orthogonal to all the vectors y = αx.
Among the most used matrices G, we have the Lorentz matrix when
G=

1

−In−1

(3)

and the Minkowski matrix if
G=

I3

−1

which has important applications in the mathematics of the relativity theory.
Now we give some deﬁnition about real matrices with respect to (2).

A Discrete Approach for the Inverse Singular Value Problem

123

Deﬁnition 5. A real square matrix A of order n is said G-adjoint of B ∈ IRn×n
if
(Ax, y) = (x, By)
for all x, y ∈ IRn .
The matrix G−adjoint of A is usually denoted by A+ . From (2) it follows that
A+ = GAT G.

Deﬁnition 6. A real square matrix A of order n is called G−selfadjoint if
(Ax, y) = (x, Ay).
If A is G−selfadjoint then A = A+ = GAT G. The concept of G−orthogonality
can be extended also to real matrix.
Deﬁnition 7. A real square matrix A of order n is said G-orthogonal (or hypernormal) if
A−1 = A+ = GAT G.
See [8] for further properties and applications of hypernormal matrices. If A is
a G−orthogonal matrix then
AGAT = G.
If we consider the quadratic group related to matrix G, i.e. the set
HG (IR) = {Y ∈ IRn×n | det(Y ) = 0, Y GY T = G},
it coincides with the set of G−orthogonal matrices. In [6] the conservative solution of diﬀerential systems in these groups has been considered. Related to
HG (IR) it is possible to deﬁne the its algebra as the set
hG = {A ∈ IRn×n | GA + AT G = 0}.
If A ∈ hG then

AT = −GAG.

If G is the identity matrix then hG is the set of real skew-symmetric matrices
(sometimes matrices in hG are also called G−skew-symmetric). If
G=

Ip O
O −In−p

A ∈ hG can be characterized in the following way
A=

A11 A12
AT12 A22

124

T. Politi

where A11 ∈ IRp×p and A22 ∈ IR(n−p)×(n−p) are real skew-symmetric matrices
and A12 ∈ IRp×(n−p) . Given A ∈ IRn×n if there exist two real G−orthogonal
matrices U and V , and a diagonal matrix Σ with nonnegative elements such
that
A = U ΣV
then it is called G−Singular Value Decomposition of A (or G − SV D). In particular it is called Singular Value Decomposition in Minkowski Space if G is the
matrix (3). For the existence of this decomposition the following theorem holds
(see [4,9]).
Theorem 1. The G−SVD of a real matrix A exists iﬀ
1. Matrix A+ A is diagonalizable and has real nonnegative eigenvalues;
2. N (A+ A) = N (A)
where N (A) denotes the null space of A.
The elements of Σ are called G−singular values of A. They are the square roots
of the eigenvalues of A+ A. In fact
A+ A = V + ΣU + U ΣV = V + Σ 2 V = V −1 Σ 2 V.

3

Inverse Singular Value Problem in Quadratic Groups

Inverse problems involving eigenvalues and singular values have been extensively
studied in last years (see [2,3,5]). The classical inverse singular value problem is
stated as follows: given real general matrices B0 , B1 , . . . , Bn ∈ IRn×n , and a set
of nonnegative numbers σ1 , . . . , σn ﬁnd the real values c1 , . . . , cn such that the
singular values of the matrix
n

B(c) = B0 +

ci Bi

(4)

i=1

are σ1 , . . . , σn . We suppose σi = σj if i = j. Starting from the same premise it is
obvious to deﬁne an analogous inverse problem also for other kinds of singular
value decomposition. It is possible to ask that matrix (4) has the G−singular
values equal to σ1 , . . . , σn . In [2] two diﬀerent approaches are analyzed for the
inverse singular value problem: the ﬁrst is continuous and the other one is discrete.
The ﬁrst approach exploits the property that the eigenvalues of the symmetric
matrix
0 A
AT 0
are plus and minus of the singular values of A, then a continuous ﬂow is derived
having the diagonal matrix of the singular values as limit point. In our case this

A Discrete Approach for the Inverse Singular Value Problem

125

approach cannot be applied since the G−singular values are the eigenvalues,
with signs plus and minus, of the non-symmetric matrix
0
A
GAT G 0
then it cannot treated as a symmetric inverse eigenvalue problem. The second
approach is a Newton-type algorithm having quadratic, but not global convergence. We try to apply this second approach to our problem. Following [2] we
deﬁne
M (Σ) = {U + ΣV ∈ IRn×n | U, V ∈ HG (IR)}

(5)

n×n

the set of all real matrices in IR
whose G−singular values are σ1 , . . . , σn .
Denoting by
B = {B(c) | c ∈ IRn }
the set of matrices of the form (4), the inverse G-singular value problems is
equivalent to ﬁnd an intersection of the two sets M (Σ) and B. If X (m) ∈ M (Σ)
there exist two matrices U (m) , V (m) ∈ HG (IR) such that
+

U (m) X (m) V (m) = Σ.

(6)

We recall that
U (m)

−1

+

T

= U (m) = GU (m) G,

V (m)

−1

+

T

= V (m) = GV (m) G.

Now we need to ﬁnd an intercept of the line tangent to the manifold M (Σ) at
X (m) with the set B. Since a tangent vector T (X) to M (Σ) at a point X ∈ M (Σ)
has the form
T (X) = XK − HX
where K, H are G−skew-symmetric matrices and it is well-known that
X + T (X) = X + XK − HX
represents the line tangent to M (Σ) emanating from X then we need to ﬁnd
two matrices H (m) , K (m) ∈ hG such that
X (m) + X (m) K (m) − H (m) X (m) = B(c(m+1) ).
From (6) it is also:
X (m) = U (m) ΣV (m)

−1

(7)

,

then (7) becomes:
U (m) ΣV (m)
ΣV (m)

−1

−1

+ U (m) ΣV (m)

+ ΣV (m)

−1

−1

K (m) − H (m) U (m) ΣV (m)

K (m) − U (m)

−1

H (m) U (m) ΣV (m)

−1

−1

= B(c(m+1) )

= U (m)

−1

B(c(m+1) )

126

T. Politi

Σ + ΣV (m)

−1

K (m) V (m) − U (m)

T

−1

H (m) U (m) Σ = U (m)

T

−1

B(c(m+1) )V (m)

T

Σ +ΣGV (m) GK (m) V (m) −GU (m) GH (m) U (m) Σ = GU (m) GB(c(m+1) )V (m) .
Setting

˜ (m) = GV (m) T GK (m) V (m)
K
˜ (m) = GU (m) T GH (m) U (m)
H
˜ (m) = GU (m) T GB(c(m+1) )V (m) .
B

the equation becomes
˜ (m) − H
˜ (m) Σ = B
˜ (m) .
Σ + ΣK

(8)

˜ (m) are in the tangent space hG , in fact
˜ (m) and H
The matrices K
˜ (m) + K
˜ (m)T G = G GV (m) T GK (m) V (m) + GV (m) T GK (m) V (m)
GK
T

T

T

T

T

G=

T

= V (m) GK (m) V (m) + V (m) K (m) GV (m) =
= V (m) GK (m) V (m) − V (m) GK (m) V (m) = 0.
˜ (m) is similar. We can exploit the n2 scalar equations (8) to evalThe proof for H
˜ (m) and H
˜ (m) and the unknown vector
uate the elements of unknown matrices K
(m+1)
c
. Let us consider if they are enough to compute the elements unknown.
˜ (m) are in hG , whose dimension is [n(n − 1)]/2 hence they
˜ (m) and H
Matrices K
are characterized by n2 − n parameters plus n elements of vector c(m+1) we have
exactly n2 unknowns, which can be computed from the equations. For i = j (8)
gives
˜ (m) σj
˜b(m) = σi k˜(m) − h
(9)
ij
ij
ij
while the equations for diagonal elements are
˜b(m) = σi ,
ii

i = 1, . . . , n.

(10)

Using (10) it is possible to compute the elements of the vector c(m+1) . In fact
σi = ˜bii

(m)

˜ (m) ei
= eTi B

where ei is the i−th unit vector of IRn , and from (4)
˜ (m) = GU (m) T GB(c(m+1) )V (m) =
B

T

= GU (m) G B0 +



n

cj Bj  V (m) =

j=1
T

= GU (m) GB0 V (m) +

n
j=1

T

cj GU (m) GBj V (m) .

A Discrete Approach for the Inverse Singular Value Problem

Hence


˜b(m) = eT GU (m) T GB0 V (m) +
i
ii

127



n

cj GU

(m) T

GBj V (m)  ei =

j=1
n

T

= eTi GU (m) GB0 V (m) ei +

T

cj eTi GU (m) GBj V (m) ei .

j=1

The vector c(m+1) is the solution of the linear system
A(m) c(m+1) = σ − b,

(11)

where A ∈ IRn×n is the matrix with elements
(m)

aij
and

T

= eTi GU (m) GBj V (m) ei
T

bi = eTi GU (m) GB0 V (m) ei

and σ denotes the vector (σ1 , . . . , σn ). If (11) has a unique solution then vector
˜ (m) and H
˜ (m) we exploit (9). We recall
c(m+1) is known. To compute matrices K
the structure of matrices in hG
˜ (m) =
K
(m)

(m)

K11

(m) T

K12

(m)

K12

(m)

K22

(m)

where K11 ∈ IRp×p and K22 ∈ IR(n−p)×(n−p) are real skew-symmetric matrices. First we consider the equations (i, j), with 1 ≤ i < j ≤ p and
p + 1 ≤ i < j ≤ n:
˜ (m) σj
˜b(m) = σi k˜(m) − h
(12)
ij
ij
ij
and

˜ (m) σi = −σj k˜(m) + σi h
˜ (m) .
˜b(m) = σj k˜(m) − h
ji
ji
ji
ij
ij

(13)

Solving (12) and (13) we obtain:
(m)
k˜ij =

σi˜bij − σj ˜bji
σi2 − σj2

˜ (m) =
h
ij

σi˜bji + σj ˜bij
.
σi2 − σj2

(m)

(m)

(m)

(m)

From the equations (i, j), 1 ≤ i ≤ p and p + 1 ≤ j ≤ n, it is:
˜b(m) = σi k˜(m) − h
˜ (m) σj
ij
ij
ij
and

(14)

128

T. Politi

˜ (m) σi = σj k˜(m) − σi h
˜b(m) = σj k˜(m) − h
˜ (m) .
ji
ji
ji
ij
ij

(15)

Solving (14) and (15) we obtain:
(m)
k˜ij =

(m)
(m)
σj ˜bji − σi˜bij
σj2 − σi2

˜ (m) =
h
ij

σi˜bji − σj ˜bij
.
σj2 − σi2
(m)

(m)

˜ (m) to compute K (m) and H (m) , in fact
˜ (m) and H
We use K
−1
˜ (m) V (m)−1 = V (m) K
˜ (m) GV (m)T G
K (m) = G(V (m) )T GK

˜ (m) U (m)
H (m) = G(U (m) )T GH
−1

−1

˜ (m) GU (m) G.
= U (m) K
T

The next step is to project B(c(m+1) ) to M (Σ). It is possible to use the exponential map, since, given a matrix H in hG , then eH belongs to the quadratic
group. The computation of the exponential of a matrix is too expensive, then
the Cayley transform can be used, if
D=
then

I+

H (m)
2

I−

H (m)
2

−1

,

F =

I+

K (m)
2

I−

K (m)
2

−1

,

X (m+1) = D+ X (m) F.

The computation of the matrix can be avoided since only G−orthogonal
matrices U (m) and V (m) are needed:
U (m+1) = D+ U (m) ,

V (m+1) = F V (m) .

The convergence of the method will not be considered in this work.

4

A Numerical Example and Conclusions

In this last section we present a numerical example concerning the discrete
method introduced in the previous section.
Example 1. In this case we have considered a problem of dimension 4, the matrix
G is
I2 0
G=
0 −I2
and we have chosen the following random

1.4353 1.1345
 1.1345 0.7065
B0 = 
 0.7833 0.8811
0.5754 1.1264

symmetric matrices Bi :

0.7833 0.5754
0.8811 1.1264 

0.9568 1.2707 
1.2707 1.7857

A Discrete Approach for the Inverse Singular Value Problem



0.5462
 1.0596
B1 = 
 0.9154
1.0762

0.8796
 0.7333
B2 = 
 0.7728
0.5254

1.9338
 0.8019
B3 = 
 1.6053
0.1655

0.9805
 1.2666
B4 = 
 0.7582
1.4403

1.0596
1.8168
0.3103
0.4132

0.9154
0.3103
1.2816
0.3617

0.7333
1.1831
0.9896
0.9110

0.7728
0.9896
1.1885
0.5023

0.8019
1.6375
1.1775
1.0814

1.6053
1.1775
0.6922
0.5885

1.2666
0.8244
0.9508
0.5583

0.7582
0.9508
1.3864
1.0502

129


1.0762
0.4132 

0.3617 
1.9886

0.5254
0.9110 

0.5023 
1.2917

0.1655
1.0814 

0.5885 
1.7120

1.4403
0.5583 
.
1.0502 
0.3976

Then we have taken the following vector c:
c = (6.2520E − 01, 7.3336E − 01, 3.7589E − 01, 9.8765E − 03)T
and have considered as G−singular values those of matrix B(c):
σ = (0.39364, 1.1338, 2.5057, 10.4847)T .
Hence we have perturbed each entry of the vector c with random quantities
between 0 and 0.5, and have considered this one as the initial guess for the
iterations. In fact the algorithm, when it converges, has only a local convergence
(see [2]) then it is necessary to start from a point near the solution. In Table 1
the error, measured in the 2-norm, on the G−singular values at each iteration
are shown.
Table 1. Example 1
Iteration

Error

0
1
2
3
4
5
6

2.5289e + 0
1.1410e + 0
7.2304e − 1
3.2589e − 2
8.9911e − 3
3.0011e − 4
9.8812e − 5

130

T. Politi

In the paper we have presented a discrete approach to the inverse singular value
problem in quadratic groups. The algorithm is similar to the one already introduced for the usual inverse singular value problem. The algorithm described
needs to be investigated more carefully, in fact it breaks down very often (the
matrix A(m) is some times singular or the matrix B(c(m) ) does not satisfy the
hypotheses on the existence of the G-singular value decomposition. A possible
solution to these troubles could be a diﬀerent parameterization of the manifold
(5). This features will be analyzed in future together with the solvability of the
problem. Moreover it should be considered the analogous problem for the Hyperbolic Singular Value Decomposition (see [1,7,11]) which has some applications
in signal processing and other ﬁelds of engineering. Some other aspects of the
problem need further investigations, for instance the application of continuous
techniques (see [2]).

References
1. Bojanczyk A.W., Onn R., Steinhardt A.O.: Existence of The Hyperbolic Singular
Value Decomposition. Linear Algebra Appl. 185 (1993) 21–30
2. Chu M.T.: Numerical methods for inverse singular value problems. SIAM J. Numer.
Anal. 29 (3) (1992) 885–903
3. Chu M.T.: Inverse eigenvalue problems. SIAM Rev. 40 (1) (1998) 1–39
4. Di Lena G., Piazza G., Politi T.: An algorithm for the computation of the
G−singular values of a matrix. Manuscript.
5. Friedland S.: Inverse eigenvalue problems. Linear Algebra Appl. 17 (1977) 15–51
6. Lopez L., Politi T.: Applications of the Cayley Approach in the Numerical Solution
of Matrix Diﬀerential Systems on Quadratic Groups. Appl. Num. Math. 36 (2001)
35–55
7. Onn R., Steinhardt A.O., Bojanczyk A.W.: The Hyperbolic Singular Value Decomposition and applications. IEEE Trans. Sign. Proc. 39 (7) (1991) 1575–1588
8. Rader C.M., Steinhardt A.O.: Hyperbolic Householder Transforms. SIAM J. Matrix Anal. 9 (1988) 269–290
9. Renardy M.: Singular values decomposition in Minkowski space. Linear Algebra
Appl. 236 (1996) 53–58
10. Xing Z.: On the deterministic and nondeterministic Mueller matrix. J. ModernOpt.
39 (1992) 461–484
11. Zha H.: A note on the existence of The Hyperbolic Singular Value Decomposition.
Linear Algebra Appl. 240 (1996) 199–205

