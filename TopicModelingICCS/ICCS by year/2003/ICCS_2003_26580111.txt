A Survey on Methods for Computing Matrix
Exponentials in Numerical Schemes for ODEs
Nicoletta Del Buono and Luciano Lopez
Dipartimento Interuniversitario di Matematica,
Universit`
a degli Studi di Bari
Via E. Orabona, 4 - I-70125 Bari ITALY
{delbuono,lopezl}@dm.uniba.it

Abstract. This paper takes a look at numerical procedures for computing approximation of the exponential of a matrix of large dimension. Existing approximation methods to evaluate the exponentiation of a matrix
will be reviewed paying more attention to Krylov subspace methods and
Schur factorization techniques. Some theoretical results on the bounds
for the entries of the exponential matrix and some implementation details
will be also discussed.

1

Introduction

Several problems in mathematics and physics can be formulated in terms of
ﬁnding a suitable approximations to certain matrix functions. Particularly, the
issue of computing the matrix exponential f (A) = e−tA , t ≥ t0 , is one of the
most frequently encountered tasks in matrix function approximation and it has
received a renew attention from the numerical analysis community. This problem arises in many areas of applications, as for instance the solution of linear
parabolic partial diﬀerential equations which needs the numerical solution of n
dimensional systems of ODEs y˙ = −Ay + b(t), y(0) = y0 , t > 0 [3,15,16], or
recently in the ﬁeld of geometric integration. In fact, most Lie group methods,
as Runge-Kutta/Munthe-Kaas schemes, Magnus expansions and Fer expansions
[19,20,28], require to suitable approximate the matrix exponential from a Lie
algebra g ⊂ IRn×n once (and often repeatedly) at each time step. This can be a
very challenging task for large dimension matrix. Moreover, the context of Liegroup method imposes a crucial extra requirement on the approximant: it has
to reside in the Lie group G ⊂ GL(n, IR) associated to the Lie algebra g.
In general, this property is not fulﬁlled by many standard approximations
unless the exponential is evaluated exactly.
This can be done for instance for a 3 × 3 skew symmetric matrices, whose
exponential is given exactly by the Euler Rodriguez formula
exp(A) = I +

sin(α)
1
A+
α
2

sin(α/2)
α/2

where
P.M.A. Sloot et al. (Eds.): ICCS 2003, LNCS 2658, pp. 111–120, 2003.
c Springer-Verlag Berlin Heidelberg 2003

2

A2 ,

112

N. Del Buono and L. Lopez


0 a3 −a2
A =  −a3 0 a1  ,
a2 −a1 0


α = (a21 + a22 + a23 )1/2 .

Other exact formulas for exponentials of skew symmetric matrices can be obtained making use of the Cayley-Hamilton theorem which for every A ∈
m−1
GL(n, IR) allows us to have the basic decomposition exp(tA) = k=0 fk (t)Ak ,
where m is the degree of the minimal polynomial of A and f0 , . . . , fm−1 are some
analytic functions that depend on the characteristic polynomial of A. However,
these algorithms are practical only up to dimension eight. In fact, for large scale
matrices, they require the computation of high powers of the matrix in question
and hence high computational costs due to the matrix-matrix multiplication;
moreover they suﬀer of some computational instabilities due to the direct use of
the characteristic polynomial.
Based on the above remarks, it appears clear that exponential of large matrices cannot be evaluated analytically and that an algorithm which is simple
and eﬃcient and satisﬁes some geometric properties is highly desiderable.
In the following pages, we will review some existing approximation methods,
highlighting also some theoretical aspects. In the last section we will give also a
brief overview on a new methodology with can be consider an hybrid scheme for
exponential approximation.

2

Review of Existing Approximation Methods

Classical methods for the evaluation of a matrix exponential can be classiﬁed
into three main categories:
(i) rational approximants;
(ii) Krylov subspace methods;
(iii) techniques based on numerical linear algebra;
further, within the context of Lie-group theory other methods for the approximation of the exponential have been recently introduced:
(iv) the splitting methods.
2.1

Rational Approximants

A rational approximation of the exponential function replaces it by a rational
function, exp(z) ≈ r(z) := pα (z)/qβ (z), where p and q are polynomials of degree
α and β respectively, q(0) = 1 and the error term ez − p(z)/q(z) is small in a
chosen norm. Thus for the matrix exp(A) one has to compute two matrix-value
polynomials, p(A) and q(A), and invert the latter to obtain r(A).
Probably, the most popular approximant of this kind is the diagonal (ν, ν)
Pad´e approximations where
ν

pν (z) =
k=1

(2ν − k)!ν! k
z
2ν!k!(ν − k)!

and qν (z) = pν (−z).

A Survey on Methods for Computing Matrix Exponentials

113

Unfortunately, the Pad´e approximants are good only near the origin (see [13]).
However, this problem can be overcome with the so-called scaling and squaring
technique, which exploits the identity
exp(A) = (exp(A/2k ))2

k

as follows. First, a suﬃciently large k is chosen so that A/2k is close to 0, then a
diagonal Pad´e approximant is used to calculate exp(A/2k ), and ﬁnally the result
is squared k times to obtain the required approximation to exp(A). This basic
approach is implemented in the Matlab function expm to evaluate the exponential
of a matrix with a computational cost between 20n3 and 30n3 operations.
About the behavior of rational approximants when applied to matrix belonging to a Lie algebra, the following theorem states that an important category of
Lie group leads itself to suitable rational approximations.
Theorem 1. (see [5]) Let G = {Y ∈ GL(n, IR) : Y T P Y = P }, where P is a
non-singular n × n matrix and let g = {X ∈ gl(n, IR) : XP + P X T = 0} be the
corresponding Lie algebra. Let φ be a function analytic in a neighborhood U0 of
0, with φ(0) = 1 and φ (0) = 1. If
φ(z)φ(−z) = 1,

∀z ∈ U0 ,

(1)

then ∀X ∈ g, Φ(tX) ∈ G for all t ∈ IR+ suﬃciently small.
Examples of Lie group verifying the above theorem are the orthogonal and
the symplectic group. Moreover, the diagonal Pad´e approximants are analytic
functions that fulﬁll (1), and this guarantees that their approximation of the
exponential of a matrix in the Lie-algebra of a quadratic Lie group lies in the
corresponding group.
Another class of approximations of exp(A) are based on the Chebyshev series
expansion of the function exp(z) on the spectral interval of A. Now let A be an
Hermitian matrix and suppose that ez is analytic in a neighborhood of the
spectral interval [λ1 , λn ] of A. The approximation of exp(A)v has the form
m

exp(A)v ≈
k=0

1
1
ak Ck ( A − I)v,
l1
l2

where l1 = (λn − λ1 )/2 and l2 = (λn + λ1 )/2, Ck is the kth Chebyshev polynomial of the ﬁrst kind and the ak are the Chebyshev coeﬃcients of e(l1 t+l2 ) ,
t ∈ [−1, 1]. Chebyshev polynomial approximation for the exponential of symmetric large sparse matrices was considered in [34], while in [2] a lower bound
for exp(−τ A)v 2 based on the computation of an eigenvector associated with
λ1 was obtained. The exponential of a non-symmetric matrix has also been approximated in [26,27] using the Faber polynomial and the Faber series.

114

2.2

N. Del Buono and L. Lopez

Methods of Numerical Linear Algebra

A simple technique to evaluate exp(A) is by spectral decomposition: if A =
V DV −1 where D is diagonal matrix with diagonal elements λi , i = 1, . . . , n,
then

 λ1
0 ... 0
e

 0 eλ2
 −1

exp(A) = V exp(D)V −1 = V  . .
V

 .. . .
. . . eλn

This, however, is not a viable techniques for general matrices. In place of spectral
decomposition, one can factorize A in a diﬀerent form, e.g. into Schur decomposition (see [13]) A = QT QT , where Q is an orthogonal n × n matrix and T is
an n × n upper triangular matrix or a block upper triangular matrix (when the
eigenvalue of A can be clustered in blocks) and therefore exp(A) = Q exp(T )QT .
2.3

Krylov Subspace Methods

The basic idea of the Krylov subspace techniques is to project the exponential of
the large matrix onto a small Krylov subspace. Particularly, an approximation
to the matrix exponential operation exp(A)v of the form
exp(A)v ≈ pm−1 (A)v,
where A ∈ GL(n, IR), v is an arbitrary nonzero vector, and pm−1 is a polynomial
of degree m − 1. Since this approximation is an element of the Krylov subspace
Km (A, v) = span{v, Av, · · · , Am−1 v},
the problem can be reformulated as that of ﬁnding an element of Km (A, v). To
ﬁnd such an approximation can be used the Arnoldi and nonsymmetric Lanczos
algorithms, respectively. The Arnoldi algorithm generates an orthogonal basis
of the Krylov subspace using v1 = v/ v as initial vector.
Arnoldi algorithm
1. Compute v1 = v/ v .
2. Do j = 1, 2, . . . , m.
w := Avj
Do i = 1, 2, . . . , j
hi,j := (w, vj )
Compute hj+1 = w

2

w := w − hi,j vj
and vj+1 = w/hj+1,j .

Arnoldi algorithm produces an orthonormal basis Vm = [v1 , v2 , . . . , vm ] of
the Krylov subspace Km (A, v) and a m × m upper Hessemberg matrix Hm with
elements hi,j such that
AVm = Vm Hm + hm+1,m vm+1 eTm

A Survey on Methods for Computing Matrix Exponentials

115

from which we get Hm = VmT AVm . Therefore Hm represents the projection of
the linear transformation A onto the subspace Km , with respect to the basis
Vm . Based on this we can also write exp(A)v ≈ βVm exp(Hm )e1 (where β =
v 2 ). Now, the problem of computing exp(A)v has been reduced to the task
of computing the lower-dimensional expression exp(Hm )e1 , which for m
n is
usually much easier, e.g., by diagonalization of Hm .
The computational cost required by the Krylov subspace methods with the
Arnoldi iteration is sum of the following partial amounts of operations, counting
both multiplications and additions: 2mn2 +2nm(m−1) operation to compute the
Krylov subspace Km (A, v) of dimension m; Cm3 computations for the evaluation
of the exponential of the Hessemberg matrix Hm ; 2nm operations arising from
the multiplication of exp(Hm ) with the orthogonal basis. However, when n is
large and m
n, these costs are subsumed in that of the Arnoldi iteration, and
the leading factor is 2mn2 (2mn3 for matrices).
Some error bounds for the Arnoldi approximation of exp(A)v can be given
for various classes of matrices.
Theorem 2. (see [17]) Let A be a complex square matrix of large dimension n, and v a given n-dimensional vector of unit length ( v = 1) and
errm = exp(τ A)v −Vm exp(τ Hm )e1 be the error in the Arnoldi approximation
of exp(τ A). Then errm satisﬁes the following bounds:
(1) if A is Hermitian negative semideﬁnite matrix with eigenvalues in the interval [−4ρ, 0] then:
√
2
errm ≤ 10e−m /5ρτ ,
4ρτ ≤ m ≤ 2ρτ,
errm ≤ 10(ρτ )−1 e−ρτ (eρτ /m)m , m ≥ 2ρτ ;
(2) if A is skew-Hermitian matrix with eigenvalues in an interval on the imaginary axis of length 4ρ, then
errm ≤ 12e−(ρτ )

2

/m

(eρτ /m)m ,

m ≥ 2ρτ ;

(3) if A has numerical range contained in the disk |z + ρ| ≤< ρ, then:
errm ≤ 12e−ρτ (eρτ /m)m ,

m ≥ 2ρτ.

Another well known algorithm for constructing a convenient basis of Km is
the Lanczos algorithm which, starting from two vectors v1 and w1 , generates a
biorthogonal basis of the subspaces Km (A, v1 ) and Km (AT , w1 ).
Lanczos algorithm
1. Compute v1 = v/ v and select w1 so that (v1 , w1 ) = 1.
2. Do j = 1, 2, . . . , m.
αj := (Avj , wj )
vˆj+1 := Avj − αj vj − βj vj−1
w
ˆj+1 := AT wj − αj wj − δj wj−1
βj+1 := |(ˆ
vj+1 , w
ˆj+1 )|, δj+1 := βj+1 sign[(ˆ
vj+1 , w
ˆj+1 )]
ˆj+1 /βj+1 .
vj+1 = vˆj+1 /δj+1 and wj+1 = w

116

N. Del Buono and L. Lopez

Setting Vm = [v1 , v2 , . . . , vm ], Wm = [w1 , w2 , . . . , wm ] and Tm =
T
tridiag(δi , αi , βi ) then Wm
Vm = Vm Wm = I, with I the identity matrix and
AVm = Vm Tm + δm+1 vm+1 eTm ,
from which we can derive the approximation exp(A)v ≈ βVm exp(Tm )e1 . The
exponential matrix exp(Tm ) can be computed again from the eigendecomposition
Tm = Qm Dm QTm , with diagonal Dm , via exp(Tm ) = Qm exp(Dm )QT .
Observe, that both these methods reduce to the same technique when the
matrix is symmetric.
For the Lanczos method it can also be proved analogous error bounds (except
for diﬀerent constants) as that given for the Arnoldi method in Theorem 2 (see
[17]).
2.4

Splitting Methods

Splitting methods have been considered by various authors in diﬀerent contexts:
for constructing symplectic methods or volume preserving algorithms, and in
PDE context [5,7,15]. A good survey can be found in [24].
The idea these methods are based on is the following: a given n × n matrix
A in a Lie algebra g is split in the form
s

A=

Ak ,

(2)

k=1

where Ak ∈ g, k = 1, 2, . . . , s, has an exponential that can be easily evaluated
exactly so that
exp(tA1 ) exp(tA2 ) · · · exp(tAs ) = exp(tA) + O(tp+1 )
for suﬃciently large value of p ≥ 1. Of course this procedure is competitive with
direct evaluations of exp(tA) only when each exp(tAi ) is easy to evaluate exactly
and their products are cheap.
These requirements are satisﬁes when each C = Ai is a low rank matrix, i.e.
p
C = l=1 αl βlT = αβ T , with αl , βl ∈ IRn , where p ≥ 1 is a small integer and
α = [α1 , . . . , αp ], β = [β1 , . . . , βp ] are n × p matrices. In this case, the function
exp(tC) can be calculated explicitely via the formula
exp(tC) = I + αD−1 (exp(tD) − I)β T .

(3)

where D = β T α is nonsingular (see [5]). Observe that each exp(tD) can be
approximated with a diagonal (ν, ν)-Pad´e approximant at the cost of νp3 ﬂops.
This low-rank splitting can be easy generalized splitting A into matrices
having one raw and one column (or a few rows and a few columns) (see [5] for
more details). Recently, it has been demonstrated also that the splitting (2) can
be improved when A belongs to a Lie algebra g by letting
r

A=

ak Qk ,
k=1

(4)

A Survey on Methods for Computing Matrix Exponentials

117

where r = dim(g) and Q1 , Q2 , . . . , Qr is a basis of the algebra, and
exp(tA) ≈ exp(g1 (t)Q1 ) exp(g2 (t)Q2 ) . . . exp(gr (t)Qr ),
and g1 , g2 , . . . , gr are polynomials ([6]). In this case, the right choice of the basis
and the use of certain features of the Lie algebra assume a fundamental importance [6].

3

A Recent Approach Based on SVD Techniques

Recently, other approaches for evaluating both exp(A) and exp(τ A)y where A
is a sparse skew symmetric matrix of large dimension n, y is a given vector, and
τ is a scaling factor, have been proposed which takes advantage from some of
the approaches discussed before. In [9] a procedure based on an eﬀective Schur
decomposition of the skew symmetric matrix A consisting into two main steps
is presented. We will describe brieﬂy this method. In the ﬁrst step the skewsymmetric matrix A is tranformed via the Arnoldi procedure into its Hessenberg
form H, e.g. A = QHQT with Q n × n orthogonal matrix and H an Hessenberg
n × n matrix possessing a tridiagonal structure. Then in the second step a Schur
decomposition of H is obtained by using the singular value decomposition of
a bidiagonal matrix of half size of H. The proposed procedure allows to take
full advantage of the sparsity of A and of the tridiagonal form of H. In fact, the
main cost for evaluating exp(A) is 92 n3 ﬂops which reduces to 2n3 ﬂops when the
banded structure of H is exploited; further, kn2 ﬂops are needed for evaluating
exp(τ A)y.
Remark 1. In the implementation of a method to approximate the exponential of
a matrix it could be important to know whether the entries of the matrix function
exp(A) exhibit some kind of decay behavior away from the main diagonal, and
to be able to estimate the rate of decay. Generally, even A is a sparse, exp(A)
is usually a dense matrix. However when the matrix in study presents a banded
structure it can be proved that the elements of the exponential decay very rapidly
away from the diagonal and in practical computation they can be set to zero
away from some bandwidth which depends on the required accuracy threshold
(see [1,21]. Let A be a banded matrix of bandwidth s ≥ 1 then the extra diagonal
entries of exp(A) rapidly decay, as
|[exp(A)]k,l | < exp(ρ)I|k−l| (2ρ),

for k, l = 1, 2, . . . , n,

(5)

where [exp(A)]k,l denotes the (k, l) entry of the matrix exp(A), ρ = max2≤j≤n hj
r
and Ir (z) is the modiﬁed Bessel function with Ir (2ρ) ∼
= ρr! for r >> 1 (see [21]
for details). It not always clear at all how, for a bounded A, the nearness of
exp(A) to a banded matrix can be exploited for improving Krylov subspace
approximation or into rational approximation, but it has been used successfully
in the context of splitting methods and in linear algebra methods to save some
computational costs.

118

4

N. Del Buono and L. Lopez

Exponentiation in Numerical Methods for ODEs

As observed above, the use of exponential of matrices for the numerical integration of ordinary diﬀerential equations on manifolds has been successfully adopted
for designing a large number of geometrical methods which behave more favorably than standard integrators (see for instance [14]). In this ﬁnal section, we
will report some geometric integrators for linear and nonlinear diﬀerential equations.
Second and fourth order Magnus methods for y = A(t)y
MG2
A1 = A(tn + h/2);
ωn = A1 ;
yn+1 = exp(hωn )yn

MG4
√
A1 = A(tn + ( 12 − √63 )h)
A2 = A(tn + ( 12 + 63√)h)
ωn = 12 (A1 + A2 ) + 123 h[A2 , A1 ];
yn+1 = exp(hωn )yn

Third order Crouch-Grossman method for y = A(t, y)y:
A1 = A(tn , yn );
A2 = A(tn + 34 h, exp( 34 hA1 )yn );
1
A3 = A(tn + 34 h, exp( 108
hA2 ) exp( 119
216 hA1 )yn );
2
24
yn+1 = exp( 13
51 hA3 ) exp(− 3 hA2 ) exp( 17 hA1 )yn

Exponential methods for nonlinear initial value problem y = f (y)
– The exponential ﬁtted Euler method:
yn+1 = yn + hϕ(hA)f (yn ),
– A symmetric exponential method:
yn+1 − yn = exp(hA)(yn−1 − yn ) + 2hϕ(hA)f (yn ),
where with A = f (yn ) is Jacobian matrix of f and ϕ =

ez −1
z .

References
1. Benzi M., Golub G.H.: Bounds fro the entries of matrix functions with applications
to preconditioning, BIT 39 (3) (1999) pp 417–438.
2. Bergamaschi, L., Vianello, M.: Eﬃcient computation of the exponential operator
for large, sparse, symmetric matrices. Numer. Linear Algebra Appl. 7 (2000), pp.
27–45.
3. Calvetti D., Reichel L.: Exponential Integration methods for large stiﬀ systems of
diﬀerential equations. Iterative Methods in Scientiﬁc Computation II (1999), pp
1–7.

A Survey on Methods for Computing Matrix Exponentials

119

4. Castillo P., Saad Y.: Preconditioning the matrix exponential operator with applications. J. Sci. Comput. 13(3) (1998), pp. 275–302.
5. Celledoni E., Iserles A.: Approximating the exponential from a Lie algebra to a
Lie group. Math. Comp. 69 (2000), pp. 1457–1480.
6. Celledoni E., Iserles A.: Numerical calculation of the exponential based on WeiNorman equation Tech. Report, University of Cambridge.
7. Celledoni E., Iserles A.: Methods for the approximation of the matrix exponential
in a Lie-algebraic setting. IMA J. Numer. Anal., No. 21, (2001) pp. 463–488.
8. Cheng, H.-W., Yau S.S.T.: More explicit formulas for the matrix exponential. Linear Algebra Appl. 262 (1997), pp. 131–163.
9. Del Buono N., Lopez L., Peluso R.: Computation of Exponentials of Real Skew
Symmetric Matrices by SVD Techniques, Tech. Report 7/03 (2003).
10. Dieci L., Papini A.: Pad´e approximation for the exponential of a block triangular
matrix. Linear Algebra Appl. 308 (2000), pp. 183–202.
11. Dieci L., Papini A.: Conditioning of the exponential of a block triangular matrix.
Numer. Algorithms 28, (2001), pp. 137–150.
12. Gallopoulos E., Saad Y.: Eﬃcient solution of parabolic equations by Krylov approximation methods, SIAM J. Sci. Stat. Comput. 13, No. 5, pp. 1236–1264, 1992.
13. Golub G.H., van Loan C.F.: Matrix Computation. 3th edn, John Hopikns, Baltimore.
14. Hairer E., Lubich C., Wanner. G.: Geometric Numerical Integration: StructurePreserving Algorithms for Ordinary Diﬀerential Equations. Springer-Verlag,
Berlin, 2002.
15. Hochbruck M., Lubich C.: Exponential integrators for quantum-classical molecular
dynamics, BIT 39, No.4, pp. 620–645, 1999.
16. Hochbruck M., Lubich C.: Exponential integrators for large systems of diﬀerential
equations, SIAM J. Sci. Comput. 19, No.5, pp. 1552–1574, 1998.
17. Hochbruck M., Lubich C.: On Krylov subspace approximations to the matrix exponential operator SIAM J. Numer. Anal. 34, No.5, pp. 1911–1925, (1997).
18. Horn R., Johnson C.: Topics in Matrix Analysis. Cambridge University Press, New
York, 1995.
19. Iserles, A., Nørsett S.P., Rasmussen A.F.: Time symmetry and high-order Magnus
methods. Appl. Numer. Math. 39, (2001) 379–401.
20. Iserles A., Munthe-Kaas H., Nørsett S., Zanna A.: Lie-group methods, Acta numerica. Cambridge: Cambridge University Press. Acta Numer. 9, 215–365 (2000).
21. Iserles A.: How large is the exponential of a banded matrix? N. Z. J. Math. 29,
No.2, pp. 177–192 (2000).
22. Iserles A., Zanna A.: Eﬃcient computation of the matrix exponential by Generalized Polar Decompositions. DAMTP Technical Report NA2002/09, University of
Cambridge, UK (2002).
23. Leite F.S., Crouch, P.: Closed forms for the exponential mapping on matrix Lie
groups based on Putzer’s method. J. Math. Phys. 40, No.7, pp. 3561–3568, 1999.
24. McLachlan, R. I., Quispel G.R.W.: Splitting methods. Acta Numerica 2002.
25. Moler C.B., Van Loan C.F.: Nineteen dubious ways to compute matrix exponential.
SIAM Review 20 (1978), pp. 801–836 .
26. Moret I., Novati P.: An interpolatory approximation of the matrix exponential
based on Faber polynomials. J. Comput. Appl. Math. 131 (2001) pp. 361–380.
27. Moret I., Novati P.: The computation of functions of matrices by truncated Faber
series. Numer. Funct. Anal. Optimization 22, (2001), pp. 697–719.
28. Munthe-Kaas, H.: High order Runge Kutta methods on manifolds. Appl. Numer.
Math. 29(1) (1999), pp. 115–127.

120

N. Del Buono and L. Lopez

29. Saad Y.: Analysis of some Krylov subspace approximation to the matrix exponential operator, SIAM J. Numer. Anal. 29, No. 1, pp. 209–228, 1992.
30. Van Loan C.: The sensitivity of the matrix exponential. SIAM J. Matrix Anal.
Appl. 14 (1977), pp. 971–981.
31. Ben Taher R., Rachidi, M.: Some explicit formulas for the polynomial decomposition of the matrix exponential and applications. Linear Algebra Appl. 350, (2002)
pp. 171–184.
32. Politi T.: A formula for the exponential of a real skew-symmetric matrix of order
4. BIT 41, No.4, (2001) pp. 842–845.
33. Tal-Ezer H.: Polynomial approximation of functions of matrices and applications.
J. Sci. Comput. 4 (1989), pp. 25–60.
34. Tal-Ezer H.: Spectral methods in time for parabolic problems. SIAM J. Numer.
Anal. 26 (1989), pp. 1–11.
35. Zanna A., Munthe-Kaas H.Z.: Generalized polar decompositions for the approximation of the matrix exponential. SIAM J. Matrix Anal. Appl. 23(3) (2002), pp.
840–862.

