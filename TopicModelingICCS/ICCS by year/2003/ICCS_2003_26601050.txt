Performance Characteristics of a Cost-Eﬀective
Medium-Sized Beowulf Cluster Supercomputer
Andre L.C. Barczak1 , Chris H. Messom1 , and Martin J. Johnson1
Massey University, Institute of Information and Mathematical Sciences, Albany
Campus Private bag 102 904,
North Shore Mail Center, Auckland, New Zealand
{a.l.barczak, c.h.messom, m.j.johnson}@massey.ac.nz
http://www.massey.ac.nz

Abstract. This paper presents some performance results obtained from
a new Beowulf cluster, the Helix, built at Massey University, Auckland funded by the Allan Wilson Center for Evolutionary Ecology. Issues concerning network latency and the eﬀect of the switching fabric
and network topology on performance are discussed. In order to assess
how the system performed using the message passing interface (MPI),
two test suites (mpptest and jumpshot) were used to provide a comprehensive network performance analysis. The performance of an older
fast-ethernet/single processor based cluster is compared to the new Gigabit/SMP cluster.

1

Introduction

The Helix is the newest and most powerful supercomputer ( Beowulf cluster )
in New Zealand. Its hardware was funded by Alan Wilson Center for Molecular Ecology and Massey University a centre of research excellence established
in 2002. The primarily purpose of the Helix is to run Bioinformatics programs
such as BLAST for whole genome searches, however secondary users span the
fundamental sciences, mathematics and computer science. Having built an experimental cluster three years ago [1], the Center for Parallel Computing at the
Institute of Information and Mathematical Sciences (IIMS) was given the opportunity to design and build a signiﬁcantly larger cluster. Several research groups
in mathematics, computer science and bioinformatics within New Zealand would
beneﬁt from such a machine. The budget was extremely limited and it was necessary to choose hardware that would give the best price/performance possible.
To evaluate the price/ performance ratio both the ﬂoating point and integer performance were considered. Many benchmarks are available today (see [2]) but
few are able to characterize a cluster in a useful and reproducible way. Tests
are often misleading, Gropp and Lusk discuss many of the traps found in performance measurement [3]. The Linpack benchmark was chosen as the ﬂoating
point benchmark as it would test the scalability of the Helix rather than just
the raw peak performance. The integer benchmark was not required to scale as
many bioinformatics applications that primarily make use of integer operations
P.M.A. Sloot et al. (Eds.): ICCS 2003, LNCS 2660, pp. 1050–1059, 2003.
c Springer-Verlag Berlin Heidelberg 2003

Performance Characteristics

1051

are ”embarrassingly parallel”, that is, they require little interprocess communication, meaning that they scale well. As well as the macro performance of the
machine as illustrated by the Linpack benchmark, we were also interested in the
micro performance of the Helix system. In order to do this we used the ’mpptest’
test suite developed by Gropp and Lusk [3]. Using these programs to measure
diﬀerent characteristics of the cluster has lead to some interesting observations.
A ring test was also carried out, showing that the topology has virtually no eﬀect
on point to point operations.
1.1

Background

With the availability of cheap commodity processors and network interconnects,
the cluster architecture for supercomputers has become popular. As commodity
processors have increased in cock speed and are able to simultaneous execute
multiple ﬂoating point instructions they have started to rival the performance of
dedicated scientiﬁc processor architectures. The programming techniques suitable for networked computers are fully discussed in [4] For several years researchers have been investigating the clustering of legacy hardware using discarded pcs, servers. and network cards[5], [6]. These systems can produce good
performance ﬁgures for embarrassingly parallel applications, but suﬀer from scalability problems due to small memory, low bandwidth and high latency interconnects. An interesting study of the eﬀects on applications and the sensitivity
to diﬀerences in bandwith and latency is found in [7]. With recently released
processors such as the Intel Xeons and AMD Athlons, cheap RAM and commodity gigabit Ethernet with reasonable latency it has become possible to build
scalable high performance computer system.

2
2.1

A Brief Description of the Clusters
The Sisters

The Sisters was the ﬁrst Beowulf cluster built at Massey University. It consists of
16 nodes connected by a fast Ethernet network. The nodes use Pentium III CPUs
running at 667MHz with 256MB Ram. The server is a dual Pentium III with
1GB Ram. There is a single switch linking the main server and the nodes. Key
bottlenecks in the system include the high latency and low bandwidth network
interconnects.
2.2

The Helix

Given the requirement of a homogeneous cluster for the Helix, the main decision
was whether to use Xeon or Athlon processors. For similarly rated cpus Xeon
2.2GHz and Athlon MP2200, the Xeon’s clock speeds are higher, but the ﬂoating
point performance are about equivalent while the Athlon has a signiﬁcant advantage on integer operations. Bioinformatics applications rely heavily on integer

1052

A.L.C. Barczak, C.H. Messom, and M.J. Johnson

operations, therefore even with equivalent costs the applications would favour
the choice of the Athlon processor. In terms of price/ performance the ﬂoating
point performance favours the Athlon. Considering all options the nodes were
built with dual Athlon MP2100+ CPUs, 1GB memory and dual 3com 64bit gigabit network interface cards. The dual processor conﬁguration was favoured so
that the network interconnect costs would be contained. Standard unmanaged
gigabit switches with two network interface cards in each node were chosen. The
two network interface cards will theoretically increase each nodes available bandwidth, but due to bus contention issues, the main advantage of the two cards
are the increased connectivity, namely the number of nodes that are directly
connected via a single switch. The nodes are arranged in rows and columns on
the same switch and each node will statically choose the best path (either located in the same switch or with the smallest number of hops) to the next node.
Typically, if they are within the same switch they will have a low latency and
high bandwidth. Given the constraints, i.e., the number of nodes, availability of
ports on the switches etc, the ﬁnal design is shown in ﬁgure 1. In this design
there is either a direct connection to another node via a single switch, or at most
up to 3 switched.

3

Performance

Tests were conducted with just two nodes, network cards and one network switch
from each vendor. The selection of the nodes and switches was determined by
the performance on the Linpack benchmark for four processors. Two dual AMD
2100 resulted in a 8.45 Gﬂops,(Rmax) while two dual Xeon 2.0 resulted in 8.36
Gﬂops. The bandwidth and latency characteristics of the network are shown in
table 1.
Table 1. Gigabit Ethernet real bandwith versus PCI bus
PCI bus

Bandwith

64 bit (133 MHz) 920 Mbps
64 bit (100 MHz) 850 Mbps
32 bit (100 MHz) 600 Mbps

Having selected and assembled the cluster, the testing of the Helix performance on the Linpack benchmark and speciﬁc issues regarding the network infrastructure and its eﬀects on overall performance (using mmptest and jumpshot)
were investigated.
3.1

Linpack Performance Results

The Linpack benchmark was run as soon as the Helix’s construction was completed. The Linpack benchmark measures the performance of a distributed mem-

Performance Characteristics

1053

IIMS

NODES
1

2

3

4

5

6

7

8

9

10

11

12

13

14

15

16

17

18

19

20

21

22

23

24

25

26

27

28

29

30

31

32

33

34

35

36

37

38

39

40

41

42

43

44

45

46

47

48

49

50

51

52

53

54

55

56

57

58

59

60

61

62

63

64

65

66

switch 7

Helix 0

switch 1

switch 6

switch 2

switch 5

switch 3

switch 4

Fig. 1. Helix’s topology.

ory computer while solving a dense linear system in double precision. Several
factors determine the performance achieved on this benchmark, but the bandwidth and latency are a signiﬁcant component. High bandwidth and low latency
ensure that the communication is not a bottleneck in the algorithm. The algorithm tends to be more eﬃcient for larger problem sizes and since the optimal
performance is when the problem ﬁts into available system memory, the larger
the memory of the machine the better. Each of the nodes in the Helix contains
1GB of RAM, so this provides a limit to the maximum problem size. Since we
require 8 bytes per double precision ﬂoating point number, the maximum theoretical matrix size is 94,118 by 94,118. However since there is operating system
overhead, a more realistic size using 80% of the memory gives a ﬁgure of 80,000
by 80,000. The theoretical peak performance (Rpeak) of the Helix (the maximum
Gﬂop rating that the machine is guaranteed never to exceed) is 448.8 Gﬂops [9],
however this is not a useful measure since it will not be achieved even if every
machine is working independently. The peak Linpack performance (Rmax) using
a problem size of 82080 was 234.8 Gﬂops [10].
The performance of the Helix on the Linpack benchmark vs processors (and
memory) shows that the system scales almost linearly (see ﬁgure 2), this is due
to the high bandwidth, reasonable latency switching and the grid layout of the
nodes. The switches each have 24 ports and as seen in ﬁgure 1 the vertically
connections connect up to 22 nodes on a single switch. This means that up to
44 processors can be used with a single high bandwidth hop between processors.
When a larger number of processors are required for the Linpack benchmark,
then the nodes must be selected to make optimal use of the grid layout. The

1054

A.L.C. Barczak, C.H. Messom, and M.J. Johnson

Fig. 2. Linpack Rmax rating versus number of processors.

almost linear increase in performance versus number of processors for the Helix
indicates that the architecture is scalable and so additional nodes can be added
to the Helix to increase the size of problems solved as well as the speed at which
they will be solved. The grid layout can be expanded to accommodate row and
column sizes of 23 nodes which will be equivalent to 1058 processors with an
expected Linpack rating of 1.88 Tﬂops putting it in the 24th position in the
November 2002 Top 500 list at an estimate cost of approximately US$ 1 million.
3.2

An Analysis of Performance Using Mpptests

There are two main reasons we chose ’mpptests’ for the performance assessments. There was a need to assess performance when using standard libraries in
a fairly standard way, oﬀering average numbers for the programmers to check
their own applications performance. This set of applications allows evaluation
of how eﬃciently the hardware is being used by the whole software structure,
including the operating system and MPI. The second reason for using ’mpptests’
is that their results are reproducible despite the simplicity of the tests. There are
other benchmarks available that will test a variety of the MPI calls, for example
SkaMPI [8]. It has been noted before that a single factor can drag performance
down signiﬁcantly. For example, it is well known that certain device drivers do
not use the full bandwidth or work poorly with certain Gigabit adapters. For
this reason a careful selection of cards and drivers was done.
A point to point test shows that for short messages in a Gigabit Ethernet
the times vary very little up to 1000 bytesand their values are around 35 microseconds. This may be considered as an indication of the latency time. This
compares favourably with the same test run in the older cluster (the sisters)
with fast Ethernet. Due to the low bandwidth as the message size increases the
times build up quickly and it goes to 250 microseconds for a message size of 1000
bytes. It is interesting to note that theoretically the gigabit Ethernet should be
10 times faster but due to latency eﬀects, the speedup is not as high for small

Performance Characteristics

1055

Fig. 3. Point to point messages in Helix (long, blocking)

messages. The latency of less than 40ms is reasonable for the gigabit Ethernet
system. The times for even very short messages are about 75% when compared
to the fast Ethernet and there is a gain in speed for any message size. The bigger
the messages, the more advantageous becomes the use of Gigabit Ethernet.
With longer messages, the picture changes dramatically. Now the bandwidth
on the Gigabit Ethernet shows the full advantage of its use. Figure 3 shows that
the times grows linearly up to 65KB in the Helix. The same tests running on the
Sisters are shown on ﬁgure 4. For 65KB the transfer times are approximately
6.7 times the ones obtained on Helix. The same trends were observed when using
non-blocking/point to point messages. The maximum theoretical bandwidth in
the fast Ethernet and on a Gigabit Ethernet are 12.5MB/s and 125MB/s respectively. For long messages (ﬁgures 3 and 4) Fast Ethernet is approximately 87%
as fast as the maximum. A similar analysis for the Gigabit Ethernet shows that
it is only about 59% as fast as the theoretical maximum.

Fig. 4. Point to point messages in Sisters (long, blocking)

1056

A.L.C. Barczak, C.H. Messom, and M.J. Johnson

The other common tests are the ones with collective operations. Both clusters
were measured using ”goptest” with -bcast option operations. An interesting
eﬀect of the smp machines using Allreduce operations can be noticed in ﬁgure
5. The nodes were ordered in such a way that only one processor was used up
to 64 nodes, then the list restarts from node 1 and went in order up to the 64th
node (the order was 1-2-3-4-...64-1-2-3-4...64). When the Allreduce operation was
done over more then 64 processors, the time suddenly increases and it seems

Fig. 5. Goptest running in 128 processors with the default order

to be independent of the message sizes. The test was repeated several times
to make sure the eﬀect was not due to some speciﬁc state of the networking
structure. It was noticed that the same test running with the nodes arranged in a
diﬀerent order had a diﬀerent impact on the times. Many combinations were tried
with varying results. Ordering the nodes properly the eﬀect on the Allreduce
operations can be diﬀerent. In ﬁgure 6 the order was 1-2-3-4...63-64-64-63-...43-2-1. This eﬀect seems to be due to the way that the AllReduce operator is
implemented in mpich. Using the new ordering the number of processes that
communicate with each other on the same smp node are reduced, eliminating
this detrimental eﬀect.
3.3

Ring Test Using Analysis Jumpshot

An interesting observation was made by Zaki et. al. [11].They found that the
Beowulf network structure can sometimes be revealed by running a ring test
application in MPI and observing the CLOG ﬁle. Applying this same concept,
ring tests were run in the Helix cluster. When two neighbouring nodes are not
located in the same switch, it would be expected that a slightly longer message
passing time would be measured. For example, there are two hops between nodes
36 and 37 (ﬁgure 1). Running a ring test with a 127KB message size on the
cluster the following CLOG ﬁle was obtained and it is shown in ﬁgure 7:

Performance Characteristics

1057

Fig. 6. Goptest running in 128 processors using a diﬀerent node ordering

Fig. 7. A ring test using 127KB message and 128 processors

Figure 7 shows that there is no noticeable eﬀect of the topology, even when
using relatively long messages. Several tests using the processors in diﬀerent
order were carried out to conﬁrm the results. This ﬁgure also shows that the
point to point messages are being delivered with the same speed as the single
point to point messages in ﬁgure 3 with little degradation when making multiple
hops over switches. This could be explained by the fact that the additional delay
introduced by the switches is small compared to the communication overhead of
MPI.
3.4

Bisectional Bandwidth

Figure 8 shows that the bisectional bandwidth of the Helix is approx 220Mbps
for a broad range of message sizes. These results were obtained using just one
process at each node as reliable reproducible results could not be obtained running two processes in each node. These results are consistent with a theoretical

1058

A.L.C. Barczak, C.H. Messom, and M.J. Johnson

Fig. 8. Bisectional bandwidth of the Helix

analysis of the network topology where we section the network in two halves
at the node 32-33 boundary. This means that we have about 110Mbps of full
duplex bisectional bandwidth available for each processor which means that the
architecture will give reasonable performance for algorithms that have not necessarily been optimised for the Helix. The reasonable bisectional bandwidth is
a result of the large number of nodes (almost half of the cluster) that are connected directly to each node by a Gigabit Ethernet connection. As the network
size increases to the proposed maximum of 23 x 23 nodes, or 1058 processors,
the bisectional bandwidth will not scale well and so the algorithms will have to
be modiﬁed to suit the grid topology.

4

Conclusions

The use of gigabit Ethernet has proved to be a good choice, even considering
that the gain in performance for short messages is not 10 times. Although the
relative latency is high, especially with very short messages, there are gains for
any message size and the absolute latency is reasonable. The bigger the message
the better the gains when compared to fast Ethernet. The eﬀect of using SMP
machines with two Gigabit Ethernet adapters each proved to be economical
and performs well. Even considering that the two cards cannot achieve their
maximum bandwidth when communicating simultaneously due to motherboard
constraints it still plays an important role of fairly distributing communication
loads across the network. The collective tests showed that by carefully ordering
the way the processors work with certain MPI collective operations a better
result can be achieved.

Performance Characteristics

1059

References
1. Grosz, L. and Barczak, A.: Building an Inexpensive Parallel Computer. Res. Lett.
Inf. Math. Sci.1 (2000) 113–118
2. Benchweb: www.netlig.org/benchweb/
3. Gropp, and Lusk, : Reproducible measurements of MPI performace characteristics.
In: Dongarra,J., Luque, E., and Margalef, T., (editors): Recent Advances in Parallel
Virtual Machine and Message Passing Interface, volume 1697 Lecture Notes in
Computer Science, pages 11–18. Springer Verlag (1999).
4. Wilkinson, B. and Allen, M.: Parallel Programming. Prentice Hall, New Jersey
(1999)
5. Savarese, D. F. and Sterling, T.: Beowulf. In: Buyya, R. (ed.): High Performance
Cluster Computing, Vol.1. Prentice Hall PTR, New Jersey (1999)
6. Ridge, D., Becker, D., Merkey, P., Sterling, T.: Beowulf: Harnessing the Power of
Parallelism in a Pile-of-PCs Proceedings, IEEE Aerospace, 1997
7. Plaat, A., Bal, H.E., Hofman, R.F.H., Kielmann, T.: Sensitivity of parallel applications to large diﬀerences in bandwidth and latency in two-layer interconnects
Future Generation Computer Systems 17 (6): 769-782 APR 2001
8. Reussner, R.: Recent Advances in SKaMPI In: Krause, E. and J¨
ager, W. (eds):
High Performance Computing in Science and Engineering 2000 Transactions of
the High Performance Computing Center Stuttgart (HLRS) 520–530, SPRINGER
(2001)
9. clusters.top500.org
10. www.top500.org
11. Zaki,O. and Lusk E. and Gropp,L. and Swider, D: Toward Scalable Performance
Visualization with Jumpshot. High Performance Computing Applications vol 13
number 2 (1999) 277–288

