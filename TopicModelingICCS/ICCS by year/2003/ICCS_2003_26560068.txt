On the Limitations of Universally Composable
Two-Party Computation without Set-up
Assumptions
Ran Canetti1 , Eyal Kushilevitz2 , and Yehuda Lindell1
1

IBM T.J.Watson Research, 19 Skyline Drive, Hawthorne NY 10532, USA.
canetti@watson.ibm.com, lindell@us.ibm.com
2
Computer Science Department, Technion, Haifa 32000, Israel.
eyalk@cs.technion.ac.il

Abstract. The recently proposed universally composable (UC) security
framework, for analyzing security of cryptographic protocols, provides
very strong security guarantees. In particular, a protocol proven secure in
this framework is guaranteed to maintain its security even when deployed
in arbitrary multi-party, multi-protocol, multi-execution environments.
Protocols for securely carrying out essentially any cryptographic task in a
universally composable way exist, both in the case of an honest majority
(in the plain model, i.e., without set-up assumptions) and in the case of
no honest majority (in the common reference string model). However, in
the plain model, little was known for the case of no honest majority and,
in particular, for the important special case of two-party protocols.
We study the feasibility of universally composable two-party function
evaluation in the plain model. Our results show that very few functions
can be computed in this model so as to provide the UC security guarantees. Speciﬁcally, for the case of deterministic functions, we provide a
full characterization of the functions computable in this model. (Essentially, these are the functions that depend on at most one of the parties’
inputs, and furthermore are “eﬃciently invertible” in a sense deﬁned
within.) For the case of probabilistic functions, we show that the only
functions computable in this model are those where one of the parties
can essentially uniquely determine the joint output.

1

Introduction

Traditionally, cryptographic protocol problems were considered in a model where
the only involved parties are the actual participants in the protocol, and only a
single execution of the protocol takes place. This model allows for relatively concise problem statements, simpliﬁes the design and analysis of protocols, and is a
natural choice for the initial study of protocols. However, this model of “standalone computation” does not fully capture the security requirements from cryptographic protocols in modern computer networks. In such networks, a protocol
Part of this work was done while the author was a visitor at IBM T.J. Watson
Research Center.
E. Biham (Ed.): EUROCRYPT 2003, LNCS 2656, pp. 68–86, 2003.
c International Association for Cryptologic Research 2003

On the Limitations of Universally Composable Two-Party Computation

69

execution may run concurrently with an unknown number of other copies of the
protocol and, even worse, with unknown, arbitrary protocols. These arbitrary
protocols may be executed by the same parties or other parties, they may have
potentially related inputs and the scheduling of message delivery may be adversarially coordinated. Furthermore, the local outputs of a protocol execution may
be used by other protocols in an unpredictable way. These concerns, or “attacks”
on a protocol are not captured by the stand-alone model. Indeed, over the years
deﬁnitions of security became more and more sophisticated and restrictive, in
an eﬀort to guarantee security in more complex, multi-execution environments.
(Examples include [gk88,ny90,b96,ddn00,dns98,rk99,gm00,ck01] and many
more). However, in spite of the growing complexity, none of these notions guarantee security in arbitrary multi-execution, multi-protocol environments.
A recently proposed alternative approach to guaranteeing security in arbitrary protocol environments is to use notions of security that are preserved
under general protocol composition. Speciﬁcally, a general framework for deﬁning security of protocols has been proposed [c01]. In this framework (called the
universally composable (UC) security framework), protocols are designed and analyzed as stand-alone. Yet, once a protocol is proven secure, it is guaranteed that
the protocol remains secure even when composed with an unbounded number of
copies of either the same protocol or other unknown protocols. (This guarantee
is provided by a general composition theorem.)
UC notions of security for a given task tend to be considerably more stringent
than other notions of security for the same task. Consequently, many known
protocols (e.g., the general protocol of [gmw87], to name one) are not UC-secure.
Thus, the feasibility of realizing cryptographic tasks requires re-investigation
within the UC framework. Let us brieﬂy summarize the known results.
In the case of a majority of honest parties, there exist UC secure protocols for
computing any functionality [c01] (building on [bgw88,rb89,cfgn96]). Also,
in the honest-but-curious case (i.e., when even corrupted parties follow the protocol speciﬁcation), UC secure protocols exist for essentially any functionality
[clos02]. However, the situation is diﬀerent when no honest majority exists and
the adversary is malicious (in which case the corrupted parties can arbitrarily
deviate from the protocol speciﬁcation). In this case, UC secure protocols have
been demonstrated for a number of speciﬁc (but important) functionalities such
as key exchange and secure communication, assuming authenticated channels
[c01]. However, it has also been shown that in the plain model (i.e., assuming
authenticated channels, but without any additional set-up assumptions), there
are a number of natural two-party functionalities that cannot be securely realized in the UC framework. These include coin-tossing, bit commitment, and
zero-knowledge [c01,cf01]. In contrast, in the common reference string model,
UC secure protocols exist for essentially any two-party and multi-party functionality, with any number of corrupted parties [clos02].
A natural question that remains open is what are the tasks that can be
securely realized in the UC framework, with a dishonest majority, a malicious
adversary and without set-up assumptions (i.e., in the plain model). We note

70

R. Canetti, E. Kushilevitz, and Y. Lindell

that previous results left open the possibility that useful relaxations of the cointossing, bit commitment and zero-knowledge functionalities can be securely realized. (Indeed, our research began with an attempt to construct UC protocols
for such relaxations.)
Our results. We concentrate on the restricted (but still fairly general) case
of two-party function evaluation, where the parties wish to evaluate some predeﬁned function of their local inputs. We consider both deterministic and probabilistic functions. Our results (which are mostly negative in nature) are quite
far-reaching and apply to many tasks of interest. In a nutshell, our results can be
summarized as follows. Say that a function g is eﬃciently invertible if there exists
an inverting algorithm M that successfully inverts g (i.e., M (g(x)) ∈ g −1 (g(x)))
for any samplable distribution on the input x. Then, our main results can be
informally described as follows:
1. Let f (·, ·) be a deterministic two-party function. Then, f can be securely
evaluated in the UC framework if and only if f depends on at most one of
its two inputs (e.g., f (x, y) = g(x) for some function g(·)) and, in addition,
g is eﬃciently invertible.
2. Let f (·, ·) be a probabilistic two-party function. Then, f can be securely
evaluated in the UC framework only if for any party, and for any input x
for that party, there exists an input y for the other party such that f (x, y)
is “almost” deterministic (i.e., essentially all the probability mass of f (x, y)
is concentrated on a single value).
These results pertain to protocols where both parties obtain the same output.
Interestingly, the results hold unconditionally, in spite of the fact that they rule
out protocols that provide only computational security guarantees. We remark
that UC-security allows “early stopping”, or protocols where one of the parties
aborts after learning the output and before the other party learned the output.
Hence, our impossibility results do not (and cannot) rely on an early stopping
strategy by the adversary (as used in previous impossibility results like [c86]).
Our results provide an alternative proof to previous impossibility results regarding UC zero-knowledge and UC coin-tossing in the plain model [c01,cf01].
In fact, our results also rule out any interesting relaxation of these functionalities. We stress, however, that these results do not rule out the realizability (in
the plain model) of interesting functionalities like key-exchange, secure message
transmission, digital signatures, and public-key encryption (see [c01,ck02]). Indeed, as noted above, these functionalities are realizable in the plain model.
Techniques. Our impossibility results utilize in an essential way the strong requirements imposed by the UC framework. The UC deﬁnition follows the standard paradigm of comparing a real protocol execution to an ideal process involving a trusted third party. It also diﬀers in a very important way. The traditional
model considered for secure computation includes the parties running the protocol, plus an adversary A that controls a set of corrupted parties. In the UC
framework, an additional adversarial entity called the environment Z is introduced. This environment generates the inputs to all parties, reads all outputs,

On the Limitations of Universally Composable Two-Party Computation

71

and in addition interacts with the adversary in an arbitrary way throughout the
computation. A protocol securely computes a function f in this framework if for
any adversary A that interacts with the parties running the protocol, there exists an ideal process adversary (or “simulator”) S that interacts with the trusted
third party, such that no environment can tell whether it is interacting with A
and the parties running the protocol, or with S in the ideal process.
On a high level, our results are based on the following observation. A central
element of the UC deﬁnition is that the real and ideal process adversaries A and
S interact with the environment Z in an “on-line” manner. This implies that S
must succeed in simulation while interacting with an external adversarial entity
that it cannot “rewind”. Given the fact that here is no honest majority and
that there are no setup assumptions that can be utilized, it turns out that the
simulator S has no advantage over a real participant. Thus, a corrupted party
can actually run the code of the simulator.
Given the above observation, we demonstrate our results in two steps. First,
in Section 3, we prove a general “technical lemma,” asserting that a certain
adversarial behavior (which is based on running the code of the simulator) is
possible in our model. We then use this lemma to prove the characterization
mentioned above, in several steps. Let us outline these steps. First, we concentrate on functions where only one of the parties has input, and show that these
functions are computable iﬀ they are eﬃciently invertible. Next, we consider
functions where both parties have inputs, and demonstrate that such functions
are computable only if they totally ignore at least one of the two inputs. This is
done as follows. First, we show that functions that contain an “insecure minor,”
as deﬁned in [bmm99], cannot be realized. (A series of values α1 , α1 , α2 , α2 constitute an insecure minor for f if f (α1 , α2 ) = f (α1 , α2 ) but f (α1 , α2 ) = f (α1 , α2 );
see Table 1 in Section 4.2.) Next, we show that functions that contain an
“embedded-XOR” cannot be realized. (Values α1 , α1 , α2 , α2 form an embeddedXOR if f (α1 , α2 ) = f (α1 , α2 ) = f (α1 , α2 ) = f (α1 , α2 ) = f (α1 , α2 ); see Table 2
in Section 4.3.) We then prove that a function that contains neither an insecure
minor nor an embedded-XOR must ignore at least one of its inputs.
Impossibility for relaxed versions of UC. The impossibility results presented in this paper actually also rule out two natural relaxations of the UC
deﬁnition. First, consider an analogous deﬁnition where the environment machine Z is uniform. (We remark that the UC theorem has been shown to hold
under this deﬁnition [hms03].) In this case, some variations of our results hold
(for example, an almost identical characterization holds in the case that the
functions are deﬁned over a ﬁnite domain). Next, consider a relaxed deﬁnition of
security, where the relaxation relates to the order of quantiﬁers. The actual UC
deﬁnition requires that for every adversary A, there exists a simulator S such
that no environment Z can distinguish real executions with A from ideal process
executions with S (i.e., ∀A∃S∀Z). Thus, a single simulator S must successfully
simulate for all environments Z. A relaxation of this would allow a diﬀerent
simulator for every environment; i.e., ∀A, Z∃S. (We note that the UC composition theorem is not known to hold in this case.) As above, the characterization

72

R. Canetti, E. Kushilevitz, and Y. Lindell

remains almost the same even for this relaxed deﬁnition. Due to lack of space,
we present our results only for the standard UC deﬁnition, however, similar
impossibility results do hold for the relaxed deﬁnitions above; see [ckl03].
Related work. Characterizations of the functions that are securely computable were made in a number of other models and with respect to diﬀerent notions of security. E.g., in the case of honest-but-curious parties and informationtheoretic privacy, characterization of the functions that can be computed were
found for the two-party case [ck89,k89], and for boolean functions in the multiparty case [ck89]. In [bmm99], the authors consider a setting of computational
security against malicious parties where the output is given to only one of the
parties, and provide a characterization of the complete functions. (A function is
complete if given a black-box for computing it, it is possible to securely compute
any other function.) Some generalizations were found in [k00]. Similar completeness results for the information-theoretic honest-but-curious setting are given in
[kkmo00]. Interestingly, while the characterizations mentioned above are very
diﬀerent from each other, there is some similarity in the type of structures considered in those works and in ours (e.g., the insecure minor of [bmm99] and the
embedded-OR of [kkmo00]).

2

Review of UC Security

We present a very brief overview of how security is deﬁned in the UC framework,
restricted to our case of two parties, non-adaptive adversaries, and authenticated
communication. For further details see [c01,ckl03].
As in other general deﬁnitions (e.g., [gl90,mr91,b91]), the security requirements of a given task (i.e., the functionality expected from a protocol that carries
out the task) are captured via a set of instructions for a “trusted party” that obtains the inputs of the participants and provides them with the desired outputs
(in one or more iterations). Informally, a protocol securely carries out a given
task if running the protocol with a realistic adversary amounts to “emulating”
an ideal process where the parties hand their inputs to a trusted party with the
appropriate functionality and obtain their outputs from it, without any other
interaction. We call the algorithm run by the trusted party an ideal functionality.
To allow proving a universal composition theorem, the notion of emulation
in this framework is considerably stronger than in previous ones. Traditionally,
the model of computation includes the parties running the protocol and an
adversary, A, that controls the communication channels and potentially corrupts
parties. “Emulating an ideal process” means that for any adversary A there
should exist an “ideal process adversary” (or, simulator) S that results in a
similar distribution on the outputs for the parties. Here an additional entity,
called the environment Z, is introduced. The environment generates the inputs
to all parties, reads all outputs, and in addition interacts with the adversary in an
arbitrary way throughout the computation. A protocol is said to securely realize
a given ideal functionality F if for any “real-life” adversary A that interacts with
the protocol and the environment there exists an “ideal-process adversary” S,

On the Limitations of Universally Composable Two-Party Computation

73

such that no environment Z can tell whether it is interacting with A and parties
running the protocol, or with S and parties that interact with F in the ideal
process. In a sense, here Z serves as an “interactive distinguisher” between a
run of the protocol and the ideal process with access to F. A bit more precisely,
Let realπ,A,Z be the ensemble describing the output of environment Z after
interacting with parties running protocol π and with adversary A. (Without
loss of generality, we assume that the environment outputs one bit.) Similarly,
let idealF ,S,Z be the ensemble describing the output of environment Z after
interacting in the ideal process with adversary S and parties that have access to
ideal functionality F.
Deﬁnition 1 Let F be an ideal functionality and let π be a two-party protocol.
We say that π securely realizes F if for any adversary A there exists an idealprocess adversary S such that for any environment Z, the ensembles idealF ,S,Z
and realπ,A,Z are indistinguishable.
Non-trivial protocols and the requirement to generate output. In
the UC framework, the ideal process does not require the ideal-process adversary
to deliver messages that are sent by the ideal functionality to the dummy parties.
Consequently, the deﬁnition provides no guarantee that a protocol will ever
generate output or “return” to the calling protocol. Indeed, in our setting where
message delivery is not guaranteed, it is impossible to ensure that a protocol
“terminates” or generates output. Rather, the deﬁnition concentrates on the
security requirements in the case that the protocol generates output.
A corollary of the above fact is that a protocol that “hangs”, never sends any
messages and never generates output, securely realizes any ideal functionality.
However, such a protocol is clearly not interesting. We thus use the notion of a
non-trivial protocol. Such a protocol has the property that if the real-life adversary
delivers all messages and does not corrupt any parties, then the ideal-process
adversary also delivers all messages (and does not corrupt any parties). In the
rest of this work we concentrate on non-trivial protocols.

3

The Main Theorem – Deterministic Functions

In this section, we prove a theorem that serves as the basis for our impossibility
results. To motivate the theorem, recall the way an ideal-model simulator typically works. Such a simulator interacts with an ideal functionality by sending
it an input (in the name of the corrupted party) and receiving back an output.
Since the simulated view of the corrupted party is required to be indistinguishable from its view in a real execution, it must hold that the input sent by the
simulator to the ideal functionality corresponds to the input that the corrupted
party (implicitly) uses. Furthermore, the corrupted party’s output from the protocol simulation must correspond to the output received by the simulator from
the ideal functionality. That is, such a simulator can “extract” the input used by
the corrupted party, and can cause the corrupted party to output a value that
corresponds to the output received by the simulator from the ideal functionality.

74

R. Canetti, E. Kushilevitz, and Y. Lindell

The following theorem shows that, essentially, a malicious P2 can do “whatever” the simulator can do. That is, consider the simulator that exists when P1
is corrupted. This simulator can extract P1 ’s input and can cause its output to
be consistent with the output from the ideal functionality. Therefore, P2 (when
interacting with an honest P1 ) can also extract P1 ’s input and cause its output
to be consistent with an ideally generated output. Indeed, P2 succeeds in doing
this by internally running the ideal-process simulator for P1 . In other models of
secure computation, this cannot be done because a simulator typically has some
additional “power” that a malicious party does not. (This power is usually the
ability to rewind a party or to hold its description or code.) Thus, we actually
show that in the plain model and without an honest majority, the simulator
for the UC setting has no power beyond what a real (adversarial) party can do
in a real execution. This enables P2 to run the simulator as required. We now
describe the above-mentioned strategy of P2 .
Strategy description for P2 : The malicious P2 internally runs two separate
machines (or entities): P2a and P2b . Entity P2a interacts with (the honest) P1 and
runs the simulator that is guaranteed to exist for P1 , as described above. In
contrast, entity P2b emulates the ideal functionality for the simulator as run by
P2a . Loosely speaking, when P2a obtains P1 ’s input, it hands it to P2b , who then
computes the function output and hands it back to P2a . Entity P2a then continues
with the emulation, and causes P1 to output the value that P2a received from P2b .
We now formally deﬁne this strategy of P2 . We begin by deﬁning the structure
of this adversarial attack, which we call a “split adversarial strategy”, and then
proceed to deﬁne what it means for such a strategy to be “successful”.
Deﬁnition 2 (split adversarial strategy): Let f : X ×X → {0, 1}∗ be a function
and let Πf be a protocol. Let X2 ⊆ X be a polynomial-size subset of inputs (i.e.,
|X2 | = poly(k), where k is the security parameter), and let x2 ∈ X2 . Then,
a corrupted party P2 is said to run a split adversarial strategy if it proceeds as
follows. P2 internally runs P2a and P2b . Next:
1. Upon input (X2 , x2 ), party P2 internally gives the machine P2b the input
pair (X2 , x2 ). (P2a does not receive the input value. This fact will become
important later.)
2. An execution between (an honest) P1 running Πf and P2 works as follows:
a) P2a interacts with P1 according to some speciﬁed strategy (to be ﬁxed).
b) At some stage of the execution P2a hands P2b a value x1 .
c) When P2b receives x1 from P2a , it computes y = f (x1 , x2 ) for some x2 ∈
X2 of its choice.1
d) P2b hands P2a the value y, and P2a continues interacting with P1 .
Informally speaking, a split adversarial strategy is said to be successful if the
value x1 procured by P2a is essentially (the honest) P1 ’s input. Furthermore, P2a
1

The choice of x2 can depend on the values of both x1 and x2 and can be chosen by
any eﬃcient strategy. The fact that x2 must come from the polynomial-size subset
of inputs X2 is needed later.

On the Limitations of Universally Composable Two-Party Computation

75

should succeed in causing P1 to output the value y that it received from P2b . As
mentioned, P2a does this by internally running the simulator that is guaranteed to
exist for a corrupted P1 . This means that step 2b above corresponds to the ability
of the simulator (as run by P2a ) to extract P1 ’s input, and step 2d corresponds
to the simulator’s ability to cause the output of P1 to be consistent with the
ideally computed output. Formally,
Deﬁnition 3 Let Z be an environment who hands an input x1 ∈ X to P1
and a pair (X2 , x2 ) to P2 , where X2 ⊆ X, |X2 | = poly(k), and x2 ∈r X2 .
Furthermore, Z continually activates P1 and P2 in succession. Then, a split
adversarial strategy for P2 is said to be successful if in a real execution with the
above Z and an honest P1 , the following holds:
1. The value x1 output by P2a in step 2b of Deﬁnition 2 is such that for every
x2 ∈ X2 , f (x1 , x2 ) = f (x1 , x2 ).
2. P1 outputs y, where y is the value that P2b gives P2a in step 2d of Deﬁnition 2.
Loosely speaking, the theorem below states that a successful split adversarial
strategy exists for any protocol that securely realizes a two-party function. In
Section 4, we will show that the existence of successful split adversarial strategies
rules out the possibility of securely realizing large classes of functions. We are
now ready to state the theorem:
Theorem 4 Let f be a two-party function, and let Ff be the two-party ideal
functionality that receives x1 and x2 from P1 and P2 , respectively, and hands
both parties f (x1 , x2 ). If Ff can be securely realized by a non-trivial protocol 2
Πf , then there exists a machine P2a such that, except with negligible probability,
the split adversarial strategy for P2 = (P2a , P2b ) is successful.
Proof: The intuition behind the proof is as follows. If Ff can be securely
realized by a protocol ΠF , then this implies that for any real-life adversary A
(and environment Z), there exists an ideal-process adversary (or “simulator”) S.
As we have mentioned, this simulator interacts with the ideal process and must
hand it the input that is (implicitly) used by the corrupted party. That is, S
must be able to extract a corrupted party’s input. Now, consider the case that A
controls party P1 and so S must extract P1 ’s input. The key point in the proof
is that S must essentially accomplish this extraction without any rewinding and
without access to anything beyond the protocol messages. This is due to the
following observations. First, S interacts with the environment Z in the same
way that parties interact with each other in a real execution. That is, Z is a
party that sits externally to S (or, otherwise stated, S has only black-box access
to Z) and S is not able to rewind Z. Now, since Z and A can actively cooperate
in attacking the protocol, Z can generate all the adversarial messages, with A
2

Recall that a non-trivial protocol is such that if the real model adversary corrupts no
party and delivers all messages, then so does the ideal model adversary. This rules
out the trivial protocol that does not generate output. See Section 2 for details.

76

R. Canetti, E. Kushilevitz, and Y. Lindell

just forwarding them to their intended recipients. This implies that S actually
has to extract the input from Z. However, as mentioned, S interacts with Z in
the same way that real parties interact in a protocol execution. Given that S can
extract in such circumstances, it follows that a malicious P2 interacting in a real
protocol execution with an honest P1 , can also extract P1 ’s input (by using S).
Thus, P2a ’s strategy is to run the code of the simulator S (and the role of P2b is to
emulate S’s interface with Ff ). The above explains the ability of P2a to extract
P1 ’s input. The fact that P2a causes the honest P1 to output y = f (x1 , x2 ) also
follows from the properties of S. We now formally prove the above, by formalizing
a split adversarial strategy, and proving that it is successful.
First, we formulate the simulator S mentioned above. Assume that Ff can
be securely realized by a protocol Πf . Then, for every adversary A there exists
a simulator S such that no environment Z can distinguish between an execution
of the ideal process with S and Ff and an execution of the real protocol Πf with
A. We now deﬁne a speciﬁc adversary A and environment Z. The adversary A
controls party P1 and is a dummy adversary who acts as a bridge that passes
messages between Z and P2 . Now, let X2 be some polynomial-size set of inputs
(chosen by Z), and let (x1 , x2 ) be P1 and P2 ’s respective inputs as decided by
Z, where x2 ∈r X2 . Then, Z writes (X2 , x2 ) on P2 ’s input tape and plays the
role of honest P1 on input x1 by itself. That is, Z runs P1 ’s protocol instructions
in Πf on input x1 and the incoming messages that it receives from A (which
are in turn received from P2 ). The messages that Z passes to A are exactly
the messages as computed by an honest P1 according to Πf . At the conclusion
of the execution of Πf , the environment Z obtains some output, as deﬁned by
the protocol speciﬁcation for P1 that Z runs internally; we call this Z’s local
P1 -output. Z then reads P2 ’s output tape and outputs 1 if and only if Z’s local
P1 -output and P2 ’s output both equal f (x1 , x2 ). Observe that in the real-life
model Z outputs 1 with probability negligibly close to 1. This is because such
an execution of Πf , with the above Z and A, looks exactly like an execution
between two honest parties P1 and P2 upon inputs x1 and x2 , respectively. Now,
since Πf is a non-trivial protocol, in an ideal execution with no corrupted parties,
both parties output f (x1 , x2 ). Therefore, the same result must also hold in a real
execution (except with negligible probability).
We now describe the strategy for P2a . By the assumption that Πf securely
realizes Ff , there exists an ideal process simulator S for the speciﬁc A (and
Z) described above, where P1 is corrupted. P2a invokes this simulator S and
emulates an ideal process execution of S with the above A and Z. That is, every
message that P2a receives from P1 it forwards to S as if S received it from Z.
Likewise, every message that S sends to Z in the emulation, P2a forwards to P1
in the real execution. When S outputs a value x1 that it intends to send to Ff ,
entity P2a hands it to P2b . Then, when P2a receives a value y back from P2b , it
passes this to S, as if S receives it from Ff , and continues with the emulation.
(Recall that this value y is computed by P2b and equals f (x1 , x2 ).)
We now prove that, except with negligible probability, the above P2a is such
that P2 = (P2a , P2b ) is a successful split adversarial strategy. That is, we prove

On the Limitations of Universally Composable Two-Party Computation

77

that items (1) and (2) from Deﬁnition 3 hold with respect to this P2 . We begin
by proving that, except with negligible probability, the value x1 output by P2a
is such that for every x2 ∈ X2 , f (x1 , x2 ) = f (x1 , x2 ). First, we claim that S’s
view in the ideal process with the above A and Z is identical to its view in the
emulation with P2 = (P2a , P2b ). (Actually, for now it suﬃces to show that this
holds until the point that S outputs x1 .) To see this, notice that in the ideal
process with A and Z, the simulator S holds the code of A. However, all A
does is forwarding messages between Z and P2 . Thus, the only “information”
received by S is through the messages that it receives from Z. Now, recall that in
this ideal process, Z plays the honest P1 strategy upon input x1 . Therefore, the
messages that S receives from Z are distributed exactly like the messages that
S receives from the honest P1 in the emulation with P2 = (P2a , P2b ). Since this is
the only information received by S until the point that S outputs x1 , we have
that its views in both cases are identical. It remains to show that in the ideal
process with A and Z, simulator S must obtain and send Ff an input x1 such
that for every x2 ∈ X2 , f (x1 , x2 ) = f (x1 , x2 ), except with negligible probability.
(Item (1) follows from this because if S obtains such an x1 in the ideal process,
then it also obtains it in the emulation with P2 = (P2a , P2b ) where its view is
identical.) This can be seen as follows. Assume, by contradiction, that with nonnegligible probability x1 is such that for some x
˜2 ∈ X2 , f (x1 , x
˜2 ) = f (x1 , x
˜2 ).
˜2 and S sends x1 to Ff , then
Now, if in an ideal execution, P2 ’s input equals x
P2 outputs f (x1 , x2 ) = f (x1 , x2 ). By the speciﬁcation of Z, when this occurs
Z outputs 0. Now, recall that X2 is of polynomial size and that P2 ’s input is
uniformly chosen from X2 . Furthermore, the probability that S sends x1 such
that f (x1 , x
˜2 ) = f (x1 , x
˜2 ) is independent of the choice of x2 for P2 . Therefore,
the overall probability that Z outputs 0 in the ideal process is non-negligible.
However, we have already argued above that in a real protocol execution, Z
outputs 1 with overwhelming probability. Thus, Z distinguishes the real and
ideal executions, contradicting the security of the protocol. We conclude that
except with negligible probability, item (1) of Deﬁnition 3 holds.
We proceed to prove item (2) of Deﬁnition 3. Assume, by contradiction, that
in the emulation with P2 = (P2a , P2b ), party P1 outputs y = y with non-negligible
probability (recall that y = f (x1 , x2 )). First, consider the following thought
experiment: Modify P2b so that instead of choosing x2 as some function of x1
and x2 , it chooses x2 ∈r X2 instead; denote this modiﬁed party P˜2b . It follows
that with probability 1/|X2 |, the value chosen by the modiﬁed P˜2b equals the
value chosen by the unmodiﬁed P2b . Therefore, the probability that P1 outputs
y = y in an emulation with the modiﬁed P˜2 = (P2a , P˜2b ) equals 1/|X2 | times the
(non-negligible) probability that this occurred with the unmodiﬁed P2b . Since X2
is of polynomial size, we conclude that P1 outputs y = y with non-negligible
probability in an emulation with the modiﬁed P˜2 = (P2a , P˜2b ). Next, we claim
that the view of S in the ideal process with Z and Ff is identical to its view in
the emulation by P˜2 = (P2a , P˜2b ). The fact that this holds until S outputs x1 was
shown above in the proof of item (1). The fact that it holds from that point on
follows from the observation that in the emulation by P˜2 = (P2a , P˜2b ), simulator

78

R. Canetti, E. Kushilevitz, and Y. Lindell

S receives f (x1 , x2 ) where x2 ∈r X2 . However, this is exactly the same as it
receives in an ideal execution (where Z chooses x2 ∈r X2 and gives it to the
honest P2 ). It follows that the distribution of messages received by P1 in a real
execution with P˜2 = (P2a , P˜2b ) is exactly the same as the distribution of messages
received by Z from S in the ideal process. Thus, Z’s local P1 -output is identically
distributed to P1 ’s output in the emulation with P˜2 = (P2a , P˜2b ). Since in this
emulation P1 outputs y = y with non-negligible probability, we have that Z’s
local P1 -output in the ideal process is also not equal to y with non-negligible
probability. By the speciﬁcation of Z, it follows that Z outputs 0 in the ideal
process with non-negligible probability. This is a contradiction because in a real
execution Z outputs 0 with at most negligible probability. Thus, Z distinguishes
the real and ideal processes. This completes the proof.

4

A Characterization for Deterministic Functionalities

This section provides a characterization of two-party deterministic functions with
a single output that can be securely computed in the UC framework without any
set-up assumptions. More speciﬁcally, let f : X × X → {0, 1}∗ be a deterministic
function. Say that f is realizable if there exists a protocol that securely realizes
the ideal functionality Ff that obtains a value x1 ∈ X from P1 , a value x2 ∈ X
from P2 , and hands f (x1 , x2 ) to both P1 and P2 . We provide a characterization
of the realizable functions f .
This section is organized as follows: We ﬁrst present a characterization for the
case of functions that depend on only one of the two inputs. Next, we present
two diﬀerent impossibility results for functions that depend on both inputs.
Finally, we combine all results to obtain the desired characterization. All the
impossibility results here are obtained by applying Theorem 2 to the speciﬁc
setting.
It is stressed that the functionalities that we consider provide outputs to both
parties, and that our results do not rely in any way on the fact that a corrupted
party can always abort the computation after it has learned the joint output, and
before the other party does. Indeed, the UC framework explicitly permits such
behavior, by stating that even in the ideal process, parties are not guaranteed
to obtain output (if the ideal-model adversary chooses to do so).
4.1

Functions of One Input

This section considers functions that depend on only one party’s input. We show
that a function of this type can be securely computed in a universally composable
way if and only if it is eﬃciently invertible. Formally,
Deﬁnition 5 A function f : X → {0, 1}∗ is eﬃciently invertible if there exists
a probabilistic polynomial-time inverting machine M such that for every (nonˆ over X,
uniform) polynomial-time samplable distribution X
Prx←Xˆ [M (1k , f (x)) ∈ f −1 (f (x))] > 1 − µ(k)
for some negligible function µ(·).

On the Limitations of Universally Composable Two-Party Computation

79

Discussion. A few remarks regarding Deﬁnition 5: First, note that every function f on a ﬁnite domain X is eﬃciently invertible. Second, note that a function
that is not eﬃciently invertible is not necessarily even weakly one-way. This is
because the deﬁnition of invertibility requires the existence of an inverter that
works for all distributions, rather than only for the uniform distribution (as in
the case of one way functions). In fact, a function that is not eﬃciently invertible
can be constructed from any NP-language L that is not in BP P , as follows. Let
RL be the NP-relation for L, i.e., x ∈ L iﬀ ∃w s.t. RL (x, w) = 1. Then, deﬁne
fL (x, w) = (x, RL (x, w)). It is easy to see that fL is not eﬃciently invertible
ˆ are allowed to be
unless L ∈ BP P (this holds only when the distributions X
non-uniform).
Finally, note that a function fL as deﬁned above corresponds in fact to the
ideal zero-knowledge functionality for the language L. That is, the ideal functionality FfL as deﬁned above is exactly the ideal zero-knowledge functionality
RL
Fzk
for relation RL , as deﬁned in [c01,clos02]. Consequently, the characterization theorem below (Theorem 6) provides, as a special case, an alternative
RL
proof that Fzk
cannot be realized unless L ∈ BP P [c01].
We now show that a function f that depends on only one party’s input is
realizable if and only if it is eﬃciently invertible.
Theorem 6 Let f : X → {0, 1}∗ be a function and let Ff be a functionality that
receives x from P1 and sends f (x) to P2 . Then, Ff can be securely realized in a
universally composable way by a non-trivial protocol if and only if f is eﬃciently
invertible.
Proof: We ﬁrst show that if f is eﬃciently invertible then it can be securely
realized. This is done by the following simple protocol: Upon input x and security
parameter k, party P1 computes y = f (x) and runs the inverting machine M on
(1k , y). Then, P1 sends P2 the value x output by M . (In order to guarantee
security against an external adversary that does not corrupt any party, the
value x will be sent encrypted, say using a shared key that is the result of a
universally composable key exchange protocol run by the parties.) Simulation of
this protocol is demonstrated by constructing a simulator who receives y = f (x),
and simulates P1 sending P2 the output of M (1k , y). Details are omitted.
Let f be a function that is not eﬃciently invertible. Then, for every nonuniform polynomial-time machine M there exists a polynomial-time samplable
ˆ over X such that Pr ˆ [M (1k , f (x)) = f −1 (f (x))] is nondistribution X
x←X
negligible. We now show the impossibility of realizing such an f . Assume, by
contradiction, that there exists a protocol Πf that securely realizes f . Consider
a real execution of Πf with an honest P1 (with input x), and a corrupted P2
who runs a successful split adversarial strategy. (By Theorem 2, such a successful
strategy exists.) The adversary A, who controls P2 , is such that at the conclusion
of the execution, it hands Z the value x obtained in step 2b of Deﬁnition 2.
Finally, deﬁne the environment Z for this scenario to be so that it samples a
ˆ and hands it to P1 . Then, Z outputs 1 if and
value x from some distribution X
−1
only if x ∈ f (f (x)), where x is the value that it receives from A. Observe

80

R. Canetti, E. Kushilevitz, and Y. Lindell

that by item (1) of Deﬁnition 3, f (x ) = f (x) with overwhelming probability
and so in a real execution, Z outputs 1 with overwhelming probability. (Here,
the set X2 described in Deﬁnition 2 contains the empty input.)
Next, consider an ideal execution with the same Z and with an ideal-process
simulator S2 for the above P2 . Clearly, in such an ideal execution S2 receives f (x)
only (because all it sees is the output of the corrupted party P2 ). Nevertheless,
S2 succeeds in handing Z a value x such that f (x ) = f (x); otherwise, Z would
distinguish a real execution from an ideal one. Thus, S2 can be used to construct
an inverting machine M for f : Given y = f (x), M runs S2 , gives it y in the name
of Ff , and outputs whatever value S2 hands to Z. The fact that M is a valid
inverting machine follows from the above argument. That is, if there exists an
ˆ for which M does not succeed in inverting
eﬃciently samplable distribution X
ˆ it distinguishes the
f , then when the environment Z chooses x according to X,
real and ideal executions with non-negligible probability. This contradicts the
fact that f is not eﬃciently invertible, concluding the proof.

Table 1. An Insecure Minor (assuming b = c)
α2 α2
α1 a b
α1 a c

4.2

Functions with Insecure Minors

This section presents an impossibility result for realizing two-input functions
with a special combinatorial property, namely the existence of an insecure minor.
In fact, this property was already used to show non-realizability results in a
diﬀerent context of informational-theoretic security [bmm99].
A function f : X × X → {0, 1}∗ is said to contain an insecure minor if there
exist inputs α1 , α1 , α2 and α2 such that f (α1 , α2 ) = f (α1 , α2 ) and f (α1 , α2 ) =
f (α1 , α2 ); see Table 1. (In the case of boolean functions, the notion of an insecure
minor boils down to the so called “embedded-OR”; see, e.g., [kkmo00].) Such a
function has the property that when P2 has input α2 , then party P1 ’s input is
“hidden” (i.e., given y = f (x1 , α2 ), it is impossible for P2 to know whether P1 ’s
input, x1 , was α1 or α1 ). Furthermore, α1 and α1 are not “equivalent”, in that
when P2 has α2 for input, then the result of the computation with P1 having
x1 = α1 diﬀers from the result when P1 has x1 = α1 (because f (α1 , α2 ) =
f (α1 , α2 )). We stress that there is no requirement that f (α1 , α2 ) = f (α1 , α2 ) or
f (α1 , α2 ) = f (α1 , α2 ) (i.e., in Table 1, a may equal b or c, but clearly not both).
We now show that no function containing an insecure minor can be securely
computed without set-up assumptions. (In the treatment below the roles of P1
and P2 may be switched.)

On the Limitations of Universally Composable Two-Party Computation

81

Theorem 7 Let f be a two-party function containing an insecure minor, and
let Ff be the two-party ideal functionality that receives x1 and x2 from P1 and
P2 respectively, and hands both parties f (x1 , x2 ). Then, Ff cannot be securely
realized by a non-trivial protocol.
Proof: We prove this theorem using Theorem 2 and item (1) of Deﬁnition 3.
As we have mentioned above, a function with an insecure minor hides the input
of P1 . However, by Theorem 2, P2 can obtain P1 ’s input. This results in a
contradiction.
Formally, let f be a function and let α1 , α1 , α2 , α2 form an insecure minor in
f . Assume by contradiction that Ff can be securely realized by a protocol Πf .
Then, consider a real execution of Πf with an honest P1 and a corrupted P2
who runs a successful split adversarial strategy. (By Theorem 2, such a successful
strategy exists.) The environment Z for this execution chooses a pair of inputs
(x1 , x2 ) where x1 ∈r {α1 , α1 } and x2 ∈r {α2 , α2 }.3 (I.e., in this case the set
X2 described in Deﬁnition 2 equals {α2 , α2 }.) Z then gives P1 and P2 their
respective inputs, x1 and x2 . Now, since P2 is successful, P2a must output x1
such that for every x2 ∈ X2 , f (x1 , x2 ) = f (x1 , x2 ). When A receives the value
x1 (from P2 ), it checks if f (x1 , x2 ) = f (α1 , x2 ) or if f (x1 , x2 ) = f (α1 , x2 ) (recall
that A knows x2 ). Note that since α2 ∈ X2 , the value x1 must match only one
of α1 and α1 . A then hands Z the value α1 or α1 appropriately (i.e., according
to which input matches x1 ). Finally, Z outputs 1 if and only if the value that it
receives from A equals x1 . Observe that by item (1) of Deﬁnition 3, in the above
real execution, Z outputs 1 with overwhelming probability.
Next, consider an ideal execution with the above A and Z. By the assumption that Ff is secure, there exists an appropriate ideal process simulator S2 for
the corrupt P2 . Now, S2 is given the output f (x1 , x2 ) and must provide Z with
the value x1 with overwhelming probability. (Recall that in the real execution,
A provides Z with this value with overwhelming probability.) We conclude by
analyzing the probability that S can succeed in such a task. First, with probability 1/2, we have that x2 = α2 . In this case, f (α1 , x2 ) = f (α1 , x2 ). Therefore,
S2 can always succeed in obtaining the correct x1 . However, with probability
1/2, we have that x2 = α2 . In this case, f (α1 , x2 ) = f (α1 , x2 ). Therefore, information theoretically, S2 can obtain the correct x1 with probability at most 1/2.
It follows that Z outputs 0 in such an ideal execution with probability at least
1/4. Therefore, Z distinguishes the ideal and real executions with non-negligible
probability, in contradiction to the security of Πf .
4.3

Functions with Embedded-XOR

This section presents an impossibility result for realizing two-input functions
with another combinatorial property, namely the existence of an embedded-XOR.
A function f is said to contain an embedded-XOR if there exist inputs
def
α1 ,α1 ,α2 and α2 such that the two sets A0 = {f (α1 , α2 ), f (α1 , α2 )} and
3

Since Z is non-uniform, we can assume that it is given an insecure minor of f for
auxiliary input.

82

R. Canetti, E. Kushilevitz, and Y. Lindell
Table 2. An Embedded-XOR – if {a, d} ∩ {b, c} = ∅.
α2 α2
α1 a b
α1 c d

def

A1 = {f (α1 , α2 ), f (α1 , α2 )} are disjoint; see Table 2. (In other words, the
table describes an embedded-XOR if no two elements in a single row or column are equal. The name “embedded-XOR” originates from the case of boolean
functions f , where one can pick A0 = {0} and A1 = {1}.) The intuitive idea is
that none of the parties, based on its input (among those in the embedded-XOR
sub-domain), should be able to bias the output towards one of the sets A0 , A1 of
its choice. In our impossibility proof, we will in fact show a strategy for, say, P2
to bias the output. We now show that no function containing an embedded-XOR
can be securely computed in the plain model.
Theorem 8 Let f be a two-party function containing an embedded-XOR, and
let Ff be the two-party ideal functionality that receives x1 and x2 from P1 and
P2 respectively, and hands both parties f (x1 , x2 ). Then, Ff cannot be securely
realized by a non-trivial protocol.
Proof: Again, we prove this theorem using Theorem 2. However, the use here
is diﬀerent. That is, instead of relying on the extraction property (step 2b of
Deﬁnition 2 and item (1) of Deﬁnition 3), we rely on the fact that P2 can
inﬂuence the output by choosing its input as a function of P1 ’s input (step 1),
and then cause P1 to output the value y that corresponds to these inputs (step 2d
and item (2) of Deﬁnition 3). That is, P2 is able to bias the output, something
which it should not be able to do when a function has an embedded-XOR.
Formally, let f be a function and let α1 , α1 , α2 , α2 form an embedded-XOR
in f with corresponding sets A0 , A1 (as described above). Assume by contradiction that Ff can be securely realized by a protocol Πf . Then, consider a
real execution of Πf with an honest P1 and a corrupted P2 who runs a successful split adversarial strategy. (By Theorem 2, such a successful strategy exists.)
The environment Z for this execution chooses a pair of inputs (x1 , x2 ) where
x1 ∈r {α1 , α1 } and x2 ∈r {α2 , α2 }. (I.e., in this case, the set X2 from Definition 2 equals {α2 , α2 }.) Z then gives P1 and P2 their respective inputs, x1
and x2 . Now, by item (1) of Deﬁnition 3, except with negligible probability, P2a
must output x1 such that for every x2 ∈ X2 , f (x1 , x2 ) = f (x1 , x2 ). Given this
x1 , machine P2b chooses x2 so that f (x1 , x2 ) ∈ A0 . (Such an x2 exists by the
deﬁnition of an embedded-XOR. Also, recall that any eﬃcient strategy by which
P2b chooses x2 is allowed, see step 1 of Deﬁnition 2.) At the conclusion, Z outputs 1 if and only if the output of P1 is in the set A0 . Observe that by item
(2) of Deﬁnition 3, Z outputs 1 with overwhelming probability in the above real
execution.
Next, consider an ideal execution with the above A and Z. By the assumption
that Ff is secure, there exists an appropriate ideal process simulator S2 for the

On the Limitations of Universally Composable Two-Party Computation

83

corrupted P2 . In particular, the result of an ideal execution with S must be that
P1 outputs a value in A0 with overwhelming probability. We now analyze the
probability of this event happening. First, notice that in an ideal execution, S2
must hand the corrupted P2 ’s input to Ff before receiving anything. Thus, S2 has
no information on x1 when it chooses x2 ∈ {α2 , α2 }. Since x1 ∈r {α1 , α1 } and
since, by the deﬁnition of embedded-XOR, exactly one of f (x1 , α2 ), f (x1 , α2 ) is
in A0 (and the other is in A1 ), it follows that no matter what S2 chooses as P2 ’s
input, it cannot cause the output to be in A0 with probability greater than 1/2.
Therefore, Z outputs 1 in an ideal execution with probability at most negligibly
greater than 1/2, while in the real execution Z output 1 with overwhelming
probability; i.e., Z distinguishes the real and ideal executions. This contradicts
the security of Πf .
4.4

A Characterization

Let f : X × X → {0, 1}∗ . Each of Theorems 7 and 8 provides a necessary condition for Ff to be securely realizable (namely, f should not contain an insecure
minor or an embedded-XOR, respectively). Theorem 6 gives a characterization of
those functionalities Ff that are securely realizable, assuming that f depends on
the input of one party only. In this section we show that the combination of these
three theorems is actually quite powerful. Indeed, we show that this provides a
full characterization of the two-party, single output deterministic functions that
are realizable (with the output known to both parties). In fact, we show that
the realizable functions are very simple.
Theorem 9 Let f : X × X → {0, 1}∗ be a function and let Ff be a functionality
that receives x1 and x2 from P1 and P2 respectively, and hands both parties
f (x1 , x2 ). Then, Ff can be securely realized in a universally composable way
by a non-trivial protocol if and only if f is an eﬃciently invertible function
depending on (at most) one of the inputs (x1 or x2 ).
Proof: First, we prove the theorem for the case that f contains an insecureminor or an embedded-XOR (with respect to either P1 or P2 ). By Theorems 7
and 8, in this case Ff cannot be securely realized. Indeed, such functions f do
not solely depend on the input of a single party; that is, for each party there is
some input for which the output depends on the other party’s input.
Next, we prove the theorem for the case that f does not contain an insecureminor (with respect to either P1 or P2 ) or an embedded-XOR. We prove that in
this case f depends on the input of (at most) one party and hence, by Theorem 6,
the present theorem follows. Pick any x ∈ X and let a = f (x, x). Let B1 =
{x1 |f (x1 , x) = a} and B2 = {x2 |f (x, x2 ) = a}. Since f (x, x) = a then both
¯1 and B
¯2 is empty;
sets are non-empty. Next, we claim that at least one of B
¯
¯
otherwise, if there exist α1 ∈ B1 and α2 ∈ B2 , then setting α1 = α2 = x gives
us a minor which is either an insecure minor or an embedded-XOR. To see this,
¯1 , B
¯2 both b and c are
denote b = f (α1 , x) and c = f (x, α2 ); by the deﬁnition of B
diﬀerent than a. Consider the possible values for d = f (α1 , α2 ). If d = b or d = c,

84

R. Canetti, E. Kushilevitz, and Y. Lindell

we get an insecure minor; if d = a or d ∈
/ {a, b, c}, we get an embedded-XOR.
¯1 , B
¯2 is empty; assume, without loss of
Thus, we showed that at least one of B
¯2 . There are two cases:
generality, that it is B
¯1 is also empty: In this case, f is constant. This follows because when
1. B
¯2 = φ, we have that for every x1 , x2 , f (x1 , x) = f (x, x2 ) = a. Assume,
¯1 = B
B
by contradiction, that there exists a point (x1 , x2 ) such that f (x1 , x2 ) = a
(and so f is not constant). Then, x, x1 , x, x2 constitutes an insecure minor,
and we are done.
¯1 is not empty: In this case, we have that for every x1 ∈ B
¯1 the function
2. B
is ﬁxed (i.e., f (x1 , ·) is a constant function). This follows from the following.
Assume by contradiction that there exists a point x2 such that f (x1 , x) =
f (x1 , x2 ) (as must be the case if f (x1 , ·) is not constant). We claim that
x, x1 , x, x2 constitutes an insecure minor. To see this, notice that f (x, x) =
¯2 = φ). However, f (x1 , x) = f (x1 , x2 ). By deﬁnition,
f (x, x2 ) = a (because B
this is an insecure minor.
We conclude that either f is a constant function, or for every x1 , the function
f (x1 , ·) is constant. That is, f depends only on the input of P1 , as needed.

5

Probabilistic Functionalities

This section concentrates on probabilistic two-party functionalities where both
parties obtain the same output. We show that the only functionalities that can
be realized are those where one of the parties can almost completely determine
the (joint) output. This rules out the possibility of realizing any “coin-tossing
style” functionality, or any functionality whose outcome remains “unpredictable”
even if one of the parties deviates from the protocol. It is stressed, however, that
our result does not rule out the realizability of other useful probabilistic functionalities, such as functionalities where, when both parties remain uncorrupted,
they can obtain a random value that is unknown to the adversary. An important
example of such a functionality, that is indeed realizable, is key-exchange.
Let f = {fk } be a family of functions where, for each value of the security
parameter k, we have that fk : X × X → {0, 1}∗ is a probabilistic function.
We say that f is unpredictable for P2 if there exists a value x1 ∈ X such that
for all x2 ∈ X and for all v ∈ {0, 1}∗ it holds that Pr(fk (x1 , x2 ) = v) < 1 − ,
where = (k) is a non-negligible function. (We term such x1 a safe value.)
Unpredictable functions for P1 are deﬁned analogously. A function family is
unpredictable if it is unpredictable for either P1 or P2 .
Theorem 10 Let f = {fk } be an unpredictable function family and let Ff be
the two-party functionality that, given a security parameter k, waits to receive
x1 from P1 and x2 from P2 , then samples a value v from the distribution of
fk (x1 , x2 ), and hands v to both P1 and P2 . Then, Ff cannot be securely realized
by any non-trivial protocol.
The proof can be found in the full version of this paper [ckl03], and follows
from a theorem for the probabilistic case that is analogous to Theorem 2.

On the Limitations of Universally Composable Two-Party Computation

85

References
[b91]

[b96]
[bmm99]

[bgw88]

[c01]

[cfgn96]
[cf01]
[ck01]

[ck02]

[ckl03]

[clos02]

[ck89]
[c86]
[ddn00]
[dns98]
[gm00]
[gk88]
[gmw87]

[gl90]

D. Beaver. Secure Multi-party Protocols and Zero-Knowledge Proof Systems Tolerating a Faulty Minority. Journal of Cryptology, 4(2):75–122,
1991.
D. Beaver. Adaptive Zero-Knowledge and Computational Equivocation. In
28th STOC, pages 629–638, 1996.
A. Beimel, T. Malkin and S. Micali. The All-or-Nothing Nature of TwoParty Secure Computation. In CRYPTO’99, Springer-Verlag (LNCS 1666),
pages 80–97, 1999.
M. Ben-Or, S. Goldwasser and A. Wigderson. Completeness Theorems
for Non-Cryptographic Fault-Tolerant Distributed Computation. In 20th
STOC, pages 1–10, 1988.
R. Canetti. Universally Composable Security: A New Paradigm for Cryptographic Protocols. In 42nd FOCS, pages 136–145, 2001. Full version
available at http://eprint.iacr.org/2000/067.
R. Canetti, U. Feige, O. Goldreich and M. Naor. Adaptively Secure MultiParty Computation. In 28th STOC, pages 639–648, 1996.
R. Canetti and M. Fischlin. Universally Composable Commitments. In
CRYPTO 2001, Springer-Verlag (LNCS 2139), pages 19–40, 2001.
R. Canetti and H. Krawczyk. Analysis of Key Exchange Protocols and
Their Use for Building Secure Channels. In Eurocrypt 2001, Springer-Verlag
(LNCS 2045), pages 453–474, 2001.
R. Canetti and H. Krawczyk. Universally composable key exchange and
secure channels. In Eurocrypt 2002, Springer-Verlag (LNCS 2332), pages
337–351, 2002.
R. Canetti, E. Kushilevitz and Y. Lindell. On the Limitations of Universally
Composable Two-Party Computation Without Set-up Assumptions (full
version). Cryptology ePrint Archive, http://eprint.iacr.org/, 2003.
R. Canetti, Y. Lindell, R. Ostrovsky and A. Sahai. Universally Composable
Two-Party and Multi-Party Computation. In 34th STOC, pages 494–503,
2002.
B. Chor, and E. Kushilevitz. A Zero-One Law for Boolean Privacy. In 21st
STOC, pages 62–72, 1989.
R. Cleve. Limits on the security of coin-ﬂips when half the processors are
faulty. In 18th STOC, pages 364–369, 1986.
D. Dolev, C. Dwork and M. Naor. Non-malleable cryptography. SIAM
Journal of Computing, 30(2):391–437, 2000.
C. Dwork, M. Naor, and A. Sahai. Concurrent Zero-Knowledge. In 30th
STOC, pages 409–418, 1998.
J. Garay and P. Mackenzie. Concurrent Oblivious Transfer. In 41st FOCS,
pages 314–324, 2000.
O. Goldreich and H. Krawczyk. On the composition of zero-knowledge
proof systems. SIAM Journal of Computing, 25(1):169–192, 1996.
O. Goldreich, S. Micali and A. Wigderson. How to Play any Mental Game
– A Completeness Theorem for Protocols with Honest Majority. In 19th
STOC, pages 218–229, 1987.
S. Goldwasser and L. Levin. Fair Computation of General Functions in
Presence of Immoral Majority. In CRYPTO’90, Springer-Verlag (LNCS
537), pages 77–93, 1990.

86
[hms03]

R. Canetti, E. Kushilevitz, and Y. Lindell

D. Hofheinz, J. M¨
uller-Quade and R. Steinwandt. On Modeling IND-CCA
Security in Cryptographic Protocols. Cryptology ePrint Archive, Report
2003/024, http://eprint.iacr.org/, 2003.
[kkmo00] J. Kilian, E. Kushilevitz, S. Micali, and R. Ostrovsky. Reducibility and
Completeness in Private Computations. SICOMP, 29(4):1189–1208, 2000.
[k00]
J. Kilian. More general completeness theorems for secure two-party computation. In 32nd STOC, pages 316–324, 2000.
[k89]
E. Kushilevitz. Privacy and Communication Complexity. In 30th FOCS,
pages 416–421, 1989.
[mr91]
S. Micali and P. Rogaway. Secure computation. Unpublished manuscript,
1992. Preliminary version in CRYPTO’91, Springer-Verlag (LNCS 576),
pages 392–404, 1991.
[ny90]
M. Naor and M. Yung. Public key cryptosystems provably secure against
chosen ciphertext attacks. In 22nd STOC, 427–437, 1990.
[rb89]
T. Rabin and M. Ben-Or. Veriﬁable Secret Sharing and Multi-party Protocols with Honest Majority. In 21st STOC, pages 73–85, 1989.
[rs91]
C. Rackoﬀ and D. Simon. Non-interactive zero-knowledge proof of knowledge and chosen ciphertext attack. In CRYPTO’91, Springer-Verlag (LNCS
576), pages 433–444, 1991.
[rk99]
R. Richardson and J. Kilian. On the Concurrent Composition of ZeroKnowledge Proofs. In Eurocrypt’99, Springer-Verlag (LNCS 1592), pages
415–431, 1999.

