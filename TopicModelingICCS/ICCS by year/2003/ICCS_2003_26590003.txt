A Bayes Algorithm for the Multitask Pattern
Recognition Problem – Direct Approach
Edward Puchala
Wroclaw University of Technology, Chair of Systems and Computer Networks,
Wybrzeze Wyspianskiego 27, 50-370 Wroclaw, Poland
puchala@zssk.pwr.wroc.pl

Abstract. The paper presents algorithms of the multitask recognition for the
direct approach. First one, with full probabilistic information and second one,
algorithms with learning sequence. Algorithm with full probabilistic
information was working on basis of Bayes decision theory. Full probabilistic
information in a pattern recognition task, denotes a knowledge of the classes
probabilities and the class-conditional probability density functions. Optimal
algorithm for the selected loss function will be presented. Some tests for
algorithm with learning were done.

1 Introduction
The classical pattern recognition problem is concerned with the assignment of a
given pattern to one and only one class from a given set of classes. Multitask
classification problem refers to a situation in which an object undergoes several
classification tasks. Each task denotes recognition from a different point of view and
with respect to different set of classes. For example, such a situation is typical for
compound medical decision problems where the first classification denotes the
answer to the question about the kind of disease, the next task states recognition of the
stadium of disease, the third one determines the kind of therapy, etc. Let us consider
the non-Hodgkin lymphoma as a common dilemma in hematology practice. For this
medical problem we can utilise the multitask classification ( this is caused by the
structure of the decision process), which leads to the following scheme. In the first
task of recognition, we arrive at a decision i1 about the lymphoma type. After the type
of lymphoma has been determined, it is essential for diagnosis and therapy to
recognize its stage. The values of decision i2 denote the first, the second, the third and
the fourth stage of lymphoma development, respectively. Apart from that, each stage
of lymphoma may assume two forms. Which of such forms occurs is determined by
decision i3. If i3=1, then lymphoma assumes the form A (there are no additional
symptoms). For i3=2, lymphoma takes on form B (there are other symptoms, as well).
Decisions i4 determines therapy, that is one of the known schemes of treatment
(e.g. CHOP, BCVP, COMBA, MEVA, COP-BLAM-I). A therapy (scheme of
treatment) cannot be used in its original form in every case. Because of the side
P.M.A. Sloot et al. (Eds.): ICCS 2003, LNCS 2659, pp. 3−10, 2003.
 Springer-Verlag Berlin Heidelberg 2003

4

E. Puchala

effects of cytostatic treatment it is necessary to modify such a scheme. Decision about
modification is i5. In the present paper I have focused my attention on the concept of
multitask pattern recognition. In particular, so-called direct approach for problem
solution will be taken into consideration.

2 Direct Approach to the Multitask Pattern Recognition
Algorithm
Let us consider N-task pattern recognition problem. We shall assume that the
vector of features xk∈Xk and the class number jk∈Mk for the k-th recognition task of
the pattern being recognized are observed values of random variables xk and jk,
respectively [5]. When a priori probabilities of the whole random vector
j=(j1,j2,...,jN) denote as P(j=j)=p(j)=p(j1,j2,..jN) and class-conditional probability
density functions of x=(x1,,x2,...,xN) denote as f(x1,x2,..xN/j1,j2,..,jN) are known then
we can derive the optimal Bayes recognition algorithm minimizing the risk function
[3], [4]:
R = E L(i, j)

(1)

i.e. expected value of the loss incurred if a pattern from the classes
j = ( j1 , j 2 ,..., j N ) is assigned to the classes i = (i1 , i2 ,..., i N ) .
In the case of multitask classification we can define the action of recognizer, which
leads to so-called direct approach. [1].
In that instance, classification is a single action. The object is classified to the
classes
i = (i1 , i2 ,..., i N ) on the basis of full features vector

x = ( x1 , x 2 ,..., x N ) simultaneously. That we can see below (Fig.1).

x1
x2
xN

..
.

Ψ(x)

..
.

Fig. 1. Block scheme of the direct multitask pattern recognition algorithm.

i1
i
i

A Bayes Algorithm for the Multitask Pattern Recognition Problem

Let

5

Ψ (x) denotes direct pattern recognition algorithm:

Ψ ( x) = Ψ ( x1 , x 2 ,..., x N ) = (i1 , i 2 ,..., i N ) xk∈Xk, ik∈Mk

(2)

Minimization of the risk function R:

R[Ψ ( x)] = E{L(i1 , i2 ,..., i N ), ( j1 , j 2 ,..., j N )}
where L denotes the loss function, leads to the optimal algorithm

(3)

Ψ* .

R (Ψ * ) = min R (Ψ )

(4)

Ψ

Average risk (3) expresses formula:

R(Ψ ) = ∫ { ∑
X

∑ ... ∑ L[(i , i
1

j1∈M 1 j 2 ∈M 2

2

,...i N ), ( j1 , j 2 ,..., j N )]*

j N ∈M N

(5)

* p ( j1 , j 2 ,... j N / x)} f ( x) dx
where:

p ( j1 , j 2 ,... j N / x)} =

p ( j1 , j 2 ,... j N ) f ( x / j1 , j 2 ,... j N )
f ( x)

(6)

denotes a’posteriori probability for the set of classes j1, j2,…,jN As we can easily
show the formula:

r (i1 , i 2 ...i N , x) = E[ L(i1 , i2 ,..., i N ), ( j1 , j 2 ,... j N ) / x] =
=

∑ ∑ ... ∑ L[(i , i
1

j1∈M 1 j2 ∈M 2

2

(7)

,..., i N ), ( j1 , j 2 ,..., j N )] × p ( j1 , j 2 ,..., j N / x)

j N ∈M N

presents average conditional risk. Hence, the Bayes algorithm for multitask pattern
recognition for direct approach may be derived. As we can see, it is result of

6

E. Puchala

optimization problem (4) solution. Thus, we have obtained optimal algorithm like
below:

Ψ ∗ ( x) = (i1 , i2 ,..., i N ) if

r (i1 , i 2 ,..., i N , x) = ' min
r (i1' , i 2' ,..., i N' , x)
'
'

(8)

i1 ,i 2 ,...i N

Ψ ∗ ( x) = (i1 , i2 ,..., i N ) if

∑ ∑ ... ∑ L[(i , i ,..., i
1

j1∈M 1 j 2 ∈M 2

2

N

), ( j1 , j 2 ,..., j N )] × p( j1 , j 2 ,..., j N ) ×
(9)

j N ∈M N

× f ( x / j1 , j 2 ,..., j N ) =
= ', min
'
'

i1 ,i2 ,...,i N

∑ ∑ ... ∑ L[(i , i ,.., i
'
1

j1∈M 1 j 2 ∈M 2

'
2

'
N

), ( j1 , j 2 ,..., j N ) ×

j N ∈M N

× p( j1 , j 2 ,... j N ) × f ( x / j1 , j 2 ,... j N )
Let us consider characteristic form of loss function L. Value of this function depends
on number of misclassification decisions:

L [(i1 , i 2 ,..., i N ), ( j1 , j 2 ,..., j N )] = n
Where n denotes number of pairs (algorithm’s decision

(10)

i k and real class) for witch

i k = j k . In this case, average conditional risk has the following form:
r (i1 , i 2 ,..., i N , x) = N − [ p(i1 / x) + p (i 2 / x) + ... + p (i N / x)]

(11)

Because number of tasks N is constant for each practical problem and we are looking
for minimum of average conditional risk, then optimal multitask pattern recognition
algorithm for so called direct approach will be allowed to write like below:

Ψ ∗ ( x) = (i1 , i2 ,..., i N ) if
N

∑ p(ik / x) = 'max
'
'
k =1

i1 ,i2 ,...,i N

N

∑ p(ik' / x)
k =1

(12)

A Bayes Algorithm for the Multitask Pattern Recognition Problem

7

The average risk function, for the loss function L (10), is the sum of the incorrect
classification probabilities in individual tasks:
N

N

n =1

n =1

R[Ψ ] = ∑ Pe (n) = ∑ [1 − Pc (n)]
Pc (n) =

∑ ∑ ... ∑ q( j

j1∈M 1 j 2 ∈M 2

n

(13)

/ j1 , j 2 ,..., j N ) × p ( j1 , j 2 ,..., j N )

j N ∈M N

q ( j n / j1 , j 2 ,..., j N ) is the probability of correct classification for object
from classes ( j1 , j 2 ,..., j N ) in n-th task:

where

q ( jn / j1 , j2 ,..., j N ) =
=

∑ ... ∑

∑ ... ∑

∫ f ( x / j ,..., j
1

N

)dx

(14)

i1 ∈M 1 i n −1 ∈M n −1 i n +1 ∈M n +1 i N ∈M N D ( i1 ,...,iN )
x

D x( i1 ,...,iN ) - decision area for algorithm Ψ (x) .

3 Multitask Recognition with Learning
In the real world there is often a lack of exact knowledge of a priori probabilities
and class-conditional probability density functions. For instance, there are situations
in which only a learning sequence:
SL = ( x1, j1 ), ( x2, j2 ),...,( xm ,jm )

(15)

where:
xk=(x1k,...,xNk)∈X,

jk=(j1k,...,jNk)∈M

(16)

as a set of correctly classified samples, is known.
In this case we can use the algorithms known for conventional pattern recognition,
but now algorithm must be formulated in the version corresponding to above concept.
As an example let us consider α - nearest neighbour ( α -NN) multitask recognition
algorithm for direct approach.

8

E. Puchala

Let us denote:

pm ( j) =

(15)

Ij
m

estimator of the a’priori classes probability,
where:
I j - number of objects from class j in learning sequence,
m – number of objects in learning sequence,
and

f m ( x / j) =

α j ( x)

(16)

I j × V j ( x)

estimator of the density function for α -NN algorithm,
where:
I j - number of objects from class j in learning sequence,

α - number of object’s x neighbours,
α j (x) - number of objects from class j which are neighbours of x and belong to

the area with

V j (x) volume.

On the basis of (12) and (15), (16) final form of the multitask pattern recognition
algorithm with learning is done:

Ψ m ( x1 , x 2 ,..., x N ) = (i1 , i 2 ,...i N ) if
N

∑α
k =1

ik

( x) / Vik ( x) = ' max
'
'

i1 ,i2 ,...,i N

(17)

N

∑α
k =1

ik'

( x) /Vi ' ( x)
k

In the Fig.2 we can see values of probability of correct classification (for α nearest neighbour ( α -NN) multitask recognition algorithm for direct approach)
depending on the length of learning sequence. Probability of correct classification
rises, when numbers of elements in learning sequence rises too. When m ≅ 350 or
more, probability has values between 0,8 and 0,9. These results where obtained for
computer simulated (generated) set of correctly classified samples.

probability of correct classyfication

A Bayes Algorithm for the Multitask Pattern Recognition Problem

9

1
0,9
0,8
0,7

α=3
α=5
α=7

0,6
0,5
0,4
0,3
0,2
0,1
0
50

100

150

200

250

300

350

400

450

500

length of learning sequence

Fig. 2. Probability of correct classification as function of learning sequence’s length, for
various number of neighbors α ( α - NN algorithm)

The superiority the multitask α -NN algorithm in direct version over the classical
pattern recognition one demonstrates the effectiveness of this concept in such
multitask classification problems for which the decomposition is necessary from the
functional or computational point of view (e.g. in medical diagnosis). Direct approach
to multitask recognition algorithms gives better results then decomposed approach
because such algorithms take into consideration correlation between individual
classification problems.

Acknowledgement.
The work presented in this paper is a part of the project The Artificial Intelligence
Methods for Decisions Support Systems. Analysis and Practical Applications realized
in the Higher State School of Professional Education in Legnica.

References
1. Kurzynski, M., Puchala, E., :Algorithms of the multiperspective recognition. Proc. of the
11th Int. Conf. on Pattern Recognition, Hague (1992)
2. Puchala, E., Kurzynski, M.,: A branch-and-bound algorithm for optimization of
multiperspective classifier. Proceedings of the 12th IAPR, Jerusalem, Israel, (1994) 235-239

10

E. Puchala

3. Parzen, E.,: On estimation of a probability density function and mode. Ann. Math. Statist.,
(1962) Vol.33, 1065-1076
4. Duda, R., Hart, P.,: Pattern classification and scene analysis. John Wiley & Sons, New York
(1973)
5. Fukunaga, K., : Introduction to Statistical Pattern Recognition, Academic Press,
New York (1972).

