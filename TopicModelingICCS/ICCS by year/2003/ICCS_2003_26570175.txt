Parallelization Scheme for an Approximate
Solution to Time Constraint Problems
Tuan-Anh Nguyen, Pierre Kuonen
University of Applied Sciences Western Switzerland, EIA-FR
tuananh.nguyen@epfl.ch, pierre.kuonen@eif.ch

Abstract. Solving time constraint problems in wide area distributed
computing environment is a challenge. We address this challenge by providing programmers a method to express their problems based on a parallelization scheme. The scheme consists of a decomposition tree defining
possible decompositions of a problem into sub-problems and the decomposition dependency graph showing the relative order of execution of
sub-problems. We have developed algorithms to address the following
issues of the parallelization scheme: the execution of the scheme, the
dependency of sub-problems, the min-max problem related to the time
constraints of decomposed components. A genetic algorithm has been
developed for the max-min problem. Experiment results show the good
scalability of the algorithms up to thousands of nodes in each decomposition.

1

Introduction

Many practical problems require that the solution be obtained within some specific time constraints. When a sequential task can not give a satisfactory solution,
this task should be decomposed into sub-tasks to be solved in parallel to meet
the time constraint.
Number of on going researches on time constraint problems focus on various
aspects of scheduling issues such as in real-time CORBA[3], heterogeneous task
mapping[4, 5] or multiple variant programming methodology. Multiple variant
programming, for instance, enables the user to elaborate a number of versions
to solve the problem into a single program. Each version has a different level of
computational requirements. Depending on the environment, a suitable version
will be executed. In [7], the author describes an evolution approach for scheduling
several variances of independent tasks on a set of identical processors to minimize
the total violations of deadline. Gunnels, in [8], presents variances of matrix
multiplication algorithms and the evaluation of required performance based on
the shape of matrices.
We present in this paper an approach for solving time constraint problems
based on dynamic parallelism. Dynamic parallelism enables applications to exploit automatically and dynamically the suitable scale of parallelism, depending
on the available resources. This is an important issue in efficient achievement
P.M.A. Sloot et al. (Eds.): ICCS 2003, LNCS 2657, pp. 175−184, 2003.
 Springer-Verlag Berlin Heidelberg 2003

176

T.-A. Nguyen and P. Kuonen

of heterogeneous computing since the applications should somehow adapt themselves to the heterogeneity and the volatility of the environment.
The paper is organized as follow: section 2 presents a parallelization scheme
for developers to describe their time constraint applications. The scheme provides
a new programming paradigm based on the decomposition tree and decomposition dependency graph (DDG). Then in section 3, we describe an algorithm
that achieves dynamic parallelism to satisfy the time constraint in wide area
heterogeneous environments. This algorithm leads to a min-max problem that
we will address in section 4 . Section 5 is the conclusion.

2

Parallelization Scheme

A parallelization scheme S of a given problem P consists of a decomposition tree
and a set of decomposition dependency graphs (DDG) which are defined bellow:
bigger

P1

Parallelism grain

P11

P12

AND
P13

Problem

decomposition

5
2

OR
1

7

4
3

6

Sub-problems

smaller

DDG

Fig. 1. Decomposition Tree

Fig. 2. Decomposition Dependency Graph

Definition 1 (Decomposition tree). The composition tree DT(P) of a given
problem P is constructed as follow:
Step 1: Problem P (also known as P1 ) is decomposed into sub-problems P11 , P12 . . . P1n .
This is level L1 of the decomposition. The decomposition tree is constructed
with the root as P1 and the child nodes as P11 , P12 . . . P1n . We denote the
decomposition set of P1 : D(P1 ) = {P11 , P12 , ...P1n }.
Step 2: Let Pij be a leaf node of the tree. The set of smaller problems derived
from Pij , denoted as D(Pij ), forms the child nodes of Pij in the decomposition tree. The above process is recursively repeated until the level the user
wants.
Definition 2 (Decomposition Dependency Graph). Consider the decomposition of a problem P into sub-problems D(P ). The decomposition dependency
graph of P is defined as a directed acyclic graph DDG(P) = D(P ), E with
the set of vertices D(P ) and the set of edges E ⊆ D(P ) × D(P ). Each edge
e = Pi , Pj ∈ E means solving Pj should be after solving Pi .
The decomposition tree DT(P) represents all possible paths to the solution
of problem P . It consists of two types of parallelism:
OR parallelism The relationship between P and D(P ): a solution can be obtained by solving P sequentially or by solving D(P ) in parallel.

Parallelization Scheme for an Approximate Solution to Time Constraint Problems

177

AND parallelism The relationship among problems within the same decomposition set D(P ): if D(P ) = {P1 , P2 . . . Pn } then the solution can be obtained by solving P1 P2 . . . Pn .
While the decomposition tree gives an overall view of the parallelization
process, the DDG shows the structure of parallelization. The DDG defines the
order of solving for the set of sub-problems (Fig. 2). It is similar to the data flow
graph however, DDG is not a data flow graph. For instance, there is no edge in
DDG between two pipelined sub-problems.
Definition 3 (Decomposition cut). A decomposition cut of a tree is a set of
nodes χ such that for every set ζ of nodes of a path from the root to any leaf,
| ζ χ |= 1.
Theorem 1. A decomposition cut of the tree forms a solution to the problem.
Proof. Each decomposition cut is an ”AND” predicate of sub-problems. Executing the resolution rule: replacing all sub problems of the same parent by their
parent problem (OR parallelism) generates an equivalent cut. The recursion of
the rule leads to the original solution to the problem P (the root).
Definition 4. An N-complete decomposition tree of degree δ, denoted by T (δ, N )
is a tree where all paths from the root to leaves have the same length N and each
node except the leaf nodes has exactly δ child nodes.
Theorem 2. The total number of decomposition cut Un of T (δ, N ) satisfies:
Un ≥ 2δ(n−1) for n ≥ 1)
Proof. We notice that T (δ, N ) is constructed by δ trees T (δ, N − 1). Therefore
the number of cuts Un can be calculated as the combination of all of cuts of δ
trees T (δ, N − 1) plus the cut at the root: Un = (Un−1 )δ + 1. For n ≥ 1, we can
easily see that Un ≥ 2δ(n−1)
An exhausted search for an optimal solution is an NP-complete problem. In the
next section, we will present an algorithm to find an acceptable solution based
on the parallelization scheme:
S(P ) =< DT (P ), {DDG(Pi )|Pi ∈ DT (P )} >, where P is the original problem to be solved.

3
3.1

An Algorithm for Solving Time Constraint Problems
Problem Statement

Given problem P with the time constraint T . We assume:
1. The parallelization scheme S(P ) of P is known.
2. For any node Pi in DT (P ), the complexity C(Pi ) is known (or can be
estimated).

178

T.-A. Nguyen and P. Kuonen

3. We do not know the complete resources set in the highly heterogeneous
computational environment.
The first assumption requires the programmer to specify the parallelization
scheme. The second assumption describes the class of applications. In such applications, for a given input, we should know the total computation power needed
(e.g. in term of Mflop). In many case, this number is unknown, therefore, an
estimation is acceptable.
The third assumption is about the computational environment. We tend to
develop a model for some uncertainty environments such as Grid[1] or Peer-toPeer[2]. In such environments, users can not rely on the predefined resources.
Instead, they should discover resources on the fly. Based on the discovery results,
the model automatically selects a suitable grain of parallelism.
We state our objective as follow: given a problem P and its parallelization
scheme S(P ), solve P within the user specified time constraint T .
We need to deal with how to find a suitable solution (among all potential
solutions) that satisfies the time constraint. This is a lack information problem
since we do not know about the resource characteristics. In addition, we only
know the time constraint T0 = T of the root problem P0 in S(P )
This is similar to the task scheduling problem in which the user need to choose
among possible assignments the one that satisfies some criteria. However, our
problem is more complex since we have to find a suitable decomposition cut
that fits the computational environment. Finding the optimal solution is an NPcomplete problem. In addition, since the computational environment is dynamic
as the assumption, the instance of the solution can vary time by time.
3.2

Algorithm

Input:
– A decomposition tree whose root is P0 .
– The time constraint T0 .
Output: A configuration of the solution that satisfies the time constraint.
Algorithm:
S1 Let P = P0 (the root of the decomposition tree).
Let T = T0 (the time constraint provided by the user).
S2 Find a resource with the effective power C(P )/T .
If success, assign P to that resource to solve sequentially and return.
If not, go to step S3.
S3 If D(P0 ) = ∅ then return fail.
For each child nodes Pi of P:
– Evaluate the time constraint Ti of Pi (see section 4).
– Perform recursively the algorithm with inputs: the decomposition
tree whose root is Pi and the time constraint Ti
The algorithm shows how the time constraint problem be solved. We start
with the root P0 where we know the time constraint. From the assumption of
known complexity C(P0 ), we can estimate the computing power of the resource

Parallelization Scheme for an Approximate Solution to Time Constraint Problems

179

needed. We first try to solve the problem at the root sequentially (S1) by allocating the resource in the environment. If no such a resource exists, we need to
find an alternative solution based on the composition set D(P0 ). We know the
time constraint T0 to solve D(P0 ). However, the time constraint for each problem Pi ∈ D(P0 ) is unknown at the moment. If the DDG(P0 ) has no edge (all
sub-problems are independent), the time constraint of sub-problems is T0 . Otherwise, time constraints of sub-problems are dependent. We provide a method
to estimate these time constraints in section 4. When all time constraints of
problems in D(P0 ) are evaluated, we can repeat this process for the sub-trees
whose roots are in D(P0 ).

4

Time Constraints in the Decomposition Tree

Let consider a single decomposition of problem P into subproblems D(P ) =
{P1 , P2 , . . . , Pn }. Suppose that we know the time constraint T of P . We need to
find the time constraints T1 , T2 , . . . , Tn for P1 , P2 , . . . , Pn .
Definition 5 (Sequential diagram). Given a DDG of problem P, a step is
the minimum execution unit so that at least one problem is solved. Sequential
diagram is a directed acyclic graph < V, E > where V = {S1 , S2 , . . . , Sm } is the
set of steps and E ⊆ V × V is the set of n =| D(P ) | edges whose labels are
P1 , P2 , . . . , Pn . Sequential diagram must satisfy:
– ∀ < Si , Sj >∈ E ⇒ i < j.
– ∀i < m ⇒< Si , Si+1 >∈ E.
– If Pk is the label of < Si , Sj > then Pk should start at step Si but not before
and Pk should be finished before Sj .
P5
AND

Problem

decomposition

5
2

OR
1

7

4
3

S1

P1

S2

P2

S3

P4

6

Sub-problems

P3

DDG

S4

P6

S5

P7

Fig. 3. Sequential Diagram

Figure 3 shows an example of DDG and its sequential diagram. Sequential
diagram specifies the start points and the end points of sub-problems. It is used
to show the time dependency of sub-problems and to schedule the tasks in order
to satisfy the overall time constraint.
4.1

Algorithm to FFind the Sequential Diagram

Input A DDG of problem P.
Output A sequential graph G.
Algorithm

180

T.-A. Nguyen and P. Kuonen

S1 Let L(Pi ) be the mark status of vertex Pi in DDG(P).
Initially, ∀Pi , L(Pi ) = 0 (”unmarked”).
S2 For every unmarked vertex Pi whose all input vertices are marked
– Let s = 0 if Pi has no ingoing edge,
otherwise s = max{L(Pk )| < Pk , Pi > is an edge in DDG(P)}.
– Mark Pi as L(Pi ) = s + 1
Repeat this step until all vertices are marked.
S3 Let m = max{L(Pi )}. The sequential diagram consists of (m+1) vertices
S1 , S2 , . . . , Sm , Sm+1 = Ss and n edges in D(P ) = {P1 , P2 , . . . , Pn }. Each
edge Pj starts at vertex SLPj and ends at Sx where x is the minimum
value of mark status of all output vertices of Pj in DDG(P) or (m + 1)
if Pj has no output vertex.
The idea is to find the earliest step in which the problem can be started (S1,
S2). This step is defines as the next step of the latest step of all problems that
this problem depends on. S2 will always terminate because DDG(P) is a graph
without circle. S3 constructs the sequential diagram that satisfies the definition
since from the way we mark the vertices in DDG(P), the index of start node is
always greater than the index of the end node; there always exists an edge that
connects Si with Si+1 (if (i < m)).
4.2

Time Constraint of Sub-problems

Until now, we only know the time constraint T of the root problem P. The
question is if we can not solve P sequentially within the time T, we will need to
solve the decomposition sub-problem set D(P ). Hence we need to calculate the
time constraint for each Pi ∈ D(P ).
s
are the time constraints of steps S1 , S2 , . . . , Sm .
Let assume T1s , T2s , . . . , Tm
Let α be the parallel efficiency coefficient (0 < α ≤ 1). In order to satisfy the
overall time constraint T, we have:
∀i, Tis > 0

(1)

Σ Tjs ≤ αT

(2)

m

j=1

In the worst case, (2) becomes:
m

Σ Tjs = αT

j=1

(3)

The time constraint Ti of problem Pi is determined based on the time constraints of steps:
l−1

Ti = Σ Tjs
j=k

where Pi is the label of edge < Sk , Sl >

(4)

There are many solutions satisfying (1) and (3). In section ??, we assume that
the number and the characteristics of resources in the environment are unknown.

Parallelization Scheme for an Approximate Solution to Time Constraint Problems

181

Therefore, we need to find the time constraint of each step to increase the chance
to find resources for sub-problems. We choose the following criterion: find the
time constraints of steps such that they minimize the maximum computation
power required by all sub-problems Pi of a decomposition.
Let C= IR+ and T= IR+ be the complexity space and the time constraint
space of the problem. For each problem Pi , the resource function gi : C × T →
IR+ defines the mapping of a sequential solution of problem with the complexity c
and the time constraint t to the requirement of resource with the power gi (c, t). In
the simple case where the complexity of problem is the total number of floatingpoint operations, the resource function can be evaluated as the number of flops:
gi (c, t) =

c
t

(5)

s
We need to find T1s , T2s , . . . , Tm
satisfying the conditions (1) and (3) such
that:
s
]
[T1s , T2s , . . . , Tm
= arg min max{g1 (C(P1 ), T1 ), g2 (C(P2 ), T2 ), . . . , gn (C(Pn ), Tn )}

(6)

where C(Pi ) is the complexity of problem Pi , Ti is the time constraint of
problem Pi that satisfies (4).
This is a min-max problem with constraints. Generally, the optimal solution
to (6) leads to an NP-complete problem. Therefore, instead of searching for the
optimal solution, we will find an approximate solution using a genetic algorithm.
The algorithm will be describe in section 4.3.
Now we consider a special case where each problem spans exactly one step
(e.i. n=m). The solution to (6) can be obtained by considering the complexity
C(Pi ) as the ”weight” for the time constraint Tis (see [6] for more details):
Tis =
4.3

C(Pi )
αT
C(Pj )

(7)

Genetic Algorithm for Min-Max Problem

s
We find an approximate solution of T1s , T2s , . . . , Tm
to (6) with the conditions (1)
and (3).
The algorithm is described as follow: the population consists of W individuals. Each individual is visualized as a circle with circumference of αT . The circle
s
. By this representation,
is split into m sectors whose lengths are T1s , T2s , . . . , Tm
the constraints (1) and (3) are satisfied. Initially, all W individuals are randomly selected. The evolution process is performed by mutation and crossover
operations on the population with the correspondent probabilities ρ1 and ρ2 .

Mutation. For an individual D, we randomly select a sector Tis of the circle
and increase by x% ( x is a random number in range < −100 . . . 100 > \ {0},
negative value of x means ”decrease Tis ”). All other sectors Tjs (j = i) will
be adjusted accordingly:

182

T.-A. Nguyen and P. Kuonen

Tj,s =

Tjs 1 +

x
100

,

if i=j;

xT s

Tjs 1 − 100(αTi−T s ) , otherwise.
i
Crossover. This operation consists of 2 steps: first, randomly select two individuals from the population, select the cut index on the circle and swap two
parts of the two individuals to generate the new generation (see fig. 5); then
normalize the other parts of the circle by shrinking or expanding so that the
new circles have the same circumference αT .
T s1

T sm

T s1

T sm
T

s

T s2

2

T s3

T s3

Mutation

T s4

T s4

Fig. 4. Mutation operation

After performing mutation and crossover, a new generation is created. For
each individual, a fitness function obtained from (6) will be evaluated:
s
F (T1s , T2s , . . . , Tm
) = max{g1 (C(P1 ), T1 ), g2 (C(P2 ), T2 ), . . . , gn (C(Pn ), Tn )} (8)

Fitness function shows how ”good” an individual is: the smaller the fitness
value, the better the match of the individual to the target. In order to keep the
fix size population after performing crossover and mutation, we remove ”bad”
individuals with the biggest values of fitness.
The evolution process stops when a number of generations has been reached
or the ”best” individual does not improve after a number of iterations (e.g. 100
iterations).
T s1

Tsm

T sm

T s2
Ts

T s1

T s2

Ts

T

4

T s5
T sm

T

s

T sm

T s1
T s2

T s3

3

s

T s3

4

T s4

T s5
Crossover

1

T sm

Normalization

T s1

m

T s5

T s1 T s
2

T s3

T s2
T s3

T s2
T s3

T s5 T s4

T

s

T s4
T s5

T s4

T s5

Fig. 5. Crossover operation between two individuals

4.4

Experimental Results

We have performed the genetic algorithm in section 4.3 with the following parameters:
– Population size: W = 200
– Mutation probability: ρ1 = 0.4

Parallelization Scheme for an Approximate Solution to Time Constraint Problems

183

– Crossover probability: ρ2 = 0.2
– Stop criteria: after 100000 generations or when the best individual does not
improve after 100 iterations.
The input data is a sequential diagram randomly generated. We follow the
two experiments: first we generate a simple data set in which each problem Pi
spans exactly one step. In this case, the optimal solution to (6) can be calculated
using (7). The results are then compared with the results obtained by using the
genetic algorithm. The second experiment deals with the performance of the
algorithm on more complex data set where the number of steps is smaller than
the number of sub-problems. In both cases, the complexities of sub-problems
are randomly generated, the time constraint T = 1 and the parallel effective
coefficient α = 1. For the latter case, the sequential diagram is also randomly
generated such that the number of sub-problems is twice the number of steps
(average of two sub-problems to be solved in each step).
Table 1. Genetic Algorithm on Simple Data Set
Size:
200
400
600
800 1000 1200 1400
Number of epoch:
23199 35396 46798 52197 58899 60200 81297
MaxPower(GA):
964.0 2040.6 2958.9 4054.7 5044.8 6032.4 7135.2
MaxPower(Optimal): 963.9 2040.1 2957.8 4052.4 5041.6 6026.4 7130.2

The results for the simple data set is shown in Table 1 where the number
of sub-problems is also the number of steps. M axP ower is the return value
of function F in (8). The genetic algorithm gives good results in compared to
the optimal solution. In all cases, the difference is not considerable (about 0.1%
bigger).
Table 2. Genetic Algorithm on Complex Data Set
Sub problems:
200
400
800 1200
1600
2000
2400
2800
Number of epoch:
2698 3896 8100 5290
8197 12500 13097 14997
Computation time: 0m07s 0m28s 2m59s 4m02s 10m33s 24m09s 36m17s 56m01s

Table 2 shows the convergent speed of the genetic algorithm in the second
experiment. All tests are done on a Linux/Pentium 4, 1.7GHz machine. The
convergence speed depends not only on the number of sub-problems but also
on the connectivity between subproblems in the sequential diagram. It is quite
fast when the number of sub-problems in the sequential diagram is small (7
seconds for 200 sub-problems/the min-max problem of 100 steps or variables).
This increases up to about 56 minutes for a decomposition of 2800 sub-problems
(the min-max problem with 1400 variables).

5

Conclusion

Solving time constraint problems is a hard question. It is event more difficult
to find a feasible solution in heterogeneous computing environments where resources in the pool can change time by time.
We have presented in this paper a parallelization scheme. The scheme consists
of a decomposition tree defining possible decompositions of a problem into subproblems and the decomposition dependency graph showing the relative order

184

T.-A. Nguyen and P. Kuonen

of execution of sub-problems. The scheme provides a way for programmers to
express their time constraint applications.
An algorithm based on the decomposition tree was constructed, showing how
the time constraint problem to be solved. It can be designed as a supporting
framework or can be integrated into programming languages. In many parallel
programming languages such as ParCel[9], the decomposition tree can also be
automatically generated by extracting the different levels of granularity from
the same design. In other parallel object-oriented programming languages such
as ParoC++[6], the construction of decomposition tree is simply by aggregating
and replacing objects with different levels of data processing or functionality.
The algorithm also raises a min-max problem associating with the decomposition dependency graph for evaluating time constraints of sub-problems within
each decomposition step. We have solved this min-max problem by presenting
an approximate approach of genetic algorithm. Some experiment results were
discussed, showing that genetic algorithm can deal with large scale of decomposition up to thousands of sub-problems. For a given volume of input, the min-max
problem can be solved in advance before the real execution of the problem. The
solution to the min-max problem can even be derived from the history results
for an arbitrary input volume if the ratio of complexities of sub-problems in the
decomposition tree is unchanged.
Other research is on going to develop a parallel object-oriented framework
using the presented parallelization scheme to solve time constraint problems.

References
1. I. Foster, C. Kesselman, S. Tuecke. The Anatomy of the Grid: Enabling Scalable
Virtual Organizations. International J. Supercomputer Applications, 15(3), 2001.
2. David Barkai. Peer-to-Peer Computing: Technologies for Sharing and collaborating
on the Net. Intel Press, 2002.
3. Object Management Group. Real-Time CORBA specification. http://www.omg.org.
4. T. D. Braun, H. J. Siegel and A. A. Maciejewski. Static Mapping Heuristics for Tasks
with Dependencies, Priorities, Deadlines and Multiple Versions in Heterogeneous
Environments. 16th International Parallel and distributed Processing Symposium,
2002.
5. M. Maheswaran, S. Ali, H. J. Siegel, D. Hensgen and R. F. Freund. Dynamic Mapping of a Class of Independent Tasks onto Heterogeneous Computing Systems. Journal of Parallel and distributed Computing Vol. 59(2), p. 107-131, Nov. 1999.
6. T. A. Nguyen, P. Kuonen. A Model of Dynamic Parallel Objects for Metacomputing. The 2002 International Conference on Parallel and Distributed Processing
Techniques and Applications, 2002, Las Vegas, Nevada, USA.
7. P. J¸edrzejowicz, I. Wierzbowska. Scheduling multiple variant programs under hard
real-time constraints. European Journal of Operational Research 127 (2000) 458-465.
8. J. Gunnels, C. Lin, G. Morrow and R. van de Geijn. Analysis of a Class of Parallel
Matrix Multiplication Algorithms. Proc. of the First Merged International Parallel
Processing Symposium and Symposium on Parallel and Distributed Processing,
p.110-116, 1998.
9. J.-P. Cagnard. The Parallel Cellular Programming Model. The 8th euromicro workshop on Parallel and Distributed Processing, 2000.

