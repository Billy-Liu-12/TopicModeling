Adaptive Model Trust Region Methods for
Generalized Eigenvalue Problems
P.-A. Absil1 , C.G. Baker1 , K.A. Gallivan1 , and A. Sameh2
1
School of Computational Science,
Florida State University, Tallahassee,
FL 32306-4120, USA
{absil, cbaker, gallivan}@csit.fsu.edu,
http://www.csit.fsu.edu/{∼absil,∼cbaker,∼gallivan}
2
Department of Computer Sciences,
Purdue University, West Lafayette,
IN 47907-2066, USA

Abstract. Computing a few eigenpairs of large-scale matrices is a signiﬁcant problem in science and engineering applications and a very active area of research. In this paper, two methods that compute extreme
eigenpairs of positive-deﬁnite matrix pencils are combined into a hybrid
scheme that inherits the advantages of both constituents. The hybrid
algorithm is developed and analyzed in the framework of model-based
methods for trace minimization.

1

Introduction

We consider the computation of a few smallest eigenvalues and the corresponding
eigenvectors of the generalized eigenvalue problem
Ax = λBx,

(1)

where A and B are n × n symmetric positive-deﬁnite matrices. Positive deﬁniteness of A and B guarantees that all the eigenvalues are real and positive. This
eigenvalue problem appears in particular in the computation of the lower modes
of vibration of a mechanical structure, assuming that there are no rigid-body
modes and that all the degrees of freedom are mass-supplied: A is the stiﬀness
matrix, B is the mass matrix, x is a mode of vibration and λ is the square of
the circular frequency associated with the mode x.
Inverse iteration (INVIT) and Rayleigh quotient iteration (RQI) are the conceptually simplest methods for the generalized eigenvalue problem [1–§15-9]. Interestingly, they have complementary properties: unshifted INVIT converges to
the smallest (or leftmost) eigenpair for almost all initial conditions but with linear local convergence only; RQI has cubic local convergence but it can converge
to diﬀerent eigenpairs depending on the initial condition.
This work was supported by NSF Grants ACI0324944 and CCR9912415, and by the
School of Computational Science of Florida State University.
V.S. Sunderam et al. (Eds.): ICCS 2005, LNCS 3514, pp. 33–41, 2005.
c Springer-Verlag Berlin Heidelberg 2005

34

P.-A. Absil et al.

It is therefore quite natural to try to combine these two methods and obtain
a hybrid method that enjoys strong global convergence and fast local convergence; see in particular Szyld [2] for the problem of ﬁnding an eigenvalue in a
given interval. For the computation of the leftmost eigenpair, such a hybrid
method would use INVIT until reaching the basin of attraction of the leftmost eigenvector under RQI; then the method would switch to RQI in order
to exploit its superlinear convergence. However, to our knowledge, a practical and reliable switching criterion between the two methods has yet to be
found that guarantees global convergence of the hybrid method: if the switch
is made too early, RQI may not converge to the leftmost eigenspace. A second
drawback of exact INVIT and RQI is their possibly high computational cost,
since large-scale linear systems with system matrix (A − σB) have to be solved
exactly.
In this paper, we propose a remedy to these two diﬃculties. In Phase I,
in order to reduce the computational cost, we replace INVIT with the Basic
Tracemin algorithm of Sameh et al. [3, 4]. Basic Tracemin proceeds by successive unconstrained approximate minimization of inexact quadratic local models
of a generalized Rayleigh quotient cost function, using the stiﬀness matrix A
as the model Hessian. This method is closely related to INVIT: if the model
minimizations are carried out exactly (which happens in particular when an
exact factorization of A is used for preconditioning), then Basic Tracemin is
mathematically equivalent to (block) INVIT.
Extending an observation made by Edelman et al. [5–§4.4], we point out that
the stiﬀness matrix used as the model Hessian is quite diﬀerent from the true
Hessian of the generalized Rayleigh quotient—this is why the model is called “inexact”. However, we emphasize that the choice of an inexact Hessian in Phase I
does not conﬂict with the ﬁndings in [5]: using the exact Hessian is important
only when the iteration gets close to the solution, in order to achieve superlinear
convergence. Therefore, using an inexact Hessian in Phase I is not necessarily
a liability. On the contrary, the stiﬀness matrix as the model Hessian oﬀers a
useful property: as shown in [3, 4], any decrease in the inexact model induces
a decrease in the cost function (i.e., the Rayleigh quotient). Therefore, in the
presence of an inexact preconditioner, Basic Tracemin can be thought of as an
inexact INVIT, that reduces computational cost per step while preserving the
global convergence property. Due to its link with INVIT, it quickly purges the
eigenvectors whose eigenvalues are well separated from the leftmost eigenvalues.
This is particularly true when a good preconditioner is available, as is often the
case in the very sparse problems encountered in structural mechanics. Consequently, the Basic Tracemin iteration is eﬃcient when the iterates are still far
away from the solution.
On the other hand, close to the solution, Basic Tracemin suﬀers from the linear rate of convergence (due to the use of an inexact Hessian), especially when
the leftmost eigenvalues are not well separated from the immediately higher ones.
This is why a superlinear method, like RQI, is required in Phase II. However, we
want to remedy both drawbacks of convergence failure and high computational

Adaptive Model Trust Region Methods

35

cost mentioned above. This motivates the use of the recently proposed Riemannian trust-region (RTR) algorithm [6, 7, 8]. Unlike Basic Tracemin, this method
uses the true Hessian as the model Hessian (for superlinear convergence), along
with a trust-region safeguard that prevents convergence to non-leftmost eigenpairs that could otherwise occur if switching takes place too early. Moreover,
the computational cost is reduced by using a truncated conjugate-gradient algorithm to solve the trust-region problems inexactly while preserving the global
and locally superlinear convergence.
However, in spite of its convergence properties (which make it an excellent choice for Phase II), there is a reason not to use the trust-region method
in Phase I: far from the solution, the trust-region conﬁnement often makes
it diﬃcult to exploit the full power of a good preconditioner—eﬃcient preconditioned steps are likely to be rejected for falling outside of the trust region. A possible remedy, which we are currently investigating, is to relax the
trust-region requirement so as to accept eﬃcient preconditioned steps. Another
remedy, which we study in this paper, is to use a Basic Tracemin / RTR
hybrid.
In summary, we use Basic Tracemin in Phase I (far away from the solution) and the RTR algorithm with exact Hessian in Phase II (close to the
solution). In this work, we develop and analyze the hybrid scheme by unifying the two constituents and their combination using the framework of adaptive model-based algorithms for the minimization of the generalized Rayleigh
quotient.

2

Model-Based Scheme for Trace Minimization

We want to compute the p leftmost eigenpairs of the generalized eigenproblem (1), where A and B are positive deﬁnite n × n matrices. We denote the
eigenpairs by (λ1 , v1 ), . . . , (λn , vn ) with 0 < λ1 ≤ . . . ≤ λn and take v1 , . . . , vn
B-orthonormal; see, e.g., [9] for details. The methods presented here aim at
computing the leftmost p-dimensional eigenspace V of (A, B), namely, V =
span(v1 , . . . , vp ). To ensure uniqueness of V, we assume that λp < λp+1 . When
p is small, which is inherent in most applications, it is computationally inexpensive to recover the eigenvectors v1 , . . . , vp from V by solving a reduced-order
generalized eigenvalue problem.
It is well known (see for example [4]) that the leftmost eigenspace V of (A, B)
is the column space of any minimizer of the Rayleigh cost function
→ R : Y → trace((Y T BY )−1 (Y T AY )),
f : Rn×p
∗

(2)

where Rn×p
denotes the set of full-rank n × p matrices. It is readily checked that
∗
the right-hand side only depends on colsp(Y ). Therefore, f induces a well-deﬁned
real-valued function on the set of p-dimensional subspaces of Rn .
The proposed methods iteratively compute the minimizer of f by (approximately) minimizing successive models of f . The minimization of the models

36

P.-A. Absil et al.

themselves is done via an iterative process, which is referred to as inner iteration, to distinguish it with the principal, outer iteration. We present here the
process in a way that does not require a background in diﬀerential geometry; we
refer to [7] for the mathematical foundations of the technique.
Let Y , a full-rank n × p matrix, be the current iterate. The task of the inner
iteration is to produce a correction S of Y such that f (Y + S) < f (Y ). A diﬃculty is that corrections of Y that do not modify its column space do not aﬀect
the value of the cost function. This situation leads to unpleasant degeneracy if it
is not addressed. Therefore, we require S to satisfy some complementarity condition with respect to the space VY := {Y M : M p × p invertible}. Here, in order
to simplify later developments, we impose complementarity via B-orthogonality,
namely S ∈ HY where
HY := {Z ∈ Rn×p : Y T BZ = 0}.
Consequently, the inner iteration aims at minimizing the function
fY (S) := trace

(Y + S)T B(Y + S)

−1

(Y + S)T A(Y + S)

,

S ∈ HY .
(3)

A Taylor expansion of fY around S = 0 yields the “exact” quadratic model
(S) = trace((Y T BY )−1 (Y T AY )) + trace (Y T BY )−1 S T 2AY
mexact
Y
1
+ trace (Y T BY )−1 S T 2 AS − BS(Y T BY )−1 Y T AY , S ∈ HY .
2

(4)

Throughout this paper, we let P := I − BY (Y T B 2 Y )−1 Y T B denote the orthogonal projector onto HY , where Y is the current iterate. From (4), and using the
inner product
Z1 , Z2 := trace (Y T BY )−1 Z1T Z2 ,

Z1 , Z2 ∈ HY ,

(5)

we identify 2P AY to be the gradient of fY at S = 0 and the operator S →
2P AS − BS(Y T BY )−1 Y T AY to be the Hessian of fY at S = 0; see [7] for
details. In the sequel, we will use the more general form
mY (S) = trace((Y T BY )−1 (Y T AY )) + trace (Y T BY )−1 S T 2AY
1
+ trace (Y T BY )−1 S T HY [S] , S ∈ HY
2

(6)

to allow for the use of inexact quadratic expansions.
The proposed general algorithm is a model trust-region scheme deﬁned as
follows.
Algorithm 1 (outer iteration)
Data: symmetric positive-definite n × n matrices A and B.
¯ and ρ ∈ [0, 1 ).
Parameters: ∆¯ > 0, ∆0 ∈ (0, ∆],
4

Adaptive Model Trust Region Methods

37

Input: initial iterate Y0 (full-rank n × p matrix).
Output: sequence of iterates {Yk }.
for k = 0, 1, 2, . . . until an outer stopping criterion is satisfied:
– Using Algorithm 2, obtain Sk that (approximately) solves the trust-region
subproblem
min mYk (S)

S∈HYk

s.t. S

2
M

:= trace (YkT BYk )−1 S T M S ≤ ∆2k ,

where m is defined in (6) and M is a preconditioner.
– Evaluate
fYk (0) − fYk (Sk )
ρk :=
mYk (0) − mYk (Sk )

(7)

(8)

where f is defined in (3).
– Update the trust-region radius:
if ρk < 14
∆k+1 = 14 ∆k
else if ρk > 34 and Sk = ∆k
¯
∆k+1 = min(2∆k , ∆)
else
∆k+1 = ∆k ;
– Update the iterate:
if ρk > ρ ,
Yk+1 = orth(Yk + Sk ), where orth denotes an orthonormalization process
which prevents loss of rank;
else
Yk+1 = Yk ;
end (for).
The inner iteration (Algorithm 2) employed to generate Sk is a preconditioned truncated conjugate gradient method, directly inspired from the work of
Steihaug [10] and Toint [11]. The notation (P M P )† R below denotes the solution
˜ of the system P M P R
˜ = R, P R
˜ = R,
˜ P R = R, which is given by the Olsen
R
˜ = M −1 R − M −1 BY (Y T BM −1 BY )−1 Y T BM −1 R.
formula R
Algorithm 2 (inner iteration)
˜ 0 = (P M P )† R0 ,
Set S0 = 0, R0 = P AYk = AYk − BYk (YkT B 2 Yk )−1 YkT BAYk , R
δ0 = −R0 ;
for j = 0, 1, 2, . . . until an inner stopping criterion is satisfied, perform the
following operations, where , denotes the inner product (5) and HYk denotes
model Hessian in (6).
if δj , HYk δj ≤ 0
Compute τ such that S = Sj + τ δj minimizes m(S) in (6)
and satisfies S M = ∆;
return S;

38

P.-A. Absil et al.

˜ j / δj , HY δj ; Set Sj+1 = Sj + αj δj ;
Set αj = Rj , R
k
if Sj+1 M ≥ ∆
Compute τ ≥ 0 such that S = Sj + τ δj satisfies S M = ∆;
return S;
˜ j+1 = (P M P )† Rj+1 ;
Set Rj+1 = Rj + αHYk δj ; Set R
˜ j+1 / Rj , R
˜ j ; Set δj+1 = −R
˜ j+1 + βj+1 δj ;
Set βj+1 = Rj+1 , R
end (for).
We refer to [7] for a convergence analysis of Algorithm 1.
The two-phase scheme outlined in Section 1 can now be formalized.
Algorithm 3 Phase I: Iterate Basic Tracemin—or, formally, Algorithm 1 with
H[S] := P AP S, ∆0 := +∞ and ρ := 0—until some switching criterion is satisfied.
Phase II: Continue using Algorithm 1 with H[S] :=P (AS−BS(Y TBY )−1 Y TAY ),
some initial ∆ and some ρ ∈ (0, 14 ).
In the algorithms above, stopping and switching criteria, as well as the choices
of some parameters and preconditioner, were left unspeciﬁed: depending on
the information available about the structure of the problem and on the accuracy/speed requirements, various choices may be appropriate. We refer to the
next section for examples of speciﬁc choices.

3

Numerical Experiments

In order to illustrate the practical relevance of the scheme proposed in Algorithm 3, we conducted experiments on the Calgary Olympic Saddledome arena
matrices (n = 3562) available on Matrix Market: A = BCSSTK24 and B =
BCSSTM24. The task was to compute the p = 5 leftmost eigenpairs of (A, B),
which correspond to the lower modes of vibration.
With a view to achieving superlinear convergence when the exact model (4)
is utilized, we used an inner stopping criterion of the form
Rj ≤ R0 min( R0

θ

, κ),

(9)

for some θ > 0 and κ > 0. We chose θ = 1 (striving for quadratic local convergence) and κ = .5. Several other choices are possible, notably based on the
discussion in [3] in the case of Phase I.
In the ﬁrst step of Phase II, we chose ∆ := S− M , where S− is the last S
computed in Phase I, and we chose ρ = .1. The outer stopping criterion was a
threshold on the norm of P AYk . Transition between Phase I and Phase II was
forced after various prescribed numbers of outer iterations, to illustrate the eﬀect
of diﬀerent switching points on the behaviour of the algorithm. An initial iterate
Y0 was selected from a normal distribution. In all experiments, the preconditioner
was kept constant throughout the iteration.

Adaptive Model Trust Region Methods

39

In the ﬁrst set of experiments (Figure 1–left), the preconditioner M was
chosen as an incomplete Cholesky factorization of A with relative drop tolerance set to 10−6 . Basic Tracemin converges linearly and (relatively) slowly. RTR
(curve ‘0’) is eventually faster that Basic Tracemin but it is initially slower,
due to the trust-region constraint. Curves ‘5’ and ‘10’ show that there is a
rather large “sweet-spot” for eﬃcient switching from Basic Tracemin to RTR
such that the hybrid method performs better than both of its pure components. If switching is done too late (curve ‘20’), then superlinear convergence
is still observed and the problem is solved, but with less eﬃciency than the
properly switched versions. Preliminary results suggest that the evolution of
the trace function (2) gives valuable information on when switching should
take place. Note that all that is at stake in the choice of the switching criterion between Basic Tracemin and RTR is eﬃciency, and neither success nor
accuracy.
In a second set of experiments (Figure 1–right), we initially applied a permutation on A and B as returned by the the Matlab function symamd(A). The
preconditioner was then deﬁned to be the exact Cholesky factorization of A,
which is very sparse due to the approximate minimum degree permutation. Consequently, all algorithms converged (much) faster. Note that Phase I is mathematically equivalent to INVIT in this case. We observe a similar sweet-spot for
the switching and the superiority of the global superlinear convergence of the
trust-region-based method.
(BCSSTK24,BCSSTM24)

2

(BCSSTK24,BCSSTM24)

2

10

10

0

0

10

0

10

0
−2

5

dist to solution

dist to solution

−2

10

10

−4

20

10

−6

10

1

5
−4

10

10

30

−6

10

−8

−8

10

10

−10

10

10

−10

0.5

1

1.5

2
2.5
number ops

3

3.5

4
10

x 10

10

0

2

4

6
8
number ops

10

12

14
8

x 10

Fig. 1. Left: Experiments on Algorithm 3 with an incomplete Cholesky factorization of
A as preconditioner. The distance to the solution is measured as the largest principal
angle between the column space of Y and the leftmost p-dimensional eigenspace. Since
the numerical cost of inner iterations diﬀers in both phases, the distance is plotted
versus an estimation of the number of operations. Circles and stars correspond to
outer iterations of Basic Tracemin and RTR, respectively. The numbers on the curves
indicate after how many steps of Basic Tracemin switching occurred to RTR. Right:
Same, with exact preconditioner after approximate minimum degree permutation. Note
that the formation of the preconditioner does not appear in the operation count

40

P.-A. Absil et al.

Finally, we conducted experiments where RTR was replaced by a block RQI.1
Because RQI does not have global convergence to the leftmost eigenspace, convergence to a non-leftmost eigenspace may occur if switching is done too early.
This was observed in the experiments. To our knowledge, there is no switching
criterion that guarantees convergence to the leftmost eigenspace when RQI is
used. This is a major reason for using RTR instead of RQI in Phase II (another reason is to reduce the computational cost with the truncated CG inner
iteration).

4

Conclusion

We have shown that an appropriate combination of the Basic Tracemin algorithm of [3, 4] and the Riemannian trust-region algorithm of [8] yields an eﬃcient
method for high precision computation of the smallest eigenpairs of positivedeﬁnite generalized eigenproblems. In future work, we will further investigate
the choice of the inner stopping criterion and the switching criterion between
the two algorithms. For the latter, there is evidence that the decrease of the
trace function (2) provides an useful guideline.

References
1. Parlett, B.N.: The Symmetric Eigenvalue Problem. Prentice-Hall, Inc., Englewood
Cliﬀs, N.J. 07632 (1980) republished by SIAM, Philadelphia, 1998.
2. Szyld, D.B.: Criteria for combining inverse and Rayleigh quotient iteration. SIAM
J. Numer. Anal. 25 (1988) 1369–1375
3. Sameh, A.H., Wisniewski, J.A.: A trace minimization algorithm for the generalized
eigenvalue problem. SIAM J. Numer. Anal. 19 (1982) 1243–1259
4. Sameh, A., Tong, Z.: The trace minimization method for the symmetric generalized
eigenvalue problem. J. Comput. Appl. Math. 123 (2000) 155–175
5. Edelman, A., Arias, T.A., Smith, S.T.: The geometry of algorithms with orthogonality constraints. SIAM J. Matrix Anal. Appl. 20 (1998) 303–353
6. Absil, P.A., Baker, C.G., Gallivan, K.A.: Trust-region methods on Riemannian
manifolds with applications in numerical linear algebra. In: Proceedings of the
16th International Symposium on Mathematical Theory of Networks and Systems
(MTNS2004), Leuven, Belgium, 5–9 July 2004. (2004)
7. Absil, P.A., Baker, C.G., Gallivan, K.A.: Trust-region methods on Riemannian
manifolds. Technical Report FSU-CSIT-04-13, School of Computational Science,
Florida State University (2004) http://www.csit.fsu.edu/∼absil/Publi/RTR.htm.
8. Absil, P.A., Baker, C.G., Gallivan, K.A.: A truncated-CG style method for symmetric generalized eigenvalue problems. submitted (2004)
9. Stewart, G.W.: Matrix algorithms, Vol II: Eigensystems. Society for Industrial
and Applied Mathematics, Philadelphia (2001)
1

We used the NG algorithm mentioned in [12, 13], which reduces to RQI in the p =
1 case. This algorithm reﬁnes estimates of invariant subspaces, without favoring
convergence to the leftmost one.

Adaptive Model Trust Region Methods

41

10. Steihaug, T.: The conjugate gradient method and trust regions in large scale
optimization. SIAM J. Numer. Anal. 20 (1983) 626–637
11. Toint, P.L.: Towards an eﬃcient sparsity exploiting Newton method for minimization. In Duﬀ, I.S., ed.: Sparse Matrices and Their Uses. Academic Press, London
(1981) 57–88
12. Lundstr¨
om, E., Eld´en, L.: Adaptive eigenvalue computations using Newton’s
method on the Grassmann manifold. SIAM J. Matrix Anal. Appl. 23 (2002)
819–839
13. Absil, P.A., Sepulchre, R., Van Dooren, P., Mahony, R.: Cubically convergent
iterations for invariant subspace computation. SIAM J. Matrix. Anal. Appl. 26
(2004) 70–96

