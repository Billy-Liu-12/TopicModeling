TH-VSS: An Asymmetric Storage Virtualization
System for the SAN Environment
Da Xiao, Jiwu Shu, Wei Xue, and Weimin Zheng
Department of Computer Science and Technology, Tsinghua University,
100084 Beijing, China
xiaoda99@mails.tsinghua.edu.cn

Abstract. Storage virtualization is a key technique to exploit the potential of SANs. This paper describes the design and implementation of
a storage virtualization system for the SAN environment. This system
has asymmetric architecture, and virtualization operations are done by
a metadata server allowing management to be accomplished at a single
point. The system has better scalability compared to symmetric systems
and can support heterogeneous platforms of hosts. Agent software was
implemented in the volume management layer of hosts so that any standard HBA card can be used. The metadata server manages storage resources automatically and conﬁguration of storage pools can be changed
dynamically. The metadata server is also used for system monitoring,
which is the basis of dynamic storage resource management and automatic failure recovery. Test results showed that the overhead introduced
by our virtualization layer is negligible, and the performance of storage
system was enhanced eﬀectively by the striping strategy in organizing
storage pools. The number of seeks per second to a logical volume allocated from a pool of four disks was increased by 55.2% compared to a
plain FC disk.

1

Introduction

The introduction of storage area networks (SANs) can signiﬁcantly improve the
reliability, availability, and performance of storage systems. However, SANs must
be managed eﬀectively so that their potential can be fully exploited. Storage
virtualization is often seen as the key technology in SAN management. Storage
virtualization separates physical storage from the server’s operating system and
provides storage users with uniﬁed storage pools and logical volumes. It can
provide users with much larger storage space than a single physical disk and
a much better utilization of disk capacity. Furthermore, virtualization oﬀers a
new level of ﬂexibility. Storage systems can be added to or removed from storage
pools without downtime, thus enabling fast adaptation to new requirements.
The ﬁrst approach to storage virtualization in a cluster environment is hostbased. Two typical examples of this approach are CLVM[3] and the EVMS cluster[5]. CLVM is an extension of Linux LVM in a single system environment to
add cluster support. The EVMS cluster is a framework proposed by IBM which
V.S. Sunderam et al. (Eds.): ICCS 2005, LNCS 3516, pp. 399–406, 2005.
c Springer-Verlag Berlin Heidelberg 2005

400

D. Xiao et al.

also supports virtualization in cluster environments. Both of these approaches
adopt a host-based symmetric architecture. Each node in the cluster has the
right to perform virtualization management tasks. The consistency of metadata
is maintained by communications between nodes. This puts a heavy burden on
hosts and reduces the scalability of the system. Besides, heterogeneous platforms
of hosts are not supported by systems with this symmetric architecture.
Another approach is network-based in-band virtualization. This approach
calls for an appliance to be installed between the hosts and the storage devices, which redirects all I/Os between hosts and storage devices. Representatives of this approach are DataCore Software’s SANsymphony[6] and HP’s
StorageApps[4]. The advantage of this approach is that agent software need not
be installed on hosts, so transparency to hosts is achieved. Its main drawback is
that the appliance may become a SAN bottleneck, thus limiting the SAN performance and scalability and signiﬁcantly complicating the design of large-scale
highly available conﬁgurations.
A third approach is network-based out-of-band virtualization. In this approach, the handling of metadata is separated from the data path and is done
by a dedicated appliance. It enables direct data transfer between hosts and storage subsystems. StoreAge’s SVM[7] adopts such an architecture.
Moreover, the main consideration in all the approaches mentioned above is
storage capacity when grouping physical disks into storage pools. These approaches ignore other properties of virtual storage such as bandwidth, latency,
and reliability.
This paper describes the design and implementation of a storage virtualization system for SAN environments called the Tsinghua virtualization storage
system (TH-VSS). In this system, an asymmetric architecture is adopted. Metadata management tasks are done by a metadata server so that single-point management is achieved. The system has better scalability compared to symmetric
virtualization architecture and can support heterogeneous platforms in hosts.
Agent software is implemented in the volume management layer of hosts, which
results in improved ﬂexibility. Any standard HBA card can be used. Storage
resources are managed automatically by the metadata server, and the conﬁguration of storage pools can be changed dynamically. The metadata server is
also used for system monitoring, which is the basis of dynamic storage resource
management and automatic failure recovery.

2
2.1

TH-VSS Design
Architecture of TH-VSS

TH-VSS consists of three parts: the virtualization agents on the hosts, a meta
server, and storage devices. The hosts and meta server are attached to the SAN
via a FC HBA on the motherboard. Storage devices are also attached to the
SAN via their ﬁbre channel interfaces. The meta server and agent are connected
to each other through Ethernet. The TH-VSS was designed for a heterogeneous

TH-VSS: An Asymmetric Storage Virtualization System

401

host

application
meta server

file system / Database
mapping
module

host
interface

host
interface

management
module

block device
driver

TCP/IP

TCP/IP

block device
driver

FC driver

Network
device driver

Network
device driver

FC driver

Fibre Channel Storage Area Network

JBOD

RAID

Tape

Storage
Server

Fig. 1. Overall architecture of the TH-VSS

SAN environment. Hosts may run diﬀerent OSs, and storage devices include
JBOD, RAID subsystems, tape drives, and storage servers, as well as others.
The metadata management module on the meta server performs the metadata
management tasks, and logical to physical address mapping of I/O requests are
handled by the mapping layer of the agent on the host. The software architecture
of the system is shown in Fig. 1 (gray boxes are software modules implemented
in the TH-VSS).
2.2

Virtualization Process

The system works as shown in Fig. 2. On startup, the system monitor on the meta
server scans all physical storage devices attached to the SAN. For each device,
the system monitor writes a label and a UUID on the device to create a physical
volume on it. Then the system monitor groups the storage devices into diﬀerent
storage pools according their detected properties. Next, the administrator issues
a command to create a logical volume through the administration interface.
The administration interface tells the metadata manager to allocate space for
the logical volume from the storage pool and create it. Then the administrator
issues another command to assign the volume to the Solaris server. On receiving
such a command, the meta server sends the UUIDs of the physical volumes in
the storage pool and the mapping table of the volume to the agent on the Solaris
server through host interfaces. The agent locates the correct disks according to
the UUIDs and creates the logical volume in a kernel for further use. Then it
sends a response to the meta server to tell it that the volume has successfully
been created. Finally, applications on the Solaris server can access the logical
volume directly without consulting the meta server.

402

D. Xiao et al.
Ethernet

Linux
server

Windows
server

Solaris
server

5
7

8

2
4

6

SAN

3

Logical volume

Meta server

1
Storage pool

Physical disks

Fig. 2. Virtualization process

3
3.1

Key Techniques in TH-VSS Implementation
Storage Resource Management

The lowest level in the TH-VSS storage hierarchy is the physical volume. Multiple
physical volumes are merged into a storage pool, from which logical volumes can
be allocated. The TH-VSS supports concatenation (linear), stripe (RAID-0) and
mirror (RAID-1) schemes.
In commercial data storage environments, various kinds of storage systems
can be attached to the SAN, including the JBOD system, the RAID subsystem,
tape drives and storage servers. Diﬀerent kinds of storage systems have diﬀerent
properties in bandwidth, latency, and reliability. When the meta server starts
up, it scans all the storage devices attached to the SAN and writes a label and
a UUID at the head of each detected device to mark it as a physical volume.
Then it puts the physical volume into the appropriate storage pool according
to its property. The composition of a storage pool can be changed dynamically.
When a new storage device is added to the SAN, the system monitor on the
meta server will detect the addition of the new device, query its type, create a
physical volume on it and put it into the appropriate storage pool.
When a physical volume is added to a striped storage pool, the data of the
logical volumes must be re-striped among physical volumes to improve performance. A mirroring mechanism is used to ensure that the data of the logical
volumes is not corrupted during the re-striping process. Fig. 3 shows the process
of re-striping. Data movement can be interrupted at any time due to errors or
other reasons. LV1 can continue to function normally because it contains the
newest copy of data.

TH-VSS: An Asymmetric Storage Virtualization System
1

Write Chunk 3 Command

Data chunk of LV1

Data chunk of LV1'

1'

Write Chunk 3 Command

1

2

1

2

3

4

3

4

5

6

5

6

1'

2'

PV 1

PV 2

(1) before re-striping

403

Free space

Write Chunk 3 Command

3'

1

2

4'

5'

6'

4

5

PV 1

PV 2

PV 3

PV 1

PV 2

(2) during re-striping
LV and LV' are mirrored

PV

(3) after re-striping

Fig. 3. Process of re-striping

3.2

Online Change of Mapping Table

In order to meet the requirements for uninterrupted service, some management
tasks need to change mapping tables of logical volumes online. In SAN environments, where storage is shared among multiple hosts, the access of hosts to
logical volumes must be controlled so that the data of logical volumes is consistent. A locking mechanism is used to ensure data consistency when the mapping
table of logical volume is changed. A meta server may send lock and unlock
requests of a particular logical volume to the agent. Upon the agent’s receiving
a lock request, any I/O request that has already been mapped by the mapping
table of a logical volume but has not yet completed will be ﬂushed. Any subsequent I/O request to that logical volume will be postponed for as long as the
volume is locked. On receiving an unlock request, any postponed I/O request
will be mapped by the new mapping table and gets re-queued for processing.
The process by which mapping tables are changed online is as follows. First,
the meta server identiﬁes the hosts to which this logical volume has been assigned, and sends a LOCK LV request to agents on these hosts. On receiving
the request, the agent performs the locking operation and then sends a LOCK LV
response to the meta server. After receiving the responses from the agents to the
LOCK LV request, the metadata manager generates a new mapping table. Then
it sends a RELOAD TABLE request to the agents. The agents replace their old
mapping tables of the logical volume with the new ones and send a response to
the meta server. After all the old mapping tables are replaced with the new ones,
the meta server sends an UNLOCK LV request to the agents. The agents map
the postponed I/O requests with the new mapping table for processing and send
an UNLOCK LV response. Finally, the metadata manager of the meta server
writes the updated metadata back to the heads of the physical volumes.

404

D. Xiao et al.

In this process, data inconsistency due to hosts’ access to a logical volume
with diﬀerent mapping tables is avoided by having the meta server send an
UNLOCK LV request after all the old mapping tables have been replaced with
new ones successfully.

4

Experimental Results

In this section we will present the experimental results of our virtualization
system. The test system consisted of two Linux servers, two Windows servers,
a meta server and an FC disk array, all of which were connected to a 2 Gigabit
FC switch. Their conﬁgurations are shown in Table 1.
Table 1. Test conﬁguration
Machine Linux server

Windows server

CPU
Memory
OS
FC HBA
FC Disk

Intel Itanium2 1GHz x 2 Intel Xeon 2.4GHz x 2
1GB
1GB
Windows Server 2003
Linux(kernel: 2.4.26)
Emulex LP982(2Gb/s) Emulex LP9802

4.1

Intel Xeon 2.4GHz x 2
1GB
Linux(kernel: 2.4.26)
Emulex LP982(2Gb/s)
Seagate ST3146807FC x 5

Meta server

Overhead of Virtualization Layer

We used the Intel Company’s IOMeter test program to evaluate the overhead
introduced by the virtualization layer. First we tested the performance of a plain
FC disk. Then we created a linear storage group on the disk and allocated a
logical volume from it. To derive the overhead, we compared the performance of
the VSS volume to the performance of the physical disk. The access pattern was
sequential reading in 4-KB blocks. The comparison of average response times for
diﬀerent block sizes is shown in Fig. 4.
We can see from the results that the impact of the virtualization layer on the
I/O performance was negligible in respect to bandwidth and average response
time.
4.2

Impact of Stripe Mapping Strategy

In order to evaluate the performance improvement when using striped storage
pools, ﬁrst we created a striped pool with one disk and allocated a logical volume
LV1 from it. We tested the performance of LV1 using Bonnie Benchmark. We
created a ﬁle system on LV1 and allowed random access to ﬁxed size ﬁles. We
measured the number of random seeks per second on the LV1. Then we added
another disk to the pool and repeated the test. The process was repeated until

TH-VSS: An Asymmetric Storage Virtualization System

405

Fig. 4. Overhead of TH-VSS. First ﬁgure shows the comparison of bandwidth. Second
ﬁgure shows the comparison of average response time

Fig. 5. Impact of stripe mapping strategy on performance

four disks had been added to the pool. The results are shown in Fig. 5, including
a comparison of the results with the case of a plain disk labeled with 0.
The ﬁgure shows that the striped logical volume can provide better I/O
capacity than that of a single disk, and the more disks in the striped storage
pool, the better the performance of the LV1. When the LV1 is striped over 4
disks, the number of seeks per second is increased by 55.2%.

5

Conclusion

In this study, we designed and implemented a storage virtualization system for
the SAN environment. An asymmetric architecture was adopted to achieve a

406

D. Xiao et al.

single point of management, and metadata consistency was ensured in the SAN
environment where storage is shared by multiple hosts. The new system’s meta
server implements the function of dynamic management of storage resources
and system monitoring. Storage pools with diﬀerent properties can be provided.
The test results showed that the addition of our virtualization layer introduces
little overhead to the system, and the striped storage pool can improve I/O
performance eﬀectively.

Acknowledgements
The work described in this paper was supported by the National Natural Science
Foundation of China (No. 60473101) and the National High-Tech Research and
Development Plan of China (No. 2004AA111120).

References
1. David, T., Heinz, M.: Volume Managers in Linux. In Proceedings of the FREENIX
Track:2001 USENIX Annual Technical Conference, Boston, Massachusetts, USA
(2001)
2. Sistina Software, Inc.: Logical Volume Manager. http://www.sistina.com
3. Heinz, M.: Linux Cluster Logical Volume Manager. In Proceedings of the 11th
Linux Kongress , Erlangen, Germany (2004)
4. Hewlett-Packard Company: HP StorageApps sv3000 White Paper. (2002)
5. Ram, P.: EVMS Cluster Design Document. http://evms.sourceforge.net/clustering/
6. SAN Symphony version 5 datasheet. http://www.datacore.com (2002)
7. StoreAge White Paper. High-Performance Storage Virtualization Architecture.
http://www.storeage.com
8. Andr´e, B., Michael, H.: V:Drive - Costs and Beneﬁts of an Out-of-Band Storage
Virtualization System. In Proceedings of the 12th NASA Goddard, 21st IEEE
Conference on Mass Storage Systems and Technologies, College Park, Maryland,
USA (2004)
9. Chang Soo Kim, Gyoung Bae Kim, Bum Joo Shin: Volume Management in SAN
Environment. In Proceedings of 2001 International Conference on Parallel And
Distributed Systems, KyongJu City, Korea (2001)
10. Common Information Model (CIM) Speciﬁcation, v2.2. http://www.dmtf.org/
standards/cim
11. Friedhelm, S.: The SCSI Bus & D E Interface. Addison-Wesley, second edition
(1998)

