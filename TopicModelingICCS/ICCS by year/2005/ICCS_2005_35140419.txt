An All-Reduce Operation in Star Networks
Using All-to-All Broadcast Communication
Pattern
Eunseuk Oh, Hongsik Choi, and David Primeaux
Department of Computer Science, School of Engineering,
Virginia Commonwealth University,
Richmond, VA 23284-3068, USA
{eoh, hchoi, dprimeau}@vcu.edu

Abstract. Most parallel computations require the exchange of data between processing elements. One of important basic communication operations is all-reduce, a variation of the reduction operation. This paper
presents an all-reduce communication operation scheme using all-to-all
broadcast communication pattern. All-to-all broadcast is the operation
in which each processor sends its message to all other processors, and receives messages from all other processors in the system. In this paper, we
develop an eﬃcient all-reduce operation scheme in a star network topology with the single-port communication capability. Communication time
is compared against known broadcasting schemes to verify the eﬃciency
of the suggested scheme.
Keywords: all-reduce, all-to-all broadcast, distributed memory parallel
computing systems, inter-processor communication, star network.

1

Introduction

Due to rapid progress in hardware technology, designing a distributed memory
parallel computing system connecting autonomous microprocessors has become
feasible. In such a system, high-performance microprocessors communicate by
message passing and have no shared memory or global clock. Proper implementation of basic communication operations such as broadcast, reduction, and
all-reduce on various parallel computing systems is key to the design of eﬃcient
parallel algorithms for distributed memory parallel systems.
One-to-all broadcast is an operation that disseminates information across
processors in a multiprocessor system. It is not diﬃcult to see that broadcasting
stands as a foundation for many applications on parallel computing systems. To
list a few applications that use broadcasting, we mention Fast Fourier Transformation (FFT), parallel matrix algorithms, parallel graph algorithms, and distributed algorithms. The all-reduce operation combines the arriving content in
the input buﬀer of each processor using an associative operator (e.g. sum, maximum), and the result appears in the result buﬀer of all processors. All-reduce
V.S. Sunderam et al. (Eds.): ICCS 2005, LNCS 3514, pp. 419–426, 2005.
c Springer-Verlag Berlin Heidelberg 2005

420

E. Oh, H. Choi, and D. Primeaux

Table 1. Comparison of topological properties for parallel computer models of similar
sizes: n-Cube (hypercube), MCT (mesh connected tree), Dn (de Bruijn network), and
HSn,m (Cartesian product of hypercube and star)
Model
S5
7-Cube
M CT4 (3)
D7
HS4,3

Size Degree Diameter Model Size Degree Diameter
120
4
6
S6
720
5
7
128
7
7
10-Cube 1024 10
10
81
12
16
M CT6 (3) 729
18
24
128
4
7
D10
1024
4
10
144
5
7
HS5,3 720
6
9

is typically used for barrier synchronization on a distributed memory parallel
computing system. Also, all-reduce is one of the most important MPI routines;
a case study reveals that more than 40% of the execution time of MPI routines
is spent in all-reduce or reduction operations [9].
In this paper, we study an all-reduce communication operation in star networks by using all-to-all broadcast communication pattern. The all-reduce operation is identical to performing an all-to-one reduction which is followed by
a one-to-all broadcast of the result. Thus, we will compare the communication
time of our scheme with all-reduce operation by using the best known broadcast
scheme proposed in [10]. We ﬁrst design a recursive all-to-all broadcast scheme
that can be utilized to perform the all-reduce operation.
The star model has attracted considerable interest in the parallel processing
research community [1, 2, 3, 7, 8, 10, 11] due to its numerous desirable properties
for building large parallel computer systems. Basic parameters such as size, degree, and diameter for the models whose size is similar to Sn are shown in Table 1.
Broadcasting schemes vary according to the communication capability of the
channels or links. With single-port communication capability, every processor
can simultaneously send and receive at most one message in one communication step. Also, a channel or link may be bidirectional or unidirectional. Cost
measurements for the suggested scheme are provided under the single-port and
bidirectional communication capability. Following the terminology used in [4],
our scheme is “NODUP” in that there is no duplication of information on messages carried during the communication process.
The remainder of this paper is organized as follows. In Section 2 we introduce the communication model and the assumptions made about that model. In
Section 3 we present an all-to-all broadcast scheme. In Section 4 we present an
all-reduce operation based on our all-to-all broadcast scheme. In Section 5, we
provide concluding remarks.

2

Model and Assumptions

The network model considered here is the star graph model. An n-star graph, Sn ,
consists of n! nodes labeled with the n! permutations on the symbols {1, 2, . . . , n}.

An All-Reduce Operation in Star Networks

421

There is a communication link between two processors pi and pj in Sn if and
only if the permutation label of pj can be obtained from the permutation label
of pi by exchanging the symbol in the ﬁrst position in pi with the symbol in
some other position in pi . If the label of pj is obtained from the label of pi by
exchanging the ﬁrst symbol of pi with the symbol in kth position of pi , then pi
and pj are said to be connected along the communication link k.
The pattern of interconnected processors in the star network can be viewed
recursively as follows. S1 is a trivial network with one processor. Suppose that
i
, i = 1 . . . n,
Sn−1 is deﬁned inductively, then Sn is composed of n graphs, Sn−1
i
where each Sn−1 is an isomorphic copy of Sn−1 with symbols {1, . . . , n} − i, and
i
. Connecting
with symbol i appearing as the nth symbol in each processor in Sn−1
j
i
the nodes in diﬀerent copies Sn−1
and Sn−1 is done with respect to the above
j
i
deﬁnition. A node u in Sn−1
is connected to a node v in Sn−1
, i = j when the
label of v can be obtained from the label of u by exchanging the ﬁrst symbol
with the nth symbol.
The communication model for parallel computers varies depending on the
communication hardware and the memory bus bandwidth. Most commercial
systems support the single-port model. In the single-port communication model,
a processor can send a message on only one of its communication links at a time.
The sending and the receiving ports are not necessarily the same. The system
model we consider is as follows; (1) The system is completely connected with
synchronous communication. (2) A processor sends a message to a connected
processor in one communication step. (3) Single-port communication and the
communication links are bidirectional.

3

All-to-All Broadcast

All-to-all broadcast is performed recursively. After performing all-to-all broadi
, i = 1, . . . , n, all-to-all broadcast in Sn is performed. To avoid
cast in each Sn−1
sending a message more than once to the same processor in the network, the private memory of each processor will be divided into two parts: the result buﬀer,
and the outgoing message buﬀer. The partial sum will be stored in the result
buﬀer.
All-to-All broadcast in S4 : The algorithm ﬁrst calls itself recursively to perform broadcast in each of S31 , S32 , S33 , and S34 . The base of the recursion is when
the network is an S2 . The algorithm performs broadcast in S2 by a simple exchange of messages between the two processors. Then each processor in S2 sends
its message along communication link 3, and saves it in its result buﬀer. After
that each processor sends its received message along communication link 2, and
broadcast in S3 is terminated. Now each S3 in S4 performs all-to-all broadcast in
parallel fashion. At this point every processor computes its message by concatenating the message in its outgoing buﬀer with the message in its result buﬀer.
Each processor stores the concatenated message in the result buﬀer, and writes
a copy of the concatenated message over the current content of the outgoing
buﬀer. We call this concatenated message the meta message. Then, every pro-

422

E. Oh, H. Choi, and D. Primeaux

cessor sends its meta message along communication link 4, and saves its message
in its result buﬀer. Once a processor receives the message along communication
link 4, it only needs to broadcast within each S3 . It starts this process by sending its message along communication link 3, then 2, meanwhile storing received
messages in the result buﬀer.
All-to-All broadcast in Sn : In general, suppose inductively that all-to-all
i
, i = 1 . . . n, and let us see how
broadcast has been completed within each Sn−1
this can be extended to all-to-all broadcast in Sn . Since all-to-all broadcast has
i
i
, each processor in Sn−1
has received the messages
been completed in each Sn−1
i
i
share the same
from all other processors in Sn−1 , and hence all processors in Sn−1
n
i
i
information. Denote the meta message in Sn−1 by ∆n−1 . Let ∆n = i=1 ∆in−1 ,
then all-to-all broadcast in Sn is achieved once every processor in Sn holds ∆n .
All-to-all broadcast is performed as follows. In the ﬁrst stage every processor p in
i
, i = 1 . . . n, broadcasts its meta message ∆in−1 along communication link
Sn−1
i
n and saves the meta message in its result buﬀer. After this stage, each Sn−1
contains all messages in ∆n among its processors. Thus, the only thing left to
i
.
be done in the second stage is to propagate the information within each Sn−1
This step needs to be done with some care so that to avoid sending a message
more than once to the same processor. Once p receives the meta message ∆jn−1 ,
i
j = i, along communication link n, it propagates ∆jn−1 across Sn−1
by sending
it along communication links n − 1, n − 2, . . . , 2, respectively. Also, the received
message is stored in the result buﬀer.
Theorem 1. At the termination of all-to-all broadcast, each processor in Sn
holds the meta message ∆n .

4

All-Reduce Operation

We perform all-reduce by using the communication pattern of all-to-all broadcast. Throughout this discussion, without loss of generality, we assume that
addition is the associative operation performed in the all-reduce. An illustration
of the all-reduce operation on S3 is given in Figure 1. At each node, the ﬁnal sum
is obtained by adding the content in the result buﬀer and the outgoing buﬀer.

Fig. 1. The all-reduce operation on S3 . At each node, parentheses show the local sum
in the outgoing buﬀer and the contents in the box is the local sum accumulated in the
result buﬀer

An All-Reduce Operation in Star Networks

423

All-Reduce
i
1. each Sn−1
performs All-Reduce recursively;
i
have the sum of corresponding
/* At this point, all processors in Sn−1
numbers to be added. */
i
, i = 1 . . . n, sends the local sum along
2. each processor in Sn−1
the communication link n and saves the local sum in its result buﬀer;
3. d = n − 1;
i
receives the local sum from
4. once each processor in Sn−1
j
a processor in Sn−1 , where j = i, it performs the following
while d ≥ 2 do
i
along the communication
send the local sum to a neighbor of Sn−1
link d;
add the number received from the processor and the content of
i
;
the result buﬀer in Sn−1
d = d − 1;
5. every processor adds the contents in its result buﬀer and the outgoing
message buﬀer;

Fig. 2. All-reduce communication operation scheme

Assume that each number in the box, initially in the result buﬀer, is a number
to be added.
An all-reduce operation follows the communication steps of all-to-all broadcast, but adds two numbers instead of concatenating messages. Thus, each message transferred in the all-reduce operation has only one word, where each word
hold the partial sum of numbers. At the termination of the all-reduce operation,
each node holds the sum (1 + 2 + . . . + n!). Figure 3 shows all-reduce performed
in S4 . The all-reduce scheme All-Reduce is shown in Figure 2.
Theorem 2. At the termination of the all-reduce operation, each processor in
n!
Sn holds the sum i=1 i.
Proof. The statement is vacuously true when n = 1. Let n > 1, and assume
inductively that when All-reduce on Sn−1 terminates, each processor in Sn−1
contains the sum of corresponding numbers. When All-reduce is called on Sn ,
i
i
All-reduce calls itself recursively on each Sn−1
, i = 1 . . . n. Since each Sn−1
is
a copy of Sn−1 , by the inductive hypothesis, when each of these recursive calls
i
, i = 1 . . . n, holds the sum of corresponding
terminates, each processor in Sn−1
numbers.
i
is linked
From the recursive deﬁnition of Sn given in Section 2, each Sn−1
j
to the other Sn−1 , j = i by exactly (n − 2)! links along communication link n.
Thus, after the execution of step 2 of All-reduce, exactly (n − 2)! processors in

424

E. Oh, H. Choi, and D. Primeaux

Fig. 3. The all-reduce operation on S4
j
i
i
Sn−1
holds the sum received from Sn−1
. Also, each processor in Sn−1
has the
i
local sum stored in its result buﬀer. Now, each Sn−1 holds the partial sums such
n!
i
that adding all partial sums results in i=1 i. Let p1 be a processor in Sn−1
, and
j
let j the partial sum received from Sn−1 . From the above discussion, we know
i
that hold the partial sum j .
that there are exactly (n − 2)! processors in Sn−1
If p1 is one of these processors then we are done. Suppose this is not the case.
Now from step 4 in All-reduce, we know that each of these processors will send
j along communication links n − 1, . . . , 2, respectively. We ﬁrst claim that no
i
is the neighbor of two distinct processors p2 and p3 that hold
processor p1 in Sn−1
at
the
beginning
of step 4. Suppose, for the sake of contradiction, that this
j
were not the case. Let p1 = σ1 . . . σn , and suppose that p1 is the neighbor of p2
and p3 along communication links x and y ∈ {1, . . . , n − 1}, respectively. Notice
ﬁrst that x = y since a processor is connected to exactly one processor along any
given communication link. Since p1 is connected to p2 along communication link
x, p2 = σx , . . . , σ1 , . . . σn . Similarly p3 = σy , . . . , σ1 , . . . σn . Since both p2 and p3
j
have j , both p2 and p3 are connected to Sn−1
along communication link n.
j
Now all processors in Sn−1 have the same nth symbol. Since p2 and p3 are the
j
neighbors of two processors in Sn−1
along communication link n, it follows that
both p2 and p3 have the same ﬁrst symbol and σx = σy , a contradiction, since
σx and σy are two symbols in the representation of processor p1 , and hence must
be distinct.

An All-Reduce Operation in Star Networks

425

Table 2. Comparison of all-reduce operations on communication time
Dimension of Sn Size of Sn Tseng [11] Sheu [10]
3
6
12
6
4
24
27
12
5
120
48
18
6
720
75
26
7
5040
108
34
8
40320
127
42
9
362880
192
50
10
3628800
243
60

Ours
3
6
10
15
21
28
36
45

It follows from the above claim that the neighbors of the processors possessing the sum j at the beginning of step 4 of All-reduce are distinct. Now each
processor that holds j broadcasts it to exactly n − 2 neighbors along communication links n − 1, . . . , 2. Since all these neighbors are distinct, the number of
i
i
that receive j from a processor in Sn−1
at the beginning
processors in Sn−1
i
of step 4 is (n − 2)(n − 2)!. Thus, the total number of processors in Sn−1
that
hold j at the end of All-reduce is (n − 2)! + (n − 2)(n − 2)! = (n − 1)!. It
i
hold j and in particular p1 . Since p1 and j
follows that all processors in Sn−1
i
possesses every j at the end
were arbitrarily chosen, every processor in Sn−1
of All-reduce, and hence holds the total sum

n!
j=1

j.

Theorem 3. All-reduce performs an all-reduce operation on Sn in time n(n−
1)/2.
Proof. The above theorem proves that when the algorithm All-reduce termin!
nates, each processor holds the sum i=1 in the system. Let T (n) be the number
i
performs
of communication steps performed by All-reduce on Sn . Each Sn−1
an all-reduce operation within itself and then sends a single message along communication link n, and then along communication links n − 1, . . . , 2. Thus, the
i
is T (n − 1) + n − 1.
number of communication steps performed by each Sn−1
i
Since all the Sn−1 ’s do this in parallel, the number of communication steps for
i
.
Sn is the same as the number of communication steps performed by each Sn−1
i
Thus, the total number of communication steps performed by each Sn−1 , and
hence, by the whole network is given by the recurrence T (n) = T (n − 1) + n − 1.
It gives T (n) = n(n − 1)/2.

5

Concluding Remarks

In this paper we presented an eﬃcient all-reduce communication operation scheme
by using the all-to-all broadcast communication pattern. Our scheme performs
an all-reduce operation on an n-star network with the single-port capability in
n(n − 1)/2 time steps. If we use an all-to-one reduction followed by a one-to-all

426

E. Oh, H. Choi, and D. Primeaux
n

broadcast, an all-reduce can be performed in time 2 i=2 ( log(i−1) +1) by the
broadcast scheme proposed in [10] and in time 3(n − 1)2 by the scheme proposed
in [11]. In terms of the communication time shown in Table 2, our algorithm
provides an improvement over the algorithms in [10, 11].

References
1. S. B. Akers, D. Harel, and B. Krishnamurthy, “The Star Graph: An Attractive Alternative to The n-cube,” Proc. Int’l. Conf. of Parallel Processing, 1987,
pp. 393-400.
2. S. B. Akers and B. Krishnamurthy, “The Fault Tolerance of Star Graphs,”
Proc. 2nd Int’l. Conf. on Supercomputing, 1987, pp. 270-276.
3. S. G. Akl, K. Qiu, and I. Stojmenovic, “Fundamental Algorithms for The
Star and Pancake Interconnection Networks With Applications to Computational
Geometry,” Networks, 1993, vol.23, no. 4, pp. 215-225.
4. M. -S. Chen, P. S. Yu, and K. -L. Wu, “Optimal NODUP All-to-All Broadcast Schemes in Distributed Computing Systems,” IEEE Trans. Parallel and Distributed Systems, 1994, pp. 1275-1284.
5. K. Efe and A. Fernandez, “Computational Properties of Mesh Connected
Trees: Versatile Architecture for Parallel Computation,” Int’l Conference on Parallel Processing, 1994, pp. 72-76.
6. Intel Corp. iPSC/2 User’s Guide, Intel Corp., Mar, 1988.
7. I. M. Mkwawa and D. D. Kouvatsos, “An Optimal Neighbourhood Broadcasting Scheme for Star Interconnection Networks,” J. Interconnection Networks,
2004, vol. 4, pp. 103-112.
8. E. Oh and J. Chen, “Strong Fault-Tolerance: Parallel Routing in Star Networks
with Faults,” J. Interconnection Networks, 2003, vol.4, pp. 113-126.
9. R. Rabenseifner and P. Adamidis, “Collective Reduction Operation on Cray
X1 and Other Platforms,” Proc. the Cray User Group, 2004
10. J. -P. Sheu, C. -T. Wu, and T. -S. Chen, “An Optimal Broadcasting Algorithm Without Message Redundancy in Star Graphs,” IEEE Trans. Parallel and
Distributed Systems 1995, vol.6, no. 6, pp. 653-658.
11. Y. -C. Tseng and J. -P. Sheu, “Toward Optimal Broadcast in A Star Graph
Using Multiple Spanning Trees,” IEEE Trans. Computers 1997, vol. 46, pp. 593599.

