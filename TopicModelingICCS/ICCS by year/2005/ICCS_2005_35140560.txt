Snap-Stabilizing k-Wave Synchronizer
Doina Bein1 , Ajoy K. Datta2 , Mehmet H. Karaata3 , and Safaa Zaman4
1
2

School of Computer Science, University of Nevada Las Vegas, NV
siona@cs.unlv.edu
School of Computer Science, University of Nevada Las Vegas, NV
datta@cs.unlv.edu
3
Department of Computer Engineering, Kuwait University
karaata@cairo.eng.kuniv.edu.kw
4
Department of Computer Engineering, Kuwait University

Abstract. We propose a snap-stabilizing k-wave algorithm for routed
trees, called kW A, which implements k consecutive waves, k > 1. A snapstabilizing algorithm, starting from an arbitrary system configuration,
always behaves according to its specification. The maximum number of
states per process is 2k + 1, better than the previous result of at most 3k
states per process. A snap-stabilizing algorithm guarantees that, starting
from an arbitrary system configuration, always behaves according to its
specification.
Keywords: Distributed computing, fault tolerance, multi wave algorithm, routed tree, snap stabilization, synchronizer.

1

Introduction

Solutions to a large number of fundamental problems in distributed computing
require the execution of a constant number of consecutive distinct waves with
corresponding feedbacks [1] (e.g. broadcasting of information, distributed inﬁmum computation, leader election, mutual exclusion, spanning tree construction
problems, clock synchronization, etc.). Consecutive distinct broadcasts with the
corresponding feedbacks are implemented by the multi-wave algorithms, also
referred to as the asynchronous clock synchronizers.
In the literature, P IF (propagation of information with feedback) algorithms
are also referred to as echo algorithms [2]. Such wave algorithms are essential
components of some distributed algorithms: leader election [3], snapshot [4],
global reset [4, 5, 6, 7], synchronization [6, 8, 9].
A self-stabilizing system automatically recovers to normal behavior in case of
transient faults, without a centralized control. Regardless of the current conﬁguration, the system reaches a legal state in ﬁnite number of steps and the system
state remains legal thereafter [10], being able to withstand transient failures (a
fault that perturbs the state of the system but not the program). Dolev provides
a detailed survey on self-stabilization in [11].
V.S. Sunderam et al. (Eds.): ICCS 2005, LNCS 3514, pp. 560–567, 2005.
c Springer-Verlag Berlin Heidelberg 2005

Snap-Stabilizing k-Wave Synchronizer

561

If a certain stabilizing algorithm always behaves according to its speciﬁcation,
then is said to be snap-stabilizing, therefore it has optimal stabilization time.
The notion of snap-stabilization was introduced in 1999 in [12]. The ﬁrst snapstabilizing P IF algorithm for arbitrary networks is proposed in [13]. A snapstabilizing implementation of the P IF with cleaning called the PFC scheme
(propagation of information with feedback and cleaning) is proposed by Bui et.
al in [12], in which are three states in every wave. Therefore, a straight forward
solution for a k-wave snap-stabilizing algorithm will have 3k states per process.
One of the main advantages of the P IF scheme to be snap-stabilizing is that
the arbitrary initial state has limited or no eﬀect on the pace of the broadcast
propagation, so it implements the propagation with the optimal delay. Using
the snap-stabilizing P IF algorithm of Bui et. al [12], distinct consecutive waves
can be implemented by means of a counter per each process incremented upon
receipt of each broadcast message. However, notice that the resulting algorithm
is no longer stabilizing due to the need to initialize the counters. Therefore,
the adaptation of the snap-stabilizing wave algorithm of Bui et. al to multiple
consecutive distinct waves with feedback is not straight forward. The primary
challenge stems from the introduction of new states representing the new phases
while maintaining the snap-stabilization property.
Contributions. In this paper, we propose a snap-stabilizing k-wave algorithm
(kW A), k > 1, for routed trees. The algorithm runs forever and the delay to
start a kW A cycle is O(h) rounds, which is optimal, where h is the height of
the tree (the delay represents the time taken by the algorithm to start a kW A
cycle beginning in an arbitrary initial state). A round refers to a minimum
execution sequence in which each enabled action is performed at least once.
The maximum number of states per process is 2k + 1, which is better than the
previous implementations of 3k states per process and the same delay. A round
refers to a minimum execution sequence in which each enabled action is taken
at least once. The algorithm is optimal with respect to its time complexity and
can be generalized to general networks using any of the self-stabilizing spanning
tree construction algorithms proposed in [5, 6, 14].
Outline of the paper. Section 2 describes the distributed model used and snapstabilization. Section 3 describes the speciﬁcation of propagation of information
with feedback (P IF ). The k-wave snap-stabilizing algorithm is presented in the
Section 4. Because of lack of space, the complete correctness proof of kW A
algorithm is omitted, only few important remarks are given. The paper ends
with concluding remarks and discussions on future work in Section 5.

2

Stabilization in Distributed Systems

A tree can be represented by an undirected graph T = (V, E) with vertex (node)
set V and edge set E, where |V | = n, and |E| = n − 1. We assume that each
vertex of T is a process with a unique ID. We denote the set of leaf processes in

562

D. Bein et al.

T by L and the set of internal nodes by I. For each process i, N.i, D.i, and p.i
denote the set of its neighbors, the set of its children and its parent, respectively.
Each process maintains a set of local variables whose values can only be
updated by the process itself after inspecting them and the local variables of its
neighbors.
The distributed program of some process can be expressed as a ﬁnite set of
guarded commands or guarded actions [15]:
* [ G[1] → A[1] G[2] → A[2] ... G[k] → A[k] ],
In the guarded action G[] → A[], the guard G[] is a Boolean function of the
variables of the process and its neighbors, and action A[] can read the variables
of the process and its neighbors, and update only the process variables. If G[] is
true, then the guarded action is called enabled. A process that has at least one
guard enabled is called enabled too. *[S] corresponds to the repeated execution
of S while there exists enabled guards. is called the nondeterminism symbol,
and means that one of the guarded actions separated by those symbols is selected
nondeterministically in each iteration and executed.
We assume the presence of a distributed daemon or scheduler : if more than
one process is enabled, then a subset of enabled processes is selected, and exactly one enabled guard of each selected process is executed atomically and
concurrently. In addition, we assume that the scheduler is weakly fair, i.e., any
action remaining enabled is eventually executed. A self-stabilizing algorithm implementing this can be adapted from [16, 17]. Therefore, an implementation of
this is omitted.
The state (configuration) of a process is the set of variables used by the
process. The system state is the cartesian product of the states of the processes in
the system. The set of all the possible system states is denoted by C. An execution
is a maximal sequence of states e = c1 , c2 , ... such that ∀ci ∈ C : ci−1 → ci by
executions some guarded action, for each i > 0. Given P a predicate deﬁned over
C, the set of all states that satisfy P , LP , is called the set of all legitimate states
with respect to P , or simply, the set of all legitimate states. Let P be a predicate
deﬁned over C. The notation c P means that an element c ∈ C satisﬁes the
predicate P deﬁned on the set C. Given a task T , the speciﬁcation of T is a
predicate called SP T , and the distributed protocol to accomplish the task T
is denoted by PT . The set of all possible computations of PT in the system is
denoted by E.
Definition 1 (Snap-stabilization). Let T be a task, and SPT the specification
of T . The program PT is snap-stabilizing for SP T on E if and only if ∀e ∈ E ::
e SP T .

3

Propagation of Information with Feedback (P IF )

Wave algorithms are often implemented using the propagation of information
with feedback (P IF ) algorithms [2, 18]. A snap-stabilizing implementation of the
P IF with cleaning called the PFC scheme (propagation of information with

Snap-Stabilizing k-Wave Synchronizer

563

feedback and cleaning) is proposed by Bui et. al in [12]. The value C, called the
cleaning state, denotes the initial state of a process before it participates in a
P IF -cycle, or the ending state after it participates in a P IF cycle.
Each process i maintains a variable Si called the S-value of i. The root can
use only two state values: B (broadcast) and C (cleaning). When executing
its B-action and C-action, the root changes its state from C to B and B to C,
respectively. The leaf processes use only F (feedback) and C states: the F-action
changes the state from C to F , and the C-action changes the state from F to
C. An internal process can be in C, B, and F states: after executing a B-action,
F-action, respectively C-action, the S-value changes from C to B, from B to F
and F to C, respectively.
According to the P F C cycle speciﬁcation, a normal broadcast phase is followed by a feedback phase which is initiated by the leaf processes. After initiating
the feedback phase, in the next round, the leaf processes can initiate the cleaning
phase by changing their states from F to C. So, the feedback and the cleaning
phases run concurrently. Eventually, the feedback phase reaches the descendants
of the root. The root now executes its C-action, i.e., changes its state from B to
C, and then waits until all its descendants are in cleaning phase. Then it changes
its state from C to B. This marks the end of the current P IF cycle and the
start of the next P IF cycle. Since this solution requires three phases (broadcast,
f eedback, and cleaning) in the P IF cycle, the method is called the propagation
of information with feedback and cleaning (PFC) and the corresponding cycle
the PFC cycle.
We say that a process p is B-done, F-done, or C-done to indicate that p has
executed its corresponding actions (B, F , or C, respectively).

4

k-Wave Snap-Stabilizing Algorithm (kW A)

We present the k-wave algorithm, k > 1, to implement k consecutive distinct
waves scheme on tree structured networks.
A kWA cycle consists of k consecutive P IF cycles where each P IF has its own
broadcast and feedback phase with a speciﬁc task. Starting from an arbitrary
conﬁguration where no message has yet been broadcasted, the root (r) initiates
the first broadcast phase by sending a first broadcast message to its children. The
internal processes forward the broadcast message to their children upon receipt
of the ﬁrst broadcast message. Once the ﬁrst broadcast phase reaches the leaf
processes, they notify their parents of the termination of the broadcast phase
by initiating the first feedback phase by sending first feedback messages. After
receiving ﬁrst feedback messages from all its children, a process sends a ﬁrst
feedback message to its parent. When all the neighbors of the root are in the
ﬁrst feedback phase, the second wave will be initiated by the root through the
second broadcast phase, which acts similarly. Following the k feedback phase, a
clear phase is initiated.
These k waves can overlap in a manner similar to those of the PFC cycle.
Unlike the PFC cycle, in a kW A cycle the feedback phase of each wave and the

564

D. Bein et al.

cleaning phase do not execute concurrently. Instead, upon termination of the l
feedback phase, the l + 1 wave is initiated, 1 ≤ l < k.
The state value C denotes the initial state of any process before it participates
in a kW A cycle. The state values B l , respectively F l denotes the broadcast,
respectively feedback state of a process, 1 ≤ l < k. The root uses k + 1 state
values: C, B l , 1 ≤ l < k. An internal process can have 2k + 1 diﬀerent state
values C, B l and F l , 1 ≤ l < k. A leaf process uses only k + 1 state values, C,
F l , 1 ≤ l < k.
The root executes its rB 1 -action, rB l -action, 1 < l < k, and rC-action
by changing its state from C to B 1 , B l−1 to B l , and B k to C, respectively.
An internal process executes its iB 1 -action, iF 1 -action, iB l -action, iF l -action,
1 < l ≤ k, and iC-action, by changing its S-value from C to B1, from B 1 to F 1 ,
from F l−1 to B l , from B l to F l , and from F k to C respectively. A leaf process
executes its lF 1 -action, lF l -action, 1 < l ≤ k, and lC-action respectively by
changing its state from C to F 1 , from F l−1 to F l , and from F k to C respectively.
Based on the above description, we deﬁne the kW A cycle in terms of the following kWA-actions: B 1 -action, F 1 -action, B 2 -action, F 2 -action, ..., B k -action,
F k -action, C-action. The B l -action, F l -action, with 1 ≤ l ≤ k, refer to the
actions executed during l-th broadcast, l-th feedback phase respectively. The
C-action is executed in order to terminate the current kW A cycle.
Definition 2 (kW A cycle). A finite computation e ∈ E is called a kWA cycle,
denoted by e kW A-cycle, if and only if the following conditions hold:
L1 At least one process r, called the initiator, sends a message or completes the
kW A cycle (rB1).
L2 If an internal process i (i ∈ I) receives a ﬁrst/second/.../ k-th broadcast
message from its parent p.i, then i eventually sends this message to all its
descendants (iB1, iBl).
L3 If a leaf process i (i ∈ L) receives a ﬁrst/second/.../ k-th broadcast message
from its parent p.i, then i eventually sends a ﬁrst/second/.../k-th feedback
message to p.i (lF1, lFl).
L4 If an internal process i (i ∈ I) receives the ﬁrst/second/... k-th feedback
message from all of its descendants, then i eventually sends this message to
its parent p.i (iF1, iFl).
L5 If the initiator is in l-th broadcast phase and receives l-th feedback message
from all of its descendents, then it sends the (l + 1)-th broadcast message to
all of its descendents (rBl).
L6 If the root r receives the k-th feedback message from all of its descendants,
then r eventually terminates the current kW A cycle (rC).
S The root r cannot terminate the kW A cycle (rC) more than once.
Conditions [L1] to [L6] are called the liveness properties, and condition [S] is
called the safety property. By Condition [L1], we know that there exists at least
one initiator in a kW A cycle.
The algorithm implementing the above mechanism is given in Figure 1. The
value of l is 1 < l ≤ k.

Snap-Stabilizing k-Wave Synchronizer

565

Variables
Si (i = r) ∈ {B 1 , B 2 , ..., B k , C} for the root
Si (i ∈ I) ∈ {B 1 , F 1 , B 2 , F 2 , ..., B k , F k , C} for internal processes
Si (i ∈ L) ∈ {F 1 , F 2 , ...F k , C} for the leaf processes
Actions
{Program for root process i }
rB1

∗ Si = C ∧ ∀j∈N.i Sj = C

−→ Si := B 1 ;

rBl
rC

Si = B l−1 ∧ ∀j∈D.i Sj = F l
(Si = B k ∧ (∀j∈N.i Sj = F k )∨
(Si = B 1 ∧ ∃j∈D.i Sj ∈
/ {B 1 , F 1 , C})∨

−→ Si := B l ;

/ {B l−1 , F l−1 , B l , F l })
(1 < l < k ∧ Si = B l ∧ ∃j∈D.i Sj ∈

−→ Si := C;

{Program for internal process i }
iB1

∗ Si = C ∧ Sp.i = B1 ∧ ∀j∈D.i Sj = C

−→ Si := B 1 ;

iF1
iBl
iFl
iC

Si = B1 ∧ Sp.i = B1 ∧ ∀j∈D.i Sj = F 1
Si = F l−1 ∧ Sp.i = B l ∧ ∀j∈D.i Sj = F l−1
Si = B l ∧ Sp.i = B l ∧ ∀j∈D.i Sj = F l
(Si = F k ∧ Sp.i = F k ∧ ∀j∈D.i Sj ∈ {F k , C})∨
(1 < l ≤ k ∧ Si = B l−1 ∧ (Sp.i ∈
/ {B l−1 , F l−1 , B l }∨
l−1
l−1
l
/ {B , F
, F })∨
∃j∈D.i Sj ∈
/ {B l−1 , F l−1 , B l }∨
(1 < l ≤ k ∧ Si = F l−1 ∧ (Sp.i ∈

−→ Si := F 1 ;
−→ Si := B l ;
−→ Si := F l ;

∃j∈D.i Sj = F l−1 )

−→ Si := C;

{Program for leaf process i }
lF1

∗ Si = C ∧ Sp.i = B 1

−→ Si := F 1 ;

lFl
lC

Si = F l−1 ∧ Sp.i = B l
(Si = F k ∧ Sp,i = F k )∨
(1 < l ≤ k ∧ Si = F l−1 ∧ Sp.i ∈
/ {B l−1 , F l−1 , B l })∨

−→ Si := F l ;

/ {B k , F k , C})
(Si = F k ∧ Sp.i ∈

−→ Si := C;

Fig. 1. The Snap-Stabilizing k-Wave Algorithm (Algorithm kWA)

Let Init be the set of processes that initiate kW A cycles. We will call a
kW A cycle as a normal kW A cycle if Init = {r} and the ﬁrst action of r is a
rB 1 -action.
Definition 3 (kWA scheme). We define computation e as a kW A scheme,
denoted by e SP kW A , as an infinite sequence of kW A cycles, e = e0 , e1 , ....
such that ∀i ≥ 1, ei is a complete kW A cycle.

566

D. Bein et al.

The following proposition follows from the deﬁnition of the kW A cycle.
Proposition 1. In a normal kW A cycle, every process executes a B 1 -action, a
F 1 -action, ..., a B k -action, and a F k -action at most once.
From Deﬁnition 2, it is obvious that once p executes one of the kW A-actions, p
will not execute that action anymore in the same kW A cycle.
In any conﬁguration, either the root is enabled to execute and the other
processes have no enabled actions, or the root has no enabled actions and the
other processes are enabled to execute. The following proposition follows from
Deﬁnition 2 and Proposition 1.
Proposition 2. In any configuration of the kW A cycle, either one of the following conditions holds:
(i) The root r is enabled to start the lth wave, 1 ≤ l ≤ k, by executing its
l
B -action.
(ii) The root r is surrounded by a maximal set of processes all of which are
enabled to execute a kW A-action corresponding to either lth wave, 1 ≤ l ≤ k,
such that each process between r and a process in this set (including r) is B l -done.
Definition 4 (Snap-Stabilizing kW A). A kW A algorithm is snap-stabilizing
if and only if the root executes a kW A-action infinitely often, and the kW A
cycle satisfies SP kW A .
Theorem 1. Algorithm kWA is snap-stabilizing.
The optimal delay of starting a kW A cycle of any implementation of the kW A
cycle is shown to be O(h) rounds in [6]. We can show that the delay to start the
ﬁrst kW A cycle using Algorithm kWA is bounded by O(h) rounds. Therefore,
the algorithm kW A is optimal with respect to its delay to start a kP W cycle.

5

Conclusions

In this paper, we propose a snap-stabilizing k-wave algorithm for routed trees,
called kW A, with k > 1. The algorithm ensures that a cycle may start after
O(h) rounds, which is optimal, where h is the height of the tree. The maximum
number of states per process is 2k + 1, which is better than the previous implementations of 3k states per process and the same delay. We intend to apply
the concept of kW A scheme in devising optimal snap-stabilizing algorithms for
some fundamental problems in distributed computing.

References
1. G. Tel. Total algorithms. In: Vogt F.H. (ed) Concurrency 88, Springer-Verlag
LNCS 335, pages 277–291, 1988.
2. E.J. Chang. Echo algorithms: depth parallel operations on general graphs. IEEE
Transactions on Software, SE-8:391–401, 1982.

Snap-Stabilizing k-Wave Synchronizer

567

3. S. Dolev, A. Israeli, and S. Moran. Uniform dynamic self-stabilizing leader election.
IEEE Transactions on Parallel and Distributed Systems, 8(4):424–440, 1997.
4. G. Varghese. Self-stabilization by counter flushing. SIAM Journal on Computing,
30(2):486–510, 2000.
5. A. Arora and M.G. Gouda. Distributed reset. IEEE Transactions on Computers,
43:1026–1038, 1994.
6. B. Awerbuch, S. Kutten, Y. Mansour, B. Patt-Shamir, and G. Varghese. Time
optimal self-stabilizing synchronization (extended abstract). In Proceedings of the
Twenty-Fifth Annual ACM Symposium on the Theory of Computing, pages 652–
661, San Diego, California, 16–18 May 1993.
7. B. Awerbuch, B. Patt-Shamir, and G. Varghese. Self-stabilization by local checking
and correction. In Proceedings of the 32th Annual IEEE Symposium on Foundations
of Computer Science, pages 268–277, 1991.
8. L.O. Alima, J. Beauquier, A.K. Datta, and S. Tixeuil. Self-stabilization with global
rooted synchronizers. In ICDCS98 Proceedings of the 18th International Conference on Distributed Computing Systems, pages 102–109, 1998.
9. B. Awerbuch and G. Varghese. Distributed program checking: a paradigm for
building self-stabilizing distributed protocols. In FOCS91 Proceedings of the 31st
Annual IEEE Symposium on Foundations of Computer Science, pages 258–267,
1991.
10. E.W. Dijkstra. Self-stabilizing systems in spite of distributed control. In EWD 391,
In Selected Writings on Computing: A Personal Perspective, pages 41-46,1973.
11. S. Dolev. Self-Stabilization. MIT Press, Cambridge, MA, 2000. Ben-Gurion University of the Negev, Israel.
12. A. Bui, A.K. Datta, F. Petit, and V. Villain. State-optimal snap-stabilizing PIF in
tree networks. In Proceedings of the Third Workshop on Self-Stabilizing Systems
(published in association with ICDCS99 The 19th IEEE International Conference
on Distributed Computing Systems), pages 78–85. IEEE Computer Society, 1999.
13. A. Cournier, A.K. Datta, F. Petit, and V. Villain. Snap-stabilizing PIF algorithm
in arbitrary networks. In ICDCS02 The 22nd IEEE International Conference on
Distributed Computing Systems, pages 199–206, 2002.
14. S.T. Huang and N.S. Chen. A self-stabilizing algorithm for constructing breadthfirst trees. Information Processing Letters, 41:109–117, 1992.
15. E.W. Dijkstra. Guarded commands, nondeterminacy, and formal derivation of
programs. Communications of the ACM, 18(8):453–457, 1975.
16. M.G. Gouda and F. Haddix. The alternator. In Proceedings of the Third Workshop
on Self-Stabilizing Systems (published in association with ICDCS99 The 19th IEEE
International Conference on Distributed Computing Systems), pages 48–53. IEEE
Computer Society, 1999.
17. M. Mizuno and M. Nesterenko. A transformation of self-stabilizing serial model
programs for asynchronous parallel computing environments. Information Processing Letters, 66(6):285–290, 30 June 1998.
18. A. Segall. Distributed network protocols. IEEE Transactions on Information
Theory, IT-29(1):23–35, January 1983.

