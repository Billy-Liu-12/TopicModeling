Solving Coupled Geoscience Problems on High
Performance Computing Platforms
Dany Kemmler1 , Panagiotis Adamidis2 , Wenqing Wang1 , Sebastian Bauer1 ,
and Olaf Kolditz1
1

2

Center for Applied Geoscience, University of Tuebingen, Germany
dani@uni-tuebingen.de
http://www.uni-tuebingen.de/zag/
High Performance Computing Center, University of Stuttgart, Germany
http://www.hlrs.de
Abstract. This paper describes a finite element geo-process modeling
software, which is able to solve multiphysics problems in the area of geo
science. It is one of the first applications providing complete thermal,
two-phase-flow, inelastic deformation court within one framework. As a
real-life example the FEBEX T H 2 M coupling problems are described.
These kind of problems are very demanding in terms of CPU time and
memory space which cannot be provided by single processor architecture.
Therefore the exploitation of high performance architectures is required.
Here we present the results of the vectorization of the existing serial code
as a first step towards parallelization.

1

Introduction

Modeling complex coupled transient and dynamic processes found in geo-systems
requires increasingly high resolution models for a more precise and qualiﬁed validation and evaluation. The more exactly the investigation of the given problem
can be accomplished, the better the utilization, management and planning of
the given geo-resources can be undertaken.
1.1

GeoSys/RockFlow

GeoSys/RockFlow (GS/RF) [1] is a ﬁnite element geo-process modeling software
tool ﬁnding increased application in modeling and simulation in ﬁelds such as
water resource management, geotechnics, design of geo-engineered barriers, exploitation of geothermal energy, soil and groundwater contaminant transport and
remediation strategies. Its current design is based upon a sequential approach.
Here we present a new project which started in fall 2004 and is funded by
the German Research Foundation (DFG) and the State of Baden-Wuerttemberg
(BW). Its goal is to bundle the knowledge of the experts at the High Performance
Computing Center (HLRS) in Stuttgart and the scientists and researchers working at the Center for Applied Geology, (ZAG) at the University of Tuebingen.
This project is truly interdisciplinary and multinational as specialists in mathematics, computer science, hydrology, geo science, chemistry, just to name a few,
V.S. Sunderam et al. (Eds.): ICCS 2005, LNCS 3515, pp. 1064–1071, 2005.
c Springer-Verlag Berlin Heidelberg 2005

Solving Coupled Geoscience Problems on High Performance

1065

are continuing to develop the already existing serial software and to ﬁnally transform it into a GRID application deploying massive-parallel systems in Stuttgart
and Tuebingen.

2

GeoSys/RockFlow Model Features

The ﬁnite element simulator GS/RF covers a wide range of physical and chemical
processes relevant to environmental hydrosystems. The processes can be grouped
in 4 diﬀerent categories as summarized below:
1. Hydrological Processes:
– Groundwater ﬂow in conﬁned and unconﬁned aquifers
– Multi phase ﬂow
– Fracture ﬂow, dual porosity
– Density dependent ﬂow (thermal, tracer)
– River ﬂow (based on averaged 1-D Saint-Venant equations)
2. Thermal Processes:
– Heat transport with density changes
– Non isothermal multiphase ﬂow with phase changes
3. Chemical Processes:
– Multi-component transport with density changes
– Sorption models
– Reactive Transport (i.e. Freundlich Isotherm)
– Chemical reactions via coupling to PHREEQC2
4. Mechanical Processes:
– Poro elasticity
– Thermo elasticity
– Elasto plasticity (hardening)
Models can be created and run using a graphical user interface (Windows application). Built-in mesh generators are: gmsh, PrisGen and TetMesh. Meshing
of complex structures can be done using gOcad or the pre-processor GINA developed by the Federal Institute for Geosciences and Natural Resources (BGR).
Hybrid meshing is possible. ArcGIS shape ﬁles can be read and converted to
polylines which are then used to create meshes or assign boundary or initial conditions etc. An interface to Gstat allows for the generation of three dimensional
heterogeneous conductivity ﬁelds. Two dimensional contour plots and time-value
graphs are displayed during the simulation in the GUI or Tecplot output ﬁles
are written directly or created with the post-processor RF2TP also developed
by the BGR.
Despite its long history dating back to 1985 the code is programmed according to recent programming principles. The software was constantly improved.
In 2003/2004 the code underwent a major re-organization to beneﬁt even more
from object-orientation and to allow an easier switching between process couplings. Most recent changes are: use of C++, organization of RockFlow into
GeoSys: GEOLib, MSHLib, FEMLib and the creation and encapsulation of
process-oriented objects (PCS). These changes provide a solid basis for further
program development within a growing research team.

1066

3

D. Kemmler et al.

The FEBEX Experiment

As an example of the necessity of high performance computing in environmental
science we present results of diﬀerent simulations of the FEBEX experiment [2]
which is a full scale engineering barriers experiment in crystalline rock (Fig. 1)
for high level radioactive waste (HLW) repository.

Fig. 1. Layout of the FEBEX experiment

In the experiment, the heaters experienced a temperature of 100◦ C. Bentonit was used as ﬁlling material (geotechnical barrier) around the heater. The
heaters and geotechnical barrier were in a tunnel (2.4 m in diameter) surrounded
by granite. The bentonite swells as a result of the intrusion of ground water
from fractures. The swelling eﬀect changed the microstructure of bentonite, i.e.
porosity and in particular permeability of the bentonite. On the other hand
it increased the swelling pressure and accordingly the eﬀective tension in the
bentonite. The expanded bentonite in turn exerted pressure on the surrounding granite and aﬀected the stress conditions and possibly the water path in
it. Gas was produced by the high temperature of the heaters from the water,
which moved outward. The experiment was conducted in order to understand
the thermo-hydro-mechanically (THM) coupled phenomena in the bentonite and
surrounding granite and furthermore provide a valid reference for long term HLW
repository.
3.1

Simulation Results

One part of the simulation was the analysis of the T H 2 M coupling problems
(always with the same boundary conditions and with the same material parameters) using a ﬁne mesh of 3276 nodes and 2640 hexahedra. The geometry and
mesh are depicted in Fig. 2, in which a cross section close to the front of the
present saturation propagation is selected to portray the inside domain and z
direction points downward in order to permit a close look around the canister
surface.
In the following ﬁgures (Fig. 3) the primary variables of the T H 2 M simulations after 10 (t = 105 sec) time steps are depicted.
The FEBEX simulations are to our knowledge the ﬁrst all-in-one 3D fully
coupled T H 2 M C n simulations in which the complete thermal, two-phase-ﬂow,

Solving Coupled Geoscience Problems on High Performance

1067

Y
X

Z
0

2

Z
4
4

Y

2
0
2
0

4

X

Fig. 2. Mesh and slice section

inelastic deformation court is provided by one unique code. T H 2 M C n simulations are very sensitive to time and space scale discretizations and such simulations are very expensive with respect to computation time.
3.2

FEBEX Performance

The results obtained from this analysis show strong TH2 M coupling phenomena
in the present FEBEX type problem. The change of displacements and stresses
is large in the buﬀer. This change is partially due to the material properties
of the buﬀer but mainly it is induced by stiﬀ boundary condition around the
buﬀer. Therefore, the safety assessment must be focused on this domain. This
computation required much more CPU time due to the necessity of solving four
algebraic equations within coupling iterations, nonlinear iterations for diﬀerent
processes. For example, a converged Newton step for plasticity deformation took
an average of 2.5 hours using a machine with an Intel III 2.0GHz CPU. To
compute 100 time steps of FEBEX on 4 processors the serial version of GS/RF
runs nonstop for one week. To improve computation eﬃciency high performance
computing on powerful parallel machines has to be deployed.

4

Parallelization Strategy

Today’s modern single processor machines are not powerful enough to process
large scale models necessary for the realistic simulation of in situ highly complex
geoscience systems.
Through the combination of the wide ﬁeld of geoscience and high performance
computation supported through eﬀective algorithms found in computer science
a complete new level in simulation and modeling can be attained. The ﬁnal goal
is to compute models consisting of more than ten million of elements with a
higher grade of specialization.

D. Kemmler et al.
Z

Z

Y

Y

X

X

5

5

Z

3

2

1

4

3

SATURATION2
0.7625
0.725
0.6875
0.65
0.6125
0.575
0.5375
0.5
0.4625
0.425
0.3875
0.35
0.3125
0.275
0.2375

Z

4

PRESSURE1
919639
839278
758917
678556
598195
517834
437473
357112
276751
196390
116029
35667.6
-44693.4
-125054
-205415

2

1

0

0

4

4
0

0

2

4

2

0

Y

X

Gas pressure pg

0

4

2

X

Liquid saturation S l
Z

Z

Y

Y

X

X

5

5

TEMPERATURE1
368.428
4
363.856
359.284
354.712
350.14
3
345.568
340.996
336.424
2
331.852
327.28
322.708
318.136
1
313.564
308.992
304.42

Z

2

Y

DISPLACEMENTX1
0.000163447
0.000151447
0.000139446
0.000127445
0.000115445
0.000103444
9.14437E-05
7.94431E-05
6.74426E-05
5.5442E-05
4.34414E-05
3.14409E-05
1.94403E-05
7.43973E-06
-4.56084E-06

Z

1068

DISPLACEMENTZ1
0.000974645
0.000865417
0.000756189
0.000646961
0.000537733
0.000428505
0.000319277
0.000210049
0.000100821
-8.4076E-06
-0.000117636
-0.000226864
-0.000336092
-0.00044532
-0.000554548

4

Z

3

2

1

0

0

4

4
2
0

Y

0

2

4

2

0

Y

X

0

4

2

X

Temperature T

Displacement ux
Z

Z

Y

Y

X

X

5

4

4

3

Z

3

5

DISPLACEMENTY1
0.00127904
0.00119249
0.00110595
0.0010194
0.000932855
0.000846309
0.000759763
0.000673217
0.000586671
0.000500125
0.000413579
0.000327033
0.000240487
0.000153941
6.73946E-05

2

1

2

1

0
4

0
4

2

Y

0

0

2

4

2

X

Displacement uy

Y

0

0

4

2

X

Displacement uz

Fig. 3. Primary variables of TH2 M simulations for t = 105 sec

4.1

Computing Platform

The target parallel platform for the work presented in this paper is a GRID consisting of three parallel computing platforms, located at the computing center of

Solving Coupled Geoscience Problems on High Performance

1069

the University of Tuebingen, the HLRS in Stuttgart and a institute-own Linux
cluster. As the most powerful environment within the GRID can be found in
Stuttgart we have ported a ﬁrst version of GS/RF on a NEC SX6 at the HLRS.
The SX6, in summer being replaced by a SX8, implements a parallel architecture
with distributed memory, in particular a cluster of symmetric multi-processors
(SMPs). Each of its SMP nodes consists of 8 vector processors. To take advantage of the features of such an architecture the code has to be vectorised and
parallelized.
In order to run an application on such a computational environment the work
and the data have to be distributed among the processors. The decision that has
to be made is what work will be done on which data. In case each process is
performing a diﬀerent task on diﬀerent data we are talking about a MPMD
(Multiple Program Multiple Data) programming model. If on the other hand
the same program is running on every processor but with diﬀerent data, this
corresponds to a SPMD (Single Program Multiple Data) programing model. In
this project we decided to use the SPMD paradigm.
Domain decomposition and load balancing techniques are a very suitable way
of decomposing data and work in numerical analysis. Using load balancing tools
like those of Jostle[3], Metis [4] or ParMetis the original computational domain
can be divided into a number of subdomains, equal to the number of available
computing units, and these subdomains can be distributed to the processors.
In this way the resulting linear systems become smaller compared to the global
one. Generally, the two CPU time-consuming parts of a ﬁnite element program
are the calculation of the entries in the system matrices (element loop) and the
solving of the resulting linear systems.
In particular, the calculation of the system matrices is done element-wise
without any dependency on the computation between diﬀerent elements. Therefore, the contributions of diﬀerent elements can be calculated on diﬀerent processors. When solving the linear system of equations additional care has to be
taken for the unknowns residing on the artiﬁcial boundaries.

5

GS/RF Performance

We tested GS/RF on the SX6 with an artiﬁcial 2D test model consisting of 10620
elements.
The fastest solver used to solve the linear system of equations is the BiCGSTAB
method. Applying the original non-optimized code took 3412.62 seconds and
achieved 21.2 MFlops on one NEC SX6 processor. A more detailed analysis of
the relevant GS/RF code showed that the most CPU time-consuming part of the
BiCGSTAB algorithm is the matrix-vector multiplication. 99.7 % of the CPU
time (3403.294 sec) is spent in computing the matrix-vector product.
The available NEC compilers oﬀer directives which support code vectorization. Employing some of these directives the same test case terminates after
only 74.04 secs, thus achieving a performance of 1015.1 MFlops. The overall

1070

D. Kemmler et al.
Table 1. Comparision of non-vectorized and vectorized code versions
non-vectorized version
total
matrix-vector
Computation time [secs] 3412.62
3403.29
Performance [MFlops]
21.2
21.2

vectorized version
total matrix-vector
74.04
63.46
1015.1
1182.6

time spent in the matrix-vector multiplication of the vectorized code is only
63.46 seconds at a rate of 1182.6 MFlops. Table 1 summarizes these performance
measurements.
Fig. 4 depicts the resulting pressure distribution between the top right and
bottom left corner of the computational domain calculated on the SX6.

Frame 001 ⏐ 07 Jan 2005 ⏐

20000

Y

15000

10000

5000

0

0

10000

20000

30000

40000

X

Fig. 4. Pressure distribution between top right and bottom left

Not all codes are suitable for vectorization. Our results presented above show
that the GS/RF code is suitable for vectorization.
Our real-world target models consist of hundreds of millions of elements.
Thus, in order to take advantage of the architecture of the NEC SX6 and the
SX8 respectively and to achieve higher performance, it is necessary to parallelize
the code. The next step of our project is therefore to implement a parallel version
of the BiCGSTAB algorithm based on domain decomposition and load balancing
techniques. In a later stage GS/RF should be imbedded either in the Globus [5]
or UNICORE [6] environment. For parameter studies the usage of Condor [7]
seems to be very interesting.

Solving Coupled Geoscience Problems on High Performance

6

1071

Conclusion

In this work we have presented a geo-process software system, modeling complex coupled processes found in geo-systems. The results of the simulation of the
FEBEX problem, a full scale engineering barriers experiment in crystalline rock
for high level radioactive waste (HLW) repository show that the use of conventional hardware impose limitations on reliable modeling and simulation due to
computation time.
A GRID environment can provide and enable the exploitation of the necessary resources to overcome the above mentioned limitations. As shown in this
paper, vectorization of the serial GS/RF code leads to performance gains which
is the ﬁrst step in porting the code to high performance computing architectures.
Further work on the GS/RF code will be done on parallelizing the solver algorithm alongside the coupling of shallow water and groundwater ﬂow, as well as
including kinetic biogeochemical reactions as well as improving the multi-view
and three-dimensional graphics for the GUI.

References
1. GeoSys/RockFlow Team: GeoSys/RockFlow Web Page (2005) http://www.unituebingen.de/zag/geohydrology/index.html.
2. Wang, W., Mingliang Xie, O.K.: FEBEX Benchmark Studies. GeoSys - Preprint
2004-40, Center of Applied Geosciences, University of T¨
ubingen (2004)
3. Walshaw, C., Cross, M.: Parallel Optimisation Algorithms for Multilevel Mesh
Partitioning. Parallel Comput. 26 (2000) 1635–1660 (originally published as Univ.
Greenwich Tech. Rep. 99/IM/44).
4. Karypis, G., Kumar, V.: METIS:A Software Package for Partitioning Unstructured
Graphs, Partitioning Meshes, and Computing Fill-Reducing Orderings of Sparse
Matrices. Technical report, University of Minnesota,Department of Computer Science / Army HPC Research Center (1998)
5. Foster, I., Kesselmann, C.:
The Globus Project: A Status Report.
In:
IPPS/SPDP’98 Heterogeneous Computing Workshop. (1998) 4–18
6. (Ed.), D.E.: Joint Project Report for the BMBF-Project UNICORE Plus. ISBN:
3-00-011592-7 (January 2000 - December 2002) Grant Number: 01 IR 001 A-D.
7. Tannenbaum, T., Wright, D., Miller, K., Livny, M.: Condor – a distributed job
scheduler. In Sterling, T., ed.: Beowulf Cluster Computing with Linux. MIT Press
(2001)

