A Bandwidth Sensitive Distributed Continuous
Media File System Using the Fibre Channel
Network
Cuneyt Akinlar1 and Sarit Mukherjee2
1

Computer Eng. Dept., Anadolu University, Eskisehir, Turkey
cakinlar@anadolu.edu.tr
2
Lucent Technologies, Holmdel NJ, USA
sarit@lucent.com

Abstract. A recent trend in storage systems design is to move disks
to a storage area network for direct client access. While several highly
scalable ﬁle systems have been built using such direct attached disks,
they have all been designed for traditional text-based data, and are not
well suited for streaming continuous media, i.e., audio and video ﬁles,
which are characterized by high volumes of data and require strict timing requirements during storage and retrieval. In this paper, we propose
a scalable distributed continuous media ﬁle system built using Storage
Area Network (SAN)-Attached disks, and describe bandwidth and time
sensitive read/write procedures for our ﬁle system. We present experimental results on the performance of our Linux-based prototype implementation of the ﬁle system and show that the ﬁle system can provide
strict bandwidth guarantees for continuous media streams.

1

Introduction

A new trend in storage systems design is to move disks, that traditionally reside
behind a centralized server (e.g., Network File System (NFS) [6], Symphony [7],
Continuous Media File System (CMFS) [1]), to a storage area network (SAN)
for direct client access. While several highly scalable ﬁle systems have been built
using such direct attached disks (e.g., Global File System (GFS) [8], File systems
for NASD [4], xFS [2] among many others), they have all been designed for
traditional text-based data (i.e., ﬁle systems consisting of many small ﬁles), and
are not well suited for streaming continuous media ﬁles, which are characterized
by large volumes of data and stringent bandwidth requirements.
In this paper we propose a scalable distributed continuous media ﬁle system
based on Storage Area Network (SAN)-Attached disks that can provide strict
bandwidth guarantees to open media streams. We brieﬂy describe the general
architecture of the ﬁle system and discuss diﬀerent ways to implement this “realtime” sensitivity. We present experimental results on the performance of our
Linux-based prototype implementation of the ﬁle system using the Fibre Channel
V.S. Sunderam et al. (Eds.): ICCS 2005, LNCS 3514, pp. 396–404, 2005.
c Springer-Verlag Berlin Heidelberg 2005

A Bandwidth Sensitive Distributed Continuous Media File System

397

SAN. Our ﬁle system appears as a (Ext2fs) [3] ﬁle system to users, and can
provide strict bandwidth guarantees for continuous media streams.

2

Architecture of the Distributed File System

The architecture of the Fibre Channel Distributed File System (FCDFS) is
shown in Figure 1(a). Main components are a ﬁle server (FCDFS-Server) and
several Storage Area Network (SAN)-Attached disks, which are directly exposed
to the clients. While the SAN is used for data transmission between the disks
and the clients, a control network connects the clients and the server and is used
for exchange of ﬁle system meta-data and control messages. Although the ﬁgure
shows separate logical networks for the SAN and the Control Network, both can
coexist on the same physical network depending on the capabilities of the SAN.

Fig. 1. (a) Architecture of FCDFS: A ﬁle server, client and disks connected together,
(b) Open, and (c) Read/Write Operations: A and D denotes attribute and data disks,
respectively

FCDFS consists of a client part, which we call the Client-FCDFS, and a
FCDFS-Server. While Client-FCDFS is responsible for maintaining open ﬁles
(streams) and actual reading and writing of the data from/to the disks, the
server is responsible for maintaining the meta-data for all volumes, ﬁles and
directories, and coordinating client access to the data disks. The Client-FCDFS
and the server work together in a coordinated fashion to make the operations
seamless to the users.
An overview of the ﬁle read/write procedure is shown in Figure 1(c). To
read or write a block of data, the Client-FCDFS sends a read/write request
to the server specifying the logical block of the ﬁle to read/write along with
the deadline of the request. The server converts the logical block into a (disk,
physical block) pair, and schedules the request based on its deadline. Once the
client gets a response to its read/write request, it directly accesses the data disk.
Therefore, during a read operation, the data directly comes from the disk to the
client and during a write operation, the data directly goes from the client to
the disk. Excluding the ﬁle server from the path of data transfers makes the ﬁle

398

C. Akinlar and S. Mukherjee

system scalable. The server is contacted when the ﬁle is opened and closed and
for read/write request scheduling and acts like a data disk access coordinator.
The actual disk request is carried out by the client.

3

Bandwidth Allocation and Enforcement

To preserve the quality guarantee across all streams, per stream bandwidth
allocation and its enforcement is necessary, and involves the following steps:
1. Bandwidth allocation: Each stream (user) is allocated a certain bandwidth.
FCDFS-Client computes a deadline for each pull request using the negotiated bandwidth.
2. Bandwidth enforcement: FCDFS-Server employs a time-sensitive scheduling
discipline to ensure that no request misses its deadline.
3.1

Bandwidth Allocation by Client-FCDFS

Client-FCDFS keeps per-stream state information, performs bandwidth negotiation during ﬁle open, and subsequently assigns a service deadline to each request.
The deadline, speciﬁed in milliseconds, deﬁnes an interval (starting at the current time) by the end of which the request must be completed if the promised
stream bandwidth is to be satisﬁed. FCDFS ensures that the average bandwidth
usage per stream is within the negotiated value.
Figure 2(a) shows the procedure used to compute the deadline. To keep track
of past bandwidth usage by a stream, the Client-FCDFS keeps a virtual clock [9],

Fig. 2. (a) Algorithm for deadline computation, (b) An example assignment of relative
deadlines at the client. Negotiated bandwidth is 10Mbps and requests are in FCDFSblock size of 1MB. Notice that the deadline for the third request is negative, which
happens when a stream gets (or asks) less than what it negotiated for a period of time.
The negative deadline allows the stream to eventually catch up

A Bandwidth Sensitive Distributed Continuous Media File System

399

vc, for each stream. During the admission of the stream, vc is initialized to 0.
It is modiﬁed only when there is any I/O request for the stream. vc keeps
account of how long it should have taken for a request to complete had the
stream obtained the exact negotiated bandwidth. Note that FCDFS allows the
deadline to become negative if a stream does not use its allocated bandwidth.
However, FCDFS limits the maximum credit (i.e., the parameter MaxCredit
in bytes in the following algorithm) that a stream can accumulate during any
inactive period (i.e., no I/O). This ensures that the burst size to the system is
limited and regulated.
3.2

Bandwidth Enforcement by the FCDFS-Server

The bandwidth enforcement deals with eﬀective and eﬃcient scheduling of the
user requests at the server such that a request completes by its deadline. This
proceeds in two steps as detailed in the following two sections.
3.2.1 Global Request Deadline Computation
Upon reception of a request, the server converts the relative deadline of the
request (recall that each request carries a relative deadline that speciﬁes an
interval within which the request must be completed) into an absolute deadline
so that the relative order among requests from diﬀerent client hosts can be
constructed and maintained. Computation of the absolute deadline of a request,
srequest , simply is: srequest = tcur + rdeadline , where tcur is the current time at
the server, rdeadline is the relative deadline speciﬁed by the client.
3.2.2 Request Scheduling
To achieve time-sensitive scheduling of client requests, the server keeps an ordered queue of requests (RQ) for each data disk. The requests in RQ are scheduled using the Earliest Deadline First (EDF) algorithm [5] with the absolute
deadline, srequest , as the key.
Upon reception of a new request, the server checks the RQ for the disk. If
there is no request accessing the disk, then the server schedules the new request
by sending a RUN message to the client. Notice that the server schedules a
request for execution, but it is the Client-FCDFS that actually executes the
request. Thus the server is not on the path of data ﬂow between the disk and
the client host.
If there is already a request accessing the disk when a request arrives, the
server has two choices: (1) It can wait until the currently executing request
completes and then schedule the next request from among the pending requests
or (2) If the new request has a smaller deadline, it can preempt the currently
executing request and schedule the new one. These two scheduling disciplines
are called non-preemptive scheduling and preemptive scheduling.
Figure 3(a) shows how 2 requests from diﬀerent clients are scheduled by the
non-preemptive scheduling discipline. First, Client 1 sends a READ request.
Because the disk is idle, the server schedules it immediately, and sends a RUN
reply. Client 1 starts accessing the disk. While the disk read is in progress,

400

C. Akinlar and S. Mukherjee

Fig. 3. (a) Sequence of Events in non-preemtive scheduling, (b) Sequence of Events in
preemptive scheduling

Client 2 sends a READ request, which waits its turn until the currently executing request is done. When Client 1 is done reading the disk, it sends a
DONE message to the server. The server then schedules Client 2’s request and
sends a RUN reply. Client 2 reads the data from the disk, and sends a DONE
message back to the server when done. If Client 2’s request had a stricter
deadline than Client 1’s request, it might miss its deadline because it must
wait until the currently executing request is done.
Figure 3(b) shows how 2 requests from diﬀerent clients are scheduled by
the preemptive scheduling discipline. First, Client 1 sends a READ request.
Since the disk is idle, the server schedules it immediately, sends a RUN reply. Client 1 starts accessing the disk. While Client 1 is reading from the
disk, Client 2 sends a READ request. Because the request from Client 2 has
a stricter deadline than the currently executing request from Client 1, the
server preempts it by sending a PAUSE to Client 1 and a RUN to Client
2. From this point on, Client 2 starts reading the disk. When Client 2 is
DONE, it sends a DONE message back to the server. The server then reschedules Client 1’s request by sending a RUN reply. Client 1 then resumes the
execution of the request and ﬁnishes it. It sends a DONE message to the server
when done.
Since a FCDFS-block resides contiguously on a disk, the non-preemptive
policy ensures that disk head movement will be minimal during the service.
Therefore, the policy can yield high disk utilization. On the other hand, since
servicing a 1MB FCDFS-block may take a considerable amount of time (about
100ms for a disk with an average transfer rate of 10MB/s), the higher priority requests may get delayed when the disk is busy, which may cause deadline
misses.
3.2.3 Request Execution at the Client-FCDFS
Although requests are scheduled by the server, it is the Client-FCDFS that
executes them. To execute disk read/write requests, the Client-FCDFS runs a
kernel thread for each data disk, which runs the algorithm shown in ﬁgure 4.

A Bandwidth Sensitive Distributed Continuous Media File System

401

Fig. 4. Request execution at the Client-FCDFS

Instead of issuing a single request for an FCDFS block of size 1MB, ExecuteRequest function issues 1024 read requests, each for a 1KB portion of the
block. Therefore “r.blocks” in the algorithm is initialized to 1024 and saved properly in the case of a request preemption. ExecuteRequest does not issue each 1KB
request separately, rather it prepares groups of requests of size BatchSize and
gives this group of requests to the disk driver at once. Once BatchSize requests
are complete, the next group of requests are prepared. This process continues
until the whole block is read/written.
When the server employs the preemptive scheduling discipline, it sends a
PAUSE message to preempt a request. Upon reception of a PAUSE message,
ExecuteRequest saves the current state of the request and suspends the execution. The execution resumes when the server reschedules the request by sending
a RUN message. Notice that the execution of the current request will be preempted only after BatchSize disk requests are complete even if PAUSE message
arrives early. Therefore, the value of BatchSize determines how quickly (i.e., the
frequency) the Client-FCDFS can respond to preemption messages.

4

Numerical Results

To evaluate the bandwidth enforcement policies of our ﬁle system, we set up
an architecture consisting of 4 client hosts, a server and 8 disks connected to
a Fibre Channel Arbitrated Loop (FC AL) with a maximum transfer rate of
800Mbps. Client hosts and the server are connected with a switched 100Mbps
Ethernet network. The block size for the ﬁle system was ﬁxed at 1MB 1 . We
use a volume with 6 disks, and 18 users equally distributed across 4 client
hosts: 2 hosts run 5 and 2 hosts run 4. We then use the following workloads for
evaluation:
1

Optimal block size of 1MB was determined by experimentation.

402

C. Akinlar and S. Mukherjee

Fig. 5. Workload (a): equal bandwidth distribution, Workload (b): skewed bandwidth
distribution. Notice that with non-preemptive scheduling some clients get less than
what they negotiated for

(a) Equal bandwidth distribution: All 18 users ask for the same bandwidth
of 35Mbps for a total of 630Mbps.
(b) Skewed bandwidth distribution: 4 very fast users ask for 95Mbps and
14 slow users ask for 14 Mbps for a total of 576Mbps.
Figure 5 shows the results of the experiments. Each user is represented by
a vertical bar in the ﬁgures. The gray portion of the bar corresponds to the
requested bandwidth and is always 100 in the ﬁgures. The black portion of
the bar shows the percentage of the extra bandwidth received to the requested
× 100. The received
bandwidth and is simply computed by Received−Requested
Requested
bandwidth is overlayed on top of the requested bandwidth. If the client gets
at least what it requested, the black portion of the bar will be above the requested bandwidth of 100% and shows the extra bandwidth that the user has
received. (This is seen in Figure 5(a)). If however, the user gets less than what
it requested, the black portion will be negative and will be below the 100%
requested bandwidth overriding the gray portion of the bar (This is seen in Figure 5(b)). In the x-axis we show the actual bandwidth requested by the user in
Mbps.
First column of Figure 5(a) and (b) show the results for non-preemptive
scheduling: In (a), we see that the requested bandwidths are enforced. Since all
users are asking for the same bandwidth, the request deadlines will pretty be
similar. So the server will alternate between the requests from the users and
non-preemptive EDF scheduling enforces the requested bandwidths for all users.

A Bandwidth Sensitive Distributed Continuous Media File System

403

In (b) however, we observe that negotiated bandwidths are not enforced: The
slow users get a lot more than what they asked for, while the fast users get less
than what they asked for.
Observing that non-preemptive EDF scheduling does not always enforce
bandwidth, we evaluated the eﬀectiveness of preemptive scheduling by conducting the same set of experiments. We have assumed a BatchSize of 64 in these
experiments 2 . The second column of Figure 5(a) and (b) show the results for
each individual user. When all users ask for the same bandwidth, the bandwidths are enforced as in non-preemptive scheduling. When there are a mix of
slow and fast users, preemptive scheduling is still able to enforce user bandwidths as shown in Figure 5(b). In this experiment, we observed that 48% of
the requests are preempted by the server, which causes total disk bandwidth to
go down to 590Mbps instead of 640Mbps with no preemption. That’s the price
paid to enforce negotiated bandwidths of all users.

5

Concluding Remarks

In this paper we presented the architecture and bandwidth enforcement algorithms of our Continuous Media Fibre Channel Distributed File System (FCDFS).
Experimental results obtained from our prototype implementation of the ﬁle
system in Linux platform are presented to evaluate the eﬀectiveness of the
ﬁle system in enforcing the real-time bandwidth guarantees. We conclude that
the proposed ﬁle system is well-suited for emerging continuous media
applications.

References
1. D. P. Anderson, Y. Osawa, and R. Govindan. File System for Continuous Media.
ACM Transactions on Computer Systems, pages 311–337, November 1992.
2. T. E. Anderson, M. D. Dahlin, J. M. Neefe, D. A. Patterson, D. S. Roselli, and R. Y.
Wang. Serverless Network File Systems. ACM Transactions on Computer Systems,
February 1996.
3. M. Beck, H. Bohme, M. Dziadzka, U. Kunitz, R. Magnus, and D. Verworner. Linux
Kernel Internals. Addison-Wesley, 1998.
4. G. A. Gibson, D. F. Nagle, K. Amiri, J. Butler, F. W. Chang, H. Gobioﬀ, C. Hardin,
E. Riedel, D. Rochberg, and J. Zelenka. Filesystems for Network-Attached Secure
Disks. Technical Report CMU-CS-97-118, Carnegie Mellon University, July 1997.
5. K. Ramamritham and J. A. Stankovic. Scheduling Algorithms and Operating Systems Support for Real-Time Systems. Proceedings of the IEEE, 82(1), January 1994.
6. R. Sandberg, D. Goldberg, S. Kleiman, D. Walsh, and B. Lyon. Design and Implementation of the Sun Network File System. In Proceedings of the Summer USENIX
Conference, pages 119–130, 1985.
2

Optimal BatchSize of 64 was determined by experimentation.

404

C. Akinlar and S. Mukherjee

7. P. J. Shenoy, P. Goyal, S. S. Rao, and H. M. Vin. Symphony: An Integrated Multimedia File System. In ACM SIGMETRICS Conference on Modeling and Evaluation
of Computer Systems, 1998.
8. S. R. Soltis, G. M. Erickson, K. W. Preslan, M. T. O’Keefe, and T. M. Ruwart. The
Global File System: A File System for Shared Disk Storage. Submitted to the IEEE
Transactions on Parallel and Distributed Systems, 1997.
9. L. Zhang. Virtual Clock: A New Traﬃc Control Algorithm for Packet Switching
Networks. In Proceedings of SIGCOMM, 1990.

