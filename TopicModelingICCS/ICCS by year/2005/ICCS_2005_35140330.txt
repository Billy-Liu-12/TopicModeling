Statistical Methods for Automatic Performance
Bottleneck Detection in MPI Based Programs
Michael Kluge, Andreas Kn¨
upfer, and Wolfgang E. Nagel
Technische Universit¨
at Dresden, Dresden, Germany
{kluge, knuepfer, nagel}@zhr.tu-dresden.de

Abstract. With increasing number of processors the visualization of
trace data for MPI communication primitives becomes harder due to
many limitations. An automatic analysis of the recorded data should be
able to identify some clues for the optimization of the source code. The
approach presented with this paper is able to divide the communication
time for each communication relationship into a necessary part and the
overhead. We use statistical methods to deﬁne a time for each single
communication event that we will accept as the ’usual’ time for this
event. Additional runtime will be considered as overhead. At the end
of this article, the beneﬁts from this method are demonstrated on the
Sweep3D benchmark.

1

Introduction

2005, the supercomputer IBM ’Blue Gene’ will be installed with a processor
count of 131072. The ﬁrst desktop systems with more than a single processor
are already available for everyone. To use such systems eﬃciently parallel programs have to be developed that utilizes all processors. Writing parallel programs
today is commonly done by using one (or both) of the two popular programming paradigms, OpenMP [2] and MPI [4]. This work presents a new approach
for automatic bottleneck detection in MPI programs.
The ﬁrst section gives an overview on actual approaches in MPI program
analysis. The next sections are dedicated to the motivation and our statistical
approach for ﬁnding a maximal communication time. Then we will present some
tests for the practical relevance of these maximal communication times. At the
end of this paper we will show how a proﬁler using our approach can be used
for automatic performance bottleneck detection in the Sweep3D benchmark.

2

State of the Art in MPI Program Analysis

The question how well a program is scaling with an increasing amount of processors has driven the development of many tools (see [11], [10], [3] etc.). Recording
all program activities during runtime and an appropriate post-mortem analysis
of the recorded data is still state of the art in parallel program analysis. This is
V.S. Sunderam et al. (Eds.): ICCS 2005, LNCS 3514, pp. 330–337, 2005.
c Springer-Verlag Berlin Heidelberg 2005

Statistical Methods for Automatic Performance Bottleneck Detection

331

due to two main reasons. First of all, in the beginning of an optimization process
one does typically not know where to start. What is needed is a trace of all program activities. The outcome of this is at the same time the second reason for the
post-mortem analysis, a huge amount of data. The user himself will not be able
to do any analysis during runtime. An automated analysis will most probably
interfere with the program execution in a way that no meaningful conclusions
can be drawn from these data. Actually there exist several approaches for trace
ﬁle compression or removing of redundant information during the trace process
itself ([7],[1]) and during the post-mortem analysis ([8]).

3

Motivation

The aim of this article is to deﬁne a value that reﬂects the maximum execution
time for a MPI function call under ideal conditions. Within our approch we try
to determine, how the data within a sample generated with a repeated execution
of this function are distributed. If those data are distributed like a theoretical
distribution (like a Normal/Gaussian or Exponential distribution) it is possible
to deﬁne for example the time that is associated with 99% of this distribution as
the maximum time. First experiments have shown that the data we are looking
at are usually not normally distributed. Instead of this we have seen asymetric
distributions. As a result of this, a simple approach like calculating the mean µ
and variation σ 2 and afterwards using the time associated with µ + 3σ would
end up with a maximum execution time without meaning. Because a calculation
that µ + 3σ includes 99.73% of the distribution is only true for the assumption
of a Gaussian distribution.

4

The Statistical Approach

To be able to divide the time t needed for each MPI communication event into
a necessary and a wasted part we will deﬁne a time tmax as a maximum time
we want to accept for an execution without overhead. A communication event
is a call of an MPI function of the MPI 1.1 standard that transmits data or
completes one of the non-blocking communication calls [4–p. 41]. The time t
is the execution time a function call needs during the program execution. To
determine tmax we take a sample S = {t1 , . . . , tN } with the sample size N of
execution times for a communication event and deﬁne the wasted time toh as
follows:
t ≤ tmax → toh = 0
t > tmax → toh = t − tmax
We assume that a sample taken with a synthetic program represents the typical behavior of the underlying MPI implementation and hardware. The problem
that we have to face is that although each time ti within the sample is taken
by executing the same function with the same parameters those times are not

332

M. Kluge, A. Kn¨
upfer, and W.E. Nagel

identical. They are distributed in a way that diﬀers from machine to machine.
Each time ti can not be smaller than a minimum tmin that is determined by
the MPI implementation and the hardware. It is most likely that none of the
times ti will match tmin exactly. Instead of that each is superimposed with one
or more errors. If we assume an exclusive use of the communication network
we can consider that those errors result from the overlapping execution of processes, from the operation system or from the resolution of the timer used for
the measurement. If we assume further on that those processes are activated
periodically and each activation does only inﬂuence one measurement they can
be represented by a Poisson distribution. A sum of Poisson distributions is again
a Poisson distribution. If we presume t as
t = tmin + terr ,

(1)

we can model the distribution of the error terr as a Poisson distribution. During
our experiments we have seen that there are other statistical distributions that
sometimes ﬁt to the error within a sample better than a Poisson distribution.
The upper bound of the error that can be accepted can be ﬁxed by establishing a quantile qmax for the adapted theoretical distribution. With this quantile
(reasonable values are 0.8 ≤ qmax ≤ 1) we are now able to calculate a maximum
error time terr,max . If we assume that min(S) is close to tmin we can deﬁne
tmax as
(2)
tmax = min(S) + terr,max .

5

Used Models

In this evaluation we have used two theoretical distributions as the base for
the models. The ﬁrst one is the already mentioned Poisson distribution. The
probability density function
λk −λ
e , (k = 0, 1, . . .)
(3)
k!
with the parameter lambda is ﬁtted to the histogram of the error.
The second used model is the discretization of an originally continuous exponential distribution. We have seen in our experiments on diﬀerent computer
architectures that the type of the distribution diﬀers from architecture to architecture. The observed error is sometimes distributed in a way that is similar to
an exponential distribution. For the discretization of an exponential we will part
the continuous function
pk =

f (x) =

0 :
λe−λx :

x<0
x≥0

(4)

into windows of equal size.
By introducing a window size i in a way that
pk = F ((k + 1) ∗ i) − F (k ∗ i),
where k is the class number we get

(5)

Statistical Methods for Automatic Performance Bottleneck Detection

333

Fig. 1. Relationship between the continuous and discrete exponential distribution

pk = e−λki (1 − e−λi )

(6)

as a discrete exponential distribution. Some connections of the original and the
discretization can be taken from Figure 1.

6

Fitting the Model to the Data

To actually ﬁt a sample to a model we ﬁrst create a histogram from the sample.
To do this we need to sort the data into windows of a certain size. This window
size should not be smaller than the resolution of the timer used to get the sample.
Now we are able to use the maximum likelihood method
L(x1 , . . . , xn ; γ) = pf11 (γ) ∗ pf22 (γ) ∗ . . . ∗ pfrr (γ),

(7)

to ﬁt the model to the data. The result of this process is a set of parameters for
a model.

7

Quality of the Model Fitting

To determine how well a model ﬁts to the distribution the χ2 test is a way to
check the quality of the model ﬁtting in the case of discrete distributions. The χ2
test uses a weighted sum of the square of the diﬀerence between the theoretical
frequency and the practical frequency for each class.
k

χ
ˆ2 =
i=1

2

(Bi − Ei )
Ei

(8)

If the sum χ
ˆ2 does not exceed an upper limit, we accept the model ﬁtting. This
upper limit can be found in tables in adequate books (for example [12]) or can be

334

M. Kluge, A. Kn¨
upfer, and W.E. Nagel

approximated for those cases that use more than 30 classes. This approximation
is deﬁned as
3
2
2
+ zα
,
(9)
χ2ν ≈ ν 1 −
9ν
9ν
where ν are the degrees of freedom and zα depends on the selected level of
signiﬁcance and has to be taken from an adequate table. We always try to ﬁt
each model to the sample and use the model with the lowest χ
ˆ2 . If all models
are rejected, there are two choices: generation of a new sample or building a new
model.
To increase the quality of the ﬁtting we can use function (9) as the objective
function for an optimization
fi − pi (λ) .

min

λ∈R+

(10)

i

In this case we can use the parameters we got from the maximum likelihood
method as the start point for the optimization. The optimization itself will result
in slight changes of this parameters and a better ﬁtting of the model in the sense
of the χ2 test.

8

Creating a Complete Profile

One problem that arises while analyzing the MPI related behavior of diﬀerent
applications at diﬀerent numbers of processors is that there are a lot of varying
message sizes. It is not necessary to pay attention to each possible single message
size at each possible processor count for each MPI function if the assumptions is
made, that the application will always be analyzed with the same environment
(hardware, MPI library, environment variables) the model has been built in. If
this environment does not change during the model ﬁtting (and the generation
of the program traces later) we are able to deﬁne a discrete function
Tf : p, b → tmax

(11)

for each MPI function that uses the number of used processors (p) and the number of bytes (b) to deﬁne the time tmax for this pair (p, b). In order to be able
to evaluate Tf for each possible pair (p, b) we will deﬁne Tf for some base points
and interpolate all other points from these. To keep the error introduced by
this technique as low as possible the initial pairs (p, b) have to be chosen carefully. Within the BenchIt project [5] we have seen that for those communication
relations the following heuristic will work well for a start:
– Numbers of processors: each power of two and a number in between. (2n and
1.5 ∗ 2n with n = {1, 2, . . .})
– Message size: 1 Byte, 10, 20, . . . , 100, 200, . . . , 1000, and so on up to 4 MB
By using a bilinear interpolation between this base points we are now able to
evaluate Tf for each reasonable pair (p, b).

Statistical Methods for Automatic Performance Bottleneck Detection

9

335

Profile for a SGI Origin 3800

As a ﬁrst example, we present the measurements for a subset of what we have
proposed within the last section. We will use just a single processor count and a
single message size to create a proﬁle for a MPI Allreduce() with one integer and
eight processors on a SGI Origin 3800. For a ﬁrst try, we have taken a sample

Fig. 2. Runtimes for repeated execution of a MPI Allreduce() with one integer and
eight processors on a SGI Origin 3800

of size 3000. This sample is shown in Figure 2. To actually see how large the
sample really needs to be, we have ﬁtted the models to subsets of this sample.
The ﬁrst subset size we used is 50. This size is increased by 10 (up to 3000).
Each try produces a tmax which is plotted in Figure 3 against the subset size.

Fig. 3. Result of the model ﬁtting for the model with the Poisson distribution plotted
against the sample size

One conclusions from this example and our experiments at all [6] is that
sample sizes around 200 are already enough to ﬁt a model to the data. What we
have seen during the experiments is that the error χ
ˆ2 is sensitive to the sample
size and has always increased with it even when the result tmax is stable.

336

10

M. Kluge, A. Kn¨
upfer, and W.E. Nagel

Application to Sweep3D

As a more practical example for the usefulness we have implemented a proﬁler
that is able to trace each call of a MPI function back to a source code location.
This source code location is either a source ﬁle and the associated line number or
the name of the function that called this MPI function. By using the arguments
applied to the MPI function (mainly the message size s and the execution time
t) and the number of processors participating in a collective MPI function call
it is possible to ﬁnd the (p,b) pair for each call. Now we can evaluate Tf to
ﬁgure out how long this call should have taken at maximum. By adding up toh
for each source code location it is possible to automatically ﬁnd the source code
locations, where the application is wasting time during communication. These
position will be a good entry point for optimizing the communication behavior.
Table 1. Total communication time in seconds for Sweep3D for diﬀerent numbers of
processors
numbers of processors
4
8
12
14
21
24
28
communication time 26.79 55.03 73.78 109.42 129.35 124.40 166.10

As an practical example we have chosen the Sweep3D application [9] from
the ASCI benchmark collection. The communication time is mainly claimed by
point to point communication. In Table 2 we see the overhead for those two
lines in the source code in the source ﬁle msg_stuff.cpp that need most of the
communication time.
Table 2. Overhead in seconds caused by the two source code lines performing the point
to point communication within the ﬁle msg stuﬀ.cpp at diﬀerent number of processors
Numbers of processors
Line Function
4
8
12
14
21
24
28
311 MPI Send() 5.9508 11.7368 9.3950 12.5326 13.9212 10.0274 12.2072
364 MPI Recv() 18.0278 36.9847 54.6950 85.4923 98.2872 95.7525 129.9080

With this information and the knowledge about the total time spent during
communication (Table 1) we can conclude that there is a late sender/late receiver
problem dominating the communication time with the bigger fraction for the late
sender. By replacing the blocking MPI function calls with non-blocking calls and
one MPI Waitall() we were able to attenuate this problem.

11

Conclusion and Future Work

We have shown that it is possible to use statistical distributions to model the
overhead that occur during a repeated execution of a MPI function. By using

Statistical Methods for Automatic Performance Bottleneck Detection

337

a ﬁxed quantile of the distribution we are able to deﬁne the time each MPI
function call should not exceed. With this information we are able to ﬁnd the
overhead within communication relations. Mapping this overhead back to the
source code location gives the user clues about good entry points for program
optimization.
Future work will be the an automatic measurement for a complete MPI proﬁle
without the heuristics.

References
1. Dong H. Ahn and Jeﬀrey S. Vetter. Scalable analysis techniques for microprocessor
performance counter metrics. In Proceedings of Supercomputing 2002, pages 1–16,
2002.
2. OpenMP Architecture Review Board. OpenMP Application Program Interface,
Version 2.5. http://www.openmp.org/drupal/mp-documents/draft spec25.pdf,
November 2004.
3. Luiz DeRose and Felix Wolf. CATCH - A Call-Graph Based Automatic Tool for
Capture of Hardware Perfromance Metrics for MPI and OpenMP Applications. In
Euro-Par 2002, 2002.
4. Message Passing Interface Forum. MPI: A Message-Passing Interface Standard.
Technical report, University of Tennessee, 1995.
5. Guido Juckeland, Stefan B¨
orner, Michael Kluge, Sebastian K¨
olling, Wolfgang E.
Nagel, Stefan Pﬂ¨
uger, Heike R¨
odling, Stephan Seidl, Thomas William, and Robert
Wloch. BenchIt-Performance Measurement and Comparison for Scientiﬁc Applications. In G. R. Joubert et. al., Eds., Advances in Parallel Computing, Vol. 13,
pages 501–508. Elsevier, 2004.
6. Michael Kluge. Statistische Analyse von Programmspuren f¨
ur MPI-Programme.
Diploma thesis, November 2004.
7. Andreas Kn¨
upfer. A New Data Compression Technique for Event Based Program
Traces. In International Conference on Computational Science 2003, pages 956–
965, 2003.
8. Andreas Kn¨
upfer and Wolfgang E. Nagel. Compressible Memory Data Structures
for Event Based Trace Analysis. Future Generation Computer Systems by Elsevier,
2004.
9. Lawrence Livermore National Laboratory.
The ASCI Sweep3D Benchmark Code. http://www.llnl.gov/asci benchmarks/asci/limited/sweep3d/
asci sweep3d.html, 1995.
10. Tom LeBlanc and Wagner Meira. Measurement and Prediction of
Parallel
Program
Performance,
http://www.cs.rochester.edu/u/leblanc/
prediction.html.
http://www.cs.rochester.edu/u/leblanc/prediction.
html, 1997.
11. Wolfgang E. Nagel, Alfred Arnold, Michael Weber, Hans-Christian Hoppe, and
Karl Solchenbach. VAMPIR: Visualization and Analysis of MPI Resources. In
Supercomputer 63, Volume XII, Number 1, pages 69–80, 1996.
12. Lothar Sachs. Applied Statistics: A Handbook of Techniques. Springer, 11 edition,
2002.

