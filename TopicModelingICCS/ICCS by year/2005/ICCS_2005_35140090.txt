A High-Order Recursive Quadratic
Learning Algorithm
Qi Zhu1, , Shaohua Tan2 , and Ying Qiao3
1

3

University of Houston - Victoria, TX 77901
Tel.: +1-361-570-4312, Fax: +1-361-570-4207
zhuq@uhv.edu
2
National University of Singapore,
Singapore, 119260
Virginia Polytechnic Institute and State University,
Virginia, 24060

Abstract. A k-order Recursive Quadratic learning algorithm is proposed and its features are described in detail in this paper. Simulations
are carried out to illustrate the eﬃciency and eﬀectiveness of this new
algorithm by comparing the results with both the projection algorithm
and the conventional least squares algorithm.

1

Introduction

The Least Squares (LS) algorithm [3] and its adaptive version Recursive Least
Squares (RLS) algorithm [2] are well-known algorithms widely used in areas
such as Data Compression [8], Neural Network [5], Parameter identiﬁcation [10],
Pattern Recognition [4], Graphics [7], and Gene/DNA studies[9]. RLS is often used in linear-in-the-parameter (LIP) models. However, some undesirable
features of RLS algorithm are that the regression matrix should be of full
rank, convergence slows down considerably at certain low level error, and it
persists over a considerable number of iteration steps before dropping eventually below the prescribed error bound. This is due to the fact that at low
error level, RLS algorithm takes very small step sizes to ensure the
convergence.
This paper develops a high-order Recursive Quadratic (RQ for short) learning algorithm, initially proposed in [11], which avoids the problem of RLS for
identifying a general class of linear and nonlinear LIP models. RQ algorithm is
thoroughly investigated and we reveal its features as a high-order extension of
the Projection algorithm with a quadratic cost function. The convergence and
accuracy analysis of the algorithm is performed along with the simulations to
demonstrate various specialized properties of the algorithm.
Corresponding author.
V.S. Sunderam et al. (Eds.): ICCS 2005, LNCS 3514, pp. 90–98, 2005.
c Springer-Verlag Berlin Heidelberg 2005

A High-Order Recursive Quadratic Learning Algorithm

2
2.1

91

The RQ Algorithm
Preliminaries

The Linear-in-the-Parameters (LIP) is one of the most widely used model structures with the following general form [12]:
m

y(t) =

ξl (x(t))ωl = ϕT (x(t))θ

(1)

l=1

where {x(t), y(t)}(t > 0) is a set of sample data, θ = [ω1 , ..., ωl , ...]T is the
desired weight, and ϕ(x(t)) = [ξ1 (x(t)), ..., ξl (x(t)), ...]T is an m-vector of basis
functions.
We deﬁne a kt-dimensional matrix Λt to be the system forgetting factor:
Λt =

0k(t−1) 0
0
Λ(kt, k)

(2)

where Λ(kt, k) is a k-dimensional diagonal matrix Λ(kt, k) = diag[λ1 , ..., λk ] and
λi (i = 1, 2, ..., k) are some positive real scalars. The constant k is the order of
the algorithm.
2.2

RQ Learning Algorithm for LIP Models

For a particular k, we deﬁne the abbreviated notations of input matrix, output
vector, and output error vector as follows:
Φt = Φt (kt, k) = [ ϕt (x(k(t − 1) + 1)), ϕt (x(k(t − 1) + 2)), ..., ϕt (x(k(t−1)+k))] T
(3)
Yt = Yt (kt, k) = [ yt (k(t − 1) + 1), yt (k(t − 1) + 2), ..., yt (k(t − 1) + k) ]T
(4)
Et = Et (kt, k) = Yt (kt, k) − Φt (kt, k)θˆt−1
= [ et (k(t − 1) + 1), et (k(t − 1) + 2), ..., et (k(t − 1) + k) ]T

(5)

where et (kt) = yt (kt) − ϕTt (x(kt))θˆt−1 , subscript t denotes that the parameters
are estimated at time t. Introduce Jt to be the quadratic function:
1 T
1
E Λ(kt, k)Et = (Yt − Φt θˆt−1 )T Λ(kt, k)(Yt − Φt θˆt−1 )
2 t
2
1 T
1
T
= θˆt−1
ΦTt Λ(kt, k)Φt θˆt−1 − θˆt−1
ΦTt Λ(kt, k)Yt + YtT Λ(kt, k)Yt
2
2
1 T
1
T
= θˆt−1
Pt θˆt−1 − θˆt−1
Qt + Rt
(6)
2
2
where Λ(kt, k) is a k-dimensional identity matrix if we select λi = 1(∀i =
1, 2, ..., k), and
Jt = Jt (kt, k) =

Pt = Pt (kt, k) = ΦTt (kt, k)Λ(kt, k)Φt (kt, k) = ΦTt Φt
Qt = Qt (kt, k) = ΦTt (kt, k)Λ(kt, k)Yt (kt, k) = ΦTt Yt
Rt = Rt (kt, k) = YtT (kt, k)Λ(kt, k)Yt (kt, k) = YtT Yt

92

Q. Zhu, S. Tan, and Y. Qiao

Using the above notations we can introduce the k-order RQ algorithm as
follows:
θˆt = = θˆt−1 +

αJt (Qt − Pt θˆt−1 )
β + (Qt − Pt θˆt−1 )T (Qt − Pt θˆt−1 )

(7)

where t = 1, 2, ...; and β > 0 , 0 < α < 4.
Theorem 1. The algorithm (7) is obtained by solving the following optimization
problem: Given θˆt−1 and Yt , determine θˆt so that Jθˆ = 12 θˆt − θˆt−1 2 is minimized
subject to
(Yt − Φt θˆt−1 )T Yt = (Yt − Φt θˆt−1 )T ΦTt θˆt
(8)
Proof. Introducing a Lagrange multiplier λ for the constraint (8), we have the
augmented function as Jθˆ = 12 θˆt − θˆt−1 2 + 2λ[Yt − Φt θˆt−1 ]T [Yt − Φt θˆt ]. The
necessary conditions for an optimization are ∂Jθˆ/∂ θˆt = 0 and ∂Jθˆ/∂λ = 0,
which are
θˆt − θˆt−1 − 2λΦTt [Yt − Φt θˆt−1 ] = 0
[Yt − Φt θˆt−1 ]T [Yt − Φt θˆt ] = 0

(9)
(10)

From (9) we obtain θˆt = θˆt−1 + 2λΦTt [Yt − Φt θˆt−1 ], substituting into (10) gives
λ=

− Φt θˆt−1 ]T [Yt − Φt θˆt−1 ]
[Yt − Φt θˆt−1 ]T Φt ΦTt [Yt − Φt θˆt−1 ]
1
2 [Yt

(11)

And we have ΦTt (Yt − Φt θˆt−1 ) = ΦTt Et = Qt − Pt θˆt−1 , then substituting this as
well as (5),(6), and (11) into (9) gives
θˆt = θˆt−1 +

2Jt (Qt − Pt θˆt−1 )
(Qt − Pt θˆt−1 )T (Qt − Pt θˆt−1 )

To avoid division by zero, a small constant β is added to the denominator of
the above formula. In order to adjust the convergence rate of the algorithm,
we multiply a constant α to the numerator of the algorithm that erases the
constant 2. This leads to the slightly modiﬁed form (7) of the new high-order
RQ algorithm.
Figure 1 illustrates the geometric interpretation of the RQ algorithm with
parameter of two dimensions θˆ = (θ0 θ1 ) and the order k = 4. The parameters
θ0 and θ1 span a plane if they are linearly independent. The input matrix Φt =
{ϕt1 , ϕt2 , ϕt3 , ϕt4 } and each vector ϕ has two dimensions. Through Figure 1, we
know that θˆt is convergent to the desired parameter value θ using the shortest
path, and it eventually reaches θ , i.e., θˆt − θˆt−1 is minimized by RQ learning
algorithm.

A High-Order Recursive Quadratic Learning Algorithm

93

θ1

*
θ

ϕt1
ϕt3 ϕt2
ϕt4

θt
θ t-1

θ0

Fig. 1. Geometric interpretation of the Recursive Quadratic learning algorithm

2.3

Properties of RQ Learning Algorithm

Lemma 1 provides the convergent assessment of the RQ algorithm.
Lemma 1. For any given initial value θˆ0 , (7) has the following properties:
(i)
θˆt − θ

≤ θˆt−1 − θ

≤ · · · ≤ θˆ0 − θ

for any t ≥ 1.

(ii)
lim Jt = 0

t→∞

(iii)
limt→∞

θˆt − θˆt−s = 0 for any ﬁnite positive integer s.

Proof. The complete proof is given in [11].
In Lemma 2, we consider the k-order RQ algorithm in the presence of a
Gaussian noise.
Lemma 2. Modify the system model (1) into :
yt = ϕTt θ + wt

(12)

wt is a sequence of Gaussian noise, so that limt→∞ E[wt ] = 0 and limt→∞ E[wt2 ] =
σ 2 . Deﬁne the k-order noise vector Wt = [wk(t−1)+1 , wk(t−1)+2 , ..., wk(t−1)+k ]T ,
then the RQ algorithm in (7) has the following properties:
(i) The output error converges in the mean
lim E[et ] = 0

t→∞

94

Q. Zhu, S. Tan, and Y. Qiao

(ii)
lim E[eTt et ] = σ 2

t→∞

Proof. (i)
et = yt − ϕTt θˆt−1 = −ϕTt θ˜t−1 + wt
Then
lim E[et ] = lim E[−ϕt θ˜t−1 + wt ]

t→∞

t→∞

= − lim E[ϕt ] lim E[θ˜t−1 ] + lim E[wt ]
t→∞

t→∞

t→∞

As wt is white noise, limt→∞ E[wt ] = 0, with the parameter θˆt being unbiased
and the parameter error converging to zero, i.e. limt→∞ θ˜t−1 = limt→∞ (θˆt−1 −
θ ) = 0, then limt→∞ E[et ] = 0.
To prove (ii), we observe ﬁrst that
kt

e2i = EtT Et = (Yt − Φt θˆt−1 )T (Yt − Φt θˆt−1 )

i=k(t−1)+1

= (−Φt θ˜t−1 + Wt )T (−Φt θ˜t−1 + Wt )
= θ˜T ΦT Φt θ˜t−1 − θ˜T ΦT Wt − W T Φt θ˜t−1 + W T Wt
t−1

t

t−1

t

t

t

The covariance estimate of the output error is:
T
T
lim E[EtT Et ] = lim E[θ˜t−1
ΦTt Φt θ˜t−1 ] − lim E[θ˜t−1
ΦTt ] lim E[Wt ]

t→∞

t→∞

− lim

t→∞

t→∞

E[WtT ]

t→∞

lim E[Φt θ˜t−1 ] + lim E[WtT Wt ]

t→∞

t→∞

As the vector Wt is composed of a sequence of white noise wt , we can conclude
that limt→∞ E[Wt ] = 0k is a k-order zero vector, and limt→∞ E[WtT Wt ] = kσ 2 .
T
ΦTt Φt θ˜t−1 } is a scalar, we have
Since limt→∞ θ˜t−1 = 0, and limt→∞ {θ˜t−1
T
T
lim E[θ˜t−1
ΦTt Φt θ˜t−1 ] = lim E[tr{θ˜t−1
ΦTt Φt θ˜t−1 }]

t→∞

t→∞

T ˜
= lim E[tr{θ˜t−1
θt−1 ΦTt Φt }]
t→∞

T ˜
= lim tr{E[θ˜t−1
θt−1 ]E[ΦTt Φt }] = 0
t→∞

Therefore
lim E[EtT Et ] = lim

t→∞

t→∞

kt

E[e2i ] = kσ 2
i=k(t−1)+1

Finally
lim E[e2t ] = σ 2

t→∞

A High-Order Recursive Quadratic Learning Algorithm

95

Lemma 2 allows us to conclude that (7) converges under the white noisy data
as well.
Lemma 3. When k = 1, the new 1st-order RQ learning algorithm is equivalent
to the Projection algorithm in [1].
θˆt = θˆt−1 +

αφt
[yt − φTt θˆt−1 ]
β + φTt φt

(13)

Proof. When k = 1, the 1st-order RQ learning algorithm becomes:
θˆt = θˆt−1 +

α e2t ϕt
(yt − ϕTt θˆt−1 )
β + e2t ϕTt ϕt

(14)

where t > 0,β > 0, 0 < α = 12 α < 2, and et = yt − ϕTt θˆt−1 .
Since β can be chosen as any positive number for preventing the denominator
to be zero, selecting β = β/e2t , (14) can be interpreted in terms of (13). Thus,
the Projection algorithm is a special case of the new RQ learning algorithm when
we choose a set of speciﬁc parameters.

3

Simulations

In this section, we present an example 1 to assess the computational features of
the RQ algorithm. We provide the performance comparisons among the k-order
RQ learning algorithm, the projection algorithm, and the conventional Recursive
Least Squares algorithm as well as the data statistics.
Example. The moving average (MA) model
xt = 0.1(t − 1)
1

yt = 5 sin xt − 1 cos xt e xt +10 + 2 ln(xt + 10) + wt
where t = 1, 6, 11, ..., 101 and wt is a sequence of Gaussian noise with variance
0.01, m = 3 is the number of basis functions.
Figure 2 show the parameters convergence using 21 input and output data
for identifying the system for k = 15, where k is the order of the RQ learning
algorithm. With the choice of k, the number of multiplications (NM) is 3m2 +
4m+km2 +km+k+3 and the number of additions (NA) is m2 +m+m2 k+mk+k.
These results are also compared to the Projection algorithm (13) and the
RLS algorithm in [6]. For the Projection algorithm, the NM is 3m + 1 and NA
is 3m − 1 per iteration step. For RLS algorithm, the NM is 6m2 + 3m + 2 and
NA is 5m2 − m. We choose the same initial parameter value θˆ0 = [0, 0, 0]T and
the same error bound (3 × 10−4 ) for all the three algorithms.
1

Limit to space, we just show one example here. However, we have done many simulations for all kinds of LIP models

96

Q. Zhu, S. Tan, and Y. Qiao
k=15

parameter

5

4

Parameter Values

3

2

1

0

−1

−2

0

5

10

15
Iteration Steps

20

25

30

Fig. 2. When k = 15, RQ algorithm converges to 3 × 10−4 in 28 steps

The Number of Convergence steps

The Projection algorithm can reach the error bound with 2339 iteration steps,
its convergence rate is the slowest. The RLS algorithm converges faster than the
Projection algorithm with 1387 iteration steps. However, after the initial fast
convergence, the step length of the algorithm changes to be very small at low
error level to avoid the convergence to the wrong parameter values.
The k-order RQ learning algorithm, on the other hand, can reach the error
bound in only 28 iterations with k = 15. This shows that the speed of convergence
of the k-order RQ learning algorithm is much faster than both the RLS algorithm
and the Projection algorithm. Counting the total number of multiplications,
additions and CPU time needed for the convergence, the RQ algorithm are 6636,
5796 and 1.43 seconds respectively, which are much less than RLS algorithm
(90155 for NM, 58254 for NA and 22.57 seconds for CPU time) and Projection
algorithm (23390 for NM, 18712 for NA and 71.70 seconds for CPU time).
We also observe that the choice of the order k is very critical. As shown in
the Figure 3, if k is chosen to be large, then the convergence is fast but the

log
k=1

3.4

2.9
k=3

2.4
k=5

1.9

k=21

k=10

A

k=15

1.4
1.5

2.0

2.5

log

The NM in every step

Fig. 3. Relationship between the steps and NM (both axes are scaled logarithmically)

A High-Order Recursive Quadratic Learning Algorithm

97

computation at each iteration step is quite intensive. When k is too large, the
convergence slows down again, at the same time, the NM and NA are very high.
On the other hand, if k is chosen to be too small, then the computation is much
simpler at each step, but it has much slower convergence. In this example, k
around point A is the optimal choice, both the NM for each step and the total
number of convergence steps are low. Currently the choice of k is based largely
on intuitions rather than analytical means.

4

Conclusion

In this paper, we have developed a new high-order Recursive Quadratic (RQ)
learning algorithm for Linear-in-the-Parameter models. This new RQ learning
algorithm is derived as a high-order extension of the Projection algorithm with a
new form of quadratic cost function. RQ algorithm is thoroughly described along
with complete investigations to reveal its various features. The convergence and
accuracy analysis of the algorithm is performed along with the simulations to
demonstrate various specialized properties of the RQ algorithm.
We only developed the high-order RQ learning algorithm by choosing a speciﬁc Λt in this paper. One future research is to explore the learning algorithm by
choosing diﬀerent kinds of matrix Λt . In our research, choosing an appropriate
order k is very critical. The larger the order, the faster the convergent speed
and the more complex the computation at very iteration step. Hence, in future
research it is very important that how can we choose a proper order k such
that the convergent speed is reasonably fast, and yet the computation at every
iteration step is reasonably simple in practice. Moreover, we can also extend the
RQ learning algorithm for the purpose of on-line identiﬁcation.

References
1. K.J. Astrom and B. Wittenmark, “Adaptive Control.” Addison-Wesley, Mass.,
1989.
2. Y. Boutalis, C. Papaodysseus, E. Koukoutsis, “A New Multichannel Recursive
Least Squares Algorithm for Very Robust and Eﬃcient Adaptive Filtering.” Journal of Algorithms. Vol. 37, No.2, pp. 283-308, 2000.
3. L. E. Ghaoui and H. Lebret, “Robust Least Squares and Applications.” In Proc.
CDC, pp. 249-254, Kobe, Japan, 1996.
4. S. Ghosal, R. Udupa, N. K. Ratha, S. Pankanti, “Hierarchical Partitioned Least
Squares Filter-Bank for Fingerprint Enhancement.” In Proc. ICPR. pp. 3338-3341,
2000.
5. A. P. Grinko, M. M. Karpuk, “Modiﬁcation of Method of Least Squares for Tutoring Neural Networks.” In Proc. Intelligent Information Systems. pp. 157-164,
2002.
6. T. C. Hsia, “System Identiﬁcation: Least-Squares methods.” Lexington Books,
1977.
7. H. A. Lensch, J. Kautz, M. Goesele, W. Heidrich, H. Seidel, “Image-based Reconstruction of Spatial Appearance and Geometric Detail.” ACM Transactions on
Graphics. Vol. 22, No. 2, pp: 234 - 257, 2003.

98

Q. Zhu, S. Tan, and Y. Qiao

8. B. Meyer, P. E. Tischer, “Glicbawls - Grey Level Image Compression by Adaptive
Weighted Least Squares.” Data Compression Conference. pp. 503, 2001.
9. D. V. Nguyen, D. M. Rocke, “Multi-class Cancer Classiﬁcation Via Partial Least
Squares with Gene Expression Proﬁles. ” Bioinformatics. Vol. 18, No.9, pp. 12161226, 2002.
10. B. D. Saunders, “Black Box Methods for Least Squares Problems.” In Proc. ISSAC.
pp. 297-302, 2001.
11. S. Tan, X. H. Zhang and Q. Zhu, “On a new kth-order Quadratic Learning Algorithm.” IEEE Trans. Circ. and Syst. Vol. 44, No. 1, pp. 186-190, 1997 .
12. W. X. Zheng, “Modiﬁed Least-squares Identiﬁcation of Linear Systems with Noisy
Input and Output Observations.” In Proc. CDC, pp. 1067-1068, Kobe, Japan,
1996.

