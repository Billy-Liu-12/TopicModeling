Simulated Annealing Based-GA Using Injective
Contrast Functions for BSS
J.M. G´
orriz, C.G. Puntonet, J.D. Morales, and J.J. delaRosa
Facultad de Ciencias, Universidad de Granada,
Fuentenueva s/n, 18071 Granada, Spain
gorriz@ugr.es

Abstract. In this paper we present a novel GA-ICA method which converges to the optimum. The new method for blindly separating unobservable independent component signals from their linear mixtures (Blind
Source Separation BSS), uses genetic algorithms (GA) to ﬁnd the separation matrices which minimize a cumulant based contrast function. The
paper also include a formal prove on the convergence of the proposed algorithm using guiding operators, a new concept in the genetic algorithms
scenario. This approach is very useful in many ﬁelds such as biomedical
applications i.e. EEG which usually use a high number of input signals.
The Guiding GA (GGA) presented in this work converges to uniform
populations containing just one individual, the optimum.

1

Introduction

The starting point in the Independent Component Analysis (ICA) research can
be found in [1] where a principle of redundancy reduction as a coding strategy in
neurons was suggested, i.e. each neural unit was supposed to encode statistically
independent features over a set of inputs. But it was in the 90´s when Bell
and Sejnowski applied this theoretical concept to the blindly separation of the
mixed sources (BSS) using a well known stochastic gradient learning rule [2]
and originating a productive period of research in this area [3]. In this way ICA
algorithms have been applied successfully to several ﬁelds such as biomedicine,
speech, sonar and radar, signal processing, etc. and more recently also to time
series forecasting [4], i.e. using stock data.
In general, any abstract task to be accomplished can be viewed as a search
through a space of potential solutions and whenever we work with large spaces,
GAs are suitable artiﬁcial intelligence techniques for developing this optimization
[4]. Such search requires balancing two goals: exploiting the best solutions and
exploring the whole search space. In this work we prove how GA-ICA algorithms
converge to the optimum. They work eﬃciently in the search of the separation
matrix (i.e. EEG and scenarios with the BSS problem in higher dimension)
proving the convergence to the optimum. We organize the essay as follows. In
section 2 and 3 we give a brief overview of the basic ICA and GA theory. Then
we introduce a set of genetic operators in sections 3 and 4 and prove convergence.
Finally state some conclusions in section 6.
V.S. Sunderam et al. (Eds.): ICCS 2005, LNCS 3514, pp. 585–592, 2005.
c Springer-Verlag Berlin Heidelberg 2005

586

2

J.M. G´
orriz et al.

ICA and Statistical Independence Criterion

We deﬁne ICA using a statistical latent variables model (Jutten & Herault, 1991).
Assuming the number of sources n is equal to the number of mixtures, the linear
model can be expressed, using vector-matrix notation and deﬁning a time series
vector x = (x1 , . . . , xn )T , s, ˜s and the matrix A = {aij } and B = {bij } as:
˜s = Bx = BAs = Gs

(1)

where we deﬁne G as the overall transfer matrix. The estimated original sources
will be, under some conditions included in Darmois-Skitovich theorem [5], a
permuted and scaled version of the original ones. The statistical independence
of a set of random variables can be described in terms of their joint and individual
probability distribution. This is equivalent to [6]:
βλ βλ∗∗ Γλ,λ∗

Π=

˜
|λ| + |λ∗ | < λ

(2)

{λ,λ∗ }

where the expression deﬁnes a summation of cross cumulants [6] and is used
as a ﬁtness function in the GA. The latter function satisﬁes the deﬁnition of
a contrast function Ψ deﬁned in [7] as can be seen in the following generalized
proposition given in [8].
Proposition 1. The criterion of statistical independence based on cumulants
deﬁnes a contrast function Ψ given by:
ψ(G) = Π − log|det(G)| − h(s)

(3)

where h(s) is the entropy of the sources and G is the overall transfer matrix.
Prove: To prove this proposition see Appendix A in [8] and apply the multilinear property of the cumulants.

3

Genetic Algorithms: A Theoretical Background

Let C the set of all possible creatures in a given world and a function f : C → R+ ,
namely ﬁtness function. Let Ξ : C → VC a bijection from the creature space onto
the free vector space over A , where A = {a(i), 0 ≤ i ≤ a − 1} is the alphabet
which can be identiﬁed by V1 the free vector space over A. Then we can establish
VC = ⊗λ=1 V1 and deﬁne the free vector space over populations VP = ⊗N
σ=1 VC
with dimension L = · N and aL elements. Finally let S ⊂ VP be the set of
probability distributions over PN , that is the state which identiﬁes populations
with their probability value.
Deﬁnition 1. Let S ⊂ VP , n, k ∈ N and {Pc , Pm } a variation schedule. A Genetic Algorithm is a product of stochastic matrices (mutation, selection, crossover,
etc..) act by matrix multiplication from the left:
Gn = Fn · CkPnc · MPnm

(4)

Simulated Annealing Based-GA Using Injective Contrast Functions for BSS

587

where Fn is the selection operator, CkPnc = C(K, Pc ) is the simple crossover
operator and MPnm is the local mutation operator (see [6] and [10])
In order to improve the convergence speed of the algorithm we could include
another mechanisms such as elitist strategy (a further discussion about reduction
operators, can be found in [11]). Another possibility is:

4

Guided GAs

In order to include statistical information into the algorithm we deﬁne an hybrid statistical genetic operator as follows. The value of the probability to go
from individual pi to qi depends on contrast functions (i.e. based on cumulants)
(qi )
; pi , qi ∈ C where ℵ(Tn )
as: P (ξn+1 = pi |ξn = qi ) = ℵ(T1 n ) exp − Ψ (pi )+Ψ
Tn
is the normalization constant depending on iteration n; temperature follows a
variation decreasing schedule, that is Tn+1 < Tn converging to zero, and Ψ (qi )
is the value of the selected contrast function over the individual (an encoded
separation matrix). This sampling (Simulated Annealing -SA- law) is applied to
the population and oﬀspring emerging from the canonical genetic procedure.
Proposition 2. The guiding operator can be described using its associated transition probability function (t.p.f.) by column stochastic matrices Mn
G, n ∈ N
acting on populations.
1. The components are determined as follows: Let p and q ∈ ℘N , then we have
q, Mn
Gp

N!
=
z0q !z1q ! . . . zaL −1q !

aL −1

{P (i)}zi q ;

p, q ∈ PN

(5)

i=0

where ziq is the number of occurrences of individual i on population q and
P (i) is the probability of producing individual i from population p given above.
The value of the guiding probability P (i) = P (i, Ψ ) depends on the ﬁtness
function used:1 P (i) =

Ψ (pi )+Ψ (qi )
Tn
Ψ (pi )+Ψ (qi )
zip exp −
Tn

zip exp −
aL −1
i=0

n
n
2. For every permutation π ∈ ΠN , we have πMn
G = MG = MG π.
n
3. MG is an identity map on U in the optimum, that is p, Mn
G p = 1; and
p
>
0
∀p
∈
P
.
has strictly positive diagonals since p, Mn
N
G
4. All the coeﬃcients of a GA consisting of the product of stochastic matrices:
the simple crossover CkPc , the local multiple mutation Mn
Pm and the guiding
for
all
n,
k
∈
N
are
uniformly
bounded
away
from 0.
operator Mn
G
1

The condition that must be satisﬁed the transition probability matrix P (i, f ) is that
it must converge to a positive constant as n → ∞ (since we can always deﬁne a suitable normalization constant). The ﬁtness function or selection method of individuals
used in it must be injective.

588

J.M. G´
orriz et al.

Proof: (1) follows from the transition probability between states. (2) is obvious and (3) follows from [7] and checking how matrices act on populations. (4)
follows from the fact that Mn
Pm is fully positive acting on any stochastic matrix S.
It can be viewed as a suitable ﬁtness selection and as a certain Reduction Operator, since it preserves the best individuals into the next generation using a non
heuristic rule, unlike the majority of GAs used.
The convergence and strong and weak ergodicity of the proposed algorithm
can be proved using several ways. A MC modelling a CGA has been proved
to be strongly ergodic (hence weak ergodic, see [10]). So we have to focus our
attention on the transition probability matrix that emerges when we apply the
guiding operator. We can write the overall process as:
q, Gn p =
v∈℘N

n
q, Mn
G v v, C p

(6)

where Cn is the stochastic matrix associated to the CGA and Mn
G is given by
equation 5.
Proposition 3. Weak Ergodicity
A MC with transition probability function associated to guiding operators that
converges to uniform populations (populations with the same individual) satisﬁes
weak ergodicity.
Prove: If we deﬁne a GGA on CGAs, the ergodicity properties depends on the
new deﬁned operator since they satisfy them as we said before. To prove this
proposition we just have to check the convergence of the t.p.f. of the guiding
operator on uniform populations. If the following condition is satisﬁed:
u, Gn p → 1

u∈U

(7)

Then we can ﬁnd a series of numbers which satisﬁes:
∞
n=1

min( u, Gn p ) = ∞ ≤
n,p

∞

min
n=1

q,p

v∈℘N

n
min ( v, Mn
G p v, C q )

(8)

which is equivalent to weak ergodicity [9].
Proposition 4. Strong Ergodicity
k
describe a model for crossover
Let Mn
Pm describe multiple local mutation, CPn
c
n
n
and F describe the ﬁtness selection. Let (Pm , Pcn )n ∈ N be a variation schedule
and (φn )n∈N a ﬁtness scaling sequence associated to Mn
G describing the guiding
k
represent the ﬁrst
operator according to this scaling. 2 Let Cn = Fn · Mn
Pm · CPn
c
n steps of a CGA. In this situation,
2

A scaling sequence φn : (R+ )N → (R+ )N is a sequence of functions connected
with a injective ﬁtness criterion f as fn (p) = φn (f (p)) p ∈ ℘N such that M∞
G =
limn→∞ Mn
G exist.

Simulated Annealing Based-GA Using Injective Contrast Functions for BSS
∞ n
v∞ = lim Gn v0 = lim (M∞
G C ) v0
n→∞

n→∞

589

(9)

exists and is independent of the choice of v0 , the initial probability distribution. Furthermore, the coeﬃcients v∞ , p of the limit probability distribution
are strictly positive for every population p ∈ ℘N .
Prove: The demonstration of this proposition is rather obvious using the results
of Theorem 16 in [10] and the point 4 in Proposition 2. In order to obtain the
results of the latter theorem we only have to replace the canonical selection operator Fn with our guiding selection operator Mn
G which has the same essential
properties.
Proposition 5. Convergence to the Optimum
Under the same conditions of propositions 3, 4 the GGA algorithm converges to
the optimum.
Prove: To reach this result, one has to prove that the probability to go from any
uniform population to the population containing only the optimum is equal to 1
when n → ∞:
(10)
lim p∗ , Gn u = 1
n→∞

since the GGA is an strongly ergodic MC hence any population tends to uniform
in time. If we check this expression we ﬁnally have the equation 10. In addition we
have to use point 3 in Proposition 2 to make sure the optimum is the convergence
point. Thus any guiding operator following a simulated annealing law converges
to the optimum uniform population in time.

5

Simulations

At the ﬁrst step, we compare the previous canonical method for apply GAs to
ICA [12] with the GGA version for a reduced input space dimension (n = 3).
The Computer used in these simulations was a PC 2 GHz, 256 MB RAN in
the case of a low number of signals and the software used is an extension of
ICATOOLBOX2.0 in MatLab code, protected by the Spanish law N◦ CA-235/04.
We test these two algorithms for a set of independent signals plotted in ﬁgure 2(a)
using 50 randomly chosen mixing matrices (50 runs); i.e. using the mixing matrix:
B = {1.0000, −0.9500, 0.5700; −0.5800, 1.0000, 0.0900; 0.6300, −0.0100, 1.0000},
we get the signals shown in ﬁgure 2(b). We have chosen two super-gaussian
signals and one bimodal signal for the ﬁrst attempt (ni nps = 3). The order of the
statistics used is the same in both methods (cumulants of 4th order)3 and the size
3

Based on section 2, we can deﬁne the ﬁtness function approach for BSS as:
stimes

||Cum(yi , yj , . . .)||

f (po ) =

∀i, j, . . . ∈ [1, . . . , n]

(11)

i,j,...

where po is the parameter vector (individual) containing the separation matrix and
|| . . . || denotes the absolute value.

590

J.M. G´
orriz et al.

(a) Original signals

(b) Mixed signals

Fig. 1. Set of independent series used in the comparison GA-GGA and a mixed case
Table 1. Mean and deviation of the parameters in the separation over 50 runs for
the cost function of 4th order by the GA-method, GGA method and the FASTICA
method. 1st row GA-ICA, 2nd row GGA-ICA and 3th row FATICA
param
a11
a12
a13
a21
a22
a23
a31
a32
a33
mean -0.2562 0.1473 -0.1657 -0.0647 -0.1393 0.2475 -0.4910 0.0998 -0.0350
dev.(%) ≤ 5
≤5
≤5
≤5
≤5
≤5
≤5
≤5
≤5
mean -0.1481 0.1647 -0.2564 0.1401 -0.2464 -0.0649 -0.1003 0.0345 -0.4914
dev.(%) ≤ 6.5 ≤ 6.5 ≤ 6.5 ≤ 6.5 ≤ 6.5 ≤ 6.5 ≤ 6.5 ≤ 6.5 ≤ 6.5
mean 0.0756 -0.1099 0.2271 -0.0715 0.1648 0.0659 0.0512 -0.0226 0.4435
dev.(%) ≤ 10
≤ 10
≤ 10
≤ 10
≤ 10
≤ 10
≤ 10
≤ 10
≤ 10

of population was 100. In this way we can compare the search eﬃciency of both
methods. Later we will focus our attention with a third statistical algorithm for
ICA, the well-known FastICA [3]. This method uses the same level of information
in its contrast function (4th order) thus the comparison is signiﬁcant.
Results obtained from simulations are conclusive. We ﬁnd out how the number of iterations (CPU time) needed to reach convergence is higher using the
proposed method in [12]. This is due to blind search strategy used in the latter reference unlike the guided strategy proposed in this paper. We measure
convergence by means of the well-known methods: Crosstalk (between original
and recovered signals) and Normalized Round Mean Square Error (NRMSE).
The set of recovering signals using GGA method can be found in ﬁgure 2(b) . In
the case of the three method comparison we observe how the eﬃciency of the
FastICA in low dimension is better than the genetic approaches (see ﬁgure 2
somehow the standard deviation in time and error measure in higher than the
genetic methods (see tables 1 and 2) since it suﬀers the local minima eﬀect. As
is shown in the latter tables the genetic procedures are slower but ﬁnally reach
a better solution (the new proposed method is faster than the method in [12]).

Simulated Annealing Based-GA Using Injective Contrast Functions for BSS

(a) Schematic Representation of the
Separation System in ICA-GA

591

(b) Comparison for number of inputs equal to 3 GGA (red) ,GA
(light blue) and FastICA (blue).
Observe how GA methods obtain
the same level of recovery but the
time efficiency is quite different

Fig. 2. Schematic representation and comparison of the 3 method
Table 2. Mean and deviation of the parameters in the separation over 50 runs for the
cost function of 4th order by the GA-method, GGA method and the FASTICA method
(cont)
Method

param. Comp. Time(s)
mean
10.21
GA-ICA
dev.(%)
≤2
mean
3.3
GGA-ICA
dev.(%)
≤2
mean
1.64
FastICA
dev.(%)
≤5

NRMSE Crosstalk(dB)
1.5635−4
-34.709
≤1
≤1
1.5408−4
-37.7507
≤1
≤1
1.6355−4
-29.663
≤2
≤4

Finally, we checked the performance of the proposed hybrid algorithm in a
high dimensional scenario [6]. The results for the crosstalk were conclusive: FASTICA convergence rate decreases as dimension increases whereas GA approaches
work eﬃciently. Of course we used the number of starting points equal to the
number of individuals in the genetic generation.

6

Conclusions

A GGA-based BSS method has been developed to solve BSS problem from the
linear mixtures of independent sources. The proposed method obtain a good performance overcoming the local minima problem over multidimensional domains.

592

J.M. G´
orriz et al.

Extensive simulation results prove the ability of the proposed method. This is
particular useful in some medical applications where input space dimension increases and in real time applications where reaching fast convergence rates is
the major objective. In this work we have focussed our attention to linear mixtures. The nonlinear problem can be interpreted as a piece-wise linear model
and is expected that results improve even more since the higher parameters to
encode the better results we obtain. GAs are the best strategies in high dimensional domains so it would be interesting how these algorithms (non CGAs) face
the nonlinear ICA. The experimental work on this part is on the way. In the
theoretical section we have prove the convergence of the proposed algorithm to
the optimum unlike the ICA algorithms which usually suﬀer of local minima
and non-convergent cases. Any injective contrast function can be used to build
a guiding operator, as a elitist strategy i.e. the Simulated Annealing function
deﬁned in section 4. The convergence is shown under little restrictive conditions
for the guiding operator: its eﬀect must disappear in time like the simulated
annealing.

References
1. Barlow, H.B, Possible principles underlying transformation of Sensory messages.
Sensory Communication, MIT Press, New York, (1961).
2. Bell,A.J. et al. An Information-Maximization Approach to BSS and Blind Deconvolution. Neural Comp., 7, 1129-1159 (1995).
3. Hyv¨
arinen, A. et al. A fast ﬁxed point algorithm for ICA Neural Comp., 9: 14831492
4. G´
orriz, J.M. et al. New Model for Time Series Forecasting using rbfs and Exogenous
Data. Neural Comp. and Appl., 13/2 (2004)
5. Cao, X.R. et al. General Approach to BSS. IEEE Trans. on Signal Proc., 44/3,
562-571 (1996)
6. G´
orriz J.M. et al. Hybridizing GAs with ICA in Higher dimension LNCS 3195,414421, (2004)
7. Comon, P., ICA, a new concept? Signal Proc. 36 (1994) 287-314
8. Cruces, S. et al. Robust BSS algorithms using cumulants. Neurocomp. 49 (2002)
87-118
9. Isaacson, D.L. et al. MCs: Theory and Appli., Wiley, 1985.
10. Schmitt, L.M. et al. Linear Analysis of GAs, Theoretical Computer Science, 200,
pp 101-134, 1998.
11. Rudolph, G., Convergence Analysis of CGAs, IEEE Trans. on NN, 5/1,(1994)
96-101.
12. Tan, Y. et al. Nonlinear BSS Using HOS and a GA. IEEE Trans. on Evol. Comp.,
5/6 (2001)

