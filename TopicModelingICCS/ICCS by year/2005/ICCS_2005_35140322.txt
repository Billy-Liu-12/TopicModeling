“gRpas”, a Tool for Performance Testing and
Analysis
Laurentiu Cucos and Elise de Doncker
Western Michigan University
{lcucos, elise}@cs.wmich.edu
http://aegis.cs.wmich.edu/∼lcucos,
http://www.cs.wmich.edu/∼elise

Abstract. This paper presents “gRpas”, a tool written in Java and
designed to help analyzing test results from scientiﬁc computing applications. “gRpas” stands for “gather Results / plot, analyze, and store”.
As one of its main features, the tool is easy to interface with the user
program. Furthermore it provides for one click data ﬁltering and plot
generation, eﬀective graphical display of program output, and statistical
report generation on algorithm results and performance. gRpas also has
built-in functionality for comparison testing between two or more algorithms or algorithm versions. We will present examples of its use with
parallel multivariate integration routines. However, its target applications cover a wide class of scientiﬁc computing programs.

1

Introduction

Software applications in general and scientiﬁc computing programs in particular
can be seen as multidimensional functions: f : Rn → Rm . A program can take
an n-variate input, and generate an m-variate output. To analyze the program
behavior, the investigator selects a set of input values and compares the output
with results obtained using a diﬀerent method. The complexity of evaluating the
program arises through a number of factors including: the problem to be solved
and its parameters, the algorithm used and its parameters, random factors, total
number of processors involved, etc.
For a given problem, some algorithms perform better than others, while for
a given algorithm some problems will be solved more eﬃciently. Eﬃciency related results are generally aﬀected by the problem parameters and the algorithm
parameters.
The random factor is related to the fact that some algorithms require a set
of random values. Ideally, the outcome must not depend on the random values;
however this is not always the case. By repeating the same test a number of
times, the developer is able to see if the output of the algorithm tends to the
same value.
The number of processors involved can complicate the analysis. On the one
hand, for each processor, the developer must monitor a number of eﬃciency
V.S. Sunderam et al. (Eds.): ICCS 2005, LNCS 3514, pp. 322–329, 2005.
c Springer-Verlag Berlin Heidelberg 2005

“gRpas”, a Tool for Performance Testing and Analysis

323

related parameters; on the other hand, a disturbance of the distributed environment (such as in sharing bandwidth with another program) can possibly aﬀect
the outcome, introducing a random factor.
For those problems for which there are no proved solutions (either because
they are too hard and require approximations - as for NP complete problems,
or because they involve too many extra conditions - as the parallel processing,
adaptive numerical integration, etc.), the development cycle often iterates as
follows:
a) ﬁnd a new algorithm (or start from a current version);
b) perform a set of tests;
c) based on results analysis improve the algorithm.
For the later stages, some common questions are: How much testing must be
done to obtain signiﬁcant results? Is the new algorithm better and how much
better? Despite the fact that there are no practical limits on the amount of data
that can be collected from the tests, it is desired to perform as few tests as
possible (to save CPU cycles, or decrease development time).
Tests are usually automated by varying one input parameter while keeping
the others constant. If, for the ith parameter, ki diﬀerent values are tested, then
a problem with n input parameters and m output values generates m 1≤i≤n ki
numbers to analyze.
Often, to evaluate improvements, the same set of tests are performed for
diﬀerent versions. In [1], Hooker mentions two types of algorithm comparisons:
(i) competitive testing, and (ii) controlled experimentation. Competitive testing
is more suitable for development, and tells which algorithm is faster but not why,
whereas controlled experimentation is suitable for research and gives insight on
how the code behaves under certain conditions. We can relate this classiﬁcation
with the following: (i) compare results from two or more versions. (ii) compare
results from the same algorithm version;
For comparing algorithms in (i), it is indicated to examine the diﬀerence
in result vectors from diﬀerent algorithms for corresponding input parameters.
There is often a trade-oﬀ between the amount of work done and the quality of the
result. Algorithm A may be pessimistic and B more optimistic, so that requesting
less accuracy from A may lead to results equivalent to B’s. The performance
proﬁles technique of [4] accounts for this characteristic in the comparison of A
and B.
In (ii), the algorithm may be tested with respect to the inﬂuence of random
factors, either due to the environment or a random component in the algorithm
(such as through the use of random numbers). The diﬀerence between the (sample) result vectors in m-space is signiﬁcant to measure performance dependence
on the random factor(s). Their distance can be accounted for by using the mean
and variance with respect to some or all of the output components.
Performance measures to test algorithm behavior as a function of varying
input parameters are problem related. For example, “result” and “estimated

324

L. Cucos and E. de Doncker

error” output parameters can be analyzed as a function of input “tolerated
error” by comparing the actual error (if known) and estimated error to the
level tolerated. The situation is complicated by the fact that the dependence
on diﬀerent input parameters may be correlated, such as that of “actual error”
on “tolerated error” and “allowed number of iterations”. As another example,
various scalability measures (as a function of the number of processors used) are
discussed in [5].
Another issue to take into account is the stability of the algorithm. An unstable algorithm may cause a large change in results for a small change in the input
parameters, or, magnify the small errors from earlier computation stages until
the result deviates completely from the true answer. Examples
√ can be found in
recurrence relations, like in computing the power of φ = − 12 ( 5 + 1) using the
recursive formula φn+1 = φn−1 − φn . Solving Fredholm equations of the ﬁrst
kind also belongs to this category [6].
In this paper we introduce the gRpas tool, designed to help analyze test
results from scientiﬁc computing applications. The idea was originally motivated
by applications in numerical integration. Section 2 below outlines the numerical
integration framework that lead to the development of gRpas. Section 3 presents
the tool’s main characteristics, and Section 4 concludes the paper.

2

Testing Numerical Integration Algorithms

The two main ingredients of a numerical integration problem are: I(α) the integral, and M (β) the integration method, where α and β are various parameters.
We denote by Ij a problem I(αj ) and Mk a method M (βk ). We consider the integrand function, integration region, requested accuracy and maximum number
of function evaluations as part of the integration parameters α. A few examples
of integration methods are: adaptive, Monte Carlo and quasi-Monte Carlo methods. Various cubature rules that can be used at the basis of adaptive multivariate
integration methods are listed in [2].
A major issue in numerical integration is that often we don’t know the characteristics of the integrand function. In general, we don’t have answers to questions
like: are there any singularities, how steep are they and where are they located?
We denote with Mjo the best algorithm that solves Ij , that is, the algorithm
with the best convergence. Let C(I, M ) be the cost of solving I using algorithm
M , expressed in an appropriate measurement unit (e.g., number of function
evaluations performed).
Given M , it is our goal to develop a set of tests to:
a) ﬁnd βk such that on the average over all Ij , C(I, M ) is minimum. - Since it is
impossible to handle all Ij , we need to determine a suitable test set for which we
require that on the average C(I, M ) is minimized. Alternatively we may require
that M (βk ) = Mjo for Ij in the test set. Or, we may just want to extract as
much information as possible about the algorithm behavior.
b) compare Mk with other algorithms for the entire test space.

“gRpas”, a Tool for Performance Testing and Analysis

3

325

gRpas Tool

gRpas [7] is a small application designed to help analyze results from scientiﬁc
applications. Its main features are listed bellow.
3.1

Flexible Application Interface

For inter-operability and ﬂexibility, all data is stored in an XML like format. For
each input/output parameter, the user speciﬁes a set of attributes: name, type,
and tag. These are stored in a type repository and are used by the GUI module
and re-used for similar tests. All results, along with input parameters, are stored
in a tagged format.
3.2

Option for Filtering or Combining Results

Tests are performed in groups. Usually a set of tests is performed, analyzed and
if there are satisfactory results, another group of tests is performed. If the results
are not as expected, the code is changed and the same set of tests is repeated.
gRpas can combine the results from diﬀerent groups of tests or can present them
individually. Figure 1 shows the tree structure: applications - versions - tests.
In Figure 2 a Test Set is expanded to show the relation between Individual
Tests and input parameters for an application with 4 parameters: p1 , p2 , p3 , p4 ,
where p1 gets assigned three values and p2 two. The Test Set is composed of 6
Individual Tests.
Note that although the version can be seen as an algorithm parameter, from
the practical point of view it is better to consider it as a separate entity.
The user can ﬁlter certain results across multiple sets of tests for individual
analysis, or can combine multiple tests from multiple sources.
3.3

Flexible User Interface

The application is mainly designed for algorithm comparisons, and is mainly
intended for scientiﬁc software developers. The user interface is simple and is
Applications

Versions

....

Test Sets

Individual Tests

....

Fig. 1. Tests Tree Structure

....

326

L. Cucos and E. de Doncker

Individual Tests

1

1

P1

Parameters

P1
1

2

P2

P1
P

3
1

P3

P

*

1

P2

P1
P

P1 P2 P3 P4

2

2

4

P3

P

4

3
1
*

2

P1 P2 P3 P4

Fig. 2. Individual Tests and Parameters

point-and-click based. This allows for a short learning curve and fast response
to the user. After data is collected, extended plots can be generated. Parameters
deﬁned in the input ﬁles can be selected by their names from drop-down menus.
Multiple plots can be drawn in the same window and multiple windows can be
displayed at once.
Figure 3 shows the main user interface. The table in the lower part lists all
the Test Sets performed for a particular version of code (they correspond to the
shaded areas in the Test Sets level in Figure 1). In the rightmost column the
title shows the name of the code version, and the cells show the time when the

Fig. 3. gRpas: multiple plots, multiple window

“gRpas”, a Tool for Performance Testing and Analysis

327

Sets were generated. The middle column shows the parameters used in the Set,
and the left column shows the associated name for the Set.
Once the user has selected a Test Set, its parameters are shown in the control
group situated in the right (they correspond to the type of area that is shaded
in the Individual Tests level in Figure 1). This section has a double role: to
show and to select the parameters to be plotted. In the top part the user selects
the Y coordinate from a drop list containing all the output variables, while the
X coordinate is selected from the input parameters with multiple values. Once
the user has selected a parameter set, the plots are displayed in the center-left
area.
The plots in Figure 1 graph results from solving multivariate integration problems using a quasi-Monte Carlo method with up to 60 processors. The variable
input parameters are: Integrand Function, Number of Processors, and Function Limit. This particular Set consists of 325 Individual Tests, each repeated
a number of times. On the left, the Run Time is plotted versus the Number of
Processors for all the integrand functions using a diﬀerent Function evaluation
Limit, whereas on the right the obtained error is plotted. Each plotted point
represents the median of multiple values. The data interval is shown as a closed
segment.
Figure 4 presents two algorithm versions (note an additional column in the
table Set area). The two versions have a number of identical Test Sets: Test s01,
Test s02, Test s05, and Test s06 (which can be identiﬁed by the date in the
algorithm version column). The plot shows the diﬀerences in the result values.

Fig. 4. gRpas: multiple algorithm plot

328

L. Cucos and E. de Doncker

3.4

User Defined Plugins

Extern modules can be linked to gRpas for additional functionality. The user can
pre-process the output before displaying, or can link in a diﬀerent application.
The plugins must be written in java. The extern modules are called using menus.
For example, Figure 5 shows the eﬀect of applying the scalability plugin: for
each test, the plot shows the inverse of the runtime multiplied with the runtime
of the ﬁrst test (which corresponds to the sequential run).
3.5

Statistical Reports

As mentioned earlier, the developer is interested in two types of comparisons.
First, for a given algorithm M, what is the value of β such that M (β) gives the
best performance for a set of problems Ij . Second, given two or more algorithms,
which one performs better for the same set of problems.
Most of the time, a simple inspection of the plots gives enough information
to continue analyzing the algorithm or to change it.
To generate more elaborate statistics we have two options, either implement
a small set of functions in java as plugins, or export the data to applications like
Splus or SAS. It is our goal to implement a set of functions that can perform
comparisons at the click of a button.
For further consideration, we envision a new functionality for generating plots
from non-common-parameters tests. Instead of performing Test Sets where some
parameters are ﬁxed while others vary, is possible to generate indicative plots

Fig. 5. gRpas: scalability plugin

“gRpas”, a Tool for Performance Testing and Analysis

329

from Tests that don’t share common parameters. In this way it is possible to
reduce the number of performed tests, while increasing the problem investigation
space.

4

Conclusions

In this paper we presented “gRpas”, a tool to help analyze test results for scientiﬁc computing applications. Intended to be used for both competitive testing
as well as controlled experimentation, the application has been extensively used
in our parallel numerical integration research. We included samples of its usage
from solving multivariate t-distribution (MVT) problems using a quasi-Monte
Carlo method with up to 60 processors. The “gRpas” application and code is
available for download at [7]. It can also be executed online as an applet at [7].

Acknowledgements
The authors thank Krishna K, and Brendon Thiede for important programming
contributions.

References
1. J. N. Hooker, “Testing heuristics: We have it all wrong”, J. Heuristics, vol. 1, no. 1,
pp. 33-42, 1995.
2. R. Cools. Monomial cubature rules since “Stroud”: a compilation – part 2. J. Comput. Appl. Math., 112(1-2): pp. 21-27, 1999.
3. C. H. Chen, S. D. Wu, L. Dai, “Ordinal Comparisons of Heuristics Algorithms Using
Stochastic Optimization”, IEEE Transactions on Robotics and Automation, vol. 15,
no. 1, pp. 44-56, 1999.
4. J. N. Lyness and J. J. Kaganove, “A Technique for Comparing Automatic Quadrature Routines”, The Computer Journal, vol. 20, no. 2, pp. 170-177, 1977.
5. R. Zanny, K. Kaugars and E. de Doncker, “Scalability of Branch-and-Bound and
Adaptive Integration”, in Proceedings of the Conference on Parallel Distributed
Processing Techniques and Applications (PDPTA’01), pp. 674-680, 2001.
6. Press, W. H., Teukolsky, S. A., Vetterling, W. T. and Flannery, B. P, “Numerical Recipes in C - The Art of Scientiﬁc Computing”, Second Edition, Cambridge
University Press”,1997.
7. “gRpas”, http://aegis.cs.wmich.edu/%7Elcucos/prjs/grpas/doc/gRpas.html

