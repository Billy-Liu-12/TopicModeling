A Parallel Implementation for Determining Genomic
Distances Under Deletion and Insertion
Vijaya Smitha Kolli1, Hui Liu1, Michelle Hong Pan2, and Yi Pan1
1

Department of Computer Science, Georgia State University,
Atlanta, GA 30303, USA
hui.anitaliu@gmail.com, pan@cs.gsu.edu
2
Centers for Disease Control and Prevention, Office of Workforce
and Career Development, Career Development Division,
Public Health Informatics Fellow Program, 1600 Clifton Rd,
Atlanta, GA 30333, USA
hdp1@cdc.gov

Abstract. As the need for comparing genomes of different species has grown
dramatically with the fast progress of the Human Genome Project, the evolution
at the level of whole genomes has attracted more and more attention from both
biologists and computer scientists. They are especially interested in the
scenarios in which the genome evolves through insertions, deletions, and
movements of genes along its chromosomes. Marron et al proposed a
polynomial-time approximation algorithm to compute (near) minimum edit
distances under inversions, deletions, and unrestricted insertions. Our work is
based on their algorithm, which carries out lots of comparisons and sorting to
calculate the edit distance. These comparisons and sorting are extremely timeconsuming, and they result in decrease of computational efficiency. We believe
the time of the algorithm can be improved through parallelization. We
parallelize their algorithm via OpenMP using Intel C++ compiler for Linux 7.1,
and compare three levels of parallelism: coarse grain, fine grain and
combination of both. The experiments are conducted for a varying number of
threads and different lengths of the gene sequences. The experimental results
show that either coarse grain parallelism or fine grain parallelism alone does not
improve the performance of the algorithm very much. However, the use of
combination of both fine grain and coarse grain parallelism improves the
performance of the algorithm drastically.

1 Introduction
A gene is the fundamental physical and functional unit of heredity. Each chromosome
can be represented by an ordering of signed genes. The gene orders can be rearranged
via evolutionary events like inversions and transpositions. The motivation of studying
the gene sequencing arises in molecular biology. The ability to compare genomes of
different species has grown considerably with the rapid advancement of Human
Genome Project. One of the most effective methods of finding the similarity between
genomes is to compare the order of appearance of identical genes in the two species.
V.S. Sunderam et al. (Eds.): ICCS 2005, LNCS 3515, pp. 1003 – 1010, 2005.
© Springer-Verlag Berlin Heidelberg 2005

1004

V.S. Kolli et al.

By finding the corresponding shortest edit distance between two signed gene
permutations, we can know the smallest number of insertions, deletions, and
inversions required to change one string of genes into another, where insertion,
deletion and inversion are the process of adding, deleting and reversing a single or
sequence of genes in the existing gene sequence respectively. However, it is very
difficult to compute the edit distance between two genomes; for example, this
problem is NP-hard for unsigned permutations even with equal gene content and only
inversion allowed.
Many researchers have proposed various algorithms on finding the minimum edit
distances [3,4,5,6,10]. Most of these algorithms involve lots of comparisons and
sorting while computing edit distances. Marron et al’s algorithm [2] is of particular
interest for our implementation purposes. This algorithm handles duplications as well
as insertions and presents an alternate framework for computing (near) minimal edit
sequences involving insertions, deletions, and inversions.
Marron et al’s algorithm is implemented sequentially, which is not efficient in
terms of computation time, because the algorithm carries out many comparisons and
sorting. Parallelism can be employed in the time-consuming comparisons and sorting,
thus increasing the efficiency of the algorithm. We have performed profiling on the
whole algorithm and identified the functions that are utilizing maximum time. We use
OpenMP on Intel® C++ for Linux compiler 7.1 to parallelize the algorithm [9].
The rest of this paper proceeds as follows. Section 2 provides preliminaries on the
sequential algorithm by Marron et al. We illustrate the details of our parallel
implementation in Section 3. The experimental results are analyzed in Section 4.
Section 5 draws the conclusion and proposes future works.

2 Preliminary Existing Algorithm
Marron et al’s algorithm [2] is based on a new canonical form for edit sequences.
They showed that shortest edit sequences can be transformed into equivalent
sequences of equal length in which all insertions are performed first, followed by all
inversions, and then by all deletions. This canonical form allows taking advantage of
El-Mabrouk's exact algorithm for inversions and deletions, which can be extended by
finding the best possible prefix of insertions, producing an approximate solution with
bounded error.
2.1 Canonical Form
Marron et al [2] have proved some positive results about shortest edit sequences.
These results will enable to obtain a “canonical form” into which any shortest edit
sequence can always be transformed without losing optimality. Marron et al has
proved the following two theorems.
Theorem 1: One two substrings become correctly oriented, they remain correctly
oriented.

A Parallel Implementation for Determining Genomic Distances

1005

Theorem 2: All insertions can be done before all inversions and deletions in a
Minimum Edit Sequence.
2.2 Sequences Cover
A group of substrings from the target should be determined such that every element in
the source appears in one of those substrings. The goal of the job is to cover all the
non-deleted target elements with one from the subject. A minimal cover is one that
uses fewest substrings of the subject. At each step, we try to cover the target from the
left to as far as right as possible with contiguous subsequences of the subject. At last,
this method produces a minimal cover by a greedy algorithm. The cover bound is
proved in theorem 3.
Theorem 3: There exists a cover of at most 2| π|+1 for a sequence of S.
Now we can summarize Marron et al’s Genomic Distances algorithm [2] in the
following three steps.
Step 1: Relocate insertions to obtain the canonical form of sequences;
Step 2: Resolve duplicates by finding the minimum cover through greedy
method;
Step 3: Then run exact EI-Mabrouk algorithm on the inversions and
deletions.

3 Details of Parallel Implementation
This section gives the details of the implementation steps carried out to parallelize the
Marron et al’s genomic distances algorithm.
3.1 Profiling
Profiling is a good procedure to determine the most time-consuming parts of the code.
By profiling the sequential code, we obtain a flat profile. Flat profile consists of the
percentage of time used to complete the particular function, number of calls made for
the function, self milli seconds per call and total milli seconds per call. From this flat
profile, we can deduce which function uses greater percentage of time and further
which function consumes more total milli seconds per call. The gprof utility provides
a fast and easy way to do procedural-level profiling of the code. We use gprof and
focus on the identification of such expensive functions with respect to time, and
acknowledge the code to be parallelized. So that, these functions may utilize less time
and eventually good speedup can be obtained. We compile the code using -pg option
and run the code as usual. Then the code will produce an output file named gmon.out,
which can be analyzed for further purposes.

1006

V.S. Kolli et al.

3.2 Porting the Code Between GNU 3.2 and Intel C++ Compiler 7.1 for Linux
In order to parallelize the algorithm using OpenMP we need to compile the program
on Intel C++ compiler because GNU 3.2 does not support OpenMP. There are some
significant porting issues encountered while porting program from GNU 3.2 to Intel
C++ compiler 7.1. The header files in GNU 3.2 compiler are declared with extension
.h as #include < stack.h > but in Intel C++ compiler it is declared without .h extension
as #include <stack>.
The initialization of the hash_map variables is much different in GNU compiler
and Intel C++ compiler. In GNU, the hash_map is initialized with three parameters,
while in Intel C++ compiler it does not accept three parameters in the initialization.
Due to this, the code has to be changed to compile successfully on the Intel C++
Compiler without the third parameter.
3.3 Parallel Implementation of Genomic Distances Algorithm
Genomic Distances algorithm is parallelized by using OpenMP on Intel C++ compiler
7.1 for Linux. Parallelism is implemented by applying fine-grain parallelism, coarsegrain parallelism and combination of both. OpenMP method makes use of fork-join
technique. Master thread spawns team of threads as needed.
In coarse-grain parallelism, each function is given attention to find the functional
dependencies, which is needed for synchronization, and mutual exclusive sections.
Then the independent functions are taken and parallelized using the parallel and
section directive. The mutually exclusive sections are provided with critical section
directive, which specifies a region of code that must be executed by only one thread at
a time.
In order to implement fine grain parallelism, first data dependency and critical
sections within loops is checked to determine whether variables in nested loops
should be declared as private or shared in OpenMP program after profiling and
porting the program. The variables in the for loops are limited to individual threads,
therefore they should be declared as private using private directive. Every thread has
its own copy of these variables in parallel loops. A variable without the declaration is
treated as shared variables by default. Second, the appropriate directive parallel for is
added in front of loops to inform compilers to execute the code in parallel mode. All
threads are scheduled dynamically with varying chunk sizes.
To implement the combination of coarse-grain and fine-grain parallelism in the
Genomic Distances algorithm, both the parallel section and parallel for directive are
applied appropriately in the required regions. Special attention is paid to mutual
exclusive section.

4 Analysis of Experimental Results
This section discusses the experimental environment and results obtained in
implementation of Genomic Distances algorithm via OpenMP. The Algorithm is

A Parallel Implementation for Determining Genomic Distances

1007

parallelized by using fine-grain, coarse-grain and combination of both coarse and fine
grain parallelism.
4.1 Experimental Environment
We test OpenMP performance by Intel C++ compiler 7.1. This compiler has advanced
optimization techniques for the Intel processor. The machine used is the Dell
poweredge 6600 server and it has four quad SMP CPU’s. Dell server is a powerful
scalable parallel system. Experiments are conducted by varying the number of threads
(2, 4, 6, 8) and length of gene sequences (400, 800, 1000, 2500, 5000, 10000).
Speedup and program execution time for OpenMP are measured. Experiment results
are verified and made sure that the identical gene sequences are utilized for sequential
and parallel program.
4.2 Coarse-Grain Parallelism
As a first step to improve the efficiency of the Genomic Distance algorithm, we adopt
OpenMP parallel sections construct for all the time-consuming functions. Special
attention is paid to the functional dependencies. Fig. 1 compares sequential timing
with the coarse-grain parallelism timing with varying the number of threads and
length of gene sequences.

Fig. 1. Average execution time relative to the length of gene sequences under coarse-grain
parallelism

Here the sequential algorithm was the most expensive in terms of execution time.
We will compare the speedup values among three cases; coarse-grain parallelism,
fine-grain parallelism, and the combination of coarse-grain and fine-grain parallelism
when the length of gene sequence is 10000, and the number of threads is 8. For
coarse-grain parallelism, the speedup value is 1.32. Thus, we conclude that by
parallelizing the algorithm with the parallel section construct there is not much

1008

V.S. Kolli et al.

improvement in execution time. Since there is a lot of dependency in the code, coarsegrain parallelism by sections does not yield a great performance progress.
4.3 Fine-Grain Parallelism
Later, to improve the efficiency of the Genomic Distance algorithm, we use OpenMP
parallel for construct for all the time-consuming loops in the program. As the for
directive specifies that the iterations of the loop immediately following it must be
executed in parallel by the team of threads, each for loop will divide the array used
into chunks of the specified size dynamically, and the number of threads will work on
each individual chunk. With varying sequence size, care is taken to make the chunk
size equally distributable among the available threads. Fig. 2 shows the results
obtained as a part of fine-grain parallelism.

Fig. 2. Average execution time relative to the length of gene sequences under fine-grain
parallelism

Here the sequential algorithm is the most consuming in terms of execution time.
However, by parallelizing the algorithm with the parallel for construct there is a
certain improvement. For example, the speedup value is 5.91 when gene sequence
length is 10000, and the number of treads is 8. This is a good improvement compared
with only coarse-grain parallelism.
4.4 The Combination of Coarse-Grain Parallelism and Fine-Grain Parallelism
Finally, to get a more efficient Genomic Distance algorithm, we adopt both OpenMP
fine-grain and coarse-grain parallelism by implementing both parallel for construct
and parallel sections construct to all the time consuming loops and functions. Fig. 3
shows the results obtained as a part of both fine-grain and coarse-grain parallelism.

A Parallel Implementation for Determining Genomic Distances

1009

Fig. 3. Average execution time relative to the length of gene sequences under the combination
of coarse-grain parallelism and fine-grain parallelism

As in the earlier cases, the figure shows that the sequential algorithm is the most
consuming in terms of execution time. On the other hand, by parallelizing the
algorithm with the parallel for and parallel sections constructs there is a very good
improvement. For example, the speedup is 7.36 with gene sequence length 10000 and
eight threads, which has achieved much better performance compared with only
coarse-grain parallelism and only fine-grain parallelism.
By observing all the above three cases, we notice that the combination of both finegrain and coarse-grain parallelism has improved the performance of the algorithm to a
great extent, because it implements both loop level parallelism, which takes care
about all the time-consuming loops, and functional level parallelism, which handles
the independent functions and execute them in parallel.

5 Conclusion and Future Work
The Genomic Distances algorithm in [2] provides a polynomial-time approximation
algorithm with bounded error to compute edit distances under inversions, deletions,
and unrestricted insertions from the perfectly sorted sequence to any other. The
algorithm consists of many comparisons and sorting, so it is extremely timeconsuming. In order to improve the efficiency of the algorithm, we parallelize the
algorithm using OpenMP. Furthermore, we study extensively the performance metrics
for fine-grain and coarse-grain parallelism and both together. Since we can solve the
problem quickly, we can handle larger gene sequences which are very important in
practices in bioinformatics research.
From our experimental results, we conclude that coarse grain parallelism is not
effective for this algorithm since there is lot of functional dependencies, and many
functions are not able to execute concurrently. There is improvement in performance
when the algorithm is parallelized with fine grain parallelism, for all the timeconsuming for loops are made to run in parallel. When the algorithm is parallelized by

1010

V.S. Kolli et al.

the combination of both fine and coarse grain parallelism, there is very good
improvement in the efficiency of the algorithm.
In the future, we can obtain better performance of the Genomic Distances
algorithm by using both MPI and OpenMP. MPI handles the larger-grained
communications among multiprocessors, while the lighter-weight threads of OpenMP
handle the processor interactions within each multiprocessor. By adding MPI
function calls to the OpenMP source program, the program can be transformed into a
MPI/OpenMP program suitable for execution on a cluster of multiprocessors.

References
1. D.A. Bader, B.M.E. Moret, and M. Yan. A fast linear-time algorithm for inversion
distance with an experimental comparison. J. Comput. Biol., 8(5):483. 491, 2001.
2. Mark Marron, Krister M. Sweson, and Bernard M. E. Moret. Genomic Distances under
Deletions and Insertions. Proc. 9th Int'l Combinatorics and Computing Conf.
COCOON'03, 2003, 537-547.
3. A. Caprara. Sorting by reversals is difficult. In Proc. 1st Int'l Conf. on Comput.Mol. Biol.
RECOMB97, pages 75.83. ACM Press, 1997.
4. S. Hannenhalli and P. Pevzner. Transforming cabbage into turnip (polynomial algorithm
for sorting signed permutations by reversals). In Proc. 27th Ann.Symp. Theory of
Computing STOC 95, pages 178.189. ACM Press, 1995.
5. N. El-Mabrouk. Genome rearrangement by reversals and insertions/deletions of contiguous
segments. In Proc. 11th Ann. Symp. Combin. Pattern MatchingCPM 00, volume 1848 of
Lecture Notes in Computer Science, pages 222.234. Springer-Verlag, 2000.
6. T. Liu, B.M.E. Moret, and D.A. Bader. An exact linear-time algorithm for computing
genomic distances under inversions and deletions U. New Mexico, TR-CS-2003-31.
7. Tutorial on OpenMP, http://www.llnl.gov/computing/tutorials/openMP/
8. Michael Quinn, Parallel Programming in C with MPI and OpenMP, The McGraw-Hill
Companies, 2004.
9. Intel C++ Compiler for Linux, http://www.intel.com/software/products/compilers/clin/
clinux.htm
10. Haim Kaplan, Ron Shamir and Robert E. Tarjan, “Faster and Simpler Algorithm for
Sorting Signed Permutations by Reversals”, Proc. SODA 97 pages 344—351,SIAM
Journal on Computing 29 (3) 880--892 (1999).

