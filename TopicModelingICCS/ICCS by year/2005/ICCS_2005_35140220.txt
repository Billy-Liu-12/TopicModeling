The Symmetric–Toeplitz Linear System
Problem in Parallel
Pedro Alonso and Antonio Manuel Vidal
Universidad Polit´ecnica de Valencia, cno. Vera s/n, 46022 Valencia, Spain
{palonso, avidal}@dsic.upv.es

Abstract. Many algorithms exist that exploit the special structure of
Toeplitz matrices for solving linear systems. Nevertheless, these algorithms are diﬃcult to parallelize due to its lower computational cost and
the great dependency of the operations involved that produces a great
communication cost. The foundation of the parallel algorithm presented
in this paper consists of transforming the Toeplitz matrix into a another structured matrix called Cauchy–like. The particular properties of
Cauchy–like matrices are exploited in order to obtain two levels of parallelism that makes possible to highly reduce the execution time. The
experimental results were obtained in a cluster of PC’s.

1

Introduction

In this paper, we present a parallel algorithm for the solution of the linear system
Tx = b ,

(1)

n−1
where T ∈ IRn×n is a symmetric Toeplitz matrix T = (tij )n−1
i,j=0 = (t|i−j| )i,j=0
n
and b, x ∈ IR are the independent and the solution vector, respectively.
It is diﬃcult to obtain eﬃcient parallel versions of fast algorithms, because
they have a reduced computational cost and they also have many dependencies
among ﬁne–grain operations. These dependencies produce many communications, which are a critical factor to obtain eﬃcient parallel algorithms, especially
on distributed memory computers. This problem could explain partially the
small number of parallel algorithms implemented so far dealing with Toeplitz
matrices. For instance, it can be found parallel algorithms to solve Toeplitz
systems using systolic arrays [1] or dealing only with positive deﬁnite matrices
or with symmetric matrices [2]. There also exist parallel algorithms for shared
memory computers [3, 4, 5] and, more recently, several parallel algorithms for
distributed architectures have been proposed [6].
One of our main goals is to oﬀer eﬃcient parallel algorithms for general purpose architectures, especially, clusters of personal computers. Furthermore, the
codes are portable because they are based on standard libraries, both sequential,

Supported by Spanish MCYT and FEDER under Grant TIC 2003-08238-C02-02.
V.S. Sunderam et al. (Eds.): ICCS 2005, LNCS 3514, pp. 220–228, 2005.
c Springer-Verlag Berlin Heidelberg 2005

The Symmetric–Toeplitz Linear System Problem in Parallel

221

LAPACK [7], and parallel, ScaLAPACK [8]. We are mainly interested in the reduction of parallel runtime because one of the main set of applications requires
real time computation of the linear system (1) like digital signal analysis [9].
In the next section, the mathematical background used is summarized. In
Sections 3, 4 and 5 the parallel algorithm is described. Finally, the experimental
results are shown in the last section.

2

Rank Displacement and Cauchy–Like Matrices

It is said that a matrix of order n is structured if its displacement representation
has a lower rank regarding n. The displacement representation of a symmetric
Toeplitz matrix T (1) can be deﬁned in several ways depending on the form of
the displacement matrices. A useful form for our purposes is
∇F T = F T − T F = G H G T ;

(2)

where F = T (e1 ), called displacement matrix, is a n × n symmetric Toeplitz
matrix with the second column of the identity matrix as the ﬁrst column; G ∈
IRn×4 is the generator matrix and H ∈ IR4×4 is a skew–symmetric signature
matrix. The rank of ∇F T is 4, that is, lower than n and independent of n.
It is easy to see that the displacement of T with respect to F is a matrix of a
considerably sparsity from which it is not diﬃcult to obtain an analytical form
of G and H.
A symmetric Cauchy–like matrix C is a structured matrix that can be deﬁned
as the unique solution of the displacement equation
(3)
∇Λ C = Λ C − C Λ = Gˆ H GˆT ,
n and independent of n.
being Λ = diag(λ1 , . . . , λn ), where rank(∇Λ C)
Now, we use the normalized Discrete Sine Transformation (DST) S as deﬁned
in [10]. Since S is symmetric, orthogonal and SF S = Λ [11, 12], we obtain
ˆ GˆT ,
S(F T − T F )S = S(GHG T )S → ΛC − CΛ = GH
where C = ST S and Gˆ = SG. This shows how it can be transformed (2) into (3).
In this paper, we solve the Cauchy–like linear system C x
ˆ = ˆb, where x
ˆ = Sx
ˆ
and b = Sb, by performing the triangular decomposition C = LDLT , being
L unit lower triangular and D diagonal. The solution of (1) is obtained by
ˆ = y and x = S x
ˆ.
computing Ly = ˆb, y ← D−1 y, LT x
Solving a symmetric Toeplitz linear system by transforming it into a symmetric Cauchy–like system has an interesting advantage due to the symmetric
Cauchy–like matrix has an important sparsity. Matrix C has the form (x only
denotes non–zero entries),
⎞
⎛
x 0 x 0 ...
⎜0 x 0 x ...⎟
⎟
⎜
⎟
⎜
C = ⎜x 0 x 0 ...⎟ .
⎜0 x 0 x ...⎟
⎠
⎝
.. .. .. .. . .
. . . . .

222

P. Alonso and A.M. Vidal

We deﬁne the odd–even permutation matrix Poe as the matrix that, after applied to a vector, groups the odd entries in the ﬁrst positions and the even entries
T
T
in the last ones, Poe x1 x2 x3 x4 x5 x6 . . .
= x1 x3 x5 . . . x2 x4 x6 . . . .
T
to a symmetric Cauchy–like matrix C gives
Applying transformation Poe (.)Poe
T
Poe CPoe
=

C0

,

C1

(4)

where C0 and C1 are symmetric Cauchy–like matrices of order n/2 and n/2 ,
respectively. In addition, it can be shown that matrices C0 and C1 have a displacement rank of 2, as opposed to C that has a displacement rank of 4 [5].
The two submatrices arising in (4) have the displacement representation
Λj Cj − Cj Λj = Gj Hj GTj ,
where

Λ0
Λ1

i = 0, 1 ,

(5)

0 1
. As it is shown in [13],
−1 0

T
and H0 = H1 =
= Poe SΛSPoe
T

and the ﬁrst column of the identity
given vector uT = 0 t2 t3 · · · tn−2 tn−1
matrix e0 , the generators of (5) can be computed as
G0
G1

=

√
2Poe S u e0

.

(6)

The odd–even permutation matrix is used to decouple the symmetric Cauchy–
like matrix arising from a real symmetric Toeplitz matrix into the following two
Cauchy–like systems of linear equations
¯bj ,
ˆ¯j = ˆ
Cj x
T

j = 0, 1 ,
T

T

(7)

T

¯b = ˆ
ˆ¯ = x
ˆ¯T1
ˆ¯T0 x
¯
¯b ˆ
= Poe Sx and ˆ
= Poe Sb.
where x
0 b1
Each one of both linear systems are of half the size and half the displacement
rank so this yields substantial saving over the non–symmetric forms of the displacement equation. Furthermore, it can be exploited in parallel by solving each
of the two independent sub–systems into two diﬀerent processors.

3

The Parallel Algorithm

For the parallel solution we used a two dimensional mesh of p/2 × 2 processors as shown in Fig. 1, where each one of the p processors is denoted by the
corresponding row and column index.
We used the ScaLAPACK tools in order to manage data distribution over this
logical conﬁguration of the processors. Once the symmetric Toeplitz system has
been converted into a symmetric Cauchy–like one, the two subsystems arisen (7)
will be solved independently on each “logical column” of the two–dimensional
processors mesh. This is what the external loop (j = 0, 1) of Algorithm 1 represents, that is, iteration 0 and 1 are concurrently executed by processors column
0 and 1, respectively.

The Symmetric–Toeplitz Linear System Problem in Parallel

P0,0
P1,0

Pp/2−1,0

✄

  ✄

✂✄

✁  ✂✄

 
P0,1
✁ 

✂

✁ ✂

P1,1
✁

✄

  ✄

✂

✁ ✂

223

 
Pp/2−1,1
✁

Fig. 1. 2D mesh of processors

Algorithm 1 (Parallel Algorithm for the solution of a symmetric–
Toeplitz system with Cauchy–like transformation). Given T ∈ IRn×n
a symmetric Toeplitz matrix and b ∈ IRn an independent vector, this algorithm
returns the solution vector x ∈ IRn of the linear system T x = b. For each Pi,j ,
1. for j = 0, 1, do
for i = 0, . . . , p/2 − 1, do
1.1. “Previous computations”.
1.2. Cj = Lj Dj LTj (4).
¯bj (7).
ˆ¯j = ˆ
1.3. Solution of Lj Dj LTj x
end for.
end for.
T
ˆ¯T0 x
ˆ¯T1
2. P0,0 computes x = SPoe
x

T

.

Steps 1.1 and 1.2 are explained in the next two sections, respectively. Step 1.3
is performed by ScaLAPACK and PBLAS parallel subroutines. This last step
can be repeated several times for iterative reﬁnement. Finally, processor P0,0
gathers the partial solutions of the two independent Cauchy–like linear systems
and computes the solution of (1).

4

Parallel Triangularization of Symmetric Cauchy–Like
Matrices

The workload of the step 1.2 of Algorithm 1 falls in the operation with the n × 2
entries of the generators G0 and G1 (6). The logical column of processors Pi,j ,
i = 0, . . . , p/2 − 1, performs the triangular decomposition of the corresponding
matrix Cj = Lj Dj LTj , j = 0, 1, where Lj is unit lower triangular and Dj is
diagonal. The parallel algorithm exploits the fact that operations performed by
each column of processors can be carried out independently on each row of Gj .
Let
Λ0
Λ1

C −C

Λ0
Λ1

=

G0
G1

H

G0
G1

T

,

(8)

224

P. Alonso and A.M. Vidal

be the displacement representation of a given symmetric Cauchy–like matrix
C00 C01
C =
, then the Schur complement Csc of C11 regarding C00 is also
C10 C11
structured for any partition of C [14] so
Λ1 Csc − Csc Λ1 = G1 HG1T .

(9)

The parallel algorithm uses a sequential algorithm that, given the generator, the
displacement matrix and the diagonal of C in equation (8) as entries, returns
G1T , the diagonal of Csc of equation (9) and the factorization C00 = L00 D00 LT00 .
Let mj denotes the size of Cj (4), j = 0, 1, respectively. Each generator
Gj (5) is partitioned in mj /ν blocks of size ν × 2 for a given integer ν, 1 ≤
ν ≤ (mj /p), and cyclically distributed onto the respective column of processors
Pk,j , for k = 0, . . . , p/2 − 1, in such a way that block Gi,j , i = 0, . . . , mj /ν − 1,
belongs to processor P mod(i,p),j . For simplicity in the exposition we will assume
in the next that (mj mod ν) = 0. The unit lower triangular factor Lj obtained
by the algorithm is partitioned in a two dimensional array of (mj /ν) × (mj /ν)
square blocks of order ν, where each square block Lji,l , for l = 0, . . . , i, belongs to
processor P mod(i,p),j as the generators blocks. The diagonal matrix Dj is stored
in the diagonal entries of Lj since all diagonal entries of Lj are implicitly one. In
Fig. 2 it can be seen an example of distribution of both Gj and Lj in the logical
column j = 0, 1 formed of three processors.

P0,j G0,j Lj0,0
P1,j G1,j Lj1,0 Lj1,1
P2,j G2,j Lj2,0 Lj2,1 Lj2,2
P0,j G3,j Lj3,0 Lj3,1 Lj3,2 Lj3,3
P1,j G4,j Lj4,0 Lj4,1 Lj4,2 Lj4,3 Lj4,4
..
..
..
..
..
..
.. . .
.
.
.
.
.
.
.
.
Fig. 2. Example of data distribution in a mesh of 3 × 2 processors

In each iteration k, k = 0, . . . , (mj /ν −1), the processor containing block Gk,j
computes Ljk,k by means of the sequential algorithm described at the beginning of
this section and broadcasts the suitable information to the rest of the processors
of the column that compute Lji,k and update the Gi,j , for i > k.

5

Previous Computations

Step 1.1 of Algorithm 1, called “previous computations”, involves four diﬀerent
tasks, denoted as Task00, Task10, Task01 and Task11, respectively, in which is

The Symmetric–Toeplitz Linear System Problem in Parallel

225

divided the computation of G0 and G1 (6), the computation of the displacement matrices Λ0 and Λ1 (5), the computation of the diagonal of C [5] and the
¯b0 and ˆ
¯b1 (7).
computations of ˆ
If there are only two processors (p = 2), P0,0 computes Taski0 while P0,1
computes Taski1, for i = 0, 1. For p ≥ 4, processor Pi,j computes Taskij. After
the computation of the tasks, all processors perform two communication steps:
1. A multicast of the results obtained by the tasks within each column; 2. A data
interchange between pairs of processors in each row of the processors mesh.
Several a DST’s are carried out in step 1.1 of Algorithm 1. There exist algorithms related with the DFT that involves O(n log n) operations to perform the
DST. But the performance of these algorithms highly depends on the size of the
greatest prime number of the primes decomposition of (n+1). In our algorithms,
we used a method to avoid this dependency by applying several power of 2 of
order DFT’s using the Chirp-z factorization described in [10].

6

Experimental Results

All experimental analysed were carried out in a cluster with 20 nodes connected
by a SCI network with a topology of a 4 × 5 torus. Each node is a two–processor
board with two Intel Xeon at 2 GHz. and 1 Gb. of RAM memory per node.
The ﬁrst analysis concerns the block size ν. At the light of the experiments,
it can be concluded that there exist a wide range of values for that minimize
the execution time. But, none of these values must be selected close to 1 (this
implies too many communications) or close to n/p (this implies not enough concurrency between communications and computations). The best value obtained
by experimental tuning is a ﬁxed size of ν = 20 that is only hardware dependent.
The second experimental analysis deals with the weight of each step of Algorithm 1 on its total cost. Table 1 shows the time spent on each step. It can be
observed that the time spent in Step 1.1 grows with the problem size. The Chirpz factorization used to perform the DST makes the cost of this step independent
of the size of the prime numbers in which (n + 1) is decomposed. The weight of
this ﬁrst step is ≈ 25% of the total computational cost of the algorithm. The
most costly step is the second one in which is performed the factorization of one
Table 1. Execution time in seconds of Algorithm 1 in one processor
n
4000
6000
8000
10000
12000
14000
16000
18000
20000

Step 1.1
0.11
0.25
0.35
0.60
0.75
0.95
1.17
1.71
1.96

Step 1.2

Step 1.3

0.24
0.51
0.88
1.35
1.93
2.59
3.36
4.21
5.18

0.05
0.12
0.20
0.35
0.47
0.63
0.81
1.09
1.30

total
0.39
0.87
1.42
2.28
3.13
4.14
5.28
6.96
8.37

226

P. Alonso and A.M. Vidal

of the two Cauchy–like matrices (C0 or C1 ). The weight of this step is ≈ 60% of
the total cost of the algorithm. The third step involves ≈ 15% of the total time.
As it was explained in Section 5, the ﬁrst step is divided in four tasks, each
of one is carried out concurrently so it can be obtained a reduction in time in
this step using up to 4 processors (Table 2).

Table 2. Execution time in seconds and eﬃciency of Step 1.1

n

1 processor 2 processors
4 processors
time
time eﬃciency time eﬃciency

4000
6000
8000
10000
12000
14000
16000
18000
20000

0.11
0.25
0.35
0.60
0.75
0.95
1.17
1.71
1.96

0.06
0.14
0.19
0.33
0.41
0.53
0.66
0.91
1.07

92%
89%
92%
91%
91%
90%
89%
94%
92%

0.04
0.10
0.13
0.22
0.27
0.35
0.43
0.60
0.70

69%
63%
67%
68%
69%
68%
68%
71%
70%

In Table 3 it can be seen the execution time and the eﬃciency of Step 1.2.
The important eﬀort performed in the parallelization of the triangularization
process gives a good eﬃciency even with 10 processors. The low time obtained
with the most costly step using several processors lets to obtain a low total time.
This result, although it cannot be as eﬃcient as it was desirable, it can be very
useful in applications with real time constraints like digital signal processing.
We note that the eﬃciency obtained with 2 processors is quite good mainly
due to the triangular decomposition of the two independent Cauchy–like matrices
over two independent processors.

Table 3. Execution time in seconds and eﬃciency of Step 1.2

n
4000
6000
8000
10000
12000
14000
16000
18000
20000

1 proc.
2 procs.
time
time eﬃ.
0.24
0.51
0.88
1.35
1.93
2.59
3.36
4.21
5.18

0.13
0.27
0.48
0.73
1.03
1.39
1.80
2.25
2.71

92%
94%
92%
92%
94%
93%
93%
94%
96%

4 procs.
time eﬃ.

6 procs.
time eﬃ.

8 procs.
time eﬃ.

10 procs.
time eﬃ.

0.09
0.16
0.28
0.39
0.54
0.76
0.91
1.21
1.48

0.07
0.13
0.20
0.30
0.41
0.53
0.67
0.83
1.00

0.06
0.11
0.18
0.25
0.34
0.45
0.56
0.69
0.83

0.05
0.10
0.16
0.23
0.30
0.39
0.49
0.60
0.71

67%
80%
79%
87%
89%
85%
92%
87%
88%

57%
65%
73%
75%
78%
81%
84%
85%
86%

50%
58%
61%
68%
71%
72%
75%
76%
78%

48%
51%
55%
59%
64%
66%
69%
70%
73%

sec.

The Symmetric–Toeplitz Linear System Problem in Parallel
9
8
7
6
5
4
3
2
1
0

227

n
8000
12000
16000
20000

1

2

3

4

5

6

7

8

9

10

Processors
Fig. 3. Time in seconds of the parallel algorithm

Finally, we analyze the execution time of the parallel algorithm. In Fig. 3,
it can be seen that the time decreases with the increment of the number of
processors. This reduction in time is more signiﬁcant if the problem size increases.
The algorithm also reduces its execution time with more than four processors
although Step 1.1 does not exploit more processors in parallel than this quantity.
We emphasize the reduction in time regarding the scalability of the parallel
algorithm by its utility in applications with real time constraints.

References
1. Sweet, D.R.: The use of linear-time systolic algorithms for the solution of toeplitz
problems. Technical Report JCU-CS-91/1, Department of Computer Science,
James Cook University (1991) Tue, 23 Apr 1996 15:17:55 GMT.
2. Evans, D.J., Oka, G.: Parallel solution of symmetric positive deﬁnite Toeplitz
systems. Parallel Algorithms and Applications 12 (1998) 297–303
3. Gohberg, I., Koltracht, I., Averbuch, A., Shoham, B.: Timing analysis of a parallel
algorithm for Toeplitz matrices on a MIMD parallel machine. Parallel Computing
17 (1991) 563–577
4. Gallivan, K., Thirumalai, S., Dooren, P.V.: On solving block toeplitz systems using
a block schur algorithm. In: Proceedings of the 23rd International Conference on
Parallel Processing. Volume 3., Boca Raton, FL, USA, CRC Press (1994) 274–281
5. Thirumalai, S.: High performance algorithms to solve Toeplitz and block Toeplitz
systems. Ph.d. th., Grad. College of the U. of Illinois at Urbana–Champaign (1996)
6. Alonso, P., Bad´ıa, J.M., Vidal, A.M.: Parallel algorithms for the solution of toeplitz
systems of linear equations. Lecture Notes in Computer Science 3019 (2004) 969–
976
7. Anderson, E., et al.: LAPACK Users’ Guide. SIAM, Philadelphia (1995)
8. Blackford, L., et al.: ScaLAPACK Users’ Guide. SIAM, Philadelphia (1997)
9. Alonso, P., Bad´ıa, J.M., Gonz´
alez, A., Vidal, A.M.: Parallel design of multichannel
inverse ﬁlters for audio reproduction. In: Parallel and Distributed Computing and
Systems, IASTED. Volume II., Marina del Rey, CA, USA (2003) 719–724

228

P. Alonso and A.M. Vidal

10. Loan, C.V.: Computational Frameworks for the Fast Fourier Transform. SIAM
Press, Philadelphia (1992)
11. Heinig, G.: Inversion of generalized Cauchy matrices and other classes of structured
matrices. Linear Algebra and Signal Proc., IMA Math. Appl. 69 (1994) 95–114
12. Gohberg, I., Kailath, T., Olshevsky, V.: Fast Gaussian elimination with partial
pivoting for matrices with displacement structure. Mathematics of Computation
64 (1995) 1557–1576
13. Alonso, P., Vidal, A.M.: An eﬃcient and stable parallel solution for symmetric
toeplitz linear systems. TR DSIC-II/2005, DSIC–Univ. Polit. Valencia (2005)
14. Kailath, T., Sayed, A.H.: Displacement structure: Theory and applications. SIAM
Review 37 (1995) 297–386

