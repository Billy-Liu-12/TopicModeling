Genetically Dynamic Optimization Based Fuzzy
Polynomial Neural Networks
Ho-Sung Park1, Sung-Kwun Oh2, Witold Pedrycz3, and Yongkab Kim1
1

Department of Electrical Electronic and Information Engineering, Wonkwang University,
344-2, Shinyong-Dong, Iksan, Chon-Buk, 570-749, South Korea
2 Department of Electrical Engineering, The University of Suwon, San 2-2 Wau-ri,
Bongdam-eup, Hwaseong-si, Gyeonggi-do, 445-743, South Korea
ohsk@suwon.ac.kr
3
Department of Electrical and Computer Engineering, University of Alberta,
Edmonton, AB T6G 2G6, Canada
and Systems Research Institute, Polish Academy of Sciences, Warsaw, Poland

Abstract. In this paper, we introduce a new architecture of genetically dynamic
optimization based Fuzzy Polynomial Neural Networks (gdFPNN) and discuss
its comprehensive design methodology involving mechanisms of genetic optimization, especially genetic algorithms (GAs). The proposed gdFPNN gives
rise to a structurally and parametrically optimized network through an optimal
parameters design available within FPN. Through the consecutive process of
such structural and parametric optimization, an optimized and flexible gdFPNN
is generated in a dynamic fashion. The performance of the proposed gdFPNN is
quantified through experimentation that exploits standard data already used in
fuzzy modeling. These results reveal superiority of the proposed networks over
the existing fuzzy and neural models.

1 Introduction
The challenging quest for constructing models of the systems that come with significant approximation and generalization abilities as well as are easy to comprehend has
been within the community for decades [1], [2], [3], [4]. The most successful approaches to hybridize fuzz systems with learning and adaptation have been made in
the realm of CI [5]. As one of the representative design approaches which are advanced tools, a family of fuzzy polynomial neuron (FPN)-based SOPNN(called
“FPNN” as a new category of neuro-fuzzy networks)[6] were introduced to build
predictive models for such highly nonlinear systems. The FPNN algorithm exhibits
some tendency to produce overly complex networks as well as a repetitive computation load by the trial and error method and/or the repetitive parameter adjustment by
designer like in case of the original GMDH algorithm.
In this study, in addressing the above problems with the conventional SOPNN (especially, FPN-based SOPNN called “FPNN” [6], [7]) as well as the GMDH algorithm, we introduce a new genetic design approach; as a consequence we will be referring to these networks as genetically dynamic optimization based FPNN (to be
V.S. Sunderam et al. (Eds.): ICCS 2005, LNCS 3514, pp. 792 – 797, 2005.
© Springer-Verlag Berlin Heidelberg 2005

Genetically Dynamic Optimization Based Fuzzy Polynomial Neural Networks

793

called “gdFPNN”). The determination of the optimal values of the parameters available within an individual FPN (viz. the number of input variables, the order of the
polynomial, input variables, the number of membership function, and the apexes of
membership function) leads to a structurally and parametrically optimized network.

2 The Architecture and Development of Fuzzy Polynomial Neural
Networks (FPNN)
2.1 FPNN Based on Fuzzy Polynomial Neurons (FPNs)
The FPN consists of two basic functional modules. The first one, labeled by F, is a
collection of fuzzy sets that form an interface between the input numeric variables and
the processing part realized by the neuron. The second module (denoted here by P) is
about the function – based nonlinear (polynomial) processing. This nonlinear processing involves some input variables.
2.2 The Review of Conventional FPNN Architecture
Proceeding with the conventional FPNN architecture as presented in [6], [7], essential
design decisions have to be made with regard to the number of input variables and the
order of the polynomial occurring in the conclusion part of the rule. The overall selection process of the conventional FPNN architecture is shown in Fig. 1
Design items being considered in FPNN architecture

No. of system input
variables

No. of input
variables each layer

Order of the
polynomial in the
conclusion part

Generic type

Basic FPNN

Case 1

Scheme 1

Advanced Type

Modified FPNN

Case 2

Scheme 2

Input variables of
the consequence part

Fig. 1. Taxonomy of the conventional FPNN architecture

3 The Algorithms and Design Procedure of Genetically Dynamic
Optimization Based FPNN
3.1 Genetic Optimization of FPNN
GAs is optimization techniques based on the principles of natural evolution. In essence, they are search algorithms that use operations found in natural genetics to
guide a comprehensive search over the parameter space. GAs has been theoretically
and empirically demonstrated to provide robust search capabilities in complex spaces
thus offering a valid solution strategy to problems requiring efficient and effective
searching [8]. In this study, for the optimization of the FPNN model, GA uses the

794

H.-S. Park et al.

serial method of binary type, roulette-wheel used in the selection process, one-point
crossover in the crossover operation, and a binary inversion (complementation) operation in the mutation operator. To retain the best individual and carry it over to the nest
generation, we use elitist strategy [9].
3.2 Design Procedure of Genetically Dynamic Optimization Based FPNN
[Step 1] Determine system’s input variables
[Step 2] Form training and testing data
The input-output data set (xi, yi)=(x1i, x2i, …, xni, yi), i=1, 2, …, N is divided into
two parts, that is, a training and testing dataset.
[Step 3] Decide initial information for constructing the gdFPNN structure
[Step 4] Decide FPN structure using genetic design
We divide the chromosome to be used for genetic optimization into four subchromosomes. The 1st sub-chromosome contains the number of input variables, the
2nd sub-chromosome involves the order of the polynomial of the node, the 3rd subchromosome contains input variables, and the 4th sub-chromosome (remaining bits)
involves the number of MF coming to the corresponding node (FPN).
[Step 5] Design of structurally optimized gdFPNN
In this step, we design the structurally optimized gdFPNN by means of FPNs that
obtained in [Step 4].
[Step 6] Identification of membership value using dynamic searching method of GAs
[Step 7] Design of parametrically optimized gdFPNN
Sub-step 1) We set up initial genetic information necessary for generation of the
gdFPNN architecture.
Sub-step 2) The nodes (FPNs) are generated through the genetic design.
Sub-step 4) we calculate the fitness function.
Sub-step 5) To move on to the next generation, we carry out selection, crossover, and
mutation operation using genetic initial information and the fitness values.
Sub-step 6) We choose optimal gdFPNN characterized by the best fitness value in the
current generation. For the elitist strategy, selected best fitness value used.
Sub-step 7) We generate new populations of the next generation using operators of
GAs obtained from Sub-step 2. We use the elitist strategy. This sub-step carries out
by repeating sub-step 2-6.
Sub-step 8) Until the last generation, this sub-step carries out by repeating sub-step 2-7.

4 Experimental Studies
The performance of the gdFPNN is illustrated with the aid of well-known and widely
used dataset of a gas furnace process utilized by Box and Jenkins [10].
We try to model the gas furnace using 296 pairs of input-output data. The total data
set consisting 296 input-output pairs was split into two parts. The first one (consisting
of 148 pairs) was used for training. The remaining part of the series serves as a testing

Genetically Dynamic Optimization Based Fuzzy Polynomial Neural Networks

795

set. In order to carry out the simulation, we use six-input [u(t-3), u(t-2), u(t-1), y(t-3),
y(t-2), y(t-1)] and one-output (y(t)).To come up with a quantitative evaluation of the
network, we use the standard MSE performance index.
Table 1 summarizes the performance index of gdFPNN when using dynamic
searching method.
Table 1. Performance index of gdFPNN for the gas furnace process data (3rd layer)

(a) Selected input variables
Max Triangular MF
PI
EPI
2
0.019
0.102
3
0.015
0.115

(b) Entire system input variables

Gaussian-like MF
PI
EPI
0.013
0.101
0.011
0.116

Triangular MF
PI
EPI
0.011
0.105
0.011
0.110

Gaussian-like MF
PI
EPI
0.012
0.102
0.007
0.106

Fig. 2 illustrates the detailed optimal topologies of gdFPNN for 3 layer when using
Max=2 and Gaussian-like MF: the results of the network have been reported as
PI=0.012 and EPI=0.102.
As shown in Fig 2, the proposed network enables the architecture to be a structurally and parametrically more optimized and simplified network than the conventional
FPNN.

u(t-3)

FPN6

3
2

2

u(t-2)

2
FPN2 6

3
2

u(t-1)

2

2
3

2

FPN1 8

2

yˆ

2

FPN2 7

3
2

y(t-2)

1

FPN1 8

2
3

y(t-3)

2

2

1

FPN1 9

2
2

2

y(t-1)

2

Fig. 2. Genetically dynamic optimization based FPNN (gdFPNN) architecture
0.015

0.16

Parametric
optimizatoin

Structuraloptimization
0.014

Parametric
optimizatoin

Structuraloptimization
0.15

Testing data error

Training data error

0.013
0.012
0.011
0.01

0.14

0.13

0.12

0.009

0.11

0.008
0.007

0

50

100

150

200

250

300

Generation

350

400

450

500

550

0.1

0

50

100

150 200

250

300

Generation

Fig. 3. The optimization process by the genetic algorithms

350 400

450 500

550

796

H.-S. Park et al.

Fig. 3 illustrates the optimization process by visualizing the values of the performance index obtained in successive generations of GA.
Table 2 gives a comparative summary of the network with other models.
Table 2. Comparative analysis of the performance of the network; considered are models
reported in the literature
Model
Box and Jenkin’s model [10]
Tong’s model [11]
Sugeno and Yasukawa’s model [12]
Xu and Zailu’s model [13]
Oh and Pedrycz’s model [14]
Lin and Cunningham’s model [15]
CASE I
FPNN [16]
CASE II
Triangular MF
gFPNN [17]
Gaussian-like MF
3rd
layer
Proposed
Triangular MF
gdFPNN
Gaussian-like MF

PI
0.710
0.469
0.190
0.328
0.123

Max=4
Max=4
Max=2
Max=3

PIs

EPIs

0.020
0.071
0.016
0.012
0.018
0.020
0.011
0.007

0.271
0.261
0.116
0.125
0.122
0.104
0.105
0.106

5 Concluding Remarks
In this study, the design procedure of genetically dynamic optimization based Fuzzy
Polynomial Neural Networks (gdFPNN) along with their architectural considerations
has been investigated. In contrast to the conventional FPNN structures and their learning, the proposed model comes with a diversity of local characteristics of FPNs that
are extremely useful when coping with various nonlinear characteristics of the system
under consideration. The design methodology comes as a structural and parametrical
optimization being viewed as two fundamental phases of the design process. The
comprehensive experimental studies involving well-known datasets quantify a superb
performance of the network in comparison to the existing fuzzy and neuro-fuzzy
models. Most importantly, through the proposed framework of genetic optimization
we can efficiently search for the optimal network architecture (structurally and parametrically optimized network) and this becomes crucial in improving the performance
of the resulting model.
Acknowledgement. This work was supported by Korea Research Foundation Grant
(KRF-2004-002-D00257)

References
1. Cherkassky, V., Gehring, D., Mulier, F.: Comparison of adaptive methods for function estimation from samples. IEEE Trans. Neural Networks. 7 (1996) 969-984
2. Dickerson, J. A., Kosko, B.: Fuzzy function approximation with ellipsoidal rules. IEEE
Trans. Syst., Man, Cybernetics. Part B. 26 (1996) 542-560

Genetically Dynamic Optimization Based Fuzzy Polynomial Neural Networks

797

3. Sommer, V., Tobias, P., Kohl, D., Sundgren, H., Lundstrom, L.: Neural networks and abductive networks for chemical sensor signals: A case comparison. Sensors and Actuators
B. 28 (1995) 217-222
4. Kleinsteuber, S., Sepehri, N.: A polynomial network modeling approach to a class of
large-scale hydraulic systems. Computers Elect. Eng. 22 (1996) 151-168
5. Cordon, O., et al.: Ten years of genetic fuzzy systems: current framework and new trends.
Fuzzy Sets and Systems. 2003(in press)
6. Oh, S.K., Pedrycz, W.: Self-organizing Polynomial Neural Networks Based on PNs or
FPNs : Analysis and Design. Fuzzy Sets and Systems. 142(2) (2003) 163-198
7. Oh, S.K., Pedrycz, W.: Fuzzy Polynomial Neuron-Based Self-Organizing Neural Networks. Int. J. of General Systems. 32 (2003) 237-250
8. Michalewicz, Z.: Genetic Algorithms + Data Structures = Evolution Programs. 3rd edn.
Springer-Verlag, Berlin Heidelberg New York. (1996)
9. Jong, D., K. A.: Are Genetic Algorithms Function Optimizers?. Parallel Problem Solving
from Nature 2, Manner, R. and Manderick, B. eds., North-Holland, Amsterdam. (1992)
10. Box, D.E., Jenkins, G.M.: Time Series Analysis, Forcasting and Control, California, Holden Day. (1976)
11. Tong, R.M.: The evaluation of fuzzy models derived from experimental data. Fuzzy Sets
and Systems. 13 (1980) 1-12
12. Sugeno, M., Yasukawa, T.: A Fuzzy-Logic-Based Approach to Qualitative Modeling.
IEEE Trans. Fuzzy Systems. 1 (1993) 7-31
13. Xu, C.W., Xi, T.G., Zhang, Z.J.: A clustering algorithm for fuzzy model identification.
Fuzzy Sets and Systems. 98 (1998) 319-329
14. Oh, S.K., Pedrycz, W.: Identification of Fuzzy Systems by means of an Auto-Tuning Algorithm and Its Application to Nonlinear Systems. Fuzzy Sets and Systems. 115 (2000)
205-230
15. Lin, Y., Cunningham III, G. A.: A new approach to fuzzy-neural modeling. IEEE Trans.
Fuzzy Systems. 3 (1995) 190-197
16. Park, H.S., Oh, S.K., Yoon, Y.W.: A New Modeling Approach to Fuzzy-Neural Networks
Architecture. Journal of Control, Automation and Systems Engineering. 7 (2001) 664674(in Koreans)
17. Oh, S.K., Pedrycz, W., Park, H.S.: Genetically Optimized Fuzzy Polynomial Neural Networks. IEEE Trans. Fuzzy Systems. (2004) (submitted)
18. Park, B.J., Lee, D.Y., Oh, S.K.: Rule-based Fuzzy Polynomial Neural Networks in Modeling Software Process Data. International journal of Control, Automations, and Systems.
1(3) (2003) 321-331

