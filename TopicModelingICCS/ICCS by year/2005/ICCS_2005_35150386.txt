Self-organizing Dynamics for Optimization
Stefan Boettcher
Physics Department, Emory University, Atlanta, Georgia 30322-2430, USA

Abstract. Motivated by noise-driven cellular automata models of selforganized criticality (SOC), a new paradigm for the treatment of hard
combinatorial optimization problems is proposed. An extremal selection
process preferentially advances variables in a poor local state. The ensuing dynamic process creates broad ﬂuctuations to explore energy landscapes widely, with frequent returns to near-optimal conﬁgurations.

1

Motivation from Self-organized Criticality

The study of certain randomly-driven, dissipative automata model with local
threshold dynamics has provided a plausible view of many self-organizing processes ubiquitous in Nature [1]. Most famously, the Abelian sandpile model [3]
has been used to describe the statistics of earthquakes [1]. Another variant is
the Bak-Sneppen model (BS) [2], in which variables are updated sequentially
based on a global threshold condition. It provides an explanation for broadly
distributed extinction events [21] and the “missing link” problem [14]. Complexity in these SOC models emerges purely from the dynamics, without ﬁne-tuning
of parameters, as long as driving is slow and the ensuing avalanches are fast.
In the BS, “species” are located on the sites of a lattice, and have an associated “ﬁtness” value between 0 and 1. At each time step, the one species with the
smallest value (poorest degree of adaptation) is selected for a random update,
having its ﬁtness replaced by a new value drawn randomly from a ﬂat distribution on the interval [0, 1]. But the change in ﬁtness of one species impacts the
ﬁtness of interrelated species. Therefore, all of the species at neighboring lattice
sites have their ﬁtness replaced with new random numbers as well. After a sufﬁcient number of steps, the system reaches a highly correlated state known as
self-organized criticality (SOC) [3]. In that state, almost all species have reached
a ﬁtness above a certain threshold. These species, however, possess punctuated
equilibrium [14]: only one’s weakened neighbor can undermine one’s own ﬁtness.
This coevolutionary activity gives rise to chain reactions called “avalanches”,
large ﬂuctuations that rearrange major parts of the system, potentially making
any conﬁguration accessible.
We have described [7] the spatio-temporal complexity of the BS by a delay
equation for the evolution of activity P (r, t) after a perturbation at (r = 0, t = 0),
∂t P (r, t) = ∇2r P (r, t) +

t

dt V (t − t )P (r, t ),

t =0

V.S. Sunderam et al. (Eds.): ICCS 2005, LNCS 3515, pp. 386–394, 2005.
c Springer-Verlag Berlin Heidelberg 2005

V (t) ∼ t−α ,

(1)

Self-organizing Dynamics for Optimization

387

1

which has the solution P (r, t) ∼ exp{−C(rD /t) D−1 } with D = 2/(α − 1). Thus,
it is the memory of all previous events that determines the current activity.

2

Extremal Optimization

Although coevolution may not have optimization as its exclusive goal, it serves
as a powerful paradigm. We have used it as motivation for a new approach to
approximate hard optimization problems [8, 9]. The heuristic we have introduced,
called extremal optimization (EO), follows the spirit of the BS, updating those
variables which have among the “worst” values in a solution and replacing them
by random values without ever explicitly improving them. The resulting heuristic
explores the conﬁguration space Ω widely with frequent returns to near-optimal
solutions, without ﬁne-tuning of parameters.
To introduce EO, let us consider a spin glass [20] as a speciﬁc example of an
NP-hard optimization problem. It consists of a d-dimensional hyper-cubic lattice
of length L with periodic boundary conditions, with a spin variable xi ∈ {−1, 1}
at each site i, 1 ≤ i ≤ n (= Ld ). A spin is connected to each of its nearest
neighbors j via a bond variable Jij ∈ {−1, 1}, assigned at random. We minimize
C(S) = H(x) = −

i,j

Jij xi xj

(2)

as (Hamiltonian) cost function, the sum extending over all nearest-neighbor pairs
of spins. Due to frustration [20], ground states Smin are hard to ﬁnd for d > 2.
To ﬁnd near-optimal solutions for a particular optimization problem, EO
performs a neighborhood search on a single conﬁguration S ∈ Ω. As in the spin
problem in Eq. (2), S consists of a large number n of variables xi . We assume
that each S possesses a neighborhood N (S) that rearranges the state of merely a
small number of the variables. This is a characteristic of a local search, in contrast
to a genetic algorithm, say, where cross-overs may eﬀect O(n) variables on each
update. The cost C(S) is assumed to consist of the individual cost contributions,
or “ﬁtnesses”, λi for each variable xi . The ﬁtness of each variable assesses its
contribution to the total cost and typically the ﬁtness λi depends on the state
of xi in relation to connected variables. For example, for the Hamiltonian in
Eq. (2), we assign to each spin xi the ﬁtness
n

λi = xi

Jij xj ,
<,j>

C(S) = −

λi .

(3)

i=1

Each spin’s ﬁtness thus corresponds to (the negative of) its local energy contribution to the overall energy of the system. In similarity to the BS, EO then
proceeds through a neighborhood search of Ω by sequentially changing variables
with “bad” ﬁtness on each update, for instance, via single spin-ﬂips. After each
update, the ﬁtnesses of the changed variable and of all its connected neighbors
are reevaluated according to Eq. (3).
The algorithm operates on a single conﬁguration S at each step. Each variable xi in S has a ﬁtness, of which the “worst” is identiﬁed. This ranking of the

388

S. Boettcher

variables provides the only measure of quality on S, implying that all other variables are “better” in the current S. In the move to a neighboring conﬁguration,
typically only a small number of variables change state, so only a few connected
variables need to be re-evaluated [step (2a)] and re-ranked [step (2b)]. In detail:
1. Initialize conﬁguration S at will; set Sbest := S.
2. For the “current” conﬁguration S,
(a) evaluate λi for each variable xi ,
(b) ﬁnd j satisfying λj ≤ λi for all i, i.e., xj has the “worst ﬁtness”,
(c) choose S ∈ N (S) such that xj must change,
(d) accept S := S unconditionally,
(e) if C(S) < C(Sbest ) then set Sbest := S.
3. Repeat at step (2) as long as desired.
4. Return Sbest and C(Sbest ).
There is no parameter to adjust for the selection of better solutions. It is
the memory encapsulated in the ranking that directs EO into the neighborhood of increasingly better solutions. Like BS, those “better” variables possess
punctuated equilibrium: their memory only get erased when they happen to be
connected to one of the variables forced to change. On the other hand, in the
choice of move to S , there is no consideration given to the outcome of such a
move, and not even the worst variable xj itself is guaranteed to improve its ﬁtness. Accordingly, large ﬂuctuations in the cost can accumulate in a sequence of
updates. Merely the bias against extremely “bad” ﬁtnesses produces improved
solutions.
Tests have shown that this basic algorithm is very competitive for optimization problems [8]. But in cases such as the single spin-ﬂip neighborhood for the
spin Hamiltonian, focusing on only the worst ﬁtness [step (2b)] leads to a deterministic process, leaving no choice in step (2c): If the “worst” spin xj has
to ﬂip and any neighbor S diﬀers by only one ﬂipped spin from S, it must be
S = (S/xj ) ∪ {−xj }. This deterministic process inevitably will get stuck near
some poor local minimum. To avoid these “dead ends” and to improve results
[8], we introduce a single parameter into the algorithm. Ranking all xi according
to ﬁtness λi , i.e., we ﬁnd a permutation Π of the variable labels i with
λΠ(1) ≤ λΠ(2) ≤ . . . ≤ λΠ(n) .

(4)

The worst variable xj [step (2b)] is of rank 1, j = Π(1), and the best variable is
of rank n. Now, consider a scale-free probability distribution over the ranks k,
Pk ∝ k −τ ,

1 ≤ k ≤ n,

(5)

for a ﬁxed value of the parameter τ . At each update, select a rank k according
to Pk . Then, modify step (2c) so that xj with j = Π(k) changes its state.
For τ = 0, this “τ -EO” algorithm is simply a random walk through Ω. Conversely, for τ → ∞, it approaches a deterministic local search, only updating the
lowest-ranked variable, and is bound to reach a dead end (see Fig. 1). However,

Self-organizing Dynamics for Optimization
n=216
n=343
n=512
n=729
n=1000

−1.76

n=8190
n=4094
n=2046
n=1022

0.06

<cost>/n

<Energy>/n

−1.75

−1.77

389

0.05

−1.78

1.0

1.2

1.4

τ

1.6

1.8

1.1

1.2

1.3

1.4

τ

1.5

1.6

1.7

Fig. 1. Plot of costs obtained by EO for a ±J spin glass (left) and for graph bipartitioning (right), both as a function of τ . For each size n, a number of instances were
generated. For each instance, 10 diﬀerent EO runs were performed at each τ . The results were averaged over runs and instances. Although both problems are quite distinct,
in either case the best results are obtained at a value of τ with τ → 1+ for n → ∞

for ﬁnite values of τ the choice of a scale-free distribution for Pk in Eq. (5) ensures that no rank gets excluded from further evolution while maintaining a bias
against variables with bad ﬁtness. In all problems studied, a value of
τ − 1 ∼ 1/ ln n

(n → ∞)

(6)

seems to work best [9, 10]. We have studied a simple model problem for which the
asymptotic behavior of τ -EO can be solved exactly [6]. The model reproduces
Eq. (6) exactly in cases where the model develops a “jam” amongst its variables,
which is quite a generic feature of frustrated systems.
In Fig. 2 we show the range of states that are sampled during a typical run
of EO, here for a spin-glass instance with n = 73 and for the image alignment
problem [19]. Starting with a random initial condition, for the ﬁrst O(n) update
steps EO establishes local order, leading to a rapid decrease in the energy. After

Cost

10000

1000

100

1000

Updates

Fig. 2. Plots of the range of states attained by EO during single run on a particular
instance (of an L = 7 cubic spin glass (left) and of the image alignment problem [19]
(right). After an initial transient, the ultimate “steady state” is reached in which EO
ﬂuctuates widely through near-optimal conﬁgurations, obtaining increasingly better
energy records ( ) while scaling ever higher barriers ( )

390

S. Boettcher

that EO searches through a wide band of states with frequent returns to nearoptimal conﬁgurations.

3

Numerical Results for EO

In the few years since we ﬁrst proposed (EO) as a general purpose heuristic
for some of the hardest combinatorial optimization problems [8], ample evidence has been provided for its practicality [9, 10, 11]. Our own studies have
focused on demonstrating elementary properties of EO in a number of implementations for classic NP-hard combinatorial problems such as graph bipartitioning [8, 10], 3-coloring [11], spin glasses [9], and the traveling salesperson [8].
Several other researchers have picked up on our initial results, and have successfully applied EO to problems as diverse as pattern recognition [19], signal
ﬁltering of EEG noise [24], artiﬁcial intelligence [18], and 3d spin-glass models [12, 23]. Comparative studies have shown that EO holds signiﬁcant promise
to provide a new, alternative approach to approximate many intractable problems [8, 12, 18].
3.1

Results on Spin Glasses

To gauge τ -EO’s performance for larger 3d-lattices, we have run our implementation also on two instances, toruspm3-8-50 and toruspm3-15-50, with n = 512 and
n = 3375, considered in the 7th DIMACS challenge for semi-deﬁnite problems1 .
The best available bounds (thanks to F. Liers) established for the larger instance
are Hlower = −6138.02 (from semi-deﬁnite programming) and Hupper = −5831
(from branch-and-cut). EO found HEO = −6049 (or H/n = −1.7923), a signiﬁcant improvement on the upper bound and already lower than limn→∞ H/n ≈
−1.786 . . . found in Ref. [9]. Furthermore, we collected 105 such states, which
roughly segregate into three clusters with a mutual Hamming distance of at least
100 distinct spins; though at best a small sample of the ≈ 1073 ground states
expected [15]! For the smaller instance the bounds given are −922 and −912,
while EO ﬁnds −916 (or H/n = −1.7891) and was terminated after ﬁnding 105
such states. While this run (including sampling degenerate states) took only a
few minutes of CPU (at 800 MHz), the results for the larger instance required
about 16 hours.
More recently, we have combined EO with reduction methods for sparse
graphs [4, 5]. These reductions strip graphs of all low-connected variables (α ≤
3), thereby eliminating many entropic barriers that tend to bog down local
searches [22]. Along the way, the rules allow for an accounting of the exact
ground-state energy and entropy, and even of the approximate overlap distribution [5]. The “remainder” graph is subsequently handled eﬃciently with EO.
With such a meta-heuristic approach, for example, we have been able to determine the defect energy distribution [13] for d = 3, . . . , 7 dimensional spin
1

http://dimacs.rutgers.edu/Challenges/Seventh/

Self-organizing Dynamics for Optimization

391

glasses, bond-diluted to just above their percolation point, with great accuracy
for lattices up to L = 30 [4]. As one result, we reduced the error on the stiﬀness
exponent in d = 3, yd=3 = 0.240(5), from 20% to about 2%. This fundamental exponent describes the energetic cost ∆E of perturbations (here, induced
interfaces) of size L, σ(∆E) ∼ Ly .
Currently, we are using this meta-heuristic to explore the (possible) onset of
replica symmetry breaking (RSB) for sparse mean-ﬁeld and lattice models just
above percolation. So far, we have only some preliminary data for Spin glasses
on random graphs. In this model at connectivities near percolation α ≈ αp = 1,
many spins may be entirely unconnected while a ﬁnite fraction is suﬃciently
connected to form a “giant component” in which interconnected spins may become overconstrained. There the reduction rules allow us to reduce completely
a statistically signiﬁcant number of graphs with up to n = 218 spins even well
above αp , since even higher-connected spins may become reducible eventually
after totally reducible substructures (trees, loops, etc) emanating from them
have been eliminated. At the highest connectivities reached, even graphs originally of n = 218 had collapsed to at most 100 irreducible spins, which EO easily
optimized.

0.4

Data
Scaling Fit

Entropy per Spin

10

Cost

8
6
4
2
0

0.9

1

1.1

1.2

Connectivity

1.3

0.3

0.9

1

1.1

1.2

1.3

Connectivity

Fig. 3. Plot (left) of the cost and (right) of the entropy per spin, as a function of
connectivity α for random graphs of size n = 28 , 29 , . . . , 218 . For increasing n, the cost
approaches a singularity at αcrit = 1.003(9), as determined from a ﬁnite-size scaling ﬁt
(lines on left) to C (α, n) ∼ nδ f (α − αcrit ) n1/ν . The ﬁt predicts also δ = 0.11(2)
and ν = 3.0(1). The entropy per spin quickly converges to ≈ (1 − α/2) ln 2 (dashed
line), exact for α < αcrit = 1, continues unaﬀected through the transition, but deviates
from that line for α > αcrit

As a result, we have measured the cost of ground states, Eq. (2), as a function of connectivity α on 40 000 instances for each size n = 28 , 29 , . . . , 214 , and
400 instances for n = 215 , . . . , 218 , at each of 20 diﬀerent connectivities α as
shown in Figure 3. We also account exactly for the degeneracy of each instance,
18
which could number up to exp[0.3 × 218 ]; minuscule compared to all 22 conﬁgurations! Not entirely reduced graphs had their entropy determined with EO in
our meta-heuristic. Consistent with theory [17], Figure 3 shows that the entropy
per spin follows s ≈ (1 − α/2) ln 2 for α < αcrit = 1, then continues smoothly

392

S. Boettcher

through the transition but deviates from that line for α > αcrit . Similar data
for the overlap-moments [5] may determine the onset of RSB expected for this
model.
3.2

Applications of EO by Others

The generality of the EO method beyond the domain of spin-glass problems
has recently been demonstrated by Meshoul and Batouche [19] who used the
EO algorithm as described above successfully on a standard cost function for
aligning natural images. Fig. 4 demonstrates the results of their implementation of τ -EO for this pattern recognition problem. Here, τ -EO ﬁnds an optimal
aﬃne transformation between a target image and its reference image using a
set of n adjustable reference points which try to attach to characteristic features
of the target image. The crucial role played by EO’s non-equilibrium fluctuations in the local search is demonstrated in Fig. 2. The ﬂuctuations in the
image alignment problem are amazingly similar to those we have found for spin
glasses. As our discussion in Sec. 2 suggests, they are one of the key distinguishing features of EO, and are especially relevant for optimizing highly disordered systems. For instance, Dall and Sibani [12] have observed a signiﬁcantly
broader distribution of states visited – and thus, better solutions found – by
τ -EO compared to simulated annealing [16] when applied to the Gaussian spinglass problem.

Fig. 4. Application of EO to the image matching problem, after [19]. Two diﬀerent
images of the same scene (top row and bottom row) are characterized by a set of n
points assigned by a standard pattern recognition algorithm. Starting from an initial
assignment (left, top and bottom), the points are updated according to EO, see also
Fig. 2, leading to an optimal assignment (center, top and bottom). This optimal assignment minimizes a cost function for the aﬃne transformation, facilitating an automated
alignment of the two images (right). Note that the points move to the part of the scene
for which both images overlap. Special thanks to M. Batouche for providing those
images

Self-organizing Dynamics for Optimization

393

Acknowledgements
I would like to thank M. Paczuski, A.G. Percus, and M. Grigni for their collaboration on many aspects of the work presented here. This work was supported
under NSF grant DMR-0312510 and Emory’s URC.

References
1. P. Bak, How Nature Works (Springer, New York, 1996).
2. P. Bak and K. Sneppen, Punctuated Equilibrium and Criticality in a simple Model
of Evolution, Phys. Rev. Lett. 71, 4083-4086 (1993).
3. P. Bak, C. Tang, and K. Wiesenfeld, Self-Organized Criticality, Phys. Rev. Lett.
59, 381 (1987).
4. S. Boettcher, Low-Temperature Excitations of Dilute Lattice Spin Glasses, Europhys. Lett. 67, 453-459 (2004).
5. S. Boettcher, Reduction of Spin Glasses applied to the Migdal-Kadanoﬀ Hierarchical Lattice, Euro. Phys. J. B 33, 439-445 (2003).
6. S. Boettcher and M. Grigni, Jamming Model for the Extremal Optimization Heuristic, J. Math. Phys. A: Math. Gen. 35, 1109-1123 (2002).
7. S. Boettcher and M. Paczuski, Ultrametricity and Memory in a Solvable Model of
Self-Organized Criticality, Physical Review E 54, 1082 (1996).
8. S. Boettcher and A. G. Percus, Nature’s Way of Optimizing, Artiﬁcial Intelligence
119, 275-286 (2000).
9. S. Boettcher and A. G. Percus, Optimization with Extremal Dynamics, Phys. Rev.
Lett. 86, 5211-5214 (2001).
10. S. Boettcher and A. G. Percus, Extremal Optimization for Graph Partitioning,
Phys. Rev. E 64, 026114 (2001).
11. S. Boettcher and A. G. Percus, Extremal Optimization at the Phase Transition of
the 3-Coloring Problem, Physical Review E 69, 066703 (2004).
12. J. Dall and P. Sibani, Faster Monte Carlo Simulations at Low Temperatures: The
Waiting Time Method, Computer Physics Communication 141, 260-267 (2001).
13. K. H. Fischer and J. A. Hertz, Spin Glasses (Cambridge University Press, Cambridge, 1991).
14. S. J. Gould and N. Eldridge, Punctuated Equilibria: The Tempo and Mode of Evolution Reconsidered, Paleobiology 3, 115-151 (1977).
15. A. K. Hartmann, Ground-state clusters of two-, three- and four-dimensional +-J
Ising spin glasses, Phys. Rev. E 63, 016106 (2001).
16. S. Kirkpatrick, C. D. Gelatt, and M. P. Vecchi, Optimization by simulated annealing, Science 220, 671-680 (1983).
17. M. Leone, F. Ricci-Tersenghi, and R. Zecchina, Phase coexistence and ﬁnite-size
scaling in random combinatorial problems, J. Phys. A. 34, 4615 (2001).
18. M. B. Menai and M. Batouche, Approximate solution of Max-SAT problem using
Extremal Optimization heuristic, Journal of Automated Reasoning, (to appear).
19. S. Meshoul and M. Batouche, Robust Point Correspondence for Image Registration
using Optimization with Extremal Dynamics, Lect. Notes Comput. Sc. 2449, 330337 (2002).
20. M. Mezard, G. Parisi, and M. A. Virasoro, Spin Glass Theory and Beyond (World
Scientiﬁc, Singapore, 1987).

394

S. Boettcher

21. D. M. Raup and J. J. Sepkoski, Periodic Extinction of Families and Genera, Science
231, 833-836.
22. F. Ricci-Tersenghi, M. Weigt, and R. Zecchina, Simplest random K-satisﬁability
problem, Phys. Rev. E 63, 026702 (2001).
23. J.-S. Wang and Y. Okabe, A comparison of extremal optimization with ﬂathistogram dynamics for ﬁnding spin-glass ground states, J. Phys. Soc. Jpn. 72,
1380-1383 (2003).
24. E. Yom-Tov, A. Grossman, and G. F. Inbar, Movement-related potentials during the
performance of a motor task I: The eﬀect of learning and force, Bio. Cybernatics
85, 395-399 (2001).

