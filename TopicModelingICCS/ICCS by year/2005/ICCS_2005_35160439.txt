Storage QoS Control with Adaptive I/O
Deadline Assignment and Slack-Stealing EDF
Young Jin Nam and Chanik Park†
School of Computer and Information Technology,
Daegu University,
Kyungbuk, Republic of Korea
yjnam@daegu.ac.kr
†
Department of Computer Science and Engineering/PIRL,
Pohang University of Science and Technology,
Kyungbuk, Republic of Korea
cipark@postech.ac.kr

Abstract. Storage QoS control enforces a given storage QoS requirement for each I/O request from diﬀerent storage clients that share an
underlying storage system. This paper proposes an eﬃcient storage QoS
control scheme that features adaptive I/O deadline assignment and slackstealing EDF scheduling. Simulation results with various I/O workloads
show that the proposed scheme outperforms previous approaches in
terms of response time variation, average response times, and miss ratio
of the target response time.

1

Introduction

Embedding QoS feature into a storage system needs to deﬁne storage QoS speciﬁcations, map the storage QoS speciﬁcations (requirements) onto the underlying storage
resources, and enforce the storage QoS requirements for each I/O request from different virtual disks (storage clients). This paper mainly emphasizes the storage QoS
enforcement, also called the real-time QoS control (brieﬂy QoS control). It is generally
accepted that a storage system is characterized by its I/O performance; that is, the
IOPS and RT relationship that depicts the variation of an average response time as a
function of I/O requests per second (brieﬂy IOPS). Thus, our QoS speciﬁcation should
capture this basic feature of a storage system as the ﬁrst step. While the storage QoS
speciﬁcation in a broad sense may encompass other features of a storage system, such
as data reliability, system costs, our QoS speciﬁcation focuses mainly on the aspect
of storage I/O performance that includes an average request size, a target response
time, and a target IOPS. We deﬁne a QoS requirement of the virtual disk as a storage
service required from a virtual disk in terms of QoS speciﬁcation. The QoS requirement from a virtual disk i (V Di ) is represented as (SZi , IOP Sitarg , RTitarg ), where SZi
represents an average I/O request size(KB), IOP Sitarg represents a target IOPS, and
RTitarg represents a target response time(msec) [1]. The QoS requirement can be easily
expanded to support a storage cluster environment and a more detailed speciﬁcation
having multiple pairs of a target IOPS and a target response time [1].
A few QoS control schemes for storage resources have been introduced [1, 2, 3, 5].
We can categorize the characteristics of the previous schemes into three classes. Class 1
V.S. Sunderam et al. (Eds.): ICCS 2005, LNCS 3516, pp. 439–446, 2005.
c Springer-Verlag Berlin Heidelberg 2005

440

Y.J. Nam and C. Park

includes the derivatives of packet-based fair queuing schemes for network resources [3].
It proportionates the entire storage bandwidth according to a given set of resource
weights allotted to each virtual disk that shares the same storage system. In addition,
it attempts to reorder a given I/O sequence in order to reduce overhead caused by disk
head movements. Note that they do not directly take control of the demanded response
time; instead, they control only the storage bandwidth. Class 2 operates mainly on a
rate-based QoS control using a leaky bucket [5]. It attempts to guarantee a given
QoS requirement simply by throttling the IOPS of the incoming I/O requests from
each virtual disk. Class 2 is expected to have the same drawbacks as Class 1. Class 3
guarantees the target response time by assigning a deadline time to each incoming I/O
request only if the current IOPS is not greater than its target IOPS and then scheduling
the pending I/O requests according to the EDF (Earliest Deadline First) scheme [2].
If the current IOPS is greater than its target IOPS, I/O requests have no deadline.
Otherwise, the deadline is set by adding the target response time to the current time.
Let us call this type of I/O deadline assignment target-IOPS-based I/O assignment.
In contrast to Class 1 and 2, this approach directly controls the target response time
for a given target IOPS. It can also support a QoS requirement with multiple pairs of
a target IOPS and a target response time.

2

The Proposed Scheme

The key features of the proposed QoS control scheme include the adaptive I/O deadline
assignment based on the current IOPS and the current queue depth of a virtual disk,
and slack-stealing EDF scheduling that exploits any available slack between the current
time and the earliest deadline time to minimize the underlying storage overhead. Note
that the proposed scheme falls into Class 3 that takes control of both the target IOPS
and the target RT for each I/O request from diﬀerent virtual disks.

Adaptive I/O Deadline Assignment. The key idea is to adaptively determine a deadline time of each I/O request according to the current IOPS and the current queue depth
for each virtual disk. First, the proposed assignment scheme obtains an actual target
response time denoted by act RTitarg of V Di according to the current IOPS condition
with respect to its target IOPS, as given in Algorithm 1. If the current IOPS is higher
than its target IOPS, an I/O request is served as if it were a best-eﬀort I/O request
having no deadline. If the current IOPS is equal to the target IOPS, the actual target
RT is the same as the original target RT. If the current IOPS is lower than its target
IOPS, its actual target RT decreases in proportion to the ratio of the current IOPS
to the target IOPS. Second, the proposed assignment scheme empirically measures the
for each V Di . It computes a unit
current average queue depth denoted by qdepthtarg
i
for act RTitarg , as given in Algorithm 1.
target response time denoted by unit rttarg
i
is meaningful only if the current IOPS is not greater than
Note that the unit rttarg
i
the target IOPS. Finally, the proposed assignment scheme assigns the deadline of rik
as a function of the current position in the queue denoted by qposcur
(rik ); that is, the
i
deadline of an I/O request increases from the queue head in a piece-wise linear manner
. In consequence, the proposed assignment scheme
up to its target RT by unit rttarg
i
can avoid delays with the processing of I/O requests from a virtual disk having a larger
target RT until all the I/O requests from virtual disks having a smaller target RT are
processed.

Storage QoS Control with Adaptive I/O Deadline Assignment

441

Algorithm 1: Adaptive I/O deadline assignment
: rik , IOP Sicur , Tcur

input

output : deadline(rik )
begin
if (IOP Sicur > IOP Sitarg ) then
deadline(rik ) ← Tbe ;
else
Tbe
act RTitarg ←
IOP Sicur
RTitarg IOP S targ

deadline(rik )

if IOP Sicur ≤ IOP Sitarg

i

targ

act RTi
targ
qdepthi

unit rttarg
←
i

if IOP Sicur > IOP Sitarg

;

;

← Tcur + min{RTitarg , unit rttarg
×qposi (rik )};
i

end
end

0
+∞

0
+∞

current IOPS = 6IOPS
act_RT = Tbe
0
+10

0
+10

0
+10

0
+10

0
+∞

0
+∞

0
+∞

VD1

0
+∞

0
+16

(4KB, 5IOPS, 16msec)

0
+8

0
+6

0
+4

0
+2

I/O Sched

VD2

0
+10

0
+10

0
+10

0
+8

0
+4

VD1

0
+10

0
+8

0
+6

0
+4

0
+2

I/O Sched

VD2

(4KB, 8IOPS, 10msec)

(a) heavier

(b) equal

0
+9.6

0
+6.4

0
+3.2

VD1

current IOPS = 3IOPS
(4KB, 5IOPS, 16msec)
act_RT = 9.6msec, avg_qdepth=3
0
+10

0
+12

current IOPS = 5IOPS
(4KB, 5IOPS, 16msec)
act_RT = 16msec, avg_qdepth=4

current IOPS = 8IOPS
(4KB, 8IOPS, 10msec)
act_RT = 10ms, avg_qdepth=5

0
+10

0
+16

0
+10

0
+10

0
+8

0
+6

0
+4

0
+2

I/O Sched

VD2

(4KB, 8IOPS, 10msec)

(c) lighter
Fig. 1. Examples of the adaptive I/O deadline assignment with IOP S2cur = IOP S2targ :
(a) heavier (IOP S1cur > IOP S1targ ), (b) equal (IOP S1cur = IOP S1targ ), (c) lighter
(IOP S1cur = IOP S1targ )

Figure 1 show three examples of the adaptive I/O deadline assignment scheme.
Each example assumes that Q1 and Q2 for V D1 and V D2 are (4KB, 5IOPS, 16msec)
and (4KB, 8IOPS, 10msec), respectively. First, Figure 1 presents an example for the
condition that IOP S1cur > IOP S1targ and IOP S2cur = IOP S2targ . Since the current
IOPS of V D1 is higher than its target IOPS, the I/O deadline of each I/O request for
V D1 is set to Tbe , implying no deadline. By contrast, since the current IOPS of V D2
is equal to its target IOPS, its actual target RT of act RT2targ is 10msec, as with its
original target RT. Assuming that the observed average queue depth is 5, we obtain

442

Y.J. Nam and C. Park

= 2 msec. Finally, the deadline of each I/O request from the queue
that unit rttarg
2
head increases from 2msec up to 10msec by 2msec.
Second, Figure 1(b) shows an example of the proposed deadline assignment when
the current IOPS of both storage clients are the same as their target IOPS; that is,
IOP S1cur = IOP S1targ and IOP S2cur = IOP S2targ . The I/O deadline assignment of
V D1 is performed as with V D2 in the previous example. Since the current IOPS of V D1
is equal to its target IOPS, its actual target RT of act RT1targ is 20msec. Assuming that
= 5 msec. Finally, the
the observed average queue depth is 4, we obtain that unit rttarg
1
deadline of each I/O request from the queue head starts from 5msec and increases up to
20msec by 5msec. The I/O deadline assignments by the proposed scheme for V D1 and
V D2 in Figure 1(b) reveals that the I/O scheduler can choose the I/O requests from
each queue in a fair manner. Third, Figure 1(c) presents an illustrative example for
the condition that the current IOPS of V D1 is smaller than its target IOPS. To begin,
the actual target RT of act RT1targ decreases to 9.6msec in proportion to the ratio of
the current IOPS to its target IOPS. Assuming that the observed average queue depth
= 3.2 msec. Finally, the deadline of each I/O request
is 3, we obtain that unit rttarg
1
from the queue head starts from 3.2msec and increases up to 9.6msec by 3.2msec.
Finally, observe that the proposed assignment scheme determines a deadline time of
each I/O request adaptively to the changing IOPS by adjusting its target response time
and by increasing the I/O deadline by its unit target response time from the queue
head. Consequently, we expect that the adaptive I/O deadline assigner will be able to
give better fairness in servicing I/O requests with low RT variations in response times
and provide better performance isolation to diﬀerent virtual disks that share the same
storage system.

Slack-Stealing EDF Scheduling: Another feature of the proposed QoS control scheme
is to exploit the available slack between the current time and the earliest deadline time
statistically in order to minimize storage overhead. The proposed scheduling algorithm
selects an I/O request that not only minimizes the underlying storage overhead when
scheduled, but also causes no deadline miss for the I/O request having the earliest
deadline time. The proposed scheduling algorithm operates in two steps. First, it determines a set of I/O requests that entail no deadline miss for the I/O request with
the earliest deadline, denoted by Religible . Next, it selects an I/O request that is likely
to minimize the underlying storage overhead caused mainly by mechanical disk head
movements. Then, the design of the proposed scheduling algorithm raises the following
two issues: how to predict the service time of an I/O request and how to estimate
storage overhead caused by scheduling the I/O request.
The proposed scheduling algorithm needs to compute the service time of a given
I/O request in order to exploit or steal any existing slack time between the current time
and the time when the service of the I/O request having the earliest deadline should be
started to meet its deadline. Unfortunately, it is not possible to precisely predict the
I/O service time under either a disk or a storage system. Previous research to estimate
the I/O service time exists based on a theoretical model of disks or storage systems [6].
However, this approach has the following drawbacks. First, it cannot capture the feature
of a time-varying service time according to changes in I/O workload patterns. Second,
modeling a disk or a storage system requires understanding the detailed architectures
for the disk or the storage system that are generally unavailable. Thus, the proposed
scheduling algorithm assumes that the I/O service time is time-variant, and it measures
the time-varying I/O service time by monitoring the service time of each I/O request
as used in [4]. That is, it collects the service time of each I/O request during a given

Storage QoS Control with Adaptive I/O Deadline Assignment

443

monitoring interval and then averages out the service times during the interval. We
denote with serv time(t) the current I/O service time at time t. Note that a single
I/O service time is used for all virtual disks that share the underlying storage, because
the service time does not include the queuing delay in the pending queues. Storage
overhead of a single disk is equal to the overhead time to move from the current head
position to the position to serve the I/O request. An exact computation of overhead
time demands to estimate a seek time and a rotational delay between two I/O requests.
However, in large-scale storage systems typically equipped with a large non-volatile
cache memory, the exact estimation of storage overhead becomes almost impossible.
Thus, the proposed scheduling algorithm simply estimates the overhead time between
two I/O requests as the absolute distance between the start block address of the given
I/O request and that of its previous I/O request.
Algorithm 2 gives the description on the proposed scheduling algorithm. We assume
that N virtual disks share the same storage system. Recall that P Qi represents the
I/O pending queue dedicated to V Di . The notation of rih represents the I/O request
at the head of P Qi . Denote with addr(rik ) the start block address of rik . We deﬁne a
set of eligible I/O requests Religible that resides at the head of the I/O pending queue
and entails no deadline miss for the I/O request that has the earliest deadline time.

Algorithm 2: Slack-stealing EDF scheduling
Tcur ← current time;
serv time(t) ← current I/O service time;
assume that deadlineearliest = deadline(reh );
addrlast ← the start block address scheduled at last;
// determine Religible set
if Tcur + serv time(t) ≤ deadlineearliest − serv time(t) then
Religible ← {rih |rih ∈ P Qi and P Qi = ∅};
ﬁnd rsh , abs(addr(rsh ), addrlast ) = minrh ∈Religible {abs(addr(rih ), addrlast )};
i

else
rsh ← reh ;
endif
remove rsh from P Qs and schedule it to the underlying storage;

In order to determine Religible , the proposed scheduling algorithm examines only
the ﬁrst I/O request at each queue in order to minimize the I/O scheduling overhead.
When the proposed scheduling algorithm inspects all the pending I/O requests within
all the queues, its time complexity becomes O(N + M ), where N is the number of the
virtual disks that share the storage system and M is the maximum number of I/O
requests within the pending queues. Considering M >> N , the approach that checks
all pending I/O requests is expected to cause a considerable amount of overhead for
I/O scheduling with the increase of the number of the pending I/O requests and the
number of virtual disks that share the storage system. By contrast, the time complexity
of the proposed scheduling algorithm is only O(N ).

444

3

Y.J. Nam and C. Park

Performance Evaluations

We evaluate the performance of the proposed QoS control scheme on our storage
simulator that consists of an I/O workload generator, a set of virtual disks (storage clients), an underlying storage system. (See [1] for more details.) We also assume
the following simulation environments. Two virtual disks named V D1 and V D2 issue only read I/O requests. The QoS requirement of each virtual disk is deﬁned
as (SZ1targ , IOP S1targ , RT1targ ) = (4KB, 45IOPS, 70msec) and (SZ2targ , IOP S2targ ,
RT2targ ) = (4KB, 45IOPS, 100msec). Note that the underlying storage system can
satisfy the given QoS requirements properly. We determined the QoS requirements
empirically by measuring the I/O performance of the underlying storage system for
the given I/O workloads.

Adaptive I/O Deadline Assignment: To begin, we focus on evaluating the eﬀectiveness of the proposed assignment scheme by disabling the slack-stealing EDF scheme;
instead, the EDF scheduler is employed as an I/O scheduler. We use four workload
sets: W S1d , W S2d , W S3d , and W S4d . Each workload set is characterized by diﬀerent I/O
traﬃc intensity from V D2 ; that is, the current IOPS of V D2 becomes higher than its
target IOPS in W S1d (IOP S2cur > IOP S2targ ), the current IOPS of V D2 is equal to
its target IOPS in W S2d (IOP S2cur = IOP S2targ ), the current IOPS of V D2 becomes
lower than its target IOPS in W S3d (IOP S2cur < IOP S2targ ), and the current IOPS of
V D2 becomes much lower than its target IOPS in W S4d (IOP S2cur << IOP S2targ ). We
can expect that the proposed assignment scheme will outperform the previous targetIOPS based I/O assignment under the workload sets of W S3d and W S4d , where the
current IOPS of V D2 becomes smaller than its target IOPS. Table 1 compares the
performance results of the target-IOPS-based I/O deadline assignment and the proposed assignment scheme. As expected, the proposed assignment scheme reduced the
variation of response times of V D2 in W S3d and W S4d On the contrary, it increased the
variation of response times of V D1 , whereas the target RT miss ratio still remains zero.
In summary, the simulations results veriﬁed that the proposed assignment scheme could
overcome the drawbacks of the target IOPS-based I/O deadline assignment; that is, a
high RT variation of response times due to unfairness in the processing of I/O requests
from virtual disks having a larger target response time, and poor performance isolation
by assigning its deadline based on its original target RT regardless of the current IOPS.
Slack-Stealing EDF Scheduling: Recall that the proposed scheduling algorithm selects an I/O request that not only minimizes the underlying storage overhead when
scheduled, but also causes no deadline miss for the I/O request having the earliest
deadline time. Note that the scheduling algorithms under test will employ the adaptive
I/O deadline assigner. Three workload sets, W S1s , W S2s , and W S3s , are used for this
performance evaluation. The I/O traﬃc intensity for each virtual disk in W S1s is equal
to the given QoS requirements; that is, IOP S1targ and IOP S2targ . However, the I/O
traﬃc intensity for each virtual disk in W S2s and W S3s is increased by 10% and 20%
respectively, compared with its target IOPS. Table 2 compares the performance results
of the EDF scheduling and the proposed scheduling scheme. Observe that the proposed
scheduling scheme reduced the target response time(RT) miss ratio of V D1 and V D2
in W S2s and W S3s by improving the response times of each virtual disk. To summarize,
the simulation results veriﬁed that the slack-stealing EDF scheduling algorithm could
reduce storage overhead by reordering the I/O requests as long as the deadline times
of I/O requests are not missed, resulting in better target RT miss ratios and average
response times, compared with the EDF scheduling algorithm.

Storage QoS Control with Adaptive I/O Deadline Assignment

445

Table 1. Summary of the performance results of the adaptive I/O deadline assignment
Avg. IOPS (IOPS) Avg. RT (msec)
Var. RT
Targ. RT miss ratio
targ-IOPS
prop targ-IOPS prop targ-IOPS prop targ-IOPS
prop
W S1d V D1
V D2
W S2d V D1
V D2
W S3d V D1
V D2
W S4d V D1
V D2

43.5
67.7
44.8
44.0
44.6
24.7
44.5
5.0

43.4
68.0
44.0
44.7
44.2
25.0
44.5
5.1

41.4
122.1
22.8
26.9
16.4
16.8
12.4
14.0

40.8
118.8
22.1
27.5
17.0
15.9
12.6
12.5

–
–
105.0
159.5
14.6
20.8
2.2
17.9

–
–
73.6
161.4
12.7
17.3
2.6
6.5

0.21
0.58
0
0
0
0
0
0

0.19
0.58
0
0
0
0
0
0

Table 2. Summary of the performance results of the slack-stealing I/O scheduling
Avg. IOPS (IOPS) Avg. RT (msec) Targ. RT miss ratio
EDF
prop EDF
prop EDF
prop
W S1s V D1
V D2
W S2s V D1
V D2
W S3s V D1
V D2

4

44.0
44.7
48.7
49.1
53.4
52.2

44.5
44.4
49.2
49.1
53.8
53.1

22.1
27.5
28.2
40.8
45.1
58.8

22.4
23.6
28.5
29.8
39.3
41.0

0.00
0.00
0.03
0.02
0.16
0.10

0.00
0.00
0.02
0.00
0.07
0.01

Conclusion and Future Work

This paper proposed an eﬃcient QoS control scheme that enforces the QoS requirements of multiple virtual disks (or storage clients) that share the same storage system.
The proposed QoS control scheme consists of two key components: the adaptive I/O
deadline assignment and the slack-stealing EDF scheduling for storage systems. The
key of the adaptive I/O deadline assignment is to adaptively determine the deadline
time of each I/O request according to the current IOPS and the current queue depth.
Thus, it could overcome the drawbacks of the target IOPS-based deadline assignment:
a high RT variation of response times due to unfairness in the processing of I/O requests from virtual disks having a larger target response time, and poor performance
isolation by assigning its deadline based on its original target RT regardless of the
current IOPS. The key of the slack-stealing EDF scheduling is to steal any available
slack between the current time and the earliest deadline time in order to minimize
the underlying storage overhead. The proposed scheduling algorithm selects an I/O
request that minimizes the underlying storage overhead when scheduled, while causing
no deadline miss for the I/O request having the earliest deadline time. We raised two
design issues concerning how to predict the I/O service time of an I/O request and
how to estimate storage overhead (disk head movement) caused by scheduling an I/O
request, and provided reasonable solutions. We implemented the proposed QoS control
scheme on our storage simulator. The simulation results for the adaptive I/O deadline

446

Y.J. Nam and C. Park

assignment under various competing I/O workload sets showed that the proposed assignment scheme not only provides better fairness with lower RT variation, but also
assures a better performance isolation for each virtual disk. Performance evaluations
for the slack-stealing EDF scheduling revealed that the proposed scheduling scheme
could provide better target RT miss ratios and response times by reducing storage
overhead under various I/O workload sets.
In future work, we plan to implement the proposed QoS control scheme on top
of an actual storage system and evaluate its performance with actual I/O traﬃc. In
addition, we need to evaluate diﬀerent techniques to predict an I/O service time and
storage overhead for the slack-stealing EDF scheduling.

Acknowledgments
This research was supported by the Daegu University Research Grant, No 20040825.
This research has been also supported in part by the Ministry of Education of Korea
for its support toward the Electrical and Computer Engineering Division at POSTECH
through its BK21 program, in part by HY-SDR IT Research Center, and in part by
grant No. R01-2003-000-10739-0 from the Basic Research Program of the Korea Science
and Engineering Foundation.

References
1. Y. J. Nam, Dynamic Storage QoS Control for Storage Cluster and RAID Performance Enhancement Techniques. Ph.D Dissertation, POSTECH, February 2004.
2. C. Lumb, A. Merchant, and G. Alvarez, “Facade: Virtual storage devices with performance guarantees,” in Proceedings of Conference on File and Storage Technologies,
March 2003.
3. Y. Nam and C. Park, “A new proportional-share disk scheduling algorithm: Tradingoﬀ I/O throughput and qos guarantees,” Lecture Notes in Computer Science,
vol. 1067, pp. 257–266, June 2003.
4. A. Chandra, W. Gong, and P. Shenoy, “Dynamic resource allocation for shared
data centers using online measurements,” in Proceedings of the 11th International
Workshop on Quality of Service, June 2003.
5. H. Lee, Y. Nam, and C. Park, “Regulating I/O performance of shared storage
with a control theoretical approach,” in Proceedings of the 21st IEEE Mass Storage
Systems Symposium/12th NASA Goddard Conference on Mass Storage Systems and
Technologies (MSST2004), April 2004.
6. M. Uysal, G. Alvarez, and A. Merchant, “A modular, analytical throughput model
for modern disk arrays,” in Proceedings of the Ninth International Symposium on
Modeling, Analysis and Simulation of Computer and Telecommunications Systems,
pp. 183–192, August 2001.

