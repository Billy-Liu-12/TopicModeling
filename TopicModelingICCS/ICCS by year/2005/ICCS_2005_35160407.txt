Design and Implementation of the Home-Based
Cooperative Cache for PVFS
In-Chul Hwang, Hanjo Jung, Seung-Ryoul Maeng, and Jung-Wan Cho
Division of Computer Science, Dept. of Electrical Engineering & Computer Science,
KAIST, 373-1 Kusung-dong Yusong-gu, Taejon, 305-701, Republic of Korea
{ichwang, hanjo, maeng, jwcho}@calab.kaist.ac.kr

Abstract. Recently, there has been much research about cluster computing to
get high performance using low-cost PCs connected with high-speed interconnection networks. In many research areas, many distributed file systems
have been developed. In many distributed file systems, PVFS (Parallel Virtual
File System) provides users with high bandwidth by stripping data over I/O
servers. In PVFS, there is no file system cache. For a new file system cache for
PVFS, we designed and implemented cooperative cache for PVFS (CoopcPVFS). Because the previous developed Coopc-PVFS is a hint-based
cooperative cache, a cache manager reads/writes files using approximately
correct information so that it has a little read/write overhead. And previous
studies about cooperative cache are only focused on shared read data and don’t
care about write performance. In this paper, we describe the design and
implementation of the home-based Coopc-PVFS to improve read/write
performance. Also, we evaluate and analysis the performance of the homebased Coopc-PVFS in comparison to PVFS and to the hint-based Coopc-PVFS.

1 Introduction
Recently, there has been much research about cluster computing to get high
performance using low-cost PCs connected with high-speed inter-connection
networks. For an efficient usage of cluster computing, many efficient components of
cluster is necessary - efficient interconnection networks, Operating System supports
and etc. Among many research areas, many distributed file systems which access
disks slower than any other component in cluster computing have been developed.
Among research areas of distributed file system, cooperative cache [3, 4, 5] was
proposed to reduce servers' load and to get high performance. Because the access time
of other clients’ memories is faster than that of servers’ disks, to get a block from
other clients’ memories is faster than to get the block from servers’ disk. In
cooperative cache, a client finds a block first from its own file system cache, and then
other clients' file system caches before getting the block from servers' disks.
In many distributed file systems, PVFS (Parallel Virtual File System) [1] provides
users with high bandwidth by stripping data over I/O servers. There is no file system
cache in PVFS - PVFS just supports data transfers from/to I/O servers. For a new file
system cache for PVFS, we designed and implemented cooperative cache for PVFS
(Coopc-PVFS) [9].
V.S. Sunderam et al. (Eds.): ICCS 2005, LNCS 3516, pp. 407 – 414, 2005.
© Springer-Verlag Berlin Heidelberg 2005

408

I.-C. Hwang et al.

Because the previous developed Coopc-PVFS is a hint-based cooperative cache, a
cache manager reads/writes files using approximately correct information. If a hint is
incorrect, read/write overhead can be large. And previous studies about cooperative
cache are only focused on shared read data and don't care about write performance.
To solve this problem, we design and implement the home-based Coopc-PVFS. In the
home-based Coopc-PVFS, every cache manager manages exact information with little
management overhead and written data can be buffered in home nodes. We also
evaluate and analysis the performance of the home-based Coopc-PVFS in comparison
to PVFS and to the hint-based Coopc-PVFS.
This paper is organized as follows. In the next section, we present about PVFS and
cooperative cache. In section 3, we describe about Coopc-PVFS and the previous
developed hint-based Coopc-PVFS. In section 4, we present the design and
implementation of the home-based Coopc-PVFS. In section 5, we evaluate and
analysis the performance of the home-based Coopc-PVFS in comparison to PVFS and
to the hint-based Coopc-PVFS. Finally, we summarize major contributions of this
work and discuss future work in section 6.

2 Related Work
2.1 PVFS (Parallel Virtual File System)
PVFS consists of compute nodes, a metadata manager and I/O servers. The compute
nodes are clients that use PVFS services. A metadata manager manages metadata of
PVFS files. The I/O servers store actual data of PVFS files. In PVFS, a file data is
stripped over I/O servers.
There was a study for the file system caching effect of PVFS. Vilayannur et al. [2]
designed and implemented a file system cache of a client. They showed that a file
system cache in a client is efficient if many applications in the client share files
among them. But their research was limited to a file system cache in a single node.
2.2 Cooperative Cache
Cooperative cache [3, 4, 5] was proposed to reduce servers’ load and to get high
performance. In cooperative cache, if a file system cache in a client doesn’t handle a
request to a file, the client sends the request to the other client’s cache that caches the
file rather than to the server because the access time of another client’s memory is
faster than that of the server’s disk. Servers’ load can be reduced in cooperative cache
so that it is scalable as the number of clients increases. Because there is much more
memory in the cooperative cache than in a single file system cache, the cooperative
cache can handle more requests and improve overall system performance.
There have been many studies about cooperative caching. Dahlin et al. [3]
suggested the efficient cache management scheme called N-chance algorithm, Feeley
et al. [4] suggested another efficient cache management scheme called modified Nchance algorithm in GMS (Global Memory Service). Sarkar et al. [5] suggested the
hint-based cooperative caching to reduce the management overhead using hint. Thus,
the hint-based cooperative cache is scalable and can be adopted in the large-scale
system such as cluster computer.

Design and Implementation of the Home-Based Cooperative Cache for PVFS

409

3 Cooperative Cache for PVFS
3.1 Overview of Cooperative Cache for PVFS
In the PVFS, an application reads/writes a file through I/O servers without a file
system caching facility. We made Coopc-PVFS as we added a cache manager to a
PVFS client.
In figure 1, we present the workflow of Coopc-PVFS added to a PVFS client.

Fig. 1. Workflow of Coopc-PVFS

After we added a cache manager to a PVFS client, a cache manager can cache data
and does cooperation with other clients’ cache managers.
3.2 Hint-Based Cooperative Cache for PVFS
Because of large overhead to maintain accurate information about cached blocks, we
designed Coopc-PVFS as a hint-based cooperative cache [9]. To maintain the hint –
opened clients list, we added new function to the metadata manager to keep the clients
list that contains information of clients that opened the file before. Whenever a client
opens a file, the client gets both the metadata and the opened clients list of the file from
the metadata manager. To accurately look up where is a block, a client must manage
information about cached blocks in its own cache and other clients. To maintain this
information, each cache manager manages its own bitmap and exchanges its own
information with each other when it gets the block from another client.
Unlike previous hint-based cooperative cache research [5], we managed
information and cached blocks per block, not per file. Because many clients share
large files among them in a parallel file system, it is more adaptable to manage
information and to cache per block than per file in Coopc-PVFS.
In PVFS, all the accesses to files go through the I/O servers. To conserve the
consistency the same as in PVFS, the cache manager must invalidate blocks cached in
other clients before writing the block to the I/O server in Coopc-PVFS. To do so,
whenever an application writes a block, the cache manager sends cache invalidation
messages to the others through the metadata manager before sending the written block
to the I/O server.

410

I.-C. Hwang et al.

4 Design and Implementation of the Home-Based Cooperative
Cache for PVFS
In the home-based Coopc-PVFS, every cached block has its own 'home node'. The
home node of a block is statically determined and is not changed forever. When a
client reads/writes a file, a cache manager finds a block first in its own cache and
sends block request to the home node if a block is not in its own cache. Every
read/write request to a block missed in every client’ file system cache goes to the
home node so that there is cache locality in the home-based Coopc-PVFS. Cache
locality and little information management overhead in the home-based Coopc-PVFS
can improve more read performance than the hint-based Coopc-PVFS. Also, a home
node can buffer written data from clients and all write operations to a block can be
gathered and buffered in a home node. Therefore, the home-based cooperative cache
can improve more write performance than the hint-based Coopc-PVFS and other
previous cooperative caches - only focused on shared read data - because of write
buffering and gathering. In the hint-based Coopc-PVFS, there is much penalty of false
information and information management overhead so that read/write performance
could be worse than the home-based Coopc-PVFS.

Fig. 2. Message transfers when read operations occur in Home-based Coopc-PVFS

For scalability of home-based Coopc-PVFS, we manage servers per file to reduce
home node management overhead: every file has its management server. The
management server allocates and manages home nodes of a file. Therefore, we can
distribute home node management overhead over nodes. The management server of a
file is determined at first file open time in the system and this management server
information is saved in the metadata. Therefore, every client can know the
management server of a file.
Figure 2 shows message transfers when read operations occur.
When a client reads a block, a cache manager in the client finds a block from its
own cache first and read operation is over if there is a block in its own cache. If there
is not in its own cache, a cache manager finds a home node of the block. If the home
node is not allocated, a cache manager gets the home node from a management server
of the file. After knowing the home node, a cache manager gets the block from the
home node and caches it so that read operation is over. When a home node gets block

Design and Implementation of the Home-Based Cooperative Cache for PVFS

411

requests from other nodes, it transfers the block if there is the block in its own cache
or it transfers the block to other clients' cache managers after it gets the block from
I/O servers if there is not. Also, the home node remembers nodes that read the block
so that the home node can invalidate all cached blocks when there are write
operations to the block.
Figure 3 shows message transfers when write operations occur.

Fig. 3. Message transfers when write operations occur in Home-based Coopc-PVFS

When a client writes a block, a cache manager looks up which node is the home
node. If there is no home node, a cache manager gets the home node from a manager
node.
If the home node is itself, a cache manager invalidates all cached blocks in others'
cache before writing a block and buffers written data to its own cached block. If the
home node is another node, a cache manager sends written data to the home node and
the home node cache manager invalidates all cached blocks in others' caches before
buffering written data to its own cache. Therefore, all written data are buffering in
cooperative cache and write performance can be improved in home-based cooperative
cache. Also, all written data for a block can be buffering in only one node-home node
and all write operations to a block are gathered in the home node so that we can
reduce I/O servers' overhead.
The home-based Coopc-PVFS is implemented like the previous hint-based
cooperative cache. A cache manager is not using Linux page cache system so that a
replacement manager in a cache manager manages cached blocks according to the
amount of free memory in the system using LRU list. For managing home nodes, a
management server per file manages home lists. To manage management servers of
files in a metadata manager, we added management servers’ lists to a metadata
manager. After a client opens a file, a cache manager gets home nodes of a file from a
management server. Using this information, a cache manager can read/write blocks
to/from home nodes of blocks.

5 Performance Evaluation
We used CAN cluster [6] in KAIST to evaluate the performance of Coopc-PVFS. The
system configuration of CAN cluster is presented in table 1.

412

I.-C. Hwang et al.
Table 1. System configuration
CPU
Memory
Disk
Network

Pentium IV 1.8GHz
512MByte 266MHz DDR
IBM 60G 7200rpm
3c996B-T(Gigabit Ethernet)
3c17701-ME(24port Gigabit Ethernet Switch)

OS , PVFS

Linux(Kernel version 2.4.18) , 1.6.0

The metadata manager was allocated in one node and the I/O server was allocated
in another node. And four other clients were used to execute the test applications –a
simple matrix multiplication program and BTIO benchmark programs. Each program
operates like below:
! Matrix multiplication program: Each process in four nodes reads each part of
two input files of 1024*1024 matrix and does matrix multiplication, and then
writes results to a output file. Using the MPI library [8], applications are
executed in four nodes.
! BTIO benchmark programs: BTIO is a parallel file system benchmark. It is one
of NAS Parallel Benchmark [7]. BTIO contains four programs. In table 2, we
present each program. We can evaluate the performance using four nodes with
smallest size -class s- in BTIO.
Table 2. BTIO benchmark programs
Full (mpi_io_full)
Simple (mpi_io_simple)
Fortran(fortran_io)
Epi (ep_io)

MPI I/O with collective buffering
MPI I/O without collective buffering
Fortran 77 file operations used
Each process writes the data belonging to its
part of the domain to a separate file

Normalized Read/Write Time
120

%

100
80

PVFS

60

Hint-Based

40

Home-Based

20
0
Read

Write
Read/Write

Fig. 4. Normalized read/write time in matrix multiplication program

Design and Implementation of the Home-Based Cooperative Cache for PVFS

413

5.1 Execution Time of Matrix Multiplication Program
In figure4, we present normalized total read/write time based on total read/ write time
in PVFS when we execute matrix multiplication program.
Matrix multiplication program is a read-dominant program and total read time in
PVFS is about 12 seconds. In the hint-based Coopc-PVFS and in the home-based
Coopc-PVFS, total read time is about 0.5 seconds and 0.3 seconds. In this program,
read files (input files) are always read and written file (output file) is always written.
So, total read time in the home-based Coopc-PVFS is similar to in the hint-based
Coopc-PVFS. Total write time in home-based Coopc-PVFS decreases about 94% of
total write time in hint-based Coopc-PVFS because there is no write overhead and
home nodes do write-buffering in home-based Coopc-PVFS.
5.2 Execution Times of BTIO Benchmark Programs
In figure 5, we present normalized total read/write time based on total read/write time
in PVFS when we execute BTIO benchmark programs.

Normalized Time

600
500

%

400
Read Ops.

300

Write Ops.

200
100

epio

fortran

simple

Home-Based

Hint-Based

PVFS

Home-Based

Hint-Based

PVFS

Home-Based

Hint-Based

PVFS

Home-Based

Hint-Based

PVFS

0

full

BTIO Programs

Fig. 5. Normalized read/write time in BTIO benchmark programs

Total read time in the home-based Coopc-PVFS is shorter than in the hint-based
Coopc-PVFS and in PVFS because of caching effects and small management
overhead. When read/write are all mixed up (fortran and simple cases), total write
time in the hint-based Coopc-PVFS is much worst than in PVFS. In all programs,
total write time in the home-based Coopc-PVFS is much shorter than in the hintbased Coopc-PVFS and in PVFS because all write operation is buffered in home
nodes.

414

I.-C. Hwang et al.

6 Conclusion and Future Work
In this paper, we present the home-based Coopc-PVFS. Also, we evaluate and
analysis the performance of the home-based Coopc-PVFS in comparison to PVFS and
to the hint-based Coopc-PVFS. The home-based Coopc-PVFS improve read/write
performance more than the hint-based Coopc-PVFS. When read operation occurs, a
client gets a block just from the home node in the home-based Coopc-PVFS with less
overhead than in the hint-based Coopc-PVFS so that there is cache locality in the
home-based Coopc-PVFS. When a client write a block, the home-based Coopc-PVFS
does write buffering in the home node and gathers all write operations to a block in
the home node so that write performance in the home-based Coopc-PVFS is much
better than in the Coopc-PVFS and in PVFS.
In the future, we will evaluate the performance of Coopc-PVFS with real parallel
programs and various benchmarks. We will do more research about cache
management schemes and home node management schemes in the home-based
cooperative cache.

References
[1] P. H. Carns, W. B. Ligon III, R. B. Ross, and R. Thakur, "PVFS: A Parallel File System
For Linux Clusters'', Proceedings of the 4th Annual Linux Showcase and Conference,
Atlanta, GA, October 2000, pp. 317-327
[2] M.Vilayannur,M.Kandemir, A.Sivasubramaniam, "Kernel-Level Caching for Optimizing
I/O by Exploiting Inter-Application Data Sharing", IEEE International Conference on
Cluster Computing (CLUSTER'02),September 2002
[3] Dahlin, M., Wang, R., Anderson, T., and Patterson, D. 1994. "Cooperative Caching: Using
remote client memory to improve file system performance", In Proceedings of the First
USENIX Symposium on Operating Systems Design and Implemntation. USENIX Assoc.,
Berkeley, CA, 267-280
[4] Feeley, M. J., Morgan, W. E., Pighin, F. H., Karlin, A. R., and Levy, H. M. 1995.
"Implementing global memory management in a workstation cluster", In Proceedings of
the 15th symposium on Operating System Principles (SOSP). ACM Press, New york, NY,
201-212
[5] Prasenjit Sarkar , John Hartman, "Efficient cooperative caching using hints", Proceedings
of the second USENIX symposium on Operating systems design and implementation,
p.35-46, October 29-November 01, 1996, Seattle, Washington, United States
[6] Can cluster, http://camars.kaist.ac.kr/~nrl
[7] Parkson Wong, Rob F. Van der Wijngaart, NAS Parallel Benchmark I/O Version 2.4, NAS
Technical Report NAS-03-002, NASA Ames Research Center, Moffett Field, CA
94035-1000
[8] W. Gropp, S. Huss-Lenderman, A. Lumsdaine, E. Lusk, W. Nitzberg, W. Saphir, M. Snir.
MPI: The Complete Reference (vold.2). MIT Press, 1998
[9] In-Chul Hwang, Hojoong Kim, Hanjo Jung, Dong-Hwan Kim, Hojin Ghim, Seung-Ryoul
Maeng and Jung-Wan Cho, Design and Implementation of the Coopeartive Cache for
PVFS, International Conference on Computational Science 2004, Krakow, Poland, June
2004.

