Efficient Multimodality Volume Fusion
Using Graphics Hardware
Helen Hong1, Juhee Bae2, Heewon Kye2, and Yeong Gil Shin2
1
School of Computer Science and Engineering,
BK21: Information Technology, Seoul National University
hlhong@cse.snu.ac.kr
2
School of Computer Science and Engineering, Seoul National University,
San 56-1 Shinlim 9-dong, Kwanak-gu, Seoul 151-742, Korea
{jhbay, kuei, yshin}@cglab.snu.ac.kr

Abstract. We propose a novel technique of multimodality volume fusion using
graphics hardware that solves the depth cueing problem with less time
consumption. Our method consists of three steps. First, it takes two volumes
and generates sample planes orthogonal to the viewing direction following 3D
texture mapping volume rendering. Second, it composites textured slices each
from different modalities with several compositing operations. Third, alpha
blending for all the slices is performed. For the efficient volume fusion, a pixel
program is written in HLSL(High Level Shader Language). Experimental
results show that our hardware-accelerated method distinguishes the depth of
overlapping region of the volume and renders them much faster than
conventional ones on software.

1 Introduction
In clinical medicine, several different modalities such as Positron Emission
Tomography (PET), Computed Tomography (CT), and Magnetic Resonance Imaging
(MRI) are useful for radiologists and surgeons to help diagnosis support and treatment
planning. These modalities give complementary information when they are shown
simultaneously. Thus many medical applications require visual output generated from
multimodality volumes rather than only one volume.
Several methods have been proposed to combine multiple volumes obtained from
multimodality imaging. Cai and Sakas [1] proposed three different intermixing
approaches. The shortcoming of the method was a lack of precise depth cueing among the
multiple volumes, even though they tried to use Z-buffer value. Jacq and Roux [2]
presented a multi-volume rendering focused on material classification which applied
different material percentages and merging rules at each sampling point. Zuiderveld et al.
[3] described how to cache calculated voxel properties using sized hash table that
contribute to the final result. Previous researches have performed multimodality volume
fusion on software. However, the software-based multimodality fusion has a limitation in
depth presentation and processing time. In this paper, we propose an efficient hardwareaccelerated solution to combine multimodality volumes at interactive rates.
V.S. Sunderam et al. (Eds.): ICCS 2005, LNCS 3516, pp. 842 – 845, 2005.
© Springer-Verlag Berlin Heidelberg 2005

Efficient Multimodality Volume Fusion Using Graphics Hardware

843

2 Multimodality Volume Fusion
Fig. 1 shows the pipeline of our method for multimodality volume fusion. We assume
reference and float volume datasets have the same orientation by setting rigid
transformation that would be sufficient for the registration of the datasets [4].

Fig. 1. The pipeline of multimodality volume fusion

2.1 Depth Compositing of the Volume
Since multiple volumes have different size and orientation, it needs correct depth
composition and orientation. Fig. 2 shows different ways to combine two slices
from each volume. In Fig. 2(a), it is required to set the orders of each slice of the
volumes which is a troublesome work. We combine each texture with same
texture coordinates from different volumes before blending as shown in Fig. 2(b).
Two voxels (ultimately texels) referred to the same texture coordinates are
combined by the compositing operations and then they are mapped to the same
slice.

(a)

(b)

Fig. 2. A comparison of blending textures (black dotted line: slice of reference volume, red
solid line: slice of float volume) (a): alpha blending in slice orders (b): combine textures before
alpha blending

2.2 Compositing Operations for Volume Fusion
Several compositing operations are experimented to find an appropriate combining
method; some are referred from the previous compositing researches [7][8].

844

H. Hong et al.

The results of combined color and opacity of over, XOR, plus and weighted addition
operation are shown from Eq. (1) to Eq. (4), respectively.

α A ⋅ C A + (1 − α A )α B ⋅ C B
α A + (1 − α A ) ⋅ α B
α = α A + (1 − α A ) ⋅ α B

(1 − α B ) ⋅ α A ⋅ C A + (1 − α A )α B ⋅ C B
(1)
(1 − α B ) ⋅ α A + (1 − α A ) ⋅ α B
α = (1 − α B ) ⋅ α A + (1 − α A ) ⋅ α B

C=

C=

(2)

weight ⋅ α A ⋅ C A + (1 − weight ) ⋅ α B ⋅ C B
(4)
weight ⋅ α A + (1 − weight ) ⋅ α B
α = α A +αB
α = weight ⋅ α A + (1 − weight ) ⋅ α B
(0≤ weight ≤1)
where C indicates color, α indicates alpha, A refers to the first volume, and B to the
second volume.
C = C A + CB

(3)

C=

2.3 Weighted Opacity for Surface Extraction

To see a float volume through a reference volume, we consider weight to the
opacity as shown in Eq. (5). The weight value tells whether the pixel is a part of
the surface or a part of the homogeneous region by gradient magnitude. In the
preprocessing step, gradient magnitude is used as normal for shading. Here, we
use gradient magnitude as a numerical divergence which implies that the surface
has higher value while homogeneous part has a lower value. During fusion
process, we let the homogeneous area to be transparent so that we could see
through the reference volume by eliminating the opacity value of the
homogeneous area between reference and float volume of the overlapping area.

0 if (| ∇ | < c ) (c : const ant )
1 else

weight=

(5)

3 Experimental Results
All our implementation and tests have been performed on an Intel Pentium IV PC
containing 2.53 GHz CPU with ATI Radeon X800 256 MB RAM. Our method

(a)

(b)

(c)

(d)

Fig. 3. The results of compositing operations at opacity 8. (a) over (b) XOR(exclusive OR)
(c)weight addition (d) plus operation

Efficient Multimodality Volume Fusion Using Graphics Hardware

845

has been applied to the brain of 256 x 256 x 96 MR images and 128 x 128 x 40
PET images with 2 bytes-intensity. The performance of our method is evaluated
with the aspects of visual inspection and processing time.
Fig. 3 displays the results using several compositing operations. Fig. 3(a) and
Fig. 3(b) show the result of over operation and XOR(exclusive OR). Fig. 3(c)
using weight addition displays rather a dark but manifest result compared to
others. Fig. 3(d) represents a result of plus operation to be mainly white because
the colors are simply added which induces overflow.
The average of the processing time of the fusion volume supported by software is
3703 msec while fusion on hardware is 625 msec, respectively. We gained about 5 to
6 times of speed enhancement on rendering time.

4 Conclusion
We have developed an efficient multimodality volume fusion method using graphics
hardware. Our method presents several approaches of combining two different
volume datasets into a singular image representation. MR and PET images of the
brain have been used for the performance evaluation with the aspects of visual
inspection and processing time. Our method shows the exact depth of each volume
and the realistic views with interactive rate in comparison with the software-based
multimodality volume fusion. Distinguishable and fast result of the proposed method
can be successfully utilized in medical diagnosis.

References
1. Cai W., Sakas G., Data Intermixing and Multi-volume Rendering, Computer Graphics
Forum (1999), 18(3): 359-368.
2. Jaeq, J., Roux, C., A Direct Multi-volumes Rendering Methods Aiming at Comparison of
3D Images and Methods, IEEE Trans. On Information Technology in Biomedicine (1997)
1(1):30-43.
3. Zuiderveld, K.J., Viergever, M.A., Multi-modal Volume Visualization using Objectoriented methods, in Proc. IEEE/ACM Volume Visualization '94 Symposium (1994) 59-66.
4. Hong, H., Shin, Y., Intensity-based registration and combined visualization of multimodal
brain images for noninvasive epilepsy surgery planning, Proc. of SPIE Medical Imaging
(2003).

