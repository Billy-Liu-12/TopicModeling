Multiscale Interpolation, Backward
in Time Error Analysis for Data-Driven
Contaminant Simulation
Craig C. Douglas1,2 , Yalchin Efendiev3 , Richard Ewing3 ,
Victor Ginting3 , Raytcho Lazarov3 , Martin J. Cole4 ,
Greg Jones4 , and Chris R. Johnson4
1

2

University of Kentucky, Department of Computer Science,
325 McVey Hall, Lexington, KY 40506-0045, USA
{craig.douglas, ceshan0}@uky.edu
Yale University, Department of Computer Science, P.O. Box 208285,
New Haven, CT 06520-8285, USA
douglas-craig@cs.yale.edu
3
Texas A&M University, ISC, College Station, TX, USA
{efendiev, ginting, lazarov}@math.tamu.edu,
richard-ewing@tamu.edu
4
Scientiﬁc Computing and Imaging Institute, University of Utah,
Salt Lake City, UT, USA
{gjones, mjc}@sci.utah.edu, crj@cs.utah.edu

Abstract. We describe, devise, and augment dynamic data-driven application simulations (DDDAS). DDDAS oﬀers interesting computational
and mathematically unsolved problems. In this paper, we discuss how to
update the solution as well as input parameters involved in the simulation based on local measurements. The updates are performed in time.
We test our method on various synthetic examples.

1

Introduction

In recent years, immense computing power has become available at the national
and international supercomputer centers and local clusters of fast PCs. We also
have had a proliferation of data acquisition and generation through the deployment of sophisticated new generations of sensors. The lack of coordination between current computational capacity and sensor technology impairs our ability
to eﬀectively utilize the ﬂood of information available. This is a substantial barrier to achieving the potential beneﬁt computational science can deliver to many
application areas including contaminant tracking, wildﬁre modeling, transportation optimization, and many other ﬁelds.
Sensors and data generating devices may take many forms including other
running computational simulations. The intent of this paper is to address several DDDAS enabling technologies in the context of a speciﬁc application area
in order to provide techniques and tools to eﬀectively demonstrate the potential
V.S. Sunderam et al. (Eds.): ICCS 2005, LNCS 3515, pp. 640–647, 2005.
c Springer-Verlag Berlin Heidelberg 2005

Multiscale Interpolation, Backward in Time Error Analysis

641

of dynamic data driven simulations for other areas. Our primary application is
contaminant tracking, which in groundwater reservoirs is modeled by strongly
coupled systems of convection-reaction-diﬀusion equations. The solution process of such systems becomes more complicated when modeling remediation and
clean-up technologies since they exhibit strong nonlinearities and local behavior.
Many of these applications are essentially computer models that solve nonlinear, unsteady, coupled, partial diﬀerential equations. All require consistent initial
conditions, adequate forcing ﬁelds, and boundary conditions to advance the solution in time. Collectively these ﬁelds represent the input data necessary to run
the models. The input data can originate from observations, e.g., sensor based
telemetry, can be internally generated from ensemble type simulations, or can be
externally generated (e.g., providing boundary conditions for very high resolution regional models). The skill of these models to adequately represent realistic
conditions is intimately tied to the quality, spatial and temporal coverage, and
intelligent use of their input data sets. These applications in turn generate large
amounts of output data that must be either analyzed or passed on to other more
specialized subcomponents.
The update is performed based on the sensor measurements, which streamed
from few spatial locations. As data is injected, we propose to update (1) the solution (2) the initial condition. We have also considered the update of the media
properties, but this will not be discussed in this paper. Because of the heterogeneities of the porous media, we employ multiscale interpolation technique for
updating the solution. This is done in the context of general nonlinear parabolic
operators that include subsurface processes. The main idea of this interpolation
is that we do not alter the heterogeneities of the random ﬁeld that drives the contaminant. Rather based on the sensor data we rescale the solution in a manner
that it preserves the heterogeneities. This rescaling uses the solution of the local
problems. We compare the numerical results for simulations that employ both
updating the data at sensor location and the simulations that do not update
the locations. The simulation results that do not use the update or use it less
frequently produces large errors. This was observed in our numerical studies.
The errors in the simulations will persist if one does not change the input
parameters. As new data are obtained from sensors measurements, the initial
data needs to be updated. This update reduces the computational errors associated with incorrect initial data and improves the predictions. In this paper, we
consider linear subsurface ﬂows involving convection and diﬀusion. Initial data
is sought in a ﬁnite dimensional space. Using the ﬁrst set of measurements, the
approximation of the initial data is recovered. As new data are incorporated
into the simulator, we update the initial data using an objective function. We
note that the formulated problem is ill-posed. Two facts can be attributed to
this ill-posedness. First, the data gathered from the sensor measurements always
contain some defects that come from factors such as human errors and inherent factory errors of the sensors. Secondly, the number of sensors that can be
installed are limited, and in general are much fewer than the ﬁnite dimensional
space describing the initial data. For the latter, we can regularize the problem by

642

C.C. Douglas et al.

using the prior information about the initial data. This prior information is the
updated initial data. The penalization constants depend on time of update and
can be associated with the relative diﬀerence between simulated and measured
values. Numerical examples are presented in the paper showing the improvement
of the predictions as new data is taken into account.

2

Backward Error Analysis and Initial Data Recovery

As new data is injected from sensor measurements, “the initial data” can be
updated. Here, by initial data we mean contaminant distribution at some early
time. Due to poor knowledge of the initial location of contaminant, this type of
errors can be dominant in simulations. We consider a linear transport dominated
by convection and diﬀusion
∂C
+ v · ∇C − ∇ · (D∇C) = 0 in Ω,
∂t

(1)

where by Darcy’s Law, we have v = −k∇p, with the pressure p satisﬁes
−∇ · (k∇p) = 0

(2)

with some prescribed boundary conditions and initial condition/data C(x, 0) =
C 0 (x). Here the variable C(x, t) is designated for a contaminant concentration
over the porous medium Ω and at time level t, k is the permeability of the porous
medium, and D is the diﬀusion coeﬃcient.
We seek the initial data in a ﬁnite dimensional space. The dimension of this
space is an important factor in our simulations. The problem becomes more illposed if this dimension increases. In general, one can reduce this dimension using
multiscale representation of the initial data (note that initial data represents the
solution at early times, not necessarily at time zero). As new data are incorporated into the simulator, we update the initial data using an objective function.
Before we formulate the objective function, we note that the formulated problem is ill-posed. Two facts can be attributed to this ill-posedness. First, the data
gathered from the sensor measurements always contain some defects that come
from factors such as human errors and inherent factory errors of the sensors.
Secondly, the number of sensors that can be installed are limited, and in general
are much fewer than the ﬁnite dimensional space describing the initial data. For
the latter, we can regularize the problem by using the prior information about
the initial data. This prior information is the updated initial data. The penalization constants depend on time of update and can be associated with the relative
diﬀerence between simulated and measured values.
To formulate the objective function we introduce some notations. Let Ns
be the number of sensors installed in various points in the porous medium and
s
{xj }N
j=1 denote such points. Let Nt be the number of how many times the cont
centration is measured in time and {tk }N
k=1 denote such time levels. Furthermore,

Multiscale Interpolation, Backward in Time Error Analysis

643

γj (tk ) denotes the measured concentration at sensor located in xj and at time
tk . We seek initial data in a ﬁnite dimensional space spanned by C˜i0 (x),
Nc

αi C˜i0 (x),

C˜ 0 (x) =

(3)

i=1

for some α = (α1 , α2 , · · · , αNc ). Furthermore, let C˜i (x, t) be the solution of (1)
using an initial condition C˜i0 (x). Then by superposition principle, the solution
of (1) using C˜ 0 (x) in (3) as an initial condition has the following form:
Nc

˜ t) =
C(x,

αi C˜i (x, t).

(4)

i=1

The objective function which can be formulated in terms of α, quantiﬁes the
diﬀerence between the measured concentration and the simulated concentration,
˜ t). In general the number of the sensors are less than the dimension of
C(x,
the space for initial data. Hence, an attempt to minimize an objective function
that only contains the diﬀerence between measurements and simulations will
lead to an ill-posed problem. To regularize the problem, we add a penalty term
that contains the prior information related to the initial data, and consider the
following objective function
Ns

2

Nc

αi C˜i (xj , t) − γj (t)

F (α) =
j=1

i=1

Nc

2

κi (αi − βi ) .

+

(5)

i=1

Here κ = (κ1 , κ2 , · · · , κNc ) is the penalty coeﬃcients for an a priori vector β =
(β1 , β2 , · · · , βNc ). This prior information will be updated during the simulation
to achieve higher accuracy.
Next we present a representative numerical example. More numerical and
theoretical studies have been performed and the results will be reported elsewhere. We use Ω = [0, 1] × [0, 1]. The boundary conditions in the subsurface
ﬂow for the pressure equation (2) are given pressure at the inlet and outlet
edges (i.e., x = 0 and x = 1, respectively), and no ﬂow at the bottom and top
edges (i.e., z = 0 and z = 1, respectively). The permeability k is generated with
given correlation length lx = 0.25 and lz = 0.02, with a spherical variogram
using GSLIB algorithms [2]. For the convection-diﬀusion equation (1), we set
the diﬀusion coeﬃcient D = 0.1 over all domain. We assume zero concentration
at the inlet, bottom, and top edges, and a zero diﬀusion, i.e., (D∇C) · n = 0,
at the outlet edge, with n being the unit normal vector pointing outward on
the outlet edge. The initial condition C 0 (x, z) is set to be nonzero in the region
(0.2, 0.4) × (0.2, 0.4) and zero elsewhere.
Both pressure and convection-diﬀusion equations are solved by the ﬁnite volume method using rectangular grids. We discretize the domain into 100 × 100
elements, i.e., 100 elements in each direction. The sensors information are obtained from the concentration solved with the given “true” initial condition. We

C.C. Douglas et al.
1

1

z

1

0.9

0.9

0.8

0.8

0.8

0.7

0.7

0.7

0.6

0.6

0.6

0.5

0.5

0.9

z

644

0.025

0.02

0.015

0.5

0.4

0.4

0.4

0.3

0.3

0.3

0.2

0.2

0.2

0.1

0.1

0.1

0

0

0
0.2

0

0.8

0.6

0.4

1

x

x

x
x

x

x

x

x

0.2

0.005

x

x

x
0

0.01

x

x

0.4

0.6

0.8

0

1

x

x

Fig. 1. Left: The initial condition proﬁle - Right: Concentration at t=0.4 ((x) indicates
the sensor location)

use time step ∆t = 0.01. For the example presented in this paper we use initial
condition shown on the right side of Figure 1.
The data from measurement are taken from multiple set of numerical simulations with the initial condition mentioned earlier. The measurement is assumed
to be conducted at time level t1 = 0.1, t2 = 0.2, t3 = 0.3, and t4 = 0.4. A number of sensors are installed at various locations in the porous medium. Figure 1
shows the concentration proﬁle at t = 0.4 along with the sensor locations which
are denoted by (x) indicator.
We have performed numerical tests by only updating β as well as updating
β and κ. In all cases, we observed signiﬁcant improvement as the initial data
is updated. Here we present only the case of both β and κ are updated. In
particular, κ is increased during the simulations to reﬂect the fact that the
updated initial condition is a better representation of the true initial condition.
In general, one can change κ in various ways. In our examples, we update κi
by increasing 10 times after each update. Figure 2 shows the updated initial
condition with both β and κ updated for the case of larger support. The prior
for κ is κ0i = 2 × 10−12 for all i, and when updated it is multiplied by ten. The
ﬁgure shows signiﬁcant improvement on the predicted initial condition.
As mentioned earlier, sensor measurements contain errors and uncertainties.
In our numerical simulations, we can take into account these uncertainties by
sampling the sensor data from the known distribution. As a result, one obtains
1

1

1

0.8

0.9

1

1

0.9

0.9

0.9

0.8

0.8

0.8

0.8

0.7

0.7

0.7

0.7

0.6

0.6

0.6

0.6

0.5

0.5

0.5

0.5

0.4

0.4

0.4

0.4

0.3

0.3

0.3

0.3

0.2

0.2

0.2

0.2

0.1

0.1

0.1

0.9

0.7

0.8

0.5

0.5

0.4

0.4

0.3

0.3

z

z

0.6

z

0.6

0.7

0.2
0.2

0.1

0.1
0

0

0.2

0.4

0.6

x

0.8

1

0

0

0.2

0.4

0.6

x

0.8

1

0

0

0.1

0

0.2

0.4

0.6

0.8

1

0

x

Fig. 2. Updated initial condition: t = 0.1 (left), t = 0.3 (middle), t = 0.4 (right) - Both
β and κ are updated

Multiscale Interpolation, Backward in Time Error Analysis

645

various realizations of the initial data. In our subsequent work [4], we employ the
least squares approach in developing Bayesian methods for nonlinear problems.
To quantify uncertainties in the measurements and a priori knowledge about
the initial data, the Markov Chain Monte Carlo method (MCMC) can be used.
Because this method is expensive due to rejection of the proposals, we propose
an approach that combines the least squares method with Bayesian approaches
that will give high acceptance rates.

3

Multiscale Interpolation Techniques

Our goal in this section is to discuss the mapping of the sensor data to the ﬁnite
dimensional space where the solution is calculated. This procedure is nontrivial
in general, because the solution space usually has high dimension, while the
sensors are located only at few locations. Our simpliﬁed approach presented in
this paper consists of passing the sensor data to the simulations and its use for
the next time step simulations. Since the sensor data represents the solution
only at few coarse locations one has to modify the solution conditioned to this
data. This step we call multiscale interpolation which consists of mapping the
sensor data to the solution space. At each time step the sensor data is received
by the simulator. There are two options to handle the data. We can treat it as
hard data or as soft data. The latter means that the data contains some noise
and not needed to be imposed exactly. In this paper the ﬁrst approach, “hard
constraint”, will be considered. At the beginning of each time step we need to
map the sensor data to the solution space. This is performed using DDDAS
mapping operator, the main feature of which is not to alter the heterogeneous
ﬁeld.
The proposed mapping for the sensor data is general and applicable to various
classes of equations. To demonstrate this we consider general nonlinear parabolic
equations
∂
u = ∇ · (a (x, t, u , ∇u )) + a0, (x, t, u , ∇u ), in Ω × [0, T ],
∂t

(6)

where indicates the presence of the small scales heterogeneities. This equation
includes various physical process that occur in subsurfaces. In the next section
numerical examples for particular cases of (6) will be discussed. Assume the
domain is divided into the coarse grid such that the sensor points are the nodal
points of the coarse grid. Note that we do not require all nodal points to be
sensor locations. Further denote by S h the space of piecewise linear functions on
this partition,
Sh = {vh ∈ C 0 (Ω) : the restriction vh is linear for each triangle K ∈ Πh }.
Our objective now is to map the function deﬁned on S h to the ﬁne grid that represents the heterogeneities. This grid is obtained from apriori information about

646

C.C. Douglas et al.

the ﬁeld using geostatistical packages. Denote by the operator E the mapping
from the coarse dimensional space into the ﬁne grid,
E : Sh → V h,
which is constructed as follows. For each element in uh ∈ S h at a given time tn
we construct space time function u ,h (x, t) in K × [tn , tn+1 ] such that it satisﬁes
∂
u ,h (x, t) = ∇ · (a (x, t, η, ∇u ,h ))
(7)
∂t
in each coarse element K, where η is the average of uh . u ,h (x, t) is calculated
by solving (7) on the ﬁne grid, and thus it is a ﬁne scale function. To complete
the construction of E we need to set boundary and initial conditions for (7).
One can set diﬀerent boundary and initial conditions and this will give rise to
diﬀerent maps. In our numerical simulations we will take the boundary and
initial condition for the local problems to be linear with prescribed nodal values.
These values are obtained from the sensor data, if available. If the sensor data is
not available at some location we use the values obtained from the simulations.
Diﬀerent local boundary conditions can be also imposed and will be discussed
later. Mathematical aspects of this interpolation operator, such as convergence
and etc, are described in [5].
Once the solution at time t = tn is computed its values with sensor data at
the sensor locations are compared. After changing the values of the solution we
interpolate it to the ﬁne grid and use it for the next time step. At the last step
we use multiscale approach which is computationally eﬃcient. In particular, the
solution at the next time step is calculated based on

Ω

tn+1

(uh (x, tn+1 ) − uh (x, tn ))vh dx +
K

tn

a0, (x, t, η, ∇u

K

((a (x, t, η, ∇u

,h ), ∇vh )+

(8)

tn+1
,h )vh )dxdt

=
tn

Ω

f dxdt.

Here Ω refers to the spatial domain and K are the coarse elements. We would
like to note that our approach has limitations and it is not applicable when there
are large deviations between sensor data and the solution.
We now present a representative numerical example that demonstrates the
accuracy and limitations of our proposed method. The detailed numerical studies will be reported elsewhere. The systems we consider are intended to represent
cross sections (in x − z) of the subsurface. The ﬁne-scale permeability ﬁeld is
generated on 121 × 121 grid using GSLIB algorithms [2] with an exponential
∂
u = ∇ · (a (x)∇u), where the original “true”
covariance model. We consider ∂t
diﬀusion coeﬃcient a (x) = exp(α (x)), where α (x) is a realization of the random ﬁeld with correlation lengths lx = 0.3, lz = 0.02 and variance σ = 0.75.
For the simulation purposes we consider the diﬀusion coeﬃcients to be the same
realization of the random ﬁeld but with σ = 1.5. Thus we assume that the heterogeneities have the same nature and the only diﬀerence between the true ﬁeld

Multiscale Interpolation, Backward in Time Error Analysis
1

0.42

true solution
updated
non−updated

0.9

true solution
updated
non−updated

0.4

0.8

averaged solution

0.7
averaged solution

647

0.6

0.5

0.4
0.3

0.38

0.36

0.34

0.2
0.32

0.1

0
0

0.2

0.4

0.6
x

0.8

1

0.3
0

0.2

0.6

0.4

0.8

1

z

Fig. 3. Comparisons of the average solutions across x and z directions: Solid line designates the true solution, dotted line designates the solution obtained using our simulations with 4 updates, and the dashed line designates the solution that has not been
updated

and the one used in the computations is associated with the scaling. Objective of
this numerical results is to demonstrate how frequency of updating sensor data
in the simulations improves the accuracy of the method. In this example the
sensor data is used four times during simulations, i.e., the frequency of updating
is 4. In Figure 3 we plot the average of the solutions. Solid line designates the ﬁne
scale (true) simulation results, while dotted line represents the results obtained
using our methodology with 4 updating. The dashed line represents the simulation results with no updating. As we see from this ﬁgure the simulation with
data updating performs better compare to that with no updating. The l2 error
between the true solution and the one corresponding with 4 updating is about 6
percent, while l2 error corresponding with no updating is almost 9 percent. Simulations with more frequent update indicate that the frequent updating improve
the accuracy of the predictions and thus important for DDDAS. This is also
observed in various numerical examples for both linear and nonlinear equations
which we have tested (see [3]).

References
1. J. Bear, Dynamics of Fluids in Porous Media, Elsevier, 1972.
2. C. V. Deutsch and A. G. Journel, GSLIB: Geostatistical software library and
user’s guide, 2nd edition, Oxford University Press, New York, 1998.
3. C. C. Douglas, C. Shannon, Y. Efendiev, R. Ewing, V. Ginting,
R. Lazarov, M. Cole, G. Jones, C. Johnson, and J. Simpson, A note on
data-driven contaminant simulation, Lecture Notes in Computer Science, SpringerVerlag, 3038 (2004), pp. 701–708.
4. C. C. Douglas, Y. Efendiev, R. Ewing, V. Ginting, and R. Lazarov,
Bayesian approaches for initial data recovery in dynamic data-driven simulations,
in preparation.
5. Y. Efendiev and A. Pankov. Numerical homogenization of nonlinear random
parabolic operators. SIAM Multiscale Modeling and Simulation, 2(2):237–268, 2004.
6. C.R. Johnson, S. Parker, and et al. SCIRun: A scientiﬁc computing problem solving
environment. http://software.sci.utah.edu/scirun.html.

