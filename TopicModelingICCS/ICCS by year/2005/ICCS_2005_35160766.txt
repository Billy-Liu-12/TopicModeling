Finding the Smallest Eigenvalue by the Inverse
Monte Carlo Method with Refinement
Vassil Alexandrov1 and Aneta Karaivanova2
1

School of Systems Engineering, University of Reading, Reading RG6 6AY, UK
v.n.alexandrov@rdg.ac.uk
http://www.cs.reading.ac.uk/people/V.Alexandrov.htm
2
IPP - BAS, Acad. G. Bonchev St., Bl.25A, 1113 Soﬁa, Bulgaria
anet@parallel.bas.bg
http://parallel.bas.bg/∼anet/

Abstract. Finding the smallest eigenvalue of a given square matrix A
of order n is computationally very intensive problem. The most popular
method for this problem is the Inverse Power Method which uses LUdecomposition and forward and backward solving of the factored system
at every iteration step. An alternative to this method is the Resolvent
Monte Carlo method which uses representation of the resolvent matrix
[I −qA]−m as a series and then performs Monte Carlo iterations (random
walks) on the elements of the matrix. This leads to great savings in
computations, but the method has many restrictions and a very slow
convergence.
In this paper we propose a method that includes fast Monte Carlo
procedure for ﬁnding the inverse matrix, reﬁnement procedure to improve approximation of the inverse if necessary, and Monte Carlo power
iterations to compute the smallest eigenvalue. We provide not only theoretical estimations about accuracy and convergence but also results from
numerical tests performed on a number of test matrices.
Keywords: Monte Carlo methods, eigenvalues, Markov chains, parallel
computing, parallel eﬃciency.

1

Introduction

Let A be a real symmetric matrix. Consider the problem of evaluating the eigenvalues of A, i.e. the values of λ for which
Au = λu

(1)

holds. Suppose, the n eigenvalues of A are ordered as follows |λ1 | > |λ2 | ≥ . . . ≥
|λn−1 | > |λn |.
Supported by the Ministry of Education and Science of Bulgaria under Grant No.
I1405/04.
V.S. Sunderam et al. (Eds.): ICCS 2005, LNCS 3516, pp. 766–774, 2005.
c Springer-Verlag Berlin Heidelberg 2005

Finding the Smallest Eigenvalue

767

It is known that the problem of calculating the smallest eigenvalue of A is
more diﬃcult from numerical point of view than the problem of evaluating the
largest eigenvalue. Nevertheless, for many applications in physics and engineering
it is important to estimate the smallest one, because it usually deﬁnes the most
stable state of the system which is described by the considered matrix.

2
2.1

Background
Inverse Power Method

One of the most popular methods for ﬁnding extremal eigenvalues is the power
method ([6], [8]) which for a given matrix A is deﬁned by the iteration
xnew = Axold .
Except for special starting points, the iterations converge to an eigenvector corresponding to the eigenvalue of A with largest magnitude (dominant eigenvalue). The least squares solution µ to the overdetermined linear system
µxk = xk+1
is an estimate for λ1 which is called the Raleigh quotient.
µ=

xTk xk+1
.
xTk xk

Suppose that we want to compute the eigenvector corresponding to the eigenvalue of A of smallest magnitude. Letting (λ1 , e1 ) through (λn , en ) denote the
eigenpairs of A, the corresponding eigenpairs of C = A−1 are (1/λ1 , e1 ) through
(1/λn , en ) . If λn is the eigenvalue of A of smallest magnitude, then 1/λn is
C s eigenvalue of largest magnitude and the power iteration xnew = A−1 xold
converges to the vector en corresponding to the eigenvalue 1/λn of C = A−1 .
When implementing the inverse power method, instead of computing the inverse
matrix A−1 we multiply by A to express the iteration xnew = A−1 xold in the
form Axnew = xold . Replacing A by its LU factorization yields
(LU )xnew = xold .

(2)

In each iteration of the inverse power method, the new x is obtained from the old
x by forward and back solving the factored system (2). This scheme is computationally more eﬃcient. With k iterations, the number of arithmetic operations
is O(kn2 ) for Power method and O(n3 + kn2 ) for Inverse Power Method.

768

V. Alexandrov and A. Karaivanova

2.2

Resolvent Monte Carlo Method

Given a matrix A, consider an algorithm based on Monte Carlo iterations by the
resolvent matrix Rq = [I − qA]−1 ∈ IRn×n . The following representation
[I − qA]−m =

∞
i
q i Cm+i−1
Ai , |qλ| < 1

(3)

i=0

is valid because of behaviors of binomial expansion and the spectral theory of
linear operators. Let remind that the eigenvalues of the matrices Rq and A are
1
, and the eigenvectors coincide. The following
connected by the equality µ = 1−qλ
expression (see [4])

µ(m) =

([I − qA]−m f, h)
1
, f ∈ IRn , h ∈ IRn ,
→m→∞ µ =
1 − qλ
([I − qA]−(m−1) f, h)

(4)

is valid (f and h are vectors in IRn ). For negative values of q, the largest eigenvalue
µmax of Rq corresponds to the smallest eigenvalue λmin of the matrix A.
Moreover, if |λmax < 1| where λmax is the largest eigenvalue of the matrix
A = {|aij |}ni,j=1 , the following statement holds, [4], [2]:
∞

([I − qA]−m f, h) = E

i
q i Cm+i−1
(Ai f, h) .
i=0

Now we can construct Monte Carlo method. Deﬁne Markov chain k0 → k1 →
. . . → ki (1 ≤ kj ≤ n are natural numbers) with initial and transition densities,
P r(k0 = α) = pα , P r(kj = β|kj−1 = α) = pαβ , (see [4],[2]). Now deﬁne the
random variables Wj using the following recursion formula:
W0 =

ak k
hk 0
, Wj = Wj−1 j−1 j , j = 1, . . . , i.
pk 0
pkj−1 kj

(5)

Than it can be proven that (see [4], [2]):
λmin ≈
E
E
where W0 =

hk0
pk0

1
q

1−

1
µ(m)

∞
i−1 i−1
Ci+m−2 Wi fxi
i=1 q
∞
iC i
q
i+m−1 Wi fxi
i=0

=

=

(A[I − qA]−m f, h)
=
([I − qA]−m f, h)

E
E

l
i i
i=0 q Ci+m−1 Wi+1 fxi
l
i i
i=0 q Ci+m−1 Wi fxi

,

(6)

n
and Wi are deﬁned by (5). The coeﬃcients Ci+m
are calculated

i−1
i
i
using the presentation Ci+m
= Ci+m−1
+ Ci+m−1
.

This method has strong requirements about matrices for which it can be applied.
The systematic error consists of two parts:

Finding the Smallest Eigenvalue

769

– an error from the Power method applied on the resolvent matrix [I − qA]−1
which determines the value of the parameter m in the following way: The
rate of convergence is
1 + |q|λn
1 + |q|λn−1

O

m

.

The parameter m has to be chosen such that ( µµ21 )m < ε.
– an error which comes from the series expansion of the resolvent matrix it determines the value of the parameter l (length of each random walk).
Considering the presentation
l

([I − qA]−m f, g) ≈

l
i
q i Cm+i−1
(Ai f, g) =

i=0

ui ,
i=1

the parameter l has to be chosen such that |ul+1 | < ε2 .
– rounding errors - to maintain them at a low level the condition ( max |ui |) ε
1≤i≤l

< αµ must be satisﬁed, where α < 1 represents the requested precision and
ε is the machine precision parameter.
Unfortunately, the parameter l can not be relatively large because the binol
mial coeﬃcients Cm+l−1
grow exponentially with l. This is serious restriction for
using the resolvent Monte Carlo.
The Resolvent Monte Carlo method computes the smallest eigenvalue with
only O(lN ) operations, where l is the average length of the Markov chains and
N is the number of walks. But this method has a lot of restrictions, and, also,
gives very rough approximation.

3

Inverse Monte Carlo with Refinement

Here we propose a method that includes fast Monte Carlo scheme for matrix
inversion, reﬁnement of the inverse matrix (if necessary) and Monte Carlo power
iterations for computing the largest eigenvalue of the inverse (the smallest eigenvalue of the given matrix).
3.1

Monte Carlo for Computing the Inverse Matrix

To ﬁnd the inverse A−1 = C = {crr }nr,r =1 of some matrix A, we must ﬁrst
compute the elements of matrix M = I − A, where I is the identity matrix.
Clearly, the inverse matrix is given by
∞

M i,

C=
i=0

which converges if M < 1 (suﬃcient condition).

(7)

770

V. Alexandrov and A. Karaivanova

To estimate the element crr of the inverse matrix C, we deﬁne Markov chain
s0 → s1 → · · · → sk , where the si , i = 1, 2, · · · , k, belongs to the state space
S = {1, 2, · · · , n}. Then for α, β ∈ S, p0 (α) = p(s0 = α) is the probability
that the Markov chain starts at state α and p(sj+1 = β|sj = α) = pαβ is the
transition probability from state α to state β. The set of all probabilities pαβ
deﬁnes a transition probability matrix P = {pαβ }nα,β=1 .
We then can use the following Monte Carlo method (see, for example, [11],
[3]) for calculating elements of the inverse matrix C:
⎡
⎤
N
1
⎣
(8)
Wj ⎦ ,
crr ≈
N s=1
(j|sj =r )

where (j|sj = r ) means that only
Wj =

Mrs1 Ms1 s2 . . . Msj−1 sj
prs1 ps1 s2 . . . psj−1 pj

(9)

for which sj = r are included in the sum (8).
Since Wj is included only into the corresponding sum for r = 1, 2, . . . , n,
then the same set of N chains can be used to compute a single row of the inverse
matrix, which is one of the inherent properties of MC making them suitable for
parallelization.
Furthermore, if necessary we transform matrix A to matrix D where D is
diagonally dominant matrix, we ﬁnd its inverse and after that apply a ﬁlter
procedure in order to ﬁnd A−1 [5].
The probable error of the method, is deﬁned as rN = 0.6745 Dθ/N , where
P {|θ¯ − E(θ)| < rN } ≈ 1/2 ≈ P {|θ¯ − E(θ)| > rN }, if we have N independent
realizations of random variable (r.v.) θ with mathematical expectation Eθ and
average θ¯ [11].
3.2

Refinement

Given a square nonsingular matrix A, Monte Carlo gives fast but very rough
estimation of the elements of the inverse matrix C0 . Under the condition ||R0 || ≤
δ < 1 where R0 = I − AC0 , the elements of A−1 may be computed to whatever
degree of accuracy is convenient using the iterative process: Cm = Cm−1 (I +
Rm−1 ), Rm = I − ACm for m = 1, 2, . . .. It can be shown that
m

δ2
.
1−δ
It follows that once we have an initial approximation C0 of the inverse such that
||I − AC0 || ≤ k < 1 we can use the above iterative scheme and than the number
of correct digits increases in geometric progression.
If we have inverted matrix D then we need to apply the ﬁlter procedure to
obtain the A−1 using the formula
||Cm − A−1 || ≤ ||C0 ||

−1
A−1
k = Ak+1 +

−1
A−1
k+1 Si+1 Ak+1

1 − trace(A−1
k+1 Si+1 )

, i = k − 1, k − 2, ..., 1, 0.

Finding the Smallest Eigenvalue

3.3

771

Evaluation of the Smallest Eigenvalue

Once we have the matrix C = A−1 we apply the Monte Carlo power iterations
to ﬁnd its largest eigenvalue performing random walks on the elements of the
matrix C:
λmax (C) ≈
Here Wj =

Ck0 k1 Ck1 k2 ...Ckj−1 kj
pk0 k1 pk1 k2 ...pkj−1 pj

E{Wm fki }
.
E{Wm−1 fki−1 }

, and m is suﬃciently large to ensure the conver-

n
)m .
gence of the power method which is O( λλn−1
The computational cost of power Monte Carlo is mN , where m is determined
by the convergence of the power iterations and N is the number of walks.

3.4

Restrictions of the Method and How to Avoid Them

Suppose that λn < λn−1 ; in this case the power method is convergent. The
only possible restriction in the above scheme is the requirement ||M || < 1 for
the convergence of the Monte Carlo inversion. This can easily be avoided using
the presented bellow algorithm for inverting general type matrices using Monte
Carlo:
Algorithm: Finding A−1 .
1. Initial data: Input matrix A, parameters γ and .
2. Preprocessing:
2.1 Split A = D − (D − A), where D is a diagonally dominant matrix.
2.2 Set D = B − B1 where B is a diagonal matrix bii = dii i = 1, 2, ..., n.
2.3 Compute the matrix T = B −1 B1 .
1
2
2.4 Compute ||T ||, the Number of Markov Chains N = ( 0.6745 . (1−||T
|| ) .
3. For i=1 to n;
3.1 For j=1 to j=N;
Markov Chain Monte Carlo Computation:
3.1.1 Set tk = 0(stopping rule), W0 = 1, SU M [i] = 0 and P oint = i.
3.1.2 Generate an uniformly distributed random number nextpoint.
3.1.3 If T [point][nextpoint]! = 0.
LOOP
3.1.3.1 Compute Wj = Wj−1 PT [point][nextpoint]
[point][nextpoint] .
3.1.3.2 Set P oint = nextpoint and SU M [i] = SU M [i] + Wj .
3.1.3.3 If |Wj | < γ, tk = tk + 1
3.1.3.4 If tk ≥ n, end LOOP.
3.1.4 End If
3.1.5 Else go to step 3.1.2.
3.2 End of loop j.
3.3 Compute the average of results.
4. End of loop i.

772

V. Alexandrov and A. Karaivanova

Obtain The matrix V = (I − T )−1 .
Therefore D−1 = V B −1 .
Compute the MC inversion D−1 = B(I − T )−1 .
Set D0 = D−1 (approximate inversion) and R0 = I − DD0 .
use filter procedure Ri = I − DDi , Di = Di−1 (I + Ri−1 ), i = 1, 2, ..., m,
where m ≤ k.
10. Consider the accurate inversion of D by step 9 given by D0 = Dk .
11. Compute S = D − A where S can be any matrix with all non-zero elements
in diagonal and all of its oﬀ-diagonal elements are zero.
12. Main function for obtaining the inversion of A based on D−1 step 9:
12.1 Compute the matrices Si , i = 1, 2, ..., k, where each Si has just one
element of matrix S.
12.2 Set A0 = D0 and Ak = A + S
5.
6.
7.
8.
9.

−1
12.3 Apply A−1
k = Ak+1 +

A−1
S
A−1
k+1 i+1 k+1

1−trace(A−1
S
)
k+1 i+1

, i = k − 1, k − 2, ..., 1, 0.

13. Printthe inversion of matrix A.
14. End of algorithm.

4

Parallel Implementation and Numerical Tests

The method presented in this paper consists of 3 procedures:
– Monte Carlo inversion
– Reﬁnement
– Power Monte Carlo iterations
We study the convergence and the parallel behaviour of each of the above
procedures separately in order to have the overall picture.
It is well known that Monte Carlo algorithms have high parallel eﬃciency.
In fact, in the case where a copy of the non-zero matrix elements of A is sent
to each processor, the execution time for computing an extremal eigenvalue
on p processors is bounded by O(lN/p) where N is the number of performed
random walks and l is the mean value of the steps in a single walk. This result
assumes that the initial communication cost of distributing the matrix, and the
ﬁnal communication cost of collecting and averaging the distributed statistics
is negligible compared to the cost of generating the Markov chains and forming
the chosen statistic.
The numerical tests presented include example with 128 × 128 matrix, where
we have applied rough Monte Carlo estimate and no reﬁnement. In this case, the
procedure for ﬁnding the largest eigenvalue of the inverse matrix using power
Monte Carlo iterations, has no convergence. After that we applied a reﬁnement
procedure to ﬁnd more accurate inverses with = 10−6 and 10−8 . Applying
power Monte Carlo iterations to ﬁnd the largest eigenvalue of the reﬁned inverse
matrix gives the value λInvmax = 3.3330 which is closed enough to the value
given by the MATLAB eigenvalue procedure which is 3.3062.

Finding the Smallest Eigenvalue

773

In addition to convergence test, we also performed parallel computations to
empirically conﬁrm the parallel eﬃciency of this Monte Carlo method. A lot of
parallel tests concerning Monte Carlo inversion and reﬁnement can be found in
[1], here we test the Monte Carlo part for computing the desired eigenvalue. The
parallel numerical test was performed on a Compaq Alpha parallel cluster with 8
DS10 processors each running at 466 megahertz using MPI to provide the parallel
calls. Each processor executes the same program for N/p trajectories (here p is
the number of processors), and at the end of the trajectory computations, a
designated host processor collects the results of all realizations and computes
the desired average values. The results shown in Figure that the high parallel
eﬃciency of Monte Carlo methods is preserved for this problem as well.
8

Speedup

6

4

2

0

0

2

4
Number of Processors

6

8

Fig. 1. Computing the largest eigenvalue of the inverse matrix - Speedup vs number
of processors

5

Conclusion

The Monte Carlo methods with reﬁnement have been proposed and extensively
testes for solving systems of linear algebraic equations (see, for example, [1, 5]).
In this paper we propose a method that includes Monte Carlo procedure for
ﬁnding the inverse matrix, reﬁnement procedure to improve approximation of the
inverse if necessary, and Monte Carlo power iterations to compute the smallest
eigenvalue. This method gives better results than the Resolvent Monte Carlo.

References
1. V. N. Alexandrov, S.Branford, and C. Weihrauch, A New Parallel Hybrid Monte
Carlo Algorithm for Matrix Computations’, Proc. Of PMAA 2004, September
2004.
2. I. Dimov, V. Alexandrov, A. Karaivanova, Resolvent Monte Carlo Methods for
Linear Algebra Problems, Mathematics and Computers in Simulations, Vol. . 55,
2001, pp. 25-36.

774

V. Alexandrov and A. Karaivanova

3. I. Dimov, T. Dimov, T. Gurov, A new iterative Monte Carlo Approach for Inverse
Matrix Problem, J. of Comp. and Appl. Mathematics, 92, 1998, pp. 15-35.
4. I. Dimov, A. Karaivanova, Parallel computations of eigenvalues based on a Monte
Carlo approach, Journal of MC Methods and Appl., Vol.4,Num.1, 1998 pp.33-52.
5. B. Fathi, B. Liu and V. Alexandrov, Mixed Monte Carlo Parallel Algorithms for
Matrix Computations, Computational Science - ICCS2002, LNCS 2330, pp. 609618, 2002.
6. G. H. Golub, C.F. Van Loon, Matrix computations, The Johns Hopkins Univ.
Press, Baltimore, 1996.
7. J.H. Halton, Sequential Monte Carlo Techniques for the Solution of Linear Systems,
SIAM Journal of Scientific Computing, Vol.9, pp. 213-257, 1994.
8. William W. Hager, Applied Numerical Linear Algebra, Prentice Hall International
(UK) Limited, London, 1989.
9. J.M. Hammersley, D.C. Handscomb, Monte Carlo methods, John Wiley & Sons,
inc., New York, London, Sydney, Methuen, 1964.
10. G.A. Mikhailov, Optimization of the ”weight” Monte Carlo methods, Nauka,
Moscow, 1987.
11. I.M. Sobol Monte Carlo numerical methods, Nauka, Moscow, 1973.

