GEDAS: A Data Management System for Data
Grid Environments
Jaechun No1 and Hyoungwoo Park2
1
Dept. of Computer Software,
College of Electronics and Information Engineering,
Sejong University, Seoul, Korea
2
Supercomputing Center,
Korea Institute of Science and Technology Information,
Daejeon city, Korea

Abstract. In data grid environments, many large-scale scientiﬁc experiments and simulations generate very large amounts of data in the
distributed storages, spanning thousands of ﬁles and data sets. In such
environments, the replication technique for the fast data sharing between the community of researchers, and the high-performance I/O for
the storage and eﬃcient data accesses on heterogeneous resources present
an extremely challenging task. Several data replication techniques have
been developed to support high-performance data accesses to the remotely produced scientiﬁc data. However, most of those techniques were
implemented with the assumption that the data being replicated is readonly so that it would not be modiﬁed once it has been generated. Furthermore, those techniques mainly focus on measuring up the network
performance, but ignoring I/O overhead incurred during the data generation and replication. We have developed a software system, called Grid
Environment-based Data Management System (GEDAS), that provides
a high-level, user-friendly interface, while maintaining the consistent data
replicas among the grid communities. We describe the design and implementation of GEDAS and present performance results on Linux cluster.

1

Introduction

In data grid environments, many large-scale scientiﬁc experiments and simulations generate very large amounts of data [1, 2, 3](on the order of several hundred
gigabytes to terabytes) in the geographically distributed storages. Furthermore,
these data are shared between the researchers and colleagues for data analysis,
data visualization, and so forth. Several data replication techniques, including
Globus toolkit [4, 5, 6], have been developed to support high-performance data
accesses to the remotely produced scientiﬁc data.
However, most of those data replication techniques were implemented with
the assumption that the data being replicated is read-only so that once it has
been generated would it not be modiﬁed in any grid site. Furthermore, those
V.S. Sunderam et al. (Eds.): ICCS 2005, LNCS 3514, pp. 485–492, 2005.
c Springer-Verlag Berlin Heidelberg 2005

486

J. No and H. Park

techniques mainly focus on measuring up the network performance, but ignoring
the I/O overhead incurred during the data generation and replication on the
heterogeneous storages.
We have developed a software system, called Grid Environment-based Data
Management System (GEDAS), that provides a high-level, user-friendly interface
to share the remotely produced data among the grid communities. As a related
work, we have implemented Scientiﬁc Data Manager (SDM) that combines the
good features of both ﬁle I/O and databases [7, 8]. We, in GEDAS, extend the
SDM capabilities to maintain the consistent data replicas among the grid communities and to support high-performance I/O using MPI-IO to store the real
and replicated data. GEDAS interacts with database to store application-related
and system-related metadata to support the integrated data replicas and highperformance I/O on the distributed resources, while taking advantage of various
I/O optimizations available in MPI-IO, such as collective I/O and noncontiguous
requests, in a manner that is transparent to the user.
The rest of this paper is organized as follows. In Section 2, we discuss our
goals in developing GEDAS. In Section 3, we present the design and implementation of GEDAS. Performance results on the Linux cluster located at Sejong
University are presented in Section 4. We conclude in Section 5.

2

Design Motivation

Our main objectives in developing GEDAS were to maintain consistent data
replication among the geographically distributed sites, to provide high-performance
parallel I/O, to provide a high-level application programming interface (API)
and to support a convenient data-retrieval capability.
– Consistent Data Replication. In order to maintain the consistently integrated data replicas among the distributed resources, GEDAS uses the
version checking method that is much similar to the locking mechanism of
distributed ﬁle systems [9, 10]. Whenever an application running on a site
modiﬁes the replicated data, it informs the data modiﬁcation to the sites that
share the same data, including the owner that originally generated the data,
using the version number, and thus would have them access the recently
modiﬁed data.
– High-Performance I/O. To achieve high-performance I/O, GEDAS uses
MPI-IO to access real data. MPI-IO, the I/O interface deﬁned as part of
the MPI-2 standard [11], is rapidly emerging as the standard, portable API
for I/O in parallel applications. High-performance implementations of MPIIO, both vendor and public-domain implementation, are available for most
platforms. MPI-IO is speciﬁcally designed to enable the optimizations that
are critical for high-performance parallel I/O. Examples of these optimizations include collective I/O, the ability to access noncontiguous data sets,
and the ability to pass hints to the implementation about access patterns,
ﬁle-striping parameters, and so forth.

GEDAS: A Data Management System for Data Grid Environments

487

– High-Level API. Our goal was to provide a high-level uniﬁed API for any
kind of application (regular or irregular) while encapsulating the details of
either MPI-IO or a database. The user can specify the data with a highlevel description, together with annotations, and use a similar API for data
retrieval. GEDAS internally translates the user’s request into appropriate
MPI-IO calls, including creating MPI derived data types for noncontiguous
data. GEDAS also interacts with the database when necessary, by using
embedded SQL functions.

3

Implementation Details

We describe the design and implementation of GEDAS.
3.1

GEDAS Metadata Structure

Figure 1 describes an overview of GEDAS. GEDAS stores application-related
metadata and system-related metadata to application registry table, data registry
table, ﬁle registry table, performance registry table, and system registry table.
These data base tables are made for each application to provide a high-level remote data access abstraction, while supporting a transparent, user-friendly user
interface.

Fig. 1. GEDAS Architecture

488

J. No and H. Park

The data registry tabe includes the properties of each data set, such as data
type, storage order, data access pattern, and global size. Also, it contains data
replication-related metadata, such as the owner ip, replication location ip, and
version number to be used in checking the replica consistency.
The ﬁle registry table stores a globally determined ﬁle oﬀset denoting the
starting oﬀset of the ﬁle of each data set. GEDAS uses this information to make
appropriate MPI-IO calls to access the real data. The ﬁle registry table also
includes the physical ﬁle name to be mapped to the data set.
The system registry table contains the system-related metadata, including
ﬁlesystem type and striping unit size where the real data is stored. This information is used to choose the optimal ﬁle layout for the real data storage.
The performance registry table includes the performance values evaluated
using buﬀered I/O and directed I/O on each ﬁlesystem type registered in the
system registry table. In the buﬀered I/O, all the data transfers between the
user buﬀer and the storage go through the kernel buﬀer. On the other hand, in
the directed I/O, the data transfers bypass the kernel. These values are measured
using a multiple of 2Mbytes of data size. When the data sets are accessed on a
certain registered ﬁlesystem, GEDAS compares the performance values evaluated
using buﬀered I/O and direct I/O to those that have been obtained using the
closest data granularity in the performance registry table. If the direct I/O is
selected as a better I/O option, then the direct I/O ﬂag deﬁned in the MPI-IO
is turned on to bypass the kernel buﬀer during the data transfer.
GEDAS provides the capability to maintain the consistent data replication
among the remote resources using the data version checking. When the data
requested by an application could not be found on the local storage, GEDAS
connects the data owner to replicate the data to the local storage. The associated metadata stored in the application registry table, data registry table, and
ﬁle registry table are also replicated to the local database.
If the data set needed to the application has been replicated before, then
GEDAS asks the data owner for the version number associated to the data
set. When the version number stored in the local database table and the one
received from the data owner are the same, GEDAS reads the data replica from
the local storage. Otherwise, GEDAS invalidates the data replica stored in the
local storage, and then receives the data from the data owner, while updating
the version number in the local database to the highest one.
3.2

GEDAS API

In GEDAS, users can specify groups of data sets by assigning properties to
the ﬁrst data set in a group and by propagating them to the other data sets
belonging to the same group. The main reason for making groups of data sets
is that GEDAS can then use diﬀerent ways of organizing data in ﬁles, with
diﬀerent performance implications. For example, each data set can be written
in a separate ﬁle, or the data sets of a group can be written to a single ﬁle.
The properties assigned to each data set are stored in the database by invoking
GEDAS set attributes. In case of read operation, data from a speciﬁc run can be

GEDAS: A Data Management System for Data Grid Environments

489

retrieved by specifying attributes of the data, such as the date of the run. Also,
the properties of the data sets need not be speciﬁed because GEDAS retrieves
this information from the database.
The main GEDAS functions for writing and reading data are GEDAS write
and GEDAS read. Before calling these functions, the user must provide the information necessary for GEDAS to perform I/O, such as the starting points and
sizes of the subarray in each dimension in the case of block distribution, or the
size of process grids and distribution arguments in each dimension in the case
of cyclic distribution.

4

Performance Evaluation

10

10

8

8

6
B/W

4

B/W (MB/Sec.)

B/W

(MB/sec.)

In order to measure the performance, we used two Linux clusters located at
Sejong university. Each cluster consists of four nodes having Pentium3 866MHz
CPU, 256 MB of RAM, and 100Mbps of Fast Ethernet each. The operating
system installed on those machines was RedHat 9.0 with Linux kernel 2.4.20-8.
Each cluster uses its own PostgreSQL to store the metadata.
The performance results were obtained using the template implemented based
on the three-dimensional astrophysics application, developed at the University
of Chicago. The application stores block-distributed data in each dimension. In
the template, we tested GEDAS library for the cyclic-distributed data as well.

6
B/W

4
2

2

0

0
Indep. Interm. Long-.
File Layout

Fig. 2. Bandwidth to access the data from
the remote owner in the block distribution. The accessed data is stored on the
remote owner in three ﬁle layouts: independent ﬁle layout (Indep.), intermediate
ﬁle layout (Interm.), and long-length ﬁle
layout (Long-.)

Indep. Interm. Long-.
File Layout

Fig. 3. Bandwidth to access the data
from the remote owner in the cyclic distribution. The accessed data is stored
on the remote owner in three ﬁle layouts: independent ﬁle layout (Indep.),
intermediate ﬁle layout (Interm.), and
long-length ﬁle layout (Long-.)

490

J. No and H. Park

On one of the clusters, the template has generated six ﬂoating-point arrays for
data analysis, another six ﬂoating-point arrays for restart, and seven character
arrays for visualization. Once those data are generated, we accessed the data
from the other cluster using the GEDAS library. The template grouped the data
sets into three data groups, according to the usage.
Figures 2 and 3 describe the performance results when the data sets are
accessed on one cluster, while the requested data sets are stored on the other
cluster in three diﬀerent ways of ﬁle layout. The data access patterns tested
are block distribution in Figure 2 and cyclic distribution in Figure 3. The performance values include the database overhead to access the associated metadata.
As can be seen, the bandwidth measured in the block-distributed data access
pattern is higher than that measured in the cyclic data access pattern due to the
smaller communication overhead incurred in the MPI-IO’s collective operation.
In both distributions, the independent ﬁle layout shows the least bandwidth
because it generates more ﬁles than the other two ﬁle layout methods, resulting
in the higher ﬁle-open and close costs.
Figures 4 and 5 demonstrate the performance results obtained to access the
replica of the desired data sets in the block distribution and cyclic distribution,
respectively. The replicated data is stored in the independent layout, intermediate layout, and long-length layout. When compared to Figures 2 and 3, accessing
the data replica shows almost as much as twice bandwidth accessing from the
remote owner because of the less communication overhead.

10

18
16

8

12
10

B/W

8
6

B/W (MB/Sec.)

B/W (MB/Sec.)

14

6

B/W
4
2

4
2

0

0

Indep.

Interm. Long-.

Indep.

Interm. Long-.
File Layout

File Layout

Fig. 4. Bandwidth to access the data
replica in the block distribution, on top of
three ﬁle layouts: independent ﬁle layout
(Indep.), intermediate ﬁle layout (Interm.),
and long-length ﬁle layout (Long-.)

Fig. 5. Bandwidth to access the data
replica in the cyclic distribution, on top
of three ﬁle layouts: independent ﬁle
layout (Indep.), intermediate ﬁle layout (Interm.), and long-length ﬁle layout (Long-.)

10

12

8

10

6
Time

4
2

Time (sec.)

Time (Sec.)

GEDAS: A Data Management System for Data Grid Environments

491

8
6

Time

4
2

0
Indep. Interm. Long-.
File Layout

Fig. 6. Open and close overheads of three
ﬁle layouts in time

0

conn. selectcomm. insert
Command

Fig. 7. Database overhead in time for
the data replication

Figure 6 shows the time overhead to open and close ﬁles, on top of Linux
kernel 2.4.20-8. In the template, the independent ﬁle layout produces ﬁfty-seven
ﬁles: eighteen ﬁles for data analysis, twenty-one ﬁles for data visualization, and
another eighteen ﬁles for restart. The intermediate ﬁle layout generates nineteen
ﬁles: six ﬁles for data analysis, another six ﬁles for restart, and seven ﬁles for
data visualization. The long-length ﬁle layout generates only three ﬁles, one for
each data group.
Figure 6 shows that the less ﬁles are generated to store the data sets, the less
ﬁle-open and close costs are incurred on Linux kernel 2.4.20-8.
Figure 7 shows the times to access the associated metadata from the remote database necessary for replicating the desired data set to the local storage.
It demonstrates that the cost for inserting the metadata received to the local
database is the highest overhead in the metadata communication.

5

Conclusion

We have presented the design and implementation of a toolkit, called GEDAS,
for high-performance scientiﬁc data management in data grid environments.
GEDAS provides a simple, high-level interface and performs all necessary I/O
optimizations to access and replicate the remote data transparently to the user.
We also experimented with diﬀerent ways of organizing data in ﬁles, called independent ﬁle layout, intermediate ﬁle layout, and long-length ﬁle layout. In
general, when ﬁle-open cost on a particular ﬁle system is high, long-length ﬁle
layout performs well because it minimizes the number of ﬁles created. If the
ﬁle-open cost is small, the performance of the three ﬁle layouts depends on how
the number and size of ﬁles aﬀect performance on the particular ﬁle system. An

492

J. No and H. Park

appropriate ﬁle layout policy can thereby be chosen for a particular ﬁle system.
Also, we compared the performance produced in accessing the data sets from
replicas to that in accessing the data sets from the remote owner. In the future,
we plan to use GEDAS with more applications and evaluate both the usability
and performance.

References
1. B. Allcock, I. Foster, V. Nefedova, A. Chervenak, E. Deelman, C. Kesselman,
J. Leigh, A. Sim, A. Shoshani, B. Drach, and D. Williams. High-Performance
Remote Access to Climate Simulation Data: A Challenge Problem for Data Grid
Technologies. SC2001, November 2001
2. R. Moore, A. Rajasekar. Data and Metadata Collections for Scientiﬁc Applications.
High Performance Computing and Networking (HPCN 2001), Amsterdam, NL,
June 2001
3. A. Chervenak, E. Deelman, C. Kesselman, L. Pearlman, and G. Singh. A Metadata
Catalog Service for Data Intensive Applications. GriPhyN technical report, 2002
4. I. Foster, C. Kesselman, J. Nick, and S. Tuecke. The Physiology of the Grid: An
Open Grid Services Architecture for Distributed Systems Integration WG, Global
Grid Forum, June 22, 2002
5. A. Chervenak, I. Foster, C. Kesselman, C. Salisbury, S. Tuecke. The Data Grid:
Towards an Architecture for the Distributed Management and Analysis of Large
Scientiﬁc Datasets. Journal of Network and Computer Applications, 23:187-200,
2001
6. I. Foster, C. Kesselman. Globus: A Metacomputing Infrastructure Toolkit. Intl J.
Supercomputer Applications, 11(2):115-128, 1997.
7. J. No, R. Thakur, and A. Choudhary. High-Performance Scientiﬁc Data Management System. Journal of Parallel and Distributed Computing, (64)4:434-447, April
2003
8. X. Shen, A. Choudhary. A Multi-Storage Resource Architecture and I/O Performance Prediction for Scientiﬁc Computing. In 9th IEEE Symposium on High
Performance Distributed Computing, 2000
9. C. A. Thekkath, T. Mann, and E. K. Lee. Frangipani: A Scalable Distributed File
System. In Proceedings of the Symposium on Operating Systems Principles, 1997,
pages 224–237
10. K. W. Preslan, A. P. Barry, J. E. Brassow, G. M. Erickson, E. Nygaard, C. J. Sabol,
S. R. Soltis, D. C. Teigland, and M. T. O’Keefe. A 64-bit Shared Disk File System
for Linux. In Proceedings of Sixteenth IEEE Mass Storage Systems Symposium
Seventh NASA Goddard Conference on Mass Storage Systems & Technologies,
March 15-18, 1999
11. R. Thakur and W. Gropp. Improving the Performance of Collective Operations in
MPICH. In Proceedings of the 10th European PVM/MPI Users’ Group Conference
(Euro PVM/MPI 2003), September 2003

