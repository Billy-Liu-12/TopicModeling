On the Fundamental Tautology of Validating
Data-Driven Models and Simulations
John Michopoulos and Sam Lambrakos
Materials Science and Technology Division,
U.S. Naval Research Laboratory, Washington, DC. 20375, U.S.A
{john.michopoulos, lambrakos}@nrl.navy.mil

Abstract. Recent advances in Dynamic Data Driven Application Systems (DDDAS) facilitated by the present level of computational technologies, as well as advances in data-driven modeling and simulation,
impose the need for a critical evaluation of paradigms underlying Qualiﬁcation, Validation and Veriﬁcation (QV&V). This paper discusses the
fundamental irrelevance of conventional validation procedures with respect to data-driven models and simulations. This inherent property of
data-driven models and simulations makes the data-driven approaches
extremely desirable from a reliability perspective. An informal comparison of the logical ﬂow of traditional and evolved QV&V demonstrates
the tautological nature of data-driven model validation. A brief epistemological review of the origins of traditional and evolved QV&V is also
presented.

1

Introduction

The need for high ﬁdelity simulation of realistic size and complexity systems have
motivated studies concerning methodologies for the development of Dynamic
Data Driven Application Systems (DDDAS) by various groups. The National
Science Foundation has taken a leadership role in fostering research on such
eﬀorts through the initiative on DDDAS [1].
In the past, aspects of DDDAS and dynamic data in particular, have been
used to reduce the eﬀect of uncertainties on weather prediction modeling methods, and correct error propagation to improve the ﬁdelity of model predictions.
In addition, data-driven aspects of DDDAS have also been used in the past for
very successful material characterization.
Recent advances in DDDAS facilitated by the present level of computational
achievements, as well as advances in data-driven modeling and simulation, impose the need for a critical evaluation of paradigms underlying Qualiﬁcation,
Validation and Veriﬁcation (QV&V). This paper focuses on the fundamental
irrelevance of conventional validation procedures with respect to data-driven
models and simulations. This follows since conventional validation procedures
are based on paradigms and associated terminology, that are historically biased
and related to concepts and implementations that do not correspond to today’s
advanced computational practices and possibilities.
V.S. Sunderam et al. (Eds.): ICCS 2005, LNCS 3515, pp. 738–745, 2005.
c Springer-Verlag Berlin Heidelberg 2005

On the Fundamental Tautology of Validating Data-Driven Models

739

Additional areas of contemporary interest where there is a need for dynamic
data-driven Modeling and Simulation (M&S) of systems are multi-physics systemic behavior prediction, multi-scale ﬁre dynamics prediction, disaster management decision support, bio-chemical contaminant dispersion prediction, hydrogen storage, welding and heat deposition processes, artiﬁcial muscle autonomous
systems, fuel cells, soft robotics and nano-robotics applications, to name only a
few.
There are formal aspects of “data-driven system modeling” that are related
to methods of “system identiﬁcation” and “parameter estimation,” which are
based on formal systems theory. In the discussions that follow, the concept of
system identiﬁcation has been extended to include that of data-driven modeling.
Further, “dynamic” data driven is to be understood as implying system identiﬁcation within the context real-time input of data, which is considered as having
been obtained most recently from sensors. Similarly, the use of most recently
obtained data for adaptive simulation control or steering, constitutes the second
use of the term “dynamic data driven” as has been discussed elsewhere [2, 3, 4].
A key aspect of our development concerns the concept of “weak termination”
that implies informally the incomplete character of validation with respect to the
predictive capability of a model. A rigorous examination of the mathematical
foundations of this concept is beyond the scope of this presentation in that it
relates to temporal and ontological formalisms of logical meta-systems [5].
In this paper, we ﬁrst present the deﬁnitions and origins of the particular
QV&V terms. We subsequently identify how weak termination undermines the
ﬁdelity, accuracy and reusability of models. We further demonstrate how the application of data-driven M&S avoids weak termination and enables reusability of
models and simulations. We conclude with a presentation of the association of scientiﬁc methods with forward and inverse (data-driven) modeling, indicating that
forward modeling is an artifact of practices based on applying the hypotheticodeductive method. These practices are not necessary when behavioral data can
be collected and utilized for M&S.

2

Qualiﬁcation, Validation and Veriﬁcation

Model ﬁdelity, accuracy, and high conﬁdence in predictability requires that contemporary M&S are subject to various QV&V procedures. There are many descriptions of how QV&V relates to modeling and simulation. Figure 1 represents
the modeling and simulation process along with the associated QV&V in terms of
logical ﬂow. This logical ﬂow represents a uniﬁcation of the abstractions deﬁned
by many organizations such as the AIAA [6], ASME [7], DoD’s Defense Modeling and Simulation Oﬃce (DMSO) [8] and DOE Defense Program’s (DOE/DP)
Accelerated Strategic Computing Initiative (ASCI) [9]. The dotted arrows shown
in this ﬁgure represent human activities that are implemented with various degrees of automation, allowing a transitioning from the physical system to the
system’s conceptual model (via analysis), next to the computational model (via
computational methods), and then back to the physical system (via simulation).

740

J. Michopoulos and S. Lambrakos

A conceptual model is constructed by analysis of the behavioral structure of
the physical system within an application context. The conceptual model can
be a set of partial diﬀerential equations (PDEs) representing conservation laws
with appropriate constitutive equations. Typically, this type of mathematical
representation is known as an analytical model that encapsulates the conceptual
model. This model reproduces the behavior of the physical system and belongs
to the class of models designated as “explicit” or “physics-aware” [10].
Another form of conceptual model is that of a “physics-agnostic” model such
as rule-based (e.g. Cellular Automata or Genetic Algorithms) or input-output associator technologies (e.g. Neural Nets, Perceptrons, Support Vector Machines).
The corresponding analytical or mathematical model introduces errors associated with its underlying idealized assumptions.
The computational model is encapsulated by the software that implements
the conceptual model within the computational infrastructure. It is constructed
by a variety of programming techniques based on various degrees of computational automation ranging from manual to automated software generation that
exploits the ability of commercial design tools. The computational model justiﬁes the need for veriﬁcation because it introduces additional uncertainty that
is associated with space and time discretization, i.e. errors associated with the
discrete representation of diﬀerential operators and machine truncation.
The formal deﬁnitions of the QV&V terms are as follows [6]:
– Qualiﬁcation is the process of determining that a conceptual model implementation represents correctly a real physical system.
– Veriﬁcation is the process of determining that a computational model implementation represents correctly a conceptual model of the physical system.
– Validation is the process of determining the degree to which a computer
model is an accurate representation of the physical system from the perspective of the intended uses of the model.
Both validation and qualiﬁcation attempt to establish the representational ﬁdelity of the conceptual (qualiﬁcation) and computational (validation) models
relative to the physical system. For this reason, qualiﬁcation may be considered
another form of validation. This consideration explains why most of the bibliography concerns veriﬁcation and validation (V&V). Veriﬁcation attempts to
determine the error and accuracy between two models (conceptual and computational). The modeling of uncertainty has to address the error originating from
various sources extending from sensor device noise to algorithmic approximation and computational implementation of mathematical primitives by particular
hardware architectures.
Shown in Fig. 1 are QV&V concepts in terms of a comparison between the
behavior of the physical system (via experimentation), and the conceptual and
computational systems (via simulation). In addition to complexity, the most critical issue associated with this data-driven behavioral comparison among the various system representations is “weak termination”. Unfortunately, termination
of the recursive process of minimizing the diﬀerences between model predicted
data sets and data sets generated by the physical system, is not guaranteed for

On the Fundamental Tautology of Validating Data-Driven Models

741

Fig. 1. Traditional QV&V for the M&S process

the user deﬁned accuracy (as a measure of ﬁdelity of simulation) in a ﬁnite time,
nor is it guaranteed that there is a unique model prediction that converges to the
physical system behavior with a desirable speed. This is represented clearly by
the fact that as long as the comparison modules in Fig. 1 evaluate to “no,” the
methodology will continue to iterate. This situation forces the user in general,
to employ “engineering approximations.” That is to say, attitudes of accepting
models within “acceptable bounds” for speciﬁc applications, which consequently
lead to a plethora of low-conﬁdence models that vary according to the personal
modeling assumptions of the speciﬁc user.

3

Embedding Validation into Modeling and Simulation

Addressing weak termination as well as moving towards a realistic approach
that guarantees relative ﬁdelity in a model requires considering an alternative
approach. This approach involves implementing an M&S methodology that can
utilize: (1) data-streams of controlled continuous system behavior (stimulusresponse pairs of all observables regardless of their ﬁeld or non-ﬁeld nature); (2)
analytical representations of the model that can accurately reproduce a subset
of the acquired data via system identiﬁcation through successive dynamic model
adaptation with the help of optimization techniques (encoding intrinsically the
validity of the derived model); and (3), derived models for simulating the predictive response of the originally modeled system or any other system that shares
the identiﬁed continuous behavior with the original system.
Figure 2 shows the proposed data driven M&S process as a modiﬁcation
of that shown in Fig. 1. The modules located within the dashed-line region
constitute an alternative conceptual model that is deﬁned by the optimization
structure comprising these modules. This approach eﬀectively embeds data asso-

742

J. Michopoulos and S. Lambrakos

Fig. 2. Evolved QV&V for data-driven M&S process

ciated with observed behavior of the physical system, into the conceptual model,
thus guaranteeing the intrinsic validity of the conceptual system. For continuous
systems, this model usually refers to the mathematical formulation representing
the conservation laws and associated constitutive equations. Since no comparative module exists between the physical and simulated data (see Fig. 2) this
methodology eliminates entirely the weak termination problem. Termination is
strong because the adaptively computed model is trained on the behavioral data
and therefore must terminate if it satisﬁes the optimization criteria, including
any acceptable error tolerance between actual and simulated data. Accordingly,
termination occurs prior to using the model for prediction of behavior.

4

Epistemological Origins of QV&V in M&S

A more detailed examination of the logic underlying V&V in traditional M&S
as shown in Fig. 1, indicates a paradigm for prediction that is based on the
hypothetico-deductive scientiﬁc method (HDSM) [11], and that is applied by the
majority of researchers. This paradigm was ﬁrst applied by Newton [12, 13], who
established the foundation for the Kantian reconciliation of the Cartesian rationalism with the Baconian empiricism [11]. Its intermediate stages were marked
by the inﬂuence of positivism that was based on Compte’s strict empiricist claims
that valid knowledge is only that which is based on experience [12, 13]. The “Vienna Circle” of scientists and logicians, further contributed by establishing the
logical positivism aspect of the method, which accepted Wittgenstein’s [12] veriﬁcation theory of meaning that considers as meaningful only those prepositions
(theories) that can be empirically veriﬁed.
The HDFM program involves the steps of making an original observation of a
physical system’s behavior, making a hypothesis (i.e. making a highly idealized

On the Fundamental Tautology of Validating Data-Driven Models

743

abstract model) that may explain (predict) the observation, formulating a theory
to be used for predictions, and ﬁnally generating observations via experiments for
the purpose of verifying [14] or falsifying [15] the theory. If the experiment veriﬁes the theory, then the theory can be considered valid until another experiment
renders it false. Further softening of these attitudes resulted from the Logical
Empiricism of Carnap [16] that replaced the idea of veriﬁcation with the idea of
“gradually increasing conﬁrmation.” That is to say, if the experiment falsiﬁes the
theory, then one must change the hypothesis and/or the theory and then reattempt validation. The most distinctive and highly argued [15] characteristics of
a theory generated by this approach, are that it is tentative, non-permanent and
iterative. These characteristics imply the presence weak termination as identiﬁed
earlier for the M&S. There is a strong tendency in the literature to link Popper’s
falsiﬁcation idea to the inductive scientiﬁc method leading to inverse modeling.
From our perspective however, since weak termination is inherently present (i.e.
if a contradictory experimental result appears, then the model is false), we have
considered this approach as just a more robust HDSM than that of Hempel [10].
As an alternative to a strict adherence to a single scientiﬁc method, Critical
Relativism advocates [17] that there is no single scientiﬁc method. Instead, disciplinary knowledge (as exercised through particular scientiﬁc methods) is to be
viewed as contingent upon the beliefs, values, standards, methods and cognitive
aims of its practitioners. Consequently, critical relativism rejects the aspect of
positivism that implies that there is a single knowable reality to be discovered
via the scientiﬁc method [18]. This relativist view permits the transcendence of
the inductivist idea of Roger and Francis Bacon, that was recommended (but not
practiced) by Newton and systematized by Herschel [12, 13]. Accordingly, this
idea tunneled trough HDFM practices, and into the middle sixties at the Naval
Research Laboratory (NRL) as an industrialized inductive pragmatic scientiﬁc
method (IIPSM) [10, 19, 20].
At NRL, the advent of coupling hydraulically controlled mechanical power
technology with that of computational management and control in the area of
testing machines as eﬃcient empirical fact collectors provided the industrialized character of eﬃciency through automation. The inductive character of the
method was provided by insisting that only theories and/or models of continuum system behavior that are consistent with the facts be considered as their
functional abstractions. The pragmatism of Pierce and James [12] provided the
scope for such a method by enforcing the idea that what matters is not the
structural elegance of a theory or model, but instead, the conﬁdence and ﬁdelity
of prediction of a theory/model that makes it suitable for application.
The IIPSM program involves the massive automated acquisition of experimental measurements of a physical system’s behavioral response. This is followed
by construction of a theory in the form of a speciﬁc mathematical representation with adjustable parameters that are capable of encoding system behavior.
The values of these parameters are assigned by minimizing the error between
predicted and measured data sets. Finally, the theory is used for predicting system response given the condition that the stimulus to behavior (or input to the

744

J. Michopoulos and S. Lambrakos

system) is within the bounds of the original experimentally observed data-set. it
follows that for this approach theory/model validation is inherent (since it has
been constructed to reproduce the observed data) and therefore weak termination of its veriﬁability is not relevant.
In the case of HDSM we start with a small amount of data (or no data at
all) that represents casual empirical observations, then we generate a plausible
theory/model (a set of equations associated with a given axiomatic foundation),
then we generate a predicted-behavior data set (simulation), and ﬁnally we perform experimentation to collect the actual behavior data set, in order to compare
the two. This implies a logical ﬂow from theory to prediction and then to data.
In the sense that we are marching forward from theory to data, this approach has
also been referred to by various investigators as the “forward,” “theory-driven”
or “traditional” approach. On the other hand, the IIPSM implies a logical ﬂow
from data to theory, and then to prediction. Relative to the directionality of
logical ﬂow between data and theory, this approach is inverse to the HDSM and
is termed the “data-driven” approach.

5

Conclusions

Our discussion has brought to light signiﬁcant aspects of validation as they relate to M&S. In particular, that validation is actually not needed by those who
pursue data-driven adaptive M&S methods because it is an inherent property of
these methods. This inherent validation makes data-driven methods very desirable from a reliability perspective and suggests that when enough data can be
collected then data-driven methods should be considered favorably.

Acknowledgments
The authors acknowledge the support by the National Science Foundation under
grant ITR-0205663 and express their gratitude to Dr. F. Darema for her support
of our DDDAS-related activities. Partial support from NRL’s 6.1 core-funding
is also greatly acknowledged.

References
1. Darema, F., Dynamic Data Driven Applications Systems: A New Paradigm for
Application Simulations and Measurements. International Conference on Computational Science - ICCS’04, Krakow, Poland, June 6-9, 2004, Part III Series: Lecture
Notes in Computer Science, Bubak, M.; Albada, G.D.v.; Sloot, P.M.A.; Dongarra,
J. (Eds.), LNCS 3038, (2004), 662-669.
2. Michopoulos, J., Tsompanopoulou, P., Houstis, E., Rice, J., Farhat, C., Lesoinne,
M., Lechenault, F., DDEMA: A Data Driven Environment for Multiphysics Applications,in: Proceedings of International Conference of Computational Science ICCS’03, Sloot, P.M.A., et al. (Eds.) Melbourne Australia, June 2-4, LNCS 2660,
Part IV, Springer-Verlag, Haidelberg, (2003), 309-318.

On the Fundamental Tautology of Validating Data-Driven Models

745

3. Michopoulos, J., Tsompanopoulou, P., Houstis, E., Farhat, C., Lesoinne, M., Rice,
J., Joshi, A., On a Data Driven Environment for Multiphysics Applications, Future
Generation Computer Systems, in-print (2005).
4. Michopoulos, J., Tsompanopoulou, P., Houstis, E., Joshi, A., Agent-based Simulation of Data-Driven Fire Propagation Dynamics, Proceedings, International Conference Computational Science - ICCS’04, Krakow, Poland, June 6-9, 2004, Part
III Series: Lecture Notes in Computer Science, Bubak, M.; Albada, G.D.v.; Sloot,
P.M.A.; Dongarra, J. (Eds.), LNCS 3038, (2004), 732-739.
5. Van Benthem, J.F.A.K, The Logic of Time, D. Reidel Publishing Company, Doordrecht, (1949).
6. American Institute of Aeronautics and Astronautics Standards Program, Guide
for the Veriﬁcation and Validation of Computational Fluid Dynamics Simulations,
AIAA report G-077-1998, (1998).
7. ASME-JFE, Journal of Fluids Engineering Editorial Policy Statement on Control
or Numerical Accuracy, Jnl. Of Fluids Engineering, Vol. 115, 3, (1993), 339-340.
8. DoD, Veriﬁcation, Validation, and Accredization (VV&A) Recommended Practices Guide, Defense Modeling Simulation Oﬃce, Oﬃce of the Director of Defense
Research and Engr., available: www.dmso.mil/docslib
9. M. Pilch, T. Trucano, J. Moya, G. Froelich, A. Hodges, D. Peercy, Guidelines for
Sandia ASCI Veriﬁcation and Validation Plans - Content and Format: Version 2.0,
Sandia Reports SAN2000-3101, (2001).
10. Michopoulos, J.G., Mechatronically Automated Characterization of Material Constitutive Response, in Proceedings of the 6th World Congress on Computational
Mechanics (WCCM-VI), September 5-10 2004, Beijing China, Tsinghua University
Press and Springer, (2004) 486-491.
11. P. Thagard, Computational Philosophy of Science, MIT Press, Cam- bridge, MA,
(1988).
12. Cooper, D.E., Epistemology: The Classic Readings, Blackwell Publ., Oxford,
(1999).
13. Losee, J., A Historical Introduction to the Philosophy of Science, Oxford Un. Press,
Oxford (2001).
14. C. G. Hempel, Aspects of Scientiﬁc Explanation, Free Press, New York, (1965).
15. K. Popper, Unended Quest, Fontana, London, 1976.
16. Carnap, R., The Philosophy of Science, New York, NY: Basic Books, (1966).
17. Anderson, P.F., Marketing, Scientiﬁc Progress, and Scientiﬁc Method, Journal of
Marketing, 47, (Fall), (1983), 18-31.
18. Olson, J.C., Towards a Science of Consumer Behavior, in Advances in Consumer
Research, Vol. 9, ed. Andrew A. Mitchell, Ann Arbor, MI: Association for Consumer Research, v- x, (1981).
19. Michopoulos, J.G., Computational and Mechatronic Automation of Multiphysics
Research for Structural and Material Systems, Book Chapter, in ”Recent advances
in Composite Materials” in honor of A.A. Paipetis, Kluwer Academic publishing
(2003).
20. Lambrakos, S.G., and Milewski, J.O., Analysis of Processes Involving Heat Deposition Using Constrained Optimization, Sci. and Tech. Of Welding and Joining, 7
(3), (2002), 137.

