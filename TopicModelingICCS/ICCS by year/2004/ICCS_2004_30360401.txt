Communication Primitives for Minimally
Synchronous Parallel ML
Fr´ed´eric Loulergue
Laboratory of Algorithms, Complexity and Logic, Cr´eteil, France
loulergue@univ-paris12.fr

Abstract. Minimally Synchronous Parallel ML is a functional parallel
language whose execution time can then be estimated and dead-locks and
indeterminism are avoided. Programs are written as usual ML programs
but using a small set of additional primitives. It follows the cost model
of the Message Passing Machine model (MPM). This paper explore two
versions of an additional communication function: one uses this small set
of primitives, the other one is considered as a primitive and implemented
at a lower level.

1

Introduction

Bulk Synchronous Parallel ML (BSML) is a functional parallel language for
the programming of Bulk Synchronous Parallel (BSP) [4] algorithms. It is an
extension of the ML family of languages by a small set of parallel operations
taken from a conﬂuent extension of the λ-calculus. It is thus a deterministic and
dead-lock free language.
We designed a new functional parallel language, without the synchronization
barriers of BSML, called Minimally Synchronous Parallel ML (MSPML) [1]. As a
ﬁrst phase we aimed at having (almost) the same source language and high level
semantics (programming view) than BSML (in particular to be able to use with
MSPML work done on type system and proof of parallel BSML programs), but
with a diﬀerent (and more eﬃcient for unbalanced programs) low-level semantics
and implementation.
MSPML will also be our framework to investigate extensions which are not
suitable for BSML, such as the nesting of parallel values or which are not intuitive enough in BSML, such as spatial parallel composition. We could also mix
MSPML and BSML for meta-computing. Several BSML programs could run on
several parallel machines and being coordinated by a MSPML-like program.
MSPML Programs are written as usual ML programs but using a small set
of additional functions. Provided functions are used to access the parameters of
the parallel machine and to create and operate on a parallel data structure. This
paper explore the writing of an additional communication function using this
small set of primitives. This function could also be considered as a primitive.
These two versions are compared.
M. Bubak et al. (Eds.): ICCS 2004, LNCS 3036, pp. 401–404, 2004.
c Springer-Verlag Berlin Heidelberg 2004

402

2

F. Loulergue

Minimally Synchronous Parallel ML

BSPWB, for BSP Without Barrier, is a model directly inspired by the BSP
model [4]. It proposes to replace the notion of super-step by the notion of m-step
deﬁned as: at each m-step, each process performs a sequential computation phase
then a communication phase. During this communication phase the processes
exchange the data they need for the next m-step.
The parallel machine in this model is characterized by three parameters (expressed as multiples of the processors speed): the number of processes p, the
latency L of the network, the time g which is taken to one word to be exchanged
between two processes. This model could be applied to MSPML but it will be
not accurate enough because the bounds used in the cost model are too coarse.
The Message Passing Machine model (MPM) [3] gives a better bound Φs,i .
The parameters of the Message Passing Machine are the same than those of
the BSPWB model. The model uses the set Ωs,i for a process i and a m-step s
deﬁned as:
Ωs,i = {j/process j sends a message to process i at m-step s} ∪ {i}
Processes included in Ωs,i are called “incoming partners” of process i at
m-step s. Φs,i is inductively deﬁned as:
Φ1,i =
Φs,i =

max{w1,j /j ∈ Ω1,i } + (g × h1,i + L)
max{Φs−1,j + ws−1,j /j ∈ Ωs,i } + (g × hs,i + L)

−
where hs,i = max{h+
s,i , hs,i } for i ∈ {0, . . . , p − 1} and s ∈ {2, . . . , R}. Execution
time for a program is thus bounded by: Ψ = max{ΦR,j /j ∈ {0, 1, . . . , p − 1}}.
The MPM model takes into account that a process only synchronizes with
each of its incoming partners and is therefore more accurate. The MPM model
is used as the execution and cost model for our Minimally Synchronous Parallel
ML language.
There is no implementation of a full Minimally Synchronous Parallel ML
(MSPML) language but rather a partial implementation as a library for the
functional programming language Objective Caml [2] (using TCP/IP for communications). The so-called MSPML library is based on the following elements:

bsp_p: unit->int
mkpar: (int->’a)->’a par

apply: (’a->’b) par->’a par->’b par
get: ’a par->int par->’a par

It gives access to the parameters of the underling architecture which is considered as a Message Passing Machine (MPM). In particular, p() is p, the static
number of processes of the parallel machine. The value of this variable does not
change during execution. There is also an abstract polymorphic type ’a par
which represents the type of p-wide parallel vectors of objects of type ’a, one
per process. The nesting of par types is prohibited. This can be ensured by a
type system .
The parallel constructs of MSPML operate on parallel vectors. Those parallel
vectors are created by: mkpar so that (mkpar f ) stores (f i) on process i for

Communication Primitives for Minimally Synchronous Parallel ML

403

i between 0 and (p − 1). We usually write fun pid-¿e for f to show that the
expression e may be diﬀerent on each processor. This expression e is said to be
local. The expression (mkpar f ) is a parallel object and it is said to be global.
In the MPM model, an algorithm is expressed as a combination of asynchronous local computations and phases of communication. Asynchronous phases
are programmed with mkpar and the point-wise parallel application apply
which is such as apply(mkpar f )(mkpar e) stores (f i)(e i) on process i.
The communication phases are expressed by get. Its semantics is given by:
get v0 , . . . , vp−1
i0 , . . . , ip−1 = vi0 %p , . . . , vi(p−1)%p
When get is called, each process i stores the value vi in its communication
environment. This value can then be requested by a process j which arrived at
a later m-step.

3

A New Communication Function

The communication function described below uses functions from the MSPML
standard library (http://mspml.free.fr) and some sequential functions. Their semantics follow:
ft n1 n2 = [n1 ; n1 + 1; . . . ; n2 ]
procs() = [0, . . . , p()]
ﬁll e [x1 ; . . . ; xn ] m = [x1 ; . . . ; xn ; e; . . . ; e]
m−n

nﬁrst n [x1 ; . . . xn ] = [x1 ; . . . ; xk ] where k = min{m, n}
The get list function is similar to get but it takes as second argument a
parallel vector of lists of integers rather than a parallel vector of integers. Its
semantics is thus given by:
; . . . ; ijkp−1 ]
get list v0 , . . . , vp−1 [i01 ; . . . ; i0k0 ], . . . , [ij1 ; . . . ; ijkj ], . . . , [ip−1
1
= [vi01 ; . . . ; vi0k ] , . . . , [vij ; . . . ; vij ] , . . . , [vip−1 ; . . . ; vij ]
0

1

kj

1

kp−1

The problem is that the lists may not have the same length. Thus we will
proceed in two phases. First we deﬁne an auxiliary get list sl function which
takes a third argument: the length of the lists which are assumed to have the
same length. Then we use it to deﬁne the general get list function:
let rec get_list_sl vv vl = function
0 -> replicate []
| n -> let vh=parfun List.hd vl and vt=parfun List.tl vl in
let vg = get vv vh in
parfun2 (fun h t->h::t) vg (get_list_sl vv vt (n-1))
let get_list vv vl =
let vlen=parfun List.length vl in let mlen=reduce max vlen in
let vl2=apply2 (mkpar fill) vl mlen in
parfun2 nfirst vlen (get_list_sl vv vl2 (unsafe_proj mlen))
The reduce function has the following semantics:
reduce ⊕ v0 , . . . , vp−1 = ⊕0≤k<p vk , . . . , ⊕0≤k<p vk , . . . , ⊕0≤k<p vk .

404

F. Loulergue

The get list function implemented using the get primitive has MPM cost
given by the following formula: n + treduce + n × (s × g + L) where n is the length
of the biggest list, s is the size of each element of the lists and treduce is the
time required to compute the maximum length. For a direct reduce function
treduce = p + 2 × (g × (p − 1) + L).
The low level implementation does not need the computation of the maximum
length. Furthermore it is possible to use threads for the requests: a process will
send sequentially its requests to the processes given by the lists without waiting
for the answers. Thus the cost formula is: L+ni ×s×g plus an additional overhead
introduced by the use of the threads. This formula is given for a process i where
ni is the length of the integer list at process i.
We performed tests to compare the two get list using the following programs:
let mshift d l v=get_list v (mkpar(fun i->ft (i+d) (i+d+l)))
let pre n v=get_list v (mkpar(fun i->ft n (n+1+(nmod(i-n)(p())))))
We performed the tests on a cluster of 11 Pentium III processors with a fast
Ethernet dedicated network. Let summarizes the results where the values are the
average (10 tests from 2 to 11 processors were run) eﬃciency of the high-level
implementation with respect to the low-level implementation. For the mshift
function the ratio range from 20% to 60% for size between 1 and 1000. For sizes
greater than 10K the eﬃciency is almost the same for the two versions. For the
pre function the ratio is between 15% and 70%. The advantage of the primitive
decreases with the size but the asynchronous nature of the function makes the
advantage still interesting.

4

Conclusions and Future Work

We have explored how to write communication functions for the Minimally Synchronous Parallel ML language using only the unary get communication primitive: the get list function allows to receive messages from several processes.
It could also be considered as a communication primitive: the low-level parallel implementation described in this paper follows the execution model of the
Message Passing Model. This implementation is more eﬃcient but the proof of
correctness is not done, while it is simple for the ﬁrst version.

References
1. M. Arapinis, F. Loulergue, F. Gava, and F. Dabrowski. Semantics of Minimally
Synchronous Parallel ML. In W. Dosch and R. Y. Lee, editors, SNPD’03, pages
260–267. ACIS, 2003.
2. Xavier Leroy. The Objective Caml System 3.07, 2003. web pages at www.ocaml.org.
3. J. L. Roda, C. Rodr´ıguez, D. G. Morales, and F. Almeida. Predicting the execution
time of message passing models. Concurrency: Practice and Experience, 11(9):461–
477, 1999.
4. D. B. Skillicorn, J. M. D. Hill, and W. F. McColl. Questions and Answers about
BSP. Scientiﬁc Programming, 6(3):249–274, 1997.

