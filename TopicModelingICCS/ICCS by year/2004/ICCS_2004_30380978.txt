A Framework for 3D Polysensometric Comparative
Visualization
Javed I. Khan, Xuebin Xu, and Yongbin Ma
Media Communications and Perceptual Engineering Research Laboratory
Department of Math & Computer Science
Kent State University
233 MSB, Kent, OH 44242
{javed,xxu1,yma}@kent.edu

Abstract. Typically any single sensor instrument suffers from physical/observation constraints. This paper discusses a generalized framework,
called polysensometric visual information fusion framework (PVIF) that
enables information from multiple sensors to be fused and compared to gain
broader understanding of a target of observation. An automate software shell
supporting comparative cognition has been developed to form 3D models based
on the datasets from different sensors. This fusion framework not only provides
an informatic engineering tool to overcome the constraints of individual sensor’s observation scope but also provides a means where theoretical understanding surrounding a complex target can be mutually validated and incrementally enhanced by comparative cognition about the object of interest.

1 Introduction
3D visualization is becoming widespread and is being used in many fields. However,
a relatively new area is comparative visualization. The methodology of correlating
information from different experimental techniques to find the 3D structures improves
scientists’ understanding about the objects they work on. This has been recently
demonstrated in a growth in interest in combining CT (computerized tomography) and
MRI (magnetic resonance imaging) information [1,2]. Comparative cognition is an
important and strong means to obtain new knowledge about any complex system.
Multiple sets of data are collected, processed and compared in the process. The proper
application of these data sets is however often quite a complex process and depends
on the nature of the observation target, the field of research and the purpose of the
research is, etc. The most straightforward usage is to combine the data sets to obtain a
composite view about the interest object when these multiple sets of data are
complementary to one another. Current research in the area mainly has produced very
domain specific systems which fuses data from very specific modes. In our project we
investigate and propose how a generalized fusion framework can be constructed. We
propose an algorithmic model of multi-sensor information fusion that closely
correspond the cognitive process of comparison based knowledge exploration and
hypothesis refinement. As a proof of concept, along with it we have also developed a
new automated system. It accepts sets of information from multiple techniques and
M. Bubak et al. (Eds.): ICCS 2004, LNCS 3038, pp. 978–985, 2004.
© Springer-Verlag Berlin Heidelberg 2004

A Framework for 3D Polysensometric Comparative Visualization

979

combines them for 3D visualization. In the system, all sensor data are mapped into a
unified information domain. The system can be easily extended to any new sensor
instrument. Naturally the user knows the physical-chemical meaning of data. Thus
s/he can insert domain specific intermediate data processing elements and yet reuse
more generic or available information processing tools from library in a plug and play
fashion. The actual framework works as an exploration shell for its user which allows
experimentation. The paper briefly presents this new framework.
The following section first explains polysensometric visual information fusion
framework. Section 3 and 4 then describe the exploration algorithms for inferring a
3D models and the architecture of the exploration shell. Finally section 5 presents a
working example of polysensometric visualization which offers fusion of XPS X-ray
Photoelectron Spectroscopy (XPS) and 3D Laser Scanning Confocal Microscopy
(LSCM) data sets in the comparative study of liquid crystal film.

2 Polysensometric Visual Information Fusion (PVIF) Framework
The basic idea is illustrated in figure 1, in which we take a real physical-chemical
experiment as an example. The proposed polysensometric visual information fusion
model generates estimation of 3D structure of objects, based on multiple techniques
and allows their systematic comparison and fusion. In this example, the data,
collected from XPS and LSCM, might be modified by domain specific filters, as well
as geometric transformations (such as rotation, cropping, zooming, tilting and data
mapping, etc.). After being filtered, their corresponding 3D models are generated.
Next, it allows these 3D models to be visualized as a composite model. It can further
generate a fuller model by volume-filling algorithm set and measure the differences
by using 3D comparison algorithm set. This process is extensible to additional sensor
modalities. Overall the system allows domain specific as well as generic geometric
algorithms, filters, and transformation algorithms to be easily infused in the
processing pathway leading to the comparative 3D model.

Fig. 1. Polysensometric visual information fusion model. In this figure, we derive the 3D
models from sets of information, which generated by different experimental techniques, and
compare these models.

In order to explain our information framework, we only use two abstract
techniques A and B. At the same time, we use the following concepts and
corresponding notations to facilitate explaining our visualization strategy: (i) I - the
total information about the experimental object; (ii) Estimation I - estimation of I

980

J.I. Khan, X. Xu, and Y. Ma
A

B

from experimental data; (iii) I - the information generated by technique A; (iv) I the information generated by technique B; (v) F - framework for the real structure of
A
experimental object; (vi) F - framework for the structure “seen” through technique A;
B
R
(vii) F - framework for the structure “seen” through technique B; (viii) F framework for the registration between the intersection of frameworks A and B, i.e.
V
R
registration framework; (ix) F - visualization framework for F ; (x) Transformations
T[i] - any transformation i applied on information set; (xi) Error E - the difference
between two information sets;
In reality, for a complex target of observation we are never able to get the real
structure of the target, total I under framework F. But we can approximate it as close
as possible to obtain estimation I’. So our objective is to obtain estimation I and
visualize it under FV, which is close enough to F. The relations among these concepts
and notations are: the total information, I, is corresponding to the total framework, F;
A
A
B
I is information about I under its framework F ; I is information about I under its
B
A
B
framework F ; F and F are registered towards FR based on the intersection part
R
R
between them; IA is part of information IA after registration under FR; IB is part of
A
B
B
R
information I after registration under F ; we use the union of transformed I and I as
V
the estimation of I, I; I is visualized under F . The inferring algorithm is showed in
figure 2 and the specific details will be covered in the next part of this paper.

3 Algorithms
The data fusion is achieved through a set of processing: space projection, registration,
volumetric filling and comparison, etc. This section interprets those algorithms used
to obtain reasonable data fusion results.
3.1 Sensometric to Geometric Projection Algorithms
The original input datasets are collected from some sensors. These sensors probably
have different space coordinate systems and resolutions. In these cases, we need
algorithms to project the sensor domain sample datasets to a common geometric
space. After the original datasets processed by these algorithms, they should have the
same spatial orientation and resolution and be ready for the following operations. The
frequently used transformations are including zoom and tilt etc.
I * T[0] * …* T[n] -> I’

(1)

In this formula, I is input dataset, I’ is output dataset. T[i](0 <= i <= n) are n+1
transformations applied on input dataset I.
3.2 Registration Algorithms
After we project the original sensometric datasets to geometric space, we need the
automated registration algorithms to register the voxel in one spatial datasets with the
corresponding voxel in the other spatial datasets. In our research, registration

A Framework for 3D Polysensometric Comparative Visualization

981

algorithms are divided into two groups: 2D registration and 3D registration. We use
mutual information [4,5] or FFT [6] to align our datasets in registration framework, in
which the common part of two datasets is normally 2D. Also, we use 3D registration
V
V
to fuse two 3D estimation models, IA and IB , in visualization framework. Some 3D
transformations are applied on two models, and difference between them is
calculated. Difference metrics are discussed later in comparison algorithm. Here least
error method is used to decide which transformation will be used to get the final
estimation of the real model:
V

A

A

V

B

B

(2)

IA = fi [I ];

(3)

IB = fi [I ];
V

V

E’ = Diff(IA , IB );

(4)

In these formula, fi means ith transformation on dataset A, Diff means difference
metric calculation between two parameters.
3.3 Comparison Algorithms
Comparison algorithms are used for inferring the final 3D model. It checks if the loop
ending conditions have been met. There are many 2D whole-image level comparison
algorithms, such as mean-absolute-error (MAE), root-mean-square-error (RMSE),
peak signal-to-noise ratio (SNR) etc. In this example system, we used comparison
algorithms that are extension of those used in 2D. The greater MAE and RMSE, the
more different these two 3D models are; SNR is on the contrary; the closer p is to 1,
the more correlated are the two sample images.
3.4 3D Model Inferring Algorithm
We use 3D model inferring algorithm to infer the final 3D visualization model for the
object of interest (see figure 2). The algorithm is composed of two loops. We explain
the algorithm by using two techniques, A and B, as an example. First, the input
datasets from sensors generate 3D models of their own by using hypotheses Ha and Hb
respectively. Then, these two 3D models are projected onto registration framework,
R
A
B
F . In registration framework, the intersection parts of F and F are registered by
applying a series of transformations. Least errors are used to get the optimum
registration position. After registered, IA and IB are compared to verify the hypotheses
at the very beginning. Least error principle is also used as the optimum means to
obtain the best theoretic 3D model about the object of interest. At last, we use the
union of IA and IB from the hypotheses with least error as the estimation of total
information I.

982

J.I. Khan, X. Xu, and Y. Ma

Fig. 2. “Pounding-and-inferring” algorithm for 3D model based on sets of information from
two techniques A and B

4 System Architecture and Working Process
The architecture of our shell system is illustrated in figure 3. The shell is composed of
such components as task controller, flowchart object store and some other functional
parts, such as flowchart designer, data combiner, information verifier etc. Task
controller works as a central control manager. It controls the state of our program and
coordinates the work of the functional parts. Its work includes deletion of a flowchart
object from the flowchart store, let the flowchart designer insert a flowchart object
into the store according to the requirement of the user, injection of input data through
the data combiner, declaration and definition of filters and algorithms through
filter/algorithm combiner, verification of information in the flowchart object through
verifer and driving the transformation engine to process the data following the
controls in flowchart object and generating the final model. The flowchart object store
is used to store all the information about data and operations what will be applied on
the data. Information transformation engine drives the input data through the control
of the flowchart and generate 3D results. Also, hypothesis refinement can be done by
this component.

A Framework for 3D Polysensometric Comparative Visualization

983

Fig. 3. System architecture

There are total 5 phases to run the whole process: design phase, name-binding
phase, parameter-binding phase, verification phase and run phase. During design
phase, a flowchart object can be designed by using flowchart designer. There are two
kinds of flowchart object in the project: one is to take 2D experimental data as input
and generate a 3D model; another is to take 3D models, which are generated by some
flowchart object before, as input and generate the fusion 3D object or comparison
between two 3D models. During name Binding Phase, we declare filters and
algorithms. Another important thing in this phase is to bind experiment method and
image data. During parameter Binding Phase, we set parameters for each filter and
algorithm declared in the current flowchart object. During verification Phase, all
those input information need to be verified before the data are to be transformed
following the datapath defined by the flowchart object. During run Phase, a new 3D
model will be created from 2D or 3D data. This model will be displayed in a separate
window for further visualization. In this phase, user can also make parameters of
filters and algorithms change on the fly in order to get a content result. These changes
will be flush back to the flowchart aggregation component when the user pushes a
button in the 3D model presentation window.

5 Case Study
We visualize the fused information about liquid crystal polymeric film, which was
collected by using XPS and LSCM [3]. The polymeric film is 2 microns thick and
composed of two non-equally distributed polymers: PVC and PMMA. In XPS, only
top and bottom surface with 0.01 microns thick into film have contribution to top
image and bottom image of 265 by 265 pixels respectively. The dimension of the shot
area is 700 by 700 microns. We also know that the highest pixel intensity in the image
corresponds to 35 percent PVC and the lowest pixel intensity is to 5 percent PVC.
LSCM generates 32 images of 600 by 800 pixels in this example. The shot area is 300
by 400 microns. The first and last LSCM images are in the depth 0.2 microns to the

984

J.I. Khan, X. Xu, and Y. Ma

Fig. 4. Result images. Left: the stack of result images by combining XPS data and LSCM data
st
in 3D presentation. We put the top image of XPS on the top of 1 image of LSCM and the
bottom image of XPS under the last image of LSCM. Middle: the difference between theoretic
model 1 and fusion model. MAE: 119.3, SNR: 129.8. Right: the difference between theoretic
model 2 and fusion model. MAE = 87.3; SNR = 99.0

surface of the film. The highest pixel intensity in the fourth image corresponds to
70 percent PVC and the lowest pixel intensity of the same image is to 20 percent
PVC. Also, we are ensured that the shot area in XPS is partially overlapped with that
in LSCM. Now our task is to visualize the fusion information based on data from XPS
and LSCM. Also, we let user choose theoretic model to fill space in between XPS top
and bottom images and then compare them with fusion model. Thus, user can obtain a
proposed theoretic model, which is closest to experimental data.
Our system is used to process the input XPS and LSCM datasets. First, we design
two flowchart objects to process XPS and LSCM datasets respectively. These two
flowchart objects should adjust the resolution, image dimension, concentration
mapping, etc., in order to let them match for fusion. After 3D models of each sensor
domain are generated, we fused them to form a combined 3D model. And we also use
3D in-filling algorithms to fill the space between XPS images to form several
theoretic 3D models. Then comparison metrics are calculated and the best proposed
3D structure of material is decided.
The result of our example is illustrated in the attached figures. In figure 4, a fused
st
3D model of XPS and LSCM is illustrated. We put XPS top image on the top of the 1
image of LSCM image stack and XPS bottom image at the bottom of the last image of
LSCM image stack to form a composite 3D model based on these two techniques.
Also, we use linear insertion algorithm insert 30 images between top image and
bottom image of XPS datasets. Thus both datasets have the same number of images.
After using all filters, they also have the same resolution, dimension and been well
registered. So we use our comparison metrics, MAE and SNR, to compare these two
3D models. The results are: MAE = 119.3; SNR = 129.8. The comparison results are
illustrated in figure 4. We also use another different formula to fill the space in
between the top and bottom image: 2*topPixel*x+botPixel*(1-x). Comparison
between the new 3D XPS model and LSCM model is calculated: MAE = 87.3; SNR =
99.0. So according to MAE and SNR, the second in-filling function generates a result
closer to LSCM. The results are illustrated in figure 4.
From the above case study, we see our system can generate a fused and theoretic
3D model based on a set of experimental data and user-defined algorithms. The user
might propose different theoretic 3D models and compare these models with real

A Framework for 3D Polysensometric Comparative Visualization

985

experimental data. Thus the best theoretic model of chemical structure of the material
of interest could be chosen based on the comparison metrics. This the framework can
used to refine understanding about the chemical /physical process as well.

6 Conclusion
In this paper, we have proposed the Polysensometric Visual Information Fusion
(PVIF) Model for estimation of total information of the object of interest based on
partial information from multiple instruments. We have implemented an integrated
system to automate fusion of the partial information and to compare different 3D
models. Our system can be used as a tool for chemists and physicists to explore the
structure of a material. There are still many complicated issues. Among these
difficulties, 2D/3D registration algorithm itself is a challenge; how to find a good 3D
comparison metric is also interesting. The effectiveness of various morphing and
interpolation algorithms also changes when comparison is desired at 3D level. Speed
of overall computation remains an ever important issue as it affects the mental process
of perceptualization.
Acknowledgements. Dr. Julia Fulgum’s analytical and surface chemistry research
group at Kent State University, Department of Chemistry, and later at University of
New Mexico, Chemical and Nuclear Engineering Department, has provided the XPS
and Confocal datasets for the system. The work has been supported by National
Science Foundation (NSF) Grant NSF-ITR-0113724.

References
1. Schiers C, Tiede U, Hohne KH 1989, Interactive 3D-registration of image volumes from
different sources. In: Lemke HU et al. (eds), Computer Assisted Radiology (Proc. CAR ’89),
Springer, Berlin.
2. Hohne, K.H., Bomans, M, et al, 3D-Visualization of tomographic volume data using the
generalized voxel-model, CH Volume Visualization workshop.
3. K. Artyushkova, “Application of Multitechnique Correlation And Multivariate Analysis To
Heterogeneous Polymer Systems”, dissertation, Kent State University, Dec., 2001.
4. William M. Wells, Paul Viola, Hideki Atsumi and Shin Nakajima, Multi-Modal Volume
Registration by Maximization of Mutual Information, Medical Image Analysis, vol. 1, no. 1,
pp. 35--51, March 1996.
5. M. E. Leventon and E. L. Grimson, Multi-Modal Volume Registration Using Joint Intensity
Distributions, in Medical Image Computing and Computer-Assisted InterventionMICCAI'98, ed. by W.M. Wells, A. Colchester, S. Delp, Cambridge, MA, Springer LNCS
1496, pp 1057.
6. P.E. Anuta, Spatial registration of multispectral and multitemporal digital imagery using fast
fourier transform techniques. IEEE Trans. on Geoscience Electronics, GE-8(4):353--368,
October 1970.

