Proposing a New Learning Algorithm to Improve Fault
Tolerance of Neural Networks
Mahdi Jalili-Kharaajoo
Young researchers Club, Azad University, Tehran, Iran
mahdijalili@ece.ut.ac.ir

Abstract. Fault tolerant neural network architecture, based on Multilayer
Perceptron (MPL) is presented. We modify the conventional Back error
propagation (BP) algorithm to be applied to this architecture with the least
learning degradation for fault tolerant nodes. Simulation results for random s-a0 faults demonstrating the fault tolerance improvement are presented.

1 Introduction
Neural Networks have been successfully used for fault diagnosis in nonlinear systems
[1]. Using conventional fault tolerant techniques, such as Triple Modular Redundancy
(TMR) and Triple Time Redundancy (TTR) [2], yields to either a very expensive and
large system or a long time overhead. Two main approaches have been proposed to
improve fault tolerance in an artificial neural network: 1) working on learning
algorithms and 2) working on architectures [3,4].
In this paper we will first introduce fault tolerant neural network architecture,
based on Multilayer Perceptron (MLP) and a new learning algorithm based on
conventional Back error Propagation (BP) algorithm.

2

MLP and BP Algorithm

MLP consists of several cascaded layers of neurons with sigmoid activation functions
[5]. The input vector, feeds into each of the first layer neurons, the outputs of this
layer feed into each of the second layer neurons and so on, as in Fig. 1.

Fig. 1. Architecture of a typical MLP

M. Bubak et al. (Eds.): ICCS 2004, LNCS 3037, pp. 717–721, 2004.
© Springer-Verlag Berlin Heidelberg 2004

718

M. Jalili-Kharaajoo

Most often the nodes are fully connected, i.e., every node in layer l is connected to
every node in layer l+1. In this paper, we assume input vector as the first layer in the
neural network. Usually output neurons use linear activation functions rather than
nonlinear sigmoid, since this tends to make learning easier. We assume that the
number of hidden layers is one and activation function of each neuron in hidden layer
is a bipolar sigmoid by the following equation
1 − exp(ui ) ;
(1)
f (ui ) =
ui =
wij × xi − θ i

∑

1 + exp(ui )

j

where wij is the connection weight between neuron j in the preceding layer and neuron
i. BP algorithm changes wij in order to reduce the error of output layer defined by
1
(2)
E=
(t − o ) 2
2

∑

i

i

i

where tj is output target and oj is the estimated output [7]. Using the steepest-descent
gradient rule, the change of wij is expressed as
∂E
(3)
∆wij = η
∂wij

Selecting a suitable value plays an important role in network learning convergence.
According to BP algorithm
P
P
P
P
∆wij = ηδ iP o Pj , δ i = (t i − oi ). f ′(u i ) , δ P = ( w .δ P ). f ′(u ) ; f ′( x) = 2 f ( x ).(1 − f ( x )) (4)
i

∑

ki

k

i

k

3 Fault Model
There are usually three kinds of faults, considered in a neural network: 1) connection,
2) weights and 3) neuron body itself. The first two faults are often modeled as stuckat-0 and most often occur during a memory disappearance or a link disconnection in
VLSI. On the other hand faults due to a neuron cell, usually subject its output to one
of the positive or negative saturation voltages. This kind of faults is modeled as stuckat-(1) or stuck-at-(-1). In this paper we consider only stuck-at-0 (s-a-0) faults.

4 FTNN Architecture
It is evident that a simple system has naturally more fault tolerance capabilities than a
large and complex one. Table 1. summarizes the results of injecting 1000 random s-a0 fault patterns to links of a sample MLP, consisting of 2 inputs, 15 neurons in hidden
layer and one output node. It is clear that the links in hidden layer have tolerated
faults better than links in output layer, which are sensitive and can cause remarkable
errors. So, replacing the conventional output nodes with nonfaulty nodes will improve
significantly the total fault behavior in the MLP.
Table 1. Average absolute errors due to 1000 random stuck-at-0 faults in a 2-15-1 MLP.
Condition
Average Error

No faults
0.006

Fault in hidden layer
0.026

Fault in output layer
0.058

Proposing a New Learning Algorithm to Improve Fault Tolerance

719

To have such a robust node we first use suggested linear activation function, which is
a simple adder indeed, rather than the nonlinear sigmoid function. Then, we use a
wired connection as links and eliminate memory usage by the elimination of
connection weights. Fig. 2 shows our suggestion to obtain such nodes in two steps,
while preserving the learning ability of neural network in output nodes, as much as
possible. We call the resultant node as FTN (Fault Tolerant Neuron).

Fig. 2(a). A conventional nonlinear node

Fig. 2(b). A linear node with similar
weights

Fig. 2(c). Fault Tolerant node with wired
links

Fig. 2(d). An FTNN architecture using
FTN nodes in output layer (shadowed
nodes).

5 UWLA Learning Algorithm
According to the architecture described in previous section, a learning algorithm is
introduced for output layer, which we call it as UWLA (Uniform Weight Learning
Algorithm). To make this term as close as possible to zero, we add another criterion,
variance of wij, to the learning process, which is defined as
(5)
σj =
( wij − m j ) 2 , m = 1 w

∑
i

j

n

∑

ij

i

where j and i denote the neurons in output and hidden layer, respectively. mj is the
mean value of wij for output neuron j that is calculated after each training step. Our
aim is to diminish sj as much as possible. The following algorithm is suggested:
UWLA Algorithm:
Step 0: Initialize weights with small values,
Step 1: While stopping condition is false do steps 2-9,
Step 2: For each input vector do Steps 3-8,
Step 3: Each input unit receives input signal and broadcast it to hidden layer units,
Step 4: Each hidden unit sums its weighted input signal and applies its activation
function according to equations (1),
Step 5: Each output unit sums over its all input signals and produces its output, too.
Step 6:
Step 6-1: For each output unit compute its error information term, Calculate mean[
wij (old) ],
using equation (4).
Calculate: wij( new)= wij( old)+• wij. ••••••using

720

M. Jalili-Kharaajoo

If abs[wij(new)-mean [wij(old)]] is greater than abs[wij(old) -mean [wij(old)]], then
choose = 2 ,which 2 < 1.
Step 7: for each hidden unit compute its error information term, using equation (4),
Step 8: For each output and hidden unit update weights according to: wij( new)= wij(
old)+• wij. ,
Step 9: Test stopping condition.

6 FTNN Behavior
To evaluate the UWLA algorithm, an experiment is managed in which an MLP
network with two inputs, fifteen hidden nodes and one output node is to approximate
the nonlinear function of z =0.5 ×Sin(x+y)+0.3
For comparison, we consider standard BP and MFTA (Multiple Fault Training
Algorithm) algorithm introduced by [4], for single s-at-0 faults. In all simulations the
training process lasts after 20000 iterations, with a decreasing learning rate of = 0.2
to = 0.95. Tabs. 2 and 3 summarize the results obtained after 1000 injection of
random s-a-0 faults in hidden layer and output layer weights. It is clear that FTNN
trained through UWLA has slightly improved fault tolerance in the hidden layer (first
layer) as compared to MFTA. Figs. 3 and 4 show the results for 1000 random s-a-0
fault injections for the trained networks. Clearly, Fault tolerance performance of
FTNN is twice the MFTA and approximately four times the standard BP algorithm.
Table 2. Average absolute errors due to 1000 random s-at-0
network.
Fault numbers
Standard BP
MFTA
FTNN

No faults
0.006
0.009
0.007

One fault
0.026
0.019
0.015

faults in hidden layer of a 2-15-1
Two faults
0.044
0.028
0.026

Table 3. Average absolute errors due to 1000 random s-at-0 faults in output layer of a 2-15-1
network.
Fault numbers
Standard BP
MFTA
FTNN

No faults
0.006
0.009
0.008

Fig. 3. Error percentage versus threshold
value after 1000 random s-at-0 faults in
hidden layer.

One fault
0.058
0.021
0.022

Two faults
0.090
0.038
0.039

Fig. 4. Error percentage versus threshold value
after 1000 random s-at-0 faults in output layer.

Proposing a New Learning Algorithm to Improve Fault Tolerance

721

7 Conclusion
In this paper, we developed FTNN architecture by introducing FTN nodes and
implementing them in output layer, which are the critical nodes in MLP networks.
The proposed fault tolerant node is subjected to have the least memory usage. Then,
we introduced UWLA, to extend BP. Simulations demonstrate that the resultant
network shows a superior performance over the standard BP and commonly used fault
injection training algorithms such as MFTA.

References
1. Khunasaraphan C., K.Vanapipat and C. Lursinsap, Weight Shifting Techniques for SelfRecovery Neural Networks, IEEE Tran. Neural Networks Vol 5, No. 4, July 1994.
2. Johnson B.W., Design and Analysis of Fault Tolerant Digital Systems, Addison Wesley,
1989.
3. Murrayand A.F., Peter J. Edwards, Synaptic Weight Noise during Multilayer Perceptron
Training: Fault Tolerance and Training Improvement, IEEE Tran. Neural Networks, Vol 4,
No.4, July 1993.
4. Tai-Chiu C., K. Mehrotra, Chiukuri K. Mohan and S. Ranka, Training Techniques to
Obtain Fault Tolerant Neural Networks, Proc. 24th international symposium on fault
tolerant computing, 1994.
5. Fausell L., Fundamental of Neural Networks, Prentice Hall, 1994.
6. Hush and Horne, Progress in Supervised Neural Networks, IEEE Signal Proc. Mag., Jan
1993.
7. Beale R. and T. Jackson, Neural Computing: An introduction, York University, IOP Pub.,
1990.

