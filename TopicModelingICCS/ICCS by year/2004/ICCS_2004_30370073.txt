On a Family of A-stable Collocation Methods
with High Derivatives
Gennady Y. Kulikov1 , Arkadi I. Merkulov2 , and Ekaterina Y. Khrustaleva2
1

2

School of Computational and Applied Mathematics, University of the
Witwatersrand, Private Bag 3, Wits 2050, Johannesburg, South Africa
gkulikov@cam.wits.ac.za
Ulyanovsk State University, L. Tolstoy Str. 42, 432970 Ulyanovsk, Russia
merkul@vda.ru, shabalkina@mail.ru

Abstract. In this paper we develop a family of A-stable one-step methods with high derivatives by means of a collocation technique. We present
construction details and a theory to justify such sort of methods. We also
concentrate on an eﬀective way of their practical implementation.

1

Introduction

In this paper we study one-step numerical methods for ordinary diﬀerential equations (ODEs) of the form
x (t) = g t, x(t) ,

t ∈ [t0 , t0 + T ],

x(t0 ) = x0

(1)

where x(t) ∈ Rn and g : D ⊂ Rn+1 → Rn is a suﬃciently smooth function.
Problem (1) is quite usual in theoretical research and in practical engineering
(see, for example, [2], [4], [6], [7], [12]). Therefore eﬃcient methods for ﬁnding
its solution with any set accuracy (up to round-oﬀ errors) are important as in
theory as in practice.
Here, we present a family of one-step collocation methods with high derivatives. Note that such topic of research proved its eﬃciency. So, there are many
papers in this ﬁeld (see [5], [7], [8], [11] and so on). Especially, we pay attention
to the general linear methods developed by Butcher [4] which also include high
derivatives. In general, all that methods are implicit. Therefore the most diﬃcult
task is their correct and eﬀective implementation because we have to involve an
additional iterative scheme.
The outline of this paper is organized as follows: We present the family
of one-step collocation methods with high derivatives in Sect. 2. There, we also
study convergence and A-stability of these methods. An eﬀective implementation
based on Newton-like iterations is given in Sect. 3. The last section of the paper
is devoted to the problem of step size selection. Theoretical results are clearly
conﬁrmed by numerical experiments.

2

Collocation Methods with High Derivatives

As in [1], we use the collocation technique to construct numerical methods with
high derivatives. First, we ﬁx the points tk , tk+1/2 and tk+1 on an arbitrary
M. Bubak et al. (Eds.): ICCS 2004, LNCS 3037, pp. 73–80, 2004.
c Springer-Verlag Berlin Heidelberg 2004

74

G.Y. Kulikov, A.I. Merkulov, and E.Y. Khrustaleva

interval [tk , tk+1 ] as collocation ones. Second, we assume that we know derivatives
of the right-hand part of problem (1) at the nodes tk and tk+1 up to order p.
Now we can construct the Hermit interpolation formula (see, for example, [3])
and substitute it into the right-hand part of system (1) instead of g t, x(t) .
Integrating the newly obtained system on the interval [tk , tk+1 ] we come to the
numerical method
p

τkr a1 gk + a3 gk+1 + τk a2 gk+1/2 ,

(2a)

τkr b1 gk + b3 gk+1 + τk b2 gk+1/2 ,

(2b)

(r) (r)

xk+1/2 = xk + τk

(r) (r)

(0)

r=0
p
(r) (r)

xk+1 = xk + τk

(r) (r)

(0)

r=0

k = 0, 1, . . . , K − 1, where
p−r
(r)
aj

=
i=0
p−r

(r)
bj

=
i=0

1 (θ − (j − 1)/2)p+1
i!r!
Ψ (θ)
1 (θ − (j − 1)/2)p+1
Ψ (θ)
i!r!

1/2

(i)
θ=(j−1)/2

Ψ (θ)
dθ,
(θ − (j − 1)/2)p−i−r+1

0
1

(i)
θ=(j−1)/2

0

Ψ (θ)
dθ
(θ − (j − 1)/2)p−i−r+1

when j = 1, 3 and r = 0, 1, . . . , p,
(−1)p+1
a2 =
4p+1

1/2

θp+1 (θ − 1)p+1 dθ,
0

b2 =

1

(−1)p+1
4p+1

θp+1 (θ − 1)p+1 dθ,
0

x0 = x0 , θ = (t − tk )/τk , Ψ (θ) = θp+1 (θ − 1/2)(θ − 1)p+1 , gk denotes the r-th
derivative1 of the right-hand part of problem (1) with respect to t evaluated at
the point tk , and τk is a step size which may be ﬁxed or variable.
The results presented above give a way to derive
(r)

Theorem 1 Let the right-hand part of ODE (1) possess continuous derivatives
up to order 2p+5 in a neighborhood of the solution x(t) on the interval [t0 , t0 +T ],
where p is a nonnegative integer number. Then method (2) is convergent, it has
stage order 2p + 3 and classical order 2p + 4, and its coeﬃcients satisfy
(r)

a1 =

p+1
r!2p+r+2

p−r i+r p+1

i=0 l=0 j=0 q=0

a2 =
1

i

(−1)l (i + r)!(p + q)!
, (3a)
l!(i + r − l)!j!(p + 1 − j)!(l + j + 2)q!2q

(p + 1)!
2

p+1
l=0

(−1)l
,
l!(p + 1 − l)!(2l + 1)

Here and further the zero-derivative implies the original function.

(3b)

On a Family of A-stable Collocation Methods with High Derivatives

(r)

a3 =

(−1)r+1 (p + 1)
r!2p+r+2
i

×
q=0

(p + q)!
,
q!2q

p−r i+r p+1
i=0 l=0 j=0

(3c)

r = 0, 1, . . . , p,

b1 = a1 + (−1)r a3 ,
(r)

(−1)j (i + r)!
l!(i + r − l)!j!(p + 1 − j)!(l + j + 2)

75

(r)

(r)

b2 = 2a2 ,

b3 = (−1)r a1 + a3 .
(r)

(r)

(r)

(3d)

Theorem 2 One-step method (2) is A-stable for any integer p ≥ 0.
Further we call methods of the form (2) as E-methods with high derivatives, for short. As an example of E-methods, we present the following numerical
method of order 8 obtained by Theorem 1 when p = 2:
689
169 2 (1)
17 3 (2)
81
(0)
(0)
τk gk +
τ g +
τ g −
τk gk+1
2240
4480 k k
8960 k k
2240
19 3 (2)
8
41 2 (1)
(0)
τk gk+1 −
τk gk+1 + τk gk+1/2 ,
+
4480
26880
35

xk+1/2 = xk +

1
57
(0)
(0)
(1)
(1)
τk gk + gk+1 + τk2 gk − gk+1
210
35
16
1 3 (2)
(2)
(0)
τ g + gk+1 + τk gk+1/2 .
+
840 k k
35

(4a)

xk+1 = xk +

3

(4b)

Implementation of E-methods with High Derivatives

When implementing method (2) in practice, we have to involve some additional
iterative scheme to treat the corresponding nonlinear algebraic system. We applied the simple (ﬁxed-point) iteration and the modiﬁed (or full) Newton one in
[10] to obtain the following combined algorithms:
E-method with the simple iteration (SI-method):
i
¯ τ X i−1 ,
Xk+1
=G
k k+1
0
Xk+1
= Π(tk+1/2 )T , Π(tk+1 )T

T

i = 1, 2, . . . , N,
∈ R2n ,

(5a)

k = 0, 1, . . . , K − 1,

(5b)

T
def
¯ τ : R2n → R2n
where Xk+1 = (xk+1/2 )T , (xk+1 )T
∈ R2n , the mapping G
k
presents the right-hand part of system (4), x
¯k is an approximate solution of
def

(r) def

¯k = g (r) (tk , x
¯k ), and
problem (2) obtained after N iterations; i.e., x
¯k = xN
k , g
Π(t) implies a predictor which is an interpolation formula based on the values
of numerical solution x
¯k (Π(t) ≡ x
¯k in the trivial case).
E-method with the Newton iteration (N-method):
i−1
i−1 −1 ¯ τ i−1
i
Xk+1
= Xk+1
− ∂ F¯kτ (Xk+1
) Fk Xk+1 ,
0
Xk+1
= Π(tk+1/2 )T , Π(tk+1 )T

T

∈ R2n ,

i = 1, 2, . . . , N,

(6a)

k = 0, 1, . . . , K − 1,

(6b)

76

G.Y. Kulikov, A.I. Merkulov, and E.Y. Khrustaleva

¯ τ (E2n is the identity operator in R2n ) and ∂ F¯ τ (X i−1 ) is
where F¯kτ = E2n − G
k
k
k+1
i−1
.
the Jacobian of the mapping F¯kτ evaluated at the point Xk+1
E-method with the modiﬁed Newton iteration (MN-method):
def

i−1
i−1
i
0
Xk+1
,
= Xk+1
− ∂ F¯kτ (Xk+1
)−1 F¯kτ Xk+1
0
Xk+1
= Π(tk+1/2 )T , Π(tk+1 )T

T

∈ R2n ,

i = 1, 2, . . . , N,

(7a)

k = 0, 1, . . . , K − 1.

(7b)

Let us test our combined algorithms with the underlying method (4) on the
restricted three body problem
x1 (t) = x1 (t) + 2x2 (t) − µ1

x1 (t) + µ2
x1 (t) − µ1
− µ2
,
y1 (t)
y2 (t)

(8a)

x2 (t)
x2 (t)
− µ2
,
y1 (t)
y2 (t)

(8b)

x2 (t) = x2 (t) − 2x1 (t) − µ1
y1 (t) = (x1 (t) + µ2 )2 + x2 (t)2

3/2

,

y2 (t) = (x1 (t) − µ1 )2 + x2 (t)2

3/2

, (8c)

where t ∈ [0, T ], T = 17.065216560157962558891, µ1 = 1 − µ2 and µ2 =
0.012277471. The initial values of problem (8) are: x1 (0) = 0.994, x1 (0) = 0,
x2 (0) = 0, x2 (0) = −2.00158510637908252240. The solution path of this problem is a periodic one with the period T (see, for example, [6]).
Table 1. Global errors of the EN-algorithm with trivial predictor for E-method (4)
N
1
2
3
4

1.000 · 10+04

2.000 · 10+04

K
4.000 · 10+04

8.000 · 10+04

1.600 · 10+05

2.068 · 10+00
2.193 · 10−02
2.582 · 10−04
2.582 · 10−04

1.571 · 10+00
6.138 · 10−05
2.059 · 10−07
2.059 · 10−07

2.066 · 10+00
1.587 · 10−07
6.728 · 10−10
6.727 · 10−10

2.000 · 10+00
4.408 · 10−10
4.039 · 10−13
3.974 · 10−13

2.612 · 10+00
1.043 · 10−12
7.865 · 10−14
7.865 · 10−14

Table 2. Global errors of the EMN-algorithm with trivial predictor for E-method (4)
N
1
2
3
4
5

1.000 · 10+04

2.000 · 10+04

K
4.000 · 10+04

8.000 · 10+04

1.600 · 10+05

2.068 · 10+00
1.841 · 10+00
3.716 · 10−03
4.660 · 10−04
2.589 · 10−04

1.571 · 10+00
2.938 · 10−01
2.582 · 10−05
3.322 · 10−07
2.060 · 10−07

2.066 · 10+00
8.422 · 10−03
1.192 · 10−07
1.264 · 10−10
6.726 · 10−11

2.000 · 10+00
2.615 · 10−04
4.959 · 10−10
3.025 · 10−13
4.651 · 10−13

2.612 · 10+00
8.146 · 10−06
2.401 · 10−12
5.085 · 10−14
4.150 · 10−14

On a Family of A-stable Collocation Methods with High Derivatives

77

Table 3. Global errors of the ESI-algorithm with trivial predictor for E-method (4)
N
3
4
5
6
7
8
9
10

1.000 · 10+04

2.000 · 10+04

K
4.000 · 10+04

8.000 · 10+04

1.600 · 10+05

5.689 · 10+00
9.384 · 10−01
1.971 · 10−01
8.074 · 10−03
1.900 · 10−03
1.225 · 10−03
3.920 · 10−04
2.817 · 10−04

1.254 · 10+00
1.360 · 10−01
6.712 · 10−03
1.655 · 10−04
1.944 · 10−05
4.420 · 10−06
5.595 · 10−07
2.309 · 10−07

2.098 · 10−01
8.004 · 10−03
2.045 · 10−04
2.605 · 10−06
1.742 · 10−07
1.622 · 10−08
8.233 · 10−10
8.464 · 10−11

2.661 · 10−02
4.604 · 10−04
6.314 · 10−06
3.924 · 10−08
1.427 · 10−09
6.113 · 10−11
1.136 · 10−12
3.899 · 10−13

3.416 · 10−03
2.738 · 10−05
1.964 · 10−07
5.952 · 10−10
1.155 · 10−11
5.988 · 10−13
1.060 · 10−13
1.206 · 10−13

Tables 1–3 exhibit that all the methods under testing are convergent for problem (8) and their orders depend on the number of iterations per grid point. We
also see that the full (or modiﬁed) Newton iteration demonstrates the maximum
order convergence with fewer iteration steps than the simple one. Unfortunately,
it may be too expensive to compute the exact Jacobian of method (2) because
of the high derivatives. Now we show how to simplify it.
First of all we replace the exact Jacobian ∂ F¯kτ in method (6) with the following matrix:
def

Ak =

(0)

(0)

1 − τk a2 ∂gk+1/2 −τk a3 ∂gk+1
(0)
(0)
−τk b2 ∂gk+1/2 1 − τk b3 ∂gk+1

,

and obtain the E-method with the simpliﬁed Newton iteration (ESN-method). If
we test the new method on our problem (8) we come to Table 4. It is obvious
that the ESN-method is cheaper than the previous Newton-type methods, but
we see that it acts like the EMN-method. The corresponding theoretical result
is given by
Table 4. Global errors of the ESN-algorithm with trivial predictor for E-method (4)
N
1
2
3
4
5

1.000 · 10+04

2.000 · 10+04

K
4.000 · 10+04

8.000 · 10+04

1.600 · 10+05

2.102 · 10+00
2.976 · 10+00
2.347 · 10−02
2.066 · 10−04
2.571 · 10−04

1.862 · 10+00
2.467 · 10−01
3.725 · 10−04
1.274 · 10−07
2.059 · 10−07

2.056 · 10+00
1.024 · 10−02
5.377 · 10−06
9.618 · 10−11
6.739 · 10−11

1.989 · 10+00
3.774 · 10−04
8.578 · 10−08
1.803 · 10−12
4.583 · 10−13

1.839 · 10+00
1.304 · 10−05
1.393 · 10−09
1.140 · 10−13
4.428 · 10−14

Theorem 3 Let the right-hand part of ODE (1) possess continuous derivatives
up to order 2p+5 in a neighborhood of the solution x(t) on the interval [t0 , t0 +T ],

78

G.Y. Kulikov, A.I. Merkulov, and E.Y. Khrustaleva

where p is a nonnegative integer number. Then the ESN-algorithm based on
method (2) is convergent, and its error satisﬁes
x(tk ) − x
¯k (N ) ≤ Cµ τ µ ,

k = 1, 2, . . . , K,

(9)

where µ = min{(ξ + 1)(N + 1) − 2, 2p + 4}, ξ = min{ζ + 1, 2p + 3}, ζ is the
order of predictor Π(t) which is used to compute an initial approximation in the
def

simpliﬁed Newton iteration, τ = max{τk } is a diameter of the grid, and Cµ is
a constant.

4

Step Size Control for E-methods with High Derivatives

Now we consider the problem of step size selection for E-methods with high
derivatives. Let us assume that the exact solution of problem (1) is known at
the point tk ; i. e., xk = x(tk ), and the local error tolerance l is given. The
notation x
ˆk+1 denotes the numerical solution calculated by two steps of method
(2) with the size τk /2. Then the algorithm to control the local error of E-methods
is presented as follows:
Algorithm 1: Local step size control
Step 1. Compute x
˜k+1 , x
ˆk+1 .
ˆk+1 − x
˜k+1 /(1 − 1/22p+4 ).
Step 2. Calculate ∆˜
xk+1 = x
1/(2p+5)

Step 3. τ˜k := θ

l/

∆˜
xk+1

τk .

Step 4. If ∆˜
xk+1 > l then τk := τ˜k and go to Step 1,
xk+1 and τk+1 := τ˜k .
else x
˜k+1 := x
˜k+1 + ∆˜
Step 5. tk := tk + τk , k := k + 1 and go to Step 1.
Here, θ ∈ (0, 1) is a safety factor.
If we want to apply the local error control mechanism to iterative methods
(5)–(7) we have to provide suﬃcient iterations at each grid point as the following
estimates indicate:
N ≥ log2
N≥

2p + 7
ξ+1

2p + 7
−1
ξ+1

N ≥ 2p + 6 − ξ

for the EN-method,

for the EMN- and ESN-methods,
for the ESI-method.

(10)

(11)
(12)

Let us test our methods with the local step size control on problem (8). If we
use the combined algorithms mentioned above to solve this problem we come to
the data presented in Table 5. The number of iterations per grid point for each
type of the iterative E-methods was calculated by formulas (10)–(12), respectively. These numbers are given in parentheses at the ﬁrst column of Table 5.

On a Family of A-stable Collocation Methods with High Derivatives

79

Table 5. Global errors of variable step size iterative algorithms based on E-method
(4) with Algorithm 1 to control step size and with extrapolation
Type of
iteration
SI(9)
N(3)
MN(5)
SN(5)

l

= 10−03

3.67 · 10−03
1.46 · 10−03
1.51 · 10−03
2.92 · 10−03

l

= 10−04

5.25 · 10−04
1.94 · 10−04
1.74 · 10−04
6.67 · 10−04

required accuracy
−05
l = 10
3.85 · 10−06
2.53 · 10−05
2.74 · 10−05
2.36 · 10−05

l

= 10−06

1.90 · 10−06
9.16 · 10−06
2.08 · 10−06
2.55 · 10−06

l

= 10−07

1.44 · 10−07
3.73 · 10−07
3.00 · 10−07
1.34 · 10−07

Table 6. Global errors of variable step size iterative algorithms based on E-method
(4) with Algorithm 2 to control step size and with extrapolation ( l = g )
Type of
iteration
SI(9)
N(3)
MN(5)
SN(5)

g

= 10−03

1.54 · 10−04
2.89 · 10−04
2.16 · 10−04
1.63 · 10−04

g

= 10−04

1.95 · 10−05
3.60 · 10−05
2.85 · 10−05
3.38 · 10−05

required accuracy
−05
g = 10
1.71 · 10−06
3.15 · 10−06
3.28 · 10−06
2.80 · 10−06

g

= 10−06

2.00 · 10−07
4.44 · 10−07
4.79 · 10−07
5.04 · 10−07

g

= 10−07

1.96 · 10−08
4.27 · 10−08
4.02 · 10−08
4.09 · 10−08

We see that the local step size selection is quite eﬃcient for all the types
of iterations, but, unfortunately, it does not allow to ﬁnd automatically the
numerical solution with the accuracy set by the user. To improve it, we have to
involve a global error control in the step size selection procedure.
By this reason, we apply the local-global step size selection developed with
the aim of controlling the global error of Runge-Kutta formulas in [9] to our
E-methods with high derivatives. First, we suppose that the numerical solution
x
˜k has been computed at the point tk with an accuracy of O(τk2p+5 ). Second, the
local and global errors tolerances l and g have been given. Third, we assume
that some step size τk has been ﬁxed. Then the algorithm of the local-global
step size selection for method (2) is presented as follows:
Algorithm 2: Local-global step size control
Step 1. By Algorithm 1, ﬁnd estimates ∆˜
xk+1 ∆˜
xk+2 of the local error of method
(2) at the points tk+1 , tk+2 and step sizes τ˜k , τ˜k+1 for the tolerance l (change
the step size τk , if necessary).
Step 2. Determine the coeﬃcient of the principal term of the local error
ψˆ2p+5 (tk+1 ).
Step 3. Compute the coeﬃcient of the principal term of the global error
ψ2p+4 (tk+1 ) by the formula
˜k+1 )
ψ2p+4 (tk+1 ) := En − τk ∂x g(tk+1 , x

−1

τk ψˆ2p+5 (tk+1 ) .

(14)

Step 4. Find an estimate ∆ψ2p+4 (tk+1 ) of the local error of method (14) by the
Richardson extrapolation or by two one-step methods of diﬀerent orders (see
[9], for details).

80

G.Y. Kulikov, A.I. Merkulov, and E.Y. Khrustaleva

Step 5. If ∆ψ2p+4 (tk+1 ) > g /100 then calculate a new step size τk as shown
in Algorithm 1 and go to Step 1,
1/(2p+4)

else q := τk

ψ2p+4 (tk+1 ) /

g

+ 1.

Step 6. If q > 1 then τk := τk /q and go to Step 1,
else x
˜k+1 := x
˜k+1 + ∆˜
xk+1 , ∆˜
xk+1 := ∆˜
xk+2 and τk+2 := τ˜k+1 .
Step 7. tk := tk + τk , k := k + 1 and go to Step 1.
Here, the square brackets mean an integer part of the number. A fuller description of Algorithm 2 for the iterative E-methods with high derivatives will appear
in [10].
If we now test our iterative E-methods with Algorithm 2 to select a step size
on problem (8) we come to Table 6. These data show the great advantage of
the local-global step size control (Algorithm 2) over the local one (Algorithm 1),
if we compare the global errors obtained in the last numerical experiment with
Table 5 . Thus, E-methods presented in the paper together with the local-global
step size control can be a good computational technique to solve many practical
problems including stiﬀ ones.

References
1. Aul’chenko, S. M., Latypov, A. F., Nikulichev, Yu. V.: A method for the numerical
integration of systems of ordinary diﬀerential equations using Hermite interpolation
polynomials. (in Russian) Zh. Vychisl. Mat. Mat. Fiz. 38 (1998) No. 10, 1665–1670;
translation in Comput. Math. Math. Phys. 38 (1998) No. 10, 1595–1601
2. Bakhvalov, N.S.: Numerical methods. (in Russian) Nauka, Moscow, 1975
3. Berezin, I.S., Zhidkov, N.P.: Methods of computations. V. 1. (in Russian) Gos.
izd-vo ﬁz.-mat. lit-ry, Moscow, 1962
4. Butcher, J.C.: Numerical methods for ordinary diﬀerential equations. John Wiley
& Son, Chichester, 2003
5. Fehlberg, E.: New high-order Runge-Kutta formulas with step size control for systems of ﬁrst and second order diﬀerential equations. ZAMM. 44 (1964) T17–T19
6. Hairer, E., Nørsett, S.P., Wanner, G.: Solving ordinary diﬀerential equations I:
Nonstiﬀ problems. Springer-Verlag, Berlin, 1987
7. Hairer, E., Wanner, G.: Solving ordinary diﬀerential equations II: Stiﬀ and
diﬀerential-algebraic problems. Springer-Verlag, Berlin, 1996
8. Kastlunger, K.H., Wanner, G.: Runge-Kutta processes with multiple nodes. Computing. 9 (1972) 9–24
9. Kulikov, G.Yu.: A local-global version of a stepsize control for Runge-Kutta methods. Korean J. Comput. Appl. Math. 7 (2000) No. 2, 289–318
10. Kulikov, G.Yu., Merkulov, A.I.: On one-step collocation methods with high derivatives for solving ordinary diﬀerential equations. (in Russian) Zh. Vychisl. Mat. Mat.
Fiz. (to appear); translation in Comput. Math. Math. Phys. (to appear)
11. Nørsett, S.P.: One-step methods of Hermite type for numerical integration of stiﬀ
systems. BIT. 14 (1974) 63–77
12. Ortega, J.M., Poole, W.G.: An introduction to numerical methods for diﬀerential
equations. Pitman Publishing Inc., 1981

