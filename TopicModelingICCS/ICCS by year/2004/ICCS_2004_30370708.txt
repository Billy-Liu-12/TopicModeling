Identification and Control Using Direction
Basis Function Neural Network
Mahdi Jalili-Kharaajoo
Young Researchers Club, Azad University, Tehran, Iran
mahdijalili@ece.ut.ac.ir

Abstract. In this paper, adaptive identification and control of nonlinear
dynamical systems are investigated using Two Synaptic Weight Neural
Networks (TSWNN). The identification algorithm has the properties of rapid
convergence and persistent adaptability that make it suitable for real-time
control. A nonlinear example is simulated to demonstrate the effectiveness of
the identification and control algorithms.

1

Introduction

Similar to the Multilayer Feedforward Neural Networks (MFNN), Two Synaptic
Weight Neural Networks (TSWNN) possesses the capacity of universally
approximating nonlinear multi-variable functions. An n-means clustering technique
was regarded as a better method for updating the kernels by Feng Cao [1]. For on-line
and adaptive applications of neural network model, some kinds of recursive
identification algorithms are naturally required [2-5].
In this paper, the kernels and weights of the TSWNN are also updated
simultaneously. A novel approach, however, is presented to train the TSWNN.
Specifically, an Adaptive Fuzzy Generalized Learning Vector Quantization
(AFGLVQ) technique is adopted to update the kernels, its weights and recursive least
squares algorithm with variable forgetting factor (VRLS) is used to estimate the
weights.

2

Two Synaptic Weight Neural Networks for Modeling Nonlinear System

Many single-input single-output non-linear systems can be described as
y (t ) = f s ( y (t − 1),..., y (t − n y ), u (t − 1),..., u (t − nu ))

(1)
where y(t) and u(t) are the system output and input, respectively; n y and nu the lags of
the output and input respectively; and f s (.) some non-linear function. The TSWNN is
a two-layer processing structure. In the first layer, the neurons are represented by
TSWNN with the kernels c i , which are interconnected by weights wij , w′ji . Factor s, p
is chosen. The second layer is essentially a linear combiner.
The overall response of such a network is a mapping f r : R m → R , that is
M. Bubak et al. (Eds.): ICCS 2004, LNCS 3037, pp. 708–712, 2004.
© Springer-Verlag Berlin Heidelberg 2004

Identification and Control Using Direction Basis Function Neural Network

709

n

f r ( x) = ∑θ iφ ( x, ci , wi )

(2)

i =1

where x ∈ R , m is network input vector, c i ∈ R ,1 ≤ i ≤ n are the kernels and
φ (.) : R → R . θ i is the connection weights; and n is the number of neurons. When the
TSWNN is used to approximate the dynamical system (1), define m= ny + nu and let
m

M

x(t ) = [ y(t − 1),..., y(t − n y ); u(t − 1),..., u(t − nu )]

Then the TSWNN output

(3)

yˆ (t ) = f r ( x(t ))

(4)

acts as the estimator of the y(t). Let

e(t ) = yˆ (t ) − y (t )

(5)
Hence, the goal of training the TSWNN is to make e(t) as small as possible. In this
paper, the function is chosen as function
s
 

 m  wj 
φ ( x, c i , wi ) = cos  ∑ 
w j (X j − c j )


 j =1 w j 

3

p



−θ 


(6)

Hybrid AFGLVQ and VRLS Algorithm

3.1 Adaptive Fuzzy Generalized Learning Vector Quantization (FGLVQ)
3.1.1
FGLVQ
In the GLVQ algorithm, for the winner node i(i=arg min| x(t)-cj(t)|, 1< i< n), the
updating rules of the kernels and its weights are
 D2 − D + x(t ) − ci (t) 
ci (t + 1) = ci (t) + b(t)
(x(t) − ci (t )), wi (t + 1) = wi (t) + ηo x(t ) − ci (t) (x(t) − ci (t ))
D2



(7)

For the rest nodes j (j=1,…, n, j ≠ i ), the rules are
 D2 − D + x(t) − c j (t) 
 x(t) − c j (t) , wj (t +1) = wj (t) +ηo x(t) − c j (t) x(t) − c j (t)
c j (t +1) = c j (t) + b(t)


D2



(

)

(

)

(8)

n

where D = ∑ x(t ) − c i (t ) , η o , η1 1 is proportion factor; b(t) is the learning rate which
i =1

is chosen satisfy the two conditions: As t → ∞ ; b(t ) → 0 and b(t ) → ∞ . b(t) can be
taken as
b(t ) = b(t − 1) / 1 + t / n

(9)
The algorithm can give more reasonable solution when D>1. But it has been found
that GLVQ behaves exactly opposite to what was desired when D<1. To overcome
this problem, we can give a fuzzy modification of GLVQ. Let Lx be a loss function
which measures the locally weighted mismatch (error) with respect to the winner
n

L x = L( x, c1 , c 2 ,..., c n ) = ∑ µ j x − c j
j =1

2

(10)

710

M. Jalili-Kharaajoo

where µ j is chosen to be a fuzzy member function as
 n x−c

j
µj = 
 p =1 x − c p


∑



2 



2

−1

(11)

Assume x − c p > 0 , the gradient of L x with respect to c p can be calculated as
∇ c p L = −2nµ 2p ( x − c p )

(12)

Thus, the updating equation of the kernels can be rewritten as

(

c j (t + 1) = c j (t ) + 2b(t )nµ 2j x(t ) − c j (t )

)

(13)

3.1.2
Adaptive Modification of FGLVQ
In adaptive identification and control, since a plant is unknown and the structure of a
given TSWNN is limited, therefore the modeling error exists inevitably. Here we give
an updating equation of b(t)
b(t ) = bo ε (t ) / (1 + ε (t )

)

where 0 < bo < 1 , thus 0 < b(t ) < 1 . b(t) varies with
converges to zero, b(t) will tend to zero too.

ε (t ) . Of course, when

(14)
ε (t )

3.2 Recursive Least Squares with Variable Forgetting Factor
Define the hidden layer output vector at the instant t as
Φ t = [φ1 (t ), φ 2 (t ),..., φ n (t )]T

(15)

The connection weight vector at t as

Θ t = [θ 1 (t ), θ 2 (t ),..., θ n (t )]T

(16)

Thus, Θ t can be recursively updated by the equations
Θ t +1 = Θ t + K t ε (t ) K =
, t

Pt Φ t
1 + Φ Tt PΦ t

If ρ t < ρ min , set ρ t = ρ min ;

,

Pt +1 = ( I − K t Φ Tt ) Pt / ρ t ρ t = 1 − (1 + Φ Tt K t )ε t2 / ∑ o

,

(17)

∑ o > 0 reflects the amplitude of the noise.

4 Adaptive Control Law Optimization
Define the predictive control error

e(t + 1) = yˆ (t + 1) − [αy(t ) + (1 − α ) y r (t + 1)]

(18)

where y r (t + 1) is the reference output. Let the initial value of u(t) be
µ o (t ) = µ (t − 1)

(19)

and the corresponding output of the TSWNN be
yˆ o (t + 1) = f r ( x o (t + 1))

x (t + 1) = [ y (t ),..., y (t − n + 1); u (t ),..., u (t − n + 1)]

o
y
u
;
(20)
Similarly, u k (t ) denotes the control value. The corresponding output of the TSWNN is

yˆ k (t +1) = f r (xk (t +1)) xk (t +1) =[y(t),...,y(t −ny +1);uk (t),...,uk (t −nu +1)] ek (t +1) = yˆ (t +1) − [αy(t) + (1−α ) yr (t +1)]

;

,

(21)

Identification and Control Using Direction Basis Function Neural Network

711

The optimization procedure is described as follows.
- Let k max be the maximum iteration number and e max be the desired control
accuracy. Set the initial Hessian matrix H o = ho I ,0 < ho < 1, k = 0 .
- Let k=k+1. u(t) at the k iteration is calculated by
µ k (t ) = µ k −1 (t ) − H k ∇yˆ k −1e k −1 H k = (λc H k−−11 + ∇ T yˆ k −1 + α c I ) −1

-

,
(22)
where 0 < λc < 1,0 < α c < 1 ; ∇yˆ k −1 is the gradient of y k −1 (t + 1) with respect to u k −1 (t ) .
If k = k max or e k (t + 1) = e max , then stop calculating; Otherwise, go to the previous
step. In the DLS algorithm, the updating equation for H k is
H k = (λ c H k−−11 + ∇yˆ k −1∇ T yˆ k −1 ) −1

(23)
The item α c I of the equation (22) is not included in the equation (23). It has been
found that when yˆ k −1 is small and 0 < λ c < 1 0, the H k will become divergent
infinitely as k increases.

5

Simulation Results

Consider

y(k ) =

y ( k − 10)
1 + y 2 (k − 1)

+ u 3 ( k − 1) ;

m = 7, ho = 0.09, bo = 0.21, λ c = 0.001, ε (t ) = e −800t

In Fig. 1 the problem of set point tracking of the closed-loop system is shown, while
Fig. 2 indicates growth pattern (number of rules).
2.5

10
9

2
Number of hedden units

8

Output

1.5

1

0.5

7
6
5
4
3
2
1

0

0

20

40

60

80

100 120
Samples

140

160

180

200

Fig. 1. Set point trajectory and output

6

0

0

20

40

60

80

100 120
Samples

140

160

180

200

Fig. 2. Growth pattern (number of rules)

Conclusion

In this paper, firstly, we proposed a novel approach to train the TSWNN. An
AFGLVQ technique was adopted to adjust the kernels of the TSWNN, and VRLS
was applied to update the connection weights of the network. Secondly, on the basis
of the one-step ahead TSWNN predictor, a numerically stable Davidon's least
squares-based minimization approach was used to optimize the control law iteratively
in each sample period. The simulations demonstrated the rapid learning and adaptive
property of the identification algorithm and the effectiveness of control algorithm.

712

M. Jalili-Kharaajoo

References
1. Cao S.G., Rees N.W., Feng G. Analysis and design for a complex control systems, part
I: fuzzy modeling and identification. Automatica, 6(8), pp.1017-1028, 1997.
2. Yingwei Lu and P. Saratchandran, Identification of time-varying nonlinear systems
using minimal radial basis function neural networks, IEE Proc.: Cont. Theory and
App.144(2), pp. 202-208, 1997.
3. Fabri S., and Kadirkamanatham V., Dynamic Structure Neural Networks for Stable
Adaptive Control of Nonlinear Systems. IEEE Transaction on Neural Networks, 7(5),
pp.1151-1167, 1996.
4. Jeffrey T.S. and Kevin M. Passino, Stable Adaptive Control Using Fuzzy Systems and
Neural Networks, IEEE Transactions on Fuzzy Systems 4(3), pp.339-359, 1996.
5. Sanner R.M. and Jean-Jacques E. Slotine, Gaussian Networks for Direct Adaptive
Control, IEEE Transaction on Neural Networks, 3(6), pp.837-867, 1992.

