An Atmospheric Sciences Workflow and Its
Implementation with Web Services
David Abramson 1, Jagan Kommineni 1, John L. McGregor 2, and Jack Katzfey 2
1

School of Computer Science and Software Eng., Monash University, 900 Dandenong Rd,
Caulfield East, 3145, Australia
2
Division of Atmospheric Science, CSIRO, PMB 1, Aspendale,Vic, 3195, Australia

Abstract. Computational and data Grids couple geographically distributed
resources such as high performance computers, workstations, clusters, and
scientific instruments. Grid Workflows consist of a number of components,
including: computational models, distributed files, scientific instruments and
special hardware platforms. In this paper, we describe an interesting grid
workflow in atmospheric sciences and show how it can be implemented using
Web Services. An interesting attribute of our implementation technique is that
the application codes can be adapted to work on the Grid without source
modification.

1

Introduction

Computational and data Grids couple geographically distributed resources such as
high performance computers, workstations, clusters, and scientific instruments.
Accordingly, they have been proposed as the next generation computing platform for
solving large-scale problems in science, engineering, and commerce [4][5]. Unlike
traditional high performance computing systems, such Grids provide more than just
computing power, because they address issues of wide area networking, wide area
scheduling and resource discovery in ways that allow many resources to be assembled
on demand to solve large problems.
Grid applications have the potential to allow real time processing of data streams
from scientific instruments such as particle accelerators and telescopes in ways which
are much more flexible and powerful than are currently available. Of particular
interest are applications, called “Grid Workflows”, that consist of a number of
components, including: computational models, distributed files, scientific instruments
and special hardware platforms (such as visualisation systems) [3]. Importantly, such
workflows are interconnected in a flexible and dynamic way to give the appearance of
a single application that has access to a wide range of data, running on a single
platform. Grid workflows have been specified for a number of different scientific
domains including physics [6] and gravitational wave physics [2].
In this paper we describe a Grid workflow for solving problems in atmospheric
sciences. The workflow supports the coupling of a number of pre-existing legacy
computational models across distributed computers. An important aspect of the work
M. Bubak et al. (Eds.): ICCS 2004, LNCS 3036, pp. 164–173, 2004.
© Springer-Verlag Berlin Heidelberg 2004

An Atmospheric Sciences Workflow and Its Implementation with Web Services

165

is that we do not require source modification of the codes. In fact, we don’t even
require access to the source code. In order to implement the workflow we overload
the normal file IO operations to allow them to work in the Grid. We also leverage
existing Grid middleware layers like Globus [4] [5] to provide access to control of the
underlying resources.
In Section 2 we describe an atmospheric science workflow, with some detail of the
functions of the various components. Section 3 discusses the implementation
techniques, and Section 4 provides some experimental results.

2

An Atmospheric Sciences Workflow

Global climate models: A global climate model is a computer model representing the
atmosphere, oceans, land and sea-ice. By solving mathematical equations based upon
the laws of physics, a GCM simulates the behaviour of the climate system. The model
divides the planet into a number of vertical layers representing levels in the
atmosphere and depths in the oceans, and divides the surface of the planet into a grid
of horizontal boxes separated by lines which may be similar to latitudes and
longitudes. In this way, the planet is covered by a three-dimensional grid of boxes
(Figure 1).

Fig. 1. Representation of the Earth’s surface
and atmosphere in a typical global climate
model.

Fig. 2. DARLAM model domain for
simulations using a 3 km grid over Sydney.

Global climate models capture large scale features like the deserts and tropics very
well, but have difficulty capturing smaller features like cyclones and thunderstorms
because they occur at scales much smaller than the grid boxes.
Regional climate models: To improve regional detail in climate models, it is
desirable to reduce the spacing between grid points. However, due to the complexity
of global climate modeling, computational requirements become prohibitive if the
horizontal grid resolution is less than a few hundred kilometers. At this resolution,
vitally important small-scale phenomena, like tropical cyclones and cold fronts, are
poorly captured. This affects simulated patterns of temperature and rainfall, and hence
the ability to realistically simulate observed regional climate features in GCMs.
A computationally feasible alternative to a coarse resolution global climate model
is to use a finer resolution model over a small part of the globe. A regional climate
model (RCM), with a horizontal resolution of about 100 km or less, is able to simulate

166

D. Abramson et al.

regional weather patterns better than most GCMs [7][8][11]. Part of the reason for the
improved climate simulation relative to GCMs is the fact that coastlines and
mountains are represented in more detail in RCMs. Since topographic features
strongly influence regional temperature and rainfall, more detailed features are likely
to give a better climate simulation.
A regional climate model requires meteorological information at its lateral
boundaries in order to simulate weather within its boundaries. For climate change
studies, an RCM is typically driven at its boundaries by information from a coarserscale GCM. This is commonly called nesting an RCM inside a GCM. One-way
nesting allows information to flow from the GCM to the RCM each simulated day,
but the weather simulated by the RCM does not affect the GCM interactively. This
means that the RCM can be run after the GCM experiment has been completed.
A Grid Workflow: Traditionally, the GCM and the RCM have been executed on
the same computer system, and data is passed between them using conventional files.
However, the computational Grid discussed in Section 1 provides an ideal framework
for executing these models on different machines that are physically distributed.
There are many reasons why one might wish to do this. First, both models may have
different computational requirements and be suited to different types of platforms. For
example, one may execute well on a vector supercomputer and the other may be
efficient on a parallel processor. Second, both models may not have been ported to the
same hardware. Thus, it may be time consuming and expensive to couple them on one
machine. Third, the models may be “owned” by different organizations. As discussed
in [4] the computational grid facilitates the construction of a “virtual organization” in
which the models are linked into a single grid application without actually moving the
codes to a single organization. Finally, it may be possible to pipeline the
computations, providing a quicker solution than if they were run sequentially on one
system. This can be achieved if the data files are replaced by communication pipes
that allow one program to write data concurrently with a downstream one reading the
same data. Figure 3 shows such a grid workflow based around the models discussed
above. In this paper we discuss a particular system involving three models – a GCM
called C-CAM, a RCM called DARLAM and a data filter called cc2lam.

Data
Data
Data
Data

Global
Climate
Model 1

Organisation 1

Data
Data
Data
Data

Global
Climate
Model 2

Data
Data
Data
Data

Regional
Climate
Model 1

Data
Data
Data
Data

Organisation 2

Regional
Climate
Model 2

Organisation 3

Fig. 3. An Atmospheric Sciences Grid Workflow

3

An Implementation with Web Services

Clearly it is possible to implement the system described in the previous section in a
number of ways. For example, the programs could be run unmodified, and some
system could be responsible for copying files from one machine to another. This is

An Atmospheric Sciences Workflow and Its Implementation with Web Services

167

effectively the practice that atmospheric scientists have employed manually for some
time now. Or, the files could even be shared by a single distributed file system like
NFS or AFS, removing the need to explicitly copy them from one local file system to
another. The major disadvantage of this general approach is that it does not take
advantage of any potential overlap in the computations. Another option is to modify
the programs so that they do not read directly from files, but instead they use a
message passing library like PVM or MPI to send data from one model to another.
This allows the computations to be pipelined, however the programs would need to be
modified at the source level. Further, once modified that would no longer work as
stand alone codes, limiting the flexibility of the individual components. A third
alternative is to modify the file system library so it performs message passing rather
than writing to and reading from local files. This achieves the advantages of both of
the previous alternatives, requires no source modification (only relinking the
application) and does not permanently fix the way the programs operate. For example,
by linking the normal file system primitives, the programs behaves as normal, reading
and writing local files. However, when the message passing library is linked, the
program sends messages for writes, and receives messages for read. In a previous
paper we proposed such a mechanism, called NetFiles, for implementing parallel
master-slave programs [1]. NetFiles were implemented within a single cluster or
parallel machine and could not cross administrative domains. Here we have
broadened the approach to support interprocess communication across the
computational Grid. Accordingly we have called this mechanism GridFiles. Figure 4
shows how two legacy computations can be coupled using GridFiles.
In the GridFiles approach, the conventional system calls (like open, read, write
etc.,) are replaced (transparently) by a call to a module called a “File Multiplexer”.
The File Multiplexer is responsible for passing the file system operations onto an
appropriate service, and has the flexibility to change the mappings dynamically. Thus,
the File Multiplexer can redirect IO requests to local files, local processes, remote
files or remote processes. This means that a program can perform a READ operation,
and this might read a local file, or a remote one, or even be connected directly to a
WRITE operation on a remote machine. The latter mode is the equivalent of
connecting the two programs by sockets, and allows complete overlap of IO
operations, and is called the buffer service. This structure is depicted in Figure 4. The
File Multiplexer is composed of three clients. The Local File Client performs local
file operations. The Grid File Client communicates with the Grid Buffer Service, and
the GNS Client communicates with the GriddLeS Name Service. The Grid Buffer
and Name Services are both implemented using Web Service technologies such a
SOAP and XML [9][10].
The GridBuffer service acts as a sink for WRITE operations and a source for
READs. In order to support random read and write operations, data is stored in a hash
table rather than a sequential buffer. Thus, if a read is issued for a block that has not
been written yet, the read waits for the data to arrive. After a block has been read from
the hash table it is written to a cache file and then deleted from the hash table. The
cache file is provided for two main reasons. First, it allows a block to be reread even
after it has been deleted from the hash table. This occurs when the reader seeks back
to a previous block. Second, it provides a mechanism for implementing broadcast
operations to more than one process. When this happens, the first reader obtains the

168

D. Abramson et al.

data from the hash table, but subsequent readers retrieve the data from the cache file.
It is possible to have an arbitrary number of readers using the approach without the
need to inform the Grid Buffer Service before hand.
Application

Application
Read,
Write,
etc

Read,
Write,
etc

Grid FTP
Server

Remote File
Client

Grid FTP
Server

Remote File
Client

Local File
System

Local File
Client

Local File
System

Local File
Client

Grid Buffer
Server

Grid Buffer
Client

Grid Buffer
Server

Grid Buffer
Client

alb48.151.-34.04.01
o3_data.18
co2_48.151.-34.04
radon48.151.-34.04
topout48.151.-34.04
co2_data.18
rough48.151.-34.04.01
veg48.151.-34.04
eigenv18-5.300
rsmin48.151.-34.04.01
glob48cc48-60.010120.0000

CCAM
Model

s_00a
s_12a
input.8
qg_00
log.globpe90.cc48-8.00
qg_12
surfilecc48-8.010120.0000

soil48.151.-34.04

GNS
Client

GriddLeS Name
Server (GNS)

File Multiplexer

GNS
Client
File Multiplexer

Fig. 4. Using GridFiles to link apps

Intermediate-process
(cc2lam)

eigenv18-5.300
co2_data.18
o3_data.18
albedo.syd3km.01
roughness.syd3km.01
soil.syd3km
top.syd3km.100x100.ff
rsmin.syd3km.01
veg.syd3km

DARLAM
Model

log.syd3km.01012000
trcf.syd3km.01012000
scrn.syd3km.01012000.nc
outf.syd3km.01012000.nc
tmax.syd3km.01012000.nc

Fig. 5. File dataflow

The GriddLes Name Service (GNS) is responsible for configuring the grid
application. Each entry in the GNS indicates what should happen when a particular
file is opened on a particular resource. At the time of the deployment the GNS reads
the data from the configuration file and keeps it in a hash table. These keys and values
correspond to the global naming schema. The reader and writer applications can use
different keys which are mapped to the same entry in the GNS, indicating both are
linked to either the same file or buffer. Buffers are distinguished from files by the
word “buffer” in the value. If an entry in the GNS represents a buffer, then additional
configuration information is contained. The GNS can be updated at any time, a this
reconfigures the Grid application dynamically.
All the applications and web services can run on the same system or each one can
be distributed to the different Grid nodes which are located geographically at different
locations.

4

Experimental Results

Figure 5 shows a particular configuration of the three atmospheric models discussed
in section 2, namely C-CAM, cc2lam and DARLAM. Importantly, most of the
computation is performed by C-CAM and DARLAM, and cc2lam provides simple
data manipulation and filtering between the two codes. In this section we describe an
experiment in which C-CAM, cc2lam and DARLAM are coupled by Grid Buffers,
that is, the output of C-CAM is streamed into DARLAM (via cc2lam). Because we
used a file multiplexer as discussed in section 3, it was possible to do this without
source modification. This is important because C-CAM and DARLAM are legacy
codes written in Fortran, and we did not wish to make significant modifications to
their structure. This scheme is very flexible. All models and services can run on a
single machine, or on different nodes of a cluster or on different machines in a
computational grid. Further, these configuration changes can occur without any
modification to the atmospheric models.

An Atmospheric Sciences Workflow and Its Implementation with Web Services

169

When the first writer application (C-CAM) writes block of data (typically of 4096
bytes) using the write statement, then the data need to be transferred over to the
GridBuffer service by using the client component of GridBuffer service which is in
FMP underneath the application layer without the user interaction. Once the data
arrives, the reader application (in this context cc2lam) uses the GriddBuffer client and
obtain the block of data and writes into the other GridBuffer in a similar way, so that
the other reader application (in this context the DARLAM Model) reads the block of
data. In some instances, DARLAM rereads some of the input data. Because the data
has already been deleted from the hash table in the Grid buffer Service, it is read form
the cache file instead. This occurs transparently to the DARLAM model.
In this experiment we utilised a number of machines shown in Table 1, in three
countries (AU, US and UK).
Table 1. Machine List

In this case study, three different experiments were performed.
Case 1: All models are executed concurrently on the same machine. There are two
variations on this experiment. First, the programs read and write conventional local
files. The results of this experiment are shown in the “Files” column in Table 2.
Second, the programs use Grid Buffers instead of files. These times are shown in the
Buffers column in Table 2. All times are cumulative, and thus the DARLAM time
also indicates the total time taken. In this experiment the code was run for 480 and
960 time steps.
The results shown in Table 2 highlight that using buffers is always faster than
using files when the codes are run on the same system. This is interesting because the
models are multiprocessing the single CPU on these machines, and thus it suggests
that buffers are allowing some overlap of computation and communication. Even
more interesting is that most of the runs performed with buffers were actually faster
than running the codes sequentially on the same platform, again because of the
overlap in IO and computation. The exceptions to this are those run on dione and
vpac27, which are presumably because of the relative speed of the computation and
the IO on these two machines.
Case 2: C-CAM and DARLAM are executed on different computers at different
locations, whilst cc2lam is run on the same machine as C-CAM. Because there are a
large number of potential pairings of machines, we have selected a few interesting

170

D. Abramson et al.
Table 2. Cumulative concurrent runs on the same system (time in hr:min:sec)
Computer
dione

brecca

freak

bouscat

dragon

Model
C-CAM
cc2lam
DARLAM
C-CAM
cc2lam
DARLAM
C-CAM
cc2lam
DARLAM
C-CAM
cc2lam
DARLAM
C-CAM
cc2lam
DARLAM

480 time steps
Files
Buffers
00:41:18 00:44:10
00:41:56 00:44:15
01:08:17 00:49:12
00:18:13 00:20:05
00:18:25 00:20:12
00:27:58 00:22:57
00:34:35 00:35:21
00:35:26 00:35:33
00:52:39 00:40:30
01:10:22 01:17:51
01:10:39 01:18:10
01:55:27 01:29:59
00:41:25 00:41:28
00:41:46 00:41:41
01:06:17 00:46:24

960 time steps
Files
Buffers
01:23:57 01:29:59
01:25:13 01:30:09
02:19:13 01:35:00
00:36:29 00:41:21
00:36:50 00:41:24
00:56:35 00:44:11
01:22:28 01:28:15
01:23:15 01:30:01
02:14:22 01:36:15
02:44:03 02:42:42
02:44:39 02:43:00
04:14:54 02:54:55
01:23:01 01:27:57
01:23:43 01:28:08
02:13:12 01:32:57

ones and present the timing results in Table 3. Again all times are cumulative, and
thus the DARLAM time also indicates the total time taken. In the case where local
files are written we have also included the time taken to copy the files in the
cumulative totals. The runs were done for both 480 and 960 time steps.
Table 3. Cumulative concurrent runs (Time in hr:min:sec)

The results show that buffers are always faster for systems which have good
network connections than when files are used. Interestingly, the results for 960 time
steps are less than double those for 480. This is because the startup overheads are
masked in the longer runs, and thus the parallel efficiency is higher.
The results for distant machines tell a different story. Because these machines
have poorer networks between them, it is not always faster to use buffers than to copy

An Atmospheric Sciences Workflow and Its Implementation with Web Services

171

the files. For example for the shorter runs of 480 time steps, it is always better to use
file copies. On the other hand, buffers are more efficient for some of the longer 960
time step runs. The results shown here highlight the importance of being able to
reconfigure the application dynamically because it is not always possible to know
which configuration will be more efficient.
Case 3: In this experiment C-CAM and cc2lam are executed on brecca and
DARLAM model is run on dragon. We run models for 480 time but data is exchanged
at different intervals from 60 to 15 time steps. The results are shown in Table 4.
Table 4. Varying the output frequency (Time specified in hr:min:sec format)
Amount of Output Data produced by
different models in MB
Print Interval
in time steps
60
30
15

CCAM

cc2lam

72.663
137.253
264.428

DARLAM

42.485
80.249
155.771

43.000
83.599
163.760

Total Time
with Files
00:29:38
00:35:50
00:38:54

Total time
with Buffers
00:23:23
00:26:30
00:26:21

As expected, as the write interval reduces, the magnitude of data produced by each
model increases. Even though there is an increase in computation time as the write
interval reduces, the total computation time with the buffers is less than with files
because more overlap is possible.
Table 5 analyses the degree of overlap in the computations, and therefore the
efficiency, for a few different machine configurations. Here we calculate the best
theoretical time that could have been achieved assuming no startup costs and perfect
networking, and compare this to the actual time. The results indicate that the
efficiency can be quite high for low latency, high bandwidth networks, especially for
the longer runs.
Table 5. Overlap comparisons (Time specified in hr:min:sec format)
Machines
Brecca-2
& vpac27
dione
& vpac27
Brecca-2
& vpac27

Time
Steps

C-CAM
with Files

DARLAM

with Files

Total time with
Best
Buffers
Theoretical

Efficiency

480

0:16:34

0:31:00

0:40:43

0:31:00

76%

960

0:32:32

1:04:07

1:14:35

1:04:07

86%

480

0:28:21

0:31:00

0:48:47

0:31:00

64%

960

0:55:18

1:04:07

1:26:04

1:04:07

74%

480

0:16:34

0:13:16

0:24:58

0:16:34

66%

960

0:32:32

0:25:54

0:44:27

0:32:32

73%

172

5

D. Abramson et al.

Conclusions

In this paper we have discussed the implementation of a grid workflow that couples
two legacy atmospheric science applications, namely a global climate model and a
regional weather model. One of the more significant achievements of the experiment
is that we managed to do this without any changes in the source code of the two
models. This is no mean feat since they are legacy codes written in Fortran and were
designed without any knowledge of the underlying grid infrastructure. Inter-process
communication is provided by a software device called a File Multiplexer, and we
have chosen to implement this with web services.
The performance results indicate that there are cases when it is advantageous to
couple the models tightly using pipes, and other cases where it is more efficient to
write files locally, copy them to the receiving node and read them locally. An
important feature of our implementation is that the decision about whether to copy or
use buffers can be delayed until the time that the application is configured and does
not need to be integrated into the source code of the models. We are not aware of
other systems with equivalent flexibility.
The case study discussed here is actually a small fragment of a much larger system
currently being constructed. Rather than just couple 2 models, we plan to couple a
number of atmospheric science models including air pollution codes. We also plan to
retrieve data directly from scientific instruments like temperature and pressure
sensors. Such an application would form an interesting grid application because it
would involve integration of a number of separate computational models, running on
different computer systems (possibly owned by different organizations) and taking
data from real time scientific instruments.

Acknowledgements. The Australian Research Council and Hewlett Packard support
this work under an ARC linkage grant. Kommineni was supported by an Australian
Postgraduate Research Award.

References
1.

2.

3.

4.
5.

Chan, P. and Abramson, D. “NetFiles: A Novel Approach to Parallel Programming of
Master/Worker Applications”, HPC Asia 2001, 24-28 September 2001 • Royal Pines
Resort Gold Coast, Queensland, Australia.
Deelman, E., Blackburn, K. et al., "GriPhyN and LIGO, Building a Virtual Data Grid for
Gravitational Wave Scientists," presented at 11th Intl Symposium on High Performance
Distributed Computing, 2002.
Deelman, E., Blythe, J., Gil, Y., Kesselman, C., Mehta, G., Vahi, K., Lazzarini, A., Arbree,
A., Cavanaugh, R. and Koranda, S. “Mapping Abstract Complex Workflows onto Grid
Environments”, Journal of Grid Computing, Vol. 1, No. 1, pp 9--23, 2003.
Foster, I. and Kesselman, C., Globus: A Metacomputing Infrastructure Toolkit,
International Journal of Supercomputer Applications, 11(2): 115-128, 1997.
Foster, I., and Kesselman, C. (editors), The Grid: Blueprint for a New Computing
Infrastructure, Morgan Kaufmann Publishers, USA, 1999.

An Atmospheric Sciences Workflow and Its Implementation with Web Services
6.
7.

173

GriPhyN 2003, www.griphyn.org
McGregor, J. L., Nguyen, K. C., and Katzfey, J. J. Regional climate simulations using a
stretched-grid global model. In: Research activities in atmospheric and oceanic modelling.
H. Ritchie (ed.). (CAS/JSC Working Group on Numerical Experimentation Report; 32;
WMO/TD - no. 1105) [Geneva]: WMO. p. 3.15-3.16, 2002.
8. McGregor, J.L., Walsh, K.J. and Katzfey, J.J. Nested modelling for regional climate
studies. In: A.J. Jakeman, M.B. Beck and M.J. McAleer (eds.), Modelling Change in
Environmental Systems, J. Wiley and Sons, 367–386, 1993.
9. Robert van Engelen, “The gSOAP toolkit 2.0.”, Technical report, Florida State University,
http://www.cs.fsu.edu/~engelen/soap.html, 2001.
10. Sun Microsystems, “Web Services Made Easier: The Java APIs & Architectures for
XML”, 2002, http://java.sun.com/webservices/white/ &
http://java.sun.com/webservices/webservicespack.html
11. Walsh, K.J. and McGregor, J.L. January and July climate simulations over the Australian
region using a limited-area model. J. Climate, 8 (10), 2387–2403, 1995.

