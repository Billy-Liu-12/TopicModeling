Aspects of a Massively Distributed Stable
Component Space
Klaus Schmaranz1 and Dirk Schwartmann2
1

Institute for Information Systems and Computer Media (IICM), Graz, Austria
klaus@iicm.edu
2
Institute of Aerospace Medicine, Cologne, Germany
dirk.schwartmann@dlr.de

Abstract. Modern healthcare requires massively distributed virtual electronic patient records. Recent component-based approaches are the
best available solutions for the development of such systems. However,
all of today’s solutions have one very important problem: they do not
support consistent and robust object-addressing for the dynamic case,
although this is one of the most crucial features in massively distributed
environments. The goal that has to be achieved is to replace physical addresses by stable and robust logical handles, managed by a distributed
lookup service. Naive implementations suﬀer from very severe scalability
problems that have to be overcome without violating robustness requirements.
DOLSA (=Distributed Object Lookup Service Algorithm) is designed
to provide a lookup-based object addressing mechanism that is robust
against any kind of object-, server- and network-dynamics. The algorithm is fully scalable in respect to the number of managed mappings,
the number of lookup-requests, the number of distributed servers and
the number (and frequency) of object movement operations.
Keywords: Robust Globally Unique Handles, Distributed Object System, Distributed Component System, Distributed Lookup Service, Dinopolis, MTP

1

Introduction

One of the main problems in today’s heterogeneous and massively distributed
information spaces is found in the way objects are addressed. As has been stated
in [5], all addressing mechanisms that are in use today, share the same problem:
most algorithms are not robust against object-movement. The few implementations that deal with this problem (see e.g. [7]) do not scale well, considering
the number of objects, users and especially considering the number of moveoperations. During development of the Dinopolis distributed componentware
framework (see also [5] and [3]) it became clear that consistency and robustness of object-addressing are the most crucial features in a massively distributed
system. The goal that had to be achieved was the development of an algorithm
M. Bubak et al. (Eds.): ICCS 2004, LNCS 3039, pp. 1156–1164, 2004.
c Springer-Verlag Berlin Heidelberg 2004

Aspects of a Massively Distributed Stable Component Space

1157

providing stable, globally unique handles (short GUH s). The key features of such
GUH s can be summarized as follows:
– One GUH always refers to one and the same object, no matter whether
and how often an object changes its physical location. This also includes the
case that the original location may no longer exist and one or more diﬀerent
systems have taken over its responsibility.
– A GUH that is used for one object must never be used for any other object,
even if the original object is deleted. Otherwise it could happen that an
object is unintentially replaced by a diﬀerent one.
– GUH s can be stored anywhere, e.g. on the users’ harddisks when bookmarking objects. Therefore update-operations on GUH s by means of pushtechnologies are unthinkable.
– Closely and loosely synchronized replication (see [5]) of objects have to be
supported by the GUH -resolving mechanism.
There are two main families of algorithms that are robust against objectmovement:
1. Forwarding algorithms that rely on traces which are left whenever objects
are moved to diﬀerent locations.
2. Algorithms based on lookup-service strategies that rely on updates of the
lookup-service whenever objects are moved.
It can easily be seen that pure forwarding algorithms do not scale at all
for frequent object movements. Further, servers that go oﬄine would break the
consistency of any traces that objects left on them. Finally, the requirement
for transparent component replication would not work with forwarding anyhow.
Therefore the algorithm of choice for implementing the concept of GUH s has
to be a lookup-service based algorithm. Considering a huge number of GUH s
in large and highly dynamic distributed systems, a naive lookup-algorithm (e.g.
a central lookup-service) would not scale either. Hence an algorithm for implementing an arbitrarily distributable lookup-service has to be found. Deﬁning
GUH s with an internal hierarchical structure like hostnames in DNS (see [4]) is
not realistic too, because the resolve order would then be deﬁned by the GUH s
rather than by the lookup service. What is required is that the resolve-order has
to be dynamic and self-organizing in a way so that administrative aspects are
not neglected. For reasons of robustness, stability, scalability and availability as
well as for some administrative reasons the DOLSA algorithm presented in this
paper is a combination of the advantages of both algorithm families mentioned
above. DOLSA implements a self-organizing (but inﬂuencable!), arbitrarily distributed lookup-service with robust caching and robust short-term forwarding
for scalability reasons.
In the following only the ﬁnal version of the resolving algorithm together
with the GUH -structure is described. A detailed description of all the decisions
that lead to this algorithm would be beyond scope of this paper as well as an
in-depth discussion of some special operations. Interested readers can ﬁnd these
details in [6].

1158

2

K. Schmaranz and D. Schwartmann

Structure of the Distributed Object Lookup Service

The structure of the distributed lookup service is straightforward:
– There exists an arbitrarily large number of Object Lookup Servers (short
OLS ).
– Each OLS is responsible for GUH -to-address mappings for an administratordeﬁned area of responsibility.
– OLS s do not know about each other and do not form any kind of hierarchy.
A general handle is a pair of IDs:
1. OLS-ID (=object lookup-service ID). Every OLS has such a unique ID.
2. LOID (=local object ID). Every object gets such an ID and this ID is unique
within the lookup-space of one OLS.
As can easily be seen, this pair results in a general globally unique handle.
Please note that such a simple handle is not enough for the dynamics addressed
by DOLSA, therefore this is not yet a GUH in our sense. The ﬁnal structure of a
GUH is described in section 3. Although the hostname of an OLS would already
fulﬁll the requirement to be a unique ID, it is not useful for the dynamic case
of OLS -splitting in combination with object movement. Therefore the OLS-ID
is an arbitrary, unique ID managed by a special Server Lookup-Service (short
SLSvc) which maps the ID to an actual hostname on request. The ID itself
does not contain any administrative structures like e.g. the resolve-order, as is
the case with DNS. Whenever a lookup-server is moved, an update of the IDto-hostname mapping is performed in the SLSvc. With this indirection GUH s
become completely independent from hostnames.
For availability and scalability reasons the SLSvc itself is distributed across
the network in a semi self-organizing manner. In order to avoid getting sidetracked here let us just assume that a scalable and reliable, distributed, selforganizing SLSvc is realistic.

3

Structure of GUH s and the DOLSA Base-Algorithm

The DOLSA algorithm is based on the following structure of GUH s: A GUH
always consists of three handles, each of them being a pair [OLS − ID, LOID]
as already described. The three handles are:
1. BPH (=birthplace handle)
2. MBPH (=moved-birthplace handle)
3. CH (=current handle)
The following deﬁnitions apply to the single handles of a GUH :
– The BPH always contains a non-empty entry refering to the birthplace of
an object.

Aspects of a Massively Distributed Stable Component Space

1159

– The BPH is deﬁned when an object is created and it must never change
during the whole lifetime of an object.
– It is guaranteed that the BPH can always be resolved.
– The BPH and only the BPH is taken when comparing two handles for
equality.
– The MBPH only contains a non-empty entry if the birthplace-OLS of an
object was taken oﬄine and therefore one or more diﬀerent OLS s have taken
over its responsibilities. Otherwise the MBPH is empty.
– It is not guaranteed that the MBPH can always be resolved. Resolving is
not possible if the so-called moved-birthplace-OLS was taken oﬄine. In this
case it is guaranteed that the SLSvc can resolve the BPH.
– The CH only contains a non-empty entry if the object was moved across
OLS -responsibility boundaries. Otherwise the CH is empty.
– It is not guaranteed that the CH can always be resolved. Resolving is not
possible if the object has moved on. In this case the MBPH or the BPH are
taken for resolving, as will be described later.
With this deﬁnition of a GUH the DOLSA base-algorithm can be sketched.
Two diﬀerent views exist that make up the whole algorithm:
1. The view of the distributed lookup-service as a whole that has to react on
diﬀerent dynamic aspects to guarantee robust GUH -resolving.
2. The view of the requestor that holds a GUH in hands and wants it to be
resolved.
3.1

The View of the Distributed Lookup-Service

The core-functionality of DOLSA can be found in the organization of the distributed lookup-service. At the moment let us assume that the starting point for
the following description is an existing but empty SLSvc. It is further assumed
that one host of the SLSvc is known and that the SLSvc is contacted through
this known host.
Adding an OLS to the Distributed Service. If a new OLS is going to be
added to the distributed service, the following steps have to be performed:
– The new OLS has to contact the SLSvc to register its hostname.
– The SLSvc calculates a unique OLS-ID, registers the appropriate OLS-IDto-hostname mapping and returns the OLS-ID.
Changing an OLS ’s Hostname. If an OLS changes its hostname, it contacts
the SLSvc and sends an update request. The SLSvc updates the registered OLSID-to-hostname mapping. The OLS-ID remains untouched.

1160

K. Schmaranz and D. Schwartmann

Moving an Entry to a Diﬀerent OLS. Moving objects around inside the
area of responsibility of an OLS is trivial. The more interesting situation occurs
when an object is moved from the area of responsibility of one OLS to another
OLS. In this case the following actions are necessary:
1. Triggered by the source-OLS the destination-OLS calculates a LOID that
the entry will obtain when moved. This LOID is passed on to the source
OLS.
2. The source-OLS marks the entry to be moved and sets an appropriate temporary forwarder.
3. As soon as the entry in the destination-OLS is active, the destination-OLS
informs the birthplace (or moved-birthplace respectively) that the entry was
moved. The birthplace-OLS (or moved-birthplace-OLS respectively) updates its appropriate birthplace-entry to reﬂect the changes.
4. As soon as the birthplace updated its entry, the destination-OLS notiﬁes the
source-OLS which in turn can drop the temporary forwarder.
In case that the source-OLS is the birthplace of the object, a shortcut of the
algorithm without network-traﬃc in step 4 can be implemented.
Dealing with an OLS Going Oﬄine. If an OLS goes oﬄine and its lookup
tables therefore have to be moved to one or more existing OLS s the following
actions have to take place:
1. The source-OLS that will go oﬄine contacts the SLSvc and reports the start
of the move operation as well as all destination-OLS s that will take over its
responsibilities.
2. The tables are moved to the appropriate destination-OLS s.
3. The source-OLS reports the end of the move operation to the SLSvc and
can then go oﬄine.
If the SLSvc is contacted to resolve an OLS that is just in the middle of
performing such a move it returns the original OLS ’s hostname together with
the destination OLS s’ hostnames, OLS-IDs and a ﬂag that alerts the requestor
that a move operation is just taking place.
If the SLSvc is contacted to resolve an OLS after moving of the whole content has been ﬁnished, only the destination OLS s’ hostnames and OLS-IDs are
returned.
The responsibility of each of the destination OLS s depends on the type of
the entry that it receives:
Standard-entry: If a standard-entry (i.e. not a birthplace-entry, see below) is
moved to the destination, it is treated as already described in section 3.1.
Birthplace-entry: If a birthplace-entry is moved to the destination (i.e. the
corresponding object had its birthplace in the area of responsibility of the
source OLS ) the BPH of the entry is stored in the destination OLS ’s birthplace mapping table. Further, a standard move-operation, as described in
section 3.1, takes place.

Aspects of a Massively Distributed Stable Component Space

1161

In case that an OLS is already the moved-birthplace for one or more other
OLS s the same algorithm applies. So-called moved-birthplace-entries are treated the same as birthplace-entries. For moved-birthplace-OLS s going oﬄine the
SLSvc has the responsibility to compact the resulting move-chains to a one level
uncertainty-indirection.
Splitting up an OLS. If for whatever reason (e.g. massive request-overload) an
OLS has to be split up without going oﬄine itself, this is considered the same
case as moving many entries from one OLS to another. Therefore no MBPH
comes into play, because the birthplace server is still online.
This behaviour is the reason why the GUH contains the CH, otherwise the
load on the original-OLS could not be reduced by splitting it up. In this situation
network traﬃc could not be gotten under control any more and the algorithm
would not scale. By introducing the CH the original-OLS is only contacted once
per GUH and then the traﬃc calms down automatically.
3.2

The View of the Requestor

From the point of view of the requestor two diﬀerent instances exist in principle
which have to be contacted for resolving: OLS and SLSvc. For simplicity reasons
let us assume for the moment that the SLSvc is a server with a known hostname.
In principle resolving a GUH is a two-step process:
1. For a given GUH ask the SLSvc for the hostname of the OLS which knows
the GUH -to-address mapping. In most cases the answer will contain the
hostname of exactly one OLS.
2. Contact the obtained OLS and ask for resolving of the GUH.
With dynamic aspects taken into account resolving a GUH looks as follows:
1. The Requestor contacts the SLSvc and passes the GUH on to it.
2. The SLSvc decides on the content of the GUH, which OLS-ID is relevant
for resolving:
CH is non-empty: In this case the SLSvc looks up its internal tables,
whether the OLS-ID in the CH points to an OLS that is registered
as being online. If yes, the SLSvc goes on with step 3. Otherwise the
decision process goes on with the next step described below.
MBPH is non-empty: In this case the SLSvc looks up its internal tables,
whether the OLS-ID in the MBPH points to an OLS that is registered
as being online. If yes, the SLSvc goes on with step 4. Otherwise the
decision process goes on with the next step described below.
BPH resolving: If the GUH is a valid handle at all, the BPH contains
all information necessary for resolving. In this case the SLSvc looks up
its internal tables which OLS-ID the BPH refers to. Three cases are
possible:

1162

K. Schmaranz and D. Schwartmann

a) The birthplace-OLS is online and therefore the OLS-ID can directly
be resolved to a hostname. In this case the SLSvc goes on with step 5.
b) The birthplace-OLS is no longer online and its content is now handled by one or more moved-birthplace-OLS s. In this case the SLSvc
goes on with step 6.
c) The birthplace-OLS is just in progress of moving its content to one
or more moved-birthplace-OLS s and then going oﬄine. In this case
the SLSvc goes on with step 7.
3. The hostname of the current-OLS is returned to the requestor together with
a ﬂag that CH -resolving has resulted in the given hostname.
The following tasks have to be performed then:
a) The requestor contacts the given OLS and passes on the GUH with a
resolve request for the CH.
b) If the OLS can resolve the CH, the mapped address is returned and
resolving is ﬁnished.
c) If the OLS cannot resolve the CH (=entry was moved to a diﬀerent
OLS ), it returns an appropriate error. The requestor invalidates the CH part in the GUH and continues with step 1.
4. The hostname of the moved-birthplace-OLS is returned to the requestor together with a ﬂag that MBPH -resolving has resulted in the given hostname.
The following tasks have to be performed then:
a) The requestor contacts the given OLS and passes on the GUH with a
resolve request for the MBPH.
b) If the OLS is still the home of the entry it returns a CH (the CH is
usually diﬀerent from the MBPH after a birthplace-move!) together with
the mapped address. The requestor updates the CH -part in the GUH
and resolving is ﬁnished.
c) If the requested entry already moved on to a diﬀerent OLS, only a CH
is returned. In this case the requestor updates the CH -part in the GUH
and continues with step 1.
5. The hostname of the birthplace-OLS is returned to the requestor together
with a ﬂag that BPH -resolving has resulted in the given hostname.
The following tasks have to be performed then:
a) The requestor contacts the given OLS and passes on the GUH with a
resolve request for the BPH.
b) If the OLS is still the home of the entry it returns the mapped address
and resolving is ﬁnished.
c) If the requested entry already moved on to a diﬀerent OLS, only a CH
is returned. In this case the requestor updates the CH -part in the GUH
and continues with step 1.
6. The list of MBPH s together with the corresponding hostnames which could
be the moved-birthplace-OLS s of the entry is returned to the requestor. A
ﬂag is set that there is uncertainty about the current OLS which manages
the given GUH.

Aspects of a Massively Distributed Stable Component Space

1163

The requestor then follows in principle the same procedure that was described in step 4 with one diﬀerence: According to the uncertainty-approach the
moved-birthplace-OLS s are contacted one by one, until the handle can be
resolved. The MBPH -part of the GUH is updated with the MBPH of the
OLS that is ﬁnally able to resolve the request. Please note that uncertaintyresolving has to be done only once because then the MBPH is ﬁxed. Therefore no scalability problems occur.
7. The hostname of the birthplace-OLS is returned to the requestor together
with a list of MBPH s and corresponding hostnames which could be the
moved-birthplace-OLS s of the entry. An according ﬂag is set to signal that
moving entries are resolved.
In this case the requestor performs in principle the same procedure that was
described in step 6, with the only diﬀerence that the birthplace-OLS (i.e.
the original host) is contacted ﬁrst and asked for resolving. The uncertaintyapproach only has to be taken for entries that already moved on before.
For sketching the base of DOLSA it was assumed that the SLSvc “exists and
is somehow distributed”. However, clever distribution of the SLSvc is one of the
most crucial points in order to make DOLSA useful in practice. Nonetheless a
detailed description of all distribution aspects would be beyond scope of this
paper. Interested readers can ﬁnd an in-depth discussion in [6].

4

Conclusion

The DOLSA base-algorithm, as presented in this paper, is the ﬁrst algorithm
meeting the requirements of massively distributed and highly dynamic component systems such as Dinopolis (see [3], [5]). It provides globally unique object
handles that are robust against object movement. All critical aspects of scalability are taken into account and DOLSA scales well in respect to the number
of object lookup servers distributed across the network, the number of objects
managed by the object lookup servers, objects which change their locations frequently and the number of lookup-operations.
Attentive readers will have noticed that race-conditions during object movement can occur. The obvious candidates for such problems are resolve-to-access
delays, lookup-server-update delays and fast-moving objects. To deal with these
race-conditions, special algorithms for moving objects and for catching up on
fast-moving objects were developed. Again, interested readers can ﬁnd a detailed discussion of these aspects in [6].
Although security aspects were not especially mentioned in this discussion,
because they would have been way beyond the scope of this paper, these aspects
are extremely important within the area of medical applications. All security
considerations that could aﬀect DOLSA were taken into account:
– GUH s are designed in a way so that they allow electronic signatures for
interrelations, even if objects move.

1164

K. Schmaranz and D. Schwartmann

– DOLSA is designed not to interfere with authentication, authorization and
encryption mechanisms.
One very important point remains to be mentioned: DOLSA does not dictate, which kind of mappings are stored. GUH s can be mapped to “anything”.
Therefore the algorithm is not only suited for distributed component systems,
it is also possible to manage today’s standard hyperlinks with DOLSA. GUH s
also support a textual representation that is compatible to today’s URLs. Thus
it would be easy to replace today’s inconsistent URLs by consistent and robust
GUH s. The well-known browsers would only have to implement one simple additional lookup operation before loading a page and the well-known servers would
only have to report restructuring operations of their areas to an OLS.

References
1. Andrews K., Kappe F., Maurer H., Schmaranz K.: On Second Generation Hypermedia Systems, Proceedings ED-MEDIA 95, Graz (1995), 75–80.
2. Aly F., Bethke K., Bartels E., Novotny J., Padeken D., Schmaranz K.,
Schwartmann D., Wilke D., Wirtz M.: Medical Intranets for Telemedicine Services: Concepts and Solutions, Proceedings G7 Meeting “The Impact of Telemedicine on Health Care Management”, Regensburg (1998), available online at http://www.uni-regensburg.de/Fakultaeten/Medizin/Uch/g7/
program/mon.htm.
3. Dallermassl C., Haub H., Maurer H., Schmaranz K., Zambelli P.: Dinopolis - A
Leading Edge Application Framework for the Internet and Intranets, Proceedings
WebNet 2000, San Antonio, TX (2000), 111–116.
4. Mockapetris P., Dunlap K. J.: Development of the domain name system, Proceedings ACM SIGCOMM 1988, Stanford, CA (1988), 123–133.
5. Schmaranz K.: On Second Generation Distributed Component Systems, J.UCS
Vol.8, No.1, 97–116 (2002).
6. Schmaranz K.: Dinopolis – A Massively Distributable Componentware System,
available online at
http://www.dinopolis.org/documentation/publications/
habilthesis2002 dinopolis-a massively distributable componentware
system.pdf
7. ObjectSpace’s Home page, available online at http://www.objectspace.com.

