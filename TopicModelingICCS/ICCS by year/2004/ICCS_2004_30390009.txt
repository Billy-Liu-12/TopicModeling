On New Radon-Based Translation, Rotation,
and Scaling Invariant Transform for Face
Recognition
Tomasz Arod´z1,2
1
2

Institute of Computer Science, AGH, al. Mickiewicza 30, 30-059 Krak´
ow, Poland
Academic Computer Centre – CYFRONET, Nawojki 11, 30-950 Krak´
ow, Poland
gefarodz@cyf-kr.edu.pl

Abstract. The Radon transform has some interesting properties
concerning the scaling, rotation-in-plane and translation of the input
image. In the paper, these properties are a basis for deriving a transformation invariant to the aforementioned spatial image variations,
a transformation that uses direct translation, angle representation
and 1-D Fourier transform. As the face images often diﬀer in pose
and scale of the face, such a transformation can ease the recognition
task. Experimental results show that the proposed method can achieve
96% and 89% recognition accuracy for, respectively, uniformly and
non-uniformly illuminated images.
Keywords: Face recognition; Radon transform; Invariant recognition

1

Introduction

In face recognition, one often encounters the problem that pictures of the same
person are taken in diﬀerent conditions. These include variations in facial expression, illumination as well as spatial location and pose of the face in the
picture. The latter ones consist of translation and scaling of the face, rotation
on-the-plane and rotation in depth. In this paper, a method for compensating
translation, scaling and rotation-on-the-plane will be shown. The goal is to present a transformation which will give identical results if applied to a pair of
images that are in the similarity relation with each other. The result of such a
transformation form a new, invariant set of features for recognition.
Several approaches to the problem of invariant object recognition have been
proposed up to date. These include group of methods based on the Fourier and
log-polar or the Fourier-Mellin transform [6,8] or Taylor and Hessian invariants
[2]. Methods based on algebraic moments, e.g. the Zernike [11] or Hu [3] moments are also proposed. Another approach, using only object boundary has been
proposed, e.g. a method utilizing wavelets [4] or multi-vector eigenvector shape
descriptors [5]. Other methods, using e.g. a group of random lines through the
object are also used [10]. The Radon transform has been used as part of the invariant transform [7], albeit not in the face recognition. In this paper the Radon
transform will be used as a preliminary step for deriving the invariance.
M. Bubak et al. (Eds.): ICCS 2004, LNCS 3039, pp. 9–17, 2004.
c Springer-Verlag Berlin Heidelberg 2004

10

T. Arod´z

The paper is arranged as follows. In Sect. 2 the Radon transform is studied
and subsequently the full invariance is derived. Section 3 discusses implementation choices and the experimental results for Yale [1] dataset. Finally, Sect. 4
concludes the paper.

2

Method Details

The proposed method consists of two main steps. First, the Radon transform of
the image is calculated. Then, the result of the transform is processed to achieve
full similarity invariance, using direct translation, angle representation and 1-D
Fourier transform.
2.1

Radon Transform

The behaviour of the Radon transform for translated, rotated and scaled images
is presented in this section. The result of the Radon transform RAD of the image
f : IR × IR → [0, 1] is a function g : IR × [0, 2π) → IR+ deﬁned as:
g (s, θ) = RAD (f (x, y)) =

∞
−∞

f (s cos θ − u sin θ, s sin θ + u cos θ) du ,

(1)

where:
s
cos θ sin θ
=
u
− sin θ cos θ

x
y

.

(2)

Given the original image f in Cartesian (f (x, y)) and polar (fpolar (r, φ))
coordinates, the following images and their Radon transforms can be deﬁned [7]:
– translated image: RAD (f (x − x0 , y − y0 )) = g (s − x0 cos θ − y0 sin θ, θ),
– rotated image: RAD (fpolar (r, φ + θ0 )) = g (s, (θ + θ0 ) mod 2π),
1
– scaled image: RAD (f (αx, αy)) = |α|
g (αs, θ).
The translation in any direction in spatial domain leads to translation in the
s direction in the Radon domain, with the translation value varying with the θ
dimension. The rotation in spatial domain leads to circular translation along the
θ axis in the Radon domain. Finally, the scaling along both axes in the spatial
domain results in the scaling along the s axis in the Radon domain and scaling
of the value of the transform. These properties are depicted in the Fig. 1.
2.2

Achieving Full Similarity-Transform Invariance

The result of the Radon transform is further processed to obtain translation,
rotation and scale invariance. To simplify the notation, the g (s) will denote the
values of g (s, θ) for any speciﬁed θ in places where it will not lead to confusion.

On New Radon-Based Translation

11

Original images

Radon - transformed images

Fig. 1. Eﬀects of the Radon transform for diﬀerent image transformations

Translation Invariance. To achieve the translation invariance, the result g
of the Radon transform is directly translated along the s axis. The value of
the translation sg (θ) depends on the θ dimension. The resulting function gt is
deﬁned as follows:
gt (s) = g (s + sg ) , where

sg (θ) = min {s : g (s, θ) > 0} .

(3)

The function gt is translation invariant and preserves the scale variance.
Theorem 1. Let g (s) and G (αs + S) be two functions that are translated and
scaled version of each other, i.e. αg (s) = G (αs + S). The functions gt and Gt ,
as deﬁned in (3), are scaled version of each other.
The proof of the theorem is straightforward and is omitted for brevity.

Scale Invariance. Scale invariance is based on the method that can be called
angle-representation. The process of achieving translation and scale invariance
is depicted in Fig. 2.

a)

b)

g
0

G

gt
1s

0

d)

c)

Gt
g0
1s

0

h H

H

h

g ts=g ts
1x 0

g0

Fig. 2. Achieving invariance to translation: a, b and scale: b, c, d

p/2 g

12

T. Arod´z

In the method, a group of functions deﬁned on the basis of the function gt
is used. Let:
grev (s) = gt (1 − s) ,
hrev (x) =

x dgrev
ds
0

where s ∈ [0, 1] ;

(4)

ds, where x ∈ [0, 1] ;

(5)

h (x) = hrev (1 − x)

.

(6)

It can be shown that these functions have the following properties.
Theorem 2. If one assumes that function gt (s) has continuous ﬁrst derivative
in [0, 1] then the function h (x) is well deﬁned, diﬀerentiable, nonincreasing and
nonnegative in [0, 1]. Also, h (1) = 0 and h (0) > 0.
The function h can be used to derive scale invariant transforms. First, let
hP be a variant of the function h narrowed to the domain (0, X], where X =
h
h
be a function gts
: 0, π2 → IR+ deﬁned as:
min ({x : h (x) = 0}). Let gts
h
gts
(γ) = |hP (x)| ,

where γ = arctan

hP (x)
x

.

(7)

h
It can be shown the function gts
is scale-invariant.

Theorem 3. Let Gt (y) and gt (x) be two functions with the translation removed
and meeting the constraints of Theorem 2, such that Gt (y) = αgt (x) and y =
h
αx, i.e two functions that are scaled versions of each other. The function gts
is
h
H
scale invariant, i.e. gts (γ) = gts (γ), where the functions h and H are deﬁned
on the basis of gt and Gt according to (6).
Proof. Since the function hP is derived from gt with only integration, diﬀerentiation, reorientation of the axis and narrowing of the domain, the functions hP
and HP are also scaled versions of each other, i.e. HP (y) = αhP (x). Thus:
γH = arctan

HP (y)
y

H
gts
(γ) =

= arctan

αhP (x)
αx

= arctan

hP (x)
x

= γh ,

dH
dH (αx)
1 dαh (x)
dh
h
(γ) .
=
=
=
= gts
dy
dαx
α dx
dx

(8)

(9)

h
Therefore, gts
is indeed scale invariant.
h
For clarity, the gts
(γ), or simply gts , have been derived for 1-D function
gt (s) = gt (s, θ = const), but in fact it is deﬁned in 2-D: gts (γ, θ) as gt is.

Rotation Invariance. In order to eliminate the rotation variance, modulus of
the one-dimensional discrete Fourier transform is used along the θ axis.

On New Radon-Based Translation

13

It can be shown [11], that for the discrete function f : [0, X] → R the magnitude of the Fourier transform is invariant with respect to circular translation:
|DF T (f (x))| = |DF T (f ((x + x0 ) mod X))| .

(10)

The rotation in the image is reduced to the circular translation in the θ direction
by the Radon transform and is preserved in this form by translation and scale
variance elimination, both operating along the s axis. Therefore, the function:
Gtsr (γ, Θ) = |DF Tγ (Gts (γ = const, θ))| .

(11)

where Gts is a discrete approximation of gts , is translation, scale and rotation
invariant.

3

Experimental Results and Discussion

The developed invariant transform can be applied to face recognition, either as
a pre-processing for methods that are not invariant or as a stand-alone method.
In the latter case, the transform Gtsr of input image has to be compared
with a set of labelled, transformed sample images. This can be done using the
nearest-neighbour decision rule, i.e. the person minimizing the distance is chosen.
Several metrics for transformed images has been evaluated, i.e. the Euclidean
and Manhattan distance and the Tanimoto dissimilarity measure, deﬁned as the
inverse of the Tanimoto similarity measure [9].
It should be noted that apart from spatial variance, other variances, such as
illumination, exist in face images. The issue of eliminating the eﬀects of diﬀerent lighting in images is beyond the scope of this paper, but simple histogram
equalization is used as a pre-processing step to compensate for minor lighting
variances. Also, simple wavelet-based method is used in some experiments.
The experiments were conducted on the Yale faces dataset [1], consisting of 11
pictures for each of 15 individuals. These images have relatively high resolution
and consist of the whole head and neck of a person (see Fig. 3).

Fig. 3. Faces from the Yale dataset [1]

14

T. Arod´z
Table 1. Results for Yale dataset
3-NN, leave-one-out, 9 imgs/person 8 test and 1 sample imgs/person
Tanimoto Euclidean Manhattan Tanimoto Euclidean Manhattan
0.892
0.892
256 × 256 0.941
0.933
0.919
0.909
0.963
0.900
0.867
0.850
64 × 64 0.963
0.956
0.933
0.933
0.775
0.758
0.775
32 × 32 0.919

3.1

Experimental Setup

Two pictures for each person from the Yale dataset, containing faces with side
illumination has been eliminated from the tests. Two experimental conﬁgurations
were used. In the ﬁrst one the ”leave-one-out” cross-validation was used to obtain
the recognition accuracy. In the second conﬁguration the faces were split into
two groups, the sample images database containing 1 image per person and
a test set containing remaining 8 images of each person. The picture labelled
”normal” in the Yale set, was chosen as a sample image. In both conﬁgurations,
the nearest-neighbour decision rule was used.
The images consist of a person’s head on black background. All heads in the
test and sample sets were randomly rotated in the range of − π2 , π2 , scaled in
the range of [66%, 100%] and placed randomly in the picture.
The images, after histogram
equalization, were transformed using the Radon
√
transform to obtain 256 2 × 256 images. Since the data points in the angle
representation after the application of scale-invariance transform are not regularly spaced along the γ axis, piecewise cubic Hermite interpolation was used
to obtain regular grid of the size 256 × 256 pixels. Finally, modulus of the FFT
along the Θ dimension was calculated.
3.2

Discussion of Results

The results of the tests for the Yale images are summarized in the Table 1. Since
the calculation of the Radon transform of the 256×256 image is time consuming,
the method has been applied to images downsized to the size 64×64 and 32×32.
While in the nearest-neighbour scenario the reduction of size slightly increases
the performance of the recognition, in the scenario with single sample image per
person a decrease in accuracy can be observed, especially for 32×32 images. This
decrease is caused by the diﬀerence in the scale among the images. The relation
of the decrease in accuracy to the variations in scale between the faces can be
observed with results for 32 × 32 images randomly rotated and translated, but
with no random scaling introduced. For such pictures, the decrease of recognition
accuracy is not present, as the recognition accuracy ranges from 0.88 to 0.9
depending on the metric used in the test.
Unlike the translation and rotation, the method for achieving scale invariance
utilizes a transformation from the spatial domain to the angle domain. As the
size of the image becomes small, the discrete nature of the image becomes more

On New Radon-Based Translation

15

evident. First, the Radon transform becomes less accurate, which aﬀects also the
rotation invariance. Next, the transformation from the Radon spatial domain to
the angle domain becomes less stable, e.g. the γ parameter in (7) cannot be
determined with good accuracy. Finally, the quality of the interpolation used to
change the irregular points along the γ axis into a regular grid deteriorates. While
the scale invariance method is used also during the tests with no random scaling,
since the scaling in all images is the same, the inaccuracies introduced by the
transformation become systematic and do not inﬂuence the results signiﬁcantly.
A similar eﬀect can be observed in the nearest-neighbour scenario. Since the
number of images that an image is compared with is large, there always exist an
image with similar scale of face, for which the errors of transformation are of the
same nature as for the tested image. Thus, the recognition rate is not decreased.
Finally, to allow for comparison with other methods, the results for the full
Yale set, including two side-illuminated images per person were computed. To
deal with the large illumination variance, a wavelet-based method for compensation of illumination eﬀects was used in addition to histogram equalization. This
method is labelled HEQ-WV, while the normal histogram equalization is labelled HEQ-64. The results for the images using the nearest-neighbour method are
presented in Table 2.
Table 2. Results for 3-NN,leave-one-out, 11 images per person, Yale dataset

HEQ-64
HEQ-WV

Tanimoto Euclidean Manhattan
0.830
0.824
0.818
0.891
0.885
0.879

The results of the tests summarized above allow for some insight into the optimal conﬁguration of the method. Usually the Tanimoto and Euclidean metrics
allow for better recognition accuracy than the Manhattan metric. In case of large
illumination variance, the wavelet based method for illumination compensation
outperforms the histogram equalization. In case the method is used with a large
set of sample images, as in the ”leave-one-out” method, the optimal input image
size is 64 × 64. In case of small samples set size, larger input images yield better
results.
3.3

Comparison with Other Methods

The proposed method operating on randomly translated, rotated and scaled
faces has been compared with results for other methods cited after [6]. Nearestneighbour method with ”leave-one-out” cross-validation technique has been used
for the measurement of the recognition accuracy. Two representative methods
were used: Eigenface (PCA) and Fisherface. The results are summarized in Table
3. It should be noted that, since the proposed method does not claim to be
illumination invariant, the tests of the method were conducted on a test set

16

T. Arod´z

with the two images per person, containing right- and left-side illumination,
eliminated. Results of the best performing variant of the method, i.e. images of
size 64×64 and Tanimoto metric were used. Additionally, for ease of comparison,
the results for the whole set, including the two aforementioned images are also
presented for the images of size 256 × 256 and the Tanimoto metric. The results
for the benchmark methods are for full Yale dataset, but contrary to the tests
of the presented method, the images are not translated, rotated nor scaled.
Table 3. Comparison with other methods using Yale dataset
Method Recognition accuracy
Proposed method (no side-illuminated images)
96%
Proposed method
89%
Eigenface [6]
81%
Eigenface w/o 1st three components[6]
89%
94%
Fisherface [6]

4

Conclusions

The new Radon-based face recognition method has been proposed. It has the
property of invariance with respect to spatial image translation, on-the-plane
rotation and scaling. While allowing for recognition of faces pictured in diﬀerent
positions, rotation and scale, the proposed method gives results comparable or
event better than some existing, established non-invariant methods. The proposed transformation can be used as a stand-alone method for face recognition
or as a preliminary step providing an invariant feature set for some other noninvariant methods.
Acknowledgements. The author would like to thank prof. Witold Dzwinel for
guidance. The author is also grateful to Mr. Marcin Kurdziel for his remarks.

References
1. Belhumeur, P.N., Hespanha, J.P., Kriegman, D.J., 1997. Eigenfaces vs. Fisherfaces:
recognition using class speciﬁc linear projection. IEEE Trans. Pattern Anal. Mach.
Intell. 19(7) pp. 711-720
2. Brandt, R.D., Lin, F., 1996. Representations that uniquely characterize images
modulo translation, rotation and scaling. Pattern Recognition Letters 17 pp. 10011015
3. Hu, M.K., 1962. Visual pattern recognition by moment invariants. IEEE Trans.
Inform. Theory, vol. IT-8, pp. 179-187

On New Radon-Based Translation

17

4. Khalil, M.I., Bayoumi, M.M., 2002. Aﬃne invariants for object recognition using
the wavelet transform. Pattern Recognition Letters 23 pp. 57-72
5. Kim, H-K., Kim, J-D., 2000. Region-based shape descriptor invariant to rotation,
scale and translation. Signal Processing: Image Communication 16 pp. 87-93
6. Lai, J.H., Yuen, P.C., Feng, G.C., 2001. Face recognition using holistic Fourier
invariant features. Pattern Recognition 34 pp. 95-109
7. Shao, Y., Celenk, M., 2001. Higher-order spectra (HOS) invariants for shape recognition. Pattern Recognition 34 pp. 2097-2113
8. Sujan, V.A., Mulqueen, M.P., 2002. Fingerprint identiﬁcation using space invariant
transforms. Pattern Recognition Letters 23 pp. 609-919
9. Theodoridis, S., Koutroumbas, K., 1999. Pattern Recognition. Academic Press,
10. de Vel, O., Aeberhard, S., 2000. Object recognition using random image-lines.
Image and Vision Computing 18 pp. 193-198
11. Wood, J., 1996, Invariant pattern recognition: A review. Pattern Recognition, Vol
29. No. 1, pp. 1-17

