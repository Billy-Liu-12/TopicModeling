Non-negative Matrix Factorization for Filtering
Chinese Document*
Jianjiang Lu
1

1,2,3

1,2

1

, Baowen Xu , Jixiang Jiang , and Dazhou Kang

1

Department of Computer Science and Engineering, Southeast University,
Nanjing, 210096, China
2
Jiangsu Institute of Software Quality, Nanjing, 210096, China
3
PLA University of Science and Technology, Nanjing, 210007, China
jjlu@seu.edu.cn

Abstract. There are two nasty classical problems of synonymy and polysemy in
the filtering systems of Chinese documents. To deal with these two problems,
we would ideally like to represent documents not by words, but by the semantic
relations between words. Non-negative matrix factorization (NMF) is applied to
dimensionality reduction of the words space. NMF is distinguished from the
latent semantic indexing (LSI) by its non-negativity constraints. These
constraints lead to a parts-based representation because they allow only
additive, not subtractive, combinations. Also, NMF computation is based on the
simple iterative algorithm; it is therefore advantageous for applications
involving large sparse matrices. The experimental results show that, comparing
with LSI, NMF method not only improves filtering precision markedly, but also
has the merits of fast computing speed and less memory occupancy.

1

Introduction

Automatic filtering of information from document sources has become increasingly
important in recent years. Information filtering systems are designed to shift through
large quantities of dynamically generated documents and display only those which
may be relevant to a user’s interests [1]. Two major types of filtering systems have
been proposed: content-based filtering [2] and collaborative filtering. Collaborative
filtering selects documents based on user’s evaluations of the documents. On the other
hand, content-based filtering selects documents based on the contents of documents
and each user’s preference.
There exist several types of content-based filtering systems. In the vector space
model [3], user profiles and document profiles are represented as weighted vectors of
the words in the system. The relevance of each document to each user is calculated
according to the similarity between the user profile vector and the document profile
vector. There are two nasty classical problems of synonymy and polysemy in the
vector space model. To deal with these two and other similar problems, we would
*

This work was supported in part by the Young Scientist's Fund of NSFC (60373066,
60303024), National Grand Fundamental Research 973 Program of China (2002CB312000),
National Research Foundation for the Doctoral Program of Higher Education of China.

M. Bubak et al. (Eds.): ICCS 2004, LNCS 3037, pp. 113–120, 2004.
© Springer-Verlag Berlin Heidelberg 2004

114

J. Lu et al.

ideally like to represent documents not by words, but by the semantic relations
between words.
Latent semantic indexing (LSI) analysis based on singular-value decomposition
(SVD) [4, 5] is an information retrieval method that attempts to capture the semantic
relations by using techniques from linear algebra. LSI constructs a low-dimensional
semantic space wherein words and documents that are closely associated are placed
near one another. SVD allows the arrangement of the space to reflect the major
associative patterns in the data, and ignore the smaller, less important influences.
However, the cost of SVD computation will be prohibitive when matrices become
large. In addition, SVD is lack of intuitive notion.
In this paper, a method based on non-negative matrix factorization (NMF) [6, 7]
for constructing Chinese user profile is presented. This method proposes to apply
NMF to dimensionality reduction of the document vectors. NMF can decompose a
non-negative matrix into two non-negative matrices. One of the decomposed matrices
can be regarded as the basis vectors. The dimensionality reduction can be performed
by projecting the document vectors onto the lower dimensional space which is formed
by these basis vectors. NMF is distinguished from LSI by its non-negativity
constraints. These constraints lead to a parts-based representation because they allow
only additive, not subtractive, combinations. Also, NMF computation is based on the
simple iterative algorithm; it is therefore advantageous for applications involving
large sparse matrices.
The remainder of this paper is organized as follows. In section 2, we briefly review
how to represent a set of unstructured Chinese documents as a vector space model. In
section 3, we introduce non-negative matrix factorization. In section 4, a NMF
method for constructing Chinese user profile is presented. In section 5, the
experimental results of NMF method are compared with LSI. Finally, section 6 gives
the conclusions.

2

Vector Space Models for Documents

Let D1 = {d11 , d 21 ," , d n11 } be a set of Chinese topic documents, let D2 = {d12 , d 22 ," , d n22 } be
a set of non-topic documents, n1 + n2 = n , D = D1 ∪ D2 . We briefly review how to
represent a set of unstructured Chinese documents as a vector space model. The preprocessing is as following.
(1) Chinese documents are written as characters strings with no spaces between
words, so we first use word segmentation algorithm [8] to segment the Chinese
documents.
(2) After word segmentation, we eliminate non-content-bearing “stopwords”.
(3) Using heuristic or information-theoretic criteria, eliminate non-content-bearing
“high-frequency” and “low-frequency” words. Such words and the stopwords are both
known as “function” words. Eliminating function words removes little or no information, while speeding up the computation.
(4) After above elimination, suppose m unique words remain, let be
T = {t1 , t 2 , " , t m } . We use normalized word frequency-inverse document frequency

Non-negative Matrix Factorization for Filtering Chinese Document

115

scheme [9] to obtain word-document matrix X = ( x ij ) m×n . The i element x ij of the
th

document vector x j = ( x1 j , x 2 j , " , x mj ) T is given by
x ij = # ( x j , t i ) log

n
hi

where #(xj, ti) denotes the number that the word ti appears in the document xj, hi
denotes the number of the documents in which the word ti appears, n is the total
document number. Document vectors are usually normalized to a unit vector, that is,
xij =

xij

∑i =1 xij2
m

, i = 1,2," , m

Intuitively, the effect of normalization is to retain only the direction of the
document vectors. This ensures that documents dealing with the same subject matter
(that is, using similar words), but differing in length lead to similar document vectors.
After the preprocessing, Chinese documents are represented as m dimensional
document vectors x j , j = 1,2, " , n . Let X 1 = {x11 , x 12 , " , x 1n1 } be topic document vectors,

{

X 2 = v12 , v 22 , " , v n22

}

be non-topic document vectors, X = X 1 ∪ X 2 . These document
vectors make up of word-document matrix X = ( x ij ) m×n .

3

Non-negative Matrix Factorization

Given a non-negative matrix X = ( x ij ) m×n , NMF finds the non-negative m × r matrix
U = (u ij ) m×r and the non-negative r × n matrix V = (vij ) r×n such that
X ≈ UV

(1)

The r is generally chosen to satisfy (n + m ) r < nm , so that the product UV can be
regarded as a compressed form of the data in X.
The equation (1) can be rewritten column by columns as
x ≈ Uv

(2)

where x and v are the corresponding columns of X and V. Each vector x is
approximated by a linear combination of the columns of U, weighted by the
components of v. Therefore, U can be regarded as containing a basis vector that is
optimized for the linear approximation of the vector in X. Since relatively few basis
vectors are used to represent many vectors, good approximation can only be achieved
if the basis vectors discover structure that is latent in the vectors.
Here, we introduce an algorithm based on iterative estimation of U and V. At each
iteration of the algorithm, the new value of U and V is found by multiplying the
current value by some factor that depends on the quality of the approximation in
equation (1). Repeated iteration of the update rules is guaranteed to converge to a
locally optimal matrix factorization.
The update rules given in the next equations [7].

116

J. Lu et al.

x ki
y kj

(3)

xik
v jk
yik

(4)

v ij ← v ij ∑ u ki
k

u ij ← u ij ∑
k

u ij ←

(5)

u ij

∑ u kj
k

where U and V are initial stochastic matrices.
The update rules maximize the following objective function:


xij
− xij + y ij  + α ∑ aij − β ∑ bii
F ( X , Y ) = ∑  xij log


y ij
i, j 
i, j
i


(6)

where aij is the components of U T U , bii is the diagonal components of
VV T , α , β > 0 are some constants, Y = UV = ( y ij ) m×n .

4

Constructing Chinese User Profile

After word-document matrix X is decomposed by the NMF in section 3, the
m dimensional document vectors are projected into the r dimensional vectors. Let
V1 = {v11 , v12 ," , v1n1 } be projecting of the topic document vectors, V2 = {v12 , v 22 ," , v n22 } be
projecting of the non-topic document vectors. We can compute the mean vector of
topic document vectors.
O = (o1 , o 2 , " , o m ) =

1
n1

n1

∑ x 1j
j =1

In the same way, we can compute the mean vector of projecting of the topic
document vectors.
O 1 = (o11 , o 12 , " , o 1r ) =

1
n1

n1

∑ v1j
j =1

According to equation (2), we can easily obtain.
O ≈ UO 1 = u1 o11 + u 2 o 12 + " + u r o 1r

(7)

A simply way is to select UO1 as a user profile, but this user profile may be
ineffective. Next, we define the class discriminative degree of the basis vectors in
order to obtain an effective user profile.

Non-negative Matrix Factorization for Filtering Chinese Document

Definition1. Let v 1j = (v11 j , v 12 j , " , v 1rj ) T ,

j = 1,2, ", n1 ,

117

v 2j = (v12j , v 22 j , " , v rj2 ) T ,

j = 1,2,", n2 Class discriminative degree of the basis vector u s to topic documents is
defined as follows:
ds =

If the average weight
and the average weight

1
n1
1
n2

n1

1
n1

n2

1
∑ v1sj − n ∑ vsj2 ,
j =1

s = 1,2," , r

(8)

2 j =1

n1

∑ v1sj of the basis vector
j =1

n2

∑ vsj2 of the basis vector
j =1

u s in the topic documents is big,
u s in the non-topic documents is

small, then the class discriminative degree ds is large. That is to say, the basis vector us
has strong discriminative ability between topic documents and the non-topic
documents. We select k basis vectors with big class discriminative degrees, simply let
be u1 , u 2 ," , u k , k ≤ r . According to equation (7), we use these k basis vectors to
construct a m dimensional vector of the words as follows.
u1o11 + u 2 o12 + " + u k o1k

(9)

Then, we sort the components of the m dimensional vector by the value, and select
l components with big values as the user profile.

User _ Pr ofile = {< t1 , g 1 >, < t 2 , g 2 >," , < t l , g l >}
where t i is a word, g i is the component value with respect to word t i , i = 1,2," , l .

5

Experimental Results

All documents in the experiment are downloaded from http://www.sina.com.cn. The
topic documents include: Chess (277), Gym (149), Badminton (110), Box (69), PingPong (177), Volleyball (171), Racing (95), Swimming (126), Tennis ball (208),
Baseball (60), Skating (155), Golf (111), Track and field (147), Billiards (58) and
Martial art (43). In addition, there have 2044 non-topic documents.
In order to test 15 topic documents, we partition each topic documents into four
groups, and select one group as testing documents, other groups as training
documents. When recall was set to 0%, 10%, 20%, 30%, …, 90%, 100%, the average
precision at 11 points of filtering systems based on NMF and LSI [5] are compared.
We select 2843 words in the experiment, let r = 200, figure 1 shows the
experimental results on the four topic documents. Figure 1(a) and figure 1(b) show
the average precisions of filtering systems based on NMF and LSI with the selecting
words respectively. In the filtering systems based on NMF, we select k basis vectors
that have strong discriminative ability between topic documents and the non-topic
documents. Figure 2 shows the average precisions of 15 topic documents.
The experimental results show that the average precision of filtering systems based
on NMF is better than LSI. In addition, the memory occupancy of NMF is lesser than
LSI. For example, the memory occupancy of the left singular matrix in LSI is

118

J. Lu et al.

126.3M, and the memory occupancy of the right singular matrix is 33.0M. Whereas
the memory occupancy of the left non-negative matrix in NMF is 6.9M, and the
memory occupancy of the right non-negative matrix is 6.7M. Furthermore, NMF only
needs 30-40 iterative times, so NMF costs less computation time than LSI. In the
experiment, using the SVDPACK from http://www.netlib.org/svdpack, NMF only
costs about half time of LSI.

(a) LSI (r=200)

(b)

NMF (r=200)

Fig. 1. Comparing the precision

Non-negative Matrix Factorization for Filtering Chinese Document

119

Fig. 2. Comparing average precision

6

Conclusion

Automatic filtering of information from document sources has become increasingly
important as the volume of electronically accessible documents has exploded in recent
years. In this paper, a method based on NMF for constructing Chinese user profile is
presented. This method proposes to apply NMF to dimensionality reduction of the
document vectors in the word-document matrices. NMF decomposes a non-negative
matrix into two non-negative matrices. One of the decomposed matrices can be
regarded as the basis vectors. The dimensionality reduction can be performed by
projecting the document vectors onto the lower dimensional space which is formed by
these basis vectors. NMF is distinguished from LSI by its non-negativity constraints.
These constraints lead to a parts-based representation because they allow only
additive, not subtractive, combinations. Also, NMF computation is based on the
simple iterative algorithm, it is therefore advantageous for applications involving
large sparse matrices. The experimental results show that, comparing with LSI, NMF
method not only improves filtering precision markedly, but also has the merits of fast
computing speed and less memory occupancy. In the future work, we will discuss
how to use NMF in the Chinese document clustering and classification.

120

J. Lu et al.

References
1.
2.
3.
4.
5.
6.
7.
8.
9.

Belkin, N.J., Croft, W. B.: Information Filtering and Information Retrieval: two sides of
the same coin. Communication of ACM, 35(12). (1992) 29-38
Chen, L., Katia, S.: WebMate: A Personal Agent for Browsing and Searching. ACM
AGENTS’98, Proceedings of the International Conference on Autonomous Agents,
Minneapolis (1998) 132-139
Yart, T. W., Garcia-Molina, H.: Index Structures for Information Filtering under the
Vector Space Model. Proceedings of the 10th International Conference on Data
Engineering, Alamitos, CA, IEEE (1994) 337-347
Papadimitriou, C. H., Raghavan, P., Tamaki, H.: Latent Semantic Indexing: A
Probabilistic Analysis. Proceedings of PODS’98, Seattle, WA (1998)159-168
Lu, Z., Lu, H., Li, Y.: FDS Expressive Method in Information Filtering. Journal of
Tsinghua University (science and technology), 39(9). (1999)118-121
Lee, D. D., Seung, H. S.: Learning the Parts of Objects by Non-negative Matrix
Factorization. Nature, 401. (1999) 788-791
Li, S. Z., Hou, X. W., Zhang, H. J.: Learning Spatially Localized Parts-based
Representation. Proceedings of IEEE International Conference on Computer Vision and
Pattern Recognition, Hawaii (2001) 207 –212
Chen, G. L., Wang, Y. C., HAN, K. S., Wang, G.: An Improved Fast Algorithm for
Chinese Word Segmentation. Journal of Computer Research and Development, 37(4).
(2000) 418-424
Kolda, T. G.: Limited-Memory Matrix Methods with Applications. Ph.D. thesis, The
Applied Mathematics Program, University of Maryland, College Park, Mayland (1977)

