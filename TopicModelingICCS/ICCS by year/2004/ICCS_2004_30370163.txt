Load Balancing Issues for a Multiple Front
Method
Christophe Denis1 , Jean-Paul Bouﬄet1 , Piotr Breitkopf2 , Michel Vayssade2 ,
and Barbara Glut3∗
1

Department of Computing Engineering, UMR 6599 Heudiasyc,
Compi`egne University of Technology, BP 20529
F-60205 Compi`egne cedex, France
2
Department of Mechanical Engineering, UMR 6066 Roberval
Compi`egne University of Technology, BP 20529
F-60205 Compi`egne cedex, France
{Christophe.Denis,Jean-Paul.Boufflet,
Piotr.Breitkopf,Michel.Vayssade}@utc.fr
3
Institute of Computer Science
AGH University of Science and Technology
Cracow, Poland

Abstract. We investigate a load balancing strategy that uses a model
of the computational behavior of a parallel solver to correct an initial
partition of data.

1

Introduction

We deal with linear systems K · u = f issued from ﬁnite elements. The frontal
approach interleaves assembly and elimination avoiding to directly manage the
entire matrix K. A variable is eliminated when its corresponding equation is
fully summed (I. Duﬀ et al [1,2]). Rather than parallelize an existing code (P.R.
Amestoy et al [3]), one can perform tasks in independent modules like in J.
Scott [4] MP42 solver based on the frontal code by I. Duﬀ and J. Reid [2]. We
use an implementation of a multiple front parallel method in the context of our
academic software SIC [5,6].
The domain is partitioned using M ET IS [7] and CHACO [8]. This initial
partition tends to minimize the communications and to balance the subdomain
amount of data assuming that the computation cost is proportional to the number of vertices of the subgraph and that the order of assembly does not matter.
[9] seems to conﬁrm the analysis presented by B. Hendrickson [10,11]:
equipartitioning of data volumes does not result systematically in well balanced
computational times.
We design a load balancing process transferring ﬁnite elements between subdomains to improve the initial partition. Test data are from the PARASOL
project (http://www.parallab.uib.no/parasol).

M. Bubak et al. (Eds.): ICCS 2004, LNCS 3037, pp. 163–170, 2004.
c Springer-Verlag Berlin Heidelberg 2004

164

2

C. Denis et al.

Problem Formulation

We use a non-overlapping domain decomposition:
1. the graph Gelem associated with the ﬁnite element mesh is partitioned into
Ns subdomains SD(j) ;
2. each SD(j) is partially condensed in parallel;
3. an interface problem is built and then treated.
This process for an equivalent assembled matrix K can be block ordered:
 (1)

(1)
Kii
Kib


..
..


.
.




(N )
(N )
Kib s
Kii s


(1)
(N )
(j)
Ns
Kbi . . . Kbi s Kbb = j=1
Kbb
(j)

Subscript i indicates “internal” and b “boundary”. Kii are the terms of
(j)
K associated with the internal variables of SD(j) . The terms of Kbi (resp.
(j)
Kib ) correspond to the interactions between internal variables of SD(j) and the
boundary ones. For each SD(j) , we build the following matrix and a partial LU
condensation
 
 


(j)
(j)
(j)
(j)
(j)
Uii Uib
Lii 0
Kii Kib
 (j) (j)   (j)  
(j) 
(1)

 Kbi Kbb  =  Lbi I  ·  0 Sbb
(j)

The block Sbb denotes the local Schur complement of SD(j) .
Ns

S=
j=1

(j)
Sbb

Ns

=
j=1

(j)

(j)

(j)

Kbb − (Lbi · Uib )

(2)

Using (1) we get the global Schur complement matrix :
Ns

S=
j=1

(j)

(j)

(j)

(j)

Kbb − Kbi · (Kii )−1 · Kib

(3)

We use a frontal method to partially condense the matrices associated with
each SD(j) and to treat the interface problem. The nested multiple front ap(j)
(j)
(j)
(j)
proach is based on the treatment of groups of (Kbb − Kbi · (Kii )−1 · Kib ).
(j)
Matrices Sbb can be viewed as a super-elements that can be assembled in a
frontal matrix and partially condensed. The computational scheme we consider
is a tree of tasks (1).
Deﬁnition 1. A computation tree ANs ,y has Ns leaves and y levels.

Load Balancing Issues for a Multiple Front Method
SD (2) SD (3)

SD (1)

165

SD (4)

L(1)
cond

cond

cond

v

v

w

S bb

cond

S bb

L(2)
u

inter

K inter

inter

u

u

S bb

L(3)
inter

w

max q(u)

+
max q(u)

+
max q(u)

=Q
Fig. 1. The computation tree A4,3 and the principle of the estimation of the computation

A task u is associated with a vertex u of the computation tree and q(u) is
an estimation of the number of operations. Let L(i) be the set of vertices of
ANs ,y at level i. The leaves in L(1) correspond to the partial condensations of
the SD(j) . For a task u associated with an internal vertex of ANs ,y we deﬁne:
u
u
of
– Sbb
obtained by partial condensation on SD(j) or on the assembly Kinter
(i)
two matrices Sbb ;
(i)
u
– Kinter
the interface matrix obtained by assembling two matrices Sbb .

On the computation tree A4,3 of Fig. (1), subdomains SD(3) and SD(4) are
v
w
partially condensed by tasks v and w. We obtain two matrices Sbb
and Sbb
. They
u
are assembled in the interface matrix Kinter . The boundary variables between
subdomains SD(3) and SD(4) correspond to fully summed rows and columns of
u
u
u
Kinter
. We obtain Sbb
by partially condensing Kinter
. The interface problem of
level L(3) is then solved and individual variables are obtained by successive back
substitutions.
We use a coarse grain parallel approach where tasks are the partial condensation and interface problems. The communication times and the back restitution
times are negligible.
The goal is to correct an initial partition of the graph Gelem . An estimator
of the number of operations of the frontal method is applied on SD(j) :
Q1 (Ve(j) , SD(j) ) =
γ∈assembling
(j)

|Fγ |2

|Fγ | +

(4)

γ∈elimination

where Ve is the reordering vector of the ﬁnite elements of SD(j) . Q1 [12] counts
operations and gives 10% error between the estimated time for SD(j) and the
actual time TSD(j) .

166

C. Denis et al.

The second estimator counts the number of operations for the partial conu
.
densation of Kinter
We evaluate then the maximum number of operations max q(u) for each level
L(i) as shown in Fig. (1). The sum Q =
max q(u) provides an estimation of
the cost. In an ideal case of equal tasks at each level, Q is a tight estimation,
otherwise it gives an upper bound.
We consider balanced trees obtained with multi-level tools [7]. First a unique
(i)
task of L(1) is assigned per processor. Then, Sbb are sent to processors computing the associated tasks according to the computation tree.
Table 1. The PARASOL data and the ﬁnite element meshes used for our experiments
name
name
No. elts order
MT1
5 328 97 578 SUSPEN D1
C1
SHIPSEC8 35 280 114 919
X104
6 019 108 384 MISSILE4

3

No.
18
42
27

elts order
171 14 517
689 34 707
804 166 824

Principle of the Heuristics

The initial partition P is ﬁrst computed using [7]. Then we apply the following
heuristics:
(j)

(j)

1. for each subdomain SD(j) compute ﬁrst a Ve , then Q1 (Ve , SD(j) ) ;
2. select SD(max) with maximum estimated number of operations;
3. determine the set Nmax of indices of subdomains that are neighbors to
SD(max) ;
4. virtually aggregate the subdomains of Nmax ;
max
5. compute QN
moy the average number of operation of these subdomains;
6. compute Qtrans the number of operations to be transferred from SD(max) ;
7. compute mt the number of elements to be transferred;
8. transfer a subset of mt ﬁnite elements from SD(max) to the virtual subdomain.
The volume Qtrans is half the diﬀerence between the maximum estimated
max
number of operations and QN
and mt is ratio Qtrans over the number of
moy
operations per element. By applying this process k times we improve the initial
partition. For our experiments we set k = 100 and select the best result.
A transfer primitive chooses ﬁnite elements near the common boundary in
order to limit the growth of the interface. Consider examples from Fig. (2) to
Fig. (4).
In Fig. (2) SD(1) has the maximum estimated number of operations. Grey
elements are near the boundary between SD(1) and the virtual subdomain
[SD(2) ,SD(3) ] ( N1 = {2, 3}).

Load Balancing Issues for a Multiple Front Method

167

Table 2. Q the estimated amount of computation, Tglob the real computing time (in
s), the obtained gain (in %), and ∆mesu the load balancing criterion
name
MT1

tree
A
A
B
B
C
C
SHIPSEC8 A
A
B
B
C
C
X104
A
A
B
B
C
C
SUSPEN D1 A
A
B
B
C
C
A
C1
A
B
B
C
C
MISSILE4 A
A
B
B
C
C

method
Pmetis
PmetisC
Pmetis
PmetisC
Pmetis
PmetisC
Pmetis
PmetisC
Pmetis
PmetisC
Pmetis
PmetisC
Pmetis
PmetisC
Pmetis
PmetisC
Pmetis
PmetisC
Pmetis
PmetisC
Pmetis
PmetisC
Pmetis
PmetisC
Pmetis
PmetisC
Pmetis
PmetisC
Pmetis
PmetisC
Pmetis
PmetisC
Pmetis
PmetisC
Pmetis
PmetisC

Q
1, 17.1011
1, 00.1011
1, 09.1011
8, 05.1010
4, 26.1010
3, 15.1010
4, 02.1011
3, 85.1011
2, 75.1011
1, 75.1011
1, 40.1011
1, 20.1011
2, 01.1011
2, 01.1011
7, 84.1010
6, 39.1010
4, 53.1010
4, 48.1010
2, 67.109
1, 19.109
1, 20.109
9, 28.108
7, 75.108
4, 05.108
1, 78.1010
8, 87.109
9, 23.109
3, 38.109
3, 11.109
1, 74.109
1, 44.1011
3, 91.1010
1, 04.1011
4, 81.1010
6, 13.1010
2, 26.1010

Tglob (s) gain (%) ∆mesu (%)
94,4
96
98
89,6
5,1
66,7
71
80
59.1
11,4
54
43,0
36,9
14,2
72
266,6
81
239,1
10,3
89
160,5
71
112,7
29,8
89
71
108,3
101,1
6,6
79
98
162,9
98
162,9
0
70,1
76
57,4
18.1
91
48,5
46
48
46,8
3.5
21, 1
88
9, 9
53,1
73
50
9, 9
4, 1
58, 6
80
6, 7
44
43, 8
73
3, 8
80
129, 9
35, 2
88
84, 2
99, 4
54
30, 1
69, 7
90
54
24, 7
90
37, 7
15, 4
71
1220
395, 3
67, 6
99
891, 6
65
67
406, 6
54, 4
448, 3
59
204, 5
54, 4
93

We compute a level structure through SD(1) from the boundary elements of
[SD(2) ,SD(3) ]. We apply the BF S algorithm on the element graph of SD(1) initializing its queue with boundary elements corresponding to level 0. We obtain
a spanning tree where level l contains elements at a distance of l edges to level
0. It may be seen as using a virtual vertex r connecting SD(1) and its associated virtual subdomain (Fig. (3)). We assume mt =4. We then transfer selected
elements to the neighbor subdomains (Fig. (4)).

168

C. Denis et al.
(2)
SD

(1)
SD

(4)
SD

(3)
SD
(N )
SD 1

Fig. 2. The initial partition of the domain into 4 subdomains
(2)
SD

(1)
SD

(4)
SD

r

(3)
SD

Fig. 3. Initialisation of the root r of the level structure in order to select the ﬁnite
elements to be transfered from SD(1) to the virtually aggregated subdomain
(1)
SD

(2)
SD

0110
10

(4)
SD

(3)
SD

Fig. 4. mt ﬁnite element are transfered

4

Results

The experiments were performed on a 10 Athlon 2200+, 1Gb bi-processors cluster, running LINUX Red Hat 7.1, with a 1 Gbit/s Ethernet network. Table (1)
gives the sizes of the PARASOL data, and of some arbitrary meshes. The order
column gives the size of the assembled matrix.
Three types of computation tree were used, and we deﬁne the labels: A for
A2,2 , 2 subdomains and 2 levels; B for A4,3 , 4 subdomains and 3 levels and C
for A8,4 , 8 subdomains and 4 levels.
Table (2) presents the results: estimates Q, and measures Tglob . P metis is
the original M ET IS decomposition and P metisC is the corrected one. TSD(j)
is measured for each SD(j) along with the load balancing criterion:
N

∆mesu =

s
1
j=1 TSD (j)
Ns maxj TSD(j)

In the ideal case the TSD(j) are equal and ∆mesu = 1.

Load Balancing Issues for a Multiple Front Method

169

11

4.5

x 10

300

Pmetis
PmetisC

4

Pmetis
PmetisC

250

3.5
200
Tglob in s

3
Q

2.5
2

150
100

1.5
1

50

0.5
0

0

A

B

C

Fig. 5. Q : the estimated amount of
computation before and after applying
the heuristics for the SHIPSEC8 data

A

B

C

Fig. 6. Tglob : the real computing time
(in s) before and after applying the
heuristics for the SHIPSEC8 data

Table (2) shows that ∆mesu is improved. The transfer primitive was modiﬁed
in order to limit the number of interface nodes.
Figs. (5) and (6) show a good correlation between Q and Tglob . However, we
do not obtain a perfect balance, because the estimations do not reﬂect exactly
the real computations. Moreover, moving elements inﬂuences the ordering and
consequently the computation time. It is therefore diﬃcult to attain ∆mesu = 1.
As a rule, fewer than 10 iterations of the heuristics provide the maximum gain
reported in Table (2).

5

Conclusion

We propose a heuristics to correct an initial domain decomposition based on
equal volumes of data, in order to balance the estimated number of operations
of a multiple-front method. With this coarse-grained parallel approach, the preliminary results obtained on the benchmark improve computing time. The modiﬁcation of the boundary due to the transfer of ﬁnite elements can increase the
number of interface nodes and the size of the interface problem.

References
1. Duﬀ, I., Erisman, A., Reid, J.: Direct Methods for Sparse Matrices. Monographs
on Numerical Analysis. Clarendon Press - Oxford (1986)
2. Duﬀ, I.S., Scott, J.A.: MA42 – A new frontal code for solving sparse unsymmetric
systems, technical report ral 93-064. Technical report, Chilton, Oxon, England
(1993)
3. P.R. Amestoy, I.S. Duﬀ, J.Y.L., Koster, J.: A fully asynchronous multifrontal solver
using distributed dynamic scheduling. SIAM J. Matrix Anal. Appl. 23 (2001) 15–
41
4. Scott, J.: The design of a parallel frontal solver,technical report ral-tr99-075. Technical report, Rutherford Appleton Laboratory (1999)

170

C. Denis et al.

5. Escaig, Y., Vayssade, M., Touzot, G.: Une m´ethode de d´ecomposition de domaines
multifrontale multiniveaux. Revue Europ´eenne des El´ements Finis 3 (1994) 311–
337
6. Breitkopf, P., Escaig, Y.: Object oriented approach and distributed ﬁnite element
simulations. Revue Europ´eenne des El´ements Finis 7 (1998) 609–626
7. Karypis, G., Kumar, V.: Metis : A software package for partitioning unstructured
graphs, partitioning meshes, and computing ﬁll-reducing orderings of sparse matrices. Technical report, University of Minnesota, Department of Computer Science
(1998)
8. Hendrickson, B., Leland, R.: The chaco user’s guide, version 2.0. Technical report,
Sandia National Laboratories (1995)
9. Bouﬄet, J., Breitkopf, P., Denis, C., Rassineux, A., Vayssade, M.: Optimal element numbering schemes for direct solution of mechanical problems using domain
decomposition method. In: 4th ECCOMAS Solid Mechanics Conference. (2000)
Espagne.
10. Hendrickson, B.: Graph partitioning and parallel solvers: Has the emperor no
clothes? In: Irregular’98, Lecture Notes in Computer Science. Volume 1457. (1998)
218–225
11. Hendrickson, B.: Load balancing ﬁctions, falsehoods and fallacies. Applied Mathematical Modelling 25 (2000) 99–108
12. Bouﬄet, J., Breitkopf, P., Denis, C., Rassineux, A., Vayssade, M.: Equilibrage
en volume de calcul pour un solveur parall`ele multi-niveau. In: 6`eme Colloque
National en Calcul des Structures. (2001) 349–356 Giens, France.

