Teaching Scientiﬁc Computing
B.A. Shadwick1,2
1

Institute for Advanced Physics
10875 US Hwy. 285, Suite 199, Conifer, C0 80433 USA
2
Center for Beam Physics
LBNL, Berkeley, CA 94720, USA
BAShadwick@IAPhysics.org

Abstract. We consider the wide scale use of computers in scientiﬁc research and the implications for the educational curriculum. We examine
the practice of scientiﬁc computing from the perspective of obtaining
robust numerical results. We explore the philosophy underlying this process and isolate the essential elements. We discuss in detail practices that
are designed to minimize the likelihood of errors and also survey a set
of supporting skills. Throughout, examples are used to illustrate the key
ideas.

1

Introduction

Less than a generation ago, the routine use of computers in scientiﬁc research
was largely restricted to specialists. CPU time was metered and in reasonably
tight supply for all but the largest institutions. Modern tools and the tremendous computing power1 that is routinely available to researchers has resulted in
widespread, almost casual, use of computing in the physical sciences. A greater proportion of scientists are part-time practitioners of numerical computing,
and numerous software packages make it seemingly easy to the casual user to
do elementary numerical computations: to evaluate functions for graphing or
for other purposes; to numerically (or symbolically) evaluate integrals; to solve
ODEs and simple PDEs and so forth. Regrettably many of these scientists have
little understanding of the mechanics underlying these numerical results. The
scale of problem that the non-specialist is willing to take on scales with the growth in desktop computing power. This trend began with the general availability
of desktop systems some two decades ago. It exists as a consequence of the natural human tendency to categorize; we make the (artiﬁcial) distinction between
what we do on our desktop computers and what “computational physicists” do
with supercomputers. We tend to view users of truly large-scale computing as
specialists who have the detailed knowledge to perform these grand calculations,
whereas the researcher who does the occasional computation using the a computer which is also used for email and writing papers does not need (or often even
1

Of course, a college freshman might not consider today’s computers necessarily that
remarkable. Context is important.

M. Bubak et al. (Eds.): ICCS 2004, LNCS 3039, pp. 1234–1241, 2004.
c Springer-Verlag Berlin Heidelberg 2004

Teaching Scientiﬁc Computing

1235

want) such specialized knowledge. The code that runs today in a few minutes
on a workstation would have taken hours (if not days) on the central systems
commonly in use twenty-ﬁve years ago. If we cannot distinguish specialists from
non-specialists through the kinds of problems the address, then how can we differentiate them? Of course, there is no distinction and herein lies the potential
problem.
As in society at large, scientists have come to view computers as mere appliances. Fortunately, it is also becoming more common for graduate and undergraduate programs to include courses in numerical methods. At the same time
there is still a perception (probably less so in engineering than in, say, physics)
that only students who plan to specialize in numerical work need to take such
courses.
It is increasingly common for students to take on “large” computational
projects with less-than-adequate supervision. Most faculty members would be
reluctant to suggest a student tackle an analytical problem in an area where the
advisor has little or no experience. Unfortunately, this same reticence does not
extend to suggesting a student undertake numerical work, even when neither the
advisor nor the student has signiﬁcant computational experience. While there
is something to be said for the “sink or swim” philosophy, failing to recognize
the inherent complexity and subtlety of obtaining correct computational results
does a grave disservice to students.
These observations argue that all students need more than a cursory introduction to numerical methods. Though impractical, computational methods
should ideally be viewed as a mathematical tool as important as calculus, and
receive similar weight in the curriculum. There is, for lack of a better term, a
core process (and an accompanying mindset) by which we obtain computational results that we can believe. By concentrating on this crucial core, it is still
possible to construct a course of study which the curriculum will bear.
We begin with a detailed examination of these processes, while providing
examples which illustrate the key ideas. We turn next to broad survey of some
essential skills and specialized knowledge required. By the end, we will have
identiﬁed the essential elements, and hopefully have outlined a means to impart
our process and our mindset to our students.

2

Getting the Right Answer

In this section we will examine the process by which we can obtain computational
results in which we have conﬁdence. To provide a reference, the discussion will be
geared towards the task of obtaining numerical solutions of diﬀerential equations.
Partial diﬀerential equations are ubiquitous in the physical sciences and students
are often faced with formidable PDEs from which they must wrest a numerical
solution. While the philosophy presented here is generic to scientiﬁc computing;
some of the details will be diﬀerent when considering numerical tasks other than
solving diﬀerential equations.

1236

B.A. Shadwick

Perhaps this section could be more accurately titled “Avoiding Wrong Answers” as that is actually what one does. It is quite rare for one to be able to
claim that a numerical calculation is “right,” since such a claim would imply an
independent, exact (i.e., an analytical) solution to the problem in question. If
such a solution exists, then why the recourse to numerical solutions? So what do
we meant by “the right answer”? The best we can normally accomplish is a belief
that our computer code is not wrong. That is, at best, we ﬁnd no evidence that
the result of our calculation is wrong; we have successfully performed a series of
tests — typically running the code with sets of parameters for which something
is known about the solution (more on this below) — which show that the code is
functioning as we expect. We then extrapolate this ostensively correct behaviour
to cases where we know less about (but have more interest in) the solution. This
is what we mean by our use of the term “right.”
We do not wish to be overly pessimistic about computational results, but it
is essential that we are mindful of the nature of the endeavour. In some respects,
numerical computation has more in common with experimental science (in terms
of assessing the outcome) that it does with analytical work2 . While numerical
work is “theoretical,” the required mindset more closely matches that of the
experimentalist than the theoretician. An analytical calculation always “works”;
one either ﬁnishes the calculation, or reaches a point where further progress
appears impossible. The point where diﬃculties arise is clear and the calculator
is typically aware of the step which serves as the stumbling block. An experiment,
however, has a diﬀerent character. In some cases, even though the apparatus is
functioning as the experimentalist expects, there is no signal, i.e., the experiment
doesn’t work. Even when there is a signal, the experimentalist must determine
whether this signal has the meaning intended when the experiment was designed.
The only demonstrable evidence that something is badly wrong with a numerical calculation occurs when the code crashes (i.e., the computer operating
system terminates the program due to bad behaviour) or the output contains
some quantity of NaNs. Absent these diﬃculties, some output is always obtained.
Thus we are always faced with the question “Does the output of this code mean
what we think it means?” The importance of recognizing this question is inherent
in all numerical computation and cannot be over emphasised. We are always at
risk of being misled by the output of our codes. There is a natural predisposition
to believe all of those digits before us have some intrinsic meaning. Compiler
errors and errors in the internal implementation of the machine instructions are
thankfully quite rare.3 The output of a computer program is almost certainly in
2

3

The dreadful term “numerical experiment” should not come to mind in this context.
Numerical computations are no more or no less than analytical theory; in both cases,
the goal is to determine the predictions of a particular model. The term “numerical
experiment” seems intended to elevate the stature of computing beyond that of
other theoretical tools. Numerical “experiments” are not experiments. Unlike a real
experiment, which always entertains the possibility of the discovery of new physical
phenomena beyond what is already known, the output of a numerical computation
can contain no more physics than the input.
The infamous Pentium division bug of the mid 1990’s notwithstanding.

Teaching Scientiﬁc Computing

1237

precise agreement with what the programmer instructed (or thought he instructed) the computer to calculate. It is easy, especially for beginning practitioners,
to confuse this correctness with the question of whether or not the algorithm as
implemented (or as envisioned) does indeed solve the problem under consideration, given the current set of computational parameters. As Acton points out [1],
most errors in computational results are ultimately due to human errors. Only
by recognizing this fact can we instill practices and attitudes which will work to
mitigate such errors.
We minimize the likelihood of errors by performing tests which indicate
whether the numerical solution is consistent with the solution of the problem at
hand. We will now discuss some techniques for building conﬁdence in the results
of a computation, as well as some general guidelines for avoiding problems.
Validation. The construction of eﬀective tests is critical to verify proper operation of numerical codes. There are two classes of tests: those designed to test
that the algorithm implemented in the code solves the problem it was designed
to solve (referred to as benchmarks), and those designed to test the implementation logic (this class of tests is often referred to as regression tests by software
engineers). While both classes of tests overlap in terms of the functionality they
probe, the philosophy behind the two classes is markedly diﬀerent.
Regression tests are primarily used to verify that alterations to a program do
not result in unintended consequences. Regression tests are used to identify changes in behaviour over time; their purpose is to verify that a given set of inputs
produces the same outputs today as yesterday. The cases used for regression
testing do not have to produce meaningful output, only consistent output. The
parameters do not need to be physically reasonable, nor does the resolution need
to be ﬁne enough to provide converged results. Consequently, regression tests can
be very short runs that take small amounts of CPU time. Due to their nature,
regression tests cannot be used to validate a code in an absolute sense. After a
particular version of a code has been validated with benchmarks, the regression
tests are run, the output is deemed to be correct, and these test results are then
used for future comparisons. It is not uncommon for even moderate-sized codes
to be run through a nightly set of regression tests. In any event, after code modiﬁcations, regression testing should be performed before any “production runs”
are made.
Benchmarks, on the other hand, are meant to verify that a program solves
its intended problem. This is normally accomplished by considering parameter
regimes (e.g., small amplitude excitation) where either exact or approximate solutions exist, or where speciﬁc properties of the solution are known. For example,
a linearized version of the model equations may be exactly soluble or parameters
(or initial conditions) may exist which eﬀectively reduce the dimensionality of
the problem. The most valuable tests are those where an exact solution exists
in a certain limit; these tests tend to be quite sensitive to algorithmic errors.
Additionally, there are often analytical constraints on the solution (for example
conservation of energy) which should be respected by the numerical solution to

1238

B.A. Shadwick

a level consistent with the algorithm. Generally, numerical solutions should be
tested for agreement with all known analytical properties.
Recently, we encountered a rather serious example of poor benchmarking.
There is a simple analytical relationship between the values of put and call
options (so called put-call parity), yet we discovered that the software for computing options prices used by a trading desk in an investment bank produced
results that signiﬁcantly violated this constraint. This was a production setting:
the software was used for negotiating contracts and to determine prices both
for internal auditing purposes. An error of this nature should never have gone
undetected by any reasonable testing procedure.
Convergence. Verifying convergence of the numerical solution is critically
important; except for certain classes of ODEs and linear PDEs, there is no general theorem regarding the convergence of the solution of ﬁnite diﬀerence equations as the discretization parameters tend to zero.4 While there is no guarantee
that the numerical solution will converge to the analytical solution (assuming
such exists), a necessary condition for the numerical solution to be consistent
with the analytical solution is that the numerical solution tends to a limit as
the ﬁnite diﬀerence cell size tends to zero. Moreover, verifying that the convergence rate is consistent with the order of the discretization is a valuable test
of a code and can uncover subtle errors. The most important reason for studying convergence is as a means to determine the resolution necessary to produce
results of acceptable accuracy. It is quite diﬃcult to determine, a priori, the
necessary resolution. An example of this presented itself recently in our work
related to the propagation of an intense laser pulse in a long plasma. One would
normally suppose that a resolution of, say, ten to twenty grid-points per laser
cycle would be suﬃcient to obtain reasonable results. Studying the convergence
of the solution led to the surprising discovery that a resolution on the order of
one hundred grid-points per cycle was necessary to obtain the correct results. At
lower resolutions, the solution appeared quite reasonable, but in fact, the laser
was coupling too strongly to the plasma and transferring energy considerably
more rapidly than was physically correct. While the answer appeared plausible,
it contained a qualitative error : the graph of laser energy versus time had the
wrong concavity. This error was only uncovered by performing a series of runs
that essentially produced a fully converged result. Often one hears comments to
the eﬀect “I doubled the number of grid-points and the solution only changed
by ten percent, so that’s good enough.” While the asymptotic order of the error
terms will be known from formal analysis, convergence rates will only match
asymptotic estimates for suﬃciently “small” grid sizes. Only by examining the
changes in the solution over a series of step sizes is it possible to determine if
the step sizes involved are such that the asymptotic error scaling applies. Furthermore, knowing that asymptotic scaling applies, in conjunction with observed
convergence rates, allows for global error estimates.
4

Clearly such a theorem would imply an existence theorem for the solution which
explains the dearth of results for nonlinear PDEs.

Teaching Scientiﬁc Computing

1239

Algorithms. It is an exceedingly bad idea to modify the model equations
solely for the convenience of numerical solution (an exception to this injunction
is the conversion of a PDE to integral form from which a weak solution is obtained). For the most part, simpliﬁcations which appeal to the analytical eye
do not necessarily make the numerical solution “easier” to obtain. Arithmetical
complexity is not generally an important concern. It is certainly of questionable
worth to reduce the validity of the model (say by linearizing certain terms) to
“make it simpler for the computer.” The complexity of the calculation is only
at issue as it relates to amount of CPU time used to obtain a solution. Rarely is
“making the equation easier to program” a valid consideration.5 In any case, it
makes the most sense to implement the full model. After that code is properly
working one can explore any “simpliﬁcations” which may prove advantageous.
Experience suggests that it is universally easier to impose new assumptions than
to remove existing ones.
As a general principle, there must always exist a control parameter (which
is part of the input data) that in some limit takes the discretized equations into
formal agreement with the model equations. Grid sizes and time steps introduced in the ﬁnite diﬀerence solutions of diﬀerential equations are examples of such
control parameters. As an illustration of the danger of uncontrolled approximations, consider the following: the wave equation for a laser pulse propagating in
a plasma was modiﬁed to facilitate solution, and the essence of the modiﬁcation
was that the formal expression for an inverse of the diﬀerential operator was
approximated in Fourier space by expansion in a power series. Only if all terms
were kept was there formal agreement with the unmodiﬁed equations. The code
in question kept only the ﬁrst term in this expansion, with the result that the
group velocity of the laser pulse, after a long propagation distance, was grossly
incorrect (as was the ﬁnal spatial position of the pulse). In the absence of a
control parameter, there was no easy way to discover that the program was, in
fact, solving a very diﬀerent equation than the author of the program intended.
Provided the propagation distance was small enough, this error could be considered tolerable. Of course, as is virtually always the case, the original restriction
on propagation distance imposed by this modiﬁcation was forgotten, and the
code was used in a regime where it was invalid. This discrepancy was only found when an independent group compared the (published) erroneous solutions
with the results from a code which made no such approximations. It is worth
noting that reliable algorithms for solving the unmodiﬁed equations were readily
available, but apparently unknown to the researchers. (In the ﬁnal analysis, in
addition to producing the wrong answer, the modiﬁcations in question made
the equations harder to solve numerically.) Modiﬁcation of model equations
should not be thought of as a numerical technique; there are likely assumptions
about the solution and/or input parameters which are necessary for solutions of
the modiﬁed model to approximate solutions of the original model. Above all,
modifying an equation should be considered a last, desperate, resort.
5

With the use of various code generation techniques, arguably, simplicity of implementation is never a valid consideration.

1240

3

B.A. Shadwick

General Knowledge

In addition to the techniques discussed in the previous sections, there also exists
a general body of knowledge and skills with which researchers involved in computation must have at least a passing acquaintance.
Numerical Analysis. General numerical analysis is well covered in any
number of references. The texts by Acton [1,2] are both eminently readable and
genuinely insightful. They serve as an excellent starting point for acquiring an
understanding of ﬂoating point arithmetic, round oﬀ error, numerical stability of
algorithms, numerical integration, solving linear and non-linear equations and
the like. This said, students must be aware that the “cookbook” approach to
problem solving will inevitably fail. For example, any one of a number of “standard” methods can be used to solve a set of ODEs (assuming, of course, that
there are not critical properties which must be exactly mirrored in the numerical
solution), but with the exception of certain standard PDEs, oﬀ-the-shelf PDE
solvers simply don’t exist. Algorithms have to be constructed speciﬁcally for the
equation in question.
To be successful, it is necessary to understand the mathematical basis for
the various algorithms one uses. That all algorithms solve some problem exactly is an important realization. In the case of diﬀerential equations, one obtains
numerically the exact solution of a slightly modiﬁed diﬀerential operator. This
modiﬁed operator reduces to the original equations as the time-step and/or grid
spacing tends to zero. In the case of systems of linear equations [3,4] the numerical result is a solution to a system with a slightly modiﬁed coeﬃcient matrix.
In all cases studying the modiﬁed equations can lead to signiﬁcant insight into
characteristics of the numerical solution. These techniques, collectively known as
backwards error analysis, are the bases for constructing the structure preserving
methods.
Software Engineering. There has been much improvement in software engineering practices in recent years. Ideally, practitioners of scientiﬁc computing
should, as much as possible, avail themselves of the current “best practices” in
software engineering. While not wanting to wade into the partisan arguments
over which programming language is best suited to scientiﬁc computing, it is
nonetheless worth noting that object-oriented design and coding practices now
dominate commercial software development and for very good reason. Adhering
to good design principles (encapsulation, abstraction, structured programming,
etc.) can greatly reduce the possibility of bugs as well as improve productivity.
Regardless of which language is used, it is critical to have a thorough understanding of how to properly use the chosen language. In particular, it is important
to understand that most languages will have undeﬁned behaviours — language
constructs which will compile but whose consequence at runtime is not deﬁned
by the language standard — any of which can result in hard-to-ﬁnd bugs, and
therefore, must be actively avoided.
In addition to understanding the proper use of the programming language,
the use of some form of source code revision control is also essential. A fun-

Teaching Scientiﬁc Computing

1241

damental aspect of science is the reproducibility of results. All to often, this
hallmark is missing from computational work. Research codes are constantly
changing, and only through tracking source code changes is it possible to revisit
previous work. A critical element of this reproducibility is the ability to know
which version of the code produced a particular result. For example, each time
one of our own codes is run, the parameters, along with the current version of
the code, are stored and the run is assigned a unique identiﬁer. In addition,
all hard (and soft) copies of results are labeled with this same identiﬁer. This
identiﬁer, in conjunction with version control, allows any run (especially those
used in publications) to be reproduced at any time.

4

Conclusions

Few students will master all the material we have discussed here during their
graduate careers, nor should we expect this to occur. This material should be
regarded as general framework that we encourage our students to ﬁll in as appropriate. The question of what should comprise the undergraduate curriculum
is particularly challenging, given all the forces competing for lecture time. At
the undergraduate level, beyond the usual elementary numerical analysis, it is
most important to emphasize the diﬃculty of validating any computation. This
mindset is critical; if we can condition our students to ask “is this right?” then
they will experience much greater success, while simultaneously encountering
less frustration.
Acknowledgements. The advice contained is this paper is the consequence
of over two decades of practical computing. Over that time, the author has beneﬁted from countless discussions about software and numerical methods with
colleagues and collaborators alike and with pleasure acknowledges John C. Bowman, W. F. Buell, Andrew Charman, E. H. Esarey, Richard Fitzpatrick, C. B.
Schroeder, W. F. Shadwick, J. D. Talman, and G. M. Tarkenton. This work was
supported by the US DoE under contract No. DE-AC03-76SF0098 and by The
Institute for Advanced Physics.

References
1. Acton, F.S.: REAL Computing Made Real: Preventing Errors in Scientiﬁc and
Engineering Calculations. Princeton University Press, Princeton, NJ (1996)
2. Acton, F.S.: Numerical Methods That Work. Mathematical Association of America,
Washington DC (1990)
3. von Neumann, J., Goldstine, H.H.: Numerical inverting of matrices of high order.
Bull. Amer. Math. Soc. 53 (1947) 1021–1099
4. Turing, A.M.: Rounding-oﬀ errors in matrix processes. Quart. J. Mech. Appl. Math.
1 (1948) 287–308

