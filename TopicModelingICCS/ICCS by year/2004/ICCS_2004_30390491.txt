Using Parallel Monte Carlo Methods in
Large-Scale Air Pollution Modelling
V.N. Alexandrov1 and Z. Zlatev2
1
2

Advanced Computing and Emergent Technologies Centre, University of Reading,
Reading, UK (v.n.alexandrov@reading.ac.uk)
National Environmental Research Institute, Frederiksborgvej 399, P. O. Box 358,
DK-4000 Roskilde, Denmark (zz@dmu.dk)

Abstract. Large-scale air pollution models can successfully be used in
diﬀerent environmental studies. These models are described mathematically by systems of partial diﬀerential equations. Splitting procedures
followed by discretization of the spatial derivatives lead to several large
systems of ordinary diﬀerential equations of order up to 80 millions.
These systems have to be handled numerically at up to 250 000 timesteps. Furthermore, many scenarios are often to be run in order to study
the dependence of the model results on the variation of some key parameters (as, for example, the emissions). Such huge computational tasks
can successfully be treated only if (i) fast and suﬃciently
accurate numerical methods are used and (ii) the models can eﬃciently
be run on parallel computers. Eﬃcient Monte Carlo methods for some
subproblems will be presented and applications of the model in the solution of some environmental tasks will also be made.

1

Introduction

The control of the pollution levels in diﬀerent highly polluted regions of Europe
and North America (as well as in other highly industrialized parts of the world)
is an important task for the modern society. Its relevance has been steadily increasing during the last two-three decades. The need to establish reliable control
strategies for the air pollution levels will become even more important in the future. Large-scale air pollution models can successfully be used to design reliable
control strategies. Many diﬀerent tasks have to be solved before starting to run
operationally an air pollution model. The following tasks are most important:
– describe in an adequate way all important physical and chemical processes,
– apply fast and suﬃciently accurate numerical methods in the diﬀerent parts
of the model,
– ensure that the model runs eﬃciently on modern high-speed computers (and,
ﬁrst and foremost, on diﬀerent types of parallel computers),
– use high quality input data (both meteorological data and emission data) in
the runs,
– verify the model results by comparing them with reliable measurements taken in diﬀerent parts of the space domain of the model,
M. Bubak et al. (Eds.): ICCS 2004, LNCS 3039, pp. 491–498, 2004.
c Springer-Verlag Berlin Heidelberg 2004

492

V.N. Alexandrov and Z. Zlatev

– carry out some sensitivity experiments to check the response of the model
to changes of diﬀerent key parameters
and
– visualize and animate the output results to make them easily understandable
also for non-specialists.
The performance of the model on high-speed computers will be discussed in
this paper.
1.1

Main Physical and Chemical Processes

Five physical and chemical processes have to be described by mathematical terms
in the beginning of the development of an air pollution model. These processes
are: (i) horizontal transport (advection), (ii) horizontal diﬀusion, (iii)chemical
transformations in the atmosphere combined with emissions from diﬀerent sources, (iv) deposition of pollutants to the surface and (v) vertical exchange (containing both vertical transport and vertical diﬀusion).
It is important to describe in an adequate way all these processes. However,
this is an extremely diﬃcult task; both because of the lack of knowledge for
some of the processes (this is mainly true for some chemical reactions and for
some of the mechanisms describing the vertical diﬀusion) and because a very
rigorous description of some of the processes will lead to huge computational
tasks which may make the treatment of the model practically impossible. The
main principles used in the mathematical description of the main physical and
chemical processes as well as the need to keep the balance between the rigorous
description of the processes and the necessity to be able to run the model on the
available computers are discussed in [6].
1.2

Mathematical Formulation of a Large Air Pollution Model

The description of the physical and chemical processes by mathematical terms
leads to a system of partial diﬀerential equations (PDEs) of the following type:
∂cs
∂(ucs ) ∂(vcs ) ∂(wcs )
=−
−
−
∂t
∂x
∂y
∂z
+

∂
∂x

Kx

∂cs
∂x

+

∂
∂y

Ky

∂cs
∂y

(1)
+

∂
∂z

Kz

+Es − (κ1s + κ2s )cs + Qs (c1 , c2 , . . . , cq ),

∂cs
∂z
s = 1, 2, . . . , q,

where (i) the concentrations of the chemical species are denoted by cs , (ii) u, v
and w are wind velocities, (iii) Kx , Ky and Kz are diﬀusion coeﬃcients, (iv) the
emission sources are described by Es , (v) κ1s and κ2s are deposition coeﬃcients

Using Parallel Monte Carlo Methods

493

and (vi) the chemical reactions are denoted by Qs (c1 , c2 , . . . , cq ). The CBM IV
chemical scheme, which has been proposed in [4], is actually used in the version
of DEM (the Danish Eulerian Model; [6], [7]) that will be considered in this
paper.

2

Achieving Parallelism

Delivering eﬃcient parallel algorithms for treating large scale air pollution is
very important. Note, for example, that the size of the computational tasks can
be formidable if we need ﬁner resolution, e.g. 480 × 480 mesh, leading to solving
8064000 equations per time step and depending on the number of species and
time steps potentially to a systems of ordinary diﬀerential equations of order up
to 80 millions.
Therefore the preparation of a parallel code is by no means an easy task.
Moreover, it may happen that when the code is ready the computing centre
exchanges the computer which has been used in the preparation of the code
with another (hopefully, more powerful) computer. This is why it is desirable to
use only standard tools in the preparation of the code. This will facilitate the
transition
of the code from one computer to another when this becomes necessary. Only
standard MPI ([3]) tools are used in the parallel versions of DEM.
2.1

Development of MPI Versions of DEM

The approach used when MPI tools are to be implemented is based in dividing
the space domain of the model into p sub-domains, where p is the number of
processors which are to be used in the run. Two speciﬁc modules are needed in
the MPI versions: (i) a pre-processing module and (ii) a post-processing module.
– The pre-processing module. corresponding to the p sub-domains obtained in the division of the space domain. In this way, each processor will work
during the whole computational process with its own set of input data.
– The post-processing module. Each processor prepares its own set of
output data. During the post-processing the p sets of
output data corresponding to the p sub-domains are collected and common
output ﬁles are prepared for future use.
– Beneﬁts of using the two modules. Excessive communications during the
computational process are avoided when the two modules are used. It should
be stressed, however, that not all communications during the computational process are avoided. Some communications along the inner boundaries
of the sub-domains are still needed. However, these communications are to
be carried only once per step and only a few data are to be communicated.
Thus, the actual communications that are to be carried out during the computations are rather cheap when the pre-processing and the post-processing
modules are proper implemented.

494

V.N. Alexandrov and Z. Zlatev

It is important to emphasize here that the introduction of p sub-domains leads
to a reduction of the main arrays by a factor of p. Consider as an illustrations the
major arrays used in the chemical sub-model. The dimensions of these arrays are
reduced from (Nx × Ny , Ns ) to (Nx × Ny /p, Ns ). It is clear that this is equivalent
to the use of p chunks. Chunks of length Nx ×Ny /p are still very large. Therefore,
the following algorithm has also to be used (in each sub-domain) when the MPI
versions are used:
DO ICHUNK=1,NCHUNKS
Copy chunk ICHUNK from some of the eight
large arrays into small two-dimensional
arrays with leading dimension NSIZE
DO J=1,NSPECIES
DO I=1,NSIZE
Perform the chemical reactions involving
species J for grid-point I
END DO
END DO
Copy some of the small two-dimensional
arrays with leading dimension NSIZE
into chunk ICHUNK of the corresponding
large arrays
END DO

However, the reduction of the arrays leads to a reductions of the copies that
are to be made in the beginning and in the end of the algorithm. Thus, the
reduction of the arrays leads to a better utilization of the cache memory.
The automatic reduction of the sizes of the involved arrays, and the resulting from this reduction better utilization of the cache memory, make the MPI
versions attractive also when shared memory machines are available.

3

Description of the Grid of Sun Computers

Sun computers located at the Danish Centre for Scientiﬁc Computing (the Danish Technical University in Lyngby) were used in the runs. The computers and
the their characteristics are shown in Table 1. All these computers were connected with a 1Gbit/s Switch.
The computers are united in a grid (consisting of 216 processors) so that a job
sent without a special demand will be assigned on the computer on which there
are suﬃciently many free processors. The diﬀerent computers have processors
of diﬀerent power (therefore, it is in principle possible to use the grid as a
heterogeneous architecture, but this option is not available yet).
We are in general allowed to use no more than 16 processors, but several runs
on more that 16 processors were performed with a special permission from the
Danish Centre for Scientiﬁc Computing. In the runs in this section we used only
”newton” (i.e. we had always a requirement specifying the particular computer
on which the job must be run)
More details about the high speed computers that are available at the Technical University of Denmark can be found in [5].

Using Parallel Monte Carlo Methods

495

Table 1. The computers available at the Sun grid
Computer
Bohr
Erlang
Hald
Euler
Hilbert
Newton

4

Type
Sun Fire
Sun Fire
Sun Fire
Sun Fire
Sun Fire
Sun Fire

6800
6800
12k
6800
6800
15k

Power
UltraSparc-III 750 MHrz
UltraSparc-III 750 MHrz
UltraSparc-III 750 MHrz
UltraSparc-III 750 MHrz
UltraSparc-III 750 MHrz
UltraSparc-IIIcu 900 MHrz

RAM Processors
48 GB
24
48 GB
24
144 GB
48
24
24 GB
36 GB
24
404 GB
72

Running the MPI Versions of DEM

Four MPI versions of DEM have been tested: (i) the 2-D model on a coarse grid,
(ii) the 3-D version on a coarse grid, (iii) the 2-D version on a ﬁne grid and (iv)
the 3-D version on a ﬁne grid.
The problems were run with three diﬀerent sizes N SIZE of chunks: (a) the
minimal size of the chunks, N SIZE = 1 for all cases, (b) a medium size of
the chunks, N SIZE = 24 for all cases and (c) the maximal size of the chunks,
which is N SIZE = 1152 for the coarse grid when 8 processors are used and
N SIZE = 28800 for the ﬁne grid (again when 8 processors are used).
Finally, in most of the cases both 1 processor and 8 processors were used.
Some of the jobs were also run on more than 8 processors.
All runs of the versions discretized on the coarse grid were run for the typical
period of one year (in which case it is possible to study seasonal variations). The
2-D version of DEM discretized on the ﬁne grid was run over a period of one
month. Finally, the 3-D version of DEM discretized on the ﬁne grid was run over
a time period of 42 hours. This is a rather short period, but it is still meaningful
to a certain degree because several changes from day to night and from night to
day occur in this period, which is important for the test of the photo-chemical
reactions.
The computing times in all tables are given in seconds. The abbreviations
used in the tables can be explained as follows:
– ADV stands for the horizontal transport + diﬀusion process,
– CHEM stands for the process uniting the chemical reactions, the treatment
of the emissions and the deposition part,
– COMM stands for the part needed to perform communications along the
inner boundaries,
– VERT stands for the vertical exchange processes
– TOTAL stands for the total computing time (including the sum of the times
given in the same column above the last item + the computing times needed for performing input-output operations, pre-processing, post-processing,
etc.)

496

V.N. Alexandrov and Z. Zlatev
Table 2. Running DEM discretized on a 96 × 96 × 10 grid on one processor

Process
ADV
CHEM
VERT
COMM
TOTAL

N SIZE = 1
Time
Part
169776 31.5%
337791 62.7%
23221 4.3%
2 0.0%
538953 100.0%

N SIZE = 24
Time
Part
159450 37.8%
233471 55.3%
21473 5.1%
2 0.0%
421763 100.0%

N SIZE = 1152
Part
Time
169865 30.9%
348769 63.4%
23014
4.2%
2
0.0%
549835 100.0%

Table 3. Running DEM discretized on a 96 × 96 × 10 grid on eight processors

Process
ADV
CHEM
VERT
COMM
TOTAL

N SIZE = 1
Time
Part Speed-up
18968 27.4%
9.0
41334 59.6%
8.2
1213 1.7%
19.1
911 1.3%
69325 100.0%
7.8

N SIZE = 24
Time
Part Speed-up
8.6
18498 33.3%
29189 52.3%
8.0
1200 2.2%
17.9
878 1.6%
7.6
55723 100.0%

N SIZE = 1152
Time
Part Speed-up
18641 26.3%
9.1
43291 61.3%
8.1
1240 1.8%
18.6
973 1.4%
70653 100.0%
7.8

The percentages of the computing times for the diﬀerent processes related to
the total computing times are given in the columns under ”Part”. The ”Speedup” is the ratio of the computing time on one processor and the computing time
on p processors (where p is the number of processors that are used in the run
under considerations; as mentioned above, eight processors were as a rule used
in our experiments).
Running the 3-D MPI version discretized on the coarse grid. Results
from the six runs with this code are shown in Table 2 (runs on one processor
performed by using three values of N SIZE) and Table 3 (runs on 8 processors
performed again with three values of N SIZE).
Running the 3-D MPI version discretized on the ﬁne grid. Results
from the six runs with this code are shown in Table 4 (runs on one processor
performed by using three values of N SIZE) and Table 5 (runs on 8 processors
performed again with three values of N SIZE).
Further, in the next table we present a comparison of the eﬃciency of using
Monte Carlo algorithms [1,2] for solving subproblems (e.g. linear systems arising
after discretization) in the model. Comparisons of the eﬃciency of these methods
with some of the traditional ones such as LU is made. It is seen in Table 6 that
with the growth of the problem size Monte Carlo calculations become more
eﬃcient than LU for the sparse structured matrices treated in the advection
submodel.
Several runs were performed by using up to 60 processors. The 3-D reﬁned
version, where high eﬃciency is most desirable, was used in this runs. The results

Using Parallel Monte Carlo Methods

497

Table 4. Running DEM discretized on a 480 × 480 × 10 grid on one processor

Process
ADV
CHEM
VERT
COMM
TOTAL

N SIZE = 1
Time
Part
261631 67.0%
86317 22.1%
40721 10.4%
1 0.0%
390209 100.0%

N SIZE = 24
Time
Part
271419 72.9%
56797 15.3%
42320 11.4%
1 0.0%
372173 100.0%

N SIZE = 28800
Part
Time
268337
49.8%
42.3%
228216
41223
7.6%
1
0.0%
539319 100.0%

Table 5. Running DEM discretized on a 480 × 480 × 10 grid on eight processors

Process
ADV
CHEM
VERT
COMM
TOTAL

N SIZE = 1
Time
Part Speed-up
13606 46.2%
19.2
8.3
10398 35.3%
2830 9.6%
14.4
2316 7.9%
29449 100.0%
13.3

N SIZE = 24
Time
Part Speed-up
13515 52.7%
20.1
6681 26.0%
8.5
2802 10.9%
15.1
2340 9.1%
25654 100.0%
14.5

N SIZE = 28800
Time
Part Speed-up
13374 28.9%
20.1
25888 56.0%
8.8
2709 5.9%
15.2
3925 8.5%
46210 100.0%
11.7

are given in Table 7 and indicate that the parallel algorithms applied in DEM
scale very well.
Major conclusions from the runs. It is seen that the exploitation of the
cache memory is always giving good results (compare the results for N SIZE =
24 with the results for N SIZE = 1 and N SIZE = 1152(28800). The speed-ups
for the physical processes are super-linear (greater for ADV and VERT than
for CHEM, which should be expected, because chunks are used in the chemical
parts). The speed-ups for the total computing time are lower, but anyway at
least close to linear.

5

Conclusion

It has been shown in this paper, that based on our parallelization approach,
diﬀerent submodels of DEM scale very well when the number of the processors
used is increased. For some parts of the model we continue to observe superlinear speedup. In addition, the experiments with Monte Carlo show that for
larger problems these algorithms are more eﬃcient that the traditional ones,
scale well and can lead to a reduction of computational time.
Further work is required, for example, the improvement of the ﬁne resolution
versions of DEM, especially the 3-D ﬁne resolution version, is an important
task which must be resolved in the near future. It is necessary both to improve
the performance of the diﬀerent versions of the model and to have access to
more processors (and/or to more powerful computers) in order to be able to run
operationally ﬁne resolution versions of DEM.

498

V.N. Alexandrov and Z. Zlatev

Table 6. Running DEM with Monte Carlo in the Advection part for various grid
reﬁnements with time step 200, 800 and 2400 respectively
Pr. Size Method error Startup Time ADVEC Total Time
8×8
LU 0.001
0.0
0.090
0.090
8×8
M C 0.001
2.1
0.090
2.190
32 × 32
LU 0.001
0.01
31.06
31.07
32 × 32
M C 0.001
3.52 29.230
32.72
96 × 96
LU 0.001
0.01 227.57
227.58
96 × 96
M C 0.001
63.8
88.8
152.6
Table 7. Running DEM discretized on a 480 × 480 × 10 on diﬀerent numbers of
processors
Processors Time Speed-up
1 372173
15 12928
28.79
30 7165
51.94
60 4081
91.20

Acknowledgements. A grant (CPU-1101-17) from the Danish Centre for
Scientiﬁc Computing (DCSC) gave us access to the Sun computers at the Technical University of Denmark. The members of the staﬀ of DCSC helped us to
resolve some diﬃcult problems related to the eﬃcient exploitation of the grid of
Sun computers.

References
1. Alexandrov V.N., Eﬃcient parallel Monte Carlo Methods for Matrix Computation, Mathematics and computers in Simulation, Elsevier 47 pp. 113-122, Netherlands, (1998).
2. Dimov I., Alexandrov V.N. and Karaivanova A., Resolvent Monte Carlo
Methods for Linear Algebra Problems, Mathematics and Computers in Simulation,
Vo155, pp. 25-36, 2001.
3. W. Gropp, E. Lusk and A. Skjellum, Using MPI: Portable programming with
the message passing interface, MIT Press, Cambridge, Massachusetts (1994).
4. M. W. Gery, G. Z. Whitten, J. P. Killus and M. C. Dodge, A photochemical kinetics mechanism for urban and regional computer modeling, Journal of
Geophysical Research, Vol. 94 (1989), 12925–12956.
5. WEB-site of the Danish Centre for Scientific Computing at the Technical University of Denmark, Sun High Performance Computing Systems,
http://www.hpc.dtu.dk, 2002.
6. Z. Zlatev, Computer treatment of large air pollution models, Kluwer Academic
Publishers, Dordrecht-Boston-London (1995).
7. Z. Zlatev, Massive data set issues in air pollution modelling, In: Handbook on
Massive Data Sets (J. Abello, P. M. Pardalos and M. G. C. Resende, eds.), pp.
1169-1220, Kluwer Academic Publishers, Dordrecht-Boston-London (2002).

