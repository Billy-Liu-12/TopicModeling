Procedia Computer Science
Volume 80, 2016, Pages 1180–1190
ICCS 2016. The International Conference on Computational
Science

Simulations of Partial Update LMS Algorithms in
Application to Active Noise Control
Dariusz Bismor1
Institute of Automatic Control, Silesian University of Technology, 44-190 Gliwice, Poland
Dariusz.Bismor@polsl.pl

Abstract
Partial Update LMS (PU LMS) algorithms started to play an important role in adaptive processing of sound. Due to reduction of computational power demands, these algorithms allow to
use longer adaptive ﬁlters, and therefore achieve better results in adaptive ﬁltering applications,
e.g., system identiﬁcation, acoustic echo cancellation, and active noise control (ANC). There are
two main groups of PU algorithms: data-independent and data-dependent algorithms. While
application of a data-independent algorithm almost always results in a degradation of performance, application of data-dependent PU algorithms may even result in an increase of the
performance, compared with full parameters update. However, the latter group of algorithms
requires sorting.
A number of updated parameters is the factor that allows to decide how much of performance
should be sacriﬁced to obtain computational power savings. In the extreme case only one ﬁlter
tap out of possibly hundreds is updated during each sampling period. The goal of this paper is
to show extensive simulations proving that careful selection of this one tap results in a useful
and well performing algorithm, even in the demanding application of active noise control.
Keywords: adaptive algorithms, simulation, least mean squares, active noise control

1

Introduction

Partial updates (PU) is a modern technique that allows to reduce a computational eﬀort during
an adaptive ﬁlter update phase [2]. These algorithms gained a considerable attention in both
research and practical applications in recent years.
There are two main groups of PU algorithms: data-independent algorithms and datadependent algorithms. The application of an algorithm belonging to the ﬁrst group allows
for the best computational power savings, but almost always results in a degradation of performance. On the other hand, careful, data-dependent selection of an adaptive ﬁlter tap to be
updated, may even result in an increase of the performance, compared with full parameters
updates. However, these algorithms require sorting.
1180

Selection and peer-review under responsibility of the Scientiﬁc Programme Committee of ICCS 2016
c The Authors. Published by Elsevier B.V.

doi:10.1016/j.procs.2016.05.451

Simulations of Partial Update LMS Algorithms. . .

D. Bismor

In this paper we focus our attention on the following popular, time-domain, PU least mean
squares (LMS) algorithms: sequential LMS, M-max LMS, M-max normalized LMS (NLMS),
and selective LMS algorithms [1, 2, 3, 4, 9].
The number of ﬁlter taps to be updated in each sampling period is a very important design
parameter. A useful range of values for this parameter starts from half of the ﬁlter taps (which
may result in reduction of the computational power necessary to update the ﬁlter by two), to
be equal to one in the extreme case. The lower the number of the ﬁlter taps updated, the
slower the convergence—this intuitive deduction is absolutely true for all the data-independent
PU algorithms. Moreover, the convergence rate slowdown is approximately proportional to
the reduction of the number of parameters updated. However, data-dependent PU algorithms,
developed as an answer to convergence rate problems, do not suﬀer so much from the PU
techniques. One of the data-dependent PU algorithms, called the selective LMS algorithm, is
the in the focus of the study presented in this paper. A particular setup of this algorithm, where
only one ﬁlter parameter is updated during each sampling period, will be addressed to as the
One Tap Update (OTU) LMS algorithm, and will be applied to active noise control problem.
Nevertheless, we will compare performance of the selective LMS and OTU LMS algorithms
with other partial updates and with the full update normalized LMS (NLMS) algorithm.

1.1

Notation
T

In this paper we use the usual notation, with w(n) = [w0 (n), w1 (n), . . . wL−1 (n)] being the
vector of an adaptive ﬁlter taps (or weights) of length L. The corresponding input vector
T
u(n) = [u(n), u(n − 1), . . . u(n − L + 1)] is composed of samples of some input signal, delayed
in time (a transversal ﬁlter). The signal with the desired properties is denoted by d(n), and
the error signal, e(n), is calculated as e(n) = d(n) − wT (n)u(n). With these assumptions, the
LMS algorithm update equation is given by:
w(n + 1) = w(n) + μ(n)u(n)e(n),

(1)

where μ(n) is the step size (μ(n) = μ for algorithms with a constant step size).

2

Partial Update LMS Algorithms

All the PU algorithms considered in this paper can be described be a general update equation:
w(n + 1) = w(n) + μ(n)IM (n)u(n)e(n).

(2)

The only diﬀerence between the full update LMS algorithm (1) and PU algorithms lies in the
matrix IM (n), which is called a weight selection matrix, and is deﬁned as:
⎤
⎡
i0 (n)
0
···
0
L−1
⎢ 0
0 ⎥
i1 (n) · · ·
⎥
⎢
,
i
(n)
∈
{0,
1},
ik (n) = M.
(3)
IM (n) = ⎢ .
⎥
.
.
k
..
..
..
⎦
⎣ ..
.
0

···

0

iL−1 (n)

k=0

Thus, the weight selection matrix is a diagonal matrix, with elements equal to 0 or 1, and the
total number of 1s in the matrix is equal to M . The selection matrix is also subscripted with
the number M , because it is the number of the adaptive ﬁlter weights that are selected for the
update in each sampling period.
1181

Simulations of Partial Update LMS Algorithms. . .

D. Bismor

In every subsequent sampling period, the elements on the diagonal of the selection matrix
are set to 0 or 1, according to the following formula:
ik (n) =

1 if k ∈ IM (n)
0 otherwise

,

(4)

where IM (n) denotes a set of the ﬁlter weight indices that deﬁne the coeﬃcients to be updated
in the n-th iteration; the number of the elements in this set is equal to M . Diﬀerent PU LMS
algorithms deﬁne this set in a diﬀerent way.

2.1

Sequential LMS Algorithm

The sequential LMS algorithm is used in this paper as a representative of the data-independent
PU algorithms. A basic idea of this algorithm is to divide the ﬁlter coeﬃcients into several
subsets, and to select each of the subsets for the update sequentially in each iteration [3]. The
number of the subsets the ﬁlter is partitioned with can be calculated as B = L/M , where
· denotes ceil operation. Only in a special case when L mod M = 0 the subsets will be of
equal lengths, otherwise there will be B − 1 subsets with M coeﬃcients and one subset with L
mod B coeﬃcients. It is a designers choice how to partition the ﬁlter coeﬃcients into subsets.
The very simple subset selection technique used with this algorithm results in the best
computational power savings among all the PU algorithms discussed in this paper. On the
other hand, the algorithm have many drawbacks, e.g., poor convergence rate, instability for
cyclostationary input signals, etc. [2].

2.2

M-max LMS Algorithm

The M-max LMS algorithm is the simplest data-dependent algorithm. The idea of the M-max
LMS is based on selecting M those entries in the input vector u(n), that result in the largest
magnitude changes of the ﬁlter weights. Therefore, the weight selection matrix (3) entries can
be deﬁned as:
1 if |u(n − k)| ∈ max (|u(n − l)|, M )
0≤l≤L−1
ik (n) =
,
(5)
0 otherwise
where maxl (ul , M ) denotes a set of M maxima of elements ul [2]. This algorithm does not use
the step size normalization (i.e., operates with a constant step size).
The M-max LMS algorithm requires ranking (sorting) of the input vector elements, based
on their absolute values. Thus, the time saving this algorithm oﬀers is smaller than in case of
data-independent algorithms. However, computationally eﬃcient sorting algorithms are now
available [2]. Especially in case when the input vector is constructed from samples of some
signal delayed in time (as assumed in Section 1.1), where only one sample is exchanged in each
sampling period, the shift structure can be exploited to perform the sorting very eﬃciently [11].

2.3

M-max NLMS Algorithm

The idea of M-max LMS can also be used together with the normalization of the step size,
resulting in the M-max NLMS algorithm. In this case the weight selection matrix entries are
deﬁned identically as in case of the M-max LMS algorithm (see Eq. (5)). However, normalization
of the step size is performed as in the NLMS algorithm:
μ
˜
μ(n) = T
,
μ
˜ — the normalized step size.
(6)
u (n)u(n)
1182

Simulations of Partial Update LMS Algorithms. . .

2.4

D. Bismor

Selective LMS Algorithm

The M-max NLMS algorithm has a subtle feature: it uses an estimate of the power of the whole
input vector (the uT (n)u(n) term in Eq. (6)) for the step size normalization in spite of the fact
that only a part of the input vector elements are selected for actual weight update. To correct
this issue, it is possible to calculate the normalized step size as:
μ(n) =

uT (n)I

μ
˜
.
M (n)u(n)

(7)

This results in an algorithm known as the selective LMS algorithm [9].
It is also possible to derive the selective LMS algorithm as a result of instantaneous approximation of the Newton’s method, or as an application of the minimum disturbance principle
[2]. Both the derivations result in the algorithm deﬁned by Eqs. (2) and (7).
Although the diﬀerence between the selective LMS and the M-max NLMS algorithms is
small, it results in substantial diﬀerences in performance of the algorithms, as shown below.

2.5

One Tap Update LMS

In this paper we will also use the selective LMS algorithm with M = 1, i.e., the algorithm that
updates only one tap during each sampling period. By combining Eqs. (7) and (2) and using
M = 1 we notice that such algorithm is given by:
w(n + 1) = w(n) +

μ
˜
I1 (n)u(n)e(n),
u2max (n)

(8)

where umax (n) = max |u(n)| denotes the maximum absolute value in the input vector at a
discrete time n. Remembering that wk denotes the k − th ﬁlter coeﬃcient, the algorithm may
be expressed as:
⎧
μ
˜e(n)
⎨
wk (n) +
, if |u(n − k)| = max |u(n)|,
wk (n + 1) =
.
(9)
u(n − k)
⎩
wk (n),
otherwise
This simple update algorithm, developed by Douglas [5], updates only one coeﬃcient—the
coeﬃcient that corresponds to the input sample with the maximum absolute value. Moreover,
the update is computationally very eﬃcient: sorting is not necessary, and it requires only one
multiplication, one division and one addition per each update. This algorithm will be referred
to as the One Tap Update LMS (OTU LMS).
Considering the shift structure of the input vector, the maxline algorithm [11] can be used
for ﬁnding its maximum absolute value. The worst case computational complexity of this
algorithm is L + 1 comparisons. However, it has been shown in [11] that the average number
of comparisons, in case of a random input data with the uniform distribution, is approximately
equal to 3.

3

Active Noise Control and Filtered-x Modiﬁcation

A simpliﬁed block diagram of an active noise control (ANC) system is presented in Fig. 1.
In this ﬁgure, P (z −1 ) represents a primary path (a path from a reference microphone to an
error microphone) and S(z −1 ) represents a secondary path (a path from the ANC system
1183

Simulations of Partial Update LMS Algorithms. . .

D. Bismor

Figure 1: The block diagram of the ANC System
control value to the error microphone). Presence of the secondary path is the main diﬀerence
between the ANC system and other LMS algorithm applications, e.g., acoustic echo cancellation.
Unfortunately, it seriously inﬂuences stability of the LMS algorithm [7]. There are many ways
to neutralize the eﬀect of the secondary path (see e.g., [7, 10]), but the most popular way is to
involve the ﬁltered-x LMS (FxLMS) structure, where the input signal to the LMS algorithm is
ˆ −1 ), as presented in Fig. 1. As a
ﬁltered with a secondary path transfer function estimate S(z
consequence, the FxLMS update equation is given by:
w(n + 1) = w(n) + μ(n)ˆ
u(n)e(n),

(10)

T

ˆ (n) = [ˆ
where u
u(n), u
ˆ(n − 1), . . . u
ˆ(n − L + 1)] , is a vector of ﬁltered input samples, and
u
ˆ(n) = sˆ ∗ u(n), where sˆ is the secondary path impulse response estimate and ∗ represents time
convolution.
To apply PU algorithms in the FxLMS structure, it is necessary to modify the general PU
LMS algorithm update equation (2) in a similar way, i.e., to use the ﬁltered input vector u (n)
instead of the input vector u(n). Moreover, the sorting, or the maximum element search (if
required) must be performed according to the ﬁltered input vector values as well.
Another issue with ANC systems is the accuracy of the secondary path modeling. A perfect
ˆ −1 ) = S(z −1 )) is possible in simulations only. In practical
model of the secondary path (i.e., S(z
applications, imperfect secondary path models must be considered. Errors in the secondary
path modeling always require decrease of the step size and result in decrease of the convergence
rate. For example, it is common to practical ANC applications to use the normalized step sizes
smaller than 0.01 to maintain stability.

4

Simulation Study

The ANC system depicted in Fig. 1 has been simulated using Matlab/Simulink software. The
sampling frequency was set to 1 kHz for all the simulations. Each experiment was repeated 200
times, with diﬀerent noise sequences and diﬀerent sinusoid initial phases, and the results were
averaged to obtain smoother mean squared error (MSE) curves.
The primary and secondary path transfer functions used in these simulations were obtained
as a result of the identiﬁcation experiments, based on real data acquired in an ANC lab (room
1184

Simulations of Partial Update LMS Algorithms. . .

D. Bismor

MSE averaged over 200 runs,
perfect secondary path estimate
NLMS, μ
˜ = 0.02
Seq. LMS, μ = 0.005
M-max LMS, μ = 0.005
M-max NLMS, μ
˜ = 0.02
Selective LMS, μ
˜ = 0.02
OTULMS, μ
˜ = 0.02

−10
−15
MSE, dB

Spectrum of the error before and after attenuation (single run),
perfect secondary path estimate

−20
−25

−10
−20
−30
Level, dB

−5

−40
−50
−60

Noise
NLMS, μ
˜ = 0.02
Seq. LMS, μ = 0.005
M-max LMS, μ = 0.005
M-max NLMS, μ
˜ = 0.02

−70
−30
−35
0

−80
1000

2000
3000
Time, ms

4000

5000

−90
0

(a)

100

200
300
Frequency, Hz

400

500

(b)

Figure 2: The results of experiments with two sinusoids 100 and 300 Hz input signal and perfect
secondary path model: (a) MSE, (b) spectra of the error signals.
ANC). The length of the impulse response was 256 in case of the secondary path model, and
512 in case of the primary path model. Two types of the input signal were considered: two
sinusoids embedded in a white noise, and a band-limited noise. Two cases were considered:
perfect and imperfect secondary path modeling.
In the results of simulations presented below, the ANC ﬁlter with 256 taps was used, and
the number of taps updated using PU algorithms was M = 16 (except for the OTU LMS).
An eﬀort was made to ﬁnd a step size that gives good convergence performance for all the
experiments; the step sizes used in the experiments are given in the ﬁgures.

4.1

Perfect Secondary Path Modeling

Figure 2(a) presents MSE curves resulting from the simulations with the input (disturbing)
signal consisting of two sinusoids: 100 and 300 Hz. With such input signal, the ANC system
works as a narrowband ANC system [6]. The reference NLMS algorithm converges after approx.
1200 iterations, giving the ﬁnal attenuation about 19 dB. The selective LMS with M = 16, yields
roughly the same convergence rate and the same attenuation level. The OTU LMS algorithm,
which updates only one ﬁlter tap during each iteration, results in almost the same convergence
rate as the above two algorithms (its curve overlays the two others on the ﬁgure). Both the
M-max algorithms are slower to converge, and the sequential LMS is the slowest.
Figure 2(b) presents the spectra of the attenuated signal (i.e., the signal denoted as d(n) in
Fig. 1) as well as the error signals after the convergence for selected three algorithms. It may
be noticed that all the algorithms attenuated both the sinusoids down to the level of the noise.
Figure 3(a) presents the results of the simulations of another input signal, consisting of two
sinusoids: 60 and 250 Hz. In this case the reference NLMS algorithm converges very slowly,
almost as slow as the the M-max LMS and M-max NLMS algorithms. The convergence of the
sequential LMS is the slowest. However, the convergence of the selective LMS and the OTU
LMS is the best: both the algorithms give about 21 dB of attenuation after 1200 iterations.
The surprisingly fast convergence of the selective LMS and the OTU LMS algorithms was
anticipated by theoretical derivations. In this case, the diﬃculty of the attenuated signal lies
in a very large spread of eigenvalues of the input signal autocorrelation matrix, and it is very
well known that the LMS algorithm converges slowly for signals with such properties. However,
1185

Simulations of Partial Update LMS Algorithms. . .

D. Bismor

MSE averaged over 200 runs,
perfect secondary path estimate
NLMS, μ
˜ = 0.02
Seq. LMS, μ = 0.005
M-max LMS, μ = 0.005
M-max NLMS, μ
˜ = 0.02
Selective LMS, μ
˜ = 0.02
OTULMS, μ
˜ = 0.02

−10
−15
MSE, dB

MSE averaged over 200 runs,
perfect secondary path estimate

−20

−22

NLMS, μ
˜ = 0.02
Seq. LMS, μ = 0.005
M-max LMS, μ = 0.005
M-max NLMS, μ
˜ = 0.02
Selective LMS, μ
˜ = 0.02
OTULMS, μ
˜ = 0.02

−24
−26
MSE, dB

−5

−25

−28
−30
−32
−34

−30
−35
0

−36
1000

2000
3000
Time, ms

4000

−38
0

5000

2000

(a)

4000
6000
Time, ms

8000

10000

(b)

Figure 3: MSE in the experiments with perfect secondary path modeling: (a) two sinusoids 60
and 250 Hz input signal, (b) ﬁltered noise input signal.
MSE averaged over 200 runs,
truncated and disturbed secondary path estimate
NLMS, μ
˜ = 0.02
Seq. LMS, μ = 0.005
M-max LMS, μ = 0.005
M-max NLMS, μ
˜ = 0.02
Selective LMS, μ
˜ = 0.02
OTULMS, μ
˜ = 0.02

−10
−15
MSE, dB

MSE averaged over 200 runs,
truncated and disturbed secondary path estimate

−20
−25

−22

NLMS, μ
˜ = 0.02
Seq. LMS, μ = 0.005
M-max LMS, μ = 0.005
M-max NLMS, μ
˜ = 0.02
Selective LMS, μ
˜ = 0.02
OTULMS, μ
˜ = 0.02

−24
−26
MSE, dB

−5

−28
−30
−32
−34

−30
−35
0

−36
1000

2000
3000
Time, ms

(a)

4000

5000

−38
0

2000

4000
6000
Time, ms

8000

10000

(b)

Figure 4: MSE in the experiments with the truncated and disturbed secondary path model:
(a) two sinusoids 60 and 250 Hz input signal, (b) ﬁltered noise input signal.
the PU LMS algorithms “see” a diﬀerent input signal autocorrelation matrix (the exact form
of this matrix depends on a particular PU LMS algorithm), and this matrix may have diﬀerent
eigenvalue spread [2]. In this speciﬁc case, the selective LMS and the OUT LMS algorithms
“see” the autocorrelation matrix with smaller eigenvalue spread.
Figure 3(b) presents the results of simulation of the third input signal: a wideband noise
signal. The signal was obtained by ﬁltration of a Gaussian white noise sequence with a bandpass
ﬁlter with cutoﬀ frequencies 125 and 175 Hz. With such inputs signal, the ANC system works in
a wideband mode, which diﬀers considerably from the narrowband mode [6]. It may be noticed
that the sequential LMS algorithm failed to converge to a decent level during the observation
time of 10 s, and the M-max LMS algorithm exhibited an extremely poor convergence rate
(note a diﬀerent time scale in the ﬁgure). The M-max NLMS algorithm was only a bit faster
to converge. The algorithms with the best convergence rate were the full update NLMS and
the selective LMS. However, the OTU LMS algorithm was only slightly worse, giving hope for
a good performance in real applications.
1186

Simulations of Partial Update LMS Algorithms. . .

D. Bismor

Mean Squared Error
5

Spectrum of the error before and after attenuation (single run)
−30
Noise
NLMS
−40
M-max NLMS, M=64
−50
Selective LMS, M=64
OTU LMS
−60

NLMS
M-max NLMS, M=64
Selective LMS, M=64
OTU LMS

0

Level, dB

MSE, dB

−5
−10
−15

−70
−80
−90
−100

−20

−110
−25
0

0.5

1

1.5
Time, ms

2

2.5

−120
0

4

x 10

(a)

200

400
600
Frequency, Hz

800

1000

(b)

Figure 5: Results of laboratory experiments with two sinusoids 150 and 250 Hz input signal:
(a) MSE, (b) spectra of the error signals.

4.2

Imperfect Secondary Path Modeling

As already mentioned, perfect secondary path modeling can only occur in simulations. In real
active noise and vibration applications, the most common type of imperfectness is undermodeling (i.e., the secondary path model impulse response function is shorter than the real one) and
modeling errors [12]. Such realistic condition was also tested in the simulations. The secondary
path model was obtained by truncating the real impulse response (with 256 parameters) to
64 parameters, and by adding a random error with a small variance to each impulse response
parameter.
Figure 4 presents the results of the simulations for the imperfect secondary path model: (a)
for the input signal consisting of the sinusoids 60 and 250 Hz, and (b) for the ﬁltered noise signal
described in the previous section. From the ﬁgure it is clear that this type of imperfectness of
the secondary path model does not aﬀect convergence of the adaptive algorithms.

5

Real experiments

The simulation results obtained in the previous section indicated that some of partial update
LMS algorithms can be applied to ANC. The goal of the experiments presented in this section
was to validate this hypothesis in real experiments. Based on the simulations, and considering
that the room ANC system requires a relatively fast adaptation, the sequential LMS and the
M-max LMS algorithms were rejected due to their very poor convergence rate.
The ANC setup used during the experiments consisted of two microphones with preampliﬁers, two loudspeakers with power ampliﬁers (one to generate the noise to be canceled, and
the second the canceling one), a DSP board with A/D and D/A converters, and all the accompanying equipment. The sampling frequency was 2 kHz. The secondary path model was
obtained prior to each experiment, by the means of oﬀ-line identiﬁcation—the length of the
model was only 256, due to hardware restrictions. The canceling ﬁlter with 1024 taps was used.
To maintain the same as in the simulations ratio of the updated parameters to the ﬁlter length,
the number of the updated parameters for PU algorithms was M = 64 (except for the OTU
LMS). Unfortunately, it was not possible to apply the same step size during all the experiments
and maintain stability. Therefore, the step sizes were individually adjusted, by trial and error,
1187

Simulations of Partial Update LMS Algorithms. . .

D. Bismor

Mean Squared Error
5
0

Level, dB

−5
MSE, dB

Spectrum of the error before and after attenuation (single run)
−30
Noise
NLMS
−40
M-max NLMS, M=64
−50
Selective LMS, M=64
OTU LMS
−60

NLMS
M-max NLMS, M=64
Selective LMS, M=64
OTU LMS

−10
−15

−70
−80
−90

−20

−100
−25
−30
0

−110
0.5

1

1.5
Time, ms

(a)

2

2.5
4

x 10

−120
0

200

400
600
Frequency, Hz

800

1000

(b)

Figure 6: Results of laboratory experiments with two sinusoids 300 and 400 Hz input signal:
(a) MSE, (b) spectra of the error signals.
for each algorithm tested, prior to each group of experiments.
Input signals consisting of a single sinusoid were tested ﬁrst. For the majority of the frequencies tested all the algorithms were convergent and resulted in a good attention level. However,
from the theory it follows that in a single-input, single-output ANC system there are frequencies
for which the attention cannot be attained [7]—this was also conﬁrmed during the experiments.
Input signals consisting of two sinusoids were tested next. Figure 5 presents the results of
the experiments with the input signal consisting of 150 and 250 Hz sinusoids, a typical example
of this family. The NLMS algorithm, provided for a reference, achieves a level of attenuation
of about 23 dB and converges in approx. 0.6 second. The M-max LMS algorithm achieves
the same level of attenuation in about 0.8 s. The selective LMS, on the other hand, initially
converges very fast, and achieves the level of attenuation of about 22 dB in around 0.1 s.
As anticipated, the OTU algorithm, which updates only one ﬁlter weight (out of 1024), also
converges faster than the full update NLMS algorithm, but its convergence rate is approx. 2.5
times poorer than in case of the selective LMS.
The spectra of the noise and the error signal, presented in Fig. 5(b), show that all the
algorithms attenuate the two sinusoids down to the level of the background noise. However,
spurious frequency components are present in both the noise and the error signal, which are
not present in the reference signal, and therefore are not attenuated.
Figure 6 presents another typical example; in this case, the input signal consisted of 300
and 400 Hz sinusoids. The results of the experiments are very similar for the NLMS, M-max
LMS, and OTU LMS: the last one is the fastest to converge, the second the slowest. However,
the selective LMS, which converges initially very fast, starts to diverge after approx. 0.2 s
of the experiment. This is very clearly visible in Fig. 6(b), where the attenuated frequency
components are again visible in the spectrum towards the end of the experiment. The reason
for this behavior may be too long step size. The step size may be decreased, but this will result
in a slower convergence rate. Other reasons are also possible, e.g., too short secondary path
model, inﬂuene of the acoustic feedback, etc.
Finally, Fig. 7 presents one of the results of experiments with the ﬁltered noise input signal.
In case of this complex signal, the attenuation level does not exceed 11 dB (which is typical for
wideband signals). The M-max LMS is again slower to converge than the full update NLMS,
and both the selective LMS and the OTU LMS are faster. However, the advantage of the
1188

Simulations of Partial Update LMS Algorithms. . .

D. Bismor

Mean Squared Error
2
0

Level, dB

−2
MSE, dB

Spectrum of the error before and after attenuation (single run)
−60
Noise
NLMS
M-max NLMS, M=64
−70
Selective LMS, M=64
OTU LMS
−80

NLMS
M-max NLMS, M=64
Selective LMS, M=64
OTU LMS

−4
−6

−100

−8

−110

−10
−12
0

−90

0.5

1

1.5
Time, ms

(a)

2

2.5
4

x 10

−120
0

200

400
600
Frequency, Hz

800

1000

(b)

Figure 7: Results of laboratory experiments with ﬁltered noise input signal: (a) MSE, (b)
spectra of the error signals.
selective LMS over the other algorithms is extraordinarily clear: it converges in around 0.5 s,
while the OTU LMS needs almost 2 s, and the other more than 5 s (not shown in the ﬁgure).

6

Conclusions

The goal of the study presented in this paper was to simulate the behavior of selected partial update LMS algorithms in the ﬁltered-x LMS structure used in active noise control, and
to validate the simulation results during laboratory experiments. The simulations conﬁrmed
that PU algorithms can be applied in the FxLMS structure, and revealed that for some input
signals PU algorithms can achieve better performance than the full-update NLMS algorithm.
This result was further conﬁrmed during the laboratory experiments, where the selective LMS
algorithm exhibited a converge rate 5–10 times better than the NLMS algorithm, depending
on the input signal. Moreover, the OTU LMS algorithm, which updates only one ﬁlter weight
during each sampling period and therefore gives substantial computational power savings, can
also be successfully used in this demanding application.

Acknowledgment
The research is within a project ﬁnanced by the National Science Center, based on decision no.
DEC-2012/07/B/ST7/01408.

References
[1] D. Bismor. Stability of a Class of Least Mean Square Algorithms. Problemy wsp´
olczesnej automatyki i robotyki. AOW EXIT, Warszawa, 2016.
[2] Kutluyıl Do˘
gan¸cay. Partial-Update Adaptive Signal Processing. Design, Analysis and Implementation. Academic Press, Oxford, 2008.
[3] S. C. Douglas. Simpliﬁed stochastic gradient adaptive ﬁlters using partial updating. In 1994 Sixth
IEEE Digital Signal Processing Workshop, pages 265–268, 1994.

1189

Simulations of Partial Update LMS Algorithms. . .

D. Bismor

[4] S. C. Douglas. Adaptive ﬁlters employing partial updates. IEEE Transactions on Circuits and
Systems II: Analog and Digital Signal Processing, 44(3):209–216, 1997.
[5] S.C. Douglas. A family of normalized LMS algorithms. IEEE Signal Processing Letters, 1(3):49–51,
March 1994.
[6] S. Elliott. Signal Processing for Active Noise Control. Academic Press, London, 2001.
[7] S. M. Kuo and D. R. Morgan. Active Noise Control Systems. John Wiley & Sons, New York,
1996.
[8] M. Lawry´
nczuk. Approximate state-space model predictive control. In Methods and Models in
Automation and Robotics (MMAR), 2015 20th International Conference on, pages 770–775, Aug
2015.
[9] Khaled Mayyas. A variable step-size selective partial update LMS algorithm. Digital Signal
Processing, 23(1):75–85, 2013.
[10] K. Mazur and M. Pawelczyk. Hammerstein nonlinear active noise control with the Filtered-Error
LMS algorithm. Archives of Acoustics, 38(2):197–203, 2013.
[11] I. Pitas. Fast algorithms for running ordering and max/min calculation. IEEE Transactions on
Circuits and Systems, 36(6):795–804, Jun 1989.
[12] Stanislaw Wrona and Marek Pawelczyk. Controllability-oriented placement of actuators for active
noise-vibration control of ﬂexible structures using memetic algorithms. Archives of Acoustics,
38(4):529–536, 2013.

1190

