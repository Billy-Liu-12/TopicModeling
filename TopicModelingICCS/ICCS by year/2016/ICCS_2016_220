Procedia Computer Science
Volume 80, 2016, Pages 1712–1723
ICCS 2016. The International Conference on Computational
Science

A New Design Based-SVM of the CNN Classifier
Architecture with Dropout for Offline Arabic
Handwritten Recognition
Mohamed Elleuch1, Rania Maalej2 and Monji Kherallah3
1

National School of Computer Science (ENSI), University of Manouba, TUNISIA.
2
National School of Engineers (ENIS), University of Sfax, TUNISIA.
3
Faculty of Sciences, University of Sfax, TUNISIA.
mohamed.elleuch.2015@ieee.org, rania.mlj@gmail.com, monji.kherallah@gmail.com

Abstract
In this paper we explore a new model focused on integrating two classifiers; Convolutional Neural
Network (CNN) and Support Vector Machine (SVM) for offline Arabic handwriting recognition
(OAHR) on which the dropout technique was applied. The suggested system altered the trainable
classifier of the CNN by the SVM classifier. A convolutional network is beneficial for extracting
features information and SVM functions as a recognizer. It was found that this model both
automatically extracts features from the raw images and performs classification. Additionally, we
protected our model against over-fitting due to the powerful performance of dropout. In this work, the
recognition on the handwritten Arabic characters was evaluated; the training and test sets were taken
from the HACDB and IFN/ENIT databases. Simulation results proved that the new design based-SVM
of the CNN classifier architecture with dropout performs significantly more efficiently than CNN
based-SVM model without dropout and the standard CNN classifier. The performance of our model is
compared with character recognition accuracies gained from state-of-the-art Arabic Optical Character
Recognition, producing favorable results.
Keywords: CNN, dropout, Arabic handwritten recognition, over-fitting, based-SVM, features, HACDB

1 Introduction and Related Works
During the two last decades, on the basis of signal processing and pattern recognition, offline and
online data classification, has won big concern. As a result, it has been extensively practiced to a
variety of research domains like vision recognition task [1, 2], Automatic Speech Recognition (ASR)
[3] and EEG signal [4] classification.
Lately, Handwriting Recognition has become a popular area of research because of the advances in
technology such as the handwriting capturing devices and impressive mobile computers. Because it is
a challenging topic, Arabic handwritten script recognition, in the domain of handwriting recognition

1712

Selection and peer-review under responsibility of the Scientiﬁc Programme Committee of ICCS 2016
c The Authors. Published by Elsevier B.V.

doi:10.1016/j.procs.2016.05.512

A New Design Based-SVM of the CNN Classiﬁer Architecture with Dropout ...

M. Elleuch et al.

has been deeply studied for a couple of decades by researchers who have utilized dissimilar
algorithms, like Support Vector Machine (SVM), Multi-Layer Perceptron (MLP), Hidden Model
Markov (HMM), Deep Networks (DNN) , Recurrent Neural Networks (RNN) and Convolutional
Neural Networks (CNN), etc.,. The outcomes were various and satisfactory. These machines learning
(ML) systems have demonstrated their reliability and performance in a large domain of applications as
well as winning triumph in optical character recognition (OCR) in Latin and Asian languages [5, 6].
The major drawback of these architectures is the large number of parameters, so over-fitting can
occur.
Considering recognition of offline Arabic handwriting, our researches have highlighted and
insisted more on the recognition aspects. Because of differences in forms, concavities, curvatures, and
strokes, the handwritten characters and overlapping characters are highly varying. For this reason, a
special care and importance was given from our part to the recognition of intricate Arabic handwritten
text. Thanks to this work [7], architecture based on CNN and SVM classifier is investigated to the
handwritten Arabic domain [8]. On the other hand, in this study to prevent our architecture from overfitting and to improve its performance, dropout is applied. This technique consists of temporarily
removing a unit from the network. This removed unit is randomly selected only during the training
stage [9]. This architecture mixes the advantages of the two approaches described below.
Developed by LeCun et al [10], CNN which is hierarchical neural network possesses huge
representational capacity that learns the good features at every layer of the visual hierarchy. It has also
been effectively applied to a lot of vision problems like visual object recognition [11] and handwriting
recognition [12]. These features are automatically extracted from the input image having the benefit of
being invariant to the shift and shape distortions of the input textual images.
On the other hand, Support vector machines (SVM) considered as one of the strongest and robust
algorithm in machine learning (ML) created by Vapnik [13], have become a well–known approach
exploited in many domains [14, 15, 16], like pattern recognition, classification, and image processing.
CNN includes a number of convolutional and sub-sampling layers which are optionally
accompanied by Fully Connected Layers (FCL). The FCL are uniform to the layers in a standard
Multi-Layer Perceptron. Yet, MLP offer two borders in classification tasks: To begin with, there is
absence of theoretical relationship between the classification task and the MLP structure. Next, MLP
drift hyper-planes separation surfaces, in feature representation space, that are not optimal in terms of
margin between the examples of two different classes. To find a suitable solution to these problems, in
our experiments, we modified CNN structure by replacing the output layer of the FCL with an SVM
classifier. The purpose of SVM is to understate the generalization errors in the training set by using
the Structural Risk Minimization (SRM) principle. Consequently, the generalization ability of SVM
surpasses than that of MLP [7].
By presenting a Deep CNNs trained on MNIST [17] as well as on NIST SD 19 database [18]
including lower and upper case letters and digit, Ciresan et al [19] proved the robustness of their
model by constructing seven CNNs. We can consider the average error rate obtained as best results.
Later, a new hybrid CNN/SVM model was proposed by Niu and Suen [7] to solve the handwritten
digit recognition problem exploiting MNIST digits database. It is noticeable that error classification
rate gained by the hybrid model has achieved better results. Théodore et al [20] inquired into the
combination of convolutional neural networks and hidden Markov models for handwritten word
recognition and they achieved satisfactory results by using CNN/HMM hybrid model on IAM [21]
and Rimes [22] databases.
Another classifier which is used extensively is Support Vector Machines (SVM). Survey
applications of pattern recognition were presented by Byun and Lee [23]. They used SVM and they
reviewed seven categories based on their aims like face detection/verification, object recognition,
handwritten character/digit recognition and others while Chen et al. [24] presented a recognition
system using SVM. The efficiency of Gabor features was proved over the previous used features
techniques for Arabic sub-word recognition. Recently, Elleuch et al. [25] investigated the performance

1713

A New Design Based-SVM of the CNN Classiﬁer Architecture with Dropout ...

M. Elleuch et al.

of Deep Network using SVM classifier (DSVM) to recognize Arabic handwritten text using HACDB
Database. DSVM permits to extract high level discriminative features with support vectors
maximizing the margin and it guarantees generalization performance alike. The experimental study
has proved favorable results comparable to the state-of-the-art Arabic OCR.
Most of these networks and especially who have a deep architecture as CNN, Deep CNN, RNN
and DNN etc., are characterized by a large number of hidden layers and too many parameters.
However, over-fitting is a serious problem in such networks. Dropout is a technique for addressing
this problem [9]. This technique was successfully applied with several types of neural networks and it
shows a significant improvement for a recognition rate [9, 26, 27, 28].
Hinton et al. [9] introduced dropout training as a way to control over-fitting by randomly omitting
subsets of features at every iteration of a training procedure. They showed that dropout improves the
performance of neural networks on supervised learning tasks in vision, speech recognition, document
classification and computational biology, obtaining state-of-the-art results on many benchmark data
sets.
Until recently, no researchers have had applied CNN and SVM approaches yet to the handwritten
Arabic field. In this study, a new design based-SVM of the CNN classifier architecture with dropout
for offline Arabic handwritten text recognition has been suggested. Purposely, we study the plausible
advantages of the proposed CNN and SVM classifier without and with dropout; CNN based-SVM
model took the CNN as an automatic feature extractor from raw images and it let SVM do
classification by analyzing the error classification rate on the Arabic handwritten character
classification task. Dropout training is an effective way to control over-fitting by randomly omitting
subsets of features at every iteration of a training procedure.
The organization of the rest of the paper is the following. In Section 2, we introduce the basic
concepts behind Convolutional Neural Networks (CNN) and Support Vector Machine (SVM)
classifiers. CNN based-SVM model designed for Arabic handwriting recognition was presented, and
then dropout adapted for this model was described. Our experimental study and results are given and
analyzed in Section 3. Finally, Section 4 presents some concluding remarks.

2 System Overview
In this section we briefly summarize Convolutional Neural Networks and Support Vector Machine
classifier. We then describe our proposed CNN based-SVM model with dropout for offline Arabic
handwriting recognition (OAHR).

2.1 CNN Classifier
Being hierarchical, multi-layer neural networks with a deep supervised learning architecture
trained with the back-propagation algorithm [10], Convolutional Neural Networks are composed of an
automatic feature extractor and a trainable classifier. CNN are exploited to learn complex, highdimentional data, and differ in how convolutional and sub-sampling layers are inquired into. The
difference is in their architecture. Many CNN architectures are suggested for different problems
among which object recognition [29] and handwriting digit/character recognition [10, 30]. The best
performance on pattern recognition task was achieved. In addition, to guarantee some degree of
invariance to scale, shift and distortion, CNN mix three main hierarchical aspects such as local
receptive fields, weight sharing and spatial sub-sampling [10].
As shown in Fig. 1, the net represents a typical Convolutional Neural Network architecture for
handwritten character recognition. It includes a set of several layers. Initially, the input is convoluted
with a set of filters (C hidden layers) in order to obtain the values of the feature map. Next, in order to
diminish the dimensionality (S hidden layers) of the spatial resolution of the feature map, each

1714

A New Design Based-SVM of the CNN Classiﬁer Architecture with Dropout ...

M. Elleuch et al.

convolution layer is pursued by a sub-sampling layer. Convolutional layers alternate sub-sampling
layers constitute the feature extractor to retrieves discriminating features from the raw images.
Ultimately, these layers were pursued by two fully connected layers (FCL) and the output layer. The
output of the previous layer is taken by each layer as the input.

Figure 1: A typical CNN architecture composed of layers for feature maps applied for Arabic
handwritten characters recognition.

2.2 SVM Classifier
Developed by Vapnik [13] and Cortes [31], Support Vector Machine is powerful discriminative
classifier. It has been widely exploited with positive results for many pattern classification/recognition
tasks [32]. It’s regarded as the state-of-the-art tool for resolving linear and non-linear (see figure 2)
classification problems [13], thanks to its parsimony, flexibility, prediction capacity and the global
optimum character. The basis of their formulation is the structural risk minimization, rather than the
empirical risk minimization which is traditionally used in artificial neural networks [13].
SVM is basically used to determine an optimal separating hyper-plane (equation 1) or decision
surface by embracing a novel technique based on mapping the sample points into a high-dimensional
feature space and it is categorized using a nonlinear transformation Φ, even when the data are linearly
inseparable. The optimal hyper-plane is gained by solving a quadratic programming problem which is
reliant on regularization parameters. This transformation was carried out by kernel functions like
linear, radial basis function, sigmoid and polynomial kernel types;
x The linear kernel:
K(x, y) = x × y
x The polynomial kernel: K(x, y) = [(x × y) + 1] d
x The Sigmoid kernel:
K(x, y) = tanh (β0 x y + β1)
x RBF kernel (Radial Basis Function): K(x, y) = exp(- γ ||x - y||2)
With d, β0, β1, and γ are parameters that will be determinate empirically.
ƒ(x) = WT Φ(x) + b
Where W ϵ Rn , b ϵ R and Φ(x) is a feature map.

(1)

In this work, because the feature space is linearly inseparable, we applied a transformation by
mapping the input data (xi, yi) into a higher dimensional feature space by using a nonlinear operator
Φ(x). As a result, the optimal hyper-plane can be defined as:
(2)
ƒ(x) = sgn( ∑ yiαi K(xi , x) + b )
2
Where K(xi, x) = exp(-γ||xi - x|| ) is the kernel function founded on a radial basis function (RBF),
and sgn(.) is the sign function. This classifier model called RBF kernel SVM is added to replacing the
last output layers of the CNN architecture to carry out classification for Arabic Handwritten Text.

1715

A New Design Based-SVM of the CNN Classiﬁer Architecture with Dropout ...

M. Elleuch et al.

Figure 2: Principle of Support Vector Machine; (a) two-class hyper-plane example, (b) one-versus-all method

2.3 Architecture of the Proposed ML System
In this section, we present the architecture of our OAHR system based on CNN and SVM, wherein
CNN is considered as a deep learning algorithm, on which the dropout technique has been applied
during training. Our proposed system was tailored by altering the trainable classifier of the CNN with
an SVM classifier. Our target is to mix the CNN respective capacities and the SVM to obtain a new
effectual recognition system inspired by the two formalisms.
We showed the network architecture of the CNN based-SVM model in Fig. 3. It was noted that it
appears like as follow. Firstly, the first layer welcomes raw image pixels as input. Secondly, the
second and fourth layer of the network is convolution layers alternator with sub-sampling layers,
which take the pooled maps as input. Consequently, they are able to extract features that are more and
more invariant to local transformations of the input image. FCL is the sixth layer which consists of N
neurons. The final layer was substituted by SVM with an RBF kernel for classification. Because of
using a huge number of data and parameters, over-fitting can occur. So to prevent our network from
this problem and to improve it, dropout is applied. This technique consists of temporarily removing a
unit from the network. This removed unit is randomly selected only during the training. Dropout is
applied only at FCL layer and for more precisely, it is applied to feed-forward connections
(perceptron). This choice is based on the fact that since the convolutional layers don't have a lot of
parameters, over-fitting is not a problem and therefore dropout would not have much effect [26].
The outputs from the hidden units are taken by the SVM as a feature vector for the training
process. After that, the training stage continues till realizing good trained. Finally, classification on the
test set was performed by the SVM classifier with such automatically extracted features.
The structure of the CNN based-SVM model with dropout adopted in our experiments is presented
in Section 3, paragraph 3.3.

Figure 3: Architecture of the CNN based-SVM model with dropout.

1716

A New Design Based-SVM of the CNN Classiﬁer Architecture with Dropout ...

M. Elleuch et al.

3 Experiments, Results and Discussion
We carried our experimental studies so that we could explore the efficiency of dropout technique
by using CNN based-SVM model in order to recognize offline Arabic character. We tested this new
architecture of CNN on HACDB database [33] and IFN/ENIT database [34]. Outcomes are itemized
and discussed in the following subsections.

3.1 HACDB and IFN/ENIT Databases
The HACDB database [33] contains 6.600 shapes of handwritten characters written by 50 persons
(Figure 4-b). Each writer has generated two forms for 66 shapes: 58 shapes of characters and 8 shapes
of overlapping characters (representing 24 basic characters/overlapping characters without dots). The
dataset is divided into a training set of 5.280 images and a test set of 1.320 images [33]. The
IFN/ENIT database [34] consists of 26.459 Arabic words handwritten by more than 411 different
writers. The handwritten words represent 937 Tunisian town/village names. The images are partitioned
into four sets (a-d). It is one of the most widely used databases. In this study words are segmented into
letters from set (a) and (b). We have kept 1.120 images as test data. These images include 56 shapes of
characters (Figure 4-a). The both databases consist of gray scale images normalized 28 by 28 pixels.
Details of the class for each shape are presented in Table 1.

Figure 4: Samples from (a) the IFN/ENIT database and (b) the HACDB database written by 10 different
writers.
Arabic Script
Aeen (‫)ﻉ‬

Shape

class

Shape

class

1

34

2

35

3*

Alif (‫)ﺍ‬

Arabic Script

Lam_Alif (‫)ﻻ‬

36

4

37 *

5*

38

6

Lam_Jeem (‫)ﻟﺣـ‬

39 *

7

Lam_Mem (‫)ﻟﻣـ‬

40 *

8

Lam_Mem_Jeem
(‫)ﻟﻣﺣـ‬

41 *

1717

A New Design Based-SVM of the CNN Classiﬁer Architecture with Dropout ...

M. Elleuch et al.

Alif_Lam_Jeem
(‫)ﺍﻟﺣـ‬

9*

Baa (‫)ﺏ‬

10

43

11

44

12

45

Daal (‫)ﺩ‬

Meem (‫)ﻡ‬

13

Mem_Jeem (‫)ـﻣﺣـ‬

14

Noon (‫)ﻥ‬

15
Faa (‫)ﻑ‬

16

Haa (‫)ﻩ‬

46 *
47
48

Raa (‫)ﺭ‬

17
18

42

49
50

Saad (‫)ﺹ‬

51

19

52

20

53

21

54

22

Seen (‫)ﺱ‬

55

23

56 *

24

57

Hamza (‫)ء‬

25

58 *

Jeem (‫)ﺝ‬

26

59

27

60

28

Taa (‫)ﻁ‬

29
Kaaf (‫)ﻛـ‬

30

62
Waao (‫)ﻭ‬

31
Laam (‫)ﻝ‬

32

61

63
64

Yaa (‫)ﻯ‬

33

65
66

Table 1: Class for each shape of an Arabic script
* Classes does not exist in IFN/ENIT database

3.2 System Settings
For the purpose of evaluating the efficiency of the proposed CNN based-SVM trainable feature
extractor model without and with dropout, we inquired its working performance to train and recognize
characters of HACDB database. We observed that convolutional networks need a huge number of
samples to learn the parameters. Hence, in order to best train the model on further data so that we can
better take account the variability of handwriting, we spread out the size of the training set ten times
by the technique of elastic deformation suggested by Simard et al [35]. Also, to evaluate the working
of our system, the IFN/ENIT database is exploited. We give the technical implementation details of
the adopted system in the following sub-section.
For the pre-processing, the HACDB Database, utilized in this experiment study, does not require to

1718

A New Design Based-SVM of the CNN Classiﬁer Architecture with Dropout ...

M. Elleuch et al.

be normalized (noise reduction, segmentation). Some basic pre-processing tasks are a must in order to
be performed during the database development. Yet, as for IFN/ENIT and for the sake to gain better
character images after segmentation of words, pre-processing step implies binarization, noise
reduction and filtrating the input text image to improve the quality of the image. As
for feature extraction, CNN is utilized in this experiment as a compact end-to-end
model, consequently the input to the network is the raw images. Finally, for the parameters
setting: For the setting architecture, we have to define the number of convolutional layers, size of
the feature maps, weights, kernel, and biais in each layer of CNN. After that, defining the
optimal kernel parameter and penalty parameter of SVM.

3.3 Experiments using CNN based-SVM Model
In this section, we investigated the performance of the CNN based-SVM model with dropout for
training and recognizing Arabic characters. We parameterize a convolutional layer for the setting
architecture, by the size and the number of the maps, kernel sizes, skipping factors, and the connection
table. About SVM classifier we have to define mainly two parameters of the RBF kernel; Gamma (γ)
and C. We chose the appropriate parameters for our suggested model by making empirical tests. An
experimental study was created to assess the proposed model and we chose the parameters on the basis
of the criterion of the error classification rate on the train set.
CNN based-SVM network architecture with dropout is shown in Fig. 3, utilized in experiments
applied to HACDB database with elastic distortion and is given in the following way: 1×28×28-6C2S12C2S represents a net with input images of size 28 × 28 pixels giving an input dimensionality of 784
with four Convolutional-Subsampling layers that is possibly to be viewed as a trainable feature
extractor. The final output layer of the fully connected hidden layers of CNN was substituted by an
SVM classifier to recognize the anonymous handwriting text. The one-versus-all method with 66 way
is utilized for the multi-class SVM.
It is noted that first convolutional layer “C1” possesses 6 feature maps each having 25 weights,
constituting a 5 × 5 trainable kernel, and a bias. The feature maps’ size is 24 × 24. This guarantees a
low-level feature extraction. The second hidden layer “S1” named sub-sampling includes 2 features
maps with size 12 × 12. This lowers their sensitivity to shifts, variations in scale and distortions and
rotation. The third layer “C2” possesses 12 convolutional maps of size 8 × 8 and the fourth layer “S2”
possesses 2 sub-sampling maps of size 4 × 4. When training this architecture, the feature maps of the
4th layer are merged into a feature vector feeds into the fully connected layers. Then, dropout was
applied to convolutional neural networks only to fully connected hidden layers with the probability of
retaining a hidden unit p=0.5. We temporarily remove 50% of nodes. Those units are randomly
selected only during the training stage. Finally, the SVM takes the outputs from the hidden units as a
feature vector for classification.
We noted that in our experiments LIBSVM [36] is utilized to construct multi-class SVM with RBF
kernel. We successfully found that the selection of the best parameters (C, γ) to be experimentally
efficient by using cross-validation method. The optimal values of main parameters got after the tests
on the training Arabic handwritten text database HACDB were synthesized in table 2. After training,
we test our best network with two test sets; the first contains 1.320 images of the HACDB database
and the second composes of 1.120 images extracted from sets (a) and (b) of IFN/ENIT database.
CNN based-SVM Model with dropout
Learning rate
0.8
Kernel parameter γ
Batch size
40
Penalty parameter C

2
30

Table 2: Training parameters for CNN based-SVM model

1719

A New Design Based-SVM of the CNN Classiﬁer Architecture with Dropout ...

M. Elleuch et al.

3.4 Results and Discussion
The new architecture of CNN classifier, introduced in this work, enabled to couple a CNN method
with RBF kernel SVM classifier. In order to improve this architecture, we added a dropout technique.
Our proposed system was compared to a number of other methods being recently proposed.
The best performing convolutional neural network achieve an error classification rate of 14.71%
[37]. A novel architecture of CNN based on SVM as well further reduces the error to 6.59% [8] on the
testing HACDB dataset with 66 classes. Adding dropout to the CNN based-SVM model, only to the
fully connected hidden layers, reduces the error to 5.83% (see figure 5). Even more decrease in error
rate was obtained by our system using dropout technique applying with IFN/ENIT database (see Table
3). Our network was trained using gradient descent for 200 epochs.

Figure 5: Samples of 77 misclassified characters using CNN based-SVM model with dropout

HACDB database
Approach
CNN [37]
CNN based-SVM
CNN based-SVM with
Dropout

IFN/ENIT database

24 classes

66 classes

56 classes

5%
2.65 %

14.71 %
6.59 %

7.32 %

2.09 %

5.83 %

7.05 %

Table 3: Error rate for our proposed systems applied on HACDB and IFN/ENIT Databases
It was obviously noted that our model based on dropout outperforms any other methods. In table 3,
it is shown that CNN based-SVM model with dropout outperforms highly the basic CNN classifier
and lightly CNN based-SVM model without dropout once tested on HACDB database (dropped by
0.76% with 66 classes and 0.56% with 24 classes) or IFN/ENIT database (dropped by 0.27%). It
proved that there is quite an enormous gain in error classification rate (ECR) compared to typical CNN
in which the absolute recognition rate was ameliorated by 8.88% with the suggested CNN and SVM
system with dropout for the 66 class problem. We came to a result that the error rate gained with the
CNN based-SVM model with dropout (with 66 classes) on the HACDB database equals to 5.83%

1720

A New Design Based-SVM of the CNN Classiﬁer Architecture with Dropout ...

M. Elleuch et al.

which means that it is statistically significantly important compared to character recognition
accuracies obtained from state-of-the-art offline digit/Latin/Arabic systems (see Table 4) mainly that
they were gained with raw data without any feature extraction step [19,20]. Again the same
architecture is used with the 24 class problem; error rate decrease approximately two times giving
2.09% when applied dropout technique in CNN based-SVM model.
Authors

Methods

Databases (class)

Present work

CNN based-SVM
with Dropout

HACDB (24)
HACDB (66)
IFN/ENIT (56)

Lawgali et al [38]

ANN

Arabic characters
(old version of
HACDB)

Azeem
Ahmad [39]

HMM (128
Mixtures)

IFN/ENIT

and

Ciresan et al [19]

Deep CNN

Théodore et al
[20]

CNN/HMM

Chen et al [24]

SVM

NIST [40] (62)
NIST (52)
Rimes database [22]
IAM database [21]
AMA Arabic Dataset
(34) [41]

ECR
2.09%
5.83%
7.05%
3.44%
(1600 shapes)
21.18%
(5600 shapes)
7%
11.88%
21.41%
19.5%
10%
17.3%

Table 4: Performance comparisons with other methods
A comparative study of the performance of our architecture was also performed with other
methods utilizing handwritten Arabic database. Our CNN based-SVM model with dropout still
outperforms hand-crafted features-based approach stating as example ANN, HMM and SVM methods
[24, 38, 39]. In fact, CNN, with automatic feature extractor stage, deduces features that differentiate
between characters, and then SVM classifier insists on predicting the correct class of character. These
learned features, being more robust than computed hand-crafted features, establish an adequate
representation for characters. Thanks to these experimental results which enabled us to achieve higher
recognition rates, it is demonstrated that the proposed architecture of CNN based-SVM outperforms
the other current methods. Furthermore, additional gain in performance was obtained by adding
dropout technique in the fully connected hidden layers.

4 Conclusion and Perspectives
In this study, we have explored the applicability of dropout in our CNN based-SVM model on
Arabic handwritten recognition and demonstrated the efficiency of the system for Arabic handwritten
character recognition applied on HACDB and IFN/ENIT databases.
Overall, we deduce that CNN based on SVM classifier offers the state-of-the-art significant results
without much emphasis on feature extraction and pre-processing stages. CNN based-SVM model is
indeed a full of promise classification method in the handwriting recognition domain. Yet, it is a must,
to extend our proposed architecture so that we will be able to deal with handwritten words in different
languages and to enhance the recognition rate. In addition, ensembles of pre-trained and fine-tuned
CNNs could be explored as well.

1721

A New Design Based-SVM of the CNN Classiﬁer Architecture with Dropout ...

M. Elleuch et al.

References
[1] H. Lee, R. Grosse, R. Ranganath, A. Y. Ng, “Unsupervised learning of hierarchical
representations with convolutional deep belief networks,” Communications of the ACM, 54(10)
(2011) 95-103.
[2] G.-B. Huang, H. Zhou, X. Ding, R. Zhang, “Extreme Learning Machine for Regression and
Multiclass Classification,” IEEE Transactions on Systems, Man, and Cybernetics - Part B:
Cybernetics, vol. 42(2), pp. 513-529, 2012.
[3] H. Lee, P.T. Pham, Y. Largman, A.Y. Ng, “Unsupervised feature learning for audio classification
using convolutional deep belief networks,” Advances in Neural Information Processing Systems
(NIPS), pp. 1096-1104, 2009.
[4] Y. Ren, Y. Wu, “Convolutional Deep Belief Networks for Feature Extraction of EEG Signal,”
International Joint Conference on Neural Networks (IJCNN), pp. 2850-2853, 2014.
[5] D.C. Ciresan, U. Meier, J. Schmidhuber, “Transfer Learning for Latin and Chinese Characters
with Deep Neural Networks,” In Proceedings of International Joint Conference on Neural
Networks, 2012.
[6] D.C. Ciresan, J. Schmidhuber, “Multi-Column Deep Neural Networks for Offline Handwritten
Chinese Character Classification,” In Proceedings of CoRR. 2013.
[7] X.-X. Niu, Ching Y. Suen, “Novel hybrid CNN–SVM classifier for recognizing handwritten
digits,” Pattern Recognition, vol. 45, pp. 1318-1325, 2012.
[8] M. Elleuch, N. Tagougui and M. Kherallah, “A Novel Architecture of CNN based on SVM
Classifier for Recognizing Arabic Handwritten,” International Journal of Intelligent Systems
Technologies and Applications, vol. 15, 2016, in press.
[9] G. E. Hinton, N. Srivastava, A. Krizhevsky, I. Sutskever, and R. R. Salakhutdinov, “Improving
neural networks by preventing co-adaptation of feature detectors,” arXiv preprint
arXiv:1207.0580, 2012.
[10] Y. LeCun, L. Bottou, Y. Bengio, P. Haffner, “Gradient-based learning applied to document
recognition,” Proceedings of the IEEE, vol. 86(11), pp. 2278-2324, 1998.
[11] D. C. Ciresan, U. Meier, J. Masci, L. M. Gambardella, and J. Schmidhuber, “High performance
neural networks for visual object classi_cation,” Technical Report IDSIA-01-11, Dalle Molle
Institute for Artificial Intelligence, 2011.
[12] Y. LeCun, B. Boser, J. S. Denker, D. Henderson, R. E. Howard, W. Hubbard, and L. D. Jackel,
“Backpropagation applied to handwritten zip code recognition,” Neural Computation, vol. 1, pp.
541-551, 1989.
[13] V. Vapnik, “Statistical Learn Theory,” John Wiley, New York, 1998.
[14] H. Byun, S.-W. Lee, “A survey on pattern recognition applications of Support Vector Machines”,
International Journal of Pattern Recognition and Artificial Intelligence, vol. 17, pp. 459-486,
2003.
[15] D. Gorgevik, D. Cakmakov, V. Radevski, “Handwritten digit recognition by combining support
vector machines using rule-based reasoning,” Proc. 23rd Int. Conf. Information Technology
Interfaces (ITI), pp. 139-144, 2001.
[16] G. Guo, S. Z. Li, K. Chan, “Face Recognition by Support Vector Machines,” Proc. 4th IEEE Intl.
Conf. on Face and Gesture Recognition, pp. 196-201, 2000.
[17] MNIST: http://yann.lecun.com/exdb/mnist/
[18] P. J. Grother, “Nist special database 19 - handprinted forms and characters database,” National
Institute of Standards and Thechnology (NIST), Tech. Rep., 1995.
[19] D. C. Ciresan, U. Meier, L. M. Gambardella, J. Schmidhuber, “Convolutional Neural Network
Committees For Handwritten Character Classification,” 11th International Conference on
Document Analysis and Recognition (ICDAR), 2011.
[20] B. Théodore, N. Hermann, K. Christopher, “Tandem HMM with convolutional neural network for
handwritten word recognition,” In: 38th International Conference on Acoustics Speech and Signal
Processing (ICASSP), pp. 2390-2394, 2013.
[21] U. V. Marti and H. Bunke, “The IAM-database: an English sentence database for offline
handwriting recognition,” International Journal on Document Analysis and Recognition, vol. 5,
no. 1, pp. 39-46, 2002.
[22] E. Augustin, M. Carré, E. Grosicki, J.-M. Brodin, E. Geoffrois, F. Preteux, “RIMES evaluation
campaign for handwritten mail processing,” In Workshop on Frontiers in Handwriting
Recognition, 2006 (1).

1722

A New Design Based-SVM of the CNN Classiﬁer Architecture with Dropout ...

M. Elleuch et al.

[23] H. Byun, S.-W. Lee, “Applications of Support Vector Machines for Pattern Recognition: A
Survey,” In Proceedings of the First International Workshop, SVM 2002, pp. 213-236, 2002.
[24] J. Chen, H. Cao, R. Prasad, A. Bhardwaj, P. Natarajan, “Gabor features for offline arabic
handwriting recognition,” In Proceedings of the 9th IAPR International Workshop on Document
Analysis Systems (DAS), pp. 53-58, 2010.
[25] M. Elleuch and M. Kherallah, “An Improved Arabic Handwritten Recognition System using Deep
Support Vector Machines,” International Journal of Multimedia Data Engineering and
Management, vol. 7(2), pp. 1-14, 2016.
[26] N. Srivastava, G. E. Hinton, A. Krizhevsky, I. Sutskever and R. Salakhutdinov, “Dropout: A
simple way to prevent neural networks from overfitting,” The Journal of Machine Learning
Research, 2014, vol. 15, no 1, pp. 1929-1958.
[27] R. Maalej, N. Tagougui and M. Kherallah, “Online Arabic Handwriting Recognition with
Dropout Applied in Deep Recurrent Neural Networks,” In 12th IAPR International Workshop on
Document Analysis Systems (DAS), 2016, in press.
[28] V. Pham, T. Bluche, C. Kermorvant, and J. Louradour, “Dropout improves recurrent neural
networks for handwriting recognition,” arXiv preprint arXiv:1312.4569, 2013.
[29] Y. LeCun, F.J. Huang, L. Bottou, “Learning methods for generic object recognition with
invariance to pose and lighting,” Proc. Computer Vision and Pattern Recognition Conference
(CVPR), IEEE Press, 2004.
[30] M. Ranzato, F. Huang, Y. Boureau, Y. LeCun, “Unsupervised learning of invariant feature
hierarchies with applications to object recognition,” Proc. Computer Vision and Pattern
Recognition Conference (CVPR), IEEE Press, 2007.
[31] C. Cortes, V. Vapnik, “Support vector networks,” Machine Learning, vol. 20, pp. 273-297, 1995.
[32] C. Burges, “A tutorial on support vector machines for pattern recognition,” Data Mining
Knowledge Discovery, vol. 2(2), pp. 121-167, 1998.
[33] A. Lawgali, M. Angelova, A. Bouridane, “HACDB: Handwritten Arabic characters database for
automatic character recognition,” EUropean Workshop on Visual Information Processing
(EUVIP), pp. 255-259, 2013.
[34] M. Pechwitz, S.S. Maddouri, V. Märgner, N. Ellouze and H. Amiri, “IFN/ENIT database of
handwritten Arabic words,” In: Colloque International Francophone sur l’Ecrit et le Document
(CIFED), pp. 127-136, 2002.
[35] P. Simard, D. Steinkraus, J. C. Platt, “Best Practices for Convolutional Neural Networks Applied
to Visual Document Analysis,” International Conference on Document Analysis and Recognition
(ICDAR), pp. 958-962, 2003.
[36] C.C. Chang, C.J. Lin, “LIBSVM: A Library for Support Vector Machines,” Software Available at
http://www.csie.ntu.edu.tw/~cjlin/libsvm, 2001.
[37] M. Elleuch, N. Tagougui, and M. Kherallah, “Towards Unsupervised Learning for Arabic
Handwritten Recognition Using Deep Architectures,” Neural Information Processing - 22nd
International Conference, ICONIP 2015, Istanbul, Turkey, part. (1), pp. 363-372, 2015.
[38] A. Lawgali, A. Bouridane, M. Angelova and Z. Ghassemlooy, “Handwritten Arabic character
recognition: Which feature extraction method?,” International Journal of Advanced Science and
Technology, vol. 34, pp. 1-8, 2011.
[39] S. A. Azeem and H. Ahmed, “Effective technique for the recognition of offline Arabic
handwritten words using hidden Markov models,” International Journal on Document Analysis
and Recognition (IJDAR), vol. 16(4), pp. 399-412, 2013.
[40] P. J. Grother, “Nist special database 19 - handprinted forms and characters database,” National
Institute of Standards and Thechnology (NIST), Tech. Rep., 1995.
[41] Applied Media Analysis, Arabic-Handwritten-1.0, http://appliedmediaanalysis.com/Datasets.htm.
2007.

1723

