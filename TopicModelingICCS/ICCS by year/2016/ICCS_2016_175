Procedia Computer Science
Volume 80, 2016, Pages 1745–1754
ICCS 2016. The International Conference on Computational
Science

Ramp Loss Linear Programming Nonparallel
Support Vector Machine
Dalian Liu1,2 , Dandan Chen3,4 , Yong Shi1,4,5,6 , and Yingjie Tian4,5
1

School of Computer and Information Technology, Beijing Jiaotong University, Beijing 100044, China
2
Department of Basic Course Teaching, Beijing Union University, Beijing 100101, China
ldlluck@sina.com
3
College of Mathematical Sciences, University of Chinese Academy of Sciences, Beijing 100049, China
4
Research Center on Fictitious Economy and Data Science, University of Chinese Academy of Sciences, Beijing
100190, China, chendandan1@126.com
5
Key Laboratory of Big Data Mining and Knowledge Management, University of Chinese Academy of Sciences,
Beijing 100190, China
6
College of Information Science and Technology, University of Nebraska at Omaha, Omaha, NE 68182, USA
yshi@ucas.ac.cn, tyj@ucas.ac.cn

Abstract
Motivated by the fact that the l1 -penalty is piecewise linear, we proposed a ramp loss linear programming nonparallel support vector machine (ramp-LPNPSVM), in which the l1 -penalty is applied for the
RNPSVM, for binary classiﬁcation. Since the ramp loss has the piecewise linearity as well, rampLPNPSVM is a piecewise linear minimization problem and a local minimum can be effectively found
by the Concave Convex Procedure and experimental results on benchmark datasets conﬁrm the effectiveness of the proposed algorithm. Moreover, the l1 -penalty can enhance the sparsity.
Keywords: support vector machine, nonparallel, CCCP, linear programming, ramp loss

1

Introduction

As the computationally powerful tools for pattern classiﬁcation, support vector machines (SVMs) developed fast [1, 4, 5, 25, 26]. Recently, the nonparallel hyperplane SVM is proposed and has attracted many
interests, such as the generalized eigenvalue proximal support vector machine (GEPSVM) [11] and the
twin support vector machine (TWSVM) [7]. For the binary classiﬁcation problem, TWSVM seeks
two nonparallel proximal hyperplanes such that each hyperplane is closer to one of the two classes
and is at least one distance from the other. It is implemented by solving two smaller quadratic programming problems (QPPs) instead of a larger one, which increases the TWSVM training speed by
approximately fourfold compared to that of standard SVM. TWSVMs have been studied extensively
[2, 8, 9, 13, 14, 15, 16, 17, 18, 19, 23, 22], in which the nonparallel support vector machine (NPSVM)
[23] is superior theoretically and overcomes several drawbacks of the existing TWSVMs.
Selection and peer-review under responsibility of the Scientiﬁc Programme Committee of ICCS 2016
c The Authors. Published by Elsevier B.V.

doi:10.1016/j.procs.2016.05.432

1745

Ramp Loss Linear Programming Nonparallel Support Vector Machine

Dalian Liu, etal.

However, researchers have shown that classical SVMs or TWSVMs are sensitive to the presence of
outliers and yield poor generalization performance, since the outliers tend to have the largest margin
losses according to the character of the convex loss functions used in them, such as the convex loss
functions such as the Hinge loss function and ”-insensitive loss function. Therefore, several methods
are applied to construct the robust SVM models [3, 6, 10, 12, 20, 21, 24, 29], of which the ramp loss
function has been investigated widely in the theoretical literature in order to improve the robustness of
SVMs. They constructed a ramp loss support vector machine (RSVM) by taking the Ramp loss instead
of the Hinge loss in the classical SVM, the Ramp loss function limits its maximal loss value distinctly
and can put deﬁnite restrictions on the inﬂuences of outliers so that it is much less sensitive to their
presence. However, it will also cause the objective of SVMs losing convexity, as a consequence, the
concave-convex programming (CCCP) procedure is applied to solves a sequence of convex problems to
produce faster and sparser SVMs. For the NPSVM [23], by introducing the ramp loss function and also
propose a new non-convex and non-differentiable loss function based on the ε-insensitive loss function,
a novel ramp loss NPSVM termed as RNPSVM is proposed [11], which can explicitly incorporate noise
and outlier suppression in the training process, has less support vectors and the increased sparsity leads
to its better scaling properties.The non-convexity of RNPSVM can be efﬁciently solved by the Concave
Convex Procedure.
In this paper, we propose a ramp-LPNPSVM based on our proposed ramp loss NPSVM (RNPSVM)
[23], which implies that the algorithm proposed later involves no more quadratic programming problems (QPPs) but linear programming problems (LPPs). Similarly to RNPSVM, the proposed rampLPNPSVM enjoys the robustness and sparsity. The problems related to ramp-NPLPSVM leads to a
polyhedral concave problem, which minimizes a concave function on one polyhedron. Moreover, rampNPLPSVM has the piecewise linear objective functions, which made a sequence of LPPs to be solved
efﬁciently in the CCCP procedure. The rest of this paper is organized as follows. Section 2 brieﬂy
dwells on the hinge loss SVM, ramp Loss SVM and ramp loss NPSVM. Section 3 proposes the rampLPNPSVM and its correponding algorithm. Section 4 deals with experimental results and Section 5
contains concluding remarks.

2

Background

In this section, we brieﬂy introduce hinge loss SVM, ramp loss SVM and RNPSVM [11].

2.1

Hinge Loss SVM

Given a training set
T = {(x1 , y1 ), · · · , (xl , yl )}

(1)

where xi ∈ Rn , yi ∈ Y = {1, −1}, i = 1, · · · , l, the Hinge Loss SVM relies on the classical Hinge
loss function
Hs (z) = max(0, s − z)
(2)
to be formulated as the following optimization problem
min
w,b

1
w
2

2

l

H1 (yi f (xi )),

+C

(3)

i=1

where f (x) is the decision function with the form of f (x) = (w · Φ(x)) + b, and Φ(·) is the chosen
feature map, often implicitly deﬁned by a Mercer kernel K(x, x ) = (Φ(x) · Φ(x )) [26]. Hinge loss
SVM has the sensitivity to outlier observations and its generalization performance is degraded [27].
1746

Ramp Loss Linear Programming Nonparallel Support Vector Machine

2.2

Dalian Liu, etal.

Ramp Loss SVM

Ramp loss SVM increases the robustness of SVM by the ramp loss function [3]
⎧
z>1
⎨ 0,
1 − z, s z 1
Rs (z) =
⎩
1 − s,
z<s

(4)

which makes the loss function ﬂat for scores z smaller than a predeﬁned value s < 1. Rs (z) can be
decomposed into the sum of the convex Hinge Loss and a concave loss,
Rs (z) = H1 (z) − Hs (z),

(5)

therefore the ramp loss SVM (RSVM) is formulated as a following optimization problem
1
w
2

2

1
w
=
2

2

min
w,b

l

Rs (yi f (xi ))

+C
i=1

l

l
i=1

(6)

Hs (yi f (xi )),

H1 (yi f (xi )) −C

+C

i=1

convex

concave

4

4

3

3

3

2

2

2

1

0

−1

1

0

s=−1

0
z

1

2

−3
−3

3

(a) The Ramp Loss

0

−2

−2

−2

1

−1

−1

−2

−3
−3

Concave Loss

4

Hinge Loss

Ramp Loss

which can be solved by the Concave-Convex Procedure (CCCP) [28]

−2

−1

0
z

1

2

−3
−3

3

(b) The Hinge Loss

−2

s=−1

0
z

1

2

3

(c) The Concave Loss

4

4

3

3

3

2

2

2

1
0
-1

ε−insensitive loss

4

ε−insensitive loss

ε−insensitive loss

Figure 1: [11] The Ramp Loss function (left) can be decomposed into the sum of the convex Hinge Loss
(middle) and a concave loss (right).

1
0
-1
-2

-2
-3
-3

-2

-1

0

1

2

z

(a) The Ramp Loss

3

-3
-3

1
0
-1
-2

-2

-1

0

1

2

z

(b) The Hinge Loss

3

-3
-3

-2

-1

0

1

2

3

z

(c) The Concave Loss

Figure 2: [11] Ramp ε-insensitive Loss function (left) can be decomposed into the sum of the convex
ε-insensitive Loss (middle) and a Concave loss (right).

1747

Ramp Loss Linear Programming Nonparallel Support Vector Machine

2.3

Dalian Liu, etal.

RNPSVM

Given the training set
T = {(x1 , +1), · · · , (xp , +1), (xp+1 , −1), · · · , (xp+q , −1)}

(7)

RNPSVM seeks two nonparallel hyperplanes f+ (x) = (w+ · Φ(x)) + b+ = 0 and f− (x) = (w− ·
Φ(x)) + b− = 0 by solving two problems
1
w+
min
w+ ,b+ 2

p
2

Rε,t (f (xi )) + C2

+ C1

p+q
2

Rs (−f + (xj ))

(8)

Rs (−f − (xj ))

(9)

j=p+1

i=1

and
1
min
w−
w− ,b− 2

p+q

+

p

−

Rε,t (f (xi )) + C4

+ C3

i=1

j=p+1

where Ci > 0, i = 1, · · · , 4 are penalty parameters, and Rε,t is the proposed ε-insensitive ramp loss
function in [11] (see Fig.2(a)),
⎧
|z| > t
⎨ t − ε,
|z| − ε, ε |z| 1
Rε,t (z) =
(10)
⎩
0,
|z| < ε
which makes the ε-insensitive loss function
Iε (z) = max(0, |z| − ε)

(11)

ﬂat for scores z larger than a predeﬁned value t > ε. It is obvious that Rε,t (z) can be decomposed into
the sum of the convex ε-insensitive loss and a concave loss,
Rε,t (z) = Iε (z) − It (z)

(12)

From (5) and (12), the problems (8) and (9) of the RNPSVM is reformulated as
min

w+ ,b+

1
w+
2

2

p

+ C1

Iε (f + (xi )) + C2

i=1

p

H1 (−f + (xj ))

j=p+1
convex
p+q

+

(13)
+

Hs (−f (xj )),

It (f (xi )) + C2

= C1

p+q

j=p+1

i=1

concave

and
min

w− ,b−

1
w−
2

2

p+q

= C3

p+q

+ C3

H1 (−f + (xj ))

i=1

j=p+1

−

p

Iε (f − (xi )) + C4
convex
p

Hs (−f (xj )),

It (f (xi )) + C4
j=p+1

(14)
−

i=1
concave

RNPSVM has been proved to explicitly incorporate noise and outlier suppression in the training
process, has less support vectors and the increased sparsity leads to its better scaling properties.
1748

Ramp Loss Linear Programming Nonparallel Support Vector Machine

3

Dalian Liu, etal.

Ramp Loss Linear Programming NPSVM

In this section, we propose the Ramp Loss Linear Programming NPSVM, termed as ramp- LPNPSVM,
for which the l1 regularization terms are applied, and the algorithm proposed later involves a sequence
of linear programming problems.

3.1

Linear case

We seek the two nonparallel hyperplanes f+ (x) = (w+ · Φ(x)) + b+ = 0 and f− (x) = (w− · Φ(x)) +
b− = 0 by solving two problems
min

w+ ,b+

1
w+
2

p
1

p+q

+

Iε (f (xi )) + C2

+ C1

j=p+1

i=1

p

convex
p+q

+

(15)
+

Hs (−f (xj )),

It (f (xi )) + C2

= C1

H1 (−f + (xj ))

j=p+1

i=1

concave

and
min

w− ,b−

1
w−
2

p+q
1

+ C3

p

−

Iε (f (xi )) + C4

p+q

convex
p

−

= C3

H1 (−f + (xj ))

i=1

j=p+1

(16)
−

Hs (−f (xj )),

It (f (xi )) + C4
i=1

j=p+1
concave
2

2

where we only change the w− and w− in (15) and (16) into the l1 -penalty w+ 1 and w− 1 . We
can see that the two problems has the piecewise linear objective functions being composed of a convex
part and a concave part. Follow the same idea in [11], for the problem with such objective function, the
CCCP algorithm is an efﬁcient iterative procedure that solves a sequence of convex programs. Here we
take the ﬁrst problem as the example, the second is the similar. Let the concave part of the problem (15)
p

Pcav (w+ , b+ ) = −C1

It (f + (xi )) − C2

p+q

Hs (−f + (xj ))

(17)

j=p+1

i=1

The CCCP framework for the problem (15) is constructed as in Algorithm 1.
t
, bt+ ) is non-differentiable at some points, for simpliﬁcation purposes, we introNote that Pcav (w+
duce the sub-gradient notations
δj = −C2 yj

∂Hs (yj f+ (xj ))
=
∂f+ (xj )

C2 ,
0,

if yj f+ (xj ) < s
otherwise

(18)

for j = p + 1, · · · , p + q, and

⎧
−C1 , if f+ (xj ) > t
∂It (f+ (xj )) ⎨
C1 , if f+ (xj ) < −t
=
θj = −C1
⎩
∂f+ (xj )
0,
otherwise

(19)
1749

Ramp Loss Linear Programming Nonparallel Support Vector Machine

Dalian Liu, etal.

Algorithm 1 CCCP for the problem (15)
(1) Initialize (w0 , b0 ), set k = 0;
(2) Construct and solve the problem
p

1
w+
2

min

(∗)
w+ ,b+ ,η+ ,ξ−

(ηi + ηi∗ ) + C2

1 + C1

p+q

t
ξj + Pcav (w+
, bt+ ) · (w+ b+ )

j=p+1

i=1

(w+ · xi ) + b+ ε + ηi , i = 1, · · · , p,
− (w+ · xi ) − b+ ε + ηi∗ , i = 1, · · · , p,

s.t.

(20)

(w+ · xj ) + b+ −1 + ξj , j = p + 1, · · · , p + q,
ηi , ηi∗ 0, ξj 0, i = 1, · · · , p, j = p + 1, · · · , p + q
get the solution (wk+1 , bk+1 );
(3) If (wk , bk ) not convergence, set k = k + 1, go to step (2).

for i = 1, · · · , p. And we also introduce the variable vector u+ such that ui = |w+i |, i = 1, · · · , n,
therefore the problem (20) turns to be a LPP

min

w+ ,u+ ,b+

p

1
w+
2

(ηi + ηi∗ ) + C2

1 + C1

p+q

p
i=1

j=p+1

i=1

θi ((w+ · xi ) + b+ )

ξj +

p+q

δj yj ((w+ · xi ) + b+ )

+
j=p+1

s.t.

(21)

(w+ · xi ) + b+

ε + ηi , i = 1, · · · , p,

− (w+ · xi ) − b+ ε + ηi∗ , i = 1, · · · , p,
(w+ · xj ) + b+ −1 + ξj , j = p + 1, · · · , p + q,
ηi , ηi∗

0, ξj

0,

i = 1, · · · , p, j = p + 1, · · · , p + q

Another LPP can be formulated as follows if two variable vectors u+ and v+ are introduced satisfying
w+ = u+ − v+ , |w+i | = u+i + v+i , i = 1, · · · , n,

p

n
u+ ,v+ ,b+

i=1

i=1

ξj
j=p+1

p+q

p

δj yj (((u+ − v+ ) · xi ) + b+ )

θi (((u+ − v+ ) · xi ) + b+ ) +

+

j=p+1

i=1

s.t.

p+q

(ηi + ηi∗ ) + C2

(u+i + v+i ) + C1

min

((u+ − v+ ) · xi ) + b+

ε + ηi , i = 1, · · · , p,

− ((u+ − v+ ) · xi ) − b+ ε + ηi∗ , i = 1, · · · , p,
((u+ − v+ ) · xj ) + b+ −1 + ξj , j = p + 1, · · · , p + q,
ηi , ηi∗
1750

0, ξj

0,

i = 1, · · · , p, j = p + 1, · · · , p + q

(22)

Ramp Loss Linear Programming Nonparallel Support Vector Machine

3.2

Dalian Liu, etal.

Nonlinear case

In [11], we have

p

p+q

(α∗i − αi − θi )xi −

w+ =

(βj − δj )xj

(23)

j=p+1

i=1

where α, α∗ , β ≥ 0 are the corresponding Lagrangian multiplier vectors, therefore we can assume that
l
w+ = i=1 u+i xi and w+ 2 is the convex function of u+ = (u+1 , · · · , u+l ) . If we take some
convex function f (s+ ) to replace w+ 2 , typically some norm or seminorm of u+ , we will get the
generalized formulation. Here we choose the 1-norm of u+ , at the same time introduce the kernel
function K(x, x ) to get the LPPs for the nonlinear case
p

l

(ηi +

s+i + C1

min

u+ ,s+ ,b+

ηi∗ )

p
i=1

p+q

l

θi (

+

ξj

+ C2
j=p+1

i=1

i=1

p+q

l

δj y j (

u+k K(xk , xi ) + b+ ) +
j=p+1

k=1

u+k K(xk , xj ) + b+ )

k=1

l

u+k K(xk , xi ) + b+

s.t.

ε + ηi , i = 1, · · · , p,

k=1
l

−

u+k K(xk , xi ) − b+

(24)

ε + ηi∗ , i = 1, · · · , p,

k=1
l

u+k K(xk , xj ) + b+

−1 + ξj , j = p + 1, · · · , p + q,

k=1

ηi , ηi∗

0, ξj

0,

i = 1, · · · , p, j = p + 1, · · · , p + q

The CCCP framework for the nonlinear case is similar to Algorithm 1 and only the subproblems is
different, the decision functions constructed are
l

l

f+ (x) =

u−i K(xi , x);

u+i K(xi , x); f− (x) =

(25)

i=1

i=1

for the new point x ∈ Rn , it is predicted to the Class by
Class = arg min fm (x)
m=+,−

4

(26)

Experimental Results

In this section, in order to validate the performance of our ramp-LPNPSVM, we compare it with
RNPSVM on several publicly available benchmark datasets which are used in [11]. All methods are
implemented in MATLAB 2010 on a PC with an Intel Core I5 processor and 2GB RAM. All methods
are solved by the optimization toolbox. For each data set, we randomly select the same number of samples from different classes to compose a balanced training set, therefore, based on this set to verify the
above methods. This procedure is repeated 5 times, and Table 1 lists the average tenfold cross-validation
results of these methods in terms of accuracy (The results of RNPSVM are reported in [11]. The parameters are chosen to be the same as used in RNPSVM, where the parameters t, s of ramp loss are set
1751

Ramp Loss Linear Programming Nonparallel Support Vector Machine

Dalian Liu, etal.

t ∈ (ε, 1), s ∈ (−1, 1). The best test accuracies are in boldface. From the results we can ﬁnd that the
ramp-LPNPSVM gets the accuracy as good as RNPSVM, while it runs faster since we used the linear
programming toolbox.
Data Sets
Australian
BUPA liver
CMC
Heart-Statlog
Hepatitis
Ionosphere
Pima Indian
Sonar
Votes
WPBC

ramp-LPNPSVM
Accuracy %
84.57 ± 3.25
73.26 ± 2.83
75.64 ± 3.19
85.71 ± 3.27
83.05 ± 3.22
89.28 ± 2.47
78.36 ± 3.55
88.27 ± 3.37
95.01 ± 2.54
84.96 ± 3.61

RNPSVM
Accuracy %
86.81 ± 3.19
74.65 ± 2.66
76.32 ± 4.47
87.03 ± 3.41
85.27 ± 3.18
90.12 ± 3.04
79.68 ± 4.53
89.69 ± 5.19
95.97 ± 4.38
86.11 ± 3.06

Table 1: The average tenfold cross-validation results on UCI data sets in terms of accuracy

5

Conclusion

In this paper, we have proposed a ramp loss linear programming NPSVM, termed ramp-LPNPSVM,
by introducing the l1 regularization term to the ramp loss NPSVM, which involves a sequence of linear
programming problems in the CCCP procedure. Compared with the RNPSVM, ramp-LPNPSVM not
only has the advantages of RNPSVM, but also has the less training time. Experimental results on
benchmark datasets conﬁrm the effectiveness of the proposed algorithm. In [6], Johan A.K. Suykens
et. al proposed the ramp-LPSVM and pointed out that the problem related to ramp-LPSVM leads to a
polyhedral concave problem which is easier to handle, and they established algorithms including DC
programming for local minimization and hill detouring for global search. Since ramp-LPNPSVM is
similar to ramp-LPSVM, so we will consider their method in the future.

6

Acknowledgments

This work has been partially supported by grants from National Natural Science Foundation of China
(No.61472390, No.11271361, No.71331005, No.11226089), the Major International (Regional) Joint
Research Project (No.71110107026) and the Beijing Natural Science Foundation (No. 1162005).

References
[1] C. Burges. A tutorial on support vector machines for pattern recognition. Data Mining and Knowledge Discovery, 2:121–167, 1998.
[2] X. Chen, J. Yang, Q. Ye, and J. Liang. Recursive projection twin support vector machine via
within-class variance minimization. Pattern Recognition., 44:2643–2655, 2011.
[3] R. Collobert, F F. Sinz, J. Weston, and L. Bottou. Trading convexity for scalability. In ICML,
2006.
1752

Ramp Loss Linear Programming Nonparallel Support Vector Machine

Dalian Liu, etal.

[4] C. Cortes and V. N. Vapnik. Support-vector networks. Machine Learning, 20(3):273–297, 1995.
[5] N.Y. Deng, Y.J. Tian, and C.H. Zhang. Support Vector Machines: Optimization Based Theory,
Algorithms, and Extensions. Chapman and Hall/CRC, 2012.
[6] X.L. Huang, L. Shi, and J. A.K. Suykens. Ramp loss linear programming support vector machine.
Journal of Machine Learning Research, 15:2185–2211, 2014.
[7] R Khemchandani, Suresh Chandra, et al. Twin support vector machines for pattern classiﬁcation.
Pattern Analysis and Machine Intelligence, IEEE Transactions on, 29(5):905–910, 2007.
[8] R. Khemchandani, R. K. Jayadeva, and S. Chandra. Optimal kernel selection in twin support vector
machines. Optim. Lett., 3(1):77–88, 2009.
[9] M. A. Kumar and M. Gopal. Application of smoothing technique on twin support vector machines.
Pattern Recognition Letters, 29(13):1842–1848, 2008.
[10] C. Lin and S. Wang. Fuzzy support vector machines. IEEE Transactions on Neural Networks,
13:464–471, 2002.
[11] Dalian Liu, Yong Shi, and Yingjie Tian. Ramp loss nonparallel support vector machine for pattern
classiﬁcation. Knowledge-Based Systems, 85:224–233, 2015.
[12] L. Mason, J. Baxter, P.L. Bartlett, and M. Frean. Boosting algorithms as gradient descent in
function space. Advances in Neural Information Processing Systems, 12:512–518, 2000.
[13] Xinjun Peng. Tsvr: an efﬁcient twin support vector machine for regression. Neural Networks,
23(3):365–372, 2010.
[14] Z.Q. Qi, Y.J. Tian, and Y. Shi. Laplacian twin support vector machine for semi-supervised classiﬁcation. Neural Networks, 35:46–53, 2012.
[15] Z.Q. Qi, Y.J. Tian, and Y. Shi. Twin support vector machine with universum data. Neural Networks,
36:112–119, 2012.
[16] Z.Q. Qi, Y.J. Tian, and Y. Shi. Robust twin support vector machine for pattern classiﬁcation.
Pattern Recognition, 46:305–316, 2013.
[17] Z.Q. Qi, Y.J. Tian, and Y. Shi. Structural twin support vector machine for classiﬁcation.
Knowledge-Based Systems, 43:74–81, 2013.
[18] Y. H. Shao and N. Y. Deng. A coordinate descent margin based-twin support vector machine for
classiﬁcation. Neural networks, 25:114–121, 2012.
[19] Y. H. Shao, C. H. Zhang, X. B. Wang, and N. Y. Deng. Improvements on twin support vector
machines. IEEE Transactions on Neural Networks, 22(6):962–968, 2011.
[20] X. Shen, G. C. Tseng, X. Zhang, and W. Wong. On ψ-learning. Journal of the American Statistical
Association, 98(463):724–734, 2003.
[21] J. Suykens, J. DeBrabanter, and L. Lukas. Weighted least squares support vector machines: robustness and sparse approximation. Neurocomputing, 48:85–105, 2002.
[22] Y.J. Tian, X.C. Ju, Z.Q. Qi, and Y. Shi. Improved twin support vector machine. Science China
Mathematic, 57(2):417–432, 2014.
1753

Ramp Loss Linear Programming Nonparallel Support Vector Machine

Dalian Liu, etal.

[23] Y.J. Tian, Z.Q. Qi, X.C. Ju, Y. Shi, and X.H. Liu. Nonparallel support vector machines for pattern
classiﬁcation. IEEE Transactions on Cybernetics, 44(7):1067–1079, 2014.
[24] B. Trafalis and C.Gilbert. Robust classiﬁcation and regression using support vector machines.
European Journal of Operational Research, 173:893–909, 2006.
[25] V. N. Vapnik. The Nature of Statistical Learning Theory. New York: Springer, 1996.
[26] V. N. Vapnik. Statistical Learning Theory. New York: John Wiley and Sons, 1998.
[27] L. Wang, H. D. Jia, and J. Li. Training robust support vector machine with smooth ramp loss in
the primal space. Neurocomputing, 71:3020–3025, 2008.
[28] A. L. Yuille and A. Rangarajan. The concave-convex procedure. Neural Computation, 15:915–
936, 2003.
[29] P. Zhong and M. Fukushima. Second order cone programming formulations for robust multi-class
classiﬁcation. Neural Computation, 2007:258–282, 19.

1754

