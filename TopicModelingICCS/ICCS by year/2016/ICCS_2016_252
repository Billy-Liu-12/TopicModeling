Procedia Computer Science
Volume 80, 2016, Pages 1658–1669
ICCS 2016. The International Conference on Computational
Science

GPU acceleration of a non-hydrostatic ocean
model with a multigrid Poisson/Helmholtz solver
Takateru Yamagishi1, Yoshimasa Matsumura2
1

Research Organization for Information Science and Technology.
2
Institute of Low Temperature Science, Hokkaido University.

Abstract
To meet the demand for fast and detailed calculations in numerical ocean
simulations, we implemented a non-hydrostatic ocean model on a graphics processing
unit (GPU). We improved the model’s Poisson/Helmholtz solver by optimizing the
memory access, using instruction-level parallelism, and applying a mixed precision
calculation to the preconditioning of the Poisson/Helmholtz solver. The GPUimplemented model was 4.7 times faster than a comparable central processing unit
execution. The output errors due to this implementation will not significantly influence
oceanic studies.
Keywords: GPU, CUDA, multigrid method, mixed precision, ocean modeling

1 Introduction
Oceanic numerical simulations play an important role in climate studies and the development of
oceanic resources. Ocean circulation comprises dynamics of various scales, such as turbulent mixing
induced by winds and tides, descending gravity currents and their entrainment, deep convection,
mesoscale eddies, and huge currents. Long execution times are required to predict and study climate
change. The ability to quickly resolve small processes in a huge domain is essential to effectively
study large-scale oceanic circulation. Graphics processing units (GPUs) are expected to meet these
growing demands owing to their low cost and high performance. However, few studies have been
conducted on the role of GPU in ocean models; subsequently, the experimental research available is
insufficient to support the study of oceanic processes [1, 2, 3, 4].
This study aims to examine the execution of a non-hydrostatic ocean model on a GPU and to
create an experimental model to study small oceanic processes. We implemented a numerical, nonhydrostatic ocean model called “kinaco” [5] on a GPU following basic but essential methods. We
improved the model’s Poisson/Helmholtz (P/H) solver by optimizing the memory access, using
instruction-level parallelism, and applying a mixed precision calculation to the preconditioning of the
P/H solver.

1658

Selection and peer-review under responsibility of the Scientiﬁc Programme Committee of ICCS 2016
c The Authors. Published by Elsevier B.V.

doi:10.1016/j.procs.2016.05.502

GPU acceleration of a non-hydrostatic ...

Takateru Yamagishi and Yoshimasa Matsumura

On the GPU (NVIDIA Tesla K20c), we achieved an execution time 4.7 times faster compared to
the central processing unit (CPU) (Fujitsu SPARC 64VIIIfx). The output errors due to this
implementation would not significantly influence the outcomes of oceanic studies.

2 The numerical non-hydrostatic ocean model “kinaco”
Kinaco is a non-hydrostatic ocean model that was developed for high-resolution numerical ocean
studies [5]. Kinaco simulates ocean dynamics using the three-dimensional (3D) non-hydrostatic
Navier–Stokes equation in an orthogonal curvilinear coordinate system. Given certain assumptions,
this equation reduces to the 3D Poisson equation and two-dimensional (2D) Helmholtz equation for
the pressure field and free-surface elevation, respectively. Kinaco also simulates various values such
as potential temperature, salinity, and other passive properties of seawater using advection–diffusion
equations. Additionally, kinaco considers and describes various physical processes, such as large

Figure 1: Formation of Antarctic bottom water in the southern Weddell Sea simulated by kinaco.

eddies, buoyancy, strain rate, bottom stress, and viscosity. The model code was parallelized using
OpenMP and MPI and optimized for execution on a supercomputer with a large number of
computational nodes.
As a result of its detailed description of the ocean and high performance on supercomputers,
kinaco has reproduced various realistic phenomena in the ocean in terms of velocity fields and
pathways of descending dense water. Figure 1 shows the result of a high-resolution simulation of
Antarctic bottom water formation in the southern Weddell Sea [6, 7].
As we mentioned in the previous section, recent numerical ocean studies require a large number of
grids. This type of problem is suitable for execution on modern supercomputers that comprise many
computational nodes and cores and require large numbers of grids to hide arithmetic and memory
access latencies.
However, with supercomputers, performance scalability with increasing computational nodes is
generally a problem. In numerical non-hydrostatic ocean simulations, previous studies have suffered
from the increase in communication cost between computational nodes in executions with a large
number of grids. Cases wherein the models adopt iterative methods to solve the equations, the number
of iterations typically increases as the number of grids increases.
Kinaco adopts an iterative method to solve the P/H equations because it is suitable for the ocean’s
complicated boundary conditions. The discretized P/H equations for the pressure field and free surface
elevation appearing in non-hydrostatic ocean models are a system of linear equations. The
discretization is based on finite-difference approximations between six adjacent grids; as a result, the
equations are simplified to a sparse matrix solution problem. Kinaco adopts the CG method with a

1659

GPU acceleration of a non-hydrostatic ...

Takateru Yamagishi and Yoshimasa Matsumura

multigrid preconditioner (MGCG) as its iterative solver. The number of iterations of MGCG remains
constant even when N becomes larger; therefore, the numerical cost of MGCG is proportional to the
number of cells. Further, the convergence rate of MGCG is usually much higher than that of simple
standalone multigrid iterations.
As for a multigrid smoother, kinaco uses the sparse approximate inverse method that derives a
sparse approximate inverse based on norm minimization [8, 9, 10]. The application of the smoother
only requires a matrix-vector multiplication, and the multiplication is inherently data parallel;
therefore, the smoother should be appropriate for parallel execution with the large number of grids.
We evaluated the performance of kinaco on the K computer in Japan with 10 billion grids, and it
showed an almost linear scaling with increasing computational nodes. The number of MGCG
iterations did not change with the increase in computational nodes. Although the collective
communication time between the computational nodes showed a slight increase, it was not significant.

3 GPU implementation
For this implementation, we focused on execution on a single GPU because it is the first step for
the execution on multiple GPUs, which is our final aim. We do not address the application of the MPI
library to an execution on multiple GPUs because the techniques used for the implementation on a
single GPU are applicable to an execution on multiple GPUs.
GPUs require an abundant number of threads and coalesced memory access, invoking the same
instructions on multiple threads and minimization of memory transfer between the CPU and GPU.
These ideas are essential for the efficient utilization of a GPU’s resources.
Kinaco was originally written in Fortran 90; therefore, we adopted PGI CUDA Fortran because it
is a set of extensions from Fortran and can describe several GPU instructions in Fortran with intrinsic
expressions that are essentially the same as those used by CUDA C, which is used by most GPU
developers. The implementation techniques and optimizations found in previous studies with CUDA
C can therefore be applied to CUDA Fortran.
As mentioned in the previous section, kinaco is highly developed and optimized for executions
with a large number of grids, therefore, large numbers of threads were invoked to hide latencies by

Figure 2: The components and the flow of the time integration in kinaco.

1660

GPU acceleration of a non-hydrostatic ...

Takateru Yamagishi and Yoshimasa Matsumura

swapping stalled threads for threads that are ready to execute. An intrinsic character of ocean
simulations is that the horizontal axis is much larger than the vertical axis; therefore, we set the size of
the domain for the GPU to (256, 256, 32). We then set 3D threads (256, 256, 32) or 2D threads (256,
256, 1). The configuration of threads is (32, 8, 1) per block, which is common for both 3D and 2D
threads. The total number of 2D threads (256, 256, 1) was approximately 65K, which is more than the
maximum number of threads for a GPU (26K for NVIDIA Tesla K20c).
The equations are all discretized on structured grids; each grid systematically accesses the adjacent
grids, and the same instructions are sequentially executed. We took advantage of this systematic
characteristic for the coalesced memory access and invoked the same instruction on multiple threads.
We maintained the order of the arrays in the original kinaco code of (x, y, z). Some numerical
models for climate or ocean simulation set the z-axis as the innermost index, i.e., (z, x, y), to
efficiently use the CPU cache because the number of grids on the z-axis is smaller than those on the
other two axes. For GPU, the typical order (x, y, z) is suitable because the large number of grids on
the innermost x-axis enables it to perform efficient parallel calculations.
For coalesced memory access, we rewrote the array of structures used in original code into
ordinary arrays; furthermore, we eliminated a recursive description in the multigrid kernel to reduce
the overhead of invoking the kernel.
The cost of memory transfer between the CPU and GPU is significant; therefore, it is important to
reduce redundant memory transfers. Numerical ocean models are systems that evolve through time,
and they repeatedly iterate the same procedures to express the evolution of natural phenomena in the
ocean. Redundant memory transfers between the CPU and GPU in iterated procedures will deteriorate
the model’s performance. Therefore, all procedures in the iteration part of the code were implemented
and executed on the GPU and not on the CPU. Data transfer was limited to the initial and final
procedures (Figure 2).

4 Improvements of the MGCG solver on the GPU
In the previous section, we described the basic and essential methods for the efficient usage of a
GPU. In this section, we introduce methods to improve kinaco’s MGCG solver.

4.1 Optimization for efficient usage of the GPU
In kinaco’s MGCG solver, sparse matrix vector multiplication accounts for the majority of its
numerical cost. Figure 3 shows one of the sparse matrix vector multiplication kernels in the original
CPU code. Array “a” represents the sparse matrix for a coefficient of the discretized ocean dynamics
equations, and the first dimension, which ranges from −3 to 3, represents the index of the adjacent
grids in the 3D domain.
We set the size of the arrays (n1, n2, n3) to (256, 256, 32). When we execute kinaco on a CPU, the
outer loop (k) is parallelized using OpenMP. For example, if we assign eight threads, each thread
calculates a domain of (256, 256, 4). In the CPU execution, each thread sequentially accesses the first
dimension; therefore, we can take advantage of the CPU cache line for the loop iteration and reuse the
array “a” in the CPU cache.

1661

GPU acceleration of a non-hydrostatic ...

Takateru Yamagishi and Yoshimasa Matsumura

ùûèøõûúïôëÆĆĕĕđĞÙĉ×Îē×ÒÆēØÒÆēÙÒÆĆÒÆĎĉĝÒÆĝÒÆĔĚęÏÆ
ÆÆïôúëíëøÒÆĎēęĊēęÎĎēÏÆÆààÆē×ÒÆēØÒÆēÙÆ
ÆÆøëçòÎÞÏÒÆĎēęĊēęÎĎēÏÆÆààÆĆÆÆÎÓÙàÙÒÆ×àē×ÒÆ×àēØÒÆ×àēÙÏÆ
ÆÆïôúëíëøÒÆĎēęĊēęÎĎēÏÆÆààÆĎĉĝÎ×àēÙÏÆ
ÆÆøëçòÎÞÏÒÆĎēęĊēęÎĎēÏÆÆààÆĝÆÆÎÖàē×Ñ×ÒÆÖàēØÑ×ÒÆÖàēÙÑ×ÏÆ
ÆÆøëçòÎÞÏÒÆĎēęĊēęÎĔĚęÏÆààÆĔĚęÎÖàē×Ñ×ÒÆÖàēØÑ×ÒÆÖàēÙÑ×ÏÆ
ÆÆïôúëíëøÆààÆĎÒÆďÒÆĐÒÆĐĐÆ
Æ
ÇÊõóöÆĕĆėĆđđĊđÆĉĔÆĕėĎěĆęĊÎĐĐÏÆ
ÆÆêõÆĐã×ÒÆēÙÆ
ÆÆÆÆÆĐĐÆãÆĎĉĝÎĐÏÆ
ÆÆÆÆÆêõÆďã×ÒÆēØÆ
ÆÆÆÆÆêõÆĎã×ÒÆē×Æ
ÆÆÆÆÆÆÆÆÆĔĚęÎĎÒďÒĐÏÆãÆĆÎÓÙÒĎÒďÒĐĐÏÆÐÆĝÎĎÒÆÆďÒÆÆĐÓ×ÏÆÆÆÌÆ
ÆÆÆÆÆÆÆÆÆÆÆÆÆÆÆÆÆÆÆÆÑÆĆÎÓØÒĎÒďÒĐĐÏÆÐÆĝÎĎÒÆÆďÓ×ÒĐÆÆÏÆÆÆÌÆ
ÆÆÆÆÆÆÆÆÆÆÆÆÆÆÆÆÆÆÆÆÑÆĆÎÓ×ÒĎÒďÒĐĐÏÆÐÆĝÎĎÓ×ÒďÒÆÆĐÆÆÏÆÆÆÌÆ
ÆÆÆÆÆÆÆÆÆÆÆÆÆÆÆÆÆÆÆÆÑÆĆÎÆÖÒĎÒďÒĐĐÏÆÐÆĝÎĎÒÆÆďÒÆÆĐÆÆÏÆÆÆÌÆ
ÆÆÆÆÆÆÆÆÆÆÆÆÆÆÆÆÆÆÆÆÑÆĆÎÆ×ÒĎÒďÒĐĐÏÆÐÆĝÎĎÑ×ÒďÒÆÆĐÆÆÏÆÆÆÌÆ
ÆÆÆÆÆÆÆÆÆÆÆÆÆÆÆÆÆÆÆÆÑÆĆÎÆØÒĎÒďÒĐĐÏÆÐÆĝÎĎÒÆÆďÑ×ÒĐÆÆÏÆÆÆÌÆ
ÆÆÆÆÆÆÆÆÆÆÆÆÆÆÆÆÆÆÆÆÑÆĆÎÆÙÒĎÒďÒĐĐÏÆÐÆĝÎĎÒÆÆďÒÆÆĐÑ×ÏÆ
ÆÆÆÆÆëôêÆêõÆ
ÆÆÆÆÆëôêÆêõÆ
ÆÆëôêÆêõÆ
Æ
ëôêÆùûèøõûúïôëÆĆĕĕđĞÙĉ×Æ

Figure 3: The sparse matrix vector multiplication kernel used in the original CPU code.

In the GPU implementation, GPU threads are normally generated following the number of loop
indexes. For example, we can set the configuration of threads to (256, 256, 32). Such a configuration
does not consider the first dimension of the array “a” but makes each thread access the array “a” in
seven intervals (Figure 4). In the implementation, we exchanged the first dimension for the fourth
dimension, i.e., (−3:3, i, j, k) to (i, j, k, −3:3); this enables us to exploit coalesced access to global
memory (Figure 5).

Figure 4: Global memory access by each GPU thread depending on the order of array’s dimension.

1662

GPU acceleration of a non-hydrostatic ...

Takateru Yamagishi and Yoshimasa Matsumura

The configuration of threads is an essential and critical factor for the efficient usage of a GPU.
Launching many threads is a well-known and effective strategy to hide memory/arithmetic latencies,
and we can achieve high occupancy for each streaming multiprocessor. In this case, thread-level
parallelism (TLP) is used to hide the latencies; however, instruction-level parallelism (ILP) can also
effectively hide latencies [11].
ĆęęėĎćĚęĊĘÎČđĔćĆđÏÆùûèøõûúïôëÆĆĕĕđĞÙĉ×Îē×ÒÆēØÒÆēÙÒÆĆÒÆĎĉĝÒÆĝÒÆĔĚęÏÆ
ÆÆïôúëíëøÒÆěĆđĚĊÆÒÆĎēęĊēęÎĎēÏÆÆààÆē×ÒÆēØÒÆēÙÆ
ÆÆøëçòÎÞÏÒÆĉĊěĎĈĊÒÆĎēęĊēęÎĎēÏÆÆààÆĆÆÆÎ×àē×ÆÆÒÆ×àēØÆÆÒÆ×àēÙÆÆÒÆÓÙàÙÏÆ
ÆÆïôúëíëøÒÆĉĊěĎĈĊÒÆĎēęĊēęÎĎēÏÆÆààÆĎĉĝÎ×àēÙÏÆÆÆ
ÆÆøëçòÎÞÏÒÆĉĊěĎĈĊÒÆĎēęĊēęÎĎēÏÆÆààÆĝÆÆÎÖàē×Ñ×ÒÆÖàēØÑ×ÒÆÖàēÙÑ×ÏÆ
ÆÆøëçòÎÞÏÒÆĉĊěĎĈĊÒÆĎēęĊēęÎĔĚęÏÆààÆĔĚęÎÖàē×Ñ×ÒÆÖàēØÑ×ÒÆÖàēÙÑ×ÏÆ
Æ
ÆÆïôúëíëøÆààÆĎÒÆďÒÆĐÒÆĐćÆ
Æ
ÆÆĎÆãÆęčėĊĆĉĎĉĝËĝÆÑÆćđĔĈĐĉĎĒËĝÆÐÆÎćđĔĈĐĎĉĝËĝÓ×ÏÆ
ÆÆďÆãÆęčėĊĆĉĎĉĝËĞÆÑÆćđĔĈĐĉĎĒËĞÆÐÆÎćđĔĈĐĎĉĝËĞÓ×ÏÆÆÆ
Æ
ÆÆêõÆĐã×ÒÆēÙÆ
ÆÆÆÆĐćãĎĉĝÎĐÏÆ
ÆÆÆÆĔĚęÎĎÒďÒĐÏÆãÆĆÎĎÒďÒĐĐÒÓÙÏÆÐÆĝÎĎÒÆÆďÒÆÆĐÓ×ÏÆÆÌÆ
ÆÆÆÆÆÆÆÆÆÆÆÆÆÆÆÑÆĆÎĎÒďÒĐĐÒÓØÏÆÐÆĝÎĎÒÆÆďÓ×ÒĐÆÆÏÆÆÌÆ
ÆÆÆÆÆÆÆÆÆÆÆÆÆÆÆÑÆĆÎĎÒďÒĐĐÒÓ×ÏÆÐÆĝÎĎÓ×ÒďÒÆÆĐÆÆÏÆÆÌÆ
ÆÆÆÆÆÆÆÆÆÆÆÆÆÆÆÑÆĆÎĎÒďÒĐĐÒÆÖÏÆÐÆĝÎĎÒÆÆďÒÆÆĐÆÆÏÆÆÌÆ
ÆÆÆÆÆÆÆÆÆÆÆÆÆÆÆÑÆĆÎĎÒďÒĐĐÒÆ×ÏÆÐÆĝÎĎÑ×ÒďÒÆÆĐÆÆÏÆÆÌÆ
ÆÆÆÆÆÆÆÆÆÆÆÆÆÆÆÑÆĆÎĎÒďÒĐĐÒÆØÏÆÐÆĝÎĎÒÆÆďÑ×ÒĐÆÆÏÆÆÌÆ
ÆÆÆÆÆÆÆÆÆÆÆÆÆÆÆÑÆĆÎĎÒďÒĐĐÒÆÙÏÆÐÆĝÎĎÒÆÆďÒÆÆĐÑ×ÏÆ
ÆÆëôêÆêõÆ
Æ
ëôêÆùûèøõûúïôëÆĆĕĕđĞÙĉ×Æ

Figure 5: The sparse matrix vector multiplication kernel used in the GPU code.

We compared two sets of thread configurations in the implementation of the matrix-vector
multiplication kernel. One configuration exploits as much TLP as possible by launching 3D threads
(256, 256, 32). Each thread calculates and updates only one value. The other configuration exploits
both TLP and ILP by using fewer threads. Here, we set 2D threads (256, 256, 1), and each thread was
in charge of one vertical column, which comprised thirty-two values. Each thread issued instructions
repeatedly to one value thirty-two times in the z-direction therefore taking advantage of ILP.
We found that the configuration of 2D threads showed better performance; therefore, we adopted
2D threads for the P/H solver. The number of (256, 256, 1) threads is approximately 65K, which is
more than the maximum number of threads for a GPU (26K for NVIDIA Tesla K20c). Furthermore,
the GPU memory usage of 2D threads (256, 256, 1) is approximately 1.8 GB for the execution in this
study. The capacity of the GPU global memory we used is 5 GB (NVIDIA Tesla K20c); therefore,
this implementation has the potential to increase TLP by increasing the number of horizontal grids.

4.2 The mixed precision multigrid pre-conditioned CG method

1663

GPU acceleration of a non-hydrostatic ...

Takateru Yamagishi and Yoshimasa Matsumura

ĘĊęÆĎēĎęĎĆđÆĕÎÖÏÆ
ėÎÖÏÆãÆĖÆ⽝ÆòĕÎÖÏÆ
ùĔđěĊÆòĚÎÖÏÆãÆėÎÖÏÆėĔĚČčđĞÆĚĘĎēČÆęčĊÆĒĚđęĎČėĎĉÆĒĊęčĔĉÆ
̦ÎÖÏÆãÆÎĚÎÖÏÒÆėÎÖÏÏÆ
êõÆēÆãÆÖÒÆ×ÒÆƧÆýîïòëÆġėÎÖÏġÆäÆ̚ġĖġÆÇÆĈĔēěĊėČĊēĈĊÆęĊĘęÆ
ÆÆÆ̖ÆãÆ̦ÎēÏÕÎĚÎēÏÒÆòĚÎēÏÏÆ
ÆÆÆĕÎēÑ×ÏÆãÆĕÎēÏÆÑÆ̖ĚÎēÏÆ
ÆÆÆėÎēÑ×ÏÆãÆėÎēÏÆ⽝Æ̖òĚÎēÏÆ
ÆÆÆùĔđěĊÆòĚÎēÑ×ÏÆãÆėÎēÑ×ÏÆėĔĚČčđĞÆĚĘĎēČÆęčĊÆĒĚđęĎČėĎĉÆĒĊęčĔĉÆ
ÆÆÆ̦ÎēÑ×ÏÆãÆÎĚÎēÑ×ÏÒÆėÎēÑ×ÏÏÆ
ÆÆÆ̗ÆãÆ̦ÎēÑ×ÏÕ̦ÎēÏÆ
ÆÆÆĚÎēÑ×ÏÆãÆĚÎēÑ×ÏÆÑÆ̗ĚÎēÏÆ
ëôêÆêõÆ

Figure 6: The MGCG method used to solve Lp = q.

MGCG is an efficient method for numerical non-hydrostatic ocean models with large numbers of
grids. Figure 6 shows the MGCG method used to solve the system of equations Lp = q. The solution p
is estimated by an iteration corresponding to the do loop in Figure 6, and the preconditioning
corresponds to roughly solving Lu = r using the multigrid method in Figure 7.
The multigrid method effectively dumps low-frequency errors, which is critical for fast
calculations with iterative methods. On a coarse grid, errors regarded as high-frequency errors can be
efficiently dumped (Figure 7). With an appropriate grid size for each type of execution, the numerical
cost of the multigrid method enables linear scalability in execution with a large number of cells.
Previous studies implemented the GPU using multigrid methods and achieved high performance by

Figure 7: Procedure for the multigrid method to roughly solve Lu = r.

exploiting the character of data parallelism and large number of grids [12, 13].
Although MGCG is an efficient method, it needs to be executed with a small number of grids for
structural reasons (Figure 7). The GPU, conversely, needs a large number of grids to take advantage
of its GPU cores and to hide latencies; therefore, fewer threads cause its performance to deteriorate.
To compensate for this deterioration, we applied a mixed precision calculation to the
preconditioning step. The objective of preconditioning is to roughly solve the equation for the fast
convergence of the CG methods; therefore, we assumed that single precision was sufficient for
preconditioning to roughly solve the equation.

1664

GPU acceleration of a non-hydrostatic ...

Takateru Yamagishi and Yoshimasa Matsumura

The hardware specification of the GPU’s computational performance is almost doubled in the case
of single precision compared to double precision, and the bytes per floating number is halved;
therefore, the memory transfer bandwidth per floating number is essentially doubled.
All arrays for multigrid preconditioning, such as the smoother matrices, residuals, and temporal
arrays, were set as 4-byte single-precision floating-point numbers. The conjugate gradient method,
however, was calculated in double-precision.

5 Experimental settings
The GPU implementation was evaluated on a workstation with an Intel CPU (Core i7 3930K) and
NVIDIA GPU (Tesla K20c). To validate the outputs and evaluate the performance, we also executed
the original code on a single node of the K computer, which incorporated a Fujitsu CPU (SPARC64
VIIIfx). We used the PGI Fortran Accelerator compiler 14.10 and Fujitsu Fortran compiler for
executions on the GPU and CPUs, respectively.
The size of the domain was set to (256, 256, 32). The total numbers of time steps were 150 and
3600, which were set for performance comparisons and validation of outputs, respectively. Each time
step simulated is a period of 120 s. The configuration of the GPU threads was set to 3D (256, 256, 32)
or 2D (256, 256, 1). The GPU thread block was set to (32, 8, 1), which is common for both 3D and 2D
threads. For the execution on the CPU, we set eight threads to eight CPU cores, and a domain of (256,
256, 4) was assigned to each thread.
It was too complicated to evaluate the outputs of a realistic boundary condition because it would
cause nonlinear reactions; therefore, we adopted the test case with idealistic and symmetric
temperature forcing and boundary conditions [14]. The case assumes the occurrence of baloclinic
instability caused by temperature forcing and geostrophic balances. Baloclinic instability plays
important roles in realistic ocean circulations; therefore, this experimental setting should be a
substantial validation for this GPU implementation.

6 Results and discussion
To compare the executions on the Fujitsu SPARC64 VIIIfx, we excluded the costs of initialization,
finalization, and memory transfer between the CPU and GPU. In practical executions of the numerical
ocean model, the iterations are repeated many times and their cost dominates; therefore, we focused
on the cost of the iteration part of the model.

all
P/H solver
others

CPU
174.2
36.8
137.4

GPU_1
42.6
15.8
26.9

GPU_2
39.2
12.4
26.8

GPU_3
37.3
10.5
26.8

speedup
4.7
3.5
5.1

Table 1: Elapsed time for each component of the CPU/GPU executions. GPU_1 denotes the
GPU-implemented kinaco without improvements to the P/H solver, GPU_2 denotes GPU_1 plus
Subsection 4.1 and GPU_3 denotes GPU_2 plus Subsection 4.2.

The GPU-implemented kinaco, P/H solver, and other calculations, including the diffusionadvection equations and physical processes, ran 4.7, 3.5, and 5.1 times faster, respectively, on the
NVIDIA K20c (Table 1). The improvement of the P/H solver (Subsection 4.1) accelerated the P/H
solver by 1.27 times (15.8 s to 12.4 s), and the application of mixed precision to the preconditioning
(Subsection 4.2) accelerated the P/H solver by 1.18 times (12.4 s to 10.5 s). The elapsed time of the

1665

GPU acceleration of a non-hydrostatic ...

Takateru Yamagishi and Yoshimasa Matsumura

calculations, not including the P/H solver, was longer than that of the P/H solver. Because we
implemented these calculations following the basic methods explained in Section 3, we expect that the
model can be further improved.


Computational performance (GFLOPS)
GFLOPS/PEAK (%)
Memory transfer (GB/S)

CPU
7.7
6.0
22.2

GPU_3
42.3(dp)/3.8(sp)
3.6(dp)/0.1(sp)
114.1

Table 2: Basic performance metrics for the CPU/GPU executions. dp: double precision, sp: single precision

Table 2 shows another metric to compare the model performance. The ratio of the computational
performance throughput to the hardware specification is 3.6% for the GPU, which is almost half that
of the CPU (Table 2). Explaining this lower ratio for the GPU requires a further and more detailed
investigation.

apply3d3

double precision
383.5

mixed precision
195.3

speedup
2.0

Table 3: The elapsed time (ms) of the preconditioning kernel apply3d3 with and without mixed precision.

ĆęęėĎćĚęĊĘÎČđĔćĆđÏÆùûèøõûúïôëÆĆĕĕđĞÙĉÙÎē×ÒÆēØÒÆēÙÒÆćÒÆĎĉĝÒÆĎēÒÆĔĚęÏÆ
ÆÆÆïôúëíëøÒÆěĆđĚĊÆÒÆïôúëôúÎïôÏÆÆÆÆààÆē×ÒÆēØÒÆēÙÆ
ÆÆÆøëçòÎÞÏÒÆĉĊěĎĈĊÒÆïôúëôúÎïôÏÆÆÆÆààÆćÆÆÎ×àē×ÆÆÒÆ×àēØÆÆÒÆ×àēÙÆÆÒÆÓÙàÙÏÆ
ÆÆÆïôúëíëøÒÆĉĊěĎĈĊÒÆïôúëôúÎïôÏÆÆÆÆààÆĎĉĝÎ×àēÙÏÆ
ÆÆÆøëçòÎÞÏÒÆĉĊěĎĈĊÒÆïôúëôúÎïôÏÆÆÆÆààÆĎēÆÎÖàē×Ñ×ÒÆÖàēØÑ×ÒÆÖàēÙÑ×ÏÆ
ÆÆÆøëçòÎÞÏÒÆĉĊěĎĈĊÒÆïôúëôúÎïôõûúÏÆààÆĔĚęÎÖàē×Ñ×ÒÆÖàēØÑ×ÒÆÖàēÙÑ×ÏÆ
ÆÆÆÆāøëçòÎÞÏÆĎĘÆĘĜĎęĈčĊĉÆęĔÆøëçòÎÚÏÆĎēÆĒĎĝĊĉÆĕėĊĈĎĘĎĔēÆĕėĊĈĔēĉĎęĎĔēĎēČăÆ

ÆÆÆïôúëíëøÆààÆĎÒÆďÒÆĐÒÆĐĐÆ
Æ
ÆÆÆĎÆãÆęčėĊĆĉĎĉĝËĝÆÑÆćđĔĈĐĉĎĒËĝÆÐÆÎćđĔĈĐĎĉĝËĝÓ×ÏÆ
ÆÆÆďÆãÆęčėĊĆĉĎĉĝËĞÆÑÆćđĔĈĐĉĎĒËĞÆÐÆÎćđĔĈĐĎĉĝËĞÓ×ÏÆÆ
ÆÆ
ÆÆÆêõÆĐã×ÒÆēÙÆ
ÆÆÆÆÆĐĐÆãÆĎĉĝÎĐÏÆ
ÆÆÆÆÆĔĚęÎĎÒÆďÒÆĐÏÆãÆĔĚęÎĎÒÆďÒÆĐÏÆÆÆÆÆÆÆÆÆÆÆÆÆÆÆÌÆ
ÆÆÆÆÆÆÆÆÆÆÑÆćÎĎÒÆďÒÆĐĐÒÆÓÙÏÆÐÆĎēÎĎÒÆÆďÒÆÆĐÓ×ÏÆÆÌÆ
ÆÆÆÆÆÆÆÆÆÆÑÆćÎĎÒÆďÒÆĐĐÒÆÓØÏÆÐÆĎēÎĎÒÆÆďÓ×ÒĐÆÆÏÆÆÌÆ
ÆÆÆÆÆÆÆÆÆÆÑÆćÎĎÒÆďÒÆĐĐÒÆÓ×ÏÆÐÆĎēÎĎÓ×ÒďÒÆÆĐÆÆÏÆÆÌÆ
ÆÆÆÆÆÆÆÆÆÆÑÆćÎĎÒÆďÒÆĐĐÒÆÆÖÏÆÐÆĎēÎĎÒÆÆďÒÆÆĐÆÆÏÆÆÌÆ
ÆÆÆÆÆÆÆÆÆÆÑÆćÎĎÒÆďÒÆĐĐÒÆÆ×ÏÆÐÆĎēÎĎÑ×ÒďÒÆÆĐÆÆÏÆÆÌÆ
ÆÆÆÆÆÆÆÆÆÆÑÆćÎĎÒÆďÒÆĐĐÒÆÆØÏÆÐÆĎēÎĎÒÆÆďÑ×ÒĐÆÆÏÆÆÌÆ
ÆÆÆÆÆÆÆÆÆÆÑÆćÎĎÒÆďÒÆĐĐÒÆÆÙÏÆÐÆĎēÎĎÒÆÆďÒÆÆĐÑ×ÏÆ
ÆÆÆëôêÆêõÆ
Æ
ëôêÆùûèøõûúïôëÆĆĕĕđĞÙĉÙÆ

ĉĔĚćđĊ㻌
đĔĆĉÆàÆ×ØÚè㻌
ĘęĔėĊàÆÆÆÞè㻌
ęĔęĆđàÆ×ÙØè㻌

ĒĎĝĊĉ㻌
đĔĆĉÆàÆÆÜÚè㻌
ĘęĔėĊàÆÆÆÚè㻌
ęĔęĆđàÆÆÜÞè㻌

Figure 8: The preconditioning kernel and amount of transferred data.

1666

GPU acceleration of a non-hydrostatic ...

Takateru Yamagishi and Yoshimasa Matsumura

Figure 9: The distribution of the surface velocity (vector) and the anomaly from CPU (shade). Left:
CPU, Center: GPU, Right: GPU with mixed precision preconditioning

Figure 10: The distribution of temperature (contour) and the anomaly from CPU (shade) at the central
cross section. Top Left: CPU, Top right: GPU, Bottom: GPU with mixed precision preconditioning

Most of the preconditioning kernels are accelerated by the application of mixed precision to the
preconditioning. For example, one of the matrix multiplication kernels in the preconditioning,

1667

GPU acceleration of a non-hydrostatic ...

Takateru Yamagishi and Yoshimasa Matsumura

apply3d3, ran 2.0 times faster with the application of mixed precision on the NVIDIA K20c (Table 3).
The performance of this kernel is highly dependent on data transfer between the GPU processor and
global memory. We compared the amount of transferred data for the two types of precision: the ratio
of the transferred data in the preconditioning with double precision to that with mixed precision is
132/68 = 1.9, which is consistent with the 2.0 times speedup (Figure 8).
For the output of the experiment over 3600 time steps (equivalent to 5 days simulation), all
experiments reproduced growing meanders of ocean current because of baloclinic instability (Figure
9) and vertical convection of water (Figure 10). Although output errors existed because of this
implementation, they would not significantly influence the outcomes of the oceanic studies.

7 Summary and Future work
We implemented a non-hydrostatic ocean model on a GPU and improved the P/H solver of the
kinaco model by optimizing the memory access, using instruction-level parallelism, and applying a
mixed precision calculation to the preconditioning of the P/H solver. The GPU-implemented model
was 4.7 times faster than the comparable execution on a CPU, and the measures to improve the P/H
solver were effective. With the improved P/H solver, the output errors should not significantly
influence oceanic studies. This study demonstrates a numerical ocean model that is suitable for GPU
implementation in terms of both high performance and output accuracy.
There is potential for further improvement in this ocean model. The calculations, except for the
P/H solver, are implemented following basic methods; therefore, there is room for further
improvement. Furthermore, we have not utilized shared memory to remove redundant access to global
memory, except in the parallel sum reduction kernel. The memory access could be optimized using
the specific characteristics of the numerical ocean model, such as the uniformity of some coefficients
in the model equations. This study shows that the application of mixed precision is an effective
method and suggests that further research is needed to identify other applicable kernels and to verify
them from both computational and geophysical viewpoints.

Acknowledgement
The authors would like to thank Mr. Takahiro Inoue (Research Organization for Information
Science and Technology) for his instructive suggestions concerning the optimization of the MGCG
solver.

References
[1]
[2]
[3]

1668

F. Bleichrodt, R. H. Bisseling and H. A. Dijkstra, "Accelerating a barotropic ocean model
using a GPU," Ocean Modelling, 2012.
M. Milakov, P. Messmer and T. Bradley, "Accelerating NEMO with OpenACC," GPU
Technology Conference 2013, 2013.
B. v. Werkhoven, J. Maassen, M. Kliphuis, H. A. Dijkstra, S. E. Brunnabend, M. v.
Meersbergen, F. J. Seinstra and H. E. Bal, "A distributed computing approach to improve the
performance of the Parallel Ocean Program (v2.1).," Geoscientific Model Development
Discussions, 2013.

GPU acceleration of a non-hydrostatic ...

[4]
[5]
[6]
[7]
[8]
[9]
[10]
[11]
[12]

[13]

[14]

Takateru Yamagishi and Yoshimasa Matsumura

S. Xu, X. Huang, L. -Y. Oey, F. Xu, H. Fu, Y. Zhang and G. Yang, "POM.gpu-v1.0: a
GPU-based Princeton Ocean Model," Geoscientific Model Development, 2015.
Y. Matsumura and H. Hasumi, "A non-hydrostatic ocean model with a scalable multigrid
Poisson solver," Ocean Modelling, 2008.
Y. Matsumura and H. Hasumi, "Modeling ice shelf water overflow and bottom water
formation in the southern Weddell Sea.," Journal of Geophysical Research: Oceans, 2010.
Y. Matsumura and H. Hasumi, "Dynamics of cross-isobath dense water transport induced
by slope topography.," Journal of Physical Oceanography, 2011.
M. J. Grote and T. Huckle, "Parallel Preconditioning with Sparse Approximate Inverses,"
SIAM J. Sci. Comput., 1997.
W.-P. Tang and W. L. Wan, "Sparse Approximate Inverse Smoother for Multigrid,"
SIAM Journal on Matrix Analysis and Applications, 2000.
O. Bröker and M. J. Grote, "Sparse approximate inverse smoothers for geometric and
algebraic multigrid," Applied Numerical Mathematics, 2002.
V. Volkov, "Better performance at lower occupancy," GPU Technology Conference
2010, 2010.
N. Goodnight, C. Woolley, G. Lewin, D. Luebke and G. Humphreys, "A multigrid solver
for boundary value problems using programmable graphics hardware," ACM SIGGRAPH
2005 Courses, 2005.
Z. Feng and Z. Zeng, "Parallel multigrid preconditioning on graphics processing units
(GPUs) for robust power grid analysis," Proceedings of the 47th Design Automation
Conference, 2010.
M. Visbeck, J. Marshall and H. Jones, "Dynamics of Isolated Convective Regions in the
Ocean," Journal of Physical Oceanography, 1996.

1669

