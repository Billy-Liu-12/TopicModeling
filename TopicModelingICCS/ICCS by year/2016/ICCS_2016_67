Procedia Computer Science
Volume 80, 2016, Pages 1897–1905
ICCS 2016. The International Conference on Computational
Science

Energy Study of Monte Carlo and Quasi-Monte Carlo
Algorithms for Solving Integral Equations
Todor Gurov1 , Aneta Karaivanova1, and Vassil Alexandrov23
1

3

IICT-BAS, Acad. G. Bonchev St., Bl. 25A, Soﬁa 1113, Bulgaria
(anet,gurov)@parallel.bas.bg
2
ICREA-BSC, C/ Jordi Girona 29, 08034, Barcelona, Spain
vassil.alexandrov@bsc.es
Distinguished Visiting Professor, Tecnologico de Monterrey (ITESM), Campus Monterrey, Mexico

Abstract
In the past few years the development of exascale computing technology necessitated to obtain
an estimate for the energy consumption when large-scale problems are solved with diﬀerent
high-performance computing (HPC) systems. In this paper we study the energy eﬃciency of
a class of Monte Carlo (MC) and Quasi-Monte Carlo (QMC) algorithms for a given integral
equation using hybrid HPC systems. The algorithms are applied to solve quantum kinetic
integral equations describing ultra-fast transport in quantum wire. We compare the energy
performance of the algorithms using a GPU-based computer platform and CPU-based computer
platform both with and without hyper-threading (HT) technology. We use SPRNG library and
CURAND generator to produce parallel pseudo-random (PPR) sequences for the MC algorithms
on CPU-based and GPU -based platforms, respectively. For our QMC algorithms Sobol and
Halton sequences are used to produce parallel quasi-random (PQR) sequences.
We compare the obtained results of the tested algorithms with respect to the given energy
metric. The results of our study demonstrate the importance of taking into account not only
scalability of the HPC intensive algorithms but also their energy eﬃciency. They also show the
need for further optimisation of the QMC algorithms when GPU-based computing platforms
are used.
Keywords: Monte Carlo and Quasi-Monte Carlo algorithms, Integral Equations, Energy Study

1

Introduction

In the past decade the development of energy-eﬃcient algorithms has become more important
with increasing power of supercomputer systems and growing size of the applied problems
[1, 2]. The newest complex extreme-scale computing systems, based on diverse computing
devices (CPU, GPU, accelerators) posed the question of scalability in the light not only of
parallel eﬃciency, but also in terms of energy consumption. The Flops/Watt (F/W) metric
Selection and peer-review under responsibility of the Scientiﬁc Programme Committee of ICCS 2016
c The Authors. Published by Elsevier B.V.

doi:10.1016/j.procs.2016.05.492

1897

Energy Study of MC and QMC Algorithms ...

Gurov T. at al.

was introduced and now this metric is used as the standard in measuring the energy eﬃciency
of supercomputing systems (see [3]). Taking into account the probability of an error during
a run, some authors [4, 5] propose the time to solution metric for estimating the algorithm
performance. This metric can be written in the following way:
F (T, T0 ) × E,

(1)

where E is the cost of consumed energy; F (T, T0 ) is the penalty function; T is the execution
time of the given algorithm, and T0 is the ﬁxed time, i.e. the time in which we want the task
(running job) to be completed.
In this work we concentrate on studying energy eﬃciency of MC and QMC algorithms for
integral equations on extreme-scale parallel computing systems with respect to metric (1). The
extreme-scale parallel computing systems are HPC systems with low latency interconnection,
equipped with GPGPU devices and/or co-processors to speed up the calculations that make
use of thousands of processor cores in high-density deployment. Such supercomputers are called
hybrid HPC systems if they are equipped additionally with CPU-based platform. The integral
equations that we solve with MC and QMC algorithms describe ultrafast carrier transport in
semiconductors, namely, a femtosecond relaxation process of optically excited electrons which
interact with phonons in one-band semiconductor [6]. In fact we consider the inhomogeneous
case - ultrafast transport in quantum wire with applied electric ﬁeld, which is the more realistic
case [7].
The paper is organized as follows: In the next section we present the quantum-kinetic
equation in the integral form and the MC and QMC algorithms for solving this equation.
The parallel implementation of the algorithms using CPU-based and GPU-based computer
platforms is also described. The numerical tests and our ﬁndings are presented in section 3. In
the conclusion we summarise the main outcomes.

2

Solving Integral Equations by Monte Carlo and QuasiMonte Carlo algorithms

In our study of energy eﬃciency we consider MC and QMC algorithms for solving Wigner
equation for the nanometer and femtosecond transport regime. For this purpose we use the
formulation of the Wigner equation in an inhomogeneous case (in quantum wire - more realistic
case) where the electron evolution depends on the energy and space coordinates [7]. The
electrons, which can be initially injected or optically generated in the wire, begin to interact
with three-dimensional phonons. In this particular case the electron-phonon interaction is
described on the quantum-kinetic level by the Levinson equation [9, 10], but the evolution
problem becomes inhomogeneous due to the spatial dependence of the initial condition. The
direction of the wire is chosen to be z, the corresponding component of the wave vector is
kz . The electrons are in the ground state Ψ(r⊥ ) in the plane normal to the wire, which is an
assumption consistent at low temperatures. The initial carrier distribution is assumed Gaussian
both in energy and space coordinates, and an electric ﬁeld can be applied along the wire.
The integral representation of the quantum kinetic equation for the electron Wigner function
1898

Energy Study of MC and QMC Algorithms ...

Gurov T. at al.

fw in this case has the form [8]:
t
t
¯F 2
h
¯ kz
h
t+
t , kz , 0) +
dq⊥ dkz ×
dt
dt
m
2m
0
t
¯hF 2
¯hq
hkz
¯
(t − t ) +
(t − t 2 ) + z (t − t ), kz , t
S(kz , kz , t , t , q⊥ )fw z −
m
2m
2m
hkz
¯
¯hF 2
h
¯
q
(t − t ) +
(t − t 2 ) − z (t − t ), kz , t
−S(kz , kz , t , t q⊥ )fw z −
m
2m
2m

fw (z, kz , t) = fw (z −

(2)

.

Here, fw (z, kz , t) is the Wigner function described in the 2D phase space of the carrier wave
vector kz and the position z, and t is the evolution time. The kernel S(kz , kz , t , t , q⊥ ) of the
integral equation in (2) is well described in [7, 8].
In the inhomogeneous case the wave vector (and respectively the energy) and the density
distributions are given by the integrals
f (kz , t) =

dz
fw (z, kz , t);
2π

n(z, t) =

dkz
fw (z, kz , t).
2π

(3)

MC algorithms [19, 11] and QMC algorithms [16] are developed to estimate the quantities (3) and the Wigner function (2). To sample numerical trajectories in the MC algorithms
SPRNG library [14] and CURAND generator is used to produce parallel pseudo-random (PPR)
sequences. In the case of QMC algorithms we use low-discrepancy sequences to sample numerical trajectories. The computations were performed with both Sobol and modiﬁed Halton
sequences. The implementations of these sequences are given in [12, 17] and [13].

2.1

Parallel implementation

The MC and QMC algorithms are perceived as computationally intensive, but naturally parallel. The so-called “master-worker” model is usable for dynamic load-balancing, while static
load-balancing is suﬃcient in the case when the variations in the computational load can be
controlled. In this model, a large MC task is split into smaller independent subtasks, which are
then executed separately. One process or thread is designated as ”master” and is responsible
for the communications with the ”worker” processes or threads, which perform the actual computations. The partial results are collected and used to assemble an accumulated result with
smaller variance than that of a single copy. In the MC algorithms when the subtasks are of the
same size, their computational time is also similar, i.e., we can use static load balancing. In the
case of QMC computations where one can not aﬀord to lose computations we use the blocking
approach in the generation of the sequences, combined with static load balancing. Our parallel
implementation uses MPI for the CPU-based parallelisation and CUDA for the GPU-based
parallelisation.

2.2

Implementation on CPUs and GPUs

In our numerical tests we compared the CPU performance of the parallel code with and without
hyper-threading (HT) for both type algorithms. We note that some codes using HT do not
improve the overall speed of calculations because the ﬂoating point units of the processor are
shared between the threads. In our experience substantial gains may be achieved without any
additional coding eﬀort by simply using logical instead of physical cores when sizing the launch
of the MPI job. We have observed about 30% improvement of the CPU performance of the MC
1899

Energy Study of MC and QMC Algorithms ...

Gurov T. at al.

algorithms [11] when HT is turned on. This result shows that our overall code is reasonably
eﬃcient.
In the case of GPGPU-based implementation we used CUDA for parallel computations.
Parallel processing is based on splitting the computations on a grid of threads. We use thread
size of 256, which is optimal taking into account the relatively large number of registers. Parallel
quasi-random (PQR)generators for the scrambled Sobol sequence and modiﬁed Halton sequence
have been developed and tested in our previous works [12, 13].

3

Numerical Results and Energy Eﬃciency Study

The MC and QMC algorithms under considaration were tested on the heterogeneous cluster
HPCG in the HPC Centre of the Institute of Information and Communication Technologies,
Bulgarian Academy of Sciences (IICT-BAS) [15].
This cluster combines CPU-based computing platform with 36 blades BL 280c (dual Intel
X5560, 24 GB RAM per a blade,total 576 CPU logic cores) and GPGPU-based platform with
high-end GPU computing devices (2 HP ProLiant SL390s G7 servers, each with dual Intel Xeon
E5649, 96 GB RAM, with a total of 16 NVIDIA Tesla M2090 graphic cards). Both computing
platforms are connected with 2 InﬁniBand switches and use 3 storage ﬁle systems with a total of
132 TB storage. We note that GPUs have continued to increase in terms of energy usage, while
CPU designers have recently focused on improving performance per Watt. High performance
GPUs may now be the largest power consumer in a system. The peak performance of any
system is essentially limited by the amount of power it can draw. Consequently, performance
per watt of a GPU design translates directly into peak performance of a system that uses that
design.
The numerical results presented in Tables 1-3 and on Figures 1-3 are obtained for the problem
with GaAs material parameters: the electron eﬀective mass is 0.063, the optimal phonon energy
is 36 meV, the static and optical dielectric constants are εs = 12.9 and ε∞ = 10.92. The
initial condition is a product of two Gaussian distributions of the energy and space. The kz2
distribution corresponds to a generating laser pulse with an excess energy of about 180 meV.
The z distribution is centered around zero. The side of the wire is chosen to be 10 nanometers.
The number of the realized numerical trajectories in MC and QMC algorithms is N = 10
million. For the MC algorithms we use SPRNG library [14] to sample numerical trajectories on
the CPU-based computing platform while we use CURAND generator on the GPGPU-based
platform. Sobol and Halton sequences are used to produce PQR sequences for QMC algorithms.
In our tests the price of energy is assumed to be 10 euro cents per 1KW h. In case there are
no running jobs the energy consumption of n number of CPU blades or GPU devices is denoted
by W0 . When we have running jobs using n number of devices than the energy consumption is
denoted by Wn . The diﬀerence ΔWn = Wn − W0 is attributed to the computational workload
being run. The function F (T ) that penalizes the algorithms is chosen in the following way:
F (T ) = exp (a|T /T0 − 1|).
Using this penalty function allows the metric (1) to ﬁnd the minimum number of blades and
GPU cards necessary for the completion of the task (running job) for a ﬁxed time - T0 , leaving
the energy cost close to the minimum. In our test T0 = 600s and the parameter a = 1. The
parameter a is chosen so that the results are more easily comparable for all cases.
The test results for MC and QMC algorithms using CPU devices with and without HT
are shown in Tables 1 and 2. In each table the optimal values are underlined. Looking at
1900

Energy Study of MC and QMC Algorithms ...

Gurov T. at al.

Figure 1: Compering the energy cost using CPU devices without HT for the MC and QMC
algorithms.

Figure 2: Compering the energy cost using CPU devices with HT for the MC and QMC
algorithms.

1901

Energy Study of MC and QMC Algorithms ...

Gurov T. at al.

Table 1: Test results for the MC and QMC algorithmits using CPU devices without HyperThreading.
PPR sequence/
PQR sequence

Blades/
Cores

CPU
Time (s)

ΔWn

Energy
cost: E

Energy metric
F (T ) × (E)

SPRNG

1/8
2/8
4/8
8/8
16/8

2728.02
1545.63
702.91
350.89
193.64

211
353
831
1863
2819

16.00
15.15
16.22
18.16
15.16

955.95
92.12
24.11
31.31
39.78

Sobol

1/8
2/8
4/8
8/8
16/8

2401.52
1206.56
605.12
307.55
160.12

272
415
861
1624
3152

18.14
13.91
14.47
13.87
14.02

365.37
38.22
14.60
22.59
29.18

Halton

1/8
2/8
4/8
8/8
16/8

2499.10
1254.81
631.52
355.82
179.81

372
435
951
1622
3073

25.82
15.16
16.68
16.03
15.35

611.84
45.16
17.58
24.08
30.92

Table 2: Test results for the MC and QMC algorithmits using CPU devices with HyperThreading.
PPR sequence/
PQR sequence

Blades/
Cores/HT

CPU
Time (s)

ΔWn

Energy
cost: E

Energy metric
F (T ) × (E)

SPRNG

1/8/2
2/8/2
4/8/2
8/8/2
16/8/2

1818.17
904.50
456.96
231.15
123.20

609
712
1130
1767
3624

15.98
18.63
12.47
14.10
13.33

121.71
30.94
15.82
26.07
29.51

Sobol

1/8/2
2/8/2
4/8/2
8/8/2
16/8/2

1555.49
780.93
388.36
195.33
100.56

414
543
1081
1987
3689

17.89
11.78
11.66
10.78
10.30

87.94
15.92
16.59
21.16
23.69

Halton

1/8/2
2/8/2
4/8/2
8/8/2
16/8/2

1587.63
803.14
403.22
222.39
121.20

609
712
1130
1767
3624

26.86
15.88
12.66
10.91
12.20

139.29
22.28
17.57
20.47
27.10

the results we see that there is a diﬀerence between the SPRNG, Sobol and Halton sequences.
Here we observe that by turning on the hyper-threading in the case of Sobol sequence one can
achieve the same metrics with just 2 blades instead of 4. For both the Halton sequences and
the SPRNG sequences it is optimal to use 4 blades.
The energy cost for the diﬀerent algorithms using CPU-based computations with and without HT is compared on Figures 1-2. We see that QMC algorithms consume a little bit less
1902

Energy Study of MC and QMC Algorithms ...

Gurov T. at al.

Table 3: Test results for the MC and QMC algorithms using GPU devices.
PPR sequence/
PQR sequence

NVIDIA
Tesla M2090

GPGPU
Time (s)

ΔWn

Energy
cost: E

Energy metric
F (T ) × (E)

CURAND

1
2
4
8
16

564.34
292.37
164.41
105.28
87.56

398
555
895
1404
2525

6.24
4.51
4.09
4.11
6.14

6.62
7.53
8.45
9.36
14.42

Sobol

1
2
4
8
16

893.24
456.32
246.15
155.88
103.97

156
316
638
1295
2419

3.87
4.01
4.36
5.61
6.99

6.31
5.09
7.87
11.75
15.97

Halton

1
2
4
8
16

803.85
411.01
228.86
142.07
98.24

158
307
614
1094
2494

3.53
3.50
3.90
4.32
6.81

4.96
4.80
7.25
9.26
15.71

Figure 3: Compering the energy cost using GPU devices for the MC and QMC algorithms.

energy that MC algorithms as in the case of HT the results are improving. Taking in the
account that QMC algorithms converge to the exact solution better than MC algorithms [18]
we can conclude that QMC algorithms in our case are preferable for CPU-based computational
platform.
In the case of GPGPU computations the test results for both algorithms are presented in
Table 3 while the energy cost using diﬀerent number of GPU cards is shown on Figure 3. We
observe that in case of small number of GPU devices the energy cost of the QMC algorithms
is smaller that for MC algorithm. With increasing the number of GPU devices the picture is
changed. This situation can be explained by the fact that algorithms are not optimized for
1903

Energy Study of MC and QMC Algorithms ...

Gurov T. at al.

calculations with many cards. In part this is due to the relatively high initial startup overhead
for the CUDA and MPI implementations.
Comparing results using both computational platform we can conclude that GPU-based
platform is preferable because the energy cost is reduced by 30% to 40%.

4

Conclusion

In this paper we compared the energy performance of a class of MC and QMC algorithms for
solving quantum kinetic integral equations describing ultra-fast transport in quantum wire. The
algorithms were implemented on a GPU-based computer platform and on CPU-based computer
platform with and without hyper-threading. Although the quasi-random sequence generators
are not optimized for the GPU-based platform, the QMC algorithms demonstrate better energy
eﬃciency. As a penalty function in our work we used an exponential function. The purpose
of this function is to ﬁnd the minimal number of CPU/GPU devices needed to solve the given
problem for a ﬁxed execution time.
The new computing platforms lead to new challenges in programming. That is why it is
necessary to optimize the codes (in some cases they need rewriting) of the existing quasi-random
generators in order to exploit the advantages of the computer hardware with accelerators. In
our future work we plan to compare the energy performance of MC and QMC algorithms for
solving linear algebra problems. We also foresee porting these algorithms on new extremescale parallel system Avitohol [20], which consists of 150 heterogeneous computing HP Cluster
Platform SL250S GEN8 servers, equipped with 2 Intel Xeon E 2650 v2 CPUs and 2 Intel Xeon
Phi 7120P coprocessors and study the eﬃciency there.

Acknowledgment
This work was supported by the National Science Fund of Bulgaria under Grant DFNI-I02/8.

References
[1] Demmel, J., Gearhart, A., Lipshitz, B., Schwartz, O., Perfect Strong Scaling Using No Additional
Energy. Proc. of IEEE 27th IPDPS13. IEEE Computer Society, 2013.
[2] Meswani, M., Carrington, L., Unat, D., Peraza, J., Snavely, A., Baden, S., and Poole, S., Modeling
and Predicting Application Performance on Hardware Accelerators, International Journal of High
Performance Computing (2012).
[3] The Green 500, www.green500.org, EEHPCWG PowerMeasurementMethodology.pdf, 2015
[4] Bekas, C., Curioni, A., A new energy aware performance metric, Springer, Comput. Sci. Res Dev
(2010) 25: 187-195, DOI 10.1007/s00450-010-0119-z.
[5] Bekas, C., Curioni, A., Fedulova, I., Low cost high performance uncertainty quantiﬁcation, Workshop on HPC ﬁnance, SC09, Portland, OR, USA, (November 2009).
[6] Gurov, T.V., Nedjalkov, M., Whitlock, P.A., Kosina, H., Selberherr, S., Femtosecond Relaxation
of Hot Electrons by Phonon Emission in Presence of Electric Field, Physica B, vol. 314, pp. 301304,
(2002).
[7] Nedjalkov, M., Gurov, T., Kosina, H., Vasileska D., Palankovski, V., Femtosecond Evolution of
Spatially Inhomogeneous Carrier Excitations: Part I: Kinetic Approach, Springer LNCS, 3743,
pp. 149-156, (2006).

1904

Energy Study of MC and QMC Algorithms ...

Gurov T. at al.

[8] Gurov, T., Atanassov, E., Dimov I., Palankovski, V., Femtosecond Evolution of Spatially Inhomogeneous Carrier Excitations: Part II: Stochastic Approach and GRID Implementation, Springer
LNCS, 3743, pp. 157-163, (2006).
[9] Levinson, I., Translational invariance in uniform ﬁelds and the equation for the density matrix in
the Wigner representation, Sov. Phys. JETP, 30, pp. 362-367, 1970.
[10] Herbst, M., Glanemann, M., Axt, V., and Kuhn, T., Electron-phonon quantum kinetics for spatially inhomogeneous excitations, Physical Review B, 67, 195305:1–18, 2003.
[11] Karaivanova, A., Atanassov, E., and Gurov, T., Monte Carlo Simulation of Ultrafast Carrier
Transport: Scalability Study, ICCS2013, Published by Elsevier B.V., Procedia Computer Science,
v.18, 2298 2306, (2013).
[12] Atanassov, E., et all, Tuning the Generation of Sobol Sequence with Owen Scrambling, LNCS
5910, Springer, 459-466, 2010.
[13] Atanassov, E. I. and Durchova, M. K., Generating and testing the modiﬁed Halton sequences,
LNCS 2542, Springer, 91-98, 2003.
[14] Scalable Parallel Random Number Generators Library for Parallel Monte Carlo Computations,
http://www.sprng.org/ , 2016.
[15] Atanassov, E., Gurov, T., Karaivanova, A., Ivanovska, S., Durchova, M., Georgiev, D., Dimitrov,
D., Tuning for Scalability on Hybrid HPC Cluster, Mathematics in Industry, Cambridge Scholar
Publishing, pp. 64-77, (2014).
[16] E. Atanassov et all, Ultra-fast Semiconductor Carrier Transport Simulation on the Grid, Sci. Int.
J. for Par. and Dist. Comp., SCPE, Vol. 11, no.2, 137-147, 2010.
[17] Sobol, I., Asotsky, D., Kreinin, A., Kucherenko, S., Construction and Comparison of HighDimensional Sobol Generators. Wilmott Journal Nov: 6479, (2011) .
[18] Karaivanova, A., Atanassov, E., Gurov, T., Ivanovska S., Durchova, M., Parallel Quasi-Random
Applications on Heterogeneous Grid, Journal of Scalable Computing: Practice and Experience
(SCPE), Vol. 11, no.1, pp.73-80, (2010).
[19] Atanassov, E., Gurov, T., Karaivanova, A., Nedjalkov, M., Vasileska, D., Raleva, K., Electronphonon interaction in nanowires: A Monte Carlo study of the eﬀect of the ﬁeld, Mathematics and
Computers in Simulation, No: 81, pp. 515-521, (2010).
[20] Avitohol HPC system, http://www.hpc.acad.bg/index.php/system-1/, 2016.

1905

