Procedia Computer Science
Volume 80, 2016, Pages 2128–2140
ICCS 2016. The International Conference on Computational
Science

Data quality control for St. Petersburg flood
warning system
Jose Luis Araya Lopez1, Anna V. Kalyuzhnaya1, Sergey S. Kosukhin1,
Sergey V. Ivanov1
1

ITMO University. St. Petersburg, Russian Federation
jlaraya1978@gmail.com, kalyuzhnaya.ann@gmail.com

Abstract
This paper focuses on techniques for dealing with imperfect data in a frame of early warning system
(EWS). Despite the fact that data may be technically damaged by presenting noise, outliers or missing
values, met-ocean simulation systems have to deal with them to provide data transaction between
models, real time data assimilation, calibration, etc. In this context data quality-control becomes one of
the most important parts of EWS. St. Petersburg FWS was considered as an example of EWS. Quality
control in St. Petersburg FWS contains blocks of technical control, human mistakes control, statistical
control of simulated fields, statistical control and restoration of measurements and control using
alternative models. Domain specific quality control was presented as two types of procedures based on
theoretically proved methods were applied. The first procedure is based on probabilistic model of
dynamical system, where processes are spatially interrelated and could be implemented in a form of
multivariate regression (MRM). The second procedure is based on principal component analysis
extended for taking into account temporal relations in data set (ePCA).
Keywords: outliers, quality-control, principal components, gap filling

1 Problem statement
In the context of the data revolution, the presence of data has become ubiquitous in many fields.
We observe that substantial amount of data is publicly available for scientific purposes. Examples
include repositories with government statistics; historical weather information, forecasts; DNA
sequencing; information on traffic conditions in large metropolitan areas; product reviews and
comments in the social networks; demographics, pictures, and videos. We also find information
gathered using citizen-science platforms; and data collected by sensor networks measuring parameters
such as temperature, air humidity, atmospheric pressure and precipitation (Assunção et al., 2015).
There has been a lot of discussion about the best practice for establishing data interoperability in
computing systems with integration of heterogeneous information sources. In the context of early
warning systems (EWS) we find that the main types of data that are normally targeted for this purpose

2128

Selection and peer-review under responsibility of the Scientiﬁc Programme Committee of ICCS 2016
c The Authors. Published by Elsevier B.V.

doi:10.1016/j.procs.2016.05.532

Data Quality Control for Saint Petersburg ﬂood warning system

J. L. Araya-Lopez et al

are: Geographical Information Systems (GIS) layers, outputs of models, observed data from sensor
networks, knowledge of experts. However, in relation to EWS the historical and real-time data records
of environmental variables play a very important role. So it is clear why it is important to tend to
facilitate and simplify the procedures at any level of the process.
In relation to environmental hazard prevention systems, there are some peculiar aspects of the
operational work that are most of the times neglected and not properly referred in the literature, since
they are considered to be obvious parts of the process. However, the problem of data management and
the way that the inputs have to be handled remain a current issue in the operational work of those
dealing with natural hazard prevention systems (Cannata et al., 2013). When dealing with emergency
simulation it is necessary to find a balance between the urgency of the situation, fulfillment of
deadlines and requirements with respect to data quality (Cencerrado et al., 2012). Establishing a
proper urgent computing system requires quality control procedures and on-demand access to
resources. The human factors are also very important, since not only generation of accurate results is
critical, but also the correct interpretation and dissemination to the users (Urgent Computing, 2008).
Model input data varies extensively depending on the goal of the EWS. The efforts in this direction
are focused on defining a standard operational collection system in which observation and related data
are stored (Beven, 2002). Other sources for input models may originate from published data by
national agencies or other institutions. The sources vary extensively at different levels depending on
the purpose and goal of the hazard prevention system. This can be noticed in those studies that involve
measurements at different, similar sites. This occurs frequently in the area of environmental sciences
and it is considered a good practice, although there may be constraints due to economic or technical
concerns. In these open heterogeneous systems, it is recommended that the inputs for the models shall
be the same despite the plurality of data sources available, since it has been demonstrated that in some
cases the difference between replicates may become large and inconsistent over time (de Roo et al.,
2003).
The existence of data itself does not guarantee that we are adhering to good practices about quality.
A broader definition of data quality implies that good data quality is achieved when data are
comprehensive, understandable, consistent, relevant and timely. Besides, the quality control stages
should consider aspects such as completeness, consistency, validity, conformity, accuracy and integrity
(Singh and Singh, 2010). Some data issue problems that may require correction are the following
(Palepu and Rao 2012): inadequate selection of data sources, no validation routines at the sources,
unnoticed changes of data sources, multiple data sources generate heterogeneous outputs that lead to
data quality concerns, contradictory information embedded in data sources, inappropriateness in data
formats, multiplicity of sources for the same data, inconsistency in the use of special characters in the
data.
The other point in which one may require to be careful in the whole process is at the extraction,
transformation and loading of data. The data issues at this level include: influence of the data
warehouse architecture on the data quality, relational and non relational effects on the data warehouse,
problems cause by some domain rules that end up affecting the quality of the data warehouse and
finally lack of periodical update of the data. Other key aspects are the data warehouse structure and
architecture, which should be defined according to the goals of the main project, according to quality
control standards and using the best quality control tools available, customer requirements and
participation of stakeholders (Palepu and Reao, 2012)
This paper discusses problems of data quality control in St. Petersburg flood warning system with
emphasis on domain specific quality-control that include detection of outliers in dataset and missing
values restoration.

2129

Data Quality Control for Saint Petersburg ﬂood warning system

J. L. Araya-Lopez et al

2 Data quality control in St. Petersburg flood warning system
The FWS in St. Petersburg was designed for prediction and prevention of coastal floods. In the
eastern part of the Gulf of Finland coastal floods are caused by storm surges (Boukhanovsk et al.,
2012). Deep cyclones activity over the central part of Baltic Sea is the main reason for storm surge
generation, that is why forecasting system have to provide simulations of met ocean fields for whole
Baltic Sea grid. St. Petersburg FWS is running for managing the Flood Protection Barrier [St.
Petersburg Flood Prevention Facility Complex. http://dambaspb.ru ] in cases of flood
St. Petersburg FWS consists of forecasting system and Met-ocean data warehouse (fig. 1).
Forecasting system include models that provide a complex simulation for prediction a sea level in St.
Petersburg: Numerical Weather Prediction (NWP) models, sea waves model, sea dynamics models.
Each model in forecasting system is coupled with data assimilation procedure. Forecasting models
which are in operation in St. Petersburg FWS include three NWP model WRF (Michalakes, 2004),
three sea dynamics models (: two-dimensional model BSM (property of barrier authority), threedimensional model Balt-P (Russian central met-office), three-dimensional model NEMO. Forecasting
system also includes model for generation of a schedule for gates maneuvering process that provides
optimal time schedule (for gates closure and opening) to decision making group. Except for core
system designed for only one task of prevention, FWS contains additional models which allow
organizing effective management of barrier structures and city territory and taking into account
unfavorable scenarios (dry land inundation, evacuation).
In the context of early warning systems, we should admit that all pieces of information that might
be used in process of flood forecasting and prevention are imperfect and contain mistakes, gaps or are
absent. Although, problem of imperfect information could be addressed to models and their structure,
its role should be decreased on stages of model adjustment, calibration, data assimilation and others
procedures of uncertainty management process (Kalyuzhnaya and Boukhanovsky, 2015). In this paper
we discuss the problem of imperfect information addressed to data flows in FWS.

2130

Data Quality Control for Saint Petersburg ﬂood warning system

Wave spec.
fields
Sea level and
currents fields
Maneuvering
schedules
Structures
stability results
Spreaded
pollution fields
Evacuation
model results

Technical
control

Output

Simulation system

River flow

Statistical control
of simulated fields

Sea level

Data
assimilation

Numerical
Weather
Prediction model

Data
assimilation

Sea waves model

Data
assimilation

Sea dynamics
model

Gates maneuvering schedule
generator

Human mistakes control

Human mistakes control layer

Input

Control using
alternative models

Measurements

Sea waves

Atmospheric
fields

Simulation results

Metocean data warehouse

Statistical control of
simulated fields

Technical
control

Meteo

Technical
control

Sensors

Input

Statistical control and
restoration of measurements

External
storage
Internet
resources

Simulation results

External
storage
Internet
resources

Measurements

Input and alternative data sources

Data quality control layer

J. L. Araya-Lopez et al

Barrier hydraulic structures
stability model
Dry land inundation (residual)
model

Evacuation
model

Pollution
spreading
model

Figure 1. Scheme of St. Petersburg FWS with quality control blocks
The Metocean data warehouse is a collector for all that comes from outside and generated inside
the FWS. Data warehousing conception assumes a few principles for data storing, as well as principles
for data processing and implementation of heterogeneous data sources as unified source of
information. Taking into account that all computational models display different formats for input and
output data, an unified format for data storing and processing is a must (Ivanov, et. al., 2012).
Moreover, data management in a frame of FWS assumes that all pieces of information are important
(we do not want to have extra losses in data due to the quality assessment procedures). At the same
time, quality assurance is an extremely important issue for such critical system as St. Petersburg FWS.
These facts set extended requirements for quality-control procedures in St. Petersburg flood warning
system.
The Scheme of St. Petersburg FWS on figure 1 contains several quality control blocks: technical
control, human mistakes control, statistical control of simulated fields, statistical control and
restoration of measurements and control using alternative models.
Technical control assumes checking all the data for the validity and completeness, control of
adequacy of data set (we should receive what we expect), control of relevance of data set (for real time
forecast system data should be received in proper time).
In most cases in critical systems are commonly used pre-configured models. But despite this fact
the problem of human mistakes remains important because of constant process of improvement and
extension of the system by new models.
Transformation to appropriate unified data format and consistency data analysis are typical
technical tasks for data processing systems and can be resolved using a set of well-known approaches.

2131

Data Quality Control for Saint Petersburg ﬂood warning system

J. L. Araya-Lopez et al

A more interesting approach is domain specific quality control of data that can be designed on a base
of statistical analysis. Domain specific quality control of data should be implemented as a procedure
that takes into account the nature and specific features of the investigated process. In a frame of EWS
tasks, an essential part of implementation process is a detailed analysis and verification of the solution
before it can be available for usage in life critical systems. Statistical quality control and data
restoration of measurements assumes domain-specific procedures for quality assurance. These
procedures could be based on probabilistic models for spatial and temporal relations and should be
determined and verified for each process (wind, sea waves, sea level and currents, etc.).
Sometimes models give unexpected results caused by one or a set of factors: inappropriate input
data, incorrect observational data, internal model errors (e.g. inaccessible accuracy of solution for
numerical convergence), etc. In forecasting systems incorrect simulation results can transfer from one
model (as output) to another (as input), propagating and increasing the error. That is why it is
important to apply procedure for quality control of simulated data at every step of forecasting process
(or for each simulation block). Hydrodynamic model simulation results assume not only time series in
several points of interest but time sequence of fields of output variables. For quality control of
simulated fields it is plausible to apply statistical methods that are similar to those for statistical
control of measurements. Also, control of model results could be achieved by methods based on
alternative models. Such methods include different ensemble techniques (Kalyuzhnaya and
Boukhanovsky, 2015) as ensemble forecasts (single and interval), comparison and proximity analysis
of alternative results.

3 Checking data for the validity and completeness
Checking the validity and completeness of the data is related to the need to anchor the results of
calculations to the input data, executable files and computational configuration. The problem is that
the simulation workflow and all input data for it consist of hundreds of files which cannot be
controlled manually. For this purpose, we have implemented a mechanism for checking the validity
and completeness of the data, based on the concept of provenance (Davidson, 2007). The basic idea is
the indexing of each file in the system (including the executables) with a hash tag using the MD5
algorithm. In the case of configuration files, implying dynamic data, we store a hash tag of the
template. Calculations are only possible in predetermined configurations (combinations of input files
with hash tags). This combination regulates the specific list of files that should be used to run the
simulation. Before running the simulation, special module “Combinations checker” checks files
downloaded from the data storage. Checking procedure means that we again compute hash tag for
every file and compare results with data from a predefined permitted combination (downloaded from
storage too).
Simulation Стенд
Server
Simulation
им. Дениса
permission
(данные
после расчета
Simulation
удаляются)
framework

Combinations checker
2013.12.11_12_00.bin
2013.12.11_18_00.bin
2013.12.11_12_00.dat
…...
2013.12.11_18_00.dat
…...

Data Transfer
(output)

Forecast

Storage

Project
templates

data

Data Transfer
(input)

Configurations

Executable

Simulation results (with provenance)

Predefined
permitted
combinations

Figure 2. Scheme of the checking the validity and completeness of the input data for simulation

2132

Data Quality Control for Saint Petersburg ﬂood warning system

J. L. Araya-Lopez et al

Combination checker gives permission for simulation if only all the hash tags are the same and its
amount is the same as well. Even the situation with too many files is considered as incorrect, and the
simulation will not be launched (see Figure 2). All the unexpected combinations of input files will be
blocked with a particular exception message for the user. This approach ensures the run of simulation
in only predefined combinations if input files. Combinations checker generates a special file with all
hash tags of input data which is stored with simulation results. This allows to understand the
environment of a particular simulation and the re-run the model with guaranteed input. Furthermore, it
does not provide much additional load on the storage as files with hash tags have a size of less than a
few kilobytes. Here, all operations are automated. The user just has to generate files with permitted
combinations of input files.

4 Statistical quality control and missing values restoration in
datasets
There is a large amount of missing data completion methods that may be applied in the field of
atmospheric sciences and oceanography depending on the parameter involved, its temporal and spatial
resolution, the features of the data set and the goals of the study. It is important to point out that the
type of data we discussed here are basically time series, environmental parameters changing in time
that are commonplace in the daily life of practitioners. Basically, there are two types of methods that
can be applied in this context. The first type is those methods that make use of incomplete time series
for estimating its own missing data. The other approach involves the use of several time series that
have some redundant information at a spatial level. By making use of these similarities one can
reconstruct the missing information at the place of interest. In these methods we have Gaussian
process regression methods, optimal interpolation (Gandin, 1988), dimensionality reduction methods,
etc.
Several strategies for detection and validation of outliers were applied. For example, range tests for
checking that the data were inside specific boundaries (Meek and Hatfield, 1994). Also a check for
detecting repeated sequences was applied for the five datasets. As part of the preprocessing stage, it
was necessary to detect and complete the temporal gaps of missing data, which is important for
comparison among placements in synchronous periods of time (Eischeid et al, 1995).
We also applied two different methods for filling gaps in datasets. The problem of missing data is
one of the key aspects to consider when dealing with data. Some methods may be preferred to others
depending on the field, in other cases they may be defined based on the practicality and technical
constraints of the situation. In the case of oceanographic or meteorological fields, the large temporal
and spatial correlation that these fields exhibit motivates the application of a number of strategies that
may take advantage of these properties to estimate missing values. It is a very common situation that
these types of data are time series highly correlated at temporal and spatial levels, which constraint the
range of applicability of many methods that suppose independence on the data. However, there is a
well established body of knowledge regarding dimensional reduction methods that take advantage of
these properties for coming up with estimation of missing information.
Sometimes the gaps can be present for days or months, which complicate the situation for
univariate methods that make use of its own history to make predictions. In these cases, it is very
useful to look at neighboring placements that may help reproduce to some extent the process described
by the lost information in our time series. The weakness of this approach (or its strength) lies on the
fact that one has to count on redundant information for doing the analysis.
As an example we consider statistical quality control of sea level data in St. Petersburg FWS. The
data used in this study were obtained from the official website of the Baltic Operational

2133

Data Quality Control for Saint Petersburg ﬂood warning system

J. L. Araya-Lopez et al

Oceanographic System (www.boos.org). They correspond to sea level hourly data. For this study in
particular the time period that I analyzed goes from November 2009 to December 2010. The data
correspond to hourly sea-level measurements (measured in centimeters) on five different locations,
namely: St. Petersburg, Kronstadt, Sillamae, Tallin and Paldinski. These cities are spread along the
Gulf of Finland.
Problem oriented quality control of sea level data is based on spatial and temporal relations
probabilistic model of sea dynamics. Practically it means that we can use data from past or data from
other stations to control adequacy or restore a gap in another spatial-temporal point. Using this
strategy for problem oriented quality control, system becomes “greedy” for data (the more data the
better), although it is not a common practice for hydrometeorological EWS that traditionally use
spatial gridded observational information for data assimilation. In a frame of data assimilation
problem amount of close neighbored stations could be redundant, but in a frame of quality control
problem the larger number of stations guarantees the higher robustness of measured data.
To provide such a strategy in St. Petersburg FWS two types of procedures based on theoretically
proved methods were applied. The first procedure is based on probabilistic model of dynamical
system, where processes are spatially interrelated and could be implemented in a form of multivariate
regression (MRM). The second procedure is based on principal component analysis extended for
taking into account temporal relations in data set (ePCA).
4.1

The multivariate regression model (MRM)

The first procedure was implemented in a form of multivariate regression that could be
identified in grid points provided with observational data. The model can be constructed using sea
level time series xs (t ) on stations s 1...S , Kij (W ) (with covariance functions between stations i
and j). It is assumed that i = j, which means data restoration and control using only prehistory in one
point. But in general case time series, which is used for data restoration and control, is chosen
according to maximum value of covariance function Kij (W ) between targeted station and others. The
Interval of statistically approved values for

xi (t ) element is based on the expression:

n
¦ F x (t  k ) , where x (k ) are proved values. The F coefficients are defined from
j
z
z j
z
z 1
Yule-Walker equations (in matrix form): AF B , where Al , m Kij (kl  k p ), Bl Kij (kl  t ),
xi (t )

n

l , m 1...n . The residual variance De is defined as: De

¦ Fz Kij (t  k z ) . In order for this to be
z 1

proved, a deviation G of element xi (t ) should be within certain limits: G d G critical , where

G critical

rC De . The coefficient C could be defined using classical approach for interval

estimation with certain confidence level (e.g. for 99% confidence interval C=3) or using empirical
rules. For taking into account fast sea level rising (due to storm surges) in St. Petersburg C is set equal
to 5.
4.2

Extended Principal Components method (ePCA)

The principal component analysis is applied here in the context of hourly sea level data for
satisfying the need of providing complete time series for data assimilation purposes (Wilks, 2006). The
peculiarity of the approach shown here is that the method is applied taking into account the physical
fact that the sea level in the five different locations are related in time, but at different lags. This

2134

Data Quality Control for Saint Petersburg ﬂood warning system

J. L. Araya-Lopez et al

occurs due to the east-west displacement of the crest and valleys of the sea waves that induce related
values in the sea level data across the shore on the Gulf of Finland, which dominates the sea level
dynamics to the extent of affecting the entries in the cross-correlation matrix. So the method starts by
defining introducing an alternate definition of the cross-correlation matrix used in the calculations. We
can express the data matrix as a succession of column-wise vectors X=[x1,x2,... xn ]. X was modified
so that it included the lagged time series for all the time series, so that in the end one ends up with a
lagged matrix. After reshaping our original matrix we calculated a lagged cross-correlation by using
௑ ′ ൈ௑

. The principal components are calculated using cross correlation matrix, so that the
σൌ
௡
associated eigenvalues and eigenvectors are extracted. After that, the principal components Y can be
షభ

calculated using the relationshipܻ ൌ ܺ‫ ܮܧ‬మ , where E corresponds to the eigenvector matrix and L to
భ

the eigenvalue matrix. The original data can be estimated using ܺ ൌ ܻ‫ܮ‬Ԣమ ‫ ் ܧ‬by retaining the most
important principal components. It was necessary to take into consideration the length of the gaps
when doing data restoration.

4.3

Quality estimation tests for MRM and ePCA methods

Several strategies for detection and validation of outliers could be commonly applied. For
example, range tests for checking that the data were inside specific boundaries (Meek and Hatfield,
1994). Also a check for detecting repeated sequences was applied for the five datasets. As part of the
preprocessing, it was necessary to detect and complete the temporal gaps of missing data, which is
important for comparison among placements in synchronous periods of time (Eisheid, 1995). In the
case of the range tests, they allowed to detect values that exceed technical theoretical or climatic
limits. These thresholds are objective and they are easy to determine. These tests can be applied taking
a sensor-based approach; in this case one uses the ranges provided by the sensor's manufacturer to rule
out nonsensical values. On the other hand we have another approach, which consists of looking at the
history of the data extent long in time it is possible to generate very robust climatic thresholds and
calculate ranges that can be used for quality control purposes (Fiebrich et al, 2010). The temporal
checks were also quite useful to spot outliers in time series. They rely on the analysis of the rate of
change of a meteorological parameters, particularly in time, This sort of tests have been studied by
Gaybreal et al ( 2004). They allow extracting atypical values in a very efficient way and they are
easily automated. Their advantage is that they allow detecting large changes for detection and further
validation. These quality control tests are well-known and regularly applied on an operational basis
(Shafer et al 2000). We also made use of persistency checks, which make use of some of domain
knowledge, operational experience, statistical criteria and climatology studies for proposing situations
that may lead to the generation of such errors. Concrete examples of the application of temporal
checks are applied by Upton and Rahimi (2003) and Jimenez et al (2010). Finally, spatial quality
control checks facilitate the detection of atypical values using the information provided by
neighboring stations (Fiebrich et al, 2010). The method also requires defining criteria for the
thresholds so that those values that are considered to be beyond these thresholds can be isolated and
validated.
In our research we run two types of tests for algorithms estimation: we tested ability of
algorithm find and improve suspicious values and ability of algorithms to restore chains of gaps.
Regarding the reference dataset, this corresponds to five hourly time series from five different
locations along the Gulf of Finland. Completeness in this case was a must in order to calculate metrics
for the errors in the different realizations. We introduce a method that makes use of a random number
generator of atypical values that are artificially substituted on the real data (complete datasets),
generating artificial errors on the data and estimating the substituted values with both methods. We

2135

Data Quality Control for Saint Petersburg ﬂood warning system

J. L. Araya-Lopez et al

used data from the Baltic area, hourly time series corresponding to 2163 hours of consecutive data.
The method includes the substitution of 100 values by artificial ones and 10000 iterative substitutions
on the original data sets, so that robust statistics can be generated from the results. Besides, it was
necessary to generate files for the indexes in the original data that in order to substitute those by
synthetic erroneous data. For doing that we generated iterative indexes that allowed to generate those
artificial gaps. We also generated atypical values that were systematically removed in the original
datasets without losing information about their location (row and column indexes), so that
comparisons with the original data could be performed. It was also required to generate artificial gaps
in the datasets, so that one can generate robust statistics about the ability of the filling method to
generate satisfying results at different time gaps. The first test for estimation ability of algorithms to
detect and correct outliers consists of two stages: artificial noise insertion and artificial noise filtering
using algorithms. Artificial noise was generated as Gaussian distributed sample (Figure 3a) of induced
errors in percentage of sea level values. Figure 3 shows the results of some experiments that were
meant to assess the performance of the filling methods for detecting atypical values. We introduce a
method that makes use of a random number generator of atypical values that are artificially substituted
on the real data (complete datasets), generating artificial errors on the data and estimating the
substituted values with both methods. We used data from the Baltic area, hourly time series
corresponding to 2163 hours of consecutive data. The method includes the substitution of 100 values
by artificial ones and 10000 iterative substitutions on the original data sets, so that robust statistics can
be generated from the results. This allowed us to analyze the ability of the imputation methods to be
used as a solution of the detection of the atypical values that may be generated. We saw that the MRM
method tends to detect more atypical values for all the range of induced errors that were generated for
this study. Figure 3b shows the percentage of data detected as a function of the induced error interval
Figure 3c the calculated error as a function of the interval. It can be noticed that the MRM works
better than ePCA in the detection of outliers and in the estimation of missing data. We also made used
ோି௉
of the normalized error (‫ܧ‬ே ), which was defined as ‫ܧ‬ே ൌ ைି௉, where R represents the result of the
estimation, P represents the original value and O is the induced outlier (Figure 3c). Figure 3c shows
couples of box and whiskers plots, which are horizontally aligned for displaying the correspondence
with Figure 3b for the range of induced errors intervals. The vertical axis in Figure 3c shows the
calculated error and how it compares both MRM and ePCA methods. Again, the MRM calculated
errors are less spread than the corresponding ePCA calculated errors in all the induced interval ranges.
Figure 3a displays a density plot of the results and it can be noticed the density distribution of
the induced errors and how these spread in a Gaussian fashion. Figure 3b shows the percentage of data
detected as a function of the induced error interval Figure 3c the calculated error as a function of the
interval. It can be noticed that the MRM works better than ePCA in the detection of outliers and in the
estimation of missing data. Figure 3c shows couples of box and whiskers plots, which are horizontally
aligned for displaying the correspondence with Figure 3b for the range of induced errors intervals. The
vertical axis in Figure 3c shows the calculated error and how it compares both MRM and ePCA
methods. Again, the MRM calculated errors are less spread than the corresponding ePCA calculated
errors in all the induced interval ranges.

2136

Data Quality Control for Saint Petersburg ﬂood warning system

J. L. Araya-Lopez et al

Figure 3. Characterization of induced and calculated errors: a) Density plot for the induced
errors. b) Comparison of the percentage of outliers detected for any category of induced errors. c) Box
and whiskers plots display the distribution of induced errors for several intervals

The second test for estimation ability of algorithms to restore series of gaps in dataset
assumes sequential running of algorithms for number of datasets with inserted missing values chains
(1, 3, 5, 11, 15). Restoration of missing values was produced for time series in St. Petersburg taking
into account all other stations from BOOS dataset. Time series from all stations except of St.
Petersburg were assumed perfect (from reference dataset). Here we use several metrics whose use is
widespread for the quantification of errors in datasets. The Mean Bias Error ‫ ܧܤܯ‬ൌ
Absolute Error expressed by ‫ ܧܣܯ‬ൌ

σ೙
೔సభȁா೔ ȁ
௡

σ೙
೔సభ ா೔
௡

, the Mean

,the Root Mean Square Deviation ܴ‫ ܦܵܯ‬ൌ ቂ
మ
σ೙
೔సభ ா೔

మ
σ೙
೔సభ ா೔

௡

ଵȀଶ

ቃ

,

(Figure 4). It is
and the MSE which is related to the MSE given by the relationship ‫ ܧܵܯ‬ൌ
௡
clear that taking the square root of MSE yields the RMSD. RMSD has the property amplifying and
punishing large errors. The use of MAE, RMSD and MSE allow establishing the quality of the
estimations of the missing data completion method on single time series (Willmott and Matsuura,
2005).
Figure 4 shows the errors as a function of the gap length. It can be noticed that in all cases
MRM seems to give better results than ePCA. It is also clear that the magnitudes for the errors tend to

2137

Data Quality Control for Saint Petersburg ﬂood warning system

J. L. Araya-Lopez et al

increase with increasing gap lengths. Moreover, we observe that the high values are very penalized for
the case of RMSD (ePCA method) and the MSE to a lesser extent.

Figure 4. Metrics for error calculated for filled data as a function of the induced gap lengths in
datasets: a) Mean Absolute error and Mean Biased Error b) Root Mean Square Deviation and mean
Square Error

5 Conclusions
This paper highlights the problem of data quality control in a frame of early warning system with
emphasis on domain specific quality control that includes detection of outliers in datasets and missing
values restoration. St. Petersburg FWS was considered as an example of such system. Quality control
in St. Petersburg FWS contains blocks of technical control, human mistakes control, statistical control
of simulated fields, statistical control and restoration of measurements and control using alternative
models. Domain specific quality control was presented as two types of procedures based on
theoretically proved methods. The first procedure is based on a probabilistic dynamical system model,
where processes are spatially interrelated and could be implemented in a form of multivariate
regression (MRM). The second procedure is based on principal component analysis extended for
taking into account temporal relations in the data set (ePCA).
In the conditions in which the experiments were performed in this work, preliminary
conclusions suggest that MRM is more accurate than the ePCA method in its current stage of
development. However, it is important to take into account that the ePCA method may be improved to
some extent, this by taking into account peculiarities in the structure of the data, as well as the
introduction of heuristic rules that may make it more effective in practice (such heuristics was already
done for the MRM before becoming operational).
In the experiments run in this study it was confirmed that the gap length plays a key role in the
magnitude of some common error metrics. This should be taken into account when developing an
operational system in which missing information may be transmitted real-time on a regular basis, at
different gap lengths and in more irregular ways than those tested in this study. As part of this work,
we recommend to define ways to come up for possible solutions and characterizations of the
distribution of missing values in an operational context, so that some solutions can be considered for

2138

Data Quality Control for Saint Petersburg ﬂood warning system

J. L. Araya-Lopez et al

dealing with situations that are close to what occurs in real situations. In this sense, we think it is
important to take into account situation of larger gap lengths, distribution of missing values in the data
sets and further considerations about practical limitations of these methods in the context of big data
flows. There are plenty of environmental observation parameters with their peculiarities at an
operational level. They require constant evaluation regarding the effectiveness of these methods as
solution to the data restoration problem. The results of such a research may be useful to practitioners
and IT managers that have to cope with actual environmental data networks and take decisions
regarding the methods and criteria to apply in such contexts.
Acknowledgements. This paper is financially supported by The Russian Scientific Foundation,
Agreement #14-11-00823 (15.07.2014).

6 Bibliography
Assunção M.D., Calheiros R.N., Bianchi S., Netto M.A., Rajkumar Buyya R., 2015: Big Data
computing and clouds: Trends and future directions, Journal of Parallel and Distributed Computing,
Volumes 79–80, pp 3-15.
Beven K., 2002: Towards a coherent philosophy for modeling the environment. Proceedings of the
Royal Society A: Mathematical, Physical and Engineering Sciences, 458 (2026) , pp. 2465-2484.
Boukhanovsky, A.V., Ivanov, S.V.,Urgent computing for operational storm surge forecasting in
Saint-Petersburg (2012) . Procedia Computer Science, 9, pp. 1704-1712.
Cannata, M., Antonovic, M., Molinari, M., Pozzoni, M., 2013: ISTOS, sensor observation
management system: a real case application of hydro-meteorological data for flood protection.
International Archives of the Photogrammetry Remote Sensing and Spatial Information Sciences, Vol.
XL-5/W3.
Cencerrado, A., Cortes, A., Margalef, T., 2012: On the way of applying urgent computing Solutions
to Forest Fire Propagation Predictions.Procedia Computer Science, Volume 9, 2012, pp 1657-1666.
Urgent Computing. "Exploring Supercomputing's New Role". CTWatch Quarterly 4.1 (2008). pp.
60.
Brock F. and Richardson J.S., (2001). Meteorological Measurement Systems. Oxford University
Press, New York, 290 pp.
Fiebrich C.A, Morgan C.R., McCombs A.G., Hall Jr P.K., and McPherson R.A., 2010: Quality
Assurance Procedures for Mesoscale Meteorological Data. J. Atmos. Oceanic Technol., 27, pp 1565–
1582.
de Roo A.P. , Gouweleeuw B. , Thielen J., Bartholmes J., Bongioannini-Cerlini P. , Todini E. ,
Bates P.D. , Horritt M. , Hunter N. , Beven K. , Pappenberger F. , Heise E. , Rivin G., Hils M.
,Hollingsworth A. , Holst B. , Kwadijk J. , Paolo P. , Van Dijk M. , Sattler K. , Sprokkereef E.
Development of a European flood forecasting system. International Journal of River Basin
Management . Vol. 1, Iss. 1, 2003.
Eischeid J.K., Baker C.B., Karl T.R. and Diaz H.F., 1995: The Quality Control of Long-Term
Climatological Data Using Objective Data Analysis. Journal of Applied Meteorology,34, pp 27872795.
Gandin, L. S., 1988: Complex Quality Control of Meteorological Data. Mon. Wea. Rev., 116, pp
1137-1156.
Graybeal D.Y., DeGaetano A.T. y Eggleston K.L., 2004: Improved Quality Assurance for Historial
Hourly Temperature and Humidity: Development and Application to Environmental Analysis. Journal
of Applied Meteorology. 43, pp 1722-1735.

2139

Data Quality Control for Saint Petersburg ﬂood warning system

J. L. Araya-Lopez et al

Gerhard Tutz, Shahla Ramzan. Improved methods for the missing data completion of missing data
by nearest neighbor methods. Computational Statistics & Data Analysis, Volume 90, October 2015, pp
84-99.
Ivanov S.V., Kosukhin S.S., Kaluzhnaya A.V., Boukhanovsky A.V., 2012: Simulation-based
collaborative decision support for surge floods prevention in St. Petersburg, Journal of Computational
Science, Volume 3, Issue 6, pp 450-455.
Kalyuzhnaya, A.V. , Boukhanovsky,A.V., 2015: Computational Uncertainty Management for
Coastal Flood Prevention System, Procedia Computer Science, Volume 51, 2015, pp 2317-2326.
Rob J. Hyndman, Anne B. Koehler, Another look at measures of forecast accuracy, International
Journal of Forecasting, Volume 22, Issue 4, October-December 2006, Pages 679-688.
Jiménez P.A., González-Rouco J.F., Navarro J., Montávez J.P. y García-Bustamante E., (2010).
Quality Assurance of Surface Wind Observations from Automated Weather Stations. Journal of
Atmospheric and Oceanic Technology, 27, pp1101–1122.
Palepu, R. B., & Rao, D. K. S. (2012). Meta-data quality-control architecture in data warehousing.
International Journal of Computer Science, Engineering and Information Technology,pp 15-24.
Singh, R., & Singh, K. (2010). A descriptive classification of causes of data quality problems in
data warehousing. International Journal of Computer Science Issues, 7(3), pp41-50.
Shafer M.A., Friebrich C.A., Arndt D.S., Fredrickson S.E. and Hughes, T.W.,2000: Quality
Assurance Procedures in the Oklahoma Mesonetwork. Journal of Atmospheric and Oceanic
Technology.,Volume 17,pp 474-494.
Meek D. W., and Hatfield J.L., 1994. Data Quality Checking for single station meteorological
databases. Agricultural and Forest Meteorology. 69:1, 1994, pp85-109.
Michalakes J., 2004: The Weather Research and Forecasting Model: software architecture and
performance. in: Proceedings of 11th ECMWF Workshop on the use of High Performance Computing
in Meteorology. Reading, UK.
Upton G. J. G. and Rahimi A. R. ,2003: On-line detection of errors in tipping-bucket raingauges.
Journal of Hidrology. , Volume 278, pp 197-212.
NEMO model (http://www.nemo-ocean.eu/)
Wilks D.S., 2006: Statistical Methods in Atmospheric Sciences. Elsevier Academic Press, Inc., 627
pp.
Willmott, C.J., Matsuura, K., 2005. Advantages of the mean absolute error (MAE) over the root
mean square error (RMSE) in assesising average model performance. Climate Research. 30: pp 79-82.

2140

