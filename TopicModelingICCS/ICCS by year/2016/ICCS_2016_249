Procedia Computer Science
Volume 80, 2016, Pages 398–406
ICCS 2016. The International Conference on Computational
Science

An Exploratory Sentiment and Facial Expressions
Analysis of Data from Photo-sharing on Social Media:
The Case of Football Violence
Vasiliy Boychuk1, Kirill Sukharev1, Daniil Voloshin1
and Vladislav Karbovskii1
1
ITMO University, Saint-Petersburg, Russia
pphator@gmail.com, kvsuharev@gmail.com,
achoched@gmail.com, vladislav.k.work@gmail.com

Abstract
In this article we propose the possibility to increase the level of security during football matches due to
analysis of data that are placed on the social networks of these events visitors. We considered different
ways to recognize emotions from photographs to trace the changes in the level of emotions in the photos
depending on whether these pictures were taken during the game with fights in the stands or during
normal games. We tested this assumption and our hypothesis has been partially confirmed. The software
solution for emotion recognition from Microsoft Oxford showed that the level of emotional anger is
noticeably higher in the photographs taken during the match with fights. In addition, other curious results
were obtained, including an analysis of the key of the comments left by events visitors’ photos.
Keywords: sentiment analysis, facial expression recognition, social media, football violence, data analysis

1 Introduction
According to research of Statistic Brain Research Institute, football is the most popular sport in the
world. About 3.5 billion people follow this type of competition[1]. Each week football matches are held
at various levels around the world. The average attendance at football games is relatively high compared
to other sporting events – for example, around 40 thousand spectators came to every Bundesliga game
in Germany in 2015 [2]. In the United States, during the same period, the audience at football matches
exceeded 20 thousand visitors [3]. In terms of match attendance, Russia has one of the lowest numbers
in Europe, but the number of spectators at the events of the Russian Major League exceeded 10 thousand
people [2].
Football culture and violence have been inseparable since the introduction of professional football
in England in the late 19th century [4]. Constant skirmishes between fans occur both outside the stadium

398

Selection and peer-review under responsibility of the Scientiﬁc Programme Committee of ICCS 2016
c The Authors. Published by Elsevier B.V.

doi:10.1016/j.procs.2016.05.340

Facial Expressions Analysis: Football Violence Case

Boychuk, Sukharev, Voloshin and Karbovskii

and in the stands. Violent clashes during a match only contribute to the increasing tensions between
rival fans. In the entire history of football in the world there have been many notable incidents are further
the most important ones described. On November 6, 1955 during the match “Napoli” - “Bologna” in
Naples, there was unrest in the stands, injuring 152 people [5]. October 31, 1976 in Cameroon was
marked by fights among the fans during a match of the local national team against Congo, which resulted
in the deaths of two people [6].
Similar events continue to occur today. The biggest tragedy in recent years took place in Egypt. The
match between the “Al-Masri” and “Al-Ahly” clubs, in which the hosts triumphed with a score of 3:1,
ended in a massive attack on a group of spectators supporting the visiting team [7].
To avoid such cases, the authorities develop special regulations, imposing more control over the fans
or increasing the number of police officers in the stands. However, the quantity of law enforcers present
during the event has very little to do with the accurate prediction of potential confrontations between
the fans or the infliction of material damage, since the intentions of football fans to start a clash are
hidden from police.
This study has been carried out in an attempt to pursue the ultimate goal – assist the anticipation and
timely prevention of future fights in stadium stands. Moreover, this is closely related to the market of
mobile devices and social networks.
Today there are around 2.6 billion smart phones in use worldwide [8]. Analysts predict that by 2020
the number of devices will grow up to 6.1 billion [8]. Along with the increasing popularity and
availability of smartphones, the social media market continues to evolve, too. As of August 2015, there
were more than 2.2 billion active social network users in the world [9]. Today, social networks are not
just a platform for interaction, communication, and cooperation. Services like Facebook, Twitter,
Instagram also facilitate the collective participatory formation of an aggregate reflective view of the
most important events of the present as well as an outlook to the past and future. Growth of the market
of mobile devices and social networks is motivating the use of these funds for the solution of
contemporary problems of society.
These facts lead us to assume that conclusions about the prevailing mood as a proxy for violent
intentions during a soccer match can be drawn from the assessment of the “tone” of the facial
expressions found on the photos taken by spectators and comments related to them. In this research, we
gave preference to Instagram as the source of both textual and visual data. This social network has 400
million monthly active users [10], is developing dynamically (its growth doubled in 2015), is mainly
accessed from smart phones, stores user geo-location data linked to entries and is widely distributed in
Europe, North and South America [11]. Instagram is a convenient source of data – it provides researchers
not just visual data (photos and short videos), but text (comments to posts). And last but not least,
Instagram provides developers with a convenient API to retrieve data from servers.

2 Related Works
Sentiment analysis is a part of appraisal theory, which is focused on automatically identifying text
topics or semantic which influence on real life action. Many studies focus on the semantic analyses to
understand “positive”, “negative” or “neutral” sentiment polarity and ones weights [12]. Sentiment
analysis in online communication between people successfully predicts product consumption
preferences and the trust among friends. The experiment results show that informal and short posts can
predict preferences without any information history about preferences before [13].
Methods of emotion recognition are divided into three categories, based on the methods of face
recognition: template-based methods (associated with holistic approaches to face recognition), featurebased methods (for the analytic-following approach to recognition of faces), and hybrid ones [17].
The Active Appearance Model (AAM), Labeled Graph Method (LGM), Point Distribution Model
(PDM), Random Block Eigenvectors (RBE) belong to the first category, while Facial Characteristics

399

Facial Expressions Analysis: Football Violence Case

Boychuk, Sukharev, Voloshin and Karbovskii

Point Method, Dual-View Point-Based Method belong to the second. Fiducial Grid & Gabor Wavelets,
Potential Net belong to the third [17].
AAM was first used by Edwards, Cootes, and Taylor in the context of the analysis of individuals in
the third International Conference of Pattern Recognition and gestures. The algorithm is based on
optimizing the differences between the current image and the original sample. This method allows to
achieve an accuracy of 88% [18].
Random Block Eigenvector Method allows one to get rid of redundant information in the study of
facial features. The system selects only the necessary parts of the projection, which allows for an 86%
algorithm efficiency[19].
Dual-View Point-Based Method uses a combination of frontal and side view of the same person, to
achieve a successful outcome in 86% [20].
Fiducial Grid & Gabor Wavelets combination of methods possible to combine the advantages of two
different approaches to get rid of the sensitivity of the method of Gabor Wavelets and get a successful
result in 82% [21].
In this section, we describe the currently known methods of recognition of emotions from
photographs. These methods are widely used in commercial products with various degrees of success.
Since these methods demonstrate a similar level of effectiveness in performing standard tasks, we relied
on the comparison of the existing commercial solutions (presented in Section 3 of the paper) when
choosing specific technologies to be applied to the study.

3 Study
3.1 Dataset
The choice of cases to be considered in this research has been limited to those events that occurred
in 2015 primarily due to the two-fold increase in Instagram users that year [10]. Based on the information
obtained from news websites, a total of eight matches, characterized by the incidence of fights in the
stands, have been chosen. In contrast, we have selected home games of the same teams, which took
place in the 2015 and during which there were no fights in the stands. It is worth noting that though a
lot more than eight games with fights in the stands have been found, the majority of them have been
excluded from the investigation as lacking sufficient number of relevant photos uploaded to Instagram.
Moreover, such factors as the features of the stadiums, results of the games, and the history of prior
confrontation between the fans of different teams has not been taken into consideration in this study.
For
each
case
under
investigation, a set of pictures
containing geo-location marks and
uploaded to Instagram in the course
of the game was extracted. With the
help of Google Maps service [22],
the coordinates of the stadium were
acquired and transferred as input data
for the media data search function of
Instagram API. It was also applied to
the start time of the match, and the
time interval for searching photos
was equal to five hours. These photos
have been transferred to the input for
Figure 1: Instagram posts time distribution
each of the services we have chosen
to recognize emotions.

400

Facial Expressions Analysis: Football Violence Case

Boychuk, Sukharev, Voloshin and Karbovskii

Each of the used services returns the results of the processing of each of the detected faces in a photo.
Instagram photo comments were used as initial data of the discourse analysis of social unrest prediction.
Those photos were the same as in the photo analyses. For Instagram comments, collecting API requests
with the photo id was applied. As output, 1248 text data with photo “tags” and “comments” were
collected.

3.2 Text Sentiment Analysis
In this study, semantic analysis of photo comments was applied with the hypothesis that the higher
level of “negative” posts correlate with the social unrest in football matches. Firstly, all texts for creating
the initial form were lemmatized with the help of open software “mystem.exe” from “Yandex”[21]. Due
to its nature, the textual content (predominantly user-generated) that is found on social network sites has
many specific words and figures of speech, which complicate the process of the model training. [22]
For reducing error in training comments sentiment, Support Vector Machines (SVM) models were
applied on the Russian social network sites dictionary.

3.3 Face Emotion Analysis
Currently technology recognition cease to be unattainable, and they are increasingly found in the
implementation of commercial products.
FaceReader by Noldus Information Technology [23] is among the solutions, that are based on
template-based methods.. Development of Dutch scientists can recognize "happy," "sad," "angry,"
"surprised", "afraid", "dissatisfied" and "neutral" facial expressions. FaceReader is also able to identify
age, gender, and ethnicity from a face in a photo. The average percentage of emotion recognition is 89%
[23]. In addition, the market is represented by the development of the company Visual Recognition GladOrSad [24]. Emotion recognition system creates
3D-model of the person with the identification of 12
key areas, such as the corners of his eyes and the
corners of the mouth. The program is able to allocate
six emotions: anger, sadness, fear, surprise, disgust and
happiness, as well as the seventh - their mix [24].
There is also Luxand Face Recognition system [25].
The software product of the American company
Luxand can automatically determine the sex of the
subject based on the image or video. Developers claim
success of identification as 93% in the image
processing, and 97% - in the video. In addition, the
program is capable of measuring "happiness level".
[25]. And the newest one is the Microsoft Oxford
project (Figure 2) [26]. Introduced in November 2015,
Figure 2: Example of Facial emotion
technology, relying on machine learning algorithms,
recognition by Microsoft Emotion API
can recognize eight emotions (anger, contempt,
disgust, fear, happiness, neutral, sadness, surprise). Developers allowed free use of the the API of the
service, which made it widespread [26].
The next group of services is feature-based methods. SkyBiometry system is able to determine the
level of presence in the top six photos of emotions and a neutral mood [27]. Affective Computing
Research Group System [28] is based on Gabor Wavelets and is able to assess the six emotions (anger,
sadness, fear, surprise, disgust and happiness) [28] Face++ is designed primarily for identification and
verification of persons. In addition, Face ++ is able to assess the gender, age and level of human joy.

401

Facial Expressions Analysis: Football Violence Case

Boychuk, Sukharev, Voloshin and Karbovskii

Developers marked the success level of the application on 87%. The system is based on the structure of
deep neural network [29] And an application by Sightcorp – InSight - can track the tiniest movements
of the facial muscles on the face of the user and recognize six emotions. In addition, the system can
uniquely track eye movement in order to understand the laws of the user's attention and visual stimuli
that captures their interest. The developers say that the accuracy of the application is 93.2% [30].
Based on our research, we have chosen to operate two services – Microsoft Oxford Project and
InSight by SightCorp. Our selection is based on usability services through web-API, the limited
possibilities of free use, a high level of recognition of the claimed emotions and appropriate for us to
limit requests to the server.
(a)

(b)

Figure 3: (a) Comments word clouds in matches with fight (EN/RU);
(b) Comments word clouds in matches without fight (EN/RU)

4 Results
Two word clouds (Figure 3) were created for visual data representation. These word clouds show
the most frequently used words (the size and color depends on the word frequency). There are more
offensive language words, threats and people associate the fans in games with fights (as seen in Figure
3a). Moreover, in those events where fights did not occur, there were less negative words and more
words with high positive scores such as love, family etc. (in Figure 3b).
We have analyzed the collected photos with two services and receive results displayed in Table 2
and Table 3.
Service
MS
SightCorp

Anger
/100

Contempt
/100

Disgust
/100

Fear
/100

Happiness
/100

Neutral
/100

Sadness
/100

Surprise
/100

0.59

1.08

0.23

0.15

67.06

28.09

0.92

1.87

11.17

-

11.55

6.86

36.59

-

13.91

3.74

Table 1: Emotions scores for normal matches

Service
MS
SightCorp

Anger Contempt
/100
/100

Disgust
/100

Fear
/100

Surprise
/100

2.89

1.19

0.63

0.19

64.02

28.52

0.95

1.60

10.45

-

12.13

7.84

35.28

-

13.99

4.53

Table 2: Emotions scores for fight matches

402

Happiness Neutral/ Sadness
/100
100
/100

Facial Expressions Analysis: Football Violence Case

Boychuk, Sukharev, Voloshin and Karbovskii

Table 2 includes the results of the processing of photos collected during the matches held in normal
conditions, with no fights. Table 3 contains the results of the data collected during the game with fights
in the stands.
After examining the output of the two emotion recognition services, we found that all of them have
recorded the prevalence of positive or neutral emotions over negative during all considered matches. In
addition, SightCorp system didn’t detect elevated levels of any emotion during games with fights.
However, the exploring results of Microsoft Oxford Project showed that the average of anger emotions
in photos taken during matches with fights, more than in the normal ones in 2.4 times. The distributions
of anger is shown if Fig. 4. Except the peaks at zero, the left distribution has peak at the high level of
emotion scores.

Figure 4: Distribution of anger (by MS Oxford)

We suppose that these results can be interpreted as a beginning of a confirmation of the hypothesis
that the level of negative emotions in the photos during matches with episodes of violence in the stands
will be higher than in the photos taken during normal games.

5 Match Classifier Development
After exploring the results of the emotion
analysis, we began to develop a classifier for further
construction of the system of detection of fights in
the stands during the match. For example in the
Figure 5 it can be seen that mean level of anger of
fight matches is noticeably higher than in normal
matches.
For each match we had the following
information: data from Instagram (the number of
photos, videos, likes, comments, the Instagram users
who tagged in the photo), the two sets of emotions
values (by MS Oxford Project and SightCorp), and
information about filled the stands during the match.
Features for classification were selected by function
“SelectKBest” from Python library scikit-learn,
which removes all but the k highest scoring features.

Figure 5: Emotions radar-plot (by MS Oxford)

403

Facial Expressions Analysis: Football Violence Case

Boychuk, Sukharev, Voloshin and Karbovskii

As classifiers were used Naive Bayes classifier, K-nearest neighbors classifier, Decision tree
classifier and Extra-tree classifier. The classifiers results verification was done with cross-validation,
which was implemented in scikit-learn library. In addition, permutation test was used for testing
classifiers. Series of experiments were performed for the variate of the number of using features
(Table 4).
Features
#
3
4
5
6
7
8
9
10

Naive Bayes
Perm
85.5 76.0
81.5 75.6
77.6 77.8
77.7 76.3
77.4 71.3
79.8 70.5
78.5 69.6
79.0 70.6

KNN(k=5)
77.4
77.2
79.5
79.6
79.9
75.7
75.8
79.9

Perm
80.4
79.6
79.4
79.4
79.5
79.5
79.6
79.7

Decision tree
Perm
86.0
74.4
85.5
80.1
84.6
78.7
83.7
74.9
82.7
75.3
81.6
74.5
81.3
73.2
80.8
73.9

Extra-tree
88.9
87.7
84.1
83.0
82.8
82.9
83.3
82.4

Perm
74.6
76.8
77.1
78.0
78.9
78.0
78.0
77.6

Table 4: Result of classifiers tests

In addition, after extended feature selection we have chosen the five features: ratio of the number of
photos with level of “anger” higher than 70 (by MS Oxford Project evaluation), average of likes, max
“surprise” level (by SightCorp evaluation), median “fear” level (by SightCorp evaluation), ratio of the
number of photos with level of “fear” higher than 50 (by MS Oxford Project evaluation), see Table 5.

accuracy
p-value

Naive Bayes
Perm.
83.3 48.3
0.0232

KNN
70.5
0.9767

Perm.
79.5

Decision tree
Perm.
83.6 77.2
0.0930

Extra-tree
Perm.
82.5 75.3
0.0465

Table 5: Result for classification with specific features

The p-value is then given by the percentage of runs for which the score obtained is greater than the
classification score obtained in the first place.
We got a result, which based
on analysis of only five
features – four different
emotions level and likes
value from Instagram data –
by Naive Bayes Classifier.
We did not use sentiment
analysis of text from
comments. The dependence
between
accuracy
of
classifier and time is shown in
Fig 6. In addition to test the
classifier,
we
used
Figure 6: Dependency between accuracy and time from the beginning permutation
tests.
Tags
of a match
“fight” for the matches set
were shuffled before running
the classifier randomly. During this test, the accuracy was 48.3. This result is not good enough to confirm

404

Facial Expressions Analysis: Football Violence Case

Boychuk, Sukharev, Voloshin and Karbovskii

our hypothesis. We hope the further development of methods and increase of data allow developing this
area.

6 Discussion & Conclusion
Our study was the beginning on the way of the confirmation hypothesis about the dependence of the
level of emotion in photos, which were taken during the match, the availability of forms of violence in
the stands during the match.
To fully confirm this hypothesis it is necessary to consider the set accompanying factors such as the
history of the opposition contenders, the gender composition of the spectators in the stands, the
availability of alcohol, the scoring, and the general mood of society.
However, the confirmation of the hypothesis opens the possibility of implementing a system of
monitoring the state of the fans in real time for prevent episodes of violence in the stands. In addition,
such system can be used not just during sporting events, but also during concerts, rallies and other public
events. In our study we could not get direct confirmation of our hypothesis. We believe that to
successfully perform this task, it is necessary to have more data, diverse data sources, and take into
account more factors that affect the final outcome.
As one of the measures to improve the accuracy of predictions, we see the possibility of video
processing, which are made by visitors during matches or obtained from other sources. This kind of
data is different from images and it needs different methods of processing. To accomplish this, the
conduction of additional study of existing methods and software solutions in the field of dynamic
emotion analysis should be done.
Acknowledgments. This paper is financially supported by Ministry of Education and Science of the
Russian Federation, Agreement #14.584.21.0015 (11.11.2015), project id RFMEFI58415X0015. This
work was partly performed by the Master students (Vasiliy Boychuk) of the Master’s Programme in
Computational Science [31].

7 References
[1]
[2]
[3]

[4]
[5]
[6]
[7]
[8]

“Most
Popular
Sports
Worldwide,”
2015.
[Online].
Available:
http://www.statisticbrain.com/most-popular-sports-worldwide/.
“EUROPEAN ATTENDANCES,” 2015. [Online]. Available: http://www.european-footballstatistics.co.uk/attn.htm.
“Average per game attendance of the five major sports leagues in North America 2014/15,”
2015. [Online]. Available: http://www.statista.com/statistics/207458/per-game-attendance-ofmajor-us-sports-leagues/.
T. Madensen and J. Eck, “Spectator Violence in Stadiums,” 2008. [Online]. Available:
http://www.popcenter.org/problems/spectator_violence/print/.
“Football violence in history.” [Online]. Available: http://www.sirc.org/publik/fvhist.html.
“Soccer-related Incidents,” 1996. [Online]. Available: http://articles.latimes.com/1996-1018/sports/sp-55148_1_soccer-fans.
“Egypt football violence leaves many dead in Port Said,” 2012.
I. Lunden, “6.1B Smartphone Users Globally By 2020, Overtaking Basic Fixed Phone
Subscriptions,” 2015. [Online]. Available: techcrunch.com/2015/06/02/6-1b-smartphone-users-

405

Facial Expressions Analysis: Football Violence Case

[9]
[10]
[11]
[12]
[13]
[14]
[15]

[16]

[17]
[18]
[19]
[20]
[21]
[22]
[23]
[24]
[25]
[26]
[27]
[28]
[29]
[30]
[31]

406

Boychuk, Sukharev, Voloshin and Karbovskii

globally-by-2020-overtaking-basic-fixed-phone-subscriptions/.
“Social Networks - Statistics & Facts | Statista.” [Online]. Available:
http://www.statista.com/topics/1164/social-networks/. [Accessed: 28-Jan-2016].
“Celebrating
a
Community
of
400
Million.”
[Online].
Available:
http://blog.instagram.com/post/129662501137/150922-400million.
J. Egan, “14 eye-opening Instagram statistics,” 2015. [Online]. Available:
http://www.prdaily.com/Main/Articles/14_eyeopening_Instagram_statistics_18058.aspx.
C. S.-G. Khoo, A. Nourbakhsh, and N. Jin-Cheon, “Sentiment analysis of online news text: a
case study of appraisal theory,” Online Inf. Rev., vol. 36, no. 6, pp. 858–878, 2012.
D. H. Alahmadi and X.-J. Zeng, “ISTS: Implicit social trust and sentiment based approach to
recommender systems,” Expert Syst. Appl., vol. 42, no. 22, pp. 8840–8849, 2015.
C. Musto, G. Semeraro, P. Lops, and M. De Gemmis, “CrowdPulse: A framework for real-time
semantic analysis of social streams,” Inf. Syst., vol. 54, pp. 127–146, 2015.
P. Burnap, O. Rana, M. Williams, A. Edwars, J. Morgan, L. Sloan, and J. Conejero, “COSMOS:
Towards an integrated and scalable service for analysing social media on demand,” Int. J.
Parallel, Emergent Distrib. Syst., vol. 30, no. 2, pp. 80–100, 2014.
R. Moraes, J. F. Valiati, and W. P. Gavião Neto, “Document-level sentiment classification: An
empirical comparison between SVM and ANN,” Expert Systems with Applications, vol. 40, no.
2. pp. 621–633, 2013.
M. Pantic and L. J. M. Rothkrantz, “Automatic analysis of facial expressions: The state of the
art,” IEEE Trans. Pattern Anal. Mach. Intell., vol. 22, no. 12, pp. 1424–1445, 2000.
G. J. Edwards, T. F. Cootes, and C. J. Taylor, “Face recognition using active appearance
models,” Comput. Vision—ECCV’98, pp. 581–595, 1998.
C. Padgett and G. W. Cottrell, “Representing face images for emotion classification,” Adv.
Neural Inf. Process. Syst., pp. 894–900, 1997.
M. Pantic and L. Rothkrantz, “Facial action detection from dual-view static face images,” IEEE
Int. Conf. Fuzzy Syst., vol. 1, pp. 39–44, 2004.
M. J. Lyons, “Automatic classification of single facial images,” IEEE Trans. Pattern Anal.
Mach. Intell., vol. 21, no. 12, pp. 1357–1362, 1999.
“Google Maps.” [Online]. Available: https://www.google.ru/maps.
“FaceReader.”
[Online].
Available:
http://www.noldus.com/human-behaviorresearch/products/facereader.
“Visual Recognition.” [Online]. Available: http://www.visual-recognition.nl/.
“Luxand.” [Online]. Available: https://www.luxand.com/.
“Project Oxford.” [Online]. Available: https://www.projectoxford.ai/.
“SkyBiometry.” [Online]. Available: https://www.skybiometry.com/.
“Affeciva.” [Online]. Available: http://www.affectiva.com/.
“Face++.” [Online]. Available: http://www.faceplusplus.com/.
“SightCorp.” [Online]. Available: http://sightcorp.com/.
V. V. Krzhizhanovskaya, A. V. Dukhanov, A. Bilyatdinova, A. V. Boukhanovsky, and P. M.
A. Sloot, “Russian-Dutch double-degree Master’s programme in computational science in the
age of global education,” J. Comput. Sci., vol. 10, pp. 288–298, 2015.

