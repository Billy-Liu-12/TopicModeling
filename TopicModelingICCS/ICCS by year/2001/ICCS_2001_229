Load Balancing for the Electronic Structure
Program GREMLIN in a Very Heterogenous
SSH-Connected WAN-Cluster of UNIX-Type
Hosts
Siegfried Höﬁnger1
Dipartimento di Chimica ”G. Ciamician”,
Universita’ degli Studi di Bologna,
Via F. Selmi 2,
I-40126, Bologna, Italy
sh@ciam.unibo.it
http://www.ciam.unibo.it

Abstract. Five far distant machines located at some French, Austrian
and Italian research institutions are connected to a WAN-cluster via
PVM 3.4.3. The secure shell protocol is used for connection and communication purposes between the diﬀerent hosts. Operating-Systems, architectures and cpu-performances of all the 5 machines vary from LINUX2.2.14/INTEL PPro-200MHz, over LINUX-2.2.13/INTEL PII-350MHz,
OSF I V5.0/ALPHA EV6-500MHz, IRIX64 6.5/MIPS R10000-195MHz,
up to IRIX64 6.5/MIPS R12000-300MHz. An initial benchmark run
with the Hartree Fock program GREMLIN reveals a speed diﬀerence of
roughly a factor 7x between the slowest and the fastest running machine.
Taking into account these various speed data within a special dedicated
load balancing tool in an initial execution stage of GREMLIN, may lead
to a rather well balanced parallel performance and good scaling characteristics for this program if run in such a kind of heterogenous Wide
Area Network cluster.

1

Introduction

Computer Science and Industry has made great progress in recent years and as
a result of this, the average desktop personal computer as of today has become
superior in many aspects to his supercomputer analogues. The other most rapid
emerging ﬁeld has been the internet and internet based technology, and therefore todays probably most potential computing resources might be lying in these
huge number of ordinary internet computers, that are accessible in principal to
everyone else on the net, but mainly remain idle and serve for minor computational tasks. Scientiﬁc research in many areas however suﬀers from limited
access to computational resources and therefore great attention should be payed
to development eﬀorts especially focusing on parallel and distributed computing
strategies and all the problems connected to them.
V.N. Alexandrov et al. (Eds.): ICCS 2001, LNCS 2074, pp. 801–810, 2001.
c Springer-Verlag Berlin Heidelberg 2001
�

802

S. Höﬁnger

One such example for a really demanding scientiﬁc discipline is ab initio
quantum chemistry, or electronic structure theory, which currently is about to
enter the ﬁeld of mainly application oriented sciences and bio-sciences as well,
and thus experiences a never foreseen popularity, which all in all may be due to
awarding the Nobel Price in Chemistry to J.A. Pople and W. Kohn in 1998.
In a previous article [1] we introduced one such quantum chemical program,
which shall from hereafter be called GREMLIN, that solves the time independent Schrödinger equation [2] according to the Hartree Fock Method [3] [4].
One of the main features of this program had been the capability to execute the
most expensive part in it in parallel mode on distributed cluster architectures as
well as on shared memory multiprocessor machines [5]. In addition, what makes
this application particularly attractive for a distributed computing solution, is
its modest fraction in communication time, which on the other hand implies a
principal possible extension to a Wide Area Network (WAN) cluster, where the
individual ”working” nodes are usually formed form a number of UNIX-type machines1 of usually hetereogenous architecture and the connection between them
is simply realized from the ordinary low-bandwidth/high-latency internet.
Following previous results [1], a properly balanced distribution of the global
computational work requires some basic interference with the theoretical concept
of recursive ERI (Electron Repulsion Integrals) computation [6]. However, taking
into account a system inherent, partial inseparability of the net amount of computational work, allows an estimation and decomposition into fairly equal sized
fractions of node work, and from this adequate node speciﬁc pair lists may be
built. The present article intends to describe, how one may extend this concept
to an additional consideration of diﬀerent node performance, since the previous
study was based on multiprocessor machines made of equally fast performing
CPUs.
1.1

Computational Challenge

Here we brieﬂy want to recall, what makes ab-initio electronic structure calculation a real computational challenge. The main problem lies in the evaluation
of ERIs, the Electron Repulsion Integrals, which are 6-dimensional, 4-center integrals over the basis functions ϕ.
� �
ϕi (r1 )ϕj (r1 )

ERI =
r1 r 2

1
ϕk (r2 )ϕl (r2 )dr1 dr2
|r2 − r1 |

(1)

and the basis functions ϕi are expanded in a series over Primitive Gaussians
χj
ϕi (r) =

�

di,j χj (r) ,

j
1

although PVM 3.4.3 would support WIN32 like OSs as well

(2)

Load Balancing for the Electronic Structure Program GREMLIN

803

which typically are Cartesian Gaussian Functions located at some place
(Ax , Ay , Az ) in space2 [7] [8].
χj (r) = Nj (x − Ax )l (y − Ay )m (z − Az )n e−αj (r−A)

2

(3)

Although somewhat reduced from numerical screening, the principal number
of ERIs to be considered grows with the 4th power of the number of basis functions, which themselve is proportional to the number of atoms in the molecule.
However, since the quality of the employed basis set must be kept high in order to
enable quantitative reasoning, the according number of ERIs very soon exceeds
conventional RAM and diskspace limits and thus becomes the only limiting factor at all. For example, a simple, small molecule like the amino acid alanine (13
atoms), that has been used as a test molecule throughout this present study, at
a basis set description of aug-cc-pVDZ quality [9] [10] (213 basis functions of S,
P and D type) leads to a theoretical number of approximately 260 x 106 ERIs,
which requires about 2.1 GigaByte of either permanent or temporary memory
and goes far beyond usual available computational resources.
Fortunately there is partially independence in the mathematical action of
these many ERIs and one may solve the problem in a so called ”Direct”
way, which means, that a certain logical block of related ERIs is ﬁrst calculated recursively3 , then the action of these block on all the corresponding Fock-matrix elements – from which there luckily are only a number of
(number of basisf unctions)2 – is considered, and then the procedure is repeated and a new block of ERIs overwrites the old one and thus only a small
amount of working memory is permanently involved. Further complifying is the
fact, that one has to respect a hierarchic structure in spawning the space to
the ﬁnal primitive cartesian gaussian functions χj , where, following the notation
introduced in [1], a certain center i refers to an according block of contracted
shells → (j)...(k), from which each of them maps onto corresponding intervals of
basis functions l...m and the later are expanded from primitive cartesian gaussian functions χj as seen from (2). Therefore, after deﬁning a particular centre
quartette i1 i2 i3 i4 , all the implicit dependencies down to the primitive
cartesian gaussians χj must be regarded and as a consequence rather granular
blocks of integrals must be solved all at once, which becomes the major problem
when partitioning the global amount of integrals into equally sized portions.
1.2

Speed Weighted Load Balancing

Concerning parallelization, we follow a common downstream approach and deﬁne node speciﬁc pair lists, that assign a certain subgroup of centre quartettes
2

3

An S-type basis function will consist of primitive gaussians with l = m = n = 0, a
P-type however of primitives with l + m + n = 1, which may be solved at 3 diﬀerent
ways, either l = 1 and m = n = 0, or m = 1 and l = n = 0, or n = 1 and l = m = 0.
D-type speciﬁcation will likewise be l + m+ n = 2 and similarly F-type l +m +n = 3.
All complicated ERI-types (l +m+n > 0) may be deduced from the easier computed
(Si , Sj |Sk , Sl ) type.

804

S. Höﬁnger

to each of the individual nodes, which then shall work independently on their
corresponding partial amount of global computational work. Ideally these pair
lists are built in a way, such that each of the nodes needs the same time for
executing its according fraction of net work. Suppose the total number of theoretical centre quartettes is represented from the area of a rectangle, like shown
in Table 1, and one wants to distribute these many centre quartettes now onto
a number of parallel executing nodes, then the simplest method would certainly
be an arithmetic mean scheme (left picture in Table 1), where the theoretic
number of centre quartettes is devided by the number of nodes and all of them
get exactly this arithmetic mean fraction to work on. Due to the fact that several centres may now substantially diﬀer in the number of deducable contracted
shells → basis functions → primitive gaussians, this simple procedure has been
shown to not be applicable for the case of distributing global computational
work for recursive ERI calculation done in parallel [1]. In fact, instead, one had
to take into account all these hierarchic dependencies down to the level of primitive gaussians χj in order to be able to estimate the real fraction one particular
centre quartette actually had of the global amount of work measured in terms of
theoretic establishable quartettes of primitive gaussians now. However, following
this pathway led to considerable improvements in parallel performance and the
according pair lists of center quartettes may then be symbolized like shown in
the medium picture of Table 1. Note, that up to now, the indicated, individual
4 nodes are still considered to all operate at the same CPU speed and despite
the fact, that the actual number of centre quartettes each node has to process
has become apparently diﬀerent now, the execution time for each of the nodes
is now much more comparable – if not equal – to each other, which stands in
great contrast to the arithmetic mean picture.
Going one step further and assuming diﬀerent node performance next, would
change the situation again. For example, let us hypothetically think of a parallel
machine, where node II is twice as fast as node I, and node III and IV are
running three times and four times as fast as I respectively. Then, we could
equally well think of a parallel machine made up of 10 nodes of the speed of
type I, divide the global amount of work (measured again at the innermost level
of potential primitive gaussian quartettes) into 10 equal sized fractions, and let
the fastest node (IV) work on 4 portions of that estimated unit metric, while
3
2
and 10
of the global work and node I will just deal with
node III and II get 10
1
the remaining 10 of the global amount. The schematic representation of such a
kind of partitioning is given in the right picture of Table 1. On the other hand,
4
one could obtain a theoretical speed up factor of 2.5 (= 10
4 ) for such a case , if
at ﬁrst instance communication time is said to be extremly small and bare serial
execution intervals are neglected completely.

4

compared to the situation where 4 equally fast performing CPUs operate on already
load balanced pair lists

Load Balancing for the Electronic Structure Program GREMLIN

805

Table 1. Comparision between diﬀerent partitioning schemes of the outermost loop
over centre quartettes, represented from the partial areas of the 4 rectangles, that stand
for node speciﬁc fractions of the global amount of theoretical combinations of centre
quartettes. For the Speed Weighted Load Balancing, node II is assumed to be twice as
fast as I, and nodes III and IV, are said to be three times and four times as fast as I.
Symbolization of
Arithmetic Mean
Partitioning of
Centre Quartettes
With Respect to
Equally Fast
Performing
Nodes

Symbolization of
Load Balanced
Partitioning of
Centre Quartettes
With Respect to
Equally Fast
Performing
Nodes

I❧

Symbolization of
Speed Weighted
Load Balanced
Partitioning of
Centre Quartettes
With Respect to
Diﬀerently Fast
Performing Nodes
I❧

I❧

❧
II
❧
II
❧
III

❧
II

⇒

⇒

❧
III
❧
III
❧
IV
❧
IV

❧
IV

806

1.3

S. Höﬁnger

Computational Implementation of Speed Weighted Load
Balancing

As already explained within Sect. 1.2, we want to distribute the theoretical number of centre quartettes onto a number of nodes of diﬀerent CPU performance.
For this purpose we implement the following steps:
1. Determine the number of participating hosts (NMB) and their according relative speed factors (SPEED[I]). The speed factor is the relative performance
of a particular node related to the slowest CPU. So it will either become 1.0
(weakest node), or greater than 1.0 for faster CPUs.
2. Estimate the net amount of computational work (GLOBAL WORK) at the
level of quartettes of primitive gaussians to be considered.
3. Form a unit portion (PORTN) of the dimension of
GLOBAL WORK
PORTN = �
NMB
I=1 SPEED[I]

(4)

4. Loop again over all quartettes of centres and the related contracted shells
and basis functions and primitive gaussians either, as if you were calculating
GLOBAL WORK, and successively ﬁll the upcoming pair lists for centre
quartettes until in the work estimation variable (usually a simple counter,
incremented for each new quartette of primitive gaussians) becomes of the
size of PORTN*SPEED[I]; then leave the current pair list writing for node I
and switch forward to the next node and start with setting up pair lists for
this one.

2

Computational Set-Up

In this section we just want to focus on the practical aspects of setting up a
Wide Area Network cluster and running GREMLIN thereon in the described
speed-weighted, load-balanced way.
2.1

WAN Cluster Description

Five university sites at GUP LINZ (A) (2 nodes), RIST++ SALZBURG (A)
(1 node), ICPS STRASBOURG (F) (1 node) and G. Ciamician BOLOGNA
(I) (1 node) were connected via the Parallel Virtual Machine (PVM rel.3.4.3)
package [11]. One of the remarkable nice features of this release is, that the
communication between diﬀerent, interconnected hosts may be realized with the
secure shell protocol, that implements RSA authentication with 1024 bit long
public/private keys. According to what has been said above, at ﬁrst one needed
to get an overview of the diﬀerent node perfomance of all the individual hosts
involved in the cluster. Therefore an initial benchmark run with the PVM version
of GREMLIN (1 node at each location seperately) on a very small training

Load Balancing for the Electronic Structure Program GREMLIN

807

Table 2. Conditioning and description of the individual host performance in the WAN
cluster. The data is due to a Hartree/Fock DSCF calculation on glycine/631g with the
program GREMLIN. (Result: -271.1538 Hartree in 22 iterations)
Physical
Location
node I
G.C. BOLOGNA
Italy

Architecture/
Clock Speed/
RAM/2L-Cache

Operating real usr sys Rel.
System
[s]
[s] [s] Speed

INTEL Dual PPro 200 MHz LINUX
256 MB/512 KB
2.2.14

2431 159

0 1.000

node II
ICPS STRASBOURG MIPS R10000 200 MHz
France
20 GB/4 MB

IRIX 64
6.5

1186

9

2 1.934

node III
GUP LINZ
Austria

INTEL PII 350 MHz
128 MB/512 KB

LINUX
2.2.13

1167 60

1 2.054

node IV
GUP LINZ
Austria

MIPS R12000 300 MHz
20 GB/8 MB

IRIX 64
6.5

767

6

1 2.990

node V
RIST SALZBURG
Austria

ALPHA EV6 21264 500 MHz OSF I
512 MB/4 MB
V 5.0

341

6

1 6.823

system (glycine/631g, 10 centre, 55 basis functions) was performed, which led
to the data shown in Table 2.
The timings were obtained with the simple UNIX-style time a.out command. According to the fact, that the PVM version of GREMLIN consists of
a master-code and a node-code part, and since the node-code part got a different executable name, the mentioned time-command could easily distinguish
between the parallel and the serial (diagonalization and pre-ERI work) fractions
of the program execution. Thus to focus on the sections that really were running
in parallel, one simply had to substract the usr+sys timings from the real one
and could straightforwardly obtain the relative speed factors shown in Table 3.
Note, that node I and III were lacking from special tuned LAPACK libraries,
so their usr timings became signiﬁcantly higher.
2.2

Estimation of Network Latency and Communication Time

To get a feeling for the time, that is lost through inter host communication —
when nodes are receiving/sending data — we simply measured the bandwidth we
got from the diﬀerent host positions towards those node serving as the master
machine in the WAN cluster later on (node III). For the real application of
alanine/aug-cc-pVDZ (13 atoms, 213 basis functions) we had to expect a data
transfer of the size of 1452 kB per iteration, which results in a net amount of 27.6

808

S. Höﬁnger

Table 3. Speed-Factor and Network-Latency table for the WAN cluster. Speed-Factors
represent the relative performance of all the individual hosts in the WAN cluster with
respect to the slowest performing CPU. Network bandwidth was obtained from measuring transfer rates between nodes and the future master-machine (node III).
Physical
Location
node I
G.C. BOLOGNA
Italy

Architecture/
Clock Speed/
RAM/2L-Cache

Operating Relative Network Exp.Total
System
Speed
Bandwidth Comm.
Factor
[kB/s]
Time [s]

INTEL Dual PPro 200 MHz LINUX
256 MB/512 KB
2.2.14

1.000000

166

166

node II
ICPS STRASBOURG MIPS R10000 200 MHz
France
20 GB/4 MB

IRIX 64
6.5

1.933617

608

45

node III
GUP LINZ
Austria

INTEL PII 350 MHz
128 MB/512 KB

LINUX
2.2.13

2.054250

—

—

node IV
GUP LINZ
Austria

MIPS R12000 300 MHz
20 GB/8 MB

IRIX 64
6.5

2.989474

918

30

node V
RIST SALZBURG
Austria

ALPHA EV6 21264 500 MHz OSF I
512 MB/4 MB
V 5.0

6.822822

592

47

MB for all the 19 iterations needed throughout the whole calculation. Network
transfer rates and estimated total times spent on communication are also shown
in Table 3.

3

Discussion

A ﬁnal calculation of the above mentioned alanine/aug-cc-pVDZ (13 atoms, 213
basis functions) system on a successive increasing WAN cluster was performed
and led to the execution timings and according Speed Up factors shown in Table
4. A similar, graphical representation of the Speed Up factors is shown in Fig.
1. Instead of strictly applying Amdahl’ s Law, Speed Up ≤ s+ 11−s , we tended
Ncpu

to simply relate (real-usr) timings to each other, which was estimated to have
almost no inﬂuence on relative values, and neglectable inﬂuence on absolute values.
Comparision of the ﬁnal column of Table 3 to the 2nd column of Table 4 reveals
a neglectable inﬂuence of communication time as well.
The 3rd column of Table 4 might be best suited to explain the actual heterogenity of the WAN cluster. In principle there should be one uniform amount of time
spent on the diagonalization- and pre-ERI work, which basically is all what is reﬂected in the Usr Time. However, temporary network bottlenecks, OS-competion
for CPU-time, temporary I/O management excess, CPU-time competition from
interactive user operation — which all was allowed during program execution —
led to that much more realistic, more variational picture.
The plot in Fig. 1 deﬁnes the number of machines in a cumulative way from left
to the right on the abscissa, thus the always added new hosts are indicated at

Load Balancing for the Electronic Structure Program GREMLIN

809

Table 4. Execution timings and Speed Up factors for the DSCF Hartree Fock calculation of alanine/aug-cc-pVDZ with GREMLIN in a WAN cluster made of 1 to 5
nodes.
WAN Cluster Real
Conﬁguration Time
[s]

Usr Sys
Theor.
Real
Time Time �Speed Up Speed Up
[s]
[s]
SPEED[I]

master III
nodes I

240 061 9 268

3

1.000

1.000

master III
nodes I,II

90 280 9 261

8

2.934

2.847

master III
nodes I,II,III 60 496 9 368

2

4.988

4.516

master III
nodes I,II,III 45 014 9 923
IV

3

7.977

6.577

master III
nodes I,II,III 27 038 9 482
IV,V

6

14.800

13.116

Obtained Speed Up for running GREMLIN in a WAN cluster
16

Speed Up obtained
Speed Up ideal

14
12

Speed Up

10
8
6
4
2

21264-500, A, 14.80

R12K-300, A, 7.98

P II-350, A, 4.99

R10K-200, F, 2.93

PPro-200, I, 1.00

0

Ideal Cluster Performance [x times speed of PPro-200]

Fig. 1. Representation of the obtained and ideal Speed Up factors for the DSCF
Hartree Fock calculation of alanine/aug-cc-pVDZ with GREMLIN in a ssh-connected
WAN-cluster, made of up to 5 machines.

810

S. Höﬁnger

those ﬁnal ideal speed level — relative to the slowest node — the cluster should
ideally achieve at that very conﬁguration.
3.1

Conclusion

Considering individual node performance in a heterogenous WAN cluster
properly, may result in excellent parallel scalability for special dedicated
applications, that are characterized from small communication time and large
independent node intervals.
Acknowledgement
The author would like to thank Prof. Zinterhof from RIST++ Salzburg, Prof.
Volkert from GUP Linz and Dr. Romaric David from ICPS Strasbourg for providing access to their supercomputer facilities.

References
1. Höﬁnger, S., Steinhauser, O., Zinterhof, P.: Performance Analysis and Derived
Parallelization Strategy for a SCF Program at the Hartree Fock Level. Lect. Nt.
Comp. Sc. 1557 (1999) 163–172
2. Schrödinger, E.: Quantisierung als Eigenwertproblem. Ann. d. Phys. 79, 80, 81
(1926)
3. Hartree, D.R.: Proc. Camb. Phil. Soc., 24 (1928) 89
4. Fock, V.: Näherungsmethoden zur Lösung des Quantenmechanischen
Mehrkörperproblems. Z. Phys. 61 (1930) 126 62 (1930) 795
5. Höﬁnger, S., Steinhauser, O., Zinterhof, P.: Performance Analysis, PVM and MPI
Implementation of a DSCF Hartree Fock Program. J. Comp. Inf. Techn. 8 (1)
(2000) 19–30
6. Obara, S., Saika, A.: Eﬃcient recursive computation of molecular integrals over
Cartesian Gaussian functions. J. Chem. Phys. 84 (7) (1986) 3963–3974
7. Davidson, E.R., Feller, D.: Basis Set Selection for Molecular Calculations. Chem.
Rev., 86 (1986) 681–696
8. Shavitt, I.: The Gaussian Function in Calculations of Statistical Mechanics and
Quantum Mechanics. Methods in Comp. Phys. ac. New York, 2 (1963) 1–44
9. Dunning Jr., T. H.: J. Chem. Phys. 90 (1989) 1007–1023
10. Woon, D. E., Dunning Jr., T. H.: J. Chem. Phys. 98 (1993) 1358–1371
11. Geist, G., Kohl, J., Manchel, R., Papadopoulos, P.: New Features of PVM 3.4 and
Beyond. Hermes Publishing, Paris Sept. (1995) 1–10

