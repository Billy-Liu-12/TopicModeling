Parallel High-Dimensional Integration:
Quasi-Monte Carlo versus Adaptive Cubature
Rules
Rudolf Schürer
Department of Scientiﬁc Computing, University of Salzburg, AUSTRIA
Abstract. Parallel algorithms for the approximation of a multi-dimensional integral over an hyper-rectangular region are discussed. Algorithms based on quasi-Monte Carlo techniques are compared with adaptive algorithms, and scalable parallel versions of both algorithms are presented. Special care has been taken to point out the role of the cubature
formulas the adaptive algorithms are based on, and diﬀerent cubature
formulas and their impact on the performance of the algorithm are evaluated. Tests are performed for the sequential and parallel algorithms
using Genz’s test function package.

1

Introduction

We consider the problem of estimating an approximation Qf for the multi-variate
integral
�
f (x) dx
If :=
Cs

for a given function f : Cs → IR, where Cs denotes an s-dimensional hyperrectangular region [r1 , t1 ] × · · · × [rs , ts ] ⊂ IRs . Common methods to tackle this
problem on (parallel) computer systems are presented in [1,2].
Numerical integration in high dimensions is usually considered a domain
of Monte Carlo and quasi-Monte Carlo techniques. This paper will show that
adaptive algorithms can be preferable for dimensions as high as s = 40.

2
2.1

Algorithms
Quasi-Monte Carlo Integration

Quasi-Monte Carlo methods are the standard technique for high-dimensional
numerical integration and have been successfully applied to integration problems
in dimensions beyond s = 300.
In this implementation a quasi-Monte Carlo algorithm based on Sobol’s (t, s)sequence [3,4] is used. The sequence used in particular is an (s − 1)-dimensional
sequence using Gray code order to speed up generation as described in [5]. The
ﬁrst point of the sequence (the corner (0, . . . , 0) of the unit cube) is skipped and
V.N. Alexandrov et al. (Eds.): ICCS 2001, LNCS 2073, pp. 1262–1271, 2001.
c Springer-Verlag Berlin Heidelberg 2001
�

Parallel High-Dimensional Integration: Quasi-Monte Carlo

1263

a (t, m, s)-net of dimension s is constructed by adding an additional equidistributed coordinate as described in [3].
This algorithm can be parallelized easily: The net is split into equal sized
blocks, with each processing node taking care of one of them. This can be implemented eﬃciently, because Sobol’s sequence allows fast jumping to arbitrary
positions. The only communication required is the ﬁnal gathering of the estimates calculated by each node.
2.2

Adaptive Algorithm

The key concept of adaptive integration is to apply a basic cubature rule successively to smaller subregions of the original integration domain. The selection of
these subregions adapts to “diﬃcult” areas in the integration domain by reﬁning
subregions with large estimated errors.
The basic sequential algorithm can be outlined as follows:
1. The basic rule is applied to the whole integration domain to estimate the
integral and the error of this approximation.
2. The region is stored in the region collection.
3. The region with the largest estimated error is taken from the region collection.
4. This region is split into two subregions.
5. Estimations for result and error for the new subregions are calculated.
6. Both regions are stored in the region collection.
7. If not done, goto step 3
The loop can be terminated either when a certain absolute or relative error is
reached or when the number of integrand evaluations exceeds an upper bound.
Parallelization. The most straight-forward parallelization of this algorithm
uses a dedicated manager node maintaining the region collection and serving all
other nodes with regions for reﬁnement (see e. g. [6,7]). However, as was shown
in [8], this approach scales badly even for moderate numbers of processing nodes.
To improve scalability, all global communication has to be removed. This
implies that there can not be a dedicated manager node. To balance workload,
some communication between processing nodes is required. However, communication has to be restricted to a small number of nodes in the neighborhood of
each processing node.
The basic idea of this algorithm is that each node executes the sequential
algorithm on a subset of subregions of the initial integration domain. It uses
its own (local) region collection to split regions into subregions and to adapt to
diﬃculties found there. The union of all subregions maintained by all processing
nodes is always equal to the whole integration domain. So the ﬁnal result can
be obtained easily by summing up the results from all processing nodes.
If this algorithm is used without further load balancing, eventually most of
the processing nodes will work on irrelevant reﬁnements on regions with low

1264

R. Schürer

(global) estimated errors, while only a few nodes tackle “bad” regions. To avoid
this problem, regions with large estimated errors have to be redistributed evenly
among processing nodes.
To accomplish this, the nodes are arranged in a G-dimensional periodical
mesh. If the number k of processing nodes is a power of 2, a hypercube with
dimension G = log2 k provides the optimal topology.
After a certain number of reﬁnement steps is performed, each node contacts
its direct neighbor in the ﬁrst of G possible directions and exchanges information
about the total estimated error in its local region collection. The node with the
larger error sends its worst regions to the other node to balance the total error in
both region collections. When redistribution takes place again, it is performed
for the next direction. After G redistribution steps, each node has exchanged
regions with all its direct neighbors, and a bad region that was sent during the
ﬁrst redistribution step may have propagated to any other node in the mesh
by this time. This ensures that bad regions are distributed evenly among all
processing nodes.
The basic idea of this algorithm was ﬁrst published in [9] and was further
developed in [10,11,12].

3

Cubature Rules

Adaptive algorithms are based on cubature rules with error estimation, which
are deﬁned by two terms
(1)

Q

(1)

f :=

n
�
i=1

(2)

(1)
(1)
wi f (xi )

and

Q

(2)

f :=

n
�

(2)

(2)

wi f (xi ) ,

i=1

�
�
with Qf := Q(1) f being an approximation for If , and Ef := �Q(1) f − Q(2) f �
being an estimated error bound for the integration error |If − Qf |.
Usually interpolatory rules are used, which means that Qp is exact for all
(multivariate) polynomials p up to a certain degree d, i. e. Ip = Qp for all p ∈ IP sd .
Q(2) is usually a cubature rule with a degree less than the degree of Q(1) and
requiring signiﬁcantly less integrand evaluations, i. e. n (2) � n(1) . In some cases,
the abscissas of Q(2) are even a subset of the abscissas of Q(1) . For these embedded
rules no extra integrand evaluations are required to estimate the integration
error.
Most empirical evaluations of adaptive integration routines focus on the comparison of diﬀerent adaptive algorithms, but little is known about how the underlying cubature rules aﬀect the performance of the algorithm. After an initial
evaluation of 9 basic cubature rules, the adaptive algorithm described in the
previous section was evaluated based on four diﬀerent cubature rules with error
estimation, leading to signiﬁcantly diﬀerent results.

Parallel High-Dimensional Integration: Quasi-Monte Carlo

3.1

1265

Evaluated Rules

Table 1 lists the basic cubature rules used, together with
�n their degree, their
number of abscissas, their sum of the absolute weights i=1 |wi |, which serves
as a quality parameter and should be as low as possible, and a reference to the
literature.
Table 1. Basic cubature rules
�n
Name
Degree n
Reference
i=1 |wi |
Octahedron
3
O(s)
1
[13]
Hammer & Stroud
5
O(s2 ) 0.62s2 + O(s)
[14]
Stroud
5
O(s2 ) 1.4s2 + O(s)
[15]
Phillips
7
O(s3 ) 0.23s3 + O(s2 )
[16]
Stenger
9
O(s4 ) 1.24 + O(s3 )
[17,18]
Genz & Malik
7
O(2s ) 0.041s2 + O(s)
[19]

Table 2 lists the four cubature formula pairs that were actually used by the
integration algorithm. Formula 7-5-5 is a special case, because it uses 2 additional
basic rules Q(2) and Q(2) for error estimation: Ef is calculated by the formula
� �
��
��
� �
�
�
Ef := max �Q(1) f − Q(2) f � , �Q(1) f − Q(2) f � .
As we will see, this construction leads to superior results for discontinuous functions, which comes at little additional cost in high dimensions, because there the
total number of abscissas is dominated by the nodes of Q(1) .
Table 2. Cubature rules with error estimation
Name
Q(1)
Q(2)
5-3 Hammer & Stroud
Octahedron
7-5-5
Phillips
Hammer & Stroud, Stroud
9-7
Stenger
Phillips
7-5
Genz & Malik

Genz & Malik does already contain an embedded ﬁfth degree rule for error
estimation, so no additional basic rule is required for 7-5. This formula has
often been used throughout the literature, primarily due to
small number of
�its
n
abscissas for s ≤ 10 and its exceptionally low value for i=1 |wi |. For higher
dimensions, however, the number of nodes increases exponentially, resulting in
a strong performance degradation.

1266

4

R. Schürer

Testing

Numerical tests have been performed using the test function package proposed by
Genz [20]. This package deﬁnes six function families, each of them characterized
by some peculiarity. Table 3 gives an overview of these functions.
Table 3. Genz’s test integrand families
Integrand Family
�
�
s
�
f1 (x) := cos 2πu1 +
ai x i

�a�1 Attribute
110
√ Oscillatory
s3

i=1
s
�

1
2
−2
a
+
(x
i − ui )
i
i=1
�
�−(s+1)
s
�
a i xi
f3 (x) := 1 +
f2 (x) :=

600
Product Peak
s2
600
Corner Peak
s2

i=1

� �
�
s
f4 (x) := exp −
a2i (xi − ui )2

100
Gaussian
s

� �
�
s
ai |xi − ui |
f5 (x) := exp −

150
C0 Function
s2

i=1

i=1

f6 (x) :=



0

 exp

��
s

�
a i xi

x 1 > u1 ∨ x 2 > u 2
otherwise

100
Discontinuous
s2

i=1

For each family, n = 20 instances are created by choosing unaﬀective and
1
1
, 1 − 20
]. Afterwards
aﬀective parameters ui and ai pseudo-randomly from [ 20
the vector of aﬀective parameters a = (a1 , . . . , as ) is scaled so that �a�1 meets
the requested diﬃculty as speciﬁed in Table 3.
For each instance k (1 ≤ k ≤ n = 20) of an integrand family, the error e k
relative to the average magnitude of the integral of the current function family
is calculated by the formula
ek =

|Ifk − Qfk |
�n
1
i=1 |Ifi |
n

for k = 1, . . . , n .

For easier interpretation of ek , the number of correct digits dk in the result is
obtained by the formula
dk = − log10 ek

for k = 1, . . . , n .

Based on these values derived for each test integrand instance, statistical methods are used to evaluate the integrand family. The following charts show mean
values with error bars based on standard deviation.

Parallel High-Dimensional Integration: Quasi-Monte Carlo

5

1267

Results

All algorithms are implemented in a C++ program using MPI for inter-process
communication. Standard double precision ﬂoating point numbers are used for all
numerical calculations, forcing an upper bound of about 15 decimal digits on the
accuracy of all algorithms. Tests are performed on an SGI Power Challenge GR
located at the RIST++ (University of Salzburg), based on 20 R10000 MIPS
processors.
We have chosen 20 test functions from each family in dimension 5, 7, 10, 15,
20, 25, 30, and 40. This set of functions is used to test all parallel algorithms,
running on 2, 4, 8, and 16 processors. All calculations are also performed by
the sequential algorithms to measure speed-up and evaluate the accuracy of the
parallel algorithms. The number of allowed integrand evaluations is raised by a
factor of 2 up to a maximal number of 225 evaluations.
5.1

Which Algorithm Performs Best?

If quasi-Monte Carlo or an adaptive algorithm performs best, depends highly
on the integrand and its dimension. While quasi-Monte Carlo degrades little for
high dimensions and non-smooth integrands, the domain of adaptive algorithms
is clearly that of smooth integrands in low dimensions. Which cubature rule
leads to the best results is also dependent on the integrand and the dimension:
While 9-7 works great for smooth integrands, 7-5-5 is better for discontinuous
functions, while 7-5 is especially powerful in low dimensions.
Figure 1 shows which algorithm performs best for a given integrand family in
a given dimension. If two algorithms are reported as “best” for a given problem,
the one in the major part of the ﬁeld achieved the best performance, but the
one in the lower right corner is expected to beat the ﬁrst one eventually, if the
number of allowed integrand evaluations is increased beyond 225 .
Best Accuracy / Processing Time

Best Accuracy / Integrand Evaluation
5

7

10

15

20

25

30

40

Dimension

5

7

10

15

20

25

Oscillatory
Product Peak
Corner Peak
Gaussian

C0-Function
Discontinuous

9-7

7-5

7-5-5

5-3

Quasi-Monte Carlo

Fig. 1. Best algorithm depending on integrand family and dimension

30

40

1268

R. Schürer

The left chart shows which algorithm achieves the highest accuracy per integrand evaluation, while the right chart measures accuracy depending on execution time. So the right chart is most appropriate for integrands that are fast to
evaluate (like the test integrands used here), while the left chart should be used
for integrands that require expensive calculations. In this case the time required
for integrand evaluations will dominate the total calculation time, making it
most important to achieve optimal accuracy with a minimum number of integrand evaluations. The diﬀerence between these two charts is due to the diﬀerent
speed of the abscissa set generation for quasi-Monte Carlo and cubature rules:
The time for generating a single point increases linearly with the dimension for
the quasi Monte-Carlo algorithm, while the adaptive algorithm actually speeds
up for increasing dimensions due to smaller region collections.
Both charts show the results for parallel algorithms running on 16 processing
nodes. However, for a diﬀerent number of processors, or even for the sequential
algorithm, the charts are almost identical. This proves that both parallel algorithms scale equally well to at least 16 processing nodes. It follows that choosing
the best algorithm does not depend on the number of processing nodes available, but only on the dimensionality of the problem and the properties of the
integrand function.
5.2

Details and Discussion

This section will show in more detail how the results in Figure 1 have been
obtained. For each function family and dimension, two charts have been created:
The ﬁrst one showing the number of correct digits depending on the number of
integrand evaluation, the other one depending on execution time. Due to the
huge amount of data, only a few examples can be discussed here.
The left chart in Figure 2 shows the results for Genz function f1 (Oscillatory)
for s = 40. This integrand is very smooth, so the adaptive algorithm performs
better than quasi-Monte Carlo even for a dimension as high as s = 40. Only
two adaptive algorithms can be seen in this chart, because the cubature rules
9-7 and 7-5 require too many points to be evaluated even a single time. 5-3 and
7-5-5, however, proof to be optimal even for high dimensions if the integrand is
smooth enough.
The cubature rule showing the best performance depends on the dimension:
Due to the smoothness of f1 , 9-7 performs better than 7-5 for s up to 10. For
s ≥ 20, 7-5-5 performs best and is overtaken by 5-3 for s = 40, because of the
signiﬁcantly smaller number of integrand evaluation this rule requires in this
dimension.
For Genz function f4 (Gaussian), the adaptive algorithm with the cubature
rule 7-5-5 shows the best performance for dimensions up to s = 10. For dimensions s = 15 and up, however, all cubature rules are inferior to the quasi-Monte
Carlo algorithm. The right chart in Figure 2 contains the result for s = 15,
showing quasi-Monte Carlo integration superior to all four cubature rule based
algorithms.

Parallel High-Dimensional Integration: Quasi-Monte Carlo
Oscillatory, s=40, #PN=16
14
13

Gaussian, s=15, #PN=16
9

7-5-5
5-3
Sobol

7-5
9-7
7-5-5
5-3
Sobol

8

11

# Correct Digits

# Correct Digits

12

1269

10
9
8
7
6

7
6
5
4

5
4

1000

10000

100000

1e+06

3

1e+07

1000

10000

100000

# Integrand Evaluations

1e+06

1e+07

# Integrand Evaluations

Fig. 2. Results for f1 Oscillatory (left) and for f

4

Gaussian (right)

The left chart in Figure 3 shows Genz function f5 (C0 -Function) for s = 5.
For this type of integrand, with f not diﬀerentiable on s hyperplanes, the quasiMonte Carlo algorithm is optimal for all dimensions. For s = 5, it seems possible
that 7-5-5 may converge faster if the number of abscissas increases beyond 225 .
For higher dimensions, however, the adaptive algorithms are completely outperformed.

C0, s=5, #PN=16

8

# Correct Digits

7

Discontinuous, s=15, #PN=16
16

7-5
9-7
7-5-5
5-3
Sobol

14
12
# Correct Digits

9

6
5
4

10
8
6

3

4

2

2

1

1000

10000

100000
1e+06
# Integrand Evaluations

1e+07

7-5
9-7
7-5-5
5-3
Sobol

0

1000

10000

100000
1e+06
# Integrand Evaluations

1e+07

Fig. 3. Results for f5 C 0 -Function (left) and for f6 Discontinuous (right)

The right chart in Figure 3 shows the results for Genz function f6 (Discontinuous) for s = 15. It would be reasonable to assume that the adaptive
algorithms perform even worse here than for f5 . However, this is not the case.
The solution to this paradox is that f5 is discontinuous only on two hyperplanes
(x1 = u1 ∨ x2 = u2 ), while f5 in not diﬀerentiable on s hyperplanes. Especially
the adaptive algorithm 7-5-5 with its additional basic rule for error detection can
cope with this situation, is able to reﬁne on the regions with discontinuity and
can beat the quasi-Monte Carlo algorithm even in dimensions as high as s = 25.

1270

6

R. Schürer

Conclusion

Algorithms based on quasi-Monte Carlo techniques as well as adaptive algorithms are suitable approaches for numerical integration up to at least s = 40
dimensions. Both algorithms can be implemented eﬃciently on parallel systems
and provide good speedups up to at least 16 processing nodes.
If quasi-Monte Carlo or adaptive algorithms perform better depends on the
dimension s of the integration problem, but also on the smoothness of the integrand. For smooth integrands, adaptive algorithms may outperform quasi-Monte
Carlo techniques in dimensions as high as s = 40. For discontinuous or C0 functions, on the other hand, quasi-Monte Carlo may be superior for dimensions
as low as s = 5.
The performance of adaptive algorithms depends highly on the cubature
formula the algorithm is based on. Depending on the dimension and the type of
integrand, diﬀerent cubature rules should be used.

7

Acknowledgments

This work was partially supported by Österreichische Nationalbank, Jubiläumsfonds project no. 6788.

References
1. A. Krommer and C. Überhuber. Numerical Integration on Advanced Computer
Systems. Number 848 in Lecture Notes in Computer Science. Springer-Verlag,
Berlin, Heidelberg, New York, Tokyo, 1994.
2. A. Krommer and C. Überhuber. Computationl integration. SIAM Society for
Industrial and Applied Mathematics, Philadelphia, USA, 1998.
3. I. Sobol. On the distribution of points in a cube and the approximate evaluation
of integrals. U. S. S. R. Computational Mathematics and Mathematical Physics,
7(4):86–112, 1967.
4. I. Sobol. Uniformly distributed sequences with an additional uniform property.
U. S. S. R. Computational Mathematics and Mathematical Physics, 16:236–242,
1976.
5. I. Antonov and V. Saleev. An economic method of computing LP τ -sequences.
U. S. S. R. Computational Mathematics and Mathematical Physics, 19(1):252–256,
1979.
6. V. Miller and G. Davis. Adaptive quadrature on a message-passing multiprocessor.
Journal of Parallel and Distributed Computing, 14:417–425, 1992.
7. R. Čiegis, R. Šablinskas, and J. Waśniewski. Numerical integration on distributedmemory parallel systems. Informatica, 9(2):123–140, 1998.
8. R. Schürer. Adaptive numerical integration on message-passing systems. In
G. Okša, R. Trobec, A. Uhl, M. Vajteršic, R. Wyrzykowski, and P. Zinterhof, editors, Proceedings of the International Workshop Parallel Numerics ParNum 2000,
pages 93–101. Department of Scientiﬁc Computing, Salzburg University and Department of Informatics, Slovak Academy of Science, 2000.

Parallel High-Dimensional Integration: Quasi-Monte Carlo

1271

9. A. Genz. The numerical evaluation of multiple integrals on parallel computers. In
P. Keast and G. Fairweather, editors, Numerical Integration. Recent developments,
software and applications, number C 203 in ASI Ser., pages 219–229. NATO Adv.
Res. Workshop, Halifax/Canada, 1987.
10. J. Bull and T. Freeman. Parallel algorithms for multi-dimensional integration.
Parallel and Distributed Computing Practices, 1(1):89–102, 1998.
11. M. D’Apuzzo, M. Lapegna, and A. Murli. Scalability and load balancing in adaptive
algorithms for multidimensional integration. Parallel Computing, 23:1199–1210,
1997.
12. I. Gladwell and M. Napierala. Comparing parallel multidimensional integraion
algorithms. Parallel and Distributed Computing Practices, 1(1):103–122, 1998.
13. A. Stroud. Remarks on the disposition of points in numerical integration formulas.
Mathematical Tables and other Aids to Computation, 11:257–261, 1957.
14. P. Hammer and A. Stroud. Numerical evaluation of multiple integrals II. Mathematical Tables and other Aids to Computation, 12(64):272–280, 1958.
15. A. Stroud. Extensions of symmetric integration formulas. Mathematics of Computation, 22:271–274, 1968.
16. G. Phillips. Numerical integration over an n-dimensional rectangular region. The
Computer Journal, 10:297–299, 1967.
17. F. Stenger. Numerical integration in n dimensions, 1963.
18. A. Stroud. Approximate Calculation of Multiple Integrals. Prentice-Hall, Englewood Cliﬀs, NJ, USA, 1971.
19. A. Genz and A. Malik. Remarks on algorithm 006: An adaptive algorithm for
numerical integration over an n-dimensional rectangular region. Journal of Computational and Applied Mathematics, 6(4):295–302, 1980.
20. A. Genz. Testing multidimensional integration routines. Tools, Methods and Languages for Scientiﬁc and Engineering Computation, pages 81–94, 1984.

