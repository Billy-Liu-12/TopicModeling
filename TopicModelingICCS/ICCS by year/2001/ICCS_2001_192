The DOE Parallel Climate Model (PCM):
The Computational Highway and Backroads
Thomas Bettge, Anthony Craig, Rodney James,
Vincent Wayland, and Gary Strand
National Center for Atmospheric Research, 1850 Table Mesa Drive,
Boulder, Colorado 80303
(bettge, tcraig, rodney, wayland, strandwg) @ucar.edu

Abstract. The DOE Parallel Climate Model (PCM) is used to simulate the
th
earth’s climate system and has been used to study the climate of the 20 century
st
and to project possible climate changes into the 21 century and beyond. It was
designed for use on distributed memory, highly parallel, architectures. The
computational requirements and design of the model are discussed, as well as
its performance and scalability characteristics. A method for port validation is
demonstrated. The shortcomings of the current model are summarized and
future design plans are presented.

1 Introduction
The U.S. Department of Energy (DOE) Parallel Climate Model (PCM), constructed at
the National Center for Atmospheric Research (NCAR) during the mid-1990s, is a
comprehensive model of the earth’s climate system, including fully active,
scientifically contemporary, components of the atmosphere, ocean, land, river runoff,
and sea ice.
The model has been used to simulate numerous climate states and climate change
scenarios, including control climates of 1870, 1990, and 2060 (before and after the
onset of anthropogenic activities), and the transient climate during 1870-2000 using
observed greenhouse gas, sulfate aerosol, and solar variance forcing (Washington et
al., 2000). Ensembles of future simulations extending to 2100 and beyond have been
performed using the Intergovernmental Panel On Climate Change (IPCC) Third
Assessment Report best case and worse case greenhouse gas forcing scenarios
(Cubasch et al., 2001). In all, the PCM has been used to simulate over 7000 years of
the earth’s climate during the past four years.
At inception the target architectures of the PCM were massively parallel, distributed
memory machines. The mid-1990s generation of Massively Parallel Processing
(MPP) computer architectures forced the design of PCM into an all message passing
application where the components themselves were stacked into the local memory on
each computational node and run sequentially. Subsequently, as computer hardware
V.N. Alexandrov et al. (Eds.): ICCS 2001, LNCS 2073, pp. 149-158, 2001.
© Springer-Verlag Berlin Heidelberg 2001

150

T. Bettge et al.

at DOE facilities evolved with the deployment of Distributed Shared Memory (DSM)
architectures and clusters of Symmetric Multi-Processors (SMPs), the PCM was
ported and used successfully on a variety of these types of machines.
While the primary focus of the PCM is scientific investigation and discovery,
computational performance is a high priority. The purpose of this paper is to review
and present several computational aspects of the PCM including the early decision
making process, the issue of portability across machines, and performance and
productivity on available machines.

2 Computational Requirements
A major goal of the PCM was to use existing model components developed under the
DOE Computer Hardware Advanced Mathematics Model Physics (CHAMMP)
program. These individual models were targeted for machines with distributed
memory across a large number of processors – the MPP class of architectures.
Many of the computational requirements of the PCM are obvious and include:
•
•
•
•

must easily port across a number of platforms
must validate across a number of platforms (i.e., must produce statistically
identical climate simulations)
must achieve a minimum level of high performance
must use existing component models (PCM was not a model/software
development project, but rather was charged to modify existing models to the
extent necessary to achieve the requirements in a substantial way)

With respect to software development, the PCM was created using software
engineering techniques which could focus on rapid development with customized
software specific to, and documented among, a small group of developers. The
limited scope of the project did not require the overhead and management which is
recommended for larger projects which develop software for a much larger
community.

3 The Model Components
The PCM is composed of individual models of the atmosphere, land, ocean, river
runoff, and sea ice.
The atmospheric model is the parallel version of the NCAR Community Climate
Model version 3 (CCM3) at T42 spectral truncation horizontal resolution (2.8 degrees
latitude by longitude) with 18 vertical levels (see Kiehl et al., 1998).

The DOE Parallel Climate Model (PCM)

151

The land surface model is a one-dimensional model of the energy, momentum, and
water exchanges between the earth’s surface and the atmosphere, on the same
horizontal grid as the atmosphere (Bonan, 1996). A river runoff scheme is included
which directs freshwater from the land into the ocean according to predetermined
(observed) basin flows (Branstetter et al., 2001).
The ocean component is the Los Alamos National Laboratory Parallel Ocean Program
(POP) model with a displaced polar grid in the northern hemisphere. The grid has an
average horizontal resolution of 2/3 degress latitude and longitude, with 32 vertical
levels (Dukowicz and Smith, 1994).
The sea-ice model provides a full dynamic and thermodynamic representation of the
polar ice caps on a 27 km Cartesian grid centered over each pole (Washington et al.,
2000).
Since the PCM is composed of components from independent model developers,
considerable effort was required to create a software environment where the full PCM
system would perform optimally, including the construction and implementation of a
so-called coupler, which facilitates the exchange of information between individual
components. The coupler steers the execution of the components and provides the
mechanisms necessary to coordinate and support the transfer of data between the
components, including computation of fluxes at the component interfaces,
conservation of energy, and interpolation of data from one grid to another using the
technique of Jones (1999).

4 Computational Implementation
When considering the implementation of the various components of PCM into a fully
integrated computational model, it was necessary to evaluate the suitability of the
available hardware to the problem at hand. Since the original target, the CrayT3D/T3E, did not have at the time an acceptable mechanism for allowing high speed
message passing between active processes on separate partitions, it was decided to
implement the PCM as a Single Program Multiple Data (SPMD) application,
overlaying each component within the memory on the local compute node of the
partition.
Fig. 1 shows a simplified representation of how the PCM is integrated sequentially in
time. The primary time period over which the PCM components are synchronized,
defined as the synchronous coupling interval, is one day. During this period the
components each advance one day in sequence, with each component gathering or
passing data as needed to the coupler for processing and distribution to the next
component. All components and the coupler use all of the available computational
nodes during the period in which they are active. Message passing within each
component is achieved by use of the standard Message Passing Interface (MPI)
library.

152

T. Bettge et al.

Two major obstacles encountered in this scheme are (1) the horizontal grid is different
for each component, and (2) the internal data decomposition employed in the
distributed partitioning is different for each component. Thus, the partitioning
strategy in the coupler became one of optimizing the use of MPI to gather and scatter
information as needed when it is processed (in interpolating data from one grid to
another, for example). Since the exchange of data between components involves very
few floating point operations, the coupler activity is inherently non-scalable. For the
PCM, this data movement problem was minimized by passing a majority of the data
between components at the day boundaries.

Fig. 1. Simplified integration sequence of the component models in PCM over the synchronous
time interval of one day.

There are two characteristics of the current components used in PCM which limit its
overall performance and scalability. First, the atmospheric model contains a onedimensional decomposition, which limits the number of processors that can be used in
the solution to the number of latitude lines of the chosen horizontal grid resolution,
which is 64. Second, the ocean model contains an elliptic equation solver which
requires significant numbers of messages (MPI all-reduces) and many iterations
during each two-hour timestep. When the problem is partitioned over more than 64
processors, the number of messages generated simply overwhelms the communication
ability of the current target machines.

The DOE Parallel Climate Model (PCM)

153

Finally, the PCM is an all message passing application. It is partitioned in a way such
that there is a straightforward, one-to-one, mapping of MPI processes to processors.
There is no threading on compute nodes that contain multiple processors.

5 Performance
5.1 Productivity
The overall performance as well as productivity of the PCM on its targeted production
machines is summarized in Fig. 2, which shows the number of simulated years per
wallclock month achieved by a fixed version of PCM over the past several years.
Climate modelers prefer to measure model performance in terms of scientific
productivity rather than floating point operation counts for at least two reasons: (1)
productivity, not floating point counts, is the goal of the scientific investigation, and
(2) floating point operation counts are difficult to determine on many machines.

Fig. 2. PCM productivity for a fixed model version on 64pes of various machines from 1995
through 2000. Cray T3D and T3E, SGI Origin 2000 and 3000 (O2K, O3K, with chip clock
speed given as last three digits), and IBM SP (WHI is WinterHawk I node, WHII is
WinterHawk II node, and NHII is NightHawk II node) are shown.

154

T. Bettge et al.

The original target machine of the PCM was the Cray-T3D, on which the rate of
production was one simulated year per wallclock day using 64 processors. With the
introduction of the Cray-T3E, especially the T3E-900, the production rate of PCM
increased to the same range as other similar climate model codes which were
designed for and used on parallel vector machines in production at various U.S. sites
in 1997.
In successive years new machines became available. The design of PCM, in
particular the use of a standard message passing library (MPI), allowed quick
deployment of the model onto these machines. The PCM was given the enviable
opportunity of being one of the first users at a number of sites, and the fact that we
were able to take significant advantage of these opportunities is witness to the flexible
design and portability of the PCM. The scientific returns with PCM for a fixed
problem size increased rapidly as new, faster machines were introduced. Our current
production rate for a single integration is six simulated years per wallclock day – a
six-fold increase in productivity over a five-year period.
5.2 Scalability
The question arises as to how well PCM scales on these machines. Fig. 3 shows the
model scaling for a variety of machines. Generally speaking, PCM scales reasonably
well up to 32 processors and acceptably well up to 64 processors.

Fig. 3. PCM scalability from 8 to 64 processors on four machines. Simulated years per
wallclock day.

The DOE Parallel Climate Model (PCM)

155

As was mentioned in section 4, the PCM is limited inherently to 64 processors due to
the internal data decomposition of the atmospheric model. Additionally, the PCM
limit of scalability is reached at 64 processors on the current generation of machines
due to a combination of (1) the message latency and bandwidth on the hardware (an
exception is the Cray-T3E), and (2) the message traffic generated by the model, in
particular the elliptic equation solver in the ocean model. With respect to the
communication within the components themselves, the individual model developers
are exploring new methods to minimize messages, but no attempt has been made to
address these issues within the PCM.

6 Port Validation
Use of the PCM across a variety of vendor platforms at a number of diverse
computational centers involves validating the fact that the model was ported
successfully to a new machine or site. The topic of port validation is important for
obvious reasons. From platform to platform the model solutions, obtained using
identical code and identical initial conditions, will diverge. How does one know the
answers are correct?
The portability of PCM is validated by a procedure outlined by Rosinski and
Williamson (1997). On a machine with which one has confidence in the solution, two
simulations are produced. The second simulation is identical to the first, except that
the solution path is forced to be different by introducing a random perturbation, which
can be felt by the model at the precision of the machine, into the initial state. The
growth of differences between the two solutions is considered to be representative of
machine roundoff errors, and is assumed to accumulate only because of the initial,
low-order perturbation. By design these errors are typical of the errors expected
when the model is moved to a different architecture or when a new or different
compiler is used, re-ordering some calculations. The difference of the simulation on
the destination machine versus the original simulation on the first machine should
exhibit similar characteristics as the differences generated by the two simulations on
the first machine. Specifically, the differences during the first few timesteps should
be close to the magnitude of machine rounding, and the differences should grow no
faster than the differences from the two control simulations on the first machine.
Fig. 4 shows the result of a PCM port validation exercise. Note that the simulation
differences, as measured by the RMS difference of surface temperature, between two
machines are within one to two orders of machine rounding during the first few
timesteps. During the first 144 timesteps (two days), the growth of the difference
between the original and ported solutions do not exceed the growth of the initial
perturbation introduced into the low-order bits of the original solution. In this case,
the port is considered valid.

156

T. Bettge et al.

Fig. 4. Port validation of the PCM. The black line is the difference between two simulations
on the control machine, which differ only because of an initial, low-order perturbation. The red
line is the difference between two identical simulations produced on different architectures.

With climate modeling, it is equally important that the original and ported climate
states are statistically identical. This can be accomplished only by making longer
integrations and comparing the climates of the two states. This test is also part of the
standard PCM validation procedure.

7 Scientific Results
The overall ability of PCM to simulate the earth’s climate is demonstrated by the
model’s ability to simulate the climate during recent past, shown in Fig. 5. Arguably,
the model reproduces the observed changes during the past 130 years in the globally
averaged surface temperature. This version of the model was forced using observed
greenhouse gas values (which act to produce heating at the surface), observed
atmospheric sulfate aerosols (which act to produce cooling at the surface), and
observed values of the incoming solar energy at the top of the atmosphere (which is
observed to have slight variations over time). It is believed that the observed

The DOE Parallel Climate Model (PCM)

157

warming at the surface of the earth is caused by the net effect of changes in the
radiative forcing on the earth, of which these three are major contributors.

Fig. 5. Global surface temperature anomaly in degrees centigrade for 1860-1999. Observed is
the black line. PCM simulation is the red line.

The magnitude of the temperature change, as well as the specific causes responsible
for the change, are the subject of continuing scientific debate. In addition, the coarse
resolution of the horizontal and vertical grid at the earth’s surface is insufficient to
allow definitive conclusions to be drawn concerning potential regional climate
changes. The answers to questions like these lie, at least partially, in the effectiveness
with which scientists can obtain and use increased computational power.

8 Future
At NCAR a climate model similar to the PCM was build with funding from the
National Science Foundation – the Climate System Model (CSM) (CSM Model Plan,
2000). The CSM was designed originally and primarily for shared memory, vector
parallel, architectures. Over the next year the PCM and the CSM will combine

158

T. Bettge et al.

efforts, and a new model is currently under construction and will contain the
following features, which address many of the original PCM shortcomings:
•
•
•
•
•

individual components will integrate in Multiple Instruction Multiple Data
(MIMD) mode; the components will run concurrently over the synchronous
time period
all components will contain a general two-dimensional data decomposition
hybrid parallelism will be implemented within all components (threading on
node, and message passing between compute nodes)
communication between components via the coupler will be generalized and
optimized
specific problems (e.g., the elliptic equation solver in the ocean model) will
be addressed with improved, computationally efficient, algorithms

The model should be ready for use in 2002.

References
1.

2.

3.
4.

5.
6.
7.
8.
9.

Bonan, G.B.: A Land Surface Model (LSM version1) for Ecological, Hydrological, and
Atmospheric Studies: Technical Description and User’s Guide. NCAR Technical Note
NCAR/TN-417+STR. National Center for Atmospheric Research, Boulder, Colorado
(1996)
Branstetter, M.L., Famiglietti, J.S., Washington, W.M., and Craig, A.P.: Using a 200-year
simulation of a fully coupled climate system model to investigate the role of the
continental runoff flux on the global climate system. Climate Variability, the Oceans, and
Societal Impacts Symposium, American Meteorological Society, Albuquerque. (2001)
Community Climate System Model Plan (CCSM) for 2000-2005. Prepared by CCSM
Scientific Steering Committee. National Center for Atmospheric Research, Boulder,
Colorado (2000)
Cubasch, U., Meehl, G.A., Boer, G.J., Stouffer, R.J., Dix, M., Noda, A., Senior, C.A.,
Raper, S. and Yap, K.S.: Projections of Future Climate Change. Climate Change 2001:
The Scientific Basis. In: Houghton, T, Ding Y., Griggs, D.J., Noguer, M. van der Linden,
P., Dai, X., Maskell, K, and Johnson, C.I. (eds.) Contribution of Working Group I to the
Third Assessment Report of the Intergovernmental Panel on Climate Change (IPCC).
Cambridge University Press (2001)
Dukowicz, J.K.and Smith, R.D.: Implicit Free-Surface Method for the Bryan-Cox-Semtner
Ocean Model. Journal of Geophysical Research (1994)
Jones, P.: First and Second -Order Conservative Remapping. Monthly Weather Review
(1999)
Kiehl, J.T.: Simulation of the Tropical Pacific Warm Pool with the NCAR Climate System
Model. Journal of Climate (1998) 11:1342-1355
Rosinski, J.M. and Williamson, D.L.: The Accumulation of Rounding Errors and Port
Validation for Global Atmospheric Models. Journal of Scientific Computation, Vol. 18:2
(1997)
Washington, W.M., Weatherly, J.W., Meehl, G.A., Semtner, A.J., Bettge, T.W., Craig,
A.P., Strand, W.G., Arblaster, J.M., Wayland, V.B., James, R., Zhang, Y.: Parallel
Climate Model (PCM) Control and Transient simulations. Climate Dynamics, Vol. 16
(2000) 755-774

