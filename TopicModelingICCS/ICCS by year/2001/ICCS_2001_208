Bayesian Parameter Estimation:
A Monte Carlo Approach

Ray Gallagher
Department of Computer Science, University of Liverpool, Liverpool L69 7ZF, United
Kingdom.
Email addresses: rayg@csc.liv.ac.uk

Tony Doran
Department of Computer Science, University of Liverpool, Liverpool L69 7ZF, United
Kingdom.
Email addresses: tdustdum@csc.liv.ac.uk

Abstract. This paper presents a Bayesian approach, using parallel Monte Carlo
modelling algorithms for combining expert judgements when there is inherent
variability amongst these judgements. The proposed model accounts for the
situation when the derivative method for finding the maximum likelihood
breaks down

Introduction
An expert is deemed to mean a person with specialised knowledge about a given
subject area or matter of interest. This paper concerns itself with the situation where
we are interested in an uncertain quantity or event and expert opinion is sort out by a
decision-maker. The question then arises as to how a decision-maker should then
make optimal use of the expert opinion available to them. Moreover, how does a
decision-maker make optimal use of expert opinion when several experts are available
to them and further resolve conflicting opinions amongst the group of experts. The
opinions of an expert may come in many ways: a point estimate, parameters of
uncertainty distribution or a best guess with upper and lower bounds. The challenge
for the decision-maker is to correctly take full advantage of the data provided.
Formally uncertainty can be represented in terms of probability and the ultimate aim
is to reach a consensus to arrive at a probability distribution for the uncertain quantity
of interest. This distribution should fully reflect the information provided by the
experts.
Various consensus procedures for the pooling of experts' opinions and probability
distributions have been suggested, encompassing merely the simple averaging of
V.N. Alexandrov et al. (Eds.): ICCS 2001, LNCS 2073, pp. 812-822, 2001.
© Springer-Verlag Berlin Heidelberg 2001

Bayesian Parameter Estimation: A Monte Carlo Approach

813

expert probability distributions through to a formal Bayesian approach. Bayesian
methods have been favoured by a number of researchers. Reviews of the available
1
2
3
literature being provided by French , Cooke together with Genest and Zidek . The
4-6
7,8
9,10
11
models proposed include those by Lindley ,Morris ,Winkler and Mosley This
paper examines two different methods that allow the decision-maker to make the
optimum decision based on available expert opinion. The methods are:

•
•

Derivative Method
Monte Carlo

Making the optimal decision based on the derivative method means that the function
12
must be differentiable. We note there are other methods, discussed in Zacks, to
address this situation. If the function is not differentiable then we must employ a
numerical method (in our case Monte Carlo) to arrive at an estimate of the quantity of
interest. We further make use of parallel architectures using MIMD methods to
increase the efficiency of the Monte Carlo method in situations where we may have a
large body of expert opinion available.

Uncertainty Modelling of Expert Opinion
Suppose we have a parameter

θ

= (θ1, θ2,........ θn) and to obtain the best decision

about θ we have to use some expert opinion given by E = {x1 , x 2 ,......., x N }
th
where xi is the estimate of the i expert for an unknown quantity x, with the
recognition that the particular value being estimated by that expert may be different
from that being estimated by another expert.
The quantity of interest may be a fixed parameter but its exact value is unknown such
as the height of a building or it may be an inherently variable quantity such as the IQs
of individual members of a group of people.
*

*

*

The situation arises, for example, when experts provide estimates based on experience
with sub-populations of a non-homogeneous population. The objective is to develop
an estimate of the distribution representing the variability of x in light of the evidence
presented.
We attempt to aggregate these expert opinions to reach the "best" decision based on
the estimation of

θ

.

For simplification we restrict ourselves to the situation when

θ

comprises one or two

elements. We then provide a general solution for θ dependent on N elements. For
formalisation of this discussion we consider the Bayesian approach to probability.
Let us consider the following definition of Bayess Theorem

π ( µ | E ) = k −1 L( E | µ )π 0 ( µ )

814

R. Gallagher and T. Doran

Where:
θ ≡ The value of interest to the decision maker,
E ≡ the set of experts opinions about the value of θ, the decision-maker treats this set
of opinions as evidence/data,
π0(θ) ≡ the decision makers prior state of knowledge on θ,
π(µ|E) ≡ the decision makers posterior state of knowledge on θ,
L(E|θ) ≡ the likelihood of observing the evidence E, given that the true value of the
unknown quantity is θ,
k ≡ P(E), the normalisation factor that makes π(θ|E) a probability distribution.
The problem of expert opinion is thus reduced to the assessment of the prior, π0, and
the likelihood, L, by the decision-maker. The key element in this approach is the
likelihood. The likelihood function is the decision makers tool to measure the
accuracy of the experts estimate after considering the experts level of pertinent
experience, calibration as an assessor, any known bias, and dependence to other
experts.
In this section of the paper we summarise how we can receive

∧

π (θ | E )

regard to experience, what is the best decision depends on E. Since every

i.e. with

x i* is just

some information concerning xi we consider f(xi|θ) as the actual distribution of the
quantity of interest, x. We consider
experts estimate is

L( x i* | θ ) is the probability density that the

x i* if the decision maker believes that the ith expert is perfect then

L( x i* | θ ) = f(xi|θ).
Since the experts are considered independent then we have
(1)

n

L( E | θ ) = L( x1* , x 2* ,........, x n* | θ ) = ∏ L( x i* | θ )
i =1

Moreover,

π (θ | x1* , x 2* ,......., x *N ) = k −1 L( x1* , x 2* ,.......x n* | θ )π 0 (θ ) .

method we should first obtain k such that
distribution. Suppose Pi= Pi( x

*
i

π (θ | x1* , x 2* ,...... x n* ) is

In this

the conditional

| x i ), (this Pi is one, if and only if, the expert is
*

considered to be perfect) is the probability that the i expert says x i when in fact the
true value is xi. The quantity Pi is the decision makers probability density that the
th

experts estimate is

x i* when he is attempting to estimate xi.

We should note that xi is one possible value of x and x is distributes according to
f(x|θ). Then

Bayesian Parameter Estimation: A Monte Carlo Approach

⎧⎪ ∫ Pi ( xi*|xf ( x|θ ) dx if
*
Li ( x i | θ ) = ⎨ P ( x*|x ) P ( x |P )
⎪⎩ ∑j i j j

815

(2)
X
if

continuous
X

discrete

For N independent experts we have

⎧ −1 *
⎪ k { Pi ( xi |x) f ( x|θ )dx}π0 (θ )
π (θ | x1* , x2* ,.....xn* ) = ⎨ ⎧∫ n
⎫⎪
⎪
Pi ( xi*|x j ) f ( x j |θ )⎬π0 (θ )
⎪ k−1 ⎨⎪∏∑
⎪⎭
⎩ ⎩ i =1 j

(3)

For the best decision based on the evidence, E, we can use the derivative method if
the derivative exists i.e.

∂
π (θ | x1* , x 2* ,......x n* ) = 0
∂θ j

(4)

j = 1,2,..., n

These systems named normal equations, and receive

∧

θj =θ

j

and for the maximum

of L must be

∂2
π (θ | x1* , x 2* ,......x n* ) ∧ < 0
θ j =θ j
∂θ 2j

j = 1,2,..., n

(5)

Example:
Suppose the decision-maker is interested in assessing the probability distribution of a
random variable that takes only two values i.e. let
X = {x1, x2}.

(6)

816

R. Gallagher and T. Doran

A discrete distribution of X is completely known if we know P, where
0 ≤ θ ≤ 1.
θ ≡ Pr[X= x1] and 1- θ ≡ Pr[X= x2] ,
Suppose now the decision-maker asks the opinion of N experts on whether X =x1 or
whether X =x2. Let E, defined as
responses where

E = { x1* , x 2* ,......., x *N } be the set of expert

x i* , the ith response can be either X =x1 or X =x2. Then we have

π (θ | E ) = k −1 L( E | θ )π 0 (θ ) where
n

n

i =1

j =1

L( E | θ ) = ∏ Li ( x i* | θ ) and L( x i* | θ ) = ∑ Pr ( x i* | x j ) Pr ( x j | θ ) .

(7)

It is trivial that

⎧θ
⎪
Pr ( x j | θ ) = ⎨
⎪1 − θ
⎩
*

Where Pr( xi

if

j =1

if

j=2

(8)

| x j ), is the probability that the ith expert says xi* when in fact X = xj .

These values represent how good the decision-maker thinks the experts are. For
example, let us assume that the decision-maker consults two experts who he believes
to be perfect and independent. For simplicity we assume a uniform prior in the closed
interval [0,1], i.e. π0(θ) = 1, and consider the following two cases.

Case (i)
The two experts have opposing opinions, e.g.
likelihood is

x1* = x1 and x 2* = x 2 . Then, the

n

L = ∏ Li ( x i | θ ) = θ (1 − θ )

(9)

i =1

and the posterior will be:

π (θ | x1 , x 2 ) = 6θ (1 − θ )

(10)

Bayesian Parameter Estimation: A Monte Carlo Approach

With regard to equation (9) we have
be a conditional distribution then

π (θ | E ) = k −1θ (1 − θ )

817

since π(θ|E) should

(11)

1

−1
∫ π (θ | E )dθ = k ⇒ k = 1 6
0

Then we have

π (θ | x1* , x 2* ,....... x n* ) = 6θ (1 − θ ) = 6θ − 6θ 2

0 ≤θ ≤1

(12)

Now, with regards to derivative tests for finding the extreme points we have
∧
∂π
= 6 − 12θ = 0 ⇒ θ = 1
2
∂θ

∂ 2π
∂θ 2

1
θ =
2

<0

(13)

(14)

This represents the distribution of all possible distributions of X. The most probable
distribution (i.e. the mode of the posterior π(θ|x1,x2)) is given by θ=1/2. It means that
starting from complete lack of knowledge about the distribution of X, the opposing
opinions of two independent experts have caused the decision maker to think most
probably X= x1 and X= x2 are equally likely.

Case (ii)
The two experts have the same opinion; that is, for example,
The posterior in this case will be

π (θ | x1 , x 2 ) = 3θ 2

x1* = x1 and x 2* = x1 .
(15)

We leave the proof of the second case as it is essentially the same operation of case i.

818

R. Gallagher and T. Doran

The main idea of this paper is when the situation arises when we wish to arrive at the
12
optimum decision when there is no derivative, Zacks, . In this situation we can use
the finite difference gradient algorithm.
(16)

∧

θ i +1 = θ i + α i ∇ π (θ | E )
Therefore
∧
∧
⎛ ∧
∂π
⎜ ∂π ∂π
∇π = ⎜
,
,......... ,
∂θ n
⎜ ∂θ 1 ∂θ 2
⎝
∧

(17)

⎞
⎟
⎟⎟
⎠

where
∧

g ((θ 1 + ∆θ ),θ 2 ,.....,θ n ) − g ((θ 1 − ∆θ ),θ 2 ,.....,θ n )
∂π
=
∂θ 1
2∆θ

(18)

i = 1,2,�, n
In this case we can consider a Monte Carlo random search algorithm to estimate the
optimum decision for θ.

Random Search
14

We choose the random search double trial algorithm, Rubinstein

θ i +1 = θ i +

.

αi
[π (θ i + ∆θ i t i ) − π (θ i − ∆θ i ti )].ti
2∆θ i

(19)

where αi and ∆θ1 are greater than 0. This estimation θ of θ, converges to θ in
13
quadratic mean, in probability, and with probability one, Halton .
This algorithm may be performed by generating the random vector ti continuously
distributed on the n-dimensional unit sphere.

Bayesian Parameter Estimation: A Monte Carlo Approach

For this algorithm, if π is a real function which depends on

819

θ = (θ 1 ,θ 2 ,..., θ m ),

→

then we can use t i = (ti1 , t i 2 ,.....tin ) , i = 1,...,m, and use n random vectors, in this
situation we have a lot of random samples and we can try by parallel processing
th
methods in a MIMD environment to obtain θi by the i processor, in a small time
interval.

θ = (θ 1 , θ 2 ,..., θ m ), we generate (i = 1,...,m)
If
random vectors and

t i = (t i1 , t i 2 ,�, t in ) m

t 01

t11

t12

t13

...

...

...

t1n

t 02

t21

t22

t23

...

...

...

t2n

�
�

�
�

�
�

tm1

tm2

tm3

�
�
...

�
�
...

�
�
...

�
�
tmn

�
�
t 0m

⎡ t 01 ⎤
⎢t ⎥
02
� ⎢ ⎥
t = ⎢t 03 ⎥
⎢ ⎥
⎢� ⎥
⎢⎣t 0 n ⎥⎦

is a vector with m rows and each element is n-tuples of random

numbers. Then we collect the following random vectors

⎡t11 ⎤
⎢t ⎥
⎢ 21 ⎥
�
t 01 = ⎢t 31 ⎥ ,
⎢ ⎥
⎢�⎥
⎢⎣t n1 ⎥⎦

⎡ t12 ⎤
⎢t ⎥
⎢ 22 ⎥
�
t 02 = ⎢ t 32 ⎥
⎢ ⎥
⎢ � ⎥
⎢⎣t n 2 ⎥⎦

,...,

�
t0m

⎡ t1m ⎤
⎢t ⎥
⎢ 2m ⎥
= ⎢t 3 m ⎥ ,
⎢ ⎥
⎢ � ⎥
⎢⎣t nm ⎥⎦

for distribution to processors 1, 2, ......, m respectively, enabling us to obtain the
result

θi

th

from the i processor. We will then have

an estimate for θ.

∧

∧

∧

θ = (θ 1 ,...,θ n )

such that

∧

θ

is

820

R. Gallagher and T. Doran

Algorithm 1
1.

Set Sum = 0

2.

Do N times

3.

Generate

→

t i = (t i1 , t i 2 ,�, t im ) sampling from z 0i

4.
5.

Do until convergence
Set

θ i +1 = θ i +
6.

Set Sum = Sum + θI+1

7.

Goto 3.

8.

Goto 2.

9.

Set Sum = Sum/N

10.

Set

αi
[π (θ i + ∆θ i t i ) − π (θ i − ∆θ i t i )].t i
2 ∆θ i

∧

θ =θn .

Algorithm 2
θ

Get the parameter

2.

Generate the random vector

z 01

=

(θ1 ,...,θ m ) , and π (θ | E ) .

1.

⎡ z11 ⎤
⎢z ⎥
⎢ 21 ⎥
= ⎢ z 31 ⎥ ,
⎢ ⎥
⎢ � ⎥
⎢⎣ z n1 ⎥⎦

z = ( z11 , z12 ,..., z1n ) for i = 1,2,...,m.

z 02

⎡ z12 ⎤
⎢z ⎥
⎢ 22 ⎥
= ⎢ z 32 ⎥
⎢ ⎥
⎢ � ⎥
⎢⎣ z n 2 ⎥⎦

,...,

z 0m

⎡ z1m ⎤
⎢z ⎥
⎢ 2m ⎥
= ⎢ z 3m ⎥ ,
⎢ ⎥
⎢ � ⎥
⎢⎣ z nm ⎥⎦

3.

Collect

4.

Send

5.

Algorithm 1.

6.

Get θ 1 ,θ 2 ,.......,θ m i.e. the Monte Carlo estimates of θi.

z 01 , z 02 ,..., z 0 m to processors 1, 2, ...,m.
∧

∧

∧

Bayesian Parameter Estimation: A Monte Carlo Approach

7.

∧

∧

∧

∧

821

∧

θ = (θ 1 ,θ 2 ,...,θ m ) , as an estimation of θ such as π (θ | E ) is
an optimal estimate of π (θ | E ) without the need to resort to the derivative
Consider

method.

Conclusion
This approach allows a solution to be obtained when there is no derivative. Further by
virtue of parallel processing it allows complex models containing many experts to be
calibrated. It is intended to consider the problems of dependencies amongst experts in
a further paper where the computational demands are considered to be excessive.

References
1.
2.
3.
4.
5.
6.
7.
8.
9.
10.
11.
12.
13.

French, S., Group Consensus Probability Distribution: A Critical Survey. In Bayesian
Statistics 2, ed J. M. Bernardo, M. H. DeGroot, D. V. Lindley & A. F. M. Smith.
North Holland, Amsterdam, (1985), pp. 183-201.
Cooke, R. M., Expert Opinion and Subjective Probability in Science. Delft University
of Technology Report, Chapter 11, (1990).
Genest, C. & Zidek, J. V., Combining Probability Distributions: A Critique and an
Annotated Bibliography. Statistical Science., 1 (1986) 114-48.
Lindley, D. V., Reconciliation of Probability Distributions. Operational Research.,
31 (1983) 866-80.
Lindley, D. V. & Singpurwalla, N., Reliability (and Fault Tree) Analysis Using
Expert Opinions. Journal American Statistical Association., 81 (393) (1986) 87-90.
Lindley, D. V., Tversky, A. & Brown, R. V., On the Reconciliation of Probability
Assessments (with discussion). J. R. Statist. Soc. Ser A, 142 (1979) 146-80.
Morris, P. A. Combining Expert Judgements: A Bayesian Approach. Management
Science., 23 (1977) 679-93.
Morris, P. A., An Axiomatic Approach to Expert Resolution. Management Science.,
29 (1983) 866-80.
Winkler, R. L., The Consensus of Subjective Probability Distributions Management
Science., 15 (1968) B61-B75).
Winkler, R. L., Combining Probability Distributions from Dependent Information
Sources. Management Science., 27 (1981) 479-88.
Mosley, A. Bayesian modeling of expert-to-expert variability and dependence in
estimating rare event frequencies. Reliability Engineering and System Safety 38
(1992) 47-57.
Zacks, S. The Theory of Statistical Inference. John Wiley and Sons. New York
(1971)
pages (230-233).
Halton, J. H., A retrospective survey of the Monte Carlo method. Siam Rev, 12(1)
(1970) 1-63.

822

R. Gallagher and T. Doran

14.

Rubinstein, R.Y., Simulation and the Monte Carlo Method. John Wiley and Sons.
New York (1981) Page 238.

Contact Information: Ray Gallagher. Telephone 44-151-794-3161. Facsimile 44-151-3715

