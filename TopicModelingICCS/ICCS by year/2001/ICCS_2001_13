On Eﬃcient Application of Implicit
Runge-Kutta Methods to Large-Scale Systems
of Index 1 Diﬀerential-Algebraic Equations�
Gennady Yu. Kulikov1 and Alexandra A. Korneva1
Ulyanovsk State University, L. Tolstoy Str. 42, 432700 Ulyanovsk, Russia

Abstract. In the paper we study how to integrate numerically largescale systems of semi-explicit index 1 diﬀerential-algebraic equations by
implicit Runge-Kutta methods. In this case, if Newton-type iterations
are applied to the discrete problems we need to solve high dimension
linear systems with sparse coeﬃcient matrices. Therefore we develop an
eﬀective way for packing such matrices of coeﬃcients and derive special
Gaussian elimination for parallel factorization of nonzero blocks of the
matrices. As a result, we produce a new eﬃcient procedure to solve linear
systems arising from application of Newton iterations to the discretizations of large-scale index 1 diﬀerential-algebraic equations obtained by
implicit Runge-Kutta methods. Numerical tests support theoretical results of the paper.

1

Introduction

In this paper we deal with index 1 diﬀerential-algebraic systems of the form
�
�
x� (t) = g x(t), y(t) ,
(1a)
�
�
y(t) = f x(t), y(t) ,

(1b)

x(0) = x0 , y(0) = y 0 .

(1c)

→R ,f :D⊂R
→
where t ∈ [0, T ], x(t) ∈ R , y(t) ∈ R , g : D ⊂ R
Rn , and where initial conditions (1c) are consistent; i. e., y 0 = f (x0 , y 0 ). Note
that we consider only autonomous systems because any nonautonomous system
may be converted to an autonomous one by introducing a new independent
variable.
To solve problem (1) numerically, we apply an l-stage implicit Runge-Kutta
(RK) method given by the table
m

n

m+n

c A
bT
�

m

m+n

,

This work was supported in part by the Russian Foundation of the Basic Research
(grants No. 01-01-00066 and No. 00-01-00197).

V.N. Alexandrov et al. (Eds.): ICCS 2001, LNCS 2073, pp. 832–841, 2001.
c Springer-Verlag Berlin Heidelberg 2001
�

On Eﬃcient Application of Implicit Runge-Kutta Methods

833

where A is a real matrix of dimension l × l, b and c are real vectors of dimension
l, to problem (1) and obtain the following discrete problem:
xki = xk + τ

l
�

aij g(xkj , ykj ),

(2a)

i = 1, 2, ..., l,

(2b)

bi g(xki , yki ),

(2c)

k = 0, 1, ..., K − 1,

(2d)

j=1

yki = f (xki , yki ),
xk+1 = xk + τ

l
�
i=1

yk+1 = f (xk+1 , yk+1 ),
0

x0 = x ,

y0 = y

0

(2e)

where τ is a stepsize. Algebraic system (2) is solved then by an iterative method.
Usually the iterative process is taken in the form of simple or Newton-type
iterations with trivial (or nontrivial) predictor [1], [4], [7], [8], [10], [12].
Newton (or modiﬁed Newton) iterations are more preferable for solving diﬀerential-algebraic equations (1) than simple ones. First, RK method (2) is convergent in the case of simple iterations only if the inﬁnite norm of the Jacobian
of the right-hand part of (1b) is bounded on a convex set containing the exact
solution of the original problem by a constant d < 1 (see theorem 2 in [8] and
theorem 3 in [10]). In the case of Newton-type iterations it is enough to satisfy
the implicit function theorem for algebraic part (1b) in order to obtain convergent numerical methods (see theorems 3, 4 in [8] and theorems 1, 2 in [10]). The
last condition holds obviously for any suﬃciently smooth diﬀerential-algebraic
system (1) of index 1. This follows from [2], [3] and [6]. Second, when using
Newton (full or modiﬁed) method we can limit ourselves by a less number of
iterations to attain the order of the underlying RK formula. Moreover, if we
apply an RK method of suﬃciently high stage order with appropriate nontrivial
predictor to problem (1) then we can carry out only 2 iterations per time point
to get the maximum order convergence [10].
The only drawback of Newton iterations is severe requirements on RAM and
CPU time caused by the increasing dimension of discrete problem (2) in l times
when an l-stage implicit RK method is used. Thus, it is necessary to solve linear
systems of dimension (m + n)l many times during the integration. Therefore the
basic problem is how to simplify the numerical solving of the linear systems with
coeﬃcient matrices of the special form arising from the application of Newton
methods (full or modiﬁed) to diﬀerential-algebraic system (1).
In the paper we use the special structure of the matrices mentioned above to
obtain a modiﬁcation of Gaussian elimination. This modiﬁcation allows RAM
and CPU time to be signiﬁcantly reduced in the numerical integration of problem
(1) by an implicit RK method. We give the estimates of such reduction both in
theory and in practice.
Finally, we study how to integrate numerically large-scale systems of diﬀerential-algebraic equations of index 1. In this case we need to solve high dimension

834

G.Y. Kulikov and A.A. Korneva

linear systems with sparse coeﬃcient matrices. We develop ﬁrstly an eﬀective way
for packing such matrices of coeﬃcients. After that we derive special Gaussian
elimination for parallel factorization of nonzero blocks of the matrices. Thus, we
produce a new eﬃcient procedure to solve linear systems arising from application
of Newton iterations to the discretizations of large-scale index 1 diﬀerentialalgebraic systems obtained by implicit RK methods. We also give numerical
tests which support the theoretical results.

2

Eﬃcient Implementation of Iterative Runge-Kutta
Methods

In the preceding section we have shown that Newton iterations are eﬀective to
solve semi-explicit index 1 diﬀerential-algebraic equations numerically. The basic
part of the Newton method applied to discrete problem (2) consists of solving
linear systems of the form
i−1
i−1
i−1
i
)(Zk+1
− Zk+1
) = F̄kτ Zk+1
∂ F̄kτ (Zk+1

(3)

where lower indices mean time points, and upper ones denote iterations. Here
�T
def �
def
Zk+1 = (zk1 )T , ..., (zk,l−1 )T , (zkl )T
∈ R(m+n)l , where the vector zkj =
�
�
T
(xkj )T , (ykj )T
∈ Rm+n , j = 1, 2, . . . , l, unites components of the j-th stage
value of l-stage RK formula (2). The mapping F̄kτ is the nontrivial part of disi−1
) denotes
crete problem (2) for computing the stage values Zk+1 , and ∂ F̄kτ (Zk+1
i−1
τ
the Jacobian of the mapping F̄k evaluated at the point Zk+1 .
i−1
) has the block structure
It is easy to see that the matrix ∂ F̄kτ (Zk+1
 ∂ F̄ τ (Z) 
k

1

 ∂ F̄kτ (Z)2 

∂ F̄kτ (Z) = 
..


.
τ
∂ F̄k (Z)l
def

(4)

where each block ∂ F̄kτ (Z)j , j = 1, 2, . . . , l, is an (m + n) × (m + n)l-matrix of
the form
 O(τ ) · · · O(τ ) 1 + O(τ ) · · · O(τ ) O(τ ) · · · O(τ ) O(τ ) · · · O(τ )  �
.. 
..
..
..
..
..
..
..
..
..
..
m
 ...
.
.
.
.
. 
.
.
.
.
.
.

 O(τ ) · · · O(τ ) O(τ ) · · · 1 + O(τ ) O(τ ) · · · O(τ ) O(τ ) · · · O(τ ) 


 0 ··· 0
z
···
z
z
···
z
0
···
0 
� .
 .
..
.. 
..
..
..
..
..
..
..
..
..
 .
.
0

�

.
···

.
0

��

�

(m+n)(j−1)

.
z

�

.
···

��

.
z

.
z

�

�

m

Here z means in general a nontrivial element.

.
···

.
z

��

�

n

.
0

.
···

.
0

� �� �

(m+n)(l−j)

n

On Eﬃcient Application of Implicit Runge-Kutta Methods

835

Having used the structure of matrix (4), Kulikov and Thomsen suggested
in [7] to exclude the zero blocks situated below the main diagonal from LUfactorization and forward substitution. That reduces the number of arithmetical
operations and, hence, CPU time signiﬁcantly when linear system (3) is solved.
However, the advantage of the new version of Gaussian elimination becomes
greater if we have interchanged the x-components of the vector Zk+1 with the
corresponding y-components. It means that we have interchanged the ﬁrst m
rows with the last n ones in each submatrix ∂ F̄kτ (Z)j . In this case we can exclude
all the zero blocks of matrix (4) from the LU-factorization, forward and backward
substitutions and solve problem (3) very eﬀectively [9].
Let us now compare eﬃciency of all the versions of Gaussian elimination for
linear system (3). To do this, we compute and compare the total number of
multiplications and divisions for each version when m = n = 2 and l = 1, 2, 3, 4.
It is well-known that the ordinary Gauss method requires
�
��
�2
(m + n)l (m + n)l + 3(m + n)l − 1
(5)
3
operations of multiplication and division (see, for example, [13]). The total number of operations for Kulikov and Thomsen’s version (Modiﬁcation I) can be
easily obtained from [7]
�
�
�
�
(m + n)l − 1 (m + n)l 2(m + n)l − 1
6
�
�
(m + n)l ml + n − 1
n(m + n)2 (l − 1)l(2l − 1)
−
+
2
6

�

(m + n)l

�2

+

−

(6)

n(m + n)(m + n − 1)l(l − 1)
.
4

The formula
�
�
(m + n − 1)(m + n) 2(m + n) − 1 l
(m + n)l(ml + n) +
6
+

m(m + n)(2m + n)l(l − 1) m2 (m + n)(l − 1)l(2l − 1)
+
4
6
+

(7)

n(n − 1)l m(m − 1)l
+
+ mnl
2
2

gives the number of multiplications and divisions for Kulikov and Korneva’s
version (Modiﬁcation II) [9].
Now we substitute m = n = 2 and l = 1, 2, 3, 4 in formulas (5)–(7) and obtain
Table 1. We see from the table that Modiﬁcation II is the most preferable to be
used even in such a low dimension case. For example, this version of Gaussian

836

G.Y. Kulikov and A.A. Korneva
Table 1. Total numbers of multiplications and divisions when m = n = 2
Number of Gauss Modiﬁcation Modiﬁcation
stages, l method
I
II
1
36
36
36
2
232
180
128
3
716
496
308
4
1616
1048
608
Table 2. RAM (in bytes) needed to store matrix (4) when m = n = 2
Number of
stages, l
1
2
3
4

Gauss Modiﬁcation Modiﬁcation
method
I
II
128
128
128
512
448
384
1152
960
768
2048
1664
1280

elimination requires roughly three times less operations for 4-stage implicit RK
formulas than the ordinary Gauss method. It is also better than Modiﬁcation I.
The advantage of this method will obviously increase for a higher dimension
diﬀerential-algebraic systems (1).
Except CPU time, it is important to compare RAM needed to store matrix
(4). If we assume that the type double is used to store elements of the matrix then Table 2 gives such information. These data also conﬁrm advantage of
Modiﬁcation II over other methods.

3

Large-Scale Systems

In Section 2 we presented the eﬃcient way of applying implicit RK methods
to problem (1). Unfortunately, it is not enough when we solve large-scale semiexplicit index 1 diﬀerential-algebraic systems. As an example, we may take the
model of overall regulation of body ﬂuids [5]. This model is a problem of the
form (1) containing about two hundred variables. Having applied any implicit
3- or 4-stage RK method to the model we encounter the situation when the
dimension of discrete problem (2) is too high to solve it by Newton iterations.
On the other hand, the Jacobian of a large-scale diﬀerential-algebraic system
is often a sparse matrix. For instance, the model mentioned above is the case.
Thus, we must solve two problems in this section. The ﬁrst problem is how to
pack a matrix of the form (4) excluding all trivial elements. The second one is
how to implement Modiﬁcation II of Gaussian elimination for the packed matrix
(4) eﬃciently.

On Eﬃcient Application of Implicit Runge-Kutta Methods

837

First of all we rearrange the variables and deﬁne the vectors:
�T
def �
Xk+1 = (xk1 )T , ..., (xk,l−1 )T , (xkl )T
∈ Rml ,
�T
def �
Yk+1 = (yk1 )T , ..., (yk,l−1 )T , (ykl )T
∈ Rnl .
�T
def �
Now Zk+1 = (Xk+1 )T , (Yk+1 )T
and matrix (4) has the form
�
�
∂ F̄kτ (Z)Y
def
∂ F̄kτ (Z) =
.
∂ F̄kτ (Z)X

(8)

Here each submatrix has also the block structure
 ∂ F̄ τ (Z)Y 

∂ F̄kτ (Z)

1
k
τ
Y
∂
F̄
(Z)

2
k
Y def 

= 

..
.
∂ F̄kτ (Z)Yl

 ∂ F̄ τ (Z)X 
1

k





and ∂ F̄kτ (Z)X

∂ F̄kτ (Z)X
2 
def 

= 
..


.
∂ F̄kτ (Z)X
l

where

def
∂ F̄kτ (Z)Yi =

0
 ...
0

�

···
..
.
···

��

0
..
.

z
..
.

0

z

� �

···
..
.
···

��

z

�

n

n(i−1)

z
..
.

0
..
.
0

�

···
..
.
···

0
..
.

z
..
.

0

z

��

�

n(l−i)+m(i−1)

�

···
..
.
···

��

0
..
.

z
..
.

0

z

� �

m

···
..
.
···


0 �
.. 
n
.
0

��

�

m(l−i)

and
O(τ )
def
=  ...
O(τ )



∂ F̄kτ (Z)X
i

�

···
..
.
···

O(τ )
..
.
O(τ )

��
nl+m(i−1)

�

1 + O(τ )
..
.
O(τ )

···
..
.
···

�

��
m

O(τ )
..
.
1 + O(τ )

�

O(τ )
..
.
O(τ )

�

···
..
.
···

��

O(τ ) �
..  m
.
O(τ )



�

m(l−i)

We note that, when solving linear system (3) with matrix (8) by Modiﬁcation II of Gaussian elimination, LU-factorization of any submatrix ∂ F̄kτ (Z)Yi
does not inﬂuence the submatrices ∂ F̄kτ (Z)Yj for j �= i. This means that the
factorization of the matrix ∂ F̄kτ (Z)Y falls into l independent LU-factorizations
of the submatrices ∂ F̄kτ (Z)Yi , i = 1, 2, . . . , l. Moreover, structures of the similar
nonzero blocks of all the submatrices ∂ F̄kτ (Z)Yi (i.e., the number and the places
of nonzero elements) coincide, if the stepsize τ is suﬃciently small.
Taking into account the above observation we conclude that the storage of
nonzero elements of the matrix ∂ F̄kτ (Z)Y has to give simultaneous access to all
elements corresponding components of the stage values with the same subscript.
It allows for the parallel factorization of the matrix ∂ F̄kτ (Z)Y . To provide this,
we will store the matrix ∂ F̄kτ (Z)Y in the form of an array of links to chained
lists. Every component of the list representation consists of:
– an l dimension array to store elements of the matrix ∂ F̄kτ (Z)Y corresponding
components of the stage values with the same subscript (fij );

838

G.Y. Kulikov and A.A. Korneva

✲ f11

1

r1

✲

..
.

2

···

n

ri1

✲

rin

✲

..
.
fi 1 l

f1l
..
.

✲ fi1 1

..
.

✲ fn1
..
.
fnl

rn

✲

···

✲ fin 1
..
.
fin l

Fig. 1. The storage of nonzero elements of the matrix ∂ F̄kτ (Z)Y .

– the subscript of components of the stage values (ri );
– a link to the next element of the list (or to nil).
Since the dimension of the matrix ∂ F̄kτ (Z)Y is equal to nl we need n such lists
(see Fig. 1).
To store the matrix ∂ F̄kτ (Z)X , we can use any packing appropriate for sparse
matrices in a general case because the matrix of coeﬃcients of the RK method
may have zero elements and, hence, ∂ F̄kτ (Z)X also contains zero blocks. Fortunately, we will have great advantage by applying only RK methods with dense
coeﬃcient matrix A (i.e., aij �= 0, i, j = 1, 2, . . . , l) to problem (1). We call such
methods dense RK methods. For example, Gauss methods are dense RK methods
(see [4]). And there exist no dense methods among explicit RK formulas.
It is easy to see that the matrix ∂ F̄kτ (Z)X can be obtained by pairwise products of elements of the matrices τ A and ∂g l (Z) where the m × (m + n)l-matrix
�
def �
∂g l (Zk+1 ) = ∂yk1 g(zk1 ) . . . ∂ykl g(zkl ) ∂xk1 g(zk1 ) . . . ∂xkl g(zkl ) .
(9)
Therefore if we have applied a dense RK formula to problem (1) then all the
τ
X
have the same
submatrices ∂ F̄kτ (Z)X
i , i = 1, 2, . . . , l, of the matrix ∂ F̄k (Z)
structure, maybe, except the diagonal elements. Moreover, the structure of any
l
submatrix ∂ F̄kτ (Z)X
i and the structure of the matrix ∂g (Z) coincide with the
exception mentioned above. Then all nonzero elements of the matrix ∂ F̄kτ (Z)X
can be reconstructed by the matrix τ A and nonzero elements of the matrix
∂g l (Z). Thus, if the stage number l > 1, and m, n are large, it is better to store
two matrices of dimensions l × l and m × (m + n)l instead of one matrix of
dimension ml × (m + n)l.
Now we discuss Gaussian elimination for system (3) with matrix (8). It was
noted earlier that the elimination of variables from system (3) is splitted up

On Eﬃcient Application of Implicit Runge-Kutta Methods

839

into two stages. We eliminated the y-components, by using the parallel factorization of the submatrices ∂ F̄kτ (Z)Yi , i = 1, 2, . . . , l, on the ﬁrst stage. Then we
eliminated the x-components. The ﬁrst stage is more important for optimization
because the most part of arithmetical operations falls on it. Therefore we give
further a way to decrease the number of operations at this stage.
Let us consider the reduced matrix
�
�
∂ F̄kτ (Z)Y
(10)
∂g l (Z)
of dimension (nl + m) × (m + n)l. The next theorem establishes that we can
use the reduced matrix (10) instead of the full matrix (8) while eliminating the
y-components. That is more preferable for us.
Theorem 1. Let a dense l-stage RK formula with coeﬃcient matrix A be used
�(µ)
�
obtained after the
for constructing matrix (8). Then the matrix ∂ F̄kτ (Z)X
µ-th step of Gaussian elimination can be reconstructed uniquely by pairwise prod�
�(µ)
ucts of elements of the matrices τ A and ∂g l (Z)
when 0 ≤ µ ≤ nl.
The proof of theorem 1 will appear in [11]. Thus, taking into account this
theorem we use the lower dimension matrix (10) on the ﬁrst nl steps of the
�(nl)
�
by the
Gaussian elimination. After that we reconstruct the matrix ∂ F̄kτ (Z)X
� l
�(nl)
matrices τ A and ∂g (Z)
and proceed the elimination of the x-components of
system (3). However, we must remember the parallel factorization of the matrix
∂ F̄kτ (Z)Y . By this reason, we have also to store nonzero elements of the matrix
∂g l (Z) given in (9) by the packing suggested for the matrix ∂ F̄kτ (Z)Y (see Fig. 1).
The only diﬀerence is the number of the chained lists that are necessary to store
the matrix ∂g l (Z). In this case we use m such lists.

4

Numerical Example

To compare all the versions of Gaussian elimination presented in this paper, we
take the model of overall regulation of body ﬂuids mentioned above as a test
problem. The version from Section 3 is called further Modiﬁcation III. To solve
the problem in the interval [0, 10], we apply Gauss-type implicit RK methods up
to order 8 with the stepsize τ = 1/60 and fulﬁl two Newton iterations per time
point. Table 3 contains execution time (in sec.) for all the versions of Gaussian
elimination. Lines in the table mean that the Jacobian of the discrete problem
exceeds the available RAM.
This practical example shows that Modiﬁcation III is the best method to
solve linear problems arising in application of implicit RK formulas to large-scale
systems of semi-explicit index 1 diﬀerential-algebraic equations. Indeed, we see
from Figure 2 that the Jacobian of the model of overall regulation of body ﬂuids
is a sparse matrix (points mean nonzero elements), and Modiﬁcation III operates
only with the nonzero elements. Also it is important to note that the growth of

840
..

G.Y. Kulikov and A.A. Korneva

.. ..
..
..

..
.

.
.

.

.

.

.

.

. .

..

..

..

.

.

.

.

.

..
. . . ..
.
. . .
.
.. .
.

.
..
.
.

..
.

.. .

..

..

.

.

.

.
.
.

.

.

.

.

.
.

..
...
..
.
...

.
.

..
..

.

.
.

...
..

.
.

..
.
.

.

. .

..

..
..

.

..

..

. ..

.

..

..

.

.
.

.

.

.

.

. .
. . . ..
.
. . .
.
.. . .. . .
.. .
. .
. ..
..
.
.

.

.

...
.
.

.

.
.
. ..
..
.

..

.
.

.

.

. ...
..

..
.
.
.
..

.
..
.

.
.

..

..
..

.

.. .
.

..

.
.
.. . ...
.
. ..
. .. .. .. .
. .
.
.
.. . . .
.
.
.

.
.
. .
. . .

..

Fig. 2. The structure of the Jacobian of the model of overall regulation of body ﬂuids.

Table 3. Execution time (in sec.) for the processor Intel Pentium 200
Number of
stages, l
1
2
3
4

Gauss Modiﬁcation Modiﬁcation Modiﬁcation
method
I
II
III
1169.85
993.05
983.77
51.80
9271.15 4146.33
2551.29
215.80
—
—
—
679.71
—
—
—
763.90

On Eﬃcient Application of Implicit Runge-Kutta Methods

841

the execution time decreases with the growth of the number of stages in implicit
RK formulas. It is a good result to apply implicit RK formulas of high order in
practice.

References
1. Ascher, U.M., Petzold, L.P.: Computer methods for ordinary diﬀerential equations
and diﬀerential-algebraic equations. SIAM, Philadelphia, 1998
2. Gear, C.W., Petzold, L.R.: ODE methods for the solution of diﬀerential/algebraic
systems. SIAM J. Numer. Anal. 21 (1984) 716–728
3. Gear, C.W.: Diﬀerential-algebraic equations index transformations. SIAM J. Sci.
Stat. Comput., 9 (1988) 39–47
4. Hairer, E., Wanner, G.: Solving ordinary diﬀerential equations II: Stiﬀ and
diﬀerential-algebraic problems. Springer-Verlag, Berlin, 1991
5. Ikeda, N., Marumo, F., Shiratare, M., Sato, T.: A model of overall regulation of
body ﬂuids. Ann. Biomed. Eng. 7 (1979) 135–166
6. Kulikov, G.Yu.: The numerical solution of the autonomous Cauchy problem with an
algebraic relation between the phase variables (non-degenerate case). (in Russian)
Vestnik Moskov. Univ. Ser. 1 Mat. Mekh. (1993) No. 3, 6–10; translation in Moscow
Univ. Math. Bull. 48 (1993) No. 3, 8–12
7. Kulikov, G.Yu., Thomsen, P.G.: Convergence and implementation of implicit
Runge-Kutta methods for DAEs. Technical report 7/1996, IMM, Technical University of Denmark, Lyngby, 1996
8. Kulikov, G.Yu.: Convergence theorems for iterative Runge-Kutta methods with
a constant integration step. (in Russian) Zh. Vychisl. Mat. Mat. Fiz. 36 (1996)
No. 8, 73–89; translation in Comp. Maths Math. Phys. 36 (1996) No. 8, 1041–1054
9. Kulikov, G.Yu., Korneva, A.A.: On eﬀective implementation of iterative RungeKutta methods for diﬀerential-algebraic equations of index 1. (in Russian) In: Basic
problems of mathematics and mechanics. 3 (1997), Ulyanovsk State University,
Ulyanovsk, 103–112
10. Kulikov, G.Yu.: Numerical solution of the Cauchy problem for a system of
diﬀerential-algebraic equations with the use of implicit Runge-Kutta methods with
nontrivial predictor. (in Russian) Zh. Vychisl. Mat. Mat. Fiz. 38 (1998) No. 1, 68–
84; translation in Comp. Maths Math. Phys. 38 (1998) No. 1, 64–80
11. Kulikov, G.Yu., Korneva, A.A.: On numerical solution of large-scale systems of
index 1 diﬀerential-algebraic equations. (in Russian) Fundam. Prikl. Mat. (to appear)
12. Kværnø, A.: The order of Runge-Kutta methods applied to semi-explicit DAEs of
index 1, using Newton-type iterations to compute the internal stage values. Technical report 2/1992, Mathematical Sciences Div., Norwegian Institute of Technology,
Trondheim, 1992
13. Samarskiy, A.A., Gulin, A.V.: Numerical methods. Nauka, Moscow, 1989

