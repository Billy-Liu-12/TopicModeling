Optimizing Register Spills for Eager Functional
Languages
S. Mishra1 , K. Sikdar1 , and M. Satpathy2
1

Stat-Math Unit, Indian Statistical Institute, 203 B. T. Road, Calcutta-700 035
Dept. of Computer Science, University of Reading, Reading RG6 6AY, UK
(res9513, sikdar)@isical.ac.in, M.Satpathy@reading.ac.uk

2

Abstract. Functional programs are referentially transparent in the
sense that the order of evaluation of subexpressions in an expression
does not matter. Any order of evaluation leads to the same result. In
the context of a compilation strategy for eager functional languages, we
discuss an optimization problem, which we call the Optimal Call Ordering Problem. We shaow that the problem is NP-complete and discuss
heuristics to solve this problem.

Keywords: NP-Completeness; Functional Languages; Register Utilization

1

Introduction

Functional programs are referentially transparent [3]. This means, the value of
an expression never changes throughout its computational context. As a consequence, we can evaluate an expression in any order. Consider the expression
f (x, y) + g(x, y). If it is an expression in a functional program, the values of x
and y remain the same throughout the computation of the expression. So, we
can do any of the following to get the ﬁnal result: (i) ﬁrst make the call to f ,
then make the call to g and add their results, or alternatively, (ii) ﬁrst evaluate
the call to g, then the call to f and add their results.
So far as evaluation order is concerned, functional languages are divided
into eager functional languages and the lazy functional languages. In an eager
functional language, the arguments of a function are evaluated before the call
to the function is made, whereas, in a lazy functional language, an argument
of a function is evaluated only if it is needed. If two operands are present in
machine registers, operations like additions and subtraction usually take one
machine cycle. On the other hand, if the operands are present in memory, they
are ﬁrst brought to registers, and then the addition or the subtraction operation
is performed. Thus memory operations are usually far more costlier than the
register operations. So it is always preferable that, as much operations as possible
should be done in registers. But it is not always possible since the number of
registers is small, and the number of values which are live at a given point of
time could be high [2]. Further, function calls and programming constructs like
recursion complicate the matter. When we cannot accommodate a live value in
V.N. Alexandrov et al. (Eds.): ICCS 2001, LNCS 2074, pp. 128–137, 2001.
c Springer-Verlag Berlin Heidelberg 2001
�

Optimizing Register Spills for Eager Functional Languages

129

a register, we need to store it in memory. Storing a register value in memory and
retrieving it later to a register constitute a register spill or simply a spill. It is
desired that a program should be computed with minimum number of spills.
In this paper, we will discuss an optimization problem which aims at minimizing the number of register spills in the context of a compilation strategy for
eager functional programs. Section 2 introduces the problem. Section 3 discusses
the context of the problem. Section 4 shows that the problem is NP-complete.
In Section 5, we discus about heuristics for this problem. Section 6 concludes
the paper.

2

Our Problem

We make the following assumptions: (i) The language is an eager functional
language, and (ii) a function call may destroy the contents of the live registers.
The later assumption is necessary because we shall have to deal with recursion.
We now illustrate our problem through an example.
Example 1: Figure 1 shows the directed acyclic graph (DAG) for the following
expression. The arguments of calls F1 and F2 share subexpressions.
F1 (1 + (x ∗ (2 − (y ∗ z)))) + F2 ((x ∗ (2 − y ∗ z)) + ((2 − y ∗ z) + (y ∗ z/3)))
We can evaluate calls F1 and F2 in any order. For evaluating F1 , we need to
evaluate the argument of F1 and hence the expression DAG enclosed within
LP2. Similarly, before making the call to F2 , we have to evaluate the argument
of F2 which is the expression DAG enclosed within LP3. LP2 and LP3 have
many shared subexpressions between them. Now let us see the relative merits
and demerits of the two possible orders of evaluation. Note that we are only
considering the number of spills which result due to the shared nodes between
the argument DAGs of F1 and F2 .

LP1

LP2

+
F1

F

+

+

2

3

LP3

*
x

+

1

2

−
2

/

3
3

*
y

F2

F1

1

1

z

Fig. 1. Sharing between the arguments of two calls F1 and F2

130

S. Mishra, K. Sikdar, and M. Satpathy

– Evaluation of F1 is followed by F2 : We have to ﬁrst evaluate LP2 . Then the
call to F1 will be made. Next, LP3 will be evaluated and it will be followed by
the call to F2 . Observe that, during the evaluation of the DAG within LP 2 ,
it will evaluate three shared computations, marked by circled 1, 2 and 3 in
the ﬁgure. Immediately after the evaluation of LP2 , they are in registers. But
following our assumptions, the call to F2 may destroy them. So when LP3
will be evaluated they may not in the registers in which they got evaluated.
So we need to store them in memory. In conclusion, we need to make 3 spills.
– Evaluation of F2 is followed by F1 : We have to ﬁrst evaluate LP3 . Then
the call to F2 will be made. After that LP2 will be evaluated and it will be
followed by the call to F1 . Observe that, during the course of LP3� s evaluation,
it will evaluate only one shared computations, marked by circled 1 in the
ﬁgure, which will be needed by LP2 . So in memory we need to store its value
and retrieve it when LP2 needs it. In conclusion, we will make 1 spill.
We observed that the second approach is more eﬃcient than the ﬁrst approach
since we need to make lesser number of spills. Note that we are not just saving
two or three spills. Such calls may occur in a recursive environment. In such
a case the number of spills that we save by choosing a better evaluation order
could be very very large. So, choosing the better evaluation order is important.

3

The Problem Context

For computing an arithmetic expression, it is usually represented as a DAG [2].
Such a DAG is simple in the sense that the interior nodes are machine operators and the leaf-nodes are either literals or memory locations. The problem of
computing a simple expression DAG with minimum number of instructions (or
minimum number of registers such that no spill occurs) is N P -complete [1].
Aho, Johnsson and Ullman have discussed various heuristics for computing an
expression DAG. One such heuristic is called the top down greedy and we will refer it as the AJU algorithm in our discussion. Satpathy et al. [8] have extended
the AJU algorithm to generate code for an expression in an eager functional
language. The DAG that can represent a generalized expression in a functional
language may have function calls or if-expressions as interior nodes. Figure 2
shows one such DAG. A node representing a function call or an if-expression is
termed as a non-linear node. The strategy as mentioned in [8] partitions a generalized expression DAG into linear regions. The linear regions are the maximal
regions of a DAG such that its interior nodes are all machine operators, and
their leaves could be non-linear nodes. Figure 2 shows the linear regions as the
regions in dashed lines.
A strategy for evaluating a generalized expression DAG could be as follows.
Here, for simplicity, let us assume that the DAG has function calls as the only
non-linear nodes. Our analysis remains valid in presence of if-expressions but
their presence will make the analysis look complicated.
Step 1: Partition the DAG into linear regions. [8] discusses such a partitioning
algorithm.

Optimizing Register Spills for Eager Functional Languages

131

Step 2: Select any linear region that lies at the leaf-level of the DAG (such a
linear region do not have any non-linear node at any of its leaves). Evaluate
this non-linear region using the AJU algorithm and store its result in memory. Now replace this non-linear region in the DAG by a leaf node labeled
with the above memory location.
Step 3: Evaluate all function calls whose arguments are available in memory
locations. Replace the calls in the DAG by their results in memory locations.
Step 4: Continue steps 2 and 3 till the result of the original DAG is available
in a single memory location.
Alternatively, we can express the above strategy as follows:
– First evaluate all the nonlinear nodes lying at the leaves of a linear region one
by one and spill their results. Then evaluate rest of the linear region using
the AJU algorithm assuming that the results of all the nonlinear nodes at
the leaves are in memory.
Note that when all the non-linear nodes of a linear region are already evaluated, their results are in memory. So, at this point of time, the linear region
under evaluation is a simple expression DAG and so it could be evaluated using
the AJU algorithm. Further notice that, this is a bottom-up strategy. We elaborate it through the evaluation of the DAG in ﬁgure 2. In the ﬁgure, the linear
regions LR1 till LR4 are shown as the regions in dotted lines. For evaluating
such a DAG, evaluation always starts with a linear region such that none of its
leaf-level nodes is a non-linear node. In the present ﬁgure, LR2 and LR4 are the
only candidates for evaluation. We can evaluate any of them ﬁrst. Let us choose
to evaluate LR4 (the argument of function J). After evaluation of LR4 (its result
is in memory), a call to J is made since it satisﬁes the step 3 of the evaluation
strategy. Let the result of J be stored in memory location m2 . Now LR3 is one
linear region whose only non-linear node is available in memory. So LR3 could
be evaluated by using AJU algorithm. After the evaluation of LR3, a call to
H is made and let us store its result in m3 . Next the linear region which is a
candidate for evaluation is LR2. It is evaluated and a call to G is made. Let the
result of this call be stored in m1 . At this stage, LR1 could be evaluated using
AJU algorithm since the results of all of its non-linear nodes are in memory. Let
us store the result of LR1 in location m0 . And this the desired result.
3.1

Our Problem in General Form

In Figure 3, function calls F1 , F2 , . . . , Fn lie at the leaf-levels of the linear region
LP. DAG1 ,..., DAGn respectively represent the arguments of such calls. For simplicity, we have assumed that all such functions are single argument functions.
Further, we can assume that there are no non-linear nodes present in such argument DAGs (if they are present, they will have already been evaluated and
their results will be residing in memory). So, all such argument DAGs are simple
expression DAGs and they could be evaluated using the AJU algorithm in any
order.

132

S. Mishra, K. Sikdar, and M. Satpathy
m0 +

LR1

y
+

m1

m3

G

H
LR3

+

LR2

+

m2
x

1

J

+

y

+
1

LR4
1

z

Fig. 2. Evaluation of a generalized DAG through a bottom-up strategy

F1 , F2 , . . . , Fn will induce, due to the sharing between the argument DAGs,
certain number of spills which are as discussed in the previous section. So now
the problem is: how to choose an evaluation order of the calls to F1 , ..., Fn such
that the number of spills due to the sharing among argument DAGs is minimum.
From now on we will refer to this problem as the optimal evaluation ordering
problem (OEOP). Formally, OEOP can be deﬁned as follows:
Instance : A linear region whose leaves have function calls F1 , F2 , . . . , Fn at its
LP

F1

DAG1

F2

DAG2

F3

Fn

DAG3

DAGn

Fig. 3. Sharing between arguments of n calls F1 , F2 , . . . , Fn

leaf-level, and the calls respectively have DAG1 , ..., DAGn as argument DAGs.
All such DAGs are simple expression DAGs (refer to Figure 3).
Solution : A permutation π = (π1 , π2 , . . . , πn ) of (1, 2, . . . , n).
Cost : w(π) = number of shared nodes to be spilled to evaluate n function calls
(due to sharing between the argument DAGs) in the order Fπ1 , Fπ2 , . . . , Fπn .
Goal: Minimize the cost function.

Optimizing Register Spills for Eager Functional Languages

3.2

133

Assumptions and Problem Formulation

To analyse the OEOP, we will make the following simplifying assumptions.
– All the functions are of one arguments: This is to make the analysis simpler.
– Assumption of simple-sharing: An expression is shared between at most two
function DAGs. Further, if a subexpression is shared by two calls F i and Fj
then no subexpression of it is shared by any function Fk (k � =i, j).
+
+

F3

F2

F1

+

+
1

+

4
3

1

2

Fig. 4. DAGs of A, B and C not satisfying the assumption of simple sharing

In Figure 4, the node marked 1 is shared by the three functions A, B and C. Let
B be evaluated before C. Now, whether B will spill node 1 for C depends on
whether A has been evaluated already or not (i.e. whether A has spilled the node
1 or not). In other words, the spilling decisions between B and C depends on the
context in which they are evaluated. To make such spilling decision between any
two function calls independent of the context in which they are evaluated, we
have introduced the assumption of simple sharing. We shall denote the OEOP
satisfying above assumptions as OEOP(S). Obviously, OEOP(S) is in class NP.

4

NP-Completeness of OEOP

Let Figure 3 represent an instance of OEOP(S). Given n calls as in the ﬁgure, we
can obtain a weighted symmetric digraph G = (Vn , A, w) which will preserve all
the sharing information. In the digraph, the node set Vn = {1, 2, . . . , n}, where
node i corresponds to the function Fi . There will be a directed edge from node
i to node j with weight wij if the argument DAG of Fi cuts the argument DAG
of Fj at wij points. What it means is that if the call to Fi is evaluated ahead
of the call to Fj , then wij shared nodes will have to be spilled to memory. Note
that the weight of the edge from i to j may be diﬀerent from the weight of the
edge from j to i. For instance, if we construct a digraph for the functions A and
B in the ﬁgure 1, then there will be an edge from node A to B with weight 3
and the reverse edge will have weight 1.
Let us evaluate the calls F1 , . . . Fn in the ﬁgure in the order Fπ1 , Fπ2 . . . Fπn ,
where π = (π1 , . . . πn ) is a permutation of (1, 2, . . . n). Then the number of spills

134

S. Mishra, K. Sikdar, and M. Satpathy

we will have to make immediately after the evaluation of Fπ1 is the sum of the
weights of all the out-going edges from π1 . The number of spills we will make
immediately after the evaluation of Fπ2 is the sum of weights of the out-going
edges from π2 to all nodes excepting π1 . This is because Fπ1 has already been
evaluated, and π2 should not bother about Fπ1 . Continuing in this manner,
the number of spills we will make after the evaluation of Fπj is the sum of
weights of all outgoing edges from πj other than the edges to π1 , π2 , . . . πj−1 .
Let us call the number of spills that are made after the evaluation of F πi be
SPILLi . So the evaluation order Fπ1 , Fπ2 , . . . Fπn will make it necessary to make
SPILL(Fπ1 , Fπ2 , . . . Fπn ) = SPILLπ1 + SPILLπ2 + ... + SPILLπn spills. Note
that the value of SPILLπn will be zero since after its evaluation we need not
make any spill. To ﬁnd out the minimum number of spills we shall have to
ﬁnd out the value of SPILL(Fπ1 , Fπ2 , . . . , Fπn ) for all possible evaluation order
Fπ1 , Fπ2 , . . . , Fπn and take the minimum of these values.
F2

3
2

4

4

2

F1
4

5

F4

F3
1

Fig. 5. Graph showing sharing information of an instance of OEOP(S).

Example: Let A, B, C and D be functions satisfying the simple sharing assumption (Figure 5). Let the evaluation order be B, D, C, and A. Then:
SPILL(B, C, D, A) = SPILLB + SPILLC + SPILLD + SPILLA = 13.
We shall convert this arc weighted symmetric digraph G to a nonnegative arc
weighted complete digraph Gn = (Vn , An , w) by adding new arcs to G with zero
arc weights.
Lemma 1. The number of spills in the complete graph Gn due to the evaluation
order Fπ1 , Fπ2 , . . . , Fπn is same as the number of spills in G (the n function calls
with sharing as represented by G) under the same evaluation order.
�
For computing the total number of spills for the order Fπ1 , Fπ2 , . . . , Fπn ; the
weights of the arcs of the form (πi , πj ) with j > i are considered in deciding
on the number of spills. It is easy to see that these arcs corresponding to the
above evaluation order form an acyclic tournament on Vn in Gn [7]. This we
shall refer to as the acyclic tournament Tπ = {(πi , πj )|i < j} on Vn in Gn
represented by the permutation π. A permutation of nodes of Vn deﬁnes a unique
acyclic tournament on Vn and vice versa. In conclusion, for obtaining the optimal
evaluation order in relation to the graph G, it would be enough if we obtain an

Optimizing Register Spills for Eager Functional Languages

135

acyclic tournament on Vn in Gn = (Vn , An , w) with minimum total arc weight.
This problem is known as MINLOP and it is known to be NP-complete [6].
MINLOP12 is same as MINLOP with only diﬀerence that weight of an arc is
either 1 or 2. We will show that MINLOP12 is NP-complete by reducing it from
MINFAS. In MINFAS, for a given digraph G = (Vn , A) we are asked to ﬁnd
a subset B of A with minimum cardinality such that (Vn , A − B) is acyclic.
MINFAS is known to be NP-complete [5].
Lemma 2. Let G = (Vn , A) be a digraph and B ⊆ A be a FAS (feedback arc
set) of G. Then there exists a C ⊆ B such that (Vn , C) is acyclic and C is a
FAS of G.
Proof If (Vn , B) is acyclic then C = B. Otherwise, (Vn , B) has at least one
directed cycle. Since B is a FAS, (Vn , A − B) is acyclic. Now construct an acyclic
tournament (Vn , T ) of Gn with (A − B) ⊆ T . We deﬁne C = T c ∩ B where
T c = {(j, i)|(i, j) ∈ T }. C is FAS of G because A − C = A ∩ T and A ∩ T is
acyclic (see Figure 6).
�
An

T
Tc

A
B
C

Fig. 6. C ⊆ B ⊆ A and T ∪ T c = An

Given an FAS B of G the set C in the Lemma 2 can be constructed in polynomial
time as follows. Let (v1 , v2 , . . . , vn ) be a permutation of Vn for which v1 is a node
in (Vn , A − B) with zero indegree, v2 is a node with zero indegree in the acyclic
subdigraph of (Vn , A − B) induced by the node set Vn − {v1 } and so on. Note
that v1 , v2 , . . . , vn can be chosen in polynomial time. Let T = {(vi , vj )|i < j}.
Clearly A − B ⊆ T and T is an acyclic tournament on Vn . C = T c ∩ B can be
constructed in polynomial time. In conclusion, every FAS induces another FAS
which is acyclic. From now on, by FAS we will mean this induced acyclic FAS.
Lemma 3. The no. of arcs in an acyclic tournament on Vn in Gn is 12 n(n − 1).
Theorem 1. MINLOP12 is NP-complete.
Proof - MINLOP12 is in NP since MINLOP is. To show that MINLOP12 is
NP-hard we shall reduce MINFAS to MINLOP12 .
Let G = (Vn , A) be an instance of MINFAS. From this we shall construct an
instance Gn = (Vn , An , w) of MINLOP12 with w(e) = 1 if e ∈
/ A and w(e) = 2 if
e ∈ A. Let (Vn , T ) be an acyclic tournament of Gn . From the proof of Lemma

136

S. Mishra, K. Sikdar, and M. Satpathy

2, it can be easily seen that T ∩ A is a FAS of G. By Lemma 3 and from the
deﬁnition of the weight function w it follows that w(T ) = 12 n(n − 1) + |T ∩ A|.
Next we shall show that, B is a FAS of G with |B| ≤ k if and only if there
exists an acyclic tournament (Vn , TB ) of Gn with w(TB ) ≤ 12 n(n − 1) + k.
⇐: Let (Vn , TB ) be an acyclic tournament of Gn with w(TB ) ≤ 12 n(n − 1) + k.
1
1
n(n − 1) + |TB ∩ A| ≤ n(n − 1) + k
2
2
i.e. |TB ∩ A| ≤ k
w(TB ) =

Choose B = TB ∩ A. Since B is an FAS of G we are done.
⇒: Let B be a FAS with |B| ≤ k. By Lemma 2 (Vn , B) is acyclic. From deﬁnition
of feedback arc set, (Vn , A − B) is acyclic. Let (Vn , T ) be an acyclic tournament
with (A − B) ⊆ T . This arc set T can be constructed in polynomial time. Deﬁne
TB = T c . Clearly TB ∩ B ⊆ B and (TB ∩ B) = (TB ∩ A).
i.e.

1
n(n − 1) + |TB ∩ A|
2
1
1
= n(n − 1) + |TB ∩ B| ≤ n(n − 1) + k.
2
2

w(TB ) =

Theorem 2. OEOP(S) is NP-hard.
Proof sketch: We will reduce MINLOP12 to OEOP(S). Let Gn = (Vn , An , w)
be an instance (call it X) of MINLOP12 , where An = {(i, j)|i � =j and i, j ∈
Vn }. Let Vn = {1, 2, . . . , n}. Construct an instance of OEOP(S) from X. The
subdigraph induced by any pair of nodes i and j will into one of:
Case 1: w(i, j) = w(j, i) = 1; Case 2: w(i, j) = w(j, i) = 2 and
Case 3: either w(i, j) = 1 and w(j, i) = 2 or w(i, j) = 2 and w(j, i) = 1.
For Case 1, we include the subexpressions t1k = ak ∗(bk +ck ) and t2k = (bk +ck )+dk
in the functions Fi and Fj respectively. It can be seen easily that if we evaluate
t1k and t2k in any order only one memory spill will be required and it ﬁts with
the arc weights. Similarly the expressions t1k and t2k are chosen for Case 2 and
Case 3. The idea is that the weights between nodes i and j in X fall into one of
the three cases. The corresponding expressions will be linked to the arguments
of the function calls Fi and Fj such that the simple sharing assumption is not
violated. Doing this for all pair of nodes in the instance X, we can obtain Y, the
instance of OEOP(S). Y will have function calls F1 till Fn and their arguments
will be shared. And then it can
be �
established that for any permutation π =
�n−1
n
(π1 , π2 , . . . πn ) of (1, 2, . . . n), i=1 j=i+1 w(Fπi , Fπj ) is same as the number
of spills that will be made if the functions of Y are evaluated according to the
same permutation.
�

5

Heuristics for OEOP

The amount of sharing between the arguments of functions in user programs
is usually not high. However, such programs, for optimization purposes, are

Optimizing Register Spills for Eager Functional Languages

137

transformed into equivalent programs. Function unfolding is one such important
transformation technique, and this technique introduces sharing [4]. Our observation is that the number of function call nodes at the leaf-level of an expression
DAG (after unfolding) does not go beyond 8 or 9, and further, the number of
shared nodes between the argument DAGs of two functions also hardly goes
beyond 8 or 9. So, the graph that we will construct will not have nodes more
than 9 and further the weights of the edges in it will be within 0 to 9. With such
restrictions, the following greedy heuristic will give satisfactory results.
Construct the weighted graph from the DAG of the linear region. Then ﬁnd
the node such that the sum of the outgoing arc weights for this node is minimum.
Evaluate the corresponding call. Then the call is removed from the linear region
and it is replaced with a memory location. Get the new weighted graph, and
repeat the process till a single call remains. This is the last call to be evaluated.

6

Conclusion

We have discussed an optimization problem that occurs while following a compilation technique for eager functional languages. We have shown that the problem
is NP–Complete. The context in which problem arises, keeps the dimension of the
problem small. Our experiments show that greedy heuristics provide satisfactory
results.
Acknowledgements: The authors would like to thank A. Sanyal, A. Diwan
and C.R. Subramanian for useful discussions.

References
1. Aho A.V., Johnson S.C. & Ullman J.D., Code generation for expressions with
common subexpressions, JACM, Vol. 24(1), January 1977, pp. 146-160.
2. Aho A.V., Sethi R. & Ullman J.D.,Compilers : Principles, Techniques and Tools,
Addison Wesley, 1986.
3. Bird R. & Wadler P., Introduction to Functional Programming, Printice Hall, 1988.
4. Davidson J.W. & Holler A.M., Subprogram Inlining: A study of its eﬀect on program execution time, IEEE Tr. on Software Engineering, Vol. 18(2), Feb. 1992, pp.
89-102.
5. Garey M.R. & Johnson D.S., Computers and Intractability: A Guide to the Theory
of NP-Completeness, Freeman and Company, New York, 1979.
6. Grotschel M., Junger M. & Reinelt G., On the acyclic subgraph polytope, Math.
Programming 33 (1985), pp. 28-42.
7. Harary F., Graph Theory, Addition-Wesley, Reading, MA, 1969.
8. Satpathy M., Sanyal A & Venkatesh G., Improved Register Usage of Functional
Programs through multiple function versions, Journal of Functional and Logic Programming, December 1998, MIT Press.

