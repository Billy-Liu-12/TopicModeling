Cluster Con�guration Aided by Simulation
Dieter F. Kvasnicka1 , Helmut Hlavacs2 , and Christoph W. Ueberhuber3
1

2

Institute for Physical and Theoretical Chemistry,
Vienna University of Technology,
dieter@theochem.tuwien.ac.at
Institute for Computer Science and Business Informatics,
University of Vienna
hlavacs@ani.univie.ac.at
3
Institute for Applied and Numerical Mathematics,
Vienna University of Technology,
christoph@aurora.anum.tuwien.ac.at

Abstract. The acquisition of PC clusters is often limited by �nancial restrictions. A person planning to buy such a cluster must choose between
numerous con�gurations possible due to the large number of different PC
components a cluster may be built of. Even if some of the applications
that will be run on the planned cluster are known, it is generally dif�cult
if not impossible to identify the one con�guration yielding the optimum
price/performance ratio for these applications a priori.
In this paper it is demonstrated how to use the newly developed simulation tool Clue to decide which con�guration of the components of a
cluster yields the best price/performance ratio for a particular software
package from computational chemistry. Due to the simulation based approach, even the impact of components available in the future only can be
evaluated.

1

Introduction

Due to the dramatic price decrease of standard PC components and the exponential performance growth as described by Moores law, traditional supercomputers are gradually being substituted by PC clusters consisting of several standard
PCs containing between one and four processors and being interconnected by
either Fast Ethernet or gigabit networks.
As there are numerous companies offering off-the-shelf PC components at
very different prices and capabilities, a potential cluster buyer is usually faced
with a large number of different possible cluster con�gurations. However, there
is usually an upper limit to the overall cluster price, rendering some con�gurations as being too expensive. An example for a critical and dif�cult decision is,
whether to use the available budget for buying more nodes, more processors per
node, more main memory per node or a faster node interconnection network.
For selecting one particular con�guration it is necessary to know the number of
users that are projected to use the cluster and the type of applications that will
be run on it.
V.N. Alexandrov et al. (Eds.): ICCS 2001, LNCS 2073, pp. 243252, 2001.
c Springer-Verlag Berlin Heidelberg 2001
�

244

D.F. Kvasnicka, H. Hlavacs, and C.W. Ueberhuber

Although there is usually a general understanding of the in�uence of each
component, it is often impossible to judge the total impact the chosen con�guration will have on the applications that are foreseen to be run on the cluster.
This is even more dif�cult if the projected applications are parallel applications
consisting of several processes run on more than one processor or node. At this
point, usually rules of thumb are applied, leaving room for unpleasant surprises
once the cluster has been bought and installed.
In this paper it will be demonstrated how to apply the newly developed
CLUster Evaluator Clue to assess the performance of various cluster con�gurations for given parallel software, in this case the software package Wien 97 [3],
an application code from computational chemistry. This software is available in
two different parallel implementations, one requiring a large amount of memory
for each processor node, the other one relying on a fast communication network.
Both of these requirements nearly double the price of each node.

2

Related Work

In the past, several attempts have been made to simulate the performance of
parallel programs.
In trace-driven approaches as carried out for example by the PVM Simulator
PS [1], Tau [9] or Dip [8], it is assumed that the interprocess communication
patterns, i.e., the number and directions of sent messages, are �xed and do
not depend on the run-time situation. This assumption is valid, for example, for
routines from the ScaLapack library. In cases where the communication depends
on the run-time situation, however, for example when simulating the effect of
load balancing mechanisms, in contrast to execution driven simulation, this
approach cannot be used.
In case the simulation kernel is execution driven, however, as provided for
example by SimOS [10], changing communication patterns may also be taken
into account at the expense of increasing the simulation time drastically.
Another approach is taken in the Edpepps [4] tool. Here, users may construct
application models for PVM programs by using the graphical program representation language PVMGraph. This representation then drives the simulation
kernel. Approaches like this are primarily meant for rapid prototyping, the main
drawback being the need for creating program models in addition to the actual
implementation.

3

The Simulation Tool CLUE

Being based on the Machine Independent Simulation System for PVM 3 (Miss-PVM
[7], the simulation tool Clue is meant to support (i) con�guration decisions concerning clusters of SMPs, (ii) the development of software for parallel computers
which are not yet available, (iii) reproducible performance assessments in environments with constantly changing load characteristics (like NOWs), and (iv)
the debugging of parallel programs.

Cluster Con�guration Aided by Simulation

245

Clue allows the simulator user to model cluster con�gurations by specifying
important parameters concerning the communication network and the computational nodes. These parameters are easily obtained either by carrying out
measurements on real systems or by taking known or extrapolated parameters
(see Section 5.1). The simulator assumes that the simulated parallel applications
use the message passing library Parallel Virtual Machine (PVM) [6] for communication between the processes. The software models are most easily constructed
by taking the original source code as input for the simulator. Thus it is not necessary to rewrite existing C or Fortran code or to create additional code. PVM
based code can be used without modi�cation.
The simulator then is driven by actually executing the original parallel program, the simulator routines being activated by catching all calls to PVM and
redirecting them to special routines implemented in Clue. The structure of Clue
is shown in Fig. 1. At the virtual layer, for each running process one instance of
Application Program

Application Program

pvmV_send()

pvmV_recv()

Virtual Layer

pvm_send()

pvm_recv()

libpvm3

libpvm3

TCP

pvmd

TCP
UDP

pvmd

Network

Fig. 1. Structure of Clue.

the simulator is created as well. The simulator itself thus consists of distributed
instances communicating with other instances by exchanging PVM messages.
Each simulator instance maintains its own local virtual time, representing the
simulated computing time of this process. The virtual time, however, must not
be confused with the real simulation runtime. Instead, it is a variable controlled
by the simulator instances and increased if the respective attached application
process has carried out some computation. This is detected by measuring the
CPU time consumed by the attached processes between two adjacent calls to
PVM. The local virtual time is then increased by this measured CPU time multiplied by the processing time factor of the node hosting the process as speci�ed
in the con�guration �le.

246

D.F. Kvasnicka, H. Hlavacs, and C.W. Ueberhuber

Virtual time is also increased when sending or receiving application messages, as carried out by the executed application program. It must be assured,
however, that the reception of messages is in correct order with respect to the
global virtual time, i. e., if at virtual time t1 process P1 sends a message to P2
waiting for messages to arrive, it must be impossible that after being woken
up and receiving this message, another process P3 being at virtual time t2 < t1
sends a message to P1 . Thus, the simulator instances must use a protocol for distributed simulation in order to synchronize their local virtual times and guarantee message delivery in the right order. Therefore, the used protocol relies on an
State MISSdaemon

master

slave 1

slave 2

(u,u,u)

pvm_send()
SendQuestion
Data
(s,u,u)

LineQuestion
to slave 1
LineAnswer

(u,u,u)
(u,b,u)
(u,b,b)
(b,b,b)
(b,u,b)

StateBlockedReceive

pvm_recv()
pvm_recv()

StateBlockedReceive
StateBlockedReceive

pvm_recv()

ReceiveQuestion
from master

Real Time
Process States:

u: unknown
b: blocking receive
s: waiting for line (sending)

Fig. 2. Distributed simulation protocol.

additionally spawned process called MISSdaemon for synchronization. The
daemon keeps track of all processes, marking each process to be either in state
unknown, blocking receive, waiting for line, waiting for probe or killed. Fig. 2 shows
sample protocol messages in case a master process at the application level sends
a message to a slave process. Upon calling a PVM routine, each simulator instance sends its new state to the daemon. As soon as it can be assured that the
global virtual time can not be violated, the receiving slave is allowed to receive
the message.
Due to the execution driven simulation approach, processor performance
and memory access, including cache performance, are modeled quite accurately.
Also network properties are modeled with high accuracy, only depending on a
few input parameters. Disk I/O, however, is not modeled at all.

Cluster Con�guration Aided by Simulation

4

247

Hardware Con�guration Parameters

The cluster con�guration model used by Clue is called a virtual machine.
Virtual machines are de�ned by an input �le being read in at simulation start.
This �le contains the speci�cation of a machine used for the master program
and for additional machines or host types. For each record possible parameters
are:
Name of the machine or host type. This name is used in pvm_spawn. If the
machine performance factor (described next) is 0, the machine is assumed to
be real, and the program is started on this machine. Otherwise the machine
has a virtual name, and PVM 3 is asked to look for a suitable machine.
Performance factor. This is a �oating-point multiplier p for calculating the computation time. If this parameter is 0, the computation timing results are not
changed. Otherwise, if a process of a parallel application consumes n CPU
seconds between two adjacent calls to the virtual layer, the virtual time is
increased by p × n seconds.
Initialization Time. This is the time needed for pvm_spawn to start a new child
process, measured on the childs side.
Spawn Time. This is the time spent in pvm_spawn by the calling process.
Send Time. This is the time used for sending a message using pvm_send or
pvm_mcast. This time contains packing the message, resolving the address
of the host and starting the transmission (as far as the sending process is
involved). The parameters may be speci�ed as k, d, where the send time
s(m) depending on the message length m is given by s = k × m + d, or may
be speci�ed as tuples (mx , s(mx )), where the actual send time is interpolated
linearly between these points.
Receive Time. This is the time used in calling the receive routines pvm_recv,
pvm_nrecv and pvm_probe. This time is always the same whether these
routines succeed or fail.
Transmission Time. This is the time used to transfer a message minor the send
delay. As with the send time, the transmission time may be speci�ed as linear
model or linearly interpolated data points.
Packing Time. This is the time used to pack the message into the PVM 3 send
buffer.
Various values for time granularity have already been used for simulation. Fast
interconnection networks can be modeled accurately with 1µs granularity. Send
and transmission times may be speci�ed for any pair of hosts, they may also
be speci�ed for the send and transmission of messages from one host to itself,
in case multiprocessor machines are to be modelled. If the actual performance
model turns out not to be exact enough, it can easily be changed by modifying
the con�guration �le.

248

5

D.F. Kvasnicka, H. Hlavacs, and C.W. Ueberhuber

Case Study: Finding an Optimum Cluster Con�guration for
WIEN 97

In this case study it is demonstrated how to apply Clue to �nd an optimum
hardware con�guration for a particular parallel application, in this case by taking the well known computational chemistry package Wien 97 as an example.
It is assumed that an institute is planning to purchase a PC cluster to run the
computationally expensive Wien 97 simulations. Furthermore it is assumed that
there is a tight limit on the budget that the institute can spend.
The most time consuming part of Wien 97 is spent in a routine called lapw1,
basically solving a generalized symmetric eigenproblem by applying Cholesky
factorization and transforming the problem to a (simpler) tridiagonal eigenproblem. Thus, a cluster running Wien 97 should be optimized for running parallel
versions of the Cholesky factorization and for tridiagonalization, preferably as
provided by the basic linear algebra subprograms Blas [5], which are used by
the parallel linear algebra package ScaLapack [2].
Also, two application cases have been chosen for simulation:
 A small case with matrix sizes of 2500×2500 which can be solved on one
processor on a standard PC.
 A larger case with matrix sizes of 6000×6000. In this case it is necessary to
either have more memory for each processor, or to distribute each matrix to
several processors.
Each case is representative for 50 % of the overall workload of the PC cluster.
5.1

Communication Models

Though Clue can apply various communication models, in this case study, piecewise linear models are used for the send and transmission times (see Fig. 3). All
model parameters have been measured on existing networks by running custom programs for measuring parameters like latency, bandwidth and processing
speed.
Additionally, a simple contention model is used which increases the communication time (both send and transmission time) by a factor depending on
the number of simultaneous communication operations carried out.
Alternative approaches for estimating con�guration parameters include for
instance taking published values from benchmarks or parameters provided by
companies for their products (often speci�ed, for example, for gigabit networks).
Also, a popular rule of thumb speci�es that when increasing the processor clock
rate by N %, then the performance gain will not exceed N/2%. This rule may then
be applied for extrapolating the performance of systems, even if the respective
processors are not available yet. However, it does not apply if new processor
cores with new features like SIMD instructions are introduced or if other possible
performance bottlenecks like caches or main memory are changed.

Cluster Con�guration Aided by Simulation

�
����
�

����

249

���� ��� ������������ ����

���� ���� ����
�������� ���� ����
���� ������������ ����
�������� ������������ ����

����
����
���� � ��
�

��
�

�
��
������� ���� ����

���

�����

Fig. 3. Send and transmission time for a Fast Ethernet PC cluster. Sender and receiver
are on the same node. Although they share the same memory, they communicate via a
message passing library.

5.2

Hardware Con�gurations

To choose the con�guration of PC clusters with highest performance for Wien 97,
several con�gurations are examined. The budget limit being set to $ 20.000,
re�ects only pure hardware investments, costs for the cluster installation and
con�guration as well as for software are not included and are assumed to be
similar for all con�gurations.
Table 1 shows the con�gurations affordable according to the budget limit. In
this table, information on the clock rate and the manufacturer were omitted due
to the fact that updates on clock rates occur too often and customers will always
aim at buying the fastest available version of a processor at the time of purchase.
Thus, this decision is delayed to the �nal stage of the evaluation.
Table 1. Cluster con�gurations under consideration.
Name
Network
Memory
Fine
Coarse
Cheap
Cheapest

Number of
Nodes
6
6
7
7
10
15

Memory Processors Interconnection
per Node per Node
Network
256 MB
2
Gigabit Class
1024 MB
2
Fast Ethernet
128 MB
1
Gigabit Class
1024 MB
1
Fast Ethernet
256 MB
2
Fast Ethernet
128 MB
1
Fast Ethernet

250

D.F. Kvasnicka, H. Hlavacs, and C.W. Ueberhuber

�������
���� �������� ����������� �����������
���� �������� ����������� ���������
������� ����� ����������� �����������
������� ����� ����������� ���������

��
��
� ��
��
��
�

�

�

�

�
�
�
�
������ �� ���������� �

�

�

��

Fig. 4. Simulation of the Cholesky factorization using Clue (n = 2000).

6

Simulation Results

Preliminary experiments simulating the Level 3 Blas based Cholesky factorization have been performed with (i) a model for Fast Ethernet communication and
(ii) a model assuming gigabit class communication. The communication model
was constructed and validated using the PC cluster of the RWTH Aachen using an SCI network. Results with increasing number of processors are given in
Fig. 4. It can be seen that for both network types, best results are achieved with
a rectangular (e. g., 2×2, 2×3, 2×4 or 3×3) processor grid. In general, empirical
observation shows that linear algebra algorithms work best if the processors
can be mapped onto a N × M, N > 1, M > 1 processor grid. Thus, most cluster
con�gurations under consideration (with two exceptions) hold this property.
Furthermore, for the small application case it turned out that only the number
of processors, their performance, and in the dual processor nodes memory contention have a signi�cant effect on the overall performance, the in�uence of the
network or the memory size being negligible. Thus, instead of applying simulation, an analytical model was chosen to evaluate the con�guration performance.
The performance P (C) of con�guration C is given by P (C) = N × f , where N
denotes the number of processors and f denotes the memory contention factor
for dual processor nodes. This factor is further de�ned to be f = 0.5×fc +0.5×ft ,
where fc = 1.0 is the memory contention factor for the Cholesky factorization
and ft being the memory contention factor for the tridiagonalization, which has
been measured on a Pentium II system to be 0.77 for dual processor and 1.0 for
single processor nodes.

Cluster Con�guration Aided by Simulation

251

Results of the �nal simulation experiments for the large application case
are shown in Table 2. In some cases more than one parallelization strategy per
con�guration is shown. The column high level parallelization tells how many
instances of lapw1 are solved concurrently. The column low level parallelization tells how many processors are used for each instance of lapw1 (including
shared-memory parallelism). The column parallelism tells how many processors
are engaged in total (the product of high level and low level). The column empty
tells how many processors are not used in the program run. The columns Kernel
1 and 2 tell the overall �oating-point performance in M�op/s for the Cholesky
factorization (Kernel 1) and the tridiagonalization (Kernel 2).
Table 2. Floating-point performance (in M�op/s) for the large test case.
High Low
Kernel Kernel
Name Level Level Parallelism Empty
1
2
Network
3
4
12
0
5194 1578
Memory
3
4
12
0
4153 1501
Memory
3
2
6
6
3066
884
Fine
1
6
6
1
2225
814
Fine
1
7
7
0
1852
702
Coarse
3
2
6
1
2283 1025
Coarse
3
1
3
4
1552
587
Cheap
3
6
18
2
4666 1913
Cheapest 3
4
12
3
3284 1763
Cheapest 3
5
15
0
3248 1543

7

Choosing the Optimum Con�guration

In order to �nd the optimum cluster con�guration for Wien 97, the simulation
results shown in Table 2 and the results given by the analytical model must
further be investigated. This is done by distributing points to each result in such
a way that the fasted con�guration is rewarded 100 points. The points given
to the remaining con�gurations then show how many percent of the fastest
performance they achieved. Furthermore, the points for the large test case are
split equally to the two kernels, each being assigned at most 50 points. The result
is shown in Table 3, for con�gurations having two entries in Table 2, only the
better entry has been considered.
It turns out that the Cheap con�guration has to be considered for the �nal
decision, whereas the Cheapest con�guration being second best. The Network
con�guration performs also well for the large test case and might even win the
competition (for the large test case) if communication becomes more important.
This may happen either
 if the processor speed increases,
 if less high level parallelism is available, or
 if smaller problems have to be solved. In this case the ratio of communication
to computation is increased.

252

D.F. Kvasnicka, H. Hlavacs, and C.W. Ueberhuber

The other con�gurations perform rather poor, mainly because of a lack of raw
compute power, since they have fewer processors.
Table 3. Assessment by points.
P (C) Large Case Large Case
Name
Kernel 1 Kernel 2
Cheap
100
45
50
Cheapest 85
32
46
Network 60
50
41
Memory
60
40
39
Coarse
40
22
27
Fine
40
22
21

8

Sum
195
163
151
139
89
83

Conclusion

In this paper it has been demonstrated how to use the cluster evaluator Clue for
�nding optimum PC cluster con�gurations for given parallel applications and
budget limits. By applying this technique, a priori performance evaluations of
PC clusters to be bought may be carried out, thus being able to assess different
cluster con�gurations by a quantitative procedure rather than a rule of thumb.

References
1. R. Aversa, A. Mazzeo, N. Mazzocca, U. Villano, Heterogeneous system performance
prediction and analysis using PS, IEEE Concurrency 63 (1998), pp. 2029.
2. L. S. Blackford et al., ScaLapack Users Guide, SIAM Press, Philadelphia, 1997.
3. P. Blaha, K. Schwarz, P. Sorantin, S. B. Trickey, Full-Potential, Linearized Augmented
Plane Wave Programs for Crystalline Systems, Comp. Phys. Commun. 59 (1990),
pp. 399415.
4. T. Delaitre et al., A Graphical Toolset for Simulation Modelling of Parallel Systems,
Parallel Computing 2213 (1997).
5. J. J. Dongarra, J. Du Croz, I. S. Duff, S. Hammarling, A Set of Level 3 Blas, ACM
Trans. Math. Software 16 (1990), pp. 117,1828.
6. A. Geist, A. Beguelin, J. J. Dongarra, W. Jiang, R. Manchek, V. Sunderam, PVM: Parallel Virtual MachineA Users Guide and Tutorial for Networked Parallel Computing, MIT Press, Cambridge London, 1994.
7. D. F. Kvasnicka, C. W. Ueberhuber, Developing Architecture Adaptive Algorithms
using Simulation with MISS-PVM for Performance Prediction, Proceedings of the
International Conference on Supercomputing, ACM, 1997, pp. 333339.
8. J. Labarta et al., Dip: A parallel program development environment, Proc. Euro-Par
96, Vol. II, Springer-Verlag, Berlin, 1996, pp. 665674.
9. W. Mohr, A. Malony, K. Shanmugam, Speedy: An integrated performance extrapolation tool for pC++, Proc. Joint Conf. Performance Tools 95 and MMB 95, SpringerVerlag, Berlin, 1995.
10. M. Rosenblum et al., Using the SimOS Machine Simulator to Study Complex Computer Systems, ACM TOMACS Special Issue on Computer Simulation (1997).

