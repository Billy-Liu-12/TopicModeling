Attractor Density Models with Application to
Analyzing the Stability of Biological Neural
Networks
Christian Storm and Walter J. Freeman
University of California at Berkeley, Graduate Group in Biophysics,
9 Wellman Court, Berkeley, CA USA 94720
raystorm@uclink4.berkeley.edu, wfreeman@socrates.berkeley.edu
http://sulcus.berkeley.edu

Abstract. An attractor modeling algorithm is introduced which draws
upon techniques found in nonlinear dynamics and pattern recognition.
The technique is motivated by the need for quantitative measures that
are able to assess the stability of biological neural networks which utilize
nonlinear dynamics to process information.

1

Introduction

In formulating neural networks which rely on nonlinear dynamics as an information processing medium [1] [2], two pertinent questions arise when the stability
of the network is considered: 1) How stable is the network dynamics to input?
2) If one or more of the parameters in the network are altered, to what degree
are the dynamics modiﬁed? These questions are central to understanding the
utility of chaotic neural networks since their answers form the foundation for
computational stability and robustness or, ultimately, the degree in which such
systems can be relied on to perform extended parallel computations. However,
of the present tools oﬀered to dynamicists none are capable of answering these
questions on a quantitative basis.
1.1

Nonlinear System Identiﬁcation

To date a suite of nonlinear system identiﬁcation methods have been introduced
for dynamical systems, each geared toward solving a particular identiﬁcation
problem [3] [4]. However, of the current methods oﬀered in the literature, none
are comparative in nature in the sense that approximative models are created
for the sole purpose of comparison. In this paper a novel nonlinear system identiﬁcation method is presented which relies on tools found in nonlinear dynamics
and pattern recognition to form approximative models that lend themselves to
comparison on a statistical basis. The density of states approach is founded upon
the notion that a dynamical system has a corresponding set of attractors where
the dynamical evolution of the system in a particular volume of phase space is
governed by the attractor or its associated basin which occupies that space [5].
V.N. Alexandrov et al. (Eds.): ICCS 2001, LNCS 2074, pp. 231–234, 2001.
c Springer-Verlag Berlin Heidelberg 2001
�

232

2

C. Storm and W.J. Freeman

Attractor Density Models

Using a set of trajectories originating from an independent and identically distributed initial condition set S intersecting a collection of attractors A or their
associated basins B, a particular region of state space can be independently explored. From a probabilistic reinterpretation of these independent state space
trajectories a probability density model may be deﬁned. Speciﬁcally, given a realization set {y} generated from a known model F (p) with unknown parameters
or inputs p acting on a known domain {x}, F (p) : {x} → {y}, a probability
function may be estimated from a transformation of the set {y}, P (T ({y})),
such that P (T ({y})) corresponds with F (p) : {x}.
2.1

Feature Selection and Segmentation

The methodology used to form attractor density models is motivated by the two
tenets of pattern recognition- Feature selection and segmentation of the feature
set. Feature selection is carried out by decomposing independent explorations
of state space, generated from a ﬁxed domain, into reconstruction vectors. Segmentation of the feature vectors relies on exploiting the variance, covariance
structure of a reconstruction vector set.
Given a time series h(at ) = yt where A is a compact ﬁnite-dimensional set of
states a ∈ A observed with a generic function h, h : A → R, the states a may be
reproduced by n-dimensional vectors b produced by a linear operation on the
time series values
b = M [h(a), h(a−τ ), ..., h(a−(n−1)τ )]T

(1)

where τ is a positive real number and n ≥ 2d + 1 with box counting dimension
d [6]. In the present context of feature selection in a nonstationary process,
the reconstruction operator M = M4 M3 M2 M1 is comprised of three terms: M1
ﬁnds and removes the mean from the realization, M2 ﬁnds and normalizes the
demeaned realization to unit energy, M3 ﬁnds the spectral coeﬃcients through
the convolution of the remaining realization with a Morlet ﬁlter bank [7]. This has
the eﬀect of breaking each realization of the chaotic process into its dynamical
modes. To segment each set of vectors, the dominant modes are extracted using
singular value decomposition, M4 .
2.2

Density Estimation

Each dynamical state vector represents a state a of one of the attractors A ∈
A. The density of trajectories in the neighborhood of state a can be viewed
statistically in terms of having a certain probability of being visited. Using this
notion an attractor representation may be built by calculating the density of
states of an attractor in reconstruction space.
Under the assumption that each dynamical state vector is a sample from an
unknown probability density function f , a kernel function K provides a means

Attractor Density Models with Application to Analyzing the Stability

233

of generating a nonparametric estimate fˆ of f . The kernel density estimate fˆ in
dimension d at point a� with samples ai is deﬁned as [4]
�
� �
n
1 �
(a − ai )T (a� − ai )
�
ˆ
f (a ) =
K
nhd i=1
h2

(2)

where K is deﬁned as the multivariate Gaussian joint probability density function.

3

A Test of Segmentation

An interesting demonstration of this methodology is to see how well attractor
density models are segmented for a range of parameter values in the MackeyGlass equation [8]. First, attractor density models are built for each parameterization (τ = 6, 8, ..., 200, a = 0.16, 0.18, ..., 0.24) using forty realizations.

Fig. 1. Rounded estimates (dotted lines) of a given one and forty realizations. The raw
estimates are denoted by solid lines. The true value is a = 0.2 for all values of τ

Fig. 2. Rounded estimates (dotted lines) of τ given one and forty realizations. The true
values are denoted by the 45◦ solid lines

234

C. Storm and W.J. Freeman

Given a set of one and forty realizations generated with unknown parameters
τ = 6, 8, ..., 200 and a = 0.20, which model best describes the data? This question is answered by ﬁnding the center of mass of the density values found for each
model using the points in each realization set. It is evident from the estimations
of a and τ in Figure 1 and 2, respectively, that increased sample sizes reduce
the estimate variances indicating that with modest sample sizes a good degree
of segmentation can be expected. The variability in the estimates is attributed
to statistical ﬂuctuations.

4

Conclusion

In this paper, the basis for creating attractor density models is described. The
aim in formulating this technique lies in the need for nonlinear system identiﬁcation methods which promote the comparison between models. With such
methods, the ability to analyze the stability of chaotic neural networks to changes
in internal parameters or input will be greatly improved.
This work was supported by ARO MURI grant ARO DAAH 04-96-0341.

References
1. Freeman, W.J.: Tutorial in Neurobiology: From Single Neurons to Brain Chaos.
International Journal of Bifurcation and Chaos 2 (1992) 451-482
2. Kaneko, K., Tsuda, I.: Complex Systems: Chaos and Beyond. Springer-Verlag b,
Berlin (2001)
3. Kosko, B.: Fuzzy Engineering. Prentice Hall, New Jersey (1997)
4. Silverman, B. W.: Density Estimation for Statistics and Data Analysis. Chapman
and Hall, New York (1986)
5. Storm, C.: A Novel Dynamical Invariant Measure Addresses the Stability of the
Chaotic KIII Neural Network. Proceedings of International Joint Conference on
Neural Networks 1 (1999) 725-729
6. Sauer, T., Yorke, J. A., Casdagli, M.: Embedology. J. Stat. Phys. 65 (1991) 579-616
7. Teolis, A.: Computational Signal Processing with Wavelets. Birkhäuser, Boston
(1998)
8. Mackey, M. C., Glass, L.: Oscillation and chaos in physiological control systems.
Science 197 (1977) 287

