Architecture Independent Analysis of Parallel
Programs
Ananth Grama1 , Vipin Kumar2 , Sanjay Ranka3 , and Vineet Singh4
1

2

Dept. of Computer Sciences, Purdue University, W. Lafayette, IN 47907
ayg@cs.purdue.edu
Dept. of Computer Sciences, University of Minnesota, Minneapolis, MN 55455
kumar@cs.umn.edu
3
Dept. of Computer Sciences, University of Florida, Gainesville, FL 32611
ranka@cis.ufl.edu
4
10535 Cordova Road, Cupertino, CA 95014

Abstract. The presence of a universal machine model for serial algorithm design, namely the von Neumann model, has been one of the key
ingredients of the success of uniprocessors. The presence of such a model
makes it possible for an algorithm to be ported across a wide range of
uniprocessors eﬃciently. Universal algorithm design models proposed for
parallel computers however tend to be limited in the range of parallel
platforms they can eﬃciently cover. Consequently, portability of parallel
programs is attained at the expense of loss of eﬃciency. In this paper, we
explore desirable and attainable properties of universal models of architecture independent parallel program design. We study various models
that have been proposed, classify them based on important machine parameters and study their limitations.

1

Introduction

Conventional sequential computing is based on the von Neumann model. This
role of this model is illustrated in Figure 1. Given problem P, an optimal algorithm O is designed for solving it for model I. A description of this algorithm,
along with a description of the architecture of the target machine A is fed into
the translator T. The translator generates machine code for the target machine.
The set A spans the set of most conventional sequential computers and for this
set, the von Neumann model ﬁlls in the role of model I.
In the above framework, the architecture independent algorithm design model
I plays a very important role. It ensures that the same algorithm can be executed
eﬃciently on a large variety of serial computers. Speciﬁcally, programs designed
for this model have the property that they run in asymptotically the same time
on every serial computer. The runtime of the algorithm on the said model is
an accurate reﬂection of its asymptotic runtime on all computers subsumed by
that model. The constants involved may not be reﬂected in the model but the
model does provide a reasonable estimate. We refer to this as the predictive
property of the model. Another interpretation of the predictive property is that
V.N. Alexandrov et al. (Eds.): ICCS 2001, LNCS 2074, pp. 599–608, 2001.
c Springer-Verlag Berlin Heidelberg 2001
�

600

A. Grama et al.
Abstract Computation
Model M

Problem P

Algorithm
Designer
Algorithm A for
solving problem P on model M
Description of
Architecture C

Translator T

Machine code for solving
Problem P on arch. C

Fig. 1. The role of algorithm design model I in solving problem P on architecture A.

if two algorithms O1 and O2 are fed to the model, the model is able to predict which algorithm will have an asymptotically better performance on each
computer subsumed by the model. We refer to this as the ordering property
of the model. Finally, a translator exists that is capable of taking an algorithm
speciﬁed for the model and converting it into a form that is executable by the
target machine in asymptotically the same time as on model I. The translation
process itself should not take an unreasonable amount of time. In the absence
of such an architecture independent algorithm design model, we would have to
design separate algorithms for each architecture.
The desirable properties of an architecture independent model do not change
signiﬁcantly in the context of parallel computers. However, due to added complexity, any such model can become diﬃcult to program. It is therefore necessary
to limit the complexity of the model. An architecture independent parallel programming model must have good coverage in architecture and algorithm space,
be simple to work with, and have desirable performance prediction properties
in the absolute, asymptotic, or relative (ordering) sense. It is unreasonable to
expect accurate absolute (or even asymptotic) performance prediction from a
model with good architecture and algorithm coverage. However, ordering properties are highly desirable for any proposed model.

2

Taxonomy of Parallel Algorithm Design Models

A number of models have been proposed that tradeoﬀ accuracy, coverage, and
simplicity to varying degrees. Whereas some models are relatively easy to design
algorithms for, they may be diﬃcult to realize in practice. In contrast, models
that are closer to real machines tend to be less architecture independent and
diﬃcult to program. It is critical to identify various factors that diﬀerentiate the
models and how they render models more or less realizable.
Global Memory Versus Seclusive Memories. The strongest models of algorithm
design allow each word in a part of the memory to be addressed independent
of all others (i.e., assume a global p-ported memory). Access to a certain word

Architecture Independent Analysis of Parallel Programs
No Data Volume Locality

No Bulk Access Locality

p-ported global memory

601

Models for parallel computation

PRAM
Data Volume Locality
LPRAM

No Data Volume Locality

HPRAM

Bulk Access Locality
Data Volume Locality

BPRAM

QSM

YPRAM
No Bulk Access Locality

No Data Volume Locality

Randomized Data Access

Structured Data Access

p-single ported memories

Data Volume Locality

BSP, MPC
Randomized Data Access

Structured Data Access

No Data Volume Locality

Randomized Data Access

Bulk Access Locality

Structured Data Access

Data Volume Locality

YPRAM

Randomized Data Access
CCN, LogP, C^3

Structured Data Access
Modified Hierarchical Models

Fig. 2. Taxonomy of models for parallel programming.

in this global memory by a processor does not block access to any other word
by another processor. Unfortunately, such a strong model comes with signiﬁcant
costs. If the total number of words in global memory is m and the number
of processors is p, a completely non-blocking switch has a complexity Θ(mp).
Furthermore, the presence of processor caches imposes stronger requirements
of the global memory. Most PRAM models (PRAM, LPRAM [3], BPRAM [4])
assume a global memory.
The switch complexity can be reduced by organizing memory into banks
(p banks of single-ported memories). When a processor is accessing a bank,
access to all other words in the bank by other processors is blocked. If the
switched memory is organized into b banks, the switch complexity of a nonblocking (at the bank level) switch is Θ(pb). This complexity is realizable at
the current state of the art. Such a model is referred to as a Seclusive Memory
model. When combined with appropriate communication cost models, seclusive
memory models approximate machine architectures much better than global

602

A. Grama et al.

memory models. Algorithm design models based on the DSM class of models
include MPC, XPRAM[17], CCN, and LogP[5].
Bulk Access Locality. Bulk access locality requires accessing data in bulk due
to non-negligible startup latencies. A dominant part of the startup latency is
spent in packaging data, computing routing information, establishing protocols
and programming routing hardware. This time varies signiﬁcantly depending
on the protocols used. Nevertheless, it is signiﬁcantly higher than the processor
cycle time in all cases. Although startup latency may not seem to be a major
architecture parameter, it is in fact responsible, in signiﬁcant part, for enforcing
a degree of granularity in communication (and hence computation). Algorithm
design models can be classiﬁed into those that reward bulk data access and those
that do not. Models in the latter category are benign to algorithms that do not
incorporate granularity. For such algorithms, the startup latency may have to be
paid for each remote access on a real machine. Consequently, algorithms designed
for such models perform poorly on real machines.
Data Volume Locality. The rate at which data can be fetched from local memory
is generally much higher than that from a remote processor’s memory. Eﬃcient
parallel algorithms must therefore minimize the volume of remote data accessed.
This is referred to as data volume locality. Such locality is particularly important
for platforms based on local area networks where the remote access rate (10s of
MB/s) is much lower than local access rate (100s of MB/s). On many platforms,
the degree of connectivity of the interconnection network is limited. For such
platforms, the eﬀective per-word transfer time for many permutations must be
scaled up to accommodate the limited cross-section bandwidth. Machines with
remote access time comparable to local memory access time, may have higher
eﬀective remote access times when compensated for cross-section bandwidth.
Even if it were possible to construct machines with inter-processor communication bandwidth matching local memory bandwidth, the presence of caches
complicates the situation. Assuming a high cache hit ratio on serial computations, the eﬀective local access time is the time to access the cache (and not the
main memory). The inter-processor communication bandwidth must therefore
match the local cache bandwidth. This is impossible to achieve without making
bulk accesses. Therefore, even models that assume communication bandwidth
equal to local memory bandwidth must incorporate bulk-access to mask cache
eﬀects and startup latency.
Structured Communication Operations. Typical applications can be viewed as
a set of interacting subtasks executing on diﬀerent processors. Interaction is by
means of exchange of messages. It is often possible to formulate these tasks
in such a way that the computation proceeds in interleaved steps of computation and communication. All processors (or subsets thereof) participate in the
communication step. Since the communication involves more than a single pair
of processors (aggregates or groups), such communication operations are also
referred to as aggregate or collective communication operations.

Architecture Independent Analysis of Parallel Programs

603

Most current parallel computers are capable of exploiting the structure of
aggregate operations. However, many models ignore this structure and assume
that all redistributions take equal time. A very common assumption made is that
if the maximum amount of data entering or leaving a processor in a particular
redistribution is m then the communication time is proportional to m and is independent of the number of processors p. This can be a misleading assumption.
Consider a many-to-many personalized communication with ﬁxed data fan-in
and fan-out. This communication operation is also referred to as the transportation primitive [15]. It is possible to perform this operation in Θ(m) time (m is
the data fan-in or fan-out) only in the presence of slack and O(p) cross-section
bandwidth [15]. The accuracy of models making the above assumption can be
poor and the class of real machines covered is relatively small.
There is a class of communication operations for which the optimal algorithms ﬁt into a hierarchical framework. A hierarchical framework consists of
hierarchies of redistributions. Redistributions involve only processors within a
group. It is assumed that redistributions at various levels have no structure.
Hierarchical models of parallel computers (YPRAM, PRAM) are capable of exploiting communication operations that ﬁt into this hierarchical framework.

3

Existing Models for Parallel Programming

A variety of parallel algorithm design models have been proposed. They diﬀer
in their coverage, complexity, and accuracy.
LPRAM and BPRAM Models. The LPRAM [3] model of computing is a
global memory model. It assumes that the startup latency is negligible. Both
of these parameters are unrealizable in real machines of reasonable sizes. Furthermore, it does not reward structured data accesses. Algorithms designed for
this model may potentially pay signiﬁcant startup overheads and make poor
use of available communication bandwidth. The bandwidth for remote access
is assumed to be less than that for local memory access. Therefore, algorithms
designed for LPRAMs reward only data volume locality.
The BPRAM [4] model assumes non-negligible startup latencies and a communication bandwidth identical to local memory bandwidth. It therefore rewards
bulk access locality but fails to capture data volume locality. Machines conforming to this model are diﬃcult to construct because of the presence of a global
memory. Furthermore, if the processing nodes have a hierarchical memory structure, the eﬀective bandwidth for local access is the bandwidth to cache (assuming
a high hit ratio). The communication bandwidth must therefore match the bandwidth to cache in this model. This is diﬃcult to realize unless communication is
highly clustered.
Hierarchical RAM models (HPRAM, YPRAM, HMM). The HPRAM
[11,10] and YPRAM [6] models of computation allow recursive decomposition

604

A. Grama et al.

of the machine until the algorithm uses only one processor and executes sequentially. For more than one processor, the communication cost charged is a
function of the number of processors available. Assigning diﬀerent cost to data
redistributions in which communication is limited to smaller subsets of processors allows the design of algorithms that are more eﬀective in utilizing locality
especially for divide and conquer type applications. Despite the availability of
variable cost redistributions, the formulations described in [11,6] suﬀer from limitations similar to the BSP model (described subsequently) in terms of slack and
worst case bandwidth requirements for every step. Analyzing time requirements
using these models is diﬃcult for non-recursively decomposable algorithms.
Bulk Synchronous Parallel and Related Models. The XPRAM [17] model
allows PRAM algorithms to run within a constant factor of optimal as long
as there is suﬃcient slack. The XPRAM model forms the basis for Bulk Synchronous Parallelism (BSP). It assumes a seclusive memory and for eﬃcient
operation, assumes startup latency to be negligible and per-word access time to
be similar to cycle time. Although such a model is not as diﬃcult to construct as
global memory models, the restrictions on startup latency and per-word transfer
time are diﬃcult to realize in real machines. The main feature of BSP is that it
allows simulation of p log p PRAM processors on p seclusive memory processors
in asymptotically the same time as on p PRAM processors.
Although BSP [16] is an attractive theoretical model, it has several drawbacks. The cost of making a bulk access of h words by a processor in a superstep
is assumed to be s + g � h. Here, s is the startup cost and g � is the eﬀective perword transfer time. For eﬃcient simulation, this cost must be comparable to the
cost of h local memory accesses. This is diﬃcult to achieve in realistic BSP simulations on actual platforms. The presence of hierarchical memory at processing
nodes also requires bulk access locality. Even if the startup latency t s of the
network was somehow reduced to a negligible amount, the time for realizing a h
relation must be similar to the cache access time for h words on a uniprocessor
(and not local memory access time). This can be achieved only through bulk
access of data from processors.
The CGM model, proposed by Dehne [7], is an extension of the BSP model. It
addresses the undesirable possibility in BSP of a large number of small messages.
A CGM algorithm consists of alternating phases of computation and global communication. A single computation-communication pair corresponds to a BSP superstep and the communication phase corresponds to a single h-relation in BSP.
However, in the case of CGM, the size of the h-relation is given by h = n/p,
where n corresponds to the problem size and p to the number of processors.
With this restriction on permutation size, the communication cost of a parallel algorithm is simply captured in the number of communication phases. This
communication cost model, rewards a small number of larger messages, thus
capturing spatial locality better than the BSP model.
Another extension of the BSP model proposed by Baumker et al. [2] also attempts to increase the granularity of messages by adding a parameter B, which

Architecture Independent Analysis of Parallel Programs

605

is the minimum message size for the message to be bandwidth bound as opposed
to latency bound. Messages of size less than B are assigned the same cost as messages of size B, thus rewarding higher message granularity. The time taken by a
BSP* algorithm is determined by the sum of computation and communication
supersteps. A computation superstep with n local computations at each processor is assigned a cost max{L, n}. A communication step with a h-relation of size
s is assigned a cost max{g × h × � Bs �, L}. Thus algorithms aggregate messages
into larger h-relations to utilize all available bandwidth.
CCN, LogP and Related Models. The Completely Connected Network
(CCN) model of computing is a seclusive memory model. Although CCN has
never been formally proposed as a model for architecture independent programming, its variants have been used extensively [12]. It assumes the presence of p
processing elements connected over a completely connected network. The cost of
sending a message of size m from one processor to any other is given by ts +tw m.
Here, ts is the startup latency and tw is the bandwidth term. It therefore rewards
bulk access and data volume locality.
In many architectures, small, ﬁxed size messages can be sent with a small
overhead using active messages. This forms the basis for the LogP model of
computation. In the LogP model processors communicate using small ﬁxed size
messages of size m. The model rewards bulk access at the level of small messages (using parameter L) and data volume locality (using parameter g). Larger
messages are sent in LogP by breaking them into smaller messages and pipelining them. For each of the smaller messages, the processor incurs an overhead
of o at the sending end. The message can then be injected into the network.
Subsequent messages can be injected at gaps of g. Here, g is determined by the
network bandwidth. If a sequence of short messages needs to be sent, the gap
of the ﬁrst message can be overlapped with the preparation overhead o of the
second message. Depending on values of g and o, two situations arise: g > o:
in this case, the sending processor can prepare the messages faster than the
network bandwidth. In this case, the network bandwidth will not be wasted. In
the second case o > g: in this case, the processor cannot inject messages into
the network fast enough to utilize available bandwidth. Therefore, it is desirable
that the size of the small ﬁxed message m be such that g > o. This implies that
the model rewards bulk access to the level of m words. Indeed, if g > o and there
is no overlap of computation and communication, this bulk is suﬃcient to mask
startup latencies.
The postal model [1], which predates the LogP model, is a simpliﬁcation
of the LogP model. The postal model assumes a message passing system of p
processors with full connectivity (processors can send point-to-point messages
to other processors), and simultaneous I/O (a processor can send one message
and receive a message from another processor simultaneously). As is the case
with LogP, messages have a predeﬁned size. Larger messages are composed as
aggregates of these smaller messages. In a postal model with latency λ, a (small)
message sent at time t renders the sender busy for the time interval [t, t + 1] and

606

A. Grama et al.

the receiver busy for the interval [t + λ − 1, t + λ]. Notice that this corresponds
to the overhead o of the LogP model being 1 and the latency λ being identical
to L.
The LogP model relies on short messages. However, several machines handle
long messages much better than a sequence of short messages. In such cases,
the LogP communication model is not very accurate. The LogGP model aims
to remedy this by adding a parameter G, which captures the bandwidth for
large messages. G corresponds to the gap per byte (or data item) for large
messages. In the LogP model, sending a k item message takes time o + (k −
1) ∗ max{g, o} + L + o cycles. In contrast, sending the message in the LogGP
model (assuming the message is large) is given by o + (k − 1)G + L + o cycles. As
before, the sending and receiving processors are busy only during the o cycles of
overhead and the rest of the time can be overlapped with useful computation. By
reducing the overhead associated with large messages, LogGP rewards increased
communication granularity.
These models suﬀer from drawbacks that stem from their inability to exploit
structure in underlying communication patterns. They assume that all redistributions can be accomplished in time proportional to message size (Θ(m) for a m
word permutation). However, there are no known algorithms that can perform
the permutation in this time even on Θ(p) cross-section bandwidth networks
without slack. (The transportation primitive of Shankar and Ranka [15] performs this permutation in Θ(m) but it requires a total slack of Θ(p).) Therefore,
the cost model is a lower bound for data redistributions on all architectures.
Now consider an adaptation of these models for a lower connectivity network
such as a 2-D mesh. The eﬀective bandwidth is scaled down by the cross-section
bandwidth to account for the limited bandwidth of the mesh while performing
generalized redistributions. However, aggregate communication operations such
as grid-communication and broadcasts consist entirely of redistributions that do
not congest the network and are therefore capable of extracting the same network performance from a mesh as from a dense network such as a hypercube.
In these cases, the communication cost of both models is an upper bound. The
communication time predicted by the models is therefore neither an upper bound
nor a lower bound. To remedy these problems the LogP model itself suggests
use of diﬀerent values of gap g for diﬀerent communication patterns, however, it
does not provide details on how to do so.
Contention Modeling Abstractions. The Queuing Shared Memory Model
(QSM) of Ramachandran et al. [13] also follows a bulk synchronous format with
phases of shared memory reads, shared memory writes, and local computation.
The performance model of QSM explicitly accounts for remote memory accesses,
contention, and congestion. The primary parameters in the QSM model include
the number of processors p and the remote data access rate in terms of local
instruction cycles g. If in a phase, a parallel algorithm executes m op local memory
operations and mrw remote operations, with a contention of κ, QSM associates
a cost max{mop , g.mrw , κ} with it. Minimizing this maximizes local data access

Architecture Independent Analysis of Parallel Programs

607

and minimizes contention to shared data. In addition to these, QSM also has a set
of secondary parameters - latency l, barrier time L, message sending overhead o,
memory bank contention hr , and network congestion c. The process of algorithm
design can be largely simpliﬁed by considering only the primary parameters and
optimizing the secondary factors using known techniques such as pipelining for
latency hiding, randomization for bank conﬂicts, etc.
The C3 model, proposed by Hambrusch and Khokhar [9] abstracts a pprocessor machine in terms of the communication packet size l, the setup cost
for a message s, communication latency h, and bisection width b. A parallel algorithm is assumed to proceed in supersteps of computation and communication.
The cost of the computation phase of a superstep is determined by the maximum
number of data items accessed by any processor (expressed in terms of number
of packets). The cost of the communication phase is determined by whether
the communication is blocking or non-blocking. A blocking communication of m
packets is charged a time 2(s + h) + m + h at the sender and s + 2h + m at the
receiver. A non-blocking communication of m packets is charged a time s+m+h
at the sender and m at the receiver. Communication congestion is modeled in
terms of network congestion and processor congestion. This is determined by the
number of processor pairs communicating c, and the average number of packets r being communicated by any pair. The network congestion Cl is given by
Cl = (r × c)/b and the processor congestion Cp by Cp = (r × c × h)/p. The overall communication cost is the maximum of sender time, receiver time, processor
congestion, and network congestion.
While both of these models are reﬁnements of BSP and LogP models, they
suﬀer from many of the same drawbacks as LogP and BSP models.
The A3 model [8] expresses communication cost as a combination of the
architecture and underlying permutation. It classiﬁes commonly used communication operations into those that are sensitive to the bisection bandwidth, and
those that are not. It then uses the bisection width of the network to derive
appropriate costs for aggregate communication operations. Algorithm analysis
is reduced to the process of a table lookup for accurate or approximate cost
associated with each operation. It is shown that this model has good coverage
in algorithm in architecture space and has excellent predictive properties.

4

Concluding Remarks

In this paper, we presented a brief survey of prominent parallel algorithm design
models. As is evident, in spite of extensive research, the search for a universal
parallel algorithm design model has had only limited success. Advances in this
area have tremendous signiﬁcance, not just for parallel architectures, but also
for serial processors with deep memory hierarchies.

References
1. A. Bar-Noy and S. Kipnis. Designing Broadcasting Algorithms in the Postal Model
for Message-Passing Systems. Proceedings of 4-th ACM Symp. on Parallel Algorithms and Architectures, pp. 13-22, 1992.

608

A. Grama et al.

2. Armin Bumker, Wolfgang Dittrich, Friedhelm Meyer auf der Heide. Truly Eﬃcient
Parallel Algorithms: 1-optimal Multisearch for an Extension of the BSP Model.
Theoretical Computer Science, 203(2): 175-203, 1998.
3. A. Agarwal, A. K. Chandra, and M. Snir. Communication complexity of PRAMs.
Technical Report RC 14998 (No.64644), IBM T.J. Watson Research Center, Yorktown Heights, NY, 1989.
4. A. Agarwal, A. K. Chandra, and M. Snir. On communication latency in PRAM
computations. Technical Report RC 14973 (No.66882), IBM T.J. Watson Research
Center, Yorktown Heights, NY, 1989.
5. D. Culler, R. Karp, D. Patterson, A. Sahay, K.E. Schauser, E. Santos, R. Subramonian, and T. von Eicken. LogP: Towards a Realistic Model of Parallel Computation.
Proceedings of 4-th ACM SIGPLAN Symp. on Principles and Practices of Parallel
Programming, pp. 1-12, 1993.
6. Pilar de la Torre and Clyde P. Kruskal. Towards a single model of eﬃcient computation in real parallel machines. In Future Generation Computer Systems, pages
395 – 408, 8(1992).
7. F. Dehne, A. Fabri, and A. Rau-Chaplin. Scalable parallel computational geometry for coarse grained multicomputers. International Journal on Computational
Geometry, Vol. 6, No. 3, 1996, pp. 379 - 400.
8. Ananth Grama, Vipin Kumar, Sanjay Ranka, and Vineet Singh. A 3 : A simple
and asymptotically accurate model for parallel computation. In Proceedings of
Conference on Frontiers of Massively Parallel Computing, page 8 pp, Annapolis,
MD, 1996.
9. S. Hambrusch and A. Khokhar. C3 : A parallel model for coarse-grained machines.
Journal of Parallel and Distributed Computing, 32(2):139–154, Feb 1996.
10. T. Heywood and S. Ranka. A practical hierarchical model of parallel computation.
i. the model. Journal of Parallel and Distributed Computing, 16(3):212–32, Nov.
1992.
11. T. Heywood and S. Ranka. A practical hierarchical model of parallel computation.
ii. binary tree and ﬀt algorithms. Journal of Parallel and Distributed Computing,
16(3):233–49, Nov. 1992.
12. Vipin Kumar, Ananth Grama, Anshul Gupta, and George Karypis. Introduction
to Parallel Computing: Algorithm Design and Analysis. Benjamin Cummings/
Addison Wesley, Redwod City, 1994.
13. Vijaya Ramachandran. QSM: A General Purpose Shared-Memory Model for Parallel Computation. FSTTCS 1-5, 1997.
14. A. G. Ranade. How to emulate shared memory. In Proceedings of the 28th IEEE
Annual Symposium on Foundations of Computer Science, pages 185 – 194, 1987.
15. R. Shankar and S. Ranka. Random data access on a coarse grained parallel machine
ii. one-to-many and many-to-one mappings. Technical report, School of Computer
and Information Science, Syracuse University, Syracuse, NY, 13244, 1995.
16. L. G. Valiant. A bridging model for parallel computation. Communications of the
ACM, 33(8), 1990.
17. L. G. Valiant. General purpose parallel architectures. Handbook of Theoretical
Computer Science, 1990.
18. J. S. Vitter and E. A. M. Shriver. Algorithms for Parallel Memory II: Hierarchical
Multilevel Memories. newblock Algorithmica, 12(2-3), 1994, 148-169.
19. T. von Eicken, D. E. Culler, S. C. Goldstein, and K. E. Schauser. Active messages:
a mechanism for integrated communication and computation. In Proceedings of
the 19th International Symposium on Computer Architecture, 1992.

