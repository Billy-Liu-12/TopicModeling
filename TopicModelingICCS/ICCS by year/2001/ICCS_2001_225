TRaDe: Data Race Detection for Java
Mark Christiaens1 and Koen De Bosschere1
ELIS, Ghent University,
Sint-Pietersnieuwstraat 41, 9000 Gent, Belgium,
{mchristi,kdb}@elis.rug.ac.be

Abstract. This article presents the results of a novel approach to race
detection in multi-threaded Java programs. Using a topological approach
to reduce the set of objects needing to be observed for data races, we have
succeeded in reducing signiﬁcantly the time needed to do data race detection. Currently, we perform data race detection faster than all known
competition. Furthermore, TRaDe can perform data race detection for
applications with a high, dynamically varying number of threads through
the use of a technique called ‘accordion clocks’.

1

Introduction

The technique of multi-threaded design has become a permanent part of almost
every serious programmer’s toolbox. When applying a multi-threaded design to
a programming task, a task is split into several threads. Each of these subtasks
can be executed on a separate processor, increasing parallelism and speed. A
group of threads can be dedicated to respond to external events, so a very fast
response time can be obtained using this design.
Together with the advantages of multi-threaded design, a new type of bug
has appeared which is very hard to handle: a data race. As each thread performs
its task in the program, it modiﬁes data structures along the way. It is up to
the programmer to ensure that the operations of threads happen in an orderly
fashion. If the threads do not behave properly, but modify a data structure in
a random order, a data race occurs. An example of a data race can be seen in
Figure 1.
Data Races are very hard to locate. They are non-deterministic, since their
occurrence is dependent on the execution speed of the threads involved. On
top of that, data races are non-local. Two pieces of code whose only relation
is that they both utilize the same data structures, independently can function
perfectly but when executed simultaneously, they will wreak havoc. Finding or
even suspecting a data race can take days of debugging time.

2

State of the Art in Data Race Detection

In general, ﬁnding data races is at least an NP-hard problem [9], so usually,
techniques are proposed to detect data races in one particular execution and
V.N. Alexandrov et al. (Eds.): ICCS 2001, LNCS 2074, pp. 761–770, 2001.
c Springer-Verlag Berlin Heidelberg 2001
�

762

M. Christiaens and K. De Bosschere

T2

T1

T2

T1

A=5

A=6

A=6

A=5

Fig. 1. On the left we see thread 2 accessing a common object, A, and writing the value
5. This is followed by thread 1, writing the value 6 to A. The result of this operation
is that A contains the value 6. On the right, thread 1 for some reason executes faster
which results in the same events happening but in reverse order. This is possible since
there is no synchronisation between thread 1 and 2. A now contains the value 5.

not in all possible executions of a program. Several systems for detecting data
races for general programs have been devised [10,11]. These techniques can ﬁnd
some types of data races (their deﬁnitions of data race diﬀer) but have the
disadvantage of being very time consuming. A time overhead of a factor of 30
is not uncommon. This time overhead is caused by the fact that every read and
write operation performed by the threads is observed.
Recently, modern programming languages with a more object oriented philosophy have seen the light. One of these is the Java programming language
designed by Sun [8,13] at the beginning of the 1990s. Java was designed with
multi-threading in mind. Several language constructs are dedicated to the manipulation of threads. Since it is so easy to make a multi-threaded program in
Java, the data race problem manifests itself even more frequently than in the
usual C-programs.
It is therefore no coincidence that for this language, already two tools exists
that detect data races in particular executions of Java programs: AssureJ and
JProbe [6,7]. Here too, the problem of overhead pops up. A time overhead of
at least a factor 10 is common. This is smaller than for general programs because each thread in Java has a local stack. The values on this stack can only
be manipulated by the thread owning the stack. Therefore all read and write
operations performed on this stack need not be observed to ﬁnd data races.

3

Topological Race Detection

Java has a speciﬁc property that can be exploited to make race detection even
more eﬃcient. The only way an object can be manipulated is through a reference
to that object. Pointers do not exist so the problem that through a pointer every
location in memory can be modiﬁed, does not exist.
It is therefore possible to determine exactly which objects can be reached by
a thread at a certain point in an execution and which cannot. Objects that are

TRaDe: Data Race Detection for Java

763

reachable by only one thread cannot be involved in a data race since no other
thread can simultaneously be modifying that object.
In Java we have exploited this fact as follows. Initially, at an object’s construction through invoking one of the bytecodes new, newarray, anewarray and
multianewarray, the only reference to it is stored on the stack of the thread
creating the object. No other threads can access this reference, therefore at this
point this object cannot be involved in a data race.
From that point forward, this reference can be manipulated by the thread,
maybe even be stored in an object or a class. We have devised a system, called
TRaDe (Topological RAce Detection), to detect dynamically when an object
becomes reachable by more than one thread i.e. the object becomes global. It
constantly observes all manipulations of references which alter the interconnection graph of the objects used in a program (hence the name “topological” of
the method).
Using this classiﬁcation mechanism, we can detect data races more eﬃciently.
Only for objects that are reachable by more than one thread, is full data race
detection necessary. This means that we save execution time and memory by not
analyzing the read and write operation to these objects and by not maintaining
data structures for race detection. During the last year we have implemented
our ideas by modifying the interpreter (and shutting down the JIT compiler)
contained in the source of the JDK1.2.1 from Sun so that it detects data races.
To measure the performance of this implementation, we collected a set of applications consisting of a number of large programs to be used as a benchmark (see
Table 1).
Table 1. Description of the Java benchmarks used
Name
SwingSet
Jess
Resin
Colt
Raja

Description
A highly multi-threaded demo of the Swing widget set
A clone of the expert system shell CLIPS
Web server entirely written in Java
Open source libraries for high performance scientiﬁc and technical computing
Ray tracer

In Table 2, we have measured the execution time and memory consumption
of the benchmarks while doing race detection with TRaDe, AssureJ and JProbe.
For comparison, we have added the execution time when executing with a modern JIT compiler, the Hotspot 1.0.1 build f and with the interpreter version on
which TRaDe itself is based.
The benchmarks were performed on a Sun Ultra 5 workstation with 512 MB
of memory and a 333 MHz UltraSPARC IIi with a 16 KB L1 cache and a 2 MB L2
cache. JProbe could not complete all benchmarks. In this case, the benchmarks
were run on a larger system to get an indication of JProbe’s performance. The

764

M. Christiaens and K. De Bosschere

larger system was a Sun Enterprise 450 with 2048 MB of memory, 4X400 Mhz
UltraSPARC IIi processors with 16 KB L1 direct mapped cache and 4 MB L2
direct mapped cache. These measurements are indicated with a †.
As can be seen in Table 2, using TRaDe, we can outperform all known competition. TRaDe is approximately a factor 1.6 faster than AssureJ, the fastest
competition. AssureJ is the closest competitor in terms of speed and usually
beats us in terms of memory consumption. We must note however that we have
found AssureJ not to detect all data races. When a thread is being destroyed,
AssureJ apparently prematurely removes all information relating to this thread
although future activities of other threads can still cause a data race with activities of this thread.
Table 2. Execution time and memory consumption of the Java benchmarks. At the
bottom, averages and normalized averages (with TRaDe=1) are given for all systems
except JProbe who couldn’t perform all benchmarks.
TRaDe
s
MB
SwingSet
98.3 126.6
Jess
370.3 12.1
Resin
56.5 27.6
Colt
132.5 25
Raja
204.8 19.5
average
172.48 42.16
normalized 1
1

4

AssureJ
s
MB
160.6 73.3
610 17.5
68 27.3
187.8 21.2
372 17.8
279.68 31.42
1.62 0.75

JProbe
s
MB
>1200† >650 †
>3600† >650†
193 226.17
471.6
71
1945† 1037 †

HotSpot
s
MB
20 41.8
22 19.8
11.8 27.9
27.8 23.6
14.6 24.5
19.24 27.51
0.11 0.65

Interpreter
s
MB
15
29.8
76
8
10
13.7
40.4 13.5
42.2 11.3
36.72 15.24
0.21 0.36

Accordion Clocks

A second problem we have tackled in TRaDe is the problem of the dynamic creation of threads. Many applications continuously create new threads: a browser
updating a picture in a web page, an editor checking for modiﬁed ﬁles on disk,
etc. To explain why this is a problem, we have to delve a little deeper in how
data race are detected.
In parallel shared-memory programs there are two types of races: data races
and synchronisation races. An example of a data race was already given in Figure 1.
In Figure 2, we see the other type of race, a synchronisation race. A synchronisation race occurs when two threads perform a synchronisation operation (like
obtaining a mutex, a P operation on a semaphore, ...) on the same synchronisation object in a non-determined order.
The set of operations that are performed by a thread between two consecutive synchronisation operations is called a segment (e.g. S1 , S2 , S3 , S4 , . . . in
Figure 2).

TRaDe: Data Race Detection for Java

765

Fig. 2. The threads enter a mutex (indicated by the opening and closing brackets)
before modifying the common variable. The ﬁnal result of the execution is determined
by which thread enters the mutex ﬁrst. The order in which the threads access the
mutex is indicated by the solid arrows.

Synchronisation primitives induce a partial order on the segments. For example, in Figure 2, thread T2 is able to enter the mutex ﬁrst. Thread T1 must
wait until T2 leaves the mutex. Therefore, segment S3 is ordered before S2 . We
syn
write S3 → S2 .
In addition, segments are partially ordered simply by the fact that they are
sequentially executed by their thread. Segment S1 for example is executed before
seq
S2 by thread T1 . We write S1 → S2 .
Using these two partial orders, we can deﬁne the “execution order” on two
segments Sx and Sy as the transitive closure of the union of the two previous
relations:
syn

seq

Sx → Sy ≡ Sx → Sy ∨ Sx → Sy ∨ ∃Sz .(Sx → Sz ∧ Sz → Sy )

(1)

Two segments are said to be “parallel” if there is no execution order between
them i.e.
(2)
Sx � Sy ≡ ¬((Sx → Sy ) ∨ (Sy → Sx ))
A data race occurs when operations from two parallel segments access a
common variable and at least one operation is a write operation. If we deﬁne the
set of locations written to resp. read from by a segment S as W (S) and R(S)
then a data race occurs between two segments, Sx and Sy , when the following
two conditions hold:
S x � Sy
(R(Sx ) ∪ W (Sx )) ∩ W (Sy ) � =∅ ∨ (R(Sy ) ∪ W (Sy )) ∩ W (Sx ) � =∅

(3)

Checking in practice whether or not two segments are ordered can be done
using a construct called a “vector clock” [12], [4],[5]. A vector clock is essentially just an array of integers (one for every thread in the program) that is
updated each time a synchronisation operation occurs. A comparison function,

766

M. Christiaens and K. De Bosschere

<, is deﬁned for vector clocks, V1 and V2 , as follows
V1 < V2 ≡ ((∀i.V1 [i] ≤ V2 [i]) ∧ V1 � =V2 )

(4)

It has the nice property of being a mapping of the execution order i.e. if V C is
a function producing the vector clock of a segment then the following holds:
Sx → Sy ≡ V C(Sx ) < V C(Sy )

(5)

In [3] it is proven that the size of a vector clock is proportional to the maximum number of parallel threads in the multi-threaded program. In general, we
can’t just reduce the size of the vector clock when a thread dies.
Why this is the case can be seen in Figure 3. We see a simple FTP-server
which creates a new slave thread for each ﬁle request. Depending on the size
of the ﬁle to transfer, the bandwidth of the network connection to the client
and many other factors, this slave thread can take a variable amount of time to
ﬁnish. When the slave thread has ﬁnished transferring the ﬁle, it is destroyed.

T1
T2

e1

T3
e2
...

Fig. 3. An FTP-server spawning new threads for every ﬁle request.

Clearly, over a long period of time, only a small fraction of the slave threads
will run concurrently. Still, the size of the vector clock will be equal to the total
number of threads executed by the FTP-server. Indeed, if we would try to remove
the information about the execution of thread T2 when the thread has ﬁnished,
we would run the risk of no longer detecting all data races.
Take for example event e1 on thread T2 . Even when thread T2 has ﬁnished
execution, this event can still cause a race with another event e2 on thread T3 .
Indeed, if T2 had run a little slower, e2 would have been executed before e1 ,
which clearly constitutes a data race. The cause for this data race is that T2
simply dies when it has ﬁnished its task without synchronizing with any other
thread. Therefore, the risk for data races with event e1 will continue to exist
indeﬁnitely and we can therefore never remove T2 ’s index from the vector clock.

TRaDe: Data Race Detection for Java

767

We have devised a new approach called ’accordion clocks’ which allows us
to surmount the problem of the ever growing size of vector clocks. The idea
behind accordion clocks is simple. Suppose a thread has ﬁnished its execution.
In general we cannot yet remove its index from the vector clocks since we might
still need it to verify if another thread accesses the data structures touched by
the dead thread in a not properly synchronized manner. If however we detect
that a thread has ended and there are no data structures left which were touched
by this thread then we can safely remove the index from the vector clock since
no races are possible anymore.
Accordion clocks do just that. Initially, at program startup, all accordion
clocks contain one index since only one thread is active. As threads are created
during program execution the accordion clocks are expanded transparently to
include new indices for the new threads. If a thread dies, this is noted and
periodically all objects on the Java heap are checked to see whether there are
any left which were once accessed by this deceased thread. If there are none
left, the index corresponding to the deceased thread is removed transparently
from all the accordion clocks in the system. The time overhead of this check is
comparable to the time overhead of a full garbage collection cycle.
We have implemented accordion clocks in TRaDe and tested them using the
benchmarks described in Table 3. The results can be seen in Figure 4. The total
memory used for storing all data structures to do data race detection is shown
as it evolves in time. We have divided the memory consumption in two part: the
top part indicates the memory used for accordion clocks and vector clocks. The
bottom part is used for other per object data structures to perform data race
detection. On the left, we see the execution of the benchmarks without using
accordion clocks. On the right accordion clocks are used.
It is clear that after a short period of time the memory used for storing
accordion clocks becomes overwhelming. This prohibits searching data races in
long running applications. When using accordion clocks, the memory consumed
for storing clocks levels oﬀ after a while. For some applications, like Forte and
Java2D, memory consumption seems to level oﬀ to a constant cost. For Swing,
accordion clocks are clearly beneﬁcial but some work will have to be done to
stop the other data structures expanding.

5

Conclusions

In this article we have shown that, using a topological approach, it is possible to
more eﬃciently detect data races in Java. We have implemented this technique
in a Java virtual machine and have found it superior in execution speed to
competing systems. Furthermore, we have found that a large part of the memory
consumption while doing data race detection is due to storing vector clocks. We
have solved the problem of the increasing size of vector clocks by introducing
accordion clocks. Accordion clocks can grow and shrink as threads are created
and destroyed while maintaining correct data race detection.

768

M. Christiaens and K. De Bosschere

Fig. 4. Evolution of memory consumption during an execution of TRaDe without (on
the left) accordion clocks and with (on the right) accordion clocks.

TRaDe: Data Race Detection for Java

769

Table 3.
Name
Description
SwingSet Demo A highly multi-threaded demo, included with the JDK
1.2, of the Swing widget set. It demonstrates buttons,
sliders, text areas, ... Every tab in the demo was clicked
twice during our tests. Immediately thereafter it was shut
down.
Java2D Demo A highly multi-threaded demo, included with the JDK
1.2. It demonstrates the features of the Java 2D drawing
libraries and is highly multi threaded. Every tab in the
demo was clicked 5 times. Immediately thereafter it was
shut down.
Forte for Java A full-blown Java IDE with object browsers, visual construction of GUI code, ... To exercise the benchmark, we
created a small dialog while doing race detection.

6

Future Work

We are currently exploring whether it is possible to do data race detection by
instrumenting the bytecode of class ﬁles. Recently, a lot of research has been
devoted to escape analysis [2,1]. This is a technique allowing to detect at compile
time that an object will be local to a thread. We believe that this technique could
be used to reduce the number of memory accesses that need to be observed
thereby speeding up data race detection even further.

Acknowledgments
Mark Christiaens is supported by the GOA project 12050895. Our thanks go to
Michiel Ronsse, with whom we had many stimulating discussions.

References
1. J. Aldrich, C. Chambers, E. G. Sirer, and S. Eggers. Static analyses for eliminating
unnecessary synchronization from Java programs. In Static Analysis Symposium
99, September 1999.
2. Jeﬀ Bogda and Urs Hölzle. Removing unnecessary synchronisation in Java .
Technical report, Department of Computer Science, University of California, Santa
Barbara, CA 93106, april 1999.
3. Bernadette Charron-Bost. Concerning the size of logical clocks in distributed systems. Information Processing Letters, 1(39):11–16, July 1991.
4. C.J. Fidge. Partial orders for parallel debugging. In Proceedings of the ACM
SIGPLAN and SIGOPS Workshop on Parallel and distributed debugging, pages
183–194, May 1988.
5. Dieter Haban and Wolfgang Weigel. Global events and global breakpoints in distributed systems. In 21st Annual Hawaii International Conference on System
Sciences, volume II, pages 166–175. IEEE Computer Society, January 1988.

770

M. Christiaens and K. De Bosschere

6. KL Group, 260 King Street East, Toronto, Ontario, Canada. Getting Started with
JProbe.
7. Kuck & Associates, Inc., 1906 Fox Drive, Champaign, IL 61820-7345, USA. Assure
User’s Manual, 2.0 edition, march 1999.
8. Tim Lindholm and Frank Yellin. The Java Virtual Machine Speciﬁcation. The
Java Series. Addison-Wesley, 1997.
9. R. H. B. Netzer. Race Condition Detection for Debugging Shared-Memory Parallel
Programs. PhD thesis, University of Wisconsin-Madison, 1991.
10. Michiel Ronsse and Koen De Bosschere. JiTI: Tracing Memory References for
Data Race Detection. In E. D’Hollander, F.J. Joubert, and U. Trottenberg, editors,
Proceedings of ParCo97: Parallel Computing: Fundamentals, Applications and New
Directions, volume 12 of Advances in Parallel Computing, pages 327–334, Bonn,
February 1998. North Holland.
11. Stefan Savage, Michael Burrows, Greg Nelson, Patrick Sobalvarro, and Thomas
Anderson. Eraser: A dynamic data race detector for multi-threaded programs. In
Operating Systems Review, volume 31, pages 27–37. ACM, October 1997.
12. Reinhard Schwarz and Friedman Mattern. Detecting causal relationships in distributed computations: in search of the holy grail. Distributed Computing, 7(3):149–
174, 1994.
13. Bill Venners. Inside the Java Virtual Machine. McGraw-Hill, New York, New
York, USA, second edition, 1999.

