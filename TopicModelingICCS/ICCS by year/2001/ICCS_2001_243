Finding Steady State of Safety Systems Using
the Monte Carlo Method
Ray Gallagher
rayg@csc.liv.ac.uk
Department of Computer Science, University of Liverpool
Chadwick Building, Peach Street,
Liverpool, UK

Abstract In this paper we consider ﬁnding the steady state of a safety
system. We present parallel Monte Carlo Algorithms for solving eigenvalue
problems which we employ to ﬁnd the steady state.
The algorithms run on a cluster of workstations under MPICH. Examples
would be drawn from safety systems such as controllers etc.
Keywords: Monte Carlo Algorithms, Markov Chain, Parallel Algorithms

1

Introduction

Markov modelling techniques have been used extensively in reliability engineering, particularly with regard to repairable systems that are typical
in an industrial environment. These systems oﬀer many advantages in
terms of system availability and safety. We will show that the problem of
reliability of the system considered can be reduced to ﬁnding the dominant eigenvalue(s). Thus, generally, the problem transposes to a linear
algebra problem.
Let us now consider Monte Carlo menthods for Linear Algebra. Monte
Carlo methods give statistical estimates for the functional of the solution
by performing random sampling of a certain random variable whose mathematical expectation is the desired functional. They can be implemented
on parallel machines eﬃciently due to their inherent parallelism and loose
data dependencies. Using powerful parallel computers it is possible to apply Monte Carlo methods for evaluating large-scale irregular problems,
which are sometimes diﬃcult to solve by well-known numerical methods.
Let J be any functional that we estimate by the Monte Carlo method;
θN be the estimator, where N is the number of trials. The probable error
for the usual Monte Carlo method [7] is deﬁned as parameter rN for which
P r{|J −θN | ≥ rN } = 1/2 = P r{|J −θN | ≤ rN }. If the standard deviation
is bounded, i.e. D(θN ) < ∞, the normal convergence in the central limit
theorem holds, so we have
rN ≈ 0.6745D(θN )N −1/2 .
V.N. Alexandrov et al. (Eds.): ICCS 2001, LNCS 2073, pp. 1253−1261, 2001.
© Springer-Verlag Berlin Heidelberg 2001

(1)

1254 R. Gallagher

In this paper we present Monte Carlo algorithms for evaluating the
dominant eigenvalue of large real sparse matrices and their parallel implementation on a cluster of workstations using MPICH and how they
can be used to ﬁnd the steady state of systems. These algorithms use the
idea of the Power method combined with Monte Carlo iterations by the
given matrix and the inverse matrix correspondingly. In [8], [7], [5], [6] one
can ﬁnd Monte Carlo methods for evaluation of the dominant (maximal
by modulus) eigenvalue of an integral operator. In [9], [10] Monte Carlo
algorithms for evaluating the smallest eigenvalue of real symmetric matrices are proposed. Here we generalize the problem. Finding dominant
eigenvalues by parallel Monte Carlo methods can be found in [10],[11]

2
2.1

Background
Algebraic Approach to Calculating Steady State

Assume we are given a transition matrix P . (In most cases of safety
systems such matrices are sparse.) Applying an algebraic approach to
obtain the steady state transition matrix we have P n+1 = P P n = P n P.
Let K = lim P n as n tends to inﬁnity, assuming the limit exists. Then K
is the solution of K = P K and K = KP .
Let us consider evaluating P n . Consider a Markov Chain with a ﬁnite
number of states k, where the transition matrix P is a k × k array (as
above). We assume that the eigenvalues of P are λi , i = 1, 2, ..., n and
P x = λx has non-zero solutions x if, and only if, λ is an eigenvalue of
P . Hence, we can ﬁnd the corresponding right-hand eigenvectors xi of
P . The matrix P can be decomposed into a similar system of matrices
comprising its eigenvalues and eigenvectors P = BΛB−1 . Where Λ is a
diagonal matrix of its eigenvalues λ1 , λ2 ...λn , and where the matrix B is
formed from the corresponding eigenvectors. It follows immediately that
P n = (BΛB −1 )n = BΛn B −1 .
Therefore, we can calculate the steady state of the system. We will
use the Monte Carlo method to calculate the eigenvalues and the method
is described below.
2.2

Power Method.

Suppose A ∈ Rn×n is diagonalizable, X −1 AX = diag(λ1 , . . . , λn ), X =
(x1 , . . . , xn ), and |λ1 | > |λ2 | ≥ . . . ≥ |λn |. Given f (0) ∈ C n , the power
method ([2]) produces a sequence of vectors f(k) as follows:

Finding Steady State of Safety Systems 1255

z (k) = Af (k−1),
f (k) = z (k) /||z(k) ||2 ,
λ(k) = [f (k) ]H Af (k) , k = 1, 2, . . . .
Except for special starting points, the iterations converge to an eigenvector corresponding to the eigenvalue of A with largest magnitude (dominant eigenvalue) with rate of convergence:
(k)

|λ1 − λ

  
 λ2 k
| = O   .
λ

(2)

1

Consider the case when we want to compute the smallest eigenvalue.
To handle this case and others, the power method is altered in the following way: The iteration matrix A is replaced by B, where A and B
have the same eigenvectors, but diﬀerent eigenvalues. Letting σ denote
a scalar, then the three common choices for B are: B = A − σI which
is called the shifted power method, B = A−1 which is called the inverse
power method, and B = (A − σI)−1 which is called the inverse shifted
power method.
Table 1. Relationship between eigenvalues of A and B

B

Eigenvalue Eigenvalue
of B
of A

A−1
A − σI
(A − σI)−1

1
λA

λA − σ
1
λA −σ

1
λB

λB + σ
σ + λ1B

Computational Complexity: Having k iterations, the number of
arithmetic operations in the Power method is O(4kn2 + 3kn), so the
Power method is not suitable for large sparse matrices. In order to reduce
the computational complexity we propose a Power method with Monte
Carlo iterations.

3
3.1

Monte Carlo Algorithms
Monte Carlo Iterations

Consider a matrix A = {aij }ni,j=1 , A ∈ Rn×n , and vectors f = (f1 , . . . , fn )t ∈
Rn×1 and h = (h1 , . . . , hn )t ∈ Rn×1 . The algebraic transformation Af ∈

1256 R. Gallagher

Rn×1 is called iteration and plays a fundamental role in iterative Monte
Carlo methods.
Consider the following Markov chain:
k0 → k1 → . . . → ki ,

(3)

where kj = 1, 2, . . . , n for j = 1, . . . , i are natural numbers. The rules
for constructing the chain (3) are:
|hα |
,
P r(k0 = α) = n
α=1 |hα |
|aαβ |
P r(kj = β|kj−1 = α) = n
, α = 1, . . . , n.
β=1 |aαβ |
Such a choice of the initial density vector and the transition density
matrix leads to almost optimal Monte Carlo algorithms for matrix computations.
Deﬁne the random variables Wj using the following recursion formula:
W0 =

3.2

ak k
hk0
, Wj = Wj−1 j−1 j , j = 1, . . . , i.
pk 0
pkj−1 kj

(4)

Direct Monte Carlo Algorithm

The dominant eigenvalue can be obtained using the iteration process mentioned in the introduction:
λmax = limi→∞

(h, Ai f )
,
(h, Ai−1 f )

where we calculate scalar products having in mind [7], [8], [4] that
(h, Ai f ) = E{Wi fki }, i = 1, 2, . . . .
Thus we have
λmax ≈

E{Wi fki }
E{Wi−1 fki−1 }

(5)

Finding Steady State of Safety Systems 1257

3.3

Balancing of Errors

There are two kind of errors in the Power method with Monte Carlo
iterations:
– systematic error (from the Power method, (2)):
  
 µ2 k
O   ,
µ
1

where µi = λi if B = A, µi = λ1i if B = A−1 , µi = λi −σ if B = A−σI,
µi = λi1−σ if B = (A − σI)−1 and λi and µi are the eigenvalues of A
and B correspondingly;
– stochastic error (because we calculate mathematical expectations approximately, (1)):
O(D(θN )N −1/2 )
To obtain good results the stochastic error must be approximately
equal to the systematic one. It is not necessary to use a large number of
realizations N in order to have a small stochastic error if the systematic
error is large.
3.4

Computational Complexity

Monte Carlo algorithms can be used to reduce the number of required
operations to ﬁnd the dominant eigenvalue. Dimov and Karaivanova have
shown the mathematical expectation of the total number of operations
for the Resolvent MC Method ([9], [10] is:
1
(6)
ET1 (RM C) ≈ 2τ (k + γA )lA + dlL lN + 2τ n(1 + d),
2
where l is the number of moves in every Markov chain, N is the number
of Markov chains, d is the mean value of the number of non-zero elements
per row, γA is the number of arithmetic operations for calculating the
random variable (in our code-realization of the algorithm γA = 6), lA and
lL are arithmetic and logical sub-operations in one move of the Markov
chain, k is the number of arithmetic operations for generating the random
number (k is equal to 2 or 3).
The main term of (6) does not depend on n, the size of the matrix.
This means that the time required for calculating the eigenvalue by RMC
is practically independent of n. The parameters l and N depend on the
spectrum of the matrix, but do not depend on its size n. The above
mentioned result was conﬁrmed for a wide range of matrices during the
realized numerical experiments [10],[11].




1258 R. Gallagher

4

Numerical Tests

The numerical tests are made on a cluster of 48 Hewlett Packard 900 series
700 Unix workstations under MPICH (version 1.1). The workstations are
networked via 10Mb switched ethernet segments and each workstation has
at least 64Mb RAM and run at least 60 MIPS. Each processor executes
the same program for N/p number of trajectories, i.e. it computes N/p
independent realizations of the random variable (here p is the number
of processors). At the end the host processor collects the results of all
realizations and computes the desired value. The computational time does
not include the time for initial loading of the matrix because we consider
our problem as part of a bigger problem (for example, spectral portraits
of matrices) and suppose that every processor constructs it.
The test matrices are sparse and stored in packed row format (i.e. only
nonzero elements). The results for average time and eﬃciency are given
in table 2 and look promising. The relative accuracy is 10−3 [11].
We consider the parallel eﬃciency E as a measure that characterizes
the quality of the proposed algorithms. We use the following deﬁnition:
E(X) =

ET1 (X)
,
pETp (X)

where X is a Monte Carlo algorithm, ETi (X) is the expected value of the
computational time for implementation of the algorithm X on a system
of i processors.

5
5.1

Discussion
A Numerical Method

An alternative numerical method to calculate the steady states is proposed by Goble [3].
Let us consider as an example a 1oo2 Duplex Control System which
has 5 states. We apply a Markov model solution with a numerical approach (Steady State 1oo2 System) using failure rate values given in the
transition matrix [1].
The limiting state probabilities for the system of eventual failure into
state 3 and state 4 may be calculated as follows: The matrix P can be
decomposed to form four sub-matrices of the form shown below:


Q R
P =
null I



Finding Steady State of Safety Systems 1259
Table 2. Direct Monte Carlo Algorithm using MPI (number of trajectories - 100000).

matrix matrix matrix
n=128 n=1024 n =2000
1 pr
T (ms)
2 pr
T (ms)
E
3 pr
T (ms)
E
4 pr
T (ms)
E
5 pr
T(ms)
E

34

111

167

17
1

56
0.99

83
1

11
1.03

37
1

56
1

8
1.06

27
1.003

42
1

7
0.97

21
1.06

35
0.96

Where the upper-left sub-matrix Q is used to calculate the ”Mean
Time to Failure” or MTTF. The lower left matrix is a null matrix that
contains only zeros and the lower right matrix is an ”Identity” matrix.
The upper-right matrix is called the R matrix and relates transition from
the transient states to the absorbing states.

Q=

0.99999005 0.0000039537 0.0000006363
0.125
0.87499450 0.0
0.0
0.0
0.9999945




R=

0.000005105 0.000000255
0.00000295 0.00000255
0.00000295 0.00000255



The matrix P is truncated to obtain a matrix Q formed by crossingout the rows and columns of the absorbing states of transition matrix
P . The Q matrix is then subtracted from the identity matrix I to form
matrix I − Q. The above I − Q matrix is then inverted to obtain the
matrix N .
⎡

⎤

166764.67 5.27 19293.16
⎢
⎥
N = ⎣ 166757.33 13.27 19292.31⎦
0.0 0.0 181818.18

1260 R. Gallagher

Multiplying the N matrix by the R matrix leads to the limiting state
probabilities for the system. (a point where P n+1 = P n and the transition
probabilities will not change for higher powers of matrix P ).
⎡

⎤

0.908264011 0.091735989
⎢
⎥
N R = ⎣ 0.908247648 0.091752352 ⎦
0.536363636 0.463636364
The values in the elements of the matrix N R relate to the R submatrix in P as shown below.
Thus, if the system starts in state ”0” it will end in the state given by
[ 1 0 0 0 0 ] * P = [ 0 0 0 0.908264011 0.091735989 ]
Similarly, starting from state 1, the multiplying row vector would be [
0 1 0 0 0 ] resulting in row 2 of matrix NR, [ 0 0 0 0.908247648 0.091752352
], and starting from state 3, the multiplying row vector would be [ 0 0 1
0 0 ] resulting in row 3 of matrix NR, [ 0 0 0 0.536363636 0.463636364] .
Hence, the limiting state probability that the system fails with outputs de-energised (state 3) is 0.908264011. The limiting state probability the system fails with outputs energised ”fail-to-danger” (state 4) is
0.091735989.
5.2

Comparison of Methods

Whereas, the Monte Carlo solution for the eigenvalues of the system is
applicable to large scale problems, methods such as Goble’s become impracticable. As we have seen the parallel Monte Carlo method can be
used to ﬁnd several of the maximal eigenvalues eﬃciently or, if necessary,
all eigenvalues. In the case where we seek all the eigenvalues then we can
use the spectral resolution representation, and we are able to calculate
the relevant P n and thus the steady state of the system. The method is
advantageous in that it is the magnitude of the underlying eigenvalues
(in particular the dominant eigenvalue(s)) that govern both the manner
and the rate of degeneracy of these systems to their limiting state probabilities.

6

Conclusion

Parallel Monte Carlo algorithms for calculating eigenvalues are outlined.
The application of these methods for the calculation of the steady states
of safety systems is considered.

Finding Steady State of Safety Systems 1261

They can be applied for well balanced matrices (which have nearly
equal sums of elements per row) in order to provide good accuracy. We
propose to use them when it is required to calculate the dominant eigenvalue or several of the largest eigenvalues of very large sparse matrices.
This is because the computational time is almost independent of the dimension of the matrix and also their parallel eﬃciency is superlinear.

7

Acknowledgements

This work was partially supported by the Health and Safety Executive,
UK. Thanks to V. N. Alexandrov for his fruitful discussion and comments.

References
1. R. Gallagher, , MSc Dissertation, Liverpool, 1986.
2. G. H. Golub, Ch. F. Van Loon, Matrix Computations, The Johns Hopkins Univ.
Press, Baltimore and London, 1996.
3. W. M. Goble, Evaluating Conttrol Systems Reliability, Instrumant Society of
America, 1992 pp 235-270.
4. J.H. Halton, Sequential Monte Carlo Techniques for the Solution of Linear Systems,
TR 92-033, University of North Carolina at Chapel Hill, Department of Computer
Science, 46 pp., 1992.
5. G.A. Mikhailov, A new Monte Carlo algorithm for estimating the maximum eigenvalue of an integral operator, Docl. Acad. Nauk SSSR, 191, No 5 (1970), pp.
993 – 996.
6. G.A. Mikhailov, Optimization of the ”weight” Monte Carlo methods (Nauka,
Moscow, 1987).
7. I.M. Sobol, Monte Carlo numerical methods, Nauka, Moscow, 1973.
8. V.S.Vladimirov, On the application of the Monte Carlo method to the ﬁnding of the
least eigenvalue, and the corresponding eigenfunction, of a linear integral equation,
in Russian: Teoriya Veroyatnostej i Yeye Primenenie, 1, No 1 (1956),pp. 113
– 130.
9. I. Dimov, A. Karaivanova and P. Yordanova, Monte Carlo Algorithms for calculating eigenvalues, Springer Lectur Notes in Statistics, v.127 (1998) (H.
Niederreiter, P. Hellekalek, G. Larcher and P. Zinterhof, Eds)), pp.205-220.
10. I. Dimov, A. Karaivanova Parallel computations of eigenvalues based on a Monte
Carlo approach, Journal of MC Methods and Appl., 1998 (to appear).
11. I. Dimov, A. Karaivanova, V. N. Aleksandrov Performance Analysis of Monte
Carlo Algorithms for Eigenvalue Problem, ILAS Conference, Barcelona, 1999.
12. Megson, G., V. Aleksandrov, I. Dimov, Systolic Matrix Inversion Using a Monte
Carlo Method, Journal of Parallel Algorithms and Applications , 3, No 1
(1994), pp. 311-330.

