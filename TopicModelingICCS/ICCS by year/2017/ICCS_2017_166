Available online at www.sciencedirect.com

ScienceDirect

This space is reserved for the Procedia header,
This Procedia
space isComputer
reserved
for 108C
the Procedia
header,
Science
(2017) 1145–1154
This space is reserved for the Procedia header,
This space is reserved for the Procedia header,

do
do
do
do

not
not
not
not

use
use
use
use

it
it
it
it

International Conference on Computational Science, ICCS 2017, 12-14 June 2017,
Zurich, Switzerland

Machine learning models in error and variant detection
Machine learning models in error and variant detection
Machine
learning high-throughput
models in error and
variant detection
high-variation
sequencing
datasets
Machine
learning high-throughput
models in error and
variant detection
high-variation
sequencing
datasets
high-variation
high-throughput
sequencing
datasets
1∗
1
1
Milko Krachunov
, Maria Nisheva1 , and
Dimitar Vassilev
high-variation
high-throughput
sequencing
datasets
1∗
1
Milko Krachunov , Maria Nisheva , and Dimitar Vassilev
Faculty
Faculty
Faculty
Faculty

1∗

1

1

Krachunov
, MariaSofia
Nisheva
, and
Dimitar
Vassilev
of Milko
Mathematics
and Informatics,
University,
5 James
Bourchier
blvd.,1Sofia
1∗
1
Krachunov
, MariaSofia
Nisheva
, and
Dimitar
Vassilev
of Milko
Mathematics
and Informatics,
University,
5 James
Bourchier
blvd., Sofia
Bulgaria
Bulgaria
of Mathematics and Informatics,
Sofia
University, 5 James Bourchier blvd., Sofia
milkok@fmi.uni-sofia.bg
of Mathematics and Informatics,
Sofia
University, 5 James Bourchier blvd., Sofia
milkok@fmi.uni-sofia.bg
Bulgaria
Bulgaria
milkok@fmi.uni-sofia.bg
milkok@fmi.uni-sofia.bg

in
in
in
in

1164,
1164,
1164,
1164,

Abstract
Abstract
In high-variation genomics datasets, such as found in metagenomics or complex polyploid
Abstract
In
high-variation
genomics
datasets,
such as
foundareinimpeded
metagenomics
or complex
polyploid
genome
analysis, error
detection
and variant
calling
by the difficulty
in discerning
Abstract
In
high-variation
genomics
datasets,
such
as
found
in
metagenomics
or
complex
genome
analysis,
error
detection
and
variant
calling
are
impeded
by
the
difficulty
in
discerning
sequencing errors from actual biological variation. Confirming base candidates with polyploid
high freIn
high-variation
genomics
datasets,
such
as
found
inimpeded
metagenomics
or complex
polyploid
genome
analysis,
and
variant
calling
are
by the
difficulty
in discerning
sequencing
errorserror
from
actual
biological
variation.
Confirming
candidates
with
high
quency of
occurrence
isdetection
no longer
a reliable
measure,
because
ofbase
the
natural
variation
and frethe
genome
analysis,
error
detection
and
variant
calling Confirming
are
impeded
by the
difficulty
in discerning
sequencing
errors
from
actual
biological
variation.
base
candidates
with
high
quency
of
occurrence
is
no
longer
a
reliable
measure,
because
of
the
natural
variation
and frethe
presence of rare bases.
sequencing
errors
fromis actual
biological
variation.
Confirming
base
candidates
with high
frequency
of
occurrence
no
longer
a
reliable
measure,
because
of
the
natural
variation
and
the
presence
of
rare
bases.
This work employs machine learning models to classify bases into erroneous and rare variquency
ofwork
occurrence
ismachine
no longer
a reliable
measure,
because
of the
natural
variation
andvarithe
presence
of
rare
bases.
This
employs
learning
models
to
classify
bases
into
erroneous
and
rare
ations, after preselecting potential error candidates with a weighted frequency measure, which
presence
of
rare
bases.
This
workpreselecting
employs
machine
learning
models
to inter-sequence
classify
bases into
erroneous
and rare
variations,
potential
errorbycandidates
with a weighted
frequency
measure,
which
aims
to after
focus
on
unexpected
variations
using the
pairwise
similarity.
Different
This
workpreselecting
employs
machine
learning
models
to inter-sequence
classify
bases into
erroneous
and rare
variations,
after
potential
error
candidates
with
a
weighted
frequency
measure,
which
aims
to
focus
on
unexpected
variations
by
using
the
pairwise
similarity.
Different
similarity measures are used to account for different types of datasets. Four machine learning
ations,
after
preselecting
potential
error
candidates
with
a
weighted
frequency
measure,
which
aims
to are
focus
on unexpected
using
the inter-sequence
pairwise
similarity.
similarity
measures
are usedvariations
to accountbyfor
different
types of datasets.
Four
machine Different
learning
models
tested.
aims
to are
focus
on unexpected
using
the inter-sequence
pairwise
similarity.
similarity
measures
are usedvariations
to accountbyfor
different
types of datasets.
Four
machine Different
learning
models
tested.
Keywords:
machine
learning,
discovery,
calling,
metagenomics,
genomes
similarity
measures
are used
to account
different
types
of datasets. polyploid
Four machine
learning
©
2017 The
byerror
Elsevier
B.V. forvariant
models
areAuthors.
tested.Published
Keywords:
machine
learning,oferror
discovery,
variantofcalling,
metagenomics,
polyploid
genomes Science
Peer-review
responsibility
the scientific
committee
the International
Conference
on Computational
models areunder
tested.
Keywords: machine learning, error discovery, variant calling, metagenomics, polyploid genomes
Keywords: machine learning, error discovery, variant calling, metagenomics, polyploid genomes

1 Introduction
1 Introduction
1
A lot Introduction
of genomics research has to deal with high-variation datasets. This includes metage1
A
lot Introduction
of[19],
genomics
has to
This includes
metagenomics
where research
it is common
to deal
workwith
with high-variation
the genomes ofdatasets.
multiple organisms
in the
same

A
lot of[19],
genomics
research
has[13]
to
deal
with
high-variation
This
metagenomics
where
it isspecies
common
to that
workhave
withmore
the genomes
multiple
organisms
in the
same
sample,
and
polyploid
than oneofdatasets.
similar
and
hardincludes
to differentiate
A
lot of[19],
genomics
research
has[13]
to
deal
with
high-variation
datasets.
This
includes
metagenomics
where
it
is
common
to
work
with
the
genomes
of
multiple
organisms
in
the
same
sample,
and
polyploid
species
that
have
more
than
one
similar
and
hard
to
differentiate
subgenomes.
nomics
[19],
where
it isspecies
common
to that
workhave
withmore
the genomes
ofsimilar
multiple
organisms
in the same
sample,
and
polyploid
[13]
than
one
and
hard
to
differentiate
subgenomes.
Metagenomics studies micro-organism communities in environmental samples. These range
sample,
and polyploid species
[13] that have
more thanin one
similar andsamples.
hard to These
differentiate
subgenomes.
Metagenomics
micro-organism
communities
from
soil samples instudies
agriculture
[11] to human
microbiome
inenvironmental
health research
[14]. Whether range
these
subgenomes.
Metagenomics
micro-organism
communities
ininenvironmental
samples.
These
range
from
soilhave
samples
instudies
agriculture
human
microbiome
health
research
[14].onWhether
these
studies
a specific
focus
on[11]
thetoindividual
variations,
or
a general
focus
the
diversity
Metagenomics
studies
micro-organism
communities
ininenvironmental
samples.
These
range
from
soil
samples
in
agriculture
[11]
to
human
microbiome
health
research
[14].
Whether
studies
have
a
specific
focus
on
the
individual
variations,
or
a
general
focus
on
the
diversity
and interspecies evolutionary relations, they all face difficulties because of the errors inthese
the
from
soilhave
samples
in agriculture
[11]
toindividual
human
microbiome
in or
health
research
[14].
studies
a specific
a general
focus
onWhether
the
diversity
and
interspecies
evolutionary
relations,
they
allvariations,
face
difficulties
because
of
the
errors
inthese
the
datasets
[9], which
has focus
lead toon
a the
common
practice
of discarding
entire
sequences
suspected
to
studies
have
a specific
focus
on
the
individual
variations,
or a general
focus
on errors
the
diversity
and
interspecies
evolutionary
relations,
they
all
face
difficulties
because
of
the
in
the
datasets
[9],
which
has
lead
to
a
common
practice
of
discarding
entire
sequences
suspected
to
contain errors [6].
and
interspecies
evolutionary
relations,
they
all face
difficulties entire
because
of the errors
in the
datasets
[9],
which
has
lead
to
a
common
practice
of
discarding
sequences
suspected
to
contain
errors
[6].
With the development of cheaper and faster sequencing methods, this approach is becoming
datasets
[9],
which
has lead
to
a common
practice
of discarding
entire
sequences
suspected
to
contain
errors
[6].
With
the
development
of
cheaper
and
faster
sequencing
methods,
this
approach
is
becoming
excessively prohibitive and unsuitable. New research efforts have focused on mapping the
contain
errors
[6].
With theprohibitive
development
of unsuitable.
cheaper and faster
sequencing
methods,
approach
is becoming
excessively
and
New research
efforts
have this
focused
on mapping
the
∗ With
To whom
correspondence
should
be addressed
theprohibitive
development
of unsuitable.
cheaper
and faster
sequencing
methods,
this
approach
is becoming
excessively
and
New
research
efforts
have
focused
on
mapping
the
∗ To whom correspondence should be addressed
excessively
prohibitive and unsuitable. New research efforts have focused on mapping the
∗ To whom correspondence should be addressed
1
∗ To whom correspondence should be addressed
1
1
1877-0509 © 2017 The Authors. Published by Elsevier B.V.
1
Peer-review under responsibility of the scientific committee of the International Conference on Computational Science
10.1016/j.procs.2017.05.242

1146	

Machine learning in error detection
Krachunov,
Nisheva and Vassilev
Milko Krachunov et al. / Procedia Computer Science 108C
(2017) 1145–1154

microbiome of urban environments across the world [18], requiring the sequencing and analysis
of a vast amount of samples, collected in countless urban environments. These are increasingly
relying on nanopore sequencing, whose speed and price had come at the expense of quality—
nanopore error rates average 5–8% [16], with some studies measuring errors of up to 40% [10].
This necessitates the use of better error detection and correction approaches, as the assurance
of quality has to be moved from the hardware platform to the software post-processing stage.
Problems of similar nature are encountered by the researchers who analyse polyploid
genomes. An example of this is the hexaploid bread wheat, or common wheat. It has six
sets of chromosomes inherited from three distinct species that diverged 7 and 5–6 million years
ago, and merged again in two hybridisation events that happened 5800–8200 and 2300–4300
years ago [13]. As a result, any computational research has to process and analyse three copies
of the genome, which in diverged genes and loci may be easy to differentiate, but in conserved
loci may be only identifiable by subtle differences, hard to discern from the occasional errors.
The complex and unstudied nature of the datasets makes them a suitable target for the use
of machine learning (ML), which allows the creation of models that incorporate properties that
are not yet explored, or difficult to formalise.

2

Proposed error detection methods

This paper presents two ways to discern natural variation from errors of the sequencing procedure. The first is an analytical method that is used to select the suitable error candidates. It
uses similarity weights between the pairs of sequences to find the bases which are rare among
fuzzy groupings of close sequences. The selected candidates are then filtered with a classification
model that is based on an appropriate ML method, and rests on frequency vectors.
For the analytical approach, a weighted frequency function is introduced, and it is used
together with a pairwise similarity function. Given a particular base in a particular sequence
read, this weighted frequency answers the question—which bases are infrequent in the fuzzy set
of sequences that are similar to the evaluated one? Two accompanying similarity functions are
proposed and tested for metagenomics, and another one—for hexaploid wheat.
The similarity weights alone aren’t enough to precisely isolate the errors, which is why the
candidates need to be filtered using a ML-based classification model. Artificial neural networks
(ANN) [17] were initially picked as the primary model, because they can be trained to produce
a numerical value proportional to the error probability, instead of a simple binary classification,
and that could allow us to use these results inside a more complex fuzzy error model.
In addition, a random forest (RF) [1], a Hoeffding tree (HT) [5], and RIPPER propositional
rule learner [2] have been tested in two separate computational experiments.

3

Analytical approach for error candidate selection

The base approach for picking error candidates is to select bases by their frequency, using their
similarity as a weight. The following weighted frequency function was introduced in [8].
sweighted (r, k) =

p=r

sim(r, p, k)[rk = pk ]
p=r
p∈R sim(r, p, k)

p∈R

(1)

It computes the weighted frequency of the base rk at position k in read r, among all reads
in the dataset R. It focuses on the reads similar to r using the similarity function sim(r, p, k).
Error candidates—bases rare among those reads—are selected by comparing (1) to a threshold.
2

	

Machine learning in error detection
Krachunov,
Nisheva and Vassilev
Milko Krachunov et al. / Procedia Computer Science 108C
(2017) 1145–1154

3.1

Similarity weights for metagenomics

For the processing of metagenomics data, consisting of medium-long reads (300-500 bases) of
the same region, it seemed prudent to compute the similarity on the bases close to the evaluated
position k. This would minimise the effect of problems in other parts of the sequences, and in
particular, the effect of the particularly inaccurate tails present in the test datasets. During the
empirical selection of the parameters for the ML-based models in 4.2, it was discovered that
distant bases had little effect on the classification accuracy (see 3 in 5.1.5). For these reasons,
a window with radius w around the evaluated position was selected.
window(r, k) = {i : ∃ri , i ∈ [k − w, k − 1] ∪ [k + 1, k + w]}

(2)

To reduce the effect of the sudden cut-off at the window edges, and to reduce the importance
of w as a parameter, the similarity functions include a fading factor q, which decreases the
importance of differences further from the evaluated base. Two similarity measures were tested.
The first similarity corresponds to the commonly used Hamming distance [15].
sim(r, p, k) =



i∈window(r,k)



q |k−i| [ri = pi ]

i∈window(r,k)

q |k−i|

(3)

However, it results in an additive penalization of differences—meaning, each additional
difference linearly reduces the similarity with the same fixed amount. In reality, differences
occurring by chance (i.e. errors) are independent and unlikely, hence it makes sense to use a
multiplicative penalization instead—that way, each difference would significantly increase the
chance that the pair of sequences have different origins.
For this reason, the following ‘sharp’ similarity was also tested, with the expectation that
its interpretation of differences would be closer to their biological meaning.
sim(r, p, k) =

ri
=pi

1

q |k−i|

(4)

i∈window(r,k)

It should be noted that, while the sharp similarity indeed proved to be a better selector for
error candidates, once the ML-based filter was applied afterwards, the difference was no longer
visible in the final results. For this reason, little focus is put on the choice of similarity function
and its parameters. However, removing the similarity weight altogether lead to a measurably
worse error selection even after the ML-based filter, demonstrating its importance.

3.2

Similarity weights for polyploid genome sequencing data analysis

For the processing of hexaploid wheat data, a significantly modified similarity function was
needed. The sequence reads were much shorter, and would contain various fragments from
different locations inside the wheat genome. Most significantly, pairs of sequences would have
various degree of overlap, ranging from 1 to 50-60 bases. This means that limiting the evaluation
inside a window would be less beneficial, while it would be necessary to take the sequence overlap
into account. For these reasons, the following similarity measure was used instead.
sim(r, p, k) = overlap(r, p)

ri
=pi

i:∃ri ∃pi

1
2

(5)

3

1147

1148	

Machine learning in error detection
Krachunov,
Nisheva and Vassilev
Milko Krachunov et al. / Procedia Computer Science 108C
(2017) 1145–1154

In addition to the similarity formula, a different technique for the choice of the error candidate threshold was used. For metagenomics, a simple variable per-position threshold, proportional to the measured local error rate, was computed. In contrast, the position-agnostic
threshold used in polyploid processing had to take into account the predicted distribution of the
three subgenomes in the sample. It also had to be adjusted to match the type of base variant
errors had to be distinguished from.
For that reason, a more complex model, published in [7], modelling the subgenome distribution and computing the threshold, was used during candidate selection.

4

Machine learning approach

The main task is to distinguish errors, which occur by chance, from natural variants, which
have a complex unknown relationship with the remaining bases. Because variants would be
reproduced in other sequences, the use of frequencies seems reasonable, but it is insufficient.
Machine learning provides the tool to create models of complex unknown relationships,
which makes them very suitable for the final classification of the error candidates produced
in section 3. Using a single training scheme, we trained four different ML-based classification
models. Unlike the selection of error candidates, the same machine learning models were used
for both metagenomics and polyploid genomes. These models can either further reclassify the
error candidates pre-selected using (1) in section 3, or can be applied to all bases.

4.1

Machine learning models

The ML-based classification models were selected among those offered by the Weka machine
learning suite [20]. Twenty different models were tested for raw accuracy on the training sets,
without preselection of error candidates using the analytical approach from section 3, and
then four of them were picked for the full testing and tested in two separate experiments on
metagenomics.
Artificial neural networks [17] are inspired by the biological neural network comprising
the human brain. They consist of several layers of ‘neurons’—numerical values computed using a
non-linear parametric formula from the previous layer of neurons. The parameters are changed
until the model produces a prediction with an optimal accuracy. They can model complex
non-linear dependencies, and can produce non-discrete outputs.
Random forests [1] rely on a set of decision trees trained by a random selection of features.
The prediction uses the statistical mode of all the predictions from the individual trees. They
are very fast predictors commonly used for a wide variety tasks in Bioinformatics, and were the
model that had the highest raw accuracy without preselection of error candidates during our
tests. However, they ended up performing poorly with preselection.
Hoeffding trees [5] are built using a very fast decision tree (VFDT) model. It produces
trees similar to the trees produced by the C4.5 algorithm, but it relies on a Hoeffding bound [4]
limiting the amount of attributes for making a decision. It can be trained faster, and can be
further refined should new examples be provided. This makes them suitable for training new
models for individual datasets. They had a very high raw accuracy without preselection of error
candidates, and comparable performance to ANNs with one.
RIPPER [2] is a propositional rule learner. It uses a grow-and-simplify approach. The
training data is split into growing and pruning sets—the growing set is used to create a collection
of propositional rules which are overfitting it, and then these rules are pruned until they gain
4

	

Machine learning in error detection
Krachunov,
Nisheva and Vassilev
Milko Krachunov et al. / Procedia Computer Science 108C
(2017) 1145–1154

a high prediction accuracy on the pruning set. RIPPER had the second-highest accuracy after
RF with no preselection of candidates, but like RF performed poorly with preselection.

4.2

Learning input

The ML-based models would classify an input example for a given evaluated base rk —
preselected by (1) or otherwise—producing a Boolean prediction (correct or incorrect) as its
output. Each example in our models would be a vector of frequency statistics about the base
rk . In particular, each number would be its frequency in a different subset of the reads.
0.94

0.95

0.63

0.25

0.93

0.64

0.29

……T…G…… ……T…X…… ……X…X…… …A………T… …A………X… …X………X…
……X…G……
…X………T…
CATTGTA CATTGTA CATTGTA CATTGTA CATTGTA CATTGTA

Figure 1: Example learning input
For several pairs of adjacent bases (at a given offset k ± i), the set of remaining reads
R \ {r} would be split into three subsets—the reads p that match in both of those positions
(rk±i = pk±i ), the reads p that match in only one of those positions (rk−i = pk−i or rk+i = pk+i ),
and the reads p that match in neither of the two (rk±i = pk±i ). For each of those subsets, a
number would be added to the input vector signifying the frequency of base rk at position k
among the chosen subset of R \ {r}. This is illustrated on figure 1,

4.3

Training with virtual errors

Except in the nanopore datasets, the error rates are very low. This was true in both the
metagenomics and polyploid sequencing datasets that were processed as part of our evaluation.
The actual errors in the datasets would provide insufficient number of training examples for
the ML-based models. Additionally, researchers might want to retrain the models for the kind
of data and equipment that they use, in the absence of known errors. For these reasons, the
ML-based models were trained using so-called “virtual” errors.
Because of the low error rate, an average base in the reads can be assumed with high
probability to be correct, and can be used as a negative training example, classified as a correct
base. Each such example can be supplanted by a second, positive example, classified as an
error, in which that particular base has been replaced with one of the other three bases (a
virtual error). There’s less than 1% chance that these training examples would be themselves
incorrect [3], and they would only slightly affect the classification accuracy of the model.

5

Experiments and results

A comprehensive set of experiments were performed to validate the proposed error detection
methods. This includes empirical evaluated of all the parameters of the ML-based model,
5

1149

1150	

Machine learning in error detection
Krachunov,
Nisheva and Vassilev
Milko Krachunov et al. / Procedia Computer Science 108C
(2017) 1145–1154

parameters of the ANN model in particular, evaluation of different similarity measures in the
analytical approach and others. Only the important results are summarised in this section.

5.1

Experiments on metagenomics sequencing data

The metagenomics test dataset includes two sets of data containing between 30000 and 50000
short reads of up to 1000 bases, produced by the 454 platform from environmental samples from
copper mines. To reduce length biases, reads shorter than 300 bases were excluded from the test,
and reads longer than 500 bases were trimmed. The sets were clustered with CD-HIT-454 [12],
generating sets of up to 6000 sequences, and the largest were used for the test.
In addition to the metegenomics dataset, a set of known sequences was available to measure
the exact error rate produced by the sequencing machine, which was used in the validation
procedure to simulate errors with the same distribution.
5.1.1

Validation

Like finding a good training set, it is difficult to obtain a good testing set. For this reason, the
full test, with preselection of candidates, relies on a two-phase validation procedure previously
published in [8]. In the first phase, the number of error predictions is measured on the original
dataset, which is then corrected with bases picked using the analytical approach. In the second
phase, simulated errors are introduced in the data, and the number of missed errors is measured.
The second phase directly measures the sensitivity on simulated errors, and the first phase
indirectly indicates the specificity of the error classification algorithm through the overall number of error predictions. A probabilistic method was used to measure the distribution of errors
produced by the sequencing machine used in the metagenomics experiment, so the simulated
errors follow the same pattern.
The aim is to get the highest projected specificity (or lowest number of predictions) in
phase one, without affecting the sensitivity in phase two, or without affecting it a lot. This is of
significant importance, because the errors represent a disproportionally small part of the data,
and any over-sensitivity would produce a significant amount of false positives.
5.1.2

Using the similarity-based analytical approach

The first experimental comparison involved measuring the effect on specificity after introducing
the weighted frequency (1). As shown in table 1, by using the similarity weights alone, the
selection of error candidates is reduced by 6–35% without any consistent decrease in the number
of missed errors when the similarity measure is added to the frequency formula.
Corrections
No sim.

1548
813
699
391
610
287
280

Similarity

Difference

1451 -97 (-6%)
698 -115 (-14%)
613 -86 (-12%)
250 -141 (-35%)
518 -92 (-15%)
232 -55 (-19%)
204 -76 (-27%)

Missed simulated errors
No sim.

Similarity

392
386
415
445
70
73
81

391
391
412
448
69
74
82

Difference

-1
+5
-3
+3
-1
+1
+1

(-0.2%)
(+1.3%)
(-0.7%)
(+0.7%)
(-1.4%)
(+1.4%)
(+1.2%)

Table 1: Comparison in corrections and missed errors

6

	

Machine learning in error detection
Krachunov,
Nisheva and Vassilev
Milko Krachunov et al. / Procedia Computer Science 108C
(2017) 1145–1154

5.1.3

Using ML-based filtering

Initially, the training data from 4.3 was split into training and test set to measure the raw
model accuracy, without preselection of error canidates. Most of the ML models had a raw
accuracy over 99%. The ANN model had a sensitivity of 99.563%, and a specificity of 99.705%.
The RF model had a sensitivity of 99.760%, and a specificity of 99.922%. The RIPPER model
had a sensitivity of 99.714%, a specificity of 99.748%.
It was not enough to compare the accuracy without preselection, as the achieved specificity
was insufficient for the small observed error rates. Furthermore, an interaction may exist
between the preselection and the filter. For that reason, the combination of the ML-based filter
on candidates selected by the weighted frequency (1) was compared to the naı̈ve predictions
using regular frequences, using the procedure from 5.1.1. This is shown on table 2.
Corrections on raw data
Missed simulated errors
No sim. Sim. + ANN Sim. + RF No sim. Sim. + ANN Sim. + RF
1908 896 -53.0% 875 -54.1%
398 388
-2.5% 450 +13.1%
406 392
-3.4% 450 +10.8%
723 372 -48.5% 385 -46.7%
70 69
-1.4% 67
-4.3%
56 54
-3.5% 54
-3.5%
2374 1132 -52.3% 1093 -53.9%
419 400
-4.5% 462 +10.3%
353 336
-4.8% 378 +7.0%
892 470 -47.3% 478 -46.4%
49 47
-4.1% 48
-2.0%
35 35
= 35
=
4534 1827 -59.7% 1748 -61.4%
351 330
-6.0% 380 +8.3%
338 315
-6.8% 363 +7.3%
1529 798 -47.8% 785 -48.6%
40 41
+2.5% 40
=
51 48
-5.8% 49
-3.9%

Table 2: Comparison between the naı̈ve approach and two combined approaches
As it can be seen, both the ANN and RF algorithms, when combined with the similarity
weights, lead to a major increase in the specificity, throwing out 47.3%–61.4% rare bases that
are not errors. The ANN model does that without any drop in the sensitivity—in fact, because
of the weights, there are more correctly identified simulated errors.
Unfortunately, here the RF model shows a significant increase in the number of missed errors.
Even though it had a very high sensitivity when selecting among all bases, that sensitivity is
significantly diminished when applied only the bases preselected by the analytical approach.
This is the kind of interaction between the preselection and filter that necessitates testing the
two methods together, and not individually. For that reason, RF was excluded from further
testing.
5.1.4

Filtering accuracy of ANN, RIPPER and HT

The final comparison on metagenomics data aims to highlight only the effect of the ML-based
model, when it is applied on the preselected by the analytical approach error candidates, thus
measuring the ML-based contribution to the combination. This allows us to compare the MLbased models, and their performance with preselection, without having to think about the
performance of the preselection itself.
As seen in table 3, HT and ANN have a comparable specificity, each outperforming the other
on a different test dataset. The ANN seems to show the more significant over-performance of
the two, which seems to indicate it is the better method, pending further testing.
RIPPER outperforms both methods in specificity, but its sensitivity on missed errors among
the preselected candidates, similarly to RF, is incredibly low. On one of the two datasets,
RIPPER filtered half of the simulated errors, which is unacceptable.
7

1151

1152	

Milko Krachunov et al. / Procedia Computer Science 108C (2017) 1145–1154
Machine learning in error detection
Krachunov, Nisheva and Vassilev

Model
RIPPER
HT
ANN

Corrections on raw data
Total Conf. Rej. Confirm %
2107
986 1121
46.7964%
754
451
303
59.8143%
2107
1254
853
59.5159%
754
467
287
61.9363%
2107
1132
975
53.7257%
754
470
284
62.3342%

Errors
213
44
213
44
213
44

Missed simulated errors
Found Missed Sensitivity
120
93
56.3380%
43
1
97.7273%
212
1
99.5305%
44
0 100.0000%
209
4
98.1221%
42
2
95.4545%

Table 3: Quantity of predictions on raw data and missed simulated errors
HT shows the highest sensitivity, but more tests are needed to demonstrate this with statistical significance.
5.1.5

Results during model parameter tuning

In addition to the main results above, several other test results need to be mentioned.
1. Before the ML-based filter, using a non-uniform threshold reduces the number of corrections with 5.59%–151.91%, without a consistent increase in the number of missed
simulated errors.
2. Before the ML-based filter, using the ‘sharp’ similarity function (4) instead of (3) reduces
the number of corrections with 3.765%–11.916%, without a consistent increase in the
number of missed simulated errors. When measured with a ML filter, the difference in
the number of corrections is less than 1%, and the result is inconsistent.
3. Varying the ML input vector size on a test without error candidate preselection shows that
there is no increase in the accuracy beyond an offset of 6 positions, with the 12 nearest
bases being enough. Using only distant bases leads to significantly worse accuracy. Using
12 bases at an offset more than 4 gives worse accuracy than using only the 4 nearest bases.
4. Adding a ML input with the position in the read leads to no change in the accuracy.

5.2

Application during variant calling in complex polyploid genomes

In polyploid sequencing datasets, the difficulty in detecting the errors is less significant, as there
are only three subgenomes, versus dozens of different organisms. This may make it easier for the
proposed combination to identify the errors, which is why we performed a few computational
experiments on polyploid wheat datasets. Unfortunately, due to the lack of a dataset to measure
the error distribution with, we could only measure the raw training accuracy of the ML-based
models on the data from 4.3, and we couldn’t employ the same validation as in metagenomics.
Table 4 shows the ANN raw accuracy, without preselection of candidates using the analytical
approach. The above-99% accuracy seen in 5.1.3 seems to still hold in wheat. Another interesting feature is the complete lack of missed errors. It should be also noted that the misidentified
‘correct’ bases are close to the suspected error rate of the sequencing equipment. Because of
the nature of the procedure for generating the training and raw testing dataset in 4.3, the error
rate may need to be subtracted from rate of misclassified correct bases. The result is therefore
consistent with a hypothesis that the ML-based model alone, without any preselection, offers a
8

	

Milko Krachunov et al. / Procedia Computer Science 108C (2017) 1145–1154
Machine learning in error detection
Krachunov, Nisheva and Vassilev

Cross-validation
Input size 15
Input size 45
Different gene
Input size 15
Input size 45

Non-error
Error
Non-error
Error
Non-error
Error
Non-error
Error

Model
40423
40514
27823
27960
175119
175427
128014
128424

correct
99.78%
100.00%
99.51%
100.00%
99.81%
99.98%
99.68%
100.00%

Misclassified
91 0.22%
0 0.00%
137 0.49%
0 0.00%
335 0.19%
27 0.02%
410 0.32%
0 0.00%

Table 4: Results with ANN
high-enough accuracy for error detection in wheat. This, however, cannot be confirmed without
performing more rigorous validation, similar to the one in 5.1.1.
To judge the effect of the error detection on variant calling results, the analytical approach
from section 3, using the similarity function (5), and the set of threshold formulas published
in [7], was used to preselect errors, as well as select SNP variants using their frequency. Then
the preselected errors were filtered using the ANN model. The ANN model confirmed 98.49%99.85% of the predicted SNPs, reclassifying the rest as errors, and confirmed only 12.1%–23.26%
of the predicted errors, reclassifying the rest as potential SNPs.
The validity of these choices would need to be confirmed by a more rigorous validation
procedure, but as evidenced, the ML-based model significantly impacts the selection of variants.

6

Conclusions

This paper presents a combination of two approaches to identify errors in high-variation
datasets, such as found in metagenomics and polyploid genome analysis. The first analytical
approach relies on the introduction of weighted frequencies, while the second trains different
machine learning models to perform the error reclassification.
On metagenomics, both the use of weights and the ML-based reclassification were shown
to significantly reduce the set of potential errors, without missing simulated errors along the
way. This was true for artificial neural networks and the Hoeffding tree classifier. However, the
RIPPER rule learner and random forest classifier showed significantly diminished performance.
On hexaploid wheat, the ML-based models showed raw accuracy high enough to suggest
that they might be used independently of the analytical approach for error discovery. It was
also shown that the ML model significantly alters the results if it is used as an additional
filter during variant calling. Further validation is required to confirm the results achieved on
hexaploid wheat.
The results show that ML-based classification models are highly applicable on genomics
data, and have a great potential to be used for error discovery, whether directly, or as an
additional filter, and also as an aid during variant calling.

Acknowledgements
This work has been supported by the National Science Fund of Bulgaria within the “Methods
for Data Analysis and Knowledge Discovery in Big Sequencing Datasets” Project, Contract
I02/7/2014. Part of this study is based on the MSc theses of Kiril Kirov and Milena Sokolova,
supervised by Maria Nisheva, Dimitar Vassilev and Milko Krachunov.
9

1153

1154	

Milko Krachunov et al. / Procedia Computer Science 108C (2017) 1145–1154
Machine learning in error detection
Krachunov, Nisheva and Vassilev

References
[1] Leo Breiman. Random forests. Machine Learning, 45(1):5–32, October 2001.
[2] William W. Cohen. Fast effective rule induction. In Twelfth International Conference on Machine
Learning, pages 115–123. Morgan Kaufmann, 1995.
[3] Andr Gilles, Emese Meglcz, Nicolas Pech, Stphanie Ferreira, Thibaut Malausa, and Jean-Franois
Martin. Accuracy and quality assessment of 454 gs-flx titanium pyrosequencing. BMC Genomics,
12:245, May 2011.
[4] Wassily Hoeffding. Probability inequalities for sums of bounded random variables. Journal of the
American Statistical Association, 58(301):13–30, March 1963.
[5] Geoff Hulten, Laurie Spencer, and Pedro Domingos. Mining time-changing data streams. In ACM
SIGKDD Intl. Conf. on Knowledge Discovery and Data Mining, pages 97–106. ACM Press, 2001.
[6] Susan Huse, Julie Huber, Hilary Morrison, Mitchell Sogin, and David Welch. Accuracy and quality
of massively parallel DNA pyrosequencing. Genome Biology, 8(7):R143, 2007.
[7] Kiril Kirov, Milko Krachunov, Ognyan Kulev, Maria Nisheva, and Dimitar Vassilev. Improving
SNP differentiation in bread wheat: a computational approach. Comptes rendus de l’Acadmie
bulgare des Sciences, 69(2):155–160, 2016.
[8] Milko Krachunov and Dimitar Vassilev. An approach to a metagenomic data processing workflow.
Journal of Computational Science, 5:357–362, 2014.
[9] Victor Kunin, Anna Engelbrektson, Howard Ochman, and Philip Hugenholtz. Wrinkles in the
rare biosphere: pyrosequencing errors can lead to artificial inflation of diversity estimates. Environmental microbiology, 12(1):118–123, 2010.
[10] Thomas Laver, James W. Harrison, P.A. O’Neill, Karen Moore, Audrey Farbos, Konrad
Paszkiewicz, and David J Studholme. Assessing the performance of the Oxford Nanopore Technologies MinION. Biomolecular Detection and Quantification, 3:1 – 8, 2015.
[11] Robert W. Li, editor. Metagenomics and its Applications in Agriculture, Biomedicine and Environmental Studies. Nova Science Pub Inc, 2010.
[12] Weizhong Li and Adam Godzik. Cd-hit: a fast program for clustering and comparing large sets
of protein or nucleotide sequences. Bioinformatics, 22(13):1658–1659, July 2006.
[13] Thomas Marcussen, Simen R. Sandve, Lise Heier, Manuel Spannagl, Matthias Pfeifer, The International Wheat Genome Sequencing Consortium, Kjetill S. Jakobsen, Brande B. H. Wulff, Burkhard
Steuernagel, Klaus F. X. Mayer, and Odd-Arne Olsen. Ancient hybridizations among the ancestral
genomes of bread wheat. Science, 345(6194), 2014.
[14] Karen E. Nelson and Bryan A. White. Metagenomics and its applications to the study of the
human microbiome. In Metagenomics: Theory, Methods and Applications, chapter 10, pages 171–
182. Caister Academic Press, 2010.
[15] Hildete Prisco Pinheiroa, Alusio de Souza Pinheiroa, and Pranab Kumar Sen. Comparison of
genomic sequences using the Hamming distance. Journal of Statistical Planning and Inference,
130:325–329, 2005.
[16] Jason A. Reuter, Damek V. Spacek, and Michael P. Snyder. High-throughput sequencing technologies. Molecular Cell, 58(4):586–597, 09 2016.
[17] Raul Rojas. Neural networks: a systematic introduction. Springer-Verlag, 1996.
[18] The MetaSUB International Consortium. The metagenomics and metadesign of the subways and
urban biomes (metasub) international consortium inaugural meeting report. Microbiome, 4(1):24,
2016.
[19] Jos R. Valverde and Rafael P. Mellado. Analysis of metagenomic data containing high biodiversity
levels. PLoS ONE, 8(3), 2013.
[20] Ian H. Witten, Eibe Frank, and Mark A. Hal. Data Mining: Practical machine learning tools and
techniques. Morgan Kaufmann Publishers, San Francisco, third edition, 2011.

10

