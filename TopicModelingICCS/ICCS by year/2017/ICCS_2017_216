Available online at www.sciencedirect.com

ScienceDirect

This space is reserved for the Procedia header, do not use it
Science
(2017) 2313–2317
This Procedia
space isComputer
reserved
for 108C
the Procedia
header, do not use it
This space is reserved for the Procedia header, do not use it

International Conference on Computational Science, ICCS 2017, 12-14 June 2017,
Zurich, Switzerland

Performance and Scalability Study of FMM Kernels
Performance and Scalability Study of FMM Kernels
on Novel Multiand Many-core
Performance
and Scalability
Study Architectures
of FMM Kernels
on Novel Multiand Many-core
Architectures
1
1
Antón Rey
, Francisco D. Igual11 , Manuel
Prieto-Matı́as
, and Jan F. Prins22
on
Many-core
Architectures
1Novel Multi- and
1
Antón Rey , Francisco D. Igual , Manuel Prieto-Matı́as , and Jan F. Prins
1

Dept. Arquitectura de Computadores y Automática, UCM. Madrid, Spain.

1
1
1
1
Antón Rey
, Francisco
D.
, Manuel
Prieto-Matı́as
and Jan
F. Prins2
Dept.
Arquitectura
deIgual
Computadores
y Automática,
UCM. ,Madrid,
Spain.
{anrey,figual,mpmatias}@ucm.es
2
2
2

{anrey,figual,mpmatias}@ucm.es
Department
of Computer
Science, University of North Carolina at Chapel Hill, USA.
1
Dept. Arquitectura
Computadores
y Automática,
UCM.atMadrid,
Department
of Computerde
Science,
University
of North Carolina
ChapelSpain.
Hill, USA.
prins@cs.unc.edu
{anrey,figual,mpmatias}@ucm.es
prins@cs.unc.edu
Department of Computer Science, University of North Carolina at Chapel Hill, USA.
prins@cs.unc.edu

Abstract
Abstract
We provide efficient implementations of common Fast Multipole Method (FMM) tasks for
We
provide
efficient
implementations
of common
Fast
Multipole
Method
(FMM)
tasks and
for
modern
multi-core
(Intel
Xeon Haswell),
many-core
(Intel
Xeon Phi
Knights
Landing)
Abstract
modern
multi-core
(Inteloffering
Xeon Haswell),
many-core
(Intel
Xeon
Phi Knights
Landing) and
Nvidia
Pascal
GPUs,
optimization
guidelines
for
each
kernel
and
architecture,
We provide efficient implementations of common Fast Multipole Method (FMM) tasks for
Nvidia Pascal
GPUs, offering
optimization
guidelines
for each kernel
and architecture,
and
exposing
task granularity
issuesHaswell),
with
evaluations
on performance
scalability.
These results
modern multi-core
(Intel Xeon
many-core
(Intel Xeon and
Phi Knights
Landing)
and
exposing
task
granularity
issues
with models
evaluations
on performance
and scalability.
These
results
motivate
the
use
of
hybrid
execution
for
FMM
in
heterogeneous
architectures,
in
which
Nvidia Pascal GPUs, offering optimization guidelines for each kernel and architecture, and
motivate
the
use of hybrid
execution models
FMM
in adaptability
heterogeneous
in which
per-kernel
execution
configurations
set byforthe
kernel
toarchitectures,
the processor.
exposing task
granularity
issues withare
evaluations
on
performance
and scalability.
These results
per-kernel execution configurations are set by the kernel adaptability to the processor.
motivate
use
ofPublished
hybrid
execution
modelsmany-core,
for FMM in
heterogeneous
architectures,
in which
Keywords:
Fast
Multipole
Method,
multi-core,
task
parallelism, performance
evaluation
©
2017 Thethe
Authors.
by
Elsevier
B.V.
Keywords: under
Fast
Multipole
Method,
multi-core,
many-core,
task
parallelism,
performance
evaluation
per-kernel
execution
configurations
are committee
set by
the
adaptability
to
the
processor.
Peer-review
responsibility
of the scientific
of kernel
the International
Conference
on Computational
Science
Keywords: Fast Multipole Method, multi-core, many-core, task parallelism, performance evaluation

1 Introduction
1 Introduction
Heterogeneous architectures have become an appealing solution to face the ever-increasing
1
Introduction
Heterogeneous
architectures
have
becomethat
an appealing
solution
face the ever-increasing
performance demands
of many
problems
arise in science
andtoengineering
including, for
performance
demandsparticle
of many problems that
ariseproblems
in science
and engineering
including,
for
example,
large-scale
in general,
andthe
FMM
in particuHeterogeneous
architectures simulations.
have become N-body
an appealing
solution
to face
ever-increasing
example,
large-scale
particle
simulations.
N-body
problems in
general,
andtype
FMM
in
particular,
are
ideal
candidates
to
exploit
the
potential
performance
offered
by
this
of
computing
performance demands of many problems that arise in science and engineering including, for
lar,
are ideal
candidates
to exploit
the potential
performance increases,
offered by thisunderlying
type of computing
platforms.
However,
as
the
heterogeneity
the architectures
example, large-scale
particle
simulations.ofN-body
problems in general,the
and FMM in software
particuplatforms.
However, asgrowth
the heterogeneity
of the
architectures
increases,
the
underlying
software
suffers
a
proportional
in
complexity:
specific
codes
need
to
be
adapted
archilar, are ideal candidates to exploit the potential performance offered by this typeto
of each
computing
suffers
aand,
proportional
growth
in complexity:
specific
codes needs
need to
be
adapted
to each
architecture
moreover,
the
heterogeneous
parallel
execution
to
be
orchestrated
to
reduce
platforms. However, as the heterogeneity of the architectures increases, the underlying software
tecture
and,
moreover,
the
heterogeneous
parallel
execution
needs
to
be
orchestrated
to
reduce
idle
times,
improve load
balancing
and correctly
exploit
the characteristics
of eachtoarchitecture.
suffers
a proportional
growth
in complexity:
specific
codes
need to be adapted
each archiidleFor
times,
improve
load balancing
andnamely:
correctly
exploit
the characteristics
ofwhich
each architecture.
FMM,
two
alternatives
arise,
(1)
Task-based
models,
in
fundamental
tecture and, moreover, the heterogeneous parallel execution needs to be orchestrated
to reduce
Forkernels
FMM, are
two alternatives
arise, namely:
(1) units
Task-based
models,
in which
fundamental
FMM
to different
as
dependencies
are satisfied,
and
idle times,
improvemapped
load balancing
and processing
correctly exploit
thedata
characteristics
of each
architecture.
FMM
kernels
are mapped
to different
processing
units
asofdata
dependencies are
satisfied, and
internally
executed
in
a
sequential
fashion
–in
the
case
multi-core-based
architecturesor
For FMM, two alternatives arise, namely: (1) Task-based models, in which fundamental
internally
executed
inaccelerator
a sequential
fashion
–in
the
case of multi-core-based
architecturesor
occupy
the
complete
–in
the
case
of
GPU-based
architectures-,
have
been
used
in
FMM kernels are mapped to different processing units as data dependencies are satisfied, and
occupy
the
complete
accelerator
–in
the
case
of
GPU-based
architectures-,
have
been
used
in
previous
works
to
address
the
efficient
exploitation
of
heterogeneous
architectures
[2],
and
to
internally executed in a sequential fashion –in the case of multi-core-based architectures- or
previous
works
to
address
the
efficient
exploitation
of
heterogeneous
architectures
[2],
and
to
extract
processors
[3, 1]. Other
optimization
occupy the
the potential
complete parallelism
accelerator on
–instandalone
the case ofmulti-core
GPU-based
architectures-,
have been
used in
extract
the potential
parallelism of
onvector
standalone
multi-core
processorsmulti-core
[3, 1]. Other
optimization
efforts
include
the
exploitation
units
on
state-of-the-art
processors
[5, to
4]
previous works to address the efficient exploitation of heterogeneous architectures [2], and
efforts
includetothe
exploitation
of vectorinunits
on state-of-the-art
multi-core
processors
[5, 4]
or
techniques
improve
load
balancing
heterogeneous
systems
[7].
(2)
Fork-join
models,
extract the potential parallelism on standalone multi-core processors [3, 1]. Other optimization
or techniques to improve load balancing in heterogeneous systems [7]. (2) Fork-join models,
efforts include the exploitation of vector units on state-of-the-art multi-core processors [5, 4]
or techniques to improve load balancing in heterogeneous systems [7]. (2) Fork-join models,1
1
1877-0509 © 2017 The Authors. Published by Elsevier B.V.
Peer-review under responsibility of the scientific committee of the International Conference on Computational Science
10.1016/j.procs.2017.05.128

1

2314	

AntónofRey
et al.Kernels
/ Procedia Computer Science 108C (2017)
Performance and Scalability Study
FMM
Rey,2313–2317
Igual, Prieto-Matı́as, Prins
Kernel name
P2P
P2M
L2P
M2L, M2M, L2L

Input array
4 × N
4 × N
C × (P + 1)2
C × (P + 1)2

Output array
×N
C × (P + 1)2
×N
C × (P + 1)2

Kernel parameters
None
Cell center
Cell center
Cells distance & PM

Interaction
1/|xi − xj |
Particle multipole moment
Local field at particle
Expansion translation

Table 1: FMM kernels characteristics.
that sequentially proceed level by level, and only exploit the available parallelism by means of
a parallel execution of each kernel, occupying the complete processor on each invocation.
Hybrid models, in which task parallelism is exploited, but individual kernels can adapt the
number of threads to their algorithmic nature, available parallelism, features of the underlying
platform and, more importantly, execution parameters (FMM tree depth, desired accuracy,
etc.) have been scarcely studied in the past. Given the differences in the algorithmic features
of FMM kernels, and their different impact on the time-to-solution depending on the execution
parameters, hybrid models can be an appealing solution to boost performance in heterogeneous
platforms. However, the development of these models and their integration into task schedulers,
require a deep study of the performance and scalability of each individual kernel involved in
FMM, and rely on fully optimized implementations for each target platform.
The Fast Multipole Method (FMM) was first formulated in [6], and it is nowadays one of the
most popular approaches to accelerate the solution of N-body problems, reducing its complexity
from O(N 2 ) to O(N ) or O(N log N ). In our case, we approximate the computation of the field
N
f (xi ) = j=1 qj /|xi − xj |, ∀i, from 3D particle positions xi , xj and charges qj by adding near(all-pairs) and far-field contributions, the latter represented by spherical harmonics expansions.

2

FMM kernels implementations

All-pairs (P2P) algorithm has already been studied in a broad set of architectures, so P2M,
L2P, M2M, L2L and M2L kernels are considered here. Two algorithmic baselines allowed us to
treat each kernel as a simple two-loop update –input array (IA) → output array (OA)– via a
kernel dependent interaction (see Table 1).
Expansion re-indexing. Conceptually, these tasks exhibit two nested loops over the complex
coefficients represented by the l, m spherical harmonics indexes, l ∈ [0, P ], m ∈ [−l, l], naturally
traversed by a l-indexed outer loop out of a m-indexed inner loop. These loops are simplified to
a i-indexed single loop for i ∈ [0, (P +1)2 ) and i → l, m precomputed, to ease SIMD exploitation.
Precomputed matrices (PM). M2M, L2L and M2L kernels map a pair of (P +1)2 -sized arrays. In 3D, 8+8+290 = 306 direction-fixed matrices representing these maps are precomputed,
avoiding the costly computation of spherical harmonics while facilitating SIMD exploitation.

2.1

Specific optimizations on Intel Processors

Two alternative strategies were implemented: one based in a SIMD reduction (SR) and another
based on cache blocking (CB). The SR strategy requires a #pragma simd reduction clause
in the inner loop. The CB implementation uses a cached chunk of size CSI to minimize data
movement (see Algorithm 1) and sequentially updates chunks of size CSO. Note that the optimal
parameters for these implementations, such as the chunk sizes (CSI and CSO) and the SIMD
lane size, in principle depend on the underlying architecture and the type of arrays they are
referring to (single/double precision and real/complex arithmetic).
2

	

AntónofRey
et al.Kernels
/ Procedia Computer Science 108C (2017)
Performance and Scalability Study
FMM
Rey,2313–2317
Igual, Prieto-Matı́as, Prins

Table 2: Optimal tuning parameters for each FMM kernel. CB(X) or CB(X-Y) refers to the
optimal Cache Blocking implementation being X and Y well-performing chunk sizes for CSR,
and X/2, Y/2 for CSC. Any means that both SR and CB performed practically the same.
P2P

P2M

L2P

M2L
DP

SP

DP

SP

DP

Xeon

SR

Any

SR

SR

CB
(16-32)

CB
(16-32)

Any

Any

Any

KnL

CB
(32)

CB
(32)

SR

SR

CB
(16-32)

CB
(16-32)

Any

Any

Any

Pascal

256/
512

1024/
128

256/
1024

512/
1024

1024/
1024

512/
128

1024/
128

512/
128

SP

DP

L2L

DP

2.2

SP

M2M

SP

1024/
128

512/
128

SP

DP

1024/
128

512/
128

Specific optimizations on Nvidia GPUs

Our CUDA kernels for P2M, M2L, M2M and
L2L rely on assigning one block of threads of
Algorithm 1: Cache Blocking (CB) imdimension TPB per element to update in OA.
plementation for Intel processors.
The TPB threads are assigned to TPB-sized
for each chunk cj in OA (omp parallel for) do
chunks of IA, performing sequential jumps
LocalChunk[CSO] = 0;
until IA is fully read, computing the assofor each chunk ci of size CSI in IA do
ciated interaction and adding it to its local
for each element j in cj (omp simd) do
for each element i in ci do
register. Once all threads in the block are
LocalChunk(j) ←
done in this stage, a parallel reduction is perLocalChunk(j) +
formed within the block using register shuffle
interaction(ci ∗ CSI + i, cj ∗
features and shared memory.
CSJ + j);
Add LocalChunk to cj chunk - omp simd
For L2P, after storing the local expansion
array in shared memory, each thread traverses
the (P + 1)2 elements using shared memory broadcast feature using the same l, m, thus avoiding
warp serialization due to l, m-dependent branching within spherical harmonics function.
Contrary to processors that take advantage of SIMD-based instructions, Array of Structures
(AoS) implementations for P2P, P2M, L2P are meant to exploit SIMD parallelism through
GPU-specific SIMT paradigm and to achieve high bandwidth via coalesced transactions.

3

Experimental Results

The correctness of the implemented kernels was confirmed for the desired accuracy, –5-6 and
10-11 digits–, corresponding to P = 9 in single (SP) and P = 32 in double (DP) precision. Note
that the size and arithmetic density –in terms of FLOP/byte– of these kernels with P = 32 are
significantly greater compared with P = 9, since both measures are proportional to (P + 1)2 .
We have evaluated three different architectures that represent state-of-the-art parallel processors, namely: Xeon is an Intel Xeon E5-2695v3 processor (28 cores, AVX2); KnL is an
Intel Xeon 7250 processor (68 cores, 4 threads/core, AVX512); Pascal is a Nvidia GeForce
GTX1080 GPU with 20 SM (Streaming Multiprocessors) (128 CUDA cores/SM).
The best implementation parameters found for coarse-grained kernels are exposed in Table 2.
On Intel processors, SR and CB implementations were tested, ranging CSR in 4, 8, 16, 32 and
CSC in 2, 4, 8, 16 elements. CSR and CSC refer to the chunk size of input/output arrays storing
real –CSR– or complex –CSC– values. On Pascal, a range of thread blocks and threads per
block was explored to find the optimal threading parameters shown in Table 2.
3

2315

Performance and Scalability Study
ofRey
FMM
Rey,2313–2317
Igual, Prieto-Matı́as, Prins
Antón
et al.Kernels
/ Procedia Computer Science 108C (2017)

Table 3: Measured GFLOP/sec for Table 2 parameters and associated arithmetic density.
Scalable kernels
P2P
DP

Xeon
KnL
Pascal

570.6
1753.5
4984.2

63.5
690.0
88.3

51.5
45.5
1624.8

kFLOP/b

0.0009N

0.0004N

0.46

Parallel efficiency

Xeon - SP

Size-bounded kernels

P2M
SP
DP

SP

Xeon - DP

L2P

M2L
SP
DP

M2M
SP
DP

SP

DP

34.0
76.2
51.2

61.6
139.9
2586.2

35.2
88.3
54.2

4.9
1.4
3.0

23.4
25.1
41.7

6.9
1.4
3.9

22.3
47.9
63.7

7.5
1.4
3.8

26.6
47.4
64.0

5.6

0.41

5.04

0.07

0.75

0.07

0.75

0.07

0.75

Xeon Phi - SP

Xeon Phi - DP

GTX1080 - SP

GTX1080 - DP

1.2

1.2

1.2

1.2

1.2

1.2

1

1

1

1

1

1

0.8

0.8

0.8

0.8

0.8

0.8

0.6

0.6

0.6

0.6

0.6

0.6

0.4

0.4

0.4

0.4

0.4

0.4

0.2

0.2

0.2

0

0
10

15

20

25

0.2
N=4096

0
10

15

20

25

0.2
N=65536

30

40

50

60

0.2

N=1048576

0
20

L2L
DP

SP

N=1024

0
20

30

40

50

60

0
5

10

15

20

5

10

15

Number of processors

Number of processors

Number of processors

Number of processors

Number of processors

Number of processors

Xeon - SP

Xeon - DP

Xeon Phi - SP

Xeon Phi - DP

GTX1080 - SP

GTX1080 - DP

1.2

1.2

1.2

1

1

1

1.2
1

1

1

0.8

0.8

0.8

0.8

0.8

0.8

0.6

0.6

0.6

0.6

0.6

0.6

0.4

0.4

0.4

0.4

0.4

0.4

0.2

0.2

0.2

0.2

0.2

0.2

0

0

0

0

0

N=1024

Parallel efficiency

2316	

10

15

20

25

Number of processors

10

15

20

25

Number of processors

20

30

40

50

N=4096

60

Number of processors

1.2
N=65536

20

30

20

1.2

N=1048576

40

50

60

Number of processors

0
5

10

15

20

Number of processors

5

10

15

20

Number of processors

Figure 1: P2M (top row) and L2P (bottom row) parallel efficiency.

3.1

Kernels performance

According to Table 3, P2P attains the maximum performance of all kernels, mainly due to
its high arithmetic density and available parallelism. The drop in performance for the DP
implementation in Xeon is a direct consequence of the lack of reciprocal square root support in
double precision in AVX2. The significant performance drop for P2M in relation to L2P for all
processors can be explained by the limited data-write parallelism in P2M, bounded by the small
value of (P +1)2 compared with N . While data-read parallelism in the inner loop of P2M is only
exploited through 4- or 8-fold vectorization for Intel kernels, the P2M Pascal implementation
takes advantage from GPU massive parallelism and efficient warp-level reductions, reaching
TFLOP/sec speeds. The relatively fine granularity and low arithmetic density of M2L, M2M,
L2L kernels explains its low performance in SP, however greatly increased in DP case.
All in all, many-core architectures (Pascal in SP, KnL in DP) clearly outperform Xeon
on scalable kernels; on size-bounded kernels, however, Xeon is competitive operating in SP, and
again many-core architectures perform better on DP, as the arithmetic density increases.

3.2

Strong scaling

Note that these multiprocessors can be partially occupied, being able to concurrently run unrelated tasks. For Xeon, 1, 7, 14 and 28 cores were used. For KnL, its occupation was set to 1,
17, 34 and 68 cores (4, 68, 136 and 272 threads with compact policy). For Pascal processor,
all kernels were run by setting 1, 5 and 10 thread blocks (on the same number of SM s) setting
the optimal number of threads per block for each case. Furthermore, to fully utilize the 20 SM
available in Pascal, threading parameters specified in Table 2 were used.
4

	

Performance and Scalability Study
ofRey
FMM
Rey,2313–2317
Igual, Prieto-Matı́as, Prins
Antón
et al.Kernels
/ Procedia Computer Science 108C (2017)

Figure 1 report the parallel efficiency in terms of strong scaling for P2M and L2P kernels,
normalized to the single core/SM execution. Note how parallel efficiency for N = 1024 and
N = 4096 drops significantly for almost all cases; a FMM execution under these premises should
consider partial processor allocation instead of filling the complete multiprocessor, so available
cores can be used for other concurrent tasks keeping the overall efficiency high, following a hybrid
execution model. L2P kernel (and P2P, not shown) still holds a decent efficiency for maximum
occupations, thanks to its higher data-write parallelism. In general, the efficiency holds higher
in DP compared with SP, due to the corresponding greater granularity and arithmetic density.

4

Conclusions

We have presented highly tuned implementations that exploit the SIMD characteristics of FMM
kernels on novel multi- and many-core architectures, reporting metrics in terms of performance
and scalability. In an heterogeneous or highly parallel system, a great amount of fine-grained
tasks (deep trees/low accuracy FMM) cope with processor starvation. However, as observed,
too fine-grained tasks allocated to this kind of parallel multiprocessors yield low per-task performance. On the other hand, fewer coarse-grained tasks yield near optimal per-task performance
but incur lower task parallelism and higher risk of load imbalance. This analysis on performance
and scalability suggests new hybrid execution models in which both kernel-to-processor mapping and kernel threading configuration are adapted to the available underlying resources at run
time. Moreover, this study will assist the design of efficient schedules on future FMM executions
on similar processors, operating in isolation or as a part of an heterogeneous environment.

Acknowledgements
This work has been supported by the EU (FEDER); the Spanish MINECO, under grants
TIN2015-65277-R, TIN2012-32180 and BES2016-076806; and Erasmus+ Int’l Programme.

References
[1] A. Abdelhalim, S. Matsuoka, M. Pericas, N. Maruyama, K. Taura, R. Yokota, and P. Balaji. Scaling
FMM with data-driven openmp tasks on multicore architectures. In IWOMP’16. Springer, 2016.
[2] Emmanuel Agullo, Berenger Bramas, Olivier Coulaud, Eric Darve, Matthias Messner, and Toru
Takahashi. Task-based FMM for heterogeneous architectures. CC:P&E, 28(9).
[3] Emmanuel Agullo, Brenger Bramas, Olivier Coulaud, Eric Darve, Matthias Messner, and Toru
Takahashi. Task-based fmm for multicore architectures. SIAM JSC, 36(1):C66–C93, 2014.
[4] A. Chandramowlishwaran, K. Madduri, and R. Vuduc. Diagnosis, tuning, and redesign for multicore performance: A case study of the fast multipole method. In 2010 ACM/IEEE International
Conference for High Performance Computing, Networking, Storage and Analysis, pages 1–12, 2010.
[5] A. Chandramowlishwaran, S. Williams, L. Oliker, I. Lashuk, G. Biros, and R. Vuduc. Optimizing
and tuning the fast multipole method for state-of-the-art multicore architectures. In 2010 IEEE
International Symposium on Parallel Distributed Processing (IPDPS), pages 1–12, April 2010.
[6] L. Greengard and V. Rokhlin. A fast algorithm for particle simulations. J. Comput. Phys.,
73(2):325–348, December 1987.
[7] R. E. Overman, J. F. Prins, L. A. Miller, and M. L. Minion. Dynamic load balancing of the
adaptive fast multipole method in heterogeneous systems. In 2013 IEEE International Symposium
on Parallel Distributed Processing, Workshops and Phd Forum, pages 1126–1135, May 2013.

5

2317

