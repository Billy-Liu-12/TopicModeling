Available online at www.sciencedirect.com

ScienceDirect
Procedia Computer Science 108C (2017) 1982–1989

International Conference on Computational Science, ICCS 2017, 12-14 June 2017,
Zurich, Switzerland

GPU Acceleration of CFD Algorithm: HSMAC and
GPU
CFD
GPU Acceleration
Acceleration of
of SIMPLE
CFD Algorithm:
Algorithm: HSMAC
HSMAC and
and
SIMPLE
SIMPLE
Yue Xiang1*, Bo Yu2†, Qing Yuan1‡and Dongliang Sun2§

National Engineering Laboratory
for2†
Pipeline Safety, Beijing
Key Laboratory of Urban
Oil and
1*
1‡
2§
Yue
Xiang
1*, Bo Yu2†, Qing Yuan1‡and Dongliang Sun2§
Yue
Xiang
,
Bo
Yu
,
Qing
Yuan
and
Dongliang
Sun
Gas
Distribution
Technology,
China
University
of
Petroleum,
Beijing,
102249,
China
1
Engineering Laboratory for Pipeline Safety, Beijing Key Laboratory of Urban Oil and
1National
2
National
Laboratory
for Pipeline
Safety, Beijing
Key Laboratory
of Urban
Oil and
School Engineering
of Mechanical
Engineering,
Beijing Institute
of Petrochemical
Technology,
Beijing,
Gas Distribution Technology, China University of Petroleum, Beijing, 102249, China
University
of Petroleum, Beijing, 102249, China
102617,
China
2 Gas Distribution Technology, China
2School of Mechanical Engineering, Beijing Institute of Petrochemical Technology, Beijing,
School of Mechanicalyubobox@vip.163.com,
Engineering, Beijing Institute
of Petrochemicaldlsun0917@163.com
Technology, Beijing,
cupxy@foxmail.com,
1424602519@qq.com,
102617, China
102617, China
cupxy@foxmail.com, yubobox@vip.163.com, 1424602519@qq.com, dlsun0917@163.com
cupxy@foxmail.com, yubobox@vip.163.com, 1424602519@qq.com, dlsun0917@163.com
1

Abstract
CFD (Computational Fluids Dynamic) is an important branch of fluid dynamics. It applies various kinds
Abstract
of discrete mathematical method to analyze and simulate problems in fluid mechanics with the use of
Abstract
CFD (Computational Fluids Dynamic) is an important branch of fluid dynamics. It applies various kinds
CFD
(Computational
Dynamic)huge
is ancomputational
important branch
of fluid
It applies
variousit kinds
computer.
During theFluids
computation,
tasks
on a dynamics.
single CPU
often makes
very
of discrete to
mathematical
method
to analyze
and simulate
problems
in fluid mechanics
with the use of
inefficient
get the result,
so there
is an increasing
number
of application
of parallel computation
in
of discrete mathematical
method
to analyze
and simulate
problems
in fluid mechanics
with the use of
computer.
the computation,
computational
tasks
onGPU
a single
CPU Processing
often makes
it very
CFD.
WithDuring
more powerful
computinghuge
capability
and lower
price,
(Graphic
Unit)
has
computer.
During
the computation,
huge
computational
tasks
on a single
CPU often makes
it very
inefficient to get the result, so there is an increasing number of application of parallel computation in
inefficient
to getsolution
the result,
there is
an increasing
number
of application
of parallel
computation
in
become
a better
forso
parallel
computing
than CPU
in some
cases in recent
years.
In this paper,
CFD. With more powerful computing capability and lower price, GPU (Graphic Processing Unit) has
CFD.
With more the
powerful
computing
capability
and lower
price, For
GPUthe
(Graphic
Processing
Unit) has
we
implemented
HSMAC
and SIMPLE
algorithms
on GPU.
simulation
of 2D lid-driven
become a better solution for parallel computing than CPU in some cases in recent years. In this paper,
becomeflow,
a better
solution
for parallel
computing
some
in recent
years.
In this
paper,
cavity
the GPU
version
could get
a speedupthan
up toCPU
58x in
and
21x cases
respectively
with
double
precision,
we
implemented
the HSMAC
and SIMPLE
algorithms
on GPU. CPU
For the
simulation
of 2D lid-driven
and
78x and 32x with
single precision,
compared
to the sequential
version.
It demonstrates
a good
we implemented
the HSMAC
and SIMPLE
algorithms
on GPU. For the
simulation
of 2D lid-driven
cavity flow,
GPU
version could
get algorithms.
a speedup up to 58x and 21x respectively with double precision,
prospects
of the
GPU
acceleration
of CFD
cavity flow,
the
GPU
version could
get a speedup up to 58x and 21x respectively with double precision,
and 78x and 32x with single precision, compared to the sequential CPU version. It demonstrates a good
and 78x and 32x with single precision, compared to the sequential CPU version. It demonstrates a good
© 2017 TheofAuthors.
Published byof
Elsevier
B.V.
prospects
GPU
acceleration
CFD algorithms.
Keywords:
GPU
acceleration,
CFD,
HSMAC,
SIMPLE, parallel computing
prospects
GPU
acceleration
algorithms.
Peer-reviewofunder
responsibility
of of
theCFD
scientific
committee of the International Conference on Computational Science
Keywords: GPU acceleration, CFD, HSMAC, SIMPLE, parallel computing
Keywords: GPU acceleration, CFD, HSMAC, SIMPLE, parallel computing

1 Introduction
1
Introduction
method has been
1 CFD
Introduction

developed rapidly in recent two decades, which mainly due to the
development of computer hardware, the computational model and method has been constantly improved.
CFD method
been developed
rapidly
recent
two especially
decades, which
mainlysimulation
due to the
However,
with thehas
enhancement
of problem
scalein
difficulty,
in numerical
of
CFD method
has
been developed
rapidly
inandrecent
two decades, which
mainly due to the
development of computer hardware, the computational model and method has been constantly improved.
development
computer
hardware, number,
the computational
model and
hascomputing
been constantly
improved.
turbulent
flowofwith
high Reynolds
the disadvantage
of method
sequential
will inevitably
However, with the enhancement of problem scale and difficulty, especially in numerical simulation of
However,even
withwe
theuse
enhancement
problem
and
difficulty,
especially inwith
numerical
of
exposed
the fastest of
CPU
in the scale
world.
Thus,
some researchers
spirit ofsimulation
exploration
turbulent flow with high Reynolds number, the disadvantage of sequential computing will inevitably
turbulent flow with high Reynolds number, the disadvantage of sequential computing will inevitably
exposed even we use the fastest CPU in the world. Thus, some researchers with spirit of exploration
exposed
even we use the fastest CPU in the world. Thus, some researchers with spirit of exploration
*
Designed the parallel program and created the first draft of this paper
Collated the results and improved the structure of this paper
*‡
program
andprogram
created the first draft of this paper
Improved the
the parallel
performance
of the
* Designed
†§ Designed the parallel program and created the first draft of this paper
and improved the structure of this paper
Debuggedthe
theresults
program
† Collated
Collated
the
results
and
improved
the
structure of this paper
‡
‡ Improved the performance of the program
§ Improved the performance of the program
§ Debugged the program
Debugged the program
†

1877-0509 © 2017 The Authors. Published by Elsevier B.V.
Peer-review under responsibility of the scientific committee of the International Conference on Computational Science
10.1016/j.procs.2017.05.124

	

Yue Xiang et al. / Procedia Computer Science 108C (2017) 1982–1989

tried to introduce parallel computing technology into CFD[1-3], nevertheless, it usually requires the
researcher to buy expensive CPU clusters or cooperate with some super-computing centers. In addition,
the communication between nodes seems to cause another thorny problem when you are in a distributed
multiprocessor system. GPU overcomes these problems well, now you could buy a GPU with only
hundreds of dollars, which has dozens of times as much computing capacity as the fastest CPU.
GPGPU (General Purpose GPU Programming) technique arose in 2003, but it was not welcomed
because it must program based on graphic API which is hard to master for those who are not familiar
with graphic programming. Things changed after NVIDIA introduced CUDA (compute unified device
architecture) in 2007 which made it much easier to program on GPU, and it was soon applied in many
fields. In recent research of GPU accelerating in fields of CFD, Jaiswal 错误 !未找到引用源。
accomplished GPU parallelization for arbitrary collocated polyhedral finite volume grids and Menshov
错 误 ! 未 找 到 引 用 源 。 implemented an implicit matrix-free solver for gas dynamics on GPUaccelerated clusters, they both got a considerable speedup. Next we will give a brief introduction of
CUDA, and the implementation of HSMAC and SIMPLE on GPU.

2 CUDA programming model
CUDA is a parallel computing architecture for CPU+GPU heterogeneous systems. In this
architecture, GPU which takes the main parallel computing tasks acts as a co-processor of CPU, and the
main control logic tasks and the serial computing part are completed by CPU[6]. The diagram of the
CUDA programming model is shown in Fig 2.1. A kernel represents a step which could be paralleled
in the algorithm. It could be seen that there are two levels (or granularities) for parallelism: parallel
among blocks in a grid and threads in a block. For most CFD algorithms, we would like to map a grid
node to a single thread and then the division mode of a grid will remarkably affect the efficiency of
program on GPU. A good division mode of a grid should satisfy the coalesced access to the global
memory for a thread warp, and the memory organization and communication overlap also have
important influence on the efficiency of program. Unfortunately, there is not a general optimization
method which suit every parallel algorithms on GPU about how to divide grid, organize memory and
etc. In the next two section, we will also mention optimization for a certain algorithm on GPU.

Fig 2.1 CUDA programming model

1983

1984	

Yue Xiang et al. / Procedia Computer Science 108C (2017) 1982–1989

3 HSMAC Implementation on GPU
3.1 An Introduction of HSMAC
HSMAC (Highly simplified marker and cell)[7] is a method to solve unsteady fluid flow. The
method avoid direct solution of pressure Poisson equation and correct velocity and pressure
simultaneously by which the stability and convergence is guaranteed. The steps of the HSMAC is
introduced below:
Substitute the tentative velocity U  got by iteration into the continuity equation, after which a
residual D would be obtained on the right side of the equation:
(1)
U   D
Consider a method to make D equal to zero by correcting velocity and pressure. The corrected
velocity should satisfy:
  U    U   0
(2)

0 . Now we need to find the relation between  U and
D is a function of pressure, so D  p   p  

 p so that we could correct velocity and pressure simultaneously.
If n represents a certain time step, the momentum equation expressed by explicit scheme and semiexplicit scheme could be written as
1
(3)
U n 
U n  tF  p n ,U n 
1
U n 
U n  tF  p n 1 ,U n 

We replace U n 1 with tentative U  in eq.(3), then eq.(4) minus eq.(3):
U n1  U  / t    pn1  pn 

(4)
(5)

Make 
U U n 1  U  , 
p pn 1  p , we can get the expression of  p :





(6)



Iterative the eq.(5) and eq.(6) until the residual D is less than the predetermined value and achieve
convergence.
 1
1
1
 2 2
2
y
z
 x

 D /  2t 
p

3.2 GPU Programming of HSMAC
According to Section 2, the physical region was divided into several small calculation area mapped
to the block and each thread in a block is responsible for a node of the area. Since the HSMAC avoids
a direct solution of Poisson equation, the solution of correction value of each node does not depend on
that of surrounding nodes for either pressure or velocity correction equation, so only one global
synchronization is required within an inner iterative step. The synchronization could be executed by
ending a kernel function. The method has a relative small amount of data communication and high
parallelism.
It can be easily observed that data access in the method is local, so it is easy to satisfy coalesced
access to global memory on GPU if we used a padded memory (Note the velocity of Y direction must
be stored by columns). Fig 3.1 shows the program flow chart on GPU:

	

Yue Xiang et al. / Procedia Computer Science 108C (2017) 1982–1989

Fig 3.1 Program flow chart of HSMAC on GPU

3.3 Results
We would verify our accuracy and acceleration with the case of lid-driven cavity flow. The GPU
used in our numerical experiment is NVIDIA GTX980 and the CPU is Intel E5-2640v3. We compared
the results in different Reynolds number (Re=1000 and Re=5000), different size of the mesh (128×128,
256×256, 512×512) and different precision (single and double). The results are shown in Fig 3.2.

(a) CPU result
(b) GPU result
(c) Benchmark Result
Fig 3.2 Stream-function of Lid-driven Cavity Flow Using HSMAC (Re=5000)

1985

1986	

Yue Xiang et al. / Procedia Computer Science 108C (2017) 1982–1989

Fig 3.3 Speedups (GPU/CPU) of different conditions

4 SIMPLE Implementation on GPU
4.1 An Introduction of SIMPLE
SIMPLE (Semi-Implicit Method for Pressure Linked Equations) is a numerical method to solve
incompressible or compressible flow. The core idea is “assume-correct”: According to a certain pressure
field which could be an assumed value or a result obtained from the last iteration, we solve the
discretization momentum equation for velocity fields, then substitute it into continuity equation to
correct the pressure. The detailed introduction can be found in reference[8]. Discretization momentum
equation and pressure Poisson equation need to solved within each inner iterative step in SIMPLE, and
the form of the pressure Poisson equation is shown below:
(7)
aP pi, j  ae pi1, j  aw pi1, j  as pi, j 1  an pi, j 1  bi , j
where p  represents the correction value of pressure.
The iterative solution of the equation takes up a lot of computation time, so efficient parallel solution
of Poisson equation would remarkable improve the performance of the program.

4.2 GPU Programming of SIMPLE
We will introduce a method to parallelize the iteration solution of Poisson equation. In general, we
use central difference scheme to discretize the pressure Poisson equation and then we can get a series of
five-point difference equations. In sequential computation, we usually use Gauss-Seidel iterative
method to solve the equation, which nevertheless has a strong data dependence, is not suitable for
directly parallelizing. So we introduce a red-black scheme[9] for Gauss-Seidel iterative method shown
in Fig 4.1.

Fig 4.1 Red-Black Iterative Scheme

	

Yue Xiang et al. / Procedia Computer Science 108C (2017) 1982–1989

In a red-black scheme, the mesh is divided into two staggered groups which are red and black. For
each iteration, the red points are first calculated and then calculate the black group. In five-point
difference equations, the solution of each point only relies on surrounding four points, so the red phase
and black phase could both be parallelized.
Different from HSMAC, the implementation of red-black scheme on GPU will bring a more serious
problem—an uncoalesced access to memory which will lead thread suspended and degrade program
performance. So we would apply an array for each of the red and black groups on shared memory owned
privately by a thread block, and read data continuously from the global memory with registers as a
medium and finally write into shared memory. This approach will bring about 30% increase of the
program performance for us.
For the solution of discretization momentum equation, the form of the equations may vary with
different difference schemes. Here we use Jacobi iterative method which could easily be parallelized.
Fig 4.2 shows the program flow chart of SIMPLE on GPU:

Fig 4.2 Program flow chart of SIMPLE on GPU

4.3 Results
Here we also compared the accuracy and speed of GPU version and CPU version in two different
Reynolds number (Re=1000 and Re=5000), three different mesh size (128×128, 256×256, 512×512)
and two precision (single and double). The results are shown below:

1987

Yue Xiang et al. / Procedia Computer Science 108C (2017) 1982–1989

1988	

(a) CPU result
(b) GPU result
(c) Benchmark Result
Fig 4.2 Stream-function of Lid-driven Cavity Flow Using SIMPLE (Re=5000)

Fig 4.3 Speedups (GPU/CPU) of different conditions

5 Conclusion
We implemented HSMAC and SIMPLE on GPU and simulated a lid-driven cavity flow. The results
showed the GPU version has a considerable high accuracy and speedup. A speedup up to 78x and 32x
could be obtained for HSMAC and SIMPLE of GPU version with a 512×512 mesh size and single
precision when Re=5000. Even in double precision, the GPU also performed quite well. It also could be
observed that the speedup gradually approach a constant when we further increase the mesh size, this is
probably caused by the threads competition, and the constant will also increase when porting the
program to a better GPU. In addition, the parallel idea in this paper is also applicable to CFD algorithm
with similar features.

Acknowledgements
The study is supported by the National Natural Science Foundation of China (No. 51325603 and No.
51636006).

	

Yue Xiang et al. / Procedia Computer Science 108C (2017) 1982–1989

References
[1]. Tanaka, T., & Omoda, K. (1990). CFD simulation on a massively parallel processor. 8th NAL
Symposium on Aircraft Computational Aerodynamics. 8th NAL Symposium on Aircraft
Computational Aerodynamics.
[2]. Solchenbach, K. (1988). Parallel CFD algorithms on suprenum.
[3]. Gropp, W. D., Kaushik, D. K., Keyes, D. E., & Smith, B. F. (2001). High-performance parallel
implicit CFD. Parallel Computing, 27(27), 337–362.
[4]. Jaiswal, S., Reddy, R., Banerjee, R., Sato, S., Komagata, D., & Ando, M., et al. (2016). An efficient
GPU Parallelization for arbitrary collocated polyhedral finite volume grids and its application to
incompressible fluid flows. IEEE High PERFORMANCE Computing Workshop. IEEE.
[5]. Menshov, I., & Pavlukhin, P. (2016). Highly scalable implementation of an implicit matrix-free
solver for gas dynamics on gpu-accelerated clusters. Journal of Supercomputing, 1-8.
[6]. Cook, S. (2012). CUDA Programming: A Developer's Guide to Parallel Computing with GPUs.
Elsevier, MK.
[7]. Sakamoto, M., & Kawamura, H. (1992). Parallel computation of a two-dimensional flow field
using the mac methods. Transactions of the Japan Society of Mechanical Engineers B, 60(569),
99-106.
[8]. Patankar, S. V., & Spalding, D. B. (1972). A calculation procedure for heat, mass and momentum
transfer in three-dimensional parabolic flows. International Journal of Heat & Mass Transfer,
15(10), 1787-1806.
[9]. Zhang, J. (1997). Acceleration of five-point red-black gauss-seidel in multigrid for poisson
equation. Applied Mathematics & Computation, 80(1), 73-93.

1989

