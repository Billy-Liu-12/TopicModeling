Available online at www.sciencedirect.com

ScienceDirect
Procedia
108CProcedia
(2017) 285–294
This space
isComputer
reservedScience
for the
header, do not use it

This space is reserved for the Procedia header, do not use it
This space is reserved for the Procedia header, do not use it

International Conference on Computational Science, ICCS 2017, 12-14 June 2017,
Zurich, Switzerland

Learning Robust Low-Rank Approximation
Learning
Robust Low-Rank
Approximation
for
Crowdsourcing
on Riemannian
Manifold
Learning
Robust Low-Rank
Approximation
for
Crowdsourcing
on
Riemannian
Manifold
3
Qian Li1 ,for
Zhichao
Wang2∗, Gang Lion
, Yanan
Cao1 , Gang Manifold
Xiong1 , and Li Guo1
Crowdsourcing
Riemannian
1
1
Qian 1LiInstitute
, Zhichao
Wang2∗
, Engineering,
Gang Li33 , Chinese
YananAcademy
Cao11 , Gang
Xiong
, andChina
Li Guo11
of Information
of Sciences,
Beijing,
1
2∗
1
Qian Li , Zhichao Wang2 ,Tsinghua
Gang Li
, Yanan
Cao China
, Gang Xiong , and Li Guo
University,
Beijing,
1
1

3
Institute of Information
Engineering,
Chinese
Academy
of Sciences, Beijing, China
Deaking
University,
Melbourne,
Australia
2
Institute of Information
Engineering,
ChineseBeijing,
Academy
of Sciences, Beijing, China
Tsinghua
University,
China
3 2 Tsinghua University, Beijing, China
Deaking University, Melbourne, Australia
3
Deaking University, Melbourne, Australia

Abstract
Recently, crowdsourcing has attracted substantial research interest due to its eﬃciency in colAbstract
lecting labels for machine learning and computer vision tasks. This paper proposes a RiemanAbstract
Recently,
crowdsourcing
hasalgorithm,
attracted substantial
research
interest
due to its eﬃciency
in colnian manifold
optimization
ROLA (Robust
Low-rank
Approximation),
to aggregate
Recently,
crowdsourcing
has
attracted
substantial
research
interest
due
to proposes
its eﬃciency
in collecting
labels
for
machine
learning
and
computer
vision
tasks.
This
paper
a
Riemanthe labels from a novel perspective. Speciﬁcally, a novel low-rank approximation model is prolecting
labels
for
machine
learning
and
computer
vision
tasks.
This
paper
proposes
a
Riemannian
optimization
(Robust Low-rank
Approximation),
to aggregate
posedmanifold
to capture
underlying algorithm,
correlation ROLA
among annotators
meanwhile
identify annotator-speciﬁc
nian
manifold
optimization
algorithm,
ROLA (Robust
Low-rank
Approximation),
to aggregate
the
labels
from
a
novel
perspective.
Speciﬁcally,
a
novel
low-rank
approximation
model
is pronoise. More signiﬁcantly, ROLA deﬁnes the label noise in crowdsourcing as annotator-specific
the
labels
from
a
novel
perspective.
Speciﬁcally,
a
novel
low-rank
approximation
model
is proposed
to
capture
underlying
correlation
among
annotators
meanwhile
identify
annotator-speciﬁc
noise, which can be well regularized by l2,1 -norm. The proposed ROLA can improve the aggreposed
to
capture
underlying
correlation
among
annotators
meanwhile
identify
annotator-speciﬁc
noise. More
signiﬁcantly,
deﬁnes
label noise incrowdsourcing
crowdsourcingmethods.
as annotator-specific
gation
performance
when ROLA
compared
withthe
state-of-the-art
noise.
More signiﬁcantly,
ROLA deﬁnes
the
label The
noiseproposed
in crowdsourcing
annotator-specific
noise, which
can be well regularized
by l2,1
-norm.
ROLA canasimprove
the aggreKeywords:
Crowdsourcing,
Low-Rank,
Riemannian
Optimization
noise,
which
can
be
well
regularized
by
l
-norm.
The
proposed
ROLA
can
improve
©
2017 The
Authors. Published
by Elsevier B.V.
gation
performance
when compared
with2,1state-of-the-art crowdsourcing methods. the aggrePeer-review
under responsibility
of the scientific
committee
of the International
Conferencemethods.
on Computational Science
gation performance
when compared
with
state-of-the-art
crowdsourcing
Keywords: Crowdsourcing, Low-Rank, Riemannian Optimization
Keywords: Crowdsourcing, Low-Rank, Riemannian Optimization

1 Introduction
1
Introduction
Crowdsourcing
is an eﬃcient and inexpensive way to collect labelled data in many applica1
Introduction
tion domains,
ranging from computer vision to natural language processing [9]. Services such

Crowdsourcing
is an eﬃcient
and inexpensive
to collect
labelled
datathe
in requesters
many applicaas Amazon Mechanical
Turk and
CrowdFlowerway
provide
platforms
where
can
Crowdsourcing
is an eﬃcient
and inexpensive
way
to collect
labelled
data [9].
in many
application
domains,
ranging
from
computer
vision
to
natural
language
processing
Services
such
post tasks and collect labels from online annotators. An advantage of crowdsourcing
is that
a
tion
domains,
ranging from
computer
vision to natural
language
processing
[9].requesters
Services such
as
Amazon
Turk
CrowdFlower
platforms
where
the
can
large
numberMechanical
of labels can
be and
obtained
in a shortprovide
time with
relatively
low cost.
Traditionally
as
Amazon
Mechanical
Turkfrom
and online
CrowdFlower
provide
platforms of
where
the requesters
can
post
tasks and
collect
labels
annotators.
An advantage
crowdsourcing
is that
researchers
resort
to the
redundancy
mechanism
for quality
assurance, which
assigns each
ques-a
post
tasks
andofcollect
labels
from
onlineinannotators.
Anwith
advantage
of crowdsourcing
is that a
large
number
labels
can
be
obtained
a
short
time
relatively
low
cost.
Traditionally
tion to multiple annotators and then aggregates their labels. Thus, a fundamental challenge
large
number
of labels
be obtained in a short
time
with relativelywhich
low cost.
Traditionally
researchers
resort
to thecan
redundancy
quality
assigns
each quesarises in crowdsourcing:
how to infermechanism
true labels for
from
noisy assurance,
but redundant labels
contributed
by
researchers
resortannotators
to the redundancy
mechanism
for
quality
assurance,
which
assigns each
question
to
multiple
and
then
aggregates
their
labels.
Thus,
a
fundamental
challenge
a crowd of unreliable annotators.
tion
to multiple
annotators
thentrue
aggregates
theirnoisy
labels. Thus,
a fundamental
challenge
arises
howand
tonumerous
infer
labels from
labels contributed
by
In in
thecrowdsourcing:
past several years,
approaches
based but
on redundant
latent statistical
models have
arises
in of
crowdsourcing:
how to infer true labels from noisy but redundant labels contributed by
a
crowd
unreliable
annotators.
emerged and taken both the expertise of each annotator and the diﬃculty of questions into
a crowd
of past
unreliable
annotators.
In the
several
years, Beta
numerous
approaches
basedasonthe
latent
models have
consideration.
For
instance,
distribution
is chosen
priorstatistical
for characterizing
the
In theand
past
several
years,
numerousofapproaches
basedand
on the
latent
statistical
models have
emerged
taken
both
the
expertise
each
annotator
diﬃculty
of
questions
annotator’s expertise in [8]. Moreover, some extensions further argue that the expertiseinto
of
emerged
and taken
both the expertise
of each annotator
theprior
diﬃculty
of questions into
consideration.
For instance,
Beta distribution
is chosen and
as the
for characterizing
the
consideration.
For instance,
distribution
is chosen further
as the prior
annotator’s
expertise
in [8]. Beta
Moreover,
some extensions
argue for
thatcharacterizing
the expertisethe
of
∗ Corresponding author, email: wzchary@gmail.com.
annotator’s
expertise in [8]. Moreover, some extensions further argue that the expertise of
∗ Corresponding

author, email: wzchary@gmail.com.
author, email: wzchary@gmail.com.
1877-0509 © 2017 The Authors. Published by Elsevier B.V.
Peer-review under responsibility of the scientific committee of the International Conference on Computational Science
10.1016/j.procs.2017.05.179
∗ Corresponding

1
1
1

286	

ROLA for Crowdsourcing on Riemannian
Qian Li etManifold
al. / Procedia Computer Science 108C (2017) 285–294

annotators varies across diﬀerent types of questions, and they adopt the confusion matrix to
model their biases and skills [3, 11, 6]. Besides the annotators’ expertise, the diﬃculties of
questions are also modeled in recent research work [18, 1]. For example, Whitehill [18] assumes
the Gaussian prior distribution for both the annotators’ capabilities and the question diﬃculties
with diﬀerent levels.
Although these latent statistical methods have been broadly applicable in many scenarios,
some common limitations exist: ﬁrst, it is unreasonable to simply assume that every label is
unreliable. Considering the fact that labeling tasks are typically micro and simple to answer [9],
annotators who are strongly motivated by interests or payment, tend to provide the correct
labels for all questions. Neglecting this annotator-specific property may result in an ineﬃcient
and ineﬀective aggregation. Second, the noise is usually caused by various factors, and especially
the carelessness or malicious behaviors may bring in arbitrary noise magnitude which is not
suﬃcient to be modeled by traditional probabilistic distribution [14, 6, 7]. Third, each annotator
in existing work is treated independently with individual label-generating model, while ignoring
the underlying correlations among the annotators. More importantly, most statistic models for
crowdsourcing involve the non-convex log-likelihood function, few studies provide theoretical
justiﬁcation on their convergence. Even though satisfactory performance of statistic models
can be observed in practice, the aggregation results may not be global optimum.
Recent advances in low-rank approximation bring new insights into the underlying structure of the observed labels, and provide the potential for further improving the aggregation
performance. This paper proposes an eﬃcient and robust low-rank approach to infer the true
labels from the noisy labels provided by crowdsourcing annotators. The main contributions of
this paper are in the following aspects:
• This paper proposes a robust low-rank model for aggregating the crowd labels from a
novel perspective. By deﬁning the label noise as annotator-specific and sparse regularized
by l2,1 -norm, the noise can be easily identiﬁed simplifying the later aggregation process.
• The probabilistic interpretation for the low-rank crowdsourcing model is also provided.
The probabilistic inference validates the low-rank assumptions and provides the rigorous
theory for the low-rank crowdsosurcing model.
• A novel optimization algorithm ROLA is developed to eﬃciently solve the proposed lowrank model. ROLA fully explores the matrix manifold of the observed aggregations and
designs the gradient-based optimization to obtain the low-rank matrix for reﬁned labels.

2

Related Work

Majority voting (MV) is one of the most popular crowd sourcing methods owing to its simplicity. However, their diﬀerence in expertise or motivations is usually ignored [22, 12]. Probabilistic approaches have been proposed to build statistical latent models that simulates the
label-generating process to infer the true labels [3, 1]. Dawid & Skene model (DS) [3] models
the annotator’s expertise by a probabilistic confusion matrix. DARE [1] ﬁrst evaluates the
annotator’s expertise via questionnaires with correct answers. Alternative methods evaluate
the reliability of annotators by the spammer score, which indicates how spammer the annotator
is [12]. Probabilistic matrix factorization (PMF) [6] was proposed to deduce the latent feature
vectors for annotators and questions respectively, and then to complete unobserved labels for all
questions. Then, the consensus judgment can be made using majority voting. Prior work [21]
2

	

ROLA for Crowdsourcing on Riemannian
Qian Li etManifold
al. / Procedia Computer Science 108C (2017) 285–294

287

presents a matrix completion method to predict certain unknown preferences of a particular
user by exploiting the pairwise comparisons provided by the crowd users.
Although probabilistic approaches for crowdsourcing has improved the aggregation performance, it requires a suﬃciently large number of manual labels to tune the complicated hidden
factors or hyperparameters [20]. Alternatively, several studies apply the matrix completion to
simplify the crowdsourcing model without the prior knowledge on model parameters. However,
they usually assume that the noise is randomly distributed among the observed labels [21],
while overlooking the annotator-speciﬁc noise in crowdsourcing. In addition, approaches based
on matrix-completion usually involve the expensive computation cost in the repetitive singular
value decomposition.

3
3.1

Robust Low-rank Modeling for Crowdsourcing
Problem Formulation

Assume that one crowd labeling task involves m annotators, n questions, from which the observed label matrix Z with size m × n can be constructed, where zij denotes the label given
by annotator j to question i. The i-th row Zi: ∈ R1×n represents all n labels from the annotator i. Since a fraction of unreliable annotators cause arbitrary deviations from the observed
labels, the label noise E is deﬁned as annotator-specific noise. When removing these noise,
the observed Z contains underlying correlations capturing the similarities among annotators.
These correlations promote a low-rank structure, denoted as X. The observed label Z can be
decomposed as the sum of X and E, as shown in Fig 1.

Figure 1: Robust low-rank model for crowdsourcing:
The nuclear norm denoted by ∥ · ∥∗ is used to characterize low-rank matrix X for reﬁned
labels and exploit the underlying correlations among annotators. l2,1 -norm deﬁned by ∥ · ∥2,1
regularizes the noise E to identify the annotator-speciﬁc outliers. Consequently, the crowdsourcing problem can be formulated as
min ∥X∥∗ + λ∥E∥2,1
X,E

(1)

s.t. PΩ (Z) = PΩ (X + E)
where λ > 0 is a given regularization parameter. PΩ (·) is a linear operator that restricts the
equality only on the entries of the observed labels. So far, the crowdsourcing model of Eq. (1)
is deduced by analyzing the underlying low-rank structure of the observed label matrix. As a
complement, the following part attempts to derive the crowdsourcing model via the probabilistic
inference.
3

288	

ROLA for Crowdsourcing on Riemannian
Qian Li etManifold
al. / Procedia Computer Science 108C (2017) 285–294

3.2

Probabilistic Interpretation

By capturing multiple signiﬁcant and latent factors with prior probability, probabilistic inference
is adopted to verify the low-rank assumptions and thus make the proposed low-rank crowdsourcing model understandable. Let A and B represent the annotator-speciﬁc and question-speciﬁc
latent feature matrix, respectively, then X = AT B. The likelihood function for the observation
Z can now be formulated as the product of the probabilities of zij ∈ Z:
m ∏
n
∏
N (PΩ (Z)ij |PΩ (AT B + E)ij , σ 2 )
Z∼
(2)
i=1 j=1

where N (x|µ, σ) is the Gaussian probability density with mean µ and standard deviation σ.
Let Ai ∈ Rk×1 and Bj ∈ Rk×1 represent the feature vectors of A and B, respectively. With a
zero mean Gaussian priors imposed on Ai and Bj [14], two conditional Gaussian distributions
are deﬁned over the annotator and the question feature vectors:
m
m
∏
∏
1
2
2
−1
A∼
N (Ai |0, σA
Im ) =
λA · exp{− AT
Ai }
i (σA Im )
2
i=1
i=1
(3)
n
n
∏
∏
1 T 2
2
−1
B∼
N (Bj |0, σB In ) =
λB · exp{− Bj (σB In ) Bj }
2
j=1
j=1
Diﬀerent from the Gaussian noises as assumed in [12, 6], we assume that annotator-speciﬁc
vectors Ei: are drawn independently from the Exponential Power (EP) distributions.
Ei: ∼ Exp(Ei: |ηi ) = λE · exp{ηi ∥Ei: ∥2 }

(4)

where ηi is a positive hyperparameter. Since the label quality of annotators is independent
and identically distributed [5], each vector is assumed in {E1: , ..., Em: } is drawn independently
from the prior in Eq. (4). Let us denote η = [η1 , ..., ηm ]T ∈ Rm , then the prior for E can be
expressed as:
m
m
∏
∏
p(E|η) =
Exp(Ei: |ηi ) =
λE · exp{ηi ∥Ei: ∥2 }
(5)
i=1

i=1

Combining Eq (2), Eq (3) and Eq (5) together, Bayes’s rule implies the complete likelihood
function to be:
2
2
p(A, B, E|Z, Θ) ∝ p(Z|A, B, E, η)p(A|σA
)p(B|σB
)p(E|η),

2
2
Θ = (σA
, σB
, η)

2
2
Let ηi = η, C be a constant and σA
= σB
as suggested by previous work [6, 14], then
m
n
m
n
1 ∑ T
1 ∑ T
1 ∑∑
PΩ (Z − AT B − E)2ij − 2
A i Ai − 2
B Bj
log p(A, B, E|Z, Θ) = − 2
2σ i=1 j=1
2σA i=1
2σB j=1 j

−

m
∑
i=1

ηi ∥Ei: ∥2 + C = −

Assuming µ =

2
σA
σ2

1
1
1
∥PΩ (Z − X − E)∥2F − 2 ∥A∥2F − 2 ∥B∥2F − η∥E∥2,1 + C
2σ 2
2σA
2σB

2
and λ = ησA
, maximization of log p(A, B, E|Z, Θ) can be transformed as:

min
X,E

µ
1
∥PΩ (Z − X − E)∥2F + (∥A∥2F + ∥B∥2F ) + λ∥E∥2,1
2
2

(6)

Based on Lemma 1 in [13] and Eq. (6), the objective function Eq. (7) is equivalent to Eq. (1).
µ
min ∥PΩ (Z − X − E)∥2F + ∥X∥∗ + λ∥E∥2,1
(7)
X,E 2
4

	

ROLA for Crowdsourcing on Riemannian
Qian Li etManifold
al. / Procedia Computer Science 108C (2017) 285–294

4

289

ROLA algorithm

This section proposes the Robust Low-Rank Approximation for Crowdsourcing (ROLA) algorithm to eﬃciently solve Eq. (7) via Grassmann optimization. Speciﬁcally, X refers to the
low rank matrix for reﬁned labels and E are annotator-speciﬁc noise. Two subproblems with
respect to X and E can be split from Eq. (7).
{
min µ2 ∥PΩ (Z − X − E)∥2F + ∥X∥∗
X
(8)
min λ∥E∥2,1 + µ2 ∥PΩ (Z − X − E)∥2F
E

Algorithm 1 ROLA: Robust Low-rank Approximation for Crowdsourcing on Matrix Manifold
Require: The tolerance err, rank r, total iterations l and parameters γ and ψ.
Ensure: X: the low-rank component; E: sparse noise;
1: Initialize (U1 , S1 , V1 ) ← SVD(X1 )
2: for ∥Xk+1 − Xk ∥F < err and k <= l do
3:
E ← Ek and compute Pk by Eq. (13)
4:
Calculate the Riemannian gradient grad f (Uk ) and grad f (Vk ) by Eq. (14)
5:
Perform Armijo rule to ﬁnd αk and compute Uk+1 and Vk+1 based on Eq. (16)
6:
Compute Sk+1 by Eq. (17)
T
7:
Xk+1 ← Uk+1 Sk+1 Vk+1
8:
X ← Xk+1 and compute Ek+1
9: end for
10: return X and E

4.1

Low-rank Subproblem for X

The low rank matrix for reﬁned labels are speciﬁed by X and the minimization subproblem
with respect to X is as the following:
min
X

µ
∥PΩ (Z − X − E)∥2F + ∥X∥∗
2

(9)

Let f (X, E) = µ2 ∥PΩ (Z − X − E)∥2F . Note that the variable E is ﬁxed when solving X, for
convenience f (X, E) is written as f (X) in this part. When updating X at iterate k, f (X) is
linearized at the ﬁxed point Xk by adding a proximal term:
γ
f (X) ≈ f (Xk ) + ⟨∇f (Xk ), X − Xk ⟩ + ∥X − Xk ∥2F
(10)
2
where γ is a proximal parameter, ⟨·, ·⟩ is the inner product and ∇f (Xk ) = −µPΩ (Z − Xk − E).
The optimization objective can be further simpliﬁed as the quadratic form:
γ
1
min ∥X − Xk − ∇f (Xk )∥2F + ∥X∥∗
(11)
X 2
γ
4.1.1

Grassmann Manifold Optimization

Grassmann manifold has been successfully applied in many ﬁelds [16, 4, 10]. To obtain Grassmann geometries of the resulting search space, SVD decomposes matrix X in Eq. (11) as
X = USVT = (UO)(OT SO)(VO)T where O is an orthogonal matrix. Deﬁne [U] = {UO :
UO ∈ Sm,r } and [V] = {VO : VO ∈ Sn,r }. [U] and [V] satisfy the notion of Grassmann
5

290	

Qian Li etManifold
al. / Procedia Computer Science 108C (2017) 285–294
ROLA for Crowdsourcing on Riemannian

manifold [10], which also guarantees the uniqueness of matrix factorization. By Eq. (10) and
above SVD decomposition, Eq. (9) can be represented as a Riemannian optimization problem
as follows.
γ
1
min F = ∥USVT − Uk Sk VkT − ∇f (Uk Sk VkT )∥2F + ∥S∥∗
U,S,V
2
γ
(12)
s.t. : [U] ∈ Gm,r , [S] ∈ Rr×r , [V] ∈ Gn,r
where ∇f (Uk Sk VkT ) = PΩ (Z − Uk Sk VkT − E)∥2F , and E is ﬁxed when solving X. Note that
from the deﬁnition of the nuclear norm, we have ∥S∥∗ = ∥X∥∗ .
Alternating direction method (ADM) can be adopted to decompose the optimization objective function in Eq. (12) into several subproblems with respect to each parameter. More
speciﬁcally, the iteration of ADM goes as follows:

= Uk Sk VkT − µγ PΩ (Z − Uk Sk VkT − E)
 Pk



= arg min ∥USk VkT − Pk ∥2F
U


 k+1
[U]∈Gm,r
(13)
V
=
arg min∥Uk Sk VT − Pk ∥2F
k+1


[V]∈Gn,r


γ
 S
T
2

 k+1 = arg min 2 ∥Uk+1 SVk+1 − Pk ∥F + ∥S∥∗
[S]∈Rr×r

4.1.2

Updating U and V

Updating procedures for U and V are similar, since they have similar subproblems, which can
be solved by optimizing on Grassmann manifold. The following part will describe the updating
on U. Grassmann gradient is ﬁrst achieved by projecting the Euclidean gradient of F (·) with
operator PTU Gn,r . It follows that
grad F (U) = PTU Gn,r (∇F (U))

(14)

where Euclidean gradient is ∇F (U) = 2Sk VkT ∥USk VkT −Pk ∥F and PTU Gn,r : Y → (I−UUT )Y.
Then, Retraction maps Y from tangent space to the manifold Gn,r by using QR decomposition,
RGn,r : Y → QR(Y)

(15)

The descent step size is obtained to satisfy Armijo condition that guarantees a suﬃcient degree
of accuracy. Based on above analysis, the optimum U and V at k + 1 is
Uk+1 = qf(Uk + αk grad F (Uk )),
4.1.3

Vk+1 = qf(Vk + αk grad F (Vk ))

(16)

Updating S

To update S, Theorem 2.1 mentioned in [2] is used. Let Pk = Xk − µγ PΩ (Z − Xk − E), the
optimal S at iteration k + 1 for Eq. (12) can be yielded by the soft-thresholding operator:
1
D γ1 (ΣS ) = diag(max(0, σi − ))
Sk+1 = D γ1 (UT Pk V) = GD γ1 (ΣS )HT ,
(17)
γ
where GΣHT is the SVD of UT Pk V, D γ1 (ΣS ) shrinks the singular values of UT Pk V below
the threshold

4.2

1
γ.

The Sparse Subproblem for E

The annotator-speciﬁc noise is speciﬁed by matrix E and the sparse subproblem for annotatorspeciﬁc noise in Eq. (8) is min λ∥E∥2,1 + µ2 ∥PΩ (Z − X − E)∥2F . This can be further reformulated
E

6

	

Qian Li etManifold
al. / Procedia Computer Science 108C (2017) 285–294
ROLA for Crowdsourcing on Riemannian

by adopting the Eq. (10) as follows:
µ
ψ
min ∥E − Ek − PΩ (Z − Ek − X)∥2F + λ∥E∥2,1
E 2
ψ

291

(18)

2
σU
σ2

where ψ is a proximal parameter and µ =
. Let Dk = Ek + ψµ PΩ (Z − Ek − X), the
annotator-speciﬁc noise matrix E can be easily obtained by Lemma 3.3 in [19].

5

Experiments

ROLA can eﬀectively remove the annotator-speciﬁc noise and produce the reﬁned labels with
low-rank property. Majority voting is then used to infer the true labels from such reﬁned lowrank label matrix. This section experimentally analyzes the aggregation accuracy of ROLA on
both synthetic and real-world datasets.

5.1

Analysis on Synthetic Datasets

This section simulates one crowdsourcing task with binary options that is widely used in realworld crowdsourcing platforms. Three crowdsourcing tasks with 100, 500 and 1000 questions,
respectively, are generated. 5 and 20 annotators are simulated to label each question. Each
question has 2 possible answers {-1, +1} and are sampled from the Bernoulli distribution. Two
common malicious behaviors are simulated: (a) random behavior of labeling each question 1
or −1 with probability of 0.5. (b) adversary behavior of labeling incorrectly. The number of
assigned questions per annotator varies from 5 to 35. Ten-fold cross-validation is performed on
each label matrix, and the average over all folds is compared.
To exploit the eﬀect of noise, the degree of missing labels or noise for three label matrices
is varied with diﬀerent sizes, as shown in Fig. 2. The portion of the malicious annotators is
ﬁxed as 30% in Fig. 2(a) and Fig. 2(c), so that in expectation there are 30 malicious annotators
among 100 annotators. Fig. 2(a) depicts the error rates of ROLA under the diﬀerent number of
annotators per question. For three label matrices with diﬀerent sizes, the error rates obtained
by ROLA generally decrease as each question is assigned to more annotators. When assigning
to a small number of annotators, ROLA can obtain signiﬁcantly high accuracy for 200 questions
compared with 500 and 1000 questions. The gap of the error rates among these three label
matrices decreases and is approximately close to 0.02. Under the same settings, Fig. 2(a) shows
the running time consumed by ROLA for inferring the true labels. It is clear that the time
cost of ROLA grows as the size of the label matrix grows, but is irrelevant with the number of
annotators per question, hence it is steady for aggregating all the three matrices.
The noise degree is varied by deﬁning diﬀerent portions of malicious annotators, and the
performance of ROLA is shown in Fig. 2(b) and Fig. 2(d). The number of annotators per
question is set as 30 for both cases. Apparently, the accuracy obtained by ROLA decreases
as the percentage of malicious annotators increases. ROLA adopts l2,1 -norm to regularize the
sparse annotator-speciﬁc deviations in the observed labels. When the percentage of malicious
annotators keeps increasing, the annotator-speciﬁc noise is no longer sparse and thus deteriorate
the performance of ROLA. Fig. 2(d) shows that the running time of ROLA for 1000 labels is
30% faster than that of 500 labels, and 60% faster than that of 200 labels, respectively.

5.2

Analysis on Real-world Datasets

ROLA’s performance is evaluated against four crowdsourcing benchmark methods, including
Majority Voting (MV), Dawid & Skene Model (DS) [3], Diﬃculty-Ability-REsponse estimation
7

292	

ROLA for Crowdsourcing on Riemannian
Qian Li etManifold
al. / Procedia Computer Science 108C (2017) 285–294

(a)

(b)

(c)

(d)

Figure 2: The performance of ROLA on synthetic datasets.

(a) RTE Dataset

(b) TEMP Dataset

(c) Bluebird Dataset

(d) Duchenne Dataset

Figure 3: The accuracy versus the number of annotators per question
model (DARE) [1] and SpEM [12] on real-world crowdsourcing datasets. Four crowdsourcing
datasets from Amazon Mechanical Turk [?] are used in this experiment: RTE (Recognizing
Textual Entailment) [15], TEMP (Temporal event recognition) [15], Duchenne [18] and Bluebird [17].
Fig. 3 shows the error rate vs the number of annotators per question for MV, DS, SpEM,
DARE and ROLA. Note that on average the accuracy of ROLA is higher than that of MV,
DS, SpEM and DARE, under four scenarios in Fig. 3. The performance of ROLA is followed
by DS, SpEM and DARE. MV performs the worst for RTE, TMPE and Bluebird, due to the
8

	

ROLA for Crowdsourcing on Riemannian
Qian Li etManifold
al. / Procedia Computer Science 108C (2017) 285–294

293

inappropriate assumption that all annotators have same error probability. Note that all the
algorithms achieve worse accuracy on Duchenne than on other datasets, because the ratio of
correct labels in Duchenne is less. The gaps between ROLA and other three methods decrease
as the number of annotators per question increases.
Table 1: The t-test of accuracy metric
T-test
Accuracy

DataSet
RTE
TEMP
Duchenne
Bluebird

MV
0-0-10
0-1-10
0-0-10
0-0-10

DS
1-1-8
1-2-7
1-0-9
2-1-7

DARE
1-0-9
3-1-6
3-3-4
1-2-7

SpEM
2-1-7
3-2-5
2-2-6
3-1-6

Table 1 gives t-test to assess whether the means of ROLA are statistically diﬀerent from
others. The test on each data set is carried out 10 rounds with signiﬁcant level at 95%. Set
the null hypothesis as that the average accuracy of ROLA is smaller than other methods. The
results use the form “Worse-TIE-Better” when comparing ROLA with MV, DS, DARE or
SpEM. The results are calculated over two dataset RTE and TEMP, by averaging all the cases
from 1 annotator per question to 10 annotators per question. For instance, the second item
“1-1-8” for RTE dataset indicates that ROLA is signiﬁcantly better than DS in 8 rounds, worse
in 1 round and not signiﬁcant diﬀerent in 1 round. Most results from ROLA are signiﬁcantly
more accurate than those from the compared algorithms with conﬁdence level of 95%. In
conclusion, ROLA achieves improved performance over MV, DS, DARE and SpEM.

6

Conclusion

This paper proposes a novel optimization algorithm on matrix manifold, namely Robust LowRank Approximation for Crowdsourcing (ROLA), to aggregate the labels from a novel perspective. We deﬁne the label noise as annotator-speciﬁc and sparse noise, which can be well regularized by l2,1 -norm. Speciﬁcally, ROLA exploits the underlying low-rank structure of the observed
label matrix and removes the annotator-speciﬁc noise. Consequently, the inference based on the
low-rank label matrix is eﬀective to overcome the diﬃculties incurred by the annotator-speciﬁc
noises and incomplete labels, which further improves the quality of aggregation results. More
importantly, ROLA adopts the state-of-the-art optimization theory, Riemannian optimization,
which is more eﬃciency than other low-rank solvers and also converges with guarantee.
Acknowledgements. Research was partially supported by the National Key Research and Development Program of China (NO.2016YFB0801200), the National Natural Science Foundation
of China (No.61602472, No.U1636217, No.61403369).

References
[1] Yoram Bachrach, Thore Graepel, Tom Minka, and John Guiver. How to grade a test without
knowing the answers—a bayesian graphical model for adaptive crowdsourcing and aptitude testing.
arXiv preprint arXiv:1206.6386, 2012.
[2] Jianfeng Cai, Emmanuel J ès, and Zuowei Shen. A singular value thresholding algorithm for matrix
completion. SIAM Journal on Optimization, 20(4):1956–1982, 2010.
[3] Alexander Philip Dawid and Allan M Skene. Maximum likelihood estimation of observer errorrates using the em algorithm. Applied statistics, pages 20–28, 1979.

9

294	

Qian Li et
al. / Procedia Computer Science 108C (2017) 285–294
ROLA for Crowdsourcing on Riemannian
Manifold

[4] Alan Edelman, Tomás A Arias, and Steven T Smith. The geometry of algorithms with orthogonality constraints. SIAM journal on Matrix Analysis and Applications, 20(2):303–353, 1998.
[5] Hyun Joon Jung et al. Temporal modeling of crowd work quality for quality assurance in crowdsourcing. PhD thesis, 2016.
[6] Hyun Joon Jung and Matthew Lease. Improving quality of crowdsourced labels via probabilistic
matrix factorization. In Workshops at the Twenty-Sixth AAAI Conference on Artificial Intelligence, 2012.
[7] Hyun Joon Jung and Matthew Lease. Inferring missing relevance judgments from crowd workers via
probabilistic matrix factorization. In Proceedings of the 35th international ACM SIGIR conference
on Research and development in information retrieval, pages 1095–1096, 2012.
[8] Qiang Liu, Jian Peng, and Alex T Ihler. Variational inference for crowdsourcing. In Advances in
Neural Information Processing Systems, pages 692–700, 2012.
[9] Jafar Muhammadi, Hamid Reza Rabiee, and Abbas Hosseini. Crowd labeling: a survey. arXiv
preprint arXiv:1301.2774, 2013.
[10] Thanh Ngo and Yousef Saad. Scaled gradients on grassmann manifolds for matrix completion. In
Advances in Neural Information Processing Systems, pages 1412–1420, 2012.
[11] Vikas C Raykar, Shipeng Yu, Linda H Zhao, Gerardo Hermosillo Valadez, Charles Florin, Luca
Bogoni, and Linda Moy. Learning from crowds. The Journal of Machine Learning Research,
11:1297–1322, 2010.
[12] V Rayker. Eliminating spammers and ranking annotators for crowdsourced labeling tasks. Journal
of Machine Learning Research, 13:491–518, 2011.
[13] Jasson D. M. Rennie and Nathan Srebro. Fast maximum margin matrix factorization for collaborative prediction. In International Conference, pages 713–719, 2005.
[14] Ruslan Salakhutdinov and Andriy Mnih. Probabilistic matrix factorization. Citeseer, 2011.
[15] Rion Snow, Brendan O’Connor, Daniel Jurafsky, and Andrew Y Ng. Cheap and fast—but is
it good?: evaluating non-expert annotations for natural language tasks. In Proceedings of the
conference on empirical methods in natural language processing, pages 254–263. Association for
Computational Linguistics, 2008.
[16] Pavan Turaga, Ashok Veeraraghavan, and Rama Chellappa. Statistical analysis on stiefel and
grassmann manifolds with applications in computer vision. In Computer Vision and Pattern
Recognition, 2008. CVPR 2008. IEEE Conference on, pages 1–8. IEEE, 2008.
[17] Peter Welinder, Steve Branson, Pietro Perona, and Serge J Belongie. The multidimensional wisdom
of crowds. In Advances in neural information processing systems, pages 2424–2432, 2010.
[18] Jacob Whitehill, Ting-fan Wu, Jacob Bergsma, Javier R Movellan, and Paul L Ruvolo. Whose
vote should count more: Optimal integration of labels from labelers of unknown expertise. In
Advances in neural information processing systems, pages 2035–2043, 2009.
[19] Junfeng Yang, Wotao Yin, Yin Zhang, and Yilun Wang. A fast algorithm for edge-preserving
variational multichannel image restoration. SIAM Journal on Imaging Sciences, 2(2):569–592,
2009.
[20] Jinfeng Yi, Rong Jin, Anil K Jain, and Shaili Jain. Crowdclustering with sparse pairwise labels:
A matrix completion approach. In AAAI Workshop on Human Computation, volume 2. Citeseer,
2012.
[21] Jinfeng Yi, Rong Jin, Shaili Jain, and Anil Jain. Inferring users preferences from crowdsourced
pairwise comparisons: A matrix completion approach. In First AAAI Conference on Human
Computation and Crowdsourcing, 2013.
[22] Dongqing Zhu and Ben Carterette. An analysis of assessor behavior in crowdsourced preference
judgments. In SIGIR 2010 workshop on crowdsourcing for search evaluation, pages 17–20. Citeseer,
2010.

10

