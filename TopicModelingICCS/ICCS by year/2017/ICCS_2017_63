Available online at www.sciencedirect.com

ScienceDirect
Procedia Computer Science 108C (2017) 988–997

International Conference on Computational Science, ICCS 2017, 12-14 June 2017,
Zurich, Switzerland

Clustering Mixed-Attribute Data using Random Walk
Clustering Mixed-Attribute
Data using Random Walk
Andrew Skabar
School of Engineering and Mathematical Sciences
AndrewMelbourne,
Skabar Australia
La Trobe University,
School of Engineering
and Mathematical Sciences
a.skabar@latrobe.edu.au
La Trobe University, Melbourne, Australia
a.skabar@latrobe.edu.au

Abstract
Most clustering algorithms rely in some fundamental way on a measure of either similarity or distance—
either
between objects themselves, or between objects and cluster centroids. When the dataset contains
Abstract
mixed
attributes,algorithms
defining arely
suitable
measure
can be way
problematic.
This of
paper
presents
a general
graphMost clustering
in some
fundamental
on a measure
either
similarity
or distance—
based
method for
clustering
mixed-attribute
thatcluster
does not
requireWhen
any explicit
measure
of
either between
objects
themselves,
or between datasets
objects and
centroids.
the dataset
contains
mixed attributes,
defining
a suitable
measure
can beofproblematic.
paper
presents
a general
graphsimilarity
or distance.
Empirical
results
on a range
well-knownThis
datasets
using
a range
of evaluation
based method
datasetscompetitive
that does not
anyclustering
explicit measure
of
measures
show for
thatclustering
the methodmixed-attribute
achieves performance
withrequire
traditional
algorithms
similarity
or distance.
Empirical results
on a range
of well-known
datasets
using
a range
of evaluation
that
require
explicit calculation
of distance
or similarity,
as well
as with
more
recently
proposed
clustering
algorithms
on matrix
factorization.
measures show
that thebased
method
achieves
performance competitive with traditional clustering algorithms
that require explicit calculation of distance or similarity, as well as with more recently proposed
© 2017 The Authors. Published by Elsevier B.V.
Keywords: Clustering,
Graph
Mixed-attribute
data, Random walk
clustering
algorithms
basedcentrality,
on
matrix
factorization.
Peer-review
under responsibility
of the
scientific
committee of the International Conference on Computational Science
Keywords: Clustering, Graph centrality, Mixed-attribute data, Random walk

1 Introduction
clustering algorithms rely in some fundamental way on a measure of similarity or distance. For
1 Most
Introduction

example, relational clustering algorithms such as k-medoids (Kaufman & Rousseeuw, 1987), spectral
Most clustering
in some fundamental
way
on a measure
similarity
distance. For
clustering
(Luxburg,algorithms
2007) andrely
affinity-propagation
(Frey
& Dueck,
2007)of
take
pairwiseorsimilarities
or
distances
between objects
directly
as input,
whereas
algorithms
such as&k-means
(MacQueen,
1967),
example, relational
clustering
algorithms
such
as k-medoids
(Kaufman
Rousseeuw,
1987), spectral
fuzzy
c-means
(Dunn,
1973)
mixture model approaches
(Duda 2007)
et al., take
2000)
operatesimilarities
directly inora
clustering
(Luxburg,
2007)
andand
affinity-propagation
(Frey & Dueck,
pairwise
distances
between
directly
input,and
whereas
algorithms
such or
as distances
k-means (MacQueen,
vector space,
takingobjects
attribute
data asasinput,
calculating
similarities
as required. 1967),
fuzzy
c-means
(Dunn,it1973)
mixture
model
approachessimilarity/distance
(Duda et al., 2000)
operate For
directly
in a
In many
situations
is not and
difficult
to find
an appropriate
measure.
example,
vector
space,consisting
taking attribute
as input,attributes,
and calculating
similarities
distances
required.are often
in
domains
solely data
of numeric
Euclidean
distanceorand
cosine as
similarity
appropriate
measures it
ofisdistance
and to
similarity
respectively,
and on categorical
domains
In many situations
not difficult
find an appropriate
similarity/distance
measure.
ForHamming
example,
distance
or the
Tanimoto
similarity
measure
are oftenEuclidean
used. Mixed-attribute
pose a greater
chalin domains
consisting
solely
of numeric
attributes,
distance anddatasets
cosine similarity
are often
appropriate
measures
of to
distance
respectively,
and onfrom
categorical
domains
Hamming
lenge, and one
approach
dealingand
withsimilarity
these is to
convert attributes
categorical
to numeric
and
distance
the Tanimoto
similarity
measure
are often
Mixed-attribute
datasets
a greater
chalthen use or
a numeric
similarity
or distance
measure,
orused.
alternatively,
to discretize
thepose
numeric
attributes
lenge,
andapply
one approach
to dealing
withAthese
is to convert
attributes
from
categorical
numeric and
and
then
a categorical
measure.
significant
problem
with this,
however,
is thetopossibility
of
then use aresults
numeric
similarity
or distance
measure,
or alternatively,
to discretize
the numeric
attributes
distorted
arising
from either
the loss
of information
in converting
from numeric
to categorical,
or
the introduction
information
in converting
fromwith
categorical
to numeric.
andindeed
then apply
a categoricalofmeasure.
A significant
problem
this, however,
is the Consider,
possibilityfor
of
distorted results arising from either the loss of information in converting from numeric to categorical,
or indeed the introduction of information in converting from categorical to numeric. Consider, for
1877-0509 © 2017 The Authors. Published by Elsevier B.V.
Peer-review under responsibility of the scientific committee of the International Conference on Computational Science
10.1016/j.procs.2017.05.083

	

Andrew Skabar et al. / Procedia Computer Science 108C (2017) 988–997

example, the challenge of converting attributes on the well-known Australian Credit dataset (Quinlan,
1987). Although containing only 690 examples in total, these examples are described over 16 input
attributes, ten of which are categorical (taking between two and 14 values), and six of which are
continuous and highly skewed. Then there is the problem of missing attribute values.
There are other situations in which devising a suitable distance or similarity measure can be
problematic. For example, consider social network data in which objects (people) are described in terms
of their attributes (e.g., age, gender, salary) as well as in terms of their relationships (e.g., familial,
professional) with other objects. While clustering could be performed based on either the attribute data
alone (vector space clustering) or the relational data alone (relational clustering), intuitively one might
expect best performance to be achieved through combining these somehow; for example, by defining a
similarity/distance measure based on both the attribute and the relational data. However, defining such
a measure might prove to be a difficult task: different objects might be described over different sets of
attributes, with some pairs of objects having perhaps no attributes in common; there may be more than
one type of relational link, some of which might not be symmetrical; and, even if a suitable measure
could be devised for some particular dataset, it may not be easily adaptable to other social network data.
This paper presents a general graph-based method for clustering mixed-attribute datasets that does
not require any explicit measure of similarity or distance between the objects to be clustered. Under this
approach, objects and their categorical and numeric attribute values are represented using a bipartite
graph in which links exist between object nodes and attribute value nodes, but not between nodes of the
same type. The clustering algorithm we propose can loosely be thought of as a graph version of the
mixture model approach. However, whereas clusters in a Gaussian mixture model are specified by the
means and covariances of two or more Gaussians, in our model no explicit density model is used to
represent clusters. Instead, clusters correspond to stationary distributions over the nodes in the graph.
The problem is to discover stationary distributions that lead to a good clustering. Empirical results on a
range of well-known datasets demonstrate that the proposed method achieves performance competitive
to that of commonly used clustering algorithms.
The paper is structured as follows. Section 2 provides an overview of related work in the area of
graph-based clustering. Section 3 describes the scheme used to represent an arbitrary mixed-attribute
dataset as a bipartite graph, and Section 4 describes the graph-based algorithm used to perform
clustering. Experimental results of evaluating the method on a number of well-known datasets are
presented in Section 5, and Section 6 concludes the paper.

2 Motivation
The work described in this paper has been largely motivated by recent work in the area of co-clustering.
If we think of a dataset consisting of m rows, each corresponding to an object, and n columns, each
corresponding to an attribute, co-clustering can be thought of as the simultaneous clustering of rows and
columns. While the idea of co-clustering was first introduced by Hartigan (1972) in the early 1970s, it
wasn’t until the early 2000s that co-clustering started to appear regularly in the literature, being applied to
areas such as document clustering (Dhillon, 2001; Zha et al., 2001; Rege et al., 2006), gene expression
data clustering (Cheng & Church, 2000) and recommender systems (George & Merugu, 2005).
To understand the appeal of co-clustering, consider the problem of document clustering,
conventionally performed by clustering documents (objects) based on the words (attributes) that they
contain. The premise of word-document co-clustering (i.e., the simultaneous clustering of words and
documents) is that words and documents have a mutually supporting role: documents tend to be about
similar topics if they have words in common, and words tend to be related if they appear in the same
documents. In other words, word clustering induces document clustering, and document clustering
induces word clustering.

989

Andrew Skabar et al. / Procedia Computer Science 108C (2017) 988–997

990	

A natural way to represent this mutually reinforcing relationship between words and documents is
to use a bipartite graph structure in which connections exist between document nodes and word nodes,
but not between nodes of the same type. The weights on the graph edges might be binary, representing
the presence/absence of a word in a document, or alternatively they could be continuous, representing a
tf-idf score, or some other measure of the strength of the relationship between the word and document.
Zha et al. (2001) and Dhillon (2001) were the first to consider the problem of word-document coclustering. Whereas Zha et al. (2001) perform the graph partitioning using a singular value
decomposition of the weight matrix of the bipartite graph, Dhillon (2001) uses spectral decomposition
to perform the partitioning. Rege et al. (2006) also use a bipartite graph approach, but propose a new
algorithm to perform the partitioning. Note, however, that the co-clustering algorithm need not be graphbased. For example, Dhillon et al. (2003) use an information-theoretic approach based on maximizing
the mutual information between clustered random variables, and Shan & Banerjee (2008) treat coclustering as a generative mixture modeling problem and propose a Bayesian co-clustering technique.
What is interesting about these approaches is that they do not make any explicit use of a distance or
similarity measure between objects—any notion of distance or similarity is entirely implicit. However,
a short-coming of current approaches to co-clustering is that they are limited in the range of attribute
types to which they can be applied. For example, word-document co-clustering assumes that either all
the attributes are binary (i.e., a word is either present or not present in a document), or alternatively, that
all attributes are continuous (e.g., tf-idf scores). Moreover, in the case of continuous attributes there is
a lack of generality, since the degree to which a word is important in determining the (implicit) similarity
between two documents always increases with the magnitude of the tf-idf score for that word, and we
can easily imagine domains in which two objects may be similar by virtue of sharing a small, not large,
value for some continuous attribute.
However, not all co-clustering approaches treat continuous attributes in this way; for example, in
Cheng & Church (2000) it is the mean squared residue, which measures the coherence between genes
and conditions in a cluster, that is the foundation for discovering clusters. But note further that coclustering applied to gene expression data is always based on continuous attributes. Where co-clustering
has been applied to domains containing attributes other than continuous or binary, they are usually
treated as discrete numeric (e.g., the MovieLens and Jester datasets used in Shan & Banerjee, 2008), in
which case it is again the magnitude of the attribute values that determines the strength of the connection.

3 Graph
Representation of Mixed-Attribute Data
w
3

The graphs we use to represent mixed-attribute datasets contain two types of nodes: object nodes
(O-nodes), and attribute-value nodes (AV-nodes). O-nodes represent the objects (i.e., instances,
examples, patterns) that we are interested in clustering, and AV-nodes represent attribute values that
those objects can possess. The graphs are bipartite, meaning that links may exist only between O-nodes
and AV-nodes, but not between nodes of the same type. A sample graph is shown in Figure 1.
ATT. JOB

unemployed

1

unskilled

skilled

1

1

management

1

1

1

1

1

OBJECTS: Person_1

Person_3

Person_2

0.8
1

0.2

Person_4

1

Person_5

1
0.5

0.5

ATT. SALARY
low

medium

1
1

high

Figure 1: Bipartite graph showing five objects described over two attributes.

	

Andrew Skabar et al. / Procedia Computer Science 108C (2017) 988–997

The graph contains five O-nodes, each corresponding to one of five persons (Person_1, Person_2,
…). There are seven AV-nodes, three of which correspond to the attribute salary, and four to the attribute
job. The attribute job is categorical, and an object may normally take only one value for such an attribute.
For this reason, the weights on links between an O-node and AV-nodes corresponding to categorical
attributes are binary (i.e., 1 or 0) (To avoid clutter, we omit displaying 0-weight connections).
While the above representation for categorical attributes is straightforward, numeric attributes pose
a greater challenge. On first inspection, it might appear that each numeric attribute could be represented
using a single node, with the weight of the connection between an object and that attribute node
representing that object’s (possibly scaled) value for that attribute. However the critical problem with
this is that it does not account for the fact that two objects may be similar by virtue of sharing similar,
but not necessarily high, values for some numeric attribute. To see why, consider a random walker on
such a graph. The probability of the walker moving from one node to another depends on the weight of
the edge connecting those nodes. Since object nodes are not connected directly, walk from one object
node to some other object node can only proceed via one of the attribute nodes. If two object nodes
share a high value for some numeric attribute, then the probability that the walker proceeds through that
connection will be higher than would be the case if the object nodes both had low values for that same
attribute. Thus, a representation in which only a single node is used to represent numeric attributes falls
short in fully capturing the important relationships in the data.
This problem can be solved by using a distributed representation by which numeric attributes are
represented using multiple nodes. One approach is to simply discretize numeric attributes into a finite
number of bins, each corresponding to a unique AV-node for that attribute. For example, values taken
by the continuous attribute salary might be discretized into one of low, medium or high. This distributed
representation comes some way towards solving the problem, since random walk can now proceed
through any of the three AV nodes corresponding to that attribute. However, discretizing into one of
three bins still runs the risk of losing information, particularly at the boundaries, since two objects may
have a similar
value for the attribute, but one may be discretized to low, the other to medium. Thus,
w3
rather than using a crisp discretization, we use a fuzzy discretization scheme (see Figure 2) whereby a
numeric attribute value can be considered to be, for example, partly low, and partly medium. Note that
prior to computing membership values, it is assumed that continuous attributes have been normalized
to the interval [0, 1]. Some additional
transformation
might also be applied to highly-skewed data.
Membership
value
Low

1.0

Medium

High

0.75
x3
x2

0.5
0.25

x1
0.0
x0

0

0.1 0.2

0.3 0.4

0.5 0.6

0.7 0.8

0.9 1.0

Figure 2: Fuzzy membership functions for transforming continuous-valued attributes.

To illustrate the treatment of continuous attributes, consider Table 1, which shows the original
mixed-attribute dataset corresponding to the graph in Figure 1. Values for the continuous attribute salary
must first be normalized to the interval [0, 1], and suppose that a value of $65,000 (Person_5) normalizes
to 0.7. From Figure 2 it can be seen that for the attribute salary, Person_5 would therefore have a 0.5
membership to medium and a 0.5 membership to high.
Processing the original dataset in this way results in the m × n matrix shown in Table 2, where m is
the number of objects and n is the number of attribute values. We refer to this matrix as B.

991

992	

Andrew Skabar et al. / Procedia Computer Science 108C (2017) 988–997

Table 1: Original object-attribute table.

Table 2: Processed object-attribute table.

To represent the actual bipartite graph, we construct from matrix B the adjacency matrix M:

 0 B
M  T

0 
 B
Matrix M is a square matrix with m + n rows and columns. The first m rows and columns represent
objects (O-nodes), and the remaining n rows and columns represent attribute values (AV-nodes). The
matrix M fully describes the graph in Figure 1. We note that on most datasets the adjacency matrix M
will be very sparse. For example, on the Wisconsin Breast Cancer dataset (Wolberg & Mangasarian
1990), which contains 699 examples described using nine categorical attributes, each of which can take
one of 10 possible values, only 2% of the entries in matrix M are non-zero.
Finally, we note that the representation scheme described in this section deals in a natural and
intuitive way with missing or uncertain attribute values. For example, a missing attribute value can be
represented by inserting a link between an object and each attribute value node corresponding to the
missing attribute value, in such a way that these links are equally weighted, and sum to 1. This allows
the object to remain implicitly connected with other objects through this attribute.

4 Node Clustering using Random Walk
The algorithm we use to perform clustering can loosely be thought of as a graph version of the
mixture model approach, in which the data is modelled as a combination of components, with each
component treated as a cluster. However, unlike Gaussian mixture models, which operate in a Euclidean
space and use a likelihood function parameterized by the means and covariances of Gaussian
components, no explicit density model is used to represent clusters. Instead, clusters are represented as
stationary distributions over the nodes in the graph, and these stationary distributions are calculated by
solving an appropriate eigenvector equation. Prior to presenting the algorithm, it is appropriate to first
discuss the notions of random walk and graph centrality.
The amount of time a random walker spends visiting some node on a graph provides a measure of
the relative importance of that node, and the importance of nodes can be found by determining the

	

Andrew Skabar et al. / Procedia Computer Science 108C (2017) 988–997

993

stationary distribution of the graph. For a graph with adjacency matrix M {wij} , the stationary
distribution is represented by the dominant eigenvector c of the following equation
c dMc  (1  d )1 / N

(1)

(Newman 2010). We refer to vector c as the centrality vector, and its components as centrality scores.
The particular form of eigenvector centrality expressed in (1) is known as Katz Centrality (Katz 1953).
The second term on the right hand side of (1) can be thought of as the probability that at each step
the walker is teleported with probability (1– d) to a random node rather than following an edge.
Alternatively, the second term can be thought of as specifying the starting point, or origin, for the walk.
In (1), each node is equally likely to be the origin, and it is straightforward to modify the eigenvector
equation so as to specify some arbitrary non-uniform distribution of nodes as the origin.

c dMc  (1  d )  

(2)

Vector  in (2) specifies this arbitrary distribution; we refer to it as the personalization vector.
We note that there are two types of nodes in our graph: O-nodes and AV-nodes. Since we are
interested in clustering objects (and not attribute values), we will assume that the components of 
corresponding to AV-nodes will always be 0. Similarly, when we calculate the principal eigenvector, c,
we will set the components corresponding to AV-nodes to 0, and normalize the remaining components
to sum to 1.
Whereas clusters in a Gaussian mixture model are specified by the means and covariances of two or
more Gaussians, in our model clusters are represented using two or more personalization vectors. The
problem, therefore, is to discover these vectors. We now present the algorithm.
Let yik represent the membership of O-node i to cluster k. The membership values must satisfy the
conditions



C
k 1

yik  1

yik  0,
i 1,..., N 
; k 1,..., C

where N is the total number of O-nodes, and C is the number of clusters. Following random initialization
of personalization vectors  k , the algorithm iterates the following steps until convergence of the
membership values:
1. Compute the first eigenvector x k of the equation


x k dMx k   (1  d ) k   k   , (k = 1,…, C)
2. Update cluster membership values using the eigenvector components from Step 1, ensuring that
cluster membership values sum to 1 across clusters for each O-node

yik  xik

C

 j1 xij , (i = 1,…, N; k = 1,…, C)

3. Update personalization vectors  k by scaling eigenvector components from Step 1 with cluster
membership values from Step 2, and normalizing so that components of each  k sum to one.

 ik   xik yik 

  x y  , (i = 1,…, N; k = 1,…, C)
N

j 1

k
j

k
j

When the algorithm has converged, the stationary distribution xk corresponding to some personalization
vector  k will be equal to the personalization vector; i.e.,  k  xk , k = 1,…,C, and the cluster
membership value yik represents the membership of O-node i to cluster k. If hard clustering is required,
then an O-node can simply be assigned to the cluster for which its membership is greatest. The degree

Andrew Skabar et al. / Procedia Computer Science 108C (2017) 988–997

994	

of cluster overlap can be controlled by varying the value of d that appears in the Eigenvector equation
shown in Step 1. The higher the value of d, the crisper (i.e., less overlap in) the clustering. In all our
experiments we have used a value of d = 0.15.
To solve the eigenvector equations in Step 1 we use the power iteration method, which begins with
a random vector xi, and simply iterates the step 
xi 1 dMxi   (1  d )     until convergence, when x
will be the dominant eigenvector (Newman, 2010).

5 Experiments
In this section we provide results of applying the graph-based clustering approach to several realworld datasets, and comparing its performance with a range of existing clustering algorithms.
The datasets are all well-known and have been widely used in the machine learning and pattern
recognition literature. Historically the datasets have been used mainly to evaluate supervised learning
techniques; however, in our clustering experiments we treat the datasets as gold-standard clustering
datasets. There is no concept of training or test sets; all examples are clustered on the basis of the input
attributes, and the class labels are used only to provide an external measure of clustering quality. In each
case, we set the number of clusters equal to the (known) number of classes. All datasets are available
from the UCI Machine Learning Repository (Bache & Lichman, 2013).
The properties of the datasets are described in Table 3. The column labelled ‘Attributes’ shows the
number of categorical attribute (nc), and the number of numerical attributes (nn). All datasets have a
mixture of categorical and numerical attributes with the exception of the Iris and Wisconsin Breast
Cancer datasets. Although not shown in the table, the range of values taken by categorical variables can
vary greatly. For example on the Australian Credit dataset categorical variables can take between 2 and
14 values. Note that for the Cleveland dataset, the class attribute is integer-valued, ranging from 0
(disease not present) through to 4. Most experiments on this dataset have concentrated on simply
attempting to distinguish presence from absence. In this paper we use two versions of the dataset: one
in which the attempt is to discover two clusters, and the other which attempts to discover five.
Dataset
Aust. Credit
Cleveland H.D. (2 Cl)
Cleveland H.D. (5 Cl)
Hepatitis
Iris
Labor Relations
Wisc. Breast. Cancer

Instances
690
303
303
155
150
57
699

Attributes
(nc + nn)
9+5
5+8
5+8
13 + 6
0+4
8+8
9+0

Classes

% vals missing

2
2
5
2
3
2
2

0.65%
0.15%
0.15%
4.90%
0.00%
35.7%
0.00%

Table 3: Dataset characteristics.

We compare the performance of our graph-based algorithm with two relational clustering
algorithms, with pairwise similarities between objects calculated using the method proposed by Gower
(1971), which is one of the most popular measures of similarity for mixed data types.
The Gower Similarity Coefficient defines the similarity between two l-dimensional mixed-value
vectors xi and xj as

s( x , x )   s ( x , x )



l
l
i
j
q
i
j
q 1 q 1

wq .

If the qth coordinates of xi and x j are nominal or ordinal, sq ( xi , x j ) = 1 if xiq = x jq ; otherwise
1| xiq  x jq | rq , where rq is the range
sq ( xi , x j ) = 0. If the qth coordinates are continuous, sq ( xi , x j ) 

	

Andrew Skabar et al. / Procedia Computer Science 108C (2017) 988–997

between the maximum and minimum values of the qth coordinate. If either or both of xi and xj are
unknown, then sq ( xi , x j ) = 0 and wq = 0; otherwise wq = 1.
The following two relational clustering algorithms have been applied: relational fuzzy c-means
(RFCM), and symmetric nonnegative matrix factorization (SNMF). Relational fuzzy c-means is just the
standard fuzzy c-means algorithm; but rather than representing objects as a vector of attribute values,
objects are represented by a vector of their relationships (i.e., similarities) with other objects in the
dataset. Corsini et al. (2005) refer to this as ARCA (Any Relation Clustering Algorithm). Similarly,
SNMF is a special case of nonnegative matrix factorization (NMF) that is based on a similarity measure
between data points (Zass & Shashua, 2005; Ding et al., 2006; He et al., 2011). Given a positive
symmetric matrix V, SNMF aims to find a nonnegative matrix P such that V ≈ PPT. Thus, rows of P
represent similarity vectors, and columns represent linear combinations of these similarity vectors.
Since the matrix B (see Section 3) can be used as the input to vector space clustering algorithms, we
also compare the performance of our approach with that of fuzzy c-means (FCM) and nonnegative
matrix factorization (NMF) (Lee & Seung, 1999).
Dataset
Aust. Credit

Cleveland H.D (2 Cl)

Cleveland H.D (5 Cl)

Hepatitis

Iris

Labor Relations

Wisconsin

Algorithm
Graph Clustering
RFCM
SNMF
FCM
NMF
Graph Clustering
RFCM
SNMF
FCM
NMF
Graph Clustering
RFCM
SNMF
FCM
NMF
Graph Clustering
RFCM
SNMF
FCM
NMF
Graph Clustering
RFCM
SNMF
FCM
NMF
Graph Clustering
RFCM
SNMF
FCM
NMF
Graph Clustering
RFCM
SNMF
FCM
NMF

V-meas.
0.293
0.108
0.208
0.294
0.322
0.354
0.353
0.331
0.296
0.350
0.172
0.236
0.172
0.200
0.168
0.233
0.191
0.187
0.191
0.219
0.864
0.751
0.797
0.854
0.862
0.533
0.391
0.204
0.464
0.579
0.657
0.391
0.204
0.714
0.707

Rand
0.681
0.562
0.631
0.692
0.706
0.724
0.724
0.710
0.689
0.722
0.674
0.645
0.641
0.647
0.638
0.602
0.602
0.588
0.602
0.631
0.949
0.874
0.886
0.938
0.948
0.808
0.754
0.641
0.768
0.842
0.884
0.754
0.641
0.897
0.902

F-meas.
0.683
0.593
0.633
0.696
0.708
0.724
0.724
0.711
0.690
0.723
0.443
0.374
0.366
0.364
0.363
0.661
0.661
0.649
0.661
0.690
0.923
0.812
0.832
0.911
0.921
0.816
0.782
0.662
0.776
0.850
0.900
0.782
0.662
0.904
0.911

Table 4: Clustering results. Boldface is used to indicate the algorithm which obtained the best
performance for each evaluation measure.

Table 4 compares the performance the algorithms using three cluster evaluation measures. Vmeasure (Rosenberg & Hirchberg, 2007), also known as Normalized Mutual Information (NMI)
(Manning et al., 2008), is defined as V = hc / (h + c), where h is the homogeneity (a measure of the extent
to which clusters contain only objects from a single class), and c is the completeness (a measure of the

995

Andrew Skabar et al. / Procedia Computer Science 108C (2017) 988–997

996	

extent to which all objects from a single class are assigned to a single cluster). Details on calculating h
and c can be found in Manning et al. (2007). Rand index (Rand, 1971) and F-measure are based on a
combinatorial approach which considers each possible pair of objects. Each pair can fall into one of four
groups: if both objects belong to the same class and same cluster then the pair is a true positive (TP); if
objects belong to the same cluster but different classes the pair is a false positive (FP); if objects belong
to the same class but different clusters the pair is a false negative (FN); otherwise the objects belong to
different classes and different clusters, and the pair is a true negative (TN). The Rand index is simply
the accuracy; i.e., RI = (TP + TN)/(TP + FP + FN + TN), whereas F-measure is the harmonic mean of
the precision and recall; i.e., F-measure = 2PR/(P + R), where P = TP/(TP + FP) and R = TP/(TP + FN).
On three of the datasets, our proposed algorithm achieved best or equal-best performance across
most measures, and on the remaining datasets it achieved second- or third-best performance across most
measures, indicating that its performance is competitive with that of conventional clustering algorithms.
Of course, no clustering algorithm could be expected to give best performance across all datasets, since
the characteristics of some algorithms are better suited to certain types of data than others. For example,
if we have a dataset consisting of numerical attributes that can be accurately modelled using Gaussians,
then a mixture model approach would probably give the best performance.

6 Conclusion
The research described in this paper was motivated by the question of whether it might be possible to
devise a general graph-based clustering method that does not require explicitly defining a measure of either
distance or similarity between the objects to be clustered, and which can be applied to mixed-attribute
datasets. The results shown above confirm that by representing objects and their attribute values on a
bipartite graph using the representation proposed in Section 3, and then applying the clustering algorithm
described in Section 4, clustering performance can be achieved which is competitive with that of existing
algorithms.
While the main advantage of the approach is that it does not require explicit definition of a
similarity/distance measure, it also has several other appealing qualities. Whereas many clustering
algorithms discover hard clusters, the proposed algorithm assigns partial membership to clusters, and
the degree of overlap can be controlled through the parameter d which appears in the eigenvector
equation. As described in Section 3, the representation scheme deals in a natural and intuitive way with
missing or uncertain attribute values. Although we have been concerned with clustering on graphs which
are strictly bipartite, the method can be trivially extended to perform clustering on graphs that also
include object-object connections. Importantly, this does not require any change to either the
representation or the clustering algorithm. Perhaps the most attractive feature of the approach is its
conceptual simplicity. Objects and their properties (i.e., attribute values) are represented using a bipartite
graph. Clustering is then performed based on what is essentially no more than just simple random walk
over that graph.

References
Bache, K. & Lichman, M. UCI Machine Learning Repository. [http://archive.ics.uci.edu/ml]. Irvine,
CA: University of California, School of Information and Computer Science, 2013.
Cheng, Y. & Church, G.M. Biclustering of expression data. Proceedings of the Eighth International
Conference on Intelligent Systems for Molecular Biology, 93–103, 2000.
Corsini, P., Lazzerini, F. & Marcelloni, F. A new fuzzy relational clustering algorithm based on the
fuzzy c-means algorithm. Soft Computing, 9:439-447, 2005.

	

Andrew Skabar et al. / Procedia Computer Science 108C (2017) 988–997

Dhillon, I.S. (2001). Co-clustering documents and words using bipartite spectral graph partitioning.
Proceedings of the 7th ACM SIGKDD International Conference on Knowledge Discovery and Data
Mining, 269–274, 2001.
Dhillon, I.S., Subramanyam, M. & Modha, D.S. Information-theoretic co-clustering. Proceedings of the
Ninth ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, 89-98, 2003.
Ding, C., LI, T., Peng, W. & Park, H. Orthogonal nonnegative matrix t-factorizations for clustering.
Proceedings of the 12th ACM SIGKDD international conference on Knowledge discovery and data
mining, ACM, 126-135, 2006.
Duda, R.O., Hart, P.E., & Stork, D.G. Pattern classification. John Wiley and Sons, 2nd edition, 2000.
Dunn, J.C. A fuzzy relative of the ISODATA process and its use in detecting compact well-separated
clusters. Journal of Cybernetics, 3(3):32-57, 1973.
Frey, B.J. & Dueck, D. Clustering by passing messages between data points. Science, 315:972-976, 2007.
George, T. & Merugu, S. A scalable collaborative filtering framework based on co-clustering.
Proceedings of the International Conference on Data Mining, 625-628, 2005.
Gower, J.C. A general coefficient of similarity and some of its properties. Biometrics, 27:857-872, 1971.
Hartigan. J.A. Direct clustering of a data matrix. Journal of the American Statistical Association,
67( 337):123–129, 1972.
He, Z., Xie, S., Zdunek, R., Zhou, G. & Cichocki, A. Symmetric nonnegative matrix factorization:
algorithms and applications to probabilistic clustering. IEEE Transactions on Neural Networks,
22(12):2117-2131, 2011.
Katz, L. A new status index derived from sociometric index. Psychometrika, 18:39-43, 1953.
Kaufman, L. & Rousseeuw, P.J. Clustering by means of medoids. In Godge, Y. (ed.) Statistical
Analysis based on the L1 Norm, pp. 405-416. Elsevier, Amsterdam: North Holland, 1987.
Lee, D.D. & Seung, H.S. Learning the parts of objects by non-negative matrix factorization. Nature,
401(6755): 788-791, 1999.
Luxburg, U.V. A tutorial on spectral clustering. Statistics and Computing, 17(4):395-416, 2007.
MacQueen, J.B. Some methods for classification and analysis of multivariate observations.
Proceedings of the 5th Berkeley Symposium on Mathematical Statistics and Probability, 281-297, 1967.
Manning, C.D., Raghavan, P. & Schütze, H. Introduction to information retrieval. Cambridge
University Press, Cambridge, 2008.
Newman, M.E.J. Networks: an introduction. Oxford University Press, Oxford, New York, 2010.
Quinlan, J.R. (1987). Simplifying decision trees. International Journal of Man-Machine Studies,
27(3), 221-234.
Rand, W.M. Objective criteria for the evaluation of clustering methods. American Statistical
Association Journal, 66( 338):846-850, 1971.
Rege, M., Ming, D. & Fotouhi, F. Co-clustering documents and words using bipartite isoperimetric
graph partitioning. Proceedings of the Sixth International Conference on Data Mining, 532-541, 2006.
Rosenberg, A. & Hirschberg, J. V-Measure: a conditional entropy-based external cluster evaluation
measure. Proceedings of EMNLP, 410-420, 2007.
Shan, H. & Banerjee, A. Bayesian co-clustering. Proceedings of the 13th IEEE International
Conference on Data Mining, 530-539, 2008.
Wolberg, W.H. & Mangasarian, O.L. Multisurface method of pattern separation for medical diagnosis
applied to breast cytology. Proceedings of the National Academy of Sciences, 87:9193-9196, 1990.
Zass, R. & Shashua, A. A unifying approach to hard and probabilistic clustering. Proceedings of
Tenth IEEE International Conference on Computer Vision, (1):294-301, 2005.
Zha, H. , He, X, Ding, C., Simon, H. & Gu, M. Bipartite graph partitioning and data clustering.
Proceedings of the Tenth International Conference on Information and Knowledge Management, 2532, 2001.

997

