Available online at www.sciencedirect.com

ScienceDirect

This space is reserved for the Procedia header, do not use it
Procedia Computer Science 108C (2017) 1060–1070

This space is reserved for the Procedia header, do not use it
This space is reserved for the Procedia header, do not use it

International Conference on Computational Science, ICCS 2017, 12-14 June 2017,
Zurich, Switzerland

EfficientPMM: Finite Automata Based Efficient Pattern
EfficientPMM: Finite
Automata
Based Efficient Pattern
Matching
Machine
EfficientPMM: Finite
Automata
Based Efficient Pattern
Matching
Machine
Matching
Machine
Ramanpreet
Singh1 and
Ali A. Ghorbani2
1

2

Ramanpreet
Singh ofand
A. Ghorbani
Faculty of Computer
Science, University
NewAli
Brunswick,
Fredericton, NB, Canada
Ramanpreet
Singh1 and
Ali A. Ghorbani2
1:b4s79@unb.ca,
2:ghorbani@unb.ca
Faculty of Computer Science, University of New Brunswick, Fredericton, NB, Canada
1:b4s79@unb.ca,
ghorbani@unb.ca
Faculty of Computer Science,
University of2:New
Brunswick, Fredericton, NB, Canada
1:b4s79@unb.ca, 2:ghorbani@unb.ca
Abstract
In
text mining, vector space and bag of word models are poor candidates for topic discovery as
Abstract
they
loose
wordvector
order and
which
are very
crucial
in understanding
meaning
Abstract
In
text
mining,
spaceco-occurrence,
and bag of word
models
are poor
candidates
for topic the
discovery
as
of
a
document.
Phrase
based
models
has
proven
to
be
promising
in
capturing
the
underlying
In
text
mining,
spaceco-occurrence,
and bag of word
models
are poor
candidates
for topic the
discovery
as
they
loose
wordvector
order and
which
are very
crucial
in understanding
meaning
document
This
paper proposes
a new
document
and algorithm
tomeaning
perform
they
loose characteristics.
word Phrase
order and
co-occurrence,
are
crucialmodel
in in
understanding
of a document.
based
models
has which
proven
to very
be promising
capturing
thethe
underlying
efficient
pattern
matching
for exact,
postfix,
matching
of phrases
in underlying
near
linear
of
a document.
Phrase
based
models
has proven
toand
be infix
promising
in capturing
the
document
characteristics.
This
paperprefix,
proposes
a new
document
model
and
algorithm
to perform
time.
The
pattern
matching
machine
(PMM)
uses
the
concepts
of
graph
theory
and
the
theory
document
characteristics.
paperprefix,
proposes
a newand
document
model and
algorithm
to perform
efficient pattern
matching This
for exact,
postfix,
infix matching
of phrases
in near
linear
of
automata
to efficiently
and
intelligently
match,
index,
track,
analyze
phrase
patterns.
efficient
pattern
matching
for
exact,
prefix,
postfix,
and
infix
matching
of theory
phrases
in near
linear
time.
The
pattern
matching
machine
(PMM)
uses the
concepts
ofand
graph
and
the
theory
The
scalability,
space,
andand
time
performance
arethe
compared
with
the
benchmark
document
time.
The pattern
matching
machine
(PMM)
uses
concepts
ofand
graph
theoryphrase
and the
theory
of automata
to efficiently
intelligently
match,
index,
track,
analyze
patterns.
models.
of
automata
to efficiently
intelligently
match,
track,
andthe
analyze
phrasedocument
patterns.
The
scalability,
space, andand
time
performance
are index,
compared
with
benchmark
The
scalability,
space,
and
time
performance
are
compared
with
the
benchmark
document
models.
©
2017 The Authors.
Published
by Elsevier
B.V. Matching Machine, Finite Aautomata, Graph Theory
Keywords:
Document
Data Models,
Pattern
models. under responsibility of the scientific committee of the International Conference on Computational Science
Peer-review
Keywords: Document Data Models, Pattern Matching Machine, Finite Aautomata, Graph Theory
Keywords: Document Data Models, Pattern Matching Machine, Finite Aautomata, Graph Theory

1 Introduction, Background and Motivation
1 Introduction, Background and Motivation
In
mining, one of the major
challenges is toand
discover
understandable topics of discussion,
1 textIntroduction,
Background
Motivation

and
at the
same one
timeofstatistically
underlying
document
grouping. Topic
discovery
is a task
In text
mining,
the major valid
challenges
is to discover
understandable
topics
of discussion,
of
discovering
and
tracking
events
or
interesting
patterns
in
a
text
stream.
It
is
an
event
In
mining,
the major valid
challenges
is to discover
understandable
topics
of discussion,
andtext
at the
same one
timeofstatistically
underlying
document
grouping. Topic
discovery
is abased
task
information
management
and
organization
[1].
In
the
literature,
as
such
there
is
no
standalone
and
at the same
time
statistically
underlyingpatterns
document
TopicItdiscovery
is abased
task
of discovering
and
tracking
eventsvalid
or interesting
in agrouping.
text stream.
is an event
algorithm
tomanagement
discover
topics.
Aorganization
combination
of In
various
text
modules,
working
together
of
discovering
and tracking
events
or interesting
patterns
in amining
text
It is
based
information
and
[1].
the literature,
asstream.
such
there
isan
noevent
standalone
makes
a framework
specifically
suited
to extract
the
information
and Aorganization
[1].
thetopics.
literature,
as such
thereworking
is no standalone
algorithm
tomanagement
discover
topics.
combination
of In
various
text mining
modules,
together
In aatraditional
vector
space
model
(VSM),
term
document
could
be verytogether
sparse.
algorithm
to discover
topics.
A suited
combination
ofthe
various
text
miningmatrix
modules,
working
makes
framework
specifically
to
extract
the
topics.
The
problem
of
clustering
columns
is
that
of
clustering
document,
whereas
that
of
clustering
makes
a
framework
specifically
suited
to
extract
the
topics.
In a traditional vector space model (VSM), the term document matrix could be very sparse.
rows
isa traditional
that of
words
[2].is Clustering
and
documents
iscould
a that
dual
problem
[3].
vector space
model
(VSM),
thewords
term document
matrix
beof
very
sparse.
TheInproblem
of clustering
clustering
columns
that
of clustering
document,
whereas
clustering
This
dual
property
is
very
important
in
topic
discovery
as
the
words/features
describing
the
The
problem
of
clustering
columns
is
that
of
clustering
document,
whereas
that
of
clustering
rows is that of clustering words [2]. Clustering words and documents is a dual problem [3].
same
are
together
which
determines
feature
cluster
rows
istopic
that
of often
clustering
[2].[4].
Clustering
words and
documents
is athe
dual
problem
[3].
This dual
property
is used
verywords
important
in The
topicframework
discovery
as the
words/features
describing
the
and
document
cluster
simultaneously
is
referred
to
as
topic
modeling
[5].
This
dual
property
is
very
important
in
topic
discovery
as
the
words/features
describing
the
same topic are often used together [4]. The framework which determines the feature cluster
same
topic are cluster
often used
together [4].
The framework
which
determines
and document
simultaneously
is referred
to as topic
modeling
[5]. the feature cluster
1
and document cluster simultaneously is referred to as topic modeling [5].
1
1877-0509 © 2017 The Authors. Published by Elsevier B.V.
1
Peer-review under responsibility of the scientific committee of the International Conference on Computational Science
10.1016/j.procs.2017.05.244

	

EfficientPMM: Efficient Pattern
Matching
...
Ramanpreet
SinghMachine
et al. / Procedia
Computer Science 108C (2017) 1060–1070

1.1

Singh et al.

The Pilot Study and beyond

In 1997, a pilot study on topic detection and tracking (TDT) [1], laid down the foundation
for research in topic discovery domain. The focus was to gather researchers to set a dedicated
track for research in news-specific and event based organization of information. From pilot
study (1998) till now, the task of topic discovery evolved as “new event detection and tracking”
using clustering techniques as backbone. Various document models have been proposed in the
literature to capture the underlying document characteristics [2, 6, 7, 3, 8].
There are many topic models proposed, such as LDA and PLDA [9] in the past which more
or less perform grouping of most likely keywords which go together in any topic bucket. For
more details, refer [9] which provides a comprehensive bibliography of various topic models. In
this work the focus is not on any specific topic model, rather on the underlying document data
model that captures the salient features of documents in an efficient way; which then can be
utilized to perform various text mining tasks.
Vector space and bag of word models are poor candidates for topic discovery as they loose
word order and co-occurrence, which are very crucial in understanding the meaning of a document. Phrase based models [6, 7] efficiently mapped documents into phrase feature space and
retained the co-occurrence of words. Co-Clustering features and documents turned out to be
a promising approach for topic discovery. Instead of simply using a vector space model and a
simple bag of word approach, the focus moved towards the use of other features, more than just
words, such as phrases, frequent term sets, enriched words and parts of speech. These types
of features really make sense in topic discovery as one of the main tasks is to come up with
understandable topic labels, not just pure statistical grouping.
The notion of having one label for a cluster also moved towards having a connected list of
topics to describe a given cluster. However, the evaluation of topics is still somewhat subjective
in nature because most of the labeled dataset expect the results to be compared for the partition
of documents space; whereas, the focus of topic discovery is to discover meaningful descriptions
by compromising on the best partition.
There are mainly two benchmark document models, Suffix Tree Clustering(STC) [6] and
Phrase based Document Index Graph(DIG) [7], which satisfies most of the characteristics of
the state of the art. STC and DIG are near-linear time phrase based models with underlying
graph structure as the backbone to quickly index, retrieve and use it, for similarity analysis to
discover and group the documents and label them at the same time.
(STC) [6] uses data structure tri as an underlying structure. The tree grows as the suffixes
are added. The growth mechanism of the tree is not optimal. There are lot of redundancies
in the creation of edges. Also the phrase matching mechanism is very naive as it does not
efficiently utilize the information of already matched suffixes.
(DIG) [7] uses graph as the underlying structure. The key phrases of document are added
in the graph with document and sentence information. Every node is a word in a sentence. In
terms of redundancies, DIG is promising as it utilizes the same node every time an existing
word needs to be added to the graph. Thus, the growth mechanism of the graph is better
than STC. The pattern matching is also reduced to finding shared paths in the graph; however,
keeping information of all the shared paths and edge table in every node is work intensive.
Another model proposed by Li and Zaiane et al. [4] uses a graphical underlying structure to
build the communities of keywords, map the documents on top of these communities, and then
use the concept of community mining in social network analysis to discover the coherent hierarchical clusters and automatic taxonomies. One of the main focus was to discover descriptive
topic communities, which could be understandable by the humans.
2

1061

1062	

EfficientPMM: Efficient Pattern
Matching
...
Ramanpreet
SinghMachine
et al. / Procedia
Computer Science 108C (2017) 1060–1070

1.2

Singh et al.

Motivation

In all the models, there are two major parts, 1) Graph growing mechanism and 2) Pattern
matching approach, along with information aggregation mechanism. An optimal graph growth
and near-to-linear time pattern matching mechanism is desired. There is possibility to a grow
graph intelligently by using transition tables and failure states [10]. For pattern matching,
there exists ways to intelligently utilize the information already gathered for a given match and
utilize it later to avoid repeat matching [11, 12]. An optimal data model should accurately and
completely capture the salient features of the data [7].
Motivated by the need of an efficient phrase based document model, In next section, we
propose a new underlying document data model which allows both the growth of a graph and
pattern matching in an intelligent way by utilizing the concepts from graph theory and the
theory of automata.

2

Proposed Data model

The proposed model is not an extension of STC or DIG, rather it tackles the problem of building
the data model from a different perspective using the notion of finite state machine.
A finite automata M is a 5-tuple (Q, q0, A, Σ, δ) where Q is a set of finite states, q0 ∈
Q is the start state, A is a set of accepting states, Σ is a finite alphabet and δ is a transition
function of M, that is, δ : Q x Σ →Q.
Automata starts from the start state q0 , then it reads the input symbol ‘a’, if the present
state is q, then it makes a transition from state q to the next state defined by a transition
function δ(q, a). If the next state belongs to an accepting state A, we say machine M has
accepted the pattern read so far, otherwise it is rejected [13]. In some cases, if the next state
is reject state, a transition function directs the next state to some previously accepted state
[11][14], where the pattern could continue to match the other possibilities without starting from
the beginning - saving the unnecessary building of states. Machine induces a final state function
φ from Σ* to Q such that φ(w) is the state where M ends up in after scanning the string w.
Thus, w is accepted if and only if φ(w) belongs to A. This function is also called the output
function [11] frequently in the literature.
The definition of automata is open to the application it is being designed for. An automata
is a finite input if it accepts the finite sequence of symbols. A finite state if it contains a
finite number of states. Deterministic if for a given current state and input symbol there is
only one and only possible next state.
In this work, we design a Deterministic Finite State Machine, called Pattern Matching
Machine (PMM) on top of an underlying graph data model to efficiently match the phrase
patterns in the documents and group documents around those patterns, while at the same
time, utilizing the underlying node and edge structure.

2.1

Structure Overview

PMM is a directed finite state machine, with nodes and edges. The atomic unit of the data
model contains the following parts:
• Start State: Root node containing the paired list < Σ →q> of accepting symbols with
corresponding next state transitions. Start State is given number zero.
• State Node: In this model a state node not only contains the state number, but also contains
the statistics about the phrases and documents. This information helps the state machine
3

	

EfficientPMM: Efficient Pattern
Matching
Machine
...
Ramanpreet
Singh
et al. / Procedia
Computer Science 108C (2017) 1060–1070

Singh et al.

grow further and at the same time as matching the patterns. The following information is
stored in a state node:
1. State Number: A numeric finite state number is given to each state.
2. Depth: Measure of the degree of separation from the start node to the current node.
3. Underlying pattern: Concatenation of all the words along the edges starting from the
start node to the current node.
4. List of document-weight pair, captures the information about the document in which
the underlying phrase occurred along with its local weight in the document.
5. Edge List: Contains the list of edges(or words) outgoing from a given node.
6. Failure Function: When a match fails, defines the transition to the next state.
7. Output Function: Boolean to flag whether the underlying phrase is completed or not.
• Edge: Carries the symbol Σ information. In this work, a symbol is a word of a phrase.
Every edge also contains the information of the next transition state; that is, δ : Q x Σ →Q.
Two Nodes are connected if the underlying word of the source node appears successive to the
underlying word of the destination node. As the phrase is read, several states are connected
through the edges. The path from the start node to end node is the full phrase. Therefore,
a phrase or sentence structure is being maintained in this model.

2.2

PMM Construction

The document model is built incrementally as documents are processed. Building a state
machine graph is a one pre-step and two main step process.
In Step 0 the document is mapped to a feature space. In text, features could be single
keyword, a combination of keywords making up a phrase, n-grams, entities, part of speech tags
and other linguistic and lexical features [15, 16]. A phrase is an ordered sequence of keywords
making up a new meaning different from the constituting keywords. Use of phrases and other
lexical features in text mining have shown improvement in precision [17]. Phrases are also less
sensitive to noise [18]. In this work, we use RAKE algorithm [19], which is an unsupervised,
domain and language independent algorithm to extract weighted list of meaningful and logical
phrases from a single document. We also extract named entity phrases of interest based on
various applications. Each phrase feature of the document retains the relative weight, document
ID, and index information.
In Step 1 each phrase is broken down into consisting keywords, retaining the order sequence. Now commencing from a start state q0 , keywords are entered. If the keyword’s edge
already exists, the statistics of the underlying node is updated with new document information,
otherwise a new edge and node is created. The growth rate of the states becomes stable when
the dictionary becomes stable. When the whole phrase is read, the output function in the corresponding node is flagged TRUE. The total number of keywords in the phrase is the same as
the length of the path from start node to the end node. Hence, a phrase is converted to a path
in a graph. The overall run time complexity of this process is O(np), which is a linear term
of n as |p|  |n|. Algorithm 1 describes the process in more details. Algorithm 1 runs for K
time and for each keyword it checks if the edge exists. The list of edges are stored in hashmap,
which has an average lookup time of Θ(1) [13]. Hence, the overall complexity of Algorithm 1
is of the order of K, O(K), where K is the number of keywords in a phrase.
In Step 2 computing the failure function adds the intelligence to the given graph. It actually
helps to avoid unnecessary transitions and if for any state the transition fails to proceed further,
4

1063

1064	

EfficientPMM: Efficient Pattern
Matching
Machine
...
Ramanpreet
Singh
et al. / Procedia
Computer Science 108C (2017) 1060–1070

Singh et al.

Algorithm 1 Entering phrase in the graph, Enter(P hrasej , weight, index, Doci ID)
1:
2:
3:
4:
5:
6:
7:
8:
9:
10:
11:
12:
13:
14:
15:
16:
17:
18:
19:
20:
21:
22:
23:

currentState ← q0 : start state
for all i ← 1 to K: (Keywords in Phrase) do
if Keyword ∃ CurrentState.EdgeList then
nextState ← edge.transitionState
nextState.update<Document-Weight> List
if i = k then
nextState.outputFunction = TRUE; currentState ← startState
else
currentState ← nextState
end if
else
nextState= new StateNode(PhraseType, CurrnetNode.Depth);
currentNode.addEdgeList(Keyword,nextState);
nextState.setPhraseType= phraseType; nextState.setDepth= currentNode.depth+1
nextState.setPhrase=Concatenate(“Current-Node.Phrase”,“Keyword”)
nextState.add<Document-Weight> List
if i = k then
nextState.outputFunction = TRUE; currentState ← startState
else
currentState ← nextState
end if
end if
end for

failure function guides the machine which state to go to next. This property of the data model
helps in efficiently performing the pattern matching. If a pattern has been matched halfway, and the next keyword does not match the one in next edge list, the transition function
is guided by the failure function to find which other branch has already matched the so far
matched pattern. This avoids the process of going back to the start state and start matching
again.
Computing failure function is a process of incrementally computing the failure state at every
depth using the depth of the previous state. For nodes at depth one, failure function is directed
to start state. Algorithm 2 explains how to compute failure function of an incomplete graph
created in step one. One important step during computing failure function is to transfer the
node information from source to destination state. This step is very crucial in performing the
effective pattern matching between the various possibilities. This will be more clear with an
example in the next section. The run time of Algorithm 2 is bounded by the sum of the length
of the phrases.

2.3

An Example:

This section will take an example to explain the importance of the failure function, how the
proposed model intelligently, in linear time, matches the patterns, and how the overall model
behaves with real data.
Let us take 4 sample phrases; D1 : boston bombing (weight1 , index1 ), D2 : casualties
boston bombing (weight2 , index2 ), D3 : boston attack casualties (weight3 , index3 ),
5

	

EfficientPMM: Efficient Pattern
Matching
Machine
...
Ramanpreet
Singh
et al. / Procedia
Computer Science 108C (2017) 1060–1070 Singh et al.

Algorithm 2 PMM: Calculation of Failure Function, INPUT(startnode)
1:
2:
3:
4:
5:
6:
7:
8:
9:
10:
11:
12:
13:
14:
15:
16:
17:
18:
19:

queue ← empty; Edges< String >← startnode.getAllEdges
for all i ← 1 to e:Length of Edge<> do
currentState ← ei .getNextSate; currentState.failureState ← startstate
queue ← queue ∪ currentState
end for
while queue = empty do
State ns ← queue.nextState; queue ← queue − ns
if ns.edgeList = empty then
for j ← 1 to ns.Size do
State tempState ← jth edge in ns
queue ← queue ∪ tempState; state ← tempState.getFailureState
while jth edge ∈
/ state.egdeList do
state ← state.getFailureState
end while
temp.setFailureState←state.getTransition-for(jth edge)
state.AddDocumentList←temp.getDocumentList
end for
end if
end while

Figure 1: State Machine graph after adding
all 4 phrases

Figure 2: State Machine graph: after adding
failure Intelligence

D4 : boston bombing several casualties happened (weight4 , index4 ). A graph is built
incrementally for following 4 phrases using Algorithm 1. Figure 1 show the graph with document
information within.
In Figure 1, even though all the phrases have been entered in the graph, it is not yet
complete. From the above graph, the phrase “boston bombing” is shared between D1 and D4
at node 2. But “boston bombing” also occurs in D2 . Documents D2 , D3 and D4 should all
be grouped under the phrase “casualties”, but they are all sitting alone in nodes 3, 7 and 9.
The incomplete graph model cannot match the pre, post and infixes, leading to the incomplete
grouping of the documents. Therefore, a mechanism is needed to match phrases in all possible
ways in order to capture the implicit grouping in the data.
Suffix tree model (STC) [6] can solve this problem by splitting all of the phrases into all
of the possible suffixes and create separate tree branches to match all the possible phrasesgenerating many redundancies in the process.
6

1065

1066	

EfficientPMM: Efficient Pattern
Matching
Machine
...
Ramanpreet
Singh
et al. / Procedia
Computer Science 108C (2017) 1060–1070

Singh et al.

DIG [7] would solve this problem by having one node for a word, it would match all post, pre
and infix matches. This way requires intensive indexing of nodes. Another bottleneck to this
approach is that there is an overwhelming amount of information stored in one node. Therefore,
a trade off between redundancy and intensive lookup is required.
We proposed this model where the construction process is finite and deterministic, making
it fast and reliable. It do not also need to maintain long lookup list for edge and sentence
information. At the same time, all possible phrase matching could be performed using the
concept of failure transition. Using Algorithm 2 failure transitions are added to the graph as
can be seen in Figure 2. The node information of nodes 7 and 9 are copied in node 3, making it
a node which completely captures the information stored in the data. The phrase “casualties”
now groups the documents D2 , D3 and D4 together, which was not the case before. The phrase
“casualties” occurs in the beginning(prefix) of D2 , in the end(postfix) of D3 and in middle(infix)
of D4 .
Hence, with the concept of failure transition all the possible combinations of matching
phrases could be performed. The model is now capable enough to intelligently match the
phrases. The proposed model proves to be a promising, yet powerful data model that captures
the salient features of data in a linear and simple way.

3

Experiments and Results

In this work we do not explicitly propose a new topic model or clustering algorithm. However,
we propose an efficient and scalable document model which could serve as a backbone of various
text mining tasks. To demonstrate its utility we perform various experiments 1 as explained in
this section. To show the utility of the proposed data model in various tasks, following data
sets have been used in this work:
• UofAData: This data set is used by Li et al. in [4] to automatically generate taxonomies.
The collection contains 666 web documents2 collected for various queries.
• RCV1-SubSet: This data set is a subset of manually categorized news wire stories by
Reuters Ltd. The subset contains 11839 documents and 118 categories [20].

3.1

Document Model Construction Experiments

The algorithms to construct the state machine data graph are explained in Section 2.2. Theoretically, it should be built in linear time as it reads the document and stores the information
just once.
An experiment is being conducted in which time to build the graph is recorded vs. the
number of documents read. The statistics shown in Figure 3 are generated using RCV1-SubSet
dataset. The average length of the document is 832 characters with maximum value as 13395
characters and median as 540 characters. It can be seen that, for a lesser number of documents,
the time taken to build the graph is very short, almost in the order of seconds. For 1000
documents, it just took 13 seconds. But as the size of the document set grows, the edge set size
increases. The algorithm tends to follow its worst case run time, which is close to linear in this
case. One can see after 2000 documents, the curve is near to linear. The proposed data model
is capable of parsing 10000 documents in just 120 seconds.
1 All

experiments were performed on MacBook Pro OS X 10.8.2, Intel Core i7, 2.8 GHz, 4 core, 16 GB RAM.
document set was requested directly from the authors.

2 The

7

	

EfficientPMM: Efficient Pattern
Matching
Machine
...
Ramanpreet
Singh
et al. / Procedia
Computer Science 108C (2017) 1060–1070 Singh et al.

Figure 3: Graph Building Time: No. of documents vs. Time(s)

Figure 4: Graph Growth Rate: No. of documents vs. Number of nodes
Apart from construction time of the proposed data model, it is also important to analyze
the growth rate of the graph. In order to be scalable, it is desired to have controlled growth
rate, not exponential. Figure 4 represents the growth rate of the graph with the number of
documents on the x-axis vs. the number of state nodes on the y-axis. The growth of the
graph is controlled as the size of the document set increases because the vocabulary used in the
documents tends to stabilize; therefore, fewer new nodes are created.
3.1.1

Comparison with DIG

The benchmark DIG document model is capable of processing 2000 average sized news articles
in as little as 44 seconds and moderate-sized web documents in a little over 5 minutes [7]. For
fair evaluation, we used the same data set (RCV1-2340)[20, 7], which contains 2340 news articles
extracted from Reuters, to evaluate the statistics of the proposed data model. The proposed
model could parse 2000 average-sized news articles in as little as 25 seconds. Moreover, to parse
10000 articles, from a bigger subset of Reuters data set, it only took 120 seconds.
For the number of nodes generated, DIG performs better than the proposed model, but
the difference is not significant. If DIG generated 29075 nodes for 1500 articles, our model
generated 31141 states for the same 1500 articles. For the trade off between number of nodes
8

1067

1068	

EfficientPMM: Efficient Pattern
Matching
Machine
...
Ramanpreet
Singh
et al. / Procedia
Computer Science 108C (2017) 1060–1070

Singh et al.

generated, in terms of time taken, the proposed model performs significantly better than the
DIG model. This behavior was theoretically expected because of the deterministic behavior of
the proposed model. Whereas in DIG, as the document set grows, the edge and sentence table
information overwhelms in a given node. Hence, lookup time is more than the deterministic
automata, where for a given input, there is only one output state in one move.
In another test 10000 variable length n-gram phrases taken from RCV1-2340 documents
were parsed through proposed data model and DIG. The test was performed five times with
random order of phrases to enter. Following measures were taken to determine the significance
of improvement: mean time, median time, max time, min time, node count, edge count, memory
usage, and phrase entry time. The results are show in Figure 5. In terms of average time to
read and match all phrases, memory usage, and phrase entry time the proposed data model
out performs DIG with a promising margin. The bar graph at the bottom of Figure 5 shows
relative time taken for a phrase to be parsed through both data models.

Figure 5: DIG vs Proposed Model

3.2

Topic Mining Experiments

As the document model is built, the implicit pattern matching mechanism matches and groups
documents around the path of the PMM graph. A careful selection of the most significant paths
(nodes), in conjunction with other nodes, gives meaningful topic descriptions and stories. The
underlying grouping of documents could help in the statistical evaluation process. In this work
we focus on retaining the qualitative aspects of the topics. Hence, a trade off between best
statistical grouping and understandable topics needs to be realized.
3.2.1

Experiments on UofAData

The UofAData is used in [4] to automatically mine the taxonomies in the collection of web
articles. There are 5 major topic clusters: jaguar(181), tiger(121), avp(151), penguin(121),
9

	

EfficientPMM: Efficient Pattern
Matching
Machine
...
Ramanpreet
Singh
et al. / Procedia
Computer Science 108C (2017) 1060–1070

Algorithm
Entropy

PMM
0.109

Li et al.[4]
0.127

Singh et al.

K-Means
0.279

Table 1: UofAData: Comparisons with benchmark
Query
Jaguar

Tiger

AVP

Penguin
Michael Jordan

Topics/Senses discovered by our framework
Animal:(cat family, jaguar animal, big cats, natural habitat, jaguar
rescue center, endangered species, largest feline). Car: ( jaguar xk,
sports car, jaguar dealer, swallow sidecar company, ss jaguar). MacOS:(mac os, operating system ). Guitar:(bridge pickup, neck pickup,
vintage style floating tremolo)
Golf:( Tiger Woods, pga tour, San Francisco). Algorithm:(tiger hash
algorithm, hash function). Animal:( tiger, habitat, cat family, largest
cat)
Volleyball:( avp tour, beach volleyball). Antivirus:( anti virus, software). Product:(avon products, market cap, long term ). Airport:(international airport, Scranton)
Algorithm:(google penguin, algorithm). Club:( club penguin, kids).
Hockey:( hockey league, Pittsburgh, Stanley cup)
Player:(greatest basketball player, Chicago bulls, NBA, game winning
shot). Researcher:(California Berkeley, machine learning, computer
science)
Table 2: Query Senses Discovered in UofAData

michael jordan(91). Table 2 shows the topics and senses discovered by the proposed approach,
without any human intervention. The time taken to read all 666 web pages of average length
6185 characters, identify key phrases, and discover meaningful topics was approximately 50
seconds. For a real time application, this framework has potential to be incorporated in search
engine to show a cloud of various senses discovered for a given query by the user.
The entropy of discovered groupings, underlying each topic, are also compared with the
original groupings in Table 1. In the original paper they used human experts for evaluation
[4]. As compared to the approach adopted by Li et al. [4], the topics and senses generated by
our approach are well separated from each other, hereby, giving a clear understanding of the
various perspectives discussed.

4

Conclusions and Future Research

We present a new document model and linear pattern matching algorithm to efficiently store
and match the phrases for all possible matches. Various experiments are performed to compare
with the benchmark document models. The merits of our approach is that it is linear in time
and it scans the documents just once and match them with the past knowledge to capture the
salient features of the text corpus all in one memory efficient pattern matching machine.
There are number of possible future directions to extend this work. One direction is to
extending the approach to multi-tenant distributed environment on multiple nodes to improve
the speed and parse time. This will create an enriched meta-data index for the corpus. The
information stored in the document models could be utilized to perform various text mining
10

1069

1070	

EfficientPMM: Efficient Pattern
Matching
Machine
...
Ramanpreet
Singh
et al. / Procedia
Computer Science 108C (2017) 1060–1070

Singh et al.

task such as to discover meaningful topic groupings from a given set of documents. The proposed document model also has potential to be utilized to perform many other tasks such as
query suggestions, vocabulary tracker, phrase based structural indexing, summarization, and
important keyword extraction to name a few. It will be an interesting future work to investigate
for these extensions.

References
[1] James Allan, Jaime Carbonell, George Doddington, Jonathan Yamron, Yiming Yang, Umass
Amherst, and James Allan Umass. Topic detection and tracking pilot study, 1998.
[2] CharuC. Aggarwal and ChengXiang Zhai. A survey of text clustering algorithms. In Mining Text
Data, pages 77–128. Springer US, 2012.
[3] I S. Dhillon. Co-clustering documents and words using bipartite spectral graph partitioning. In
ACM SIGKDD intl. conference on Knowledge discovery and data mining ‘01, pages 269–274.
[4] X. Li, J. Chen, and O. Zaiane. Text document topical recursive clustering and automatic labeling of
a hierarchy of document clusters. In Advances in Knowledge Discovery and Data Mining, Lecture
Notes in CS. 2013.
[5] Th. Hofmann. Probabilistic latent semantic indexing. In 22nd annual intl. ACM SIGIR conf. on
Research and development in IR ‘99, pages 50–57, New York, NY, USA.
[6] O. Zamir and O. Etzioni. Web document clustering: a feasibility demonstration. In 21st annual
international ACM SIGIR conf. on Research and development in IR ‘98, pages 46–54.
[7] K.M. Hammouda and M.S. Kamel. Efficient phrase-based document indexing for web document
clustering. Knowledge and Data Engineering, IEEE Transactions on, 16(10):1279–1296, 2004.
[8] F Beil, M Ester, and X Xu. Frequent term-based text clustering. In Proceedings of the eighth ACM
SIGKDD international conference on Knowledge discovery data mining, pages 436–442, 2002.
[9] Topic Modeling Bibliography. http://mimno.infosci.cornell.edu/topics.html.
[10] Wojciech Skut. Finite-state machines for mining patterns in very large text repositories. In ‘09
conference on Finite-State Methods and Natural Language Processing, pages 23–23, Amsterdam.
[11] Alfred V. Aho and Margaret J. Corasick. Efficient string matching: an aid to bibliographic search.
Commun. ACM, 18(6):333–340, June 1975.
[12] Donald E. Knuth, James H. Morris Jr., and Vaughan R. Pratt. Fast pattern matching in strings.
SIAM J. Comput., 6(2):323–350, 1977.
[13] Thomas H. Cormen, Clifford Stein, Ronald L. Rivest, and Charles E. Leiserson. Introduction to
Algorithms. McGraw-Hill Higher Education, 2nd edition, 2001.
[14] A. Bremler-Barr, D. Hay, and Y. Koral. Compactdfa: Generic state machine compression for
scalable pattern matching. In INFOCOM, 2010 Proceedings IEEE, pages 1–9, March 2010.
[15] Girish K. Keyword extraction from a single document using centrality measures. In 2nd intl. conf
on Pattern recognition and machine intelligence, PReMI‘07. Springer-Verlag.
[16] A Hulth. Improved automatic keyword extraction given more linguistic knowledge. In 2003
conference on Empirical methods in natural language processing, pages 216–223, 2003.
[17] Chengxiang Z., Xiang T., Natasa M., and David A. Evaluation of syntactic phrase indexing clarit nlp track report. In The Fifth Text Retrieval Conference (TREC-5), pages 347–358, 1997.
[18] K. Hammouda, D. Matute, and M. Kamel. Corephrase:keyphrase extraction for document clustering. In 4th intl. conf. on Machine Learning Data Mining in Pattern Recognition ‘05.
[19] Stuart Rose, Dave Engel, Nick Cramer, and Wendy Cowley. Automatic Keyword Extraction from
Individual Documents, pages 1–20. John Wiley & Sons, Ltd, 2010.
[20] David D. Lewis, Yiming Yang, Tony G. Rose, and Fan Li. Rcv1: A new benchmark collection for
text categorization research. J. Mach. Learn. Res., 5:361–397, December 2004.

11

