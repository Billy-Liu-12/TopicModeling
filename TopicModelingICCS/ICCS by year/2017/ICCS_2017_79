Available online at www.sciencedirect.com

ScienceDirect
This space
reserved
for the
header, do not use it
ProcediaisComputer
Science
108CProcedia
(2017) 786–795
This space is reserved for the Procedia header, do not use it
This space is reserved for the Procedia header, do not use it

International Conference on Computational Science, ICCS 2017, 12-14 June 2017,
Zurich, Switzerland

Data Management and Volcano Plume Simulation with
Data
Management
and Volcano
PlumeHalo
Simulation
with
Parallel
SPH
Method
and
Dynamic
Domains
Data
Management
and Volcano
PlumeHalo
Simulation
with
Parallel
SPH Method
and Dynamic
Domains
1
1
2
, Abaniand
PatraDynamic
, and Matthew
Jones
ParallelZhixuan
SPH Cao
Method
Halo
Domains
1
1
2
1

2
2
2

Zhixuan Cao , Abani Patra , and Matthew Jones

Department of MAE,
University at Buffalo, Buffalo, New York, U.S.A.
Zhixuan Cao1 , Abani Patra1 , and Matthew Jones2
1 for Computational Research, University at Buffalo, Buffalo, New York, U.S.A.
Center
Department of MAE, University at Buffalo, Buffalo, New York, U.S.A.
1 for Computational Research, University at Buffalo, Buffalo, New York, U.S.A.
Center
Department of MAE, University at Buffalo, Buffalo, New York, U.S.A.
Center for Computational Research, University at Buffalo, Buffalo, New York, U.S.A.

Abstract
This
paper presents data management and strategies for implementing smoothed particle hyAbstract
drodynamics
(SPH) data
method
to simulateand
volcano
plumes.
These simulations
require
a careful
This paper presents
management
strategies
for implementing
smoothed
particle
hyAbstract
definition
of
the
domain
of
interest
and
multi-phase
material
involved
in
the
flow,
of
drodynamics
(SPH) data
method
to simulateand
volcano
plumes.
These simulations
require
aboth
careful
This
paper presents
management
strategies
for implementing
smoothed
particle
hywhich
change
over
time
and
involve
transport
over
vast
distances
in
a
short
time.
Computadefinition of the
domain
of interest
and volcano
multi-phase
material
in the
flow,aboth
of
drodynamics
(SPH)
method
to simulate
plumes.
Theseinvolved
simulations
require
careful
tional
are
developed
to overcome
theseover
challenges
by building
for
efficient
which strategies
change
over
time
and
involve
transport
vast
distances
in amechanisms
short
time.
Computadefinition
of the
domain
of interest
and multi-phase
material
involved
in the
flow,
both of
creation
and deletion
of particles
for simulation,
parallel
(using
the time.
message
passing
tional strategies
aretime
developed
to overcome
theseover
challenges
by building
for
efficient
which
change
over
and involve
transport
vast processing
distances
in
amechanisms
short
Computainterface
(MPI))
and
a
dynamically
defined
halo
domain
(a
domain
that
”optimally”
captures
creation
and deletion
of particles
for simulation,
parallel processing
(using
the message
passing
tional
strategies
are developed
to overcome
these challenges
by building
mechanisms
for efficient
all
the material
involved
in the flow).
A background
gridprocessing
is adopted
to reduce
neighbor
search
interface
(MPI))
and aofdynamically
haloparallel
domain
(a
domain(using
that
”optimally”
captures
creation
and
deletion
particles
for defined
simulation,
the message
passing
costs
and
to
decompose
the
domain.
A
Space
Filing
Curve
(SFC)
based
ordering
is
used
to
all the material
in the flow).defined
A background
grid (a
is adopted
to reduce
neighbor
search
interface
(MPI))involved
and a dynamically
halo domain
domain that
”optimally”
captures
assign
unique
identifiers
to
background
grid
entities
and
particles.
Time-dependent
SFC
based
costs
to decompose
domain.
A background
Space Filinggrid
Curve
(SFC) based
ordering
is used
to
all
theand
material
involved the
in the
flow). A
is adopted
to reduce
neighbor
search
indices
aretoassigned
to particles
to guarantee
uniqueness
of the
identifier.
Both particles
and
assignand
unique
identifiers
to
entities
and
particles.
Time-dependent
SFC
based
costs
decompose
thebackground
domain.
Agrid
Space
Filing
Curve
(SFC)
based ordering
is used
to
background
grids
aretomanaged
by
tables
whichand
canparticles.
ensure
quick
and Both
flexible
access.
An
indicesunique
are assigned
particles
tohash
guarantee
uniqueness
of the identifier.
particles
and
assign
identifiers
to background
grid
entities
Time-dependent
SFC based
SFC
based
(3D)
domain
decomposition
and
a dynamic
balancing
stratbackground
gridsdimensional
aretomanaged
by
tables
which can ensure
quick andload
flexible
access.
An
indices
are three
assigned
particles
tohash
guarantee
uniqueness
of
the
identifier.
Both
particles
and
egy
are
implemented
to
ensure
good
load
balance.
Several
strategies
are
developed
to
improve
SFC based three
domain
decomposition
and a dynamic
balancing
stratbackground
gridsdimensional
are managed(3D)
by hash
tables
which can ensure
quick andload
flexible
access.
An
performance:
dynamic
domains,
calibrated
particle
weight
optimized
load
check
egy are
implemented
tohalo
ensure
gooddomain
load
balance.
Several
strategies
are developed
to
improve
SFC
based
three
dimensional
(3D)
decomposition
and
aand
dynamic
load work
balancing
stratintervals.
Numerical
tests
show
that our
code
hasparticle
good
scalability
andoptimized
performance.
strateperformance:
dynamic
domains,
calibrated
weight
and
workThe
load
check
egy
are implemented
tohalo
ensure
good
load
balance.
Several
strategies
are
developed
to
improve
gies
described
in
this
paper
can
be
further
applied
to
many
other
implementations
of
mesh-free
intervals. Numerical
tests
show
that our
code hasparticle
good scalability
andoptimized
performance.
performance:
dynamic
halo
domains,
calibrated
weight and
workThe
loadstratecheck
methods,
especially
implementations
that
require
flexibility
inperformance.
adding andofThe
deleting
of
gies
described
in this those
papershow
can that
be further
applied
to many
other and
implementations
mesh-free
intervals.
Numerical
tests
our code
has good
scalability
strateparticles.
methods,
especially
those
implementations
that
require
flexibility
in
adding
and
deleting
of
gies described in this paper can be further applied to many other implementations of mesh-free
particles.
methods,
especially
those
implementations
that
require flexibility
adding volcano
and deleting
Keywords:
SPH, MPI,
data by
management,
decomposition,
domainin
adjusting,
plume of
©
2017 The Authors.
Published
Elsevier B.V.domain
Peer-review
under
responsibility
of
the
scientific
committee
of
the
International
Conference
on
Computational
Science
particles.
Keywords: SPH, MPI, data management, domain decomposition, domain adjusting, volcano plume
Keywords: SPH, MPI, data management, domain decomposition, domain adjusting, volcano plume

1 Introduction
1 Introduction
Smoothed particle hydrodynamics (SPH) is a mesh-free scheme, initially developed for astro1
Introduction
physical applications and later extended to computational fluid mechanics. SPH has several

Smoothed particle hydrodynamics (SPH) is a mesh-free scheme, initially developed for astrofeatures
matchhydrodynamics
well
simulation.
is easier
inforSPH
to
physical that
applications
andwith
laterdemands
extended
fluid
mechanics.
SPH has
several
Smoothed
particle
(SPH)into
isgeophysical
acomputational
mesh-freeflow
scheme,
initiallyItdeveloped
astroinclude
multiple
phases
and
handle
complicated
geometry
and
free
boundaries.
The
first
imfeatures that
match well
flow fluid
simulation.
It is easier
in SPH
to
physical
applications
andwith
laterdemands
extendedintogeophysical
computational
mechanics.
SPH has
several
plementation
of SPH
in volcanology
was in
presented
by Bursik
[1]free
withboundaries.
more
recently
include multiple
phases
and
complicated
geometry
The
features
that match
well
withhandle
demands
geophysical
flowand
simulation.
It isdeveloped
easier
in first
SPHimto
plementation
of SPH
in volcanology
was presented
by Bursik
[1]free
withboundaries.
more developed
recently
include
multiple
phases
and handle complicated
geometry
and
The first
implementation of SPH in volcanology was presented by Bursik [1] with more developed recently1
1
1877-0509 © 2017 The Authors. Published by Elsevier B.V.
Peer-review under responsibility of the scientific committee of the International Conference on Computational Science
1
10.1016/j.procs.2017.05.094

	

Data Management and VolcanoZhixuan
Plume Cao
Simulation
with SPH
Cao, Patra and Jones
et al. / Procedia
Computer Science 108C (2017) 786–795

(eg.[5]). The development of a volcano plume (see Fig. 5) is essentially a multiphase turbulent
ejection mixing process accompanied by microphysics phenomena like phase change of water,
aggregation, reaction, etc. Accurate prediction of such complicated phenomena in a given time
window requires resolution (very high particle counts) that can not be accomplished without
parallel computing. Imposing some types of boundary conditions (such as wind field, eruption
boundary condition at the vent) requires dynamically adding and removing particles during
simulation. This requires an efficient and flexible data management scheme.
Most implementations of the parallel SPH method presented to date are limited to standard
SPH and benchmark problems like dam breaks, or relatively simple scenarios like breakingwaves, flooding, etc. Work on more complicated implementations is relatively rare. Among
existing CPU parallel SPH schemes, most of them focus on neighbor searching algorithms and
dynamic load balancing. (eg. [4, 2]). Less attention has been paid to developing more flexible data management schemes. Fortunately, efficient and flexible data management strategies
for parallel computing have been successfully implemented in mesh based methods (eg. [6] for
adaptive hp finite element method (FEM), and [9] for finite volume method (FVM)). Motivated
by these techniques developed for mesh based methods, we present a complete framework for
parallelizing SPH program with the MPI standard allowing flexible and efficient data access.
Any implementation of SPH requires efficient searching and updating of neighbors during the
simulation. Of the many possible choices we adopt the popular background grid method [7],
which is also used for domain decomposition. We refer to the elements of the background grid
as buckets. As for the actual storage of data representing the physical quantities associated
to each particle, different strategies have been adopted in existing implementations of SPH.
In DualSPHysics[2], the physical quantities of each particle (position, velocity, density...) are
stored in arrays, and the particles (and the arrays with particle data) are reordered following the
order of the cells. It is not flexible enough to add, delete and especially access particles for such
fixed-size arrays. Ferrari[4] adopted linked lists using pointers so that particles can be deleted
or added during the simulation. Storage problems caused by fixed-size arrays are thereby also
eliminated. We adopt hash tables to store pointers to objects of particles and buckets, which
give us not only flexibility of deleting and adding of elements, but also quicker access compared
with linked lists. A SFC-based index is adopted to give each particle and background bucket
an unique identifier – a strategy known to preserve some locality at minimal cost. The SFC
based numbering strategy is further extended to include time step information so that particles added at the same position but different time will have different identifiers. We adopt an
easy-programming scheme based on SFC [10] to decompose the domain.
To the best of the author’s knowledge, no current implementation of SPH has the feature of
adjusting the computational domain based on simulation needs. For volcano plume simulation,
this feature will greatly reduce computational cost by avoiding computing of uninfluenced fluids. This feature is accomplished by adding a scan function to monitor the outermost layer of
the domain and turn ghost particles to real particles at the proper time thus effectively defining
a halo domain with an almost optimally defined size.

2

Data Structure and Load Balance

In SPH, the domain is discretized by particles and the position of each particle is updated at
every time step. The physical laws written in the form of partial differential equations need
to be transformed into the Lagrangian particle formalism of SPH. Using a kernel function
that provides the weighted estimation of the physical properties in the neighborhood of a
2

787

788	

Data Management and VolcanoZhixuan
Plume Cao
Simulation
with SPH
Cao, Patra and Jones
et al. / Procedia
Computer Science 108C (2017) 786–795

Start

Start

Set Up Initial Condition

Set Up Initial Condition

Simulation
Completed?
No

Yes

Simulation
Completed?
No

Determine Time Step

Pre-updating
Section

Apply Wall Boundary Condition

Searching Neighbors & Sychronize Data

Adapt Indicator
>0 ?
Yes

Yes

No

Switch Pressure Ghost Particles to
Real Potential Involved Particles
& Sychronize data
Add New Pressure Ghost Particles
& Sychronize Data

Update Velocity and Energy & Sychronize
Data

Updating
Section

Update Position & Add New Erupt Particles
& Sychronize Data

Add Wall Ghost Particles
& Sychronize Data
Pre-updating Section

Update Density & Sychronize Data
Updating Section
Need Check Load
Balance?
Yes

No

Check Load Balance & Redecompose
Domain

End

Repartition
Section

Scan the Most Outside Layer
Buckets &Return Adapt Indicator
Repartition Section

End

Figure 1: Workflow for SPH code. Figure to the left is the basic workflow. The right figure is
the workflow that enables dynamic halo domain.
discrete point (particle), the integral equations are evaluated as sums over neighbor particles.
Only particles located within the support of the kernel function will interact. Thus, physical
properties associated with the particle are updated based on its neighbors. A neighbor search
therefore needs to be carried out before updating physical properties. A basic workflow of our
SPH code is shown in Fig. 1. We use buckets, which contain all particles associated with
a sub-domain (see Fig. 2) and keep them stationary during the entire simulation, to reduce
searching cost (since the search can now be restricted to only neighboring domains). Domain
is decomposed based on a SFC going through centroids of all buckets, as shown in Fig. 2.

2.1

SFC based indexing

Our data structure starts from assigning each particle and bucket an identifier, we refer to it as
a key, which should be unique throughout the simulation. The key for a bucket is determined
by the centroid coordinate of the bucket while the key for a particle is determined by adding
coordinates and time step of the particle. The map from coordinates to key is based on SFC [11]
which maps a n-dimensional domain to a one dimensional sequence. The standard procedure
for obtaining the SFC is:
• Scale all coordinates in computational domain (X ) into an n dimensional unit hypercube
[0, 1]n : X → X, where X represents coordinates in the hypercube.
• Compute kr = hn (X). Where hn is the map hn : [0, 1]n → [0, 1].
• Convert kr to integer k by multiplying kr with a large integer and removing decimal part.
• Sort all keys to form a SFC sequence. The SFC represents a curve passing through all
particles (or centroids of buckets).
3

	

Data Management and Volcano Zhixuan
Plume Simulation
with SPH
Cao, Patra and Jones
Cao et al. / Procedia
Computer Science 108C (2017) 786–795
Process 0

Process 1

Partition Point
on SFC

Figure 2: The left figure shows SFCs passing all particles and buckets. The right figure shows
an example of a domain decomposition based on the SFC of buckets.
Details for constructing the map hn can be found in [8]. These keys denote a simple addressing
and ordering scheme for the data, i.e., a global index space for all the objects.
In some situations, new particles need to be added during the simulation. For example, new
particles need to be added at the bottom of the eject vent (see Fig. 3). To assign distinct
identifiers for particles added at the ”same position” but at different time steps, we extend the
SFC-based key to a time-dependent SFC based key by including date of birth of particles into
their keys. The time-dependent SFC based key can be denoted as: [k, t], where t is the time
step. The map hn will become:
hn : [0, 1]n × T → [0, 1] × N

(1)

Where T ⊂ [0, ∞) is the time dimension, and N = {0, 1, 2, 3...}. To guarantee locality, sorting
of particle keys is based on k, that is to say, particles with smaller k always come before particles
with larger k. For particles that have the same k, ordering will then depend on t. Figure 2
shows SFC ordering of buckets and particles in buckets. Several features of this indexing scheme
are suitable for SPH:
• Guaranteed uniqueness of keys.
• The key of each object is generated purely based on its own coordinates. When we add
new objects on different processes, the key of each object can be generated fast and
independently.
• Objects that are located closely in the Euclidian space will also be close to each other in
the one dimensional SFC key space in a mean sense. Since SPH particles only interact
with their neighbors, geometric locality can be exploited for efficient access of data.
• This type of key effectively generates a global address space. Globalism of key and conservation of spatial locality make it easy to partition the sorted key sequence and obtain
a decomposition of the problem.

2.2

Data management strategies

The most basic data structure of SPH are particles and buckets. Both are defined as C++
classes. Information that is contained in particle objects can be categorized into six categories:
identifier (the key), affiliate(rank of the process that the particle belongs to), primitive variables (variables in governing equations, e.g. density, velocity), secondary variables (physical
properties that can be computed from primitive variables, they are stored to avoid repeated
4

789

790	

Data Management and Volcano Zhixuan
Plume Simulation
with SPH
Cao, Patra and Jones
Cao et al. / Procedia
Computer Science 108C (2017) 786–795

Ptr*

Time

NULL
Ptr*

external link list

Ptr*

Time
step
2
Time
step
2

...
...

Time step 3
Ptr*

Ptr*

Ptr*

...
Time step 1

...

0
y

Ptr*
NULL

Ptr* Pointer to Objects of particles
NULL Empty bucket in hash table

particles added at setting up step
x

Time step 0

particles added during simulation

Figure 4: The influence of different load balFigure 3: Non-uniform distribution of particles ance check intervals on simulation time. The
in the [0, 1]n × T space due to adding of new bar values are the extra simulation time in log
particles at a small area of the whole domain. scale. The optimized interval is 3s.
computations, e.g., pressure and temperature), flags (indicators, such as indicator for ghost
particle or real particle) and neighbor information (it is a vector of particle keys in our application). Similarly, information that is contained in a bucket object can also be categorized
into different categories: identifier, affiliate, dimension information (maximum and minimum
coordinates, boundary information), flags (indicators, such as guest indicator, which is used to
indicate whether the bucket belongs to another adjacent subdomain or not), neighbor information (keys of 27 neighbor buckets including its own key) and owned particles (it is a vector of
particle keys in our application). Objects are then accessed through hash tables.
One of the fundamental data structures that allows flexible data access is a hash table. Based
on the keys of data, the address-calculator(hashing) function determines in which slot the data
should be stored:
Index = hash(key)
(2)
Data in a hash table can be accessed with time complexity of O(1) when there is no hash
collision. One way to handle hash collisions is to use an additional sorted vector attached to
the hash table. When several keys hash to the same index, a vector will be created. The vector
is sorted based on keys so that a binary search (with an average time complexity of O(log n))
can be used to find the correct position for adding, deleting or retrieving of data. Another
option is using an additional linked list which is more flexible in memory allocation but slower
in searching (O(n)). In addition, pointer based memory accessing of linked lists will greatly slow
down the efficiency of data access for long linked lists. Choosing the proper way to handle hash
conflicts greatly depends on the problem itself. For volcano plume simulation, new particles are
added at the bottom of the eruption vent (see Fig. 3), making the linked list a better choice.
Pointers to these new particles will be stored in external linked lists as shown in Fig. 3. For
time-independent keys, the hash function can be a simple function like:
Index =

Key − M in Key
× Hash T able Size
M ax Key − M in Key

(3)

One natural way to hash time-dependent keys [k, t] is to convert the two elements in the key
into one number taking k as the higher digit and t as the lower digit of the large number.
However, for volcano plume simulation, only ghost particles for eruption boundary condition
will be successively added at the same place: the bottom of the vent. That is to say, particles
5

	

et al. / Procedia
Computer Science 108C (2017) 786–795
Data Management and VolcanoZhixuan
Plume Cao
Simulation
with SPH
Cao, Patra and Jones

Table 1: Computational cost per particle for different steps
Step
neighbor search
update momentum and energy
update density
update position
velocity filtering
apply wall boundary condition
summation (ms)

Cost (ms)
0.41
0.70
0.42
0.01
0.43
0.75
-

Real
Yes
Yes
Yes
Yes
Yes
No
1.97

wall
No
No
No
No
No
Yes
0.75

eruption
No
No
No
Yes
No
No
0.01

pressure
No
No
No
No
No
No
0.00

are distributed non-uniformly in the [0, 1]n × T space as shown in Fig. 3. To avoid a nonuniform sparse hash table and conserve spatial locality, we only plug the first number, k, of the
key, [k, t], into the hashing function, Eq. (3). All particles with the same birth location will
hash to the same index and be handled by external linked lists.

2.3

Domain decomposition and load balancing strategy

Particles in our problem can be categorized into four types: real particles, wall ghost particles,
pressure ghost particles and eruption ghost particles. Ghost particles are for imposition of
corresponding boundary conditions (See Fig. 5). As different types of particles involve different
amount of computational work, shown by table 2.3, we assign different work load weights for
different types of particles based on profiling data. Instead of simply using number of contained
particles as work load for buckets, work load of each bucket is determined by summing up work
load weights of all particles within the bucket. The SFC of buckets now becomes a weighted
sequence. Domain decomposition is conducted by cutting the weighted SFC of buckets into
pieces. Each piece of the SFC should then have a balanced total work load.
Figure 2 shows how the domain is decomposed based on partition of the SFC of buckets.
The particles are automatically split into several groups along with buckets that contain them.
As the SFC of buckets is a curve in three dimensional (3D) space, partition of this curve
will automatically lead to 3D domain decomposition. Movement of particles, addition of new
particles, and adjustments of domain will lead to significant load imbalance between processes.
To restore load balance, computational load is monitored at a given interval which is optimized
based on numerical experiments. Repartitioning is carried out when load imbalance is larger
than a given tolerance.
As some of the neighbor particles reside in other partitions, a set of “guest” particles and
buckets are used to synchronize data across partitions. To minimize communications, data is
synchronized only where needed, using non-blocking MPI communications.

3

Dynamic Halo Domain

For plume simulation and similar phenomena, where some fluid ejects into a stationary fluid
and gets mixed, the stationary fluid must be represented. A lot of CPU time will be spent
on computing associated with these stationary particles. If simulation of stationary particles
can be avoided, the computational cost will be greatly reduced. We propose a simple strategy
to add such a feature to our code with low computational cost. We add a scan function to
monitor the outermost layer of the domain. When the ejected fluid reaches the boundary of
the current domain, pressure ghost particles (light blue particles in the right figure of Fig. 5)
6

791

792	

et al. / Procedia
Computer Science 108C (2017) 786–795
Data Management and VolcanoZhixuan
Plume Cao
Simulation
with SPH
Cao, Patra and Jones

Figure 5: The left figure shows a plume photo of Sakurajima Volcano eruption, the middle figure
shows a typical simulation results with the same input parameters as ”Run P1” in reference
[12]. The right figure is a cross-section view of the computational domain, it illustrates how
particles are deployed on different boundaries: red are real particles, dark blue are eruption
ghost particles, grey are wall ghost particles, the light blue are pressure ghost particles.
will be turned to real particles (red particles in the right figure of Fig. 5) and then add a new
layer of pressure ghost particles for pressure boundary conditions. Wall ghost particles (grey
particles in the right figure of Fig. 5) will also be added when necessary. The original workflow
is modified to enable such a feature. The modified workflow is shown on the right in Fig. 1).
An involvement flag is added to particle class to distinguish different states of involvement.
Particles are categorized into three groups based on the value of the involvement flag: involved,
potentially involved, and not involved. The involved particles are particles that have already
been affected by the mixing. The potentially involved particles are particles that have not
been involved in mixing but are adjacent to involved particles and will thus be involved in the
near future. All communication and computation associated with uninvolved particles can be
ignored. That is to say, only potential involved and involved particles need to be simulated.
As simulation progresses, the ejected fluid will reach a larger area and more and more particles
will be influenced. When originally stationary air is influenced by erupted material, the mass
fraction of the erupted material will increase from zero to a positive value. So we can determine whether a particle should turn to involved status based on whether the mass fraction of
that particle is larger than a given threshold (10−5 in our simulation) or not. Other physical
properties, like velocity, can also serve as alternative “switch criteria.”
A similar scheme is also deployed for buckets resulting in a categorization of buckets. The additional complexity is that potentially involved ghost particles must also be taken into accounted.

4

Numerical Test

We use the same governing equations and input parameters as the plume simulation presented
by Suzuki [12]. A typical simulation result of volcano plume by our code is shown in Fig. 5.

4.1

Scalability test

Experiments have been carried out on the computational cluster of Center for Computational
Research (CCR) at Buffalo. The compute servers have 12 cores (two Xeon E5645 processors
per server) running at 2.40GHz clock rate with 4GB memory per core on a Q-Logic Infiniband
network. Main memory and level 3 cache are shared on each node. The simulation domain is
7

	

et al. / Procedia
Computer Science 108C (2017) 786–795
Data Management and VolcanoZhixuan
Plume Cao
Simulation
with SPH
Cao, Patra and Jones

Figure 6: The left figure shows strong scalability tests result. middle figure is the zoomed view
of first one. It is obviously shown that strong scalability is better when the problem size is
larger. The right figure is weak scalability test results
Table 2: Simulation time for the same particle weight and
different particle weights at different physical times.
Physical time
Same weight
Different weights

10 s
1141.7
1108.2

20s
4119.4
4057.0

30 s
10371.0
10281.5

40 s
12453.7
12166.3

a box with initial dimension [−4.8km, 4.8km] × [−4.8km, 4.8km] × [0km, 6km] for case 1 and
[−10km, 10km] × [−10km, 10km] × [0km, 5km] for case 2. The smoothing length (we set initial
intervals between particles equal to smoothing length) equals to 200m and 100m respectively
for test case 1 and test case 2. So the computational work load of test case 2 is larger than that
of the test case 1. The simulations run for 20s physical time. Parallel speed up is shown in Fig.
6. Test case 2 shows better speed up than test case 1 which implies that strong scalability can
be improved by increasing total amount of work load. The weak scalability test is conducted
with the same initial domain as test case 1 and various smoothing length. Each simulation runs
for 400 time steps. The average number of real particles for each process keeps constant at
25900, so the average work load for each processor is approximately constant. Right side figure
in Fig 6 shows the timing of a weak scalability test.

4.2

Effect of workload check interval and calibrated particle weight

In section 2.3, we proposed to use different particle weights for different types of particles when
estimate the workload of buckets. The effect of using different particle weight is demonstrated
in table 4.2. The simulation time is reduced by using different particle weights. However, the
amount of time that is reduced is not as significant as we expected. One possible explanation is
that real particles are the major population, so the load imbalance caused by assigning improper
weight values to ghost particles is small.
The best load balance check interval is calibrated based on a series of simulations with different
load balance scan intervals. From 0 - 50 seconds physical time, the interval of 1 second shows a
better load balance than interval of 2 seconds. However, for the simulation of 50 - 100 seconds,
interval of 2 seconds is better. This implies that loss of load balance accumulates faster and
requires a more frequent re-decomposition of domain at the early stages of simulation. This is
consistent with the plume development process: the domain grows quickly at the beginning as
8

793

794	

et al. / Procedia
Computer Science 108C (2017) 786–795
Data Management and VolcanoZhixuan
Plume Cao
Simulation
with SPH
Cao, Patra and Jones

Table 3: Computational work load of extra
steps for domain adjusting. SWCH represents
step that switch pressure ghost particle to real
particle, ADPP is short for adding new pressure ghost particles, ADWP represents adding
wall ghost particles, SCN is short for scanning
the outmost layer of the domain. Momentum
and energy update (UPME) and position update (UPP) also included for comparison.
Functions
UPME
UPP
ADPP
ADWP
SWCH
SCN

Total time (s)
2954.8
38.55
21.51
8.88
0.08
7.72

Called times
201
201
3
3
2
201

Figure 7: The figure on the top shows execution time without domain adjusting, the figure
on the bottom shows execution time with domain adjusting. Different bins represent execution time up to specific physical time indicated
by horizontal axis.

erupted material ejects into the environment and spreads. Afterwards, the spreading speed of
the front edge slows down due to viscosity and momentum exchange leading to slowing down
of domain growth. We need to emphasize that, these optimized parameters, including weights
of particles and load balance checking interval, are case sensitive and need to be re-calibrated
for different applications and hardware architecture.

4.3

Effect of dynamic halo domains

For the test problem in this paper, the volcano plume finally reaches to a region of
[−10km 10km] × [−10km 10km] × [0km 20km] after around 300 seconds of eruption. When
numerical simulation goes up to 90 seconds, the plume is still within a region of [−3km 3km] ×
[−3km 3km] × [0km 6km]. This implies that adjusting of domain can avoid computing large
number of uninfluenced particles, especially for the early stages of simulation. On the other
hand, additional functions (see Fig. 1) for domain adjusting will add extra computational cost.
Table 3 shows the extra computational cost corresponding to these functions. Profiling data
for momentum and energy update (UPME) and position update (UPP) is also included in the
table for the purpose of comparison. As we can see from the table, the cost of SCN is even
smaller than UPP function, which is the cheapest function (as shown in table 2.3) in the regular
workflow (see Fig. 1). The other three functions, ADPP, ADWP and SWCH, are only called
a few times during the simulation, and as a consequence, the extra computational cost due to
them are negligible. To summarize, the additional cost caused by the domain adjusting functions is ignorable. Figure 7 shows that simulation time of the test problem is greatly reduced
when we adopt the domain adjusting strategy in our code.

5

Conclusion

We developed data management strategies for a MPI-parallel implementation of the SPH
method to simulate volcano plumes. Neighbor searching and domain decomposition is based
on the background grid. A SFC based index scheme is adopted to identify buckets and timedependent SFC keys are used as identifiers for particles. Hash tables with external linked lists
9

	

Data Management and VolcanoZhixuan
Plume Cao
Simulation
with SPH
Cao, Patra and Jones
et al. / Procedia
Computer Science 108C (2017) 786–795

are adopted for accessing particles and buckets data. Based on calibrated particle weights, a
dynamic load balance strategy is developed by checking load balance and re-decomposing the
domain at an optimized interval. The performance of the code was further improved by several factors by adjusting the computational domain during simulation. Overall we obtain good
scalability and performance.
The flexibility of our data access methodology enables implementing mesh-free methods for
solving more complicated problems and using more advanced techniques, such as dynamic particle splitting techniques[13, 3], which will give higher resolution at the area of interest by
splitting one large particle to several smaller ones. The data structure, particle and bucket
indexing strategies, domain decomposition, dynamic load balancing method and domain adjusting strategies in this paper can be adopted to other mesh-free methods (not just SPH).

References
[1] M Bursik, B Martınez-Hackert, H Delgado, and A Gonzalez-Huesca. A smoothed-particle hydrodynamic automaton of landform degradation by overland flow. Geomorphology, 53(1):25–44,
2003.
[2] AJC Crespo, JM Domı́nguez, BD Rogers, M Gómez-Gesteira, S Longshaw, R Canelas, R Vacondio,
A Barreiro, and O Garcı́a-Feal. Dualsphysics: Open-source parallel cfd solver based on smoothed
particle hydrodynamics (sph). Computer Physics Communications, 187:204–216, 2015.
[3] J Feldman and J Bonet. Dynamic refinement and boundary contact forces in sph with applications
in fluid flow problems. International Journal for Numerical Methods in Engineering, 72(3):295–
324, 2007.
[4] Angela Ferrari, Michael Dumbser, Eleuterio F Toro, and Aronne Armanini. A new 3d parallel sph
scheme for free surface flows. Computers & Fluids, 38(6):1203–1217, 2009.
[5] Bouchra Haddad, David Palacios, Manuel Pastor, and José Juan Zamorano. Smoothed particle
hydrodynamic modeling of volcanic debris flows: Application to huiloac gorge lahars (popocatépetl
volcano, mexico). Journal of Volcanology and Geothermal Research, 324:73–87, 2016.
[6] Andras Laszloffy, Jingping Long, and Abani K Patra. Simple data management, scheduling and
solution strategies for managing the irregularities in parallel adaptive hp finite element simulations.
Parallel Computing, 26(13):1765–1788, 2000.
[7] Joseph J Monaghan and John C Lattanzio. A refined particle method for astrophysical problems.
Astronomy and astrophysics, 149:135–143, 1985.
[8] Abani Patra and J Tinsley Oden. Problem decomposition for adaptive hp finite element methods.
Computing Systems in Engineering, 6(2):97–109, 1995.
[9] Abani K Patra, AC Bauer, CC Nichita, E Bruce Pitman, MF Sheridan, M Bursik, B Rupp,
A Webber, AJ Stinton, LM Namikawa, et al. Parallel adaptive numerical simulation of dry
avalanches over natural terrain. Journal of Volcanology and Geothermal Research, 139(1):1–21,
2005.
[10] AK Patra and DW Kim. Efficient mesh partitioning for adaptive hp finite element meshes. In In
International Conference on Domain Decomposition Methods. Citeseer, 1999.
[11] Hans Sagan. Space-filling curves. Springer Science & Business Media, 2012.
[12] Yujiro J Suzuki, Takehiro Koyaguchi, Masaki Ogawa, and Izumi Hachisu. A numerical study of
turbulent mixing in eruption clouds using a three-dimensional fluid dynamics model. Journal of
Geophysical Research: Solid Earth, 110(B8), 2005.
[13] R Vacondio, BD Rogers, and PK Stansby. Accurate particle splitting for smoothed particle hydrodynamics in shallow water with shock capturing. International Journal for Numerical Methods
in Fluids, 69(8):1377–1410, 2012.

10

795

