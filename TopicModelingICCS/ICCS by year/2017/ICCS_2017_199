Available online at www.sciencedirect.com

ScienceDirect
Procedia Computer Science 108C (2017) 2292–2297

International Conference on Computational Science, ICCS 2017, 12-14 June 2017,
Zurich, Switzerland

Optimization of DBN using Regularization Methods
Optimization
of
Regularization
Methods
Applied for Recognizing
Arabic
Handwritten
Script
Optimization
of DBN
DBN using
using
Regularization
Methods
Applied
for
Applied
for Recognizing
Recognizing Arabic
Arabic Handwritten
Handwritten Script
Script
Mohamed Elleuch1, Najiba Tagougui2 and Monji Kherallah3
1

1
2
3
National
School
of Computer
Science (ENSI), University
of Manouba, TUNISIA.
Mohamed
Elleuch
1, Najiba Tagougui2 and Monji Kherallah3
Elleuch
,
Najiba
Tagougui
and
Monji
Kherallah
1 Mohamed
National
Engineering
School
of
Tunis
(ENIT),
University
of
Tunis
El
Manar,
TUNISIA.
1National School
3 of Computer Science (ENSI), University of Manouba, TUNISIA.
2
National
School
of School
Computer
Science
(ENSI),
University
of Manouba,
TUNISIA.
Faculty
of Sciences,
University
of Sfax,ofTUNISIA.
National
Engineering
of Tunis
(ENIT),
University
Tunis
El Manar,
TUNISIA.
2
3
National Engineering
School
of
Tunis
(ENIT),
University
of
Tunis
El
Manar,
TUNISIA.
mohamed.elleuch.2015@ieee.org,
najiba.tagougui@isggb.rnu.tn,
monji.kherallah@gmail.com
Faculty
of
Sciences,
University
of
Sfax,
TUNISIA.
3
Faculty
of
Sciences,
University
of
Sfax,
TUNISIA.
mohamed.elleuch.2015@ieee.org, najiba.tagougui@isggb.rnu.tn, monji.kherallah@gmail.com
mohamed.elleuch.2015@ieee.org, najiba.tagougui@isggb.rnu.tn, monji.kherallah@gmail.com
2

Abstract
Since
the mid 2010’s, Deep learning has been regarded as a boom and consequently it has big success
Abstract
in
a
large
field2010’s,
of applications
like speech
andregarded
pattern recognition.
Handwriting
recognition
indeed
Abstract
Since the mid
Deep learning
has been
as a boom and
consequently
it has bigissuccess
amongst
the
triumphal
applications
in
the
field
of
pattern
recognition.
Despite
being
quite
matured,
Since
the
mid
2010’s,
Deep
learning
has
been
regarded
as
a
boom
and
consequently
it
has
big
success
in a large field of applications like speech and pattern recognition. Handwriting recognition is indeed
this
is
still
forlike
thespeech
Arabic
handwritten
script
and hence
severalrecognition
questions
still a
in
a field
largethe
field
ofambiguous
applications
and
pattern
recognition.
Handwriting
is
indeed
amongst
triumphal
applications
in
the field
of pattern
recognition.
Despite
being
quiteare
matured,
amongst
the
triumphal
applications
in
the
field
of
pattern
recognition.
Despite
being
quite
matured,
challenge.
In
this
study,
Deep
Belief
Network
(DBN)
for
Arabic
handwritten
script
recognition
this field is still ambiguous for the Arabic handwritten script and hence several questions are still isa
this
field isInstill
for
thearchitecture
Arabic
handwritten
script
and hence
several
questions
are stillof
investigated.
We
then
ensure
DBN
against
because
of mighty
performance
challenge.
thisambiguous
study,
Deep
Belief
Network
(DBN)over-fitting
for Arabic
handwritten
script
recognition
isa
challenge.
In
this
study,
Deep
Belief
Network
(DBN)
for
Arabic
handwritten
script
recognition
is
dropout
and
dropconnect.
Training
with
the
both
regularization
methods,
a
randomly
selected
subsets
investigated. We then ensure DBN architecture against over-fitting because of mighty performance of
investigated.
We
then
ensure
DBN
architecture
against
over-fitting
because
of
mighty
performance
of
of
activations/weights
are
dropped.
As
a
result,
the
evaluation
on
the
HACDB
database
to
deal
with
dropout and dropconnect. Training with the both regularization methods, a randomly selected subsets
character
level
shows are
an Training
improvement
of
classification
erroron
rate
using
DBN
trained
with
dropout
and
dropconnect.
with
both
regularization
methods,
a randomly
selected
subsets
of
activations/weights
dropped.
As athe
result,
the
evaluation
thewhen
HACDB
database
to deal
with
dropout
or
dropconnect.
of
activations/weights
are
dropped.
As
a
result,
the
evaluation
on
the
HACDB
database
to
deal
with
character level shows an improvement of classification error rate when using DBN trained with
character
shows an improvement of classification error rate when using DBN trained with
dropout
orlevel
dropconnect.
©
2017 The
Authors.
Published
Elsevier
B.V.
Keywords:
Deep
learning,
DBN,byArabic
handwritten,
over-fitting, dropout, dropconnect
dropout
or
dropconnect.
Peer-review under responsibility of the scientific committee of the International Conference on Computational Science
Keywords: Deep learning, DBN, Arabic handwritten, over-fitting, dropout, dropconnect
Keywords: Deep learning, DBN, Arabic handwritten, over-fitting, dropout, dropconnect

1 Introduction and related works
1
Introduction
and
related
works
it has been and
provedrelated
that the classification/recognition
systems reliant on deep networks
1 Recently,
Introduction
works

andRecently,
their derivatives
like proved
Restricted
Machine (RBM), Stacking
and
it has been
thatBoltzmann
the classification/recognition
systems Auto-Encoder
reliant on deep(SAE)
networks
Deep
Belief
Network
(DBN)
are
well
performing
with
accuracy
in
several
themes
namely
in
speech
Recently,
it
has
been
proved
that
the
classification/recognition
systems
reliant
on
deep
networks
and their derivatives like Restricted Boltzmann Machine (RBM), Stacking Auto-Encoder (SAE) and
and their
derivatives
like
Restricted
Machine
(RBM),inStacking
Auto-Encoder
and
recognition
[1]
and image
recognition
[2,
3, 4]. Likewise,
researches
have been
fruitful
and (SAE)
interesting
Deep
Belief
Network
(DBN)
are wellBoltzmann
performing
with accuracy
several
themes
namely
in speech
Deep
Belief
Network
(DBN)
are
well
performing
with
accuracy
in
several
themes
namely
in
speech
for
the
recognition
of
the
printed
handwriting
for
Latin
and
Asian
characters
in
order
to
enjoy
the
recognition [1] and image recognition [2, 3, 4]. Likewise, researches have been fruitful and interesting
recognition
[1]
and
image
recognition
[2,
3,
4].
Likewise,
researches
have
been
fruitful
and
interesting
benefits
offered
by
deep
learning.
for the recognition of the printed handwriting for Latin and Asian characters in order to enjoy the
for
of the
printed
handwriting
forbeen
Latin
and Asian
in order Arabic
to enjoyword
the
Ittheis recognition
noticeable
such
approaches
have not
practiced
yet characters
on the handwritten
benefits
offered by that
deep
learning.
recognition
problem.
For
this
reason,
we
have
investigated
into
using
one
of
the
above
methods,
the
benefits
offered
by
deep
learning.
It is noticeable that such approaches have not been practiced yet on the handwritten Arabic word
DBN,
to
construe
its
recognition
performance
on
the
Arabic
handwritten
characters/word
recognition
It
is
noticeable
that
such
approaches
have
not
been
practiced
yet
on
the
handwritten
Arabic
word
recognition problem. For this reason, we have investigated into using one of the above methods, the
recognition
problem.
For important
this reason,
we of
have
investigated
onecharacters/word
of the above
methods,
the
task
of theitsmost
cons
these
architectures
isusing
the huge
number
of parameters,
and
DBN,[5].
to One
construe
recognition
performance
on
the Arabicinto
handwritten
recognition
DBN,
to
construe
its
recognition
performance
on
the
Arabic
handwritten
characters/word
recognition
as
a
result
over-fitting
might
happen.
Yet,
Dropout
[6]
and
Dropconnect
[7]
training
is
regarded
as
an
task [5]. One of the most important cons of these architectures is the huge number of parameters, and
task
[5].
One
of
the
most
important
cons
of
these
architectures
is
the
huge
number
of
parameters,
and
as a result over-fitting might happen. Yet, Dropout [6] and Dropconnect [7] training is regarded as an
as a result over-fitting might happen. Yet, Dropout [6] and Dropconnect [7] training is regarded as an
1877-0509 © 2017 The Authors. Published by Elsevier B.V.
Peer-review under responsibility of the scientific committee of the International Conference on Computational Science
10.1016/j.procs.2017.05.070

	

Mohamed Elleuch et al. / Procedia Computer Science 108C (2017) 2292–2297

efficient method in order to inspect over-fitting by randomly omitting subsets of features at each
iteration of a training procedure.
In [8], Hinton et al. proposed a greedy layer-wise algorithm for training a multilayer belief network
(DBN). The Deep Belief Network (DBN) is a stack of Restricted Boltzmann Machines (RBMs). This
model was applied to MNIST digits [9], phone [10, 11] and handwritten Arabic text [12, 13]
recognition. Porwal et al. [12] proposed a system based on DBN for recognition of Arabic parts of
words (PAWs) using an ensemble of biased learners in a hierarchical framework. The proposed DBN
architecture which was tested on AMA Arabic PAW dataset gave favorable PAWs recognition rate. In
another DBN-based method, Elleuch et al. [13] investigated two deep architectures: Deep Belief
Networks (DBN) and Convolutional Deep Belief Networks (CDBN) which were applied respectively
on low-level and high-level dimension in Arabic textual images. The experimental study proved
promising results which are comparable to the state-of-the-art on handwritten text recognition.
A new form of regularization called Dropout has been recently proposed by Hinton et al. [6].
Further experiments proved that over-fitting is truly decreased while boosting test performance. Later,
in [14, 15, 16, 17, 18] authors showed that results can be further improved by adding dropout
regularization on different deep architectures like Neural Networks, Recurrent Neural Networks
(RNN), Deep RNN, CNN based-SVM and Deep SVM. Also known as Dropconnect [7] was intended
to prevent "co-adaptation" of units in a neural network.
In this paper, we investigated Dropout and Dropconnect for DBN architecture proposed in [19]
applied to the offline Arabic handwritten script recognition problem. Both techniques are only
possible for fully-connected layers. We conducted our experimental studies on HACDB [20] which is
an offline Arabic character database. Figure 1 presents our proposed system architecture including the
pre-processing steps, and then an automatic feature extractor named DBN using Dropout or
Dropconnect techniques.

Figure 1: Proposed system overview.
The rest of the paper was organized as follows: In section 2, we described the basic concepts
behind DBN architecture. In section 3, we briefly summarized dropout and dropconnect principles.
Our experimental study was next described. Discussion and analysis of the results were made in
section 4, while the last section was devoted for conclusions.

2 Deep Belief Networks (DBN)
Deep belief network (DBN), proposed by Hinton [9], is a probabilistic generative model
constituted of many layers of stochastic hidden variables and one layer of visible neurons. It can be
trained with an efficient algorithm by greedily learning layer-by-layer, as an RBM, to initialize the
deep network. Two steps were vital to learn in the network: the first was an unsupervised feature
learning while the second was a supervised learning of discriminating function [21, 22].
Restricted Boltzmann Machines (RBM) [23] are generative stochastic neural networks between a
set of visible neurons v = {v1, v2,…,vn}, and a set of hidden neurons h = {h1, h2,…,hm}. The visible and
hidden units were connected with a weight matrix W. The model architecture was restricted by not
allowing intra-layer connections between the units. Formally, the energy function and the probabilistic
semantics of RBM were computed as follows:

2293

Mohamed Elleuch et al. / Procedia Computer Science 108C (2017) 2292–2297

2294	

E v, h   vi h jW ij   vi ai   h j b j
i 1 j 1

i 1

(1)

j 1

1
pv    eE (v,h)
Z h

(2)
Where wij denotes the connection between visible neurons vi and hidden neurons hj , ai and bj are
their biases, respectively, and Z is the partition function.

3 Regularization methods
Due to the immense number of parameters in our network architecture, over-fitting was able to
happen and hence dropout or dropconnect (see figure 2) was practiced so that we can impede our
system from over-fitting and better its performance. Dropconnect is similar to dropout, but applied to
the weights W.
Dropout technique, primarily suggested by Hinton et al. [6] as a form of regularization for fullyconnected layers, was triumphal applied with several types of deep neural networks [14,16] and it
proves its performance in a variety of recognition tasks [17, 24]. When training with Dropout, every
element of a layer's output was left with probability p, otherwise was adjusted to zero with probability
1 - p.
Dropconnect [7] which is a generalization of dropout sets instead an arbitrarily chosen subset of
weights within the network to zero, i.e. dropout mainly drops or sets to zero complete rows or columns
of the weight matrix W. Contrarily, dropconnect is more fine grained and drops individual weight
matrix elements.

Figure 2: (a) DBN after using Dropout

(b) DBN after Dropconnect

4 Experiments, results and discussion
Our offline Arabic handwriting recognition system based on DBN classifier was presented in this
section. The dropout and dropconnect technique has been applied during training on the DBN
classifier which was tested on HACDB database. Hence, we would be able to test the efficiency of the
regularization methods separately for DBN compared to the previous works in [25]. Results will be
detailed and discussed in the next subsections.

4.1 Experiments on HACDB database
In order to evaluate the effectiveness of unsupervised feature learning approach using DBN with
dropout/dropconnect, we investigated the performance on the data of HACDB database which was
designed to cover all shapes of Arabic characters.

	

Mohamed Elleuch et al. / Procedia Computer Science 108C (2017) 2292–2297

The HACDB handwritten character classification task consists of black and white images, each
containing a character (58 shapes) or overlapping character (8 shapes) forming 66 classes. Every
Arabic script can have more than one shape according to their position in a word whether at the
beginning, middle, final, or stand alone. Each character in the 5.280 training images and 1.320 test
images is normalized 28 × 28 pixels. We scale the pixel values to the [0, 1] range before inputting to
our models. Details of the database are presented in Table 1. Moreover, for more details about the
classes for each shape of HACDB you can refer to work in [17].
DataSet
Training
Testing
Sum

Writers
50
50

Forms/Character
80
20
100

Total of Shapes
66
66

Total
5.280
1.320
6.600

Table 1: Details of HACDB database

4.2 Experiments setting
In order to boost our classification result, we chose our best powerful feature extractor network
described in [19]. This feature extractor is made up of a two layers RBM with 1000 hidden units in
each layer. The adopted architecture of DBN used in experiment on HACDB dataset was 784-10001000- 66, i.e., it represents a net with input images of size 28 × 28 pixels giving an input
dimensionality of 784 with 2 hidden layers. The last layer is composed by 66 units constituting the
output of the network. The obtained error rate was 3.64 % with 48 incorrectly classified characters
(see figure 3). In order to optimize this architecture, two regularization methods were applied for DBN
structure named dropout and dropconnect.
In the experiment, we applied dropout/dropconnect separately in each layer of our network. In both
techniques we tentatively removed 20% and 50% from visible layer and hidden layers respectively.
Those units/weights were randomly chosen only during the training stage. In the following section we
compared dropout and dropconnect with No-Drop.

Figure 3: Samples of 48 incorrectly classified characters using DBN (No-Drop)

4.3 Results and discussion
The classification error rate got with the DBN network trained without dropping units (No-Drop)
on the HACDB database equaling to 3.64% over the test set was effective if compared to previous
outcomes shown in the literature [17]. Appling dropout technique, we got 2.73% (36 wrong
characters), relative improvement in character error rate (dropped by 0.91%). Whilst applied

2295

Mohamed Elleuch et al. / Procedia Computer Science 108C (2017) 2292–2297

2296	

dropconnect to the DBN model, in visible and hidden layers, reduces the error rate to 2.27% (30
wrong characters) achieving a gain of 0.46% over the dropout technique.
It is noted that dropconnect mainly plays a better role than dropout in this handwritten Arabic
characters recognition task. Consequently, dropconnect prevented the fully-connected layer overfitting
more efficiently than the others in this experiment. As shown in Table 2, the two methods improve
over No-Drop in this Arabic HACDB database.
Compared with character recognition accuracies gained from state-of-the-art offline
Digit/Asian/Arabic systems, these rates are statistically notably paramount. A comparative study of
our architecture performance with other methods using handwritten Arabic database HACDB was also
carried out. In Table 2, it was shown that DBN with dropout and dropconnect outperforms CNN
based-SVM model with both the presence and the absence of dropout and ANN classifier once
experimented on HACDB database.
Authors

Methods

Error rate

Lawgali [26]

ANN

3.44% (1600 images)
21.18% (5600 images)

Elleuch et al [17]

CNN based-SVM
CNN based-SVM with dropout

6.59 %
5.83 %

Elleuch et al [25]

DBN without dropout (No-Drop)

3.64 %

Present work

DBN with dropout
DBN with dropconnect

2.73 %
2.27 %

Table 2: Performance comparisons with HACDB Database.

5 Conclusion
In this paper, two well-known regularization methods for deep belief network were studied and
exploited to eliminate overfitting. Yet, this problem is still present as a challenge to master when it
deals with training highly large neural networks or working in fields offering a tiny quantity of data.
Therefore, we inquired into a feature learning focused on approach by the DBN model for
handwritten Arabic characters recognition problem where we applied the dropout and dropconnect
technique during training so that we could prevent our system from over-fitting. It was proved that for
the character level one with HACDB database, the outcomes were favorable with an error
classification rate of 2.73% using dropout and 2.27% using dropconnect. Ultimately, we deduced that
applying regularization methods in a Deep Belief Network was able to better significantly the offline
Arabic handwriting recognition than No-Drop.

References
[1] A. Mohamed, G. Dahl, and G. Hinton, “Acoustic modeling using deep belief networks,” IEEE
Transactions on Audio, Speech, and Language Processing, vol. 20(1), pp. 14–22, January 2011.
[2] Y. LeCun, L. Bottou, Y. Bengio, and P. Haffner,‫“ ‏‬Gradient-based learning applied to document
recognition,” Proceedings of the IEEE, vol. 86(11), pp. 2278-2324, Nov. 1998.
[3] S. Zhong, Y. Liu, and Y. Liu, “Bilinear deep learning for image classification,” Proceedings of
the 19th ACM International Conference on Multimedia, pp. 343-352, 2011.
[4] A. Karpathy, A. Joulin, and L. Fei-Fei, “Deep Fragment Embeddings for Bidirectional Image
Sentence Mapping,” Computer Vision and Pattern Recognition, 2014.
[5] M. Elleuch, N. Tagougui, and M. Kherallah, “Arabic handwritten characters recognition using
deep belief neural networks,” In: 12th International Multi-Conference on Systems, Signals and
Devices - Conference on Communication & Signal Processing, SSD 2015, pp. 1-5, 2015.

	

Mohamed Elleuch et al. / Procedia Computer Science 108C (2017) 2292–2297

[6] G. E. Hinton, N. Srivastava, A. Krizhevsky, I. Sutskever, and R. R. Salakhutdinov, “Improving
neural networks by preventing co-adaptation of feature detectors,” arXiv preprint
arXiv:1207.0580, 2012.
[7] L. Wan, M. Zeiler, S. Zhang, Yann L. Cun, and R. Fergus, “Regularization of neural networks
using dropconnect,” in Proceedings of the 30th International Conference on Machine Learning
(ICML-13), Sanjoy Dasgupta and David Mcallester, Eds. May 2013, vol. 28, pp. 1058-1066,
JMLR Workshop and Conference Proceedings.
[8] G. E. Hinton, “A Practical Guide to Training Restricted Boltzmann Machines,” Neural Networks :
Tricks of the Trade, vol. 7700, pp. 599-619, 2012.
[9] G. Hinton, S. Osindero, and Y.W. Teh, “A fast learning algorithm for deep belief nets,” Neural
Computation, vol. 18(7), pp. 1527–1554, 2006.
[10] A. Mohamed, G. Dahl, and G. E. Hinton, “Acoustic modeling using deep belief networks,” IEEE
Transactions on Audio, Speech, and Language Processing, vol. 20(1), pp. 14-22, 2011.
[11] G. E. Dahl, M. Ranzato, A. Mohamed, and G. E. Hinton, “Phone recognition with the meancovariance restricted Boltzmann machine,” in Advances in Neural Information Processing
Systems, vol. 23, pp. 469-477, Canada, 2010.
[12] U. Porwal, Y. Zhou, and V. Govindaraju, “Handwritten Arabic Text Recognition using Deep
Belief Networks,” 21st International Conference on Pattern Recognition (ICPR), Japan, 2012.
[13] M. Elleuch, N. Tagougui, and M. Kherallah, “Deep Learning for Feature Extraction of Arabic
Handwritten Script,” Computer Analysis of Images and Patterns - 16th International Conference,
CAIP 2015, Valletta, Malta, part. (2), pp. 371-382, 2015.
[14] N. Srivastava, G. Hinton, A. Krizhevsky, I. Sutskever, and R. R. Salakhutdinov, “Dropout: A
simple way to prevent neural networks from overfitting,” The Journal of Machine Learning
Research, vol. 15(1), pp. 1929-1958, 2014.
[15] V. Pham, T. Bluche, C. Kermorvant, and J. Louradour, “Dropout improves recurrent neural
networks for handwriting recognition,” arXiv preprint arXiv:1312.4569, 2013.
[16] R. Maalej, N. Tagougui, and M. Kherallah, “Online Arabic Handwriting Recognition with
Dropout applied in Deep Recurrent Neural Networks.,” 12th IAPR International Workshop on
Document Analysis Systems (DAS), pp. 417-421, 2016.
[17] M. Elleuch, R. Maalej, and M. Kherallah, “A New Design Based-SVM of the CNN Classifier
Architecture with Dropout for Offline Arabic Handwritten Recognition,” International
Conference on Computational Science (ICCS 2016), U.S.A. 2016.
[18] M. Elleuch, R. Mokni, and M. Kherallah, “Offline Arabic Handwritten recognition system with
dropout applied in Deep networks based-SVMs,” International Joint Conference on Neural
Networks (IJCNN), pp. 3241-3248, Canada, 2016.
[19] M. Elleuch, N. Tagougui and M. Kherallah, “Arabic handwritten characters recognition using
deep belief neural networks,” In: 12th International Multi-Conference on Systems, Signals and
Devices - Conference on Communication & Signal Processing, Tunisia, 2015.
[20] A. Lawgali, M. Angelova, and A. Bouridane, “HACDB: Handwritten Arabic characters database
for automatic character recognition,” EUropean Workshop on Visual Information Processing
(EUVIP), pp. 255-259, 2013.
[21] T. Tieleman, and G. Hinton, "Using fast weights to improve persistent contrastive divergence,"
Proceedings of the 26th Annual International Conference on Machine Learning (ICML), pp.
1033-1040. June 2009.
[22] D.E. Rumelhart, G.E. Hinton, and R.J. Williams, "Learning representations by back-propagating
errors Nature,” Nature, vol. 323(9), pp. 533-536, 1986.
[23] A. Mohamed, T.N. Sainath, G. Dahl, B. Ramabhadran, G.E. Hinton, and M.A. Picheny, "Deep
belief networks using discriminative features for phone recognition," Proceedings of the IEEE
International Conference on Acoustics, Speech and Signal Processing, pp. 5060-5063, May 2011.
[24] S. Zhang, Y. Bao, P. Zhou, H. Jiang, & L. Dai, “Improving deep neural networks for LVCSR
using dropout and shrinking structure,” In Acoustics, Speech and Signal Processing (ICASSP),
2014 IEEE International Conference on, pp. 6849-6853, 2014.
[25] M. Elleuch, N. Tagougui, and M. Kherallah, “Towards Unsupervised Learning for Arabic
Handwritten Recognition Using Deep Architectures,” Neural Information Processing - 22nd
International Conference, ICONIP 2015, Istanbul, Turkey, part. (1), pp. 363-372, 2015.
[26] A. Lawgali, A. Bouridane, M. Angelova and Z. Ghassemlooy, “Handwritten Arabic character
recognition: Which feature extraction method?,” International Journal of Advanced Science and
Technology, vol. 34, pp. 1-8, 2011.

2297

