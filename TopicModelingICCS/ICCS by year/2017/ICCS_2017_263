Available online at www.sciencedirect.com

ScienceDirect
Procedia Computer Science 108C (2017) 1702–1711
This space
is reserved for the Procedia header, do not use it

This space is reserved for the Procedia header, do not use it
This space is reserved for the Procedia header, do not use it

International Conference on Computational Science, ICCS 2017, 12-14 June 2017,
Zurich, Switzerland

The Impact of Large-Data Transfers in Shared
The
Impact ofNetworks:
Large-Data
Shared
Wide-Area
AnTransfers
Empiricalin
The
Impact of Large-Data
Transfers
inStudy
Shared
Wide-Area Networks: An Empirical Study
AnvariAn
andEmpirical
Paul Lu
Wide-AreaHamidreza
Networks:
Study
Hamidreza
Anvari
and Alberta,
Paul Lu
University
of Alberta,
Edmonton,
Canada
Hamidreza
Anvari
and
Paul Lu
{hanvari, paullu}@ualberta.ca
University of Alberta, Edmonton, Alberta, Canada
{hanvari,
paullu}@ualberta.ca
University
of Alberta,
Edmonton, Alberta, Canada
{hanvari, paullu}@ualberta.ca

Abstract
Computational science sometimes requires large data files to be transferred over high bandwidthAbstract
delay-product
(BDP) wide-area networks (WANs). Experimental data (e.g., LHC, SKA), anAbstract
Computational science sometimes requires large data files to be transferred over high bandwidthalytics
logs,
and
filesystem
backups
are large
regularly
between research
and
Computational(BDP)
science
sometimes
requires
data transferred
files
to be transferred
over LHC,
high centres
bandwidthdelay-product
wide-area
networks
(WANs).
Experimental
data (e.g.,
SKA), anbetween
private-public
clouds.
Fortunately,
a
variety
of
tools
(e.g.,
GridFTP,
UDT,
PDS)
have
delay-product
(BDP)
wide-area
networks
(WANs). transferred
Experimental
data (e.g.,
LHC,centres
SKA), and
analytics
logs, and
filesystem
backups
are regularly
between
research
been
transfer bulk
data across
WANs with high performance.
alyticsdeveloped
logs,
andtofilesystem
backups
are regularly
between
research
and
between
private-public
clouds.
Fortunately,
a varietytransferred
of tools (e.g.,
GridFTP,
UDT,centres
PDS) have
However,
using large-data
transfer tools could
adversely
affect other
network
applications
on
between
private-public
clouds.
variety
of tools
GridFTP,
UDT,
PDS) have
been
developed
to transfer
bulkFortunately,
data across aWANs
with
high (e.g.,
performance.
shared
networks.
Many
of
the
tools
explicitly
ignore
TCP
fairness
to
achieve
high
performance.
been
developed
to transfer
bulk
data across
WANs
with high
performance.
However,
using
large-data
transfer
tools could
adversely
affect
other network applications on
Users
have experienced
high-latency
and
low-bandwidth
situations
whennetwork
a large-data
transferon
is
However,
usingMany
large-data
tools
could
adversely
affect other
applications
shared
networks.
of thetransfer
tools explicitly
ignore
TCP fairness
to achieve
high
performance.
underway.
But
there
have
been
few
empirical
studies
that
quantify
the
impact
of
the
tools.
As
sharedhave
networks.
Many high-latency
of the tools explicitly
ignore TCPsituations
fairness to
achieve
high performance.
Users
experienced
and low-bandwidth
when
a large-data
transfer is
an
extension
of our previous
work using
synthetic background
traffic, wea perform
an empirical
Users
have But
experienced
high-latency
and low-bandwidth
situations
large-data
is
underway.
there have
been few empirical
studies that
quantifywhen
the impact
of thetransfer
tools. As
analysis
of But
howthere
the bulk-data
transfer
tools studies
performthat
when
competing
with aofnon-synthetic,
underway.
have been
fewusing
empirical
quantify
thewe
impact
tools. As
an
extension
of our previous
work
synthetic background
traffic,
perform the
an empirical
application-based
workload (e.g.,
Network
File System).
Conversely,
show
an extension
of our
work
using tools
synthetic
background
traffic,we
wecharacterize
perform
an and
empirical
analysis
of how
theprevious
bulk-data
transfer
perform
when competing
with
a non-synthetic,
that,
for
example,
NFS
performance
can
drop
from
29
Mb/s
to
less
than
10
Mb/s
(for
a
single
analysis of how the
bulk-data
transfer
tools
competing
with a non-synthetic,
application-based
workload
(e.g.,
Network
Fileperform
System).when
Conversely,
we characterize
and show
stream)
when competing with
bulk-data
transfers
on a shared
network.
application-based
(e.g.,
Network
Filefrom
System).
Conversely,
we characterize
show
that,
for example, workload
NFS performance
can drop
29 Mb/s
to less than
10 Mb/s (forand
a single
that,
example,
NFS
performance
cannetworks;
drop from
lessdata
than
10 Mb/s
(for a shared
single
Keywords:
Bandwidth
sharing;
wide-area
high-performance
transfer;
fairness;
stream)
when
competing
with
bulk-data
transfers
on29a Mb/s
sharedtonetwork.
©
2017for
The
Authors.
Published
by Elsevier
B.V.
Peer-review
undercompeting
responsibilitywith
of thebulk-data
scientific committee
of the
Conference on Computational Science
stream) when
transfers
on International
a shared network.
network
Keywords: Bandwidth sharing; wide-area networks; high-performance data transfer; fairness; shared
Keywords:
Bandwidth sharing; wide-area networks; high-performance data transfer; fairness; shared
network
network

1 Introduction
1
Introduction
One part
of large-scale data processing projects is the problem of transferring the data across
1
Introduction
wide-area networks (WANs). Often, the data must be gathered (e.g., from remote sites), pro-

One part of large-scale data processing projects is the problem of transferring the data across
cessed,
possibly
transferred
for further processing),
and then transferring
possibly disseminated.
One
part
of
large-scale
data(e.g.,
processing
is the
the data
wide-area
networks
(WANs).
Often, theprojects
data must
be problem
gathered of
(e.g., from remote
sites),across
proIn
science,
a
number
of
world-wide
research
projects
depend
on
large-data
transfers.
For
wide-area
networks
(WANs).(e.g.,
Often,
dataprocessing),
must be gathered
(e.g.,
from disseminated.
remote
sites), processed, possibly
transferred
for the
further
and then
possibly
example,
a
community
of
over
8,000
physicists
across
more
than
40
countries
are
served
with
cessed,
possiblya transferred
for further
processing),
thenon
possibly
disseminated.
In science,
number of (e.g.,
world-wide
research
projects and
depend
large-data
transfers. For
advanced
particle
physics data,
at the scale
of petabytes
perdepend
year, generated
at thetransfers.
Large Hadron
In science,
a number
world-wide
research
projects
on countries
large-data
For
example,
a community
ofofover
8,000 physicists
across
more
than 40
are served with
Collider
(LHC)
at
CERN.
GridFTP
[3],
a
well-known
tool
for
transferring
files
across
WANs,
example,
community
ofdata,
over at
8,000
physicists
across more
thangenerated
40 countries
are
served
with
advanced aparticle
physics
the scale
of petabytes
per year,
at the
Large
Hadron
was
developed
as part
of the LHC
project.
The
Square per
Kilometregenerated
Array (SKA)
advanced
particle
at the
scalea of
petabytes
theradio
Largetelescope
Hadron
Collider (LHC)
atphysics
CERN.data,
GridFTP
[3],
well-known
toolyear,
for transferringatfiles
across
WANs,
project
will
generate
data
at
around
400
Tb/s,
which
needs
to
be
sent
to
a
supercomputer
centre
Collider
(LHC)asatpart
CERN.
GridFTP
[3], a well-known
for transferring
filesradio
acrosstelescope
WANs,
was developed
of the
LHC project.
The Square tool
Kilometre
Array (SKA)
was
developed
as partdata
of the
LHC project.
Thewhich
Square
Kilometre
Array
radio telescope
project
will generate
at around
400 Tb/s,
needs
to be sent
to a(SKA)
supercomputer
centre
project will generate data at around 400 Tb/s, which needs to be sent to a supercomputer centre1
1877-0509 © 2017 The Authors. Published by Elsevier B.V.
1
Peer-review under responsibility of the scientific committee of the International Conference on Computational Science
10.1016/j.procs.2017.05.211
1

	

HamidrezainAnvari
et al.
/ Procedia
The Impact of Large-Data Transfers
Shared
WANs:
An Computer
EmpiricalScience
Study108C (2017) 1702–1711
H. Anvari and P. Lu

1,000 km away for analysis [20]. Cloud computing sometimes requires dozens of gigabytes-toterabytes to be transferred between local and cloud storage. When allowed by service-level
agreements (SLA), and for short periods of time, it is useful to have a tool that can (unfairly
and) aggressively utilize the available bandwidth.
However, there are well-known challenges with using TCP/IP on WANs for transferring
large data files [6]. TCP is the backbone of the Internet because of its ability to share the
network among competing data streams. On a private, non-shared network, TCP can be
overly conservative and both GridFTP and UDT [8] parameterize the aggressiveness of claiming
bandwidth for a specific data transfer. Since the network is private, aggressiveness is not
necessarily a problem. On shared TCP networks, relatively little has been empirically quantified
about the performance of the standard tools and protocols for large-data transfers.
Therefore, we consider and evaluate the problem of large-data transfers on shared networks
and large round-trip-times (RTT), as are found on many WANs. Our previous study [5] used
a variety of synthetic background network traffic (e.g., uniform, TCP, UDP, square waveform,
bursty). A synthetic workload can be easier to describe, easier to automate, and easier to
reproduce than some other workloads. However, we wanted to improve upon the previous work
and consider an less synthetic, more application-based workload.
Specifically, we have used traffic based on the well-known Network File System (NFS) protocol instead of synthetic background traffic. There are no standard benchmarks or workloads
for background traffic, and NFS has the advantage of being familiar to many Unix-based researchers, and NFS is easy to set up without a lot of additional software or data files. Although
imperfect, NFS can be used as an application-based workload.
The remaining of this paper is structured as follows. In §2 we review the background of this
study and we introduce the related works and projects. In §3 our methodology and experiment
design is introduced. The empirical results are presented and discussed in §4. The paper is
concluded in §5.

2

Background and Related Work

An important goal of many network protocols, exemplified by TCP, is to provide reliable data
transfer over the network.1 This reliability is implemented as a service on top of the unreliable
physical network, usually as a functionality of the congestion control (CC) algorithm of TCP
protocol. One inherent challenge in designing CC algorithms, is to offer efficient bandwidth
utilization [2, 5]. In practice, the performance of classic network protocols is suboptimal in the
so-called high bandwidth-delay-product (BDP) networks, identified to have high capacity (i.e.,
bandwidth), and relatively high latency (i.e., delay), represented by round-trip-time (RTT) [23].
Moreover, a packet loss reinforces the low-utility problem in high-BDP networks.
In §2.1 we review a number of high-performance techniques and tools to overcome this
inefficiency in high-BDP networks.

2.1

High-performance Data Transfer Techniques

Two key strategies to overcome performance issues in high-BDP networks are through parallel
TCP channels and using UDP-based protocols.
1 There

are a number of use-cases where the data is carried over the unreliable network. These use-cases are
usually latency-sensitive; at the same time losing random data packets could be tolerated. Examples of such
use-cases are telephony and (real-time) multimedia streaming.

2

1703

HamidrezainAnvari
et al.
/ Procedia
The Impact of Large-Data Transfers
Shared
WANs:
An Computer
EmpiricalScience
Study108C (2017) 1702–1711
H. Anvari and P. Lu

Data Transfer Channel

Reliable Channel

...

Data Stream #2

Data Chunk

Sender

Data Chunk

Data Stream #1
Data Chunk

1704	

Data Stream #3

Receiver

Sender

Lossy UDP Channel

Tracking
Delivery

Receiver

Data Stream #4

Data Direction

(a) Parallel Data Streams

Data Direction

(b) UDP-based Data Transfer

Figure 1: High-Performance Data Transfer Techniques
Parallelism and Data Striping: One of the most popular techniques to optimize the data
transfer over WANs is to establish multiple connections in parallel, striping data over them
between two ends (Figure 1a). As a result, if some streams experience a decrease in the sending
rate due to packet loss, the overall rate will not experience a significant drop due to other
concurrent streams sending at high rates [10, 4]. This technique is well investigated and widely
applied on top of the TCP protocol. There are several research projects which provide parallel
protocol solutions for wide-area data transfer, ranging from proof-of-concept prototypes, to
full-fledged tools and frameworks. GridFTP is one of the most popular data transfer protocols
in this category [3]. Parallel Data Streams (PDS) is another tool that stripes data over either
cleartext (TCP) or encrypted (SSH) parallel connections [1].
UDP-based Protocols: In another approach, some reliable data-transfer protocols are designed based on unreliable UDP [19, 8, 11]. This group of protocols take advantage of UDP’s
high performance, where, unlike TCP, little or no processing overhead is imposed on this transport protocol. The reliability is then implemented as a user-level solution (Figure 1b). Compared to TCP-based tools, one of the strengths of this approach is the flexibility and wider range
of design options, which provides a platform for applying different ideas and heuristics toward
improving the data-transfer performance. The downside of this approach is that, first, both end
hosts must be configured by installing the user-level protocol stack prior to data transfer, and
second, unsophisticated design decisions may result in a resource-intensive piece of software in
terms of CPU utilization due to user-level procedures and task handlers.

2.2

Bandwidth Utilization Models in WANs

One important property of computer networks is the distinction between dedicated and shared
models in terms of traffic streams which utilize the link. In this section we briefly review the
common network utilization models.
Dedicated Networks. In this model, a private network is constructed between two or more
end-points. This approach is only feasible for a niche sector in research or industry. Examples of
this approach include the Google’s B4 private world-wide network [14], and Microsoft’s private
WAN resources [16].
Bandwidth Reservation. Another approach to establish dedicated bandwidth, is to employ
scheduling and bandwidth reservation techniques as means to provide dedicated bandwidth over
a common network path. For example, the On-demand Secure Circuits and Advance Reservation System (OSCARS) is one technique for bandwidth reservation [9]. An example of the
state-of-the-art research in this category is the On-demand Overlay Network. This framework
3

	

The Impact of Large-Data Transfers
Shared
WANs:
An Computer
EmpiricalScience
Study108C (2017) 1702–1711
H. Anvari and P. Lu
HamidrezainAnvari
et al.
/ Procedia

provides a dedicated bandwidth using the bandwidth reservation techniques [18]. In this model,
aggressiveness is a desired property for the network protocols for efficient utilization of the reserved (dedicated) bandwidth.
Shared Bandwidth. In contrast to dedicated bandwidth, bandwidth-sharing networks are
still the common practice for a large number of research and industry users. Two common challenges are attributed to the shared WANs. First, network resources are being shared between
several users at a time, which results in a dynamic behaviour and workload on the network. One
studied consequence of this dynamic behaviour is the emergence of periodic burstiness of the
traffic over the network [12, 15]. This burstiness may result in various levels of contention for
the network resources, which could lead to an increased packet loss rate and therefore decreased
bandwidth utilization. Second, in the networks with shared bandwidth, the criteria changes
for an ideal data-transfer tool. In this case, aggressiveness is not necessarily considered a good
quality and there are some implications which have to be taken into account. The protocols
should provide a good trade-off between efficiency and being fair to other traffic (i.e, background
traffic).
Fairness, in a high level of abstraction, is defined as a protocol’s tendency and commitment
toward utilizing an equal share of bandwidth as other concurrent traffic streams. There has been
several proposals for quantifying the fairness, two of popular definitions are Jain fairness [13]
and max-min fairness [17].
While a significant part of literature concerns the performance optimization of data communication over dedicated networks, there are a relatively small number of research projects in
which, directly or indirectly, the network environment is considered to be shared. One example
of such research projects is the study of distributed systems in the context of shared network
environments [21]. This work argues the importance of considering the background traffic for
evaluation of the distributed systems which are deployed on top of shared network.
Another area of research which has considered the shared nature of networks is the research
of estimating available bandwidth, where the notion of cross traffic (e.g., a mix of foreground
and background traffic) has been investigated [22].

3

Experimental Design and Methodology

In this section we introduce our methodology for investigating performance impacts of cross
traffic in the context of shared high-BDP networks. Our methodology consists of the set up
and configuration of the testbed as well as designing test scenarios for conducting background
traffic across the network.

3.1

Testbed Configuration

While the ideal environment for our network experiments is a real wide-area network, it does not
offer enough flexibility for control and tuning of different network parameters and to study their
effect. As a trade-off, we have designed and set up an emulated high-BDP testbed, implemented
on a research computing cluster at the University of Alberta.
Our high-BDP testbed employs a dumbbell topology, consisting of 8 end-nodes, grouped
into 2 virtual local-area networks (LAN), and 2 machines as router nodes. The virtual routers
are configured to relay traffic between the two LANs, while applying the desired configuration
for bandwidth and delay to the network. The schematic design of the testbed is illustrated in
Figure 2a. Dummynet [7], a well-known open-source network emulation software tool, is used
4

1705

1706	

The Impact of Large-Data Transfers
Shared
WANs:
An Computer
EmpiricalScience
Study108C (2017) 1702–1711
H. Anvari and P. Lu
HamidrezainAnvari
et al.
/ Procedia

S1

S2

S3

D1

1G
b/
s

s
b/
1G

1Gb
/s

1Gb/s

R1

1Gb

WAN

R2

500Mb/s

/s

S4

S1, D1 AMD Opt. 6134 / 8 / 2.30 32 GB
S2, D2 Intel Ci3-6100U / 4 / 2.30 32 GB

1Gb/s
1G
b/
s

s
b/
1G

D2

Node(s) CPU (Model/Cores/Freq.) RAM

D3

D4

S3, D3 Intel Ci3-6100U / 4 / 2.30 32 GB
S4, D4 AMD E2-1800 / 2 / 1.70

8 GB

R1, R2 AMD A8-5545M / 4 / 1.7

8 GB

(a) Dumbbell Network Topology

(b) Nodes Configuration

Figure 2: Testbed Architecture
on nodes R1 and R2 for implementing the router functionality. Our configuration mimics a
high-BDP network between the two LANs.
The testbed is utilized in shared mode, transferring various traffic patterns between pairs of
Si and Di respectively. To more extend the flexibility of our testbed, we have deployed three
groups of end-nodes: 1 pair of older commodity client machines, 1 pair of more-powerful server
nodes with AMD Opteron CPUs, and 2 pairs of higher-end client machines. The configuration
of the hardware nodes in the testbed are provided in Table 2b. The nodes run Linux CentOS
6 (kernel 2.6.32), all equipped with Gigabit Ethernet NICs.
In this study, all network links (except the intended bottleneck) are rated at 1 GB/s bandwidth. we have set the bottleneck bandwidth between R1 and R2 to 500 Mb/s, and the
end-to-end delay (RTT) to 128 milliseconds between the two networks. The router queuing
buffer size at R1 and R2 are set to 6 MB.
We transfer 14 GB worth of data for our large-data transfer task. All data transfers are
from memory to memory, to avoid disk I/O as a possible bottleneck. All reported results are
the average over 3 runs, error bars representing the standard deviation from the average.

3.2

Background Traffic

Synthetic Traffic. One important requirement for studying shared networks is the ability
to generate representative traffic patterns over the network with reasonable flexibility. For
this study, we use the traffic generator script, which invokes other tools, that we developed
in our previous work [5]. This traffic generator is capable of creating traffic with adjustable
burstiness at different time scales. It can be set to create constant or square-wave traffic
patterns, using either TCP or UDP transport channels. To study the effect of traffic burstiness
on performance, our tool is parameterized to generate bursty UDP-based traffic, with userprovided time arguments. The features of our traffic generator are summarized in Figure 3.
Application Traffic. While synthetic traffic could represent a wide range of real-world traffic patterns, we further extend our experiment scenarios to conduct real application traffic
patterns. We employ NFS (Network File System) for generating the patterns of application
traffic. We use cp, filesystem copy, to generate network traffic between NFS client and server.
This configuration enables us to study 1) the effect of NFS as background traffic over the data
transfer tasks, and 2) the impact of data-transfer tools on NFS performance.
2 http://software.es.net/iperf/

5

The Impact of Large-Data Transfers
Shared
WANs:
An Computer
EmpiricalScience
Study108C (2017) 1702–1711
H. Anvari and P. Lu
HamidrezainAnvari
et al.
/ Procedia

throughput

	

Background Traffic
BG-TCP-Const.
BG-SQ-UDP1
BG-SQ-UDP2
BG-SQ-TCP
Bursty UDP(x,y)

x
500 Mbps

y

x / y (seconds)
10 / 0
10 / 10
10 /10
10 / 10
x / y (user-input)

Traffic Generator
iperf-TCP2
iperf-UDP
RBUDP
iperf-TCP
iperf-UDP

Time

(a) Generated traffic pattern

(b) Configuration settings for generated traffic

Figure 3: Characterization of Synthetic Background Traffic (extends Anvari and Lu, 2016 [5])
While using UDP transport channel is an option for mounting to the NFS server, the early
results for NFS over UDP were consistently unstable. So for all following test scenarios we use
NFS over TCP transport channel.

4

Empirical Evaluation and Discussion

4.1

Impact of Cross Traffic on High-Performance Tools

To investigate the behaviour of the high-performance data-transfer tools in the presence of
background traffic, we study the performance of two high-performance data-transfer tools,
GridFTP and UDT, running along different patterns of TCP and UDP cross traffic. The
performance numbers for transferring data over a single TCP connection (iperf) is provided
as the baseline. For GridFTP, we conduct the data-transfer tasks in two configurations with 8
and 16 parallel connections, denoted by GridF T P − 8 and GridF T P − 16 respectively.
In the first scenario (Figure 4a), we transfer 14 GB worth of data between S1 and D1 (Figure 2a), while using our synthetic traffic generator to run background traffic between S2 and D2,
travelling the same bottleneck.3 The background traffic patterns include constant TCP stream,
square-wave TCP and UDP streams, and square-wave RBUDP stream (a high-performance
UDP-based tool [11]). In all cases the background traffic is a single stream, periodically cycling
ON and OFF in square-wave mode. The performance in absence of background traffic (No-BG)
is also provided as a performance baseline. Both GridFTP and UDT present a robust performance in presence of TCP-based background traffic, comparable to the baseline. However,
running along the UDP-based traffic patterns, they start to experience significant performance
degradation. UDT, as compared to GridFTP, claims a higher throughput in the presence of
such background traffic.
In the second scenario (Figure 4b), we use application traffic generated using NFS. The
nodes D2, D3, and D4 run NFS service, while S1, S2, and S3 mount to the service as clients.
We deploy several configurations for this scenario with a varying number of NFS mount points
(between 1 and 16). To Generate traffic over the networks, we copy 1 GB worth of data form the
local memory to the NFS mounted folder, consequently conducting the traffic over the network.
This workload could represent a file-backup workload, file transfer between remote user-folders,
or other use-cases of NFS service. Considering the space limits, we only present the results
for the two cases of 2 (i.e., NFS(2)) and 16 (i.e., NFS(16)) concurrent NFS data transfer tasks
over the network. The GridFTP and UDT tend to aggressively contend for the bandwidth, and
3 Updated

6

empirical evaluation using newer version of DummyNet (ipfw3-2012) and iperf (2.0.5) [5]

1707

1708	

The Impact of Large-Data Transfers
Shared
WANs:
An Computer
EmpiricalScience
Study108C (2017) 1702–1711
H. Anvari and P. Lu
HamidrezainAnvari
et al.
/ Procedia

(a) Effect of Square Pattern Background Traffic
(Updated from Anvari and Lu, 2016 [5])

(b) Effect of Various Background Traffic Patterns:
NFS and Bursty UDP(x,y)

Figure 4: Effect of Sharing Bandwidth over High-BDP Network.
are not experiencing performance loss. We revisit this scenario in §4.2 to study the impact of
large-data transfers on NFS performance.
In last scenario4 , to further investigate the performance inefficiencies in presence of bursty
UDP traffic, we expose the same data transfer task to more dynamic UDP traffic patterns. To
generate this background traffic we use our traffic generator in parameterized mode, denoted
by UDP(x, y), with several configurations. The results are depicted in Figure 4b. We have
included three configurations of UDP(1,2), UDP(1,5), and UDP(1,10), where the UDP-based
traffic cycles ON every 2, 5, and 10 seconds respectively, transferring a 1 second burst of UDP
traffic before cycling OFF. This scenario further uncovers the sensitivity of GridFTP traffic to
the bursty UDP cross traffic. To summarize our observations, the observed performance drop
is the typical behaviour of multi-stream TCP-based protocols in the presence of UDP traffic
bursts, 5 while UDT proves to be more robust in this scenario, which makes it a better option
for the shared networks with considerable UDP-based traffic over the network.

4.2

NFS Performance over Shared Network

In this section we investigate the effect of large-data transfers on the performance of NFS
client-server communication. Our approach is identical to the second test scenario in §4.1,
but rather we consider the NFS as main traffic. Considering the space limit, we only discuss
the configuration case of 16 concurrent NFS connections, leaving out the other configurations,
which mostly represent the same behaviour.
In our test scenario, a total of 16 NFS connections are established between 3 pairs of nodes
(S2, D2), (S3, D3), (S4, D4), iteratively transferring 1 GB files over the network (copying file
to NFS mount points). The baseline, copying a local 1 GB file to a mounted NFS folder takes
about 20 seconds (at 410 Mb/s) in our high-BDP network, when no other traffic runs over the
bandwidth. We will study how this performance is affected in the bandwidth-sharing scenario.
We conduct a series of large-data transfer between S1 and D1 over the same high-BDP
path, consecutively transferring 14 GB data using iperf-TCP, GridFTP-8, GridFTP-16, and
UDT. As in previous test scenarios, we run the same set of tests for 3 times; Here, rather than
4 This

scenario is taken from our previous work [5]
traffic constitutes a non-trivial share of Internet traffic in terms of telephony, media streaming,
teleconferencing, and more.
5 UDP

7

The Impact of Large-Data Transfers
Shared
WANs:
An Computer
Empirical
Study108C (2017) 1702–1711
H. Anvari and P. Lu
Hamidrezain
Anvari
et al.
/ Procedia
Science
35
400

30

300

Throughput (Mbps)

Throughput (Mbps)

350

25

20

15

250
200
150
100

10
50

5
3

5

8

10

13

15

18

0

20

3

5

8

Iteration (Increasing time order)
S2-D2 (6 streams)

S3-D3 (6 streams)

10

13

15

18

20

Iteration (Increasing time order)

S4-D4 (4 streams)

S2-D2 (6 streams)

(a) Per-Node Pair Average NFS Throughput
RTT

S3-D3 (6 streams)

S4-D4 (4 streams)

Total (16 streams)

(b) Per-Node Pair Aggregate NFS Throughput

RTT Moving Average (Window=5)

250

RTT (milliseconds)

	

200

150

100
0

25

50

75

100

125

150

175

200

225

250

275

300

(c) RTT Variation (via ping) over Time, Due to Congestion

Figure 5: Effect of Foreground Traffic on NFS(16) Throughput and Ping Latency (RTT)
reporting the average, we provide the numbers for each run separately as it provides a better
intuition.
The results are presented in Figure 5. For better readability, the results are aggregated per
hosts. Figure 5a reports the average throughput of NFS data transfers between each pair of
nodes. The x-axis represents the iterations of data transfer over time. The average performance
of each NFS stream has dropped to approximately 29 Mb/s in best case, and to less than 10
Mb/s in the worst case. The 3 repeating pattern in the graph match the 3 runs of transferring
large data across the network. 3 Peaks correspond to the 3 transfers of 14 GB data using
iperf (the single TCP stream), and the 3 troughs correspond to the extreme case of transferring
14 GB data using GridFTP-16. The total accumulated throughput of NFS streams are reported
in Figure 5b. The orange line (the highest curve) represents the overall bandwidth utilization
of 16 NFS connections. One caveat on utilizing NFS in our experiments is that the NFS is not
designed to be used over high-BDP networks; so it might not be efficient and optimized for such
a testbed. However, its communication pattern could be representative of various TCP-based
application traffic over the network, with the same impacts from the large-data transfer tasks.
To summarize this test scenario, we observe that the GridFTP and UDT could impose
considerable performance degradation on standard network traffic, which could hurt the notion
of fairness over the shared networks. 6
6 The

8

formal investigation of the fairness metrics is beyond the scope of our empirical study.

1709

1710	

The Impact of Large-Data Transfers
Shared
WANs:
An Computer
Empirical
Study108C (2017) 1702–1711
H. Anvari and P. Lu
Hamidrezain
Anvari
et al.
/ Procedia
Science

Network Congestion Impact on Latency. As part of our experiments, we have quantified
the effect of network congestion on the latency, using the standard ping tool. Figure 5c depicts
the variation in end-to-end RTT on our testbed over a 5-minute time window. While the static
RTT on the network is 128 ms, the observed latency experience increases of up to 100ms in
the event of congestion in network. This increased RTT could further have a negative impact
the network utilization. The study of the RTT dynamics in shared networks and its impacts
on performance metrics constitutes an interesting direction for extending this empirical study.

5

Concluding Remarks and Future Work

We extend our previous study on GridFTP and UDT, over shared networks [5]. Not surprisingly, an NFS-over-TCP workload as background traffic has a moderate impact on GridFTP
and UDT foreground traffic. GridFTP and UDT are both aggressive in capturing bandwidth
(Figure 4(b)). However, NFS throughput varies from 9 Mb/s to 29 Mb/s (for a single stream)
when it competes with the cycle of iPerf-GridFTP-UDT as cross traffic (Figure 5a). At the
same time, due to congestion, the network latency (via ping) varies from 130 to 230 ms which
implies a significant variation in observed performance for traffic streams.
Our results suggest that in many cases the performance of high-performance tools are degraded across shared networks. At the same time the aggressive approach of these tools could
negatively impact the other traffic sharing the same network path.
For the future work, we have planned to move to higher speed networks, at 10 Gb/s and
40 Gb/s, where we speculate the relative performance of the protocols will remain unchanged.
In next step, we will generate more real-world traffic patterns to study the dynamic and characteristics of different distributed platforms in presence of high-performance data connections.
These new traffic pattern may include the bursty UDP-based traffic as are found in real-time
streaming and telephony use-cases. Lastly, one promising research avenue will be the characterization of the protocols behaviour in shared high-BDP networks.

References
[1] Parallel Data Streams (PDS). https://github.com/paullu-ualberta/paralleldatastream.
[2] A. Afanasyev, N. Tilley, P. Reiher, and L. Kleinrock. Host-to-host congestion control for tcp.
Communications Surveys Tutorials, IEEE, 12(3):304–342, Third 2010.
[3] W. Allcock, J. Bresnahan, R. Kettimuthu, M. Link, C. Dumitrescu, I. Raicu, and I. Foster. The
globus striped gridftp framework and server. In Proceedings of the 2005 ACM/IEEE Conference
on Supercomputing, SC ’05, pages 54–, Washington, DC, USA, 2005. IEEE Computer Society.
[4] E. Altman, D. Barman, B. Tuffin, and M. Vojnovic. Parallel tcp sockets: Simple model, throughput
and validation. In INFOCOM 2006. 25th IEEE International Conference on Computer Communications. Proceedings, pages 1–12, April 2006.
[5] H. Anvari and P. Lu. Large transfers for data analytics on shared wide-area networks. In Proceedings of the ACM International Conference on Computing Frontiers, CF ’16, pages 418–423, New
York, NY, USA, 2016. ACM.
[6] C. Barakat, E. Altman, and W. Dabbous. On tcp performance in a heterogeneous network: A
survey. Comm. Mag., 38(1):40–46, January 2000.
[7] M. Carbone and L. Rizzo. Dummynet revisited. SIGCOMM Comput. Commun. Rev., 40(2):12–20,
April 2010.
[8] Y. Gu and R. L. Grossman. Udt: Udp-based data transfer for high-speed wide area networks.
Computer Networks, 51(7):1777 – 1799, 2007.

9

	

The Impact of Large-Data Transfers
Shared
WANs:
An Empirical
Study 108C (2017) 1702–1711
H. Anvari and P. Lu
HamidrezainAnvari
et al.
/ Procedia
Computer Science

[9] C. Guok, D. Robertson, M. Thompson, J. Lee, B. Tierney, and W. Johnston. Intra and interdomain
circuit provisioning using the oscars reservation system. In Broadband Communications, Networks
and Systems, 2006. BROADNETS 2006. 3rd International Conference on, pages 1–8, Oct 2006.
[10] T. J. Hacker, B. D. Athey, and B. Noble. The end-to-end performance effects of parallel tcp sockets
on a lossy wide-area network. In Proceedings of the 16th International Parallel and Distributed
Processing Symposium, IPDPS ’02, pages 314–, Washington, DC, USA, 2002. IEEE Computer
Society.
[11] E. He, J. Leigh, O. Yu, and T. A. DeFanti. Reliable blast udp: Predictable high performance
bulk data transfer. In Proceedings of the IEEE International Conference on Cluster Computing,
CLUSTER ’02, pages 317–, Washington, DC, USA, 2002. IEEE Computer Society.
[12] C. Hong, S. Kandula, R. Mahajan, M. Zhang, V. Gill, M. Nanduri, and R. Wattenhofer. Achieving
high utilization with software-driven wan. In Proceedings of the ACM SIGCOMM 2013 Conference
on SIGCOMM, SIGCOMM ’13, pages 15–26, New York, NY, USA, 2013. ACM.
[13] R. Jain, D. Chiu, and W. R. Hawe. A quantitative measure of fairness and discrimination for
resource allocation in shared computer system, volume 38. Eastern Research Laboratory, Digital
Equipment Corporation Hudson, MA, 1984.
[14] S. Jain, A. Kumar, S. Mandal, J. Ong, L. Poutievski, A. Singh, S. Venkata, J. Wanderer, J. Zhou,
M. Zhu, J. Zolla, U. Hölzle, S. Stuart, and A. Vahdat. B4: Experience with a globally-deployed
software defined wan. In Proceedings of the ACM SIGCOMM 2013 Conference on SIGCOMM,
SIGCOMM ’13, pages 3–14, New York, NY, USA, 2013. ACM.
[15] H. Jiang and C. Dovrolis. Why is the internet traffic bursty in short time scales? In Proceedings of the 2005 ACM SIGMETRICS International Conference on Measurement and Modeling of
Computer Systems, SIGMETRICS ’05, pages 241–252, New York, NY, USA, 2005. ACM.
[16] H. Harry Liu, R. Viswanathan, M. Calder, A. Akella, R. Mahajan, J. Padhye, and M. Zhang.
Efficiently delivering online services over integrated infrastructure. In 13th USENIX Symposium
on Networked Systems Design and Implementation (NSDI 16), pages 77–90, Santa Clara, CA,
2016. USENIX Association.
[17] J. Mo and J. Walrand. Fair end-to-end window-based congestion control. IEEE/ACM Trans.
Netw., 8(5):556–567, October 2000.
[18] L. Ramakrishnan, C. Guok, K. Jackson, E. Kissel, D. M. Swany, and D. Agarwal. On-demand
overlay networks for large scientific data transfers. In Proceedings of the 2010 10th IEEE/ACM
International Conference on Cluster, Cloud and Grid Computing, CCGRID ’10, pages 359–367,
Washington, DC, USA, 2010. IEEE Computer Society.
[19] J. Roskind. Quic: Multiplexed stream transport over udp. Google working design document, 2013.
[20] B. Tierney, E. Kissel, M. Swany, and E. Pouyoul. Efficient data transfer protocols for big data. In
E-Science, 2012 IEEE 8th International Conference on, pages 1–9. IEEE, 2012.
[21] K. V. Vishwanath and A. Vahdat. Evaluating distributed systems: Does background traffic matter?
In USENIX 2008 Annual Technical Conference, ATC’08, pages 227–240, Berkeley, CA, USA, 2008.
USENIX Association.
[22] Q. Yin and J. Kaur. Can Machine Learning Benefit Bandwidth Estimation at Ultra-high Speeds?,
pages 397–411. Springer International Publishing, Cham, 2016.
[23] S. Yu, N. Brownlee, and A. Mahanti. Comparative performance analysis of high-speed transfer
protocols for big data. In Local Computer Networks (LCN), 2013 IEEE 38th Conference on, pages
292–295, Oct 2013.

10

1711

