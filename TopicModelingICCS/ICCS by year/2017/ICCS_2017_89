Available online at www.sciencedirect.com

ScienceDirect
Procedia Computer Science 108C (2017) 394–403

International Conference on Computational Science, ICCS 2017, 12-14 June 2017,
Switzerland
DiscriminativeZurich,
Learning
from Selective

Discriminative
from
Recommendation
andLearning
Its Application
in AdaBoost
Discriminative
Learning
from Selective
Selective
Recommendation
and
Its
Application in
AdaBoost
Recommendation
and
Its
in
AdaBoost
1
1* Application
2†
Xiao-Yu Zhang , Shupeng Wang , Chao Li , Shiming Ge1†, Yong Wang1,

1 2†
1*
1†
1
and Binbin
Xiao-Yu Zhang11, Shupeng Wang
1*, ChaoLiLi2†, Shiming Ge1†, Yong Wang1,
Xiao-Yu
Zhang
,
Shupeng
Wang
,
Chao
Li
,
Shiming
Ge
,
Yong
Wang
,
1
1
Institute of Information Engineering,
ChineseLiAcademy
of Sciences, Beijing, China
and
Binbin
1
2
andResponse
Binbin
LiAcademyTeam/Coordination
National
Computer
Network Emergency
Technical
Center
of China,
1
Institute
of Information
Engineering,
Chinese
of Sciences, Beijing,
China
2
2

1
Institute
of Information
Engineering,
Chinese
Academy
of Sciences, Beijing,
China
Beijing,
China
National
Computer
Network Emergency
Response
Technical
Team/Coordination
Center
of China,
National
Computer
Network Emergency
Response
Technical
Team/Coordination
Center
of China,
{zhangxiaoyu,
wangshupeng,
geshiming,
wangyong,
libinbin}@iie.ac.cn,
lichao@cert.org.cn
Beijing, China
Beijing,
China
{zhangxiaoyu, wangshupeng, geshiming, wangyong, libinbin}@iie.ac.cn, lichao@cert.org.cn
{zhangxiaoyu, wangshupeng, geshiming, wangyong, libinbin}@iie.ac.cn, lichao@cert.org.cn

Abstract
The
integration of semi-supervised learning and ensemble learning has been a promising research area.
Abstract
It
is
a typical ofprocedure
that one
learnerandrecommends
the pseudo-labeled
instances
with area.
high
Abstract
The integration
semi-supervised
learning
ensemble learning
has been a promising
research
The
integration
of
semi-supervised
learning
and
ensemble
learning
has
been
a
promising
research
area.
predictive
confidence
to
another,
so
that
the
training
dataset
is
expanded.
However,
the
new
learner’s
It is a typical procedure that one learner recommends the pseudo-labeled instances with high
demand
onconfidence
recommendation
as one
well
as the
the training
possibility
of incorrect
recommendation
neglected,
It
is a typical
procedure
that
learner
recommends
the
pseudo-labeled
instances
high
predictive
to another,
so that
dataset
is expanded.
However,
theare
newwith
learner’s
which
inevitably
jeopardize
the
learning
performance.
To
address
these
issues,
this
paper
proposes
the
predictive
confidence
to
another,
so
that
the
training
dataset
is
expanded.
However,
the
new
learner’s
demand on recommendation as well as the possibility of incorrect recommendation are neglected,
demand
on
recommendation
as
well
as
the
possibility
of
incorrect
recommendation
are
neglected,
Discriminative
Learning
from
Selective
Recommendation
(DLSR)
method.
On
one
hand,
both
which inevitably jeopardize the learning performance. To address these issues, this paper proposes the
which
inevitably
jeopardizefrom
the learning
performance.
To
address
these
issues,
proposes
the
reliability
and informativeness
of
the pseudo-labeled
instances
are
taken
intothis
account
selective
Discriminative
Learning
Selective
Recommendation
(DLSR)
method.
Onpaper
onevia
hand,
both
Discriminative
Learning
from
Selective
Recommendation
(DLSR)
method.
On
one
hand,
both
recommendation.
On
the
other
hand,
the
potential
in
both
correct
and
incorrect
recommendation
are
reliability and informativeness of the pseudo-labeled instances are taken into account via selective
reliability
and
of thethe
pseudo-labeled
instances
areand
taken
into account
via selective
formulated
in informativeness
discriminative
Based
on inDLSR,
we further
propose
the selective
semirecommendation.
On the otherlearning.
hand,
potential
both
correct
incorrect
recommendation
are
supervised
AdaBoost.
With
both
recommending
and
receiving
learners
engaged
in
ensemble
model
recommendation.
On
the
other
hand,
the
potential
in
both
correct
and
incorrect
recommendation
are
formulated in discriminative learning. Based on DLSR, we further propose the selective semilearning,
the
unlabeled
instances
are
explored
in
a
more
effective
way.
formulated
in
discriminative
learning.
Based
on
DLSR,
we
further
propose
the
selective
semisupervised AdaBoost. With both recommending and receiving learners engaged in ensemble model
supervised
AdaBoost.
With
bothare
recommending
engaged in ensemble model
learning,
the
unlabeled
instances
explored
in a and
morereceiving
effective learners
way.
©
2017 The
Authors.
Published
by Elsevier
B.V. learning,
Keywords:
semi-supervised
learning,
ensemble
selective
recommendation,
discriminative learning,
learning,
the
unlabeled
instances
are
explored
in
a
more
effective
way.
Peer-review under responsibility of the scientific committee of the International Conference on Computational Science
AdaBoost
Keywords: semi-supervised learning, ensemble learning, selective recommendation, discriminative learning,
Keywords:
AdaBoost semi-supervised learning, ensemble learning, selective recommendation, discriminative learning,
AdaBoost

1 Introduction
1
Introduction
the past decades, machine learning has been thriving as an effective way to explore the world
1 Over
Introduction

by Over
extracting
patterns
in data
and learning
making has
predictions
basedason
experience
these
the past
decades,
machine
been thriving
an the
effective
way togained
explorefrom
the world
Over
the
past
decades,
machine
learning
has
been
thriving
as
an
effective
way
to
explore
the
world
patterns
[1]-[4].
Traditionally,
machine
learning
tasks
are
either
supervised
or
unsupervised.
The
by extracting patterns in data and making predictions based on the experience gained from these
former
aims
at
inferring
a
function
from
labeled
instances
which
can
generalize
to
unlabeled
instances,
by
extracting
patterns
in
data
and
making
predictions
based
on
the
experience
gained
from
these
patterns [1]-[4]. Traditionally, machine learning tasks are either supervised or unsupervised. The
whereas
the
latter
tries toa reveal
hidden
from
instances
based
some measure
of
patternsaims
[1]-[4].
Traditionally,
machine
learning
tasksunlabeled
are
either
supervised
oron
unsupervised.
The
former
at inferring
function
fromstructures
labeled
instances
which
can
generalize
to
unlabeled
instances,
former
aims
at
inferring
a
function
from
labeled
instances
which
can
generalize
to
unlabeled
instances,
whereas
the latter tries to reveal hidden structures from unlabeled instances based on some measure of
*
S. Wang
the joint
firsttoauthor
with
X.-Y. Zhang.
whereas
the islatter
tries
reveal
hidden
structures from unlabeled instances based on some measure of
†
C. Li and S. Ge are the corresponding authors.
*
S. Wang is the joint first author with X.-Y. Zhang.
†*
†

S.
firstcorresponding
author with X.-Y.
Zhang.
C. Wang
Li andisS.the
Gejoint
are the
authors.
C. Li and S. Ge are the corresponding authors.

1877-0509 © 2017 The Authors. Published by Elsevier B.V.
Peer-review under responsibility of the scientific committee of the International Conference on Computational Science
10.1016/j.procs.2017.05.080

	

Xiao-Yu Zhang et al. / Procedia Computer Science 108C (2017) 394–403

inherent similarity or distance. In real-world applications, however, labeled instances are often
difficult, expensive, or time consuming to obtain, as they require the efforts of human annotators with
specific domain experience. Meanwhile, unlabeled instances are relatively easy to collect. As a result,
the typical dataset available consists of a small amount of labeled instances and a large amount of
unlabeled instances. Semi-supervised learning addresses this bottleneck by leveraging both labeled
and unlabeled instances to improve learning performance, and has been studied widely both in theory
and in practice [5]-[7].
Apart from incorporating more instances, higher learning performance can also be achieved by
creating more learners. This type of solution is referred to as ensemble method, which trains multiple
learners to solve the same problem [8][9]. It is well known that an ensemble is usually significantly
more accurate than any single learner. One of the most influential ensemble methods is the AdaBoost
algorithm, which converts multiple weak learners to a strong one [10]. AdaBoost is adaptive in the
sense that subsequent weak learners are tweaked in favor of the instances misclassified by previous
leaners. In AdaBoost, the individual leaners can be weak, but as long as the performance of each one
is slightly better than random guessing, the final model can be proven to converge to a strong learner.
Because of this attractive property, AdaBoost has been successfully applied to various real-world
applications.
Traditionally, AdaBoost falls into the supervised learning category. It is a natural consideration to
extend AdaBoost to semi-supervised scenario. Several algorithms have been proposed in this direction,
including ASSEMBLE [11], MarginBoost [12], SemiBoost [13], RegBoost [14], etc. In these
algorithms, unlabeled instances with high predictive confidence are recommended as the pseudolabeled ones between the weak learners. Using this additional information as guidance, the weak
leaners can be further updated to obtain higher performance. However, as will be indicated in the
following text, the most confident pseudo-labeled instances from a given learner are not necessarily
demanded most by the new learner. Furthermore, for the sake of predictive accuracy, the learner
receiving recommendation should treat the pseudo-labeled instances discriminatively according to
their potential values in case of both correct and incorrect prediction, instead of accepting them
unreservedly and equally.
To address the aforementioned issues, we propose in this paper a novel Discriminative Learning
from Selective Recommendation (DLSR) method. As the name indicates, DLSR consists of two stages.
In the pseudo-labeled instance recommendation stage, we incorporate both reliability and
informativeness into a comprehensive measurement to select the most valuable recommendation
candidate. In the learner re-training stage, both the potential benefit in accepting the pseudo-labels and
the probability of denying it are jointly studied to formulate a discriminative way of learning. Then,
we apply DLSR to extend AdaBoost and propose the Selective Semi-supervised AdaBoost (SSAdaBoost) algorithm. In this way, the pseudo-labeled instances are placed under stricter inspections
from not only the learners initiating recommendation but also those receiving recommendation.
Encouraging results are received from experiments on real-world classification tasks.

2 Discriminative Learning from Selective Recommendation
This paper focuses on the classification problem in machine learning, which identifies the category
membership (label) of an observation (instance). In traditional supervised learning, the learner is
trained by passively accepting labeled instances from the oracle which is usually human expert. In
active learning, a learner can actively select several instances to query for their labels, thereby
achieving higher learning performance with limited number of training instances [15][16]. The family
of semi-supervised ensemble learning methods, collectively known as collaborative learning, assigns
pseudo-labels to unlabeled instances based on the predictions of an existing learner, and then trains a
new learner using the pseudo-labeled instances as additional training data. By leveraging both labeled

395

Xiao-Yu Zhang et al. / Procedia Computer Science 108C (2017) 394–403

396	

(1)

(2)

(3)
(4)
Figure 1: Comparison of learning processes in (1) passive learning, (2) active learning, (3) collaborative
learning, and (4) DLSR.

and unlabeled instances, collaborative learning has shown its advantage in various applications. Both
passive learning and collaborative learning share the similar learning process where the learner
receives training data in a passive way, except that the former learns from an oracle whereas the latter
from a different learner. In order to further improve the performance of learning from recommendation,
we present a selective recommendation method to actively select pseudo-labeled instances for training.
Different from active learning that mainly focuses on the informativeness of instances, we also
incorporate the reliability of pseudo-labels that reflects the recommending learner’s predictive
confidence. Moreover, in the model training stage, a discriminative learning method is presented by
assigning weights to the pseudo-labeled instances based on their potential in model improvement.
Comparison of the proposed Discriminative Learning from Selective Recommendation (DLSR)
method with passive learning, active learning, and collaborative learning is illustrated in Figure 1.
In the text that follows, we let 𝒙𝒙 ∈ 𝒳𝒳 denote the input feature of an instance, and 𝑦𝑦 ∈ 𝒴𝒴 =
{1, … , 𝐾𝐾} denote the label indicating the class that an instance falls into, where 𝐾𝐾 is the number of
classes. 𝑈𝑈 and 𝐿𝐿 stand for the unlabeled and labeled dataset, respectively. Probabilistic model is used
(𝑛𝑛)
for classification based on the posterior distribution 𝑃𝑃(𝑦𝑦|𝒙𝒙; 𝜃𝜃𝐿𝐿 ) of label 𝑦𝑦 conditioned on the input 𝒙𝒙,
(𝑛𝑛)

where 𝜃𝜃𝐿𝐿

is model parameter of the 𝑛𝑛-th learner optimized for the corresponding labeled dataset 𝐿𝐿.

2.1 Selective Recommendation

Typically, methods of learning from recommendation start with training a learner ℎ𝑛𝑛 : 𝒳𝒳 → 𝒴𝒴 using
the labeled dataset 𝐿𝐿. After that, predictions can be made w.r.t. each unlabeled instance 𝒙𝒙 ∈ 𝑈𝑈:
with predictive confidence:

(𝑛𝑛)

𝑦𝑦 ∗ = arg max 𝑃𝑃(𝑦𝑦|𝒙𝒙; 𝜃𝜃𝐿𝐿 )
𝑦𝑦∈𝒴𝒴

(𝑛𝑛)

𝒞𝒞𝑛𝑛 (𝒙𝒙, 𝑦𝑦 ∗ ) = 𝑃𝑃(𝑦𝑦 ∗ |𝒙𝒙; 𝜃𝜃𝐿𝐿 )

(1)

(2)

When recommending the pseudo-labeled instances to the next learner ℎ𝑛𝑛+1 , those with high
confidence is preferable from the ℎ𝑛𝑛 point of view.
However, the high-confident pseudo-labeled instances may provide little information, since they
are not necessarily needed most from the ℎ𝑛𝑛+1 point of view. Following the error reduction criterion
for decision-theoretic selective sampling, the informativeness of an unlabeled instance is negatively
correlated with the entropy over the rest unlabeled ones. As a result, when accepting (𝒙𝒙, 𝑦𝑦 ∗ ) as
recommendation, its value in model update can be formulated as:

	

Xiao-Yu Zhang et al. / Procedia Computer Science 108C (2017) 394–403

397

Algorithm 1: Discriminative Learning from Selective Recommendation (DLSR)
Input: labeled dataset 𝐿𝐿 ⊂ (𝒳𝒳, 𝒴𝒴);
unlabeled dataset 𝑈𝑈 ⊂ 𝒳𝒳;
number of instances for recommendation 𝑀𝑀;
leaner ℎ𝑛𝑛 ~𝜃𝜃 (𝑛𝑛) : 𝒳𝒳 → 𝒴𝒴;
(𝑛𝑛+1)
: 𝒳𝒳 → 𝒴𝒴 trained on 𝐿𝐿.
learner ℎ𝑛𝑛+1 ~𝜃𝜃𝐿𝐿
Output: updated leaner ℎ𝑛𝑛+1 ~𝜃𝜃 (𝑛𝑛+1) .
// Initialization:
1: Initialize recommendation dataset 𝑅𝑅𝑛𝑛+1 = ∅.
// Selective recommendation:
2: for 𝒙𝒙 ∈ 𝑈𝑈
3:
Predict label 𝑦𝑦 ∗ according to (1).
4:
Calculate 𝒞𝒞𝑛𝑛 , 𝒜𝒜𝑛𝑛+1 and 𝒮𝒮𝑛𝑛+1 according to (2), (3) and (5), respectively.
5: end for
6: Sort instances 𝒙𝒙 ∈ 𝑈𝑈 in descending order w.r.t. 𝒮𝒮𝑛𝑛+1 .
7: Add the top 𝑀𝑀 ranked instances into 𝑅𝑅𝑛𝑛+1 .
// Discriminative learning:
8: for 𝒙𝒙 ∈ 𝑅𝑅𝑛𝑛+1
9:
Calculate 𝒟𝒟𝑛𝑛+1 and 𝑣𝑣 (𝑛𝑛+1) according to (6) and (8), respectively.
10: end for
11: Re-train ℎ𝑛𝑛+1 incrementally based on (𝑅𝑅𝑛𝑛+1 , 𝒗𝒗(𝑛𝑛+1) ):
(𝑛𝑛+1)

ℎ𝑛𝑛+1 ∼ 𝜃𝜃 (𝑛𝑛+1) ⇐ 𝜃𝜃𝐿𝐿+𝑅𝑅𝑛𝑛+1 .

(𝑛𝑛+1)

̃; 𝜃𝜃𝐿𝐿+(𝒙𝒙,𝑦𝑦 ∗) )
𝒜𝒜𝑛𝑛+1 (𝒙𝒙, 𝑦𝑦 ∗ ) = − ∑ 𝐻𝐻(𝑦𝑦̃|𝒙𝒙

(3)

𝐻𝐻(𝑦𝑦|𝒙𝒙; 𝜃𝜃) = − ∑ 𝑃𝑃(𝑦𝑦|𝒙𝒙; 𝜃𝜃) log 𝑃𝑃(𝑦𝑦|𝒙𝒙; 𝜃𝜃)

(4)

𝒮𝒮𝑛𝑛+1 (𝒙𝒙, 𝑦𝑦 ∗ ) = 𝒞𝒞𝑛𝑛 (𝒙𝒙, 𝑦𝑦 ∗ )𝒜𝒜𝑛𝑛+1 (𝒙𝒙, 𝑦𝑦 ∗ )

(5)

̃∈𝑈𝑈−𝒙𝒙
𝒙𝒙

where 𝐿𝐿 + (𝒙𝒙, 𝑦𝑦 ∗ ) stands for the expanded labeled dataset with a new instance (𝒙𝒙, 𝑦𝑦 ∗ ) added, and
𝑦𝑦∈𝒴𝒴

represents the conditional entropy.
According to (2) and (3), 𝒞𝒞𝑛𝑛 measures the reliability of recommendation on a certain instance that
ℎ𝑛𝑛 can supply, and 𝒜𝒜𝑛𝑛+1 reflects the demand of ℎ𝑛𝑛+1 for the instance. In order to balance the “supply
and demand”, we incorporate both factors into a comprehensive measurement:

Based on 𝒮𝒮𝑛𝑛+1 , reliable and informative instances can be selected and recommended to ℎ𝑛𝑛+1 for
the following training stage.

2.2 Discriminative Learning

As the old saying goes, learning without thinking is useless. The information in hand may not
always lead to truth, and can sometimes be inaccurate or misleading. Inspired by this learning
principle underlying the cognitive process of humans, an effective machine learning paradigm should
be able to learn discriminatively. To be more specific, it is harmful to accept recommendation
unreservedly, since it comes from prediction of learner ℎ𝑛𝑛 . Instead, the possibility that (𝒙𝒙, 𝑦𝑦 ∗ ) is

Xiao-Yu Zhang et al. / Procedia Computer Science 108C (2017) 394–403

398	

mislabeled by ℎ𝑛𝑛 should be taken into account. In case that 𝒙𝒙 adopts an alternative label 𝑦𝑦 , its
usefulness can be quantified with the expected entropy over the remaining unlabeled instances.

where

𝒟𝒟𝑛𝑛+1 (𝒙𝒙, 𝑦𝑦 ∗ ) = −

1

𝑍𝑍 (𝑛𝑛+1)

(𝑛𝑛+1)

∑ 𝑃𝑃(𝑦𝑦|𝒙𝒙; 𝜃𝜃𝐿𝐿

𝑦𝑦≠𝑦𝑦 ∗

(𝑛𝑛+1)

𝑍𝑍 (𝑛𝑛+1) = ∑ 𝑃𝑃(𝑦𝑦|𝒙𝒙; 𝜃𝜃𝐿𝐿
𝑦𝑦≠𝑦𝑦 ∗

(𝑛𝑛+1)

̃; 𝜃𝜃𝐿𝐿+(𝒙𝒙,𝑦𝑦) )
) ∑ 𝐻𝐻(𝑦𝑦̃|𝒙𝒙
̃∈𝑈𝑈−𝒙𝒙
𝒙𝒙

(𝑛𝑛+1)

) = 1 − 𝑃𝑃(𝑦𝑦 ∗ |𝒙𝒙; 𝜃𝜃𝐿𝐿

)

(6)

(7)

(𝑛𝑛+1)
is the normalization factor which enables 𝑃𝑃(𝑦𝑦|𝒙𝒙; 𝜃𝜃𝐿𝐿
)⁄𝑍𝑍 (𝑛𝑛+1) to be a distribution on condition that
∗
𝑦𝑦 ≠ 𝑦𝑦 .
Compared with 𝒜𝒜𝑛𝑛+1 that measures the potential benefit of accepting (𝒙𝒙, 𝑦𝑦 ∗ ), 𝒟𝒟𝑛𝑛+1 tries to answer
the questions whether and to what extent the learner ℎ𝑛𝑛+1 would be improved if the instance 𝒙𝒙 was
assigned with labels other than 𝑦𝑦 ∗ .
Subsequently, given a recommendation dataset 𝑅𝑅𝑛𝑛+1 ⊂ 𝑈𝑈, the weight associated with each pseudolabeled instance (𝒙𝒙, 𝑦𝑦 ∗ ) is defined as:

where

𝑣𝑣 (𝑛𝑛+1) (𝒙𝒙, 𝑦𝑦 ∗ ) =

𝑉𝑉

1

∙
(𝑛𝑛+1)

𝒜𝒜𝑛𝑛+1 (𝒙𝒙, 𝑦𝑦 ∗ )
𝒟𝒟𝑛𝑛+1 (𝒙𝒙, 𝑦𝑦 ∗ )

𝑉𝑉 (𝑛𝑛+1) = ∑ 𝑣𝑣 (𝑛𝑛+1) (𝒙𝒙, 𝑦𝑦 ∗ )
𝒙𝒙∈𝑅𝑅𝑛𝑛+1

(8)

(9)

is the normalization term. With the recommendation dataset 𝑅𝑅𝑛𝑛+1 and the corresponding weight vector
𝒗𝒗(𝑛𝑛+1) utilized as expansion of the existing training dataset, learner ℎ𝑛𝑛+1 can be further updated.
The detailed procedure of DLSR is summarized in Algorithm 1.

3 Selective Semi-supervised AdaBoost
The DLSR method proposed in the previous section can be applied to improve existing machine
learning algorithms where multiple learners are available. Specifically, as one of the most influential
ensemble methods, AdaBoost has achieved great success and received extensive attention in various
learning applications. The most attractive property of AdaBoost is that it naturally consists of a set of
weak learners trained sequentially and combined adaptively for prediction. Therefore, it is a desirable
container model to implement DLSR.
Traditionally, during the learning process of AdaBoost, information is passed on from one learner
to another in the form of weights on labeled instances. This process, as a whole, can be seen as a
single information propagation path from anterior learners to posterior ones through the labeled
dataset.
In order to fully exploit unlabeled instances, semi-supervised learning is introduced into AdaBoost
to motivate collaboration among the multiple learners. In this way, another information path is
generated through the unlabeled dataset, in which unlabeled instances are selected and recommended
with the corresponding weights. Collectively, we refer to this type of methods as Semi-supervised
AdaBoost (S-AdaBoost).
Since information on unlabeled instances is far less reliable than that on labeled instances, DLSR is
undoubtedly a better choice when initiating learning between different learners. With both

	

Xiao-Yu Zhang et al. / Procedia Computer Science 108C (2017) 394–403

(1)

399

(2)

(3)
(4)
Figure 2: Illustration of information propagation path in (1) AdaBoost, (2) S-AdaBoost, (3) SS-AdaBoost
with 1-to-1 recommendation, and (4) SS-AdaBoost with m-to-1 recommendation.

recommending and receiving learners engaged in model learning, the proposed algorithm is named
Selective Semi-supervised AdaBoost (SS-AdaBoost).
Information propagation paths of AdaBoost, S-AdaBoost and SS-AdaBoost are illustrated in
Figure 2.

3.1 Labeled Path: Labeled Instances Re-weighting
Different types of AdaBoost methods share the identical labeled path, in which a weight vector 𝒘𝒘
is maintained over the training dataset.
Initially, labeled instances (𝒙𝒙𝑖𝑖 , 𝑦𝑦𝑖𝑖 ) ∈ 𝐿𝐿 are equally weighted as
𝑤𝑤 (1) (𝒙𝒙𝑖𝑖 , 𝑦𝑦𝑖𝑖 ) =

1
|𝐿𝐿|

(10)

After the training of weak learner ℎ𝑛𝑛 , the labeled instances are re-weighted according to the
corresponding training error:
𝑤𝑤 (𝑛𝑛+1) (𝒙𝒙𝑖𝑖 , 𝑦𝑦𝑖𝑖 ) =

𝑤𝑤 (𝑛𝑛) (𝒙𝒙𝑖𝑖 , 𝑦𝑦𝑖𝑖 )exp(−𝛼𝛼𝑛𝑛 𝑦𝑦𝑖𝑖 ℎ𝑛𝑛 (𝒙𝒙𝑖𝑖 ))
𝑊𝑊 (𝑛𝑛)

where 𝑊𝑊 (𝑛𝑛) is the normalization factor, and

1
1 − 𝜖𝜖𝑛𝑛
𝛼𝛼𝑛𝑛 = ln (
)
2
𝜖𝜖𝑛𝑛

(11)

(12)

400	

Xiao-Yu Zhang et al. / Procedia Computer Science 108C (2017) 394–403

Algorithm 2: Selective Semi-supervised AdaBoost (SS-AdaBoost)
Input: labeled dataset 𝐿𝐿 ⊂ (𝒳𝒳, 𝒴𝒴);
unlabeled dataset 𝑈𝑈 ⊂ 𝒳𝒳;
number of weak learners 𝑁𝑁;
number of instances for recommendation 𝑀𝑀;
type of recommendation 𝑐𝑐 ∈ {1-to-1, m-to-1}.
Output: ensemble leaner 𝐻𝐻𝑁𝑁 : 𝒳𝒳 → 𝒴𝒴.
// Initialization:
1: Initialize labeled instance weight 𝒘𝒘(1) according to (10).
(1)
2: Train weak learner ℎ1 based on (𝐿𝐿, 𝒘𝒘(1) ): ℎ1 ∼ 𝜃𝜃 (1) ⇐ 𝜃𝜃𝐿𝐿 .
3: Calculate learning error 𝜖𝜖1 and learner weight 𝛼𝛼1 according to (13) and (12), respectively.
4: for 𝑛𝑛 = 1 to 𝑁𝑁 − 1
// Labeled path:
5:
Update labeled instance weight 𝒘𝒘(𝑛𝑛+1) according to (11).
(𝑛𝑛+1)
.
6:
Train weak learner ℎ𝑛𝑛+1 based on (𝐿𝐿, 𝒘𝒘(𝑛𝑛+1) ): ℎ𝑛𝑛+1 ∼ 𝜃𝜃𝐿𝐿
// Unlabeled path:
7:
Construct ensemble learner 𝐻𝐻𝑛𝑛 ~𝛩𝛩 (𝑛𝑛) according to (14), based on {ℎ1 , … , ℎ𝑛𝑛 } and {𝛼𝛼1 , … , 𝛼𝛼𝑛𝑛 }
8:
case: 𝑐𝑐 = 1-to-1
9:
Update weak learner ℎ𝑛𝑛+1 with ℎ𝑛𝑛 according to Algorithm 1:
(𝑛𝑛+1)
).
ℎ𝑛𝑛+1 ∼ 𝜃𝜃 (𝑛𝑛+1) ⇐ DLSR(𝐿𝐿, 𝑈𝑈, 𝑀𝑀, ℎ𝑛𝑛 ~𝜃𝜃 (𝑛𝑛) , ℎ𝑛𝑛+1 ~𝜃𝜃𝐿𝐿
10:
end case
11:
case: 𝑐𝑐 = m-to-1
12:
Update weak learner ℎ𝑛𝑛+1 with 𝐻𝐻𝑛𝑛 according to Algorithm 1:
(𝑛𝑛+1)
).
ℎ𝑛𝑛+1 ∼ 𝜃𝜃 (𝑛𝑛+1) ⇐ DLSR(𝐿𝐿, 𝑈𝑈, , 𝐻𝐻𝑛𝑛 ~𝛩𝛩 (𝑛𝑛) , ℎ𝑛𝑛+1 ~𝜃𝜃𝐿𝐿
13:
end case
14:
Calculate learning error 𝜖𝜖𝑛𝑛+1 and learner weight 𝛼𝛼𝑛𝑛+1 according to (13) and (12),
respectively.
15: end for
16: Construct ensemble learner 𝐻𝐻𝑁𝑁 ~𝛩𝛩 (𝑁𝑁) according to (14), based on {ℎ1 , … , ℎ𝑁𝑁 } and {𝛼𝛼1 , … , 𝛼𝛼𝑁𝑁 }.
𝜖𝜖𝑛𝑛 = Pr𝒙𝒙 ~𝑤𝑤 (𝑛𝑛) (ℎ𝑛𝑛 (𝒙𝒙𝑖𝑖 ) ≠ 𝑦𝑦𝑖𝑖 )
𝑖𝑖

(13)

are the weight and error of ℎ𝑛𝑛 , respectively. In this re-weighting process, the misclassified instances
are inclined to be attached with larger weights, so that the following weak learner is forced to focus on
the hard instances in the training dataset. As instructive guidance, these weights are passed between
adjacent weak learners through the labeled path.

3.2 Unlabeled Path: Unlabeled Instances Recommending
Besides labeled path, S-AdaBoost and SS-AdaBoost further establish an unlabeled path, in which
unlabeled instances are explored by means of recommendation among the weak leaners.
Different from S-AdaBoost, unlabeled instances recommending is based on DLSR in SS-AdaBoost.
Given a trained learner, predictions can be made on unlabeled instances to obtain the pseudo-labeled
instances. Then, for the subsequent learner, the expanded training dataset is available with selective
recommendation. After that, the new learner is updated, or re-trained, via discriminative learning.
When 𝑛𝑛 weak learners are obtained, the ensemble learner is constructed using additive weighted
combination.

	

Xiao-Yu Zhang et al. / Procedia Computer Science 108C (2017) 394–403

𝑛𝑛

𝐻𝐻𝑛𝑛 (𝒙𝒙) = ∑ 𝛼𝛼𝑡𝑡 ℎ𝑡𝑡 (𝒙𝒙)

401

(14)

𝑡𝑡=1

According to the range of recommendation, SS-AdaBoost falls into two modes, i.e. 1-to-1 and mto-1. In 1-to-1 mode, the pseudo-labeled instances are recommended only between the immediately
adjacent weak learners, say ℎ𝑛𝑛 and ℎ𝑛𝑛+1 ; whereas in m-to-1 mode, weak learner ℎ𝑛𝑛+1 receives
recommendation from all the anterior ones ℎ1 , … , ℎ𝑛𝑛 , or equivalently from the ensemble learner 𝐻𝐻𝑛𝑛 .
The detailed procedure of SS-AdaBoost with both labeled and unlabeled paths is summarized in
Algorithm 2.

4 Experiments
In order to validate the performance of the DLSR method and its application in SS-AdaBoost, we
carry out classification tasks on datasets of malware and patent, respectively.

4.1 Datasets
Detailed description of the datasets are as follows.
Malware Classification. Malware classification is a basic application in information security. We
collect 3220 malware instances of 8 classes. Each instance is denoted with a 162-dimentional vector,
representing its API and key code fragment.
Patent Classification. As the basis for patent analysis, patent classification is indispensable for
effective management of patents and in-depth exploration of valuable information. Patent documents
come from the Innography database. 5000 patents on electric automobile are collected as the dataset,
all of which are classified manually by domain experts into 5 classes, i.e. battery, battery management,
motor, motor control, and vehicle control unit. Among the patents, 5484 terms are extracted as raw
text feature. The weight of a term within a patent is calculated with TF-IDF (term frequency–inverse
document frequency). Principal Component Analysis (PCA) is then used for dimension reduction,
arriving at a 300- dimensional feature vector.

4.2 Evaluation of DLSR
In this experiment, we compare DLSR with passive learning (PL) and collaborative learning (CL),
which were discussed theoretically in Section 2.
Each dataset is split into two equal parts: one for training and the other for testing. Independent
Component Analysis (ICA) is implemented on the input space to extract two sets of irrelevant features,
based on which two learners are trained correspondingly. Detailed setup about number of labeled and
unlabeled instances and dimension of sub-features is listed in Table 1. For classification, regularized
softmax regression is used to model the multiple classes simultaneously. Given a fixed number of
labeled instances, we examine the classification performance with gradually increasing number of
unlabeled instances recommended. Classification results are illustrated in Figure 3.
It is observed that both DLSR and CL outperform PL. It demonstrates that the exploration of
unlabeled instances can provide additional information for model improvement. Besides labeled
instance, the learners can learn from each other based on different point of view. In DLSR, unlabeled
instances are utilized in a more effective way by recommending selectively and learning
discriminatively, which brings about further improvement in the classification performance. The
highest classification performance indicates that DLSR is the most effective way of semi-supervised
ensemble learning.

Xiao-Yu Zhang et al. / Procedia Computer Science 108C (2017) 394–403

402	

Malware
Patent

labeled
360
500

Instance
unlabeled
testing
1250
1610
2000
2500

Table 1: Detailed setup on experimental datasets.

total
3220
5000

feature1
81
150

Feature
feature2
81
150

total
162
300

Figure 3: Classification performance of passive learning (PL), collaborative learning (CL), and DLSR
with increasing number of unlabeled instances.

Figure 4: Classification performance of AdaBoost, S-AdaBoost, and SS-AdaBoost with 10 and 20 base
classifiers.

4.3 Evaluation of SS-AdaBoost
In this experiment, we compare SS-AdaBoost with AdaBoost and S-AdaBoost, which were
discussed in Section 3.
Detailed setup with instances and features are identical to that in Table 1. Regularized softmax
regression is adopted as base classifier in the three AdaBoost methods. The number of classifiers in
the ensemble is empirically chosen as 10 and 20. The odd-numbered classifiers are trained with feature
1, and the even-numbered with feature 2. According to experiment 1, we fix the number of unlabeled
instances for recommendation to 30% of the number of labeled, i.e. 108 and 150 respectively, which
brings about approximately the highest performance of DLSR. Classification results are illustrated in
Figure 4.
As we can see, both SS-AdaBoost and S-AdaBoost achieve better performance than the traditional
supervised AdaBoost, because they take full advantage of the information from both labeled and
unlabeled instances. With both recommending and receiving learners engaged in model learning, the
unlabeled instances selected by SS-AdaBoost are much more instructive than those by S-AdaBoost.
Therefore, as expected, SS-AdaBoost outperforms all the competitors and receives the highest
classification accuracy. As for the recommendation mode of SS-AdaBoost, m-to-1 outperforms 1-to-1
because the recommendation made collectively from multiple learners tends to be more reliable than
that from any individual learner.

	

Xiao-Yu Zhang et al. / Procedia Computer Science 108C (2017) 394–403

5 Conclusion
In this paper, we proposed an effective way of learning from recommendation. The contributions
of this work are three-fold. First, we integrate reliability and informativeness into a comprehensive
measurement for selective recommendation to balance the “supply and demand” between the learners.
Second, the potential in both correct and incorrect recommendation are formulated in discriminative
learning to weight the pseudo-labeled instances adaptively. Last but not least, the proposed method is
implemented to improve the existing AdaBoost into the selective semi-supervised AdaBoost, which is
effective in that both recommending and receiving learners are engaged in model learning.
Encouraging results are received from experiments conducted on real-world classification tasks.

6 Acknowledgment
This work was supported by National Natural Science Foundation of China (Grant 61501457), and
the National High Technology Research and Development Program of China (Grant 2013AA013204).

References
[1]
[2]
[3]
[4]
[5]
[6]
[7]
[8]
[9]
[10]
[11]
[12]
[13]
[14]
[15]
[16]

C. M. Bishop, Pattern Recognition and Machine Learning. New York: Springer, 2006.
I. H. Witten, E. Frank, and M. A. Hall, Data Mining: Practical Machine Learning Tools and Techniques.
Burlington: Elsevier, 2011.
X. Y. Zhang, “Simultaneous Optimization for robust correlation estimation in partially observed social
network,” Neurocomputing, 205, pp. 455–462, 2016.
X. Zhang, C. Xu, J. Cheng, H. Lu, and S. Ma, “Effective annotation and search for video blogs with
integration of context and content analysis,” IEEE Transactions on Multimedia, 11(2), pp. 272–285, 2009.
X. Zhu and A. B. Goldberg, Introduction to Semi-Supervised Learning. Morgan & Claypool, 2009.
R. G. Soares, H. Chen, and X. Yao, “Semi-supervised classification with cluster regularization,” IEEE
Transactions on Neural Networks and Learning Systems, 23(11), pp. 1779–1792, 2012.
X. Zhang, “Interactive Patent classification based on multi-classifier fusion and active learning,”
Neurocomputing, 127, pp. 200–205, 2014.
Z. H. Zhou, Ensemble methods: foundations and algorithms. CRC Press, 2012.
R. Polikar, “Ensemble based systems in decision making,” IEEE Circuits and systems magazine, 6(3), pp.
21–45, 2006.
R. Rojas, “AdaBoost and the super bowl of classifiers a tutorial introduction to adaptive boosting,” Freie
University, Berlin, Tech. Rep., 2009.
K. Bennett, A. Demiriz, and R. Maclin, “Exploiting unlabeled data in ensemble methods,” In Proc. ACM
SIGKDD International Conference on Knowledge Discovery and Data Mining, pp. 289–296, 2002.
Y. Grandvalet, and C. Ambroise, “Semi-supervised marginboost,” In Advances in neural information
processing systems, pp. 553–560, 2001.
P. Mallapragada, R. Jin, A. Jain, and Y. Liu, “Semiboost: Boosting for semi-supervised learning,” IEEE
Transactions on Pattern Analysis and Machine Intelligence, 31(11), pp. 2000–2014, 2009.
K. Chen and S. Wang, “Semi-supervised learning via regularized boosting working on multiple semisupervised assumptions,” IEEE Transactions on Pattern Analysis and Machine Intelligence, 33(1), pp. 129–
143, 2011.
X. Y. Zhang, S. Wang, and X. Yun, “Bidirectional active learning: a two-way exploration into unlabeled
and labeled data set,” IEEE Transactions on Neural Networks and Learning Systems, 26(12), pp. 3034–
3044, 2015.
X. Y. Zhang, S. Wang, X. Zhu, X. Yun, G. Wu, and Y. Wang, “Update vs. upgrade: modeling with
indeterminate multi-class active learning,” Neurocomputing, 162, pp. 163–170, 2015.

403

