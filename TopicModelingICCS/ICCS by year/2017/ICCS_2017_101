Available online at www.sciencedirect.com

ScienceDirect
Procedia Computer Science 108C (2017) 1050–1059

International Conference on Computational Science, ICCS 2017, 12-14 June 2017,
Zurich, Switzerland

Efficient Hybrid Algorithms for Computing Clusters
Overlap
Efficient Hybrid Algorithms
for Computing Clusters
Overlap
Pradeep Javangula, Kourosh Modarresi, Paresh Shenoy1,2*
2†
Yi Liu
and Aran
NayebiParesh
1,2*
Pradeep Javangula,
Kourosh
Modarresi,
Shenoy1,2*
1
Adobe Inc, U.S.
2†
2†
Yi Liu2Adobe
and Aran
Nayebi
Inc, U.S.

1Adobe Inc, U.S.
javangul@adobe.com, modarres@adobe.com,
pshenoy@adobe.com, yiliu15@stanford.edu,
2
2Adobe Inc, U.S.
anayebi@stanford.edu
javangul@adobe.com, modarres@adobe.com, pshenoy@adobe.com, yiliu15@stanford.edu,
anayebi@stanford.edu
1

Abstract
Every year, marketers target different segments of the population with multitudes of advertisements.
Abstract
However, millions of dollars are wasted targeting similar segments with different advertisements.
Every
year, marketers
target expensive
different segments
of the
the similarity
populationbetween
with multitudes
of advertisements.
Furthermore,
it is extremely
to compute
the segments
because these
However,
millions
of
dollars
are
wasted
targeting
similar
segments
with
different
advertisements.
segments can be very large, on the order of millions and billions. In this project, we come
up with a fast
Furthermore,
it is extremely
expensive
to compute
thedetermine
similarity the
between
the segments
because
these
probabilistic algorithm,
described
in Section
3, that can
similarity
between large
segments
segments
can degree
be veryoflarge,
on thethan
order
of millions
and billions. In this project, we come up with a fast
with a higher
accuracy
other
known methods.
probabilistic algorithm, described in Section 3, that can determine the similarity between large segments
© 2017
TheClustering,
Authors.
by Elsevier
B.V.known methods.
with
a higher
degreePublished
of accuracy
than Inclusion-Exclusion
other
Keywords:
Jaccard
similarity,
algorithm, MinHash, HyperLogLog

Peer-review under responsibility of the scientific committee of the International Conference on Computational Science

Keywords:
Keywords: Clustering,
Clustering, Jaccard
Jaccard similarity,
similarity, Inclusion-Exclusion
Inclusion-Exclusion algorithm,
algorithm, MinHash,
MinHash, HyperLogLog
HyperLogLog

1 Introduction
clusters or segments are some of the major functionality in data analysis and digital
1 Building
Introduction

marketing. These segments are built for creation of a group of similar users. The interests would be to
Buildingthe
clusters
segments
are insome
of specific
the major
functionality
analysis
and digital
understand
users, or
their
inclination
taking
actions
and also in
in data
targeting
the users
in the
marketing.
segments
are built for creation of a group of similar users. The interests would be to
segment forThese
specific
campaign.
understand
the users,
theirofinclination
in taking
specific
actions
and also in
targeting
the sense
users of
in that
the
One prominent
feature
these segments
is that
they have
considerable
overlaps
in the
segment
forusers
specific
many same
are campaign.
members of the different segments. Campaigns aim in avoiding targeting the same
Onesoprominent
feature
of these
segments
that they
have considerable
overlaps
in the
of that
users,
finding these
overlaps
is critical
to is
prevent
redundancy
and loss of
resources.
Bysense
computing
many
same
users
are
members
of
the
different
segments.
Campaigns
aim
in
avoiding
targeting
the
same
the amount of overlaps the segments may have, one could also make a better estimate about the value
users,
so
finding
these
overlaps
is
critical
to
prevent
redundancy
and
loss
of
resources.
By
computing
of each segment.
the amount of overlaps the segments may have, one could also make a better estimate about the value
of each segment.
VP of Digital Marketing, Sr AI- Machine Learning scientist, Team Lead at Media & Advertising Solutions (All at Adobe)
Interns at Adobe Inc
**
****
The
authors
names areSrsorted
in alphabetical
order
(Adobe
FT and
VP of
of
Digital
Marketing,
AI- Machine
Machine
Learning
scientist,
Team
LeadInterns).
at Media
Media &
& Advertising
Advertising Solutions
Solutions (All
(All at
at Adobe)
Adobe)
VP
Digital
Marketing,
Sr AILearning
scientist,
Team
Lead
at
††
Interns
at
Adobe
Inc
Interns at Adobe Inc
****
**** The
The authors
authors names
names are
are sorted
sorted in
in alphabetical
alphabetical order
order (Adobe
(Adobe FT
FT and
and Interns).
Interns).
*
†

1877-0509 © 2017 The Authors. Published by Elsevier B.V.
Peer-review under responsibility of the scientific committee of the International Conference on Computational Science
10.1016/j.procs.2017.05.212

	

Pradeep Javangula et al. / Procedia Computer Science 108C (2017) 1050–1059

The objective of this work is to find efficient and accurate model for the computation of the overlaps
of segments.

2 The Current Models
Before giving our full algorithm in Section 3, we first give an overview of two initial
probabilistic approaches that we will pursue, and then will combine these two methods in Section 3 in
a novel way.

2.1 Inclusion-Exclusion Method
B:

The inclusion-exclusion method makes use of the basic property that for two segments A and
|𝐴𝐴𝐴𝐴 ∩ 𝐵𝐵𝐵𝐵| = |𝐴𝐴𝐴𝐴| + |𝐵𝐵𝐵𝐵| − |𝐴𝐴𝐴𝐴 ∪ 𝐵𝐵𝐵𝐵|.

Thus, if we can quickly compute |𝐴𝐴𝐴𝐴 ∪ 𝐵𝐵𝐵𝐵|, then we can derive the size of the overlap given by
|𝐴𝐴𝐴𝐴 ∩ 𝐵𝐵𝐵𝐵|. Currently, the HyperLogLog (HLL) algorithm (Flajolet, 2007) can generate the values of
|𝐴𝐴𝐴𝐴 ∪ 𝐵𝐵𝐵𝐵| very efficiently and with high accuracy (as it is a probabilistic algorithm, so the results will
not be guaranteed to be exact). Therefore, this approach is a viable alternative to the brute force
approach.

2.2 MinHash + HLL
Another observation for computing |𝐴𝐴𝐴𝐴 ∩ 𝐵𝐵𝐵𝐵|, is the following identity:
The quantity

|𝐴𝐴𝐴𝐴∩𝐵𝐵𝐵𝐵|

|𝐴𝐴𝐴𝐴∪𝐵𝐵𝐵𝐵|

|𝐴𝐴𝐴𝐴 ∩ 𝐵𝐵𝐵𝐵| =

|𝐴𝐴𝐴𝐴 ∩ 𝐵𝐵𝐵𝐵|
∗ |𝐴𝐴𝐴𝐴 ∪ 𝐵𝐵𝐵𝐵|.
|𝐴𝐴𝐴𝐴 ∪ 𝐵𝐵𝐵𝐵|

is known as the Jaccard similarity between A and B, and can be computed

efficiently and with high accuracy using a technique known as MinHash (Broder, 1997). Moreover,
|𝐴𝐴𝐴𝐴 ∪ 𝐵𝐵𝐵𝐵| can be computing efficiently using HyperLogLog, as in the Inclusion-Exclusion method.

For the sake of completeness, we describe MinHash and provide its pseudocode; our
description is based on the explanation provided in (Vassilvitskii, 2011). Let 𝑔𝑔𝑔𝑔1 , … , 𝑔𝑔𝑔𝑔𝑘𝑘𝑘𝑘 be 𝑘𝑘𝑘𝑘
independent random hash functions. We then construct 𝑘𝑘𝑘𝑘 new hash functions 𝐻𝐻𝐻𝐻1 , … , 𝐻𝐻𝐻𝐻𝑘𝑘𝑘𝑘 where for any
segment 𝑊𝑊𝑊𝑊,
𝐻𝐻𝐻𝐻𝑖𝑖𝑖𝑖 (𝑊𝑊𝑊𝑊) = min 𝑔𝑔𝑔𝑔𝑖𝑖𝑖𝑖 (𝑤𝑤𝑤𝑤).
𝑤𝑤𝑤𝑤∈𝑊𝑊𝑊𝑊

In other words, each hash function 𝐻𝐻𝐻𝐻𝑖𝑖𝑖𝑖 is the minimum value of the hash function 𝑔𝑔𝑔𝑔𝑖𝑖𝑖𝑖 on the segment 𝑊𝑊𝑊𝑊.
Thus, for the two segments 𝐴𝐴𝐴𝐴 and 𝐵𝐵𝐵𝐵, we first compute their summary vectors, 𝑆𝑆𝑆𝑆(𝐴𝐴𝐴𝐴) and 𝑆𝑆𝑆𝑆(𝐵𝐵𝐵𝐵), defined
as such:
𝑆𝑆𝑆𝑆(𝐴𝐴𝐴𝐴) = < 𝐻𝐻𝐻𝐻1 (𝐴𝐴𝐴𝐴), … , 𝐻𝐻𝐻𝐻𝑘𝑘𝑘𝑘 (𝐴𝐴𝐴𝐴) >

𝑆𝑆𝑆𝑆(𝐵𝐵𝐵𝐵) = < 𝐻𝐻𝐻𝐻1 (𝐵𝐵𝐵𝐵), … , 𝐻𝐻𝐻𝐻𝑘𝑘𝑘𝑘 (𝐵𝐵𝐵𝐵) >.

1051

Pradeep Javangula et al. / Procedia Computer Science 108C (2017) 1050–1059

1052	

Now, define 𝑌𝑌𝑌𝑌𝑖𝑖𝑖𝑖 = 1 if 𝐻𝐻𝐻𝐻𝑖𝑖𝑖𝑖 (𝐴𝐴𝐴𝐴) = 𝐻𝐻𝐻𝐻𝑖𝑖𝑖𝑖 (𝐵𝐵𝐵𝐵) and 𝑌𝑌𝑌𝑌𝑖𝑖𝑖𝑖 = 0 if otherwise. Then the MinHash estimate of the
Jaccard similarity is given by

∑𝑘𝑘𝑘𝑘𝑖𝑖𝑖𝑖=1 𝑌𝑌𝑌𝑌𝑖𝑖𝑖𝑖
.
𝑘𝑘𝑘𝑘

We provide the pseudocode for MinHash below (based on the above explanation):
MinHash Pseudocode
Input: Segments A and B
1.
2.

Construct 𝒌𝒌𝒌𝒌 random hash functions 𝒈𝒈𝒈𝒈𝟏𝟏𝟏𝟏 , … , 𝒈𝒈𝒈𝒈𝒌𝒌𝒌𝒌
Compute the summary vectors for A and B:
𝑺𝑺𝑺𝑺(𝑨𝑨𝑨𝑨) = < 𝑯𝑯𝑯𝑯𝟏𝟏𝟏𝟏 (𝑨𝑨𝑨𝑨), … , 𝑯𝑯𝑯𝑯𝒌𝒌𝒌𝒌 (𝑨𝑨𝑨𝑨) >
𝑺𝑺𝑺𝑺(𝑩𝑩𝑩𝑩) = < 𝑯𝑯𝑯𝑯𝟏𝟏𝟏𝟏 (𝑩𝑩𝑩𝑩), … , 𝑯𝑯𝑯𝑯𝒌𝒌𝒌𝒌 (𝑩𝑩𝑩𝑩) >,
where for any segment W,
𝑯𝑯𝑯𝑯𝒊𝒊𝒊𝒊 (𝑾𝑾𝑾𝑾) = 𝐦𝐦𝐦𝐦𝐦𝐦𝐦𝐦𝐦𝐦𝐦𝐦 𝒈𝒈𝒈𝒈𝒊𝒊𝒊𝒊 (𝒘𝒘𝒘𝒘).
𝒘𝒘𝒘𝒘∈𝑾𝑾𝑾𝑾

3. Let Y denote the number of indices i where S(A) and S(B) match. (So, 𝒀𝒀𝒀𝒀 = ∑𝒌𝒌𝒌𝒌𝒊𝒊𝒊𝒊=𝟏𝟏𝟏𝟏 𝒀𝒀𝒀𝒀𝒊𝒊𝒊𝒊 , in
our above explanation).
Return:
𝒀𝒀𝒀𝒀
.
𝒌𝒌𝒌𝒌

For further reference, our complete Python implementation of the MinHash algorithm is
given in Appendix A.2 (note that it has not been parallelized for efficiency). For our simulations
presented in Section 5, we used a parallelized implementation of MinHash found online (Zhu, 2016).

3 Our Mathematical Model
In our simulations (detailed in Section 5) of the Inclusion-Exclusion and MinHash + HLL methods,
we found that the error rate of Inclusion-Exclusion was sometimes comparable to that of MinHash +
HLL (though MinHash + HLL generally attained lower error than Inclusion-Exclusion), and overall
Inclusion-Exclusion ran faster than MinHash + HLL. Therefore, it would be advantageous to know when
we should use one method over the other, rather than simply sticking to a single method. Both
theoretically and empirically, which we will detail in Sections 4 and 5, we have found that the error
strongly depends on the Jaccard similarity between the two segments. The basic idea behind our
approach is when the error of Inclusion-Exclusion is comparable (according to an empirically defined
threshold on the Jaccard similarity of the segments) to that of MinHash + HLL, then we use InclusionExclusion; otherwise, we use MinHash + HLL, thereby optimizing between speed and performance.

3.1 Pseudocode

	

Pradeep Javangula et al. / Procedia Computer Science 108C (2017) 1050–1059

The pseudocode for our algorithm is given below:
Pseudocode
Input: N segments 𝑨𝑨𝑨𝑨𝟏𝟏𝟏𝟏 , … , 𝑨𝑨𝑨𝑨𝑵𝑵𝑵𝑵

0.

1.

By default, set the threshold 𝜶𝜶𝜶𝜶 = 𝟎𝟎𝟎𝟎. 𝟓𝟓𝟓𝟓. (This threshold can be set manually to a
different value if you wish).
If 𝑵𝑵𝑵𝑵 ≥ 𝟑𝟑𝟑𝟑, use MinHash + HLL to compute the overlap | ⋂𝑵𝑵𝑵𝑵
𝒊𝒊𝒊𝒊=𝟏𝟏𝟏𝟏 𝑨𝑨𝑨𝑨𝒊𝒊𝒊𝒊 |, since
𝑵𝑵𝑵𝑵

| ⋂𝑵𝑵𝑵𝑵
𝒊𝒊𝒊𝒊=𝟏𝟏𝟏𝟏 𝑨𝑨𝑨𝑨𝒊𝒊𝒊𝒊 |
�� 𝑨𝑨𝑨𝑨𝒊𝒊𝒊𝒊 � =
∗ �� 𝑨𝑨𝑨𝑨𝒊𝒊𝒊𝒊 �,
𝑵𝑵𝑵𝑵
|
⋃
𝒊𝒊𝒊𝒊=𝟏𝟏𝟏𝟏
𝒊𝒊𝒊𝒊=𝟏𝟏𝟏𝟏 𝑨𝑨𝑨𝑨𝒊𝒊𝒊𝒊 |
𝑵𝑵𝑵𝑵

| ⋂𝑵𝑵𝑵𝑵
𝒊𝒊𝒊𝒊=𝟏𝟏𝟏𝟏 𝑨𝑨𝑨𝑨𝒊𝒊𝒊𝒊 |

, can be computed by MinHash,

where the generalized Jaccard similarity,
and the generalized union,
2.

�⋃𝑵𝑵𝑵𝑵
𝒊𝒊𝒊𝒊=𝟏𝟏𝟏𝟏 𝑨𝑨𝑨𝑨𝒊𝒊𝒊𝒊 �,

𝒊𝒊𝒊𝒊=𝟏𝟏𝟏𝟏

| ⋃𝑵𝑵𝑵𝑵
𝒊𝒊𝒊𝒊=𝟏𝟏𝟏𝟏 𝑨𝑨𝑨𝑨𝒊𝒊𝒊𝒊 |

can be computed using HLL.

Else if 𝑵𝑵𝑵𝑵 = 𝟐𝟐𝟐𝟐, first compute |𝑨𝑨𝑨𝑨𝟏𝟏𝟏𝟏 | and |𝑨𝑨𝑨𝑨𝟐𝟐𝟐𝟐 | either exactly or approximately using
HLL. Now, suppose |𝑨𝑨𝑨𝑨𝟏𝟏𝟏𝟏 | ≥ |𝑨𝑨𝑨𝑨𝟐𝟐𝟐𝟐 |, namely suppose 𝑨𝑨𝑨𝑨𝟏𝟏𝟏𝟏 is the larger of the two sets.
i)

If

|𝑨𝑨𝑨𝑨𝟐𝟐𝟐𝟐 |

|𝑨𝑨𝑨𝑨𝟏𝟏𝟏𝟏 |

≥ 𝜶𝜶𝜶𝜶, then estimate |𝑨𝑨𝑨𝑨𝟏𝟏𝟏𝟏 ∪ 𝑨𝑨𝑨𝑨𝟐𝟐𝟐𝟐 | using HLL, and use the Inclusion-

Exclusion method to compute |𝑨𝑨𝑨𝑨𝟏𝟏𝟏𝟏 ∩ 𝑨𝑨𝑨𝑨𝟐𝟐𝟐𝟐 |. If the estimated Jaccard similarity
|𝑨𝑨𝑨𝑨𝟏𝟏𝟏𝟏 ∩𝑨𝑨𝑨𝑨𝟐𝟐𝟐𝟐 |
≥ 𝜶𝜶𝜶𝜶, then return |𝑨𝑨𝑨𝑨𝟏𝟏𝟏𝟏 ∩ 𝑨𝑨𝑨𝑨𝟐𝟐𝟐𝟐 |, which we already computed with the
|𝑨𝑨𝑨𝑨𝟏𝟏𝟏𝟏 ∪𝑨𝑨𝑨𝑨𝟐𝟐𝟐𝟐 |

Inclusion-Exclusion method. Otherwise, compute |𝑨𝑨𝑨𝑨𝟏𝟏𝟏𝟏 ∩ 𝑨𝑨𝑨𝑨𝟐𝟐𝟐𝟐 | using MinHash
+ HLL.
ii)
Else, compute |𝑨𝑨𝑨𝑨𝟏𝟏𝟏𝟏 ∩ 𝑨𝑨𝑨𝑨𝟐𝟐𝟐𝟐 | using MinHash + HLL.
Return: Computed estimate of the segment overlap:
𝑵𝑵𝑵𝑵

��

𝑨𝑨𝑨𝑨𝒊𝒊𝒊𝒊 �.

𝒊𝒊𝒊𝒊=𝟏𝟏𝟏𝟏

We recommend using at least 22000 hash functions for MinHash and at least 10,000
streams for HLL.

3.2 Explanation and Justification

For more than two segments, we use MinHash + HLL, as it is the most direct approach for
computing intersection between multiple segments since the Inclusion-Exclusion formula becomes
rather unruly and impractical for more than two segments, not to mention that the generalized Jaccard
similarity can be computed by MinHash and the generalized union can be computed by HLL.
Now, in the case of two segments, we try to attain a reasonable tradeoff between the speed of
the Inclusion-Exclusion method and the accuracy of MinHash + HLL. Both empirically and
theoretically, we have found that the Inclusion-Exclusion method attains a comparable error rate to
MinHash + HLL for extreme values of the Jaccard similarity, namely, when the Jaccard similarity is
high. The reason why we first estimate the ratio

|𝐴𝐴𝐴𝐴2 |

|𝐴𝐴𝐴𝐴1 |

because it is a surrogate for the Jaccard similarity

1053

Pradeep Javangula et al. / Procedia Computer Science 108C (2017) 1050–1059

1054	

and can be more reliably estimated (or computed exactly) than the Jaccard similarity. This is simply
because
|𝐴𝐴𝐴𝐴1 ∩ 𝐴𝐴𝐴𝐴2 | ≤ |𝐴𝐴𝐴𝐴2 | ≤ |𝐴𝐴𝐴𝐴1 | ≤ |𝐴𝐴𝐴𝐴1 ∪ 𝐴𝐴𝐴𝐴2 |,

which implies that

As a result, if

|𝐴𝐴𝐴𝐴2 |
|𝐴𝐴𝐴𝐴1 |

< 𝛼𝛼𝛼𝛼, then

|𝐴𝐴𝐴𝐴1 ∩𝐴𝐴𝐴𝐴2 |

|𝐴𝐴𝐴𝐴1 ∪𝐴𝐴𝐴𝐴2 |

|𝐴𝐴𝐴𝐴2 | |𝐴𝐴𝐴𝐴1 ∩ 𝐴𝐴𝐴𝐴2 |
≥
.
|𝐴𝐴𝐴𝐴1 | |𝐴𝐴𝐴𝐴1 ∪ 𝐴𝐴𝐴𝐴2 |

< 𝛼𝛼𝛼𝛼, which means that the Jaccard similarity is below the

threshold 𝛼𝛼𝛼𝛼, in which case we use MinHash + HLL, as in Step 2.ii. Otherwise, if

|𝐴𝐴𝐴𝐴2 |
|𝐴𝐴𝐴𝐴1 |

≥ 𝛼𝛼𝛼𝛼, then this

does not imply anything on the Jaccard similarity, so in that case, we need to still estimate the Jaccard
similarity to see whether it is above or below the threshold 𝛼𝛼𝛼𝛼, which we do in Step 2.i.

In what follows, we further justify the above algorithm to explain how we determined the
threshold 𝛼𝛼𝛼𝛼 = 0.5. Particularly, in Section 4, we provide a theoretical justification for our approach as
well as support it empirically in Section 5.

4 Theoretical Error Analysis
Before delving into the empirical results in Section 5, we will first heuristically derive error
bounds on the probabilistic approaches outlined in Sections 2.1 and 2.2 of the Inclusion-Exclusion
Method and MinHash + HLL in Sections 4.1 and 4.2. Finally, in Section 4.3, we will use our
derivations for the error rates of the two methods to justify our algorithm proposed in Section 3.
As these error bounds are intended to be a heuristic, we will assume for convenience that the estimate
of Jaccard similarity,

|𝐴𝐴𝐴𝐴∩𝐵𝐵𝐵𝐵|

, using MinHash, and the estimate of the union |𝐴𝐴𝐴𝐴 ∪ 𝐵𝐵𝐵𝐵| using HLL are

|𝐴𝐴𝐴𝐴∪𝐵𝐵𝐵𝐵|

statistically independent.

Let 𝑋𝑋𝑋𝑋 = |𝐴𝐴𝐴𝐴 ∩ 𝐵𝐵𝐵𝐵| denote the true overlap between A and B. Then, for any estimator 𝑋𝑋𝑋𝑋� of 𝑋𝑋𝑋𝑋, the error
of the estimator 𝑋𝑋𝑋𝑋� can be quantified via mean squared error and decomposed into the following biasvariance tradeoff:
2
2
2
2
𝐸𝐸𝐸𝐸 ��𝑋𝑋𝑋𝑋 − 𝑋𝑋𝑋𝑋�� � = �𝐸𝐸𝐸𝐸� 𝑋𝑋𝑋𝑋�� − 𝑋𝑋𝑋𝑋� + 𝐸𝐸𝐸𝐸 �� 𝑋𝑋𝑋𝑋� − 𝐸𝐸𝐸𝐸� 𝑋𝑋𝑋𝑋��� � = 𝐵𝐵𝐵𝐵𝐵𝐵𝐵𝐵𝐵𝐵𝐵𝐵𝐵𝐵𝐵𝐵� 𝑋𝑋𝑋𝑋�� + 𝑉𝑉𝑉𝑉𝐵𝐵𝐵𝐵𝑉𝑉𝑉𝑉𝐵𝐵𝐵𝐵𝐵𝐵𝐵𝐵𝑉𝑉𝑉𝑉𝑉𝑉𝑉𝑉𝑉𝑉𝑉𝑉� 𝑋𝑋𝑋𝑋��,

where E denotes expected value.

Now, let 𝑋𝑋𝑋𝑋𝐼𝐼𝐼𝐼𝐼𝐼𝐼𝐼 and let 𝑋𝑋𝑋𝑋𝑀𝑀𝑀𝑀𝐻𝐻𝐻𝐻 denote the estimates of 𝑋𝑋𝑋𝑋 using the Inclusion-Exclusion and MinHash +
HLL methods, respectively. Thus, the errors of the two methods are given by 𝐸𝐸𝐸𝐸[(𝑋𝑋𝑋𝑋 − 𝑋𝑋𝑋𝑋𝐼𝐼𝐼𝐼𝐼𝐼𝐼𝐼 )2 ] and
𝐸𝐸𝐸𝐸[(𝑋𝑋𝑋𝑋 − 𝑋𝑋𝑋𝑋𝑀𝑀𝑀𝑀𝐻𝐻𝐻𝐻 )2 ], respectively, and can be decomposed as above:
𝐸𝐸𝐸𝐸[(𝑋𝑋𝑋𝑋 − 𝑋𝑋𝑋𝑋𝐼𝐼𝐼𝐼𝐼𝐼𝐼𝐼 )2 ] = 𝐵𝐵𝐵𝐵𝐵𝐵𝐵𝐵𝐵𝐵𝐵𝐵𝐵𝐵𝐵𝐵( 𝑋𝑋𝑋𝑋𝐼𝐼𝐼𝐼𝐼𝐼𝐼𝐼 )2 + 𝑉𝑉𝑉𝑉𝐵𝐵𝐵𝐵𝑉𝑉𝑉𝑉𝐵𝐵𝐵𝐵𝐵𝐵𝐵𝐵𝑉𝑉𝑉𝑉𝑉𝑉𝑉𝑉𝑉𝑉𝑉𝑉(𝑋𝑋𝑋𝑋𝐼𝐼𝐼𝐼𝐼𝐼𝐼𝐼 )

𝐸𝐸𝐸𝐸[(𝑋𝑋𝑋𝑋 − 𝑋𝑋𝑋𝑋𝑀𝑀𝑀𝑀𝐻𝐻𝐻𝐻 )2 ] = 𝐵𝐵𝐵𝐵𝐵𝐵𝐵𝐵𝐵𝐵𝐵𝐵𝐵𝐵𝐵𝐵( 𝑋𝑋𝑋𝑋𝑀𝑀𝑀𝑀𝐻𝐻𝐻𝐻 )2 + 𝑉𝑉𝑉𝑉𝐵𝐵𝐵𝐵𝑉𝑉𝑉𝑉𝐵𝐵𝐵𝐵𝐵𝐵𝐵𝐵𝑉𝑉𝑉𝑉𝑉𝑉𝑉𝑉𝑉𝑉𝑉𝑉(𝑋𝑋𝑋𝑋𝑀𝑀𝑀𝑀𝐻𝐻𝐻𝐻 ).

For notational convenience, define

	

Pradeep Javangula et al. / Procedia Computer Science 108C (2017) 1050–1059

|𝐴𝐴𝐴𝐴 ∩ 𝐵𝐵𝐵𝐵|
|𝐴𝐴𝐴𝐴 ∪ 𝐵𝐵𝐵𝐵|
𝑉𝑉𝑉𝑉 = |𝐴𝐴𝐴𝐴 ∪ 𝐵𝐵𝐵𝐵|.

𝑚𝑚𝑚𝑚 =

4.1 Inclusion-Exclusion Method Error

We proceed to derive the error of the Inclusion-Exclusion Method, namely, we derive an
expression for the quantity, 𝐸𝐸𝐸𝐸[(𝑋𝑋𝑋𝑋 − 𝑋𝑋𝑋𝑋𝐼𝐼𝐼𝐼𝐼𝐼𝐼𝐼 )2 ]. From the above discussion, it suffices to find an
expression for 𝐵𝐵𝐵𝐵𝐵𝐵𝐵𝐵𝐵𝐵𝐵𝐵𝐵𝐵𝐵𝐵(𝑋𝑋𝑋𝑋𝐼𝐼𝐼𝐼𝐼𝐼𝐼𝐼 ) and 𝑉𝑉𝑉𝑉𝐵𝐵𝐵𝐵𝑉𝑉𝑉𝑉𝐵𝐵𝐵𝐵𝐵𝐵𝐵𝐵𝑉𝑉𝑉𝑉𝑉𝑉𝑉𝑉𝑉𝑉𝑉𝑉(𝑋𝑋𝑋𝑋𝐼𝐼𝐼𝐼𝐼𝐼𝐼𝐼 ). Observe that since we know the sizes of 𝐴𝐴𝐴𝐴 and 𝐵𝐵𝐵𝐵 and
are only computing |𝐴𝐴𝐴𝐴 ∪ 𝐵𝐵𝐵𝐵| using HLL, then we have that
𝐵𝐵𝐵𝐵𝐵𝐵𝐵𝐵𝐵𝐵𝐵𝐵𝐵𝐵𝐵𝐵(𝑋𝑋𝑋𝑋𝐼𝐼𝐼𝐼𝐼𝐼𝐼𝐼 ) = 𝐵𝐵𝐵𝐵𝐵𝐵𝐵𝐵𝐵𝐵𝐵𝐵𝐵𝐵𝐵𝐵(𝐻𝐻𝐻𝐻𝐻𝐻𝐻𝐻𝐻𝐻𝐻𝐻)

𝑉𝑉𝑉𝑉𝐵𝐵𝐵𝐵𝑉𝑉𝑉𝑉𝐵𝐵𝐵𝐵𝐵𝐵𝐵𝐵𝑉𝑉𝑉𝑉𝑉𝑉𝑉𝑉𝑉𝑉𝑉𝑉(𝑋𝑋𝑋𝑋𝐼𝐼𝐼𝐼𝐼𝐼𝐼𝐼 ) = 𝑉𝑉𝑉𝑉𝐵𝐵𝐵𝐵𝑉𝑉𝑉𝑉𝐵𝐵𝐵𝐵𝐵𝐵𝐵𝐵𝑉𝑉𝑉𝑉𝑉𝑉𝑉𝑉𝑉𝑉𝑉𝑉(𝐻𝐻𝐻𝐻𝐻𝐻𝐻𝐻𝐻𝐻𝐻𝐻).

Now, the expectation of HLL as 𝑉𝑉𝑉𝑉 → ∞ in estimating n is �1 + 𝜖𝜖𝜖𝜖(𝑉𝑉𝑉𝑉)�𝑉𝑉𝑉𝑉, as reported in [2, Theorem 1],
where |𝜖𝜖𝜖𝜖(𝑉𝑉𝑉𝑉)| ≤ 5×10−5 if we have 𝑉𝑉𝑉𝑉 ≥ 16 streams. Hence, 𝐵𝐵𝐵𝐵𝐵𝐵𝐵𝐵𝐵𝐵𝐵𝐵𝐵𝐵𝐵𝐵(𝐻𝐻𝐻𝐻𝐻𝐻𝐻𝐻𝐻𝐻𝐻𝐻) = 𝜖𝜖𝜖𝜖(𝑉𝑉𝑉𝑉)𝑉𝑉𝑉𝑉. Furthermore, the
variance of HLL as 𝑉𝑉𝑉𝑉 → ∞ in estimating n is 𝑉𝑉𝑉𝑉𝐵𝐵𝐵𝐵𝑉𝑉𝑉𝑉𝐵𝐵𝐵𝐵𝐵𝐵𝐵𝐵𝑉𝑉𝑉𝑉𝑉𝑉𝑉𝑉𝑉𝑉𝑉𝑉(𝐻𝐻𝐻𝐻𝐻𝐻𝐻𝐻𝐻𝐻𝐻𝐻) = �

𝛽𝛽𝛽𝛽

√𝑟𝑟𝑟𝑟

2

+ 𝜖𝜖𝜖𝜖1 (𝑉𝑉𝑉𝑉)� 𝑉𝑉𝑉𝑉2 , where 𝛽𝛽𝛽𝛽 ≈ 1.04

and |𝜖𝜖𝜖𝜖1 (𝑉𝑉𝑉𝑉)| ≤ 5×10−4 if we have 𝑉𝑉𝑉𝑉 ≥ 16 streams, as reported in in [2, Theorem 1]. As a result,
𝐵𝐵𝐵𝐵𝐵𝐵𝐵𝐵𝐵𝐵𝐵𝐵𝐵𝐵𝐵𝐵(𝑋𝑋𝑋𝑋𝐼𝐼𝐼𝐼𝐼𝐼𝐼𝐼 ) = 𝜖𝜖𝜖𝜖(𝑉𝑉𝑉𝑉)𝑉𝑉𝑉𝑉

𝑉𝑉𝑉𝑉𝐵𝐵𝐵𝐵𝑉𝑉𝑉𝑉𝐵𝐵𝐵𝐵𝐵𝐵𝐵𝐵𝐵𝐵𝐵𝐵𝐵𝐵𝐵𝐵𝐵𝐵𝐵𝐵(𝑋𝑋𝑋𝑋𝐼𝐼𝐼𝐼𝐼𝐼𝐼𝐼 ) = �

from which it follows that as 𝑉𝑉𝑉𝑉 → ∞,

𝛽𝛽𝛽𝛽

√𝑉𝑉𝑉𝑉

+ 𝜖𝜖𝜖𝜖1 (𝑉𝑉𝑉𝑉)� 𝑉𝑉𝑉𝑉2 ,

𝐸𝐸𝐸𝐸[(𝑋𝑋𝑋𝑋 − 𝑋𝑋𝑋𝑋𝐼𝐼𝐼𝐼𝐼𝐼𝐼𝐼 )2 ] = 𝜖𝜖𝜖𝜖(𝑉𝑉𝑉𝑉)2 𝑉𝑉𝑉𝑉2 + �

4.2 MinHash + HLL Error

2

𝛽𝛽𝛽𝛽

√𝑉𝑉𝑉𝑉

2

+ 𝜖𝜖𝜖𝜖1 (𝑉𝑉𝑉𝑉)� 𝑉𝑉𝑉𝑉2 .

For this error estimate, as mentioned before, we will be assuming that the estimate of Jaccard
similarity m using MinHash, and the estimate of the union n using HLL are statistically independent.
1

The estimate of m using MinHash with 𝑘𝑘𝑘𝑘 = 𝑂𝑂𝑂𝑂 �𝑙𝑙𝑙𝑙𝑙𝑙𝑙𝑙𝑔𝑔𝑔𝑔 � hash functions is (1 + 𝛿𝛿𝛿𝛿)𝑚𝑚𝑚𝑚, as can be derived
𝛿𝛿𝛿𝛿

in [3, Theorem 1] using a standard Chernoff bound argument. Note that MinHash is an unbiased
estimator, so in the limit, it does not have any bias. As a result, the bias in estimating |𝐴𝐴𝐴𝐴 ∩ 𝐵𝐵𝐵𝐵| with
MinHash + HLL is the product of the biases of MinHash and HLL (due to our independence
assumption),
𝐵𝐵𝐵𝐵𝐵𝐵𝐵𝐵𝐵𝐵𝐵𝐵𝐵𝐵𝐵𝐵(𝑋𝑋𝑋𝑋𝑀𝑀𝑀𝑀𝐻𝐻𝐻𝐻 ) = 𝜖𝜖𝜖𝜖(𝑉𝑉𝑉𝑉)𝑚𝑚𝑚𝑚𝑉𝑉𝑉𝑉.

Furthermore, the variance of MinHash + HLL can be computed again using the formula for the
variance of the product of independent random variables (by our independence assumption),
𝑉𝑉𝑉𝑉𝐵𝐵𝐵𝐵𝑉𝑉𝑉𝑉𝐵𝐵𝐵𝐵𝐵𝐵𝐵𝐵𝑉𝑉𝑉𝑉𝑉𝑉𝑉𝑉𝑉𝑉𝑉𝑉(𝑋𝑋𝑋𝑋𝑀𝑀𝑀𝑀𝐻𝐻𝐻𝐻 ) = 𝑉𝑉𝑉𝑉𝐵𝐵𝐵𝐵𝑉𝑉𝑉𝑉𝐵𝐵𝐵𝐵𝐵𝐵𝐵𝐵𝑉𝑉𝑉𝑉𝑉𝑉𝑉𝑉𝑉𝑉𝑉𝑉(𝑀𝑀𝑀𝑀𝐵𝐵𝐵𝐵𝐵𝐵𝐵𝐵𝐵𝐵𝐵𝐵𝐵𝐵𝐵𝐵𝐵𝐵𝐵𝐵ℎ ∗ 𝐻𝐻𝐻𝐻𝐻𝐻𝐻𝐻𝐻𝐻𝐻𝐻)

1055

Pradeep Javangula et al. / Procedia Computer Science 108C (2017) 1050–1059

1056	

= 𝑉𝑉𝑉𝑉𝐵𝐵𝐵𝐵𝑉𝑉𝑉𝑉𝐵𝐵𝐵𝐵𝐵𝐵𝐵𝐵𝑉𝑉𝑉𝑉𝑉𝑉𝑉𝑉𝑉𝑉𝑉𝑉(𝑀𝑀𝑀𝑀𝐵𝐵𝐵𝐵𝐵𝐵𝐵𝐵𝐵𝐵𝐵𝐵𝐵𝐵𝐵𝐵𝐵𝐵𝐵𝐵ℎ) ∗ 𝑉𝑉𝑉𝑉𝐵𝐵𝐵𝐵𝑉𝑉𝑉𝑉𝐵𝐵𝐵𝐵𝐵𝐵𝐵𝐵𝑉𝑉𝑉𝑉𝑉𝑉𝑉𝑉𝑉𝑉𝑉𝑉(𝐻𝐻𝐻𝐻𝐻𝐻𝐻𝐻𝐻𝐻𝐻𝐻) + 𝑉𝑉𝑉𝑉𝐵𝐵𝐵𝐵𝑉𝑉𝑉𝑉𝐵𝐵𝐵𝐵𝐵𝐵𝐵𝐵𝑉𝑉𝑉𝑉𝑉𝑉𝑉𝑉𝑉𝑉𝑉𝑉(𝑀𝑀𝑀𝑀𝐵𝐵𝐵𝐵𝐵𝐵𝐵𝐵𝐵𝐵𝐵𝐵𝐵𝐵𝐵𝐵𝐵𝐵𝐵𝐵ℎ) ∗ (𝐸𝐸𝐸𝐸[𝐻𝐻𝐻𝐻𝐻𝐻𝐻𝐻𝐻𝐻𝐻𝐻])2
+ 𝑉𝑉𝑉𝑉𝐵𝐵𝐵𝐵𝑉𝑉𝑉𝑉𝐵𝐵𝐵𝐵𝐵𝐵𝐵𝐵𝑉𝑉𝑉𝑉𝑉𝑉𝑉𝑉𝑉𝑉𝑉𝑉(𝐻𝐻𝐻𝐻𝐻𝐻𝐻𝐻𝐻𝐻𝐻𝐻) ∗ (𝐸𝐸𝐸𝐸[𝑀𝑀𝑀𝑀𝐵𝐵𝐵𝐵𝐵𝐵𝐵𝐵𝐵𝐵𝐵𝐵𝐵𝐵𝐵𝐵𝐵𝐵𝐵𝐵ℎ])2 .

Now, to finish the computation of 𝑉𝑉𝑉𝑉𝐵𝐵𝐵𝐵𝑉𝑉𝑉𝑉𝐵𝐵𝐵𝐵𝐵𝐵𝐵𝐵𝑉𝑉𝑉𝑉𝑉𝑉𝑉𝑉𝑉𝑉𝑉𝑉(𝑋𝑋𝑋𝑋𝑀𝑀𝑀𝑀𝐻𝐻𝐻𝐻 ), all that is left is to compute
𝑉𝑉𝑉𝑉𝐵𝐵𝐵𝐵𝑉𝑉𝑉𝑉𝐵𝐵𝐵𝐵𝐵𝐵𝐵𝐵𝑉𝑉𝑉𝑉𝑉𝑉𝑉𝑉𝑉𝑉𝑉𝑉(𝑀𝑀𝑀𝑀𝐵𝐵𝐵𝐵𝐵𝐵𝐵𝐵𝐵𝐵𝐵𝐵𝐵𝐵𝐵𝐵𝐵𝐵𝐵𝐵ℎ), which is the variance of MinHash. Observe that for each hash function 𝐵𝐵𝐵𝐵 =
1, … , 𝑘𝑘𝑘𝑘, let 𝑌𝑌𝑌𝑌𝑖𝑖𝑖𝑖 be the Bernoulli random variable which denotes whether there is a hash collision, where
Pr[𝑌𝑌𝑌𝑌𝑖𝑖𝑖𝑖 = 1] = 𝑚𝑚𝑚𝑚, so the MinHash estimate is
𝑉𝑉𝑉𝑉𝐵𝐵𝐵𝐵𝑉𝑉𝑉𝑉𝐵𝐵𝐵𝐵𝐵𝐵𝐵𝐵𝑉𝑉𝑉𝑉𝑉𝑉𝑉𝑉𝑉𝑉𝑉𝑉(𝑀𝑀𝑀𝑀𝐵𝐵𝐵𝐵𝐵𝐵𝐵𝐵𝐵𝐵𝐵𝐵𝐵𝐵𝐵𝐵𝐵𝐵𝐵𝐵ℎ) = 𝑉𝑉𝑉𝑉𝐵𝐵𝐵𝐵𝑉𝑉𝑉𝑉𝐵𝐵𝐵𝐵𝐵𝐵𝐵𝐵𝑉𝑉𝑉𝑉𝑉𝑉𝑉𝑉𝑉𝑉𝑉𝑉 �

∑𝑘𝑘𝑘𝑘
𝑖𝑖𝑖𝑖=1 𝑌𝑌𝑌𝑌𝑖𝑖𝑖𝑖
𝑘𝑘𝑘𝑘

. Therefore,

∑𝑘𝑘𝑘𝑘𝑖𝑖𝑖𝑖=1 𝑉𝑉𝑉𝑉𝐵𝐵𝐵𝐵𝑉𝑉𝑉𝑉𝐵𝐵𝐵𝐵𝐵𝐵𝐵𝐵𝑉𝑉𝑉𝑉𝑉𝑉𝑉𝑉𝑉𝑉𝑉𝑉(𝑌𝑌𝑌𝑌𝑖𝑖𝑖𝑖 ) 𝑘𝑘𝑘𝑘𝑚𝑚𝑚𝑚(1 − 𝑚𝑚𝑚𝑚)
∑𝑘𝑘𝑘𝑘𝑖𝑖𝑖𝑖=1 𝑌𝑌𝑌𝑌𝑖𝑖𝑖𝑖
𝑚𝑚𝑚𝑚(1 − 𝑚𝑚𝑚𝑚)
�=
=
=
,
𝑘𝑘𝑘𝑘
𝑘𝑘𝑘𝑘 2
𝑘𝑘𝑘𝑘 2
𝑘𝑘𝑘𝑘

since 𝑉𝑉𝑉𝑉𝐵𝐵𝐵𝐵𝑉𝑉𝑉𝑉𝐵𝐵𝐵𝐵𝐵𝐵𝐵𝐵𝑉𝑉𝑉𝑉𝑉𝑉𝑉𝑉𝑉𝑉𝑉𝑉(𝑌𝑌𝑌𝑌𝑖𝑖𝑖𝑖 ) = 𝑚𝑚𝑚𝑚(1 − 𝑚𝑚𝑚𝑚), as it is a Bernoulli random variable with mean m. Thus, as a
reminder of what we already know, we have that
𝑉𝑉𝑉𝑉𝐵𝐵𝐵𝐵𝑉𝑉𝑉𝑉𝐵𝐵𝐵𝐵𝐵𝐵𝐵𝐵𝐵𝐵𝐵𝐵𝐵𝐵𝐵𝐵𝐵𝐵𝐵𝐵(𝐻𝐻𝐻𝐻𝐻𝐻𝐻𝐻𝐻𝐻𝐻𝐻) = �

𝛽𝛽𝛽𝛽

√𝑉𝑉𝑉𝑉

2

+ 𝜖𝜖𝜖𝜖1 (𝑉𝑉𝑉𝑉)� 𝑉𝑉𝑉𝑉2

𝐸𝐸𝐸𝐸[𝐻𝐻𝐻𝐻𝐻𝐻𝐻𝐻𝐻𝐻𝐻𝐻] = �1 + 𝜖𝜖𝜖𝜖(𝑉𝑉𝑉𝑉)�𝑉𝑉𝑉𝑉
𝐸𝐸𝐸𝐸[𝑀𝑀𝑀𝑀𝐵𝐵𝐵𝐵𝐵𝐵𝐵𝐵𝐵𝐵𝐵𝐵𝐵𝐵𝐵𝐵𝐵𝐵𝐵𝐵ℎ] = 𝑚𝑚𝑚𝑚.

Therefore, it follows that
𝑉𝑉𝑉𝑉𝐵𝐵𝐵𝐵𝑉𝑉𝑉𝑉𝐵𝐵𝐵𝐵𝐵𝐵𝐵𝐵𝑉𝑉𝑉𝑉𝑉𝑉𝑉𝑉𝑉𝑉𝑉𝑉(𝑋𝑋𝑋𝑋𝑀𝑀𝑀𝑀𝐻𝐻𝐻𝐻 )
=

2

2

𝑚𝑚𝑚𝑚(1 − 𝑚𝑚𝑚𝑚) 𝛽𝛽𝛽𝛽
𝑚𝑚𝑚𝑚(1 − 𝑚𝑚𝑚𝑚)
𝛽𝛽𝛽𝛽
� + 𝜖𝜖𝜖𝜖1 (𝑉𝑉𝑉𝑉)� 𝑉𝑉𝑉𝑉2 +
�1 + 𝜖𝜖𝜖𝜖(𝑉𝑉𝑉𝑉)� 2 𝑉𝑉𝑉𝑉2 + � + 𝜖𝜖𝜖𝜖1 (𝑉𝑉𝑉𝑉)� 𝑉𝑉𝑉𝑉2 𝑚𝑚𝑚𝑚2 .
𝑘𝑘𝑘𝑘
𝑘𝑘𝑘𝑘
𝑉𝑉𝑉𝑉
√
√𝑉𝑉𝑉𝑉

Hence, using the bias-variance tradeoff, the error of MinHash + HLL becomes,
𝐸𝐸𝐸𝐸[(𝑋𝑋𝑋𝑋 − 𝑋𝑋𝑋𝑋𝑀𝑀𝑀𝑀𝐻𝐻𝐻𝐻 )2 ]

2

𝑚𝑚𝑚𝑚(1 − 𝑚𝑚𝑚𝑚) 𝛽𝛽𝛽𝛽
𝑚𝑚𝑚𝑚(1 − 𝑚𝑚𝑚𝑚)
= 𝜖𝜖𝜖𝜖(𝑉𝑉𝑉𝑉) 𝑚𝑚𝑚𝑚 𝑉𝑉𝑉𝑉 +
� + 𝜖𝜖𝜖𝜖1 (𝑉𝑉𝑉𝑉)� 𝑉𝑉𝑉𝑉2 +
�1 + 𝜖𝜖𝜖𝜖(𝑉𝑉𝑉𝑉)� 2 𝑉𝑉𝑉𝑉2
𝑘𝑘𝑘𝑘
𝑘𝑘𝑘𝑘
√𝑉𝑉𝑉𝑉
2

2 2

+�

𝛽𝛽𝛽𝛽

√𝑉𝑉𝑉𝑉

2

+ 𝜖𝜖𝜖𝜖1 (𝑉𝑉𝑉𝑉)� 𝑉𝑉𝑉𝑉2 𝑚𝑚𝑚𝑚2 .

4.3 Theoretical Justification for our Proposed Algorithm in Section 3

Now that we have derived expressions for the errors of both the Inclusion-Exclusion Method
and MinHash + HLL, we proceed to understand the conditions under which we may expect one
algorithm to achieve lower error than the other, as this understanding underlies our proposed algorithm
in Section 3. Therefore, we aim to understand when

or equivalently, when

𝐸𝐸𝐸𝐸[(𝑋𝑋𝑋𝑋 − 𝑋𝑋𝑋𝑋𝑀𝑀𝑀𝑀𝐻𝐻𝐻𝐻 )2 ] < 𝐸𝐸𝐸𝐸[(𝑋𝑋𝑋𝑋 − 𝑋𝑋𝑋𝑋𝐼𝐼𝐼𝐼𝐼𝐼𝐼𝐼 )2 ],

	

Pradeep Javangula et al. / Procedia Computer Science 108C (2017) 1050–1059

𝜖𝜖𝜖𝜖(𝑉𝑉𝑉𝑉)2 𝑚𝑚𝑚𝑚2 𝑉𝑉𝑉𝑉2 +

2

2

𝑚𝑚𝑚𝑚(1 − 𝑚𝑚𝑚𝑚) 𝛽𝛽𝛽𝛽
𝑚𝑚𝑚𝑚(1 − 𝑚𝑚𝑚𝑚)
𝛽𝛽𝛽𝛽
� + 𝜖𝜖𝜖𝜖1 (𝑉𝑉𝑉𝑉)� 𝑉𝑉𝑉𝑉2 +
�1 + 𝜖𝜖𝜖𝜖(𝑉𝑉𝑉𝑉)� 2 𝑉𝑉𝑉𝑉2 + � + 𝜖𝜖𝜖𝜖1 (𝑉𝑉𝑉𝑉)� 𝑉𝑉𝑉𝑉2 𝑚𝑚𝑚𝑚2
𝑘𝑘𝑘𝑘
𝑘𝑘𝑘𝑘
𝑉𝑉𝑉𝑉
√
√𝑉𝑉𝑉𝑉
2 2

< 𝜖𝜖𝜖𝜖(𝑉𝑉𝑉𝑉) 𝑉𝑉𝑉𝑉 + �

𝛽𝛽𝛽𝛽

√𝑉𝑉𝑉𝑉

Observe that we can cancel out 𝑉𝑉𝑉𝑉2 from both sides to simplify the inequality to become
2

2

+ 𝜖𝜖𝜖𝜖1 (𝑉𝑉𝑉𝑉)� 𝑉𝑉𝑉𝑉2 .

2

𝑚𝑚𝑚𝑚(1 − 𝑚𝑚𝑚𝑚) 𝛽𝛽𝛽𝛽
𝑚𝑚𝑚𝑚(1 − 𝑚𝑚𝑚𝑚)
𝛽𝛽𝛽𝛽
� + 𝜖𝜖𝜖𝜖1 (𝑉𝑉𝑉𝑉)� +
�1 + 𝜖𝜖𝜖𝜖(𝑉𝑉𝑉𝑉)� 2 + � + 𝜖𝜖𝜖𝜖1 (𝑉𝑉𝑉𝑉)� 𝑚𝑚𝑚𝑚2
𝜖𝜖𝜖𝜖(𝑉𝑉𝑉𝑉) 𝑚𝑚𝑚𝑚 +
𝑘𝑘𝑘𝑘
𝑘𝑘𝑘𝑘
√𝑉𝑉𝑉𝑉
√𝑉𝑉𝑉𝑉
2

1057

2

2

Now, to simplify the above expression, we define the following quantities
𝐴𝐴𝐴𝐴 = 𝜖𝜖𝜖𝜖(𝑉𝑉𝑉𝑉)2

𝐵𝐵𝐵𝐵 = �
𝐶𝐶𝐶𝐶 =

Therefore, the inequality above becomes
𝐴𝐴𝐴𝐴𝑚𝑚𝑚𝑚2 +

𝛽𝛽𝛽𝛽

√𝑉𝑉𝑉𝑉

< 𝜖𝜖𝜖𝜖(𝑉𝑉𝑉𝑉) + �

𝛽𝛽𝛽𝛽

√𝑉𝑉𝑉𝑉

+ 𝜖𝜖𝜖𝜖1 (𝑉𝑉𝑉𝑉)� .

2

+ 𝜖𝜖𝜖𝜖1 (𝑉𝑉𝑉𝑉)�

2

𝐵𝐵𝐵𝐵 + �1 + 𝜖𝜖𝜖𝜖(𝑉𝑉𝑉𝑉)�
.
𝑘𝑘𝑘𝑘

�1 + 𝜖𝜖𝜖𝜖(𝑉𝑉𝑉𝑉)� 2 2
𝐵𝐵𝐵𝐵 2
(𝑚𝑚𝑚𝑚 − 𝑚𝑚𝑚𝑚) +
(𝑚𝑚𝑚𝑚 − 𝑚𝑚𝑚𝑚) + 𝐵𝐵𝐵𝐵𝑚𝑚𝑚𝑚2 < 𝐴𝐴𝐴𝐴 + 𝐵𝐵𝐵𝐵,
𝑘𝑘𝑘𝑘
𝑘𝑘𝑘𝑘

which can be further simplified to

(𝐴𝐴𝐴𝐴 + 𝐵𝐵𝐵𝐵 + 𝐶𝐶𝐶𝐶)𝑚𝑚𝑚𝑚2 − 𝐶𝐶𝐶𝐶𝑚𝑚𝑚𝑚 < 𝐴𝐴𝐴𝐴 + 𝐵𝐵𝐵𝐵.

To make the above inequality concrete, we will try reasonable values of the variables, which we
actually used in our simulations in Section 4. Specifically, we used 𝑘𝑘𝑘𝑘 = 22000 hash functions, 𝑉𝑉𝑉𝑉 =
10816 streams, and we suppose for worst-case analysis that 𝜖𝜖𝜖𝜖(𝑉𝑉𝑉𝑉) and 𝜖𝜖𝜖𝜖1 (𝑉𝑉𝑉𝑉) achieve their maximum
values of 5×10−5 and 5×10−4 , respectively. Therefore,
𝐴𝐴𝐴𝐴 = 2.5×10−9

𝐵𝐵𝐵𝐵 = 0.00011025

𝐶𝐶𝐶𝐶 ≈ 0.000045464.

Hence, the quadratic inequality we are supposed to solve becomes
(0.00015572)𝑚𝑚𝑚𝑚2 − (0.000045464)𝑚𝑚𝑚𝑚 < 0.00011025,

from which it follows that for this inequality is satisfied only if

−0.708018 < 𝑚𝑚𝑚𝑚 < 0.999977.

2

1058	

Pradeep Javangula et al. / Procedia Computer Science 108C (2017) 1050–1059

Since the Jaccard similarity 0 ≤ 𝑚𝑚𝑚𝑚 ≤ 1, then this means that for almost all values of the Jaccard
similarity (unless the two sets are identical whereby 𝑚𝑚𝑚𝑚 = 1), MinHash + HLL should attain a lower
error than the Inclusion-Exclusion method. In other words, we should expect MinHash + HLL to
generally always have a lower error than the Inclusion-Exclusion Method.
But, we also have seen that Inclusion-Exclusion can be much faster than MinHash + HLL, so
if we slightly weaken this inequality such that the error of Inclusion-Exclusion is similar to that of
MinHash + HLL, for what values of the Jaccard similarity 𝑚𝑚𝑚𝑚 would we prefer to therefore use
Inclusion-Exclusion instead? In other words, for what values of 𝑚𝑚𝑚𝑚 and some given tolerance 0 < 𝛾𝛾𝛾𝛾 <
1, when should we expect that
(1 + 𝛾𝛾𝛾𝛾)�(0.00015572)𝑚𝑚𝑚𝑚2 − (0.000045464)𝑚𝑚𝑚𝑚� > 0.00011025.

It appears that answer is that for extreme values of of Jaccard similarity, then we should use the
Inclusion-Exclusion method over MinHash + HLL, as it will achieve a comparable error to MinHash +
HLL but is faster than MinHash + HLL. For instance, for 𝛾𝛾𝛾𝛾 = 0.25, the solution is that
𝑚𝑚𝑚𝑚 > 0.912436.

Empirically, as we will show in Section 5, it appears that we can further loosen this threshold to 𝑚𝑚𝑚𝑚 ≥
0.5 as we discuss next, to use in our proposed algorithm in Section 3, which is a hybrid combination
of MinHash + HLL and the Inclusion-Exclusion method.

5 Empirical Results and Conclusion
For our simulations, we constructed two sets A and B of random numbers with predetermined
intersection ratio and where the size of each set can also be specified. Here, we define intersection ratio
to mean the proportion of B that is in A, where |𝐴𝐴𝐴𝐴| ≥ |𝐵𝐵𝐵𝐵|, without loss of generality. More formally, the
intersection ratio (in some literature is called the overlap) is defined to be:
|𝐴𝐴𝐴𝐴 ∩ 𝐵𝐵𝐵𝐵|
.
𝐼𝐼𝐼𝐼𝑉𝑉𝑉𝑉𝐼𝐼𝐼𝐼𝑉𝑉𝑉𝑉𝑉𝑉𝑉𝑉𝑉𝑉𝑉𝑉𝑉𝑉𝑉𝑉𝑉𝑉𝑉𝑉𝐼𝐼𝐼𝐼𝐵𝐵𝐵𝐵𝐵𝐵𝐵𝐵𝐵𝐵𝐵𝐵 𝑅𝑅𝑅𝑅𝐵𝐵𝐵𝐵𝐵𝐵𝐵𝐵𝐵𝐵𝐵𝐵𝐵𝐵𝐵𝐵 =
|𝐵𝐵𝐵𝐵|

	

Pradeep Javangula et al. / Procedia Computer Science 108C (2017) 1050–1059

We varied the size of the two sets on a log scale from 3 to 6 (i.e. from 1000 to 1000000) as
well as the intersection ratio of the two sets in the range [0.01, 0.1, 0.25, 0.5, 0.75, 0.9, 0.99] and
observed how the error rate of the two methods, Inclusion-Exclusion and MinHash + HLL, compared.
We used k=22000 hash functions for all of our simulations.
The empirical results confirm the theory in section 4 and 5 and justify our criteria on |B|/|A|.
The results also display the accuracy of our hybrid model (figure above).
In the implementation of the algorithm, the Jaccard Similarity is just an estimator of the
actual Jaccard Similarity, therefore as a caution, we recommend a higher level of 𝛼𝛼𝛼𝛼 (i.e. 𝛼𝛼𝛼𝛼 = 0.5) to
compensate for the possible error in the estimates.

In conclusion, we should say that although the problem of computing the union |𝐴𝐴𝐴𝐴 ∪ 𝐵𝐵𝐵𝐵| of two large
segments A and B has been well-studied and there are many viable probabilistic approaches, computing
the overlap (or intersection) |𝐴𝐴𝐴𝐴 ∩ 𝐵𝐵𝐵𝐵| is not as well-studied, as well as the overlap for more than two
segments. Here, we explored two probabilistic approaches as an alternative to the impractical brute force
approach, and found that a novel combination of MinHash + HLL and Inclusion-Exclusion based on a
Jaccard similarity threshold seems to be the most viable approach as it consistently achieves a lower
error percentage and has a very reasonable running time for large segments. Our results are supported
both by theory and experiment, making it a practical and reliable for computing market segment
overlaps.

References
Broder, Andrei Z. (1997). On the resemblance and containment of documents. Compression
and Complexity of Sequences: Proceedings. Positano, Amalfitan Coast, Salerno, Italy,
June 11-13, 1997: pp. 21-29
Flajolet, P.; Fusy, E.; Gandouet, O.; Meunier, F. (2007). HyperLogLog: the analysis of
a near-optimal cardinality estimation algorithm. AOFA ‘07: Proceedings of the 2007
International Conference on the Analysis of Algorithms
McCormick, Chris (2015). Example Python code for comparing documents using MinHash.
GitHub Repository. https://github.com/chrisjmccormick/MinHash
Vassilvitskii, Sergei (2011). COMS 6998-12: Lecture 1 Notes
http://www.cs.columbia.edu/~coms699812/lecture1.pdf.
Zhu, Eric (2016). Probabilistic data structures for processing very large datasets, Github
Repository https://github.com/ekzhu/datasketch

1059

