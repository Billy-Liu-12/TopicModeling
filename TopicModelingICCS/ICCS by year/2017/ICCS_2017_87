Available online at www.sciencedirect.com

This
This
This
This

ScienceDirect

space is reserved for the Procedia header,
space
reserved
for the
header,
ProcediaisComputer
Science
108CProcedia
(2017) 455–464
space is reserved for the Procedia header,
space is reserved for the Procedia header,

do
do
do
do

not
not
not
not

use
use
use
use

it
it
it
it

International Conference on Computational Science, ICCS 2017, 12-14 June 2017,
Zurich, Switzerland

Devising a computational model based on data mining
Devising a computational model based on data mining
Devising
a computational
modelcompressive
based on data
mining
techniques
to predict concrete
strength
Devising
a computational
modelcompressive
based on data
mining
techniques
to predict concrete
strength
techniques
to
predict
concrete
1
1compressive strength
2
Daniel Alencar
, Darlinton
Carvalho1compressive
, Eduardus Koenders
,
techniques
to predict
concrete
strength
1
2
Daniel Alencar
, Darlinton
Carvalho
,
Eduardus
Koenders
,
1
1
Rocha1
Fernando
Mourão1 , and Leonardo
1
1
2
Daniel Alencar
, Darlinton
, Eduardus
, and Leonardo
RochaKoenders2 ,
Fernando
Mourão Carvalho
1
1
Daniel Alencar
,
Darlinton
Carvalho
,
Eduardus
Koenders
,
1
1
, and Leonardo Rocha1
Fernando
Mourão
Universidade
Federal
1 de São João del Rei, Brazil1
1
and
Rocha
Fernando
Mourão
Universidade
Federal, de
SãoLeonardo
João del Rei,
Brazil
{darlinton,lcrocha}@ufsj.edu.br
1 2
{darlinton,lcrocha}@ufsj.edu.br
Universidade
Federal de São
João del Germany
Rei, Brazil
Technische
Universität
Darmstadt,
1 2

Universidade
Federal de São
João del Germany
Rei, Brazil
Technische
Universität
Darmstadt,
{darlinton,lcrocha}@ufsj.edu.br
koenders@wib.tu-darmstadt.de
{darlinton,lcrocha}@ufsj.edu.br
koenders@wib.tu-darmstadt.de
Technische
Universität Darmstadt, Germany
2
Technische
Universität Darmstadt, Germany
koenders@wib.tu-darmstadt.de
koenders@wib.tu-darmstadt.de
2

Abstract
Abstract
Predicting the compressive strength of concrete is an essential task in the construction process,
Abstract
Predicting
theknowledge
compressive
strength
of concrete
is an
essentialspeed
task inand
thequality
construction
since a prior
on such
information
helps
enhancing
of the process,
process.
Abstract
Predicting
the
compressive
strength
of
concrete
is
an
essential
task
in
the
construction
since
a
prior
knowledge
on
such
information
helps
enhancing
speed
and
quality
of the process,
process.
Recently, many computational methods and techniques have been developed to predict
distinct
Predicting
theknowledge
compressive
strength
of concrete
is an
essential
taskdeveloped
inand
thequality
construction
process,
since
a
prior
on
such
information
helps
enhancing
speed
of
the
process.
Recently,
many
computational
methods
and
techniques
have
been
to
predict
distinct
properties of concrete. However, a practical use of these solutions requires a high degree
of
since
a prior
knowledge
on
suchmethods
information
helps
enhancing
speeddeveloped
and
quality
of thedegree
process.
Recently,
many
computational
and
techniques
have
been
to
predict
distinct
properties
of
concrete.
However,
a
practical
use
of
these
solutions
requires
a
high
of
engineering expertise and programming skills. Alternatively, this work advocates that software
Recently,
many
computational
methods
and
techniques
havesolutions
been
developed
toa predict
distinct
properties
of
concrete.
However,
a
practical
use
of
these
requires
high
degree
of
engineering
expertise
and
programming
skills.
Alternatively,
this
work
advocates
that
software
packages with off-the-shelf data mining algorithms can empower researchers and engineers on
properties
ofexpertise
concrete.
However,
a practical
use
ofcan
these
solutions
requires
a high
degree
of
engineering
and
programming
skills.
Alternatively,
this
work
advocates
that
software
packages
with
off-the-shelf
data
mining
algorithms
empower
researchers
and
engineers
on
this task, while demanding less effort. In this direction, we present a detailed study on the use of
engineering
expertise
and
programming
skills.
Alternatively,
this
work
advocates
that
software
packages
with
dataeffort.
mining
canwe
empower
and
engineers
on
this
task,
whileoff-the-shelf
demanding
less
In algorithms
this direction,
present
acompressive
detailed study
on theofuse
of
Weka,
evaluating
different regression
algorithms
for predicting
the researchers
strength
conpackages
with
off-the-shelf
dataeffort.
mining
algorithms
canwe
empower
researchers
and
engineers
on
this
task,
while
demanding
less
In
this
direction,
present
a
detailed
study
on
the
use
of
Weka,
evaluating
different
regression
algorithms
for
predicting
the
compressive
strength
of
concrete. Using the most complete dataset available at the UCI dataset repository, we demonstrate
this
task,
while
demanding
less effort.
Inavailable
this direction,
we
present
acompressive
detailed
study
on theofuse
of
Weka,
evaluating
different
regression
algorithms
for
predicting
the
strength
concrete.
Using
the
most
complete
dataset
at
the
UCI
dataset
repository,
we
demonstrate
that most of the techniques available in Weka produces results close to the best ones reported in
Weka,
evaluating
different
regression
algorithms
for
predicting
the
compressive
strength
of
concrete.
Using
most
complete
dataset
atpredicting
the results
UCI dataset
repository,
we
demonstrate
that
most
of the
techniques
available
in available
Weka
produces
close to
the bestaones
reported
in
the literature.
For
instance,
most
of the
evaluated
models
generates
Mean
Absolute
crete.
Using
the
most
complete
dataset
available
atpredicting
the results
UCI dataset
repository,
we
demonstrate
that
most
of
the
techniques
available
in
Weka
produces
close
to
the
best
ones
reported
in
the
literature.
For
instance,
most
of
the
evaluated
models
generates
a
Mean
Absolute
Error (MAE) inferior to 10, while the best result found is 8. Moreover, by fine-tuning the pathat
most
of the
techniques
available
inbest
Weka
produces
results
close to
thebybest
ones
reported
in
the
literature.
For
instance,
most
of
the
evaluated
predicting
models
generates
a
Mean
Absolute
Error
(MAE)
inferior
to
10,
while
the
result
found
is
8.
Moreover,
fine-tuning
the
parameters of the regression algorithm Bagging with REPTree, we achieved a MAE value inferior
the
literature.
For
instance,
most
of
the
evaluated
predicting
models
generates
a
Mean
Absolute
Error
inferior
todataset.
10,
whileHence,
the
best
is 8. we
Moreover,
by
fine-tuning
theas
parameters
the
regression
algorithm
Bagging
with found
REPTree,
a MAE
value
inferior
to 3.3 (MAE)
for of
the
evaluated
theresult
process
considered
inachieved
this study
is also
useful
a
Error
(MAE)
inferior
todataset.
10,
whileHence,
the
best
result
found
is 8. we
Moreover,
by
fine-tuning
theas
parameters
of
the
regression
algorithm
Bagging
with
REPTree,
achieved
a
MAE
value
inferior
to
3.3
for
the
evaluated
the
process
considered
in
this
study
is
also
useful
guideline to devise new computational models based on off-the-shelf data mining algorithms. a
rameters
the
regression
algorithm
Bagging
with
REPTree,
weinachieved
amining
MAE
value
inferior
to 3.3 for of
the
evaluated
dataset.
Hence,
the process
considered
thisdata
study
is also
useful
as a
guideline
to
devise
new computational
models
based
on off-the-shelf
algorithms.
Keywords:
Computational
Modeling,
Data
Mining,
Prediction
to
3.3
for
the
evaluated
dataset.
Hence,
the
process
considered
in
this
study
is
also
useful
as a
©
2017
The
Authors.
Published
by
Elsevier
B.V.
guideline to devise new computational models based on off-the-shelf data mining algorithms.
Keywords:
Computational
Modeling,
Datacommittee
Mining,based
Prediction
Peer-review
responsibility
of the scientific
of
the International
Conference
Computational
Science
guideline under
to
devise
new computational
models
on off-the-shelf
dataon
mining
algorithms.
Keywords: Computational Modeling, Data Mining, Prediction
Keywords: Computational Modeling, Data Mining, Prediction

1 Introduction
1 Introduction
1
Introduction
A major
structural material used in construction is the concrete [26], which is cast from a
1
Introduction
A
major
structural
material
used in construction
is the
concrete
[26], water,
which is
cast from
combination
of different
and individual
base materials,
such
as cement,
among
others.a

A
major structural
material
used inconsists
construction
is phases
the
concrete
[26], water,
which
is
cast
from
combination
different
and
individual
base of
materials,
such
as cement,
among
others.
Moreover,
theofproduction
of concrete
many
(i.e.,
hydrated
cement
paste,
tran-a
A
major structural
material
used inconsists
construction
is phases
the
concrete
[26], water,
which
is
cast
from
a
combination
of
different
and
individual
base
materials,
such
as
cement,
among
others.
Moreover,
the
production
of
concrete
of
many
(i.e.,
hydrated
cement
paste,
sition zone, etc.) and its compressive strength may vary according to some parameters (e.g.,tranmix
combination
of
different
and
individual
base
materials,
such
as
cement,
water,
among
others.
Moreover,
production
ofcompressive
concrete strength
consists
of
many
phases
hydrated
cement
paste,
sition
etc.)
and its
compressive
may
according
to
some parameters
(e.g.,tranmix
ratios)zone,
[6]. the
Predicting
the
strength
of vary
concrete
is(i.e.,
an essential
task
in construction
Moreover,
the
production
ofcompressive
concrete strength
consists
of
many
phases
(i.e.,
hydrated
cement
paste,
transition
zone,
etc.)
and
its
compressive
may
vary
according
to
some
parameters
(e.g.,
mix
ratios)
[6].
Predicting
the
strength
of
concrete
is
an
essential
task
in
construction
technology, since a prior knowledge of it may increase the speed and quality of construction [28].
sition
etc.)
its
compressive
may
toquality
some parameters
(e.g., [28].
mix
ratios)zone,
[6]. Predicting
the
compressive
of vary
concrete
is an
essential
task
in construction
technology,
since and
a prior
knowledge
ofstrength
itstrength
may increase
theaccording
speed
and
of construction
ratios)
[6].
Predicting
the
compressive
strength
of
concrete
is
an
essential
task
in
construction
technology, since a prior knowledge of it may increase the speed and quality of construction [28].
1
technology, since a prior knowledge of it may increase the speed and quality of construction [28].
1
1
1877-0509 © 2017 The Authors. Published by Elsevier B.V.
1
Peer-review under responsibility of the scientific committee of the International Conference on Computational Science
10.1016/j.procs.2017.05.018

Daniel Alencar et al. / Procedia Computer Science 108C (2017) 455–464

456	

Experiments and tests conducted on physical laboratories have been the main manner to
develop fundamental research and learning knowledge in this field. However, conducting physical laboratory experiments requires complex and expensive lab-infrastructures with significant
allocation of resources from educational and research institutes. Furthermore, the achieved
results have a limited range of exposure and are available for a relatively small audience (i.e.,
high costs versus relatively low benefits) [17, 5]. Aiming to overcome such limitation, recent
works focus on proposing novel domain-specific computational methods and techniques to predict properties of concrete [8, 9]. Given the high levels of efficacy in several domains, special
attention has been given to methods based on regression techniques [20].
Despite the existence of those effective regression-based proposals, the practical use of them
requires a high degree of engineering expertise and programming skills. At the same time, there
are many data mining software packages that can empower researchers of engineering area on
this task, while demanding less effort[27, 32]. However, the efficiency and effectiveness of these
packages were not evaluated for real scenarios such as prediction of the compressive strength
of concrete, since they were originally proposed for educational purposes.
In this paper, we present a detailed study of one of these packages, more specifically
Weka [32], evaluating different regression algorithms implemented on it, for the problem of predicting the compressive strength of concrete. Our analyses used the most complete dataset publicly available at UCI dataset repository [27] (composed by 1.030 instances with 9 attributes),
which had also been evaluated in others consolidated works in the area [20, 34]. Thus, we performed a complete numerical evaluation with 27 regression algorithms, measuring the quality
of the model generated by each one. The quality was assessed considering the metrics Mean
Absolute Error (MAE), Root Mean Squared Error (RMSE), Relative Absolute Error (RAE)
and Root Relative Squared Error (RRSE). In our results, we demonstrate that most of the
evaluated techniques present, for example, a MAE value inferior to 10%. Moreover, we verify
that the regression algorithm Bagging with REPTree may achieve a MAE value inferior to
3.3%, which correspond to an error inferior to the best one reported in literature [20].

2

Related Work

The first works related to the analysis of compressive strength of concrete were based on experiments and tests in physical laboratories [28, 20, 34, 21]. However, the emergence of new
technologies, associated with more sophisticated processing and significant advances in computing architectures, has boosted studies focused on computational analysis of real data and
simulation of mathematical models [26, 6, 8, 9]. These works reinforce the practical relevant of
computational methods applied to engineering, since the proper prediction of the compressive
strength of concrete may improve the control quality and speed of the construction process,
among other benefits. In this direction, recent researches exploit two different approaches: (1)
one related to the construction of models based on Artificial Neural Networks [34, 25]; and (2)
another one related to predicting models constructed from regression algorithms [20].
Regarding the first approach, in [34] the authors employed and adapted artificial neural networks (ANN) in order to predict the compressive strength of high-performance concrete (HPC).
Using a set of trial batches of HPC, produced in the laboratory and demonstrated satisfactory
experimental results, this study led to the conclusion that a strength model based on ANN is
more accurate than a regression analysis. However, the authors compared the proposed strategy against few regression algorithms. A major contribution of this work is a well consolidate
dataset, available at UCI dataset repository [27], considered in our experiments. Similar work is
presented in [25], in which the authors present their effort in applying identification techniques

	

Daniel Alencar et al. / Procedia Computer Science 108C (2017) 455–464

based on neural network system to predict the compressive strength of concrete. Basically, the
system aims to learn optimal concrete mix proportions, maximum aggregate size, and slump of
fresh concrete. Despite presenting good results, the dataset used in the experiments was not
made public.
Considering the second research approach, Naresh Kumar Nagwani and Shirish V. Deo’s [17]
worked on estimating the concrete compressive strength (CCS) by applying the Hard Clustering
and Fuzzy Clustering algorithms. Based on previous works [8, 9], the authors used clustering
along with regression in order to reduce prediction errors. The proposed method comprises two
steps: first, it clusters the data in order to group data with similar characteristics. Then, is
applies a regression technique in order to predict the compressive strength of each individual
cluster. The authors state that Fuzzy clustering with C-means performs better than K-means
algorithm. They adopted the same dataset used in [34], reporting better results. More specifically, they report Mean Absolute Error value of 8% for the model, which correspond to the
best result reported in literature for this dataset.
As a common characteristic of both research approaches for predicting compressive strength
of concrete, the effective use of these proposals requires a high degree of engineering expertise
and programming skills. Alternatively, in this work we evaluate the Weka [32], an educational
software package, which has several data mining algorithms implemented, including regression
techniques. Using the same complete dataset publicly available at UCI dataset repository [27],
which had also being evaluated in other consolidated works in the area [20, 34], we perform
a complete numerical evaluation with 27 regression algorithms, achieving results better than
those found in the literature [20].

3

Methodology

This study exploits the Weka Suite (3.6.14 version) [32] in order to create different computational models to solve a relevant construction problem, the prediction of compressive strength
of concrete. Weka provides off-the-shelf implementations for a collection of machine learning
algorithms designed to solve different tasks related to data mining.

3.1

Dataset

In order to perform the study proposed in this paper, we adopt in our evaluation a dataset publicly available at the UCI dataset repository [27]. It comprises the most complete dataset found
related to the problem of Compressive Strength of Concrete and it was publicly made available
by Yeh [34] with 1030 instances and 9 attributes, as presented by Table 1. The task considered
for this dataset is a numeric nonlinear regression to predict the compressive strength of concrete.
Attribute
Age
Blast Furnace Slag
Cement
Coarse Aggregate
Compressive Strength of Concrete
Fine Aggregate
Fly Ash
Super Plasticizer
Water

Measurement
Days (1 to 365)
Kg in m3 mixture
Kg in m3 mixture
Kg in m3 mixture
MPa
Kg in m3 mixture
Kg in m3 mixture
Kg in m3 mixture
Kg in m3 mixture

Description
Input
Input
Input
Input
Output
Input
Input
Input
Input

Table 1: Attributes of the Compressive Strength of Concrete dataset

457

Daniel Alencar et al. / Procedia Computer Science 108C (2017) 455–464

458	

3.2

Regression algorithms

Tree

In this section, we present a summary of the algorithms evaluated in our study. Among all
algorithms implemented by Weka, we selected those focused on predictions based on numeric
values. Weka classifies the algorithms according to their characteristics. Table 2 presents a
brief description of each evaluated algorithm, for each class, as originally described by Weka.

Algorithms
REPTree
Decision Stump

Meta

Rule

M5P
ZeroR
M5 Rules
Decision Table
Conjunctive
Rule
Additive
Regression
Bagging
with
REPTree
CV Parameter
Selection
MultiScheme

Lazy

Random
SubSpace
Regression By
Discretization
Stacking
Vote
IBK
KStar (K*)

Function

LWL
Gaussian Processes
Isotonic Regression
Least MedSq
Linear Regression
MultiLayer Perceptron
Pace Regression
RBFNetwork
Simple
Linear
Regression
SMOreg

Description
This is a fast decision tree learner that builds a regression tree using information variance
and reduces error by back fitting.
This is usually used in conjunction with a boosting algorithm and builds regression predictions based on mean-squared error.
This implements base routines for generating M5 Model trees and rules [30].
This takes the mean of the target values as the prediction. It is useful as a baseline
performance to compare with other methods.
This generates a decision list for regression problems using separate-and-conquer. In each
iteration, it builds a model tree using M5 and makes the ”best” leaf into a rule.
This is a simple decision table majority classifier [15].
This implements a single conjunctive rule learner that can predict for numeric and nominal
class labels.
This enhances the performance of a regression base classifier. Prediction is accomplished
by interactively adjusting residuals left in a fitting model considering previous iterations.
This improves prediction accuracy, reducing variance and helping to avoid over fitting. It
was designed to work better with tree based learners [3].
This is used to determine the best configuration of a classifier by cross-validating a selected
parameter [16].
This is used for selecting a classifier among several using cross-validation and variable
performance criteria on the training data.
This constructs a decision tree based classifier that maintains highest accuracy on training
data and improves on generalization accuracy as it grows in complexity [10].
This is a regression scheme that employs any regression algorithm on a copy of the data
that has the class attribute (equal-width) discretized.
This combines several regression algorithms using the stacking method [33].
This is used for combining classifiers. Different combinations of probability estimates for
classification are available [18, 13].
This uses a K-nearest neighbors classifier and can select appropriate value of K based on
cross-validation [1].
This is an instance-based classifier that predicts by determining the similarity of the training instances using an entropy-based distance function [4].
This uses an instance-based algorithm to assign instance weights that are used by a locally
weighted learning (LWL) predictor [7, 2].
This is used for regression without hyper parameter-tuning [19]. Uses lazy learning and
measures the similarity between points to predict the value.
This picks the attribute that result in the lowest squared error.
This implements a least median squared linear regression using the existing Weka Linear
Regression class to form predictions [23].
This uses the Akaike criterion for model selection and is able to deal with weighted instances.
This uses back propagation to classify instances. The network can be built by hand,
created by an algorithm or both.
This consists of a group of estimators that are either overall optimal or optimal under
certain conditions [29, 31].
This implements a normalized Gaussian radial basis function network. It uses the k-means
clustering algorithm to provide the basis functions and learns either a logistic regression
or linear regression on top of that.
This learns a simple linear regression model picking the attribute that result in the lowest
squared error.
This implements the support vector machine (SVM) for regression [24].

Table 2: Descriptions of Weka Algorithms

	

Daniel Alencar et al. / Procedia Computer Science 108C (2017) 455–464

3.3

Evaluation Process and Metrics

Besides building predictive models based on each selected algorithm, Weka also allows us to
evaluate each model through many quality metrics with a 10-fold cross-validation process. A
10-fold cross validation process consists of splitting the input dataset into ten parts, randomly.
Nine parts are used to create the regression model and then, its quality is verified in the remaining part. This process is repeated ten times and the final quality reported is related to
the mean of the ten runs. The cross validation is useful to avoid bias over a training set. In
our case, this quality is measured using standard metrics for prediction techniques [11]: Mean
Absolute Error (MAE), Root Mean Squared Error (RMSE), Relative Absolute Error (RAE)
and Root Relative Squared Error (RRSE). These metrics represent the prediction quality considering different perspectives of analysis. MAE and RMSE treats the error on an absolute
scale, presenting an amount of error on a scale of the prediction value considered in the tested
dataset. In the other hand, RMSE gives an idea of the error in terms of a proportional scale
calculated as percentage and RAE is the overall discrepancy between the predicted value and
its expected value described by the means of the all relative differences. These metrics provides
an evaluation for the accuracy of the computational model. The smaller the metrics values, the
more accurate is a given model.
Additionally, aiming to compare the quality of each different technique, we employ a Paired
T-Tests [12]. A Paired T-Test compares the means of two populations of compatible samples,
establishing which one is lower or whether they are equivalent. By applying this test over all
results, we determine the technique that provides the best results among all (e.g., we identify
the lowest MAE, for example).
Finally, the best technique identified in our analyses is further investigated by a setup tuning. It is noteworthy that we used the default parameter configuration set by Weka in the
first evaluation of all available techniques. Once the most promising technique is identified, the
configuration parameters for this technique is scrutinized by a multiple trials process. Usually
this process takes into account constraints like processing time and available memory. In the
following section, we detail this process and further investigation that was conducted based on
the predictions generated by the best computational model created.

4

Results and Discussion

As mentioned in Section 3, our first experiment consists in evaluating all regression algorithms
provided by Weka, considering the default parameter configurations. The results related to this
experiment are presented in Table 3.
Observing the results presented in the table, the model generated by Bagging with REPTree
model achieved the best quality assessment for all four quality metrics. Considering the Mean
Absolute Error, the best result reported in the literature for the same dataset was 8% [20], however, the Bagging with REPTree achieved the value of 4.348%, which is almost twice smaller.
This is an important result, since we are dealing with a tool originally proposed for educational
purposes, with its default parameters and without any kind of tuning. Moreover, it is important
to mention that 11 of the evaluated algorithms presented a Mean Absolute Error smaller that the
best one reported in literature. This result demonstrates the effectiveness of Weka in a real scenario, which makes it an excellent initial option for empowering researchers of engineering area.

459

Daniel Alencar et al. / Procedia Computer Science 108C (2017) 455–464

460	

Algorithms
REPTree
Decision Stump
ZeroR
M5 Rules
M5P
DecisionTable
ConjunctiveRule
Additive Regression
Bagging with REPTree
CVParameter Selection
MultiScheme
Random SubSpace
Regression By Discretization
Stacking
Vote
IBK
KStar
LWL
Gaussian Processes
Isotonic Regression
Least MedSq
Linear Regression
MultiLayer Perceptron
Pace Regression
RBFNetwork
Simple Linear Regression
SMOreg

Mean Absolute
Error
5.1918
11.572
13.4761
4.8098
4.7296
8.4862
10.9847
6.3588
4.348
13.4761
13.4761
6.9844
5.0341
13.4761
13.4761
6.4529
6.3784
10.5468
6.1482
10.8623
9.3346
8.3027
7.1261
8.308
13.4018
11.884
8.2068

Root Mean
Squared Error
7.0811
14.5081
16.7133
6.434
6.3566
11.2742
14.0398
8.0759
5.9854
16.7133
16.7133
8.8658
6.9518
16.7133
16.7133
9.0807
8.7098
13.2746
7.9166
13.5556
16.5684
10.5166
8.9398
10.5329
16.5898
14.5181
10.9598

Relative Absolute
Error (%)
38.5263
85.8706
100.0
35.691
35.0959
62.9718
81.5125
47.1846
32.2649
100.0
100.0
51.8283
37.3555
100.0
100.0
47.8839
47.331
78.2629
45.6227
80.6045
69.2676
61.6104
52.8796
61.6499
99.4485
88.1858
60.8991

Root Relative
Squared Error (%)
42.3677
86.8057
100.0
38.4965
38.0334
67.4564
84.0034
48.3202
35.8122
100.0
100.0
53.0465
41.5943
100.0
100.0
54.3323
52.113
79.4251
47.3672
81.1065
99.1329
62.9232
53.4889
63.0209
99.2608
86.8655
65.5754

Table 3: Quality assessment of the prediction models created with default parameters of Weka.
Since the Bagging was the best algorithm among all for this dataset, a further investigation
was conducted in order to determine its best configuration. Using only tree and rule based algorithms with Bagging, Weka provides seven algorithms to be combined with Bagging. They are
REPTree, M5P, Decision Stump, Conjuctive Rule, Decision Table, M5Rules and ZeroR. In Table 4, we present the quality assessments for each tested model considering the default setup of
the algorithms and the best fine-tuning found for their parameters, which is marked by the symbol ”∗”. The best parameter configuration of each algorithm can be reproduced using the specifications presented in Appendix A. The results related to this experiment are presented in Table 4
Considering the Paired T-Test, Bagging with REPTree with fine-tuning presents the best
overall quality assessment, with a Mean Absolute Error value of 3.298%. This is undoubtedly
the best result already reported for this collection, which further strengthens our conclusions.

4.1

Correlating Attributes Values with Model Error

Going further in the analysis of the results produced by the best prediction model created
previously, a deeper investigation was conducted scrutinizing the predictions by a comparison
with its expected value. The best computational model was used to predict the concrete
compressive strength of all 1030 instances and the results evaluated considering emerging trends.
From all predictions, 390 present more than 10% errors, being the largest error of 163% for
an instance of the age of 1 day. Considering these instances with prediction error higher than
10%, 178 instances have age attribute value inferior to 28 days. Splitting those 390 high error
predictions into two groups of age smaller than 28 and the remaining, the first group of early-age
predictions has a mean absolute error of 29.3, against a mean error of 19.6 of the second group.
Therefore, it is more likely to obtain a worse error when predicting for early age scenarios (i.e.,
before 28 days). This result is relevant because it indicates that early age prediction is harder

	

Daniel Alencar et al. / Procedia Computer Science 108C (2017) 455–464

Algorithms
Bagging with
REPTree
Bagging with
REPTree *
Bagging with
M5P
Bagging with
M5P *
Bagging with
Decision Stump
Bagging with
Conjuctive Rule
Bagging with
Conjuctive Rule *
Bagging with
Decision Table
Bagging with
Decision Table *
Bagging with
M5Rules
Bagging with
M5Rules *
Bagging with
ZeroR

Mean Absolute
Error

Root Mean
Squared Error

Relative Absolute
Error (%)

Root Relative
Squared Error (%)

4.348

5.9854

32.2649

35.8122

3.2988

4.8493

24.4786

29.0144

4.3535

5.7735

32.3055

34.5441

3.5522

5.0857

26.3594

30.4288

11.0675

13.8512

82.1269

82.8753

10.2073

12.9435

75.7436

77.4444

10.1344

12.8603

75.2029

76.9466

9.0036

12.0996

66.8113

72.3951

7.9429

10.4932

58.9402

62.7835

4.4026

5.8506

32.6697

35.0056

4.2374

5.6752

31.4435

33.9559

13.4738

16.7151

99.983

100.0106

Table 4: Results of the models created by different settings for the Bagging algorithm.

(a) Age

(b) Cement

Figure 1: Position of the values of Concrete Compressive Strength versus each parameter.
and requires more data for this phase to deliver a better prediction model. Moreover, Figure 1
(a) presents plots for errors versus amount in the mixture by age. As we can observe, for early
days the predicted value of concrete compressive strength presents worse errors, that is, it is
hard, even impossible, to preview safely the compressive strength of concrete with mixtures in
the early days.
We also compare errors versus amount in the mixture for all other attributes. Figure 1 (b)
is related to attribute Cement and shows that there is no visible relation between the amount

461

Daniel Alencar et al. / Procedia Computer Science 108C (2017) 455–464

462	

of cement in the mixture and the amount of the error. Therefore, any amount of cement can
present larger errors because of itself or because of other components in the mixture. The same
conclusion as the cement is also visible for the remaining attributes. We intend, as future work,
evaluate in detail relations among attributes themselves and also between attributes and error.
The relative absolute error has the mean value of 11.6% ± 16%. Although errors peak
163.0%, only 0.68% of predictions are higher than 100%, 2.912% are higher than 50%, 3.98%
are higher than 40%, 6.893% are higher than 30%, 14.95% are higher than 20% and 37.86%
are higher than 10%. The median is 6.9%, what means that if the prediction of the concrete
strength has a discount of 7.0% the model will be successful in most cases. Although this low
median error is a promising result, the real margin that should be taken into account is the
worst case (163.0%), what is not feasible in real-life application. Nevertheless, the predictions
can be enhanced by providing more training data, and a thousand of training instances seem
to be not enough to create a suitable prediction model for the hard problem of estimating the
compressive strength of concrete, especially at early-age.

5

Conclusions & Future Works

Predicting compressive strength of concrete is an essential task in construction. There are
many proposals related to models and regression techniques applied to this problem. However, the effective use of these proposals requires a high degree of engineering expertise and
programming skills. Aiming to identify promising directions that demand less effort, this work
presents a complete study of 27 regression algorithms implemented in Weka [32], an educational
data mining software package, for this prediction task. In our experiments we adopt the most
complete dataset publicly available at UCI dataset repository [27], evaluating the quality of all
27 regression algorithms using some traditional metrics, such as Mean Absolute Error, Root
Mean Squared Error, Relative Absolute Error and Root Relative Squared Error. Comparing
the results of this evaluation with the best ones reported in literature for the same dataset [20],
we demonstrated that even using the algorithms implemented in an educational package it is
possible to achieve good regression models. Indeed, while the best result reported in literature achieved a Mean Absolute Error value equals to 8%, most of evaluated techniques found
Mean Absolute Error values inferior to 10%. Moreover, we verify that the regression algorithm
Bagging with REPTree presents a Mean Absolute Error inferior to 3.3%.
As future work, we aim to evaluate the algorithms implemented by other educational package for the same problem, such as knime [14], R [22], among others, in order to compare with
Weka implementations. Moreover, we will evaluate all these algorithms for the same problem,
however, using larger datasets. Finally, we also intend to replicate the same methodology in order to solve similar problems. This work adds up to the body of research on new computational
technologies applied to construction, expecting to foster the construction research community
to share their datasets in order to improve their prediction models. The new created models
are being publicly published in a virtual laboratory for multi-scale modeling of concrete [5],
available at http://vlmmc.com.

Acknowledgments
This work was partially supported by CNPq, CAPES, Fapemig, INWEB and MAsWeb.

	

Daniel Alencar et al. / Procedia Computer Science 108C (2017) 455–464

References
[1] D. Aha and D. Kibler”. ”instance-based learning algorithms”. ”Machine Learning”, ”6”:”37–66”,
”1991”.
[2] C. Atkeson, A. Moore, and S. Schaal”. ”locally weighted learning”. ”AI Review”, ”1996”.
[3] L. Breiman”. ”bagging predictors”. ”Machine Learning”, ”24”(”2”):”123–140”, ”1996”.
[4] J. G. Cleary and L. E. Trigg”. ”k*: An instance-based learner using an entropic distance measure”.
In ”12th International Conference on Machine Learning”, pages ”108–114”, ”1995”.
[5] E. Dado, E. Koenders, and D. Carvalho. Netcentric Virtual Laboratories for Composite Materials. Composites and Their Properties, http://www.intechopen.com/books/composites-and-theirproperties/netcentric-virtual-laboratories-for-composite-materials, 2012.
[6] M. Erdal. Prediction of the compressive strength of vacuum processed concretes using artificial
neural network and regression techniques. Scientific Research and Essay, 4(10):1057–1065, October
2009.
[7] E. Frank, M. Hall, and B. Pfahringer”. ”locally weighted naive bayes”. In ”19th Conference in
Uncertainty in Artificial Intelligence”, pages ”249–256”. ”Morgan Kaufmann”, ”2003”.
[8] O. Gencel. The application of artificial neural networks technique to estimate mass attenuation coefficient of shielding barrier. International Journal of Physical Sciences, 4(12):743–751, December
2009.
[9] O. Gencel, F. Kocabas, M. S. Gok, and F. Koksal. Comparison of artificial neural networks and
general linear model approaches for the analysis of abrasive wear of concrete. Construction and
Building Materials, 28(8):3486–3494, August 2011.
[10] T. K. Ho”. ”the random subspace method for constructing decision forests”. ”IEEE Transactions
on Pattern Analysis and Machine Intelligence”, ”20”(”8”):”832–844”, ”1998”.
[11] R. J. Hyndman and A. B. Koehler. Another look at measures of forecast accuracy. International
Journal of Forecasting, 22(4):679 – 688, 2006.
[12] R. Jain. The art of computer systems performance analysis - techniques for experimental design,
measurement, simulation, and modeling. Wiley professional computing. Wiley, 1991.
[13] J. Kittler, M. Hatef, R. P. Duin, and J. Matas”. ”on combining classifiers”. ”IEEE Transactions
on Pattern Analysis and Machine Intelligence”, ”20”(”3”):”226–239”, ”1998”.
[14] Knime. Knime open for innovation. https://www.knime.org, 2017. [Online; accessed 09-January2017].
[15] R. Kohavi”. ”the power of decision tables”. In ”8th European Conference on Machine Learning”,
pages ”174–189”. ”Springer”, ”1995”.
[16] R. Kohavi”. ”Wrappers for Performance Enhancement and Oblivious Decision Graphs”. PhD
thesis, ”Stanford University”, ”Department of Computer Science, Stanford University”, ”1995”.
[17] F. Kuester and T. Hutchinson. A virtualized laboratory for earthquake engineering education.
ASEE Journal of Engineering Education, 15(1):15–29, 2007.
[18] L. I. Kuncheva”. ”Combining Pattern Classifiers: Methods and Algorithms”. ”John Wiley and
Sons, Inc.”, ”2004”.
[19] D. J. Mackay”. ”introduction to gaussian processes”, ”1998”.
[20] N. Nagwani and S. Deo. Estimating the concrete compressive strength using hard clustering and
fuzzy clustering based regression techniques. The Scientific World Journal, 2014:16 pages, 2014.
[21] A. A. Öztaş, M. Pala, E. Özbay, E. Kanca, N. Çagľar, and M. A. Bhatti. Predicting the compressive
strength and slump of high strength concrete using neural network. Construction and Building
Materials, 20(9):769–775, 2006.
[22] R. The r project for statistical computing. https://www.r-project.org/, 2017. [Online; accessed
09-January-2017].
[23] P. J. Rousseeuw and A. M. Leroy”. ”Robust regression and outlier detection”. ”1987”.

463

Daniel Alencar et al. / Procedia Computer Science 108C (2017) 455–464

464	

[24] S. Shevade, S. Keerthi, C. Bhattacharyya, and K. Murthy”. ”improvements to the smo algorithm
for svm regression”. In ”IEEE Transactions on Neural Networks”, ”1999”.
[25] D. S. T. and S. M. Abduullah. Artificial neural network model for predicting compressive strength
of concrete. Tikrit Journal of Eng. Sciences, 16(3), 2009.
[26] G. Trtnik, F. Kavčič, and G. Turk. The use of artificial neural networks in adiabatic curves
modeling. Automation in Construction, 18(1):10–15, 2008.
[27] UCI. Machine Learning Repository. https://archive.ics.uci.edu/ml/datasets/Concrete+
Compressive+Strength, 2016. [Online; accessed 02-December-2016].
[28] M. Viviani, B. Glisic, K. L. Scrivener, and I. F. Smith. Equivalency points: Predicting concrete
compressive strength evolution in three days. Cement and Concrete Research, 38:1070–1078, 2008.
[29] Y. ”Wang. ”A new approach to fitting linear models in high dimensional spaces”. PhD thesis,
”Department of Computer Science, University of Waikato”, ”Hamilton, New Zealand”, ”2000”.
[30] Y. Wang and I. H. Witten”. ”induction of model trees for predicting continuous classes”. In
”Poster papers of the 9th European Conference on Machine Learning”. ”Springer”, ”1997”.
[31] Y. ”Wang and I. H. Witten. ”modeling for optimal probability prediction”. In ”Proceedings of the
Nineteenth International Conference in Machine Learning”, pages ”650–657”, ”Sydney, Australia”,
”2002”.
[32] Weka. Weka - Interface Classifier. http://weka.sourceforge.net/doc.dev/weka/classifiers/
Classifier.html, 2016. [Online; accessed 02-December-2016].
[33] D. H. Wolpert”. ”stacked generalization”. ”Neural Networks”, ”5”:”241–259”, ”1992”.
[34] I.-C. Yeh. Modeling of strength of high performance concrete using artificial neural networks.
Cement and Concrete Research, 28(12):1797–1808, 1998.

Appendix A
Bagging with REPTree: weka.classif iers.meta.Bagging − P 100 − S1 − I200 − W
weka.classif iers.trees.REP T ree − − − M 0 − V 1.0E − 5 − N 5 − S1 − L − 1–P
Bagging with M5P: weka.classif iers.meta.Bagging − P 100 − S1 − I100 − W
weka.classif iers.trees.M 5P − − − N − U − M 4.0
Bagging with Conj. Rule: weka.classif iers.meta.Bagging − P 100 − S1 − I10 − W
weka.classif iers.rules.ConjunctiveRule − − − N 2 − M 2.0 − P − 1 − S1
Bagging with Decision Table: weka.classif iers.meta.Bagging − P 100 − S1 − I10 − W
weka.classif iers.rules.DecisionT able − − − X1 − I − S
”weka.attributeSelection.SubsetSizeF orwardSelection − I − K50 − T 0 − F 5 − S1 − E
weka.attributeSelection.Classif ierSubsetEval − − − B
weka.classif iers.rules.ZeroR − T − H
Bagging with M5Rules: weka.classif iers.meta.Bagging − P 100 − S1 − I10 − W
weka.classif iers.rules.M 5Rules − − − U − M 4.0

