Available online at www.sciencedirect.com

ScienceDirect
Science
(2017) 2303–2307
This Procedia
space isComputer
reserved
for 108C
the Procedia
header, do not use it

This space is reserved for the Procedia header, do not use it

International Conference on Computational Science, ICCS 2017, 12-14 June 2017,
Zurich, Switzerland

MCM: A new MPI Communication Management for Cloud
MCM: A new MPI Communication
EnvironmentsManagement for Cloud
Environments
1
1
1
Laura Espı́nola , Daniel Franco , and Emilio Luque
Laura
Espı́nola1 , Daniel Franco1 , and Emilio Luque1
Universidad Autónoma de Barcelona, Barcelona, Spain
{lauramaria.espinola,daniel.franco,emilio.luque}@uab.cat
Universidad Autónoma de Barcelona, Barcelona, Spain
{lauramaria.espinola,daniel.franco,emilio.luque}@uab.cat

Abstract
On-demand
Abstract Cloud Computing offers an attractive new dimension to High Performance Computing
(HPC).
Many
HPC applications
moving to
clouds
due toto
some
characteristics
this
On-demand
Cloud
Computing
offers anare
attractive
new
dimension
High
Performance ofComenvironment,
like
elasticity,
pay
per
use,
and
the
maintenance
cost.
HPC
applications
normally
puting (HPC). Many HPC applications are moving to clouds due to some characteristics of this
use
Message Passing
Interface
However,
this kind of
applications
on cloud normally
environenvironment,
like elasticity,
pay(MPI).
per use,
and the for
maintenance
cost.
HPC applications
ments,
network
performance
remains
a
challenge
due
to
the
effects
of
network
virtualization.
use Message Passing Interface (MPI). However, for this kind of applications on cloud environTo
use anetwork
cloud environment
scientific
applications
of this
ments,
performancewith
remains
a challenge
due to
the kind,
effectslow
of latency
networkcommunication
virtualization.
mechanisms
are
required,
and
the
hidden
network
topology
information
makes communication
difficult to use
To use a cloud environment with scientific applications of this kind, low latency
the
existing are
optimizations
based
network
topology
information.
In makes
this paper
a method
mechanisms
required, and
the on
hidden
network
topology
information
difficult
to use
named
MPI
Communication
Management
(MCM)
is
presented.
This
method
characterizes
the
the existing optimizations based on network topology information. In this paper a method
underlying
network
topology
and
analyzes
parallel
applications
behavior
in
the
cloud,
improvnamed MPI Communication Management (MCM) is presented. This method characterizes the
ing
the application’s
messagesand
latency
time.
MCMapplications
achieves lower
application
time
underlying
network topology
analyzes
parallel
behavior
in the execution
cloud, improvin
case
of
congestion,
obtaining
better
performance
in
clouds.
Finally,
experiments
verify
the
ing the application’s messages latency time. MCM achieves lower application execution time
functionality
and
improvements
of
MCM
with
MPI
Applications
in
Amazon
EC2
public
cloud.
in case of congestion, obtaining better performance in clouds. Finally, experiments verify the
functionality
and improvements
of MCM
©
2017 The Authors.
Published by Elsevier
B.V. with MPI Applications in Amazon EC2 public cloud.
Peer-review
under
responsibility
of
the
scientific
committee of High
the International
Conference
on Computational
Science
Keywords: Cloud Computing; MPI Communications;
Performance
Computing;
Alternative
path
routing
Keywords: Cloud Computing; MPI Communications; High Performance Computing; Alternative path
routing

1 Introduction
1
Introduction
High Performance Computing (HPC) applications normally need a lot of computing and network
to perform
their (HPC)
operations.
In ordernormally
to satisfyneed
thosea requirements,
applications
High capacity
Performance
Computing
applications
lot of computing
and netof
this
type
run
normally
in
clusters
and
grids,
but
nowadays
Cloud
Computing
emerged as
work capacity to perform their operations. In order to satisfy those requirements, applications
an
alternative.
HPCinApplications
deployed
in publicCloud
cloudsComputing
like Amazon’s
Elastic
of this
type runMany
normally
clusters and are
grids,
but nowadays
emerged
as
1
Compute
Cloud
(EC2)
with
good
results.
In
cloud,
users
can
acquire
resources
in
a
flexible
an alternative. Many HPC Applications are deployed in public clouds like Amazon’s Elastic
way
due to
the elasticity
thatgood
it provides.
usersusers
pay can
onlyacquire
for their
used resources,
and
Compute
Cloud
(EC2)1 with
results. Also,
In cloud,
resources
in a flexible
obtain
benefits
of
utility,
computing
and
storage,
plus
eliminates
the
initial
cluster
setup
cost
way due to the elasticity that it provides. Also, users pay only for their used resources, and
time
[5][1].
obtain
benefits of utility, computing and storage, plus eliminates the initial cluster setup cost
1 Successful
time
[5][1]. use cases are found in http://aws.amazon.com/solutions/case-studies/#hpc
1 Successful

use cases are found in http://aws.amazon.com/solutions/case-studies/#hpc

1
1877-0509 © 2017 The Authors. Published by Elsevier B.V.
Peer-review under responsibility of the scientific committee of the International Conference on Computational Science
10.1016/j.procs.2017.05.069

1

2304	

Laura Espínola et al. / Procedia Computer Science 108C (2017) 2303–2307
MCM: A new MPI Communication Management in cloud.
Espı́nola, Franco and Luque

(a) MPI Ping Pong without congestion

(b) MPI Ping Pong with congestion

Figure 1: MPI communications behavior in Cloud

However, virtualized networks in Cloud Computing cause a significant degradation of HPC
application communications, specially affecting their message latency [2]. Hence, incrementing
their execution times and causing higher costs due to this problem. Many parallel applications
use the standard protocol Message Passing Interface (MPI) and there are many studies in the
area that verify the degradation of performance in MPI communications caused by network
virtualization in cloud [4][3]. To exhibit the problem, a parallel application called Ping Pong is
executed without and with disturbance from another application. Figure 1(a) and Figure 1(b)
show the results, and comparing both figure is posible to observe the effect in terms of latency
caused in communications of some nodes when a congestion occurs.
The proposed method, MPI Communication Management (MCM), tries to improve latency
communication time for MPI applications running in public clouds. It performs a method to
discover the underlying network topology, and mainly introduces a distributed management of
MPI application messages. The technique is capable of dynamically select an alternative path
with a minimum overhead, based in communication locality of MPI applications running in
cloud and current network status. MCM runs at user level and can be applied at any cloud.
This paper is organized as follows: in section 2, the designed MCM method with its process
and functionalities is detailed; in section 3, experimental results and analysis are showed; finally
in section 4, conclusions and future works are discussed.

2

MCM

MCM is a communications management method for MPI, based in the study of communication
latencies between processes. The core of MCM is a daemon which captures sent and received
messages between processes in order to compute alternative paths avoiding congested routes.
After selection of a better path is made, the message is forwarded through this new calculated
path. MCM have two processes, one defined as Offline process because is executed prior the
application execution; and another defined as Online process considering it is executed along
user’s application.

2.1

Offline Process

This process is in charge of Network Topology Characterization and Creation of Alternative Paths Table. Alternative paths table will be used for path configuration to select the
best route for messages.
2

	

Laura Espínola et al. / Procedia Computer Science 108C (2017) 2303–2307
MCM: A new MPI Communication Management in cloud.
Espı́nola, Franco and Luque

1 2 3 4 5 6 7 8
1 0 2
2
0
3
0
4
0
5
0 1
6
0
7
0
8
0

(a) Steps with congestion in link 7
and 8 to Determine Related nodes

(b) Steps with congestion in link 4
and 3 to Determine Related nodes

(c) Sensibility
between nodes

Matrix

Figure 2: Sensibility Matrix Construction

The approach initiates with the Network topology characterization. This information
is not available in clouds, so is necessary to find out similar data input. Thereby, an analysis
of MPI communications between all pairs of virtual machines in cloud is made. The network
performance of processes in MPI applications is related to the network performance of their
corresponding virtual machines. This information is used to characterize the network, building
two matrixes: Matrix of Latency Distances and Sensibility Matrix .
With the Matrix of Latency Distances the main idea is to obtain a basic detail of
distances between nodes. Every intersection of rows and columns represent a path between
one node source and destination. It is built by latencies information obtained from the Ping
Pong application running alone, without disturbance of any other application. Then with the
Sensibility Matrix the robustness of paths is obtained. The number of times in which a path
is affected for congestion between any other pair of nodes is stored as a weight. This procedure
is shown in Figure 2. The process is repeated for each combination of congested pair of nodes,
in order to obtain all the relationships between nodes of Cloud, and finally the robustness of
every path. Using these two matrix, MCM configures in the Alternative Path Table at most
two alternative paths to reach a destination node from a source node with lowest latency and
good robustness.

2.2

Online Process

This process is in charge of capturing all messages from an MPI application before they reach
to the MPI library and select the best path without congestion, verifying the latency of every
path. The process tasks and the software architecture are described as follows:
Monitoring. Includes the latency values measure and contending flows identification, these
actions are performed by each MCM daemons when a message is sent(Figure 3(a)).
Notification. It is performed at destination end nodes in which an Acknowledge (ACK)
message with path information is created and sent back to the source (Figure 3(a)). The
capture of an ACK message, generates an update of latency information for the corresponding
path into the alternative path table.
Detecting Congestion. This process identifies the time when an action must be done. Three
areas are selected, taking into account a threshold latency provided by the user; one with low
latency, another with medium latency, and high latency (extreme congestion). In low and
3

2305

2306	

Laura Espínola et al. / Procedia Computer Science 108C (2017) 2303–2307
MCM: A new MPI Communication Management in cloud.
Espı́nola, Franco and Luque

(a) First Scenario

(b) Second Scenario

(c) MCM Software Architecture

Figure 3: MPI communications Management

medium latency areas, the method does not search for alternative paths, but when a transition
to medium and high latency occurs, the approach looks for a new path (Figure 3(b)).
Path configuration. This task involves the configuration of new alternative paths according
to latency values, also performed at source nodes. If there are saved solutions for a congestion
situation, the paths are taken from an alternative path table. Otherwise, new alternative paths
are created in this table (Figure 3(b)).
Alternative path selection. When new messages are injected into the network, selection
procedures distribute messages among the paths configured in previous task (Figure 3(b)).
Repetitive communication behavior allows to compute better routes.
MCM software architecture. MCM module is between user application and MPI library
(Figure 3(c)). This module includes MCM functions described before, and it is in charge of
path selection for messages from user applications. To use MCM, users only need to launch
its application together with MCM and a script is provided for that. User application recompilation is not needed.

3

Experiment Results

Experiments with NAS-CG Class B and C are performed in 16 nodes, using t2.medium instances
in Amazon EC2 cloud. Both application with 1000 iteration to converge and with 16 processes.
The experiments try to expose the benefits of MCM if a congestion occurs, and the added
overhead. In Scenario A (Figure 4(a)), MCM with 2 alternative paths per node source presents
5% of overhead compared with the normal application execution and reduce the execution
time in approximately 30% compared with the application with congestion. For Scenario B,
the scalability of the method is evaluated with different problem sizes of NAS-CG and also
applied a congestion between two nodes. This time supposing the worst scenario in which only
1 alternative path for each node source is found, results are shown in Figure 4(b). MCM reduces
the NAS-CG Class B execution time 16, 2% compared to execution time of the same application
with congestion, and with NAS-CG Class C, it reduces the execution time 12, 7%.
In all the scenarios MCM improves the application execution time if a congestion occurs in
the network. MCM provides to the user a better performance in cloud environments. With
lowest message latency better applications execution times are obtained.
4

Laura Espínola et al. / Procedia Computer Science 108C (2017) 2303–2307
MCM: A new MPI Communication Management in cloud.
Espı́nola, Franco and Luque

App

App-MCM

App-Congestion

App-Congestion-MCM

Application-No-Congest

600

1400

480

1120

360

840

Seconds

Seconds

	

240

Application-Congest-MCM

560
280

120
0

Application-Congest

NAS CG - CLASS B

(a) Scenario A - MCM improvement and
overhead added

0

CG-CLASS B

CG -CLASS C

(b) Scenario B - MCM with different problem
sizes of NAS-CG

Figure 4: Analisys of NAS-CG Execution Time

4

Conclusions and Future Work

HPC is currently migrating to Cloud environments due to versatility and facilities that such
environments provide. Scientific applications, which requires high performance are the best
candidates to employ this kind of architecture. One of the primary concerns about running
MPI application in Cloud is that network communication does not have efficient support for
virtualized environments. To address this problem, the MCM method is proposed.
MCM look up for alternative paths under a congestion situation, considering the variability
of Cloud networks and the dynamic traffic behavior. MCM manages messages communications
between process and improves latency times, reflected in the decreased application execution
time in the cloud. Future work is to improve MCM method and verify it, with benchmark
applications and scientific applications, detecting dynamic congestions.

5

Acknowledgments

This research has been supported by the MINECO Spain under contract TIN2014-53172-P and
it has been partially supported by the CloudMas as Government Competency of AMAZON
Web Services (AWS).

References
[1] M. Armbrust, A. Fox, R. Griffith, A. D. Joseph, R. Katz, A. Konwinski, G. Lee, D. Patterson,
A. Rabkin, and I. Stoica. A view of cloud computing. Magazine Communications of the ACM,
53(4), 2010.
[2] K. Dashdavaa, S. Date, H. Yamanaka, and E. Kawai. Architecture of a high-speed mpi bcast
leveraging software-defined network. 2013 IEEE Symposium on Computers and Communications
(ISCC), pages 885–894, 2014.
[3] Y. Gong, B. He, and J. Zhong. Network performance aware mpi collective communication operations
in the cloud. IEEE Transactions on Parallel and Distributed Systems, pages 1–1, 2013.
[4] Q. He, S. Zhou, B. Kobler, D. Duffy, and T. McGlynn. Case study for running hpc applications in
public clouds. HPDC, 2010.
[5] P. Rad, R. V. Boppana, P. Lama, G. Berman, and M. Jamshidi. Low-Latency Software Defined
Network for High Performance Clouds. 10th System of Systems Engineering Conference (SoSE),
pages 486–491, 2015.

5

2307

