Available online at www.sciencedirect.com

ScienceDirect

This space is reserved for the Procedia header, do not use it
This space
reserved
for the
header, do not use it
ProcediaisComputer
Science
108CProcedia
(2017) 375–383
This space is reserved for the Procedia header, do not use it

International Conference on Computational Science, ICCS 2017, 12-14 June 2017,
Zurich, Switzerland

An Ensemble of Kernel Ridge Regression for Multi-class
An Ensemble of Kernel Ridge Regression for Multi-class
Classification
An Ensemble of Kernel
Ridge Regression for Multi-class
Classification
Katuwal Rakesh
and P.N. Suganthan*
Classification
Katuwal Rakesh and P.N. Suganthan*
Nanyang Technological University, Singapore
Katuwal
Rakesh and P.N. Suganthan*
Nanyangrakeshku001@e.ntu.edu.sg
Technological University, Singapore
*epnsugan@ntu.edu.sg
rakeshku001@e.ntu.edu.sg
Nanyang Technological University, Singapore
*epnsugan@ntu.edu.sg
rakeshku001@e.ntu.edu.sg
*epnsugan@ntu.edu.sg

Abstract
Abstract
We propose an ensemble of kernel ridge regression based classifiers in this paper. Kernel ridge
regression
admits
a closedofform
solution
it faster
compute in
and
also
making
it suitable
We
propose
an ensemble
kernel
ridge making
regression
basedtoclassifiers
this
paper.
Kernel
ridge
Abstract
to
use
for
ensemble
methods
for
small
and
medium
sized
data
sets.
Our
method
uses
random
regression
admits
a
closed
form
solution
making
it
faster
to
compute
and
also
making
it
suitable
We propose an ensemble of kernel ridge regression based classifiers in this paper. Kernel ridge
vector
network
tosolution
generate
training
samples
for
kernel
ridge
regression
classifiers.
to
use functional
for ensemble
methods
for
small and
medium
sizedtodata
sets. and
Ouralso
method
uses
random
regression
admits link
a closed
form
making
it faster
compute
making
it suitable
Several
kernel
ridge
regression
classifiers
are
constructed
from
different
training
subsets
in each
vector
functional
link
network
to
generate
training
samples
for
kernel
ridge
regression
classifiers.
to use for ensemble methods for small and medium sized data sets. Our method uses random
base
classifier.
The
partitioning
of
the
training
samples
into
different
subsets
leads
to
a
reducSeveral
kernel
ridge
regression
classifiers
are
constructed
from
different
training
subsets
in
each
vector functional link network to generate training samples for kernel ridge regression classifiers.
tion
in
computational
complexity
when
calculating
matrix
inverse
compared
with
the
standard
base
classifier.
The
partitioning
of
the
training
samples
into
different
subsets
leads
to
a
reducSeveral kernel ridge regression classifiers are constructed from different training subsets in each
approach
of using
Ncomplexity
samples for
kernel
matrix
inversion.
The
proposed
method
istoevaluated
tion
computational
calculating
matrix
inverse
compared
with
standard
base in
classifier.
Theallpartitioning
ofwhen
the
training
samples
into
different
subsets
leadsthe
a reducusing
well
known
multi-class
UCI
data
sets.
Experimental
results
show
the
proposed
ensemble
approach
of
using
all
N
samples
for
kernel
matrix
inversion.
The
proposed
method
is
evaluated
tion in computational complexity when calculating matrix inverse compared with the standard
method
outperforms
single
kernel
ridge
regression
classifier
its bagging
version.
using
well
UCI
sets.
Experimental
results
show
themethod
proposed
ensemble
approach
ofknown
using multi-class
all the
N samples
fordata
kernel
matrix
inversion.
Theand
proposed
is evaluated
method
outperforms
the
single
kernel
ridge
regression
classifier
and
its
bagging
version.
using well known multi-class UCI data sets. Experimental results show the proposed ensemble

Keywords:
kernel ridge
regression,
multi-class
classification, random vector functional link network
©
2017 The Authors.
Published
by Elsevier
B.V.
Peer-review
under
responsibility
of the scientific
committee
of the International
on Computational
Science
method
outperforms
single
kernel
ridge
regression
classifier
and itsfunctional
bagging
version.
Keywords:
kernel
ridgethe
regression,
multi-class
classification,
random Conference
vector
link network
Keywords: kernel ridge regression, multi-class classification, random vector functional link network

1 Introduction
1 Introduction
“Many heads are better than one.” “Many are smarter than the Few.” An avalanche of such
1
Introduction
dicta, accentuating
the power
and wisdom
crowds
or experts,
areFew.”
very popular
in our society.
“Many
heads are better
than one.”
“Manyofare
smarter
than the
An avalanche
of such
Several
studies
based
onpower
sociological,
psychological
and
of human
life in
concord
dicta,
the
and wisdom
ofare
crowds
orother
experts,
areFew.”
very
popular
our society.
“Manyaccentuating
heads are
better
than one.”
“Many
smarter
thanaspects
the
An avalanche
of these
such
statements
[19,
16].
Inspired
by
the
power
of
crowds,
such
a
committee
have
been
used
not
only
Several
studies
based
on
sociological,
psychological
and
other
aspects
of
human
life
concord
these
dicta, accentuating the power and wisdom of crowds or experts, are very popular in our society.
on
the
decision
making
process
of
humans
but
also
in
machine
related
tasks.
Machine
Learning
statements
[19,
16].
Inspired
by
the
power
of
crowds,
such
a
committee
have
been
used
not
only
Several studies based on sociological, psychological and other aspects of human life concord these
is
such
a
field
where
such
committees
or
ensembles
are
widely
popular
and
consequential.
on
the
decision
making
process
of
humans
but
also
in
machine
related
tasks.
Machine
Learning
statements [19, 16]. Inspired by the power of crowds, such a committee have been used not only
is
a field where
such
committees
or ensembles
widely
popular
and consequential.
In machine
learning,
ensemble
methods
employ
models
to tasks.
achieve
improved
perforonsuch
the
decision
making
process
of humans
but
also multiple
inare
machine
related
Machine
Learning
mance
compared
to
a
single
model.
Extensive
researches,
both
theoretical
and
empirical,
advoIn
machine
learning,
ensemble
methods
employ
multiple
models
to
achieve
improved
perforis such a field where such committees or ensembles are widely popular and consequential.
cateInthe
advantages
of
ensemble
methods.
Bias-variance
decomposition
[10],
margin-theory
[18]
mance
compared
to
a
single
model.
Extensive
researches,
both
theoretical
and
empirical,
advomachine learning, ensemble methods employ multiple models to achieve improved perforand
strength-correlation
[3],
all
explain
the
theory
behind
the
superior
performance
of
ensemble
cate
the
advantages
of
ensemble
methods.
Bias-variance
decomposition
[10],
margin-theory
[18]
mance compared to a single model. Extensive researches, both theoretical and empirical, advomethods.
and
strength-correlation
[3],
all
explain
the
theory
behind
the
superior
performance
of
ensemble
cate the advantages of ensemble methods. Bias-variance decomposition [10], margin-theory [18]
methods.
[2], Boosting[3],
[8],all
and
Random
are the
popular
ensembleoftechniques
andBagging
strength-correlation
explain
the Forest
theory [3]
behind
themost
superior
performance
ensemble
[15].
These
models
mainly
use
decision
trees
as
base
classifiers.
There
is
a
myriad of
research
Bagging
[2],
Boosting
[8],
and
Random
Forest
[3]
are
the
most
popular
ensemble
techniques
methods.
[15].Bagging
These models
mainly
use
decision
trees
as
base
classifiers.
There
is
a
myriad
of
research
[2], Boosting [8], and Random Forest [3] are the most popular ensemble techniques
[15]. These models mainly use decision trees as base classifiers. There is a myriad of research1
1
1877-0509 © 2017 The Authors. Published by Elsevier B.V.
Peer-review under responsibility of the scientific committee of the International Conference on Computational Science
10.1016/j.procs.2017.05.109

1

376	

Ensemble of Kernel Ridge Regression
...
Katuwal Rakesh
et al. / Procedia Computer Science 108C (2017) Katuwal
375–383 and Suganthan

on ensemble methods with decision trees in the literature [24, 22, 21]. However, an ensemble
can be created with any base classifier provided its performance is slightly better than random
guessing [20, 15].
Kernel Ridge Regression (KRR) and Support Vector Machine (SVM) are the best known
members using kernel methods. When there is non-linear structure in the data, kernel based
methods are quite useful [6]. KRR is simpler and faster to train with its closed form solution, and
can achieve performance comparable to sophisticated methods such as SVM. In the literature,
however, there is limited work on KRR ensemble. Thus, in this paper, we propose an ensemble of
classifiers based on kernel ridge regression and random vector functional link (RVFL) network.
One difficulty in applying KRR technique is the inversion of kernel matrix, which requires
O(N 3 ) time and O(N 2 ) memory. Such scaling is prohibitive when N , the number of data
samples, is large. Although time may not be a constraint except in case of large data sets, the
application of KRR can be halted by out-of-memory failures. Thus, to reduce the computational
complexity of KRR, we use RVFL to partition the data samples into several subsets. This helps
to avoid the out-of-memory failures to an extent. This is because, the matrix inverse is computed
for n(< N ) training samples where, n is the number of samples in each subset. Our proposed
method thus, befits naturally to parallel and distributed computation.
KRR can be generally regarded as a strong classifier. However, we create an unstable KRR
classifier, appropriate for ensemble methods, by training it with only a subset of the whole
training data set generated using RVFL. For the purpose of brevity, we name our method as
ERK: an ensemble of RVFL with KRR.
In ERK, multiple KRR classifiers are constructed in each base classifier (a base classifier
comprises of RVFL and m KRR classifiers where, m is the number of classes) with training
samples from each subset. Each subset is associated with a class and consists of training
samples with all the samples from its true class and only a fraction of samples from other
classes (different subsets will have different fractions of samples from other classes). Thus, the
training sets for each m KRR classifiers are different. Our approach fits perfectly to multi-class
classification however, it is unsuitable for data sets with binary classes. This is because training
samples for two KRR classifiers will be the same resulting in two identical KRR classifiers in a
base classifier.
The remainder of this paper is organized as follows. In Sections 2 and 3, we provide a
background on kernel ridge regression and random vector functional link network. Our proposed
ensemble method, ERK, is elucidated in Section 4. We present our experimental setup along
with the performance of ERK compared with other classifiers in Section 5. Lastly, in Section
6, we give our conclusion and discuss future works.

2

Kernel Ridge Regression

Suppose we have a training set ((x1 , y1 ), . . . , (xN , yN )), where N is the total number of training
samples. X is a features matrix, [x1 , x2 , . . . , xN ], of size N × d and Y = [1, 2, . . . , m] is a N × 1
vector of class labels.
Kernel ridge regression is based on Ordinary Least Squares (OLS) and ridge regression [17].
The OLS minimizes the squared loss:
minY − Xβ2
β

(1)

where . denotes the L2 norm. A shrinkage or ridge parameter λ is added to the above
expression to control the trade-off between the bias and variance of the estimate that leads to
2

	

Ensemble of Kernel Ridge Regression
...
Katuwal Rakesh
et al. / Procedia Computer Science 108C (2017) Katuwal
375–383 and Suganthan

the following problem:
minY − Xβ2 + λβ2
β

(2)

The closed form solution for the above problem is β = (X T X + λI)−1 X T Y where, I is an
identity matrix. The predicted label of a new unlabeled example x is given by β T x.
Kernel ridge regression extends linear regression into non-linear and high-dimensional (or
even infinite dimensional) space using the “kernel trick”. The data xi in X is replaced with the
feature vectors: xi → Φ = Φ(xi ) induced by the kernel where Kij = k(xi , xj ) = Φ(xi )Φ(xj ).
Thus, the predicted class label of a new example x is now given by:
Y T (K + λI)−1 k

(3)

where k = (k1 , . . . , kN )T , kn = xn · x and n = 1, . . . , N .
In Kernel ridge regression, one can simply employ the commonly used kernel functions such
as Gaussian or linear or polynomial without accessing the feature vectors Φ(x).
Since KRR is naturally poised for handling regression problems, we extend its use for classification by transforming classification problems as regression problems with class labels. To
achieve this, we define the output target Y with 0-1 coding [1]. More specifically, if there are
m classes with N samples, the output Y is a N ∗ m matrix which can be generated by the
following equation:

1 if ith sample belongs to j th class
Yij =
0 otherwise

3

Random Vector Functional Link Network

Random Vector Functional Link Network is a randomized variant of Functional Link Neural
Network (FLNN) [13, 4]. It is mainly characterized by the absence of backpropagation (BP)
and the presence of direct links between the input and output nodes. The weights between the
input and hidden neurons in RVFL are randomly generated from a suitable range. The direct
links in RVFL regularize the network from the effects of randomization leading to a simpler
model with small number of hidden neurons while increasing the prediction accuracy [23]. Using
signal processing dialects, the direct links can be interpreted as the delay lines of finite impulse
response (FIR) filter [14]. The output layer of RVFL consists of nodes corresponding to the
number of classes, with each node assigning a score for each class. The predicted class for a
sample x is the class represented by a node with arg max(si (x)), i ∈ {1, . . . , m} where, s is
the score given by each output node i. Since only output weights need to be computed with a
closed form solution available, RVFL is faster to train and test [14, 23].

4

Construction of ERK

Bagging is a popular method of ensemble generation. In Bagging, multiple versions (bags)
(D1 , D2 , . . . , DL ) of original data set D is constructed by sampling with replacement, and a
classifier is constructed for each Dj . The same bagging principle is employed here. However,
in ERK, each bag (Dj ) is divided into m subsets, equivalent to the number of classes in each
data set, and an independent kernel ridge regression classifier is computed for each subset. In
other words, instead of learning a single KRR classifier on all the training samples of a bag, we
divide the training samples of the bag into m subsets and compute a local KRR classifier for
each subset. A visual representation of this idea is shown in Fig. 1 for m = 4.
3

377

378	

Ensemble of Kernel Ridge Regression
...
Katuwal Rakesh
et al. / Procedia Computer Science 108C (2017) Katuwal
375–383 and Suganthan

In the training phase, each sample xi is passed to RVFL. As mentioned above, the output
nodes of the RVFL bear a score for each class. In general classification by RVFL, the predicted
class is the class represented by the node with the maximum score. But, instead of classification
by RVFL, it is employed in our method to partition the training samples into m subsets. The
sample xi is used as the training sample for the classes with the highest and the second-highest
scores. Here, by classes we imply the subset associated with each class. In this way, a training
set is created for each subset and a KRR classifier is created. The final model is an ensemble of
such base classifiers. In cases, where the true class of xi is neither the highest nor the secondhighest class, the training sample is still placed in its true class subset. The algorithm for the
construction of ERK is presented in Table 1.

Table 1: ERK Construction
Algorithm:
Training:
For j = 1, . . . , L
1. For i = 1, . . . , N
(a) Pass the training data xi to the RVFL network.
(b) Select the classes with the highest and the second highest scores and store xi as
the training data for the two subsets associated with those two classes.
2. Construct KRR classifier from each subset.
Testing:
The test data is first passed to RVFL. Similar to the training phase, two classes with the
highest and the second highest scores are selected based on the output of RVFL. The test data
is pushed down to the KRR classifiers belonging to those two classes. The final prediction
is based on the majority votes of the KRR classifiers.

5
5.1

Performance Evaluation
Experimental Setup and Data set specification

To verify the effectiveness of ERK, 10 multi-class UCI data sets [11] are used in the experiment.
Table 2 gives an overview of the data sets used in this paper. The experimental setup and data
partitions for training, parameter tuning and testing is based on [7]. Randomized stratified
sampling is employed to generate a training and a test set (each with 50% of the available
patterns). These sets are used for parameter tuning. Then, using the tuned parameters a 4-fold
cross validation is developed using the whole data. The test result is the average over the 4
test sets. For those data sets (optical, st-landstat), with already available training and testing
data, the classifiers (with the tuned parameters) are trained and tested on the respective sets.
The feature vectors of the data sets are normalized by removing the mean and dividing by its
variance. The ensemble size, L, for the experiment is fixed at 500.
4

	

Ensemble of Kernel Ridge Regression
...
Katuwal Rakesh
et al. / Procedia Computer Science 108C (2017) Katuwal
375–383 and Suganthan

Training
Data

Subset 1

KRR Classifier 1

Subset 2

KRR Classifier 2

Subset 3

KRR Classifier 3

Subset 4

KRR Classifier 4

RVFL

Figure 1: A base classifier of ERK

5.2

Parameter Settings

To foster diversity amongst the classifiers of the ensemble, RVFL is randomized in each base
classifier. The parameter settings for RVFL is based on [23, 7] where each RVFL randomly
picks the activation function and network parameters from the parameter settings listed below:
1. Number of hidden neurons, N = 3:203
2. λ (=(1/2)C ) in ridge regression, C = -5:14
3. Activation Functions: radbas, sine and tribas
4. Range of the randomization for weights [-S,+S] and bias [0,S], where S = 2t with t =
-1.5:0.5:1.5
The RVFL has direct link from input layer to output layer with bias term in the output neuron.
All the data sets in our experiment use the Gaussian radial basis kernel
K(x1 , x2 ) = exp(−γx1 − x2 22 )
The kernel parameter γ and ridge parameter λ for KRR are tuned for each data set. γ is set
to 2m where m = -15:3 and λ is set to 2n where n = -5:14.

5.3

Experimental Results

In this section, the performance of ERK is compared with three other classifiers. The two
classifiers are single KRR, denoted by KRR(S), and its bagging version, denoted by KRR(B).
These two classifiers are tuned with the same set of parameters as used in ERK. The Bagged
KRR, KRR(B), is trained with the same 500 bags and same Gaussian kernel in each base
classifier as done in ERK to avoid any bias. The other classifier is RVFL. The performance of
RVFL on these data sets are extracted from [23] as it uses the same data partitions as ours. In
the first phase, we compare the performance of ERK with KRR(S) and RVFL. In the second
phase, we discuss the performance of two ensemble methods, ERK and KRR(B).
5

379

380	

Ensemble of Kernel Ridge Regression
...
Katuwal Rakesh
et al. / Procedia Computer Science 108C (2017) Katuwal
375–383 and Suganthan

Table 2: Overview of the Data sets
Datasets
Molec-biol-splice
Optical
St-image
St-landstat
St-vehicle
Steel plates
Thyroid
Wall-following
W-qua-white
Yeast

Patterns
3190
3823
2310
4435
846
1941
3772
5456
4898
1484

Features
60
62
18
36
18
27
21
24
11
8

Classes
3
10
7
6
4
7
3
4
7
10

Table 3: Classification accuracies of different methods
Datasets
Molec-biol-splice
Optical
St-image
St-landstat
St-vehicle
Steel plates
Thyroid
Wall-following
W-qua-white
Yeast

KRR(S)
79.49
96.93
95.53
90.15
71.56
77.01
94.46
87.51
65.62
59.56

KRR(B)
78.95
96.99
95.62
90.25
71.45
76.60
94.25
87.68
65.80
59.50

ERK
86.42
98.07
97.05
91.34
83.18
77.01
96.41
92.14
66.16
60.44

RVFL
83.97
97.22
94.84
86.40
82.58
76.60
93.96
86.62
56.17
61.12

Bold values represent the highest accuracy.

Friedman test is used, as suggested in [5], to perform statistical comparison of the classifiers.
It uses the rank of each classifier in each data set to reflect the overall performance of the
classifier. In this approach, the classifiers are ranked for each data set separately, with the best
performing algorithm getting the rank of 1, the second best rank 2..., average ranks are assigned
in case of ties. The overall average rank of each classifier based on the 10 ranks in each data
set is summarized in Table 4.
Let N and k denote the number of data sets and classifiers respectively. The Friedman test
Table 4: Average rank values based on classification accuracies of each method. Lower rank
reflects better performance.
Rank

6

KRR(S)
2.85

KRR(B)
2.95

ERK
1.15

RVFL
3.05

	

Ensemble of Kernel Ridge Regression
...
Katuwal Rakesh
et al. / Procedia Computer Science 108C (2017) Katuwal
375–383 and Suganthan


compares the average ranks of the classifiers, Rj = i rij where, rij is the rank of the j th of the
k classifier on the ith of N data set. The null hypothesis states that the performance of all the
classifiers are similar and so their ranks Rj should be equal. When N and k are large enough,
the Friedman statistic


2

k(k + 1) 
12N 
,
(4)
Rj2 −
χ2F =
k(k + 1) j
4

is distributed according to χ2F with k-1 degrees of freedom under the null hypothesis. However,
in this case, χ2F is undesirably conservative. A better statistics is given by
FF =

(N − 1)χ2F
,
N (k − 1) − χ2F

(5)

which is distributed according to F -distribution with k-1 and (k-1)(N -1) degrees of freedom.
If the null-hypothesis is rejected, the Nemenyi post-hoc test [12] can be used to check whether
the performance of two among k classifiers is significantly different. The performance of two
classifiers is significantly different if the corresponding average ranks of the classifiers differ by
at least the critical difference (CD)

k(k + 1)
,
(6)
CD = qα
6N
√
where critical values qα are based on the Studentized range statistic divided by 2. α is the
significance level and is equal to 0.05 in this paper.
By simple calculations we obtain, χ2F = 14.7 and FF = 8.65. With 4 classifiers and 10 data
sets, FF is distributed according to the F -distribution with 4 − 1 = 3 and (4 − 1)(10 − 1) = 27
degrees of freedom. The critical value for F(3,27) for α = 0.05 is 2.96, so we reject the null
hypothesis.
 Based on the Nemenyi test, the critical difference is CD = qα (k(k + 1))/(6N ) =
2.569 ∗ 4 ∗ 5/(6 ∗ 10)  1.48.

Figure 2: Comparison of classifiers against each other with the Nemenyi test. Groups of
classifiers that are not significantly different (at α = 0.05) are connected.
From Figure 2, we can see that our proposed method ERK is significantly better than
KRR(S) and RVFL. ERK either performs equally or outperforms the other two classifiers in
every data set except one, the Yeast data set. Although, KRR(S) is slighly better than RVFL,
there is no significant difference between the two classifiers. The mean accuracies of KRR(S),
KRR(B), ERK and RVFL in these data sets are 81.78, 81.7, 84.82 and 81.94 respectively. From
Table 3, it is worth noticing that ERK consistently performs better than KRR(S) and RVFL
7

381

382	

Ensemble of Kernel Ridge Regression
...
Katuwal Rakesh
et al. / Procedia Computer Science 108C (2017) Katuwal
375–383 and Suganthan

even if one among KRR or RVFL is performing worse. This is in congruent with the generally
accepted notion that ensemble methods perform better than a single classifier.
5.3.1

Comparison with Bagged KRR

In all the data sets above, ERK has outperformed KRR(B). We also notice that KRR(B) is
either only slightly better or worse in most data sets when compared with KRR(S). The bad
performance of bagged KRR can be explained by the bias-variance decomposition of error.
Generally, bagging improves the performance of unstable methods, such as decision trees and
neural network, by lowering the variance with slight increase in bias. However, for stable
methods with low bias and low variance, such as KRR, bagging can degrade the performance
as stated by Breiman in [2]. However, in ERK, each KRR is constructed with only a subset
of the training data which makes it unstable and hence, performance is improved by using an
ensemble of several unstable KRR classifiers.

6

Conclusion and Future Work

In this paper, we proposed an ensemble of kernel ridge regression classifiers, ERK, for multiclass classification. Kernel ridge regression has an elegant closed form solution, and can easily
be extended to perform classification. Several KRR classifiers were constructed in each base
classifier of the ensemble from the subset of samples partitioned using random vector functional
link network. In our method, matrix inverse is computed on reduced number of samples than
original sample size, thus out-of-memory issues during matrix inversion can be circumvented to
an extent. Experimental results show that our method consistently performs better than single
and bagged KRR and RVFL.
In ERK, we used a single Gaussian kernel in each base classifier. But, as suggested in [9],
it can be useful to use multiple kernels instead of a single kernel. One possible future direction
can be to extend our method using multiple kernels and introduce further diversity amongst the
base classifiers. We also plan to use our method in all multi-class UCI data sets and compare
its performance with other ensemble methods.

References
[1] Senjian An, Wanquan Liu, and Svetha Venkatesh. Face recognition using kernel ridge regression.
In Computer Vision and Pattern Recognition, 2007. CVPR’07. IEEE Conference on, pages 1–7.
IEEE, 2007.
[2] Leo Breiman. Bagging predictors. Machine learning, 24(2):123–140, 1996.
[3] Leo Breiman. Random forests. Machine learning, 45(1):5–32, 2001.
[4] Satchidananda Dehuri and Sung-Bae Cho. A comprehensive survey on functional link neural networks and an adaptive pso–bp learning for cflnn. Neural Computing and Applications, 19(2):187–
205, 2010.
[5] Janez Demšar. Statistical comparisons of classifiers over multiple data sets. Journal of Machine
learning research, 7(Jan):1–30, 2006.
[6] Petros Drineas and Michael W Mahoney. On the nyström method for approximating a gram
matrix for improved kernel-based learning. Journal of Machine Learning Research, 6(Dec):2153–
2175, 2005.

8

	

Ensemble of Kernel Ridge Regression
...
Katuwal Rakesh
et al. / Procedia Computer Science 108C (2017) Katuwal
375–383 and Suganthan

[7] Manuel Fernández-Delgado, Eva Cernadas, Senén Barro, and Dinani Amorim. Do we need hundreds of classifiers to solve real world classification problems. J. Mach. Learn. Res, 15(1):3133–3181,
2014.
[8] Yoav Freund. Boosting a weak learning algorithm by majority. In COLT, volume 90, pages
202–216, 1990.
[9] Mehmet Gönen and Ethem Alpaydın. Multiple kernel learning algorithms. Journal of Machine
Learning Research, 12(Jul):2211–2268, 2011.
[10] Ron Kohavi, David H Wolpert, et al. Bias plus variance decomposition for zero-one loss functions.
In ICML, volume 96, pages 275–83, 1996.
[11] M. Lichman. UCI machine learning repository, 2013.
[12] Peter Nemenyi. Distribution-free multiple comparisons. In Biometrics, volume 18, page 263.
International Biometric SOC 1441 I ST, NW, SUITE 700 Washington, DC, 20005-2210, 1962.
[13] Yoh-Han Pao, Stephen M Phillips, and Dejan J Sobajic. Neural-net computing and the intelligent
control of systems. International Journal of Control, 56(2):263–289, 1992.
[14] Ye Ren, PN Suganthan, N Srikanth, and Gehan Amaratunga. Random vector functional link
network for short-term electricity load demand forecasting. Information Sciences, 2016.
[15] Lior Rokach. Ensemble-based classifiers. Artificial Intelligence Review, 33(1-2):1–39, 2010.
[16] Louis B Rosenberg. Human swarms, a real-time paradigm for collective intelligence. Collective
Intelligence, 2015.
[17] Craig Saunders, Alexander Gammerman, and Volodya Vovk. Ridge regression learning algorithm
in dual variables. In (ICML-1998) Proceedings of the 15th International Conference on Machine
Learning, pages 515–521. Morgan Kaufmann, 1998.
[18] Robert E Schapire, Yoav Freund, Peter Bartlett, Wee Sun Lee, et al. Boosting the margin: A
new explanation for the effectiveness of voting methods. The annals of statistics, 26(5):1651–1686,
1998.
[19] James Surowiecki. The wisdom of crowds. Anchor, 2005.
[20] Giorgio Valentini and Francesco Masulli. Ensembles of learning machines. In Italian Workshop on
Neural Nets, pages 3–20. Springer, 2002.
[21] Le Zhang, Ye Ren, and Ponnuthurai N Suganthan. Instance based random forest with rotated feature space. In Computational Intelligence and Ensemble Learning (CIEL), 2013 IEEE Symposium
on, pages 31–35. IEEE, 2013.
[22] Le Zhang, Ye Ren, and Ponnuthurai N Suganthan. Towards generating random forests via extremely randomized trees. In Neural Networks (IJCNN), 2014 International Joint Conference on,
pages 2645–2652. IEEE, 2014.
[23] Le Zhang and PN Suganthan. A comprehensive evaluation of random vector functional link
networks. Information Sciences, 367:1094–1105, 2016.
[24] Le Zhang and Ponnuthurai N Suganthan. Oblique decision tree ensemble via multisurface proximal
support vector machine. IEEE transactions on cybernetics, 45(10):2165–2176, 2015.

9

383

