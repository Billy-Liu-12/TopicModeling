Available online at www.sciencedirect.com

ScienceDirect
Procedia Computer Science 108C (2017) 1040–1049

This space is reserved for the Procedia header, do not use it
This space is reserved for the Procedia header, do not use it
This space is reserved for the Procedia header, do not use it
International Conference
on Computational Science, ICCS 2017, 12-14 June 2017,
Zurich, Switzerland

Finding
Finding the
the Winner
Winner of
of aa Hypothesis
Hypothesis Test
Test via
via Sample
Sample Allocation
Allocation
Optimization
Optimization
Finding the Winner of a Hypothesis
Test via Sample Allocation
Kourosh Modarresi‡‡ and Khashayar Khosravi§§
Optimization
Kourosh Modarresi
and Khashayar Khosravi
Adobe Inc., San Jose, CA, U.S.A.

‡ khosravi@stanford.edu
Adobe Inc., San
Jose,
CA, U.S.A. Khosravi§
modarres@adobe.com,
Kourosh
Modarresi
and
Khashayar

modarres@adobe.com, khosravi@stanford.edu
Adobe Inc., San Jose, CA, U.S.A.
modarres@adobe.com, khosravi@stanford.edu

Abstract
Abstract
This paper studies the optimal adaptive sample allocation for finding the best among a finite set of opThis
studies
thestatistical
optimal adaptive
sample
allocation
for finding
the best
set of options paper
with the
highest
guarantees
and in
the quickest
time. With
the among
growth ainfinite
the number
of
tions
with the
highest
statistical
guarantees
and
in the
quickest
time. algorithm
With the that
growth
in thethe
number
of
Abstract
experiments
that
companies
run, the
existence
of an
optimal
allocation
reduces
required
experiments
that
companies
run,
the
existence
of
an
optimal
allocation
algorithm
that
reduces
the
required
This
paper
studies
the
optimal
adaptive
sample
allocation
for
finding
the
best
among
a
finite
set
of
opexperimentation time (and hence the cost) while providing statistical confidence in the result is crucial. Old
experimentation
time (and
hence
the
cost)
statistical
confidence
the result
isthey
crucial.
Old
tions
with the
highest
statistical
guarantees
andproviding
in methods
the quickest
time.
Withthis
theinproblem
growth
in
number
of
frequentist
hypothesis
testing
methods
are while
primary
for addressing
andthe
provide
frequentist
hypothesis
testing
methods
are primary
for
addressing
this
problem
and they
provide
experiments
that companies
the existence
of an methods
optimal
algorithm
that reduces
theare
required
statistical
guarantees
for
the run,
decision.
These
algorithms
workallocation
poorly
when
the
number
of options
large,
statistical
guarantees
thehence
decision.
These
algorithms
work
poorly
when
thebandit
number
of options
are
large,
experimentation
timeisfor
(and
cost)
while
providing
statistical
confidence
in themethods
result
is are
crucial.
Old
which unfortunately
the
case
inthe
many
practical
applications.
Multi-armed
adaptive
which
unfortunately
is
the
case
in
many
practical
applications.
Multi-armed
bandit
methods
are
adaptive
frequentist
hypothesis
testing
methods
are
primary
methods
for
addressing
this
problem
and
they
provide
allocation rules designed for maximizing a predefined reward function that depends on the allocation during
allocation
rules
designed
maximizing
a predefined
function
that
on the
allocation
during
statistical
guarantees
for for
the
decision.
These
algorithms
poorly for
when
the
number
of options
are
large,
the
experiment.
Although
bandit
algorithms
are
shownreward
towork
be optimal
thedepends
reward
maximization,
they
canthe
Although
algorithms
are shown
to be Best-arm
optimal
for
the reward
maximization,
canwhich
unfortunately
isconfidence
thebandit
case in
many
practical
applications.
Multi-armed
bandit
methods are
adaptive
not experiment.
provide
statistical
for
the decision
they
make.
identification
algorithms
arethey
adaptive
not
provide
statistical
confidence
for
the
decision
they
make.
Best-arm
identification
algorithms
are
adaptive
allocation
rules
designed
for
maximizing
a
predefined
reward
function
that
depends
on
the
allocation
during
exploratory allocation rules that achieve a compelling performance in finding the winner but they also fail
exploratory
allocation
rulesbandit
that achieve
a compelling
performance
inforfinding
the winner
but they
alsocanfail
the
experiment.
Although
algorithms
are we
shown
to be optimal
the Confidence
reward
maximization,
they
to provide
statistical
guarantees.
In this paper,
introduce
OptoAllocate
Interval
(OACI),
an
to
provide
statistical
guarantees.
In the
thisdecision
paper,
we
introduce
OptoAllocate
Confidence
Interval
(OACI),
an
not
provide
statistical
confidence
for
they
make. such
Best-arm
identification
algorithms
adaptive
adaptive
allocation
rule
that assigns
samples
among
options
that the
confidence
interval
of are
the
winning
adaptive
allocation
rule
that
assigns
samples
among
options
such
that
the
confidence
interval
of
the
winning
exploratory
allocation
rules
that
achieve
a
compelling
performance
in
finding
the
winner
but
they
also
fail
option has the largest separation from all other options’ confidence intervals. Simulations show that in many
option
hasOACI
the largest
separation
alltest
other
options’
confidence
intervals.
Simulations
show that
in many
to
provide
statistical
In the
this
paper,
wea introduce
OptoAllocate
Confidence
Interval
(OACI),
an
scenarios,
findsguarantees.
the winnerfrom
of
with
higher
accuracy
and with
a better statistical
guarantees
scenarios,
OACI finds
the
winner
of the
test with
a higher
accuracy
and
a betterinterval
statistical
guarantees
adaptive
that
assigns
samples
among
options
such
that
thewith
confidence
of the
winning
than the allocation
variants
ofrule
hypothesis
testing,
multi-armed
bandit,
or best-arm
identification
algorithms.
than
of hypothesis
bandit,
or best-arm
identification
algorithms.
optionthe
hasvariants
the largest
separationtesting,
from allmulti-armed
other options’
confidence
intervals.
Simulations
show that in many
Keywords:
hypothesis
testing,
multi-armed
bandit,
best-arm
identification,
optimization,
confidence
intervals,
p-value
© 2017 TheOACI
Authors.
Published
by Elsevier
B.V.with
scenarios,
finds
the
winner
of
the
test
a
higher
accuracy
and
with
a
better
statistical
guarantees
Keywords: hypothesis testing, multi-armed bandit, best-arm identification, optimization, confidence intervals,
p-value
Peer-review
under
responsibility
of
the
scientific
committee
of
the
International
Conference
on
Computational
Science
than the variants of hypothesis testing, multi-armed bandit, or best-arm identification algorithms.
Keywords: hypothesis testing, multi-armed bandit, best-arm identification, optimization, confidence intervals, p-value

1
1

Introduction
Introduction

Companies frequently seek for algorithms that optimize their product offerings; they have to find out which
Companies
frequently
seek
for algorithms
optimize
theirorproduct
to find factors
out which
1
Introduction
offer among
all possible
offers
achieves thethat
highest
interest
profit. offerings;
There arethey
twohave
important
for
offer
among 1-whether
all possiblethe
offers
achieves
highestthe
interest
or profit.
There are the
two company
importantcan
factors
a company:
chosen
offer the
is actually
best offer,
and 2-whether
trust for
on
aCompanies
company:
1-whetherthe
thefirst
chosen
offer
is that
actually
the on
best
offer,
and
2-whether
the
company
canout
trust
on
frequently
seek
for
algorithms
optimize
their
product
have
to learn
find
which
the
result. Achieving
factor
usually
depends
designing
anofferings;
algorithmthey
which
can
about
all
the
result.
Achieving
the
first
factor
usually
depends
on
designing
an
algorithm
which
can
learn
about
all
offer
among
all
possible
offers
achieves
the
highest
interest
or
profit.
There
are
two
important
factors
for
offers very well, and maintaining the second factor mostly depends on the confidence in the winning offer
offers
very
well,
and maintaining
the
factorthe
mostly
depends
the
in the winning
awhich
company:
1-whether
the
chosen
offersecond
isor
actually
best
offer, Indeed,
andon2-whether
thecompanies
company
can situation
trustoffer
on
can be
captured
by using
p-values
confidence
intervals.
forconfidence
many
the
which
can
capturedinterval
by first
using
p-values
or confidence
intervals.
Indeed,
for
companies
the desirable.
situation
the
result.
Achieving
the
factor
usually
depends
on
designing
an
algorithm
which
about
all
where
the be
confidence
of
the winning
offer is well-separated
from
all many
other
offerscan
is learn
very
where
the
confidence
interval
of
the
winning
offer
is
well-separated
from
all
other
offers
is
very
desirable.
offers
very
well,
and
maintaining
the
second
factor
mostly
depends
on
the
confidence
in
the
winning
offer
Some algorithms proposed for this task have their roots in statistics, such as frequentist hypothesis testing
Some
algorithms
proposed
for this
task
have
their
rootsintervals.
in
suchfor
as many
frequentist
hypothesis
testing
which
canand
be captured
using
p-values
or confidence
Indeed,
companies
the situation
methods,
some
werebyoriginated
in computer
science,
likestatistics,
multi-armed
bandit
algorithms.
methods,
some were
originated
computer
science,
like multi-armed
algorithms.
where theand
confidence
interval
of theinwinning
offer
is well-separated
from bandit
all other
offers is very desirable.
‡ Senior Machine Learning-AI Scientist at Adobe Inc.
Some
algorithms proposed for this task have their roots in statistics, such as frequentist hypothesis testing
‡ Senior Machine Learning-AI Scientist at Adobe Inc.
§
This work
donewere
whenoriginated
K. Khosraviin
was
a Machinescience,
Learninglike
Scientist
Intern at Adobe
methods,
andwas
some
computer
multi-armed
banditInc.
algorithms.
§
This work was done when K. Khosravi was a Machine Learning Scientist Intern at Adobe Inc.

‡ Senior
§ This

Machine Learning-AI Scientist at Adobe Inc.
work was done when K. Khosravi was a Machine Learning Scientist Intern at Adobe Inc.

1877-0509 © 2017 The Authors. Published by Elsevier B.V.
Peer-review under responsibility of the scientific committee of the International Conference on Computational Science
10.1016/j.procs.2017.05.160

1
1
1

	

Kourosh Modarresi et al. / Procedia Computer Science 108C (2017) 1040–1049

1041

Hypothesis testing (or A/B testing), which is one of the most important and practical tools for comparing
different distributions, has recently been drawn a huge interest among the practitioners [11, 5]. Usually the
samples are randomly divided into equally sized subgroups and each subgroup would be allocated to one
offer. After sample allocation is finished, one can build confidence intervals or compute p-values to decide
whether the winning offer is significantly better than other offers. Although due to the randomness, A/B
testing methods are very efficient in capturing exogenous factors that can influence the experiment outcomes,
when the number of offers are large, they require a very large sample size to be effective. We refer reader to
[8] for a comprehensive study of the hypothesis testing methods.
Multi-armed bandit methods which were first introduced in the mid-twenty century [12, 7], have recently
been shown promising performances for efficient sample allocation [10]. Bandit methods work well for the
regret minimization, but they are not suitable to provide confidence in the winner. Best-arm Identification
algorithms first introduced in [2] are simple variations of Multi-armed bandit algorithms that allocate samples
adaptively. These algorithms have shown a promising performance in finding the winner very quickly in both
frequentist [2] and bayesian settings [9]. However, these algorithms also fail to provide statistical guarantees.
The main contribution of this paper is introducing an adaptive allocation algorithm for maintaining both
accuracy and also confidence by making the confidence interval of the winner non-overlapping with others.
As simulations suggest, our proposed algorithm achieves a high accuracy, while it also provides the highest
confidence in the result by outperforming all its competitors. The organization of paper is as follows: kin
§2 we introduce the problem, in §3 and §4 we explain hypothesis testing and multi-armed bandit algorithms
respectively, in §5 we discuss the proposed OACI method and the optimization problem (3) which is the
main underlying idea of OACI, in §6 we provide simulations to validate OACI’s performance, and in §7 we
conclude.

2

Problem Formulation

Let k be the total number of options and let p = (p1 , p2 , . . . , pk ) denote the vector of true means. In this
paper, we only focus on the case that the observed outcome is distributed according to a bernoulli random
variable. However, all of the results and algorithms can be easily extended to the arbitrary outcome case.
Suppose that the response of a user to option i, is an independent sample from the bernoulli distribution
with parameter pi . The winner of the test is the option with the highest mean, i.e. argmaxi pi . Let N be the
total number of users that are available for the experiment and let Ni denote the total number of samples
allocated to option i. Let Xi1 , Xi2 , . . . , XiNi be the 0 − 1 observed outcomes of samples allocated to option i.

3

Hypothesis Testing Using Equal Allocation

Conventional hypothesis testing approaches address this problem by dividing N samples into equal parts
randomly, i.e., Ni = N/k. Then, the significance is measured by computing the confidence intervals or more
accurately by computing t-scores of tests between different options which are described in the next sections.

3.1

Confidence Intervals

The α-level confidence interval for the mean of option i is constructed as




p̂
(1
−
p̂
)
p̂
(1
−
p̂
)
i
i
i
i
,
[Li , Ui ] = p̂i − α
, p̂i + α
Ni
Ni

(1)

Ni
Xij and α comes from the tail of a t-student distribution with Ni − 1 degrees of freedom.
where p̂i = N1i j=1
This confidence set is constructed based on the fact that
p̂ − pi
i

p̂i (1−p̂i )
Ni

2

Kourosh Modarresi et al. / Procedia Computer Science 108C (2017) 1040–1049

1042	

is distributed according to a TNi −1 , a t-student distribution with Ni − 1 degrees of freedom. The value of α
controls the probability that the true mean lies in the confidence interval. For instance, to have 95 percent
(α) if pˆi ≥ pˆj and Li ≥ Uj .
coverage α ≈ 1.96. We can state i is better than j at the level of confidence TN−1
i −1
However, overlapping confidence intervals does not necessarily mean that the difference is not significant.
More accurate comparison, is based on the t-test which is described in the next section.

3.2

t-test

Assuming the same conditions as the previous part, one can compute the t-score between options i and j as
tij = 

pˆi − pˆj

pˆi (1−pˆi )
Ni

+

pˆj (1−pˆj )
Nj

,

(2)

and check whether this number is above the threshold α or not; if tij ≥ α, this means that option i is better
(α). This follows from the fact that
than option j at the level of confidence TN−1
i +Nj −1
pˆ − pˆj − (pi − pj )
i
pˆ (1−pˆ )
pˆi (1−pˆi )
+ j Nj j
Ni

is distributed according to a t-student distribution with Ni + Nj − 1 degrees of freedom. A simple way for
measuring the confidence is checking if the t-score of all tests between the winner and other options are above
α. Two non-overlapping confidence intervals leads to a t-score above α, but the reverse is not necessarily
true.

3.3

Bonferroni Correction

When performing several hypotheses tests, one should apply the Bonferroni correction. Assuming that events
Ai hold with probability 1 − πi ,for i = 1, 2, . . . , H, using a union bound, the probability that all of them
H
happen is lower bounded by 1 − i=1 πi . The Bonferroni correction replace α with α/H in each test, so that
the probability bounds become valid. However, this adjustment is too strict in many scenarios and leads
to large and impractical values of N . The interesting point about our setting is that comparing
the winner
 
with all other options is sufficient, which reduces the number of hypotheses tests from k2 to k − 1.

4

Multi-Armed Bandit Algorithms

Multi-armed bandit algorithms aim to maximize the reward obtained during the experiment. Reward in our
setting is defined as the sum of 0 − 1 observed outcomes over all k arms. In the beginning of the experiment,
we have no information about the winner and we need to learn about options’ means. By the end of
experiment, there is more certainty about the winner and it is better to allocate samples to this winning
offer. The former task of learning the dynamic of experiment is called Exploration, while the later greedy
task for maximizing the reward is called Exploitation. This well-known exploration-exploitation trade-off
that always exists in multi-armed bandit problems, plays the key difficulty in designing algorithms. Let π be
a deterministic sample allocation policy and let π(t) be the sample allocated at time t. The instantaneous
reward at time t is a bernoulli random variable with mean pπ(t) and therefore the expected reward is equal
to rπ (t) = pπ(t) . Note that the oracle policy π ∗ , sets π ∗ (j) = k (assuming pk = argmaxi pi ). The expected
regret of policy π is defined as the difference between the expected reward of the oracle and the policy π as
following
N
N
N



rπ (t) =
(pπ∗ (t) − pπ(t) ) =
(pk − pπ(t) ).
Rπ (N ) =
t=1

t=1

t=1

The aim of a multi-armed bandit algorithm is to minimize the total expected regret Rπ (N ). It has been
shown in [3] that when the arm rewards come from bernoulli distributions, the best achievable regret is
3

	

Kourosh Modarresi et al. / Procedia Computer Science 108C (2017) 1040–1049

1043

O(log N ). In fact, Thompson Sampling [1] and Upper Confidence Bounds (UCB) [3] algorithms are able
to achieve this lower bound. Although these methods are able to minimize the regret, they usually fail to
provide statistical guarantees. Furthermore, these algorithms are designed for a setting that learning and
evaluation are together which is not the case in many practical applications. A slight variation of multi-armed
bandit algorithms actually address the same two-phase setting.

4.1

Best-Arm Identification Algorithms

Best-arm identification algorithms were first introduced in [2] for the frequentist setting and have recently
been extended to the bayesian setting in [9]. As explained, in many practical scenarios there are two rounds
in the testing: in the first round options need to be explored, and in the second round the best identified
option will be used afterwards. For instance, a company wants to put one from k possible promotion offers
on their website. The company runs a test to learn which of these offers receives more interest and that offer
will be placed on the company’s website from then onwards. Best-arm identification algorithms are designed
to sequentially allocate samples among different offers so that by the end of the first round, the expected
reward upon choosing the winning offer is maximized. Ideally, by the end of experiment, the decision-maker
wants to choose the arm with the highest mean, but there is a nonzero probability that the observed data
indicates another arm as the optimal one. This setting is very close to the setting that we study as we also
consider the efficient sample allocation rules during the learning period. Despite their efficiency in maximizing
the exploration, best-arm identification algorithms are incapable of providing statistical guarantees for the
winning offer. If the decision-maker needs to be confident about the offer that is chosen by the end of the
first phase, the confidence interval of the winning offer must be separated from the others.
In the next section, we describe the proposed OptoAllocate Confidence Interval (OACI) algorithm for
finding the winner. OACI is an adaptive algorithm for allocating samples in such a way that the confidence
interval of the winner has the highest separation from confidence intervals of other options.

5

OptoAllocate Confidence Interval(OACI)

Having confidence intervals defined in (1), the goal is proposing an algorithm for allocating N samples among
different options so that the winner can be found in the quickest time, while achieving some statistical
guarantees. After drawing M samples, sort options based on their empirical mean. Without any loss in
generality, let p̂1 ≤ p̂2 ≤ . . . ≤ p̂k . With the aim of achieving the highest statistical guarantee, we form the
following optimization problem for the sample allocation
maximize
subject to

Lk − maxk−1
i=1 Ui
k
N
=
N,
i
i=1

(3)

which has a concave objective function w.r.t Ni . As the constraints are all convex w.r.t Ni , the maximization
can be solved using the convex optimization methods. Knowing the optimal values for Ni and the number of
samples that have already been allocated among first M samples, the next sample is picked with a probability
proportional to the difference between these two (giving the optimal allocation for the remaining samples).
However, there are some practical issues with this method in its current format. First, the highest mean
might change at each iteration. Second, if we solve the optimization problem only once, then the error in
estimation of p̂i might not vanish, even after allocating all N samples. Finally, since we deal with really small
conversion rates in practice, we need a reasonable number of samples from each offer to have our normal
or t-student approximations valid. This is not an issue for non-winner options, but it is possible that the
estimated mean of the winning offer is much smaller than its true value. We need to make sure that our
algorithm is robust to this issue and allocates enough samples to each offer.
For solving the first two issues, instead of once, we solve the optimization problem after each K rounds
and update the result. For solving the last issue, we add a small amount of exploration by giving more
weights to options with fewer samples. For simplicity, we take this exploration probability vector to be
4

Kourosh Modarresi et al. / Procedia Computer Science 108C (2017) 1040–1049

1044	

proportional to 1/(Ni + 1) and gradually decrease its weight. This adds some randomness and mitigates the
existing issue. Therefore, after each K iterations, the following optimization problem needs to be solved:
minimize
subject to

t − Lk
k
i=1 Ni = N
Ui − t ≤ 0, 1 ≤ i ≤ k − 1
Ni ≥ 0,

(4)

with real variables N1 , N2 , . . . Nk , and t. This can be derived from (3) by some slight modifications. This
again is a convex optimization problem and can be solved by convex optimization methods. The full algorithm
is described in Algorithm 1.
Algorithm 1 : OptoAllocate Confidence Interval (OACI)
Input: N : the number of samples, α: coverage of confidence sets
K: number of samples drawn in each update, upd: the optimization update period,
winit : the initial exploration weight, W : the exploration weight update period,
cupd : the exploration update coefficient, wmin : minimum exploration weight,
Initialize Nisofar = 0, and p̂i = 12 for 1 ≤ i ≤ k, w = winit , count = 0
while count ≤ N do
if count mod W = 0 and count ≥ 1 then
w ← min(cupd w.wmin )
end if
if count mod upd = 0 then
Solve the optimization problem in (4) and update N1 , N2 , . . . , Nk
end if
Compute remaining samples required for each option and truncate to zero: Nirmn ← max(Ni −Nisofar , 0)
πopt ← k 1N rmn (N1rmn , N2rmn , . . . , Nkrmn ) (Optimization probability vector)
i=1

πexp ←

k

i=1

i

1
1
1
1
(
,
, . . . , N sofar
)
+1
1/(Nisofar +1) N1sofar +1 N2sofar +1
k

(Exploration probability vector)

Allocate K samples from the set {1, 2, . . . , k} with the probability π = (πopt + wπexp )/(1 + w)
Update Nisofar , the vector of conversion rate p̂, and count ← count + K
end while
Let winner ← w = argmaxki=1 p̂i . Compute t-scores twi , i = w in (2). Let minTscore to be their minimum.
isSignificant ← I(minTscore ≥ α)
Output: winner: The winner of the test, isSignificant: Whether the result is statistically significant
It is worth mentioning that, one could count the winner as a significant one if the confidence sets were
non-overlapping, however here we used t-scores which are more accurate. Furthermore, for having a valid
upper bound on the false positive rate α, one can apply the Bonferroni correction and replace α with α/H.

5.1

How to solve the optimization problem in (4)

While existing optimization packages such as CVX [4] can be used to solve the convex optimization problem
in (4), based on our observations, the answers are often not accurate or convergence is slow. For solving this
issue, we implement our own solver. Form the Lagrangian of the convex optimization problem in (4) as

L(t, N, λ, θ) = t − p̂k − α (p̂k (1 − p̂k ))

1/2

−1/2

Nk

+

k−1

i=1



−1/2
1/2
λi p̂i + α (p̂i (1 − p̂i ) Ni
− t + θ(1T N − N ),
5

	

Kourosh Modarresi et al. / Procedia Computer Science 108C (2017) 1040–1049

1045

with N = (N1 , N2 , . . . , Nk ) and λ = (λ1 , λ2 , . . . , λk−1 ). The KKT conditions for optimality are given by
1T N − N = 0 (Primal Feasibility)

∂L(t, N, λ, θ)
= 1 − 1T λ = 0 (Stationarity)
∂t
zk −3/2
∂L(t, N, λ, θ)
= − Nk
+ θ = 0 (Stationarity)
∂Nk
2
zi −3/2
∂L(t, N, λ, θ)
= −λi Ni
+ θ = 0 (Stationarity)
∂Ni
2
−1/2

λi (p̂i − zi Ni

− t) = 0 (Complementary Slackness),

1/2

where zi = α (p̂i (1 − p̂i )) . It is easy to check that if λi = 0 for some i, then θ = 0, implying λ = 0, which
contradicts the second equation. Therefore, the second term in the last equation for all i = 1, 2, . . . , k − 1 is
zero and we obtain
1T N − N = 0,
−1/2

p̂i − zi Ni
k−1

i=1

3/2
Ni

zi

−1/2

= p̂j − zj Nj
=

3/2
Nk

, 1 ≤ i ≤ j ≤ k − 1,

(5)
(6)

zk

that can be solved numerically. In our implementation, we use optimize.root function of SciPy [6] in
Python, which is quick and works well.

5.2

Solution of (4) in case of two options k = 2

In order to give some insight on how OACI proceeds, we explicitly derive the OACI allocation rule for k = 2.
Equation (6) after some algebra implies that
Ni =

(p̂i (1 − p̂i ))1/3
N, i = 1, 2.
(p̂1 (1 − p̂1 ))1/3 + (p̂2 (1 − p̂2 ))1/3

An interesting observation is that the equal allocation is almost optimal; the function f (x) = (x(1 − x))1/3 is
symmetric around 1/2 and increasing on [0, 1/2], with f (0.02) = 0.27 and f (0.5) = 0.63. Thus, ratios N1 /N
and N2 /N are close. In an extreme case of p1 = 0.02, p2 = 0.5, the optimal allocation is 30/70. In most
practical scenarios, this allocation is almost 50/50, suggesting that the equal allocation is optimal for k = 2.

5.3

Extension to the general first m-winners case

The proposed idea can be extended for finding m options that have higher means than the others. Assume
that we also want to rank these first m winners and make their confidence intervals non-overlapping too.
Here, we explain the extension of OACI to this setting. We form the optimization problem as
maximize min(Lk − Uk−1 , Lk−1 − Uk−2 , · · · , Lk−m+2 − Uk−m+1 , Lk−m+1 − maxk−m
j=1 Ui )
k
N
=
N,
subject to
i
i=1

(7)

which by a simple change of variable turns into
maximize
subject to

u
Lj − Uj−1 ≥ u, j = k − m + 2, k − m + 3, . . . , k
Lk−m+1 − t ≥ u
t ≥ Ui , 1 ≤ i ≤ k − m
k
i=1 Ni = N,

(8)

with variables N = (N1 , N2 , . . . , Nk ) ∈ Rk , u ∈ R, and t ∈ R. This again is a convex optimization problem
and can be solved with similar methods to those described beforehand.
6

Kourosh Modarresi et al. / Procedia Computer Science 108C (2017) 1040–1049

1046	

6

Simulations

We run simulations to compare the performance of OACI with other state-of-the-art algorithms. The list of
algorithms is as following:
• (OACI): OptoAllocate Confidence Interval with K = upd = W = 1000, winit = 1, cupd = 0.9, cmin =
0.01, α = 1.96
• (TS): Thompson Sampling using uniform beta priors
• (EA): Equal Allocation among different options suggested by conventional hypothesis testing methods
• (UCB-E): Upper-Confidence Bound Exploration algorithm of [2], for a = cN/H1 , where c = 1 and
k−1
H1 = i=1 1/∆2i . The value ∆i = pk − pi is the gap between rewards of the sub-optimal arm i and
the optimal arm k
• (SR): Successive Reject algorithm of [2]
We considered five different scenarios for the vector of means p:
1.
2.
3.
4.
5.

Experiment 1 : K = 5, p = (0.03, 0.04, 0.05, 0.06, 0.07), N ∈ {2000, 4000, . . . , 20000}
Experiment 2 : K = 7, p = (0.05, 0.05, 0.05, 0.06.0.06, 0.06, 0.07), N ∈ {3000, 6000, . . . , 30000}
Experiment 3 : K = 5, p = (0.04, 0.045, 0.05, 0.055, 0.06), N ∈ {10000, 20000, . . . , 100000}
Experiment 4 : K = 4, p = (0.5 − 0.2, 0.5 − 0.22 , 0.5 − 0.23 , 0.5), N ∈ {20000, 40000, . . . , 140000}
Experiment 5 : K = 10, p = (0.05, 0.05, 0.05, 0.05, 0.05, 0.1, 0.11, 0.12, 0.13, 0.14), N ∈ {5000, . . . , 50000}

For each vector of true underlying means, we ran the experiment 1000 times and at the end of each of these
runs, we computed : 1-whether the predicted winner matches the true underlying winner, and averaged
it to get accuracy, 2-whether the winner is significant based on the t-scores above the threshold α = 1.96
and averaged it, and 3-whether the winner is significant based on the winner’s confidence interval being
non-overlapping with the others and averaged it. In Figures 1-5 the plots of five different experiments are
shown. For each experiment, four figures are depicted: 1-accuracy of all algorithms vs N , 2-significance
based on t-scores vs N , 3-significance based on confidence intervals vs N , and 4-average allocation of OACI
among different arms. As it can be observed, OACI predicts the winner with a higher success rate and with
a higher statistical confidence (based on both t-scores and confidence intervals) in all experiments.

7

Conclusion

In this paper we introduced OACI, an algorithm for adaptive allocation of samples among different options
during an experiment. OACI algorithm is capable of finding the winner with high accuracy while providing
very strong statistical confidence in the result. The key difficulty in designing the OACI algorithm is solving
an ill-behaved convex optimization problem, which we managed to solve after some reparameterization.
Simulations confirm our motivation and show that OACI is very effective in making the confidence intervals
non-overlapping and it outperforms other existing competitors in both accuracy and statistical confidence.

References
[1] Shipra Agrawal and Navin Goyal. Analysis of thompson sampling for the multi-armed bandit problem.
In COLT, pages 39–1, 2012.
[2] Jean-Yves Audibert and Sébastien Bubeck. Best arm identification in multi-armed bandits. In COLT23th Conference on Learning Theory-2010, pages 13–p, 2010.
[3] Sébastien Bubeck, Nicolo Cesa-Bianchi, et al. Regret analysis of stochastic and nonstochastic multiR in Machine Learning, 5(1):1–122, 2012.
armed bandit problems. Foundations and Trends
7

Kourosh Modarresi et al. / Procedia Computer Science 108C (2017) 1040–1049

1.00

True decisions

0.95

0.90

0.85

0.80

EA
TS
OACI
UCB-E
SR

0.75

0.70

2500

5000

7500

10000

12500

15000

17500

Significant winner with t-scores above α

	

0.7

EA
TS
OACI
UCB-E
SR

0.6

0.5

0.4

0.3

0.2

0.1

0.0

20000

2500

5000

7500

10000

12500

15000

17500

20000

Number of Samples N

8000

0.40

EA
TS
OACI
UCB-E
SR

0.35
0.30

N
N
N
N
N
N
N
N
N
N

7000

Allocated Samples Ni

Significant winner with non-overlapping CIs

Number of Samples N

0.25
0.20
0.15
0.10
0.05

6000
5000
4000
3000

= 2000
= 4000
= 6000
= 8000
= 10000
= 12000
= 14000
= 16000
= 18000
= 20000

2000
1000

0.00
0
2500

5000

7500

10000

12500

15000

17500

20000

0.03

0.04

0.05

0.06

0.07

Means pi

Number of Samples N

1.0

True decisions

0.9

0.8

0.7

EA
TS
OACI
UCB-E
SR

0.6

0.5

5000

10000

15000

20000

25000

Significant winner with t-scores above α

Figure 1: Experiment 1 results, top-left: accuracy, top-right: percentage of times with t-scores above 1.96,
bottom-left: percentage of times with separated confidence intervals, bottom-right: average OACI allocation

0.6

EA
TS
OACI
UCB-E
SR

0.5

0.4

0.3

0.2

0.1

0.0

30000

5000

10000

0.20

0.15

15000

20000

25000

30000

Number of Samples N

EA
TS
OACI
UCB-E
SR

N
N
N
N
N
N
N
N
N
N

10000

Allocated Samples Ni

Significant winner with non-overlapping CIs

Number of Samples N

0.10

0.05

8000

6000

4000

= 3000
= 6000
= 9000
= 12000
= 15000
= 18000
= 21000
= 24000
= 27000
= 30000

2000
0.00
0
5000

10000

15000

20000

Number of Samples N

25000

30000

0.05

0.05

0.05

0.06

0.06

0.06

0.07

Means pi

Figure 2: Experiment 2 results, top-left: accuracy, top-right: percentage of times with t-scores above 1.96,
bottom-left: percentage of times with separated confidence intervals, bottom-right: average OACI allocation
8

1047

1.00

True decisions

0.95

0.90

0.85

0.80

EA
TS
OACI
UCB-E
SR

0.75

0.70
20000

40000

60000

80000

Significant winner with t-scores above α

Kourosh Modarresi et al. / Procedia Computer Science 108C (2017) 1040–1049

1048	

EA
TS
OACI
UCB-E
SR

0.8

0.6

0.4

0.2

100000

20000

40000

EA
TS
OACI
UCB-E
SR

0.6

0.5

60000

80000

100000

Number of Samples N

N
N
N
N
N
N
N
N
N
N

40000

Allocated Samples Ni

Significant winner with non-overlapping CIs

Number of Samples N

0.4

0.3

0.2

30000

20000

= 10000
= 20000
= 30000
= 40000
= 50000
= 60000
= 70000
= 80000
= 90000
= 100000

10000

0.1

0.0
0
20000

40000

60000

80000

100000

0.04

0.045

0.05

0.055

0.06

Means pi

Number of Samples N

1.00

True decisions

0.95

0.90

0.85

EA
TS
OACI
UCB-E
SR

0.80

20000

40000

60000

80000

100000

120000

Significant winner with t-scores above α

Figure 3: Experiment 3 results, top-left: accuracy, top-right: percentage of times with t-scores above 1.96,
bottom-left: percentage of times with separated confidence intervals, bottom-right: average OACI allocation

EA
TS
OACI
UCB-E
SR

0.8
0.7
0.6
0.5
0.4
0.3
0.2
0.1

140000

20000

40000

60000

80000

100000

120000

140000

Number of Samples N

70000

EA
TS
OACI
UCB-E
SR

0.5

0.4

60000

Allocated Samples Ni

Significant winner with non-overlapping CIs

Number of Samples N

0.3

0.2

0.1

50000

40000

N
N
N
N
N
N
N

= 20000
= 40000
= 60000
= 80000
= 100000
= 120000
= 140000

30000

20000

10000

0.0

0
20000

40000

60000

80000

100000

Number of Samples N

120000

140000

0.3

0.46

0.492

0.5

Means pi

Figure 4: Experiment 4 results, top-left: accuracy, top-right: percentage of times with t-scores above 1.96,
bottom-left: percentage of times with separated confidence intervals, bottom-right: average OACI allocation
9

Kourosh Modarresi et al. / Procedia Computer Science 108C (2017) 1040–1049

1.00
0.95

True decisions

0.90
0.85
0.80
0.75

EA
TS
OACI
UCB-E
SR

0.70
0.65
0.60
10000

20000

30000

40000

Significant winner with t-scores above α

	

EA
TS
OACI
UCB-E
SR

0.8
0.7
0.6
0.5
0.4
0.3
0.2
0.1
0.0

50000

10000

20000

0.5

0.4

30000

40000

50000

Number of Samples N

EA
TS
OACI
UCB-E
SR

N
N
N
N
N
N
N
N
N
N

20000
17500

Allocated Samples Ni

Significant winner with non-overlapping CIs

Number of Samples N

0.3

0.2

0.1

15000
12500
10000
7500

= 5000
= 10000
= 15000
= 20000
= 25000
= 30000
= 35000
= 40000
= 45000
= 50000

5000
2500

0.0
0
10000

20000

30000

Number of Samples N

40000

50000

0.05

0.05

0.05

0.05

0.05

0.1

0.11

0.12

0.13

0.14

Means pi

Figure 5: Experiment 5 results, top-left: accuracy, top-right: percentage of times with t-scores above 1.96,
bottom-left: percentage of times with separated confidence intervals, bottom-right: average OACI allocation
[4] Michael Grant and Stephen Boyd. Cvx: Matlab software for disciplined convex programming.
[5] Ramesh Johari, Leo Pekelis, and David Walsh. Always valid inference: Bringing sequential analysis to
a/b testing. 2015.
[6] Eric Jones, Travis Oliphant, Pearu Peterson, et al. SciPy: Open source scientific tools for Python,
2001–. [Online; accessed 2017-04-10].
[7] Tze Leung Lai and Herbert Robbins. Asymptotically efficient adaptive allocation rules. Advances in
applied mathematics, 6(1):4–22, 1985.
[8] Erich L Lehmann and Joseph P Romano. Testing statistical hypotheses. Springer Science & Business
Media, 2006.
[9] Daniel Russo. Simple bayesian algorithms for best arm identification. arXiv preprint arXiv:1602.08448,
2016.
[10] Steven L Scott. Multi-armed bandit experiments in the online service economy. Applied Stochastic
Models in Business and Industry, 31(1):37–45, 2015.
[11] Diane Tang, Ashish Agarwal, Deirdre O’Brien, and Mike Meyer. Overlapping experiment infrastructure:
More, better, faster experimentation. In Proceedings of the 16th ACM SIGKDD international conference
on Knowledge discovery and data mining, pages 17–26. ACM, 2010.
[12] William R Thompson. On the likelihood that one unknown probability exceeds another in view of the
evidence of two samples. Biometrika, 25(3/4):285–294, 1933.

10

1049

