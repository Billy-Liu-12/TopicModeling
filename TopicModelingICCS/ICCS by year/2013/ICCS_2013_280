Available online at www.sciencedirect.com

Procedia Computer Science 18 (2013) 2599 – 2602

International Conference on Computational Science, ICCS 2013

MapReduce operations with WS-VLAM workﬂow management
system
Mikolaj Baranowskia,∗, Adam Bellouma , Marian Bubaka,b
b AGH

a Informatics Institute, University of Amsterdam, Science Park 904, 1098 XH Amsterdam, The Netherlands
University of Science and Technology, Department of Computer Science, Mickiewicza 30, 30-059 Krakow, Poland

Abstract
Workﬂow management systems are widely used to solve scientiﬁc problems as they enable orchestration of remote and local services such as database queries, job submission and running an application. To extend the role that workﬂow systems play
in data-intensive science, we propose a solution that integrates WMS and MapReduce model. In this paper, we discuss possible
solution of combining MapReduce and workﬂow applications, we describe the implementation of chosen solution based on
metaprogramming approach in Ruby programming language and evaluate it with an example of word count application.
Keywords: scientiﬁc workﬂow; Ruby; MapReduce; Hadoop; parallel processing

1. Introduction
Workﬂow Management Systems (WMS) proved to be an easy and eﬃcient way to describe complex systems
and to be adaptable to use new technologies. Recently, we can observe an explosion of data and data intensive
applications are going to play the more important role in a science. As authors of [1] point out that today’s
computers has relatively low I/O performance and algorithms should be designed to make computations close to
the place where data is stored since any data transfer is very costly [2]. MapReduce is a programming model that
has capabilities of processing large volumes of data and meet these new challenges. WMSs that can describe well
scientiﬁc procedures have to ﬁnd their place in the data-centric research.
The main objective of our work is to provide access to MapReduce framework from the WMS level. It would
enable an access to data stored in a MapReduce-oriented storage like Hadoop Distributed File System (HDFS). The
result of this investigations will be implemented using WS-VLAM workﬂow system as it can be easily extended
due to its modular structure [3].
2. Background
The MapReduce [4] programming model was designed for processing large datasets by Google. In Hadoop [5]
framework which is an open source implementation of this model, Map/Reduce operations are deﬁned in Java
programming language or, by using Hadoop Streaming interface, by any executable. It implies that almost any
∗ Corresponding

author. Tel.: +31-20-525-7562 ; fax: +31-20-525-7490 .
E-mail address: m.p.baranowski@uva.nl.

1877-0509 © 2013 The Authors. Published by Elsevier B.V. Open access under CC BY-NC-ND license.
Selection and peer review under responsibility of the organizers of the 2013 International Conference on Computational Science
doi:10.1016/j.procs.2013.05.449

2600

Mikolaj Baranowski et al. / Procedia Computer Science 18 (2013) 2599 – 2602

WS-VLAM
a
MapReduce
Map

Reduce

b

Fig. 1. (a) An example of a workﬂow that deﬁnes Map/Reduce operations. It consists normal workﬂow tasks – rounded shape, and MapReduce
components. Map/Reduce operations are deﬁned as separate subworkﬂows, on a graph they contain two sequential tasks each. The main
MapReduce component groups Map/Reduce subworkﬂows. (b) An example of WS-VLAM workﬂow that deﬁnes Map/Reduce operations.
Nodes a and b are tasks that send and receive data from the MapReduce component. The Map node contains a Ruby script which deﬁnes Map
operation and the Reduce node a name that refers to the applicable implementation.

programming language can be used to deﬁne Map/Reduce operations. Pig [6] and Sawzall [7] are tools created by
Yahoo and Google to process large number of records with the MapReduce methodology. In Pig framework (built
on top of Hadoop), MapReduce jobs are modeled in Pig Lating – a SQL-like declarative language. In Sawzall in
turn, there is a Domain Speciﬁc Language (DSL) also named Sawzall that allows users to specify Map operations.
In [8], authors are focused on a solution that enriches Kepler with the universal interface to MapReduce. In
order to execute MapReduce operations on Hadoop, the MapReduce actor has to be created. It consists two separate sub-workﬂows that are responsible for two phases - the Map and the Reduce (Fig. 1a). The main advantage of
this solution is a fact that Map/Reduce operations are deﬁned in a same way as the rest of the workﬂow. It makes
the model easy to understand for workﬂow users and allows to use existing WMS modules to deﬁne Map/Reduce
operations. During execution, Kepler engine and MapReduce subworkﬂows are distributed to working nodes to
perform MapReduce tasks. In the experiment authors measured execution times of the word count application
and compared them with the performance of the application that does not use Kepler engine. The native Java
implementation turned out to be six times faster than the elaborated solution. Authors conclude that this overhead
is due to Kepler engine initialization and workﬂow parsing. This method of integrating MapReduce with WMS
has other disadvantage – some MapReduce implementations – that do not have Hadoop features and wont let to
deploy Kepler engine on working nodes – might be unable to integrate. There are also attempts to introduce programming languages to workﬂow modeling. SMAWL [9] which is is a workﬂow language based on Calculus of
Communicating Systems and application model based on Ruby programming language [10] are notable examples.
When designing our application, we followed notable features from diﬀerent solutions as the use of DSLs in
MapReduce frameworks, metaprogramming techniques in workﬂow modeling and the speciﬁc ideas as the way
of choosing Aggregators in Sawzall or an idea of MapReduce workﬂow tasks from Kepler+Hadoop integration.
3. Enhancing WS-VLAM with MapReduce
WS-VLAM [3] is WMS which covers the entire lifecycle of scientiﬁc workﬂows from design, execution phase
to sharing and reuse complete workﬂows and their components. The aim of our work is to integrate WS-VLAM
application with MapReduce framework in such a way that a deﬁnition of MapReduce operations will be included
in a workﬂow description and WS-VLAM workﬂow engine will cooperate with MapReduce environment.
Integrating MapReduce tools as single notes/tasks with WMS would not require that tight integration between
MapReduce framework and WMS as Kepler+Hadoop solution. The Map/Reduce operations would be speciﬁed

Mikolaj Baranowski et al. / Procedia Computer Science 18 (2013) 2599 – 2602

2601

Listing 1. Map operation of word count application
require ’ mrtoolkit ’
class MapWords < MapBase
def declare
field : text
emit : word
emit : counter
end
def process ( input , output )
out = []
input . text . split . each do | w |
output = new_output ; output . word = w
output . counter = 1; out << output
end
out
end
end

in the most eﬃcient way, however, it would cause inconsistency in the workﬂow description – Map/Reduce definitions would be created using external tools. In our solution, we follow observations made by the authors of
Sawzall [7]. They point out that eﬃciency is more important in Reduce operations. Map operations spend most
of the time waiting for I/O events and they are frequently modiﬁed by MapReduce users when Reduce operations,
in turn, are rarely changed. It justiﬁes deﬁning Map operations in less eﬃcient way using a metaprogramming
approach. We introduce two kinds of workﬂow tasks – the ﬁrst one consists a deﬁnition of a Map operation in
scripting language. The second speciﬁes a Reduce operation by providing the name that refers to the existing
implementation of Reduce operation. It has to be clear that these two tasks form a one MapReduce speciﬁcation.
4. Implementation
Apache Hadoop [5] was chosen as a MapReduce framework. Thanks to Hadoop Streaming interface, we were
able to deﬁne Map operations using the MapReduce Toolkit from New York Times – MRToolkit [11]. It uses
metaprogramming abilities of the Ruby programming language to express Map and Reduce operations. We deﬁne
Map operations as a single Ruby class that contains a deﬁnition of record ﬁelds, a speciﬁcation of parameters
for the emitter and a function that performs a Map operation as it is showed in Listing 1. A Reduce operation is
implemented in Java. A workﬂow user chooses its class in a workﬂow description. In Fig. 1b shows a graphical
representation of discussed approach.
5. Results
In a purpose of proving good abilities of deﬁning Map/Reduce operations in WS-VLAM, the word count
application was implemented. Map operation splits a text into words and for each word it emits a pair (<word>, 1)
where <word> stands for one word from the text. Its implementation is presented in Listing 1. Then, in a Reduce
operation these records are grouped basing on <word> and all values are added. The sum stands for a total number
of occurrences of a particular word. The applicable implementation is already included into Hadoop and after
adopting it to our needs, it can be reused. Tests were performed in DAS-4 environment that consists 8 nodes of
dual quad-core 2.4 GHz CPUs and 24 GB memory each connected with InﬁniBand and Gigabit Ethernet. We
prepared 2.6 GB of English books in a plain text format taken from Project Gutenberg. They were stored in HDFS
and used in tests as a whole set and smaller subsets of 2.6 GB, 1.3 GB and 930 MB of texts. The word count
implementation in Java was executed and compared with elaborated solution. Execution command that runs our
application is included in Listing 2. It uses implementation of the Map operation showed in Listing 1.
Table 1 gathers the execution times. It shows that slowdown is increasing with a grow of input sets. Nearly
linear relation implies that a management and administration share in overall cost is decreasing while the actual
computation cost increasingly inﬂuence a total execution time. It is hard to compare execution times of our test
with the results achieved by the Kepler-based solution. Tests were performed in a diﬀerent environments and on

2602

Mikolaj Baranowski et al. / Procedia Computer Science 18 (2013) 2599 – 2602
Listing 2. Executing word counter implemented with MRToolkit (map operation presented in Listing 1). Line 3 speciﬁes Map operation
while 4 speciﬁes Reduce. Line 5 contains a list of ﬁles that have to be available on working nodes – they are required by the MapWords class.

1
2
3
4
5

hadoop jar \ $HADOOP_HOME / contrib / streaming / hadoop - streaming -1.0.0. jar \
- input texts - output output \
- mapper " ruby wordcount . rb -s MapWords " \
- reducer SumReducer \
- file wordcount . rb - file mrtoolkit / lib / stream_runner . rb - file mrtoolkit / lib / mrtoolkit . rb

Table 1. Word counter execution times.
Size of input data
Time (Java)
2.6GB (6144 items)
626 s
1.3GB (3534 items)
367 s
930MB (2102 items)
234 s

Time (map operation in MRToolkit)
1073 s
620 s
382 s

Slowdown in %
71%
69%
63%

diﬀerent data. However, we gained better relative execution times – our solution was slower by 63-71% comparing
to the Java implementation while the Kepler-based solution was 4-6 times slower [8].
6. Conclusions and future work
The elaborated approach provides a simple environment for deﬁning MapReduce queries in workﬂow models.
It follows the examples of Pig and Sawzall applications and their DSL-based solutions. The performance of our
application is acceptable and metaprogramming approach is considered to be easy to use. However, more complex
applications have to be investigated in the future. However, the presented solution has a one major disadvantage –
information processed by Ruby (but also by the other executable run with Hadoop Streaming interface) in a Map
operation, looses its type and then, it has to be cast to a required data type in Reduce phase.
In the future work, other programming languages can be considered as an alternative to Ruby, specially,
statically typed languages based on Java Virtual Machine platform such as Scala programming language [12].
They can provide good constructs for metaprogramming approach and at the same time, they can be directly used
in the Hadoop environment in the same JVM instance. It would let us to resign from Hadoop Streaming interface
and to gain better performance and robustness.
Acknowledgements. This work was partially supported by the Dutch national program COMMIT. We would like
to thank Reginald Cushing and Spiros Koulouzis from University of Amsterdam for discussions and suggestions.
References
[1] T. Hey, S. Tansley, K. Tolle (Eds.), The Fourth Paradigm: Data-Intensive Scientiﬁc Discovery, Microsoft Research, Redmond, Washington, 2009.
[2] S. Koulouzis, R. Cushing, K. Karasavvas, A. Belloum, M. Bubak, Enabling web services to consume and produce large datasets, Internet
Computing, IEEE 16 (1) (Jan.-Feb.) 52–60.
[3] A. Belloum, M. Inda, D. Vasunin, V. Korkhov, Z. Zhao, H. Rauwerda, T. Breit, M. Bubak, L. Hertzberger, Collaborative e-science
experiments and scientiﬁc workﬂows, Internet Computing, IEEE 15 (4) (July-Aug.) 39–47.
[4] J. Dean, S. Ghemawat, Mapreduce: simpliﬁed data processing on large clusters, Commun. ACM 51 (1) (2008) 107–113.
[5] A. S. Foundation, Apache hadoop, http://hadoop.apache.org/.
[6] C. Olston, B. Reed, U. Srivastava, R. Kumar, A. Tomkins, Pig latin: a not-so-foreign language for data processing, in: Proceedings
of the 2008 ACM SIGMOD international conference on Management of data, SIGMOD ’08, ACM, New York, NY, USA, 2008, pp.
1099–1110.
[7] R. Pike, S. Dorward, R. Griesemer, S. Quinlan, Interpreting the data: Parallel analysis with sawzall, Scientiﬁc Programming 13 (4)
(2005) 277.
[8] J. Wang, D. Crawl, I. Altintas, Kepler + hadoop: a general architecture facilitating data-intensive applications in scientiﬁc workﬂow
systems, in: Proceedings of the 4th Workshop on Workﬂows in Support of Large-Scale Science, WORKS ’09, ACM, New York, NY,
USA, 2009, pp. 12:1–12:8.
[9] C. Stefansen, Smawl: A small workﬂow language based on ccs, in: Harvard University, 2005.
[10] M. Baranowski, A. Belloum, M. Bubak, M. Malawski, Constructing workﬂows from script applications, Scientiﬁc Programming.
[11] N. Y. Times, Mrtoolkit, http://code.google.com/p/mrtoolkit/.
[12] P. M. L. of EPFL, Scala programming language, http://www.scala-lang.org/.

