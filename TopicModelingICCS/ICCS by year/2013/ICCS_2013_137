Available online at www.sciencedirect.com

Procedia Computer Science 18 (2013) 1436 – 1445

International Conference on Computational Science, ICCS 2013

An experience of e-assessment in an introductory course on
computer organization
Eladio D. Guti´erreza,∗, Mar´ıa A. Trenasa , Francisco Corberaa , Juli´an Ramosa , Sergio
Romeroa
a Dept.

Computer Architecture, University of M´alaga, 29071 M´alaga, Spain

Abstract
This work describes how the CTPracticals Moodle module can be used for e-assessment in an introductory course on computer
organization, where the practical content consists of the design and simulation of a basic CPU implemented using Logisim, a
schematic-based educational tool for the design and simulation of digital circuits. A previous work extended this module to
support the veriﬁcation of codes written in Matlab. This work shows how this feature can be exploited to invoke external tools,
i.e. Logisim, not only for the automatic veriﬁcation of the student submissions, but also for a detailed analysis of the results
that improves the assessment. The paper will also present some customization of Logisim that was necessary to improve its
batch mode simulation in order to add some useful options that make it generate a richer output format.
Keywords:
Automatic assessment; computer organization; Logisim; Moodle; CTPracticals

1. Introduction
During this last decade, Learning Management Systems (LMS), and in general new technologies, have been
adopted very quickly by higher education, and even are identiﬁed as a way of reforming it [1]. LMS systems
have changed several aspects of the teaching-learning process, advancing towards a more interactive learning,
encouraging constructivist pedagogical approaches [2, 3], self-learning, collaborative learning and teamwork [4, 5]
which are increasingly valuable skills in the social network era. Furthermore, from the point of view of teachers
and instructors, LMSs are helpful to gain tidiness and order while pedagogical activities are packaged in a uniform
way across courses.
In the context of Engineering and Computer Science teaching, the CTPracticals module joins together two
elements: LMSs, particularly Moodle [6], and the automatic veriﬁcation of programming-like assignments. Motivations for choosing Moodle (Modular Object-Oriented Dynamic Learning Environment) include its popularity,
its modular organization and its free availability. Most present-day educators are used to working with LMSs,
platforms that by means of web-based interfaces provide a panoply of activities (forums, chats, surveys, ...) with
which a standard course can be set up. Both teachers and students can harness the integration of automatic assessment features into this kind of platforms. Teachers can better organize the lab works, and give a better use
∗ Corresponding

author. Tel.: +34-952-132821; fax: +34-952-132790.
Email addresses: eladio@uma.es (Eladio D. Guti´errez), matrenas@uma.es (Mar´ıa A. Trenas), corbera@ac.uma.es (Francisco
Corbera), julian@uma.es (Juli´an Ramos), sromero@uma.es (Sergio Romero)

1877-0509 © 2013 The Authors. Published by Elsevier B.V. Open access under CC BY-NC-ND license.
Selection and peer review under responsibility of the organizers of the 2013 International Conference on Computational Science
doi:10.1016/j.procs.2013.05.311

Eladio D. Gutiérrez et al. / Procedia Computer Science 18 (2013) 1436 – 1445

1437

to the unproductive time spent in checking manually practical assignments, specially in crowded classes [7]. In
turn, students beneﬁt from a faster response and they feel more comfortable with an environment they are already
familiar with. Automatic assessment and veriﬁcation of lab assignments has been addressed in numerous works
found in the literature [8, 9], including high level language programming [10, 11, 12, 13] and lower levels or even
hardware [14, 15].
This work is focused on the usage of the CTPracticals module in the practical component of a basic subject
on computer organization taught in the ﬁrst course of the Computer Science degree, whose lab work consists of
the design and simulation of a basic CPU. This is our study case, but there are multiple introductory courses in
the Computer Science and Engineering curricula where students have to face the design and simulation of digital
circuits and elementary processors. Two topics of discussion about how to teach digital design in ﬁrst courses are
the use of academic versus commercial tools and the use of hardware description languages (e.g. VHDL) versus
schematic-based design [16, 17]. Although using commercial design tools with hardware description languages is
clearly the best option for high level courses and professional proﬁles, in authors’ experience this approach may
distract students from their learning target for an introductory course, taking into account that current commercial
tools are quite complex and not easy neither to learn or to use.
Looking for a simple tool, with a fast learning curve but also complex enough to design simple processors,
we have moved to Logisim, a schematic-based system for logic circuit design and functional simulation [18, 19]
developed for educational purposes. Insomuch as Logisim can provide textual output in batch mode, it can be
invoked directly from one of the veriﬁcation script languages, TCL or Matlab, supported by CTPracticals. Particularly, Matlab has been preferred, given its good features for data processing and analysis and its simpler syntax.
Additionally, as described in next sections, some customization of Logisim has been carried out to improve the
Logisim batch mode simulation, adding some useful options and making it generate a richer output format.
This paper is organized as follows. Next section brieﬂy introduces the CTPracticals Moodle module and
third section will provide an overview of the course and its practical assignments. Section four will present the
improvements incorporated into the program Logisim. Then, section ﬁve will describe how the Matlab testers are
designed for the automatic assessment of the previously described lab-works. And ﬁnally, the two last sections
provide, respectively, some results of the experience and brief conclusions.
2. CTPracticals Moodle module
CTPracticals is an e-assessment solution oriented to automatically verify programming-like assignments,
which has been developed as an activity of the popular LMS Moodle. It exhibits features and functionality comparable or better than other systems for the automatic assessment of programming assignments [9, 20]. Originally,
CTPracticals was created to give support to assignments consisting of VHDL projects, but the module can be
adapted to other programming languages [21]. In particular, the module was extended to support Matlab [22], a
highly adaptable language and also widely known in the scientiﬁc and engineering environments1 .
From the developer’s viewpoint, it is feasible to extend the module to other languages apart from VHDL and
Matlab. Nevertheless the door is opened to use Maltab as a scripting language for testing. Not only it allows to
invoke external tools, but to also analyze the veriﬁcation results at a quite detailed level. Therefore, the module
can be used to verify and assess student projects created with tools like Logisim, as long as such a tool provides its
output in a suitable format. Extra modiﬁcations of the module are not necessary: by invoking Java as an external
application from the Matlab script, Logisim is executed2 .
Though next lines in this section will brieﬂy review the main interfaces and functionality of the module,
the reader is addressed to [21] and the demonstration site for further details (see section 8). CTPracticals is
written in PHP and it is oﬀered as a Moodle module that can be installed in a standard linux-based Moodle
1 GNU

Octave, a Matlab free clone, is also supported.
addition to some security mechanisms implemented by CTPracticals, like chroot-ed executions, Matlab scripts are preprocessed to
avoid malicious behaviors, avoiding the execution of external commands. An exception to that limitation needs to be introduced to allow
executing Logisim through Java. Nevertheless, the command line invoking Logisim needs to match certain regular expression in order to
prevent code injection vulnerabilities.
2 In

1438

Eladio D. Gutiérrez et al. / Procedia Computer Science 18 (2013) 1436 – 1445

platform3 . CTPracticals allows users to deﬁne activity instances in the course sections, like other Moodle modules
(assignments, forums, questionnaires, ...) and its conﬁguration aspects can be controlled through a control block
on a side panel of the course page (Fig. 1). By clicking each instance, the user can access the activity itself, whose
behaviour varies from students to teachers. The module makes use of three basic objects, that can be managed by
means of three tabs on the teacher interface (Fig. 2), the Practical Assignments, the Testers, and the Veriﬁcations:
Practical assignments: Deﬁned by the teacher, each activity may contain one or several assignments, for which
each student team must submit a zip ﬁle containing its lab work. Typical attributes of a practical includes:
deadline, required ﬁles, ﬁlename constrains (expressed as a function of the team id), visibility of results,
and several others.
Testers: Designed by the teacher, the tester deﬁnes the key aspects of the veriﬁcation. It includes the command
ﬁle containing the veriﬁcation script, the result ﬁle with the reference correct output, auxiliary ﬁles used
during the veriﬁcation process and several other control options. The veriﬁcation script is responsible
for launching the code submitted by the students. It may either simply generate an output that is later
compared with the correct one, or make a more complex analysis of this output in order to get a ﬁner
grading. The teacher will have to decide the tester complexity and the amount of information provided to
students. Fig. 2(d) shows the panel for editing every attribute of a tester. A tester script template is sketched
in Fig. 3 together with the execution ﬂow of the veriﬁcation process. The tester attributes in Fig. 2 are
related to their use in the script of Fig. 3 by means of labels.
Veriﬁcations: Launched at the moment of submission, or later by the teacher in batch mode, one veriﬁcation
comprises all the outputs/ﬁles generated by the application of a certain tester, together with other important
assessment elements. These are: format errors for the submission, the result itself (OK/Wrong), the grade,
a textual short feedback, and a detailed veriﬁcation log. Statistics are also gathered during this process.
As refers to the CTPracticals student interface (Fig. 1), in addition to submitting the works, students can
browse information about their practical assignments, like their speciﬁcations, deadlines, etc. and the most valuable information: the results of the automatic veriﬁcation and, consequently, their assessment. As a remarkable
feature, the module organizes students in teams, each of which has a unique identiﬁer (team name). This allows
a true team work, unlike common Moodle assignments which are submitted and graded on a per-user basis4 . The
team creation process is very interactive and social, because students must negotiate the teams using an invitation
system provided by the module. Moreover, forcing the use of team name for deﬁning ﬁlenames can mitigate
plagiarism (stage ➀ of the template script in Fig. 3).
3. Overview of the course
Computer Technology is a basic course on computer organization delivered in the Computer Science academic
plan of the University of M´alaga (Spain). The objective of the course is to introduce how the CPU of a computer
system works at the register transfer level, covering the gap between the machine level and the assembler language.
To this end, the laboratory assignments consist of the design, implementation and simulation of a basic monocycle
implementation of the MIPS processor [23].
The chosen tool is Logisim [19], an educational tool for the design and simulation of digital circuits. Licensed
under the GNU GPL, Logisim is an open-source and multi-platform application developed in Java5 . It is also very
lightweight, so it can be used by the students even in their underperforming laptops. Logisim projects are stored
in XML format so that the storage requirements are low both in the user and the server side. The tool is simple
enough to have a quick learning curve, but complex enough to allow the design of a simple processor:
• It includes most of the basic digital circuit elements: inputs, outputs, gates, multiplexors, arithmetic circuits,
memory elements, etc. These are both easy to create and modify.
3 Moodle

is developed over the LAMP solution stack: Linux, Apache HTTP server, mysql and PHP.
that although Moodle deﬁnes the so called groups, these are used only with organizational purposes (e.g. classrooms).
5 Logisim 2.7.1, released in 2011. It requires Java 5 or later.

4 Note

Eladio D. Gutiérrez et al. / Procedia Computer Science 18 (2013) 1436 – 1445

1439

Fig. 1. (a) A CTPracticals activity in a typical Moodle course page. By clicking the activity, students can browse and submit their work in a
zip ﬁle (b). The module has associated two side blocks: one for conﬁguration (left) and another with a ranking of teams (right).

• It allows the development of hierarchical designs (top-down methodologies).
• The circuit can be veriﬁed all along its implementation (for example, mismatched sizes of buses and other
connection errors are reported).
The students lab work consists of three practical assignments. The ﬁnal score is calculated by weighing each
one with 40%, 30% y 30% respectively. These assignments are:
1. Designing a basic CPU. The students will implement a mono cycled MIPS [23], with a reduced instruction
set: J, BEQ, ALU, LW, SW. The student is provided with a full speciﬁcation of the design, the main objective
of the assessment being to get acquainted with the simulation tool (Logisim) and how to use it for the implementation, programming and simulation of the processor. The ﬁnal score is obtained after the execution,
on the student’s implementation, of a teacher’s designed program, that will test all the instruction set.
2. Extending the original instruction set. Students will introduce two new instructions: load and store with
a relative auto-incremented addressing. The only speciﬁcation is the behaviour of the instructions. Now,
they will have to modify their datapath in order for the new instructions to execute correctly. Besides, the
students delivery will include a simple program making use of them: it will have to copy 6 source data
memory positions to 6 destination data memory positions. The addresses of both sources and destinations
are speciﬁed by the teacher. Both the hardware implementation and the program must be correct in order
for the lab work to be considered a right one.
3. Optimizing an assembler program with optional further extension of the instruction set. The proposed
algorithm is the computation of the sum of the absolute value of the diﬀerences between the corresponding
elements of two vectors (SAD), operation that is used to measure their similarity. Students will perceive the
diﬃculty of writing this code, though easy, when the instruction set is so reduced, and they will have the
freedom of implementing new instructions looking for the best performance. In fact, the grade (from 0 to
10) depends on the total execution time (T EX ) of the program:
⎧
⎪
0.0,
if there are format errors or the execution result is WRONG
⎪
⎪
⎪
⎪
⎪
⎪
5.0,
if the result is OK and T EX ≥ T MAX
⎪
⎪
⎪
⎨
grade = ⎪
10.0,
if
the result is OK and T EX ≤ T MIN
⎪
⎪
⎪
⎪
−
T
T
⎪
MAX
EX
⎪
⎪
⎪
⎪
⎩5.0 + T MAX − T MIN × 5 if the result is OK and T MIN < T EX < T MAX ,
where T MAX (max. execution time) and T MIN (min. execution time) are thresholds deﬁned by the teacher.

1440

Eladio D. Gutiérrez et al. / Procedia Computer Science 18 (2013) 1436 – 1445

Fig. 2. Teacher interfaces: (a) An activity containing several practicals; (b) Per-team global view of all submissions/veriﬁcations carried out
for the activity; (c) Tester panel; (d) Editing a tester. Interface (d) is used for conﬁguring attributes of the tester. The command ﬁle contains
the veriﬁcation script whose design depends on the other attributes.

An activity instance of the CTPracticals module has been created to allow the automatic veriﬁcation and
scoring of each of the three practical assignments. It appears in the course page together with other materials,
such as documents, templates and downloads (see sample course in Fig. 1).
4. Tailoring Logisim to our needs
As aforementioned, Logisim can be invoked from the command line generating a textual output, which makes
possible the design of Matlab scripts for the automatic checking of the student’s work. However, certain features
of the original Logisim were not well suited for automatic veriﬁcation from CTPracticals. As Logisim is freely
available, it was possible tailoring the source code to our needs. Some useful options were added to make Logisim
generate a richer output format:
1. In contrast to Logisim’s native command output, which is very plain (only rows with pin value changes), the
new enhanced format contributes valuable data for subsequent analysis: full hierarchy information, signal
radix and width, circuit names, and time information (tick count) (see Fig. 3(c)).
2. The simulation results should contain not only the evolution in time of the circuit pins and probes, but also
the contents of user speciﬁed memory positions. This is important when verifying a program execution (the
program writes in the data memory). To inspect memory positions 0, 1, 2, 20, 21 and 22, you may use,
when invoked in command line mode, the new parameter:
-ram [memory locations to show comma separated]
for example:
-ram 0,1,2,20,21,22

Eladio D. Gutiérrez et al. / Procedia Computer Science 18 (2013) 1436 – 1445

1441

Fig. 3. Designing a tester: (a) Veriﬁcation ﬂow; (b) Skeleton for a veriﬁcation script; (c) Log output (summarized) of a real execution. The
diﬀerent stages in the script depend on the attributes deﬁned in the tester (Fig. 2). Teacher can perform further checking before launching
Logisim, and make some processing on the submitted circuit. After simulation the script must write results according to some CTPracticals
conventions (CTP preﬁxed functions in (a)). The log example (c) shows the new output format incorporated into original Logisim: it includes
a ﬁrst column with the clock tick count and allows showing pin and probes recursively as well as RAM locations.

3. In order to simplify the analysis of the results during the veriﬁcation stage, two new parameters allow to
select which pins/probes will appear in the simulation results: -noprobe, which disables speciﬁed probes
and -nopin, disabling the speciﬁed circuit pins.
4. Control of the simulation length is required. Originally, Logisim simulations stop after the activation of the
one bit output halt: when performing an unattended simulation, a student error would cause a malfunctioning of the veriﬁcation system. The new parameter, -stopat allows to specify the simulation length as
a maximum number of clock ticks.
5. To be able of initialize RAM contents. It is important if we want to verify whether the student’s CPU
implementation executes the teacher’s test program, and/or the teacher’s data inputs. Logisim, on circuit
load or after a reset, always initializes RAM contents to zero. The new behaviour is achieved by associating
to RAM memory a text ﬁle with the same format used by Logisim for ROM memory, such as:
v2.0 raw
0 5 4 fffffffb fffffffc 3 2 1

On circuit reset, and when the simulation is launched on the command line invocation, the ﬁle contents
are copied to the associated RAM memory. This implied an additional format modiﬁcation for the .circ
ﬁles containing the circuit description. A new text ﬁeld, “Dataﬁle on reset”, will contain the name of the
initialization ﬁle for each RAM module:
<comp lib="4" loc="(220,150)" name="RAM">
<a name="Datafile on reset" val="datafile.txt"/>
</comp>

Also, changes on the Logisim graphic interfaces were required: a new ﬁeld when visualizing the RAM
module properties allows to specify the initialization ﬁle (as shown in Fig. 4).

1442

Eladio D. Gutiérrez et al. / Procedia Computer Science 18 (2013) 1436 – 1445

Fig. 4. Screen shots showing new options added to Logisim 2.7.1. Main panel corresponds to the data RAM of the CPU to be designed .

Fig. 3 shows the log results after a tester execution. The output (Logisim simulation output=) was obtained using our modiﬁed Logisim version (logisim-dac1.1.jar), invoked as:
java -jar logisim-dac1.1.jar TestCircuit.circ -tty table -noprobe -nopin -ram 0 -stopat 1000

where TestCircuit.circ is the circuit description ﬁle, and parameter -tty table is for a command line
simulation. Due to parameters -noprobe and -nopin, the simulation output will not contain any probes or circuit
outputs, and parameter -ram 0 will include in the output the contents of memory position 0. Due to -stopat
1000 the simulation ends after 1000 clock ticks.
It is also worth mentioning that in addition to the described speciﬁc output format oriented to facilitate the
automatic veriﬁcation, Logisim was modiﬁed to generate Value Change Dump ﬁles (VCD), as it is shown in
Fig. 4. This feature allows Logisim to be combined with Gtkwave [24], a free fully featured wave viewer, which
completes the student experience with a graphical depiction of the circuit simulations.
5. Using the CTPracticals module to verify the assignments
A tester is but a MALAB script that should perform the following tasks (Fig. 3):
1. Initial checking:
• checking the submitted Logisim ﬁle (.circ): it should have been implemented using the enhanced
version of Logisim. A content search is performed inside the .circ ﬁle (a XML-like one) looking for
the strings of the names of the ﬁles specifying the RAM memory contents. If not found, testing is
aborted and an informative error message is produced inside the log ﬁle. See ➁ in Figs. 3 and 2.
• when required, the contents of the data and/or instruction memories are changed so that the test
data/program designed by the teacher will be used/executed: the corresponding strings are changed in

Eladio D. Gutiérrez et al. / Procedia Computer Science 18 (2013) 1436 – 1445

1443

the .circ ﬁle. These ﬁles were uploaded by the teacher when creating the tester as auxiliary ﬁles.
See ➂ in Figs. 3 and 2.
2. Logisim program is invoked with the appropriate parameters for the simulation and collection of the output
data required for the future veriﬁcation and assessment. If execution fails (error code when executing
Logisim) this will be reported as a format error in the log ﬁle, as well as a copy of the Logisim output. See
➃ in Fig. 3.
3. The output data is corrected comparing against a reference one and the grade is automatically produced.
See ➄ and ➅ in Figs. 3 and 2.
Now, we will describe in further detail the particularities of each of the three Logisim testers designed for the
course:
5.1. Tester for the practical assignment 1
The content of the instruction memory is substituted with a teacher’s designed program that will test all the
speciﬁed instructions. But also, it is designed in order to produce enough output information as to allow the
automatic tester to inform of which is the incorrect instruction: between every two tested instructions, the program
stores in the location 0 of the data memory the last computed value. Logisim will be invoked so that the evolution
in time of this 0 memory position will be shown as a part of its output. This output will be compared against the
reference one: the log ﬁle will inform of which are the incorrect instructions and the grade will be automatically
produced as a function of the number of errors. A fragment of a log ﬁle corresponding to an incorrect lab-work is
shown in 3(c).
5.2. Tester for the practical assignment 2
In this case, none of the contents of the memories are modiﬁed. Logisim is invoked so that the contents of the
6 source data memory positions and the 6 destination ones are shown as a part of its output. The grade will be
determined after the comparison of the source positions content with the destination ones.
5.3. Tester for the practical assignment 3
As part of the speciﬁcation, the student program should read the vectors size and their location in memory
from ﬁxed memory locations. This makes it possible to change the contents of the data memory, and thus, to use
teacher’s deﬁned test vectors. The instruction memory is not changed, as students are encouraged to extend the
instruction set searching for a reduction of the execution time of their programs. Logisim will show the contents
of the data memory position where the program output will be stored, that will be compared against the reference
one. As the Logisim output also includes the cycle count for each reported change (see section 4), when the script
ﬁnds the correct content in the output memory position, this number will be used as the execution time in order to
automatically compute the grade as described in section 3.
As a way of promoting competitiveness, the CTPracticals ranking block has been used (Fig. 1(c)). The
ranking orders the student teams according to their grades in the diﬀerent assignments. The objective is to instill
excellence: students will try to improve their CPU by designing new instructions looking for a better performance.
6. Students experience
Several have been the facets brought into play by using CTPracticals as e-assessment tool integrated in Moodle
in the dynamic of a Computer Technology laboratory. First, it allows students to work cooperatively in teams.
Second, it helps to carry out competitive learning strategies, as it happens with the third practical assignment
previously described. For this assignment, a progressive grading and the possibility of multiple retries made
teams contend for getting higher marks. This competitive approach would be diﬃcult to implement without an
automatic veriﬁcation tool. Finally, the CTPracticals infrastructure has made easier to move the lab work to a
lightweight, full featured and portable digital design tool like Logisim. Its ease of use was highly appreciated by
students.

1444

Eladio D. Gutiérrez et al. / Procedia Computer Science 18 (2013) 1436 – 1445

Tables 1 and 2 summarize some ﬁgures about the experience. About 3300 submissions were performed by
239 students distributed into ﬁve classes. Fig. 5 depicts a breakdown of the results. The inverse bathtub curve of
the wrong submissions can be explained because students need to get used to the tools when they start and the
speciﬁcation complexity is slightly increasing. Practical assignment 3 is of special interest. It was very motivating
for students, as they could know their position in the ranking. It only showed the team identiﬁers (not students’
names), giving certain anonymity to prevent possible negative eﬀects. Data from table 2 and Fig. 6 suggest
the good students’ attitude towards improving their grade. They faced optional lab work despite the baseline
design was the only mandatory to pass this assignment, and the pressure they usually have at the end of the
semester. Teachers observations agree that the automatic assessment system has acted as motivation, by providing
an eﬀective feedback mechanism that gives promptness and dynamism to the lab work.
# Students
# Registered teams
# Teams submitting all assignments
# Teams passing 3rd assignment (grade ≥ 50%)
# Submissions passing 3rd assignment
# Trials per team for 3rd assignment (avg.)
% Teams increasing its grade w.r.t. its ﬁrst submission
Improvement of grade (avg.)

239
150
93 (62%)
78 (52%)
407
4.3
43.6%
26.6%

Table 1. Some ﬁgures of the experience for the academic course
2011/12.

Fig. 5. Submissions according to results. Notice that teams can resubmit their works as many times as desired before the deadline. Resubmissions are mainly due to wrong works or grade improvement.

Number
Number of teams % of teams that Improvement
of submissions (% of the total) improve grade
of grade
1

24 (30.7%)

0 (baseline)

0 (baseline)

2-3

19 (24.4%)

26.3%

11.9%

>3

35 (44.9%)

82.9%

29.1%

Total

78 (100%)

43.6%

26.6%

Table 2. Grade evolution of 3rd practical assignment (processor
design) for those teams passing this assignment.

Fig. 6. Grade evolution of 3rd assignment for each new submission for teams, that passed this assignment, belonging to a
sample class (legend shows their team identiﬁers).

7. Conclusions
This paper has described the application of the Moodle module CTPracticals, developed by the authors, to a
computer organization laboratory taught in the ﬁrst course of a Computer Science degree. Such a module provided Moodle with a new activity class that allows the automatic veriﬁcation and e-assessment of VHDL/Matlab
assignments, as well as an enhanced management of such veriﬁcations and other useful features like team work
support. As it is an introductory course, Logisim, a simple but full featured educational tool for designing and
simulating digital circuit, has been used. With no deeper modiﬁcation, the existing support in CTPracticals can be
used to invoke this simulator, through the existing veriﬁcation engine for Matlab assignments. The simple syntax
and powerful capabilities of Matlab help teachers to design suitable testers and veriﬁcation scripts. However it
was necessary to add some new features to the original Logisim program in order to make feasible the automatic
veriﬁcation process and its use friendlier. As a way of motivating the students, the CTPracticals module has been
complemented with a new side block, a ranking that lists the team names ordered by their score. The experience
has been very satisfactory for students and teachers, making possible combining team work and soft competitive
approaches and bringing a learning-2.0 style to the practical classes.

Eladio D. Gutiérrez et al. / Procedia Computer Science 18 (2013) 1436 – 1445

1445

8. Demonstration site and acknowledgements
A demonstration web site can be visited at: http://guac.ac.uma.es/demo (User: demoteacher, password: 1234). This work has been partially supported by the educative innovation projects PIE08-062 and PIE10-140
granted by the University of M´alaga.
References
[1] H. Coates, R. James, G. Baldwin, A critical examination of the eﬀects of learning management systems on university teaching and
learning, Tertiary Education & Management 11 (1) (2005) 19–36.
[2] D. Jonassen, Constructivist learning environment on the Web: engaging students in meaningful learning, in: Education Technology
Conference and Exhibition, 1999, pp. 215–239.
[3] A. Koohang, L. Riley, T. Smith, J. Schreurs, E-learning and constructivism: From theory to application, Interdisciplinary Journal of
E-Learning and Learning Objects 5 (1) (2009) 91–109.
[4] P. Resta, T. Laferri`ere, Technology in support of collaborative learning, Educational Psychology Review 19 (1) (2007) 65–83.
[5] L. Moreno, C. Gonz´alez, I. Castilla, E. Gonz´alez, J. Sigut, Applying a constructivist and collaborative methodological approach in
engineering education, Computers and Education 49 (3) (2007) 891–915. doi:10.1016/j.compedu.2005.12.004.
[6] Main Moodle site, http://moodle.org.
[7] W. Rice, Y. Beg, C. Waters, M. Bokreta, J. Santiago-Avil´es, New pedagogical approaches to computer science education: A case study
in peer learning, in: Proceedings of the 29th ASEE/IEEE frontiers in education conference, 1999, pp. 13A4/12–13A4/15.
[8] U. St¨odberg, A research review of e-assessment, Assessment & Evaluation in Higher Education 37 (5) (2012) 591–604.
[9] R. Queir´os, J. P. Leal, Programming exercises evaluation systems: An interoperability survey, in: CSEDU 2012 - Proceedings of the 4th
International Conference on Computer Supported Education, SciTePress, 2012, pp. 83–90.
[10] D. Jackson, M. Usher, Grading student programs using ASSYST, SIGCSE Bulletin 29 (1) (1997) 335–339.
[11] L. Malmi, A. Korhonen, R. Saikkonen, Experiences in automatic assessment on mass courses and issues for designing virtual courses,
SIGCSE Bulletin 34 (3) (2002) 55–59. doi:10.1145/637610.544433.
[12] S. H. Edwards, Using software testing to move students from trial-and-error to reﬂection-in-action, ACM SIGCSE Bulletin 36 (1) (2004)
26–30. doi:10.1145/1028174.971312.
[13] C. Douce, D. Livingstone, J. Orwell, Automatic test-based assessment of programming: A review, Journal on Educational Resources in
Computing 5 (3) (2005) 4:1–13. doi:10.1145/1163405.1163409.
[14] J. Djordjevic, B. Nikolic, A. Milenkovic, Flexible web-based educational system for teaching computer architecture and organization,
IEEE Transactions on Education 48 (2) (2005) 264–273. doi:10.1109/TE.2004.842918.
[15] D. A. Poplawski, Z. Kurmas, JLS: a pedagogically targeted logic design and simulation tool, in: ITiCSE ’08: Proceedings of the 13th
annual conference on Innovation and technology in computer science education, ACM, 2008, p. 314. doi:10.1145/1384271.1384357.
[16] N. Calazans, F. Moraes, VLSI hardware design by computer science students: How early can they start? how far can they go?, in:
Frontiers in Education Conference, Vol. 3, IEEE, 1999, pp. 13C6–12.
[17] J. N. Amaral, P. Berube, P. Mehta, Teaching digital design to computing science students in a single academic term, IEEE Trans. on
Educ. 48 (1) (2005) 127–132. doi:10.1109/TE.2004.837048.
[18] C. Burch, Logisim: a graphical system for logic circuit design and simulation, Journal on Educational Resources in Computing 2 (1)
(2002) 5–16. doi:10.1145/545197.545199.
[19] Logisim, an educational tool for designing and simulating digital logic circuits, http://ozark.hendrix.edu/~burch/logisim/.
[20] P. Ihantola, T. Ahoniemi, V. Karavirta, O. Sepp¨al¨a, Review of recent systems for automatic assessment of programming assignments, in:
Proceedings of the 10th Koli Calling International Conference on Computing Education Research, ACM, 2010, pp. 86–93.
[21] E. Guti´errez, M. Trenas, J. Ramos, F. Corbera, S. Romero, A new Moodle module supporting automatic veriﬁcation of VHDL-based
assignments, Computers & Education 54 (2) (2010) 562–577. doi:10.1016/j.compedu.2009.09.006.
[22] J. Ramos, M. Trenas, E. Guti´errez, S. Romero, E-assessment of Matlab assignments in Moodle: Application to an introductory programming course for engineers, Computer Applications in Engineering Education doi:10.1002/cae.20520.
[23] D. A. Patterson, J. L. Hennessy, Computer Organization and Design: The Hardware/Software Interface (Revised 4th Edition), The
Morgan Kaufmann Series in Computer Architecture and Design, Academic Press, 2012.
[24] T. Bybell, Gtkwave electronic waveform viewer, http://gtkwave.sourceforge.net (2010).

