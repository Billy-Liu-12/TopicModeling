Available online at www.sciencedirect.com

Procedia Computer Science 18 (2013) 591 – 600

International Conference on Computational Science, ICCS 2013

Parallelization of shallow-water equations with the algorithmic
skeleton library SkelGIS
Coullon H´el`enea,b , Le Minh-Hoangc , Limet S´ebastiena
a LIFO,

Bˆatiment IIIA rue L´eonard de Vinci, 45067 Orl´eans, France
101 rue Jacques Charles, 45160 Olivet, France
Math´ematiques route de Chartres, 45067 Orl´eans, France

b G´
eo-Hyd,
c MAPMO,Bˆ
atiment

Abstract
Giving an easy access to non computer-scientists to high performance computing is a very important challenge of the research
in computer sciences. SkelGIS is an algorithmic skeleton library that aims at writing eﬃcient parallel programs in a sequential
way. In this paper, a parallel implementation of shallow-water equations using SkelGIS is presented. This implementation
is compared, in terms of learning eﬀorts and performances, to an MPI (Message Passing Interface) implementation. New
concepts of algorithmic skeletons are proposed by SkelGIS, and oﬀer a better expressiveness to the user than existing skeleton
solutions. As a result, SkelGIS enables an eﬃcient parallelization of complex numerical method with algorithmic skeletons,
and therefore without any knowledge on high performance computing.
Keywords: Parallelism; Numerical Simulation; Algorithmic Skeletons; Fluid Dynamics

1. Introduction
The accuracy of numerical models has been greatly improved during the past years mainly thanks to progress
of data acquisition in every scientiﬁc domains. As a consequence, numerical simulations need increasingly intensive computation and memory resources. This is why it becomes crucial for many scientiﬁc simulations to
improve their programs so that they can process very precise models in a reasonable time. On the other hand,
huge national calculation centers are coming out to provide to scientists heavy parallel architectures and very
powerful resources. However, it is not that easy to take advantage of these centers. Actually, high performance
computing is a tricky and specialized domain of computer sciences. It requires speciﬁc knowledge on programming, network, memory and cache accesses etc. Even if almost all scientists learn how to program during their
studies, high performance computing needs a higher level of knowledge on computing sciences. Then, two solutions seem possible to get rid of this situation. The ﬁrst one consists in teaching to every scientist parallel and
high performance programming. The second one is to provide a computing engineer staﬀ to every scientiﬁc team
which needs eﬃcient parallel programs. Unfortunately, both solutions are not practicable because of lack of time
and human resources. An alternative is to provide programming tools that allow non computer-scientists to easily
use high performance computing. This later solution is a major research challenge in computer science.
The parallelization of shallow-water equations with a new kind of algorithmic skeleton library, named SkelGIS, is presented in this article. In parallel algorithmic skeleton libraries, parallelism is hidden by distributed
E-mail address: helene.coullon@univ-orleans.fr, sebastien.limet@univ-orleans.fr, minh.hoang.le@math.cnrs.fr.

1877-0509 © 2013 The Authors. Published by Elsevier B.V. Open access under CC BY-NC-ND license.
Selection and peer review under responsibility of the organizers of the 2013 International Conference on Computational Science
doi:10.1016/j.procs.2013.05.223

592

Coullon Hélène et al. / Procedia Computer Science 18 (2013) 591 – 600

data structures and high-level abstraction functions named skeletons. The main feature of a skeleton is to provide
an adequate abstraction level that hides technicalities due to parallelism while producing eﬃcient programs. By
means of a new concept of skeletons and a hierarchical structure of skeletons, SkelGIS proposes a solution to give
access to an eﬃcient parallelization in a transparent way. SkelGIS aims at providing a 2D parallel library where
the application programming interface (API) looks like a classic sequential API for the user. In this article are
shown the feasibility and the performances of the parallelization of complex scientiﬁc simulations (shallow-water
equations) with SkelGIS. The advantages and the limitations of this approach are given.
In Section 2, we present the shallow-water equations, the numerical method of the simulation and the programming diﬃculties. Existing algorithmic skeleton libraries and contributions of SkelGIS are detailed in Section 3.
Shallow-water equations implementation with SkelGIS as well as experimental results are given in Section 4.
Finally, Section 5 concludes the article by comparing this work with some related ones.
2. Shallow-water equations and numerical method
2.1. Shallow water equations
We are concerned with the simulation of geophysical ﬂows, for example rivers, channels, dambreak problems,
ocean currents, estuarine systems, etc. In such a context, the ﬂow is often described by the shallow water equations which express the mass and momentum conservation. Shallow water equations are obtained from the three
dimensional incompressible Navier–Stokes equations by assuming the hydrostatic pressure and averaging on the
vertical direction (see e.g. [1]). In two space dimensions, the conservative form is
⎧
⎪
∂t h + div(hv) = 0,
⎪
⎪
⎨
gh2
(1)
⎪
⎪
⎪
= −gh(∇zb + S f ),
⎩ ∂t (hv) + div(hv ⊗ v) + ∇
2
where h is the water depth, v = (u, v) the ﬂow velocity, g the gravitational acceleration and zb the bed surface
elevation. The friction term S f can be estimated by several empirical relations. The most common used are the
Chezy, the Darcy-Weisbach and the Manning-Strickler laws.
2.2. Numerical method
Finite volume schemes are known to be robust for the numerical simulation of such model, i.e. the hyperbolic
system of conservation laws with source term. Using Cartesian mesh and denoting Δx, Δy the space step in x−
and y− direction respectively, the method consists in calculating the cell-centered approximation W njk of exact
solution W = (h, hu, hv)T on each cell C jk = [( j − 1/2)Δx, ( j + 1/2)Δx] × [(k − 1/2)Δy, (k + 1/2)Δy], i.e.
W njk

1
|C jk |

W(x, y, tn )dxdy.
C jk

The two-dimensional semi-discrete ﬁnite volume scheme of second order accuracy can be written under the form
1
1
d
− F Rj−1/2,k − Fc jk +
− GRj,k−1/2 − Gc jk = 0,
W jk (t) +
FL
GL
dt
Δx j+1/2,k
Δy j,k+1/2

(2)

L,R
where F L,R
j±1/2,k , G j,k±1/2 are the numerical ﬂuxes through the cell interfaces and Fc jk , Gc jk the cell-centered terms
added to preserve the consistency as presented in the sequence.
The second order accuracy in time is obtained as usual by applying the Heun’s method (second order Runge
Kutta TVD). Rewriting scheme (2) under the form

d
W(t) = A(W),
dt
the Heun’s method to obtain W n+1 from W n is
W ∗ = W n + A(W n ),

W n+1 = W n +

A(W n ) + A(W ∗ )
.
2

(3)

593

Coullon Hélène et al. / Procedia Computer Science 18 (2013) 591 – 600

The friction is treated numerically using a fractional method. It consists in rewriting (1) under the form
∂t W + ∂ x F(W) + ∂yG(W) = S x (W) + S y (W),

(4)

∂t (hv) = −ghS f ,

(5)

where
⎞
⎞
⎛
⎛
⎞
⎞
⎛
⎛
hv
hu
⎟⎟⎟
⎜⎜⎜ 0 ⎟⎟⎟
⎜⎜⎜ 0 ⎟⎟⎟
⎟⎟⎟
⎜⎜⎜
⎜⎜⎜
⎟
⎟
⎜
⎜
⎟
⎟
⎜
⎜
F(W) = ⎜⎜⎜⎜hu2 + 12 gh2 ⎟⎟⎟⎟ , G(W) = ⎜⎜⎜⎜ huv ⎟⎟⎟⎟ , S x (W) = ⎜⎜⎜⎜−gh∂ x zb ⎟⎟⎟⎟ , S y (W) = ⎜⎜⎜⎜ 0 ⎟⎟⎟⎟ .
⎠
⎝
⎝
⎠
⎠
⎝ 2 1 2⎠
⎝
−gh∂y zb
0
huv
hv + 2 gh
According to [2], a modiﬁed semi-implicit Euler scheme can be used to discretize the friction equation (5). For
the case of Manning’s law, this gives
(hv)n+1
∗
,
(6)
(hv)n+1 =
n2 |vn |
1 + gΔt n+1 4/3
(h )
the water ﬂux obtained from the ﬁrst step, i.e. system (4).
where Δt is the time step and (hv)n+1
∗
We are concerned now to numerical solver of system (4): the shallow water equations without friction. As
L,R
this system is invariant by rotation, the numerical ﬂuxes F L,R
j±1/2,k and G j,k±1/2 , also the cell-centered terms Fc jk
def

and Gc jk , have the same formulation. For the sake of simplicity, we denote W j = W jk and only describe hereafter
def

def

L,R
the formulation of F L,R
j±1/2 = F j±1/2,k and Fc j = Fc jk . Considering now the following one dimensional problem
of (4) in x− direction
(7)
∂t W + ∂ x F(W) = S x (W).

A delicate problem is to design a numerical scheme which can preserve exactly the steady states of system (7),
i.e. the exact functions h, u satisfying
hu = cst,
(8)
u2 /2 + g(h + zb ) = cst.
Such a scheme is called well-balanced scheme, since [3]. Among possible solutions, we are interested to use the
hydrostatic reconstruction technique which is known to be a simple and eﬃcient way to achieve the well-balanced
properties (see [4]).
Hydrostatic reconstruction method is motivated for nearly hydrostatic ﬂows, i.e. u
gh. In that case, the
steady states (8) are replaced by simpler relations
u = cst,
h + zb = cst.

(9)

Given the discrete data (W j , W j+1 , zb j , zb j+1 ), the ﬁrst-order scheme with hydrostatic reconstruction consists in
deﬁning the numerical ﬂuxes F L,R
j+1/2 by carrying out the two following steps:
1. Computation of the reconstructed water height according to (9)
h j+1/2L = max(0, h j + zb j − zb j+1/2 ),

h j+1/2R = max(0, h j+1 + zb j+1 − zb j+1/2 ),

(10)

where zb j+1/2 = max(zb j , zb j+1 ). The associated reconstructed states are deﬁned as
W j+1/2L = (h j+1/2L , h j+1/2L u j , h j+1/2L v j ),

W j+1/2R = (h j+1/2R , h j+1/2R u j+1 , h j+1/2R v j+1 ).

(11)

2. Deﬁnition of the numerical ﬂuxes based on the reconstructed states
⎞
⎞
⎛
⎛
0
0
⎟⎟⎟
⎟⎟⎟
⎜⎜⎜
⎜⎜⎜
⎟⎟ R
⎟⎟⎟
⎜⎜⎜ g 2
⎟
⎜⎜⎜⎜ g (h2 − h2
L
2
⎟
⎜
F j+1/2 = F (W j+1/2L , W j+1/2R )+⎜⎜
j+1/2L )⎟
⎟⎟⎠ , F j+1/2 = F (W j+1/2L , W j+1/2R )+⎜⎜⎜⎝ 2 (h j+1 − h j+1/2R )⎟⎟⎟⎟⎠ (12)
⎜⎝ 2 j
0
0

594

Coullon Hélène et al. / Procedia Computer Science 18 (2013) 591 – 600

where F (W j+1/2L , W j+1/2R ) is any consistent numerical ﬂux for the homogeneous system
∂t W + ∂ x F(W) = 0.
In this work, we used the HLLC ﬂux which is known to be a simple and eﬃcient solver for both accuracy
and implementation aspects (see [5]).
Starting from the ﬁrst-order method, i.e. procedure (10, 11, 12), a common way to obtain a second-order
accuracy on space is to compute the ﬂuxes (12) from limited reconstructed values on both sides of each interface
denoted {W j+1/2± , zb j+1/2± } rather than cell-centered values {W j , zb j }. According to [6], the limited reconstruction
operators MUSCL has been recommended for numerical methods in the context of shallow ﬂows. The reconstruction applied to the water height h is written as
h j−1/2+ = h j −

Δx
Dh j ,
2

h j+1/2− = h j +

Δx
Dh j ,
2

(13)

where the operator Dh j is deﬁned by

Dh j = minmod

h j − h j−1 h j+1 − h j
,
,
Δx
Δx

⎧
⎪
⎪
min(a, b) if a, b ≥ 0
⎪
⎪
⎪
⎨
minmod(a, b) = ⎪
max(a, b) if a, b ≤ 0
⎪
⎪
⎪
⎪
⎩0
else.

An analogous of (13) has been also applied to the free surface z s = h + zb to deduce the topography zb j−1/2+ =
z s j−1/2+ − h j−1/2+ and zb j+1/2− = z s j+1/2− − h j+1/2− . The reconstruction has been also applied on u, v as follows
u j−1/2+ = u j −

h j+1/2− Δx
Du j ,
hj
2

u j+1/2− = u j +

h j−1/2+ Δx
Du j .
hj
2

(14)

Finally, a simple well-balanced choice for the cell-centered source term can be used as
⎞
⎛
0
⎟⎟⎟
⎜⎜⎜
⎟⎟
⎜⎜⎜ g
⎜
Fc j = ⎜⎜− (h j−1/2+ + h j+1/2− )Δzb j ⎟⎟⎟⎟ , where Δzb j = zb j+1/2− − zb j−1/2+ .
⎟⎠
⎜⎝ 2
0

(15)

2.3. Programming diﬃculties
The numerical method described above is very complex to program, even in sequential programming style.
The ﬁrst one is the amount of data to compute since the second order simulation needs the use of thirty seven
matrices. This means that the total amount of data is about thirty seven time the size of the initial domain of the
simulation. As a result, this simulation is time and memory consuming, and a parallel version would be very interesting to get better scientiﬁc results. For example, the application would be able to run on bigger domains or on
more precised domains. The second diﬃculty of this program is that it needs complex neighborhood calculations.
Actually, in the hydrostatic reconstruction (Equations 10-12) and the second-order reconstruction (Equations 1314) for example, are based on complex relations between matrices. Indeed, the result matrix needs few other
matrices to be computed and theses matrices do not represent points of the same domain but interfaces of the cells
in the domain. Figure 1 gives an illustration on a simple case with two matrices. The ﬁrst matrix A is calculated from the second matrix B. Cells of Matrix B represent interfaces of Matrix A. A last diﬃcult programming
point comes from the boundary conditions. Actually, this point is responsible for performance problems because
it causes additional computations for physical borders of the domain. This creates load balancing problems and
performance damages.

Coullon Hélène et al. / Procedia Computer Science 18 (2013) 591 – 600

Fig. 1. Example of neighborhood relations between matrices

3. SkelGIS
3.1. Algorithmic skeleton libraries
Parallel algorithmic skeletons were introduced in 1988 by Muray Cole [7]. Algorithmic skeleton libraries
are based on functional programming paradigm. Actually, a skeleton library is based on two major concepts :
distributed data structures and algorithmic skeletons. The distributed data structure is responsible for the parallel
execution of the ﬁnal application. Skeleton concept is used to express what action has to be done on the distributed
matrix in parallel. Diﬀerent types of skeletons exist to express diﬀerent kind of actions that can be done on data
structures. For example, the map skeleton applies a sequential function on a set of elements of an input distributed
data structure and writes the resulting elements in a new data structure. The function needed by the map skeleton
describes, in a sequential programming style, what has to be done on a single element of the data structure. Thus,
the programmer needs to declare the distributed data structure on which the computation is going to be made,
write a sequential function to describe the action on one element, and call the map skeleton. The calculation is
applied in parallel by the skeleton on the distributed data structure, but a sequential code has been written by the
user. Another example of well known skeleton is called zip. It works almost as the map but on two distributed
data structures as inputs.
Many libraries are based on the principles of Muray Cole and on his recommendations for algorithmic skeleton
libraries [8, 9]. Among them, two C++ skeleton libraries propose two dimensional data structures and are closed
to SkelGIS: Muesli [10] from M¨unster and SkeTo [11] from Tokyo. Muesli is less competitive than SkeTo because
of lack of C++ optimizations, but provides an hybrid MPI/OpenMP version for SMP clusters. The two libraries
oﬀer some basic skeletons as map, zip and reduce. SkeTo is very eﬃcient on the standard use of these skeletons.
Though, map and zip skeletons are too restricted to program interesting parallel problems since they perform
only local calculations. Actually, the user function prototype used by those skeletons is f : E −→ E, where E
is the type of one element of the data structure. In other words, f applies locally on a single element. This is
too restrictive in general. This is why communication skeletons were introduced. For example, shift skeleton is
a communication skeleton that shifts the data structure to get a neighbor element. However, such a solution does
not really ﬁt targeted applications. Indeed, for a simple example (illustrated Figure 2) that computes a minimum
value between a cell and its eight neighbors in a 2D matrix, twenty-four skeletons must be called. This damages
both readability and eﬃciency of the program. One can note that the code presented Figure 2 is far away from
usual sequential programming.
The performance decrease comes from both the number of scans of the data structures and the number of data
structure copies needed to perform the twenty-four calls. Indeed, each call scans the data structure entirely to
apply the user function on each element and each shift skeleton perform a copy of the data structure.
For example, on a four cores machine with 4GB of memory, SkeTo library needs 30ms to perform the computation of Figure 2 on a 339 × 225 matrix and 101s on a 14786 × 10086 matrix. The equivalent SkelGIS program,
presented in Figure 3, needs only 21ms for the ﬁrst matrix and 31s for the second one.
SkelGIS proposes a new approach of algorithmic skeletons which diﬀers from initial Muray Cole’s deﬁnitions
and from functional programming concepts. However this new approach oﬀers more opportunities for parallelization of scientiﬁc simulations.
3.2. Concepts of SkelGIS library
The SkelGIS library relies on two main concepts.
The ﬁrst concept of SkelGIS is to propose basic skeletons where the user function deﬁnes what to do with a
matrix instead of a single element. This is a major diﬀerence with the classical skeletons where the user deﬁnes

595

596

Coullon Hélène et al. / Procedia Computer Science 18 (2013) 591 – 600

z i p w i t h ( min ,
z i p w i t h ( min ,
z i p w i t h ( min ,
z i p w i t h ( min , map ( f ,m) , map ( f ,
s h i f t <1 ,1 >( s h i f t i d ,m ) ) ) ,
z i p w i t h ( min , map ( f , s h i f t <1 ,0 >( s h i f t i d ,m) ) ,
map ( f , s h i f t <1 , −1 >( s h i f t i d ,m ) ) ) ) ,
z i p w i t h ( min ,
z i p w i t h ( min , map ( f , s h i f t <0 , −1 >( s h i f t i d ,m) ) ,
map ( f , s h i f t < −1 , −1 >( s h i f t i d ,m ) ) ) ,
z i p w i t h ( min , map ( f , s h i f t < −1 ,0 >( s h i f t i d ,m) ) ,
map ( f , s h i f t < −1 ,1 >( s h i f t i d ,m ) ) ) ) ) ,
map ( f , s h i f t <0 ,1 >( s h i f t i d ,m) )
);

Fig. 2. Nested skeleton calls for 8-neighbors algorithm

/ / i n p u t and o u t p u t m a t r i x i t e r a t o r s
DMatrix < f l o a t > : : i t e r a t o r i t = i n p u t . b e g i n ( ) ;
DMatrix < i n t > : : i t e r a t o r i t O = o u t p u t . b e g i n ( ) ;
f l o a t ∗ nghb ;
f o r ( ; i t < i n p u t . end ( ) ; ++ i t , ++ i t O u t )
{ / / get the value of the current i t e r a t o r
float value = input [ i t ] ;
/ / get the 8 neighbors
i n p u t . g e t 8 N e i g h b o r s ( i t , nghb ) ;
f l o a t min= v a l u e ; i n t d i r m i n =9;
f o r ( i n t i =0; i <8; i ++)
i f ( nghb [ i ] <minimum )
{ min=nghb [ i ] ; d i r m i n = i ; }
/ / w r i t e t h e minimum v a l u e i n o u t p u t
output [ itO ] = dirmin ; }
Fig. 3. SkelGIS code for 8-neighbors algorithm

functions of type f : E −→ E (E is the type of elements of a matrix m). This concept is responsible for the
eﬃciency of SkelGIS on complex simulations. Actually, this feature resolves two problems of existing skeletons.
First, communication skeletons are not needed anymore since the user function describes directly the calculation
on a matrix (with free access to elements). Secondly, the readability of the programs is improved since the user
does not need to use many nested skeleton calls. As a result, SkelGIS programs are closer to sequential programs
and do not suﬀer from performance decrease.
The data structure used by SkelGIS skeletons is called distributed matrix. This data structure supports datadistribution as well as data access. Four kinds of basic skeletons dwell in SkelGIS. The apply unary skeleton
takes a distributed matrix and a user function as inputs and returns a new distributed matrix as outputs. The
apply binary skeleton takes two distributed matrices and a user function as inputs and returns a single distributed
matrix as output. The applyList skeleton, deﬁned by Equation (16), takes a list of distributed matrices and a user
function as inputs and returns a list of distributed matrices as outputs. Finally, the apply reduction skeleton takes
a distributed matrix and a user function as inputs and returns a single element. These skeletons require diﬀerent
types of user function (Equation (17) deﬁnes the prototype of the user function used by applyList). All of them
manipulates matrices as inputs instead of a single element of the data structure.
applyList : Flist × {DMatrix} −→ {DMatrix} (16)

Flist = { f : {DMatrix} −→ {DMatrix}}

(17)

The matrix manipulated in the user function (named a DMatrix) is distributed with a static domain decomposition, but this point is hidden from the user thanks to some DMatrix tools. The library proposes iterators, (i, j)
accessors, functions to manage physical border of a simulation, and functions to get a neighborhood around an
element. As a result the user code looks like a C++ code using the STL library. To illustrate this point, the Figure 3
represents the user function deﬁnition for the example of Section 3.1. No parallelization concept is needed at all,
the code is easy to write and read compared to the one of Figure 2. It is also more eﬃcient as shown in Section 3.1.
The second concept of SkelGIS is to propose a hierarchy of skeletons where every higher abstraction level
skeleton uses basic skeletons. Ensuring the scalability and durability of a library is an important feature to make it
live and used. Skeleton libraries are based on parallel libraries depending on hardware, and hardware is changing
quite quickly. Existing libraries propose a set of skeletons that are independent from each others. Each skeleton is
implemented on one or several speciﬁc libraries. As a result, taking into account a new hardware (or a new parallel
library) requires to re-implement all the skeletons of the library. In SkelGIS, every skeleton inherits optimization
and hardware support from basic skeletons. New optimizations or new hardware support only have to be added in
basic skeletons to be available in the whole library. This hierarchy also provides a clear abstraction choice for the
user where each abstraction level keeps a sequential programming style and a totally hidden parallelization.

Coullon Hélène et al. / Procedia Computer Science 18 (2013) 591 – 600

4. Shallow-water equations and SkelGIS
4.1. Implementation with SkelGIS
Parallelization of the shallow-water equations with SkelGIS was made in two steps. The ﬁrst step was to adapt
the order 1 program to SkelGIS. A mathematician made this implementation in collaboration with a computer
scientist who knows SkelGIS. The second step was to implement the order 2 algorithm which is much more tricky
than the order 1. This step was left to the mathematician. After a basic learning of SkelGIS (which took few days),
no major diﬃculties appeared to get a parallel and eﬃcient version of the code. The Algorithm 1 represents the
pseudo code of the shallow-water equations simulation. In this algorithm, calls to SkelGIS library are written in
red. First, initializations of matrices have to be made with the DMatrix object, then each algorithm step has to be
executed through a call to the applyList skeleton. Finally, the sequential code has to be sightly modiﬁed to use the
accessors and iterators of Dmatrix instead of classical access of C++ arrays. This implementation is available at
the following URL http://www.univ-orleans.fr/mapmo/soft/FullSWOF/shallow-water_equations_
skelgis.tar.gz.
Algorithm 1: Shallow-water equations algorithm with additional calls in red for SkelGIS
Initialization of variables with DMatrix
for t ← 0 to N do
for i ← 0 to 2 do
applyList : Boundary Conditions
applyList : Second order reconstruction : equations 13 and 14
applyList : Hydrostatic reconstruction : equation 10 and 11
applyList : Numerical ﬂux computation : equation 12
applyList : Cell-centered source term computation : equation 15
applyList : Scheme computation : equation 2
applyList : Frictions : equation 6
end
applyList : Order 2 along t - Method of Heun : equation 3
end

4.2. Results
In this section, a comparison of two parallel versions of shallow-water equations is made. One version was
parallelized by a mathematician engineer using the MPI library, and the second version was parallelized using
SkelGIS as described in the previous section. Both mathematicians are specialists of the sequential simulation
code but did not know much on parallel programming. The two versions were written simultaneously. The ﬁrst
version needed an extensive training of parallel programming using MPI library. The resulting MPI program is not
optimized but has good performances. SkelGIS version did not require any parallelization knowledge, but a short
learning of basic tools of the library. Two series of experiments have been done for these benchmarks. For both,
the domain of the simulation is a matrix of 5120 × 5120 points (Recall that the simulation requires thirty seven
times such a matrix). The ﬁrst one executes 5000 iterations of the simulation whereas the second one executes
20.000 iterations of the simulation. Figure 4 plots the execution times as a function of the number of processors,
using logarithmic scales base two on both axis. This graph represents the inverse line of the speedup but also
illustrates the execution times.
Four points can be noticed in this ﬁgures. First, the sequential execution time of SkelGIS version is much
better than the MPI one. This is due to the fact that a problem exists in the sequential version of the simulation.
Actually, the sequential version of the simulation has memory issues, leaks or useless memory allocations. As
a result, for the sequential execution, the simulation must use memory swapping which is very time consuming.
This is attested by the fact that execution times become similar to SkelGIS with 32 cores. At this point, indeed,
enough memory is used on PC-cluster to not swap memory anymore. This remark points out an unexpected

597

598

Coullon Hélène et al. / Procedia Computer Science 18 (2013) 591 – 600

(a) Matrix 5120x5120 5000 iterations

(b) Matrix 5120x5120 20.000 iterations

Fig. 4. Experimental comparisons between MPI and SkelGIS versions of the simulation

consequence of SkeGIS use: with almost no coding eﬀorts, but only using SkelGIS data structures, the sequential
code becomes eﬃcient and resolves memory management problems. The second point to notice is that for both
MPI and SkelGIS versions, a landing is observed from 4 to 8 cores in the parallel simulation. This problem is
due to load balancing problems that appear for 8 and more cores because of boundary conditions. It is diﬃcult to
deal with this problem with a static domain decomposition, however phenomena could be improved with metaprogramming or with speciﬁc optimizations. The third important point is that SkelGIS speedup is linear from 8
to 256 processors for 5000 iterations as for 20.000 iterations, which proves the good scalability of codes written
with SkelGIS. The last noticeable point is that the SkelGIS version is more eﬃcient than the MPI one with less
coding eﬀorts. From 32 cores to 256 cores, SkelGIS is about 5% better with 5000 iterations and 2.6% better with
20.000 iterations.
In addition to the good eﬃciency and scalability of the program, few other advantages can be noticed for
the SkelGIS version of shallow-water equations. First, this version is modular and can easily be modiﬁed. It is
actually quite easy to add new features to this parallel version of the shallow-water equations. This version of
the program does not contain any explicit calls to MPI library. Furthermore, thanks to its skeleton hierarchy, an
OpenMP version of SkelGIS will be soon available. As a result, shallow-water equations simulation will work on
shared memory architectures without any code modiﬁcation. On the other hand, the MPI version is intrinsically
bound to the MPI library and its distributed memory model. The learning eﬀorts made for the MPI version have
to be done again to develop a new version based on another parallelism model.
5. Related work and conclusion
The best way to get an optimal parallel application remains to rewrite with speciﬁc optimizations a parallel
version of a sequential code using MPI, OpenMP, CUDA or any other language adapted to a parallel hardware
architecture. However, deep knowledge is needed on both the scientiﬁc domain of the simulation and on parallel
programming. Most scientists are not experts of computer science, but develop their own applications to keep
control on their codes. The problem is that, even with a double scientiﬁc skill, it becomes more and more diﬃcult
to take advantages of parallel architectures without the help of an expert of parallel computing. This is why lots
of projects associate computer scientists with other thematic scientists. In the context of shallow-water equations
solvers or simulations, some work have been made on the parallelization of diﬀerent numerical methods. For
example, the work of K. Ganeshamoorthy & al [12] and the work of A. Delis [13] implement a shared-distributed
memory solution using MPI and pthread libraries in C++. In another parallel paradigm, the work of M.J. Castro [14] proposes an implementation of the simulation using SSE instructions (“Streaming SIMD Extensions”)
and MPI. It is almost impossible to compare results of these diﬀerent papers to each other and to those presented

Coullon Hélène et al. / Procedia Computer Science 18 (2013) 591 – 600

in Section 4. The numerical models chosen to implement equations are not the same, and as a result the complexity and the execution time of programs are diﬀerent. However, each of these papers focuses on the tricky
optimizations used to obtain an eﬃcient parallel program using a speciﬁc parallel library.
Unfortunately, it is not possible to implement parallel version of each scientiﬁc application this way, because
of lack of time and human resources. This is why it becomes a crucial point to let scientists write their own
eﬃcient parallel applications. Except “parallel algorithmic skeletons” that have been discussed in Section 3.1, two
other solutions have been proposed namely the concept of “Domain Speciﬁc Language” (DSL) and the “Stencil”
programming.
The purpose of a DSL is to deﬁne and write a language adapted to a speciﬁc scientiﬁc domain. As a result,
experts of the domain would be able to write their own applications without knowledge in C, Fortran, C++ or
any other programming language. A complex analysis is needed to write a well-deﬁned language that is really
adapted to the targeted domain, but the result would be the minimization of learning eﬀorts for the expert. To
propose eﬃcient applications to scientists it becomes essential to propose parallel DSL. In this particular domain,
the most powerful solution is the Scala and Delite [15, 16] frameworks. Scala is a DSL that permits to deﬁne
other DSL. With the deﬁnition of a DSL made with Scala, Delite is able to compile the language and to generate a
parallel execution application adapted to the hardware architecture. In this solution, the generated code is in Java.
SkelGIS solution is not as general as these frameworks. Actually, even if a skeleton library can be considered as
a large DSL itself, it is not possible to deﬁne a new language with skeletons. New skeletons can be written with
existing ones, but the language will remain the same. However, SkelGIS does not aim to deﬁne DSL for multiple
reasons. First, the deﬁnition of a new parallel DSL with Scala and Delite is not an easy task, and it seems diﬃcult
for a non-computer scientist to deﬁne a DSL for the domain he works on. As a result, even if the ﬁnal DSL can
be used easily by the scientist for any application of his domain and proposes an eﬃcient parallel execution of
it, a computer-scientist is needed to deﬁne the DSL. Then, time and human resources problems are moved from
the parallel library use (MPI, CUDA etc.) to the deﬁnition of an adapted DSL. SkelGIS aims at ﬁnding a general
solution to write parallel eﬃcient applications, not especially close to a speciﬁc domain. It proposes regular tools
already known by scientists to oﬀer them a true autonomy. Indeed, in every scientiﬁc schooling at university,
computer sciences are taught and almost all scientists are able to write a sequential program. This is why SkelGIS
tries to oﬀer a good compromise between the learning eﬀort and the eﬃciency of the generated programs. Lighter
DSL solutions as ZPL [17] or Odeint [18] propose simpler DSL adapted to a speciﬁc domain. ZPL is made for
matrices calculations and proposes an MPI implementation. Odeint is made for solving and simulating ordinary
diﬀerential equations and proposes a CUDA implementation. Both these DSL are very eﬃcient, and well designed,
however it seems that it is not possible or very diﬃcult to use them in a more general and complex simulation
because languages are too speciﬁc.
Conversely, stencil programming is not dedicated to a speciﬁc domain but tries to give an eﬃcient general
solution for a recurrent problem in numerical simulation code namely the neighborhood computation. In other
words, stencil programming proposes a high abstraction level of programming for speciﬁc calculations where a
neighborhood of the current element is needed. A stencil is actually a mask window representing the neighborhood, and a function indicates the calculation to perform on this mask. The mask is moved in the global matrix
and the function is applied to each mapping of the mask on the matrix. Every stencil frameworks that we studied are designed for shared memory parallel architectures. Functional deﬁnitions have been studied for stencil
programming. For example, a Haskell solution has been proposed by Gabriele Keller & al [19]. Some implementations propose iterative stencil solutions as for example the work of Michael Lesniak [20], written in Haskell too,
where in addition to the stencil, a stopping condition is given. Shoaib Kamil & al [21] give an auto-tuning hybrid
optimization solution that is very powerful. Stencil programming is very close to SkelGIS because it tries to give
a general solution, to hide the parallel code, and to propose a regular sequential programming style to the user.
However stencil programming is less expressive than SkelGIS skeletons. In addition to this, if a problem cannot
be expressed as a stencil program it cannot be parallelized using this technique. In fact, it is possible to implement
a stencil skeleton in SkelGIS based on the basic skeletons.
Many studies have been done to give a transparent access to parallelism to every scientists. However, as far
as we know, no parallelizations of complex simulations have been done using these techniques. In one hand,
with existing DSL on matrix calculations, as for example ZPL, a parallelization of shallow-water equations seems
really diﬃcult and long because of the speciﬁcity of the language. On the other hand, with existing simple stencil

599

600

Coullon Hélène et al. / Procedia Computer Science 18 (2013) 591 – 600

solutions, diﬃcult notions as hydro-statical reconstruction seem not possible to express. And ﬁnally, with existing algorithmic skeletons as map, zip and shift (as in Figure 2) it would be very complex and long to write
shallow-water equations resolution. Moreover, interactions between matrices that represent the domain and those
that represent the interfaces (as seen in Figure 1) would drastically complicate the code. Results given in Section 4 show that SkelGIS is enough open to allow an eﬃcient implementation of very complex simulations but
also enough closed to hide parallel technicalities from the user. The work presented in this article opens new
perspectives in both shallow-water simulation and computer-science. Thanks to its scalability and its eﬃciency,
the SkelGIS implementation will be the basis of a ﬂooding simulation at the scale of a river catchment basin. It
will also facilitate the introduction of new parameters in the simulation. On the computer-science side, SkelGIS
will be enriched in several directions. First, several implementations using diﬀerent parallel libraries of the basic
skeleton are under construction. Some middle and higher level skeletons are also under development. Finally it is
planned to implement new skeletons on diﬀerent distributed data structures like trees to be able to code eﬃciently
other kinds of simulations as, for example, rivers pollutant dissemination.
References
[1] S. Ferrari, F. Saleri, A new two-dimensional shallow water model including pressure eﬀects and slow varying bottom topography, M2AN
Math. Model. Numer. Anal. 38 (2) (2004) 211–234.
[2] M.-O. Bristeau, B. Coussin, Boundary conditions for the shallow water equations solved by kinetic schemes, Tech. Rep. 4282, INRIA
(Oct. 2001).
[3] J. M. Greenberg, A.-Y. LeRoux, A well-balanced scheme for the numerical processing of source terms in hyperbolic equation, SIAM
Journal on Numerical Analysis 33 (1996) 1–16.
[4] F. Bouchut, Nonlinear stability of ﬁnite volume methods for hyperbolic conservation laws, and well-balanced schemes for sources, Vol.
2/2004, Birkh¨auser Basel, 2004. doi:10.1007/b95203.
[5] E. Toro, Shock-Capturing Methods for Free-Surface Shallow Flows, Wiley and Sons Ltd., 2001.
[6] O. Delestre, Simulation du ruissellement d’eau de pluie sur des surfaces agricoles/ rain water overland ﬂow on agricultural ﬁelds simulation, Ph.D. thesis, Universit´e d’Orl´eans (in French), available from TEL: tel.archives-ouvertes.fr/INSMI/tel-00531377/fr (Jul. 2010).
[7] M. I. Cole, Algorithmic skeletons: a structured approach to the management of parallel computation, Ph.D. thesis, University of Glasgow,
aAID-85022 (1988).
[8] A. Benoit, M. Cole, Two fundamental concepts in skeletal parallel programming, in: Proceedings of the 5th international conference on
Computational Science - Volume Part II, ICCS’05, Springer-Verlag, Berlin, Heidelberg, 2005, pp. 764–771.
[9] M. Cole, Bringing skeletons out of the closet: a pragmatic manifesto for skeletal parallel programming, Parallel Comput. 30 (2004)
389–406.
[10] G. H. Botorog, H. Kuchen, Eﬃcient parallel programming with algorithmic skeletons., in: L. Boug, P. Fraigniaud, A. Mignotte, Y. Robert
(Eds.), Euro-Par, Vol. I, Vol. 1123 of Lecture Notes in Computer Science, Springer, 1996, pp. 718–731.
[11] K. Matsuzaki, H. Iwasaki, K. Emoto, Z. Hu, A library of constructive skeletons for sequential style of parallel programming, in: Infoscale,
2006, p. 13.
[12] K. Ganeshamoorthy, D. N. Ranasinghe, K. P. M. K. Silva, R. Wait, Parallel implementation of shallow water model on distributed
memory architectures, in: ISCA PDCS, 2007, pp. 181–186.
[13] A. I. Delis, E. N. Mathioudakis, A ﬁnite volume method parallelization for the simulation of free surface shallow water ﬂows, Math.
Comput. Simul. 79 (11) (2009) 3339–3359.
[14] M. J. Castro, J. A. Garc´ıa-Rodr´ıguez, J. M. Gonz´alez-Vida, C. Par´es, Solving shallow-water systems in 2d domains using ﬁnite volume
methods and multimedia sse instructions, J. Comput. Appl. Math. 221 (1) (2008) 16–32.
[15] H. Chaﬁ, Z. Devito, A. Moors, T. Rompf, A. K. Sujeeth, P. Hanrahan, M. Odersky, K. Olukotun, Language virtualization for heterogeneous parallel computing (2010).
[16] K. J. Brown, A. K. Sujeeth, H. Lee, T. Rompf, H. Chaﬁ, M. Odersky, K. Olukotun, A heterogeneous parallel framework for domainspeciﬁc languages, in: PACT, 2011, pp. 89–100.
[17] L. Snyder, The design and development of zpl, in: HOPL, 2007, pp. 1–37.
[18] K. Ahnert, M. Mulansky, Odeint - solving ordinary diﬀerential equations in c++, CoRR abs/1110.3397.
[19] B. Lippmeier, G. Keller, Eﬃcient parallel stencil convolution in haskell, SIGPLAN Not. 46 (12) (2011) 59–70.
doi:10.1145/2096148.2034684.
[20] M. Lesniak, Pastha: parallelizing stencil calculations in haskell, in: Proceedings of the 5th ACM SIGPLAN workshop on Declarative
aspects of multicore programming, DAMP ’10, ACM, New York, NY, USA, 2010, pp. 5–14. doi:10.1145/1708046.1708052.
[21] S. Kamil, C. Chan, L. Oliker, J. Shalf, S. Williams, An auto-tuning framework for parallel multicore stencil computations, in: IPDPS,
2010, pp. 1–12.

