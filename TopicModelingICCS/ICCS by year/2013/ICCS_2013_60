Available online at www.sciencedirect.com

Procedia Computer Science 18 (2013) 2458 – 2467

Adaptive Preshufﬂing in Hadoop clusters
Jiong Xie, Yun Tian, Shu Yin, Ji Zhang, Xiaojun Ruan,
and Xiao Qin
Department of Computer Science and Software Engineering
Auburn University, Auburn, AL 36849-5347
Email: {jzx0009, tianyun, szy0004, Jzz0014, xzr0001}@eng.auburn.edu,
xqin@auburn.edu http://www.eng.auburn.edu/∼xqin

Abstract—MapReduce has become an important distributed

II. M OTIVATIONS

processing model for large-scale data-intensive applications like
data mining and web indexing. Hadoop–an open-source imple-

A. Shufﬂe-Intensive Hadoop Applications

mentation of MapReduce is widely used for short jobs requiring
low response time. In this paper, We proposed a new preshufﬂing

Recall that a Hadoop application has two important phases

strategy in Hadoop to reduce high network loads imposed by

- map and reduce. The execution model of Hadoop can be

shufﬂe-intensive applications. Designing new shufﬂing strategies

divided into two separate steps. In the ﬁrst step, a map task

is very appealing for Hadoop clusters where network intercon-

loads input data and generates some ¡key,value¿ pairs. In this

nects are performance bottleneck when the clusters are shared

step, multiple map tasks can be executed in parallel on multiple

among a large number of applications. The network interconnects
are likely to become scarce resource when many shufﬂe-intensive

nodes in a cluster. In step two, all the pairs for a particular

applications are sharing a Hadoop cluster. We implemented the

key are pulled to a single reduce task after the reduce task

push model along with the preshufﬂing scheme in the Hadoop

communicates and checks all the map tasks in the cluster.

system, where the 2-stage pipeline was incorporated with the
preshufﬂing scheme. We implemented the push model and a
pipeline along with the preshufﬂing scheme in the Hadoop system.

Reduce tasks depend on map tasks; map tasks are followed
by reduce tasks. This particular sequence prevents reduce

Using two Hadoop benchmarks running on the 10-node cluster,

tasks from sharing the computing resources of a cluster with

we conducted experiments to show that preshufﬂing-enabled

map tasks, because there is no parallelism between a pair of

Hadoop clusters are faster than native Hadoop clusters. For

map and reduce tasks. During an individually communication

example, the push model and the preshufﬂing scheme powered
by the 2-stage pipeline can shorten the execution times of the
WordCount and Sort Hadoop applications by an average of 10%
and 14%, respectively.

between a set of map tasks and a reduce task, an amount
of intermediate data (i.e., result generated by the map tasks)
is transferred from the map tasks to the reduce task through
the network interconnect of a cluster. This communication

I. INTRODUCTION
In the past decade, the MapReduce framework has been

between the map and reduce tasks is also known as the shufﬂe
phase of a Hadoop application.

employed to develop a wide variety of data-intensive appli-

In an early stage of this study, we observe that a Hadoop ap-

cations in large-scale systems. In this paper, we focus on a

plication’s execution time is greatly affected by the amount of

new reshufﬂing scheme to further improve Hadoop’s system

data transferred during the shufﬂe phase. Hadoop applications

performance.

generally fall into two camps, namely, non-shufﬂe-intensive

1877-0509 © 2013 The Authors. Published by Elsevier B.V. Open access under CC BY-NC-ND license.
Selection and peer review under responsibility of the organizers of the 2013 International Conference on Computational Science
doi:10.1016/j.procs.2013.05.422

Jiong Xie et al. / Procedia Computer Science 18 (2013) 2458 – 2467

2459

and shufﬂe-intensive applications. Non-shufﬂe-intensive ap-

2) Second, there is no need for reduce tasks to wait for

plications transfer a small amount of data during the shufﬂe

map tasks to generate an entire intermediate data set

phase. For instance, compared with I/O-intensive applications,

before the data can be transferred to the reduce tasks.

computation-intensive applications may generate a less amount

Rather, a small portion of the intermediate data set can

of data in shufﬂe phases. On the other hand, shufﬂe-intensive

be immediately delivered to the reduce tasks as soon as

applications move a large amount of data in shufﬂe phases,

the portion becomes available.

imposing high network and disk I/O loads. Typical shufﬂe-

3) Third, heavy network loads can be hidden by over-

intensive applications include the inverted-index tool used in

lapping data communications with the computations of

search engines and the k-means tool applied in the machine

map tasks. To improve the throughput of the com-

learning ﬁeld. These two applications transfer more than 30%

munication channel among nodes, intermediate results

data through network during shufﬂe phases.

are transferred from map tasks to reduce tasks in a
pipelining manner. Our preliminary ﬁndings show that

B. Alleviate Network Load in the Shufﬂe Phase

shufﬂe time is always much longer than map tasks’

In this paper, we propose a new shufﬂing strategy in Hadoop

computation time; this phenomenon is especially true

to reduce heavy network loads caused by shufﬂe-intensive

when network interconnects in a Hadoop cluster are

applications. The new shufﬂing strategy is important, because

saturated. A pipeline in the shufﬂe phase can help in

network interconnects in a Hadoop cluster is likely to become

improving throughput of Hadoop clusters.

a performance bottleneck when the cluster is shared among a

4) Finally, map and reduce tasks allocated within a single

large number of applications running on virtual machines. In

computing node can be coordinated in a way to have

particular, the network interconnects become scarce resource

their executions overlapped. Overlapping these opera-

when many shufﬂe-intensive applications are running on a

tions inside a node can efﬁciently shorten the execution

Hadoop cluster in parallel.

times of shufﬂe-intensive applications. A reduce task

We propose the following three potential ways of reducing

checks all available data from map nodes in a Hadoop

network loads incurred by shufﬂe-intensive applications on

cluster. If reduce and map tasks can be grouped with

Hadoop clusters.

particular key-value pairs, network loads incurred in the

1) First, decreasing the amount of data transferred during

shufﬂe phase can be alleviated.

the shufﬂe phase can effectively reduce the network burden caused by the shufﬂe-intensive applications. To re-

C. Beneﬁts and Challenges of the Preshufﬂing Scheme

duce the amount of transferred data in the shufﬂe phase,

There are three beneﬁts of our preshufﬂing scheme:

combiner functions can be applied to local outputs by

•

mized.

map tasks prior to storing and transferring intermediate
data. This strategy can minimize the amount of data that

•

Long data transfer times are hidden by a pipelining
mechanism.

needs to be transferred to the reducers and speeds up the
execution time of the job.

Data movement activities during shufﬂe phases is mini-

•

Grouping map and reduce pairs to reduce network load.

2460

Jiong Xie et al. / Procedia Computer Science 18 (2013) 2458 – 2467

Before obtaining the above beneﬁts from the preshufﬂing

a user-deﬁned reduce function is executed to process each

scheme, we face a few design challenges. First, we have to

assigned key and its list of values.

design a mechanism allowing a small portion of intermediate

To fetch intermediate data from map tasks in the shufﬂe

data to be periodically transferred from map to reduce tasks

phase, HTTP requests are issued by a reduce task to ﬁve (this

without waiting an entire intermediate data set to be ready.

default value can be conﬁgured) number of TaskTrackers. The

Second, we must design a grouping policy that arranges map

locations of these TaskTrackers are managed by the JobTracker

and reduce tasks within a node to shorten the shufﬂe time

located in the Master node of a Hadoop cluster. When a map or

period by overlapping the computations of the map and reduce

reduce TaskTracker ﬁnishes, the TaskTracker sends a heartbeat

tasks.

to the JobTracker in the master node, which assigns a new
task to the TaskTracker. The master node is in charge of

D. Organization

determining time when reduce tasks start running and data
The rest of the paper is organized as follows. Section III
describes the design of our preshufﬂing algorithm after presenting the system architecture. Section IV presents the implementation details of the preshufﬂing mechanism in the
Hadoop system. In Section V, we evaluate the performance
of our preshufﬂing scheme. Section VI reviews related work
and Section VII concludes the paper with future research
directions.

to be processed. Map task and reduce tasks are stored in two
different queues.
Reduce tasks pull intermediate data (i.e., (key, value) pairs)
from each TaskTracker that is storing the intermediate data.
In this design, application developers can simply implement
separate map tasks and reduce tasks without dealing with the
coordination between the map and reduce tasks. In the shufﬂe
phase the above pull model is not efﬁcient, because reduce

III. D ESIGN I SSUES
In this section, we ﬁrst present the design goals of our
preshufﬂing algorithm. Then, we describe how to incorporate
the preshufﬂing scheme into the Hadoop system. We also show
a way of reducing the shufﬂing times of a Hadoop application
by overlapping map and reduce operations inside a node.

tasks are unable to start their execution until the intermediate
data are retrieved. To improve the performance of the shufﬂe
phase, we change the pull model into a push model. In the push
model, map tasks automatically push intermediate data in the
shufﬂe phase to reduce tasks. Map tasks start pushing (key,
value) pairs to reduce tasks as soon as the pairs are produced.
We refer to the above new push model in the shufﬂe phase

A. Push Model of the Shufﬂe Phase
A typical reduce task consists of three phases, namely, the
shufﬂe phase, the sort phase, and the reduce phase. After map

as the preshufﬂing technique. In what follows, we describe the
design issues of our preshufﬂing scheme that applies the push
model in the shufﬂe phase.

tasks generate intermediate (key, value) pairs, reduce tasks
fetch in the shufﬂe phase the (key, value) pairs. In the shufﬂe

B. A Pipeline in Preshufﬂing

phase, each reduce task handles a portion of the key range

When a new job submitted to a Hadoop cluster, the Job-

divided among all the reduce tasks. In the sort phase, records

Tracker assigns map and reduce tasks to available TaskTrack-

sharing the same key are groups together; in the reduce phase,

ers in the cluster. Unlike the pulling model, the pushing model

Jiong Xie et al. / Procedia Computer Science 18 (2013) 2458 – 2467

of preshufﬂing push intermediate data produced by map tasks

2461

computations of map tasks.

to reduce tasks. The preshufﬂing scheme allows the map tasks

We design a mechanism to create two separate threads in

to determine a partition records to be transferred a reduce

a map task. The ﬁrst thread processes input data, generates

task. Upon the arrival of the partition records, the reduce task

intermediate records, and completes the sort phase. The sec-

sorts and stores these records into the node hosting the reduce

ond thread manages the aforementioned pipeline that sends

task. Once the reduce task is informed that all the map tasks

intermediate data from map tasks to reduce tasks immediately

have been completed, the reduce task performs a user-deﬁned

when the intermediate outputs are produced. The two threads

function to process each assigned key and its list of values.

can work in parallel in a pipelining manner. In other words,

The map tasks continue generating intermediate records to be

the ﬁrst thread implements the ﬁrst stage of the pipeline; the

delivered the reduce tasks.

second thread performs the second stage of the pipeline. In this

Let us consider a simple case where a cluster has enough

pipeline, the ﬁrst stage is focusing on producing intermediate

free slots allowing all the tasks of a job to run after the

results to be stored in the memory buffers, whereas the second

job is submitted to the cluster. In this case, we establish

stage periodically retrieves the intermediate results from the

communication channels between a reduce task and all the

buffers and transfers the results to the connected reduce tasks.

map tasks pushing intermediate data to the reduce task. Since
each map task decides reduce tasks to which the intermediate

C. In-memory Buffer

data should be pushed, the map task transfers the intermediate

The push model does not require reduce tasks to wait a

data to the corresponding reduce tasks immediately after the

long time period before map tasks complete the entire map

data are produced by the map task.

phase. Nevertheless, pushing intermediate data from map to

In some cases, there might not be enough free slots available

reduce tasks in the preshufﬂing phase is still a time-consuming

to schedule every task in a new Hadoop job. If a reduce task

process. The combiner process in a map task is an aggregate

can not be executed due to limited number of free slots, map

function (a reduce-like function) that groups multiple distinct

tasks can store intermediate results in memory buffers or local

values together as input to form a single value. If we plan

disks. After a free slot is assigned to the reduce task, the

to implement the preshufﬂing mechanism to directly send

intermediate results buffered in the map tasks can be sent to

intermediate outputs from map to reduce tasks, we will have

the reduce task.

to ignore the combiner process in map tasks. In the native

Shufﬂe phase time in many cases is much longer than map

Hadoop system, the combiner can help map tasks to illuminate

phase time (i.e., tasks’ computation time); this problem is

relevant data, thereby reducing data transfer costs. Sending all

more pronounced true when network interconnects are scarce

the data generated from map tasks to reduce tasks increases

resource in a Hadoop cluster. To improve the performance

response time and downgrades the performance of Hadoop

of the preshufﬂing scheme, we build a pipeline in the shufﬂe

applications. Without the pre-sorting and ﬁltering process in

phase to proactively transfer intermediate data from map tasks

the combiner stage, reduce tasks should spend much time in

to reduce tasks. The pipeline aims at increasing the throughput

sorting for merging values.

of preshufﬂing by overlapping data communications with the

Instead of sending an entire buffered content to reduce tasks

2462

Jiong Xie et al. / Procedia Computer Science 18 (2013) 2458 – 2467

directly, we design a buffer mechanism to temporary collect

neck.

intermediate data. The buffer mechanism immediately sends a
IV. IMPLEMENTATION

small portion of the intermediate data to reduce tasks as soon
as the portion is produced. A conﬁgurable threshold is used to

In Hadoop, reduce tasks will not start their executions until

control the size of the portion. Thus, once the size of buffered

entire intermediate output of all map tasks have been produced,

intermediate results reaches the threshold, the map task sorts

although some map tasks may generate some intermediate

the intermediate data based on reduce keys. Next, the map

results earlier than the other map tasks. In our preshufﬂing

task writes the buffer to its local disk. Then, the second stage

scheme, map tasks do not need to be synchronized in the way

of the pipeline is invoked to check whether reduce tasks have

to produce a group of intermediate data to be sent to reduce

enough free slots. If nodes hosting reduce tasks are ready, a

tasks at the same time. Thus, a reduce task can immediately

communication channel between the map and reduce tasks are

receive corresponding intermediate data generated by map

established. The combined data produced in the ﬁrst stage of

tasks. However, the reduce task is unable to apply the reduce

the pipeline can be passed to reduce tasks in the second stage

function on the intermediate data until all the date produced by

of the pipeline.

every map task become available. Like reduce tasks, a Hadoop

In cases where nodes hosting reduce tasks are not ready, the
second stage of the pipeline will have to wait until the reduce

job must wait for all map tasks to ﬁnish before producing a
ﬁnal result.

tasks are available to receive the pushed data. This pipeline

As described in Section III, a map task consists of two

mechanism aims to improve the throughput of the shufﬂing

phases: map and map-transfer. The map phase processes an

stage, because the pipeline makes it possible for map tasks

entire input ﬁle, sorts intermediate results, and then sends

to send intermediate data as soon as a portion of the data is

them to an output buffer. The sort phase in the map task

produced by map functions.

groups records sharing the same key together; this group

In the design of our preshufﬂing scheme, it is ﬂexible to

procedure otherwise should be performed in the reduce phase.

dynamically control the amount of data pushed from map

In the map-transfer phase, intermediate data is transferred from

to reduce tasks by adjusting the buffer’s threshold. A high

buffer in map tasks to reduce tasks.

threshold value means that each portion to be pushed from

A reduce task consists two main phases - shufﬂe and reduce.

map tasks in the second stage of the pipeline is large; a

In the shufﬂe phase, the reduce task not only receives its

small threshold value indicates that each portion shipped to

portion of intermediate output from each map task, but also

reduce tasks is large. If network interconnects are not overly

performs a merge sort on the intermediate output from map

loaded, map tasks may become a performance bottleneck. This

tasks. In reduce tasks, the shufﬂe phase time accounts for a

bottleneck problem can be addressed by increasing the buffer’s

majority of the total reduce tasks’ execution time. For example,

threshold so that each data portion pushed to reduce tasks is

70% of a reduce task’s time is spent in the shufﬂe phase. The

large. A large threshold is recommended for Hadoop clusters

shufﬂe phase is time consuming, because a large amount of

with fast network interconnects; a small threshold is practical

intermediate output from map tasks must be merged and sorted

for Hadoop clusters where networks are a performance bottle-

in this phase. To improve the performance of the shufﬂe phase,

Jiong Xie et al. / Procedia Computer Science 18 (2013) 2458 – 2467

2463

we implement a preshufﬂing scheme where intermediate data

transfer times among the map and reduce tasks. The preshuf-

are immediately merged and sorted when the data are produced

ﬂing scheme, when used in combination with the push model,

by map tasks. After receiving required intermediate data

can boost the performance of Hadoop clusters. The perfor-

from all map tasks, the reduce task performs a ﬁnal merge

mance improvement offered by preshufﬂing and the push

sort function based on intermediate output produced by the

model becomes more pronounced when network interconnec-

preshufﬂing scheme. When the reduce task completes its ﬁnal

tion is a performance bottleneck of the clusters.

merge sort, the task reaches the reduce phase.
In a Hadoop cluster, a master node monitors the progress of
each task’s execution. When a map task starts its execution,

V. EVALUATION PERFORMANCE
A. Experimental Environment

the master node assigns a progress score anywhere in the
To evaluate the performance of the proposed preshufﬂing
range between 0 and 1. The value of a progress score is
scheme incorporated in the push model with a pipelining
assigned based on how much of the input data the map task
technique, we run Hadoop benchmarks on a 10-node cluster.
has processed [3]. Similarly, we introduce a progress score,
Table I summarizes the conﬁguration of the cluster used as a
allowing the preshufﬂing scheme to monitor the progress of
testbed for the performance evaluation. Each computing node
reduce tasks. Progress scores of reduce tasks are assigned
in the cluster is equipped with two dual-core 2.4 GHz Intel
based on how much intermediate data of each portion has
processors, 2GB main memory, 146 SATA hard disk, and a
been consumed by the reduce tasks. The progress score is
Gigabit Ethernet network interface card.
incorporated with the data structure of intermediate data. Thus,
when a partition of intermediate ﬁle is transferred to a reduce
task, the progress score of this partition is also received by
the reduce task. The average progress score of all relevant
partitions in each intermediate data ﬁle can be considered as

TABLE I: Test Bed
CPU
Memory
Disk
Operation System
Hadoop version

Intel Xeon 2.4GHz
2GB Memory
SEGATE 146GB
Ubuntu 10.4
0.20.2

the progress of a reduce task.
Each node hosting reduce tasks individually runs the tasks.

In our experiments, we vary the block size in HDFS to

In heterogeneous Hadoop clusters, nodes may run tasks at dif-

evaluate the impacts of block size system performance. In this

ferent speed. Once a reduce task has made sufﬁcient progress,

study, we focus on impact of preshufﬂing and the push model

the task reports its progress score written to a temporary ﬁle

on Hadoop and; therefore, we disable the data replica feature

on HDFS. For example, we can set several granularity; the

of HDFS. Nevertheless, using the preshufﬂing mechanism in

user can set the default value as 20%, 40%, 60%, 80%, and

combination with the data replica mechanism can signiﬁcantly

100%. When reduce progress reaches this value, the progress

improve performance of Hadoop clusters.

score will be automatically written down to HDFS.

We test the following two Hadoop benchmarks running on

By aggressively pushing data from map tasks to reduce

the cluster, in which the preshufﬂing scheme is integrated with

tasks, the push model can increase the throughput of the

the push model to improve the performance of the shufﬂe

Hadoop system by partially overlapping communication and

phase in Hadoop applications.

2464

Jiong Xie et al. / Procedia Computer Science 18 (2013) 2458 – 2467

1) WordCount (WC): This Hadoop application counts the

Fig. 2: Impact of block size on the preshufﬂing-enabled
cluster running WordCount.

frequency of occurrence for each word in a text ﬁle. Map
16

tasks process different sections of input ﬁles and return

14

frequency. Then, reduce tasks add up the values for each
identity word. The Word-Count is a memory-intensive
application.
2) Sort: This Hadoop application puts elements of a list

Improvement(100%)

intermediate data that consists of several pairs word and
12
10
8
6
4

in a certain order. The most-used orders are numerical

2

order and lexicographical order. The output list of this

0

16MB

32MB

64MB

128MB

256MB

Block Size

application is in a non-decreasing order.
B. In Cluster
We compare the overall performance between the native

Fig. 3: Impact of block size on the preshufﬂing-enabled
Hadoop cluster running Sort.
16

cluster. We measure the execution times of the two tested

14

Hadoop benchmarks running on the Hadoop cluster, where

12

the default block size is 64 MB.
Figure ?? illustrates the progress trend of WordCount processing 1GB data on the native Hadoop. The progress trend
shown in Figure ?? indicates how the map and reduce tasks are
coordinating. For example, Figure 1(a) shows that in the native
Hadoop system, the reduce task does not start its execution

Improvement(100%)

Hadoop and the preshufﬂing-enabled Hadoop on a 10-node

10
8
6
4
2
0

16MB

32MB

64MB

128MB

256MB

Block Size

until the all the map tasks complete their executions at time
50. Figure 1(b) proves that in the preshufﬂing-enabled Hadoop,
our push model makes it possible for the reduce task in

60 seconds to complete the map in the preshufﬂing-enabled

WordCount to begins its execution almost immediately after

Hadoop. The preshufﬂing-enabled Hadoop system has a longer

the map task gets started.

map task than the native Hadoop, because in our push model

Our solution shortens the execution time of WordCount by

part of the shufﬂe phase is handled by the map task rather

approximately 15.6%, because the reduce task under the push

than the reduce task in the native Hadoop. Forcing the map

model receives intermediate output produced by the map tasks

task to process the preshufﬂing phase is an efﬁcient way of

as soon as the output become available.

reducing heavy load imposed on reduce tasks. As a result,

Figures 1(a) and 1(b) show that it takes 50 seconds to

the preshufﬂing-enabled Hadoop cluster can complete the

ﬁnish the map task in the native Hadoop and its takes about

execution of WordCount faster than the native Hadoop cluster.

Jiong Xie et al. / Procedia Computer Science 18 (2013) 2458 – 2467

100

100
map
reduce

80

80

70

70

60
50
40

60
50
40

30

30

20

20

10

10
0

50

100
150
Time (seconds)

200

250

map
reduce

90

Progress (100%)

Progress (100%)

90

0

2465

0

0

50

100
150
Time (seconds)

200

250

(a) The execution time of WordCount processing 1GB data on the native (b) The execution time of WordCount processing 1GB on the preshufﬂingHadoop system is 450 seconds.
enabled Hadoop system is 380 seconds.

Fig. 1: The progress trend of WordCount processing 1GB data on the 10-node Hadoop cluster.

C. Large Blocks vs. Small Blocks

of percentage is saturated when the block size is larger than
128 MB.

Now we evaluate the impact of block size on the perfor-

Figure 3 shows the performance improvement of preshuf-

mance of preshufﬂing-enabled Hadoop clusters. The goal of

ﬂing on the 10-node cluster running the Sort application. The

this set of experiments is to quantify the sensitivity of our

results plotted in Figure 3 are consistent with those shown in

preshufﬂing scheme on the block size using the two Hadoop

Figure 2. For the two Hadoop benchmarks, the performance

benchmarks. We run the WordCount and Sort benchmarks on

improvement offered by preshufﬂing is sensitive to block size

both the native Hadoop and the preshufﬂing-enabled Hadoop

when the block size is smaller than 128 MB.

clusters when the block size is set to 16MB, 32MB, 64MB,
128MB, and 256MB, respectively.

VI. R ELATED WORK

Figures 2 and 3 shows the performance improvement of the

Implementations of MapReduce. MapReduce framework

preshufﬂing-enabled Hadoop cluster over the native Hadoop

is inspired by the map and reduce functions commonly used

cluster as a function of the block size. Figure 2 demonstrates

in functional programming [4]. MapReduce is useful in a

that the improvement offered by preshufﬂing in case the of

wide range of applications including: distributed grep, dis-

WordCount increases when the block size goes up from 16 MB

tributed sort, web link-graph reversal, term-vector per host,

to 128 MB. However, increasing the block size from 128 MB

web access log stats, inverted index construction, document

to 256 MB does not provide a higher improvement percentage.

clustering, machine learning [2], and statistical machine trans-

Rather, the improvement slightly drops from 12.5% to 12.2%

lation. Moreover, the MapReduce model has been adapted to

when the block size is changed from 128 MB to 256 MB.

several computing environments like multi-core and many-

The experimental results plotted in Figure 2 suggest that

core systems [1][11], desktop grids, volunteer computing

a large block size allows the preshufﬂing scheme to offer

environments [9], and dynamic cloud environments [10].

good performance improvement. The improvement in terms

Shufﬂing. Duxbury et al. built a theoretical model to ana-

2466

Jiong Xie et al. / Procedia Computer Science 18 (2013) 2458 – 2467

lyze the impacts of MapReduce on network interconnects [12].

performance optimization opportunities.

There are two new ﬁndings in their study. First, during
VII. CONCLUSION

the shufﬂe phase, each reduce task communicates with all
map tasks in a cluster to retrieve required intermediate data.

A Hadoop application’s execution time is greatly affected

Network load is increased during the shufﬂe phase due to

by the shufﬂing phase, where an amount of data is transferred

intermediate data transfers. Second, at the end reduce phase,

from map tasks to reduce tasks. Moreover, improving per-

ﬁnal results of the Hadoop job is written to HDFS. Their study

formance of the shufﬂing phase is very critical for shufﬂe-

shows evidence that the shufﬂe phase can cause high network

intensive applications, where a large amount of intermediate

loads. Our experimental results conﬁrm that 70% of a reduce

data is delivered in shufﬂe phases. Making a high-efﬁcient

task’s time is spent in the shufﬂe phase. In this paper, we

shufﬂing scheme is an important issue, because shufﬂe-

propose a preshufﬂing scheme combined with a push model

intensive applications impose heavy network and disk I/O

to release the network burden imposed by the shufﬂe phase.

loads during the shufﬂe phase. In this paper, we proposed a

Pipeline. Dryad [8] and DryadLINQ [13] offer a data-

new push model, a new preshufﬂing module, and a pipelining

parallel computing framework that is more general than

mechanism to efﬁciently boost the performance of Hadoop

MapReduce. This new framework enables efﬁcient database

clusters running shufﬂe-intensive applications.

joins and automatic optimizations within and across MapRe-

In the push model, map tasks automatically send intermedi-

ductions using techniques similar to query execution planning.

ate data in the shufﬂe phase to reduce tasks. Unlike map tasks

In the Dryad-based MapReduce implementation, outputs pro-

in the traditional pull model, map tasks in the push model

duced by multiple map tasks are combined at the node level

proactively start sending intermediate data to reduce tasks as

to reduce the amount of data transferred during the shufﬂe

soon as the data are produced. The push model allows reduce

phase. Compared with this combining technique, partial hiding

tasks to start their executions earlier rather than waiting until

latencies of reduce tasks is more important and effective for

an entire intermediate data set becomes available. The push

shufﬂe-intensive applications. Such a latency-hiding technique

model improves the efﬁciency of the shufﬂe phase, because

may be extended to other MapReduce implementations.

reduce tasks do not need to be strictly synchronized with their

Recently, researchers extended the MapReduce program-

map tasks waiting for the entire intermediate data set.

ming model to support database management systems in order

Our preshufﬂing scheme aims to release the load of reduce

to process structured ﬁles [7]. For example, Olston et. al

tasks by moving the pre-sorting and ﬁltering process from

developed the Pig system [6], which is a high-level parallel

reduce tasks to map tasks. As a result, reduce tasks with the

data processing platform integrated with Hadoop. The Pig

support of the preshufﬂing scheme spend less time in sorting

infrastructure contains a compiler that produces sequences of

to merge values.

Hadoop programs. Pig Latin - a textual language - is the

In the light of the push model and the preshufﬂing scheme,

programming language used in Pig. The Pig Latin language

we built a 2-stage pipeline to efﬁciently move intermediate

not only makes it easy for programmers to implement em-

data from map tasks to reduce tasks. In stage one, local buffers

barrassingly parallel data analysis applications, but also offer

in a node hosting map tasks temporarily store combined inter-

Jiong Xie et al. / Procedia Computer Science 18 (2013) 2458 – 2467

2467

mediate data. In stage two, a small portion of the intermediate

[3] Tyson Condie, Neil Conway, Peter Alvaro, Joseph M. Hellerstein,

data stored in the buffers is sent to reduce tasks as soon as

Khaled Elmeleegy, and Russell Sears. Mapreduce online. In Proceedings
of the 7th USENIX conference on Networked systems design and

the portion is produced. In the second stage of the pipeline,
the availability of free slots in nodes hosting reduce tasks
are checked. If there are free slots, a communication channel

implementation, NSDI’10, pages 21–21, Berkeley, CA, USA, 2010.
USENIX Association.
[4] J. Dean and S. Ghemawat. Mapreduce: Simpliﬁed data processing on
large clusters. OSDI ’04, pages 137–150, 2008.

between the map and reduce tasks are established. In the 2-

[5] Adam Dou, Vana Kalogeraki, Dimitrios Gunopulos, Taneli Mielikainen,

stage pipeline, the combined data produced in the ﬁrst stage

and Ville H. Tuulos. Misco: a mapreduce framework for mobile systems.

of the pipeline can be passed to reduce tasks in the second

In Proceedings of the 3rd International Conference on PErvasive Technologies Related to Assistive Environments, PETRA ’10, pages 32:1–

stage of the pipeline.

32:8, 2010.

We implemented the push model along with the preshufﬂing
scheme in the Hadoop system, where the 2-stage pipeline

[6] Apache

Software

Foundation.

The

pig

project.

http://hadoop.apache.org/pig.
[7] Eric Friedman, Peter Pawlowski, and John Cieslewicz. Sql/mapreduce:

was incorporated with the preshufﬂing scheme. Our experi-

a practical approach to self-describing, polymorphic, and parallelizable

mental results based on two Hadoop benchmarks shows that

user-deﬁned functions. Proc. VLDB Endow., 2:1402–1413, August 2009.

preshufﬂing-enabled Hadoop clusters are signiﬁcantly faster

[8] Michael Isard, Mihai Budiu, Yuan Yu, Andrew Birrell, and Dennis
Fetterly.

than native Hadoop clusters with the same hardware conﬁg-

Dryad: distributed data-parallel programs from sequential

building blocks. SIGOPS Oper. Syst. Rev., 41:59–72, March 2007.

urations. For example, the push model and the preshufﬂing

[9] Heshan Lin, Xiaosong Ma, Jeremy Archuleta, Wu-chun Feng, Mark

scheme powered by the 2-stage pipeline can shorten the exe-

Gardner, and Zhe Zhang. Moon: Mapreduce on opportunistic environments. In Proceedings of the 19th ACM International Symposium on

cution times of the two Hadoop applications (i.e., WordCount
and Sort) by an average of 10% and 14%, respectively.

High Performance Distributed Computing, HPDC ’10, pages 95–106,
New York, NY, USA, 2010. ACM.
[10] Fabrizio Marozzo, Domenico Talia, and Paolo Trunﬁo. A peer-to-peer

ACKNOWLEDGMENTS
This research was supported by the U.S. National Sci-

framework for supporting mapreduce applications in dynamic cloud
environments. In Nick Antonopoulos and Lee Gillam, editors, Cloud
Computing, volume 0 of Computer Communications and Networks,

ence Foundation under Grants CCF-0845257 (CAREER),
CNS-0917137 (CSR), CNS-0757778 (CSR), CCF-0742187
(CPA), CNS-0831502 (CyberTrust), CNS-0855251 (CRI),

pages 113–125. Springer London, 2010.
[11] C. Ranger, R. Raghuraman, A. Penmetsa, G. Bradski, and C. Kozyrakis.
Evaluating mapreduce for multi-core and multiprocessor systems. HighPerformance Computer Architecture, International Symposium on, 0:13–

OCI-0753305 (CI-TEAM), DUE-0837341 (CCLI), and DUE0830831 (SFS), as well as Auburn University under a startup
grant, and a gift (No. 2005-04-070) from the Intel Corporation.

24, 2007.
[12] Raplesf.

Analyzing

network

load

in

map/reduce.

http://blog.rapleaf.com/dev/2010/08/24/analyzing-network-lo
´
[13] Yuan Yu, Michael Isard, Dennis Fetterly, Mihai Budiu, Ulfar
Erlingsson,
Pradeep Kumar Gunda, and Jon Currey. Dryadlinq: a system for general-

R EFERENCES
[1] B.He, W.Fang, Q.Luo, N.Govindaraju, and T.Wang. Mars: a MapReduce
framework on graphics processors. ACM, 2008.
[2] Cheng-Tao Chu, Sang Kyun Kim, Yi-An Lin, YuanYuan Yu, Gary R.
Bradski, Andrew Y. Ng, and Kunle Olukotun. Map-reduce for machine
learning on multicore. In NIPS, pages 281–288, 2006.

purpose distributed data-parallel computing using a high-level language.
In Proceedings of the 8th USENIX conference on Operating systems
design and implementation, OSDI’08, pages 1–14, Berkeley, CA, USA,
2008. USENIX Association.

