Available online at www.sciencedirect.com

Procedia Computer Science 18 (2013) 1475 ‚Äì 1484

International Conference on Computational Science, ICCS 2013

Markov Chain Analysis of Agent-based Evolutionary Computing
in Dynamic Optimization
Aleksander Byrskia , Robert Schaefera
a AGH

University of Science and Technology, Al. Mickiewicza 30, 30-059 Krakow, Poland

Abstract
In this paper a Markov model for Evolutionary Multi-Agent System is recalled. The model allows to study dynamic features of
the computation and increases understanding the considered classes of systems by e.g., proving the ergodicity of the Markov
chain modelling EMAS. This feature may be considered as a reason to use such complex techniques, as following the Michael
Vose‚Äôs approach, similar feature is proven for EMAS, showing that this system is able to reach any possible state of the system
space (including of course optima sought). The main contribution of the paper is showing possibilities of applying the already
proposed model to dynamic optimization problems. The impact of these enhancements on the ergodicity feature is also discussed.

¬© 2013 The Authors. Published by Elsevier B.V. Open access under CC BY-NC-ND license.
c 2012 The
Published
Elsevier B.V.of the organizers of the 2013 International Conference on Computational
Selection
andAuthors.
peer review
underby
responsibility
Selection and/or peer-review under responsibility of the 2013 International Conference on Computational Science.
Science
Keywords: multi-agent systems; Markov chain modelling; ergodicity; dynamic optimization

1. Introduction
When looking for an optimal heuristics to solve a certain problem, one must recall the famous ‚Äúno free lunch‚Äù
theorem [1], stating, that there are no algorithm that will be equally good for all possible problems. On the
other hand, it would be good to be sure, if a certain algorithm is at least capable of solving the considered task,
especially, when constructing complex metaheuristics. This is important, because complex search methods may
aÔ¨Äect the ability to Ô¨Ånd all possible answers to the given problem, therefore, formal proving of certain features of
the computation becomes an important argument in the discussion of applicability of certain search methods.
Handling dynamic optimization problems, where the Ô¨Åtness value changes over the time, requires special
approaches to constantly maintain or even refresh the diversity of the population. The methods known as hypermutation (increasing mutation rate after the change of the Ô¨Åtness function) or random immigrants (introducing
randomly generated individuals into the population) were developed [2]. An interesting approach solution of dynamic optimization problems may also be performed by employing diÔ¨Äerential evolution [3]. Other techniques,
as employing dominance and diploidy were also considered [4]. Some other approaches try to predict the changes
in the optima position [5].
‚àó Corresponding author. Tel.: +48-12-328-33-30 ; fax: +48-12-617-51-72 .
E-mail address: olekb@agh.edu.pl.

1877-0509 ¬© 2013 The Authors. Published by Elsevier B.V. Open access under CC BY-NC-ND license.
Selection and peer review under responsibility of the organizers of the 2013 International Conference on Computational Science
doi:10.1016/j.procs.2013.05.315

Aleksander Byrski and Robert Schaefer / Procedia Computer Science 18 (2013) 1475 ‚Äì 1484

	






	








	










			


	


	

	










	







	


	







	
	


	
	







	


	

	
	







	
	





	
	





1476





	
	




 
	







(a) Evolutionary multi-agent system (EMAS)



(b) Scheme of the synchronisation mechanism

Fig. 1: EMAS structure and behaviour and the synchronisation mechanism
There are few formal models prepared for analysing of stochastic features of population-based heuristics, and
virtually none covering dynamic optimization. Several of the most known ones are cited in this paragraph. The
model presented by Vose [6] proves in the most simple, yet eÔ¨Äective way, asymptotic guarantee of success, i.e.
‚Äúability to Ô¨Ånd all local maximizers (minimizers) with probability 1 after inÔ¨Ånite number of epochs‚Äù [7, 8, 9]
in the analysis of the Simple Genetic Algorithm (SGA) behaviour, formally conÔ¨Årming the possibility of using
SGA for global optimisation. Formal models for genetic algorithms have also been proposed by other researchers,
providing a deeper insight into the long term, steady state behaviour of large population EAs [10, 11, 12] or
modelling speciÔ¨Åc features of EAs such as selection, genetic drift, niching etc. [13, 14, 15].
In this paper, basic features of the stochastic Markov models already introduced in the works of Byrski,
Schaefer et al. (e.g. [16, 17, 18, 19, 20]) are recalled. Formulas giving e.g., description of system state space
and the dynamics of the model are given, however it is carried out only in order to build a background for the
considerations regarding the extending of EMAS. These works were devoted to modelling EMAS (Evolutionary
Multi-Agent Systems) introduced by Cetnarowicz [21], being a general optimisation system leveraging paradigms
of evolutionary computation and agency (that can be counted to computational intelligence solutions [22, 23]).
The main contribution of the paper is an analysis of possibility of proving asymptotic features (e.g. asymptotic
guarantee of success) of EMAS applied to dynamic optimization problem. The full formal proof of ergodicity has
been carried out in the work of Byrski, Schaefer, Smo≈Çka and Cotta [24].
The paper starts with an overview of EMAS then the Markov chain model of EMAS is presented (including
deÔ¨Ånition of the system state and transition functions). Then, the adaptation of the presented model for dynamic
optimization problem and a discussion of the impact of this adaptation on the possibility of proving the ergodicity
are shown.
2. Evolutionary Multi-Agent System in dynamic optimization
EMAS is a general-purpose optimisation system leveraging paradigms of evolutionary computation and agency,
(following work of Cetnarowicz [21]) that has already proven its eÔ¨Éciency for certain class of problems (see e.g.,
[25, 26, 27, 28, 29]).
Fig. 1a shows the simplest possible model of an evolutionary multi-agent system, with one type of agents and
one resource (called energy) deÔ¨Åned. This energy becomes a base for distributed selection mechanism‚Äîthe more
energy agent possesses, the more likely it may reproduce, on the other hand, when its energy falls below certain
level, the agent is removed from the system.
The agent possesses genotype that represents feasible solutions to the problem. The energy is transferred
between agents in the process of evaluation. When the agent Ô¨Ånds out that one of its neighbours (e.g. randomly
chosen), has lower Ô¨Åtness, it takes a part of its neighbour‚Äôs energy, otherwise it passes part of its own energy

Aleksander Byrski and Robert Schaefer / Procedia Computer Science 18 (2013) 1475 ‚Äì 1484

1477

to the evaluated neighbour. The level of life energy triggers the actions of Reproduction (performed when the
agent‚Äôs energy raises above a certain level, followed by production of a new individual in cooperation with one
of its neighbours, with genotype based on parents‚Äô genotypes (crossed over and mutated) and part of energy also
taken from its parents), Death (the agent is removed from the system when its energy falls below a certain level,
the remaining energy is distributed among its neighbours), Migration (the agent may migrate when its energy
rises above a certain level, then it is removed from one evolutionary island and moved to another according to
predeÔ¨Åned topology).
Each action is attempted randomly with a certain probability, and it is performed only when their basic preconditions are met (e.g., an agent may attempt to perform the action of reproduction, but it will reproduce only if
its energy rises above certain level and it meets an appropriate neighbour).
To derive the deÔ¨Ånition of dynamic optimization, let us start from the deÔ¨Ånition of stationary global optimization. This problem is deÔ¨Åned by the closed search space U and a quality function (Ô¨Åtness) F : U ‚Üí R. The task
is to determine the set of extrema X ‚äÜ U, deÔ¨Åned as:
X = {gen ‚àà U; ‚àÄgen ‚ààU F(gen) ‚â• F(gen )}

(1)

In the discussed case, U is a Ô¨Ånite genetic universum #U = r < +‚àû [8]. Of course the minimization problem may
be easily changed into maximization by negating the Ô¨Åtness value.
A dynamic optimization problem is deÔ¨Åned by the search space U, a set of quality functions F (t) : U ‚Üí R, t ‚àà
N0 . The goal of this task is to determine the set of all extrema X(t) ‚äÇ U(t ‚àà N0 ), deÔ¨Åned as:
X(t) = {gen ‚àà U; ‚àÄgen ‚ààU F (t) (gen) ‚â• F (t) (gen )}

(2)

Again in this case U is a Ô¨Ånite genetic universum #U = r < +‚àû [30].
Handling such dynamic optimization problems, requires special approaches to constantly maintain or even
refresh the diversity of the population. The approaches known as hypermutation (increasing mutation rate after
the change of the Ô¨Åtness function) or random immigrants (introducing randomly generated individuals into the
population) were developed [2]. The allopatric niching technique‚Äîisland model of evolution‚Äîis also considered
as a valuable approach to this problem.
Covering such problem in the considered Markov chain EMAS model consists mainly in appropriately proposing the Ô¨Åtness function and changing only one important element of the model, that utilizes the Ô¨Åtness function:
the function that compares two agents [17] in order to transfer the energy from the worse one to the better one.
Following this modiÔ¨Åcation, popular mechanism supporting dynamic optimization may be easily implemented,
i.e.: allopatric speciation has been covered in EMAS right from the start of the research on this heuristics, the parameters of the variation operators (e.g., mutation) may be also changed, in order to perform hypermutation,
randomly generated agents may be introduced into the system using the cloning action coupled with hypermutation. The impact of these changes on the Markovian model of EMAS introduced by Byrski et al. [16, 17, 19] will
be discussed later in this paper, along with possibilities of proving the feature of ergodicity in this case.
3. EMAS state
In this section the description of the EMAS state given in [19, 31] will be recalled. The full EMAS state
description is given in [24]
Computational EMAS agents belong to the predeÔ¨Åned Ô¨Ånite set Ag one-to-one mapped on set U √ó P, where
P = {1, . . . , p} and p is assumed to be the maximum number of agents contain the same genotype, so each agent
aggen,n ‚àà Ag is uniquely represented by its signature (gen, n) ‚àà U √ó P.
Agents are assigned to locations Loc = {1, . . . , s}. The locations are linked by channels along which agents
may migrate from one location to another. The topology of channels is determined by the symmetric relation
T op ‚äÇ Loc2 . We assume that the connection graph Loc, T op is coherent and does not change during the
system evolution. Each agent possesses a variable parameter called energy, its value is quantized and belongs to
{0, Œîe, 2 ¬∑ Œîe, 3 ¬∑ Œîe, . . . , m ¬∑ Œîe}. The current value of the energy exhibits the maturity of agent in solving the
optimization problem, aÔ¨Äecting its abilities (reproduction, cloning, migration) (see [32]).

1478

Aleksander Byrski and Robert Schaefer / Procedia Computer Science 18 (2013) 1475 ‚Äì 1484

Let us consider the set of three-dimensional, incidence and energy matrices x ‚àà X with s layers (corresponding
to all locations) x(i) = {x(i, gen, n), gen ‚àà U, n ‚àà P}, i ‚àà Loc. The layer x(i) will contain energies of agents in
i-th location. In other words, x(i, gen, k) > 0 means that the k-th clone of the agent containing the gene gen ‚àà U is
active, its energy equals x(i, gen, k) and it is located in i-th location.
The following coherency conditions are assumed:
‚Ä¢ each layer x(i) contains at most qi values greater than zero, which denotes the maximum capacity of the i-th
location, moreover, the quantum of energy Œîe is lower or equal than total energy divided by the maximal
number of individuals that may be present in the system Œîe ‚â§ s1 qi what allows to achieve maximal
i=1
population of agents in the system,
s
qi . We assume that p =
‚Ä¢ reasonable values of p should be greater or equal to 1 and less or equal to i=1
s
q
which
assures
that
each
conÔ¨Åguration
of
agents
in
locations
is
available,
respecting
the constrained
i=1 i
s
qi . Increasing p over this value does not enhance the descriptive power
total number of active agents i=1
of the presented model,
‚Ä¢ (¬∑, j, k)-th column contains at most one value greater than zero, which expresses that the agent with k-th
copy of j-th genotype may be present in only one location at a time, whereas other agents containing copies
of j-th genotype may be present in other locations,
‚Ä¢ entries in the incidence and energy matrices are non-negative x(i, j, k) ‚â• 0, ‚àÄ i = 1, . . . , s, j = 1, . . . , r, k =
p
s
r
1, . . . , p and i=1
j=1 k=1 x(i, j, k) = 1, which means that the total energy contained in the whole system
is constant, equal to 1.
Gathering all these conditions, the set of three-dimensional incidence and energy matrices was described in
the following way:
X = x ‚àà {0, Œîe, 2 ¬∑ Œîe, 3 ¬∑ Œîe, . . . , m ¬∑ Œîe} s¬∑r¬∑p , Œîe ¬∑ m = 1,
s

r

p

r

p

x(i, j, k) = 1 and ‚àÄ i = 1, . . . , s
i=1 j=1 k=1

[x(i, j, k) > 0] ‚â§ qi
j=1 k=1

s

and ‚àÄ j = 1, . . . , r, k = 1, . . . , p

[x(i, j, k) > 0] ‚â§ 1

(3)

i=1

where [¬∑] denotes the value of the logical expression contained in the parentheses.
Note that the formula (3) implies that there must exist at least one agent in the system i.e. at least one location
is non-empty at a time.
EMAS may be modeled as the following tuple:
< U, Loc, T op, Ag, {agseli }i‚ààLoc , locsel, {LAi }i‚ààLoc , MA, œâ, Act >

(4)

where:
MA (master agent) is used to synchronize the work of the locations; it allows to perform actions in particular
locations. This agent is also used to introduce necessary synchronization into the system.
locsel : X ‚Üí M(Loc) is the function used by MA to determine which location should be allowed to perform the
next action.
LAi (local agent) is assigned to each location; it is used to synchronize the work of computational agents present
in its location, LAi chooses the computational agent and lets it evaluate a decision and perform the action,
at the same time asking MA whether this action may be performed.
agseli : X ‚Üí M(U √ó P) is a family of functions used by local agents to select the agent that may perform the
action, so every location i ‚àà Loc has its own function agseli . The probability agseli (x)(gen, n) vanishes
when the agent aggen,n is inactive in the state x ‚àà X or it is present in other than i-th location,

Aleksander Byrski and Robert Schaefer / Procedia Computer Science 18 (2013) 1475 ‚Äì 1484

1479

œâ : X √ó U ‚Üí M(Act) is the function used by agents for selecting actions from the set Act; both these symbols
will be described later.
Act is a predeÔ¨Åned, Ô¨Ånite set of actions.
Here and later M(¬∑) stands for the space of probabilistic measures.
Generally speaking, in order to appropriately model a system using a Markov chain, a synchronization mechanism must be developed, to control the changes of the state by the system actions. The agent-based synchronization mechanism taking into account parallel executions of some actions is explained in details e.g., in [16, 17].
Shortly speaking, the agents situated in diÔ¨Äerent levels of hierarchy (computational agents and local agents) ask
their predecessor for the permission to perform certain task (see Fig. 1b) and conduct their work only in the case,
when they receive a permission. In the considered case of dynamic optimization, it is assumed that all actions
performed in the system are considered global. Though, the presented synchronization mechanism is similar to
the one presented in e.g., [16], assuming that the probability of performing global action is equal to 1.
The population of agents is initialized by using introductory sampling. It may be explained as a one-time
sampling from X according to the predeÔ¨Åned probability distribution (possibly uniform) from M(X). Every agent
starts its work in EMAS immediately after being activated. At every observable moment a certain agent on each
location gains the possibility of changing the state of the system by executing its action.
The function agseli is used by the Local Agent LAi to determine which agent present on i-th location will be
the next one to interact with the system. After being chosen, the agent aggen,n chooses one of the possible actions
according to the probability distribution œâ(x, gen). Notice the relationship of this probability distribution with the
concept of Ô¨Åne-grain schedulers introduced in the syntactic model for memetic algorithms in [33].
Next, the agent applies to LAi for the permission to perform this action. When the permission is granted, aggen,n
checks whether the associated condition is true, and if so, the agent performs the action. The agent suspends its
work in the system after performing the action which brings its energy to zero.
Master agent MA manages the activities of LAi allowing them to grant permissions for their agents (thus
relating to coarse-grain schedulers in [33]). Each action Œ± ‚àà Act is the pair of families of random functions
}gen‚ààU,n‚ààP and {œëgen,n
}gen‚ààU,n‚ààP where
{Œ¥gen,n
Œ±
Œ±
: X ‚Üí M({0, 1})
Œ¥gen,n
Œ±

(5)

(1) by agent aggen,n in state x ‚àà X i.e.
will denote the decision. The action Œ± is performed with probability Œ¥gen,n
Œ±
when the decision is undertaken. Moreover
: X ‚Üí M(X)
œëgen,n
Œ±

(6)

deÔ¨Ånes the non-deterministic state transition caused by the execution of action Œ± by agent aggen,n . The trivial state
transition
œënull : X ‚Üí M(X)
(7)
such that for all x ‚àà X
œënull (x)(x ) =

1
0

if x = x
otherwise

(8)

(x)(0), i.e. when decision Œ¥Œ± is not undertaken (Œ¥gen,n
(x) is evaluated as zero).
is performed with probability Œ¥gen,n
Œ±
Œ±
The value of the probability transition function for action Œ± for the agent containing the n-th copy of genotype
gen being in the location l
gen,n
: X ‚Üí M(X)
(9)
Œ±
for the arbitrary current state x ‚àà X and the next one x ‚àà X is given by:
gen,n
(x)(x
Œ±

) = Œ¥gen,n
(x)(0) ¬∑ œënull (x)(x ) + Œ¥gen,n
(x)(1) ¬∑ œëgen,n
(x)(x )
Œ±
Œ±
Œ±

(10)

Notice Ô¨Ånally that it is formally possible to consider a very large (yet Ô¨Ånite) set Act, comprising all actions
up to a certain description length (using a G¬®odel numbering or any appropriate encoding). This implies that this
set may be implicitly deÔ¨Åned by such an encoding, allowing much Ô¨Çexibility in the set of actions available (a
connection can be drawn with multimeme algorithms [34]).

1480

Aleksander Byrski and Robert Schaefer / Procedia Computer Science 18 (2013) 1475 ‚Äì 1484

4. EMAS dynamics
In this section, the description of dynamics of Markov chain EMAS model will be recalled after [19]. The full
description of the dynamics will is given in [24]. As it was stated before, it is assumed, that all actions identiÔ¨Åed
in the system applied to dynamic optimization problem are considered global.
At the observable moment at which EMAS takes state x ‚àà X all agents in all locations notify their local
agents their intent to perform an action, all local agents choose an agent using the distribution given by the
agseli (x), i ‚àà Loc function and then notify the master agent of their intent to let perform an action by one of their
agents. The master agent chooses the location using the random function locsel(x).
The transition function for the whole system looks as follows:
‚éõ
‚éû‚éû
‚éõ
p
‚éú‚éú‚éú
‚éü‚éü‚éü‚éü‚éü‚éü
‚éú‚éú‚éú
gen,n
‚éú
‚éú
locsel(x)(i) ‚éú‚éú‚éù
agseli (x)(gen, n)¬∑ ‚éú‚éú‚éú‚éù
œâ(x, gen)(Œ±) ¬∑ Œ± (x)(x )‚éü‚éü‚éü‚éü‚é†‚éü‚éü‚éü‚éü‚é†
(11)
œÑ(x)(x ) =
i‚ààLoc

Œ±‚ààActgl

gen‚ààU n=1

It is easy to see that
Observation 1. The stochastic state transition of EMAS given by formula (11) satisÔ¨Åes the Markov condition.
Moreover, the Markov chain deÔ¨Åned by these functions is stationary.
5. Adaptation of the model to dynamic optimization case
Adaptation of the model to cover dynamic optimization problem requires introducing appropriate deÔ¨Ånition
of Ô¨Åtness function. In this section, certain type of Ô¨Åtness dynamism will be presented, that has enough descriptive
power to support dynamic optimization problems. Generally speaking, the modiÔ¨Åcation of the Ô¨Åtness function
consists in assuming, that the values returned by the original Ô¨Åtness may be modiÔ¨Åed (perturbed) by the random
variable of a given distribution. Let {[lgen , hgen ]}gen‚ààU be the assumed ranges of Ô¨Åtness perturbations. Let us denote
by
IPF = { f : U ‚Üí R; f (gen) ‚àà [lgen , hgen ], ‚àÄgen ‚àà U}
(12)
The perturbation will be described as the following random function:
Per : X ‚Üí M(IPF).

(13)

Fitness will be now represented by the following random function:
Fitness : X ‚Üí M(F + IPF)

(14)

where ‚Äú+‚Äù stands for the set translation operator and F stands for Ô¨Åtness function in stationary case.
Let I ‚äÇ R be any measurable set. Now,
‚àÄ gen ‚àà U, x ‚àà X (Fitness(x)(I))(gen) = Per(x)((I ‚à© ([lgen , hgen ] + F(gen))) ‚àí F(gen)).

(15)

In particular case, when the perturbations are deÔ¨Åned by the family of pairwise independent functions, the following random function vector may be deÔ¨Åned:
pergen : X ‚Üí M([lgen , hgen ])

(16)

in such way, that for any measurable sets Agen ‚äÇ [lgen , hgen ], gen ‚àà U and the sets of functions: A = { f ‚àà
IPF; f (gen) ‚àà Agen }:
Per(x)(A) =
pergen (x)(Agen ).
(17)
gen‚ààU

Let us assume the sequel of EMAS states x , x , . . . , x(t‚àí1) , x(t) , . . . then F (t) may be deÔ¨Åned as the eÔ¨Äect of
sampling Fitness(x(t‚àí1) ) such that
(0)

(1)

F (t) (gen) = F(gen) + pergen (x(t‚àí1) ), ‚àÄgen ‚àà U

(18)

where the underlining denotes the eÔ¨Äect of sampling the underlined random variable or function.
Further implementation of selected techniques supporting dynamic optimization may require redeÔ¨Ånition of
MUT and CROS S functions, by simply making them dependent on the state of the system, therefore allowing to
change the parameters of these distributions based on, e.g., number of evolutionary generations passed etc.

Aleksander Byrski and Robert Schaefer / Procedia Computer Science 18 (2013) 1475 ‚Äì 1484

1481

6. EMAS actions
Let us consider a sample EMAS with the following set of actions: Act = {get, repr, clo, migr}
The Ô¨Åtness value and their changes do not aÔ¨Äect of all actions expecting get, so they are set exactly the same
as in [19]. The action get will be partially redeÔ¨Åned in order to cover dynamic optimization with the model.
Due to space limitations we describe the actions informally, underlining only the necessary conditions for the
subsequent analysis of the systems‚Äôs ergodicity in the dynamic optimization case. Complete formal descriptions
of these actions leading to the probability transition functions (5) and (6) may be found in [16] and will is also
given in [24].
In the following (gen, n) stands for the signature of a generic agent that attempts to execute the following
actions:
6.1. Energy transfer action ‚Äúget‚Äù
Decision Œ¥gen,n
get for energy transfer is positive when there is at least one agent more on the same location. Agent
chooses randomly one of its neighbours and during the meeting, the energy is exchanged between agents, what
may be considered somewhat as a tournament (see tournament selection [35]). The direction of the energy Ô¨Çow is
determined by a probability distribution CMP : X √ó U √ó U ‚Üí M({0, 1}) dependent on agents‚Äô Ô¨Åtnesses and the
current state of the system, that will be described in details later in this section. It is to note, that the comparing
function CMP returns 0 if the Ô¨Årst agent passed as a parameter is better in the means of Ô¨Åtness value from the
second one, and 1 otherwise. In the next state one of the agents receives a predeÔ¨Åned part of energy Œîe from its
s
neighbour, which is assumed to satisfy Œîe ‚â§ ( i=1
qi )‚àí1 .
Utilising the dynamic Ô¨Åtness function deÔ¨Ånition given by (18) the following implementation of CMP function
may be proposed. Let us introduce the auxiliary random function:
W(x, gen1 , gen2 ) =

1‚àí

if Fitness(x)(gen1 ) < Fitness(x)(gen2 )
otherwise

(19)

where ‚àà (0, 1) is the arbitrary small constant. We assume the probability of returning 0 by CMP(x, gen1 , gen2 )
is W(x, gen1 , gen2 ), and returning 1 by 1 ‚àí W(x, gen1 , gen2 ).
The probability of the event Fitness(x)(gen1 ) < Fitness(x)(gen2 ) might be computed in the following way.
Let us denote by Ii = [lgeni , hgeni ] + F(geni ), i = 1, 2 and by m = min{lgen1 , lgen2 }, m = max{hgen1 , hgen2 }. Then
Pr(Fitness(x)(gen1 ) < Fitness(x)(gen2 )) =

m

per1 (x)(m, Œ±) per2 (x)(Œ±, m)dŒ±.

(20)

m

It is clear, that the probability of returning zero or 1 is bounded from below by ŒπCMP = min{ , 1 ‚àí } for any
state x ‚àà X and any pair of genes gen1 , gen2 ‚àà U.
6.2. Reproduction action ‚Äúrepr‚Äù
Decision Œ¥gen,n
repr for reproduction is positive when the energy of the agent performing the action is greater than
a reproduction threshold erepr and there is at least one agent more in the same location satisfying the same energy
condition. We assume that erepr ‚â§ 2Œîe. These agents create an oÔ¨Äspring agent based on their solutions using a
predeÔ¨Åned mixing operator. Part of the parents‚Äô energy (e0 = n0 ¬∑ Œîe, n0 is even) is passed to the oÔ¨Äspring.
In order to introduce new individuals into the system, the action employs crossover and mutation probability
distributions, in order to localize new individuals in the system state space based on their parents. Assuming that
the distributions imposed by these two functions may change over time (being adapted in an arbitrary way), one
can follow a similar approach as in the case of get action, making them dependent on the current system state:
CROS S : X √ó U √ó U ‚Üí M(U), MUT : X √ó U √ó U ‚Üí M(U)
This delivers enough descriptive power to cover the possibilities of adapting the actual distributions imposed
by these two functions. Moreover, random immigrants may be introduced in the system (cf. Section 2), still
by modiÔ¨Åcation of the MUT probability distribution, that in certain cases may introduce completely random
individuals, instead of the ones based on parents genotypes.

1482

Aleksander Byrski and Robert Schaefer / Procedia Computer Science 18 (2013) 1475 ‚Äì 1484

6.3. Cloning action ‚Äúclo‚Äù
Decision Œ¥gen,n
for cloning is based on checking the amount of agent‚Äôs energy only. An agent with enough
clo
energy strictly greater than Œîe, creates an oÔ¨Äspring agent based on its solution (applying a predeÔ¨Åned mutation
operator MUT ). Also in this case, this parameter considers the current state of the system, that makes possible
adaptation of the probability distribution imposed: MUT : X √ó U ‚Üí M(U). Part of the parent‚Äôs energy Œîe is
passed to the oÔ¨Äspring.
In this case, similar to the deÔ¨Ånition of repr function, random immigrants may be introduced in the system
(cf. Section 2), still by modiÔ¨Åcation of the MUT probability distribution, that in certain cases may introduce
completely random individuals, instead of the ones based on parents genotypes.
6.4. Migration action ‚Äúmigr‚Äù
Decision Œ¥gen,n
migr is positive when an agent has enough energy greater than emigr and there exists a location that
is able to accept it (the number of agents there is lower than its capacity). When these conditions are met the agent
is moved from its location to another. We assume, that emigr < s‚àí1 .
Introducing of this action, along with supporting the allopatric speciation in EMAS model allows to support
dynamic optimization per se, by increasing the diversity of the population [2].
7. Ergodicity
Now we pass to study the ergodicity of the Markov chain imposed by the EMAS solving dynamic global
optimization problem deÔ¨Åned by the Ô¨Åtness introduced in Section 5. We will utilize the following theorem which
was formulated (see [19] and [17]) and proved (see [24]) for the more general case of EMAS that allow for parallel
execution of selected actions.
Theorem 1. (see Theorem 1 in [19]) Given the following assumptions:
1. The capacity of every location is greater than one, qi > 1, i = 1, . . . , s.
2. The graph of locations is connected.
3. Each active agent can be selected by its local agent with strictly positive probability, so
‚àÉ Œπagsel > 0; ‚àÄ i ‚àà Loc, ‚àÄ gen ‚àà U, ‚àÄ n ‚àà P, ‚àÄ x ‚àà {y ‚àà X; y(i, gen, n) > 0},
agseli (x)(gen, n) ‚â• Œπagsel .
4. The families of probability distributions being the parameters of EMAS have the uniform, strictly positive
lower bounds:
‚àÉ Œπœâ > 0; ‚àÄ x ‚àà X, gen ‚àà U, Œ± ‚àà Act, œâ(gen, x)(Œ±) ‚â• Œπœâ ,
‚àÉ ŒπCMP > 0; ‚àÄ gen, gen ‚àà U, ‚àÄx ‚àà X CMP(x, gen, gen ) ‚â• ŒπCMP ,
‚àÉ Œπmut > 0; ‚àÄgen, gen ‚àà U, ‚àÄx ‚àà X MUT (x, gen)(gen ) ‚â• Œπmut ,
‚àÉ 0 < Œπlocsel < 1; ‚àÄ x ‚àà X, ‚àÄ j ‚àà Loc, locsel(x)( j) ‚â• Œπlocsel .
We can construct a Ô¨Ånite sequence of transitions between two arbitrarily chosen system states which may be passed
with strictly positive probability. Moreover we can deliver the upper bound of the number of such transitions,
which can be eÔ¨Äectively computed based on the system‚Äôs parameters.
It is easy to observe, that all assumptions of this theorem hold for the EMAS case discussed in this paper.
First, the case of the Markov model describing execution of mutually excluded actions only can be obtained by
setting the probability of parallel actions processing Œ∂ loc ‚â° 0 in the formula deÔ¨Åning the probability transition
(see (19) in [19]). Moreover we have already proved, that the probability of agent comparing CMP is uniformly
bounded from below by the strictly positive constant (see Section 6.1), that satisÔ¨Åes the second inequality of the
assumption 4 of Theorem 1. Other assumptions are also satisÔ¨Åed, because the EMAS under consideration does
cover all remaining features of the one presented in [19].
Remark 1. The above Theorem 1 makes all states containing the extrema reachable in a Ô¨Ånite number of states,
thus EMAS satisÔ¨Åes an asymptotic guarantee of success [8], [9]. Moreover the Markov chain modelling EMAS
solving dynamic global optimization problem under consideration is ergodic.

Aleksander Byrski and Robert Schaefer / Procedia Computer Science 18 (2013) 1475 ‚Äì 1484

1483

The possible impact of the Markov chain ergodicity on the dynamic optimization problem is twofold. First,
it is easy to see, that independent on the Ô¨Åtness changes all the possible system states are reachable from every
arbitrarily chosen state. In other words, two arbitrarily chosen states may be transferred one to another in the Ô¨Ånite
number of steps. This brings the second possible impact on dynamic optimization. In the extended version of
the paper giving full formal proof for the stationary optimization case [24] upper bounds of the number of steps
needed to connect any two states are computed. Therefore, this number may be used as an estimate of adaptation
capabilities, and be used along with the prediction-based approaches [5] to analyse the possibilities and the time
of reaching the vicinity of the changing optimum.
8. Conclusions
In this paper, a formal model for EMAS presented in [16, 17, 19] has been recalled. The structure and behaviour of EMAS modelled by Markov chain were shown. The model was appropriately modiÔ¨Åed to cover the
dynamic optimization problem by changing some of its features (mutation, crossover and evaluation probability
distributions) to support selected ways of solving these problems: hypermutation, allopatric speciation and random immigrants. Actual experiments conducted in a system constructed according to the described model were
presented in [36]. One of the most important matters was the detailed deÔ¨Ånition (with an example) of the random
function CMP used to compare the agents in the course of action get.
One of the main implications of the analysis conducted here is the formulation and proof draft of Theorem
1 stating that the Markov chain based model of EMAS is stationary and ergodic. This will lead to an important
conclusion, namely that EMAS possesses the feature of asymptotic guarantee of success. The assumed way of
proving this theorem was also sketched out. The detailed description of the EMAS Markov model along with
complete proof of ergodicity have been already formulated in [24].
The possible impact of the Markov chain ergodicity on the dynamic optimization problem analysis is the
observation, that regardless the Ô¨Åtness changes all the possible system states are reachable from every arbitrarily
chosen state. The estimate of upper bound of the number of steps connecting two arbitrarily chosen states of the
system space in the full paper [24] may allow to further research the features of dynamic optimizing EMAS, e.g.,
help in implementing prediction based approaches.
The presented extension of the formal framework may be adapted to model other simulation or computing
problems. Selected possible examples are: evolutionary programming in multi-robot environment [37], hierarchic
genetic search [38] or co-evolutionary optimization [39].
Acknowledgements
The research presented here was partially supported by the grants ‚ÄúBiologically inspired mechanisms in planning and management of dynamic environments‚Äù No. N N516 500039, ‚ÄúParallel hierarchical adaptive algorithm
for solving challenging inverse problems‚Äù No. N N519 447739, and ‚ÄúMulti-model, multi-criterial and multiadaptive strategies for solving the inverse tasks‚Äù, No. DEC-2011/03/B/ST6/01393, all funded by the Polish National Science Centre.
References
[1] D. H. Wolpert, W. G. Macready, No free lunch theorems for optimization, IEEE Transactions on Evolutionary Computation 1 (1) (1997)
67‚Äì82.
[2] Y. Jin, J. Branke, Evolutionary optimization in uncertain environment‚Äîa survey, IEEE Transactions on Evolutionary Computation 9
(2005) 303‚Äì317.
[3] R. Storn, K. Price, DiÔ¨Äerential evolution‚Äîa simple and eÔ¨Écient heuristic for global optimization over continuous spaces, Journal of
Global Optimization 11.
[4] D. Goldberg, R. Smith, Nonstationary function optimization using genetic algorithms with dominance and diploidy, in: Proc. of the
Second International Conference on Genetic Algorithms, 1987, pp. 59‚Äì68.
[5] A. Simoes, E. Costa, Improving prediction in evolutionary algorithms for dynamic environments, in: Proc. of the 2009 Genetic and
Evolutionary Computation Conference, 2009, pp. 875‚Äì888.
[6] M. Vose, The Simple Genetic Algorithm: Foundations and Theory, MIT Press, Cambridge, MA, USA, 1998.
[7] R. Schaefer, Foundations of global genetic optimization, Springer Verlag, 2007.

1484

Aleksander Byrski and Robert Schaefer / Procedia Computer Science 18 (2013) 1475 ‚Äì 1484

[8] R. Horst, P. Pardalos, Handbook of Global Optimization, Kluwer, 1995.
[9] A. Rinnoy Kan, G. Timmer, Stochastic global optimization methods, Mathematical Programming 39 (1987) 27‚Äì56.
[10] T. E. Davis, J. C. Principe, A simulated annealing like convergence theory for the simple genetic algorithm, in: Proc. of the Fourth
International Conference on Genetic Algorithms, San Diego, CA, 1991, pp. 174‚Äì181.
[11] J. Suzuki, A Markov Chain Analysis on a Genetic Algorithm, in: S. Forrest (Ed.), Proc. of the 5th ICGA, Morgan Kaufmann, 1993, pp.
146‚Äì154.
[12] G. Rudolph, Massively parallel simulated annealing and its relation to evolutionary algorithms, Evolutionary Computation 1 (1994)
361‚Äì383.
[13] D. Goldberg, P. Segrest, Finite Markov chain analysis of genetic algorithms, in: Proceedings of the Second International Conference on
Genetic Algorithms on Genetic algorithms and their application, L. Erlbaum Associates Inc., Hillsdale, NJ, USA, 1987, pp. 1‚Äì8.
[14] S. Mahfoud, Finite Markov Chain Models of an Alternative Selection Strategy for the Genetic Algorithm, Complex Systems 7 (1991)
155‚Äì170.
[15] J. Horn, Finite Markov Chain Analysis of Genetic Algorithms with Niching, in: Proceedings of the Fifth International Conference on
Genetic Algorithms, Morgan Kaufmann, 1993, pp. 110‚Äì117.
[16] A. Byrski, R. Schaefer, Stochastic model of evolutionary and immunological multi-agent systems: Mutually exclusive actions, Fundamenta Informaticae 95 (2-3) (2009) 263‚Äì285.
[17] R. Schaefer, A. Byrski, M. Smo≈Çka, Stochastic model of evolutionary and immunological multi-agent systems: Parallel execution of
local actions, Fundamenta Informaticae 95 (2-3) (2009) 325‚Äì348.
[18] A. Byrski, R. Schaefer, M. Smo≈Çka, Asymptotic features of parallel agent-based immunological system, in: T. Burczy¬¥nski, J. Ko≈Çodziej,
A. Byrski, M. Carvalho (Eds.), Proc. of 25th European Conference on Modelling and Simulation, 2011.
[19] A. Byrski, R. Schaefer, M. Smo≈Çka, C. Cotta, Asymptotic analysis of computational multi-agent systems, in: R. S. et al. (Ed.), PPSN
2010 Proceedings, Vol. 3256 of LNCS, Springer-Verlag, 2010.
[20] A. Byrski, R. Schaefer, Formal model for agent-based asynchronous evolutionary computation, in: IEEE Congress of Evolutionary
Computation, Trondheim, Norway (submitted to), IEEE, 2009.
[21] K. Cetnarowicz, M. Kisiel-Dorohinicki, E. Nawarecki, The application of evolution process in multi-agent world (MAW) to the prediction system, in: M. Tokoro (Ed.), Proc. of the 2nd Int. Conf. on Multi-Agent Systems (ICMAS‚Äô96), AAAI Press, 1996.
[22] M. Kisiel-Dorohinicki, G. Dobrowolski, E. Nawarecki, Agent populations as computational intelligence, in: L. Rutkowski, J. Kacprzyk
(Eds.), Neural Networks and Soft Computing: Proc. of 6th International Conference on Neural Networks and Soft Computing Location:
ZAKOPANE, POLAND Date: JUN 11-15, 2002, Advances in Soft Computing, Springer-Verlag, 2003, pp. 608‚Äì613.
[23] A. Byrski, M. Kisiel-Dorohinicki, Agent-based model and computing environment facilitating the development of distributed computational intelligence systems, in: Proc. of ICCS 2009, Vol. 5545 of Computational Science, Springer, 2009, pp. 865‚Äì874.
[24] A. Byrski, R. Schaefer, M. Smo≈Çka, C. Cotta, Asymptotic guarantee of success for multi-agent memetic systems, Bulletin of the Polish
Academy of Sciences: Technical Sciences, accepted for printing.
[25] A. Byrski, M. Kisiel-Dorohinicki, E. Nawarecki, Agent-Based Evolution of Neural Network Architecture, in: M. Hamza (Ed.), Proc. of
the IASTED Int. Symp.: Applied Informatics, IASTED/ACTA Press, 2002.
[26] A. Byrski, M. Kisiel-Dorohinicki, Immunological selection mechanism in agent-based evolutionary computation, in: Proc. of IIS:
IIPWM ‚Äô05 conference : Gdansk, Poland, Advances in Soft Computing, Springer, 2005.
[27] A. Byrski, M. Kisiel-Dorohinicki, Agent-based evolutionary and immunological optimization, in: Computational Science - ICCS 2007,
7th International Conference, Beijing, China, May 27 - 30, 2007, Proceedings, Springer, 2007.
[28] A. Byrski, M. Kisiel-Dorohinicki, Immune-based optimization of predicting neural networks, in: V. Sunderam, G. Van Albada, P. Sloot
(Eds.), Proc. of 5th International Conference on Computational Science (ICCS 2005) Location: Atlanta, GA Date: MAY 22-25, 2005,
Vol. 3516 of Lecture Notes in Computer Science, Springer-Verlag, 2002, pp. 703‚Äì710.
[29] A. Byrski, M. Kisiel-Dorohinicki, M. Carvalho, A crisis management approach to mission survivability in computational multi-agent
systems, Computer Science 11 99‚Äì113.
[30] D. Goldberg, Genetic Algorithms in Search, Optimization, and Machine Learning, Massachusetts: Addison-Wesley, 1989.
[31] A. Byrski, R. Schaefer, Formal model for agent-based asynchronous evolutionary computation, in: Proc. of IEEE Congress on Evolutionary Computation Location: Trondheim, NORWAY Date: MAY 18-21, 2009, 2009.
[32] M. Kisiel-Dorohinicki, Agent-Oriented Model of Simulated Evolution, in: W. I. Grosky, F. Plasil (Eds.), SofSem 2002: Theory and
Practice of Informatics, Vol. 2540 of LNCS, Springer-Verlag, 2002.
[33] N. Krasnogor, J. Smith, A tutorial for competent memetic algorithms: Model, taxonomy, and design issues, IEEE Transactions on
Evolutionary Computation 9 (5) (2005) 474‚Äì488. doi:10.1109/TEVC.2005.850260.
[34] N. Krasnogor, S. Gustafson, A study on the use of ‚Äúself-generation‚Äù in memetic algorithms, Natural Computing 3 (2004) 53‚Äì76.
[35] Z. Michalewicz, Genetic Algorithms Plus Data Structures Equals Evolution Programs, Springer-Verlag New York, Inc., Secaucus, NJ,
USA, 1994.
[36] M. Kisiel-Dorohinicki, Agent-based evolutionary computing in dynamic optimization problems, Computing and Informatics‚Äî
Submitted for review X (X).
[37] A. Kub¬¥ƒ±k, Distributed genetic algorithm: a case-study of evolution by direct exchange of chromosomes, Computing and Informatics
23 (6) (2004) 575‚Äì596.
[38] P. Jojczyk, R. Schaefer, Global impact balancing in the hierarchic genetic search, Computing and Informatics 28 (2).
[39] R. DreÀôzewski, Co-evolutionary multi-agent system with speciation and resource sharing mechanisms, Computing and Informatics 25 (4).

