Available online at www.sciencedirect.com

Procedia Computer Science 18 (2013) 1684 – 1689

International Conference on Computational Science, ICCS 2013

Cost-Sensitive Support Vector Machine for Semi-Supervised
Learning
Zhiquan Qia , Yingjie Tiana,∗, Yong Shia,∗, Xiaodan Yub
a Research
b College

Center on Fictitious Economy & Data Science, Chinese Academy of Sciences, Beijing 100190, China
of Information Science & Technology, University of Nebraska at Omaha, Omaha, NE 68182, USA.

Abstract
Cost-sensitive learning has been a hot research topic in machine learning. Many cost-sensitive methods have been successfully
applied in many real-world applications such as disease diagnosis, fraud detection and business decision making. In this paper,
we proposed a new Cost-Sensitive Laplacian Support Vector Machine(called Cos-LapSVM) , which can deal with the costsensitive problem in Semi-Supervised Learning. The eﬀectiveness of the proposed method is demonstrated via experiments on
UCI datasets.
Keywords: SVM, Cost-Sensitive, Semi-Supervised Learning

1. Introduction
Semi-Supervised Learning (SSL) [1, 2, 3] has attracted an increasing amount of interests in machine learning
[4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15]. One main reason is that the labeled examples are always rare but there are
large amount of unlabeled examples available in many practical problems. Several novel approaches for making
use of the unlabeled data to improve the performance of classiﬁers have been proposed. Graph based methods
are very important branch, where nodes in the graph are the labeled and unlabeled points, and weighted edges
reﬂect the similarities of nodes. The initially assumption of these methods is that all points are located in a low
dimensional manifold, and the graph is used to approximate the underlying manifold. Neighboring point pairs
connected by large weight edges tend to have the same labels and vice versa. By the means, the labels associated
with data can be propagated throughout the graph. By using the graph Laplacian, [16] proposed a novel Laplacian
Support Vector Machine (LapSVM). Unlike other methods based on graph [17, 18, 19], LapSVM is a natural
out-of-sample extension, which can classify data that becomes available after the training process, without having
to retrain the classiﬁer or resort to various heuristics [16]. A lot of experiments show that LapSVM achieves state
of the art performance in semi-supervised classiﬁcation [20].
In many real-world applications, diﬀerent misclassiﬁcations often have diﬀerent costs, such as disease diagnosis, fraud detection, business decision making [21, 22] and object recognition [23, 24] and so on. These
classiﬁcation problem is usually called cost-sensitive learning problem [25, 26, 27, 28, 29, 30], which aimed to
minimize the total misclassiﬁcation costs.
∗ Corresponding

author
Email addresses: qizhiquan@ucas.ac.cn (Zhiquan Qi), tyj@ucas.ac.cn (Yingjie Tian)

1877-0509 © 2013 The Authors. Published by Elsevier B.V. Open access under CC BY-NC-ND license.
Selection and peer review under responsibility of the organizers of the 2013 International Conference on Computational Science
doi:10.1016/j.procs.2013.05.336

1685

Zhiquan Qi et al. / Procedia Computer Science 18 (2013) 1684 – 1689

In this paper, we consider the problem of how to extend LapSVM algorithms to solve the cost-sensitive learning problem and proposed a new algorithm: Cos-LapSVM.
The remaining parts of the paper are organized as follows. Section 2 describes the detail of Cos-LapSVM;
All public datasets experiment results on real data sets are shown in the section 3; The last section gives the
conclusions.
2. Cost-Sensitive Laplacian Support Vector Machine(Cos-LapSVM)
2.1. Laplacian Semi-supervised Learning Framework
Regularization [31] is a key technology for obtaining smooth decision functions and thus avoiding over-ﬁtting
to the training data, which is widely used in machine learning [32, 16]. Recently, the regularization framework
has been recently extended in SSL ﬁeld by [16] as follows.
Given a set of labeled data
(1)
T = {(x1 , y1 ), · · · , (xl , yl )} ∈ ( n × Y)l ,
where xi ∈

n

, yi ∈ Y = {1, −1}, i = 1, · · · , l, and a set of unlabeled data
(xl+1 , · · · , xl+u ),

(2)

where xi ∈ n . For a kernel function K(·, ·) , which associated with a reproducing kernel Hilbert space Hk , the
decision function can be obtained by minimizing
f ∗ = arg min
f ∈Hk

l

V(xi , yi , f ) + γH f

2
H

+ γM f

2
M,

(3)

i=1

where f is a unknown decision function, V represents some loss function on the labeled data, γH is the weight
of f 2H and controls the complexity of f in the reproducing kernel Hilbert space. γM is the weight of f 2M and
controls the complexity of the function in the intrinsic geometry of marginal distribution, f 2M is able to penalize
f along the Riemann manifold M.
2.2. Cos-LapSVM
Similar to the LapSVM, we use the decision function
f (x) = (w · Φ(x)) + b,

(4)

where Φ is a nonlinear mapping from a low dimensional space to a higher dimensional Hilbert space H. According
to Hilbert space theory([33]), w can be expressed as w = l+u
i=1 αi Φ(xi ). So, the decision function is written as
l+u

αi K(xi , x) + b,

f (x) =

(5)

i=1

where K is an chosen kernel function: K(xi · x j ) = (Φ(xi ) · Φ(x j )). Also, the regularization term f
expressed as
f

2
H

= w

2

= (Φα) (Φα) = α Kα.

2
H

can be
(6)

For the manifold regularization f+ 2M , a data adjacency graph W(l+u)×(l+u) is deﬁned by nodes Wi, j , which represents the similarity of every pair of input samples. The weight matrix W may be deﬁned by k nearest neighbor or
graph kernels as follows([16]):
Wi j =

exp(− xi − x j 22 /2σ2 ), if xi , x j are neighbor;
0,
Otherwise,

(7)

1686

Zhiquan Qi et al. / Procedia Computer Science 18 (2013) 1684 – 1689

where xi − x j

2
2

denotes the Euclidean norm in

n

1
=
(l + u)2

l+u

f

2
M

. So the manifold regularization is deﬁned by
Wi, j ( f (xi ) − f (x j ))2 = f L f,

(8)

i, j=1

where L = D − W is the graph Laplacian, D is a diagonal matrix with its i-th diagonal Dii = l+u
j=1 Wi j , and
f = [ f (x1 ), · · · , f (xl+u )] = Kα, here we drop the bias term b. According to the cost-sensitive learning method of
[30], the primal problem of Cos-LapSVM can be written as
ξi + λ2

min [λ1

ξ∈Rl ,α∈Rn

{i|yi =1}

ξi ] + γH α Kα
{i|yi =−1}

+ γM α KLKα,
l+u

αi K(xi , x j ) + b ≥ 1 − ξi , yi = 1,

s.t.
j=1

(9)

l+u

αi K(xi , x j ) + b ≤ −λ3 + ξi , yi = −1,
j=1

ξi ≥ 0, i = 1, · · · , l,
where
λ1 = C1 ,

λ2 = 2C−1 − 1,

1
,
2C−1 − 1

λ3 =

(10)

C1 and C−1 are the penalty factors for the positive and negative class.
By introducing its Lagrange function
ξi + λ2

L(ξ, α, b, β, ρ, η) =[λ1

ξi ] + γH α Kα

{i|yi =1}

{i|yi =−1}
l+u

+ γM α KLKα −

βi (
{i|yi =1}

αi K(xi , x j )
j=1

(11)

l+u

+ b − 1 + ξi ) +

ρi (
{i|yi =−1}

αi K(xi , x j )+
j=1

l

b + λ 3 − ξi ) −

ηi ξi ,
i=1

where βi , ρi , ηi ∈ R are the Lagrange multipliers, therefore the dual problem of (11) can be formulated as
max L(ξ, α, b, β, ρ, η),

ξ,α,b,β,ρ,η

s.t. ∇ξ,α,b L(ξ, α, b, β, ρ, η) = 0,
βi , ρi , ηi ≥ 0.

(12)
(13)
(14)

From equation (13) we get
∇b L =

βi −
{i|yi =1}

ρi = 0,

(15)

{i|yi =−1}

∇ξi L = λ1 − βi − ηi = 0, yi = 1,
∇ξi L = λ2 − ρi − ηi = 0, yi = −1,

(16)
(17)

∇α L = (2γH K + 2γM KLK)α − K J1 β + K J2 ρ = 0,

(18)

1687

Zhiquan Qi et al. / Procedia Computer Science 18 (2013) 1684 – 1689

where J1 = [E1 0] is a l × (l + u) matrix with E1 as l × l diagonal matrix ((E1 )ii = 1, if yi = 1, otherwise, (E1 )ii = 0);
similarly, J2 = [E2 0] is a l × (l + u) matrix with E2 as l × l diagonal matrix ((E2 )ii = −1, if yi = −1, otherwise,
(E1 )ii = 0); β ∈ Rl , if yi = −1, βi = 0; η ∈ Rl , if yi = 1, ηi = 0.
Substituting the above equations into problem (13), the dual problem can be expressed as
1
max − (β J1 − ρ J2 )K(2γH + 2γM KL)−1 (J1 β − J2 ρ )
2
β ,ρ
l

(βi + λ3 ρi )

+
i=1

βi −

s.t.
{i|yi =1}

(19)

ρi = 0,
{i|yi =−1}

0 ≤ βi ≤ λ1 , yi = 1,
0 ≤ ρi ≤ λ2 , yi = −1.
It is not diﬃcult to ﬁnd that (19) will degenerate to the standard LapSVM when C1 = C−1 = 1.
3. Experiments

Table 1. Parameters and Samples on UCI datasets

Datasets

Dimensions

Number of
samples

cost(P, N)

cost(N, P)

Hepatitis
Australian
BUPA liver
CMC
Credit
Diabetis
Flare-Solar

19
14
6
9
19
8
9

155
690
345
844
690
768
1066

0.67
0.75
0.8
0.83
0.85
0.87
0.88

0.33
0.25
0.2
0.17
0.15
0.13
0.12

Table 2. Results on UCI datasets

Datasets

Cos-LapSVM

LapSVM

TS VM

Hepatitis
Australian
BUPA liver
CMC
Credit
Diabetis
Flare-Solar

0.208
0.424
0.389
0.485
0.343
0.457
0.477

0.223
0.476
0.412
0.521
0.432
0.460
0.501

0.254
0.462
0.433
0.553
0.411
0.471
0.535

The performance of the Cos-LapSVM was evaluated in UCI datasets (see Table 1). We compare the CosLapSVM against LapSVM and TSVM[34]. LapSVM and TSVM are cost-blind. We use sampling method [35]
to make these two method solve the cost-sensitive problem. Each data set is split into two equal halves, one for
training and the other for testing. Each training set contains 20% labeled examples. The RBF kernel is always
used. The testing accuracies of all experiments are computed using standard 10-fold cross validation. γH , γM and
RBF kernel parameter σ are all selected from the set {2i |i = −7, · · · , 7} by 10-fold cross validation on the tuning
set comprising of random 10% of the training data. Once the parameters are selected, the tuning set was returned

1688

Zhiquan Qi et al. / Procedia Computer Science 18 (2013) 1684 – 1689

to the training set to learn the ﬁnal decision function. We use the Average Cost(AC) to evaluate these algorithms’
performance. The AC can be expressed as
AC =

FP × cost(N, P) + FN × cost(P, N)
,
T P + T N + FP + FN

(20)

where TP is true positive, TN is true negative, FP is false positive and FN is false negative; cost(N, P) is the cost
of which negative data is classiﬁed into positive one and cost(P, N) is the cost of which positive data is classiﬁed
into negative one. All algorithms are implemented by using MATLAB 2010. The experiment environment: Intel
Core i7-2600 CPU, 4 GB memory.
From Table 2, we ﬁnd that Cos-LapSVM signiﬁcantly outperforms the cost-sensitive extensions of LapSVM
and TSVM. LapSVM has a better performance than TSVM in most cases. The main reason is that our costsensitive method roots in the method of [30], which is derived as the minimizer of the associated risk and can
avoid the shortcomings of previous approaches to cost-sensitive SVM design.
4. Conclusion
In this paper, we proposed a new Cost-Sensitive Laplacian Support Vector Machine(called Cos-LapSVM). All
experiments in datasets show that the performance of the Cos-LapSVM is better than that of the LapSVM and the
TSVM. However, since Cos-LapSVM needs to computing an extra inverse matrix before solving the quadratic
programming, so in the future work, we will try to solve the Cos-LapSVM in its primal formulation.
5. Acknowledgment
This work has been partially supported by grants from National Natural Science Foundation of China( NO.70921061,
NO.11271361, No. 61202226), the CAS/SAFEA International Partnership Program for Creative Research Teams,
Major International( Ragional) Joint Research Project(NO.71110107026), the President Fund of GUCAS.
References
[1] M. Seeger, Learning with labeled and unlabeled data, Tech. rep. (2001).
[2] O. Chapelle, B. Sch¨olkopf, A. Zien (Eds.), Semi-Supervised Learning (Adaptive Computation and Machine Learning), The MIT Press,
2006.
[3] X. Zhu, Semi-supervised learning literature survey (2006).
[4] Y. Tian, Y. Shi, X. Liu, Recent advances on support vector machines research, Technological and Economic Development of Economy
18(1) (2012) 5–33.
[5] Z. Qi, Y. Tian, S. Yong, Twin support vector machine with universum data, Neural Networks 36C (2012) 112–119. doi:doi:
10.1016/j.neunet.2012.09.004.
[6] Z. Qi, Y. Tian, S. Yong, Robust twin support vector machine for pattern classiﬁcation, Pattern Recognition 46(1) (2013) 305–316.
doi:doi:10.1016/j.patcog.2012.06.019.
[7] Z. Qi, Y. Tian, Y. Shi, Structural twin support vector machine for classiﬁcation, Knowledge-Based Systems.
[8] C. Zhang, Y. Shao, Y. Tan, N. Deng, Mixed-norm linear support vector machine, Neural Comput & Applicdoi:10.1007/s00521-0121166-0.
[9] N. C.Zhang, Y.Tian, The new interpretation of support vector machines on statistical learning theory, Science China Mathematics 53(1)
(2010) 151–164.
[10] Y. Shao, C. Zhang, X. Wang, N. Deng, Improvements on twin support vector machines, Neural Networks, IEEE Transactions on 22 (6)
(2011) 962–968.
[11] Y. Li, Y. Shao, N. Deng, Improved prediction of palmitoylation sites using pwms and svm, Protein and Peptide Letters 18 (2) (2011)
186.
[12] Z. Qi, Y. Tian, S. Yong, Regularized multiple criteria linear programming via linear programming, in: Procedia Computer Science,
Vol. 9, 2012, pp. 1234–1239.
[13] Z. Qi, Y. Tian, S. Yong, Regular multiple criteria linear programming for semi-supervised classiﬁcation, in: ICDM(OEDM), 2012.
[14] Z. Qi, Y. Tian, S. Yong, Multi-instance classiﬁcation based on regularized multiple criteria linear programming, Neural Computing &
Applicationsdoi:10.1007/s00521-012-1008-0.
[15] Z. Qi, S. Yong, Structural regular multiple criteria linear programming for classiﬁcation problem, International Journal of Computers,
Communications & Control 7(4) (2012) 732–742. doi:doi: http://dx.doi.org/10.1016/ j.neunet. 2012.07.011.
[16] M. Belkin, P. Niyogi, V. Sindhwani, Manifold Regularization: A Geometric Framework for Learning from Labeled and Unlabeled
Examples, Journal of Machine Learning Research 7 (2006) 2399–2434.

Zhiquan Qi et al. / Procedia Computer Science 18 (2013) 1684 – 1689

1689

[17] T. Joachims, Transductive learning via spectral graph partitioning, in: In ICML, 2003, pp. 290–297.
[18] M. Belkin, P. Niyogi, Using Manifold Structure for Partially Labelled Classiﬁcation, in: NIPS, 2002, pp. 953–960.
[19] X. Zhu, Z. Ghahramani, J. Laﬀerty, Semi-supervised learning using gaussian ﬁelds and harmonic functions, in: IN ICML, 2003, pp.
912–919.
[20] Z. Qi, Y. Tian, S. Yong, Laplacian twin support vector machine for semi-supervised classiﬁcation, Neural Networks 35 (2012) 46–53.
doi:doi: http://dx.doi.org/10.1016/ j.neunet. 2012.07.011.
[21] X. Yu, D. Khazanchi, An exploratory study of the impact of it capabilities adaptation on shared mental models similarity, in: MWAIS
2011, 2011.
[22] X. Yu, D. Owens, D. Khazanchi, Building socioemotional environments in metaverses for virtual teams in healthcare: A conceptual
exploration, in: Health Information Science, Springer, 2012, pp. 4–12.
[23] Z. Qi, Y. Xu, L. Wang, Y. Song, Online multiple instance boosting for object detection, Neurocomputing 74 (10) (2011) 1769–1775.
doi:10.1016/j.neucom.2011.02.011.
URL http://dx.doi.org/10.1016/j.neucom.2011.02.011
[24] Z. Qi, Y. Tian, S. Yong, Eﬃcient railway tracks detection and turnouts recognition method using hog features, Neural Computing &
Applicationsdoi:10.1007/s00521-012-0846-0.
[25] M. Kubat, S. Matwin, Addressing the curse of imbalanced training sets: One-sided selection, in: In Proceedings of the Fourteenth
International Conference on Machine Learning, Morgan Kaufmann, 1997, pp. 179–186.
[26] S. ichi Amari, S. Wu, Improving support vector machine classiﬁers by modifying kernel functions, NEURAL NETWORKS 12 (1999)
783–789.
[27] N. V. Chawla, K. W. Bowyer, L. O. Hall, W. P. Kegelmeyer, Smote: Synthetic minority over-sampling technique., J. Artif. Intell. Res.
(JAIR) 16 (2002) 321–357.
[28] G. Wu, E. Y. Chang, KBA: Kernel Boundary Alignment Considering Imbalanced Data Distribution, IEEE Transactions on Knowledge
and Data Engineering 17 (6) (2005) 786–795. doi:10.1109/TKDE.2005.95.
[29] H. Masnadi-Shirazi, N. Vasconcelos, Cost-Sensitive Boosting, IEEE Transactions on Pattern Analysis and Machine Intelligence 33 (2)
(2011) 294–309.
[30] H. Masnadi-Shirazi, N. Vasconcelos, Risk minimization, probability elicitation, and cost-sensitive SVMs, in: J. F¨urnkranz, T. Joachims
(Eds.), Proceedings of the 27th International Conference on Machine Learning (ICML-10), Omnipress, Omnipress, Haifa, Israel, 2010,
p. 759–766.
URL http://www.icml2010.org/papers/376.pdf
[31] A. Tikhonov, Regularization of incorrectly posed problems, Sov. Math. Dokl. 4 (1963) 1624–1627.
[32] T. Evgeniou, M. Pontil, T. Poggio, Regularization Networks and Support Vector Machines, Advances in Computational Mathematics
13 (1) (2000) 1–50.
[33] B. Sch¨olkopf, A. J. Smola, Learning with kernels: support vector machines, regularization, optimization, and beyond, Adaptive computation and machine learning, MIT Press, 2002.
[34] V. N. Vapnik, The Nature of Statistical Learning Theory, Springer New York, 1996.
[35] C. Elkan, The foundations of cost-sensitive learning, in: In Proceedings of the Seventeenth International Joint Conference on Artiﬁcial
Intelligence, 2001, pp. 973–978.

