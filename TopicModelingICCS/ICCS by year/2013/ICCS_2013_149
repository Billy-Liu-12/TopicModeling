Available online at www.sciencedirect.com

Procedia Computer Science 18 (2013) 2428 – 2431

International Conference on Computational Science, ICCS 2013

Eﬃcient synchronization for stencil computations using dynamic
task graphs
Zubair Wadood Bhattia,∗, Roel Wuytsb,c,a , Pascal Costanzab,d , Davy Preuveneersa ,
Yolande Berbersa
a iMinds-DistriNet,

KU Leuven, Celestijnenlaan 200A, B-3001 Leuven, Belgium
ExaScience Lab, Kapeldreef 75, B-3001 Leuven, Belgium
c imec, Kapeldreef 75, B-3001 Leuven, Belgium
d Intel, Belgium, Veldkant 31, 2550 Kontich, Belgium

b Intel

Abstract
Executing stencil computations constitutes a signiﬁcant portion of execution time for many numerical simulations running
on high performance computing systems. Most parallel implementations of these stencil operations suﬀer from a substantial
synchronization overhead. Furthermore, with the rapidly increasing number of cores these synchronization costs keep rising.
This paper presents a novel approach for reducing the synchronization overhead of stencil computations by leveraging dynamic
task graphs to avoid global barriers and minimizing spin-waiting, and exploiting basic properties of stencil operations to
optimize the execution and memory management. Our experiments show a reduction in synchronization overhead by at least
a factor four when compared to state-of-the-art stencil compilers like Pochoir and Patus.
Keywords: Scientiﬁc computing, parallel programming, model of computation, stencil computations, synchronization;

1. Introduction and problem statement
Because of their relevance in high-performance computing, there is an ongoing interest in optimizing stencil
computations, in particular for current multicore hardware. At the forefront of these eﬀort are stencil compilers
such as Pochoir[1] and Patus[2] that take a stencil description (written in a domain speciﬁc language) and generate
optimized multithreaded code. While these state-of-the-art stencil compilers and auto-tuning frameworks are very
eﬀective in increasing the arithmetic intensity, our results show that their parallel versions require quite a lot
of synchronization. With the number of hardware cores continuously increasing the cost of synchronization is
becoming higher and higher. Therefore it has become evermore important to minimize the synchronization cost.
Figure 1 shows an often-used but naive parallel implementation of a 2D ﬁve-point stencil that uses parallel f or.
The top loop cannot be parallelized using parallel f or due to the dependencies of successive iterations, whereas,
the iterations of the second loop (line 2) are independent of each other. Therefore, it is possible to safely parallelize
them, as shown in ﬁgure 1. However, this causes over-synchronization by placing an all-to-all synchronization
barrier in each iteration of the top loop.
∗ Corresponding

author. E-mail address: zubairwadood.bhatti@cs.kuleuven.be

1877-0509 © 2013 The Authors. Published by Elsevier B.V. Open access under CC BY-NC-ND license.
Selection and peer review under responsibility of the organizers of the 2013 International Conference on Computational Science
doi:10.1016/j.procs.2013.05.416

Zubair Wadood Bhatti et al. / Procedia Computer Science 18 (2013) 2428 – 2431

y=1

y=2

ymax

2429

Time step = t - 1

Barrier synchronization

for (t=begin to t=end) {
parallel_for(y=y_min to y=y_max) {
for(x=x_min to x=x_max) {
grid[t+1,y,x] =
K*(grid[t,y,x-1] + grid[t,y,x+1]
+ grid[t,y-1,x] + grid[t,y+1,x]
- 4*grid[t,y,x]) + grid[t,y,x];
}
} //Barrier synchronization
}

y=1

y=2

ymax

Time step = t

Barrier synchronization

y=1

y=2

ymax

Time step = t + 1

Barrier synchronization

Fig. 1. A navely parallelized 2D ﬁve-point stencil for simulation of heat dissipation using the Jacobi method.

This paper proposes a novel approach that minimizes synchronization overhead for stencil computations to
result in faster execution. We model stencil computations as dynamic task graphs and execute them with a workstealing scheduler. By construction our approach avoids global barriers (synchronization is only point-to-point)
and because tasks are only created when they are ready to be executed there is minimal spin-waiting. Furthermore
we exploit basic properties of stencil computations for memory management and optimization of the execution of
dynamic task graphs.
We have implemented our approach on top of the latest version of (Threading Building Blocks) TBB library,
with its support for Flow Graphs. The TBB ﬂow graph library has low-level infrastructure that we leverage to
express and execute our dynamic task graph stencil computations. Our results show that we reduce synchronization by at least a factor of four when compared to state-of-the art stencil compilers Pochoir[1] and Patus[2]. It is
important to note that our technique is complementary to the other optimizations (e.g. cache blocking) used in
stencil compilers, that could therefore beneﬁt from our approach.
The rest of the paper is structured as follows. Section 2 explains how we express stencil operations as a dynamic task graph and execute them very eﬃciently. Our experimental results are shown in section 3 and compared
with the state-of-the art. Section 4 presents our conclusions.
2. Stencil computations as dynamic task graphs
A task graph is a directed acyclic graph where the nodes represent computational tasks and the edges represent
dependencies between them. These dependencies are either data-ﬂow dependencies or simply precedence constraints. Contrary to a simple spawn-sync model of computation where only parent-child dependencies may be
expressed, task graphs allow arbitrary dependencies between any pair of tasks. Tasks become ready for execution
when messages arrive at all their incoming edges. After ﬁnishing execution the task sends messages on all its
outgoing edges. All tasks can be executed in parallel with each other as long as their precedence constraints are
fulﬁlled and suﬃcient resources are available to execute them.
The execution schematics of a dynamic task graph (DTG) [3] are similar to a conventional task graph; however,
it allows creation and deletion of tasks and edges on the ﬂy. Dynamic task graphs cannot be scheduled statically
oﬀ-line and must be scheduled dynamically at runtime. Task-graphs synchronize through point-to-point messages
and do not require any barrier synchronizations. We express stencil computations as dynamic task graphs and use
the known properties of stencil operations to optimize their execution and memory management.
2.1. Task creation
During initialization tasks are created for the ﬁrst two timesteps to solve regions of the matrix e.g. tiles for
a 2D matrix and cubes for a 3D matrix. These initial tasks then create other tasks (along with their incoming

2430

Zubair Wadood Bhatti et al. / Procedia Computer Science 18 (2013) 2428 – 2431

V

W

X

Y

Wt

Z
Spawn

PXt+1
Timestep = t

Vt

Timestep = t+1 Vt+1

Wt

Xt

Yt

Zt

Wt+1

Xt+1

Yt+1

Zt+1

Yt+1

Vt

P1

P2

SYt
te
ea
Cr

Timestep = t+2

-- Finished task
-- Ready for execution

Xt+2

Yt+2

Zt+2

-- Waiting for predecessors

Fig. 2. A dynamic task graph for a three-point stencil. P1 and P2 are processors running TBB[4] work-stealing threads, Vt , Wt , Xt , Yt and Zt
are tasks that process the regions V, W, X, Y, and Z in the timestep t respectively. Upon execution of the task Xt , the task Yt+1 becomes ready
for execution and is thus spawned on a work queue and the task Xt+2 is created.

edges) working on the same regions for the subsequent timesteps. All tasks are created after their predecessors
and two timesteps in advance, as shown in ﬁgure 2. By creating tasks only two timesteps in advance we remove
the possibility that a task is created after one of its predecessors has ﬁnished execution.
Proof: Let Xt be a task that processes the region X in timestep t, PXt and S Xt are the sets of its immediate
predecessors and successors respectively. Create(Xt ), S tart(Xt ) and Finish(Xt ) are the creation, starting and
ﬁnishing times of the task Xt . For any stencil of a constant shape and order S Xt ≡ PXt+2 . If the task Xt creates
the task Xt+2 , Create(Xt+2 ) < Finish(Xt ). ∀T ∈ S Xt , S tart(T ) > Finish(Xt ). Therefore, Create(Xt+2 ) < S tart(T ),
∀T ∈ S Xt , thus, Create(Xt+2 ) < Finish(T ), ∀T ∈ PXt+2 .
2.2. Scheduling
Each processor runs an Intel Threading Building Blocks (TBB) work-stealing thread. Each thread has a pool
(work queue) of processes waiting for execution. A thread executes processes from its own work queue as long
as it is not empty. When the work queue of a thread is empty, it randomly steals tasks from the pools of other
threads. A task becomes ready for execution after it has received messages on all its incoming edges. When a
task becomes ready for execution it is spawned as a TBB task in a work queue. For example, when the task Xt in
ﬁgure 2 ﬁnishes execution it sends a message to all of its successors (Wt+1, Xt+1,Yt+1). Now the task Yt+1 has
a message on all its incoming edges, therefore it becomes ready for execution and is spawned as a TBB task on
a work queue. We stress that a task only becomes ready after all of its predecessors have ﬁnished and sent their
message; therefore, no synchronization is required during its execution.
2.3. Memory allocation and deallocation
We have taken care to preserve data locality, even in the presence of TBB’s work-stealing scheduler and its
“Breadth-First Theft and Depth-First Work” strategy [4]. TBB’s approach can result in the loss of the data locality
property when a TBB child task gets stolen by a thread executing on a diﬀerent socket. If this happens it is
important to reallocate the region to the new processor, essentially letting the data follow the task and reestablish
data locality. For any stencil operation the results produced by the task Xt are no longer required after start of
execution of the task Xt+2 , because S Xt ≡ PXt+2 . If Xt and Xt+2 execute on the same processor, Xt+2 simply
overwrites the results of Xt (data locality is kept), whereas, if they execute on diﬀerent processors Xt+2 deletes the
results of Xt and locally allocates space for its own results (reestablishing data locality).

Zubair Wadood Bhatti et al. / Procedia Computer Science 18 (2013) 2428 – 2431

2431

3. Results and discussion

Total spin waiting time (ms)

We implemented a ﬁve point stencil for a 2D Laplacian heat equation shown in ﬁgure 1 as a dynamic task
graph using the Intel TBB library [4]. The performance of our implementation is compared to the state-of-the-art
stencil compiler Pochoir[1] and the auto-tuning framework Patus[2] along with a naive approach using a parallel
for loop. Two diﬀerent
ﬀ
experiments were performed using diﬀ
ﬀerent domain sizes and processors.
We used the Intel R VTuneTM proﬁler locks-and-waits analysis to measure the synchronization overhead. Figure 3 presents a comparison of the synchronization overhead of the naive, Pochoir and DTGs version for the two
experiments. The Pochoir version requires less synchronization compared to the naive approach and Patus because the hyper-trapezoidal decomposition used in Pochoir allows the computation of several timesteps without
synchronization. Moreover, the synchronization occurs hierarchically within groups of zoids; therefore, this is not
an all-to-all synchronization. However, the DTG version requires on average four times less synchronization than
Pochoir for both experimental settings.
800

Patus

700

Pochoir

600

Naïve parallel_ffor

500

DTG

400
300
200

100
0
N=400x400, t=800 (Xeon E7-4870)

N=4000 x 4000, t=200 (2x Xeon X5660)

Fig. 3. Comparison of synchronization, showing a 4X+ reduction in sychronization costs for both experiments. (1) On a domain of 400x400
double precision ﬂoats for 800 timesteps, using an Intel R Xeon R E7-4870 processor and (2) a domain size of 4000x4000 and 200 timesteps
on a dual socket Intel R Xeon R X5660 system.

4. Conclusion
This paper presents an approach to implement stencil computations as dynamic task graphs in order to minimize synchronization overhead. The approach we propose avoids global barriers (synchronization is only pointto-point) and because tasks are only created when they are ready to be executed there is minimal spin-waiting.
Our experiments show that this signiﬁcantly reduces the synchronization overhead when compared to the stateof-the-art stencil compiler and auto-tuning framework (Pochoir and Patus).
Acknowledgements
This research is partially funded by the Research Fund KU Leuven, Intel and the Institute for the Promotion
of Innovation through Science and Technology in Flanders (IWT). The experiments were performed at the Intel
ExaScience Lab, Belgium.
References
[1] Y. Tang, R. A. Chowdhury, B. C. Kuszmaul, C.-K. Luk, C. E. Leiserson, The pochoir stencil compiler, in: Proceedings of the
23rd ACM symposium on Parallelism in algorithms and architectures, SPAA ’11, ACM, New York, NY, USA, 2011, pp. 117–128.
doi:10.1145/1989493.1989508.
[2] M. Christen, O. Schenk, H. Burkhart, Patus: A code generation and autotuning framework for parallel iterative stencil computations
on modern microarchitectures, in: Parallel Distributed Processing Symposium (IPDPS), 2011 IEEE International, 2011, pp. 676 –687.
doi:10.1109/IPDPS.2011.70.
[3] T. Johnson, T. A. Davis, S. M. Hadﬁeld, A concurrent dynamic task graph, Parallel Computing 22 (2) (1996) 327 – 333. doi:10.1016/
6 01678191(95)00061-5.
[4] W. Kim, M. Voss, Multicore desktop programming with intel threading building blocks, Software, IEEE 28 (1) (2011) 23 –31.
doi:10.1109/MS.2011.12.

